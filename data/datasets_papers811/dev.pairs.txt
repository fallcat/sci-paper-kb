state-of-the-art performance on Something-Something and Jester	Jester	Jester
datasets Something- Something [12] and Jester [30] with fewer parameters. We	Jester	Jester
datasets Something-something v2 [12] and Jester v1 [30] and report our	Jester	Jester
5.5. Results on Jester	Jester	Jester
Jester [30] is a dataset for	Jester	Jester
b) Jester v1 Results	Jester	Jester
with label “Thumb Up” from Jester v1 validation set	Jester	Jester
used in Something- Something and Jester experiments (main paper section 5.4	Jester	Jester
CPNet model on Something-Something and Jester datasets. Lastly in section G	Jester	Jester
Architecture used in Something-Something and Jester Experiments	Jester	Jester
in Something- Something [12] and Jester [30] experiments in Table 8	Jester	Jester
Per-class accuracy of Something-Something and Jester models	Jester	Jester
Architectures used in Something-Something and Jester dataset experiments	Jester	Jester
the respective C2D baseline on Jester in Figure 8 and Something-Something	Jester	Jester
On Jester dataset [30], the largest accuracy	Jester	Jester
accuracy gain in percentage on Jester v1 dataset due to CP	Jester	Jester
the CPNet model used in Jester v1 experiment. The spatial size	Jester	Jester
12] in Figure 12 and Jester [30] in Figure 13. They	Jester	Jester
with label “Drumming Fingers” from Jester v1 validation set	Jester	Jester
with label “Shaking Hand” from Jester v1 validation set	Jester	Jester
with label “Stop Sign” from Jester v1 validation set	Jester	Jester
Pushing Two Fingers Away” from Jester v1 validation set	Jester	Jester
on our final models on Jester v1 dataset. Approach is the	Jester	Jester
jester Dataset V1. https:// 20bn.com/datasets/jester. 2	jester	Jester
jester https://20bn.com/datasets/jester	jester	Jester
Something-Something v1 & v2 and Jester) and scene-related datasets (i.e., Kinetics	Jester	Jester
datasets including Something-Something[11], Kinetics [2], Jester [1], UCF101 [23] and HMDB-51	Jester	Jester
Something v1 & v2 and Jester) and scene-related datasets (i.e., Kinetics-400	Jester	Jester
v1 & v2 [11] and Jester [1]. For these datasets, temporal	Jester	Jester
Something v1 & v2 and Jester, we start with a learning	Jester	Jester
of the STM on the Jester compared with the state-of-the-art methods	Jester	Jester
Something-Something v1 & v2 and Jester	Jester	Jester
and greatly reduced label noise. Jester is a crowd-acted video dataset	Jester	Jester
shows the results on the Jester dataset. Our STM also gains	Jester	Jester
jester dataset v1. In https://20bn.com	jester	Jester
jester	jester	Jester
video datasets - Something- Something, Jester, and Charades - which fundamentally	Jester	Jester
various human gestures on the Jester dataset with very competitive perfor	Jester	Jester
recent video datasets (Something-Something [9], Jester [10], and Charades [11]), which	Jester	Jester
something without actually opening it’. Jester dataset [10] is another recent	Jester	Jester
recognition [9] and on the Jester dataset for hand gesture recognition	Jester	Jester
in early July 2018) [9,28], Jester dataset [10], and Charades dataset	Jester	Jester
Something-V2 174 220,847 human-object interaction Jester 27 148,092 human hand gesture	Jester	Jester
3.3 Results on Jester and Charades	Jester	Jester
the TRN-equipped networks on the Jester dataset, which is a video	Jester	Jester
the validation set of the Jester dataset are listed in Table	Jester	Jester
20BN Jester System 82.34 VideoLSTM 85.86 Guillaume	Jester	Jester
Table 3: Jester Dataset Results on (a) the	Jester	Jester
examples on a) Something-Something, b) Jester, and c) Cha- rades. For	Jester	Jester
example drawn from Something-Something and Jester, the top two predictions with	Jester	Jester
the (a) Something-Something and (b) Jester datasets using the most representative	Jester	Jester
Dataset UCF Kinetics Moments Something Jester Charades	Jester	Jester
TRN on Something- Something and Jester dataset. Only the first 25	Jester	Jester
Something Jester Frames baseline TRN baseline TRN	Jester	Jester
10. : Twentybn jester dataset: a hand gesture dataset	jester	Jester
jester (2017	jester	Jester
their capacity to learn, (2) Jester dataset to inspect their ability	Jester	Jester
2) Jester dataset [1] to learn how	Jester	Jester
Regularization: Although Kinetics-600 and Jester are very large benchmarks and	Jester	Jester
0.2 for Kinetics- 600 and Jester, it is increased to 0.9	Jester	Jester
16-frame clips are used. For Jester benchmark, it is critical to	Jester	Jester
frames from 32 frames for Jester benchmark [17	Jester	Jester
what is being eaten. • Jester dataset is currently the largest	Jester	Jester
Titan XP Jetson TX2 Kinetics-600 Jester UCF-101	Jester	Jester
Unlike Kinetics-600 benchmark, in Jester dataset, spatial content of the	Jester	Jester
the hand. That is why, Jester benchmark is suitable to inspect	Jester	Jester
data. Compared to Kinetics-600 and Jester datasets, UCF-101 contains very little	Jester	Jester
far best perfor- mance in Jester benchmark, although it has inferior	Jester	Jester
more parameters and FLOPs) at Jester benchmark	Jester	Jester
modified flexibly. The results on Jester benchmark show that depthwise convolutions	Jester	Jester
jester	jester	Jester
jester	jester	Jester
jester	jester	Jester
their capacity to learn, (2) Jester dataset to inspect their ability	Jester	Jester
2) Jester dataset [1] to learn how	Jester	Jester
Regularization: Although Kinetics-600 and Jester are very large benchmarks and	Jester	Jester
0.2 for Kinetics- 600 and Jester, it is increased to 0.9	Jester	Jester
16-frame clips are used. For Jester benchmark, it is critical to	Jester	Jester
frames from 32 frames for Jester benchmark [17	Jester	Jester
what is being eaten. • Jester dataset is currently the largest	Jester	Jester
Titan XP Jetson TX2 Kinetics-600 Jester UCF-101	Jester	Jester
Unlike Kinetics-600 benchmark, in Jester dataset, spatial content of the	Jester	Jester
the hand. That is why, Jester benchmark is suitable to inspect	Jester	Jester
data. Compared to Kinetics-600 and Jester datasets, UCF-101 contains very little	Jester	Jester
far best perfor- mance in Jester benchmark, although it has inferior	Jester	Jester
more parameters and FLOPs) at Jester benchmark	Jester	Jester
modified flexibly. The results on Jester benchmark show that depthwise convolutions	Jester	Jester
jester	jester	Jester
jester	jester	Jester
jester	jester	Jester
their capacity to learn, (2) Jester dataset to inspect their ability	Jester	Jester
2) Jester dataset [1] to learn how	Jester	Jester
Regularization: Although Kinetics-600 and Jester are very large benchmarks and	Jester	Jester
0.2 for Kinetics- 600 and Jester, it is increased to 0.9	Jester	Jester
16-frame clips are used. For Jester benchmark, it is critical to	Jester	Jester
frames from 32 frames for Jester benchmark [17	Jester	Jester
what is being eaten. • Jester dataset is currently the largest	Jester	Jester
Titan XP Jetson TX2 Kinetics-600 Jester UCF-101	Jester	Jester
Unlike Kinetics-600 benchmark, in Jester dataset, spatial content of the	Jester	Jester
the hand. That is why, Jester benchmark is suitable to inspect	Jester	Jester
data. Compared to Kinetics-600 and Jester datasets, UCF-101 contains very little	Jester	Jester
far best perfor- mance in Jester benchmark, although it has inferior	Jester	Jester
more parameters and FLOPs) at Jester benchmark	Jester	Jester
modified flexibly. The results on Jester benchmark show that depthwise convolutions	Jester	Jester
jester	jester	Jester
jester	jester	Jester
jester	jester	Jester
Accuracy vs Time on MiniKinetics	MiniKinetics	MiniKinetics
1: Accuracy vs. time on MiniKinetics for different	MiniKinetics	MiniKinetics
vs. time plot on the MiniKinetics	MiniKinetics	MiniKinetics
of the analyses on the MiniKinetics subset contain	MiniKinetics	MiniKinetics
Accuracy vs Time on MiniKinetics	MiniKinetics	MiniKinetics
Impact of α on MiniKinetics	MiniKinetics	MiniKinetics
a) Accuracy vs. time on MiniKinetics for different optical flow approaches	MiniKinetics	MiniKinetics
over all videos of the MiniKinetics validation set. (b) Accuracy of	MiniKinetics	MiniKinetics
different values of α on MiniKinetics with	MiniKinetics	MiniKinetics
Net [34] on MiniKinetics	MiniKinetics	MiniKinetics
train on Kinetics400 and MiniKinetics from scratch. For	MiniKinetics	MiniKinetics
Stream MiniKinetics Kinetics400 UCF101-1 HMDB51-1 Something	MiniKinetics	MiniKinetics
Top-1 accuracy using 16f-clips. For MiniKinetics and Kinetics400, all the streams	MiniKinetics	MiniKinetics
on MiniKinetics	MiniKinetics	MiniKinetics
time per video on the MiniKinetics validation set (250 frames per	MiniKinetics	MiniKinetics
Flow streams alone on MiniKinetics, UCF101, HMDB51	MiniKinetics	MiniKinetics
MiniKinetics using values of α	MiniKinetics	MiniKinetics
obtain a similar performance on MiniKinetics, as when	MiniKinetics	MiniKinetics
other datasets like Sports-1M and ActivityNet	ActivityNet	ActivityNet
19] for sports videos and ActivityNet [12] for human activities. However	ActivityNet	ActivityNet
frames. Therefore, unlike Sports-1M and ActivityNet, YouTube-8M is not restricted to	ActivityNet	ActivityNet
benchmarks such as Sports-1M and ActivityNet	ActivityNet	ActivityNet
annotated with 239 categories, and ActivityNet [12], with ∼200 human activity	ActivityNet	ActivityNet
transfer the learned model to ActivityNet, we used a fully-connected model	ActivityNet	ActivityNet
b) ActivityNet	ActivityNet	ActivityNet
one learnt from scratch on ActivityNet	ActivityNet	ActivityNet
the (a) Sports-1M and (b) ActivityNet	ActivityNet	ActivityNet
5.4 Results on ActivityNet Our final set of experiments	ActivityNet	ActivityNet
learned features for the ActivityNet untrimmed video classification task. Similar	ActivityNet	ActivityNet
directly train- ing on the ActivityNet dataset against pre-training on YouTube-8M	ActivityNet	ActivityNet
all metrics than training on ActivityNet alone. Notably, without the use	ActivityNet	ActivityNet
existing video benchmarks— Sports-1M and ActivityNet	ActivityNet	ActivityNet
setting a new state-of-the-art on ActivityNet	ActivityNet	ActivityNet
un- filtered) for UCF101 and ActivityNet	ActivityNet	ActivityNet
dataset of web images for ActivityNet [1]; a larger scale action	ActivityNet	ActivityNet
half the training videos in ActivityNet (which correspond to 16.2M frames	ActivityNet	ActivityNet
comparable performance when replacing half ActivityNet videos (16.2M frames) with 393K	ActivityNet	ActivityNet
the classes of UCF101 and ActivityNet	ActivityNet	ActivityNet
larger-scale dataset: Ac- tivityNet [1]. ActivityNet contains more classes (203) and	ActivityNet	ActivityNet
samples per class than UCF101. ActivityNet classes are more diverse; they	ActivityNet	ActivityNet
ActivityNet provides samples from 203 activity	ActivityNet	ActivityNet
of Activ- ityNet. Results on ActivityNet are reported in Section 5	ActivityNet	ActivityNet
We also perform experiments on ActivityNet	ActivityNet	ActivityNet
per- formance. Results reported on ActivityNet are produced us- ing the	ActivityNet	ActivityNet
5.1.2 Experimental Setup for ActivityNet	ActivityNet	ActivityNet
Table 6: Although ActivityNet is large-scale, using unfiltered web	ActivityNet	ActivityNet
5.2.2 Experimental Results for ActivityNet	ActivityNet	ActivityNet
half the training videos of ActivityNet are replaced by 393K images	ActivityNet	ActivityNet
discover ‘building-blocks’ from the large-scale ActivityNet dataset using distribution kernels. Essential	ActivityNet	ActivityNet
ActivityNet Kernelised	ActivityNet	ActivityNet
the same essential components in ActivityNet to achieve a matching using	ActivityNet	ActivityNet
jump’ and ‘triple-jump’ in the ActivityNet dataset using tSNE	ActivityNet	ActivityNet
the URL on the large-scale ActivityNet [13] dataset. Cross-dataset UAR experiments	ActivityNet	ActivityNet
HMDB51 contain trimmed videos while ActivityNet contains untrimmed ones. We first	ActivityNet	ActivityNet
Datasets ActivityNet1 consists of 10024 training, 4926	ActivityNet	ActivityNet
on ImageNet and fine-tuned on ActivityNet dataset. Overlap- ping classes between	ActivityNet	ActivityNet
ActivityNet and UCF101 are not used	ActivityNet	ActivityNet
the latest release 1.3 of ActivityNet for our experiments	ActivityNet	ActivityNet
ActivityNet is evenly divided into 5	ActivityNet	ActivityNet
Ghanem, and J. C. Niebles. ActivityNet	ActivityNet	ActivityNet
un- filtered) for UCF101 and ActivityNet	ActivityNet	ActivityNet
dataset of web images for ActivityNet [1]; a larger scale action	ActivityNet	ActivityNet
half the training videos in ActivityNet (which correspond to 16.2M frames	ActivityNet	ActivityNet
comparable performance when replacing half ActivityNet videos (16.2M frames) with 393K	ActivityNet	ActivityNet
the classes of UCF101 and ActivityNet	ActivityNet	ActivityNet
larger-scale dataset: Ac- tivityNet [1]. ActivityNet contains more classes (203) and	ActivityNet	ActivityNet
samples per class than UCF101. ActivityNet classes are more diverse; they	ActivityNet	ActivityNet
ActivityNet provides samples from 203 activity	ActivityNet	ActivityNet
of Activ- ityNet. Results on ActivityNet are reported in Section 5	ActivityNet	ActivityNet
We also perform experiments on ActivityNet	ActivityNet	ActivityNet
per- formance. Results reported on ActivityNet are produced us- ing the	ActivityNet	ActivityNet
5.1.2 Experimental Setup for ActivityNet	ActivityNet	ActivityNet
Table 6: Although ActivityNet is large-scale, using unfiltered web	ActivityNet	ActivityNet
5.2.2 Experimental Results for ActivityNet	ActivityNet	ActivityNet
half the training videos of ActivityNet are replaced by 393K images	ActivityNet	ActivityNet
we set epoch size to 1M clips due to temporal jitterring	1M	Sports-1M
Mini-Sports is a subset of Sports-1M [18], a large-scale video classification	Sports-1M	Sports-1M
as in section 4 for Sports-1M and AudioSet. For Kinetics, we	Sports-1M	Sports-1M
Second, G-Blend, when fine-tuned from Sports-1M, outperforms Shift-Attention Network [6] and	Sports-1M	Sports-1M
Sports- 1M and AudioSet. On Sports-1M, G-Blend significantly outperforms previously published	Sports-1M	Sports-1M
1M [18], a large-scale video classification	1M	Sports-1M
1M videos of 487 different fine-grained	1M	Sports-1M
1M and AudioSet. For Kinetics, we	1M	Sports-1M
1M, outperforms Shift-Attention Network [6] and	1M	Sports-1M
current best methods on Sports- 1M and AudioSet. On Sports-1M, G-Blend	1M	Sports-1M
Sports A + RGB 60.2 RGB	Sports	Sports-1M
Sports using video top-1 validation accuracy	Sports	Sports-1M
Sports, and mini- AudioSet. Kinetics is	Sports	Sports-1M
Mini-Sports is a subset of Sports	Sports	Sports-1M
Sports, 13% for mini-AudioSet). For evaluation	Sports	Sports-1M
Sports (right). On both Kinetics and	Sports	Sports-1M
Sports, the audio model overfits the	Sports	Sports-1M
Sports Learning Curve	Sports	Sports-1M
Sports	Sports	Sports-1M
Sports (right). Solid lines plot validation	Sports	Sports-1M
Sports), and acoustic event detection (mini-AudioSet	Sports	Sports-1M
Sports	Sports	Sports-1M
Sports demonstrates that the weights used	Sports	Sports-1M
Sports mini-AudioSet Method Clip V@1 V@5	Sports	Sports-1M
Sports, and mini-AudioSet. G-Blend consistently outperforms	Sports	Sports-1M
as in section 4 for Sports	Sports	Sports-1M
Second, G-Blend, when fine-tuned from Sports	Sports	Sports-1M
with current best methods on Sports	Sports	Sports-1M
- 1M and AudioSet. On Sports	Sports	Sports-1M
to the state-of-the- art on Sports-1M, Kinetics, UCF101, and HMDB51	Sports-1M	Sports-1M
the state-of-the-art on the challenging Sports-1M benchmark. This result is both	Sports-1M	Sports-1M
performance on both Kinetics and Sports-1M	Sports-1M	Sports-1M
recognition accuracy. For example, on Sports-1M using RGB as input, R(2+1)D	Sports-1M	Sports-1M
R3D, R2D, and P3D on Sports-1M (see Table 4	Sports-1M	Sports-1M
We use Kinetics [4] and Sports-1M [16] as the primary benchmarks	Sports-1M	Sports-1M
by pretraining our models on Sports-1M and Kinetics, and finetuning them	Sports-1M	Sports-1M
Comparison with the state-of-the-art on Sports-1M	Sports-1M	Sports-1M
ture on four public benchmarks. Sports-1M is a large-scale dataset for	Sports-1M	Sports-1M
all 3 splits. Results on Sports-1M	Sports-1M	Sports-1M
R(2+1)D-Two-Stream none 73.9 90.9 R(2+1)D-RGB Sports-1M 74.3 91.4 R(2+1)D-Flow Sports-1M 68.5	Sports-1M	Sports-1M
R(2+1)D-Two-Stream Sports-1M 75.4 91.9	Sports-1M	Sports-1M
on RGB. R(2+1)D pretrained on Sports-1M outperforms I3D pre- trained on	Sports-1M	Sports-1M
baseline for comparison. Videos in Sports-1M are very long, over 5	Sports-1M	Sports-1M
4 shows the results on Sports-1M	Sports-1M	Sports-1M
the best published result on Sports-1M	Sports-1M	Sports-1M
finetuning the model pretrained on Sports-1M	Sports-1M	Sports-1M
Our R(2+1)D pre- trained on Sports-1M also outperforms I3D pretrained on	Sports-1M	Sports-1M
20] using models pretrained on Sports-1M and	Sports-1M	Sports-1M
92.4 62.0 Conv Pooling [42] Sports-1M 88.6 - FSTCN [33] ImageNet	Sports-1M	Sports-1M
that Kinetics is better than Sports-1M for pretraining our models	Sports-1M	Sports-1M
Kinetics (not those finetuned from Sports-1M) in order to understand the	Sports-1M	Sports-1M
1M, Kinetics, UCF101, and HMDB51	1M	Sports-1M
1M benchmark. This result is both	1M	Sports-1M
recognition benchmarks such as Sports- 1M [16] and Kinetics [17	1M	Sports-1M
1M	1M	Sports-1M
1M using RGB as input, R(2+1)D	1M	Sports-1M
1M (see Table 4	1M	Sports-1M
1M [16] as the primary benchmarks	1M	Sports-1M
1M and Kinetics, and finetuning them	1M	Sports-1M
set epoch size to be 1M for tem- poral jittering. The	1M	Sports-1M
1M	1M	Sports-1M
1M is a large-scale dataset for	1M	Sports-1M
1M videos of 487 fine-grained sport	1M	Sports-1M
1M	1M	Sports-1M
We train R(2+1)D-34 on Sports- 1M [16] with both 8-frame and	1M	Sports-1M
1M 74.3 91.4 R(2+1)D-Flow Sports-1M 68.5	1M	Sports-1M
1M 75.4 91.9	1M	Sports-1M
1M outperforms I3D pre- trained on	1M	Sports-1M
1M are very long, over 5	1M	Sports-1M
1M	1M	Sports-1M
1M	1M	Sports-1M
1M	1M	Sports-1M
1M also outperforms I3D pretrained on	1M	Sports-1M
1M and	1M	Sports-1M
1M 88.6 - FSTCN [33] ImageNet	1M	Sports-1M
1M for pretraining our models	1M	Sports-1M
1M) in order to understand the	1M	Sports-1M
of the art on Sports- 1M, Kinetics, UCF101, and HMDB51. We	1M	Sports-1M
to the state-of-the- art on Sports	Sports	Sports-1M
the state-of-the-art on the challenging Sports	Sports	Sports-1M
action recognition benchmarks such as Sports	Sports	Sports-1M
performance on both Kinetics and Sports	Sports	Sports-1M
recognition accuracy. For example, on Sports	Sports	Sports-1M
R3D, R2D, and P3D on Sports	Sports	Sports-1M
We use Kinetics [4] and Sports	Sports	Sports-1M
by pretraining our models on Sports	Sports	Sports-1M
Comparison with the state-of-the-art on Sports	Sports	Sports-1M
ture on four public benchmarks. Sports	Sports	Sports-1M
all 3 splits. Results on Sports	Sports	Sports-1M
-1M. We train R(2+1)D-34 on Sports	Sports	Sports-1M
R(2+1)D-Two-Stream none 73.9 90.9 R(2+1)D-RGB Sports	Sports	Sports-1M
-1M 74.3 91.4 R(2+1)D-Flow Sports	Sports	Sports-1M
R(2+1)D-Two-Stream Sports	Sports	Sports-1M
on RGB. R(2+1)D pretrained on Sports	Sports	Sports-1M
baseline for comparison. Videos in Sports	Sports	Sports-1M
4 shows the results on Sports	Sports	Sports-1M
the best published result on Sports	Sports	Sports-1M
finetuning the model pretrained on Sports	Sports	Sports-1M
Our R(2+1)D pre- trained on Sports	Sports	Sports-1M
20] using models pretrained on Sports	Sports	Sports-1M
92.4 62.0 Conv Pooling [42] Sports	Sports	Sports-1M
that Kinetics is better than Sports	Sports	Sports-1M
Kinetics (not those finetuned from Sports	Sports	Sports-1M
state of the art on Sports	Sports	Sports-1M
to the state-of-the- art on Sports-1M, Kinetics, UCF101, and HMDB51	Sports-1M	Sports-1M
the state-of-the-art on the challenging Sports-1M benchmark. This result is both	Sports-1M	Sports-1M
performance on both Kinetics and Sports-1M	Sports-1M	Sports-1M
recognition accuracy. For example, on Sports-1M using RGB as input, R(2+1)D	Sports-1M	Sports-1M
R3D, R2D, and P3D on Sports-1M (see Table 4	Sports-1M	Sports-1M
We use Kinetics [4] and Sports-1M [16] as the primary benchmarks	Sports-1M	Sports-1M
by pretraining our models on Sports-1M and Kinetics, and finetuning them	Sports-1M	Sports-1M
Comparison with the state-of-the-art on Sports-1M	Sports-1M	Sports-1M
ture on four public benchmarks. Sports-1M is a large-scale dataset for	Sports-1M	Sports-1M
all 3 splits. Results on Sports-1M	Sports-1M	Sports-1M
R(2+1)D-Two-Stream none 73.9 90.9 R(2+1)D-RGB Sports-1M 74.3 91.4 R(2+1)D-Flow Sports-1M 68.5	Sports-1M	Sports-1M
R(2+1)D-Two-Stream Sports-1M 75.4 91.9	Sports-1M	Sports-1M
on RGB. R(2+1)D pretrained on Sports-1M outperforms I3D pre- trained on	Sports-1M	Sports-1M
baseline for comparison. Videos in Sports-1M are very long, over 5	Sports-1M	Sports-1M
4 shows the results on Sports-1M	Sports-1M	Sports-1M
the best published result on Sports-1M	Sports-1M	Sports-1M
finetuning the model pretrained on Sports-1M	Sports-1M	Sports-1M
Our R(2+1)D pre- trained on Sports-1M also outperforms I3D pretrained on	Sports-1M	Sports-1M
20] using models pretrained on Sports-1M and	Sports-1M	Sports-1M
92.4 62.0 Conv Pooling [42] Sports-1M 88.6 - FSTCN [33] ImageNet	Sports-1M	Sports-1M
that Kinetics is better than Sports-1M for pretraining our models	Sports-1M	Sports-1M
Kinetics (not those finetuned from Sports-1M) in order to understand the	Sports-1M	Sports-1M
1M, Kinetics, UCF101, and HMDB51	1M	Sports-1M
1M benchmark. This result is both	1M	Sports-1M
recognition benchmarks such as Sports- 1M [16] and Kinetics [17	1M	Sports-1M
1M	1M	Sports-1M
1M using RGB as input, R(2+1)D	1M	Sports-1M
1M (see Table 4	1M	Sports-1M
1M [16] as the primary benchmarks	1M	Sports-1M
1M and Kinetics, and finetuning them	1M	Sports-1M
set epoch size to be 1M for tem- poral jittering. The	1M	Sports-1M
1M	1M	Sports-1M
1M is a large-scale dataset for	1M	Sports-1M
1M videos of 487 fine-grained sport	1M	Sports-1M
1M	1M	Sports-1M
We train R(2+1)D-34 on Sports- 1M [16] with both 8-frame and	1M	Sports-1M
1M 74.3 91.4 R(2+1)D-Flow Sports-1M 68.5	1M	Sports-1M
1M 75.4 91.9	1M	Sports-1M
1M outperforms I3D pre- trained on	1M	Sports-1M
1M are very long, over 5	1M	Sports-1M
1M	1M	Sports-1M
1M	1M	Sports-1M
1M	1M	Sports-1M
1M also outperforms I3D pretrained on	1M	Sports-1M
1M and	1M	Sports-1M
1M 88.6 - FSTCN [33] ImageNet	1M	Sports-1M
1M for pretraining our models	1M	Sports-1M
1M) in order to understand the	1M	Sports-1M
of the art on Sports- 1M, Kinetics, UCF101, and HMDB51. We	1M	Sports-1M
to the state-of-the- art on Sports	Sports	Sports-1M
the state-of-the-art on the challenging Sports	Sports	Sports-1M
action recognition benchmarks such as Sports	Sports	Sports-1M
performance on both Kinetics and Sports	Sports	Sports-1M
recognition accuracy. For example, on Sports	Sports	Sports-1M
R3D, R2D, and P3D on Sports	Sports	Sports-1M
We use Kinetics [4] and Sports	Sports	Sports-1M
by pretraining our models on Sports	Sports	Sports-1M
Comparison with the state-of-the-art on Sports	Sports	Sports-1M
ture on four public benchmarks. Sports	Sports	Sports-1M
all 3 splits. Results on Sports	Sports	Sports-1M
-1M. We train R(2+1)D-34 on Sports	Sports	Sports-1M
R(2+1)D-Two-Stream none 73.9 90.9 R(2+1)D-RGB Sports	Sports	Sports-1M
-1M 74.3 91.4 R(2+1)D-Flow Sports	Sports	Sports-1M
R(2+1)D-Two-Stream Sports	Sports	Sports-1M
on RGB. R(2+1)D pretrained on Sports	Sports	Sports-1M
baseline for comparison. Videos in Sports	Sports	Sports-1M
4 shows the results on Sports	Sports	Sports-1M
the best published result on Sports	Sports	Sports-1M
finetuning the model pretrained on Sports	Sports	Sports-1M
Our R(2+1)D pre- trained on Sports	Sports	Sports-1M
20] using models pretrained on Sports	Sports	Sports-1M
92.4 62.0 Conv Pooling [42] Sports	Sports	Sports-1M
that Kinetics is better than Sports	Sports	Sports-1M
Kinetics (not those finetuned from Sports	Sports	Sports-1M
state of the art on Sports	Sports	Sports-1M
is below 2% on the Sports-1M benchmarks [14]. As a result	Sports-1M	Sports-1M
is the case with the Sports-1M dataset), they can still provide	Sports-1M	Sports-1M
two different video classification tasks: Sports-1M (Section 4.1) and UCF-101 (Section	Sports-1M	Sports-1M
model and then fine-tuned on Sports-1M videos	Sports-1M	Sports-1M
the Sports-1M and UCF-101 datasets with the	Sports-1M	Sports-1M
4.1. Sports-1M dataset The Sports-1M dataset [14] consists of roughly	Sports-1M	Sports-1M
Although Sports-1M is the largest publicly available	Sports-1M	Sports-1M
pooling architectures (Figure 2) on Sports-1M using a 120- frame AlexNet	Sports-1M	Sports-1M
improvements in Hit@1 on the Sports-1M dataset	Sports-1M	Sports-1M
feature pooling architectures on the Sports-1M dataset when using a 120	Sports-1M	Sports-1M
single-frames selected at random from Sports-1M videos. Results (Table 2) show	Sports-1M	Sports-1M
and LSTM. Experiments performed on Sports-1M using 30-frame Conv-Pooling and LSTM	Sports-1M	Sports-1M
Optical flow is noisy on Sports-1M and if used alone, results	Sports-1M	Sports-1M
number of noisy images in Sports-1M com- pared to ImageNet	Sports-1M	Sports-1M
the previous state-of-art on the Sports-1M dataset at the time of	Sports-1M	Sports-1M
models that are trained in Sports-1M dataset perform in UCF-101	Sports-1M	Sports-1M
prior work on the in Sports-1M dataset. Hit@1, and Hit@5 are	Sports-1M	Sports-1M
Compared to Sports-1M, optical flow in UCF-101 pro	Sports-1M	Sports-1M
state-of-the-art performance on both the Sports-1M and UCF-101 benchmarks, supporting the	Sports-1M	Sports-1M
is the case in the Sports-1M dataset. In order to take	Sports-1M	Sports-1M
per- formance measure for the Sports-1M benchmark	Sports-1M	Sports-1M
1M benchmarks [14]. As a result	1M	Sports-1M
1M dataset), they can still provide	1M	Sports-1M
1M (Section 4.1) and UCF-101 (Section	1M	Sports-1M
1M videos	1M	Sports-1M
1M and UCF-101 datasets with the	1M	Sports-1M
1M dataset The Sports-1M dataset [14	1M	Sports-1M
1M is the largest publicly available	1M	Sports-1M
1M using a 120- frame AlexNet	1M	Sports-1M
1M dataset	1M	Sports-1M
1M dataset when using a 120	1M	Sports-1M
1M videos. Results (Table 2) show	1M	Sports-1M
1M using 30-frame Conv-Pooling and LSTM	1M	Sports-1M
1M and if used alone, results	1M	Sports-1M
1M com- pared to ImageNet	1M	Sports-1M
1M dataset at the time of	1M	Sports-1M
1M dataset perform in UCF-101	1M	Sports-1M
1M dataset. Hit@1, and Hit@5 are	1M	Sports-1M
1M, optical flow in UCF-101 pro	1M	Sports-1M
1M and UCF-101 benchmarks, supporting the	1M	Sports-1M
1M dataset. In order to take	1M	Sports-1M
1M benchmark	1M	Sports-1M
previously published results on the Sports 1 mil- lion dataset (73.1	Sports	Sports-1M
is below 2% on the Sports	Sports	Sports-1M
is the case with the Sports	Sports	Sports-1M
two different video classification tasks: Sports	Sports	Sports-1M
model and then fine-tuned on Sports	Sports	Sports-1M
the Sports	Sports	Sports-1M
4.1. Sports-1M dataset The Sports	Sports	Sports-1M
Although Sports	Sports	Sports-1M
pooling architectures (Figure 2) on Sports	Sports	Sports-1M
improvements in Hit@1 on the Sports	Sports	Sports-1M
feature pooling architectures on the Sports	Sports	Sports-1M
single-frames selected at random from Sports	Sports	Sports-1M
and LSTM. Experiments performed on Sports	Sports	Sports-1M
Optical flow is noisy on Sports	Sports	Sports-1M
number of noisy images in Sports	Sports	Sports-1M
be expected given that the Sports dataset consists of YouTube videos	Sports	Sports-1M
the previous state-of-art on the Sports	Sports	Sports-1M
models that are trained in Sports	Sports	Sports-1M
prior work on the in Sports	Sports	Sports-1M
Compared to Sports	Sports	Sports-1M
state-of-the-art performance on both the Sports	Sports	Sports-1M
is the case in the Sports	Sports	Sports-1M
per- formance measure for the Sports	Sports	Sports-1M
ResNet achieves clear improvements on Sports-1M video classifica- tion dataset against	Sports-1M	Sports-1M
Comparisons of different models on Sports-1M dataset in terms of accuracy	Sports-1M	Sports-1M
fine-tuning ResNet-152 with frames in Sports-1M dataset [10] may achieve better	Sports-1M	Sports-1M
and Top-1&5 video-level accuracy on Sports-1M	Sports-1M	Sports-1M
ResNet here was conducted on Sports-1M dataset [10], which is one	Sports-1M	Sports-1M
efficient training on the large Sports-1M training set, we randomly select	Sports-1M	Sports-1M
architecture could be trained on Sports-1M dataset from scratch or fine	Sports-1M	Sports-1M
our P3D ResNet architecture on Sports-1M dataset, the networks could be	Sports-1M	Sports-1M
from those sport-related data in Sports-1M bench- mark, resulting in not	Sports-1M	Sports-1M
by C3D learnt purely on Sports-1M data. Instead, ResNet-152 trained on	Sports-1M	Sports-1M
1M video classifica- tion dataset against	1M	Sports-1M
1M dataset in terms of accuracy	1M	Sports-1M
1M dataset [10] may achieve better	1M	Sports-1M
1M	1M	Sports-1M
1M dataset [10], which is one	1M	Sports-1M
1M training set, we randomly select	1M	Sports-1M
1M dataset from scratch or fine	1M	Sports-1M
1M dataset, the networks could be	1M	Sports-1M
1M bench- mark, resulting in not	1M	Sports-1M
1M data. Instead, ResNet-152 trained on	1M	Sports-1M
ResNet architecture learnt on Sports- 1M dataset validate our proposal and	1M	Sports-1M
ResNet achieves clear improvements on Sports	Sports	Sports-1M
Comparisons of different models on Sports	Sports	Sports-1M
fine-tuning ResNet-152 with frames in Sports	Sports	Sports-1M
and Top-1&5 video-level accuracy on Sports	Sports	Sports-1M
ResNet here was conducted on Sports	Sports	Sports-1M
million videos annotated with 487 Sports labels. There are 1K-3K videos	Sports	Sports-1M
efficient training on the large Sports	Sports	Sports-1M
architecture could be trained on Sports	Sports	Sports-1M
our P3D ResNet architecture on Sports	Sports	Sports-1M
from those sport-related data in Sports	Sports	Sports-1M
by C3D learnt purely on Sports	Sports	Sports-1M
P3D ResNet architecture learnt on Sports	Sports	Sports-1M
generalizes to other datasets like Sports-1M and ActivityNet. We achieve state-of-the-art	Sports-1M	Sports-1M
with the avail- ability of Sports-1M [19] for sports videos and	Sports-1M	Sports-1M
given its frames. Therefore, unlike Sports-1M and ActivityNet, YouTube-8M is not	Sports-1M	Sports-1M
on other benchmarks such as Sports-1M and ActivityNet	Sports-1M	Sports-1M
available video benchmarks are the Sports-1M [19], with 487 sports related	Sports-1M	Sports-1M
is 10 − 15 seconds, Sports-1M is 336 seconds and in	Sports-1M	Sports-1M
perform full fine-tuning experiments on Sports-1M, which is large enough to	Sports-1M	Sports-1M
for other tasks, such as Sports-1M sports classification and AcitvityNet activity	Sports-1M	Sports-1M
5.3 Results on Sports-1M Next, we investigate generalization of	Sports-1M	Sports-1M
learn- ing experiments on the Sports-1M dataset. The Sports-1M dataset [19	Sports-1M	Sports-1M
a) Sports-1M	Sports-1M	Sports-1M
YouTube-8M dataset to the (a) Sports-1M and (b) ActivityNet	Sports-1M	Sports-1M
and fine-tune them on the Sports-1M dataset (along with a new	Sports-1M	Sports-1M
various video-level representations on the Sports-1M dataset. Our learned features are	Sports-1M	Sports-1M
of the videos in the Sports-1M dataset, including optical flow, and	Sports-1M	Sports-1M
video classification task. Similar to Sports-1M experiments, we compare directly train	Sports-1M	Sports-1M
experiments on existing video benchmarks— Sports-1M and ActivityNet. Our experiments show	Sports-1M	Sports-1M
1M and ActivityNet. We achieve state-of-the-art	1M	Sports-1M
1M [19] for sports videos and	1M	Sports-1M
1M and ActivityNet, YouTube-8M is not	1M	Sports-1M
1M and ActivityNet	1M	Sports-1M
1M [19], with 487 sports related	1M	Sports-1M
activities and 1M videos, the YFCC-100M [34], with	1M	Sports-1M
1M is 336 seconds and in	1M	Sports-1M
1M, which is large enough to	1M	Sports-1M
1M sports classification and AcitvityNet activity	1M	Sports-1M
1M Next, we investigate generalization of	1M	Sports-1M
1M dataset. The Sports-1M dataset [19	1M	Sports-1M
1M	1M	Sports-1M
1M and (b) ActivityNet	1M	Sports-1M
1M dataset (along with a new	1M	Sports-1M
1M dataset. Our learned features are	1M	Sports-1M
1M dataset, including optical flow, and	1M	Sports-1M
on such a large dataset (1M videos), pre-training on YouTube-8M still	1M	Sports-1M
1M experiments, we compare directly train	1M	Sports-1M
1M and ActivityNet. Our experiments show	1M	Sports-1M
generalizes to other datasets like Sports	Sports	Sports-1M
with the avail- ability of Sports	Sports	Sports-1M
given its frames. Therefore, unlike Sports	Sports	Sports-1M
on other benchmarks such as Sports	Sports	Sports-1M
available video benchmarks are the Sports	Sports	Sports-1M
Action-adventure game Strategy video game Sports game Call of Duty Grand	Sports	Sports-1M
My Little Pony Nike; Inc. Sports Motorsport Football Winter sport Cycling	Sports	Sports-1M
is 10 − 15 seconds, Sports	Sports	Sports-1M
perform full fine-tuning experiments on Sports	Sports	Sports-1M
for other tasks, such as Sports	Sports	Sports-1M
5.3 Results on Sports	Sports	Sports-1M
learn- ing experiments on the Sports	Sports	Sports-1M
-1M dataset. The Sports	Sports	Sports-1M
a) Sports	Sports	Sports-1M
YouTube-8M dataset to the (a) Sports	Sports	Sports-1M
and fine-tune them on the Sports	Sports	Sports-1M
various video-level representations on the Sports	Sports	Sports-1M
of the videos in the Sports	Sports	Sports-1M
video classification task. Similar to Sports	Sports	Sports-1M
experiments on existing video benchmarks— Sports	Sports	Sports-1M
architectures, we collected a new Sports-1M dataset, which consists of 1	Sports-1M	Sports-1M
classes of sports. We make Sports-1M available to the re- search	Sports-1M	Sports-1M
whether features learned on the Sports-1M dataset are generic enough to	Sports-1M	Sports-1M
low-level features learned on the Sports-1M dataset than by training the	Sports-1M	Sports-1M
categories (which we release as Sports-1M dataset) and report significant gains	Sports-1M	Sports-1M
first present results on our Sports-1M dataset and	Sports-1M	Sports-1M
4.1. Experiments on Sports-1M	Sports-1M	Sports-1M
Dataset. The Sports-1M dataset consists of 1 million	Sports-1M	Sports-1M
Figure 4: Predictions on Sports-1M test data. Blue (first row	Sports-1M	Sports-1M
the 200,000 videos of the Sports-1M test set. Hit@k values indicate	Sports-1M	Sports-1M
results. The results for the Sports-1M	Sports-1M	Sports-1M
of our analysis on the Sports-1M dataset in	Sports-1M	Sports-1M
we cannot guarantee that the Sports-1M dataset has no overlap with	Sports-1M	Sports-1M
provides the best performance on Sports-1M	Sports-1M	Sports-1M
1M dataset, which consists of 1	1M	Sports-1M
1M available to the re- search	1M	Sports-1M
1M dataset are generic enough to	1M	Sports-1M
1M dataset than by training the	1M	Sports-1M
1M dataset) and report significant gains	1M	Sports-1M
1M dataset and	1M	Sports-1M
1M	1M	Sports-1M
1M dataset consists of 1 million	1M	Sports-1M
1M test data. Blue (first row	1M	Sports-1M
1M test set. Hit@k values indicate	1M	Sports-1M
1M	1M	Sports-1M
1M dataset in	1M	Sports-1M
1M dataset has no overlap with	1M	Sports-1M
1M	1M	Sports-1M
architectures, we collected a new Sports	Sports	Sports-1M
classes of sports. We make Sports	Sports	Sports-1M
whether features learned on the Sports	Sports	Sports-1M
low-level features learned on the Sports	Sports	Sports-1M
categories (which we release as Sports	Sports	Sports-1M
used datasets (KTH, Weizmann, UCF Sports, IXMAS, Hollywood 2, UCF-50) only	Sports	Sports-1M
first present results on our Sports	Sports	Sports-1M
4.1. Experiments on Sports	Sports	Sports-1M
Dataset. The Sports	Sports	Sports-1M
internal nodes such as Aquatic Sports, Team Sports, Winter Sports, Ball	Sports	Sports-1M
Sports, Combat Sports, Sports with Animals, and generally becomes	Sports	Sports-1M
Figure 4: Predictions on Sports	Sports	Sports-1M
the 200,000 videos of the Sports	Sports	Sports-1M
results. The results for the Sports	Sports	Sports-1M
Sports class Δ AP Δ AP	Sports	Sports-1M
Sports class Juggling Club 0.12 -0.07	Sports	Sports-1M
per-class average precision for all Sports classes and highlight the ones	Sports	Sports-1M
of our analysis on the Sports	Sports	Sports-1M
flute, guitar, piano, etc.) and Sports	Sports	Sports-1M
the performance improve- ments on Sports classes relative to classes from	Sports	Sports-1M
initialize with a fully trained Sports CNN and then begin training	Sports	Sports-1M
same evaluation protocol as for Sports across the 3 suggested folds	Sports	Sports-1M
Musical Instruments 0.42 0.65 0.46 Sports 0.57 0.79 0.80 All groups	Sports	Sports-1M
we cannot guarantee that the Sports	Sports	Sports-1M
provides the best performance on Sports	Sports	Sports-1M
can be attributed to the Sports categories in UCF-101, but the	Sports	Sports-1M
to improvements on non-Sports categories: Sports per- formance only decreases from	Sports	Sports-1M
a dynamic dataset such as Sports	Sports	Sports-1M
range of benchmarks except for Sports-1M and UCF101. On UCF101, we	Sports-1M	Sports-1M
we train our C3D on Sports-1M dataset [18] which is currently	Sports-1M	Sports-1M
Training is done on the Sports-1M train split. As Sports-1M has	Sports-1M	Sports-1M
Sports-1M classification results: Table 2 presents	Sports-1M	Sports-1M
Table 2. Sports-1M classification result. C3D outperforms [18	Sports-1M	Sports-1M
on I380K, C3D trained on Sports-1M, and C3D trained on I380K	Sports-1M	Sports-1M
and fine-tuned on Sports-1M	Sports-1M	Sports-1M
from their model pre-trained on Sports-1M, spa- tial stream network in	Sports-1M	Sports-1M
addition, C3D is trained on Sports-1M and used as is without	Sports-1M	Sports-1M
1M and UCF101. On UCF101, we	1M	Sports-1M
1M dataset [18] which is currently	1M	Sports-1M
categories. Compared with UCF101, Sports- 1M has 5 times the number	1M	Sports-1M
1M train split. As Sports-1M has	1M	Sports-1M
1M classification results: Table 2 presents	1M	Sports-1M
1M classification result. C3D outperforms [18	1M	Sports-1M
1M, and C3D trained on I380K	1M	Sports-1M
1M	1M	Sports-1M
1M, spa- tial stream network in	1M	Sports-1M
1M and used as is without	1M	Sports-1M
is trained only on Sports- 1M videos without any fine-tuning while	1M	Sports-1M
range of benchmarks except for Sports	Sports	Sports-1M
we train our C3D on Sports	Sports	Sports-1M
sports categories. Compared with UCF101, Sports	Sports	Sports-1M
Training is done on the Sports	Sports	Sports-1M
-1M train split. As Sports	Sports	Sports-1M
Sports	Sports	Sports-1M
Table 2. Sports	Sports	Sports-1M
on I380K, C3D trained on Sports	Sports	Sports-1M
on I380K and fine-tuned on Sports	Sports	Sports-1M
from their model pre-trained on Sports	Sports	Sports-1M
addition, C3D is trained on Sports	Sports	Sports-1M
C3D is trained only on Sports	Sports	Sports-1M
multiple datasets we test, including HMDB, Kinetics, and Moments in Time	HMDB	HMDB51
public datasets we tested, including HMDB, Kinetics, and Moments in time	HMDB	HMDB51
detail, we use following datasets: HMDB [12] is a dataset of	HMDB	HMDB51
Table 1. HMDB split 1 comparison to baselines	HMDB	HMDB51
HMDB HMDB	HMDB	HMDB51
Table 2. HMDB performances averaged over the 3	HMDB	HMDB51
HMDB	HMDB	HMDB51
Method Kinetics Charades HMDB MiT	HMDB	HMDB51
38.1 81.8 31.1 Evolved on HMDB 77.0 37.5 82.3 31.6 Best	HMDB	HMDB51
of them when evolved for HMDB or Kinetics. An average activity	HMDB	HMDB51
are around 12 seconds, while HMDB and Kinetics videos are on	HMDB	HMDB51
search vs. evolutionary algorithm on HMDB	HMDB	HMDB51
T. Poggio, and T. Serre. HMDB	HMDB	HMDB51
176 × 176 (for HMDB and Kinetics) or 64	HMDB	HMDB51
T. Poggio, and T. Serre. HMDB	HMDB	HMDB51
such as Kinetics [13] and HMDB [14]. We hypothesize that the	HMDB	HMDB51
infor- mation. Note that our HMDB performance in this table is	HMDB	HMDB51
HMDB51 datasets using 100 iterations to	HMDB	HMDB51
HMDB	HMDB	HMDB51
HMDB	HMDB	HMDB51
Tiny-Kinetics HMDB	HMDB	HMDB51
in- puts. Note that the HMDB performance in [21] was reported	HMDB	HMDB51
HMDB	HMDB	HMDB51
on Kinetics before training/testing with HMDB	HMDB	HMDB51
Kinetics HMDB HMDB	HMDB	HMDB51
the state-of-the-arts on Kinetics and HMDB	HMDB	HMDB51
T. Poggio, and T. Serre. HMDB	HMDB	HMDB51
V100 GPUs. When fine-tuning on HMDB, the learning rate started at	HMDB	HMDB51
dropout at 0.5 and for HMDB it was set to 0.8	HMDB	HMDB51
HMDB51 datasets using 100 iterations to	HMDB51	HMDB51
action classification datasets (UCF-101 and HMDB	HMDB	HMDB51
action classification, reaching 80.9% on HMDB	HMDB	HMDB51
magnitude larger than previous datasets, HMDB	HMDB	HMDB51
and then fine-tuning each on HMDB	HMDB	HMDB51
layer, showing some improvement on HMDB while requiring less test time	HMDB	HMDB51
5k steps on UCF-101 and HMDB	HMDB	HMDB51
and testing on either UCF-101, HMDB	HMDB	HMDB51
test sets of UCF-101 and HMDB	HMDB	HMDB51
UCF-101 HMDB	HMDB	HMDB51
testing on split 1 of HMDB	HMDB	HMDB51
rameters and that UCF-101 and HMDB	HMDB	HMDB51
is however higher than on HMDB	HMDB	HMDB51
lack of training data in HMDB	HMDB	HMDB51
on UCF-101, much higher on HMDB	HMDB	HMDB51
HMDB	HMDB	HMDB51
HMDB	HMDB	HMDB51
HMDB	HMDB	HMDB51
HMDB	HMDB	HMDB51
HMDB	HMDB	HMDB51
directly training on UCF-101 and HMDB	HMDB	HMDB51
table 5, on UCF-101 and HMDB	HMDB	HMDB51
on UCF-101 and 70.3% on HMDB	HMDB	HMDB51
UCF-101 HMDB	HMDB	HMDB51
Performance on the UCF-101 and HMDB	HMDB	HMDB51
Original: train on UCF-101 or HMDB	HMDB	HMDB51
layer trained on UCF-101 or HMDB	HMDB	HMDB51
end-to-end fine-tuning on UCF-101 or HMDB	HMDB	HMDB51
Model UCF-101 HMDB	HMDB	HMDB51
state-of-the-art on the UCF-101 and HMDB	HMDB	HMDB51
on UCF-101 and 80.9 on HMDB	HMDB	HMDB51
HMDB	HMDB	HMDB51
T. Poggio, and T. Serre. HMDB	HMDB	HMDB51
Iwashita et al. 2014) and HMDB dataset (Kuehne et al. 2011	HMDB	HMDB51
is an extremely challenging dataset. HMDB was chosen due to its	HMDB	HMDB51
DogCentric) or 51 nodes (for HMDB) and used soft-max. The network	HMDB	HMDB51
HMDB HMDB is a relatively large-scale video	HMDB	HMDB51
the ap- proaches using the HMDB Dataset. In this experiment, we	HMDB	HMDB51
per-activity temporal filters, tested with HMDB	HMDB	HMDB51
T.; and Serre, T. 2011. HMDB	HMDB	HMDB51
HMDB	HMDB	HMDB51
400 [19] for the ablation experi	400	Kinetics-400
contains about 260K videos of 400 different human action categories. We	400	Kinetics-400
400 [19]. Sports1M is a large-scale	400	Kinetics-400
Dataset. We use Kinetics-400 [19] for the ablation experi	Kinetics-400	Kinetics-400
CSNs on Sports1M [18] and Kinetics-400 [19]. Sports1M is a large-scale	Kinetics-400	Kinetics-400
yet accurate. On Sports1M and Kinetics, our CSNs are com- parable	Kinetics	Kinetics-400
art methods on Sports1M and Kinetics while being 2–3 times faster	Kinetics	Kinetics-400
Dataset. We use Kinetics	Kinetics	Kinetics-400
experi- ments in this section. Kinetics is a standard benchmark for	Kinetics	Kinetics-400
model ob- tains 69.7% on Kinetics validation (vs. 70.3% of vanilla	Kinetics	Kinetics-400
for ip-CSN-101 and ResNet3D-101 on Kinetics	Kinetics	Kinetics-400
video top-1 accuracy on the Kinetics validation set vs the model	Kinetics	Kinetics-400
Video top-1 accuracy on the Kinetics validation set against computation cost	Kinetics	Kinetics-400
CSNs on Sports1M [18] and Kinetics	Kinetics	Kinetics-400
provided with the dataset. For Kinetics, we use the train split	Kinetics	Kinetics-400
for video prediction. On Kinetics, since the 30 crops eval	Kinetics	Kinetics-400
from our ablation. Results on Kinetics	Kinetics	Kinetics-400
Accuracy is measured on the Kinetics validation set. For fair evaluation	Kinetics	Kinetics-400
two major benchmarks: Sports1M and Kinetics	Kinetics	Kinetics-400
Dataset. We use Kinetics-400 [19] for the ablation experi	Kinetics-	Kinetics-400
CSNs on Sports1M [18] and Kinetics-400 [19]. Sports1M is a large-scale	Kinetics-	Kinetics-400
with 260k videos [19] of 400 human action classes. We use	400	Kinetics-400
the best audio-visual results on Kinetics in Table 2. Pre-training fails	Kinetics	Kinetics-400
Kinetics	Kinetics	Kinetics-400
late fusion multi-modal networks on Kinetics and mini-Sports using video top-1	Kinetics	Kinetics-400
a multi-modal network (RGB+Audio) on Kinetics	Kinetics	Kinetics-400
state-of-the-art accuracy on benchmarks including Kinetics, Sports1M, and AudioSet. It applies	Kinetics	Kinetics-400
datasets for our ablation experiments: Kinetics, mini-Sports, and mini- AudioSet. Kinetics	Kinetics	Kinetics-400
the optimal weights (8% for Kinetics and mini-Sports, 13% for mini-AudioSet	Kinetics	Kinetics-400
curves of these models on Kinetics (left) and mini-Sports (right). On	Kinetics	Kinetics-400
both Kinetics and mini-Sports, the audio model	Kinetics	Kinetics-400
Kinetics Learning Curve	Kinetics	Kinetics-400
of naive audio-video models on Kinetics and mini-Sports. The learning curves	Kinetics	Kinetics-400
joint audio-video (AV) model on Kinetics (left) and mini-Sports (right). Solid	Kinetics	Kinetics-400
and single best modality on Kinetics	Kinetics	Kinetics-400
on different multi-modal problems on Kinetics	Kinetics	Kinetics-400
On Kinetics, we study all combinations of	Kinetics	Kinetics-400
and benchmarks: action recog- nition (Kinetics), sport classification (mini-Sports), and acoustic	Kinetics	Kinetics-400
by significant margins on both Kinetics and mini-Sports. On mini-AudioSet, Gradient-Blend	Kinetics	Kinetics-400
failures of auxiliary loss on Kinetics and mini-Sports demonstrates that the	Kinetics	Kinetics-400
and bottom 20 classes on Kinetics where Gradient-Blend makes the most	Kinetics	Kinetics-400
Dataset Kinetics mini-Sports mini-AudioSet Method Clip V@1	Kinetics	Kinetics-400
well as single-modal networks on Kinetics, mini-Sports, and mini-AudioSet. G-Blend consistently	Kinetics	Kinetics-400
them with state-of-the-art methods on Kinetics, Sports1M, and AudioSet. Our G-Blend	Kinetics	Kinetics-400
for Sports-1M and AudioSet. For Kinetics, we follow the same 30-crop	Kinetics	Kinetics-400
with current state-of-the-art methods on Kinetics	Kinetics	Kinetics-400
and achieves state-of-the-art accuracy on Kinetics	Kinetics	Kinetics-400
competitive methods reporting results on Kinetics, due to the space limit	Kinetics	Kinetics-400
Comparison with state-of-the-art methods on Kinetics	Kinetics	Kinetics-400
or more FC layers on Kinetics	Kinetics	Kinetics-400
block 2 is unfeasible. On Kinetics, we found 3-D concat after	Kinetics	Kinetics-400
times. We found that on Kinetics, it works the best when	Kinetics	Kinetics-400
Kinetics	Kinetics	Kinetics-400
400 and Something-Something-V1, the two major	400	Kinetics-400
action classification benchmarks: Kinetics-400 [15] (400 classes) and Something-Something-V1 [11] (174	400	Kinetics-400
is the number of classes (400 for Kinetics-400 and 174 for	400	Kinetics-400
400 [15] and Something-Something- V1 [11	400	Kinetics-400
]. Kinetics-400 consists of 400 actions. Its videos are from	400	Kinetics-400
videos, well balanced across all 400 classes. The test set labels	400	Kinetics-400
400 is an excellent dataset for	400	Kinetics-400
400 and their duration typically spans	400	Kinetics-400
400, temporal information and tempo- ral	400	Kinetics-400
400 (e.g., ’building cabi- net’), they	400	Kinetics-400
400 dataset. Our baseline, which consists	400	Kinetics-400
400, on Something- Something-V1 we follow	400	Kinetics-400
400 dataset. This dataset provides a	400	Kinetics-400
of a large set of 400 classes. First, we present an	400	Kinetics-400
are defined by the Kinetics- 400 dataset and were originally generated	400	Kinetics-400
by manually clus- tering the 400 classes into 38 parent classes	400	Kinetics-400
400 (table 4). We compare against	400	Kinetics-400
400 dataset	400	Kinetics-400
400 I3D + GCN [30] (ECCV’18	400	Kinetics-400
400 Non-local I3D + GCN [30	400	Kinetics-400
400 TrajectoryNet [39] (NIPS’18) ResNet18 44.0	400	Kinetics-400
400 Our baseline (GB, sec. 3.1	400	Kinetics-400
400 dataset. While	400	Kinetics-400
we obtain state-of-the-art performance on Kinetics-400 and Something-Something-V1, the two major	Kinetics-400	Kinetics-400
large- scale action classification benchmarks: Kinetics-400 [15] (400 classes) and Something-Something-V1	Kinetics-400	Kinetics-400
number of classes (400 for Kinetics-400 and 174 for Something-Something-V1), while	Kinetics-400	Kinetics-400
largest datasets for action recognition: Kinetics-400 [15] and Something-Something- V1 [11	Kinetics-400	Kinetics-400
]. Kinetics-400 consists of 400 actions. Its	Kinetics-400	Kinetics-400
performance on the validation set. Kinetics-400 is an excellent dataset for	Kinetics-400	Kinetics-400
on average than those of Kinetics-400 and their duration typically spans	Kinetics-400	Kinetics-400
two datasets is that, on Kinetics-400, temporal information and tempo- ral	Kinetics-400	Kinetics-400
they are very specific on Kinetics-400 (e.g., ’building cabi- net’), they	Kinetics-400	Kinetics-400
of our model on the Kinetics-400 dataset. Our baseline, which consists	Kinetics-400	Kinetics-400
the standard testing protocol for Kinetics-400, on Something- Something-V1 we follow	Kinetics-400	Kinetics-400
proach on the Kinetics-400 dataset. This dataset provides a	Kinetics-400	Kinetics-400
report Top-1 and Top-5 accuracy. Kinetics-400 (table 4). We compare against	Kinetics-400	Kinetics-400
in the literature on the Kinetics-400 dataset	Kinetics-400	Kinetics-400
42] (ECCV’18) BN-Inception+ResNet18 46.4 – Kinetics-400 I3D + GCN [30] (ECCV’18	Kinetics-400	Kinetics-400
) ResNet50 43.3 75.1 Kinetics-400 Non-local I3D + GCN [30	Kinetics-400	Kinetics-400
] (ECCV’18) ResNet50 46.1 76.8 Kinetics-400 TrajectoryNet [39] (NIPS’18) ResNet18 44.0	Kinetics-400	Kinetics-400
39] (NIPS’18) ResNet18 47.8 – Kinetics-400 Our baseline (GB, sec. 3.1	Kinetics-400	Kinetics-400
3.8% by pre-training on the Kinetics-400 dataset. While	Kinetics-400	Kinetics-400
we obtain state-of-the-art performance on Kinetics	Kinetics	Kinetics-400
large- scale action classification benchmarks: Kinetics	Kinetics	Kinetics-400
number of classes (400 for Kinetics	Kinetics	Kinetics-400
largest datasets for action recognition: Kinetics	Kinetics	Kinetics-400
15] and Something-Something- V1 [11]. Kinetics	Kinetics	Kinetics-400
performance on the validation set. Kinetics	Kinetics	Kinetics-400
on average than those of Kinetics	Kinetics	Kinetics-400
two datasets is that, on Kinetics	Kinetics	Kinetics-400
they are very specific on Kinetics	Kinetics	Kinetics-400
of our model on the Kinetics	Kinetics	Kinetics-400
the standard testing protocol for Kinetics	Kinetics	Kinetics-400
proach on the Kinetics	Kinetics	Kinetics-400
These are defined by the Kinetics	Kinetics	Kinetics-400
report Top-1 and Top-5 accuracy. Kinetics	Kinetics	Kinetics-400
in the literature on the Kinetics	Kinetics	Kinetics-400
42] (ECCV’18) BN-Inception+ResNet18 46.4 – Kinetics	Kinetics	Kinetics-400
30] (ECCV’18) ResNet50 43.3 75.1 Kinetics	Kinetics	Kinetics-400
30] (ECCV’18) ResNet50 46.1 76.8 Kinetics	Kinetics	Kinetics-400
39] (NIPS’18) ResNet18 47.8 – Kinetics	Kinetics	Kinetics-400
3.8% by pre-training on the Kinetics	Kinetics	Kinetics-400
a new model and the Kinetics dataset. In IEEE Conference on	Kinetics	Kinetics-400
we obtain state-of-the-art performance on Kinetics-400 and Something-Something-V1, the two major	Kinetics-	Kinetics-400
large- scale action classification benchmarks: Kinetics-400 [15] (400 classes) and Something-Something-V1	Kinetics-	Kinetics-400
number of classes (400 for Kinetics-400 and 174 for Something-Something-V1), while	Kinetics-	Kinetics-400
largest datasets for action recognition: Kinetics-400 [15] and Something-Something- V1 [11	Kinetics-	Kinetics-400
]. Kinetics-400 consists of 400 actions. Its	Kinetics-	Kinetics-400
performance on the validation set. Kinetics-400 is an excellent dataset for	Kinetics-	Kinetics-400
on average than those of Kinetics-400 and their duration typically spans	Kinetics-	Kinetics-400
two datasets is that, on Kinetics-400, temporal information and tempo- ral	Kinetics-	Kinetics-400
they are very specific on Kinetics-400 (e.g., ’building cabi- net’), they	Kinetics-	Kinetics-400
of our model on the Kinetics-400 dataset. Our baseline, which consists	Kinetics-	Kinetics-400
the standard testing protocol for Kinetics-400, on Something- Something-V1 we follow	Kinetics-	Kinetics-400
proach on the Kinetics-400 dataset. This dataset provides a	Kinetics-	Kinetics-400
These are defined by the Kinetics- 400 dataset and were originally	Kinetics-	Kinetics-400
report Top-1 and Top-5 accuracy. Kinetics-400 (table 4). We compare against	Kinetics-	Kinetics-400
in the literature on the Kinetics-400 dataset	Kinetics-	Kinetics-400
42] (ECCV’18) BN-Inception+ResNet18 46.4 – Kinetics-400 I3D + GCN [30] (ECCV’18	Kinetics-	Kinetics-400
) ResNet50 43.3 75.1 Kinetics-400 Non-local I3D + GCN [30	Kinetics-	Kinetics-400
] (ECCV’18) ResNet50 46.1 76.8 Kinetics-400 TrajectoryNet [39] (NIPS’18) ResNet18 44.0	Kinetics-	Kinetics-400
39] (NIPS’18) ResNet18 47.8 – Kinetics-400 Our baseline (GB, sec. 3.1	Kinetics-	Kinetics-400
3.8% by pre-training on the Kinetics-400 dataset. While	Kinetics-	Kinetics-400
on public datasets such as Kinetics [13] and HMDB [14]. We	Kinetics	Kinetics-400
large video datasets such as Kinetics [13]. However, these approaches still	Kinetics	Kinetics-400
used a subset of the Kinetics dataset [13] with 100k videos	Kinetics	Kinetics-400
Kinetics	Kinetics	Kinetics-400
Kinetics and LowRes-HMDB51 datasets using 100	Kinetics	Kinetics-400
Kinetics LowRes-HMDB	Kinetics	Kinetics-400
Kinetics LowRes-HMDB	Kinetics	Kinetics-400
of iterations on our Tiny- Kinetics dataset for learning and not	Kinetics	Kinetics-400
Kinetics dataset using 10 iterations with	Kinetics	Kinetics-400
Kinetics	Kinetics	Kinetics-400
Kinetics	Kinetics	Kinetics-400
Kinetics	Kinetics	Kinetics-400
Kinetics HMDB	Kinetics	Kinetics-400
the model was pre-trained on Kinetics before training/testing with HMDB. Missing	Kinetics	Kinetics-400
Kinetics HMDB HMDB(+Kin) Run-time (ms	Kinetics	Kinetics-400
accuracies with the state-of-the-arts on Kinetics and HMDB. For this, we	Kinetics	Kinetics-400
momentum set to 0.9. For Kinetics and Tiny-Kinetics, the initial learning	Kinetics	Kinetics-400
iterations of the algorithm. For Kinetics and Tiny- Kinetics, we used	Kinetics	Kinetics-400
with 260k videos [19] of 400 human action classes. We use	400	Kinetics-400
the best audio-visual results on Kinetics in Table 2. Pre-training fails	Kinetics	Kinetics-400
Kinetics	Kinetics	Kinetics-400
late fusion multi-modal networks on Kinetics and mini-Sports using video top-1	Kinetics	Kinetics-400
a multi-modal network (RGB+Audio) on Kinetics	Kinetics	Kinetics-400
state-of-the-art accuracy on benchmarks including Kinetics, Sports1M, and AudioSet. It applies	Kinetics	Kinetics-400
datasets for our ablation experiments: Kinetics, mini-Sports, and mini- AudioSet. Kinetics	Kinetics	Kinetics-400
the optimal weights (8% for Kinetics and mini-Sports, 13% for mini-AudioSet	Kinetics	Kinetics-400
curves of these models on Kinetics (left) and mini-Sports (right). On	Kinetics	Kinetics-400
both Kinetics and mini-Sports, the audio model	Kinetics	Kinetics-400
Kinetics Learning Curve	Kinetics	Kinetics-400
of naive audio-video models on Kinetics and mini-Sports. The learning curves	Kinetics	Kinetics-400
joint audio-video (AV) model on Kinetics (left) and mini-Sports (right). Solid	Kinetics	Kinetics-400
and single best modality on Kinetics	Kinetics	Kinetics-400
on different multi-modal problems on Kinetics	Kinetics	Kinetics-400
On Kinetics, we study all combinations of	Kinetics	Kinetics-400
and benchmarks: action recog- nition (Kinetics), sport classification (mini-Sports), and acoustic	Kinetics	Kinetics-400
by significant margins on both Kinetics and mini-Sports. On mini-AudioSet, Gradient-Blend	Kinetics	Kinetics-400
failures of auxiliary loss on Kinetics and mini-Sports demonstrates that the	Kinetics	Kinetics-400
and bottom 20 classes on Kinetics where Gradient-Blend makes the most	Kinetics	Kinetics-400
Dataset Kinetics mini-Sports mini-AudioSet Method Clip V@1	Kinetics	Kinetics-400
well as single-modal networks on Kinetics, mini-Sports, and mini-AudioSet. G-Blend consistently	Kinetics	Kinetics-400
them with state-of-the-art methods on Kinetics, Sports1M, and AudioSet. Our G-Blend	Kinetics	Kinetics-400
for Sports-1M and AudioSet. For Kinetics, we follow the same 30-crop	Kinetics	Kinetics-400
with current state-of-the-art methods on Kinetics	Kinetics	Kinetics-400
and achieves state-of-the-art accuracy on Kinetics	Kinetics	Kinetics-400
competitive methods reporting results on Kinetics, due to the space limit	Kinetics	Kinetics-400
Comparison with state-of-the-art methods on Kinetics	Kinetics	Kinetics-400
or more FC layers on Kinetics	Kinetics	Kinetics-400
block 2 is unfeasible. On Kinetics, we found 3-D concat after	Kinetics	Kinetics-400
times. We found that on Kinetics, it works the best when	Kinetics	Kinetics-400
Kinetics	Kinetics	Kinetics-400
as one of 400 human action categories. Note that	400	Kinetics-400
Moments in Time [17] and Kinetics [2]. Ac	Kinetics	Kinetics-400
Kinetics. The Kinetics dataset contains 236763 training	Kinetics	Kinetics-400
Kinetics dataset contains a bit more	Kinetics	Kinetics-400
Kinetics CoST(a) 73.6 90.8 82.2	Kinetics	Kinetics-400
the Moments in Time and Kinetics	Kinetics	Kinetics-400
Kinetics 73.2 90.2 81.7	Kinetics	Kinetics-400
validation set of Kinetics	Kinetics	Kinetics-400
frames mentioned earlier. While on Kinetics, we sample 32	Kinetics	Kinetics-400
on the validation set of Kinetics	Kinetics	Kinetics-400
On the Kinetics dataset, CoST achieves state-of-the-art	Kinetics	Kinetics-400
and 0.19 respectively. While on Kinetics they are 0.77, 0.08 and	Kinetics	Kinetics-400
discriminate different actions than Kinetics	Kinetics	Kinetics-400
Moments in Time Kinetics	Kinetics	Kinetics-400
400 dataset), which has about 25k	400	Kinetics-400
400, and compares with base- lines	400	Kinetics-400
400 Nov. 2018 version. Note that	400	Kinetics-400
400	400	Kinetics-400
400 are added for context. These	400	Kinetics-400
0 200 400 600 800 1000 1200 1400	400	Kinetics-400
400	400	Kinetics-400
400 accuracy. Note that * are	400	Kinetics-400
400 new old	400	Kinetics-400
use the currently available version (Kinetics-400 dataset), which has about 25k	Kinetics-400	Kinetics-400
accuracy of our algorithm on Kinetics-400, and compares with base- lines	Kinetics-400	Kinetics-400
Table 3. Performances on Kinetics-400 Nov. 2018 version. Note that	Kinetics-400	Kinetics-400
than the initial version of Kinetics-400	Kinetics-400	Kinetics-400
V100 GPU. Accuracy numbers on Kinetics-400 are added for context. These	Kinetics-400	Kinetics-400
two different set- tings of Kinetics-400	Kinetics-400	Kinetics-400
Table 13. Kinetics-400 accuracy. Note that * are	Kinetics-400	Kinetics-400
Method Kinetics-400 new old	Kinetics-400	Kinetics-400
datasets we test, including HMDB, Kinetics, and Moments in Time. We	Kinetics	Kinetics-400
ResNet-like architectures obtained for the Kinetics dataset. Modules are repeated R	Kinetics	Kinetics-400
datasets we tested, including HMDB, Kinetics, and Moments in time. This	Kinetics	Kinetics-400
activity recognition video datasets including Kinetics	Kinetics	Kinetics-400
videos of 51 action classes. Kinetics [10] is a large challenging	Kinetics	Kinetics-400
use the currently available version (Kinetics	Kinetics	Kinetics-400
fewer training videos than original Kinetics dataset (i.e., missing about 10	Kinetics	Kinetics-400
found with shorter videos like Kinetics	Kinetics	Kinetics-400
to baselines, with and without Kinetics pre-training. The models were all	Kinetics	Kinetics-400
Kinetics	Kinetics	Kinetics-400
accuracy of our algorithm on Kinetics	Kinetics	Kinetics-400
Table 3. Performances on Kinetics	Kinetics	Kinetics-400
than the initial version of Kinetics	Kinetics	Kinetics-400
previously reported results (we use Kinetics pre-training as in [36]). As	Kinetics	Kinetics-400
evaluate the models evolved on Kinetics by training it on another	Kinetics	Kinetics-400
Method Kinetics Charades HMDB MiT	Kinetics	Kinetics-400
Evolved on Kinetics 77.2 37.8 82.3 31.8 Evolved	Kinetics	Kinetics-400
V100 GPU. Accuracy numbers on Kinetics	Kinetics	Kinetics-400
from different hybrid meta- architectures. Kinetics dataset	Kinetics	Kinetics-400
and have longest temporal duration. Kinetics dataset	Kinetics	Kinetics-400
when evolved for HMDB or Kinetics	Kinetics	Kinetics-400
12 seconds, while HMDB and Kinetics videos are on the average	Kinetics	Kinetics-400
11. Stretching iTGM kernels from Kinetics to Charades	Kinetics	Kinetics-400
Stretched (L = 11) 34.2 Kinetics EvaNet 37.7 Kinetics EvaNet Stretched	Kinetics	Kinetics-400
a model from the Kinetics dataset and ‘stretch’ the iTGM	Kinetics	Kinetics-400
with L = 3 on Kinetics and stretched to L	Kinetics	Kinetics-400
176 (for HMDB and Kinetics) or 64 × 176	Kinetics	Kinetics-400
Table 12. Kinetics performance comparison to baselines	Kinetics	Kinetics-400
two different set- tings of Kinetics	Kinetics	Kinetics-400
-400. Note that Kinetics is periodically re- moving some	Kinetics	Kinetics-400
Table 13. Kinetics	Kinetics	Kinetics-400
num- bers on the initial Kinetics dataset, which is no longer	Kinetics	Kinetics-400
numbers based on the new Kinetics version from Nov 2018. The	Kinetics	Kinetics-400
Method Kinetics	Kinetics	Kinetics-400
architectures found when searching on Kinetics using RGB inputs. We observe	Kinetics	Kinetics-400
ini- tialized with ImageNet or Kinetics weights	Kinetics	Kinetics-400
ImageNet Kinetics	Kinetics	Kinetics-400
to the architectures found on Kinetics	Kinetics	Kinetics-400
of evolved ar- chitectures per Kinetics	Kinetics	Kinetics-400
-RGB and Kinetics	Kinetics	Kinetics-400
Figure 10. Kinetics RGB Top 1 with Inception	Kinetics	Kinetics-400
Figure 11. Kinetics RGB Top 2 with Inception	Kinetics	Kinetics-400
Figure 12. Kinetics RGB Top 3 with Inception	Kinetics	Kinetics-400
Figure 13. Kinetics optical flow Top 1 with	Kinetics	Kinetics-400
Figure 14. Kinetics optical flow Top 2 with	Kinetics	Kinetics-400
Figure 15. Kinetics optical flow Top 3 with	Kinetics	Kinetics-400
Figure 19. Kinetics RGB Top 1 with ResNet	Kinetics	Kinetics-400
Figure 20. Kinetics RGB Top 2 with ResNet	Kinetics	Kinetics-400
Figure 21. Kinetics RGB Top 3 with ResNet	Kinetics	Kinetics-400
Figure 22. Kinetics optical flow Top 1 with	Kinetics	Kinetics-400
Figure 23. Kinetics optical flow Top 2 with	Kinetics	Kinetics-400
Figure 24. Kinetics optical flow Top 3 with	Kinetics	Kinetics-400
use the currently available version (Kinetics-400 dataset), which has about 25k	Kinetics-	Kinetics-400
accuracy of our algorithm on Kinetics-400, and compares with base- lines	Kinetics-	Kinetics-400
Table 3. Performances on Kinetics-400 Nov. 2018 version. Note that	Kinetics-	Kinetics-400
than the initial version of Kinetics-400	Kinetics-	Kinetics-400
V100 GPU. Accuracy numbers on Kinetics-400 are added for context. These	Kinetics-	Kinetics-400
two different set- tings of Kinetics-400	Kinetics-	Kinetics-400
Table 13. Kinetics-400 accuracy. Note that * are	Kinetics-	Kinetics-400
Method Kinetics-400 new old	Kinetics-	Kinetics-400
of evolved ar- chitectures per Kinetics-	Kinetics-	Kinetics-400
RGB and Kinetics-	Kinetics-	Kinetics-400
400 [19] for the ablation experi	400	Kinetics-400
contains about 260K videos of 400 different human action categories. We	400	Kinetics-400
400 [19]. Sports1M is a large-scale	400	Kinetics-400
Dataset. We use Kinetics-400 [19] for the ablation experi	Kinetics-400	Kinetics-400
CSNs on Sports1M [18] and Kinetics-400 [19]. Sports1M is a large-scale	Kinetics-400	Kinetics-400
yet accurate. On Sports1M and Kinetics, our CSNs are com- parable	Kinetics	Kinetics-400
art methods on Sports1M and Kinetics while being 2–3 times faster	Kinetics	Kinetics-400
Dataset. We use Kinetics	Kinetics	Kinetics-400
experi- ments in this section. Kinetics is a standard benchmark for	Kinetics	Kinetics-400
model ob- tains 69.7% on Kinetics validation (vs. 70.3% of vanilla	Kinetics	Kinetics-400
for ip-CSN-101 and ResNet3D-101 on Kinetics	Kinetics	Kinetics-400
video top-1 accuracy on the Kinetics validation set vs the model	Kinetics	Kinetics-400
Video top-1 accuracy on the Kinetics validation set against computation cost	Kinetics	Kinetics-400
CSNs on Sports1M [18] and Kinetics	Kinetics	Kinetics-400
provided with the dataset. For Kinetics, we use the train split	Kinetics	Kinetics-400
for video prediction. On Kinetics, since the 30 crops eval	Kinetics	Kinetics-400
from our ablation. Results on Kinetics	Kinetics	Kinetics-400
Accuracy is measured on the Kinetics validation set. For fair evaluation	Kinetics	Kinetics-400
two major benchmarks: Sports1M and Kinetics	Kinetics	Kinetics-400
Dataset. We use Kinetics-400 [19] for the ablation experi	Kinetics-	Kinetics-400
CSNs on Sports1M [18] and Kinetics-400 [19]. Sports1M is a large-scale	Kinetics-	Kinetics-400
Kinetics contains 300k videos spanning 400 human action classes with more	400	Kinetics-400
than 400 examples for each class. The	400	Kinetics-400
action datasets: HMDB51, UCF101, and Kinetics	Kinetics	Kinetics-400
on the HMDB51, UCF101 and Kinetics datasets. In particular, if the	Kinetics	Kinetics-400
it outperforms models pre-trained on Kinetics	Kinetics	Kinetics-400
and beating I3D pre-trained on Kinetics, thus clearly showing the impact	Kinetics	Kinetics-400
UCF101 (96.9%), HMDB51 (74.5%) and Kinetics (73.5	Kinetics	Kinetics-400
such as ActivityNet [4] and Kinetics [22]. ActivityNet contains 849 hours	Kinetics	Kinetics-400
video, including 28,000 action instances. Kinetics contains 300k videos spanning 400	Kinetics	Kinetics-400
140K ’19 Kinetics [22] - - 600	Kinetics	Kinetics-400
such as ActivityNet, Kinetics, and YouTube-8M have col- lected	Kinetics	Kinetics-400
category taxonomy diversity of Youtube8M, Kinetics	Kinetics	Kinetics-400
datasets, such as, Sports1M [21], Kinetics [22], and AVA [18] were	Kinetics	Kinetics-400
which have competitive results on Kinetics and UCF101 datasets. To measure	Kinetics	Kinetics-400
model on video datasets like Kinetics to fasten the process of	Kinetics	Kinetics-400
large scale datasets, HVU and Kinetics	Kinetics	Kinetics-400
Pre-Training Dataset UCF101 HMDB51 Kinetics From Scratch 65.2 33.4 65.6	Kinetics	Kinetics-400
Kinetics 89.8 62.1 - HVU 91.1	Kinetics	Kinetics-400
performance comparison of HVU and Kinetics datasets for transfer learning generalization	Kinetics	Kinetics-400
Dataset CNN Backbone UCF101 HMDB51 Kinetics Two Stream (spatial stream) [36	Kinetics	Kinetics-400
Kinetics Inception v3 93.2 - 72.5	Kinetics	Kinetics-400
Kinetics Inception v1 95.6 74.8 72.1	Kinetics	Kinetics-400
RGB-I3D [5] Kinetics Inception v1 95.6 74.8 71.6	Kinetics	Kinetics-400
ResNet 101 (16 frames) [19] Kinetics ResNet101 88.9 61.7 62.8 3D	Kinetics	Kinetics-400
ResNext 101 (16 frames) [19] Kinetics ResNext101 90.7 63.8 65.1 STC-ResNext	Kinetics	Kinetics-400
101 (16 frames) [7] Kinetics ResNext101 92.3 65.4 66.2 STC-ResNext	Kinetics	Kinetics-400
101 (64 frames) [7] Kinetics ResNext101 96.5 74.9 68.7 C3D	Kinetics	Kinetics-400
45] Kinetics ResNet18 89.8 62.1 65.6 ARTNet	Kinetics	Kinetics-400
45] Kinetics ResNet18 93.5 67.6 69.2 R(2+1)D	Kinetics	Kinetics-400
42] Kinetics ResNet50 96.8 74.5 72 SlowFast	Kinetics	Kinetics-400
11] Kinetics ResNet50 - - 75.6 HATNet	Kinetics	Kinetics-400
16 frames) Kinetics ResNet18 94.1 69.2 70.4 3D-ResNet18	Kinetics	Kinetics-400
UCF101, HMDB51 test sets and Kinetics validation set. The results on	Kinetics	Kinetics-400
over three splits, and for Kinetics is Top-1 mAP on validation	Kinetics	Kinetics-400
5.4. Transfer Learning: HVU vs Kinetics	Kinetics	Kinetics-400
3D-ResNet18 using Kinetics versus using HVU and then	Kinetics	Kinetics-400
fine-tuning on UCF101, HMDB51 and Kinetics	Kinetics	Kinetics-400
on smaller datasets (i.e. HVU, Kinetics	Kinetics	Kinetics-400
can improve the results on Kinetics also, although it is marginal	Kinetics	Kinetics-400
5.5. Comparison on UCF, HMDB, Kinetics	Kinetics	Kinetics-400
with HVU and another with Kinetics, and then fine-tune on the	Kinetics	Kinetics-400
pre-trained models on ImageNet and Kinetics	Kinetics	Kinetics-400
datasets. Note that for Kinetics dataset, HATNet even with ResNet18	Kinetics	Kinetics-400
Transfer Learning: HVU vs Kinetics	Kinetics	Kinetics-400
Comparison on UCF, HMDB, Kinetics	Kinetics	Kinetics-400
category taxonomy diversity of Youtube8M, Kinetics- 600 and HACS, we use	Kinetics-	Kinetics-400
10 seconds. The dataset covers 400 human-centric classes and each class	400	Kinetics-400
has at least 400 video clips. For unknown reasons	400	Kinetics-400
train the flow branch on Kinetics data [14]. After that, considering	Kinetics	Kinetics-400
Kinetics [14] is a large-scale trimmed	Kinetics	Kinetics-400
at each iteration on the Kinetics dataset	Kinetics	Kinetics-400
Comparisons with non-local networks on Kinetics	Kinetics	Kinetics-400
P3D and 3D-CMA models on Kinetics when varying the count of	Kinetics	Kinetics-400
We train CMA iter1-R on Kinetics and visualize the attention maps	Kinetics	Kinetics-400
These samples are taken from Kinetics randomly. Each set contains three	Kinetics	Kinetics-400
Sports1M or ImageNet). Kinetics has 400 action classes and about 240K	400	Kinetics-400
0 200 400 600 800 1000 GFLOPS	400	Kinetics-400
pop- ular datasets, such as Kinetics, UCF-101 and HMDB-51	Kinetics	Kinetics-400
state-of-the-art accuracy of 77.7% on Kinetics with FLOPs as high as	Kinetics	Kinetics-400
Accuracy of clip-level backbones on Kinetics	Kinetics	Kinetics-400
Datasets. We choose the Kinetics [20] dataset as the major	Kinetics	Kinetics-400
testbed for FASTER. Kinetics is among the most popular	Kinetics	Kinetics-400
simplify, all reported results on Kinetics are trained from scratch, without	Kinetics	Kinetics-400
datasets (e.g., Sports1M or ImageNet). Kinetics has 400 action classes and	Kinetics	Kinetics-400
much smaller, thus we use Kinetics for pre- training and report	Kinetics	Kinetics-400
As the average length of Kinetics videos is about 10 seconds	Kinetics	Kinetics-400
architectures for ag- gregation on Kinetics	Kinetics	Kinetics-400
in differ- ent settings on Kinetics	Kinetics	Kinetics-400
state of the art on Kinetics, UCF-101, and HMDB-51	Kinetics	Kinetics-400
the two clip-level backbones on Kinetics	Kinetics	Kinetics-400
measure accuracy and GFLOPs on Kinetics	Kinetics	Kinetics-400
up the runtime over 100 Kinetics videos. The results are listed	Kinetics	Kinetics-400
to the state-of-the-art methods on Kinetics	Kinetics	Kinetics-400
K” denotes pre- training on Kinetics	Kinetics	Kinetics-400
most popular video datasets, i.e., Kinetics, UCF-101 and HMDB-51. We only	Kinetics	Kinetics-400
frames as inputs. Results on Kinetics are shown in Table 7	Kinetics	Kinetics-400
GFLOPs vs. accuracy comparisons on Kinetics	Kinetics	Kinetics-400
SomethingSomethingv1 [10]. Kinetics400 consists of 400	400	Kinetics-400
72.7% accuracy on Kinetics compared to 72.0% and 65.6	Kinetics	Kinetics-400
tains 72.7% accuracy on Kinetics compared to 72.0	Kinetics	Kinetics-400
troduced the Kinetics dataset [18], which was large	Kinetics	Kinetics-400
streams pretrained on Kinetics, I3D [2] achieved the state	Kinetics	Kinetics-400
tion: Kinetics400 [18], HMDB51 [20], UCF101 [33	Kinetics	Kinetics-400
SomethingSomethingv1 [10]. Kinetics400 consists of 400	Kinetics	Kinetics-400
Training MARS on Kinetics400	Kinetics	Kinetics-400
accuracies while training MARS on Kinetics400 from scratch for α	Kinetics	Kinetics-400
curacy. For Kinetics400 and SomethingSomethingv1, we	Kinetics	Kinetics-400
architecture due its performance on Kinetics400	Kinetics	Kinetics-400
train on Kinetics400 and MiniKinetics from scratch. For	Kinetics	Kinetics-400
Stream MiniKinetics Kinetics400 UCF101-1 HMDB51-1 Something	Kinetics	Kinetics-400
using 16f-clips. For MiniKinetics and Kinetics400, all the streams are trained	Kinetics	Kinetics-400
the streams are finetuned from Kinetics400 pretrained models	Kinetics	Kinetics-400
Stream Kinetics400 UCF101-1 HMDB51-1 Something	Kinetics	Kinetics-400
Top-1 accuracy using 64f-clips. For Kinetics400, all the streams are trained	Kinetics	Kinetics-400
are finetuned from Kinetics400 pretrained models. Optical	Kinetics	Kinetics-400
in accuracy over RGB on Kinetics400, UCF101- 1, HMDB51-1, and SomethingSomethingv1	Kinetics	Kinetics-400
ever, on Kinetics400, MARS performs worse than the	Kinetics	Kinetics-400
Flow for 6 classes of Kinetics400 dataset	Kinetics	Kinetics-400
al. [22] also find that Kinetics400 is bi	Kinetics	Kinetics-400
pretrained on Kinetics400 does not generalize well enough	Kinetics	Kinetics-400
approaches in Table 3 for Kinetics400 and in Ta	Kinetics	Kinetics-400
Flow. For Kinetics400, when using only RGB frames	Kinetics	Kinetics-400
and Flow as inputs. On Kinetics400	Kinetics	Kinetics-400
results for Kinetics400 validation set. (*Calculated on the	Kinetics	Kinetics-400
held-out test set of Kinetics400	Kinetics	Kinetics-400
ResNext101 [11] RGB Kinetics 94.5 70.1	Kinetics	Kinetics-400
R(2+1)D [38] RGB Kinetics 96.8 74.5	Kinetics	Kinetics-400
MARS RGB Kinetics 97.4 79.3 48.7	Kinetics	Kinetics-400
MARS+RGB RGB Kinetics 97.6 79.5 51.7	Kinetics	Kinetics-400
R(2+1)D [38] RGB+Flow Kinetics 97.3 78.7	Kinetics	Kinetics-400
Flow RGB+Flow Kinetics 98.1 80.9 53.0	Kinetics	Kinetics-400
and scene-related datasets (i.e., Kinetics- 400, UCF-101, and HMDB-51) with the	400	Kinetics-400
400, UCF-101, and HMDB-51). The base	400	Kinetics-400
2) scene-related datasets, including Kinetics- 400 [2], UCF-101 [23] and HMDB-51	400	Kinetics-400
400 dataset com- pared with the	400	Kinetics-400
400, UCF-101, and HMDB-51 in this	400	Kinetics-400
400 is a large-scale human action	400	Kinetics-400
video dataset with 400 classes. It contains 236,763 clips	400	Kinetics-400
400 dataset. We train STM with	400	Kinetics-400
Jester) and scene-related datasets (i.e., Kinetics-400, UCF-101, and HMDB-51). The base	Kinetics-400	Kinetics-400
of the STM on the Kinetics-400 dataset com- pared with the	Kinetics-400	Kinetics-400
STM on three scene-related datasets: Kinetics-400, UCF-101, and HMDB-51 in this	Kinetics-400	Kinetics-400
section. Kinetics-400 is a large-scale human action	Kinetics-400	Kinetics-400
com- peting methods on the Kinetics-400 dataset. We train STM with	Kinetics-400	Kinetics-400
Jester) and scene-related datasets (i.e., Kinetics	Kinetics	Kinetics-400
large-scale video datasets such as Kinetics [2]. With the help of	Kinetics	Kinetics-400
public benchmark datasets including Something-Something[11], Kinetics [2], Jester [1], UCF101 [23	Kinetics	Kinetics-400
the help of high-quality large-scale Kinetics dataset and the	Kinetics	Kinetics-400
Jester) and scene-related datasets (i.e., Kinetics	Kinetics	Kinetics-400
lationship; (2) scene-related datasets, including Kinetics	Kinetics	Kinetics-400
when T = 16). For Kinetics, Something- Something v1 & v2	Kinetics	Kinetics-400
Kinetics	Kinetics	Kinetics-400
I3D [2] 3D ResNet-50 Kinetics 32 41.6 72.2	Kinetics	Kinetics-400
TSN [33] ResNet-50 Kinetics 8 19.7 46.6 - 27.8	Kinetics	Kinetics-400
TSM [19] ResNet-50 Kinetics 16 44.8 74.5 - 58.7	Kinetics	Kinetics-400
UCF-101 and HMDB-51, we use Kinetics pre-trained model as initialization and	Kinetics	Kinetics-400
of the STM on the Kinetics	Kinetics	Kinetics-400
82.3 51.6 STC [4] ResNet101 Kinetics 93.7 66.8	Kinetics	Kinetics-400
with TSN [32] 3D ResNet-18 Kinetics 94.3 70.9 ECO [42] BNInception+3D	Kinetics	Kinetics-400
ResNet-18 Kinetics 94.8 72.4	Kinetics	Kinetics-400
Kinetics 95.1 74.3I3D two-stream [2] X	Kinetics	Kinetics-400
Kinetics 91.1 -TSN two-Stream [33] X	Kinetics	Kinetics-400
Kinetics 94.5 70.7 StNet [12] ResNet50	Kinetics	Kinetics-400
Kinetics 93.5	Kinetics	Kinetics-400
Kinetics 95.9 - STM ResNet-50 ImageNet+Kinetics	Kinetics	Kinetics-400
STM on three scene-related datasets: Kinetics	Kinetics	Kinetics-400
and HMDB-51 in this section. Kinetics	Kinetics	Kinetics-400
com- peting methods on the Kinetics	Kinetics	Kinetics-400
temporal-related datasets, most actions of Kinetics can be recognized by scene	Kinetics	Kinetics-400
with the ImageNet pre-trained model, Kinetics pre-train can significantly improve the	Kinetics	Kinetics-400
on UCF101, which also uses Kinetics as pre-train data but the	Kinetics	Kinetics-400
Jester) and scene-related datasets (i.e., Kinetics- 400, UCF-101, and HMDB-51) with	Kinetics-	Kinetics-400
Jester) and scene-related datasets (i.e., Kinetics-400, UCF-101, and HMDB-51). The base	Kinetics-	Kinetics-400
lationship; (2) scene-related datasets, including Kinetics- 400 [2], UCF-101 [23] and	Kinetics-	Kinetics-400
of the STM on the Kinetics-400 dataset com- pared with the	Kinetics-	Kinetics-400
STM on three scene-related datasets: Kinetics-400, UCF-101, and HMDB-51 in this	Kinetics-	Kinetics-400
section. Kinetics-400 is a large-scale human action	Kinetics-	Kinetics-400
com- peting methods on the Kinetics-400 dataset. We train STM with	Kinetics-	Kinetics-400
Sports1M or ImageNet). Kinetics has 400 action classes and about 240K	400	Kinetics-400
0 200 400 600 800 1000 GFLOPS	400	Kinetics-400
pop- ular datasets, such as Kinetics, UCF-101 and HMDB-51	Kinetics	Kinetics-400
state-of-the-art accuracy of 77.7% on Kinetics with FLOPs as high as	Kinetics	Kinetics-400
Accuracy of clip-level backbones on Kinetics	Kinetics	Kinetics-400
Datasets. We choose the Kinetics [20] dataset as the major	Kinetics	Kinetics-400
testbed for FASTER. Kinetics is among the most popular	Kinetics	Kinetics-400
simplify, all reported results on Kinetics are trained from scratch, without	Kinetics	Kinetics-400
datasets (e.g., Sports1M or ImageNet). Kinetics has 400 action classes and	Kinetics	Kinetics-400
much smaller, thus we use Kinetics for pre- training and report	Kinetics	Kinetics-400
As the average length of Kinetics videos is about 10 seconds	Kinetics	Kinetics-400
architectures for ag- gregation on Kinetics	Kinetics	Kinetics-400
in differ- ent settings on Kinetics	Kinetics	Kinetics-400
state of the art on Kinetics, UCF-101, and HMDB-51	Kinetics	Kinetics-400
the two clip-level backbones on Kinetics	Kinetics	Kinetics-400
measure accuracy and GFLOPs on Kinetics	Kinetics	Kinetics-400
up the runtime over 100 Kinetics videos. The results are listed	Kinetics	Kinetics-400
to the state-of-the-art methods on Kinetics	Kinetics	Kinetics-400
K” denotes pre- training on Kinetics	Kinetics	Kinetics-400
most popular video datasets, i.e., Kinetics, UCF-101 and HMDB-51. We only	Kinetics	Kinetics-400
frames as inputs. Results on Kinetics are shown in Table 7	Kinetics	Kinetics-400
GFLOPs vs. accuracy comparisons on Kinetics	Kinetics	Kinetics-400
SomethingSomethingv1 [10]. Kinetics400 consists of 400	400	Kinetics-400
72.7% accuracy on Kinetics compared to 72.0% and 65.6	Kinetics	Kinetics-400
tains 72.7% accuracy on Kinetics compared to 72.0	Kinetics	Kinetics-400
troduced the Kinetics dataset [18], which was large	Kinetics	Kinetics-400
streams pretrained on Kinetics, I3D [2] achieved the state	Kinetics	Kinetics-400
tion: Kinetics400 [18], HMDB51 [20], UCF101 [33	Kinetics	Kinetics-400
SomethingSomethingv1 [10]. Kinetics400 consists of 400	Kinetics	Kinetics-400
Training MARS on Kinetics400	Kinetics	Kinetics-400
accuracies while training MARS on Kinetics400 from scratch for α	Kinetics	Kinetics-400
curacy. For Kinetics400 and SomethingSomethingv1, we	Kinetics	Kinetics-400
architecture due its performance on Kinetics400	Kinetics	Kinetics-400
train on Kinetics400 and MiniKinetics from scratch. For	Kinetics	Kinetics-400
Stream MiniKinetics Kinetics400 UCF101-1 HMDB51-1 Something	Kinetics	Kinetics-400
using 16f-clips. For MiniKinetics and Kinetics400, all the streams are trained	Kinetics	Kinetics-400
the streams are finetuned from Kinetics400 pretrained models	Kinetics	Kinetics-400
Stream Kinetics400 UCF101-1 HMDB51-1 Something	Kinetics	Kinetics-400
Top-1 accuracy using 64f-clips. For Kinetics400, all the streams are trained	Kinetics	Kinetics-400
are finetuned from Kinetics400 pretrained models. Optical	Kinetics	Kinetics-400
in accuracy over RGB on Kinetics400, UCF101- 1, HMDB51-1, and SomethingSomethingv1	Kinetics	Kinetics-400
ever, on Kinetics400, MARS performs worse than the	Kinetics	Kinetics-400
Flow for 6 classes of Kinetics400 dataset	Kinetics	Kinetics-400
al. [22] also find that Kinetics400 is bi	Kinetics	Kinetics-400
pretrained on Kinetics400 does not generalize well enough	Kinetics	Kinetics-400
approaches in Table 3 for Kinetics400 and in Ta	Kinetics	Kinetics-400
Flow. For Kinetics400, when using only RGB frames	Kinetics	Kinetics-400
and Flow as inputs. On Kinetics400	Kinetics	Kinetics-400
results for Kinetics400 validation set. (*Calculated on the	Kinetics	Kinetics-400
held-out test set of Kinetics400	Kinetics	Kinetics-400
ResNext101 [11] RGB Kinetics 94.5 70.1	Kinetics	Kinetics-400
R(2+1)D [38] RGB Kinetics 96.8 74.5	Kinetics	Kinetics-400
MARS RGB Kinetics 97.4 79.3 48.7	Kinetics	Kinetics-400
MARS+RGB RGB Kinetics 97.6 79.5 51.7	Kinetics	Kinetics-400
R(2+1)D [38] RGB+Flow Kinetics 97.3 78.7	Kinetics	Kinetics-400
Flow RGB+Flow Kinetics 98.1 80.9 53.0	Kinetics	Kinetics-400
on the JHMDB, HMDB and UCF101	UCF	UCF101
mance on JHMDB, HMDB, UCF101	UCF	UCF101
state of the art on UCF101	UCF	UCF101
The UCF101 dataset [34] consists of around	UCF	UCF101
HMDB, JHMDB and UCF101 have 3 train/test splits	UCF	UCF101
method streams GT-JHMDB-1 JHMDB-1 HMDB-1 UCF101	UCF	UCF101
JHMDB, HMDB and UCF101 datasets. We observe a clear	UCF	UCF101
on HMDB and +4% on UCF101	UCF	UCF101
by more than 10%. On UCF101, we also report state-of-the- art	UCF	UCF101
Method JHMDB HMDB UCF101	UCF	UCF101
accuracy on JHMDB, HMDB and UCF101 averaged over the 3	UCF	UCF101
UCF101 datasets. Future work includes experimenting	UCF	UCF101
R. Zamir, and M. Shah. UCF101	UCF	UCF101
on the JHMDB, HMDB and UCF101	UCF101	UCF101
mance on JHMDB, HMDB, UCF101	UCF101	UCF101
state of the art on UCF101	UCF101	UCF101
The UCF101 dataset [34] consists of around	UCF101	UCF101
HMDB, JHMDB and UCF101 have 3 train/test splits	UCF101	UCF101
method streams GT-JHMDB-1 JHMDB-1 HMDB-1 UCF101	UCF101	UCF101
JHMDB, HMDB and UCF101 datasets. We observe a clear	UCF101	UCF101
on HMDB and +4% on UCF101	UCF101	UCF101
by more than 10%. On UCF101, we also report state-of-the- art	UCF101	UCF101
Method JHMDB HMDB UCF101	UCF101	UCF101
accuracy on JHMDB, HMDB and UCF101 averaged over the 3	UCF101	UCF101
UCF101 datasets. Future work includes experimenting	UCF101	UCF101
R. Zamir, and M. Shah. UCF101	UCF101	UCF101
in current action classification datasets (UCF	UCF	UCF101
on HMDB-51 and 98.0% on UCF	UCF	UCF101
previous datasets, HMDB-51 [18] and UCF	UCF	UCF101
fine-tuning each on HMDB-51 and UCF	UCF	UCF101
up to 5k steps on UCF	UCF	UCF101
training and testing on either UCF	UCF	UCF101
split 1 test sets of UCF	UCF	UCF101
UCF	UCF	UCF101
testing on split 1 of UCF	UCF	UCF101
of pa- rameters and that UCF	UCF	UCF101
on Ki- netics than on UCF	UCF	UCF101
than that of RGB on UCF	UCF	UCF101
un- seen) videos of the UCF	UCF	UCF101
classifiers for the classes of UCF	UCF	UCF101
each net- work for the UCF	UCF	UCF101
-101/HMDB-51 classes (using the UCF	UCF	UCF101
and again evaluate on the UCF	UCF	UCF101
performance than directly training on UCF	UCF	UCF101
methods in table 5, on UCF	UCF	UCF101
streams, and gets 94.6% on UCF	UCF	UCF101
UCF	UCF	UCF101
Table 4. Performance on the UCF	UCF	UCF101
pretrained weights. Original: train on UCF	UCF	UCF101
the last layer trained on UCF	UCF	UCF101
pre-training with end-to-end fine-tuning on UCF	UCF	UCF101
Model UCF	UCF	UCF101
Comparison with state-of-the-art on the UCF	UCF	UCF101
overall performance to 98.0 on UCF	UCF	UCF101
dataset (Kinetics) to another dataset (UCF	UCF	UCF101
R. Zamir, and M. Shah. UCF101	UCF	UCF101
R. Zamir, and M. Shah. UCF101	UCF101	UCF101
on the HMDB51 [20] and UCF101 [33] datasets. Meth	UCF	UCF101
tion: Kinetics400 [18], HMDB51 [20], UCF101 [33], and	UCF	UCF101
splits. UCF101 contains 101 actions classes with	UCF	UCF101
first split for HMDB51 and UCF101 as HMDB51-1 and	UCF	UCF101
UCF101	UCF	UCF101
UCF101, and HMDB51 [11]. Following the	UCF	UCF101
the smaller HMDB51 and UCF101 datasets	UCF	UCF101
Stream MiniKinetics Kinetics400 UCF101	UCF	UCF101
are trained from scratch. For UCF101	UCF	UCF101
Stream Kinetics400 UCF101	UCF	UCF101
are trained from scratch. For UCF101	UCF	UCF101
accuracy over RGB on Kinetics400, UCF101	UCF	UCF101
Flow streams alone on MiniKinetics, UCF101, HMDB51	UCF	UCF101
ble 4 for UCF101, HMDB51 and SomethingSomethingv1	UCF	UCF101
Method Streams Pretrain UCF101 HMDB51 Something	UCF	UCF101
sults of UCF101 and HMDB51 are averaged over	UCF	UCF101
ics400, UCF101, HMDB51, and SomethingSomethingv1	UCF	UCF101
UCF101	UCF	UCF101
on the HMDB51 [20] and UCF101 [33] datasets. Meth	UCF101	UCF101
tion: Kinetics400 [18], HMDB51 [20], UCF101 [33], and	UCF101	UCF101
splits. UCF101 contains 101 actions classes with	UCF101	UCF101
first split for HMDB51 and UCF101 as HMDB51-1 and	UCF101	UCF101
UCF101	UCF101	UCF101
UCF101, and HMDB51 [11]. Following the	UCF101	UCF101
the smaller HMDB51 and UCF101 datasets	UCF101	UCF101
Stream MiniKinetics Kinetics400 UCF101	UCF101	UCF101
are trained from scratch. For UCF101	UCF101	UCF101
Stream Kinetics400 UCF101	UCF101	UCF101
are trained from scratch. For UCF101	UCF101	UCF101
accuracy over RGB on Kinetics400, UCF101	UCF101	UCF101
Flow streams alone on MiniKinetics, UCF101, HMDB51	UCF101	UCF101
ble 4 for UCF101, HMDB51 and SomethingSomethingv1	UCF101	UCF101
Method Streams Pretrain UCF101 HMDB51 Something	UCF101	UCF101
sults of UCF101 and HMDB51 are averaged over	UCF101	UCF101
ics400, UCF101, HMDB51, and SomethingSomethingv1	UCF101	UCF101
UCF101	UCF101	UCF101
challenging human action datasets: HMDB51, UCF101, and Kinetics. The dataset and	UCF	UCF101
state-of-the-art results on the HMDB51, UCF101 and Kinetics datasets. In particular	UCF	UCF101
HATNet achieves remarkable performance on UCF101 (96.9%), HMDB51 (74.5%) and Kinetics	UCF	UCF101
available. The HMDB51 [24] and UCF101 [37] datasets are currently the	UCF	UCF101
7K ’11 UCF101 [37] - - 101	UCF	UCF101
KTH [25], HMDB51 [24], and UCF101 [37] that inspired the design	UCF	UCF101
competitive results on Kinetics and UCF101 datasets. To measure the performance	UCF	UCF101
Pre-Training Dataset UCF101 HMDB51 Kinetics From Scratch 65.2	UCF	UCF101
Method Pre-Trained Dataset CNN Backbone UCF101 HMDB51 Kinetics Two Stream (spatial	UCF	UCF101
7. State-of-the-art performance comparison on UCF101, HMDB51 test sets and Kinetics	UCF	UCF101
validation set. The results on UCF101 and HMDB51 are average mAP	UCF	UCF101
HVU and then fine-tuning on UCF101, HMDB51 and Kinetics. Obvi- ously	UCF	UCF101
UCF101 and HMDB51). As it can	UCF	UCF101
5.5. Comparison on UCF, HMDB, Kinetics	UCF	UCF101
with the state-of-the-art methods on UCF101, HMDB51 and Ki- netics. For	UCF	UCF101
on the target datasets. For UCF101 and HMDB51, we report the	UCF	UCF101
5.5 . Comparison on UCF, HMDB, Kinetics	UCF	UCF101
challenging human action datasets: HMDB51, UCF101, and Kinetics. The dataset and	UCF101	UCF101
state-of-the-art results on the HMDB51, UCF101 and Kinetics datasets. In particular	UCF101	UCF101
HATNet achieves remarkable performance on UCF101 (96.9%), HMDB51 (74.5%) and Kinetics	UCF101	UCF101
available. The HMDB51 [24] and UCF101 [37] datasets are currently the	UCF101	UCF101
7K ’11 UCF101 [37] - - 101	UCF101	UCF101
KTH [25], HMDB51 [24], and UCF101 [37] that inspired the design	UCF101	UCF101
competitive results on Kinetics and UCF101 datasets. To measure the performance	UCF101	UCF101
Pre-Training Dataset UCF101 HMDB51 Kinetics From Scratch 65.2	UCF101	UCF101
Method Pre-Trained Dataset CNN Backbone UCF101 HMDB51 Kinetics Two Stream (spatial	UCF101	UCF101
7. State-of-the-art performance comparison on UCF101, HMDB51 test sets and Kinetics	UCF101	UCF101
validation set. The results on UCF101 and HMDB51 are average mAP	UCF101	UCF101
HVU and then fine-tuning on UCF101, HMDB51 and Kinetics. Obvi- ously	UCF101	UCF101
UCF101 and HMDB51). As it can	UCF101	UCF101
with the state-of-the-art methods on UCF101, HMDB51 and Ki- netics. For	UCF101	UCF101
on the target datasets. For UCF101 and HMDB51, we report the	UCF101	UCF101
three challenging action datasets: namely, UCF	UCF	UCF101
the overall per- formance. The UCF	UCF	UCF101
split 1 test set of UCF	UCF	UCF101
results in Table 1 on UCF	UCF	UCF101
we improve by 9.7% on UCF	UCF	UCF101
overall performance to 97.4% on UCF	UCF	UCF101
OF STATE-OF-THE-ART METHODS ON THE UCF	UCF	UCF101
Methods Pre-train dataset UCF	UCF	UCF101
increase by 0.4%, on the UCF101 dataset, and 7.8% on the	UCF	UCF101
WITH DIFFERENT COMPONENTS ON THE UCF	UCF	UCF101
Base Model Methods UCF	UCF	UCF101
We conduct an experiment on UCF	UCF	UCF101
and our CCS network on UCF	UCF	UCF101
DIFFERENT MODEL PARAMETERS VALUES ON UCF	UCF	UCF101
select three classes from the UCF	UCF	UCF101
increase by 0.4%, on the UCF101 dataset, and 7.8% on the	UCF101	UCF101
studies on two popular datasets, UCF101 [30] and Kinet- ics [14	UCF	UCF101
UCF	UCF	UCF101
experiments from Ki- netics to UCF	UCF	UCF101
Comparison with state-of-the-art on the UCF	UCF	UCF101
R. Zamir, and M. Shah. UCF101	UCF	UCF101
studies on two popular datasets, UCF101 [30] and Kinet- ics [14	UCF101	UCF101
R. Zamir, and M. Shah. UCF101	UCF101	UCF101
scene-related datasets (i.e., Kinetics- 400, UCF	UCF	UCF101
Something-Something[11], Kinetics [2], Jester [1], UCF101 [23] and HMDB-51 [17	UCF	UCF101
and scene-related datasets (i.e., Kinetics-400, UCF	UCF	UCF101
datasets, including Kinetics- 400 [2], UCF	UCF	UCF101
CMM are randomly initialized. For UCF	UCF	UCF101
Performance of the STM on UCF	UCF	UCF101
Method Backbone Flow Pre-train Data UCF	UCF	UCF101
on three scene-related datasets: Kinetics-400, UCF	UCF	UCF101
and 19,095 clips for validation. UCF	UCF	UCF101
6766 labeled video clips. For UCF	UCF	UCF101
also conduct experiments on the UCF	UCF	UCF101
I3D with RGB stream on UCF101, which also uses Kinetics as	UCF	UCF101
Something-Something[11], Kinetics [2], Jester [1], UCF101 [23] and HMDB-51 [17	UCF101	UCF101
I3D with RGB stream on UCF101, which also uses Kinetics as	UCF101	UCF101
on the HMDB51 [20] and UCF101 [33] datasets. Meth	UCF	UCF101
tion: Kinetics400 [18], HMDB51 [20], UCF101 [33], and	UCF	UCF101
splits. UCF101 contains 101 actions classes with	UCF	UCF101
first split for HMDB51 and UCF101 as HMDB51-1 and	UCF	UCF101
UCF101	UCF	UCF101
UCF101, and HMDB51 [11]. Following the	UCF	UCF101
the smaller HMDB51 and UCF101 datasets	UCF	UCF101
Stream MiniKinetics Kinetics400 UCF101	UCF	UCF101
are trained from scratch. For UCF101	UCF	UCF101
Stream Kinetics400 UCF101	UCF	UCF101
are trained from scratch. For UCF101	UCF	UCF101
accuracy over RGB on Kinetics400, UCF101	UCF	UCF101
Flow streams alone on MiniKinetics, UCF101, HMDB51	UCF	UCF101
ble 4 for UCF101, HMDB51 and SomethingSomethingv1	UCF	UCF101
Method Streams Pretrain UCF101 HMDB51 Something	UCF	UCF101
sults of UCF101 and HMDB51 are averaged over	UCF	UCF101
ics400, UCF101, HMDB51, and SomethingSomethingv1	UCF	UCF101
UCF101	UCF	UCF101
on the HMDB51 [20] and UCF101 [33] datasets. Meth	UCF101	UCF101
tion: Kinetics400 [18], HMDB51 [20], UCF101 [33], and	UCF101	UCF101
splits. UCF101 contains 101 actions classes with	UCF101	UCF101
first split for HMDB51 and UCF101 as HMDB51-1 and	UCF101	UCF101
UCF101	UCF101	UCF101
UCF101, and HMDB51 [11]. Following the	UCF101	UCF101
the smaller HMDB51 and UCF101 datasets	UCF101	UCF101
Stream MiniKinetics Kinetics400 UCF101	UCF101	UCF101
are trained from scratch. For UCF101	UCF101	UCF101
Stream Kinetics400 UCF101	UCF101	UCF101
are trained from scratch. For UCF101	UCF101	UCF101
accuracy over RGB on Kinetics400, UCF101	UCF101	UCF101
Flow streams alone on MiniKinetics, UCF101, HMDB51	UCF101	UCF101
ble 4 for UCF101, HMDB51 and SomethingSomethingv1	UCF101	UCF101
Method Streams Pretrain UCF101 HMDB51 Something	UCF101	UCF101
sults of UCF101 and HMDB51 are averaged over	UCF101	UCF101
ics400, UCF101, HMDB51, and SomethingSomethingv1	UCF101	UCF101
UCF101	UCF101	UCF101
than 1% across major datasets (UCF101 [41], HMDB51 [22] and 20BN-Something-Something	UCF	UCF101
choices with RGB/flow stream on UCF101	UCF	UCF101
recognition datasets for this experiment: UCF101 and HMDB51. UCF101 [41] contains	UCF	UCF101
RGB and flow stream on UCF101 and HMDB51 datasets. We also	UCF	UCF101
the RGB stream. For both UCF101 and HMDB51, the best performing	UCF	UCF101
results of action recognition on UCF101 [41], HMDB51 [22] and a	UCF	UCF101
large scale dataset–20BN-V2 [28]. For UCF101 and HMDB51, we report mean	UCF	UCF101
results with previous methods on UCF101	UCF	UCF101
class accuracy of 95.7%/72.0% on UCF101	UCF	UCF101
Results of action recognition on UCF101 and HMDB51. We compare the	UCF	UCF101
model is -1.1% worse on UCF101 and -4.1% on HMDB51. This	UCF	UCF101
of results remains consistent as UCF101	UCF	UCF101
is even larger (+1.4% on UCF101 and +1.3% on HMDB51). This	UCF	UCF101
localization dataset [18]–a subset of UCF101 with bounding box annotations for	UCF	UCF101
UCF101 I3D RGB 94.8 94.7 0.1	UCF	UCF101
for all testing videos of UCF101 and HMDB51. We compare their	UCF	UCF101
an accuracy of 95.1%/71.6% on UCF101	UCF	UCF101
model by - 0.6%/-0.4% on UCF101	UCF	UCF101
Roshan Zamir, and M. Shah. UCF101	UCF	UCF101
than 1% across major datasets (UCF101 [41], HMDB51 [22] and 20BN-Something-Something	UCF101	UCF101
choices with RGB/flow stream on UCF101	UCF101	UCF101
recognition datasets for this experiment: UCF101 and HMDB51. UCF101 [41] contains	UCF101	UCF101
RGB and flow stream on UCF101 and HMDB51 datasets. We also	UCF101	UCF101
the RGB stream. For both UCF101 and HMDB51, the best performing	UCF101	UCF101
results of action recognition on UCF101 [41], HMDB51 [22] and a	UCF101	UCF101
large scale dataset–20BN-V2 [28]. For UCF101 and HMDB51, we report mean	UCF101	UCF101
results with previous methods on UCF101	UCF101	UCF101
class accuracy of 95.7%/72.0% on UCF101	UCF101	UCF101
Results of action recognition on UCF101 and HMDB51. We compare the	UCF101	UCF101
model is -1.1% worse on UCF101 and -4.1% on HMDB51. This	UCF101	UCF101
of results remains consistent as UCF101	UCF101	UCF101
is even larger (+1.4% on UCF101 and +1.3% on HMDB51). This	UCF101	UCF101
localization dataset [18]–a subset of UCF101 with bounding box annotations for	UCF101	UCF101
UCF101 I3D RGB 94.8 94.7 0.1	UCF101	UCF101
for all testing videos of UCF101 and HMDB51. We compare their	UCF101	UCF101
an accuracy of 95.1%/71.6% on UCF101	UCF101	UCF101
model by - 0.6%/-0.4% on UCF101	UCF101	UCF101
Roshan Zamir, and M. Shah. UCF101	UCF101	UCF101
finally achieve leading performance on UCF	UCF	UCF101
accuracy in UCF	UCF	UCF101
leading performance in UCF	UCF	UCF101
our paper is evaluated on UCF	UCF	UCF101
our experiments, 9537 videos of UCF	UCF	UCF101
and achieve 94.3% accuracy on UCF	UCF	UCF101
temporal features. We also utilize UCF	UCF	UCF101
evaluated on UCF	UCF	UCF101
UCF101	UCF	UCF101
can achieve leading performance on UCF	UCF	UCF101
the UCF	UCF	UCF101
R , Shah M . UCF101	UCF	UCF101
UCF101	UCF101	UCF101
R , Shah M . UCF101	UCF101	UCF101
resulted in significant overfitting for UCF	UCF	UCF101
achieved 94.5% and 70.2% on UCF	UCF	UCF101
Representative video datasets, such as UCF	UCF	UCF101
and other popular video datasets (UCF	UCF	UCF101
The HMDB-51 [17] and UCF	UCF	UCF101
datasets, as well as the UCF	UCF	UCF101
3D CNNs trained on the UCF101 and HMDB51 datasets were inferior	UCF	UCF101
experiments described below us- ing UCF	UCF	UCF101
16], 3D CNNs trained on UCF	UCF	UCF101
Kinetics pretrained 3D CNNs on UCF	UCF	UCF101
perform well on relatively small UCF	UCF	UCF101
study focuses on four datasets: UCF	UCF	UCF101
UCF	UCF	UCF101
human action classes. Similar to UCF	UCF	UCF101
a) UCF	UCF	UCF101
losses. The validation losses on UCF	UCF	UCF101
16], 3D CNNs trained on UCF	UCF	UCF101
we used split 1 of UCF	UCF	UCF101
figure, the validation losses on UCF	UCF	UCF101
video. The validation accuracies of UCF	UCF	UCF101
3D CNNs from scratch on UCF	UCF	UCF101
Kinetics pretrained 3D CNNs on UCF	UCF	UCF101
ResNet-18 trained from scratch, in UCF	UCF	UCF101
Table 4: Top-1 accuracies on UCF	UCF	UCF101
Method UCF	UCF	UCF101
on Kinetics is effective on UCF	UCF	UCF101
Table 5: Top-1 accuracies on UCF	UCF	UCF101
Method Dim UCF	UCF	UCF101
resulted in significant overfitting for UCF	UCF	UCF101
outperforms complex 2D architectures on UCF	UCF	UCF101
achieved 94.5% and 70.2% on UCF	UCF	UCF101
Roshan Zamir, and M. Shah. UCF101	UCF	UCF101
3D CNNs trained on the UCF101 and HMDB51 datasets were inferior	UCF101	UCF101
Roshan Zamir, and M. Shah. UCF101	UCF101	UCF101
large number of experiments on UCF101 and HMDB51 datasets. Our experiments	UCF	UCF101
performance on the public datasets UCF101 [24] and HMDB51 [25	UCF	UCF101
applied on video ‘‘v_Diving_g11_c02.avi’’ in UCF101 dataset. Firstly, the video is	UCF	UCF101
experiments on two standard datasets UCF101 and HMDB51. UCF101 dataset established	UCF	UCF101
we perform several tests on UCF101 andHMDB51 datasets (over all splits	UCF	UCF101
Related experiments are performed on UCF101 and HMDB51 datasets (over all	UCF	UCF101
with different sample segments on UCF101 and HMDB51 dataset (over all	UCF	UCF101
different attention module settings on UCF101 and HMDB51 datasets (over all	UCF	UCF101
testing. We exper- iment with UCF101 and HMDB51 datasets (over all	UCF	UCF101
different numbers of layers on UCF101 and HMDB51 datasets (over all	UCF	UCF101
The results are evaluated on UCF101 dataset (over all splits). We	UCF	UCF101
is performed on UCF101 and HMDB51 datasets (over all	UCF	UCF101
We perform the experiments on UCF101 dataset (over all splits). Segment	UCF	UCF101
the state-of-the-art methods on the UCF101 and HMDB51 datasets (over all	UCF	UCF101
We perform the experiments on UCF101 and HMDB51 datasets with R-STAN-101	UCF	UCF101
UCF101	UCF	UCF101
large number of experiments on UCF101 and HMDB51 datasets. Our experiments	UCF101	UCF101
performance on the public datasets UCF101 [24] and HMDB51 [25	UCF101	UCF101
applied on video ‘‘v_Diving_g11_c02.avi’’ in UCF101 dataset. Firstly, the video is	UCF101	UCF101
experiments on two standard datasets UCF101 and HMDB51. UCF101 dataset established	UCF101	UCF101
we perform several tests on UCF101 andHMDB51 datasets (over all splits	UCF101	UCF101
Related experiments are performed on UCF101 and HMDB51 datasets (over all	UCF101	UCF101
with different sample segments on UCF101 and HMDB51 dataset (over all	UCF101	UCF101
different attention module settings on UCF101 and HMDB51 datasets (over all	UCF101	UCF101
testing. We exper- iment with UCF101 and HMDB51 datasets (over all	UCF101	UCF101
different numbers of layers on UCF101 and HMDB51 datasets (over all	UCF101	UCF101
The results are evaluated on UCF101 dataset (over all splits). We	UCF101	UCF101
is performed on UCF101 and HMDB51 datasets (over all	UCF101	UCF101
We perform the experiments on UCF101 dataset (over all splits). Segment	UCF101	UCF101
the state-of-the-art methods on the UCF101 and HMDB51 datasets (over all	UCF101	UCF101
We perform the experiments on UCF101 and HMDB51 datasets with R-STAN-101	UCF101	UCF101
UCF101	UCF101	UCF101
two large benchmarks, HMDB51 and UCF101, demonstrate the effectiveness of the	UCF	UCF101
datasets, namely HMDB51 [38] and UCF101 [39]. The HMDB51 dataset is	UCF	UCF101
30 clips for testing. The UCF101 dataset consists of 13320 video	UCF	UCF101
on the HMDB51 and the UCF101 dataset. For both datasets, we	UCF	UCF101
dataset and 94.3%, on the UCF101 dataset. In comparison with the	UCF	UCF101
Method HMDB51 UCF101 Slow Fusion CNN [41	UCF	UCF101
ment on the UCF101 dataset is smaller. The performance	UCF	UCF101
of the UCF101 dataset is approaching saturation (>94	UCF	UCF101
94.3% on the HMDB51 and UCF101 dataset, respectively	UCF	UCF101
two large benchmarks, HMDB51 and UCF101, demonstrate the effectiveness of the	UCF101	UCF101
datasets, namely HMDB51 [38] and UCF101 [39]. The HMDB51 dataset is	UCF101	UCF101
30 clips for testing. The UCF101 dataset consists of 13320 video	UCF101	UCF101
on the HMDB51 and the UCF101 dataset. For both datasets, we	UCF101	UCF101
dataset and 94.3%, on the UCF101 dataset. In comparison with the	UCF101	UCF101
Method HMDB51 UCF101 Slow Fusion CNN [41	UCF101	UCF101
ment on the UCF101 dataset is smaller. The performance	UCF101	UCF101
of the UCF101 dataset is approaching saturation (>94	UCF101	UCF101
94.3% on the HMDB51 and UCF101 dataset, respectively	UCF101	UCF101
in current action classification datasets (UCF	UCF	UCF101
on HMDB-51 and 98.0% on UCF	UCF	UCF101
previous datasets, HMDB-51 [18] and UCF	UCF	UCF101
fine-tuning each on HMDB-51 and UCF	UCF	UCF101
up to 5k steps on UCF	UCF	UCF101
training and testing on either UCF	UCF	UCF101
split 1 test sets of UCF	UCF	UCF101
UCF	UCF	UCF101
testing on split 1 of UCF	UCF	UCF101
of pa- rameters and that UCF	UCF	UCF101
on Ki- netics than on UCF	UCF	UCF101
than that of RGB on UCF	UCF	UCF101
un- seen) videos of the UCF	UCF	UCF101
classifiers for the classes of UCF	UCF	UCF101
each net- work for the UCF	UCF	UCF101
-101/HMDB-51 classes (using the UCF	UCF	UCF101
and again evaluate on the UCF	UCF	UCF101
performance than directly training on UCF	UCF	UCF101
methods in table 5, on UCF	UCF	UCF101
streams, and gets 94.6% on UCF	UCF	UCF101
UCF	UCF	UCF101
Table 4. Performance on the UCF	UCF	UCF101
pretrained weights. Original: train on UCF	UCF	UCF101
the last layer trained on UCF	UCF	UCF101
pre-training with end-to-end fine-tuning on UCF	UCF	UCF101
Model UCF	UCF	UCF101
Comparison with state-of-the-art on the UCF	UCF	UCF101
overall performance to 98.0 on UCF	UCF	UCF101
dataset (Kinetics) to another dataset (UCF	UCF	UCF101
R. Zamir, and M. Shah. UCF101	UCF	UCF101
R. Zamir, and M. Shah. UCF101	UCF101	UCF101
large number of experiments on UCF101 and HMDB51 datasets. Our experiments	UCF	UCF101
performance on the public datasets UCF101 [24] and HMDB51 [25	UCF	UCF101
applied on video ‘‘v_Diving_g11_c02.avi’’ in UCF101 dataset. Firstly, the video is	UCF	UCF101
experiments on two standard datasets UCF101 and HMDB51. UCF101 dataset established	UCF	UCF101
we perform several tests on UCF101 andHMDB51 datasets (over all splits	UCF	UCF101
Related experiments are performed on UCF101 and HMDB51 datasets (over all	UCF	UCF101
with different sample segments on UCF101 and HMDB51 dataset (over all	UCF	UCF101
different attention module settings on UCF101 and HMDB51 datasets (over all	UCF	UCF101
testing. We exper- iment with UCF101 and HMDB51 datasets (over all	UCF	UCF101
different numbers of layers on UCF101 and HMDB51 datasets (over all	UCF	UCF101
The results are evaluated on UCF101 dataset (over all splits). We	UCF	UCF101
is performed on UCF101 and HMDB51 datasets (over all	UCF	UCF101
We perform the experiments on UCF101 dataset (over all splits). Segment	UCF	UCF101
the state-of-the-art methods on the UCF101 and HMDB51 datasets (over all	UCF	UCF101
We perform the experiments on UCF101 and HMDB51 datasets with R-STAN-101	UCF	UCF101
UCF101	UCF	UCF101
large number of experiments on UCF101 and HMDB51 datasets. Our experiments	UCF101	UCF101
performance on the public datasets UCF101 [24] and HMDB51 [25	UCF101	UCF101
applied on video ‘‘v_Diving_g11_c02.avi’’ in UCF101 dataset. Firstly, the video is	UCF101	UCF101
experiments on two standard datasets UCF101 and HMDB51. UCF101 dataset established	UCF101	UCF101
we perform several tests on UCF101 andHMDB51 datasets (over all splits	UCF101	UCF101
Related experiments are performed on UCF101 and HMDB51 datasets (over all	UCF101	UCF101
with different sample segments on UCF101 and HMDB51 dataset (over all	UCF101	UCF101
different attention module settings on UCF101 and HMDB51 datasets (over all	UCF101	UCF101
testing. We exper- iment with UCF101 and HMDB51 datasets (over all	UCF101	UCF101
different numbers of layers on UCF101 and HMDB51 datasets (over all	UCF101	UCF101
The results are evaluated on UCF101 dataset (over all splits). We	UCF101	UCF101
is performed on UCF101 and HMDB51 datasets (over all	UCF101	UCF101
We perform the experiments on UCF101 dataset (over all splits). Segment	UCF101	UCF101
the state-of-the-art methods on the UCF101 and HMDB51 datasets (over all	UCF101	UCF101
We perform the experiments on UCF101 and HMDB51 datasets with R-STAN-101	UCF101	UCF101
UCF101	UCF101	UCF101
method. Experimental results on the UCF	UCF	UCF101
-Sports, J-HMDB and UCF101 action detection datasets show that	UCF	UCF101
detection with state-of-the-art results on UCF	UCF	UCF101
-Sports, J-HMDB and UCF101 datasets. Our contributions are as	UCF	UCF101
Comparision of frame-level proposals on UCF	UCF	UCF101
k frame optical flows. Left: UCF	UCF	UCF101
using RGB frames. Interestingly, on UCF	UCF	UCF101
are better than RPN-a on UCF	UCF	UCF101
significant mo- tion occurring on UCF	UCF	UCF101
action detection on three datasets: UCF	UCF	UCF101
-Sports, J-HMDB and UCF	UCF	UCF101
the metrics used for evaluation. UCF	UCF	UCF101
results over the three splits. UCF	UCF	UCF101
split only. In contrast to UCF	UCF	UCF101
trun- cated to the action, UCF	UCF	UCF101
and video-AP measurement on the UCF	UCF	UCF101
0.05, 0.1, 0.2, 0.3] on UCF101	UCF	UCF101
NMS or score combination–ours). Left: UCF	UCF	UCF101
training the two-stream R-CNN on UCF	UCF	UCF101
the mentioned iterations on the UCF101 dataset empirically since it is	UCF	UCF101
averaging. We report results for UCF	UCF	UCF101
Test scales Train scales UCF	UCF	UCF101
-Sports J-HMDB UCF-Sports J-HMDB UCF	UCF	UCF101
one frame (Flow-1) on both UCF	UCF	UCF101
i.e. we gain 10.27% on UCF	UCF	UCF101
decreases the result significantly on UCF	UCF	UCF101
and performs on par on UCF	UCF	UCF101
multi-region two-stream faster R-CNN on UCF	UCF	UCF101
the RGB-1 R-CNN model on UCF	UCF	UCF101
obtains 82.3% and 56.6% on UCF	UCF	UCF101
models in Table 2 for UCF	UCF	UCF101
essential for an action. On UCF	UCF	UCF101
actions (everyday actions), while for UCF	UCF	UCF101
by 2.21% and 1.6% on UCF	UCF	UCF101
14.1% for “Golf” on UCF	UCF	UCF101
5.16% for “Swing 1” on UCF	UCF	UCF101
IoU threshold of δ on UCF	UCF	UCF101
- Sports, J-HMDB, and UCF101 datasets. Both of our approaches	UCF	UCF101
obtain 94.82% and 70.88% on UCF	UCF	UCF101
but are the similar for UCF	UCF	UCF101
Table 4: Video mAP on UCF	UCF	UCF101
-Sports, J-HMDB and UCF101 (split 1) with variant IoU	UCF	UCF101
UCF-Sports J-HMDB UCF101 (with temporal loc) UCF101 (w/o	UCF	UCF101
Table 5: Classification results on UCF	UCF	UCF101
UCF	UCF	UCF101
R-CNN model on J-HMDB and UCF101, and performs similarly on UCF-Sports	UCF	UCF101
The lack in improvement on UCF	UCF	UCF101
performance actually does improve for UCF	UCF	UCF101
perform temporal lo- calization on UCF101	UCF	UCF101
the average class accuracy on UCF	UCF	UCF101
obtains 95.74% and 71.08% on UCF	UCF	UCF101
the state-of-the-art results on both UCF	UCF	UCF101
on par with [9] on UCF101	UCF	UCF101
4.3%, 12.4% and 0.2% on UCF	UCF	UCF101
-Sports, J-HMDB and UCF101, respectively. Both [8] and [9	UCF	UCF101
UCF-Sports J-HMDB UCF101 (split 1	UCF	UCF101
for difficult datasets such as UCF101	UCF	UCF101
9]. In our experiments on UCF101, we observed that a limitation	UCF	UCF101
K., Zamir, A.R., Shah, M.: UCF101	UCF	UCF101
on the UCF-Sports, J-HMDB and UCF101 action detection datasets show that	UCF101	UCF101
results on UCF-Sports, J-HMDB and UCF101 datasets. Our contributions are as	UCF101	UCF101
0.05, 0.1, 0.2, 0.3] on UCF101	UCF101	UCF101
the mentioned iterations on the UCF101 dataset empirically since it is	UCF101	UCF101
on UCF- Sports, J-HMDB, and UCF101 datasets. Both of our approaches	UCF101	UCF101
mAP on UCF-Sports, J-HMDB and UCF101 (split 1) with variant IoU	UCF101	UCF101
UCF-Sports J-HMDB UCF101 (with temporal loc) UCF101 (w/o	UCF101	UCF101
R-CNN model on J-HMDB and UCF101, and performs similarly on UCF-Sports	UCF101	UCF101
perform temporal lo- calization on UCF101	UCF101	UCF101
on par with [9] on UCF101	UCF101	UCF101
0.2% on UCF-Sports, J-HMDB and UCF101, respectively. Both [8] and [9	UCF101	UCF101
UCF-Sports J-HMDB UCF101 (split 1	UCF101	UCF101
for difficult datasets such as UCF101	UCF101	UCF101
9]. In our experiments on UCF101, we observed that a limitation	UCF101	UCF101
K., Zamir, A.R., Shah, M.: UCF101	UCF101	UCF101
state of the art on UCF	UCF	UCF101
train and 1530 test videos. UCF	UCF	UCF101
all splits for HMDB and UCF in Section 4.4	UCF	UCF101
b) UCF	UCF	UCF101
previous work on HMDB and UCF	UCF	UCF101
3-split avg on HMDB and UCF, but is not comparable to	UCF	UCF101
initialization methods on HMDB and UCF	UCF	UCF101
Roshan Zamir, and Mubarak Shah. UCF101	UCF	UCF101
Roshan Zamir, and Mubarak Shah. UCF101	UCF101	UCF101
capture motion patterns, and (3) UCF	UCF	UCF101
3) UCF	UCF	UCF101
converges. For the training of UCF	UCF	UCF101
benchmarks and immune to over-fitting, UCF	UCF	UCF101
is increased to 0.9 for UCF	UCF	UCF101
in [9]. For Kinetics-600 and UCF	UCF	UCF101
Recognition: For Kinetics-600 and UCF	UCF	UCF101
XP Jetson TX2 Kinetics-600 Jester UCF	UCF	UCF101
UCF101 dataset is an action recognition	UCF	UCF101
to Kinetics-600 and Jester datasets, UCF	UCF	UCF101
over-fitting. For the evaluation of UCF	UCF	UCF101
used only split-1. We selected UCF	UCF	UCF101
inferior results in Kinetics-600 and UCF	UCF	UCF101
mance on both Kinetics-600 and UCF	UCF	UCF101
UCF101 dataset is an action recognition	UCF101	UCF101
the top layers on the UCF	UCF	UCF101
formance improvements compared to the UCF	UCF	UCF101
65.4%, up from 41.3%) on UCF	UCF	UCF101
training the entire network on UCF	UCF	UCF101
since only some classes in UCF	UCF	UCF101
apply our networks to the UCF	UCF	UCF101
established by training networks on UCF	UCF	UCF101
commonly used datasets (KTH, Weizmann, UCF Sports, IXMAS, Hollywood 2, UCF-50	UCF	UCF101
and the recently in- troduced UCF	UCF	UCF101
transfer learning experi- ments on UCF	UCF	UCF101
Table 3: Results on UCF	UCF	UCF101
4.2. Transfer Learning Experiments on UCF	UCF	UCF101
learn- ing experiments on the UCF	UCF	UCF101
net- work from scratch on UCF	UCF	UCF101
Results. To prepare UCF	UCF	UCF101
the YouTube video IDs of UCF	UCF	UCF101
Slow Fusion net- work on UCF	UCF	UCF101
dataset has no overlap with UCF	UCF	UCF101
Slow Fusion network in our UCF	UCF	UCF101
of classes present in the UCF	UCF	UCF101
to the Sports categories in UCF	UCF	UCF101
Our transfer learning experiments on UCF	UCF	UCF101
R. Zamir, and M. Shah. UCF101	UCF	UCF101
R. Zamir, and M. Shah. UCF101	UCF101	UCF101
e.g., 45.4% v.s. 61.2% on UCF101	UCF	UCF101
based on two popular dataset UCF101 [36] and HMDB51 [23]. Our	UCF	UCF101
we incorporate five datasets: the UCF101 [36], the Kinetics [19], the	UCF	UCF101
Unless specifically state, we use UCF101 dataset for our model pre-training	UCF	UCF101
UCF101 dataset [36] consists of 13,320	UCF	UCF101
When pre-training on UCF101 train split 1 video data	UCF	UCF101
statistics for action recognition on UCF101	UCF	UCF101
using different statistics design on UCF101 train split 1. For local	UCF	UCF101
finetune the pre-train model on UCF101 train split 1 data with	UCF	UCF101
and their combina- tion on UCF101 and HMDB51 dataset as shown	UCF	UCF101
a useful self-supervised signals for UCF101 and HMDB51 dataset. The motion	UCF	UCF101
for action recognition on the UCF101 dataset	UCF	UCF101
different supervision signals on the UCF101 and the HMDB51 datasets	UCF	UCF101
Domain UCF101 acc.(%) HMDB51 acc. (%) From	UCF	UCF101
interesting to note that although UCF101 only improves 1% when combined	UCF	UCF101
with the state-of-the-art both on UCF101 and HMDB51. Compared with methods	UCF	UCF101
that are pre- trained on UCF101 dataset, we improve 9.3% accuracy	UCF	UCF101
12] and 2.5% accuracy on UCF101 than [24]. Compared with the	UCF	UCF101
also achieve 0.6% improvement on UCF101 and 5.1% improvement on HMDB51	UCF	UCF101
video representation learning methods on UCF101 and HMDB51	UCF	UCF101
Method UCF101 acc.(%) HMDB51 acc	UCF	UCF101
Geometry [12] 55.1 23.3 Ours (UCF101	UCF	UCF101
Use the C3D pre-trained on UCF101 with labels as feature extractor	UCF	UCF101
Use the C3D pre-trained on UCF101 with our self-supervised task as	UCF	UCF101
Use the C3D finetuned on UCF101 on our self-supervised model as	UCF	UCF101
pre-trained model with labels on UCF101, we can further get around	UCF	UCF101
The performance on UCF101, HMDB51 and ASLAN dataset shows	UCF	UCF101
achieves state-of-the-art perfor- mances on UCF101 and HMDB51 datasets. This strongly	UCF	UCF101
e.g., 45.4% v.s. 61.2% on UCF101	UCF101	UCF101
based on two popular dataset UCF101 [36] and HMDB51 [23]. Our	UCF101	UCF101
we incorporate five datasets: the UCF101 [36], the Kinetics [19], the	UCF101	UCF101
Unless specifically state, we use UCF101 dataset for our model pre-training	UCF101	UCF101
UCF101 dataset [36] consists of 13,320	UCF101	UCF101
When pre-training on UCF101 train split 1 video data	UCF101	UCF101
statistics for action recognition on UCF101	UCF101	UCF101
using different statistics design on UCF101 train split 1. For local	UCF101	UCF101
finetune the pre-train model on UCF101 train split 1 data with	UCF101	UCF101
and their combina- tion on UCF101 and HMDB51 dataset as shown	UCF101	UCF101
a useful self-supervised signals for UCF101 and HMDB51 dataset. The motion	UCF101	UCF101
for action recognition on the UCF101 dataset	UCF101	UCF101
different supervision signals on the UCF101 and the HMDB51 datasets	UCF101	UCF101
Domain UCF101 acc.(%) HMDB51 acc. (%) From	UCF101	UCF101
interesting to note that although UCF101 only improves 1% when combined	UCF101	UCF101
with the state-of-the-art both on UCF101 and HMDB51. Compared with methods	UCF101	UCF101
that are pre- trained on UCF101 dataset, we improve 9.3% accuracy	UCF101	UCF101
12] and 2.5% accuracy on UCF101 than [24]. Compared with the	UCF101	UCF101
also achieve 0.6% improvement on UCF101 and 5.1% improvement on HMDB51	UCF101	UCF101
video representation learning methods on UCF101 and HMDB51	UCF101	UCF101
Method UCF101 acc.(%) HMDB51 acc	UCF101	UCF101
Geometry [12] 55.1 23.3 Ours (UCF101) 58.8 32.6	UCF101	UCF101
Use the C3D pre-trained on UCF101 with labels as feature extractor	UCF101	UCF101
Use the C3D pre-trained on UCF101 with our self-supervised task as	UCF101	UCF101
Use the C3D finetuned on UCF101 on our self-supervised model as	UCF101	UCF101
pre-trained model with labels on UCF101, we can further get around	UCF101	UCF101
The performance on UCF101, HMDB51 and ASLAN dataset shows	UCF101	UCF101
achieves state-of-the-art perfor- mances on UCF101 and HMDB51 datasets. This strongly	UCF101	UCF101
capture motion patterns, and (3) UCF	UCF	UCF101
3) UCF	UCF	UCF101
converges. For the training of UCF	UCF	UCF101
benchmarks and immune to over-fitting, UCF	UCF	UCF101
is increased to 0.9 for UCF	UCF	UCF101
in [9]. For Kinetics-600 and UCF	UCF	UCF101
Recognition: For Kinetics-600 and UCF	UCF	UCF101
XP Jetson TX2 Kinetics-600 Jester UCF	UCF	UCF101
UCF101 dataset is an action recognition	UCF	UCF101
to Kinetics-600 and Jester datasets, UCF	UCF	UCF101
over-fitting. For the evaluation of UCF	UCF	UCF101
used only split-1. We selected UCF	UCF	UCF101
inferior results in Kinetics-600 and UCF	UCF	UCF101
mance on both Kinetics-600 and UCF	UCF	UCF101
UCF101 dataset is an action recognition	UCF101	UCF101
capture motion patterns, and (3) UCF	UCF	UCF101
3) UCF	UCF	UCF101
converges. For the training of UCF	UCF	UCF101
benchmarks and immune to over-fitting, UCF	UCF	UCF101
is increased to 0.9 for UCF	UCF	UCF101
in [9]. For Kinetics-600 and UCF	UCF	UCF101
Recognition: For Kinetics-600 and UCF	UCF	UCF101
XP Jetson TX2 Kinetics-600 Jester UCF	UCF	UCF101
UCF101 dataset is an action recognition	UCF	UCF101
to Kinetics-600 and Jester datasets, UCF	UCF	UCF101
over-fitting. For the evaluation of UCF	UCF	UCF101
used only split-1. We selected UCF	UCF	UCF101
inferior results in Kinetics-600 and UCF	UCF	UCF101
mance on both Kinetics-600 and UCF	UCF	UCF101
UCF101 dataset is an action recognition	UCF101	UCF101
UCF101	UCF	UCF101
Keywords: Action Dataset, UCF101, UCF50, Action Recognition	UCF	UCF101
UCF101	UCF	UCF101
UCF101	UCF	UCF101
We introduce UCF101 which is currently the largest	UCF	UCF101
the best of our knowledge, UCF101 is currently the most challenging	UCF	UCF101
e.g. KTH [11], Weizmann [3], UCF Sports [10], IXMAS [12] datasets	UCF	UCF101
by actors; HOHA [7] and UCF Sports are composed of movie	UCF	UCF101
and clips. (HMDB51 [5] and UCF50 [9] are the currently the	UCF	UCF101
for 6 action classes of UCF101	UCF	UCF101
of 6 action classes from UCF101	UCF	UCF101
Action Classes: UCF101 includes total number of 101	UCF	UCF101
UCF101 is an extension of UCF50 which included the following 50	UCF	UCF101
UCF101	UCF	UCF101
2. 101 actions included in UCF101 shown with one sample frame	UCF	UCF101
new classes are introduced in UCF101	UCF	UCF101
for each action class of UCF101	UCF	UCF101
1. Summary of Characteristics of UCF101	UCF	UCF101
the dataset (available at http://crcv.ucf.edu/data/ UCF101	UCF	UCF101
UCF101	UCF	UCF101
UCF101	UCF	UCF101
to provide baseline results on UCF101	UCF	UCF101
Static No 2005 Actor Staged UCF Sports [10] 9 182 Dynamic	UCF	UCF101
Static No 2006 Actor Staged UCF11 [6] 11 1168 Dynamic Yes	UCF	UCF101
800 Dynamic Yes 2010 YouTube UCF50 [9] 50 6681 Dynamic Yes	UCF	UCF101
Yes 2011 Movies, YouTube, Web UCF101 101 13320 Dynamic Yes 2012	UCF	UCF101
of the reported tests on UCF101	UCF	UCF101
4. Related Datasets UCF Sports, UCF11, UCF50 and UCF101 are the	UCF	UCF101
action datasets compiled by UCF in chronological order; each one	UCF	UCF101
ifications in the portion of UCF101 which includes UCF50 videos: the	UCF	UCF101
characteristics of each. Note that UCF101 is remarkably larger than the	UCF	UCF101
5. Conclusion We introduced UCF101 which is the most challeng	UCF	UCF101
outstandingly larger than other datasets. UCF101 is composed of unconstrained videos	UCF	UCF101
bag of words approach on UCF101	UCF	UCF101
UCF101	UCF101	UCF101
Keywords: Action Dataset, UCF101, UCF50, Action Recognition	UCF101	UCF101
UCF101	UCF101	UCF101
UCF101	UCF101	UCF101
We introduce UCF101 which is currently the largest	UCF101	UCF101
the best of our knowledge, UCF101 is currently the most challenging	UCF101	UCF101
for 6 action classes of UCF101	UCF101	UCF101
of 6 action classes from UCF101	UCF101	UCF101
Action Classes: UCF101 includes total number of 101	UCF101	UCF101
UCF101 is an extension of UCF50	UCF101	UCF101
UCF101	UCF101	UCF101
2. 101 actions included in UCF101 shown with one sample frame	UCF101	UCF101
new classes are introduced in UCF101	UCF101	UCF101
for each action class of UCF101	UCF101	UCF101
1. Summary of Characteristics of UCF101	UCF101	UCF101
the dataset (available at http://crcv.ucf.edu/data/ UCF101	UCF101	UCF101
UCF101	UCF101	UCF101
UCF101	UCF101	UCF101
to provide baseline results on UCF101	UCF101	UCF101
Yes 2011 Movies, YouTube, Web UCF101 101 13320 Dynamic Yes 2012	UCF101	UCF101
of the reported tests on UCF101	UCF101	UCF101
UCF Sports, UCF11, UCF50 and UCF101 are the four	UCF101	UCF101
ifications in the portion of UCF101 which includes UCF50 videos: the	UCF101	UCF101
characteristics of each. Note that UCF101 is remarkably larger than the	UCF101	UCF101
5. Conclusion We introduced UCF101 which is the most challeng	UCF101	UCF101
outstandingly larger than other datasets. UCF101 is composed of unconstrained videos	UCF101	UCF101
bag of words approach on UCF101	UCF101	UCF101
significant im- provements over the UCF101 and HMDB51 benchmarks	UCF	UCF101
conducted on two widely-used benchmarks, UCF101 [38] and HMDB51 [18]. UCF101	UCF	UCF101
Method Feature Setting HMDB51 UCF101 ST [45] BoW T 15.0±3.0	UCF	UCF101
tween 5 and 10 minutes. UCF101 is composed of realistic action	UCF	UCF101
ping classes between ActivityNet and UCF101 are not used during fine-tuning	UCF	UCF101
seen/unseen splits for HMDB51 and UCF101 are 27/26 and 51/50, respectively	UCF	UCF101
Dataset HMDB51 UCF101 Setting Cross-Dataset Transductive Cross-Dataset Transductive	UCF	UCF101
6.9% in transductive scenario on UCF101	UCF	UCF101
the concepts of HMDB51 and UCF101 are not very distinctive. We	UCF	UCF101
on HMDB51 and 1% on UCF101 be- tween ‘Ours’ and ‘No	UCF	UCF101
R. Zamir, and M. Shah. UCF101	UCF	UCF101
significant im- provements over the UCF101 and HMDB51 benchmarks	UCF101	UCF101
conducted on two widely-used benchmarks, UCF101 [38] and HMDB51 [18]. UCF101	UCF101	UCF101
Method Feature Setting HMDB51 UCF101 ST [45] BoW T 15.0±3.0	UCF101	UCF101
tween 5 and 10 minutes. UCF101 is composed of realistic action	UCF101	UCF101
ping classes between ActivityNet and UCF101 are not used during fine-tuning	UCF101	UCF101
seen/unseen splits for HMDB51 and UCF101 are 27/26 and 51/50, respectively	UCF101	UCF101
Dataset HMDB51 UCF101 Setting Cross-Dataset Transductive Cross-Dataset Transductive	UCF101	UCF101
6.9% in transductive scenario on UCF101	UCF101	UCF101
the concepts of HMDB51 and UCF101 are not very distinctive. We	UCF101	UCF101
on HMDB51 and 1% on UCF101 be- tween ‘Ours’ and ‘No	UCF101	UCF101
R. Zamir, and M. Shah. UCF101	UCF101	UCF101
of Sports1M, which we name miniSports	miniSports	miniSports
on the test set of miniSports, since our aim is to	miniSports	miniSports
on the training set of miniSports	miniSports	miniSports
achieved by MC3-18 on the miniSports test set with K	miniSports	miniSports
on the test set of miniSports with K = 10 sampled	miniSports	miniSports
of MC3-18 on the miniSports test set vs the number	miniSports	miniSports
Evaluation is done on the miniSports dataset, with the MC3-18 clip	miniSports	miniSports
video-level classi- fication accuracy on miniSports, obtained by varying the number	miniSports	miniSports
of Sports1M, which we name miniSports	miniSports	miniSports
on the test set of miniSports, since our aim is to	miniSports	miniSports
on the training set of miniSports	miniSports	miniSports
achieved by MC3-18 on the miniSports test set with K	miniSports	miniSports
on the test set of miniSports with K = 10 sampled	miniSports	miniSports
of MC3-18 on the miniSports test set vs the number	miniSports	miniSports
Evaluation is done on the miniSports dataset, with the MC3-18 clip	miniSports	miniSports
video-level classi- fication accuracy on miniSports, obtained by varying the number	miniSports	miniSports
datasets, i.e. Moments in Time [17] and Kinetics [2]. Ac	Moments in Time	Moments in Time
Moments in Time. The Moments in Time dataset contains	Moments in Time	Moments in Time
2, on both of the Moments in Time and Kinetics	Moments in Time	Moments in Time
validation set of Moments in Time	Moments in Time	Moments in Time
Their performances on the Moments in Time and Kinet	Moments in Time	Moments in Time
of Moments in Time	Moments in Time	Moments in Time
is longer than those in Moments in Time	Moments in Time	Moments in Time
On the Moments in Time dataset, Table 7 shows a	Moments in Time	Moments in Time
the 1st place in the Moments in Time Challenge	Moments in Time	Moments in Time
Moments in Time, the mean coefficients of the	Moments in Time	Moments in Time
Moments in Time Kinetics	Moments in Time	Moments in Time
in Time Challenge 2018. Moreover, based on	Time	Moments in Time
Time Interest Points	Time	Moments in Time
datasets, i.e. Moments in Time [17] and Kinetics [2]. Ac	Time	Moments in Time
Moments in Time	Time	Moments in Time
. The Moments in Time dataset contains	Time	Moments in Time
both of the Moments in Time and Kinetics	Time	Moments in Time
validation set of Moments in Time	Time	Moments in Time
performances on the Moments in Time and Kinet	Time	Moments in Time
of Moments in Time	Time	Moments in Time
than those in Moments in Time	Time	Moments in Time
On the Moments in Time dataset, Table 7 shows a	Time	Moments in Time
place in the Moments in Time Challenge	Time	Moments in Time
Moments in Time, the mean coefficients of the	Time	Moments in Time
ments in Time dataset depends more on temporal	Time	Moments in Time
Moments in Time Kinetics	Time	Moments in Time
in Time dataset. We sum up the	Time	Moments in Time
the 1st place in the Moments	Moments	Moments in Time
datasets, i.e. Moments in Time [17] and Kinetics	Moments	Moments in Time
Moments in Time. The Moments in Time dataset contains	Moments	Moments in Time
Moments CoST(a) 29.3 55.8 42.6	Moments	Moments in Time
2, on both of the Moments in Time and Kinetics	Moments	Moments in Time
Moments 29.0 56.1 42.5	Moments	Moments in Time
validation set of Moments in Time	Moments	Moments in Time
Their performances on the Moments in Time and Kinet	Moments	Moments in Time
resolution, i.e. 32 frames. On Moments in	Moments	Moments in Time
of Moments in Time. Methods marked in	Moments	Moments in Time
is longer than those in Moments in Time	Moments	Moments in Time
On the Moments in Time dataset, Table 7	Moments	Moments in Time
the 1st place in the Moments in Time Challenge	Moments	Moments in Time
Moments in Time, the mean coefficients	Moments	Moments in Time
each action category on the Moments	Moments	Moments in Time
Moments in Time Kinetics	Moments	Moments in Time
Moments in time dataset: one million	Moments	Moments in Time
for action recognition in videos. Existing deep neural net	in	Moments in Time
learn spatial and temporal features in	in	Moments in Time
and won the 1st place in the Moments	in	Moments in Time
in Time Challenge 2018. Moreover, based	in	Moments in Time
attention considering its potential in a wide range of appli	in	Moments in Time
task lies in joint spatiotemporal feature learning. The	in	Moments in Time
in an action and the scene	in	Moments in Time
feature captures motion cues embedded in the evolving	in	Moments in Time
tion information explicitly and in parallel to spatial informa	in	Moments in Time
exactly the same amount of in	in	Moments in Time
tion cues directly. As shown in Figure 2(c), by fusing com	in	Moments in Time
and color blobs also exist in tempo	in	Moments in Time
2) Convolution kernels in C2D networks are inherently re	in	Moments in Time
less prone to overfitting, resulting in better per	in	Moments in Time
efficient for each channel in each view, which allows the	in	Moments in Time
the task of action recognition in videos, experiments	in	Moments in Time
dynamics and long range dependences in videos	in	Moments in Time
tion in videos	in	Moments in Time
is shown in Figure 3(a). To handle 3D	in	Moments in Time
video action recognition is illustrated in Table 1. For con	in	Moments in Time
are converted to 3D by in	in	Moments in Time
3. As ex- plored in [30], given a residual unit	in	Moments in Time
3 (C3D3×3×3) as shown in Figure 3(b), or inflate the	in	Moments in Time
1 (C3D3×1×1) as shown in Figure 3(c). Experiments in [30	in	Moments in Time
is more computationally efficient. Therefore, in our imple	in	Moments in Time
W for spatial feature. While in	in	Moments in Time
learned during training in an end-to-end manner	in	Moments in Time
CoST(a). As illustrated in Figure 4, the coefficients α	in	Moments in Time
with back-propagation during training. During in	in	Moments in Time
architecture of CoST(b) is illustrated in Figure 5. The	in	Moments in Time
function f in Equation (3). Specifically, for each	in	Moments in Time
the proposed CoST is shown in Fig	in	Moments in Time
As shown in Figure 6(a), if the coefficients	in	Moments in Time
enabled in CoST, although the receptive field	in	Moments in Time
voxels in total, the corresponding 19 parameters	in	Moments in Time
The number of multiply-adds involved in the	in	Moments in Time
voxels in the receptive field are duplicately	in	Moments in Time
multiple views in our current implementation. With an	in	Moments in Time
the task of action recognition in videos, we perform ex	in	Moments in Time
datasets, i.e. Moments in Time [17] and Kinetics [2	in	Moments in Time
in all experiments	in	Moments in Time
Moments in Time. The Moments in Time dataset contains	in	Moments in Time
frames, resulting in 8 frames in total. Next, image patches	in	Moments in Time
4 GPUs are employed, resulting in a total	in	Moments in Time
in Table 2, on both of	in	Moments in Time
the Moments in Time and Kinetics	in	Moments in Time
mechanism introduced in our model. It also reveals	in	Moments in Time
is adopted in the following experiments	in	Moments in Time
three convolutional layers in Figure 5 are learned indepen	in	Moments in Time
that spatiotemporal features are learned in a de	in	Moments in Time
coupled manner. As listed in Table 3, with weight sharing	in	Moments in Time
three spatial and temporal views in	in	Moments in Time
validation set of Moments in Time	in	Moments in Time
Their performances on the Moments in Time and Kinet	in	Moments in Time
ics datasets are listed in Table 4 and Table 5	in	Moments in Time
in Section 3.4	in	Moments in Time
i.e. 32 frames. On Moments in	in	Moments in Time
of Moments in Time. Methods marked in gray	in	Moments in Time
in this dataset is longer than	in	Moments in Time
those in Moments in Time	in	Moments in Time
On the Moments in Time dataset, Table 7 shows	in	Moments in Time
the ResNet-50 C2D baseline reported in [17] by	in	Moments in Time
2.9% and 5.5% in terms of top-1 and top-5	in	Moments in Time
RGB, optical flow and audio) in [17] by a large	in	Moments in Time
which won the 1st place in the Moments in Time Challenge	in	Moments in Time
performance. As shown in Table 6, CoST has a	in	Moments in Time
Moments in Time, the mean coefficients of	in	Moments in Time
ments in Time dataset depends more on	in	Moments in Time
three views in all CoST layers of the	in	Moments in Time
This also verifies the conclusion in [32] that temporal rep	in	Moments in Time
Moments in Time Kinetics	in	Moments in Time
views in CoST layers of various depths	in	Moments in Time
in Time dataset. We sum up	in	Moments in Time
in Figure 7, for actions such	in	Moments in Time
of different views as shown in	in	Moments in Time
supported by the spacetime model in the context	in	Moments in Time
jor challenge for action recognition in videos. In this pa	in	Moments in Time
in replacement for	in	Moments in Time
Moments in time dataset: one million videos	in	Moments in Time
volutional networks for action recognition in videos. In	in	Moments in Time
Speed-accuracy trade-offs in video classification. In ECCV	in	Moments in Time
test, including HMDB, Kinetics, and Moments in Time	Moments in Time	Moments in Time
standard clas- sification evaluation protocol. Moments in Time [18] is a large-scale dataset	Moments in Time	Moments in Time
Transfer learned architectures - Moments in Time	Moments in Time	Moments in Time
training it on another dataset: Moments in Time [18]. Table 5 shows the	Moments in Time	Moments in Time
Time Neural Architectures for Videos	Time	Moments in Time
HMDB, Kinetics, and Moments in Time	Time	Moments in Time
Time	Time	Moments in Time
sification evaluation protocol. Moments in Time [18] is a large-scale dataset	Time	Moments in Time
learned architectures - Moments in Time	Time	Moments in Time
on another dataset: Moments in Time [18]. Table 5 shows the	Time	Moments in Time
test, including HMDB, Kinetics, and Moments in Time. We will open	Moments	Moments in Time
tested, including HMDB, Kinetics, and Moments in time. This is done	Moments	Moments in Time
standard clas- sification evaluation protocol. Moments in Time [18] is a	Moments	Moments in Time
Transfer learned architectures - Moments in Time: We evaluate the	Moments	Moments in Time
training it on another dataset: Moments in Time [18]. Table 5	Moments	Moments in Time
Table 5. Moments in time. We show that	Moments	Moments in Time
Gutfruend, Carl Vondrick, et al. Moments in time dataset: one million	Moments	Moments in Time
Gutfruend, Carl Vondrick, et al. Moments in time dataset: one million	Moments	Moments in Time
that capture rich spatio-temporal information in videos. Previous work, taking advantage	in	Moments in Time
including HMDB, Kinetics, and Moments in Time. We will open source	in	Moments in Time
with many successful prior approaches, in	in	Moments in Time
capture the rich spatio-temporal interactions in video data	in	Moments in Time
the rich spatio-temporal information present in videos. Neural architecture search and	in	Moments in Time
in	in	Moments in Time
various spatial and temporal interactions in videos. We encourage explo- ration	in	Moments in Time
the search space for video in	in	Moments in Time
we learn 2D spatial filters in addition to the temporal Gaus	in	Moments in Time
allow learning of joint features in 3D. The 2D filter is	in	Moments in Time
in	in	Moments in Time
time capture longer temporal information in videos	in	Moments in Time
temporal size of the filters in each module. See text for	in	Moments in Time
The proposed algorithm results in novel architectures which comprise interesting	in	Moments in Time
different types of layers combined in the same module e.g., an	in	Moments in Time
of the architecture, which is in contrast to previous handcrafted models	in	Moments in Time
including HMDB, Kinetics, and Moments in time. This is done with	in	Moments in Time
and 2D convolutional lay- ers in addition to the 3D layers	in	Moments in Time
design is also widely adopted in action recognition, which takes optical	in	Moments in Time
flow inputs in addition to raw RGBs [3	in	Moments in Time
on capturing longer temporal information in continuous videos using pooling [19	in	Moments in Time
of an additional temporal dimension in the input and all intermediate	in	Moments in Time
than CNNs and becomes prohibitive in many cases. Further, expanding 2D	in	Moments in Time
2D kernels L times, results in state-of-the-art performance [1]. (2+1)D convolutional	in	Moments in Time
layer, which was often used in video CNNs such as R(2+1)D	in	Moments in Time
µ is constrained to be in [0, L), µ = (1/2)(L−1	in	Moments in Time
of inflated TGMs are shown in Fig. 3	in	Moments in Time
in	in	Moments in Time
in	in	Moments in Time
their temporal duration can vary in large ranges	in	Moments in Time
courages more diverse architectures early in the search. Neural architecture evolution	in	Moments in Time
ResNet- like meta-architecture is illustrated in Figure 4. This meta- architecture	in	Moments in Time
and pooling layers we allow in each module, and N	in	Moments in Time
9 is the number modules in the meta-architecture. There are 2	in	Moments in Time
P , where each individual in the population is a particular	in	Moments in Time
P controls the randomness in of the parent selec- tion	in	Moments in Time
population, P Evaluate each individual in P for i < number	in	Moments in Time
the most fit individual in S child = parent for	in	Moments in Time
architecture search space we describe in Section 4.1 efficiently, we consider	in	Moments in Time
Importantly, we design the mutation in our al- gorithm to happen	in	Moments in Time
applying many mu- tation operators in the early stage of the	in	Moments in Time
ducing the amount of mutations in the later stages, which is	in	Moments in Time
to controlling the learning rate in a CNN model learning. As	in	Moments in Time
described in Algorithm 1, we apply max(d	in	Moments in Time
operators we want to apply in the beginning, and r controls	in	Moments in Time
is added to the population, in order to maintain the size	in	Moments in Time
did not make much difference in our case	in	Moments in Time
clas- sification evaluation protocol. Moments in Time [18] is a large-scale	in	Moments in Time
derstanding of actions and events in videos (339 classes, 802,264 training	in	Moments in Time
Architecture evolution is done in parallel on smaller in- put	in	Moments in Time
iterations. Details can be found in the appendix. We perform evolution	in	Moments in Time
1) uses L = 7 in the first 3D conv. layer	in	Moments in Time
and L = 3 in all the other 3D layers	in	Moments in Time
2) uses L = 3 in all its layers	in	Moments in Time
This is not only done in terms of recognition accuracy but	in	Moments in Time
also in terms of computational efficiency. As	in	Moments in Time
shown in Table 7, our individ- ual	in	Moments in Time
this set is ∼10% smaller (in training/validation set size) than the	in	Moments in Time
we use Kinetics pre-training as in [36]). As shown, we outperform	in	Moments in Time
use RGB input (i.e., one-stream) in this experiment	in	Moments in Time
Transfer learned architectures - Moments in Time: We evaluate the models	in	Moments in Time
it on another dataset: Moments in Time [18]. Table 5 shows	in	Moments in Time
Table 5. Moments in time. We show that models	in	Moments in Time
ran- dom models. As shown in Table 13, we compared with	in	Moments in Time
forming inference on a video in ∼100 ms (Table 7). Note	in	Moments in Time
faster, 258 ms, than previous in	in	Moments in Time
and faster runtimes. This gain in runtime is due to the	in	Moments in Time
at most of the locations in the ar- chitectures, while being	in	Moments in Time
Kinetics. An average activity duration in Charades videos are around 12	in	Moments in Time
iTGM layers are most common in the best models (Ta- ble	in	Moments in Time
76.6 Arch Search without iTGM in space 76.8 EvaNet 77.2	in	Moments in Time
Chen, and Shuicheng Yan. Network in net- work. 2013. 3	in	Moments in Time
Carl Vondrick, et al. Moments in time dataset: one million videos	in	Moments in Time
Ryoo. Learn- ing latent sub-events in activity videos using temporal atten	in	Moments in Time
Laptev, and Abhinav Gupta. Hollywood in homes: Crowdsourcing data collection for	in	Moments in Time
volutional networks for action recognition in videos. In Ad- vances in	in	Moments in Time
Dense detailed labeling of actions in complex videos. In- ternational Journal	in	Moments in Time
Torralba. Tem- poral relational reasoning in videos. arXiv preprint arXiv:1711.08496, 2017	in	Moments in Time
architectures following the Inception-like meta-architectures in Figs. 10, 11, 12, 13	in	Moments in Time
validation becomes the ‘fitness’ used in our algorithm. We observed that	in	Moments in Time
and we used this setting in our evolutionary algorithm to reduce	in	Moments in Time
mutation rate. As we described in the main sec- tion of	in	Moments in Time
bi/rc where r is 100 in our experimental setting. That is	in	Moments in Time
addition to the experimental results in the main paper, we below	in	Moments in Time
here present diverse architectures evolved in the fol- lowing figures. The	in	Moments in Time
inception module is quite different in all three networks. In Figures	in	Moments in Time
used much more com- monly in both RGB and optical flow	in	Moments in Time
Chen, and Shuicheng Yan. Network in net- work. 2013. 3	in	Moments in Time
Carl Vondrick, et al. Moments in time dataset: one million videos	in	Moments in Time
Ryoo. Learn- ing latent sub-events in activity videos using temporal atten	in	Moments in Time
Laptev, and Abhinav Gupta. Hollywood in homes: Crowdsourcing data collection for	in	Moments in Time
volutional networks for action recognition in videos. In Ad	in	Moments in Time
vances in Neural Information Processing Systems (NIPS	in	Moments in Time
Dense detailed labeling of actions in complex videos. In- ternational Journal	in	Moments in Time
Torralba. Tem- poral relational reasoning in videos. arXiv preprint arXiv:1711.08496, 2017	in	Moments in Time
4.2 Datasets Moments in Time (MiT) Dataset The Moments in	Moments in Time	Moments in Time
action classification accu- racies on Moments in Time [16]. Method modality Top-1 Top-5	Moments in Time	Moments in Time
16GB per core. For the Moments in Time (MiT) dataset training, 8 videos	Moments in Time	Moments in Time
4.2 Datasets Moments in Time (MiT) Dataset The Moments in	Time	Moments in Time
Time (MiT) dataset [16] is a	Time	Moments in Time
accu- racies on Moments in Time [16]. Method modality Top-1 Top-5	Time	Moments in Time
core. For the Moments in Time (MiT) dataset training, 8 videos	Time	Moments in Time
4.2 Datasets Moments in Time (MiT) Dataset The	Moments	Moments in Time
Moments in Time (MiT) dataset [16	Moments	Moments in Time
action classification accu- racies on Moments in Time [16]. Method modality	Moments	Moments in Time
Gutfruend, C. Vondrick, et al. Moments in time dataset: one million	Moments	Moments in Time
16GB per core. For the Moments in Time (MiT) dataset training	Moments	Moments in Time
Searching for Multi-Stream Neural Connectivity in Video Architectures	in	Moments in Time
capture both appearance and motion in videos. We interpret a video	in	Moments in Time
combining representations that abstract different in	in	Moments in Time
approaches on public video datasets, in some cases by a great	in	Moments in Time
so for understanding semantic contents in videos such as activity recognition	in	Moments in Time
9, 7, 8, 6]. However, in most cases, combining appearance and	in	Moments in Time
between different streams) was done in a handcrafted way, e.g., by	in	Moments in Time
visual clues, a longstanding problem in video understanding. We propose a	in	Moments in Time
to address two main questions in video representation learning: (1) what	in	Moments in Time
convolutional layers to be repeated in	in	Moments in Time
the 1D temporal conv. layers in it	in	Moments in Time
weights to guide the evolution, in addition to randomly combining/splitting/connecting sub-network	in	Moments in Time
both spatial and temporal information in the video. Full 3D space-time	in	Moments in Time
studied replacing 2D convolutional layers in standard image-based CNNs such as	in	Moments in Time
has also been extremely popular in video CNNs [20, 7–9]. The	in	Moments in Time
capture both motion and appearance in videos. The idea is to	in	Moments in Time
image). On the other hand, in this work, our objective is	in	Moments in Time
understanding. We experimentally confirm that in our multi-stream video CNN case	in	Moments in Time
directed acyclic graph. Each node in the graph corresponds to a	in	Moments in Time
Nodes. A node in our graph representation is a	in	Moments in Time
sink nodes. A source node in the graph directly takes the	in	Moments in Time
The 1D conv. is omitted in optical flow stems. A sink	in	Moments in Time
pooling. More details are provided in Appendix	in	Moments in Time
Each node in the graph also has two	in	Moments in Time
of the temporal convolutional layers in different blocks, which are discussed	in	Moments in Time
the channels of all nodes in the same block level to	in	Moments in Time
with different temporal resolutions as in [6] or by using temporally	in	Moments in Time
standard 2D dilated convolutions used in [4] or [31	in	Moments in Time
different from the one used in [13], which designed a specific	in	Moments in Time
the standard dilated convolutions used in [4, 31	in	Moments in Time
node is computed as F in	in	Moments in Time
was high enough to survive in the population pool and eventually	in	Moments in Time
guide’ our architecture evolution algorithm in a preferable way, which we	in	Moments in Time
discuss more in Section 3.2	in	Moments in Time
inputs from different nodes differ in their spatial size, we add	in	Moments in Time
the representations is always consistent in our graphs, as there is	in	Moments in Time
no temporal striding in our formulation and the layers	in	Moments in Time
in the nodes are fully convolutional	in	Moments in Time
that modify nodes and edges in architectures over iterations. The algorithm	in	Moments in Time
making the number of channels in their convolutional layers half that	in	Moments in Time
parent). More details are found in Appendix. As a result, we	in	Moments in Time
randomly added which were not in the parent architecture. We enumerate	in	Moments in Time
the parent architecture as described in Section 3.2. 0∼4 random number	in	Moments in Time
4.2 Datasets Moments in Time (MiT) Dataset The Moments	in	Moments in Time
in Time (MiT) dataset [16] is	in	Moments in Time
dataset [19] which is unique in the activity recognition domain as	in	Moments in Time
classification accu- racies on Moments in Time [16]. Method modality Top-1	in	Moments in Time
activities. Activities may temporally overlap in a Charades video, requiring the	in	Moments in Time
is outperforming the prior works in both datasets, setting the new	in	Moments in Time
above 31% (with all previous in the tens and twenties). We	in	Moments in Time
ing. Four-stream architectures are reported in this paper for the first	in	Moments in Time
of the three top-performing models in each pool	in	Moments in Time
all outputs of the blocks in the 2nd last level. (3	in	Moments in Time
and L. Torresani. Connectivity learning in multi-branch networks. In Workshop on	in	Moments in Time
video action recognition. In Advances in Neural Information Processing Systems (NeurIPS	in	Moments in Time
analysis of selection schemes used in genetic algorithms. In Foundations of	in	Moments in Time
C. Vondrick, et al. Moments in time dataset: one million videos	in	Moments in Time
convolutional networks for action recognition in videos. In Advances in Neural	in	Moments in Time
feature learning: Speed- accuracy trade-offs in video classification. In Proceedings of	in	Moments in Time
A. Torralba. Temporal relational reasoning in videos. In Proceedings of European	in	Moments in Time
As we described in the main paper, each node	in	Moments in Time
filters of the convolutional layers in the block. When splitting or	in	Moments in Time
D4 = 512. The layers in the stems have 64 channels	in	Moments in Time
A.2 Hand-designed models used in ablation study	in	Moments in Time
hand-designed (2+1)D CNN models used in our ablation study. We also	in	Moments in Time
connectivity within them are learned in the process. We observe that	in	Moments in Time
description of the networks used in the main paper: “Two-stream (late	in	Moments in Time
baseline (2+1)D CNN models used in our ablation study	in	Moments in Time
always connected to every node in the immediate next level. “Flow->RGB	in	Moments in Time
provide the final AssembleNet model in table form in Table 4	in	Moments in Time
i.e., the connections). As mentioned in the main paper, 2D and	in	Moments in Time
2+1)D residual modules are repeated in each block. The number of	in	Moments in Time
max pooling layer. As suggested in the main paper, the model	in	Moments in Time
the number of video classes in the dataset. The sink node	in	Moments in Time
per core. For the Moments in Time (MiT) dataset training, 8	in	Moments in Time
we used is 12.5 fps in both datasets. The spatial input	in	Moments in Time
used the standard Momentum Optimizer in TensorFlow with the learning rate	in	Moments in Time
A.2 Hand-designed models used in ablation study	in	Moments in Time
The paucity of videos in current action classification datasets (UCF-101	in	Moments in Time
paper re-evaluates state-of-the-art architec- tures in light of the new Kinetics	in	Moments in Time
considerably improve upon the state-of-the-art in action classification, reaching 80.9% on	in	Moments in Time
used for other tasks and in other domains. One of the	in	Moments in Time
challenge [10, 23]. Furthermore, improvements in the deep architecture, changing from	in	Moments in Time
fed through to commensurate improvements in the PASCAL VOC performance [25	in	Moments in Time
going? Actions can be ambiguous in individual frames, but the limitations	in	Moments in Time
will give a similar boost in performance when ap- plied to	in	Moments in Time
there is always a boost in performance by pre-training, but the	in	Moments in Time
tures has matured quickly in recent years, there is still	in	Moments in Time
of the ma- jor differences in current video architectures are whether	in	Moments in Time
pre-computed opti- cal flow; and, in the case of 2D ConvNets	in	Moments in Time
architectures we evaluate is shown in figure 2 and the specification	in	Moments in Time
their temporal interfaces is given in table 1	in	Moments in Time
normalization [13], and morph it in different ways. The expectation is	in	Moments in Time
that with this back bone in common, we will be able	in	Moments in Time
whole video [15]. This is in the spirit of bag of	in	Moments in Time
22, 33]; but while convenient in practice, it has the issue	in	Moments in Time
of all models is given in table 1	in	Moments in Time
Figure 2. Video architectures considered in this paper. K stands for	in	Moments in Time
the total number of frames in a video, whereas N stands	in	Moments in Time
112 × 112-pixel crops as in the original implementation. Differently from	in	Moments in Time
to the original model is in the first pooling layer, we	in	Moments in Time
a temporal stride of 2 in	in	Moments in Time
low-level motion which is critical in many cases. It is also	in	Moments in Time
paper approximately using Inception-V1. The in	in	Moments in Time
including the two-stream averaging process in the original model	in	Moments in Time
it will be shown in section 4	in	Moments in Time
been developed over the years, in part through painstaking trial and	in	Moments in Time
for boring videos are constant in time, the outputs of pointwise	in	Moments in Time
Pacing receptive field growth in space, time and net- work	in	Moments in Time
and means that features deeper in the networks are equally affected	in	Moments in Time
image locations increasingly far away in both dimensions. A symmetric receptive	in	Moments in Time
If it grows too quickly in time relative to space, it	in	Moments in Time
layer, besides the max-pooling layers in parallel Inception branches. In our	in	Moments in Time
to not perform temporal pooling in the first two max- pooling	in	Moments in Time
3 kernels and stride 1 in time), while having symmetric kernels	in	Moments in Time
and strides in all other max-pooling layers. The	in	Moments in Time
The overall architecture is shown in fig. 3. We train the	in	Moments in Time
whereas optical flow algorithms are in some sense recurrent (e.g. they	in	Moments in Time
a two-stream configuration – shown in fig. 2, e) – with	in	Moments in Time
momen- tum set to 0.9 in all cases, with synchronous paralleliza	in	Moments in Time
All the models were implemented in Tensor- Flow [1	in	Moments in Time
sizes for a few layers in the network are provided in	in	Moments in Time
The predictions are obtained convolutionally in time and averaged	in	Moments in Time
it was built is given in [16	in	Moments in Time
chitectures described in section 2 whilst varying the	in	Moments in Time
new I3D models do best in all datasets, with either RGB	in	Moments in Time
and without ImageNet pretraining. Numbers in brackets () are the Top-5	in	Moments in Time
on HMDB-51; this may be in part due to lack of	in	Moments in Time
training data in HMDB-51 but also because this	in	Moments in Time
many clips have different actions in the ex- act same scene	in	Moments in Time
substantially lower on Kinetics. Visual in	in	Moments in Time
discern actions from flow alone in Kinetics, and this was rarely	in	Moments in Time
the value of training models in Kinet- ics starting from ImageNet-pretrained	in	Moments in Time
the results are shown in table 3. It can be	in	Moments in Time
that ImageNet pre-training still helps in all cases and this is	in	Moments in Time
flow network filters, the one in the middle shows filters from	in	Moments in Time
Inception-v1 filters, while the filters in the RGB I3D network are	in	Moments in Time
Best seen on the computer, in colour and zoomed in	in	Moments in Time
The results are given in table 4. The clear outcome	in	Moments in Time
of the models after pretraining in Ki- netics (Fixed) also leads	in	Moments in Time
as much from the images in ImageNet. The difference over the	in	Moments in Time
els and previous state-of-the-art methods in table 5, on UCF-101 and	in	Moments in Time
the trained models are shown in fig. 4	in	Moments in Time
return to the question posed in the introduction, “is there a	in	Moments in Time
benefit in transfer learning from videos?”. It	in	Moments in Time
there is a considerable benefit in pre-training on	in	Moments in Time
there has been such benefits in pre-training ConvNets on ImageNet for	in	Moments in Time
if there is a benefit in using Kinetics pre-training for other	in	Moments in Time
dataset’s release to facilitate research in this area	in	Moments in Time
mechanisms [20] to fo- cus in on the human actors. Recent	in	Moments in Time
by incorporating linked object detections in	in	Moments in Time
gone out of the box in attempts to capture this relationship	in	Moments in Time
to also include these models in our comparison but we could	in	Moments in Time
on the Kinetics project and in particular Brian Zhang and Tim	in	Moments in Time
Hu- man focused action localization in video. In International Workshop on	in	Moments in Time
region proposal networks. In Advances in neural information processing systems, pages	in	Moments in Time
in videos. British Machine Vision Conference	in	Moments in Time
convolutional networks for action recognition in videos. In Advances in Neural	in	Moments in Time
human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402	in	Moments in Time
train deep neural networks [25]. Time contrast networks are used for	Time	Moments in Time
Time	Time	Moments in Time
C., Hsu, J., Levine, S.: Time	Time	Moments in Time
Dataset UCF Kinetics Moments Something Jester Charades	Moments	Moments in Time
Temporal Relational Reasoning in Videos	in	Moments in Time
to dis- cover temporal relations in videos. Through only sparsely sampled	in	Moments in Time
can accurately predict human-object interactions in the Something-Something dataset and identify	in	Moments in Time
networks and 3D convolution networks in recognizing daily activities in the	in	Moments in Time
interpretable visual common sense knowledge in videos1	in	Moments in Time
Activity recognition in videos has been one of	in	Moments in Time
the core topics in computer vision. However, it remains	in	Moments in Time
convolutional neural networks still struggle in situations where data and observations	in	Moments in Time
beyond the appearance of objects in the frames and the optical	in	Moments in Time
that enables temporal relational reasoning in neural networks. This module is	in	Moments in Time
by the relational network proposed in [7], but instead of modeling	in	Moments in Time
the temporal relations between observations in videos. Thus, TRN can learn	in	Moments in Time
module that can be used in a plug-and-play fashion with any	in	Moments in Time
Temporal Relational Reasoning in Videos 3	in	Moments in Time
Activity Recognition. Activity recog- nition in videos is a core problem	in	Moments in Time
in computer vision. With the rise	in	Moments in Time
also used to recognize activities in videos [14]. Recently, I3D networks	in	Moments in Time
computationally expensive, given the redundancy in consecutive frames; 3) Since sequences	in	Moments in Time
Temporal Information in Activity Recognition. For activity recogni	in	Moments in Time
For recognizing the complex activities in these three datasets, it is	in	Moments in Time
to learn the temporal relations in end-to-end training. One relevant work	in	Moments in Time
on modeling the cause-effect in videos is [23]. [23] uses	in	Moments in Time
the multiple frames information, both in training and testing	in	Moments in Time
modeling the multi-scale temporal relations in videos. In the domain of	in	Moments in Time
to learn various temporal relations in videos in a supervised learning	in	Moments in Time
Temporal Relational Reasoning in Videos 5	in	Moments in Time
sense knowledge to recognize activities in videos	in	Moments in Time
overall network framework is illustrated in Fig.2	in	Moments in Time
networks is able to run in real-time on a desktop to	in	Moments in Time
competitive results on activity classification in the Charades dataset [11], outperforming	in	Moments in Time
2nd release of the dataset in early July 2018) [9,28], Jester	in	Moments in Time
Charades dataset [11] are listed in Table 1. All three datasets	in	Moments in Time
are crowd-sourced, in which the videos are collected	in	Moments in Time
activities. Unlike the Youtube-type videos in UCF101 and Kinetics, there is	in	Moments in Time
and end of each activity in the crowd-sourced video, emphasizing the	in	Moments in Time
Statistics of the datasets used in evaluating the TRNs	in	Moments in Time
features play an important factor in visual recognition tasks [29]. Features	in	Moments in Time
module for temporal relational reasoning in videos. Thus, we fix the	in	Moments in Time
Temporal Relational Reasoning in Videos 7	in	Moments in Time
BN-Inception) pretrained on ImageNet used in [31] because of its balance	in	Moments in Time
after global pooling as used in [16]. We keep the network	in	Moments in Time
We set k = 3 in the experiments as the number	in	Moments in Time
of accumulated relation triples in each relation module. gφ is	in	Moments in Time
the training can be finished in less than 24 hours for	in	Moments in Time
TRN (thus N = 8 in Eq.3), as including higher frame	in	Moments in Time
the objects characterize the activities in the dataset	in	Moments in Time
Something- V2 datasets are listed in Table 2a. The baseline is	in	Moments in Time
average pooling of frames used in TSN [16] achieves better score	in	Moments in Time
set, the results are shown in Table 2.a	in	Moments in Time
the temporal ordering of patterns in the features. We keep all	in	Moments in Time
As shown in Table 2b, our models outperform	in	Moments in Time
see that additional frames included in the relation bring further significant	in	Moments in Time
the Something-V1. TRN outperforms TSN in a large margin as the	in	Moments in Time
the Jester dataset are listed in Table 3a. The result on	in	Moments in Time
the top methods are listed in Table 3b. MultiScale TRN again	in	Moments in Time
recognition. The results are listed in Table 4. Our method outperforms	in	Moments in Time
the three datasets are shown in Figure 3. The examples in	in	Moments in Time
Temporal Relational Reasoning in Videos 9	in	Moments in Time
shown in reverse. Moreover, the successful prediction	in	Moments in Time
of categories in which an individual pretends to	in	Moments in Time
something into something’ as shown in the second row) suggests that	in	Moments in Time
of several lower-level actions contained in short segments conveys crucial semantic	in	Moments in Time
in	in	Moments in Time
in	in	Moments in Time
section, we have a more in	in	Moments in Time
TRNs to recognize an activity in the same video. We can	in	Moments in Time
some de- gree of confidence in the correct action, but is	in	Moments in Time
the network needs additional frames in the TRNs to correctly recognize	in	Moments in Time
example is the last video in Figure 4: The action’s context	in	Moments in Time
motion of the individuals hand in the third frame of the	in	Moments in Time
the object, thus, solidifying confidence in the correct class prediction	in	Moments in Time
in	in	Moments in Time
in	in	Moments in Time
in	in	Moments in Time
in	in	Moments in Time
in	in	Moments in Time
Temporal Relational Reasoning in Videos 11	in	Moments in Time
Zooming in with full hand Pushing hand	in	Moments in Time
the scenario with input frames in temporal order and in shuffled	in	Moments in Time
training the TRNs, as shown in Figure 6a. For training the	in	Moments in Time
we randomly shuffle the frames in the relation modules. The significant	in	Moments in Time
importance of the temporal order in the activity recognition. More interestingly	in	Moments in Time
recognition for the Youtube-type videos in UCF101 doesn’t necessarily require the	in	Moments in Time
ordering influences activity recogni- tion in TRN, we examine and plot	in	Moments in Time
show the largest dif- ferences in the class accuracy between ordered	in	Moments in Time
drawn from the Something-Something dataset, in Figure 6b. In general, actions	in	Moments in Time
severe if penalizing at all in some cases, with several categories	in	Moments in Time
make the correct prediction. Particularly in challenging ambiguous cases, for example	in	Moments in Time
rise to a curious difference in accuracy for that action	in	Moments in Time
activity. But rec- ognizing activities in UCF101 does not necessarily require	in	Moments in Time
evaluate the two pool strategies in detail as shown in Table	in	Moments in Time
5. The difference in the performance using average pool	in	Moments in Time
the importance of temporal orders in a video dataset. The tested	in	Moments in Time
Temporal Relational Reasoning in Videos 13	in	Moments in Time
be due to that activity in the randomly trimmed Youtube videos	in	Moments in Time
TRN can better differentiate activities in Something-Something dataset	in	Moments in Time
15 most frequent activity classes in the validation set. We can	in	Moments in Time
observe the similarity among categories in the visualization map. For example	in	Moments in Time
by the first frame shown in the left column, is used	in	Moments in Time
forecasts and corresponding probabilities listed in the middle column. The ground	in	Moments in Time
lenging yet less explored problem in activity recognition. Here we evaluate	in	Moments in Time
and 50% of the frames in each validation video. Results are	in	Moments in Time
shown in Table 6. For comparison, we	in	Moments in Time
to enable temporal relational reasoning in neural networks for videos. We	in	Moments in Time
discover visual common sense knowledge in videos	in	Moments in Time
A.O.. It is also supported in part by the Intelligence Advanced	in	Moments in Time
Temporal Relational Reasoning in Videos 15	in	Moments in Time
for un- derstanding human actions in videos? arXiv preprint arXiv:1708.02696 (2017	in	Moments in Time
human actions classes from videos in the wild. Proc. CVPR (2012	in	Moments in Time
networks for action recog- nition in videos. In: In Advances in	in	Moments in Time
I., Gupta, A.: Hol- lywood in homes: Crowdsourcing data collection for	in	Moments in Time
volutional neural networks. In: Advances in neural information processing systems. (2012	in	Moments in Time
using places database. In: Advances in neural information processing systems. (2014	in	Moments in Time
of intuitive physics. In: Advances in Neural Information Processing Systems. (2016	in	Moments in Time
Temporal Relational Reasoning in Videos	in	Moments in Time
Segment Networks for Action Recognition in Videos	in	Moments in Time
recognition. However, for action recognition in videos, their advantage over traditional	in	Moments in Time
framework for learning action models in videos. This method, called temporal	in	Moments in Time
easily adapted for action recognition in both trimmed and untrimmed videos	in	Moments in Time
5], owing to its applications in many areas like security and	in	Moments in Time
behavior analysis. For action recognition in videos, there are two crucial	in	Moments in Time
6] have achieved great success in classifying images of objects [7	in	Moments in Time
representations from raw visual data in large- scale supervised datasets (e.g	in	Moments in Time
of ConvNets to action recognition in unconstrained videos is impeded by	in	Moments in Time
for understanding the dy- namics in traditional methods [19], [20], [21	in	Moments in Time
considered as a critical factor in deep ConvNet frameworks [1], [15	in	Moments in Time
deploy the learned action models in a realistic setting we often	in	Moments in Time
action models to action recognition in untrimmed videos	in	Moments in Time
remain limited in both size and diversity, making	in	Moments in Time
the action recogni- tion problem in this paper from the following	in	Moments in Time
framework for learning action models in videos. It is based on	in	Moments in Time
be more favorable and efficient in this case. The TSN framework	in	Moments in Time
structures over the whole video, in a way that its computational	in	Moments in Time
To tackle the practical difficulties in learning and apply	in	Moments in Time
to perform Batch Normalization (BN) in a fine-tuning scenario, denoted as	in	Moments in Time
which has numerous potential applications in real-world problems	in	Moments in Time
our method for action recognition in both trimmed and untrimmed videos	in	Moments in Time
method secures the 1st place in untrimmed video classification at the	in	Moments in Time
different aspects of the problems in efficiently and effectively learning and	in	Moments in Time
extends our previous work [31] in a number of aspects. First	in	Moments in Time
Challenge 2016, which ranks #1 in untrimmed video classification among 24	in	Moments in Time
recognition has been studied extensively in recent years and readers can	in	Moments in Time
For action recognition in videos, the visual representation plays	in	Moments in Time
a set of mid-level patches in a strongly-supervised manner. Similar to	in	Moments in Time
ConvNet architectures for action recognition in videos [1], [4], [5], [15	in	Moments in Time
use recurrent neural networks (RNN), in particular LSTM, to model the	in	Moments in Time
frame features for action recognition in videos	in	Moments in Time
to learn the model parameters in an iterative approach. Wang et	in	Moments in Time
As discussed in Sec. 1, long-range temporal modeling	in	Moments in Time
is important for action understanding in videos. The existing deep architectures	in	Moments in Time
1], [16], it still suffers in both computational and modeling aspects	in	Moments in Time
it totally samples 64 frames in the work of [23] and	in	Moments in Time
the frames are densely recorded in the videos, the content changes	in	Moments in Time
to model the temporal structures in a human action. Normally, the	in	Moments in Time
a video-level framework as shown in Figure 1, which would be	in	Moments in Time
explained in the next subsection	in	Moments in Time
its corresponding segment. Each snippet in this sequence produces its own	in	Moments in Time
details on these consensus functions in the next subsection	in	Moments in Time
K is number of segments in temporal segment net- work. When	in	Moments in Time
function is an important component in our temporal segment net- work	in	Moments in Time
video. On the other hand, in particular for noisy videos with	in	Moments in Time
phases may play different roles in recogniz- ing action classes. This	in	Moments in Time
the basic back- propagation formula in Eq. 3 should be rectified	in	Moments in Time
3.4 TSN in Practice	in	Moments in Time
other ConvNet architectures de- ployed in videos [1], [16], this architecture	in	Moments in Time
the potential of TSN framework in video classification	in	Moments in Time
work, we extend this approach in two aspects, namely accuracy and	in	Moments in Time
speed. As shown in Figure 2, in addition to	in	Moments in Time
help to improve the accuracy in motion perception and thus boost	in	Moments in Time
16] and motion vector [17] in motion modeling, we revisit the	in	Moments in Time
work on dense optical flow in [60], the partial derivatives of	in	Moments in Time
to time play critical roles in computing optical flow. It is	in	Moments in Time
the power of optical flow in representing motion could be learned	in	Moments in Time
for action recognition are limited in terms of sizes. In practice	in	Moments in Time
strategies to improve the training in the temporal segment network framework	in	Moments in Time
networks take RGB images as in	in	Moments in Time
models across the RGB channels in the first layer and replicate	in	Moments in Time
increases the risk of over-fitting in the transfer learning process, due	in	Moments in Time
number of training sam- ples in target dataset. Therefore, after initialization	in	Moments in Time
dropout ratio (set as 0.8 in experiment) after the global pooling	in	Moments in Time
scale jittering technique [8] used in ImageNet classification to action recognition	in	Moments in Time
this framework to recognize actions in realistic videos. In this section	in	Moments in Time
devise a series of techniques in order to improve the robustness	in	Moments in Time
4.1 Action Recognition in Trimmed Video	in	Moments in Time
share the model pa- rameters in temporal segment networks, the learned	in	Moments in Time
determined empirically. It is described in Sec. 3.2 that the segmental	in	Moments in Time
normalization. To test the models in compliance with their training, we	in	Moments in Time
4.2 Action Recognition in Untrimmed Videos	in	Moments in Time
major obstacle for action recognition in untrimmed videos is the large	in	Moments in Time
portion of irrelevant content in the input videos. Since our	in	Moments in Time
averaging scores from every location in a video, has a high	in	Moments in Time
risk of factoring in the unpredictable responses of the	in	Moments in Time
Background issue: the irrelevant content in a video can have high	in	Moments in Time
snippets from the input videos in a fixed sampling rate (e.g	in	Moments in Time
on these sampled snippets. Then, in order to cover the highly	in	Moments in Time
Formally, for a video in length of M seconds, we	in	Moments in Time
the results of our approach in the ActivityNet challenge 2016 and	in	Moments in Time
not specifically noted, the experiments in the section are conducted with	in	Moments in Time
set a smaller learning rate in our experiments. On the dataset	in	Moments in Time
and scale jittering, as specified in Section 3.4. For the extraction	in	Moments in Time
of the good practices described in Sec. 3.4, including the train	in	Moments in Time
propose two new training strategies in Section 3.4, namely cross modality	in	Moments in Time
only pre-train spatial stream as in [1]; (3) with cross modality	in	Moments in Time
dropout. The results are summarized in Table 1. First, we see	in	Moments in Time
nition performance to 92.0%. Therefore, in the remaining experiments, we employ	in	Moments in Time
two new types of modalities in Section 3.4: RGB difference and	in	Moments in Time
modalities and report the results in Table 2. These experiments are	in	Moments in Time
all the good practices verified in Table 1	in	Moments in Time
which is the basic combination in the two-stream ConvNets also works	in	Moments in Time
action recognition methods as shown in Table 2. This suggests that	in	Moments in Time
segment network framework. As described in Sec 3, the framework has	in	Moments in Time
the temporal segment network framework in	in	Moments in Time
flow fields for input modalities in this exploration. Finally, to demonstrate	in	Moments in Time
the importance of TSN in long-range modeling	in	Moments in Time
of different segment numbers K in temporal segment networks on the	in	Moments in Time
use the average consensus function in these experiments. Dataset UCF101 ActivityNet	in	Moments in Time
segment number K as 7 in these experiments. Dataset UCF101 ActivityNet	in	Moments in Time
the sparse snippet sampling scheme in TSN is the number of	in	Moments in Time
approaches. The results are summarized in Table 3. We observe that	in	Moments in Time
we set K = 7 in the following experiments	in	Moments in Time
The experimental results are summarized in Table 4. On UCF101, which	in	Moments in Time
pooling for complex videos (ActivityNet) in later experiments	in	Moments in Time
and the results are summarized in Table 5. We use K	in	Moments in Time
1 in these experiments, which is equivalent	in	Moments in Time
TABLE 7 Winning entries in the untrimmed video classification task	in	Moments in Time
We present the recognition accuracies in the form of mAP values	in	Moments in Time
values in the challenge. Team mAP Top1	in	Moments in Time
the effect of the components in temporal segment networks and coming	in	Moments in Time
UCF101. The results are summarized in the left columns of Table	in	Moments in Time
v1.2. The results are summarized in the right columns of Table	in	Moments in Time
7 and the aggregation function in TSN is top-K pooling. Our	in	Moments in Time
with TSN also perform well in untrimmed videos, given a reasonable	in	Moments in Time
testing scheme, as described in Sec. 4.2	in	Moments in Time
network framework is further verified in the ActivityNet large scale activity	in	Moments in Time
we follow the approach described in Sec. 4.2. Understanding that the	in	Moments in Time
architecture plays an important role in boosting the performance, we also	in	Moments in Time
and test the recognition accuracy in terms of mean average precision	in	Moments in Time
on validation set are summarized in Table 6. We observe that	in	Moments in Time
the testing set are summarized in Table 7. Out entry “CES	in	Moments in Time
other participants of this challenge in Table 7. It is worth	in	Moments in Time
efficiency of TSN, our models in the challenge can be trained	in	Moments in Time
time visualize interesting class information in action recognition ConvNet models. We	in	Moments in Time
left) and y (right) directions in gray-scales. Note all these images	in	Moments in Time
purely random pixels. Left: classes in UCF101. Right: classes in ActivityNet	in	Moments in Time
ing. The results are shown in Fig. 3. For both RGB	in	Moments in Time
the scenery patterns and objects in the videos as significant evidences	in	Moments in Time
for action recognition. For example, in the class “Diving”, the single-frame	in	Moments in Time
models focus more on humans in the videos, and seem to	in	Moments in Time
different poses can be identified in the image, depicting various stages	in	Moments in Time
Similar observation would be identified in other action classes such as	in	Moments in Time
better, which is well reflected in our quantitative experiments	in	Moments in Time
good practices that we explored in this work. The former provides	in	Moments in Time
works for action recognition in videos,” in NIPS, 2014, pp	in	Moments in Time
trajectories,” in ICCV, 2013, pp. 3551–3558. [3	in	Moments in Time
human motion recognition,” in CVPR, 2013, pp. 2674–2681. [4	in	Moments in Time
net- works for video classification,” in CVPR, 2015, pp. 4694–4702	in	Moments in Time
with trajectory- pooled deep-convolutional descriptors,” in CVPR, 2015, pp. 4305– 4314	in	Moments in Time
with deep convolutional neural networks,” in NIPS, 2012, pp. 1106–1114	in	Moments in Time
works for large-scale image recognition,” in ICLR, 2015, pp. 1–14	in	Moments in Time
Rabinovich, “Going deeper with convolutions,” in CVPR, 2015, pp. 1–9	in	Moments in Time
scene recognition using places database,” in NIPS, 2014, pp. 487–495	in	Moments in Time
of deep convolutional neural networks,” in ECCV, 2016, pp. 467–482	in	Moments in Time
images by fusing deep channels,” in CVPR, 2015, pp. 1600–1609	in	Moments in Time
neural networks for event recognition in still images,” CoRR, vol. abs/1609.00162	in	Moments in Time
classification with convolutional neural networks,” in CVPR, 2014, pp. 1725–1732	in	Moments in Time
with 3d convolutional net- works,” in ICCV, 2015, pp. 4489–4497	in	Moments in Time
with enhanced motion vector CNNs,” in CVPR, 2016, pp. 2718–2726	in	Moments in Time
A large-scale hierarchical image database,” in CVPR, 2009, pp. 248– 255	in	Moments in Time
motion segments for activity classification,” in ECCV, 2010, pp. 392–405	in	Moments in Time
video evolution for action recognition,” in CVPR, 2015, pp. 5378–5387	in	Moments in Time
for visual recognition and description,” in CVPR, 2015, pp. 2625–2634	in	Moments in Time
action recog- nition for videos in the wild,” Computer Vision and	in	Moments in Time
for human activity under- standing,” in CVPR, 2015, pp. 961–970	in	Moments in Time
human actions classes from videos in the wild,” CoRR, vol. abs/1212.0402	in	Moments in Time
database for human motion recognition,” in ICCV, 2011, pp. 2556–2563	in	Moments in Time
residual learning for image recognition,” in CVPR, 2016, pp. 770–778	in	Moments in Time
inception architecture for computer vision,” in CVPR, 2016, pp. 2818–2826	in	Moments in Time
practices for deep action recognition,” in ECCV, 2016, pp. 20–36	in	Moments in Time
motion synthesis,” Foundations and Trends in Computer Graphics and Vision, vol	in	Moments in Time
scale-invariant spatio-temporal interest point detector,” in ECCV, 2008, pp. 650–663	in	Moments in Time
nition via sparse spatio-temporal features,” in IEEE International Workshop on PETS	in	Moments in Time
Action recognition by dense trajectories,” in CVPR, 2011, pp. 3169–3176	in	Moments in Time
realistic human actions from movies,” in CVPR, 2008, pp. 1–8	in	Moments in Time
spatio-temporal descriptor based on 3D-gradients,” in BMVC, 2008, pp. 1–12	in	Moments in Time
categorization with bags of keypoints,” in ECCV Workshop on statistical learning	in	Moments in Time
in computer vision, 2004, pp. 1–22	in	Moments in Time
super vector for action recognition,” in CVPR, 2014, pp. 596–603	in	Moments in Time
parts from mid-level video representations,” in CVPR, 2012, pp. 1242–1249	in	Moments in Time
videos using mid-level discriminative patches,” in CVPR, 2013, pp. 2571–2578	in	Moments in Time
for detailed action under- standing,” in ICCV, 2013, pp. 2248–2255	in	Moments in Time
Tu, “Action recognition with actons,” in ICCV, 2013, pp. 3559–3566	in	Moments in Time
high-level represen- tation of activity in video,” in CVPR, 2012, pp	in	Moments in Time
using 3d human pose annotations,” in ICCV, 2009, pp. 1365–1372	in	Moments in Time
using factorized spatio-temporal convolutional networks,” in ICCV, 2015, pp. 4597–4605	in	Moments in Time
Xue, “Modeling spatial- temporal clues in a hybrid deep learning framework	in	Moments in Time
for video classification,” in ACM Multimedia, 2015, pp. 461–470	in	Moments in Time
fusion for video action recognition,” in CVPR, 2016, pp. 1933–1941	in	Moments in Time
of actions with segmental grammars,” in CVPR, 2014, pp. 612–619	in	Moments in Time
action detection with relational dynamic-poselets,” in ECCV, 2014, pp. 565–580	in	Moments in Time
by reducing internal covariate shift,” in ICML, 2015, pp. 448–456	in	Moments in Time
for realtime tv-L1 optical flow,” in 29th DAGM Symposium on Pattern	in	Moments in Time
Zisserman, “Return of the devil in the details: Delving deep into	in	Moments in Time
convolutional nets,” in BMVC, 2014	in	Moments in Time
networks for video action recognition,” in NIPS, 2016, pp. 3468–3476	in	Moments in Time
for the thu- mos workshop,” in ICCV Workshop on THUMOS Challenge	in	Moments in Time
about classifying and localizing actions?” in CVPR, 2015, pp. 46–55	in	Moments in Time
depth for large-scale action recognition,” in ECCV. Springer, 2016, pp. 668–684	in	Moments in Time
recognition via trajectory group selection,” in CVPR, 2015, pp. 3698–3706	in	Moments in Time
deep framework for action recognition,” in CVPR, 2016, pp. 1991– 1999	in	Moments in Time
3.4 TSN in Practice	in	Moments in Time
4.1 Action Recognition in Trimmed Video	in	Moments in Time
4.2 Action Recognition in Untrimmed Videos	in	Moments in Time
Segment Networks for Action Recognition in Videos	in	Moments in Time
recognition. However, for action recognition in videos, their advantage over traditional	in	Moments in Time
framework for learning action models in videos. This method, called temporal	in	Moments in Time
easily adapted for action recognition in both trimmed and untrimmed videos	in	Moments in Time
5], owing to its applications in many areas like security and	in	Moments in Time
behavior analysis. For action recognition in videos, there are two crucial	in	Moments in Time
6] have achieved great success in classifying images of objects [7	in	Moments in Time
representations from raw visual data in large- scale supervised datasets (e.g	in	Moments in Time
of ConvNets to action recognition in unconstrained videos is impeded by	in	Moments in Time
for understanding the dy- namics in traditional methods [19], [20], [21	in	Moments in Time
considered as a critical factor in deep ConvNet frameworks [1], [15	in	Moments in Time
deploy the learned action models in a realistic setting we often	in	Moments in Time
action models to action recognition in untrimmed videos	in	Moments in Time
remain limited in both size and diversity, making	in	Moments in Time
the action recogni- tion problem in this paper from the following	in	Moments in Time
framework for learning action models in videos. It is based on	in	Moments in Time
be more favorable and efficient in this case. The TSN framework	in	Moments in Time
structures over the whole video, in a way that its computational	in	Moments in Time
To tackle the practical difficulties in learning and apply	in	Moments in Time
to perform Batch Normalization (BN) in a fine-tuning scenario, denoted as	in	Moments in Time
which has numerous potential applications in real-world problems	in	Moments in Time
our method for action recognition in both trimmed and untrimmed videos	in	Moments in Time
method secures the 1st place in untrimmed video classification at the	in	Moments in Time
different aspects of the problems in efficiently and effectively learning and	in	Moments in Time
extends our previous work [31] in a number of aspects. First	in	Moments in Time
Challenge 2016, which ranks #1 in untrimmed video classification among 24	in	Moments in Time
recognition has been studied extensively in recent years and readers can	in	Moments in Time
For action recognition in videos, the visual representation plays	in	Moments in Time
a set of mid-level patches in a strongly-supervised manner. Similar to	in	Moments in Time
ConvNet architectures for action recognition in videos [1], [4], [5], [15	in	Moments in Time
use recurrent neural networks (RNN), in particular LSTM, to model the	in	Moments in Time
frame features for action recognition in videos	in	Moments in Time
to learn the model parameters in an iterative approach. Wang et	in	Moments in Time
As discussed in Sec. 1, long-range temporal modeling	in	Moments in Time
is important for action understanding in videos. The existing deep architectures	in	Moments in Time
1], [16], it still suffers in both computational and modeling aspects	in	Moments in Time
it totally samples 64 frames in the work of [23] and	in	Moments in Time
the frames are densely recorded in the videos, the content changes	in	Moments in Time
to model the temporal structures in a human action. Normally, the	in	Moments in Time
a video-level framework as shown in Figure 1, which would be	in	Moments in Time
explained in the next subsection	in	Moments in Time
its corresponding segment. Each snippet in this sequence produces its own	in	Moments in Time
details on these consensus functions in the next subsection	in	Moments in Time
K is number of segments in temporal segment net- work. When	in	Moments in Time
function is an important component in our temporal segment net- work	in	Moments in Time
video. On the other hand, in particular for noisy videos with	in	Moments in Time
phases may play different roles in recogniz- ing action classes. This	in	Moments in Time
the basic back- propagation formula in Eq. 3 should be rectified	in	Moments in Time
3.4 TSN in Practice	in	Moments in Time
other ConvNet architectures de- ployed in videos [1], [16], this architecture	in	Moments in Time
the potential of TSN framework in video classification	in	Moments in Time
work, we extend this approach in two aspects, namely accuracy and	in	Moments in Time
speed. As shown in Figure 2, in addition to	in	Moments in Time
help to improve the accuracy in motion perception and thus boost	in	Moments in Time
16] and motion vector [17] in motion modeling, we revisit the	in	Moments in Time
work on dense optical flow in [60], the partial derivatives of	in	Moments in Time
to time play critical roles in computing optical flow. It is	in	Moments in Time
the power of optical flow in representing motion could be learned	in	Moments in Time
for action recognition are limited in terms of sizes. In practice	in	Moments in Time
strategies to improve the training in the temporal segment network framework	in	Moments in Time
networks take RGB images as in	in	Moments in Time
models across the RGB channels in the first layer and replicate	in	Moments in Time
increases the risk of over-fitting in the transfer learning process, due	in	Moments in Time
number of training sam- ples in target dataset. Therefore, after initialization	in	Moments in Time
dropout ratio (set as 0.8 in experiment) after the global pooling	in	Moments in Time
scale jittering technique [8] used in ImageNet classification to action recognition	in	Moments in Time
this framework to recognize actions in realistic videos. In this section	in	Moments in Time
devise a series of techniques in order to improve the robustness	in	Moments in Time
4.1 Action Recognition in Trimmed Video	in	Moments in Time
share the model pa- rameters in temporal segment networks, the learned	in	Moments in Time
determined empirically. It is described in Sec. 3.2 that the segmental	in	Moments in Time
normalization. To test the models in compliance with their training, we	in	Moments in Time
4.2 Action Recognition in Untrimmed Videos	in	Moments in Time
major obstacle for action recognition in untrimmed videos is the large	in	Moments in Time
portion of irrelevant content in the input videos. Since our	in	Moments in Time
averaging scores from every location in a video, has a high	in	Moments in Time
risk of factoring in the unpredictable responses of the	in	Moments in Time
Background issue: the irrelevant content in a video can have high	in	Moments in Time
snippets from the input videos in a fixed sampling rate (e.g	in	Moments in Time
on these sampled snippets. Then, in order to cover the highly	in	Moments in Time
Formally, for a video in length of M seconds, we	in	Moments in Time
the results of our approach in the ActivityNet challenge 2016 and	in	Moments in Time
not specifically noted, the experiments in the section are conducted with	in	Moments in Time
set a smaller learning rate in our experiments. On the dataset	in	Moments in Time
and scale jittering, as specified in Section 3.4. For the extraction	in	Moments in Time
of the good practices described in Sec. 3.4, including the train	in	Moments in Time
propose two new training strategies in Section 3.4, namely cross modality	in	Moments in Time
only pre-train spatial stream as in [1]; (3) with cross modality	in	Moments in Time
dropout. The results are summarized in Table 1. First, we see	in	Moments in Time
nition performance to 92.0%. Therefore, in the remaining experiments, we employ	in	Moments in Time
two new types of modalities in Section 3.4: RGB difference and	in	Moments in Time
modalities and report the results in Table 2. These experiments are	in	Moments in Time
all the good practices verified in Table 1	in	Moments in Time
which is the basic combination in the two-stream ConvNets also works	in	Moments in Time
action recognition methods as shown in Table 2. This suggests that	in	Moments in Time
segment network framework. As described in Sec 3, the framework has	in	Moments in Time
the temporal segment network framework in	in	Moments in Time
flow fields for input modalities in this exploration. Finally, to demonstrate	in	Moments in Time
the importance of TSN in long-range modeling	in	Moments in Time
of different segment numbers K in temporal segment networks on the	in	Moments in Time
use the average consensus function in these experiments. Dataset UCF101 ActivityNet	in	Moments in Time
segment number K as 7 in these experiments. Dataset UCF101 ActivityNet	in	Moments in Time
the sparse snippet sampling scheme in TSN is the number of	in	Moments in Time
approaches. The results are summarized in Table 3. We observe that	in	Moments in Time
we set K = 7 in the following experiments	in	Moments in Time
The experimental results are summarized in Table 4. On UCF101, which	in	Moments in Time
pooling for complex videos (ActivityNet) in later experiments	in	Moments in Time
and the results are summarized in Table 5. We use K	in	Moments in Time
1 in these experiments, which is equivalent	in	Moments in Time
TABLE 7 Winning entries in the untrimmed video classification task	in	Moments in Time
We present the recognition accuracies in the form of mAP values	in	Moments in Time
values in the challenge. Team mAP Top1	in	Moments in Time
the effect of the components in temporal segment networks and coming	in	Moments in Time
UCF101. The results are summarized in the left columns of Table	in	Moments in Time
v1.2. The results are summarized in the right columns of Table	in	Moments in Time
7 and the aggregation function in TSN is top-K pooling. Our	in	Moments in Time
with TSN also perform well in untrimmed videos, given a reasonable	in	Moments in Time
testing scheme, as described in Sec. 4.2	in	Moments in Time
network framework is further verified in the ActivityNet large scale activity	in	Moments in Time
we follow the approach described in Sec. 4.2. Understanding that the	in	Moments in Time
architecture plays an important role in boosting the performance, we also	in	Moments in Time
and test the recognition accuracy in terms of mean average precision	in	Moments in Time
on validation set are summarized in Table 6. We observe that	in	Moments in Time
the testing set are summarized in Table 7. Out entry “CES	in	Moments in Time
other participants of this challenge in Table 7. It is worth	in	Moments in Time
efficiency of TSN, our models in the challenge can be trained	in	Moments in Time
time visualize interesting class information in action recognition ConvNet models. We	in	Moments in Time
left) and y (right) directions in gray-scales. Note all these images	in	Moments in Time
purely random pixels. Left: classes in UCF101. Right: classes in ActivityNet	in	Moments in Time
ing. The results are shown in Fig. 3. For both RGB	in	Moments in Time
the scenery patterns and objects in the videos as significant evidences	in	Moments in Time
for action recognition. For example, in the class “Diving”, the single-frame	in	Moments in Time
models focus more on humans in the videos, and seem to	in	Moments in Time
different poses can be identified in the image, depicting various stages	in	Moments in Time
Similar observation would be identified in other action classes such as	in	Moments in Time
better, which is well reflected in our quantitative experiments	in	Moments in Time
good practices that we explored in this work. The former provides	in	Moments in Time
works for action recognition in videos,” in NIPS, 2014, pp	in	Moments in Time
trajectories,” in ICCV, 2013, pp. 3551–3558. [3	in	Moments in Time
human motion recognition,” in CVPR, 2013, pp. 2674–2681. [4	in	Moments in Time
net- works for video classification,” in CVPR, 2015, pp. 4694–4702	in	Moments in Time
with trajectory- pooled deep-convolutional descriptors,” in CVPR, 2015, pp. 4305– 4314	in	Moments in Time
with deep convolutional neural networks,” in NIPS, 2012, pp. 1106–1114	in	Moments in Time
works for large-scale image recognition,” in ICLR, 2015, pp. 1–14	in	Moments in Time
Rabinovich, “Going deeper with convolutions,” in CVPR, 2015, pp. 1–9	in	Moments in Time
scene recognition using places database,” in NIPS, 2014, pp. 487–495	in	Moments in Time
of deep convolutional neural networks,” in ECCV, 2016, pp. 467–482	in	Moments in Time
images by fusing deep channels,” in CVPR, 2015, pp. 1600–1609	in	Moments in Time
neural networks for event recognition in still images,” CoRR, vol. abs/1609.00162	in	Moments in Time
classification with convolutional neural networks,” in CVPR, 2014, pp. 1725–1732	in	Moments in Time
with 3d convolutional net- works,” in ICCV, 2015, pp. 4489–4497	in	Moments in Time
with enhanced motion vector CNNs,” in CVPR, 2016, pp. 2718–2726	in	Moments in Time
A large-scale hierarchical image database,” in CVPR, 2009, pp. 248– 255	in	Moments in Time
motion segments for activity classification,” in ECCV, 2010, pp. 392–405	in	Moments in Time
video evolution for action recognition,” in CVPR, 2015, pp. 5378–5387	in	Moments in Time
for visual recognition and description,” in CVPR, 2015, pp. 2625–2634	in	Moments in Time
action recog- nition for videos in the wild,” Computer Vision and	in	Moments in Time
for human activity under- standing,” in CVPR, 2015, pp. 961–970	in	Moments in Time
human actions classes from videos in the wild,” CoRR, vol. abs/1212.0402	in	Moments in Time
database for human motion recognition,” in ICCV, 2011, pp. 2556–2563	in	Moments in Time
residual learning for image recognition,” in CVPR, 2016, pp. 770–778	in	Moments in Time
inception architecture for computer vision,” in CVPR, 2016, pp. 2818–2826	in	Moments in Time
practices for deep action recognition,” in ECCV, 2016, pp. 20–36	in	Moments in Time
motion synthesis,” Foundations and Trends in Computer Graphics and Vision, vol	in	Moments in Time
scale-invariant spatio-temporal interest point detector,” in ECCV, 2008, pp. 650–663	in	Moments in Time
nition via sparse spatio-temporal features,” in IEEE International Workshop on PETS	in	Moments in Time
Action recognition by dense trajectories,” in CVPR, 2011, pp. 3169–3176	in	Moments in Time
realistic human actions from movies,” in CVPR, 2008, pp. 1–8	in	Moments in Time
spatio-temporal descriptor based on 3D-gradients,” in BMVC, 2008, pp. 1–12	in	Moments in Time
categorization with bags of keypoints,” in ECCV Workshop on statistical learning	in	Moments in Time
in computer vision, 2004, pp. 1–22	in	Moments in Time
super vector for action recognition,” in CVPR, 2014, pp. 596–603	in	Moments in Time
parts from mid-level video representations,” in CVPR, 2012, pp. 1242–1249	in	Moments in Time
videos using mid-level discriminative patches,” in CVPR, 2013, pp. 2571–2578	in	Moments in Time
for detailed action under- standing,” in ICCV, 2013, pp. 2248–2255	in	Moments in Time
Tu, “Action recognition with actons,” in ICCV, 2013, pp. 3559–3566	in	Moments in Time
high-level represen- tation of activity in video,” in CVPR, 2012, pp	in	Moments in Time
using 3d human pose annotations,” in ICCV, 2009, pp. 1365–1372	in	Moments in Time
using factorized spatio-temporal convolutional networks,” in ICCV, 2015, pp. 4597–4605	in	Moments in Time
Xue, “Modeling spatial- temporal clues in a hybrid deep learning framework	in	Moments in Time
for video classification,” in ACM Multimedia, 2015, pp. 461–470	in	Moments in Time
fusion for video action recognition,” in CVPR, 2016, pp. 1933–1941	in	Moments in Time
of actions with segmental grammars,” in CVPR, 2014, pp. 612–619	in	Moments in Time
action detection with relational dynamic-poselets,” in ECCV, 2014, pp. 565–580	in	Moments in Time
by reducing internal covariate shift,” in ICML, 2015, pp. 448–456	in	Moments in Time
for realtime tv-L1 optical flow,” in 29th DAGM Symposium on Pattern	in	Moments in Time
Zisserman, “Return of the devil in the details: Delving deep into	in	Moments in Time
convolutional nets,” in BMVC, 2014	in	Moments in Time
networks for video action recognition,” in NIPS, 2016, pp. 3468–3476	in	Moments in Time
for the thu- mos workshop,” in ICCV Workshop on THUMOS Challenge	in	Moments in Time
about classifying and localizing actions?” in CVPR, 2015, pp. 46–55	in	Moments in Time
depth for large-scale action recognition,” in ECCV. Springer, 2016, pp. 668–684	in	Moments in Time
recognition via trajectory group selection,” in CVPR, 2015, pp. 3698–3706	in	Moments in Time
deep framework for action recognition,” in CVPR, 2016, pp. 1991– 1999	in	Moments in Time
3.4 TSN in Practice	in	Moments in Time
4.1 Action Recognition in Trimmed Video	in	Moments in Time
4.2 Action Recognition in Untrimmed Videos	in	Moments in Time
Segment Networks for Action Recognition in Videos	in	Moments in Time
recognition. However, for action recognition in videos, their advantage over traditional	in	Moments in Time
framework for learning action models in videos. This method, called temporal	in	Moments in Time
easily adapted for action recognition in both trimmed and untrimmed videos	in	Moments in Time
5], owing to its applications in many areas like security and	in	Moments in Time
behavior analysis. For action recognition in videos, there are two crucial	in	Moments in Time
6] have achieved great success in classifying images of objects [7	in	Moments in Time
representations from raw visual data in large- scale supervised datasets (e.g	in	Moments in Time
of ConvNets to action recognition in unconstrained videos is impeded by	in	Moments in Time
for understanding the dy- namics in traditional methods [19], [20], [21	in	Moments in Time
considered as a critical factor in deep ConvNet frameworks [1], [15	in	Moments in Time
deploy the learned action models in a realistic setting we often	in	Moments in Time
action models to action recognition in untrimmed videos	in	Moments in Time
remain limited in both size and diversity, making	in	Moments in Time
the action recogni- tion problem in this paper from the following	in	Moments in Time
framework for learning action models in videos. It is based on	in	Moments in Time
be more favorable and efficient in this case. The TSN framework	in	Moments in Time
structures over the whole video, in a way that its computational	in	Moments in Time
To tackle the practical difficulties in learning and apply	in	Moments in Time
to perform Batch Normalization (BN) in a fine-tuning scenario, denoted as	in	Moments in Time
which has numerous potential applications in real-world problems	in	Moments in Time
our method for action recognition in both trimmed and untrimmed videos	in	Moments in Time
method secures the 1st place in untrimmed video classification at the	in	Moments in Time
different aspects of the problems in efficiently and effectively learning and	in	Moments in Time
extends our previous work [31] in a number of aspects. First	in	Moments in Time
Challenge 2016, which ranks #1 in untrimmed video classification among 24	in	Moments in Time
recognition has been studied extensively in recent years and readers can	in	Moments in Time
For action recognition in videos, the visual representation plays	in	Moments in Time
a set of mid-level patches in a strongly-supervised manner. Similar to	in	Moments in Time
ConvNet architectures for action recognition in videos [1], [4], [5], [15	in	Moments in Time
use recurrent neural networks (RNN), in particular LSTM, to model the	in	Moments in Time
frame features for action recognition in videos	in	Moments in Time
to learn the model parameters in an iterative approach. Wang et	in	Moments in Time
As discussed in Sec. 1, long-range temporal modeling	in	Moments in Time
is important for action understanding in videos. The existing deep architectures	in	Moments in Time
1], [16], it still suffers in both computational and modeling aspects	in	Moments in Time
it totally samples 64 frames in the work of [23] and	in	Moments in Time
the frames are densely recorded in the videos, the content changes	in	Moments in Time
to model the temporal structures in a human action. Normally, the	in	Moments in Time
a video-level framework as shown in Figure 1, which would be	in	Moments in Time
explained in the next subsection	in	Moments in Time
its corresponding segment. Each snippet in this sequence produces its own	in	Moments in Time
details on these consensus functions in the next subsection	in	Moments in Time
K is number of segments in temporal segment net- work. When	in	Moments in Time
function is an important component in our temporal segment net- work	in	Moments in Time
video. On the other hand, in particular for noisy videos with	in	Moments in Time
phases may play different roles in recogniz- ing action classes. This	in	Moments in Time
the basic back- propagation formula in Eq. 3 should be rectified	in	Moments in Time
3.4 TSN in Practice	in	Moments in Time
other ConvNet architectures de- ployed in videos [1], [16], this architecture	in	Moments in Time
the potential of TSN framework in video classification	in	Moments in Time
work, we extend this approach in two aspects, namely accuracy and	in	Moments in Time
speed. As shown in Figure 2, in addition to	in	Moments in Time
help to improve the accuracy in motion perception and thus boost	in	Moments in Time
16] and motion vector [17] in motion modeling, we revisit the	in	Moments in Time
work on dense optical flow in [60], the partial derivatives of	in	Moments in Time
to time play critical roles in computing optical flow. It is	in	Moments in Time
the power of optical flow in representing motion could be learned	in	Moments in Time
for action recognition are limited in terms of sizes. In practice	in	Moments in Time
strategies to improve the training in the temporal segment network framework	in	Moments in Time
networks take RGB images as in	in	Moments in Time
models across the RGB channels in the first layer and replicate	in	Moments in Time
increases the risk of over-fitting in the transfer learning process, due	in	Moments in Time
number of training sam- ples in target dataset. Therefore, after initialization	in	Moments in Time
dropout ratio (set as 0.8 in experiment) after the global pooling	in	Moments in Time
scale jittering technique [8] used in ImageNet classification to action recognition	in	Moments in Time
this framework to recognize actions in realistic videos. In this section	in	Moments in Time
devise a series of techniques in order to improve the robustness	in	Moments in Time
4.1 Action Recognition in Trimmed Video	in	Moments in Time
share the model pa- rameters in temporal segment networks, the learned	in	Moments in Time
determined empirically. It is described in Sec. 3.2 that the segmental	in	Moments in Time
normalization. To test the models in compliance with their training, we	in	Moments in Time
4.2 Action Recognition in Untrimmed Videos	in	Moments in Time
major obstacle for action recognition in untrimmed videos is the large	in	Moments in Time
portion of irrelevant content in the input videos. Since our	in	Moments in Time
averaging scores from every location in a video, has a high	in	Moments in Time
risk of factoring in the unpredictable responses of the	in	Moments in Time
Background issue: the irrelevant content in a video can have high	in	Moments in Time
snippets from the input videos in a fixed sampling rate (e.g	in	Moments in Time
on these sampled snippets. Then, in order to cover the highly	in	Moments in Time
Formally, for a video in length of M seconds, we	in	Moments in Time
the results of our approach in the ActivityNet challenge 2016 and	in	Moments in Time
not specifically noted, the experiments in the section are conducted with	in	Moments in Time
set a smaller learning rate in our experiments. On the dataset	in	Moments in Time
and scale jittering, as specified in Section 3.4. For the extraction	in	Moments in Time
of the good practices described in Sec. 3.4, including the train	in	Moments in Time
propose two new training strategies in Section 3.4, namely cross modality	in	Moments in Time
only pre-train spatial stream as in [1]; (3) with cross modality	in	Moments in Time
dropout. The results are summarized in Table 1. First, we see	in	Moments in Time
nition performance to 92.0%. Therefore, in the remaining experiments, we employ	in	Moments in Time
two new types of modalities in Section 3.4: RGB difference and	in	Moments in Time
modalities and report the results in Table 2. These experiments are	in	Moments in Time
all the good practices verified in Table 1	in	Moments in Time
which is the basic combination in the two-stream ConvNets also works	in	Moments in Time
action recognition methods as shown in Table 2. This suggests that	in	Moments in Time
segment network framework. As described in Sec 3, the framework has	in	Moments in Time
the temporal segment network framework in	in	Moments in Time
flow fields for input modalities in this exploration. Finally, to demonstrate	in	Moments in Time
the importance of TSN in long-range modeling	in	Moments in Time
of different segment numbers K in temporal segment networks on the	in	Moments in Time
use the average consensus function in these experiments. Dataset UCF101 ActivityNet	in	Moments in Time
segment number K as 7 in these experiments. Dataset UCF101 ActivityNet	in	Moments in Time
the sparse snippet sampling scheme in TSN is the number of	in	Moments in Time
approaches. The results are summarized in Table 3. We observe that	in	Moments in Time
we set K = 7 in the following experiments	in	Moments in Time
The experimental results are summarized in Table 4. On UCF101, which	in	Moments in Time
pooling for complex videos (ActivityNet) in later experiments	in	Moments in Time
and the results are summarized in Table 5. We use K	in	Moments in Time
1 in these experiments, which is equivalent	in	Moments in Time
TABLE 7 Winning entries in the untrimmed video classification task	in	Moments in Time
We present the recognition accuracies in the form of mAP values	in	Moments in Time
values in the challenge. Team mAP Top1	in	Moments in Time
the effect of the components in temporal segment networks and coming	in	Moments in Time
UCF101. The results are summarized in the left columns of Table	in	Moments in Time
v1.2. The results are summarized in the right columns of Table	in	Moments in Time
7 and the aggregation function in TSN is top-K pooling. Our	in	Moments in Time
with TSN also perform well in untrimmed videos, given a reasonable	in	Moments in Time
testing scheme, as described in Sec. 4.2	in	Moments in Time
network framework is further verified in the ActivityNet large scale activity	in	Moments in Time
we follow the approach described in Sec. 4.2. Understanding that the	in	Moments in Time
architecture plays an important role in boosting the performance, we also	in	Moments in Time
and test the recognition accuracy in terms of mean average precision	in	Moments in Time
on validation set are summarized in Table 6. We observe that	in	Moments in Time
the testing set are summarized in Table 7. Out entry “CES	in	Moments in Time
other participants of this challenge in Table 7. It is worth	in	Moments in Time
efficiency of TSN, our models in the challenge can be trained	in	Moments in Time
time visualize interesting class information in action recognition ConvNet models. We	in	Moments in Time
left) and y (right) directions in gray-scales. Note all these images	in	Moments in Time
purely random pixels. Left: classes in UCF101. Right: classes in ActivityNet	in	Moments in Time
ing. The results are shown in Fig. 3. For both RGB	in	Moments in Time
the scenery patterns and objects in the videos as significant evidences	in	Moments in Time
for action recognition. For example, in the class “Diving”, the single-frame	in	Moments in Time
models focus more on humans in the videos, and seem to	in	Moments in Time
different poses can be identified in the image, depicting various stages	in	Moments in Time
Similar observation would be identified in other action classes such as	in	Moments in Time
better, which is well reflected in our quantitative experiments	in	Moments in Time
good practices that we explored in this work. The former provides	in	Moments in Time
works for action recognition in videos,” in NIPS, 2014, pp	in	Moments in Time
trajectories,” in ICCV, 2013, pp. 3551–3558. [3	in	Moments in Time
human motion recognition,” in CVPR, 2013, pp. 2674–2681. [4	in	Moments in Time
net- works for video classification,” in CVPR, 2015, pp. 4694–4702	in	Moments in Time
with trajectory- pooled deep-convolutional descriptors,” in CVPR, 2015, pp. 4305– 4314	in	Moments in Time
with deep convolutional neural networks,” in NIPS, 2012, pp. 1106–1114	in	Moments in Time
works for large-scale image recognition,” in ICLR, 2015, pp. 1–14	in	Moments in Time
Rabinovich, “Going deeper with convolutions,” in CVPR, 2015, pp. 1–9	in	Moments in Time
scene recognition using places database,” in NIPS, 2014, pp. 487–495	in	Moments in Time
of deep convolutional neural networks,” in ECCV, 2016, pp. 467–482	in	Moments in Time
images by fusing deep channels,” in CVPR, 2015, pp. 1600–1609	in	Moments in Time
neural networks for event recognition in still images,” CoRR, vol. abs/1609.00162	in	Moments in Time
classification with convolutional neural networks,” in CVPR, 2014, pp. 1725–1732	in	Moments in Time
with 3d convolutional net- works,” in ICCV, 2015, pp. 4489–4497	in	Moments in Time
with enhanced motion vector CNNs,” in CVPR, 2016, pp. 2718–2726	in	Moments in Time
A large-scale hierarchical image database,” in CVPR, 2009, pp. 248– 255	in	Moments in Time
motion segments for activity classification,” in ECCV, 2010, pp. 392–405	in	Moments in Time
video evolution for action recognition,” in CVPR, 2015, pp. 5378–5387	in	Moments in Time
for visual recognition and description,” in CVPR, 2015, pp. 2625–2634	in	Moments in Time
action recog- nition for videos in the wild,” Computer Vision and	in	Moments in Time
for human activity under- standing,” in CVPR, 2015, pp. 961–970	in	Moments in Time
human actions classes from videos in the wild,” CoRR, vol. abs/1212.0402	in	Moments in Time
database for human motion recognition,” in ICCV, 2011, pp. 2556–2563	in	Moments in Time
residual learning for image recognition,” in CVPR, 2016, pp. 770–778	in	Moments in Time
inception architecture for computer vision,” in CVPR, 2016, pp. 2818–2826	in	Moments in Time
practices for deep action recognition,” in ECCV, 2016, pp. 20–36	in	Moments in Time
motion synthesis,” Foundations and Trends in Computer Graphics and Vision, vol	in	Moments in Time
scale-invariant spatio-temporal interest point detector,” in ECCV, 2008, pp. 650–663	in	Moments in Time
nition via sparse spatio-temporal features,” in IEEE International Workshop on PETS	in	Moments in Time
Action recognition by dense trajectories,” in CVPR, 2011, pp. 3169–3176	in	Moments in Time
realistic human actions from movies,” in CVPR, 2008, pp. 1–8	in	Moments in Time
spatio-temporal descriptor based on 3D-gradients,” in BMVC, 2008, pp. 1–12	in	Moments in Time
categorization with bags of keypoints,” in ECCV Workshop on statistical learning	in	Moments in Time
in computer vision, 2004, pp. 1–22	in	Moments in Time
super vector for action recognition,” in CVPR, 2014, pp. 596–603	in	Moments in Time
parts from mid-level video representations,” in CVPR, 2012, pp. 1242–1249	in	Moments in Time
videos using mid-level discriminative patches,” in CVPR, 2013, pp. 2571–2578	in	Moments in Time
for detailed action under- standing,” in ICCV, 2013, pp. 2248–2255	in	Moments in Time
Tu, “Action recognition with actons,” in ICCV, 2013, pp. 3559–3566	in	Moments in Time
high-level represen- tation of activity in video,” in CVPR, 2012, pp	in	Moments in Time
using 3d human pose annotations,” in ICCV, 2009, pp. 1365–1372	in	Moments in Time
using factorized spatio-temporal convolutional networks,” in ICCV, 2015, pp. 4597–4605	in	Moments in Time
Xue, “Modeling spatial- temporal clues in a hybrid deep learning framework	in	Moments in Time
for video classification,” in ACM Multimedia, 2015, pp. 461–470	in	Moments in Time
fusion for video action recognition,” in CVPR, 2016, pp. 1933–1941	in	Moments in Time
of actions with segmental grammars,” in CVPR, 2014, pp. 612–619	in	Moments in Time
action detection with relational dynamic-poselets,” in ECCV, 2014, pp. 565–580	in	Moments in Time
by reducing internal covariate shift,” in ICML, 2015, pp. 448–456	in	Moments in Time
for realtime tv-L1 optical flow,” in 29th DAGM Symposium on Pattern	in	Moments in Time
Zisserman, “Return of the devil in the details: Delving deep into	in	Moments in Time
convolutional nets,” in BMVC, 2014	in	Moments in Time
networks for video action recognition,” in NIPS, 2016, pp. 3468–3476	in	Moments in Time
for the thu- mos workshop,” in ICCV Workshop on THUMOS Challenge	in	Moments in Time
about classifying and localizing actions?” in CVPR, 2015, pp. 46–55	in	Moments in Time
depth for large-scale action recognition,” in ECCV. Springer, 2016, pp. 668–684	in	Moments in Time
recognition via trajectory group selection,” in CVPR, 2015, pp. 3698–3706	in	Moments in Time
deep framework for action recognition,” in CVPR, 2016, pp. 1991– 1999	in	Moments in Time
3.4 TSN in Practice	in	Moments in Time
4.1 Action Recognition in Trimmed Video	in	Moments in Time
4.2 Action Recognition in Untrimmed Videos	in	Moments in Time
for activity detection, such as THUMOS (Jiang et al., 2014), Ac	THUMOS	Multi-THUMOS
4.2. THUMOS / MultiTHUMOS	THUMOS	Multi-THUMOS
an extended version of the THUMOS dataset that densely annotates the	THUMOS	Multi-THUMOS
classes, compared to 20 in THUMOS, and contains on average 10.5	THUMOS	Multi-THUMOS
age ∼1 activity per video. THUMOS and MultiTHUMOS consists of YouTube	THUMOS	Multi-THUMOS
and us- ing IoU=0.5 in THUMOS	THUMOS	Multi-THUMOS
on the continuous version of THUMOS	THUMOS	Multi-THUMOS
duration of activities in Multi- THUMOS (3.3 seconds), we find that	THUMOS	Multi-THUMOS
learned TGM kernels. On Multi- THUMOS, it learns to focus on	THUMOS	Multi-THUMOS
Shah, M., and Sukthankar, R. THUMOS chal- lenge: Action recognition with	THUMOS	Multi-THUMOS
THUMOS14	THUMOS	Multi-THUMOS
THUMOS14	THUMOS	Multi-THUMOS
Multi	Multi	Multi-THUMOS
short duration of activities in Multi	Multi	Multi-THUMOS
several learned TGM kernels. On Multi	Multi	Multi-THUMOS
datasets and challenges such as THUMOS [12], ActivityNet [6] and Charades	THUMOS	Multi-THUMOS
is an extension of the THUMOS [12] dataset with the untrimmed	THUMOS	Multi-THUMOS
classes. Unlike Activi- tyNet and THUMOS, MultiTHUMOS has on average 10.5	THUMOS	Multi-THUMOS
the segmented training videos from THUMOS, since super- event learning is	THUMOS	Multi-THUMOS
M. Shah, and R. Sukthankar. THUMOS chal- lenge: Action recognition with	THUMOS	Multi-THUMOS
THUMOS14	THUMOS	Multi-THUMOS
THUMOS14	THUMOS	Multi-THUMOS
Two-stream + LSTM [31] 28.1 Multi	Multi	Multi-THUMOS
Charades Dataset We further test on	Charades	Charades
the popular Charades dataset [19] which is unique	Charades	Charades
all with higher-than-50% accuracy on Charades	Charades	Charades
action classification per- formances on Charades [19]. Method modality mAP	Charades	Charades
may temporally overlap in a Charades video, requiring the model to	Charades	Charades
trained and tested on the Charades and MiT datasets. For Charades	Charades	Charades
note that the performances on Charades is even more impressive at	Charades	Charades
Architecture MiT Charades	Charades	Charades
A. Farhadi, and K. Alahari. Charades	Charades	Charades
The batch size used for Charades is 128 with 128 frames	Charades	Charades
the entire video. For the Charades dataset where each video duration	Charades	Charades
major video recognition benchmarks, Kinetics, Charades and AVA. Code will be	Charades	Charades
the Kinetics-400 [27], Kinetics-600 [2], Charades [40] and AVA [17] datasets	Charades	Charades
recent Kinetics- 600 [2], and Charades [40]. For action detection experiments	Charades	Charades
Charades [40] has ∼9.8k training videos	Charades	Charades
Charades [40] is a dataset with	Charades	Charades
Comparison with the state-of-the-art on Charades	Charades	Charades
For Charades, we fine-tune from Kinetics pre-trained	Charades	Charades
video datasets: AVA, EPIC-Kitchens, and Charades	Charades	Charades
action classifica- tion [6], and Charades video classification [38]. Our abla	Charades	Charades
6. Experiments on Charades	Charades	Charades
evaluate our approach on the Charades dataset [38]. The Charades dataset	Charades	Charades
Table 3. Training schedule on Charades	Charades	Charades
4. Action recognition accuracy on Charades	Charades	Charades
NL′ to work better on Charades, so we adopt it in	Charades	Charades
top-1) AVA EPIC Verbs (top-1) Charades	Charades	Charades
For Charades, we experiment with both ResNet-50-I3D	Charades	Charades
test sets. The improvement on Charades is not as large as	Charades	Charades
to 60 seconds is useful. Charades videos are much shorter (∼30	Charades	Charades
datasets like AVA, EPIC-Kitchens, and Charades	Charades	Charades
used for computing L, so Charades	Charades	Charades
Appendix H. Charades Training Schedule	Charades	Charades
Appendix I. Charades NL Block Details	Charades	Charades
Pre-activation vs. post-activation NL′ on Charades	Charades	Charades
choose post-activation as default for Charades due to the stronger performance	Charades	Charades
achieve state-of-the-art results on both Charades and Something-Something datasets. Especially for	Charades	Charades
our experiments in the challenging Charades [20] and 20BN-Something- Something [21	Charades	Charades
action recognition. Especially in the Charades dataset, we obtain 4.4% boost	Charades	Charades
on two recent challenging datasets: Charades [20] and Something-Something [21]. We	Charades	Charades
on our target datasets (e.g. Charades or Something- Something) as following	Charades	Charades
d = 512. Since both Charades and Something-Something dataset are in	Charades	Charades
loss functions when training for Charades and Something-Something datasets. For Something-Something	Charades	Charades
the softmax loss function. For Charades, we apply binary sigmoid loss	Charades	Charades
two different datasets. As for Charades, the scenes are more cluttered	Charades	Charades
we sample 10 clips for Charades and 2 clips for Something-Something	Charades	Charades
Table 2. Ablations on Charades	Charades	Charades
6.2 Experiments on Charades	Charades	Charades
In the Charades experiments, following the official split	Charades	Charades
actually very small. In the Charades dataset, our graph is defined	Charades	Charades
specifically, for each video in Charades, besides the action class labels	Charades	Charades
Classification mAP (%) in the Charades dataset [20]. NL is short	Charades	Charades
is very different from the Charades dataset. In the Charades dataset	Charades	Charades
gains we have in the Charades dataset. The reason is mainly	Charades	Charades
ble to the previous version. Charades [26] is an activity recognition	Charades	Charades
seconds on average. We chose Charades to particularly confirm whether our	Charades	Charades
Table 4. Charades classification results against state-of-the-arts	Charades	Charades
Charades	Charades	Charades
our approach on the popular Charades dataset. Table 4 compares against	Charades	Charades
Method Kinetics Charades HMDB MiT	Charades	Charades
37.8 82.3 31.8 Evolved on Charades 76.5 38.1 81.8 31.1 Evolved	Charades	Charades
or 11) when evolved for Charades, while they only had a	Charades	Charades
An average activity duration in Charades videos are around 12 seconds	Charades	Charades
iTGM kernels from Kinetics to Charades	Charades	Charades
Stretched (L = 11) 38.1 Charades EvaNet 38.1	Charades	Charades
layers and apply it to Charades, which has activities with much	Charades	Charades
to L = 11 on Charades, which shows similar performance. Evolution	Charades	Charades
176 × 176 (for Charades) where 32 and 64 are	Charades	Charades
12, this time using the Charades dataset	Charades	Charades
Table 14. Charades performance comparison to baselines, all	Charades	Charades
Inception- like architectures evolved on Charades	Charades	Charades
Charades, the architectures generally capture longer	Charades	Charades
Figure 16. Charades RGB Top 1	Charades	Charades
Figure 17. Charades RGB Top 2	Charades	Charades
Figure 18. Charades RGB Top 3	Charades	Charades
Something- Something, Jester, and Charades - which fundamentally depend on	Charades	Charades
Something-Something [9], Jester [10], and Charades [11]), which are constructed for	Charades	Charades
Left’, and ‘Turning hand counterclockwise’. Charades dataset is also a high-level	Charades	Charades
on activity classification in the Charades dataset [11], outperforming the Flow+RGB	Charades	Charades
9,28], Jester dataset [10], and Charades dataset [11] are listed in	Charades	Charades
27 148,092 human hand gesture Charades 157 9,848 daily indoor activity	Charades	Charades
3.3 Results on Jester and Charades	Charades	Charades
MultiScale TRN on the recent Charades dataset for daily activity recognition	Charades	Charades
Table 4: Results on Charades Activity Classification	Charades	Charades
2 predictions are shown above Charades frames	Charades	Charades
UCF Kinetics Moments Something Jester Charades	Charades	Charades
mAP of 22.4% on the Charades [43] benchmark, outperforming the state-of-the-art	Charades	Charades
22.4% mAP on the challenging Charades [43] benchmark	Charades	Charades
our evaluation, we chose the Charades dataset [43]. This dataset is	Charades	Charades
follow the training setup in Charades [43] and con- sider a	Charades	Charades
Qi(X) at each frame. The Charades dataset has the property that	Charades	Charades
1. Video classification results on Charades [43]. The left shows the	Charades	Charades
as a part of the Charades dataset (allenai.org/plato/charades	Charades	Charades
charades	charades	Charades
on the UCF-101, HMDB-51, and Charades dataset	Charades	Charades
34], HMDB- 51 [18], and Charades [32], our approach significantly out	Charades	Charades
UCF-101 [34], HMDB-51 [18], and Charades [32]. UCF-101 and HMDB-51 contain	Charades	Charades
annotated with one action label. Charades contains longer (∼ 30- second	Charades	Charades
un- less otherwise stated. The Charades dataset contains 9,848 videos split	Charades	Charades
softmax following TSN [44]. On Charades we use mean average precision	Charades	Charades
videos to 340× 256. As Charades con- tains both portrait and	Charades	Charades
UCF- 101/HMDB-51 and 0.03 for Charades	Charades	Charades
Table 7: Accuracy on Charades [32]. Without using ad- ditional	Charades	Charades
evaluate our method on the Charades dataset (Table 7). As Charades	Charades	Charades
other state- of-the-art methods on Something-Something V1 dataset. Single crop STM beats	Something-Something V	Something-Something V2
on both temporal-related datasets (i.e., Something-Something v1 & v2 and Jester	Something-Something	Something-Something V2
several public benchmark datasets including Something-Something	Something-Something	Something-Something V2
conduct abundant ablation studies with Something-Something v1 to analyze the effectiveness	Something-Something	Something-Something V2
categories: (1) temporal-related datasets, including Something-Something v1 & v2 [11] and	Something-Something	Something-Something V2
of the STM on the Something-Something v1 and v2 datasets compared	Something-Something	Something-Something V2
Method Backbone Flow Pretrain Frame Something-Something v1 Something-Something v2top-1 val top-5	Something-Something	Something-Something V2
methods on temporal-related datasets including Something-Something v1 & v2 and Jester	Something-Something	Something-Something V2
174 classes with 108,499 videos. Something-Something v2 is an updated version	Something-Something	Something-Something V2
compared with the state-of-the-art on Something-Something v1 and v2. The results	Something-Something	Something-Something V2
on Something- Something v1. On Something-Something v2, STM also gains 34.5	Something-Something	Something-Something V2
valida- tion sets of both Something-Something v1 and v2, and just	Something-Something	Something-Something V2
our pro- posed STM on Something-Something v1 dataset. All the ablation	Something-Something	Something-Something V2
other state- of-the-art methods on Something-Something V1 dataset. Single crop STM	Something-Something	Something-Something V2
and several state-of-the-art methods on Something-Something v1 dataset. All evaluations are	Something-Something	Something-Something V2
on both temporal-related datasets (i.e., Something	Something	Something-Something V2
Something v1 & v2 and Jester	Something	Something-Something V2
several public benchmark datasets including Something	Something	Something-Something V2
Something	Something	Something-Something V2
on both temporal-related datasets (i.e., Something	Something	Something-Something V2
- Something v1 & v2 and Jester	Something	Something-Something V2
conduct abundant ablation studies with Something	Something	Something-Something V2
Something v1 to analyze the effectiveness	Something	Something-Something V2
categories: (1) temporal-related datasets, including Something	Something	Something-Something V2
Something v1 & v2 [11] and	Something	Something-Something V2
T = 16). For Kinetics, Something	Something	Something-Something V2
- Something v1 & v2 and Jester	Something	Something-Something V2
of the STM on the Something	Something	Something-Something V2
Something v1 and v2 datasets compared	Something	Something-Something V2
Method Backbone Flow Pretrain Frame Something	Something	Something-Something V2
-Something v1 Something	Something	Something-Something V2
Something v2top-1 val top-5 val top-1	Something	Something-Something V2
methods on temporal-related datasets including Something	Something	Something-Something V2
Something v1 & v2 and Jester	Something	Something-Something V2
. Something- Something v1 is a large collection	Something	Something-Something V2
174 classes with 108,499 videos. Something	Something	Something-Something V2
Something v2 is an updated version	Something	Something-Something V2
compared with the state-of-the-art on Something	Something	Something-Something V2
Something v1 and v2. The results	Something	Something-Something V2
16 frames inputs respectively on Something	Something	Something-Something V2
- Something v1. On Something	Something	Something-Something V2
Something v2, STM also gains 34.5	Something	Something-Something V2
valida- tion sets of both Something	Something	Something-Something V2
Something v1 and v2, and just	Something	Something-Something V2
our pro- posed STM on Something	Something	Something-Something V2
Something v1 dataset. All the ablation	Something	Something-Something V2
other state- of-the-art methods on Something	Something	Something-Something V2
Something V1 dataset. Single crop STM	Something	Something-Something V2
and several state-of-the-art methods on Something	Something	Something-Something V2
Something v1 dataset. All evaluations are	Something	Something-Something V2
Something V2 dataset). The three kinds of	Something V2	Something-Something V2
Something V2, the epoch number is halved	Something V2	Something-Something V2
Something V2 with the same experimental settings	Something V2	Something-Something V2
Something V2	Something V2	Something-Something V2
of experi- ments on Something Something V2 and Charades datasets with the	Something V2	Something-Something V2
with Inception-V3 backbone on Something- Something V2, and improve the mAP from	Something V2	Something-Something V2
Something V2 (left) and Charades (right) datasets	Something V2	Something-Something V2
the CAM [54] on Something- Something V2 dataset. CAM can visualize the	Something V2	Something-Something V2
Something V2 validation set. The representations are	Something V2	Something-Something V2
Something V2 dataset with 20 randomly selected	Something V2	Something-Something V2
the samples are originated from Something-Something V2 dataset). The three kinds of	Something-Something V2	Something-Something V2
procedure takes 100 epochs. For Something-Something V2, the epoch number is halved	Something-Something V2	Something-Something V2
Something-Something V1 and 2.5% on Something-Something V2 with the same experimental settings	Something-Something V2	Something-Something V2
Something-Something V1 and 61.3% on Something-Something V2	Something-Something V2	Something-Something V2
Inception or Inception-V3 architecture on Something-Something V2 (left) and Charades (right) datasets	Something-Something V2	Something-Something V2
of the video representation on Something-Something V2 validation set. The representations are	Something-Something V2	Something-Something V2
The experiment was conducted on Something-Something V2 dataset with 20 randomly selected	Something-Something V2	Something-Something V2
samples are originated from Something-Something V2 dataset). The three kinds of	V2	Something-Something V2
data sets: Something-Something V1 [9], V2 [16] and Charades [29]. Both	V2	Something-Something V2
recognition: Something- Something V1 [9], V2 [16] and Charades [29]. We	V2	Something-Something V2
V2 [16]: The dataset is twice	V2	Something-Something V2
takes 100 epochs. For Something-Something V2, the epoch number is halved	V2	Something-Something V2
Every video in Something-Something-V1 and V2 datasets is assigned to a	V2	Something-Something V2
results on Something-Something V1 and V2 validation set. Considering that they	V2	Something-Something V2
V1 and 2.5% on Something-Something V2 with the same experimental settings	V2	Something-Something V2
V1 and 61.3% on Something-Something V2	V2	Something-Something V2
ON THE SOMETHING-SOMETHING V1 AND V2 DATASETS. WE ONLY REPORT THE	V2	Something-Something V2
OF OUR MODEL ON SOMETHING-SOMETHING V2 DATASET	V2	Something-Something V2
experi- ments on Something Something V2 and Charades datasets with the	V2	Something-Something V2
Inception-V3 backbone on Something- Something V2, and improve the mAP from	V2	Something-Something V2
or Inception-V3 architecture on Something-Something V2 (left) and Charades (right) datasets	V2	Something-Something V2
CAM [54] on Something- Something V2 dataset. CAM can visualize the	V2	Something-Something V2
the video representation on Something-Something V2 validation set. The representations are	V2	Something-Something V2
experiment was conducted on Something-Something V2 dataset with 20 randomly selected	V2	Something-Something V2
the samples are originated from Something-Something V2 dataset). The three kinds of	Something-Something V	Something-Something V2
three large- scale data sets: Something-Something V1 [9], V2 [16] and Charades	Something-Something V	Something-Something V2
procedure takes 100 epochs. For Something-Something V2, the epoch number is halved	Something-Something V	Something-Something V2
shows all the results on Something-Something V1 and V2 validation set. Considering	Something-Something V	Something-Something V2
a large margin 4.1% on Something-Something V1 and 2.5% on Something-Something V2	Something-Something V	Something-Something V2
overall performance to 49.8% on Something-Something V1 and 61.3% on Something-Something V2	Something-Something V	Something-Something V2
Inception or Inception-V3 architecture on Something-Something V2 (left) and Charades (right) datasets	Something-Something V	Something-Something V2
of the video representation on Something-Something V2 validation set. The representations are	Something-Something V	Something-Something V2
The experiment was conducted on Something-Something V2 dataset with 20 randomly selected	Something-Something V	Something-Something V2
widely-used large-scale datasets, such as Something-Something and Charades, and the results	Something-Something	Something-Something V2
the samples are originated from Something-Something V2 dataset). The three kinds	Something-Something	Something-Something V2
three large- scale data sets: Something-Something V1 [9], V2 [16] and	Something-Something	Something-Something V2
Something-Something	Something-Something	Something-Something V2
Something-Something	Something-Something	Something-Something V2
procedure takes 100 epochs. For Something-Something V2, the epoch number is	Something-Something	Something-Something V2
Every video in Something-Something	Something-Something	Something-Something V2
A baseline model provided in Something-Something dataset exploits multi-layer 3D convolution	Something-Something	Something-Something V2
The results on Something-Something [9] and Charades [28], [29	Something-Something	Something-Something V2
shows all the results on Something-Something V1 and V2 validation set	Something-Something	Something-Something V2
a large margin 4.1% on Something-Something V1 and 2.5% on Something-Something	Something-Something	Something-Something V2
overall performance to 49.8% on Something-Something V1 and 61.3% on Something-Something	Something-Something	Something-Something V2
Inception or Inception-V3 architecture on Something-Something V2 (left) and Charades (right	Something-Something	Something-Something V2
of the video representation on Something-Something V2 validation set. The representations	Something-Something	Something-Something V2
The experiment was conducted on Something-Something V2 dataset with 20 randomly	Something-Something	Something-Something V2
evaluated the proposed model on Something-Something and Charades datasets and estab	Something-Something	Something-Something V2
MODULE OF OUR MODEL ON SOMETHING-SOMETHING V2 DATASET	SOMETHING-SOMETHING V2	Something-Something V2
widely-used large-scale datasets, such as Something	Something	Something-Something V2
Something and Charades, and the results	Something	Something-Something V2
the samples are originated from Something	Something	Something-Something V2
Something V2 dataset). The three kinds	Something	Something-Something V2
three large- scale data sets: Something	Something	Something-Something V2
Something V1 [9], V2 [16] and	Something	Something-Something V2
Covering Something	Something	Something-Something V2
with Something	Something	Something-Something V2
benchmark datasets for activity recognition: Something	Something	Something-Something V2
- Something V1 [9], V2 [16] and	Something	Something-Something V2
Something	Something	Something-Something V2
Something	Something	Something-Something V2
Something	Something	Something-Something V2
Something	Something	Something-Something V2
procedure takes 100 epochs. For Something	Something	Something-Something V2
Something V2, the epoch number is	Something	Something-Something V2
temporal axis for Charades and Something	Something	Something-Something V2
- Something dataset respectively and the prediction	Something	Something-Something V2
Every video in Something	Something	Something-Something V2
Something	Something	Something-Something V2
A baseline model provided in Something	Something	Something-Something V2
Something dataset exploits multi-layer 3D convolution	Something	Something-Something V2
The results on Something	Something	Something-Something V2
Something [9] and Charades [28], [29	Something	Something-Something V2
shows all the results on Something	Something	Something-Something V2
Something V1 and V2 validation set	Something	Something-Something V2
a large margin 4.1% on Something	Something	Something-Something V2
-Something V1 and 2.5% on Something	Something	Something-Something V2
Something V2 with the same experimental	Something	Something-Something V2
overall performance to 49.8% on Something	Something	Something-Something V2
-Something V1 and 61.3% on Something	Something	Something-Something V2
Something V2. Compare with the most	Something	Something-Something V2
serial of experi- ments on Something Something V2 and Charades datasets	Something	Something-Something V2
54.7% with Inception-V3 backbone on Something	Something	Something-Something V2
- Something V2, and improve the mAP	Something	Something-Something V2
Inception or Inception-V3 architecture on Something	Something	Something-Something V2
Something V2 (left) and Charades (right	Something	Something-Something V2
visualize the CAM [54] on Something	Something	Something-Something V2
- Something V2 dataset. CAM can visualize	Something	Something-Something V2
of the video representation on Something	Something	Something-Something V2
Something V2 validation set. The representations	Something	Something-Something V2
The experiment was conducted on Something	Something	Something-Something V2
Something V2 dataset with 20 randomly	Something	Something-Something V2
evaluated the proposed model on Something	Something	Something-Something V2
Something and Charades datasets and estab	Something	Something-Something V2
Something V2 dataset). The three kinds of	Something V2	Something-Something V2
Something V2, the epoch number is halved	Something V2	Something-Something V2
Something V2 with the same experimental settings	Something V2	Something-Something V2
Something V2	Something V2	Something-Something V2
of experi- ments on Something Something V2 and Charades datasets with the	Something V2	Something-Something V2
with Inception-V3 backbone on Something- Something V2, and improve the mAP from	Something V2	Something-Something V2
Something V2 (left) and Charades (right) datasets	Something V2	Something-Something V2
the CAM [54] on Something- Something V2 dataset. CAM can visualize the	Something V2	Something-Something V2
Something V2 validation set. The representations are	Something V2	Something-Something V2
Something V2 dataset with 20 randomly selected	Something V2	Something-Something V2
the samples are originated from Something-Something V2 dataset). The three kinds of	Something-Something V2	Something-Something V2
procedure takes 100 epochs. For Something-Something V2, the epoch number is halved	Something-Something V2	Something-Something V2
Something-Something V1 and 2.5% on Something-Something V2 with the same experimental settings	Something-Something V2	Something-Something V2
Something-Something V1 and 61.3% on Something-Something V2	Something-Something V2	Something-Something V2
Inception or Inception-V3 architecture on Something-Something V2 (left) and Charades (right) datasets	Something-Something V2	Something-Something V2
of the video representation on Something-Something V2 validation set. The representations are	Something-Something V2	Something-Something V2
The experiment was conducted on Something-Something V2 dataset with 20 randomly selected	Something-Something V2	Something-Something V2
samples are originated from Something-Something V2 dataset). The three kinds of	V2	Something-Something V2
data sets: Something-Something V1 [9], V2 [16] and Charades [29]. Both	V2	Something-Something V2
recognition: Something- Something V1 [9], V2 [16] and Charades [29]. We	V2	Something-Something V2
V2 [16]: The dataset is twice	V2	Something-Something V2
takes 100 epochs. For Something-Something V2, the epoch number is halved	V2	Something-Something V2
Every video in Something-Something-V1 and V2 datasets is assigned to a	V2	Something-Something V2
results on Something-Something V1 and V2 validation set. Considering that they	V2	Something-Something V2
V1 and 2.5% on Something-Something V2 with the same experimental settings	V2	Something-Something V2
V1 and 61.3% on Something-Something V2	V2	Something-Something V2
ON THE SOMETHING-SOMETHING V1 AND V2 DATASETS. WE ONLY REPORT THE	V2	Something-Something V2
OF OUR MODEL ON SOMETHING-SOMETHING V2 DATASET	V2	Something-Something V2
experi- ments on Something Something V2 and Charades datasets with the	V2	Something-Something V2
Inception-V3 backbone on Something- Something V2, and improve the mAP from	V2	Something-Something V2
or Inception-V3 architecture on Something-Something V2 (left) and Charades (right) datasets	V2	Something-Something V2
CAM [54] on Something- Something V2 dataset. CAM can visualize the	V2	Something-Something V2
the video representation on Something-Something V2 validation set. The representations are	V2	Something-Something V2
experiment was conducted on Something-Something V2 dataset with 20 randomly selected	V2	Something-Something V2
the samples are originated from Something-Something V2 dataset). The three kinds of	Something-Something V	Something-Something V2
three large- scale data sets: Something-Something V1 [9], V2 [16] and Charades	Something-Something V	Something-Something V2
procedure takes 100 epochs. For Something-Something V2, the epoch number is halved	Something-Something V	Something-Something V2
shows all the results on Something-Something V1 and V2 validation set. Considering	Something-Something V	Something-Something V2
a large margin 4.1% on Something-Something V1 and 2.5% on Something-Something V2	Something-Something V	Something-Something V2
overall performance to 49.8% on Something-Something V1 and 61.3% on Something-Something V2	Something-Something V	Something-Something V2
Inception or Inception-V3 architecture on Something-Something V2 (left) and Charades (right) datasets	Something-Something V	Something-Something V2
of the video representation on Something-Something V2 validation set. The representations are	Something-Something V	Something-Something V2
The experiment was conducted on Something-Something V2 dataset with 20 randomly selected	Something-Something V	Something-Something V2
widely-used large-scale datasets, such as Something-Something and Charades, and the results	Something-Something	Something-Something V2
the samples are originated from Something-Something V2 dataset). The three kinds	Something-Something	Something-Something V2
three large- scale data sets: Something-Something V1 [9], V2 [16] and	Something-Something	Something-Something V2
Something-Something	Something-Something	Something-Something V2
Something-Something	Something-Something	Something-Something V2
procedure takes 100 epochs. For Something-Something V2, the epoch number is	Something-Something	Something-Something V2
Every video in Something-Something	Something-Something	Something-Something V2
A baseline model provided in Something-Something dataset exploits multi-layer 3D convolution	Something-Something	Something-Something V2
The results on Something-Something [9] and Charades [28], [29	Something-Something	Something-Something V2
shows all the results on Something-Something V1 and V2 validation set	Something-Something	Something-Something V2
a large margin 4.1% on Something-Something V1 and 2.5% on Something-Something	Something-Something	Something-Something V2
overall performance to 49.8% on Something-Something V1 and 61.3% on Something-Something	Something-Something	Something-Something V2
Inception or Inception-V3 architecture on Something-Something V2 (left) and Charades (right	Something-Something	Something-Something V2
of the video representation on Something-Something V2 validation set. The representations	Something-Something	Something-Something V2
The experiment was conducted on Something-Something V2 dataset with 20 randomly	Something-Something	Something-Something V2
evaluated the proposed model on Something-Something and Charades datasets and estab	Something-Something	Something-Something V2
MODULE OF OUR MODEL ON SOMETHING-SOMETHING V2 DATASET	SOMETHING-SOMETHING V2	Something-Something V2
widely-used large-scale datasets, such as Something	Something	Something-Something V2
Something and Charades, and the results	Something	Something-Something V2
the samples are originated from Something	Something	Something-Something V2
Something V2 dataset). The three kinds	Something	Something-Something V2
three large- scale data sets: Something	Something	Something-Something V2
Something V1 [9], V2 [16] and	Something	Something-Something V2
Covering Something	Something	Something-Something V2
with Something	Something	Something-Something V2
benchmark datasets for activity recognition: Something	Something	Something-Something V2
- Something V1 [9], V2 [16] and	Something	Something-Something V2
Something	Something	Something-Something V2
Something	Something	Something-Something V2
Something	Something	Something-Something V2
Something	Something	Something-Something V2
procedure takes 100 epochs. For Something	Something	Something-Something V2
Something V2, the epoch number is	Something	Something-Something V2
temporal axis for Charades and Something	Something	Something-Something V2
- Something dataset respectively and the prediction	Something	Something-Something V2
Every video in Something	Something	Something-Something V2
Something	Something	Something-Something V2
A baseline model provided in Something	Something	Something-Something V2
Something dataset exploits multi-layer 3D convolution	Something	Something-Something V2
The results on Something	Something	Something-Something V2
Something [9] and Charades [28], [29	Something	Something-Something V2
shows all the results on Something	Something	Something-Something V2
Something V1 and V2 validation set	Something	Something-Something V2
a large margin 4.1% on Something	Something	Something-Something V2
-Something V1 and 2.5% on Something	Something	Something-Something V2
Something V2 with the same experimental	Something	Something-Something V2
overall performance to 49.8% on Something	Something	Something-Something V2
-Something V1 and 61.3% on Something	Something	Something-Something V2
Something V2. Compare with the most	Something	Something-Something V2
serial of experi- ments on Something Something V2 and Charades datasets	Something	Something-Something V2
54.7% with Inception-V3 backbone on Something	Something	Something-Something V2
- Something V2, and improve the mAP	Something	Something-Something V2
Inception or Inception-V3 architecture on Something	Something	Something-Something V2
Something V2 (left) and Charades (right	Something	Something-Something V2
visualize the CAM [54] on Something	Something	Something-Something V2
- Something V2 dataset. CAM can visualize	Something	Something-Something V2
of the video representation on Something	Something	Something-Something V2
Something V2 validation set. The representations	Something	Something-Something V2
The experiment was conducted on Something	Something	Something-Something V2
Something V2 dataset with 20 randomly	Something	Something-Something V2
evaluated the proposed model on Something	Something	Something-Something V2
Something and Charades datasets and estab	Something	Something-Something V2
V2 [16] to evaluate the overall	V2	Something-Something V2
V2 dataset. For a fair comparison	V2	Something-Something V2
V2	V2	Something-Something V2
achieves the state-of-the-art performance on Something-Something and Jester. We pro- vide	Something-Something	Something-Something V2
5.4. Results on Something-Something	Something-Something	Something-Something V2
Something-Something [12] is a recently released	Something-Something	Something-Something V2
a) Something-Something v2 Results	Something-Something	Something-Something V2
on a flat surface” from Something-Something v2 validation set	Something-Something	Something-Something V2
and our CPNet model on Something-Something and Jester datasets. Lastly in	Something-Something	Something-Something V2
E. Architecture used in Something-Something and Jester Experiments	Something-Something	Something-Something V2
F. Per-class accuracy of Something-Something and Jester models	Something-Something	Something-Something V2
8: CPNet Architectures used in Something-Something and Jester dataset experiments	Something-Something	Something-Something V2
Jester in Figure 8 and Something-Something in Figure 10	Something-Something	Something-Something V2
On Something-Something dataset [12], the largest ac	Something-Something	Something-Something V2
Kinetics [18] in Figure 11, Something-Something [12] in Figure 12 and	Something-Something	Something-Something V2
accuracy gain in percentage on Something-Something v2 dataset due to CP	Something-Something	Something-Something V2
Turning something upside down” from Something-Something v2 validation set	Something-Something	Something-Something V2
label “Picking something up” from Something-Something v2 validation set	Something-Something	Something-Something V2
label “Moving something down” from Something-Something v2 validation set	Something-Something	Something-Something V2
something next to something” from Something-Something v2 validation set	Something-Something	Something-Something V2
on our final models on Something-Something v2 dataset. Approach is the	Something-Something	Something-Something V2
achieves the state-of-the-art performance on Something	Something	Something-Something V2
Something and Jester. We pro- vide	Something	Something-Something V2
published methods on action-centric datasets Something	Something	Something-Something V2
- Something [12] and Jester [30] with	Something	Something-Something V2
ex- periments on action-centric datasets Something	Something	Something-Something V2
5.4. Results on Something	Something	Something-Something V2
Something	Something	Something-Something V2
Something	Something	Something-Something V2
Something [12] is a recently released	Something	Something-Something V2
a) Something	Something	Something-Something V2
Something v2 Results	Something	Something-Something V2
on a flat surface” from Something	Something	Something-Something V2
Something v2 validation set	Something	Something-Something V2
on the architecture used in Something	Something	Something-Something V2
- Something and Jester experiments (main paper	Something	Something-Something V2
and our CPNet model on Something	Something	Something-Something V2
Something and Jester datasets. Lastly in	Something	Something-Something V2
E. Architecture used in Something	Something	Something-Something V2
Something and Jester Experiments	Something	Something-Something V2
the CPNet architectures used in Something	Something	Something-Something V2
- Something [12] and Jester [30] experiments	Something	Something-Something V2
F. Per-class accuracy of Something	Something	Something-Something V2
Something and Jester models	Something	Something-Something V2
8: CPNet Architectures used in Something	Something	Something-Something V2
Something and Jester dataset experiments	Something	Something-Something V2
Jester in Figure 8 and Something	Something	Something-Something V2
Something in Figure 10	Something	Something-Something V2
On Something	Something	Something-Something V2
Something dataset [12], the largest ac	Something	Something-Something V2
Kinetics [18] in Figure 11, Something	Something	Something-Something V2
Something [12] in Figure 12 and	Something	Something-Something V2
Something being deflected from something	Something	Something-Something V2
Something falling like a rock	Something	Something-Something V2
Something colliding with something and both	Something	Something-Something V2
Something falling like a feather or	Something	Something-Something V2
Something colliding with something and both	Something	Something-Something V2
accuracy gain in percentage on Something	Something	Something-Something V2
Something v2 dataset due to CP	Something	Something-Something V2
Turning something upside down” from Something	Something	Something-Something V2
Something v2 validation set	Something	Something-Something V2
label “Picking something up” from Something	Something	Something-Something V2
Something v2 validation set	Something	Something-Something V2
label “Moving something down” from Something	Something	Something-Something V2
Something v2 validation set	Something	Something-Something V2
something next to something” from Something	Something	Something-Something V2
Something v2 validation set	Something	Something-Something V2
on our final models on Something	Something	Something-Something V2
Something v2 dataset. Approach is the	Something	Something-Something V2
V2 [28] where the Something-V2 is	V2	Something-Something V2
V2 174 220,847 human-object interaction Jester	V2	Something-Something V2
set of Something-V1 and Something- V2 datasets are listed in Table	V2	Something-Something V2
V2 respec- tively. We further submit	V2	Something-Something V2
V2 Val Test Val Test	V2	Something-Something V2
V2 Dataset (Both Top1 and Top5	V2	Something-Something V2
predict human-object interactions in the Something-Something dataset and identify various human	Something-Something	Something-Something V2
on three recent video datasets (Something-Something [9], Jester [10], and Charades	Something-Something	Something-Something V2
on sequential activity recog- nition: Something-Something dataset [9] is collected for	Something-Something	Something-Something V2
highly competitive results on the Something-Something dataset for human-interaction recognition [9	Something-Something	Something-Something V2
statistics of the three datasets Something-Something dataset (Something- V1 [9] and	Something-Something	Something-Something V2
3.2 Results on Something-Something Dataset	Something-Something	Something-Something V2
Something-Something is a recent video dataset	Something-Something	Something-Something V2
3: Prediction examples on a) Something-Something, b) Jester, and c) Cha	Something-Something	Something-Something V2
For each example drawn from Something-Something and Jester, the top two	Something-Something	Something-Something V2
the validation set of the Something-Something dataset	Something-Something	Something-Something V2
of videos from the (a) Something-Something and (b) Jester datasets using	Something-Something	Something-Something V2
The significant difference on the Something-Something dataset shows the importance of	Something-Something	Something-Something V2
shuffled inputs drawn from the Something-Something dataset, in Figure 6b. In	Something-Something	Something-Something V2
Something-Something Ordered Shuffled	Something-Something	Something-Something V2
frames and shuffled frames, on Something-Something and UCF101 dataset respectively. On	Something-Something	Something-Something V2
can better differentiate activities in Something-Something dataset	Something-Something	Something-Something V2
three recent video datasets - Something	Something	Something-Something V2
- Something, Jester, and Charades - which	Something	Something-Something V2
predict human-object interactions in the Something	Something	Something-Something V2
Something dataset and identify various human	Something	Something-Something V2
on three recent video datasets (Something	Something	Something-Something V2
Something [9], Jester [10], and Charades	Something	Something-Something V2
on sequential activity recog- nition: Something	Something	Something-Something V2
Something dataset [9] is collected for	Something	Something-Something V2
highly competitive results on the Something	Something	Something-Something V2
Something dataset for human-interaction recognition [9	Something	Something-Something V2
statistics of the three datasets Something	Something	Something-Something V2
-Something dataset (Something	Something	Something-Something V2
- V1 [9] and Something	Something	Something-Something V2
-V2 [28] where the Something	Something	Something-Something V2
Something-V1 174 108,499 human-object interaction Something	Something	Something-Something V2
3.2 Results on Something	Something	Something-Something V2
Something Dataset	Something	Something-Something V2
Something	Something	Something-Something V2
Something is a recent video dataset	Something	Something-Something V2
are challenging, such as ‘Tearing Something into two pieces’ versus ‘Tearing	Something	Something-Something V2
Something just a little bit’, ‘Turn	Something	Something-Something V2
set and test set of Something	Something	Something-Something V2
-V1 and Something	Something	Something-Something V2
the valida- tion set of Something	Something	Something-Something V2
-v1 and Something	Something	Something-Something V2
on the validation set of Something	Something	Something-Something V2
-V1 and Something	Something	Something-Something V2
Something-V1 Something	Something	Something-Something V2
and test set of the Something	Something	Something-Something V2
-V1 Dataset (Top1 Accuracy) and Something	Something	Something-Something V2
the validation set of the Something	Something	Something-Something V2
3: Prediction examples on a) Something	Something	Something-Something V2
Something, b) Jester, and c) Cha	Something	Something-Something V2
For each example drawn from Something	Something	Something-Something V2
Something and Jester, the top two	Something	Something-Something V2
highlight a pattern characteristic to Something	Something	Something-Something V2
the validation set of the Something	Something	Something-Something V2
Something dataset	Something	Something-Something V2
of videos from the (a) Something	Something	Something-Something V2
Something and (b) Jester datasets using	Something	Something-Something V2
The significant difference on the Something	Something	Something-Something V2
Something dataset shows the importance of	Something	Something-Something V2
shuffled inputs drawn from the Something	Something	Something-Something V2
Something dataset, in Figure 6b. In	Something	Something-Something V2
Something	Something	Something-Something V2
Something Ordered Shuffled	Something	Something-Something V2
frames and shuffled frames, on Something	Something	Something-Something V2
Something and UCF101 dataset respectively. On	Something	Something-Something V2
Something- Something, the temporal order is critical	Something	Something-Something V2
Dataset UCF Kinetics Moments Something Jester Charades	Something	Something-Something V2
can better differentiate activities in Something	Something	Something-Something V2
Something dataset	Something	Something-Something V2
using the MultiScale TRN on Something	Something	Something-Something V2
- Something and Jester dataset. Only the	Something	Something-Something V2
Something Jester Frames baseline TRN baseline	Something	Something-Something V2
Something V2 [12]. We show that when	Something V2	Something-Something V2
Something V2 [12	Something V2	Something-Something V2
Something V2 [12] are constructed to serve	Something V2	Something-Something V2
Something V2, we construct a few-shot dataset	Something V2	Something-Something V2
Something V2 dataset incorporates an assumption of	Something V2	Something-Something V2
Kinetics Something V2 Method 1-shot 5-shot 1-shot 5-shot	Something V2	Something-Something V2
both the Kinetics and Something- Something V2 datasets are listed in Table	Something V2	Something-Something V2
Kinetics Something V2 matching type 1-shot 5-shot 1-shot	Something V2	Something-Something V2
Something V2 than that of Kinetics, so	Something V2	Something-Something V2
Something V2, while the gap is closed	Something V2	Something-Something V2
Something V2	Something V2	Something-Something V2
recognition datasets: Kinetics [17] and Something-Something V2 [12]. We show that when	Something-Something V2	Something-Something V2
recognition datasets, Kinetics [17] and Something-Something V2 [12	Something-Something V2	Something-Something V2
Kinetics [17] and Something-Something V2 [12] are constructed to serve	Something-Something V2	Something-Something V2
split for few-shot classification on Something-Something V2, we construct a few-shot dataset	Something-Something V2	Something-Something V2
training. Since the label in Something-Something V2 dataset incorporates an assumption of	Something-Something V2	Something-Something V2
clues in each frame of Something-Something V2 than that of Kinetics, so	Something-Something V2	Something-Something V2
using Mean is prominent for Something-Something V2, while the gap is closed	Something-Something V2	Something-Something V2
Something-Something V2	Something-Something V2	Something-Something V2
V2, and show that our model	V2	Something-Something V2
datasets: Kinetics [17] and Something-Something V2 [12]. We show that when	V2	Something-Something V2
V2 [12], in which the videos	V2	Something-Something V2
datasets, Kinetics [17] and Something-Something V2 [12	V2	Something-Something V2
Kinetics [17] and Something-Something V2 [12] are constructed to serve	V2	Something-Something V2
for few-shot classification on Something-Something V2, we construct a few-shot dataset	V2	Something-Something V2
Since the label in Something-Something V2 dataset incorporates an assumption of	V2	Something-Something V2
Kinetics Something V2 Method 1-shot 5-shot 1-shot 5-shot	V2	Something-Something V2
the Kinetics and Something- Something V2 datasets are listed in Table	V2	Something-Something V2
Kinetics Something V2 matching type 1-shot 5-shot 1-shot	V2	Something-Something V2
in each frame of Something-Something V2 than that of Kinetics, so	V2	Something-Something V2
Mean is prominent for Something-Something V2, while the gap is closed	V2	Something-Something V2
Something-Something V2	V2	Something-Something V2
recognition datasets: Kinetics [17] and Something-Something V2 [12]. We show that when	Something-Something V	Something-Something V2
crowd-sourced videos: Jester[1], Charades [34], Something-Something V1	Something-Something V	Something-Something V2
recognition datasets, Kinetics [17] and Something-Something V2 [12	Something-Something V	Something-Something V2
Kinetics [17] and Something-Something V2 [12] are constructed to serve	Something-Something V	Something-Something V2
split for few-shot classification on Something-Something V2, we construct a few-shot dataset	Something-Something V	Something-Something V2
training. Since the label in Something-Something V2 dataset incorporates an assumption of	Something-Something V	Something-Something V2
clues in each frame of Something-Something V2 than that of Kinetics, so	Something-Something V	Something-Something V2
using Mean is prominent for Something-Something V2, while the gap is closed	Something-Something V	Something-Something V2
Something-Something V2	Something-Something V	Something-Something V2
recognition datasets: Kinetics [17] and Something-Something V2 [12]. We show that	Something-Something	Something-Something V2
crowd-sourced videos: Jester[1], Charades [34], Something-Something V1&V2 [12], in which the	Something-Something	Something-Something V2
recognition datasets, Kinetics [17] and Something-Something V2 [12	Something-Something	Something-Something V2
Kinetics [17] and Something-Something V2 [12] are constructed to	Something-Something	Something-Something V2
split for few-shot classification on Something-Something V2, we construct a few-shot	Something-Something	Something-Something V2
training. Since the label in Something-Something V2 dataset incorporates an assumption	Something-Something	Something-Something V2
clues in each frame of Something-Something V2 than that of Kinetics	Something-Something	Something-Something V2
using Mean is prominent for Something-Something V2, while the gap is	Something-Something	Something-Something V2
Something-Something V2	Something-Something	Something-Something V2
challenging real-world datasets, Kinetics and Something	Something	Something-Something V2
- Something	Something	Something-Something V2
recognition datasets: Kinetics [17] and Something	Something	Something-Something V2
Something V2 [12]. We show that	Something	Something-Something V2
crowd-sourced videos: Jester[1], Charades [34], Something	Something	Something-Something V2
Something V1&V2 [12], in which the	Something	Something-Something V2
recognition datasets, Kinetics [17] and Something	Something	Something-Something V2
Something V2 [12	Something	Something-Something V2
Kinetics [17] and Something	Something	Something-Something V2
Something V2 [12] are constructed to	Something	Something-Something V2
split for few-shot classification on Something	Something	Something-Something V2
Something V2, we construct a few-shot	Something	Something-Something V2
training. Since the label in Something	Something	Something-Something V2
Something V2 dataset incorporates an assumption	Something	Something-Something V2
Kinetics Something V2 Method 1-shot 5-shot 1-shot	Something	Something-Something V2
on both the Kinetics and Something	Something	Something-Something V2
- Something V2 datasets are listed in	Something	Something-Something V2
Kinetics Something V2 matching type 1-shot 5-shot	Something	Something-Something V2
clues in each frame of Something	Something	Something-Something V2
Something V2 than that of Kinetics	Something	Something-Something V2
using Mean is prominent for Something	Something	Something-Something V2
Something V2, while the gap is	Something	Something-Something V2
Something	Something	Something-Something V2
Something V2	Something	Something-Something V2
Something-Something (ours) human-object interaction 108,499 4.03s	Something-Something	Something-Something V2
Something	Something	Something-Something V2
Something (ours) human-object interaction 108,499 4.03s	Something	Something-Something V2
Something	Something	Something-Something V2
Something	Something	Something-Something V2
Something	Something	Something-Something V2
Something	Something	Something-Something V2
Something	Something	Something-Something V2
surface without it falling down Something (not) falling over an edgeMoving	Something	Something-Something V2
Something	Something	Something-Something V2
like a feather or paper Something falling[Something] falling like a rock	Something	Something-Something V2
they collide with each other Something passing/hitting another thingMoving [something] and	Something	Something-Something V2
V2 [28]. For UCF101 and HMDB51	V2	Something-Something V2
V2) [28] dataset. 20BN-V2 has over	V2	Something-Something V2
V2 dataset [28]. We report top-1/top-5	V2	Something-Something V2
V2	V2	Something-Something V2
V2 in Table 3. With 1/5	V2	Something-Something V2
Something-Something [11, 28]) with almost no	Something-Something	Something-Something V2
on the chal- lenging 20BN Something-Something	Something-Something	Something-Something V2
Something	Something	Something-Something V2
Something [11, 28]) with almost no	Something	Something-Something V2
on the chal- lenging 20BN Something	Something	Something-Something V2
Something	Something	Something-Something V2
Evaluations of pipelines on the HMDB	HMDB	HMDB-51
HMDB	HMDB	HMDB-51
Evaluations of pipelines on the HMDB	HMDB	HMDB-51
1 presents results on the HMDB	HMDB	HMDB-51
the classification accuracy on the HMDB	HMDB	HMDB-51
Figure 5b illustrates on the HMDB	HMDB	HMDB-51
4 shows results on the HMDB	HMDB	HMDB-51
fig. 5b) sketching on the HMDB	HMDB	HMDB-51
state of the art on HMDB	HMDB	HMDB-51
For HMDB	HMDB	HMDB-51
and ground truth representations on HMDB	HMDB	HMDB-51
Tomaso Poggio, and Thomas Serre. HMDB	HMDB	HMDB-51
Evaluations of pipelines on the HMDB-51 dataset. We compare (HAF only	HMDB-	HMDB-51
HMDB-51 [41] consists of 6766 internet	HMDB-	HMDB-51
Evaluations of pipelines on the HMDB-51 dataset. We compare (HAF+BoW/FV halluc	HMDB-	HMDB-51
1 presents results on the HMDB-51 dataset. As expected, the (HAF	HMDB-	HMDB-51
the classification accuracy on the HMDB-51 dataset (split 1) when our	HMDB-	HMDB-51
Figure 5b illustrates on the HMDB-51 dataset (split 1) that applying	HMDB-	HMDB-51
4 shows results on the HMDB-51 dataset. For	HMDB-	HMDB-51
fig. 5b) sketching on the HMDB-51 dataset (split 1 only	HMDB-	HMDB-51
state of the art on HMDB-51	HMDB-	HMDB-51
For HMDB-51 and YUP++, we use the	HMDB-	HMDB-51
and ground truth representations on HMDB-51 (split 1). Experi- ments in	HMDB-	HMDB-51
51	51	HMDB-51
51 dataset. We compare (HAF only	51	HMDB-51
51 [41] consists of 6766 internet	51	HMDB-51
videos over 51 classes; each video has ∼20–1000	51	HMDB-51
51 dataset. We compare (HAF+BoW/FV halluc	51	HMDB-51
51 dataset. As expected, the (HAF	51	HMDB-51
51 dataset (split 1) when our	51	HMDB-51
51 dataset (split 1) that applying	51	HMDB-51
51 dataset. For	51	HMDB-51
51 dataset (split 1 only	51	HMDB-51
51	51	HMDB-51
denotes human-centric pre-processing for 512 pixels (height	51	HMDB-51
firstly resize RGB frames to 512 pixels (height) rather than 256	51	HMDB-51
but computed for the increased 512 pixel resolution (height) denoted by	51	HMDB-51
51 and YUP++, we use the	51	HMDB-51
51 (split 1). Experi- ments in	51	HMDB-51
513, Mar. 2011. 3	51	HMDB-51
51	51	HMDB-51
Evaluations of pipelines on the HMDB-51 dataset. We compare (HAF only	HMDB-51	HMDB-51
HMDB-51 [41] consists of 6766 internet	HMDB-51	HMDB-51
Evaluations of pipelines on the HMDB-51 dataset. We compare (HAF+BoW/FV halluc	HMDB-51	HMDB-51
1 presents results on the HMDB-51 dataset. As expected, the (HAF	HMDB-51	HMDB-51
the classification accuracy on the HMDB-51 dataset (split 1) when our	HMDB-51	HMDB-51
Figure 5b illustrates on the HMDB-51 dataset (split 1) that applying	HMDB-51	HMDB-51
4 shows results on the HMDB-51 dataset. For	HMDB-51	HMDB-51
fig. 5b) sketching on the HMDB-51 dataset (split 1 only	HMDB-51	HMDB-51
state of the art on HMDB-51	HMDB-51	HMDB-51
For HMDB-51 and YUP++, we use the	HMDB-51	HMDB-51
and ground truth representations on HMDB-51 (split 1). Experi- ments in	HMDB-51	HMDB-51
classifying “drink” and “eat” from HMDB	HMDB	HMDB-51
from experiment of TSN on HMDB	HMDB	HMDB-51
baseline model TSN [38] on HMDB	HMDB	HMDB-51
240 spatial resolution. The HMDB	HMDB	HMDB-51
test set of UCF-101 and HMDB	HMDB	HMDB-51
in Table 1 on UCF-101, HMDB	HMDB	HMDB-51
on UCF-101 and by 22.9%on HMDB	HMDB	HMDB-51
on UCF-101 and 81.9% on HMDB	HMDB	HMDB-51
Overall, our result 81.9% on HMDB	HMDB	HMDB-51
METHODS ON THE UCF-101 AND HMDB	HMDB	HMDB-51
Methods Pre-train dataset UCF-101 HMDB	HMDB	HMDB-51
dataset, and 7.8% on the HMDB51, which demonstrates that conducting modality	HMDB	HMDB-51
COMPONENTS ON THE UCF-101 , HMDB	HMDB	HMDB-51
achieves the best result on HMDB	HMDB	HMDB-51
classifying “drink” and “eat” from HMDB-51 [15] dataset. The input consist	HMDB-	HMDB-51
from experiment of TSN on HMDB-51 dataset	HMDB-	HMDB-51
baseline model TSN [38] on HMDB-51 [15], of that case. Our	HMDB-	HMDB-51
240 spatial resolution. The HMDB-51 dataset has 6766 video clips	HMDB-	HMDB-51
test set of UCF-101 and HMDB-51 datasets	HMDB-	HMDB-51
in Table 1 on UCF-101, HMDB-51 (split 1) and something-something-V2 dataset	HMDB-	HMDB-51
on UCF-101 and by 22.9%on HMDB-51	HMDB-	HMDB-51
on UCF-101 and 81.9% on HMDB-51	HMDB-	HMDB-51
Overall, our result 81.9% on HMDB-51 clearly sets a new state-of-the-art	HMDB-	HMDB-51
METHODS ON THE UCF-101 AND HMDB-51 DATASETS (SPLIT 1). WE REPORT	HMDB-	HMDB-51
Methods Pre-train dataset UCF-101 HMDB-	HMDB-	HMDB-51
COMPONENTS ON THE UCF-101 , HMDB-51 DATASETS. “METHOD” DENOTES THE COMPONENT	HMDB-	HMDB-51
achieves the best result on HMDB- 51 dataset. This verifies that	HMDB-	HMDB-51
51 [15] dataset. The input consist	51	HMDB-51
51 dataset	51	HMDB-51
51 [15], of that case. Our	51	HMDB-51
51 dataset has 6766 video clips	51	HMDB-51
with 51 categories. something-something-v2 an interesting temporal	51	HMDB-51
51 datasets	51	HMDB-51
51 (split 1) and something-something-V2 dataset	51	HMDB-51
51	51	HMDB-51
51	51	HMDB-51
51 clearly sets a new state-of-the-art	51	HMDB-51
51 DATASETS (SPLIT 1). WE REPORT	51	HMDB-51
30] sports-1M 82.3 - - 51	51	HMDB-51
Baseline 51	51	HMDB-51
51 DATASETS. “METHOD” DENOTES THE COMPONENT	51	HMDB-51
the best result on HMDB- 51 dataset. This verifies that the	51	HMDB-51
516	51	HMDB-51
classifying “drink” and “eat” from HMDB-51 [15] dataset. The input consist	HMDB-51	HMDB-51
from experiment of TSN on HMDB-51 dataset	HMDB-51	HMDB-51
baseline model TSN [38] on HMDB-51 [15], of that case. Our	HMDB-51	HMDB-51
240 spatial resolution. The HMDB-51 dataset has 6766 video clips	HMDB-51	HMDB-51
test set of UCF-101 and HMDB-51 datasets	HMDB-51	HMDB-51
in Table 1 on UCF-101, HMDB-51 (split 1) and something-something-V2 dataset	HMDB-51	HMDB-51
on UCF-101 and by 22.9%on HMDB-51	HMDB-51	HMDB-51
on UCF-101 and 81.9% on HMDB-51	HMDB-51	HMDB-51
Overall, our result 81.9% on HMDB-51 clearly sets a new state-of-the-art	HMDB-51	HMDB-51
METHODS ON THE UCF-101 AND HMDB-51 DATASETS (SPLIT 1). WE REPORT	HMDB-51	HMDB-51
COMPONENTS ON THE UCF-101 , HMDB-51 DATASETS. “METHOD” DENOTES THE COMPONENT	HMDB-51	HMDB-51
larly on HMDB51 (split-1), MARS obtains 80.1% ac	HMDB	HMDB-51
art on the HMDB51 [20] and UCF101 [33] datasets	HMDB	HMDB-51
tion: Kinetics400 [18], HMDB51 [20], UCF101 [33], and	HMDB	HMDB-51
troduced by [44]. HMDB51 consists of 51 action classes	HMDB	HMDB-51
the first split for HMDB51 and UCF101 as HMDB51-1 and	HMDB	HMDB-51
UCF101, and HMDB51 [11]. Following the setting of	HMDB	HMDB-51
the smaller HMDB51 and UCF101 datasets	HMDB	HMDB-51
Stream MiniKinetics Kinetics400 UCF101-1 HMDB51	HMDB	HMDB-51
trained from scratch. For UCF101-1, HMDB51	HMDB	HMDB-51
Stream Kinetics400 UCF101-1 HMDB51	HMDB	HMDB-51
HMDB51	HMDB	HMDB-51
RGB on Kinetics400, UCF101- 1, HMDB51	HMDB	HMDB-51
streams alone on MiniKinetics, UCF101, HMDB51	HMDB	HMDB-51
HMDB51	HMDB	HMDB-51
In some cases, e.g., on HMDB51 with 64f	HMDB	HMDB-51
ble 4 for UCF101, HMDB51 and SomethingSomethingv1	HMDB	HMDB-51
where motion is important, as HMDB51	HMDB	HMDB-51
not important. In contrast, on HMDB51 and SomethingSomethingv1, the gain is	HMDB	HMDB-51
Method Streams Pretrain UCF101 HMDB51 Something	HMDB	HMDB-51
sults of UCF101 and HMDB51 are averaged over 3 splits	HMDB	HMDB-51
ics400, UCF101, HMDB51, and SomethingSomethingv1	HMDB	HMDB-51
Tomaso Poggio, and Thomas Serre. HMDB	HMDB	HMDB-51
by [44]. HMDB51 consists of 51 action classes	51	HMDB-51
RGB+Flow 74.5 97.5 79.8 51	51	HMDB-51
MARS+RGB 74.8 97.3 80.6 51	51	HMDB-51
MARS+RGB RGB Kinetics 97.6 79.5 51	51	HMDB-51
on challenging human action datasets: HMDB51, UCF101, and Kinetics. The dataset	HMDB	HMDB-51
achieves state-of-the-art results on the HMDB51, UCF101 and Kinetics datasets. In	HMDB	HMDB-51
remarkable performance on UCF101 (96.9%), HMDB51 (74.5%) and Kinetics (73.5	HMDB	HMDB-51
influential action datasets available. The HMDB51 [24] and UCF101 [37] datasets	HMDB	HMDB-51
Event Attribute Concept #Videos Year HMDB51 [24] - - 51	HMDB	HMDB-51
first ones were KTH [25], HMDB51 [24], and UCF101 [37] that	HMDB	HMDB-51
Pre-Training Dataset UCF101 HMDB51 Kinetics From Scratch 65.2 33.4	HMDB	HMDB-51
Pre-Trained Dataset CNN Backbone UCF101 HMDB51 Kinetics Two Stream (spatial stream	HMDB	HMDB-51
State-of-the-art performance comparison on UCF101, HMDB51 test sets and Kinetics validation	HMDB	HMDB-51
The results on UCF101 and HMDB51 are average mAP over three	HMDB	HMDB-51
and then fine-tuning on UCF101, HMDB51 and Kinetics. Obvi- ously, there	HMDB	HMDB-51
datasets (i.e. HVU, Kinetics⇒UCF101 and HMDB51	HMDB	HMDB-51
5.5. Comparison on UCF, HMDB, Kinetics	HMDB	HMDB-51
the state-of-the-art methods on UCF101, HMDB51 and Ki- netics. For our	HMDB	HMDB-51
target datasets. For UCF101 and HMDB51, we report the average accuracy	HMDB	HMDB-51
5.5 . Comparison on UCF, HMDB, Kinetics	HMDB	HMDB-51
10, 16, 38, 39, 48, 51	51	HMDB-51
51	51	HMDB-51
Year HMDB51 [24] - - 51 - - - 7K ’11	51	HMDB-51
C3D [40] Sport1M VGG11 82.3 51	51	HMDB-51
51	51	HMDB-51
such as Kinetics, UCF-101 and HMDB	HMDB	HMDB-51
results on UCF-101 [31] and HMDB	HMDB	HMDB-51
art on Kinetics, UCF-101, and HMDB	HMDB	HMDB-51
Model Pre-train UCF-101 HMDB	HMDB	HMDB-51
8: Comparisons on UCF-101 and HMDB	HMDB	HMDB-51
datasets, i.e., Kinetics, UCF-101 and HMDB	HMDB	HMDB-51
also achieved on UCF-101 and HMDB	HMDB	HMDB-51
such as Kinetics, UCF-101 and HMDB-51	HMDB-	HMDB-51
results on UCF-101 [31] and HMDB-51 [23]. These datasets are much	HMDB-	HMDB-51
art on Kinetics, UCF-101, and HMDB-51	HMDB-	HMDB-51
Model Pre-train UCF-101 HMDB-	HMDB-	HMDB-51
8: Comparisons on UCF-101 and HMDB-51	HMDB-	HMDB-51
datasets, i.e., Kinetics, UCF-101 and HMDB-51	HMDB-	HMDB-51
also achieved on UCF-101 and HMDB-51 (Table 8). It shows learning	HMDB-	HMDB-51
51	51	HMDB-51
1×1×1, 1281×3×3, 128 1×1×1, 512	51	HMDB-51
1×3×3, 288 3×1×1, 128 1×1×1, 512	51	HMDB-51
1×1×1, 5121×3×3, 512 1×1×1, 2048	51	HMDB-51
1×1×1, 512 1×3×3, 1152 3×1×1, 512	51	HMDB-51
51 [23]. These datasets are much	51	HMDB-51
51	51	HMDB-51
51	51	HMDB-51
51	51	HMDB-51
51 (Table 8). It shows learning	51	HMDB-51
such as Kinetics, UCF-101 and HMDB-51	HMDB-51	HMDB-51
results on UCF-101 [31] and HMDB-51 [23]. These datasets are much	HMDB-51	HMDB-51
art on Kinetics, UCF-101, and HMDB-51	HMDB-51	HMDB-51
8: Comparisons on UCF-101 and HMDB-51	HMDB-51	HMDB-51
datasets, i.e., Kinetics, UCF-101 and HMDB-51	HMDB-51	HMDB-51
also achieved on UCF-101 and HMDB-51 (Table 8). It shows learning	HMDB-51	HMDB-51
HMDB	HMDB	HMDB-51
exper- iment 10-times on the HMDB	HMDB	HMDB-51
experiments use ResNet-152 features on HMDB	HMDB	HMDB-51
HMDB	HMDB	HMDB-51
HMDB	HMDB	HMDB-51
We used three splits for HMDB	HMDB	HMDB-51
achieve 9% improvement on the HMDB	HMDB	HMDB-51
on each dataset. On the HMDB	HMDB	HMDB-51
and when fine-tuned on the HMDB	HMDB	HMDB-51
sequences of increasing lengths in HMDB	HMDB	HMDB-51
81.5%) is not significant (on HMDB	HMDB	HMDB-51
To this end, we re-categorized HMDB	HMDB	HMDB-51
raw RGB frames) on an HMDB	HMDB	HMDB-51
to raw RGB frames from HMDB	HMDB	HMDB-51
E., Poggio, T., Serre, T.: HMDB	HMDB	HMDB-51
end-to-end learning setup on the HMDB	HMDB	HMDB-51
our end-to-end training setup on HMDB	HMDB	HMDB-51
performance of this experiment on HMDB	HMDB	HMDB-51
classifying the DSP descriptors (on HMDB	HMDB	HMDB-51
and SVM-based DSP classification on HMDB	HMDB	HMDB-51
L)inear and non-linear (NL) on HMDB	HMDB	HMDB-51
bi-directional) rank pooling (GRP) on HMDB	HMDB	HMDB-51
HMDB-51 [29]: is a popular video	HMDB-	HMDB-51
exper- iment 10-times on the HMDB-51 split-1 features. In Figure 2(a	HMDB-	HMDB-51
experiments use ResNet-152 features on HMDB-51 split-1 with a fooling rate	HMDB-	HMDB-51
HMDB-51 NTU-RGBD YUP	HMDB-	HMDB-51
HMDB-51	HMDB-	HMDB-51
We used three splits for HMDB-51	HMDB-	HMDB-51
achieve 9% improvement on the HMDB-51 dataset split-1 and 5%−8% improvement	HMDB-	HMDB-51
on each dataset. On the HMDB-51 dataset, we also report accuracy	HMDB-	HMDB-51
and when fine-tuned on the HMDB-51 leads to about 80.9% accuracy	HMDB-	HMDB-51
sequences of increasing lengths in HMDB-51 split-1	HMDB-	HMDB-51
81.5%) is not significant (on HMDB-51	HMDB-	HMDB-51
To this end, we re-categorized HMDB-51 into subsets of sequences according	HMDB-	HMDB-51
raw RGB frames) on an HMDB-51 video sequences. First column shows	HMDB-	HMDB-51
to raw RGB frames from HMDB-51 videos – i.e., instead of	HMDB-	HMDB-51
end-to-end learning setup on the HMDB-51 dataset split1 using a ResNet-152	HMDB-	HMDB-51
our end-to-end training setup on HMDB-51 split1	HMDB-	HMDB-51
performance of this experiment on HMDB-51 split-1. For the MLP, we	HMDB-	HMDB-51
classifying the DSP descriptors (on HMDB-51 split1	HMDB-	HMDB-51
and SVM-based DSP classification on HMDB-51 split-1 with ResNet152	HMDB-	HMDB-51
L)inear and non-linear (NL) on HMDB-51 split-1 with ResNet152. Right:Comparison of	HMDB-	HMDB-51
bi-directional) rank pooling (GRP) on HMDB-51 (two-stream ResNet-152), NTU, and YUP	HMDB-	HMDB-51
51 [29]: is a popular video	51	HMDB-51
of 6766 Internet videos over 51 classes; each video is about	51	HMDB-51
51 split-1 features. In Figure 2(a	51	HMDB-51
51 split-1 with a fooling rate	51	HMDB-51
51 NTU-RGBD YUP	51	HMDB-51
51	51	HMDB-51
51	51	HMDB-51
51 dataset split-1 and 5%−8% improvement	51	HMDB-51
51 dataset, we also report accuracy	51	HMDB-51
51	51	HMDB-51
51 leads to about 80.9% accuracy	51	HMDB-51
classes 51 49 34 27 23 21	51	HMDB-51
51 split-1	51	HMDB-51
51) in comparison to our results	51	HMDB-51
51 into subsets of sequences according	51	HMDB-51
51 video sequences. First column shows	51	HMDB-51
51 videos – i.e., instead of	51	HMDB-51
51	51	HMDB-51
51 dataset split1 using a ResNet-152	51	HMDB-51
51 split1	51	HMDB-51
51 split-1. For the MLP, we	51	HMDB-51
then passed through a d× 51 weight matrix learned against the	51	HMDB-51
51 split1	51	HMDB-51
51 split-1 with ResNet152	51	HMDB-51
51 split-1 with ResNet152. Right:Comparison of	51	HMDB-51
51 (two-stream ResNet-152), NTU, and YUP	51	HMDB-51
HMDB-51 [29]: is a popular video	HMDB-51	HMDB-51
exper- iment 10-times on the HMDB-51 split-1 features. In Figure 2(a	HMDB-51	HMDB-51
experiments use ResNet-152 features on HMDB-51 split-1 with a fooling rate	HMDB-51	HMDB-51
HMDB-51 NTU-RGBD YUP	HMDB-51	HMDB-51
HMDB-51	HMDB-51	HMDB-51
We used three splits for HMDB-51	HMDB-51	HMDB-51
achieve 9% improvement on the HMDB-51 dataset split-1 and 5%−8% improvement	HMDB-51	HMDB-51
on each dataset. On the HMDB-51 dataset, we also report accuracy	HMDB-51	HMDB-51
and when fine-tuned on the HMDB-51 leads to about 80.9% accuracy	HMDB-51	HMDB-51
sequences of increasing lengths in HMDB-51 split-1	HMDB-51	HMDB-51
81.5%) is not significant (on HMDB-51) in comparison to our results	HMDB-51	HMDB-51
To this end, we re-categorized HMDB-51 into subsets of sequences according	HMDB-51	HMDB-51
raw RGB frames) on an HMDB-51 video sequences. First column shows	HMDB-51	HMDB-51
to raw RGB frames from HMDB-51 videos – i.e., instead of	HMDB-51	HMDB-51
end-to-end learning setup on the HMDB-51 dataset split1 using a ResNet-152	HMDB-51	HMDB-51
our end-to-end training setup on HMDB-51 split1	HMDB-51	HMDB-51
performance of this experiment on HMDB-51 split-1. For the MLP, we	HMDB-51	HMDB-51
classifying the DSP descriptors (on HMDB-51 split1	HMDB-51	HMDB-51
and SVM-based DSP classification on HMDB-51 split-1 with ResNet152	HMDB-51	HMDB-51
L)inear and non-linear (NL) on HMDB-51 split-1 with ResNet152. Right:Comparison of	HMDB-51	HMDB-51
bi-directional) rank pooling (GRP) on HMDB-51 (two-stream ResNet-152), NTU, and YUP	HMDB-51	HMDB-51
case into Fig. UCF101 HMDB51	HMDB	HMDB-51
HMDB51 under different cross-stream connections	HMDB	HMDB-51
videos in 101 categories and HMDB51 [18], consisting	HMDB	HMDB-51
appearance, scale and pose. HMDB51 [18] is a more chal	HMDB	HMDB-51
HMDB51	HMDB	HMDB-51
of UCF101 and HMDB51, resp. In comparison, merely	HMDB	HMDB-51
first split of UCF101 and HMDB51	HMDB	HMDB-51
of a centre initialization on HMDB51	HMDB	HMDB-51
to the temporal nature of HMDB51 in	HMDB	HMDB-51
comparison to UCF101: HMDB51 exhibits a higher degree	HMDB	HMDB-51
temporal init. pool time UCF101 HMDB51	HMDB	HMDB-51
HMDB51 under different temporal filtering layers	HMDB	HMDB-51
UCF101 HMDB51	HMDB	HMDB-51
and HMDB51	HMDB	HMDB-51
for the spatial network on HMDB51, which might be due to	HMDB	HMDB-51
UCF101 and HMDB51, respectively. Here, the relative	HMDB	HMDB-51
and HMDB51 datasets. We train 50 and	HMDB	HMDB-51
diction layer outputs. On HMDB51 we weight the temporal	HMDB	HMDB-51
pearance network degrades performance on HMDB51 (we	HMDB	HMDB-51
sizable gain on HMDB51	HMDB	HMDB-51
Method UCF101 HMDB51	HMDB	HMDB-51
HMDB51 and UCF101	HMDB	HMDB-51
and 72.2% on HMDB51	HMDB	HMDB-51
still a 3.7% increase on HMDB51, which we think	HMDB	HMDB-51
to dominant camera motion in HMDB51 that	HMDB	HMDB-51
HMDB	HMDB	HMDB-51
1×1, 128 3×3, 128 1×1, 512	51	HMDB-51
1×1, 512 3×3, 512 1×1, 2048	51	HMDB-51
128 3×3, 128 ‡ 1×1, 512	51	HMDB-51
1×1, 512 3×3, 512	51	HMDB-51
1×1, 128 3×3, 128 1×1, 512	51	HMDB-51
1×1, 512 3×3, 512 1×1, 2048	51	HMDB-51
of 6766 videos in 51 categories. UCF101 provides videos	51	HMDB-51
511	51	HMDB-51
513	51	HMDB-51
i.e., Kinetics- 400, UCF-101, and HMDB	HMDB	HMDB-51
Jester [1], UCF101 [23] and HMDB	HMDB	HMDB-51
datasets (i.e., Kinetics-400, UCF-101, and HMDB	HMDB	HMDB-51
400 [2], UCF-101 [23] and HMDB	HMDB	HMDB-51
randomly initialized. For UCF-101 and HMDB	HMDB	HMDB-51
the STM on UCF-101 and HMDB	HMDB	HMDB-51
Backbone Flow Pre-train Data UCF-101 HMDB	HMDB	HMDB-51
scene-related datasets: Kinetics-400, UCF-101, and HMDB	HMDB	HMDB-51
and 13,320 clips in total. HMDB	HMDB	HMDB-51
video clips. For UCF-101 and HMDB	HMDB	HMDB-51
experiments on the UCF-101 and HMDB	HMDB	HMDB-51
T. Poggio, and T. Serre. HMDB	HMDB	HMDB-51
i.e., Kinetics- 400, UCF-101, and HMDB-51	HMDB-	HMDB-51
Jester [1], UCF101 [23] and HMDB-51 [17	HMDB-	HMDB-51
datasets (i.e., Kinetics-400, UCF-101, and HMDB-51	HMDB-	HMDB-51
400 [2], UCF-101 [23] and HMDB-51 [17] where the back- ground	HMDB-	HMDB-51
randomly initialized. For UCF-101 and HMDB-51, we use Kinetics pre-trained model	HMDB-	HMDB-51
the STM on UCF-101 and HMDB-51 compared with the state-of-the-art methods	HMDB-	HMDB-51
Backbone Flow Pre-train Data UCF-101 HMDB-51	HMDB-	HMDB-51
scene-related datasets: Kinetics-400, UCF-101, and HMDB-51 in this section. Kinetics-400 is	HMDB-	HMDB-51
and 13,320 clips in total. HMDB-51 is also a small video	HMDB-	HMDB-51
video clips. For UCF-101 and HMDB-51, we followed [33] to adopt	HMDB-	HMDB-51
experiments on the UCF-101 and HMDB-51 to study the generalization ability	HMDB-	HMDB-51
51) with the help of encoding	51	HMDB-51
51 [17	51	HMDB-51
51	51	HMDB-51
51 [17] where the back- ground	51	HMDB-51
51, we use Kinetics pre-trained model	51	HMDB-51
51 compared with the state-of-the-art methods	51	HMDB-51
51	51	HMDB-51
27] 3D VGG-11 Sports-1M 82.3 51	51	HMDB-51
51 in this section. Kinetics-400 is	51	HMDB-51
51 is also a small video	51	HMDB-51
dataset with 51 classes and 6766 labeled video	51	HMDB-51
51, we followed [33] to adopt	51	HMDB-51
51 to study the generalization ability	51	HMDB-51
516 66.5G 32.0 V/s 49.8	51	HMDB-51
i.e., Kinetics- 400, UCF-101, and HMDB-51) with the help of encoding	HMDB-51	HMDB-51
Jester [1], UCF101 [23] and HMDB-51 [17	HMDB-51	HMDB-51
datasets (i.e., Kinetics-400, UCF-101, and HMDB-51	HMDB-51	HMDB-51
400 [2], UCF-101 [23] and HMDB-51 [17] where the back- ground	HMDB-51	HMDB-51
randomly initialized. For UCF-101 and HMDB-51, we use Kinetics pre-trained model	HMDB-51	HMDB-51
the STM on UCF-101 and HMDB-51 compared with the state-of-the-art methods	HMDB-51	HMDB-51
Backbone Flow Pre-train Data UCF-101 HMDB-51	HMDB-51	HMDB-51
scene-related datasets: Kinetics-400, UCF-101, and HMDB-51 in this section. Kinetics-400 is	HMDB-51	HMDB-51
and 13,320 clips in total. HMDB-51 is also a small video	HMDB-51	HMDB-51
video clips. For UCF-101 and HMDB-51, we followed [33] to adopt	HMDB-51	HMDB-51
experiments on the UCF-101 and HMDB-51 to study the generalization ability	HMDB-51	HMDB-51
across major datasets (UCF101 [41], HMDB51 [22] and 20BN-Something-Something [11, 28	HMDB	HMDB-51
Method Mean Class AccuracyUCF101 HMDB51	HMDB	HMDB-51
HMDB51	HMDB	HMDB-51
for this experiment: UCF101 and HMDB51	HMDB	HMDB-51
videos from 101 action categories. HMDB51 [22] includes 6,766 videos from	HMDB	HMDB-51
flow stream on UCF101 and HMDB51 datasets. We also include results	HMDB	HMDB-51
stream. For both UCF101 and HMDB51, the best performing method is	HMDB	HMDB-51
performance of RGB stream on HMDB51	HMDB	HMDB-51
action recognition on UCF101 [41], HMDB51 [22] and a large scale	HMDB	HMDB-51
dataset–20BN-V2 [28]. For UCF101 and HMDB51, we report mean class accu	HMDB	HMDB-51
HMDB51	HMDB	HMDB-51
HMDB51	HMDB	HMDB-51
Method Mean Class AccuracyUCF101 HMDB51	HMDB	HMDB-51
action recognition on UCF101 and HMDB51	HMDB	HMDB-51
bet- ter than ResNet50 on HMDB51 [14]. The performance of our	HMDB	HMDB-51
on UCF101 and -4.1% on HMDB51	HMDB	HMDB-51
Method Mean Class AccuracyUCF101 HMDB51 I3D RGB (backbone) 94.8 70.9	HMDB	HMDB-51
HMDB51	HMDB	HMDB-51
on UCF101 and +1.3% on HMDB51	HMDB	HMDB-51
HMDB51 I3D RGB 70.9 70.2 0.7	HMDB	HMDB-51
testing videos of UCF101 and HMDB51	HMDB	HMDB-51
HMDB51, under-performing our model by	HMDB	HMDB-51
HMDB51	HMDB	HMDB-51
T. Poggio, and T. Serre. HMDB	HMDB	HMDB-51
51	51	HMDB-51
51	51	HMDB-51
differs from [12, 9, 26, 51	51	HMDB-51
51, 12, 26] did not consider	51	HMDB-51
51, 26	51	HMDB-51
22] includes 6,766 videos from 51 action cat- egories. We evaluate	51	HMDB-51
51	51	HMDB-51
51	51	HMDB-51
51	51	HMDB-51
51	51	HMDB-51
51	51	HMDB-51
51	51	HMDB-51
51	51	HMDB-51
Saliency Map (DSS [17]) 51	51	HMDB-51
51	51	HMDB-51
a) Something-v1 (b) EPIC-KITCHENS (c) HMDB51	HMDB	HMDB-51
Something-v1 [3], EPIC-KITCHENS [23] and HMDB51 [24]. Something-v1 consists of 86K	HMDB	HMDB-51
obtained on the test set. HMDB51 dataset consists of videos collected	HMDB	HMDB-51
to both actions and objects. HMDB51 is a smaller dataset with	HMDB	HMDB-51
Model Backbone Pre-training Accuracy (%)Something-v1 HMDB51 TDN [2] ResNet-50 ImageNet	HMDB	HMDB-51
validation set of something-v1 and HMDB51 datasets	HMDB	HMDB-51
a) Something-v1 dataset (b) HMDB51 dataset	HMDB	HMDB-51
on (a) Something-v1 and (b) HMDB51, plotted against the computational complexity	HMDB	HMDB-51
Something-v1 and HMDB51	HMDB	HMDB-51
state-of-the-art techniques on Something-v1 and HMDB51 datasets. For Something-v1 dataset, in	HMDB	HMDB-51
For HMDB51, we augment three baselines, TSN	HMDB	HMDB-51
three baselines. As mentioned previously, HMDB51 consists of actions with shorter	HMDB	HMDB-51
HMDB51	HMDB	HMDB-51
around 6000 video clips from 51 action categories. The dataset is	51	HMDB-51
51 2.06 TSN (FUSION) [1] 48.23	51	HMDB-51
51 6.73 73.40 33.77 18.64 19.98	51	HMDB-51
of experiments on UCF101 and HMDB51 datasets. Our experiments show that	HMDB	HMDB-51
public datasets UCF101 [24] and HMDB51 [25	HMDB	HMDB-51
two standard datasets UCF101 and HMDB51	HMDB	HMDB-51
accuracy over the three splits. HMDB51 con- tains 6766 videos clips	HMDB	HMDB-51
are performed on UCF101 and HMDB51 datasets (over all splits). The	HMDB	HMDB-51
sample segments on UCF101 and HMDB51 dataset (over all splits). We	HMDB	HMDB-51
module settings on UCF101 and HMDB51 datasets (over all splits). ‘‘Baseline	HMDB	HMDB-51
exper- iment with UCF101 and HMDB51 datasets (over all splits), and	HMDB	HMDB-51
of layers on UCF101 and HMDB51 datasets (over all splits). We	HMDB	HMDB-51
is performed on UCF101 and HMDB51 datasets (over all splits). As	HMDB	HMDB-51
methods on the UCF101 and HMDB51 datasets (over all splits). The	HMDB	HMDB-51
the experiments on UCF101 and HMDB51 datasets with R-STAN-101 framework. Using	HMDB	HMDB-51
HMDB	HMDB	HMDB-51
The videos are divided into 51 classes indicate 51 action categories	51	HMDB-51
recognition task we employ the HMDB51 dataset [38] that includes 6766	HMDB	HMDB-51
task over all splits of HMDB51	HMDB	HMDB-51
of our method on the HMDB51	HMDB	HMDB-51
lower recognition scores (comparing with HMDB51	HMDB	HMDB-51
51	51	HMDB-51
5179 2.1607 0.8599 0.6400 0.3593 2.0644	51	HMDB-51
that includes 6766 video from 51 human action classes. We decided	51	HMDB-51
Kinetics 62.7 C3D [55] Sports-1M 51	51	HMDB-51
51	51	HMDB-51
1-task multi BMI - - 51	51	HMDB-51
5142	51	HMDB-51
5154, 2018	51	HMDB-51
51	51	HMDB-51
of experiments on UCF101 and HMDB51 datasets. Our experiments show that	HMDB	HMDB-51
public datasets UCF101 [24] and HMDB51 [25	HMDB	HMDB-51
two standard datasets UCF101 and HMDB51	HMDB	HMDB-51
accuracy over the three splits. HMDB51 con- tains 6766 videos clips	HMDB	HMDB-51
are performed on UCF101 and HMDB51 datasets (over all splits). The	HMDB	HMDB-51
sample segments on UCF101 and HMDB51 dataset (over all splits). We	HMDB	HMDB-51
module settings on UCF101 and HMDB51 datasets (over all splits). ‘‘Baseline	HMDB	HMDB-51
exper- iment with UCF101 and HMDB51 datasets (over all splits), and	HMDB	HMDB-51
of layers on UCF101 and HMDB51 datasets (over all splits). We	HMDB	HMDB-51
is performed on UCF101 and HMDB51 datasets (over all splits). As	HMDB	HMDB-51
methods on the UCF101 and HMDB51 datasets (over all splits). The	HMDB	HMDB-51
the experiments on UCF101 and HMDB51 datasets with R-STAN-101 framework. Using	HMDB	HMDB-51
HMDB	HMDB	HMDB-51
The videos are divided into 51 classes indicate 51 action categories	51	HMDB-51
over from-scratch training on the HMDB dataset, getting almost half-way to	HMDB	HMDB-51
experiment on smaller datasets like HMDB, we use Sync-SGD with 8	HMDB	HMDB-51
art on UCF-101 [46] and HMDB	HMDB	HMDB-51
in-the-wild videos. Target test videos: HMDB	HMDB	HMDB-51
3 splits. We use the HMDB split 1 for ablative experiments	HMDB	HMDB-51
performance on all splits for HMDB and UCF in Section 4.4	HMDB	HMDB-51
using percentage accuracy on the HMDB	HMDB	HMDB-51
dot represents a video from HMDB training set, and is color	HMDB	HMDB-51
evaluated using percentage accuracy on HMDB	HMDB	HMDB-51
evaluated using percentage accuracy on HMDB	HMDB	HMDB-51
DistInit, and show the downstream HMDB	HMDB	HMDB-51
that set. Performance reported on HMDB	HMDB	HMDB-51
it two times. Reported on HMDB	HMDB	HMDB-51
PlaceNet [60], and obtain 36.8% HMDB fine-tuning performance as opposed to	HMDB	HMDB-51
a) HMDB	HMDB	HMDB-51
Comparison with previous work on HMDB and UCF. We split the	HMDB	HMDB-51
and 95.1% 3-split avg on HMDB and UCF, but is not	HMDB	HMDB-51
models and initialization methods on HMDB and UCF. For these comparisons	HMDB	HMDB-51
Figure 7: HMDB classes with largest gain using	HMDB	HMDB-51
of inflation. The plot shows HMDB per-class accuracy differ- ence between	HMDB	HMDB-51
Tomaso Poggio, and Thomas Serre. HMDB	HMDB	HMDB-51
art on UCF-101 [46] and HMDB-51 [28] in Section 4.4	HMDB-	HMDB-51
in-the-wild videos. Target test videos: HMDB-51 [28] contains 6766 realistic and	HMDB-	HMDB-51
using percentage accuracy on the HMDB-51 dataset, Split 1. The models	HMDB-	HMDB-51
evaluated using percentage accuracy on HMDB-51 split 1	HMDB-	HMDB-51
evaluated using percentage accuracy on HMDB-51 split 1	HMDB-	HMDB-51
DistInit, and show the downstream HMDB-51 split-1 performance in the line	HMDB-	HMDB-51
that set. Performance reported on HMDB-51 split 1	HMDB-	HMDB-51
it two times. Reported on HMDB-51 split 1	HMDB-	HMDB-51
a) HMDB-51 Model Architecture #frames Pre-training Split	HMDB-	HMDB-51
8, 9, 15, 16, 44, 51	51	HMDB-51
51	51	HMDB-51
51	51	HMDB-51
51, 56]. However, these models no	51	HMDB-51
51	51	HMDB-51
51	51	HMDB-51
51 [28] in Section 4.4	51	HMDB-51
51 [28] contains 6766 realistic and	51	HMDB-51
varied video clips from 51 action classes. Evaluation is performed	51	HMDB-51
R(2+1)D-18 Kinetics pre-training - 51	51	HMDB-51
51 dataset, Split 1. The models	51	HMDB-51
51	51	HMDB-51
51 split 1	51	HMDB-51
51 split 1	51	HMDB-51
51	51	HMDB-51
51 split-1 performance in the line	51	HMDB-51
51 split 1	51	HMDB-51
51 split 1	51	HMDB-51
51	51	HMDB-51
51 Model Architecture #frames Pre-training Split	51	HMDB-51
C3D [3] Custom 16 Scratch 51	51	HMDB-51
51	51	HMDB-51
51	51	HMDB-51
art on UCF-101 [46] and HMDB-51 [28] in Section 4.4	HMDB-51	HMDB-51
in-the-wild videos. Target test videos: HMDB-51 [28] contains 6766 realistic and	HMDB-51	HMDB-51
using percentage accuracy on the HMDB-51 dataset, Split 1. The models	HMDB-51	HMDB-51
evaluated using percentage accuracy on HMDB-51 split 1	HMDB-51	HMDB-51
evaluated using percentage accuracy on HMDB-51 split 1	HMDB-51	HMDB-51
DistInit, and show the downstream HMDB-51 split-1 performance in the line	HMDB-51	HMDB-51
that set. Performance reported on HMDB-51 split 1	HMDB-51	HMDB-51
it two times. Reported on HMDB-51 split 1	HMDB-51	HMDB-51
a) HMDB-51 Model Architecture #frames Pre-training Split	HMDB-51	HMDB-51
provements over the UCF101 and HMDB51 benchmarks	HMDB	HMDB-51
widely-used benchmarks, UCF101 [38] and HMDB51 [18]. UCF101 and HMDB51 contain	HMDB	HMDB-51
Method Feature Setting HMDB51 UCF101 ST [45] BoW T	HMDB	HMDB-51
an average duration of 7.2s. HMDB51 includes 6766 videos of 51	HMDB	HMDB-51
scenario. The seen/unseen splits for HMDB51 and UCF101 are 27/26 and	HMDB	HMDB-51
Dataset HMDB51 UCF101 Setting Cross-Dataset Transductive Cross-Dataset	HMDB	HMDB-51
rich and the concepts of HMDB51 and UCF101 are not very	HMDB	HMDB-51
is large (roughly 5% on HMDB51 and 1% on UCF101 be	HMDB	HMDB-51
T. Poggio, and T. Serre. HMDB	HMDB	HMDB-51
the success of CNNs [49, 51, 48]. With the help of	51	HMDB-51
28.9±1.2 20.1±1.4 Ours GMIL-D CD 51	51	HMDB-51
HMDB51 includes 6766 videos of 51 action classes extracted from a	51	HMDB-51
34.2 Kodirov et al. [17] 51 50 10 14.0 Liu et	51	HMDB-51
al. [22] 51 50 5 14.9 Xu et	51	HMDB-51
al. [47] 51 50 50 22.9 Li et	51	HMDB-51
al. [21] 51 50 30 26.8 Mettes and	51	HMDB-51
Snoek [28] - 20 10 51	51	HMDB-51
and UCF101 are 27/26 and 51	51	HMDB-51
13.6 No TJM 48.9 50.5 51	51	HMDB-51
36.6 38.1 38.6 Ours 49.6 51	51	HMDB-51
51	51	HMDB-51
popular dataset UCF101 [36] and HMDB51 [23]. Our method achieves the	HMDB	HMDB-51
36], the Kinetics [19], the HMDB51 [23], the ASLAN [21], and	HMDB	HMDB-51
HMDB51 dataset [23] is a smaller	HMDB	HMDB-51
parison with others, we use HMDB51 train split 1 to fine	HMDB	HMDB-51
action recog- nition accuracy on HMDB51 test split 1	HMDB	HMDB-51
also finetune the C3D with HMDB51 train split 1 to get	HMDB	HMDB-51
combina- tion on UCF101 and HMDB51 dataset as shown in Table	HMDB	HMDB-51
self-supervised signals for UCF101 and HMDB51 dataset. The motion statistics is	HMDB	HMDB-51
on the UCF101 and the HMDB51 datasets	HMDB	HMDB-51
Domain UCF101 acc.(%) HMDB51 acc. (%) From scratch 45.4	HMDB	HMDB-51
combined motion and appearance, the HMDB51 dataset benefits a lot from	HMDB	HMDB-51
state-of-the-art both on UCF101 and HMDB51	HMDB	HMDB-51
we improve 9.3% accuracy on HMDB51 than [12] and 2.5% accuracy	HMDB	HMDB-51
UCF101 and 5.1% improvement on HMDB51	HMDB	HMDB-51
learning methods on UCF101 and HMDB51	HMDB	HMDB-51
Method UCF101 acc.(%) HMDB51 acc	HMDB	HMDB-51
The performance on UCF101, HMDB51 and ASLAN dataset shows that	HMDB	HMDB-51
perfor- mances on UCF101 and HMDB51 datasets. This strongly supports that	HMDB	HMDB-51
con- tains 6766 videos and 51 action classes. It also consists	51	HMDB-51
21] 60.9 C3D, random initialization 51	51	HMDB-51
dom initialization model can achieve 51	51	HMDB-51
V1, the two major large-scale action	V1	Something-Something V1
V1 [11] (174 classes). Our results	V1	Something-Something V1
V1), while N is a hyper-parameter	V1	Something-Something V1
recognition: Kinetics-400 [15] and Something-Something- V1 [11]. Kinetics-400 consists of 400	V1	Something-Something V1
V1 consists of 174 actions and	V1	Something-Something V1
V1 and a simple 2D RGB	V1	Something-Something V1
are relatively generic on Something-Something- V1 (e.g., ’plugging something into something	V1	Something-Something V1
V1 we follow the standard practice	V1	Something-Something V1
V1 dataset	V1	Something-Something V1
V1 (table 5). We compare against	V1	Something-Something V1
V1 using different backbone architectures, which	V1	Something-Something V1
state-of-the-art performance on Kinetics-400 and Something-Something	Something-Something	Something-Something V1
Kinetics-400 [15] (400 classes) and Something-Something	Something-Something	Something-Something V1
for Kinetics-400 and 174 for Something-Something	Something-Something	Something-Something V1
action recognition: Kinetics-400 [15] and Something-Something	Something-Something	Something-Something V1
extensive set of action classes. Something-Something	Something-Something	Something-Something V1
they are relatively generic on Something-Something	Something-Something	Something-Something V1
in the literature on the Something-Something	Something-Something	Something-Something V1
Something-Something	Something-Something	Something-Something V1
state-of-the-art performance on Kinetics-400 and Something	Something	Something-Something V1
Something	Something	Something-Something V1
Kinetics-400 [15] (400 classes) and Something	Something	Something-Something V1
Something	Something	Something-Something V1
for Kinetics-400 and 174 for Something	Something	Something-Something V1
Something	Something	Something-Something V1
action recognition: Kinetics-400 [15] and Something	Something	Something-Something V1
Something	Something	Something-Something V1
extensive set of action classes. Something	Something	Something-Something V1
Something	Something	Something-Something V1
temporal information is essential for Something	Something	Something-Something V1
- Something	Something	Something-Something V1
they are relatively generic on Something	Something	Something-Something V1
Something	Something	Something-Something V1
testing protocol for Kinetics-400, on Something	Something	Something-Something V1
- Something	Something	Something-Something V1
in the literature on the Something	Something	Something-Something V1
Something	Something	Something-Something V1
Something	Something	Something-Something V1
Something	Something	Something-Something V1
the literature reports results on Something	Something	Something-Something V1
- Something	Something	Something-Something V1
Stream MiniKinetics Kinetics400 UCF101-1 HMDB51-1 Something	Something	Something-Something V1
Stream Kinetics400 UCF101-1 HMDB51-1 Something	Something	Something-Something V1
Method Streams Pretrain UCF101 HMDB51 Something	Something	Something-Something V1
Something V1 dataset. Single crop STM beats	Something V1	Something-Something V1
other state- of-the-art methods on Something-Something V1 dataset. Single crop STM beats	Something-Something V1	Something-Something V1
other state- of-the-art methods on Something-Something V1 dataset. Single crop STM beats	Something-Something V	Something-Something V1
state- of-the-art methods on Something-Something V1 dataset. Single crop STM beats	V1	Something-Something V1
on both temporal-related datasets (i.e., Something-Something v1 & v2 and Jester	Something-Something	Something-Something V1
several public benchmark datasets including Something-Something	Something-Something	Something-Something V1
conduct abundant ablation studies with Something-Something v1 to analyze the effectiveness	Something-Something	Something-Something V1
categories: (1) temporal-related datasets, including Something-Something v1 & v2 [11] and	Something-Something	Something-Something V1
of the STM on the Something-Something v1 and v2 datasets compared	Something-Something	Something-Something V1
Method Backbone Flow Pretrain Frame Something-Something v1 Something-Something v2top-1 val top-5	Something-Something	Something-Something V1
methods on temporal-related datasets including Something-Something v1 & v2 and Jester	Something-Something	Something-Something V1
174 classes with 108,499 videos. Something-Something v2 is an updated version	Something-Something	Something-Something V1
compared with the state-of-the-art on Something-Something v1 and v2. The results	Something-Something	Something-Something V1
on Something- Something v1. On Something-Something v2, STM also gains 34.5	Something-Something	Something-Something V1
valida- tion sets of both Something-Something v1 and v2, and just	Something-Something	Something-Something V1
our pro- posed STM on Something-Something v1 dataset. All the ablation	Something-Something	Something-Something V1
other state- of-the-art methods on Something-Something V1 dataset. Single crop STM	Something-Something	Something-Something V1
and several state-of-the-art methods on Something-Something v1 dataset. All evaluations are	Something-Something	Something-Something V1
on both temporal-related datasets (i.e., Something	Something	Something-Something V1
Something v1 & v2 and Jester	Something	Something-Something V1
several public benchmark datasets including Something	Something	Something-Something V1
Something	Something	Something-Something V1
on both temporal-related datasets (i.e., Something	Something	Something-Something V1
- Something v1 & v2 and Jester	Something	Something-Something V1
conduct abundant ablation studies with Something	Something	Something-Something V1
Something v1 to analyze the effectiveness	Something	Something-Something V1
categories: (1) temporal-related datasets, including Something	Something	Something-Something V1
Something v1 & v2 [11] and	Something	Something-Something V1
T = 16). For Kinetics, Something	Something	Something-Something V1
- Something v1 & v2 and Jester	Something	Something-Something V1
of the STM on the Something	Something	Something-Something V1
Something v1 and v2 datasets compared	Something	Something-Something V1
Method Backbone Flow Pretrain Frame Something	Something	Something-Something V1
-Something v1 Something	Something	Something-Something V1
Something v2top-1 val top-5 val top-1	Something	Something-Something V1
methods on temporal-related datasets including Something	Something	Something-Something V1
Something v1 & v2 and Jester	Something	Something-Something V1
. Something- Something v1 is a large collection	Something	Something-Something V1
174 classes with 108,499 videos. Something	Something	Something-Something V1
Something v2 is an updated version	Something	Something-Something V1
compared with the state-of-the-art on Something	Something	Something-Something V1
Something v1 and v2. The results	Something	Something-Something V1
16 frames inputs respectively on Something	Something	Something-Something V1
- Something v1. On Something	Something	Something-Something V1
Something v2, STM also gains 34.5	Something	Something-Something V1
valida- tion sets of both Something	Something	Something-Something V1
Something v1 and v2, and just	Something	Something-Something V1
our pro- posed STM on Something	Something	Something-Something V1
Something v1 dataset. All the ablation	Something	Something-Something V1
other state- of-the-art methods on Something	Something	Something-Something V1
Something V1 dataset. Single crop STM	Something	Something-Something V1
and several state-of-the-art methods on Something	Something	Something-Something V1
Something v1 dataset. All evaluations are	Something	Something-Something V1
40], ActivityNet [8], Kinetics [25], Something-Something [14], AVA [15], and Charades	Something-Something	Something-Something V1
UCF101 [40], HMDB51 [29] and Something-Something [14]. During training we randomly	Something-Something	Something-Something V1
pouring", etc.). The results on Something-Something show that pretraining on Moments	Something-Something	Something-Something V1
40], ActivityNet [8], Kinetics [25], Something	Something	Something-Something V1
Something [14], AVA [15], and Charades	Something	Something-Something V1
Fine-Tuned Pretrained UCF HMDB Something	Something	Something-Something V1
UCF101 [40], HMDB51 [29] and Something	Something	Something-Something V1
Something [14]. During training we randomly	Something	Something-Something V1
pouring", etc.). The results on Something	Something	Something-Something V1
Something show that pretraining on Moments	Something	Something-Something V1
Something V1 [9], V2 [16] and Charades	Something V1	Something-Something V1
datasets for activity recognition: Something- Something V1 [9], V2 [16] and Charades	Something V1	Something-Something V1
Something V1 and V2 validation set. Considering	Something V1	Something-Something V1
Something V1 and 2.5% on Something-Something V2	Something V1	Something-Something V1
Something V1 and 61.3% on Something-Something V2	Something V1	Something-Something V1
three large- scale data sets: Something-Something V1 [9], V2 [16] and Charades	Something-Something V1	Something-Something V1
shows all the results on Something-Something V1 and V2 validation set. Considering	Something-Something V1	Something-Something V1
a large margin 4.1% on Something-Something V1 and 2.5% on Something-Something V2	Something-Something V1	Something-Something V1
overall performance to 49.8% on Something-Something V1 and 61.3% on Something-Something V2	Something-Something V1	Something-Something V1
the samples are originated from Something-Something V2 dataset). The three kinds of	Something-Something V	Something-Something V1
three large- scale data sets: Something-Something V1 [9], V2 [16] and Charades	Something-Something V	Something-Something V1
procedure takes 100 epochs. For Something-Something V2, the epoch number is halved	Something-Something V	Something-Something V1
shows all the results on Something-Something V1 and V2 validation set. Considering	Something-Something V	Something-Something V1
a large margin 4.1% on Something-Something V1 and 2.5% on Something-Something V2	Something-Something V	Something-Something V1
overall performance to 49.8% on Something-Something V1 and 61.3% on Something-Something V2	Something-Something V	Something-Something V1
Inception or Inception-V3 architecture on Something-Something V2 (left) and Charades (right) datasets	Something-Something V	Something-Something V1
of the video representation on Something-Something V2 validation set. The representations are	Something-Something V	Something-Something V1
The experiment was conducted on Something-Something V2 dataset with 20 randomly selected	Something-Something V	Something-Something V1
OF STATE-OF-ART METHODS ON THE SOMETHING-SOMETHING V1 AND V2 DATASETS. WE ONLY	SOMETHING-SOMETHING V1	Something-Something V1
large- scale data sets: Something-Something V1 [9], V2 [16] and Charades	V1	Something-Something V1
for activity recognition: Something- Something V1 [9], V2 [16] and Charades	V1	Something-Something V1
V1 [9]: The dataset is a	V1	Something-Something V1
twice as many videos as V1, collected by workers with humans	V1	Something-Something V1
V1 and V2 datasets is assigned	V1	Something-Something V1
all the results on Something-Something V1 and V2 validation set. Considering	V1	Something-Something V1
large margin 4.1% on Something-Something V1 and 2.5% on Something-Something V2	V1	Something-Something V1
performance to 49.8% on Something-Something V1 and 61.3% on Something-Something V2	V1	Something-Something V1
STATE-OF-ART METHODS ON THE SOMETHING-SOMETHING V1 AND V2 DATASETS. WE ONLY	V1	Something-Something V1
Frames # Params FLOPs V1 V2top-1 top-5 top-1 top-5	V1	Something-Something V1
widely-used large-scale datasets, such as Something-Something and Charades, and the results	Something-Something	Something-Something V1
the samples are originated from Something-Something V2 dataset). The three kinds	Something-Something	Something-Something V1
three large- scale data sets: Something-Something V1 [9], V2 [16] and	Something-Something	Something-Something V1
Something-Something	Something-Something	Something-Something V1
Something-Something	Something-Something	Something-Something V1
procedure takes 100 epochs. For Something-Something V2, the epoch number is	Something-Something	Something-Something V1
Every video in Something-Something	Something-Something	Something-Something V1
A baseline model provided in Something-Something dataset exploits multi-layer 3D convolution	Something-Something	Something-Something V1
The results on Something-Something [9] and Charades [28], [29	Something-Something	Something-Something V1
shows all the results on Something-Something V1 and V2 validation set	Something-Something	Something-Something V1
a large margin 4.1% on Something-Something V1 and 2.5% on Something-Something	Something-Something	Something-Something V1
overall performance to 49.8% on Something-Something V1 and 61.3% on Something-Something	Something-Something	Something-Something V1
Inception or Inception-V3 architecture on Something-Something V2 (left) and Charades (right	Something-Something	Something-Something V1
of the video representation on Something-Something V2 validation set. The representations	Something-Something	Something-Something V1
The experiment was conducted on Something-Something V2 dataset with 20 randomly	Something-Something	Something-Something V1
evaluated the proposed model on Something-Something and Charades datasets and estab	Something-Something	Something-Something V1
widely-used large-scale datasets, such as Something	Something	Something-Something V1
Something and Charades, and the results	Something	Something-Something V1
the samples are originated from Something	Something	Something-Something V1
Something V2 dataset). The three kinds	Something	Something-Something V1
three large- scale data sets: Something	Something	Something-Something V1
Something V1 [9], V2 [16] and	Something	Something-Something V1
Covering Something	Something	Something-Something V1
with Something	Something	Something-Something V1
benchmark datasets for activity recognition: Something	Something	Something-Something V1
- Something V1 [9], V2 [16] and	Something	Something-Something V1
Something	Something	Something-Something V1
Something	Something	Something-Something V1
Something	Something	Something-Something V1
Something	Something	Something-Something V1
procedure takes 100 epochs. For Something	Something	Something-Something V1
Something V2, the epoch number is	Something	Something-Something V1
temporal axis for Charades and Something	Something	Something-Something V1
- Something dataset respectively and the prediction	Something	Something-Something V1
Every video in Something	Something	Something-Something V1
Something	Something	Something-Something V1
A baseline model provided in Something	Something	Something-Something V1
Something dataset exploits multi-layer 3D convolution	Something	Something-Something V1
The results on Something	Something	Something-Something V1
Something [9] and Charades [28], [29	Something	Something-Something V1
shows all the results on Something	Something	Something-Something V1
Something V1 and V2 validation set	Something	Something-Something V1
a large margin 4.1% on Something	Something	Something-Something V1
-Something V1 and 2.5% on Something	Something	Something-Something V1
Something V2 with the same experimental	Something	Something-Something V1
overall performance to 49.8% on Something	Something	Something-Something V1
-Something V1 and 61.3% on Something	Something	Something-Something V1
Something V2. Compare with the most	Something	Something-Something V1
serial of experi- ments on Something Something V2 and Charades datasets	Something	Something-Something V1
54.7% with Inception-V3 backbone on Something	Something	Something-Something V1
- Something V2, and improve the mAP	Something	Something-Something V1
Inception or Inception-V3 architecture on Something	Something	Something-Something V1
Something V2 (left) and Charades (right	Something	Something-Something V1
visualize the CAM [54] on Something	Something	Something-Something V1
- Something V2 dataset. CAM can visualize	Something	Something-Something V1
of the video representation on Something	Something	Something-Something V1
Something V2 validation set. The representations	Something	Something-Something V1
The experiment was conducted on Something	Something	Something-Something V1
Something V2 dataset with 20 randomly	Something	Something-Something V1
evaluated the proposed model on Something	Something	Something-Something V1
Something and Charades datasets and estab	Something	Something-Something V1
Something V1 [9], V2 [16] and Charades	Something V1	Something-Something V1
datasets for activity recognition: Something- Something V1 [9], V2 [16] and Charades	Something V1	Something-Something V1
Something V1 and V2 validation set. Considering	Something V1	Something-Something V1
Something V1 and 2.5% on Something-Something V2	Something V1	Something-Something V1
Something V1 and 61.3% on Something-Something V2	Something V1	Something-Something V1
three large- scale data sets: Something-Something V1 [9], V2 [16] and Charades	Something-Something V1	Something-Something V1
shows all the results on Something-Something V1 and V2 validation set. Considering	Something-Something V1	Something-Something V1
a large margin 4.1% on Something-Something V1 and 2.5% on Something-Something V2	Something-Something V1	Something-Something V1
overall performance to 49.8% on Something-Something V1 and 61.3% on Something-Something V2	Something-Something V1	Something-Something V1
the samples are originated from Something-Something V2 dataset). The three kinds of	Something-Something V	Something-Something V1
three large- scale data sets: Something-Something V1 [9], V2 [16] and Charades	Something-Something V	Something-Something V1
procedure takes 100 epochs. For Something-Something V2, the epoch number is halved	Something-Something V	Something-Something V1
shows all the results on Something-Something V1 and V2 validation set. Considering	Something-Something V	Something-Something V1
a large margin 4.1% on Something-Something V1 and 2.5% on Something-Something V2	Something-Something V	Something-Something V1
overall performance to 49.8% on Something-Something V1 and 61.3% on Something-Something V2	Something-Something V	Something-Something V1
Inception or Inception-V3 architecture on Something-Something V2 (left) and Charades (right) datasets	Something-Something V	Something-Something V1
of the video representation on Something-Something V2 validation set. The representations are	Something-Something V	Something-Something V1
The experiment was conducted on Something-Something V2 dataset with 20 randomly selected	Something-Something V	Something-Something V1
OF STATE-OF-ART METHODS ON THE SOMETHING-SOMETHING V1 AND V2 DATASETS. WE ONLY	SOMETHING-SOMETHING V1	Something-Something V1
large- scale data sets: Something-Something V1 [9], V2 [16] and Charades	V1	Something-Something V1
for activity recognition: Something- Something V1 [9], V2 [16] and Charades	V1	Something-Something V1
V1 [9]: The dataset is a	V1	Something-Something V1
twice as many videos as V1, collected by workers with humans	V1	Something-Something V1
V1 and V2 datasets is assigned	V1	Something-Something V1
all the results on Something-Something V1 and V2 validation set. Considering	V1	Something-Something V1
large margin 4.1% on Something-Something V1 and 2.5% on Something-Something V2	V1	Something-Something V1
performance to 49.8% on Something-Something V1 and 61.3% on Something-Something V2	V1	Something-Something V1
STATE-OF-ART METHODS ON THE SOMETHING-SOMETHING V1 AND V2 DATASETS. WE ONLY	V1	Something-Something V1
Frames # Params FLOPs V1 V2top-1 top-5 top-1 top-5	V1	Something-Something V1
widely-used large-scale datasets, such as Something-Something and Charades, and the results	Something-Something	Something-Something V1
the samples are originated from Something-Something V2 dataset). The three kinds	Something-Something	Something-Something V1
three large- scale data sets: Something-Something V1 [9], V2 [16] and	Something-Something	Something-Something V1
Something-Something	Something-Something	Something-Something V1
Something-Something	Something-Something	Something-Something V1
procedure takes 100 epochs. For Something-Something V2, the epoch number is	Something-Something	Something-Something V1
Every video in Something-Something	Something-Something	Something-Something V1
A baseline model provided in Something-Something dataset exploits multi-layer 3D convolution	Something-Something	Something-Something V1
The results on Something-Something [9] and Charades [28], [29	Something-Something	Something-Something V1
shows all the results on Something-Something V1 and V2 validation set	Something-Something	Something-Something V1
a large margin 4.1% on Something-Something V1 and 2.5% on Something-Something	Something-Something	Something-Something V1
overall performance to 49.8% on Something-Something V1 and 61.3% on Something-Something	Something-Something	Something-Something V1
Inception or Inception-V3 architecture on Something-Something V2 (left) and Charades (right	Something-Something	Something-Something V1
of the video representation on Something-Something V2 validation set. The representations	Something-Something	Something-Something V1
The experiment was conducted on Something-Something V2 dataset with 20 randomly	Something-Something	Something-Something V1
evaluated the proposed model on Something-Something and Charades datasets and estab	Something-Something	Something-Something V1
widely-used large-scale datasets, such as Something	Something	Something-Something V1
Something and Charades, and the results	Something	Something-Something V1
the samples are originated from Something	Something	Something-Something V1
Something V2 dataset). The three kinds	Something	Something-Something V1
three large- scale data sets: Something	Something	Something-Something V1
Something V1 [9], V2 [16] and	Something	Something-Something V1
Covering Something	Something	Something-Something V1
with Something	Something	Something-Something V1
benchmark datasets for activity recognition: Something	Something	Something-Something V1
- Something V1 [9], V2 [16] and	Something	Something-Something V1
Something	Something	Something-Something V1
Something	Something	Something-Something V1
Something	Something	Something-Something V1
Something	Something	Something-Something V1
procedure takes 100 epochs. For Something	Something	Something-Something V1
Something V2, the epoch number is	Something	Something-Something V1
temporal axis for Charades and Something	Something	Something-Something V1
- Something dataset respectively and the prediction	Something	Something-Something V1
Every video in Something	Something	Something-Something V1
Something	Something	Something-Something V1
A baseline model provided in Something	Something	Something-Something V1
Something dataset exploits multi-layer 3D convolution	Something	Something-Something V1
The results on Something	Something	Something-Something V1
Something [9] and Charades [28], [29	Something	Something-Something V1
shows all the results on Something	Something	Something-Something V1
Something V1 and V2 validation set	Something	Something-Something V1
a large margin 4.1% on Something	Something	Something-Something V1
-Something V1 and 2.5% on Something	Something	Something-Something V1
Something V2 with the same experimental	Something	Something-Something V1
overall performance to 49.8% on Something	Something	Something-Something V1
-Something V1 and 61.3% on Something	Something	Something-Something V1
Something V2. Compare with the most	Something	Something-Something V1
serial of experi- ments on Something Something V2 and Charades datasets	Something	Something-Something V1
54.7% with Inception-V3 backbone on Something	Something	Something-Something V1
- Something V2, and improve the mAP	Something	Something-Something V1
Inception or Inception-V3 architecture on Something	Something	Something-Something V1
Something V2 (left) and Charades (right	Something	Something-Something V1
visualize the CAM [54] on Something	Something	Something-Something V1
- Something V2 dataset. CAM can visualize	Something	Something-Something V1
of the video representation on Something	Something	Something-Something V1
Something V2 validation set. The representations	Something	Something-Something V1
The experiment was conducted on Something	Something	Something-Something V1
Something V2 dataset with 20 randomly	Something	Something-Something V1
evaluated the proposed model on Something	Something	Something-Something V1
Something and Charades datasets and estab	Something	Something-Something V1
40], ActivityNet [8], Kinetics [25], Something-Something [14], AVA [15], and Charades	Something-Something	Something-Something V1
UCF101 [40], HMDB51 [29] and Something-Something [14]. During training we randomly	Something-Something	Something-Something V1
pouring", etc.). The results on Something-Something show that pretraining on Moments	Something-Something	Something-Something V1
40], ActivityNet [8], Kinetics [25], Something	Something	Something-Something V1
Something [14], AVA [15], and Charades	Something	Something-Something V1
Fine-Tuned Pretrained UCF HMDB Something	Something	Something-Something V1
UCF101 [40], HMDB51 [29] and Something	Something	Something-Something V1
Something [14]. During training we randomly	Something	Something-Something V1
pouring", etc.). The results on Something	Something	Something-Something V1
Something show that pretraining on Moments	Something	Something-Something V1
filters used by the Inception V1 ar- chitecture [2] into 3D	V1	Something-Something V1
several action classification benchmarks (Kinetics, Something	Something	Something-Something V1
of Sports-1M [5], Kinetics [6], Something	Something	Something-Something V1
qualitatively different kinds of datasets: Something	Something	Something-Something V1
tion datasets, such as Kinetics, Something	Something	Something-Something V1
The second main dataset is Something	Something	Something-Something V1
step 70k to 0.001. Since Something	Something	Something-Something V1
Mini- Kinetics-200 and 64 for Something	Something	Something-Something V1
model on the Kinetics-Full and Something	Something	Something-Something V1
the Kinetics dataset and the Something	Something	Something-Something V1
We believe this is because Something	Something	Something-Something V1
Kinetics-Full Something	Something	Something-Something V1
Top-1 accuracy on Kinetics-Full and Something	Something	Something-Something V1
on this dataset. However, on Something	Something	Something-Something V1
models on Mini- Kinetics-200 and Something	Something	Something-Something V1
frames. Left: Mini-Kinetics-200 dataset. Right: Something	Something	Something-Something V1
also outperforms I3D on the Something	Something	Something-Something V1
from 45.8% to 47.3% for Something	Something	Something-Something V1
derived from images in the Something	Something	Something-Something V1
the S3D model on the Something	Something	Something-Something V1
and feature gating on the Something	Something	Something-Something V1
outperforms S3D and I3D on Something	Something	Something-Something V1
to the complexity of the Something-Something dataset, we finetune the network	Something-Something	Something-Something V1
Comparison with state-of- the-arts on Something-Something dataset. Last row shows the	Something-Something	Something-Something V1
datasets Kinetics [1] and Something-Something [3]. Moreover, we applied the	Something-Something	Something-Something V1
a much heavier network. On Something-Something, it outperforms the other methods	Something-Something	Something-Something V1
or fewer samples. On the Something-Something dataset, where the temporal context	Something-Something	Something-Something V1
Fig. 5: Examples from the Something-Something dataset. In this dataset, the	Something-Something	Something-Something V1
Kinetics [1], ActivityNet [2], and Something	Something	Something-Something V1
- Something [3] have contributed more diversity	Something	Something-Something V1
to the complexity of the Something	Something	Something-Something V1
Something dataset, we finetune the network	Something	Something-Something V1
Comparison with state-of- the-arts on Something	Something	Something-Something V1
Something dataset. Last row shows the	Something	Something-Something V1
datasets Kinetics [1] and Something	Something	Something-Something V1
Something [3]. Moreover, we applied the	Something	Something-Something V1
a much heavier network. On Something	Something	Something-Something V1
Something, it outperforms the other methods	Something	Something-Something V1
or fewer samples. On the Something	Something	Something-Something V1
Something dataset, where the temporal context	Something	Something-Something V1
Fig. 5: Examples from the Something	Something	Something-Something V1
Something dataset. In this dataset, the	Something	Something-Something V1
action recognition datasets (Jester and Something-Something) and achieved competitive performances for	Something-Something	Something-Something V1
action recognition datasets, Jester (top), Something-Something (middle), and UCF101 (bottom	Something-Something	Something-Something V1
these datasets, Jester [1] and Something-Something [8] include more detailed physical	Something-Something	Something-Something V1
these datasets. In particular, the Something-Something dataset has little corre- lation	Something-Something	Something-Something V1
As datasets, Jester [1] and Something-Something [8] are used because these	Something-Something	Something-Something V1
the datasets of Jester and Something-Something, RGB images extracted from videos	Something-Something	Something-Something V1
version (MFNet-S) on Jester and Something-Something validation sets. All models use	Something-Something	Something-Something V1
Dataset Jester Something-Something	Something-Something	Something-Something V1
better performance on Jester and Something-Something datasets with the temporal sampling	Something-Something	Something-Something V1
743 videos for testing. The Something-Something	Something-Something	Something-Something V1
values are on JESTER and Something-Something validation sets. All models are	Something-Something	Something-Something V1
Dataset Jester Something-Something	Something-Something	Something-Something V1
vs. 17.4% on Jester and Something-Something datasets respectively. The trend is	Something-Something	Something-Something V1
to 2.8%) than MFNet-S on Something-Something dataset. On the other hand	Something-Something	Something-Something V1
any action-related visual features in Something-Something dataset	Something-Something	Something-Something V1
and ResNet-101) on Jester and Something-Something datasets. We re-implemented the I3D	Something-Something	Something-Something V1
Dataset Jester Something-Something	Something-Something	Something-Something V1
significant effect on performance. In Something-Something case, accuracy gets also saturated	Something-Something	Something-Something V1
scratch for fair comparison on Something-Something dataset. We followed data augmentation	Something-Something	Something-Something V1
of frames in Jester and Something-Something datasets, we trained I3D with	Something-Something	Something-Something V1
TRN [37]. Because Jester and Something-Something are recently released datasets in	Something-Something	Something-Something V1
top-1 accuracies on Jester and Something-Something test datasets respectively on official	Something-Something	Something-Something V1
Dataset Jester Something-Something	Something-Something	Something-Something V1
results on the Jester and Something-Something datasets from the official leaderboards	Something-Something	Something-Something V1
Jester Something-Something	Something-Something	Something-Something V1
b) Something-Something	Something-Something	Something-Something V1
on two datasets, Jester and Something-Something, and obtain outperforming results compared	Something-Something	Something-Something V1
action recognition datasets (Jester and Something	Something	Something-Something V1
Something) and achieved competitive performances for	Something	Something-Something V1
action recognition datasets, Jester (top), Something	Something	Something-Something V1
Something (middle), and UCF101 (bottom	Something	Something-Something V1
these datasets, Jester [1] and Something	Something	Something-Something V1
Something [8] include more detailed physical	Something	Something-Something V1
these datasets. In particular, the Something	Something	Something-Something V1
Something dataset has little corre- lation	Something	Something-Something V1
As datasets, Jester [1] and Something	Something	Something-Something V1
Something [8] are used because these	Something	Something-Something V1
the datasets of Jester and Something	Something	Something-Something V1
Something, RGB images extracted from videos	Something	Something-Something V1
version (MFNet-S) on Jester and Something	Something	Something-Something V1
Something validation sets. All models use	Something	Something-Something V1
Dataset Jester Something	Something	Something-Something V1
Something	Something	Something-Something V1
better performance on Jester and Something	Something	Something-Something V1
Something datasets with the temporal sampling	Something	Something-Something V1
743 videos for testing. The Something	Something	Something-Something V1
Something	Something	Something-Something V1
values are on JESTER and Something	Something	Something-Something V1
Something validation sets. All models are	Something	Something-Something V1
Dataset Jester Something	Something	Something-Something V1
Something	Something	Something-Something V1
vs. 17.4% on Jester and Something	Something	Something-Something V1
Something datasets respectively. The trend is	Something	Something-Something V1
to 2.8%) than MFNet-S on Something	Something	Something-Something V1
Something dataset. On the other hand	Something	Something-Something V1
any action-related visual features in Something	Something	Something-Something V1
Something dataset	Something	Something-Something V1
and ResNet-101) on Jester and Something	Something	Something-Something V1
Something datasets. We re-implemented the I3D	Something	Something-Something V1
Dataset Jester Something	Something	Something-Something V1
Something	Something	Something-Something V1
significant effect on performance. In Something	Something	Something-Something V1
Something case, accuracy gets also saturated	Something	Something-Something V1
scratch for fair comparison on Something	Something	Something-Something V1
Something dataset. We followed data augmentation	Something	Something-Something V1
of frames in Jester and Something	Something	Something-Something V1
Something datasets, we trained I3D with	Something	Something-Something V1
TRN [37]. Because Jester and Something	Something	Something-Something V1
Something are recently released datasets in	Something	Something-Something V1
top-1 accuracies on Jester and Something	Something	Something-Something V1
Something test datasets respectively on official	Something	Something-Something V1
various methods on Jester and Something	Something	Something-Something V1
Dataset Jester Something	Something	Something-Something V1
Something	Something	Something-Something V1
results on the Jester and Something	Something	Something-Something V1
Something datasets from the official leaderboards	Something	Something-Something V1
Jester Something	Something	Something-Something V1
Something	Something	Something-Something V1
performances on the Jester and Something	Something	Something-Something V1
- Something datasets	Something	Something-Something V1
MFNet-C50 on Jester (left) and Something	Something	Something-Something V1
- Something (right) datasets. As discussed in	Something	Something-Something V1
b) Something	Something	Something-Something V1
Something	Something	Something-Something V1
on two datasets, Jester and Something	Something	Something-Something V1
Something, and obtain outperforming results compared	Something	Something-Something V1
three datasets Something-Something dataset (Something- V1 [9] and Something-V2 [28] where	V1	Something-Something V1
V1 174 108,499 human-object interaction Something-V2	V1	Something-Something V1
V1 and Something- V2 datasets are	V1	Something-Something V1
V1 and Something-V2 respec- tively. We	V1	Something-Something V1
V1 Something-V2 Val Test Val Test	V1	Something-Something V1
V1 Dataset (Top1 Accuracy) and Something-V2	V1	Something-Something V1
V1	V1	Something-Something V1
predict human-object interactions in the Something-Something dataset and identify various human	Something-Something	Something-Something V1
on three recent video datasets (Something-Something [9], Jester [10], and Charades	Something-Something	Something-Something V1
on sequential activity recog- nition: Something-Something dataset [9] is collected for	Something-Something	Something-Something V1
highly competitive results on the Something-Something dataset for human-interaction recognition [9	Something-Something	Something-Something V1
statistics of the three datasets Something-Something dataset (Something- V1 [9] and	Something-Something	Something-Something V1
3.2 Results on Something-Something Dataset	Something-Something	Something-Something V1
Something-Something is a recent video dataset	Something-Something	Something-Something V1
3: Prediction examples on a) Something-Something, b) Jester, and c) Cha	Something-Something	Something-Something V1
For each example drawn from Something-Something and Jester, the top two	Something-Something	Something-Something V1
the validation set of the Something-Something dataset	Something-Something	Something-Something V1
of videos from the (a) Something-Something and (b) Jester datasets using	Something-Something	Something-Something V1
The significant difference on the Something-Something dataset shows the importance of	Something-Something	Something-Something V1
shuffled inputs drawn from the Something-Something dataset, in Figure 6b. In	Something-Something	Something-Something V1
Something-Something Ordered Shuffled	Something-Something	Something-Something V1
frames and shuffled frames, on Something-Something and UCF101 dataset respectively. On	Something-Something	Something-Something V1
can better differentiate activities in Something-Something dataset	Something-Something	Something-Something V1
three recent video datasets - Something	Something	Something-Something V1
- Something, Jester, and Charades - which	Something	Something-Something V1
predict human-object interactions in the Something	Something	Something-Something V1
Something dataset and identify various human	Something	Something-Something V1
on three recent video datasets (Something	Something	Something-Something V1
Something [9], Jester [10], and Charades	Something	Something-Something V1
on sequential activity recog- nition: Something	Something	Something-Something V1
Something dataset [9] is collected for	Something	Something-Something V1
highly competitive results on the Something	Something	Something-Something V1
Something dataset for human-interaction recognition [9	Something	Something-Something V1
statistics of the three datasets Something	Something	Something-Something V1
-Something dataset (Something	Something	Something-Something V1
- V1 [9] and Something	Something	Something-Something V1
-V2 [28] where the Something	Something	Something-Something V1
Something-V1 174 108,499 human-object interaction Something	Something	Something-Something V1
3.2 Results on Something	Something	Something-Something V1
Something Dataset	Something	Something-Something V1
Something	Something	Something-Something V1
Something is a recent video dataset	Something	Something-Something V1
are challenging, such as ‘Tearing Something into two pieces’ versus ‘Tearing	Something	Something-Something V1
Something just a little bit’, ‘Turn	Something	Something-Something V1
set and test set of Something	Something	Something-Something V1
-V1 and Something	Something	Something-Something V1
the valida- tion set of Something	Something	Something-Something V1
-v1 and Something	Something	Something-Something V1
on the validation set of Something	Something	Something-Something V1
-V1 and Something	Something	Something-Something V1
Something-V1 Something	Something	Something-Something V1
and test set of the Something	Something	Something-Something V1
-V1 Dataset (Top1 Accuracy) and Something	Something	Something-Something V1
the validation set of the Something	Something	Something-Something V1
3: Prediction examples on a) Something	Something	Something-Something V1
Something, b) Jester, and c) Cha	Something	Something-Something V1
For each example drawn from Something	Something	Something-Something V1
Something and Jester, the top two	Something	Something-Something V1
highlight a pattern characteristic to Something	Something	Something-Something V1
the validation set of the Something	Something	Something-Something V1
Something dataset	Something	Something-Something V1
of videos from the (a) Something	Something	Something-Something V1
Something and (b) Jester datasets using	Something	Something-Something V1
The significant difference on the Something	Something	Something-Something V1
Something dataset shows the importance of	Something	Something-Something V1
shuffled inputs drawn from the Something	Something	Something-Something V1
Something dataset, in Figure 6b. In	Something	Something-Something V1
Something	Something	Something-Something V1
Something Ordered Shuffled	Something	Something-Something V1
frames and shuffled frames, on Something	Something	Something-Something V1
Something and UCF101 dataset respectively. On	Something	Something-Something V1
Something- Something, the temporal order is critical	Something	Something-Something V1
Dataset UCF Kinetics Moments Something Jester Charades	Something	Something-Something V1
can better differentiate activities in Something	Something	Something-Something V1
Something dataset	Something	Something-Something V1
using the MultiScale TRN on Something	Something	Something-Something V1
- Something and Jester dataset. Only the	Something	Something-Something V1
Something Jester Frames baseline TRN baseline	Something	Something-Something V1
V1 ImageNet 48.2 - TSN [1	V1	Something-Something V1
of 24.2% is obtained on Something	Something	Something-Something V1
a) Something	Something	Something-Something V1
on the validation set of Something	Something	Something-Something V1
three standard action recognition benchmarks, Something	Something	Something-Something V1
EPIC-KITCHENS [23] and HMDB51 [24]. Something	Something	Something-Something V1
on the three splits. Both Something	Something	Something-Something V1
videos with actions involving objects. Something	Something	Something-Something V1
3: (a) Action classes in Something	Something	Something-Something V1
on the validation set of Something	Something	Something-Something V1
Something	Something	Something-Something V1
a) Something	Something	Something-Something V1
various state-of-the-art techniques on (a) Something	Something	Something-Something V1
Something	Something	Something-Something V1
approach with state-of-the-art techniques on Something	Something	Something-Something V1
-v1 and HMDB51 datasets. For Something	Something	Something-Something V1
M. Mueller-Freitag, et al., “The” Something	Something	Something-Something V1
Something	Something	Something-Something V1
Stream MiniKinetics Kinetics400 UCF101-1 HMDB51-1 Something	Something	Something-Something V1
Stream Kinetics400 UCF101-1 HMDB51-1 Something	Something	Something-Something V1
Method Streams Pretrain UCF101 HMDB51 Something	Something	Something-Something V1
Baseline 22.3 25.0 29.7 13.6 14	14	THUMOS' 14
14	14	THUMOS' 14
14, with IoU=0.5	14	THUMOS' 14
1412	14	THUMOS' 14
1412	14	THUMOS' 14
long and the maximum is 14	14	THUMOS' 14
Table 14	14	THUMOS' 14
11.2 I3D + 3 TGMs 14	14	THUMOS' 14
3 TGMs + super-events 14	14	THUMOS' 14
In Table 14, we present the results of	14	THUMOS' 14
Charades Dataset We further test on	Charades	Charades
the popular Charades dataset [19] which is unique	Charades	Charades
all with higher-than-50% accuracy on Charades	Charades	Charades
action classification per- formances on Charades [19]. Method modality mAP	Charades	Charades
may temporally overlap in a Charades video, requiring the model to	Charades	Charades
trained and tested on the Charades and MiT datasets. For Charades	Charades	Charades
note that the performances on Charades is even more impressive at	Charades	Charades
Architecture MiT Charades	Charades	Charades
A. Farhadi, and K. Alahari. Charades	Charades	Charades
The batch size used for Charades is 128 with 128 frames	Charades	Charades
the entire video. For the Charades dataset where each video duration	Charades	Charades
to crop video around humans. Charades [55] consist of of 9848	Charades	Charades
of our methods on the Charades dataset	Charades	Charades
new state of the art. Charades	Charades	Charades
video datasets: AVA, EPIC-Kitchens, and Charades	Charades	Charades
action classifica- tion [6], and Charades video classification [38]. Our abla	Charades	Charades
6. Experiments on Charades	Charades	Charades
evaluate our approach on the Charades dataset [38]. The Charades dataset	Charades	Charades
Table 3. Training schedule on Charades	Charades	Charades
4. Action recognition accuracy on Charades	Charades	Charades
NL′ to work better on Charades, so we adopt it in	Charades	Charades
top-1) AVA EPIC Verbs (top-1) Charades	Charades	Charades
For Charades, we experiment with both ResNet-50-I3D	Charades	Charades
test sets. The improvement on Charades is not as large as	Charades	Charades
to 60 seconds is useful. Charades videos are much shorter (∼30	Charades	Charades
datasets like AVA, EPIC-Kitchens, and Charades	Charades	Charades
used for computing L, so Charades	Charades	Charades
Appendix H. Charades Training Schedule	Charades	Charades
Appendix I. Charades NL Block Details	Charades	Charades
Pre-activation vs. post-activation NL′ on Charades	Charades	Charades
choose post-activation as default for Charades due to the stronger performance	Charades	Charades
i.e., JHMDB, HMDB, and Charades) show that, PA3D out	Charades	Charades
marks, i.e., JHMDB, HMDB and Charades	Charades	Charades
involves daily activities. Charades [23] is a recent large	Charades	Charades
Charades instead of Kinetics [13] with	Charades	Charades
the other hand, Charades contains activities of 267 differ	Charades	Charades
durations. All these facts make Charades reason	Charades	Charades
Charades, under the implementation	Charades	Charades
Charades	Charades	Charades
video contains multiple labels in Charades	Charades	Charades
vs. pretraining on the large-scale Charades	Charades	Charades
formance on JHMDB, HMDB and Charades	Charades	Charades
Approaches Charades	Charades	Charades
Table 8. State-of-the-art on Charades (mAP). † denotes our repro	Charades	Charades
periments on JHMDB, HMDB and Charades, where our	Charades	Charades
datasets, such as Something-Something and Charades, and the results show that	Charades	Charades
V1 [9], V2 [16] and Charades [29]. Both three datasets are	Charades	Charades
V1 [9], V2 [16] and Charades [29]. We first introduce the	Charades	Charades
Charades [29]: The dataset is composed	Charades	Charades
along its temporal axis for Charades and Something- Something dataset respectively	Charades	Charades
As Charades is a multi-label action classification	Charades	Charades
results on Something-Something [9] and Charades [28], [29] are shown in	Charades	Charades
the evaluation results on the Charades dataset, a multi-label activity recognition	Charades	Charades
on Something Something V2 and Charades datasets with the different head	Charades	Charades
with Inception- V3 backbone on Charades dataset. The intuition behind this	Charades	Charades
on Something-Something V2 (left) and Charades (right) datasets	Charades	Charades
most in our experiments on Charades datasets (24 FPS). Uniformly spare	Charades	Charades
Figure 6. The performance on Charades dataset with different position to	Charades	Charades
proposed model on Something-Something and Charades datasets and estab- lished competitive	Charades	Charades
IN TERMS OF ON THE CHARADES DATASET. ”x/y“ IN THE THIRD	CHARADES	Charades
DIFFERENT VIDEO SAMPLING STRATEGY ON CHARADES DATASETS	CHARADES	Charades
achieve state-of-the-art results on both Charades and Something-Something datasets. Especially for	Charades	Charades
our experiments in the challenging Charades [20] and 20BN-Something- Something [21	Charades	Charades
action recognition. Especially in the Charades dataset, we obtain 4.4% boost	Charades	Charades
on two recent challenging datasets: Charades [20] and Something-Something [21]. We	Charades	Charades
on our target datasets (e.g. Charades or Something- Something) as following	Charades	Charades
d = 512. Since both Charades and Something-Something dataset are in	Charades	Charades
loss functions when training for Charades and Something-Something datasets. For Something-Something	Charades	Charades
the softmax loss function. For Charades, we apply binary sigmoid loss	Charades	Charades
two different datasets. As for Charades, the scenes are more cluttered	Charades	Charades
we sample 10 clips for Charades and 2 clips for Something-Something	Charades	Charades
Table 2. Ablations on Charades	Charades	Charades
6.2 Experiments on Charades	Charades	Charades
In the Charades experiments, following the official split	Charades	Charades
actually very small. In the Charades dataset, our graph is defined	Charades	Charades
specifically, for each video in Charades, besides the action class labels	Charades	Charades
Classification mAP (%) in the Charades dataset [20]. NL is short	Charades	Charades
is very different from the Charades dataset. In the Charades dataset	Charades	Charades
gains we have in the Charades dataset. The reason is mainly	Charades	Charades
on the UCF-101, HMDB-51, and Charades dataset	Charades	Charades
34], HMDB- 51 [18], and Charades [32], our approach significantly out	Charades	Charades
UCF-101 [34], HMDB-51 [18], and Charades [32]. UCF-101 and HMDB-51 contain	Charades	Charades
annotated with one action label. Charades contains longer (∼ 30- second	Charades	Charades
un- less otherwise stated. The Charades dataset contains 9,848 videos split	Charades	Charades
softmax following TSN [44]. On Charades we use mean average precision	Charades	Charades
videos to 340× 256. As Charades con- tains both portrait and	Charades	Charades
UCF- 101/HMDB-51 and 0.03 for Charades	Charades	Charades
Table 7: Accuracy on Charades [32]. Without using ad- ditional	Charades	Charades
evaluate our method on the Charades dataset (Table 7). As Charades	Charades	Charades
Something- Something, Jester, and Charades - which fundamentally depend on	Charades	Charades
Something-Something [9], Jester [10], and Charades [11]), which are constructed for	Charades	Charades
Left’, and ‘Turning hand counterclockwise’. Charades dataset is also a high-level	Charades	Charades
on activity classification in the Charades dataset [11], outperforming the Flow+RGB	Charades	Charades
9,28], Jester dataset [10], and Charades dataset [11] are listed in	Charades	Charades
27 148,092 human hand gesture Charades 157 9,848 daily indoor activity	Charades	Charades
3.3 Results on Jester and Charades	Charades	Charades
MultiScale TRN on the recent Charades dataset for daily activity recognition	Charades	Charades
Table 4: Results on Charades Activity Classification	Charades	Charades
2 predictions are shown above Charades frames	Charades	Charades
UCF Kinetics Moments Something Jester Charades	Charades	Charades
We test our model on YouTube-8M Large-Scale Video Understand- ing dataset	YouTube-8M	YouTube-8M
remained to be solved. Googles YouTube-8M team introduced a large multi-label	YouTube-8M	YouTube-8M
We test our model on YouTube	YouTube	YouTube-8M
remained to be solved. Googles YouTube	YouTube	YouTube-8M
We train these models on YouTube	YouTube	YouTube-8M
8M Large-Scale Video Understand- ing dataset	8M	YouTube-8M
8M team introduced a large multi-label	8M	YouTube-8M
youtube-8m 2018	youtube-8m	YouTube-8M
youtube-8m	youtube-8m	YouTube-8M
YouTube-8M	YouTube-8M	YouTube-8M
In this paper, we introduce YouTube-8M, the largest multi-label video classification	YouTube-8M	YouTube-8M
unprecedented scale and diversity of YouTube-8M will lead to ad- vances	YouTube-8M	YouTube-8M
Figure 1: YouTube-8M is a large-scale benchmark for	YouTube-8M	YouTube-8M
In this paper, we introduce YouTube-8M 1, a large-scale bench- mark	YouTube-8M	YouTube-8M
Therefore, unlike Sports-1M and ActivityNet, YouTube-8M is not restricted to action	YouTube-8M	YouTube-8M
Overall, YouTube-8M contains more than 8 million	YouTube-8M	YouTube-8M
illus- trates the scale of YouTube-8M, compared to existing image and	YouTube-8M	YouTube-8M
YouTube-8M fills the gap in video	YouTube-8M	YouTube-8M
3. YOUTUBE-8M DATASET YouTube-8M is a benchmark dataset for	YouTube-8M	YouTube-8M
Dataset Train Validate Test Total YouTube-8M 5,786,881 1,652,167 825,602 8,264,650	YouTube-8M	YouTube-8M
3.4 Dataset Statistics The YouTube-8M dataset contains 4, 800 classes	YouTube-8M	YouTube-8M
Top-level category statistics of the YouTube-8M dataset	YouTube-8M	YouTube-8M
multi-label classification approaches on the YouTube-8M dataset. We then evaluate the	YouTube-8M	YouTube-8M
rated test set of the YouTube-8M dataset. A comparison with the	YouTube-8M	YouTube-8M
5.2 Results on YouTube-8M Table 3 shows results for	YouTube-8M	YouTube-8M
all approaches on the YouTube-8M	YouTube-8M	YouTube-8M
learned using the YouTube-8M dataset and perform transfer learn	YouTube-8M	YouTube-8M
PCA matrix learned on the YouTube-8M dataset, and train MoE or	YouTube-8M	YouTube-8M
in performance by pre-training on YouTube-8M or using the transfer learnt	YouTube-8M	YouTube-8M
video representations learned on the YouTube-8M dataset to the (a) Sports-1M	YouTube-8M	YouTube-8M
LSTM layers pre-trained on the YouTube-8M task, and fine-tune them on	YouTube-8M	YouTube-8M
dataset (1M videos), pre-training on YouTube-8M still helps, and improves the	YouTube-8M	YouTube-8M
ActivityNet dataset against pre-training on YouTube-8M for aggregation based and LSTM	YouTube-8M	YouTube-8M
shows that features learned on YouTube-8M generalize very well to other	YouTube-8M	YouTube-8M
of the videos present in YouTube-8M	YouTube-8M	YouTube-8M
In this paper, we introduce YouTube-8M, a large-scale video	YouTube-8M	YouTube-8M
classification and representation learning. With YouTube-8M, our goal is to advance	YouTube-8M	YouTube-8M
YouTube	YouTube	YouTube-8M
In this paper, we introduce YouTube	YouTube	YouTube-8M
multiple) labels, we used a YouTube video annotation system, which labels	YouTube	YouTube-8M
unprecedented scale and diversity of YouTube	YouTube	YouTube-8M
Figure 1: YouTube	YouTube	YouTube-8M
In this paper, we introduce YouTube	YouTube	YouTube-8M
Therefore, unlike Sports-1M and ActivityNet, YouTube	YouTube	YouTube-8M
appear as topic annotations for YouTube videos based on the YouTube	YouTube	YouTube-8M
combination of their popularity on YouTube and manual ratings of their	YouTube	YouTube-8M
Overall, YouTube	YouTube	YouTube-8M
illus- trates the scale of YouTube	YouTube	YouTube-8M
YouTube	YouTube	YouTube-8M
3. YOUTUBE-8M DATASET YouTube	YouTube	YouTube-8M
a video. We start with YouTube videos since they are a	YouTube	YouTube-8M
many more. We use the YouTube video annotation system [2] to	YouTube	YouTube-8M
1, 000 views, using the YouTube video annotation system [2]. We	YouTube	YouTube-8M
the YouTube video annotation system. This completes	YouTube	YouTube-8M
Dataset Train Validate Test Total YouTube	YouTube	YouTube-8M
3.4 Dataset Statistics The YouTube	YouTube	YouTube-8M
Top-level category statistics of the YouTube	YouTube	YouTube-8M
Set The annotations from the YouTube video annotation system can	YouTube	YouTube-8M
multi-label classification approaches on the YouTube	YouTube	YouTube-8M
various benchmark baselines on the YouTube	YouTube	YouTube-8M
rated test set of the YouTube	YouTube	YouTube-8M
5.2 Results on YouTube	YouTube	YouTube-8M
for all approaches on the YouTube	YouTube	YouTube-8M
learned using the YouTube	YouTube	YouTube-8M
sports activities with 1.2 million YouTube videos and is one of	YouTube	YouTube-8M
PCA matrix learned on the YouTube	YouTube	YouTube-8M
in performance by pre-training on YouTube	YouTube	YouTube-8M
video representations learned on the YouTube	YouTube	YouTube-8M
LSTM layers pre-trained on the YouTube	YouTube	YouTube-8M
dataset (1M videos), pre-training on YouTube	YouTube	YouTube-8M
ActivityNet dataset against pre-training on YouTube	YouTube	YouTube-8M
shows that features learned on YouTube	YouTube	YouTube-8M
of the videos present in YouTube	YouTube	YouTube-8M
In this paper, we introduce YouTube	YouTube	YouTube-8M
classification and representation learning. With YouTube	YouTube	YouTube-8M
from popularity sig- nals on YouTube as well as manual curation	YouTube	YouTube-8M
3. YOUTUBE-8M DATASET YouTube-8M is a benchmark	YOUTUBE-8M	YouTube-8M
8M	8M	YouTube-8M
8M, the largest multi-label video classification	8M	YouTube-8M
8M will lead to ad- vances	8M	YouTube-8M
8M is a large-scale benchmark for	8M	YouTube-8M
8M 1, a large-scale bench- mark	8M	YouTube-8M
8M is not restricted to action	8M	YouTube-8M
8M contains more than 8 million	8M	YouTube-8M
8M, compared to existing image and	8M	YouTube-8M
8M fills the gap in video	8M	YouTube-8M
8M DATASET YouTube-8M is a benchmark	8M	YouTube-8M
8M 5,786,881 1,652,167 825,602 8,264,650	8M	YouTube-8M
8M dataset contains 4, 800 classes	8M	YouTube-8M
8M dataset	8M	YouTube-8M
8M dataset. We then evaluate the	8M	YouTube-8M
benchmark baselines on the YouTube- 8M dataset. We find that binary	8M	YouTube-8M
8M dataset. A comparison with the	8M	YouTube-8M
8M Table 3 shows results for	8M	YouTube-8M
8M	8M	YouTube-8M
8M dataset and perform transfer learn	8M	YouTube-8M
8M dataset, and train MoE or	8M	YouTube-8M
8M (4.1.3) 67.6 65.7 86.2	8M	YouTube-8M
8M 74.1 72.5 89.3	8M	YouTube-8M
8M 77.6 74.9 91.6	8M	YouTube-8M
8M (4.1.3) 75.6 74.2 92.4	8M	YouTube-8M
8M or using the transfer learnt	8M	YouTube-8M
8M dataset to the (a) Sports-1M	8M	YouTube-8M
8M task, and fine-tune them on	8M	YouTube-8M
8M still helps, and improves the	8M	YouTube-8M
8M for aggregation based and LSTM	8M	YouTube-8M
8M generalize very well to other	8M	YouTube-8M
8M	8M	YouTube-8M
8M, a large-scale video	8M	YouTube-8M
8M, our goal is to advance	8M	YouTube-8M
iments on multiple datasets, including Charades and MultiTHUMOS, confirm the effectiveness	Charades	Charades
public datasets including MultiTHUMOS and Charades, and was able to outperform	Charades	Charades
Kay et al., 2017), and Charades (Sigurdsson et al., 2016b) provided	Charades	Charades
Table 7. Per-frame mAP on Charades, evaluated with the ‘Cha- rades_v1_localize	Charades	Charades
4.3. Charades Dataset Charades (Sigurdsson et al., 2016b) is	Charades	Charades
experiments, we follow the original Charades detec- tion setting (i.e., Charades_v1_localize	Charades	Charades
original localization setting of the Charades dataset. Notably, it is performing	Charades	Charades
Charades L=30Charades L=15	Charades	Charades
to capture shorter events. On Charades, the Gaussians have a larger	Charades	Charades
1-D convolution. Note that for Charades, the temporal ker- nels learned	Charades	Charades
datasets including MultiTHU- MOS and Charades, obtaining the best known performance	Charades	Charades
of M on MultiTHUMOS and Charades using RGB I3D features. For	Charades	Charades
MultiTHUMOS Charades	Charades	Charades
of Cout on MultiTHUMOS and Charades using RGB I3D features. For	Charades	Charades
MultiTHUMOS Charades	Charades	Charades
1-D convolution. Note that for Charades, the temporal kernels are learned	Charades	Charades
of 3.3 seconds long. On Charades, the TGM kernels learn to	Charades	Charades
of L on MultiTHUMOS and Charades using only RGB I3D features	Charades	Charades
MultiTHUMOS Charades 1 Layer 3 Layers 1-D	Charades	Charades
movies. Existing datasets, such as Charades, have very specific actions that	Charades	Charades
Charades L=30Charades L=15	Charades	Charades
to capture shorter events. On Charades, the Gaussians have a larger	Charades	Charades
tiTHUMOS, Charades and MLB-YouTube by measuring per-frame	Charades	Charades
as the average activity in charades is 12.8 seconds and larger	charades	Charades
as the average activity in charades is 12.8 seconds and larger	charades	Charades
THUMOS [12], ActivityNet [6] and Charades [25] provided these approaches training	Charades	Charades
actions/activities, including MultiTHU- MOS [31], Charades [25], and AVA [10	Charades	Charades
what they used for the Charades Chal- lenge 2017. We also	Charades	Charades
4.3. Charades dataset	Charades	Charades
Dataset: Charades [25] is a large scale	Charades	Charades
experiments, we follow the original Charades test setting (i.e., Charades v1	Charades	Charades
stead, we followed the original Charades localization test setting (v1) from	Charades	Charades
baselines and LSTMs on the Charades dataset v1. These num- bers	Charades	Charades
Table 4. Results on Charades original dataset (i.e., Cha- rades	Charades	Charades
bit differ- ent from the Charades Challenge 2017 competition setting, whose	Charades	Charades
the localization setting of the Charades dataset. Notably, it is performing	Charades	Charades
Results from a video in Charades	Charades	Charades
drawn from 192 movies. Unlike Charades, which has individual activity classes	Charades	Charades
of spatial location. Identical to Charades (and MultiTHUMOS), we evaluate our	Charades	Charades
detection datasets, including MultiTHUMOS and Charades	Charades	Charades
for the stand-up action in Charades	Charades	Charades
human word er- ror rate (WER) of 5.9%/11.3% on the Switchboard/CallHome	WER	swb_hub_500 WER fullSWBCH
with [2] which quotes a WER of 4%, the 5.9% estimate	WER	swb_hub_500 WER fullSWBCH
achieved a surprisingly low 6.8% WER for CallHome (we were expecting	WER	swb_hub_500 WER fullSWBCH
latest ASR system achieves 5.5%/10.3% WER on SWB/CH. This means that	WER	swb_hub_500 WER fullSWBCH
as well as the human WER reported in [1]. Unsurprisingly, there	WER	swb_hub_500 WER fullSWBCH
WER SWB WER CH Transcriber 1 raw 6.1	WER	swb_hub_500 WER fullSWBCH
3 QC 5.2 7.6 Human WER from [1] 5.9 11.3	WER	swb_hub_500 WER fullSWBCH
checking contrasted with the human WER reported in [1	WER	swb_hub_500 WER fullSWBCH
way (without speaker-adversarial MTL). The WER improve- ment from adding the	WER	swb_hub_500 WER fullSWBCH
strong complementarity which improves the WER for all testsets	WER	swb_hub_500 WER fullSWBCH
WER [%] SWB CH	WER	swb_hub_500 WER fullSWBCH
Table 8: WER on SWB and CH with	WER	swb_hub_500 WER fullSWBCH
Table 8 shows WER on SWB and CH with	WER	swb_hub_500 WER fullSWBCH
LMs, word-LSTM-MTL achieved the best WER of 5.6% and 10.3% on	WER	swb_hub_500 WER fullSWBCH
we achieved 5.5% and 10.3% WER for SWB and CH respectively	WER	swb_hub_500 WER fullSWBCH
ASR performance is the average WER across all testsets which is	WER	swb_hub_500 WER fullSWBCH
argument is that the human WER of expert transcribers that were	WER	swb_hub_500 WER fullSWBCH
eval transcripts; word error rates (WER) on the NIST 2000 Switchboard	WER	swb_hub_500 WER fullSWBCH
Language model PPL WER 4-gram LM (baseline) 69.4 8.6	WER	swb_hub_500 WER fullSWBCH
Configuration WER	WER	swb_hub_500 WER fullSWBCH
to obtain the lowest possible WER on the Switchboard dataset re	WER	swb_hub_500 WER fullSWBCH
Model WER SWB WER CH RNN sigmoid (CE) 10.8	WER	swb_hub_500 WER fullSWBCH
themselves, they achieve a similar WER as our previous best model	WER	swb_hub_500 WER fullSWBCH
Table 2: WER on the SWB part of	WER	swb_hub_500 WER fullSWBCH
Model WER SWB WER CH 1-layer 1024 bottleneck 11.8	WER	swb_hub_500 WER fullSWBCH
Model WER SWB WER CH CE ST CE ST	WER	swb_hub_500 WER fullSWBCH
strong complementarity which improves the WER by 0.6% and 0.9% on	WER	swb_hub_500 WER fullSWBCH
Model WER SWB WER CH RNN maxout 9.3 15.4	WER	swb_hub_500 WER fullSWBCH
trained on more data. The WER improved by 1.0% for SWB	WER	swb_hub_500 WER fullSWBCH
LM WER SWB WER CH 30K vocab, 4M n-grams	WER	swb_hub_500 WER fullSWBCH
to a 0.4%-0.7% decrease in WER over the LSTM. Multi- layer	WER	swb_hub_500 WER fullSWBCH
yield the word error rate (WER	WER	swb_hub_500 WER fullSWBCH
this baseline by 2.4% absolute WER and 13.0% relative. The model	WER	swb_hub_500 WER fullSWBCH
DNN-HMM FSH) [28] achieves 19.9% WER when trained on the Fisher	WER	swb_hub_500 WER fullSWBCH
WER) on Switchboard dataset splits. The	WER	swb_hub_500 WER fullSWBCH
perform about the same, 9.2% WER and 9.0% WER for the	WER	swb_hub_500 WER fullSWBCH
the noisy model achieves 22.6% WER over the clean model’s 28.7	WER	swb_hub_500 WER fullSWBCH
% WER, a 6.1% absolute and 21.3	WER	swb_hub_500 WER fullSWBCH
WER) for 5 systems evaluated on	WER	swb_hub_500 WER fullSWBCH
task performance, word error rate (WER), and the proximal task of	WER	swb_hub_500 WER fullSWBCH
GMMs, yielded substantial reductions in WER on multiple challenging LVCSR tasks	WER	swb_hub_500 WER fullSWBCH
units in DNNs leads to WER gains and simpler training procedures	WER	swb_hub_500 WER fullSWBCH
be improved. We analyze the WER and classification errors made by	WER	swb_hub_500 WER fullSWBCH
ultimately lead to overall system WER improvements. Further, we look at	WER	swb_hub_500 WER fullSWBCH
minimize the word error rate (WER) of the final LVCSR system	WER	swb_hub_500 WER fullSWBCH
. WER measures mistakes at the word	WER	swb_hub_500 WER fullSWBCH
frame level and overall system WER is complex and not well	WER	swb_hub_500 WER fullSWBCH
frame-level error metrics and system-level WER to elicit insights about the	WER	swb_hub_500 WER fullSWBCH
Table I shows frame-level and WER evaluations of acoustic models of	WER	swb_hub_500 WER fullSWBCH
always a good proxy for WER performance of a final system	WER	swb_hub_500 WER fullSWBCH
. We evaluate WER on a subset of the	WER	swb_hub_500 WER fullSWBCH
DNN acoustic models substantially reduce WER on the training set. Indeed	WER	swb_hub_500 WER fullSWBCH
suggest that further training set WER reductions are possible by continuing	WER	swb_hub_500 WER fullSWBCH
on the training set in WER do not translate to large	WER	swb_hub_500 WER fullSWBCH
plot training and eval- uation WER performance during DNN training. Figure	WER	swb_hub_500 WER fullSWBCH
4 shows WER performance for our 100M and	WER	swb_hub_500 WER fullSWBCH
training. We find that training WER reduces fairly dramatically at first	WER	swb_hub_500 WER fullSWBCH
23] found a reduction in WER when using dropout on a	WER	swb_hub_500 WER fullSWBCH
Dev CrossEnt Dev Acc(%) Train WER SWBD WER CH WER EV	WER	swb_hub_500 WER fullSWBCH
WER	WER	swb_hub_500 WER fullSWBCH
4. Train and test set WER as a function of training	WER	swb_hub_500 WER fullSWBCH
with dropout to compare generalization WER performance against that of the	WER	swb_hub_500 WER fullSWBCH
to 0.4% reduction in absolute WER on the test set. While	WER	swb_hub_500 WER fullSWBCH
analyzing the training and test WER curves in Figure 4 we	WER	swb_hub_500 WER fullSWBCH
select the lowest test set WER the system achieves during DNN	WER	swb_hub_500 WER fullSWBCH
200M parameter DNN achieves 20.7% WER on the EV subset	WER	swb_hub_500 WER fullSWBCH
achieves only a 0.5% absolute WER reduction over the much smaller	WER	swb_hub_500 WER fullSWBCH
small reduction in final system WER [2	WER	swb_hub_500 WER fullSWBCH
Fig. 5. WER as a function of DNN	WER	swb_hub_500 WER fullSWBCH
Early realignment leads to better WER performance than all models we	WER	swb_hub_500 WER fullSWBCH
5 shows training and test WER curves for 100M parameter DNN	WER	swb_hub_500 WER fullSWBCH
realignment both train and test WER increase briefly. This is not	WER	swb_hub_500 WER fullSWBCH
of DNN training to reduce WER while requiring minimal additional training	WER	swb_hub_500 WER fullSWBCH
time. The training time and WER reduction of DNNs with early	WER	swb_hub_500 WER fullSWBCH
Model Features Acc(%) SWBD WER CH WER EV WER	WER	swb_hub_500 WER fullSWBCH
Results: Table IV shows both WER performance and classification accuracy of	WER	swb_hub_500 WER fullSWBCH
outperforms the CM optimizer, but WER performance across all evaluation sets	WER	swb_hub_500 WER fullSWBCH
with � “ 0.001 had WER more than 1% absolute higher	WER	swb_hub_500 WER fullSWBCH
shows the frame classification and WER performance of 5 hidden layer	WER	swb_hub_500 WER fullSWBCH
Optimizer µmax Acc(%) SWBD WER CH WER EV WER RT03	WER	swb_hub_500 WER fullSWBCH
WER	WER	swb_hub_500 WER fullSWBCH
of both frame classification and WER across all evaluation sets. Unlike	WER	swb_hub_500 WER fullSWBCH
to significant over-fitting problems in WER	WER	swb_hub_500 WER fullSWBCH
a 3.8% relative gain in WER from the 100M DNN as	WER	swb_hub_500 WER fullSWBCH
200M DNN there is relative WER gain of 2.5%. Finally the	WER	swb_hub_500 WER fullSWBCH
total parameters yields a relative WER gain of 1%. There are	WER	swb_hub_500 WER fullSWBCH
of diminishing relative gains in WER also occurs on the RT03	WER	swb_hub_500 WER fullSWBCH
both frame clas- sification and WER is the performance gain of	WER	swb_hub_500 WER fullSWBCH
VIII. WER AND FRAME CLASSIFICATION ERROR ANALYSIS	WER	swb_hub_500 WER fullSWBCH
of frame classification accuracy and WER into their constituent compo- nents	WER	swb_hub_500 WER fullSWBCH
which have the same final WER may have different rates of	WER	swb_hub_500 WER fullSWBCH
the constituent components of the WER metric	WER	swb_hub_500 WER fullSWBCH
Layers Layer Size Acc(%) SWBD WER CH WER EV WER RT03	WER	swb_hub_500 WER fullSWBCH
WER	WER	swb_hub_500 WER fullSWBCH
Figure 6 shows decomposed WER performance of HMM- DNN systems	WER	swb_hub_500 WER fullSWBCH
see that decreases in overall WER as a function of DNN	WER	swb_hub_500 WER fullSWBCH
the smaller components of overall WER	WER	swb_hub_500 WER fullSWBCH
to senones. While the three WER sub-components are linked, it is	WER	swb_hub_500 WER fullSWBCH
layer models presented in our WER analysis of Figure 6 and	WER	swb_hub_500 WER fullSWBCH
Sub Del Ins WER 0	WER	swb_hub_500 WER fullSWBCH
Fig. 6. Eval2000 WER of 5 hidden layer DNN	WER	swb_hub_500 WER fullSWBCH
of varying total parameter count. WER is broken into its sub-components	WER	swb_hub_500 WER fullSWBCH
WER performance, but only up to	WER	swb_hub_500 WER fullSWBCH
frame classification and final system WER	WER	swb_hub_500 WER fullSWBCH
on both frame classification and WER when the training corpus was	WER	swb_hub_500 WER fullSWBCH
VIII WER and Frame Classification Error Analysis	WER	swb_hub_500 WER fullSWBCH
level of 5.5%/10.3% on the Switchboard	Switchboard	Switchboard + Hub500
WER) of 5.9%/11.3% on the Switchboard	Switchboard	Switchboard + Hub500
is attainable on this particular Switchboard subset (although not achieved yet	Switchboard	Switchboard + Hub500
CallHome task. What makes the Switchboard and CallHome testsets so different	Switchboard	Switchboard + Hub500
collected by LDC under the Switchboard and Fisher protocols is almost	Switchboard	Switchboard + Hub500
entirely Switchboard	Switchboard	Switchboard + Hub500
hours of Switchboard 1 audio with transcripts provided	Switchboard	Switchboard + Hub500
data from LDC, in- cluding Switchboard, Fisher, Gigaword, and Brodcast News	Switchboard	Switchboard + Hub500
im- provements to our English Switchboard system that resulted in a	Switchboard	Switchboard + Hub500
since the release of the Switchboard corpus in the 1990s. In	Switchboard	Switchboard + Hub500
disfluencies that are pervasive. The Switchboard [10] and later Fisher [11	Switchboard	Switchboard + Hub500
with, specifically the CallHome and Switchboard portions of the NIST eval	Switchboard	Switchboard + Hub500
cite. The error rate on Switchboard is about 5.9%, and for	Switchboard	Switchboard + Hub500
numbers are 5.9% for the Switchboard portion, and 11.3% for the	Switchboard	Switchboard + Hub500
human error rate of the Switchboard data. Interestingly, the same informality	Switchboard	Switchboard + Hub500
from the DARPA EARS program: Switchboard (3M words), BBN Switchboard-2 transcripts	Switchboard	Switchboard + Hub500
WER) on the NIST 2000 Switchboard test set	Switchboard	Switchboard + Hub500
maximally complemen- tary. The RT-02 Switchboard set was used for this	Switchboard	Switchboard + Hub500
the commonly used English CTS (Switchboard and Fisher) corpora. Evaluation is	Switchboard	Switchboard + Hub500
test set, which comprises both Switchboard (SWB) and CallHome (CH) subsets	Switchboard	Switchboard + Hub500
of the CallHome conversations.3 The Switchboard	Switchboard	Switchboard + Hub500
com- mon words in the Switchboard and Fisher corpora. The de	Switchboard	Switchboard + Hub500
WER of 2.7% on the Switchboard portion of the NIST 2000	Switchboard	Switchboard + Hub500
deal of variability between the Switchboard and CallHome subsets, with 5.8	Switchboard	Switchboard + Hub500
of the art on the Switchboard recognition task. Inspired by machine	Switchboard	Switchboard + Hub500
6.9% on the NIST 2000 Switchboard task. The combined system has	Switchboard	Switchboard + Hub500
WER) on the NIST 2000 Switchboard test set	Switchboard	Switchboard + Hub500
from the DARPA EARS program: Switchboard (3M words), BBN Switchboard-2 transcripts	Switchboard	Switchboard + Hub500
The RNNLMs were trained on Switchboard and Fisher tran- scripts as	Switchboard	Switchboard + Hub500
are maximally complementary. The RT-02 Switchboard set was used for this	Switchboard	Switchboard + Hub500
the commonly used English CTS (Switchboard and Fisher) corpora. Evaluation is	Switchboard	Switchboard + Hub500
test set, which comprises both Switchboard (SWB) and CallHome (CH) subsets	Switchboard	Switchboard + Hub500
. The Switchboard	Switchboard	Switchboard + Hub500
most common words in the Switchboard and Fisher corpora. The decoder	Switchboard	Switchboard + Hub500
6.9% on the NIST 2000 Switchboard set. We believe this is	Switchboard	Switchboard + Hub500
art to 6.2% on the Switchboard test data	Switchboard	Switchboard + Hub500
of the art on the Switchboard recognition task. Inspired by machine	Switchboard	Switchboard + Hub500
6.9% on the NIST 2000 Switchboard task. The combined system has	Switchboard	Switchboard + Hub500
WER) on the NIST 2000 Switchboard test set	Switchboard	Switchboard + Hub500
from the DARPA EARS program: Switchboard (3M words), BBN Switchboard-2 transcripts	Switchboard	Switchboard + Hub500
The RNNLMs were trained on Switchboard and Fisher tran- scripts as	Switchboard	Switchboard + Hub500
are maximally complementary. The RT-02 Switchboard set was used for this	Switchboard	Switchboard + Hub500
the commonly used English CTS (Switchboard and Fisher) corpora. Evaluation is	Switchboard	Switchboard + Hub500
test set, which comprises both Switchboard (SWB) and CallHome (CH) subsets	Switchboard	Switchboard + Hub500
. The Switchboard	Switchboard	Switchboard + Hub500
most common words in the Switchboard and Fisher corpora. The decoder	Switchboard	Switchboard + Hub500
6.9% on the NIST 2000 Switchboard set. We believe this is	Switchboard	Switchboard + Hub500
art to 6.2% on the Switchboard test data	Switchboard	Switchboard + Hub500
a record 6.6% on the Switchboard subset of the Hub5 2000	Switchboard	Switchboard + Hub500
lowest possible WER on the Switchboard dataset re- gardless of other	Switchboard	Switchboard + Hub500
and maxout RNNs on the Switchboard and CallHome subsets of Hub5’00	Switchboard	Switchboard + Hub500
with such models on the Switchboard task using the Torch toolkit	Switchboard	Switchboard + Hub500
text data from LDC, including Switchboard, Fisher, Gigaword, and Broadcast News	Switchboard	Switchboard + Hub500
3. Conclusion In our previous Switchboard system paper [11] we have	Switchboard	Switchboard + Hub500
achieving human performance on the Switchboard data (estimated to be around	Switchboard	Switchboard + Hub500
since the release of the Switchboard corpus in the 1990s. In	Switchboard	Switchboard + Hub500
disfluencies that are pervasive. The Switchboard [10] and later Fisher [11	Switchboard	Switchboard + Hub500
with, specifically the CallHome and Switchboard portions of the NIST eval	Switchboard	Switchboard + Hub500
cite. The error rate on Switchboard is about 5.9%, and for	Switchboard	Switchboard + Hub500
numbers are 5.9% for the Switchboard portion, and 11.3% for the	Switchboard	Switchboard + Hub500
human error rate of the Switchboard data. Interestingly, the same informality	Switchboard	Switchboard + Hub500
from the DARPA EARS program: Switchboard (3M words), BBN Switchboard-2 transcripts	Switchboard	Switchboard + Hub500
WER) on the NIST 2000 Switchboard test set	Switchboard	Switchboard + Hub500
maximally complemen- tary. The RT-02 Switchboard set was used for this	Switchboard	Switchboard + Hub500
the commonly used English CTS (Switchboard and Fisher) corpora. Evaluation is	Switchboard	Switchboard + Hub500
test set, which comprises both Switchboard (SWB) and CallHome (CH) subsets	Switchboard	Switchboard + Hub500
of the CallHome conversations.3 The Switchboard	Switchboard	Switchboard + Hub500
com- mon words in the Switchboard and Fisher corpora. The de	Switchboard	Switchboard + Hub500
WER of 2.7% on the Switchboard portion of the NIST 2000	Switchboard	Switchboard + Hub500
deal of variability between the Switchboard and CallHome subsets, with 5.8	Switchboard	Switchboard + Hub500
system of 7.5% WER. For Switchboard, we achieve 7.2%/14.6% on the	Switchboard	Switchboard + Hub500
Switchboard	Switchboard	Switchboard + Hub500
test-other by 22% relatively. On Switchboard 300h (LDC97S62) [22], we obtain	Switchboard	Switchboard + Hub500
7.2% WER on the Switchboard portion of the Hub5’00 (LDC2002S09	Switchboard	Switchboard + Hub500
obtain 6.8%/14.1% WER on the Switchboard	Switchboard	Switchboard + Hub500
basic (LB), LibriSpeech double (LD), Switchboard mild (SM) and Switchboard strong	Switchboard	Switchboard + Hub500
for LibriSpeech and 1k for Switchboard	Switchboard	Switchboard + Hub500
training set transcripts. For the Switchboard 300h task, transcripts from the	Switchboard	Switchboard + Hub500
is subsequently turned off. For Switchboard 300h, label smoothing is turned	Switchboard	Switchboard + Hub500
For Switchboard, we use a two-layer RNN	Switchboard	Switchboard + Hub500
transcripts of the Fisher and Switchboard datasets. We find the fusion	Switchboard	Switchboard + Hub500
our experiments on LibriSpeech and Switchboard with SpecAugment. We report state-of-the-art	Switchboard	Switchboard + Hub500
4.2. Switchboard 300h	Switchboard	Switchboard + Hub500
For Switchboard 300h, we use the Kaldi	Switchboard	Switchboard + Hub500
com- bined vocabulary of the Switchboard and Fisher transcripts	Switchboard	Switchboard + Hub500
and without label smoothing for Switchboard 300h on the Switchboard and	Switchboard	Switchboard + Hub500
Table 4: Switchboard 300h WER (%) evaluated for	Switchboard	Switchboard + Hub500
We then train LAS-6-1280 on Switchboard 300h with schedule L (training	Switchboard	Switchboard + Hub500
Switchboard, whose fusion parameters are obtained	Switchboard	Switchboard + Hub500
Table 5: Switchboard 300h WERs	Switchboard	Switchboard + Hub500
the Lib- riSpeech 960h and Switchboard 300h tasks on end-to-end LAS	Switchboard	Switchboard + Hub500
Switchboard 300h	Switchboard	Switchboard + Hub500
a record 6.6% on the Switchboard subset of the Hub5 2000	Switchboard	Switchboard + Hub500
lowest possible WER on the Switchboard dataset re- gardless of other	Switchboard	Switchboard + Hub500
and maxout RNNs on the Switchboard and CallHome subsets of Hub5’00	Switchboard	Switchboard + Hub500
with such models on the Switchboard task using the Torch toolkit	Switchboard	Switchboard + Hub500
text data from LDC, including Switchboard, Fisher, Gigaword, and Broadcast News	Switchboard	Switchboard + Hub500
3. Conclusion In our previous Switchboard system paper [11] we have	Switchboard	Switchboard + Hub500
achieving human performance on the Switchboard data (estimated to be around	Switchboard	Switchboard + Hub500
of the art on the Switchboard recognition task. Inspired by machine	Switchboard	Switchboard + Hub500
6.9% on the NIST 2000 Switchboard task. The combined system has	Switchboard	Switchboard + Hub500
WER) on the NIST 2000 Switchboard test set	Switchboard	Switchboard + Hub500
from the DARPA EARS program: Switchboard (3M words), BBN Switchboard-2 transcripts	Switchboard	Switchboard + Hub500
The RNNLMs were trained on Switchboard and Fisher tran- scripts as	Switchboard	Switchboard + Hub500
are maximally complementary. The RT-02 Switchboard set was used for this	Switchboard	Switchboard + Hub500
the commonly used English CTS (Switchboard and Fisher) corpora. Evaluation is	Switchboard	Switchboard + Hub500
test set, which comprises both Switchboard (SWB) and CallHome (CH) subsets	Switchboard	Switchboard + Hub500
. The Switchboard	Switchboard	Switchboard + Hub500
most common words in the Switchboard and Fisher corpora. The decoder	Switchboard	Switchboard + Hub500
6.9% on the NIST 2000 Switchboard set. We believe this is	Switchboard	Switchboard + Hub500
art to 6.2% on the Switchboard test data	Switchboard	Switchboard + Hub500
word error rate on the Switchboard part of the Hub5-2000 evaluation	Switchboard	Switchboard + Hub500
versus Gaussian mixture models, the Switchboard corpus has become the de	Switchboard	Switchboard + Hub500
result in im- provements on Switchboard tend to work well on	Switchboard	Switchboard + Hub500
which were developed first on Switchboard and then became ubiquitous as	Switchboard	Switchboard + Hub500
Since Switchboard is such a well-studied corpus	Switchboard	Switchboard + Hub500
achieved a 43% WER on Switchboard [3]. In 2000, Cambridge University	Switchboard	Switchboard + Hub500
follows: 262 hours from the Switchboard 1 data collection, 1698 hours	Switchboard	Switchboard + Hub500
21.4K words, 40 speakers) of Switchboard data and 1.6 hours (21.6K	Switchboard	Switchboard + Hub500
12% relative gain on a Switchboard 300 hours setup over the	Switchboard	Switchboard + Hub500
e.g. LDC) training data, including Switchboard, Fisher, Gigaword, and Broadcast News	Switchboard	Switchboard + Hub500
for significant contributions to the Switchboard system	Switchboard	Switchboard + Hub500
system on the Switchboard corpus,” in Proceedings of Speech	Switchboard	Switchboard + Hub500
benchmark datasets Broadcast News and Switchboard 300 [16]. However, in contrast	Switchboard	Switchboard + Hub500
from training on a non-standard Switchboard	Switchboard	Switchboard + Hub500
Babel in 3.1 and on Switchboard in 3.2	Switchboard	Switchboard + Hub500
3.2. Switchboard 300	Switchboard	Switchboard + Hub500
Hub5’00 SWB (table 5). The Switchboard experiments focus on the very	Switchboard	Switchboard + Hub500
use multi-scale features in the Switchboard experiments, but did use speaker-dependent	Switchboard	Switchboard + Hub500
Switchboard 300	Switchboard	Switchboard + Hub500
results on the widely studied Switchboard Hub5’00, achieving 16.0% error on	Switchboard	Switchboard + Hub500
previously published methods on the Switchboard Hub5’00 corpus, achieving 16.0% error	Switchboard	Switchboard + Hub500
of 16.0% on the full Switchboard Hub5’00 test set—the best published	Switchboard	Switchboard + Hub500
WSJ read 80 280 Switchboard conversational 300 4000 Fisher conversational	Switchboard	Switchboard + Hub500
Speech. The Wall Street Journal, Switchboard and Fisher [3] corpora are	Switchboard	Switchboard + Hub500
5.1 Conversational speech: Switchboard Hub5’00 (full	Switchboard	Switchboard + Hub500
split this set into “easy” (Switchboard) and “hard” (CallHome) instances, often	Switchboard	Switchboard + Hub500
on only the 300 hour Switchboard conversational telephone speech dataset and	Switchboard	Switchboard + Hub500
trained on both Switchboard (SWB) and Fisher (FSH) [3	Switchboard	Switchboard + Hub500
in a similar manner as Switchboard	Switchboard	Switchboard + Hub500
only with 300 hours from Switchboard conversational telephone speech when testing	Switchboard	Switchboard + Hub500
Since the Switchboard and Fisher corpora are distributed	Switchboard	Switchboard + Hub500
when trained on 300 hour Switchboard	Switchboard	Switchboard + Hub500
trained on the Fisher and Switchboard transcriptions. Again, hyperparameters for the	Switchboard	Switchboard + Hub500
Published error rates (%WER) on Switchboard dataset splits. The columns labeled	Switchboard	Switchboard + Hub500
several hundreds of hours (e.g. Switchboard and Broadcast News). Larger benchmark	Switchboard	Switchboard + Hub500
5.1 Conversational speech: Switchboard Hub5'00 (full	Switchboard	Switchboard + Hub500
results on the widely studied Switchboard Hub5’00, achieving 16.0% error on	Switchboard	Switchboard + Hub500
previously published methods on the Switchboard Hub5’00 corpus, achieving 16.0% error	Switchboard	Switchboard + Hub500
of 16.0% on the full Switchboard Hub5’00 test set—the best published	Switchboard	Switchboard + Hub500
WSJ read 80 280 Switchboard conversational 300 4000 Fisher conversational	Switchboard	Switchboard + Hub500
Speech. The Wall Street Journal, Switchboard and Fisher [3] corpora are	Switchboard	Switchboard + Hub500
5.1 Conversational speech: Switchboard Hub5’00 (full	Switchboard	Switchboard + Hub500
split this set into “easy” (Switchboard) and “hard” (CallHome) instances, often	Switchboard	Switchboard + Hub500
on only the 300 hour Switchboard conversational telephone speech dataset and	Switchboard	Switchboard + Hub500
trained on both Switchboard (SWB) and Fisher (FSH) [3	Switchboard	Switchboard + Hub500
in a similar manner as Switchboard	Switchboard	Switchboard + Hub500
only with 300 hours from Switchboard conversational telephone speech when testing	Switchboard	Switchboard + Hub500
Since the Switchboard and Fisher corpora are distributed	Switchboard	Switchboard + Hub500
when trained on 300 hour Switchboard	Switchboard	Switchboard + Hub500
trained on the Fisher and Switchboard transcriptions. Again, hyperparameters for the	Switchboard	Switchboard + Hub500
Published error rates (%WER) on Switchboard dataset splits. The columns labeled	Switchboard	Switchboard + Hub500
several hundreds of hours (e.g. Switchboard and Broadcast News). Larger benchmark	Switchboard	Switchboard + Hub500
5.1 Conversational speech: Switchboard Hub5'00 (full	Switchboard	Switchboard + Hub500
exper- iments use the standard Switchboard benchmark corpus, which contains approximately	Switchboard	Switchboard + Hub500
training data by combining the Switchboard and Fisher corpora. This larger	Switchboard	Switchboard + Hub500
LVCSR system by combining the Switchboard and Fisher corpora. This results	Switchboard	Switchboard + Hub500
presents our experiments on the Switchboard corpus, which focus on regularization	Switchboard	Switchboard + Hub500
present experiments on the combined Switchboard and Fisher corpora in Section	Switchboard	Switchboard + Hub500
in separate experiments using the Switchboard 300 hour corpus and a	Switchboard	Switchboard + Hub500
size and overfitting on the Switchboard corpus while Section VI uses	Switchboard	Switchboard + Hub500
the same baseline Switchboard system to compare DCNN and	Switchboard	Switchboard + Hub500
experiments on the 300 hour Switchboard conversational telephone speech corpus (LDC97S62	Switchboard	Switchboard + Hub500
set consisting of both the Switchboard and CallHome subsets of the	Switchboard	Switchboard + Hub500
technique we evaluated on the Switchboard corpus. We note that only	Switchboard	Switchboard + Hub500
acoustic models using the same Switchboard training data as used for	Switchboard	Switchboard + Hub500
COMBINED LARGE CORPUS On the Switchboard 300 hour corpus we observed	Switchboard	Switchboard + Hub500
transcription task, we combine the Switchboard corpus with the larger Fisher	Switchboard	Switchboard + Hub500
accurate than those of the Switchboard corpus	Switchboard	Switchboard + Hub500
Switchboard training set contain 8725 tied	Switchboard	Switchboard + Hub500
the Fisher transcripts and the Switchboard Mississippi State transcripts. Kneser-Ney smoothing	Switchboard	Switchboard + Hub500
to evaluate systems on the Switchboard 300hr task. This evaluation set	Switchboard	Switchboard + Hub500
corpus to those trained on Switchboard alone. Second, we use the	Switchboard	Switchboard + Hub500
sets. Unlike with our smaller Switchboard training corpus experiments, increasing DNN	Switchboard	Switchboard + Hub500
overall as compared with our Switchboard corpus DNNs. We believe this	Switchboard	Switchboard + Hub500
a certain point. For the Switchboard corpus, we found that regularization	Switchboard	Switchboard + Hub500
V Switchboard 300 Hour Corpus	Switchboard	Switchboard + Hub500
DNNS, DCNNs, and DLUNNS on Switchboard	Switchboard	Switchboard + Hub500
exper- iments use the standard Switchboard benchmark corpus, which contains approximately	Switchboard	Switchboard + Hub500
training data by combining the Switchboard and Fisher corpora. This larger	Switchboard	Switchboard + Hub500
LVCSR system by combining the Switchboard and Fisher corpora. This results	Switchboard	Switchboard + Hub500
presents our experiments on the Switchboard corpus, which focus on regularization	Switchboard	Switchboard + Hub500
present experiments on the combined Switchboard and Fisher corpora in Section	Switchboard	Switchboard + Hub500
in separate experiments using the Switchboard 300 hour corpus and a	Switchboard	Switchboard + Hub500
size and overfitting on the Switchboard corpus while Section VI uses	Switchboard	Switchboard + Hub500
the same baseline Switchboard system to compare DCNN and	Switchboard	Switchboard + Hub500
experiments on the 300 hour Switchboard conversational telephone speech corpus (LDC97S62	Switchboard	Switchboard + Hub500
set consisting of both the Switchboard and CallHome subsets of the	Switchboard	Switchboard + Hub500
technique we evaluated on the Switchboard corpus. We note that only	Switchboard	Switchboard + Hub500
acoustic models using the same Switchboard training data as used for	Switchboard	Switchboard + Hub500
COMBINED LARGE CORPUS On the Switchboard 300 hour corpus we observed	Switchboard	Switchboard + Hub500
transcription task, we combine the Switchboard corpus with the larger Fisher	Switchboard	Switchboard + Hub500
accurate than those of the Switchboard corpus	Switchboard	Switchboard + Hub500
Switchboard training set contain 8725 tied	Switchboard	Switchboard + Hub500
the Fisher transcripts and the Switchboard Mississippi State transcripts. Kneser-Ney smoothing	Switchboard	Switchboard + Hub500
to evaluate systems on the Switchboard 300hr task. This evaluation set	Switchboard	Switchboard + Hub500
corpus to those trained on Switchboard alone. Second, we use the	Switchboard	Switchboard + Hub500
sets. Unlike with our smaller Switchboard training corpus experiments, increasing DNN	Switchboard	Switchboard + Hub500
overall as compared with our Switchboard corpus DNNs. We believe this	Switchboard	Switchboard + Hub500
a certain point. For the Switchboard corpus, we found that regularization	Switchboard	Switchboard + Hub500
V Switchboard 300 Hour Corpus	Switchboard	Switchboard + Hub500
DNNS, DCNNs, and DLUNNS on Switchboard	Switchboard	Switchboard + Hub500
results on the widely studied Switchboard Hub5’00, achieving 16.0% error on	Switchboard	Switchboard + Hub500
previously published methods on the Switchboard Hub5’00 corpus, achieving 16.0% error	Switchboard	Switchboard + Hub500
of 16.0% on the full Switchboard Hub5’00 test set—the best published	Switchboard	Switchboard + Hub500
WSJ read 80 280 Switchboard conversational 300 4000 Fisher conversational	Switchboard	Switchboard + Hub500
Speech. The Wall Street Journal, Switchboard and Fisher [3] corpora are	Switchboard	Switchboard + Hub500
5.1 Conversational speech: Switchboard Hub5’00 (full	Switchboard	Switchboard + Hub500
split this set into “easy” (Switchboard) and “hard” (CallHome) instances, often	Switchboard	Switchboard + Hub500
on only the 300 hour Switchboard conversational telephone speech dataset and	Switchboard	Switchboard + Hub500
trained on both Switchboard (SWB) and Fisher (FSH) [3	Switchboard	Switchboard + Hub500
in a similar manner as Switchboard	Switchboard	Switchboard + Hub500
only with 300 hours from Switchboard conversational telephone speech when testing	Switchboard	Switchboard + Hub500
Since the Switchboard and Fisher corpora are distributed	Switchboard	Switchboard + Hub500
when trained on 300 hour Switchboard	Switchboard	Switchboard + Hub500
trained on the Fisher and Switchboard transcriptions. Again, hyperparameters for the	Switchboard	Switchboard + Hub500
Published error rates (%WER) on Switchboard dataset splits. The columns labeled	Switchboard	Switchboard + Hub500
several hundreds of hours (e.g. Switchboard and Broadcast News). Larger benchmark	Switchboard	Switchboard + Hub500
5.1 Conversational speech: Switchboard Hub5'00 (full	Switchboard	Switchboard + Hub500
Table 2: LibriSpeech test WER (%) evaluated for varying	LibriSpeech test	LibriSpeech test-clean
achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks	LibriSpeech	LibriSpeech test-clean
outperforming all prior work. On LibriSpeech, we achieve 6.8% WER on	LibriSpeech	LibriSpeech test-clean
of Language Models (LMs). On LibriSpeech [20], we achieve 2.8% Word	LibriSpeech	LibriSpeech test-clean
an LM trained on the LibriSpeech LM corpus, we are able	LibriSpeech	LibriSpeech test-clean
a series of hand-crafted policies, LibriSpeech basic (LB), LibriSpeech double (LD	LibriSpeech	LibriSpeech test-clean
of vocabulary size 16k for LibriSpeech and 1k for Switchboard. The	LibriSpeech	LibriSpeech test-clean
WPM for LibriSpeech 960h is con- structed using	LibriSpeech	LibriSpeech test-clean
si = 140k for LibriSpeech 960h, and is subsequently turned	LibriSpeech	LibriSpeech test-clean
For LibriSpeech, we use a two-layer RNN	LibriSpeech	LibriSpeech test-clean
which is trained on the LibriSpeech LM corpus. We use identical	LibriSpeech	LibriSpeech test-clean
we describe our experiments on LibriSpeech and Switchboard with SpecAugment. We	LibriSpeech	LibriSpeech test-clean
4.1. LibriSpeech 960h	LibriSpeech	LibriSpeech test-clean
For LibriSpeech, we use the same setup	LibriSpeech	LibriSpeech test-clean
LAS-6- 1280 are trained on LibriSpeech 960h with a combination of	LibriSpeech	LibriSpeech test-clean
Table 2: LibriSpeech test WER (%) evaluated for	LibriSpeech	LibriSpeech test-clean
Table 3: LibriSpeech 960h WERs	LibriSpeech	LibriSpeech test-clean
the RT- 03 corpus. Unlike LibriSpeech, the fusion parameters do not	LibriSpeech	LibriSpeech test-clean
rate is being decayed for LibriSpeech when label smoothing is applied	LibriSpeech	LibriSpeech test-clean
the learning rate schedule for LibriSpeech	LibriSpeech	LibriSpeech test-clean
Figure 3: LAS-6-1280 on LibriSpeech with schedule D	LibriSpeech	LibriSpeech test-clean
LibriSpeech 960h	LibriSpeech	LibriSpeech test-clean
been synthe- sised via superimposing clean audio with a noisy audio	clean	LibriSpeech test-clean
Rate (WER) on the test- clean set and 6.8% WER on	clean	LibriSpeech test-clean
clean and 5.8% WER on test	clean	LibriSpeech test-clean
clean other clean other	clean	LibriSpeech test-clean
clean other clean other	clean	LibriSpeech test-clean
per- formance (2.5% WER on test-clean and 5.8% WER on test	test-clean	LibriSpeech test-clean
RWTH ASR Systems for LibriSpeech	LibriSpeech	LibriSpeech test-clean
LibriSpeech task. Detailed descriptions of the	LibriSpeech	LibriSpeech test-clean
when training on the full LibriSpeech training set, are the best	LibriSpeech	LibriSpeech test-clean
comparison shows that on the LibriSpeech 960h	LibriSpeech	LibriSpeech test-clean
a reduced 100h-subset of the LibriSpeech training	LibriSpeech	LibriSpeech test-clean
tention, LibriSpeech	LibriSpeech	LibriSpeech test-clean
the LibriSpeech task	LibriSpeech	LibriSpeech test-clean
The LibriSpeech task comprises English read speech	LibriSpeech	LibriSpeech test-clean
End-to-end results on LibriSpeech were presented in [5–9	LibriSpeech	LibriSpeech test-clean
obtained on the LibriSpeech task reflect state-of-the-art per	LibriSpeech	LibriSpeech test-clean
model officially distributed with the LibriSpeech dataset [2	LibriSpeech	LibriSpeech test-clean
perform the best LibriSpeech attention system presented in [6	LibriSpeech	LibriSpeech test-clean
and hybrid DNN/HMM results on LibriSpeech with 12k CART labels and	LibriSpeech	LibriSpeech test-clean
the LibriSpeech corpus. For comparison, also a	LibriSpeech	LibriSpeech test-clean
with the LibriSpeech corpus: dev-clean, dev-other, test-clean	LibriSpeech	LibriSpeech test-clean
2: Hybrid DNN/HMM results on LibriSpeech with differ	LibriSpeech	LibriSpeech test-clean
decoder-attention model results on LibriSpeech with different	LibriSpeech	LibriSpeech test-clean
results from other papers on LibriSpeech 960 h. CDp are	LibriSpeech	LibriSpeech test-clean
hybrid and attention-based models on LibriSpeech, to the best	LibriSpeech	LibriSpeech test-clean
two ASR systems for the LibriSpeech	LibriSpeech	LibriSpeech test-clean
state-of-the-art performance on the LibriSpeech 960h task in	LibriSpeech	LibriSpeech test-clean
comparison shows that on the LibriSpeech 960h	LibriSpeech	LibriSpeech test-clean
a reduced 100h-subset of the LibriSpeech training	LibriSpeech	LibriSpeech test-clean
on the full LibriSpeech training set, are the best	LibriSpeech	LibriSpeech test-clean
by 15% relative on the clean and 40% relative on	clean	LibriSpeech test-clean
clean, and	clean	LibriSpeech test-clean
clean, and filter randomly such that	clean	LibriSpeech test-clean
clean	clean	LibriSpeech test-clean
clean other clean other	clean	LibriSpeech test-clean
clean and dev-other sets, then	clean	LibriSpeech test-clean
clean and test-other sets. We	clean	LibriSpeech test-clean
Transformer model, respectively on the clean and other sets	clean	LibriSpeech test-clean
clean, dev-other, test-clean	clean	LibriSpeech test-clean
and test-other. The difference between clean and other is the	clean	LibriSpeech test-clean
clean quality is higher than the	clean	LibriSpeech test-clean
system only shows improvements on clean but degradation on	clean	LibriSpeech test-clean
and SAT gives mixed WER: clean	clean	LibriSpeech test-clean
clean other clean other	clean	LibriSpeech test-clean
clean	clean	LibriSpeech test-clean
clean other clean other	clean	LibriSpeech test-clean
clean	clean	LibriSpeech test-clean
clean	clean	LibriSpeech test-clean
clean other clean other	clean	LibriSpeech test-clean
clean and 9.9	clean	LibriSpeech test-clean
clean and 5.5% on test-other. Rescor	clean	LibriSpeech test-clean
clean and 5.0% on test-other. The	clean	LibriSpeech test-clean
clean and by 3.9% relative WER	clean	LibriSpeech test-clean
clean and by 27.6% relative WER	clean	LibriSpeech test-clean
clean and by 9.1% relative WER	clean	LibriSpeech test-clean
clean and by over 40% relative	clean	LibriSpeech test-clean
clean and by 13.8% relative WER	clean	LibriSpeech test-clean
clean and	clean	LibriSpeech test-clean
clean	clean	LibriSpeech test-clean
by 15% relative on the clean and 40% relative	clean	LibriSpeech test-clean
respectively apply them to the test-clean and test-other sets. We	test-clean	LibriSpeech test-clean
the LibriSpeech corpus: dev-clean, dev-other, test-clean	test-clean	LibriSpeech test-clean
a WER of 3.2% on test-clean and 9.9	test-clean	LibriSpeech test-clean
WER of 2.6% on test-clean and 5.5% on test-other. Rescor	test-clean	LibriSpeech test-clean
WER of are 2.3% on test-clean and 5.0% on test-other. The	test-clean	LibriSpeech test-clean
WER on test-clean and by 3.9% relative WER	test-clean	LibriSpeech test-clean
on test-clean and by 27.6% relative WER	test-clean	LibriSpeech test-clean
11.5% relative WER on test-clean and by 9.1% relative WER	test-clean	LibriSpeech test-clean
15% relative WER on test-clean and by over 40% relative	test-clean	LibriSpeech test-clean
WER on test-clean and by 13.8% relative WER	test-clean	LibriSpeech test-clean
test-clean	test-clean	LibriSpeech test-clean
with a greedy decoder on LibriSpeech test	LibriSpeech test	LibriSpeech test-clean
results among end-to-end models1 on LibriSpeech test	LibriSpeech test	LibriSpeech test-clean
we achieve 3.86% WER on LibriSpeech test	LibriSpeech test	LibriSpeech test-clean
improve the SOTA WER on LibriSpeech test	LibriSpeech test	LibriSpeech test-clean
we report state-of-the-art results on LibriSpeech among end-to-end speech recognition models	LibriSpeech	LibriSpeech test-clean
with a greedy decoder on LibriSpeech test-clean. We also report competitive	LibriSpeech	LibriSpeech test-clean
end-to- end models on the LibriSpeech and 2000hr Fisher+Switchboard tasks. Like	LibriSpeech	LibriSpeech test-clean
new state-of-the-art (SOTA) results on LibriSpeech [13] test-clean of 2.95% WER	LibriSpeech	LibriSpeech test-clean
results among end-to-end models1 on LibriSpeech test-other. We show competitive results	LibriSpeech	LibriSpeech test-clean
we achieve 3.86% WER on LibriSpeech test-clean	LibriSpeech	LibriSpeech test-clean
improve the SOTA WER on LibriSpeech test-clean	LibriSpeech	LibriSpeech test-clean
for WSJ and 64 for LibriSpeech and F+S	LibriSpeech	LibriSpeech test-clean
3: Sequence Masking: Greedy WER, LibriSpeech for Jasper 10x4 after 50	LibriSpeech	LibriSpeech test-clean
better on specific subsets of LibriSpeech	LibriSpeech	LibriSpeech test-clean
4: Residual Connections: Greedy WER, LibriSpeech for Jasper 10x3 after 400	LibriSpeech	LibriSpeech test-clean
3: LM perplexity vs WER. LibriSpeech dev-other. Vary- ing perplexity is	LibriSpeech	LibriSpeech test-clean
Table 5: LibriSpeech, WER	LibriSpeech	LibriSpeech test-clean
creased the WER on dev-clean LibriSpeech from 4.00% to 3.64%, a	LibriSpeech	LibriSpeech test-clean
on two read speech datasets: LibriSpeech and Wall Street Journal (WSJ	LibriSpeech	LibriSpeech test-clean
leads to SOTA results on LibriSpeech and competitive results on other	LibriSpeech	LibriSpeech test-clean
clean	clean	LibriSpeech test-clean
clean of 2.95% WER and SOTA	clean	LibriSpeech test-clean
clean	clean	LibriSpeech test-clean
clean	clean	LibriSpeech test-clean
clean dev-other test-clean test-other	clean	LibriSpeech test-clean
clean LibriSpeech from 4.00% to 3.64	clean	LibriSpeech test-clean
clean subset and SOTA among end-to-end	clean	LibriSpeech test-clean
a greedy decoder on LibriSpeech test-clean	test-clean	LibriSpeech test-clean
SOTA) results on LibriSpeech [13] test-clean of 2.95% WER and SOTA	test-clean	LibriSpeech test-clean
achieve 3.86% WER on LibriSpeech test-clean	test-clean	LibriSpeech test-clean
the SOTA WER on LibriSpeech test-clean	test-clean	LibriSpeech test-clean
Model E2E LM dev-clean dev-other test-clean test-other	test-clean	LibriSpeech test-clean
achieve SOTA performance on the test-clean subset and SOTA among end-to-end	test-clean	LibriSpeech test-clean
with a greedy decoder on LibriSpeech test-clean	LibriSpeech test-clean	LibriSpeech test-clean
we achieve 3.86% WER on LibriSpeech test-clean	LibriSpeech test-clean	LibriSpeech test-clean
improve the SOTA WER on LibriSpeech test-clean	LibriSpeech test-clean	LibriSpeech test-clean
Table 2: LibriSpeech test WER (%) evaluated for varying	LibriSpeech test	LibriSpeech test-clean
achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks	LibriSpeech	LibriSpeech test-clean
outperforming all prior work. On LibriSpeech, we achieve 6.8% WER on	LibriSpeech	LibriSpeech test-clean
of Language Models (LMs). On LibriSpeech [20], we achieve 2.8% Word	LibriSpeech	LibriSpeech test-clean
an LM trained on the LibriSpeech LM corpus, we are able	LibriSpeech	LibriSpeech test-clean
a series of hand-crafted policies, LibriSpeech basic (LB), LibriSpeech double (LD	LibriSpeech	LibriSpeech test-clean
of vocabulary size 16k for LibriSpeech and 1k for Switchboard. The	LibriSpeech	LibriSpeech test-clean
WPM for LibriSpeech 960h is con- structed using	LibriSpeech	LibriSpeech test-clean
si = 140k for LibriSpeech 960h, and is subsequently turned	LibriSpeech	LibriSpeech test-clean
For LibriSpeech, we use a two-layer RNN	LibriSpeech	LibriSpeech test-clean
which is trained on the LibriSpeech LM corpus. We use identical	LibriSpeech	LibriSpeech test-clean
we describe our experiments on LibriSpeech and Switchboard with SpecAugment. We	LibriSpeech	LibriSpeech test-clean
4.1. LibriSpeech 960h	LibriSpeech	LibriSpeech test-clean
For LibriSpeech, we use the same setup	LibriSpeech	LibriSpeech test-clean
LAS-6- 1280 are trained on LibriSpeech 960h with a combination of	LibriSpeech	LibriSpeech test-clean
Table 2: LibriSpeech test WER (%) evaluated for	LibriSpeech	LibriSpeech test-clean
Table 3: LibriSpeech 960h WERs	LibriSpeech	LibriSpeech test-clean
the RT- 03 corpus. Unlike LibriSpeech, the fusion parameters do not	LibriSpeech	LibriSpeech test-clean
rate is being decayed for LibriSpeech when label smoothing is applied	LibriSpeech	LibriSpeech test-clean
the learning rate schedule for LibriSpeech	LibriSpeech	LibriSpeech test-clean
Figure 3: LAS-6-1280 on LibriSpeech with schedule D	LibriSpeech	LibriSpeech test-clean
LibriSpeech 960h	LibriSpeech	LibriSpeech test-clean
been synthe- sised via superimposing clean audio with a noisy audio	clean	LibriSpeech test-clean
Rate (WER) on the test- clean set and 6.8% WER on	clean	LibriSpeech test-clean
clean and 5.8% WER on test	clean	LibriSpeech test-clean
clean other clean other	clean	LibriSpeech test-clean
clean other clean other	clean	LibriSpeech test-clean
per- formance (2.5% WER on test-clean and 5.8% WER on test	test-clean	LibriSpeech test-clean
best sequence-to-sequence model [9]. On clean speech, the im- provement is	clean	LibriSpeech test-clean
which contains 80 hours of clean read speech, and Librispeech [25	clean	LibriSpeech test-clean
with separate train/dev/test splits for clean and noisy speech. Each dataset	clean	LibriSpeech test-clean
clean and train- other. The validation	clean	LibriSpeech test-clean
dev-clean when testing on test- clean, and dev-other when testing on	clean	LibriSpeech test-clean
system, selected either on dev- clean or dev-other. The sequence-to-sequence baseline	clean	LibriSpeech test-clean
over CAPIO (Single) on the clean part, and is the current	clean	LibriSpeech test-clean
LM improves similarly on the clean and noisier parts, learning the	clean	LibriSpeech test-clean
gives similar performance on the clean part but significantly improves the	clean	LibriSpeech test-clean
clean	clean	LibriSpeech test-clean
clean dev-other	clean	LibriSpeech test-clean
clean and dev-other also decreases following	clean	LibriSpeech test-clean
conduct exper- iments on the LibriSpeech 100hr, 460hr, and 960hr tasks	LibriSpeech	LibriSpeech test-clean
the three subsets of the LibriSpeech task [19]: 100hr, 460hr, and	LibriSpeech	LibriSpeech test-clean
4. LibriSpeech Experimental Setup 4.1. Dataset The	LibriSpeech	LibriSpeech test-clean
LibriSpeech task [19] has three subsets	LibriSpeech	LibriSpeech test-clean
not be as relevant for LibriSpeech evaluation as we use the	LibriSpeech	LibriSpeech test-clean
official LibriSpeech lexicon without modification	LibriSpeech	LibriSpeech test-clean
especially for tasks such as LibriSpeech which feature long utterances (∼15s	LibriSpeech	LibriSpeech test-clean
WERs from previous work on LibriSpeech 960hr; for fair comparison, systems	LibriSpeech	LibriSpeech test-clean
has limited benefits for the LibriSpeech task. Table 8: WERs	LibriSpeech	LibriSpeech test-clean
Conclusion Our experiments on different LibriSpeech subsets show that word-piece and	LibriSpeech	LibriSpeech test-clean
LibriSpeech Experimental Setup	LibriSpeech	LibriSpeech test-clean
data are both split into clean and other subsets, each of	clean	LibriSpeech test-clean
dev test data (h) Size clean other clean other	clean	LibriSpeech test-clean
Unit Param. dev testclean other clean other	clean	LibriSpeech test-clean
clean, earlier than on the dev-other	clean	LibriSpeech test-clean
Unit LM dev testclean other clean other Phoneme 3-gram 5.6 15.8	clean	LibriSpeech test-clean
clean and test-other sets. For fur	clean	LibriSpeech test-clean
clean and dev-other WERs. We obtain	clean	LibriSpeech test-clean
clean, and 10.3% on the test-other	clean	LibriSpeech test-clean
clean	clean	LibriSpeech test-clean
clean	clean	LibriSpeech test-clean
scenarios. Train Unit dev testdata clean other clean other	clean	LibriSpeech test-clean
clean WER (which typi- cally also	clean	LibriSpeech test-clean
clean set; on the test-other set	clean	LibriSpeech test-clean
dev test clean other clean other	clean	LibriSpeech test-clean
Num. dev test hyp. clean other clean other	clean	LibriSpeech test-clean
a well brushed hat and clean shoes”, where bozzle is not	clean	LibriSpeech test-clean
clean other clean other Param. Word-Piece (WP) 4.4	clean	LibriSpeech test-clean
4.5% and 13.3% on the test-clean and test-other sets. For fur	test-clean	LibriSpeech test-clean
WERs of 3.6% on the test-clean, and 10.3% on the test-other	test-clean	LibriSpeech test-clean
2.5, 7.7) on the dev-clean/other, test-clean	test-clean	LibriSpeech test-clean
in both cases on the test-clean set; on the test-other set	test-clean	LibriSpeech test-clean
on the Switchboard 300h and LibriSpeech 1000h	LibriSpeech	LibriSpeech test-clean
clean evaluation subsets of LibriSpeech	LibriSpeech	LibriSpeech test-clean
300h-Switchboard and LibriSpeech [49]. In particular on Lib	LibriSpeech	LibriSpeech test-clean
of transcriptions was used. For LibriSpeech, we	LibriSpeech	LibriSpeech test-clean
for Switchboard and LibriSpeech (the weight on the attention	LibriSpeech	LibriSpeech test-clean
For LibriSpeech, we also train Kneser-Ney smoothed	LibriSpeech	LibriSpeech test-clean
and dev-other sets of LibriSpeech	LibriSpeech	LibriSpeech test-clean
and LibriSpeech, we first used the BPE	LibriSpeech	LibriSpeech test-clean
be optimal for Switchboard and LibriSpeech respec	LibriSpeech	LibriSpeech test-clean
6.3. LibriSpeech 1000h	LibriSpeech	LibriSpeech test-clean
LibriSpeech training dataset consist of about	LibriSpeech	LibriSpeech test-clean
for systems trained only using LibriSpeech data	LibriSpeech	LibriSpeech test-clean
Table 3: Comparisons on LibriSpeech 1000h. The attention	LibriSpeech	LibriSpeech test-clean
LibriSpeech task, we obtained competitive results	LibriSpeech	LibriSpeech test-clean
only the official LibriSpeech training data is used	LibriSpeech	LibriSpeech test-clean
LibriSpeech 1000h	LibriSpeech	LibriSpeech test-clean
clean and 3.82% on the test	clean	LibriSpeech test-clean
clean evaluation subsets of LibriSpeech. We	clean	LibriSpeech test-clean
clean	clean	LibriSpeech test-clean
clean evaluation subsets, which are the	clean	LibriSpeech test-clean
clean	clean	LibriSpeech test-clean
clean	clean	LibriSpeech test-clean
clean set and restricting it to	clean	LibriSpeech test-clean
clean and 3.82% on the test-clean	clean	LibriSpeech test-clean
clean other clean other	clean	LibriSpeech test-clean
clean other clean other	clean	LibriSpeech test-clean
clean and 3.82% on the	clean	LibriSpeech test-clean
clean subsets are the best results	clean	LibriSpeech test-clean
and 3.82% on the test-clean evaluation subsets, which are the	test-clean	LibriSpeech test-clean
dev-clean and 3.82% on the test-clean subsets	test-clean	LibriSpeech test-clean
test-clean subsets are the best results	test-clean	LibriSpeech test-clean
WSJ eval92 LibriSpeech test-clean LibriSpeech test	LibriSpeech test	LibriSpeech test-clean
of the art performance on LibriSpeech (Panayotov et al., 2015	LibriSpeech	LibriSpeech test-clean
both for the WSJ and LibriSpeech datasets (Panayotov et al., 2015	LibriSpeech	LibriSpeech test-clean
of the art systems on LibriSpeech (960h	LibriSpeech	LibriSpeech test-clean
experimental results on WSJ and LibriSpeech	LibriSpeech	LibriSpeech test-clean
of the art models on LibriSpeech also employ this approach (Panayotov	LibriSpeech	LibriSpeech test-clean
LibriSpeech Low Dropout 17 0.25/0.25 200/750	LibriSpeech	LibriSpeech test-clean
of labeled audio data) and LibriSpeech (Panayotov et al., 2015) (about	LibriSpeech	LibriSpeech test-clean
and EVAL92 for evaluation. For LibriSpeech, we considered the two available	LibriSpeech	LibriSpeech test-clean
model1 (about 200K words) for LibriSpeech	LibriSpeech	LibriSpeech test-clean
mini-batches of 4 utterances on LibriSpeech	LibriSpeech	LibriSpeech test-clean
about 17M trainable parameters. For LibriSpeech, architectures have about 130M and	LibriSpeech	LibriSpeech test-clean
Figure 3: LibriSpeech Letter Error Rate (LER) and	LibriSpeech	LibriSpeech test-clean
on (a) WSJ and (b) LibriSpeech	LibriSpeech	LibriSpeech test-clean
b) LibriSpeech	LibriSpeech	LibriSpeech test-clean
LER and WER on the LibriSpeech development sets, for the first	LibriSpeech	LibriSpeech test-clean
we report WERs on the LibriSpeech development sets, both for our	LibriSpeech	LibriSpeech test-clean
the art ASR systems on LibriSpeech	LibriSpeech	LibriSpeech test-clean
letter-based approaches on WSJ and LibriSpeech	LibriSpeech	LibriSpeech test-clean
performance for letter-based models on LibriSpeech is held by DEEP SPEECH	LibriSpeech	LibriSpeech test-clean
it achieves 6.7% WER on LibriSpeech clean data even with no	LibriSpeech	LibriSpeech test-clean
Concerning LibriSpeech, we summarize existing state of	LibriSpeech	LibriSpeech test-clean
other systems on WSJ and LibriSpeech	LibriSpeech	LibriSpeech test-clean
WSJ eval92 LibriSpeech test-clean LibriSpeech test-other	LibriSpeech	LibriSpeech test-clean
models), both on WSJ and LibriSpeech	LibriSpeech	LibriSpeech test-clean
system’s performance is competitive on LibriSpeech, suggesting pronunciations is implicitly well	LibriSpeech	LibriSpeech test-clean
clean, (b) on dev-other	clean	LibriSpeech test-clean
clean dev-other model LER WER LER	clean	LibriSpeech test-clean
clean	clean	LibriSpeech test-clean
al., 2018) on noisy and clean subsets respectively. On WSJ state	clean	LibriSpeech test-clean
outperforms DEEP SPEECH 2 on clean data, even though our system	clean	LibriSpeech test-clean
achieves 6.7% WER on LibriSpeech clean data even with no decoder	clean	LibriSpeech test-clean
clean LibriSpeech test-other	clean	LibriSpeech test-clean
WSJ eval92 LibriSpeech test-clean LibriSpeech test-other	test-clean	LibriSpeech test-clean
WSJ eval92 LibriSpeech test-clean LibriSpeech test-other	LibriSpeech test-clean	LibriSpeech test-clean
WSJ eval’93 6.94 4.98 8.08 LibriSpeech test	LibriSpeech test	LibriSpeech test-clean
-clean 7.89 5.33 5.83 LibriSpeech test	LibriSpeech test	LibriSpeech test-clean
conversational 300 Fisher conversational 2000 LibriSpeech read 960 Baidu read 5000	LibriSpeech	LibriSpeech test-clean
the Linguistic Data Consortium. The LibriSpeech dataset [46] is available free	LibriSpeech	LibriSpeech test-clean
advantage of the recently developed LibriSpeech corpus constructed using audio books	LibriSpeech	LibriSpeech test-clean
WSJ eval’93 6.94 4.98 8.08 LibriSpeech test-clean 7.89 5.33 5.83 LibriSpeech	LibriSpeech	LibriSpeech test-clean
providing a small benefit on clean data. The change from one	clean	LibriSpeech test-clean
system to further improve on clean read speech without further domain	clean	LibriSpeech test-clean
clean 7.89 5.33 5.83 LibriSpeech test-other	clean	LibriSpeech test-clean
CHiME eval clean 6.30 3.34 3.46 CHiME eval	clean	LibriSpeech test-clean
on noisy speech. “CHiME eval clean	clean	LibriSpeech test-clean
similar noise synthetically added to clean speech. Note that we use	clean	LibriSpeech test-clean
VoxForge (http://www.voxforge.org) dataset, which has clean speech read from speakers with	clean	LibriSpeech test-clean
of synthetically adding noise to clean speech	clean	LibriSpeech test-clean
eval’93 6.94 4.98 8.08 LibriSpeech test-clean 7.89 5.33 5.83 LibriSpeech test-other	test-clean	LibriSpeech test-clean
WSJ eval’93 6.94 4.98 8.08 LibriSpeech test-clean 7.89 5.33 5.83 LibriSpeech test-other	LibriSpeech test-clean	LibriSpeech test-clean
Wall Street Journal (WSJ) and LibriSpeech [19] datasets. The input to	LibriSpeech	LibriSpeech test-clean
On LibriSpeech dataset, the model is trained	LibriSpeech	LibriSpeech test-clean
Table 2. Performance from LibriSpeech dataset. Policy de- notes model	LibriSpeech	LibriSpeech test-clean
6]* 3.60% Ours 5.53% Ours (LibriSpeech) 4.67	LibriSpeech	LibriSpeech test-clean
methods on WSJ eval92 dataset. LibriSpeech denotes model trained using LibriSpeech	LibriSpeech	LibriSpeech test-clean
with other end-to-end methods on LibriSpeech dataset. Amodei et al. used	LibriSpeech	LibriSpeech test-clean
Comparative results from WSJ and LibriSpeech dataset are illustrated in tables	LibriSpeech	LibriSpeech test-clean
Amodei et al. [6] on LibriSpeech without using additional data. To	LibriSpeech	LibriSpeech test-clean
generalizes, we also tested our LibriSpeech model on the WSJ dataset	LibriSpeech	LibriSpeech test-clean
relative performance on WSJ and LibriSpeech by 13% and 4% over	LibriSpeech	LibriSpeech test-clean
clean and test-other set, respectively	clean	LibriSpeech test-clean
clean and test-other sets	clean	LibriSpeech test-clean
clean CER 1.76% 1.69%WER 5.33% 5.10	clean	LibriSpeech test-clean
clean CER 1.87% 1.75%WER 5.67% 5.42	clean	LibriSpeech test-clean
clean and dev-other are used for	clean	LibriSpeech test-clean
clean test-other	clean	LibriSpeech test-clean
5.42% and 14.70% on Librispeech test-clean and test-other set, respectively	test-clean	LibriSpeech test-clean
and 14.70% WER on Librispeech test-clean and test-other sets	test-clean	LibriSpeech test-clean
test-clean CER 1.87% 1.75%WER 5.67% 5.42	test-clean	LibriSpeech test-clean
Method test-clean test-other	test-clean	LibriSpeech test-clean
experiments were performed with the LibriSpeech [7] dataset. We used the	LibriSpeech	LibriSpeech test-clean
with TIMIT, DIRHA, CHiME, and LibriSpeech datasets. As a showcase to	LibriSpeech	LibriSpeech test-clean
DIRHA CHiME LibriSpeech MLP 26.1 18.7 6.5 LSTM	LibriSpeech	LibriSpeech test-clean
clean set for the hyperparameter search	clean	LibriSpeech test-clean
clean part using the fglarge decoding	clean	LibriSpeech test-clean
are re- ported on the test-clean part using the fglarge decoding	test-clean	LibriSpeech test-clean
large vocabulary setup, on the LibriSpeech evaluation dataset [38], chosen because	LibriSpeech	LibriSpeech test-clean
are trained only on the LibriSpeech training set (or on a	LibriSpeech	LibriSpeech test-clean
architecture trained and evaluated on LibriSpeech	LibriSpeech	LibriSpeech test-clean
on different subsets of the LibriSpeech dataset, with and without data	LibriSpeech	LibriSpeech test-clean
the train-clean split of the LibriSpeech dataset and 500h in the	LibriSpeech	LibriSpeech test-clean
not have much impact on LibriSpeech	LibriSpeech	LibriSpeech test-clean
architecture trained and evaluated on LibriSpeech	LibriSpeech	LibriSpeech test-clean
clean split of the LibriSpeech dataset	clean	LibriSpeech test-clean
was only applied to the clean data. For example 460x2 means	clean	LibriSpeech test-clean
460h of clean data + 460h of augmented	clean	LibriSpeech test-clean
clean dev-other test-clean test-other 460 6.3	clean	LibriSpeech test-clean
have much impact on LibriSpeech’s clean test sets (dev-clean and test-clean	clean	LibriSpeech test-clean
clean dev-other test-clean test-other nnet-256 7.3	clean	LibriSpeech test-clean
Num. hours dev-clean dev-other test-clean test-other 460 6.3 21.8 6.6	test-clean	LibriSpeech test-clean
clean test sets (dev-clean and test-clean	test-clean	LibriSpeech test-clean
Model dev-clean dev-other test-clean test-other nnet-256 7.3 19.2 7.6	test-clean	LibriSpeech test-clean
WSJ eval’92 4.94 3.60 5.03 WSJ eval	WSJ eval	WSJ eval93
WSJ read 80 Switchboard conversational 300	WSJ	WSJ eval93
English. The Wall Street Journal (WSJ), Switchboard and Fisher [13] corpora	WSJ	WSJ eval93
from the Wall Street Journal (WSJ) corpus of read news articles	WSJ	WSJ eval93
WSJ eval’92 4.94 3.60 5.03 WSJ eval’93 6.94 4.98 8.08 LibriSpeech	WSJ	WSJ eval93
has 1320 utterances from the WSJ test set read in various	WSJ	WSJ eval93
the Wall Street Journal dataset (WSJ) and on the 1000h Librispeech	WSJ	WSJ eval93
the best end-to-end systems; on WSJ, our results are competitive with	WSJ	WSJ eval93
show additional improvements on both WSJ and Librispeech by varying the	WSJ	WSJ eval93
of the Wall Street Journal (WSJ) dataset [24], which contains 80	WSJ	WSJ eval93
contain 37 million tokens for WSJ, 800 million tokens for Librispeech	WSJ	WSJ eval93
the open vocabulary task of WSJ	WSJ	WSJ eval93
Training/test splits On WSJ, models are trained on si284	WSJ	WSJ eval93
front-end for our approach). On WSJ, we use the lighter version	WSJ	WSJ eval93
and linear layer on both WSJ and Librispeech. The kernel size	WSJ	WSJ eval93
the words (162K) in the WSJ training corpus, while only the	WSJ	WSJ eval93
Word Error Rates (WER) on WSJ for the cur- rent state-of-the-art	WSJ	WSJ eval93
end-to-end systems trained only on WSJ, and hence the most	WSJ	WSJ eval93
consistent with our results on WSJ	WSJ	WSJ eval93
of our best models on WSJ and Librispeech. The fig- ure	WSJ	WSJ eval93
SCALE MEL SCALE LEARNT SCALE (WSJ	WSJ	WSJ eval93
SCALE (LIBRI/40 FILTERS) LEARNT SCALE (WSJ	WSJ	WSJ eval93
End-to-end speech recognition using lattice-free 		Switchboard (300hr)
MMI  Hossein Hadian1,2 ∗, Hossein Sameti1		Switchboard (300hr)
, Daniel Povey2,3, Sanjeev Khudanpur2,3  1Department of Computer Engineering, Sharif		Switchboard (300hr)
 University of Technology, Tehran, Iran		Switchboard (300hr)
, 2Center for Language and 		Switchboard (300hr)
Speech Processing, Johns Hopkins University, 		Switchboard (300hr)
Baltimore, MD, USA,  3Human Language Technology Center of		Switchboard (300hr)
 Excellence, Johns Hopkins University, Baltimore		Switchboard (300hr)
, USA. hhadian@jhu.edu, sameti@sharif.edu, dpovey@gmail.com, 		Switchboard (300hr)
khudanpur@jhu.edu  Abstract We present our work		Switchboard (300hr)
 on end-to-end training of acoustic		Switchboard (300hr)
 models using the lattice-free maximum		Switchboard (300hr)
 mutual information (LF-MMI) objective function		Switchboard (300hr)
 in the context of hidden		Switchboard (300hr)
 Markov models. By end-to-end training		Switchboard (300hr)
, we mean flat-start training 		Switchboard (300hr)
of a single DNN in 		Switchboard (300hr)
one stage without using any 		Switchboard (300hr)
previously trained models, forced alignments, 		Switchboard (300hr)
or building state-tying decision trees. 		Switchboard (300hr)
We use full biphones to 		Switchboard (300hr)
enable context-dependent modeling with- out 		Switchboard (300hr)
trees, and show that our 		Switchboard (300hr)
end-to-end LF-MMI approach can achieve 		Switchboard (300hr)
comparable results to regular LF-MMI 		Switchboard (300hr)
on well-known large vocabulary tasks. 		Switchboard (300hr)
We also compare with other 		Switchboard (300hr)
end-to-end methods such as CTC 		Switchboard (300hr)
in character-based and lexicon-free set- 		Switchboard (300hr)
tings and show 5 to 25 percent relative reduction in word		Switchboard (300hr)
 er- ror rates on different		Switchboard (300hr)
 large vocabulary tasks while using		Switchboard (300hr)
 signifi- cantly smaller models. Index		Switchboard (300hr)
 Terms: Hidden Markov model, end-to-end		Switchboard (300hr)
, automatic speech recognition, lattice-free 		Switchboard (300hr)
MMI, flat-start  1. Introduction In recent years		Switchboard (300hr)
, end-to-end approaches to automatic 		Switchboard (300hr)
speech recognition have received a 		Switchboard (300hr)
lot of attention. These methods 		Switchboard (300hr)
typ- ically aim to train 		Switchboard (300hr)
a neural-network-based acoustic model in 		Switchboard (300hr)
one stage without relying on 		Switchboard (300hr)
alignments from an initial model (		Switchboard (300hr)
usu- ally an HMM-GMM 		Switchboard (300hr)
model) [1] [2] [3]. For 		Switchboard (300hr)
simplicity, it is desirable to 		Switchboard (300hr)
avoid using a lexicon or 		Switchboard (300hr)
language model in these approaches; 		Switchboard (300hr)
however using a language model 		Switchboard (300hr)
usually improves the 		Switchboard (300hr)
results [4] [2] [5] [6].  On the other hand, conventional		Switchboard (300hr)
 DNN-based speech recog- nition methods		Switchboard (300hr)
 (i.e. CD-DNN-HMM) rely on alignments		Switchboard (300hr)
 and phonetic decision trees from		Switchboard (300hr)
 an HMM-GMM system [7]. These		Switchboard (300hr)
 methods usually use a frame-level		Switchboard (300hr)
 objective function – such as		Switchboard (300hr)
 cross-entropy – for training the		Switchboard (300hr)
 DNN using the alignments		Switchboard (300hr)
.  Currently, three popular end-to-end approaches		Switchboard (300hr)
 are Con- nectionist Temporal Classification		Switchboard (300hr)
 (CTC), RNN-Transducers and attention-based methods		Switchboard (300hr)
 [8]. CTC introduces a sequence		Switchboard (300hr)
- level objective function to 		Switchboard (300hr)
enable training a neural network 		Switchboard (300hr)
on sequences of speech signals 		Switchboard (300hr)
without using prior alignments [9] 		Switchboard (300hr)
and RNN-Transducer is an extension 		Switchboard (300hr)
of CTC with two sepa- 		Switchboard (300hr)
rate RNNs [10]. CTC was 		Switchboard (300hr)
a pioneering approach in end-to-end 		Switchboard (300hr)
speech recognition and state-of-the-art results 		Switchboard (300hr)
were achieved on the challenging 		Switchboard (300hr)
Fisher+Switchboard task [11] when it 		Switchboard (300hr)
was used with deep recurrent 		Switchboard (300hr)
neural networks.  By contrast, attention-based models use		Switchboard (300hr)
 a novel structure based on		Switchboard (300hr)
 an encoder network which maps		Switchboard (300hr)
 the input sequence into a		Switchboard (300hr)
 fixed-sized vector and a decoder		Switchboard (300hr)
 network which, using an attention		Switchboard (300hr)
 mechanism, generates the output sequence		Switchboard (300hr)
 using		Switchboard (300hr)
		Switchboard (300hr)
The first author performed the 		Switchboard (300hr)
work while at CLSP, Johns 		Switchboard (300hr)
Hop- kins University. This work 		Switchboard (300hr)
was partially supported by IARPA 		Switchboard (300hr)
MATE- RIAL award no. FA8650-17-C-9115 		Switchboard (300hr)
and NSF grant no. CRI-1513128.  this vector as its input		Switchboard (300hr)
. These models have performed 		Switchboard (300hr)
very well in a few 		Switchboard (300hr)
tasks such as machine 		Switchboard (300hr)
translation [12] but, unless the 		Switchboard (300hr)
training data is very large, 		Switchboard (300hr)
they have not been as 		Switchboard (300hr)
effective for speech recognition 		Switchboard (300hr)
tasks [6].  Currently the lattice-free MMI (i.e		Switchboard (300hr)
. LF-MMI) method [13] achieves 		Switchboard (300hr)
state-of-the-art results on many speech 		Switchboard (300hr)
recognition tasks [13, 14, 15, 16		Switchboard (300hr)
]. This method, like CTC, 		Switchboard (300hr)
uses a sentence- level posterior 		Switchboard (300hr)
for training the neural network 		Switchboard (300hr)
but unlike end- to-end approaches, 		Switchboard (300hr)
still loosely relies on alignments 		Switchboard (300hr)
from an HMM-GMM model. The 		Switchboard (300hr)
objective function used in this 		Switchboard (300hr)
method is maximum mutual information (		Switchboard (300hr)
MMI) in the context of 		Switchboard (300hr)
hidden Markov models [17].  In the work presented here		Switchboard (300hr)
, we aim to train 		Switchboard (300hr)
these powerful models without running 		Switchboard (300hr)
the common HMM-GMM training and 		Switchboard (300hr)
tree-building pipeline (i.e. in a 		Switchboard (300hr)
flat-start manner). Two prior 		Switchboard (300hr)
studies [18, 19] performed GMM-free 		Switchboard (300hr)
training, but used state- tying 		Switchboard (300hr)
decision trees (created using alignments 		Switchboard (300hr)
from the DNN model) for 		Switchboard (300hr)
context dependent (CD) modeling. However, 		Switchboard (300hr)
we do not use state-tying 		Switchboard (300hr)
trees, and we perform the 		Switchboard (300hr)
entire training pro- cess in 		Switchboard (300hr)
one stage (i.e. without generating 		Switchboard (300hr)
re-alignments, build- ing trees, or 		Switchboard (300hr)
performing prior estimation). Another difference 		Switchboard (300hr)
is that we use the 		Switchboard (300hr)
LF-MMI objective function instead of 		Switchboard (300hr)
maxi- mum likelihood (ML) for 		Switchboard (300hr)
training the network. In our 		Switchboard (300hr)
recently accepted journal paper [20], 		Switchboard (300hr)
flat-start LF-MMI was investigated in 		Switchboard (300hr)
a phoneme-based setting. In this 		Switchboard (300hr)
study, we explore character- based 		Switchboard (300hr)
training and lexicon-free decoding and 		Switchboard (300hr)
show that the end- to-end 		Switchboard (300hr)
LF-MMI setup outperforms other end-to-end 		Switchboard (300hr)
approaches under similar conditions.  In the two following sections		Switchboard (300hr)
, regular LF-MMI and CTC 		Switchboard (300hr)
will be briefly described, and 		Switchboard (300hr)
then in Section 4 we 		Switchboard (300hr)
will describe the end-to-end LF-MMI 		Switchboard (300hr)
setup. The experimental setup and 		Switchboard (300hr)
re- sults will be presented 		Switchboard (300hr)
in Section 5. Finally, the 		Switchboard (300hr)
conclusions appear in Section 6.  2. Regular LF-MMI The hidden		Switchboard (300hr)
 Markov model (HMM) is a		Switchboard (300hr)
 generative model com- monly used		Switchboard (300hr)
 for speech recognition. It is		Switchboard (300hr)
 usually used jointly with a		Switchboard (300hr)
 Gaussian mixture model (GMM), or		Switchboard (300hr)
 a DNN to model acoustic		Switchboard (300hr)
 data. A common approach for		Switchboard (300hr)
 learning the HMM parameters is		Switchboard (300hr)
 through maximum likelihood (ML) estimation		Switchboard (300hr)
 which has the following objective		Switchboard (300hr)
 function		Switchboard (300hr)
:  FML = U		Switchboard (300hr)
∑  u=1		Switchboard (300hr)
		Switchboard (300hr)
log pλ(x (u)|Mw(u))  = U		Switchboard (300hr)
∑  u=1		Switchboard (300hr)
		Switchboard (300hr)
log ∑  s∈M w(u		Switchboard (300hr)
)  Tu−1		Switchboard (300hr)
∏  t=0		Switchboard (300hr)
		Switchboard (300hr)
p(st+1|st)p(x(u)t |st) (1)  where λ is the set		Switchboard (300hr)
 of all HMM parameters, U		Switchboard (300hr)
 is the total number of		Switchboard (300hr)
 training utterances, and x(u) is		Switchboard (300hr)
 the uth speech utterance with		Switchboard (300hr)
		Switchboard (300hr)
Interspeech 2018 2-6 September 2018, 		Switchboard (300hr)
Hyderabad  12 10.21437/Interspeech.2018-1423		Switchboard (300hr)
		Switchboard (300hr)
		Switchboard (300hr)
		Switchboard (300hr)
		Switchboard (300hr)
		Switchboard (300hr)
		Switchboard (300hr)
		Switchboard (300hr)
		Switchboard (300hr)
		Switchboard (300hr)
		Switchboard (300hr)
		Switchboard (300hr)
		Switchboard (300hr)
		Switchboard (300hr)
		Switchboard (300hr)
		Switchboard (300hr)
		Switchboard (300hr)
		Switchboard (300hr)
1423		Switchboard (300hr)
		Switchboard (300hr)
		Switchboard (300hr)
		Switchboard (300hr)
transcription w(u) and with length 		Switchboard (300hr)
Tu. The composite HMM graph 		Switchboard (300hr)
Mw(u) represents all the possible 		Switchboard (300hr)
state sequences s per- taining 		Switchboard (300hr)
to the transcriptionw(u).  An alternative objective function is		Switchboard (300hr)
 maximum mutual in- formation (MMI		Switchboard (300hr)
). MMI is a discriminative 		Switchboard (300hr)
objective function which aims to 		Switchboard (300hr)
maximize the probability of the 		Switchboard (300hr)
reference tran- scription, while minimizing 		Switchboard (300hr)
the probability of all other 		Switchboard (300hr)
tran- scriptions:  FMMI = U		Switchboard (300hr)
∑  u=1		Switchboard (300hr)
		Switchboard (300hr)
log pλ(x  (u)|Mw(u)) pλ(x(u		Switchboard (300hr)
))  (2		Switchboard (300hr)
)  The denominator can be approximated		Switchboard (300hr)
 as		Switchboard (300hr)
:  pλ(x (u		Switchboard (300hr)
		Switchboard (300hr)
w  pλ(x (u)|Mw) ≈ pλ(x(u)|Mden) (3		Switchboard (300hr)
)  where Mden is an HMM		Switchboard (300hr)
 graph that includes all possible		Switchboard (300hr)
 se- quences of words. This		Switchboard (300hr)
 is called the denominator graph		Switchboard (300hr)
, as op- posed to 		Switchboard (300hr)
Mw(u) which is called the 		Switchboard (300hr)
numerator graph.  The denominator graph has traditionally		Switchboard (300hr)
 been estimated us- ing n-best		Switchboard (300hr)
 lists and later using lattices		Switchboard (300hr)
 [21][22]. That is because a		Switchboard (300hr)
 full denominator graph can make		Switchboard (300hr)
 the computations slow. Us- ing		Switchboard (300hr)
 a full denominator graph has		Switchboard (300hr)
 been investigated in [23] with		Switchboard (300hr)
 HMM-GMM models. More recently Povey		Switchboard (300hr)
 et. al [13] used MMI		Switchboard (300hr)
 training with HMM-DNN models using		Switchboard (300hr)
 a full denomi- nator graph		Switchboard (300hr)
 by adopting a few different		Switchboard (300hr)
 techniques such as us- ing		Switchboard (300hr)
 a phone language model (LM		Switchboard (300hr)
) (instead of a word 		Switchboard (300hr)
LM) for the denominator graph 		Switchboard (300hr)
and most importantly performing the 		Switchboard (300hr)
de- nominator computation on GPU 		Switchboard (300hr)
hardware. The phone LM for 		Switchboard (300hr)
the denominator graph was a 		Switchboard (300hr)
pruned n-gram LM trained using 		Switchboard (300hr)
the phone alignments of the 		Switchboard (300hr)
training data. Also, the composite 		Switchboard (300hr)
HMM was not used as 		Switchboard (300hr)
the numerator graph and instead 		Switchboard (300hr)
a spe- cial acyclic graph 		Switchboard (300hr)
was used which could exploit 		Switchboard (300hr)
the alignment information from a 		Switchboard (300hr)
previous HMM-GMM model. More specif- 		Switchboard (300hr)
ically, the numerator graph in 		Switchboard (300hr)
the regular LF-MMI method is 		Switchboard (300hr)
an expanded version of the 		Switchboard (300hr)
composite HMM, where the amount 		Switchboard (300hr)
of expansion of the self-loops 		Switchboard (300hr)
for each utterance is determined 		Switchboard (300hr)
according to its alignment (i.e. 		Switchboard (300hr)
it has no self-loops). The 		Switchboard (300hr)
phone model used with regular 		Switchboard (300hr)
LF-MMI is a 2-state HMM 		Switchboard (300hr)
as shown in Figure 1c.  3. CTC The CTC method		Switchboard (300hr)
 uses a blank label		Switchboard (300hr)
 – which can appear between		Switchboard (300hr)
 characters – to define an		Switchboard (300hr)
 objective function which sums over		Switchboard (300hr)
 all possible alignments of the		Switchboard (300hr)
 reference label sequence with the		Switchboard (300hr)
 input sequence of speech frames		Switchboard (300hr)
 [9		Switchboard (300hr)
]:  FCTC = U		Switchboard (300hr)
∑  u=1		Switchboard (300hr)
		Switchboard (300hr)
log p(w(u)|x(u))  = U		Switchboard (300hr)
∑  u=1		Switchboard (300hr)
		Switchboard (300hr)
log ∑  π∈B−1(w(u		Switchboard (300hr)
))  Tu−1		Switchboard (300hr)
∏  t=0		Switchboard (300hr)
		Switchboard (300hr)
p(πt|x(u)) (4)  where p(πt|x(u)) is the network		Switchboard (300hr)
 output for label sequence π		Switchboard (300hr)
 at time t given utterance		Switchboard (300hr)
 x(u), and B is a		Switchboard (300hr)
 many-to-one map that removes repetitive		Switchboard (300hr)
 labels and then blanks from		Switchboard (300hr)
 a label sequence		Switchboard (300hr)
.  3.1. Relation to HMM		Switchboard (300hr)
		Switchboard (300hr)
The CTC objective function can 		Switchboard (300hr)
be thought of as the 		Switchboard (300hr)
HMM likelihood over a composite 		Switchboard (300hr)
HMM, where each label (e.g. 		Switchboard (300hr)
a character, in character-based CTC) 		Switchboard (300hr)
has a special 2-state HMM  topology as shown in Figure		Switchboard (300hr)
 1a [24]. If we create		Switchboard (300hr)
 the compos- ite HMM by		Switchboard (300hr)
 starting with a blank state		Switchboard (300hr)
 (with a self-loop and a		Switchboard (300hr)
 forward null transition) and concatenate		Switchboard (300hr)
 the label HMMs, while inserting		Switchboard (300hr)
 a single blank state between		Switchboard (300hr)
 repetitive labels, we can see		Switchboard (300hr)
 that the set of all		Switchboard (300hr)
 paths in this composite HMM		Switchboard (300hr)
 is identical to the set		Switchboard (300hr)
 {π|π ∈ B−1(w(u))}. Therefore, comparing		Switchboard (300hr)
 equations 1 and 4 we		Switchboard (300hr)
 can see that CTC is		Switchboard (300hr)
 a special case of HMM		Switchboard (300hr)
, when the state priors, 		Switchboard (300hr)
observation priors, and transition probabilities 		Switchboard (300hr)
are all uniform and fixed. 		Switchboard (300hr)
Since CTC was the first 		Switchboard (300hr)
successful method used for end-to-end 		Switchboard (300hr)
speech recognition, we will use 		Switchboard (300hr)
its HMM topology in our 		Switchboard (300hr)
setup to compare with the 		Switchboard (300hr)
other HMM topologies shown in 		Switchboard (300hr)
Figure 1.  4. End-to-end LF-MMI In regular		Switchboard (300hr)
 LF-MMI, the DNN outputs correspond		Switchboard (300hr)
 to tied bi- phone or		Switchboard (300hr)
 triphone HMM states, where the		Switchboard (300hr)
 tying is done accord- ing		Switchboard (300hr)
 to a context-dependency tree. This		Switchboard (300hr)
 tree is in turn created		Switchboard (300hr)
 using alignments from an HMM-GMM		Switchboard (300hr)
 system [25]. We re- move		Switchboard (300hr)
 this prerequisite by using monophones		Switchboard (300hr)
 or full biphones (cf. Section		Switchboard (300hr)
 4.1). Moreover, we use the		Switchboard (300hr)
 composite HMM (with self-loops) as		Switchboard (300hr)
 the numerator graph instead of		Switchboard (300hr)
 the special acyclic graph used		Switchboard (300hr)
 in regular LF-MMI		Switchboard (300hr)
.  As a result, unlike regular		Switchboard (300hr)
 LF-MMI, there is no prior		Switchboard (300hr)
 align- ment information in the		Switchboard (300hr)
 numerator graph and there is		Switchboard (300hr)
 no re- striction on the		Switchboard (300hr)
 self-loops so there is much		Switchboard (300hr)
 more freedom for the neural		Switchboard (300hr)
 network to learn the alignments		Switchboard (300hr)
. Since we do not 		Switchboard (300hr)
have alignments for the training 		Switchboard (300hr)
data, we estimate the phone 		Switchboard (300hr)
language model for the denominator 		Switchboard (300hr)
graph using the training transcriptions (		Switchboard (300hr)
choosing a random pronunciation for 		Switchboard (300hr)
words with alternative pronunciations in 		Switchboard (300hr)
the phoneme-based setting), after inserting 		Switchboard (300hr)
silence phones with probability 0.2 		Switchboard (300hr)
between the words and with 		Switchboard (300hr)
probability 0.8 at the beginning 		Switchboard (300hr)
and end of the sen- 		Switchboard (300hr)
tences. The derivatives for MMI 		Switchboard (300hr)
are as follows:  ∂FMMI ∂y		Switchboard (300hr)
		Switchboard (300hr)
u) t (s)  = NUMγ (u) t (s		Switchboard (300hr)
)− DENγ(u)t (s) (5)  where y(u)t (s) is the		Switchboard (300hr)
 network output for state s		Switchboard (300hr)
 at time t given in		Switchboard (300hr)
- put utterance u which 		Switchboard (300hr)
we interpret as the logarithm 		Switchboard (300hr)
of an HMM state likelihood (		Switchboard (300hr)
i.e. log p(xt|s)) since state 		Switchboard (300hr)
priors have no ef- fect 		Switchboard (300hr)
in MMI training [13]. NUMγ(u)t (		Switchboard (300hr)
s) is the numerator HMM 		Switchboard (300hr)
occupation probability for state s 		Switchboard (300hr)
at time t for utterance 		Switchboard (300hr)
u, and DENγ  (u) t (s) is defined		Switchboard (300hr)
 similarly for the denominator graph		Switchboard (300hr)
.  The HMM transition probabilities are		Switchboard (300hr)
 fixed in our setup. Training		Switchboard (300hr)
 these shouldn’t make a difference		Switchboard (300hr)
 as long as there is		Switchboard (300hr)
 no state-tying because their effect		Switchboard (300hr)
 can be fully replicated by		Switchboard (300hr)
 the neural network output (i.e		Switchboard (300hr)
. the transition probabilities act 		Switchboard (300hr)
like a scale for the 		Switchboard (300hr)
state likelihoods). In other words, 		Switchboard (300hr)
the network will ignore them.  4.1. Tree-free context-dependent modeling		Switchboard (300hr)
		Switchboard (300hr)
Our initial experiments with monophone 		Switchboard (300hr)
end-to-end LF-MMI showed a remarkable 		Switchboard (300hr)
gap between the results of 		Switchboard (300hr)
end-to-end and regular LF-MMI. To 		Switchboard (300hr)
enable context-dependent modeling in an 		Switchboard (300hr)
end-to-end manner, we adopt a 		Switchboard (300hr)
simple approach where we use 		Switchboard (300hr)
full left biphones (or bichars 		Switchboard (300hr)
in the character-based case). This 		Switchboard (300hr)
is implemented as a trivial 		Switchboard (300hr)
full biphone tree. This tree 		Switchboard (300hr)
is not pruned at all (		Switchboard (300hr)
and does not do any 		Switchboard (300hr)
tying), so there is no 		Switchboard (300hr)
need for alignments and the 		Switchboard (300hr)
approach may be considered end-to-end (		Switchboard (300hr)
in the sense of not 		Switchboard (300hr)
requiring any previously trained models). 		Switchboard (300hr)
In other words, we assume 		Switchboard (300hr)
a separate HMM model for 		Switchboard (300hr)
each and every possible pair 		Switchboard (300hr)
of phonemes (or characters in 		Switchboard (300hr)
character-  13		Switchboard (300hr)
		Switchboard (300hr)
based conditions). 1 This will 		Switchboard (300hr)
create biphones that never occur 		Switchboard (300hr)
in the training data, but 		Switchboard (300hr)
they are never activated during 		Switchboard (300hr)
training and the network learns 		Switchboard (300hr)
to ignore 		Switchboard (300hr)
		Switchboard (300hr)
		Switchboard (300hr)
a) CTC’s HMM topology (b) 1		Switchboard (300hr)
-state HMM topology  (c) 2-state HMM topology (d		Switchboard (300hr)
) 3-state HMM topology  Figure 1: Different HMM topologies		Switchboard (300hr)
. The state marked 		Switchboard (300hr)
with “-” is CTC’s blank 		Switchboard (300hr)
state and is shared across 		Switchboard (300hr)
all the labels.  5. Experiments 5.1. Experimental setup		Switchboard (300hr)
		Switchboard (300hr)
We do most of our 		Switchboard (300hr)
experiments on two ASR corpora: 		Switchboard (300hr)
Switch- board [26], and WSJ (		Switchboard (300hr)
Wall Street Journal) [27]. Switchboard 		Switchboard (300hr)
is a database with 300 		Switchboard (300hr)
hours of transcribed speech. We 		Switchboard (300hr)
eval- uate on the 		Switchboard (300hr)
Hub5 ’00 set (also known 		Switchboard (300hr)
as eval2000). We re- port 		Switchboard (300hr)
word error rates (WER) on 		Switchboard (300hr)
the ”switchboard” portion of eval2000 (		Switchboard (300hr)
indicated as SW) but where 		Switchboard (300hr)
stated, we also report WER 		Switchboard (300hr)
on the Callhome subset (indicated 		Switchboard (300hr)
as CH). Where stated, we 		Switchboard (300hr)
use the Fisher data for 		Switchboard (300hr)
acoustic modeling as well (together 		Switchboard (300hr)
with Switchboard, a total of 2000 hours). For decoding we use		Switchboard (300hr)
 the Fisher+Switchboard training transcriptions to		Switchboard (300hr)
 train a 4-gram word LM		Switchboard (300hr)
 (in lexicon-based settings) or a		Switchboard (300hr)
 9-gram character LM (in lexicon-free		Switchboard (300hr)
 settings). WSJ is a database		Switchboard (300hr)
 with 80 hours of transcribed		Switchboard (300hr)
 speech. We test on the		Switchboard (300hr)
 ”eval92” subset. For lexicon- based		Switchboard (300hr)
 decoding, we use a 3-gram		Switchboard (300hr)
 LM trained on the WSJ		Switchboard (300hr)
 train- ing set transcriptions using		Switchboard (300hr)
 an extended lexicon as in		Switchboard (300hr)
 [4		Switchboard (300hr)
].  For running the experiments, we		Switchboard (300hr)
 use Kaldi [28]2. We do		Switchboard (300hr)
 not use i-vectors or other		Switchboard (300hr)
 speaker adaptation techniques in any		Switchboard (300hr)
 of the experiments. In all		Switchboard (300hr)
 the experiments we use a		Switchboard (300hr)
 TDNN- LSTM structure [14], which		Switchboard (300hr)
 has interleaving LSTM [29] and		Switchboard (300hr)
 TDNN layers [30]; please refer		Switchboard (300hr)
 to [14] for more details		Switchboard (300hr)
. As in [13], we 		Switchboard (300hr)
use a frame subsampling factor 		Switchboard (300hr)
of 3 which speeds up 		Switchboard (300hr)
training by a factor of 2		Switchboard (300hr)
. We also augment the 		Switchboard (300hr)
data with 2-fold speed perturbation 		Switchboard (300hr)
in all the experiments [31] 		Switchboard (300hr)
unless otherwise stated.  In all the end-to-end experiments		Switchboard (300hr)
, we use SGD to 		Switchboard (300hr)
train the network (in a 		Switchboard (300hr)
single stage, for 4 epochs), 		Switchboard (300hr)
on 40-dimensional MFCC features extracted 		Switchboard (300hr)
from 25ms frames every 10ms. 		Switchboard (300hr)
The features are normalized on 		Switchboard (300hr)
a per-speaker basis to have 		Switchboard (300hr)
zero mean and unit variance; 		Switchboard (300hr)
no other feature normalization or 		Switchboard (300hr)
fea- ture transform is used. 		Switchboard (300hr)
The network parameters are initialized 		Switchboard (300hr)
randomly to have zero mean 		Switchboard (300hr)
and a small variance. Unlike 		Switchboard (300hr)
other work, we do not 		Switchboard (300hr)
perform re-alignments during training.  In regular LF-MMI, all utterances		Switchboard (300hr)
 are split into chunks of		Switchboard (300hr)
 150 frames to make GPU		Switchboard (300hr)
 computations efficient. However, in end-to-end		Switchboard (300hr)
 LF-MMI, we can’t split the		Switchboard (300hr)
 utterances because we don’t have		Switchboard (300hr)
 alignments. Instead, we ensure that		Switchboard (300hr)
 all the utter- ances are		Switchboard (300hr)
 modified to be one of		Switchboard (300hr)
 around 30 distinct lengths (i.e		Switchboard (300hr)
.  1For example, on WSJ, which		Switchboard (300hr)
 has 42 phonemes (including silence		Switchboard (300hr)
), we will have a 		Switchboard (300hr)
total of 43*42*2 = 3612 		Switchboard (300hr)
HMM states (which are not 		Switchboard (300hr)
tied) when using a 2-state 		Switchboard (300hr)
HMM topology.  2This toolkit is open-source and		Switchboard (300hr)
 the source code related to		Switchboard (300hr)
 this study are available online		Switchboard (300hr)
 for reproducing the results		Switchboard (300hr)
.  Table 1: Effect of using		Switchboard (300hr)
 different HMM topologies in end-to-end		Switchboard (300hr)
 LF-MMI. 1state means 1-state HMM		Switchboard (300hr)
 topology and so on (as		Switchboard (300hr)
 in Figure 1). CT means		Switchboard (300hr)
 CTC’s equivalent HMM topology (Figure		Switchboard (300hr)
 1a). These results are without		Switchboard (300hr)
 CD modeling		Switchboard (300hr)
.  Phoneme Character 1state 2state 3state		Switchboard (300hr)
 CT 1state 2state 3state		Switchboard (300hr)
		Switchboard (300hr)
Switchboard 11.7 10.7 10.7 14.5 14		Switchboard (300hr)
.2 13.3 13.2 WSJ 3.1 3		Switchboard (300hr)
.1 3.3 5.4 5.3 5.2 5		Switchboard (300hr)
.4  Table 2: Effect of full		Switchboard (300hr)
 tree-free biphone/bichar modeling in end		Switchboard (300hr)
- to-end LF-MMI (EE-LF-MMI).  Switchboard WSJ Phone Char Phone		Switchboard (300hr)
 Char		Switchboard (300hr)
		Switchboard (300hr)
Regular LF-MMI 9.1 10.4 2.8 3		Switchboard (300hr)
.5 EE-LF-MMI (monophone) 10.7 13.3 3		Switchboard (300hr)
.1 5.2 EE-LF-MMI (full biphone) 9		Switchboard (300hr)
.6 10.9 3.0 4.1 EE-LF-MMI (		Switchboard (300hr)
regular biphone)* 9.3 10.5 2.9 3		Switchboard (300hr)
.7 * This uses regular 		Switchboard (300hr)
LF-MMI’s context-dependency tree.  buckets). When using speed perturbation		Switchboard (300hr)
, we modify the length 		Switchboard (300hr)
of each utterance to the 		Switchboard (300hr)
nearest of the distinct lengths. 		Switchboard (300hr)
Other- wise, we can pad 		Switchboard (300hr)
each utterance with silence to 		Switchboard (300hr)
reach one of the distinct 		Switchboard (300hr)
lengths.  5.2. Phone/Character HMM Topology		Switchboard (300hr)
		Switchboard (300hr)
One of the advantages of 		Switchboard (300hr)
using HMM is that we 		Switchboard (300hr)
can poten- tially improve the 		Switchboard (300hr)
alignment learning process by designing 		Switchboard (300hr)
the HMM topology for the 		Switchboard (300hr)
phones (or characters). We compare 		Switchboard (300hr)
three topologies as shown in 		Switchboard (300hr)
Figure 1{b,c,d} in Table 1, 		Switchboard (300hr)
both in character-based and phoneme-based 		Switchboard (300hr)
setups. For the character- based 		Switchboard (300hr)
setup, we also test with 		Switchboard (300hr)
CTC’s equivalent HMM topology (Figure 		Switchboard (300hr)
1a). It can be seen 		Switchboard (300hr)
that CTC’s topology performs sim- 		Switchboard (300hr)
ilar to a 1-state HMM. 		Switchboard (300hr)
Also a 2-state model performs 		Switchboard (300hr)
remark- ably better than a 		Switchboard (300hr)
single state model but a 3		Switchboard (300hr)
-state model does not significantly 		Switchboard (300hr)
outperform the 2-state model. For 		Switchboard (300hr)
the rest of the experiments 		Switchboard (300hr)
in this paper, we use 		Switchboard (300hr)
the 2-state HMM topology.  5.3. Tree-free full biphone modeling		Switchboard (300hr)
		Switchboard (300hr)
The first two rows of 		Switchboard (300hr)
Table 2 compare monophone end-to-end 		Switchboard (300hr)
LF-MMI results with regular LF-MMI (		Switchboard (300hr)
using regular pruned biphone tree) 		Switchboard (300hr)
results. Note that with regular 		Switchboard (300hr)
LF-MMI, conven- tional biphone and 		Switchboard (300hr)
triphone trees lead to similar 		Switchboard (300hr)
WERs (not shown). We can 		Switchboard (300hr)
see there is a large 		Switchboard (300hr)
gap between regular and end-to-end 		Switchboard (300hr)
LF-MMI in all cases except 		Switchboard (300hr)
in phoneme-based WSJ which is 		Switchboard (300hr)
fairly easier than other tasks. 		Switchboard (300hr)
The third row of Table 2 shows the impact of full		Switchboard (300hr)
 CD (i.e. context-dependent) model- ing		Switchboard (300hr)
 using biphones/bichars as explained in		Switchboard (300hr)
 Section 4.1, which has helped		Switchboard (300hr)
 significantly. In particular, for Switchboard		Switchboard (300hr)
 it has improved the WER		Switchboard (300hr)
 by 1.1% in phoneme-based and		Switchboard (300hr)
 2.4% in character-based setups. This		Switchboard (300hr)
 means that in a phoneme-based		Switchboard (300hr)
 setup, end-to-end LF-MMI is only		Switchboard (300hr)
 0.5% worse than regular LF		Switchboard (300hr)
- MMI on the 300hr 		Switchboard (300hr)
Switchboard task and almost the 		Switchboard (300hr)
same on WSJ. For WSJ, 		Switchboard (300hr)
there is no improvement in 		Switchboard (300hr)
the phoneme-based setup but the 		Switchboard (300hr)
WER has been improved more 		Switchboard (300hr)
than 1% in the character-based 		Switchboard (300hr)
setup. For comparison, we also 		Switchboard (300hr)
show the re- sult of 		Switchboard (300hr)
using regular LF-MMI’s tree (which 		Switchboard (300hr)
is a pruned context- dependency 		Switchboard (300hr)
tree built using HMM-GMM alignments) 		Switchboard (300hr)
in our ap- proach. Note 		Switchboard (300hr)
that this is not end-to-end 		Switchboard (300hr)
any more. We can see 		Switchboard (300hr)
that our simple full CD 		Switchboard (300hr)
technique performs almost as well 		Switchboard (300hr)
as common tree-based CD modeling.  14		Switchboard (300hr)
		Switchboard (300hr)
Table 3: Comparison of WER 		Switchboard (300hr)
for character-based end-to-end LF-MMI (EE-LF-MMI) 		Switchboard (300hr)
and CTC on the 300hr 		Switchboard (300hr)
Switchboard.  Method Parameters Lexicon LM SW		Switchboard (300hr)
 CH CTC [32] 50M N		Switchboard (300hr)
 Char NG 19.8 32.1 EE-LF-MMI		Switchboard (300hr)
 26M N Char NG 14.4		Switchboard (300hr)
 25.2 EE-LF-MMI 26M N Char		Switchboard (300hr)
 RNN 13.0 23.6 CTC [32		Switchboard (300hr)
] 50M Y Word NG 15		Switchboard (300hr)
.1 26.3 EE-LF-MMI 26M Y 		Switchboard (300hr)
Word NG 10.9 20.6 		Switchboard (300hr)
CTC [32] 50M Y Word 		Switchboard (300hr)
RNN 14.0 25.3 EE-LF-MMI 26M 		Switchboard (300hr)
Y Word RNN 9.3 18.9 		Switchboard (300hr)
EE-LF-MMI no-SP 26M Y Word 		Switchboard (300hr)
RNN 10.2 20.0  Table 4: Comparison of WER		Switchboard (300hr)
 for character-based end-to-end LF-MMI (EE-LF-MMI		Switchboard (300hr)
) and related methods on 		Switchboard (300hr)
the 2000hr Fisher+Switchboard task. no-SP 		Switchboard (300hr)
means no speed perturbation. Tot 		Switchboard (300hr)
means on all of eval2000.  Method Params Lex. LM SW		Switchboard (300hr)
 CH Tot† CTC [32] 50M		Switchboard (300hr)
 N Char NG 13.8 21.8		Switchboard (300hr)
 17.8 Attention* [33] 100M N		Switchboard (300hr)
 N 8.6 17.8 13.2 RNN-T		Switchboard (300hr)
* [33] 120M N N 8		Switchboard (300hr)
.5 16.4 12.5 EE-LF-MMI 26M 		Switchboard (300hr)
N Char NG 12.1 21.7 16		Switchboard (300hr)
.9 EE-LF-MMI 26M N Char 		Switchboard (300hr)
RNN 12.0 21.9 17.0 		Switchboard (300hr)
CTC [32] 50M Y Word 		Switchboard (300hr)
NG 11.3 18.7 15.0 		Switchboard (300hr)
RNN-T* [33] 120M Y Word 		Switchboard (300hr)
NG 8.1 17.5 12.8 EE-LF-MMI 		Switchboard (300hr)
26M Y Word NG 9.3 18		Switchboard (300hr)
.6 14.0 EE-LF-MMI no-SP 26M 		Switchboard (300hr)
Y Word NG 9.7 19.0 14		Switchboard (300hr)
.4 CTC [32] 50M Y 		Switchboard (300hr)
Word RNN 10.2 17.7 14.0 		Switchboard (300hr)
EE-LF-MMI 26M Y Word RNN 8		Switchboard (300hr)
.0 17.6 12.8 Phone 		Switchboard (300hr)
CTC [34] – Y Word 		Switchboard (300hr)
NG 10.2 16.5 13.3 Phone 		Switchboard (300hr)
EE-LF-MMI 26M Y Word NG 8		Switchboard (300hr)
.6 15.5 12.0 Phone EE-LF-MMI 		Switchboard (300hr)
26M Y Word RNN 7.5 14		Switchboard (300hr)
.6 11.0 Regular LF-MMI 28M 		Switchboard (300hr)
Y Word NG 8.3 15.0 11		Switchboard (300hr)
.6 Regular LF-MMI 28M Y 		Switchboard (300hr)
Word RNN 7.3 14.2 10		Switchboard (300hr)
.7 * These use data 		Switchboard (300hr)
augmentation by adding background 		Switchboard (300hr)
noise. † The total eval2000 		Switchboard (300hr)
WER for CTC and Attention 		Switchboard (300hr)
is the average of SW 		Switchboard (300hr)
and CH (as it is 		Switchboard (300hr)
not reported).  5.4. Comparison to other end-to-end		Switchboard (300hr)
 approaches		Switchboard (300hr)
		Switchboard (300hr)
In this section we compare 		Switchboard (300hr)
end-to-end LF-MMI with other end-to-end 		Switchboard (300hr)
methods. Tables 3 and 4 		Switchboard (300hr)
show the results on the 		Switchboard (300hr)
300hr Switchboard and 2000hr Fisher+Switchboard 		Switchboard (300hr)
tasks re- spectively, and Table 5 shows the results on WSJ		Switchboard (300hr)
. The “LM” column shows 		Switchboard (300hr)
what LM was used for 		Switchboard (300hr)
decoding. The characters we use 		Switchboard (300hr)
in character-based modeling are the 		Switchboard (300hr)
digits, the letters, apostrophe, and 		Switchboard (300hr)
space. We report WER for 		Switchboard (300hr)
both lexicon-based and lexicon-free decoding. 		Switchboard (300hr)
In lexicon-free decoding we decode 		Switchboard (300hr)
characters and separate the words 		Switchboard (300hr)
by the decoded space charac- 		Switchboard (300hr)
ters. The LMs we use 		Switchboard (300hr)
for lexicon-free decoding are character 		Switchboard (300hr)
n-grams (Char NG) and character 		Switchboard (300hr)
RNN-LMs. We use a 9-gram 		Switchboard (300hr)
character LM trained on the 		Switchboard (300hr)
training transcriptions.  On the larger 2000hr Fisher+Switchboard		Switchboard (300hr)
, end-to-end LF- MMI has 		Switchboard (300hr)
achieved around 1% improvement (on 		Switchboard (300hr)
all of eval2000) over CTC 		Switchboard (300hr)
in the lexicon-free decoding case 		Switchboard (300hr)
but the best results are 		Switchboard (300hr)
for RNN-Transducer. When using lexicon-based 		Switchboard (300hr)
decod- ing, character-based end-to-end LF-MMI 		Switchboard (300hr)
and RNN-Transducer achieve the same 		Switchboard (300hr)
result (12.8) outperforming CTC (14.0). 		Switchboard (300hr)
How- ever, the best overall 		Switchboard (300hr)
results are for the phoneme-based 		Switchboard (300hr)
end- to-end LF-MMI achieving a 		Switchboard (300hr)
total WER of 11.0 on 		Switchboard (300hr)
eval2000 (7.5 on the Switchboard 		Switchboard (300hr)
subset). For comparison, the WERs 		Switchboard (300hr)
for regular LF-MMI (which is 		Switchboard (300hr)
not end-to-end) are also shown  Table 5: Comparison of WER		Switchboard (300hr)
 for character-based end-to-end LF-MMI (EE-LF-MMI		Switchboard (300hr)
) and related methods on 		Switchboard (300hr)
WSJ.  Method Parameters Lexicon LM WER		Switchboard (300hr)
 Phone CTC [4] – Y		Switchboard (300hr)
 Word NG 7.3 Attention [35		Switchboard (300hr)
] 6.6M Y Word NG 6		Switchboard (300hr)
.7 EE-LF-MMI 8.2M Y Word 		Switchboard (300hr)
NG 4.1 EE-LF-MMI no-SP 8.2M 		Switchboard (300hr)
Y Word NG 5.3 EE-LF-MMI 8		Switchboard (300hr)
.2M N Char NG 5.4  at the end of Table		Switchboard (300hr)
 4. Note that the models		Switchboard (300hr)
 used in end-to- end LF-MMI		Switchboard (300hr)
 are considerably smaller. Also note		Switchboard (300hr)
 that in train- ing the		Switchboard (300hr)
 attention-based and RNN-Transducer models (which		Switchboard (300hr)
 are substantially larger) [33], data		Switchboard (300hr)
 augmentation with background noise has		Switchboard (300hr)
 been applied. For comparison, we		Switchboard (300hr)
 have included re- sults without		Switchboard (300hr)
 speed perturbation (no-SP) too. Meanwhile		Switchboard (300hr)
, we see significant improvements 		Switchboard (300hr)
on the smaller 300hr Switchboard 		Switchboard (300hr)
and 80hr WSJ task, in 		Switchboard (300hr)
both lexicon-free and lexicon-based de- 		Switchboard (300hr)
coding. Specifically, we see 4 		Switchboard (300hr)
to 5 percent absolute improve- 		Switchboard (300hr)
ment in WER on the 		Switchboard (300hr)
300hr Switchboard task, and 1.4 		Switchboard (300hr)
percent improvement on WSJ in 		Switchboard (300hr)
similar conditions (i.e. no-SP).  5.5. Training and decoding speed		Switchboard (300hr)
		Switchboard (300hr)
Even though the LF-MMI objective 		Switchboard (300hr)
function requires a denom- inator 		Switchboard (300hr)
computation which is nontrivial, the 		Switchboard (300hr)
training is quite fast because 		Switchboard (300hr)
we can use considerably smaller 		Switchboard (300hr)
models (compared to other end-to-end 		Switchboard (300hr)
models or CD-HMM-DNN models which 		Switchboard (300hr)
use cross-entropy) while achieving better 		Switchboard (300hr)
results. For exam- ple, the 		Switchboard (300hr)
training speed of end-to-end LF-MMI 		Switchboard (300hr)
on the 2000hr Fisher+Switchboard task 		Switchboard (300hr)
is approximately 2.1 hours of 		Switchboard (300hr)
speech per minute on a 		Switchboard (300hr)
GeForce GTX 1080 Ti GPU. 		Switchboard (300hr)
The overall data preparation and 		Switchboard (300hr)
feature extraction takes about 20 		Switchboard (300hr)
hours on a 32- core 		Switchboard (300hr)
machine and the overall network 		Switchboard (300hr)
training lasts about 3 days 		Switchboard (300hr)
on a machine with 8 		Switchboard (300hr)
GTX 1080 Ti GPUs. The 		Switchboard (300hr)
decoding real- time factor is 0		Switchboard (300hr)
.9 on Switchboard and 0.4 		Switchboard (300hr)
on WSJ (on CPU).  6. Conclusions		Switchboard (300hr)
		Switchboard (300hr)
In this study, we described 		Switchboard (300hr)
a simple HMM-based end-to-end method 		Switchboard (300hr)
for ASR and evaluated it 		Switchboard (300hr)
on well-known large vo- cabulary 		Switchboard (300hr)
speech recognition tasks. This acoustic 		Switchboard (300hr)
model is all-neural (except for 		Switchboard (300hr)
decoding/LM part), GMM-free, tree- free, 		Switchboard (300hr)
and is trained in a 		Switchboard (300hr)
flat-start manner in one stage (		Switchboard (300hr)
us- ing lattice-free MMI) without 		Switchboard (300hr)
requiring any initial alignments, pre-training, 		Switchboard (300hr)
prior estimation, or transition training. 		Switchboard (300hr)
Through experiments, we showed that 		Switchboard (300hr)
our end-to-end method outper- forms 		Switchboard (300hr)
other end-to-end methods in similar 		Switchboard (300hr)
settings, especially on smaller databases 		Switchboard (300hr)
such as the 300hr Switchboard 		Switchboard (300hr)
or 80hr WSJ tasks where 		Switchboard (300hr)
the relative improvements in WER 		Switchboard (300hr)
range from 15 to 25 		Switchboard (300hr)
percent. By training our end-to-end 		Switchboard (300hr)
model on the 2000hr Fisher+Switchboard 		Switchboard (300hr)
database, we achieved a WER 		Switchboard (300hr)
of 12.8 on all of 		Switchboard (300hr)
eval2000 (8.0 on the Switchboard 		Switchboard (300hr)
subset) in the character-based case, 		Switchboard (300hr)
and a WER of 11.0 		Switchboard (300hr)
on all of eval2000 (7.5 		Switchboard (300hr)
on the Switchboard subset) in 		Switchboard (300hr)
the phoneme-based setting. We also 		Switchboard (300hr)
showed that by using a 		Switchboard (300hr)
full biphone modeling technique, our 		Switchboard (300hr)
approach can perform almost as 		Switchboard (300hr)
well as regular LF-MMI.  7. Acknowledgements		Switchboard (300hr)
		Switchboard (300hr)
The authors would like to 		Switchboard (300hr)
thank Pegah Ghahremani, Vimal Manohar, 		Switchboard (300hr)
and Arlo Faria for their 		Switchboard (300hr)
valuable comments.  15		Switchboard (300hr)
		Switchboard (300hr)
8. References [1] A. Graves, 		Switchboard (300hr)
A.-r. Mohamed, and G. 		Switchboard (300hr)
Hinton, “Speech recognition  with deep recurrent neural networks		Switchboard (300hr)
,” in Proceedings of IEEE 		Switchboard (300hr)
In- ternational Conference on Acoustics, 		Switchboard (300hr)
Speech and Signal Process- ing (		Switchboard (300hr)
ICASSP). IEEE, 2013, pp. 6645–6649.  [2] A. Graves and N		Switchboard (300hr)
. Jaitly, “Towards end-to-end speech 		Switchboard (300hr)
recognition with recurrent neural networks,” 		Switchboard (300hr)
in Proceedings of the 31st 		Switchboard (300hr)
Inter- national Conference on Machine 		Switchboard (300hr)
Learning, 2014, pp. 1764–1772.  [3] A. Hannun, C. Case		Switchboard (300hr)
, J. Casper, B. Catanzaro, 		Switchboard (300hr)
G. Diamos, E. Elsen, R. 		Switchboard (300hr)
Prenger, S. Satheesh, S. Sengupta, 		Switchboard (300hr)
A. Coates et al., “Deep 		Switchboard (300hr)
speech: Scaling up end-to-end speech 		Switchboard (300hr)
recognition,” arXiv preprint arXiv:1412.5567, 2014.  [4] Y. Miao, M. Gowayyed		Switchboard (300hr)
, and F. Metze, “EESEN: 		Switchboard (300hr)
End-to-end speech recognition using deep 		Switchboard (300hr)
RNN models and WFST-based de- 		Switchboard (300hr)
coding,” in Proceedings of IEEE 		Switchboard (300hr)
Workshop on Automatic Speech Recognition 		Switchboard (300hr)
and Understanding (ASRU). IEEE, 2015, 		Switchboard (300hr)
pp. 167– 174.  [5] A. Maas, Z. Xie		Switchboard (300hr)
, D. Jurafsky, and A. 		Switchboard (300hr)
Ng, “Lexicon-free conversa- tional speech 		Switchboard (300hr)
recognition with neural networks,” in 		Switchboard (300hr)
Proceedings of Conference of the 		Switchboard (300hr)
North American Chapter of the 		Switchboard (300hr)
Association for Computational Linguistics: Human 		Switchboard (300hr)
Language Technologies, 2015, pp. 345–354.  [6] D. Bahdanau, J. Chorowski		Switchboard (300hr)
, D. Serdyuk, P. Brakel, 		Switchboard (300hr)
and Y. Ben- gio, “End-to-end 		Switchboard (300hr)
attention-based large vocabulary speech recog- 		Switchboard (300hr)
nition,” in Proceedings of IEEE 		Switchboard (300hr)
International Conference on Acoustics, Speech 		Switchboard (300hr)
and Signal Processing (ICASSP). IEEE, 2016, pp. 4945–4949		Switchboard (300hr)
.  [7] G. E. Dahl, D		Switchboard (300hr)
. Yu, L. Deng, and 		Switchboard (300hr)
A. Acero, “Context-dependent pre-trained deep 		Switchboard (300hr)
neural networks for large-vocabulary speech 		Switchboard (300hr)
recognition,” IEEE Transactions on Audio, 		Switchboard (300hr)
Speech, and Language Processing, vol. 20, no. 1, pp. 30–42, 2012		Switchboard (300hr)
.  [8] J. Chorowski, D. Bahdanau		Switchboard (300hr)
, K. Cho, and Y. 		Switchboard (300hr)
Bengio, “End-to- end continuous speech 		Switchboard (300hr)
recognition using attention-based recurrent NN: 		Switchboard (300hr)
first results,” arXiv preprint arXiv:1412.1602, 2014		Switchboard (300hr)
.  [9] A. Graves, S. Fernández		Switchboard (300hr)
, F. Gomez, and J. 		Switchboard (300hr)
Schmidhuber, “Con- nectionist temporal classification: 		Switchboard (300hr)
labelling unsegmented se- quence data 		Switchboard (300hr)
with recurrent neural networks,” in 		Switchboard (300hr)
Proceedings of International Conference on 		Switchboard (300hr)
Machine Learning. ACM, 2006, pp. 369		Switchboard (300hr)
–376.  [10] A. Graves, “Sequence transduction		Switchboard (300hr)
 with recurrent neural net- works		Switchboard (300hr)
,” arXiv preprint arXiv:1211.3711, 2012.  [11] A. Hannun, C. Case		Switchboard (300hr)
, J. Casper, B. Catanzaro, 		Switchboard (300hr)
G. Diamos, E. Elsen, R. 		Switchboard (300hr)
Prenger, S. Satheesh, S. Sengupta, 		Switchboard (300hr)
A. Coates et al., “Deep 		Switchboard (300hr)
speech: Scaling up end-to-end speech 		Switchboard (300hr)
recognition,” arXiv preprint arXiv:1412.5567, 2014.  [12] D. Bahdanau, K. Cho		Switchboard (300hr)
, and Y. Bengio, “Neural 		Switchboard (300hr)
machine trans- lation by jointly 		Switchboard (300hr)
learning to align and translate,” 		Switchboard (300hr)
arXiv preprint arXiv:1409.0473, 2014.  [13] D. Povey, V. Peddinti		Switchboard (300hr)
, D. Galvez, P. Ghahrmani, 		Switchboard (300hr)
V. Manohar, X. Na, Y. 		Switchboard (300hr)
Wang, and S. Khudanpur, “Purely 		Switchboard (300hr)
sequence-trained neu- ral networks for 		Switchboard (300hr)
ASR based on lattice-free MMI,” 		Switchboard (300hr)
in Proceedings of INTERSPEECH, 2016.  [14] V. Peddinti, Y. Wang		Switchboard (300hr)
, D. Povey, and S. 		Switchboard (300hr)
Khudanpur, “Low latency acoustic modeling 		Switchboard (300hr)
using temporal convolution and LSTMs,” 		Switchboard (300hr)
IEEE Signal Processing Letters, 2017.  [15] K. J. Han, S		Switchboard (300hr)
. Hahm, B.-H. Kim, J. 		Switchboard (300hr)
Kim, and I. Lane, “Deep 		Switchboard (300hr)
learning-based telephony speech recognition in 		Switchboard (300hr)
the wild,” in Pro- ceedings 		Switchboard (300hr)
of INTERSPEECH, 2017, pp. 1323–1327.  [16] W. Xiong, J. Droppo		Switchboard (300hr)
, X. Huang, F. Seide, 		Switchboard (300hr)
M. Seltzer, A. Stolcke, D. 		Switchboard (300hr)
Yu, and G. Zweig, “The 		Switchboard (300hr)
Microsoft 2016 conversational speech recognition 		Switchboard (300hr)
system,” in Proceedings of IEEE 		Switchboard (300hr)
International Con- ference on Acoustics, 		Switchboard (300hr)
Speech and Signal Processing (ICASSP). 		Switchboard (300hr)
IEEE, 2017, pp. 5255–5259.  [17] L. Bahl, “Maximum mutual		Switchboard (300hr)
 information estimation of hidden Markov		Switchboard (300hr)
 model parameters for speech recognition		Switchboard (300hr)
,” in Proceed- ings of 		Switchboard (300hr)
IEEE International Conference on Acoustics, 		Switchboard (300hr)
Speech and Signal Processing (ICASSP). 		Switchboard (300hr)
IEEE, 1986, pp. 701–704.  [18] A. Senior, G. Heigold		Switchboard (300hr)
, M. Bacchiani, and H. 		Switchboard (300hr)
Liao, “GMM-free DNN acoustic model 		Switchboard (300hr)
training,” in Proceedings of IEEE 		Switchboard (300hr)
Interna- tional Conference on Acoustics, 		Switchboard (300hr)
Speech and Signal Processing (ICASSP). 		Switchboard (300hr)
IEEE, 2014, pp. 5602–5606.  [19] C. Zhang and P		Switchboard (300hr)
. C. Woodland, “Standalone training 		Switchboard (300hr)
of context- dependent deep neural 		Switchboard (300hr)
network acoustic models,” in Proceedings 		Switchboard (300hr)
of IEEE International Conference on 		Switchboard (300hr)
Acoustics, Speech and Sig- nal 		Switchboard (300hr)
Processing (ICASSP). IEEE, 2014, pp. 5597		Switchboard (300hr)
–5601.  [20] H. Hadian, H. Sameti		Switchboard (300hr)
, D. Povey, and S. 		Switchboard (300hr)
Khudanpur, “Flat- start single-stage discriminatively 		Switchboard (300hr)
trained HMM-based models for ASR,” 		Switchboard (300hr)
IEEE Transactions on Audio, Speech, 		Switchboard (300hr)
and Language Pro- cessing (accepted), 2018		Switchboard (300hr)
.  [21] V. Valtchev, J. Odell		Switchboard (300hr)
, P. C. Woodland, and 		Switchboard (300hr)
S. J. Young, “Lattice- based 		Switchboard (300hr)
discriminative training for large vocabulary 		Switchboard (300hr)
speech recogni- tion,” in Proceedings 		Switchboard (300hr)
of IEEE International Conference on 		Switchboard (300hr)
Acous- tics, Speech, and Signal 		Switchboard (300hr)
Processing (ICASSP), vol. 2. IEEE, 1996, pp. 605–608		Switchboard (300hr)
.  [22] P. C. Woodland and		Switchboard (300hr)
 D. Povey, “Large scale discriminative		Switchboard (300hr)
 train- ing of hidden Markov		Switchboard (300hr)
 models for speech recognition,” Computer		Switchboard (300hr)
 Speech & Language, vol. 16		Switchboard (300hr)
, no. 1, pp. 25–47, 2002		Switchboard (300hr)
.  [23] S. F. Chen, B		Switchboard (300hr)
. Kingsbury, L. Mangu, D. 		Switchboard (300hr)
Povey, G. Saon, H. Soltau, 		Switchboard (300hr)
and G. Zweig, “Advances in 		Switchboard (300hr)
speech transcription at IBM under 		Switchboard (300hr)
the DARPA EARS program,” IEEE 		Switchboard (300hr)
Transactions on Audio, Speech, and 		Switchboard (300hr)
Language Processing, vol. 14, no. 5, pp. 1596–1608, 2006		Switchboard (300hr)
.  [24] A. Zeyer, E. Beck		Switchboard (300hr)
, R. Schlüter, and H. 		Switchboard (300hr)
Ney, “CTC in the con- 		Switchboard (300hr)
text of generalized full-sum HMM 		Switchboard (300hr)
training,” in Proceedings of INTERSPEECH, 2017, pp. 944–948		Switchboard (300hr)
.  [25] S. J. Young, J		Switchboard (300hr)
. J. Odell, and P. 		Switchboard (300hr)
C. Woodland, “Tree-based state tying 		Switchboard (300hr)
for high accuracy acoustic modelling,” 		Switchboard (300hr)
in Proceedings of The Workshop 		Switchboard (300hr)
on Human Language Technology. Association 		Switchboard (300hr)
for Computational Linguistics, 1994, pp. 307		Switchboard (300hr)
–312.  [26] J. J. Godfrey, E		Switchboard (300hr)
. C. Holliman, and J. 		Switchboard (300hr)
McDaniel, “SWITCH- BOARD: Telephone speech 		Switchboard (300hr)
corpus for research and develop- 		Switchboard (300hr)
ment,” in Proceedings of IEEE 		Switchboard (300hr)
International Conference on Acoustics, Speech 		Switchboard (300hr)
and Signal Processing (ICASSP), vol. 1		Switchboard (300hr)
. IEEE, 1992, pp. 517–520.  [27] D. B. Paul and		Switchboard (300hr)
 J. M. Baker, “The design		Switchboard (300hr)
 for the Wall Street Journal-based		Switchboard (300hr)
 CSR corpus,” in Proceedings of		Switchboard (300hr)
 The Workshop on Speech and		Switchboard (300hr)
 Natural Language. Association for Computational		Switchboard (300hr)
 Linguistics, 1992, pp. 357–362		Switchboard (300hr)
.  [28] D. Povey, A. Ghoshal		Switchboard (300hr)
, G. Boulianne, L. Burget, 		Switchboard (300hr)
O. Glembek, N. Goel, M. 		Switchboard (300hr)
Hannemann, P. Motlicek, Y. Qian, 		Switchboard (300hr)
P. Schwarz et al., “The 		Switchboard (300hr)
Kaldi speech recognition toolkit,” in 		Switchboard (300hr)
Proceedings of IEEE 2011 Workshop 		Switchboard (300hr)
on Automatic Speech Recognition and 		Switchboard (300hr)
Under- standing, no. EPFL-CONF-192584. IEEE 		Switchboard (300hr)
Signal Processing So- ciety, 2011.  [29] S. Hochreiter and J		Switchboard (300hr)
. Schmidhuber, “Long short-term memory,” 		Switchboard (300hr)
Neural Computation, vol. 9, no. 8, pp. 1735–1780, 1997		Switchboard (300hr)
.  [30] A. Waibel, T. Hanazawa		Switchboard (300hr)
, G. Hinton, K. Shikano, 		Switchboard (300hr)
and K. J. Lang, “Phoneme 		Switchboard (300hr)
recognition using time-delay neural networks,” 		Switchboard (300hr)
in Readings in Speech Recognition. 		Switchboard (300hr)
Elsevier, 1990, pp. 393–404.  [31] T. Ko, V. Peddinti		Switchboard (300hr)
, D. Povey, and S. 		Switchboard (300hr)
Khudanpur, “Audio augmen- tation for 		Switchboard (300hr)
speech recognition,” in Proceedings of 		Switchboard (300hr)
INTERSPEECH, 2015, pp. 3586–3589.  [32] G. Zweig, C. Yu		Switchboard (300hr)
, J. Droppo, and A. 		Switchboard (300hr)
Stolcke, “Advances in all-neural speech 		Switchboard (300hr)
recognition,” in Proceedings of IEEE 		Switchboard (300hr)
Interna- tional Conference on Acoustics, 		Switchboard (300hr)
Speech, and Signal Processing (ICASSP). 		Switchboard (300hr)
IEEE, 2017, pp. 4805–4809.  [33] E. Battenberg, J. Chen		Switchboard (300hr)
, R. Child, A. Coates, 		Switchboard (300hr)
Y. Gaur, Y. Li, H. 		Switchboard (300hr)
Liu, S. Satheesh, D. Seetapun, 		Switchboard (300hr)
A. Sriram et al., “Exploring 		Switchboard (300hr)
neural transducers for end-to-end speech 		Switchboard (300hr)
recognition,” arXiv preprint arXiv:1707.07413, 2017.  [34] K. Audhkhasi, B. Ramabhadran		Switchboard (300hr)
, G. Saon, M. Picheny, 		Switchboard (300hr)
and D. Na- hamoo, “Direct 		Switchboard (300hr)
acoustics-to-word models for english conversa- 		Switchboard (300hr)
tional speech recognition,” in Proceedings 		Switchboard (300hr)
of INTERSPEECH, 2017, pp. 959–963.  [35] J. Chorowski and N		Switchboard (300hr)
. Jaitly, “Towards better decoding 		Switchboard (300hr)
and lan- guage model integration 		Switchboard (300hr)
in sequence to sequence models,” 		Switchboard (300hr)
in Pro- ceedings of INTERSPEECH, 2017		Switchboard (300hr)
.  16		Switchboard (300hr)
		Switchboard (300hr)
regular LF- MMI on the 300hr Switchboard task and almost the	(300hr)	Switchboard (300hr)
EE-LF-MMI) and CTC on the 300hr Switchboard	(300hr)	Switchboard (300hr)
show the results on the 300hr Switchboard and 2000hr Fisher+Switchboard tasks	(300hr)	Switchboard (300hr)
significant improvements on the smaller 300hr Switchboard and 80hr WSJ task	(300hr)	Switchboard (300hr)
ment in WER on the 300hr Switchboard task, and 1.4 percent	(300hr)	Switchboard (300hr)
smaller databases such as the 300hr Switchboard or 80hr WSJ tasks	(300hr)	Switchboard (300hr)
Switchboard task [11] when it was	Switchboard	Switchboard (300hr)
WSJ (Wall Street Journal) [27]. Switchboard is a database with 300	Switchboard	Switchboard (300hr)
modeling as well (together with Switchboard, a total of 2000 hours	Switchboard	Switchboard (300hr)
Switchboard training transcriptions to train a	Switchboard	Switchboard (300hr)
Switchboard 11.7 10.7 10.7 14.5 14.2	Switchboard	Switchboard (300hr)
Switchboard WSJ Phone Char Phone Char	Switchboard	Switchboard (300hr)
helped significantly. In particular, for Switchboard it has improved the WER	Switchboard	Switchboard (300hr)
LF- MMI on the 300hr Switchboard task and almost the same	Switchboard	Switchboard (300hr)
and CTC on the 300hr Switchboard	Switchboard	Switchboard (300hr)
Switchboard task. no-SP means no speed	Switchboard	Switchboard (300hr)
the results on the 300hr Switchboard and 2000hr Fisher+Switchboard tasks re	Switchboard	Switchboard (300hr)
Switchboard, end-to-end LF- MMI has achieved	Switchboard	Switchboard (300hr)
on eval2000 (7.5 on the Switchboard subset). For comparison, the WERs	Switchboard	Switchboard (300hr)
improvements on the smaller 300hr Switchboard and 80hr WSJ task, in	Switchboard	Switchboard (300hr)
in WER on the 300hr Switchboard task, and 1.4 percent improvement	Switchboard	Switchboard (300hr)
Switchboard task is approximately 2.1 hours	Switchboard	Switchboard (300hr)
time factor is 0.9 on Switchboard and 0.4 on WSJ (on	Switchboard	Switchboard (300hr)
databases such as the 300hr Switchboard or 80hr WSJ tasks where	Switchboard	Switchboard (300hr)
Switchboard database, we achieved a WER	Switchboard	Switchboard (300hr)
of eval2000 (8.0 on the Switchboard subset) in the character-based case	Switchboard	Switchboard (300hr)
of eval2000 (7.5 on the Switchboard subset) in the phoneme-based setting	Switchboard	Switchboard (300hr)
regular LF- MMI on the 300hr Switchboard task and almost the	300hr	Switchboard (300hr)
EE-LF-MMI) and CTC on the 300hr Switchboard	300hr	Switchboard (300hr)
show the results on the 300hr Switchboard and 2000hr Fisher+Switchboard tasks	300hr	Switchboard (300hr)
significant improvements on the smaller 300hr Switchboard and 80hr WSJ task	300hr	Switchboard (300hr)
ment in WER on the 300hr Switchboard task, and 1.4 percent	300hr	Switchboard (300hr)
smaller databases such as the 300hr Switchboard or 80hr WSJ tasks	300hr	Switchboard (300hr)
experiments was performed with the TIMIT corpus, considering the standard phoneme	TIMIT	TIMIT
for the test set of TIMIT with various neural architectures	TIMIT	TIMIT
discuss the baselines obtained with TIMIT, DIRHA, CHiME, and LibriSpeech datasets	TIMIT	TIMIT
the experimental validation conducted on TIMIT	TIMIT	TIMIT
Table 2: PER(%) obtained on TIMIT when progressively applying some techniques	TIMIT	TIMIT
the best-published performance on the TIMIT test-set	TIMIT	TIMIT
firming our previous achievements on TIMIT	TIMIT	TIMIT
experiments was performed with the TIMIT corpus, considering the standard phoneme	TIMIT	TIMIT
for the test set of TIMIT with various neural architectures	TIMIT	TIMIT
discuss the baselines obtained with TIMIT, DIRHA, CHiME, and LibriSpeech datasets	TIMIT	TIMIT
the experimental validation conducted on TIMIT	TIMIT	TIMIT
Table 2: PER(%) obtained on TIMIT when progressively applying some techniques	TIMIT	TIMIT
the best-published performance on the TIMIT test-set	TIMIT	TIMIT
firming our previous achievements on TIMIT	TIMIT	TIMIT
experiments was performed with the TIMIT corpus, considering the standard phoneme	TIMIT	TIMIT
for the test set of TIMIT with various neural architectures	TIMIT	TIMIT
discuss the baselines obtained with TIMIT, DIRHA, CHiME, and LibriSpeech datasets	TIMIT	TIMIT
the experimental validation conducted on TIMIT	TIMIT	TIMIT
Table 2: PER(%) obtained on TIMIT when progressively applying some techniques	TIMIT	TIMIT
the best-published performance on the TIMIT test-set	TIMIT	TIMIT
firming our previous achievements on TIMIT	TIMIT	TIMIT
for a GRU trained on TIMIT in a chunk of the	TIMIT	TIMIT
for a GRU trained on TIMIT	TIMIT	TIMIT
Features Dataset. TIMIT DIRHA CHiME TED	TIMIT	TIMIT
set of experiments with the TIMIT corpus was performed to test	TIMIT	TIMIT
will then be reported for TIMIT, DIRHA-English, CHiME as well as	TIMIT	TIMIT
MFCC features and trained with TIMIT	TIMIT	TIMIT
with and without batch normalization (TIMIT dataset, MFCC features	TIMIT	TIMIT
results of CTC on the TIMIT data set. In these experiments	TIMIT	TIMIT
for the test set of TIMIT with various CTC RNN architectures	TIMIT	TIMIT
for the test set of TIMIT with various RNN architectures	TIMIT	TIMIT
E. Other results on TIMIT	TIMIT	TIMIT
ASR performance obtained with the TIMIT dataset. The first row reports	TIMIT	TIMIT
best published performance on the TIMIT test-set	TIMIT	TIMIT
TABLE VI: PER(%) of the TIMIT dataset (MFCC features) split into	TIMIT	TIMIT
layers and neurons for each TIMIT RNN model. The outcome of	TIMIT	TIMIT
first set of experiments on TIMIT, in this sub-section we assess	TIMIT	TIMIT
comparable to that observed for TIMIT, confirming that Li-GRU still outperform	TIMIT	TIMIT
XXXXXXXXArch. Dataset. TIMIT DIRHA CHiME TED	TIMIT	TIMIT
V-E Other results on TIMIT	TIMIT	TIMIT
experiments was performed with the TIMIT corpus, considering the standard phoneme	TIMIT	TIMIT
for the test set of TIMIT with various neural architectures	TIMIT	TIMIT
discuss the baselines obtained with TIMIT, DIRHA, CHiME, and LibriSpeech datasets	TIMIT	TIMIT
the experimental validation conducted on TIMIT	TIMIT	TIMIT
Table 2: PER(%) obtained on TIMIT when progressively applying some techniques	TIMIT	TIMIT
the best-published performance on the TIMIT test-set	TIMIT	TIMIT
firming our previous achievements on TIMIT	TIMIT	TIMIT
experiments was performed with the TIMIT corpus, considering the standard phoneme	TIMIT	TIMIT
for the test set of TIMIT with various neural architectures	TIMIT	TIMIT
discuss the baselines obtained with TIMIT, DIRHA, CHiME, and LibriSpeech datasets	TIMIT	TIMIT
the experimental validation conducted on TIMIT	TIMIT	TIMIT
Table 2: PER(%) obtained on TIMIT when progressively applying some techniques	TIMIT	TIMIT
the best-published performance on the TIMIT test-set	TIMIT	TIMIT
firming our previous achievements on TIMIT	TIMIT	TIMIT
experiments was performed with the TIMIT corpus, considering the standard phoneme	TIMIT	TIMIT
for the test set of TIMIT with various neural architectures	TIMIT	TIMIT
discuss the baselines obtained with TIMIT, DIRHA, CHiME, and LibriSpeech datasets	TIMIT	TIMIT
the experimental validation conducted on TIMIT	TIMIT	TIMIT
Table 2: PER(%) obtained on TIMIT when progressively applying some techniques	TIMIT	TIMIT
the best-published performance on the TIMIT test-set	TIMIT	TIMIT
firming our previous achievements on TIMIT	TIMIT	TIMIT
experiments was performed with the TIMIT corpus, considering the standard phoneme	TIMIT	TIMIT
for the test set of TIMIT with various neural architectures	TIMIT	TIMIT
discuss the baselines obtained with TIMIT, DIRHA, CHiME, and LibriSpeech datasets	TIMIT	TIMIT
the experimental validation conducted on TIMIT	TIMIT	TIMIT
Table 2: PER(%) obtained on TIMIT when progressively applying some techniques	TIMIT	TIMIT
the best-published performance on the TIMIT test-set	TIMIT	TIMIT
firming our previous achievements on TIMIT	TIMIT	TIMIT
experiments was performed with the TIMIT corpus, considering the standard phoneme	TIMIT	TIMIT
for the test set of TIMIT with various neural architectures	TIMIT	TIMIT
discuss the baselines obtained with TIMIT, DIRHA, CHiME, and LibriSpeech datasets	TIMIT	TIMIT
the experimental validation conducted on TIMIT	TIMIT	TIMIT
Table 2: PER(%) obtained on TIMIT when progressively applying some techniques	TIMIT	TIMIT
the best-published performance on the TIMIT test-set	TIMIT	TIMIT
firming our previous achievements on TIMIT	TIMIT	TIMIT
for a GRU trained on TIMIT in a chunk of the	TIMIT	TIMIT
for a GRU trained on TIMIT	TIMIT	TIMIT
Features Dataset. TIMIT DIRHA CHiME TED	TIMIT	TIMIT
set of experiments with the TIMIT corpus was performed to test	TIMIT	TIMIT
will then be reported for TIMIT, DIRHA-English, CHiME as well as	TIMIT	TIMIT
MFCC features and trained with TIMIT	TIMIT	TIMIT
with and without batch normalization (TIMIT dataset, MFCC features	TIMIT	TIMIT
results of CTC on the TIMIT data set. In these experiments	TIMIT	TIMIT
for the test set of TIMIT with various CTC RNN architectures	TIMIT	TIMIT
for the test set of TIMIT with various RNN architectures	TIMIT	TIMIT
E. Other results on TIMIT	TIMIT	TIMIT
ASR performance obtained with the TIMIT dataset. The first row reports	TIMIT	TIMIT
best published performance on the TIMIT test-set	TIMIT	TIMIT
TABLE VI: PER(%) of the TIMIT dataset (MFCC features) split into	TIMIT	TIMIT
layers and neurons for each TIMIT RNN model. The outcome of	TIMIT	TIMIT
first set of experiments on TIMIT, in this sub-section we assess	TIMIT	TIMIT
comparable to that observed for TIMIT, confirming that Li-GRU still outperform	TIMIT	TIMIT
XXXXXXXXArch. Dataset. TIMIT DIRHA CHiME TED	TIMIT	TIMIT
V-E Other results on TIMIT	TIMIT	TIMIT
We performed experiments on the TIMIT dataset. We achieved 17.3% phone	TIMIT	TIMIT
experiments were performed on the TIMIT dataset, and we achieved 17.3	TIMIT	TIMIT
We used the TIMIT dataset to evaluate the segmental	TIMIT	TIMIT
the standard protocol of the TIMIT dataset, and our experiments were	TIMIT	TIMIT
to the fact that the TIMIT dataset is small and it	TIMIT	TIMIT
using segmental CRFs on the TIMIT dataset is reported in [28	TIMIT	TIMIT
experiments were performed on the TIMIT dataset, and we achieved strong	TIMIT	TIMIT
error rate (PER) on the TIMIT phoneme recognition task, it can	TIMIT	TIMIT
task using the widely- used TIMIT dataset. At each time step	TIMIT	TIMIT
the conventional approaches on the TIMIT dataset. Moreover, we propose a	TIMIT	TIMIT
experiments were performed on the TIMIT corpus [19]. We used the	TIMIT	TIMIT
split from the Kaldi [20] TIMIT s5 recipe. We trained on	TIMIT	TIMIT
ground truth phone location from TIMIT	TIMIT	TIMIT
As TIMIT is a relatively small dataset	TIMIT	TIMIT
and N. L. Dahlgren. DARPA TIMIT acoustic phonetic continuous speech corpus	TIMIT	TIMIT
ground truth phone location from TIMIT	TIMIT	TIMIT
ground truth phone location from TIMIT	TIMIT	TIMIT
ARSG. Results of force-aligning concatenated TIMIT utterances. Each dot represents a	TIMIT	TIMIT
error of 17.7% on the TIMIT phoneme recognition benchmark, which to	TIMIT	TIMIT
experiments were performed on the TIMIT corpus [25]. The standard 462	TIMIT	TIMIT
Table 1. TIMIT Phoneme Recognition Results. ‘Epochs’ is	TIMIT	TIMIT
the filterbank inputs (bottom). The TIMIT ground truth segmentation is shown	TIMIT	TIMIT
phoneme recog- nition on the TIMIT database. An obvious next step	TIMIT	TIMIT
25] DARPA-ISTO, The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT	TIMIT	TIMIT
and D. S. Pallett, “Darpa timit acoustic-phonetic continous speech corpus cd-rom	timit	TIMIT
phoneme recognition experiments with the TIMIT corpus. More precisely, QCNNs obtain	TIMIT	TIMIT
equivalent real-valued model on the TIMIT [10] phonemes recognition task (Section	TIMIT	TIMIT
The conducted experiments on the TIMIT dataset yielded a phoneme error	TIMIT	TIMIT
4.1. TIMIT dataset and acoustic features of	TIMIT	TIMIT
The TIMIT [10] dataset is composed of	TIMIT	TIMIT
phoneme recognition task of the TIMIT dataset are reported in Table	TIMIT	TIMIT
CNN based models on the TIMIT phoneme recognition task. The results	TIMIT	TIMIT
for recognizing phonemes in the TIMIT corpus using RBM for feature	TIMIT	TIMIT
our approach on two datasets: TIMIT (Garofolo et al., 1993) and	TIMIT	TIMIT
Speech recognition on the TIMIT dataset involves tran- scribing the	TIMIT	TIMIT
to the 60 phonemes from TIMIT plus special start-of-sequence and end-of-sequence	TIMIT	TIMIT
Phone error rate on the TIMIT dataset for different online methods	TIMIT	TIMIT
small size of the TIMIT dataset and the resulting variabil	TIMIT	TIMIT
of the dataset, performance on TIMIT is often highly dependent on	TIMIT	TIMIT
and Pallett, David S. DARPA TIMIT acoustic-phonetic continous speech corpus. 1993	TIMIT	TIMIT
D.1.1. TIMIT	TIMIT	TIMIT
small small training set, like TIMIT [2]. Also, we introduce a	TIMIT	TIMIT
baseline. We decided to use TIMIT in our exper- iments as	TIMIT	TIMIT
experiments were performed on the TIMIT corpus. For training we used	TIMIT	TIMIT
different fea- ture sets on TIMIT corpus	TIMIT	TIMIT
alignments for phones provided with TIMIT, thus phone error rates (PER	TIMIT	TIMIT
TIMIT is a small dataset with	TIMIT	TIMIT
Table 2: Phones recognition on TIMIT corpus. LAS – a model	TIMIT	TIMIT
sequence-level articulatory features detection on TIMIT	TIMIT	TIMIT
tors to waveform segments from TIMIT markup using attention peaks (the	TIMIT	TIMIT
It is worth noting that TIMIT has explicit phone sequences for	TIMIT	TIMIT
TIMIT Acoustic-phonetic Continuous Speech Corpus,” in	TIMIT	TIMIT
RWTH ASR Systems for LibriSpeech	LibriSpeech	LibriSpeech test-other
LibriSpeech task. Detailed descriptions of the	LibriSpeech	LibriSpeech test-other
when training on the full LibriSpeech training set, are the best	LibriSpeech	LibriSpeech test-other
comparison shows that on the LibriSpeech 960h	LibriSpeech	LibriSpeech test-other
a reduced 100h-subset of the LibriSpeech training	LibriSpeech	LibriSpeech test-other
tention, LibriSpeech	LibriSpeech	LibriSpeech test-other
the LibriSpeech task	LibriSpeech	LibriSpeech test-other
The LibriSpeech task comprises English read speech	LibriSpeech	LibriSpeech test-other
End-to-end results on LibriSpeech were presented in [5–9	LibriSpeech	LibriSpeech test-other
obtained on the LibriSpeech task reflect state-of-the-art per	LibriSpeech	LibriSpeech test-other
model officially distributed with the LibriSpeech dataset [2	LibriSpeech	LibriSpeech test-other
perform the best LibriSpeech attention system presented in [6	LibriSpeech	LibriSpeech test-other
and hybrid DNN/HMM results on LibriSpeech with 12k CART labels and	LibriSpeech	LibriSpeech test-other
the LibriSpeech corpus. For comparison, also a	LibriSpeech	LibriSpeech test-other
with the LibriSpeech corpus: dev-clean, dev-other, test-clean	LibriSpeech	LibriSpeech test-other
2: Hybrid DNN/HMM results on LibriSpeech with differ	LibriSpeech	LibriSpeech test-other
decoder-attention model results on LibriSpeech with different	LibriSpeech	LibriSpeech test-other
results from other papers on LibriSpeech 960 h. CDp are	LibriSpeech	LibriSpeech test-other
hybrid and attention-based models on LibriSpeech, to the best	LibriSpeech	LibriSpeech test-other
two ASR systems for the LibriSpeech	LibriSpeech	LibriSpeech test-other
state-of-the-art performance on the LibriSpeech 960h task in	LibriSpeech	LibriSpeech test-other
comparison shows that on the LibriSpeech 960h	LibriSpeech	LibriSpeech test-other
a reduced 100h-subset of the LibriSpeech training	LibriSpeech	LibriSpeech test-other
on the full LibriSpeech training set, are the best	LibriSpeech	LibriSpeech test-other
them to the test-clean and test-other sets. We	test-other	LibriSpeech test-other
and test-other	test-other	LibriSpeech test-other
on test-other (Table 4). Evaluating our sequence	test-other	LibriSpeech test-other
on test-clean and 5.5% on test-other	test-other	LibriSpeech test-other
on test-clean and 5.0% on test-other	test-other	LibriSpeech test-other
by 3.9% relative WER on test-other	test-other	LibriSpeech test-other
by 27.6% relative WER on test-other	test-other	LibriSpeech test-other
on test-other	test-other	LibriSpeech test-other
on test-other	test-other	LibriSpeech test-other
by 13.8% relative WER on test-other	test-other	LibriSpeech test-other
the other test sets in terms of	other	LibriSpeech test-other
clean other clean other	other	LibriSpeech test-other
other sets, then	other	LibriSpeech test-other
other sets. We	other	LibriSpeech test-other
respectively on the clean and other sets	other	LibriSpeech test-other
other, test-clean	other	LibriSpeech test-other
other	other	LibriSpeech test-other
The difference between clean and other is the	other	LibriSpeech test-other
quality is higher than the other	other	LibriSpeech test-other
other	other	LibriSpeech test-other
improves, other degrades. Introducing an hybrid DNN/HMM	other	LibriSpeech test-other
clean other clean other	other	LibriSpeech test-other
clean other clean other	other	LibriSpeech test-other
models and important results from other papers on LibriSpeech 960 h	other	LibriSpeech test-other
clean other clean other	other	LibriSpeech test-other
other (Table 4). Evaluating our sequence	other	LibriSpeech test-other
other	other	LibriSpeech test-other
other	other	LibriSpeech test-other
other	other	LibriSpeech test-other
other	other	LibriSpeech test-other
other	other	LibriSpeech test-other
other	other	LibriSpeech test-other
other	other	LibriSpeech test-other
GMM/HMM system, the other system was an attention-based	other	LibriSpeech test-other
to other end-to-end approaches	other	LibriSpeech test-other
on the other test sets. Our hybrid system	other	LibriSpeech test-other
Table 2: LibriSpeech test WER (%) evaluated for varying	LibriSpeech test	LibriSpeech test-other
achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks	LibriSpeech	LibriSpeech test-other
outperforming all prior work. On LibriSpeech, we achieve 6.8% WER on	LibriSpeech	LibriSpeech test-other
of Language Models (LMs). On LibriSpeech [20], we achieve 2.8% Word	LibriSpeech	LibriSpeech test-other
an LM trained on the LibriSpeech LM corpus, we are able	LibriSpeech	LibriSpeech test-other
a series of hand-crafted policies, LibriSpeech basic (LB), LibriSpeech double (LD	LibriSpeech	LibriSpeech test-other
of vocabulary size 16k for LibriSpeech and 1k for Switchboard. The	LibriSpeech	LibriSpeech test-other
WPM for LibriSpeech 960h is con- structed using	LibriSpeech	LibriSpeech test-other
si = 140k for LibriSpeech 960h, and is subsequently turned	LibriSpeech	LibriSpeech test-other
For LibriSpeech, we use a two-layer RNN	LibriSpeech	LibriSpeech test-other
which is trained on the LibriSpeech LM corpus. We use identical	LibriSpeech	LibriSpeech test-other
we describe our experiments on LibriSpeech and Switchboard with SpecAugment. We	LibriSpeech	LibriSpeech test-other
4.1. LibriSpeech 960h	LibriSpeech	LibriSpeech test-other
For LibriSpeech, we use the same setup	LibriSpeech	LibriSpeech test-other
LAS-6- 1280 are trained on LibriSpeech 960h with a combination of	LibriSpeech	LibriSpeech test-other
Table 2: LibriSpeech test WER (%) evaluated for	LibriSpeech	LibriSpeech test-other
Table 3: LibriSpeech 960h WERs	LibriSpeech	LibriSpeech test-other
the RT- 03 corpus. Unlike LibriSpeech, the fusion parameters do not	LibriSpeech	LibriSpeech test-other
rate is being decayed for LibriSpeech when label smoothing is applied	LibriSpeech	LibriSpeech test-other
the learning rate schedule for LibriSpeech	LibriSpeech	LibriSpeech test-other
Figure 3: LAS-6-1280 on LibriSpeech with schedule D	LibriSpeech	LibriSpeech test-other
LibriSpeech 960h	LibriSpeech	LibriSpeech test-other
we achieve 6.8% WER on test-other without the use of a	test-other	LibriSpeech test-other
and 6.8% WER on the test-other set, without the use of	test-other	LibriSpeech test-other
state of the art on test-other by 22% relatively. On Switchboard	test-other	LibriSpeech test-other
F mF T p mT test-other test	test-other	LibriSpeech test-other
other without the use of a	other	LibriSpeech test-other
in the time direction. The other two augmentations, inspired by “Cutout	other	LibriSpeech test-other
other set, without the use of	other	LibriSpeech test-other
and 5.8% WER on test- other), improving the current state of	other	LibriSpeech test-other
other by 22% relatively. On Switchboard	other	LibriSpeech test-other
while the confidence of the other labels are increased accordingly. As	other	LibriSpeech test-other
learning rate sched- ules, all other hyperparameters were fixed, and no	other	LibriSpeech test-other
other set in Table 2. We	other	LibriSpeech test-other
other performance. State of the art	other	LibriSpeech test-other
clean other clean other	other	LibriSpeech test-other
clean other clean other	other	LibriSpeech test-other
5 summarizes our results with other work. We also apply shallow	other	LibriSpeech test-other
other test	other	LibriSpeech test-other
corpora show that Kaldi-RNNLM rivals other recurrent neural net- work language	other	LibriSpeech test-other
work has moved on to other topologies, such as LSTMs (e.g	other	LibriSpeech test-other
Kaldi-RNNLM achieves better perplexities than other toolk- its, and we hypothesize	other	LibriSpeech test-other
into how to in- corporate other types of subword information for	other	LibriSpeech test-other
with a greedy decoder on LibriSpeech test	LibriSpeech test	LibriSpeech test-other
results among end-to-end models1 on LibriSpeech test	LibriSpeech test	LibriSpeech test-other
we achieve 3.86% WER on LibriSpeech test	LibriSpeech test	LibriSpeech test-other
improve the SOTA WER on LibriSpeech test	LibriSpeech test	LibriSpeech test-other
we report state-of-the-art results on LibriSpeech among end-to-end speech recognition models	LibriSpeech	LibriSpeech test-other
with a greedy decoder on LibriSpeech test-clean. We also report competitive	LibriSpeech	LibriSpeech test-other
end-to- end models on the LibriSpeech and 2000hr Fisher+Switchboard tasks. Like	LibriSpeech	LibriSpeech test-other
new state-of-the-art (SOTA) results on LibriSpeech [13] test-clean of 2.95% WER	LibriSpeech	LibriSpeech test-other
results among end-to-end models1 on LibriSpeech test-other. We show competitive results	LibriSpeech	LibriSpeech test-other
we achieve 3.86% WER on LibriSpeech test-clean	LibriSpeech	LibriSpeech test-other
improve the SOTA WER on LibriSpeech test-clean	LibriSpeech	LibriSpeech test-other
for WSJ and 64 for LibriSpeech and F+S	LibriSpeech	LibriSpeech test-other
3: Sequence Masking: Greedy WER, LibriSpeech for Jasper 10x4 after 50	LibriSpeech	LibriSpeech test-other
better on specific subsets of LibriSpeech	LibriSpeech	LibriSpeech test-other
4: Residual Connections: Greedy WER, LibriSpeech for Jasper 10x3 after 400	LibriSpeech	LibriSpeech test-other
3: LM perplexity vs WER. LibriSpeech dev-other. Vary- ing perplexity is	LibriSpeech	LibriSpeech test-other
Table 5: LibriSpeech, WER	LibriSpeech	LibriSpeech test-other
creased the WER on dev-clean LibriSpeech from 4.00% to 3.64%, a	LibriSpeech	LibriSpeech test-other
on two read speech datasets: LibriSpeech and Wall Street Journal (WSJ	LibriSpeech	LibriSpeech test-other
leads to SOTA results on LibriSpeech and competitive results on other	LibriSpeech	LibriSpeech test-other
results among end-to-end models1 on LibriSpeech test-other	LibriSpeech test-other	LibriSpeech test-other
among end-to-end models1 on LibriSpeech test-other	test-other	LibriSpeech test-other
E2E LM dev-clean dev-other test-clean test-other	test-other	LibriSpeech test-other
speech recogni- tion models on test-other	test-other	LibriSpeech test-other
using time-delay neural networks (TDNN), other forms of convolu- tional neural	other	LibriSpeech test-other
ReLU and batch normalization outperform other activation and normalization schemes that	other	LibriSpeech test-other
other	other	LibriSpeech test-other
ReLU and batch norm outperform other combi- nations for regularization and	other	LibriSpeech test-other
batch norm with ReLU outperformed other choices. Thus, leading us to	other	LibriSpeech test-other
other	other	LibriSpeech test-other
other test-clean test-other	other	LibriSpeech test-other
other	other	LibriSpeech test-other
50 epochs. We compare to other models trained using the same	other	LibriSpeech test-other
LibriSpeech and competitive results on other bench- marks. Our Jasper architecture	other	LibriSpeech test-other
tion with bidirectional lstm and other neural network architec- tures,” Neural	other	LibriSpeech test-other
and dev-other when testing on test-other	test-other	LibriSpeech test-other
best end-to-end sys- tem on test-other with an improvement of 2.3	test-other	LibriSpeech test-other
parallel the prior results on other application domains that convolutional archi	other	LibriSpeech test-other
The other hyper-parameters α, β, γ	other	LibriSpeech test-other
concatenation of train-clean and train- other	other	LibriSpeech test-other
other when testing on test-other	other	LibriSpeech test-other
other	other	LibriSpeech test-other
other with an improvement of 2.3	other	LibriSpeech test-other
other	other	LibriSpeech test-other
other	other	LibriSpeech test-other
other also decreases following the same	other	LibriSpeech test-other
quency does not inform on other important characteristics such as its	other	LibriSpeech test-other
WSJ eval’93 6.94 4.98 8.08 LibriSpeech test	LibriSpeech test	LibriSpeech test-other
-clean 7.89 5.33 5.83 LibriSpeech test	LibriSpeech test	LibriSpeech test-other
conversational 300 Fisher conversational 2000 LibriSpeech read 960 Baidu read 5000	LibriSpeech	LibriSpeech test-other
the Linguistic Data Consortium. The LibriSpeech dataset [46] is available free	LibriSpeech	LibriSpeech test-other
advantage of the recently developed LibriSpeech corpus constructed using audio books	LibriSpeech	LibriSpeech test-other
WSJ eval’93 6.94 4.98 8.08 LibriSpeech test-clean 7.89 5.33 5.83 LibriSpeech	LibriSpeech	LibriSpeech test-other
LibriSpeech test-clean 7.89 5.33 5.83 LibriSpeech test-other 21.74 13.25 12.69	LibriSpeech test-other	LibriSpeech test-other
test-clean 7.89 5.33 5.83 LibriSpeech test-other 21.74 13.25 12.69	test-other	LibriSpeech test-other
also yielded great advances in other appli- cation areas such as	other	LibriSpeech test-other
The other commonly used technique for mapping	other	LibriSpeech test-other
Units (GRU) [11], though many other variations exist. A recent comprehensive	other	LibriSpeech test-other
variants are competitive with each other [32]. We decided to examine	other	LibriSpeech test-other
input features prior to any other processing, can slightly improve ASR	other	LibriSpeech test-other
for porting speech systems to other languages [59]. Direct output to	other	LibriSpeech test-other
is available free on-line. The other datasets are internal Baidu corpora	other	LibriSpeech test-other
while keeping the depth and other architectural parameters constant. We evaluate	other	LibriSpeech test-other
other 21.74 13.25 12.69	other	LibriSpeech test-other
transcription. We can solve the other problems by modifying our network	other	LibriSpeech test-other
a corresponding−∞ value from the other matrix, which results in−∞, effectively	other	LibriSpeech test-other
large vocabulary setup, on the LibriSpeech evaluation dataset [38], chosen because	LibriSpeech	LibriSpeech test-other
are trained only on the LibriSpeech training set (or on a	LibriSpeech	LibriSpeech test-other
architecture trained and evaluated on LibriSpeech	LibriSpeech	LibriSpeech test-other
on different subsets of the LibriSpeech dataset, with and without data	LibriSpeech	LibriSpeech test-other
the train-clean split of the LibriSpeech dataset and 500h in the	LibriSpeech	LibriSpeech test-other
not have much impact on LibriSpeech	LibriSpeech	LibriSpeech test-other
architecture trained and evaluated on LibriSpeech	LibriSpeech	LibriSpeech test-other
Num. hours dev-clean dev-other test-clean test-other 460 6.3 21.8 6.6 23.1	test-other	LibriSpeech test-other
marked as other (dev- other, test-other	test-other	LibriSpeech test-other
Model dev-clean dev-other test-clean test-other nnet-256 7.3 19.2 7.6 19.6	test-other	LibriSpeech test-other
open source [50], while the other components will be opensourced in	other	LibriSpeech test-other
other split). The data augmentation was	other	LibriSpeech test-other
other test-clean test-other 460 6.3 21.8	other	LibriSpeech test-other
on the datasets marked as other (dev- other, test-other). In general	other	LibriSpeech test-other
other test-clean test-other nnet-256 7.3 19.2	other	LibriSpeech test-other
These tradeoffs and comparison with other trained models led us to	other	LibriSpeech test-other
that it can share with other intents. For instance, the room	other	LibriSpeech test-other
may be replaced by any other	other	LibriSpeech test-other
hand, and G on the other	other	LibriSpeech test-other
On the other hand, crowdsourcing – widely used	other	LibriSpeech test-other
contributors can be reached easily, other languages such as French, German	other	LibriSpeech test-other
recognition: word error minimization and other applications of confusion networks. Computer	other	LibriSpeech test-other
eval92	eval92	WSJ eval92
corpora: Switch- board [26], and WSJ (Wall Street Journal) [27]. Switchboard	WSJ	WSJ eval92
character LM (in lexicon-free settings). WSJ is a database with 80	WSJ	WSJ eval92
3-gram LM trained on the WSJ train- ing set transcriptions using	WSJ	WSJ eval92
1For example, on WSJ, which has 42 phonemes (including	WSJ	WSJ eval92
10.7 14.5 14.2 13.3 13.2 WSJ 3.1 3.1 3.3 5.4 5.3	WSJ	WSJ eval92
Switchboard WSJ Phone Char Phone Char	WSJ	WSJ eval92
all cases except in phoneme-based WSJ which is fairly easier than	WSJ	WSJ eval92
and almost the same on WSJ	WSJ	WSJ eval92
. For WSJ, there is no improvement in	WSJ	WSJ eval92
5 shows the results on WSJ	WSJ	WSJ eval92
EE-LF-MMI) and related methods on WSJ	WSJ	WSJ eval92
smaller 300hr Switchboard and 80hr WSJ task, in both lexicon-free and	WSJ	WSJ eval92
and 1.4 percent improvement on WSJ in similar conditions (i.e. no-SP	WSJ	WSJ eval92
on Switchboard and 0.4 on WSJ (on CPU	WSJ	WSJ eval92
the 300hr Switchboard or 80hr WSJ tasks where the relative improvements	WSJ	WSJ eval92
the Wall Street Journal dataset (WSJ) and on the 1000h Librispeech	WSJ	WSJ eval92
the best end-to-end systems; on WSJ, our results are competitive with	WSJ	WSJ eval92
show additional improvements on both WSJ and Librispeech by varying the	WSJ	WSJ eval92
of the Wall Street Journal (WSJ) dataset [24], which contains 80	WSJ	WSJ eval92
contain 37 million tokens for WSJ, 800 million tokens for Librispeech	WSJ	WSJ eval92
the open vocabulary task of WSJ	WSJ	WSJ eval92
Training/test splits On WSJ, models are trained on si284	WSJ	WSJ eval92
front-end for our approach). On WSJ, we use the lighter version	WSJ	WSJ eval92
and linear layer on both WSJ and Librispeech. The kernel size	WSJ	WSJ eval92
the words (162K) in the WSJ training corpus, while only the	WSJ	WSJ eval92
Word Error Rates (WER) on WSJ for the cur- rent state-of-the-art	WSJ	WSJ eval92
end-to-end systems trained only on WSJ, and hence the most	WSJ	WSJ eval92
consistent with our results on WSJ	WSJ	WSJ eval92
of our best models on WSJ and Librispeech. The fig- ure	WSJ	WSJ eval92
SCALE MEL SCALE LEARNT SCALE (WSJ	WSJ	WSJ eval92
SCALE (LIBRI/40 FILTERS) LEARNT SCALE (WSJ	WSJ	WSJ eval92
knowledge, this is the best WSJ eval92 per- formance without sequence training	WSJ eval92	WSJ eval92
acoustic model architecture. On the WSJ eval92 task, we report a 3.47	WSJ eval92	WSJ eval92
knowledge, this is the best WSJ eval92 per- formance without sequence training	WSJ eval	WSJ eval92
acoustic model architecture. On the WSJ eval92 task, we report a 3.47	WSJ eval	WSJ eval92
the Wall Street Journal (WSJ) eval92 task or more than 8	eval92	WSJ eval92
as our development set and eval92 as our test set. We	eval92	WSJ eval92
verged dev93 and the corresponding eval92 WERs. We use the same	eval92	WSJ eval92
Model dev93 WER eval92 WER GMM Kaldi tri4b 9.39	eval92	WSJ eval92
Cell Size Layers dev93 WER eval92 WER 128 1 8.19 5.19	eval92	WSJ eval92
Model dev93 WER eval92 WER DNN-BLSTM 7.40 3.92 BLSTM-DNN	eval92	WSJ eval92
this is the best WSJ eval92 per- formance without sequence training	eval92	WSJ eval92
Epochs Time (hrs) dev93 WER eval92 WER SGD 17 51.5 6.58	eval92	WSJ eval92
SGD dev93 SGD eval92 3x ASGD dev93 3x ASGD	eval92	WSJ eval92
eval92	eval92	WSJ eval92
model architecture. On the WSJ eval92 task, we report a 3.47	eval92	WSJ eval92
on the Wall Street Journal (WSJ) eval92 task or more than	WSJ	WSJ eval92
for the Wall Street Journal (WSJ) corpus	WSJ	WSJ eval92
Results We experiment with the WSJ dataset. We use si284 with	WSJ	WSJ eval92
first is the Kaldi s5 WSJ recipe with sigmoid DNN model	WSJ	WSJ eval92
knowledge, this is the best WSJ eval92 per- formance without sequence	WSJ	WSJ eval92
acoustic model architecture. On the WSJ eval92 task, we report a	WSJ	WSJ eval92
WSJ eval’92 4.94 3.60 5.03 WSJ eval	WSJ eval	WSJ eval92
WSJ read 80 Switchboard conversational 300	WSJ	WSJ eval92
English. The Wall Street Journal (WSJ), Switchboard and Fisher [13] corpora	WSJ	WSJ eval92
from the Wall Street Journal (WSJ) corpus of read news articles	WSJ	WSJ eval92
WSJ eval’92 4.94 3.60 5.03 WSJ eval’93 6.94 4.98 8.08 LibriSpeech	WSJ	WSJ eval92
has 1320 utterances from the WSJ test set read in various	WSJ	WSJ eval92
10, contain respectively 15k/1.3k and 123k/37	10	YAGO3-10
10 123k 37 1M 5k 5k	10	YAGO3-10
experiments on a Quadro GP 100 GPU. The code is available	10	YAGO3-10
10, which is larger in scale	10	YAGO3-10
10	10	YAGO3-10
10 MRR H@10 MRR H@10 MRR	10	YAGO3-10
10 MRR H@10 Pa	10	YAGO3-10
consisted of two learning rates: 10	10	YAGO3-10
−1 and 10	10	YAGO3-10
−2, two batch-sizes: 25 and 100, and regularization co- efficients in	10	YAGO3-10
0, 10−3, 5.10−3, 10−2, 5.10−2, 10	10	YAGO3-10
10	10	YAGO3-10
10, we limited our models to	10	YAGO3-10
rank 1000 and used batch-sizes 500 and	10	YAGO3-10
10 (Bordes et al. (2013)). We	10	YAGO3-10
epoch for ComplEx with batch-size 100 on FB15K took about 110s	10	YAGO3-10
of 25. We trained for 100 epochs to ensure convergence, reported	10	YAGO3-10
10 benchmarks, the latter because of	10	YAGO3-10
10 may be slightly underestimated, since	10	YAGO3-10
10	10	YAGO3-10
10	10	YAGO3-10
0 20 40 60 80 100 Epochs	10	YAGO3-10
mini-batch size = 100 mini-batch size = 25	10	YAGO3-10
0 20 40 60 80 100 Epochs	10	YAGO3-10
mini-batch size = 100 mini-batch size = 25	10	YAGO3-10
difference is large even after 100 epochs and the effect is	10	YAGO3-10
10), pp. 471–478, 2010	10	YAGO3-10
10881, 2017	10	YAGO3-10
Graphs. Proceedings of the IEEE, 104	10	YAGO3-10
1010	10	YAGO3-10
1031	10	YAGO3-10
1068, 2016	10	YAGO3-10
10 0.97 Ma et al. (2017	10	YAGO3-10
10 0.51 Dettmers et al. (2017	10	YAGO3-10
10 0.93 Shen et al. (2016	10	YAGO3-10
10 0.49 Dettmers et al. (2017	10	YAGO3-10
10 MRR 0.52 Dettmers et al	10	YAGO3-10
10 0.66 Dettmers et al. (2017	10	YAGO3-10
we experiment on, FB15K and YAGO3	YAGO3	YAGO3-10
15k 237 272k 18k 20k YAGO3	YAGO3	YAGO3-10
2017) also introduced the dataset YAGO3	YAGO3	YAGO3-10
Model WN18 WN18RR FB15K FB15K-237 YAGO3	YAGO3	YAGO3-10
10−2, 5.10−2, 10−1, 5.10−1]. On YAGO3	YAGO3	YAGO3-10
performances on the WN18RR and YAGO3	YAGO3	YAGO3-10
DistMult on FB15K-237, WN18RR and YAGO3	YAGO3	YAGO3-10
improvements observed on FB15K and YAGO3	YAGO3	YAGO3-10
harder datasets WN18RR, FB15K-237 and YAGO3	YAGO3	YAGO3-10
YAGO3	YAGO3	YAGO3-10
we experiment on, FB15K and YAGO3-10, contain respectively 15k/1.3k and 123k/37	YAGO3-10	YAGO3-10
15k 237 272k 18k 20k YAGO3-10 123k 37 1M 5k 5k	YAGO3-10	YAGO3-10
2017) also introduced the dataset YAGO3-10, which is larger in scale	YAGO3-10	YAGO3-10
Model WN18 WN18RR FB15K FB15K-237 YAGO3-10	YAGO3-10	YAGO3-10
10−2, 5.10−2, 10−1, 5.10−1]. On YAGO3-10, we limited our models to	YAGO3-10	YAGO3-10
performances on the WN18RR and YAGO3-10 benchmarks, the latter because of	YAGO3-10	YAGO3-10
DistMult on FB15K-237, WN18RR and YAGO3-10 may be slightly underestimated, since	YAGO3-10	YAGO3-10
improvements observed on FB15K and YAGO3-10	YAGO3-10	YAGO3-10
harder datasets WN18RR, FB15K-237 and YAGO3-10	YAGO3-10	YAGO3-10
YAGO3-10 MRR 0.52 Dettmers et al	YAGO3-10	YAGO3-10
10 (Dettmers et al., 2018) is	10	YAGO3-10
each entity contains at least 10 relations	10	YAGO3-10
10 123k 37 1M 5k 5k	10	YAGO3-10
popular metrics filtered HITS@1, 3, 10 and mean reciprocal rank (MRR	10	YAGO3-10
MRR 1 3 10 1 3 10	10	YAGO3-10
regularization coefficient λ ∈ [10−5, 10	10	YAGO3-10
−4, 10	10	YAGO3-10
with batch sizes ∈ [512, 1024, 2048] and negative sample ratio	10	YAGO3-10
1, 6, 10	10	YAGO3-10
10, 2L = 200 for WN18	10	YAGO3-10
10	10	YAGO3-10
1000	10	YAGO3-10
1000	10	YAGO3-10
10 HITS@N	10	YAGO3-10
MRR 1 3 10 1 3 10 1 3	10	YAGO3-10
10	10	YAGO3-10
100	10	YAGO3-10
100	10	YAGO3-10
100	10	YAGO3-10
100	10	YAGO3-10
100	10	YAGO3-10
100	10	YAGO3-10
100	10	YAGO3-10
1067 https://www.aclweb.org/anthology/D14-1067 https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf https://openreview.net/forum?id=Syg-YfWCW https://openreview.net/forum?id=Syg-YfWCW	10	YAGO3-10
1038 https://www.aclweb.org/anthology/P17-2088 https://www.aclweb.org/anthology/P17-2088 https://www.aclweb.org/anthology/P17-2088 https://www.aclweb.org/anthology/P17-1162 https://www.aclweb.org/anthology/P17-1162	10	YAGO3-10
1082 https://aclweb.org/anthology/D15-1082 https://aclweb.org/anthology/D15-1082 https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571 https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571 http://proceedings.mlr.press/v70/liu17d/liu17d.pdf	10	YAGO3-10
1016 https://www.aclweb.org/anthology/P15-1016 https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12484 https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12484 http://www.icml-2011.org/papers/438_icmlpaper.pdf http://www.icml-2011.org/papers/438_icmlpaper.pdf	10	YAGO3-10
1060 https://www.aclweb.org/anthology/D17-1060 https://arxiv.org/abs/1412.6575 https://arxiv.org/abs/1412.6575 https://arxiv.org/abs/1412.6575 https://openreview.net/pdf?id=Skh4jRcKQ	10	YAGO3-10
many-to- many such as actor_in_film). YAGO3	YAGO3	YAGO3-10
2018) is a subset of YAGO3 (Suchanek et al., 2007) with	YAGO3	YAGO3-10
15k 237 273k 18k 20k YAGO3	YAGO3	YAGO3-10
600 for both FB15K-237 and YAGO3	YAGO3	YAGO3-10
ConvE in FB15K, WN18RR and YAGO3	YAGO3	YAGO3-10
WN18RR FB15K-237 YAGO3	YAGO3	YAGO3-10
many-to- many such as actor_in_film). YAGO3-10 (Dettmers et al., 2018) is	YAGO3-10	YAGO3-10
15k 237 273k 18k 20k YAGO3-10 123k 37 1M 5k 5k	YAGO3-10	YAGO3-10
600 for both FB15K-237 and YAGO3-10, 2L = 200 for WN18	YAGO3-10	YAGO3-10
ConvE in FB15K, WN18RR and YAGO3-10	YAGO3-10	YAGO3-10
WN18RR FB15K-237 YAGO3-10 HITS@N	YAGO3-10	YAGO3-10
go- ing from N = 100, 000 to N = 1	10	YAGO3-10
that is, scoring against 10	10	YAGO3-10
10 (Mahdisoltani, Biega, and Suchanek 2015	10	YAGO3-10
which have a minimum of 10 relations each. It has 123,182	10	YAGO3-10
10	10	YAGO3-10
100, 200}, batch size {64, 128	10	YAGO3-10
10 @3 @1	10	YAGO3-10
10 and FB15k: embed- ding dropout	10	YAGO3-10
10) and AUC-PR (Countries) statistics on	10	YAGO3-10
variance, as such we average 10 runs and produce 95% confidence	10	YAGO3-10
use an embedding size of 100, AdaGrad (Duchi, Hazan, and Singer	10	YAGO3-10
10 and Countries are shown in	10	YAGO3-10
on inverse relations for YAGO3- 10 and FB15k-237. The procedure used	10	YAGO3-10
10, for some metrics on FB15k	10	YAGO3-10
10	10	YAGO3-10
10 @3 @1 MR MRR @10	10	YAGO3-10
10 @3 @1 MR MRR @10	10	YAGO3-10
10 with more than 8M parame	10	YAGO3-10
10 and FB15k-237 compared to WN18RR	10	YAGO3-10
has an indegree of over 10,	10	YAGO3-10
000. Many of these 10,	10	YAGO3-10
10 vs DistMult 0.728 Hits@10; for	10	YAGO3-10
10 vs DistMult 0.938 Hits@10. This	10	YAGO3-10
10 and Countries	10	YAGO3-10
10 Countries Hits AUC-PR	10	YAGO3-10
10 @3 @1 S1 S2 S3	10	YAGO3-10
10 0.43±0.07 ConvE 1676 .44 .62	10	YAGO3-10
10	10	YAGO3-10
10 of ConvE wrt. DistMult	10	YAGO3-10
104 0.06 WN18 0.125 0.45 FB15k	10	YAGO3-10
10 0.988 0.21 Countries S3 1.415	10	YAGO3-10
10	10	YAGO3-10
10 and Countries, and about 4	10	YAGO3-10
10 between DistMult and ConvE is	10	YAGO3-10
In Proceedings of NIPS 2012, 1097	10	YAGO3-10
graphs. Proceedings of the IEEE 104	10	YAGO3-10
10 results. The new results are	10	YAGO3-10
10 @3 @1	10	YAGO3-10
Hits@k (%): 100	10	YAGO3-10
graphs such as Freebase and YAGO3	YAGO3	YAGO3-10
YAGO3	YAGO3	YAGO3-10
2015) is a subset of YAGO3 which consists of entities which	YAGO3	YAGO3-10
instead recommend FB15k-237, WN18RR, and YAGO3	YAGO3	YAGO3-10
parameters works well on WN18, YAGO3	YAGO3	YAGO3-10
mean reciprocal rank (WN18, FB15k, YAGO3	YAGO3	YAGO3-10
in Table 4; results on YAGO3	YAGO3	YAGO3-10
up on inverse relations for YAGO3	YAGO3	YAGO3-10
formance for all metrics on YAGO3	YAGO3	YAGO3-10
our model on datasets like YAGO3	YAGO3	YAGO3-10
5: Link prediction results for YAGO3	YAGO3	YAGO3-10
YAGO3	YAGO3	YAGO3-10
0.599 0.04 FB15-237 0.733 0.16 YAGO3	YAGO3	YAGO3-10
the most central nodes in YAGO3	YAGO3	YAGO3-10
and Suchanek, F. M. 2015. YAGO3	YAGO3	YAGO3-10
2018-07-04: – Added new YAGO3	YAGO3	YAGO3-10
YAGO3-10 (Mahdisoltani, Biega, and Suchanek 2015	YAGO3-10	YAGO3-10
instead recommend FB15k-237, WN18RR, and YAGO3-10	YAGO3-10	YAGO3-10
parameters works well on WN18, YAGO3-10 and FB15k: embed- ding dropout	YAGO3-10	YAGO3-10
mean reciprocal rank (WN18, FB15k, YAGO3-10) and AUC-PR (Countries) statistics on	YAGO3-10	YAGO3-10
in Table 4; results on YAGO3-10 and Countries are shown in	YAGO3-10	YAGO3-10
formance for all metrics on YAGO3-10, for some metrics on FB15k	YAGO3-10	YAGO3-10
our model on datasets like YAGO3-10 and FB15k-237 compared to WN18RR	YAGO3-10	YAGO3-10
5: Link prediction results for YAGO3-10 and Countries	YAGO3-10	YAGO3-10
YAGO3-10 Countries Hits AUC-PR	YAGO3-10	YAGO3-10
0.599 0.04 FB15-237 0.733 0.16 YAGO3-10 0.988 0.21 Countries S3 1.415	YAGO3-10	YAGO3-10
the most central nodes in YAGO3-10 and Countries, and about 4	YAGO3-10	YAGO3-10
2018-07-04: – Added new YAGO3-10 results. The new results are	YAGO3-10	YAGO3-10
the-art results on the WN18RR dataset, a chal	WN18RR	WN18RR
WN18RR dataset (Dettmers et al., 2018	WN18RR	WN18RR
4.1 WN18RR Dataset	WN18RR	WN18RR
leased the WN18RR set, removing seven relations	WN18RR	WN18RR
relations and constructed corresponding subsets: WN18RR with 11 relations and FB15K-237	WN18RR	WN18RR
41k 18 141k 5k 5k WN18RR 41k 11 87k 3k 3k	WN18RR	WN18RR
200 for WN18 and WN18RR	WN18RR	WN18RR
and even ConvE in FB15K, WN18RR and YAGO3-10. The result demonstrates	WN18RR	WN18RR
WN18RR FB15K-237 YAGO3-10 HITS@N	WN18RR	WN18RR
4: Link prediction results on WN18RR and FB15K-237 datasets. Results marked	WN18RR	WN18RR
WN18RR 41k 11 87k 3k 3k	WN18RR	WN18RR
mod- ified datasets: FB15K-237 and WN18RR	WN18RR	WN18RR
Model WN18 WN18RR FB15K FB15K-237 YAGO3-10	WN18RR	WN18RR
on the FB15K, FB15K-237, WN18, WN18RR datasets, with a rank set	WN18RR	WN18RR
2017) includes performances on the WN18RR and YAGO3-10 benchmarks, the latter	WN18RR	WN18RR
performances of DistMult on FB15K-237, WN18RR and YAGO3-10 may be slightly	WN18RR	WN18RR
appear on the harder datasets WN18RR, FB15K-237 and YAGO3-10. We checked	WN18RR	WN18RR
on WN18RR the significance of that gain	WN18RR	WN18RR
WN18RR MRR 0.46 Dettmers et al	WN18RR	WN18RR
and an asymmetric (hy- pernym) WN18RR relation. Wderivationally related form is	WN18RR	WN18RR
lexical re- lations between words. WN18RR (Dettmers et al., 2018) is	WN18RR	WN18RR
200. For WN18 and WN18RR, which both contain a sig	WN18RR	WN18RR
risk of overfitting (WN18 and WN18RR), whereas higher dropout values (0.3	WN18RR	WN18RR
WN18 and (0.01, 1.0) for WN18RR (see Table 5 in the	WN18RR	WN18RR
14,541 237 WN18 40,943 18 WN18RR 40,943 11	WN18RR	WN18RR
and dr = 30 on WN18RR (∼9.4 vs∼16.4 million), TuckER consistently	WN18RR	WN18RR
more parameters than TuckER on WN18RR	WN18RR	WN18RR
WN18RR FB15k-237	WN18RR	WN18RR
3: Link prediction results on WN18RR and FB15k-237. The RotatE (Sun	WN18RR	WN18RR
30 0.2 0.1 0.2 0.1 WN18RR 0.01 1.0 200 30 0.2	WN18RR	WN18RR
els using standard datasets (FB15k-237, WN18RR, FB15k, WN18, YAGO3-10), across which	WN18RR	WN18RR
WN18RR [3] a subset of WN18	WN18RR	WN18RR
40,943 18 FB15k-237 14,541 237 WN18RR 40,943 11 YAGO3-10 123,182 37	WN18RR	WN18RR
WN18 and mean rank on WN18RR, FB15k-237, WN18, and YAGO3-10. Given	WN18RR	WN18RR
report the obtained results on WN18RR in Table 7. We perform	WN18RR	WN18RR
4. Link prediction results on WN18RR and FB15k-237. The RotatE [19	WN18RR	WN18RR
WN18RR FB15k-237	WN18RR	WN18RR
7. Link prediction results on WN18RR	WN18RR	WN18RR
WN18RR MR MRR H@10 H@3 H@1	WN18RR	WN18RR
with and without hypernetwork on WN18RR and FB15k-237	WN18RR	WN18RR
WN18RR FB15k-237	WN18RR	WN18RR
we compare HypER results on WN18RR and FB15k-237 with the hypernetwork	WN18RR	WN18RR
of no benefit, especially on WN18RR	WN18RR	WN18RR
improvement in prediction scores for WN18RR	WN18RR	WN18RR
WN18RR FB15k-237	WN18RR	WN18RR
Knowledge Base Completion We use WN18RR and NELL995 knowledge graph datasets	WN18RR	WN18RR
for evaluation. WN18RR [6] is created from the	WN18RR	WN18RR
protocol as in [6] for WN18RR and in [38, 5] for	WN18RR	WN18RR
as the evaluation metrics for WN18RR, and use mean average precision	WN18RR	WN18RR
the metrics on NELL995 and WN18RR datasets. Additional experiments on the	WN18RR	WN18RR
2: The results on the WN18RR dataset, in the form of	WN18RR	WN18RR
We observed similar trends on WN18RR	WN18RR	WN18RR
and different search horizons on WN18RR dataset, with results shown in	WN18RR	WN18RR
to traditional methods on the WN18RR dataset. The first question is	WN18RR	WN18RR
about 20–60 unique candidates on WN18RR	WN18RR	WN18RR
hyperparameter and error analysis on WN18RR	WN18RR	WN18RR
both training and testing on WN18RR with different values of search	WN18RR	WN18RR
WN18RR 86,835 3,134 11 40,943 2.19	WN18RR	WN18RR
unique entities and 200 relations. WN18RR contains 93, 003 triples with	WN18RR	WN18RR
dataset and 5 in the WN18RR dataset. After the STOP action	WN18RR	WN18RR
128 MCTS paths in the WN18RR dataset. We use the ADAM	WN18RR	WN18RR
results for the NELL995 and WN18RR tasks to support our analysis	WN18RR	WN18RR
hyperparameter and error analysis on WN18RR	WN18RR	WN18RR
with WN18. We thus create WN18RR to reclaim WN18 as a	WN18RR	WN18RR
of the complete knowledge graph. WN18RR1 contains 93,003 triples with 40,943	WN18RR	WN18RR
WN18 and instead recommend FB15k-237, WN18RR, and YAGO3-10	WN18RR	WN18RR
of our inverse model on WN18RR, which was derived using the	WN18RR	WN18RR
4: Link prediction results for WN18RR and FB15k-237	WN18RR	WN18RR
WN18RR FB15k-237 Hits Hits	WN18RR	WN18RR
YAGO3-10 and FB15k-237 compared to WN18RR, is that these datasets contain	WN18RR	WN18RR
average relation- specific indegree (like WN18RR and WN18), a shallow model	WN18RR	WN18RR
less complex graphs (e.g. WN18 WN18RR	WN18RR	WN18RR
WN18RR 0.104 0.06 WN18 0.125 0.45	WN18RR	WN18RR
investigated datasets exists, we derive WN18RR	WN18RR	WN18RR
tested thus far (Kinship, WN18, WN18RR, FB15k-237). • 2018-03-28	WN18RR	WN18RR
els on two benchmark datasets WN18RR and FB15k-237	WN18RR	WN18RR
ConvKB on two benchmark datasets: WN18RR (Dettmers et al., 2018) and	WN18RR	WN18RR
and the highest Hits@10 on WN18RR, and pro- duces the highest	WN18RR	WN18RR
R | #Triples in train/valid/test WN18RR 40,943 11 86,835 3,034 3,134	WN18RR	WN18RR
ConvKB on two benchmark datasets: WN18RR (Dettmers et al., 2018) and	WN18RR	WN18RR
237 (Toutanova and Chen, 2015). WN18RR and FB15k-237 are correspondingly subsets	WN18RR	WN18RR
Dettmers et al. (2018). Therefore, WN18RR and FB15k- 237 are created	WN18RR	WN18RR
2 presents the statistics of WN18RR and FB15k-237	WN18RR	WN18RR
Method WN18RR FB15k-237 MR MRR H@10 MR	WN18RR	WN18RR
Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR	WN18RR	WN18RR
to 2 decimal places on WN18RR	WN18RR	WN18RR
and k = 50 for WN18RR, and using l1-norm, learning rate	WN18RR	WN18RR
learning rate at 1e−4 on WN18RR	WN18RR	WN18RR
and highest Hits@10 scores on WN18RR and also the highest	WN18RR	WN18RR
and Hits@10 than ConvE on WN18RR	WN18RR	WN18RR
both datasets (except MRR on WN18RR and MR on FB15k-237), thus	WN18RR	WN18RR
models on two benchmark datasets WN18RR and FB15k-237. Our code is	WN18RR	WN18RR
has been noted that the WN18 and FB15k datasets suffer from	WN18	WN18
achieve state-of-the-art results on both WN18 and FB15k. To ensure that	WN18	WN18
WN18 (Bordes et al. 2013a) is	WN18	WN18
and, for such a reason, WN18 tends to follow a strictly	WN18	WN18
Toutanova and Chen (2015) that WN18 and FB15k suffer from test	WN18	WN18
ing state-of-the-art results on both WN18 and FB15k. In order to	WN18	WN18
we also find flaws with WN18	WN18	WN18
thus create WN18RR to reclaim WN18 as a dataset, which cannot	WN18	WN18
recommend against using FB15k and WN18 and instead recommend FB15k-237, WN18RR	WN18	WN18
of parameters works well on WN18, YAGO3-10 and FB15k: embed- ding	WN18	WN18
to the mean reciprocal rank (WN18, FB15k, YAGO3-10) and AUC-PR (Countries	WN18	WN18
that the training datasets of WN18 and FB15k have 94% and	WN18	WN18
standard bench- marks FB15k and WN18 are shown in Table 3	WN18	WN18
metrics for both FB15k and WN18	WN18	WN18
and it does well on WN18	WN18	WN18
3: Link prediction results for WN18 and FB15k	WN18	WN18
WN18 FB15k Hits Hits	WN18	WN18
specific indegree (like WN18RR and WN18), a shallow model like DistMult	WN18	WN18
WN18) and high (high-FB15k) relation-specific in	WN18	WN18
WN18) and low (low- FB15k) relation-specific	WN18	WN18
WN18 we have ConvE 0.952 Hits@10	WN18	WN18
model less complex graphs (e.g. WN18 WN18RR	WN18	WN18
WN18RR 0.104 0.06 WN18 0.125 0.45 FB15k 0.599 0.04	WN18	WN18
the most central nodes in WN18 have a PageRank value more	WN18	WN18
leakage through inverse relations of WN18 and FB15k was first reported	WN18	WN18
can achieve state-of-the-art results on WN18 and FB15k. To ensure robust	WN18	WN18
results are slightly better for WN18 and slightly worse for FB15k	WN18	WN18
we tested thus far (Kinship, WN18, WN18RR, FB15k-237). • 2018-03-28	WN18	WN18
has been noted that the WN18 and FB15k datasets suffer from	WN	WN18
achieve state-of-the-art results on both WN18 and FB15k. To ensure that	WN	WN18
WN18 (Bordes et al. 2013a) is	WN	WN18
and, for such a reason, WN18 tends to follow a strictly	WN	WN18
Toutanova and Chen (2015) that WN18 and FB15k suffer from test	WN	WN18
ing state-of-the-art results on both WN18 and FB15k. In order to	WN	WN18
we also find flaws with WN18	WN	WN18
thus create WN18RR to reclaim WN18 as a dataset, which cannot	WN	WN18
recommend against using FB15k and WN18 and instead recommend FB15k-237, WN18RR	WN	WN18
of parameters works well on WN18, YAGO3-10 and FB15k: embed- ding	WN	WN18
to the mean reciprocal rank (WN18, FB15k, YAGO3-10) and AUC-PR (Countries	WN	WN18
that the training datasets of WN18 and FB15k have 94% and	WN	WN18
standard bench- marks FB15k and WN18 are shown in Table 3	WN	WN18
metrics for both FB15k and WN18	WN	WN18
and it does well on WN18	WN	WN18
3: Link prediction results for WN18 and FB15k	WN	WN18
WN18 FB15k Hits Hits	WN	WN18
specific indegree (like WN18RR and WN18	WN	WN18
WN18	WN	WN18
WN18	WN	WN18
WN18 we have ConvE 0.952 Hits@10	WN	WN18
model less complex graphs (e.g. WN18 WN18RR	WN	WN18
WN18RR 0.104 0.06 WN18 0.125 0.45 FB15k 0.599 0.04	WN	WN18
the most central nodes in WN18 have a PageRank value more	WN	WN18
leakage through inverse relations of WN18 and FB15k was first reported	WN	WN18
can achieve state-of-the-art results on WN18 and FB15k. To ensure robust	WN	WN18
results are slightly better for WN18 and slightly worse for FB15k	WN	WN18
we tested thus far (Kinship, WN18, WN18RR, FB15k-237). • 2018-03-28	WN	WN18
simple models to do well. WN18 (Bordes et al., 2013) is	WN18	WN18
2018) is a subset of WN18, created by removing the inverse	WN18	WN18
dr = 200. For WN18 and WN18RR, which both contain	WN18	WN18
thus less risk of overfitting (WN18 and WN18RR), whereas higher dropout	WN18	WN18
FB15k- 237, (0.005, 0.995) for WN18 and (0.01, 1.0) for WN18RR	WN18	WN18
14,951 1,345 FB15k-237 14,541 237 WN18 40,943 18 WN18RR 40,943 11	WN18	WN18
datasets (apart from hits@10 on WN18 where	WN18	WN18
WN18 FB15k	WN18	WN18
4: Link prediction results on WN18 and FB15k	WN18	WN18
200 0.3 0.4 0.5 0.1 WN18 0.005 0.995 200 30 0.2	WN18	WN18
simple models to do well. WN18 (Bordes et al., 2013) is	WN	WN18
2018) is a subset of WN18, created by removing the inverse	WN	WN18
dr = 200. For WN18 and WN18RR, which both contain	WN	WN18
thus less risk of overfitting (WN18 and WN18RR), whereas higher dropout	WN	WN18
FB15k- 237, (0.005, 0.995) for WN18 and (0.01, 1.0) for WN18RR	WN	WN18
14,951 1,345 FB15k-237 14,541 237 WN18 40,943 18 WN18RR 40,943 11	WN	WN18
datasets (apart from hits@10 on WN18 where	WN	WN18
WN18 FB15k	WN	WN18
4: Link prediction results on WN18 and FB15k	WN	WN18
200 0.3 0.4 0.5 0.1 WN18 0.005 0.995 200 30 0.2	WN	WN18
standard datasets (FB15k-237, WN18RR, FB15k, WN18, YAGO3-10), across which it consistently	WN18	WN18
WN18 [1] a subset of WordNet	WN18	WN18
test sets of FB15k and WN18 contain the inverse of many	WN18	WN18
WN18RR [3] a subset of WN18, created by removing the inverse	WN18	WN18
FB15k 14,951 1,345 WN18 40,943 18 FB15k-237 14,541 237	WN18	WN18
from mean reciprocal rank on WN18 and mean rank on WN18RR	WN18	WN18
, FB15k-237, WN18, and YAGO3-10. Given that mean	WN18	WN18
5. Link prediction results on WN18 and FB15k	WN18	WN18
WN18 FB15k	WN18	WN18
standard datasets (FB15k-237, WN18RR, FB15k, WN18, YAGO3-10), across which it consistently	WN	WN18
WN18 [1] a subset of WordNet	WN	WN18
test sets of FB15k and WN18 contain the inverse of many	WN	WN18
WN18RR [3] a subset of WN18, created by removing the inverse	WN	WN18
FB15k 14,951 1,345 WN18 40,943 18 FB15k-237 14,541 237	WN	WN18
from mean reciprocal rank on WN18 and mean rank on WN18RR	WN	WN18
, FB15k-237, WN18, and YAGO3-10. Given that mean	WN	WN18
5. Link prediction results on WN18 and FB15k	WN	WN18
WN18 FB15k	WN	WN18
datasets such as FB15K or WN18 are surprising. First, let us	WN18	WN18
have a small impact. On WN18 however, they make up 60	WN18	WN18
on the hierarchical predicates of WN18 despite its symmetricity assumption	WN18	WN18
N P Train Valid Test WN18 41k 18 141k 5k 5k	WN18	WN18
WN18 and FB15K are popular benchmarks	WN18	WN18
Model WN18 WN18RR FB15K FB15K-237 YAGO3-10	WN18	WN18
regularizers on the FB15K, FB15K-237, WN18, WN18RR datasets, with a rank	WN18	WN18
and the extensive experiments on WN18 and FB15K reported in Kadlec	WN18	WN18
to 0.95 filtered MRR on WN18, or from 0.46 to 0.86	WN18	WN18
WN18 MRR 0.94 Trouillon et al	WN18	WN18
datasets such as FB15K or WN18 are surprising. First, let us	WN	WN18
have a small impact. On WN18 however, they make up 60	WN	WN18
on the hierarchical predicates of WN18 despite its symmetricity assumption	WN	WN18
N P Train Valid Test WN18 41k 18 141k 5k 5k	WN	WN18
WN18 and FB15K are popular benchmarks	WN	WN18
Model WN18 WN18RR FB15K FB15K-237 YAGO3-10	WN	WN18
regularizers on the FB15K, FB15K-237, WN18, WN18RR datasets, with a rank	WN	WN18
and the extensive experiments on WN18 and FB15K reported in Kadlec	WN	WN18
to 0.95 filtered MRR on WN18, or from 0.46 to 0.86	WN	WN18
as hyponym or hypernym in WN, we expect the filtered MRR	WN	WN18
WN18 MRR 0.94 Trouillon et al	WN	WN18
for generic facts and WordNet (WN18) for lexical relationships between words	WN18	WN18
14,951 1,345 483,142 50,000 59,071 WN18 40,943 18 141,442 5,000 5,000	WN18	WN18
Dataset statistics for FB15K and WN18	WN18	WN18
filt.) of all models on WN18 and FB15K cate- gories into	WN18	WN18
Models WN18 FB15K	WN18	WN18
The resulting hyperparameters for the WN18 dataset are m = 200	WN18	WN18
subset of representative models on WN18 and FB15K. The performance scores	WN18	WN18
WN18 FB15	WN18	WN18
in the literature on the WN18 and FB15K datasets. For the	WN18	WN18
8.3x speedup on FB15K and WN18, re- spectively, on a single	WN18	WN18
FB15K WN18	WN18	WN18
FB15K WN18	WN18	WN18
for generic facts and WordNet (WN18	WN	WN18
14,951 1,345 483,142 50,000 59,071 WN18 40,943 18 141,442 5,000 5,000	WN	WN18
Dataset statistics for FB15K and WN18	WN	WN18
filt.) of all models on WN18 and FB15K cate- gories into	WN	WN18
Models WN18 FB15K	WN	WN18
The resulting hyperparameters for the WN18 dataset are m = 200	WN	WN18
subset of representative models on WN18 and FB15K. The performance scores	WN	WN18
WN18 FB15	WN	WN18
in the literature on the WN18 and FB15K datasets. For the	WN	WN18
8.3x speedup on FB15K and WN18, re- spectively, on a single	WN	WN18
FB15K WN18	WN	WN18
FB15K WN18	WN	WN18
experiments on two standard benchmarks: WN18 a subset of Wordnet [24	WN18	WN18
train/valid/test sets as in [4]. WN18 contains 40, 943 entities, 18	WN18	WN18
Table 1: Results on WN18 and FB15k. Best results are	WN18	WN18
WN18 FB15k	WN18	WN18
set the learning rate for WN18 to 0.1 and for FB15k	WN18	WN18
example per positive example for WN18 and 10 negative examples per	WN18	WN18
set every 50 iterations for WN18 and every 100 iterations for	WN18	WN18
size and λ values on WN18 for SimplE-ignr were 200 and	WN18	WN18
baselines on both datasets. On WN18, SimplE-ignr and SimplE perform as	WN18	WN18
FB15k and has part in WN18	WN18	WN18
we conducted an experiment on WN18 in which we incorporated several	WN18	WN18
experiments on two standard benchmarks: WN18 a subset of Wordnet [24	WN	WN18
train/valid/test sets as in [4]. WN18 contains 40, 943 entities, 18	WN	WN18
Table 1: Results on WN18 and FB15k. Best results are	WN	WN18
WN18 FB15k	WN	WN18
set the learning rate for WN18 to 0.1 and for FB15k	WN	WN18
example per positive example for WN18 and 10 negative examples per	WN	WN18
set every 50 iterations for WN18 and every 100 iterations for	WN	WN18
size and λ values on WN18 for SimplE-ignr were 200 and	WN	WN18
baselines on both datasets. On WN18, SimplE-ignr and SimplE perform as	WN	WN18
FB15k and has part in WN18	WN	WN18
we conducted an experiment on WN18 in which we incorporated several	WN	WN18
4.2. Datasets: FB15K and WN18	WN18	WN18
E| |R| #triples in Train/Valid/Test WN18 40,943 18 141,442 / 5,000	WN18	WN18
split for the FB15K and WN18 datasets	WN18	WN18
model on the FB15K and WN18 datasets. FB15K is a subset	WN18	WN18
KB of general facts, whereas WN18 is a subset of Wordnet	WN18	WN18
WN18 describes lexical and semantic hierarchies	WN18	WN18
considered and each relation of WN18, confirming the advantage of our	WN18	WN18
WN18 FB15K MRR Hits at MRR	WN18	WN18
tested on the FB15K and WN18 datasets. Hits@m metrics are filtered	WN18	WN18
relation of the Wordnet dataset (WN18	WN18	WN18
second. CP performs poorly on WN18 due to the small number	WN18	WN18
negatives), but not much on WN18	WN18	WN18
while not so much on WN18	WN18	WN18
B. WN18 embeddings visualization We used principal	WN18	WN18
relations of the wordnet dataset (WN18	WN18	WN18
Most of WN18 relations describe hierarchies, and are	WN18	WN18
fourth (Bottom) components of the WN18 relations embeddings using PCA. Left	WN18	WN18
4.2. Datasets: FB15K and WN18	WN	WN18
E| |R| #triples in Train/Valid/Test WN18 40,943 18 141,442 / 5,000	WN	WN18
split for the FB15K and WN18 datasets	WN	WN18
model on the FB15K and WN18 datasets. FB15K is a subset	WN	WN18
KB of general facts, whereas WN18 is a subset of Wordnet	WN	WN18
WN18 describes lexical and semantic hierarchies	WN	WN18
considered and each relation of WN18, confirming the advantage of our	WN	WN18
WN18 FB15K MRR Hits at MRR	WN	WN18
tested on the FB15K and WN18 datasets. Hits@m metrics are filtered	WN	WN18
relation of the Wordnet dataset (WN18	WN	WN18
second. CP performs poorly on WN18 due to the small number	WN	WN18
negatives), but not much on WN18	WN	WN18
while not so much on WN18	WN	WN18
B. WN18 embeddings visualization We used principal	WN	WN18
relations of the wordnet dataset (WN18	WN	WN18
Most of WN18 relations describe hierarchies, and are	WN	WN18
fourth (Bottom) components of the WN18 relations embeddings using PCA. Left	WN	WN18
Datasets We used the WordNet (WN) and Freebase (FB15k) datasets introduced	WN	WN18
in (Bordes et al., 2013b). WN contains 151, 442 triplets with	WN	WN18
and T “ 300 on WN (T was determined based on	WN	WN18
FB15k FB15k-401 WN MRR HITS@10 MRR HITS@10 MRR	WN	WN18
performance on both FB and WN, which suggests overfitting. Compared to	WN	WN18
and 90.9% vs. 89.2% on WN) using the same evaluation metric	WN	WN18
performance than TransE, especially on WN	WN	WN18
. Note that WN contains much more entities than	WN	WN18
comparable performance to Bilinear on WN	WN	WN18
is relatively small (compared to WN), the simple form of BILINEAR-DIAG	WN	WN18
R| # train # test WN18 40,943 18 141,442 5,000 FB15k	WN18	WN18
104 WN18	WN18	WN18
k =∞), the performance for WN18 does not deteriorate as it	WN18	WN18
entity prediction experiments. Data Set WN18 FB15K Metric Mean rank Hits@10	WN18	WN18
objects in WN18 have on average few neighbors	WN18	WN18
Since most object pairs of WN18 have a 1-neighborhood whose size	WN18	WN18
R| # train # test WN18 40,943 18 141,442 5,000 FB15k	WN	WN18
104 WN18	WN	WN18
k =∞), the performance for WN18 does not deteriorate as it	WN	WN18
entity prediction experiments. Data Set WN18 FB15K Metric Mean rank Hits@10	WN	WN18
objects in WN18 have on average few neighbors	WN	WN18
Since most object pairs of WN18 have a 1-neighborhood whose size	WN	WN18
measure of 71.5% on the DAVIS 2017 validation set. We make	DAVIS	DAVIS-2017
and the introduction of the DAVIS datasets [29, 31], there has	DAVIS	DAVIS-2017
ponents. For example, the 2018 DAVIS challenge was won by PReMVOS	DAVIS	DAVIS-2017
65% J&F score on the DAVIS 2017 vali- dation set	DAVIS	DAVIS-2017
tuning on the DAVIS 2017 validation dataset with a	DAVIS	DAVIS-2017
algorithm and won the 2018 DAVIS Challenge [2] and also the	DAVIS	DAVIS-2017
training data we use the DAVIS 2017 [31] training set (60	DAVIS	DAVIS-2017
network is evaluated on the DAVIS	DAVIS	DAVIS-2017
2016 [29] validation set, the DAVIS 2017 [31] validation and test-dev	DAVIS	DAVIS-2017
YouTube-Objects [32, 19] dataset. The DAVIS 2016 validation set consists of	DAVIS	DAVIS-2017
single instance is annotated. The DAVIS 2017 dataset comprises a training	DAVIS	DAVIS-2017
set that ex- tends the DAVIS 2016 validation set to a	DAVIS	DAVIS-2017
2. Quantitative results on the DAVIS 2017 validation set. FT denotes	DAVIS	DAVIS-2017
seconds. †: timing extrapolated from DAVIS 2016 assuming linear scaling in	DAVIS	DAVIS-2017
3. Quantitative results on the DAVIS 2017 test-dev set. FT denotes	DAVIS	DAVIS-2017
seconds. †: timing extrapolated from DAVIS 2016 assuming linear scaling in	DAVIS	DAVIS-2017
4. Quality versus timing on DAVIS 2017. The pro- posed FEELVOS	DAVIS	DAVIS-2017
with multiple instances annotated. The DAVIS 2017 test- dev set also	DAVIS	DAVIS-2017
the evaluation measures defined by DAVIS [29]. The first evaluation criterion	DAVIS	DAVIS-2017
compare our results on the DAVIS 2017 validation and test-dev sets	DAVIS	DAVIS-2017
4. Quantitative results on the DAVIS 2016 validation set. FT denotes	DAVIS	DAVIS-2017
does not provide results for DAVIS 2017. On the val- idation	DAVIS	DAVIS-2017
slow PReMVOS [26]. On the DAVIS 2017 test-dev set, FEELVOS achieves	DAVIS	DAVIS-2017
the results on the simpler DAVIS 2016 validation set which only	DAVIS	DAVIS-2017
Table 6. Ablation study on DAVIS 2017. FF and PF denote	DAVIS	DAVIS-2017
components of FEELVOS on the DAVIS 2017 val- idation set. For	DAVIS	DAVIS-2017
we only use the smaller DAVIS 2017 training set as training	DAVIS	DAVIS-2017
5. Qualitative results on the DAVIS 2017 validation set and on	DAVIS	DAVIS-2017
results of FEELVOS on the DAVIS 2017 validation set and the	DAVIS	DAVIS-2017
the art re- sults on DAVIS 2017 for VOS without fine-tuning	DAVIS	DAVIS-2017
and J. Pont-Tuset. The 2018 DAVIS challenge on video object segmentation	DAVIS	DAVIS-2017
object segmentation 2018. The 2018 DAVIS Challenge on Video Object Segmentation	DAVIS	DAVIS-2017
L. Van Gool. The 2017 DAVIS chal- lenge on video object	DAVIS	DAVIS-2017
neural networks for the 2017 DAVIS challenge on video object segmentation	DAVIS	DAVIS-2017
. The 2017 DAVIS Challenge on Video Object Segmentation	DAVIS	DAVIS-2017
of 71.5% on the DAVIS 2017 validation set. We make our	2017	DAVIS-2017
J&F score on the DAVIS 2017 vali- dation set	2017	DAVIS-2017
tuning on the DAVIS 2017 validation dataset with a J&F	2017	DAVIS-2017
data we use the DAVIS 2017 [31] training set (60 videos	2017	DAVIS-2017
29] validation set, the DAVIS 2017 [31] validation and test-dev sets	2017	DAVIS-2017
instance is annotated. The DAVIS 2017 dataset comprises a training set	2017	DAVIS-2017
Quantitative results on the DAVIS 2017 validation set. FT denotes fine-tuning	2017	DAVIS-2017
Quantitative results on the DAVIS 2017 test-dev set. FT denotes fine-tuning	2017	DAVIS-2017
Quality versus timing on DAVIS 2017	2017	DAVIS-2017
multiple instances annotated. The DAVIS 2017 test- dev set also contains	2017	DAVIS-2017
our results on the DAVIS 2017 validation and test-dev sets to	2017	DAVIS-2017
not provide results for DAVIS 2017	2017	DAVIS-2017
PReMVOS [26]. On the DAVIS 2017 test-dev set, FEELVOS achieves a	2017	DAVIS-2017
6. Ablation study on DAVIS 2017	2017	DAVIS-2017
of FEELVOS on the DAVIS 2017 val- idation set. For simplicity	2017	DAVIS-2017
only use the smaller DAVIS 2017 training set as training data	2017	DAVIS-2017
Qualitative results on the DAVIS 2017 validation set and on YouTube-Objects	2017	DAVIS-2017
of FEELVOS on the DAVIS 2017 validation set and the YouTube-Objects	2017	DAVIS-2017
art re- sults on DAVIS 2017 for VOS without fine-tuning. Overall	2017	DAVIS-2017
object seg- mentation. In CVPR, 2017	2017	DAVIS-2017
fully con- nected crfs. PAMI, 2017	2017	DAVIS-2017
segmenta- tion. arXiv preprint arXiv:1706.05587, 2017	2017	DAVIS-2017
separa- ble convolutions. In CVPR, 2017	2017	DAVIS-2017
metric learning. arXiv preprint arXiv:1703.10277, 2017	2017	DAVIS-2017
appli- cations. arXiv preprint arXiv:1704.04861, 2017	2017	DAVIS-2017
video object segmentation. In NeurIPS, 2017	2017	DAVIS-2017
In arXiv preprint arXiv: 1703.09554, 2017	2017	DAVIS-2017
from static images. In CVPR, 2017	2017	DAVIS-2017
in street scenes. In CVPR, 2017	2017	DAVIS-2017
and L. Van Gool. The 2017 DAVIS chal- lenge on video	2017	DAVIS-2017
object segmentation. arXiv preprint arXiv:1704.00675, 2017	2017	DAVIS-2017
coco detection and segmentation challenge 2017 entry. ICCV COCO Chal- lenge	2017	DAVIS-2017
Workshop, 2017	2017	DAVIS-2017
lutional neural networks for the 2017 DAVIS challenge on video object	2017	DAVIS-2017
segmentation. The 2017 DAVIS Challenge on Video Object	2017	DAVIS-2017
Segmentation - CVPR Workshops, 2017	2017	DAVIS-2017
video object segmentation. In BMVC, 2017	2017	DAVIS-2017
40,943 18 141,442 5,000 5,000 FB15K 14,951 1,345 483,142 50,000 59,071	FB15K	FB15k
in each split for the FB15K and WN18 data sets	FB15K	FB15k
5.3 Real Sparse Data Sets: FB15K and WN18	FB15K	FB15k
we evaluated ComplEx on the FB15K and WN18 data sets, as	FB15K	FB15k
for the link prediction task. FB15K is a subset of Freebase	FB15K	FB15k
max-margin ranking loss, especially on FB15K	FB15K	FB15k
WN18 FB15K MRR Hits at MRR Hits	FB15K	FB15k
the models tested on the FB15K and WN18 data sets. Hits@N	FB15K	FB15k
Shimbo, 2017), score divergence on FB15K is only due to the	FB15K	FB15k
1), and three hours on FB15K (K = 200, η	FB15K	FB15k
On FB15K, the gap is much more	FB15K	FB15k
not so many parameters. On FB15K though, it probably overfits due	FB15K	FB15k
on the filtered MRR on FB15K (up to +0.08 improvement from	FB15K	FB15k
to be very important on FB15K, while not so	FB15K	FB15k
MRR for ComplEx on the FB15K and WN18 data sets for	FB15K	FB15k
matrices and not entities. On FB15K, the difference is much more	FB15K	FB15k
time, on WN18 (top) and FB15K (bottom) for each model for	FB15K	FB15k
RESCAL on WN18, TransE on FB15K	FB15K	FB15k
four days to train on FB15K, whereas other models took between	FB15K	FB15k
on larger data sets, as FB15K is but a small subset	FB15K	FB15k
training time to convergence on FB15K for the ComplEx model withK	FB15K	FB15k
leads to better results on FB15K	FB15K	FB15k
the performance of ComplEx on FB15K	FB15K	FB15k
and achieves good performance on FB15K and WN18 data sets. However	FB15K	FB15k
5.3 Real Sparse Data Sets: FB15K and WN18	FB15K	FB15k
237) dataset	237	FB15k-237
237 (Toutanova et al., 2015), NELL-995	237	FB15k-237
237 were created to resolve the	237	FB15k-237
93,003 2.12 1 FB15k-237 14,541 237 272,115 17,535 20,466 310,116 18.71	237	FB15k-237
237 Hits@N Hits@N	237	FB15k-237
237 test sets. Hits@N values are	237	FB15k-237
237, and on two metrics for	237	FB15k-237
237	237	FB15k-237
237 dataset. Y-axis represents attention values	237	FB15k-237
237 6.87 0.237 UMLS 740 0.247	237	FB15k-237
on the popular Free- base (FB15K-237) dataset	FB15K-237	FB15k-237
WN18RR FB15K-237 Hits@N Hits@N	FB15K-237	FB15k-237
Experimental results on WN18RR and FB15K-237 test sets. Hits@N values are	FB15K-237	FB15k-237
process of our model on FB15K-237 dataset. Y-axis represents attention values	FB15K-237	FB15k-237
WN18RR (Dettmers et al., 2018), FB15k	FB15k	FB15k-237
ing subset datasets WN18RR and FB15k	FB15k	FB15k-237
3034 3134 93,003 2.12 1 FB15k	FB15k	FB15k-237
results on five metrics for FB15k	FB15k	FB15k-237
5 shows this distribution on FB15k	FB15k	FB15k-237
1.32 0.025 WN18RR 2.44 -0.01 FB15k	FB15k	FB15k-237
WN18RR (Dettmers et al., 2018), FB15k-237 (Toutanova et al., 2015), NELL-995	FB15k-237	FB15k-237
ing subset datasets WN18RR and FB15k-237 were created to resolve the	FB15k-237	FB15k-237
3034 3134 93,003 2.12 1 FB15k-237 14,541 237 272,115 17,535 20,466	FB15k-237	FB15k-237
results on five metrics for FB15k-237, and on two metrics for	FB15k-237	FB15k-237
5 shows this distribution on FB15k-237	FB15k-237	FB15k-237
1.32 0.025 WN18RR 2.44 -0.01 FB15k-237 6.87 0.237 UMLS 740 0.247	FB15k-237	FB15k-237
FB15K-237 15k 237 272k 18k 20k YAGO3-10 123k	237	FB15k-237
237 and WN18RR. These datasets are	237	FB15k-237
237 YAGO3-10	237	FB15k-237
237, WN18, WN18RR datasets, with a	237	FB15k-237
237, WN18RR and YAGO3-10 may be	237	FB15k-237
237 and YAGO3-10. We checked on	237	FB15k-237
237 MRR 0.32 Dettmers et al	237	FB15k-237
FB15K-237 15k 237 272k 18k 20k	FB15K-237	FB15k-237
created two mod- ified datasets: FB15K-237 and WN18RR. These datasets are	FB15K-237	FB15k-237
Model WN18 WN18RR FB15K FB15K-237 YAGO3-10	FB15K-237	FB15k-237
and regularizers on the FB15K, FB15K-237, WN18, WN18RR datasets, with a	FB15K-237	FB15k-237
The performances of DistMult on FB15K-237, WN18RR and YAGO3-10 may be	FB15K-237	FB15k-237
on the harder datasets WN18RR, FB15K-237 and YAGO3-10. We checked on	FB15K-237	FB15k-237
FB15K-237 MRR 0.32 Dettmers et al	FB15K-237	FB15k-237
237 (Toutanova et al., 2015) was	237	FB15k-237
237, we set entity and relation	237	FB15k-237
compared to FB15k and FB15k- 237, we set de = 200	237	FB15k-237
237	237	FB15k-237
FB15k, (0.0005, 1.0) for FB15k- 237, (0.005, 0.995) for WN18 and	237	FB15k-237
FB15k 14,951 1,345 FB15k-237 14,541 237 WN18 40,943 18 WN18RR 40,943	237	FB15k-237
237), making their results incomparable to	237	FB15k-237
237	237	FB15k-237
237 HypER (Balažević et al., 2019	237	FB15k-237
237	237	FB15k-237
237 with embedding sizes de	237	FB15k-237
237	237	FB15k-237
237 0.0005 1.0 200 200 0.3	237	FB15k-237
237, used to produce the result	237	FB15k-237
237	237	FB15k-237
diction datasets (see Table 2): FB15k (Bordes et al., 2013) is	FB15k	FB15k-237
database of real world facts. FB15k	FB15k	FB15k-237
al., 2015) was created from FB15k by removing the inverse of	FB15k	FB15k-237
FB15k and FB15k	FB15k	FB15k-237
ber of relations compared to FB15k and FB15k- 237, we set	FB15k	FB15k-237
0.4, 0.5) are required for FB15k and FB15k-237. We choose the	FB15k	FB15k-237
best results: (0.003, 0.99) for FB15k, (0.0005, 1.0) for FB15k- 237	FB15k	FB15k-237
FB15k 14,951 1,345 FB15k	FB15k	FB15k-237
relations. For example, improvement on FB15k is +14% over ComplEx and	FB15k	FB15k-237
TuckER on WN18RR; 5.5x on FB15k	FB15k	FB15k-237
WN18RR FB15k	FB15k	FB15k-237
prediction results on WN18RR and FB15k	FB15k	FB15k-237
WN18 FB15k	FB15k	FB15k-237
prediction results on WN18 and FB15k	FB15k	FB15k-237
train all three models on FB15k	FB15k	FB15k-237
for different embeddings sizes on FB15k	FB15k	FB15k-237
FB15k 0.003 0.99 200 200 0.2	FB15k	FB15k-237
0.2 0.3 0. FB15k	FB15k	FB15k-237
for ComplEx and SimplE on FB15k	FB15k	FB15k-237
for ComplEx and SimplE on FB15k	FB15k	FB15k-237
database of real world facts. FB15k-237 (Toutanova et al., 2015) was	FB15k-237	FB15k-237
FB15k and FB15k-237, we set entity and relation	FB15k-237	FB15k-237
are required for FB15k and FB15k-237	FB15k-237	FB15k-237
FB15k 14,951 1,345 FB15k-237 14,541 237 WN18 40,943 18	FB15k-237	FB15k-237
TuckER on WN18RR; 5.5x on FB15k-237), making their results incomparable to	FB15k-237	FB15k-237
WN18RR FB15k-237	FB15k-237	FB15k-237
prediction results on WN18RR and FB15k-237	FB15k-237	FB15k-237
train all three models on FB15k-237 with embedding sizes de	FB15k-237	FB15k-237
for different embeddings sizes on FB15k-237	FB15k-237	FB15k-237
200 0.2 0.2 0.3 0. FB15k-237 0.0005 1.0 200 200 0.3	FB15k-237	FB15k-237
for ComplEx and SimplE on FB15k-237, used to produce the result	FB15k-237	FB15k-237
for ComplEx and SimplE on FB15k-237	FB15k-237	FB15k-237
237, WN18RR, FB15k, WN18, YAGO3-10), across	237	FB15k-237
237 dataset, which determines ne and	237	FB15k-237
237	237	FB15k-237
237 created by [21], noting that	237	FB15k-237
237 is a subset of FB15k	237	FB15k-237
WN18 40,943 18 FB15k-237 14,541 237 WN18RR 40,943 11 YAGO3-10 123,182	237	FB15k-237
237, where we set input dropout	237	FB15k-237
237 takes approximately 12 seconds on	237	FB15k-237
237, WN18, and YAGO3-10. Given that	237	FB15k-237
237	237	FB15k-237
237	237	FB15k-237
237 M-Walk [16] − .437	237	FB15k-237
237	237	FB15k-237
237	237	FB15k-237
237 with the hypernetwork component removed	237	FB15k-237
237, from which they deem label	237	FB15k-237
237	237	FB15k-237
mod- els using standard datasets (FB15k	FB15k	FB15k-237
-237, WN18RR, FB15k, WN18, YAGO3-10), across which it	FB15k	FB15k-237
ConvE and HypER (for the FB15k	FB15k	FB15k-237
for ConvE and HypER on FB15k	FB15k	FB15k-237
FB15k [1] a subset of Freebase	FB15k	FB15k-237
FB15k	FB15k	FB15k-237
validation and test sets of FB15k and WN18 contain the inverse	FB15k	FB15k-237
simple models to do well. FB15k	FB15k	FB15k-237
-237 is a subset of FB15k with the inverse relations removed	FB15k	FB15k-237
FB15k 14,951 1,345 WN18 40,943 18	FB15k	FB15k-237
FB15k	FB15k	FB15k-237
hidden dropout 0.3, apart from FB15k	FB15k	FB15k-237
size 128. One epoch on FB15k	FB15k	FB15k-237
and mean rank on WN18RR, FB15k	FB15k	FB15k-237
prediction results on WN18RR and FB15k	FB15k	FB15k-237
WN18RR FB15k	FB15k	FB15k-237
prediction results on WN18 and FB15k	FB15k	FB15k-237
WN18 FB15k	FB15k	FB15k-237
without hypernetwork on WN18RR and FB15k	FB15k	FB15k-237
WN18RR FB15k	FB15k	FB15k-237
HypER results on WN18RR and FB15k	FB15k	FB15k-237
on mean reciprocal rank for FB15k	FB15k	FB15k-237
a negative influence on the FB15k scores and as such, exclude	FB15k	FB15k-237
WN18RR FB15k	FB15k	FB15k-237
mod- els using standard datasets (FB15k-237, WN18RR, FB15k, WN18, YAGO3-10), across	FB15k-237	FB15k-237
ConvE and HypER (for the FB15k-237 dataset, which determines ne and	FB15k-237	FB15k-237
for ConvE and HypER on FB15k-237	FB15k-237	FB15k-237
FB15k-237 created by [21], noting that	FB15k-237	FB15k-237
simple models to do well. FB15k-237 is a subset of FB15k	FB15k-237	FB15k-237
14,951 1,345 WN18 40,943 18 FB15k-237 14,541 237 WN18RR 40,943 11	FB15k-237	FB15k-237
hidden dropout 0.3, apart from FB15k-237, where we set input dropout	FB15k-237	FB15k-237
size 128. One epoch on FB15k-237 takes approximately 12 seconds on	FB15k-237	FB15k-237
and mean rank on WN18RR, FB15k-237, WN18, and YAGO3-10. Given that	FB15k-237	FB15k-237
prediction results on WN18RR and FB15k-237	FB15k-237	FB15k-237
WN18RR FB15k-237	FB15k-237	FB15k-237
without hypernetwork on WN18RR and FB15k-237	FB15k-237	FB15k-237
WN18RR FB15k-237	FB15k-237	FB15k-237
HypER results on WN18RR and FB15k-237 with the hypernetwork component removed	FB15k-237	FB15k-237
on mean reciprocal rank for FB15k-237, from which they deem label	FB15k-237	FB15k-237
WN18RR FB15k-237	FB15k-237	FB15k-237
237 with 8x and 17x fewer	237	FB15k-237
237 – a subset of FB15k	237	FB15k-237
237, we also find flaws with	237	FB15k-237
237, WN18RR, and YAGO3-10	237	FB15k-237
237	237	FB15k-237
237 does not remove certain symmetric	237	FB15k-237
237, we could not replicate the	237	FB15k-237
237 with 0.23M parameters performs better	237	FB15k-237
237 with 0.425 Hits@10. Comparing to	237	FB15k-237
237	237	FB15k-237
237 Hits Hits	237	FB15k-237
237 Inverse Model 13526 .35 .35	237	FB15k-237
237 compared to WN18RR, is that	237	FB15k-237
237), but that shal- low models	237	FB15k-237
237 0.733 0.16 YAGO3-10 0.988 0.21	237	FB15k-237
237	237	FB15k-237
237	237	FB15k-237
noted that the WN18 and FB15k datasets suffer from test set	FB15k	FB15k-237
results on both WN18 and FB15k	FB15k	FB15k-237
than DistMult and R-GCNs on FB15k	FB15k	FB15k-237
a con- volution model on FB15k – one of the dataset	FB15k	FB15k-237
FB15k (Bordes et al. 2013a) is	FB15k	FB15k-237
Chen (2015) that WN18 and FB15k suffer from test leakage through	FB15k	FB15k-237
Toutanova and Chen (2015) introduced FB15k	FB15k	FB15k-237
-237 – a subset of FB15k where inverse relations are removed	FB15k	FB15k-237
results on both WN18 and FB15k	FB15k	FB15k-237
to each dataset. Apart from FB15k, which was cor- rected by	FB15k	FB15k-237
FB15k	FB15k	FB15k-237
research, we recommend against using FB15k and WN18 and instead recommend	FB15k	FB15k-237
FB15k	FB15k	FB15k-237
well on WN18, YAGO3-10 and FB15k	FB15k	FB15k-237
the mean reciprocal rank (WN18, FB15k, YAGO3-10) and AUC-PR (Countries) statistics	FB15k	FB15k-237
training datasets of WN18 and FB15k have 94% and 81% test	FB15k	FB15k-237
on the standard bench- marks FB15k and WN18 are shown in	FB15k	FB15k-237
many different metrics for both FB15k and WN18. How- ever, it	FB15k	FB15k-237
relations for YAGO3- 10 and FB15k	FB15k	FB15k-237
and Chen (2015) to derive FB15k	FB15k	FB15k-237
YAGO3-10, for some metrics on FB15k, and it does well on	FB15k	FB15k-237
For FB15k	FB15k	FB15k-237
can see that ConvE for FB15k	FB15k	FB15k-237
achieves state-of-the- art results on FB15k	FB15k	FB15k-237
prediction results for WN18 and FB15k	FB15k	FB15k-237
WN18 FB15k Hits Hits	FB15k	FB15k-237
prediction results for WN18RR and FB15k	FB15k	FB15k-237
WN18RR FB15k	FB15k	FB15k-237
on datasets like YAGO3-10 and FB15k	FB15k	FB15k-237
FB15k) relation-specific in- degree and reverse	FB15k	FB15k-237
high (high-WN18) and low (low- FB15k) relation-specific indegree datasets by deleting	FB15k	FB15k-237
FB15k we have ConvE 0.586 Hits@10	FB15k	FB15k-237
model more complex graphs (e.g. FB15k and FB15k-237), but that shal	FB15k	FB15k-237
0.104 0.06 WN18 0.125 0.45 FB15k 0.599 0.04 FB15-237 0.733 0.16	FB15k	FB15k-237
Table 7: Ablation study for FB15k	FB15k	FB15k-237
the most central nodes in FB15k	FB15k	FB15k-237
inverse relations of WN18 and FB15k was first reported by Toutanova	FB15k	FB15k-237
state-of-the-art results on WN18 and FB15k	FB15k	FB15k-237
WN18 and slightly worse for FB15k	FB15k	FB15k-237
I was unable to replicate FB15k scores that I initially reported.4	FB15k	FB15k-237
thus far (Kinship, WN18, WN18RR, FB15k	FB15k	FB15k-237
than DistMult and R-GCNs on FB15k-237 with 8x and 17x fewer	FB15k-237	FB15k-237
Toutanova and Chen (2015) introduced FB15k-237 – a subset of FB15k	FB15k-237	FB15k-237
which was cor- rected by FB15k-237, we also find flaws with	FB15k-237	FB15k-237
and WN18 and instead recommend FB15k-237, WN18RR, and YAGO3-10	FB15k-237	FB15k-237
relations for YAGO3- 10 and FB15k-237	FB15k-237	FB15k-237
and Chen (2015) to derive FB15k-237 does not remove certain symmetric	FB15k-237	FB15k-237
For FB15k-237, we could not replicate the	FB15k-237	FB15k-237
can see that ConvE for FB15k-237 with 0.23M parameters performs better	FB15k-237	FB15k-237
achieves state-of-the- art results on FB15k-237 with 0.425 Hits@10. Comparing to	FB15k-237	FB15k-237
prediction results for WN18RR and FB15k-237	FB15k-237	FB15k-237
WN18RR FB15k-237 Hits Hits	FB15k-237	FB15k-237
on datasets like YAGO3-10 and FB15k-237 compared to WN18RR, is that	FB15k-237	FB15k-237
complex graphs (e.g. FB15k and FB15k-237), but that shal- low models	FB15k-237	FB15k-237
Table 7: Ablation study for FB15k-237	FB15k-237	FB15k-237
thus far (Kinship, WN18, WN18RR, FB15k-237	FB15k-237	FB15k-237
instance, on the benchmark dataset FB15K (Bordes et al., 2013), the	FB15K	FB15k
largest datasets we experiment on, FB15K and YAGO3-10, contain respectively 15k/1.3k	FB15K	FB15k
non- symmetric datasets such as FB15K or WN18 are surprising. First	FB15K	FB15k
such as capital_of . In FB15K, those type of problematic queries	FB15K	FB15k
two minutes per epoch on FB15K, we decided to use the	FB15K	FB15k
of the nuclear 3-norm on FB15K	FB15K	FB15k
41k 11 87k 3k 3k FB15K 15k 1k 500k 50k 60k	FB15K	FB15k
FB15K	FB15K	FB15k
WN18 and FB15K are popular benchmarks in the	FB15K	FB15k
created two mod- ified datasets: FB15K	FB15K	FB15k
Model WN18 WN18RR FB15K FB15K	FB15K	FB15k
algorithms and regularizers on the FB15K, FB15K-237, WN18, WN18RR datasets, with	FB15K	FB15k
ComplEx with batch-size 100 on FB15K took about 110s and 325s	FB15K	FB15k
of the good performances on FB15K of DistMult and the extensive	FB15K	FB15k
experiments on WN18 and FB15K reported in Kadlec et al	FB15K	FB15k
The performances of DistMult on FB15K	FB15K	FB15k
to 0.46 filtered MRR on FB15K for CP and 0.70 to	FB15K	FB15k
MRR per relation type on FB15K	FB15K	FB15k
to 0.86 filtered MRR on FB15K	FB15K	FB15k
the biggest improvements observed on FB15K and YAGO3-10. Following the analysis	FB15K	FB15k
on the harder datasets WN18RR, FB15K	FB15K	FB15k
Effect of the batch-size on FB15K in the Standard (top) and	FB15K	FB15k
the mean reciprocal rank on FB15K for CP) nor the impact	FB15K	FB15k
FB15K MRR 0.84 Kadlec et al	FB15K	FB15k
FB15K	FB15K	FB15k
diction datasets (see Table 2): FB15k (Bordes et al., 2013) is	FB15k	FB15k
database of real world facts. FB15k	FB15k	FB15k
al., 2015) was created from FB15k by removing the inverse of	FB15k	FB15k
FB15k and FB15k	FB15k	FB15k
ber of relations compared to FB15k and FB15k- 237, we set	FB15k	FB15k
0.4, 0.5) are required for FB15k and FB15k-237. We choose the	FB15k	FB15k
best results: (0.003, 0.99) for FB15k, (0.0005, 1.0) for FB15k- 237	FB15k	FB15k
FB15k 14,951 1,345 FB15k	FB15k	FB15k
relations. For example, improvement on FB15k is +14% over ComplEx and	FB15k	FB15k
TuckER on WN18RR; 5.5x on FB15k	FB15k	FB15k
WN18RR FB15k	FB15k	FB15k
prediction results on WN18RR and FB15k	FB15k	FB15k
WN18 FB15k	FB15k	FB15k
prediction results on WN18 and FB15k	FB15k	FB15k
train all three models on FB15k	FB15k	FB15k
for different embeddings sizes on FB15k	FB15k	FB15k
FB15k 0.003 0.99 200 200 0.2	FB15k	FB15k
0.2 0.3 0. FB15k	FB15k	FB15k
for ComplEx and SimplE on FB15k	FB15k	FB15k
for ComplEx and SimplE on FB15k	FB15k	FB15k
mod- els using standard datasets (FB15k	FB15k	FB15k
-237, WN18RR, FB15k, WN18, YAGO3-10), across which it	FB15k	FB15k
ConvE and HypER (for the FB15k	FB15k	FB15k
for ConvE and HypER on FB15k	FB15k	FB15k
FB15k [1] a subset of Freebase	FB15k	FB15k
FB15k	FB15k	FB15k
validation and test sets of FB15k and WN18 contain the inverse	FB15k	FB15k
simple models to do well. FB15k	FB15k	FB15k
-237 is a subset of FB15k with the inverse relations removed	FB15k	FB15k
FB15k 14,951 1,345 WN18 40,943 18	FB15k	FB15k
FB15k	FB15k	FB15k
hidden dropout 0.3, apart from FB15k	FB15k	FB15k
size 128. One epoch on FB15k	FB15k	FB15k
and mean rank on WN18RR, FB15k	FB15k	FB15k
prediction results on WN18RR and FB15k	FB15k	FB15k
WN18RR FB15k	FB15k	FB15k
prediction results on WN18 and FB15k	FB15k	FB15k
WN18 FB15k	FB15k	FB15k
without hypernetwork on WN18RR and FB15k	FB15k	FB15k
WN18RR FB15k	FB15k	FB15k
HypER results on WN18RR and FB15k	FB15k	FB15k
on mean reciprocal rank for FB15k	FB15k	FB15k
a negative influence on the FB15k scores and as such, exclude	FB15k	FB15k
WN18RR FB15k	FB15k	FB15k
subset of Wordnet [24], and FB15k a subset of Freebase [2	FB15k	FB15k
and 5, 000 test triples. FB15k contains 14, 951 entities, 1	FB15k	FB15k
1: Results on WN18 and FB15k	FB15k	FB15k
WN18 FB15k	FB15k	FB15k
WN18 to 0.1 and for FB15k to 0.05 and used adagrad	FB15k	FB15k
examples per positive example in FB15k	FB15k	FB15k
and every 100 iterations for FB15k and selected the iteration that	FB15k	FB15k
size and λ values on FB15k for SimplE-ignr were 200 and	FB15k	FB15k
state-of-the-art tensor factorization model. On FB15k, SimplE outperforms the existing baselines	FB15k	FB15k
for the friendship relation in FB15k, if an entity e1 is	FB15k	FB15k
for, e.g., netflix genre in FB15k and has part in WN18	FB15k	FB15k
40,943 18 141,442 5,000 5,000 FB15K 14,951 1,345 483,142 50,000 59,071	FB15K	FB15k
in each split for the FB15K and WN18 data sets	FB15K	FB15k
5.3 Real Sparse Data Sets: FB15K and WN18	FB15K	FB15k
we evaluated ComplEx on the FB15K and WN18 data sets, as	FB15K	FB15k
for the link prediction task. FB15K is a subset of Freebase	FB15K	FB15k
max-margin ranking loss, especially on FB15K	FB15K	FB15k
WN18 FB15K MRR Hits at MRR Hits	FB15K	FB15k
the models tested on the FB15K and WN18 data sets. Hits@N	FB15K	FB15k
Shimbo, 2017), score divergence on FB15K is only due to the	FB15K	FB15k
1), and three hours on FB15K (K = 200, η	FB15K	FB15k
On FB15K, the gap is much more	FB15K	FB15k
not so many parameters. On FB15K though, it probably overfits due	FB15K	FB15k
on the filtered MRR on FB15K (up to +0.08 improvement from	FB15K	FB15k
to be very important on FB15K, while not so	FB15K	FB15k
MRR for ComplEx on the FB15K and WN18 data sets for	FB15K	FB15k
matrices and not entities. On FB15K, the difference is much more	FB15K	FB15k
time, on WN18 (top) and FB15K (bottom) for each model for	FB15K	FB15k
RESCAL on WN18, TransE on FB15K	FB15K	FB15k
four days to train on FB15K, whereas other models took between	FB15K	FB15k
on larger data sets, as FB15K is but a small subset	FB15K	FB15k
training time to convergence on FB15K for the ComplEx model withK	FB15K	FB15k
leads to better results on FB15K	FB15K	FB15k
the performance of ComplEx on FB15K	FB15K	FB15k
and achieves good performance on FB15K and WN18 data sets. However	FB15K	FB15k
5.3 Real Sparse Data Sets: FB15K and WN18	FB15K	FB15k
noted that the WN18 and FB15k datasets suffer from test set	FB15k	FB15k
results on both WN18 and FB15k	FB15k	FB15k
than DistMult and R-GCNs on FB15k	FB15k	FB15k
a con- volution model on FB15k – one of the dataset	FB15k	FB15k
FB15k (Bordes et al. 2013a) is	FB15k	FB15k
Chen (2015) that WN18 and FB15k suffer from test leakage through	FB15k	FB15k
Toutanova and Chen (2015) introduced FB15k	FB15k	FB15k
-237 – a subset of FB15k where inverse relations are removed	FB15k	FB15k
results on both WN18 and FB15k	FB15k	FB15k
to each dataset. Apart from FB15k, which was cor- rected by	FB15k	FB15k
FB15k	FB15k	FB15k
research, we recommend against using FB15k and WN18 and instead recommend	FB15k	FB15k
FB15k	FB15k	FB15k
well on WN18, YAGO3-10 and FB15k	FB15k	FB15k
the mean reciprocal rank (WN18, FB15k, YAGO3-10) and AUC-PR (Countries) statistics	FB15k	FB15k
training datasets of WN18 and FB15k have 94% and 81% test	FB15k	FB15k
on the standard bench- marks FB15k and WN18 are shown in	FB15k	FB15k
many different metrics for both FB15k and WN18. How- ever, it	FB15k	FB15k
relations for YAGO3- 10 and FB15k	FB15k	FB15k
and Chen (2015) to derive FB15k	FB15k	FB15k
YAGO3-10, for some metrics on FB15k, and it does well on	FB15k	FB15k
For FB15k	FB15k	FB15k
can see that ConvE for FB15k	FB15k	FB15k
achieves state-of-the- art results on FB15k	FB15k	FB15k
prediction results for WN18 and FB15k	FB15k	FB15k
WN18 FB15k Hits Hits	FB15k	FB15k
prediction results for WN18RR and FB15k	FB15k	FB15k
WN18RR FB15k	FB15k	FB15k
on datasets like YAGO3-10 and FB15k	FB15k	FB15k
FB15k) relation-specific in- degree and reverse	FB15k	FB15k
high (high-WN18) and low (low- FB15k) relation-specific indegree datasets by deleting	FB15k	FB15k
FB15k we have ConvE 0.586 Hits@10	FB15k	FB15k
model more complex graphs (e.g. FB15k and FB15k-237), but that shal	FB15k	FB15k
0.104 0.06 WN18 0.125 0.45 FB15k 0.599 0.04 FB15-237 0.733 0.16	FB15k	FB15k
Table 7: Ablation study for FB15k	FB15k	FB15k
the most central nodes in FB15k	FB15k	FB15k
inverse relations of WN18 and FB15k was first reported by Toutanova	FB15k	FB15k
state-of-the-art results on WN18 and FB15k	FB15k	FB15k
WN18 and slightly worse for FB15k	FB15k	FB15k
I was unable to replicate FB15k scores that I initially reported.4	FB15k	FB15k
thus far (Kinship, WN18, WN18RR, FB15k	FB15k	FB15k
2] and also the 2018 YouTube	YouTube	YouTube
set (60 videos) and the YouTube	YouTube	YouTube
and test-dev sets, and the YouTube	YouTube	YouTube
FEELVOS (-YTB-VOS) denotes training without YouTube	YouTube	YouTube
also contains 30 sequences. The YouTube	YouTube	YouTube
5. Quantitative results on the YouTube	YouTube	YouTube
The used evaluation protocol for YouTube	YouTube	YouTube
2.4% higher when not using YouTube	YouTube	YouTube
5 shows the results on YouTube	YouTube	YouTube
2017 validation set and on YouTube	YouTube	YouTube
2017 validation set and the YouTube	YouTube	YouTube
refinement and merging for the YouTube	YouTube	YouTube
S. Cohen, and T. Huang. YouTube	YouTube	YouTube
the large-scale and challenging NIST IJB-A un- constrained face recognition benchmark	IJB-A	IJB-A
of our submissions to NIST IJB-A 2017 face recognition competitions, where	IJB-A	IJB-A
available face recognition datasets (e.g., IJB-A (15)) is usually unbalanced and	IJB-A	IJB-A
of pose distribution in the IJB-A (15) dataset w/o and w	IJB-A	IJB-A
NIST) IARPA Janus Benchmark A (IJB-A) (15) unconstrained face recognition benchmark	IJB-A	IJB-A
identification tracks in the NIST IJB-A 2017 face recognition competitions. This	IJB-A	IJB-A
identification tracks in the NIST IJB-A 2017 face recognition competitions	IJB-A	IJB-A
unconstrained face recognition benchmark dataset IJB-A (15). IJB-A (15) contains both	IJB-A	IJB-A
of DA-GAN with state-of-the-arts on IJB-A (15) verification protocol. For all	IJB-A	IJB-A
i.e., verification and identification) on IJB-A (15) benchmark dataset with three	IJB-A	IJB-A
baselines and other state-of-the-arts on IJB-A (15) unconstrained face verification and	IJB-A	IJB-A
and identification tracks in NIST IJB-A 2017 face recognition competitions3. This	IJB-A	IJB-A
and identification protocols to NIST IJB-A 2017 face recognition competition committee	IJB-A	IJB-A
of DA-GAN with state-of-the-arts on IJB-A (15) identification protocol. For FNIR	IJB-A	IJB-A
identification closed set results for IJB-A (15) split1 to gain insights	IJB-A	IJB-A
the large-scale and challenging NIST IJB-A unconstrained face recognition benchmark without	IJB-A	IJB-A
and identification tracks in NIST IJB-A 2017 face recognition competitions. It	IJB-A	IJB-A
on 26th, Apirl, 2017. The IJB-A benchmark dataset, relevant information and	IJB-A	IJB-A
the large-scale and challenging NIST IJB	IJB	IJB-A
of our submissions to NIST IJB	IJB	IJB-A
available face recognition datasets (e.g., IJB	IJB	IJB-A
of pose distribution in the IJB	IJB	IJB-A
NIST) IARPA Janus Benchmark A (IJB	IJB	IJB-A
identification tracks in the NIST IJB	IJB	IJB-A
performance on the challenging NIST IJB	IJB	IJB-A
identification tracks in the NIST IJB	IJB	IJB-A
unconstrained face recognition benchmark dataset IJB	IJB	IJB-A
-A (15). IJB	IJB	IJB-A
of DA-GAN with state-of-the-arts on IJB	IJB	IJB-A
i.e., verification and identification) on IJB	IJB	IJB-A
baselines and other state-of-the-arts on IJB	IJB	IJB-A
and identification tracks in NIST IJB	IJB	IJB-A
and identification protocols to NIST IJB	IJB	IJB-A
of DA-GAN with state-of-the-arts on IJB	IJB	IJB-A
identification closed set results for IJB	IJB	IJB-A
the large-scale and challenging NIST IJB	IJB	IJB-A
and identification tracks in NIST IJB	IJB	IJB-A
on 26th, Apirl, 2017. The IJB	IJB	IJB-A
A un- constrained face recognition benchmark	A	IJB-A
A 2017 face recognition competitions, where	A	IJB-A
A (15)) is usually unbalanced and	A	IJB-A
A (15) dataset w/o and w	A	IJB-A
Technology (NIST) IARPA Janus Benchmark A (IJB-A) (15) unconstrained face recognition	A	IJB-A
A 2017 face recognition competitions. This	A	IJB-A
on the challenging NIST IJB- A (15) unconstrained face recognition benchmark	A	IJB-A
A 2017 face recognition competitions	A	IJB-A
A (15). IJB-A (15) contains both	A	IJB-A
A (15) verification protocol. For all	A	IJB-A
A (15) benchmark dataset with three	A	IJB-A
A (15) unconstrained face verification and	A	IJB-A
A 2017 face recognition competitions3. This	A	IJB-A
A 2017 face recognition competition committee	A	IJB-A
A (15) identification protocol. For FNIR	A	IJB-A
A (15) split1 to gain insights	A	IJB-A
A unconstrained face recognition benchmark without	A	IJB-A
A 2017 face recognition competitions. It	A	IJB-A
A benchmark dataset, relevant information and	A	IJB-A
4] J.-C. Chen, R. Ranjan, A	A	IJB-A
Chollet. keras. https://github.com/fchollet/keras, 2015. [7] A	A	IJB-A
C. Stauffer, Q. Cao, and A	A	IJB-A
Xu, D. Warde-Farley, S. Ozair, A	A	IJB-A
Klare, B. Klein, E. Taborsky, A	A	IJB-A
Cheney, K. Allen, P. Grother, A	A	IJB-A
A	A	IJB-A
20] I. Masi, A	A	IJB-A
arXiv preprint arXiv:1411.1784, 2014. [22] A	A	IJB-A
2016. [23] O. M. Parkhi, A	A	IJB-A
. Vedaldi, and A	A	IJB-A
arXiv:1401.4082, 2014. [27] S. Sankaranarayanan, A	A	IJB-A
28] A	A	IJB-A
D. Wang, C. Otto, and A	A	IJB-A
H. Lai, S. Yan, and A	A	IJB-A
Liu, X. Nie, J. Feng, A	A	IJB-A
. A	A	IJB-A
. Kassim, and S. Yan. A live face swapper. In Proceedings	A	IJB-A
margin on the pub- lic IJB-A [20] and IJB-B [37] face	IJB-A	IJB-A
challenging public face recognition datasets IJB-A [20] and IJB-B [37] are	IJB-A	IJB-A
as [5, 10, 18, 25], IJB-A and IJB-B are intended for	IJB-A	IJB-A
consider in this work. The IJB-A dataset contains 5,712 images and	IJB-A	IJB-A
dataset is an extension of IJB-A with a total of 11,754	IJB-A	IJB-A
the standard benchmark procedure for IJB-A and IJB-B, and evaluate on	IJB-A	IJB-A
apart from the fact that IJB-A defines 10 test splits, while	IJB-A	IJB-A
two galleries for identification. For IJB-A and for IJB-B iden- tification	IJB-A	IJB-A
Network deployment. In the IJB-A and IJB-B datasets, there are	IJB-A	IJB-A
against the state-of-the-art on the IJB-A and IJB-B datasets. The currently	IJB-A	IJB-A
identification and verification on both IJB-A and IJB-B datasets. In particular	IJB-A	IJB-A
surpasses [44] marginally on the IJB-A verification task, despite the fact	IJB-A	IJB-A
to Rank-10 for identification on IJB-A	IJB-A	IJB-A
; but this is because IJB-A is not challenging enough and	IJB-A	IJB-A
IJB-A	IJB-A	IJB-A
state-of-the-art for verification on the IJB-A and IJB-B datasets. A higher	IJB-A	IJB-A
IJB-A	IJB-A	IJB-A
state-of-the-art for identification on the IJB-A and IJB-B datasets. A higher	IJB-A	IJB-A
the state-of-the-art on the challenging IJB-A and IJB-B benchmarks by a	IJB-A	IJB-A
exceeds the state-of-the-art on the IJB	IJB	IJB-A
margin on the pub- lic IJB	IJB	IJB-A
-A [20] and IJB	IJB	IJB-A
challenging public face recognition datasets IJB	IJB	IJB-A
-A [20] and IJB	IJB	IJB-A
as [5, 10, 18, 25], IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
consider in this work. The IJB	IJB	IJB-A
and 4.2 videos, respectively. The IJB	IJB	IJB-A
dataset is an extension of IJB	IJB	IJB-A
the standard benchmark procedure for IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
apart from the fact that IJB	IJB	IJB-A
defines 10 test splits, while IJB	IJB	IJB-A
two galleries for identification. For IJB	IJB	IJB-A
-A and for IJB	IJB	IJB-A
Network deployment. In the IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
4.3 Ablation studies on IJB	IJB	IJB-A
it to baselines on the IJB	IJB	IJB-A
larger and more challenging than IJB	IJB	IJB-A
1: Verification performance on the IJB	IJB	IJB-A
2: Identification performance on the IJB	IJB	IJB-A
against the state-of-the-art on the IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
identification and verification on both IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
surpasses [44] marginally on the IJB	IJB	IJB-A
to Rank-10 for identification on IJB	IJB	IJB-A
-A; but this is because IJB	IJB	IJB-A
measures on the more challenging IJB	IJB	IJB-A
IJB	IJB	IJB-A
IJB	IJB	IJB-A
state-of-the-art for verification on the IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
vs 0.705 for verification on IJB	IJB	IJB-A
0.743 for iden- tification on IJB	IJB	IJB-A
vs 0.671 for verification on IJB	IJB	IJB-A
vs 0.706 for verification on IJB	IJB	IJB-A
IJB	IJB	IJB-A
IJB	IJB	IJB-A
state-of-the-art for identification on the IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
Fig. 3: Results on the IJB	IJB	IJB-A
the state-of-the-art on the challenging IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
a tem- plate in the IJB	IJB	IJB-A
A straightforward method to tackle multiple	A	IJB-A
Y. Zhong, R. Arandjelović and A	A	IJB-A
A [20] and IJB-B [37] face	A	IJB-A
A few methods go beyond simple	A	IJB-A
Y. Zhong, R. Arandjelović and A	A	IJB-A
Feature extraction. A neural network is used to	A	IJB-A
Y. Zhong, R. Arandjelović and A	A	IJB-A
Y. Zhong, R. Arandjelović and A	A	IJB-A
A [20] and IJB-B [37] are	A	IJB-A
A and IJB-B are intended for	A	IJB-A
A dataset contains 5,712 images and	A	IJB-A
A with a total of 11,754	A	IJB-A
A and IJB-B, and evaluate on	A	IJB-A
A defines 10 test splits, while	A	IJB-A
A and for IJB-B iden- tification	A	IJB-A
A and IJB-B datasets, there are	A	IJB-A
Y. Zhong, R. Arandjelović and A	A	IJB-A
and more challenging than IJB- A	A	IJB-A
performance on the IJB-B dataset. A higher value of TAR is	A	IJB-A
performance on the IJB-B dataset. A higher value of TPIR is	A	IJB-A
Y. Zhong, R. Arandjelović and A	A	IJB-A
A and IJB-B datasets. The currently	A	IJB-A
A and IJB-B datasets. In particular	A	IJB-A
A verification task, despite the fact	A	IJB-A
A	A	IJB-A
A is not challenging enough and	A	IJB-A
A	A	IJB-A
the IJB-A and IJB-B datasets. A higher value of TAR is	A	IJB-A
Y. Zhong, R. Arandjelović and A	A	IJB-A
A	A	IJB-A
the IJB-A and IJB-B datasets. A higher value of TPIR is	A	IJB-A
A cc	A	IJB-A
A and IJB-B benchmarks by a	A	IJB-A
Arandjelović, R., Gronat, P., Torii, A	A	IJB-A
3] Arandjelović, R., Zisserman, A	A	IJB-A
Xie, W., Parkhi, O.M., Zisserman, A	A	IJB-A
.: VGGFace2: A dataset for recognising faces across	A	IJB-A
Chen, J., Ranjan, R., Kumar, A	A	IJB-A
Parkhi, O.M., Cao, Q., Zisserman, A	A	IJB-A
He, X., Gao, J.: MS-Celeb-1M: A dataset and benchmark for large-scale	A	IJB-A
17] Jégou, H., Zisserman, A	A	IJB-A
Klein, B., Taborsky, E., Blanton, A	A	IJB-A
Allen, K., Grother, P., Mah, A	A	IJB-A
., Jain, A	A	IJB-A
A	A	IJB-A
Parkhi, O.M., Simonyan, K., Vedaldi, A	A	IJB-A
., Zisserman, A.: A compact and discriminative face track	A	IJB-A
25] Parkhi, O.M., Vedaldi, A	A	IJB-A
., Zisserman, A	A	IJB-A
Ma, S., Huang, S., Karpathy, A	A	IJB-A
., Khosla, A	A	IJB-A
., Bernstein, M., Berg, A	A	IJB-A
Kalenichenko, D., Philbin, J.: Facenet: A unified embedding for face recognition	A	IJB-A
32] Turaga, P., Veeraraghavan, A	A	IJB-A
., Srivastava, A	A	IJB-A
33] Vedaldi, A	A	IJB-A
Y. Zhong, R. Arandjelović and A	A	IJB-A
Whitelam, C., Taborsky, E., Blanton, A	A	IJB-A
Miller, T., Kalka, N., Jain, A	A	IJB-A
Xie, W., Shen, L., Zisserman, A	A	IJB-A
39] Xie, W., Zisserman, A	A	IJB-A
Zhang, R., Isola, P., Efros, A	A	IJB-A
A	A	IJB-A
state-of-the-art results on the challenging IJB-A dataset, achieving True Accept Rate	IJB-A	IJB-A
algorithms on the publicly available IJB-A [16] dataset. Data quality im	IJB-A	IJB-A
achieves new state-of-the-art results on IJB-A dataset, and competing results on	IJB-A	IJB-A
14], YouTube Face [19] and IJB-A [16	IJB-A	IJB-A
of 0.0001 on the challenging IJB-A [16] dataset	IJB-A	IJB-A
a) Face Verification Performance on IJB-A dataset. The templates are divided	IJB-A	IJB-A
b) Sample template images from IJB-A dataset with high, medium and	IJB-A	IJB-A
simple ex- periment on the IJB-A [16] dataset where we divide	IJB-A	IJB-A
six different sets for the IJB-A face verification protocol. It can	IJB-A	IJB-A
set- ting, and the challenging IJB-A dataset [16] on the 1:1	IJB-A	IJB-A
for 1:1 verification protocol on IJB-A [16] as shown in Table	IJB-A	IJB-A
Table 2. TAR on IJB-A 1:1 Verification Protocol @FAR 0.0001	IJB-A	IJB-A
it improves the TAR@FAR=0.0001 on IJB-A by more than 10% (Table	IJB-A	IJB-A
Table 3. TAR on IJB-A 1:1 Verification Protocol @FAR 0.0001	IJB-A	IJB-A
14], YouTube Face [34] and IJB-A [16] datasets. We crop and	IJB-A	IJB-A
The IJB-A dataset [16] contains 500 subjects	IJB-A	IJB-A
of recent DCNN-based methods on IJB-A dataset. We achieve state-of-the-art result	IJB-A	IJB-A
using the training splits of IJB-A	IJB-A	IJB-A
FAR of 0.0001 on IJB-A	IJB-A	IJB-A
LFW [14], YTF [34] and IJB-A [16] datasets clearly suggests the	IJB-A	IJB-A
Experiments on LFW, YTF and IJB-A datasets show that the proposed	IJB-A	IJB-A
Identification and Verification Evaluation on IJB-A dataset	IJB-A	IJB-A
IJB-A Verification (TAR@FAR) IJB-A Identification Method 0.0001 0.001 0.01	IJB-A	IJB-A
state-of-the-art results on the challenging IJB	IJB	IJB-A
algorithms on the publicly available IJB	IJB	IJB-A
achieves new state-of-the-art results on IJB	IJB	IJB-A
14], YouTube Face [19] and IJB	IJB	IJB-A
of 0.0001 on the challenging IJB	IJB	IJB-A
a) Face Verification Performance on IJB	IJB	IJB-A
b) Sample template images from IJB	IJB	IJB-A
simple ex- periment on the IJB	IJB	IJB-A
six different sets for the IJB	IJB	IJB-A
set- ting, and the challenging IJB	IJB	IJB-A
for 1:1 verification protocol on IJB	IJB	IJB-A
Table 2. TAR on IJB	IJB	IJB-A
it improves the TAR@FAR=0.0001 on IJB	IJB	IJB-A
Table 3. TAR on IJB	IJB	IJB-A
14], YouTube Face [34] and IJB	IJB	IJB-A
The IJB	IJB	IJB-A
of recent DCNN-based methods on IJB	IJB	IJB-A
using the training splits of IJB	IJB	IJB-A
FAR of 0.0001 on IJB	IJB	IJB-A
LFW [14], YTF [34] and IJB	IJB	IJB-A
Experiments on LFW, YTF and IJB	IJB	IJB-A
achieves the state-of-the-art result on IJB	IJB	IJB-A
Identification and Verification Evaluation on IJB	IJB	IJB-A
IJB-A Verification (TAR@FAR) IJB	IJB	IJB-A
deep convolutional neural networks (DCNNs). A typical pipeline for face veri	A	IJB-A
A dataset, achieving True Accept Rate	A	IJB-A
A [16] dataset. Data quality im	A	IJB-A
A dataset, and competing results on	A	IJB-A
A [16	A	IJB-A
A [16] dataset	A	IJB-A
A recent approach [33] introduced center	A	IJB-A
SphereFace [20] proposes angular softmax (A	A	IJB-A
to its correct identity label. A softmax loss function is used	A	IJB-A
A dataset. The templates are divided	A	IJB-A
A dataset with high, medium and	A	IJB-A
Figure 2. A general pipeline for training and	A	IJB-A
A [16] dataset where we divide	A	IJB-A
A face verification protocol. It can	A	IJB-A
upper bound for the parameter. A better performance is obtained by	A	IJB-A
maximum of 100K iter- ations. A training batch size of 256	A	IJB-A
A dataset [16] on the 1:1	A	IJB-A
A similar trend is observed for	A	IJB-A
A [16] as shown in Table	A	IJB-A
A 1:1 Verification Protocol @FAR 0.0001	A	IJB-A
A by more than 10% (Table	A	IJB-A
A 1:1 Verification Protocol @FAR 0.0001	A	IJB-A
A [16] datasets. We crop and	A	IJB-A
pre-trained on ImageNet [26] dataset. A fully-connected layer of dimension 512	A	IJB-A
A dataset [16] contains 500 subjects	A	IJB-A
A dataset. We achieve state-of-the-art result	A	IJB-A
A	A	IJB-A
A	A	IJB-A
A [16] datasets clearly suggests the	A	IJB-A
A datasets show that the proposed	A	IJB-A
the state-of-the-art result on IJB- A [16] dataset. In conclusion, L2-softmax	A	IJB-A
A dataset	A	IJB-A
A Verification (TAR@FAR) IJB-A Identification Method	A	IJB-A
References [1] M. Abadi, A	A	IJB-A
C. Citro, G. S. Corrado, A	A	IJB-A
S. Ghe- mawat, I. Goodfellow, A	A	IJB-A
4] J.-C. Chen, R. Ranjan, A	A	IJB-A
6] A	A	IJB-A
Kavukcuoglu, and C. Farabet. Torch7: A matlab-like environment for machine learning	A	IJB-A
C. Stauffer, Q. Cao, and A	A	IJB-A
He, and J. Gao. Ms-celeb-1m: A dataset and benchmark for large-scale	A	IJB-A
10] A	A	IJB-A
Labeled faces in the wild: A database for studying face recognition	A	IJB-A
Klare, B. Klein, E. Taborsky, A	A	IJB-A
Cheney, K. Allen, P. Grother, A	A	IJB-A
. Mah, and A	A	IJB-A
A	A	IJB-A
J.-C. Chen, and R. Chellappa. A proximity- aware hierarchical clustering of	A	IJB-A
22] I. Masi, A	A	IJB-A
Sankara- narayanan, J.-C. Chen, and A	A	IJB-A
24] O. M. Parkhi, A	A	IJB-A
. Vedaldi, and A	A	IJB-A
Satheesh, S. Ma, Z. Huang, A	A	IJB-A
. Karpathy, A	A	IJB-A
. Khosla, M. Bernstein, A	A	IJB-A
27] S. Sankaranarayanan, A	A	IJB-A
Kalenichenko, and J. Philbin. Facenet: A uni- fied embedding for face	A	IJB-A
D. Wang, C. Otto, and A	A	IJB-A
Z. Li, and Y. Qiao. A discrimina- tive feature learning approach	A	IJB-A
S. Pranata, and S. Shen. A good practice towards top performance	A	IJB-A
task (Rank-1 accuracy) on the IJB-A dataset [4] for ResNet-18. For	IJB-A	IJB-A
4.2. Evaluation on IJB-A with Full Pose Variation	IJB-A	IJB-A
called IARPA Janus Benchmark A (IJB-A) [13] that cov- ers full	IJB-A	IJB-A
2 reports our results on IJB-A	IJB-A	IJB-A
2. Comparative performance analysis on IJB-A benchmark. Results reported are the	IJB-A	IJB-A
10 folds specified in the IJB-A protocol. Symbol ‘-’ indicates that	IJB-A	IJB-A
MS-Celeb-1M and directly tested on IJB-A with- out fine-tuning. Data augmentation	IJB-A	IJB-A
on the training splits of IJB-A	IJB-A	IJB-A
ment. Extensive results on CFP, IJB-A, and MS-Celeb-1M datasets demonstrate the	IJB-A	IJB-A
task (Rank-1 accuracy) on the IJB	IJB	IJB-A
4.2. Evaluation on IJB	IJB	IJB-A
called IARPA Janus Benchmark A (IJB	IJB	IJB-A
videos. The faces in the IJB	IJB	IJB-A
2 reports our results on IJB	IJB	IJB-A
2. Comparative performance analysis on IJB	IJB	IJB-A
10 folds specified in the IJB	IJB	IJB-A
MS-Celeb-1M and directly tested on IJB	IJB	IJB-A
on the training splits of IJB	IJB	IJB-A
ment. Extensive results on CFP, IJB	IJB	IJB-A
faces compared to frontal faces. A key reason is that the	A	IJB-A
Figure 1. A state-of-the-art face recognition model [34	A	IJB-A
the recognition of profile faces. A large body of methods normalize	A	IJB-A
A dataset [4] for ResNet-18. For	A	IJB-A
recognition models [29, 25, 33]. A fully connected layer is then	A	IJB-A
A with Full Pose Variation	A	IJB-A
benchmark called IARPA Janus Benchmark A (IJB-A) [13] that cov- ers	A	IJB-A
The faces in the IJB- A dataset contain extreme poses and	A	IJB-A
A	A	IJB-A
A benchmark. Results reported are the	A	IJB-A
A protocol. Symbol ‘-’ indicates that	A	IJB-A
A with- out fine-tuning. Data augmentation	A	IJB-A
Figure 6. A comparison between the false positive	A	IJB-A
A	A	IJB-A
4.3. A Further Analysis on Influences of	A	IJB-A
A, and MS-Celeb-1M datasets demonstrate the	A	IJB-A
G. Duan, and J. Sun. A practical transfer learning algorithm for	A	IJB-A
J. Sun. Bayesian face revisited: A joint formulation. In ECCV, 2012	A	IJB-A
O. Parkhi, Q. Cao, and A	A	IJB-A
Labeled faces in the wild: A database for studying face recognition	A	IJB-A
Klare, B. Klein, E. Taborsky, A	A	IJB-A
Cheney, K. Allen, P. Grother, A	A	IJB-A
. Mah, and A	A	IJB-A
and recognition: IARPA Janus Benchmark A	A	IJB-A
facial landmarks in the wild: A large-scale, real- world database for	A	IJB-A
15] K. Lenc and A	A	IJB-A
20] I. Masi, A	A	IJB-A
21] A	A	IJB-A
Nguyen, J. Yosinski, Y. Bengio, A	A	IJB-A
23] S. Sankaranarayanan, A	A	IJB-A
24] S. Sankaranarayanan, A	A	IJB-A
Kalenichenko, and J. Philbin. Facenet: A uni- fied embedding for face	A	IJB-A
N. Srivastava, G. E. Hinton, A	A	IJB-A
D. Wang, C. Otto, and A	A	IJB-A
Z. Li, and Y. Qiao. A discriminative fea- ture learning approach	A	IJB-A
Y. Sugano, M. Fritz, and A	A	IJB-A
exposed faces. The experiments on IJB-A, YouTube Face, Celebrity-1000 video face	IJB-A	IJB-A
YouTube Face dataset [42], the IJB-A dataset [19], and the Celebrity-1000	IJB-A	IJB-A
2. Face images in the IJB-A dataset, sorted by their scores	IJB-A	IJB-A
1. Performance comparison on the IJB-A dataset. TAR/FAR: True/False Accept Rate	IJB-A	IJB-A
3 for details) in the IJB-A dataset [19] on the extracted	IJB-A	IJB-A
train the network on the IJB-A dataset again, and Table 1	IJB-A	IJB-A
the IARPA Janus Benchmark A (IJB-A) [19], the YouTube Face dataset	IJB-A	IJB-A
3.3. Results on IJB-A dataset	IJB-A	IJB-A
The IJB-A dataset [19] contains face images	IJB-A	IJB-A
2. Performance evaluation on the IJB-A dataset. For verification, the true	IJB-A	IJB-A
and the baselines on the IJB-A dataset over 10 splits	IJB-A	IJB-A
compared to the results on IJB-A	IJB-A	IJB-A
exposed faces. The experiments on IJB	IJB	IJB-A
YouTube Face dataset [42], the IJB	IJB	IJB-A
2. Face images in the IJB	IJB	IJB-A
1. Performance comparison on the IJB	IJB	IJB-A
3 for details) in the IJB	IJB	IJB-A
train the network on the IJB	IJB	IJB-A
the IARPA Janus Benchmark A (IJB	IJB	IJB-A
3.3. Results on IJB	IJB	IJB-A
The IJB	IJB	IJB-A
2. Performance evaluation on the IJB	IJB	IJB-A
and the baselines on the IJB	IJB	IJB-A
compared to the results on IJB	IJB	IJB-A
A, YouTube Face, Celebrity-1000 video face	A	IJB-A
the need for frame-to-frame matching. A straightforward solution might be extracting	A	IJB-A
A ug	A	IJB-A
A dataset [19], and the Celebrity-1000	A	IJB-A
A dataset, sorted by their scores	A	IJB-A
A dataset. TAR/FAR: True/False Accept Rate	A	IJB-A
A dataset [19] on the extracted	A	IJB-A
A dataset again, and Table 1	A	IJB-A
datasets: the IARPA Janus Benchmark A (IJB-A) [19], the YouTube Face	A	IJB-A
A dataset	A	IJB-A
A dataset [19] contains face images	A	IJB-A
A dataset. For verification, the true	A	IJB-A
A dataset over 10 splits	A	IJB-A
A	A	IJB-A
7] J.-C. Chen, R. Ranjan, A	A	IJB-A
9] A	A	IJB-A
C. Stauffer, Q. Cao, and A	A	IJB-A
12] A	A	IJB-A
Klare, B. Klein, E. Taborsky, A	A	IJB-A
Cheney, K. Allen, P. Grother, A	A	IJB-A
. Mah, M. Burge, and A	A	IJB-A
25] I. Masi, A	A	IJB-A
O. M. Parkhi, K. Simonyan, A	A	IJB-A
. Vedaldi, and A. Zisserman. A compact and discriminative face track	A	IJB-A
28] O. M. Parkhi, A	A	IJB-A
. Vedaldi, and A	A	IJB-A
30] S. Sankaranarayanan, A	A	IJB-A
Kalenichenko, and J. Philbin. FaceNet: A unified embedding for face recognition	A	IJB-A
D. Erhan, V. Vanhoucke, and A	A	IJB-A
37] P. Turaga, A. Veeraraghavan, A	A	IJB-A
D. Wang, C. Otto, and A	A	IJB-A
Z. Li, and Y. Qiao. A discriminative fea- ture learning approach	A	IJB-A
In contrast, the newly released IJB-A face recognition dataset [3] unifies	IJB-A	IJB-A
template. Extensive performance evaluations on IJB-A show a surprising result, that	IJB-A	IJB-A
the same top performance on IJB-A for template-based face verification and	IJB-A	IJB-A
The IJB-A dataset [3] was created to	IJB-A	IJB-A
set of images [14]. The IJB-A dataset is the only public	IJB-A	IJB-A
Recent evaluations on IJB-A [3] are also based on	IJB-A	IJB-A
publishing the earliest results on IJB-A	IJB-A	IJB-A
and worst templates pairs in IJB-A for verification (identification errors are	IJB-A	IJB-A
Figure 2. IJB-A Evaluation. (top) 1:1 DET for	IJB-A	IJB-A
and gallery template pairs in IJB-A split 1 and CMC for	IJB-A	IJB-A
identification on IJB-A split 1 (see section 4.2	IJB-A	IJB-A
4.2. IJB-A Evaluation	IJB-A	IJB-A
the experimental system on the IJB-A verification and iden- tification protocols	IJB-A	IJB-A
3]. IJB-A contains 5712 images and 2085	IJB-A	IJB-A
Performance evaluation for IJB-A requires evaluation of ten random	IJB-A	IJB-A
the overall evaluation results on IJB-A	IJB-A	IJB-A
and gallery template pairs in IJB-A split 1 and CMC for	IJB-A	IJB-A
an unmodeled dataset bias for IJB-A faces, or that CASIA is	IJB-A	IJB-A
image only, while IJB-A is imagery and videos	IJB-A	IJB-A
identification CMC curves (right) for IJB-A split-1. (top) Template adaptation compared	IJB-A	IJB-A
tween the templates are the IJB-A Template IDs for probe and	IJB-A	IJB-A
art per- formance on the IJB-A dataset. Furthermore, we showed that	IJB-A	IJB-A
hold for other datasets. The IJB-A dataset is currently the only	IJB-A	IJB-A
In contrast, the newly released IJB	IJB	IJB-A
template. Extensive performance evaluations on IJB	IJB	IJB-A
the same top performance on IJB	IJB	IJB-A
The IJB	IJB	IJB-A
set of images [14]. The IJB	IJB	IJB-A
of template adaptation on the IJB	IJB	IJB-A
Recent evaluations on IJB	IJB	IJB-A
publishing the earliest results on IJB	IJB	IJB-A
and worst templates pairs in IJB	IJB	IJB-A
Figure 2. IJB	IJB	IJB-A
and gallery template pairs in IJB	IJB	IJB-A
and CMC for identification on IJB	IJB	IJB-A
4.2. IJB	IJB	IJB-A
the experimental system on the IJB	IJB	IJB-A
and iden- tification protocols [3]. IJB	IJB	IJB-A
Performance evaluation for IJB	IJB	IJB-A
the overall evaluation results on IJB	IJB	IJB-A
and gallery template pairs in IJB	IJB	IJB-A
and CMC for identification on IJB	IJB	IJB-A
not provide much benefit on IJB	IJB	IJB-A
an unmodeled dataset bias for IJB	IJB	IJB-A
CASIA is image only, while IJB	IJB	IJB-A
identification CMC curves (right) for IJB	IJB	IJB-A
tween the templates are the IJB	IJB	IJB-A
art per- formance on the IJB	IJB	IJB-A
hold for other datasets. The IJB	IJB	IJB-A
A face recognition dataset [3] unifies	A	IJB-A
A show a surprising result, that	A	IJB-A
A for template-based face verification and	A	IJB-A
age, pose, illumination and expression (A	A	IJB-A
uncontrolled collection and amount of A	A	IJB-A
A dataset [3] was created to	A	IJB-A
A pr	A	IJB-A
instead of image-to-image or video-to-video. A template is a set of	A	IJB-A
A dataset is the only public	A	IJB-A
template adaptation on the IJB- A dataset has generated surprising results	A	IJB-A
A [3] are also based on	A	IJB-A
A	A	IJB-A
First, we provide preliminary definitions. A media ob- servation x is	A	IJB-A
all frames in a video. A template X is a set	A	IJB-A
template P to gallery G. A gallery contains templates G	A	IJB-A
A for verification (identification errors are	A	IJB-A
A Evaluation. (top) 1:1 DET for	A	IJB-A
A split 1 and CMC for	A	IJB-A
A split 1 (see section 4.2	A	IJB-A
A Evaluation	A	IJB-A
A verification and iden- tification protocols	A	IJB-A
A contains 5712 images and 2085	A	IJB-A
A requires evaluation of ten random	A	IJB-A
A	A	IJB-A
A split 1 and CMC for	A	IJB-A
identification on IJB- A split 1. This study shows	A	IJB-A
provide much benefit on IJB- A, in contrast with reported performance	A	IJB-A
A faces, or that CASIA is	A	IJB-A
A is imagery and videos	A	IJB-A
A split-1. (top) Template adaptation compared	A	IJB-A
A Template IDs for probe and	A	IJB-A
A dataset. Furthermore, we showed that	A	IJB-A
A dataset is currently the only	A	IJB-A
beled faces in the wild: A database for studying face recog	A	IJB-A
Klein, B., Taborsky, E., Blanton, A	A	IJB-A
Allen, K., Grother, P., Mah, A	A	IJB-A
., Jain, A	A	IJB-A
and recognition: IARPA Janus benchmark A	A	IJB-A
4] Parkhi, O., Vedaldi, A	A	IJB-A
., Zisserman, A	A	IJB-A
Kalenichenko, D., Philbin, J.: FaceNet: A unified embedding for face recognition	A	IJB-A
Learned-Miller, E., Huang, G., RoyChowdhury, A	A	IJB-A
Labeled Faces in the Wild: A Survey. In: Advances in Face	A	IJB-A
Otto, C., Klare, B., Jain, A	A	IJB-A
Hill, M., Swindle, J., O’Toole, A	A	IJB-A
15] Sankaranarayanan, S., Alavi, A	A	IJB-A
18] RoyChowdry, A	A	IJB-A
19] Simonyan, K., Zisserman, A	A	IJB-A
Erhan, D., Vanhoucke, V., Rabinovich, A	A	IJB-A
Sun, J.: Bayesian face revisited: A joint formulation. In: ECCV. (2012	A	IJB-A
Chen, J., Ranjan, R., Kumar, A	A	IJB-A
24] Pan, S.J., Yang, Q.: A survey on transfer learning. Knowl	A	IJB-A
25] Razavian, A	A	IJB-A
28] Malisiewicz, T., Gupta, A	A	IJB-A
., Efros, A	A	IJB-A
Wang, D., Otto, C., Jain, A	A	IJB-A
Parkhi, O.M., Simonyan, K., Vedaldi, A	A	IJB-A
., Zisserman, A.: A compact and discriminative face track	A	IJB-A
Wang, X., Lin, C.: Liblinear: A library for large linear classification	A	IJB-A
variations in viewpoint and illumination (IJB-A [23] dataset). We address this	IJB-A	IJB-A
on the IARPA Janus Benchmark-A (IJB-A) [23] dataset. The dataset contains	IJB-A	IJB-A
For IJB-A dataset, given a template containing	IJB-A	IJB-A
Identification and Verification Evaluation on IJB-A dataset	IJB-A	IJB-A
IJB-A Verification (TAR@FAR) IJB-A Identification Method 0.001 0.01 0.1	IJB-A	IJB-A
End-to-End face recognition systems on IJB-A	IJB-A	IJB-A
with recently published methods on IJB-A	IJB-A	IJB-A
that are present in the IJB-A dataset for better image clarity	IJB-A	IJB-A
variations in viewpoint and illumination (IJB	IJB	IJB-A
on the IARPA Janus Benchmark-A (IJB	IJB	IJB-A
For IJB	IJB	IJB-A
Identification and Verification Evaluation on IJB	IJB	IJB-A
IJB-A Verification (TAR@FAR) IJB	IJB	IJB-A
End-to-End face recognition systems on IJB	IJB	IJB-A
with recently published methods on IJB	IJB	IJB-A
that are present in the IJB	IJB	IJB-A
A [23] dataset). We address this	A	IJB-A
A	A	IJB-A
Fig. 2: A general multitask learning framework for	A	IJB-A
map size of 6× 6. A dimensionality reduction layer is added	A	IJB-A
or computed using HyperFace [36]. A cross- entropy loss LG is	A	IJB-A
A	A	IJB-A
error by more than 30%. A low standard deviation of 0.13	A	IJB-A
A (IJB-A) [23] dataset. The dataset	A	IJB-A
A dataset, given a template containing	A	IJB-A
descriptors, as explained in [41]. A naive way to measure the	A	IJB-A
cosine distance between their descriptors. A better way is to learn	A	IJB-A
A dataset	A	IJB-A
A Verification (TAR@FAR) IJB-A Identification Method	A	IJB-A
A	A	IJB-A
A	A	IJB-A
A dataset for better image clarity	A	IJB-A
6] J.-C. Chen, A	A	IJB-A
R. Ranjan, V. M. Patel, A	A	IJB-A
Alavi, and R. Chel- lappa. A cascaded convolutional neural network for	A	IJB-A
8] J.-C. Chen, R. Ranjan, A	A	IJB-A
C. Stauffer, Q. Cao, and A	A	IJB-A
I. Goodfellow, Y. Bengio, and A	A	IJB-A
H. Han, C. Otto, and A	A	IJB-A
C. Fang, and X. Ding. A new biologically inspired active appearance	A	IJB-A
beled faces in the wild: A database for studying face recognition	A	IJB-A
Jain and E. Learned-Miller. Fddb: A benchmark for face detec- tion	A	IJB-A
21] A	A	IJB-A
Klare, B. Klein, E. Taborsky, A	A	IJB-A
Cheney, K. Allen, P. Grother, A	A	IJB-A
. Mah, M. Burge, and A	A	IJB-A
facial landmarks in the wild: A large-scale, real-world database for facial	A	IJB-A
25] A	A	IJB-A
26] A	A	IJB-A
J. Brandt, and G. Hua. A convolutional neural network cascade for	A	IJB-A
30] S. Liao, A	A	IJB-A
. Jain, and S. Li. A fast and accurate unconstrained face	A	IJB-A
32] I. Masi, A	A	IJB-A
34] O. M. Parkhi, A	A	IJB-A
. Vedaldi, and A	A	IJB-A
M. Patel, and R. Chellappa. A deep pyramid deformable part model	A	IJB-A
Patel, and R. Chellappa. Hyperface: A deep multi- task learning framework	A	IJB-A
S. Zhou, J. C. Chen, A	A	IJB-A
. Kumar, A	A	IJB-A
Jr. and T. Tesafaye. Morph: A longitudinal image database of normal	A	IJB-A
41] S. Sankaranarayanan, A	A	IJB-A
Kalenichenko, and J. Philbin. Facenet: A unified embedding for face recognition	A	IJB-A
45] K. E. A	A	IJB-A
R. Uijlings, T. Gevers, and A	A	IJB-A
parts responses to face detection: A deep learning approach. In IEEE	A	IJB-A
Face alignment across large poses: A 3d solution. arXiv preprint arXiv:1511.07212	A	IJB-A
A Multi-task Learning	A	IJB-A
A	A	IJB-A
A	A	IJB-A
A Face Detection	A	IJB-A
face recognition accuracy on the IJB-A and IJB-B benchmarks: using the	IJB-A	IJB-A
on the highly challeng- ing IJB-A [22] and IJB-B benchmarks [43	IJB-A	IJB-A
A [22] and B [43] (IJB-A and IJB-B). Importantly, these benchmarks	IJB-A	IJB-A
2. Verification and identification on IJB-A and IJB-B, com- paring landmark	IJB-A	IJB-A
face alignment methods. Three baseline IJB-A results are also provided as	IJB-A	IJB-A
IJB-A [22] Crosswhite et al. [9	IJB-A	IJB-A
a) ROC IJB-A (b) CMC IJB-A	IJB-A	IJB-A
Verification and identification results on IJB-A and IJB-B. ROC and CMC	IJB-A	IJB-A
and identification results on both IJB-A and IJB-B are provided in	IJB-A	IJB-A
provides, as reference, three state-of-the-art IJB-A re- sults [9, 31, 36	IJB-A	IJB-A
addition, our verification scores on IJB-A outperform the scores reported for	IJB-A	IJB-A
views of the faces in IJB-A and IJB-B	IJB-A	IJB-A
face recognition accuracy on the IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
on the highly challeng- ing IJB	IJB	IJB-A
-A [22] and IJB	IJB	IJB-A
A [22] and B [43] (IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
2. Verification and identification on IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
face alignment methods. Three baseline IJB	IJB	IJB-A
IJB	IJB	IJB-A
IJB	IJB	IJB-A
a) ROC IJB-A (b) CMC IJB	IJB	IJB-A
c) ROC IJB-B (d) CMC IJB	IJB	IJB-A
Verification and identification results on IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
and identification results on both IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
provides, as reference, three state-of-the-art IJB	IJB	IJB-A
baseline results from [43] for IJB	IJB	IJB-A
verification and identification accuracies on IJB	IJB	IJB-A
addition, our verification scores on IJB	IJB	IJB-A
views of the faces in IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
A and IJB-B benchmarks: using the	A	IJB-A
A	A	IJB-A
A [22] and IJB-B benchmarks [43	A	IJB-A
one face image and another. A different interpretation of align- ment	A	IJB-A
3. A critique of facial landmark detection	A	IJB-A
A second potential problem lies in	A	IJB-A
p,1]T = A	A	IJB-A
where A and R are the camera	A	IJB-A
too small for this purpose. A key problem is therefore obtaining	A	IJB-A
the images in this set. A potential danger in using an	A	IJB-A
produced by the camera matrix A, rotation matrix R, and the	A	IJB-A
face recognition: IARPA Janus Benchmark A [22] and B [43] (IJB-A	A	IJB-A
A and IJB-B, com- paring landmark	A	IJB-A
A results are also provided as	A	IJB-A
A [22] Crosswhite et al. [9	A	IJB-A
A (b) CMC IJB-A	A	IJB-A
A and IJB-B. ROC and CMC	A	IJB-A
A and IJB-B are provided in	A	IJB-A
A re- sults [9, 31, 36	A	IJB-A
A outperform the scores reported for	A	IJB-A
A N/A N/A N/A N/A 0.6	A	IJB-A
A and IJB-B	A	IJB-A
References [1] A	A	IJB-A
4] A	A	IJB-A
. Bansal, B. Russell, and A	A	IJB-A
O. Parkhi, Q. Cao, and A	A	IJB-A
H. Kaya, H. Dibeklioglu, and A	A	IJB-A
20] A	A	IJB-A
21] D. E. King. Dlib-ml: A machine learning toolkit. J. Mach	A	IJB-A
Klare, B. Klein, E. Taborsky, A	A	IJB-A
Cheney, K. Allen, P. Grother, A	A	IJB-A
. Mah, M. Burge, and A	A	IJB-A
A	A	IJB-A
facial landmarks in the wild: A large-scale, real- world database for	A	IJB-A
24] A	A	IJB-A
25] A. Kumar, A	A	IJB-A
31] I. Masi, T. Hassner, A	A	IJB-A
33] I. Masi, A	A	IJB-A
34] O. M. Parkhi, A	A	IJB-A
. Vedaldi, and A	A	IJB-A
W. Liu, J. Kosecka, and A	A	IJB-A
42] A	A	IJB-A
43] C. Whitelam, E. Taborsky, A	A	IJB-A
Adams, T. Miller, N. Kalka, A	A	IJB-A
. K. Jain, J. A	A	IJB-A
Guibas, and S. Savarese. Objectnet3D: A large scale database for 3D	A	IJB-A
and S. Savarese. Beyond pascal: A benchmark for 3D object detection	A	IJB-A
Z. Yang and R. Nevatia. A multi-scale cascade fully con- volutional	A	IJB-A
49] A	A	IJB-A
Face alignment across large poses: A 3D solution. In Proc. Conf	A	IJB-A
Experiments on the challeng- ing IJB-A dataset show that the proposed	IJB-A	IJB-A
simple clustering experiments on both IJB-A and LFW datasets	IJB-A	IJB-A
algorithms on the publicly available IJB-A dataset ([2], [3]) that was	IJB-A	IJB-A
age. In the case of IJB-A dataset, the data is divided	IJB-A	IJB-A
4 followed by results on IJB-A and CFP datasets and a	IJB-A	IJB-A
media collection from LFW and IJB-A datasets	IJB-A	IJB-A
Since the release of the IJB-A dataset [2], there have been	IJB-A	IJB-A
deep CNN models on the IJB-A dataset (Table 2 in Results	IJB-A	IJB-A
Figure 2: Performance improvement on IJB-A split 1: FAR (vs) TAR	IJB-A	IJB-A
for split 1 of the IJB-A verify protocol for the three	IJB-A	IJB-A
the ten splits of the IJB-A dataset	IJB-A	IJB-A
datasets: 1. IARPA Janus Benchmark-A (IJB-A) [2]: This dataset	IJB-A	IJB-A
60). The faces in the IJB-A dataset contain extreme poses and	IJB-A	IJB-A
Some sample images from the IJB-A dataset are shown in Figure	IJB-A	IJB-A
An additional challenge of the IJB-A verification protocol is that the	IJB-A	IJB-A
given test template of the IJB-A data we perform two kinds	IJB-A	IJB-A
from. The metadata provided with IJB-A gives us the media id	IJB-A	IJB-A
Figure 3: Images from the IJB-A dataset	IJB-A	IJB-A
a given image. For the IJB-A dataset, since most images contain	IJB-A	IJB-A
provided bounding box. In the IJB-A dataset, there are few images	IJB-A	IJB-A
NVIDIA TitanX GPU. For the IJB-A dataset, we use the training	IJB-A	IJB-A
Method IJB-A Verification (FNMR@FMR) IJB-A Identification	IJB-A	IJB-A
and Verification results on the IJB-A dataset. For identification, the scores	IJB-A	IJB-A
More specifi- cally, for the IJB-A dataset, given a template containing	IJB-A	IJB-A
types of results for the IJB-A dataset: Veri- fication and Identification	IJB-A	IJB-A
the evaluation metrics for the IJB-A proto- col can be found	IJB-A	IJB-A
Performance on IJB-A	IJB-A	IJB-A
to existing results for the IJB-A Verification and Iden- tification protocol	IJB-A	IJB-A
formance provided along with the IJB-A dataset. • Parkhi et al	IJB-A	IJB-A
many test images of the IJB-A dataset contain extreme poses, harsh	IJB-A	IJB-A
raw features specifically to the IJB-A dataset	IJB-A	IJB-A
and identification protocols of the IJB-A dataset, with the exception of	IJB-A	IJB-A
the training data of the IJB-A splits. Let’s denote the pooled	IJB-A	IJB-A
2. We use the IJB-A dataset and cluster the templates	IJB-A	IJB-A
for each split in the IJB-A verify protocol	IJB-A	IJB-A
the split 1 of the IJB-A dataset. Top row (a,b) shows	IJB-A	IJB-A
5: Clustering metrics over the IJB-A 1:1 protocol. The standard deviation	IJB-A	IJB-A
Clustering IJB-A:- The IJB-A dataset is processed as de	IJB-A	IJB-A
Precision-Recall (PR) curve for the IJB-A clustering experiment in Figure 6	IJB-A	IJB-A
PR curve for clustering the IJB-A data using embedded features exhibits	IJB-A	IJB-A
method on two challenging datasets: IJB-A and CFP and achieved performance	IJB-A	IJB-A
algorithm on the LFW and IJB-A datasets. For future work, we	IJB-A	IJB-A
Experiments on the challeng- ing IJB	IJB	IJB-A
simple clustering experiments on both IJB	IJB	IJB-A
algorithms on the publicly available IJB	IJB	IJB-A
age. In the case of IJB	IJB	IJB-A
4 followed by results on IJB	IJB	IJB-A
media collection from LFW and IJB	IJB	IJB-A
Since the release of the IJB	IJB	IJB-A
deep CNN models on the IJB	IJB	IJB-A
Figure 2: Performance improvement on IJB	IJB	IJB-A
for split 1 of the IJB	IJB	IJB-A
the ten splits of the IJB	IJB	IJB-A
datasets: 1. IARPA Janus Benchmark-A (IJB	IJB	IJB-A
60). The faces in the IJB	IJB	IJB-A
Some sample images from the IJB	IJB	IJB-A
An additional challenge of the IJB	IJB	IJB-A
given test template of the IJB	IJB	IJB-A
from. The metadata provided with IJB	IJB	IJB-A
Figure 3: Images from the IJB	IJB	IJB-A
a given image. For the IJB	IJB	IJB-A
provided bounding box. In the IJB	IJB	IJB-A
NVIDIA TitanX GPU. For the IJB	IJB	IJB-A
Method IJB-A Verification (FNMR@FMR) IJB	IJB	IJB-A
and Verification results on the IJB	IJB	IJB-A
More specifi- cally, for the IJB	IJB	IJB-A
types of results for the IJB	IJB	IJB-A
the evaluation metrics for the IJB	IJB	IJB-A
Performance on IJB	IJB	IJB-A
to existing results for the IJB	IJB	IJB-A
formance provided along with the IJB	IJB	IJB-A
many test images of the IJB	IJB	IJB-A
raw features specifically to the IJB	IJB	IJB-A
and identification protocols of the IJB	IJB	IJB-A
the training data of the IJB	IJB	IJB-A
2. We use the IJB	IJB	IJB-A
for each split in the IJB	IJB	IJB-A
the split 1 of the IJB	IJB	IJB-A
5: Clustering metrics over the IJB	IJB	IJB-A
Clustering IJB-A:- The IJB	IJB	IJB-A
Precision-Recall (PR) curve for the IJB	IJB	IJB-A
PR curve for clustering the IJB	IJB	IJB-A
method on two challenging datasets: IJB	IJB	IJB-A
algorithm on the LFW and IJB	IJB	IJB-A
A dataset show that the proposed	A	IJB-A
A and LFW datasets	A	IJB-A
verification problem can be solved. A face verification algorithm compares two	A	IJB-A
A dataset ([2], [3]) that was	A	IJB-A
A dataset, the data is divided	A	IJB-A
A and CFP datasets and a	A	IJB-A
A datasets	A	IJB-A
A dataset [2], there have been	A	IJB-A
A dataset (Table 2 in Results	A	IJB-A
A split 1: FAR (vs) TAR	A	IJB-A
A verify protocol for the three	A	IJB-A
A dataset	A	IJB-A
A (IJB-A) [2]: This dataset	A	IJB-A
A dataset contain extreme poses and	A	IJB-A
A dataset are shown in Figure	A	IJB-A
A verification protocol is that the	A	IJB-A
A data we perform two kinds	A	IJB-A
A gives us the media id	A	IJB-A
A dataset	A	IJB-A
A dataset, since most images contain	A	IJB-A
A dataset, there are few images	A	IJB-A
A dataset, we use the training	A	IJB-A
A Verification (FNMR@FMR) IJB-A Identification	A	IJB-A
A dataset. For identification, the scores	A	IJB-A
A dataset, given a template containing	A	IJB-A
A dataset: Veri- fication and Identification	A	IJB-A
A proto- col can be found	A	IJB-A
A	A	IJB-A
A Verification and Iden- tification protocol	A	IJB-A
A dataset. • Parkhi et al	A	IJB-A
A dataset contain extreme poses, harsh	A	IJB-A
A dataset	A	IJB-A
A dataset, with the exception of	A	IJB-A
A splits. Let’s denote the pooled	A	IJB-A
A dataset and cluster the templates	A	IJB-A
A verify protocol	A	IJB-A
A dataset. Top row (a,b) shows	A	IJB-A
A 1:1 protocol. The standard deviation	A	IJB-A
A	A	IJB-A
A dataset is processed as de	A	IJB-A
A clustering experiment in Figure 6	A	IJB-A
A data using embedded features exhibits	A	IJB-A
A and CFP and achieved performance	A	IJB-A
A datasets. For future work, we	A	IJB-A
Labeled faces in the wild: A database for studying face recognition	A	IJB-A
B. F. Klare, E. Taborsky, A	A	IJB-A
Cheney, K. Allen, P. Grother, A	A	IJB-A
. Mah, M. Burge, and A	A	IJB-A
Kalenichenko, and J. Philbin, “Facenet: A unified embedding for face recognition	A	IJB-A
D. Wang, C. Otto, and A	A	IJB-A
7] O. M. Parkhi, A	A	IJB-A
. Vedaldi, and A	A	IJB-A
K. Simonyan, O. M. Parkhi, A	A	IJB-A
. Vedaldi, and A	A	IJB-A
S. Belongie, O. Shamir, and A	A	IJB-A
16] A	A	IJB-A
Patel, and R. Chellappa, “Hyperface: A deep multi-task learning framework for	A	IJB-A
23] I. Masi, A	A	IJB-A
D. Erhan, V. Vanhoucke, and A	A	IJB-A
C. Stauffer, Q. Cao, and A	A	IJB-A
C. Otto, D. Wang, and A	A	IJB-A
tested on the LFW and IJB-A (verification and identification) benchmarks and	IJB-A	IJB-A
proach on the LFW [11], IJB-A (verification and identification) and CS2	IJB-A	IJB-A
outperforms standard fusion techniques on IJB-A, in which subjects are described	IJB-A	IJB-A
Fusion↓ IJB-A Ver. (TAR) IJB-A Id. (Rec. Rate) Metrics	IJB-A	IJB-A
fusion tech- niques on the IJB-A benchmark for verification (ROC) and	IJB-A	IJB-A
test time matching methods on IJB-A	IJB-A	IJB-A
5.1 Results on the IJB-A benchmarks	IJB-A	IJB-A
IJB-A is a new publicly available	IJB-A	IJB-A
identification and verification methods. Both IJB-A and the Janus CS2 benchmark	IJB-A	IJB-A
and illu- mination variations, with IJB-A splits generally considered more difficult	IJB-A	IJB-A
than those in CS2. The IJB-A benchmarks consist of face verification	IJB-A	IJB-A
3 IJB-A data and splits are available	IJB-A	IJB-A
Augmentation ↓ IJB-A Ver. (TAR) IJB-A Id. (Rec. Rate) IJB-A Ver	IJB-A	IJB-A
. (TAR) IJB-A Id. (Rec. Rate	IJB-A	IJB-A
Effect of each augmentation on IJB-A performance on verification (ROC) and	IJB-A	IJB-A
In all our IJB-A and Janus CS2 results this	IJB-A	IJB-A
Image type ↓ IJB-A Ver. (TAR) IJB-A Id. (Rec	IJB-A	IJB-A
. Rate) IJB-A Ver. (TAR) IJB-A Id. (Rec. Rate	IJB-A	IJB-A
synthesis at test-time (matching) on IJB-A dataset respectively for verification (ROC	IJB-A	IJB-A
tech- nique on the challenging IJB-A dataset. Clearly, the biggest contribution	IJB-A	IJB-A
achieving state-of-the-art performance on the IJB-A benchmark. We conjecture that this	IJB-A	IJB-A
JANUS CS2 Id. (Rec. Rate) IJB-A Ver. (TAR) IJB-A Id. (Rec	IJB-A	IJB-A
analysis on JANUS CS2 and IJB-A respectively for verification (ROC) and	IJB-A	IJB-A
the art results in the IJB-A benchmark and Janus CS2 dataset	IJB-A	IJB-A
Moreover, our method improves in IJB-A verification over [37] in 15	IJB-A	IJB-A
tested on the LFW and IJB	IJB	IJB-A
proach on the LFW [11], IJB	IJB	IJB-A
outperforms standard fusion techniques on IJB	IJB	IJB-A
Fusion↓ IJB-A Ver. (TAR) IJB	IJB	IJB-A
fusion tech- niques on the IJB	IJB	IJB-A
test time matching methods on IJB	IJB	IJB-A
5.1 Results on the IJB	IJB	IJB-A
IJB	IJB	IJB-A
identification and verification methods. Both IJB	IJB	IJB-A
and illu- mination variations, with IJB	IJB	IJB-A
than those in CS2. The IJB	IJB	IJB-A
3 IJB	IJB	IJB-A
Augmentation ↓ IJB-A Ver. (TAR) IJB	IJB	IJB-A
-A Id. (Rec. Rate) IJB	IJB	IJB-A
-A Ver. (TAR) IJB	IJB	IJB-A
Effect of each augmentation on IJB	IJB	IJB-A
In all our IJB	IJB	IJB-A
Image type ↓ IJB	IJB	IJB-A
-A Ver. (TAR) IJB	IJB	IJB-A
-A Id. (Rec. Rate) IJB	IJB	IJB-A
-A Ver. (TAR) IJB	IJB	IJB-A
synthesis at test-time (matching) on IJB	IJB	IJB-A
tech- nique on the challenging IJB	IJB	IJB-A
achieving state-of-the-art performance on the IJB	IJB	IJB-A
JANUS CS2 Id. (Rec. Rate) IJB	IJB	IJB-A
-A Ver. (TAR) IJB	IJB	IJB-A
analysis on JANUS CS2 and IJB	IJB	IJB-A
the art results in the IJB	IJB	IJB-A
Moreover, our method improves in IJB	IJB	IJB-A
A (verification and identification) benchmarks and	A	IJB-A
A	A	IJB-A
2 I. Masi, A	A	IJB-A
A (verification and identification) and CS2	A	IJB-A
augmenta- tion method (Sec. 2): A domain specific data augmentation approach	A	IJB-A
4 I. Masi, A	A	IJB-A
6 I. Masi, A	A	IJB-A
8 I. Masi, A	A	IJB-A
A, in which subjects are described	A	IJB-A
10 I. Masi, A	A	IJB-A
A Ver. (TAR) IJB-A Id. (Rec	A	IJB-A
A benchmark for verification (ROC) and	A	IJB-A
A	A	IJB-A
A	A	IJB-A
A	A	IJB-A
A benchmarks	A	IJB-A
A is a new publicly available	A	IJB-A
A and the Janus CS2 benchmark	A	IJB-A
A splits generally considered more difficult	A	IJB-A
A benchmarks consist of face verification	A	IJB-A
A data and splits are available	A	IJB-A
12 I. Masi, A	A	IJB-A
A Ver. (TAR) IJB-A Id. (Rec	A	IJB-A
A Ver. (TAR) IJB-A Id. (Rec	A	IJB-A
A performance on verification (ROC) and	A	IJB-A
A and Janus CS2 results this	A	IJB-A
A Ver. (TAR) IJB-A Id. (Rec	A	IJB-A
A Ver. (TAR) IJB-A Id. (Rec	A	IJB-A
A dataset respectively for verification (ROC	A	IJB-A
A dataset. Clearly, the biggest contribution	A	IJB-A
A benchmark. We conjecture that this	A	IJB-A
A Ver. (TAR) IJB-A Id. (Rec	A	IJB-A
A respectively for verification (ROC) and	A	IJB-A
times for each training split. A network trained once with our	A	IJB-A
A benchmark and Janus CS2 dataset	A	IJB-A
A verification over [37] in 15	A	IJB-A
14 I. Masi, A	A	IJB-A
A cc	A	IJB-A
16 I. Masi, A	A	IJB-A
3. K. Chatfield, K. Simonyan, A	A	IJB-A
. Vedaldi, and A	A	IJB-A
8. R. Hartley and A	A	IJB-A
Labeled faces in the wild: A database for studying face recognition	A	IJB-A
Klare, B. Klein, E. Taborsky, A	A	IJB-A
Cheney, K. Allen, P. Grother, A	A	IJB-A
. Mah, M. Burge, and A	A	IJB-A
and recognition: IARPA Janus Benchmark A	A	IJB-A
E. Taborsky, M. Burge, and A	A	IJB-A
16. A	A	IJB-A
S. Lawrence, C. L. Giles, A	A	IJB-A
. C. Tsoi, and A	A	IJB-A
. D. Back. Face recognition: A con- volutional neural-network approach. Trans	A	IJB-A
M. H. Nguyen, J.-F. Lalonde, A	A	IJB-A
. A	A	IJB-A
O. M. Parkhi, K. Simonyan, A	A	IJB-A
. Vedaldi, and A. Zisserman. A compact and discrim- inative face	A	IJB-A
24. O. M. Parkhi, A	A	IJB-A
. Vedaldi, and A	A	IJB-A
S. Romdhani, and T. Vetter. A 3d face model for pose	A	IJB-A
Satheesh, S. Ma, Z. Huang, A	A	IJB-A
. Karpa- thy, A	A	IJB-A
28. S. Sankaranarayanan, A	A	IJB-A
Kalenichenko, and J. Philbin. Facenet: A unified embedding for face recognition	A	IJB-A
30. K. Simonyan and A	A	IJB-A
D. Wang, C. Otto, and A	A	IJB-A
18 I. Masi, A	A	IJB-A
released IARPA Janus Benchmark A (IJB-A) dataset as well as on	IJB-A	IJB-A
the Wild (LFW) dataset. The IJB-A dataset includes real- world unconstrained	IJB-A	IJB-A
of experimental evaluations on the IJB-A and the LFW datasets are	IJB-A	IJB-A
face matchers on the challenging IJB-A dataset which con- tains significant	IJB-A	IJB-A
on the CASIA-WebFace, and the IJB-A datasets to local- ize and	IJB-A	IJB-A
the training sets of the IJB-A dataset and the DCNN features	IJB-A	IJB-A
traditional EM is that for IJB-A training and test data, some	IJB-A	IJB-A
image. More details about the IJB-A dataset are given in Section	IJB-A	IJB-A
27 overlapping subjects with the IJB-A dataset, there are 10548 subjects	IJB-A	IJB-A
challenging IARPA Janus Benchmark A (IJB-A) [20], its extended version Janus	IJB-A	IJB-A
and im- ages in the IJB-A but also the original videos	IJB-A	IJB-A
the defined protocols than the IJB-A dataset. The receiver operating character	IJB-A	IJB-A
4.1. JANUS-CS2 and IJB-A	IJB-A	IJB-A
Both the IJB-A and JANUS CS2 contain 500	IJB-A	IJB-A
the JANUS CS2 dataset. The IJB-A evaluation protocol consists of verification	IJB-A	IJB-A
The main differ- ences between IJB-A and JANUS CS2 evaluation protocol	IJB-A	IJB-A
are (1) IJB-A considers the open-set identification problem	IJB-A	IJB-A
the closed-set identification and (2) IJB-A considers the more difficult pairs	IJB-A	IJB-A
images and frames from the IJB-A and JANUS CS2 datasets. A	IJB-A	IJB-A
Both the IJB-A and the JANUS CS2 datasets	IJB-A	IJB-A
uate the verification performance, the IJB-A and JANUS CS2 both divide	IJB-A	IJB-A
34], the images in the IJB-A and JANUS CS2 contain extreme	IJB-A	IJB-A
These factors essentially make the IJB-A and JANUS CS2 challenging face	IJB-A	IJB-A
4.2. Evaluation on JANUS-CS2 and IJB-A	IJB-A	IJB-A
results for JANUS CS2 and IJB-A datasets obtained after the paper	IJB-A	IJB-A
and evaluates it on the IJB-A dataset. The method proposed in	IJB-A	IJB-A
in [8] previously for the IJB-A iden- tification task because one	IJB-A	IJB-A
time. The current metadata of IJB-A has fixed those errors already	IJB-A	IJB-A
Figure 5. Results on the IJB-A dataset. (a) the average ROC	IJB-A	IJB-A
curves for the IJB-A verification protocol and (b) the	IJB-A	IJB-A
average CMC curves for IJB-A identification protocol over 10 splits	IJB-A	IJB-A
IJB-A	IJB-A	IJB-A
IJB-A	IJB-A	IJB-A
Table 3. Results on the IJB-A dataset. The TAR of all	IJB-A	IJB-A
the proposed DCNN on the IJB-A dataset is much better than	IJB-A	IJB-A
negative pairs from CASIA-Webface and IJB-A training datasets to fully utilize	IJB-A	IJB-A
4.1 . JANUS-CS2 and IJB-A	IJB-A	IJB-A
Evaluation on JANUS-CS2 and IJB-A	IJB-A	IJB-A
released IARPA Janus Benchmark A (IJB	IJB	IJB-A
the Wild (LFW) dataset. The IJB	IJB	IJB-A
of experimental evaluations on the IJB	IJB	IJB-A
face matchers on the challenging IJB	IJB	IJB-A
on the CASIA-WebFace, and the IJB	IJB	IJB-A
the training sets of the IJB	IJB	IJB-A
traditional EM is that for IJB	IJB	IJB-A
image. More details about the IJB	IJB	IJB-A
27 overlapping subjects with the IJB	IJB	IJB-A
challenging IARPA Janus Benchmark A (IJB	IJB	IJB-A
and im- ages in the IJB	IJB	IJB-A
the defined protocols than the IJB	IJB	IJB-A
4.1. JANUS-CS2 and IJB	IJB	IJB-A
Both the IJB	IJB	IJB-A
the JANUS CS2 dataset. The IJB	IJB	IJB-A
The main differ- ences between IJB	IJB	IJB-A
CS2 evaluation protocol are (1) IJB	IJB	IJB-A
the closed-set identification and (2) IJB	IJB	IJB-A
images and frames from the IJB	IJB	IJB-A
Both the IJB	IJB	IJB-A
uate the verification performance, the IJB	IJB	IJB-A
34], the images in the IJB	IJB	IJB-A
These factors essentially make the IJB	IJB	IJB-A
4.2. Evaluation on JANUS-CS2 and IJB	IJB	IJB-A
results for JANUS CS2 and IJB	IJB	IJB-A
and evaluates it on the IJB	IJB	IJB-A
in [8] previously for the IJB	IJB	IJB-A
time. The current metadata of IJB	IJB	IJB-A
Figure 5. Results on the IJB	IJB	IJB-A
average ROC curves for the IJB	IJB	IJB-A
the average CMC curves for IJB	IJB	IJB-A
IJB	IJB	IJB-A
IJB	IJB	IJB-A
Table 3. Results on the IJB	IJB	IJB-A
the proposed DCNN on the IJB	IJB	IJB-A
negative pairs from CASIA-Webface and IJB	IJB	IJB-A
4.1 . JANUS-CS2 and IJB	IJB	IJB-A
Evaluation on JANUS-CS2 and IJB	IJB	IJB-A
newly released IARPA Janus Benchmark A (IJB-A) dataset as well as	A	IJB-A
A dataset includes real- world unconstrained	A	IJB-A
A and the LFW datasets are	A	IJB-A
A dataset which con- tains significant	A	IJB-A
A datasets to local- ize and	A	IJB-A
A dataset and the DCNN features	A	IJB-A
A DCNN with small filters and	A	IJB-A
A training and test data, some	A	IJB-A
A dataset are given in Section	A	IJB-A
A dataset, there are 10548 subjects	A	IJB-A
the challenging IARPA Janus Benchmark A (IJB-A) [20], its extended version	A	IJB-A
A but also the original videos	A	IJB-A
A dataset. The receiver operating character	A	IJB-A
A	A	IJB-A
A and JANUS CS2 contain 500	A	IJB-A
A evaluation protocol consists of verification	A	IJB-A
A and JANUS CS2 evaluation protocol	A	IJB-A
A considers the open-set identification problem	A	IJB-A
A considers the more difficult pairs	A	IJB-A
IJB-A and JANUS CS2 datasets. A variety of challenging variations on	A	IJB-A
A and the JANUS CS2 datasets	A	IJB-A
A and JANUS CS2 both divide	A	IJB-A
A and JANUS CS2 contain extreme	A	IJB-A
A and JANUS CS2 challenging face	A	IJB-A
A	A	IJB-A
A datasets obtained after the paper	A	IJB-A
A dataset. The method proposed in	A	IJB-A
A iden- tification task because one	A	IJB-A
A has fixed those errors already	A	IJB-A
A	A	IJB-A
A R	A	IJB-A
A	A	IJB-A
A R	A	IJB-A
A dataset. (a) the average ROC	A	IJB-A
A verification protocol and (b) the	A	IJB-A
A identification protocol over 10 splits	A	IJB-A
A	A	IJB-A
A	A	IJB-A
A 0.884±0.025 0.934±0.016 0.954±0.007 0.974±0.005 0.977±0.007	A	IJB-A
A dataset. The TAR of all	A	IJB-A
A N/A N/A 99.20% Ours 1	A	IJB-A
face verification dataset, IARPA Benchmark A, which contains faces with full	A	IJB-A
A dataset is much better than	A	IJB-A
A training datasets to fully utilize	A	IJB-A
References [1] T. Ahonen, A	A	IJB-A
2] A	A	IJB-A
3] A	A	IJB-A
Q. Duan, and J. Sun. A practical transfer learning algorithm for	A	IJB-A
J. Sun. Bayesian face revisited: A joint formulation. In European Conference	A	IJB-A
Tzeng, and T. Darrell. Decaf: A deep convolutional acti- vation feature	A	IJB-A
Klare, B. Klein, E. Taborsky, A	A	IJB-A
Cheney, K. Allen, P. Grother, A	A	IJB-A
. Mah, M. Burge, and A	A	IJB-A
and recognition: IARPA Janus Benchmark A	A	IJB-A
21] A	A	IJB-A
O. M. Parkhi, K. Simonyan, A	A	IJB-A
. Vedaldi, and A. Zisserman. A compact and discriminative face track	A	IJB-A
Kalenichenko, and J. Philbin. Facenet: A uni- fied embedding for face	A	IJB-A
K. Simonyan, O. M. Parkhi, A	A	IJB-A
. Vedaldi, and A	A	IJB-A
27] K. Simonyan and A	A	IJB-A
D. Erhan, V. Vanhoucke, and A	A	IJB-A
Y. Taigman, M. Yang, M. A	A	IJB-A
D. Wang, C. Otto, and A	A	IJB-A
Chellappa, P. J. Phillips, and A	A	IJB-A
. Rosenfeld. Face recognition: A literature survey. ACM Computing Sur	A	IJB-A
A	A	IJB-A
A	A	IJB-A
on IARPA’s CS2 and NIST’s IJB-A in both verification and identification	IJB-A	IJB-A
Technology’s (NIST) IARPA Janus Benchmark-A (IJB-A), which is considered much more	IJB-A	IJB-A
IJB-A dataset has been quickly adopted	IJB-A	IJB-A
novel approach is applied to IJB-A dataset and is shown to	IJB-A	IJB-A
of the same sub- ject. IJB-A and CS2 share 90% of	IJB-A	IJB-A
in evaluation protocols. In particular, IJB-A in- cludes protocols for both	IJB-A	IJB-A
As mentioned before, since the IJB-A dataset uses the no- tion	IJB-A	IJB-A
preprocessed images. Testing a single IJB-A	IJB-A	IJB-A
20 minutes for a single IJB-A	IJB-A	IJB-A
IJB-A dataset contains two types of	IJB-A	IJB-A
accuracy between two templates. However, IJB-A carefully designed challenging template pairs	IJB-A	IJB-A
COTS from [11] on the IJB-A dataset. It is clear that	IJB-A	IJB-A
involves both fine tuning on IJB-A data and uses metric learning	IJB-A	IJB-A
using labeled IJB-A training data, while our algorithm	IJB-A	IJB-A
box on both CS2 and IJB-A without target domain specific tuning	IJB-A	IJB-A
Table 8: 1:N Results on IJB-A	IJB-A	IJB-A
face recognition per- formance on IJB-A benchmark compared not only to	IJB-A	IJB-A
on IARPA’s CS2 and NIST’s IJB	IJB	IJB-A
Technology’s (NIST) IARPA Janus Benchmark-A (IJB	IJB	IJB-A
IJB	IJB	IJB-A
novel approach is applied to IJB	IJB	IJB-A
22] for training and both IJB	IJB	IJB-A
of the same sub- ject. IJB	IJB	IJB-A
in evaluation protocols. In particular, IJB	IJB	IJB-A
As mentioned before, since the IJB	IJB	IJB-A
preprocessed images. Testing a single IJB	IJB	IJB-A
20 minutes for a single IJB	IJB	IJB-A
IJB	IJB	IJB-A
accuracy between two templates. However, IJB	IJB	IJB-A
COTS from [11] on the IJB	IJB	IJB-A
involves both fine tuning on IJB	IJB	IJB-A
uses metric learning using labeled IJB	IJB	IJB-A
box on both CS2 and IJB	IJB	IJB-A
Table 8: 1:N Results on IJB	IJB	IJB-A
face recognition per- formance on IJB	IJB	IJB-A
A in both verification and identification	A	IJB-A
A (IJB-A), which is considered much	A	IJB-A
A dataset has been quickly adopted	A	IJB-A
A dataset and is shown to	A	IJB-A
for training and both IJB- A [11] and IARPA’s Janus CS2	A	IJB-A
A and CS2 share 90% of	A	IJB-A
A in- cludes protocols for both	A	IJB-A
A facial landmark detection algorithm, lmd	A	IJB-A
A dataset uses the no- tion	A	IJB-A
A	A	IJB-A
A	A	IJB-A
A dataset contains two types of	A	IJB-A
A carefully designed challenging template pairs	A	IJB-A
A dataset. It is clear that	A	IJB-A
A data and uses metric learning	A	IJB-A
A training data, while our algorithm	A	IJB-A
A without target domain specific tuning	A	IJB-A
A	A	IJB-A
A benchmark compared not only to	A	IJB-A
References [1] T. Ahonen, A	A	IJB-A
Labeled faces in the wild: A database for studying face recognition	A	IJB-A
B. F. Klare, E. Taborsky, A	A	IJB-A
Cheney, K. Allen, P. Grother, A	A	IJB-A
. Mah, M. Burge, and A	A	IJB-A
12] A	A	IJB-A
15] A	A	IJB-A
18] K. Simonyan and A	A	IJB-A
D. Wang, C. Otto, and A	A	IJB-A
face recognition benchmarks (LFW and IJB-A	IJB-A	IJB-A
the BLUFR protocol. For the IJB-A benchmark, our accuracies are as	IJB-A	IJB-A
Viola-Jones face detector), and the IJB-A dataset (includes faces which are	IJB-A	IJB-A
on the LFW [3] and IJB-A [9] face datasets with an	IJB-A	IJB-A
of unconstrained face recognition, the IJB-A dataset was released in 2015	IJB-A	IJB-A
9]. IJB-A contains face images that are	IJB-A	IJB-A
828 4, 500 80M+ N/A IJB-A [9] + LFW [3	IJB-A	IJB-A
PCSO (b) LFW [3] (c) IJB-A [9] (d) CASIA [6] (e	IJB-A	IJB-A
our experiments: PCSO, LFW [3], IJB-A [9], CASIA-WebFace [6] (abbreviated as	IJB-A	IJB-A
IJB-A [9] IARPA Janus Benchmark-A (IJB-A) contains 500 subjects with a	IJB-A	IJB-A
to the LFW dataset, the IJB-A dataset is more challenging due	IJB-A	IJB-A
locations are provided with the IJB-A dataset (and used in our	IJB-A	IJB-A
two different subjects in the IJB-A dataset, captured in various conditions	IJB-A	IJB-A
recognition benchmarks (LFW [3] and IJB-A [9]) to establish its performance	IJB-A	IJB-A
5.3 IJB-A Evaluation	IJB-A	IJB-A
The IJB-A dataset [9] was released in	IJB-A	IJB-A
in the LFW protocols, the IJB-A dataset contains more challenging face	IJB-A	IJB-A
in the first folder of IJB-A protocol in 1:N face search	IJB-A	IJB-A
One unique aspect of the IJB-A evaluation protocol is that it	IJB-A	IJB-A
examples of templates in the IJB-A protocol (one per row), with	IJB-A	IJB-A
template. In particular, in the IJB-A evaluation protocol the number of	IJB-A	IJB-A
The verification protocol in IJB-A consists of 10 sets of	IJB-A	IJB-A
or video frame from the IJB-A dataset, we first attempt to	IJB-A	IJB-A
ground-truth landmarks provided with the IJB-A protocol. All possible ground truth	IJB-A	IJB-A
of web images in the IJB-A dataset with overlayed landmarks (top	IJB-A	IJB-A
0 ground-truth landmarks provided in IJB-A, respectively. DLIB fails to output	IJB-A	IJB-A
these two categories in the IJB-A dataset	IJB-A	IJB-A
The IJB-A protocol allows participants to perform	IJB-A	IJB-A
for each fold. Since the IJB-A dataset is qualitatively different from	IJB-A	IJB-A
our deep model using the IJB-A training set. The final face	IJB-A	IJB-A
and one re-trained (on the IJB-A training set following the protocol	IJB-A	IJB-A
Since all the IJB-A comparisons are defined between sets	IJB-A	IJB-A
in 1:N search protocol of IJB-A, averaged over 10 folds. Correct	IJB-A	IJB-A
we use include LFW and IJB-A data, but now we do	IJB-A	IJB-A
closed-set 1:N search protocol of IJB-A	IJB-A	IJB-A
in first fold of the IJB-A closed-set 1:N search protocol, using	IJB-A	IJB-A
5 Recognition accuracies under the IJB-A protocol. Results for GOTS and	IJB-A	IJB-A
10 folds specified in the IJB-A protocol	IJB-A	IJB-A
We use the LFW and IJB-A datasets to construct the genuine	IJB-A	IJB-A
a single image. For the IJB-A dataset, a similar process is	IJB-A	IJB-A
IJB-A based probe and mate sets	IJB-A	IJB-A
Genuine Probe Set IJB-A [3] 500 10,868 Mate Set	IJB-A	IJB-A
IJB-A [3] 500 10,626	IJB-A	IJB-A
results for the LFW and IJB-A datasets are shown in Figs	IJB-A	IJB-A
Search Evaluation on LFW and IJB-A datasets	IJB-A	IJB-A
N (log-scale), on LFW and IJB-A datasets. The performance of COTS	IJB-A	IJB-A
For both LFW and IJB-A face images, the recognition performance	IJB-A	IJB-A
method individually. Results on the IJB-A dataset are overall similar to	IJB-A	IJB-A
the overall performance on the IJB-A dataset is much lower than	IJB-A	IJB-A
given the nature of the IJB-A dataset	IJB-A	IJB-A
For both the LFW and IJB-A datasets, the open-set face search	IJB-A	IJB-A
Search Evaluation on LFW and IJB-A datasets	IJB-A	IJB-A
rate (FAR) on LFW and IJB-A datasets, using an 80M face	IJB-A	IJB-A
unconstrained face dataset, and the IJB-A dataset. On the mugshot data	IJB-A	IJB-A
reported in [9] on the IJB-A dataset, as follows: TAR of	IJB-A	IJB-A
on the LFW and the IJB-A benchmarks, we evaluate the proposed	IJB-A	IJB-A
face recognition benchmarks (LFW and IJB	IJB	IJB-A
the BLUFR protocol. For the IJB	IJB	IJB-A
Viola-Jones face detector), and the IJB	IJB	IJB-A
on the LFW [3] and IJB	IJB	IJB-A
domain datasets LFW [3] and IJB	IJB	IJB-A
of unconstrained face recognition, the IJB	IJB	IJB-A
was released in 2015 [9]. IJB	IJB	IJB-A
828 4, 500 80M+ N/A IJB	IJB	IJB-A
PCSO (b) LFW [3] (c) IJB	IJB	IJB-A
our experiments: PCSO, LFW [3], IJB	IJB	IJB-A
IJB-A [9] IARPA Janus Benchmark-A (IJB	IJB	IJB-A
to the LFW dataset, the IJB	IJB	IJB-A
locations are provided with the IJB	IJB	IJB-A
two different subjects in the IJB	IJB	IJB-A
recognition benchmarks (LFW [3] and IJB	IJB	IJB-A
5.3 IJB	IJB	IJB-A
The IJB	IJB	IJB-A
in the LFW protocols, the IJB	IJB	IJB-A
in the first folder of IJB	IJB	IJB-A
One unique aspect of the IJB	IJB	IJB-A
examples of templates in the IJB	IJB	IJB-A
template. In particular, in the IJB	IJB	IJB-A
The verification protocol in IJB	IJB	IJB-A
or video frame from the IJB	IJB	IJB-A
ground-truth landmarks provided with the IJB	IJB	IJB-A
of web images in the IJB	IJB	IJB-A
0 ground-truth landmarks provided in IJB	IJB	IJB-A
these two categories in the IJB	IJB	IJB-A
The IJB	IJB	IJB-A
for each fold. Since the IJB	IJB	IJB-A
our deep model using the IJB	IJB	IJB-A
and one re-trained (on the IJB	IJB	IJB-A
Since all the IJB	IJB	IJB-A
in 1:N search protocol of IJB	IJB	IJB-A
we use include LFW and IJB	IJB	IJB-A
closed-set 1:N search protocol of IJB	IJB	IJB-A
in first fold of the IJB	IJB	IJB-A
5 Recognition accuracies under the IJB	IJB	IJB-A
10 folds specified in the IJB	IJB	IJB-A
We use the LFW and IJB	IJB	IJB-A
a single image. For the IJB	IJB	IJB-A
IJB	IJB	IJB-A
mate sets Genuine Probe Set IJB	IJB	IJB-A
3] 500 10,868 Mate Set IJB	IJB	IJB-A
results for the LFW and IJB	IJB	IJB-A
Deep Features (IJB−A) COTS (IJB	IJB	IJB-A
−A) DF → COTS (IJB	IJB	IJB-A
Search Evaluation on LFW and IJB	IJB	IJB-A
N (log-scale), on LFW and IJB	IJB	IJB-A
For both LFW and IJB	IJB	IJB-A
method individually. Results on the IJB	IJB	IJB-A
the overall performance on the IJB	IJB	IJB-A
given the nature of the IJB	IJB	IJB-A
For both the LFW and IJB	IJB	IJB-A
COTS (LFW) Deep Features (IJB	IJB	IJB-A
−A) DF → COTS (IJB	IJB	IJB-A
Search Evaluation on LFW and IJB	IJB	IJB-A
rate (FAR) on LFW and IJB	IJB	IJB-A
unconstrained face dataset, and the IJB	IJB	IJB-A
reported in [9] on the IJB	IJB	IJB-A
on the LFW and the IJB	IJB	IJB-A
A	A	IJB-A
A benchmark, our accuracies are as	A	IJB-A
scheme in the feature space. A vast majority of face recognition	A	IJB-A
A large-scale face search system, leveraging	A	IJB-A
A dataset (includes faces which are	A	IJB-A
A [9] face datasets with an	A	IJB-A
datasets LFW [3] and IJB- A [9	A	IJB-A
20K individuals (outside the protocol). A comparable result (99.63%) was achieved	A	IJB-A
A dataset was released in 2015	A	IJB-A
A contains face images that are	A	IJB-A
A search operating in open-set protocol	A	IJB-A
TABLE 1 A summary of face search systems	A	IJB-A
A 1M+ N/A LFW [3	A	IJB-A
A FaceScrub [12] + Yahoo Imagesb	A	IJB-A
A 201, 196 N/A FERET [14	A	IJB-A
A 116, 028 N/A FRGC [16	A	IJB-A
A LFW [3] + Web Faces	A	IJB-A
A IJB-A [9] + LFW [3	A	IJB-A
Fig. 4. A face image alignment example. The	A	IJB-A
m A	A	IJB-A
A [9] (d) CASIA [6] (e	A	IJB-A
A ve	A	IJB-A
A [9], CASIA-WebFace [6] (abbreviated as	A	IJB-A
A [9] IARPA Janus Benchmark-A (IJB-A	A	IJB-A
A dataset is more challenging due	A	IJB-A
A dataset (and used in our	A	IJB-A
A dataset, captured in various conditions	A	IJB-A
the Viola-Jones face detector [40]. A total of 80 million face	A	IJB-A
A [9]) to establish its performance	A	IJB-A
A N/A N/A 99.20% COTS N/A	A	IJB-A
A N/A 90.35%±1.30% Proposed Deep Model	A	IJB-A
rates ( FAR = 0.1%). A number of new protocols for	A	IJB-A
A 58.56% 36.44% Proposed Deep Model	A	IJB-A
A Evaluation	A	IJB-A
A dataset [9] was released in	A	IJB-A
A dataset contains more challenging face	A	IJB-A
A protocol in 1:N face search	A	IJB-A
A evaluation protocol is that it	A	IJB-A
A protocol (one per row), with	A	IJB-A
A evaluation protocol the number of	A	IJB-A
A consists of 10 sets of	A	IJB-A
A dataset, we first attempt to	A	IJB-A
A protocol. All possible ground truth	A	IJB-A
A dataset with overlayed landmarks (top	A	IJB-A
A, respectively. DLIB fails to output	A	IJB-A
A dataset	A	IJB-A
A protocol allows participants to perform	A	IJB-A
A dataset is qualitatively different from	A	IJB-A
A training set. The final face	A	IJB-A
A training set following the protocol	A	IJB-A
A comparisons are defined between sets	A	IJB-A
A, averaged over 10 folds. Correct	A	IJB-A
for the third probe template. A template containing a single poorly-aligned	A	IJB-A
A data, but now we do	A	IJB-A
A	A	IJB-A
A closed-set 1:N search protocol, using	A	IJB-A
A protocol. Results for GOTS and	A	IJB-A
A protocol	A	IJB-A
A datasets to construct the genuine	A	IJB-A
A dataset, a similar process is	A	IJB-A
A based probe and mate sets	A	IJB-A
A [3] 500 10,868 Mate Set	A	IJB-A
A [3] 500 10,626	A	IJB-A
A 80,000,000	A	IJB-A
is computed with Eq. 4. A basic assumption in our search	A	IJB-A
A datasets are shown in Figs	A	IJB-A
m A	A	IJB-A
A) COTS (IJB−A) DF → COTS	A	IJB-A
A	A	IJB-A
A datasets	A	IJB-A
A datasets. The performance of COTS	A	IJB-A
A face images, the recognition performance	A	IJB-A
A dataset are overall similar to	A	IJB-A
A dataset is much lower than	A	IJB-A
A dataset	A	IJB-A
A datasets, the open-set face search	A	IJB-A
A ve	A	IJB-A
A) DF → COTS (IJB−A	A	IJB-A
A datasets	A	IJB-A
A datasets, using an 80M face	A	IJB-A
A 0.34 0.4	A	IJB-A
A dataset. On the mugshot data	A	IJB-A
A dataset, as follows: TAR of	A	IJB-A
A benchmarks, we evaluate the proposed	A	IJB-A
Labeled faces in the wild: A database for studying face recognition	A	IJB-A
4] D. Wang and A	A	IJB-A
J. Sun, “Bayesian face revisited: A joint formulation,” ECCV, 2012. 2	A	IJB-A
Klare, B. Klein, E. Taborsky, A	A	IJB-A
Cheney, K. Allen, P. Grother, A	A	IJB-A
. Mash, M. Burge, and A	A	IJB-A
10] N. Kumar, A	A	IJB-A
. C. Berg, A	A	IJB-A
and S. M. Seitz, “Megaface: A million faces for recognition at	A	IJB-A
A data-driven approach to cleaning large	A	IJB-A
17] B. Klare, A	A	IJB-A
Otto, B. F. Klare, and A	A	IJB-A
A benchmark study of large-scale unconstrained	A	IJB-A
20] A	A	IJB-A
Kalenichenko, and J. Philbin, “Facenet: A unified	A	IJB-A
A survey of content-based	A	IJB-A
learning for content-based image retrieval: A comprehensive study,” ACM MM, 2014	A	IJB-A
28] K. Simonyan and A	A	IJB-A
30] A	A	IJB-A
31] N. Srivastava, G. Hinton, A	A	IJB-A
and R. Salakhut- dinov, “Dropout: A simple way to prevent neural	A	IJB-A
34] A	A	IJB-A
. Jain, K. Nandakumar, and A	A	IJB-A
Y. Taigman, M. Yang, M. A	A	IJB-A
42] J. Klontz and A	A	IJB-A
A case study of automated face	A	IJB-A
exceeds the state-of-the-art on the IJB-B face recognition dataset. This is	IJB-B	IJB-B
pub- lic IJB-A [20] and IJB-B [37] face recognition benchmarks. These	IJB-B	IJB-B
recognition datasets IJB-A [20] and IJB-B [37] are used for evaluation	IJB-B	IJB-B
10, 18, 25], IJB-A and IJB-B are intended for template-based face	IJB-B	IJB-B
and 4.2 videos, respectively. The IJB-B dataset is an extension of	IJB-B	IJB-B
benchmark procedure for IJB-A and IJB-B, and evaluate on “1:1 face	IJB-B	IJB-B
defines 10 test splits, while IJB-B only has one split for	IJB-B	IJB-B
identification. For IJB-A and for IJB-B iden- tification, we report, as	IJB-B	IJB-B
deployment. In the IJB-A and IJB-B datasets, there are images and	IJB-B	IJB-B
4.3 Ablation studies on IJB-B	IJB-B	IJB-B
it to baselines on the IJB-B dataset, as it is larger	IJB-B	IJB-B
1: Verification performance on the IJB-B dataset. A higher value of	IJB-B	IJB-B
2: Identification performance on the IJB-B dataset. A higher value of	IJB-B	IJB-B
state-of-the-art on the IJB-A and IJB-B datasets. The currently best performing	IJB-B	IJB-B
verification on both IJB-A and IJB-B datasets. In particular, it surpasses	IJB-B	IJB-B
measures on the more challenging IJB-B benchmark, our network achieves the	IJB-B	IJB-B
IJB-B	IJB-B	IJB-B
verification on the IJB-A and IJB-B datasets. A higher value of	IJB-B	IJB-B
vs 0.705 for verification on IJB-B, and TPIR at FPIR=0.01 of	IJB-B	IJB-B
0.743 for iden- tification on IJB-B	IJB-B	IJB-B
vs 0.671 for verification on IJB-B, and TPIR at FPIR=0.01 of	IJB-B	IJB-B
vs 0.706 for verification on IJB-B	IJB-B	IJB-B
IJB-B	IJB-B	IJB-B
identification on the IJB-A and IJB-B datasets. A higher value of	IJB-B	IJB-B
Fig. 3: Results on the IJB-B dataset. Our SE-GV-4-g1 network which	IJB-B	IJB-B
on the challenging IJB-A and IJB-B benchmarks by a large margin	IJB-B	IJB-B
a tem- plate in the IJB-B dataset. The contribution (relative to	IJB-B	IJB-B
B face recognition dataset. This is	B	IJB-B
B [37] face recognition benchmarks. These	B	IJB-B
B [37] are used for evaluation	B	IJB-B
B are intended for template-based face	B	IJB-B
B dataset is an extension of	B	IJB-B
B, and evaluate on “1:1 face	B	IJB-B
B only has one split for	B	IJB-B
B iden- tification, we report, as	B	IJB-B
B datasets, there are images and	B	IJB-B
B	B	IJB-B
B dataset, as it is larger	B	IJB-B
B dataset. A higher value of	B	IJB-B
B dataset. A higher value of	B	IJB-B
B datasets. The currently best performing	B	IJB-B
B datasets. In particular, it surpasses	B	IJB-B
B benchmark, our network achieves the	B	IJB-B
B	B	IJB-B
B datasets. A higher value of	B	IJB-B
B, and TPIR at FPIR=0.01 of	B	IJB-B
B	B	IJB-B
B, and TPIR at FPIR=0.01 of	B	IJB-B
B	B	IJB-B
B	B	IJB-B
B datasets. A higher value of	B	IJB-B
B dataset. Our SE-GV-4-g1 network which	B	IJB-B
B benchmarks by a large margin	B	IJB-B
B dataset. The contribution (relative to	B	IJB-B
6] Cevikalp, H., Triggs, B	B	IJB-B
20] Klare, B., Klein, B	B	IJB-B
Yu, Z., Li, M., Raj, B	B	IJB-B
Taborsky, E., Blanton, A., Maze, B	B	IJB-B
B face dataset. In: CVPR Workshop	B	IJB-B
exceeds the state-of-the-art on the IJB	IJB	IJB-B
margin on the pub- lic IJB	IJB	IJB-B
-A [20] and IJB	IJB	IJB-B
challenging public face recognition datasets IJB	IJB	IJB-B
-A [20] and IJB	IJB	IJB-B
as [5, 10, 18, 25], IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
consider in this work. The IJB	IJB	IJB-B
and 4.2 videos, respectively. The IJB	IJB	IJB-B
dataset is an extension of IJB	IJB	IJB-B
the standard benchmark procedure for IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
apart from the fact that IJB	IJB	IJB-B
defines 10 test splits, while IJB	IJB	IJB-B
two galleries for identification. For IJB	IJB	IJB-B
-A and for IJB	IJB	IJB-B
Network deployment. In the IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
4.3 Ablation studies on IJB	IJB	IJB-B
it to baselines on the IJB	IJB	IJB-B
larger and more challenging than IJB	IJB	IJB-B
1: Verification performance on the IJB	IJB	IJB-B
2: Identification performance on the IJB	IJB	IJB-B
against the state-of-the-art on the IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
identification and verification on both IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
surpasses [44] marginally on the IJB	IJB	IJB-B
to Rank-10 for identification on IJB	IJB	IJB-B
-A; but this is because IJB	IJB	IJB-B
measures on the more challenging IJB	IJB	IJB-B
IJB	IJB	IJB-B
IJB	IJB	IJB-B
state-of-the-art for verification on the IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
vs 0.705 for verification on IJB	IJB	IJB-B
0.743 for iden- tification on IJB	IJB	IJB-B
vs 0.671 for verification on IJB	IJB	IJB-B
vs 0.706 for verification on IJB	IJB	IJB-B
IJB	IJB	IJB-B
IJB	IJB	IJB-B
state-of-the-art for identification on the IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
Fig. 3: Results on the IJB	IJB	IJB-B
the state-of-the-art on the challenging IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
a tem- plate in the IJB	IJB	IJB-B
two types of AFLW splits, AFLW-Full and AFLW-Frontal following [73]. AFLW-Full	AFLW-Full	AFLW-Full
same training samples as in AFLW-Full, but only use the 1165	AFLW-Full	AFLW-Full
CCL [73] Two-Stage [31] SAN AFLW-Full 4.05 4.35 4.25 3.92 2.72	AFLW-Full	AFLW-Full
by more than 11% on AFLW-Full	AFLW-Full	AFLW-Full
Full and AFLW-Frontal following [73]. AFLW-Full	Full	AFLW-Full
Full, but only use the 1165	Full	AFLW-Full
Method Common Challenging Full Set SDM [64] 5.57 15.40	Full	AFLW-Full
Full 4.05 4.35 4.25 3.92 2.72	Full	AFLW-Full
Full	Full	AFLW-Full
achieve 79.82 AUC@0.08 on AFLW- Full by only using the original	Full	AFLW-Full
algorithms on bench- mark datasets AFLW and 300-W. Code is publicly	AFLW	AFLW-Full
300W-Styles (≈ 12000 images) and AFLW	AFLW	AFLW-Full
transferring the 300-W [46] and AFLW [23] into dif- ferent styles	AFLW	AFLW-Full
AFLW [23]. This dataset contains 21997	AFLW	AFLW-Full
excluding invisible landmark. Faces in AFLW usually have different pose, ex	AFLW	AFLW-Full
There are two types of AFLW splits, AFLW-Full and AFLW-Frontal following	AFLW	AFLW-Full
73]. AFLW	AFLW	AFLW-Full
samples and 4386 testing samples. AFLW	AFLW	AFLW-Full
same training samples as in AFLW	AFLW	AFLW-Full
46, 31, 7, 43]. For AFLW dataset, we use the face	AFLW	AFLW-Full
CCL [73] Two-Stage [31] SAN AFLW	AFLW	AFLW-Full
AFLW	AFLW	AFLW-Full
normalized mean (NME) errors on AFLW dataset	AFLW	AFLW-Full
Results on AFLW	AFLW	AFLW-Full
shows the performance comparison on AFLW	AFLW	AFLW-Full
by more than 11% on AFLW	AFLW	AFLW-Full
-Full. On the AFLW	AFLW	AFLW-Full
mark datasets, e.g., 300-W and AFLW	AFLW	AFLW-Full
300W-Style AFLW	AFLW	AFLW-Full
datasets based on 300-W and AFLW with the original and three	AFLW	AFLW-Full
new datasets, 300W-Style and AFLW	AFLW	AFLW-Full
from 300-W for our 300W-Style. AFLW	AFLW	AFLW-Full
as 300W-Style, which transfer the AFLW dataset into three different styles	AFLW	AFLW-Full
using the training set of AFLW	AFLW	AFLW-Full
and the testing set of AFLW	AFLW	AFLW-Full
can achieve 79.82 AUC@0.08 on AFLW	AFLW	AFLW-Full
by only using the original AFLW training set, while the data	AFLW	AFLW-Full
state-of-the-art performance on 300-W and AFLW datasets	AFLW	AFLW-Full
39] J. W	W	300W
margin on the pub- lic IJB-A [20] and IJB-B [37] face	IJB-A	IJB-A
challenging public face recognition datasets IJB-A [20] and IJB-B [37] are	IJB-A	IJB-A
as [5, 10, 18, 25], IJB-A and IJB-B are intended for	IJB-A	IJB-A
consider in this work. The IJB-A dataset contains 5,712 images and	IJB-A	IJB-A
dataset is an extension of IJB-A with a total of 11,754	IJB-A	IJB-A
the standard benchmark procedure for IJB-A and IJB-B, and evaluate on	IJB-A	IJB-A
apart from the fact that IJB-A defines 10 test splits, while	IJB-A	IJB-A
two galleries for identification. For IJB-A and for IJB-B iden- tification	IJB-A	IJB-A
Network deployment. In the IJB-A and IJB-B datasets, there are	IJB-A	IJB-A
against the state-of-the-art on the IJB-A and IJB-B datasets. The currently	IJB-A	IJB-A
identification and verification on both IJB-A and IJB-B datasets. In particular	IJB-A	IJB-A
surpasses [44] marginally on the IJB-A verification task, despite the fact	IJB-A	IJB-A
to Rank-10 for identification on IJB-A	IJB-A	IJB-A
; but this is because IJB-A is not challenging enough and	IJB-A	IJB-A
IJB-A	IJB-A	IJB-A
state-of-the-art for verification on the IJB-A and IJB-B datasets. A higher	IJB-A	IJB-A
IJB-A	IJB-A	IJB-A
state-of-the-art for identification on the IJB-A and IJB-B datasets. A higher	IJB-A	IJB-A
the state-of-the-art on the challenging IJB-A and IJB-B benchmarks by a	IJB-A	IJB-A
exceeds the state-of-the-art on the IJB	IJB	IJB-A
margin on the pub- lic IJB	IJB	IJB-A
-A [20] and IJB	IJB	IJB-A
challenging public face recognition datasets IJB	IJB	IJB-A
-A [20] and IJB	IJB	IJB-A
as [5, 10, 18, 25], IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
consider in this work. The IJB	IJB	IJB-A
and 4.2 videos, respectively. The IJB	IJB	IJB-A
dataset is an extension of IJB	IJB	IJB-A
the standard benchmark procedure for IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
apart from the fact that IJB	IJB	IJB-A
defines 10 test splits, while IJB	IJB	IJB-A
two galleries for identification. For IJB	IJB	IJB-A
-A and for IJB	IJB	IJB-A
Network deployment. In the IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
4.3 Ablation studies on IJB	IJB	IJB-A
it to baselines on the IJB	IJB	IJB-A
larger and more challenging than IJB	IJB	IJB-A
1: Verification performance on the IJB	IJB	IJB-A
2: Identification performance on the IJB	IJB	IJB-A
against the state-of-the-art on the IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
identification and verification on both IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
surpasses [44] marginally on the IJB	IJB	IJB-A
to Rank-10 for identification on IJB	IJB	IJB-A
-A; but this is because IJB	IJB	IJB-A
measures on the more challenging IJB	IJB	IJB-A
IJB	IJB	IJB-A
IJB	IJB	IJB-A
state-of-the-art for verification on the IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
vs 0.705 for verification on IJB	IJB	IJB-A
0.743 for iden- tification on IJB	IJB	IJB-A
vs 0.671 for verification on IJB	IJB	IJB-A
vs 0.706 for verification on IJB	IJB	IJB-A
IJB	IJB	IJB-A
IJB	IJB	IJB-A
state-of-the-art for identification on the IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
Fig. 3: Results on the IJB	IJB	IJB-A
the state-of-the-art on the challenging IJB	IJB	IJB-A
-A and IJB	IJB	IJB-A
a tem- plate in the IJB	IJB	IJB-A
A straightforward method to tackle multiple	A	IJB-A
Y. Zhong, R. Arandjelović and A	A	IJB-A
A [20] and IJB-B [37] face	A	IJB-A
A few methods go beyond simple	A	IJB-A
Y. Zhong, R. Arandjelović and A	A	IJB-A
Feature extraction. A neural network is used to	A	IJB-A
Y. Zhong, R. Arandjelović and A	A	IJB-A
Y. Zhong, R. Arandjelović and A	A	IJB-A
A [20] and IJB-B [37] are	A	IJB-A
A and IJB-B are intended for	A	IJB-A
A dataset contains 5,712 images and	A	IJB-A
A with a total of 11,754	A	IJB-A
A and IJB-B, and evaluate on	A	IJB-A
A defines 10 test splits, while	A	IJB-A
A and for IJB-B iden- tification	A	IJB-A
A and IJB-B datasets, there are	A	IJB-A
Y. Zhong, R. Arandjelović and A	A	IJB-A
and more challenging than IJB- A	A	IJB-A
performance on the IJB-B dataset. A higher value of TAR is	A	IJB-A
performance on the IJB-B dataset. A higher value of TPIR is	A	IJB-A
Y. Zhong, R. Arandjelović and A	A	IJB-A
A and IJB-B datasets. The currently	A	IJB-A
A and IJB-B datasets. In particular	A	IJB-A
A verification task, despite the fact	A	IJB-A
A	A	IJB-A
A is not challenging enough and	A	IJB-A
A	A	IJB-A
the IJB-A and IJB-B datasets. A higher value of TAR is	A	IJB-A
Y. Zhong, R. Arandjelović and A	A	IJB-A
A	A	IJB-A
the IJB-A and IJB-B datasets. A higher value of TPIR is	A	IJB-A
A cc	A	IJB-A
A and IJB-B benchmarks by a	A	IJB-A
Arandjelović, R., Gronat, P., Torii, A	A	IJB-A
3] Arandjelović, R., Zisserman, A	A	IJB-A
Xie, W., Parkhi, O.M., Zisserman, A	A	IJB-A
.: VGGFace2: A dataset for recognising faces across	A	IJB-A
Chen, J., Ranjan, R., Kumar, A	A	IJB-A
Parkhi, O.M., Cao, Q., Zisserman, A	A	IJB-A
He, X., Gao, J.: MS-Celeb-1M: A dataset and benchmark for large-scale	A	IJB-A
17] Jégou, H., Zisserman, A	A	IJB-A
Klein, B., Taborsky, E., Blanton, A	A	IJB-A
Allen, K., Grother, P., Mah, A	A	IJB-A
., Jain, A	A	IJB-A
A	A	IJB-A
Parkhi, O.M., Simonyan, K., Vedaldi, A	A	IJB-A
., Zisserman, A.: A compact and discriminative face track	A	IJB-A
25] Parkhi, O.M., Vedaldi, A	A	IJB-A
., Zisserman, A	A	IJB-A
Ma, S., Huang, S., Karpathy, A	A	IJB-A
., Khosla, A	A	IJB-A
., Bernstein, M., Berg, A	A	IJB-A
Kalenichenko, D., Philbin, J.: Facenet: A unified embedding for face recognition	A	IJB-A
32] Turaga, P., Veeraraghavan, A	A	IJB-A
., Srivastava, A	A	IJB-A
33] Vedaldi, A	A	IJB-A
Y. Zhong, R. Arandjelović and A	A	IJB-A
Whitelam, C., Taborsky, E., Blanton, A	A	IJB-A
Miller, T., Kalka, N., Jain, A	A	IJB-A
Xie, W., Shen, L., Zisserman, A	A	IJB-A
39] Xie, W., Zisserman, A	A	IJB-A
Zhang, R., Isola, P., Efros, A	A	IJB-A
A	A	IJB-A
samples and 4386 testing samples. AFLW-Front uses the same training samples	AFLW-Front	AFLW-Front
AFLW-Front 2.94 2.75 2.74 2.68 2.17	AFLW-Front	AFLW-Front
11% on AFLW-Full. On the AFLW-Front testing set, our result is	AFLW-Front	AFLW-Front
algorithms on bench- mark datasets AFLW and 300-W. Code is publicly	AFLW	AFLW-Front
300W-Styles (≈ 12000 images) and AFLW	AFLW	AFLW-Front
transferring the 300-W [46] and AFLW [23] into dif- ferent styles	AFLW	AFLW-Front
AFLW [23]. This dataset contains 21997	AFLW	AFLW-Front
excluding invisible landmark. Faces in AFLW usually have different pose, ex	AFLW	AFLW-Front
There are two types of AFLW splits, AFLW-Full and AFLW-Frontal following	AFLW	AFLW-Front
73]. AFLW	AFLW	AFLW-Front
samples and 4386 testing samples. AFLW	AFLW	AFLW-Front
same training samples as in AFLW	AFLW	AFLW-Front
46, 31, 7, 43]. For AFLW dataset, we use the face	AFLW	AFLW-Front
CCL [73] Two-Stage [31] SAN AFLW	AFLW	AFLW-Front
AFLW	AFLW	AFLW-Front
normalized mean (NME) errors on AFLW dataset	AFLW	AFLW-Front
Results on AFLW	AFLW	AFLW-Front
shows the performance comparison on AFLW	AFLW	AFLW-Front
by more than 11% on AFLW	AFLW	AFLW-Front
-Full. On the AFLW	AFLW	AFLW-Front
mark datasets, e.g., 300-W and AFLW	AFLW	AFLW-Front
300W-Style AFLW	AFLW	AFLW-Front
datasets based on 300-W and AFLW with the original and three	AFLW	AFLW-Front
new datasets, 300W-Style and AFLW	AFLW	AFLW-Front
from 300-W for our 300W-Style. AFLW	AFLW	AFLW-Front
as 300W-Style, which transfer the AFLW dataset into three different styles	AFLW	AFLW-Front
using the training set of AFLW	AFLW	AFLW-Front
and the testing set of AFLW	AFLW	AFLW-Front
can achieve 79.82 AUC@0.08 on AFLW	AFLW	AFLW-Front
by only using the original AFLW training set, while the data	AFLW	AFLW-Front
state-of-the-art performance on 300-W and AFLW datasets	AFLW	AFLW-Front
Front uses the same training samples	Front	AFLW-Front
Front 2.94 2.75 2.74 2.68 2.17	Front	AFLW-Front
Front testing set, our result is	Front	AFLW-Front
new facial landmark detection datasets, 300W	300W	300W
300W	300W	300W
new datasets, 300W	300W	300W
As shown in Figure 7, 300W	300W	300W
using PS. Each image in 300W	300W	300W
provided from 300-W for our 300W	300W	300W
-Style. AFLW-Style is similar as 300W	300W	300W
Comparisons of NME on the 300W	300W	300W
Comparisons of NME on the 300W	300W	300W
Comparisons of NME on the 300W	300W	300W
full testing set of the 300W	300W	300W
W	W	300W
W [46] and AFLW [23] into	W	300W
W dataset. Different faces have different	W	300W
W [46]. However, it is hard	W	300W
W [46]. This dataset annotates five	W	300W
W dataset	W	300W
W dataset, we use the inte	W	300W
W	W	300W
detection algorithms on the 300- W	W	300W
W Common Testing Set	W	300W
W Challenging Testing Set	W	300W
W common and challenging testing sets	W	300W
W common set by relative 21.8	W	300W
W and AFLW. It takes two	W	300W
W common and testing sets. As	W	300W
W by using the style-discriminative features	W	300W
of k-means clustering on 300- W dataset. 300-W dataset is the	W	300W
W and AFLW with the original	W	300W
W datasets, and the other three	W	300W
W dataset, and we thus directly	W	300W
W for our 300W-Style. AFLW-Style is	W	300W
W training set and evaluate the	W	300W
W testing sets with different styles	W	300W
W	W	300W
W training set and test it	W	300W
W challenging test set, our SAN	W	300W
W	W	300W
W and AFLW datasets	W	300W
3] P. N. Belhumeur, D. W	W	300W
W, 2011. 2, 5, 6, 7	W	300W
37] W	W	300W
38] W	W	300W
W, 2017. 5	W	300W
W, 2013. 2, 4, 5, 6	W	300W
52] C. Szegedy, W	W	300W
Lin, X. Dong, Y. Yan, W	W	300W
Feng, L. Liu, X. Nie, W	W	300W
Xing, Z. Niu, J. Huang, W	W	300W
65] Y. Yan, F. Nie, W	W	300W
69] R. Zhao, W	W	300W
the near-frontal face dataset of 300W	300W	300W
Fig. 10. 4.2. Evaluation on 300W dataset	300W	300W
most widely used near frontal 300W dataset [31]. 300W containes 3	300W	300W
NME of different methods on 300W dataset. Method Common Challenging Full	300W	300W
visualization block. The diagonal matrix W contains the weights. For each	W	300W
T. Wu, Y. Wang, and W	W	300W
37] W	W	300W
Tenenbaum, A. Tor- ralba, and W	W	300W
Piramuthu, V. Jagadeesh, D. DeCoste, W	W	300W
Yu, J. Huang, S. Zhang, W	W	300W
detection accuracy measured on the 300W benchmark does not necessarily imply	300W	300W
accuracy on the widely used 300W benchmark [39] does not imply	300W	300W
68 point detection accuracies on 300W	300W	300W
AFW was included in our 300W test set, landmark detection accuracy	300W	300W
Detection accuracy on the 300W benchmark. We evalu- ate performance	300W	300W
on the 300W data set [39], the most	300W	300W
training sets used with the 300W benchmark (e.g., the HELEN [26	300W	300W
1026 images, collectively, form the 300W test set. Note that unlike	300W	300W
detection examples. Landmarks detected in 300W [39] images by projecting an	300W	300W
5] P. N. Belhumeur, D. W	W	300W
B.-C. Chen, C.-S. Chen, and W	W	300W
Poirson, P. Ammirato, C.-Y. Fu, W	W	300W
45] Y. Xiang, W. Kim, W	W	300W
accuracy on the IJB-A and IJB-B benchmarks: using the same recognition	IJB-B	IJB-B
challeng- ing IJB-A [22] and IJB-B benchmarks [43]. In particular, recognition	IJB-B	IJB-B
and B [43] (IJB-A and IJB-B	IJB-B	IJB-B
and identification on IJB-A and IJB-B, com- paring landmark detection based	IJB-B	IJB-B
IJB-B [43] GOTs [43]∗ 16.0 33.0	IJB-B	IJB-B
c) ROC IJB-B (d) CMC IJB-B Figure 5. Verification and identification	IJB-B	IJB-B
results on IJB-A and IJB-B	IJB-B	IJB-B
results on both IJB-A and IJB-B are provided in Table 2	IJB-B	IJB-B
baseline results from [43] for IJB-B (to our knowledge, we are	IJB-B	IJB-B
verification and identification accuracies on IJB-B	IJB-B	IJB-B
the faces in IJB-A and IJB-B	IJB-B	IJB-B
B benchmarks: using the same recognition	B	IJB-B
B benchmarks [43]. In particular, recognition	B	IJB-B
Janus Benchmark A [22] and B [43] (IJB-A and IJB-B). Importantly	B	IJB-B
B, com- paring landmark detection based	B	IJB-B
B [43] GOTs [43]∗ 16.0 33.0	B	IJB-B
B (d) CMC IJB-B Figure 5	B	IJB-B
B	B	IJB-B
B are provided in Table 2	B	IJB-B
B (to our knowledge, we are	B	IJB-B
B	B	IJB-B
B	B	IJB-B
4] A. Bansal, B	B	IJB-B
8] B	B	IJB-B
18] G. B	B	IJB-B
22] B. F. Klare, B	B	IJB-B
Whitelam, E. Taborsky, A. Blanton, B	B	IJB-B
face recognition accuracy on the IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
on the highly challeng- ing IJB	IJB	IJB-B
-A [22] and IJB	IJB	IJB-B
A [22] and B [43] (IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
2. Verification and identification on IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
face alignment methods. Three baseline IJB	IJB	IJB-B
IJB	IJB	IJB-B
IJB	IJB	IJB-B
a) ROC IJB-A (b) CMC IJB	IJB	IJB-B
c) ROC IJB-B (d) CMC IJB	IJB	IJB-B
Verification and identification results on IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
and identification results on both IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
provides, as reference, three state-of-the-art IJB	IJB	IJB-B
baseline results from [43] for IJB	IJB	IJB-B
verification and identification accuracies on IJB	IJB	IJB-B
addition, our verification scores on IJB	IJB	IJB-B
views of the faces in IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
exceeds the state-of-the-art on the IJB-B face recognition dataset. This is	IJB-B	IJB-B
pub- lic IJB-A [20] and IJB-B [37] face recognition benchmarks. These	IJB-B	IJB-B
recognition datasets IJB-A [20] and IJB-B [37] are used for evaluation	IJB-B	IJB-B
10, 18, 25], IJB-A and IJB-B are intended for template-based face	IJB-B	IJB-B
and 4.2 videos, respectively. The IJB-B dataset is an extension of	IJB-B	IJB-B
benchmark procedure for IJB-A and IJB-B, and evaluate on “1:1 face	IJB-B	IJB-B
defines 10 test splits, while IJB-B only has one split for	IJB-B	IJB-B
identification. For IJB-A and for IJB-B iden- tification, we report, as	IJB-B	IJB-B
deployment. In the IJB-A and IJB-B datasets, there are images and	IJB-B	IJB-B
4.3 Ablation studies on IJB-B	IJB-B	IJB-B
it to baselines on the IJB-B dataset, as it is larger	IJB-B	IJB-B
1: Verification performance on the IJB-B dataset. A higher value of	IJB-B	IJB-B
2: Identification performance on the IJB-B dataset. A higher value of	IJB-B	IJB-B
state-of-the-art on the IJB-A and IJB-B datasets. The currently best performing	IJB-B	IJB-B
verification on both IJB-A and IJB-B datasets. In particular, it surpasses	IJB-B	IJB-B
measures on the more challenging IJB-B benchmark, our network achieves the	IJB-B	IJB-B
IJB-B	IJB-B	IJB-B
verification on the IJB-A and IJB-B datasets. A higher value of	IJB-B	IJB-B
vs 0.705 for verification on IJB-B, and TPIR at FPIR=0.01 of	IJB-B	IJB-B
0.743 for iden- tification on IJB-B	IJB-B	IJB-B
vs 0.671 for verification on IJB-B, and TPIR at FPIR=0.01 of	IJB-B	IJB-B
vs 0.706 for verification on IJB-B	IJB-B	IJB-B
IJB-B	IJB-B	IJB-B
identification on the IJB-A and IJB-B datasets. A higher value of	IJB-B	IJB-B
Fig. 3: Results on the IJB-B dataset. Our SE-GV-4-g1 network which	IJB-B	IJB-B
on the challenging IJB-A and IJB-B benchmarks by a large margin	IJB-B	IJB-B
a tem- plate in the IJB-B dataset. The contribution (relative to	IJB-B	IJB-B
B face recognition dataset. This is	B	IJB-B
B [37] face recognition benchmarks. These	B	IJB-B
B [37] are used for evaluation	B	IJB-B
B are intended for template-based face	B	IJB-B
B dataset is an extension of	B	IJB-B
B, and evaluate on “1:1 face	B	IJB-B
B only has one split for	B	IJB-B
B iden- tification, we report, as	B	IJB-B
B datasets, there are images and	B	IJB-B
B	B	IJB-B
B dataset, as it is larger	B	IJB-B
B dataset. A higher value of	B	IJB-B
B dataset. A higher value of	B	IJB-B
B datasets. The currently best performing	B	IJB-B
B datasets. In particular, it surpasses	B	IJB-B
B benchmark, our network achieves the	B	IJB-B
B	B	IJB-B
B datasets. A higher value of	B	IJB-B
B, and TPIR at FPIR=0.01 of	B	IJB-B
B	B	IJB-B
B, and TPIR at FPIR=0.01 of	B	IJB-B
B	B	IJB-B
B	B	IJB-B
B datasets. A higher value of	B	IJB-B
B dataset. Our SE-GV-4-g1 network which	B	IJB-B
B benchmarks by a large margin	B	IJB-B
B dataset. The contribution (relative to	B	IJB-B
6] Cevikalp, H., Triggs, B	B	IJB-B
20] Klare, B., Klein, B	B	IJB-B
Yu, Z., Li, M., Raj, B	B	IJB-B
Taborsky, E., Blanton, A., Maze, B	B	IJB-B
B face dataset. In: CVPR Workshop	B	IJB-B
exceeds the state-of-the-art on the IJB	IJB	IJB-B
margin on the pub- lic IJB	IJB	IJB-B
-A [20] and IJB	IJB	IJB-B
challenging public face recognition datasets IJB	IJB	IJB-B
-A [20] and IJB	IJB	IJB-B
as [5, 10, 18, 25], IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
consider in this work. The IJB	IJB	IJB-B
and 4.2 videos, respectively. The IJB	IJB	IJB-B
dataset is an extension of IJB	IJB	IJB-B
the standard benchmark procedure for IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
apart from the fact that IJB	IJB	IJB-B
defines 10 test splits, while IJB	IJB	IJB-B
two galleries for identification. For IJB	IJB	IJB-B
-A and for IJB	IJB	IJB-B
Network deployment. In the IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
4.3 Ablation studies on IJB	IJB	IJB-B
it to baselines on the IJB	IJB	IJB-B
larger and more challenging than IJB	IJB	IJB-B
1: Verification performance on the IJB	IJB	IJB-B
2: Identification performance on the IJB	IJB	IJB-B
against the state-of-the-art on the IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
identification and verification on both IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
surpasses [44] marginally on the IJB	IJB	IJB-B
to Rank-10 for identification on IJB	IJB	IJB-B
-A; but this is because IJB	IJB	IJB-B
measures on the more challenging IJB	IJB	IJB-B
IJB	IJB	IJB-B
IJB	IJB	IJB-B
state-of-the-art for verification on the IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
vs 0.705 for verification on IJB	IJB	IJB-B
0.743 for iden- tification on IJB	IJB	IJB-B
vs 0.671 for verification on IJB	IJB	IJB-B
vs 0.706 for verification on IJB	IJB	IJB-B
IJB	IJB	IJB-B
IJB	IJB	IJB-B
state-of-the-art for identification on the IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
Fig. 3: Results on the IJB	IJB	IJB-B
the state-of-the-art on the challenging IJB	IJB	IJB-B
-A and IJB	IJB	IJB-B
a tem- plate in the IJB	IJB	IJB-B
2. Dianping Dataset. Dianping	Dianping	Dianping
the Criteo dataset and the Dianping dataset, we randomly split instances	Dianping	Dianping
Dianping 1.2M 18 230K Bing News	Dianping	Dianping
100 for CIN layers on Dianping and Bing News datasets. Since	Dianping	Dianping
Dianping FM 0.8165 0.3558 - DNN	Dianping	Dianping
of different models on Criteo, Dianping and Bing News datasets. The	Dianping	Dianping
Criteo Dianping Bing News Model name AUC	Dianping	Dianping
20 to 200, while on Dianping dataset, 100 is a more	Dianping	Dianping
Dianping 0.8365	Dianping	Dianping
Dianping	Dianping	Dianping
Dianping	Dianping	Dianping
Dianping	Dianping	Dianping
in g Dianping	Dianping	Dianping
Dianping	Dianping	Dianping
6.1 Datasets and Experimental Setup Amazon Dataset2. Amazon Dataset contains product	Amazon	Amazon
reviews and metadata from Amazon, which is used as benchmark	Amazon	Amazon
mini-batch size as described on Amazon Dataset	Amazon	Amazon
Amazon	Amazon	Amazon
Table 3: Model Coparison on Amazon Dataset and Movie- Lens Dataset	Amazon	Amazon
Model MovieLens. Amazon	Amazon	Amazon
Result from model comparison on Amazon Dataset and MovieLens Dataset	Amazon	Amazon
3 shows the results on Amazon dataset andMovieLens dataset. All experiments	Amazon	Amazon
the competitors. Espe- cially on Amazon Dataset with rich user behaviors	Amazon	Amazon
dimension of features in both Amazon Dataset and Movie- Lens Dataset	Amazon	Amazon
Result from model comparison on Amazon Dataset and MovieLens Dataset	Amazon	Amazon
amazon	amazon	Amazon
6.1 Datasets and Experimental Setup Amazon Dataset2. Amazon Dataset contains product	Amazon	Amazon
reviews and metadata from Amazon, which is used as benchmark	Amazon	Amazon
mini-batch size as described on Amazon Dataset	Amazon	Amazon
Amazon	Amazon	Amazon
Table 3: Model Coparison on Amazon Dataset and Movie- Lens Dataset	Amazon	Amazon
Model MovieLens. Amazon	Amazon	Amazon
Result from model comparison on Amazon Dataset and MovieLens Dataset	Amazon	Amazon
3 shows the results on Amazon dataset andMovieLens dataset. All experiments	Amazon	Amazon
the competitors. Espe- cially on Amazon Dataset with rich user behaviors	Amazon	Amazon
dimension of features in both Amazon Dataset and Movie- Lens Dataset	Amazon	Amazon
Result from model comparison on Amazon Dataset and MovieLens Dataset	Amazon	Amazon
amazon	amazon	Amazon
MovieLens data[11] contains 138,493 users, 27,278	MovieLens	MovieLens 20M
Amazon(Electro). 192,403 63,001 801 1,689,188 MovieLens	MovieLens	MovieLens 20M
100,000 2.14 billion a For MovieLens dataset, goods refer to be	MovieLens	MovieLens 20M
Model MovieLens	MovieLens	MovieLens 20M
comparison on Amazon Dataset and MovieLens Dataset	MovieLens	MovieLens 20M
comparison on Amazon Dataset and MovieLens Dataset	MovieLens	MovieLens 20M
MovieLens data[11] contains 138,493 users, 27,278	MovieLens	MovieLens 20M
Amazon(Electro). 192,403 63,001 801 1,689,188 MovieLens	MovieLens	MovieLens 20M
100,000 2.14 billion a For MovieLens dataset, goods refer to be	MovieLens	MovieLens 20M
Model MovieLens	MovieLens	MovieLens 20M
comparison on Amazon Dataset and MovieLens Dataset	MovieLens	MovieLens 20M
comparison on Amazon Dataset and MovieLens Dataset	MovieLens	MovieLens 20M
criteo	criteo	Criteo
lowing three datasets: 1. Criteo Dataset. It is a famous	Criteo	Criteo
For the Criteo dataset and the Dianping dataset	Criteo	Criteo
Datasest #instances #fields #features (sparse) Criteo 45M 39 2.3M	Criteo	Criteo
200 for CIN layers on Criteo dataset, and 100 for CIN	Criteo	Criteo
of individual models on the Criteo, Di- anping, andBingNews datasets. ColumnDepth	Criteo	Criteo
Model name AUC Logloss Depth Criteo	Criteo	Criteo
performance of different models on Criteo, Dianping and Bing News datasets	Criteo	Criteo
Criteo Dianping Bing News Model name	Criteo	Criteo
criteo	criteo	Criteo
the following two datasets. 1) Criteo Dataset: Criteo dataset 5 includes	Criteo	Criteo
To evaluate the models on Criteo dataset, we follow the pa	Criteo	Criteo
of differ- ent models on Criteo dataset by the following formula	Criteo	Criteo
prediction of different models on Criteo dataset and Company∗ dataset is	Criteo	Criteo
of Logloss) on Company∗ and Criteo datasets. • Learning high- and	Criteo	Criteo
of Logloss) on Company∗ and Criteo datasets. • Learning high- and	Criteo	Criteo
of Logloss) on Company∗ and Criteo datasets	Criteo	Criteo
Performance on CTR prediction. Company∗ Criteo	Criteo	Criteo
experiments on two real-world datasets (Criteo dataset and a commercial App	Criteo	Criteo
criteo	criteo	Criteo
criteo	criteo	Criteo
criteo	criteo	Criteo
1) Criteo: Criteo 1TB click log2 is a	Criteo	Criteo
show the overall performance on Criteo and iPinYou datasets, respectively. In	Criteo	Criteo
I: Overall Performance on the Criteo Dataset	Criteo	Criteo
perform the best on both Criteo and iPinYou datasets. As for	Criteo	Criteo
IV-A1 Criteo	Criteo	Criteo
criteo	criteo	Criteo
criteo	criteo	Criteo
criteo	criteo	Criteo
1) Criteo: Criteo 1TB click log2 is a	Criteo	Criteo
show the overall performance on Criteo and iPinYou datasets, respectively. In	Criteo	Criteo
I: Overall Performance on the Criteo Dataset	Criteo	Criteo
perform the best on both Criteo and iPinYou datasets. As for	Criteo	Criteo
IV-A1 Criteo	Criteo	Criteo
criteo	criteo	Criteo
criteo	criteo	Criteo
criteo	criteo	Criteo
1) Criteo: Criteo 1TB click log2 is a	Criteo	Criteo
show the overall performance on Criteo and iPinYou datasets, respectively. In	Criteo	Criteo
I: Overall Performance on the Criteo Dataset	Criteo	Criteo
perform the best on both Criteo and iPinYou datasets. As for	Criteo	Criteo
IV-A1 Criteo	Criteo	Criteo
10% is for testing. 2) Company	Company*	Company*
we conduct exper- iment on Company	Company*	Company*
the game center of the Company	Company*	Company*
for each individual model on Company	Company*	Company*
models on Criteo dataset and Company	Company*	Company*
in terms of Logloss) on Company	Company*	Company*
in terms of Logloss) on Company	Company*	Company*
in terms of Logloss) on Company	Company*	Company*
2: Performance on CTR prediction. Company	Company*	Company*
of AUC and Logloss on Company	Company*	Company*
3.9%. The daily turnover of Company	Company*	Company*
differ- ent deep models, on Company	Company*	Company*
3. Bing News Dataset. Bing News2 is part of Microsoft’s	Bing	Bing News
Bing search engine. In order to	Bing	Bing News
Dianping 1.2M 18 230K Bing News 5M 45 17K	Bing	Bing News
CIN layers on Dianping and Bing News datasets. Since we focus	Bing	Bing News
Bing News FM 0.8223 0.2779	Bing	Bing News
models on Criteo, Dianping and Bing News datasets. The column Depth	Bing	Bing News
Criteo Dianping Bing News Model name AUC Logloss	Bing	Bing News
the best result ON the Bing News dataset	Bing	Bing News
and 7b, model performance on Bing News dataset increases steadily when	Bing	Bing News
Bing News	Bing	Bing News
Bing News	Bing	Bing News
Bing News	Bing	Bing News
Bing News	Bing	Bing News
Bing News	Bing	Bing News
Ensemble for Click Prediction in Bing Search Ads. In Proceedings of	Bing	Bing News
3. Bing News Dataset. Bing News2 is part of Microsoft’s Bing	Bing News	Bing News
Dianping 1.2M 18 230K Bing News 5M 45 17K	Bing News	Bing News
CIN layers on Dianping and Bing News datasets. Since we focus on	Bing News	Bing News
Bing News FM 0.8223 0.2779 - DNN	Bing News	Bing News
models on Criteo, Dianping and Bing News datasets. The column Depth presents	Bing News	Bing News
Criteo Dianping Bing News Model name AUC Logloss Depth	Bing News	Bing News
the best result ON the Bing News dataset	Bing News	Bing News
and 7b, model performance on Bing News dataset increases steadily when we	Bing News	Bing News
Bing News	Bing News	Bing News
Bing News	Bing News	Bing News
Bing News	Bing News	Bing News
Bing News	Bing News	Bing News
Bing News	Bing News	Bing News
3. Bing News Dataset. Bing News2 is part of Microsoft’s Bing	News	Bing News
Dianping 1.2M 18 230K Bing News 5M 45 17K	News	Bing News
layers on Dianping and Bing News datasets. Since we focus on	News	Bing News
Bing News FM 0.8223 0.2779 - DNN	News	Bing News
on Criteo, Dianping and Bing News datasets. The column Depth presents	News	Bing News
Criteo Dianping Bing News Model name AUC Logloss Depth	News	Bing News
best result ON the Bing News dataset	News	Bing News
7b, model performance on Bing News dataset increases steadily when we	News	Bing News
Bing News	News	Bing News
Bing News	News	Bing News
Bing News	News	Bing News
sBing News	News	Bing News
Bing News	News	Bing News
Bing News	News	Bing News
Wang, “Real-time bidding benchmarking with ipinyou dataset,” arXiv:1407.7073, 2014	ipinyou	iPinYou
2) iPinYou: The iPinYou dataset3 is another real-world dataset	iPinYou	iPinYou
overall performance on Criteo and iPinYou datasets, respectively. In FM, we	iPinYou	iPinYou
II: Overall Performance on the iPinYou Dataset	iPinYou	iPinYou
best on both Criteo and iPinYou datasets. As for log loss	iPinYou	iPinYou
to the training iterations on iPinYou dataset. We find that network	iPinYou	iPinYou
3: Learning Curves on the iPinYou Dataset	iPinYou	iPinYou
IV-A2 iPinYou	iPinYou	iPinYou
Wang, “Real-time bidding benchmarking with ipinyou dataset,” arXiv:1407.7073, 2014	ipinyou	iPinYou
2) iPinYou: The iPinYou dataset3 is another real-world dataset	iPinYou	iPinYou
overall performance on Criteo and iPinYou datasets, respectively. In FM, we	iPinYou	iPinYou
II: Overall Performance on the iPinYou Dataset	iPinYou	iPinYou
best on both Criteo and iPinYou datasets. As for log loss	iPinYou	iPinYou
to the training iterations on iPinYou dataset. We find that network	iPinYou	iPinYou
3: Learning Curves on the iPinYou Dataset	iPinYou	iPinYou
IV-A2 iPinYou	iPinYou	iPinYou
Wang, “Real-time bidding benchmarking with ipinyou dataset,” arXiv:1407.7073, 2014	ipinyou	iPinYou
2) iPinYou: The iPinYou dataset3 is another real-world dataset	iPinYou	iPinYou
overall performance on Criteo and iPinYou datasets, respectively. In FM, we	iPinYou	iPinYou
II: Overall Performance on the iPinYou Dataset	iPinYou	iPinYou
best on both Criteo and iPinYou datasets. As for log loss	iPinYou	iPinYou
to the training iterations on iPinYou dataset. We find that network	iPinYou	iPinYou
3: Learning Curves on the iPinYou Dataset	iPinYou	iPinYou
IV-A2 iPinYou	iPinYou	iPinYou
L., Liu, Z., Shen, X.: ipinyou global rtb bidding algorithm compe	ipinyou	iPinYou
evaluate our models based on iPinYou dataset [27], a public real	iPinYou	iPinYou
SNAP collection (Youtube, DBLP and Amazon), and we restrict the largest	Amazon	Amazon
E| GNN LGNN MPNN∗ AGMFit Amazon 268 / 52 60 346	Amazon	Amazon
Edge Ratio vs Amazon Graph	Amazon	Amazon
graphs with la- beled communities (amazon, dblp, livejournal, orkut and friendster	amazon	Amazon
amazon [42] 334,863 1,851,744 799,080 dblp	amazon	Amazon
set of widely used graphs (amazon, dblp, livejournal, orkut, friendster) that	amazon	Amazon
amazon 0.0374 0.0809 0.0337 0.0310 0.0339	amazon	Amazon
communities in the graphs analyzed: amazon 27,004; dblp 33,626; livejournal 49,954	amazon	Amazon
to process our smaller graph amazon (resp. number of edges in	amazon	Amazon
amazon	amazon	Amazon
amazon	amazon	Amazon
80 times longer that of amazon	amazon	Amazon
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard DoBEM	Moderate	KITTI Cars Moderate
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard DoBEM	Moderate	KITTI Cars Moderate
Method Easy Moderate Hard Mono3D [4] 2.53 2.31	Moderate	KITTI Cars Moderate
Method Easy Moderate Hard Mono3D [4] 5.22 5.19	Moderate	KITTI Cars Moderate
Benchmark Easy Moderate Hard Pedestrian (3D Detection) 70.00	Moderate	KITTI Cars Moderate
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard SWC	Moderate	KITTI Cars Moderate
Subset Easy Moderate Hard AP (2D) for cars	Moderate	KITTI Cars Moderate
Method Easy Moderate Hard VeloFCN [18] 15.20 13.66	Moderate	KITTI Cars Moderate
Method Cars Pedestrians Cyclists	Cars	KITTI Cars Moderate
Method Cars Pedestrians Cyclists	Cars	KITTI Cars Moderate
Method Cars Pedestrians Cyclists	Cars	KITTI Cars Moderate
very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection	KITTI	KITTI Cars Moderate
method achieve leading positions on KITTI 3D ob- ject detection [1	KITTI	KITTI Cars Moderate
ther fine-tune it on a KITTI 2D object detection dataset to	KITTI	KITTI Cars Moderate
5m to beyond 50m in KITTI data), we predict the 3D	KITTI	KITTI Cars Moderate
for 3D object detection on KITTI [10] and SUN-RGBD [33] (Sec	KITTI	KITTI Cars Moderate
our 3D object detector on KITTI [11] and SUN-RGBD [33] benchmarks	KITTI	KITTI Cars Moderate
KITTI Tab. 1 shows the performance	KITTI	KITTI Cars Moderate
our 3D detector on the KITTI test set. We outperform previous	KITTI	KITTI Cars Moderate
object detection 3D AP on KITTI test set. DoBEM [42] and	KITTI	KITTI Cars Moderate
AP (bird’s eye view) on KITTI test set. 3D FCN [17	KITTI	KITTI Cars Moderate
3D object detection AP on KITTI val set (cars only	KITTI	KITTI Cars Moderate
3D object localization AP on KITTI val set (cars only	KITTI	KITTI Cars Moderate
We also report performance on KITTI val set (the same split	KITTI	KITTI Cars Moderate
56.32 Table 5. Performance on KITTI val set for pedestrians and	KITTI	KITTI Cars Moderate
same pipeline we used for KITTI data set, we’ve achieved state-of-the-art	KITTI	KITTI Cars Moderate
on our v1 model on KITTI data using train/val split as	KITTI	KITTI Cars Moderate
of Frustum PointNet results on KITTI val set (best viewed in	KITTI	KITTI Cars Moderate
typical 2D region proposal from KITTI val set with both 2D	KITTI	KITTI Cars Moderate
on the fly (1,024 for KITTI and 2,048 for SUN-RGBD). For	KITTI	KITTI Cars Moderate
KITTI Training The object detection benchmark	KITTI	KITTI Cars Moderate
in KITTI provides synchronized RGB images and	KITTI	KITTI Cars Moderate
the same as that in KITTI	KITTI	KITTI Cars Moderate
much lower than that in KITTI because of strong occlusions and	KITTI	KITTI Cars Moderate
compared to around 90% in KITTI	KITTI	KITTI Cars Moderate
pedestrian, and cy- clist from KITTI dataset. The final model takes	KITTI	KITTI Cars Moderate
our detector’s AP (2D) on KITTI test set. Our detector has	KITTI	KITTI Cars Moderate
than current leading players on KITTI leader board. We’ve also reported	KITTI	KITTI Cars Moderate
2D object detection AP on KITTI test set. Evaluation IoU threshold	KITTI	KITTI Cars Moderate
the first place winner on KITTI leader board for pedestrians and	KITTI	KITTI Cars Moderate
2D object detection AP on KITTI val set	KITTI	KITTI Cars Moderate
3D object detection AP on KITTI val set. By using both	KITTI	KITTI Cars Moderate
can see that compared with KITTI LiDAR data, depth images can	KITTI	KITTI Cars Moderate
Method Modality Car Pedestrian CyclistEasy Moderate Hard Easy Moderate Hard Easy	Moderate	KITTI Cars Moderate
Moderate Hard Mono3D [3] Mono 5.22	Moderate	KITTI Cars Moderate
Method Modality Car Pedestrian CyclistEasy Moderate Hard Easy Moderate Hard Easy	Moderate	KITTI Cars Moderate
Moderate Hard Mono3D [3] Mono 2.53	Moderate	KITTI Cars Moderate
Benchmark Easy Moderate Hard Car (3D Detection) 77.47	Moderate	KITTI Cars Moderate
generate detections. Experiments on the KITTI car detection bench- mark show	KITTI	KITTI Cars Moderate
detection tasks, provided by the KITTI benchmark [11]. Experimental results show	KITTI	KITTI Cars Moderate
We conduct experiments on KITTI benchmark and show that VoxelNet	KITTI	KITTI Cars Moderate
LiDAR specifi- cations of the KITTI dataset [11]. Car Detection For	KITTI	KITTI Cars Moderate
We evaluate VoxelNet on the KITTI 3D object detection	KITTI	KITTI Cars Moderate
average precision (in %) on KITTI validation set	KITTI	KITTI Cars Moderate
average precision (in %) on KITTI validation set	KITTI	KITTI Cars Moderate
the test results using the KITTI server	KITTI	KITTI Cars Moderate
the LiDAR data provided in KITTI	KITTI	KITTI Cars Moderate
4.1. Evaluation on KITTI Validation Set	KITTI	KITTI Cars Moderate
Metrics We follow the official KITTI evaluation protocol, where the IoU	KITTI	KITTI Cars Moderate
4.2. Evaluation on KITTI Test Set	KITTI	KITTI Cars Moderate
We evaluated VoxelNet on the KITTI test set by submit- ting	KITTI	KITTI Cars Moderate
other leading methods listed in KITTI benchmark use both RGB images	KITTI	KITTI Cars Moderate
Table 3. Performance evaluation on KITTI test set	KITTI	KITTI Cars Moderate
grid. Our experiments on the KITTI car detection task show that	KITTI	KITTI Cars Moderate
to some interesting failure modes. Pedestrians and cyclists are commonly misclassified	Pedestrians	KITTI Pedestrians Moderate
3D and bird’s eye view KITTI bench- marks. This detection performance	KITTI	KITTI Pedestrians Moderate
PointPillars, PP method on the KITTI [5] test set. Lidar-only methods	KITTI	KITTI Pedestrians Moderate
are top methods from the KITTI leader- board: M : MV3D	KITTI	KITTI Pedestrians Moderate
PointPillars network on the public KITTI detection challenges which require detection	KITTI	KITTI Pedestrians Moderate
We conduct experiments on the KITTI dataset and demonstrate state of	KITTI	KITTI Pedestrians Moderate
the range typically used in KITTI for ∼ 97% sparsity. This	KITTI	KITTI Pedestrians Moderate
Figure 3. Qualitative analysis of KITTI results. We show a bird’s-eye	KITTI	KITTI Pedestrians Moderate
Figure 4. Failure cases on KITTI	KITTI	KITTI Pedestrians Moderate
All experiments use the KITTI object detection bench- mark dataset	KITTI	KITTI Pedestrians Moderate
the remaining 6733 samples. The KITTI benchmark requires detections of cars	KITTI	KITTI Pedestrians Moderate
the standard literature practice on KITTI [11, 31, 28], we train	KITTI	KITTI Pedestrians Moderate
Table 1. Results on the KITTI test BEV detection benchmark	KITTI	KITTI Pedestrians Moderate
Table 2. Results on the KITTI test 3D detection benchmark	KITTI	KITTI Pedestrians Moderate
the KITTI benchmark [28, 30, 2]. First	KITTI	KITTI Pedestrians Moderate
mea- sured using the official KITTI evaluation detection metrics which are	KITTI	KITTI Pedestrians Moderate
for 2D de- tections. The KITTI dataset is stratified into easy	KITTI	KITTI Pedestrians Moderate
hard difficulties, and the official KITTI leaderboard is ranked by performance	KITTI	KITTI Pedestrians Moderate
Table 3. Results on the KITTI test average orientation similarity (AOS	KITTI	KITTI Pedestrians Moderate
to an artifact of the KITTI ground truth annotations, only lidar	KITTI	KITTI Pedestrians Moderate
vs speed (Hz) on the KITTI [5] val set across pedestrians	KITTI	KITTI Pedestrians Moderate
measured as BEV mAP on KITTI val. Learned encoders clearly beat	KITTI	KITTI Pedestrians Moderate
We demonstrate that on the KITTI chal- lenge, PointPillars dominates all	KITTI	KITTI Pedestrians Moderate
for au- tonomous driving? the KITTI vision benchmark suite. In CVPR	KITTI	KITTI Pedestrians Moderate
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard DoBEM	Moderate	KITTI Cyclists Moderate
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard DoBEM	Moderate	KITTI Cyclists Moderate
Method Easy Moderate Hard Mono3D [4] 2.53 2.31	Moderate	KITTI Cyclists Moderate
Method Easy Moderate Hard Mono3D [4] 5.22 5.19	Moderate	KITTI Cyclists Moderate
Benchmark Easy Moderate Hard Pedestrian (3D Detection) 70.00	Moderate	KITTI Cyclists Moderate
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard SWC	Moderate	KITTI Cyclists Moderate
Subset Easy Moderate Hard AP (2D) for cars	Moderate	KITTI Cyclists Moderate
Method Easy Moderate Hard VeloFCN [18] 15.20 13.66	Moderate	KITTI Cyclists Moderate
Method Cars Pedestrians Cyclists	Cyclists	KITTI Cyclists Moderate
Method Cars Pedestrians Cyclists	Cyclists	KITTI Cyclists Moderate
Method Cars Pedestrians Cyclists	Cyclists	KITTI Cyclists Moderate
very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection	KITTI	KITTI Cyclists Moderate
method achieve leading positions on KITTI 3D ob- ject detection [1	KITTI	KITTI Cyclists Moderate
ther fine-tune it on a KITTI 2D object detection dataset to	KITTI	KITTI Cyclists Moderate
5m to beyond 50m in KITTI data), we predict the 3D	KITTI	KITTI Cyclists Moderate
for 3D object detection on KITTI [10] and SUN-RGBD [33] (Sec	KITTI	KITTI Cyclists Moderate
our 3D object detector on KITTI [11] and SUN-RGBD [33] benchmarks	KITTI	KITTI Cyclists Moderate
KITTI Tab. 1 shows the performance	KITTI	KITTI Cyclists Moderate
our 3D detector on the KITTI test set. We outperform previous	KITTI	KITTI Cyclists Moderate
object detection 3D AP on KITTI test set. DoBEM [42] and	KITTI	KITTI Cyclists Moderate
AP (bird’s eye view) on KITTI test set. 3D FCN [17	KITTI	KITTI Cyclists Moderate
3D object detection AP on KITTI val set (cars only	KITTI	KITTI Cyclists Moderate
3D object localization AP on KITTI val set (cars only	KITTI	KITTI Cyclists Moderate
We also report performance on KITTI val set (the same split	KITTI	KITTI Cyclists Moderate
56.32 Table 5. Performance on KITTI val set for pedestrians and	KITTI	KITTI Cyclists Moderate
same pipeline we used for KITTI data set, we’ve achieved state-of-the-art	KITTI	KITTI Cyclists Moderate
on our v1 model on KITTI data using train/val split as	KITTI	KITTI Cyclists Moderate
of Frustum PointNet results on KITTI val set (best viewed in	KITTI	KITTI Cyclists Moderate
typical 2D region proposal from KITTI val set with both 2D	KITTI	KITTI Cyclists Moderate
on the fly (1,024 for KITTI and 2,048 for SUN-RGBD). For	KITTI	KITTI Cyclists Moderate
KITTI Training The object detection benchmark	KITTI	KITTI Cyclists Moderate
in KITTI provides synchronized RGB images and	KITTI	KITTI Cyclists Moderate
the same as that in KITTI	KITTI	KITTI Cyclists Moderate
much lower than that in KITTI because of strong occlusions and	KITTI	KITTI Cyclists Moderate
compared to around 90% in KITTI	KITTI	KITTI Cyclists Moderate
pedestrian, and cy- clist from KITTI dataset. The final model takes	KITTI	KITTI Cyclists Moderate
our detector’s AP (2D) on KITTI test set. Our detector has	KITTI	KITTI Cyclists Moderate
than current leading players on KITTI leader board. We’ve also reported	KITTI	KITTI Cyclists Moderate
2D object detection AP on KITTI test set. Evaluation IoU threshold	KITTI	KITTI Cyclists Moderate
the first place winner on KITTI leader board for pedestrians and	KITTI	KITTI Cyclists Moderate
2D object detection AP on KITTI val set	KITTI	KITTI Cyclists Moderate
3D object detection AP on KITTI val set. By using both	KITTI	KITTI Cyclists Moderate
can see that compared with KITTI LiDAR data, depth images can	KITTI	KITTI Cyclists Moderate
Method Modality Car Pedestrian CyclistEasy Moderate Hard Easy Moderate Hard Easy	Moderate	KITTI Cyclists Moderate
Moderate Hard Mono3D [3] Mono 5.22	Moderate	KITTI Cyclists Moderate
Method Modality Car Pedestrian CyclistEasy Moderate Hard Easy Moderate Hard Easy	Moderate	KITTI Cyclists Moderate
Moderate Hard Mono3D [3] Mono 2.53	Moderate	KITTI Cyclists Moderate
Benchmark Easy Moderate Hard Car (3D Detection) 77.47	Moderate	KITTI Cyclists Moderate
generate detections. Experiments on the KITTI car detection bench- mark show	KITTI	KITTI Cyclists Moderate
detection tasks, provided by the KITTI benchmark [11]. Experimental results show	KITTI	KITTI Cyclists Moderate
We conduct experiments on KITTI benchmark and show that VoxelNet	KITTI	KITTI Cyclists Moderate
LiDAR specifi- cations of the KITTI dataset [11]. Car Detection For	KITTI	KITTI Cyclists Moderate
We evaluate VoxelNet on the KITTI 3D object detection	KITTI	KITTI Cyclists Moderate
average precision (in %) on KITTI validation set	KITTI	KITTI Cyclists Moderate
average precision (in %) on KITTI validation set	KITTI	KITTI Cyclists Moderate
the test results using the KITTI server	KITTI	KITTI Cyclists Moderate
the LiDAR data provided in KITTI	KITTI	KITTI Cyclists Moderate
4.1. Evaluation on KITTI Validation Set	KITTI	KITTI Cyclists Moderate
Metrics We follow the official KITTI evaluation protocol, where the IoU	KITTI	KITTI Cyclists Moderate
4.2. Evaluation on KITTI Test Set	KITTI	KITTI Cyclists Moderate
We evaluated VoxelNet on the KITTI test set by submit- ting	KITTI	KITTI Cyclists Moderate
other leading methods listed in KITTI benchmark use both RGB images	KITTI	KITTI Cyclists Moderate
Table 3. Performance evaluation on KITTI test set	KITTI	KITTI Cyclists Moderate
grid. Our experiments on the KITTI car detection task show that	KITTI	KITTI Cyclists Moderate
Modality Car Pedestrian CyclistEasy Moderate Hard Easy Moderate Hard Easy Moderate	Hard	KITTI Cars Hard
Hard Mono3D [3] Mono 5.22 5.19	Hard	KITTI Cars Hard
Modality Car Pedestrian CyclistEasy Moderate Hard Easy Moderate Hard Easy Moderate	Hard	KITTI Cars Hard
Hard Mono3D [3] Mono 2.53 2.31	Hard	KITTI Cars Hard
Benchmark Easy Moderate Hard Car (3D Detection) 77.47 65.11	Hard	KITTI Cars Hard
generate detections. Experiments on the KITTI car detection bench- mark show	KITTI	KITTI Cars Hard
detection tasks, provided by the KITTI benchmark [11]. Experimental results show	KITTI	KITTI Cars Hard
We conduct experiments on KITTI benchmark and show that VoxelNet	KITTI	KITTI Cars Hard
LiDAR specifi- cations of the KITTI dataset [11]. Car Detection For	KITTI	KITTI Cars Hard
We evaluate VoxelNet on the KITTI 3D object detection	KITTI	KITTI Cars Hard
average precision (in %) on KITTI validation set	KITTI	KITTI Cars Hard
average precision (in %) on KITTI validation set	KITTI	KITTI Cars Hard
the test results using the KITTI server	KITTI	KITTI Cars Hard
the LiDAR data provided in KITTI	KITTI	KITTI Cars Hard
4.1. Evaluation on KITTI Validation Set	KITTI	KITTI Cars Hard
Metrics We follow the official KITTI evaluation protocol, where the IoU	KITTI	KITTI Cars Hard
4.2. Evaluation on KITTI Test Set	KITTI	KITTI Cars Hard
We evaluated VoxelNet on the KITTI test set by submit- ting	KITTI	KITTI Cars Hard
other leading methods listed in KITTI benchmark use both RGB images	KITTI	KITTI Cars Hard
Table 3. Performance evaluation on KITTI test set	KITTI	KITTI Cars Hard
grid. Our experiments on the KITTI car detection task show that	KITTI	KITTI Cars Hard
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard DoBEM [42	Hard	KITTI Cars Hard
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard DoBEM [42	Hard	KITTI Cars Hard
Method Easy Moderate Hard Mono3D [4] 2.53 2.31 2.31	Hard	KITTI Cars Hard
Method Easy Moderate Hard Mono3D [4] 5.22 5.19 4.13	Hard	KITTI Cars Hard
Benchmark Easy Moderate Hard Pedestrian (3D Detection) 70.00 61.32	Hard	KITTI Cars Hard
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard SWC 90.82	Hard	KITTI Cars Hard
Subset Easy Moderate Hard AP (2D) for cars 96.48	Hard	KITTI Cars Hard
Method Easy Moderate Hard VeloFCN [18] 15.20 13.66 15.98	Hard	KITTI Cars Hard
Method Cars Pedestrians Cyclists	Cars	KITTI Cars Hard
Method Cars Pedestrians Cyclists	Cars	KITTI Cars Hard
Method Cars Pedestrians Cyclists	Cars	KITTI Cars Hard
very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection	KITTI	KITTI Cars Hard
method achieve leading positions on KITTI 3D ob- ject detection [1	KITTI	KITTI Cars Hard
ther fine-tune it on a KITTI 2D object detection dataset to	KITTI	KITTI Cars Hard
5m to beyond 50m in KITTI data), we predict the 3D	KITTI	KITTI Cars Hard
for 3D object detection on KITTI [10] and SUN-RGBD [33] (Sec	KITTI	KITTI Cars Hard
our 3D object detector on KITTI [11] and SUN-RGBD [33] benchmarks	KITTI	KITTI Cars Hard
KITTI Tab. 1 shows the performance	KITTI	KITTI Cars Hard
our 3D detector on the KITTI test set. We outperform previous	KITTI	KITTI Cars Hard
object detection 3D AP on KITTI test set. DoBEM [42] and	KITTI	KITTI Cars Hard
AP (bird’s eye view) on KITTI test set. 3D FCN [17	KITTI	KITTI Cars Hard
3D object detection AP on KITTI val set (cars only	KITTI	KITTI Cars Hard
3D object localization AP on KITTI val set (cars only	KITTI	KITTI Cars Hard
We also report performance on KITTI val set (the same split	KITTI	KITTI Cars Hard
56.32 Table 5. Performance on KITTI val set for pedestrians and	KITTI	KITTI Cars Hard
same pipeline we used for KITTI data set, we’ve achieved state-of-the-art	KITTI	KITTI Cars Hard
on our v1 model on KITTI data using train/val split as	KITTI	KITTI Cars Hard
of Frustum PointNet results on KITTI val set (best viewed in	KITTI	KITTI Cars Hard
typical 2D region proposal from KITTI val set with both 2D	KITTI	KITTI Cars Hard
on the fly (1,024 for KITTI and 2,048 for SUN-RGBD). For	KITTI	KITTI Cars Hard
KITTI Training The object detection benchmark	KITTI	KITTI Cars Hard
in KITTI provides synchronized RGB images and	KITTI	KITTI Cars Hard
the same as that in KITTI	KITTI	KITTI Cars Hard
much lower than that in KITTI because of strong occlusions and	KITTI	KITTI Cars Hard
compared to around 90% in KITTI	KITTI	KITTI Cars Hard
pedestrian, and cy- clist from KITTI dataset. The final model takes	KITTI	KITTI Cars Hard
our detector’s AP (2D) on KITTI test set. Our detector has	KITTI	KITTI Cars Hard
than current leading players on KITTI leader board. We’ve also reported	KITTI	KITTI Cars Hard
2D object detection AP on KITTI test set. Evaluation IoU threshold	KITTI	KITTI Cars Hard
the first place winner on KITTI leader board for pedestrians and	KITTI	KITTI Cars Hard
2D object detection AP on KITTI val set	KITTI	KITTI Cars Hard
3D object detection AP on KITTI val set. By using both	KITTI	KITTI Cars Hard
can see that compared with KITTI LiDAR data, depth images can	KITTI	KITTI Cars Hard
Car Pedestrian CyclistEasy Moderate Hard Easy Moderate Hard Easy Moderate Hard	Easy	KITTI Cars Easy
Car Pedestrian CyclistEasy Moderate Hard Easy Moderate Hard Easy Moderate Hard	Easy	KITTI Cars Easy
Benchmark Easy Moderate Hard Car (3D Detection	Easy	KITTI Cars Easy
generate detections. Experiments on the KITTI car detection bench- mark show	KITTI	KITTI Cars Easy
detection tasks, provided by the KITTI benchmark [11]. Experimental results show	KITTI	KITTI Cars Easy
We conduct experiments on KITTI benchmark and show that VoxelNet	KITTI	KITTI Cars Easy
LiDAR specifi- cations of the KITTI dataset [11]. Car Detection For	KITTI	KITTI Cars Easy
We evaluate VoxelNet on the KITTI 3D object detection	KITTI	KITTI Cars Easy
average precision (in %) on KITTI validation set	KITTI	KITTI Cars Easy
average precision (in %) on KITTI validation set	KITTI	KITTI Cars Easy
the test results using the KITTI server	KITTI	KITTI Cars Easy
the LiDAR data provided in KITTI	KITTI	KITTI Cars Easy
4.1. Evaluation on KITTI Validation Set	KITTI	KITTI Cars Easy
Metrics We follow the official KITTI evaluation protocol, where the IoU	KITTI	KITTI Cars Easy
4.2. Evaluation on KITTI Test Set	KITTI	KITTI Cars Easy
We evaluated VoxelNet on the KITTI test set by submit- ting	KITTI	KITTI Cars Easy
other leading methods listed in KITTI benchmark use both RGB images	KITTI	KITTI Cars Easy
Table 3. Performance evaluation on KITTI test set	KITTI	KITTI Cars Easy
grid. Our experiments on the KITTI car detection task show that	KITTI	KITTI Cars Easy
Method Cars Pedestrians Cyclists	Cars	KITTI Cars Easy
Method Cars Pedestrians Cyclists	Cars	KITTI Cars Easy
Method Cars Pedestrians Cyclists	Cars	KITTI Cars Easy
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard	Easy	KITTI Cars Easy
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard	Easy	KITTI Cars Easy
Method Easy Moderate Hard Mono3D [4] 2.53	Easy	KITTI Cars Easy
Method Easy Moderate Hard Mono3D [4] 5.22	Easy	KITTI Cars Easy
Benchmark Easy Moderate Hard Pedestrian (3D Detection	Easy	KITTI Cars Easy
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard	Easy	KITTI Cars Easy
Subset Easy Moderate Hard AP (2D) for	Easy	KITTI Cars Easy
Method Easy Moderate Hard VeloFCN [18] 15.20	Easy	KITTI Cars Easy
very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection	KITTI	KITTI Cars Easy
method achieve leading positions on KITTI 3D ob- ject detection [1	KITTI	KITTI Cars Easy
ther fine-tune it on a KITTI 2D object detection dataset to	KITTI	KITTI Cars Easy
5m to beyond 50m in KITTI data), we predict the 3D	KITTI	KITTI Cars Easy
for 3D object detection on KITTI [10] and SUN-RGBD [33] (Sec	KITTI	KITTI Cars Easy
our 3D object detector on KITTI [11] and SUN-RGBD [33] benchmarks	KITTI	KITTI Cars Easy
KITTI Tab. 1 shows the performance	KITTI	KITTI Cars Easy
our 3D detector on the KITTI test set. We outperform previous	KITTI	KITTI Cars Easy
object detection 3D AP on KITTI test set. DoBEM [42] and	KITTI	KITTI Cars Easy
AP (bird’s eye view) on KITTI test set. 3D FCN [17	KITTI	KITTI Cars Easy
3D object detection AP on KITTI val set (cars only	KITTI	KITTI Cars Easy
3D object localization AP on KITTI val set (cars only	KITTI	KITTI Cars Easy
We also report performance on KITTI val set (the same split	KITTI	KITTI Cars Easy
56.32 Table 5. Performance on KITTI val set for pedestrians and	KITTI	KITTI Cars Easy
same pipeline we used for KITTI data set, we’ve achieved state-of-the-art	KITTI	KITTI Cars Easy
on our v1 model on KITTI data using train/val split as	KITTI	KITTI Cars Easy
of Frustum PointNet results on KITTI val set (best viewed in	KITTI	KITTI Cars Easy
typical 2D region proposal from KITTI val set with both 2D	KITTI	KITTI Cars Easy
on the fly (1,024 for KITTI and 2,048 for SUN-RGBD). For	KITTI	KITTI Cars Easy
KITTI Training The object detection benchmark	KITTI	KITTI Cars Easy
in KITTI provides synchronized RGB images and	KITTI	KITTI Cars Easy
the same as that in KITTI	KITTI	KITTI Cars Easy
much lower than that in KITTI because of strong occlusions and	KITTI	KITTI Cars Easy
compared to around 90% in KITTI	KITTI	KITTI Cars Easy
pedestrian, and cy- clist from KITTI dataset. The final model takes	KITTI	KITTI Cars Easy
our detector’s AP (2D) on KITTI test set. Our detector has	KITTI	KITTI Cars Easy
than current leading players on KITTI leader board. We’ve also reported	KITTI	KITTI Cars Easy
2D object detection AP on KITTI test set. Evaluation IoU threshold	KITTI	KITTI Cars Easy
the first place winner on KITTI leader board for pedestrians and	KITTI	KITTI Cars Easy
2D object detection AP on KITTI val set	KITTI	KITTI Cars Easy
3D object detection AP on KITTI val set. By using both	KITTI	KITTI Cars Easy
can see that compared with KITTI LiDAR data, depth images can	KITTI	KITTI Cars Easy
3D and bird’s eye view KITTI bench- marks. This detection performance	KITTI	KITTI Cyclists Moderate
PointPillars, PP method on the KITTI [5] test set. Lidar-only methods	KITTI	KITTI Cyclists Moderate
are top methods from the KITTI leader- board: M : MV3D	KITTI	KITTI Cyclists Moderate
PointPillars network on the public KITTI detection challenges which require detection	KITTI	KITTI Cyclists Moderate
We conduct experiments on the KITTI dataset and demonstrate state of	KITTI	KITTI Cyclists Moderate
the range typically used in KITTI for ∼ 97% sparsity. This	KITTI	KITTI Cyclists Moderate
Figure 3. Qualitative analysis of KITTI results. We show a bird’s-eye	KITTI	KITTI Cyclists Moderate
Figure 4. Failure cases on KITTI	KITTI	KITTI Cyclists Moderate
All experiments use the KITTI object detection bench- mark dataset	KITTI	KITTI Cyclists Moderate
the remaining 6733 samples. The KITTI benchmark requires detections of cars	KITTI	KITTI Cyclists Moderate
the standard literature practice on KITTI [11, 31, 28], we train	KITTI	KITTI Cyclists Moderate
Table 1. Results on the KITTI test BEV detection benchmark	KITTI	KITTI Cyclists Moderate
Table 2. Results on the KITTI test 3D detection benchmark	KITTI	KITTI Cyclists Moderate
the KITTI benchmark [28, 30, 2]. First	KITTI	KITTI Cyclists Moderate
mea- sured using the official KITTI evaluation detection metrics which are	KITTI	KITTI Cyclists Moderate
for 2D de- tections. The KITTI dataset is stratified into easy	KITTI	KITTI Cyclists Moderate
hard difficulties, and the official KITTI leaderboard is ranked by performance	KITTI	KITTI Cyclists Moderate
Table 3. Results on the KITTI test average orientation similarity (AOS	KITTI	KITTI Cyclists Moderate
to an artifact of the KITTI ground truth annotations, only lidar	KITTI	KITTI Cyclists Moderate
vs speed (Hz) on the KITTI [5] val set across pedestrians	KITTI	KITTI Cyclists Moderate
measured as BEV mAP on KITTI val. Learned encoders clearly beat	KITTI	KITTI Cyclists Moderate
We demonstrate that on the KITTI chal- lenge, PointPillars dominates all	KITTI	KITTI Cyclists Moderate
for au- tonomous driving? the KITTI vision benchmark suite. In CVPR	KITTI	KITTI Cyclists Moderate
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard DoBEM [42	Hard	KITTI Cyclists Hard
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard DoBEM [42	Hard	KITTI Cyclists Hard
Method Easy Moderate Hard Mono3D [4] 2.53 2.31 2.31	Hard	KITTI Cyclists Hard
Method Easy Moderate Hard Mono3D [4] 5.22 5.19 4.13	Hard	KITTI Cyclists Hard
Benchmark Easy Moderate Hard Pedestrian (3D Detection) 70.00 61.32	Hard	KITTI Cyclists Hard
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard SWC 90.82	Hard	KITTI Cyclists Hard
Subset Easy Moderate Hard AP (2D) for cars 96.48	Hard	KITTI Cyclists Hard
Method Easy Moderate Hard VeloFCN [18] 15.20 13.66 15.98	Hard	KITTI Cyclists Hard
Method Cars Pedestrians Cyclists	Cyclists	KITTI Cyclists Hard
Method Cars Pedestrians Cyclists	Cyclists	KITTI Cyclists Hard
Method Cars Pedestrians Cyclists	Cyclists	KITTI Cyclists Hard
very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection	KITTI	KITTI Cyclists Hard
method achieve leading positions on KITTI 3D ob- ject detection [1	KITTI	KITTI Cyclists Hard
ther fine-tune it on a KITTI 2D object detection dataset to	KITTI	KITTI Cyclists Hard
5m to beyond 50m in KITTI data), we predict the 3D	KITTI	KITTI Cyclists Hard
for 3D object detection on KITTI [10] and SUN-RGBD [33] (Sec	KITTI	KITTI Cyclists Hard
our 3D object detector on KITTI [11] and SUN-RGBD [33] benchmarks	KITTI	KITTI Cyclists Hard
KITTI Tab. 1 shows the performance	KITTI	KITTI Cyclists Hard
our 3D detector on the KITTI test set. We outperform previous	KITTI	KITTI Cyclists Hard
object detection 3D AP on KITTI test set. DoBEM [42] and	KITTI	KITTI Cyclists Hard
AP (bird’s eye view) on KITTI test set. 3D FCN [17	KITTI	KITTI Cyclists Hard
3D object detection AP on KITTI val set (cars only	KITTI	KITTI Cyclists Hard
3D object localization AP on KITTI val set (cars only	KITTI	KITTI Cyclists Hard
We also report performance on KITTI val set (the same split	KITTI	KITTI Cyclists Hard
56.32 Table 5. Performance on KITTI val set for pedestrians and	KITTI	KITTI Cyclists Hard
same pipeline we used for KITTI data set, we’ve achieved state-of-the-art	KITTI	KITTI Cyclists Hard
on our v1 model on KITTI data using train/val split as	KITTI	KITTI Cyclists Hard
of Frustum PointNet results on KITTI val set (best viewed in	KITTI	KITTI Cyclists Hard
typical 2D region proposal from KITTI val set with both 2D	KITTI	KITTI Cyclists Hard
on the fly (1,024 for KITTI and 2,048 for SUN-RGBD). For	KITTI	KITTI Cyclists Hard
KITTI Training The object detection benchmark	KITTI	KITTI Cyclists Hard
in KITTI provides synchronized RGB images and	KITTI	KITTI Cyclists Hard
the same as that in KITTI	KITTI	KITTI Cyclists Hard
much lower than that in KITTI because of strong occlusions and	KITTI	KITTI Cyclists Hard
compared to around 90% in KITTI	KITTI	KITTI Cyclists Hard
pedestrian, and cy- clist from KITTI dataset. The final model takes	KITTI	KITTI Cyclists Hard
our detector’s AP (2D) on KITTI test set. Our detector has	KITTI	KITTI Cyclists Hard
than current leading players on KITTI leader board. We’ve also reported	KITTI	KITTI Cyclists Hard
2D object detection AP on KITTI test set. Evaluation IoU threshold	KITTI	KITTI Cyclists Hard
the first place winner on KITTI leader board for pedestrians and	KITTI	KITTI Cyclists Hard
2D object detection AP on KITTI val set	KITTI	KITTI Cyclists Hard
3D object detection AP on KITTI val set. By using both	KITTI	KITTI Cyclists Hard
can see that compared with KITTI LiDAR data, depth images can	KITTI	KITTI Cyclists Hard
Modality Car Pedestrian CyclistEasy Moderate Hard Easy Moderate Hard Easy Moderate	Hard	KITTI Cyclists Hard
Hard Mono3D [3] Mono 5.22 5.19	Hard	KITTI Cyclists Hard
Modality Car Pedestrian CyclistEasy Moderate Hard Easy Moderate Hard Easy Moderate	Hard	KITTI Cyclists Hard
Hard Mono3D [3] Mono 2.53 2.31	Hard	KITTI Cyclists Hard
Benchmark Easy Moderate Hard Car (3D Detection) 77.47 65.11	Hard	KITTI Cyclists Hard
generate detections. Experiments on the KITTI car detection bench- mark show	KITTI	KITTI Cyclists Hard
detection tasks, provided by the KITTI benchmark [11]. Experimental results show	KITTI	KITTI Cyclists Hard
We conduct experiments on KITTI benchmark and show that VoxelNet	KITTI	KITTI Cyclists Hard
LiDAR specifi- cations of the KITTI dataset [11]. Car Detection For	KITTI	KITTI Cyclists Hard
We evaluate VoxelNet on the KITTI 3D object detection	KITTI	KITTI Cyclists Hard
average precision (in %) on KITTI validation set	KITTI	KITTI Cyclists Hard
average precision (in %) on KITTI validation set	KITTI	KITTI Cyclists Hard
the test results using the KITTI server	KITTI	KITTI Cyclists Hard
the LiDAR data provided in KITTI	KITTI	KITTI Cyclists Hard
4.1. Evaluation on KITTI Validation Set	KITTI	KITTI Cyclists Hard
Metrics We follow the official KITTI evaluation protocol, where the IoU	KITTI	KITTI Cyclists Hard
4.2. Evaluation on KITTI Test Set	KITTI	KITTI Cyclists Hard
We evaluated VoxelNet on the KITTI test set by submit- ting	KITTI	KITTI Cyclists Hard
other leading methods listed in KITTI benchmark use both RGB images	KITTI	KITTI Cyclists Hard
Table 3. Performance evaluation on KITTI test set	KITTI	KITTI Cyclists Hard
grid. Our experiments on the KITTI car detection task show that	KITTI	KITTI Cyclists Hard
3D and bird’s eye view KITTI bench- marks. This detection performance	KITTI	KITTI Cars Moderate
PointPillars, PP method on the KITTI [5] test set. Lidar-only methods	KITTI	KITTI Cars Moderate
are top methods from the KITTI leader- board: M : MV3D	KITTI	KITTI Cars Moderate
PointPillars network on the public KITTI detection challenges which require detection	KITTI	KITTI Cars Moderate
We conduct experiments on the KITTI dataset and demonstrate state of	KITTI	KITTI Cars Moderate
the range typically used in KITTI for ∼ 97% sparsity. This	KITTI	KITTI Cars Moderate
Figure 3. Qualitative analysis of KITTI results. We show a bird’s-eye	KITTI	KITTI Cars Moderate
Figure 4. Failure cases on KITTI	KITTI	KITTI Cars Moderate
All experiments use the KITTI object detection bench- mark dataset	KITTI	KITTI Cars Moderate
the remaining 6733 samples. The KITTI benchmark requires detections of cars	KITTI	KITTI Cars Moderate
the standard literature practice on KITTI [11, 31, 28], we train	KITTI	KITTI Cars Moderate
Table 1. Results on the KITTI test BEV detection benchmark	KITTI	KITTI Cars Moderate
Table 2. Results on the KITTI test 3D detection benchmark	KITTI	KITTI Cars Moderate
the KITTI benchmark [28, 30, 2]. First	KITTI	KITTI Cars Moderate
mea- sured using the official KITTI evaluation detection metrics which are	KITTI	KITTI Cars Moderate
for 2D de- tections. The KITTI dataset is stratified into easy	KITTI	KITTI Cars Moderate
hard difficulties, and the official KITTI leaderboard is ranked by performance	KITTI	KITTI Cars Moderate
Table 3. Results on the KITTI test average orientation similarity (AOS	KITTI	KITTI Cars Moderate
to an artifact of the KITTI ground truth annotations, only lidar	KITTI	KITTI Cars Moderate
vs speed (Hz) on the KITTI [5] val set across pedestrians	KITTI	KITTI Cars Moderate
measured as BEV mAP on KITTI val. Learned encoders clearly beat	KITTI	KITTI Cars Moderate
We demonstrate that on the KITTI chal- lenge, PointPillars dominates all	KITTI	KITTI Cars Moderate
for au- tonomous driving? the KITTI vision benchmark suite. In CVPR	KITTI	KITTI Cars Moderate
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard	Easy	KITTI Pedestrians Easy
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard	Easy	KITTI Pedestrians Easy
Method Easy Moderate Hard Mono3D [4] 2.53	Easy	KITTI Pedestrians Easy
Method Easy Moderate Hard Mono3D [4] 5.22	Easy	KITTI Pedestrians Easy
Benchmark Easy Moderate Hard Pedestrian (3D Detection	Easy	KITTI Pedestrians Easy
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard	Easy	KITTI Pedestrians Easy
Subset Easy Moderate Hard AP (2D) for	Easy	KITTI Pedestrians Easy
Method Easy Moderate Hard VeloFCN [18] 15.20	Easy	KITTI Pedestrians Easy
Method Cars Pedestrians Cyclists	Pedestrians	KITTI Pedestrians Easy
Method Cars Pedestrians Cyclists	Pedestrians	KITTI Pedestrians Easy
Method Cars Pedestrians Cyclists	Pedestrians	KITTI Pedestrians Easy
very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection	KITTI	KITTI Pedestrians Easy
method achieve leading positions on KITTI 3D ob- ject detection [1	KITTI	KITTI Pedestrians Easy
ther fine-tune it on a KITTI 2D object detection dataset to	KITTI	KITTI Pedestrians Easy
5m to beyond 50m in KITTI data), we predict the 3D	KITTI	KITTI Pedestrians Easy
for 3D object detection on KITTI [10] and SUN-RGBD [33] (Sec	KITTI	KITTI Pedestrians Easy
our 3D object detector on KITTI [11] and SUN-RGBD [33] benchmarks	KITTI	KITTI Pedestrians Easy
KITTI Tab. 1 shows the performance	KITTI	KITTI Pedestrians Easy
our 3D detector on the KITTI test set. We outperform previous	KITTI	KITTI Pedestrians Easy
object detection 3D AP on KITTI test set. DoBEM [42] and	KITTI	KITTI Pedestrians Easy
AP (bird’s eye view) on KITTI test set. 3D FCN [17	KITTI	KITTI Pedestrians Easy
3D object detection AP on KITTI val set (cars only	KITTI	KITTI Pedestrians Easy
3D object localization AP on KITTI val set (cars only	KITTI	KITTI Pedestrians Easy
We also report performance on KITTI val set (the same split	KITTI	KITTI Pedestrians Easy
56.32 Table 5. Performance on KITTI val set for pedestrians and	KITTI	KITTI Pedestrians Easy
same pipeline we used for KITTI data set, we’ve achieved state-of-the-art	KITTI	KITTI Pedestrians Easy
on our v1 model on KITTI data using train/val split as	KITTI	KITTI Pedestrians Easy
of Frustum PointNet results on KITTI val set (best viewed in	KITTI	KITTI Pedestrians Easy
typical 2D region proposal from KITTI val set with both 2D	KITTI	KITTI Pedestrians Easy
on the fly (1,024 for KITTI and 2,048 for SUN-RGBD). For	KITTI	KITTI Pedestrians Easy
KITTI Training The object detection benchmark	KITTI	KITTI Pedestrians Easy
in KITTI provides synchronized RGB images and	KITTI	KITTI Pedestrians Easy
the same as that in KITTI	KITTI	KITTI Pedestrians Easy
much lower than that in KITTI because of strong occlusions and	KITTI	KITTI Pedestrians Easy
compared to around 90% in KITTI	KITTI	KITTI Pedestrians Easy
pedestrian, and cy- clist from KITTI dataset. The final model takes	KITTI	KITTI Pedestrians Easy
our detector’s AP (2D) on KITTI test set. Our detector has	KITTI	KITTI Pedestrians Easy
than current leading players on KITTI leader board. We’ve also reported	KITTI	KITTI Pedestrians Easy
2D object detection AP on KITTI test set. Evaluation IoU threshold	KITTI	KITTI Pedestrians Easy
the first place winner on KITTI leader board for pedestrians and	KITTI	KITTI Pedestrians Easy
2D object detection AP on KITTI val set	KITTI	KITTI Pedestrians Easy
3D object detection AP on KITTI val set. By using both	KITTI	KITTI Pedestrians Easy
can see that compared with KITTI LiDAR data, depth images can	KITTI	KITTI Pedestrians Easy
Car Pedestrian CyclistEasy Moderate Hard Easy Moderate Hard Easy Moderate Hard	Easy	KITTI Pedestrians Easy
Car Pedestrian CyclistEasy Moderate Hard Easy Moderate Hard Easy Moderate Hard	Easy	KITTI Pedestrians Easy
Benchmark Easy Moderate Hard Car (3D Detection	Easy	KITTI Pedestrians Easy
generate detections. Experiments on the KITTI car detection bench- mark show	KITTI	KITTI Pedestrians Easy
detection tasks, provided by the KITTI benchmark [11]. Experimental results show	KITTI	KITTI Pedestrians Easy
We conduct experiments on KITTI benchmark and show that VoxelNet	KITTI	KITTI Pedestrians Easy
LiDAR specifi- cations of the KITTI dataset [11]. Car Detection For	KITTI	KITTI Pedestrians Easy
We evaluate VoxelNet on the KITTI 3D object detection	KITTI	KITTI Pedestrians Easy
average precision (in %) on KITTI validation set	KITTI	KITTI Pedestrians Easy
average precision (in %) on KITTI validation set	KITTI	KITTI Pedestrians Easy
the test results using the KITTI server	KITTI	KITTI Pedestrians Easy
the LiDAR data provided in KITTI	KITTI	KITTI Pedestrians Easy
4.1. Evaluation on KITTI Validation Set	KITTI	KITTI Pedestrians Easy
Metrics We follow the official KITTI evaluation protocol, where the IoU	KITTI	KITTI Pedestrians Easy
4.2. Evaluation on KITTI Test Set	KITTI	KITTI Pedestrians Easy
We evaluated VoxelNet on the KITTI test set by submit- ting	KITTI	KITTI Pedestrians Easy
other leading methods listed in KITTI benchmark use both RGB images	KITTI	KITTI Pedestrians Easy
Table 3. Performance evaluation on KITTI test set	KITTI	KITTI Pedestrians Easy
grid. Our experiments on the KITTI car detection task show that	KITTI	KITTI Pedestrians Easy
Method Cars Pedestrians Cyclists	Cyclists	KITTI Cyclists Easy
Method Cars Pedestrians Cyclists	Cyclists	KITTI Cyclists Easy
Method Cars Pedestrians Cyclists	Cyclists	KITTI Cyclists Easy
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard	Easy	KITTI Cyclists Easy
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard	Easy	KITTI Cyclists Easy
Method Easy Moderate Hard Mono3D [4] 2.53	Easy	KITTI Cyclists Easy
Method Easy Moderate Hard Mono3D [4] 5.22	Easy	KITTI Cyclists Easy
Benchmark Easy Moderate Hard Pedestrian (3D Detection	Easy	KITTI Cyclists Easy
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard	Easy	KITTI Cyclists Easy
Subset Easy Moderate Hard AP (2D) for	Easy	KITTI Cyclists Easy
Method Easy Moderate Hard VeloFCN [18] 15.20	Easy	KITTI Cyclists Easy
very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection	KITTI	KITTI Cyclists Easy
method achieve leading positions on KITTI 3D ob- ject detection [1	KITTI	KITTI Cyclists Easy
ther fine-tune it on a KITTI 2D object detection dataset to	KITTI	KITTI Cyclists Easy
5m to beyond 50m in KITTI data), we predict the 3D	KITTI	KITTI Cyclists Easy
for 3D object detection on KITTI [10] and SUN-RGBD [33] (Sec	KITTI	KITTI Cyclists Easy
our 3D object detector on KITTI [11] and SUN-RGBD [33] benchmarks	KITTI	KITTI Cyclists Easy
KITTI Tab. 1 shows the performance	KITTI	KITTI Cyclists Easy
our 3D detector on the KITTI test set. We outperform previous	KITTI	KITTI Cyclists Easy
object detection 3D AP on KITTI test set. DoBEM [42] and	KITTI	KITTI Cyclists Easy
AP (bird’s eye view) on KITTI test set. 3D FCN [17	KITTI	KITTI Cyclists Easy
3D object detection AP on KITTI val set (cars only	KITTI	KITTI Cyclists Easy
3D object localization AP on KITTI val set (cars only	KITTI	KITTI Cyclists Easy
We also report performance on KITTI val set (the same split	KITTI	KITTI Cyclists Easy
56.32 Table 5. Performance on KITTI val set for pedestrians and	KITTI	KITTI Cyclists Easy
same pipeline we used for KITTI data set, we’ve achieved state-of-the-art	KITTI	KITTI Cyclists Easy
on our v1 model on KITTI data using train/val split as	KITTI	KITTI Cyclists Easy
of Frustum PointNet results on KITTI val set (best viewed in	KITTI	KITTI Cyclists Easy
typical 2D region proposal from KITTI val set with both 2D	KITTI	KITTI Cyclists Easy
on the fly (1,024 for KITTI and 2,048 for SUN-RGBD). For	KITTI	KITTI Cyclists Easy
KITTI Training The object detection benchmark	KITTI	KITTI Cyclists Easy
in KITTI provides synchronized RGB images and	KITTI	KITTI Cyclists Easy
the same as that in KITTI	KITTI	KITTI Cyclists Easy
much lower than that in KITTI because of strong occlusions and	KITTI	KITTI Cyclists Easy
compared to around 90% in KITTI	KITTI	KITTI Cyclists Easy
pedestrian, and cy- clist from KITTI dataset. The final model takes	KITTI	KITTI Cyclists Easy
our detector’s AP (2D) on KITTI test set. Our detector has	KITTI	KITTI Cyclists Easy
than current leading players on KITTI leader board. We’ve also reported	KITTI	KITTI Cyclists Easy
2D object detection AP on KITTI test set. Evaluation IoU threshold	KITTI	KITTI Cyclists Easy
the first place winner on KITTI leader board for pedestrians and	KITTI	KITTI Cyclists Easy
2D object detection AP on KITTI val set	KITTI	KITTI Cyclists Easy
3D object detection AP on KITTI val set. By using both	KITTI	KITTI Cyclists Easy
can see that compared with KITTI LiDAR data, depth images can	KITTI	KITTI Cyclists Easy
Car Pedestrian CyclistEasy Moderate Hard Easy Moderate Hard Easy Moderate Hard	Easy	KITTI Cyclists Easy
Car Pedestrian CyclistEasy Moderate Hard Easy Moderate Hard Easy Moderate Hard	Easy	KITTI Cyclists Easy
Benchmark Easy Moderate Hard Car (3D Detection	Easy	KITTI Cyclists Easy
generate detections. Experiments on the KITTI car detection bench- mark show	KITTI	KITTI Cyclists Easy
detection tasks, provided by the KITTI benchmark [11]. Experimental results show	KITTI	KITTI Cyclists Easy
We conduct experiments on KITTI benchmark and show that VoxelNet	KITTI	KITTI Cyclists Easy
LiDAR specifi- cations of the KITTI dataset [11]. Car Detection For	KITTI	KITTI Cyclists Easy
We evaluate VoxelNet on the KITTI 3D object detection	KITTI	KITTI Cyclists Easy
average precision (in %) on KITTI validation set	KITTI	KITTI Cyclists Easy
average precision (in %) on KITTI validation set	KITTI	KITTI Cyclists Easy
the test results using the KITTI server	KITTI	KITTI Cyclists Easy
the LiDAR data provided in KITTI	KITTI	KITTI Cyclists Easy
4.1. Evaluation on KITTI Validation Set	KITTI	KITTI Cyclists Easy
Metrics We follow the official KITTI evaluation protocol, where the IoU	KITTI	KITTI Cyclists Easy
4.2. Evaluation on KITTI Test Set	KITTI	KITTI Cyclists Easy
We evaluated VoxelNet on the KITTI test set by submit- ting	KITTI	KITTI Cyclists Easy
other leading methods listed in KITTI benchmark use both RGB images	KITTI	KITTI Cyclists Easy
Table 3. Performance evaluation on KITTI test set	KITTI	KITTI Cyclists Easy
grid. Our experiments on the KITTI car detection task show that	KITTI	KITTI Cyclists Easy
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard DoBEM	Moderate	KITTI Pedestrians Moderate
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard DoBEM	Moderate	KITTI Pedestrians Moderate
Method Easy Moderate Hard Mono3D [4] 2.53 2.31	Moderate	KITTI Pedestrians Moderate
Method Easy Moderate Hard Mono3D [4] 5.22 5.19	Moderate	KITTI Pedestrians Moderate
Benchmark Easy Moderate Hard Pedestrian (3D Detection) 70.00	Moderate	KITTI Pedestrians Moderate
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard SWC	Moderate	KITTI Pedestrians Moderate
Subset Easy Moderate Hard AP (2D) for cars	Moderate	KITTI Pedestrians Moderate
Method Easy Moderate Hard VeloFCN [18] 15.20 13.66	Moderate	KITTI Pedestrians Moderate
Method Cars Pedestrians Cyclists	Pedestrians	KITTI Pedestrians Moderate
Method Cars Pedestrians Cyclists	Pedestrians	KITTI Pedestrians Moderate
Method Cars Pedestrians Cyclists	Pedestrians	KITTI Pedestrians Moderate
very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection	KITTI	KITTI Pedestrians Moderate
method achieve leading positions on KITTI 3D ob- ject detection [1	KITTI	KITTI Pedestrians Moderate
ther fine-tune it on a KITTI 2D object detection dataset to	KITTI	KITTI Pedestrians Moderate
5m to beyond 50m in KITTI data), we predict the 3D	KITTI	KITTI Pedestrians Moderate
for 3D object detection on KITTI [10] and SUN-RGBD [33] (Sec	KITTI	KITTI Pedestrians Moderate
our 3D object detector on KITTI [11] and SUN-RGBD [33] benchmarks	KITTI	KITTI Pedestrians Moderate
KITTI Tab. 1 shows the performance	KITTI	KITTI Pedestrians Moderate
our 3D detector on the KITTI test set. We outperform previous	KITTI	KITTI Pedestrians Moderate
object detection 3D AP on KITTI test set. DoBEM [42] and	KITTI	KITTI Pedestrians Moderate
AP (bird’s eye view) on KITTI test set. 3D FCN [17	KITTI	KITTI Pedestrians Moderate
3D object detection AP on KITTI val set (cars only	KITTI	KITTI Pedestrians Moderate
3D object localization AP on KITTI val set (cars only	KITTI	KITTI Pedestrians Moderate
We also report performance on KITTI val set (the same split	KITTI	KITTI Pedestrians Moderate
56.32 Table 5. Performance on KITTI val set for pedestrians and	KITTI	KITTI Pedestrians Moderate
same pipeline we used for KITTI data set, we’ve achieved state-of-the-art	KITTI	KITTI Pedestrians Moderate
on our v1 model on KITTI data using train/val split as	KITTI	KITTI Pedestrians Moderate
of Frustum PointNet results on KITTI val set (best viewed in	KITTI	KITTI Pedestrians Moderate
typical 2D region proposal from KITTI val set with both 2D	KITTI	KITTI Pedestrians Moderate
on the fly (1,024 for KITTI and 2,048 for SUN-RGBD). For	KITTI	KITTI Pedestrians Moderate
KITTI Training The object detection benchmark	KITTI	KITTI Pedestrians Moderate
in KITTI provides synchronized RGB images and	KITTI	KITTI Pedestrians Moderate
the same as that in KITTI	KITTI	KITTI Pedestrians Moderate
much lower than that in KITTI because of strong occlusions and	KITTI	KITTI Pedestrians Moderate
compared to around 90% in KITTI	KITTI	KITTI Pedestrians Moderate
pedestrian, and cy- clist from KITTI dataset. The final model takes	KITTI	KITTI Pedestrians Moderate
our detector’s AP (2D) on KITTI test set. Our detector has	KITTI	KITTI Pedestrians Moderate
than current leading players on KITTI leader board. We’ve also reported	KITTI	KITTI Pedestrians Moderate
2D object detection AP on KITTI test set. Evaluation IoU threshold	KITTI	KITTI Pedestrians Moderate
the first place winner on KITTI leader board for pedestrians and	KITTI	KITTI Pedestrians Moderate
2D object detection AP on KITTI val set	KITTI	KITTI Pedestrians Moderate
3D object detection AP on KITTI val set. By using both	KITTI	KITTI Pedestrians Moderate
can see that compared with KITTI LiDAR data, depth images can	KITTI	KITTI Pedestrians Moderate
Method Modality Car Pedestrian CyclistEasy Moderate Hard Easy Moderate Hard Easy	Moderate	KITTI Pedestrians Moderate
Moderate Hard Mono3D [3] Mono 5.22	Moderate	KITTI Pedestrians Moderate
Method Modality Car Pedestrian CyclistEasy Moderate Hard Easy Moderate Hard Easy	Moderate	KITTI Pedestrians Moderate
Moderate Hard Mono3D [3] Mono 2.53	Moderate	KITTI Pedestrians Moderate
Benchmark Easy Moderate Hard Car (3D Detection) 77.47	Moderate	KITTI Pedestrians Moderate
generate detections. Experiments on the KITTI car detection bench- mark show	KITTI	KITTI Pedestrians Moderate
detection tasks, provided by the KITTI benchmark [11]. Experimental results show	KITTI	KITTI Pedestrians Moderate
We conduct experiments on KITTI benchmark and show that VoxelNet	KITTI	KITTI Pedestrians Moderate
LiDAR specifi- cations of the KITTI dataset [11]. Car Detection For	KITTI	KITTI Pedestrians Moderate
We evaluate VoxelNet on the KITTI 3D object detection	KITTI	KITTI Pedestrians Moderate
average precision (in %) on KITTI validation set	KITTI	KITTI Pedestrians Moderate
average precision (in %) on KITTI validation set	KITTI	KITTI Pedestrians Moderate
the test results using the KITTI server	KITTI	KITTI Pedestrians Moderate
the LiDAR data provided in KITTI	KITTI	KITTI Pedestrians Moderate
4.1. Evaluation on KITTI Validation Set	KITTI	KITTI Pedestrians Moderate
Metrics We follow the official KITTI evaluation protocol, where the IoU	KITTI	KITTI Pedestrians Moderate
4.2. Evaluation on KITTI Test Set	KITTI	KITTI Pedestrians Moderate
We evaluated VoxelNet on the KITTI test set by submit- ting	KITTI	KITTI Pedestrians Moderate
other leading methods listed in KITTI benchmark use both RGB images	KITTI	KITTI Pedestrians Moderate
Table 3. Performance evaluation on KITTI test set	KITTI	KITTI Pedestrians Moderate
grid. Our experiments on the KITTI car detection task show that	KITTI	KITTI Pedestrians Moderate
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard DoBEM [42	Hard	KITTI Pedestrians Hard
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard DoBEM [42	Hard	KITTI Pedestrians Hard
Method Easy Moderate Hard Mono3D [4] 2.53 2.31 2.31	Hard	KITTI Pedestrians Hard
Method Easy Moderate Hard Mono3D [4] 5.22 5.19 4.13	Hard	KITTI Pedestrians Hard
Benchmark Easy Moderate Hard Pedestrian (3D Detection) 70.00 61.32	Hard	KITTI Pedestrians Hard
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard SWC 90.82	Hard	KITTI Pedestrians Hard
Subset Easy Moderate Hard AP (2D) for cars 96.48	Hard	KITTI Pedestrians Hard
Method Easy Moderate Hard VeloFCN [18] 15.20 13.66 15.98	Hard	KITTI Pedestrians Hard
Method Cars Pedestrians Cyclists	Pedestrians	KITTI Pedestrians Hard
Method Cars Pedestrians Cyclists	Pedestrians	KITTI Pedestrians Hard
Method Cars Pedestrians Cyclists	Pedestrians	KITTI Pedestrians Hard
very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection	KITTI	KITTI Pedestrians Hard
method achieve leading positions on KITTI 3D ob- ject detection [1	KITTI	KITTI Pedestrians Hard
ther fine-tune it on a KITTI 2D object detection dataset to	KITTI	KITTI Pedestrians Hard
5m to beyond 50m in KITTI data), we predict the 3D	KITTI	KITTI Pedestrians Hard
for 3D object detection on KITTI [10] and SUN-RGBD [33] (Sec	KITTI	KITTI Pedestrians Hard
our 3D object detector on KITTI [11] and SUN-RGBD [33] benchmarks	KITTI	KITTI Pedestrians Hard
KITTI Tab. 1 shows the performance	KITTI	KITTI Pedestrians Hard
our 3D detector on the KITTI test set. We outperform previous	KITTI	KITTI Pedestrians Hard
object detection 3D AP on KITTI test set. DoBEM [42] and	KITTI	KITTI Pedestrians Hard
AP (bird’s eye view) on KITTI test set. 3D FCN [17	KITTI	KITTI Pedestrians Hard
3D object detection AP on KITTI val set (cars only	KITTI	KITTI Pedestrians Hard
3D object localization AP on KITTI val set (cars only	KITTI	KITTI Pedestrians Hard
We also report performance on KITTI val set (the same split	KITTI	KITTI Pedestrians Hard
56.32 Table 5. Performance on KITTI val set for pedestrians and	KITTI	KITTI Pedestrians Hard
same pipeline we used for KITTI data set, we’ve achieved state-of-the-art	KITTI	KITTI Pedestrians Hard
on our v1 model on KITTI data using train/val split as	KITTI	KITTI Pedestrians Hard
of Frustum PointNet results on KITTI val set (best viewed in	KITTI	KITTI Pedestrians Hard
typical 2D region proposal from KITTI val set with both 2D	KITTI	KITTI Pedestrians Hard
on the fly (1,024 for KITTI and 2,048 for SUN-RGBD). For	KITTI	KITTI Pedestrians Hard
KITTI Training The object detection benchmark	KITTI	KITTI Pedestrians Hard
in KITTI provides synchronized RGB images and	KITTI	KITTI Pedestrians Hard
the same as that in KITTI	KITTI	KITTI Pedestrians Hard
much lower than that in KITTI because of strong occlusions and	KITTI	KITTI Pedestrians Hard
compared to around 90% in KITTI	KITTI	KITTI Pedestrians Hard
pedestrian, and cy- clist from KITTI dataset. The final model takes	KITTI	KITTI Pedestrians Hard
our detector’s AP (2D) on KITTI test set. Our detector has	KITTI	KITTI Pedestrians Hard
than current leading players on KITTI leader board. We’ve also reported	KITTI	KITTI Pedestrians Hard
2D object detection AP on KITTI test set. Evaluation IoU threshold	KITTI	KITTI Pedestrians Hard
the first place winner on KITTI leader board for pedestrians and	KITTI	KITTI Pedestrians Hard
2D object detection AP on KITTI val set	KITTI	KITTI Pedestrians Hard
3D object detection AP on KITTI val set. By using both	KITTI	KITTI Pedestrians Hard
can see that compared with KITTI LiDAR data, depth images can	KITTI	KITTI Pedestrians Hard
Modality Car Pedestrian CyclistEasy Moderate Hard Easy Moderate Hard Easy Moderate	Hard	KITTI Pedestrians Hard
Hard Mono3D [3] Mono 5.22 5.19	Hard	KITTI Pedestrians Hard
Modality Car Pedestrian CyclistEasy Moderate Hard Easy Moderate Hard Easy Moderate	Hard	KITTI Pedestrians Hard
Hard Mono3D [3] Mono 2.53 2.31	Hard	KITTI Pedestrians Hard
Benchmark Easy Moderate Hard Car (3D Detection) 77.47 65.11	Hard	KITTI Pedestrians Hard
generate detections. Experiments on the KITTI car detection bench- mark show	KITTI	KITTI Pedestrians Hard
detection tasks, provided by the KITTI benchmark [11]. Experimental results show	KITTI	KITTI Pedestrians Hard
We conduct experiments on KITTI benchmark and show that VoxelNet	KITTI	KITTI Pedestrians Hard
LiDAR specifi- cations of the KITTI dataset [11]. Car Detection For	KITTI	KITTI Pedestrians Hard
We evaluate VoxelNet on the KITTI 3D object detection	KITTI	KITTI Pedestrians Hard
average precision (in %) on KITTI validation set	KITTI	KITTI Pedestrians Hard
average precision (in %) on KITTI validation set	KITTI	KITTI Pedestrians Hard
the test results using the KITTI server	KITTI	KITTI Pedestrians Hard
the LiDAR data provided in KITTI	KITTI	KITTI Pedestrians Hard
4.1. Evaluation on KITTI Validation Set	KITTI	KITTI Pedestrians Hard
Metrics We follow the official KITTI evaluation protocol, where the IoU	KITTI	KITTI Pedestrians Hard
4.2. Evaluation on KITTI Test Set	KITTI	KITTI Pedestrians Hard
We evaluated VoxelNet on the KITTI test set by submit- ting	KITTI	KITTI Pedestrians Hard
other leading methods listed in KITTI benchmark use both RGB images	KITTI	KITTI Pedestrians Hard
Table 3. Performance evaluation on KITTI test set	KITTI	KITTI Pedestrians Hard
grid. Our experiments on the KITTI car detection task show that	KITTI	KITTI Pedestrians Hard
dis- cover various relations, including domain	domain	Medical domain
produced better results on a domain	domain	Medical domain
supervised baseline trained on the domain	domain	Medical domain
10Several of the domain	domain	Medical domain
illustrates this in the language domain, where sparse features are inter	domain	Medical domain
dings spaces, and integrating a domain clus- tering algorithm, our model	domain	Medical domain
one hand, formally representing a domain of knowledge (e.g. Food), and	domain	Medical domain
In domain knowledge formalization, prominent work has	domain	Medical domain
aware transformation matrix for each domain of knowledge. Our best configuration	domain	Medical domain
semantically related terms for collecting domain	domain	Medical domain
take ad- vantage of a domain	domain	Medical domain
the training set into a domain cluster C (Section 4.1). Then	domain	Medical domain
sensitive to a predefined knowledge domain, under the assump- tion that	domain	Medical domain
synset into its most representative domain, which is achieved by exploiting	domain	Medical domain
Wikipedia featured articles page each domain is composed of 128 Wikipedia	domain	Medical domain
concepts as- sociated with each domain, we leverage NASARI10	domain	Medical domain
lexical vector for each Wikipedia domain by concatenating all Wikipedia pages	domain	Medical domain
representing the given domain into a single text. Finally	domain	Medical domain
lexical vector and all the domain vectors, selecting the domain leading	domain	Medical domain
is the vector of the domain d ∈ D, ~b is	domain	Medical domain
a highly reliable set of domain labels, those	domain	Medical domain
synsets are labelled with a domain	domain	Medical domain
BabelNet synsets labelled with the domain d. For each domain-wise expanded	domain	Medical domain
a transformation matrix for each domain cluster Cd by minimizing	domain	Medical domain
For each domain, we retain 5k, 10k, 15k	domain	Medical domain
extra OIE-derived training pairs per domain (gen- erating two more systems	domain	Medical domain
pair (~xd,~yd) of a given domain d. Then, we aver- age	domain	Medical domain
global vector ~Vd for the domain d. Finally, given a test	domain	Medical domain
11Using the 25k domain	domain	Medical domain
main clusters and metrics, with domain	domain	Medical domain
domains are concerned, the biology domain seems to be easier to	domain	Medical domain
100krwd configuration performs on this domain	domain	Medical domain
. This is the only domain in which training with no	domain	Medical domain
illustrative example from the transport domain	domain	Medical domain
P@k scores for the transport domain	domain	Medical domain
our best run in each domain, and estimated precision over them	domain	Medical domain
the first one includes 25k domain	domain	Medical domain
test BabelNet synsets (20 per domain) whose hypernyms are missing in	domain	Medical domain
treatment planning in the health domain or decoration for molding in	domain	Medical domain
the art domain)14	domain	Medical domain
there exists, for a given domain	domain	Medical domain
BabelNet synsets into a predefined domain of knowledge. Then, we collect	domain	Medical domain
increase training data. Our best domain	domain	Medical domain
we see potential in the domain clustering approach for im- proving	domain	Medical domain
taxonomy are for a specific domain	domain	Medical domain
ex- tending, taxonomizing and semantifying domain ter- minologies. AAAI	domain	Medical domain
domain textual question an- swering techniques	domain	Medical domain
dings spaces, and integrating a domain clus- tering algorithm, our model	domain	Medical domain
one hand, formally representing a domain of knowledge (e.g. Food), and	domain	Medical domain
In domain knowledge formalization, prominent work has	domain	Medical domain
aware transformation matrix for each domain of knowledge. Our best configuration	domain	Medical domain
semantically related terms for collecting domain	domain	Medical domain
take ad- vantage of a domain	domain	Medical domain
the training set into a domain cluster C (Section 4.1). Then	domain	Medical domain
sensitive to a predefined knowledge domain, under the assump- tion that	domain	Medical domain
synset into its most representative domain, which is achieved by exploiting	domain	Medical domain
Wikipedia featured articles page each domain is composed of 128 Wikipedia	domain	Medical domain
concepts as- sociated with each domain, we leverage NASARI10	domain	Medical domain
lexical vector for each Wikipedia domain by concatenating all Wikipedia pages	domain	Medical domain
representing the given domain into a single text. Finally	domain	Medical domain
lexical vector and all the domain vectors, selecting the domain leading	domain	Medical domain
is the vector of the domain d ∈ D, ~b is	domain	Medical domain
a highly reliable set of domain labels, those	domain	Medical domain
synsets are labelled with a domain	domain	Medical domain
BabelNet synsets labelled with the domain d. For each domain-wise expanded	domain	Medical domain
a transformation matrix for each domain cluster Cd by minimizing	domain	Medical domain
For each domain, we retain 5k, 10k, 15k	domain	Medical domain
extra OIE-derived training pairs per domain (gen- erating two more systems	domain	Medical domain
pair (~xd,~yd) of a given domain d. Then, we aver- age	domain	Medical domain
global vector ~Vd for the domain d. Finally, given a test	domain	Medical domain
11Using the 25k domain	domain	Medical domain
main clusters and metrics, with domain	domain	Medical domain
domains are concerned, the biology domain seems to be easier to	domain	Medical domain
100krwd configuration performs on this domain	domain	Medical domain
. This is the only domain in which training with no	domain	Medical domain
illustrative example from the transport domain	domain	Medical domain
P@k scores for the transport domain	domain	Medical domain
our best run in each domain, and estimated precision over them	domain	Medical domain
the first one includes 25k domain	domain	Medical domain
test BabelNet synsets (20 per domain) whose hypernyms are missing in	domain	Medical domain
treatment planning in the health domain or decoration for molding in	domain	Medical domain
the art domain)14	domain	Medical domain
there exists, for a given domain	domain	Medical domain
BabelNet synsets into a predefined domain of knowledge. Then, we collect	domain	Medical domain
increase training data. Our best domain	domain	Medical domain
we see potential in the domain clustering approach for im- proving	domain	Medical domain
taxonomy are for a specific domain	domain	Medical domain
ex- tending, taxonomizing and semantifying domain ter- minologies. AAAI	domain	Medical domain
domain textual question an- swering techniques	domain	Medical domain
task including the general-purpose and domain	domain	Medical domain
general-purpose substask for English and domain	domain	Medical domain
subtask for English and 0.5h domain	domain	Medical domain
-specific domain	domain	Medical domain
domain sub- task for English. All	domain	Medical domain
3: Gold standard evaluation on domain	domain	Medical domain
sense embedding, showing that in domain	domain	Medical domain
ex- tending, taxonomizing and semantifying domain terminologies. In Proceedings of the	domain	Medical domain
domain ques- tion answering. In Proceedings	domain	Medical domain
scored highest in the medical domain among the competing unsu- pervised	domain	Medical domain
poorly on the music industry domain	domain	Medical domain
the vocabulary of a specific domain (Espinosa-Anke et al., 2016). This	domain	Medical domain
main vocabulary and two specialised domain vo- cabularies in English: medical	domain	Medical domain
English vo- cabularies, general language domain vocabularies for Spanish and Italian	domain	Medical domain
show that for the medical domain subtask, our system beats the	domain	Medical domain
of eighteen on the medical domain subtask with a Mean Average	domain	Medical domain
On the music indus- try domain subtask, our system ranked 13th	domain	Medical domain
representations. 82% of the medical domain input words have at least	domain	Medical domain
92% of the music industry domain input words have multi-word ex	domain	Medical domain
system scored highest in the medical domain among the competing unsu- pervised	medical domain	Medical domain
we show that for the medical domain subtask, our system beats the	medical domain	Medical domain
out of eighteen on the medical domain subtask with a Mean Average	medical domain	Medical domain
tor representations. 82% of the medical domain input words have at least	medical domain	Medical domain
constructed by crawling the .uk domain, and WaCkypedia EN (Baroni et	domain	Medical domain
dis- cover various relations, including domain	domain	Music domain
produced better results on a domain	domain	Music domain
supervised baseline trained on the domain	domain	Music domain
10Several of the domain	domain	Music domain
illustrates this in the language domain, where sparse features are inter	domain	Music domain
dings spaces, and integrating a domain clus- tering algorithm, our model	domain	Music domain
one hand, formally representing a domain of knowledge (e.g. Food), and	domain	Music domain
In domain knowledge formalization, prominent work has	domain	Music domain
aware transformation matrix for each domain of knowledge. Our best configuration	domain	Music domain
semantically related terms for collecting domain	domain	Music domain
take ad- vantage of a domain	domain	Music domain
the training set into a domain cluster C (Section 4.1). Then	domain	Music domain
sensitive to a predefined knowledge domain, under the assump- tion that	domain	Music domain
synset into its most representative domain, which is achieved by exploiting	domain	Music domain
Wikipedia featured articles page each domain is composed of 128 Wikipedia	domain	Music domain
concepts as- sociated with each domain, we leverage NASARI10	domain	Music domain
lexical vector for each Wikipedia domain by concatenating all Wikipedia pages	domain	Music domain
representing the given domain into a single text. Finally	domain	Music domain
lexical vector and all the domain vectors, selecting the domain leading	domain	Music domain
is the vector of the domain d ∈ D, ~b is	domain	Music domain
a highly reliable set of domain labels, those	domain	Music domain
synsets are labelled with a domain	domain	Music domain
BabelNet synsets labelled with the domain d. For each domain-wise expanded	domain	Music domain
a transformation matrix for each domain cluster Cd by minimizing	domain	Music domain
For each domain, we retain 5k, 10k, 15k	domain	Music domain
extra OIE-derived training pairs per domain (gen- erating two more systems	domain	Music domain
pair (~xd,~yd) of a given domain d. Then, we aver- age	domain	Music domain
global vector ~Vd for the domain d. Finally, given a test	domain	Music domain
11Using the 25k domain	domain	Music domain
main clusters and metrics, with domain	domain	Music domain
domains are concerned, the biology domain seems to be easier to	domain	Music domain
100krwd configuration performs on this domain	domain	Music domain
. This is the only domain in which training with no	domain	Music domain
illustrative example from the transport domain	domain	Music domain
P@k scores for the transport domain	domain	Music domain
our best run in each domain, and estimated precision over them	domain	Music domain
the first one includes 25k domain	domain	Music domain
test BabelNet synsets (20 per domain) whose hypernyms are missing in	domain	Music domain
treatment planning in the health domain or decoration for molding in	domain	Music domain
the art domain)14	domain	Music domain
there exists, for a given domain	domain	Music domain
BabelNet synsets into a predefined domain of knowledge. Then, we collect	domain	Music domain
increase training data. Our best domain	domain	Music domain
we see potential in the domain clustering approach for im- proving	domain	Music domain
taxonomy are for a specific domain	domain	Music domain
ex- tending, taxonomizing and semantifying domain ter- minologies. AAAI	domain	Music domain
domain textual question an- swering techniques	domain	Music domain
task including the general-purpose and domain	domain	Music domain
general-purpose substask for English and domain	domain	Music domain
subtask for English and 0.5h domain	domain	Music domain
-specific domain	domain	Music domain
domain sub- task for English. All	domain	Music domain
3: Gold standard evaluation on domain	domain	Music domain
sense embedding, showing that in domain	domain	Music domain
ex- tending, taxonomizing and semantifying domain terminologies. In Proceedings of the	domain	Music domain
domain ques- tion answering. In Proceedings	domain	Music domain
scored highest in the medical domain among the competing unsu- pervised	domain	Music domain
poorly on the music industry domain	domain	Music domain
the vocabulary of a specific domain (Espinosa-Anke et al., 2016). This	domain	Music domain
main vocabulary and two specialised domain vo- cabularies in English: medical	domain	Music domain
English vo- cabularies, general language domain vocabularies for Spanish and Italian	domain	Music domain
show that for the medical domain subtask, our system beats the	domain	Music domain
of eighteen on the medical domain subtask with a Mean Average	domain	Music domain
On the music indus- try domain subtask, our system ranked 13th	domain	Music domain
representations. 82% of the medical domain input words have at least	domain	Music domain
92% of the music industry domain input words have multi-word ex	domain	Music domain
constructed by crawling the .uk domain, and WaCkypedia EN (Baroni et	domain	Music domain
hypernym discovery (Camacho-Collados, 2017). The general idea is to learn a	general	General
of the 5 sub-tasks: 1A (general), 2A (medical), and 2B (music	general	General
so training our system on general	general	General
Evaluation shows that the general trend is that our hypernym	general	General
and David Weir. 2003. A general frame- work for distributional similarity	general	General
general	general	General
tend to refer to more general concepts and more general hypernymy	general	General
the most frequent hypernyms in general	general	General
hyperym discovery task including the general	general	General
Our hypernym discovery experiments include general	general	General
980Ti), with roughly 1.5h for general	general	General
2 shows the result on general	general	General
2: Gold standard evaluation on general	general	General
verifies our hypothesis in the general	general	General
though they work closely in general	general	General
relia- bility. Being based on general linguistic hypotheses and independent from	general	General
easily interpretable, being based on general linguistic hypotheses	general	General
likely to occur in more general contexts than their hyponyms	general	General
fact that, even though—being more general	general	General
cases the holonym is more general than the meronym (Shwartz et	general	General
should therefore not be too general	general	General
which the holonym is more general than the meronym by definition	general	General
that y is a more general word than x, which is	general	General
y is also not more general than x. We observed that	general	General
ys, many of which are general words like animal and object	general	General
of these pairs re- veals general words that appear in many	general	General
2010. Distri- butional memory: A general framework for corpus- based semantics	general	General
babi	babi	bAbi
5.2 BABI TASKS	BABI	bAbi
C ADDITIONAL RESULTS ON BABI TASKS	BABI	bAbi
different memory hops for the bAbi tasks. The model is PE+LS+RN	bAbi	bAbi
C QUESTION ANSWERING BABI TASK	BABI	bAbi
5.1 BABI STORY-BASED QA	BABI	bAbi
5.2 BABI DIALOG	BABI	bAbi
VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 32-shot learning
ablation studies, while by using VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 32-shot learning
our method on a larger VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 32-shot learning
from test videos of the VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 32-shot learning
our best models on the VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 32-shot learning
poses were taken from the VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 32-shot learning
was trained only for the VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 32-shot learning
extended qualitative comparisons on the VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 32-shot learning
extended qualitative comparison on the VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 32-shot learning
learning on a large dataset of	learning	VoxCeleb2 - 32-shot learning
to frame few- and one-shot learning of neural talking head models	learning	VoxCeleb2 - 32-shot learning
fields synthesized using ma- chine learning (including deep learning) [11, 29	learning	VoxCeleb2 - 32-shot learning
of photographs (so-called few- shot learning) and with limited training time	learning	VoxCeleb2 - 32-shot learning
on a single photograph (one-shot learning), while adding a few more	learning	VoxCeleb2 - 32-shot learning
The few-shot learning ability is obtained through exten	learning	VoxCeleb2 - 32-shot learning
learning) on a large corpus of	learning	VoxCeleb2 - 32-shot learning
learning, our sys- tem simulates few-shot	learning	VoxCeleb2 - 32-shot learning
learning tasks and learns to trans	learning	VoxCeleb2 - 32-shot learning
sets up a new adversarial learning problem with high-capacity generator and	learning	VoxCeleb2 - 32-shot learning
learning	learning	VoxCeleb2 - 32-shot learning
and, more recently, with deep learning [22, 25] (to name just	learning	VoxCeleb2 - 32-shot learning
learning stage uses the adaptive instance	learning	VoxCeleb2 - 32-shot learning
learning to obtain the initial state	learning	VoxCeleb2 - 32-shot learning
learning	learning	VoxCeleb2 - 32-shot learning
learning [41] use adversarially- trained networks	learning	VoxCeleb2 - 32-shot learning
learning stage. While these methods are	learning	VoxCeleb2 - 32-shot learning
adversarial fine-tuning into the meta- learning framework. The former is applied	learning	VoxCeleb2 - 32-shot learning
learning stage	learning	VoxCeleb2 - 32-shot learning
4, 18]. Their setting (few-shot learning of generative models) and some	learning	VoxCeleb2 - 32-shot learning
domain, the use of adversarial learning, its specific adaptation to the	learning	VoxCeleb2 - 32-shot learning
learning process and numerous im- plementation	learning	VoxCeleb2 - 32-shot learning
learning architecture involves the embedder network	learning	VoxCeleb2 - 32-shot learning
learning, we pass sets of frames	learning	VoxCeleb2 - 32-shot learning
learning stage of our approach assumes	learning	VoxCeleb2 - 32-shot learning
its t-th frame. During the learning process, as well as during	learning	VoxCeleb2 - 32-shot learning
learning stage of our approach, the	learning	VoxCeleb2 - 32-shot learning
learning stage. In general, during meta-learning	learning	VoxCeleb2 - 32-shot learning
learning, only ψ are trained directly	learning	VoxCeleb2 - 32-shot learning
learning stage	learning	VoxCeleb2 - 32-shot learning
learning stage of our approach, the	learning	VoxCeleb2 - 32-shot learning
3.3. Few-shot learning by fine-tuning	learning	VoxCeleb2 - 32-shot learning
learning has converged, our system can	learning	VoxCeleb2 - 32-shot learning
learning stage. As before, the synthe	learning	VoxCeleb2 - 32-shot learning
learning stage	learning	VoxCeleb2 - 32-shot learning
learning stage. A straightforward way to	learning	VoxCeleb2 - 32-shot learning
learning with a single video sequence	learning	VoxCeleb2 - 32-shot learning
learning stage to initialize ψ′, i.e	learning	VoxCeleb2 - 32-shot learning
learning stage. The initialization of w	learning	VoxCeleb2 - 32-shot learning
learning stage	learning	VoxCeleb2 - 32-shot learning
learning stage. For the intiailization, we	learning	VoxCeleb2 - 32-shot learning
learning dataset). However, the match term	learning	VoxCeleb2 - 32-shot learning
learning process ensures the similarity between	learning	VoxCeleb2 - 32-shot learning
Once the new learning problem is set up, the	learning	VoxCeleb2 - 32-shot learning
follow directly from the meta- learning variants. Thus, the generator parameters	learning	VoxCeleb2 - 32-shot learning
learning stage is also crucial. As	learning	VoxCeleb2 - 32-shot learning
Adam [21]. We set the learning rate of the embedder and	learning	VoxCeleb2 - 32-shot learning
different datasets with multiple few-shot learning settings. Please re- fer to	learning	VoxCeleb2 - 32-shot learning
fine-tune all models on few-shot learning sets of size T for	learning	VoxCeleb2 - 32-shot learning
learning (or pretraining) stage. After the	learning	VoxCeleb2 - 32-shot learning
few-shot learning, the evaluation is performed on	learning	VoxCeleb2 - 32-shot learning
T used in few- shot learning	learning	VoxCeleb2 - 32-shot learning
we perform one- and few-shot learning on a video of a	learning	VoxCeleb2 - 32-shot learning
learning or pretraining. We set the	learning	VoxCeleb2 - 32-shot learning
the comparison of the few-shot learning timings. Both are provided in	learning	VoxCeleb2 - 32-shot learning
allow to trade off few-shot learning speed versus the results quality	learning	VoxCeleb2 - 32-shot learning
per- forms better for low-shot learning (e.g. one-shot), while the FT	learning	VoxCeleb2 - 32-shot learning
variant allows fast (real-time) few-shot learning of new avatars, fine-tuning ultimately	learning	VoxCeleb2 - 32-shot learning
learning of ad	learning	VoxCeleb2 - 32-shot learning
and S. Levine. Model-agnostic meta- learning for fast adaptation of deep	learning	VoxCeleb2 - 32-shot learning
Y. Wu, et al. Transfer learning from speaker verification to multispeaker	learning	VoxCeleb2 - 32-shot learning
I. Kemelmacher- Shlizerman. Synthesizing Obama: learning lip sync from au- dio	learning	VoxCeleb2 - 32-shot learning
and Y. Wang. Adversarial meta- learning	learning	VoxCeleb2 - 32-shot learning
An adversarial approach to few-shot learning	learning	VoxCeleb2 - 32-shot learning
Pix2pixHD and our method, few-shot learning was done via fine-tuning for	learning	VoxCeleb2 - 32-shot learning
Method (T) Time, s Few-shot learning	learning	VoxCeleb2 - 32-shot learning
2: Quantitative comparison of few-shot learning and inference timings for the	learning	VoxCeleb2 - 32-shot learning
learning	learning	VoxCeleb2 - 32-shot learning
multiple training frames in few-shot learning problems, like in our final	learning	VoxCeleb2 - 32-shot learning
learning configu- ration, which turned out	learning	VoxCeleb2 - 32-shot learning
learning, we randomly initialize the person-specific	learning	VoxCeleb2 - 32-shot learning
learning objective and initialize the embedding	learning	VoxCeleb2 - 32-shot learning
learning or pretraining. We used eight	learning	VoxCeleb2 - 32-shot learning
shot learning problem formulation. The notation for	learning	VoxCeleb2 - 32-shot learning
learning or pretraining. We used eight	learning	VoxCeleb2 - 32-shot learning
shot learning problem formulation. The notation for	learning	VoxCeleb2 - 32-shot learning
 32	 32	VoxCeleb2 - 32-shot learning
 32 hold- out frames for each	 32	VoxCeleb2 - 32-shot learning
 32 frames in the fine-tuning set	 32	VoxCeleb2 - 32-shot learning
 32 hold-out frames for each of	 32	VoxCeleb2 - 32-shot learning
8  32	 32	VoxCeleb2 - 32-shot learning
8  32	 32	VoxCeleb2 - 32-shot learning
 32 images achieves perfect realism and	 32	VoxCeleb2 - 32-shot learning
 32 training images. The number of	 32	VoxCeleb2 - 32-shot learning
8  32	 32	VoxCeleb2 - 32-shot learning
8  32	 32	VoxCeleb2 - 32-shot learning
8  32	 32	VoxCeleb2 - 32-shot learning
8  32	 32	VoxCeleb2 - 32-shot learning
8  32	 32	VoxCeleb2 - 32-shot learning
8  32	 32	VoxCeleb2 - 32-shot learning
videos at 1 fps) and VoxCeleb2 [8] (224p videos at 25	VoxCeleb2	VoxCeleb2 - 32-shot learning
VoxCeleb2 Ours-FF (1) 46.1 0.61 0.42	VoxCeleb2	VoxCeleb2 - 32-shot learning
ablation studies, while by using VoxCeleb2 we show the full potential	VoxCeleb2	VoxCeleb2 - 32-shot learning
our method on a larger VoxCeleb2 dataset. Here, we train two	VoxCeleb2	VoxCeleb2 - 32-shot learning
from test videos of the VoxCeleb2 dataset. We rank these videos	VoxCeleb2	VoxCeleb2 - 32-shot learning
our best models on the VoxCeleb2 dataset. The number of training	VoxCeleb2	VoxCeleb2 - 32-shot learning
poses were taken from the VoxCeleb2 dataset. Digital zoom recommended	VoxCeleb2	VoxCeleb2 - 32-shot learning
was trained only for the VoxCeleb2 dataset. The comparison was car	VoxCeleb2	VoxCeleb2 - 32-shot learning
extended qualitative comparisons on the VoxCeleb2 dataset. Here, the comparison is	VoxCeleb2	VoxCeleb2 - 32-shot learning
extended qualitative comparison on the VoxCeleb2 dataset. Here, we compare qualitative	VoxCeleb2	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- tional neural networks to generate	-	VoxCeleb2 - 32-shot learning
- ate a personalized talking head	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- ter that is able to	-	VoxCeleb2 - 32-shot learning
- and one-shot learning of neural	-	VoxCeleb2 - 32-shot learning
- sarial training problems with high	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- alized photorealistic talking head models	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- sions and mimics of a	-	VoxCeleb2 - 32-shot learning
- ically, we consider the problem	-	VoxCeleb2 - 32-shot learning
- alistic personalized head images given	-	VoxCeleb2 - 32-shot learning
- marks, which drive the animation	-	VoxCeleb2 - 32-shot learning
- conferencing and multi-player games, as	-	VoxCeleb2 - 32-shot learning
- fects industry. Synthesizing realistic talking	-	VoxCeleb2 - 32-shot learning
- ity. This complexity stems not	-	VoxCeleb2 - 32-shot learning
- man visual system towards even	-	VoxCeleb2 - 32-shot learning
- pearance modeling of human heads	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- takes explains the current prevalence	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- ferencing systems	-	VoxCeleb2 - 32-shot learning
- posed to synthesize articulated head	-	VoxCeleb2 - 32-shot learning
- chine learning (including deep learning	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- age, the amount of motion	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- vNets) presents the new hope	-	VoxCeleb2 - 32-shot learning
- ever, to succeed, such methods	-	VoxCeleb2 - 32-shot learning
- lions of parameters for each	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- phisticated physical and optical modeling	-	VoxCeleb2 - 32-shot learning
- cessive for most practical telepresence	-	VoxCeleb2 - 32-shot learning
- els with as little effort	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- shot learning) and with limited	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- larly to [16, 20, 37	-	VoxCeleb2 - 32-shot learning
- yond the abilities of warping-based	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- sive pre-training (meta-learning) on a	-	VoxCeleb2 - 32-shot learning
- ing head videos corresponding to	-	VoxCeleb2 - 32-shot learning
- verse appearance. In the course	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- tem simulates few-shot learning tasks	-	VoxCeleb2 - 32-shot learning
- form landmark positions into realistically-looking	-	VoxCeleb2 - 32-shot learning
- alized photographs, given a small	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- ficient realism and personalization fidelity	-	VoxCeleb2 - 32-shot learning
- ing head models, including video	-	VoxCeleb2 - 32-shot learning
- ing of the appearance of	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- tension of the face modeling	-	VoxCeleb2 - 32-shot learning
- ability and higher complexity than	-	VoxCeleb2 - 32-shot learning
- ple, the results of face	-	VoxCeleb2 - 32-shot learning
- fledged talking head system	-	VoxCeleb2 - 32-shot learning
- tecture uses adversarial training [12	-	VoxCeleb2 - 32-shot learning
- ing projection discriminators [32]. Our	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- sifier, from which it can	-	VoxCeleb2 - 32-shot learning
- fiers of unseen classes, given	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- GAN [43], adversarial meta-learning [41	-	VoxCeleb2 - 32-shot learning
- trained networks to generate additional	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- mance, our method deals with	-	VoxCeleb2 - 32-shot learning
- ation models using similar adversarial	-	VoxCeleb2 - 32-shot learning
- marize, we bring the adversarial	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- learning framework. The former is	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- cation domain, the use of	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- plementation details	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- marks) to the embedding vectors	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- quence and with xi(t) its	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- ity of the face landmarks	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
these inputs into an N -	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- age yi(t) for the video	-	VoxCeleb2 - 32-shot learning
- sized video frame x̂i(t). The	-	VoxCeleb2 - 32-shot learning
- imize the similarity between its	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- trix P: ψ̂i = Pêi	-	VoxCeleb2 - 32-shot learning
landmark image into an N -	-	VoxCeleb2 - 32-shot learning
- criminator predicts a single scalar	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- rameters of all three networks	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- ing (K = 8 in	-	VoxCeleb2 - 32-shot learning
- domly draw a training video	-	VoxCeleb2 - 32-shot learning
- ditional K frames s1, s2	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- ding by simply averaging the	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- tion x̂i(t) using the perceptual	-	VoxCeleb2 - 32-shot learning
- responding to VGG19 [30] network	-	VoxCeleb2 - 32-shot learning
- sentially is a perceptual similarity	-	VoxCeleb2 - 32-shot learning
- respond to individual videos. The	-	VoxCeleb2 - 32-shot learning
maps its inputs to anN -	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- criminator. The match term LMCH(φ,W	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- ters θ,W,w0, b of the	-	VoxCeleb2 - 32-shot learning
- courages the increase of the	-	VoxCeleb2 - 32-shot learning
- ample x̂i(t) and the real	-	VoxCeleb2 - 32-shot learning
- nating updates of the embedder	-	VoxCeleb2 - 32-shot learning
- imize the losses LCNT,LADV and	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- sis is conditioned on the	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- timate the embedding for the	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- sponding to new landmark images	-	VoxCeleb2 - 32-shot learning
- erator using the estimated embedding	-	VoxCeleb2 - 32-shot learning
- learned parameters ψ, as well	-	VoxCeleb2 - 32-shot learning
- able identity gap that is	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- sult of the meta-learning stage	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- tors computed by the embedder	-	VoxCeleb2 - 32-shot learning
- tions of the fine-tuning stage	-	VoxCeleb2 - 32-shot learning
- learning variants. Thus, the generator	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- son et. al. [19], but	-	VoxCeleb2 - 32-shot learning
- malization [15] replaced by instance	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- efficients of instance normalization layers	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- ization layers in the downsampling	-	VoxCeleb2 - 32-shot learning
- mark images yi(t	-	VoxCeleb2 - 32-shot learning
- tional part of the discriminator	-	VoxCeleb2 - 32-shot learning
- out normalization layers). The discriminator	-	VoxCeleb2 - 32-shot learning
- pared to the embedder, has	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- serted at 32×32 spatial resolution	-	VoxCeleb2 - 32-shot learning
- tween activations of Conv1,6,11,20,29 VGG19	-	VoxCeleb2 - 32-shot learning
- nally, for LMCH we set	-	VoxCeleb2 - 32-shot learning
- tional layers to 64 and	-	VoxCeleb2 - 32-shot learning
- tor has 38 million parameters	-	VoxCeleb2 - 32-shot learning
- titative and qualitative evaluation: VoxCeleb1	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- fer to the text for	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- son not seen during meta-learning	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- reenactment scenario). For the evaluation	-	VoxCeleb2 - 32-shot learning
- out frames for each of	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- realism and identity preservation of	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- bedding vectors of the state-of-the-art	-	VoxCeleb2 - 32-shot learning
- work [9] for measuring identity	-	VoxCeleb2 - 32-shot learning
- tual similarity and realism of	-	VoxCeleb2 - 32-shot learning
- man respondents. We show people	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- not spot fakes based on	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- Celeb1 dataset). For Pix2pixHD, we	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- shot learning. X2Face, as a	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- tion, which arguably gives X2Face	-	VoxCeleb2 - 32-shot learning
- lines in three different setups	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- dom from the other video	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- perform our method on the	-	VoxCeleb2 - 32-shot learning
- imizes only perceptual metric, without	-	VoxCeleb2 - 32-shot learning
- and few-shot learning on a	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- ter correlates with visual quality	-	VoxCeleb2 - 32-shot learning
- ble 1-Top with the results	-	VoxCeleb2 - 32-shot learning
- alism and personalization degree achieved	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- out fine-tuning (by simply predicting	-	VoxCeleb2 - 32-shot learning
- ant is trained for half	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- sults, where animation is driven	-	VoxCeleb2 - 32-shot learning
- ent video of the same	-	VoxCeleb2 - 32-shot learning
- tary material and in Figure	-	VoxCeleb2 - 32-shot learning
- ble 1-Bottom) and the visual	-	VoxCeleb2 - 32-shot learning
- forms better for low-shot learning	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- ial fine-tuning	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- sons with similar geometry of	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- tographs in the source column	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- realistic virtual talking heads in	-	VoxCeleb2 - 32-shot learning
- ization score in our user	-	VoxCeleb2 - 32-shot learning
- ics representation (in particular, the	-	VoxCeleb2 - 32-shot learning
- son leads to a noticeable	-	VoxCeleb2 - 32-shot learning
- ing a different person and	-	VoxCeleb2 - 32-shot learning
- proach already provides a high-realism	-	VoxCeleb2 - 32-shot learning
- puter Graphics and Applications, 30(4):20–31	-	VoxCeleb2 - 32-shot learning
- sarial networks. In Artificial Neural	-	VoxCeleb2 - 32-shot learning
Networks and Machine Learning - ICANN, pages 594–603, 2018. 2	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- thesis of 3d faces. In	-	VoxCeleb2 - 32-shot learning
- 29, 2017, pages 1021–1030, 2017	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- learning for fast adaptation of	-	VoxCeleb2 - 32-shot learning
- ulation. In European Conference on	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- erative adversarial nets. In Advances	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- wanathan, and R. Garnett, editors	-	VoxCeleb2 - 32-shot learning
- formation Processing Systems 30, pages	-	VoxCeleb2 - 32-shot learning
- time with adaptive instance normalization	-	VoxCeleb2 - 32-shot learning
- ternational Conference on Machine Learning	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- shick, S. Guadarrama, and T	-	VoxCeleb2 - 32-shot learning
- tional architecture for fast feature	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- speech synthesis. In Proc. NIPS	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- SPEECH, 2017. 5	-	VoxCeleb2 - 32-shot learning
- gios, and I. Kokkinos. Deforming	-	VoxCeleb2 - 32-shot learning
- vised disentangling of shape and	-	VoxCeleb2 - 32-shot learning
- ropean Conference on Computer Vision	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- Shlizerman. Synthesizing Obama: learning lip	-	VoxCeleb2 - 32-shot learning
- dio. ACM Transactions on Graphics	-	VoxCeleb2 - 32-shot learning
- tral normalization for generative adversarial	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- erator architecture for generative adversarial	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- actment of RGB videos. In	-	VoxCeleb2 - 32-shot learning
- ference on Computer Vision and	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- tion, 2018. 4, 6	-	VoxCeleb2 - 32-shot learning
- learning. CoRR, abs/1806.03316, 2018. 2	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- ried out on a single	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- surement was averaged over 100	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- ing personalization fidelity and realism	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- duction of a training scheduler	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- vided by the embedder is	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- specific initialization of the discriminator	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- ration, which turned out to	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
- tice that the results for	-	VoxCeleb2 - 32-shot learning
- sonalization fidelity. We, therefore, came	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
-	-	VoxCeleb2 - 32-shot learning
shot learning of neural talking head models	shot learning	VoxCeleb2 - 32-shot learning
handful of photographs (so-called few- shot learning) and with limited training time	shot learning	VoxCeleb2 - 32-shot learning
shot learning), while adding a few more	shot learning	VoxCeleb2 - 32-shot learning
shot learning ability is obtained through exten	shot learning	VoxCeleb2 - 32-shot learning
shot learning tasks and learns to trans	shot learning	VoxCeleb2 - 32-shot learning
shot learning of generative models) and some	shot learning	VoxCeleb2 - 32-shot learning
shot learning by fine-tuning	shot learning	VoxCeleb2 - 32-shot learning
shot learning settings. Please re- fer to	shot learning	VoxCeleb2 - 32-shot learning
shot learning sets of size T for	shot learning	VoxCeleb2 - 32-shot learning
shot learning, the evaluation is performed on	shot learning	VoxCeleb2 - 32-shot learning
frames T used in few- shot learning	shot learning	VoxCeleb2 - 32-shot learning
shot learning on a video of a	shot learning	VoxCeleb2 - 32-shot learning
shot learning timings. Both are provided in	shot learning	VoxCeleb2 - 32-shot learning
shot learning speed versus the results quality	shot learning	VoxCeleb2 - 32-shot learning
shot learning (e.g. one-shot), while the FT	shot learning	VoxCeleb2 - 32-shot learning
shot learning of new avatars, fine-tuning ultimately	shot learning	VoxCeleb2 - 32-shot learning
shot learning	shot learning	VoxCeleb2 - 32-shot learning
shot learning was done via fine-tuning for	shot learning	VoxCeleb2 - 32-shot learning
shot learning	shot learning	VoxCeleb2 - 32-shot learning
shot learning and inference timings for the	shot learning	VoxCeleb2 - 32-shot learning
shot learning problems, like in our final	shot learning	VoxCeleb2 - 32-shot learning
or pretraining. We used eight shot learning problem formulation. The notation for	shot learning	VoxCeleb2 - 32-shot learning
or pretraining. We used eight shot learning problem formulation. The notation for	shot learning	VoxCeleb2 - 32-shot learning
VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 8-shot learning
ablation studies, while by using VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 8-shot learning
our method on a larger VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 8-shot learning
from test videos of the VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 8-shot learning
our best models on the VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 8-shot learning
poses were taken from the VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 8-shot learning
was trained only for the VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 8-shot learning
extended qualitative comparisons on the VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 8-shot learning
extended qualitative comparison on the VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 8-shot learning
learning on a large dataset of	learning	VoxCeleb2 - 8-shot learning
to frame few- and one-shot learning of neural talking head models	learning	VoxCeleb2 - 8-shot learning
fields synthesized using ma- chine learning (including deep learning) [11, 29	learning	VoxCeleb2 - 8-shot learning
of photographs (so-called few- shot learning) and with limited training time	learning	VoxCeleb2 - 8-shot learning
on a single photograph (one-shot learning), while adding a few more	learning	VoxCeleb2 - 8-shot learning
The few-shot learning ability is obtained through exten	learning	VoxCeleb2 - 8-shot learning
learning) on a large corpus of	learning	VoxCeleb2 - 8-shot learning
learning, our sys- tem simulates few-shot	learning	VoxCeleb2 - 8-shot learning
learning tasks and learns to trans	learning	VoxCeleb2 - 8-shot learning
sets up a new adversarial learning problem with high-capacity generator and	learning	VoxCeleb2 - 8-shot learning
learning	learning	VoxCeleb2 - 8-shot learning
and, more recently, with deep learning [22, 25] (to name just	learning	VoxCeleb2 - 8-shot learning
learning stage uses the adaptive instance	learning	VoxCeleb2 - 8-shot learning
learning to obtain the initial state	learning	VoxCeleb2 - 8-shot learning
learning	learning	VoxCeleb2 - 8-shot learning
learning [41] use adversarially- trained networks	learning	VoxCeleb2 - 8-shot learning
learning stage. While these methods are	learning	VoxCeleb2 - 8-shot learning
adversarial fine-tuning into the meta- learning framework. The former is applied	learning	VoxCeleb2 - 8-shot learning
learning stage	learning	VoxCeleb2 - 8-shot learning
4, 18]. Their setting (few-shot learning of generative models) and some	learning	VoxCeleb2 - 8-shot learning
domain, the use of adversarial learning, its specific adaptation to the	learning	VoxCeleb2 - 8-shot learning
learning process and numerous im- plementation	learning	VoxCeleb2 - 8-shot learning
learning architecture involves the embedder network	learning	VoxCeleb2 - 8-shot learning
learning, we pass sets of frames	learning	VoxCeleb2 - 8-shot learning
learning stage of our approach assumes	learning	VoxCeleb2 - 8-shot learning
its t-th frame. During the learning process, as well as during	learning	VoxCeleb2 - 8-shot learning
learning stage of our approach, the	learning	VoxCeleb2 - 8-shot learning
learning stage. In general, during meta-learning	learning	VoxCeleb2 - 8-shot learning
learning, only ψ are trained directly	learning	VoxCeleb2 - 8-shot learning
learning stage	learning	VoxCeleb2 - 8-shot learning
learning stage of our approach, the	learning	VoxCeleb2 - 8-shot learning
3.3. Few-shot learning by fine-tuning	learning	VoxCeleb2 - 8-shot learning
learning has converged, our system can	learning	VoxCeleb2 - 8-shot learning
learning stage. As before, the synthe	learning	VoxCeleb2 - 8-shot learning
learning stage	learning	VoxCeleb2 - 8-shot learning
learning stage. A straightforward way to	learning	VoxCeleb2 - 8-shot learning
learning with a single video sequence	learning	VoxCeleb2 - 8-shot learning
learning stage to initialize ψ′, i.e	learning	VoxCeleb2 - 8-shot learning
learning stage. The initialization of w	learning	VoxCeleb2 - 8-shot learning
learning stage	learning	VoxCeleb2 - 8-shot learning
learning stage. For the intiailization, we	learning	VoxCeleb2 - 8-shot learning
learning dataset). However, the match term	learning	VoxCeleb2 - 8-shot learning
learning process ensures the similarity between	learning	VoxCeleb2 - 8-shot learning
Once the new learning problem is set up, the	learning	VoxCeleb2 - 8-shot learning
follow directly from the meta- learning variants. Thus, the generator parameters	learning	VoxCeleb2 - 8-shot learning
learning stage is also crucial. As	learning	VoxCeleb2 - 8-shot learning
Adam [21]. We set the learning rate of the embedder and	learning	VoxCeleb2 - 8-shot learning
different datasets with multiple few-shot learning settings. Please re- fer to	learning	VoxCeleb2 - 8-shot learning
fine-tune all models on few-shot learning sets of size T for	learning	VoxCeleb2 - 8-shot learning
learning (or pretraining) stage. After the	learning	VoxCeleb2 - 8-shot learning
few-shot learning, the evaluation is performed on	learning	VoxCeleb2 - 8-shot learning
T used in few- shot learning	learning	VoxCeleb2 - 8-shot learning
we perform one- and few-shot learning on a video of a	learning	VoxCeleb2 - 8-shot learning
learning or pretraining. We set the	learning	VoxCeleb2 - 8-shot learning
the comparison of the few-shot learning timings. Both are provided in	learning	VoxCeleb2 - 8-shot learning
allow to trade off few-shot learning speed versus the results quality	learning	VoxCeleb2 - 8-shot learning
per- forms better for low-shot learning (e.g. one-shot), while the FT	learning	VoxCeleb2 - 8-shot learning
variant allows fast (real-time) few-shot learning of new avatars, fine-tuning ultimately	learning	VoxCeleb2 - 8-shot learning
learning of ad	learning	VoxCeleb2 - 8-shot learning
and S. Levine. Model-agnostic meta- learning for fast adaptation of deep	learning	VoxCeleb2 - 8-shot learning
Y. Wu, et al. Transfer learning from speaker verification to multispeaker	learning	VoxCeleb2 - 8-shot learning
I. Kemelmacher- Shlizerman. Synthesizing Obama: learning lip sync from au- dio	learning	VoxCeleb2 - 8-shot learning
and Y. Wang. Adversarial meta- learning	learning	VoxCeleb2 - 8-shot learning
An adversarial approach to few-shot learning	learning	VoxCeleb2 - 8-shot learning
Pix2pixHD and our method, few-shot learning was done via fine-tuning for	learning	VoxCeleb2 - 8-shot learning
Method (T) Time, s Few-shot learning	learning	VoxCeleb2 - 8-shot learning
2: Quantitative comparison of few-shot learning and inference timings for the	learning	VoxCeleb2 - 8-shot learning
learning	learning	VoxCeleb2 - 8-shot learning
multiple training frames in few-shot learning problems, like in our final	learning	VoxCeleb2 - 8-shot learning
learning configu- ration, which turned out	learning	VoxCeleb2 - 8-shot learning
learning, we randomly initialize the person-specific	learning	VoxCeleb2 - 8-shot learning
learning objective and initialize the embedding	learning	VoxCeleb2 - 8-shot learning
learning or pretraining. We used eight	learning	VoxCeleb2 - 8-shot learning
shot learning problem formulation. The notation for	learning	VoxCeleb2 - 8-shot learning
learning or pretraining. We used eight	learning	VoxCeleb2 - 8-shot learning
shot learning problem formulation. The notation for	learning	VoxCeleb2 - 8-shot learning
 8 · 101	 8	VoxCeleb2 - 8-shot learning
1  8	 8	VoxCeleb2 - 8-shot learning
1  8	 8	VoxCeleb2 - 8-shot learning
already provides a high-realism solution.  8	 8	VoxCeleb2 - 8-shot learning
 8 images. Each mea- surement was	 8	VoxCeleb2 - 8-shot learning
1  8	 8	VoxCeleb2 - 8-shot learning
1  8	 8	VoxCeleb2 - 8-shot learning
 8	 8	VoxCeleb2 - 8-shot learning
1  8	 8	VoxCeleb2 - 8-shot learning
1  8	 8	VoxCeleb2 - 8-shot learning
1  8	 8	VoxCeleb2 - 8-shot learning
1  8	 8	VoxCeleb2 - 8-shot learning
videos at 1 fps) and VoxCeleb2 [8] (224p videos at 25	VoxCeleb2	VoxCeleb2 - 8-shot learning
VoxCeleb2 Ours-FF (1) 46.1 0.61 0.42	VoxCeleb2	VoxCeleb2 - 8-shot learning
ablation studies, while by using VoxCeleb2 we show the full potential	VoxCeleb2	VoxCeleb2 - 8-shot learning
our method on a larger VoxCeleb2 dataset. Here, we train two	VoxCeleb2	VoxCeleb2 - 8-shot learning
from test videos of the VoxCeleb2 dataset. We rank these videos	VoxCeleb2	VoxCeleb2 - 8-shot learning
our best models on the VoxCeleb2 dataset. The number of training	VoxCeleb2	VoxCeleb2 - 8-shot learning
poses were taken from the VoxCeleb2 dataset. Digital zoom recommended	VoxCeleb2	VoxCeleb2 - 8-shot learning
was trained only for the VoxCeleb2 dataset. The comparison was car	VoxCeleb2	VoxCeleb2 - 8-shot learning
extended qualitative comparisons on the VoxCeleb2 dataset. Here, the comparison is	VoxCeleb2	VoxCeleb2 - 8-shot learning
extended qualitative comparison on the VoxCeleb2 dataset. Here, we compare qualitative	VoxCeleb2	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- tional neural networks to generate	-	VoxCeleb2 - 8-shot learning
- ate a personalized talking head	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- ter that is able to	-	VoxCeleb2 - 8-shot learning
- and one-shot learning of neural	-	VoxCeleb2 - 8-shot learning
- sarial training problems with high	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- alized photorealistic talking head models	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- sions and mimics of a	-	VoxCeleb2 - 8-shot learning
- ically, we consider the problem	-	VoxCeleb2 - 8-shot learning
- alistic personalized head images given	-	VoxCeleb2 - 8-shot learning
- marks, which drive the animation	-	VoxCeleb2 - 8-shot learning
- conferencing and multi-player games, as	-	VoxCeleb2 - 8-shot learning
- fects industry. Synthesizing realistic talking	-	VoxCeleb2 - 8-shot learning
- ity. This complexity stems not	-	VoxCeleb2 - 8-shot learning
- man visual system towards even	-	VoxCeleb2 - 8-shot learning
- pearance modeling of human heads	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- takes explains the current prevalence	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- ferencing systems	-	VoxCeleb2 - 8-shot learning
- posed to synthesize articulated head	-	VoxCeleb2 - 8-shot learning
- chine learning (including deep learning	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- age, the amount of motion	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- vNets) presents the new hope	-	VoxCeleb2 - 8-shot learning
- ever, to succeed, such methods	-	VoxCeleb2 - 8-shot learning
- lions of parameters for each	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- phisticated physical and optical modeling	-	VoxCeleb2 - 8-shot learning
- cessive for most practical telepresence	-	VoxCeleb2 - 8-shot learning
- els with as little effort	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- shot learning) and with limited	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- larly to [16, 20, 37	-	VoxCeleb2 - 8-shot learning
- yond the abilities of warping-based	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- sive pre-training (meta-learning) on a	-	VoxCeleb2 - 8-shot learning
- ing head videos corresponding to	-	VoxCeleb2 - 8-shot learning
- verse appearance. In the course	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- tem simulates few-shot learning tasks	-	VoxCeleb2 - 8-shot learning
- form landmark positions into realistically-looking	-	VoxCeleb2 - 8-shot learning
- alized photographs, given a small	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- ficient realism and personalization fidelity	-	VoxCeleb2 - 8-shot learning
- ing head models, including video	-	VoxCeleb2 - 8-shot learning
- ing of the appearance of	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- tension of the face modeling	-	VoxCeleb2 - 8-shot learning
- ability and higher complexity than	-	VoxCeleb2 - 8-shot learning
- ple, the results of face	-	VoxCeleb2 - 8-shot learning
- fledged talking head system	-	VoxCeleb2 - 8-shot learning
- tecture uses adversarial training [12	-	VoxCeleb2 - 8-shot learning
- ing projection discriminators [32]. Our	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- sifier, from which it can	-	VoxCeleb2 - 8-shot learning
- fiers of unseen classes, given	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- GAN [43], adversarial meta-learning [41	-	VoxCeleb2 - 8-shot learning
- trained networks to generate additional	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- mance, our method deals with	-	VoxCeleb2 - 8-shot learning
- ation models using similar adversarial	-	VoxCeleb2 - 8-shot learning
- marize, we bring the adversarial	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- learning framework. The former is	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- cation domain, the use of	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- plementation details	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- marks) to the embedding vectors	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- quence and with xi(t) its	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- ity of the face landmarks	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
these inputs into an N -	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- age yi(t) for the video	-	VoxCeleb2 - 8-shot learning
- sized video frame x̂i(t). The	-	VoxCeleb2 - 8-shot learning
- imize the similarity between its	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- trix P: ψ̂i = Pêi	-	VoxCeleb2 - 8-shot learning
landmark image into an N -	-	VoxCeleb2 - 8-shot learning
- criminator predicts a single scalar	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- rameters of all three networks	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- ing (K = 8 in	-	VoxCeleb2 - 8-shot learning
- domly draw a training video	-	VoxCeleb2 - 8-shot learning
- ditional K frames s1, s2	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- ding by simply averaging the	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- tion x̂i(t) using the perceptual	-	VoxCeleb2 - 8-shot learning
- responding to VGG19 [30] network	-	VoxCeleb2 - 8-shot learning
- sentially is a perceptual similarity	-	VoxCeleb2 - 8-shot learning
- respond to individual videos. The	-	VoxCeleb2 - 8-shot learning
maps its inputs to anN -	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- criminator. The match term LMCH(φ,W	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- ters θ,W,w0, b of the	-	VoxCeleb2 - 8-shot learning
- courages the increase of the	-	VoxCeleb2 - 8-shot learning
- ample x̂i(t) and the real	-	VoxCeleb2 - 8-shot learning
- nating updates of the embedder	-	VoxCeleb2 - 8-shot learning
- imize the losses LCNT,LADV and	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- sis is conditioned on the	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- timate the embedding for the	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- sponding to new landmark images	-	VoxCeleb2 - 8-shot learning
- erator using the estimated embedding	-	VoxCeleb2 - 8-shot learning
- learned parameters ψ, as well	-	VoxCeleb2 - 8-shot learning
- able identity gap that is	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- sult of the meta-learning stage	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- tors computed by the embedder	-	VoxCeleb2 - 8-shot learning
- tions of the fine-tuning stage	-	VoxCeleb2 - 8-shot learning
- learning variants. Thus, the generator	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- son et. al. [19], but	-	VoxCeleb2 - 8-shot learning
- malization [15] replaced by instance	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- efficients of instance normalization layers	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- ization layers in the downsampling	-	VoxCeleb2 - 8-shot learning
- mark images yi(t	-	VoxCeleb2 - 8-shot learning
- tional part of the discriminator	-	VoxCeleb2 - 8-shot learning
- out normalization layers). The discriminator	-	VoxCeleb2 - 8-shot learning
- pared to the embedder, has	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- serted at 32×32 spatial resolution	-	VoxCeleb2 - 8-shot learning
- tween activations of Conv1,6,11,20,29 VGG19	-	VoxCeleb2 - 8-shot learning
- nally, for LMCH we set	-	VoxCeleb2 - 8-shot learning
- tional layers to 64 and	-	VoxCeleb2 - 8-shot learning
- tor has 38 million parameters	-	VoxCeleb2 - 8-shot learning
- titative and qualitative evaluation: VoxCeleb1	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- fer to the text for	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- son not seen during meta-learning	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- reenactment scenario). For the evaluation	-	VoxCeleb2 - 8-shot learning
- out frames for each of	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- realism and identity preservation of	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- bedding vectors of the state-of-the-art	-	VoxCeleb2 - 8-shot learning
- work [9] for measuring identity	-	VoxCeleb2 - 8-shot learning
- tual similarity and realism of	-	VoxCeleb2 - 8-shot learning
- man respondents. We show people	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- not spot fakes based on	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- Celeb1 dataset). For Pix2pixHD, we	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- shot learning. X2Face, as a	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- tion, which arguably gives X2Face	-	VoxCeleb2 - 8-shot learning
- lines in three different setups	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- dom from the other video	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- perform our method on the	-	VoxCeleb2 - 8-shot learning
- imizes only perceptual metric, without	-	VoxCeleb2 - 8-shot learning
- and few-shot learning on a	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- ter correlates with visual quality	-	VoxCeleb2 - 8-shot learning
- ble 1-Top with the results	-	VoxCeleb2 - 8-shot learning
- alism and personalization degree achieved	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- out fine-tuning (by simply predicting	-	VoxCeleb2 - 8-shot learning
- ant is trained for half	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- sults, where animation is driven	-	VoxCeleb2 - 8-shot learning
- ent video of the same	-	VoxCeleb2 - 8-shot learning
- tary material and in Figure	-	VoxCeleb2 - 8-shot learning
- ble 1-Bottom) and the visual	-	VoxCeleb2 - 8-shot learning
- forms better for low-shot learning	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- ial fine-tuning	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- sons with similar geometry of	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- tographs in the source column	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- realistic virtual talking heads in	-	VoxCeleb2 - 8-shot learning
- ization score in our user	-	VoxCeleb2 - 8-shot learning
- ics representation (in particular, the	-	VoxCeleb2 - 8-shot learning
- son leads to a noticeable	-	VoxCeleb2 - 8-shot learning
- ing a different person and	-	VoxCeleb2 - 8-shot learning
- proach already provides a high-realism	-	VoxCeleb2 - 8-shot learning
- puter Graphics and Applications, 30(4):20–31	-	VoxCeleb2 - 8-shot learning
- sarial networks. In Artificial Neural	-	VoxCeleb2 - 8-shot learning
Networks and Machine Learning - ICANN, pages 594–603, 2018. 2	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- thesis of 3d faces. In	-	VoxCeleb2 - 8-shot learning
- 29, 2017, pages 1021–1030, 2017	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- learning for fast adaptation of	-	VoxCeleb2 - 8-shot learning
- ulation. In European Conference on	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- erative adversarial nets. In Advances	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- wanathan, and R. Garnett, editors	-	VoxCeleb2 - 8-shot learning
- formation Processing Systems 30, pages	-	VoxCeleb2 - 8-shot learning
- time with adaptive instance normalization	-	VoxCeleb2 - 8-shot learning
- ternational Conference on Machine Learning	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- shick, S. Guadarrama, and T	-	VoxCeleb2 - 8-shot learning
- tional architecture for fast feature	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- speech synthesis. In Proc. NIPS	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- SPEECH, 2017. 5	-	VoxCeleb2 - 8-shot learning
- gios, and I. Kokkinos. Deforming	-	VoxCeleb2 - 8-shot learning
- vised disentangling of shape and	-	VoxCeleb2 - 8-shot learning
- ropean Conference on Computer Vision	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- Shlizerman. Synthesizing Obama: learning lip	-	VoxCeleb2 - 8-shot learning
- dio. ACM Transactions on Graphics	-	VoxCeleb2 - 8-shot learning
- tral normalization for generative adversarial	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- erator architecture for generative adversarial	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- actment of RGB videos. In	-	VoxCeleb2 - 8-shot learning
- ference on Computer Vision and	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- tion, 2018. 4, 6	-	VoxCeleb2 - 8-shot learning
- learning. CoRR, abs/1806.03316, 2018. 2	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- ried out on a single	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- surement was averaged over 100	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- ing personalization fidelity and realism	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- duction of a training scheduler	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- vided by the embedder is	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- specific initialization of the discriminator	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- ration, which turned out to	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
- tice that the results for	-	VoxCeleb2 - 8-shot learning
- sonalization fidelity. We, therefore, came	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
-	-	VoxCeleb2 - 8-shot learning
shot learning of neural talking head models	shot learning	VoxCeleb2 - 8-shot learning
handful of photographs (so-called few- shot learning) and with limited training time	shot learning	VoxCeleb2 - 8-shot learning
shot learning), while adding a few more	shot learning	VoxCeleb2 - 8-shot learning
shot learning ability is obtained through exten	shot learning	VoxCeleb2 - 8-shot learning
shot learning tasks and learns to trans	shot learning	VoxCeleb2 - 8-shot learning
shot learning of generative models) and some	shot learning	VoxCeleb2 - 8-shot learning
shot learning by fine-tuning	shot learning	VoxCeleb2 - 8-shot learning
shot learning settings. Please re- fer to	shot learning	VoxCeleb2 - 8-shot learning
shot learning sets of size T for	shot learning	VoxCeleb2 - 8-shot learning
shot learning, the evaluation is performed on	shot learning	VoxCeleb2 - 8-shot learning
frames T used in few- shot learning	shot learning	VoxCeleb2 - 8-shot learning
shot learning on a video of a	shot learning	VoxCeleb2 - 8-shot learning
shot learning timings. Both are provided in	shot learning	VoxCeleb2 - 8-shot learning
shot learning speed versus the results quality	shot learning	VoxCeleb2 - 8-shot learning
shot learning (e.g. one-shot), while the FT	shot learning	VoxCeleb2 - 8-shot learning
shot learning of new avatars, fine-tuning ultimately	shot learning	VoxCeleb2 - 8-shot learning
shot learning	shot learning	VoxCeleb2 - 8-shot learning
shot learning was done via fine-tuning for	shot learning	VoxCeleb2 - 8-shot learning
shot learning	shot learning	VoxCeleb2 - 8-shot learning
shot learning and inference timings for the	shot learning	VoxCeleb2 - 8-shot learning
shot learning problems, like in our final	shot learning	VoxCeleb2 - 8-shot learning
or pretraining. We used eight shot learning problem formulation. The notation for	shot learning	VoxCeleb2 - 8-shot learning
or pretraining. We used eight shot learning problem formulation. The notation for	shot learning	VoxCeleb2 - 8-shot learning
T) FID↓ SSIM↑ CSIM↑ USER↓ VoxCeleb1  X2Face (1) 45.8 0.68 0.16	VoxCeleb1 	VoxCeleb1 - 1-shot learning
than the former. VoxCeleb1 	VoxCeleb1 	VoxCeleb1 - 1-shot learning
Methods. On the VoxCeleb1 	VoxCeleb1 	VoxCeleb1 - 1-shot learning
Figure 3: Comparison on the VoxCeleb1 	VoxCeleb1 	VoxCeleb1 - 1-shot learning
extended qualitative comparisons on the VoxCeleb1 	VoxCeleb1 	VoxCeleb1 - 1-shot learning
extended qualitative comparison on the VoxCeleb1 	VoxCeleb1 	VoxCeleb1 - 1-shot learning
 1	 1	VoxCeleb1 - 1-shot learning
people and even portrait paintings.  1	 1	VoxCeleb1 - 1-shot learning
motion, head rotation, and disocclusion  1	 1	VoxCeleb1 - 1-shot learning
 1	 1	VoxCeleb1 - 1-shot learning
 1 · 10−2 for VGG19 and	 1	VoxCeleb1 - 1-shot learning
 1 · 101. Fi- nally, for	 1	VoxCeleb1 - 1-shot learning
 15 million parameters, the genera- tor	 1	VoxCeleb1 - 1-shot learning
 1 fps) and VoxCeleb2 [8] (224p	 1	VoxCeleb1 - 1-shot learning
 10 times more videos	 1	VoxCeleb1 - 1-shot learning
 1	 1	VoxCeleb1 - 1-shot learning
 1, 8 and 32 frames in	 1	VoxCeleb1 - 1-shot learning
 1	 1	VoxCeleb1 - 1-shot learning
 1	 1	VoxCeleb1 - 1-shot learning
 150 epochs without the embedding matching	 1	VoxCeleb1 - 1-shot learning
 1	 1	VoxCeleb1 - 1-shot learning
 1	 1	VoxCeleb1 - 1-shot learning
 1	 1	VoxCeleb1 - 1-shot learning
 10040	 1	VoxCeleb1 - 1-shot learning
 187	 1	VoxCeleb1 - 1-shot learning
 1021	 1	VoxCeleb1 - 1-shot learning
 1126	 1	VoxCeleb1 - 1-shot learning
NeurIPS, pages 2371–2380, 2018. 2  10	 1	VoxCeleb1 - 1-shot learning
 1, we trained the models on	 1	VoxCeleb1 - 1-shot learning
 100 iterations	 1	VoxCeleb1 - 1-shot learning
the rest of the figures.  11	 1	VoxCeleb1 - 1-shot learning
32  1	 1	VoxCeleb1 - 1-shot learning
results from our final model.  12	 1	VoxCeleb1 - 1-shot learning
sequence of a different person.  13	 1	VoxCeleb1 - 1-shot learning
sequence with the same person.  14	 1	VoxCeleb1 - 1-shot learning
 16	 1	VoxCeleb1 - 1-shot learning
sequence of the same person.  15	 1	VoxCeleb1 - 1-shot learning
32  1	 1	VoxCeleb1 - 1-shot learning
 10	 1	VoxCeleb1 - 1-shot learning
3 in the main paper.  16	 1	VoxCeleb1 - 1-shot learning
 11	 1	VoxCeleb1 - 1-shot learning
3 in the main paper.  17	 1	VoxCeleb1 - 1-shot learning
32  1	 1	VoxCeleb1 - 1-shot learning
 12	 1	VoxCeleb1 - 1-shot learning
4 in the main paper.  18	 1	VoxCeleb1 - 1-shot learning
 13	 1	VoxCeleb1 - 1-shot learning
4 in the main paper.  19	 1	VoxCeleb1 - 1-shot learning
quan- titative and qualitative evaluation: VoxCeleb1 [26] (256p videos at 1	VoxCeleb1	VoxCeleb1 - 1-shot learning
T) FID↓ SSIM↑ CSIM↑ USER↓ VoxCeleb1	VoxCeleb1	VoxCeleb1 - 1-shot learning
than the former. VoxCeleb1 is used for comparison with	VoxCeleb1	VoxCeleb1 - 1-shot learning
Methods. On the VoxCeleb1 dataset we compare our model	VoxCeleb1	VoxCeleb1 - 1-shot learning
Figure 3: Comparison on the VoxCeleb1 dataset. For each of the	VoxCeleb1	VoxCeleb1 - 1-shot learning
to smaller-scale models trained on VoxCeleb1	VoxCeleb1	VoxCeleb1 - 1-shot learning
extended qualitative comparisons on the VoxCeleb1 dataset. Here, the comparison is	VoxCeleb1	VoxCeleb1 - 1-shot learning
extended qualitative comparison on the VoxCeleb1 dataset. Here, we compare qualitative	VoxCeleb1	VoxCeleb1 - 1-shot learning
shot learning of neural talking head models	shot learning	VoxCeleb1 - 1-shot learning
handful of photographs (so-called few- shot learning) and with limited training time	shot learning	VoxCeleb1 - 1-shot learning
shot learning), while adding a few more	shot learning	VoxCeleb1 - 1-shot learning
shot learning ability is obtained through exten	shot learning	VoxCeleb1 - 1-shot learning
shot learning tasks and learns to trans	shot learning	VoxCeleb1 - 1-shot learning
shot learning of generative models) and some	shot learning	VoxCeleb1 - 1-shot learning
shot learning by fine-tuning	shot learning	VoxCeleb1 - 1-shot learning
shot learning settings. Please re- fer to	shot learning	VoxCeleb1 - 1-shot learning
shot learning sets of size T for	shot learning	VoxCeleb1 - 1-shot learning
shot learning, the evaluation is performed on	shot learning	VoxCeleb1 - 1-shot learning
frames T used in few- shot learning	shot learning	VoxCeleb1 - 1-shot learning
shot learning on a video of a	shot learning	VoxCeleb1 - 1-shot learning
shot learning timings. Both are provided in	shot learning	VoxCeleb1 - 1-shot learning
shot learning speed versus the results quality	shot learning	VoxCeleb1 - 1-shot learning
shot learning (e.g. one-shot), while the FT	shot learning	VoxCeleb1 - 1-shot learning
shot learning of new avatars, fine-tuning ultimately	shot learning	VoxCeleb1 - 1-shot learning
shot learning	shot learning	VoxCeleb1 - 1-shot learning
shot learning was done via fine-tuning for	shot learning	VoxCeleb1 - 1-shot learning
shot learning	shot learning	VoxCeleb1 - 1-shot learning
shot learning and inference timings for the	shot learning	VoxCeleb1 - 1-shot learning
shot learning problems, like in our final	shot learning	VoxCeleb1 - 1-shot learning
or pretraining. We used eight shot learning problem formulation. The notation for	shot learning	VoxCeleb1 - 1-shot learning
or pretraining. We used eight shot learning problem formulation. The notation for	shot learning	VoxCeleb1 - 1-shot learning
learning on a large dataset of	learning	VoxCeleb1 - 1-shot learning
to frame few- and one-shot learning of neural talking head models	learning	VoxCeleb1 - 1-shot learning
fields synthesized using ma- chine learning (including deep learning) [11, 29	learning	VoxCeleb1 - 1-shot learning
of photographs (so-called few- shot learning) and with limited training time	learning	VoxCeleb1 - 1-shot learning
on a single photograph (one-shot learning), while adding a few more	learning	VoxCeleb1 - 1-shot learning
The few-shot learning ability is obtained through exten	learning	VoxCeleb1 - 1-shot learning
learning) on a large corpus of	learning	VoxCeleb1 - 1-shot learning
learning, our sys- tem simulates few-shot	learning	VoxCeleb1 - 1-shot learning
learning tasks and learns to trans	learning	VoxCeleb1 - 1-shot learning
sets up a new adversarial learning problem with high-capacity generator and	learning	VoxCeleb1 - 1-shot learning
learning	learning	VoxCeleb1 - 1-shot learning
and, more recently, with deep learning [22, 25] (to name just	learning	VoxCeleb1 - 1-shot learning
learning stage uses the adaptive instance	learning	VoxCeleb1 - 1-shot learning
learning to obtain the initial state	learning	VoxCeleb1 - 1-shot learning
learning	learning	VoxCeleb1 - 1-shot learning
learning [41] use adversarially- trained networks	learning	VoxCeleb1 - 1-shot learning
learning stage. While these methods are	learning	VoxCeleb1 - 1-shot learning
adversarial fine-tuning into the meta- learning framework. The former is applied	learning	VoxCeleb1 - 1-shot learning
learning stage	learning	VoxCeleb1 - 1-shot learning
4, 18]. Their setting (few-shot learning of generative models) and some	learning	VoxCeleb1 - 1-shot learning
domain, the use of adversarial learning, its specific adaptation to the	learning	VoxCeleb1 - 1-shot learning
learning process and numerous im- plementation	learning	VoxCeleb1 - 1-shot learning
learning architecture involves the embedder network	learning	VoxCeleb1 - 1-shot learning
learning, we pass sets of frames	learning	VoxCeleb1 - 1-shot learning
learning stage of our approach assumes	learning	VoxCeleb1 - 1-shot learning
its t-th frame. During the learning process, as well as during	learning	VoxCeleb1 - 1-shot learning
learning stage of our approach, the	learning	VoxCeleb1 - 1-shot learning
learning stage. In general, during meta-learning	learning	VoxCeleb1 - 1-shot learning
learning, only ψ are trained directly	learning	VoxCeleb1 - 1-shot learning
learning stage	learning	VoxCeleb1 - 1-shot learning
learning stage of our approach, the	learning	VoxCeleb1 - 1-shot learning
3.3. Few-shot learning by fine-tuning	learning	VoxCeleb1 - 1-shot learning
learning has converged, our system can	learning	VoxCeleb1 - 1-shot learning
learning stage. As before, the synthe	learning	VoxCeleb1 - 1-shot learning
learning stage	learning	VoxCeleb1 - 1-shot learning
learning stage. A straightforward way to	learning	VoxCeleb1 - 1-shot learning
learning with a single video sequence	learning	VoxCeleb1 - 1-shot learning
learning stage to initialize ψ′, i.e	learning	VoxCeleb1 - 1-shot learning
learning stage. The initialization of w	learning	VoxCeleb1 - 1-shot learning
learning stage	learning	VoxCeleb1 - 1-shot learning
learning stage. For the intiailization, we	learning	VoxCeleb1 - 1-shot learning
learning dataset). However, the match term	learning	VoxCeleb1 - 1-shot learning
learning process ensures the similarity between	learning	VoxCeleb1 - 1-shot learning
Once the new learning problem is set up, the	learning	VoxCeleb1 - 1-shot learning
follow directly from the meta- learning variants. Thus, the generator parameters	learning	VoxCeleb1 - 1-shot learning
learning stage is also crucial. As	learning	VoxCeleb1 - 1-shot learning
Adam [21]. We set the learning rate of the embedder and	learning	VoxCeleb1 - 1-shot learning
different datasets with multiple few-shot learning settings. Please re- fer to	learning	VoxCeleb1 - 1-shot learning
fine-tune all models on few-shot learning sets of size T for	learning	VoxCeleb1 - 1-shot learning
learning (or pretraining) stage. After the	learning	VoxCeleb1 - 1-shot learning
few-shot learning, the evaluation is performed on	learning	VoxCeleb1 - 1-shot learning
T used in few- shot learning	learning	VoxCeleb1 - 1-shot learning
we perform one- and few-shot learning on a video of a	learning	VoxCeleb1 - 1-shot learning
learning or pretraining. We set the	learning	VoxCeleb1 - 1-shot learning
the comparison of the few-shot learning timings. Both are provided in	learning	VoxCeleb1 - 1-shot learning
allow to trade off few-shot learning speed versus the results quality	learning	VoxCeleb1 - 1-shot learning
per- forms better for low-shot learning (e.g. one-shot), while the FT	learning	VoxCeleb1 - 1-shot learning
variant allows fast (real-time) few-shot learning of new avatars, fine-tuning ultimately	learning	VoxCeleb1 - 1-shot learning
learning of ad	learning	VoxCeleb1 - 1-shot learning
and S. Levine. Model-agnostic meta- learning for fast adaptation of deep	learning	VoxCeleb1 - 1-shot learning
Y. Wu, et al. Transfer learning from speaker verification to multispeaker	learning	VoxCeleb1 - 1-shot learning
I. Kemelmacher- Shlizerman. Synthesizing Obama: learning lip sync from au- dio	learning	VoxCeleb1 - 1-shot learning
and Y. Wang. Adversarial meta- learning	learning	VoxCeleb1 - 1-shot learning
An adversarial approach to few-shot learning	learning	VoxCeleb1 - 1-shot learning
Pix2pixHD and our method, few-shot learning was done via fine-tuning for	learning	VoxCeleb1 - 1-shot learning
Method (T) Time, s Few-shot learning	learning	VoxCeleb1 - 1-shot learning
2: Quantitative comparison of few-shot learning and inference timings for the	learning	VoxCeleb1 - 1-shot learning
learning	learning	VoxCeleb1 - 1-shot learning
multiple training frames in few-shot learning problems, like in our final	learning	VoxCeleb1 - 1-shot learning
learning configu- ration, which turned out	learning	VoxCeleb1 - 1-shot learning
learning, we randomly initialize the person-specific	learning	VoxCeleb1 - 1-shot learning
learning objective and initialize the embedding	learning	VoxCeleb1 - 1-shot learning
learning or pretraining. We used eight	learning	VoxCeleb1 - 1-shot learning
shot learning problem formulation. The notation for	learning	VoxCeleb1 - 1-shot learning
learning or pretraining. We used eight	learning	VoxCeleb1 - 1-shot learning
shot learning problem formulation. The notation for	learning	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- tional neural networks to generate	-	VoxCeleb1 - 1-shot learning
- ate a personalized talking head	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- ter that is able to	-	VoxCeleb1 - 1-shot learning
- and one-shot learning of neural	-	VoxCeleb1 - 1-shot learning
- sarial training problems with high	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- alized photorealistic talking head models	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- sions and mimics of a	-	VoxCeleb1 - 1-shot learning
- ically, we consider the problem	-	VoxCeleb1 - 1-shot learning
- alistic personalized head images given	-	VoxCeleb1 - 1-shot learning
- marks, which drive the animation	-	VoxCeleb1 - 1-shot learning
- conferencing and multi-player games, as	-	VoxCeleb1 - 1-shot learning
- fects industry. Synthesizing realistic talking	-	VoxCeleb1 - 1-shot learning
- ity. This complexity stems not	-	VoxCeleb1 - 1-shot learning
- man visual system towards even	-	VoxCeleb1 - 1-shot learning
- pearance modeling of human heads	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- takes explains the current prevalence	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- ferencing systems	-	VoxCeleb1 - 1-shot learning
- posed to synthesize articulated head	-	VoxCeleb1 - 1-shot learning
- chine learning (including deep learning	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- age, the amount of motion	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- vNets) presents the new hope	-	VoxCeleb1 - 1-shot learning
- ever, to succeed, such methods	-	VoxCeleb1 - 1-shot learning
- lions of parameters for each	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- phisticated physical and optical modeling	-	VoxCeleb1 - 1-shot learning
- cessive for most practical telepresence	-	VoxCeleb1 - 1-shot learning
- els with as little effort	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- shot learning) and with limited	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- larly to [16, 20, 37	-	VoxCeleb1 - 1-shot learning
- yond the abilities of warping-based	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- sive pre-training (meta-learning) on a	-	VoxCeleb1 - 1-shot learning
- ing head videos corresponding to	-	VoxCeleb1 - 1-shot learning
- verse appearance. In the course	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- tem simulates few-shot learning tasks	-	VoxCeleb1 - 1-shot learning
- form landmark positions into realistically-looking	-	VoxCeleb1 - 1-shot learning
- alized photographs, given a small	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- ficient realism and personalization fidelity	-	VoxCeleb1 - 1-shot learning
- ing head models, including video	-	VoxCeleb1 - 1-shot learning
- ing of the appearance of	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- tension of the face modeling	-	VoxCeleb1 - 1-shot learning
- ability and higher complexity than	-	VoxCeleb1 - 1-shot learning
- ple, the results of face	-	VoxCeleb1 - 1-shot learning
- fledged talking head system	-	VoxCeleb1 - 1-shot learning
- tecture uses adversarial training [12	-	VoxCeleb1 - 1-shot learning
- ing projection discriminators [32]. Our	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- sifier, from which it can	-	VoxCeleb1 - 1-shot learning
- fiers of unseen classes, given	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- GAN [43], adversarial meta-learning [41	-	VoxCeleb1 - 1-shot learning
- trained networks to generate additional	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- mance, our method deals with	-	VoxCeleb1 - 1-shot learning
- ation models using similar adversarial	-	VoxCeleb1 - 1-shot learning
- marize, we bring the adversarial	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- learning framework. The former is	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- cation domain, the use of	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- plementation details	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- marks) to the embedding vectors	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- quence and with xi(t) its	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- ity of the face landmarks	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
these inputs into an N -	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- age yi(t) for the video	-	VoxCeleb1 - 1-shot learning
- sized video frame x̂i(t). The	-	VoxCeleb1 - 1-shot learning
- imize the similarity between its	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- trix P: ψ̂i = Pêi	-	VoxCeleb1 - 1-shot learning
landmark image into an N -	-	VoxCeleb1 - 1-shot learning
- criminator predicts a single scalar	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- rameters of all three networks	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- ing (K = 8 in	-	VoxCeleb1 - 1-shot learning
- domly draw a training video	-	VoxCeleb1 - 1-shot learning
- ditional K frames s1, s2	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- ding by simply averaging the	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- tion x̂i(t) using the perceptual	-	VoxCeleb1 - 1-shot learning
- responding to VGG19 [30] network	-	VoxCeleb1 - 1-shot learning
- sentially is a perceptual similarity	-	VoxCeleb1 - 1-shot learning
- respond to individual videos. The	-	VoxCeleb1 - 1-shot learning
maps its inputs to anN -	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- criminator. The match term LMCH(φ,W	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- ters θ,W,w0, b of the	-	VoxCeleb1 - 1-shot learning
- courages the increase of the	-	VoxCeleb1 - 1-shot learning
- ample x̂i(t) and the real	-	VoxCeleb1 - 1-shot learning
- nating updates of the embedder	-	VoxCeleb1 - 1-shot learning
- imize the losses LCNT,LADV and	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- sis is conditioned on the	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- timate the embedding for the	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- sponding to new landmark images	-	VoxCeleb1 - 1-shot learning
- erator using the estimated embedding	-	VoxCeleb1 - 1-shot learning
- learned parameters ψ, as well	-	VoxCeleb1 - 1-shot learning
- able identity gap that is	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- sult of the meta-learning stage	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- tors computed by the embedder	-	VoxCeleb1 - 1-shot learning
- tions of the fine-tuning stage	-	VoxCeleb1 - 1-shot learning
- learning variants. Thus, the generator	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- son et. al. [19], but	-	VoxCeleb1 - 1-shot learning
- malization [15] replaced by instance	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- efficients of instance normalization layers	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- ization layers in the downsampling	-	VoxCeleb1 - 1-shot learning
- mark images yi(t	-	VoxCeleb1 - 1-shot learning
- tional part of the discriminator	-	VoxCeleb1 - 1-shot learning
- out normalization layers). The discriminator	-	VoxCeleb1 - 1-shot learning
- pared to the embedder, has	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- serted at 32×32 spatial resolution	-	VoxCeleb1 - 1-shot learning
- tween activations of Conv1,6,11,20,29 VGG19	-	VoxCeleb1 - 1-shot learning
- nally, for LMCH we set	-	VoxCeleb1 - 1-shot learning
- tional layers to 64 and	-	VoxCeleb1 - 1-shot learning
- tor has 38 million parameters	-	VoxCeleb1 - 1-shot learning
- titative and qualitative evaluation: VoxCeleb1	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- fer to the text for	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- son not seen during meta-learning	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- reenactment scenario). For the evaluation	-	VoxCeleb1 - 1-shot learning
- out frames for each of	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- realism and identity preservation of	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- bedding vectors of the state-of-the-art	-	VoxCeleb1 - 1-shot learning
- work [9] for measuring identity	-	VoxCeleb1 - 1-shot learning
- tual similarity and realism of	-	VoxCeleb1 - 1-shot learning
- man respondents. We show people	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- not spot fakes based on	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- Celeb1 dataset). For Pix2pixHD, we	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- shot learning. X2Face, as a	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- tion, which arguably gives X2Face	-	VoxCeleb1 - 1-shot learning
- lines in three different setups	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- dom from the other video	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- perform our method on the	-	VoxCeleb1 - 1-shot learning
- imizes only perceptual metric, without	-	VoxCeleb1 - 1-shot learning
- and few-shot learning on a	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- ter correlates with visual quality	-	VoxCeleb1 - 1-shot learning
- ble 1-Top with the results	-	VoxCeleb1 - 1-shot learning
- alism and personalization degree achieved	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- out fine-tuning (by simply predicting	-	VoxCeleb1 - 1-shot learning
- ant is trained for half	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- sults, where animation is driven	-	VoxCeleb1 - 1-shot learning
- ent video of the same	-	VoxCeleb1 - 1-shot learning
- tary material and in Figure	-	VoxCeleb1 - 1-shot learning
- ble 1-Bottom) and the visual	-	VoxCeleb1 - 1-shot learning
- forms better for low-shot learning	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- ial fine-tuning	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- sons with similar geometry of	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- tographs in the source column	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- realistic virtual talking heads in	-	VoxCeleb1 - 1-shot learning
- ization score in our user	-	VoxCeleb1 - 1-shot learning
- ics representation (in particular, the	-	VoxCeleb1 - 1-shot learning
- son leads to a noticeable	-	VoxCeleb1 - 1-shot learning
- ing a different person and	-	VoxCeleb1 - 1-shot learning
- proach already provides a high-realism	-	VoxCeleb1 - 1-shot learning
- puter Graphics and Applications, 30(4):20–31	-	VoxCeleb1 - 1-shot learning
- sarial networks. In Artificial Neural	-	VoxCeleb1 - 1-shot learning
Networks and Machine Learning - ICANN, pages 594–603, 2018. 2	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- thesis of 3d faces. In	-	VoxCeleb1 - 1-shot learning
- 29, 2017, pages 1021–1030, 2017	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- learning for fast adaptation of	-	VoxCeleb1 - 1-shot learning
- ulation. In European Conference on	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- erative adversarial nets. In Advances	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- wanathan, and R. Garnett, editors	-	VoxCeleb1 - 1-shot learning
- formation Processing Systems 30, pages	-	VoxCeleb1 - 1-shot learning
- time with adaptive instance normalization	-	VoxCeleb1 - 1-shot learning
- ternational Conference on Machine Learning	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- shick, S. Guadarrama, and T	-	VoxCeleb1 - 1-shot learning
- tional architecture for fast feature	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- speech synthesis. In Proc. NIPS	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- SPEECH, 2017. 5	-	VoxCeleb1 - 1-shot learning
- gios, and I. Kokkinos. Deforming	-	VoxCeleb1 - 1-shot learning
- vised disentangling of shape and	-	VoxCeleb1 - 1-shot learning
- ropean Conference on Computer Vision	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- Shlizerman. Synthesizing Obama: learning lip	-	VoxCeleb1 - 1-shot learning
- dio. ACM Transactions on Graphics	-	VoxCeleb1 - 1-shot learning
- tral normalization for generative adversarial	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- erator architecture for generative adversarial	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- actment of RGB videos. In	-	VoxCeleb1 - 1-shot learning
- ference on Computer Vision and	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- tion, 2018. 4, 6	-	VoxCeleb1 - 1-shot learning
- learning. CoRR, abs/1806.03316, 2018. 2	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- ried out on a single	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- surement was averaged over 100	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- ing personalization fidelity and realism	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- duction of a training scheduler	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- vided by the embedder is	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- specific initialization of the discriminator	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- ration, which turned out to	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- tice that the results for	-	VoxCeleb1 - 1-shot learning
- sonalization fidelity. We, therefore, came	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
framework for video face editing.  1 Introduction	 1	VoxCeleb1 - 1-shot learning
considering gdA or gdR :  1	 1	VoxCeleb1 - 1-shot learning
 11	 1	VoxCeleb1 - 1-shot learning
the function fa→v : R  1	 1	VoxCeleb1 - 1-shot learning
 125,	 1	VoxCeleb1 - 1-shot learning
 16	 1	VoxCeleb1 - 1-shot learning
 10 when the loss plateaus. Once	 1	VoxCeleb1 - 1-shot learning
 1	 1	VoxCeleb1 - 1-shot learning
 1	 1	VoxCeleb1 - 1-shot learning
 1	 1	VoxCeleb1 - 1-shot learning
 1 0.0632 0% Training stage II	 1	VoxCeleb1 - 1-shot learning
 1 0.0630 0.32% Training stage I	 1	VoxCeleb1 - 1-shot learning
 17	 1	VoxCeleb1 - 1-shot learning
 17	 1	VoxCeleb1 - 1-shot learning
 1 confirm that both training with	 1	VoxCeleb1 - 1-shot learning
 1, 000 test images from [22	 1	VoxCeleb1 - 1-shot learning
 11	 1	VoxCeleb1 - 1-shot learning
 14	 1	VoxCeleb1 - 1-shot learning
 13	 1	VoxCeleb1 - 1-shot learning
 126	 1	VoxCeleb1 - 1-shot learning
 10, 1755–1758 (2009	 1	VoxCeleb1 - 1-shot learning
 17	 1	VoxCeleb1 - 1-shot learning
3D scans [4], or by learning 3DMM parameters directly from RGB	learning	VoxCeleb1 - 1-shot learning
accordingly. This is done by learning a forward mapping fp→v from	learning	VoxCeleb1 - 1-shot learning
improving the generated results. As learning the function fa→v : R	learning	VoxCeleb1 - 1-shot learning
with L1 loss, and a learning rate of 0.001. The learning	learning	VoxCeleb1 - 1-shot learning
phase is started with a learning rate of 0.0001. Testing. The	learning	VoxCeleb1 - 1-shot learning
Abbeel, P.: Infogan: Interpretable representation learning by information maximizing generative adversarial	learning	VoxCeleb1 - 1-shot learning
Denton, E.L., Birodkar, V.: Unsupervised learning of disentangled repre- sentations from	learning	VoxCeleb1 - 1-shot learning
facial animation by joint end-to-end learning of pose and emotion. ACM	learning	VoxCeleb1 - 1-shot learning
King, D.E.: Dlib-ml: A machine learning toolkit. The Journal of Machine	learning	VoxCeleb1 - 1-shot learning
tion of unconstrained faces by learning efficient H-CNN regressors. In: Proc	learning	VoxCeleb1 - 1-shot learning
S.M., Kemelmacher-Shlizerman, I.: Synthesizing Obama: learning lip sync from audio. ACM	learning	VoxCeleb1 - 1-shot learning
X., Liu, X.: Disentangled representation learning gan for pose-invariant face recognition	learning	VoxCeleb1 - 1-shot learning
- phisticated video and image editing	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- pared to state-of-the-art self-supervised/supervised methods	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- ing frame, audio data, or	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- ing frames. These frames are	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- tions. First, we propose a	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- tion, described in Section 6	-	VoxCeleb1 - 1-shot learning
- imation (or puppeteering) given one	-	VoxCeleb1 - 1-shot learning
- erature on supervised/self-supervised approaches; here	-	VoxCeleb1 - 1-shot learning
- marks [5, 12, 21, 30	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- tors of variation (e.g. optical	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- form images of one domain	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- work. This is illustrated in	-	VoxCeleb1 - 1-shot learning
- termine how to map from	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- straints based on the identity	-	VoxCeleb1 - 1-shot learning
- quently, we introduce additional loss	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-5 and Conv7 layers (i.e. layers	-	VoxCeleb1 - 1-shot learning
-7 layers (i.e. layers encoding higher	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- figuration A) [37] trained on	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- tional mapping fv→p is needed	-	VoxCeleb1 - 1-shot learning
- nected layer with bias and	-	VoxCeleb1 - 1-shot learning
- connected linear layer with bias	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- bedding learns to encode some	-	VoxCeleb1 - 1-shot learning
- tion. Given driving audio features	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- size of 16. First, it	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- ing/testing setups. Lower is better	-	VoxCeleb1 - 1-shot learning
- centage improvement over the L1	-	VoxCeleb1 - 1-shot learning
- egy and using additional views	-	VoxCeleb1 - 1-shot learning
- main (in this case a	-	VoxCeleb1 - 1-shot learning
- cleGAN is trained on pairs	-	VoxCeleb1 - 1-shot learning
- ing unrealistic results. Additionally, our	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- tion 4.1), the 25, 993	-	VoxCeleb1 - 1-shot learning
- neutral expressions in the source	-	VoxCeleb1 - 1-shot learning
- processing (X2Face + p.-p.) can	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- mentary material. Whilst some artefacts	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- eration using another face. This	-	VoxCeleb1 - 1-shot learning
- constrained settings (e.g. an unseen	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- tioned on other modalities, the	-	VoxCeleb1 - 1-shot learning
- teresting avenue of research: how	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- eration quality of these methods	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- tions/comments. This work was funded	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-4	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- ment networks. In: Proc. ICCV	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- sentations from video. In: NIPS	-	VoxCeleb1 - 1-shot learning
- tional neural networks. In: Proc	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- actions on Graphics (TOG) (2017	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- lutional neural networks. In: Proc	-	VoxCeleb1 - 1-shot learning
- tional inverse graphics network. In	-	VoxCeleb1 - 1-shot learning
- tion of unconstrained faces by	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- tation, face swapping, and face	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- strained photo collections. In: Proc	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- scale image recognition. In: International	-	VoxCeleb1 - 1-shot learning
- sentations (2015	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- pretable transformations with encoder-decoder networks	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
-	-	VoxCeleb1 - 1-shot learning
- lation using cycle-consistent adversarial networks	-	VoxCeleb1 - 1-shot learning
VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 1-shot learning
ablation studies, while by using VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 1-shot learning
our method on a larger VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 1-shot learning
from test videos of the VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 1-shot learning
our best models on the VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 1-shot learning
poses were taken from the VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 1-shot learning
was trained only for the VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 1-shot learning
extended qualitative comparisons on the VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 1-shot learning
extended qualitative comparison on the VoxCeleb2 	VoxCeleb2 	VoxCeleb2 - 1-shot learning
 1	 1	VoxCeleb2 - 1-shot learning
people and even portrait paintings.  1	 1	VoxCeleb2 - 1-shot learning
motion, head rotation, and disocclusion  1	 1	VoxCeleb2 - 1-shot learning
 1	 1	VoxCeleb2 - 1-shot learning
 1 · 10−2 for VGG19 and	 1	VoxCeleb2 - 1-shot learning
 1 · 101. Fi- nally, for	 1	VoxCeleb2 - 1-shot learning
 15 million parameters, the genera- tor	 1	VoxCeleb2 - 1-shot learning
 1 fps) and VoxCeleb2 [8] (224p	 1	VoxCeleb2 - 1-shot learning
 10 times more videos	 1	VoxCeleb2 - 1-shot learning
 1	 1	VoxCeleb2 - 1-shot learning
 1, 8 and 32 frames in	 1	VoxCeleb2 - 1-shot learning
 1	 1	VoxCeleb2 - 1-shot learning
 1	 1	VoxCeleb2 - 1-shot learning
 150 epochs without the embedding matching	 1	VoxCeleb2 - 1-shot learning
 1	 1	VoxCeleb2 - 1-shot learning
 1	 1	VoxCeleb2 - 1-shot learning
 1	 1	VoxCeleb2 - 1-shot learning
 10040	 1	VoxCeleb2 - 1-shot learning
 187	 1	VoxCeleb2 - 1-shot learning
 1021	 1	VoxCeleb2 - 1-shot learning
 1126	 1	VoxCeleb2 - 1-shot learning
NeurIPS, pages 2371–2380, 2018. 2  10	 1	VoxCeleb2 - 1-shot learning
 1, we trained the models on	 1	VoxCeleb2 - 1-shot learning
 100 iterations	 1	VoxCeleb2 - 1-shot learning
the rest of the figures.  11	 1	VoxCeleb2 - 1-shot learning
32  1	 1	VoxCeleb2 - 1-shot learning
results from our final model.  12	 1	VoxCeleb2 - 1-shot learning
sequence of a different person.  13	 1	VoxCeleb2 - 1-shot learning
sequence with the same person.  14	 1	VoxCeleb2 - 1-shot learning
 16	 1	VoxCeleb2 - 1-shot learning
sequence of the same person.  15	 1	VoxCeleb2 - 1-shot learning
32  1	 1	VoxCeleb2 - 1-shot learning
 10	 1	VoxCeleb2 - 1-shot learning
3 in the main paper.  16	 1	VoxCeleb2 - 1-shot learning
 11	 1	VoxCeleb2 - 1-shot learning
3 in the main paper.  17	 1	VoxCeleb2 - 1-shot learning
32  1	 1	VoxCeleb2 - 1-shot learning
 12	 1	VoxCeleb2 - 1-shot learning
4 in the main paper.  18	 1	VoxCeleb2 - 1-shot learning
 13	 1	VoxCeleb2 - 1-shot learning
4 in the main paper.  19	 1	VoxCeleb2 - 1-shot learning
shot learning of neural talking head models	shot learning	VoxCeleb2 - 1-shot learning
handful of photographs (so-called few- shot learning) and with limited training time	shot learning	VoxCeleb2 - 1-shot learning
shot learning), while adding a few more	shot learning	VoxCeleb2 - 1-shot learning
shot learning ability is obtained through exten	shot learning	VoxCeleb2 - 1-shot learning
shot learning tasks and learns to trans	shot learning	VoxCeleb2 - 1-shot learning
shot learning of generative models) and some	shot learning	VoxCeleb2 - 1-shot learning
shot learning by fine-tuning	shot learning	VoxCeleb2 - 1-shot learning
shot learning settings. Please re- fer to	shot learning	VoxCeleb2 - 1-shot learning
shot learning sets of size T for	shot learning	VoxCeleb2 - 1-shot learning
shot learning, the evaluation is performed on	shot learning	VoxCeleb2 - 1-shot learning
frames T used in few- shot learning	shot learning	VoxCeleb2 - 1-shot learning
shot learning on a video of a	shot learning	VoxCeleb2 - 1-shot learning
shot learning timings. Both are provided in	shot learning	VoxCeleb2 - 1-shot learning
shot learning speed versus the results quality	shot learning	VoxCeleb2 - 1-shot learning
shot learning (e.g. one-shot), while the FT	shot learning	VoxCeleb2 - 1-shot learning
shot learning of new avatars, fine-tuning ultimately	shot learning	VoxCeleb2 - 1-shot learning
shot learning	shot learning	VoxCeleb2 - 1-shot learning
shot learning was done via fine-tuning for	shot learning	VoxCeleb2 - 1-shot learning
shot learning	shot learning	VoxCeleb2 - 1-shot learning
shot learning and inference timings for the	shot learning	VoxCeleb2 - 1-shot learning
shot learning problems, like in our final	shot learning	VoxCeleb2 - 1-shot learning
or pretraining. We used eight shot learning problem formulation. The notation for	shot learning	VoxCeleb2 - 1-shot learning
or pretraining. We used eight shot learning problem formulation. The notation for	shot learning	VoxCeleb2 - 1-shot learning
learning on a large dataset of	learning	VoxCeleb2 - 1-shot learning
to frame few- and one-shot learning of neural talking head models	learning	VoxCeleb2 - 1-shot learning
fields synthesized using ma- chine learning (including deep learning) [11, 29	learning	VoxCeleb2 - 1-shot learning
of photographs (so-called few- shot learning) and with limited training time	learning	VoxCeleb2 - 1-shot learning
on a single photograph (one-shot learning), while adding a few more	learning	VoxCeleb2 - 1-shot learning
The few-shot learning ability is obtained through exten	learning	VoxCeleb2 - 1-shot learning
learning) on a large corpus of	learning	VoxCeleb2 - 1-shot learning
learning, our sys- tem simulates few-shot	learning	VoxCeleb2 - 1-shot learning
learning tasks and learns to trans	learning	VoxCeleb2 - 1-shot learning
sets up a new adversarial learning problem with high-capacity generator and	learning	VoxCeleb2 - 1-shot learning
learning	learning	VoxCeleb2 - 1-shot learning
and, more recently, with deep learning [22, 25] (to name just	learning	VoxCeleb2 - 1-shot learning
learning stage uses the adaptive instance	learning	VoxCeleb2 - 1-shot learning
learning to obtain the initial state	learning	VoxCeleb2 - 1-shot learning
learning	learning	VoxCeleb2 - 1-shot learning
learning [41] use adversarially- trained networks	learning	VoxCeleb2 - 1-shot learning
learning stage. While these methods are	learning	VoxCeleb2 - 1-shot learning
adversarial fine-tuning into the meta- learning framework. The former is applied	learning	VoxCeleb2 - 1-shot learning
learning stage	learning	VoxCeleb2 - 1-shot learning
4, 18]. Their setting (few-shot learning of generative models) and some	learning	VoxCeleb2 - 1-shot learning
domain, the use of adversarial learning, its specific adaptation to the	learning	VoxCeleb2 - 1-shot learning
learning process and numerous im- plementation	learning	VoxCeleb2 - 1-shot learning
learning architecture involves the embedder network	learning	VoxCeleb2 - 1-shot learning
learning, we pass sets of frames	learning	VoxCeleb2 - 1-shot learning
learning stage of our approach assumes	learning	VoxCeleb2 - 1-shot learning
its t-th frame. During the learning process, as well as during	learning	VoxCeleb2 - 1-shot learning
learning stage of our approach, the	learning	VoxCeleb2 - 1-shot learning
learning stage. In general, during meta-learning	learning	VoxCeleb2 - 1-shot learning
learning, only ψ are trained directly	learning	VoxCeleb2 - 1-shot learning
learning stage	learning	VoxCeleb2 - 1-shot learning
learning stage of our approach, the	learning	VoxCeleb2 - 1-shot learning
3.3. Few-shot learning by fine-tuning	learning	VoxCeleb2 - 1-shot learning
learning has converged, our system can	learning	VoxCeleb2 - 1-shot learning
learning stage. As before, the synthe	learning	VoxCeleb2 - 1-shot learning
learning stage	learning	VoxCeleb2 - 1-shot learning
learning stage. A straightforward way to	learning	VoxCeleb2 - 1-shot learning
learning with a single video sequence	learning	VoxCeleb2 - 1-shot learning
learning stage to initialize ψ′, i.e	learning	VoxCeleb2 - 1-shot learning
learning stage. The initialization of w	learning	VoxCeleb2 - 1-shot learning
learning stage	learning	VoxCeleb2 - 1-shot learning
learning stage. For the intiailization, we	learning	VoxCeleb2 - 1-shot learning
learning dataset). However, the match term	learning	VoxCeleb2 - 1-shot learning
learning process ensures the similarity between	learning	VoxCeleb2 - 1-shot learning
Once the new learning problem is set up, the	learning	VoxCeleb2 - 1-shot learning
follow directly from the meta- learning variants. Thus, the generator parameters	learning	VoxCeleb2 - 1-shot learning
learning stage is also crucial. As	learning	VoxCeleb2 - 1-shot learning
Adam [21]. We set the learning rate of the embedder and	learning	VoxCeleb2 - 1-shot learning
different datasets with multiple few-shot learning settings. Please re- fer to	learning	VoxCeleb2 - 1-shot learning
fine-tune all models on few-shot learning sets of size T for	learning	VoxCeleb2 - 1-shot learning
learning (or pretraining) stage. After the	learning	VoxCeleb2 - 1-shot learning
few-shot learning, the evaluation is performed on	learning	VoxCeleb2 - 1-shot learning
T used in few- shot learning	learning	VoxCeleb2 - 1-shot learning
we perform one- and few-shot learning on a video of a	learning	VoxCeleb2 - 1-shot learning
learning or pretraining. We set the	learning	VoxCeleb2 - 1-shot learning
the comparison of the few-shot learning timings. Both are provided in	learning	VoxCeleb2 - 1-shot learning
allow to trade off few-shot learning speed versus the results quality	learning	VoxCeleb2 - 1-shot learning
per- forms better for low-shot learning (e.g. one-shot), while the FT	learning	VoxCeleb2 - 1-shot learning
variant allows fast (real-time) few-shot learning of new avatars, fine-tuning ultimately	learning	VoxCeleb2 - 1-shot learning
learning of ad	learning	VoxCeleb2 - 1-shot learning
and S. Levine. Model-agnostic meta- learning for fast adaptation of deep	learning	VoxCeleb2 - 1-shot learning
Y. Wu, et al. Transfer learning from speaker verification to multispeaker	learning	VoxCeleb2 - 1-shot learning
I. Kemelmacher- Shlizerman. Synthesizing Obama: learning lip sync from au- dio	learning	VoxCeleb2 - 1-shot learning
and Y. Wang. Adversarial meta- learning	learning	VoxCeleb2 - 1-shot learning
An adversarial approach to few-shot learning	learning	VoxCeleb2 - 1-shot learning
Pix2pixHD and our method, few-shot learning was done via fine-tuning for	learning	VoxCeleb2 - 1-shot learning
Method (T) Time, s Few-shot learning	learning	VoxCeleb2 - 1-shot learning
2: Quantitative comparison of few-shot learning and inference timings for the	learning	VoxCeleb2 - 1-shot learning
learning	learning	VoxCeleb2 - 1-shot learning
multiple training frames in few-shot learning problems, like in our final	learning	VoxCeleb2 - 1-shot learning
learning configu- ration, which turned out	learning	VoxCeleb2 - 1-shot learning
learning, we randomly initialize the person-specific	learning	VoxCeleb2 - 1-shot learning
learning objective and initialize the embedding	learning	VoxCeleb2 - 1-shot learning
learning or pretraining. We used eight	learning	VoxCeleb2 - 1-shot learning
shot learning problem formulation. The notation for	learning	VoxCeleb2 - 1-shot learning
learning or pretraining. We used eight	learning	VoxCeleb2 - 1-shot learning
shot learning problem formulation. The notation for	learning	VoxCeleb2 - 1-shot learning
videos at 1 fps) and VoxCeleb2 [8] (224p videos at 25	VoxCeleb2	VoxCeleb2 - 1-shot learning
VoxCeleb2 Ours-FF (1) 46.1 0.61 0.42	VoxCeleb2	VoxCeleb2 - 1-shot learning
ablation studies, while by using VoxCeleb2 we show the full potential	VoxCeleb2	VoxCeleb2 - 1-shot learning
our method on a larger VoxCeleb2 dataset. Here, we train two	VoxCeleb2	VoxCeleb2 - 1-shot learning
from test videos of the VoxCeleb2 dataset. We rank these videos	VoxCeleb2	VoxCeleb2 - 1-shot learning
our best models on the VoxCeleb2 dataset. The number of training	VoxCeleb2	VoxCeleb2 - 1-shot learning
poses were taken from the VoxCeleb2 dataset. Digital zoom recommended	VoxCeleb2	VoxCeleb2 - 1-shot learning
was trained only for the VoxCeleb2 dataset. The comparison was car	VoxCeleb2	VoxCeleb2 - 1-shot learning
extended qualitative comparisons on the VoxCeleb2 dataset. Here, the comparison is	VoxCeleb2	VoxCeleb2 - 1-shot learning
extended qualitative comparison on the VoxCeleb2 dataset. Here, we compare qualitative	VoxCeleb2	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- tional neural networks to generate	-	VoxCeleb2 - 1-shot learning
- ate a personalized talking head	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- ter that is able to	-	VoxCeleb2 - 1-shot learning
- and one-shot learning of neural	-	VoxCeleb2 - 1-shot learning
- sarial training problems with high	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- alized photorealistic talking head models	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- sions and mimics of a	-	VoxCeleb2 - 1-shot learning
- ically, we consider the problem	-	VoxCeleb2 - 1-shot learning
- alistic personalized head images given	-	VoxCeleb2 - 1-shot learning
- marks, which drive the animation	-	VoxCeleb2 - 1-shot learning
- conferencing and multi-player games, as	-	VoxCeleb2 - 1-shot learning
- fects industry. Synthesizing realistic talking	-	VoxCeleb2 - 1-shot learning
- ity. This complexity stems not	-	VoxCeleb2 - 1-shot learning
- man visual system towards even	-	VoxCeleb2 - 1-shot learning
- pearance modeling of human heads	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- takes explains the current prevalence	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- ferencing systems	-	VoxCeleb2 - 1-shot learning
- posed to synthesize articulated head	-	VoxCeleb2 - 1-shot learning
- chine learning (including deep learning	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- age, the amount of motion	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- vNets) presents the new hope	-	VoxCeleb2 - 1-shot learning
- ever, to succeed, such methods	-	VoxCeleb2 - 1-shot learning
- lions of parameters for each	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- phisticated physical and optical modeling	-	VoxCeleb2 - 1-shot learning
- cessive for most practical telepresence	-	VoxCeleb2 - 1-shot learning
- els with as little effort	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- shot learning) and with limited	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- larly to [16, 20, 37	-	VoxCeleb2 - 1-shot learning
- yond the abilities of warping-based	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- sive pre-training (meta-learning) on a	-	VoxCeleb2 - 1-shot learning
- ing head videos corresponding to	-	VoxCeleb2 - 1-shot learning
- verse appearance. In the course	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- tem simulates few-shot learning tasks	-	VoxCeleb2 - 1-shot learning
- form landmark positions into realistically-looking	-	VoxCeleb2 - 1-shot learning
- alized photographs, given a small	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- ficient realism and personalization fidelity	-	VoxCeleb2 - 1-shot learning
- ing head models, including video	-	VoxCeleb2 - 1-shot learning
- ing of the appearance of	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- tension of the face modeling	-	VoxCeleb2 - 1-shot learning
- ability and higher complexity than	-	VoxCeleb2 - 1-shot learning
- ple, the results of face	-	VoxCeleb2 - 1-shot learning
- fledged talking head system	-	VoxCeleb2 - 1-shot learning
- tecture uses adversarial training [12	-	VoxCeleb2 - 1-shot learning
- ing projection discriminators [32]. Our	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- sifier, from which it can	-	VoxCeleb2 - 1-shot learning
- fiers of unseen classes, given	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- GAN [43], adversarial meta-learning [41	-	VoxCeleb2 - 1-shot learning
- trained networks to generate additional	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- mance, our method deals with	-	VoxCeleb2 - 1-shot learning
- ation models using similar adversarial	-	VoxCeleb2 - 1-shot learning
- marize, we bring the adversarial	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- learning framework. The former is	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- cation domain, the use of	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- plementation details	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- marks) to the embedding vectors	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- quence and with xi(t) its	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- ity of the face landmarks	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
these inputs into an N -	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- age yi(t) for the video	-	VoxCeleb2 - 1-shot learning
- sized video frame x̂i(t). The	-	VoxCeleb2 - 1-shot learning
- imize the similarity between its	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- trix P: ψ̂i = Pêi	-	VoxCeleb2 - 1-shot learning
landmark image into an N -	-	VoxCeleb2 - 1-shot learning
- criminator predicts a single scalar	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- rameters of all three networks	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- ing (K = 8 in	-	VoxCeleb2 - 1-shot learning
- domly draw a training video	-	VoxCeleb2 - 1-shot learning
- ditional K frames s1, s2	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- ding by simply averaging the	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- tion x̂i(t) using the perceptual	-	VoxCeleb2 - 1-shot learning
- responding to VGG19 [30] network	-	VoxCeleb2 - 1-shot learning
- sentially is a perceptual similarity	-	VoxCeleb2 - 1-shot learning
- respond to individual videos. The	-	VoxCeleb2 - 1-shot learning
maps its inputs to anN -	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- criminator. The match term LMCH(φ,W	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- ters θ,W,w0, b of the	-	VoxCeleb2 - 1-shot learning
- courages the increase of the	-	VoxCeleb2 - 1-shot learning
- ample x̂i(t) and the real	-	VoxCeleb2 - 1-shot learning
- nating updates of the embedder	-	VoxCeleb2 - 1-shot learning
- imize the losses LCNT,LADV and	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- sis is conditioned on the	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- timate the embedding for the	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- sponding to new landmark images	-	VoxCeleb2 - 1-shot learning
- erator using the estimated embedding	-	VoxCeleb2 - 1-shot learning
- learned parameters ψ, as well	-	VoxCeleb2 - 1-shot learning
- able identity gap that is	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- sult of the meta-learning stage	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- tors computed by the embedder	-	VoxCeleb2 - 1-shot learning
- tions of the fine-tuning stage	-	VoxCeleb2 - 1-shot learning
- learning variants. Thus, the generator	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- son et. al. [19], but	-	VoxCeleb2 - 1-shot learning
- malization [15] replaced by instance	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- efficients of instance normalization layers	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- ization layers in the downsampling	-	VoxCeleb2 - 1-shot learning
- mark images yi(t	-	VoxCeleb2 - 1-shot learning
- tional part of the discriminator	-	VoxCeleb2 - 1-shot learning
- out normalization layers). The discriminator	-	VoxCeleb2 - 1-shot learning
- pared to the embedder, has	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- serted at 32×32 spatial resolution	-	VoxCeleb2 - 1-shot learning
- tween activations of Conv1,6,11,20,29 VGG19	-	VoxCeleb2 - 1-shot learning
- nally, for LMCH we set	-	VoxCeleb2 - 1-shot learning
- tional layers to 64 and	-	VoxCeleb2 - 1-shot learning
- tor has 38 million parameters	-	VoxCeleb2 - 1-shot learning
- titative and qualitative evaluation: VoxCeleb1	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- fer to the text for	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- son not seen during meta-learning	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- reenactment scenario). For the evaluation	-	VoxCeleb2 - 1-shot learning
- out frames for each of	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- realism and identity preservation of	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- bedding vectors of the state-of-the-art	-	VoxCeleb2 - 1-shot learning
- work [9] for measuring identity	-	VoxCeleb2 - 1-shot learning
- tual similarity and realism of	-	VoxCeleb2 - 1-shot learning
- man respondents. We show people	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- not spot fakes based on	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- Celeb1 dataset). For Pix2pixHD, we	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- shot learning. X2Face, as a	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- tion, which arguably gives X2Face	-	VoxCeleb2 - 1-shot learning
- lines in three different setups	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- dom from the other video	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- perform our method on the	-	VoxCeleb2 - 1-shot learning
- imizes only perceptual metric, without	-	VoxCeleb2 - 1-shot learning
- and few-shot learning on a	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- ter correlates with visual quality	-	VoxCeleb2 - 1-shot learning
- ble 1-Top with the results	-	VoxCeleb2 - 1-shot learning
- alism and personalization degree achieved	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- out fine-tuning (by simply predicting	-	VoxCeleb2 - 1-shot learning
- ant is trained for half	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- sults, where animation is driven	-	VoxCeleb2 - 1-shot learning
- ent video of the same	-	VoxCeleb2 - 1-shot learning
- tary material and in Figure	-	VoxCeleb2 - 1-shot learning
- ble 1-Bottom) and the visual	-	VoxCeleb2 - 1-shot learning
- forms better for low-shot learning	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- ial fine-tuning	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- sons with similar geometry of	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- tographs in the source column	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- realistic virtual talking heads in	-	VoxCeleb2 - 1-shot learning
- ization score in our user	-	VoxCeleb2 - 1-shot learning
- ics representation (in particular, the	-	VoxCeleb2 - 1-shot learning
- son leads to a noticeable	-	VoxCeleb2 - 1-shot learning
- ing a different person and	-	VoxCeleb2 - 1-shot learning
- proach already provides a high-realism	-	VoxCeleb2 - 1-shot learning
- puter Graphics and Applications, 30(4):20–31	-	VoxCeleb2 - 1-shot learning
- sarial networks. In Artificial Neural	-	VoxCeleb2 - 1-shot learning
Networks and Machine Learning - ICANN, pages 594–603, 2018. 2	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- thesis of 3d faces. In	-	VoxCeleb2 - 1-shot learning
- 29, 2017, pages 1021–1030, 2017	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- learning for fast adaptation of	-	VoxCeleb2 - 1-shot learning
- ulation. In European Conference on	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- erative adversarial nets. In Advances	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- wanathan, and R. Garnett, editors	-	VoxCeleb2 - 1-shot learning
- formation Processing Systems 30, pages	-	VoxCeleb2 - 1-shot learning
- time with adaptive instance normalization	-	VoxCeleb2 - 1-shot learning
- ternational Conference on Machine Learning	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- shick, S. Guadarrama, and T	-	VoxCeleb2 - 1-shot learning
- tional architecture for fast feature	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- speech synthesis. In Proc. NIPS	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- SPEECH, 2017. 5	-	VoxCeleb2 - 1-shot learning
- gios, and I. Kokkinos. Deforming	-	VoxCeleb2 - 1-shot learning
- vised disentangling of shape and	-	VoxCeleb2 - 1-shot learning
- ropean Conference on Computer Vision	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- Shlizerman. Synthesizing Obama: learning lip	-	VoxCeleb2 - 1-shot learning
- dio. ACM Transactions on Graphics	-	VoxCeleb2 - 1-shot learning
- tral normalization for generative adversarial	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- erator architecture for generative adversarial	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- actment of RGB videos. In	-	VoxCeleb2 - 1-shot learning
- ference on Computer Vision and	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- tion, 2018. 4, 6	-	VoxCeleb2 - 1-shot learning
- learning. CoRR, abs/1806.03316, 2018. 2	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- ried out on a single	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- surement was averaged over 100	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- ing personalization fidelity and realism	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- duction of a training scheduler	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- vided by the embedder is	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- specific initialization of the discriminator	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- ration, which turned out to	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
- tice that the results for	-	VoxCeleb2 - 1-shot learning
- sonalization fidelity. We, therefore, came	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
-	-	VoxCeleb2 - 1-shot learning
T) FID↓ SSIM↑ CSIM↑ USER↓ VoxCeleb1  X2Face (1) 45.8 0.68 0.16	VoxCeleb1 	VoxCeleb1 - 8-shot learning
than the former. VoxCeleb1 	VoxCeleb1 	VoxCeleb1 - 8-shot learning
Methods. On the VoxCeleb1 	VoxCeleb1 	VoxCeleb1 - 8-shot learning
Figure 3: Comparison on the VoxCeleb1 	VoxCeleb1 	VoxCeleb1 - 8-shot learning
extended qualitative comparisons on the VoxCeleb1 	VoxCeleb1 	VoxCeleb1 - 8-shot learning
extended qualitative comparison on the VoxCeleb1 	VoxCeleb1 	VoxCeleb1 - 8-shot learning
quan- titative and qualitative evaluation: VoxCeleb1 [26] (256p videos at 1	VoxCeleb1	VoxCeleb1 - 8-shot learning
T) FID↓ SSIM↑ CSIM↑ USER↓ VoxCeleb1	VoxCeleb1	VoxCeleb1 - 8-shot learning
than the former. VoxCeleb1 is used for comparison with	VoxCeleb1	VoxCeleb1 - 8-shot learning
Methods. On the VoxCeleb1 dataset we compare our model	VoxCeleb1	VoxCeleb1 - 8-shot learning
Figure 3: Comparison on the VoxCeleb1 dataset. For each of the	VoxCeleb1	VoxCeleb1 - 8-shot learning
to smaller-scale models trained on VoxCeleb1	VoxCeleb1	VoxCeleb1 - 8-shot learning
extended qualitative comparisons on the VoxCeleb1 dataset. Here, the comparison is	VoxCeleb1	VoxCeleb1 - 8-shot learning
extended qualitative comparison on the VoxCeleb1 dataset. Here, we compare qualitative	VoxCeleb1	VoxCeleb1 - 8-shot learning
 8 · 101	 8	VoxCeleb1 - 8-shot learning
1  8	 8	VoxCeleb1 - 8-shot learning
1  8	 8	VoxCeleb1 - 8-shot learning
already provides a high-realism solution.  8	 8	VoxCeleb1 - 8-shot learning
 8 images. Each mea- surement was	 8	VoxCeleb1 - 8-shot learning
1  8	 8	VoxCeleb1 - 8-shot learning
1  8	 8	VoxCeleb1 - 8-shot learning
 8	 8	VoxCeleb1 - 8-shot learning
1  8	 8	VoxCeleb1 - 8-shot learning
1  8	 8	VoxCeleb1 - 8-shot learning
1  8	 8	VoxCeleb1 - 8-shot learning
1  8	 8	VoxCeleb1 - 8-shot learning
learning on a large dataset of	learning	VoxCeleb1 - 8-shot learning
to frame few- and one-shot learning of neural talking head models	learning	VoxCeleb1 - 8-shot learning
fields synthesized using ma- chine learning (including deep learning) [11, 29	learning	VoxCeleb1 - 8-shot learning
of photographs (so-called few- shot learning) and with limited training time	learning	VoxCeleb1 - 8-shot learning
on a single photograph (one-shot learning), while adding a few more	learning	VoxCeleb1 - 8-shot learning
The few-shot learning ability is obtained through exten	learning	VoxCeleb1 - 8-shot learning
learning) on a large corpus of	learning	VoxCeleb1 - 8-shot learning
learning, our sys- tem simulates few-shot	learning	VoxCeleb1 - 8-shot learning
learning tasks and learns to trans	learning	VoxCeleb1 - 8-shot learning
sets up a new adversarial learning problem with high-capacity generator and	learning	VoxCeleb1 - 8-shot learning
learning	learning	VoxCeleb1 - 8-shot learning
and, more recently, with deep learning [22, 25] (to name just	learning	VoxCeleb1 - 8-shot learning
learning stage uses the adaptive instance	learning	VoxCeleb1 - 8-shot learning
learning to obtain the initial state	learning	VoxCeleb1 - 8-shot learning
learning	learning	VoxCeleb1 - 8-shot learning
learning [41] use adversarially- trained networks	learning	VoxCeleb1 - 8-shot learning
learning stage. While these methods are	learning	VoxCeleb1 - 8-shot learning
adversarial fine-tuning into the meta- learning framework. The former is applied	learning	VoxCeleb1 - 8-shot learning
learning stage	learning	VoxCeleb1 - 8-shot learning
4, 18]. Their setting (few-shot learning of generative models) and some	learning	VoxCeleb1 - 8-shot learning
domain, the use of adversarial learning, its specific adaptation to the	learning	VoxCeleb1 - 8-shot learning
learning process and numerous im- plementation	learning	VoxCeleb1 - 8-shot learning
learning architecture involves the embedder network	learning	VoxCeleb1 - 8-shot learning
learning, we pass sets of frames	learning	VoxCeleb1 - 8-shot learning
learning stage of our approach assumes	learning	VoxCeleb1 - 8-shot learning
its t-th frame. During the learning process, as well as during	learning	VoxCeleb1 - 8-shot learning
learning stage of our approach, the	learning	VoxCeleb1 - 8-shot learning
learning stage. In general, during meta-learning	learning	VoxCeleb1 - 8-shot learning
learning, only ψ are trained directly	learning	VoxCeleb1 - 8-shot learning
learning stage	learning	VoxCeleb1 - 8-shot learning
learning stage of our approach, the	learning	VoxCeleb1 - 8-shot learning
3.3. Few-shot learning by fine-tuning	learning	VoxCeleb1 - 8-shot learning
learning has converged, our system can	learning	VoxCeleb1 - 8-shot learning
learning stage. As before, the synthe	learning	VoxCeleb1 - 8-shot learning
learning stage	learning	VoxCeleb1 - 8-shot learning
learning stage. A straightforward way to	learning	VoxCeleb1 - 8-shot learning
learning with a single video sequence	learning	VoxCeleb1 - 8-shot learning
learning stage to initialize ψ′, i.e	learning	VoxCeleb1 - 8-shot learning
learning stage. The initialization of w	learning	VoxCeleb1 - 8-shot learning
learning stage	learning	VoxCeleb1 - 8-shot learning
learning stage. For the intiailization, we	learning	VoxCeleb1 - 8-shot learning
learning dataset). However, the match term	learning	VoxCeleb1 - 8-shot learning
learning process ensures the similarity between	learning	VoxCeleb1 - 8-shot learning
Once the new learning problem is set up, the	learning	VoxCeleb1 - 8-shot learning
follow directly from the meta- learning variants. Thus, the generator parameters	learning	VoxCeleb1 - 8-shot learning
learning stage is also crucial. As	learning	VoxCeleb1 - 8-shot learning
Adam [21]. We set the learning rate of the embedder and	learning	VoxCeleb1 - 8-shot learning
different datasets with multiple few-shot learning settings. Please re- fer to	learning	VoxCeleb1 - 8-shot learning
fine-tune all models on few-shot learning sets of size T for	learning	VoxCeleb1 - 8-shot learning
learning (or pretraining) stage. After the	learning	VoxCeleb1 - 8-shot learning
few-shot learning, the evaluation is performed on	learning	VoxCeleb1 - 8-shot learning
T used in few- shot learning	learning	VoxCeleb1 - 8-shot learning
we perform one- and few-shot learning on a video of a	learning	VoxCeleb1 - 8-shot learning
learning or pretraining. We set the	learning	VoxCeleb1 - 8-shot learning
the comparison of the few-shot learning timings. Both are provided in	learning	VoxCeleb1 - 8-shot learning
allow to trade off few-shot learning speed versus the results quality	learning	VoxCeleb1 - 8-shot learning
per- forms better for low-shot learning (e.g. one-shot), while the FT	learning	VoxCeleb1 - 8-shot learning
variant allows fast (real-time) few-shot learning of new avatars, fine-tuning ultimately	learning	VoxCeleb1 - 8-shot learning
learning of ad	learning	VoxCeleb1 - 8-shot learning
and S. Levine. Model-agnostic meta- learning for fast adaptation of deep	learning	VoxCeleb1 - 8-shot learning
Y. Wu, et al. Transfer learning from speaker verification to multispeaker	learning	VoxCeleb1 - 8-shot learning
I. Kemelmacher- Shlizerman. Synthesizing Obama: learning lip sync from au- dio	learning	VoxCeleb1 - 8-shot learning
and Y. Wang. Adversarial meta- learning	learning	VoxCeleb1 - 8-shot learning
An adversarial approach to few-shot learning	learning	VoxCeleb1 - 8-shot learning
Pix2pixHD and our method, few-shot learning was done via fine-tuning for	learning	VoxCeleb1 - 8-shot learning
Method (T) Time, s Few-shot learning	learning	VoxCeleb1 - 8-shot learning
2: Quantitative comparison of few-shot learning and inference timings for the	learning	VoxCeleb1 - 8-shot learning
learning	learning	VoxCeleb1 - 8-shot learning
multiple training frames in few-shot learning problems, like in our final	learning	VoxCeleb1 - 8-shot learning
learning configu- ration, which turned out	learning	VoxCeleb1 - 8-shot learning
learning, we randomly initialize the person-specific	learning	VoxCeleb1 - 8-shot learning
learning objective and initialize the embedding	learning	VoxCeleb1 - 8-shot learning
learning or pretraining. We used eight	learning	VoxCeleb1 - 8-shot learning
shot learning problem formulation. The notation for	learning	VoxCeleb1 - 8-shot learning
learning or pretraining. We used eight	learning	VoxCeleb1 - 8-shot learning
shot learning problem formulation. The notation for	learning	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- tional neural networks to generate	-	VoxCeleb1 - 8-shot learning
- ate a personalized talking head	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- ter that is able to	-	VoxCeleb1 - 8-shot learning
- and one-shot learning of neural	-	VoxCeleb1 - 8-shot learning
- sarial training problems with high	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- alized photorealistic talking head models	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- sions and mimics of a	-	VoxCeleb1 - 8-shot learning
- ically, we consider the problem	-	VoxCeleb1 - 8-shot learning
- alistic personalized head images given	-	VoxCeleb1 - 8-shot learning
- marks, which drive the animation	-	VoxCeleb1 - 8-shot learning
- conferencing and multi-player games, as	-	VoxCeleb1 - 8-shot learning
- fects industry. Synthesizing realistic talking	-	VoxCeleb1 - 8-shot learning
- ity. This complexity stems not	-	VoxCeleb1 - 8-shot learning
- man visual system towards even	-	VoxCeleb1 - 8-shot learning
- pearance modeling of human heads	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- takes explains the current prevalence	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- ferencing systems	-	VoxCeleb1 - 8-shot learning
- posed to synthesize articulated head	-	VoxCeleb1 - 8-shot learning
- chine learning (including deep learning	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- age, the amount of motion	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- vNets) presents the new hope	-	VoxCeleb1 - 8-shot learning
- ever, to succeed, such methods	-	VoxCeleb1 - 8-shot learning
- lions of parameters for each	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- phisticated physical and optical modeling	-	VoxCeleb1 - 8-shot learning
- cessive for most practical telepresence	-	VoxCeleb1 - 8-shot learning
- els with as little effort	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- shot learning) and with limited	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- larly to [16, 20, 37	-	VoxCeleb1 - 8-shot learning
- yond the abilities of warping-based	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- sive pre-training (meta-learning) on a	-	VoxCeleb1 - 8-shot learning
- ing head videos corresponding to	-	VoxCeleb1 - 8-shot learning
- verse appearance. In the course	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- tem simulates few-shot learning tasks	-	VoxCeleb1 - 8-shot learning
- form landmark positions into realistically-looking	-	VoxCeleb1 - 8-shot learning
- alized photographs, given a small	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- ficient realism and personalization fidelity	-	VoxCeleb1 - 8-shot learning
- ing head models, including video	-	VoxCeleb1 - 8-shot learning
- ing of the appearance of	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- tension of the face modeling	-	VoxCeleb1 - 8-shot learning
- ability and higher complexity than	-	VoxCeleb1 - 8-shot learning
- ple, the results of face	-	VoxCeleb1 - 8-shot learning
- fledged talking head system	-	VoxCeleb1 - 8-shot learning
- tecture uses adversarial training [12	-	VoxCeleb1 - 8-shot learning
- ing projection discriminators [32]. Our	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- sifier, from which it can	-	VoxCeleb1 - 8-shot learning
- fiers of unseen classes, given	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- GAN [43], adversarial meta-learning [41	-	VoxCeleb1 - 8-shot learning
- trained networks to generate additional	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- mance, our method deals with	-	VoxCeleb1 - 8-shot learning
- ation models using similar adversarial	-	VoxCeleb1 - 8-shot learning
- marize, we bring the adversarial	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- learning framework. The former is	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- cation domain, the use of	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- plementation details	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- marks) to the embedding vectors	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- quence and with xi(t) its	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- ity of the face landmarks	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
these inputs into an N -	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- age yi(t) for the video	-	VoxCeleb1 - 8-shot learning
- sized video frame x̂i(t). The	-	VoxCeleb1 - 8-shot learning
- imize the similarity between its	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- trix P: ψ̂i = Pêi	-	VoxCeleb1 - 8-shot learning
landmark image into an N -	-	VoxCeleb1 - 8-shot learning
- criminator predicts a single scalar	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- rameters of all three networks	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- ing (K = 8 in	-	VoxCeleb1 - 8-shot learning
- domly draw a training video	-	VoxCeleb1 - 8-shot learning
- ditional K frames s1, s2	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- ding by simply averaging the	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- tion x̂i(t) using the perceptual	-	VoxCeleb1 - 8-shot learning
- responding to VGG19 [30] network	-	VoxCeleb1 - 8-shot learning
- sentially is a perceptual similarity	-	VoxCeleb1 - 8-shot learning
- respond to individual videos. The	-	VoxCeleb1 - 8-shot learning
maps its inputs to anN -	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- criminator. The match term LMCH(φ,W	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- ters θ,W,w0, b of the	-	VoxCeleb1 - 8-shot learning
- courages the increase of the	-	VoxCeleb1 - 8-shot learning
- ample x̂i(t) and the real	-	VoxCeleb1 - 8-shot learning
- nating updates of the embedder	-	VoxCeleb1 - 8-shot learning
- imize the losses LCNT,LADV and	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- sis is conditioned on the	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- timate the embedding for the	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- sponding to new landmark images	-	VoxCeleb1 - 8-shot learning
- erator using the estimated embedding	-	VoxCeleb1 - 8-shot learning
- learned parameters ψ, as well	-	VoxCeleb1 - 8-shot learning
- able identity gap that is	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- sult of the meta-learning stage	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- tors computed by the embedder	-	VoxCeleb1 - 8-shot learning
- tions of the fine-tuning stage	-	VoxCeleb1 - 8-shot learning
- learning variants. Thus, the generator	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- son et. al. [19], but	-	VoxCeleb1 - 8-shot learning
- malization [15] replaced by instance	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- efficients of instance normalization layers	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- ization layers in the downsampling	-	VoxCeleb1 - 8-shot learning
- mark images yi(t	-	VoxCeleb1 - 8-shot learning
- tional part of the discriminator	-	VoxCeleb1 - 8-shot learning
- out normalization layers). The discriminator	-	VoxCeleb1 - 8-shot learning
- pared to the embedder, has	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- serted at 32×32 spatial resolution	-	VoxCeleb1 - 8-shot learning
- tween activations of Conv1,6,11,20,29 VGG19	-	VoxCeleb1 - 8-shot learning
- nally, for LMCH we set	-	VoxCeleb1 - 8-shot learning
- tional layers to 64 and	-	VoxCeleb1 - 8-shot learning
- tor has 38 million parameters	-	VoxCeleb1 - 8-shot learning
- titative and qualitative evaluation: VoxCeleb1	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- fer to the text for	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- son not seen during meta-learning	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- reenactment scenario). For the evaluation	-	VoxCeleb1 - 8-shot learning
- out frames for each of	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- realism and identity preservation of	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- bedding vectors of the state-of-the-art	-	VoxCeleb1 - 8-shot learning
- work [9] for measuring identity	-	VoxCeleb1 - 8-shot learning
- tual similarity and realism of	-	VoxCeleb1 - 8-shot learning
- man respondents. We show people	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- not spot fakes based on	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- Celeb1 dataset). For Pix2pixHD, we	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- shot learning. X2Face, as a	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- tion, which arguably gives X2Face	-	VoxCeleb1 - 8-shot learning
- lines in three different setups	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- dom from the other video	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- perform our method on the	-	VoxCeleb1 - 8-shot learning
- imizes only perceptual metric, without	-	VoxCeleb1 - 8-shot learning
- and few-shot learning on a	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- ter correlates with visual quality	-	VoxCeleb1 - 8-shot learning
- ble 1-Top with the results	-	VoxCeleb1 - 8-shot learning
- alism and personalization degree achieved	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- out fine-tuning (by simply predicting	-	VoxCeleb1 - 8-shot learning
- ant is trained for half	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- sults, where animation is driven	-	VoxCeleb1 - 8-shot learning
- ent video of the same	-	VoxCeleb1 - 8-shot learning
- tary material and in Figure	-	VoxCeleb1 - 8-shot learning
- ble 1-Bottom) and the visual	-	VoxCeleb1 - 8-shot learning
- forms better for low-shot learning	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- ial fine-tuning	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- sons with similar geometry of	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- tographs in the source column	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- realistic virtual talking heads in	-	VoxCeleb1 - 8-shot learning
- ization score in our user	-	VoxCeleb1 - 8-shot learning
- ics representation (in particular, the	-	VoxCeleb1 - 8-shot learning
- son leads to a noticeable	-	VoxCeleb1 - 8-shot learning
- ing a different person and	-	VoxCeleb1 - 8-shot learning
- proach already provides a high-realism	-	VoxCeleb1 - 8-shot learning
- puter Graphics and Applications, 30(4):20–31	-	VoxCeleb1 - 8-shot learning
- sarial networks. In Artificial Neural	-	VoxCeleb1 - 8-shot learning
Networks and Machine Learning - ICANN, pages 594–603, 2018. 2	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- thesis of 3d faces. In	-	VoxCeleb1 - 8-shot learning
- 29, 2017, pages 1021–1030, 2017	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- learning for fast adaptation of	-	VoxCeleb1 - 8-shot learning
- ulation. In European Conference on	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- erative adversarial nets. In Advances	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- wanathan, and R. Garnett, editors	-	VoxCeleb1 - 8-shot learning
- formation Processing Systems 30, pages	-	VoxCeleb1 - 8-shot learning
- time with adaptive instance normalization	-	VoxCeleb1 - 8-shot learning
- ternational Conference on Machine Learning	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- shick, S. Guadarrama, and T	-	VoxCeleb1 - 8-shot learning
- tional architecture for fast feature	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- speech synthesis. In Proc. NIPS	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- SPEECH, 2017. 5	-	VoxCeleb1 - 8-shot learning
- gios, and I. Kokkinos. Deforming	-	VoxCeleb1 - 8-shot learning
- vised disentangling of shape and	-	VoxCeleb1 - 8-shot learning
- ropean Conference on Computer Vision	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- Shlizerman. Synthesizing Obama: learning lip	-	VoxCeleb1 - 8-shot learning
- dio. ACM Transactions on Graphics	-	VoxCeleb1 - 8-shot learning
- tral normalization for generative adversarial	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- erator architecture for generative adversarial	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- actment of RGB videos. In	-	VoxCeleb1 - 8-shot learning
- ference on Computer Vision and	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- tion, 2018. 4, 6	-	VoxCeleb1 - 8-shot learning
- learning. CoRR, abs/1806.03316, 2018. 2	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- ried out on a single	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- surement was averaged over 100	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- ing personalization fidelity and realism	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- duction of a training scheduler	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- vided by the embedder is	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- specific initialization of the discriminator	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- ration, which turned out to	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- tice that the results for	-	VoxCeleb1 - 8-shot learning
- sonalization fidelity. We, therefore, came	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
shot learning of neural talking head models	shot learning	VoxCeleb1 - 8-shot learning
handful of photographs (so-called few- shot learning) and with limited training time	shot learning	VoxCeleb1 - 8-shot learning
shot learning), while adding a few more	shot learning	VoxCeleb1 - 8-shot learning
shot learning ability is obtained through exten	shot learning	VoxCeleb1 - 8-shot learning
shot learning tasks and learns to trans	shot learning	VoxCeleb1 - 8-shot learning
shot learning of generative models) and some	shot learning	VoxCeleb1 - 8-shot learning
shot learning by fine-tuning	shot learning	VoxCeleb1 - 8-shot learning
shot learning settings. Please re- fer to	shot learning	VoxCeleb1 - 8-shot learning
shot learning sets of size T for	shot learning	VoxCeleb1 - 8-shot learning
shot learning, the evaluation is performed on	shot learning	VoxCeleb1 - 8-shot learning
frames T used in few- shot learning	shot learning	VoxCeleb1 - 8-shot learning
shot learning on a video of a	shot learning	VoxCeleb1 - 8-shot learning
shot learning timings. Both are provided in	shot learning	VoxCeleb1 - 8-shot learning
shot learning speed versus the results quality	shot learning	VoxCeleb1 - 8-shot learning
shot learning (e.g. one-shot), while the FT	shot learning	VoxCeleb1 - 8-shot learning
shot learning of new avatars, fine-tuning ultimately	shot learning	VoxCeleb1 - 8-shot learning
shot learning	shot learning	VoxCeleb1 - 8-shot learning
shot learning was done via fine-tuning for	shot learning	VoxCeleb1 - 8-shot learning
shot learning	shot learning	VoxCeleb1 - 8-shot learning
shot learning and inference timings for the	shot learning	VoxCeleb1 - 8-shot learning
shot learning problems, like in our final	shot learning	VoxCeleb1 - 8-shot learning
or pretraining. We used eight shot learning problem formulation. The notation for	shot learning	VoxCeleb1 - 8-shot learning
or pretraining. We used eight shot learning problem formulation. The notation for	shot learning	VoxCeleb1 - 8-shot learning
3D scans [4], or by learning 3DMM parameters directly from RGB	learning	VoxCeleb1 - 8-shot learning
accordingly. This is done by learning a forward mapping fp→v from	learning	VoxCeleb1 - 8-shot learning
improving the generated results. As learning the function fa→v : R	learning	VoxCeleb1 - 8-shot learning
with L1 loss, and a learning rate of 0.001. The learning	learning	VoxCeleb1 - 8-shot learning
phase is started with a learning rate of 0.0001. Testing. The	learning	VoxCeleb1 - 8-shot learning
Abbeel, P.: Infogan: Interpretable representation learning by information maximizing generative adversarial	learning	VoxCeleb1 - 8-shot learning
Denton, E.L., Birodkar, V.: Unsupervised learning of disentangled repre- sentations from	learning	VoxCeleb1 - 8-shot learning
facial animation by joint end-to-end learning of pose and emotion. ACM	learning	VoxCeleb1 - 8-shot learning
King, D.E.: Dlib-ml: A machine learning toolkit. The Journal of Machine	learning	VoxCeleb1 - 8-shot learning
tion of unconstrained faces by learning efficient H-CNN regressors. In: Proc	learning	VoxCeleb1 - 8-shot learning
S.M., Kemelmacher-Shlizerman, I.: Synthesizing Obama: learning lip sync from audio. ACM	learning	VoxCeleb1 - 8-shot learning
X., Liu, X.: Disentangled representation learning gan for pose-invariant face recognition	learning	VoxCeleb1 - 8-shot learning
- phisticated video and image editing	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- pared to state-of-the-art self-supervised/supervised methods	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- ing frame, audio data, or	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- ing frames. These frames are	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- tions. First, we propose a	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- tion, described in Section 6	-	VoxCeleb1 - 8-shot learning
- imation (or puppeteering) given one	-	VoxCeleb1 - 8-shot learning
- erature on supervised/self-supervised approaches; here	-	VoxCeleb1 - 8-shot learning
- marks [5, 12, 21, 30	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- tors of variation (e.g. optical	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- form images of one domain	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- work. This is illustrated in	-	VoxCeleb1 - 8-shot learning
- termine how to map from	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- straints based on the identity	-	VoxCeleb1 - 8-shot learning
- quently, we introduce additional loss	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-5 and Conv7 layers (i.e. layers	-	VoxCeleb1 - 8-shot learning
-7 layers (i.e. layers encoding higher	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- figuration A) [37] trained on	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- tional mapping fv→p is needed	-	VoxCeleb1 - 8-shot learning
- nected layer with bias and	-	VoxCeleb1 - 8-shot learning
- connected linear layer with bias	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- bedding learns to encode some	-	VoxCeleb1 - 8-shot learning
- tion. Given driving audio features	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- size of 16. First, it	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- ing/testing setups. Lower is better	-	VoxCeleb1 - 8-shot learning
- centage improvement over the L1	-	VoxCeleb1 - 8-shot learning
- egy and using additional views	-	VoxCeleb1 - 8-shot learning
- main (in this case a	-	VoxCeleb1 - 8-shot learning
- cleGAN is trained on pairs	-	VoxCeleb1 - 8-shot learning
- ing unrealistic results. Additionally, our	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- tion 4.1), the 25, 993	-	VoxCeleb1 - 8-shot learning
- neutral expressions in the source	-	VoxCeleb1 - 8-shot learning
- processing (X2Face + p.-p.) can	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- mentary material. Whilst some artefacts	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- eration using another face. This	-	VoxCeleb1 - 8-shot learning
- constrained settings (e.g. an unseen	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- tioned on other modalities, the	-	VoxCeleb1 - 8-shot learning
- teresting avenue of research: how	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- eration quality of these methods	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- tions/comments. This work was funded	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-4	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- ment networks. In: Proc. ICCV	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- sentations from video. In: NIPS	-	VoxCeleb1 - 8-shot learning
- tional neural networks. In: Proc	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- actions on Graphics (TOG) (2017	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- lutional neural networks. In: Proc	-	VoxCeleb1 - 8-shot learning
- tional inverse graphics network. In	-	VoxCeleb1 - 8-shot learning
- tion of unconstrained faces by	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- tation, face swapping, and face	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- strained photo collections. In: Proc	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- scale image recognition. In: International	-	VoxCeleb1 - 8-shot learning
- sentations (2015	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- pretable transformations with encoder-decoder networks	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
-	-	VoxCeleb1 - 8-shot learning
- lation using cycle-consistent adversarial networks	-	VoxCeleb1 - 8-shot learning
T) FID↓ SSIM↑ CSIM↑ USER↓ VoxCeleb1  X2Face (1) 45.8 0.68 0.16	VoxCeleb1 	VoxCeleb1 - 32-shot learning
than the former. VoxCeleb1 	VoxCeleb1 	VoxCeleb1 - 32-shot learning
Methods. On the VoxCeleb1 	VoxCeleb1 	VoxCeleb1 - 32-shot learning
Figure 3: Comparison on the VoxCeleb1 	VoxCeleb1 	VoxCeleb1 - 32-shot learning
extended qualitative comparisons on the VoxCeleb1 	VoxCeleb1 	VoxCeleb1 - 32-shot learning
extended qualitative comparison on the VoxCeleb1 	VoxCeleb1 	VoxCeleb1 - 32-shot learning
quan- titative and qualitative evaluation: VoxCeleb1 [26] (256p videos at 1	VoxCeleb1	VoxCeleb1 - 32-shot learning
T) FID↓ SSIM↑ CSIM↑ USER↓ VoxCeleb1	VoxCeleb1	VoxCeleb1 - 32-shot learning
than the former. VoxCeleb1 is used for comparison with	VoxCeleb1	VoxCeleb1 - 32-shot learning
Methods. On the VoxCeleb1 dataset we compare our model	VoxCeleb1	VoxCeleb1 - 32-shot learning
Figure 3: Comparison on the VoxCeleb1 dataset. For each of the	VoxCeleb1	VoxCeleb1 - 32-shot learning
to smaller-scale models trained on VoxCeleb1	VoxCeleb1	VoxCeleb1 - 32-shot learning
extended qualitative comparisons on the VoxCeleb1 dataset. Here, the comparison is	VoxCeleb1	VoxCeleb1 - 32-shot learning
extended qualitative comparison on the VoxCeleb1 dataset. Here, we compare qualitative	VoxCeleb1	VoxCeleb1 - 32-shot learning
learning on a large dataset of	learning	VoxCeleb1 - 32-shot learning
to frame few- and one-shot learning of neural talking head models	learning	VoxCeleb1 - 32-shot learning
fields synthesized using ma- chine learning (including deep learning) [11, 29	learning	VoxCeleb1 - 32-shot learning
of photographs (so-called few- shot learning) and with limited training time	learning	VoxCeleb1 - 32-shot learning
on a single photograph (one-shot learning), while adding a few more	learning	VoxCeleb1 - 32-shot learning
The few-shot learning ability is obtained through exten	learning	VoxCeleb1 - 32-shot learning
learning) on a large corpus of	learning	VoxCeleb1 - 32-shot learning
learning, our sys- tem simulates few-shot	learning	VoxCeleb1 - 32-shot learning
learning tasks and learns to trans	learning	VoxCeleb1 - 32-shot learning
sets up a new adversarial learning problem with high-capacity generator and	learning	VoxCeleb1 - 32-shot learning
learning	learning	VoxCeleb1 - 32-shot learning
and, more recently, with deep learning [22, 25] (to name just	learning	VoxCeleb1 - 32-shot learning
learning stage uses the adaptive instance	learning	VoxCeleb1 - 32-shot learning
learning to obtain the initial state	learning	VoxCeleb1 - 32-shot learning
learning	learning	VoxCeleb1 - 32-shot learning
learning [41] use adversarially- trained networks	learning	VoxCeleb1 - 32-shot learning
learning stage. While these methods are	learning	VoxCeleb1 - 32-shot learning
adversarial fine-tuning into the meta- learning framework. The former is applied	learning	VoxCeleb1 - 32-shot learning
learning stage	learning	VoxCeleb1 - 32-shot learning
4, 18]. Their setting (few-shot learning of generative models) and some	learning	VoxCeleb1 - 32-shot learning
domain, the use of adversarial learning, its specific adaptation to the	learning	VoxCeleb1 - 32-shot learning
learning process and numerous im- plementation	learning	VoxCeleb1 - 32-shot learning
learning architecture involves the embedder network	learning	VoxCeleb1 - 32-shot learning
learning, we pass sets of frames	learning	VoxCeleb1 - 32-shot learning
learning stage of our approach assumes	learning	VoxCeleb1 - 32-shot learning
its t-th frame. During the learning process, as well as during	learning	VoxCeleb1 - 32-shot learning
learning stage of our approach, the	learning	VoxCeleb1 - 32-shot learning
learning stage. In general, during meta-learning	learning	VoxCeleb1 - 32-shot learning
learning, only ψ are trained directly	learning	VoxCeleb1 - 32-shot learning
learning stage	learning	VoxCeleb1 - 32-shot learning
learning stage of our approach, the	learning	VoxCeleb1 - 32-shot learning
3.3. Few-shot learning by fine-tuning	learning	VoxCeleb1 - 32-shot learning
learning has converged, our system can	learning	VoxCeleb1 - 32-shot learning
learning stage. As before, the synthe	learning	VoxCeleb1 - 32-shot learning
learning stage	learning	VoxCeleb1 - 32-shot learning
learning stage. A straightforward way to	learning	VoxCeleb1 - 32-shot learning
learning with a single video sequence	learning	VoxCeleb1 - 32-shot learning
learning stage to initialize ψ′, i.e	learning	VoxCeleb1 - 32-shot learning
learning stage. The initialization of w	learning	VoxCeleb1 - 32-shot learning
learning stage	learning	VoxCeleb1 - 32-shot learning
learning stage. For the intiailization, we	learning	VoxCeleb1 - 32-shot learning
learning dataset). However, the match term	learning	VoxCeleb1 - 32-shot learning
learning process ensures the similarity between	learning	VoxCeleb1 - 32-shot learning
Once the new learning problem is set up, the	learning	VoxCeleb1 - 32-shot learning
follow directly from the meta- learning variants. Thus, the generator parameters	learning	VoxCeleb1 - 32-shot learning
learning stage is also crucial. As	learning	VoxCeleb1 - 32-shot learning
Adam [21]. We set the learning rate of the embedder and	learning	VoxCeleb1 - 32-shot learning
different datasets with multiple few-shot learning settings. Please re- fer to	learning	VoxCeleb1 - 32-shot learning
fine-tune all models on few-shot learning sets of size T for	learning	VoxCeleb1 - 32-shot learning
learning (or pretraining) stage. After the	learning	VoxCeleb1 - 32-shot learning
few-shot learning, the evaluation is performed on	learning	VoxCeleb1 - 32-shot learning
T used in few- shot learning	learning	VoxCeleb1 - 32-shot learning
we perform one- and few-shot learning on a video of a	learning	VoxCeleb1 - 32-shot learning
learning or pretraining. We set the	learning	VoxCeleb1 - 32-shot learning
the comparison of the few-shot learning timings. Both are provided in	learning	VoxCeleb1 - 32-shot learning
allow to trade off few-shot learning speed versus the results quality	learning	VoxCeleb1 - 32-shot learning
per- forms better for low-shot learning (e.g. one-shot), while the FT	learning	VoxCeleb1 - 32-shot learning
variant allows fast (real-time) few-shot learning of new avatars, fine-tuning ultimately	learning	VoxCeleb1 - 32-shot learning
learning of ad	learning	VoxCeleb1 - 32-shot learning
and S. Levine. Model-agnostic meta- learning for fast adaptation of deep	learning	VoxCeleb1 - 32-shot learning
Y. Wu, et al. Transfer learning from speaker verification to multispeaker	learning	VoxCeleb1 - 32-shot learning
I. Kemelmacher- Shlizerman. Synthesizing Obama: learning lip sync from au- dio	learning	VoxCeleb1 - 32-shot learning
and Y. Wang. Adversarial meta- learning	learning	VoxCeleb1 - 32-shot learning
An adversarial approach to few-shot learning	learning	VoxCeleb1 - 32-shot learning
Pix2pixHD and our method, few-shot learning was done via fine-tuning for	learning	VoxCeleb1 - 32-shot learning
Method (T) Time, s Few-shot learning	learning	VoxCeleb1 - 32-shot learning
2: Quantitative comparison of few-shot learning and inference timings for the	learning	VoxCeleb1 - 32-shot learning
learning	learning	VoxCeleb1 - 32-shot learning
multiple training frames in few-shot learning problems, like in our final	learning	VoxCeleb1 - 32-shot learning
learning configu- ration, which turned out	learning	VoxCeleb1 - 32-shot learning
learning, we randomly initialize the person-specific	learning	VoxCeleb1 - 32-shot learning
learning objective and initialize the embedding	learning	VoxCeleb1 - 32-shot learning
learning or pretraining. We used eight	learning	VoxCeleb1 - 32-shot learning
shot learning problem formulation. The notation for	learning	VoxCeleb1 - 32-shot learning
learning or pretraining. We used eight	learning	VoxCeleb1 - 32-shot learning
shot learning problem formulation. The notation for	learning	VoxCeleb1 - 32-shot learning
 32	 32	VoxCeleb1 - 32-shot learning
 32 hold- out frames for each	 32	VoxCeleb1 - 32-shot learning
 32 frames in the fine-tuning set	 32	VoxCeleb1 - 32-shot learning
 32 hold-out frames for each of	 32	VoxCeleb1 - 32-shot learning
8  32	 32	VoxCeleb1 - 32-shot learning
8  32	 32	VoxCeleb1 - 32-shot learning
 32 images achieves perfect realism and	 32	VoxCeleb1 - 32-shot learning
 32 training images. The number of	 32	VoxCeleb1 - 32-shot learning
8  32	 32	VoxCeleb1 - 32-shot learning
8  32	 32	VoxCeleb1 - 32-shot learning
8  32	 32	VoxCeleb1 - 32-shot learning
8  32	 32	VoxCeleb1 - 32-shot learning
8  32	 32	VoxCeleb1 - 32-shot learning
8  32	 32	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- tional neural networks to generate	-	VoxCeleb1 - 32-shot learning
- ate a personalized talking head	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- ter that is able to	-	VoxCeleb1 - 32-shot learning
- and one-shot learning of neural	-	VoxCeleb1 - 32-shot learning
- sarial training problems with high	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- alized photorealistic talking head models	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- sions and mimics of a	-	VoxCeleb1 - 32-shot learning
- ically, we consider the problem	-	VoxCeleb1 - 32-shot learning
- alistic personalized head images given	-	VoxCeleb1 - 32-shot learning
- marks, which drive the animation	-	VoxCeleb1 - 32-shot learning
- conferencing and multi-player games, as	-	VoxCeleb1 - 32-shot learning
- fects industry. Synthesizing realistic talking	-	VoxCeleb1 - 32-shot learning
- ity. This complexity stems not	-	VoxCeleb1 - 32-shot learning
- man visual system towards even	-	VoxCeleb1 - 32-shot learning
- pearance modeling of human heads	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- takes explains the current prevalence	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- ferencing systems	-	VoxCeleb1 - 32-shot learning
- posed to synthesize articulated head	-	VoxCeleb1 - 32-shot learning
- chine learning (including deep learning	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- age, the amount of motion	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- vNets) presents the new hope	-	VoxCeleb1 - 32-shot learning
- ever, to succeed, such methods	-	VoxCeleb1 - 32-shot learning
- lions of parameters for each	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- phisticated physical and optical modeling	-	VoxCeleb1 - 32-shot learning
- cessive for most practical telepresence	-	VoxCeleb1 - 32-shot learning
- els with as little effort	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- shot learning) and with limited	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- larly to [16, 20, 37	-	VoxCeleb1 - 32-shot learning
- yond the abilities of warping-based	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- sive pre-training (meta-learning) on a	-	VoxCeleb1 - 32-shot learning
- ing head videos corresponding to	-	VoxCeleb1 - 32-shot learning
- verse appearance. In the course	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- tem simulates few-shot learning tasks	-	VoxCeleb1 - 32-shot learning
- form landmark positions into realistically-looking	-	VoxCeleb1 - 32-shot learning
- alized photographs, given a small	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- ficient realism and personalization fidelity	-	VoxCeleb1 - 32-shot learning
- ing head models, including video	-	VoxCeleb1 - 32-shot learning
- ing of the appearance of	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- tension of the face modeling	-	VoxCeleb1 - 32-shot learning
- ability and higher complexity than	-	VoxCeleb1 - 32-shot learning
- ple, the results of face	-	VoxCeleb1 - 32-shot learning
- fledged talking head system	-	VoxCeleb1 - 32-shot learning
- tecture uses adversarial training [12	-	VoxCeleb1 - 32-shot learning
- ing projection discriminators [32]. Our	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- sifier, from which it can	-	VoxCeleb1 - 32-shot learning
- fiers of unseen classes, given	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- GAN [43], adversarial meta-learning [41	-	VoxCeleb1 - 32-shot learning
- trained networks to generate additional	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- mance, our method deals with	-	VoxCeleb1 - 32-shot learning
- ation models using similar adversarial	-	VoxCeleb1 - 32-shot learning
- marize, we bring the adversarial	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- learning framework. The former is	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- cation domain, the use of	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- plementation details	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- marks) to the embedding vectors	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- quence and with xi(t) its	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- ity of the face landmarks	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
these inputs into an N -	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- age yi(t) for the video	-	VoxCeleb1 - 32-shot learning
- sized video frame x̂i(t). The	-	VoxCeleb1 - 32-shot learning
- imize the similarity between its	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- trix P: ψ̂i = Pêi	-	VoxCeleb1 - 32-shot learning
landmark image into an N -	-	VoxCeleb1 - 32-shot learning
- criminator predicts a single scalar	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- rameters of all three networks	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- ing (K = 8 in	-	VoxCeleb1 - 32-shot learning
- domly draw a training video	-	VoxCeleb1 - 32-shot learning
- ditional K frames s1, s2	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- ding by simply averaging the	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- tion x̂i(t) using the perceptual	-	VoxCeleb1 - 32-shot learning
- responding to VGG19 [30] network	-	VoxCeleb1 - 32-shot learning
- sentially is a perceptual similarity	-	VoxCeleb1 - 32-shot learning
- respond to individual videos. The	-	VoxCeleb1 - 32-shot learning
maps its inputs to anN -	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- criminator. The match term LMCH(φ,W	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- ters θ,W,w0, b of the	-	VoxCeleb1 - 32-shot learning
- courages the increase of the	-	VoxCeleb1 - 32-shot learning
- ample x̂i(t) and the real	-	VoxCeleb1 - 32-shot learning
- nating updates of the embedder	-	VoxCeleb1 - 32-shot learning
- imize the losses LCNT,LADV and	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- sis is conditioned on the	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- timate the embedding for the	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- sponding to new landmark images	-	VoxCeleb1 - 32-shot learning
- erator using the estimated embedding	-	VoxCeleb1 - 32-shot learning
- learned parameters ψ, as well	-	VoxCeleb1 - 32-shot learning
- able identity gap that is	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- sult of the meta-learning stage	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- tors computed by the embedder	-	VoxCeleb1 - 32-shot learning
- tions of the fine-tuning stage	-	VoxCeleb1 - 32-shot learning
- learning variants. Thus, the generator	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- son et. al. [19], but	-	VoxCeleb1 - 32-shot learning
- malization [15] replaced by instance	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- efficients of instance normalization layers	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- ization layers in the downsampling	-	VoxCeleb1 - 32-shot learning
- mark images yi(t	-	VoxCeleb1 - 32-shot learning
- tional part of the discriminator	-	VoxCeleb1 - 32-shot learning
- out normalization layers). The discriminator	-	VoxCeleb1 - 32-shot learning
- pared to the embedder, has	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- serted at 32×32 spatial resolution	-	VoxCeleb1 - 32-shot learning
- tween activations of Conv1,6,11,20,29 VGG19	-	VoxCeleb1 - 32-shot learning
- nally, for LMCH we set	-	VoxCeleb1 - 32-shot learning
- tional layers to 64 and	-	VoxCeleb1 - 32-shot learning
- tor has 38 million parameters	-	VoxCeleb1 - 32-shot learning
- titative and qualitative evaluation: VoxCeleb1	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- fer to the text for	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- son not seen during meta-learning	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- reenactment scenario). For the evaluation	-	VoxCeleb1 - 32-shot learning
- out frames for each of	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- realism and identity preservation of	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- bedding vectors of the state-of-the-art	-	VoxCeleb1 - 32-shot learning
- work [9] for measuring identity	-	VoxCeleb1 - 32-shot learning
- tual similarity and realism of	-	VoxCeleb1 - 32-shot learning
- man respondents. We show people	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- not spot fakes based on	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- Celeb1 dataset). For Pix2pixHD, we	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- shot learning. X2Face, as a	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- tion, which arguably gives X2Face	-	VoxCeleb1 - 32-shot learning
- lines in three different setups	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- dom from the other video	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- perform our method on the	-	VoxCeleb1 - 32-shot learning
- imizes only perceptual metric, without	-	VoxCeleb1 - 32-shot learning
- and few-shot learning on a	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- ter correlates with visual quality	-	VoxCeleb1 - 32-shot learning
- ble 1-Top with the results	-	VoxCeleb1 - 32-shot learning
- alism and personalization degree achieved	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- out fine-tuning (by simply predicting	-	VoxCeleb1 - 32-shot learning
- ant is trained for half	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- sults, where animation is driven	-	VoxCeleb1 - 32-shot learning
- ent video of the same	-	VoxCeleb1 - 32-shot learning
- tary material and in Figure	-	VoxCeleb1 - 32-shot learning
- ble 1-Bottom) and the visual	-	VoxCeleb1 - 32-shot learning
- forms better for low-shot learning	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- ial fine-tuning	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- sons with similar geometry of	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- tographs in the source column	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- realistic virtual talking heads in	-	VoxCeleb1 - 32-shot learning
- ization score in our user	-	VoxCeleb1 - 32-shot learning
- ics representation (in particular, the	-	VoxCeleb1 - 32-shot learning
- son leads to a noticeable	-	VoxCeleb1 - 32-shot learning
- ing a different person and	-	VoxCeleb1 - 32-shot learning
- proach already provides a high-realism	-	VoxCeleb1 - 32-shot learning
- puter Graphics and Applications, 30(4):20–31	-	VoxCeleb1 - 32-shot learning
- sarial networks. In Artificial Neural	-	VoxCeleb1 - 32-shot learning
Networks and Machine Learning - ICANN, pages 594–603, 2018. 2	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- thesis of 3d faces. In	-	VoxCeleb1 - 32-shot learning
- 29, 2017, pages 1021–1030, 2017	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- learning for fast adaptation of	-	VoxCeleb1 - 32-shot learning
- ulation. In European Conference on	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- erative adversarial nets. In Advances	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- wanathan, and R. Garnett, editors	-	VoxCeleb1 - 32-shot learning
- formation Processing Systems 30, pages	-	VoxCeleb1 - 32-shot learning
- time with adaptive instance normalization	-	VoxCeleb1 - 32-shot learning
- ternational Conference on Machine Learning	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- shick, S. Guadarrama, and T	-	VoxCeleb1 - 32-shot learning
- tional architecture for fast feature	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- speech synthesis. In Proc. NIPS	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- SPEECH, 2017. 5	-	VoxCeleb1 - 32-shot learning
- gios, and I. Kokkinos. Deforming	-	VoxCeleb1 - 32-shot learning
- vised disentangling of shape and	-	VoxCeleb1 - 32-shot learning
- ropean Conference on Computer Vision	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- Shlizerman. Synthesizing Obama: learning lip	-	VoxCeleb1 - 32-shot learning
- dio. ACM Transactions on Graphics	-	VoxCeleb1 - 32-shot learning
- tral normalization for generative adversarial	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- erator architecture for generative adversarial	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- actment of RGB videos. In	-	VoxCeleb1 - 32-shot learning
- ference on Computer Vision and	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- tion, 2018. 4, 6	-	VoxCeleb1 - 32-shot learning
- learning. CoRR, abs/1806.03316, 2018. 2	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- ried out on a single	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- surement was averaged over 100	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- ing personalization fidelity and realism	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- duction of a training scheduler	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- vided by the embedder is	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- specific initialization of the discriminator	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- ration, which turned out to	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- tice that the results for	-	VoxCeleb1 - 32-shot learning
- sonalization fidelity. We, therefore, came	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
shot learning of neural talking head models	shot learning	VoxCeleb1 - 32-shot learning
handful of photographs (so-called few- shot learning) and with limited training time	shot learning	VoxCeleb1 - 32-shot learning
shot learning), while adding a few more	shot learning	VoxCeleb1 - 32-shot learning
shot learning ability is obtained through exten	shot learning	VoxCeleb1 - 32-shot learning
shot learning tasks and learns to trans	shot learning	VoxCeleb1 - 32-shot learning
shot learning of generative models) and some	shot learning	VoxCeleb1 - 32-shot learning
shot learning by fine-tuning	shot learning	VoxCeleb1 - 32-shot learning
shot learning settings. Please re- fer to	shot learning	VoxCeleb1 - 32-shot learning
shot learning sets of size T for	shot learning	VoxCeleb1 - 32-shot learning
shot learning, the evaluation is performed on	shot learning	VoxCeleb1 - 32-shot learning
frames T used in few- shot learning	shot learning	VoxCeleb1 - 32-shot learning
shot learning on a video of a	shot learning	VoxCeleb1 - 32-shot learning
shot learning timings. Both are provided in	shot learning	VoxCeleb1 - 32-shot learning
shot learning speed versus the results quality	shot learning	VoxCeleb1 - 32-shot learning
shot learning (e.g. one-shot), while the FT	shot learning	VoxCeleb1 - 32-shot learning
shot learning of new avatars, fine-tuning ultimately	shot learning	VoxCeleb1 - 32-shot learning
shot learning	shot learning	VoxCeleb1 - 32-shot learning
shot learning was done via fine-tuning for	shot learning	VoxCeleb1 - 32-shot learning
shot learning	shot learning	VoxCeleb1 - 32-shot learning
shot learning and inference timings for the	shot learning	VoxCeleb1 - 32-shot learning
shot learning problems, like in our final	shot learning	VoxCeleb1 - 32-shot learning
or pretraining. We used eight shot learning problem formulation. The notation for	shot learning	VoxCeleb1 - 32-shot learning
or pretraining. We used eight shot learning problem formulation. The notation for	shot learning	VoxCeleb1 - 32-shot learning
3D scans [4], or by learning 3DMM parameters directly from RGB	learning	VoxCeleb1 - 32-shot learning
accordingly. This is done by learning a forward mapping fp→v from	learning	VoxCeleb1 - 32-shot learning
improving the generated results. As learning the function fa→v : R	learning	VoxCeleb1 - 32-shot learning
with L1 loss, and a learning rate of 0.001. The learning	learning	VoxCeleb1 - 32-shot learning
phase is started with a learning rate of 0.0001. Testing. The	learning	VoxCeleb1 - 32-shot learning
Abbeel, P.: Infogan: Interpretable representation learning by information maximizing generative adversarial	learning	VoxCeleb1 - 32-shot learning
Denton, E.L., Birodkar, V.: Unsupervised learning of disentangled repre- sentations from	learning	VoxCeleb1 - 32-shot learning
facial animation by joint end-to-end learning of pose and emotion. ACM	learning	VoxCeleb1 - 32-shot learning
King, D.E.: Dlib-ml: A machine learning toolkit. The Journal of Machine	learning	VoxCeleb1 - 32-shot learning
tion of unconstrained faces by learning efficient H-CNN regressors. In: Proc	learning	VoxCeleb1 - 32-shot learning
S.M., Kemelmacher-Shlizerman, I.: Synthesizing Obama: learning lip sync from audio. ACM	learning	VoxCeleb1 - 32-shot learning
X., Liu, X.: Disentangled representation learning gan for pose-invariant face recognition	learning	VoxCeleb1 - 32-shot learning
- phisticated video and image editing	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- pared to state-of-the-art self-supervised/supervised methods	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- ing frame, audio data, or	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- ing frames. These frames are	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- tions. First, we propose a	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- tion, described in Section 6	-	VoxCeleb1 - 32-shot learning
- imation (or puppeteering) given one	-	VoxCeleb1 - 32-shot learning
- erature on supervised/self-supervised approaches; here	-	VoxCeleb1 - 32-shot learning
- marks [5, 12, 21, 30	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- tors of variation (e.g. optical	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- form images of one domain	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- work. This is illustrated in	-	VoxCeleb1 - 32-shot learning
- termine how to map from	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- straints based on the identity	-	VoxCeleb1 - 32-shot learning
- quently, we introduce additional loss	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-5 and Conv7 layers (i.e. layers	-	VoxCeleb1 - 32-shot learning
-7 layers (i.e. layers encoding higher	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- figuration A) [37] trained on	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- tional mapping fv→p is needed	-	VoxCeleb1 - 32-shot learning
- nected layer with bias and	-	VoxCeleb1 - 32-shot learning
- connected linear layer with bias	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- bedding learns to encode some	-	VoxCeleb1 - 32-shot learning
- tion. Given driving audio features	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- size of 16. First, it	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- ing/testing setups. Lower is better	-	VoxCeleb1 - 32-shot learning
- centage improvement over the L1	-	VoxCeleb1 - 32-shot learning
- egy and using additional views	-	VoxCeleb1 - 32-shot learning
- main (in this case a	-	VoxCeleb1 - 32-shot learning
- cleGAN is trained on pairs	-	VoxCeleb1 - 32-shot learning
- ing unrealistic results. Additionally, our	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- tion 4.1), the 25, 993	-	VoxCeleb1 - 32-shot learning
- neutral expressions in the source	-	VoxCeleb1 - 32-shot learning
- processing (X2Face + p.-p.) can	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- mentary material. Whilst some artefacts	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- eration using another face. This	-	VoxCeleb1 - 32-shot learning
- constrained settings (e.g. an unseen	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- tioned on other modalities, the	-	VoxCeleb1 - 32-shot learning
- teresting avenue of research: how	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- eration quality of these methods	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- tions/comments. This work was funded	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-4	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- ment networks. In: Proc. ICCV	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- sentations from video. In: NIPS	-	VoxCeleb1 - 32-shot learning
- tional neural networks. In: Proc	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- actions on Graphics (TOG) (2017	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- lutional neural networks. In: Proc	-	VoxCeleb1 - 32-shot learning
- tional inverse graphics network. In	-	VoxCeleb1 - 32-shot learning
- tion of unconstrained faces by	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- tation, face swapping, and face	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- strained photo collections. In: Proc	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- scale image recognition. In: International	-	VoxCeleb1 - 32-shot learning
- sentations (2015	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- pretable transformations with encoder-decoder networks	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
-	-	VoxCeleb1 - 32-shot learning
- lation using cycle-consistent adversarial networks	-	VoxCeleb1 - 32-shot learning
also assessed Jpred4 on the 2019_test set (see Table	2019_test	2019_test set
predictors and Jpred4 assessed on 2019_test set of 618 proteins	2019_test	2019_test set
8-states Training Set 2017_test Set 2019_test Set	2019_test	2019_test set
composition of Training, 2017_test and 2019_test	2019_test	2019_test set
second smaller independent test set (2019_test) based	2019_test	2019_test set
an additional independent test set (2019_test) to fairly compare Porter 5	2019_test	2019_test set
proteins, comprising 91,375 amino acids (2019_test	2019_test	2019_test set
predictors and Jpred4 assessed on 2019_test set of 618 proteins	2019_test	2019_test set
composition of Training, 2017_test and 2019_test	2019_test	2019_test set
also assessed Jpred4 on the 2019_test set (see Table	2019_test set	2019_test set
predictors and Jpred4 assessed on 2019_test set of 618 proteins	2019_test set	2019_test set
predictors and Jpred4 assessed on 2019_test set of 618 proteins	2019_test set	2019_test set
a large set of protein structures. Porter 5	set	2019_test set
classes on a large independent set	set	2019_test set
ical limits of prediction – set at 88–90% accuracy per AA	set	2019_test set
validation experiments on the training set	set	2019_test set
adopting a considerably larger training set but without evolutionary information, we	set	2019_test set
ensemble trained on a smaller set34	set	2019_test set
NN architectures on the validation set	set	2019_test set
proteins in our case), the set of hyperparameters selected for the	set	2019_test set
preliminary testing by splitting our set into 1/5 for testing and	set	2019_test set
above) on the full training set rather than on individual training	set	2019_test set
models on a completely independent set (see “2017_test” in Methods:Datasets) con	set	2019_test set
Full set (Porter 5) 83.42% 83.39% 83.49	set	2019_test set
3. Assessment on the 2017_test set of three-state ensembles trained on	set	2019_test set
full set	set	2019_test set
on 4/5 of the training set on PSI-BLAST, HHblits and concatenated	set	2019_test set
trained on the full training set achieves 73.02% Q8 accuracy on	set	2019_test set
set described in Methods:Dataset (see also	set	2019_test set
multiple predictors on independent test set	set	2019_test set
on the 2017_test set we created, containing 3,154 proteins	set	2019_test set
5); the full set of 3,154 proteins (Table	set	2019_test set
our tests on the 2017_test set with 3-class accu- racy of	set	2019_test set
the smaller version of the set and 84.2% on the larger	set	2019_test set
AA on the full test set	set	2019_test set
Performances on the smaller 2017_test set for which Spider3 generates predictions	set	2019_test set
both versions of the 2017_test set	set	2019_test set
of the set, respectively. Porter 5 is also	set	2019_test set
5 performances on the CASP1328 set, and on 6 months of	set	2019_test set
we obtained on the 2017_test set	set	2019_test set
only section of the 2017_test set, which is roughly 90% of	set	2019_test set
is selected for the training set, while a further 150 proteins	set	2019_test set
not included in the training set are used as a blind	set	2019_test set
test set	set	2019_test set
trained on the 1,348 JPred4 set	set	2019_test set
set	set	2019_test set
same sets. While the testing set is small (150 proteins), these	set	2019_test set
Q3 when trained on a set which is	set	2019_test set
smaller than its original training set	set	2019_test set
assessed Jpred4 on the 2019_test set (see Table	set	2019_test set
8 and description of the set in the following section	set	2019_test set
predictors. In a separate test set we assessed some very recent	set	2019_test set
more recent than our 2017_test set, i.e. MUFOLD-SS43, NetSurfP-2.048 and SPOT-1D44	set	2019_test set
generated a second independent test set (also see Methods:Datasets) starting from	set	2019_test set
not access the MUFOLD-SS training set	set	2019_test set
861 original proteins in the set, Table	set	2019_test set
quality of our large training set (see Table	set	2019_test set
the experiments on a training set twice as large as the	set	2019_test set
of a larger, well-distributed training set	set	2019_test set
and Jpred4 assessed on 2019_test set of 618 proteins	set	2019_test set
on a first independent test set (2017_test), along with some of	set	2019_test set
reduction of our final test set against their training sets35	set	2019_test set
overlapped with our original test set, we generated a second smaller	set	2019_test set
independent test set (2019_test) based	set	2019_test set
Specifically, we built our training set from the PDB released on	set	2019_test set
also built an independent test set (2017_test) from the PDB released	set	2019_test set
14, 2017. We redundancy-reduced this set at a 25% identity threshold	set	2019_test set
training set	set	2019_test set
internally redundancy reduced the resulting set at a 25% identity threshold	set	2019_test set
from both sets. The training set contains	set	2019_test set
In different tests the training set is used as a whole	set	2019_test set
for hyperparameter optimization35. The 2017_test set is only used	set	2019_test set
curated an additional independent test set (2019_test) to fairly compare Porter	set	2019_test set
overlapping with our 2017_test set	set	2019_test set
2019. We then redundancy-reduced this set	set	2019_test set
SPOT-1D, NetSurfP-2.0 and our training set at 25% identity threshold. Finally	set	2019_test set
the internal redundancy of this set at a 25% sequence identity	set	2019_test set
The simple idea is to set to 1 the position	set	2019_test set
The number of hyperparameters to set in a BRNN, and the	set	2019_test set
stochastic gradient descent (SGD)65. We set momentum to 0.9 and divided	set	2019_test set
cross entropy error on training set had not decreased for 100	set	2019_test set
epochs. The train- ing set is shuffled at the end	set	2019_test set
size of a mini-batch is set to ~10 proteins, that is	set	2019_test set
multiple predictors on independent test set	set	2019_test set
NN architectures on the validation set	set	2019_test set
3 Assessment on the 2017_test set of three-state ensembles trained on	set	2019_test set
either five-fold cross-validation or full set	set	2019_test set
AA on the full test set	set	2019_test set
Performances on the smaller 2017_test set for which Spider3 generates predictions	set	2019_test set
and Jpred4 assessed on 2019_test set of 618 proteins	set	2019_test set
Table 3. Assessment on the 2017_test set of three-state ensembles trained on	2017_test set	2017_test set
on the 2017_test set we created, containing 3,154 proteins	2017_test set	2017_test set
in our tests on the 2017_test set with 3-class accu- racy of	2017_test set	2017_test set
5. Performances on the smaller 2017_test set for which Spider3 generates predictions	2017_test set	2017_test set
on both versions of the 2017_test set	2017_test set	2017_test set
results we obtained on the 2017_test set	2017_test set	2017_test set
X-ray only section of the 2017_test set, which is roughly 90% of	2017_test set	2017_test set
sets more recent than our 2017_test set, i.e. MUFOLD-SS43, NetSurfP-2.048 and SPOT-1D44	2017_test set	2017_test set
cross-validation for hyperparameter optimization35. The 2017_test set is only used	2017_test set	2017_test set
overlapping with our 2017_test set	2017_test set	2017_test set
Table 3 Assessment on the 2017_test set of three-state ensembles trained on	2017_test set	2017_test set
5 Performances on the smaller 2017_test set for which Spider3 generates predictions	2017_test set	2017_test set
a large set of protein structures. Porter 5	set	2017_test set
classes on a large independent set	set	2017_test set
ical limits of prediction – set at 88–90% accuracy per AA	set	2017_test set
validation experiments on the training set	set	2017_test set
adopting a considerably larger training set but without evolutionary information, we	set	2017_test set
ensemble trained on a smaller set34	set	2017_test set
NN architectures on the validation set	set	2017_test set
proteins in our case), the set of hyperparameters selected for the	set	2017_test set
preliminary testing by splitting our set into 1/5 for testing and	set	2017_test set
above) on the full training set rather than on individual training	set	2017_test set
models on a completely independent set (see “2017_test” in Methods:Datasets) con	set	2017_test set
Full set (Porter 5) 83.42% 83.39% 83.49	set	2017_test set
3. Assessment on the 2017_test set of three-state ensembles trained on	set	2017_test set
full set	set	2017_test set
on 4/5 of the training set on PSI-BLAST, HHblits and concatenated	set	2017_test set
trained on the full training set achieves 73.02% Q8 accuracy on	set	2017_test set
set described in Methods:Dataset (see also	set	2017_test set
multiple predictors on independent test set	set	2017_test set
on the 2017_test set we created, containing 3,154 proteins	set	2017_test set
5); the full set of 3,154 proteins (Table	set	2017_test set
our tests on the 2017_test set with 3-class accu- racy of	set	2017_test set
the smaller version of the set and 84.2% on the larger	set	2017_test set
AA on the full test set	set	2017_test set
Performances on the smaller 2017_test set for which Spider3 generates predictions	set	2017_test set
both versions of the 2017_test set	set	2017_test set
of the set, respectively. Porter 5 is also	set	2017_test set
5 performances on the CASP1328 set, and on 6 months of	set	2017_test set
we obtained on the 2017_test set	set	2017_test set
only section of the 2017_test set, which is roughly 90% of	set	2017_test set
is selected for the training set, while a further 150 proteins	set	2017_test set
not included in the training set are used as a blind	set	2017_test set
test set	set	2017_test set
trained on the 1,348 JPred4 set	set	2017_test set
set	set	2017_test set
same sets. While the testing set is small (150 proteins), these	set	2017_test set
Q3 when trained on a set which is	set	2017_test set
smaller than its original training set	set	2017_test set
assessed Jpred4 on the 2019_test set (see Table	set	2017_test set
8 and description of the set in the following section	set	2017_test set
predictors. In a separate test set we assessed some very recent	set	2017_test set
more recent than our 2017_test set, i.e. MUFOLD-SS43, NetSurfP-2.048 and SPOT-1D44	set	2017_test set
generated a second independent test set (also see Methods:Datasets) starting from	set	2017_test set
not access the MUFOLD-SS training set	set	2017_test set
861 original proteins in the set, Table	set	2017_test set
quality of our large training set (see Table	set	2017_test set
the experiments on a training set twice as large as the	set	2017_test set
of a larger, well-distributed training set	set	2017_test set
and Jpred4 assessed on 2019_test set of 618 proteins	set	2017_test set
on a first independent test set (2017_test), along with some of	set	2017_test set
reduction of our final test set against their training sets35	set	2017_test set
overlapped with our original test set, we generated a second smaller	set	2017_test set
independent test set (2019_test) based	set	2017_test set
Specifically, we built our training set from the PDB released on	set	2017_test set
also built an independent test set (2017_test) from the PDB released	set	2017_test set
14, 2017. We redundancy-reduced this set at a 25% identity threshold	set	2017_test set
training set	set	2017_test set
internally redundancy reduced the resulting set at a 25% identity threshold	set	2017_test set
from both sets. The training set contains	set	2017_test set
In different tests the training set is used as a whole	set	2017_test set
for hyperparameter optimization35. The 2017_test set is only used	set	2017_test set
curated an additional independent test set (2019_test) to fairly compare Porter	set	2017_test set
overlapping with our 2017_test set	set	2017_test set
2019. We then redundancy-reduced this set	set	2017_test set
SPOT-1D, NetSurfP-2.0 and our training set at 25% identity threshold. Finally	set	2017_test set
the internal redundancy of this set at a 25% sequence identity	set	2017_test set
The simple idea is to set to 1 the position	set	2017_test set
The number of hyperparameters to set in a BRNN, and the	set	2017_test set
stochastic gradient descent (SGD)65. We set momentum to 0.9 and divided	set	2017_test set
cross entropy error on training set had not decreased for 100	set	2017_test set
epochs. The train- ing set is shuffled at the end	set	2017_test set
size of a mini-batch is set to ~10 proteins, that is	set	2017_test set
multiple predictors on independent test set	set	2017_test set
NN architectures on the validation set	set	2017_test set
3 Assessment on the 2017_test set of three-state ensembles trained on	set	2017_test set
either five-fold cross-validation or full set	set	2017_test set
AA on the full test set	set	2017_test set
Performances on the smaller 2017_test set for which Spider3 generates predictions	set	2017_test set
and Jpred4 assessed on 2019_test set of 618 proteins	set	2017_test set
2017_test	2017_test	2017_test set
Table 3. Assessment on the 2017_test set of three-state ensembles trained	2017_test	2017_test set
73.02% Q8 accuracy on the 2017_test	2017_test	2017_test set
on the 2017_test set we created, containing 3,154	2017_test	2017_test set
in our tests on the 2017_test set with 3-class accu- racy	2017_test	2017_test set
5. Performances on the smaller 2017_test set for which Spider3 generates	2017_test	2017_test set
on both versions of the 2017_test set. Porter 5 is consistently	2017_test	2017_test set
results we obtained on the 2017_test set	2017_test	2017_test set
X-ray only section of the 2017_test set, which is roughly 90	2017_test	2017_test set
sets more recent than our 2017_test set, i.e. MUFOLD-SS43, NetSurfP-2.048 and	2017_test	2017_test set
3-states 8-states Training Set 2017_test Set 2019_test Set	2017_test	2017_test set
of AA composition of Training, 2017_test and 2019_test	2017_test	2017_test set
a first independent test set (2017_test), along with some of the	2017_test	2017_test set
built an independent test set (2017_test) from the PDB released after	2017_test	2017_test set
15,753 proteins (3,797,426 AA) and 2017_test 3,154 proteins (651,594 AA), among	2017_test	2017_test set
cross-validation for hyperparameter optimization35. The 2017_test set is only used	2017_test	2017_test set
overlapping with our 2017_test set. We removed any protein	2017_test	2017_test set
Table 3 Assessment on the 2017_test set of three-state ensembles trained	2017_test	2017_test set
5 Performances on the smaller 2017_test set for which Spider3 generates	2017_test	2017_test set
of AA composition of Training, 2017_test and 2019_test	2017_test	2017_test set
of 6125 proteins, (2) CB513 of 513	CB513	CB513
very good accuracy on CullPDB, CB513 and CASP10, but not on	CB513	CB513
similar performance trend on the CB513 test set (see Table 5	CB513	CB513
five tested methods on the CB513 and	CB513	CB513
4. The Q8 accuracy on CB513 by the models of different	CB513	CB513
Figure 5. Q3 accuracy on CB513 and two CASP (CASP10-11) test	CB513	CB513
methods on 5 datasets: CullPDB, CB513, CASP10, CASP11 and CAMEO. The	CB513	CB513
CullPDB CB513 CASP10 CASP11 CAMEO	CB513	CB513
methods on 5 datasets: CullPDB, CB513, CASP10, CASP11 and CAMEO	CB513	CB513
CullPDB CB513 CASP10 CASP11 CAMEO	CB513	CB513
methods on 5 datasets: CullPDB, CB513, CASP10, CASP11 and CAMEO	CB513	CB513
CullPDB CB513 CASP10 CASP11 CAMEO	CB513	CB513
DeepCNF and ICML2014 on the CB513 dataset	CB513	CB513
4. The Q8 accuracy on CB513 by the models of different	CB513	CB513
Figure 5. Q3 accuracy on CB513 and two CASP (CASP10-11) test	CB513	CB513
of 6125 proteins, (2) CB513 of 513	CB	CB513
very good accuracy on CullPDB, CB513 and CASP10, but not on	CB	CB513
similar performance trend on the CB513 test set (see Table 5	CB	CB513
five tested methods on the CB513 and	CB	CB513
4. The Q8 accuracy on CB513 by the models of different	CB	CB513
Figure 5. Q3 accuracy on CB513 and two CASP (CASP10-11) test	CB	CB513
methods on 5 datasets: CullPDB, CB513, CASP10, CASP11 and CAMEO. The	CB	CB513
CullPDB CB513 CASP10 CASP11 CAMEO	CB	CB513
methods on 5 datasets: CullPDB, CB513, CASP10, CASP11 and CAMEO	CB	CB513
CullPDB CB513 CASP10 CASP11 CAMEO	CB	CB513
methods on 5 datasets: CullPDB, CB513, CASP10, CASP11 and CAMEO	CB	CB513
CullPDB CB513 CASP10 CASP11 CAMEO	CB	CB513
DeepCNF and ICML2014 on the CB513 dataset	CB	CB513
4. The Q8 accuracy on CB513 by the models of different	CB	CB513
Figure 5. Q3 accuracy on CB513 and two CASP (CASP10-11) test	CB	CB513
set are used as a blind test set. The first version	blind	Jpred4 blind set
correct prediction on the JPred4 blind	blind	Jpred4 blind set
We also assessed Jpred4 on the 2019_test set (see	Jpred4	Jpred4 blind set
classes recast to match the Jpred4 class assignment. In this case	Jpred4	Jpred4 blind set
show Q3 3.7–4.7% higher than Jpred4 and similar improvements in SOV	Jpred4	Jpred4 blind set
Jpred4 77.38% 72.29% 64.96% N.A. N.A	Jpred4	Jpred4 blind set
8. Most recent predictors and Jpred4 assessed on 2019_test set of	Jpred4	Jpred4 blind set
8 Most recent predictors and Jpred4 assessed on 2019_test set of	Jpred4	Jpred4 blind set
a large set of protein structures. Porter 5	set	Jpred4 blind set
classes on a large independent set	set	Jpred4 blind set
ical limits of prediction – set at 88–90% accuracy per AA	set	Jpred4 blind set
validation experiments on the training set	set	Jpred4 blind set
adopting a considerably larger training set but without evolutionary information, we	set	Jpred4 blind set
ensemble trained on a smaller set34	set	Jpred4 blind set
NN architectures on the validation set	set	Jpred4 blind set
proteins in our case), the set of hyperparameters selected for the	set	Jpred4 blind set
preliminary testing by splitting our set into 1/5 for testing and	set	Jpred4 blind set
above) on the full training set rather than on individual training	set	Jpred4 blind set
models on a completely independent set (see “2017_test” in Methods:Datasets) con	set	Jpred4 blind set
Full set (Porter 5) 83.42% 83.39% 83.49	set	Jpred4 blind set
3. Assessment on the 2017_test set of three-state ensembles trained on	set	Jpred4 blind set
full set	set	Jpred4 blind set
on 4/5 of the training set on PSI-BLAST, HHblits and concatenated	set	Jpred4 blind set
trained on the full training set achieves 73.02% Q8 accuracy on	set	Jpred4 blind set
set described in Methods:Dataset (see also	set	Jpred4 blind set
multiple predictors on independent test set	set	Jpred4 blind set
on the 2017_test set we created, containing 3,154 proteins	set	Jpred4 blind set
5); the full set of 3,154 proteins (Table	set	Jpred4 blind set
our tests on the 2017_test set with 3-class accu- racy of	set	Jpred4 blind set
the smaller version of the set and 84.2% on the larger	set	Jpred4 blind set
AA on the full test set	set	Jpred4 blind set
Performances on the smaller 2017_test set for which Spider3 generates predictions	set	Jpred4 blind set
both versions of the 2017_test set	set	Jpred4 blind set
of the set, respectively. Porter 5 is also	set	Jpred4 blind set
5 performances on the CASP1328 set, and on 6 months of	set	Jpred4 blind set
we obtained on the 2017_test set	set	Jpred4 blind set
only section of the 2017_test set, which is roughly 90% of	set	Jpred4 blind set
is selected for the training set, while a further 150 proteins	set	Jpred4 blind set
not included in the training set are used as a blind	set	Jpred4 blind set
test set	set	Jpred4 blind set
trained on the 1,348 JPred4 set	set	Jpred4 blind set
set	set	Jpred4 blind set
same sets. While the testing set is small (150 proteins), these	set	Jpred4 blind set
Q3 when trained on a set which is	set	Jpred4 blind set
smaller than its original training set	set	Jpred4 blind set
assessed Jpred4 on the 2019_test set (see Table	set	Jpred4 blind set
8 and description of the set in the following section	set	Jpred4 blind set
predictors. In a separate test set we assessed some very recent	set	Jpred4 blind set
more recent than our 2017_test set, i.e. MUFOLD-SS43, NetSurfP-2.048 and SPOT-1D44	set	Jpred4 blind set
generated a second independent test set (also see Methods:Datasets) starting from	set	Jpred4 blind set
not access the MUFOLD-SS training set	set	Jpred4 blind set
861 original proteins in the set, Table	set	Jpred4 blind set
quality of our large training set (see Table	set	Jpred4 blind set
the experiments on a training set twice as large as the	set	Jpred4 blind set
of a larger, well-distributed training set	set	Jpred4 blind set
and Jpred4 assessed on 2019_test set of 618 proteins	set	Jpred4 blind set
on a first independent test set (2017_test), along with some of	set	Jpred4 blind set
reduction of our final test set against their training sets35	set	Jpred4 blind set
overlapped with our original test set, we generated a second smaller	set	Jpred4 blind set
independent test set (2019_test) based	set	Jpred4 blind set
Specifically, we built our training set from the PDB released on	set	Jpred4 blind set
also built an independent test set (2017_test) from the PDB released	set	Jpred4 blind set
14, 2017. We redundancy-reduced this set at a 25% identity threshold	set	Jpred4 blind set
training set	set	Jpred4 blind set
internally redundancy reduced the resulting set at a 25% identity threshold	set	Jpred4 blind set
from both sets. The training set contains	set	Jpred4 blind set
In different tests the training set is used as a whole	set	Jpred4 blind set
for hyperparameter optimization35. The 2017_test set is only used	set	Jpred4 blind set
curated an additional independent test set (2019_test) to fairly compare Porter	set	Jpred4 blind set
overlapping with our 2017_test set	set	Jpred4 blind set
2019. We then redundancy-reduced this set	set	Jpred4 blind set
SPOT-1D, NetSurfP-2.0 and our training set at 25% identity threshold. Finally	set	Jpred4 blind set
the internal redundancy of this set at a 25% sequence identity	set	Jpred4 blind set
The simple idea is to set to 1 the position	set	Jpred4 blind set
The number of hyperparameters to set in a BRNN, and the	set	Jpred4 blind set
stochastic gradient descent (SGD)65. We set momentum to 0.9 and divided	set	Jpred4 blind set
cross entropy error on training set had not decreased for 100	set	Jpred4 blind set
epochs. The train- ing set is shuffled at the end	set	Jpred4 blind set
size of a mini-batch is set to ~10 proteins, that is	set	Jpred4 blind set
multiple predictors on independent test set	set	Jpred4 blind set
NN architectures on the validation set	set	Jpred4 blind set
3 Assessment on the 2017_test set of three-state ensembles trained on	set	Jpred4 blind set
either five-fold cross-validation or full set	set	Jpred4 blind set
AA on the full test set	set	Jpred4 blind set
Performances on the smaller 2017_test set for which Spider3 generates predictions	set	Jpred4 blind set
and Jpred4 assessed on 2019_test set of 618 proteins	set	Jpred4 blind set
DeepPrivacy Results on a diverse set of images. The left image	set	2019_test set
distribution. We anonymize the validation set of the WIDER-Face dataset [27	set	2019_test set
images while preserving a large set of facial attributes. This is	set	2019_test set
techniques on the WIDER-Face validation set	set	2019_test set
in the WIDER Face validation set	set	2019_test set
the faces in the validation set	set	2019_test set
calculate FID from a validation set of 50, 000 faces from	set	2019_test set
irregular poses. However, this would set restrictions on the pose estimator	set	2019_test set
on the anonymized WIDER-Face validation set	set	2019_test set
five publicly available datasets: (1) CullPDB 53	CullPDB	CullPDB
CullPDB dataset was constructed before CASP10	CullPDB	CullPDB
CullPDB into two subsets for training	CullPDB	CullPDB
training set consists of ~5600 CullPDB	CullPDB	CullPDB
the training set (containing 5600 CullPDB	CullPDB	CullPDB
obtains very good accuracy on CullPDB, CB513 and CASP10, but not	CullPDB	CullPDB
on the CullPDB test set. Both methods fail	CullPDB	CullPDB
of Q8 accuracy on the CullPDB training set with different	CullPDB	CullPDB
tested methods on 5 datasets: CullPDB, CB513, CASP10, CASP11 and CAMEO	CullPDB	CullPDB
CullPDB CB513 CASP10 CASP11 CAMEO	CullPDB	CullPDB
tested methods on 5 datasets: CullPDB, CB513, CASP10, CASP11 and CAMEO	CullPDB	CullPDB
CullPDB CB513 CASP10 CASP11 CAMEO	CullPDB	CullPDB
tested methods on 5 datasets: CullPDB, CB513, CASP10, CASP11 and CAMEO	CullPDB	CullPDB
CullPDB CB513 CASP10 CASP11 CAMEO	CullPDB	CullPDB
DeepCNF and ICML2014 on the CullPDB test set	CullPDB	CullPDB
of Q8 accuracy on the CullPDB training set with different	CullPDB	CullPDB
the three categories in the ShapeNet [3] dataset: airplane, chair, and	ShapeNet	ShapeNet Chair
on all shapes in the ShapeNet dataset. The auto-encoder is trained	ShapeNet	ShapeNet Chair
Models are first trained on ShapeNet to learn shape representations, which	ShapeNet	ShapeNet Chair
auto-encoder trained in the full ShapeNet dataset and train a linear	ShapeNet	ShapeNet Chair
Li Yi, and Fisher Yu. ShapeNet	ShapeNet	ShapeNet Chair
Chair	Chair	ShapeNet Chair
the three categories in the ShapeNet [3] dataset: airplane, chair, and	ShapeNet	ShapeNet Airplane
on all shapes in the ShapeNet dataset. The auto-encoder is trained	ShapeNet	ShapeNet Airplane
Models are first trained on ShapeNet to learn shape representations, which	ShapeNet	ShapeNet Airplane
auto-encoder trained in the full ShapeNet dataset and train a linear	ShapeNet	ShapeNet Airplane
Li Yi, and Fisher Yu. ShapeNet	ShapeNet	ShapeNet Airplane
Airplane	Airplane	ShapeNet Airplane
Airplane	Airplane	ShapeNet Airplane
3D CAD models such as ShapeNet [18], predicting 3D representations such	ShapeNet	ShapeNet Chair
multi-view and single-view settings on ShapeNet [18] objects, real-world scenes (KITTI	ShapeNet	ShapeNet Chair
Fig. 2: Results on ShapeNet [18]. The proposed framework typically	ShapeNet	ShapeNet Chair
test the proposed model on ShapeNet [18], where ground truth views	ShapeNet	ShapeNet Chair
Table 1: ShapeNet objects: we compare our framework	ShapeNet	ShapeNet Chair
synthesis approaches only focus on ShapeNet, we are also interested in	ShapeNet	ShapeNet Chair
Views Methods Car Chair L1 SSIM L1 SSIM	Chair	ShapeNet Chair
Views Methods Car Chair L1 SSIM L1 SSIM	Chair	ShapeNet Chair
Geometry-based View Synthesis. A great amount of	View	KITTI Novel View Synthesis
nor explicit 3D model. Novel View Synthesis. [19,20] propose to directly	View	KITTI Novel View Synthesis
10. Seitz, S.M., Dyer, C.R.: View morphing. In: Special Interest Group	View	KITTI Novel View Synthesis
W., Malik, J., Efros, A.A.: View synthesis by appear- ance flow	View	KITTI Novel View Synthesis
Geometry-based View Synthesis	Synthesis	KITTI Novel View Synthesis
explicit 3D model. Novel View Synthesis	Synthesis	KITTI Novel View Synthesis
Multi-view to Novel view: Synthesizing novel views with	Novel	KITTI Novel View Synthesis
Keywords: Novel view synthesis, multi-view novel view	Novel	KITTI Novel View Synthesis
supervision nor explicit 3D model. Novel View Synthesis. [19,20] propose to	Novel	KITTI Novel View Synthesis
4.1 Novel view synthesis for objects	Novel	KITTI Novel View Synthesis
4.2 Novel view synthesis for scenes	Novel	KITTI Novel View Synthesis
T., Fritz, M., Tuytelaars, T.: Novel views of objects from a	Novel	KITTI Novel View Synthesis
ShapeNet [18] objects, real-world scenes (KITTI Visual Odometry Dataset [41]), and	KITTI	KITTI Novel View Synthesis
our framework on both real (KITTI Visual Odometry Dataset [41]) and	KITTI	KITTI Novel View Synthesis
Fig. 5: Synthesized scenes on KITTI [41] and Synthia [42] datasets	KITTI	KITTI Novel View Synthesis
KITTI The dataset [41] was originally	KITTI	KITTI Novel View Synthesis
the same preprocessing procedures as KITTI to create the training and	KITTI	KITTI Novel View Synthesis
Views Methods KITTI Synthia L1 SSIM L1 SSIM	KITTI	KITTI Novel View Synthesis
to [19] and [22] on KITTI and Syn- thia	KITTI	KITTI Novel View Synthesis
Views Methods KITTI Synthia L1 SSIM L1 SSIM	KITTI	KITTI Novel View Synthesis
improvement (car: 26%, chair: 36%, KITTI	KITTI	KITTI Novel View Synthesis
19] (car: 19%, chair: 14%, KITTI	KITTI	KITTI Novel View Synthesis
Geometry-based View Synthesis. A great amount of	View	Synthia Novel View Synthesis
nor explicit 3D model. Novel View Synthesis. [19,20] propose to directly	View	Synthia Novel View Synthesis
10. Seitz, S.M., Dyer, C.R.: View morphing. In: Special Interest Group	View	Synthia Novel View Synthesis
W., Malik, J., Efros, A.A.: View synthesis by appear- ance flow	View	Synthia Novel View Synthesis
Geometry-based View Synthesis	Synthesis	Synthia Novel View Synthesis
explicit 3D model. Novel View Synthesis	Synthesis	Synthia Novel View Synthesis
41]), and synthe- sized scenes (Synthia dataset [42]). We benchmark against	Synthia	Synthia Novel View Synthesis
by FCN [43] trained on Synthia dataset [42	Synthia	Synthia Novel View Synthesis
Odometry Dataset [41]) and synthetic (Synthia Dataset [42]) scenes	Synthia	Synthia Novel View Synthesis
scenes on KITTI [41] and Synthia [42] datasets. Our framework typically	Synthia	Synthia Novel View Synthesis
Synthia The data was originally proposed	Synthia	Synthia Novel View Synthesis
Views Methods KITTI Synthia L1 SSIM L1 SSIM	Synthia	Synthia Novel View Synthesis
Views Methods KITTI Synthia L1 SSIM L1 SSIM	Synthia	Synthia Novel View Synthesis
on the sequences extracted from Synthia dataset [42] with the same	Synthia	Synthia Novel View Synthesis
Fig. 6: Synthia FCN-results for the scenes synthesized	Synthia	Synthia Novel View Synthesis
VOC and fine- tuned on Synthia dataset. The scores are estimated	Synthia	Synthia Novel View Synthesis
26%, chair: 36%, KITTI: 10%, Synthia	Synthia	Synthia Novel View Synthesis
19%, chair: 14%, KITTI: 4%, Synthia	Synthia	Synthia Novel View Synthesis
Multi-view to Novel view: Synthesizing novel views with	Novel	Synthia Novel View Synthesis
Keywords: Novel view synthesis, multi-view novel view	Novel	Synthia Novel View Synthesis
supervision nor explicit 3D model. Novel View Synthesis. [19,20] propose to	Novel	Synthia Novel View Synthesis
4.1 Novel view synthesis for objects	Novel	Synthia Novel View Synthesis
4.2 Novel view synthesis for scenes	Novel	Synthia Novel View Synthesis
T., Fritz, M., Tuytelaars, T.: Novel views of objects from a	Novel	Synthia Novel View Synthesis
the three categories in the ShapeNet [3] dataset: airplane, chair, and	ShapeNet	ShapeNet Car
on all shapes in the ShapeNet dataset. The auto-encoder is trained	ShapeNet	ShapeNet Car
Models are first trained on ShapeNet to learn shape representations, which	ShapeNet	ShapeNet Car
auto-encoder trained in the full ShapeNet dataset and train a linear	ShapeNet	ShapeNet Car
Li Yi, and Fisher Yu. ShapeNet	ShapeNet	ShapeNet Car
Car	Car	ShapeNet Car
3D CAD models such as ShapeNet [18], predicting 3D representations such	ShapeNet	ShapeNet Car
multi-view and single-view settings on ShapeNet [18] objects, real-world scenes (KITTI	ShapeNet	ShapeNet Car
Fig. 2: Results on ShapeNet [18]. The proposed framework typically	ShapeNet	ShapeNet Car
test the proposed model on ShapeNet [18], where ground truth views	ShapeNet	ShapeNet Car
Table 1: ShapeNet objects: we compare our framework	ShapeNet	ShapeNet Car
synthesis approaches only focus on ShapeNet, we are also interested in	ShapeNet	ShapeNet Car
Views Methods Car Chair L1 SSIM L1 SSIM	Car	ShapeNet Car
Views Methods Car Chair L1 SSIM L1 SSIM	Car	ShapeNet Car
3D Motion Seg- mentation Benchmark (KT3DMoSeg	KT3DMoSeg	KT3DMoSeg
idea of its performance in KT3DMoSeg by looking at the result	KT3DMoSeg	KT3DMoSeg
on Hopkins155, Hopkins12, MTPV62 and KT3DMoSeg datasets evaluated as clas- sification	KT3DMoSeg	KT3DMoSeg
31] Hopkins12 [26] MTPV62 [23]∗∗ KT3DMoSeg	KT3DMoSeg	KT3DMoSeg
results on some sequences from KT3DMoSeg in Fig. 3 to better	KT3DMoSeg	KT3DMoSeg
The classification error over all KT3DMoSeg sequences v.s. λ and γ	KT3DMoSeg	KT3DMoSeg
and sensitivity to parameters for KT3DMoSeg benchmark	KT3DMoSeg	KT3DMoSeg
Examples of motion segmentation on KT3DMoSeg sequences	KT3DMoSeg	KT3DMoSeg
also for real-world sequences in KT3DMoSeg	KT3DMoSeg	KT3DMoSeg
propose a new dataset, the KT3DMoSeg dataset, to reflect and investigate	KT3DMoSeg	KT3DMoSeg
of perspective effects in the Hopkins155 benchmark [31]. However, it is	Hopkins	Hopkins155
mo- tion segmentation. In the Hopkins155 dataset, this is not an	Hopkins	Hopkins155
that there are actually some Hopkins sequences with non-negligible perspective effects	Hopkins	Hopkins155
21] dealing with the realistic Hopkins155 [31] sequences almost as a	Hopkins	Hopkins155
of perspective effects in the Hopkins sequences. Thus, these later works	Hopkins	Hopkins155
mentation benchmarks including the Hopkins155 [31], the Hopkins12 [26] for	Hopkins	Hopkins155
single-view and multi-view approaches on Hopkins155 benchmark [31]. Specifically, for single-view	Hopkins	Hopkins155
for subset constrained clustering on Hopkins155	Hopkins	Hopkins155
performance can be observed on Hopkins12 and MTPV62 as well. Usu	Hopkins	Hopkins155
The limitations of the Hopkins155 dataset are well- known: limited	Hopkins	Hopkins155
1: Motion segmentation results on Hopkins155, Hopkins12, MTPV62 and KT3DMoSeg datasets	Hopkins	Hopkins155
Models Hopkins155 [31] Hopkins12 [26] MTPV62 [23]∗∗ KT3DMoSeg	Hopkins	Hopkins155
12 clips Hopkins 50 clips	Hopkins	Hopkins155
the extant datasets such as Hopkins155, but also for real-world sequences	Hopkins	Hopkins155
we carry out experiments on Hopkins155, Hopkins12 and MTPV62 and achieved	Hopkins	Hopkins155
of perspective effects in the Hopkins155 benchmark [31]. However, it is	Hopkins155	Hopkins155
mo- tion segmentation. In the Hopkins155 dataset, this is not an	Hopkins155	Hopkins155
21] dealing with the realistic Hopkins155 [31] sequences almost as a	Hopkins155	Hopkins155
mentation benchmarks including the Hopkins155 [31], the Hopkins12 [26] for	Hopkins155	Hopkins155
single-view and multi-view approaches on Hopkins155 benchmark [31]. Specifically, for single-view	Hopkins155	Hopkins155
for subset constrained clustering on Hopkins155	Hopkins155	Hopkins155
The limitations of the Hopkins155 dataset are well- known: limited	Hopkins155	Hopkins155
1: Motion segmentation results on Hopkins155, Hopkins12, MTPV62 and KT3DMoSeg datasets	Hopkins155	Hopkins155
Models Hopkins155 [31] Hopkins12 [26] MTPV62 [23	Hopkins155	Hopkins155
the extant datasets such as Hopkins155, but also for real-world sequences	Hopkins155	Hopkins155
we carry out experiments on Hopkins155, Hopkins12 and MTPV62 and achieved	Hopkins155	Hopkins155
that represent different scenarios: (i) Hopkins155 for motion segmentation; (ii) Extended	Hopkins	Hopkins155
sequences with missing data; (iv) Hopkins outdoor sequences for semi-dense motion	Hopkins	Hopkins155
5.1. Hopkins155	Hopkins	Hopkins155
Hopkins155 [30] is a standard benchmark	Hopkins	Hopkins155
data gracefully, we employed the Hopkins 12 additional sequences containing incomplete	Hopkins	Hopkins155
5.4. Hopkins Outdoor: Semi-dense, Incomplete Data with	Hopkins	Hopkins155
are 21 outdoor videos in Hopkins155, the tracking code that we	Hopkins	Hopkins155
Clustering error (in %) on Hopkins 155	Hopkins	Hopkins155
Table 2: Ablation study on Hopkins 155	Hopkins	Hopkins155
Clustering error (in %) on Hopkins 12 Real Motion Sequences with	Hopkins	Hopkins155
Appendix – Hopkins Outdoor: Semi-dense, Incomplete Data with	Hopkins	Hopkins155
that represent different scenarios: (i) Hopkins155 for motion segmentation; (ii) Extended	Hopkins155	Hopkins155
5.1. Hopkins155	Hopkins155	Hopkins155
Hopkins155 [30] is a standard benchmark	Hopkins155	Hopkins155
are 21 outdoor videos in Hopkins155, the tracking code that we	Hopkins155	Hopkins155
of Biomedical Engineering, The Johns Hopkins University, USA. E-mail: rvi- dal@cis.jhu.edu	Hopkins	Hopkins155
Hopkins 155	Hopkins	Hopkins155
Hopkins 155	Hopkins	Hopkins155
Hopkins 155 Dataset	Hopkins	Hopkins155
prob- lem, we consider the Hopkins 155 dataset [66], which consists	Hopkins	Hopkins155
of different algorithms on the Hopkins 155 dataset with	Hopkins	Hopkins155
small principal angles. In the Hopkins	Hopkins	Hopkins155
dataset. As shown, in the Hopkins	Hopkins	Hopkins155
can conclude that in the Hopkins 155 dataset the challenge is	Hopkins	Hopkins155
do so, we consider the Hopkins 155 dataset [66] that consists	Hopkins	Hopkins155
of different algorithms on the Hopkins 155 dataset with	Hopkins	Hopkins155
of SSC over the entire Hopkins 155 dataset. Note that the	Hopkins	Hopkins155
for testing incomplete trajectories and MTPV62 [23] for testing stronger perspective	MTPV	MTPV62
be observed on Hopkins12 and MTPV62 as well. Usu- ally, the	MTPV	MTPV62
segmentation results on Hopkins155, Hopkins12, MTPV62 and KT3DMoSeg datasets evaluated as	MTPV	MTPV62
Models Hopkins155 [31] Hopkins12 [26] MTPV62 [23]∗∗ KT3DMoSeg	MTPV	MTPV62
experiments on Hopkins155, Hopkins12 and MTPV62 and achieved state-of-the-art performances on	MTPV	MTPV62
for testing incomplete trajectories and MTPV62 [23] for testing stronger perspective	MTPV62	MTPV62
be observed on Hopkins12 and MTPV62 as well. Usu- ally, the	MTPV62	MTPV62
segmentation results on Hopkins155, Hopkins12, MTPV62 and KT3DMoSeg datasets evaluated as	MTPV62	MTPV62
Models Hopkins155 [31] Hopkins12 [26] MTPV62 [23]∗∗ KT3DMoSeg	MTPV62	MTPV62
experiments on Hopkins155, Hopkins12 and MTPV62 and achieved state-of-the-art performances on	MTPV62	MTPV62
evaluate the proposed method on HICO	HICO	HICO-DET
We perform extensive experiments on HICO	HICO	HICO-DET
three Default category sets on HICO	HICO	HICO-DET
datasets, such as V-COCO [13], HICO	HICO	HICO-DET
train and test C on HICO	HICO	HICO-DET
We adopt two HOI datasets HICO	HICO	HICO-DET
-DET [3] and V-COCO [13]. HICO	HICO	HICO-DET
HICO-DET RPDCD HICO-DET HICO	HICO	HICO-DET
-DET RPT1CD V-COCO HICO-DET RPT2CD HICO	HICO	HICO-DET
-DET, V-COCO HICO	HICO	HICO-DET
HICO-DET RCD - HICO	HICO	HICO-DET
V-COCO RPDCD V-COCO V-COCO RPT1CD HICO	HICO	HICO-DET
-DET V-COCO RPT2CD HICO	HICO	HICO-DET
RCD - V-COCO RCT - HICO	HICO	HICO-DET
24, 12, 21, 9] on HICO	HICO	HICO-DET
with mean average precision. For HICO	HICO	HICO-DET
Know Object Full sets on HICO	HICO	HICO-DET
Table 2. Results comparison on HICO	HICO	HICO-DET
most state-of-the- art performance. On HICO	HICO	HICO-DET
and effectiveness of interactiveness. Since HICO	HICO	HICO-DET
transferring is per- formed from HICO	HICO	HICO-DET
a relatively smaller improvement on HICO	HICO	HICO-DET
HICO	HICO	HICO-DET
HICO	HICO	HICO-DET
We perform extensive experiments on HICO-DET [3], V-COCO [13] datasets. Our	HICO-DET	HICO-DET
three Default category sets on HICO-DET, 4.0 and 3.4 mAP on	HICO-DET	HICO-DET
datasets, such as V-COCO [13], HICO-DET [3], HCVRD [31], were proposed	HICO-DET	HICO-DET
train and test C on HICO-DET (referred as “RCD”). Second, we	HICO-DET	HICO-DET
We adopt two HOI datasets HICO-DET [3] and V-COCO [13]. HICO-DET	HICO-DET	HICO-DET
HICO-DET RPDCD HICO-DET HICO-DET RPT1CD V-COCO HICO-DET RPT2CD HICO-DET	HICO-DET	HICO-DET
, V-COCO HICO-DET	HICO-DET	HICO-DET
HICO-DET RCD - HICO-DET RCT - V-COCO	HICO-DET	HICO-DET
V-COCO RPDCD V-COCO V-COCO RPT1CD HICO-DET V-COCO RPT2CD HICO-DET, V-COCO V-COCO	HICO-DET	HICO-DET
RCD - V-COCO RCT - HICO-DET	HICO-DET	HICO-DET
24, 12, 21, 9] on HICO-DET, and four methods [13, 12	HICO-DET	HICO-DET
Know Object Full sets on HICO-DET	HICO-DET	HICO-DET
Table 2. Results comparison on HICO-DET [3]. D indicates the default	HICO-DET	HICO-DET
most state-of-the- art performance. On HICO-DET, RPT2CD surpasses [9] by 2.38	HICO-DET	HICO-DET
and effectiveness of interactiveness. Since HICO-DET train set (38K) is much	HICO-DET	HICO-DET
transferring is per- formed from HICO-DET to V-COCO. As we can	HICO-DET	HICO-DET
a relatively smaller improvement on HICO-DET when compared with mode RPDCD	HICO-DET	HICO-DET
HICO-DET RPDCD -65.96% RPT1CD -62.24% RPT2CD	HICO-DET	HICO-DET
HICO-DET V-COCO Method Default Full KO	HICO-DET	HICO-DET
the proposed method on HICO- DET and V-COCO datasets. Our framework	DET	HICO-DET
DET [3], V-COCO [13] datasets. Our	DET	HICO-DET
DET, 4.0 and 3.4 mAP on	DET	HICO-DET
DET [3], HCVRD [31], were proposed	DET	HICO-DET
DET (referred as “RCD”). Second, we	DET	HICO-DET
DET [3] and V-COCO [13]. HICO-DET	DET	HICO-DET
DET RPDCD HICO-DET HICO-DET RPT1CD V-COCO	DET	HICO-DET
DET RPT2CD HICO-DET, V-COCO HICO-DET	DET	HICO-DET
DET RCD - HICO-DET RCT	DET	HICO-DET
DET V-COCO RPT2CD HICO-DET, V-COCO V-COCO	DET	HICO-DET
DET	DET	HICO-DET
DET, and four methods [13, 12	DET	HICO-DET
mean average precision. For HICO- DET, we follow the settings in	DET	HICO-DET
DET	DET	HICO-DET
DET [3]. D indicates the default	DET	HICO-DET
DET, RPT2CD surpasses [9] by 2.38	DET	HICO-DET
DET train set (38K) is much	DET	HICO-DET
DET to V-COCO. As we can	DET	HICO-DET
DET when compared with mode RPDCD	DET	HICO-DET
DET RPDCD -65.96% RPT1CD -62.24% RPT2CD	DET	HICO-DET
DET V-COCO Method Default Full KO	DET	HICO-DET
the Verb in COCO and HICO	HICO	HICO-DET
Humans Interacting with Common Objects (HICO	HICO	HICO-DET
on V-COCO and 49% on HICO	HICO	HICO-DET
while sitting on a chair. HICO	HICO	HICO-DET
is a subset of the HICO dataset [3]. HICO-DET contains 600	HICO	HICO-DET
16] for both V-COCO and HICO datasets. The goal is to	HICO	HICO-DET
comparison with the state-of-the-arts on HICO	HICO	HICO-DET
single NVIDIA P100 GPU. For HICO	HICO	HICO-DET
procedures for both V-COCO and HICO	HICO	HICO-DET
V-COCO in Table 1 and HICO	HICO	HICO-DET
approaches [14, 16, 22]. For HICO	HICO	HICO-DET
the V-COCO dataset and the HICO	HICO	HICO-DET
Sample HOI detections on the HICO	HICO	HICO-DET
Jiaxuan Wang, and Jia Deng. HICO	HICO	HICO-DET
the Verb in COCO and HICO-DET datasets and show that our	HICO-DET	HICO-DET
Humans Interacting with Common Objects (HICO-DET) [4] datasets. Our results show	HICO-DET	HICO-DET
on V-COCO and 49% on HICO-DET with respect to the existing	HICO-DET	HICO-DET
while sitting on a chair. HICO-DET [3] is a subset of	HICO-DET	HICO-DET
the HICO dataset [3]. HICO-DET contains 600 HOI categories over	HICO-DET	HICO-DET
comparison with the state-of-the-arts on HICO-DET test set. The results from	HICO-DET	HICO-DET
single NVIDIA P100 GPU. For HICO-DET, training the network on the	HICO-DET	HICO-DET
procedures for both V-COCO and HICO-DET datasets. Please refer to the	HICO-DET	HICO-DET
V-COCO in Table 1 and HICO-DET in Table 2. For V-COCO	HICO-DET	HICO-DET
approaches [14, 16, 22]. For HICO-DET, we also demonstrate that our	HICO-DET	HICO-DET
the V-COCO dataset and the HICO-DET dataset. We highlight the detected	HICO-DET	HICO-DET
Sample HOI detections on the HICO-DET test set. Our model detects	HICO-DET	HICO-DET
DET datasets and show that our	DET	HICO-DET
DET) [4] datasets. Our results show	DET	HICO-DET
DET with respect to the existing	DET	HICO-DET
DET [3] is a subset of	DET	HICO-DET
DET contains 600 HOI categories over	DET	HICO-DET
DET test set. The results from	DET	HICO-DET
DET, training the network on the	DET	HICO-DET
DET datasets. Please refer to the	DET	HICO-DET
DET in Table 2. For V-COCO	DET	HICO-DET
DET, we also demonstrate that our	DET	HICO-DET
DET dataset. We highlight the detected	DET	HICO-DET
DET test set. Our model detects	DET	HICO-DET
benchmarks on images and videos: HICO	HICO	HICO-DET
on three HOI datasets, namely HICO	HICO	HICO-DET
HOI detection from im- ages (HICO	HICO	HICO-DET
first experiment is performed on HICO	HICO	HICO-DET
connecting them. Datasets. We use HICO	HICO	HICO-DET
for benchmarking our GPNN model. HICO	HICO	HICO-DET
HOI detection results (mAP) on HICO	HICO	HICO-DET
3. HOI detection results on HICO	HICO	HICO-DET
117)-Sigmoid(·) and FC(dV -26)-Sigmoid(·) for HICO	HICO	HICO-DET
Following the standard settings in HICO	HICO	HICO-DET
IoU) greater than 0.5. For HICO	HICO	HICO-DET
all 600 HOI categories in HICO (Full); ii) 138 HOI categories	HICO	HICO-DET
HOI category sets on the HICO	HICO	HICO-DET
V-COCO [17] HICO	HICO	HICO-DET
Y., Wang, J., Deng, J.: HICO	HICO	HICO-DET
benchmarks on images and videos: HICO-DET, V-COCO, and CAD-120 datasets. Our	HICO-DET	HICO-DET
on three HOI datasets, namely HICO-DET [1], V-COCO [17] and CAD-120	HICO-DET	HICO-DET
HOI detection from im- ages (HICO-DET, V-COCO) and HOI recognition and	HICO-DET	HICO-DET
first experiment is performed on HICO-DET [1] and V-COCO [17] datasets	HICO-DET	HICO-DET
connecting them. Datasets. We use HICO-DET [1] and V-COCO [17] datasets	HICO-DET	HICO-DET
for benchmarking our GPNN model. HICO-DET provides more than 150K annotated	HICO-DET	HICO-DET
HOI detection results (mAP) on HICO-DET dataset [1]. Higher values are	HICO-DET	HICO-DET
3. HOI detection results on HICO-DET [1] test images. Human and	HICO-DET	HICO-DET
117)-Sigmoid(·) and FC(dV -26)-Sigmoid(·) for HICO-DET and V-COCO, respectively	HICO-DET	HICO-DET
Following the standard settings in HICO-DET and V- COCO benchmarks, we	HICO-DET	HICO-DET
IoU) greater than 0.5. For HICO-DET dataset, we report the mAP	HICO-DET	HICO-DET
V-COCO [17] HICO-DET [1] CAD-120 [22	HICO-DET	HICO-DET
DET, V-COCO, and CAD-120 datasets. Our	DET	HICO-DET
DET [1], V-COCO [17] and CAD-120	DET	HICO-DET
DET, V-COCO) and HOI recognition and	DET	HICO-DET
DET [1] and V-COCO [17] datasets	DET	HICO-DET
DET [1] and V-COCO [17] datasets	DET	HICO-DET
DET provides more than 150K annotated	DET	HICO-DET
DET dataset [1]. Higher values are	DET	HICO-DET
DET [1] test images. Human and	DET	HICO-DET
DET and V-COCO, respectively	DET	HICO-DET
DET and V- COCO benchmarks, we	DET	HICO-DET
DET dataset, we report the mAP	DET	HICO-DET
category sets on the HICO- DET dataset. The results on V-COCO	DET	HICO-DET
DET [1] CAD-120 [22	DET	HICO-DET
Verbs in COCO (V-COCO) and HICO	HICO	HICO-DET
improvement on the newly released HICO	HICO	HICO-DET
Verbs in COCO) [13] and HICO	HICO	HICO-DET
ous components. The newly released HICO	HICO	HICO-DET
The older TUHOI [18] and HICO [4] datasets only have image	HICO	HICO-DET
Table 6. Results on HICO	HICO	HICO-DET
HICO	HICO	HICO-DET
We additionally evaluate InteractNet on HICO	HICO	HICO-DET
9. Our results on the HICO	HICO	HICO-DET
Appendix B: HICO	HICO	HICO-DET
also test InteractNet on the HICO	HICO	HICO-DET
-DET dataset [3]. HICO	HICO	HICO-DET
are not exhaustively annotated on HICO	HICO	HICO-DET
ground truth labels from the HICO	HICO	HICO-DET
J. Wang, and J. Deng. HICO	HICO	HICO-DET
Verbs in COCO (V-COCO) and HICO-DET datasets, where we show quantitatively	HICO-DET	HICO-DET
improvement on the newly released HICO-DET dataset [3]. Finally, our method	HICO-DET	HICO-DET
Verbs in COCO) [13] and HICO-DET [3]. V- COCO serves as	HICO-DET	HICO-DET
ous components. The newly released HICO-DET [3] con- tains ∼48k images	HICO-DET	HICO-DET
Table 6. Results on HICO-DET test set. InteractNet outper- forms	HICO-DET	HICO-DET
HICO-DET Dataset. We additionally evaluate InteractNet	HICO-DET	HICO-DET
on HICO-DET [3] which contains 600 types	HICO-DET	HICO-DET
9. Our results on the HICO-DET test set. Each image shows	HICO-DET	HICO-DET
Appendix B: HICO-DET Dataset	HICO-DET	HICO-DET
also test InteractNet on the HICO-DET dataset [3]. HICO-DET contains approximately	HICO-DET	HICO-DET
are not exhaustively annotated on HICO-DET	HICO-DET	HICO-DET
ground truth labels from the HICO-DET anno- tations to each person	HICO-DET	HICO-DET
DET datasets, where we show quantitatively	DET	HICO-DET
DET dataset [3]. Finally, our method	DET	HICO-DET
DET [3]. V- COCO serves as	DET	HICO-DET
DET [3] con- tains ∼48k images	DET	HICO-DET
DET test set. InteractNet outper- forms	DET	HICO-DET
DET Dataset. We additionally evaluate InteractNet	DET	HICO-DET
DET [3] which contains 600 types	DET	HICO-DET
DET test set. Each image shows	DET	HICO-DET
DET Dataset	DET	HICO-DET
DET dataset [3]. HICO-DET contains approximately	DET	HICO-DET
DET	DET	HICO-DET
DET anno- tations to each person	DET	HICO-DET
COCO datasets. Our framework outperforms state-of-the-art	COCO	V-COCO
COCO [13] datasets. Our method cooperated	COCO	V-COCO
COCO	COCO	V-COCO
COCO [13], HICO-DET [3], HCVRD [31	COCO	V-COCO
estimate his/her 17 keypoints (in COCO format [18]). Then, we link	COCO	V-COCO
COCO HOIs, then finetune C for	COCO	V-COCO
COCO train set. Last, we test	COCO	V-COCO
COCO test set. Details of the	COCO	V-COCO
COCO [13]. HICO-DET [3] includes 47,776	COCO	V-COCO
COCO [13] provides 10,346 images (2,533	COCO	V-COCO
COCO HICO-DET RPT2CD HICO-DET, V-COCO HICO-DET	COCO	V-COCO
COCO	COCO	V-COCO
COCO RPDCD V-COCO V-COCO RPT1CD HICO-DET	COCO	V-COCO
COCO RPT2CD HICO-DET, V-COCO V-COCO	COCO	V-COCO
COCO RCD - V-COCO RCT	COCO	V-COCO
object detection re- sults and COCO [18] pre-trained weights from [9	COCO	V-COCO
COCO	COCO	V-COCO
COCO, we evaluateAProle (24 actions with	COCO	V-COCO
COCO are shown in Table 3	COCO	V-COCO
COCO	COCO	V-COCO
COCO train set (2.5K), improvement is	COCO	V-COCO
COCO	COCO	V-COCO
COCO, but it shows a relatively	COCO	V-COCO
COCO [13]. D indicates the default	COCO	V-COCO
COCO RPDCD -65.98% RPT1CD -59.51% RPT2CD	COCO	V-COCO
COCO Method Default Full KO Full	COCO	V-COCO
method on HICO- DET and V	V	V-COCO
C V	V	V-COCO
extensive experiments on HICO-DET [3], V	V	V-COCO
4.0 and 3.4 mAP on V	V	V-COCO
large- scale datasets, such as V	V	V-COCO
9], HOI graph G = (V, E) is dense connected, where	V	V-COCO
V includes human node Vh and	V	V-COCO
that fits the number of V	V	V-COCO
C for 1 epoch on V	V	V-COCO
test this new C on V	V	V-COCO
HOI datasets HICO-DET [3] and V	V	V-COCO
than 150k annotated human-object pairs. V	V	V-COCO
HICO-DET RPDCD HICO-DET HICO-DET RPT1CD V	V	V-COCO
-COCO HICO-DET RPT2CD HICO-DET, V	V	V-COCO
RCD - HICO-DET RCT - V	V	V-COCO
V-COCO RPDCD V-COCO V	V	V-COCO
-COCO RPT1CD HICO-DET V	V	V-COCO
-COCO RPT2CD HICO-DET, V-COCO V	V	V-COCO
V-COCO RCD - V	V	V-COCO
13, 12, 21, 9] on V	V	V-COCO
and Known Object mode. For V	V	V-COCO
is accordingly reduced. Results on V	V	V-COCO
4.0 and 3.4 mAP on V	V	V-COCO
38K) is much bigger than V	V	V-COCO
per- formed from HICO-DET to V	V	V-COCO
RPT1CD achieves obvious improvement on V	V	V-COCO
Table 3. Results comparison on V	V	V-COCO
V	V	V-COCO
HICO-DET V	V	V-COCO
2] F. Caba Heilbron, V	V	V-COCO
6] V	V	V-COCO
method on HICO- DET and V-COCO datasets. Our framework outperforms state-of-the-art	V-COCO	V-COCO
extensive experiments on HICO-DET [3], V-COCO [13] datasets. Our method cooperated	V-COCO	V-COCO
4.0 and 3.4 mAP on V-COCO	V-COCO	V-COCO
large- scale datasets, such as V-COCO [13], HICO-DET [3], HCVRD [31	V-COCO	V-COCO
that fits the number of V-COCO HOIs, then finetune C for	V-COCO	V-COCO
1 epoch on V-COCO train set. Last, we test	V-COCO	V-COCO
this new C on V-COCO test set. Details of the	V-COCO	V-COCO
HOI datasets HICO-DET [3] and V-COCO [13]. HICO-DET [3] includes 47,776	V-COCO	V-COCO
than 150k annotated human-object pairs. V-COCO [13] provides 10,346 images (2,533	V-COCO	V-COCO
HICO-DET RPDCD HICO-DET HICO-DET RPT1CD V-COCO HICO-DET RPT2CD HICO-DET, V-COCO HICO-DET	V-COCO	V-COCO
RCD - HICO-DET RCT - V-COCO	V-COCO	V-COCO
V-COCO RPDCD V-COCO V-COCO RPT1CD HICO-DET V-COCO RPT2CD HICO-DET	V-COCO	V-COCO
, V-COCO V-COCO	V-COCO	V-COCO
V-COCO RCD - V-COCO RCT - HICO-DET	V-COCO	V-COCO
13, 12, 21, 9] on V-COCO	V-COCO	V-COCO
and Known Object mode. For V-COCO, we evaluateAProle (24 actions with	V-COCO	V-COCO
is accordingly reduced. Results on V-COCO are shown in Table 3	V-COCO	V-COCO
4.0 and 3.4 mAP on V-COCO	V-COCO	V-COCO
38K) is much bigger than V-COCO train set (2.5K), improvement is	V-COCO	V-COCO
per- formed from HICO-DET to V-COCO	V-COCO	V-COCO
RPT1CD achieves obvious improvement on V-COCO, but it shows a relatively	V-COCO	V-COCO
Table 3. Results comparison on V-COCO [13]. D indicates the default	V-COCO	V-COCO
V-COCO RPDCD -65.98% RPT1CD -59.51% RPT2CD	V-COCO	V-COCO
HICO-DET V-COCO Method Default Full KO Full	V-COCO	V-COCO
network on the Verb in COCO and HICO-DET datasets and show	COCO	V-COCO
on HOI detection: Verbs in COCO (V-COCO) [16] and Humans Interacting	COCO	V-COCO
COCO and 49% on HICO-DET with	COCO	V-COCO
COCO [16] is a subset of	COCO	V-COCO
the COCO dataset [26] that provides HOI	COCO	V-COCO
COCO includes a total of 10,346	COCO	V-COCO
COCO and HICO datasets. The goal	COCO	V-COCO
COCO trainval set with a learning	COCO	V-COCO
COCO takes 16 hours on a	COCO	V-COCO
COCO test set	COCO	V-COCO
COCO and HICO-DET datasets. Please refer	COCO	V-COCO
COCO in Table 1 and HICO-DET	COCO	V-COCO
COCO, the proposed instance-centric attention network	COCO	V-COCO
COCO dataset and the HICO-DET dataset	COCO	V-COCO
COCO test set. Our model detects	COCO	V-COCO
COCO test dataset	COCO	V-COCO
COCO dataset. However, this comes at	COCO	V-COCO
and C Lawrence Zitnick. Microsoft COCO	COCO	V-COCO
C V	V	V-COCO
HOI detection: Verbs in COCO (V	V	V-COCO
around 10% relative improvement on V	V	V-COCO
Datasets. V	V	V-COCO
26] that provides HOI annotations. V	V	V-COCO
role mAP) [16] for both V	V	V-COCO
for 300K iterations on the V	V	V-COCO
0.9. Training our network on V	V	V-COCO
with the state-of-the-arts on the V	V	V-COCO
and inference procedures for both V	V	V-COCO
in terms of AProle on V	V	V-COCO
HICO-DET in Table 2. For V	V	V-COCO
HOI detection results on the V	V	V-COCO
Sample HOI detections on the V	V	V-COCO
3: Ablation study on the V	V	V-COCO
achieves the best performance on V	V	V-COCO
HOI detection: Verbs in COCO (V-COCO) [16] and Humans Interacting with	V-COCO	V-COCO
around 10% relative improvement on V-COCO and 49% on HICO-DET with	V-COCO	V-COCO
Datasets. V-COCO [16] is a subset of	V-COCO	V-COCO
26] that provides HOI annotations. V-COCO includes a total of 10,346	V-COCO	V-COCO
role mAP) [16] for both V-COCO and HICO datasets. The goal	V-COCO	V-COCO
for 300K iterations on the V-COCO trainval set with a learning	V-COCO	V-COCO
0.9. Training our network on V-COCO takes 16 hours on a	V-COCO	V-COCO
with the state-of-the-arts on the V-COCO test set	V-COCO	V-COCO
and inference procedures for both V-COCO and HICO-DET datasets. Please refer	V-COCO	V-COCO
in terms of AProle on V-COCO in Table 1 and HICO-DET	V-COCO	V-COCO
in Table 2. For V-COCO, the proposed instance-centric attention network	V-COCO	V-COCO
HOI detection results on the V-COCO dataset and the HICO-DET dataset	V-COCO	V-COCO
Sample HOI detections on the V-COCO test set. Our model detects	V-COCO	V-COCO
3: Ablation study on the V-COCO test dataset	V-COCO	V-COCO
achieves the best performance on V-COCO dataset. However, this comes at	V-COCO	V-COCO
COCO, and CAD-120 datasets. Our approach	COCO	V-COCO
COCO [17] and CAD-120 [22], for	COCO	V-COCO
COCO) and HOI recognition and anticipation	COCO	V-COCO
COCO [17] datasets, showing that our	COCO	V-COCO
COCO [17] datasets for benchmarking our	COCO	V-COCO
COCO [28] and 117 action categories	COCO	V-COCO
COCO is a subset of MS-COCO	COCO	V-COCO
COCO [17] dataset. Legend: Set 1	COCO	V-COCO
COCO [17] test images. Human and	COCO	V-COCO
COCO, respectively	COCO	V-COCO
settings in HICO-DET and V- COCO benchmarks, we evaluate HOI detection	COCO	V-COCO
COCO dataset, since we concentrate on	COCO	V-COCO
COCO dataset (in Table 2) also	COCO	V-COCO
COCO [17] HICO-DET [1] CAD-120 [22	COCO	V-COCO
on images and videos: HICO-DET, V	V	V-COCO
C V	V	V-COCO
HOI datasets, namely HICO-DET [1], V	V	V-COCO
detection from im- ages (HICO-DET, V	V	V-COCO
Formally, let G = (V, E ,Y) denote the complete	V	V-COCO
HOI graph. Nodes v ∈ V take unique values from {1	V	V-COCO
V	V	V-COCO
e = (v, w) ∈ V × V. Each node v	V	V-COCO
of G, where Vg ⊆ V and Eg ⊆ E	V	V-COCO
V	V	V-COCO
V	V	V-COCO
complete HOI graph G = (V, E ,Y), we use dV	V	V-COCO
V |×|V |×(2dV +dE) (see in	V	V-COCO
V	V	V-COCO
V	V	V-COCO
hw, Γvw) = [W M V hv,W	V	V-COCO
M V hw,W	V	V-COCO
performed on HICO-DET [1] and V	V	V-COCO
We use HICO-DET [1] and V	V	V-COCO
28] and 117 action categories. V	V	V-COCO
HOI detection results (mAP) on V	V	V-COCO
4. HOI detection results on V	V	V-COCO
FC(dV -26)-Sigmoid(·) for HICO-DET and V	V	V-COCO
standard settings in HICO-DET and V	V	V-COCO
more training instances (Non-Rare). For V	V	V-COCO
DET dataset. The results on V	V	V-COCO
V	V	V-COCO
8. Delaitre, V	V	V-COCO
Jayasumana, S., Romera-Paredes, B., Vineet, V	V	V-COCO
on images and videos: HICO-DET, V-COCO, and CAD-120 datasets. Our approach	V-COCO	V-COCO
HOI datasets, namely HICO-DET [1], V-COCO [17] and CAD-120 [22], for	V-COCO	V-COCO
detection from im- ages (HICO-DET, V-COCO) and HOI recognition and anticipation	V-COCO	V-COCO
performed on HICO-DET [1] and V-COCO [17] datasets, showing that our	V-COCO	V-COCO
We use HICO-DET [1] and V-COCO [17] datasets for benchmarking our	V-COCO	V-COCO
28] and 117 action categories. V-COCO is a subset of MS-COCO	V-COCO	V-COCO
HOI detection results (mAP) on V-COCO [17] dataset. Legend: Set 1	V-COCO	V-COCO
4. HOI detection results on V-COCO [17] test images. Human and	V-COCO	V-COCO
FC(dV -26)-Sigmoid(·) for HICO-DET and V-COCO, respectively	V-COCO	V-COCO
more training instances (Non-Rare). For V-COCO dataset, since we concentrate on	V-COCO	V-COCO
DET dataset. The results on V-COCO dataset (in Table 2) also	V-COCO	V-COCO
V-COCO [17] HICO-DET [1] CAD-120 [22	V-COCO	V-COCO
neighboring entities and relationships from REVERB for some words from our	REVERB	Reverb
learning for QA. Using the Reverb KB and QA datasets, we	Reverb	Reverb
show that Reverb facts can be added to	Reverb	Reverb
We also use the KB Reverb as a secondary source of	Reverb	Reverb
FB2M FB5M Reverb ENTITIES 2,150,604 4,904,397 2,044,752 RELATIONSHIPS	Reverb	Reverb
be used to answer using Reverb	Reverb	Reverb
with- out being trained on Reverb data. This is a pure	Reverb	Reverb
setting of transfer learning. Reverb is interesting for this experiment	Reverb	Reverb
3. Connecting Reverb	Reverb	Reverb
adds new facts coming from Reverb to the memory. This is	Reverb	Reverb
Input module to prepro- cess Reverb facts and the Generalization module	Reverb	Reverb
system need to answer, and Reverb facts that we use, in	Reverb	Reverb
Preprocessing Reverb facts In our experiments with	Reverb	Reverb
Reverb, each fact y = (s	Reverb	Reverb
the MemNNs by us- ing Reverb	Reverb	Reverb
is then used to connect Reverb facts to the Freebase-based memory	Reverb	Reverb
and the object of a Reverb fact to Freebase entities, we	Reverb	Reverb
one alias that matches the Reverb entity string. These two processes	Reverb	Reverb
lowed to match 17% of Reverb entities to Free- base ones	Reverb	Reverb
them to Freebase entities. All Reverb relation- ships were encoded using	Reverb	Reverb
are able to store each Reverb fact as a bag-of-symbols (words	Reverb	Reverb
be successfully used to query Reverb facts	Reverb	Reverb
scoring a fact y from Reverb, we use the same embeddings	Reverb	Reverb
WebQuestions SimpleQuestions Reverb TRAIN 3,000 75,910 – VALID	Reverb	Reverb
ours but are based on Reverb rather than Freebase, and relied	Reverb	Reverb
sets of WebQuestions, SimpleQuestions and Reverb which we used for evalua	Reverb	Reverb
The Reverb test set, based on the	Reverb	Reverb
WebQuestions SimpleQuestions Reverb F1-SCORE (%) ACCURACY (%) ACCURACY	Reverb	Reverb
MEMORY NETWORKS (never trained on Reverb – only transfer) KB TRAIN	Reverb	Reverb
Transfer learning on Reverb In this set of ex	Reverb	Reverb
- periments, all Reverb facts are added to the	Reverb	Reverb
our model without training on Reverb against meth- ods specifically developed	Reverb	Reverb
impact on the performance on Reverb	Reverb	Reverb
and are well formed, while Reverb questions have more syntactic and	Reverb	Reverb
while the largest existing benchmark, WebQuestions, con- tains less than 6k	WebQuestions	WebQuestions
lent results on the benchmark WebQuestions	WebQuestions	WebQuestions
QA task on Freebase. On WebQuestions, a benchmark not primarily designed	WebQuestions	WebQuestions
entities not appearing in either WebQuestions or SimpleQuestions. Statistics of FB2M	WebQuestions	WebQuestions
WebQuestions SimpleQuestions Reverb TRAIN 3,000 75,910	WebQuestions	WebQuestions
of the test sets of WebQuestions, SimpleQuestions and Reverb which we	WebQuestions	WebQuestions
used for evalua- tion. On WebQuestions, we evaluate against previous results	WebQuestions	WebQuestions
to maximize the F1-score on WebQuestions validation set, in- dependently of	WebQuestions	WebQuestions
4. On the main benchmark WebQuestions, our best re- sults use	WebQuestions	WebQuestions
WebQuestions SimpleQuestions Reverb F1-SCORE (%) ACCURACY	WebQuestions	WebQuestions
SIQ and PRP stand for WebQuestions, SimpleQuestions and Paraphrases respectively. More	WebQuestions	WebQuestions
their result (35.3% F1-score on WebQuestions) should be compared to our	WebQuestions	WebQuestions
to train on FB5M. On WebQuestions, not specifically designed as a	WebQuestions	WebQuestions
for the model trained on WebQuestions only), which shows that the	WebQuestions	WebQuestions
seem to help much on WebQuestions and SimpleQuestions, ex- cept when	WebQuestions	WebQuestions
on Reverb. This is because WebQuestions and SimpleQuestions questions follow simple	WebQuestions	WebQuestions
state-of-the-art on the popular benchmark WebQuestions	WebQuestions	WebQuestions
order of magnitude bigger than WebQuestions	WebQuestions	WebQuestions
We use WebQuestions [1] as our evaluation bemchmark	WebQuestions	WebQuestions
WebQuestions This dataset is built using	WebQuestions	WebQuestions
WebQuestions – Train. ex. 2,778	WebQuestions	WebQuestions
was appearing in either the WebQuestions training/validation set or in ClueWeb	WebQuestions	WebQuestions
1 WebQuestions contains ∼2k entities, hence restricting	WebQuestions	WebQuestions
WebQuestions what is henry clay known	WebQuestions	WebQuestions
Table 3. Results on the WebQuestions test set	WebQuestions	WebQuestions
have been selected on the WebQuestions valida	WebQuestions	WebQuestions
performance on the competitive benchmark WebQuestions	WebQuestions	WebQuestions
answers for questions from the WebQuestions test set, among the whole	WebQuestions	WebQuestions
5.3 Evaluation on WebQuestions	WebQuestions	WebQuestions
We chose the data set WebQuestions [3], which consists of natural	WebQuestions	WebQuestions
from the test set of WebQuestions and obtained 1,538 questions labeled	WebQuestions	WebQuestions
is that most questions of WebQuestions, such as Who was vice-president	WebQuestions	WebQuestions
5.3 Evaluation on WebQuestions	WebQuestions	WebQuestions
based on a KB, called SimpleQuestions	SimpleQuestions	SimpleQuestions
extracted from the new dataset SimpleQuestions introduced in this paper. Actual	SimpleQuestions	SimpleQuestions
2.2 The SimpleQuestions dataset	SimpleQuestions	SimpleQuestions
task of simple QA called SimpleQuestions	SimpleQuestions	SimpleQuestions
We collected SimpleQuestions in two phases. The first	SimpleQuestions	SimpleQuestions
in addition to the new SimpleQuestions dataset described in Section 2	SimpleQuestions	SimpleQuestions
appearing in either WebQuestions or SimpleQuestions	SimpleQuestions	SimpleQuestions
Unlike for SimpleQuestions or the synthetic QA data	SimpleQuestions	SimpleQuestions
WebQuestions SimpleQuestions Reverb TRAIN 3,000 75,910	SimpleQuestions	SimpleQuestions
the test sets of WebQuestions, SimpleQuestions and Reverb which we used	SimpleQuestions	SimpleQuestions
previous result was published on SimpleQuestions, we only compare different versions	SimpleQuestions	SimpleQuestions
of MemNNs. SimpleQuestions questions are labeled with their	SimpleQuestions	SimpleQuestions
WebQuestions SimpleQuestions Reverb F1-SCORE (%) ACCURACY	SimpleQuestions	SimpleQuestions
and PRP stand for WebQuestions, SimpleQuestions and Paraphrases respectively. More details	SimpleQuestions	SimpleQuestions
model (41.2%). On the new SimpleQuestions dataset, the best models achieve	SimpleQuestions	SimpleQuestions
set for about 86% of SimpleQuestions questions. This shows that MemNNs	SimpleQuestions	SimpleQuestions
does not change performance on SimpleQuestions because it was based on	SimpleQuestions	SimpleQuestions
help much on WebQuestions and SimpleQuestions, ex- cept when training only	SimpleQuestions	SimpleQuestions
This is because WebQuestions and SimpleQuestions questions follow simple pat- terns	SimpleQuestions	SimpleQuestions
also introduced the new dataset SimpleQuestions, which, with 100k examples, is	SimpleQuestions	SimpleQuestions
2.2 The SimpleQuestions dataset	SimpleQuestions	SimpleQuestions
AMR 2.0 (76.3% F1 on LDC2017T10) and AMR 1.0 (70.2% F1	LDC2017T10	LDC2017T10
SMATCH scores: 76.3% F1 on LDC2017T10 and 70.2% F1 on LDC2014T12	LDC2017T10	LDC2017T10
all LDC subscribers): AMR 2.0 (LDC2017T10) and AMR 1.0 (LDC2014T12). Our	LDC2017T10	LDC2017T10
large-scale activity benchmarks, such as HICO [13], HICO- DET [12], AVA	HICO	HICO
existing well-designed datasets, for example, HICO	HICO	HICO
16], OpenImage [17], HCVRD [22], HICO [13], MPII [1], AVA [10	HICO	HICO
just report the results on HICO [13] to evaluate the im	HICO	HICO
HICO [13] contains 38,116 images in	HICO	HICO
Comparison with previous methods on HICO	HICO	HICO
means one-shot prob- lem. On HICO [13], there is obvious positive	HICO	HICO
over the state-of-the-art result on HICO [13]. And when the part	HICO	HICO
achieve surprising 62.5 mAP on HICO dataset. That is, if we	HICO	HICO
than 5 and 10) of HICO, our method can achieve more	HICO	HICO
in HOI recognition on the HICO dataset. We will make our	HICO	HICO
evaluate our model on the HICO dataset [5] and the MPII	HICO	HICO
10% relatively in mAP on HICO dataset	HICO	HICO
the predefined list. In the HICO dataset, there can be multiple	HICO	HICO
Handling Multiple Objects In HICO dataset, there can be multiple	HICO	HICO
two frequently used datasets, namely, HICO and MPII dataset. HICO dataset	HICO	HICO
and 5708 test images. Unlike HICO dataset, all person instances in	HICO	HICO
HICO We use Faster RCNN [34	HICO	HICO
the HOI labels in the HICO dataset are highly imbalanced, we	HICO	HICO
Similar to the setting for HICO dataset, we sample a maximum	HICO	HICO
with previous results on the HICO test set. The result of	HICO	HICO
two rows are results from HICO dataset and the last row	HICO	HICO
We compare our performance on HICO testing set in Table 1	HICO	HICO
model achieves 37.6 mAP on HICO testing set and 36.8 mAP	HICO	HICO
ther achieve 39.9 mAP on HICO testing set. Since [10] use	HICO	HICO
better performance than [10] on HICO and MPII dataset, and by	HICO	HICO
we conduct several experiments on HICO dataset and the results are	HICO	HICO
the baseline networks on the HICO test set. “union box” refers	HICO	HICO
body part attention model on HICO training set with different value	HICO	HICO
20 randomly picked HOIs in HICO dataset with and without the	HICO	HICO
randomly pick 20 categories in HICO dataset and compare our results	HICO	HICO
train deep networks on the HICO [16] and MPII [17] datasets	HICO	HICO
must be automatically detected in HICO	HICO	HICO
HICO Dataset	HICO	HICO
We train CNNs on the HICO and MPII datasets to predict	HICO	HICO
dataset and detected in the HICO dataset. We then use these	HICO	HICO
Humans Interacting with Common Objects (HICO) dataset [16]. The number of	HICO	HICO
fit for general VQA. The HICO dataset is currently the largest	HICO	HICO
categories. Each category in the HICO dataset is composed of a	HICO	HICO
CNNs with simple architectures on HICO and MPII datasets, and show	HICO	HICO
One limitation of the HICO dataset is that it provides	HICO	HICO
In the HICO dataset, if at least one	HICO	HICO
an unbalanced dataset. For the HICO dataset, we have to learn	HICO	HICO
two different activity classification datasets: HICO [16] and the MPII Human	HICO	HICO
Pose Dataset [17]. The HICO dataset contains labels for 600	HICO	HICO
are not provided with the HICO dataset, we run the Faster-RCNN	HICO	HICO
wrong or missing annotations. The HICO training set contains 38,116 images	HICO	HICO
for 393 actions. Unlike in HICO, each image only has a	HICO	HICO
has 5,709 images. Similar to HICO, the training set is unbalanced	HICO	HICO
of various networks on the HICO person-activity dataset. Note that usage	HICO	HICO
HICO Results. On the HICO dataset, we compare the networks	HICO	HICO
1 presents our comparison. As HICO is fairly new, the only	HICO	HICO
of training data than in HICO	HICO	HICO
best- performing network on the HICO dataset. In spite of the	HICO	HICO
of our system on the HICO dataset. Unusual use-cases of an	HICO	HICO
our Fusion-2 model on the HICO test set. Detected person instances	HICO	HICO
Fig. 4: Failure examples on HICO	HICO	HICO
HICO	HICO	HICO
68.74 72.06 77.25 54.10 59.77 HICO	HICO	HICO
HICO	HICO	HICO
HICO	HICO	HICO
full-image network trained on the HICO dataset, we obtain gains of	HICO	HICO
our Fusion-2 network trained on HICO (row 5) help improve the	HICO	HICO
class label predictions from both HICO and MPII (last row of	HICO	HICO
are complementary to those of HICO, especially in the cases when	HICO	HICO
picture’) is absent from the HICO and MPII datasets so the	HICO	HICO
single-frame models) and competitive on HICO	HICO	HICO
image based datasets such as HICO [7] and MPII [34] are	HICO	HICO
images and videos, namely MPII, HICO and HMDB51. MPII Human Pose	HICO	HICO
to equally weight all classes. HICO [7] is a recently introduced	HICO	HICO
HICO	HICO	HICO
We train our model on HICO similar to MPII, and compare	HICO	HICO
Multi-label HOI classification performance on HICO dataset. The top-half compares our	HICO	HICO
not help as much on HICO or MPII; HICO and MPII	HICO	HICO
results on the publicly available ECG5000 electrocardio- gram dataset show the	ECG	ECG5000
with our model in electrocardiogram (ECG) time series. Our contributions in	ECG	ECG5000
problem of finding sequences (e.g., ECG heartbeats) that do not conform	ECG	ECG5000
neural network for classification of ECG time series. Vig et al	ECG	ECG5000
networks for anomaly detection in ECG data. Malhotra et al. [15	ECG	ECG5000
the proposed model to electrocardiogram (ECG) time series data. The dataset	ECG	ECG5000
is the ECG5000, which was donated by Eamonn	ECG	ECG5000
II RESULTS OBTAINED ON THE ECG5000 DATASET	ECG	ECG5000
L. Vig, “Anomaly Detection in ECG Time Signals via Deep Long	ECG	ECG5000
results on the publicly available ECG5000 electrocardio- gram dataset show the	ECG5000	ECG5000
data. The dataset is the ECG5000, which was donated by Eamonn	ECG5000	ECG5000
II RESULTS OBTAINED ON THE ECG5000 DATASET	ECG5000	ECG5000
0.8099 1.0000* 0.8070 0.8070 CinC ECG 0.9949 0.8862 0.9094 0.9058 0.9058	ECG	ECG5000
ECG200 0.9200 0.9000 0.9200* 0.9100 0.9200	ECG	ECG5000
ECG5000 0.9482 0.9473 0.9478 0.9484 0.9496	ECG	ECG5000
0.9200 0.9000 0.9200* 0.9100 0.9200 ECG5000 0.9482 0.9473 0.9478 0.9484 0.9496	ECG5000	ECG5000
de- mand, space shuttle, and ECG, and two real- world engine	ECG	ECG5000
ECG Yes 1 Quasi-periodic 1 215	ECG	ECG5000
ECG 208 45 0.05 1.0 0.005	ECG	ECG5000
power demand, space shuttle valve, ECG, and engine (see Table 1	ECG	ECG5000
i) ECG-N (j) ECG	ECG	ECG5000
Engine-P and 61% for Engine-NP. ECG dataset contains quasi-periodic time-series (duration	ECG	ECG5000
also consider a quasi-periodic time-series (ECG	ECG	ECG5000
extracted from ECGs in the 2017 Atrial Fibrillation challenge [20]. The	2017	Physionet 2017 Atrial Fibrillation
network dynamics. Scientific reports, 7:44037, 2017	2017	Physionet 2017 Atrial Fibrillation
Overview and Comparative Analysis. Springer, 2017	2017	Physionet 2017 Atrial Fibrillation
389–409. Springer International Publishing, Cham, 2017	2017	Physionet 2017 Atrial Fibrillation
physionet computing in cardiology challenge 2017	2017	Physionet 2017 Atrial Fibrillation
. Computing in Cardiology, 2017	2017	Physionet 2017 Atrial Fibrillation
denoising autoencoders. arXiv preprint arXiv:1705.02737, 2017	2017	Physionet 2017 Atrial Fibrillation
Image Analysis, pages 419–430, Cham, 2017	2017	Physionet 2017 Atrial Fibrillation
series prediction. arXiv preprint arXiv:1704.02971, 2017	2017	Physionet 2017 Atrial Fibrillation
labeling tasks. arXiv preprint arXiv:1707.06799, 2017	2017	Physionet 2017 Atrial Fibrillation
Pattern Recognition, 63:397 – 405, 2017	2017	Physionet 2017 Atrial Fibrillation
The second medical dataset is Physionet, which contains time series of	Physionet	Physionet 2017 Atrial Fibrillation
223 2 20 20 [47] Physionet 2 8524 298 4 5	Physionet	Physionet 2017 Atrial Fibrillation
the real-world data from the Physionet dataset. By following a commonly	Physionet	Physionet 2017 Atrial Fibrillation
from ECGs in the 2017 Atrial Fibrillation challenge [20]. The MTS	Atrial	Physionet 2017 Atrial Fibrillation
ECGs in the 2017 Atrial Fibrillation challenge [20]. The MTS are	Fibrillation	Physionet 2017 Atrial Fibrillation
a new state-of-the-art in the BUCC shared task for 3 of	BUCC	BUCC French-to-English
ML- Doc dataset), bitext mining (BUCC dataset) and multilingual similarity search	BUCC	BUCC French-to-English
in different tasks. Some tasks (BUCC, MLDoc) tend to perform better	BUCC	BUCC French-to-English
4.2), and bi- text mining (BUCC, Section 4.3). However, all these	BUCC	BUCC French-to-English
5: F1 scores on the BUCC mining task	BUCC	BUCC French-to-English
4.3 BUCC	BUCC	BUCC French-to-English
our sentence embeddings on the BUCC mining task (Zweigen- baum et	BUCC	BUCC French-to-English
While XNLI, MLDoc and BUCC are well estab- lished benchmarks	BUCC	BUCC French-to-English
Depth Tatoeba BUCC MLDoc XNLI-en XNLI-xxErr [%] F1	BUCC	BUCC French-to-English
and Tatoeba. The effect in BUCC is negligible	BUCC	BUCC French-to-English
NLI Tatoeba BUCC MLDoc XNLI-en XNLI-xx obj. Err	BUCC	BUCC French-to-English
langs WMT BUCC MLDoc XNLI-en XNLI-xxErr [%] F1	BUCC	BUCC French-to-English
MLDoc), and bitext min- ing (BUCC	BUCC	BUCC French-to-English
Alignment of Comparable Sentences. In BUCC, pages 41–45	BUCC	BUCC French-to-English
with STACC Vari- ants. In BUCC	BUCC	BUCC French-to-English
BUCC18	BUCC	BUCC French-to-English
Using Multilingual Sentence Embeddings. In BUCC	BUCC	BUCC French-to-English
lel Sentence Identification Model. In BUCC	BUCC	BUCC French-to-English
Grégoire and Philippe Langlais. 2017. BUCC 2017 Shared Task: a First	BUCC	BUCC French-to-English
tences in Comparable Corpora. In BUCC, pages 46– 50	BUCC	BUCC French-to-English
in Chinese-English Comparable Corpora. In BUCC, pages 51–55	BUCC	BUCC French-to-English
2017. Overview of the Second BUCC Shared Task: Spotting Parallel Sentences	BUCC	BUCC French-to-English
in Comparable Corpora. In BUCC, pages 60–67	BUCC	BUCC French-to-English
2018. Overview of the Third BUCC Shared Task: Spotting Parallel Sentences	BUCC	BUCC French-to-English
in Comparable Corpora. In BUCC	BUCC	BUCC French-to-English
the resulting sentence embeddings using English annotated data only, and transfer	English	BUCC French-to-English
model from one language (e.g. English) to another, and the possi	English	BUCC French-to-English
which usu- ally consider separate English	English	BUCC French-to-English
pair- wise joint embeddings with English and one for- eign language	English	BUCC French-to-English
3.10 4.30 1000 eng en English Germanic Latin 2.6M n/a n/a	English	BUCC French-to-English
two target languages. We choose English and Spanish for that purpose	English	BUCC French-to-English
with one alignment only, usually English	English	BUCC French-to-English
the well-established evaluation frameworks for English sentence representations (Conneau et al	English	BUCC French-to-English
multi- lingual sentence embedding using English train- ing data, and evaluate	English	BUCC French-to-English
a dataset similar to the English MultiNLI for several languages (Conneau	English	BUCC French-to-English
sentences have been translated from English into 14 languages by professional	English	BUCC French-to-English
different systems are to use English training data from MultiNLI, and	English	BUCC French-to-English
hyperparameters were optimized on the English XNLI development corpus, and then	English	BUCC French-to-English
i.e. training a classifier on English data and applying it to	English	BUCC French-to-English
lower than the one on English, including distant languages like Arabic	English	BUCC French-to-English
achieves excel- lent results on English, outperforming our system by 7.5	English	BUCC French-to-English
Translate test, one English NLI system: Conneau et al	English	BUCC French-to-English
by aligning it to the English one	English	BUCC French-to-English
translate the test data into English and apply the English NLI	English	BUCC French-to-English
classifier, or 2) translate the English training data and train a	English	BUCC French-to-English
multilingual encoder us- ing the English training data, optimizing hyper- parameters	English	BUCC French-to-English
on the English development set, and evaluating the	English	BUCC French-to-English
a com- parable corpus between English and four foreign languages: German	English	BUCC French-to-English
pairs with the exception of English	English	BUCC French-to-English
models covering 4 languages each (English	English	BUCC French-to-English
/French/Spanish/German and English	English	BUCC French-to-English
consists of up to 1,000 English	English	BUCC French-to-English
re- port the accuracy on English	English	BUCC French-to-English
to be helpful to learn English sentence embeddings (Subrama	English	BUCC French-to-English
a better performance on the English NLI test set, but this	English	BUCC French-to-English
similarity error rate. This covers English, Czech, French, German and Spanish	English	BUCC French-to-English
Parity on Automatic Chinese to English News Translation. arXiv:1803.05567	English	BUCC French-to-English
English Comparable Corpora. In BUCC, pages	English	BUCC French-to-English
A community supported collection of English sentences and translations into more	English	BUCC French-to-English
is an open collection of English sen- tences and high quality	English	BUCC French-to-English
to 1,000 aligned sentences with English	English	BUCC French-to-English
stressed that, in general, the English sentences are not the same	English	BUCC French-to-English
with less than 20%, respectively (English included). The languages with less	English	BUCC French-to-English
13.40 10.00 1000 ang Old English Germanic Latin none 58.96 65.67	English	BUCC French-to-English
We introduce an architecture to learn joint multilingual sentence representations	to	BUCC French-to-English
for 93 languages, belonging to more than 30 dif- ferent	to	BUCC French-to-English
parallel corpora. This enables us to learn a classifier on top	to	BUCC French-to-English
data only, and transfer it to any of the 93 languages	to	BUCC French-to-English
of deep learning has led to impressive progress in Natural Language	to	BUCC French-to-English
NLP), these techniques are known to be par- ticularly data hungry	to	BUCC French-to-English
scenarios. An increasingly popular approach to alleviate this issue is to	to	BUCC French-to-English
language and are thus unable to leverage information across different languages	to	BUCC French-to-English
are gen- eral with respect to two dimensions: the input lan	to	BUCC French-to-English
over many languages, the desire to per- form zero-shot transfer of	to	BUCC French-to-English
from one language (e.g. English) to another, and the possi- bility	to	BUCC French-to-English
to handle code-switching. We achieve this	to	BUCC French-to-English
languages, and is usually limited to a few (most often only	to	BUCC French-to-English
substantially improve on previous work to learn joint multilingual sentence represen	to	BUCC French-to-English
a shared space, in contrast to most other works which usu	to	BUCC French-to-English
train the entire system end-to-end to predict the surrounding sentences over	to	BUCC French-to-English
This was recently ex- tended to multitask learning, combining different training	to	BUCC French-to-English
becom- ing increasingly popular is to train word embed- dings independently	to	BUCC French-to-English
corpora, and then map them to a shared space based on	to	BUCC French-to-English
em- beddings are often used to build bag-of-word rep- resentations of	to	BUCC French-to-English
that we follow here is to use a sequence-to-sequence encoder- decoder	to	BUCC French-to-English
end-to-end on parallel corpora akin to neural ma- chine translation: the	to	BUCC French-to-English
is used by the decoder to create the target sequence. This	to	BUCC French-to-English
and the encoder is kept to embed sentences in any of	to	BUCC French-to-English
work is either lim- ited to few, rather close languages (Schwenk	to	BUCC French-to-English
number of languages is limited to word embeddings (Ammar et al	to	BUCC French-to-English
1: Architecture of our system to learn multilingual sentence embeddings	to	BUCC French-to-English
language agnostic BiLSTM en- coder to build our sentence embeddings, which	to	BUCC French-to-English
parallel corpora. From Section 3.1 to 3.3, we describe its architecture	to	BUCC French-to-English
, our training strategy to scale to up to 93	to	BUCC French-to-English
to	to	BUCC French-to-English
morphologically complex language might correspond to several words of a morphologically	to	BUCC French-to-English
sentence em- beddings are used to initialize the decoder LSTM through	to	BUCC French-to-English
and are also con- catenated to its input embeddings at every	to	BUCC French-to-English
information of the input sequence to be captured by the sentence	to	BUCC French-to-English
input language is, encouraging it to learn language independent representations. In	to	BUCC French-to-English
ding that specifies the language to generate, which is concatenated to	to	BUCC French-to-English
Scaling up to almost hundred languages, which use	to	BUCC French-to-English
paper, we limit our study to a stacked BiLSTM with 1	to	BUCC French-to-English
to 5 layers, each 512-dimensional. The	to	BUCC French-to-English
embed- ding size is set to 320, while the language ID	to	BUCC French-to-English
While this approach was shown to learn high-quality representations, it poses	to	BUCC French-to-English
two obvious drawbacks when trying to scale to a large number	to	BUCC French-to-English
out of 93 languages used to trained the proposed model with	to	BUCC French-to-English
cor- pus, which is difficult to obtain for all languages. Second	to	BUCC French-to-English
a quadratic cost with respect to the number of languages, making	to	BUCC French-to-English
target languages – two seem to be enough.2 At the same	to	BUCC French-to-English
not require each source sentence to be translated into the two	to	BUCC French-to-English
of 0.001 and dropout set to 0.1, and train for a	to	BUCC French-to-English
use of its multi-GPU support to train on 16 NVIDIA V100	to	BUCC French-to-English
target language, the only way to train the encoder for that	to	BUCC French-to-English
auto- encoding, which we observe to work poorly. Having two tar	to	BUCC French-to-English
Some tasks (BUCC, MLDoc) tend to perform better when the encoder	to	BUCC French-to-English
informal sentences. In an attempt to achieve a general purpose sentence	to	BUCC French-to-English
yet a commonly accepted standard to eval- uate multilingual sentence embeddings	to	BUCC French-to-English
an NLI test set similar to MultiNLI (Williams et al., 2017	to	BUCC French-to-English
languages (Section 4.1). So as to obtain a more complete picture	to	BUCC French-to-English
become a widely used task to evaluate sentence representations (Bowman et	to	BUCC French-to-English
XNLI is a recent effort to create a dataset similar to	to	BUCC French-to-English
provided; instead, different systems are to use English training data from	to	BUCC French-to-English
the same classifier was applied to all languages of the XNLI	to	BUCC French-to-English
English data and applying it to all other lan- guages) for	to	BUCC French-to-English
Conneau et al. (2018c) correspond to max-pooling, which outperforms the last-state	to	BUCC French-to-English
points for our system, compared to 19.3 and 17.6 points for	to	BUCC French-to-English
lan- guage by aligning it to the English one	to	BUCC French-to-English
is worth mentioning that, thanks to its multilingual nature, our system	to	BUCC French-to-English
multilingual representations. In or- der to evaluate our sentence embeddings in	to	BUCC French-to-English
on Japanese can be attributed to the domain and sentence length	to	BUCC French-to-English
notion of margin is related to CSLS as proposed in Conneau	to	BUCC French-to-English
2018a). The reader is referred to Artetxe and Schwenk (2018) for	to	BUCC French-to-English
We use this method to evaluate our sentence embeddings on	to	BUCC French-to-English
Schwenk (2018). The goal is to extract parallel sentences from a	to	BUCC French-to-English
The dataset consists of 150K to 1.2M sentences for each language	to	BUCC French-to-English
four languages increased from 93.27 to 93.92. Not only are our	to	BUCC French-to-English
it can potentially be used to mine bi- text for any	to	BUCC French-to-English
guages. So as to better assess the performance of	to	BUCC French-to-English
The dataset consists of up to 1,000 English-aligned sentence pairs for	to	BUCC French-to-English
in the other language according to cosine similarity and computing the	to	BUCC French-to-English
in Section 5.3. In relation to that, Appendix E reports similarity	to	BUCC French-to-English
our encoder can also generalize to unseen languages to some extent	to	BUCC French-to-English
layers. We were not able to achieve good convergence with deeper	to	BUCC French-to-English
STM has not enough capacity to encode so many languages	to	BUCC French-to-English
Multitask learning has been shown to be helpful to learn English	to	BUCC French-to-English
adding an additional NLI objective to our system with different weighting	to	BUCC French-to-English
7, the NLI objective leads to a better performance on the	to	BUCC French-to-English
So as to better understand how our architecture	to	BUCC French-to-English
scales to a large amount of languages	to	BUCC French-to-English
lan- guages, and compare it to our main model trained on	to	BUCC French-to-English
the WMT 2014 test set to evaluate the multi- lingual similarity	to	BUCC French-to-English
the joint training also yields to overall better rep- resentations	to	BUCC French-to-English
paper, we propose an architecture to learn multilingual sentence embeddings for	to	BUCC French-to-English
available parallel corpora and applied to different downstream tasks without any	to	BUCC French-to-English
the future, we would like to explore alterna- tive architectures for	to	BUCC French-to-English
encoder. In particular, we plan to replace our BiLSTM with the	to	BUCC French-to-English
former, which has been shown to work better in different settings	to	BUCC French-to-English
2018). Moreover, we would like to explore possible strategies to exploit	to	BUCC French-to-English
train- ing data in addition to parallel corpora, such as using	to	BUCC French-to-English
2018). Finally, we would like to replace our language- specific tokenization	to	BUCC French-to-English
a language agnostic approach similar to Sentence- Piece.11	to	BUCC French-to-English
Human Parity on Automatic Chinese to English News Translation. arXiv:1803.05567	to	BUCC French-to-English
The size varies from 400k to 2M sentence pairs, in function	to	BUCC French-to-English
e.g. Ar- menian or Kazakh) to more than 50 million (e.g	to	BUCC French-to-English
We use this cor- pus to extract a separate test set	to	BUCC French-to-English
of up to 1,000 sentences for many languages	to	BUCC French-to-English
only kept 93 different languages to train the multilingual sentence embeddings	to	BUCC French-to-English
numbers in the diagonal correspond to the main results reported in	to	BUCC French-to-English
observe that our approach seems to han- dle the combination of	to	BUCC French-to-English
created test sets of up to 1,000 aligned sentences with English	to	BUCC French-to-English
iting the number of sentences to 500, we increase the coverage	to	BUCC French-to-English
to 101 languages, and even 141	to	BUCC French-to-English
less than 20% error belong to 20 different families and use	to	BUCC French-to-English
different scripts. It is nice to find six languages in this	to	BUCC French-to-English
less than 100 thousand sentences to train them. This is a	to	BUCC French-to-English
that we would be able to train a good sentence embedding	to	BUCC French-to-English
Georgian. This makes it difficult to bene- fit from joint training	to	BUCC French-to-English
high error rates. We hope to improve their performance in the	to	BUCC French-to-English
We extend our Tatoeba experiments to 29 lan- guages without any	to	BUCC French-to-English
This en- ables our encoder to perform reasonably well. We can	to	BUCC French-to-English
results in the diagonal correspond to the accuracies reported in Table	to	BUCC French-to-English
best published results on the BUCC mining task and the UN	BUCC	BUCC French-to-English
two French sentences on the BUCC training set along with their	BUCC	BUCC French-to-English
present our results on the BUCC min- ing task, UN corpus	BUCC	BUCC French-to-English
To cover all languages in BUCC, we use a separate En	BUCC	BUCC French-to-English
4.1 BUCC mining task The shared task	BUCC	BUCC French-to-English
Building and Using Comparable Corpora (BUCC) is a well- established evaluation	BUCC	BUCC French-to-English
Table 2: BUCC results (precision, recall and F1	BUCC	BUCC French-to-English
by the organizers of the BUCC workshop. We have done one	BUCC	BUCC French-to-English
Table 3: BUCC results (F1) on the test	BUCC	BUCC French-to-English
best published results on the BUCC mining task, out- performing previous	BUCC	BUCC French-to-English
Alignment of Comparable Sentences. In BUCC, pages 41–45	BUCC	BUCC French-to-English
with STACC Vari- ants. In BUCC	BUCC	BUCC French-to-English
BUCC18	BUCC	BUCC French-to-English
Using Multilingual Sentence Embeddings. In BUCC	BUCC	BUCC French-to-English
Grégoire and Philippe Langlais. 2017. BUCC 2017 Shared Task: a First	BUCC	BUCC French-to-English
tences in Comparable Corpora. In BUCC, pages 46– 50	BUCC	BUCC French-to-English
2017. Overview of the Second BUCC Shared Task: Spotting Parallel Sentences	BUCC	BUCC French-to-English
in Comparable Corpora. In BUCC, pages 60–67	BUCC	BUCC French-to-English
2018. Overview of the Third BUCC Shared Task: Spotting Parallel Sentences	BUCC	BUCC French-to-English
in Comparable Corpora. In BUCC	BUCC	BUCC French-to-English
sion points, respectively. Filtering the English	English	BUCC French-to-English
All ex- periments use an English	English	BUCC French-to-English
mine for parallel sentences between English and four foreign languages: German	English	BUCC French-to-English
NMT Finally, we filter the English	English	BUCC French-to-English
Figure 2: English	English	BUCC French-to-English
Table 5: Results on English	English	BUCC French-to-English
Table 6: Results on English	English	BUCC French-to-English
obtain 31.2 BLEU points for English	English	BUCC French-to-English
Parity on Automatic Chinese to English News Translation. arXiv:1803.05567	English	BUCC French-to-English
English News Arti- cles and Sentences	English	BUCC French-to-English
Machine translation is highly sensitive to the size and quality of	to	BUCC French-to-English
training data, which has led to an increasing interest in collect	to	BUCC French-to-English
sentence embed- dings. In contrast to previous approaches, which rely on	to	BUCC French-to-English
standard benchmarks, it is known to be particularly sen- sitive to	to	BUCC French-to-English
this context, effective ap- proaches to mine and filter parallel corpora	to	BUCC French-to-English
are crucial to apply NMT in practical settings	to	BUCC French-to-English
over bag-of-word features to distinguish between ground truth translations	to	BUCC French-to-English
combined with set expansion operations to score translation candidates through the	to	BUCC French-to-English
use an NMT inspired encoder-decoder to train sentence embeddings on existing	to	BUCC French-to-English
are then directly ap- plied to retrieve and filter new parallel	to	BUCC French-to-English
2018), who learn an encoder to score known translation pairs above	to	BUCC French-to-English
and train a separate model to dynamically scale and shift the	to	BUCC French-to-English
of a larger system, either to obtain an initial alignment that	to	BUCC French-to-English
to	to	BUCC French-to-English
shell. 0.498 While the risk to those working in ceramics is	to	BUCC French-to-English
discovered they are not free to speak their minds	to	BUCC French-to-English
1 shows our encoder-decoder architecture to learn multilingual sentence embeddings, which	to	BUCC French-to-English
ways: 1) they are used to initialize its hidden and cell	to	BUCC French-to-English
2) they are concate- nated to the input embeddings at every	to	BUCC French-to-English
of 0.001 and dropout set to 0.1. We use a sin	to	BUCC French-to-English
input embeddings size is set to 512, while the lan	to	BUCC French-to-English
2Prior to BPE segmentation, we tokenize and	to	BUCC French-to-English
and the encoder is used to map a sentence to a	to	BUCC French-to-English
multilingual encoder can be used to mine par- allel sentences by	to	BUCC French-to-English
in the target side according to cosine similarity, and filtering those	to	BUCC French-to-English
this approach has been reported to be competitive (Schwenk, 2018), we	to	BUCC French-to-English
one, thus making it impossible to filter it through a fixed	to	BUCC French-to-English
range, it is still susceptible to concentrate around dif- ferent values	to	BUCC French-to-English
do, filtering them is unlikely to cause any major harm	to	BUCC French-to-English
1: Architecture of our system to learn multilingual sentence embeddings	to	BUCC French-to-English
the average. This is equivalent to cosine similar- ity and thus	to	BUCC French-to-English
given candidate. This is proportional to the CSLS score (Conneau et	to	BUCC French-to-English
2018), which was originally motivated to mitigate the hub- ness problem	to	BUCC French-to-English
we explore the following strategies to generate candidates	to	BUCC French-to-English
causes the hubness problem. Thanks to its bidirec- tional nature, our	to	BUCC French-to-English
Backward: Equivalent to the forward strat- egy, but	to	BUCC French-to-English
candidates are then sorted according to their margin scores, and a	to	BUCC French-to-English
the development data, or adjusted to obtain the desired corpus size	to	BUCC French-to-English
on the training set, used to optimize the filtering threshold	to	BUCC French-to-English
to mine for parallel sentences between	to	BUCC French-to-English
and Chinese. There are 150K to 1.2M sentences for each language	to	BUCC French-to-English
UN model in compar- ison to previous work.9 Our proposed system	to	BUCC French-to-English
standard information was exclusively used to optimize the filtering threshold for	to	BUCC French-to-English
filtering threshold was opti- mized to maximize the F1 score on	to	BUCC French-to-English
UN corpus reconstruction So as to compare our method to the	to	BUCC French-to-English
we first pre- process it to remove all duplicated sentence pairs	to	BUCC French-to-English
de to	to	BUCC French-to-English
results (newstest2013) using different thresholds to filter ParaCrawl	to	BUCC French-to-English
corpus size from 4.59 billion to 64.4 million sentence pairs, mostly	to	BUCC French-to-English
due to dedu- plication. We then score	to	BUCC French-to-English
the top scoring entries up to the desired size. Figure 2	to	BUCC French-to-English
Table 6 compares our results to previ- ous works in the	to	BUCC French-to-English
train- ing data. In addition to our ParaCrawl system, we include	to	BUCC French-to-English
on English-German newstest2014 in comparison to previous work. wmt for WMT	to	BUCC French-to-English
our improvement can be attributed to a better fil- tering of	to	BUCC French-to-English
use a sequence-to-sequence ar- chitecture to train a multilingual sentence encoder	to	BUCC French-to-English
tion, our method obtains up to 85% precision at reconstructing the	to	BUCC French-to-English
our improvements also carry over to downstream machine transla- tion, as	to	BUCC French-to-English
the difference cannot be attributed to implementation details	to	BUCC French-to-English
Use of Comparable Corpora to Improve SMT per- formance. In	to	BUCC French-to-English
N Dauphin. 2017. Convolutional Sequence to Sequence Learning. In ICML, pages	to	BUCC French-to-English
Human Parity on Automatic Chinese to English News Translation. arXiv:1803.05567	to	BUCC French-to-English
on the WMT 15 task English	English	BUCC French-to-English
English (+2.1–3.4 BLEU), ob- taining new	English	BUCC French-to-English
for the IWSLT 15 task English	English	BUCC French-to-English
with additional monolingual data, on English	English	BUCC French-to-English
English, using training and test data	English	BUCC French-to-English
from WMT 15 for English	English	BUCC French-to-English
↔German, IWSLT 15 for English	English	BUCC French-to-English
English	English	BUCC French-to-English
For English	English	BUCC French-to-English
English, we report case-sensitive BLEU on	English	BUCC French-to-English
Table 1: English	English	BUCC French-to-English
4.1.1 English	English	BUCC French-to-English
monolin- gual data set into English	English	BUCC French-to-English
English system used for this is	English	BUCC French-to-English
English, we back-translate 4 200 000	English	BUCC French-to-English
monolingual English sentences into German, us- ing	English	BUCC French-to-English
the English	English	BUCC French-to-English
English training data	English	BUCC French-to-English
English	English	BUCC French-to-English
lower-resourced trans- lation setting than English	English	BUCC French-to-English
For both Turkish and English, we represent rare words (or	English	BUCC French-to-English
from Gigaword. We use an English	English	BUCC French-to-English
English baseline system	English	BUCC French-to-English
problem than with the larger English	English	BUCC French-to-English
English, we use gradient clipping with	English	BUCC French-to-English
1 that we use for English	English	BUCC French-to-English
4.2.1 English	English	BUCC French-to-English
Table 3 shows English	English	BUCC French-to-English
4.2.2 English	English	BUCC French-to-English
Table 4 shows English	English	BUCC French-to-English
Table 3: English	English	BUCC French-to-English
Table 4: English	English	BUCC French-to-English
English system trained on WMT data	English	BUCC French-to-English
English translation perfor- mance (BLEU) on	English	BUCC French-to-English
English WMT 15	English	BUCC French-to-English
English on the WMT 15 data	English	BUCC French-to-English
English IWSLT 14	English	BUCC French-to-English
English	English	BUCC French-to-English
English systems	English	BUCC French-to-English
Table 7: English	English	BUCC French-to-English
English system that was itself trained	English	BUCC French-to-English
English sys- tems, and of the	English	BUCC French-to-English
resulting English	English	BUCC French-to-English
English back-translation differs substantially, with a	English	BUCC French-to-English
on new- stest2015. Regarding the English	English	BUCC French-to-English
English translation performance (tokenized BLEU) on	English	BUCC French-to-English
Table 8: Phrase-based SMT results (English	English	BUCC French-to-English
English training and develop- ment set	English	BUCC French-to-English
Figure 2: English	English	BUCC French-to-English
English models. For comparability, we measure	English	BUCC French-to-English
Figure 2 shows cross-entropy for English	English	BUCC French-to-English
training data is available for English	English	BUCC French-to-English
natural according to native speaker. English	English	BUCC French-to-English
natural words. For instance, the English	English	BUCC French-to-English
→German sys- tems translate the English phrase civil rights pro- tections	English	BUCC French-to-English
data for NMT. In contrast to previous work, which combines NMT	to	BUCC French-to-English
architectures already have the capacity to learn the same information as	to	BUCC French-to-English
model, and we explore strategies to train with monolin- gual data	to	BUCC French-to-English
data, or data more similar to the translation task	to	BUCC French-to-English
rationale, adding a language model to compensate for the independence assumptions	to	BUCC French-to-English
and we expect monolingual data to be especially helpful if parallel	to	BUCC French-to-English
In contrast to previous work, which integrates a	to	BUCC French-to-English
al., 2015), we explore strategies to include monolingual training data in	to	BUCC French-to-English
This makes our approach applicable to different NMT architectures	to	BUCC French-to-English
we investigate two different methods to fill the source side of	to	BUCC French-to-English
we successfully adapt NMT models to a new domain by fine-tuning	to	BUCC French-to-English
our approach is not specific to this architecture	to	BUCC French-to-English
h j are concatenated to obtain the annotation vector hj	to	BUCC French-to-English
probability that yi is aligned to xj . The alignment model	to	BUCC French-to-English
or monolingual data more similar to the test set	to	BUCC French-to-English
serves to improve the estimate of the	to	BUCC French-to-English
into account. In con- trast to (Gülçehre et al., 2015), who	to	BUCC French-to-English
deep fusion, we propose techniques to train the main NMT model	to	BUCC French-to-English
words. We describe two strategies to do this: providing monolingual training	to	BUCC French-to-English
language, which we will refer to as back-translation	to	BUCC French-to-English
first technique we employ is to treat mono- lingual training examples	to	BUCC French-to-English
for which the network has to fully rely on the previous	to	BUCC French-to-English
to	to	BUCC French-to-English
single-word dummy source side <null> to allow processing of both parallel	to	BUCC French-to-English
force the context vector ci to be 0 for monolin- gual	to	BUCC French-to-English
sets of 20 minibatches according to length. This also groups monolin	to	BUCC French-to-English
the output layer remains sensitive to the source context, and that	to	BUCC French-to-English
from monolingual data, we propose to pair monolingual training instances with	to	BUCC French-to-English
text with mteval-v13a.pl for comparison to official WMT and IWSLT results	to	BUCC French-to-English
text with multi-bleu.perl for comparison to results by Gülçehre et al	to	BUCC French-to-English
not ensembles. We leave it to fu- ture work to explore	to	BUCC French-to-English
training with synthetic data is to the quality of the back	to	BUCC French-to-English
15 test sets to investigate a cross-domain setting.5	to	BUCC French-to-English
2007), and removal of non-surface to	to	BUCC French-to-English
We found overfitting to be a bigger problem than	to	BUCC French-to-English
et al. (2015), in contrast to the threshold 1 that we	to	BUCC French-to-English
as long as the baseline to provide the training al- gorithm	to	BUCC French-to-English
the quality improvement is due to the monolingual training instances, and	to	BUCC French-to-English
best single system achieves a to	to	BUCC French-to-English
- kenized BLEU (as opposed to untokenized scores reported in Table	to	BUCC French-to-English
of training instances varies due to differences in training time and	to	BUCC French-to-English
if it can be used to adapt a model to a	to	BUCC French-to-English
system trained on WMT data to translating TED talks	to	BUCC French-to-English
Systems 1 and 2 correspond to systems in Table 3, trained	to	BUCC French-to-English
system trained on WMT data to the TED do- main. By	to	BUCC French-to-English
BLEU on average. To compare to what extent syn- thetic data	to	BUCC French-to-English
of the parallel training text to obtain the training corpus parallelsynth	to	BUCC French-to-English
despite being out-of-domain in relation to the test sets. We speculate	to	BUCC French-to-English
monolin- gual data would lead to even higher improvements	to	BUCC French-to-English
monolingual data, but this led to decreased BLEU scores	to	BUCC French-to-English
difference in back-translation quality leads to a 0.6–0.7 BLEU difference in	to	BUCC French-to-English
size, and we leave it to future research to explore how	to	BUCC French-to-English
of each training run). Thanks to the increased diversity of the	to	BUCC French-to-English
4.3 Contrast to Phrase-based SMT	to	BUCC French-to-English
data into the source language to produce synthetic parallel text has	to	BUCC French-to-English
of training instances varies due to early stopping	to	BUCC French-to-English
In contrast to phrase-based SMT, which can make	to	BUCC French-to-English
so far not been able to use mono- lingual data to	to	BUCC French-to-English
parallel data is not limited to domain adaptation, and that even	to	BUCC French-to-English
hypothesis that domain adaptation contributes to the effectiveness of adding synthetic	to	BUCC French-to-English
data to NMT training	to	BUCC French-to-English
in data, or natural according to native speaker. English→German; newstest2015; ensemble	to	BUCC French-to-English
ency, its ability to produce natural target-language sentences. As	to	BUCC French-to-English
a proxy to sentence-level flu- ency, we investigate	to	BUCC French-to-English
of each sys- tem according to their naturalness13 , distinguish- ing	to	BUCC French-to-English
state of the language model to the de- coder state of	to	BUCC French-to-English
synthetic parallel texts bears resemblance to data augmentation techniques used in	to	BUCC French-to-English
is that self-training typically refers to scenario where the training set	to	BUCC French-to-English
continued training has been shown to be effective for neural language	to	BUCC French-to-English
and in work par- allel to ours, for neural translation models	to	BUCC French-to-English
2015). We are the first to show that we can effectively	to	BUCC French-to-English
we propose two simple methods to use monolingual training data during	to	BUCC French-to-English
NMT systems, with no changes to the network	to	BUCC French-to-English
dummy source context was successful to some ex- tent, but we	to	BUCC French-to-English
the neural net- work architecture to integrate monolingual train- ing data	to	BUCC French-to-English
approach can be easily applied to other NMT systems. We expect	to	BUCC French-to-English
on the amount (and similarity to the test set) of available	to	BUCC French-to-English
Machine Translation by Jointly Learning to Align and Trans- late. In	to	BUCC French-to-English
Roossin. 1990. A Statistical Approach to Machine Translation. Computational Linguistics, 16(2):79–85	to	BUCC French-to-English
Manning. 2015. Effective Ap- proaches to Attention-based Neural Machine Trans- lation	to	BUCC French-to-English
Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Networks	to	BUCC French-to-English
4.3 Contrast to Phrase-based SMT	to	BUCC French-to-English
a new state-of-the-art in the BUCC shared task for 3 of	BUCC	BUCC German-to-English
ML- Doc dataset), bitext mining (BUCC dataset) and multilingual similarity search	BUCC	BUCC German-to-English
in different tasks. Some tasks (BUCC, MLDoc) tend to perform better	BUCC	BUCC German-to-English
4.2), and bi- text mining (BUCC, Section 4.3). However, all these	BUCC	BUCC German-to-English
5: F1 scores on the BUCC mining task	BUCC	BUCC German-to-English
4.3 BUCC	BUCC	BUCC German-to-English
our sentence embeddings on the BUCC mining task (Zweigen- baum et	BUCC	BUCC German-to-English
While XNLI, MLDoc and BUCC are well estab- lished benchmarks	BUCC	BUCC German-to-English
Depth Tatoeba BUCC MLDoc XNLI-en XNLI-xxErr [%] F1	BUCC	BUCC German-to-English
and Tatoeba. The effect in BUCC is negligible	BUCC	BUCC German-to-English
NLI Tatoeba BUCC MLDoc XNLI-en XNLI-xx obj. Err	BUCC	BUCC German-to-English
langs WMT BUCC MLDoc XNLI-en XNLI-xxErr [%] F1	BUCC	BUCC German-to-English
MLDoc), and bitext min- ing (BUCC	BUCC	BUCC German-to-English
Alignment of Comparable Sentences. In BUCC, pages 41–45	BUCC	BUCC German-to-English
with STACC Vari- ants. In BUCC	BUCC	BUCC German-to-English
BUCC18	BUCC	BUCC German-to-English
Using Multilingual Sentence Embeddings. In BUCC	BUCC	BUCC German-to-English
lel Sentence Identification Model. In BUCC	BUCC	BUCC German-to-English
Grégoire and Philippe Langlais. 2017. BUCC 2017 Shared Task: a First	BUCC	BUCC German-to-English
tences in Comparable Corpora. In BUCC, pages 46– 50	BUCC	BUCC German-to-English
in Chinese-English Comparable Corpora. In BUCC, pages 51–55	BUCC	BUCC German-to-English
2017. Overview of the Second BUCC Shared Task: Spotting Parallel Sentences	BUCC	BUCC German-to-English
in Comparable Corpora. In BUCC, pages 60–67	BUCC	BUCC German-to-English
2018. Overview of the Third BUCC Shared Task: Spotting Parallel Sentences	BUCC	BUCC German-to-English
in Comparable Corpora. In BUCC	BUCC	BUCC German-to-English
the resulting sentence embeddings using English annotated data only, and transfer	English	BUCC German-to-English
model from one language (e.g. English) to another, and the possi	English	BUCC German-to-English
which usu- ally consider separate English	English	BUCC German-to-English
pair- wise joint embeddings with English and one for- eign language	English	BUCC German-to-English
3.10 4.30 1000 eng en English Germanic Latin 2.6M n/a n/a	English	BUCC German-to-English
two target languages. We choose English and Spanish for that purpose	English	BUCC German-to-English
with one alignment only, usually English	English	BUCC German-to-English
the well-established evaluation frameworks for English sentence representations (Conneau et al	English	BUCC German-to-English
multi- lingual sentence embedding using English train- ing data, and evaluate	English	BUCC German-to-English
a dataset similar to the English MultiNLI for several languages (Conneau	English	BUCC German-to-English
sentences have been translated from English into 14 languages by professional	English	BUCC German-to-English
different systems are to use English training data from MultiNLI, and	English	BUCC German-to-English
hyperparameters were optimized on the English XNLI development corpus, and then	English	BUCC German-to-English
i.e. training a classifier on English data and applying it to	English	BUCC German-to-English
lower than the one on English, including distant languages like Arabic	English	BUCC German-to-English
achieves excel- lent results on English, outperforming our system by 7.5	English	BUCC German-to-English
Translate test, one English NLI system: Conneau et al	English	BUCC German-to-English
by aligning it to the English one	English	BUCC German-to-English
translate the test data into English and apply the English NLI	English	BUCC German-to-English
classifier, or 2) translate the English training data and train a	English	BUCC German-to-English
multilingual encoder us- ing the English training data, optimizing hyper- parameters	English	BUCC German-to-English
on the English development set, and evaluating the	English	BUCC German-to-English
a com- parable corpus between English and four foreign languages: German	English	BUCC German-to-English
pairs with the exception of English	English	BUCC German-to-English
models covering 4 languages each (English	English	BUCC German-to-English
/French/Spanish/German and English	English	BUCC German-to-English
consists of up to 1,000 English	English	BUCC German-to-English
re- port the accuracy on English	English	BUCC German-to-English
to be helpful to learn English sentence embeddings (Subrama	English	BUCC German-to-English
a better performance on the English NLI test set, but this	English	BUCC German-to-English
similarity error rate. This covers English, Czech, French, German and Spanish	English	BUCC German-to-English
Parity on Automatic Chinese to English News Translation. arXiv:1803.05567	English	BUCC German-to-English
English Comparable Corpora. In BUCC, pages	English	BUCC German-to-English
A community supported collection of English sentences and translations into more	English	BUCC German-to-English
is an open collection of English sen- tences and high quality	English	BUCC German-to-English
to 1,000 aligned sentences with English	English	BUCC German-to-English
stressed that, in general, the English sentences are not the same	English	BUCC German-to-English
with less than 20%, respectively (English included). The languages with less	English	BUCC German-to-English
13.40 10.00 1000 ang Old English Germanic Latin none 58.96 65.67	English	BUCC German-to-English
We introduce an architecture to learn joint multilingual sentence representations	to	BUCC German-to-English
for 93 languages, belonging to more than 30 dif- ferent	to	BUCC German-to-English
parallel corpora. This enables us to learn a classifier on top	to	BUCC German-to-English
data only, and transfer it to any of the 93 languages	to	BUCC German-to-English
of deep learning has led to impressive progress in Natural Language	to	BUCC German-to-English
NLP), these techniques are known to be par- ticularly data hungry	to	BUCC German-to-English
scenarios. An increasingly popular approach to alleviate this issue is to	to	BUCC German-to-English
language and are thus unable to leverage information across different languages	to	BUCC German-to-English
are gen- eral with respect to two dimensions: the input lan	to	BUCC German-to-English
over many languages, the desire to per- form zero-shot transfer of	to	BUCC German-to-English
from one language (e.g. English) to another, and the possi- bility	to	BUCC German-to-English
to handle code-switching. We achieve this	to	BUCC German-to-English
languages, and is usually limited to a few (most often only	to	BUCC German-to-English
substantially improve on previous work to learn joint multilingual sentence represen	to	BUCC German-to-English
a shared space, in contrast to most other works which usu	to	BUCC German-to-English
train the entire system end-to-end to predict the surrounding sentences over	to	BUCC German-to-English
This was recently ex- tended to multitask learning, combining different training	to	BUCC German-to-English
becom- ing increasingly popular is to train word embed- dings independently	to	BUCC German-to-English
corpora, and then map them to a shared space based on	to	BUCC German-to-English
em- beddings are often used to build bag-of-word rep- resentations of	to	BUCC German-to-English
that we follow here is to use a sequence-to-sequence encoder- decoder	to	BUCC German-to-English
end-to-end on parallel corpora akin to neural ma- chine translation: the	to	BUCC German-to-English
is used by the decoder to create the target sequence. This	to	BUCC German-to-English
and the encoder is kept to embed sentences in any of	to	BUCC German-to-English
work is either lim- ited to few, rather close languages (Schwenk	to	BUCC German-to-English
number of languages is limited to word embeddings (Ammar et al	to	BUCC German-to-English
1: Architecture of our system to learn multilingual sentence embeddings	to	BUCC German-to-English
language agnostic BiLSTM en- coder to build our sentence embeddings, which	to	BUCC German-to-English
parallel corpora. From Section 3.1 to 3.3, we describe its architecture	to	BUCC German-to-English
, our training strategy to scale to up to 93	to	BUCC German-to-English
to	to	BUCC German-to-English
morphologically complex language might correspond to several words of a morphologically	to	BUCC German-to-English
sentence em- beddings are used to initialize the decoder LSTM through	to	BUCC German-to-English
and are also con- catenated to its input embeddings at every	to	BUCC German-to-English
information of the input sequence to be captured by the sentence	to	BUCC German-to-English
input language is, encouraging it to learn language independent representations. In	to	BUCC German-to-English
ding that specifies the language to generate, which is concatenated to	to	BUCC German-to-English
Scaling up to almost hundred languages, which use	to	BUCC German-to-English
paper, we limit our study to a stacked BiLSTM with 1	to	BUCC German-to-English
to 5 layers, each 512-dimensional. The	to	BUCC German-to-English
embed- ding size is set to 320, while the language ID	to	BUCC German-to-English
While this approach was shown to learn high-quality representations, it poses	to	BUCC German-to-English
two obvious drawbacks when trying to scale to a large number	to	BUCC German-to-English
out of 93 languages used to trained the proposed model with	to	BUCC German-to-English
cor- pus, which is difficult to obtain for all languages. Second	to	BUCC German-to-English
a quadratic cost with respect to the number of languages, making	to	BUCC German-to-English
target languages – two seem to be enough.2 At the same	to	BUCC German-to-English
not require each source sentence to be translated into the two	to	BUCC German-to-English
of 0.001 and dropout set to 0.1, and train for a	to	BUCC German-to-English
use of its multi-GPU support to train on 16 NVIDIA V100	to	BUCC German-to-English
target language, the only way to train the encoder for that	to	BUCC German-to-English
auto- encoding, which we observe to work poorly. Having two tar	to	BUCC German-to-English
Some tasks (BUCC, MLDoc) tend to perform better when the encoder	to	BUCC German-to-English
informal sentences. In an attempt to achieve a general purpose sentence	to	BUCC German-to-English
yet a commonly accepted standard to eval- uate multilingual sentence embeddings	to	BUCC German-to-English
an NLI test set similar to MultiNLI (Williams et al., 2017	to	BUCC German-to-English
languages (Section 4.1). So as to obtain a more complete picture	to	BUCC German-to-English
become a widely used task to evaluate sentence representations (Bowman et	to	BUCC German-to-English
XNLI is a recent effort to create a dataset similar to	to	BUCC German-to-English
provided; instead, different systems are to use English training data from	to	BUCC German-to-English
the same classifier was applied to all languages of the XNLI	to	BUCC German-to-English
English data and applying it to all other lan- guages) for	to	BUCC German-to-English
Conneau et al. (2018c) correspond to max-pooling, which outperforms the last-state	to	BUCC German-to-English
points for our system, compared to 19.3 and 17.6 points for	to	BUCC German-to-English
lan- guage by aligning it to the English one	to	BUCC German-to-English
is worth mentioning that, thanks to its multilingual nature, our system	to	BUCC German-to-English
multilingual representations. In or- der to evaluate our sentence embeddings in	to	BUCC German-to-English
on Japanese can be attributed to the domain and sentence length	to	BUCC German-to-English
notion of margin is related to CSLS as proposed in Conneau	to	BUCC German-to-English
2018a). The reader is referred to Artetxe and Schwenk (2018) for	to	BUCC German-to-English
We use this method to evaluate our sentence embeddings on	to	BUCC German-to-English
Schwenk (2018). The goal is to extract parallel sentences from a	to	BUCC German-to-English
The dataset consists of 150K to 1.2M sentences for each language	to	BUCC German-to-English
four languages increased from 93.27 to 93.92. Not only are our	to	BUCC German-to-English
it can potentially be used to mine bi- text for any	to	BUCC German-to-English
guages. So as to better assess the performance of	to	BUCC German-to-English
The dataset consists of up to 1,000 English-aligned sentence pairs for	to	BUCC German-to-English
in the other language according to cosine similarity and computing the	to	BUCC German-to-English
in Section 5.3. In relation to that, Appendix E reports similarity	to	BUCC German-to-English
our encoder can also generalize to unseen languages to some extent	to	BUCC German-to-English
layers. We were not able to achieve good convergence with deeper	to	BUCC German-to-English
STM has not enough capacity to encode so many languages	to	BUCC German-to-English
Multitask learning has been shown to be helpful to learn English	to	BUCC German-to-English
adding an additional NLI objective to our system with different weighting	to	BUCC German-to-English
7, the NLI objective leads to a better performance on the	to	BUCC German-to-English
So as to better understand how our architecture	to	BUCC German-to-English
scales to a large amount of languages	to	BUCC German-to-English
lan- guages, and compare it to our main model trained on	to	BUCC German-to-English
the WMT 2014 test set to evaluate the multi- lingual similarity	to	BUCC German-to-English
the joint training also yields to overall better rep- resentations	to	BUCC German-to-English
paper, we propose an architecture to learn multilingual sentence embeddings for	to	BUCC German-to-English
available parallel corpora and applied to different downstream tasks without any	to	BUCC German-to-English
the future, we would like to explore alterna- tive architectures for	to	BUCC German-to-English
encoder. In particular, we plan to replace our BiLSTM with the	to	BUCC German-to-English
former, which has been shown to work better in different settings	to	BUCC German-to-English
2018). Moreover, we would like to explore possible strategies to exploit	to	BUCC German-to-English
train- ing data in addition to parallel corpora, such as using	to	BUCC German-to-English
2018). Finally, we would like to replace our language- specific tokenization	to	BUCC German-to-English
a language agnostic approach similar to Sentence- Piece.11	to	BUCC German-to-English
Human Parity on Automatic Chinese to English News Translation. arXiv:1803.05567	to	BUCC German-to-English
The size varies from 400k to 2M sentence pairs, in function	to	BUCC German-to-English
e.g. Ar- menian or Kazakh) to more than 50 million (e.g	to	BUCC German-to-English
We use this cor- pus to extract a separate test set	to	BUCC German-to-English
of up to 1,000 sentences for many languages	to	BUCC German-to-English
only kept 93 different languages to train the multilingual sentence embeddings	to	BUCC German-to-English
numbers in the diagonal correspond to the main results reported in	to	BUCC German-to-English
observe that our approach seems to han- dle the combination of	to	BUCC German-to-English
created test sets of up to 1,000 aligned sentences with English	to	BUCC German-to-English
iting the number of sentences to 500, we increase the coverage	to	BUCC German-to-English
to 101 languages, and even 141	to	BUCC German-to-English
less than 20% error belong to 20 different families and use	to	BUCC German-to-English
different scripts. It is nice to find six languages in this	to	BUCC German-to-English
less than 100 thousand sentences to train them. This is a	to	BUCC German-to-English
that we would be able to train a good sentence embedding	to	BUCC German-to-English
Georgian. This makes it difficult to bene- fit from joint training	to	BUCC German-to-English
high error rates. We hope to improve their performance in the	to	BUCC German-to-English
We extend our Tatoeba experiments to 29 lan- guages without any	to	BUCC German-to-English
This en- ables our encoder to perform reasonably well. We can	to	BUCC German-to-English
results in the diagonal correspond to the accuracies reported in Table	to	BUCC German-to-English
best published results on the BUCC mining task and the UN	BUCC	BUCC German-to-English
two French sentences on the BUCC training set along with their	BUCC	BUCC German-to-English
present our results on the BUCC min- ing task, UN corpus	BUCC	BUCC German-to-English
To cover all languages in BUCC, we use a separate En	BUCC	BUCC German-to-English
4.1 BUCC mining task The shared task	BUCC	BUCC German-to-English
Building and Using Comparable Corpora (BUCC) is a well- established evaluation	BUCC	BUCC German-to-English
Table 2: BUCC results (precision, recall and F1	BUCC	BUCC German-to-English
by the organizers of the BUCC workshop. We have done one	BUCC	BUCC German-to-English
Table 3: BUCC results (F1) on the test	BUCC	BUCC German-to-English
best published results on the BUCC mining task, out- performing previous	BUCC	BUCC German-to-English
Alignment of Comparable Sentences. In BUCC, pages 41–45	BUCC	BUCC German-to-English
with STACC Vari- ants. In BUCC	BUCC	BUCC German-to-English
BUCC18	BUCC	BUCC German-to-English
Using Multilingual Sentence Embeddings. In BUCC	BUCC	BUCC German-to-English
Grégoire and Philippe Langlais. 2017. BUCC 2017 Shared Task: a First	BUCC	BUCC German-to-English
tences in Comparable Corpora. In BUCC, pages 46– 50	BUCC	BUCC German-to-English
2017. Overview of the Second BUCC Shared Task: Spotting Parallel Sentences	BUCC	BUCC German-to-English
in Comparable Corpora. In BUCC, pages 60–67	BUCC	BUCC German-to-English
2018. Overview of the Third BUCC Shared Task: Spotting Parallel Sentences	BUCC	BUCC German-to-English
in Comparable Corpora. In BUCC	BUCC	BUCC German-to-English
sion points, respectively. Filtering the English	English	BUCC German-to-English
All ex- periments use an English	English	BUCC German-to-English
mine for parallel sentences between English and four foreign languages: German	English	BUCC German-to-English
NMT Finally, we filter the English	English	BUCC German-to-English
Figure 2: English	English	BUCC German-to-English
Table 5: Results on English	English	BUCC German-to-English
Table 6: Results on English	English	BUCC German-to-English
obtain 31.2 BLEU points for English	English	BUCC German-to-English
Parity on Automatic Chinese to English News Translation. arXiv:1803.05567	English	BUCC German-to-English
English News Arti- cles and Sentences	English	BUCC German-to-English
Machine translation is highly sensitive to the size and quality of	to	BUCC German-to-English
training data, which has led to an increasing interest in collect	to	BUCC German-to-English
sentence embed- dings. In contrast to previous approaches, which rely on	to	BUCC German-to-English
standard benchmarks, it is known to be particularly sen- sitive to	to	BUCC German-to-English
this context, effective ap- proaches to mine and filter parallel corpora	to	BUCC German-to-English
are crucial to apply NMT in practical settings	to	BUCC German-to-English
over bag-of-word features to distinguish between ground truth translations	to	BUCC German-to-English
combined with set expansion operations to score translation candidates through the	to	BUCC German-to-English
use an NMT inspired encoder-decoder to train sentence embeddings on existing	to	BUCC German-to-English
are then directly ap- plied to retrieve and filter new parallel	to	BUCC German-to-English
2018), who learn an encoder to score known translation pairs above	to	BUCC German-to-English
and train a separate model to dynamically scale and shift the	to	BUCC German-to-English
of a larger system, either to obtain an initial alignment that	to	BUCC German-to-English
to	to	BUCC German-to-English
shell. 0.498 While the risk to those working in ceramics is	to	BUCC German-to-English
discovered they are not free to speak their minds	to	BUCC German-to-English
1 shows our encoder-decoder architecture to learn multilingual sentence embeddings, which	to	BUCC German-to-English
ways: 1) they are used to initialize its hidden and cell	to	BUCC German-to-English
2) they are concate- nated to the input embeddings at every	to	BUCC German-to-English
of 0.001 and dropout set to 0.1. We use a sin	to	BUCC German-to-English
input embeddings size is set to 512, while the lan	to	BUCC German-to-English
2Prior to BPE segmentation, we tokenize and	to	BUCC German-to-English
and the encoder is used to map a sentence to a	to	BUCC German-to-English
multilingual encoder can be used to mine par- allel sentences by	to	BUCC German-to-English
in the target side according to cosine similarity, and filtering those	to	BUCC German-to-English
this approach has been reported to be competitive (Schwenk, 2018), we	to	BUCC German-to-English
one, thus making it impossible to filter it through a fixed	to	BUCC German-to-English
range, it is still susceptible to concentrate around dif- ferent values	to	BUCC German-to-English
do, filtering them is unlikely to cause any major harm	to	BUCC German-to-English
1: Architecture of our system to learn multilingual sentence embeddings	to	BUCC German-to-English
the average. This is equivalent to cosine similar- ity and thus	to	BUCC German-to-English
given candidate. This is proportional to the CSLS score (Conneau et	to	BUCC German-to-English
2018), which was originally motivated to mitigate the hub- ness problem	to	BUCC German-to-English
we explore the following strategies to generate candidates	to	BUCC German-to-English
causes the hubness problem. Thanks to its bidirec- tional nature, our	to	BUCC German-to-English
Backward: Equivalent to the forward strat- egy, but	to	BUCC German-to-English
candidates are then sorted according to their margin scores, and a	to	BUCC German-to-English
the development data, or adjusted to obtain the desired corpus size	to	BUCC German-to-English
on the training set, used to optimize the filtering threshold	to	BUCC German-to-English
to mine for parallel sentences between	to	BUCC German-to-English
and Chinese. There are 150K to 1.2M sentences for each language	to	BUCC German-to-English
UN model in compar- ison to previous work.9 Our proposed system	to	BUCC German-to-English
standard information was exclusively used to optimize the filtering threshold for	to	BUCC German-to-English
filtering threshold was opti- mized to maximize the F1 score on	to	BUCC German-to-English
UN corpus reconstruction So as to compare our method to the	to	BUCC German-to-English
we first pre- process it to remove all duplicated sentence pairs	to	BUCC German-to-English
de to	to	BUCC German-to-English
results (newstest2013) using different thresholds to filter ParaCrawl	to	BUCC German-to-English
corpus size from 4.59 billion to 64.4 million sentence pairs, mostly	to	BUCC German-to-English
due to dedu- plication. We then score	to	BUCC German-to-English
the top scoring entries up to the desired size. Figure 2	to	BUCC German-to-English
Table 6 compares our results to previ- ous works in the	to	BUCC German-to-English
train- ing data. In addition to our ParaCrawl system, we include	to	BUCC German-to-English
on English-German newstest2014 in comparison to previous work. wmt for WMT	to	BUCC German-to-English
our improvement can be attributed to a better fil- tering of	to	BUCC German-to-English
use a sequence-to-sequence ar- chitecture to train a multilingual sentence encoder	to	BUCC German-to-English
tion, our method obtains up to 85% precision at reconstructing the	to	BUCC German-to-English
our improvements also carry over to downstream machine transla- tion, as	to	BUCC German-to-English
the difference cannot be attributed to implementation details	to	BUCC German-to-English
Use of Comparable Corpora to Improve SMT per- formance. In	to	BUCC German-to-English
N Dauphin. 2017. Convolutional Sequence to Sequence Learning. In ICML, pages	to	BUCC German-to-English
Human Parity on Automatic Chinese to English News Translation. arXiv:1803.05567	to	BUCC German-to-English
on the WMT 15 task English	English	BUCC German-to-English
English (+2.1–3.4 BLEU), ob- taining new	English	BUCC German-to-English
for the IWSLT 15 task English	English	BUCC German-to-English
with additional monolingual data, on English	English	BUCC German-to-English
English, using training and test data	English	BUCC German-to-English
from WMT 15 for English	English	BUCC German-to-English
↔German, IWSLT 15 for English	English	BUCC German-to-English
English	English	BUCC German-to-English
For English	English	BUCC German-to-English
English, we report case-sensitive BLEU on	English	BUCC German-to-English
Table 1: English	English	BUCC German-to-English
4.1.1 English	English	BUCC German-to-English
monolin- gual data set into English	English	BUCC German-to-English
English system used for this is	English	BUCC German-to-English
English, we back-translate 4 200 000	English	BUCC German-to-English
monolingual English sentences into German, us- ing	English	BUCC German-to-English
the English	English	BUCC German-to-English
English training data	English	BUCC German-to-English
English	English	BUCC German-to-English
lower-resourced trans- lation setting than English	English	BUCC German-to-English
For both Turkish and English, we represent rare words (or	English	BUCC German-to-English
from Gigaword. We use an English	English	BUCC German-to-English
English baseline system	English	BUCC German-to-English
problem than with the larger English	English	BUCC German-to-English
English, we use gradient clipping with	English	BUCC German-to-English
1 that we use for English	English	BUCC German-to-English
4.2.1 English	English	BUCC German-to-English
Table 3 shows English	English	BUCC German-to-English
4.2.2 English	English	BUCC German-to-English
Table 4 shows English	English	BUCC German-to-English
Table 3: English	English	BUCC German-to-English
Table 4: English	English	BUCC German-to-English
English system trained on WMT data	English	BUCC German-to-English
English translation perfor- mance (BLEU) on	English	BUCC German-to-English
English WMT 15	English	BUCC German-to-English
English on the WMT 15 data	English	BUCC German-to-English
English IWSLT 14	English	BUCC German-to-English
English	English	BUCC German-to-English
English systems	English	BUCC German-to-English
Table 7: English	English	BUCC German-to-English
English system that was itself trained	English	BUCC German-to-English
English sys- tems, and of the	English	BUCC German-to-English
resulting English	English	BUCC German-to-English
English back-translation differs substantially, with a	English	BUCC German-to-English
on new- stest2015. Regarding the English	English	BUCC German-to-English
English translation performance (tokenized BLEU) on	English	BUCC German-to-English
Table 8: Phrase-based SMT results (English	English	BUCC German-to-English
English training and develop- ment set	English	BUCC German-to-English
Figure 2: English	English	BUCC German-to-English
English models. For comparability, we measure	English	BUCC German-to-English
Figure 2 shows cross-entropy for English	English	BUCC German-to-English
training data is available for English	English	BUCC German-to-English
natural according to native speaker. English	English	BUCC German-to-English
natural words. For instance, the English	English	BUCC German-to-English
→German sys- tems translate the English phrase civil rights pro- tections	English	BUCC German-to-English
data for NMT. In contrast to previous work, which combines NMT	to	BUCC German-to-English
architectures already have the capacity to learn the same information as	to	BUCC German-to-English
model, and we explore strategies to train with monolin- gual data	to	BUCC German-to-English
data, or data more similar to the translation task	to	BUCC German-to-English
rationale, adding a language model to compensate for the independence assumptions	to	BUCC German-to-English
and we expect monolingual data to be especially helpful if parallel	to	BUCC German-to-English
In contrast to previous work, which integrates a	to	BUCC German-to-English
al., 2015), we explore strategies to include monolingual training data in	to	BUCC German-to-English
This makes our approach applicable to different NMT architectures	to	BUCC German-to-English
we investigate two different methods to fill the source side of	to	BUCC German-to-English
we successfully adapt NMT models to a new domain by fine-tuning	to	BUCC German-to-English
our approach is not specific to this architecture	to	BUCC German-to-English
h j are concatenated to obtain the annotation vector hj	to	BUCC German-to-English
probability that yi is aligned to xj . The alignment model	to	BUCC German-to-English
or monolingual data more similar to the test set	to	BUCC German-to-English
serves to improve the estimate of the	to	BUCC German-to-English
into account. In con- trast to (Gülçehre et al., 2015), who	to	BUCC German-to-English
deep fusion, we propose techniques to train the main NMT model	to	BUCC German-to-English
words. We describe two strategies to do this: providing monolingual training	to	BUCC German-to-English
language, which we will refer to as back-translation	to	BUCC German-to-English
first technique we employ is to treat mono- lingual training examples	to	BUCC German-to-English
for which the network has to fully rely on the previous	to	BUCC German-to-English
to	to	BUCC German-to-English
single-word dummy source side <null> to allow processing of both parallel	to	BUCC German-to-English
force the context vector ci to be 0 for monolin- gual	to	BUCC German-to-English
sets of 20 minibatches according to length. This also groups monolin	to	BUCC German-to-English
the output layer remains sensitive to the source context, and that	to	BUCC German-to-English
from monolingual data, we propose to pair monolingual training instances with	to	BUCC German-to-English
text with mteval-v13a.pl for comparison to official WMT and IWSLT results	to	BUCC German-to-English
text with multi-bleu.perl for comparison to results by Gülçehre et al	to	BUCC German-to-English
not ensembles. We leave it to fu- ture work to explore	to	BUCC German-to-English
training with synthetic data is to the quality of the back	to	BUCC German-to-English
15 test sets to investigate a cross-domain setting.5	to	BUCC German-to-English
2007), and removal of non-surface to	to	BUCC German-to-English
We found overfitting to be a bigger problem than	to	BUCC German-to-English
et al. (2015), in contrast to the threshold 1 that we	to	BUCC German-to-English
as long as the baseline to provide the training al- gorithm	to	BUCC German-to-English
the quality improvement is due to the monolingual training instances, and	to	BUCC German-to-English
best single system achieves a to	to	BUCC German-to-English
- kenized BLEU (as opposed to untokenized scores reported in Table	to	BUCC German-to-English
of training instances varies due to differences in training time and	to	BUCC German-to-English
if it can be used to adapt a model to a	to	BUCC German-to-English
system trained on WMT data to translating TED talks	to	BUCC German-to-English
Systems 1 and 2 correspond to systems in Table 3, trained	to	BUCC German-to-English
system trained on WMT data to the TED do- main. By	to	BUCC German-to-English
BLEU on average. To compare to what extent syn- thetic data	to	BUCC German-to-English
of the parallel training text to obtain the training corpus parallelsynth	to	BUCC German-to-English
despite being out-of-domain in relation to the test sets. We speculate	to	BUCC German-to-English
monolin- gual data would lead to even higher improvements	to	BUCC German-to-English
monolingual data, but this led to decreased BLEU scores	to	BUCC German-to-English
difference in back-translation quality leads to a 0.6–0.7 BLEU difference in	to	BUCC German-to-English
size, and we leave it to future research to explore how	to	BUCC German-to-English
of each training run). Thanks to the increased diversity of the	to	BUCC German-to-English
4.3 Contrast to Phrase-based SMT	to	BUCC German-to-English
data into the source language to produce synthetic parallel text has	to	BUCC German-to-English
of training instances varies due to early stopping	to	BUCC German-to-English
In contrast to phrase-based SMT, which can make	to	BUCC German-to-English
so far not been able to use mono- lingual data to	to	BUCC German-to-English
parallel data is not limited to domain adaptation, and that even	to	BUCC German-to-English
hypothesis that domain adaptation contributes to the effectiveness of adding synthetic	to	BUCC German-to-English
data to NMT training	to	BUCC German-to-English
in data, or natural according to native speaker. English→German; newstest2015; ensemble	to	BUCC German-to-English
ency, its ability to produce natural target-language sentences. As	to	BUCC German-to-English
a proxy to sentence-level flu- ency, we investigate	to	BUCC German-to-English
of each sys- tem according to their naturalness13 , distinguish- ing	to	BUCC German-to-English
state of the language model to the de- coder state of	to	BUCC German-to-English
synthetic parallel texts bears resemblance to data augmentation techniques used in	to	BUCC German-to-English
is that self-training typically refers to scenario where the training set	to	BUCC German-to-English
continued training has been shown to be effective for neural language	to	BUCC German-to-English
and in work par- allel to ours, for neural translation models	to	BUCC German-to-English
2015). We are the first to show that we can effectively	to	BUCC German-to-English
we propose two simple methods to use monolingual training data during	to	BUCC German-to-English
NMT systems, with no changes to the network	to	BUCC German-to-English
dummy source context was successful to some ex- tent, but we	to	BUCC German-to-English
the neural net- work architecture to integrate monolingual train- ing data	to	BUCC German-to-English
approach can be easily applied to other NMT systems. We expect	to	BUCC German-to-English
on the amount (and similarity to the test set) of available	to	BUCC German-to-English
Machine Translation by Jointly Learning to Align and Trans- late. In	to	BUCC German-to-English
Roossin. 1990. A Statistical Approach to Machine Translation. Computational Linguistics, 16(2):79–85	to	BUCC German-to-English
Manning. 2015. Effective Ap- proaches to Attention-based Neural Machine Trans- lation	to	BUCC German-to-English
Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Networks	to	BUCC German-to-English
4.3 Contrast to Phrase-based SMT	to	BUCC German-to-English
models on an internal US English dataset[12], which contains 24.6 hours	English	North American English
models on an internal US English dataset[12], which contains 24.6 hours	English	North American English
speech databases from which Google’s North American English and Mandarin Chinese	North	North American English
TTS systems are built. The North American English dataset contains 24.6	North	North American English
MOS in naturalness Speech samples North American English Mandarin Chinese	North	North American English
8 and 63 stimuli for North American English and Mandarin Chinese	North	North American English
North 23.3 63.6 13.1 � 10−9	North	North American English
databases from which Google’s North American English and Mandarin Chinese TTS	American	North American English
systems are built. The North American English dataset contains 24.6 hours	American	North American English
in naturalness Speech samples North American English Mandarin Chinese	American	North American English
Mandarin ChineseNorth American English	American	North American English
Mandarin ChineseNorth American English	American	North American English
Mandarin ChineseNorth American English	American	North American English
and 63 stimuli for North American English and Mandarin Chinese, respectively	American	North American English
23.3 63.6 13.1 � 10−9 American 18.7 69.3 12.0 � 10−9	American	North American English
and concatenative systems for both English and Mandarin. A single WaveNet	English	North American English
on text). We used the English multi-speaker corpus from CSTR voice	English	North American English
from which Google’s North American English and Mandarin Chinese TTS systems	English	North American English
are built. The North American English dataset contains 24.6 hours of	English	North American English
to 0.34 (51%) in US English and 0.42 to 0.13 (69	English	North American English
naturalness Speech samples North American English Mandarin Chinese	English	North American English
Mandarin ChineseNorth American English	English	North American English
Mandarin ChineseNorth American English	English	North American English
Mandarin ChineseNorth American English	English	North American English
Yamagishi, Junichi. English multi-speaker corpus for CSTR voice	English	North American English
for HMM-based speech synthesis in English, 2006. URL http://hts.sp.nitech.ac.jp/?Download	English	North American English
63 stimuli for North American English and Mandarin Chinese, respectively. Test	English	North American English
18.7 69.3 12.0 � 10−9 English 7.6 82.0 10.4 � 10−9	English	North American English
speech databases from which Google’s North American English and Mandarin Chinese TTS systems	North American English	North American English
are built. The North American English dataset contains 24.6 hours of	North American English	North American English
MOS in naturalness Speech samples North American English Mandarin Chinese	North American English	North American English
8 and 63 stimuli for North American English and Mandarin Chinese, respectively. Test	North American English	North American English
train Tacotron on an internal North American English dataset, which contains	North	North American English
Tacotron on an internal North American English dataset, which contains about	American	North American English
mean opinion score on US English, outperforming a pro- duction parametric	English	North American English
score (MOS) on an US English eval set, outperforming a production	English	North American English
on an internal North American English dataset, which contains about 24.6	English	North American English
3.82 MOS score on US English, outperforming a production parametric system	English	North American English
train Tacotron on an internal North American English dataset, which contains about 24.6	North American English	North American English
speech databases from which Google’s North American English and Mandarin Chinese	North	North American English
TTS systems are built. The North American English dataset contains 24.6	North	North American English
MOS in naturalness Speech samples North American English Mandarin Chinese	North	North American English
8 and 63 stimuli for North American English and Mandarin Chinese	North	North American English
North 23.3 63.6 13.1 � 10−9	North	North American English
databases from which Google’s North American English and Mandarin Chinese TTS	American	North American English
systems are built. The North American English dataset contains 24.6 hours	American	North American English
in naturalness Speech samples North American English Mandarin Chinese	American	North American English
Mandarin ChineseNorth American English	American	North American English
Mandarin ChineseNorth American English	American	North American English
Mandarin ChineseNorth American English	American	North American English
and 63 stimuli for North American English and Mandarin Chinese, respectively	American	North American English
23.3 63.6 13.1 � 10−9 American 18.7 69.3 12.0 � 10−9	American	North American English
and concatenative systems for both English and Mandarin. A single WaveNet	English	North American English
on text). We used the English multi-speaker corpus from CSTR voice	English	North American English
from which Google’s North American English and Mandarin Chinese TTS systems	English	North American English
are built. The North American English dataset contains 24.6 hours of	English	North American English
to 0.34 (51%) in US English and 0.42 to 0.13 (69	English	North American English
naturalness Speech samples North American English Mandarin Chinese	English	North American English
Mandarin ChineseNorth American English	English	North American English
Mandarin ChineseNorth American English	English	North American English
Mandarin ChineseNorth American English	English	North American English
Yamagishi, Junichi. English multi-speaker corpus for CSTR voice	English	North American English
for HMM-based speech synthesis in English, 2006. URL http://hts.sp.nitech.ac.jp/?Download	English	North American English
63 stimuli for North American English and Mandarin Chinese, respectively. Test	English	North American English
18.7 69.3 12.0 � 10−9 English 7.6 82.0 10.4 � 10−9	English	North American English
speech databases from which Google’s North American English and Mandarin Chinese TTS systems	North American English	North American English
are built. The North American English dataset contains 24.6 hours of	North American English	North American English
MOS in naturalness Speech samples North American English Mandarin Chinese	North American English	North American English
8 and 63 stimuli for North American English and Mandarin Chinese, respectively. Test	North American English	North American English
speech databases from which Google’s North American English and Mandarin Chinese	North	North American English
TTS systems are built. The North American English dataset contains 24.6	North	North American English
MOS in naturalness Speech samples North American English Mandarin Chinese	North	North American English
8 and 63 stimuli for North American English and Mandarin Chinese	North	North American English
North 23.3 63.6 13.1 � 10−9	North	North American English
databases from which Google’s North American English and Mandarin Chinese TTS	American	North American English
systems are built. The North American English dataset contains 24.6 hours	American	North American English
in naturalness Speech samples North American English Mandarin Chinese	American	North American English
Mandarin ChineseNorth American English	American	North American English
Mandarin ChineseNorth American English	American	North American English
Mandarin ChineseNorth American English	American	North American English
and 63 stimuli for North American English and Mandarin Chinese, respectively	American	North American English
23.3 63.6 13.1 � 10−9 American 18.7 69.3 12.0 � 10−9	American	North American English
and concatenative systems for both English and Mandarin. A single WaveNet	English	North American English
on text). We used the English multi-speaker corpus from CSTR voice	English	North American English
from which Google’s North American English and Mandarin Chinese TTS systems	English	North American English
are built. The North American English dataset contains 24.6 hours of	English	North American English
to 0.34 (51%) in US English and 0.42 to 0.13 (69	English	North American English
naturalness Speech samples North American English Mandarin Chinese	English	North American English
Mandarin ChineseNorth American English	English	North American English
Mandarin ChineseNorth American English	English	North American English
Mandarin ChineseNorth American English	English	North American English
Yamagishi, Junichi. English multi-speaker corpus for CSTR voice	English	North American English
for HMM-based speech synthesis in English, 2006. URL http://hts.sp.nitech.ac.jp/?Download	English	North American English
63 stimuli for North American English and Mandarin Chinese, respectively. Test	English	North American English
18.7 69.3 12.0 � 10−9 English 7.6 82.0 10.4 � 10−9	English	North American English
speech databases from which Google’s North American English and Mandarin Chinese TTS systems	North American English	North American English
are built. The North American English dataset contains 24.6 hours of	North American English	North American English
MOS in naturalness Speech samples North American English Mandarin Chinese	North American English	North American English
8 and 63 stimuli for North American English and Mandarin Chinese, respectively. Test	North American English	North American English
North American English and Mandarin Chinese TTS systems are built. The	Chinese	Mandarin Chinese
speech data, and the Mandarin Chinese dataset contains 34.8 hours; both	Chinese	Mandarin Chinese
to 0.13 (69%) in Mandarin Chinese	Chinese	Mandarin Chinese
samples North American English Mandarin Chinese	Chinese	Mandarin Chinese
North American English and Mandarin Chinese, respectively. Test stimuli were randomly	Chinese	Mandarin Chinese
50.6 15.6 33.8 � 10−9 Chinese 25.0 23.3 51.8 0.476	Chinese	Mandarin Chinese
systems for both English and Mandarin	Mandarin	Mandarin Chinese
Google’s North American English and Mandarin Chinese TTS systems are built	Mandarin	Mandarin Chinese
of speech data, and the Mandarin Chinese dataset contains 34.8 hours	Mandarin	Mandarin Chinese
0.42 to 0.13 (69%) in Mandarin Chinese	Mandarin	Mandarin Chinese
Speech samples North American English Mandarin Chinese	Mandarin	Mandarin Chinese
Mandarin ChineseNorth American English	Mandarin	Mandarin Chinese
Mandarin ChineseNorth American English	Mandarin	Mandarin Chinese
Mandarin ChineseNorth American English	Mandarin	Mandarin Chinese
for North American English and Mandarin Chinese, respectively. Test stimuli were	Mandarin	Mandarin Chinese
Mandarin 50.6 15.6 33.8 � 10−9	Mandarin	Mandarin Chinese
Google’s North American English and Mandarin Chinese TTS systems are built. The	Mandarin Chinese	Mandarin Chinese
of speech data, and the Mandarin Chinese dataset contains 34.8 hours; both	Mandarin Chinese	Mandarin Chinese
0.42 to 0.13 (69%) in Mandarin Chinese	Mandarin Chinese	Mandarin Chinese
Speech samples North American English Mandarin Chinese	Mandarin Chinese	Mandarin Chinese
for North American English and Mandarin Chinese, respectively. Test stimuli were randomly	Mandarin Chinese	Mandarin Chinese
North American English and Mandarin Chinese TTS systems are built. The	Chinese	Mandarin Chinese
speech data, and the Mandarin Chinese dataset contains 34.8 hours; both	Chinese	Mandarin Chinese
to 0.13 (69%) in Mandarin Chinese	Chinese	Mandarin Chinese
samples North American English Mandarin Chinese	Chinese	Mandarin Chinese
North American English and Mandarin Chinese, respectively. Test stimuli were randomly	Chinese	Mandarin Chinese
50.6 15.6 33.8 � 10−9 Chinese 25.0 23.3 51.8 0.476	Chinese	Mandarin Chinese
systems for both English and Mandarin	Mandarin	Mandarin Chinese
Google’s North American English and Mandarin Chinese TTS systems are built	Mandarin	Mandarin Chinese
of speech data, and the Mandarin Chinese dataset contains 34.8 hours	Mandarin	Mandarin Chinese
0.42 to 0.13 (69%) in Mandarin Chinese	Mandarin	Mandarin Chinese
Speech samples North American English Mandarin Chinese	Mandarin	Mandarin Chinese
Mandarin ChineseNorth American English	Mandarin	Mandarin Chinese
Mandarin ChineseNorth American English	Mandarin	Mandarin Chinese
Mandarin ChineseNorth American English	Mandarin	Mandarin Chinese
for North American English and Mandarin Chinese, respectively. Test stimuli were	Mandarin	Mandarin Chinese
Mandarin 50.6 15.6 33.8 � 10−9	Mandarin	Mandarin Chinese
Google’s North American English and Mandarin Chinese TTS systems are built. The	Mandarin Chinese	Mandarin Chinese
of speech data, and the Mandarin Chinese dataset contains 34.8 hours; both	Mandarin Chinese	Mandarin Chinese
0.42 to 0.13 (69%) in Mandarin Chinese	Mandarin Chinese	Mandarin Chinese
Speech samples North American English Mandarin Chinese	Mandarin Chinese	Mandarin Chinese
for North American English and Mandarin Chinese, respectively. Test stimuli were randomly	Mandarin Chinese	Mandarin Chinese
North American English and Mandarin Chinese TTS systems are built. The	Chinese	Mandarin Chinese
speech data, and the Mandarin Chinese dataset contains 34.8 hours; both	Chinese	Mandarin Chinese
to 0.13 (69%) in Mandarin Chinese	Chinese	Mandarin Chinese
samples North American English Mandarin Chinese	Chinese	Mandarin Chinese
North American English and Mandarin Chinese, respectively. Test stimuli were randomly	Chinese	Mandarin Chinese
50.6 15.6 33.8 � 10−9 Chinese 25.0 23.3 51.8 0.476	Chinese	Mandarin Chinese
systems for both English and Mandarin	Mandarin	Mandarin Chinese
Google’s North American English and Mandarin Chinese TTS systems are built	Mandarin	Mandarin Chinese
of speech data, and the Mandarin Chinese dataset contains 34.8 hours	Mandarin	Mandarin Chinese
0.42 to 0.13 (69%) in Mandarin Chinese	Mandarin	Mandarin Chinese
Speech samples North American English Mandarin Chinese	Mandarin	Mandarin Chinese
Mandarin ChineseNorth American English	Mandarin	Mandarin Chinese
Mandarin ChineseNorth American English	Mandarin	Mandarin Chinese
Mandarin ChineseNorth American English	Mandarin	Mandarin Chinese
for North American English and Mandarin Chinese, respectively. Test stimuli were	Mandarin	Mandarin Chinese
Mandarin 50.6 15.6 33.8 � 10−9	Mandarin	Mandarin Chinese
Google’s North American English and Mandarin Chinese TTS systems are built. The	Mandarin Chinese	Mandarin Chinese
of speech data, and the Mandarin Chinese dataset contains 34.8 hours; both	Mandarin Chinese	Mandarin Chinese
0.42 to 0.13 (69%) in Mandarin Chinese	Mandarin Chinese	Mandarin Chinese
Speech samples North American English Mandarin Chinese	Mandarin Chinese	Mandarin Chinese
for North American English and Mandarin Chinese, respectively. Test stimuli were randomly	Mandarin Chinese	Mandarin Chinese
preprocessing around 3.7B comments from Reddit available in 256M conver- sational	Reddit	PolyAI Reddit
4M dialogue turns. Furthermore, our Reddit corpus includes 2 more years	Reddit	PolyAI Reddit
substantially larger than the previous Reddit dataset of Al-Rfou et al	Reddit	PolyAI Reddit
the conversation thread ID in Reddit, guaranteeing that the same split	Reddit	PolyAI Reddit
each example. For instance, in Reddit the author of the con	Reddit	PolyAI Reddit
Using the default quotas, the Reddit script	Reddit	PolyAI Reddit
Reddit 3.7 billion comments in threaded	Reddit	PolyAI Reddit
in the public repository. The Reddit data is taken from January	Reddit	PolyAI Reddit
provides an overview of the Reddit, OpenSubtitles and AmazonQA datasets, and	Reddit	PolyAI Reddit
3.1 Reddit	Reddit	PolyAI Reddit
Reddit is an American social news	Reddit	PolyAI Reddit
in discussions on these posts. Reddit is extremely diverse (Schrading et	Reddit	PolyAI Reddit
contexts paired with appropriate responses. Reddit data has been used to	Reddit	PolyAI Reddit
allows generating datasets from the Reddit data in a reproducible manner	Reddit	PolyAI Reddit
Reddit conversations are threaded. Each post	Reddit	PolyAI Reddit
in response. In processing, each Reddit thread is used to generate	Reddit	PolyAI Reddit
Reddit OpenSubtitles AmazonQA	Reddit	PolyAI Reddit
of domestic abuse discourse on Reddit	Reddit	PolyAI Reddit
Reddit	Reddit	PolyAI Reddit
PolyAI	PolyAI	PolyAI Reddit
PolyAI Limited, London, UK	PolyAI	PolyAI Reddit
PolyAI	PolyAI	PolyAI Reddit
repository is available at github.com/ PolyAI	PolyAI	PolyAI Reddit
PolyAI	PolyAI	PolyAI Reddit
PolyAI	PolyAI	PolyAI Reddit
PolyAI	PolyAI	PolyAI Reddit
AmazonQA over 3.6 million question-response pairs	AmazonQA	PolyAI AmazonQA
of the Reddit, OpenSubtitles and AmazonQA datasets, and fig- ure 3	AmazonQA	PolyAI AmazonQA
3.3 AmazonQA	AmazonQA	PolyAI AmazonQA
Reddit OpenSubtitles AmazonQA	AmazonQA	PolyAI AmazonQA
and are particularly strong for AmazonQA, possibly because rare words such	AmazonQA	PolyAI AmazonQA
AmazonQA	AmazonQA	PolyAI AmazonQA
PolyAI	PolyAI	PolyAI AmazonQA
PolyAI Limited, London, UK	PolyAI	PolyAI AmazonQA
PolyAI	PolyAI	PolyAI AmazonQA
repository is available at github.com/ PolyAI	PolyAI	PolyAI AmazonQA
PolyAI	PolyAI	PolyAI AmazonQA
PolyAI	PolyAI	PolyAI AmazonQA
PolyAI	PolyAI	PolyAI AmazonQA
PolyAI	PolyAI	PolyAI OpenSubtitles
PolyAI Limited, London, UK	PolyAI	PolyAI OpenSubtitles
PolyAI	PolyAI	PolyAI OpenSubtitles
repository is available at github.com/ PolyAI	PolyAI	PolyAI OpenSubtitles
PolyAI	PolyAI	PolyAI OpenSubtitles
PolyAI	PolyAI	PolyAI OpenSubtitles
PolyAI	PolyAI	PolyAI OpenSubtitles
of valid pairs in the OpenSubtitles dataset is 316 million. To	OpenSubtitles	PolyAI OpenSubtitles
OpenSubtitles over 400 million lines from	OpenSubtitles	PolyAI OpenSubtitles
to December 2018, and the OpenSubtitles data from 2018	OpenSubtitles	PolyAI OpenSubtitles
an overview of the Reddit, OpenSubtitles and AmazonQA datasets, and fig	OpenSubtitles	PolyAI OpenSubtitles
3.2 OpenSubtitles	OpenSubtitles	PolyAI OpenSubtitles
OpenSubtitles is a growing online collection	OpenSubtitles	PolyAI OpenSubtitles
Reddit OpenSubtitles AmazonQA	OpenSubtitles	PolyAI OpenSubtitles
OpenSubtitles	OpenSubtitles	PolyAI OpenSubtitles
Numenta	Numenta	Numenta Anomaly Benchmark
Numenta	Numenta	Numenta Anomaly Benchmark
Numenta	Numenta	Numenta Anomaly Benchmark
Numenta	Numenta	Numenta Anomaly Benchmark
Numenta	Numenta	Numenta Anomaly Benchmark
Numenta	Numenta	Numenta Anomaly Benchmark
a Numenta, Redwood City, CA, USA	Numenta	Numenta Anomaly Benchmark
also present results using the Numenta Anomaly Benchmark (NAB	Numenta	Numenta Anomaly Benchmark
file is included in the Numenta	Numenta	Numenta Anomaly Benchmark
EC2 instance (data from the Numenta Anomaly Benchmark [2] ). A	Numenta	Numenta Anomaly Benchmark
Section 3 we review the Numenta Anomaly	Numenta	Numenta Anomaly Benchmark
such we have created the Numenta Anomaly Benchmark (NAB	Numenta	Numenta Anomaly Benchmark
Source Anomaly Type Numenta HTM CAD OSE KNN CAD	Numenta	Numenta Anomaly Benchmark
Numenta anomaly benchmark, in: Proceedings of	Numenta	Numenta Anomaly Benchmark
the VP of Research at Numenta	Numenta	Numenta Anomaly Benchmark
Direc- tor of Engineering at Numenta	Numenta	Numenta Anomaly Benchmark
Numenta, and will join Apple in	Numenta	Numenta Anomaly Benchmark
Anomaly detection	Anomaly	Numenta Anomaly Benchmark
present results using the Numenta Anomaly Benchmark (NAB	Anomaly	Numenta Anomaly Benchmark
Anomaly Benchmark corpus [2	Anomaly	Numenta Anomaly Benchmark
instance (data from the Numenta Anomaly Benchmark [2] ). A modification	Anomaly	Numenta Anomaly Benchmark
Anomaly detection in time-series is a	Anomaly	Numenta Anomaly Benchmark
3 we review the Numenta Anomaly	Anomaly	Numenta Anomaly Benchmark
Anomaly detection using HTM	Anomaly	Numenta Anomaly Benchmark
we have created the Numenta Anomaly Benchmark (NAB	Anomaly	Numenta Anomaly Benchmark
windows or the data length. Anomaly	Anomaly	Numenta Anomaly Benchmark
Detector Latency (ms) Spatial Anomaly Temporal Anomaly Concept Drift Non	Anomaly	Numenta Anomaly Benchmark
ter’s Anomaly Detection, Etsy’s Skyline, Multinomial Relative	Anomaly	Numenta Anomaly Benchmark
Source Anomaly Type Numenta HTM CAD OSE	Anomaly	Numenta Anomaly Benchmark
Chandola, A. Banerjee, V. Kumar, Anomaly detection: a survey, ACM Comput	Anomaly	Numenta Anomaly Benchmark
tomated Time-series Anomaly Detection, in: Proceedings of the	Anomaly	Numenta Anomaly Benchmark
28] P. Angelov, Anomaly detection based on eccentricity analysis	Anomaly	Numenta Anomaly Benchmark
Lee, Y.R. Yeh, Y.C.F. Wang, Anomaly detection via online oversampling prin	Anomaly	Numenta Anomaly Benchmark
Engineering: Introducing Practical and Robust Anomaly	Anomaly	Numenta Anomaly Benchmark
Banerjee , V. Kumar , Anomaly detection: A survey, ACM Com	Anomaly	Numenta Anomaly Benchmark
2 Anomaly detection using HTM	Anomaly	Numenta Anomaly Benchmark
Benchmark dataset	Benchmark	Numenta Anomaly Benchmark
results using the Numenta Anomaly Benchmark (NAB	Benchmark	Numenta Anomaly Benchmark
Anomaly Benchmark corpus [2	Benchmark	Numenta Anomaly Benchmark
data from the Numenta Anomaly Benchmark [2] ). A modification to	Benchmark	Numenta Anomaly Benchmark
Benchmark (NAB) [2] , a rigorous	Benchmark	Numenta Anomaly Benchmark
have created the Numenta Anomaly Benchmark (NAB	Benchmark	Numenta Anomaly Benchmark
1. Benchmark dataset	Benchmark	Numenta Anomaly Benchmark
3.1 Benchmark dataset	Benchmark	Numenta Anomaly Benchmark
also present results using the Numenta Anomaly Benchmark (NAB	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
EC2 instance (data from the Numenta Anomaly Benchmark [2] ). A modification to	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
such we have created the Numenta Anomaly Benchmark (NAB	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
Numenta	Numenta	Numenta Anomaly Benchmark
Numenta	Numenta	Numenta Anomaly Benchmark
Numenta	Numenta	Numenta Anomaly Benchmark
Numenta	Numenta	Numenta Anomaly Benchmark
Numenta	Numenta	Numenta Anomaly Benchmark
Numenta	Numenta	Numenta Anomaly Benchmark
a Numenta, Redwood City, CA, USA	Numenta	Numenta Anomaly Benchmark
also present results using the Numenta Anomaly Benchmark (NAB	Numenta	Numenta Anomaly Benchmark
file is included in the Numenta	Numenta	Numenta Anomaly Benchmark
EC2 instance (data from the Numenta Anomaly Benchmark [2] ). A	Numenta	Numenta Anomaly Benchmark
Section 3 we review the Numenta Anomaly	Numenta	Numenta Anomaly Benchmark
such we have created the Numenta Anomaly Benchmark (NAB	Numenta	Numenta Anomaly Benchmark
Source Anomaly Type Numenta HTM CAD OSE KNN CAD	Numenta	Numenta Anomaly Benchmark
Numenta anomaly benchmark, in: Proceedings of	Numenta	Numenta Anomaly Benchmark
the VP of Research at Numenta	Numenta	Numenta Anomaly Benchmark
Direc- tor of Engineering at Numenta	Numenta	Numenta Anomaly Benchmark
Numenta, and will join Apple in	Numenta	Numenta Anomaly Benchmark
Anomaly detection	Anomaly	Numenta Anomaly Benchmark
present results using the Numenta Anomaly Benchmark (NAB	Anomaly	Numenta Anomaly Benchmark
Anomaly Benchmark corpus [2	Anomaly	Numenta Anomaly Benchmark
instance (data from the Numenta Anomaly Benchmark [2] ). A modification	Anomaly	Numenta Anomaly Benchmark
Anomaly detection in time-series is a	Anomaly	Numenta Anomaly Benchmark
3 we review the Numenta Anomaly	Anomaly	Numenta Anomaly Benchmark
Anomaly detection using HTM	Anomaly	Numenta Anomaly Benchmark
we have created the Numenta Anomaly Benchmark (NAB	Anomaly	Numenta Anomaly Benchmark
windows or the data length. Anomaly	Anomaly	Numenta Anomaly Benchmark
Detector Latency (ms) Spatial Anomaly Temporal Anomaly Concept Drift Non	Anomaly	Numenta Anomaly Benchmark
ter’s Anomaly Detection, Etsy’s Skyline, Multinomial Relative	Anomaly	Numenta Anomaly Benchmark
Source Anomaly Type Numenta HTM CAD OSE	Anomaly	Numenta Anomaly Benchmark
Chandola, A. Banerjee, V. Kumar, Anomaly detection: a survey, ACM Comput	Anomaly	Numenta Anomaly Benchmark
tomated Time-series Anomaly Detection, in: Proceedings of the	Anomaly	Numenta Anomaly Benchmark
28] P. Angelov, Anomaly detection based on eccentricity analysis	Anomaly	Numenta Anomaly Benchmark
Lee, Y.R. Yeh, Y.C.F. Wang, Anomaly detection via online oversampling prin	Anomaly	Numenta Anomaly Benchmark
Engineering: Introducing Practical and Robust Anomaly	Anomaly	Numenta Anomaly Benchmark
Banerjee , V. Kumar , Anomaly detection: A survey, ACM Com	Anomaly	Numenta Anomaly Benchmark
2 Anomaly detection using HTM	Anomaly	Numenta Anomaly Benchmark
Benchmark dataset	Benchmark	Numenta Anomaly Benchmark
results using the Numenta Anomaly Benchmark (NAB	Benchmark	Numenta Anomaly Benchmark
Anomaly Benchmark corpus [2	Benchmark	Numenta Anomaly Benchmark
data from the Numenta Anomaly Benchmark [2] ). A modification to	Benchmark	Numenta Anomaly Benchmark
Benchmark (NAB) [2] , a rigorous	Benchmark	Numenta Anomaly Benchmark
have created the Numenta Anomaly Benchmark (NAB	Benchmark	Numenta Anomaly Benchmark
1. Benchmark dataset	Benchmark	Numenta Anomaly Benchmark
3.1 Benchmark dataset	Benchmark	Numenta Anomaly Benchmark
also present results using the Numenta Anomaly Benchmark (NAB	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
EC2 instance (data from the Numenta Anomaly Benchmark [2] ). A modification to	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
such we have created the Numenta Anomaly Benchmark (NAB	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Numenta	Numenta Anomaly Benchmark
Numenta, Inc. Redwood City, CA	Numenta	Numenta Anomaly Benchmark
Numenta, Inc. Redwood City, CA	Numenta	Numenta Anomaly Benchmark
Numenta Anomaly Benchmark,” in 14th International	Numenta	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Numenta	Numenta Anomaly Benchmark
Numenta, Inc. Redwood City, CA	Numenta	Numenta Anomaly Benchmark
Numenta, Inc. Redwood City, CA	Numenta	Numenta Anomaly Benchmark
detectors. Here we propose the Numenta Anomaly	Numenta	Numenta Anomaly Benchmark
paper is to introduce the Numenta Anomaly Benchmark	Numenta	Numenta Anomaly Benchmark
algorithms. At Numenta we have developed an anomaly	Numenta	Numenta Anomaly Benchmark
four primary algorithms are the Numenta HTM anomaly	Numenta	Numenta Anomaly Benchmark
The HTM detector (developed by Numenta and the	Numenta	Numenta Anomaly Benchmark
The Numenta algorithm has several features that	Numenta	Numenta Anomaly Benchmark
etc. Like the Numenta algorithm, Skyline is well suited	Numenta	Numenta Anomaly Benchmark
1. Numenta HTM 64.7 56.5 69.3	Numenta	Numenta Anomaly Benchmark
technical report]. Palo Alto, CA: Numenta, Inc	Numenta	Numenta Anomaly Benchmark
6] (2015) Numenta Applications [Online website]. Redwood City	Numenta	Numenta Anomaly Benchmark
CA: Numenta, Inc. Available: http://numenta.com/#applications	Numenta	Numenta Anomaly Benchmark
15] Numenta, Inc. (2015) NAB: Numenta Anomaly Benchmark	Numenta	Numenta Anomaly Benchmark
code repository]. Redwood City, CA: Numenta, Inc	Numenta	Numenta Anomaly Benchmark
City, CA: Numenta, Inc. Available	Numenta	Numenta Anomaly Benchmark
CA: Numenta, Inc. Available: http://numenta.com/#technology	Numenta	Numenta Anomaly Benchmark
Online video]. Redwood City, CA: Numenta, Inc. Available	Numenta	Numenta Anomaly Benchmark
20] Numenta, Inc. (2015) NuPIC: Numenta Platform for Intelligent	Numenta	Numenta Anomaly Benchmark
Numenta, Inc. Available: https://github.com/numenta/nupic	Numenta	Numenta Anomaly Benchmark
Evaluating Real-time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Anomaly	Numenta Anomaly Benchmark
and S. Ahmad, “Evaluating Real-time Anomaly Detection Algorithms – the	Anomaly	Numenta Anomaly Benchmark
Numenta Anomaly Benchmark,” in 14th International Conference	Anomaly	Numenta Anomaly Benchmark
Evaluating Real-time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Anomaly	Numenta Anomaly Benchmark
Here we propose the Numenta Anomaly	Anomaly	Numenta Anomaly Benchmark
energy, e-commerce, and social media. Anomaly	Anomaly	Numenta Anomaly Benchmark
is to introduce the Numenta Anomaly Benchmark	Anomaly	Numenta Anomaly Benchmark
Anomaly detection in real-world streaming applications	Anomaly	Numenta Anomaly Benchmark
B. Scoring Real-Time Anomaly Detectors	Anomaly	Numenta Anomaly Benchmark
Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Series Anomaly Detection [Online blog]. Available	Anomaly	Numenta Anomaly Benchmark
Scalable Framework for Automated Time-series Anomaly	Anomaly	Numenta Anomaly Benchmark
Numenta, Inc. (2015) NAB: Numenta Anomaly Benchmark	Anomaly	Numenta Anomaly Benchmark
Anomaly Detection [Online technical report]. Redwood	Anomaly	Numenta Anomaly Benchmark
2014, October 17) Science of Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
Benchmark (NAB), which attempts to provide	Benchmark	Numenta Anomaly Benchmark
to introduce the Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
A. Benchmark Dataset	Benchmark	Numenta Anomaly Benchmark
I. Benchmark dataset: real world time-series data	Benchmark	Numenta Anomaly Benchmark
Yahoo Labs News: Announcing A Benchmark Dataset For Time	Benchmark	Numenta Anomaly Benchmark
Inc. (2015) NAB: Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
paper is to introduce the Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
15] Numenta, Inc. (2015) NAB: Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
II. NUMENTA ANOMALY BENCHMARK	NUMENTA ANOMALY BENCHMARK	Numenta Anomaly Benchmark
Numenta	Numenta	Numenta Anomaly Benchmark
Numenta	Numenta	Numenta Anomaly Benchmark
Numenta	Numenta	Numenta Anomaly Benchmark
Numenta	Numenta	Numenta Anomaly Benchmark
Numenta	Numenta	Numenta Anomaly Benchmark
Numenta	Numenta	Numenta Anomaly Benchmark
a Numenta, Redwood City, CA, USA	Numenta	Numenta Anomaly Benchmark
also present results using the Numenta Anomaly Benchmark (NAB	Numenta	Numenta Anomaly Benchmark
file is included in the Numenta	Numenta	Numenta Anomaly Benchmark
EC2 instance (data from the Numenta Anomaly Benchmark [2] ). A	Numenta	Numenta Anomaly Benchmark
Section 3 we review the Numenta Anomaly	Numenta	Numenta Anomaly Benchmark
such we have created the Numenta Anomaly Benchmark (NAB	Numenta	Numenta Anomaly Benchmark
Source Anomaly Type Numenta HTM CAD OSE KNN CAD	Numenta	Numenta Anomaly Benchmark
Numenta anomaly benchmark, in: Proceedings of	Numenta	Numenta Anomaly Benchmark
the VP of Research at Numenta	Numenta	Numenta Anomaly Benchmark
Direc- tor of Engineering at Numenta	Numenta	Numenta Anomaly Benchmark
Numenta, and will join Apple in	Numenta	Numenta Anomaly Benchmark
Anomaly detection	Anomaly	Numenta Anomaly Benchmark
present results using the Numenta Anomaly Benchmark (NAB	Anomaly	Numenta Anomaly Benchmark
Anomaly Benchmark corpus [2	Anomaly	Numenta Anomaly Benchmark
instance (data from the Numenta Anomaly Benchmark [2] ). A modification	Anomaly	Numenta Anomaly Benchmark
Anomaly detection in time-series is a	Anomaly	Numenta Anomaly Benchmark
3 we review the Numenta Anomaly	Anomaly	Numenta Anomaly Benchmark
Anomaly detection using HTM	Anomaly	Numenta Anomaly Benchmark
we have created the Numenta Anomaly Benchmark (NAB	Anomaly	Numenta Anomaly Benchmark
windows or the data length. Anomaly	Anomaly	Numenta Anomaly Benchmark
Detector Latency (ms) Spatial Anomaly Temporal Anomaly Concept Drift Non	Anomaly	Numenta Anomaly Benchmark
ter’s Anomaly Detection, Etsy’s Skyline, Multinomial Relative	Anomaly	Numenta Anomaly Benchmark
Source Anomaly Type Numenta HTM CAD OSE	Anomaly	Numenta Anomaly Benchmark
Chandola, A. Banerjee, V. Kumar, Anomaly detection: a survey, ACM Comput	Anomaly	Numenta Anomaly Benchmark
tomated Time-series Anomaly Detection, in: Proceedings of the	Anomaly	Numenta Anomaly Benchmark
28] P. Angelov, Anomaly detection based on eccentricity analysis	Anomaly	Numenta Anomaly Benchmark
Lee, Y.R. Yeh, Y.C.F. Wang, Anomaly detection via online oversampling prin	Anomaly	Numenta Anomaly Benchmark
Engineering: Introducing Practical and Robust Anomaly	Anomaly	Numenta Anomaly Benchmark
Banerjee , V. Kumar , Anomaly detection: A survey, ACM Com	Anomaly	Numenta Anomaly Benchmark
2 Anomaly detection using HTM	Anomaly	Numenta Anomaly Benchmark
Benchmark dataset	Benchmark	Numenta Anomaly Benchmark
results using the Numenta Anomaly Benchmark (NAB	Benchmark	Numenta Anomaly Benchmark
Anomaly Benchmark corpus [2	Benchmark	Numenta Anomaly Benchmark
data from the Numenta Anomaly Benchmark [2] ). A modification to	Benchmark	Numenta Anomaly Benchmark
Benchmark (NAB) [2] , a rigorous	Benchmark	Numenta Anomaly Benchmark
have created the Numenta Anomaly Benchmark (NAB	Benchmark	Numenta Anomaly Benchmark
1. Benchmark dataset	Benchmark	Numenta Anomaly Benchmark
3.1 Benchmark dataset	Benchmark	Numenta Anomaly Benchmark
also present results using the Numenta Anomaly Benchmark (NAB	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
EC2 instance (data from the Numenta Anomaly Benchmark [2] ). A modification to	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
such we have created the Numenta Anomaly Benchmark (NAB	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Numenta	Numenta Anomaly Benchmark
Numenta, Inc. Redwood City, CA	Numenta	Numenta Anomaly Benchmark
Numenta, Inc. Redwood City, CA	Numenta	Numenta Anomaly Benchmark
Numenta Anomaly Benchmark,” in 14th International	Numenta	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Numenta	Numenta Anomaly Benchmark
Numenta, Inc. Redwood City, CA	Numenta	Numenta Anomaly Benchmark
Numenta, Inc. Redwood City, CA	Numenta	Numenta Anomaly Benchmark
detectors. Here we propose the Numenta Anomaly	Numenta	Numenta Anomaly Benchmark
paper is to introduce the Numenta Anomaly Benchmark	Numenta	Numenta Anomaly Benchmark
algorithms. At Numenta we have developed an anomaly	Numenta	Numenta Anomaly Benchmark
four primary algorithms are the Numenta HTM anomaly	Numenta	Numenta Anomaly Benchmark
The HTM detector (developed by Numenta and the	Numenta	Numenta Anomaly Benchmark
The Numenta algorithm has several features that	Numenta	Numenta Anomaly Benchmark
etc. Like the Numenta algorithm, Skyline is well suited	Numenta	Numenta Anomaly Benchmark
1. Numenta HTM 64.7 56.5 69.3	Numenta	Numenta Anomaly Benchmark
technical report]. Palo Alto, CA: Numenta, Inc	Numenta	Numenta Anomaly Benchmark
6] (2015) Numenta Applications [Online website]. Redwood City	Numenta	Numenta Anomaly Benchmark
CA: Numenta, Inc. Available: http://numenta.com/#applications	Numenta	Numenta Anomaly Benchmark
15] Numenta, Inc. (2015) NAB: Numenta Anomaly Benchmark	Numenta	Numenta Anomaly Benchmark
code repository]. Redwood City, CA: Numenta, Inc	Numenta	Numenta Anomaly Benchmark
City, CA: Numenta, Inc. Available	Numenta	Numenta Anomaly Benchmark
CA: Numenta, Inc. Available: http://numenta.com/#technology	Numenta	Numenta Anomaly Benchmark
Online video]. Redwood City, CA: Numenta, Inc. Available	Numenta	Numenta Anomaly Benchmark
20] Numenta, Inc. (2015) NuPIC: Numenta Platform for Intelligent	Numenta	Numenta Anomaly Benchmark
Numenta, Inc. Available: https://github.com/numenta/nupic	Numenta	Numenta Anomaly Benchmark
Evaluating Real-time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Anomaly	Numenta Anomaly Benchmark
and S. Ahmad, “Evaluating Real-time Anomaly Detection Algorithms – the	Anomaly	Numenta Anomaly Benchmark
Numenta Anomaly Benchmark,” in 14th International Conference	Anomaly	Numenta Anomaly Benchmark
Evaluating Real-time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Anomaly	Numenta Anomaly Benchmark
Here we propose the Numenta Anomaly	Anomaly	Numenta Anomaly Benchmark
energy, e-commerce, and social media. Anomaly	Anomaly	Numenta Anomaly Benchmark
is to introduce the Numenta Anomaly Benchmark	Anomaly	Numenta Anomaly Benchmark
Anomaly detection in real-world streaming applications	Anomaly	Numenta Anomaly Benchmark
B. Scoring Real-Time Anomaly Detectors	Anomaly	Numenta Anomaly Benchmark
Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Series Anomaly Detection [Online blog]. Available	Anomaly	Numenta Anomaly Benchmark
Scalable Framework for Automated Time-series Anomaly	Anomaly	Numenta Anomaly Benchmark
Numenta, Inc. (2015) NAB: Numenta Anomaly Benchmark	Anomaly	Numenta Anomaly Benchmark
Anomaly Detection [Online technical report]. Redwood	Anomaly	Numenta Anomaly Benchmark
2014, October 17) Science of Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
Benchmark (NAB), which attempts to provide	Benchmark	Numenta Anomaly Benchmark
to introduce the Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
A. Benchmark Dataset	Benchmark	Numenta Anomaly Benchmark
I. Benchmark dataset: real world time-series data	Benchmark	Numenta Anomaly Benchmark
Yahoo Labs News: Announcing A Benchmark Dataset For Time	Benchmark	Numenta Anomaly Benchmark
Inc. (2015) NAB: Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
paper is to introduce the Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
15] Numenta, Inc. (2015) NAB: Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
II. NUMENTA ANOMALY BENCHMARK	NUMENTA ANOMALY BENCHMARK	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Numenta	Numenta Anomaly Benchmark
Numenta, Inc. Redwood City, CA	Numenta	Numenta Anomaly Benchmark
Numenta, Inc. Redwood City, CA	Numenta	Numenta Anomaly Benchmark
Numenta Anomaly Benchmark,” in 14th International	Numenta	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Numenta	Numenta Anomaly Benchmark
Numenta, Inc. Redwood City, CA	Numenta	Numenta Anomaly Benchmark
Numenta, Inc. Redwood City, CA	Numenta	Numenta Anomaly Benchmark
detectors. Here we propose the Numenta Anomaly	Numenta	Numenta Anomaly Benchmark
paper is to introduce the Numenta Anomaly Benchmark	Numenta	Numenta Anomaly Benchmark
algorithms. At Numenta we have developed an anomaly	Numenta	Numenta Anomaly Benchmark
four primary algorithms are the Numenta HTM anomaly	Numenta	Numenta Anomaly Benchmark
The HTM detector (developed by Numenta and the	Numenta	Numenta Anomaly Benchmark
The Numenta algorithm has several features that	Numenta	Numenta Anomaly Benchmark
etc. Like the Numenta algorithm, Skyline is well suited	Numenta	Numenta Anomaly Benchmark
1. Numenta HTM 64.7 56.5 69.3	Numenta	Numenta Anomaly Benchmark
technical report]. Palo Alto, CA: Numenta, Inc	Numenta	Numenta Anomaly Benchmark
6] (2015) Numenta Applications [Online website]. Redwood City	Numenta	Numenta Anomaly Benchmark
CA: Numenta, Inc. Available: http://numenta.com/#applications	Numenta	Numenta Anomaly Benchmark
15] Numenta, Inc. (2015) NAB: Numenta Anomaly Benchmark	Numenta	Numenta Anomaly Benchmark
code repository]. Redwood City, CA: Numenta, Inc	Numenta	Numenta Anomaly Benchmark
City, CA: Numenta, Inc. Available	Numenta	Numenta Anomaly Benchmark
CA: Numenta, Inc. Available: http://numenta.com/#technology	Numenta	Numenta Anomaly Benchmark
Online video]. Redwood City, CA: Numenta, Inc. Available	Numenta	Numenta Anomaly Benchmark
20] Numenta, Inc. (2015) NuPIC: Numenta Platform for Intelligent	Numenta	Numenta Anomaly Benchmark
Numenta, Inc. Available: https://github.com/numenta/nupic	Numenta	Numenta Anomaly Benchmark
Evaluating Real-time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Anomaly	Numenta Anomaly Benchmark
and S. Ahmad, “Evaluating Real-time Anomaly Detection Algorithms – the	Anomaly	Numenta Anomaly Benchmark
Numenta Anomaly Benchmark,” in 14th International Conference	Anomaly	Numenta Anomaly Benchmark
Evaluating Real-time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Anomaly	Numenta Anomaly Benchmark
Here we propose the Numenta Anomaly	Anomaly	Numenta Anomaly Benchmark
energy, e-commerce, and social media. Anomaly	Anomaly	Numenta Anomaly Benchmark
is to introduce the Numenta Anomaly Benchmark	Anomaly	Numenta Anomaly Benchmark
Anomaly detection in real-world streaming applications	Anomaly	Numenta Anomaly Benchmark
B. Scoring Real-Time Anomaly Detectors	Anomaly	Numenta Anomaly Benchmark
Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Series Anomaly Detection [Online blog]. Available	Anomaly	Numenta Anomaly Benchmark
Scalable Framework for Automated Time-series Anomaly	Anomaly	Numenta Anomaly Benchmark
Numenta, Inc. (2015) NAB: Numenta Anomaly Benchmark	Anomaly	Numenta Anomaly Benchmark
Anomaly Detection [Online technical report]. Redwood	Anomaly	Numenta Anomaly Benchmark
2014, October 17) Science of Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
Benchmark (NAB), which attempts to provide	Benchmark	Numenta Anomaly Benchmark
to introduce the Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
A. Benchmark Dataset	Benchmark	Numenta Anomaly Benchmark
I. Benchmark dataset: real world time-series data	Benchmark	Numenta Anomaly Benchmark
Yahoo Labs News: Announcing A Benchmark Dataset For Time	Benchmark	Numenta Anomaly Benchmark
Inc. (2015) NAB: Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
paper is to introduce the Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
15] Numenta, Inc. (2015) NAB: Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
II. NUMENTA ANOMALY BENCHMARK	NUMENTA ANOMALY BENCHMARK	Numenta Anomaly Benchmark
Subutai Ahmad SAHMAD@NUMENTA.COM Numenta, Inc., 791 Middlefield Road, Redwood	Numenta	Numenta Anomaly Benchmark
Scott Purdy SPURDY@NUMENTA.COM Numenta, Inc., 791 Middlefield Road, Redwood	Numenta	Numenta Anomaly Benchmark
time Anomaly Detection Algorithms the Numenta Anomaly Benchmark. In 14th International	Numenta	Numenta Anomaly Benchmark
Real-Time Anomaly Detection for Streaming Analytics	Anomaly	Numenta Anomaly Benchmark
Real-Time Anomaly Detection for Streaming Analytics	Anomaly	Numenta Anomaly Benchmark
unique constraints for machine learning. Anomaly detection in streaming ap- plications	Anomaly	Numenta Anomaly Benchmark
Real-Time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
2. Related Work Anomaly detection in time-series is a	Anomaly	Numenta Anomaly Benchmark
Real-Time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
3. Anomaly Detection Using HTM Typical streaming	Anomaly	Numenta Anomaly Benchmark
3.1. Computing the Raw Anomaly Score	Anomaly	Numenta Anomaly Benchmark
Real-Time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
3.2. Computing Anomaly Likelihood	Anomaly	Numenta Anomaly Benchmark
Real-Time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Real-Time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Real-Time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Real-Time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Real-Time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
V. Comparative Eval- uation of Anomaly Detection Techniques for Sequence Data	Anomaly	Numenta Anomaly Benchmark
Banerjee, A, and Kumar, V. Anomaly detection: A survey. ACM Computing	Anomaly	Numenta Anomaly Benchmark
Real-Time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Kleine, and Priesterjahn, Steffen. Model-Based Anomaly Detection for Discrete Event Systems	Anomaly	Numenta Anomaly Benchmark
Scalable Framework for Automated Time-series Anomaly Detection. In Proceedings of the	Anomaly	Numenta Anomaly Benchmark
Ahmad, Subutai. Evaluating Real- time Anomaly Detection Algorithms the Numenta Anomaly	Anomaly	Numenta Anomaly Benchmark
Rinehart, Aidan W. A Model-Based Anomaly Detection Approach for Analyzing Streaming	Anomaly	Numenta Anomaly Benchmark
Sokolov, Grigory. Efficient Computer Network Anomaly Detection by Changepoint Detection Meth	Anomaly	Numenta Anomaly Benchmark
4.1. Results on Real-World Benchmark Data	Benchmark	Numenta Anomaly Benchmark
Detection Algorithms the Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
time Anomaly Detection Algorithms the Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Numenta	Numenta Anomaly Benchmark
Numenta, Inc. Redwood City, CA	Numenta	Numenta Anomaly Benchmark
Numenta, Inc. Redwood City, CA	Numenta	Numenta Anomaly Benchmark
Numenta Anomaly Benchmark,” in 14th International	Numenta	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Numenta	Numenta Anomaly Benchmark
Numenta, Inc. Redwood City, CA	Numenta	Numenta Anomaly Benchmark
Numenta, Inc. Redwood City, CA	Numenta	Numenta Anomaly Benchmark
detectors. Here we propose the Numenta Anomaly	Numenta	Numenta Anomaly Benchmark
paper is to introduce the Numenta Anomaly Benchmark	Numenta	Numenta Anomaly Benchmark
algorithms. At Numenta we have developed an anomaly	Numenta	Numenta Anomaly Benchmark
four primary algorithms are the Numenta HTM anomaly	Numenta	Numenta Anomaly Benchmark
The HTM detector (developed by Numenta and the	Numenta	Numenta Anomaly Benchmark
The Numenta algorithm has several features that	Numenta	Numenta Anomaly Benchmark
etc. Like the Numenta algorithm, Skyline is well suited	Numenta	Numenta Anomaly Benchmark
1. Numenta HTM 64.7 56.5 69.3	Numenta	Numenta Anomaly Benchmark
technical report]. Palo Alto, CA: Numenta, Inc	Numenta	Numenta Anomaly Benchmark
6] (2015) Numenta Applications [Online website]. Redwood City	Numenta	Numenta Anomaly Benchmark
CA: Numenta, Inc. Available: http://numenta.com/#applications	Numenta	Numenta Anomaly Benchmark
15] Numenta, Inc. (2015) NAB: Numenta Anomaly Benchmark	Numenta	Numenta Anomaly Benchmark
code repository]. Redwood City, CA: Numenta, Inc	Numenta	Numenta Anomaly Benchmark
City, CA: Numenta, Inc. Available	Numenta	Numenta Anomaly Benchmark
CA: Numenta, Inc. Available: http://numenta.com/#technology	Numenta	Numenta Anomaly Benchmark
Online video]. Redwood City, CA: Numenta, Inc. Available	Numenta	Numenta Anomaly Benchmark
20] Numenta, Inc. (2015) NuPIC: Numenta Platform for Intelligent	Numenta	Numenta Anomaly Benchmark
Numenta, Inc. Available: https://github.com/numenta/nupic	Numenta	Numenta Anomaly Benchmark
Evaluating Real-time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Anomaly	Numenta Anomaly Benchmark
and S. Ahmad, “Evaluating Real-time Anomaly Detection Algorithms – the	Anomaly	Numenta Anomaly Benchmark
Numenta Anomaly Benchmark,” in 14th International Conference	Anomaly	Numenta Anomaly Benchmark
Evaluating Real-time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Anomaly	Numenta Anomaly Benchmark
Here we propose the Numenta Anomaly	Anomaly	Numenta Anomaly Benchmark
energy, e-commerce, and social media. Anomaly	Anomaly	Numenta Anomaly Benchmark
is to introduce the Numenta Anomaly Benchmark	Anomaly	Numenta Anomaly Benchmark
Anomaly detection in real-world streaming applications	Anomaly	Numenta Anomaly Benchmark
B. Scoring Real-Time Anomaly Detectors	Anomaly	Numenta Anomaly Benchmark
Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Series Anomaly Detection [Online blog]. Available	Anomaly	Numenta Anomaly Benchmark
Scalable Framework for Automated Time-series Anomaly	Anomaly	Numenta Anomaly Benchmark
Numenta, Inc. (2015) NAB: Numenta Anomaly Benchmark	Anomaly	Numenta Anomaly Benchmark
Anomaly Detection [Online technical report]. Redwood	Anomaly	Numenta Anomaly Benchmark
2014, October 17) Science of Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
Benchmark (NAB), which attempts to provide	Benchmark	Numenta Anomaly Benchmark
to introduce the Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
A. Benchmark Dataset	Benchmark	Numenta Anomaly Benchmark
I. Benchmark dataset: real world time-series data	Benchmark	Numenta Anomaly Benchmark
Yahoo Labs News: Announcing A Benchmark Dataset For Time	Benchmark	Numenta Anomaly Benchmark
Inc. (2015) NAB: Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
Algorithms – the Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
paper is to introduce the Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
15] Numenta, Inc. (2015) NAB: Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
II. NUMENTA ANOMALY BENCHMARK	NUMENTA ANOMALY BENCHMARK	Numenta Anomaly Benchmark
Subutai Ahmad SAHMAD@NUMENTA.COM Numenta, Inc., 791 Middlefield Road, Redwood	Numenta	Numenta Anomaly Benchmark
Scott Purdy SPURDY@NUMENTA.COM Numenta, Inc., 791 Middlefield Road, Redwood	Numenta	Numenta Anomaly Benchmark
time Anomaly Detection Algorithms the Numenta Anomaly Benchmark. In 14th International	Numenta	Numenta Anomaly Benchmark
Real-Time Anomaly Detection for Streaming Analytics	Anomaly	Numenta Anomaly Benchmark
Real-Time Anomaly Detection for Streaming Analytics	Anomaly	Numenta Anomaly Benchmark
unique constraints for machine learning. Anomaly detection in streaming ap- plications	Anomaly	Numenta Anomaly Benchmark
Real-Time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
2. Related Work Anomaly detection in time-series is a	Anomaly	Numenta Anomaly Benchmark
Real-Time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
3. Anomaly Detection Using HTM Typical streaming	Anomaly	Numenta Anomaly Benchmark
3.1. Computing the Raw Anomaly Score	Anomaly	Numenta Anomaly Benchmark
Real-Time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
3.2. Computing Anomaly Likelihood	Anomaly	Numenta Anomaly Benchmark
Real-Time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Real-Time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Real-Time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Real-Time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Real-Time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
V. Comparative Eval- uation of Anomaly Detection Techniques for Sequence Data	Anomaly	Numenta Anomaly Benchmark
Banerjee, A, and Kumar, V. Anomaly detection: A survey. ACM Computing	Anomaly	Numenta Anomaly Benchmark
Real-Time Anomaly Detection	Anomaly	Numenta Anomaly Benchmark
Kleine, and Priesterjahn, Steffen. Model-Based Anomaly Detection for Discrete Event Systems	Anomaly	Numenta Anomaly Benchmark
Scalable Framework for Automated Time-series Anomaly Detection. In Proceedings of the	Anomaly	Numenta Anomaly Benchmark
Ahmad, Subutai. Evaluating Real- time Anomaly Detection Algorithms the Numenta Anomaly	Anomaly	Numenta Anomaly Benchmark
Rinehart, Aidan W. A Model-Based Anomaly Detection Approach for Analyzing Streaming	Anomaly	Numenta Anomaly Benchmark
Sokolov, Grigory. Efficient Computer Network Anomaly Detection by Changepoint Detection Meth	Anomaly	Numenta Anomaly Benchmark
4.1. Results on Real-World Benchmark Data	Benchmark	Numenta Anomaly Benchmark
Detection Algorithms the Numenta Anomaly Benchmark	Benchmark	Numenta Anomaly Benchmark
time Anomaly Detection Algorithms the Numenta Anomaly Benchmark	Numenta Anomaly Benchmark	Numenta Anomaly Benchmark
achieves state-of-the-art results on the THUMOS’14 and Charades datasets. We further	THUMOS’	THUMOS’14
publicly available benchmark datasets - THUMOS’14 [29], ActivityNet [30] and Charades	THUMOS’	THUMOS’14
state-of-the-art results are achieved on THUMOS’14 and Charades, while the detection	THUMOS’	THUMOS’14
tion datasets - THUMOS’14 [29], Charades [31] and Ac	THUMOS’	THUMOS’14
A. Experiments on THUMOS’14	THUMOS’	THUMOS’14
THUMOS’14 activity detection dataset contains over	THUMOS’	THUMOS’14
C3D to be trained on THUMOS’14 with a fixed learning rate	THUMOS’	THUMOS’14
TABLE I. PROPOSAL EVALUATION ON THUMOS’14 DATASET (IN PERCENTAGE). AVERAGE AUC	THUMOS’	THUMOS’14
II. ACTIVITY DETECTION RESULTS ON THUMOS’14 (IN PERCENTAGE). MAP AT DIFFERENT	THUMOS’	THUMOS’14
REPORTED. TOP THREE PERFORMERS ON THUMOS’14 CHALLENGE LEADERBOARD	THUMOS’	THUMOS’14
AP) for each class on THUMOS’14 at tIoU threshold 0.5 is	THUMOS’	THUMOS’14
THRESHOLD α = 0.5 ON THUMOS’14 (IN PERCENTAGE	THUMOS’	THUMOS’14
of the video. Compared to THUMOS’14, this is a large-scale dataset	THUMOS’	THUMOS’14
server. Experimental Setup: Similar to THUMOS’14, the length of the input	THUMOS’	THUMOS’14
as is done for the THUMOS’14 dataset (ref. Sec. IV-A). We	THUMOS’	THUMOS’14
the improved results on the THUMOS’14, we choose the two-way buffer	THUMOS’	THUMOS’14
a) THUMOS’14	THUMOS’	THUMOS’14
for two videos each on THUMOS’14 and ActivityNet. (c) shows the	THUMOS’	THUMOS’14
achieves state-of-the-art results on the THUMOS’14 and Charades datasets. We further	THUMOS’14	THUMOS’14
publicly available benchmark datasets - THUMOS’14 [29], ActivityNet [30] and Charades	THUMOS’14	THUMOS’14
state-of-the-art results are achieved on THUMOS’14 and Charades, while the detection	THUMOS’14	THUMOS’14
tion datasets - THUMOS’14 [29], Charades [31] and Ac	THUMOS’14	THUMOS’14
A. Experiments on THUMOS’14	THUMOS’14	THUMOS’14
THUMOS’14 activity detection dataset contains over	THUMOS’14	THUMOS’14
C3D to be trained on THUMOS’14 with a fixed learning rate	THUMOS’14	THUMOS’14
TABLE I. PROPOSAL EVALUATION ON THUMOS’14 DATASET (IN PERCENTAGE). AVERAGE AUC	THUMOS’14	THUMOS’14
II. ACTIVITY DETECTION RESULTS ON THUMOS’14 (IN PERCENTAGE). MAP AT DIFFERENT	THUMOS’14	THUMOS’14
REPORTED. TOP THREE PERFORMERS ON THUMOS’14 CHALLENGE LEADERBOARD	THUMOS’14	THUMOS’14
AP) for each class on THUMOS’14 at tIoU threshold 0.5 is	THUMOS’14	THUMOS’14
THRESHOLD α = 0.5 ON THUMOS’14 (IN PERCENTAGE	THUMOS’14	THUMOS’14
of the video. Compared to THUMOS’14, this is a large-scale dataset	THUMOS’14	THUMOS’14
server. Experimental Setup: Similar to THUMOS’14, the length of the input	THUMOS’14	THUMOS’14
as is done for the THUMOS’14 dataset (ref. Sec. IV-A). We	THUMOS’14	THUMOS’14
the improved results on the THUMOS’14, we choose the two-way buffer	THUMOS’14	THUMOS’14
a) THUMOS’14	THUMOS’14	THUMOS’14
for two videos each on THUMOS’14 and ActivityNet. (c) shows the	THUMOS’14	THUMOS’14
achieves state-of-the-art results on the THUMOS’14 and Charades datasets. We further	THUMOS’	THUMOS’14
publicly available benchmark datasets - THUMOS’14 [29], ActivityNet [30] and Charades	THUMOS’	THUMOS’14
state-of-the-art results are achieved on THUMOS’14 and Charades, while the detection	THUMOS’	THUMOS’14
tion datasets - THUMOS’14 [29], Charades [31] and Ac	THUMOS’	THUMOS’14
A. Experiments on THUMOS’14	THUMOS’	THUMOS’14
THUMOS’14 activity detection dataset contains over	THUMOS’	THUMOS’14
C3D to be trained on THUMOS’14 with a fixed learning rate	THUMOS’	THUMOS’14
TABLE I. PROPOSAL EVALUATION ON THUMOS’14 DATASET (IN PERCENTAGE). AVERAGE AUC	THUMOS’	THUMOS’14
II. ACTIVITY DETECTION RESULTS ON THUMOS’14 (IN PERCENTAGE). MAP AT DIFFERENT	THUMOS’	THUMOS’14
REPORTED. TOP THREE PERFORMERS ON THUMOS’14 CHALLENGE LEADERBOARD	THUMOS’	THUMOS’14
AP) for each class on THUMOS’14 at tIoU threshold 0.5 is	THUMOS’	THUMOS’14
THRESHOLD α = 0.5 ON THUMOS’14 (IN PERCENTAGE	THUMOS’	THUMOS’14
of the video. Compared to THUMOS’14, this is a large-scale dataset	THUMOS’	THUMOS’14
server. Experimental Setup: Similar to THUMOS’14, the length of the input	THUMOS’	THUMOS’14
as is done for the THUMOS’14 dataset (ref. Sec. IV-A). We	THUMOS’	THUMOS’14
the improved results on the THUMOS’14, we choose the two-way buffer	THUMOS’	THUMOS’14
a) THUMOS’14	THUMOS’	THUMOS’14
for two videos each on THUMOS’14 and ActivityNet. (c) shows the	THUMOS’	THUMOS’14
achieves state-of-the-art results on the THUMOS’14 and Charades datasets. We further	THUMOS’14	THUMOS’14
publicly available benchmark datasets - THUMOS’14 [29], ActivityNet [30] and Charades	THUMOS’14	THUMOS’14
state-of-the-art results are achieved on THUMOS’14 and Charades, while the detection	THUMOS’14	THUMOS’14
tion datasets - THUMOS’14 [29], Charades [31] and Ac	THUMOS’14	THUMOS’14
A. Experiments on THUMOS’14	THUMOS’14	THUMOS’14
THUMOS’14 activity detection dataset contains over	THUMOS’14	THUMOS’14
C3D to be trained on THUMOS’14 with a fixed learning rate	THUMOS’14	THUMOS’14
TABLE I. PROPOSAL EVALUATION ON THUMOS’14 DATASET (IN PERCENTAGE). AVERAGE AUC	THUMOS’14	THUMOS’14
II. ACTIVITY DETECTION RESULTS ON THUMOS’14 (IN PERCENTAGE). MAP AT DIFFERENT	THUMOS’14	THUMOS’14
REPORTED. TOP THREE PERFORMERS ON THUMOS’14 CHALLENGE LEADERBOARD	THUMOS’14	THUMOS’14
AP) for each class on THUMOS’14 at tIoU threshold 0.5 is	THUMOS’14	THUMOS’14
THRESHOLD α = 0.5 ON THUMOS’14 (IN PERCENTAGE	THUMOS’14	THUMOS’14
of the video. Compared to THUMOS’14, this is a large-scale dataset	THUMOS’14	THUMOS’14
server. Experimental Setup: Similar to THUMOS’14, the length of the input	THUMOS’14	THUMOS’14
as is done for the THUMOS’14 dataset (ref. Sec. IV-A). We	THUMOS’14	THUMOS’14
the improved results on the THUMOS’14, we choose the two-way buffer	THUMOS’14	THUMOS’14
a) THUMOS’14	THUMOS’14	THUMOS’14
for two videos each on THUMOS’14 and ActivityNet. (c) shows the	THUMOS’14	THUMOS’14
Facial expression recognition based on deep	Facial	Acted Facial Expressions In The Wild (AFEW)
FRAME ATTENTION NETWORKS FOR FACIAL 		Acted Facial Expressions In The Wild (AFEW)
EXPRESSION RECOGNITION IN VIDEOS  Debin Meng, Xiaojiang Peng∗, Kai		Acted Facial Expressions In The Wild (AFEW)
 Wang, Yu Qiao		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
Shenzhen Institutes of Advanced Technology, 		Acted Facial Expressions In The Wild (AFEW)
Chinese Academy of Science, Shenzhen, 		Acted Facial Expressions In The Wild (AFEW)
China Shenzhen Key Lab of 		Acted Facial Expressions In The Wild (AFEW)
Computer Vision and Pattern Recognition, 		Acted Facial Expressions In The Wild (AFEW)
Shenzhen, China  University of Chinese Academy of		Acted Facial Expressions In The Wild (AFEW)
 Sciences, Beijing, China michaeldbmeng19@outlook.com, {xj.peng		Acted Facial Expressions In The Wild (AFEW)
, kai.wang, yu.qiao}@siat.ac.cn  ABSTRACT The video-based facial expression		Acted Facial Expressions In The Wild (AFEW)
 recognition aims to classify a		Acted Facial Expressions In The Wild (AFEW)
 given video into several basic		Acted Facial Expressions In The Wild (AFEW)
 emotions. How to integrate facial		Acted Facial Expressions In The Wild (AFEW)
 features of individual frames is		Acted Facial Expressions In The Wild (AFEW)
 crucial for this task. In		Acted Facial Expressions In The Wild (AFEW)
 this paper, we propose the		Acted Facial Expressions In The Wild (AFEW)
 Frame Attention Networks (FAN)1, to		Acted Facial Expressions In The Wild (AFEW)
 automatically highlight some discriminative frames		Acted Facial Expressions In The Wild (AFEW)
 in an end-to-end framework. The		Acted Facial Expressions In The Wild (AFEW)
 network takes a video with		Acted Facial Expressions In The Wild (AFEW)
 a variable number of face		Acted Facial Expressions In The Wild (AFEW)
 images as its input and		Acted Facial Expressions In The Wild (AFEW)
 produces a fixed-dimension representation. The		Acted Facial Expressions In The Wild (AFEW)
 whole network is com- posed		Acted Facial Expressions In The Wild (AFEW)
 of two modules. The feature		Acted Facial Expressions In The Wild (AFEW)
 embedding module is a deep		Acted Facial Expressions In The Wild (AFEW)
 Convolutional Neural Network (CNN) which		Acted Facial Expressions In The Wild (AFEW)
 embeds face images into feature		Acted Facial Expressions In The Wild (AFEW)
 vectors. The frame attention module		Acted Facial Expressions In The Wild (AFEW)
 learns multiple attention weights which		Acted Facial Expressions In The Wild (AFEW)
 are used to adaptively aggregate		Acted Facial Expressions In The Wild (AFEW)
 the feature vectors to form		Acted Facial Expressions In The Wild (AFEW)
 a single discriminative video representation		Acted Facial Expressions In The Wild (AFEW)
. We conduct extensive experiments 		Acted Facial Expressions In The Wild (AFEW)
on CK+ and AFEW8.0 datasets. 		Acted Facial Expressions In The Wild (AFEW)
Our proposed FAN shows su- 		Acted Facial Expressions In The Wild (AFEW)
perior performance compared to other 		Acted Facial Expressions In The Wild (AFEW)
CNN based methods and achieves 		Acted Facial Expressions In The Wild (AFEW)
state-of-the-art performance on CK+.  Index Terms— facial expression recognition		Acted Facial Expressions In The Wild (AFEW)
, audio- video emotion recognition, 		Acted Facial Expressions In The Wild (AFEW)
frame attention networks, CNN, AFEW  1. INTRODUCTION		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
Automatic facial expression recognition (FER) 		Acted Facial Expressions In The Wild (AFEW)
has recently attracted increasing attention 		Acted Facial Expressions In The Wild (AFEW)
in academia and industry due 		Acted Facial Expressions In The Wild (AFEW)
to its wide range of 		Acted Facial Expressions In The Wild (AFEW)
applications such as affective computing, 		Acted Facial Expressions In The Wild (AFEW)
intelligent environments, and multimodal human-computer 		Acted Facial Expressions In The Wild (AFEW)
interface (HCI). Though great progress 		Acted Facial Expressions In The Wild (AFEW)
have been made re- cently, 		Acted Facial Expressions In The Wild (AFEW)
facial expression recognition in the 		Acted Facial Expressions In The Wild (AFEW)
wild remains a challenging problem 		Acted Facial Expressions In The Wild (AFEW)
due to large head pose, 		Acted Facial Expressions In The Wild (AFEW)
illumination variance, occlusion, motion blur, 		Acted Facial Expressions In The Wild (AFEW)
etc.  Video-based facial expression recognition aims		Acted Facial Expressions In The Wild (AFEW)
 to classify a video into		Acted Facial Expressions In The Wild (AFEW)
 several basic emotions, such as		Acted Facial Expressions In The Wild (AFEW)
 happy, angry, dis		Acted Facial Expressions In The Wild (AFEW)
-  ∗Xiaojiang Peng is the corresponding		Acted Facial Expressions In The Wild (AFEW)
 author. Email: xj.peng@siat.ac.cn This work		Acted Facial Expressions In The Wild (AFEW)
 was supported by the National		Acted Facial Expressions In The Wild (AFEW)
 Natural Science Foun		Acted Facial Expressions In The Wild (AFEW)
-  dation of China (U1613211, U1713208		Acted Facial Expressions In The Wild (AFEW)
), Shenzhen Research Pro- gram (		Acted Facial Expressions In The Wild (AFEW)
JCYJ20170818164704758, JSGG20180507182100698), and In- ternational 		Acted Facial Expressions In The Wild (AFEW)
Partnership Program of Chinese Academy 		Acted Facial Expressions In The Wild (AFEW)
of Sciences (172644KYSB20150019).  1Code is available at https://github.com/Open-Debin/Emotion-FAN		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
gust, fear, sad, neutral, and 		Acted Facial Expressions In The Wild (AFEW)
surprise. Given a video, the 		Acted Facial Expressions In The Wild (AFEW)
pop- ular FER pipeline with 		Acted Facial Expressions In The Wild (AFEW)
a visual clue (FER with 		Acted Facial Expressions In The Wild (AFEW)
an audio clue is out 		Acted Facial Expressions In The Wild (AFEW)
of the scope of this 		Acted Facial Expressions In The Wild (AFEW)
paper) mainly includes three steps, 		Acted Facial Expressions In The Wild (AFEW)
namely frame preprocessing, feature extraction, 		Acted Facial Expressions In The Wild (AFEW)
and classi- fication. Especially, frame 		Acted Facial Expressions In The Wild (AFEW)
preprocessing refers to face de- 		Acted Facial Expressions In The Wild (AFEW)
tection, alignment, illumination normalizing and 		Acted Facial Expressions In The Wild (AFEW)
so on. Fea- ture extraction 		Acted Facial Expressions In The Wild (AFEW)
or video representation is the 		Acted Facial Expressions In The Wild (AFEW)
key part for FER which 		Acted Facial Expressions In The Wild (AFEW)
encodes frames or sequences into 		Acted Facial Expressions In The Wild (AFEW)
compact feature vec- tors. These 		Acted Facial Expressions In The Wild (AFEW)
feature vectors are subsequently fed 		Acted Facial Expressions In The Wild (AFEW)
into a classi- fier for 		Acted Facial Expressions In The Wild (AFEW)
prediction.  Feature extraction methods for video-based		Acted Facial Expressions In The Wild (AFEW)
 FER can be roughly divided		Acted Facial Expressions In The Wild (AFEW)
 into three types: static-based methods		Acted Facial Expressions In The Wild (AFEW)
, spatial-temporal methods, and geometry-based 		Acted Facial Expressions In The Wild (AFEW)
methods.  Static-based feature extraction methods mainly		Acted Facial Expressions In The Wild (AFEW)
 inherit those methods from static		Acted Facial Expressions In The Wild (AFEW)
 image emotion recognition which can		Acted Facial Expressions In The Wild (AFEW)
 be both hand-crafted [1, 2		Acted Facial Expressions In The Wild (AFEW)
] and learned [3, 4, 5		Acted Facial Expressions In The Wild (AFEW)
]. For the hand-crafted features, 		Acted Facial Expressions In The Wild (AFEW)
Littlewort et al. [1] propose 		Acted Facial Expressions In The Wild (AFEW)
to use a bank of 		Acted Facial Expressions In The Wild (AFEW)
2D Gabor filters to extract 		Acted Facial Expressions In The Wild (AFEW)
facial features for video- based 		Acted Facial Expressions In The Wild (AFEW)
FER. Shan et al. [2] 		Acted Facial Expressions In The Wild (AFEW)
use local binary patterns (LBP) 		Acted Facial Expressions In The Wild (AFEW)
and LBP histogram for facial 		Acted Facial Expressions In The Wild (AFEW)
feature extraction. For the learned 		Acted Facial Expressions In The Wild (AFEW)
features, Tang [3] utilizes deep 		Acted Facial Expressions In The Wild (AFEW)
CNNs for feature extraction, and 		Acted Facial Expressions In The Wild (AFEW)
win the FER2013. Some winners 		Acted Facial Expressions In The Wild (AFEW)
in audio-video emotion recognition task 		Acted Facial Expressions In The Wild (AFEW)
of EmotiW2016 and EmotiW2017 only 		Acted Facial Expressions In The Wild (AFEW)
use static facial features from 		Acted Facial Expressions In The Wild (AFEW)
deep CNNs trained on large 		Acted Facial Expressions In The Wild (AFEW)
face datasets or trained with 		Acted Facial Expressions In The Wild (AFEW)
multi-level supervision [4, 5].  Spatial-temporal methods aim to model		Acted Facial Expressions In The Wild (AFEW)
 the temporal or motion information		Acted Facial Expressions In The Wild (AFEW)
 in videos. The Long Short-Term		Acted Facial Expressions In The Wild (AFEW)
 Mem- ory (LSTM) [6], and		Acted Facial Expressions In The Wild (AFEW)
 C3D [7] are two widely-used		Acted Facial Expressions In The Wild (AFEW)
 spatial- temporal methods for video-based		Acted Facial Expressions In The Wild (AFEW)
 FER. LSTM derives in- formation		Acted Facial Expressions In The Wild (AFEW)
 from sequences by exploiting the		Acted Facial Expressions In The Wild (AFEW)
 fact that feature vectors are		Acted Facial Expressions In The Wild (AFEW)
 connected semantically for successive data		Acted Facial Expressions In The Wild (AFEW)
. This pipeline is widely-used 		Acted Facial Expressions In The Wild (AFEW)
in the EmotiW challenge, 		Acted Facial Expressions In The Wild (AFEW)
e.g. [8, 9, 10, 11]. 		Acted Facial Expressions In The Wild (AFEW)
C3D, which is originally developed 		Acted Facial Expressions In The Wild (AFEW)
for video action recognition, is 		Acted Facial Expressions In The Wild (AFEW)
also popular in the EmotiW 		Acted Facial Expressions In The Wild (AFEW)
challenge.  Geometry based methods [12, 11		Acted Facial Expressions In The Wild (AFEW)
] aim to model the 		Acted Facial Expressions In The Wild (AFEW)
mo- tions of key points 		Acted Facial Expressions In The Wild (AFEW)
in faces which only leverage 		Acted Facial Expressions In The Wild (AFEW)
the geometry locations of facial 		Acted Facial Expressions In The Wild (AFEW)
landmarks in every video frames. 		Acted Facial Expressions In The Wild (AFEW)
Jung et al. [12] propose 		Acted Facial Expressions In The Wild (AFEW)
a deep temporal appearance-geometry net- 		Acted Facial Expressions In The Wild (AFEW)
work (DTAGN) which first alternately 		Acted Facial Expressions In The Wild (AFEW)
concatenates the x- coordinates and 		Acted Facial Expressions In The Wild (AFEW)
y-coordinates of the facial landmark 		Acted Facial Expressions In The Wild (AFEW)
points  Copyright 2019 IEEE. Published in		Acted Facial Expressions In The Wild (AFEW)
 the IEEE 2019 International Conference		Acted Facial Expressions In The Wild (AFEW)
 on Image Processing (ICIP 2019		Acted Facial Expressions In The Wild (AFEW)
), scheduled for 22-25 September 2019 in Taipei, Taiwan. Personal use		Acted Facial Expressions In The Wild (AFEW)
 of this material is permitted		Acted Facial Expressions In The Wild (AFEW)
. However, permission to reprint/republish 		Acted Facial Expressions In The Wild (AFEW)
this material for advertising or 		Acted Facial Expressions In The Wild (AFEW)
promotional purposes or for creating 		Acted Facial Expressions In The Wild (AFEW)
new collective works for resale 		Acted Facial Expressions In The Wild (AFEW)
or redistribution to servers or 		Acted Facial Expressions In The Wild (AFEW)
lists, or to reuse any 		Acted Facial Expressions In The Wild (AFEW)
copyrighted component of this work 		Acted Facial Expressions In The Wild (AFEW)
in other works, must be 		Acted Facial Expressions In The Wild (AFEW)
obtained from the IEEE. Contact: 		Acted Facial Expressions In The Wild (AFEW)
Manager, Copyrights and Permissions / 		Acted Facial Expressions In The Wild (AFEW)
IEEE Service Center / 445 		Acted Facial Expressions In The Wild (AFEW)
Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA		Acted Facial Expressions In The Wild (AFEW)
. Telephone: + Intl. 908-562-3966.  ar X		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
iv :1  90 7		Acted Facial Expressions In The Wild (AFEW)
.  00 19		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
3v 2		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
cs  .C V		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
1  2		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
Se  p		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
20  19		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
from each frame after normalization 		Acted Facial Expressions In The Wild (AFEW)
and then concatenates these normalized 		Acted Facial Expressions In The Wild (AFEW)
points over time for a 		Acted Facial Expressions In The Wild (AFEW)
one-dimensional tra- jectory signal of 		Acted Facial Expressions In The Wild (AFEW)
each sequence. Yan et 		Acted Facial Expressions In The Wild (AFEW)
al. [11] construct an image-like 		Acted Facial Expressions In The Wild (AFEW)
map by stretching all the 		Acted Facial Expressions In The Wild (AFEW)
normalized facial point trajectories in 		Acted Facial Expressions In The Wild (AFEW)
a sequence together as the 		Acted Facial Expressions In The Wild (AFEW)
input of a CNN.  Among all the above methods		Acted Facial Expressions In The Wild (AFEW)
, static-based methods are superior 		Acted Facial Expressions In The Wild (AFEW)
to the others according to 		Acted Facial Expressions In The Wild (AFEW)
several winner solutions in EmotiW 		Acted Facial Expressions In The Wild (AFEW)
challenges. To obtain a video-level 		Acted Facial Expressions In The Wild (AFEW)
result with varied frames, a 		Acted Facial Expressions In The Wild (AFEW)
frame aggregation operation is necessary 		Acted Facial Expressions In The Wild (AFEW)
for static- based methods. For 		Acted Facial Expressions In The Wild (AFEW)
frame aggregation, Kahou et 		Acted Facial Expressions In The Wild (AFEW)
al. [13] concatenate the n-class 		Acted Facial Expressions In The Wild (AFEW)
probability vectors of 10 segments 		Acted Facial Expressions In The Wild (AFEW)
to form a fixed-length video 		Acted Facial Expressions In The Wild (AFEW)
representation by frame averag- ing 		Acted Facial Expressions In The Wild (AFEW)
or frame expansion. Bargal et 		Acted Facial Expressions In The Wild (AFEW)
al. [4] propose a statistical 		Acted Facial Expressions In The Wild (AFEW)
encoding module (STAT) to aggregate 		Acted Facial Expressions In The Wild (AFEW)
frame features which compute the 		Acted Facial Expressions In The Wild (AFEW)
mean, variance, minimum, and maximum 		Acted Facial Expressions In The Wild (AFEW)
of the frame feature vectors.  One limitation of these existing		Acted Facial Expressions In The Wild (AFEW)
 aggregation methods is that they		Acted Facial Expressions In The Wild (AFEW)
 ignore the importance of frames		Acted Facial Expressions In The Wild (AFEW)
 for FER. For example, some		Acted Facial Expressions In The Wild (AFEW)
 faces in Figure 1 are		Acted Facial Expressions In The Wild (AFEW)
 representative for the ‘happy’ category		Acted Facial Expressions In The Wild (AFEW)
 while the others not. In		Acted Facial Expressions In The Wild (AFEW)
 this paper, inspired by the		Acted Facial Expressions In The Wild (AFEW)
 attention mechanism [14] of machine		Acted Facial Expressions In The Wild (AFEW)
 translation and the neural aggregation		Acted Facial Expressions In The Wild (AFEW)
 networks [15] of video face		Acted Facial Expressions In The Wild (AFEW)
 recog- nition, we propose the		Acted Facial Expressions In The Wild (AFEW)
 Frame Attention Networks (FAN) to		Acted Facial Expressions In The Wild (AFEW)
 adaptively aggregate frame features. The		Acted Facial Expressions In The Wild (AFEW)
 FAN is designed to learn		Acted Facial Expressions In The Wild (AFEW)
 self-attention kernels and relation-attention kernels		Acted Facial Expressions In The Wild (AFEW)
 for frame importance reasoning in		Acted Facial Expressions In The Wild (AFEW)
 an end-to-end fashion. The self-attention		Acted Facial Expressions In The Wild (AFEW)
 kernels are directly learned from		Acted Facial Expressions In The Wild (AFEW)
 frame features while the relation-attention		Acted Facial Expressions In The Wild (AFEW)
 kernels are learned from the		Acted Facial Expressions In The Wild (AFEW)
 concatenated features of a video-level		Acted Facial Expressions In The Wild (AFEW)
 anchor feature and frame features		Acted Facial Expressions In The Wild (AFEW)
. We conduct extensive experiments 		Acted Facial Expressions In The Wild (AFEW)
on CK+ and AFEW8.0 (EmotiW2018) 		Acted Facial Expressions In The Wild (AFEW)
datasets. Our proposed FAN shows 		Acted Facial Expressions In The Wild (AFEW)
superior performance compared to other 		Acted Facial Expressions In The Wild (AFEW)
CNN based methods with only 		Acted Facial Expressions In The Wild (AFEW)
facial features and achieves state-of-the- 		Acted Facial Expressions In The Wild (AFEW)
art performance on CK+.  2. FRAME ATTENTION NETWORKS		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
We propose Frame Attention Networks (		Acted Facial Expressions In The Wild (AFEW)
FAN) for video- based facial 		Acted Facial Expressions In The Wild (AFEW)
expression recognition (FER). Figure 1 		Acted Facial Expressions In The Wild (AFEW)
illus- trates the framework of 		Acted Facial Expressions In The Wild (AFEW)
our proposed FAN. It takes 		Acted Facial Expressions In The Wild (AFEW)
a facial video with a 		Acted Facial Expressions In The Wild (AFEW)
variable number of face images 		Acted Facial Expressions In The Wild (AFEW)
as its input and produces 		Acted Facial Expressions In The Wild (AFEW)
a fixed-dimension feature representation for 		Acted Facial Expressions In The Wild (AFEW)
FER. The whole network consists 		Acted Facial Expressions In The Wild (AFEW)
of two modules: feature em- 		Acted Facial Expressions In The Wild (AFEW)
bedding module and frame attention 		Acted Facial Expressions In The Wild (AFEW)
module. The feature embedding module 		Acted Facial Expressions In The Wild (AFEW)
is a deep CNN which 		Acted Facial Expressions In The Wild (AFEW)
embeds each face image into 		Acted Facial Expressions In The Wild (AFEW)
a feature vector. The frame 		Acted Facial Expressions In The Wild (AFEW)
attention module learns two-level attention 		Acted Facial Expressions In The Wild (AFEW)
weights, i.e. self-attention weights and 		Acted Facial Expressions In The Wild (AFEW)
relation-attention weights, which are used 		Acted Facial Expressions In The Wild (AFEW)
to adaptively aggregate the feature 		Acted Facial Expressions In The Wild (AFEW)
vectors to form a single 		Acted Facial Expressions In The Wild (AFEW)
discriminative video representation.  Formally, we denote a video		Acted Facial Expressions In The Wild (AFEW)
 with n frames as V		Acted Facial Expressions In The Wild (AFEW)
, and its frames as 		Acted Facial Expressions In The Wild (AFEW)
I1, I2, · · · , In, and the facial frame		Acted Facial Expressions In The Wild (AFEW)
 features are {f1		Acted Facial Expressions In The Wild (AFEW)
, · · · , 		Acted Facial Expressions In The Wild (AFEW)
fn}.  Self-attention weights. With individual frame		Acted Facial Expressions In The Wild (AFEW)
 features		Acted Facial Expressions In The Wild (AFEW)
,  Fig. 1. Our proposed frame		Acted Facial Expressions In The Wild (AFEW)
 attention network architecture		Acted Facial Expressions In The Wild (AFEW)
.  our FAN first applies a		Acted Facial Expressions In The Wild (AFEW)
 FC layer and a sigmoid		Acted Facial Expressions In The Wild (AFEW)
 function to assign coarse self-attention		Acted Facial Expressions In The Wild (AFEW)
 weights. Mathematically, the self- attention		Acted Facial Expressions In The Wild (AFEW)
 weight of the i-th frame		Acted Facial Expressions In The Wild (AFEW)
 is defined by		Acted Facial Expressions In The Wild (AFEW)
:  αi = σ(f T i		Acted Facial Expressions In The Wild (AFEW)
 q		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
0) (1)  where q0 is the parameter		Acted Facial Expressions In The Wild (AFEW)
 of FC, σ denotes the		Acted Facial Expressions In The Wild (AFEW)
 sigmoid func- tion. With these		Acted Facial Expressions In The Wild (AFEW)
 self-attention weights, we aggregate all		Acted Facial Expressions In The Wild (AFEW)
 the input frame features into		Acted Facial Expressions In The Wild (AFEW)
 a global representation f ′v		Acted Facial Expressions In The Wild (AFEW)
 as follows		Acted Facial Expressions In The Wild (AFEW)
,  f ′v		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
n i=1 αifi∑n i=1 αi  . (2		Acted Facial Expressions In The Wild (AFEW)
)  We use f ′v as		Acted Facial Expressions In The Wild (AFEW)
 a video-level global anchor for		Acted Facial Expressions In The Wild (AFEW)
 learning fur- ther accurate relation-attention		Acted Facial Expressions In The Wild (AFEW)
 weights		Acted Facial Expressions In The Wild (AFEW)
.  Relation-attention weights. We believe that		Acted Facial Expressions In The Wild (AFEW)
 learning weights from both a		Acted Facial Expressions In The Wild (AFEW)
 global feature and local features		Acted Facial Expressions In The Wild (AFEW)
 is more reliable. The self-attention		Acted Facial Expressions In The Wild (AFEW)
 weights are learned with indi		Acted Facial Expressions In The Wild (AFEW)
- vidual frame features and 		Acted Facial Expressions In The Wild (AFEW)
non-linear mapping, which are rather 		Acted Facial Expressions In The Wild (AFEW)
coarse. Since f ′v inherently 		Acted Facial Expressions In The Wild (AFEW)
contains the contents of the 		Acted Facial Expressions In The Wild (AFEW)
whole video, the attention weights 		Acted Facial Expressions In The Wild (AFEW)
can be further refined by 		Acted Facial Expressions In The Wild (AFEW)
modeling the relation between frame 		Acted Facial Expressions In The Wild (AFEW)
features and this global representation 		Acted Facial Expressions In The Wild (AFEW)
f ′v .  Inspired by the relation-Net in		Acted Facial Expressions In The Wild (AFEW)
 low-shot learning [16], we use		Acted Facial Expressions In The Wild (AFEW)
 the sample concatenation and another		Acted Facial Expressions In The Wild (AFEW)
 FC layer to esti- mate		Acted Facial Expressions In The Wild (AFEW)
 new relation-attention weights for frame		Acted Facial Expressions In The Wild (AFEW)
 features. The relation-attention weight of		Acted Facial Expressions In The Wild (AFEW)
 the i-th frame is formulated		Acted Facial Expressions In The Wild (AFEW)
 as		Acted Facial Expressions In The Wild (AFEW)
,  βi = σ([fi : f		Acted Facial Expressions In The Wild (AFEW)
 ′ v		Acted Facial Expressions In The Wild (AFEW)
]  Tq1), (3		Acted Facial Expressions In The Wild (AFEW)
)  where q1 is the parameter		Acted Facial Expressions In The Wild (AFEW)
 of FC, σ denotes the		Acted Facial Expressions In The Wild (AFEW)
 sigmoid func- tion		Acted Facial Expressions In The Wild (AFEW)
.  Finally, with self-attention and relation-attention		Acted Facial Expressions In The Wild (AFEW)
 weights, our FAN aggregates all		Acted Facial Expressions In The Wild (AFEW)
 the frame features into a		Acted Facial Expressions In The Wild (AFEW)
 new compact feature as		Acted Facial Expressions In The Wild (AFEW)
,  fv		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
n i=0 αiβi[fi : f  ′ v]∑n		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
i=0 αiβi . (4		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
3. EXPERIMENTS  3.1. Datasets and Implementation Details		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
CK+ [17] contains 593 video 		Acted Facial Expressions In The Wild (AFEW)
sequences from 123 subjects. Among 		Acted Facial Expressions In The Wild (AFEW)
these videos, 327 sequences from 118 subjects are la- beled with		Acted Facial Expressions In The Wild (AFEW)
 seven basic expression labels, i.e		Acted Facial Expressions In The Wild (AFEW)
. anger, contempt, disgust, fear, 		Acted Facial Expressions In The Wild (AFEW)
happiness, sadness, and surprise. Since 		Acted Facial Expressions In The Wild (AFEW)
CK+ does not provide training/testing 		Acted Facial Expressions In The Wild (AFEW)
splits, most of the algorithms 		Acted Facial Expressions In The Wild (AFEW)
evaluated on this database with 10		Acted Facial Expressions In The Wild (AFEW)
-fold person-independence cross-validation experiments. We 		Acted Facial Expressions In The Wild (AFEW)
constructed 10 subsets by sampling 		Acted Facial Expressions In The Wild (AFEW)
ID in ascending order with 		Acted Facial Expressions In The Wild (AFEW)
a step size of 10 		Acted Facial Expressions In The Wild (AFEW)
as in several previous 		Acted Facial Expressions In The Wild (AFEW)
works [18, 19], and report 		Acted Facial Expressions In The Wild (AFEW)
the overall accu- racy over 10 folds		Acted Facial Expressions In The Wild (AFEW)
.  AFEW 8.0 [20] served as		Acted Facial Expressions In The Wild (AFEW)
 an evaluation platform for the		Acted Facial Expressions In The Wild (AFEW)
 annual EmotiW since 2013. Seven		Acted Facial Expressions In The Wild (AFEW)
 emotion labels are in- cluded		Acted Facial Expressions In The Wild (AFEW)
 in AFEW, i.e. anger, disgust		Acted Facial Expressions In The Wild (AFEW)
, fear, happiness, sadness, surprise 		Acted Facial Expressions In The Wild (AFEW)
and neutral. AFEW contains video 		Acted Facial Expressions In The Wild (AFEW)
clips collected from different movies 		Acted Facial Expressions In The Wild (AFEW)
and TV serials with spontaneous 		Acted Facial Expressions In The Wild (AFEW)
ex- pressions, various head poses, 		Acted Facial Expressions In The Wild (AFEW)
occlusions, and illuminations. AFEW 8.0 		Acted Facial Expressions In The Wild (AFEW)
is divided into three splits: 		Acted Facial Expressions In The Wild (AFEW)
Train (773 samples), Val (383 		Acted Facial Expressions In The Wild (AFEW)
samples) and Test (653 samples), 		Acted Facial Expressions In The Wild (AFEW)
which ensures data in the 		Acted Facial Expressions In The Wild (AFEW)
three sets belong to mutually 		Acted Facial Expressions In The Wild (AFEW)
exclusive movies and ac- tors. 		Acted Facial Expressions In The Wild (AFEW)
Since the test split is 		Acted Facial Expressions In The Wild (AFEW)
not publicly available, we train 		Acted Facial Expressions In The Wild (AFEW)
our model on training split 		Acted Facial Expressions In The Wild (AFEW)
and report results on validation 		Acted Facial Expressions In The Wild (AFEW)
split.  Implementation details. We preprocess video		Acted Facial Expressions In The Wild (AFEW)
 frames by face detection and		Acted Facial Expressions In The Wild (AFEW)
 alignment in the Dlib toolbox		Acted Facial Expressions In The Wild (AFEW)
 We extend the face bounding		Acted Facial Expressions In The Wild (AFEW)
 box with a ratio of		Acted Facial Expressions In The Wild (AFEW)
 25% and then resize the		Acted Facial Expressions In The Wild (AFEW)
 cropped faces to scale of		Acted Facial Expressions In The Wild (AFEW)
 224×224. We implement our method		Acted Facial Expressions In The Wild (AFEW)
 by the Pytorch toolbox. By		Acted Facial Expressions In The Wild (AFEW)
 default, for feature em- bedding		Acted Facial Expressions In The Wild (AFEW)
, we use the ResNet18 		Acted Facial Expressions In The Wild (AFEW)
which is pre-trained on MS- 		Acted Facial Expressions In The Wild (AFEW)
Celeb-1M [21] face recognition dataset 		Acted Facial Expressions In The Wild (AFEW)
and FER Plus expres- sion 		Acted Facial Expressions In The Wild (AFEW)
dataset [22]. For training, on 		Acted Facial Expressions In The Wild (AFEW)
both CK+ and AFEW 8.0, 		Acted Facial Expressions In The Wild (AFEW)
we set a batch to 		Acted Facial Expressions In The Wild (AFEW)
have 48 instances with K 		Acted Facial Expressions In The Wild (AFEW)
frames in each instance. For 		Acted Facial Expressions In The Wild (AFEW)
frame sampling in a video, 		Acted Facial Expressions In The Wild (AFEW)
we first split the video 		Acted Facial Expressions In The Wild (AFEW)
into K segments and then 		Acted Facial Expressions In The Wild (AFEW)
randomly select one frame from 		Acted Facial Expressions In The Wild (AFEW)
each segment. By default, we 		Acted Facial Expressions In The Wild (AFEW)
set K to 3. We 		Acted Facial Expressions In The Wild (AFEW)
use the SGD method for 		Acted Facial Expressions In The Wild (AFEW)
optimization with a momentum of 0		Acted Facial Expressions In The Wild (AFEW)
.9 and a weight decay 		Acted Facial Expressions In The Wild (AFEW)
of 10−4. On CK+, we 		Acted Facial Expressions In The Wild (AFEW)
initialize the learning rate (lr) 		Acted Facial Expressions In The Wild (AFEW)
to 0.1, and modify it 		Acted Facial Expressions In The Wild (AFEW)
to 0.02 at 30 epochs, 		Acted Facial Expressions In The Wild (AFEW)
and stop training after 60 		Acted Facial Expressions In The Wild (AFEW)
epochs. On AFEW 8.0, we 		Acted Facial Expressions In The Wild (AFEW)
initialize the lr to 4e-6, 		Acted Facial Expressions In The Wild (AFEW)
and modify it to 8e-7 		Acted Facial Expressions In The Wild (AFEW)
at 60 epochs and 1.6e-7 		Acted Facial Expressions In The Wild (AFEW)
at 120 epochs, and stop 		Acted Facial Expressions In The Wild (AFEW)
training after 180 epochs.  3.2. Evaluation on CK		Acted Facial Expressions In The Wild (AFEW)
+  We evaluate our FAN on		Acted Facial Expressions In The Wild (AFEW)
 CK+ with comparisons to several		Acted Facial Expressions In The Wild (AFEW)
 state-of-the-art methods in Table 1		Acted Facial Expressions In The Wild (AFEW)
. On CK+, due to 		Acted Facial Expressions In The Wild (AFEW)
the fact that the videos 		Acted Facial Expressions In The Wild (AFEW)
show a shift from a 		Acted Facial Expressions In The Wild (AFEW)
neutral facial expression to the 		Acted Facial Expressions In The Wild (AFEW)
peak expression, most of the 		Acted Facial Expressions In The Wild (AFEW)
methods conduct data selection manually. 		Acted Facial Expressions In The Wild (AFEW)
Zhang et al [23] propose 		Acted Facial Expressions In The Wild (AFEW)
to combine a spatial CNN 		Acted Facial Expressions In The Wild (AFEW)
model and a temporal network, 		Acted Facial Expressions In The Wild (AFEW)
where the spatial CNN model 		Acted Facial Expressions In The Wild (AFEW)
only uses the last peak 		Acted Facial Expressions In The Wild (AFEW)
frame. Jung et al [12] 		Acted Facial Expressions In The Wild (AFEW)
select a fixed length sequence 		Acted Facial Expressions In The Wild (AFEW)
for each video with a 		Acted Facial Expressions In The Wild (AFEW)
lipreading method [26], and jointly 		Acted Facial Expressions In The Wild (AFEW)
fine-tune a deep temporal  Table 1. Evaluation of our		Acted Facial Expressions In The Wild (AFEW)
 FAN with a comparison to		Acted Facial Expressions In The Wild (AFEW)
 state- of-the-art methods on CK		Acted Facial Expressions In The Wild (AFEW)
+ database. Note that only 		Acted Facial Expressions In The Wild (AFEW)
those methods evaluated with 7 		Acted Facial Expressions In The Wild (AFEW)
classes are included.  Method Training data Test data		Acted Facial Expressions In The Wild (AFEW)
 Acc		Acted Facial Expressions In The Wild (AFEW)
.  ST network [23] S: the		Acted Facial Expressions In The Wild (AFEW)
 last frame T: all frames		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
S: the last frame T: 		Acted Facial Expressions In The Wild (AFEW)
all frames 98.50  DTAGN [12] Fixed length Fixed		Acted Facial Expressions In The Wild (AFEW)
 length 97.25		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
CNN+Island loss [24]  The last three frames and		Acted Facial Expressions In The Wild (AFEW)
 the first frame		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
The last three frames and 		Acted Facial Expressions In The Wild (AFEW)
the first frame  94.35		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
LOMo [25] All frames All 		Acted Facial Expressions In The Wild (AFEW)
frames 92.00  Score fusion (baseline) All frames		Acted Facial Expressions In The Wild (AFEW)
 All frames 94.80		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
FAN(w/o Relation- attention)  All frames All frames 99.08		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
FAN All frames All frames 99		Acted Facial Expressions In The Wild (AFEW)
.69  appearance-geometry network. Cai et al		Acted Facial Expressions In The Wild (AFEW)
 [24] select the last three		Acted Facial Expressions In The Wild (AFEW)
 frames and the first frame		Acted Facial Expressions In The Wild (AFEW)
 for each video, and train		Acted Facial Expressions In The Wild (AFEW)
 CNN models with a new		Acted Facial Expressions In The Wild (AFEW)
 Island loss function. We argue		Acted Facial Expressions In The Wild (AFEW)
 that manual data selection is		Acted Facial Expressions In The Wild (AFEW)
 an ad-hoc operation on CK		Acted Facial Expressions In The Wild (AFEW)
+ and it is impractical 		Acted Facial Expressions In The Wild (AFEW)
since we can not know 		Acted Facial Expressions In The Wild (AFEW)
which is the peak frame 		Acted Facial Expressions In The Wild (AFEW)
beforeahead. Sikka et al [25] 		Acted Facial Expressions In The Wild (AFEW)
use all frames with a 		Acted Facial Expressions In The Wild (AFEW)
new latent ordinal model which 		Acted Facial Expressions In The Wild (AFEW)
extracts CNN/LBP/SIFT features for sub-event 		Acted Facial Expressions In The Wild (AFEW)
detection and uses multi-instance SVM 		Acted Facial Expressions In The Wild (AFEW)
for ex- pression classification. Our 		Acted Facial Expressions In The Wild (AFEW)
baseline method uses ResNet18 to 		Acted Facial Expressions In The Wild (AFEW)
generate scores for individual frame 		Acted Facial Expressions In The Wild (AFEW)
and applies score fu- sion (		Acted Facial Expressions In The Wild (AFEW)
summation) for all frames. It 		Acted Facial Expressions In The Wild (AFEW)
achieves 94.8% which is 2.8% 		Acted Facial Expressions In The Wild (AFEW)
better than [25]. Our proposed 		Acted Facial Expressions In The Wild (AFEW)
FAN with only self- attention 		Acted Facial Expressions In The Wild (AFEW)
gets 99.08% which significantly boosts 		Acted Facial Expressions In The Wild (AFEW)
the baseline by 4.28%. Adding 		Acted Facial Expressions In The Wild (AFEW)
relation-attention weights further im- proves 		Acted Facial Expressions In The Wild (AFEW)
the accuracy to 99.69% which 		Acted Facial Expressions In The Wild (AFEW)
sets up a new state 		Acted Facial Expressions In The Wild (AFEW)
of the art on CK+.  3.3. Evaluation on AFEW 8.0		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
From the view of performance, 		Acted Facial Expressions In The Wild (AFEW)
AFEW is one of the 		Acted Facial Expressions In The Wild (AFEW)
most challenging videos FER dataset. 		Acted Facial Expressions In The Wild (AFEW)
The EmotiW challenge shares the 		Acted Facial Expressions In The Wild (AFEW)
same data from 2016 to 2018		Acted Facial Expressions In The Wild (AFEW)
. Table 2 presents the 		Acted Facial Expressions In The Wild (AFEW)
evaluation of our FAN on 		Acted Facial Expressions In The Wild (AFEW)
AFEW with comparisons to recent 		Acted Facial Expressions In The Wild (AFEW)
state-of-the-art methods. For a fair 		Acted Facial Expressions In The Wild (AFEW)
comparison, we only list these 		Acted Facial Expressions In The Wild (AFEW)
results obtained by the best 		Acted Facial Expressions In The Wild (AFEW)
single models in previous works. 		Acted Facial Expressions In The Wild (AFEW)
From the last three rows 		Acted Facial Expressions In The Wild (AFEW)
of Table 2, our proposed 		Acted Facial Expressions In The Wild (AFEW)
FAN improves the baseline by 2		Acted Facial Expressions In The Wild (AFEW)
.36%. Both [27] and [10] 		Acted Facial Expressions In The Wild (AFEW)
use VGGFace backbone and a 		Acted Facial Expressions In The Wild (AFEW)
recurrent model with long-short-term memory 		Acted Facial Expressions In The Wild (AFEW)
units. These methods aim to 		Acted Facial Expressions In The Wild (AFEW)
capture temporal dynamic information for 		Acted Facial Expressions In The Wild (AFEW)
videos. Most of the meth- 		Acted Facial Expressions In The Wild (AFEW)
ods focus on improving static 		Acted Facial Expressions In The Wild (AFEW)
face based CNN models 		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
Table 2. Evaluation of our 		Acted Facial Expressions In The Wild (AFEW)
FAN with a comparison to 		Acted Facial Expressions In The Wild (AFEW)
state- of-the-art methods on AFEW 8		Acted Facial Expressions In The Wild (AFEW)
.0 database. It is worth 		Acted Facial Expressions In The Wild (AFEW)
noting that we only compare 		Acted Facial Expressions In The Wild (AFEW)
to the best single models 		Acted Facial Expressions In The Wild (AFEW)
of previous works.  Method Model type Accuracy		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
CNN-RNN (2016) [27] Dynamic 45.43 		Acted Facial Expressions In The Wild (AFEW)
VGGFace + Undirectional LSTM (2017		Acted Facial Expressions In The Wild (AFEW)
) [10] Dynamic 48.60  HoloNet (2016) [28] Static 44.57		Acted Facial Expressions In The Wild (AFEW)
 DSN-HoloNet (2017) [29] Static 46.47		Acted Facial Expressions In The Wild (AFEW)
 DenseNet-161 (2018) [31] Static 51.44		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
DSN-VGGFace (2018) [30] Static 48.04  Score fusion (baseline) Static 48.82		Acted Facial Expressions In The Wild (AFEW)
 FAN w/o Relation-attention Static 50.92		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
FAN Static 51.18  combine scores for video-level FER		Acted Facial Expressions In The Wild (AFEW)
. Both [28] and [29] 		Acted Facial Expressions In The Wild (AFEW)
input two LBP maps and 		Acted Facial Expressions In The Wild (AFEW)
a gray image for CNN 		Acted Facial Expressions In The Wild (AFEW)
models. Deeply- supervised networks are 		Acted Facial Expressions In The Wild (AFEW)
used in [29] and [30], 		Acted Facial Expressions In The Wild (AFEW)
which add supervision on intermediate 		Acted Facial Expressions In The Wild (AFEW)
layers. For static methods, [31] 		Acted Facial Expressions In The Wild (AFEW)
gets slightly better performance than 		Acted Facial Expressions In The Wild (AFEW)
ours. However, [31] uses DenseNet-161 		Acted Facial Expressions In The Wild (AFEW)
and pretrains it on both 		Acted Facial Expressions In The Wild (AFEW)
large-scale face datasets and their 		Acted Facial Expressions In The Wild (AFEW)
own Situ emotion video dataset. 		Acted Facial Expressions In The Wild (AFEW)
Addition- ally, [31] applies complicated 		Acted Facial Expressions In The Wild (AFEW)
post-processing which extracts frame features 		Acted Facial Expressions In The Wild (AFEW)
and compute their mean vector, 		Acted Facial Expressions In The Wild (AFEW)
max-pooled vector, and standard deviation 		Acted Facial Expressions In The Wild (AFEW)
vector. These vectors are then 		Acted Facial Expressions In The Wild (AFEW)
concatenated and finally fed into 		Acted Facial Expressions In The Wild (AFEW)
an SVM classifier. Overall, our 		Acted Facial Expressions In The Wild (AFEW)
FAN improves the baseline significantly 		Acted Facial Expressions In The Wild (AFEW)
and achieves performance comparable to 		Acted Facial Expressions In The Wild (AFEW)
that of the best previous 		Acted Facial Expressions In The Wild (AFEW)
single model.  3.4. Visualization and Hyper-parameters		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
To better understand the self-attention 		Acted Facial Expressions In The Wild (AFEW)
and relation-attention modules in our 		Acted Facial Expressions In The Wild (AFEW)
FAN, we visualize the attention 		Acted Facial Expressions In The Wild (AFEW)
weights in Figure 2. Figure 2 shows one sequence for each		Acted Facial Expressions In The Wild (AFEW)
 category with blue and orange		Acted Facial Expressions In The Wild (AFEW)
 weight bars, where blue bars		Acted Facial Expressions In The Wild (AFEW)
 represent the self-attention weights (i.e		Acted Facial Expressions In The Wild (AFEW)
. α in Eq. (1)) 		Acted Facial Expressions In The Wild (AFEW)
of our FAN w/o relation-attention 		Acted Facial Expressions In The Wild (AFEW)
and orange bars the final 		Acted Facial Expressions In The Wild (AFEW)
weights (i.e. αβ in Eq. (4		Acted Facial Expressions In The Wild (AFEW)
)) of our FAN. In 		Acted Facial Expressions In The Wild (AFEW)
total, both kinds of weights 		Acted Facial Expressions In The Wild (AFEW)
can reflect the importance of 		Acted Facial Expressions In The Wild (AFEW)
frames. Comparing the blue and 		Acted Facial Expressions In The Wild (AFEW)
orange bars, we find that 		Acted Facial Expressions In The Wild (AFEW)
the final weights of our 		Acted Facial Expressions In The Wild (AFEW)
FAN can always assign higher 		Acted Facial Expressions In The Wild (AFEW)
weights to the more obvious 		Acted Facial Expressions In The Wild (AFEW)
face frames, while self-attention module 		Acted Facial Expressions In The Wild (AFEW)
could assign high weights on 		Acted Facial Expressions In The Wild (AFEW)
some ob- scure face frames, 		Acted Facial Expressions In The Wild (AFEW)
see the 1st, 2th, and 		Acted Facial Expressions In The Wild (AFEW)
3rd rows of Figure 2 (		Acted Facial Expressions In The Wild (AFEW)
left). This explicitly explains why 		Acted Facial Expressions In The Wild (AFEW)
adding relation-attention boost performance.  Evaluation of Hyper-parameters. We evaluate		Acted Facial Expressions In The Wild (AFEW)
 two hyper-parameters of our FAN		Acted Facial Expressions In The Wild (AFEW)
 on CK+, i.e. backbone CNN		Acted Facial Expressions In The Wild (AFEW)
 networks and the parameter K		Acted Facial Expressions In The Wild (AFEW)
 mentioned in implementation details, to		Acted Facial Expressions In The Wild (AFEW)
 validate the robustness of our		Acted Facial Expressions In The Wild (AFEW)
 method. For the parameter K		Acted Facial Expressions In The Wild (AFEW)
, besides the default value, 		Acted Facial Expressions In The Wild (AFEW)
we try several other values, 		Acted Facial Expressions In The Wild (AFEW)
i.e. {2, 5, 8}, and 		Acted Facial Expressions In The Wild (AFEW)
find the performance is not 		Acted Facial Expressions In The Wild (AFEW)
sensitive to K. Specifically, our 		Acted Facial Expressions In The Wild (AFEW)
FAN obtains 99.39% with K={2, 5		Acted Facial Expressions In The Wild (AFEW)
}. and gets 99.69% with 		Acted Facial Expressions In The Wild (AFEW)
K=8. Since the default value, 		Acted Facial Expressions In The Wild (AFEW)
K=3  Fig. 2. Visualization of the		Acted Facial Expressions In The Wild (AFEW)
 self-attention weights (blue bar) and		Acted Facial Expressions In The Wild (AFEW)
 the final weights of FAN		Acted Facial Expressions In The Wild (AFEW)
 (orange bar) on CK+ dataset		Acted Facial Expressions In The Wild (AFEW)
.  Fig. 3. Evaluation of backbone		Acted Facial Expressions In The Wild (AFEW)
 CNN models and training strategies		Acted Facial Expressions In The Wild (AFEW)
 on CK		Acted Facial Expressions In The Wild (AFEW)
+.  gets 99.69%, we use this		Acted Facial Expressions In The Wild (AFEW)
 default setting in the remainder		Acted Facial Expressions In The Wild (AFEW)
 of this paper		Acted Facial Expressions In The Wild (AFEW)
.  For the backbone CNN model		Acted Facial Expressions In The Wild (AFEW)
 evaluation, we try the VGGFace		Acted Facial Expressions In The Wild (AFEW)
 model which is widely-used in		Acted Facial Expressions In The Wild (AFEW)
 previous works. Similarly, we also		Acted Facial Expressions In The Wild (AFEW)
 pretrain the VGGFace model on		Acted Facial Expressions In The Wild (AFEW)
 the FER- Plus dataset. Since		Acted Facial Expressions In The Wild (AFEW)
 [5] shows that it is		Acted Facial Expressions In The Wild (AFEW)
 better to freeze all the		Acted Facial Expressions In The Wild (AFEW)
 feature learning layers after pretrained		Acted Facial Expressions In The Wild (AFEW)
 on FERPlus for VGGFace model		Acted Facial Expressions In The Wild (AFEW)
, we also conduct the 		Acted Facial Expressions In The Wild (AFEW)
same experiment on CK+ with 		Acted Facial Expressions In The Wild (AFEW)
VGGFace. Figure 3 shows the 		Acted Facial Expressions In The Wild (AFEW)
default com- parisons with different 		Acted Facial Expressions In The Wild (AFEW)
backbone CNN models. On CK+, 		Acted Facial Expressions In The Wild (AFEW)
compared with freezing all the 		Acted Facial Expressions In The Wild (AFEW)
feature layers for VGGFace, it 		Acted Facial Expressions In The Wild (AFEW)
gets better results with fine-tuning 		Acted Facial Expressions In The Wild (AFEW)
all layers which may be 		Acted Facial Expressions In The Wild (AFEW)
explained by the domain discrepancy 		Acted Facial Expressions In The Wild (AFEW)
between FERPlus and CK+. Overall, 		Acted Facial Expressions In The Wild (AFEW)
the results are significantly improved 		Acted Facial Expressions In The Wild (AFEW)
by self-attention weights and further 		Acted Facial Expressions In The Wild (AFEW)
improved by the relation- attention 		Acted Facial Expressions In The Wild (AFEW)
weights.  4. CONCLUSION		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
We propose Frame Attention Networks 		Acted Facial Expressions In The Wild (AFEW)
for video-based facial expression recognition. 		Acted Facial Expressions In The Wild (AFEW)
The FAN contains a self-attention 		Acted Facial Expressions In The Wild (AFEW)
module and a relation-attention module. 		Acted Facial Expressions In The Wild (AFEW)
The experiments on CK+ and 		Acted Facial Expressions In The Wild (AFEW)
AFEW show that our FAN 		Acted Facial Expressions In The Wild (AFEW)
with only self-attention improves the 		Acted Facial Expressions In The Wild (AFEW)
baseline significantly and adding relation- 		Acted Facial Expressions In The Wild (AFEW)
attention further boosts performance. With 		Acted Facial Expressions In The Wild (AFEW)
a visualization on CK+, we 		Acted Facial Expressions In The Wild (AFEW)
demonstrate that our FAN can 		Acted Facial Expressions In The Wild (AFEW)
automatically capture the importance of 		Acted Facial Expressions In The Wild (AFEW)
frames. Our single model achieves 		Acted Facial Expressions In The Wild (AFEW)
performance on par with that 		Acted Facial Expressions In The Wild (AFEW)
of state-of-the-art methods on AFEW 		Acted Facial Expressions In The Wild (AFEW)
and obtains state-of-the-art results on 		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
5. REFERENCES  [1] Gwen Littlewort, Marian Stewart		Acted Facial Expressions In The Wild (AFEW)
 Bartlett, Ian Fasel, Joshua Susskind		Acted Facial Expressions In The Wild (AFEW)
, and Javier Movellan, “Dynamics 		Acted Facial Expressions In The Wild (AFEW)
of facial expression extracted automatically 		Acted Facial Expressions In The Wild (AFEW)
from video,” in IVC, 2006.  [2] Caifeng Shan, Shaogang Gong		Acted Facial Expressions In The Wild (AFEW)
, and Peter W. 		Acted Facial Expressions In The Wild (AFEW)
Mcowan, “Fa- cial expression recognition 		Acted Facial Expressions In The Wild (AFEW)
based on local binary patterns: 		Acted Facial Expressions In The Wild (AFEW)
A comprehensive study,” in IVC, 2009		Acted Facial Expressions In The Wild (AFEW)
.  [3] Yichuan Tang, “Deep learning		Acted Facial Expressions In The Wild (AFEW)
 using linear support vector ma		Acted Facial Expressions In The Wild (AFEW)
- chines,” in CS, 2013.  [4] Sarah Adel Bargal, Emad		Acted Facial Expressions In The Wild (AFEW)
 Barsoum, Cristian Canton Ferrer, and		Acted Facial Expressions In The Wild (AFEW)
 Cha Zhang, “Emotion recognition in		Acted Facial Expressions In The Wild (AFEW)
 the wild from videos using		Acted Facial Expressions In The Wild (AFEW)
 images,” in ACM ICMI, 2016		Acted Facial Expressions In The Wild (AFEW)
.  [5] Boris Knyazev, Roman Shvetsov		Acted Facial Expressions In The Wild (AFEW)
, Natalia Efremova, and Artem 		Acted Facial Expressions In The Wild (AFEW)
Kuharenko, “Convolutional neural networks pretrained 		Acted Facial Expressions In The Wild (AFEW)
on large face recognition datasets 		Acted Facial Expressions In The Wild (AFEW)
for emotion classification from video,” 		Acted Facial Expressions In The Wild (AFEW)
in ACM ICMI, 2017.  [6] Sepp Hochreiter and Jrgen		Acted Facial Expressions In The Wild (AFEW)
 Schmidhuber, “Long short-term memory,” Neural		Acted Facial Expressions In The Wild (AFEW)
 Computation, 1997		Acted Facial Expressions In The Wild (AFEW)
.  [7] Du Tran, Lubomir Bourdev		Acted Facial Expressions In The Wild (AFEW)
, Rob Fergus, Lorenzo Torresani, 		Acted Facial Expressions In The Wild (AFEW)
and Manohar Paluri, “Learning spatiotemporal 		Acted Facial Expressions In The Wild (AFEW)
features with 3d convolutional networks,” 		Acted Facial Expressions In The Wild (AFEW)
in ICCV, 2015.  [8] Yuanliu Liu, Yuanliu Liu		Acted Facial Expressions In The Wild (AFEW)
, Yuanliu Liu, and Yuanliu 		Acted Facial Expressions In The Wild (AFEW)
Liu, “Video-based emotion recognition using 		Acted Facial Expressions In The Wild (AFEW)
cnn-rnn and c3d hy- brid 		Acted Facial Expressions In The Wild (AFEW)
networks,” in ACM ICMI, 2016.  [9] Xi Ouyang, Shigenori Kawaai		Acted Facial Expressions In The Wild (AFEW)
, Ester Gue Hua Goh, 		Acted Facial Expressions In The Wild (AFEW)
Sheng- mei Shen, Wan Ding, 		Acted Facial Expressions In The Wild (AFEW)
Huaiping Ming, and Dong-Yan 		Acted Facial Expressions In The Wild (AFEW)
Huang, “Audio-visual emotion recognition using 		Acted Facial Expressions In The Wild (AFEW)
deep transfer learning and multiple 		Acted Facial Expressions In The Wild (AFEW)
temporal models,” in ACM ICMI, 2017		Acted Facial Expressions In The Wild (AFEW)
.  [10] Valentin Vielzeuf, Stphane Pateux		Acted Facial Expressions In The Wild (AFEW)
, and Frdric Jurie, “Temporal 		Acted Facial Expressions In The Wild (AFEW)
multimodal fusion for video emotion 		Acted Facial Expressions In The Wild (AFEW)
classification in the wild,” in 		Acted Facial Expressions In The Wild (AFEW)
ACM ICMI, 2017.  [11] Jingwei Yan, Wenming Zheng		Acted Facial Expressions In The Wild (AFEW)
, Zhen Cui, Chuangao Tang, 		Acted Facial Expressions In The Wild (AFEW)
Tong Zhang, and Yuan 		Acted Facial Expressions In The Wild (AFEW)
Zong, “Multi-cue fusion for emotion 		Acted Facial Expressions In The Wild (AFEW)
recognition in the wild,” Neurocomputing, 2018		Acted Facial Expressions In The Wild (AFEW)
.  [12] Heechul Jung, Sihaeng Lee		Acted Facial Expressions In The Wild (AFEW)
, Junho Yim, Sunjeong Park, 		Acted Facial Expressions In The Wild (AFEW)
and Junmo Kim, “Joint fine-tuning 		Acted Facial Expressions In The Wild (AFEW)
in deep neural networks for 		Acted Facial Expressions In The Wild (AFEW)
facial expression recognition,” in ICCV, 2015		Acted Facial Expressions In The Wild (AFEW)
.  [13] Samira Ebrahimi Kahou, Christopher		Acted Facial Expressions In The Wild (AFEW)
 Pal, Xavier Bouthillier, Pierre Froumenty		Acted Facial Expressions In The Wild (AFEW)
, Roland Memisevic, Pascal Vincent, 		Acted Facial Expressions In The Wild (AFEW)
Aaron Courville, Yoshua Bengio, and 		Acted Facial Expressions In The Wild (AFEW)
Raul Chandias Ferrari, “Com- bining 		Acted Facial Expressions In The Wild (AFEW)
modality specific deep neural networks 		Acted Facial Expressions In The Wild (AFEW)
for emotion recognition in video,” 		Acted Facial Expressions In The Wild (AFEW)
in ACM ICMI, 2013.  [14] Ashish Vaswani, Noam Shazeer		Acted Facial Expressions In The Wild (AFEW)
, Niki Parmar, Jakob Uszko- 		Acted Facial Expressions In The Wild (AFEW)
reit, Llion Jones, Aidan N 		Acted Facial Expressions In The Wild (AFEW)
Gomez, Łukasz Kaiser, and Illia 		Acted Facial Expressions In The Wild (AFEW)
Polosukhin, “Attention is all you 		Acted Facial Expressions In The Wild (AFEW)
need,” in NIPS, 2017.  [15] Jiaolong Yang, Peiran Ren		Acted Facial Expressions In The Wild (AFEW)
, Dongqing Zhang, Dong Chen, 		Acted Facial Expressions In The Wild (AFEW)
Fang Wen, Hongdong Li, and 		Acted Facial Expressions In The Wild (AFEW)
Gang Hua, “Neural aggregation network 		Acted Facial Expressions In The Wild (AFEW)
for video face recognition.,” in 		Acted Facial Expressions In The Wild (AFEW)
CVPR, 2017.  [16] Flood Sung Yongxin Yang		Acted Facial Expressions In The Wild (AFEW)
, Li Zhang, Tao Xiang, 		Acted Facial Expressions In The Wild (AFEW)
Philip HS Torr, and Timothy 		Acted Facial Expressions In The Wild (AFEW)
M Hospedales, “Learning to compare: 		Acted Facial Expressions In The Wild (AFEW)
Re- lation network for few-shot 		Acted Facial Expressions In The Wild (AFEW)
learning,” in CVPR, 2018.  [17] Patrick Lucey, Jeffrey F		Acted Facial Expressions In The Wild (AFEW)
 Cohn, Takeo Kanade, Jason Saragih		Acted Facial Expressions In The Wild (AFEW)
, Zara Ambadar, and Iain 		Acted Facial Expressions In The Wild (AFEW)
Matthews, “The extended cohn- kanade 		Acted Facial Expressions In The Wild (AFEW)
dataset (ck+): A complete dataset 		Acted Facial Expressions In The Wild (AFEW)
for action unit and emotion-specified 		Acted Facial Expressions In The Wild (AFEW)
expression,” in CVPRW, 2010.  [18] Mengyi Liu, Shiguang Shan		Acted Facial Expressions In The Wild (AFEW)
, Ruiping Wang, and Xilin 		Acted Facial Expressions In The Wild (AFEW)
Chen, “Learning expressionlets on spatio-temporal 		Acted Facial Expressions In The Wild (AFEW)
manifold for dy- namic facial 		Acted Facial Expressions In The Wild (AFEW)
expression recognition,” in CVPR, 2014.  [19] Chieh-Ming Kuo, Shang-Hong Lai		Acted Facial Expressions In The Wild (AFEW)
, and Michel Sarkis, “A 		Acted Facial Expressions In The Wild (AFEW)
compact deep learning model for 		Acted Facial Expressions In The Wild (AFEW)
robust facial expression recognition,” in 		Acted Facial Expressions In The Wild (AFEW)
CVPRW, 2018.  [20] Abhinav Dhall, Amanjot Kaur		Acted Facial Expressions In The Wild (AFEW)
, Roland Goecke, and Tom 		Acted Facial Expressions In The Wild (AFEW)
Gedeon, “Emotiw 2018: Audio-video, student 		Acted Facial Expressions In The Wild (AFEW)
engagement and group-level affect prediction,” 		Acted Facial Expressions In The Wild (AFEW)
arXiv preprint:1808.07773, 2018.  [21] Yandong Guo, Lei Zhang		Acted Facial Expressions In The Wild (AFEW)
, Yuxiao Hu, Xiaodong He, 		Acted Facial Expressions In The Wild (AFEW)
and Jian- feng Gao, “Ms-celeb-1m: 		Acted Facial Expressions In The Wild (AFEW)
A dataset and benchmark for 		Acted Facial Expressions In The Wild (AFEW)
large- scale face recognition,” in 		Acted Facial Expressions In The Wild (AFEW)
ECCV, 2016.  [22] Emad Barsoum, Cha Zhang		Acted Facial Expressions In The Wild (AFEW)
, Cristian Canton Ferrer, and 		Acted Facial Expressions In The Wild (AFEW)
Zhengyou Zhang, “Training deep networks 		Acted Facial Expressions In The Wild (AFEW)
for facial expres- sion recognition 		Acted Facial Expressions In The Wild (AFEW)
with crowd-sourced label distribution,” in 		Acted Facial Expressions In The Wild (AFEW)
ACM ICMI, 2016.  [23] Kaihao Zhang, Yongzhen Huang		Acted Facial Expressions In The Wild (AFEW)
, Yong Du, and Liang 		Acted Facial Expressions In The Wild (AFEW)
Wang, “Facial expression recognition based 		Acted Facial Expressions In The Wild (AFEW)
on deep evolutional spatial-temporal networks,” 		Acted Facial Expressions In The Wild (AFEW)
IEEE TIP, 2017.  [24] Jie Cai, Zibo Meng		Acted Facial Expressions In The Wild (AFEW)
, Ahmed Shehab Khan, Zhiyuan 		Acted Facial Expressions In The Wild (AFEW)
Li, James OReilly, and Yan 		Acted Facial Expressions In The Wild (AFEW)
Tong, “Island loss for learning 		Acted Facial Expressions In The Wild (AFEW)
discriminative features in facial expression 		Acted Facial Expressions In The Wild (AFEW)
recognition,” in FG, 2018.  [25] Karan Sikka, Gaurav Sharma		Acted Facial Expressions In The Wild (AFEW)
, and Marian Bartlett, “Lomo: 		Acted Facial Expressions In The Wild (AFEW)
Latent ordinal model for facial 		Acted Facial Expressions In The Wild (AFEW)
analysis in videos,” in CVPR, 2016		Acted Facial Expressions In The Wild (AFEW)
.  [26] Ziheng Zhou, Guoying Zhao		Acted Facial Expressions In The Wild (AFEW)
, and M. Pietikainen, “Towards 		Acted Facial Expressions In The Wild (AFEW)
a practical lipreading system,” in 		Acted Facial Expressions In The Wild (AFEW)
CVPR, 2011.  [27] Yin Fan, Xiangju Lu		Acted Facial Expressions In The Wild (AFEW)
, Dian Li, and Yuanliu 		Acted Facial Expressions In The Wild (AFEW)
Liu, “Video-based emotion recognition using 		Acted Facial Expressions In The Wild (AFEW)
cnn-rnn and c3d hybrid networks,” 		Acted Facial Expressions In The Wild (AFEW)
in ACM ICMI, 2016.  [28] Anbang Yao, Dongqi Cai		Acted Facial Expressions In The Wild (AFEW)
, Ping Hu, Shandong Wang, 		Acted Facial Expressions In The Wild (AFEW)
Liang Sha, and Yurong 		Acted Facial Expressions In The Wild (AFEW)
Chen, “Holonet: towards robust emotion 		Acted Facial Expressions In The Wild (AFEW)
recognition in the wild,” in 		Acted Facial Expressions In The Wild (AFEW)
ACM ICMI, 2016.  [29] Ping Hu, Dongqi Cai		Acted Facial Expressions In The Wild (AFEW)
, Shandong Wang, Anbang Yao, 		Acted Facial Expressions In The Wild (AFEW)
and Yurong Chen, “Learning supervised 		Acted Facial Expressions In The Wild (AFEW)
scoring ensemble for emo- tion 		Acted Facial Expressions In The Wild (AFEW)
recognition in the wild,” in 		Acted Facial Expressions In The Wild (AFEW)
ACM ICMI, 2017.  [30] Yingruo Fan, Jacqueline CK		Acted Facial Expressions In The Wild (AFEW)
 Lam, and Victor OK Li		Acted Facial Expressions In The Wild (AFEW)
, “Video- based emotion recognition 		Acted Facial Expressions In The Wild (AFEW)
using deeply-supervised neural net- works,” 		Acted Facial Expressions In The Wild (AFEW)
in ACM ICMI, 2018.  [31] Chuanhe Liu, Tianhao Tang		Acted Facial Expressions In The Wild (AFEW)
, Kui Lv, and Minghao 		Acted Facial Expressions In The Wild (AFEW)
Wang, “Multi-feature based emotion recognition 		Acted Facial Expressions In The Wild (AFEW)
for video clips,” in ACM 		Acted Facial Expressions In The Wild (AFEW)
ICMI, 2018		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
1  Introduction		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
2  Frame Attention Networks		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
3  Experiments		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
3.1  Datasets and Implementation Details		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
3.2  Evaluation on CK		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
3.3  Evaluation on AFEW 8.0		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
3.4  Visualization and Hyper-parameters		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
4  Conclusion		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
5  References		Acted Facial Expressions In The Wild (AFEW)
		Acted Facial Expressions In The Wild (AFEW)
extensive experiments on CK+ and AFEW8	AFEW	Acted Facial Expressions In The Wild (AFEW)
recognition, frame attention networks, CNN, AFEW	AFEW	Acted Facial Expressions In The Wild (AFEW)
extensive experiments on CK+ and AFEW8	AFEW	Acted Facial Expressions In The Wild (AFEW)
AFEW 8.0 [20] served as an	AFEW	Acted Facial Expressions In The Wild (AFEW)
labels are in- cluded in AFEW, i.e. anger, disgust, fear, happiness	AFEW	Acted Facial Expressions In The Wild (AFEW)
, sadness, surprise and neutral. AFEW contains video clips collected from	AFEW	Acted Facial Expressions In The Wild (AFEW)
head poses, occlusions, and illuminations. AFEW 8.0 is divided into three	AFEW	Acted Facial Expressions In The Wild (AFEW)
training, on both CK+ and AFEW 8.0, we set a batch	AFEW	Acted Facial Expressions In The Wild (AFEW)
training after 60 epochs. On AFEW 8.0, we initialize the lr	AFEW	Acted Facial Expressions In The Wild (AFEW)
3.3. Evaluation on AFEW 8.0	AFEW	Acted Facial Expressions In The Wild (AFEW)
From the view of performance, AFEW is one of the most	AFEW	Acted Facial Expressions In The Wild (AFEW)
evaluation of our FAN on AFEW with comparisons to recent state-of-the-art	AFEW	Acted Facial Expressions In The Wild (AFEW)
to state- of-the-art methods on AFEW 8.0 database. It is worth	AFEW	Acted Facial Expressions In The Wild (AFEW)
The experiments on CK+ and AFEW show that our FAN with	AFEW	Acted Facial Expressions In The Wild (AFEW)
that of state-of-the-art methods on AFEW and obtains state-of-the-art results on	AFEW	Acted Facial Expressions In The Wild (AFEW)
Evaluation on AFEW 8.0	AFEW	Acted Facial Expressions In The Wild (AFEW)
is crucial for this task. In this paper, we propose the	In	Acted Facial Expressions In The Wild (AFEW)
Pro- gram (JCYJ20170818164704758, JSGG20180507182100698), and In	In	Acted Facial Expressions In The Wild (AFEW)
category while the others not. In this paper, inspired by the	In	Acted Facial Expressions In The Wild (AFEW)
I2, · · · , In, and the facial frame features	In	Acted Facial Expressions In The Wild (AFEW)
Eq. (4)) of our FAN. In total, both kinds of weights	In	Acted Facial Expressions In The Wild (AFEW)
ABSTRACT The video-based facial expression recognition aims	The	Acted Facial Expressions In The Wild (AFEW)
frames in an end-to-end framework. The network takes a video with	The	Acted Facial Expressions In The Wild (AFEW)
and produces a fixed-dimension representation. The whole network is com- posed	The	Acted Facial Expressions In The Wild (AFEW)
of two modules. The feature embedding module is a	The	Acted Facial Expressions In The Wild (AFEW)
face images into feature vectors. The frame attention module learns multiple	The	Acted Facial Expressions In The Wild (AFEW)
or motion information in videos. The Long Short-Term Mem- ory (LSTM	The	Acted Facial Expressions In The Wild (AFEW)
to adaptively aggregate frame features. The FAN is designed to learn	The	Acted Facial Expressions In The Wild (AFEW)
reasoning in an end-to-end fashion. The self-attention kernels are directly learned	The	Acted Facial Expressions In The Wild (AFEW)
fixed-dimension feature representation for FER. The whole network consists of two	The	Acted Facial Expressions In The Wild (AFEW)
module and frame attention module. The feature embedding module is a	The	Acted Facial Expressions In The Wild (AFEW)
image into a feature vector. The frame attention module learns two-level	The	Acted Facial Expressions In The Wild (AFEW)
local features is more reliable. The self-attention weights are learned with	The	Acted Facial Expressions In The Wild (AFEW)
relation-attention weights for frame features. The relation-attention weight of the i-th	The	Acted Facial Expressions In The Wild (AFEW)
The last three frames and the	The	Acted Facial Expressions In The Wild (AFEW)
The last three frames and the	The	Acted Facial Expressions In The Wild (AFEW)
most challenging videos FER dataset. The EmotiW challenge shares the same	The	Acted Facial Expressions In The Wild (AFEW)
for video-based facial expression recognition. The FAN contains a self-attention module	The	Acted Facial Expressions In The Wild (AFEW)
and a relation-attention module. The experiments on CK+ and AFEW	The	Acted Facial Expressions In The Wild (AFEW)
The extended cohn- kanade dataset (ck	The	Acted Facial Expressions In The Wild (AFEW)
extensive experiments on CK+ and AFEW8	(AFEW)	Acted Facial Expressions In The Wild (AFEW)
recognition, frame attention networks, CNN, AFEW	(AFEW)	Acted Facial Expressions In The Wild (AFEW)
extensive experiments on CK+ and AFEW8	(AFEW)	Acted Facial Expressions In The Wild (AFEW)
AFEW 8.0 [20] served as an	(AFEW)	Acted Facial Expressions In The Wild (AFEW)
labels are in- cluded in AFEW, i.e. anger, disgust, fear, happiness	(AFEW)	Acted Facial Expressions In The Wild (AFEW)
, sadness, surprise and neutral. AFEW contains video clips collected from	(AFEW)	Acted Facial Expressions In The Wild (AFEW)
head poses, occlusions, and illuminations. AFEW 8.0 is divided into three	(AFEW)	Acted Facial Expressions In The Wild (AFEW)
training, on both CK+ and AFEW 8.0, we set a batch	(AFEW)	Acted Facial Expressions In The Wild (AFEW)
training after 60 epochs. On AFEW 8.0, we initialize the lr	(AFEW)	Acted Facial Expressions In The Wild (AFEW)
3.3. Evaluation on AFEW 8.0	(AFEW)	Acted Facial Expressions In The Wild (AFEW)
From the view of performance, AFEW is one of the most	(AFEW)	Acted Facial Expressions In The Wild (AFEW)
evaluation of our FAN on AFEW with comparisons to recent state-of-the-art	(AFEW)	Acted Facial Expressions In The Wild (AFEW)
to state- of-the-art methods on AFEW 8.0 database. It is worth	(AFEW)	Acted Facial Expressions In The Wild (AFEW)
The experiments on CK+ and AFEW show that our FAN with	(AFEW)	Acted Facial Expressions In The Wild (AFEW)
that of state-of-the-art methods on AFEW and obtains state-of-the-art results on	(AFEW)	Acted Facial Expressions In The Wild (AFEW)
Evaluation on AFEW 8.0	(AFEW)	Acted Facial Expressions In The Wild (AFEW)
FRAME ATTENTION NETWORKS FOR FACIAL 		The Extended Cohn-Kanade Dataset(CK+)
EXPRESSION RECOGNITION IN VIDEOS  Debin Meng, Xiaojiang Peng∗, Kai		The Extended Cohn-Kanade Dataset(CK+)
 Wang, Yu Qiao		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
Shenzhen Institutes of Advanced Technology, 		The Extended Cohn-Kanade Dataset(CK+)
Chinese Academy of Science, Shenzhen, 		The Extended Cohn-Kanade Dataset(CK+)
China Shenzhen Key Lab of 		The Extended Cohn-Kanade Dataset(CK+)
Computer Vision and Pattern Recognition, 		The Extended Cohn-Kanade Dataset(CK+)
Shenzhen, China  University of Chinese Academy of		The Extended Cohn-Kanade Dataset(CK+)
 Sciences, Beijing, China michaeldbmeng19@outlook.com, {xj.peng		The Extended Cohn-Kanade Dataset(CK+)
, kai.wang, yu.qiao}@siat.ac.cn  ABSTRACT The video-based facial expression		The Extended Cohn-Kanade Dataset(CK+)
 recognition aims to classify a		The Extended Cohn-Kanade Dataset(CK+)
 given video into several basic		The Extended Cohn-Kanade Dataset(CK+)
 emotions. How to integrate facial		The Extended Cohn-Kanade Dataset(CK+)
 features of individual frames is		The Extended Cohn-Kanade Dataset(CK+)
 crucial for this task. In		The Extended Cohn-Kanade Dataset(CK+)
 this paper, we propose the		The Extended Cohn-Kanade Dataset(CK+)
 Frame Attention Networks (FAN)1, to		The Extended Cohn-Kanade Dataset(CK+)
 automatically highlight some discriminative frames		The Extended Cohn-Kanade Dataset(CK+)
 in an end-to-end framework. The		The Extended Cohn-Kanade Dataset(CK+)
 network takes a video with		The Extended Cohn-Kanade Dataset(CK+)
 a variable number of face		The Extended Cohn-Kanade Dataset(CK+)
 images as its input and		The Extended Cohn-Kanade Dataset(CK+)
 produces a fixed-dimension representation. The		The Extended Cohn-Kanade Dataset(CK+)
 whole network is com- posed		The Extended Cohn-Kanade Dataset(CK+)
 of two modules. The feature		The Extended Cohn-Kanade Dataset(CK+)
 embedding module is a deep		The Extended Cohn-Kanade Dataset(CK+)
 Convolutional Neural Network (CNN) which		The Extended Cohn-Kanade Dataset(CK+)
 embeds face images into feature		The Extended Cohn-Kanade Dataset(CK+)
 vectors. The frame attention module		The Extended Cohn-Kanade Dataset(CK+)
 learns multiple attention weights which		The Extended Cohn-Kanade Dataset(CK+)
 are used to adaptively aggregate		The Extended Cohn-Kanade Dataset(CK+)
 the feature vectors to form		The Extended Cohn-Kanade Dataset(CK+)
 a single discriminative video representation		The Extended Cohn-Kanade Dataset(CK+)
. We conduct extensive experiments 		The Extended Cohn-Kanade Dataset(CK+)
on CK+ and AFEW8.0 datasets. 		The Extended Cohn-Kanade Dataset(CK+)
Our proposed FAN shows su- 		The Extended Cohn-Kanade Dataset(CK+)
perior performance compared to other 		The Extended Cohn-Kanade Dataset(CK+)
CNN based methods and achieves 		The Extended Cohn-Kanade Dataset(CK+)
state-of-the-art performance on CK+.  Index Terms— facial expression recognition		The Extended Cohn-Kanade Dataset(CK+)
, audio- video emotion recognition, 		The Extended Cohn-Kanade Dataset(CK+)
frame attention networks, CNN, AFEW  1. INTRODUCTION		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
Automatic facial expression recognition (FER) 		The Extended Cohn-Kanade Dataset(CK+)
has recently attracted increasing attention 		The Extended Cohn-Kanade Dataset(CK+)
in academia and industry due 		The Extended Cohn-Kanade Dataset(CK+)
to its wide range of 		The Extended Cohn-Kanade Dataset(CK+)
applications such as affective computing, 		The Extended Cohn-Kanade Dataset(CK+)
intelligent environments, and multimodal human-computer 		The Extended Cohn-Kanade Dataset(CK+)
interface (HCI). Though great progress 		The Extended Cohn-Kanade Dataset(CK+)
have been made re- cently, 		The Extended Cohn-Kanade Dataset(CK+)
facial expression recognition in the 		The Extended Cohn-Kanade Dataset(CK+)
wild remains a challenging problem 		The Extended Cohn-Kanade Dataset(CK+)
due to large head pose, 		The Extended Cohn-Kanade Dataset(CK+)
illumination variance, occlusion, motion blur, 		The Extended Cohn-Kanade Dataset(CK+)
etc.  Video-based facial expression recognition aims		The Extended Cohn-Kanade Dataset(CK+)
 to classify a video into		The Extended Cohn-Kanade Dataset(CK+)
 several basic emotions, such as		The Extended Cohn-Kanade Dataset(CK+)
 happy, angry, dis		The Extended Cohn-Kanade Dataset(CK+)
-  ∗Xiaojiang Peng is the corresponding		The Extended Cohn-Kanade Dataset(CK+)
 author. Email: xj.peng@siat.ac.cn This work		The Extended Cohn-Kanade Dataset(CK+)
 was supported by the National		The Extended Cohn-Kanade Dataset(CK+)
 Natural Science Foun		The Extended Cohn-Kanade Dataset(CK+)
-  dation of China (U1613211, U1713208		The Extended Cohn-Kanade Dataset(CK+)
), Shenzhen Research Pro- gram (		The Extended Cohn-Kanade Dataset(CK+)
JCYJ20170818164704758, JSGG20180507182100698), and In- ternational 		The Extended Cohn-Kanade Dataset(CK+)
Partnership Program of Chinese Academy 		The Extended Cohn-Kanade Dataset(CK+)
of Sciences (172644KYSB20150019).  1Code is available at https://github.com/Open-Debin/Emotion-FAN		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
gust, fear, sad, neutral, and 		The Extended Cohn-Kanade Dataset(CK+)
surprise. Given a video, the 		The Extended Cohn-Kanade Dataset(CK+)
pop- ular FER pipeline with 		The Extended Cohn-Kanade Dataset(CK+)
a visual clue (FER with 		The Extended Cohn-Kanade Dataset(CK+)
an audio clue is out 		The Extended Cohn-Kanade Dataset(CK+)
of the scope of this 		The Extended Cohn-Kanade Dataset(CK+)
paper) mainly includes three steps, 		The Extended Cohn-Kanade Dataset(CK+)
namely frame preprocessing, feature extraction, 		The Extended Cohn-Kanade Dataset(CK+)
and classi- fication. Especially, frame 		The Extended Cohn-Kanade Dataset(CK+)
preprocessing refers to face de- 		The Extended Cohn-Kanade Dataset(CK+)
tection, alignment, illumination normalizing and 		The Extended Cohn-Kanade Dataset(CK+)
so on. Fea- ture extraction 		The Extended Cohn-Kanade Dataset(CK+)
or video representation is the 		The Extended Cohn-Kanade Dataset(CK+)
key part for FER which 		The Extended Cohn-Kanade Dataset(CK+)
encodes frames or sequences into 		The Extended Cohn-Kanade Dataset(CK+)
compact feature vec- tors. These 		The Extended Cohn-Kanade Dataset(CK+)
feature vectors are subsequently fed 		The Extended Cohn-Kanade Dataset(CK+)
into a classi- fier for 		The Extended Cohn-Kanade Dataset(CK+)
prediction.  Feature extraction methods for video-based		The Extended Cohn-Kanade Dataset(CK+)
 FER can be roughly divided		The Extended Cohn-Kanade Dataset(CK+)
 into three types: static-based methods		The Extended Cohn-Kanade Dataset(CK+)
, spatial-temporal methods, and geometry-based 		The Extended Cohn-Kanade Dataset(CK+)
methods.  Static-based feature extraction methods mainly		The Extended Cohn-Kanade Dataset(CK+)
 inherit those methods from static		The Extended Cohn-Kanade Dataset(CK+)
 image emotion recognition which can		The Extended Cohn-Kanade Dataset(CK+)
 be both hand-crafted [1, 2		The Extended Cohn-Kanade Dataset(CK+)
] and learned [3, 4, 5		The Extended Cohn-Kanade Dataset(CK+)
]. For the hand-crafted features, 		The Extended Cohn-Kanade Dataset(CK+)
Littlewort et al. [1] propose 		The Extended Cohn-Kanade Dataset(CK+)
to use a bank of 		The Extended Cohn-Kanade Dataset(CK+)
2D Gabor filters to extract 		The Extended Cohn-Kanade Dataset(CK+)
facial features for video- based 		The Extended Cohn-Kanade Dataset(CK+)
FER. Shan et al. [2] 		The Extended Cohn-Kanade Dataset(CK+)
use local binary patterns (LBP) 		The Extended Cohn-Kanade Dataset(CK+)
and LBP histogram for facial 		The Extended Cohn-Kanade Dataset(CK+)
feature extraction. For the learned 		The Extended Cohn-Kanade Dataset(CK+)
features, Tang [3] utilizes deep 		The Extended Cohn-Kanade Dataset(CK+)
CNNs for feature extraction, and 		The Extended Cohn-Kanade Dataset(CK+)
win the FER2013. Some winners 		The Extended Cohn-Kanade Dataset(CK+)
in audio-video emotion recognition task 		The Extended Cohn-Kanade Dataset(CK+)
of EmotiW2016 and EmotiW2017 only 		The Extended Cohn-Kanade Dataset(CK+)
use static facial features from 		The Extended Cohn-Kanade Dataset(CK+)
deep CNNs trained on large 		The Extended Cohn-Kanade Dataset(CK+)
face datasets or trained with 		The Extended Cohn-Kanade Dataset(CK+)
multi-level supervision [4, 5].  Spatial-temporal methods aim to model		The Extended Cohn-Kanade Dataset(CK+)
 the temporal or motion information		The Extended Cohn-Kanade Dataset(CK+)
 in videos. The Long Short-Term		The Extended Cohn-Kanade Dataset(CK+)
 Mem- ory (LSTM) [6], and		The Extended Cohn-Kanade Dataset(CK+)
 C3D [7] are two widely-used		The Extended Cohn-Kanade Dataset(CK+)
 spatial- temporal methods for video-based		The Extended Cohn-Kanade Dataset(CK+)
 FER. LSTM derives in- formation		The Extended Cohn-Kanade Dataset(CK+)
 from sequences by exploiting the		The Extended Cohn-Kanade Dataset(CK+)
 fact that feature vectors are		The Extended Cohn-Kanade Dataset(CK+)
 connected semantically for successive data		The Extended Cohn-Kanade Dataset(CK+)
. This pipeline is widely-used 		The Extended Cohn-Kanade Dataset(CK+)
in the EmotiW challenge, 		The Extended Cohn-Kanade Dataset(CK+)
e.g. [8, 9, 10, 11]. 		The Extended Cohn-Kanade Dataset(CK+)
C3D, which is originally developed 		The Extended Cohn-Kanade Dataset(CK+)
for video action recognition, is 		The Extended Cohn-Kanade Dataset(CK+)
also popular in the EmotiW 		The Extended Cohn-Kanade Dataset(CK+)
challenge.  Geometry based methods [12, 11		The Extended Cohn-Kanade Dataset(CK+)
] aim to model the 		The Extended Cohn-Kanade Dataset(CK+)
mo- tions of key points 		The Extended Cohn-Kanade Dataset(CK+)
in faces which only leverage 		The Extended Cohn-Kanade Dataset(CK+)
the geometry locations of facial 		The Extended Cohn-Kanade Dataset(CK+)
landmarks in every video frames. 		The Extended Cohn-Kanade Dataset(CK+)
Jung et al. [12] propose 		The Extended Cohn-Kanade Dataset(CK+)
a deep temporal appearance-geometry net- 		The Extended Cohn-Kanade Dataset(CK+)
work (DTAGN) which first alternately 		The Extended Cohn-Kanade Dataset(CK+)
concatenates the x- coordinates and 		The Extended Cohn-Kanade Dataset(CK+)
y-coordinates of the facial landmark 		The Extended Cohn-Kanade Dataset(CK+)
points  Copyright 2019 IEEE. Published in		The Extended Cohn-Kanade Dataset(CK+)
 the IEEE 2019 International Conference		The Extended Cohn-Kanade Dataset(CK+)
 on Image Processing (ICIP 2019		The Extended Cohn-Kanade Dataset(CK+)
), scheduled for 22-25 September 2019 in Taipei, Taiwan. Personal use		The Extended Cohn-Kanade Dataset(CK+)
 of this material is permitted		The Extended Cohn-Kanade Dataset(CK+)
. However, permission to reprint/republish 		The Extended Cohn-Kanade Dataset(CK+)
this material for advertising or 		The Extended Cohn-Kanade Dataset(CK+)
promotional purposes or for creating 		The Extended Cohn-Kanade Dataset(CK+)
new collective works for resale 		The Extended Cohn-Kanade Dataset(CK+)
or redistribution to servers or 		The Extended Cohn-Kanade Dataset(CK+)
lists, or to reuse any 		The Extended Cohn-Kanade Dataset(CK+)
copyrighted component of this work 		The Extended Cohn-Kanade Dataset(CK+)
in other works, must be 		The Extended Cohn-Kanade Dataset(CK+)
obtained from the IEEE. Contact: 		The Extended Cohn-Kanade Dataset(CK+)
Manager, Copyrights and Permissions / 		The Extended Cohn-Kanade Dataset(CK+)
IEEE Service Center / 445 		The Extended Cohn-Kanade Dataset(CK+)
Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA		The Extended Cohn-Kanade Dataset(CK+)
. Telephone: + Intl. 908-562-3966.  ar X		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
iv :1  90 7		The Extended Cohn-Kanade Dataset(CK+)
.  00 19		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
3v 2		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
cs  .C V		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
1  2		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
Se  p		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
20  19		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
from each frame after normalization 		The Extended Cohn-Kanade Dataset(CK+)
and then concatenates these normalized 		The Extended Cohn-Kanade Dataset(CK+)
points over time for a 		The Extended Cohn-Kanade Dataset(CK+)
one-dimensional tra- jectory signal of 		The Extended Cohn-Kanade Dataset(CK+)
each sequence. Yan et 		The Extended Cohn-Kanade Dataset(CK+)
al. [11] construct an image-like 		The Extended Cohn-Kanade Dataset(CK+)
map by stretching all the 		The Extended Cohn-Kanade Dataset(CK+)
normalized facial point trajectories in 		The Extended Cohn-Kanade Dataset(CK+)
a sequence together as the 		The Extended Cohn-Kanade Dataset(CK+)
input of a CNN.  Among all the above methods		The Extended Cohn-Kanade Dataset(CK+)
, static-based methods are superior 		The Extended Cohn-Kanade Dataset(CK+)
to the others according to 		The Extended Cohn-Kanade Dataset(CK+)
several winner solutions in EmotiW 		The Extended Cohn-Kanade Dataset(CK+)
challenges. To obtain a video-level 		The Extended Cohn-Kanade Dataset(CK+)
result with varied frames, a 		The Extended Cohn-Kanade Dataset(CK+)
frame aggregation operation is necessary 		The Extended Cohn-Kanade Dataset(CK+)
for static- based methods. For 		The Extended Cohn-Kanade Dataset(CK+)
frame aggregation, Kahou et 		The Extended Cohn-Kanade Dataset(CK+)
al. [13] concatenate the n-class 		The Extended Cohn-Kanade Dataset(CK+)
probability vectors of 10 segments 		The Extended Cohn-Kanade Dataset(CK+)
to form a fixed-length video 		The Extended Cohn-Kanade Dataset(CK+)
representation by frame averag- ing 		The Extended Cohn-Kanade Dataset(CK+)
or frame expansion. Bargal et 		The Extended Cohn-Kanade Dataset(CK+)
al. [4] propose a statistical 		The Extended Cohn-Kanade Dataset(CK+)
encoding module (STAT) to aggregate 		The Extended Cohn-Kanade Dataset(CK+)
frame features which compute the 		The Extended Cohn-Kanade Dataset(CK+)
mean, variance, minimum, and maximum 		The Extended Cohn-Kanade Dataset(CK+)
of the frame feature vectors.  One limitation of these existing		The Extended Cohn-Kanade Dataset(CK+)
 aggregation methods is that they		The Extended Cohn-Kanade Dataset(CK+)
 ignore the importance of frames		The Extended Cohn-Kanade Dataset(CK+)
 for FER. For example, some		The Extended Cohn-Kanade Dataset(CK+)
 faces in Figure 1 are		The Extended Cohn-Kanade Dataset(CK+)
 representative for the ‘happy’ category		The Extended Cohn-Kanade Dataset(CK+)
 while the others not. In		The Extended Cohn-Kanade Dataset(CK+)
 this paper, inspired by the		The Extended Cohn-Kanade Dataset(CK+)
 attention mechanism [14] of machine		The Extended Cohn-Kanade Dataset(CK+)
 translation and the neural aggregation		The Extended Cohn-Kanade Dataset(CK+)
 networks [15] of video face		The Extended Cohn-Kanade Dataset(CK+)
 recog- nition, we propose the		The Extended Cohn-Kanade Dataset(CK+)
 Frame Attention Networks (FAN) to		The Extended Cohn-Kanade Dataset(CK+)
 adaptively aggregate frame features. The		The Extended Cohn-Kanade Dataset(CK+)
 FAN is designed to learn		The Extended Cohn-Kanade Dataset(CK+)
 self-attention kernels and relation-attention kernels		The Extended Cohn-Kanade Dataset(CK+)
 for frame importance reasoning in		The Extended Cohn-Kanade Dataset(CK+)
 an end-to-end fashion. The self-attention		The Extended Cohn-Kanade Dataset(CK+)
 kernels are directly learned from		The Extended Cohn-Kanade Dataset(CK+)
 frame features while the relation-attention		The Extended Cohn-Kanade Dataset(CK+)
 kernels are learned from the		The Extended Cohn-Kanade Dataset(CK+)
 concatenated features of a video-level		The Extended Cohn-Kanade Dataset(CK+)
 anchor feature and frame features		The Extended Cohn-Kanade Dataset(CK+)
. We conduct extensive experiments 		The Extended Cohn-Kanade Dataset(CK+)
on CK+ and AFEW8.0 (EmotiW2018) 		The Extended Cohn-Kanade Dataset(CK+)
datasets. Our proposed FAN shows 		The Extended Cohn-Kanade Dataset(CK+)
superior performance compared to other 		The Extended Cohn-Kanade Dataset(CK+)
CNN based methods with only 		The Extended Cohn-Kanade Dataset(CK+)
facial features and achieves state-of-the- 		The Extended Cohn-Kanade Dataset(CK+)
art performance on CK+.  2. FRAME ATTENTION NETWORKS		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
We propose Frame Attention Networks (		The Extended Cohn-Kanade Dataset(CK+)
FAN) for video- based facial 		The Extended Cohn-Kanade Dataset(CK+)
expression recognition (FER). Figure 1 		The Extended Cohn-Kanade Dataset(CK+)
illus- trates the framework of 		The Extended Cohn-Kanade Dataset(CK+)
our proposed FAN. It takes 		The Extended Cohn-Kanade Dataset(CK+)
a facial video with a 		The Extended Cohn-Kanade Dataset(CK+)
variable number of face images 		The Extended Cohn-Kanade Dataset(CK+)
as its input and produces 		The Extended Cohn-Kanade Dataset(CK+)
a fixed-dimension feature representation for 		The Extended Cohn-Kanade Dataset(CK+)
FER. The whole network consists 		The Extended Cohn-Kanade Dataset(CK+)
of two modules: feature em- 		The Extended Cohn-Kanade Dataset(CK+)
bedding module and frame attention 		The Extended Cohn-Kanade Dataset(CK+)
module. The feature embedding module 		The Extended Cohn-Kanade Dataset(CK+)
is a deep CNN which 		The Extended Cohn-Kanade Dataset(CK+)
embeds each face image into 		The Extended Cohn-Kanade Dataset(CK+)
a feature vector. The frame 		The Extended Cohn-Kanade Dataset(CK+)
attention module learns two-level attention 		The Extended Cohn-Kanade Dataset(CK+)
weights, i.e. self-attention weights and 		The Extended Cohn-Kanade Dataset(CK+)
relation-attention weights, which are used 		The Extended Cohn-Kanade Dataset(CK+)
to adaptively aggregate the feature 		The Extended Cohn-Kanade Dataset(CK+)
vectors to form a single 		The Extended Cohn-Kanade Dataset(CK+)
discriminative video representation.  Formally, we denote a video		The Extended Cohn-Kanade Dataset(CK+)
 with n frames as V		The Extended Cohn-Kanade Dataset(CK+)
, and its frames as 		The Extended Cohn-Kanade Dataset(CK+)
I1, I2, · · · , In, and the facial frame		The Extended Cohn-Kanade Dataset(CK+)
 features are {f1		The Extended Cohn-Kanade Dataset(CK+)
, · · · , 		The Extended Cohn-Kanade Dataset(CK+)
fn}.  Self-attention weights. With individual frame		The Extended Cohn-Kanade Dataset(CK+)
 features		The Extended Cohn-Kanade Dataset(CK+)
,  Fig. 1. Our proposed frame		The Extended Cohn-Kanade Dataset(CK+)
 attention network architecture		The Extended Cohn-Kanade Dataset(CK+)
.  our FAN first applies a		The Extended Cohn-Kanade Dataset(CK+)
 FC layer and a sigmoid		The Extended Cohn-Kanade Dataset(CK+)
 function to assign coarse self-attention		The Extended Cohn-Kanade Dataset(CK+)
 weights. Mathematically, the self- attention		The Extended Cohn-Kanade Dataset(CK+)
 weight of the i-th frame		The Extended Cohn-Kanade Dataset(CK+)
 is defined by		The Extended Cohn-Kanade Dataset(CK+)
:  αi = σ(f T i		The Extended Cohn-Kanade Dataset(CK+)
 q		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
0) (1)  where q0 is the parameter		The Extended Cohn-Kanade Dataset(CK+)
 of FC, σ denotes the		The Extended Cohn-Kanade Dataset(CK+)
 sigmoid func- tion. With these		The Extended Cohn-Kanade Dataset(CK+)
 self-attention weights, we aggregate all		The Extended Cohn-Kanade Dataset(CK+)
 the input frame features into		The Extended Cohn-Kanade Dataset(CK+)
 a global representation f ′v		The Extended Cohn-Kanade Dataset(CK+)
 as follows		The Extended Cohn-Kanade Dataset(CK+)
,  f ′v		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
n i=1 αifi∑n i=1 αi  . (2		The Extended Cohn-Kanade Dataset(CK+)
)  We use f ′v as		The Extended Cohn-Kanade Dataset(CK+)
 a video-level global anchor for		The Extended Cohn-Kanade Dataset(CK+)
 learning fur- ther accurate relation-attention		The Extended Cohn-Kanade Dataset(CK+)
 weights		The Extended Cohn-Kanade Dataset(CK+)
.  Relation-attention weights. We believe that		The Extended Cohn-Kanade Dataset(CK+)
 learning weights from both a		The Extended Cohn-Kanade Dataset(CK+)
 global feature and local features		The Extended Cohn-Kanade Dataset(CK+)
 is more reliable. The self-attention		The Extended Cohn-Kanade Dataset(CK+)
 weights are learned with indi		The Extended Cohn-Kanade Dataset(CK+)
- vidual frame features and 		The Extended Cohn-Kanade Dataset(CK+)
non-linear mapping, which are rather 		The Extended Cohn-Kanade Dataset(CK+)
coarse. Since f ′v inherently 		The Extended Cohn-Kanade Dataset(CK+)
contains the contents of the 		The Extended Cohn-Kanade Dataset(CK+)
whole video, the attention weights 		The Extended Cohn-Kanade Dataset(CK+)
can be further refined by 		The Extended Cohn-Kanade Dataset(CK+)
modeling the relation between frame 		The Extended Cohn-Kanade Dataset(CK+)
features and this global representation 		The Extended Cohn-Kanade Dataset(CK+)
f ′v .  Inspired by the relation-Net in		The Extended Cohn-Kanade Dataset(CK+)
 low-shot learning [16], we use		The Extended Cohn-Kanade Dataset(CK+)
 the sample concatenation and another		The Extended Cohn-Kanade Dataset(CK+)
 FC layer to esti- mate		The Extended Cohn-Kanade Dataset(CK+)
 new relation-attention weights for frame		The Extended Cohn-Kanade Dataset(CK+)
 features. The relation-attention weight of		The Extended Cohn-Kanade Dataset(CK+)
 the i-th frame is formulated		The Extended Cohn-Kanade Dataset(CK+)
 as		The Extended Cohn-Kanade Dataset(CK+)
,  βi = σ([fi : f		The Extended Cohn-Kanade Dataset(CK+)
 ′ v		The Extended Cohn-Kanade Dataset(CK+)
]  Tq1), (3		The Extended Cohn-Kanade Dataset(CK+)
)  where q1 is the parameter		The Extended Cohn-Kanade Dataset(CK+)
 of FC, σ denotes the		The Extended Cohn-Kanade Dataset(CK+)
 sigmoid func- tion		The Extended Cohn-Kanade Dataset(CK+)
.  Finally, with self-attention and relation-attention		The Extended Cohn-Kanade Dataset(CK+)
 weights, our FAN aggregates all		The Extended Cohn-Kanade Dataset(CK+)
 the frame features into a		The Extended Cohn-Kanade Dataset(CK+)
 new compact feature as		The Extended Cohn-Kanade Dataset(CK+)
,  fv		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
n i=0 αiβi[fi : f  ′ v]∑n		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
i=0 αiβi . (4		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
3. EXPERIMENTS  3.1. Datasets and Implementation Details		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
CK+ [17] contains 593 video 		The Extended Cohn-Kanade Dataset(CK+)
sequences from 123 subjects. Among 		The Extended Cohn-Kanade Dataset(CK+)
these videos, 327 sequences from 118 subjects are la- beled with		The Extended Cohn-Kanade Dataset(CK+)
 seven basic expression labels, i.e		The Extended Cohn-Kanade Dataset(CK+)
. anger, contempt, disgust, fear, 		The Extended Cohn-Kanade Dataset(CK+)
happiness, sadness, and surprise. Since 		The Extended Cohn-Kanade Dataset(CK+)
CK+ does not provide training/testing 		The Extended Cohn-Kanade Dataset(CK+)
splits, most of the algorithms 		The Extended Cohn-Kanade Dataset(CK+)
evaluated on this database with 10		The Extended Cohn-Kanade Dataset(CK+)
-fold person-independence cross-validation experiments. We 		The Extended Cohn-Kanade Dataset(CK+)
constructed 10 subsets by sampling 		The Extended Cohn-Kanade Dataset(CK+)
ID in ascending order with 		The Extended Cohn-Kanade Dataset(CK+)
a step size of 10 		The Extended Cohn-Kanade Dataset(CK+)
as in several previous 		The Extended Cohn-Kanade Dataset(CK+)
works [18, 19], and report 		The Extended Cohn-Kanade Dataset(CK+)
the overall accu- racy over 10 folds		The Extended Cohn-Kanade Dataset(CK+)
.  AFEW 8.0 [20] served as		The Extended Cohn-Kanade Dataset(CK+)
 an evaluation platform for the		The Extended Cohn-Kanade Dataset(CK+)
 annual EmotiW since 2013. Seven		The Extended Cohn-Kanade Dataset(CK+)
 emotion labels are in- cluded		The Extended Cohn-Kanade Dataset(CK+)
 in AFEW, i.e. anger, disgust		The Extended Cohn-Kanade Dataset(CK+)
, fear, happiness, sadness, surprise 		The Extended Cohn-Kanade Dataset(CK+)
and neutral. AFEW contains video 		The Extended Cohn-Kanade Dataset(CK+)
clips collected from different movies 		The Extended Cohn-Kanade Dataset(CK+)
and TV serials with spontaneous 		The Extended Cohn-Kanade Dataset(CK+)
ex- pressions, various head poses, 		The Extended Cohn-Kanade Dataset(CK+)
occlusions, and illuminations. AFEW 8.0 		The Extended Cohn-Kanade Dataset(CK+)
is divided into three splits: 		The Extended Cohn-Kanade Dataset(CK+)
Train (773 samples), Val (383 		The Extended Cohn-Kanade Dataset(CK+)
samples) and Test (653 samples), 		The Extended Cohn-Kanade Dataset(CK+)
which ensures data in the 		The Extended Cohn-Kanade Dataset(CK+)
three sets belong to mutually 		The Extended Cohn-Kanade Dataset(CK+)
exclusive movies and ac- tors. 		The Extended Cohn-Kanade Dataset(CK+)
Since the test split is 		The Extended Cohn-Kanade Dataset(CK+)
not publicly available, we train 		The Extended Cohn-Kanade Dataset(CK+)
our model on training split 		The Extended Cohn-Kanade Dataset(CK+)
and report results on validation 		The Extended Cohn-Kanade Dataset(CK+)
split.  Implementation details. We preprocess video		The Extended Cohn-Kanade Dataset(CK+)
 frames by face detection and		The Extended Cohn-Kanade Dataset(CK+)
 alignment in the Dlib toolbox		The Extended Cohn-Kanade Dataset(CK+)
 We extend the face bounding		The Extended Cohn-Kanade Dataset(CK+)
 box with a ratio of		The Extended Cohn-Kanade Dataset(CK+)
 25% and then resize the		The Extended Cohn-Kanade Dataset(CK+)
 cropped faces to scale of		The Extended Cohn-Kanade Dataset(CK+)
 224×224. We implement our method		The Extended Cohn-Kanade Dataset(CK+)
 by the Pytorch toolbox. By		The Extended Cohn-Kanade Dataset(CK+)
 default, for feature em- bedding		The Extended Cohn-Kanade Dataset(CK+)
, we use the ResNet18 		The Extended Cohn-Kanade Dataset(CK+)
which is pre-trained on MS- 		The Extended Cohn-Kanade Dataset(CK+)
Celeb-1M [21] face recognition dataset 		The Extended Cohn-Kanade Dataset(CK+)
and FER Plus expres- sion 		The Extended Cohn-Kanade Dataset(CK+)
dataset [22]. For training, on 		The Extended Cohn-Kanade Dataset(CK+)
both CK+ and AFEW 8.0, 		The Extended Cohn-Kanade Dataset(CK+)
we set a batch to 		The Extended Cohn-Kanade Dataset(CK+)
have 48 instances with K 		The Extended Cohn-Kanade Dataset(CK+)
frames in each instance. For 		The Extended Cohn-Kanade Dataset(CK+)
frame sampling in a video, 		The Extended Cohn-Kanade Dataset(CK+)
we first split the video 		The Extended Cohn-Kanade Dataset(CK+)
into K segments and then 		The Extended Cohn-Kanade Dataset(CK+)
randomly select one frame from 		The Extended Cohn-Kanade Dataset(CK+)
each segment. By default, we 		The Extended Cohn-Kanade Dataset(CK+)
set K to 3. We 		The Extended Cohn-Kanade Dataset(CK+)
use the SGD method for 		The Extended Cohn-Kanade Dataset(CK+)
optimization with a momentum of 0		The Extended Cohn-Kanade Dataset(CK+)
.9 and a weight decay 		The Extended Cohn-Kanade Dataset(CK+)
of 10−4. On CK+, we 		The Extended Cohn-Kanade Dataset(CK+)
initialize the learning rate (lr) 		The Extended Cohn-Kanade Dataset(CK+)
to 0.1, and modify it 		The Extended Cohn-Kanade Dataset(CK+)
to 0.02 at 30 epochs, 		The Extended Cohn-Kanade Dataset(CK+)
and stop training after 60 		The Extended Cohn-Kanade Dataset(CK+)
epochs. On AFEW 8.0, we 		The Extended Cohn-Kanade Dataset(CK+)
initialize the lr to 4e-6, 		The Extended Cohn-Kanade Dataset(CK+)
and modify it to 8e-7 		The Extended Cohn-Kanade Dataset(CK+)
at 60 epochs and 1.6e-7 		The Extended Cohn-Kanade Dataset(CK+)
at 120 epochs, and stop 		The Extended Cohn-Kanade Dataset(CK+)
training after 180 epochs.  3.2. Evaluation on CK		The Extended Cohn-Kanade Dataset(CK+)
+  We evaluate our FAN on		The Extended Cohn-Kanade Dataset(CK+)
 CK+ with comparisons to several		The Extended Cohn-Kanade Dataset(CK+)
 state-of-the-art methods in Table 1		The Extended Cohn-Kanade Dataset(CK+)
. On CK+, due to 		The Extended Cohn-Kanade Dataset(CK+)
the fact that the videos 		The Extended Cohn-Kanade Dataset(CK+)
show a shift from a 		The Extended Cohn-Kanade Dataset(CK+)
neutral facial expression to the 		The Extended Cohn-Kanade Dataset(CK+)
peak expression, most of the 		The Extended Cohn-Kanade Dataset(CK+)
methods conduct data selection manually. 		The Extended Cohn-Kanade Dataset(CK+)
Zhang et al [23] propose 		The Extended Cohn-Kanade Dataset(CK+)
to combine a spatial CNN 		The Extended Cohn-Kanade Dataset(CK+)
model and a temporal network, 		The Extended Cohn-Kanade Dataset(CK+)
where the spatial CNN model 		The Extended Cohn-Kanade Dataset(CK+)
only uses the last peak 		The Extended Cohn-Kanade Dataset(CK+)
frame. Jung et al [12] 		The Extended Cohn-Kanade Dataset(CK+)
select a fixed length sequence 		The Extended Cohn-Kanade Dataset(CK+)
for each video with a 		The Extended Cohn-Kanade Dataset(CK+)
lipreading method [26], and jointly 		The Extended Cohn-Kanade Dataset(CK+)
fine-tune a deep temporal  Table 1. Evaluation of our		The Extended Cohn-Kanade Dataset(CK+)
 FAN with a comparison to		The Extended Cohn-Kanade Dataset(CK+)
 state- of-the-art methods on CK		The Extended Cohn-Kanade Dataset(CK+)
+ database. Note that only 		The Extended Cohn-Kanade Dataset(CK+)
those methods evaluated with 7 		The Extended Cohn-Kanade Dataset(CK+)
classes are included.  Method Training data Test data		The Extended Cohn-Kanade Dataset(CK+)
 Acc		The Extended Cohn-Kanade Dataset(CK+)
.  ST network [23] S: the		The Extended Cohn-Kanade Dataset(CK+)
 last frame T: all frames		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
S: the last frame T: 		The Extended Cohn-Kanade Dataset(CK+)
all frames 98.50  DTAGN [12] Fixed length Fixed		The Extended Cohn-Kanade Dataset(CK+)
 length 97.25		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
CNN+Island loss [24]  The last three frames and		The Extended Cohn-Kanade Dataset(CK+)
 the first frame		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
The last three frames and 		The Extended Cohn-Kanade Dataset(CK+)
the first frame  94.35		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
LOMo [25] All frames All 		The Extended Cohn-Kanade Dataset(CK+)
frames 92.00  Score fusion (baseline) All frames		The Extended Cohn-Kanade Dataset(CK+)
 All frames 94.80		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
FAN(w/o Relation- attention)  All frames All frames 99.08		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
FAN All frames All frames 99		The Extended Cohn-Kanade Dataset(CK+)
.69  appearance-geometry network. Cai et al		The Extended Cohn-Kanade Dataset(CK+)
 [24] select the last three		The Extended Cohn-Kanade Dataset(CK+)
 frames and the first frame		The Extended Cohn-Kanade Dataset(CK+)
 for each video, and train		The Extended Cohn-Kanade Dataset(CK+)
 CNN models with a new		The Extended Cohn-Kanade Dataset(CK+)
 Island loss function. We argue		The Extended Cohn-Kanade Dataset(CK+)
 that manual data selection is		The Extended Cohn-Kanade Dataset(CK+)
 an ad-hoc operation on CK		The Extended Cohn-Kanade Dataset(CK+)
+ and it is impractical 		The Extended Cohn-Kanade Dataset(CK+)
since we can not know 		The Extended Cohn-Kanade Dataset(CK+)
which is the peak frame 		The Extended Cohn-Kanade Dataset(CK+)
beforeahead. Sikka et al [25] 		The Extended Cohn-Kanade Dataset(CK+)
use all frames with a 		The Extended Cohn-Kanade Dataset(CK+)
new latent ordinal model which 		The Extended Cohn-Kanade Dataset(CK+)
extracts CNN/LBP/SIFT features for sub-event 		The Extended Cohn-Kanade Dataset(CK+)
detection and uses multi-instance SVM 		The Extended Cohn-Kanade Dataset(CK+)
for ex- pression classification. Our 		The Extended Cohn-Kanade Dataset(CK+)
baseline method uses ResNet18 to 		The Extended Cohn-Kanade Dataset(CK+)
generate scores for individual frame 		The Extended Cohn-Kanade Dataset(CK+)
and applies score fu- sion (		The Extended Cohn-Kanade Dataset(CK+)
summation) for all frames. It 		The Extended Cohn-Kanade Dataset(CK+)
achieves 94.8% which is 2.8% 		The Extended Cohn-Kanade Dataset(CK+)
better than [25]. Our proposed 		The Extended Cohn-Kanade Dataset(CK+)
FAN with only self- attention 		The Extended Cohn-Kanade Dataset(CK+)
gets 99.08% which significantly boosts 		The Extended Cohn-Kanade Dataset(CK+)
the baseline by 4.28%. Adding 		The Extended Cohn-Kanade Dataset(CK+)
relation-attention weights further im- proves 		The Extended Cohn-Kanade Dataset(CK+)
the accuracy to 99.69% which 		The Extended Cohn-Kanade Dataset(CK+)
sets up a new state 		The Extended Cohn-Kanade Dataset(CK+)
of the art on CK+.  3.3. Evaluation on AFEW 8.0		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
From the view of performance, 		The Extended Cohn-Kanade Dataset(CK+)
AFEW is one of the 		The Extended Cohn-Kanade Dataset(CK+)
most challenging videos FER dataset. 		The Extended Cohn-Kanade Dataset(CK+)
The EmotiW challenge shares the 		The Extended Cohn-Kanade Dataset(CK+)
same data from 2016 to 2018		The Extended Cohn-Kanade Dataset(CK+)
. Table 2 presents the 		The Extended Cohn-Kanade Dataset(CK+)
evaluation of our FAN on 		The Extended Cohn-Kanade Dataset(CK+)
AFEW with comparisons to recent 		The Extended Cohn-Kanade Dataset(CK+)
state-of-the-art methods. For a fair 		The Extended Cohn-Kanade Dataset(CK+)
comparison, we only list these 		The Extended Cohn-Kanade Dataset(CK+)
results obtained by the best 		The Extended Cohn-Kanade Dataset(CK+)
single models in previous works. 		The Extended Cohn-Kanade Dataset(CK+)
From the last three rows 		The Extended Cohn-Kanade Dataset(CK+)
of Table 2, our proposed 		The Extended Cohn-Kanade Dataset(CK+)
FAN improves the baseline by 2		The Extended Cohn-Kanade Dataset(CK+)
.36%. Both [27] and [10] 		The Extended Cohn-Kanade Dataset(CK+)
use VGGFace backbone and a 		The Extended Cohn-Kanade Dataset(CK+)
recurrent model with long-short-term memory 		The Extended Cohn-Kanade Dataset(CK+)
units. These methods aim to 		The Extended Cohn-Kanade Dataset(CK+)
capture temporal dynamic information for 		The Extended Cohn-Kanade Dataset(CK+)
videos. Most of the meth- 		The Extended Cohn-Kanade Dataset(CK+)
ods focus on improving static 		The Extended Cohn-Kanade Dataset(CK+)
face based CNN models 		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
Table 2. Evaluation of our 		The Extended Cohn-Kanade Dataset(CK+)
FAN with a comparison to 		The Extended Cohn-Kanade Dataset(CK+)
state- of-the-art methods on AFEW 8		The Extended Cohn-Kanade Dataset(CK+)
.0 database. It is worth 		The Extended Cohn-Kanade Dataset(CK+)
noting that we only compare 		The Extended Cohn-Kanade Dataset(CK+)
to the best single models 		The Extended Cohn-Kanade Dataset(CK+)
of previous works.  Method Model type Accuracy		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
CNN-RNN (2016) [27] Dynamic 45.43 		The Extended Cohn-Kanade Dataset(CK+)
VGGFace + Undirectional LSTM (2017		The Extended Cohn-Kanade Dataset(CK+)
) [10] Dynamic 48.60  HoloNet (2016) [28] Static 44.57		The Extended Cohn-Kanade Dataset(CK+)
 DSN-HoloNet (2017) [29] Static 46.47		The Extended Cohn-Kanade Dataset(CK+)
 DenseNet-161 (2018) [31] Static 51.44		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
DSN-VGGFace (2018) [30] Static 48.04  Score fusion (baseline) Static 48.82		The Extended Cohn-Kanade Dataset(CK+)
 FAN w/o Relation-attention Static 50.92		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
FAN Static 51.18  combine scores for video-level FER		The Extended Cohn-Kanade Dataset(CK+)
. Both [28] and [29] 		The Extended Cohn-Kanade Dataset(CK+)
input two LBP maps and 		The Extended Cohn-Kanade Dataset(CK+)
a gray image for CNN 		The Extended Cohn-Kanade Dataset(CK+)
models. Deeply- supervised networks are 		The Extended Cohn-Kanade Dataset(CK+)
used in [29] and [30], 		The Extended Cohn-Kanade Dataset(CK+)
which add supervision on intermediate 		The Extended Cohn-Kanade Dataset(CK+)
layers. For static methods, [31] 		The Extended Cohn-Kanade Dataset(CK+)
gets slightly better performance than 		The Extended Cohn-Kanade Dataset(CK+)
ours. However, [31] uses DenseNet-161 		The Extended Cohn-Kanade Dataset(CK+)
and pretrains it on both 		The Extended Cohn-Kanade Dataset(CK+)
large-scale face datasets and their 		The Extended Cohn-Kanade Dataset(CK+)
own Situ emotion video dataset. 		The Extended Cohn-Kanade Dataset(CK+)
Addition- ally, [31] applies complicated 		The Extended Cohn-Kanade Dataset(CK+)
post-processing which extracts frame features 		The Extended Cohn-Kanade Dataset(CK+)
and compute their mean vector, 		The Extended Cohn-Kanade Dataset(CK+)
max-pooled vector, and standard deviation 		The Extended Cohn-Kanade Dataset(CK+)
vector. These vectors are then 		The Extended Cohn-Kanade Dataset(CK+)
concatenated and finally fed into 		The Extended Cohn-Kanade Dataset(CK+)
an SVM classifier. Overall, our 		The Extended Cohn-Kanade Dataset(CK+)
FAN improves the baseline significantly 		The Extended Cohn-Kanade Dataset(CK+)
and achieves performance comparable to 		The Extended Cohn-Kanade Dataset(CK+)
that of the best previous 		The Extended Cohn-Kanade Dataset(CK+)
single model.  3.4. Visualization and Hyper-parameters		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
To better understand the self-attention 		The Extended Cohn-Kanade Dataset(CK+)
and relation-attention modules in our 		The Extended Cohn-Kanade Dataset(CK+)
FAN, we visualize the attention 		The Extended Cohn-Kanade Dataset(CK+)
weights in Figure 2. Figure 2 shows one sequence for each		The Extended Cohn-Kanade Dataset(CK+)
 category with blue and orange		The Extended Cohn-Kanade Dataset(CK+)
 weight bars, where blue bars		The Extended Cohn-Kanade Dataset(CK+)
 represent the self-attention weights (i.e		The Extended Cohn-Kanade Dataset(CK+)
. α in Eq. (1)) 		The Extended Cohn-Kanade Dataset(CK+)
of our FAN w/o relation-attention 		The Extended Cohn-Kanade Dataset(CK+)
and orange bars the final 		The Extended Cohn-Kanade Dataset(CK+)
weights (i.e. αβ in Eq. (4		The Extended Cohn-Kanade Dataset(CK+)
)) of our FAN. In 		The Extended Cohn-Kanade Dataset(CK+)
total, both kinds of weights 		The Extended Cohn-Kanade Dataset(CK+)
can reflect the importance of 		The Extended Cohn-Kanade Dataset(CK+)
frames. Comparing the blue and 		The Extended Cohn-Kanade Dataset(CK+)
orange bars, we find that 		The Extended Cohn-Kanade Dataset(CK+)
the final weights of our 		The Extended Cohn-Kanade Dataset(CK+)
FAN can always assign higher 		The Extended Cohn-Kanade Dataset(CK+)
weights to the more obvious 		The Extended Cohn-Kanade Dataset(CK+)
face frames, while self-attention module 		The Extended Cohn-Kanade Dataset(CK+)
could assign high weights on 		The Extended Cohn-Kanade Dataset(CK+)
some ob- scure face frames, 		The Extended Cohn-Kanade Dataset(CK+)
see the 1st, 2th, and 		The Extended Cohn-Kanade Dataset(CK+)
3rd rows of Figure 2 (		The Extended Cohn-Kanade Dataset(CK+)
left). This explicitly explains why 		The Extended Cohn-Kanade Dataset(CK+)
adding relation-attention boost performance.  Evaluation of Hyper-parameters. We evaluate		The Extended Cohn-Kanade Dataset(CK+)
 two hyper-parameters of our FAN		The Extended Cohn-Kanade Dataset(CK+)
 on CK+, i.e. backbone CNN		The Extended Cohn-Kanade Dataset(CK+)
 networks and the parameter K		The Extended Cohn-Kanade Dataset(CK+)
 mentioned in implementation details, to		The Extended Cohn-Kanade Dataset(CK+)
 validate the robustness of our		The Extended Cohn-Kanade Dataset(CK+)
 method. For the parameter K		The Extended Cohn-Kanade Dataset(CK+)
, besides the default value, 		The Extended Cohn-Kanade Dataset(CK+)
we try several other values, 		The Extended Cohn-Kanade Dataset(CK+)
i.e. {2, 5, 8}, and 		The Extended Cohn-Kanade Dataset(CK+)
find the performance is not 		The Extended Cohn-Kanade Dataset(CK+)
sensitive to K. Specifically, our 		The Extended Cohn-Kanade Dataset(CK+)
FAN obtains 99.39% with K={2, 5		The Extended Cohn-Kanade Dataset(CK+)
}. and gets 99.69% with 		The Extended Cohn-Kanade Dataset(CK+)
K=8. Since the default value, 		The Extended Cohn-Kanade Dataset(CK+)
K=3  Fig. 2. Visualization of the		The Extended Cohn-Kanade Dataset(CK+)
 self-attention weights (blue bar) and		The Extended Cohn-Kanade Dataset(CK+)
 the final weights of FAN		The Extended Cohn-Kanade Dataset(CK+)
 (orange bar) on CK+ dataset		The Extended Cohn-Kanade Dataset(CK+)
.  Fig. 3. Evaluation of backbone		The Extended Cohn-Kanade Dataset(CK+)
 CNN models and training strategies		The Extended Cohn-Kanade Dataset(CK+)
 on CK		The Extended Cohn-Kanade Dataset(CK+)
+.  gets 99.69%, we use this		The Extended Cohn-Kanade Dataset(CK+)
 default setting in the remainder		The Extended Cohn-Kanade Dataset(CK+)
 of this paper		The Extended Cohn-Kanade Dataset(CK+)
.  For the backbone CNN model		The Extended Cohn-Kanade Dataset(CK+)
 evaluation, we try the VGGFace		The Extended Cohn-Kanade Dataset(CK+)
 model which is widely-used in		The Extended Cohn-Kanade Dataset(CK+)
 previous works. Similarly, we also		The Extended Cohn-Kanade Dataset(CK+)
 pretrain the VGGFace model on		The Extended Cohn-Kanade Dataset(CK+)
 the FER- Plus dataset. Since		The Extended Cohn-Kanade Dataset(CK+)
 [5] shows that it is		The Extended Cohn-Kanade Dataset(CK+)
 better to freeze all the		The Extended Cohn-Kanade Dataset(CK+)
 feature learning layers after pretrained		The Extended Cohn-Kanade Dataset(CK+)
 on FERPlus for VGGFace model		The Extended Cohn-Kanade Dataset(CK+)
, we also conduct the 		The Extended Cohn-Kanade Dataset(CK+)
same experiment on CK+ with 		The Extended Cohn-Kanade Dataset(CK+)
VGGFace. Figure 3 shows the 		The Extended Cohn-Kanade Dataset(CK+)
default com- parisons with different 		The Extended Cohn-Kanade Dataset(CK+)
backbone CNN models. On CK+, 		The Extended Cohn-Kanade Dataset(CK+)
compared with freezing all the 		The Extended Cohn-Kanade Dataset(CK+)
feature layers for VGGFace, it 		The Extended Cohn-Kanade Dataset(CK+)
gets better results with fine-tuning 		The Extended Cohn-Kanade Dataset(CK+)
all layers which may be 		The Extended Cohn-Kanade Dataset(CK+)
explained by the domain discrepancy 		The Extended Cohn-Kanade Dataset(CK+)
between FERPlus and CK+. Overall, 		The Extended Cohn-Kanade Dataset(CK+)
the results are significantly improved 		The Extended Cohn-Kanade Dataset(CK+)
by self-attention weights and further 		The Extended Cohn-Kanade Dataset(CK+)
improved by the relation- attention 		The Extended Cohn-Kanade Dataset(CK+)
weights.  4. CONCLUSION		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
We propose Frame Attention Networks 		The Extended Cohn-Kanade Dataset(CK+)
for video-based facial expression recognition. 		The Extended Cohn-Kanade Dataset(CK+)
The FAN contains a self-attention 		The Extended Cohn-Kanade Dataset(CK+)
module and a relation-attention module. 		The Extended Cohn-Kanade Dataset(CK+)
The experiments on CK+ and 		The Extended Cohn-Kanade Dataset(CK+)
AFEW show that our FAN 		The Extended Cohn-Kanade Dataset(CK+)
with only self-attention improves the 		The Extended Cohn-Kanade Dataset(CK+)
baseline significantly and adding relation- 		The Extended Cohn-Kanade Dataset(CK+)
attention further boosts performance. With 		The Extended Cohn-Kanade Dataset(CK+)
a visualization on CK+, we 		The Extended Cohn-Kanade Dataset(CK+)
demonstrate that our FAN can 		The Extended Cohn-Kanade Dataset(CK+)
automatically capture the importance of 		The Extended Cohn-Kanade Dataset(CK+)
frames. Our single model achieves 		The Extended Cohn-Kanade Dataset(CK+)
performance on par with that 		The Extended Cohn-Kanade Dataset(CK+)
of state-of-the-art methods on AFEW 		The Extended Cohn-Kanade Dataset(CK+)
and obtains state-of-the-art results on 		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
5. REFERENCES  [1] Gwen Littlewort, Marian Stewart		The Extended Cohn-Kanade Dataset(CK+)
 Bartlett, Ian Fasel, Joshua Susskind		The Extended Cohn-Kanade Dataset(CK+)
, and Javier Movellan, “Dynamics 		The Extended Cohn-Kanade Dataset(CK+)
of facial expression extracted automatically 		The Extended Cohn-Kanade Dataset(CK+)
from video,” in IVC, 2006.  [2] Caifeng Shan, Shaogang Gong		The Extended Cohn-Kanade Dataset(CK+)
, and Peter W. 		The Extended Cohn-Kanade Dataset(CK+)
Mcowan, “Fa- cial expression recognition 		The Extended Cohn-Kanade Dataset(CK+)
based on local binary patterns: 		The Extended Cohn-Kanade Dataset(CK+)
A comprehensive study,” in IVC, 2009		The Extended Cohn-Kanade Dataset(CK+)
.  [3] Yichuan Tang, “Deep learning		The Extended Cohn-Kanade Dataset(CK+)
 using linear support vector ma		The Extended Cohn-Kanade Dataset(CK+)
- chines,” in CS, 2013.  [4] Sarah Adel Bargal, Emad		The Extended Cohn-Kanade Dataset(CK+)
 Barsoum, Cristian Canton Ferrer, and		The Extended Cohn-Kanade Dataset(CK+)
 Cha Zhang, “Emotion recognition in		The Extended Cohn-Kanade Dataset(CK+)
 the wild from videos using		The Extended Cohn-Kanade Dataset(CK+)
 images,” in ACM ICMI, 2016		The Extended Cohn-Kanade Dataset(CK+)
.  [5] Boris Knyazev, Roman Shvetsov		The Extended Cohn-Kanade Dataset(CK+)
, Natalia Efremova, and Artem 		The Extended Cohn-Kanade Dataset(CK+)
Kuharenko, “Convolutional neural networks pretrained 		The Extended Cohn-Kanade Dataset(CK+)
on large face recognition datasets 		The Extended Cohn-Kanade Dataset(CK+)
for emotion classification from video,” 		The Extended Cohn-Kanade Dataset(CK+)
in ACM ICMI, 2017.  [6] Sepp Hochreiter and Jrgen		The Extended Cohn-Kanade Dataset(CK+)
 Schmidhuber, “Long short-term memory,” Neural		The Extended Cohn-Kanade Dataset(CK+)
 Computation, 1997		The Extended Cohn-Kanade Dataset(CK+)
.  [7] Du Tran, Lubomir Bourdev		The Extended Cohn-Kanade Dataset(CK+)
, Rob Fergus, Lorenzo Torresani, 		The Extended Cohn-Kanade Dataset(CK+)
and Manohar Paluri, “Learning spatiotemporal 		The Extended Cohn-Kanade Dataset(CK+)
features with 3d convolutional networks,” 		The Extended Cohn-Kanade Dataset(CK+)
in ICCV, 2015.  [8] Yuanliu Liu, Yuanliu Liu		The Extended Cohn-Kanade Dataset(CK+)
, Yuanliu Liu, and Yuanliu 		The Extended Cohn-Kanade Dataset(CK+)
Liu, “Video-based emotion recognition using 		The Extended Cohn-Kanade Dataset(CK+)
cnn-rnn and c3d hy- brid 		The Extended Cohn-Kanade Dataset(CK+)
networks,” in ACM ICMI, 2016.  [9] Xi Ouyang, Shigenori Kawaai		The Extended Cohn-Kanade Dataset(CK+)
, Ester Gue Hua Goh, 		The Extended Cohn-Kanade Dataset(CK+)
Sheng- mei Shen, Wan Ding, 		The Extended Cohn-Kanade Dataset(CK+)
Huaiping Ming, and Dong-Yan 		The Extended Cohn-Kanade Dataset(CK+)
Huang, “Audio-visual emotion recognition using 		The Extended Cohn-Kanade Dataset(CK+)
deep transfer learning and multiple 		The Extended Cohn-Kanade Dataset(CK+)
temporal models,” in ACM ICMI, 2017		The Extended Cohn-Kanade Dataset(CK+)
.  [10] Valentin Vielzeuf, Stphane Pateux		The Extended Cohn-Kanade Dataset(CK+)
, and Frdric Jurie, “Temporal 		The Extended Cohn-Kanade Dataset(CK+)
multimodal fusion for video emotion 		The Extended Cohn-Kanade Dataset(CK+)
classification in the wild,” in 		The Extended Cohn-Kanade Dataset(CK+)
ACM ICMI, 2017.  [11] Jingwei Yan, Wenming Zheng		The Extended Cohn-Kanade Dataset(CK+)
, Zhen Cui, Chuangao Tang, 		The Extended Cohn-Kanade Dataset(CK+)
Tong Zhang, and Yuan 		The Extended Cohn-Kanade Dataset(CK+)
Zong, “Multi-cue fusion for emotion 		The Extended Cohn-Kanade Dataset(CK+)
recognition in the wild,” Neurocomputing, 2018		The Extended Cohn-Kanade Dataset(CK+)
.  [12] Heechul Jung, Sihaeng Lee		The Extended Cohn-Kanade Dataset(CK+)
, Junho Yim, Sunjeong Park, 		The Extended Cohn-Kanade Dataset(CK+)
and Junmo Kim, “Joint fine-tuning 		The Extended Cohn-Kanade Dataset(CK+)
in deep neural networks for 		The Extended Cohn-Kanade Dataset(CK+)
facial expression recognition,” in ICCV, 2015		The Extended Cohn-Kanade Dataset(CK+)
.  [13] Samira Ebrahimi Kahou, Christopher		The Extended Cohn-Kanade Dataset(CK+)
 Pal, Xavier Bouthillier, Pierre Froumenty		The Extended Cohn-Kanade Dataset(CK+)
, Roland Memisevic, Pascal Vincent, 		The Extended Cohn-Kanade Dataset(CK+)
Aaron Courville, Yoshua Bengio, and 		The Extended Cohn-Kanade Dataset(CK+)
Raul Chandias Ferrari, “Com- bining 		The Extended Cohn-Kanade Dataset(CK+)
modality specific deep neural networks 		The Extended Cohn-Kanade Dataset(CK+)
for emotion recognition in video,” 		The Extended Cohn-Kanade Dataset(CK+)
in ACM ICMI, 2013.  [14] Ashish Vaswani, Noam Shazeer		The Extended Cohn-Kanade Dataset(CK+)
, Niki Parmar, Jakob Uszko- 		The Extended Cohn-Kanade Dataset(CK+)
reit, Llion Jones, Aidan N 		The Extended Cohn-Kanade Dataset(CK+)
Gomez, Łukasz Kaiser, and Illia 		The Extended Cohn-Kanade Dataset(CK+)
Polosukhin, “Attention is all you 		The Extended Cohn-Kanade Dataset(CK+)
need,” in NIPS, 2017.  [15] Jiaolong Yang, Peiran Ren		The Extended Cohn-Kanade Dataset(CK+)
, Dongqing Zhang, Dong Chen, 		The Extended Cohn-Kanade Dataset(CK+)
Fang Wen, Hongdong Li, and 		The Extended Cohn-Kanade Dataset(CK+)
Gang Hua, “Neural aggregation network 		The Extended Cohn-Kanade Dataset(CK+)
for video face recognition.,” in 		The Extended Cohn-Kanade Dataset(CK+)
CVPR, 2017.  [16] Flood Sung Yongxin Yang		The Extended Cohn-Kanade Dataset(CK+)
, Li Zhang, Tao Xiang, 		The Extended Cohn-Kanade Dataset(CK+)
Philip HS Torr, and Timothy 		The Extended Cohn-Kanade Dataset(CK+)
M Hospedales, “Learning to compare: 		The Extended Cohn-Kanade Dataset(CK+)
Re- lation network for few-shot 		The Extended Cohn-Kanade Dataset(CK+)
learning,” in CVPR, 2018.  [17] Patrick Lucey, Jeffrey F		The Extended Cohn-Kanade Dataset(CK+)
 Cohn, Takeo Kanade, Jason Saragih		The Extended Cohn-Kanade Dataset(CK+)
, Zara Ambadar, and Iain 		The Extended Cohn-Kanade Dataset(CK+)
Matthews, “The extended cohn- kanade 		The Extended Cohn-Kanade Dataset(CK+)
dataset (ck+): A complete dataset 		The Extended Cohn-Kanade Dataset(CK+)
for action unit and emotion-specified 		The Extended Cohn-Kanade Dataset(CK+)
expression,” in CVPRW, 2010.  [18] Mengyi Liu, Shiguang Shan		The Extended Cohn-Kanade Dataset(CK+)
, Ruiping Wang, and Xilin 		The Extended Cohn-Kanade Dataset(CK+)
Chen, “Learning expressionlets on spatio-temporal 		The Extended Cohn-Kanade Dataset(CK+)
manifold for dy- namic facial 		The Extended Cohn-Kanade Dataset(CK+)
expression recognition,” in CVPR, 2014.  [19] Chieh-Ming Kuo, Shang-Hong Lai		The Extended Cohn-Kanade Dataset(CK+)
, and Michel Sarkis, “A 		The Extended Cohn-Kanade Dataset(CK+)
compact deep learning model for 		The Extended Cohn-Kanade Dataset(CK+)
robust facial expression recognition,” in 		The Extended Cohn-Kanade Dataset(CK+)
CVPRW, 2018.  [20] Abhinav Dhall, Amanjot Kaur		The Extended Cohn-Kanade Dataset(CK+)
, Roland Goecke, and Tom 		The Extended Cohn-Kanade Dataset(CK+)
Gedeon, “Emotiw 2018: Audio-video, student 		The Extended Cohn-Kanade Dataset(CK+)
engagement and group-level affect prediction,” 		The Extended Cohn-Kanade Dataset(CK+)
arXiv preprint:1808.07773, 2018.  [21] Yandong Guo, Lei Zhang		The Extended Cohn-Kanade Dataset(CK+)
, Yuxiao Hu, Xiaodong He, 		The Extended Cohn-Kanade Dataset(CK+)
and Jian- feng Gao, “Ms-celeb-1m: 		The Extended Cohn-Kanade Dataset(CK+)
A dataset and benchmark for 		The Extended Cohn-Kanade Dataset(CK+)
large- scale face recognition,” in 		The Extended Cohn-Kanade Dataset(CK+)
ECCV, 2016.  [22] Emad Barsoum, Cha Zhang		The Extended Cohn-Kanade Dataset(CK+)
, Cristian Canton Ferrer, and 		The Extended Cohn-Kanade Dataset(CK+)
Zhengyou Zhang, “Training deep networks 		The Extended Cohn-Kanade Dataset(CK+)
for facial expres- sion recognition 		The Extended Cohn-Kanade Dataset(CK+)
with crowd-sourced label distribution,” in 		The Extended Cohn-Kanade Dataset(CK+)
ACM ICMI, 2016.  [23] Kaihao Zhang, Yongzhen Huang		The Extended Cohn-Kanade Dataset(CK+)
, Yong Du, and Liang 		The Extended Cohn-Kanade Dataset(CK+)
Wang, “Facial expression recognition based 		The Extended Cohn-Kanade Dataset(CK+)
on deep evolutional spatial-temporal networks,” 		The Extended Cohn-Kanade Dataset(CK+)
IEEE TIP, 2017.  [24] Jie Cai, Zibo Meng		The Extended Cohn-Kanade Dataset(CK+)
, Ahmed Shehab Khan, Zhiyuan 		The Extended Cohn-Kanade Dataset(CK+)
Li, James OReilly, and Yan 		The Extended Cohn-Kanade Dataset(CK+)
Tong, “Island loss for learning 		The Extended Cohn-Kanade Dataset(CK+)
discriminative features in facial expression 		The Extended Cohn-Kanade Dataset(CK+)
recognition,” in FG, 2018.  [25] Karan Sikka, Gaurav Sharma		The Extended Cohn-Kanade Dataset(CK+)
, and Marian Bartlett, “Lomo: 		The Extended Cohn-Kanade Dataset(CK+)
Latent ordinal model for facial 		The Extended Cohn-Kanade Dataset(CK+)
analysis in videos,” in CVPR, 2016		The Extended Cohn-Kanade Dataset(CK+)
.  [26] Ziheng Zhou, Guoying Zhao		The Extended Cohn-Kanade Dataset(CK+)
, and M. Pietikainen, “Towards 		The Extended Cohn-Kanade Dataset(CK+)
a practical lipreading system,” in 		The Extended Cohn-Kanade Dataset(CK+)
CVPR, 2011.  [27] Yin Fan, Xiangju Lu		The Extended Cohn-Kanade Dataset(CK+)
, Dian Li, and Yuanliu 		The Extended Cohn-Kanade Dataset(CK+)
Liu, “Video-based emotion recognition using 		The Extended Cohn-Kanade Dataset(CK+)
cnn-rnn and c3d hybrid networks,” 		The Extended Cohn-Kanade Dataset(CK+)
in ACM ICMI, 2016.  [28] Anbang Yao, Dongqi Cai		The Extended Cohn-Kanade Dataset(CK+)
, Ping Hu, Shandong Wang, 		The Extended Cohn-Kanade Dataset(CK+)
Liang Sha, and Yurong 		The Extended Cohn-Kanade Dataset(CK+)
Chen, “Holonet: towards robust emotion 		The Extended Cohn-Kanade Dataset(CK+)
recognition in the wild,” in 		The Extended Cohn-Kanade Dataset(CK+)
ACM ICMI, 2016.  [29] Ping Hu, Dongqi Cai		The Extended Cohn-Kanade Dataset(CK+)
, Shandong Wang, Anbang Yao, 		The Extended Cohn-Kanade Dataset(CK+)
and Yurong Chen, “Learning supervised 		The Extended Cohn-Kanade Dataset(CK+)
scoring ensemble for emo- tion 		The Extended Cohn-Kanade Dataset(CK+)
recognition in the wild,” in 		The Extended Cohn-Kanade Dataset(CK+)
ACM ICMI, 2017.  [30] Yingruo Fan, Jacqueline CK		The Extended Cohn-Kanade Dataset(CK+)
 Lam, and Victor OK Li		The Extended Cohn-Kanade Dataset(CK+)
, “Video- based emotion recognition 		The Extended Cohn-Kanade Dataset(CK+)
using deeply-supervised neural net- works,” 		The Extended Cohn-Kanade Dataset(CK+)
in ACM ICMI, 2018.  [31] Chuanhe Liu, Tianhao Tang		The Extended Cohn-Kanade Dataset(CK+)
, Kui Lv, and Minghao 		The Extended Cohn-Kanade Dataset(CK+)
Wang, “Multi-feature based emotion recognition 		The Extended Cohn-Kanade Dataset(CK+)
for video clips,” in ACM 		The Extended Cohn-Kanade Dataset(CK+)
ICMI, 2018		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
1  Introduction		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
2  Frame Attention Networks		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
3  Experiments		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
3.1  Datasets and Implementation Details		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
3.2  Evaluation on CK		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
3.3  Evaluation on AFEW 8.0		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
3.4  Visualization and Hyper-parameters		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
4  Conclusion		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
5  References		The Extended Cohn-Kanade Dataset(CK+)
		The Extended Cohn-Kanade Dataset(CK+)
ABSTRACT The video-based facial expression recognition aims	The	The Extended Cohn-Kanade Dataset(CK+)
frames in an end-to-end framework. The network takes a video with	The	The Extended Cohn-Kanade Dataset(CK+)
and produces a fixed-dimension representation. The whole network is com- posed	The	The Extended Cohn-Kanade Dataset(CK+)
of two modules. The feature embedding module is a	The	The Extended Cohn-Kanade Dataset(CK+)
face images into feature vectors. The frame attention module learns multiple	The	The Extended Cohn-Kanade Dataset(CK+)
or motion information in videos. The Long Short-Term Mem- ory (LSTM	The	The Extended Cohn-Kanade Dataset(CK+)
to adaptively aggregate frame features. The FAN is designed to learn	The	The Extended Cohn-Kanade Dataset(CK+)
reasoning in an end-to-end fashion. The self-attention kernels are directly learned	The	The Extended Cohn-Kanade Dataset(CK+)
fixed-dimension feature representation for FER. The whole network consists of two	The	The Extended Cohn-Kanade Dataset(CK+)
module and frame attention module. The feature embedding module is a	The	The Extended Cohn-Kanade Dataset(CK+)
image into a feature vector. The frame attention module learns two-level	The	The Extended Cohn-Kanade Dataset(CK+)
local features is more reliable. The self-attention weights are learned with	The	The Extended Cohn-Kanade Dataset(CK+)
relation-attention weights for frame features. The relation-attention weight of the i-th	The	The Extended Cohn-Kanade Dataset(CK+)
The last three frames and the	The	The Extended Cohn-Kanade Dataset(CK+)
The last three frames and the	The	The Extended Cohn-Kanade Dataset(CK+)
most challenging videos FER dataset. The EmotiW challenge shares the same	The	The Extended Cohn-Kanade Dataset(CK+)
for video-based facial expression recognition. The FAN contains a self-attention module	The	The Extended Cohn-Kanade Dataset(CK+)
and a relation-attention module. The experiments on CK+ and AFEW	The	The Extended Cohn-Kanade Dataset(CK+)
The extended cohn- kanade dataset (ck	The	The Extended Cohn-Kanade Dataset(CK+)
We conduct extensive experiments on CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
and achieves state-of-the-art performance on CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
We conduct extensive experiments on CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
achieves state-of-the- art performance on CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
happiness, sadness, and surprise. Since CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
22]. For training, on both CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
weight decay of 10−4. On CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
3.2. Evaluation on CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
We evaluate our FAN on CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
methods in Table 1. On CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
to state- of-the-art methods on CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
is an ad-hoc operation on CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
state of the art on CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
hyper-parameters of our FAN on CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
of FAN (orange bar) on CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
models and training strategies on CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
conduct the same experiment on CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
different backbone CNN models. On CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
domain discrepancy between FERPlus and CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
relation-attention module. The experiments on CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
performance. With a visualization on CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
and obtains state-of-the-art results on CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
30] Yingruo Fan, Jacqueline CK Lam, and Victor OK Li	CK+	The Extended Cohn-Kanade Dataset(CK+)
Evaluation on CK	CK+	The Extended Cohn-Kanade Dataset(CK+)
third dataset we used. The street2shop dataset contains 20,000 images under	street2shop	street2shop - topwear
third dataset we used. The street2shop 	street2shop 	street2shop - topwear
-	-	street2shop - topwear
- chitecture that when trained on	-	street2shop - topwear
- bedding of the image is	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- tive filtering recommends products based	-	street2shop - topwear
- havior on the platform, but	-	street2shop - topwear
-	-	street2shop - topwear
- viously, but are noted to	-	street2shop - topwear
- lution here would be the	-	street2shop - topwear
-	-	street2shop - topwear
- cal embeddings giving the intensity	-	street2shop - topwear
-	-	street2shop - topwear
- trieve them in order of	-	street2shop - topwear
-	-	street2shop - topwear
- tensive evaluation has verified that	-	street2shop - topwear
- tance matrix instead of Euclidean	-	street2shop - topwear
- ity information, we can achieve	-	street2shop - topwear
- log similar to a user-uploaded	-	street2shop - topwear
- eral challenges in dealing with	-	street2shop - topwear
- mines whether a given set	-	street2shop - topwear
-	-	street2shop - topwear
- bedded in a multidimensional space	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- age. In order to project	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- butions of this paper are	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- ity between sample images	-	street2shop - topwear
- pare our proposed RankNet with	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- ods for different datasets. The	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- proach, i.e collect an image	-	street2shop - topwear
- ity function which when given	-	street2shop - topwear
- lar images from the storage	-	street2shop - topwear
- age data in order to	-	street2shop - topwear
- proaches were little efficient and	-	street2shop - topwear
-11	-	street2shop - topwear
- tures like SIFT and HOG	-	street2shop - topwear
- ited. Recently researchers who used	-	street2shop - topwear
- ral networks for object recognition	-	street2shop - topwear
- cess [15, 16, 17]. In	-	street2shop - topwear
- straction level. The descriptor vector	-	street2shop - topwear
- sual similarity, these descriptors are	-	street2shop - topwear
- sual similarity is a composite	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- cause for a network need	-	street2shop - topwear
-	-	street2shop - topwear
- ilar depending on the ground	-	street2shop - topwear
- ity which we are addressing	-	street2shop - topwear
- nary (similar/dissimilar) fails the objective	-	street2shop - topwear
- grained visual similarity. Therefore in	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
to 10 object classes namely - top, trouser, pullover, dress, coat	-	street2shop - topwear
- ing RankNet. CIFAR10 is an	-	street2shop - topwear
-	-	street2shop - topwear
- act matching products between the	-	street2shop - topwear
-	-	street2shop - topwear
- tive(n) images belong to the	-	street2shop - topwear
- lic URL links for those	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- dings of the data. The	-	street2shop - topwear
- volutional neural networks with shared	-	street2shop - topwear
- timized during training by minimizing	-	street2shop - topwear
- eter θ, x = f(I;θ	-	street2shop - topwear
- eters of the neural network	-	street2shop - topwear
- ner product layers. The number	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- ages apart. The network takes	-	street2shop - topwear
- ther apart then the network	-	street2shop - topwear
- beddings. There are Y layers	-	street2shop - topwear
-	-	street2shop - topwear
- tion function here rectified linear	-	street2shop - topwear
-	-	street2shop - topwear
- rameters of a parameterized function	-	street2shop - topwear
-	-	street2shop - topwear
- ther as compared to similar/positive	-	street2shop - topwear
- mensional embedding subspace	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- cided empirically. When m is	-	street2shop - topwear
- hattan distance metric provides the	-	street2shop - topwear
- rithmic and indexing methods fail	-	street2shop - topwear
- tive. The basic concept of	-	street2shop - topwear
- defined as the contrast which	-	street2shop - topwear
- ality curse from an angle	-	street2shop - topwear
- parative performance is optimizing the	-	street2shop - topwear
- ther minimizing the Contrastive loss	-	street2shop - topwear
- served that directly optimizing a	-	street2shop - topwear
- task learning. Recently some work	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- tive to scale change. Other	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- hance conventional distance metric learning	-	street2shop - topwear
- coding it as a representation	-	street2shop - topwear
-	-	street2shop - topwear
- ative sample away from the	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- lated between the embeddings. Also	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- curately than the distance-based triplet	-	street2shop - topwear
- tation is broad and can	-	street2shop - topwear
- tion	-	street2shop - topwear
type of pair of images - (1) Positive Pair ( Similar	-	street2shop - topwear
- ages ) and (2) Negative	-	street2shop - topwear
- cally selected from the datset	-	street2shop - topwear
- rate or good at recalling	-	street2shop - topwear
- trieve the most reasonably alike	-	street2shop - topwear
- ilar to the query image	-	street2shop - topwear
-	-	street2shop - topwear
- est neighbors from the same	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- age whereas the latter refers	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- search, we retrieved the in-class	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- Net. In AlexNet and PatterNet	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- togram), and then after that	-	street2shop - topwear
- riculum learning where the positive	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- ages, therefore we used a	-	street2shop - topwear
- work that incorporates different levels	-	street2shop - topwear
- ious scales [26, 27]. Deep	-	street2shop - topwear
- fication. The strong invariance encoded	-	street2shop - topwear
- ally grows higher towards the	-	street2shop - topwear
- variance makes it hard to	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- variance and capture the semantics	-	street2shop - topwear
- cause it has 19 convolutional	-	street2shop - topwear
-	-	street2shop - topwear
- pler aspects like shapes, pattern	-	street2shop - topwear
- ent convolution neural networks instead	-	street2shop - topwear
- dependent of the other two	-	street2shop - topwear
- bined with a 4096-dimensional linear	-	street2shop - topwear
-	-	street2shop - topwear
- ization. Final results show that	-	street2shop - topwear
-	-	street2shop - topwear
- ral networks on the image	-	street2shop - topwear
- sponsible for the result is	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- nal layer which allows the	-	street2shop - topwear
- works(CNN1 and CNN2) emphasis on	-	street2shop - topwear
-	-	street2shop - topwear
- tation details for training a	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- cerns were preventing and detecting	-	street2shop - topwear
- egy where we do not	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- ageNet dataset and fine-tuned the	-	street2shop - topwear
-	-	street2shop - topwear
- mented with the learning rate	-	street2shop - topwear
- mentum so that the optimizer	-	street2shop - topwear
- vent the network from getting	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- just the learning rate accordingly	-	street2shop - topwear
-	-	street2shop - topwear
- gence. The architecture was implemented	-	street2shop - topwear
- come more robust and prevented	-	street2shop - topwear
- ployed dropout in our architecture	-	street2shop - topwear
- cause dropout not only prevents	-	street2shop - topwear
- tion. Therefore both image augmentation	-	street2shop - topwear
- sentially equivalent to prevent overfitting	-	street2shop - topwear
-	-	street2shop - topwear
- itored the number of layers	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- parameter tuning and we used	-	street2shop - topwear
-	-	street2shop - topwear
- tain the same number of	-	street2shop - topwear
- ing data. The similar distribution	-	street2shop - topwear
- ized performance of the model	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- bor embedding algorithm used for	-	street2shop - topwear
-	-	street2shop - topwear
- mented using Barnes-Hut approximations which	-	street2shop - topwear
-	-	street2shop - topwear
- ization, the different category of	-	street2shop - topwear
- mally in projecting similar images	-	street2shop - topwear
-	-	street2shop - topwear
RankNet for Street2Shop Test Data - catalog images	-	street2shop - topwear
-20 recall. The top-20 recall evaluation	-	street2shop - topwear
-20 recall it is calculated that	-	street2shop - topwear
- cent of the cases the	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-20 Recall % on Exact Street2Shop	-	street2shop - topwear
- scale convolutional neural network in	-	street2shop - topwear
-	-	street2shop - topwear
- tance between two data points	-	street2shop - topwear
-	-	street2shop - topwear
- bedding space, which outperforms the	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- tional Networks for Large-Scale Image	-	street2shop - topwear
- age Retrieval Methods	-	street2shop - topwear
- age matching using SIFT, SURF	-	street2shop - topwear
- formance comparison for distorted images	-	street2shop - topwear
- scale sketch-based image search, in	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- proved image search, in ACM	-	street2shop - topwear
-	-	street2shop - topwear
- tection for web image search	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- gus. 2011. Learning invariance through	-	street2shop - topwear
- cation with Deep Convolutional Neural	-	street2shop - topwear
- works. In Proc. NIPS. 11061114	-	street2shop - topwear
- tional Networks for Large-Scale Image	-	street2shop - topwear
- houcke, and Andrew Rabinovich. 2015	-	street2shop - topwear
- MNIST: a Novel Image Dataset	-	street2shop - topwear
-	-	street2shop - topwear
- man. 2010. Geodesic star convexity	-	street2shop - topwear
- mentation. In Proc. CVPR. 31293136	-	street2shop - topwear
-	-	street2shop - topwear
-48	-	street2shop - topwear
- search Presentation: A Theoretical Foundation	-	street2shop - topwear
- rmsprop: Divide the gradient by	-	street2shop - topwear
- cent magnitude.” COURSERA: Neural networks	-	street2shop - topwear
- ing Research 15.1 (2014): 1929-1958	-	street2shop - topwear
- tion approach Qi Quan et	-	street2shop - topwear
- lyzing the performance of multilayer	-	street2shop - topwear
- ject recognition.” European Conference on	-	street2shop - topwear
- 2605	-	street2shop - topwear
-	-	street2shop - topwear
- tures: Spatial pyramid matching for	-	street2shop - topwear
-	-	street2shop - topwear
- tures. In ICCV, volume 2	-	street2shop - topwear
- lem with convergence rate o(1/sqr(k	-	street2shop - topwear
-	-	street2shop - topwear
- age retrieval with compressed fisher	-	street2shop - topwear
- portance of initialization and momentum	-	street2shop - topwear
- variance through imitation. In CVPR	-	street2shop - topwear
- ilarity from flickr groups using	-	street2shop - topwear
- notation: learning to rank with	-	street2shop - topwear
-	-	street2shop - topwear
- chine intelligence 28.12 (2006): 2037-2041	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- tures both fine and coarse	-	street2shop - topwear
- ing novel CNN architecture, called	-	street2shop - topwear
- ing levels of abstraction, we	-	street2shop - topwear
- proach performs as good as	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- els with only one third	-	street2shop - topwear
- cance of intermediate layers on	-	street2shop - topwear
- esis by checking the impact	-	street2shop - topwear
- Net, a mobile model (12	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- ers visual similarity at Fynd	-	street2shop - topwear
- trieval, Visual Search, Recommender Systems	-	street2shop - topwear
- traction, E-Commerce	-	street2shop - topwear
Introduction Instance-level-image retrieval (content based - CBIR) also known as visual	-	street2shop - topwear
-	-	street2shop - topwear
- mend users the products similar	-	street2shop - topwear
- sion (CR). For ecommerce, two	-	street2shop - topwear
- ple of a woman dress	-	street2shop - topwear
-	-	street2shop - topwear
- lar types (v neck, round	-	street2shop - topwear
-	-	street2shop - topwear
- uct image could be with	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- sists of a feature extractor	-	street2shop - topwear
- sual attributes needed for the	-	street2shop - topwear
-	-	street2shop - topwear
- ucts are always nearer. We	-	street2shop - topwear
- curacy. A deep CNN is	-	street2shop - topwear
- bust. Deep learning has achieved	-	street2shop - topwear
- ing features from a single	-	street2shop - topwear
- sification task, a deep CNN	-	street2shop - topwear
- ance which grows higher towards	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- straction from the image. They	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- plex relations required for visual	-	street2shop - topwear
- age in real-time at production	-	street2shop - topwear
-	-	street2shop - topwear
- dients, to eyes, nose, to	-	street2shop - topwear
- ers [6]. The lower first	-	street2shop - topwear
- tures. Since multiple level of	-	street2shop - topwear
- Net (Multi-Intermediate Layers Descriptors Net	-	street2shop - topwear
- gated this local features inspired	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- lar Street2shop clothing similarity data	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- ence latency. We used accuracy	-	street2shop - topwear
- imented multiple variants of MILDNet	-	street2shop - topwear
- off 5.4% top test accuracy	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- tributes and relative similarity is	-	street2shop - topwear
- turing one’s notion of visual	-	street2shop - topwear
- tecture of MILDNet with the	-	street2shop - topwear
- tion of this work is	-	street2shop - topwear
-	-	street2shop - topwear
- chitecture. Embeddings are tuned to	-	street2shop - topwear
- tive in the latent space	-	street2shop - topwear
- ies are done in the	-	street2shop - topwear
-	-	street2shop - topwear
- ways getting embeddings from passing	-	street2shop - topwear
- ding space. So, the main	-	street2shop - topwear
- tations, shape descriptors, and texture	-	street2shop - topwear
- ture in image patch) has	-	street2shop - topwear
- SIS [13] and local distance	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- pressive power	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- ding [19], have also shown	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- trieval in various prior works	-	street2shop - topwear
- formance lagged behind that of	-	street2shop - topwear
-	-	street2shop - topwear
- formation. Qualitative examples of retrieval	-	street2shop - topwear
- tures extracted from fully-connected layers	-	street2shop - topwear
- ied [15, 20] which showed	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- derless Pooling (MOP) where different	-	street2shop - topwear
- connected layer is aggregated by	-	street2shop - topwear
-	-	street2shop - topwear
- scriptors obtained by the max	-	street2shop - topwear
-	-	street2shop - topwear
- image retrieval since local characteristics	-	street2shop - topwear
- ers have these local characteristics	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- sual features and high-level concepts	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- perimented on all intermediate layers	-	street2shop - topwear
- lected the best performant layer	-	street2shop - topwear
- scale image dataset because the	-	street2shop - topwear
- strated great results using a	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- lated to theirs but consists	-	street2shop - topwear
-	-	street2shop - topwear
- olution paths which demonstrated state-of-the	-	street2shop - topwear
- mantics. A triplet-based hinge loss	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- trastive loss function [27]. In	-	street2shop - topwear
- alog/shop images as matching images	-	street2shop - topwear
- ments we used triplet loss	-	street2shop - topwear
- based loss function, widely used	-	street2shop - topwear
-	-	street2shop - topwear
- ing ecommerce product based on	-	street2shop - topwear
- ative image (relatively dissimilar to	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- ture where users can upload	-	street2shop - topwear
- alog. While catalog query image	-	street2shop - topwear
-	-	street2shop - topwear
- played to users in an	-	street2shop - topwear
- mentation we used wild query	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- ployment we used a mixture	-	street2shop - topwear
-	-	street2shop - topwear
- log images and exact street-to-shop	-	street2shop - topwear
-	-	street2shop - topwear
- tablished manually). The retrieval sets	-	street2shop - topwear
- ion clothing categories, but only	-	street2shop - topwear
- ously experimented and presented here	-	street2shop - topwear
- ages. Negative images, n of	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- class triplets in case of	-	street2shop - topwear
-	-	street2shop - topwear
- sion d, which is sufficient	-	street2shop - topwear
-	-	street2shop - topwear
- ture Extractor (CNN Architecture), 2	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- age semantics and encodes strong	-	street2shop - topwear
-	-	street2shop - topwear
-2012 dataset [31], which contains roughly	-	street2shop - topwear
- egories	-	street2shop - topwear
-	-	street2shop - topwear
- ance	-	street2shop - topwear
-	-	street2shop - topwear
- pendent CNNs: 1. Convnet(AlexNet/VGG16/VGG19) pre	-	street2shop - topwear
- trained on ImageNet dataset 2	-	street2shop - topwear
- dings from them are passed	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- ate layers. As from many	-	street2shop - topwear
-	-	street2shop - topwear
- erage pooling to flatten the	-	street2shop - topwear
-	-	street2shop - topwear
then passed through an FC - Dropout - FC layer to	-	street2shop - topwear
- duce the model size to	-	street2shop - topwear
- vnet(VGG16/MobileNet) pretrained on ImageNet dataset	-	street2shop - topwear
This are passed through FC - dropout - FC layers to	-	street2shop - topwear
-	-	street2shop - topwear
- beddings	-	street2shop - topwear
- iments. Three of which are	-	street2shop - topwear
-	-	street2shop - topwear
- ants of our proposed network	-	street2shop - topwear
- ding space	-	street2shop - topwear
- mize the mapping of images	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- tor ~q is always relatively	-	street2shop - topwear
- clidean distance. Let’s say that	-	street2shop - topwear
-	-	street2shop - topwear
- tion can be written as	-	street2shop - topwear
- tance. While for dissimilar images	-	street2shop - topwear
- lution. Contrastive Loss function have	-	street2shop - topwear
-	-	street2shop - topwear
- ally similar images from the	-	street2shop - topwear
- act Nearest Neighbour is O(n2	-	street2shop - topwear
-	-	street2shop - topwear
- ing k items where value	-	street2shop - topwear
- tion dataset of 25460 images	-	street2shop - topwear
- tive model architecture input size	-	street2shop - topwear
- mented using Keras’s real-time augmentator	-	street2shop - topwear
- erator class. We used following	-	street2shop - topwear
-	-	street2shop - topwear
- configured ML playground. We ran	-	street2shop - topwear
- ploy models at scale. Our	-	street2shop - topwear
- tectures or combination of losses	-	street2shop - topwear
- tures are presented here. For	-	street2shop - topwear
- ing results and plotting graph	-	street2shop - topwear
- racy and average inference time	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- vnet as VGG16 and 2	-	street2shop - topwear
- nections	-	street2shop - topwear
-	-	street2shop - topwear
- spired by [3	-	street2shop - topwear
-512	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- chitecture with 4 skip connections	-	street2shop - topwear
- centage of all the experiments	-	street2shop - topwear
- sists of triplets from Street2shop	-	street2shop - topwear
- ing potential for this task	-	street2shop - topwear
- chitecture as base CNN instead	-	street2shop - topwear
- curacy to 89.60%. The training	-	street2shop - topwear
- els	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-512	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- tain the same number of	-	street2shop - topwear
- ing data. The similar distribution	-	street2shop - topwear
- ized performance of the model	-	street2shop - topwear
- tion and gradually added aggregated	-	street2shop - topwear
- Net (see Figure 5. Table	-	street2shop - topwear
- ing ability and inference accuracy	-	street2shop - topwear
- prehensive we considered all the	-	street2shop - topwear
- tion. The 2048-d embeddings are	-	street2shop - topwear
- SNE [36], which is a	-	street2shop - topwear
- ding algorithm. The visualization showed	-	street2shop - topwear
- cessfully shows understanding of both	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-512	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- nore backgrounds in the images	-	street2shop - topwear
- tion 3.2), we used both	-	street2shop - topwear
-	-	street2shop - topwear
- shop dataset, and in-house automatically	-	street2shop - topwear
- age triplets from results of	-	street2shop - topwear
- larity model. Here we will	-	street2shop - topwear
- ommendation Model	-	street2shop - topwear
- sification models from our in-house	-	street2shop - topwear
- fication task, we chose the	-	street2shop - topwear
-	-	street2shop - topwear
- Net	-	street2shop - topwear
- trained on InceptionV3 architecture. Feature	-	street2shop - topwear
- tionV3 architecture. Feature vector from	-	street2shop - topwear
- ture vector	-	street2shop - topwear
- resenting pattern and 540 features	-	street2shop - topwear
-	-	street2shop - topwear
- tures are extracted from pretrained	-	street2shop - topwear
- togram. Impact of these features	-	street2shop - topwear
- ture our notion of visual	-	street2shop - topwear
- pen. For this we decided	-	street2shop - topwear
- ery day. Our system is	-	street2shop - topwear
- cal components	-	street2shop - topwear
- zon ec2 instances to store	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- der+category keys and processed in	-	street2shop - topwear
- sible by processing the data	-	street2shop - topwear
-	-	street2shop - topwear
- Net with only 512-d embedding	-	street2shop - topwear
-	-	street2shop - topwear
- beddings by changing the number	-	street2shop - topwear
- ture the notion of visual	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- racy and recall while reducing	-	street2shop - topwear
- tially adding each skip connection	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- tire production pipeline which caters	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- ploiting local features from deep	-	street2shop - topwear
-61	-	street2shop - topwear
-	-	street2shop - topwear
- sion and Pattern Recognition Workshops	-	street2shop - topwear
-	-	street2shop - topwear
- tation for large- scale image-based	-	street2shop - topwear
- volutional features for image retrieval	-	street2shop - topwear
-	-	street2shop - topwear
- tures. In ICCV, volume 2	-	street2shop - topwear
- cation using local distance functions	-	street2shop - topwear
- proach to object matching in	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- scale image retrieval with compressed	-	street2shop - topwear
-	-	street2shop - topwear
- tern Recognition, CVPR, pages 33843391	-	street2shop - topwear
European Conference on Computer Vision - ECCV, pages 584599, 2014	-	street2shop - topwear
-	-	street2shop - topwear
- derless pooling of deep convolutional	-	street2shop - topwear
- son. From generic to specific	-	street2shop - topwear
- sion and Pattern Recognition Workshops	-	street2shop - topwear
- multaneous feature learning and hash	-	street2shop - topwear
- tion by Learning an Invariant	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
- works for Large-Scale Image Recognition	-	street2shop - topwear
-02	-	street2shop - topwear
-22	-	street2shop - topwear
- els: http://www.slideshare.net/erikbern/approximate-nearest- neighbor-methods-and-vector-models-nyc-ml-meetup NYC ML	-	street2shop - topwear
-02	-	street2shop - topwear
-22	-	street2shop - topwear
-02	-	street2shop - topwear
-22	-	street2shop - topwear
- scale machine learning on heterogeneous	-	street2shop - topwear
- 2605	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-02	-	street2shop - topwear
-22	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
-	-	street2shop - topwear
are applied to the acoustic mixed-speech spectrogram. Results show that: (i	mixed-speech	TCD-TIMIT corpus (mixed-speech)
landmark features and the input mixed-speech spectrogram	mixed-speech	TCD-TIMIT corpus (mixed-speech)
applied to clean the acoustic mixed-speech spectrogram	mixed-speech	TCD-TIMIT corpus (mixed-speech)
compressed spectrogram of the single-channel mixed-speech signal. All of them perform	mixed-speech	TCD-TIMIT corpus (mixed-speech)
although IAM generation requires the mixed-speech spectrogram, separate spectrograms for each	mixed-speech	TCD-TIMIT corpus (mixed-speech)
of them, we created a mixed-speech version	mixed-speech	TCD-TIMIT corpus (mixed-speech)
98 utterances per speaker. The mixed-speech version was created following the	mixed-speech	TCD-TIMIT corpus (mixed-speech)
metric values of the input mixed-speech signal	mixed-speech	TCD-TIMIT corpus (mixed-speech)
speaker’s spectrogram from the acoustic mixed-speech spectrogram	mixed-speech	TCD-TIMIT corpus (mixed-speech)
are applied to the acoustic mixed-speech spectrogram. Results show that: (i	(mixed-speech)	TCD-TIMIT corpus (mixed-speech)
landmark features and the input mixed-speech spectrogram	(mixed-speech)	TCD-TIMIT corpus (mixed-speech)
applied to clean the acoustic mixed-speech spectrogram	(mixed-speech)	TCD-TIMIT corpus (mixed-speech)
compressed spectrogram of the single-channel mixed-speech signal. All of them perform	(mixed-speech)	TCD-TIMIT corpus (mixed-speech)
although IAM generation requires the mixed-speech spectrogram, separate spectrograms for each	(mixed-speech)	TCD-TIMIT corpus (mixed-speech)
of them, we created a mixed-speech version	(mixed-speech)	TCD-TIMIT corpus (mixed-speech)
98 utterances per speaker. The mixed-speech version was created following the	(mixed-speech)	TCD-TIMIT corpus (mixed-speech)
metric values of the input mixed-speech signal	(mixed-speech)	TCD-TIMIT corpus (mixed-speech)
speaker’s spectrogram from the acoustic mixed-speech spectrogram	(mixed-speech)	TCD-TIMIT corpus (mixed-speech)
Regarding the GRID corpus, for each of the 33	corpus	TCD-TIMIT corpus (mixed-speech)
The TCD-TIMIT corpus consists of 59 speakers (we	corpus	TCD-TIMIT corpus (mixed-speech)
speaker-dependent models on the GRID corpus with landmark motion vectors. Results	corpus	TCD-TIMIT corpus (mixed-speech)
and Xu Shao, “An audio-visual corpus for speech perception and automatic	corpus	TCD-TIMIT corpus (mixed-speech)
the limited size GRID and TCD-TIMIT datasets, that achieve speaker-independent speech	TCD-TIMIT	TCD-TIMIT corpus (mixed-speech)
on the GRID [9] and TCD-TIMIT [10] datasets in a speaker-independent	TCD-TIMIT	TCD-TIMIT corpus (mixed-speech)
using the GRID [9] and TCD-TIMIT [10] audio-visual datasets. For each	TCD-TIMIT	TCD-TIMIT corpus (mixed-speech)
The TCD-TIMIT corpus consists of 59 speakers	TCD-TIMIT	TCD-TIMIT corpus (mixed-speech)
difference. Con- trary to GRID, TCD-TIMIT utterances have different dura- tion	TCD-TIMIT	TCD-TIMIT corpus (mixed-speech)
TCD-TIMIT) to 100 fps to match	TCD-TIMIT	TCD-TIMIT corpus (mixed-speech)
Table 3. TCD-TIMIT results - speaker-independent	TCD-TIMIT	TCD-TIMIT corpus (mixed-speech)
results on the GRID and TCD-TIMIT datasets respectively. V2ML performs significantly	TCD-TIMIT	TCD-TIMIT corpus (mixed-speech)
the limited size GRID and TCD-TIMIT datasets that accomplish speaker- independent	TCD-TIMIT	TCD-TIMIT corpus (mixed-speech)
TCD-TIMIT	TCD-TIMIT	TCD-TIMIT corpus (mixed-speech)
FACE LANDMARK-BASED SPEAKER-INDEPENDENT AUDIO-VISUAL SPEECH 		TCD-TIMIT corpus (mixed-speech)
ENHANCEMENT IN MULTI-TALKER ENVIRONMENTS  Giovanni Morrone? Luca Pasa† Vadim		TCD-TIMIT corpus (mixed-speech)
 Tikhanoff		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
Sonia Bergamaschi? Luciano Fadiga† Leonardo 		TCD-TIMIT corpus (mixed-speech)
Badino†  ?Department of Engineering ”Enzo Ferrari		TCD-TIMIT corpus (mixed-speech)
”, University of Modena and 		TCD-TIMIT corpus (mixed-speech)
Reggio Emilia, Modena, Italy †Istituto 		TCD-TIMIT corpus (mixed-speech)
Italiano di Tecnologia, Ferrara, Italy  ABSTRACT		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
In this paper, we address 		TCD-TIMIT corpus (mixed-speech)
the problem of enhancing the 		TCD-TIMIT corpus (mixed-speech)
speech of a speaker of 		TCD-TIMIT corpus (mixed-speech)
interest in a cocktail party 		TCD-TIMIT corpus (mixed-speech)
scenario when vi- sual information 		TCD-TIMIT corpus (mixed-speech)
of the speaker of interest 		TCD-TIMIT corpus (mixed-speech)
is available.  Contrary to most previous studies		TCD-TIMIT corpus (mixed-speech)
, we do not learn 		TCD-TIMIT corpus (mixed-speech)
visual features on the typically 		TCD-TIMIT corpus (mixed-speech)
small audio-visual datasets, but use 		TCD-TIMIT corpus (mixed-speech)
an already available face landmark 		TCD-TIMIT corpus (mixed-speech)
detector (trained on a sep- 		TCD-TIMIT corpus (mixed-speech)
arate image dataset).  The landmarks are used by		TCD-TIMIT corpus (mixed-speech)
 LSTM-based models to gen- erate		TCD-TIMIT corpus (mixed-speech)
 time-frequency masks which are applied		TCD-TIMIT corpus (mixed-speech)
 to the acoustic mixed-speech spectrogram		TCD-TIMIT corpus (mixed-speech)
. Results show that: (i) 		TCD-TIMIT corpus (mixed-speech)
land- mark motion features are 		TCD-TIMIT corpus (mixed-speech)
very effective features for this 		TCD-TIMIT corpus (mixed-speech)
task, (ii) similarly to previous 		TCD-TIMIT corpus (mixed-speech)
work, reconstruction of the target 		TCD-TIMIT corpus (mixed-speech)
speaker’s spectrogram mediated by masking 		TCD-TIMIT corpus (mixed-speech)
is significantly more accurate than 		TCD-TIMIT corpus (mixed-speech)
direct spectrogram reconstruction, and (iii) 		TCD-TIMIT corpus (mixed-speech)
the best masks depend on 		TCD-TIMIT corpus (mixed-speech)
both motion landmark features and 		TCD-TIMIT corpus (mixed-speech)
the input mixed-speech spectrogram.  To the best of our		TCD-TIMIT corpus (mixed-speech)
 knowledge, our proposed models are		TCD-TIMIT corpus (mixed-speech)
 the first models trained and		TCD-TIMIT corpus (mixed-speech)
 evaluated on the limited size		TCD-TIMIT corpus (mixed-speech)
 GRID and TCD-TIMIT datasets, that		TCD-TIMIT corpus (mixed-speech)
 achieve speaker-independent speech enhancement in		TCD-TIMIT corpus (mixed-speech)
 a multi-talker setting		TCD-TIMIT corpus (mixed-speech)
.  Index Terms— audio-visual speech enhancement		TCD-TIMIT corpus (mixed-speech)
, cock- tail party problem, 		TCD-TIMIT corpus (mixed-speech)
time-frequency mask, LSTM, face land- 		TCD-TIMIT corpus (mixed-speech)
marks  1. INTRODUCTION		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
In the context of speech 		TCD-TIMIT corpus (mixed-speech)
perception, the cocktail party 		TCD-TIMIT corpus (mixed-speech)
effect [1, 2] is the 		TCD-TIMIT corpus (mixed-speech)
ability of the brain to 		TCD-TIMIT corpus (mixed-speech)
recognize speech in complex and 		TCD-TIMIT corpus (mixed-speech)
adverse listening conditions where the 		TCD-TIMIT corpus (mixed-speech)
attended speech is mixed with 		TCD-TIMIT corpus (mixed-speech)
competing sounds/speech.  Speech perception studies have shown		TCD-TIMIT corpus (mixed-speech)
 that watching speaker’s face movements		TCD-TIMIT corpus (mixed-speech)
 could dramatically improve our ability		TCD-TIMIT corpus (mixed-speech)
 at recognizing the speech of		TCD-TIMIT corpus (mixed-speech)
 a target speaker in a		TCD-TIMIT corpus (mixed-speech)
 multi-talker environment [3, 4		TCD-TIMIT corpus (mixed-speech)
].  This work aims at extracting		TCD-TIMIT corpus (mixed-speech)
 the speech of a target		TCD-TIMIT corpus (mixed-speech)
 speaker from single channel audio		TCD-TIMIT corpus (mixed-speech)
 of several people talking simulta		TCD-TIMIT corpus (mixed-speech)
- neously. This is an 		TCD-TIMIT corpus (mixed-speech)
ill-posed problem in that many 		TCD-TIMIT corpus (mixed-speech)
differ- ent hypotheses about what 		TCD-TIMIT corpus (mixed-speech)
the target speaker says are 		TCD-TIMIT corpus (mixed-speech)
con-  sistent with the mixture signal		TCD-TIMIT corpus (mixed-speech)
. Yet, it can be 		TCD-TIMIT corpus (mixed-speech)
solved by ex- ploiting some 		TCD-TIMIT corpus (mixed-speech)
additional information associated to the 		TCD-TIMIT corpus (mixed-speech)
speaker of interest and/or by 		TCD-TIMIT corpus (mixed-speech)
leveraging some prior knowledge about 		TCD-TIMIT corpus (mixed-speech)
speech signal properties (e.g., [5]). 		TCD-TIMIT corpus (mixed-speech)
In this work we use 		TCD-TIMIT corpus (mixed-speech)
face movements of the target 		TCD-TIMIT corpus (mixed-speech)
speaker as additional information.  This paper (i) proposes the		TCD-TIMIT corpus (mixed-speech)
 use of face landmark’s move		TCD-TIMIT corpus (mixed-speech)
- ments, extracted using 		TCD-TIMIT corpus (mixed-speech)
Dlib [6, 7] and (ii) 		TCD-TIMIT corpus (mixed-speech)
compares differ- ent ways of 		TCD-TIMIT corpus (mixed-speech)
mapping such visual features into 		TCD-TIMIT corpus (mixed-speech)
time-frequency (T-F) masks, then applied 		TCD-TIMIT corpus (mixed-speech)
to clean the acoustic mixed-speech 		TCD-TIMIT corpus (mixed-speech)
spectrogram.  By using Dlib extracted landmarks		TCD-TIMIT corpus (mixed-speech)
 we relieve our mod- els		TCD-TIMIT corpus (mixed-speech)
 from the task of learning		TCD-TIMIT corpus (mixed-speech)
 useful visual features from raw		TCD-TIMIT corpus (mixed-speech)
 pixels. That aspect is particularly		TCD-TIMIT corpus (mixed-speech)
 relevant when the training audio-visual		TCD-TIMIT corpus (mixed-speech)
 datasets are small		TCD-TIMIT corpus (mixed-speech)
.  The analysis of landmark-dependent masking		TCD-TIMIT corpus (mixed-speech)
 strategies is motivated by the		TCD-TIMIT corpus (mixed-speech)
 fact that speech enhancement mediated		TCD-TIMIT corpus (mixed-speech)
 by an explicit masking is		TCD-TIMIT corpus (mixed-speech)
 often more effective than mask-free		TCD-TIMIT corpus (mixed-speech)
 enhancement [8		TCD-TIMIT corpus (mixed-speech)
].  All our models were trained		TCD-TIMIT corpus (mixed-speech)
 and evaluated on the GRID		TCD-TIMIT corpus (mixed-speech)
 [9] and TCD-TIMIT [10] datasets		TCD-TIMIT corpus (mixed-speech)
 in a speaker-independent setting		TCD-TIMIT corpus (mixed-speech)
.  1.1. Related work		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
Speech enhancement aims at extracting 		TCD-TIMIT corpus (mixed-speech)
the voice of a tar- 		TCD-TIMIT corpus (mixed-speech)
get speaker, while speech separation 		TCD-TIMIT corpus (mixed-speech)
refers to the problem of 		TCD-TIMIT corpus (mixed-speech)
separating each sound source in 		TCD-TIMIT corpus (mixed-speech)
a mixture. Recently pro- posed 		TCD-TIMIT corpus (mixed-speech)
audio-only single-channel methods have achieved 		TCD-TIMIT corpus (mixed-speech)
very promising results [11, 12, 13		TCD-TIMIT corpus (mixed-speech)
]. However the task still 		TCD-TIMIT corpus (mixed-speech)
remains challenging. Additionally, audio-only systems 		TCD-TIMIT corpus (mixed-speech)
need separate models in order 		TCD-TIMIT corpus (mixed-speech)
to associate the estimated separated 		TCD-TIMIT corpus (mixed-speech)
audio sources to each speaker, 		TCD-TIMIT corpus (mixed-speech)
while vision easily allow that 		TCD-TIMIT corpus (mixed-speech)
in a unified model.  Regarding audio-visual speech enhancement and		TCD-TIMIT corpus (mixed-speech)
 separa- tion methods an extensive		TCD-TIMIT corpus (mixed-speech)
 review is provided in [14		TCD-TIMIT corpus (mixed-speech)
]. Here we focus on 		TCD-TIMIT corpus (mixed-speech)
the deep-learning methods that are 		TCD-TIMIT corpus (mixed-speech)
most related to the present 		TCD-TIMIT corpus (mixed-speech)
work.  Our first architecture (Section 2.1		TCD-TIMIT corpus (mixed-speech)
) is inspired by [15], 		TCD-TIMIT corpus (mixed-speech)
where a pre-trained convolutional neural 		TCD-TIMIT corpus (mixed-speech)
network (CNN) is used to 		TCD-TIMIT corpus (mixed-speech)
generate a clean spectrogram from 		TCD-TIMIT corpus (mixed-speech)
silent video [16]. Rather than 		TCD-TIMIT corpus (mixed-speech)
directly computing a time-frequency (T-F) 		TCD-TIMIT corpus (mixed-speech)
mask,  ar X		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
iv :1  81 1		TCD-TIMIT corpus (mixed-speech)
.  02 48		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
0v 3		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
cs  .C L		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
2		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
M 		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
2 01  9		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
the mask is computed by 		TCD-TIMIT corpus (mixed-speech)
thresholding the estimated clean spectrogram. 		TCD-TIMIT corpus (mixed-speech)
This approach is not very 		TCD-TIMIT corpus (mixed-speech)
effective since the pre-trained CNN 		TCD-TIMIT corpus (mixed-speech)
is designed for a different 		TCD-TIMIT corpus (mixed-speech)
task (video-to- speech synthesis). 		TCD-TIMIT corpus (mixed-speech)
In [17] a CNN is 		TCD-TIMIT corpus (mixed-speech)
trained to directly esti- mate 		TCD-TIMIT corpus (mixed-speech)
clean speech from noisy audio 		TCD-TIMIT corpus (mixed-speech)
and input video. A sim- 		TCD-TIMIT corpus (mixed-speech)
ilar model is used 		TCD-TIMIT corpus (mixed-speech)
in [18], where the model 		TCD-TIMIT corpus (mixed-speech)
jointly generates clean speech and 		TCD-TIMIT corpus (mixed-speech)
input video in a denoising-autoender 		TCD-TIMIT corpus (mixed-speech)
archi- tecture.  [19] shows that using information		TCD-TIMIT corpus (mixed-speech)
 about lip positions can help		TCD-TIMIT corpus (mixed-speech)
 to improve speech enhancement. The		TCD-TIMIT corpus (mixed-speech)
 video feature vec- tor is		TCD-TIMIT corpus (mixed-speech)
 obtained computing pair-wise distances between		TCD-TIMIT corpus (mixed-speech)
 any mouth landmarks. Similarly to		TCD-TIMIT corpus (mixed-speech)
 our approach their visual fea		TCD-TIMIT corpus (mixed-speech)
- tures are not learned 		TCD-TIMIT corpus (mixed-speech)
on the audio-visual dataset but 		TCD-TIMIT corpus (mixed-speech)
are pro- vided by a 		TCD-TIMIT corpus (mixed-speech)
system trained on different dataset. 		TCD-TIMIT corpus (mixed-speech)
Contrary to our approach, [19] 		TCD-TIMIT corpus (mixed-speech)
uses position-based features while we 		TCD-TIMIT corpus (mixed-speech)
use motion features (of the 		TCD-TIMIT corpus (mixed-speech)
whole face) that in our 		TCD-TIMIT corpus (mixed-speech)
experiments turned out to be 		TCD-TIMIT corpus (mixed-speech)
much more effective than positional 		TCD-TIMIT corpus (mixed-speech)
features.  Although the aforementioned audio-visual methods		TCD-TIMIT corpus (mixed-speech)
 work well, they have only		TCD-TIMIT corpus (mixed-speech)
 been evaluated in a speaker-dependent		TCD-TIMIT corpus (mixed-speech)
 setting. Only the availability of		TCD-TIMIT corpus (mixed-speech)
 new large and heterogeneous audio-visual		TCD-TIMIT corpus (mixed-speech)
 datasets has allowed the training		TCD-TIMIT corpus (mixed-speech)
 of deep neu- ral network-based		TCD-TIMIT corpus (mixed-speech)
 speaker-independent speech enhancement models [20		TCD-TIMIT corpus (mixed-speech)
, 21, 22].  The present work shows that		TCD-TIMIT corpus (mixed-speech)
 huge audio-visual datasets are not		TCD-TIMIT corpus (mixed-speech)
 a necessary requirement for speaker-independent		TCD-TIMIT corpus (mixed-speech)
 audio-visual speech enhancement. Although we		TCD-TIMIT corpus (mixed-speech)
 have only considered datasets with		TCD-TIMIT corpus (mixed-speech)
 simple visual scenarios (i.e., the		TCD-TIMIT corpus (mixed-speech)
 target speaker is always facing		TCD-TIMIT corpus (mixed-speech)
 the camera), we expect our		TCD-TIMIT corpus (mixed-speech)
 methods to perform well in		TCD-TIMIT corpus (mixed-speech)
 more complex scenarios thanks to		TCD-TIMIT corpus (mixed-speech)
 the robust landmark extraction		TCD-TIMIT corpus (mixed-speech)
.  2. MODEL ARCHITECTURES		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
We experimented with the four 		TCD-TIMIT corpus (mixed-speech)
models shown in Fig. 1. 		TCD-TIMIT corpus (mixed-speech)
All models receive in input 		TCD-TIMIT corpus (mixed-speech)
the target speaker’s landmark mo- 		TCD-TIMIT corpus (mixed-speech)
tion vectors and the power-law 		TCD-TIMIT corpus (mixed-speech)
compressed spectrogram of the single-channel 		TCD-TIMIT corpus (mixed-speech)
mixed-speech signal. All of them 		TCD-TIMIT corpus (mixed-speech)
perform some kind of masking 		TCD-TIMIT corpus (mixed-speech)
operation.  2.1. VL2M model		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
At each time frame, the 		TCD-TIMIT corpus (mixed-speech)
video-landmark to mask (VL2M) model (		TCD-TIMIT corpus (mixed-speech)
Fig. 1a) estimates a T-F 		TCD-TIMIT corpus (mixed-speech)
mask from visual features only (		TCD-TIMIT corpus (mixed-speech)
of the target speaker). Formally, 		TCD-TIMIT corpus (mixed-speech)
given a video sequence 		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
 = [v1		TCD-TIMIT corpus (mixed-speech)
, . . . , 		TCD-TIMIT corpus (mixed-speech)
vT ], vt ∈ Rn 		TCD-TIMIT corpus (mixed-speech)
and a target mask sequence 		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
 = [m1		TCD-TIMIT corpus (mixed-speech)
, . . . ,		TCD-TIMIT corpus (mixed-speech)
mT ], mt ∈ Rd, 		TCD-TIMIT corpus (mixed-speech)
VL2M perform a function 		TCD-TIMIT corpus (mixed-speech)
Fvl2m(v) = m̂, where m̂ 		TCD-TIMIT corpus (mixed-speech)
is the estimated mask.  The training objective for VL2M		TCD-TIMIT corpus (mixed-speech)
 is a Target Binary Mask		TCD-TIMIT corpus (mixed-speech)
 (TBM) [23, 24], computed using		TCD-TIMIT corpus (mixed-speech)
 the spectrogram of the tar		TCD-TIMIT corpus (mixed-speech)
- get speaker only. This 		TCD-TIMIT corpus (mixed-speech)
is motivated by our goal 		TCD-TIMIT corpus (mixed-speech)
of extracting the speech of 		TCD-TIMIT corpus (mixed-speech)
a target speaker as much 		TCD-TIMIT corpus (mixed-speech)
as possible indepen- dently of 		TCD-TIMIT corpus (mixed-speech)
the concurrent speakers, so that, 		TCD-TIMIT corpus (mixed-speech)
e.g., we do not need 		TCD-TIMIT corpus (mixed-speech)
to estimate their number. An 		TCD-TIMIT corpus (mixed-speech)
additional motivations is that the 		TCD-TIMIT corpus (mixed-speech)
model takes as only input 		TCD-TIMIT corpus (mixed-speech)
the visual features of the  target speaker, and a target		TCD-TIMIT corpus (mixed-speech)
 TBM that only depends on		TCD-TIMIT corpus (mixed-speech)
 the target speaker allows VL2M		TCD-TIMIT corpus (mixed-speech)
 to learn a function (rather		TCD-TIMIT corpus (mixed-speech)
 than approximating an ill-posed one-to-many		TCD-TIMIT corpus (mixed-speech)
 mapping		TCD-TIMIT corpus (mixed-speech)
).  Given a clean speech spectrogram		TCD-TIMIT corpus (mixed-speech)
 of a speaker s		TCD-TIMIT corpus (mixed-speech)
 = [s1		TCD-TIMIT corpus (mixed-speech)
, . . . , 		TCD-TIMIT corpus (mixed-speech)
sT ], st ∈ Rd, 		TCD-TIMIT corpus (mixed-speech)
the TBM is defined by 		TCD-TIMIT corpus (mixed-speech)
comparing, at each frequency bin 		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
 ∈ [1		TCD-TIMIT corpus (mixed-speech)
, . . . , 		TCD-TIMIT corpus (mixed-speech)
d], the target speaker value 		TCD-TIMIT corpus (mixed-speech)
st[f ] vs. a reference 		TCD-TIMIT corpus (mixed-speech)
threshold τ [f ]. As 		TCD-TIMIT corpus (mixed-speech)
in [15], we use a 		TCD-TIMIT corpus (mixed-speech)
function of long-term average speech 		TCD-TIMIT corpus (mixed-speech)
spectrum (LTASS) as reference threshold. 		TCD-TIMIT corpus (mixed-speech)
This threshold indicates if a 		TCD-TIMIT corpus (mixed-speech)
T-F unit is generated by 		TCD-TIMIT corpus (mixed-speech)
the speaker or refers to 		TCD-TIMIT corpus (mixed-speech)
silence or noise. The process 		TCD-TIMIT corpus (mixed-speech)
to compute the speaker’s TBM 		TCD-TIMIT corpus (mixed-speech)
is as follows:  1. The mean π[f		TCD-TIMIT corpus (mixed-speech)
 ] and the standard deviation		TCD-TIMIT corpus (mixed-speech)
 σ[f ] are computed for		TCD-TIMIT corpus (mixed-speech)
 all frequency bins of all		TCD-TIMIT corpus (mixed-speech)
 seen spectro- grams in speaker’s		TCD-TIMIT corpus (mixed-speech)
 data		TCD-TIMIT corpus (mixed-speech)
.  2. The threshold τ [f		TCD-TIMIT corpus (mixed-speech)
 ] is defined as τ		TCD-TIMIT corpus (mixed-speech)
 [f ] = π[f ]+0.6		TCD-TIMIT corpus (mixed-speech)
 ·σ[f ] where 0.6 is		TCD-TIMIT corpus (mixed-speech)
 a value selected by manual		TCD-TIMIT corpus (mixed-speech)
 inspection of several spectrogram-TBM pairs		TCD-TIMIT corpus (mixed-speech)
.  3. The threshold is applied		TCD-TIMIT corpus (mixed-speech)
 to every speaker’s speech spec		TCD-TIMIT corpus (mixed-speech)
- trogram s.  mt[f		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
1, if st[f ] ≥ 		TCD-TIMIT corpus (mixed-speech)
τ [f ], 0, otherwise.  The mapping Fvl2m(·) is carried		TCD-TIMIT corpus (mixed-speech)
 out by a stacked bi		TCD-TIMIT corpus (mixed-speech)
- directional Long Short-Term Memory (		TCD-TIMIT corpus (mixed-speech)
BLSTM) network [25]. The BLSTM 		TCD-TIMIT corpus (mixed-speech)
outputs are then forced to 		TCD-TIMIT corpus (mixed-speech)
lay within the [0, 1] 		TCD-TIMIT corpus (mixed-speech)
range. Finally the computed TBM 		TCD-TIMIT corpus (mixed-speech)
m̂ and the noisy spectrogram 		TCD-TIMIT corpus (mixed-speech)
y are element-wise multiplied to 		TCD-TIMIT corpus (mixed-speech)
ob- tain the estimated clean 		TCD-TIMIT corpus (mixed-speech)
spectrogram ŝm = m̂ ◦ 		TCD-TIMIT corpus (mixed-speech)
y, where 		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
 = [y1		TCD-TIMIT corpus (mixed-speech)
, . . . 		TCD-TIMIT corpus (mixed-speech)
yT ], yt ∈ Rd.  The model parameters are estimated		TCD-TIMIT corpus (mixed-speech)
 to minimize the loss		TCD-TIMIT corpus (mixed-speech)
:  Jvl2m = ∑T		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
t=1  ∑d f=1−mt[f ] · log(m̂t[f		TCD-TIMIT corpus (mixed-speech)
 ])− (1−mt[f ]) · log(1		TCD-TIMIT corpus (mixed-speech)
− m̂t[f ])  2.2. VL2M ref model		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
VL2M generates T-F masks that 		TCD-TIMIT corpus (mixed-speech)
are independent of the acous- 		TCD-TIMIT corpus (mixed-speech)
tic context. We may want 		TCD-TIMIT corpus (mixed-speech)
to refine the masking by 		TCD-TIMIT corpus (mixed-speech)
including such context. This is 		TCD-TIMIT corpus (mixed-speech)
what the novel VL2M ref 		TCD-TIMIT corpus (mixed-speech)
does (Fig. 1b). The computed 		TCD-TIMIT corpus (mixed-speech)
TBM m̂ and the input 		TCD-TIMIT corpus (mixed-speech)
spectrogram y are the input 		TCD-TIMIT corpus (mixed-speech)
to a function that outputs 		TCD-TIMIT corpus (mixed-speech)
an Ideal Amplitude Mask (IAM) 		TCD-TIMIT corpus (mixed-speech)
p (known as FFT-MASK 		TCD-TIMIT corpus (mixed-speech)
in [8]). Given the target 		TCD-TIMIT corpus (mixed-speech)
clean spectrogram s and the 		TCD-TIMIT corpus (mixed-speech)
noisy spectrogram y, the IAM 		TCD-TIMIT corpus (mixed-speech)
is defined as:  pt[f ] = st[f		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
yt[f ]  Note that although IAM generation		TCD-TIMIT corpus (mixed-speech)
 requires the mixed-speech spectrogram, separate		TCD-TIMIT corpus (mixed-speech)
 spectrograms for each concurrent speakers		TCD-TIMIT corpus (mixed-speech)
 are not required		TCD-TIMIT corpus (mixed-speech)
.  The target speaker’s spectrogram s		TCD-TIMIT corpus (mixed-speech)
 is reconstructed by multiplying the		TCD-TIMIT corpus (mixed-speech)
 input spectrogram with the estimated		TCD-TIMIT corpus (mixed-speech)
 IAM. Values greater than 10		TCD-TIMIT corpus (mixed-speech)
 in the IAM are clipped		TCD-TIMIT corpus (mixed-speech)
 to 10 in order to		TCD-TIMIT corpus (mixed-speech)
 obtain better numerical stability as		TCD-TIMIT corpus (mixed-speech)
 suggested in [8		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
v: video input y: noisy 		TCD-TIMIT corpus (mixed-speech)
spectrogram sm: clean spectrogram TBM 		TCD-TIMIT corpus (mixed-speech)
s: clean spectrogram IAM m: 		TCD-TIMIT corpus (mixed-speech)
TBM p: IAM  STACKED		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
BLSTM  m		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
sm  v		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
y  (a) VL2M		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
v VL2M m  y BLSTM		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
BLSTM  Fusion layer		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
BLSTM p  s		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
b) VL2M ref  v		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
y  p STACKED		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
BLSTM  s		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
c) Audio-Visual concat  sm		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
y  p STACKED		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
BLSTM  s		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
v VL2M m  (d) Audio-Visual concat-ref		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
Fig. 1. Model architectures.  The model performs a function		TCD-TIMIT corpus (mixed-speech)
 Fmr(v, y) = p̂ that		TCD-TIMIT corpus (mixed-speech)
 con- sists of a VL2M		TCD-TIMIT corpus (mixed-speech)
 component plus three different BLSTMs		TCD-TIMIT corpus (mixed-speech)
 Gm, Gy and H		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
Gm(Fvl2m(v)) = rm receives the 		TCD-TIMIT corpus (mixed-speech)
VL2M mask m̂ as in- 		TCD-TIMIT corpus (mixed-speech)
put, and Gy(y) = ry 		TCD-TIMIT corpus (mixed-speech)
is fed with the noisy 		TCD-TIMIT corpus (mixed-speech)
spectrogram. Their output rm, 		TCD-TIMIT corpus (mixed-speech)
ry ∈ Rz are fused 		TCD-TIMIT corpus (mixed-speech)
in a joint audio-visual represen- 		TCD-TIMIT corpus (mixed-speech)
tation 		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
 = [h1		TCD-TIMIT corpus (mixed-speech)
, . . . ,		TCD-TIMIT corpus (mixed-speech)
hT ], where ht is 		TCD-TIMIT corpus (mixed-speech)
a linear combination of rmt 		TCD-TIMIT corpus (mixed-speech)
and ryt : ht = 		TCD-TIMIT corpus (mixed-speech)
Whm ·rmt +Why ·ryt +bh. 		TCD-TIMIT corpus (mixed-speech)
h is the input of 		TCD-TIMIT corpus (mixed-speech)
the third BLSTM H (		TCD-TIMIT corpus (mixed-speech)
h) = p̂, where p̂ 		TCD-TIMIT corpus (mixed-speech)
lays in the [0,10] range. 		TCD-TIMIT corpus (mixed-speech)
The loss function is:  Jmr		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
T∑ t=1  d∑ f=1		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
p̂t[f ] · yt[f ]− 		TCD-TIMIT corpus (mixed-speech)
st[f ])2  2.3. Audio-Visual concat model		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
The third model (Fig. 1c) 		TCD-TIMIT corpus (mixed-speech)
performs early fusion of audio- 		TCD-TIMIT corpus (mixed-speech)
visual features. This model consists 		TCD-TIMIT corpus (mixed-speech)
of a single stacked BLSTM 		TCD-TIMIT corpus (mixed-speech)
that computes the IAM mask 		TCD-TIMIT corpus (mixed-speech)
p̂ from the concate- 		TCD-TIMIT corpus (mixed-speech)
nated [v,y]. The training loss 		TCD-TIMIT corpus (mixed-speech)
is the same Jmr used 		TCD-TIMIT corpus (mixed-speech)
to train VL2M ref. This 		TCD-TIMIT corpus (mixed-speech)
model can be regarded as 		TCD-TIMIT corpus (mixed-speech)
a simplification of VL2M ref, 		TCD-TIMIT corpus (mixed-speech)
where the VL2M operation is 		TCD-TIMIT corpus (mixed-speech)
not performed.  2.4. Audio-Visual concat-ref model		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
The fourth model (Fig. 1d) 		TCD-TIMIT corpus (mixed-speech)
is an improved version of 		TCD-TIMIT corpus (mixed-speech)
the model described in section 2		TCD-TIMIT corpus (mixed-speech)
.3. The only difference is 		TCD-TIMIT corpus (mixed-speech)
the input of the stacked 		TCD-TIMIT corpus (mixed-speech)
BLSTM that is replaced 		TCD-TIMIT corpus (mixed-speech)
by [̂sm,y] where ŝm is 		TCD-TIMIT corpus (mixed-speech)
the denoised spectrogram returned by 		TCD-TIMIT corpus (mixed-speech)
VL2M operation.  3. EXPERIMENTAL SETUP		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
3.1. Dataset  All experiments were carried out		TCD-TIMIT corpus (mixed-speech)
 using the GRID [9] and		TCD-TIMIT corpus (mixed-speech)
 TCD-TIMIT [10] audio-visual datasets. For		TCD-TIMIT corpus (mixed-speech)
 each of them, we created		TCD-TIMIT corpus (mixed-speech)
 a mixed-speech version		TCD-TIMIT corpus (mixed-speech)
.  Regarding the GRID corpus, for		TCD-TIMIT corpus (mixed-speech)
 each of the 33 speakers		TCD-TIMIT corpus (mixed-speech)
 (one had to be discarded		TCD-TIMIT corpus (mixed-speech)
) we first randomly selected 200 ut- terances (out of 1000		TCD-TIMIT corpus (mixed-speech)
). Then, for each utterance, 		TCD-TIMIT corpus (mixed-speech)
we created 3 different audio-mixed 		TCD-TIMIT corpus (mixed-speech)
samples. Each audio-mixed sample was 		TCD-TIMIT corpus (mixed-speech)
created by mixing the chosen 		TCD-TIMIT corpus (mixed-speech)
utterance with one utter- ance 		TCD-TIMIT corpus (mixed-speech)
from a different speaker.  That resulted in 600 audio-mixed		TCD-TIMIT corpus (mixed-speech)
 samples per speaker		TCD-TIMIT corpus (mixed-speech)
.  The resulting dataset was split		TCD-TIMIT corpus (mixed-speech)
 into disjoint sets of 25/4/4		TCD-TIMIT corpus (mixed-speech)
 speakers for training/validation/testing respectively		TCD-TIMIT corpus (mixed-speech)
.  The TCD-TIMIT corpus consists of		TCD-TIMIT corpus (mixed-speech)
 59 speakers (we ex- cluded		TCD-TIMIT corpus (mixed-speech)
 3 professionally-trained lipspeakers) and 98		TCD-TIMIT corpus (mixed-speech)
 utterances per speaker. The mixed-speech		TCD-TIMIT corpus (mixed-speech)
 version was created following the		TCD-TIMIT corpus (mixed-speech)
 same procedure as for GRID		TCD-TIMIT corpus (mixed-speech)
, with one difference. Con- 		TCD-TIMIT corpus (mixed-speech)
trary to GRID, TCD-TIMIT utterances 		TCD-TIMIT corpus (mixed-speech)
have different dura- tion. Thus 2 utterances were mixed only if		TCD-TIMIT corpus (mixed-speech)
 their duration dif- ference did		TCD-TIMIT corpus (mixed-speech)
 not exceed 2 seconds. For		TCD-TIMIT corpus (mixed-speech)
 each utterance pair, we forced		TCD-TIMIT corpus (mixed-speech)
 the non-target speaker’s utterance to		TCD-TIMIT corpus (mixed-speech)
 match the du- ration of		TCD-TIMIT corpus (mixed-speech)
 the target speaker utterance. If		TCD-TIMIT corpus (mixed-speech)
 it was longer, the utterance		TCD-TIMIT corpus (mixed-speech)
 was cut at its end		TCD-TIMIT corpus (mixed-speech)
, whereas if it was 		TCD-TIMIT corpus (mixed-speech)
shorter, silence samples were equally 		TCD-TIMIT corpus (mixed-speech)
added at its start and 		TCD-TIMIT corpus (mixed-speech)
end.  The resulting dataset was split		TCD-TIMIT corpus (mixed-speech)
 into disjoint sets of 51/4/4		TCD-TIMIT corpus (mixed-speech)
 speakers for training/validation/testing respectively		TCD-TIMIT corpus (mixed-speech)
.  3.2. LSTM training		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
In all experiments, the models 		TCD-TIMIT corpus (mixed-speech)
were trained using the Adam 		TCD-TIMIT corpus (mixed-speech)
optimizer [26]. Early stopping was 		TCD-TIMIT corpus (mixed-speech)
applied when the error on 		TCD-TIMIT corpus (mixed-speech)
the validation set did not 		TCD-TIMIT corpus (mixed-speech)
decrease over 5 consecutive epochs.  VL2M, AV concat and AV		TCD-TIMIT corpus (mixed-speech)
 concat-ref had 5, 3 and		TCD-TIMIT corpus (mixed-speech)
 3 stacked BLSTM layers respectively		TCD-TIMIT corpus (mixed-speech)
. All BLSTMs had 250 		TCD-TIMIT corpus (mixed-speech)
units. Hyper-parameters selection was performed 		TCD-TIMIT corpus (mixed-speech)
by using random search with 		TCD-TIMIT corpus (mixed-speech)
a limited number of samples, 		TCD-TIMIT corpus (mixed-speech)
therefore all the reported results 		TCD-TIMIT corpus (mixed-speech)
may improve through a deeper 		TCD-TIMIT corpus (mixed-speech)
hyper- parameters validation phase.  VL2M ref and AV concat-ref		TCD-TIMIT corpus (mixed-speech)
 training was performed in 2		TCD-TIMIT corpus (mixed-speech)
 steps. We first pre-trained the		TCD-TIMIT corpus (mixed-speech)
 models using the oracle TBM		TCD-TIMIT corpus (mixed-speech)
 m. Then we substituted the		TCD-TIMIT corpus (mixed-speech)
 oracle masks with the VL2M		TCD-TIMIT corpus (mixed-speech)
 component and retrained the models		TCD-TIMIT corpus (mixed-speech)
 while freezing the pa- rameters		TCD-TIMIT corpus (mixed-speech)
 of the VL2M component		TCD-TIMIT corpus (mixed-speech)
.  3.3. Audio pre- and post-processing		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
The original waveforms were resampled 		TCD-TIMIT corpus (mixed-speech)
to 16 kHz. Short- Time 		TCD-TIMIT corpus (mixed-speech)
Fourier Transform (STFT) x was 		TCD-TIMIT corpus (mixed-speech)
computed using FFT size of 512, Hann window of length 25		TCD-TIMIT corpus (mixed-speech)
 ms (400 samples), and hop		TCD-TIMIT corpus (mixed-speech)
 length of 10 ms (160		TCD-TIMIT corpus (mixed-speech)
 samples). The input spectro- gram		TCD-TIMIT corpus (mixed-speech)
 was obtained taking the STFT		TCD-TIMIT corpus (mixed-speech)
 magnitude and perform- ing power-law		TCD-TIMIT corpus (mixed-speech)
 compression |x|p with p		TCD-TIMIT corpus (mixed-speech)
 = 0.3. Finally we applied		TCD-TIMIT corpus (mixed-speech)
 per-speaker 0-mean 1-std normalization		TCD-TIMIT corpus (mixed-speech)
.  In the post-processing stage, the		TCD-TIMIT corpus (mixed-speech)
 enhanced waveform gen- erated by		TCD-TIMIT corpus (mixed-speech)
 the speech enhancement models was		TCD-TIMIT corpus (mixed-speech)
 reconstructed		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
SDR PESQ ViSQOL  Noisy −1.06 1.81 2.11 VL2M		TCD-TIMIT corpus (mixed-speech)
 3.17 1.51 1.16 VL2M ref		TCD-TIMIT corpus (mixed-speech)
 6.50 2.58 2.99 AV concat		TCD-TIMIT corpus (mixed-speech)
 6.31 2.49 2.83 AV c-ref		TCD-TIMIT corpus (mixed-speech)
 6.17 2.58 2.96		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
Table 1. GRID results - 		TCD-TIMIT corpus (mixed-speech)
speaker-dependent. The “Noisy” row refers 		TCD-TIMIT corpus (mixed-speech)
to the metric values of 		TCD-TIMIT corpus (mixed-speech)
the input mixed-speech signal.  2 Speakers 3 Speakers SDR		TCD-TIMIT corpus (mixed-speech)
 PESQ ViSQOL SDR PESQ ViSQOL		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
Noisy 0.21 1.94 2.58 −5.34 1		TCD-TIMIT corpus (mixed-speech)
.43 1.62 VL2M 3.02 1.81 1		TCD-TIMIT corpus (mixed-speech)
.70 −2.03 1.43 1.25 VL2M 		TCD-TIMIT corpus (mixed-speech)
ref 6.52 2.53 3.02 2.83 2		TCD-TIMIT corpus (mixed-speech)
.19 2.53 AV concat 7.37 2		TCD-TIMIT corpus (mixed-speech)
.65 3.03 3.02 2.24 2.49 		TCD-TIMIT corpus (mixed-speech)
AV c-ref 8.05 2.70 3.07 4		TCD-TIMIT corpus (mixed-speech)
.02 2.33 2.64  Table 2. GRID results		TCD-TIMIT corpus (mixed-speech)
 - speaker-independent		TCD-TIMIT corpus (mixed-speech)
.  by applying the inverse STFT		TCD-TIMIT corpus (mixed-speech)
 to the estimated clean spectro		TCD-TIMIT corpus (mixed-speech)
- gram and using the 		TCD-TIMIT corpus (mixed-speech)
phase of the noisy input 		TCD-TIMIT corpus (mixed-speech)
signal.  3.4. Video pre-processing		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
Face landmarks were extracted from 		TCD-TIMIT corpus (mixed-speech)
video using the Dlib [7] 		TCD-TIMIT corpus (mixed-speech)
implementation of the face landmark 		TCD-TIMIT corpus (mixed-speech)
estimator described in [6]. It 		TCD-TIMIT corpus (mixed-speech)
returns 68 x-y points, for 		TCD-TIMIT corpus (mixed-speech)
an overall 136 values. We 		TCD-TIMIT corpus (mixed-speech)
upsampled from 25/29.97 fps (GRID/TCD-TIMIT) 		TCD-TIMIT corpus (mixed-speech)
to 100 fps to match 		TCD-TIMIT corpus (mixed-speech)
the frame rate of the 		TCD-TIMIT corpus (mixed-speech)
audio spectrogram. Upsampling was carried 		TCD-TIMIT corpus (mixed-speech)
out through linear interpolation over 		TCD-TIMIT corpus (mixed-speech)
time.  The final video feature vector		TCD-TIMIT corpus (mixed-speech)
 v was obtained by com		TCD-TIMIT corpus (mixed-speech)
- puting the per-speaker normalized 		TCD-TIMIT corpus (mixed-speech)
motion vector of the face 		TCD-TIMIT corpus (mixed-speech)
landmarks by simply subtracting every 		TCD-TIMIT corpus (mixed-speech)
frame with the previ- ous 		TCD-TIMIT corpus (mixed-speech)
one. The motion vector of 		TCD-TIMIT corpus (mixed-speech)
the first frame was set 		TCD-TIMIT corpus (mixed-speech)
to zero.  4. RESULTS		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
In order to compare our 		TCD-TIMIT corpus (mixed-speech)
models to previous works in 		TCD-TIMIT corpus (mixed-speech)
both speech enhancement and separation, 		TCD-TIMIT corpus (mixed-speech)
we evaluated the perfor- mance 		TCD-TIMIT corpus (mixed-speech)
of the proposed models using 		TCD-TIMIT corpus (mixed-speech)
both speech separation  2 Speakers 3 Speakers SDR		TCD-TIMIT corpus (mixed-speech)
 PESQ ViSQOL SDR PESQ ViSQOL		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
Noisy 0.21 2.22 2.74 −3.42 1		TCD-TIMIT corpus (mixed-speech)
.92 2.04 VL2M 2.88 2.25 2		TCD-TIMIT corpus (mixed-speech)
.62 −0.51 1.99 1.98 VL2M 		TCD-TIMIT corpus (mixed-speech)
ref 9.24 2.81 3.09 5.27 2		TCD-TIMIT corpus (mixed-speech)
.44 2.54 AV concat 9.56 2		TCD-TIMIT corpus (mixed-speech)
.80 3.09 5.15 2.41 2.52 		TCD-TIMIT corpus (mixed-speech)
AV c-ref 10.55 3.03 3.21 5		TCD-TIMIT corpus (mixed-speech)
.37 2.45 2.58  Table 3. TCD-TIMIT results		TCD-TIMIT corpus (mixed-speech)
 - speaker-independent		TCD-TIMIT corpus (mixed-speech)
.  and enhancement metrics. Specifically, we		TCD-TIMIT corpus (mixed-speech)
 measured the ca- pability of		TCD-TIMIT corpus (mixed-speech)
 separating the target utterance from		TCD-TIMIT corpus (mixed-speech)
 the concurrent utterance with the		TCD-TIMIT corpus (mixed-speech)
 source-to-distortion ratio (SDR) [27, 28		TCD-TIMIT corpus (mixed-speech)
]. While the quality of 		TCD-TIMIT corpus (mixed-speech)
estimated target speech was measured 		TCD-TIMIT corpus (mixed-speech)
with the perceptual PESQ [29] 		TCD-TIMIT corpus (mixed-speech)
and ViSQOL [30] metrics. For 		TCD-TIMIT corpus (mixed-speech)
PESQ we used the narrow 		TCD-TIMIT corpus (mixed-speech)
band mode while for ViSQOL 		TCD-TIMIT corpus (mixed-speech)
we used the wide band 		TCD-TIMIT corpus (mixed-speech)
mode.  As a very first experiment		TCD-TIMIT corpus (mixed-speech)
 we compared landmark posi- tion		TCD-TIMIT corpus (mixed-speech)
 vs. landmark motion vectors. It		TCD-TIMIT corpus (mixed-speech)
 turned out that landmark positions		TCD-TIMIT corpus (mixed-speech)
 performed poorly, thus all results		TCD-TIMIT corpus (mixed-speech)
 reported here refer to landmark		TCD-TIMIT corpus (mixed-speech)
 motion vectors only		TCD-TIMIT corpus (mixed-speech)
.  We then carried out some		TCD-TIMIT corpus (mixed-speech)
 speaker-dependent experiments to compare our		TCD-TIMIT corpus (mixed-speech)
 models with previous studies as		TCD-TIMIT corpus (mixed-speech)
, to the best of 		TCD-TIMIT corpus (mixed-speech)
our knowledge, there are no 		TCD-TIMIT corpus (mixed-speech)
reported results of speaker- independent 		TCD-TIMIT corpus (mixed-speech)
systems trained and tested on 		TCD-TIMIT corpus (mixed-speech)
GRID and TCD- TIMIT to 		TCD-TIMIT corpus (mixed-speech)
compare with. Table 1 reports 		TCD-TIMIT corpus (mixed-speech)
the test-set evalua- tion of 		TCD-TIMIT corpus (mixed-speech)
speaker-dependent models on the GRID 		TCD-TIMIT corpus (mixed-speech)
corpus with landmark motion vectors. 		TCD-TIMIT corpus (mixed-speech)
Results are comparable with previ- 		TCD-TIMIT corpus (mixed-speech)
ous state-of-the-art studies in an 		TCD-TIMIT corpus (mixed-speech)
almost identical setting [15, 17].  Table 2 and 3 show		TCD-TIMIT corpus (mixed-speech)
 speaker-independent test-set results on the		TCD-TIMIT corpus (mixed-speech)
 GRID and TCD-TIMIT datasets respectively		TCD-TIMIT corpus (mixed-speech)
. V2ML performs significantly worse 		TCD-TIMIT corpus (mixed-speech)
than the other three models 		TCD-TIMIT corpus (mixed-speech)
in- dicating that a successful 		TCD-TIMIT corpus (mixed-speech)
mask generation has to depend 		TCD-TIMIT corpus (mixed-speech)
on the acoustic context. The 		TCD-TIMIT corpus (mixed-speech)
performance of the three models 		TCD-TIMIT corpus (mixed-speech)
in the speaker-independent setting is 		TCD-TIMIT corpus (mixed-speech)
comparable to that in the 		TCD-TIMIT corpus (mixed-speech)
speaker-dependent setting.  AV concat-ref outperforms V2ML ref		TCD-TIMIT corpus (mixed-speech)
 and AV concat for both		TCD-TIMIT corpus (mixed-speech)
 datasets. This supports the utility		TCD-TIMIT corpus (mixed-speech)
 of a refinement strat- egy		TCD-TIMIT corpus (mixed-speech)
 and suggests that the refinement		TCD-TIMIT corpus (mixed-speech)
 is more effective when it		TCD-TIMIT corpus (mixed-speech)
 directly refines the estimated clean		TCD-TIMIT corpus (mixed-speech)
 spectrogram, rather than refining the		TCD-TIMIT corpus (mixed-speech)
 estimated mask		TCD-TIMIT corpus (mixed-speech)
.  Finally, we evaluated the systems		TCD-TIMIT corpus (mixed-speech)
 in a more challenging testing		TCD-TIMIT corpus (mixed-speech)
 condition where the target utterance		TCD-TIMIT corpus (mixed-speech)
 was mixed with 2 utterances		TCD-TIMIT corpus (mixed-speech)
 from 2 competing speakers. Despite		TCD-TIMIT corpus (mixed-speech)
 the model was trained with		TCD-TIMIT corpus (mixed-speech)
 mixtures of two speakers, the		TCD-TIMIT corpus (mixed-speech)
 decrease of performance was not		TCD-TIMIT corpus (mixed-speech)
 dramatic		TCD-TIMIT corpus (mixed-speech)
.  Code and some testing examples		TCD-TIMIT corpus (mixed-speech)
 of our models are avail		TCD-TIMIT corpus (mixed-speech)
- able at https://goo.gl/3h1NgE.  5. CONCLUSION		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
This paper proposes the use 		TCD-TIMIT corpus (mixed-speech)
of face landmark motion vec- 		TCD-TIMIT corpus (mixed-speech)
tors for audio-visual speech enhancement 		TCD-TIMIT corpus (mixed-speech)
in a single-channel multi-talker scenario. 		TCD-TIMIT corpus (mixed-speech)
Different models are tested where 		TCD-TIMIT corpus (mixed-speech)
land- mark motion vectors are 		TCD-TIMIT corpus (mixed-speech)
used to generate time-frequency (T- 		TCD-TIMIT corpus (mixed-speech)
F) masks that extract the 		TCD-TIMIT corpus (mixed-speech)
target speaker’s spectrogram from the 		TCD-TIMIT corpus (mixed-speech)
acoustic mixed-speech spectrogram.  To the best of our		TCD-TIMIT corpus (mixed-speech)
 knowledge, some of the proposed		TCD-TIMIT corpus (mixed-speech)
 mod- els are the first		TCD-TIMIT corpus (mixed-speech)
 models trained and evaluated on		TCD-TIMIT corpus (mixed-speech)
 the limited size GRID and		TCD-TIMIT corpus (mixed-speech)
 TCD-TIMIT datasets that accomplish speaker		TCD-TIMIT corpus (mixed-speech)
- independent speech enhancement in 		TCD-TIMIT corpus (mixed-speech)
the multi-talker setting, with a 		TCD-TIMIT corpus (mixed-speech)
quality of enhancement comparable to 		TCD-TIMIT corpus (mixed-speech)
that achieved in a speaker-dependent 		TCD-TIMIT corpus (mixed-speech)
setting.  https://goo.gl/3h1NgE		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
6. REFERENCES  [1] E. Colin Cherry, “Some		TCD-TIMIT corpus (mixed-speech)
 experiments on the recognition of		TCD-TIMIT corpus (mixed-speech)
 speech, with one and with		TCD-TIMIT corpus (mixed-speech)
 two ears,” The Journal of		TCD-TIMIT corpus (mixed-speech)
 the Acoustical Society of America		TCD-TIMIT corpus (mixed-speech)
, vol. 25, no. 5, 		TCD-TIMIT corpus (mixed-speech)
pp. 975–979, 1953.  [2] Josh H McDermott, “The		TCD-TIMIT corpus (mixed-speech)
 cocktail party problem,” Current Biology		TCD-TIMIT corpus (mixed-speech)
, vol. 19, no. 22, 		TCD-TIMIT corpus (mixed-speech)
pp. R1024–R1027, 2009.  [3] Elana Zion Golumbic, Gregory		TCD-TIMIT corpus (mixed-speech)
 B. Cogan, Charles E. Schroeder		TCD-TIMIT corpus (mixed-speech)
, and David Poeppel, “Visual 		TCD-TIMIT corpus (mixed-speech)
input enhances selective speech envelope 		TCD-TIMIT corpus (mixed-speech)
tracking in auditory cortex at 		TCD-TIMIT corpus (mixed-speech)
a “cocktail party”,” Journal of 		TCD-TIMIT corpus (mixed-speech)
Neu- roscience, vol. 33, no. 4, pp. 1417–1426, 2013		TCD-TIMIT corpus (mixed-speech)
.  [4] Wei Ji Ma, Xiang		TCD-TIMIT corpus (mixed-speech)
 Zhou, Lars A. Ross, John		TCD-TIMIT corpus (mixed-speech)
 J. Foxe, and Lucas C		TCD-TIMIT corpus (mixed-speech)
. Parra, “Lip-reading aids word 		TCD-TIMIT corpus (mixed-speech)
recognition most in moderate noise: 		TCD-TIMIT corpus (mixed-speech)
A bayesian explanation using high-dimensional 		TCD-TIMIT corpus (mixed-speech)
feature space,” PLOS ONE, vol. 4, no. 3, pp. 1–14, 03		TCD-TIMIT corpus (mixed-speech)
 2009		TCD-TIMIT corpus (mixed-speech)
.  [5] Albert S Bregman, Auditory		TCD-TIMIT corpus (mixed-speech)
 scene analysis: The perceptual organi		TCD-TIMIT corpus (mixed-speech)
- zation of sound, MIT 		TCD-TIMIT corpus (mixed-speech)
press, 1994.  [6] Vahid Kazemi and Josephine		TCD-TIMIT corpus (mixed-speech)
 Sullivan, “One millisecond face align		TCD-TIMIT corpus (mixed-speech)
- ment with an ensemble 		TCD-TIMIT corpus (mixed-speech)
of regression trees,” in The 		TCD-TIMIT corpus (mixed-speech)
IEEE Conference on Computer Vision 		TCD-TIMIT corpus (mixed-speech)
and Pattern Recognition (CVPR), June 2014		TCD-TIMIT corpus (mixed-speech)
.  [7] Davis E. King, “Dlib-ml		TCD-TIMIT corpus (mixed-speech)
: A machine learning toolkit,” 		TCD-TIMIT corpus (mixed-speech)
Journal of Machine Learning Research, 		TCD-TIMIT corpus (mixed-speech)
vol. 10, pp. 1755–1758, 2009.  [8] Yuxuan Wang, Arun Narayanan		TCD-TIMIT corpus (mixed-speech)
, and DeLiang Wang, “On 		TCD-TIMIT corpus (mixed-speech)
Training Targets for Supervised Speech 		TCD-TIMIT corpus (mixed-speech)
Separation,” IEEE/ACM Transactions on Audio, 		TCD-TIMIT corpus (mixed-speech)
Speech, and Language Processing, vol. 22, no. 12, pp. 1849–1858, Dec		TCD-TIMIT corpus (mixed-speech)
. 2014.  [9] Martin Cooke, Jon Barker		TCD-TIMIT corpus (mixed-speech)
, Stuart Cunningham, and Xu 		TCD-TIMIT corpus (mixed-speech)
Shao, “An audio-visual corpus for 		TCD-TIMIT corpus (mixed-speech)
speech perception and automatic speech 		TCD-TIMIT corpus (mixed-speech)
recognition,” The Journal of the 		TCD-TIMIT corpus (mixed-speech)
Acoustical Society of America, vol. 120, no. 5, pp. 2421–2424, Nov		TCD-TIMIT corpus (mixed-speech)
. 2006.  [10] Naomi Harte and Eoin		TCD-TIMIT corpus (mixed-speech)
 Gillen, “TCD-TIMIT: An Audio-Visual Cor		TCD-TIMIT corpus (mixed-speech)
- pus of Continuous Speech,” 		TCD-TIMIT corpus (mixed-speech)
IEEE Transactions on Multimedia, vol. 17, no. 5, pp. 603–615, May		TCD-TIMIT corpus (mixed-speech)
 2015		TCD-TIMIT corpus (mixed-speech)
.  [11] Z. Chen, Y. Luo		TCD-TIMIT corpus (mixed-speech)
, and N. Mesgarani, “Deep 		TCD-TIMIT corpus (mixed-speech)
attractor network for single-microphone speaker 		TCD-TIMIT corpus (mixed-speech)
separation,” in 2017 IEEE International 		TCD-TIMIT corpus (mixed-speech)
Conference on Acoustics, Speech and 		TCD-TIMIT corpus (mixed-speech)
Signal Processing (ICASSP), March 2017, 		TCD-TIMIT corpus (mixed-speech)
pp. 246–250.  [12] Yusuf Isik, Jonathan Le		TCD-TIMIT corpus (mixed-speech)
 Roux, Zhuo Chen, Shinji Watanabe		TCD-TIMIT corpus (mixed-speech)
, and John R. 		TCD-TIMIT corpus (mixed-speech)
Hershey, “Single-channel multi-speaker separation using 		TCD-TIMIT corpus (mixed-speech)
deep clustering,” in Interspeech, 2016.  [13] Morten Kolbaek, Dong Yu		TCD-TIMIT corpus (mixed-speech)
, Zheng-Hua Tan, Jesper Jensen, 		TCD-TIMIT corpus (mixed-speech)
Morten Kolbaek, Dong Yu, Zheng-Hua 		TCD-TIMIT corpus (mixed-speech)
Tan, and Jesper Jensen, “Multitalker 		TCD-TIMIT corpus (mixed-speech)
speech separation with utterance-level permutation 		TCD-TIMIT corpus (mixed-speech)
invariant training of deep recurrent 		TCD-TIMIT corpus (mixed-speech)
neural networks,” IEEE/ACM Trans. Audio, 		TCD-TIMIT corpus (mixed-speech)
Speech and Lang. Proc., vol. 25, no. 10, pp. 1901–1913, Oct		TCD-TIMIT corpus (mixed-speech)
. 2017.  [14] Bertrand Rivet, Wenwu Wang		TCD-TIMIT corpus (mixed-speech)
, Syed Mohsen Naqvi, and 		TCD-TIMIT corpus (mixed-speech)
Jonathon Chambers, “Audiovisual Speech Source 		TCD-TIMIT corpus (mixed-speech)
Separation: An overview of key 		TCD-TIMIT corpus (mixed-speech)
methodologies,” IEEE Signal Processing Magazine, 		TCD-TIMIT corpus (mixed-speech)
vol. 31, no. 3, pp. 125		TCD-TIMIT corpus (mixed-speech)
–134, May 2014.  [15] Aviv Gabbay, Ariel Ephrat		TCD-TIMIT corpus (mixed-speech)
, Tavi Halperin, and Shmuel 		TCD-TIMIT corpus (mixed-speech)
Peleg, “Seeing through noise: Visually 		TCD-TIMIT corpus (mixed-speech)
driven speaker separation and enhancement,” 		TCD-TIMIT corpus (mixed-speech)
in ICASSP. 2018, pp. 3051–3055, 		TCD-TIMIT corpus (mixed-speech)
IEEE.  [16] Ariel Ephrat, Tavi Halperin		TCD-TIMIT corpus (mixed-speech)
, and Shmuel Peleg, “Improved 		TCD-TIMIT corpus (mixed-speech)
speech reconstruction from silent video,” 		TCD-TIMIT corpus (mixed-speech)
ICCV 2017 Workshop on Computer 		TCD-TIMIT corpus (mixed-speech)
Vision for Audio-Visual Media, 2017.  [17] Aviv Gabbay, Asaph Shamir		TCD-TIMIT corpus (mixed-speech)
, and Shmuel Peleg, “Visual 		TCD-TIMIT corpus (mixed-speech)
speech en- hancement,” in Interspeech. 2018, pp. 1170–1174, ISCA		TCD-TIMIT corpus (mixed-speech)
.  [18] Jen-Cheng Hou, Syu-Siang Wang		TCD-TIMIT corpus (mixed-speech)
, Ying-Hui Lai, Yu Tsao, 		TCD-TIMIT corpus (mixed-speech)
Hsiu-Wen Chang, and Hsin-Min 		TCD-TIMIT corpus (mixed-speech)
Wang, “Audio-Visual Speech Enhancement Us- 		TCD-TIMIT corpus (mixed-speech)
ing Multimodal Deep Convolutional Neural 		TCD-TIMIT corpus (mixed-speech)
Networks,” IEEE Trans- actions on 		TCD-TIMIT corpus (mixed-speech)
Emerging Topics in Computational Intelligence, 		TCD-TIMIT corpus (mixed-speech)
vol. 2, no. 2, pp. 117		TCD-TIMIT corpus (mixed-speech)
–128, Apr. 2018.  [19] Jen-Cheng Hou, Syu-Siang Wang		TCD-TIMIT corpus (mixed-speech)
, Ying-Hui Lai, Jen-Chun Lin, 		TCD-TIMIT corpus (mixed-speech)
Yu Tsao, Hsiu-Wen Chang, and 		TCD-TIMIT corpus (mixed-speech)
Hsin-Min Wang, “Audio-visual speech enhancement 		TCD-TIMIT corpus (mixed-speech)
using deep neural networks,” in 2016 Asia- Pacific Signal and Information		TCD-TIMIT corpus (mixed-speech)
 Processing Association Annual Sum- mit		TCD-TIMIT corpus (mixed-speech)
 and Conference (APSIPA), Jeju, South		TCD-TIMIT corpus (mixed-speech)
 Korea, Dec. 2016, pp. 1–6		TCD-TIMIT corpus (mixed-speech)
, IEEE.  [20] Ariel Ephrat, Inbar Mosseri		TCD-TIMIT corpus (mixed-speech)
, Oran Lang, Tali Dekel, 		TCD-TIMIT corpus (mixed-speech)
Kevin Wilson, Avinatan Hassidim, William 		TCD-TIMIT corpus (mixed-speech)
T. Freeman, and Michael 		TCD-TIMIT corpus (mixed-speech)
Rubinstein, “Looking to Listen at 		TCD-TIMIT corpus (mixed-speech)
the Cocktail Party: A Speaker-Independent 		TCD-TIMIT corpus (mixed-speech)
Audio-Visual Model for Speech Separation,” 		TCD-TIMIT corpus (mixed-speech)
ACM Transactions on Graphics, vol. 37, no. 4, pp. 1–11, July		TCD-TIMIT corpus (mixed-speech)
 2018, arXiv: 1804.03619		TCD-TIMIT corpus (mixed-speech)
.  [21] T. Afouras, J. S		TCD-TIMIT corpus (mixed-speech)
. Chung, and A. 		TCD-TIMIT corpus (mixed-speech)
Zisserman, “The conversation: Deep audio-visual 		TCD-TIMIT corpus (mixed-speech)
speech enhancement,” in Interspeech, 2018.  [22] Andrew Owens and Alexei		TCD-TIMIT corpus (mixed-speech)
 A Efros, “Audio-visual scene analysis		TCD-TIMIT corpus (mixed-speech)
 with self-supervised multisensory features,” European		TCD-TIMIT corpus (mixed-speech)
 Conference on Computer Vision (ECCV		TCD-TIMIT corpus (mixed-speech)
), 2018.  [23] Michael C. Anzalone, Lauren		TCD-TIMIT corpus (mixed-speech)
 Calandruccio, Karen A. Doherty, and		TCD-TIMIT corpus (mixed-speech)
 Laurel H. Carney, “Determination of		TCD-TIMIT corpus (mixed-speech)
 the potential benefit of time		TCD-TIMIT corpus (mixed-speech)
- frequency gain manipulation,” Ear 		TCD-TIMIT corpus (mixed-speech)
Hear, vol. 27, no. 5, 		TCD-TIMIT corpus (mixed-speech)
pp. 480–492, Oct 2006, 16957499[pmid].  [24] Ulrik Kjems, Jesper B		TCD-TIMIT corpus (mixed-speech)
. Boldt, Michael S. Pedersen, 		TCD-TIMIT corpus (mixed-speech)
Thomas Lunner, and DeLiang 		TCD-TIMIT corpus (mixed-speech)
Wang, “Role of mask pattern 		TCD-TIMIT corpus (mixed-speech)
in intelligibility of ideal binary-masked 		TCD-TIMIT corpus (mixed-speech)
noisy speech,” The Journal of 		TCD-TIMIT corpus (mixed-speech)
the Acoustical Society of America, 		TCD-TIMIT corpus (mixed-speech)
vol. 126, no. 3, pp. 1415		TCD-TIMIT corpus (mixed-speech)
–1426, 2009.  [25] A. Graves, A. Mohamed		TCD-TIMIT corpus (mixed-speech)
, and G. Hinton, “Speech 		TCD-TIMIT corpus (mixed-speech)
recognition with deep recurrent neural 		TCD-TIMIT corpus (mixed-speech)
networks,” in 2013 IEEE International 		TCD-TIMIT corpus (mixed-speech)
Con- ference on Acoustics, Speech 		TCD-TIMIT corpus (mixed-speech)
and Signal Processing, May 2013, 		TCD-TIMIT corpus (mixed-speech)
pp. 6645–6649.  [26] Diederik P Kingma and		TCD-TIMIT corpus (mixed-speech)
 Jimmy Ba, “Adam: A method		TCD-TIMIT corpus (mixed-speech)
 for stochastic optimization,” arXiv preprint		TCD-TIMIT corpus (mixed-speech)
 arXiv:1412.6980, 2014		TCD-TIMIT corpus (mixed-speech)
.  [27] E. Vincent, R. Gribonval		TCD-TIMIT corpus (mixed-speech)
, and C. Fevotte, “Performance 		TCD-TIMIT corpus (mixed-speech)
measure- ment in blind audio 		TCD-TIMIT corpus (mixed-speech)
source separation,” IEEE Transactions on 		TCD-TIMIT corpus (mixed-speech)
Audio, Speech and Language Processing, 		TCD-TIMIT corpus (mixed-speech)
vol. 14, no. 4, pp. 1462		TCD-TIMIT corpus (mixed-speech)
–1469, July 2006.  [28] Colin Raffel, Brian McFee		TCD-TIMIT corpus (mixed-speech)
, Eric J Humphrey, Justin 		TCD-TIMIT corpus (mixed-speech)
Salamon, Oriol Nieto, Dawen Liang, 		TCD-TIMIT corpus (mixed-speech)
Daniel PW Ellis, and C 		TCD-TIMIT corpus (mixed-speech)
Colin Raffel, “mir eval: A 		TCD-TIMIT corpus (mixed-speech)
transparent implementation of common mir 		TCD-TIMIT corpus (mixed-speech)
metrics,” in In Proceed- ings 		TCD-TIMIT corpus (mixed-speech)
of the 15th International Society 		TCD-TIMIT corpus (mixed-speech)
for Music Information Retrieval Conference, 		TCD-TIMIT corpus (mixed-speech)
ISMIR. Citeseer, 2014.  [29] A.W. Rix, J.G. Beerends		TCD-TIMIT corpus (mixed-speech)
, M.P. Hollier, and A.P. 		TCD-TIMIT corpus (mixed-speech)
Hekstra, “Perceptual evaluation of speech 		TCD-TIMIT corpus (mixed-speech)
quality (PESQ)-a new method for 		TCD-TIMIT corpus (mixed-speech)
speech qual- ity assessment of 		TCD-TIMIT corpus (mixed-speech)
telephone networks and codecs,” in 2001 IEEE In- ternational Conference on		TCD-TIMIT corpus (mixed-speech)
 Acoustics, Speech, and Signal Processing		TCD-TIMIT corpus (mixed-speech)
. Proceedings (Cat. No.01CH37221), Salt 		TCD-TIMIT corpus (mixed-speech)
Lake City, UT, USA, 2001, 		TCD-TIMIT corpus (mixed-speech)
vol. 2, pp. 749–752, IEEE.  [30] A. Hines, J. Skoglund		TCD-TIMIT corpus (mixed-speech)
, A. Kokaram, and N. 		TCD-TIMIT corpus (mixed-speech)
Harte, “ViSQOL: The Virtual Speech 		TCD-TIMIT corpus (mixed-speech)
Quality Objective Listener,” in IWAENC 2012		TCD-TIMIT corpus (mixed-speech)
; Inter- national Workshop on 		TCD-TIMIT corpus (mixed-speech)
Acoustic Signal Enhancement, Sept. 2012, 		TCD-TIMIT corpus (mixed-speech)
pp. 1		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
4		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
1  Introduction		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
1.1  Related work		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
2  MODEL ARCHITECTURES		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
2.1  VL2M model		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
2.2  VL2M_ref model		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
2.3  Audio-Visual concat model		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
2.4  Audio-Visual concat-ref model		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
3  Experimental setup		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
3.1  Dataset		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
3.2  LSTM training		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
3.3  Audio pre- and post-processing		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
3.4  Video pre-processing		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
4  Results		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
5  Conclusion		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
6  References		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
the limited size GRID and TCD	TCD	TCD-TIMIT corpus (mixed-speech)
on the GRID [9] and TCD	TCD	TCD-TIMIT corpus (mixed-speech)
using the GRID [9] and TCD	TCD	TCD-TIMIT corpus (mixed-speech)
The TCD	TCD	TCD-TIMIT corpus (mixed-speech)
difference. Con- trary to GRID, TCD	TCD	TCD-TIMIT corpus (mixed-speech)
TCD	TCD	TCD-TIMIT corpus (mixed-speech)
Table 3. TCD	TCD	TCD-TIMIT corpus (mixed-speech)
and tested on GRID and TCD	TCD	TCD-TIMIT corpus (mixed-speech)
results on the GRID and TCD	TCD	TCD-TIMIT corpus (mixed-speech)
the limited size GRID and TCD	TCD	TCD-TIMIT corpus (mixed-speech)
TCD	TCD	TCD-TIMIT corpus (mixed-speech)
the limited size GRID and TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
on the GRID [9] and TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
using the GRID [9] and TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
The TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
difference. Con- trary to GRID, TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
Table 3. TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
and tested on GRID and TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
results on the GRID and TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
the limited size GRID and TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
The TCD-TIMIT corpus consists of 59 speakers (we	TCD-TIMIT corpus	TCD-TIMIT corpus (mixed-speech)
are applied to the acoustic mixed-speech spectrogram. Results show that: (i	mixed-speech	TCD-TIMIT corpus (mixed-speech)
landmark features and the input mixed-speech spectrogram	mixed-speech	TCD-TIMIT corpus (mixed-speech)
applied to clean the acoustic mixed-speech spectrogram	mixed-speech	TCD-TIMIT corpus (mixed-speech)
compressed spectrogram of the single-channel mixed-speech signal. All of them perform	mixed-speech	TCD-TIMIT corpus (mixed-speech)
although IAM generation requires the mixed-speech spectrogram, separate spectrograms for each	mixed-speech	TCD-TIMIT corpus (mixed-speech)
of them, we created a mixed-speech version	mixed-speech	TCD-TIMIT corpus (mixed-speech)
98 utterances per speaker. The mixed-speech version was created following the	mixed-speech	TCD-TIMIT corpus (mixed-speech)
metric values of the input mixed-speech signal	mixed-speech	TCD-TIMIT corpus (mixed-speech)
speaker’s spectrogram from the acoustic mixed-speech spectrogram	mixed-speech	TCD-TIMIT corpus (mixed-speech)
are applied to the acoustic mixed-speech spectrogram. Results show that: (i	(mixed-speech)	TCD-TIMIT corpus (mixed-speech)
landmark features and the input mixed-speech spectrogram	(mixed-speech)	TCD-TIMIT corpus (mixed-speech)
applied to clean the acoustic mixed-speech spectrogram	(mixed-speech)	TCD-TIMIT corpus (mixed-speech)
compressed spectrogram of the single-channel mixed-speech signal. All of them perform	(mixed-speech)	TCD-TIMIT corpus (mixed-speech)
although IAM generation requires the mixed-speech spectrogram, separate spectrograms for each	(mixed-speech)	TCD-TIMIT corpus (mixed-speech)
of them, we created a mixed-speech version	(mixed-speech)	TCD-TIMIT corpus (mixed-speech)
98 utterances per speaker. The mixed-speech version was created following the	(mixed-speech)	TCD-TIMIT corpus (mixed-speech)
metric values of the input mixed-speech signal	(mixed-speech)	TCD-TIMIT corpus (mixed-speech)
speaker’s spectrogram from the acoustic mixed-speech spectrogram	(mixed-speech)	TCD-TIMIT corpus (mixed-speech)
Regarding the GRID corpus, for each of the 33	corpus	TCD-TIMIT corpus (mixed-speech)
The TCD-TIMIT corpus consists of 59 speakers (we	corpus	TCD-TIMIT corpus (mixed-speech)
speaker-dependent models on the GRID corpus with landmark motion vectors. Results	corpus	TCD-TIMIT corpus (mixed-speech)
and Xu Shao, “An audio-visual corpus for speech perception and automatic	corpus	TCD-TIMIT corpus (mixed-speech)
the limited size GRID and TCD-TIMIT datasets, that achieve speaker-independent speech	TCD-TIMIT	TCD-TIMIT corpus (mixed-speech)
on the GRID [9] and TCD-TIMIT [10] datasets in a speaker-independent	TCD-TIMIT	TCD-TIMIT corpus (mixed-speech)
using the GRID [9] and TCD-TIMIT [10] audio-visual datasets. For each	TCD-TIMIT	TCD-TIMIT corpus (mixed-speech)
The TCD-TIMIT corpus consists of 59 speakers	TCD-TIMIT	TCD-TIMIT corpus (mixed-speech)
difference. Con- trary to GRID, TCD-TIMIT utterances have different dura- tion	TCD-TIMIT	TCD-TIMIT corpus (mixed-speech)
TCD-TIMIT) to 100 fps to match	TCD-TIMIT	TCD-TIMIT corpus (mixed-speech)
Table 3. TCD-TIMIT results - speaker-independent	TCD-TIMIT	TCD-TIMIT corpus (mixed-speech)
results on the GRID and TCD-TIMIT datasets respectively. V2ML performs significantly	TCD-TIMIT	TCD-TIMIT corpus (mixed-speech)
the limited size GRID and TCD-TIMIT datasets that accomplish speaker- independent	TCD-TIMIT	TCD-TIMIT corpus (mixed-speech)
TCD-TIMIT	TCD-TIMIT	TCD-TIMIT corpus (mixed-speech)
FACE LANDMARK-BASED SPEAKER-INDEPENDENT AUDIO-VISUAL SPEECH 		TCD-TIMIT corpus (mixed-speech)
ENHANCEMENT IN MULTI-TALKER ENVIRONMENTS  Giovanni Morrone? Luca Pasa† Vadim		TCD-TIMIT corpus (mixed-speech)
 Tikhanoff		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
Sonia Bergamaschi? Luciano Fadiga† Leonardo 		TCD-TIMIT corpus (mixed-speech)
Badino†  ?Department of Engineering ”Enzo Ferrari		TCD-TIMIT corpus (mixed-speech)
”, University of Modena and 		TCD-TIMIT corpus (mixed-speech)
Reggio Emilia, Modena, Italy †Istituto 		TCD-TIMIT corpus (mixed-speech)
Italiano di Tecnologia, Ferrara, Italy  ABSTRACT		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
In this paper, we address 		TCD-TIMIT corpus (mixed-speech)
the problem of enhancing the 		TCD-TIMIT corpus (mixed-speech)
speech of a speaker of 		TCD-TIMIT corpus (mixed-speech)
interest in a cocktail party 		TCD-TIMIT corpus (mixed-speech)
scenario when vi- sual information 		TCD-TIMIT corpus (mixed-speech)
of the speaker of interest 		TCD-TIMIT corpus (mixed-speech)
is available.  Contrary to most previous studies		TCD-TIMIT corpus (mixed-speech)
, we do not learn 		TCD-TIMIT corpus (mixed-speech)
visual features on the typically 		TCD-TIMIT corpus (mixed-speech)
small audio-visual datasets, but use 		TCD-TIMIT corpus (mixed-speech)
an already available face landmark 		TCD-TIMIT corpus (mixed-speech)
detector (trained on a sep- 		TCD-TIMIT corpus (mixed-speech)
arate image dataset).  The landmarks are used by		TCD-TIMIT corpus (mixed-speech)
 LSTM-based models to gen- erate		TCD-TIMIT corpus (mixed-speech)
 time-frequency masks which are applied		TCD-TIMIT corpus (mixed-speech)
 to the acoustic mixed-speech spectrogram		TCD-TIMIT corpus (mixed-speech)
. Results show that: (i) 		TCD-TIMIT corpus (mixed-speech)
land- mark motion features are 		TCD-TIMIT corpus (mixed-speech)
very effective features for this 		TCD-TIMIT corpus (mixed-speech)
task, (ii) similarly to previous 		TCD-TIMIT corpus (mixed-speech)
work, reconstruction of the target 		TCD-TIMIT corpus (mixed-speech)
speaker’s spectrogram mediated by masking 		TCD-TIMIT corpus (mixed-speech)
is significantly more accurate than 		TCD-TIMIT corpus (mixed-speech)
direct spectrogram reconstruction, and (iii) 		TCD-TIMIT corpus (mixed-speech)
the best masks depend on 		TCD-TIMIT corpus (mixed-speech)
both motion landmark features and 		TCD-TIMIT corpus (mixed-speech)
the input mixed-speech spectrogram.  To the best of our		TCD-TIMIT corpus (mixed-speech)
 knowledge, our proposed models are		TCD-TIMIT corpus (mixed-speech)
 the first models trained and		TCD-TIMIT corpus (mixed-speech)
 evaluated on the limited size		TCD-TIMIT corpus (mixed-speech)
 GRID and TCD-TIMIT datasets, that		TCD-TIMIT corpus (mixed-speech)
 achieve speaker-independent speech enhancement in		TCD-TIMIT corpus (mixed-speech)
 a multi-talker setting		TCD-TIMIT corpus (mixed-speech)
.  Index Terms— audio-visual speech enhancement		TCD-TIMIT corpus (mixed-speech)
, cock- tail party problem, 		TCD-TIMIT corpus (mixed-speech)
time-frequency mask, LSTM, face land- 		TCD-TIMIT corpus (mixed-speech)
marks  1. INTRODUCTION		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
In the context of speech 		TCD-TIMIT corpus (mixed-speech)
perception, the cocktail party 		TCD-TIMIT corpus (mixed-speech)
effect [1, 2] is the 		TCD-TIMIT corpus (mixed-speech)
ability of the brain to 		TCD-TIMIT corpus (mixed-speech)
recognize speech in complex and 		TCD-TIMIT corpus (mixed-speech)
adverse listening conditions where the 		TCD-TIMIT corpus (mixed-speech)
attended speech is mixed with 		TCD-TIMIT corpus (mixed-speech)
competing sounds/speech.  Speech perception studies have shown		TCD-TIMIT corpus (mixed-speech)
 that watching speaker’s face movements		TCD-TIMIT corpus (mixed-speech)
 could dramatically improve our ability		TCD-TIMIT corpus (mixed-speech)
 at recognizing the speech of		TCD-TIMIT corpus (mixed-speech)
 a target speaker in a		TCD-TIMIT corpus (mixed-speech)
 multi-talker environment [3, 4		TCD-TIMIT corpus (mixed-speech)
].  This work aims at extracting		TCD-TIMIT corpus (mixed-speech)
 the speech of a target		TCD-TIMIT corpus (mixed-speech)
 speaker from single channel audio		TCD-TIMIT corpus (mixed-speech)
 of several people talking simulta		TCD-TIMIT corpus (mixed-speech)
- neously. This is an 		TCD-TIMIT corpus (mixed-speech)
ill-posed problem in that many 		TCD-TIMIT corpus (mixed-speech)
differ- ent hypotheses about what 		TCD-TIMIT corpus (mixed-speech)
the target speaker says are 		TCD-TIMIT corpus (mixed-speech)
con-  sistent with the mixture signal		TCD-TIMIT corpus (mixed-speech)
. Yet, it can be 		TCD-TIMIT corpus (mixed-speech)
solved by ex- ploiting some 		TCD-TIMIT corpus (mixed-speech)
additional information associated to the 		TCD-TIMIT corpus (mixed-speech)
speaker of interest and/or by 		TCD-TIMIT corpus (mixed-speech)
leveraging some prior knowledge about 		TCD-TIMIT corpus (mixed-speech)
speech signal properties (e.g., [5]). 		TCD-TIMIT corpus (mixed-speech)
In this work we use 		TCD-TIMIT corpus (mixed-speech)
face movements of the target 		TCD-TIMIT corpus (mixed-speech)
speaker as additional information.  This paper (i) proposes the		TCD-TIMIT corpus (mixed-speech)
 use of face landmark’s move		TCD-TIMIT corpus (mixed-speech)
- ments, extracted using 		TCD-TIMIT corpus (mixed-speech)
Dlib [6, 7] and (ii) 		TCD-TIMIT corpus (mixed-speech)
compares differ- ent ways of 		TCD-TIMIT corpus (mixed-speech)
mapping such visual features into 		TCD-TIMIT corpus (mixed-speech)
time-frequency (T-F) masks, then applied 		TCD-TIMIT corpus (mixed-speech)
to clean the acoustic mixed-speech 		TCD-TIMIT corpus (mixed-speech)
spectrogram.  By using Dlib extracted landmarks		TCD-TIMIT corpus (mixed-speech)
 we relieve our mod- els		TCD-TIMIT corpus (mixed-speech)
 from the task of learning		TCD-TIMIT corpus (mixed-speech)
 useful visual features from raw		TCD-TIMIT corpus (mixed-speech)
 pixels. That aspect is particularly		TCD-TIMIT corpus (mixed-speech)
 relevant when the training audio-visual		TCD-TIMIT corpus (mixed-speech)
 datasets are small		TCD-TIMIT corpus (mixed-speech)
.  The analysis of landmark-dependent masking		TCD-TIMIT corpus (mixed-speech)
 strategies is motivated by the		TCD-TIMIT corpus (mixed-speech)
 fact that speech enhancement mediated		TCD-TIMIT corpus (mixed-speech)
 by an explicit masking is		TCD-TIMIT corpus (mixed-speech)
 often more effective than mask-free		TCD-TIMIT corpus (mixed-speech)
 enhancement [8		TCD-TIMIT corpus (mixed-speech)
].  All our models were trained		TCD-TIMIT corpus (mixed-speech)
 and evaluated on the GRID		TCD-TIMIT corpus (mixed-speech)
 [9] and TCD-TIMIT [10] datasets		TCD-TIMIT corpus (mixed-speech)
 in a speaker-independent setting		TCD-TIMIT corpus (mixed-speech)
.  1.1. Related work		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
Speech enhancement aims at extracting 		TCD-TIMIT corpus (mixed-speech)
the voice of a tar- 		TCD-TIMIT corpus (mixed-speech)
get speaker, while speech separation 		TCD-TIMIT corpus (mixed-speech)
refers to the problem of 		TCD-TIMIT corpus (mixed-speech)
separating each sound source in 		TCD-TIMIT corpus (mixed-speech)
a mixture. Recently pro- posed 		TCD-TIMIT corpus (mixed-speech)
audio-only single-channel methods have achieved 		TCD-TIMIT corpus (mixed-speech)
very promising results [11, 12, 13		TCD-TIMIT corpus (mixed-speech)
]. However the task still 		TCD-TIMIT corpus (mixed-speech)
remains challenging. Additionally, audio-only systems 		TCD-TIMIT corpus (mixed-speech)
need separate models in order 		TCD-TIMIT corpus (mixed-speech)
to associate the estimated separated 		TCD-TIMIT corpus (mixed-speech)
audio sources to each speaker, 		TCD-TIMIT corpus (mixed-speech)
while vision easily allow that 		TCD-TIMIT corpus (mixed-speech)
in a unified model.  Regarding audio-visual speech enhancement and		TCD-TIMIT corpus (mixed-speech)
 separa- tion methods an extensive		TCD-TIMIT corpus (mixed-speech)
 review is provided in [14		TCD-TIMIT corpus (mixed-speech)
]. Here we focus on 		TCD-TIMIT corpus (mixed-speech)
the deep-learning methods that are 		TCD-TIMIT corpus (mixed-speech)
most related to the present 		TCD-TIMIT corpus (mixed-speech)
work.  Our first architecture (Section 2.1		TCD-TIMIT corpus (mixed-speech)
) is inspired by [15], 		TCD-TIMIT corpus (mixed-speech)
where a pre-trained convolutional neural 		TCD-TIMIT corpus (mixed-speech)
network (CNN) is used to 		TCD-TIMIT corpus (mixed-speech)
generate a clean spectrogram from 		TCD-TIMIT corpus (mixed-speech)
silent video [16]. Rather than 		TCD-TIMIT corpus (mixed-speech)
directly computing a time-frequency (T-F) 		TCD-TIMIT corpus (mixed-speech)
mask,  ar X		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
iv :1  81 1		TCD-TIMIT corpus (mixed-speech)
.  02 48		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
0v 3		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
cs  .C L		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
2		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
M 		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
2 01  9		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
the mask is computed by 		TCD-TIMIT corpus (mixed-speech)
thresholding the estimated clean spectrogram. 		TCD-TIMIT corpus (mixed-speech)
This approach is not very 		TCD-TIMIT corpus (mixed-speech)
effective since the pre-trained CNN 		TCD-TIMIT corpus (mixed-speech)
is designed for a different 		TCD-TIMIT corpus (mixed-speech)
task (video-to- speech synthesis). 		TCD-TIMIT corpus (mixed-speech)
In [17] a CNN is 		TCD-TIMIT corpus (mixed-speech)
trained to directly esti- mate 		TCD-TIMIT corpus (mixed-speech)
clean speech from noisy audio 		TCD-TIMIT corpus (mixed-speech)
and input video. A sim- 		TCD-TIMIT corpus (mixed-speech)
ilar model is used 		TCD-TIMIT corpus (mixed-speech)
in [18], where the model 		TCD-TIMIT corpus (mixed-speech)
jointly generates clean speech and 		TCD-TIMIT corpus (mixed-speech)
input video in a denoising-autoender 		TCD-TIMIT corpus (mixed-speech)
archi- tecture.  [19] shows that using information		TCD-TIMIT corpus (mixed-speech)
 about lip positions can help		TCD-TIMIT corpus (mixed-speech)
 to improve speech enhancement. The		TCD-TIMIT corpus (mixed-speech)
 video feature vec- tor is		TCD-TIMIT corpus (mixed-speech)
 obtained computing pair-wise distances between		TCD-TIMIT corpus (mixed-speech)
 any mouth landmarks. Similarly to		TCD-TIMIT corpus (mixed-speech)
 our approach their visual fea		TCD-TIMIT corpus (mixed-speech)
- tures are not learned 		TCD-TIMIT corpus (mixed-speech)
on the audio-visual dataset but 		TCD-TIMIT corpus (mixed-speech)
are pro- vided by a 		TCD-TIMIT corpus (mixed-speech)
system trained on different dataset. 		TCD-TIMIT corpus (mixed-speech)
Contrary to our approach, [19] 		TCD-TIMIT corpus (mixed-speech)
uses position-based features while we 		TCD-TIMIT corpus (mixed-speech)
use motion features (of the 		TCD-TIMIT corpus (mixed-speech)
whole face) that in our 		TCD-TIMIT corpus (mixed-speech)
experiments turned out to be 		TCD-TIMIT corpus (mixed-speech)
much more effective than positional 		TCD-TIMIT corpus (mixed-speech)
features.  Although the aforementioned audio-visual methods		TCD-TIMIT corpus (mixed-speech)
 work well, they have only		TCD-TIMIT corpus (mixed-speech)
 been evaluated in a speaker-dependent		TCD-TIMIT corpus (mixed-speech)
 setting. Only the availability of		TCD-TIMIT corpus (mixed-speech)
 new large and heterogeneous audio-visual		TCD-TIMIT corpus (mixed-speech)
 datasets has allowed the training		TCD-TIMIT corpus (mixed-speech)
 of deep neu- ral network-based		TCD-TIMIT corpus (mixed-speech)
 speaker-independent speech enhancement models [20		TCD-TIMIT corpus (mixed-speech)
, 21, 22].  The present work shows that		TCD-TIMIT corpus (mixed-speech)
 huge audio-visual datasets are not		TCD-TIMIT corpus (mixed-speech)
 a necessary requirement for speaker-independent		TCD-TIMIT corpus (mixed-speech)
 audio-visual speech enhancement. Although we		TCD-TIMIT corpus (mixed-speech)
 have only considered datasets with		TCD-TIMIT corpus (mixed-speech)
 simple visual scenarios (i.e., the		TCD-TIMIT corpus (mixed-speech)
 target speaker is always facing		TCD-TIMIT corpus (mixed-speech)
 the camera), we expect our		TCD-TIMIT corpus (mixed-speech)
 methods to perform well in		TCD-TIMIT corpus (mixed-speech)
 more complex scenarios thanks to		TCD-TIMIT corpus (mixed-speech)
 the robust landmark extraction		TCD-TIMIT corpus (mixed-speech)
.  2. MODEL ARCHITECTURES		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
We experimented with the four 		TCD-TIMIT corpus (mixed-speech)
models shown in Fig. 1. 		TCD-TIMIT corpus (mixed-speech)
All models receive in input 		TCD-TIMIT corpus (mixed-speech)
the target speaker’s landmark mo- 		TCD-TIMIT corpus (mixed-speech)
tion vectors and the power-law 		TCD-TIMIT corpus (mixed-speech)
compressed spectrogram of the single-channel 		TCD-TIMIT corpus (mixed-speech)
mixed-speech signal. All of them 		TCD-TIMIT corpus (mixed-speech)
perform some kind of masking 		TCD-TIMIT corpus (mixed-speech)
operation.  2.1. VL2M model		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
At each time frame, the 		TCD-TIMIT corpus (mixed-speech)
video-landmark to mask (VL2M) model (		TCD-TIMIT corpus (mixed-speech)
Fig. 1a) estimates a T-F 		TCD-TIMIT corpus (mixed-speech)
mask from visual features only (		TCD-TIMIT corpus (mixed-speech)
of the target speaker). Formally, 		TCD-TIMIT corpus (mixed-speech)
given a video sequence 		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
 = [v1		TCD-TIMIT corpus (mixed-speech)
, . . . , 		TCD-TIMIT corpus (mixed-speech)
vT ], vt ∈ Rn 		TCD-TIMIT corpus (mixed-speech)
and a target mask sequence 		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
 = [m1		TCD-TIMIT corpus (mixed-speech)
, . . . ,		TCD-TIMIT corpus (mixed-speech)
mT ], mt ∈ Rd, 		TCD-TIMIT corpus (mixed-speech)
VL2M perform a function 		TCD-TIMIT corpus (mixed-speech)
Fvl2m(v) = m̂, where m̂ 		TCD-TIMIT corpus (mixed-speech)
is the estimated mask.  The training objective for VL2M		TCD-TIMIT corpus (mixed-speech)
 is a Target Binary Mask		TCD-TIMIT corpus (mixed-speech)
 (TBM) [23, 24], computed using		TCD-TIMIT corpus (mixed-speech)
 the spectrogram of the tar		TCD-TIMIT corpus (mixed-speech)
- get speaker only. This 		TCD-TIMIT corpus (mixed-speech)
is motivated by our goal 		TCD-TIMIT corpus (mixed-speech)
of extracting the speech of 		TCD-TIMIT corpus (mixed-speech)
a target speaker as much 		TCD-TIMIT corpus (mixed-speech)
as possible indepen- dently of 		TCD-TIMIT corpus (mixed-speech)
the concurrent speakers, so that, 		TCD-TIMIT corpus (mixed-speech)
e.g., we do not need 		TCD-TIMIT corpus (mixed-speech)
to estimate their number. An 		TCD-TIMIT corpus (mixed-speech)
additional motivations is that the 		TCD-TIMIT corpus (mixed-speech)
model takes as only input 		TCD-TIMIT corpus (mixed-speech)
the visual features of the  target speaker, and a target		TCD-TIMIT corpus (mixed-speech)
 TBM that only depends on		TCD-TIMIT corpus (mixed-speech)
 the target speaker allows VL2M		TCD-TIMIT corpus (mixed-speech)
 to learn a function (rather		TCD-TIMIT corpus (mixed-speech)
 than approximating an ill-posed one-to-many		TCD-TIMIT corpus (mixed-speech)
 mapping		TCD-TIMIT corpus (mixed-speech)
).  Given a clean speech spectrogram		TCD-TIMIT corpus (mixed-speech)
 of a speaker s		TCD-TIMIT corpus (mixed-speech)
 = [s1		TCD-TIMIT corpus (mixed-speech)
, . . . , 		TCD-TIMIT corpus (mixed-speech)
sT ], st ∈ Rd, 		TCD-TIMIT corpus (mixed-speech)
the TBM is defined by 		TCD-TIMIT corpus (mixed-speech)
comparing, at each frequency bin 		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
 ∈ [1		TCD-TIMIT corpus (mixed-speech)
, . . . , 		TCD-TIMIT corpus (mixed-speech)
d], the target speaker value 		TCD-TIMIT corpus (mixed-speech)
st[f ] vs. a reference 		TCD-TIMIT corpus (mixed-speech)
threshold τ [f ]. As 		TCD-TIMIT corpus (mixed-speech)
in [15], we use a 		TCD-TIMIT corpus (mixed-speech)
function of long-term average speech 		TCD-TIMIT corpus (mixed-speech)
spectrum (LTASS) as reference threshold. 		TCD-TIMIT corpus (mixed-speech)
This threshold indicates if a 		TCD-TIMIT corpus (mixed-speech)
T-F unit is generated by 		TCD-TIMIT corpus (mixed-speech)
the speaker or refers to 		TCD-TIMIT corpus (mixed-speech)
silence or noise. The process 		TCD-TIMIT corpus (mixed-speech)
to compute the speaker’s TBM 		TCD-TIMIT corpus (mixed-speech)
is as follows:  1. The mean π[f		TCD-TIMIT corpus (mixed-speech)
 ] and the standard deviation		TCD-TIMIT corpus (mixed-speech)
 σ[f ] are computed for		TCD-TIMIT corpus (mixed-speech)
 all frequency bins of all		TCD-TIMIT corpus (mixed-speech)
 seen spectro- grams in speaker’s		TCD-TIMIT corpus (mixed-speech)
 data		TCD-TIMIT corpus (mixed-speech)
.  2. The threshold τ [f		TCD-TIMIT corpus (mixed-speech)
 ] is defined as τ		TCD-TIMIT corpus (mixed-speech)
 [f ] = π[f ]+0.6		TCD-TIMIT corpus (mixed-speech)
 ·σ[f ] where 0.6 is		TCD-TIMIT corpus (mixed-speech)
 a value selected by manual		TCD-TIMIT corpus (mixed-speech)
 inspection of several spectrogram-TBM pairs		TCD-TIMIT corpus (mixed-speech)
.  3. The threshold is applied		TCD-TIMIT corpus (mixed-speech)
 to every speaker’s speech spec		TCD-TIMIT corpus (mixed-speech)
- trogram s.  mt[f		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
1, if st[f ] ≥ 		TCD-TIMIT corpus (mixed-speech)
τ [f ], 0, otherwise.  The mapping Fvl2m(·) is carried		TCD-TIMIT corpus (mixed-speech)
 out by a stacked bi		TCD-TIMIT corpus (mixed-speech)
- directional Long Short-Term Memory (		TCD-TIMIT corpus (mixed-speech)
BLSTM) network [25]. The BLSTM 		TCD-TIMIT corpus (mixed-speech)
outputs are then forced to 		TCD-TIMIT corpus (mixed-speech)
lay within the [0, 1] 		TCD-TIMIT corpus (mixed-speech)
range. Finally the computed TBM 		TCD-TIMIT corpus (mixed-speech)
m̂ and the noisy spectrogram 		TCD-TIMIT corpus (mixed-speech)
y are element-wise multiplied to 		TCD-TIMIT corpus (mixed-speech)
ob- tain the estimated clean 		TCD-TIMIT corpus (mixed-speech)
spectrogram ŝm = m̂ ◦ 		TCD-TIMIT corpus (mixed-speech)
y, where 		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
 = [y1		TCD-TIMIT corpus (mixed-speech)
, . . . 		TCD-TIMIT corpus (mixed-speech)
yT ], yt ∈ Rd.  The model parameters are estimated		TCD-TIMIT corpus (mixed-speech)
 to minimize the loss		TCD-TIMIT corpus (mixed-speech)
:  Jvl2m = ∑T		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
t=1  ∑d f=1−mt[f ] · log(m̂t[f		TCD-TIMIT corpus (mixed-speech)
 ])− (1−mt[f ]) · log(1		TCD-TIMIT corpus (mixed-speech)
− m̂t[f ])  2.2. VL2M ref model		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
VL2M generates T-F masks that 		TCD-TIMIT corpus (mixed-speech)
are independent of the acous- 		TCD-TIMIT corpus (mixed-speech)
tic context. We may want 		TCD-TIMIT corpus (mixed-speech)
to refine the masking by 		TCD-TIMIT corpus (mixed-speech)
including such context. This is 		TCD-TIMIT corpus (mixed-speech)
what the novel VL2M ref 		TCD-TIMIT corpus (mixed-speech)
does (Fig. 1b). The computed 		TCD-TIMIT corpus (mixed-speech)
TBM m̂ and the input 		TCD-TIMIT corpus (mixed-speech)
spectrogram y are the input 		TCD-TIMIT corpus (mixed-speech)
to a function that outputs 		TCD-TIMIT corpus (mixed-speech)
an Ideal Amplitude Mask (IAM) 		TCD-TIMIT corpus (mixed-speech)
p (known as FFT-MASK 		TCD-TIMIT corpus (mixed-speech)
in [8]). Given the target 		TCD-TIMIT corpus (mixed-speech)
clean spectrogram s and the 		TCD-TIMIT corpus (mixed-speech)
noisy spectrogram y, the IAM 		TCD-TIMIT corpus (mixed-speech)
is defined as:  pt[f ] = st[f		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
yt[f ]  Note that although IAM generation		TCD-TIMIT corpus (mixed-speech)
 requires the mixed-speech spectrogram, separate		TCD-TIMIT corpus (mixed-speech)
 spectrograms for each concurrent speakers		TCD-TIMIT corpus (mixed-speech)
 are not required		TCD-TIMIT corpus (mixed-speech)
.  The target speaker’s spectrogram s		TCD-TIMIT corpus (mixed-speech)
 is reconstructed by multiplying the		TCD-TIMIT corpus (mixed-speech)
 input spectrogram with the estimated		TCD-TIMIT corpus (mixed-speech)
 IAM. Values greater than 10		TCD-TIMIT corpus (mixed-speech)
 in the IAM are clipped		TCD-TIMIT corpus (mixed-speech)
 to 10 in order to		TCD-TIMIT corpus (mixed-speech)
 obtain better numerical stability as		TCD-TIMIT corpus (mixed-speech)
 suggested in [8		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
v: video input y: noisy 		TCD-TIMIT corpus (mixed-speech)
spectrogram sm: clean spectrogram TBM 		TCD-TIMIT corpus (mixed-speech)
s: clean spectrogram IAM m: 		TCD-TIMIT corpus (mixed-speech)
TBM p: IAM  STACKED		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
BLSTM  m		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
sm  v		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
y  (a) VL2M		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
v VL2M m  y BLSTM		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
BLSTM  Fusion layer		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
BLSTM p  s		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
b) VL2M ref  v		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
y  p STACKED		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
BLSTM  s		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
c) Audio-Visual concat  sm		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
y  p STACKED		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
BLSTM  s		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
v VL2M m  (d) Audio-Visual concat-ref		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
Fig. 1. Model architectures.  The model performs a function		TCD-TIMIT corpus (mixed-speech)
 Fmr(v, y) = p̂ that		TCD-TIMIT corpus (mixed-speech)
 con- sists of a VL2M		TCD-TIMIT corpus (mixed-speech)
 component plus three different BLSTMs		TCD-TIMIT corpus (mixed-speech)
 Gm, Gy and H		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
Gm(Fvl2m(v)) = rm receives the 		TCD-TIMIT corpus (mixed-speech)
VL2M mask m̂ as in- 		TCD-TIMIT corpus (mixed-speech)
put, and Gy(y) = ry 		TCD-TIMIT corpus (mixed-speech)
is fed with the noisy 		TCD-TIMIT corpus (mixed-speech)
spectrogram. Their output rm, 		TCD-TIMIT corpus (mixed-speech)
ry ∈ Rz are fused 		TCD-TIMIT corpus (mixed-speech)
in a joint audio-visual represen- 		TCD-TIMIT corpus (mixed-speech)
tation 		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
 = [h1		TCD-TIMIT corpus (mixed-speech)
, . . . ,		TCD-TIMIT corpus (mixed-speech)
hT ], where ht is 		TCD-TIMIT corpus (mixed-speech)
a linear combination of rmt 		TCD-TIMIT corpus (mixed-speech)
and ryt : ht = 		TCD-TIMIT corpus (mixed-speech)
Whm ·rmt +Why ·ryt +bh. 		TCD-TIMIT corpus (mixed-speech)
h is the input of 		TCD-TIMIT corpus (mixed-speech)
the third BLSTM H (		TCD-TIMIT corpus (mixed-speech)
h) = p̂, where p̂ 		TCD-TIMIT corpus (mixed-speech)
lays in the [0,10] range. 		TCD-TIMIT corpus (mixed-speech)
The loss function is:  Jmr		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
T∑ t=1  d∑ f=1		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
p̂t[f ] · yt[f ]− 		TCD-TIMIT corpus (mixed-speech)
st[f ])2  2.3. Audio-Visual concat model		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
The third model (Fig. 1c) 		TCD-TIMIT corpus (mixed-speech)
performs early fusion of audio- 		TCD-TIMIT corpus (mixed-speech)
visual features. This model consists 		TCD-TIMIT corpus (mixed-speech)
of a single stacked BLSTM 		TCD-TIMIT corpus (mixed-speech)
that computes the IAM mask 		TCD-TIMIT corpus (mixed-speech)
p̂ from the concate- 		TCD-TIMIT corpus (mixed-speech)
nated [v,y]. The training loss 		TCD-TIMIT corpus (mixed-speech)
is the same Jmr used 		TCD-TIMIT corpus (mixed-speech)
to train VL2M ref. This 		TCD-TIMIT corpus (mixed-speech)
model can be regarded as 		TCD-TIMIT corpus (mixed-speech)
a simplification of VL2M ref, 		TCD-TIMIT corpus (mixed-speech)
where the VL2M operation is 		TCD-TIMIT corpus (mixed-speech)
not performed.  2.4. Audio-Visual concat-ref model		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
The fourth model (Fig. 1d) 		TCD-TIMIT corpus (mixed-speech)
is an improved version of 		TCD-TIMIT corpus (mixed-speech)
the model described in section 2		TCD-TIMIT corpus (mixed-speech)
.3. The only difference is 		TCD-TIMIT corpus (mixed-speech)
the input of the stacked 		TCD-TIMIT corpus (mixed-speech)
BLSTM that is replaced 		TCD-TIMIT corpus (mixed-speech)
by [̂sm,y] where ŝm is 		TCD-TIMIT corpus (mixed-speech)
the denoised spectrogram returned by 		TCD-TIMIT corpus (mixed-speech)
VL2M operation.  3. EXPERIMENTAL SETUP		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
3.1. Dataset  All experiments were carried out		TCD-TIMIT corpus (mixed-speech)
 using the GRID [9] and		TCD-TIMIT corpus (mixed-speech)
 TCD-TIMIT [10] audio-visual datasets. For		TCD-TIMIT corpus (mixed-speech)
 each of them, we created		TCD-TIMIT corpus (mixed-speech)
 a mixed-speech version		TCD-TIMIT corpus (mixed-speech)
.  Regarding the GRID corpus, for		TCD-TIMIT corpus (mixed-speech)
 each of the 33 speakers		TCD-TIMIT corpus (mixed-speech)
 (one had to be discarded		TCD-TIMIT corpus (mixed-speech)
) we first randomly selected 200 ut- terances (out of 1000		TCD-TIMIT corpus (mixed-speech)
). Then, for each utterance, 		TCD-TIMIT corpus (mixed-speech)
we created 3 different audio-mixed 		TCD-TIMIT corpus (mixed-speech)
samples. Each audio-mixed sample was 		TCD-TIMIT corpus (mixed-speech)
created by mixing the chosen 		TCD-TIMIT corpus (mixed-speech)
utterance with one utter- ance 		TCD-TIMIT corpus (mixed-speech)
from a different speaker.  That resulted in 600 audio-mixed		TCD-TIMIT corpus (mixed-speech)
 samples per speaker		TCD-TIMIT corpus (mixed-speech)
.  The resulting dataset was split		TCD-TIMIT corpus (mixed-speech)
 into disjoint sets of 25/4/4		TCD-TIMIT corpus (mixed-speech)
 speakers for training/validation/testing respectively		TCD-TIMIT corpus (mixed-speech)
.  The TCD-TIMIT corpus consists of		TCD-TIMIT corpus (mixed-speech)
 59 speakers (we ex- cluded		TCD-TIMIT corpus (mixed-speech)
 3 professionally-trained lipspeakers) and 98		TCD-TIMIT corpus (mixed-speech)
 utterances per speaker. The mixed-speech		TCD-TIMIT corpus (mixed-speech)
 version was created following the		TCD-TIMIT corpus (mixed-speech)
 same procedure as for GRID		TCD-TIMIT corpus (mixed-speech)
, with one difference. Con- 		TCD-TIMIT corpus (mixed-speech)
trary to GRID, TCD-TIMIT utterances 		TCD-TIMIT corpus (mixed-speech)
have different dura- tion. Thus 2 utterances were mixed only if		TCD-TIMIT corpus (mixed-speech)
 their duration dif- ference did		TCD-TIMIT corpus (mixed-speech)
 not exceed 2 seconds. For		TCD-TIMIT corpus (mixed-speech)
 each utterance pair, we forced		TCD-TIMIT corpus (mixed-speech)
 the non-target speaker’s utterance to		TCD-TIMIT corpus (mixed-speech)
 match the du- ration of		TCD-TIMIT corpus (mixed-speech)
 the target speaker utterance. If		TCD-TIMIT corpus (mixed-speech)
 it was longer, the utterance		TCD-TIMIT corpus (mixed-speech)
 was cut at its end		TCD-TIMIT corpus (mixed-speech)
, whereas if it was 		TCD-TIMIT corpus (mixed-speech)
shorter, silence samples were equally 		TCD-TIMIT corpus (mixed-speech)
added at its start and 		TCD-TIMIT corpus (mixed-speech)
end.  The resulting dataset was split		TCD-TIMIT corpus (mixed-speech)
 into disjoint sets of 51/4/4		TCD-TIMIT corpus (mixed-speech)
 speakers for training/validation/testing respectively		TCD-TIMIT corpus (mixed-speech)
.  3.2. LSTM training		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
In all experiments, the models 		TCD-TIMIT corpus (mixed-speech)
were trained using the Adam 		TCD-TIMIT corpus (mixed-speech)
optimizer [26]. Early stopping was 		TCD-TIMIT corpus (mixed-speech)
applied when the error on 		TCD-TIMIT corpus (mixed-speech)
the validation set did not 		TCD-TIMIT corpus (mixed-speech)
decrease over 5 consecutive epochs.  VL2M, AV concat and AV		TCD-TIMIT corpus (mixed-speech)
 concat-ref had 5, 3 and		TCD-TIMIT corpus (mixed-speech)
 3 stacked BLSTM layers respectively		TCD-TIMIT corpus (mixed-speech)
. All BLSTMs had 250 		TCD-TIMIT corpus (mixed-speech)
units. Hyper-parameters selection was performed 		TCD-TIMIT corpus (mixed-speech)
by using random search with 		TCD-TIMIT corpus (mixed-speech)
a limited number of samples, 		TCD-TIMIT corpus (mixed-speech)
therefore all the reported results 		TCD-TIMIT corpus (mixed-speech)
may improve through a deeper 		TCD-TIMIT corpus (mixed-speech)
hyper- parameters validation phase.  VL2M ref and AV concat-ref		TCD-TIMIT corpus (mixed-speech)
 training was performed in 2		TCD-TIMIT corpus (mixed-speech)
 steps. We first pre-trained the		TCD-TIMIT corpus (mixed-speech)
 models using the oracle TBM		TCD-TIMIT corpus (mixed-speech)
 m. Then we substituted the		TCD-TIMIT corpus (mixed-speech)
 oracle masks with the VL2M		TCD-TIMIT corpus (mixed-speech)
 component and retrained the models		TCD-TIMIT corpus (mixed-speech)
 while freezing the pa- rameters		TCD-TIMIT corpus (mixed-speech)
 of the VL2M component		TCD-TIMIT corpus (mixed-speech)
.  3.3. Audio pre- and post-processing		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
The original waveforms were resampled 		TCD-TIMIT corpus (mixed-speech)
to 16 kHz. Short- Time 		TCD-TIMIT corpus (mixed-speech)
Fourier Transform (STFT) x was 		TCD-TIMIT corpus (mixed-speech)
computed using FFT size of 512, Hann window of length 25		TCD-TIMIT corpus (mixed-speech)
 ms (400 samples), and hop		TCD-TIMIT corpus (mixed-speech)
 length of 10 ms (160		TCD-TIMIT corpus (mixed-speech)
 samples). The input spectro- gram		TCD-TIMIT corpus (mixed-speech)
 was obtained taking the STFT		TCD-TIMIT corpus (mixed-speech)
 magnitude and perform- ing power-law		TCD-TIMIT corpus (mixed-speech)
 compression |x|p with p		TCD-TIMIT corpus (mixed-speech)
 = 0.3. Finally we applied		TCD-TIMIT corpus (mixed-speech)
 per-speaker 0-mean 1-std normalization		TCD-TIMIT corpus (mixed-speech)
.  In the post-processing stage, the		TCD-TIMIT corpus (mixed-speech)
 enhanced waveform gen- erated by		TCD-TIMIT corpus (mixed-speech)
 the speech enhancement models was		TCD-TIMIT corpus (mixed-speech)
 reconstructed		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
SDR PESQ ViSQOL  Noisy −1.06 1.81 2.11 VL2M		TCD-TIMIT corpus (mixed-speech)
 3.17 1.51 1.16 VL2M ref		TCD-TIMIT corpus (mixed-speech)
 6.50 2.58 2.99 AV concat		TCD-TIMIT corpus (mixed-speech)
 6.31 2.49 2.83 AV c-ref		TCD-TIMIT corpus (mixed-speech)
 6.17 2.58 2.96		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
Table 1. GRID results - 		TCD-TIMIT corpus (mixed-speech)
speaker-dependent. The “Noisy” row refers 		TCD-TIMIT corpus (mixed-speech)
to the metric values of 		TCD-TIMIT corpus (mixed-speech)
the input mixed-speech signal.  2 Speakers 3 Speakers SDR		TCD-TIMIT corpus (mixed-speech)
 PESQ ViSQOL SDR PESQ ViSQOL		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
Noisy 0.21 1.94 2.58 −5.34 1		TCD-TIMIT corpus (mixed-speech)
.43 1.62 VL2M 3.02 1.81 1		TCD-TIMIT corpus (mixed-speech)
.70 −2.03 1.43 1.25 VL2M 		TCD-TIMIT corpus (mixed-speech)
ref 6.52 2.53 3.02 2.83 2		TCD-TIMIT corpus (mixed-speech)
.19 2.53 AV concat 7.37 2		TCD-TIMIT corpus (mixed-speech)
.65 3.03 3.02 2.24 2.49 		TCD-TIMIT corpus (mixed-speech)
AV c-ref 8.05 2.70 3.07 4		TCD-TIMIT corpus (mixed-speech)
.02 2.33 2.64  Table 2. GRID results		TCD-TIMIT corpus (mixed-speech)
 - speaker-independent		TCD-TIMIT corpus (mixed-speech)
.  by applying the inverse STFT		TCD-TIMIT corpus (mixed-speech)
 to the estimated clean spectro		TCD-TIMIT corpus (mixed-speech)
- gram and using the 		TCD-TIMIT corpus (mixed-speech)
phase of the noisy input 		TCD-TIMIT corpus (mixed-speech)
signal.  3.4. Video pre-processing		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
Face landmarks were extracted from 		TCD-TIMIT corpus (mixed-speech)
video using the Dlib [7] 		TCD-TIMIT corpus (mixed-speech)
implementation of the face landmark 		TCD-TIMIT corpus (mixed-speech)
estimator described in [6]. It 		TCD-TIMIT corpus (mixed-speech)
returns 68 x-y points, for 		TCD-TIMIT corpus (mixed-speech)
an overall 136 values. We 		TCD-TIMIT corpus (mixed-speech)
upsampled from 25/29.97 fps (GRID/TCD-TIMIT) 		TCD-TIMIT corpus (mixed-speech)
to 100 fps to match 		TCD-TIMIT corpus (mixed-speech)
the frame rate of the 		TCD-TIMIT corpus (mixed-speech)
audio spectrogram. Upsampling was carried 		TCD-TIMIT corpus (mixed-speech)
out through linear interpolation over 		TCD-TIMIT corpus (mixed-speech)
time.  The final video feature vector		TCD-TIMIT corpus (mixed-speech)
 v was obtained by com		TCD-TIMIT corpus (mixed-speech)
- puting the per-speaker normalized 		TCD-TIMIT corpus (mixed-speech)
motion vector of the face 		TCD-TIMIT corpus (mixed-speech)
landmarks by simply subtracting every 		TCD-TIMIT corpus (mixed-speech)
frame with the previ- ous 		TCD-TIMIT corpus (mixed-speech)
one. The motion vector of 		TCD-TIMIT corpus (mixed-speech)
the first frame was set 		TCD-TIMIT corpus (mixed-speech)
to zero.  4. RESULTS		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
In order to compare our 		TCD-TIMIT corpus (mixed-speech)
models to previous works in 		TCD-TIMIT corpus (mixed-speech)
both speech enhancement and separation, 		TCD-TIMIT corpus (mixed-speech)
we evaluated the perfor- mance 		TCD-TIMIT corpus (mixed-speech)
of the proposed models using 		TCD-TIMIT corpus (mixed-speech)
both speech separation  2 Speakers 3 Speakers SDR		TCD-TIMIT corpus (mixed-speech)
 PESQ ViSQOL SDR PESQ ViSQOL		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
Noisy 0.21 2.22 2.74 −3.42 1		TCD-TIMIT corpus (mixed-speech)
.92 2.04 VL2M 2.88 2.25 2		TCD-TIMIT corpus (mixed-speech)
.62 −0.51 1.99 1.98 VL2M 		TCD-TIMIT corpus (mixed-speech)
ref 9.24 2.81 3.09 5.27 2		TCD-TIMIT corpus (mixed-speech)
.44 2.54 AV concat 9.56 2		TCD-TIMIT corpus (mixed-speech)
.80 3.09 5.15 2.41 2.52 		TCD-TIMIT corpus (mixed-speech)
AV c-ref 10.55 3.03 3.21 5		TCD-TIMIT corpus (mixed-speech)
.37 2.45 2.58  Table 3. TCD-TIMIT results		TCD-TIMIT corpus (mixed-speech)
 - speaker-independent		TCD-TIMIT corpus (mixed-speech)
.  and enhancement metrics. Specifically, we		TCD-TIMIT corpus (mixed-speech)
 measured the ca- pability of		TCD-TIMIT corpus (mixed-speech)
 separating the target utterance from		TCD-TIMIT corpus (mixed-speech)
 the concurrent utterance with the		TCD-TIMIT corpus (mixed-speech)
 source-to-distortion ratio (SDR) [27, 28		TCD-TIMIT corpus (mixed-speech)
]. While the quality of 		TCD-TIMIT corpus (mixed-speech)
estimated target speech was measured 		TCD-TIMIT corpus (mixed-speech)
with the perceptual PESQ [29] 		TCD-TIMIT corpus (mixed-speech)
and ViSQOL [30] metrics. For 		TCD-TIMIT corpus (mixed-speech)
PESQ we used the narrow 		TCD-TIMIT corpus (mixed-speech)
band mode while for ViSQOL 		TCD-TIMIT corpus (mixed-speech)
we used the wide band 		TCD-TIMIT corpus (mixed-speech)
mode.  As a very first experiment		TCD-TIMIT corpus (mixed-speech)
 we compared landmark posi- tion		TCD-TIMIT corpus (mixed-speech)
 vs. landmark motion vectors. It		TCD-TIMIT corpus (mixed-speech)
 turned out that landmark positions		TCD-TIMIT corpus (mixed-speech)
 performed poorly, thus all results		TCD-TIMIT corpus (mixed-speech)
 reported here refer to landmark		TCD-TIMIT corpus (mixed-speech)
 motion vectors only		TCD-TIMIT corpus (mixed-speech)
.  We then carried out some		TCD-TIMIT corpus (mixed-speech)
 speaker-dependent experiments to compare our		TCD-TIMIT corpus (mixed-speech)
 models with previous studies as		TCD-TIMIT corpus (mixed-speech)
, to the best of 		TCD-TIMIT corpus (mixed-speech)
our knowledge, there are no 		TCD-TIMIT corpus (mixed-speech)
reported results of speaker- independent 		TCD-TIMIT corpus (mixed-speech)
systems trained and tested on 		TCD-TIMIT corpus (mixed-speech)
GRID and TCD- TIMIT to 		TCD-TIMIT corpus (mixed-speech)
compare with. Table 1 reports 		TCD-TIMIT corpus (mixed-speech)
the test-set evalua- tion of 		TCD-TIMIT corpus (mixed-speech)
speaker-dependent models on the GRID 		TCD-TIMIT corpus (mixed-speech)
corpus with landmark motion vectors. 		TCD-TIMIT corpus (mixed-speech)
Results are comparable with previ- 		TCD-TIMIT corpus (mixed-speech)
ous state-of-the-art studies in an 		TCD-TIMIT corpus (mixed-speech)
almost identical setting [15, 17].  Table 2 and 3 show		TCD-TIMIT corpus (mixed-speech)
 speaker-independent test-set results on the		TCD-TIMIT corpus (mixed-speech)
 GRID and TCD-TIMIT datasets respectively		TCD-TIMIT corpus (mixed-speech)
. V2ML performs significantly worse 		TCD-TIMIT corpus (mixed-speech)
than the other three models 		TCD-TIMIT corpus (mixed-speech)
in- dicating that a successful 		TCD-TIMIT corpus (mixed-speech)
mask generation has to depend 		TCD-TIMIT corpus (mixed-speech)
on the acoustic context. The 		TCD-TIMIT corpus (mixed-speech)
performance of the three models 		TCD-TIMIT corpus (mixed-speech)
in the speaker-independent setting is 		TCD-TIMIT corpus (mixed-speech)
comparable to that in the 		TCD-TIMIT corpus (mixed-speech)
speaker-dependent setting.  AV concat-ref outperforms V2ML ref		TCD-TIMIT corpus (mixed-speech)
 and AV concat for both		TCD-TIMIT corpus (mixed-speech)
 datasets. This supports the utility		TCD-TIMIT corpus (mixed-speech)
 of a refinement strat- egy		TCD-TIMIT corpus (mixed-speech)
 and suggests that the refinement		TCD-TIMIT corpus (mixed-speech)
 is more effective when it		TCD-TIMIT corpus (mixed-speech)
 directly refines the estimated clean		TCD-TIMIT corpus (mixed-speech)
 spectrogram, rather than refining the		TCD-TIMIT corpus (mixed-speech)
 estimated mask		TCD-TIMIT corpus (mixed-speech)
.  Finally, we evaluated the systems		TCD-TIMIT corpus (mixed-speech)
 in a more challenging testing		TCD-TIMIT corpus (mixed-speech)
 condition where the target utterance		TCD-TIMIT corpus (mixed-speech)
 was mixed with 2 utterances		TCD-TIMIT corpus (mixed-speech)
 from 2 competing speakers. Despite		TCD-TIMIT corpus (mixed-speech)
 the model was trained with		TCD-TIMIT corpus (mixed-speech)
 mixtures of two speakers, the		TCD-TIMIT corpus (mixed-speech)
 decrease of performance was not		TCD-TIMIT corpus (mixed-speech)
 dramatic		TCD-TIMIT corpus (mixed-speech)
.  Code and some testing examples		TCD-TIMIT corpus (mixed-speech)
 of our models are avail		TCD-TIMIT corpus (mixed-speech)
- able at https://goo.gl/3h1NgE.  5. CONCLUSION		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
This paper proposes the use 		TCD-TIMIT corpus (mixed-speech)
of face landmark motion vec- 		TCD-TIMIT corpus (mixed-speech)
tors for audio-visual speech enhancement 		TCD-TIMIT corpus (mixed-speech)
in a single-channel multi-talker scenario. 		TCD-TIMIT corpus (mixed-speech)
Different models are tested where 		TCD-TIMIT corpus (mixed-speech)
land- mark motion vectors are 		TCD-TIMIT corpus (mixed-speech)
used to generate time-frequency (T- 		TCD-TIMIT corpus (mixed-speech)
F) masks that extract the 		TCD-TIMIT corpus (mixed-speech)
target speaker’s spectrogram from the 		TCD-TIMIT corpus (mixed-speech)
acoustic mixed-speech spectrogram.  To the best of our		TCD-TIMIT corpus (mixed-speech)
 knowledge, some of the proposed		TCD-TIMIT corpus (mixed-speech)
 mod- els are the first		TCD-TIMIT corpus (mixed-speech)
 models trained and evaluated on		TCD-TIMIT corpus (mixed-speech)
 the limited size GRID and		TCD-TIMIT corpus (mixed-speech)
 TCD-TIMIT datasets that accomplish speaker		TCD-TIMIT corpus (mixed-speech)
- independent speech enhancement in 		TCD-TIMIT corpus (mixed-speech)
the multi-talker setting, with a 		TCD-TIMIT corpus (mixed-speech)
quality of enhancement comparable to 		TCD-TIMIT corpus (mixed-speech)
that achieved in a speaker-dependent 		TCD-TIMIT corpus (mixed-speech)
setting.  https://goo.gl/3h1NgE		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
6. REFERENCES  [1] E. Colin Cherry, “Some		TCD-TIMIT corpus (mixed-speech)
 experiments on the recognition of		TCD-TIMIT corpus (mixed-speech)
 speech, with one and with		TCD-TIMIT corpus (mixed-speech)
 two ears,” The Journal of		TCD-TIMIT corpus (mixed-speech)
 the Acoustical Society of America		TCD-TIMIT corpus (mixed-speech)
, vol. 25, no. 5, 		TCD-TIMIT corpus (mixed-speech)
pp. 975–979, 1953.  [2] Josh H McDermott, “The		TCD-TIMIT corpus (mixed-speech)
 cocktail party problem,” Current Biology		TCD-TIMIT corpus (mixed-speech)
, vol. 19, no. 22, 		TCD-TIMIT corpus (mixed-speech)
pp. R1024–R1027, 2009.  [3] Elana Zion Golumbic, Gregory		TCD-TIMIT corpus (mixed-speech)
 B. Cogan, Charles E. Schroeder		TCD-TIMIT corpus (mixed-speech)
, and David Poeppel, “Visual 		TCD-TIMIT corpus (mixed-speech)
input enhances selective speech envelope 		TCD-TIMIT corpus (mixed-speech)
tracking in auditory cortex at 		TCD-TIMIT corpus (mixed-speech)
a “cocktail party”,” Journal of 		TCD-TIMIT corpus (mixed-speech)
Neu- roscience, vol. 33, no. 4, pp. 1417–1426, 2013		TCD-TIMIT corpus (mixed-speech)
.  [4] Wei Ji Ma, Xiang		TCD-TIMIT corpus (mixed-speech)
 Zhou, Lars A. Ross, John		TCD-TIMIT corpus (mixed-speech)
 J. Foxe, and Lucas C		TCD-TIMIT corpus (mixed-speech)
. Parra, “Lip-reading aids word 		TCD-TIMIT corpus (mixed-speech)
recognition most in moderate noise: 		TCD-TIMIT corpus (mixed-speech)
A bayesian explanation using high-dimensional 		TCD-TIMIT corpus (mixed-speech)
feature space,” PLOS ONE, vol. 4, no. 3, pp. 1–14, 03		TCD-TIMIT corpus (mixed-speech)
 2009		TCD-TIMIT corpus (mixed-speech)
.  [5] Albert S Bregman, Auditory		TCD-TIMIT corpus (mixed-speech)
 scene analysis: The perceptual organi		TCD-TIMIT corpus (mixed-speech)
- zation of sound, MIT 		TCD-TIMIT corpus (mixed-speech)
press, 1994.  [6] Vahid Kazemi and Josephine		TCD-TIMIT corpus (mixed-speech)
 Sullivan, “One millisecond face align		TCD-TIMIT corpus (mixed-speech)
- ment with an ensemble 		TCD-TIMIT corpus (mixed-speech)
of regression trees,” in The 		TCD-TIMIT corpus (mixed-speech)
IEEE Conference on Computer Vision 		TCD-TIMIT corpus (mixed-speech)
and Pattern Recognition (CVPR), June 2014		TCD-TIMIT corpus (mixed-speech)
.  [7] Davis E. King, “Dlib-ml		TCD-TIMIT corpus (mixed-speech)
: A machine learning toolkit,” 		TCD-TIMIT corpus (mixed-speech)
Journal of Machine Learning Research, 		TCD-TIMIT corpus (mixed-speech)
vol. 10, pp. 1755–1758, 2009.  [8] Yuxuan Wang, Arun Narayanan		TCD-TIMIT corpus (mixed-speech)
, and DeLiang Wang, “On 		TCD-TIMIT corpus (mixed-speech)
Training Targets for Supervised Speech 		TCD-TIMIT corpus (mixed-speech)
Separation,” IEEE/ACM Transactions on Audio, 		TCD-TIMIT corpus (mixed-speech)
Speech, and Language Processing, vol. 22, no. 12, pp. 1849–1858, Dec		TCD-TIMIT corpus (mixed-speech)
. 2014.  [9] Martin Cooke, Jon Barker		TCD-TIMIT corpus (mixed-speech)
, Stuart Cunningham, and Xu 		TCD-TIMIT corpus (mixed-speech)
Shao, “An audio-visual corpus for 		TCD-TIMIT corpus (mixed-speech)
speech perception and automatic speech 		TCD-TIMIT corpus (mixed-speech)
recognition,” The Journal of the 		TCD-TIMIT corpus (mixed-speech)
Acoustical Society of America, vol. 120, no. 5, pp. 2421–2424, Nov		TCD-TIMIT corpus (mixed-speech)
. 2006.  [10] Naomi Harte and Eoin		TCD-TIMIT corpus (mixed-speech)
 Gillen, “TCD-TIMIT: An Audio-Visual Cor		TCD-TIMIT corpus (mixed-speech)
- pus of Continuous Speech,” 		TCD-TIMIT corpus (mixed-speech)
IEEE Transactions on Multimedia, vol. 17, no. 5, pp. 603–615, May		TCD-TIMIT corpus (mixed-speech)
 2015		TCD-TIMIT corpus (mixed-speech)
.  [11] Z. Chen, Y. Luo		TCD-TIMIT corpus (mixed-speech)
, and N. Mesgarani, “Deep 		TCD-TIMIT corpus (mixed-speech)
attractor network for single-microphone speaker 		TCD-TIMIT corpus (mixed-speech)
separation,” in 2017 IEEE International 		TCD-TIMIT corpus (mixed-speech)
Conference on Acoustics, Speech and 		TCD-TIMIT corpus (mixed-speech)
Signal Processing (ICASSP), March 2017, 		TCD-TIMIT corpus (mixed-speech)
pp. 246–250.  [12] Yusuf Isik, Jonathan Le		TCD-TIMIT corpus (mixed-speech)
 Roux, Zhuo Chen, Shinji Watanabe		TCD-TIMIT corpus (mixed-speech)
, and John R. 		TCD-TIMIT corpus (mixed-speech)
Hershey, “Single-channel multi-speaker separation using 		TCD-TIMIT corpus (mixed-speech)
deep clustering,” in Interspeech, 2016.  [13] Morten Kolbaek, Dong Yu		TCD-TIMIT corpus (mixed-speech)
, Zheng-Hua Tan, Jesper Jensen, 		TCD-TIMIT corpus (mixed-speech)
Morten Kolbaek, Dong Yu, Zheng-Hua 		TCD-TIMIT corpus (mixed-speech)
Tan, and Jesper Jensen, “Multitalker 		TCD-TIMIT corpus (mixed-speech)
speech separation with utterance-level permutation 		TCD-TIMIT corpus (mixed-speech)
invariant training of deep recurrent 		TCD-TIMIT corpus (mixed-speech)
neural networks,” IEEE/ACM Trans. Audio, 		TCD-TIMIT corpus (mixed-speech)
Speech and Lang. Proc., vol. 25, no. 10, pp. 1901–1913, Oct		TCD-TIMIT corpus (mixed-speech)
. 2017.  [14] Bertrand Rivet, Wenwu Wang		TCD-TIMIT corpus (mixed-speech)
, Syed Mohsen Naqvi, and 		TCD-TIMIT corpus (mixed-speech)
Jonathon Chambers, “Audiovisual Speech Source 		TCD-TIMIT corpus (mixed-speech)
Separation: An overview of key 		TCD-TIMIT corpus (mixed-speech)
methodologies,” IEEE Signal Processing Magazine, 		TCD-TIMIT corpus (mixed-speech)
vol. 31, no. 3, pp. 125		TCD-TIMIT corpus (mixed-speech)
–134, May 2014.  [15] Aviv Gabbay, Ariel Ephrat		TCD-TIMIT corpus (mixed-speech)
, Tavi Halperin, and Shmuel 		TCD-TIMIT corpus (mixed-speech)
Peleg, “Seeing through noise: Visually 		TCD-TIMIT corpus (mixed-speech)
driven speaker separation and enhancement,” 		TCD-TIMIT corpus (mixed-speech)
in ICASSP. 2018, pp. 3051–3055, 		TCD-TIMIT corpus (mixed-speech)
IEEE.  [16] Ariel Ephrat, Tavi Halperin		TCD-TIMIT corpus (mixed-speech)
, and Shmuel Peleg, “Improved 		TCD-TIMIT corpus (mixed-speech)
speech reconstruction from silent video,” 		TCD-TIMIT corpus (mixed-speech)
ICCV 2017 Workshop on Computer 		TCD-TIMIT corpus (mixed-speech)
Vision for Audio-Visual Media, 2017.  [17] Aviv Gabbay, Asaph Shamir		TCD-TIMIT corpus (mixed-speech)
, and Shmuel Peleg, “Visual 		TCD-TIMIT corpus (mixed-speech)
speech en- hancement,” in Interspeech. 2018, pp. 1170–1174, ISCA		TCD-TIMIT corpus (mixed-speech)
.  [18] Jen-Cheng Hou, Syu-Siang Wang		TCD-TIMIT corpus (mixed-speech)
, Ying-Hui Lai, Yu Tsao, 		TCD-TIMIT corpus (mixed-speech)
Hsiu-Wen Chang, and Hsin-Min 		TCD-TIMIT corpus (mixed-speech)
Wang, “Audio-Visual Speech Enhancement Us- 		TCD-TIMIT corpus (mixed-speech)
ing Multimodal Deep Convolutional Neural 		TCD-TIMIT corpus (mixed-speech)
Networks,” IEEE Trans- actions on 		TCD-TIMIT corpus (mixed-speech)
Emerging Topics in Computational Intelligence, 		TCD-TIMIT corpus (mixed-speech)
vol. 2, no. 2, pp. 117		TCD-TIMIT corpus (mixed-speech)
–128, Apr. 2018.  [19] Jen-Cheng Hou, Syu-Siang Wang		TCD-TIMIT corpus (mixed-speech)
, Ying-Hui Lai, Jen-Chun Lin, 		TCD-TIMIT corpus (mixed-speech)
Yu Tsao, Hsiu-Wen Chang, and 		TCD-TIMIT corpus (mixed-speech)
Hsin-Min Wang, “Audio-visual speech enhancement 		TCD-TIMIT corpus (mixed-speech)
using deep neural networks,” in 2016 Asia- Pacific Signal and Information		TCD-TIMIT corpus (mixed-speech)
 Processing Association Annual Sum- mit		TCD-TIMIT corpus (mixed-speech)
 and Conference (APSIPA), Jeju, South		TCD-TIMIT corpus (mixed-speech)
 Korea, Dec. 2016, pp. 1–6		TCD-TIMIT corpus (mixed-speech)
, IEEE.  [20] Ariel Ephrat, Inbar Mosseri		TCD-TIMIT corpus (mixed-speech)
, Oran Lang, Tali Dekel, 		TCD-TIMIT corpus (mixed-speech)
Kevin Wilson, Avinatan Hassidim, William 		TCD-TIMIT corpus (mixed-speech)
T. Freeman, and Michael 		TCD-TIMIT corpus (mixed-speech)
Rubinstein, “Looking to Listen at 		TCD-TIMIT corpus (mixed-speech)
the Cocktail Party: A Speaker-Independent 		TCD-TIMIT corpus (mixed-speech)
Audio-Visual Model for Speech Separation,” 		TCD-TIMIT corpus (mixed-speech)
ACM Transactions on Graphics, vol. 37, no. 4, pp. 1–11, July		TCD-TIMIT corpus (mixed-speech)
 2018, arXiv: 1804.03619		TCD-TIMIT corpus (mixed-speech)
.  [21] T. Afouras, J. S		TCD-TIMIT corpus (mixed-speech)
. Chung, and A. 		TCD-TIMIT corpus (mixed-speech)
Zisserman, “The conversation: Deep audio-visual 		TCD-TIMIT corpus (mixed-speech)
speech enhancement,” in Interspeech, 2018.  [22] Andrew Owens and Alexei		TCD-TIMIT corpus (mixed-speech)
 A Efros, “Audio-visual scene analysis		TCD-TIMIT corpus (mixed-speech)
 with self-supervised multisensory features,” European		TCD-TIMIT corpus (mixed-speech)
 Conference on Computer Vision (ECCV		TCD-TIMIT corpus (mixed-speech)
), 2018.  [23] Michael C. Anzalone, Lauren		TCD-TIMIT corpus (mixed-speech)
 Calandruccio, Karen A. Doherty, and		TCD-TIMIT corpus (mixed-speech)
 Laurel H. Carney, “Determination of		TCD-TIMIT corpus (mixed-speech)
 the potential benefit of time		TCD-TIMIT corpus (mixed-speech)
- frequency gain manipulation,” Ear 		TCD-TIMIT corpus (mixed-speech)
Hear, vol. 27, no. 5, 		TCD-TIMIT corpus (mixed-speech)
pp. 480–492, Oct 2006, 16957499[pmid].  [24] Ulrik Kjems, Jesper B		TCD-TIMIT corpus (mixed-speech)
. Boldt, Michael S. Pedersen, 		TCD-TIMIT corpus (mixed-speech)
Thomas Lunner, and DeLiang 		TCD-TIMIT corpus (mixed-speech)
Wang, “Role of mask pattern 		TCD-TIMIT corpus (mixed-speech)
in intelligibility of ideal binary-masked 		TCD-TIMIT corpus (mixed-speech)
noisy speech,” The Journal of 		TCD-TIMIT corpus (mixed-speech)
the Acoustical Society of America, 		TCD-TIMIT corpus (mixed-speech)
vol. 126, no. 3, pp. 1415		TCD-TIMIT corpus (mixed-speech)
–1426, 2009.  [25] A. Graves, A. Mohamed		TCD-TIMIT corpus (mixed-speech)
, and G. Hinton, “Speech 		TCD-TIMIT corpus (mixed-speech)
recognition with deep recurrent neural 		TCD-TIMIT corpus (mixed-speech)
networks,” in 2013 IEEE International 		TCD-TIMIT corpus (mixed-speech)
Con- ference on Acoustics, Speech 		TCD-TIMIT corpus (mixed-speech)
and Signal Processing, May 2013, 		TCD-TIMIT corpus (mixed-speech)
pp. 6645–6649.  [26] Diederik P Kingma and		TCD-TIMIT corpus (mixed-speech)
 Jimmy Ba, “Adam: A method		TCD-TIMIT corpus (mixed-speech)
 for stochastic optimization,” arXiv preprint		TCD-TIMIT corpus (mixed-speech)
 arXiv:1412.6980, 2014		TCD-TIMIT corpus (mixed-speech)
.  [27] E. Vincent, R. Gribonval		TCD-TIMIT corpus (mixed-speech)
, and C. Fevotte, “Performance 		TCD-TIMIT corpus (mixed-speech)
measure- ment in blind audio 		TCD-TIMIT corpus (mixed-speech)
source separation,” IEEE Transactions on 		TCD-TIMIT corpus (mixed-speech)
Audio, Speech and Language Processing, 		TCD-TIMIT corpus (mixed-speech)
vol. 14, no. 4, pp. 1462		TCD-TIMIT corpus (mixed-speech)
–1469, July 2006.  [28] Colin Raffel, Brian McFee		TCD-TIMIT corpus (mixed-speech)
, Eric J Humphrey, Justin 		TCD-TIMIT corpus (mixed-speech)
Salamon, Oriol Nieto, Dawen Liang, 		TCD-TIMIT corpus (mixed-speech)
Daniel PW Ellis, and C 		TCD-TIMIT corpus (mixed-speech)
Colin Raffel, “mir eval: A 		TCD-TIMIT corpus (mixed-speech)
transparent implementation of common mir 		TCD-TIMIT corpus (mixed-speech)
metrics,” in In Proceed- ings 		TCD-TIMIT corpus (mixed-speech)
of the 15th International Society 		TCD-TIMIT corpus (mixed-speech)
for Music Information Retrieval Conference, 		TCD-TIMIT corpus (mixed-speech)
ISMIR. Citeseer, 2014.  [29] A.W. Rix, J.G. Beerends		TCD-TIMIT corpus (mixed-speech)
, M.P. Hollier, and A.P. 		TCD-TIMIT corpus (mixed-speech)
Hekstra, “Perceptual evaluation of speech 		TCD-TIMIT corpus (mixed-speech)
quality (PESQ)-a new method for 		TCD-TIMIT corpus (mixed-speech)
speech qual- ity assessment of 		TCD-TIMIT corpus (mixed-speech)
telephone networks and codecs,” in 2001 IEEE In- ternational Conference on		TCD-TIMIT corpus (mixed-speech)
 Acoustics, Speech, and Signal Processing		TCD-TIMIT corpus (mixed-speech)
. Proceedings (Cat. No.01CH37221), Salt 		TCD-TIMIT corpus (mixed-speech)
Lake City, UT, USA, 2001, 		TCD-TIMIT corpus (mixed-speech)
vol. 2, pp. 749–752, IEEE.  [30] A. Hines, J. Skoglund		TCD-TIMIT corpus (mixed-speech)
, A. Kokaram, and N. 		TCD-TIMIT corpus (mixed-speech)
Harte, “ViSQOL: The Virtual Speech 		TCD-TIMIT corpus (mixed-speech)
Quality Objective Listener,” in IWAENC 2012		TCD-TIMIT corpus (mixed-speech)
; Inter- national Workshop on 		TCD-TIMIT corpus (mixed-speech)
Acoustic Signal Enhancement, Sept. 2012, 		TCD-TIMIT corpus (mixed-speech)
pp. 1		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
4		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
1  Introduction		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
1.1  Related work		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
2  MODEL ARCHITECTURES		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
2.1  VL2M model		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
2.2  VL2M_ref model		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
2.3  Audio-Visual concat model		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
2.4  Audio-Visual concat-ref model		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
3  Experimental setup		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
3.1  Dataset		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
3.2  LSTM training		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
3.3  Audio pre- and post-processing		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
3.4  Video pre-processing		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
4  Results		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
5  Conclusion		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
6  References		TCD-TIMIT corpus (mixed-speech)
		TCD-TIMIT corpus (mixed-speech)
the limited size GRID and TCD	TCD	TCD-TIMIT corpus (mixed-speech)
on the GRID [9] and TCD	TCD	TCD-TIMIT corpus (mixed-speech)
using the GRID [9] and TCD	TCD	TCD-TIMIT corpus (mixed-speech)
The TCD	TCD	TCD-TIMIT corpus (mixed-speech)
difference. Con- trary to GRID, TCD	TCD	TCD-TIMIT corpus (mixed-speech)
TCD	TCD	TCD-TIMIT corpus (mixed-speech)
Table 3. TCD	TCD	TCD-TIMIT corpus (mixed-speech)
and tested on GRID and TCD	TCD	TCD-TIMIT corpus (mixed-speech)
results on the GRID and TCD	TCD	TCD-TIMIT corpus (mixed-speech)
the limited size GRID and TCD	TCD	TCD-TIMIT corpus (mixed-speech)
TCD	TCD	TCD-TIMIT corpus (mixed-speech)
the limited size GRID and TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
on the GRID [9] and TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
using the GRID [9] and TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
The TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
difference. Con- trary to GRID, TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
Table 3. TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
and tested on GRID and TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
results on the GRID and TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
the limited size GRID and TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
TCD	TIMIT corpus (mixed	TCD-TIMIT corpus (mixed-speech)
The TCD-TIMIT corpus consists of 59 speakers (we	TCD-TIMIT corpus	TCD-TIMIT corpus (mixed-speech)
are applied to the acoustic mixed-speech spectrogram. Results show that: (i	mixed-speech	GRID corpus (mixed-speech)
landmark features and the input mixed-speech spectrogram	mixed-speech	GRID corpus (mixed-speech)
applied to clean the acoustic mixed-speech spectrogram	mixed-speech	GRID corpus (mixed-speech)
compressed spectrogram of the single-channel mixed-speech signal. All of them perform	mixed-speech	GRID corpus (mixed-speech)
although IAM generation requires the mixed-speech spectrogram, separate spectrograms for each	mixed-speech	GRID corpus (mixed-speech)
of them, we created a mixed-speech version	mixed-speech	GRID corpus (mixed-speech)
98 utterances per speaker. The mixed-speech version was created following the	mixed-speech	GRID corpus (mixed-speech)
metric values of the input mixed-speech signal	mixed-speech	GRID corpus (mixed-speech)
speaker’s spectrogram from the acoustic mixed-speech spectrogram	mixed-speech	GRID corpus (mixed-speech)
are applied to the acoustic mixed-speech spectrogram. Results show that: (i	(mixed-speech)	GRID corpus (mixed-speech)
landmark features and the input mixed-speech spectrogram	(mixed-speech)	GRID corpus (mixed-speech)
applied to clean the acoustic mixed-speech spectrogram	(mixed-speech)	GRID corpus (mixed-speech)
compressed spectrogram of the single-channel mixed-speech signal. All of them perform	(mixed-speech)	GRID corpus (mixed-speech)
although IAM generation requires the mixed-speech spectrogram, separate spectrograms for each	(mixed-speech)	GRID corpus (mixed-speech)
of them, we created a mixed-speech version	(mixed-speech)	GRID corpus (mixed-speech)
98 utterances per speaker. The mixed-speech version was created following the	(mixed-speech)	GRID corpus (mixed-speech)
metric values of the input mixed-speech signal	(mixed-speech)	GRID corpus (mixed-speech)
speaker’s spectrogram from the acoustic mixed-speech spectrogram	(mixed-speech)	GRID corpus (mixed-speech)
Regarding the GRID corpus, for each of the 33	corpus	GRID corpus (mixed-speech)
The TCD-TIMIT corpus consists of 59 speakers (we	corpus	GRID corpus (mixed-speech)
speaker-dependent models on the GRID corpus with landmark motion vectors. Results	corpus	GRID corpus (mixed-speech)
and Xu Shao, “An audio-visual corpus for speech perception and automatic	corpus	GRID corpus (mixed-speech)
Regarding the GRID corpus, for each of the 33	speech)	GRID corpus (mixed-speech)
The TCD-TIMIT corpus consists of 59 speakers (we	speech)	GRID corpus (mixed-speech)
speaker-dependent models on the GRID corpus with landmark motion vectors. Results	speech)	GRID corpus (mixed-speech)
and Xu Shao, “An audio-visual corpus for speech perception and automatic	speech)	GRID corpus (mixed-speech)
Regarding the GRID corpus, for each of the 33	GRID corpus	GRID corpus (mixed-speech)
of speaker-dependent models on the GRID corpus with landmark motion vectors. Results	GRID corpus	GRID corpus (mixed-speech)
FACE LANDMARK-BASED SPEAKER-INDEPENDENT AUDIO-VISUAL SPEECH 		GRID corpus (mixed-speech)
ENHANCEMENT IN MULTI-TALKER ENVIRONMENTS  Giovanni Morrone? Luca Pasa† Vadim		GRID corpus (mixed-speech)
 Tikhanoff		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
Sonia Bergamaschi? Luciano Fadiga† Leonardo 		GRID corpus (mixed-speech)
Badino†  ?Department of Engineering ”Enzo Ferrari		GRID corpus (mixed-speech)
”, University of Modena and 		GRID corpus (mixed-speech)
Reggio Emilia, Modena, Italy †Istituto 		GRID corpus (mixed-speech)
Italiano di Tecnologia, Ferrara, Italy  ABSTRACT		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
In this paper, we address 		GRID corpus (mixed-speech)
the problem of enhancing the 		GRID corpus (mixed-speech)
speech of a speaker of 		GRID corpus (mixed-speech)
interest in a cocktail party 		GRID corpus (mixed-speech)
scenario when vi- sual information 		GRID corpus (mixed-speech)
of the speaker of interest 		GRID corpus (mixed-speech)
is available.  Contrary to most previous studies		GRID corpus (mixed-speech)
, we do not learn 		GRID corpus (mixed-speech)
visual features on the typically 		GRID corpus (mixed-speech)
small audio-visual datasets, but use 		GRID corpus (mixed-speech)
an already available face landmark 		GRID corpus (mixed-speech)
detector (trained on a sep- 		GRID corpus (mixed-speech)
arate image dataset).  The landmarks are used by		GRID corpus (mixed-speech)
 LSTM-based models to gen- erate		GRID corpus (mixed-speech)
 time-frequency masks which are applied		GRID corpus (mixed-speech)
 to the acoustic mixed-speech spectrogram		GRID corpus (mixed-speech)
. Results show that: (i) 		GRID corpus (mixed-speech)
land- mark motion features are 		GRID corpus (mixed-speech)
very effective features for this 		GRID corpus (mixed-speech)
task, (ii) similarly to previous 		GRID corpus (mixed-speech)
work, reconstruction of the target 		GRID corpus (mixed-speech)
speaker’s spectrogram mediated by masking 		GRID corpus (mixed-speech)
is significantly more accurate than 		GRID corpus (mixed-speech)
direct spectrogram reconstruction, and (iii) 		GRID corpus (mixed-speech)
the best masks depend on 		GRID corpus (mixed-speech)
both motion landmark features and 		GRID corpus (mixed-speech)
the input mixed-speech spectrogram.  To the best of our		GRID corpus (mixed-speech)
 knowledge, our proposed models are		GRID corpus (mixed-speech)
 the first models trained and		GRID corpus (mixed-speech)
 evaluated on the limited size		GRID corpus (mixed-speech)
 GRID and TCD-TIMIT datasets, that		GRID corpus (mixed-speech)
 achieve speaker-independent speech enhancement in		GRID corpus (mixed-speech)
 a multi-talker setting		GRID corpus (mixed-speech)
.  Index Terms— audio-visual speech enhancement		GRID corpus (mixed-speech)
, cock- tail party problem, 		GRID corpus (mixed-speech)
time-frequency mask, LSTM, face land- 		GRID corpus (mixed-speech)
marks  1. INTRODUCTION		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
In the context of speech 		GRID corpus (mixed-speech)
perception, the cocktail party 		GRID corpus (mixed-speech)
effect [1, 2] is the 		GRID corpus (mixed-speech)
ability of the brain to 		GRID corpus (mixed-speech)
recognize speech in complex and 		GRID corpus (mixed-speech)
adverse listening conditions where the 		GRID corpus (mixed-speech)
attended speech is mixed with 		GRID corpus (mixed-speech)
competing sounds/speech.  Speech perception studies have shown		GRID corpus (mixed-speech)
 that watching speaker’s face movements		GRID corpus (mixed-speech)
 could dramatically improve our ability		GRID corpus (mixed-speech)
 at recognizing the speech of		GRID corpus (mixed-speech)
 a target speaker in a		GRID corpus (mixed-speech)
 multi-talker environment [3, 4		GRID corpus (mixed-speech)
].  This work aims at extracting		GRID corpus (mixed-speech)
 the speech of a target		GRID corpus (mixed-speech)
 speaker from single channel audio		GRID corpus (mixed-speech)
 of several people talking simulta		GRID corpus (mixed-speech)
- neously. This is an 		GRID corpus (mixed-speech)
ill-posed problem in that many 		GRID corpus (mixed-speech)
differ- ent hypotheses about what 		GRID corpus (mixed-speech)
the target speaker says are 		GRID corpus (mixed-speech)
con-  sistent with the mixture signal		GRID corpus (mixed-speech)
. Yet, it can be 		GRID corpus (mixed-speech)
solved by ex- ploiting some 		GRID corpus (mixed-speech)
additional information associated to the 		GRID corpus (mixed-speech)
speaker of interest and/or by 		GRID corpus (mixed-speech)
leveraging some prior knowledge about 		GRID corpus (mixed-speech)
speech signal properties (e.g., [5]). 		GRID corpus (mixed-speech)
In this work we use 		GRID corpus (mixed-speech)
face movements of the target 		GRID corpus (mixed-speech)
speaker as additional information.  This paper (i) proposes the		GRID corpus (mixed-speech)
 use of face landmark’s move		GRID corpus (mixed-speech)
- ments, extracted using 		GRID corpus (mixed-speech)
Dlib [6, 7] and (ii) 		GRID corpus (mixed-speech)
compares differ- ent ways of 		GRID corpus (mixed-speech)
mapping such visual features into 		GRID corpus (mixed-speech)
time-frequency (T-F) masks, then applied 		GRID corpus (mixed-speech)
to clean the acoustic mixed-speech 		GRID corpus (mixed-speech)
spectrogram.  By using Dlib extracted landmarks		GRID corpus (mixed-speech)
 we relieve our mod- els		GRID corpus (mixed-speech)
 from the task of learning		GRID corpus (mixed-speech)
 useful visual features from raw		GRID corpus (mixed-speech)
 pixels. That aspect is particularly		GRID corpus (mixed-speech)
 relevant when the training audio-visual		GRID corpus (mixed-speech)
 datasets are small		GRID corpus (mixed-speech)
.  The analysis of landmark-dependent masking		GRID corpus (mixed-speech)
 strategies is motivated by the		GRID corpus (mixed-speech)
 fact that speech enhancement mediated		GRID corpus (mixed-speech)
 by an explicit masking is		GRID corpus (mixed-speech)
 often more effective than mask-free		GRID corpus (mixed-speech)
 enhancement [8		GRID corpus (mixed-speech)
].  All our models were trained		GRID corpus (mixed-speech)
 and evaluated on the GRID		GRID corpus (mixed-speech)
 [9] and TCD-TIMIT [10] datasets		GRID corpus (mixed-speech)
 in a speaker-independent setting		GRID corpus (mixed-speech)
.  1.1. Related work		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
Speech enhancement aims at extracting 		GRID corpus (mixed-speech)
the voice of a tar- 		GRID corpus (mixed-speech)
get speaker, while speech separation 		GRID corpus (mixed-speech)
refers to the problem of 		GRID corpus (mixed-speech)
separating each sound source in 		GRID corpus (mixed-speech)
a mixture. Recently pro- posed 		GRID corpus (mixed-speech)
audio-only single-channel methods have achieved 		GRID corpus (mixed-speech)
very promising results [11, 12, 13		GRID corpus (mixed-speech)
]. However the task still 		GRID corpus (mixed-speech)
remains challenging. Additionally, audio-only systems 		GRID corpus (mixed-speech)
need separate models in order 		GRID corpus (mixed-speech)
to associate the estimated separated 		GRID corpus (mixed-speech)
audio sources to each speaker, 		GRID corpus (mixed-speech)
while vision easily allow that 		GRID corpus (mixed-speech)
in a unified model.  Regarding audio-visual speech enhancement and		GRID corpus (mixed-speech)
 separa- tion methods an extensive		GRID corpus (mixed-speech)
 review is provided in [14		GRID corpus (mixed-speech)
]. Here we focus on 		GRID corpus (mixed-speech)
the deep-learning methods that are 		GRID corpus (mixed-speech)
most related to the present 		GRID corpus (mixed-speech)
work.  Our first architecture (Section 2.1		GRID corpus (mixed-speech)
) is inspired by [15], 		GRID corpus (mixed-speech)
where a pre-trained convolutional neural 		GRID corpus (mixed-speech)
network (CNN) is used to 		GRID corpus (mixed-speech)
generate a clean spectrogram from 		GRID corpus (mixed-speech)
silent video [16]. Rather than 		GRID corpus (mixed-speech)
directly computing a time-frequency (T-F) 		GRID corpus (mixed-speech)
mask,  ar X		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
iv :1  81 1		GRID corpus (mixed-speech)
.  02 48		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
0v 3		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
cs  .C L		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
2		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
M 		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
2 01  9		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
the mask is computed by 		GRID corpus (mixed-speech)
thresholding the estimated clean spectrogram. 		GRID corpus (mixed-speech)
This approach is not very 		GRID corpus (mixed-speech)
effective since the pre-trained CNN 		GRID corpus (mixed-speech)
is designed for a different 		GRID corpus (mixed-speech)
task (video-to- speech synthesis). 		GRID corpus (mixed-speech)
In [17] a CNN is 		GRID corpus (mixed-speech)
trained to directly esti- mate 		GRID corpus (mixed-speech)
clean speech from noisy audio 		GRID corpus (mixed-speech)
and input video. A sim- 		GRID corpus (mixed-speech)
ilar model is used 		GRID corpus (mixed-speech)
in [18], where the model 		GRID corpus (mixed-speech)
jointly generates clean speech and 		GRID corpus (mixed-speech)
input video in a denoising-autoender 		GRID corpus (mixed-speech)
archi- tecture.  [19] shows that using information		GRID corpus (mixed-speech)
 about lip positions can help		GRID corpus (mixed-speech)
 to improve speech enhancement. The		GRID corpus (mixed-speech)
 video feature vec- tor is		GRID corpus (mixed-speech)
 obtained computing pair-wise distances between		GRID corpus (mixed-speech)
 any mouth landmarks. Similarly to		GRID corpus (mixed-speech)
 our approach their visual fea		GRID corpus (mixed-speech)
- tures are not learned 		GRID corpus (mixed-speech)
on the audio-visual dataset but 		GRID corpus (mixed-speech)
are pro- vided by a 		GRID corpus (mixed-speech)
system trained on different dataset. 		GRID corpus (mixed-speech)
Contrary to our approach, [19] 		GRID corpus (mixed-speech)
uses position-based features while we 		GRID corpus (mixed-speech)
use motion features (of the 		GRID corpus (mixed-speech)
whole face) that in our 		GRID corpus (mixed-speech)
experiments turned out to be 		GRID corpus (mixed-speech)
much more effective than positional 		GRID corpus (mixed-speech)
features.  Although the aforementioned audio-visual methods		GRID corpus (mixed-speech)
 work well, they have only		GRID corpus (mixed-speech)
 been evaluated in a speaker-dependent		GRID corpus (mixed-speech)
 setting. Only the availability of		GRID corpus (mixed-speech)
 new large and heterogeneous audio-visual		GRID corpus (mixed-speech)
 datasets has allowed the training		GRID corpus (mixed-speech)
 of deep neu- ral network-based		GRID corpus (mixed-speech)
 speaker-independent speech enhancement models [20		GRID corpus (mixed-speech)
, 21, 22].  The present work shows that		GRID corpus (mixed-speech)
 huge audio-visual datasets are not		GRID corpus (mixed-speech)
 a necessary requirement for speaker-independent		GRID corpus (mixed-speech)
 audio-visual speech enhancement. Although we		GRID corpus (mixed-speech)
 have only considered datasets with		GRID corpus (mixed-speech)
 simple visual scenarios (i.e., the		GRID corpus (mixed-speech)
 target speaker is always facing		GRID corpus (mixed-speech)
 the camera), we expect our		GRID corpus (mixed-speech)
 methods to perform well in		GRID corpus (mixed-speech)
 more complex scenarios thanks to		GRID corpus (mixed-speech)
 the robust landmark extraction		GRID corpus (mixed-speech)
.  2. MODEL ARCHITECTURES		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
We experimented with the four 		GRID corpus (mixed-speech)
models shown in Fig. 1. 		GRID corpus (mixed-speech)
All models receive in input 		GRID corpus (mixed-speech)
the target speaker’s landmark mo- 		GRID corpus (mixed-speech)
tion vectors and the power-law 		GRID corpus (mixed-speech)
compressed spectrogram of the single-channel 		GRID corpus (mixed-speech)
mixed-speech signal. All of them 		GRID corpus (mixed-speech)
perform some kind of masking 		GRID corpus (mixed-speech)
operation.  2.1. VL2M model		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
At each time frame, the 		GRID corpus (mixed-speech)
video-landmark to mask (VL2M) model (		GRID corpus (mixed-speech)
Fig. 1a) estimates a T-F 		GRID corpus (mixed-speech)
mask from visual features only (		GRID corpus (mixed-speech)
of the target speaker). Formally, 		GRID corpus (mixed-speech)
given a video sequence 		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
 = [v1		GRID corpus (mixed-speech)
, . . . , 		GRID corpus (mixed-speech)
vT ], vt ∈ Rn 		GRID corpus (mixed-speech)
and a target mask sequence 		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
 = [m1		GRID corpus (mixed-speech)
, . . . ,		GRID corpus (mixed-speech)
mT ], mt ∈ Rd, 		GRID corpus (mixed-speech)
VL2M perform a function 		GRID corpus (mixed-speech)
Fvl2m(v) = m̂, where m̂ 		GRID corpus (mixed-speech)
is the estimated mask.  The training objective for VL2M		GRID corpus (mixed-speech)
 is a Target Binary Mask		GRID corpus (mixed-speech)
 (TBM) [23, 24], computed using		GRID corpus (mixed-speech)
 the spectrogram of the tar		GRID corpus (mixed-speech)
- get speaker only. This 		GRID corpus (mixed-speech)
is motivated by our goal 		GRID corpus (mixed-speech)
of extracting the speech of 		GRID corpus (mixed-speech)
a target speaker as much 		GRID corpus (mixed-speech)
as possible indepen- dently of 		GRID corpus (mixed-speech)
the concurrent speakers, so that, 		GRID corpus (mixed-speech)
e.g., we do not need 		GRID corpus (mixed-speech)
to estimate their number. An 		GRID corpus (mixed-speech)
additional motivations is that the 		GRID corpus (mixed-speech)
model takes as only input 		GRID corpus (mixed-speech)
the visual features of the  target speaker, and a target		GRID corpus (mixed-speech)
 TBM that only depends on		GRID corpus (mixed-speech)
 the target speaker allows VL2M		GRID corpus (mixed-speech)
 to learn a function (rather		GRID corpus (mixed-speech)
 than approximating an ill-posed one-to-many		GRID corpus (mixed-speech)
 mapping		GRID corpus (mixed-speech)
).  Given a clean speech spectrogram		GRID corpus (mixed-speech)
 of a speaker s		GRID corpus (mixed-speech)
 = [s1		GRID corpus (mixed-speech)
, . . . , 		GRID corpus (mixed-speech)
sT ], st ∈ Rd, 		GRID corpus (mixed-speech)
the TBM is defined by 		GRID corpus (mixed-speech)
comparing, at each frequency bin 		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
 ∈ [1		GRID corpus (mixed-speech)
, . . . , 		GRID corpus (mixed-speech)
d], the target speaker value 		GRID corpus (mixed-speech)
st[f ] vs. a reference 		GRID corpus (mixed-speech)
threshold τ [f ]. As 		GRID corpus (mixed-speech)
in [15], we use a 		GRID corpus (mixed-speech)
function of long-term average speech 		GRID corpus (mixed-speech)
spectrum (LTASS) as reference threshold. 		GRID corpus (mixed-speech)
This threshold indicates if a 		GRID corpus (mixed-speech)
T-F unit is generated by 		GRID corpus (mixed-speech)
the speaker or refers to 		GRID corpus (mixed-speech)
silence or noise. The process 		GRID corpus (mixed-speech)
to compute the speaker’s TBM 		GRID corpus (mixed-speech)
is as follows:  1. The mean π[f		GRID corpus (mixed-speech)
 ] and the standard deviation		GRID corpus (mixed-speech)
 σ[f ] are computed for		GRID corpus (mixed-speech)
 all frequency bins of all		GRID corpus (mixed-speech)
 seen spectro- grams in speaker’s		GRID corpus (mixed-speech)
 data		GRID corpus (mixed-speech)
.  2. The threshold τ [f		GRID corpus (mixed-speech)
 ] is defined as τ		GRID corpus (mixed-speech)
 [f ] = π[f ]+0.6		GRID corpus (mixed-speech)
 ·σ[f ] where 0.6 is		GRID corpus (mixed-speech)
 a value selected by manual		GRID corpus (mixed-speech)
 inspection of several spectrogram-TBM pairs		GRID corpus (mixed-speech)
.  3. The threshold is applied		GRID corpus (mixed-speech)
 to every speaker’s speech spec		GRID corpus (mixed-speech)
- trogram s.  mt[f		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
1, if st[f ] ≥ 		GRID corpus (mixed-speech)
τ [f ], 0, otherwise.  The mapping Fvl2m(·) is carried		GRID corpus (mixed-speech)
 out by a stacked bi		GRID corpus (mixed-speech)
- directional Long Short-Term Memory (		GRID corpus (mixed-speech)
BLSTM) network [25]. The BLSTM 		GRID corpus (mixed-speech)
outputs are then forced to 		GRID corpus (mixed-speech)
lay within the [0, 1] 		GRID corpus (mixed-speech)
range. Finally the computed TBM 		GRID corpus (mixed-speech)
m̂ and the noisy spectrogram 		GRID corpus (mixed-speech)
y are element-wise multiplied to 		GRID corpus (mixed-speech)
ob- tain the estimated clean 		GRID corpus (mixed-speech)
spectrogram ŝm = m̂ ◦ 		GRID corpus (mixed-speech)
y, where 		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
 = [y1		GRID corpus (mixed-speech)
, . . . 		GRID corpus (mixed-speech)
yT ], yt ∈ Rd.  The model parameters are estimated		GRID corpus (mixed-speech)
 to minimize the loss		GRID corpus (mixed-speech)
:  Jvl2m = ∑T		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
t=1  ∑d f=1−mt[f ] · log(m̂t[f		GRID corpus (mixed-speech)
 ])− (1−mt[f ]) · log(1		GRID corpus (mixed-speech)
− m̂t[f ])  2.2. VL2M ref model		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
VL2M generates T-F masks that 		GRID corpus (mixed-speech)
are independent of the acous- 		GRID corpus (mixed-speech)
tic context. We may want 		GRID corpus (mixed-speech)
to refine the masking by 		GRID corpus (mixed-speech)
including such context. This is 		GRID corpus (mixed-speech)
what the novel VL2M ref 		GRID corpus (mixed-speech)
does (Fig. 1b). The computed 		GRID corpus (mixed-speech)
TBM m̂ and the input 		GRID corpus (mixed-speech)
spectrogram y are the input 		GRID corpus (mixed-speech)
to a function that outputs 		GRID corpus (mixed-speech)
an Ideal Amplitude Mask (IAM) 		GRID corpus (mixed-speech)
p (known as FFT-MASK 		GRID corpus (mixed-speech)
in [8]). Given the target 		GRID corpus (mixed-speech)
clean spectrogram s and the 		GRID corpus (mixed-speech)
noisy spectrogram y, the IAM 		GRID corpus (mixed-speech)
is defined as:  pt[f ] = st[f		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
yt[f ]  Note that although IAM generation		GRID corpus (mixed-speech)
 requires the mixed-speech spectrogram, separate		GRID corpus (mixed-speech)
 spectrograms for each concurrent speakers		GRID corpus (mixed-speech)
 are not required		GRID corpus (mixed-speech)
.  The target speaker’s spectrogram s		GRID corpus (mixed-speech)
 is reconstructed by multiplying the		GRID corpus (mixed-speech)
 input spectrogram with the estimated		GRID corpus (mixed-speech)
 IAM. Values greater than 10		GRID corpus (mixed-speech)
 in the IAM are clipped		GRID corpus (mixed-speech)
 to 10 in order to		GRID corpus (mixed-speech)
 obtain better numerical stability as		GRID corpus (mixed-speech)
 suggested in [8		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
v: video input y: noisy 		GRID corpus (mixed-speech)
spectrogram sm: clean spectrogram TBM 		GRID corpus (mixed-speech)
s: clean spectrogram IAM m: 		GRID corpus (mixed-speech)
TBM p: IAM  STACKED		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
BLSTM  m		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
sm  v		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
y  (a) VL2M		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
v VL2M m  y BLSTM		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
BLSTM  Fusion layer		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
BLSTM p  s		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
b) VL2M ref  v		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
y  p STACKED		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
BLSTM  s		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
c) Audio-Visual concat  sm		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
y  p STACKED		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
BLSTM  s		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
v VL2M m  (d) Audio-Visual concat-ref		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
Fig. 1. Model architectures.  The model performs a function		GRID corpus (mixed-speech)
 Fmr(v, y) = p̂ that		GRID corpus (mixed-speech)
 con- sists of a VL2M		GRID corpus (mixed-speech)
 component plus three different BLSTMs		GRID corpus (mixed-speech)
 Gm, Gy and H		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
Gm(Fvl2m(v)) = rm receives the 		GRID corpus (mixed-speech)
VL2M mask m̂ as in- 		GRID corpus (mixed-speech)
put, and Gy(y) = ry 		GRID corpus (mixed-speech)
is fed with the noisy 		GRID corpus (mixed-speech)
spectrogram. Their output rm, 		GRID corpus (mixed-speech)
ry ∈ Rz are fused 		GRID corpus (mixed-speech)
in a joint audio-visual represen- 		GRID corpus (mixed-speech)
tation 		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
 = [h1		GRID corpus (mixed-speech)
, . . . ,		GRID corpus (mixed-speech)
hT ], where ht is 		GRID corpus (mixed-speech)
a linear combination of rmt 		GRID corpus (mixed-speech)
and ryt : ht = 		GRID corpus (mixed-speech)
Whm ·rmt +Why ·ryt +bh. 		GRID corpus (mixed-speech)
h is the input of 		GRID corpus (mixed-speech)
the third BLSTM H (		GRID corpus (mixed-speech)
h) = p̂, where p̂ 		GRID corpus (mixed-speech)
lays in the [0,10] range. 		GRID corpus (mixed-speech)
The loss function is:  Jmr		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
T∑ t=1  d∑ f=1		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
p̂t[f ] · yt[f ]− 		GRID corpus (mixed-speech)
st[f ])2  2.3. Audio-Visual concat model		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
The third model (Fig. 1c) 		GRID corpus (mixed-speech)
performs early fusion of audio- 		GRID corpus (mixed-speech)
visual features. This model consists 		GRID corpus (mixed-speech)
of a single stacked BLSTM 		GRID corpus (mixed-speech)
that computes the IAM mask 		GRID corpus (mixed-speech)
p̂ from the concate- 		GRID corpus (mixed-speech)
nated [v,y]. The training loss 		GRID corpus (mixed-speech)
is the same Jmr used 		GRID corpus (mixed-speech)
to train VL2M ref. This 		GRID corpus (mixed-speech)
model can be regarded as 		GRID corpus (mixed-speech)
a simplification of VL2M ref, 		GRID corpus (mixed-speech)
where the VL2M operation is 		GRID corpus (mixed-speech)
not performed.  2.4. Audio-Visual concat-ref model		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
The fourth model (Fig. 1d) 		GRID corpus (mixed-speech)
is an improved version of 		GRID corpus (mixed-speech)
the model described in section 2		GRID corpus (mixed-speech)
.3. The only difference is 		GRID corpus (mixed-speech)
the input of the stacked 		GRID corpus (mixed-speech)
BLSTM that is replaced 		GRID corpus (mixed-speech)
by [̂sm,y] where ŝm is 		GRID corpus (mixed-speech)
the denoised spectrogram returned by 		GRID corpus (mixed-speech)
VL2M operation.  3. EXPERIMENTAL SETUP		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
3.1. Dataset  All experiments were carried out		GRID corpus (mixed-speech)
 using the GRID [9] and		GRID corpus (mixed-speech)
 TCD-TIMIT [10] audio-visual datasets. For		GRID corpus (mixed-speech)
 each of them, we created		GRID corpus (mixed-speech)
 a mixed-speech version		GRID corpus (mixed-speech)
.  Regarding the GRID corpus, for		GRID corpus (mixed-speech)
 each of the 33 speakers		GRID corpus (mixed-speech)
 (one had to be discarded		GRID corpus (mixed-speech)
) we first randomly selected 200 ut- terances (out of 1000		GRID corpus (mixed-speech)
). Then, for each utterance, 		GRID corpus (mixed-speech)
we created 3 different audio-mixed 		GRID corpus (mixed-speech)
samples. Each audio-mixed sample was 		GRID corpus (mixed-speech)
created by mixing the chosen 		GRID corpus (mixed-speech)
utterance with one utter- ance 		GRID corpus (mixed-speech)
from a different speaker.  That resulted in 600 audio-mixed		GRID corpus (mixed-speech)
 samples per speaker		GRID corpus (mixed-speech)
.  The resulting dataset was split		GRID corpus (mixed-speech)
 into disjoint sets of 25/4/4		GRID corpus (mixed-speech)
 speakers for training/validation/testing respectively		GRID corpus (mixed-speech)
.  The TCD-TIMIT corpus consists of		GRID corpus (mixed-speech)
 59 speakers (we ex- cluded		GRID corpus (mixed-speech)
 3 professionally-trained lipspeakers) and 98		GRID corpus (mixed-speech)
 utterances per speaker. The mixed-speech		GRID corpus (mixed-speech)
 version was created following the		GRID corpus (mixed-speech)
 same procedure as for GRID		GRID corpus (mixed-speech)
, with one difference. Con- 		GRID corpus (mixed-speech)
trary to GRID, TCD-TIMIT utterances 		GRID corpus (mixed-speech)
have different dura- tion. Thus 2 utterances were mixed only if		GRID corpus (mixed-speech)
 their duration dif- ference did		GRID corpus (mixed-speech)
 not exceed 2 seconds. For		GRID corpus (mixed-speech)
 each utterance pair, we forced		GRID corpus (mixed-speech)
 the non-target speaker’s utterance to		GRID corpus (mixed-speech)
 match the du- ration of		GRID corpus (mixed-speech)
 the target speaker utterance. If		GRID corpus (mixed-speech)
 it was longer, the utterance		GRID corpus (mixed-speech)
 was cut at its end		GRID corpus (mixed-speech)
, whereas if it was 		GRID corpus (mixed-speech)
shorter, silence samples were equally 		GRID corpus (mixed-speech)
added at its start and 		GRID corpus (mixed-speech)
end.  The resulting dataset was split		GRID corpus (mixed-speech)
 into disjoint sets of 51/4/4		GRID corpus (mixed-speech)
 speakers for training/validation/testing respectively		GRID corpus (mixed-speech)
.  3.2. LSTM training		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
In all experiments, the models 		GRID corpus (mixed-speech)
were trained using the Adam 		GRID corpus (mixed-speech)
optimizer [26]. Early stopping was 		GRID corpus (mixed-speech)
applied when the error on 		GRID corpus (mixed-speech)
the validation set did not 		GRID corpus (mixed-speech)
decrease over 5 consecutive epochs.  VL2M, AV concat and AV		GRID corpus (mixed-speech)
 concat-ref had 5, 3 and		GRID corpus (mixed-speech)
 3 stacked BLSTM layers respectively		GRID corpus (mixed-speech)
. All BLSTMs had 250 		GRID corpus (mixed-speech)
units. Hyper-parameters selection was performed 		GRID corpus (mixed-speech)
by using random search with 		GRID corpus (mixed-speech)
a limited number of samples, 		GRID corpus (mixed-speech)
therefore all the reported results 		GRID corpus (mixed-speech)
may improve through a deeper 		GRID corpus (mixed-speech)
hyper- parameters validation phase.  VL2M ref and AV concat-ref		GRID corpus (mixed-speech)
 training was performed in 2		GRID corpus (mixed-speech)
 steps. We first pre-trained the		GRID corpus (mixed-speech)
 models using the oracle TBM		GRID corpus (mixed-speech)
 m. Then we substituted the		GRID corpus (mixed-speech)
 oracle masks with the VL2M		GRID corpus (mixed-speech)
 component and retrained the models		GRID corpus (mixed-speech)
 while freezing the pa- rameters		GRID corpus (mixed-speech)
 of the VL2M component		GRID corpus (mixed-speech)
.  3.3. Audio pre- and post-processing		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
The original waveforms were resampled 		GRID corpus (mixed-speech)
to 16 kHz. Short- Time 		GRID corpus (mixed-speech)
Fourier Transform (STFT) x was 		GRID corpus (mixed-speech)
computed using FFT size of 512, Hann window of length 25		GRID corpus (mixed-speech)
 ms (400 samples), and hop		GRID corpus (mixed-speech)
 length of 10 ms (160		GRID corpus (mixed-speech)
 samples). The input spectro- gram		GRID corpus (mixed-speech)
 was obtained taking the STFT		GRID corpus (mixed-speech)
 magnitude and perform- ing power-law		GRID corpus (mixed-speech)
 compression |x|p with p		GRID corpus (mixed-speech)
 = 0.3. Finally we applied		GRID corpus (mixed-speech)
 per-speaker 0-mean 1-std normalization		GRID corpus (mixed-speech)
.  In the post-processing stage, the		GRID corpus (mixed-speech)
 enhanced waveform gen- erated by		GRID corpus (mixed-speech)
 the speech enhancement models was		GRID corpus (mixed-speech)
 reconstructed		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
SDR PESQ ViSQOL  Noisy −1.06 1.81 2.11 VL2M		GRID corpus (mixed-speech)
 3.17 1.51 1.16 VL2M ref		GRID corpus (mixed-speech)
 6.50 2.58 2.99 AV concat		GRID corpus (mixed-speech)
 6.31 2.49 2.83 AV c-ref		GRID corpus (mixed-speech)
 6.17 2.58 2.96		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
Table 1. GRID results - 		GRID corpus (mixed-speech)
speaker-dependent. The “Noisy” row refers 		GRID corpus (mixed-speech)
to the metric values of 		GRID corpus (mixed-speech)
the input mixed-speech signal.  2 Speakers 3 Speakers SDR		GRID corpus (mixed-speech)
 PESQ ViSQOL SDR PESQ ViSQOL		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
Noisy 0.21 1.94 2.58 −5.34 1		GRID corpus (mixed-speech)
.43 1.62 VL2M 3.02 1.81 1		GRID corpus (mixed-speech)
.70 −2.03 1.43 1.25 VL2M 		GRID corpus (mixed-speech)
ref 6.52 2.53 3.02 2.83 2		GRID corpus (mixed-speech)
.19 2.53 AV concat 7.37 2		GRID corpus (mixed-speech)
.65 3.03 3.02 2.24 2.49 		GRID corpus (mixed-speech)
AV c-ref 8.05 2.70 3.07 4		GRID corpus (mixed-speech)
.02 2.33 2.64  Table 2. GRID results		GRID corpus (mixed-speech)
 - speaker-independent		GRID corpus (mixed-speech)
.  by applying the inverse STFT		GRID corpus (mixed-speech)
 to the estimated clean spectro		GRID corpus (mixed-speech)
- gram and using the 		GRID corpus (mixed-speech)
phase of the noisy input 		GRID corpus (mixed-speech)
signal.  3.4. Video pre-processing		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
Face landmarks were extracted from 		GRID corpus (mixed-speech)
video using the Dlib [7] 		GRID corpus (mixed-speech)
implementation of the face landmark 		GRID corpus (mixed-speech)
estimator described in [6]. It 		GRID corpus (mixed-speech)
returns 68 x-y points, for 		GRID corpus (mixed-speech)
an overall 136 values. We 		GRID corpus (mixed-speech)
upsampled from 25/29.97 fps (GRID/TCD-TIMIT) 		GRID corpus (mixed-speech)
to 100 fps to match 		GRID corpus (mixed-speech)
the frame rate of the 		GRID corpus (mixed-speech)
audio spectrogram. Upsampling was carried 		GRID corpus (mixed-speech)
out through linear interpolation over 		GRID corpus (mixed-speech)
time.  The final video feature vector		GRID corpus (mixed-speech)
 v was obtained by com		GRID corpus (mixed-speech)
- puting the per-speaker normalized 		GRID corpus (mixed-speech)
motion vector of the face 		GRID corpus (mixed-speech)
landmarks by simply subtracting every 		GRID corpus (mixed-speech)
frame with the previ- ous 		GRID corpus (mixed-speech)
one. The motion vector of 		GRID corpus (mixed-speech)
the first frame was set 		GRID corpus (mixed-speech)
to zero.  4. RESULTS		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
In order to compare our 		GRID corpus (mixed-speech)
models to previous works in 		GRID corpus (mixed-speech)
both speech enhancement and separation, 		GRID corpus (mixed-speech)
we evaluated the perfor- mance 		GRID corpus (mixed-speech)
of the proposed models using 		GRID corpus (mixed-speech)
both speech separation  2 Speakers 3 Speakers SDR		GRID corpus (mixed-speech)
 PESQ ViSQOL SDR PESQ ViSQOL		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
Noisy 0.21 2.22 2.74 −3.42 1		GRID corpus (mixed-speech)
.92 2.04 VL2M 2.88 2.25 2		GRID corpus (mixed-speech)
.62 −0.51 1.99 1.98 VL2M 		GRID corpus (mixed-speech)
ref 9.24 2.81 3.09 5.27 2		GRID corpus (mixed-speech)
.44 2.54 AV concat 9.56 2		GRID corpus (mixed-speech)
.80 3.09 5.15 2.41 2.52 		GRID corpus (mixed-speech)
AV c-ref 10.55 3.03 3.21 5		GRID corpus (mixed-speech)
.37 2.45 2.58  Table 3. TCD-TIMIT results		GRID corpus (mixed-speech)
 - speaker-independent		GRID corpus (mixed-speech)
.  and enhancement metrics. Specifically, we		GRID corpus (mixed-speech)
 measured the ca- pability of		GRID corpus (mixed-speech)
 separating the target utterance from		GRID corpus (mixed-speech)
 the concurrent utterance with the		GRID corpus (mixed-speech)
 source-to-distortion ratio (SDR) [27, 28		GRID corpus (mixed-speech)
]. While the quality of 		GRID corpus (mixed-speech)
estimated target speech was measured 		GRID corpus (mixed-speech)
with the perceptual PESQ [29] 		GRID corpus (mixed-speech)
and ViSQOL [30] metrics. For 		GRID corpus (mixed-speech)
PESQ we used the narrow 		GRID corpus (mixed-speech)
band mode while for ViSQOL 		GRID corpus (mixed-speech)
we used the wide band 		GRID corpus (mixed-speech)
mode.  As a very first experiment		GRID corpus (mixed-speech)
 we compared landmark posi- tion		GRID corpus (mixed-speech)
 vs. landmark motion vectors. It		GRID corpus (mixed-speech)
 turned out that landmark positions		GRID corpus (mixed-speech)
 performed poorly, thus all results		GRID corpus (mixed-speech)
 reported here refer to landmark		GRID corpus (mixed-speech)
 motion vectors only		GRID corpus (mixed-speech)
.  We then carried out some		GRID corpus (mixed-speech)
 speaker-dependent experiments to compare our		GRID corpus (mixed-speech)
 models with previous studies as		GRID corpus (mixed-speech)
, to the best of 		GRID corpus (mixed-speech)
our knowledge, there are no 		GRID corpus (mixed-speech)
reported results of speaker- independent 		GRID corpus (mixed-speech)
systems trained and tested on 		GRID corpus (mixed-speech)
GRID and TCD- TIMIT to 		GRID corpus (mixed-speech)
compare with. Table 1 reports 		GRID corpus (mixed-speech)
the test-set evalua- tion of 		GRID corpus (mixed-speech)
speaker-dependent models on the GRID 		GRID corpus (mixed-speech)
corpus with landmark motion vectors. 		GRID corpus (mixed-speech)
Results are comparable with previ- 		GRID corpus (mixed-speech)
ous state-of-the-art studies in an 		GRID corpus (mixed-speech)
almost identical setting [15, 17].  Table 2 and 3 show		GRID corpus (mixed-speech)
 speaker-independent test-set results on the		GRID corpus (mixed-speech)
 GRID and TCD-TIMIT datasets respectively		GRID corpus (mixed-speech)
. V2ML performs significantly worse 		GRID corpus (mixed-speech)
than the other three models 		GRID corpus (mixed-speech)
in- dicating that a successful 		GRID corpus (mixed-speech)
mask generation has to depend 		GRID corpus (mixed-speech)
on the acoustic context. The 		GRID corpus (mixed-speech)
performance of the three models 		GRID corpus (mixed-speech)
in the speaker-independent setting is 		GRID corpus (mixed-speech)
comparable to that in the 		GRID corpus (mixed-speech)
speaker-dependent setting.  AV concat-ref outperforms V2ML ref		GRID corpus (mixed-speech)
 and AV concat for both		GRID corpus (mixed-speech)
 datasets. This supports the utility		GRID corpus (mixed-speech)
 of a refinement strat- egy		GRID corpus (mixed-speech)
 and suggests that the refinement		GRID corpus (mixed-speech)
 is more effective when it		GRID corpus (mixed-speech)
 directly refines the estimated clean		GRID corpus (mixed-speech)
 spectrogram, rather than refining the		GRID corpus (mixed-speech)
 estimated mask		GRID corpus (mixed-speech)
.  Finally, we evaluated the systems		GRID corpus (mixed-speech)
 in a more challenging testing		GRID corpus (mixed-speech)
 condition where the target utterance		GRID corpus (mixed-speech)
 was mixed with 2 utterances		GRID corpus (mixed-speech)
 from 2 competing speakers. Despite		GRID corpus (mixed-speech)
 the model was trained with		GRID corpus (mixed-speech)
 mixtures of two speakers, the		GRID corpus (mixed-speech)
 decrease of performance was not		GRID corpus (mixed-speech)
 dramatic		GRID corpus (mixed-speech)
.  Code and some testing examples		GRID corpus (mixed-speech)
 of our models are avail		GRID corpus (mixed-speech)
- able at https://goo.gl/3h1NgE.  5. CONCLUSION		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
This paper proposes the use 		GRID corpus (mixed-speech)
of face landmark motion vec- 		GRID corpus (mixed-speech)
tors for audio-visual speech enhancement 		GRID corpus (mixed-speech)
in a single-channel multi-talker scenario. 		GRID corpus (mixed-speech)
Different models are tested where 		GRID corpus (mixed-speech)
land- mark motion vectors are 		GRID corpus (mixed-speech)
used to generate time-frequency (T- 		GRID corpus (mixed-speech)
F) masks that extract the 		GRID corpus (mixed-speech)
target speaker’s spectrogram from the 		GRID corpus (mixed-speech)
acoustic mixed-speech spectrogram.  To the best of our		GRID corpus (mixed-speech)
 knowledge, some of the proposed		GRID corpus (mixed-speech)
 mod- els are the first		GRID corpus (mixed-speech)
 models trained and evaluated on		GRID corpus (mixed-speech)
 the limited size GRID and		GRID corpus (mixed-speech)
 TCD-TIMIT datasets that accomplish speaker		GRID corpus (mixed-speech)
- independent speech enhancement in 		GRID corpus (mixed-speech)
the multi-talker setting, with a 		GRID corpus (mixed-speech)
quality of enhancement comparable to 		GRID corpus (mixed-speech)
that achieved in a speaker-dependent 		GRID corpus (mixed-speech)
setting.  https://goo.gl/3h1NgE		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
6. REFERENCES  [1] E. Colin Cherry, “Some		GRID corpus (mixed-speech)
 experiments on the recognition of		GRID corpus (mixed-speech)
 speech, with one and with		GRID corpus (mixed-speech)
 two ears,” The Journal of		GRID corpus (mixed-speech)
 the Acoustical Society of America		GRID corpus (mixed-speech)
, vol. 25, no. 5, 		GRID corpus (mixed-speech)
pp. 975–979, 1953.  [2] Josh H McDermott, “The		GRID corpus (mixed-speech)
 cocktail party problem,” Current Biology		GRID corpus (mixed-speech)
, vol. 19, no. 22, 		GRID corpus (mixed-speech)
pp. R1024–R1027, 2009.  [3] Elana Zion Golumbic, Gregory		GRID corpus (mixed-speech)
 B. Cogan, Charles E. Schroeder		GRID corpus (mixed-speech)
, and David Poeppel, “Visual 		GRID corpus (mixed-speech)
input enhances selective speech envelope 		GRID corpus (mixed-speech)
tracking in auditory cortex at 		GRID corpus (mixed-speech)
a “cocktail party”,” Journal of 		GRID corpus (mixed-speech)
Neu- roscience, vol. 33, no. 4, pp. 1417–1426, 2013		GRID corpus (mixed-speech)
.  [4] Wei Ji Ma, Xiang		GRID corpus (mixed-speech)
 Zhou, Lars A. Ross, John		GRID corpus (mixed-speech)
 J. Foxe, and Lucas C		GRID corpus (mixed-speech)
. Parra, “Lip-reading aids word 		GRID corpus (mixed-speech)
recognition most in moderate noise: 		GRID corpus (mixed-speech)
A bayesian explanation using high-dimensional 		GRID corpus (mixed-speech)
feature space,” PLOS ONE, vol. 4, no. 3, pp. 1–14, 03		GRID corpus (mixed-speech)
 2009		GRID corpus (mixed-speech)
.  [5] Albert S Bregman, Auditory		GRID corpus (mixed-speech)
 scene analysis: The perceptual organi		GRID corpus (mixed-speech)
- zation of sound, MIT 		GRID corpus (mixed-speech)
press, 1994.  [6] Vahid Kazemi and Josephine		GRID corpus (mixed-speech)
 Sullivan, “One millisecond face align		GRID corpus (mixed-speech)
- ment with an ensemble 		GRID corpus (mixed-speech)
of regression trees,” in The 		GRID corpus (mixed-speech)
IEEE Conference on Computer Vision 		GRID corpus (mixed-speech)
and Pattern Recognition (CVPR), June 2014		GRID corpus (mixed-speech)
.  [7] Davis E. King, “Dlib-ml		GRID corpus (mixed-speech)
: A machine learning toolkit,” 		GRID corpus (mixed-speech)
Journal of Machine Learning Research, 		GRID corpus (mixed-speech)
vol. 10, pp. 1755–1758, 2009.  [8] Yuxuan Wang, Arun Narayanan		GRID corpus (mixed-speech)
, and DeLiang Wang, “On 		GRID corpus (mixed-speech)
Training Targets for Supervised Speech 		GRID corpus (mixed-speech)
Separation,” IEEE/ACM Transactions on Audio, 		GRID corpus (mixed-speech)
Speech, and Language Processing, vol. 22, no. 12, pp. 1849–1858, Dec		GRID corpus (mixed-speech)
. 2014.  [9] Martin Cooke, Jon Barker		GRID corpus (mixed-speech)
, Stuart Cunningham, and Xu 		GRID corpus (mixed-speech)
Shao, “An audio-visual corpus for 		GRID corpus (mixed-speech)
speech perception and automatic speech 		GRID corpus (mixed-speech)
recognition,” The Journal of the 		GRID corpus (mixed-speech)
Acoustical Society of America, vol. 120, no. 5, pp. 2421–2424, Nov		GRID corpus (mixed-speech)
. 2006.  [10] Naomi Harte and Eoin		GRID corpus (mixed-speech)
 Gillen, “TCD-TIMIT: An Audio-Visual Cor		GRID corpus (mixed-speech)
- pus of Continuous Speech,” 		GRID corpus (mixed-speech)
IEEE Transactions on Multimedia, vol. 17, no. 5, pp. 603–615, May		GRID corpus (mixed-speech)
 2015		GRID corpus (mixed-speech)
.  [11] Z. Chen, Y. Luo		GRID corpus (mixed-speech)
, and N. Mesgarani, “Deep 		GRID corpus (mixed-speech)
attractor network for single-microphone speaker 		GRID corpus (mixed-speech)
separation,” in 2017 IEEE International 		GRID corpus (mixed-speech)
Conference on Acoustics, Speech and 		GRID corpus (mixed-speech)
Signal Processing (ICASSP), March 2017, 		GRID corpus (mixed-speech)
pp. 246–250.  [12] Yusuf Isik, Jonathan Le		GRID corpus (mixed-speech)
 Roux, Zhuo Chen, Shinji Watanabe		GRID corpus (mixed-speech)
, and John R. 		GRID corpus (mixed-speech)
Hershey, “Single-channel multi-speaker separation using 		GRID corpus (mixed-speech)
deep clustering,” in Interspeech, 2016.  [13] Morten Kolbaek, Dong Yu		GRID corpus (mixed-speech)
, Zheng-Hua Tan, Jesper Jensen, 		GRID corpus (mixed-speech)
Morten Kolbaek, Dong Yu, Zheng-Hua 		GRID corpus (mixed-speech)
Tan, and Jesper Jensen, “Multitalker 		GRID corpus (mixed-speech)
speech separation with utterance-level permutation 		GRID corpus (mixed-speech)
invariant training of deep recurrent 		GRID corpus (mixed-speech)
neural networks,” IEEE/ACM Trans. Audio, 		GRID corpus (mixed-speech)
Speech and Lang. Proc., vol. 25, no. 10, pp. 1901–1913, Oct		GRID corpus (mixed-speech)
. 2017.  [14] Bertrand Rivet, Wenwu Wang		GRID corpus (mixed-speech)
, Syed Mohsen Naqvi, and 		GRID corpus (mixed-speech)
Jonathon Chambers, “Audiovisual Speech Source 		GRID corpus (mixed-speech)
Separation: An overview of key 		GRID corpus (mixed-speech)
methodologies,” IEEE Signal Processing Magazine, 		GRID corpus (mixed-speech)
vol. 31, no. 3, pp. 125		GRID corpus (mixed-speech)
–134, May 2014.  [15] Aviv Gabbay, Ariel Ephrat		GRID corpus (mixed-speech)
, Tavi Halperin, and Shmuel 		GRID corpus (mixed-speech)
Peleg, “Seeing through noise: Visually 		GRID corpus (mixed-speech)
driven speaker separation and enhancement,” 		GRID corpus (mixed-speech)
in ICASSP. 2018, pp. 3051–3055, 		GRID corpus (mixed-speech)
IEEE.  [16] Ariel Ephrat, Tavi Halperin		GRID corpus (mixed-speech)
, and Shmuel Peleg, “Improved 		GRID corpus (mixed-speech)
speech reconstruction from silent video,” 		GRID corpus (mixed-speech)
ICCV 2017 Workshop on Computer 		GRID corpus (mixed-speech)
Vision for Audio-Visual Media, 2017.  [17] Aviv Gabbay, Asaph Shamir		GRID corpus (mixed-speech)
, and Shmuel Peleg, “Visual 		GRID corpus (mixed-speech)
speech en- hancement,” in Interspeech. 2018, pp. 1170–1174, ISCA		GRID corpus (mixed-speech)
.  [18] Jen-Cheng Hou, Syu-Siang Wang		GRID corpus (mixed-speech)
, Ying-Hui Lai, Yu Tsao, 		GRID corpus (mixed-speech)
Hsiu-Wen Chang, and Hsin-Min 		GRID corpus (mixed-speech)
Wang, “Audio-Visual Speech Enhancement Us- 		GRID corpus (mixed-speech)
ing Multimodal Deep Convolutional Neural 		GRID corpus (mixed-speech)
Networks,” IEEE Trans- actions on 		GRID corpus (mixed-speech)
Emerging Topics in Computational Intelligence, 		GRID corpus (mixed-speech)
vol. 2, no. 2, pp. 117		GRID corpus (mixed-speech)
–128, Apr. 2018.  [19] Jen-Cheng Hou, Syu-Siang Wang		GRID corpus (mixed-speech)
, Ying-Hui Lai, Jen-Chun Lin, 		GRID corpus (mixed-speech)
Yu Tsao, Hsiu-Wen Chang, and 		GRID corpus (mixed-speech)
Hsin-Min Wang, “Audio-visual speech enhancement 		GRID corpus (mixed-speech)
using deep neural networks,” in 2016 Asia- Pacific Signal and Information		GRID corpus (mixed-speech)
 Processing Association Annual Sum- mit		GRID corpus (mixed-speech)
 and Conference (APSIPA), Jeju, South		GRID corpus (mixed-speech)
 Korea, Dec. 2016, pp. 1–6		GRID corpus (mixed-speech)
, IEEE.  [20] Ariel Ephrat, Inbar Mosseri		GRID corpus (mixed-speech)
, Oran Lang, Tali Dekel, 		GRID corpus (mixed-speech)
Kevin Wilson, Avinatan Hassidim, William 		GRID corpus (mixed-speech)
T. Freeman, and Michael 		GRID corpus (mixed-speech)
Rubinstein, “Looking to Listen at 		GRID corpus (mixed-speech)
the Cocktail Party: A Speaker-Independent 		GRID corpus (mixed-speech)
Audio-Visual Model for Speech Separation,” 		GRID corpus (mixed-speech)
ACM Transactions on Graphics, vol. 37, no. 4, pp. 1–11, July		GRID corpus (mixed-speech)
 2018, arXiv: 1804.03619		GRID corpus (mixed-speech)
.  [21] T. Afouras, J. S		GRID corpus (mixed-speech)
. Chung, and A. 		GRID corpus (mixed-speech)
Zisserman, “The conversation: Deep audio-visual 		GRID corpus (mixed-speech)
speech enhancement,” in Interspeech, 2018.  [22] Andrew Owens and Alexei		GRID corpus (mixed-speech)
 A Efros, “Audio-visual scene analysis		GRID corpus (mixed-speech)
 with self-supervised multisensory features,” European		GRID corpus (mixed-speech)
 Conference on Computer Vision (ECCV		GRID corpus (mixed-speech)
), 2018.  [23] Michael C. Anzalone, Lauren		GRID corpus (mixed-speech)
 Calandruccio, Karen A. Doherty, and		GRID corpus (mixed-speech)
 Laurel H. Carney, “Determination of		GRID corpus (mixed-speech)
 the potential benefit of time		GRID corpus (mixed-speech)
- frequency gain manipulation,” Ear 		GRID corpus (mixed-speech)
Hear, vol. 27, no. 5, 		GRID corpus (mixed-speech)
pp. 480–492, Oct 2006, 16957499[pmid].  [24] Ulrik Kjems, Jesper B		GRID corpus (mixed-speech)
. Boldt, Michael S. Pedersen, 		GRID corpus (mixed-speech)
Thomas Lunner, and DeLiang 		GRID corpus (mixed-speech)
Wang, “Role of mask pattern 		GRID corpus (mixed-speech)
in intelligibility of ideal binary-masked 		GRID corpus (mixed-speech)
noisy speech,” The Journal of 		GRID corpus (mixed-speech)
the Acoustical Society of America, 		GRID corpus (mixed-speech)
vol. 126, no. 3, pp. 1415		GRID corpus (mixed-speech)
–1426, 2009.  [25] A. Graves, A. Mohamed		GRID corpus (mixed-speech)
, and G. Hinton, “Speech 		GRID corpus (mixed-speech)
recognition with deep recurrent neural 		GRID corpus (mixed-speech)
networks,” in 2013 IEEE International 		GRID corpus (mixed-speech)
Con- ference on Acoustics, Speech 		GRID corpus (mixed-speech)
and Signal Processing, May 2013, 		GRID corpus (mixed-speech)
pp. 6645–6649.  [26] Diederik P Kingma and		GRID corpus (mixed-speech)
 Jimmy Ba, “Adam: A method		GRID corpus (mixed-speech)
 for stochastic optimization,” arXiv preprint		GRID corpus (mixed-speech)
 arXiv:1412.6980, 2014		GRID corpus (mixed-speech)
.  [27] E. Vincent, R. Gribonval		GRID corpus (mixed-speech)
, and C. Fevotte, “Performance 		GRID corpus (mixed-speech)
measure- ment in blind audio 		GRID corpus (mixed-speech)
source separation,” IEEE Transactions on 		GRID corpus (mixed-speech)
Audio, Speech and Language Processing, 		GRID corpus (mixed-speech)
vol. 14, no. 4, pp. 1462		GRID corpus (mixed-speech)
–1469, July 2006.  [28] Colin Raffel, Brian McFee		GRID corpus (mixed-speech)
, Eric J Humphrey, Justin 		GRID corpus (mixed-speech)
Salamon, Oriol Nieto, Dawen Liang, 		GRID corpus (mixed-speech)
Daniel PW Ellis, and C 		GRID corpus (mixed-speech)
Colin Raffel, “mir eval: A 		GRID corpus (mixed-speech)
transparent implementation of common mir 		GRID corpus (mixed-speech)
metrics,” in In Proceed- ings 		GRID corpus (mixed-speech)
of the 15th International Society 		GRID corpus (mixed-speech)
for Music Information Retrieval Conference, 		GRID corpus (mixed-speech)
ISMIR. Citeseer, 2014.  [29] A.W. Rix, J.G. Beerends		GRID corpus (mixed-speech)
, M.P. Hollier, and A.P. 		GRID corpus (mixed-speech)
Hekstra, “Perceptual evaluation of speech 		GRID corpus (mixed-speech)
quality (PESQ)-a new method for 		GRID corpus (mixed-speech)
speech qual- ity assessment of 		GRID corpus (mixed-speech)
telephone networks and codecs,” in 2001 IEEE In- ternational Conference on		GRID corpus (mixed-speech)
 Acoustics, Speech, and Signal Processing		GRID corpus (mixed-speech)
. Proceedings (Cat. No.01CH37221), Salt 		GRID corpus (mixed-speech)
Lake City, UT, USA, 2001, 		GRID corpus (mixed-speech)
vol. 2, pp. 749–752, IEEE.  [30] A. Hines, J. Skoglund		GRID corpus (mixed-speech)
, A. Kokaram, and N. 		GRID corpus (mixed-speech)
Harte, “ViSQOL: The Virtual Speech 		GRID corpus (mixed-speech)
Quality Objective Listener,” in IWAENC 2012		GRID corpus (mixed-speech)
; Inter- national Workshop on 		GRID corpus (mixed-speech)
Acoustic Signal Enhancement, Sept. 2012, 		GRID corpus (mixed-speech)
pp. 1		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
4		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
1  Introduction		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
1.1  Related work		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
2  MODEL ARCHITECTURES		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
2.1  VL2M model		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
2.2  VL2M_ref model		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
2.3  Audio-Visual concat model		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
2.4  Audio-Visual concat-ref model		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
3  Experimental setup		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
3.1  Dataset		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
3.2  LSTM training		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
3.3  Audio pre- and post-processing		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
3.4  Video pre-processing		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
4  Results		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
5  Conclusion		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
6  References		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
FACE LANDMARK-BASED SPEAKER-INDEPENDENT AUDIO-VISUAL SPEECH 	GRID corpus (mixed	GRID corpus (mixed-speech)
ENHANCEMENT IN MULTI-TALKER ENVIRONMENTS  Giovanni Morrone? Luca Pasa† Vadim	GRID corpus (mixed	GRID corpus (mixed-speech)
 Tikhanoff	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
Sonia Bergamaschi? Luciano Fadiga† Leonardo 	GRID corpus (mixed	GRID corpus (mixed-speech)
Badino†  ?Department of Engineering ”Enzo Ferrari	GRID corpus (mixed	GRID corpus (mixed-speech)
”, University of Modena and 	GRID corpus (mixed	GRID corpus (mixed-speech)
Reggio Emilia, Modena, Italy †Istituto 	GRID corpus (mixed	GRID corpus (mixed-speech)
Italiano di Tecnologia, Ferrara, Italy  ABSTRACT	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
In this paper, we address 	GRID corpus (mixed	GRID corpus (mixed-speech)
the problem of enhancing the 	GRID corpus (mixed	GRID corpus (mixed-speech)
speech of a speaker of 	GRID corpus (mixed	GRID corpus (mixed-speech)
interest in a cocktail party 	GRID corpus (mixed	GRID corpus (mixed-speech)
scenario when vi- sual information 	GRID corpus (mixed	GRID corpus (mixed-speech)
of the speaker of interest 	GRID corpus (mixed	GRID corpus (mixed-speech)
is available.  Contrary to most previous studies	GRID corpus (mixed	GRID corpus (mixed-speech)
, we do not learn 	GRID corpus (mixed	GRID corpus (mixed-speech)
visual features on the typically 	GRID corpus (mixed	GRID corpus (mixed-speech)
small audio-visual datasets, but use 	GRID corpus (mixed	GRID corpus (mixed-speech)
an already available face landmark 	GRID corpus (mixed	GRID corpus (mixed-speech)
detector (trained on a sep- 	GRID corpus (mixed	GRID corpus (mixed-speech)
arate image dataset).  The landmarks are used by	GRID corpus (mixed	GRID corpus (mixed-speech)
 LSTM-based models to gen- erate	GRID corpus (mixed	GRID corpus (mixed-speech)
 time-frequency masks which are applied	GRID corpus (mixed	GRID corpus (mixed-speech)
 to the acoustic mixed-speech spectrogram	GRID corpus (mixed	GRID corpus (mixed-speech)
. Results show that: (i) 	GRID corpus (mixed	GRID corpus (mixed-speech)
land- mark motion features are 	GRID corpus (mixed	GRID corpus (mixed-speech)
very effective features for this 	GRID corpus (mixed	GRID corpus (mixed-speech)
task, (ii) similarly to previous 	GRID corpus (mixed	GRID corpus (mixed-speech)
work, reconstruction of the target 	GRID corpus (mixed	GRID corpus (mixed-speech)
speaker’s spectrogram mediated by masking 	GRID corpus (mixed	GRID corpus (mixed-speech)
is significantly more accurate than 	GRID corpus (mixed	GRID corpus (mixed-speech)
direct spectrogram reconstruction, and (iii) 	GRID corpus (mixed	GRID corpus (mixed-speech)
the best masks depend on 	GRID corpus (mixed	GRID corpus (mixed-speech)
both motion landmark features and 	GRID corpus (mixed	GRID corpus (mixed-speech)
the input mixed-speech spectrogram.  To the best of our	GRID corpus (mixed	GRID corpus (mixed-speech)
 knowledge, our proposed models are	GRID corpus (mixed	GRID corpus (mixed-speech)
 the first models trained and	GRID corpus (mixed	GRID corpus (mixed-speech)
 evaluated on the limited size	GRID corpus (mixed	GRID corpus (mixed-speech)
 GRID and TCD-TIMIT datasets, that	GRID corpus (mixed	GRID corpus (mixed-speech)
 achieve speaker-independent speech enhancement in	GRID corpus (mixed	GRID corpus (mixed-speech)
 a multi-talker setting	GRID corpus (mixed	GRID corpus (mixed-speech)
.  Index Terms— audio-visual speech enhancement	GRID corpus (mixed	GRID corpus (mixed-speech)
, cock- tail party problem, 	GRID corpus (mixed	GRID corpus (mixed-speech)
time-frequency mask, LSTM, face land- 	GRID corpus (mixed	GRID corpus (mixed-speech)
marks  1. INTRODUCTION	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
In the context of speech 	GRID corpus (mixed	GRID corpus (mixed-speech)
perception, the cocktail party 	GRID corpus (mixed	GRID corpus (mixed-speech)
effect [1, 2] is the 	GRID corpus (mixed	GRID corpus (mixed-speech)
ability of the brain to 	GRID corpus (mixed	GRID corpus (mixed-speech)
recognize speech in complex and 	GRID corpus (mixed	GRID corpus (mixed-speech)
adverse listening conditions where the 	GRID corpus (mixed	GRID corpus (mixed-speech)
attended speech is mixed with 	GRID corpus (mixed	GRID corpus (mixed-speech)
competing sounds/speech.  Speech perception studies have shown	GRID corpus (mixed	GRID corpus (mixed-speech)
 that watching speaker’s face movements	GRID corpus (mixed	GRID corpus (mixed-speech)
 could dramatically improve our ability	GRID corpus (mixed	GRID corpus (mixed-speech)
 at recognizing the speech of	GRID corpus (mixed	GRID corpus (mixed-speech)
 a target speaker in a	GRID corpus (mixed	GRID corpus (mixed-speech)
 multi-talker environment [3, 4	GRID corpus (mixed	GRID corpus (mixed-speech)
].  This work aims at extracting	GRID corpus (mixed	GRID corpus (mixed-speech)
 the speech of a target	GRID corpus (mixed	GRID corpus (mixed-speech)
 speaker from single channel audio	GRID corpus (mixed	GRID corpus (mixed-speech)
 of several people talking simulta	GRID corpus (mixed	GRID corpus (mixed-speech)
- neously. This is an 	GRID corpus (mixed	GRID corpus (mixed-speech)
ill-posed problem in that many 	GRID corpus (mixed	GRID corpus (mixed-speech)
differ- ent hypotheses about what 	GRID corpus (mixed	GRID corpus (mixed-speech)
the target speaker says are 	GRID corpus (mixed	GRID corpus (mixed-speech)
con-  sistent with the mixture signal	GRID corpus (mixed	GRID corpus (mixed-speech)
. Yet, it can be 	GRID corpus (mixed	GRID corpus (mixed-speech)
solved by ex- ploiting some 	GRID corpus (mixed	GRID corpus (mixed-speech)
additional information associated to the 	GRID corpus (mixed	GRID corpus (mixed-speech)
speaker of interest and/or by 	GRID corpus (mixed	GRID corpus (mixed-speech)
leveraging some prior knowledge about 	GRID corpus (mixed	GRID corpus (mixed-speech)
speech signal properties (e.g., [5]). 	GRID corpus (mixed	GRID corpus (mixed-speech)
In this work we use 	GRID corpus (mixed	GRID corpus (mixed-speech)
face movements of the target 	GRID corpus (mixed	GRID corpus (mixed-speech)
speaker as additional information.  This paper (i) proposes the	GRID corpus (mixed	GRID corpus (mixed-speech)
 use of face landmark’s move	GRID corpus (mixed	GRID corpus (mixed-speech)
- ments, extracted using 	GRID corpus (mixed	GRID corpus (mixed-speech)
Dlib [6, 7] and (ii) 	GRID corpus (mixed	GRID corpus (mixed-speech)
compares differ- ent ways of 	GRID corpus (mixed	GRID corpus (mixed-speech)
mapping such visual features into 	GRID corpus (mixed	GRID corpus (mixed-speech)
time-frequency (T-F) masks, then applied 	GRID corpus (mixed	GRID corpus (mixed-speech)
to clean the acoustic mixed-speech 	GRID corpus (mixed	GRID corpus (mixed-speech)
spectrogram.  By using Dlib extracted landmarks	GRID corpus (mixed	GRID corpus (mixed-speech)
 we relieve our mod- els	GRID corpus (mixed	GRID corpus (mixed-speech)
 from the task of learning	GRID corpus (mixed	GRID corpus (mixed-speech)
 useful visual features from raw	GRID corpus (mixed	GRID corpus (mixed-speech)
 pixels. That aspect is particularly	GRID corpus (mixed	GRID corpus (mixed-speech)
 relevant when the training audio-visual	GRID corpus (mixed	GRID corpus (mixed-speech)
 datasets are small	GRID corpus (mixed	GRID corpus (mixed-speech)
.  The analysis of landmark-dependent masking	GRID corpus (mixed	GRID corpus (mixed-speech)
 strategies is motivated by the	GRID corpus (mixed	GRID corpus (mixed-speech)
 fact that speech enhancement mediated	GRID corpus (mixed	GRID corpus (mixed-speech)
 by an explicit masking is	GRID corpus (mixed	GRID corpus (mixed-speech)
 often more effective than mask-free	GRID corpus (mixed	GRID corpus (mixed-speech)
 enhancement [8	GRID corpus (mixed	GRID corpus (mixed-speech)
].  All our models were trained	GRID corpus (mixed	GRID corpus (mixed-speech)
 and evaluated on the GRID	GRID corpus (mixed	GRID corpus (mixed-speech)
 [9] and TCD-TIMIT [10] datasets	GRID corpus (mixed	GRID corpus (mixed-speech)
 in a speaker-independent setting	GRID corpus (mixed	GRID corpus (mixed-speech)
.  1.1. Related work	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
Speech enhancement aims at extracting 	GRID corpus (mixed	GRID corpus (mixed-speech)
the voice of a tar- 	GRID corpus (mixed	GRID corpus (mixed-speech)
get speaker, while speech separation 	GRID corpus (mixed	GRID corpus (mixed-speech)
refers to the problem of 	GRID corpus (mixed	GRID corpus (mixed-speech)
separating each sound source in 	GRID corpus (mixed	GRID corpus (mixed-speech)
a mixture. Recently pro- posed 	GRID corpus (mixed	GRID corpus (mixed-speech)
audio-only single-channel methods have achieved 	GRID corpus (mixed	GRID corpus (mixed-speech)
very promising results [11, 12, 13	GRID corpus (mixed	GRID corpus (mixed-speech)
]. However the task still 	GRID corpus (mixed	GRID corpus (mixed-speech)
remains challenging. Additionally, audio-only systems 	GRID corpus (mixed	GRID corpus (mixed-speech)
need separate models in order 	GRID corpus (mixed	GRID corpus (mixed-speech)
to associate the estimated separated 	GRID corpus (mixed	GRID corpus (mixed-speech)
audio sources to each speaker, 	GRID corpus (mixed	GRID corpus (mixed-speech)
while vision easily allow that 	GRID corpus (mixed	GRID corpus (mixed-speech)
in a unified model.  Regarding audio-visual speech enhancement and	GRID corpus (mixed	GRID corpus (mixed-speech)
 separa- tion methods an extensive	GRID corpus (mixed	GRID corpus (mixed-speech)
 review is provided in [14	GRID corpus (mixed	GRID corpus (mixed-speech)
]. Here we focus on 	GRID corpus (mixed	GRID corpus (mixed-speech)
the deep-learning methods that are 	GRID corpus (mixed	GRID corpus (mixed-speech)
most related to the present 	GRID corpus (mixed	GRID corpus (mixed-speech)
work.  Our first architecture (Section 2.1	GRID corpus (mixed	GRID corpus (mixed-speech)
) is inspired by [15], 	GRID corpus (mixed	GRID corpus (mixed-speech)
where a pre-trained convolutional neural 	GRID corpus (mixed	GRID corpus (mixed-speech)
network (CNN) is used to 	GRID corpus (mixed	GRID corpus (mixed-speech)
generate a clean spectrogram from 	GRID corpus (mixed	GRID corpus (mixed-speech)
silent video [16]. Rather than 	GRID corpus (mixed	GRID corpus (mixed-speech)
directly computing a time-frequency (T-F) 	GRID corpus (mixed	GRID corpus (mixed-speech)
mask,  ar X	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
iv :1  81 1	GRID corpus (mixed	GRID corpus (mixed-speech)
.  02 48	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
0v 3	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
cs  .C L	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
2	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
M 	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
2 01  9	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
the mask is computed by 	GRID corpus (mixed	GRID corpus (mixed-speech)
thresholding the estimated clean spectrogram. 	GRID corpus (mixed	GRID corpus (mixed-speech)
This approach is not very 	GRID corpus (mixed	GRID corpus (mixed-speech)
effective since the pre-trained CNN 	GRID corpus (mixed	GRID corpus (mixed-speech)
is designed for a different 	GRID corpus (mixed	GRID corpus (mixed-speech)
task (video-to- speech synthesis). 	GRID corpus (mixed	GRID corpus (mixed-speech)
In [17] a CNN is 	GRID corpus (mixed	GRID corpus (mixed-speech)
trained to directly esti- mate 	GRID corpus (mixed	GRID corpus (mixed-speech)
clean speech from noisy audio 	GRID corpus (mixed	GRID corpus (mixed-speech)
and input video. A sim- 	GRID corpus (mixed	GRID corpus (mixed-speech)
ilar model is used 	GRID corpus (mixed	GRID corpus (mixed-speech)
in [18], where the model 	GRID corpus (mixed	GRID corpus (mixed-speech)
jointly generates clean speech and 	GRID corpus (mixed	GRID corpus (mixed-speech)
input video in a denoising-autoender 	GRID corpus (mixed	GRID corpus (mixed-speech)
archi- tecture.  [19] shows that using information	GRID corpus (mixed	GRID corpus (mixed-speech)
 about lip positions can help	GRID corpus (mixed	GRID corpus (mixed-speech)
 to improve speech enhancement. The	GRID corpus (mixed	GRID corpus (mixed-speech)
 video feature vec- tor is	GRID corpus (mixed	GRID corpus (mixed-speech)
 obtained computing pair-wise distances between	GRID corpus (mixed	GRID corpus (mixed-speech)
 any mouth landmarks. Similarly to	GRID corpus (mixed	GRID corpus (mixed-speech)
 our approach their visual fea	GRID corpus (mixed	GRID corpus (mixed-speech)
- tures are not learned 	GRID corpus (mixed	GRID corpus (mixed-speech)
on the audio-visual dataset but 	GRID corpus (mixed	GRID corpus (mixed-speech)
are pro- vided by a 	GRID corpus (mixed	GRID corpus (mixed-speech)
system trained on different dataset. 	GRID corpus (mixed	GRID corpus (mixed-speech)
Contrary to our approach, [19] 	GRID corpus (mixed	GRID corpus (mixed-speech)
uses position-based features while we 	GRID corpus (mixed	GRID corpus (mixed-speech)
use motion features (of the 	GRID corpus (mixed	GRID corpus (mixed-speech)
whole face) that in our 	GRID corpus (mixed	GRID corpus (mixed-speech)
experiments turned out to be 	GRID corpus (mixed	GRID corpus (mixed-speech)
much more effective than positional 	GRID corpus (mixed	GRID corpus (mixed-speech)
features.  Although the aforementioned audio-visual methods	GRID corpus (mixed	GRID corpus (mixed-speech)
 work well, they have only	GRID corpus (mixed	GRID corpus (mixed-speech)
 been evaluated in a speaker-dependent	GRID corpus (mixed	GRID corpus (mixed-speech)
 setting. Only the availability of	GRID corpus (mixed	GRID corpus (mixed-speech)
 new large and heterogeneous audio-visual	GRID corpus (mixed	GRID corpus (mixed-speech)
 datasets has allowed the training	GRID corpus (mixed	GRID corpus (mixed-speech)
 of deep neu- ral network-based	GRID corpus (mixed	GRID corpus (mixed-speech)
 speaker-independent speech enhancement models [20	GRID corpus (mixed	GRID corpus (mixed-speech)
, 21, 22].  The present work shows that	GRID corpus (mixed	GRID corpus (mixed-speech)
 huge audio-visual datasets are not	GRID corpus (mixed	GRID corpus (mixed-speech)
 a necessary requirement for speaker-independent	GRID corpus (mixed	GRID corpus (mixed-speech)
 audio-visual speech enhancement. Although we	GRID corpus (mixed	GRID corpus (mixed-speech)
 have only considered datasets with	GRID corpus (mixed	GRID corpus (mixed-speech)
 simple visual scenarios (i.e., the	GRID corpus (mixed	GRID corpus (mixed-speech)
 target speaker is always facing	GRID corpus (mixed	GRID corpus (mixed-speech)
 the camera), we expect our	GRID corpus (mixed	GRID corpus (mixed-speech)
 methods to perform well in	GRID corpus (mixed	GRID corpus (mixed-speech)
 more complex scenarios thanks to	GRID corpus (mixed	GRID corpus (mixed-speech)
 the robust landmark extraction	GRID corpus (mixed	GRID corpus (mixed-speech)
.  2. MODEL ARCHITECTURES	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
We experimented with the four 	GRID corpus (mixed	GRID corpus (mixed-speech)
models shown in Fig. 1. 	GRID corpus (mixed	GRID corpus (mixed-speech)
All models receive in input 	GRID corpus (mixed	GRID corpus (mixed-speech)
the target speaker’s landmark mo- 	GRID corpus (mixed	GRID corpus (mixed-speech)
tion vectors and the power-law 	GRID corpus (mixed	GRID corpus (mixed-speech)
compressed spectrogram of the single-channel 	GRID corpus (mixed	GRID corpus (mixed-speech)
mixed-speech signal. All of them 	GRID corpus (mixed	GRID corpus (mixed-speech)
perform some kind of masking 	GRID corpus (mixed	GRID corpus (mixed-speech)
operation.  2.1. VL2M model	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
At each time frame, the 	GRID corpus (mixed	GRID corpus (mixed-speech)
video-landmark to mask (VL2M) model (	GRID corpus (mixed	GRID corpus (mixed-speech)
Fig. 1a) estimates a T-F 	GRID corpus (mixed	GRID corpus (mixed-speech)
mask from visual features only (	GRID corpus (mixed	GRID corpus (mixed-speech)
of the target speaker). Formally, 	GRID corpus (mixed	GRID corpus (mixed-speech)
given a video sequence 	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
 = [v1	GRID corpus (mixed	GRID corpus (mixed-speech)
, . . . , 	GRID corpus (mixed	GRID corpus (mixed-speech)
vT ], vt ∈ Rn 	GRID corpus (mixed	GRID corpus (mixed-speech)
and a target mask sequence 	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
 = [m1	GRID corpus (mixed	GRID corpus (mixed-speech)
, . . . ,	GRID corpus (mixed	GRID corpus (mixed-speech)
mT ], mt ∈ Rd, 	GRID corpus (mixed	GRID corpus (mixed-speech)
VL2M perform a function 	GRID corpus (mixed	GRID corpus (mixed-speech)
Fvl2m(v) = m̂, where m̂ 	GRID corpus (mixed	GRID corpus (mixed-speech)
is the estimated mask.  The training objective for VL2M	GRID corpus (mixed	GRID corpus (mixed-speech)
 is a Target Binary Mask	GRID corpus (mixed	GRID corpus (mixed-speech)
 (TBM) [23, 24], computed using	GRID corpus (mixed	GRID corpus (mixed-speech)
 the spectrogram of the tar	GRID corpus (mixed	GRID corpus (mixed-speech)
- get speaker only. This 	GRID corpus (mixed	GRID corpus (mixed-speech)
is motivated by our goal 	GRID corpus (mixed	GRID corpus (mixed-speech)
of extracting the speech of 	GRID corpus (mixed	GRID corpus (mixed-speech)
a target speaker as much 	GRID corpus (mixed	GRID corpus (mixed-speech)
as possible indepen- dently of 	GRID corpus (mixed	GRID corpus (mixed-speech)
the concurrent speakers, so that, 	GRID corpus (mixed	GRID corpus (mixed-speech)
e.g., we do not need 	GRID corpus (mixed	GRID corpus (mixed-speech)
to estimate their number. An 	GRID corpus (mixed	GRID corpus (mixed-speech)
additional motivations is that the 	GRID corpus (mixed	GRID corpus (mixed-speech)
model takes as only input 	GRID corpus (mixed	GRID corpus (mixed-speech)
the visual features of the  target speaker, and a target	GRID corpus (mixed	GRID corpus (mixed-speech)
 TBM that only depends on	GRID corpus (mixed	GRID corpus (mixed-speech)
 the target speaker allows VL2M	GRID corpus (mixed	GRID corpus (mixed-speech)
 to learn a function (rather	GRID corpus (mixed	GRID corpus (mixed-speech)
 than approximating an ill-posed one-to-many	GRID corpus (mixed	GRID corpus (mixed-speech)
 mapping	GRID corpus (mixed	GRID corpus (mixed-speech)
).  Given a clean speech spectrogram	GRID corpus (mixed	GRID corpus (mixed-speech)
 of a speaker s	GRID corpus (mixed	GRID corpus (mixed-speech)
 = [s1	GRID corpus (mixed	GRID corpus (mixed-speech)
, . . . , 	GRID corpus (mixed	GRID corpus (mixed-speech)
sT ], st ∈ Rd, 	GRID corpus (mixed	GRID corpus (mixed-speech)
the TBM is defined by 	GRID corpus (mixed	GRID corpus (mixed-speech)
comparing, at each frequency bin 	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
 ∈ [1	GRID corpus (mixed	GRID corpus (mixed-speech)
, . . . , 	GRID corpus (mixed	GRID corpus (mixed-speech)
d], the target speaker value 	GRID corpus (mixed	GRID corpus (mixed-speech)
st[f ] vs. a reference 	GRID corpus (mixed	GRID corpus (mixed-speech)
threshold τ [f ]. As 	GRID corpus (mixed	GRID corpus (mixed-speech)
in [15], we use a 	GRID corpus (mixed	GRID corpus (mixed-speech)
function of long-term average speech 	GRID corpus (mixed	GRID corpus (mixed-speech)
spectrum (LTASS) as reference threshold. 	GRID corpus (mixed	GRID corpus (mixed-speech)
This threshold indicates if a 	GRID corpus (mixed	GRID corpus (mixed-speech)
T-F unit is generated by 	GRID corpus (mixed	GRID corpus (mixed-speech)
the speaker or refers to 	GRID corpus (mixed	GRID corpus (mixed-speech)
silence or noise. The process 	GRID corpus (mixed	GRID corpus (mixed-speech)
to compute the speaker’s TBM 	GRID corpus (mixed	GRID corpus (mixed-speech)
is as follows:  1. The mean π[f	GRID corpus (mixed	GRID corpus (mixed-speech)
 ] and the standard deviation	GRID corpus (mixed	GRID corpus (mixed-speech)
 σ[f ] are computed for	GRID corpus (mixed	GRID corpus (mixed-speech)
 all frequency bins of all	GRID corpus (mixed	GRID corpus (mixed-speech)
 seen spectro- grams in speaker’s	GRID corpus (mixed	GRID corpus (mixed-speech)
 data	GRID corpus (mixed	GRID corpus (mixed-speech)
.  2. The threshold τ [f	GRID corpus (mixed	GRID corpus (mixed-speech)
 ] is defined as τ	GRID corpus (mixed	GRID corpus (mixed-speech)
 [f ] = π[f ]+0.6	GRID corpus (mixed	GRID corpus (mixed-speech)
 ·σ[f ] where 0.6 is	GRID corpus (mixed	GRID corpus (mixed-speech)
 a value selected by manual	GRID corpus (mixed	GRID corpus (mixed-speech)
 inspection of several spectrogram-TBM pairs	GRID corpus (mixed	GRID corpus (mixed-speech)
.  3. The threshold is applied	GRID corpus (mixed	GRID corpus (mixed-speech)
 to every speaker’s speech spec	GRID corpus (mixed	GRID corpus (mixed-speech)
- trogram s.  mt[f	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
1, if st[f ] ≥ 	GRID corpus (mixed	GRID corpus (mixed-speech)
τ [f ], 0, otherwise.  The mapping Fvl2m(·) is carried	GRID corpus (mixed	GRID corpus (mixed-speech)
 out by a stacked bi	GRID corpus (mixed	GRID corpus (mixed-speech)
- directional Long Short-Term Memory (	GRID corpus (mixed	GRID corpus (mixed-speech)
BLSTM) network [25]. The BLSTM 	GRID corpus (mixed	GRID corpus (mixed-speech)
outputs are then forced to 	GRID corpus (mixed	GRID corpus (mixed-speech)
lay within the [0, 1] 	GRID corpus (mixed	GRID corpus (mixed-speech)
range. Finally the computed TBM 	GRID corpus (mixed	GRID corpus (mixed-speech)
m̂ and the noisy spectrogram 	GRID corpus (mixed	GRID corpus (mixed-speech)
y are element-wise multiplied to 	GRID corpus (mixed	GRID corpus (mixed-speech)
ob- tain the estimated clean 	GRID corpus (mixed	GRID corpus (mixed-speech)
spectrogram ŝm = m̂ ◦ 	GRID corpus (mixed	GRID corpus (mixed-speech)
y, where 	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
 = [y1	GRID corpus (mixed	GRID corpus (mixed-speech)
, . . . 	GRID corpus (mixed	GRID corpus (mixed-speech)
yT ], yt ∈ Rd.  The model parameters are estimated	GRID corpus (mixed	GRID corpus (mixed-speech)
 to minimize the loss	GRID corpus (mixed	GRID corpus (mixed-speech)
:  Jvl2m = ∑T	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
t=1  ∑d f=1−mt[f ] · log(m̂t[f	GRID corpus (mixed	GRID corpus (mixed-speech)
 ])− (1−mt[f ]) · log(1	GRID corpus (mixed	GRID corpus (mixed-speech)
− m̂t[f ])  2.2. VL2M ref model	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
VL2M generates T-F masks that 	GRID corpus (mixed	GRID corpus (mixed-speech)
are independent of the acous- 	GRID corpus (mixed	GRID corpus (mixed-speech)
tic context. We may want 	GRID corpus (mixed	GRID corpus (mixed-speech)
to refine the masking by 	GRID corpus (mixed	GRID corpus (mixed-speech)
including such context. This is 	GRID corpus (mixed	GRID corpus (mixed-speech)
what the novel VL2M ref 	GRID corpus (mixed	GRID corpus (mixed-speech)
does (Fig. 1b). The computed 	GRID corpus (mixed	GRID corpus (mixed-speech)
TBM m̂ and the input 	GRID corpus (mixed	GRID corpus (mixed-speech)
spectrogram y are the input 	GRID corpus (mixed	GRID corpus (mixed-speech)
to a function that outputs 	GRID corpus (mixed	GRID corpus (mixed-speech)
an Ideal Amplitude Mask (IAM) 	GRID corpus (mixed	GRID corpus (mixed-speech)
p (known as FFT-MASK 	GRID corpus (mixed	GRID corpus (mixed-speech)
in [8]). Given the target 	GRID corpus (mixed	GRID corpus (mixed-speech)
clean spectrogram s and the 	GRID corpus (mixed	GRID corpus (mixed-speech)
noisy spectrogram y, the IAM 	GRID corpus (mixed	GRID corpus (mixed-speech)
is defined as:  pt[f ] = st[f	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
yt[f ]  Note that although IAM generation	GRID corpus (mixed	GRID corpus (mixed-speech)
 requires the mixed-speech spectrogram, separate	GRID corpus (mixed	GRID corpus (mixed-speech)
 spectrograms for each concurrent speakers	GRID corpus (mixed	GRID corpus (mixed-speech)
 are not required	GRID corpus (mixed	GRID corpus (mixed-speech)
.  The target speaker’s spectrogram s	GRID corpus (mixed	GRID corpus (mixed-speech)
 is reconstructed by multiplying the	GRID corpus (mixed	GRID corpus (mixed-speech)
 input spectrogram with the estimated	GRID corpus (mixed	GRID corpus (mixed-speech)
 IAM. Values greater than 10	GRID corpus (mixed	GRID corpus (mixed-speech)
 in the IAM are clipped	GRID corpus (mixed	GRID corpus (mixed-speech)
 to 10 in order to	GRID corpus (mixed	GRID corpus (mixed-speech)
 obtain better numerical stability as	GRID corpus (mixed	GRID corpus (mixed-speech)
 suggested in [8	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
v: video input y: noisy 	GRID corpus (mixed	GRID corpus (mixed-speech)
spectrogram sm: clean spectrogram TBM 	GRID corpus (mixed	GRID corpus (mixed-speech)
s: clean spectrogram IAM m: 	GRID corpus (mixed	GRID corpus (mixed-speech)
TBM p: IAM  STACKED	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
BLSTM  m	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
sm  v	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
y  (a) VL2M	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
v VL2M m  y BLSTM	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
BLSTM  Fusion layer	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
BLSTM p  s	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
b) VL2M ref  v	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
y  p STACKED	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
BLSTM  s	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
c) Audio-Visual concat  sm	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
y  p STACKED	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
BLSTM  s	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
v VL2M m  (d) Audio-Visual concat-ref	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
Fig. 1. Model architectures.  The model performs a function	GRID corpus (mixed	GRID corpus (mixed-speech)
 Fmr(v, y) = p̂ that	GRID corpus (mixed	GRID corpus (mixed-speech)
 con- sists of a VL2M	GRID corpus (mixed	GRID corpus (mixed-speech)
 component plus three different BLSTMs	GRID corpus (mixed	GRID corpus (mixed-speech)
 Gm, Gy and H	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
Gm(Fvl2m(v)) = rm receives the 	GRID corpus (mixed	GRID corpus (mixed-speech)
VL2M mask m̂ as in- 	GRID corpus (mixed	GRID corpus (mixed-speech)
put, and Gy(y) = ry 	GRID corpus (mixed	GRID corpus (mixed-speech)
is fed with the noisy 	GRID corpus (mixed	GRID corpus (mixed-speech)
spectrogram. Their output rm, 	GRID corpus (mixed	GRID corpus (mixed-speech)
ry ∈ Rz are fused 	GRID corpus (mixed	GRID corpus (mixed-speech)
in a joint audio-visual represen- 	GRID corpus (mixed	GRID corpus (mixed-speech)
tation 	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
 = [h1	GRID corpus (mixed	GRID corpus (mixed-speech)
, . . . ,	GRID corpus (mixed	GRID corpus (mixed-speech)
hT ], where ht is 	GRID corpus (mixed	GRID corpus (mixed-speech)
a linear combination of rmt 	GRID corpus (mixed	GRID corpus (mixed-speech)
and ryt : ht = 	GRID corpus (mixed	GRID corpus (mixed-speech)
Whm ·rmt +Why ·ryt +bh. 	GRID corpus (mixed	GRID corpus (mixed-speech)
h is the input of 	GRID corpus (mixed	GRID corpus (mixed-speech)
the third BLSTM H (	GRID corpus (mixed	GRID corpus (mixed-speech)
h) = p̂, where p̂ 	GRID corpus (mixed	GRID corpus (mixed-speech)
lays in the [0,10] range. 	GRID corpus (mixed	GRID corpus (mixed-speech)
The loss function is:  Jmr	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
T∑ t=1  d∑ f=1	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
p̂t[f ] · yt[f ]− 	GRID corpus (mixed	GRID corpus (mixed-speech)
st[f ])2  2.3. Audio-Visual concat model	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
The third model (Fig. 1c) 	GRID corpus (mixed	GRID corpus (mixed-speech)
performs early fusion of audio- 	GRID corpus (mixed	GRID corpus (mixed-speech)
visual features. This model consists 	GRID corpus (mixed	GRID corpus (mixed-speech)
of a single stacked BLSTM 	GRID corpus (mixed	GRID corpus (mixed-speech)
that computes the IAM mask 	GRID corpus (mixed	GRID corpus (mixed-speech)
p̂ from the concate- 	GRID corpus (mixed	GRID corpus (mixed-speech)
nated [v,y]. The training loss 	GRID corpus (mixed	GRID corpus (mixed-speech)
is the same Jmr used 	GRID corpus (mixed	GRID corpus (mixed-speech)
to train VL2M ref. This 	GRID corpus (mixed	GRID corpus (mixed-speech)
model can be regarded as 	GRID corpus (mixed	GRID corpus (mixed-speech)
a simplification of VL2M ref, 	GRID corpus (mixed	GRID corpus (mixed-speech)
where the VL2M operation is 	GRID corpus (mixed	GRID corpus (mixed-speech)
not performed.  2.4. Audio-Visual concat-ref model	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
The fourth model (Fig. 1d) 	GRID corpus (mixed	GRID corpus (mixed-speech)
is an improved version of 	GRID corpus (mixed	GRID corpus (mixed-speech)
the model described in section 2	GRID corpus (mixed	GRID corpus (mixed-speech)
.3. The only difference is 	GRID corpus (mixed	GRID corpus (mixed-speech)
the input of the stacked 	GRID corpus (mixed	GRID corpus (mixed-speech)
BLSTM that is replaced 	GRID corpus (mixed	GRID corpus (mixed-speech)
by [̂sm,y] where ŝm is 	GRID corpus (mixed	GRID corpus (mixed-speech)
the denoised spectrogram returned by 	GRID corpus (mixed	GRID corpus (mixed-speech)
VL2M operation.  3. EXPERIMENTAL SETUP	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
3.1. Dataset  All experiments were carried out	GRID corpus (mixed	GRID corpus (mixed-speech)
 using the GRID [9] and	GRID corpus (mixed	GRID corpus (mixed-speech)
 TCD-TIMIT [10] audio-visual datasets. For	GRID corpus (mixed	GRID corpus (mixed-speech)
 each of them, we created	GRID corpus (mixed	GRID corpus (mixed-speech)
 a mixed-speech version	GRID corpus (mixed	GRID corpus (mixed-speech)
.  Regarding the GRID corpus, for	GRID corpus (mixed	GRID corpus (mixed-speech)
 each of the 33 speakers	GRID corpus (mixed	GRID corpus (mixed-speech)
 (one had to be discarded	GRID corpus (mixed	GRID corpus (mixed-speech)
) we first randomly selected 200 ut- terances (out of 1000	GRID corpus (mixed	GRID corpus (mixed-speech)
). Then, for each utterance, 	GRID corpus (mixed	GRID corpus (mixed-speech)
we created 3 different audio-mixed 	GRID corpus (mixed	GRID corpus (mixed-speech)
samples. Each audio-mixed sample was 	GRID corpus (mixed	GRID corpus (mixed-speech)
created by mixing the chosen 	GRID corpus (mixed	GRID corpus (mixed-speech)
utterance with one utter- ance 	GRID corpus (mixed	GRID corpus (mixed-speech)
from a different speaker.  That resulted in 600 audio-mixed	GRID corpus (mixed	GRID corpus (mixed-speech)
 samples per speaker	GRID corpus (mixed	GRID corpus (mixed-speech)
.  The resulting dataset was split	GRID corpus (mixed	GRID corpus (mixed-speech)
 into disjoint sets of 25/4/4	GRID corpus (mixed	GRID corpus (mixed-speech)
 speakers for training/validation/testing respectively	GRID corpus (mixed	GRID corpus (mixed-speech)
.  The TCD-TIMIT corpus consists of	GRID corpus (mixed	GRID corpus (mixed-speech)
 59 speakers (we ex- cluded	GRID corpus (mixed	GRID corpus (mixed-speech)
 3 professionally-trained lipspeakers) and 98	GRID corpus (mixed	GRID corpus (mixed-speech)
 utterances per speaker. The mixed-speech	GRID corpus (mixed	GRID corpus (mixed-speech)
 version was created following the	GRID corpus (mixed	GRID corpus (mixed-speech)
 same procedure as for GRID	GRID corpus (mixed	GRID corpus (mixed-speech)
, with one difference. Con- 	GRID corpus (mixed	GRID corpus (mixed-speech)
trary to GRID, TCD-TIMIT utterances 	GRID corpus (mixed	GRID corpus (mixed-speech)
have different dura- tion. Thus 2 utterances were mixed only if	GRID corpus (mixed	GRID corpus (mixed-speech)
 their duration dif- ference did	GRID corpus (mixed	GRID corpus (mixed-speech)
 not exceed 2 seconds. For	GRID corpus (mixed	GRID corpus (mixed-speech)
 each utterance pair, we forced	GRID corpus (mixed	GRID corpus (mixed-speech)
 the non-target speaker’s utterance to	GRID corpus (mixed	GRID corpus (mixed-speech)
 match the du- ration of	GRID corpus (mixed	GRID corpus (mixed-speech)
 the target speaker utterance. If	GRID corpus (mixed	GRID corpus (mixed-speech)
 it was longer, the utterance	GRID corpus (mixed	GRID corpus (mixed-speech)
 was cut at its end	GRID corpus (mixed	GRID corpus (mixed-speech)
, whereas if it was 	GRID corpus (mixed	GRID corpus (mixed-speech)
shorter, silence samples were equally 	GRID corpus (mixed	GRID corpus (mixed-speech)
added at its start and 	GRID corpus (mixed	GRID corpus (mixed-speech)
end.  The resulting dataset was split	GRID corpus (mixed	GRID corpus (mixed-speech)
 into disjoint sets of 51/4/4	GRID corpus (mixed	GRID corpus (mixed-speech)
 speakers for training/validation/testing respectively	GRID corpus (mixed	GRID corpus (mixed-speech)
.  3.2. LSTM training	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
In all experiments, the models 	GRID corpus (mixed	GRID corpus (mixed-speech)
were trained using the Adam 	GRID corpus (mixed	GRID corpus (mixed-speech)
optimizer [26]. Early stopping was 	GRID corpus (mixed	GRID corpus (mixed-speech)
applied when the error on 	GRID corpus (mixed	GRID corpus (mixed-speech)
the validation set did not 	GRID corpus (mixed	GRID corpus (mixed-speech)
decrease over 5 consecutive epochs.  VL2M, AV concat and AV	GRID corpus (mixed	GRID corpus (mixed-speech)
 concat-ref had 5, 3 and	GRID corpus (mixed	GRID corpus (mixed-speech)
 3 stacked BLSTM layers respectively	GRID corpus (mixed	GRID corpus (mixed-speech)
. All BLSTMs had 250 	GRID corpus (mixed	GRID corpus (mixed-speech)
units. Hyper-parameters selection was performed 	GRID corpus (mixed	GRID corpus (mixed-speech)
by using random search with 	GRID corpus (mixed	GRID corpus (mixed-speech)
a limited number of samples, 	GRID corpus (mixed	GRID corpus (mixed-speech)
therefore all the reported results 	GRID corpus (mixed	GRID corpus (mixed-speech)
may improve through a deeper 	GRID corpus (mixed	GRID corpus (mixed-speech)
hyper- parameters validation phase.  VL2M ref and AV concat-ref	GRID corpus (mixed	GRID corpus (mixed-speech)
 training was performed in 2	GRID corpus (mixed	GRID corpus (mixed-speech)
 steps. We first pre-trained the	GRID corpus (mixed	GRID corpus (mixed-speech)
 models using the oracle TBM	GRID corpus (mixed	GRID corpus (mixed-speech)
 m. Then we substituted the	GRID corpus (mixed	GRID corpus (mixed-speech)
 oracle masks with the VL2M	GRID corpus (mixed	GRID corpus (mixed-speech)
 component and retrained the models	GRID corpus (mixed	GRID corpus (mixed-speech)
 while freezing the pa- rameters	GRID corpus (mixed	GRID corpus (mixed-speech)
 of the VL2M component	GRID corpus (mixed	GRID corpus (mixed-speech)
.  3.3. Audio pre- and post-processing	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
The original waveforms were resampled 	GRID corpus (mixed	GRID corpus (mixed-speech)
to 16 kHz. Short- Time 	GRID corpus (mixed	GRID corpus (mixed-speech)
Fourier Transform (STFT) x was 	GRID corpus (mixed	GRID corpus (mixed-speech)
computed using FFT size of 512, Hann window of length 25	GRID corpus (mixed	GRID corpus (mixed-speech)
 ms (400 samples), and hop	GRID corpus (mixed	GRID corpus (mixed-speech)
 length of 10 ms (160	GRID corpus (mixed	GRID corpus (mixed-speech)
 samples). The input spectro- gram	GRID corpus (mixed	GRID corpus (mixed-speech)
 was obtained taking the STFT	GRID corpus (mixed	GRID corpus (mixed-speech)
 magnitude and perform- ing power-law	GRID corpus (mixed	GRID corpus (mixed-speech)
 compression |x|p with p	GRID corpus (mixed	GRID corpus (mixed-speech)
 = 0.3. Finally we applied	GRID corpus (mixed	GRID corpus (mixed-speech)
 per-speaker 0-mean 1-std normalization	GRID corpus (mixed	GRID corpus (mixed-speech)
.  In the post-processing stage, the	GRID corpus (mixed	GRID corpus (mixed-speech)
 enhanced waveform gen- erated by	GRID corpus (mixed	GRID corpus (mixed-speech)
 the speech enhancement models was	GRID corpus (mixed	GRID corpus (mixed-speech)
 reconstructed	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
SDR PESQ ViSQOL  Noisy −1.06 1.81 2.11 VL2M	GRID corpus (mixed	GRID corpus (mixed-speech)
 3.17 1.51 1.16 VL2M ref	GRID corpus (mixed	GRID corpus (mixed-speech)
 6.50 2.58 2.99 AV concat	GRID corpus (mixed	GRID corpus (mixed-speech)
 6.31 2.49 2.83 AV c-ref	GRID corpus (mixed	GRID corpus (mixed-speech)
 6.17 2.58 2.96	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
Table 1. GRID results - 	GRID corpus (mixed	GRID corpus (mixed-speech)
speaker-dependent. The “Noisy” row refers 	GRID corpus (mixed	GRID corpus (mixed-speech)
to the metric values of 	GRID corpus (mixed	GRID corpus (mixed-speech)
the input mixed-speech signal.  2 Speakers 3 Speakers SDR	GRID corpus (mixed	GRID corpus (mixed-speech)
 PESQ ViSQOL SDR PESQ ViSQOL	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
Noisy 0.21 1.94 2.58 −5.34 1	GRID corpus (mixed	GRID corpus (mixed-speech)
.43 1.62 VL2M 3.02 1.81 1	GRID corpus (mixed	GRID corpus (mixed-speech)
.70 −2.03 1.43 1.25 VL2M 	GRID corpus (mixed	GRID corpus (mixed-speech)
ref 6.52 2.53 3.02 2.83 2	GRID corpus (mixed	GRID corpus (mixed-speech)
.19 2.53 AV concat 7.37 2	GRID corpus (mixed	GRID corpus (mixed-speech)
.65 3.03 3.02 2.24 2.49 	GRID corpus (mixed	GRID corpus (mixed-speech)
AV c-ref 8.05 2.70 3.07 4	GRID corpus (mixed	GRID corpus (mixed-speech)
.02 2.33 2.64  Table 2. GRID results	GRID corpus (mixed	GRID corpus (mixed-speech)
 - speaker-independent	GRID corpus (mixed	GRID corpus (mixed-speech)
.  by applying the inverse STFT	GRID corpus (mixed	GRID corpus (mixed-speech)
 to the estimated clean spectro	GRID corpus (mixed	GRID corpus (mixed-speech)
- gram and using the 	GRID corpus (mixed	GRID corpus (mixed-speech)
phase of the noisy input 	GRID corpus (mixed	GRID corpus (mixed-speech)
signal.  3.4. Video pre-processing	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
Face landmarks were extracted from 	GRID corpus (mixed	GRID corpus (mixed-speech)
video using the Dlib [7] 	GRID corpus (mixed	GRID corpus (mixed-speech)
implementation of the face landmark 	GRID corpus (mixed	GRID corpus (mixed-speech)
estimator described in [6]. It 	GRID corpus (mixed	GRID corpus (mixed-speech)
returns 68 x-y points, for 	GRID corpus (mixed	GRID corpus (mixed-speech)
an overall 136 values. We 	GRID corpus (mixed	GRID corpus (mixed-speech)
upsampled from 25/29.97 fps (GRID/TCD-TIMIT) 	GRID corpus (mixed	GRID corpus (mixed-speech)
to 100 fps to match 	GRID corpus (mixed	GRID corpus (mixed-speech)
the frame rate of the 	GRID corpus (mixed	GRID corpus (mixed-speech)
audio spectrogram. Upsampling was carried 	GRID corpus (mixed	GRID corpus (mixed-speech)
out through linear interpolation over 	GRID corpus (mixed	GRID corpus (mixed-speech)
time.  The final video feature vector	GRID corpus (mixed	GRID corpus (mixed-speech)
 v was obtained by com	GRID corpus (mixed	GRID corpus (mixed-speech)
- puting the per-speaker normalized 	GRID corpus (mixed	GRID corpus (mixed-speech)
motion vector of the face 	GRID corpus (mixed	GRID corpus (mixed-speech)
landmarks by simply subtracting every 	GRID corpus (mixed	GRID corpus (mixed-speech)
frame with the previ- ous 	GRID corpus (mixed	GRID corpus (mixed-speech)
one. The motion vector of 	GRID corpus (mixed	GRID corpus (mixed-speech)
the first frame was set 	GRID corpus (mixed	GRID corpus (mixed-speech)
to zero.  4. RESULTS	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
In order to compare our 	GRID corpus (mixed	GRID corpus (mixed-speech)
models to previous works in 	GRID corpus (mixed	GRID corpus (mixed-speech)
both speech enhancement and separation, 	GRID corpus (mixed	GRID corpus (mixed-speech)
we evaluated the perfor- mance 	GRID corpus (mixed	GRID corpus (mixed-speech)
of the proposed models using 	GRID corpus (mixed	GRID corpus (mixed-speech)
both speech separation  2 Speakers 3 Speakers SDR	GRID corpus (mixed	GRID corpus (mixed-speech)
 PESQ ViSQOL SDR PESQ ViSQOL	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
Noisy 0.21 2.22 2.74 −3.42 1	GRID corpus (mixed	GRID corpus (mixed-speech)
.92 2.04 VL2M 2.88 2.25 2	GRID corpus (mixed	GRID corpus (mixed-speech)
.62 −0.51 1.99 1.98 VL2M 	GRID corpus (mixed	GRID corpus (mixed-speech)
ref 9.24 2.81 3.09 5.27 2	GRID corpus (mixed	GRID corpus (mixed-speech)
.44 2.54 AV concat 9.56 2	GRID corpus (mixed	GRID corpus (mixed-speech)
.80 3.09 5.15 2.41 2.52 	GRID corpus (mixed	GRID corpus (mixed-speech)
AV c-ref 10.55 3.03 3.21 5	GRID corpus (mixed	GRID corpus (mixed-speech)
.37 2.45 2.58  Table 3. TCD-TIMIT results	GRID corpus (mixed	GRID corpus (mixed-speech)
 - speaker-independent	GRID corpus (mixed	GRID corpus (mixed-speech)
.  and enhancement metrics. Specifically, we	GRID corpus (mixed	GRID corpus (mixed-speech)
 measured the ca- pability of	GRID corpus (mixed	GRID corpus (mixed-speech)
 separating the target utterance from	GRID corpus (mixed	GRID corpus (mixed-speech)
 the concurrent utterance with the	GRID corpus (mixed	GRID corpus (mixed-speech)
 source-to-distortion ratio (SDR) [27, 28	GRID corpus (mixed	GRID corpus (mixed-speech)
]. While the quality of 	GRID corpus (mixed	GRID corpus (mixed-speech)
estimated target speech was measured 	GRID corpus (mixed	GRID corpus (mixed-speech)
with the perceptual PESQ [29] 	GRID corpus (mixed	GRID corpus (mixed-speech)
and ViSQOL [30] metrics. For 	GRID corpus (mixed	GRID corpus (mixed-speech)
PESQ we used the narrow 	GRID corpus (mixed	GRID corpus (mixed-speech)
band mode while for ViSQOL 	GRID corpus (mixed	GRID corpus (mixed-speech)
we used the wide band 	GRID corpus (mixed	GRID corpus (mixed-speech)
mode.  As a very first experiment	GRID corpus (mixed	GRID corpus (mixed-speech)
 we compared landmark posi- tion	GRID corpus (mixed	GRID corpus (mixed-speech)
 vs. landmark motion vectors. It	GRID corpus (mixed	GRID corpus (mixed-speech)
 turned out that landmark positions	GRID corpus (mixed	GRID corpus (mixed-speech)
 performed poorly, thus all results	GRID corpus (mixed	GRID corpus (mixed-speech)
 reported here refer to landmark	GRID corpus (mixed	GRID corpus (mixed-speech)
 motion vectors only	GRID corpus (mixed	GRID corpus (mixed-speech)
.  We then carried out some	GRID corpus (mixed	GRID corpus (mixed-speech)
 speaker-dependent experiments to compare our	GRID corpus (mixed	GRID corpus (mixed-speech)
 models with previous studies as	GRID corpus (mixed	GRID corpus (mixed-speech)
, to the best of 	GRID corpus (mixed	GRID corpus (mixed-speech)
our knowledge, there are no 	GRID corpus (mixed	GRID corpus (mixed-speech)
reported results of speaker- independent 	GRID corpus (mixed	GRID corpus (mixed-speech)
systems trained and tested on 	GRID corpus (mixed	GRID corpus (mixed-speech)
GRID and TCD- TIMIT to 	GRID corpus (mixed	GRID corpus (mixed-speech)
compare with. Table 1 reports 	GRID corpus (mixed	GRID corpus (mixed-speech)
the test-set evalua- tion of 	GRID corpus (mixed	GRID corpus (mixed-speech)
speaker-dependent models on the GRID 	GRID corpus (mixed	GRID corpus (mixed-speech)
corpus with landmark motion vectors. 	GRID corpus (mixed	GRID corpus (mixed-speech)
Results are comparable with previ- 	GRID corpus (mixed	GRID corpus (mixed-speech)
ous state-of-the-art studies in an 	GRID corpus (mixed	GRID corpus (mixed-speech)
almost identical setting [15, 17].  Table 2 and 3 show	GRID corpus (mixed	GRID corpus (mixed-speech)
 speaker-independent test-set results on the	GRID corpus (mixed	GRID corpus (mixed-speech)
 GRID and TCD-TIMIT datasets respectively	GRID corpus (mixed	GRID corpus (mixed-speech)
. V2ML performs significantly worse 	GRID corpus (mixed	GRID corpus (mixed-speech)
than the other three models 	GRID corpus (mixed	GRID corpus (mixed-speech)
in- dicating that a successful 	GRID corpus (mixed	GRID corpus (mixed-speech)
mask generation has to depend 	GRID corpus (mixed	GRID corpus (mixed-speech)
on the acoustic context. The 	GRID corpus (mixed	GRID corpus (mixed-speech)
performance of the three models 	GRID corpus (mixed	GRID corpus (mixed-speech)
in the speaker-independent setting is 	GRID corpus (mixed	GRID corpus (mixed-speech)
comparable to that in the 	GRID corpus (mixed	GRID corpus (mixed-speech)
speaker-dependent setting.  AV concat-ref outperforms V2ML ref	GRID corpus (mixed	GRID corpus (mixed-speech)
 and AV concat for both	GRID corpus (mixed	GRID corpus (mixed-speech)
 datasets. This supports the utility	GRID corpus (mixed	GRID corpus (mixed-speech)
 of a refinement strat- egy	GRID corpus (mixed	GRID corpus (mixed-speech)
 and suggests that the refinement	GRID corpus (mixed	GRID corpus (mixed-speech)
 is more effective when it	GRID corpus (mixed	GRID corpus (mixed-speech)
 directly refines the estimated clean	GRID corpus (mixed	GRID corpus (mixed-speech)
 spectrogram, rather than refining the	GRID corpus (mixed	GRID corpus (mixed-speech)
 estimated mask	GRID corpus (mixed	GRID corpus (mixed-speech)
.  Finally, we evaluated the systems	GRID corpus (mixed	GRID corpus (mixed-speech)
 in a more challenging testing	GRID corpus (mixed	GRID corpus (mixed-speech)
 condition where the target utterance	GRID corpus (mixed	GRID corpus (mixed-speech)
 was mixed with 2 utterances	GRID corpus (mixed	GRID corpus (mixed-speech)
 from 2 competing speakers. Despite	GRID corpus (mixed	GRID corpus (mixed-speech)
 the model was trained with	GRID corpus (mixed	GRID corpus (mixed-speech)
 mixtures of two speakers, the	GRID corpus (mixed	GRID corpus (mixed-speech)
 decrease of performance was not	GRID corpus (mixed	GRID corpus (mixed-speech)
 dramatic	GRID corpus (mixed	GRID corpus (mixed-speech)
.  Code and some testing examples	GRID corpus (mixed	GRID corpus (mixed-speech)
 of our models are avail	GRID corpus (mixed	GRID corpus (mixed-speech)
- able at https://goo.gl/3h1NgE.  5. CONCLUSION	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
This paper proposes the use 	GRID corpus (mixed	GRID corpus (mixed-speech)
of face landmark motion vec- 	GRID corpus (mixed	GRID corpus (mixed-speech)
tors for audio-visual speech enhancement 	GRID corpus (mixed	GRID corpus (mixed-speech)
in a single-channel multi-talker scenario. 	GRID corpus (mixed	GRID corpus (mixed-speech)
Different models are tested where 	GRID corpus (mixed	GRID corpus (mixed-speech)
land- mark motion vectors are 	GRID corpus (mixed	GRID corpus (mixed-speech)
used to generate time-frequency (T- 	GRID corpus (mixed	GRID corpus (mixed-speech)
F) masks that extract the 	GRID corpus (mixed	GRID corpus (mixed-speech)
target speaker’s spectrogram from the 	GRID corpus (mixed	GRID corpus (mixed-speech)
acoustic mixed-speech spectrogram.  To the best of our	GRID corpus (mixed	GRID corpus (mixed-speech)
 knowledge, some of the proposed	GRID corpus (mixed	GRID corpus (mixed-speech)
 mod- els are the first	GRID corpus (mixed	GRID corpus (mixed-speech)
 models trained and evaluated on	GRID corpus (mixed	GRID corpus (mixed-speech)
 the limited size GRID and	GRID corpus (mixed	GRID corpus (mixed-speech)
 TCD-TIMIT datasets that accomplish speaker	GRID corpus (mixed	GRID corpus (mixed-speech)
- independent speech enhancement in 	GRID corpus (mixed	GRID corpus (mixed-speech)
the multi-talker setting, with a 	GRID corpus (mixed	GRID corpus (mixed-speech)
quality of enhancement comparable to 	GRID corpus (mixed	GRID corpus (mixed-speech)
that achieved in a speaker-dependent 	GRID corpus (mixed	GRID corpus (mixed-speech)
setting.  https://goo.gl/3h1NgE	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
6. REFERENCES  [1] E. Colin Cherry, “Some	GRID corpus (mixed	GRID corpus (mixed-speech)
 experiments on the recognition of	GRID corpus (mixed	GRID corpus (mixed-speech)
 speech, with one and with	GRID corpus (mixed	GRID corpus (mixed-speech)
 two ears,” The Journal of	GRID corpus (mixed	GRID corpus (mixed-speech)
 the Acoustical Society of America	GRID corpus (mixed	GRID corpus (mixed-speech)
, vol. 25, no. 5, 	GRID corpus (mixed	GRID corpus (mixed-speech)
pp. 975–979, 1953.  [2] Josh H McDermott, “The	GRID corpus (mixed	GRID corpus (mixed-speech)
 cocktail party problem,” Current Biology	GRID corpus (mixed	GRID corpus (mixed-speech)
, vol. 19, no. 22, 	GRID corpus (mixed	GRID corpus (mixed-speech)
pp. R1024–R1027, 2009.  [3] Elana Zion Golumbic, Gregory	GRID corpus (mixed	GRID corpus (mixed-speech)
 B. Cogan, Charles E. Schroeder	GRID corpus (mixed	GRID corpus (mixed-speech)
, and David Poeppel, “Visual 	GRID corpus (mixed	GRID corpus (mixed-speech)
input enhances selective speech envelope 	GRID corpus (mixed	GRID corpus (mixed-speech)
tracking in auditory cortex at 	GRID corpus (mixed	GRID corpus (mixed-speech)
a “cocktail party”,” Journal of 	GRID corpus (mixed	GRID corpus (mixed-speech)
Neu- roscience, vol. 33, no. 4, pp. 1417–1426, 2013	GRID corpus (mixed	GRID corpus (mixed-speech)
.  [4] Wei Ji Ma, Xiang	GRID corpus (mixed	GRID corpus (mixed-speech)
 Zhou, Lars A. Ross, John	GRID corpus (mixed	GRID corpus (mixed-speech)
 J. Foxe, and Lucas C	GRID corpus (mixed	GRID corpus (mixed-speech)
. Parra, “Lip-reading aids word 	GRID corpus (mixed	GRID corpus (mixed-speech)
recognition most in moderate noise: 	GRID corpus (mixed	GRID corpus (mixed-speech)
A bayesian explanation using high-dimensional 	GRID corpus (mixed	GRID corpus (mixed-speech)
feature space,” PLOS ONE, vol. 4, no. 3, pp. 1–14, 03	GRID corpus (mixed	GRID corpus (mixed-speech)
 2009	GRID corpus (mixed	GRID corpus (mixed-speech)
.  [5] Albert S Bregman, Auditory	GRID corpus (mixed	GRID corpus (mixed-speech)
 scene analysis: The perceptual organi	GRID corpus (mixed	GRID corpus (mixed-speech)
- zation of sound, MIT 	GRID corpus (mixed	GRID corpus (mixed-speech)
press, 1994.  [6] Vahid Kazemi and Josephine	GRID corpus (mixed	GRID corpus (mixed-speech)
 Sullivan, “One millisecond face align	GRID corpus (mixed	GRID corpus (mixed-speech)
- ment with an ensemble 	GRID corpus (mixed	GRID corpus (mixed-speech)
of regression trees,” in The 	GRID corpus (mixed	GRID corpus (mixed-speech)
IEEE Conference on Computer Vision 	GRID corpus (mixed	GRID corpus (mixed-speech)
and Pattern Recognition (CVPR), June 2014	GRID corpus (mixed	GRID corpus (mixed-speech)
.  [7] Davis E. King, “Dlib-ml	GRID corpus (mixed	GRID corpus (mixed-speech)
: A machine learning toolkit,” 	GRID corpus (mixed	GRID corpus (mixed-speech)
Journal of Machine Learning Research, 	GRID corpus (mixed	GRID corpus (mixed-speech)
vol. 10, pp. 1755–1758, 2009.  [8] Yuxuan Wang, Arun Narayanan	GRID corpus (mixed	GRID corpus (mixed-speech)
, and DeLiang Wang, “On 	GRID corpus (mixed	GRID corpus (mixed-speech)
Training Targets for Supervised Speech 	GRID corpus (mixed	GRID corpus (mixed-speech)
Separation,” IEEE/ACM Transactions on Audio, 	GRID corpus (mixed	GRID corpus (mixed-speech)
Speech, and Language Processing, vol. 22, no. 12, pp. 1849–1858, Dec	GRID corpus (mixed	GRID corpus (mixed-speech)
. 2014.  [9] Martin Cooke, Jon Barker	GRID corpus (mixed	GRID corpus (mixed-speech)
, Stuart Cunningham, and Xu 	GRID corpus (mixed	GRID corpus (mixed-speech)
Shao, “An audio-visual corpus for 	GRID corpus (mixed	GRID corpus (mixed-speech)
speech perception and automatic speech 	GRID corpus (mixed	GRID corpus (mixed-speech)
recognition,” The Journal of the 	GRID corpus (mixed	GRID corpus (mixed-speech)
Acoustical Society of America, vol. 120, no. 5, pp. 2421–2424, Nov	GRID corpus (mixed	GRID corpus (mixed-speech)
. 2006.  [10] Naomi Harte and Eoin	GRID corpus (mixed	GRID corpus (mixed-speech)
 Gillen, “TCD-TIMIT: An Audio-Visual Cor	GRID corpus (mixed	GRID corpus (mixed-speech)
- pus of Continuous Speech,” 	GRID corpus (mixed	GRID corpus (mixed-speech)
IEEE Transactions on Multimedia, vol. 17, no. 5, pp. 603–615, May	GRID corpus (mixed	GRID corpus (mixed-speech)
 2015	GRID corpus (mixed	GRID corpus (mixed-speech)
.  [11] Z. Chen, Y. Luo	GRID corpus (mixed	GRID corpus (mixed-speech)
, and N. Mesgarani, “Deep 	GRID corpus (mixed	GRID corpus (mixed-speech)
attractor network for single-microphone speaker 	GRID corpus (mixed	GRID corpus (mixed-speech)
separation,” in 2017 IEEE International 	GRID corpus (mixed	GRID corpus (mixed-speech)
Conference on Acoustics, Speech and 	GRID corpus (mixed	GRID corpus (mixed-speech)
Signal Processing (ICASSP), March 2017, 	GRID corpus (mixed	GRID corpus (mixed-speech)
pp. 246–250.  [12] Yusuf Isik, Jonathan Le	GRID corpus (mixed	GRID corpus (mixed-speech)
 Roux, Zhuo Chen, Shinji Watanabe	GRID corpus (mixed	GRID corpus (mixed-speech)
, and John R. 	GRID corpus (mixed	GRID corpus (mixed-speech)
Hershey, “Single-channel multi-speaker separation using 	GRID corpus (mixed	GRID corpus (mixed-speech)
deep clustering,” in Interspeech, 2016.  [13] Morten Kolbaek, Dong Yu	GRID corpus (mixed	GRID corpus (mixed-speech)
, Zheng-Hua Tan, Jesper Jensen, 	GRID corpus (mixed	GRID corpus (mixed-speech)
Morten Kolbaek, Dong Yu, Zheng-Hua 	GRID corpus (mixed	GRID corpus (mixed-speech)
Tan, and Jesper Jensen, “Multitalker 	GRID corpus (mixed	GRID corpus (mixed-speech)
speech separation with utterance-level permutation 	GRID corpus (mixed	GRID corpus (mixed-speech)
invariant training of deep recurrent 	GRID corpus (mixed	GRID corpus (mixed-speech)
neural networks,” IEEE/ACM Trans. Audio, 	GRID corpus (mixed	GRID corpus (mixed-speech)
Speech and Lang. Proc., vol. 25, no. 10, pp. 1901–1913, Oct	GRID corpus (mixed	GRID corpus (mixed-speech)
. 2017.  [14] Bertrand Rivet, Wenwu Wang	GRID corpus (mixed	GRID corpus (mixed-speech)
, Syed Mohsen Naqvi, and 	GRID corpus (mixed	GRID corpus (mixed-speech)
Jonathon Chambers, “Audiovisual Speech Source 	GRID corpus (mixed	GRID corpus (mixed-speech)
Separation: An overview of key 	GRID corpus (mixed	GRID corpus (mixed-speech)
methodologies,” IEEE Signal Processing Magazine, 	GRID corpus (mixed	GRID corpus (mixed-speech)
vol. 31, no. 3, pp. 125	GRID corpus (mixed	GRID corpus (mixed-speech)
–134, May 2014.  [15] Aviv Gabbay, Ariel Ephrat	GRID corpus (mixed	GRID corpus (mixed-speech)
, Tavi Halperin, and Shmuel 	GRID corpus (mixed	GRID corpus (mixed-speech)
Peleg, “Seeing through noise: Visually 	GRID corpus (mixed	GRID corpus (mixed-speech)
driven speaker separation and enhancement,” 	GRID corpus (mixed	GRID corpus (mixed-speech)
in ICASSP. 2018, pp. 3051–3055, 	GRID corpus (mixed	GRID corpus (mixed-speech)
IEEE.  [16] Ariel Ephrat, Tavi Halperin	GRID corpus (mixed	GRID corpus (mixed-speech)
, and Shmuel Peleg, “Improved 	GRID corpus (mixed	GRID corpus (mixed-speech)
speech reconstruction from silent video,” 	GRID corpus (mixed	GRID corpus (mixed-speech)
ICCV 2017 Workshop on Computer 	GRID corpus (mixed	GRID corpus (mixed-speech)
Vision for Audio-Visual Media, 2017.  [17] Aviv Gabbay, Asaph Shamir	GRID corpus (mixed	GRID corpus (mixed-speech)
, and Shmuel Peleg, “Visual 	GRID corpus (mixed	GRID corpus (mixed-speech)
speech en- hancement,” in Interspeech. 2018, pp. 1170–1174, ISCA	GRID corpus (mixed	GRID corpus (mixed-speech)
.  [18] Jen-Cheng Hou, Syu-Siang Wang	GRID corpus (mixed	GRID corpus (mixed-speech)
, Ying-Hui Lai, Yu Tsao, 	GRID corpus (mixed	GRID corpus (mixed-speech)
Hsiu-Wen Chang, and Hsin-Min 	GRID corpus (mixed	GRID corpus (mixed-speech)
Wang, “Audio-Visual Speech Enhancement Us- 	GRID corpus (mixed	GRID corpus (mixed-speech)
ing Multimodal Deep Convolutional Neural 	GRID corpus (mixed	GRID corpus (mixed-speech)
Networks,” IEEE Trans- actions on 	GRID corpus (mixed	GRID corpus (mixed-speech)
Emerging Topics in Computational Intelligence, 	GRID corpus (mixed	GRID corpus (mixed-speech)
vol. 2, no. 2, pp. 117	GRID corpus (mixed	GRID corpus (mixed-speech)
–128, Apr. 2018.  [19] Jen-Cheng Hou, Syu-Siang Wang	GRID corpus (mixed	GRID corpus (mixed-speech)
, Ying-Hui Lai, Jen-Chun Lin, 	GRID corpus (mixed	GRID corpus (mixed-speech)
Yu Tsao, Hsiu-Wen Chang, and 	GRID corpus (mixed	GRID corpus (mixed-speech)
Hsin-Min Wang, “Audio-visual speech enhancement 	GRID corpus (mixed	GRID corpus (mixed-speech)
using deep neural networks,” in 2016 Asia- Pacific Signal and Information	GRID corpus (mixed	GRID corpus (mixed-speech)
 Processing Association Annual Sum- mit	GRID corpus (mixed	GRID corpus (mixed-speech)
 and Conference (APSIPA), Jeju, South	GRID corpus (mixed	GRID corpus (mixed-speech)
 Korea, Dec. 2016, pp. 1–6	GRID corpus (mixed	GRID corpus (mixed-speech)
, IEEE.  [20] Ariel Ephrat, Inbar Mosseri	GRID corpus (mixed	GRID corpus (mixed-speech)
, Oran Lang, Tali Dekel, 	GRID corpus (mixed	GRID corpus (mixed-speech)
Kevin Wilson, Avinatan Hassidim, William 	GRID corpus (mixed	GRID corpus (mixed-speech)
T. Freeman, and Michael 	GRID corpus (mixed	GRID corpus (mixed-speech)
Rubinstein, “Looking to Listen at 	GRID corpus (mixed	GRID corpus (mixed-speech)
the Cocktail Party: A Speaker-Independent 	GRID corpus (mixed	GRID corpus (mixed-speech)
Audio-Visual Model for Speech Separation,” 	GRID corpus (mixed	GRID corpus (mixed-speech)
ACM Transactions on Graphics, vol. 37, no. 4, pp. 1–11, July	GRID corpus (mixed	GRID corpus (mixed-speech)
 2018, arXiv: 1804.03619	GRID corpus (mixed	GRID corpus (mixed-speech)
.  [21] T. Afouras, J. S	GRID corpus (mixed	GRID corpus (mixed-speech)
. Chung, and A. 	GRID corpus (mixed	GRID corpus (mixed-speech)
Zisserman, “The conversation: Deep audio-visual 	GRID corpus (mixed	GRID corpus (mixed-speech)
speech enhancement,” in Interspeech, 2018.  [22] Andrew Owens and Alexei	GRID corpus (mixed	GRID corpus (mixed-speech)
 A Efros, “Audio-visual scene analysis	GRID corpus (mixed	GRID corpus (mixed-speech)
 with self-supervised multisensory features,” European	GRID corpus (mixed	GRID corpus (mixed-speech)
 Conference on Computer Vision (ECCV	GRID corpus (mixed	GRID corpus (mixed-speech)
), 2018.  [23] Michael C. Anzalone, Lauren	GRID corpus (mixed	GRID corpus (mixed-speech)
 Calandruccio, Karen A. Doherty, and	GRID corpus (mixed	GRID corpus (mixed-speech)
 Laurel H. Carney, “Determination of	GRID corpus (mixed	GRID corpus (mixed-speech)
 the potential benefit of time	GRID corpus (mixed	GRID corpus (mixed-speech)
- frequency gain manipulation,” Ear 	GRID corpus (mixed	GRID corpus (mixed-speech)
Hear, vol. 27, no. 5, 	GRID corpus (mixed	GRID corpus (mixed-speech)
pp. 480–492, Oct 2006, 16957499[pmid].  [24] Ulrik Kjems, Jesper B	GRID corpus (mixed	GRID corpus (mixed-speech)
. Boldt, Michael S. Pedersen, 	GRID corpus (mixed	GRID corpus (mixed-speech)
Thomas Lunner, and DeLiang 	GRID corpus (mixed	GRID corpus (mixed-speech)
Wang, “Role of mask pattern 	GRID corpus (mixed	GRID corpus (mixed-speech)
in intelligibility of ideal binary-masked 	GRID corpus (mixed	GRID corpus (mixed-speech)
noisy speech,” The Journal of 	GRID corpus (mixed	GRID corpus (mixed-speech)
the Acoustical Society of America, 	GRID corpus (mixed	GRID corpus (mixed-speech)
vol. 126, no. 3, pp. 1415	GRID corpus (mixed	GRID corpus (mixed-speech)
–1426, 2009.  [25] A. Graves, A. Mohamed	GRID corpus (mixed	GRID corpus (mixed-speech)
, and G. Hinton, “Speech 	GRID corpus (mixed	GRID corpus (mixed-speech)
recognition with deep recurrent neural 	GRID corpus (mixed	GRID corpus (mixed-speech)
networks,” in 2013 IEEE International 	GRID corpus (mixed	GRID corpus (mixed-speech)
Con- ference on Acoustics, Speech 	GRID corpus (mixed	GRID corpus (mixed-speech)
and Signal Processing, May 2013, 	GRID corpus (mixed	GRID corpus (mixed-speech)
pp. 6645–6649.  [26] Diederik P Kingma and	GRID corpus (mixed	GRID corpus (mixed-speech)
 Jimmy Ba, “Adam: A method	GRID corpus (mixed	GRID corpus (mixed-speech)
 for stochastic optimization,” arXiv preprint	GRID corpus (mixed	GRID corpus (mixed-speech)
 arXiv:1412.6980, 2014	GRID corpus (mixed	GRID corpus (mixed-speech)
.  [27] E. Vincent, R. Gribonval	GRID corpus (mixed	GRID corpus (mixed-speech)
, and C. Fevotte, “Performance 	GRID corpus (mixed	GRID corpus (mixed-speech)
measure- ment in blind audio 	GRID corpus (mixed	GRID corpus (mixed-speech)
source separation,” IEEE Transactions on 	GRID corpus (mixed	GRID corpus (mixed-speech)
Audio, Speech and Language Processing, 	GRID corpus (mixed	GRID corpus (mixed-speech)
vol. 14, no. 4, pp. 1462	GRID corpus (mixed	GRID corpus (mixed-speech)
–1469, July 2006.  [28] Colin Raffel, Brian McFee	GRID corpus (mixed	GRID corpus (mixed-speech)
, Eric J Humphrey, Justin 	GRID corpus (mixed	GRID corpus (mixed-speech)
Salamon, Oriol Nieto, Dawen Liang, 	GRID corpus (mixed	GRID corpus (mixed-speech)
Daniel PW Ellis, and C 	GRID corpus (mixed	GRID corpus (mixed-speech)
Colin Raffel, “mir eval: A 	GRID corpus (mixed	GRID corpus (mixed-speech)
transparent implementation of common mir 	GRID corpus (mixed	GRID corpus (mixed-speech)
metrics,” in In Proceed- ings 	GRID corpus (mixed	GRID corpus (mixed-speech)
of the 15th International Society 	GRID corpus (mixed	GRID corpus (mixed-speech)
for Music Information Retrieval Conference, 	GRID corpus (mixed	GRID corpus (mixed-speech)
ISMIR. Citeseer, 2014.  [29] A.W. Rix, J.G. Beerends	GRID corpus (mixed	GRID corpus (mixed-speech)
, M.P. Hollier, and A.P. 	GRID corpus (mixed	GRID corpus (mixed-speech)
Hekstra, “Perceptual evaluation of speech 	GRID corpus (mixed	GRID corpus (mixed-speech)
quality (PESQ)-a new method for 	GRID corpus (mixed	GRID corpus (mixed-speech)
speech qual- ity assessment of 	GRID corpus (mixed	GRID corpus (mixed-speech)
telephone networks and codecs,” in 2001 IEEE In- ternational Conference on	GRID corpus (mixed	GRID corpus (mixed-speech)
 Acoustics, Speech, and Signal Processing	GRID corpus (mixed	GRID corpus (mixed-speech)
. Proceedings (Cat. No.01CH37221), Salt 	GRID corpus (mixed	GRID corpus (mixed-speech)
Lake City, UT, USA, 2001, 	GRID corpus (mixed	GRID corpus (mixed-speech)
vol. 2, pp. 749–752, IEEE.  [30] A. Hines, J. Skoglund	GRID corpus (mixed	GRID corpus (mixed-speech)
, A. Kokaram, and N. 	GRID corpus (mixed	GRID corpus (mixed-speech)
Harte, “ViSQOL: The Virtual Speech 	GRID corpus (mixed	GRID corpus (mixed-speech)
Quality Objective Listener,” in IWAENC 2012	GRID corpus (mixed	GRID corpus (mixed-speech)
; Inter- national Workshop on 	GRID corpus (mixed	GRID corpus (mixed-speech)
Acoustic Signal Enhancement, Sept. 2012, 	GRID corpus (mixed	GRID corpus (mixed-speech)
pp. 1	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
4	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
1  Introduction	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
1.1  Related work	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
2  MODEL ARCHITECTURES	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
2.1  VL2M model	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
2.2  VL2M_ref model	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
2.3  Audio-Visual concat model	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
2.4  Audio-Visual concat-ref model	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
3  Experimental setup	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
3.1  Dataset	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
3.2  LSTM training	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
3.3  Audio pre- and post-processing	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
3.4  Video pre-processing	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
4  Results	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
5  Conclusion	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
6  References	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
evaluated on the limited size GRID and TCD-TIMIT datasets, that achieve	GRID	GRID corpus (mixed-speech)
trained and evaluated on the GRID [9] and TCD-TIMIT [10] datasets	GRID	GRID corpus (mixed-speech)
were carried out using the GRID [9] and TCD-TIMIT [10] audio-visual	GRID	GRID corpus (mixed-speech)
Regarding the GRID corpus, for each of the	GRID	GRID corpus (mixed-speech)
the same procedure as for GRID, with one difference. Con- trary	GRID	GRID corpus (mixed-speech)
to GRID, TCD-TIMIT utterances have different dura	GRID	GRID corpus (mixed-speech)
Table 1. GRID results - speaker-dependent. The “Noisy	GRID	GRID corpus (mixed-speech)
Table 2. GRID results - speaker-independent	GRID	GRID corpus (mixed-speech)
We upsampled from 25/29.97 fps (GRID	GRID	GRID corpus (mixed-speech)
systems trained and tested on GRID and TCD- TIMIT to compare	GRID	GRID corpus (mixed-speech)
of speaker-dependent models on the GRID corpus with landmark motion vectors	GRID	GRID corpus (mixed-speech)
speaker-independent test-set results on the GRID and TCD-TIMIT datasets respectively. V2ML	GRID	GRID corpus (mixed-speech)
evaluated on the limited size GRID and TCD-TIMIT datasets that accomplish	GRID	GRID corpus (mixed-speech)
are applied to the acoustic mixed-speech spectrogram. Results show that: (i	mixed-speech	GRID corpus (mixed-speech)
landmark features and the input mixed-speech spectrogram	mixed-speech	GRID corpus (mixed-speech)
applied to clean the acoustic mixed-speech spectrogram	mixed-speech	GRID corpus (mixed-speech)
compressed spectrogram of the single-channel mixed-speech signal. All of them perform	mixed-speech	GRID corpus (mixed-speech)
although IAM generation requires the mixed-speech spectrogram, separate spectrograms for each	mixed-speech	GRID corpus (mixed-speech)
of them, we created a mixed-speech version	mixed-speech	GRID corpus (mixed-speech)
98 utterances per speaker. The mixed-speech version was created following the	mixed-speech	GRID corpus (mixed-speech)
metric values of the input mixed-speech signal	mixed-speech	GRID corpus (mixed-speech)
speaker’s spectrogram from the acoustic mixed-speech spectrogram	mixed-speech	GRID corpus (mixed-speech)
are applied to the acoustic mixed-speech spectrogram. Results show that: (i	(mixed-speech)	GRID corpus (mixed-speech)
landmark features and the input mixed-speech spectrogram	(mixed-speech)	GRID corpus (mixed-speech)
applied to clean the acoustic mixed-speech spectrogram	(mixed-speech)	GRID corpus (mixed-speech)
compressed spectrogram of the single-channel mixed-speech signal. All of them perform	(mixed-speech)	GRID corpus (mixed-speech)
although IAM generation requires the mixed-speech spectrogram, separate spectrograms for each	(mixed-speech)	GRID corpus (mixed-speech)
of them, we created a mixed-speech version	(mixed-speech)	GRID corpus (mixed-speech)
98 utterances per speaker. The mixed-speech version was created following the	(mixed-speech)	GRID corpus (mixed-speech)
metric values of the input mixed-speech signal	(mixed-speech)	GRID corpus (mixed-speech)
speaker’s spectrogram from the acoustic mixed-speech spectrogram	(mixed-speech)	GRID corpus (mixed-speech)
Regarding the GRID corpus, for each of the 33	corpus	GRID corpus (mixed-speech)
The TCD-TIMIT corpus consists of 59 speakers (we	corpus	GRID corpus (mixed-speech)
speaker-dependent models on the GRID corpus with landmark motion vectors. Results	corpus	GRID corpus (mixed-speech)
and Xu Shao, “An audio-visual corpus for speech perception and automatic	corpus	GRID corpus (mixed-speech)
Regarding the GRID corpus, for each of the 33	speech)	GRID corpus (mixed-speech)
The TCD-TIMIT corpus consists of 59 speakers (we	speech)	GRID corpus (mixed-speech)
speaker-dependent models on the GRID corpus with landmark motion vectors. Results	speech)	GRID corpus (mixed-speech)
and Xu Shao, “An audio-visual corpus for speech perception and automatic	speech)	GRID corpus (mixed-speech)
Regarding the GRID corpus, for each of the 33	GRID corpus	GRID corpus (mixed-speech)
of speaker-dependent models on the GRID corpus with landmark motion vectors. Results	GRID corpus	GRID corpus (mixed-speech)
FACE LANDMARK-BASED SPEAKER-INDEPENDENT AUDIO-VISUAL SPEECH 		GRID corpus (mixed-speech)
ENHANCEMENT IN MULTI-TALKER ENVIRONMENTS  Giovanni Morrone? Luca Pasa† Vadim		GRID corpus (mixed-speech)
 Tikhanoff		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
Sonia Bergamaschi? Luciano Fadiga† Leonardo 		GRID corpus (mixed-speech)
Badino†  ?Department of Engineering ”Enzo Ferrari		GRID corpus (mixed-speech)
”, University of Modena and 		GRID corpus (mixed-speech)
Reggio Emilia, Modena, Italy †Istituto 		GRID corpus (mixed-speech)
Italiano di Tecnologia, Ferrara, Italy  ABSTRACT		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
In this paper, we address 		GRID corpus (mixed-speech)
the problem of enhancing the 		GRID corpus (mixed-speech)
speech of a speaker of 		GRID corpus (mixed-speech)
interest in a cocktail party 		GRID corpus (mixed-speech)
scenario when vi- sual information 		GRID corpus (mixed-speech)
of the speaker of interest 		GRID corpus (mixed-speech)
is available.  Contrary to most previous studies		GRID corpus (mixed-speech)
, we do not learn 		GRID corpus (mixed-speech)
visual features on the typically 		GRID corpus (mixed-speech)
small audio-visual datasets, but use 		GRID corpus (mixed-speech)
an already available face landmark 		GRID corpus (mixed-speech)
detector (trained on a sep- 		GRID corpus (mixed-speech)
arate image dataset).  The landmarks are used by		GRID corpus (mixed-speech)
 LSTM-based models to gen- erate		GRID corpus (mixed-speech)
 time-frequency masks which are applied		GRID corpus (mixed-speech)
 to the acoustic mixed-speech spectrogram		GRID corpus (mixed-speech)
. Results show that: (i) 		GRID corpus (mixed-speech)
land- mark motion features are 		GRID corpus (mixed-speech)
very effective features for this 		GRID corpus (mixed-speech)
task, (ii) similarly to previous 		GRID corpus (mixed-speech)
work, reconstruction of the target 		GRID corpus (mixed-speech)
speaker’s spectrogram mediated by masking 		GRID corpus (mixed-speech)
is significantly more accurate than 		GRID corpus (mixed-speech)
direct spectrogram reconstruction, and (iii) 		GRID corpus (mixed-speech)
the best masks depend on 		GRID corpus (mixed-speech)
both motion landmark features and 		GRID corpus (mixed-speech)
the input mixed-speech spectrogram.  To the best of our		GRID corpus (mixed-speech)
 knowledge, our proposed models are		GRID corpus (mixed-speech)
 the first models trained and		GRID corpus (mixed-speech)
 evaluated on the limited size		GRID corpus (mixed-speech)
 GRID and TCD-TIMIT datasets, that		GRID corpus (mixed-speech)
 achieve speaker-independent speech enhancement in		GRID corpus (mixed-speech)
 a multi-talker setting		GRID corpus (mixed-speech)
.  Index Terms— audio-visual speech enhancement		GRID corpus (mixed-speech)
, cock- tail party problem, 		GRID corpus (mixed-speech)
time-frequency mask, LSTM, face land- 		GRID corpus (mixed-speech)
marks  1. INTRODUCTION		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
In the context of speech 		GRID corpus (mixed-speech)
perception, the cocktail party 		GRID corpus (mixed-speech)
effect [1, 2] is the 		GRID corpus (mixed-speech)
ability of the brain to 		GRID corpus (mixed-speech)
recognize speech in complex and 		GRID corpus (mixed-speech)
adverse listening conditions where the 		GRID corpus (mixed-speech)
attended speech is mixed with 		GRID corpus (mixed-speech)
competing sounds/speech.  Speech perception studies have shown		GRID corpus (mixed-speech)
 that watching speaker’s face movements		GRID corpus (mixed-speech)
 could dramatically improve our ability		GRID corpus (mixed-speech)
 at recognizing the speech of		GRID corpus (mixed-speech)
 a target speaker in a		GRID corpus (mixed-speech)
 multi-talker environment [3, 4		GRID corpus (mixed-speech)
].  This work aims at extracting		GRID corpus (mixed-speech)
 the speech of a target		GRID corpus (mixed-speech)
 speaker from single channel audio		GRID corpus (mixed-speech)
 of several people talking simulta		GRID corpus (mixed-speech)
- neously. This is an 		GRID corpus (mixed-speech)
ill-posed problem in that many 		GRID corpus (mixed-speech)
differ- ent hypotheses about what 		GRID corpus (mixed-speech)
the target speaker says are 		GRID corpus (mixed-speech)
con-  sistent with the mixture signal		GRID corpus (mixed-speech)
. Yet, it can be 		GRID corpus (mixed-speech)
solved by ex- ploiting some 		GRID corpus (mixed-speech)
additional information associated to the 		GRID corpus (mixed-speech)
speaker of interest and/or by 		GRID corpus (mixed-speech)
leveraging some prior knowledge about 		GRID corpus (mixed-speech)
speech signal properties (e.g., [5]). 		GRID corpus (mixed-speech)
In this work we use 		GRID corpus (mixed-speech)
face movements of the target 		GRID corpus (mixed-speech)
speaker as additional information.  This paper (i) proposes the		GRID corpus (mixed-speech)
 use of face landmark’s move		GRID corpus (mixed-speech)
- ments, extracted using 		GRID corpus (mixed-speech)
Dlib [6, 7] and (ii) 		GRID corpus (mixed-speech)
compares differ- ent ways of 		GRID corpus (mixed-speech)
mapping such visual features into 		GRID corpus (mixed-speech)
time-frequency (T-F) masks, then applied 		GRID corpus (mixed-speech)
to clean the acoustic mixed-speech 		GRID corpus (mixed-speech)
spectrogram.  By using Dlib extracted landmarks		GRID corpus (mixed-speech)
 we relieve our mod- els		GRID corpus (mixed-speech)
 from the task of learning		GRID corpus (mixed-speech)
 useful visual features from raw		GRID corpus (mixed-speech)
 pixels. That aspect is particularly		GRID corpus (mixed-speech)
 relevant when the training audio-visual		GRID corpus (mixed-speech)
 datasets are small		GRID corpus (mixed-speech)
.  The analysis of landmark-dependent masking		GRID corpus (mixed-speech)
 strategies is motivated by the		GRID corpus (mixed-speech)
 fact that speech enhancement mediated		GRID corpus (mixed-speech)
 by an explicit masking is		GRID corpus (mixed-speech)
 often more effective than mask-free		GRID corpus (mixed-speech)
 enhancement [8		GRID corpus (mixed-speech)
].  All our models were trained		GRID corpus (mixed-speech)
 and evaluated on the GRID		GRID corpus (mixed-speech)
 [9] and TCD-TIMIT [10] datasets		GRID corpus (mixed-speech)
 in a speaker-independent setting		GRID corpus (mixed-speech)
.  1.1. Related work		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
Speech enhancement aims at extracting 		GRID corpus (mixed-speech)
the voice of a tar- 		GRID corpus (mixed-speech)
get speaker, while speech separation 		GRID corpus (mixed-speech)
refers to the problem of 		GRID corpus (mixed-speech)
separating each sound source in 		GRID corpus (mixed-speech)
a mixture. Recently pro- posed 		GRID corpus (mixed-speech)
audio-only single-channel methods have achieved 		GRID corpus (mixed-speech)
very promising results [11, 12, 13		GRID corpus (mixed-speech)
]. However the task still 		GRID corpus (mixed-speech)
remains challenging. Additionally, audio-only systems 		GRID corpus (mixed-speech)
need separate models in order 		GRID corpus (mixed-speech)
to associate the estimated separated 		GRID corpus (mixed-speech)
audio sources to each speaker, 		GRID corpus (mixed-speech)
while vision easily allow that 		GRID corpus (mixed-speech)
in a unified model.  Regarding audio-visual speech enhancement and		GRID corpus (mixed-speech)
 separa- tion methods an extensive		GRID corpus (mixed-speech)
 review is provided in [14		GRID corpus (mixed-speech)
]. Here we focus on 		GRID corpus (mixed-speech)
the deep-learning methods that are 		GRID corpus (mixed-speech)
most related to the present 		GRID corpus (mixed-speech)
work.  Our first architecture (Section 2.1		GRID corpus (mixed-speech)
) is inspired by [15], 		GRID corpus (mixed-speech)
where a pre-trained convolutional neural 		GRID corpus (mixed-speech)
network (CNN) is used to 		GRID corpus (mixed-speech)
generate a clean spectrogram from 		GRID corpus (mixed-speech)
silent video [16]. Rather than 		GRID corpus (mixed-speech)
directly computing a time-frequency (T-F) 		GRID corpus (mixed-speech)
mask,  ar X		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
iv :1  81 1		GRID corpus (mixed-speech)
.  02 48		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
0v 3		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
cs  .C L		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
2		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
M 		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
2 01  9		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
the mask is computed by 		GRID corpus (mixed-speech)
thresholding the estimated clean spectrogram. 		GRID corpus (mixed-speech)
This approach is not very 		GRID corpus (mixed-speech)
effective since the pre-trained CNN 		GRID corpus (mixed-speech)
is designed for a different 		GRID corpus (mixed-speech)
task (video-to- speech synthesis). 		GRID corpus (mixed-speech)
In [17] a CNN is 		GRID corpus (mixed-speech)
trained to directly esti- mate 		GRID corpus (mixed-speech)
clean speech from noisy audio 		GRID corpus (mixed-speech)
and input video. A sim- 		GRID corpus (mixed-speech)
ilar model is used 		GRID corpus (mixed-speech)
in [18], where the model 		GRID corpus (mixed-speech)
jointly generates clean speech and 		GRID corpus (mixed-speech)
input video in a denoising-autoender 		GRID corpus (mixed-speech)
archi- tecture.  [19] shows that using information		GRID corpus (mixed-speech)
 about lip positions can help		GRID corpus (mixed-speech)
 to improve speech enhancement. The		GRID corpus (mixed-speech)
 video feature vec- tor is		GRID corpus (mixed-speech)
 obtained computing pair-wise distances between		GRID corpus (mixed-speech)
 any mouth landmarks. Similarly to		GRID corpus (mixed-speech)
 our approach their visual fea		GRID corpus (mixed-speech)
- tures are not learned 		GRID corpus (mixed-speech)
on the audio-visual dataset but 		GRID corpus (mixed-speech)
are pro- vided by a 		GRID corpus (mixed-speech)
system trained on different dataset. 		GRID corpus (mixed-speech)
Contrary to our approach, [19] 		GRID corpus (mixed-speech)
uses position-based features while we 		GRID corpus (mixed-speech)
use motion features (of the 		GRID corpus (mixed-speech)
whole face) that in our 		GRID corpus (mixed-speech)
experiments turned out to be 		GRID corpus (mixed-speech)
much more effective than positional 		GRID corpus (mixed-speech)
features.  Although the aforementioned audio-visual methods		GRID corpus (mixed-speech)
 work well, they have only		GRID corpus (mixed-speech)
 been evaluated in a speaker-dependent		GRID corpus (mixed-speech)
 setting. Only the availability of		GRID corpus (mixed-speech)
 new large and heterogeneous audio-visual		GRID corpus (mixed-speech)
 datasets has allowed the training		GRID corpus (mixed-speech)
 of deep neu- ral network-based		GRID corpus (mixed-speech)
 speaker-independent speech enhancement models [20		GRID corpus (mixed-speech)
, 21, 22].  The present work shows that		GRID corpus (mixed-speech)
 huge audio-visual datasets are not		GRID corpus (mixed-speech)
 a necessary requirement for speaker-independent		GRID corpus (mixed-speech)
 audio-visual speech enhancement. Although we		GRID corpus (mixed-speech)
 have only considered datasets with		GRID corpus (mixed-speech)
 simple visual scenarios (i.e., the		GRID corpus (mixed-speech)
 target speaker is always facing		GRID corpus (mixed-speech)
 the camera), we expect our		GRID corpus (mixed-speech)
 methods to perform well in		GRID corpus (mixed-speech)
 more complex scenarios thanks to		GRID corpus (mixed-speech)
 the robust landmark extraction		GRID corpus (mixed-speech)
.  2. MODEL ARCHITECTURES		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
We experimented with the four 		GRID corpus (mixed-speech)
models shown in Fig. 1. 		GRID corpus (mixed-speech)
All models receive in input 		GRID corpus (mixed-speech)
the target speaker’s landmark mo- 		GRID corpus (mixed-speech)
tion vectors and the power-law 		GRID corpus (mixed-speech)
compressed spectrogram of the single-channel 		GRID corpus (mixed-speech)
mixed-speech signal. All of them 		GRID corpus (mixed-speech)
perform some kind of masking 		GRID corpus (mixed-speech)
operation.  2.1. VL2M model		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
At each time frame, the 		GRID corpus (mixed-speech)
video-landmark to mask (VL2M) model (		GRID corpus (mixed-speech)
Fig. 1a) estimates a T-F 		GRID corpus (mixed-speech)
mask from visual features only (		GRID corpus (mixed-speech)
of the target speaker). Formally, 		GRID corpus (mixed-speech)
given a video sequence 		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
 = [v1		GRID corpus (mixed-speech)
, . . . , 		GRID corpus (mixed-speech)
vT ], vt ∈ Rn 		GRID corpus (mixed-speech)
and a target mask sequence 		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
 = [m1		GRID corpus (mixed-speech)
, . . . ,		GRID corpus (mixed-speech)
mT ], mt ∈ Rd, 		GRID corpus (mixed-speech)
VL2M perform a function 		GRID corpus (mixed-speech)
Fvl2m(v) = m̂, where m̂ 		GRID corpus (mixed-speech)
is the estimated mask.  The training objective for VL2M		GRID corpus (mixed-speech)
 is a Target Binary Mask		GRID corpus (mixed-speech)
 (TBM) [23, 24], computed using		GRID corpus (mixed-speech)
 the spectrogram of the tar		GRID corpus (mixed-speech)
- get speaker only. This 		GRID corpus (mixed-speech)
is motivated by our goal 		GRID corpus (mixed-speech)
of extracting the speech of 		GRID corpus (mixed-speech)
a target speaker as much 		GRID corpus (mixed-speech)
as possible indepen- dently of 		GRID corpus (mixed-speech)
the concurrent speakers, so that, 		GRID corpus (mixed-speech)
e.g., we do not need 		GRID corpus (mixed-speech)
to estimate their number. An 		GRID corpus (mixed-speech)
additional motivations is that the 		GRID corpus (mixed-speech)
model takes as only input 		GRID corpus (mixed-speech)
the visual features of the  target speaker, and a target		GRID corpus (mixed-speech)
 TBM that only depends on		GRID corpus (mixed-speech)
 the target speaker allows VL2M		GRID corpus (mixed-speech)
 to learn a function (rather		GRID corpus (mixed-speech)
 than approximating an ill-posed one-to-many		GRID corpus (mixed-speech)
 mapping		GRID corpus (mixed-speech)
).  Given a clean speech spectrogram		GRID corpus (mixed-speech)
 of a speaker s		GRID corpus (mixed-speech)
 = [s1		GRID corpus (mixed-speech)
, . . . , 		GRID corpus (mixed-speech)
sT ], st ∈ Rd, 		GRID corpus (mixed-speech)
the TBM is defined by 		GRID corpus (mixed-speech)
comparing, at each frequency bin 		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
 ∈ [1		GRID corpus (mixed-speech)
, . . . , 		GRID corpus (mixed-speech)
d], the target speaker value 		GRID corpus (mixed-speech)
st[f ] vs. a reference 		GRID corpus (mixed-speech)
threshold τ [f ]. As 		GRID corpus (mixed-speech)
in [15], we use a 		GRID corpus (mixed-speech)
function of long-term average speech 		GRID corpus (mixed-speech)
spectrum (LTASS) as reference threshold. 		GRID corpus (mixed-speech)
This threshold indicates if a 		GRID corpus (mixed-speech)
T-F unit is generated by 		GRID corpus (mixed-speech)
the speaker or refers to 		GRID corpus (mixed-speech)
silence or noise. The process 		GRID corpus (mixed-speech)
to compute the speaker’s TBM 		GRID corpus (mixed-speech)
is as follows:  1. The mean π[f		GRID corpus (mixed-speech)
 ] and the standard deviation		GRID corpus (mixed-speech)
 σ[f ] are computed for		GRID corpus (mixed-speech)
 all frequency bins of all		GRID corpus (mixed-speech)
 seen spectro- grams in speaker’s		GRID corpus (mixed-speech)
 data		GRID corpus (mixed-speech)
.  2. The threshold τ [f		GRID corpus (mixed-speech)
 ] is defined as τ		GRID corpus (mixed-speech)
 [f ] = π[f ]+0.6		GRID corpus (mixed-speech)
 ·σ[f ] where 0.6 is		GRID corpus (mixed-speech)
 a value selected by manual		GRID corpus (mixed-speech)
 inspection of several spectrogram-TBM pairs		GRID corpus (mixed-speech)
.  3. The threshold is applied		GRID corpus (mixed-speech)
 to every speaker’s speech spec		GRID corpus (mixed-speech)
- trogram s.  mt[f		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
1, if st[f ] ≥ 		GRID corpus (mixed-speech)
τ [f ], 0, otherwise.  The mapping Fvl2m(·) is carried		GRID corpus (mixed-speech)
 out by a stacked bi		GRID corpus (mixed-speech)
- directional Long Short-Term Memory (		GRID corpus (mixed-speech)
BLSTM) network [25]. The BLSTM 		GRID corpus (mixed-speech)
outputs are then forced to 		GRID corpus (mixed-speech)
lay within the [0, 1] 		GRID corpus (mixed-speech)
range. Finally the computed TBM 		GRID corpus (mixed-speech)
m̂ and the noisy spectrogram 		GRID corpus (mixed-speech)
y are element-wise multiplied to 		GRID corpus (mixed-speech)
ob- tain the estimated clean 		GRID corpus (mixed-speech)
spectrogram ŝm = m̂ ◦ 		GRID corpus (mixed-speech)
y, where 		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
 = [y1		GRID corpus (mixed-speech)
, . . . 		GRID corpus (mixed-speech)
yT ], yt ∈ Rd.  The model parameters are estimated		GRID corpus (mixed-speech)
 to minimize the loss		GRID corpus (mixed-speech)
:  Jvl2m = ∑T		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
t=1  ∑d f=1−mt[f ] · log(m̂t[f		GRID corpus (mixed-speech)
 ])− (1−mt[f ]) · log(1		GRID corpus (mixed-speech)
− m̂t[f ])  2.2. VL2M ref model		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
VL2M generates T-F masks that 		GRID corpus (mixed-speech)
are independent of the acous- 		GRID corpus (mixed-speech)
tic context. We may want 		GRID corpus (mixed-speech)
to refine the masking by 		GRID corpus (mixed-speech)
including such context. This is 		GRID corpus (mixed-speech)
what the novel VL2M ref 		GRID corpus (mixed-speech)
does (Fig. 1b). The computed 		GRID corpus (mixed-speech)
TBM m̂ and the input 		GRID corpus (mixed-speech)
spectrogram y are the input 		GRID corpus (mixed-speech)
to a function that outputs 		GRID corpus (mixed-speech)
an Ideal Amplitude Mask (IAM) 		GRID corpus (mixed-speech)
p (known as FFT-MASK 		GRID corpus (mixed-speech)
in [8]). Given the target 		GRID corpus (mixed-speech)
clean spectrogram s and the 		GRID corpus (mixed-speech)
noisy spectrogram y, the IAM 		GRID corpus (mixed-speech)
is defined as:  pt[f ] = st[f		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
yt[f ]  Note that although IAM generation		GRID corpus (mixed-speech)
 requires the mixed-speech spectrogram, separate		GRID corpus (mixed-speech)
 spectrograms for each concurrent speakers		GRID corpus (mixed-speech)
 are not required		GRID corpus (mixed-speech)
.  The target speaker’s spectrogram s		GRID corpus (mixed-speech)
 is reconstructed by multiplying the		GRID corpus (mixed-speech)
 input spectrogram with the estimated		GRID corpus (mixed-speech)
 IAM. Values greater than 10		GRID corpus (mixed-speech)
 in the IAM are clipped		GRID corpus (mixed-speech)
 to 10 in order to		GRID corpus (mixed-speech)
 obtain better numerical stability as		GRID corpus (mixed-speech)
 suggested in [8		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
v: video input y: noisy 		GRID corpus (mixed-speech)
spectrogram sm: clean spectrogram TBM 		GRID corpus (mixed-speech)
s: clean spectrogram IAM m: 		GRID corpus (mixed-speech)
TBM p: IAM  STACKED		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
BLSTM  m		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
sm  v		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
y  (a) VL2M		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
v VL2M m  y BLSTM		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
BLSTM  Fusion layer		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
BLSTM p  s		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
b) VL2M ref  v		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
y  p STACKED		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
BLSTM  s		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
c) Audio-Visual concat  sm		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
y  p STACKED		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
BLSTM  s		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
v VL2M m  (d) Audio-Visual concat-ref		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
Fig. 1. Model architectures.  The model performs a function		GRID corpus (mixed-speech)
 Fmr(v, y) = p̂ that		GRID corpus (mixed-speech)
 con- sists of a VL2M		GRID corpus (mixed-speech)
 component plus three different BLSTMs		GRID corpus (mixed-speech)
 Gm, Gy and H		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
Gm(Fvl2m(v)) = rm receives the 		GRID corpus (mixed-speech)
VL2M mask m̂ as in- 		GRID corpus (mixed-speech)
put, and Gy(y) = ry 		GRID corpus (mixed-speech)
is fed with the noisy 		GRID corpus (mixed-speech)
spectrogram. Their output rm, 		GRID corpus (mixed-speech)
ry ∈ Rz are fused 		GRID corpus (mixed-speech)
in a joint audio-visual represen- 		GRID corpus (mixed-speech)
tation 		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
 = [h1		GRID corpus (mixed-speech)
, . . . ,		GRID corpus (mixed-speech)
hT ], where ht is 		GRID corpus (mixed-speech)
a linear combination of rmt 		GRID corpus (mixed-speech)
and ryt : ht = 		GRID corpus (mixed-speech)
Whm ·rmt +Why ·ryt +bh. 		GRID corpus (mixed-speech)
h is the input of 		GRID corpus (mixed-speech)
the third BLSTM H (		GRID corpus (mixed-speech)
h) = p̂, where p̂ 		GRID corpus (mixed-speech)
lays in the [0,10] range. 		GRID corpus (mixed-speech)
The loss function is:  Jmr		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
T∑ t=1  d∑ f=1		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
p̂t[f ] · yt[f ]− 		GRID corpus (mixed-speech)
st[f ])2  2.3. Audio-Visual concat model		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
The third model (Fig. 1c) 		GRID corpus (mixed-speech)
performs early fusion of audio- 		GRID corpus (mixed-speech)
visual features. This model consists 		GRID corpus (mixed-speech)
of a single stacked BLSTM 		GRID corpus (mixed-speech)
that computes the IAM mask 		GRID corpus (mixed-speech)
p̂ from the concate- 		GRID corpus (mixed-speech)
nated [v,y]. The training loss 		GRID corpus (mixed-speech)
is the same Jmr used 		GRID corpus (mixed-speech)
to train VL2M ref. This 		GRID corpus (mixed-speech)
model can be regarded as 		GRID corpus (mixed-speech)
a simplification of VL2M ref, 		GRID corpus (mixed-speech)
where the VL2M operation is 		GRID corpus (mixed-speech)
not performed.  2.4. Audio-Visual concat-ref model		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
The fourth model (Fig. 1d) 		GRID corpus (mixed-speech)
is an improved version of 		GRID corpus (mixed-speech)
the model described in section 2		GRID corpus (mixed-speech)
.3. The only difference is 		GRID corpus (mixed-speech)
the input of the stacked 		GRID corpus (mixed-speech)
BLSTM that is replaced 		GRID corpus (mixed-speech)
by [̂sm,y] where ŝm is 		GRID corpus (mixed-speech)
the denoised spectrogram returned by 		GRID corpus (mixed-speech)
VL2M operation.  3. EXPERIMENTAL SETUP		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
3.1. Dataset  All experiments were carried out		GRID corpus (mixed-speech)
 using the GRID [9] and		GRID corpus (mixed-speech)
 TCD-TIMIT [10] audio-visual datasets. For		GRID corpus (mixed-speech)
 each of them, we created		GRID corpus (mixed-speech)
 a mixed-speech version		GRID corpus (mixed-speech)
.  Regarding the GRID corpus, for		GRID corpus (mixed-speech)
 each of the 33 speakers		GRID corpus (mixed-speech)
 (one had to be discarded		GRID corpus (mixed-speech)
) we first randomly selected 200 ut- terances (out of 1000		GRID corpus (mixed-speech)
). Then, for each utterance, 		GRID corpus (mixed-speech)
we created 3 different audio-mixed 		GRID corpus (mixed-speech)
samples. Each audio-mixed sample was 		GRID corpus (mixed-speech)
created by mixing the chosen 		GRID corpus (mixed-speech)
utterance with one utter- ance 		GRID corpus (mixed-speech)
from a different speaker.  That resulted in 600 audio-mixed		GRID corpus (mixed-speech)
 samples per speaker		GRID corpus (mixed-speech)
.  The resulting dataset was split		GRID corpus (mixed-speech)
 into disjoint sets of 25/4/4		GRID corpus (mixed-speech)
 speakers for training/validation/testing respectively		GRID corpus (mixed-speech)
.  The TCD-TIMIT corpus consists of		GRID corpus (mixed-speech)
 59 speakers (we ex- cluded		GRID corpus (mixed-speech)
 3 professionally-trained lipspeakers) and 98		GRID corpus (mixed-speech)
 utterances per speaker. The mixed-speech		GRID corpus (mixed-speech)
 version was created following the		GRID corpus (mixed-speech)
 same procedure as for GRID		GRID corpus (mixed-speech)
, with one difference. Con- 		GRID corpus (mixed-speech)
trary to GRID, TCD-TIMIT utterances 		GRID corpus (mixed-speech)
have different dura- tion. Thus 2 utterances were mixed only if		GRID corpus (mixed-speech)
 their duration dif- ference did		GRID corpus (mixed-speech)
 not exceed 2 seconds. For		GRID corpus (mixed-speech)
 each utterance pair, we forced		GRID corpus (mixed-speech)
 the non-target speaker’s utterance to		GRID corpus (mixed-speech)
 match the du- ration of		GRID corpus (mixed-speech)
 the target speaker utterance. If		GRID corpus (mixed-speech)
 it was longer, the utterance		GRID corpus (mixed-speech)
 was cut at its end		GRID corpus (mixed-speech)
, whereas if it was 		GRID corpus (mixed-speech)
shorter, silence samples were equally 		GRID corpus (mixed-speech)
added at its start and 		GRID corpus (mixed-speech)
end.  The resulting dataset was split		GRID corpus (mixed-speech)
 into disjoint sets of 51/4/4		GRID corpus (mixed-speech)
 speakers for training/validation/testing respectively		GRID corpus (mixed-speech)
.  3.2. LSTM training		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
In all experiments, the models 		GRID corpus (mixed-speech)
were trained using the Adam 		GRID corpus (mixed-speech)
optimizer [26]. Early stopping was 		GRID corpus (mixed-speech)
applied when the error on 		GRID corpus (mixed-speech)
the validation set did not 		GRID corpus (mixed-speech)
decrease over 5 consecutive epochs.  VL2M, AV concat and AV		GRID corpus (mixed-speech)
 concat-ref had 5, 3 and		GRID corpus (mixed-speech)
 3 stacked BLSTM layers respectively		GRID corpus (mixed-speech)
. All BLSTMs had 250 		GRID corpus (mixed-speech)
units. Hyper-parameters selection was performed 		GRID corpus (mixed-speech)
by using random search with 		GRID corpus (mixed-speech)
a limited number of samples, 		GRID corpus (mixed-speech)
therefore all the reported results 		GRID corpus (mixed-speech)
may improve through a deeper 		GRID corpus (mixed-speech)
hyper- parameters validation phase.  VL2M ref and AV concat-ref		GRID corpus (mixed-speech)
 training was performed in 2		GRID corpus (mixed-speech)
 steps. We first pre-trained the		GRID corpus (mixed-speech)
 models using the oracle TBM		GRID corpus (mixed-speech)
 m. Then we substituted the		GRID corpus (mixed-speech)
 oracle masks with the VL2M		GRID corpus (mixed-speech)
 component and retrained the models		GRID corpus (mixed-speech)
 while freezing the pa- rameters		GRID corpus (mixed-speech)
 of the VL2M component		GRID corpus (mixed-speech)
.  3.3. Audio pre- and post-processing		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
The original waveforms were resampled 		GRID corpus (mixed-speech)
to 16 kHz. Short- Time 		GRID corpus (mixed-speech)
Fourier Transform (STFT) x was 		GRID corpus (mixed-speech)
computed using FFT size of 512, Hann window of length 25		GRID corpus (mixed-speech)
 ms (400 samples), and hop		GRID corpus (mixed-speech)
 length of 10 ms (160		GRID corpus (mixed-speech)
 samples). The input spectro- gram		GRID corpus (mixed-speech)
 was obtained taking the STFT		GRID corpus (mixed-speech)
 magnitude and perform- ing power-law		GRID corpus (mixed-speech)
 compression |x|p with p		GRID corpus (mixed-speech)
 = 0.3. Finally we applied		GRID corpus (mixed-speech)
 per-speaker 0-mean 1-std normalization		GRID corpus (mixed-speech)
.  In the post-processing stage, the		GRID corpus (mixed-speech)
 enhanced waveform gen- erated by		GRID corpus (mixed-speech)
 the speech enhancement models was		GRID corpus (mixed-speech)
 reconstructed		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
SDR PESQ ViSQOL  Noisy −1.06 1.81 2.11 VL2M		GRID corpus (mixed-speech)
 3.17 1.51 1.16 VL2M ref		GRID corpus (mixed-speech)
 6.50 2.58 2.99 AV concat		GRID corpus (mixed-speech)
 6.31 2.49 2.83 AV c-ref		GRID corpus (mixed-speech)
 6.17 2.58 2.96		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
Table 1. GRID results - 		GRID corpus (mixed-speech)
speaker-dependent. The “Noisy” row refers 		GRID corpus (mixed-speech)
to the metric values of 		GRID corpus (mixed-speech)
the input mixed-speech signal.  2 Speakers 3 Speakers SDR		GRID corpus (mixed-speech)
 PESQ ViSQOL SDR PESQ ViSQOL		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
Noisy 0.21 1.94 2.58 −5.34 1		GRID corpus (mixed-speech)
.43 1.62 VL2M 3.02 1.81 1		GRID corpus (mixed-speech)
.70 −2.03 1.43 1.25 VL2M 		GRID corpus (mixed-speech)
ref 6.52 2.53 3.02 2.83 2		GRID corpus (mixed-speech)
.19 2.53 AV concat 7.37 2		GRID corpus (mixed-speech)
.65 3.03 3.02 2.24 2.49 		GRID corpus (mixed-speech)
AV c-ref 8.05 2.70 3.07 4		GRID corpus (mixed-speech)
.02 2.33 2.64  Table 2. GRID results		GRID corpus (mixed-speech)
 - speaker-independent		GRID corpus (mixed-speech)
.  by applying the inverse STFT		GRID corpus (mixed-speech)
 to the estimated clean spectro		GRID corpus (mixed-speech)
- gram and using the 		GRID corpus (mixed-speech)
phase of the noisy input 		GRID corpus (mixed-speech)
signal.  3.4. Video pre-processing		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
Face landmarks were extracted from 		GRID corpus (mixed-speech)
video using the Dlib [7] 		GRID corpus (mixed-speech)
implementation of the face landmark 		GRID corpus (mixed-speech)
estimator described in [6]. It 		GRID corpus (mixed-speech)
returns 68 x-y points, for 		GRID corpus (mixed-speech)
an overall 136 values. We 		GRID corpus (mixed-speech)
upsampled from 25/29.97 fps (GRID/TCD-TIMIT) 		GRID corpus (mixed-speech)
to 100 fps to match 		GRID corpus (mixed-speech)
the frame rate of the 		GRID corpus (mixed-speech)
audio spectrogram. Upsampling was carried 		GRID corpus (mixed-speech)
out through linear interpolation over 		GRID corpus (mixed-speech)
time.  The final video feature vector		GRID corpus (mixed-speech)
 v was obtained by com		GRID corpus (mixed-speech)
- puting the per-speaker normalized 		GRID corpus (mixed-speech)
motion vector of the face 		GRID corpus (mixed-speech)
landmarks by simply subtracting every 		GRID corpus (mixed-speech)
frame with the previ- ous 		GRID corpus (mixed-speech)
one. The motion vector of 		GRID corpus (mixed-speech)
the first frame was set 		GRID corpus (mixed-speech)
to zero.  4. RESULTS		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
In order to compare our 		GRID corpus (mixed-speech)
models to previous works in 		GRID corpus (mixed-speech)
both speech enhancement and separation, 		GRID corpus (mixed-speech)
we evaluated the perfor- mance 		GRID corpus (mixed-speech)
of the proposed models using 		GRID corpus (mixed-speech)
both speech separation  2 Speakers 3 Speakers SDR		GRID corpus (mixed-speech)
 PESQ ViSQOL SDR PESQ ViSQOL		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
Noisy 0.21 2.22 2.74 −3.42 1		GRID corpus (mixed-speech)
.92 2.04 VL2M 2.88 2.25 2		GRID corpus (mixed-speech)
.62 −0.51 1.99 1.98 VL2M 		GRID corpus (mixed-speech)
ref 9.24 2.81 3.09 5.27 2		GRID corpus (mixed-speech)
.44 2.54 AV concat 9.56 2		GRID corpus (mixed-speech)
.80 3.09 5.15 2.41 2.52 		GRID corpus (mixed-speech)
AV c-ref 10.55 3.03 3.21 5		GRID corpus (mixed-speech)
.37 2.45 2.58  Table 3. TCD-TIMIT results		GRID corpus (mixed-speech)
 - speaker-independent		GRID corpus (mixed-speech)
.  and enhancement metrics. Specifically, we		GRID corpus (mixed-speech)
 measured the ca- pability of		GRID corpus (mixed-speech)
 separating the target utterance from		GRID corpus (mixed-speech)
 the concurrent utterance with the		GRID corpus (mixed-speech)
 source-to-distortion ratio (SDR) [27, 28		GRID corpus (mixed-speech)
]. While the quality of 		GRID corpus (mixed-speech)
estimated target speech was measured 		GRID corpus (mixed-speech)
with the perceptual PESQ [29] 		GRID corpus (mixed-speech)
and ViSQOL [30] metrics. For 		GRID corpus (mixed-speech)
PESQ we used the narrow 		GRID corpus (mixed-speech)
band mode while for ViSQOL 		GRID corpus (mixed-speech)
we used the wide band 		GRID corpus (mixed-speech)
mode.  As a very first experiment		GRID corpus (mixed-speech)
 we compared landmark posi- tion		GRID corpus (mixed-speech)
 vs. landmark motion vectors. It		GRID corpus (mixed-speech)
 turned out that landmark positions		GRID corpus (mixed-speech)
 performed poorly, thus all results		GRID corpus (mixed-speech)
 reported here refer to landmark		GRID corpus (mixed-speech)
 motion vectors only		GRID corpus (mixed-speech)
.  We then carried out some		GRID corpus (mixed-speech)
 speaker-dependent experiments to compare our		GRID corpus (mixed-speech)
 models with previous studies as		GRID corpus (mixed-speech)
, to the best of 		GRID corpus (mixed-speech)
our knowledge, there are no 		GRID corpus (mixed-speech)
reported results of speaker- independent 		GRID corpus (mixed-speech)
systems trained and tested on 		GRID corpus (mixed-speech)
GRID and TCD- TIMIT to 		GRID corpus (mixed-speech)
compare with. Table 1 reports 		GRID corpus (mixed-speech)
the test-set evalua- tion of 		GRID corpus (mixed-speech)
speaker-dependent models on the GRID 		GRID corpus (mixed-speech)
corpus with landmark motion vectors. 		GRID corpus (mixed-speech)
Results are comparable with previ- 		GRID corpus (mixed-speech)
ous state-of-the-art studies in an 		GRID corpus (mixed-speech)
almost identical setting [15, 17].  Table 2 and 3 show		GRID corpus (mixed-speech)
 speaker-independent test-set results on the		GRID corpus (mixed-speech)
 GRID and TCD-TIMIT datasets respectively		GRID corpus (mixed-speech)
. V2ML performs significantly worse 		GRID corpus (mixed-speech)
than the other three models 		GRID corpus (mixed-speech)
in- dicating that a successful 		GRID corpus (mixed-speech)
mask generation has to depend 		GRID corpus (mixed-speech)
on the acoustic context. The 		GRID corpus (mixed-speech)
performance of the three models 		GRID corpus (mixed-speech)
in the speaker-independent setting is 		GRID corpus (mixed-speech)
comparable to that in the 		GRID corpus (mixed-speech)
speaker-dependent setting.  AV concat-ref outperforms V2ML ref		GRID corpus (mixed-speech)
 and AV concat for both		GRID corpus (mixed-speech)
 datasets. This supports the utility		GRID corpus (mixed-speech)
 of a refinement strat- egy		GRID corpus (mixed-speech)
 and suggests that the refinement		GRID corpus (mixed-speech)
 is more effective when it		GRID corpus (mixed-speech)
 directly refines the estimated clean		GRID corpus (mixed-speech)
 spectrogram, rather than refining the		GRID corpus (mixed-speech)
 estimated mask		GRID corpus (mixed-speech)
.  Finally, we evaluated the systems		GRID corpus (mixed-speech)
 in a more challenging testing		GRID corpus (mixed-speech)
 condition where the target utterance		GRID corpus (mixed-speech)
 was mixed with 2 utterances		GRID corpus (mixed-speech)
 from 2 competing speakers. Despite		GRID corpus (mixed-speech)
 the model was trained with		GRID corpus (mixed-speech)
 mixtures of two speakers, the		GRID corpus (mixed-speech)
 decrease of performance was not		GRID corpus (mixed-speech)
 dramatic		GRID corpus (mixed-speech)
.  Code and some testing examples		GRID corpus (mixed-speech)
 of our models are avail		GRID corpus (mixed-speech)
- able at https://goo.gl/3h1NgE.  5. CONCLUSION		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
This paper proposes the use 		GRID corpus (mixed-speech)
of face landmark motion vec- 		GRID corpus (mixed-speech)
tors for audio-visual speech enhancement 		GRID corpus (mixed-speech)
in a single-channel multi-talker scenario. 		GRID corpus (mixed-speech)
Different models are tested where 		GRID corpus (mixed-speech)
land- mark motion vectors are 		GRID corpus (mixed-speech)
used to generate time-frequency (T- 		GRID corpus (mixed-speech)
F) masks that extract the 		GRID corpus (mixed-speech)
target speaker’s spectrogram from the 		GRID corpus (mixed-speech)
acoustic mixed-speech spectrogram.  To the best of our		GRID corpus (mixed-speech)
 knowledge, some of the proposed		GRID corpus (mixed-speech)
 mod- els are the first		GRID corpus (mixed-speech)
 models trained and evaluated on		GRID corpus (mixed-speech)
 the limited size GRID and		GRID corpus (mixed-speech)
 TCD-TIMIT datasets that accomplish speaker		GRID corpus (mixed-speech)
- independent speech enhancement in 		GRID corpus (mixed-speech)
the multi-talker setting, with a 		GRID corpus (mixed-speech)
quality of enhancement comparable to 		GRID corpus (mixed-speech)
that achieved in a speaker-dependent 		GRID corpus (mixed-speech)
setting.  https://goo.gl/3h1NgE		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
6. REFERENCES  [1] E. Colin Cherry, “Some		GRID corpus (mixed-speech)
 experiments on the recognition of		GRID corpus (mixed-speech)
 speech, with one and with		GRID corpus (mixed-speech)
 two ears,” The Journal of		GRID corpus (mixed-speech)
 the Acoustical Society of America		GRID corpus (mixed-speech)
, vol. 25, no. 5, 		GRID corpus (mixed-speech)
pp. 975–979, 1953.  [2] Josh H McDermott, “The		GRID corpus (mixed-speech)
 cocktail party problem,” Current Biology		GRID corpus (mixed-speech)
, vol. 19, no. 22, 		GRID corpus (mixed-speech)
pp. R1024–R1027, 2009.  [3] Elana Zion Golumbic, Gregory		GRID corpus (mixed-speech)
 B. Cogan, Charles E. Schroeder		GRID corpus (mixed-speech)
, and David Poeppel, “Visual 		GRID corpus (mixed-speech)
input enhances selective speech envelope 		GRID corpus (mixed-speech)
tracking in auditory cortex at 		GRID corpus (mixed-speech)
a “cocktail party”,” Journal of 		GRID corpus (mixed-speech)
Neu- roscience, vol. 33, no. 4, pp. 1417–1426, 2013		GRID corpus (mixed-speech)
.  [4] Wei Ji Ma, Xiang		GRID corpus (mixed-speech)
 Zhou, Lars A. Ross, John		GRID corpus (mixed-speech)
 J. Foxe, and Lucas C		GRID corpus (mixed-speech)
. Parra, “Lip-reading aids word 		GRID corpus (mixed-speech)
recognition most in moderate noise: 		GRID corpus (mixed-speech)
A bayesian explanation using high-dimensional 		GRID corpus (mixed-speech)
feature space,” PLOS ONE, vol. 4, no. 3, pp. 1–14, 03		GRID corpus (mixed-speech)
 2009		GRID corpus (mixed-speech)
.  [5] Albert S Bregman, Auditory		GRID corpus (mixed-speech)
 scene analysis: The perceptual organi		GRID corpus (mixed-speech)
- zation of sound, MIT 		GRID corpus (mixed-speech)
press, 1994.  [6] Vahid Kazemi and Josephine		GRID corpus (mixed-speech)
 Sullivan, “One millisecond face align		GRID corpus (mixed-speech)
- ment with an ensemble 		GRID corpus (mixed-speech)
of regression trees,” in The 		GRID corpus (mixed-speech)
IEEE Conference on Computer Vision 		GRID corpus (mixed-speech)
and Pattern Recognition (CVPR), June 2014		GRID corpus (mixed-speech)
.  [7] Davis E. King, “Dlib-ml		GRID corpus (mixed-speech)
: A machine learning toolkit,” 		GRID corpus (mixed-speech)
Journal of Machine Learning Research, 		GRID corpus (mixed-speech)
vol. 10, pp. 1755–1758, 2009.  [8] Yuxuan Wang, Arun Narayanan		GRID corpus (mixed-speech)
, and DeLiang Wang, “On 		GRID corpus (mixed-speech)
Training Targets for Supervised Speech 		GRID corpus (mixed-speech)
Separation,” IEEE/ACM Transactions on Audio, 		GRID corpus (mixed-speech)
Speech, and Language Processing, vol. 22, no. 12, pp. 1849–1858, Dec		GRID corpus (mixed-speech)
. 2014.  [9] Martin Cooke, Jon Barker		GRID corpus (mixed-speech)
, Stuart Cunningham, and Xu 		GRID corpus (mixed-speech)
Shao, “An audio-visual corpus for 		GRID corpus (mixed-speech)
speech perception and automatic speech 		GRID corpus (mixed-speech)
recognition,” The Journal of the 		GRID corpus (mixed-speech)
Acoustical Society of America, vol. 120, no. 5, pp. 2421–2424, Nov		GRID corpus (mixed-speech)
. 2006.  [10] Naomi Harte and Eoin		GRID corpus (mixed-speech)
 Gillen, “TCD-TIMIT: An Audio-Visual Cor		GRID corpus (mixed-speech)
- pus of Continuous Speech,” 		GRID corpus (mixed-speech)
IEEE Transactions on Multimedia, vol. 17, no. 5, pp. 603–615, May		GRID corpus (mixed-speech)
 2015		GRID corpus (mixed-speech)
.  [11] Z. Chen, Y. Luo		GRID corpus (mixed-speech)
, and N. Mesgarani, “Deep 		GRID corpus (mixed-speech)
attractor network for single-microphone speaker 		GRID corpus (mixed-speech)
separation,” in 2017 IEEE International 		GRID corpus (mixed-speech)
Conference on Acoustics, Speech and 		GRID corpus (mixed-speech)
Signal Processing (ICASSP), March 2017, 		GRID corpus (mixed-speech)
pp. 246–250.  [12] Yusuf Isik, Jonathan Le		GRID corpus (mixed-speech)
 Roux, Zhuo Chen, Shinji Watanabe		GRID corpus (mixed-speech)
, and John R. 		GRID corpus (mixed-speech)
Hershey, “Single-channel multi-speaker separation using 		GRID corpus (mixed-speech)
deep clustering,” in Interspeech, 2016.  [13] Morten Kolbaek, Dong Yu		GRID corpus (mixed-speech)
, Zheng-Hua Tan, Jesper Jensen, 		GRID corpus (mixed-speech)
Morten Kolbaek, Dong Yu, Zheng-Hua 		GRID corpus (mixed-speech)
Tan, and Jesper Jensen, “Multitalker 		GRID corpus (mixed-speech)
speech separation with utterance-level permutation 		GRID corpus (mixed-speech)
invariant training of deep recurrent 		GRID corpus (mixed-speech)
neural networks,” IEEE/ACM Trans. Audio, 		GRID corpus (mixed-speech)
Speech and Lang. Proc., vol. 25, no. 10, pp. 1901–1913, Oct		GRID corpus (mixed-speech)
. 2017.  [14] Bertrand Rivet, Wenwu Wang		GRID corpus (mixed-speech)
, Syed Mohsen Naqvi, and 		GRID corpus (mixed-speech)
Jonathon Chambers, “Audiovisual Speech Source 		GRID corpus (mixed-speech)
Separation: An overview of key 		GRID corpus (mixed-speech)
methodologies,” IEEE Signal Processing Magazine, 		GRID corpus (mixed-speech)
vol. 31, no. 3, pp. 125		GRID corpus (mixed-speech)
–134, May 2014.  [15] Aviv Gabbay, Ariel Ephrat		GRID corpus (mixed-speech)
, Tavi Halperin, and Shmuel 		GRID corpus (mixed-speech)
Peleg, “Seeing through noise: Visually 		GRID corpus (mixed-speech)
driven speaker separation and enhancement,” 		GRID corpus (mixed-speech)
in ICASSP. 2018, pp. 3051–3055, 		GRID corpus (mixed-speech)
IEEE.  [16] Ariel Ephrat, Tavi Halperin		GRID corpus (mixed-speech)
, and Shmuel Peleg, “Improved 		GRID corpus (mixed-speech)
speech reconstruction from silent video,” 		GRID corpus (mixed-speech)
ICCV 2017 Workshop on Computer 		GRID corpus (mixed-speech)
Vision for Audio-Visual Media, 2017.  [17] Aviv Gabbay, Asaph Shamir		GRID corpus (mixed-speech)
, and Shmuel Peleg, “Visual 		GRID corpus (mixed-speech)
speech en- hancement,” in Interspeech. 2018, pp. 1170–1174, ISCA		GRID corpus (mixed-speech)
.  [18] Jen-Cheng Hou, Syu-Siang Wang		GRID corpus (mixed-speech)
, Ying-Hui Lai, Yu Tsao, 		GRID corpus (mixed-speech)
Hsiu-Wen Chang, and Hsin-Min 		GRID corpus (mixed-speech)
Wang, “Audio-Visual Speech Enhancement Us- 		GRID corpus (mixed-speech)
ing Multimodal Deep Convolutional Neural 		GRID corpus (mixed-speech)
Networks,” IEEE Trans- actions on 		GRID corpus (mixed-speech)
Emerging Topics in Computational Intelligence, 		GRID corpus (mixed-speech)
vol. 2, no. 2, pp. 117		GRID corpus (mixed-speech)
–128, Apr. 2018.  [19] Jen-Cheng Hou, Syu-Siang Wang		GRID corpus (mixed-speech)
, Ying-Hui Lai, Jen-Chun Lin, 		GRID corpus (mixed-speech)
Yu Tsao, Hsiu-Wen Chang, and 		GRID corpus (mixed-speech)
Hsin-Min Wang, “Audio-visual speech enhancement 		GRID corpus (mixed-speech)
using deep neural networks,” in 2016 Asia- Pacific Signal and Information		GRID corpus (mixed-speech)
 Processing Association Annual Sum- mit		GRID corpus (mixed-speech)
 and Conference (APSIPA), Jeju, South		GRID corpus (mixed-speech)
 Korea, Dec. 2016, pp. 1–6		GRID corpus (mixed-speech)
, IEEE.  [20] Ariel Ephrat, Inbar Mosseri		GRID corpus (mixed-speech)
, Oran Lang, Tali Dekel, 		GRID corpus (mixed-speech)
Kevin Wilson, Avinatan Hassidim, William 		GRID corpus (mixed-speech)
T. Freeman, and Michael 		GRID corpus (mixed-speech)
Rubinstein, “Looking to Listen at 		GRID corpus (mixed-speech)
the Cocktail Party: A Speaker-Independent 		GRID corpus (mixed-speech)
Audio-Visual Model for Speech Separation,” 		GRID corpus (mixed-speech)
ACM Transactions on Graphics, vol. 37, no. 4, pp. 1–11, July		GRID corpus (mixed-speech)
 2018, arXiv: 1804.03619		GRID corpus (mixed-speech)
.  [21] T. Afouras, J. S		GRID corpus (mixed-speech)
. Chung, and A. 		GRID corpus (mixed-speech)
Zisserman, “The conversation: Deep audio-visual 		GRID corpus (mixed-speech)
speech enhancement,” in Interspeech, 2018.  [22] Andrew Owens and Alexei		GRID corpus (mixed-speech)
 A Efros, “Audio-visual scene analysis		GRID corpus (mixed-speech)
 with self-supervised multisensory features,” European		GRID corpus (mixed-speech)
 Conference on Computer Vision (ECCV		GRID corpus (mixed-speech)
), 2018.  [23] Michael C. Anzalone, Lauren		GRID corpus (mixed-speech)
 Calandruccio, Karen A. Doherty, and		GRID corpus (mixed-speech)
 Laurel H. Carney, “Determination of		GRID corpus (mixed-speech)
 the potential benefit of time		GRID corpus (mixed-speech)
- frequency gain manipulation,” Ear 		GRID corpus (mixed-speech)
Hear, vol. 27, no. 5, 		GRID corpus (mixed-speech)
pp. 480–492, Oct 2006, 16957499[pmid].  [24] Ulrik Kjems, Jesper B		GRID corpus (mixed-speech)
. Boldt, Michael S. Pedersen, 		GRID corpus (mixed-speech)
Thomas Lunner, and DeLiang 		GRID corpus (mixed-speech)
Wang, “Role of mask pattern 		GRID corpus (mixed-speech)
in intelligibility of ideal binary-masked 		GRID corpus (mixed-speech)
noisy speech,” The Journal of 		GRID corpus (mixed-speech)
the Acoustical Society of America, 		GRID corpus (mixed-speech)
vol. 126, no. 3, pp. 1415		GRID corpus (mixed-speech)
–1426, 2009.  [25] A. Graves, A. Mohamed		GRID corpus (mixed-speech)
, and G. Hinton, “Speech 		GRID corpus (mixed-speech)
recognition with deep recurrent neural 		GRID corpus (mixed-speech)
networks,” in 2013 IEEE International 		GRID corpus (mixed-speech)
Con- ference on Acoustics, Speech 		GRID corpus (mixed-speech)
and Signal Processing, May 2013, 		GRID corpus (mixed-speech)
pp. 6645–6649.  [26] Diederik P Kingma and		GRID corpus (mixed-speech)
 Jimmy Ba, “Adam: A method		GRID corpus (mixed-speech)
 for stochastic optimization,” arXiv preprint		GRID corpus (mixed-speech)
 arXiv:1412.6980, 2014		GRID corpus (mixed-speech)
.  [27] E. Vincent, R. Gribonval		GRID corpus (mixed-speech)
, and C. Fevotte, “Performance 		GRID corpus (mixed-speech)
measure- ment in blind audio 		GRID corpus (mixed-speech)
source separation,” IEEE Transactions on 		GRID corpus (mixed-speech)
Audio, Speech and Language Processing, 		GRID corpus (mixed-speech)
vol. 14, no. 4, pp. 1462		GRID corpus (mixed-speech)
–1469, July 2006.  [28] Colin Raffel, Brian McFee		GRID corpus (mixed-speech)
, Eric J Humphrey, Justin 		GRID corpus (mixed-speech)
Salamon, Oriol Nieto, Dawen Liang, 		GRID corpus (mixed-speech)
Daniel PW Ellis, and C 		GRID corpus (mixed-speech)
Colin Raffel, “mir eval: A 		GRID corpus (mixed-speech)
transparent implementation of common mir 		GRID corpus (mixed-speech)
metrics,” in In Proceed- ings 		GRID corpus (mixed-speech)
of the 15th International Society 		GRID corpus (mixed-speech)
for Music Information Retrieval Conference, 		GRID corpus (mixed-speech)
ISMIR. Citeseer, 2014.  [29] A.W. Rix, J.G. Beerends		GRID corpus (mixed-speech)
, M.P. Hollier, and A.P. 		GRID corpus (mixed-speech)
Hekstra, “Perceptual evaluation of speech 		GRID corpus (mixed-speech)
quality (PESQ)-a new method for 		GRID corpus (mixed-speech)
speech qual- ity assessment of 		GRID corpus (mixed-speech)
telephone networks and codecs,” in 2001 IEEE In- ternational Conference on		GRID corpus (mixed-speech)
 Acoustics, Speech, and Signal Processing		GRID corpus (mixed-speech)
. Proceedings (Cat. No.01CH37221), Salt 		GRID corpus (mixed-speech)
Lake City, UT, USA, 2001, 		GRID corpus (mixed-speech)
vol. 2, pp. 749–752, IEEE.  [30] A. Hines, J. Skoglund		GRID corpus (mixed-speech)
, A. Kokaram, and N. 		GRID corpus (mixed-speech)
Harte, “ViSQOL: The Virtual Speech 		GRID corpus (mixed-speech)
Quality Objective Listener,” in IWAENC 2012		GRID corpus (mixed-speech)
; Inter- national Workshop on 		GRID corpus (mixed-speech)
Acoustic Signal Enhancement, Sept. 2012, 		GRID corpus (mixed-speech)
pp. 1		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
4		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
1  Introduction		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
1.1  Related work		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
2  MODEL ARCHITECTURES		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
2.1  VL2M model		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
2.2  VL2M_ref model		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
2.3  Audio-Visual concat model		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
2.4  Audio-Visual concat-ref model		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
3  Experimental setup		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
3.1  Dataset		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
3.2  LSTM training		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
3.3  Audio pre- and post-processing		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
3.4  Video pre-processing		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
4  Results		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
5  Conclusion		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
6  References		GRID corpus (mixed-speech)
		GRID corpus (mixed-speech)
FACE LANDMARK-BASED SPEAKER-INDEPENDENT AUDIO-VISUAL SPEECH 	GRID corpus (mixed	GRID corpus (mixed-speech)
ENHANCEMENT IN MULTI-TALKER ENVIRONMENTS  Giovanni Morrone? Luca Pasa† Vadim	GRID corpus (mixed	GRID corpus (mixed-speech)
 Tikhanoff	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
Sonia Bergamaschi? Luciano Fadiga† Leonardo 	GRID corpus (mixed	GRID corpus (mixed-speech)
Badino†  ?Department of Engineering ”Enzo Ferrari	GRID corpus (mixed	GRID corpus (mixed-speech)
”, University of Modena and 	GRID corpus (mixed	GRID corpus (mixed-speech)
Reggio Emilia, Modena, Italy †Istituto 	GRID corpus (mixed	GRID corpus (mixed-speech)
Italiano di Tecnologia, Ferrara, Italy  ABSTRACT	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
In this paper, we address 	GRID corpus (mixed	GRID corpus (mixed-speech)
the problem of enhancing the 	GRID corpus (mixed	GRID corpus (mixed-speech)
speech of a speaker of 	GRID corpus (mixed	GRID corpus (mixed-speech)
interest in a cocktail party 	GRID corpus (mixed	GRID corpus (mixed-speech)
scenario when vi- sual information 	GRID corpus (mixed	GRID corpus (mixed-speech)
of the speaker of interest 	GRID corpus (mixed	GRID corpus (mixed-speech)
is available.  Contrary to most previous studies	GRID corpus (mixed	GRID corpus (mixed-speech)
, we do not learn 	GRID corpus (mixed	GRID corpus (mixed-speech)
visual features on the typically 	GRID corpus (mixed	GRID corpus (mixed-speech)
small audio-visual datasets, but use 	GRID corpus (mixed	GRID corpus (mixed-speech)
an already available face landmark 	GRID corpus (mixed	GRID corpus (mixed-speech)
detector (trained on a sep- 	GRID corpus (mixed	GRID corpus (mixed-speech)
arate image dataset).  The landmarks are used by	GRID corpus (mixed	GRID corpus (mixed-speech)
 LSTM-based models to gen- erate	GRID corpus (mixed	GRID corpus (mixed-speech)
 time-frequency masks which are applied	GRID corpus (mixed	GRID corpus (mixed-speech)
 to the acoustic mixed-speech spectrogram	GRID corpus (mixed	GRID corpus (mixed-speech)
. Results show that: (i) 	GRID corpus (mixed	GRID corpus (mixed-speech)
land- mark motion features are 	GRID corpus (mixed	GRID corpus (mixed-speech)
very effective features for this 	GRID corpus (mixed	GRID corpus (mixed-speech)
task, (ii) similarly to previous 	GRID corpus (mixed	GRID corpus (mixed-speech)
work, reconstruction of the target 	GRID corpus (mixed	GRID corpus (mixed-speech)
speaker’s spectrogram mediated by masking 	GRID corpus (mixed	GRID corpus (mixed-speech)
is significantly more accurate than 	GRID corpus (mixed	GRID corpus (mixed-speech)
direct spectrogram reconstruction, and (iii) 	GRID corpus (mixed	GRID corpus (mixed-speech)
the best masks depend on 	GRID corpus (mixed	GRID corpus (mixed-speech)
both motion landmark features and 	GRID corpus (mixed	GRID corpus (mixed-speech)
the input mixed-speech spectrogram.  To the best of our	GRID corpus (mixed	GRID corpus (mixed-speech)
 knowledge, our proposed models are	GRID corpus (mixed	GRID corpus (mixed-speech)
 the first models trained and	GRID corpus (mixed	GRID corpus (mixed-speech)
 evaluated on the limited size	GRID corpus (mixed	GRID corpus (mixed-speech)
 GRID and TCD-TIMIT datasets, that	GRID corpus (mixed	GRID corpus (mixed-speech)
 achieve speaker-independent speech enhancement in	GRID corpus (mixed	GRID corpus (mixed-speech)
 a multi-talker setting	GRID corpus (mixed	GRID corpus (mixed-speech)
.  Index Terms— audio-visual speech enhancement	GRID corpus (mixed	GRID corpus (mixed-speech)
, cock- tail party problem, 	GRID corpus (mixed	GRID corpus (mixed-speech)
time-frequency mask, LSTM, face land- 	GRID corpus (mixed	GRID corpus (mixed-speech)
marks  1. INTRODUCTION	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
In the context of speech 	GRID corpus (mixed	GRID corpus (mixed-speech)
perception, the cocktail party 	GRID corpus (mixed	GRID corpus (mixed-speech)
effect [1, 2] is the 	GRID corpus (mixed	GRID corpus (mixed-speech)
ability of the brain to 	GRID corpus (mixed	GRID corpus (mixed-speech)
recognize speech in complex and 	GRID corpus (mixed	GRID corpus (mixed-speech)
adverse listening conditions where the 	GRID corpus (mixed	GRID corpus (mixed-speech)
attended speech is mixed with 	GRID corpus (mixed	GRID corpus (mixed-speech)
competing sounds/speech.  Speech perception studies have shown	GRID corpus (mixed	GRID corpus (mixed-speech)
 that watching speaker’s face movements	GRID corpus (mixed	GRID corpus (mixed-speech)
 could dramatically improve our ability	GRID corpus (mixed	GRID corpus (mixed-speech)
 at recognizing the speech of	GRID corpus (mixed	GRID corpus (mixed-speech)
 a target speaker in a	GRID corpus (mixed	GRID corpus (mixed-speech)
 multi-talker environment [3, 4	GRID corpus (mixed	GRID corpus (mixed-speech)
].  This work aims at extracting	GRID corpus (mixed	GRID corpus (mixed-speech)
 the speech of a target	GRID corpus (mixed	GRID corpus (mixed-speech)
 speaker from single channel audio	GRID corpus (mixed	GRID corpus (mixed-speech)
 of several people talking simulta	GRID corpus (mixed	GRID corpus (mixed-speech)
- neously. This is an 	GRID corpus (mixed	GRID corpus (mixed-speech)
ill-posed problem in that many 	GRID corpus (mixed	GRID corpus (mixed-speech)
differ- ent hypotheses about what 	GRID corpus (mixed	GRID corpus (mixed-speech)
the target speaker says are 	GRID corpus (mixed	GRID corpus (mixed-speech)
con-  sistent with the mixture signal	GRID corpus (mixed	GRID corpus (mixed-speech)
. Yet, it can be 	GRID corpus (mixed	GRID corpus (mixed-speech)
solved by ex- ploiting some 	GRID corpus (mixed	GRID corpus (mixed-speech)
additional information associated to the 	GRID corpus (mixed	GRID corpus (mixed-speech)
speaker of interest and/or by 	GRID corpus (mixed	GRID corpus (mixed-speech)
leveraging some prior knowledge about 	GRID corpus (mixed	GRID corpus (mixed-speech)
speech signal properties (e.g., [5]). 	GRID corpus (mixed	GRID corpus (mixed-speech)
In this work we use 	GRID corpus (mixed	GRID corpus (mixed-speech)
face movements of the target 	GRID corpus (mixed	GRID corpus (mixed-speech)
speaker as additional information.  This paper (i) proposes the	GRID corpus (mixed	GRID corpus (mixed-speech)
 use of face landmark’s move	GRID corpus (mixed	GRID corpus (mixed-speech)
- ments, extracted using 	GRID corpus (mixed	GRID corpus (mixed-speech)
Dlib [6, 7] and (ii) 	GRID corpus (mixed	GRID corpus (mixed-speech)
compares differ- ent ways of 	GRID corpus (mixed	GRID corpus (mixed-speech)
mapping such visual features into 	GRID corpus (mixed	GRID corpus (mixed-speech)
time-frequency (T-F) masks, then applied 	GRID corpus (mixed	GRID corpus (mixed-speech)
to clean the acoustic mixed-speech 	GRID corpus (mixed	GRID corpus (mixed-speech)
spectrogram.  By using Dlib extracted landmarks	GRID corpus (mixed	GRID corpus (mixed-speech)
 we relieve our mod- els	GRID corpus (mixed	GRID corpus (mixed-speech)
 from the task of learning	GRID corpus (mixed	GRID corpus (mixed-speech)
 useful visual features from raw	GRID corpus (mixed	GRID corpus (mixed-speech)
 pixels. That aspect is particularly	GRID corpus (mixed	GRID corpus (mixed-speech)
 relevant when the training audio-visual	GRID corpus (mixed	GRID corpus (mixed-speech)
 datasets are small	GRID corpus (mixed	GRID corpus (mixed-speech)
.  The analysis of landmark-dependent masking	GRID corpus (mixed	GRID corpus (mixed-speech)
 strategies is motivated by the	GRID corpus (mixed	GRID corpus (mixed-speech)
 fact that speech enhancement mediated	GRID corpus (mixed	GRID corpus (mixed-speech)
 by an explicit masking is	GRID corpus (mixed	GRID corpus (mixed-speech)
 often more effective than mask-free	GRID corpus (mixed	GRID corpus (mixed-speech)
 enhancement [8	GRID corpus (mixed	GRID corpus (mixed-speech)
].  All our models were trained	GRID corpus (mixed	GRID corpus (mixed-speech)
 and evaluated on the GRID	GRID corpus (mixed	GRID corpus (mixed-speech)
 [9] and TCD-TIMIT [10] datasets	GRID corpus (mixed	GRID corpus (mixed-speech)
 in a speaker-independent setting	GRID corpus (mixed	GRID corpus (mixed-speech)
.  1.1. Related work	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
Speech enhancement aims at extracting 	GRID corpus (mixed	GRID corpus (mixed-speech)
the voice of a tar- 	GRID corpus (mixed	GRID corpus (mixed-speech)
get speaker, while speech separation 	GRID corpus (mixed	GRID corpus (mixed-speech)
refers to the problem of 	GRID corpus (mixed	GRID corpus (mixed-speech)
separating each sound source in 	GRID corpus (mixed	GRID corpus (mixed-speech)
a mixture. Recently pro- posed 	GRID corpus (mixed	GRID corpus (mixed-speech)
audio-only single-channel methods have achieved 	GRID corpus (mixed	GRID corpus (mixed-speech)
very promising results [11, 12, 13	GRID corpus (mixed	GRID corpus (mixed-speech)
]. However the task still 	GRID corpus (mixed	GRID corpus (mixed-speech)
remains challenging. Additionally, audio-only systems 	GRID corpus (mixed	GRID corpus (mixed-speech)
need separate models in order 	GRID corpus (mixed	GRID corpus (mixed-speech)
to associate the estimated separated 	GRID corpus (mixed	GRID corpus (mixed-speech)
audio sources to each speaker, 	GRID corpus (mixed	GRID corpus (mixed-speech)
while vision easily allow that 	GRID corpus (mixed	GRID corpus (mixed-speech)
in a unified model.  Regarding audio-visual speech enhancement and	GRID corpus (mixed	GRID corpus (mixed-speech)
 separa- tion methods an extensive	GRID corpus (mixed	GRID corpus (mixed-speech)
 review is provided in [14	GRID corpus (mixed	GRID corpus (mixed-speech)
]. Here we focus on 	GRID corpus (mixed	GRID corpus (mixed-speech)
the deep-learning methods that are 	GRID corpus (mixed	GRID corpus (mixed-speech)
most related to the present 	GRID corpus (mixed	GRID corpus (mixed-speech)
work.  Our first architecture (Section 2.1	GRID corpus (mixed	GRID corpus (mixed-speech)
) is inspired by [15], 	GRID corpus (mixed	GRID corpus (mixed-speech)
where a pre-trained convolutional neural 	GRID corpus (mixed	GRID corpus (mixed-speech)
network (CNN) is used to 	GRID corpus (mixed	GRID corpus (mixed-speech)
generate a clean spectrogram from 	GRID corpus (mixed	GRID corpus (mixed-speech)
silent video [16]. Rather than 	GRID corpus (mixed	GRID corpus (mixed-speech)
directly computing a time-frequency (T-F) 	GRID corpus (mixed	GRID corpus (mixed-speech)
mask,  ar X	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
iv :1  81 1	GRID corpus (mixed	GRID corpus (mixed-speech)
.  02 48	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
0v 3	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
cs  .C L	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
2	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
M 	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
2 01  9	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
the mask is computed by 	GRID corpus (mixed	GRID corpus (mixed-speech)
thresholding the estimated clean spectrogram. 	GRID corpus (mixed	GRID corpus (mixed-speech)
This approach is not very 	GRID corpus (mixed	GRID corpus (mixed-speech)
effective since the pre-trained CNN 	GRID corpus (mixed	GRID corpus (mixed-speech)
is designed for a different 	GRID corpus (mixed	GRID corpus (mixed-speech)
task (video-to- speech synthesis). 	GRID corpus (mixed	GRID corpus (mixed-speech)
In [17] a CNN is 	GRID corpus (mixed	GRID corpus (mixed-speech)
trained to directly esti- mate 	GRID corpus (mixed	GRID corpus (mixed-speech)
clean speech from noisy audio 	GRID corpus (mixed	GRID corpus (mixed-speech)
and input video. A sim- 	GRID corpus (mixed	GRID corpus (mixed-speech)
ilar model is used 	GRID corpus (mixed	GRID corpus (mixed-speech)
in [18], where the model 	GRID corpus (mixed	GRID corpus (mixed-speech)
jointly generates clean speech and 	GRID corpus (mixed	GRID corpus (mixed-speech)
input video in a denoising-autoender 	GRID corpus (mixed	GRID corpus (mixed-speech)
archi- tecture.  [19] shows that using information	GRID corpus (mixed	GRID corpus (mixed-speech)
 about lip positions can help	GRID corpus (mixed	GRID corpus (mixed-speech)
 to improve speech enhancement. The	GRID corpus (mixed	GRID corpus (mixed-speech)
 video feature vec- tor is	GRID corpus (mixed	GRID corpus (mixed-speech)
 obtained computing pair-wise distances between	GRID corpus (mixed	GRID corpus (mixed-speech)
 any mouth landmarks. Similarly to	GRID corpus (mixed	GRID corpus (mixed-speech)
 our approach their visual fea	GRID corpus (mixed	GRID corpus (mixed-speech)
- tures are not learned 	GRID corpus (mixed	GRID corpus (mixed-speech)
on the audio-visual dataset but 	GRID corpus (mixed	GRID corpus (mixed-speech)
are pro- vided by a 	GRID corpus (mixed	GRID corpus (mixed-speech)
system trained on different dataset. 	GRID corpus (mixed	GRID corpus (mixed-speech)
Contrary to our approach, [19] 	GRID corpus (mixed	GRID corpus (mixed-speech)
uses position-based features while we 	GRID corpus (mixed	GRID corpus (mixed-speech)
use motion features (of the 	GRID corpus (mixed	GRID corpus (mixed-speech)
whole face) that in our 	GRID corpus (mixed	GRID corpus (mixed-speech)
experiments turned out to be 	GRID corpus (mixed	GRID corpus (mixed-speech)
much more effective than positional 	GRID corpus (mixed	GRID corpus (mixed-speech)
features.  Although the aforementioned audio-visual methods	GRID corpus (mixed	GRID corpus (mixed-speech)
 work well, they have only	GRID corpus (mixed	GRID corpus (mixed-speech)
 been evaluated in a speaker-dependent	GRID corpus (mixed	GRID corpus (mixed-speech)
 setting. Only the availability of	GRID corpus (mixed	GRID corpus (mixed-speech)
 new large and heterogeneous audio-visual	GRID corpus (mixed	GRID corpus (mixed-speech)
 datasets has allowed the training	GRID corpus (mixed	GRID corpus (mixed-speech)
 of deep neu- ral network-based	GRID corpus (mixed	GRID corpus (mixed-speech)
 speaker-independent speech enhancement models [20	GRID corpus (mixed	GRID corpus (mixed-speech)
, 21, 22].  The present work shows that	GRID corpus (mixed	GRID corpus (mixed-speech)
 huge audio-visual datasets are not	GRID corpus (mixed	GRID corpus (mixed-speech)
 a necessary requirement for speaker-independent	GRID corpus (mixed	GRID corpus (mixed-speech)
 audio-visual speech enhancement. Although we	GRID corpus (mixed	GRID corpus (mixed-speech)
 have only considered datasets with	GRID corpus (mixed	GRID corpus (mixed-speech)
 simple visual scenarios (i.e., the	GRID corpus (mixed	GRID corpus (mixed-speech)
 target speaker is always facing	GRID corpus (mixed	GRID corpus (mixed-speech)
 the camera), we expect our	GRID corpus (mixed	GRID corpus (mixed-speech)
 methods to perform well in	GRID corpus (mixed	GRID corpus (mixed-speech)
 more complex scenarios thanks to	GRID corpus (mixed	GRID corpus (mixed-speech)
 the robust landmark extraction	GRID corpus (mixed	GRID corpus (mixed-speech)
.  2. MODEL ARCHITECTURES	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
We experimented with the four 	GRID corpus (mixed	GRID corpus (mixed-speech)
models shown in Fig. 1. 	GRID corpus (mixed	GRID corpus (mixed-speech)
All models receive in input 	GRID corpus (mixed	GRID corpus (mixed-speech)
the target speaker’s landmark mo- 	GRID corpus (mixed	GRID corpus (mixed-speech)
tion vectors and the power-law 	GRID corpus (mixed	GRID corpus (mixed-speech)
compressed spectrogram of the single-channel 	GRID corpus (mixed	GRID corpus (mixed-speech)
mixed-speech signal. All of them 	GRID corpus (mixed	GRID corpus (mixed-speech)
perform some kind of masking 	GRID corpus (mixed	GRID corpus (mixed-speech)
operation.  2.1. VL2M model	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
At each time frame, the 	GRID corpus (mixed	GRID corpus (mixed-speech)
video-landmark to mask (VL2M) model (	GRID corpus (mixed	GRID corpus (mixed-speech)
Fig. 1a) estimates a T-F 	GRID corpus (mixed	GRID corpus (mixed-speech)
mask from visual features only (	GRID corpus (mixed	GRID corpus (mixed-speech)
of the target speaker). Formally, 	GRID corpus (mixed	GRID corpus (mixed-speech)
given a video sequence 	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
 = [v1	GRID corpus (mixed	GRID corpus (mixed-speech)
, . . . , 	GRID corpus (mixed	GRID corpus (mixed-speech)
vT ], vt ∈ Rn 	GRID corpus (mixed	GRID corpus (mixed-speech)
and a target mask sequence 	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
 = [m1	GRID corpus (mixed	GRID corpus (mixed-speech)
, . . . ,	GRID corpus (mixed	GRID corpus (mixed-speech)
mT ], mt ∈ Rd, 	GRID corpus (mixed	GRID corpus (mixed-speech)
VL2M perform a function 	GRID corpus (mixed	GRID corpus (mixed-speech)
Fvl2m(v) = m̂, where m̂ 	GRID corpus (mixed	GRID corpus (mixed-speech)
is the estimated mask.  The training objective for VL2M	GRID corpus (mixed	GRID corpus (mixed-speech)
 is a Target Binary Mask	GRID corpus (mixed	GRID corpus (mixed-speech)
 (TBM) [23, 24], computed using	GRID corpus (mixed	GRID corpus (mixed-speech)
 the spectrogram of the tar	GRID corpus (mixed	GRID corpus (mixed-speech)
- get speaker only. This 	GRID corpus (mixed	GRID corpus (mixed-speech)
is motivated by our goal 	GRID corpus (mixed	GRID corpus (mixed-speech)
of extracting the speech of 	GRID corpus (mixed	GRID corpus (mixed-speech)
a target speaker as much 	GRID corpus (mixed	GRID corpus (mixed-speech)
as possible indepen- dently of 	GRID corpus (mixed	GRID corpus (mixed-speech)
the concurrent speakers, so that, 	GRID corpus (mixed	GRID corpus (mixed-speech)
e.g., we do not need 	GRID corpus (mixed	GRID corpus (mixed-speech)
to estimate their number. An 	GRID corpus (mixed	GRID corpus (mixed-speech)
additional motivations is that the 	GRID corpus (mixed	GRID corpus (mixed-speech)
model takes as only input 	GRID corpus (mixed	GRID corpus (mixed-speech)
the visual features of the  target speaker, and a target	GRID corpus (mixed	GRID corpus (mixed-speech)
 TBM that only depends on	GRID corpus (mixed	GRID corpus (mixed-speech)
 the target speaker allows VL2M	GRID corpus (mixed	GRID corpus (mixed-speech)
 to learn a function (rather	GRID corpus (mixed	GRID corpus (mixed-speech)
 than approximating an ill-posed one-to-many	GRID corpus (mixed	GRID corpus (mixed-speech)
 mapping	GRID corpus (mixed	GRID corpus (mixed-speech)
).  Given a clean speech spectrogram	GRID corpus (mixed	GRID corpus (mixed-speech)
 of a speaker s	GRID corpus (mixed	GRID corpus (mixed-speech)
 = [s1	GRID corpus (mixed	GRID corpus (mixed-speech)
, . . . , 	GRID corpus (mixed	GRID corpus (mixed-speech)
sT ], st ∈ Rd, 	GRID corpus (mixed	GRID corpus (mixed-speech)
the TBM is defined by 	GRID corpus (mixed	GRID corpus (mixed-speech)
comparing, at each frequency bin 	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
 ∈ [1	GRID corpus (mixed	GRID corpus (mixed-speech)
, . . . , 	GRID corpus (mixed	GRID corpus (mixed-speech)
d], the target speaker value 	GRID corpus (mixed	GRID corpus (mixed-speech)
st[f ] vs. a reference 	GRID corpus (mixed	GRID corpus (mixed-speech)
threshold τ [f ]. As 	GRID corpus (mixed	GRID corpus (mixed-speech)
in [15], we use a 	GRID corpus (mixed	GRID corpus (mixed-speech)
function of long-term average speech 	GRID corpus (mixed	GRID corpus (mixed-speech)
spectrum (LTASS) as reference threshold. 	GRID corpus (mixed	GRID corpus (mixed-speech)
This threshold indicates if a 	GRID corpus (mixed	GRID corpus (mixed-speech)
T-F unit is generated by 	GRID corpus (mixed	GRID corpus (mixed-speech)
the speaker or refers to 	GRID corpus (mixed	GRID corpus (mixed-speech)
silence or noise. The process 	GRID corpus (mixed	GRID corpus (mixed-speech)
to compute the speaker’s TBM 	GRID corpus (mixed	GRID corpus (mixed-speech)
is as follows:  1. The mean π[f	GRID corpus (mixed	GRID corpus (mixed-speech)
 ] and the standard deviation	GRID corpus (mixed	GRID corpus (mixed-speech)
 σ[f ] are computed for	GRID corpus (mixed	GRID corpus (mixed-speech)
 all frequency bins of all	GRID corpus (mixed	GRID corpus (mixed-speech)
 seen spectro- grams in speaker’s	GRID corpus (mixed	GRID corpus (mixed-speech)
 data	GRID corpus (mixed	GRID corpus (mixed-speech)
.  2. The threshold τ [f	GRID corpus (mixed	GRID corpus (mixed-speech)
 ] is defined as τ	GRID corpus (mixed	GRID corpus (mixed-speech)
 [f ] = π[f ]+0.6	GRID corpus (mixed	GRID corpus (mixed-speech)
 ·σ[f ] where 0.6 is	GRID corpus (mixed	GRID corpus (mixed-speech)
 a value selected by manual	GRID corpus (mixed	GRID corpus (mixed-speech)
 inspection of several spectrogram-TBM pairs	GRID corpus (mixed	GRID corpus (mixed-speech)
.  3. The threshold is applied	GRID corpus (mixed	GRID corpus (mixed-speech)
 to every speaker’s speech spec	GRID corpus (mixed	GRID corpus (mixed-speech)
- trogram s.  mt[f	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
1, if st[f ] ≥ 	GRID corpus (mixed	GRID corpus (mixed-speech)
τ [f ], 0, otherwise.  The mapping Fvl2m(·) is carried	GRID corpus (mixed	GRID corpus (mixed-speech)
 out by a stacked bi	GRID corpus (mixed	GRID corpus (mixed-speech)
- directional Long Short-Term Memory (	GRID corpus (mixed	GRID corpus (mixed-speech)
BLSTM) network [25]. The BLSTM 	GRID corpus (mixed	GRID corpus (mixed-speech)
outputs are then forced to 	GRID corpus (mixed	GRID corpus (mixed-speech)
lay within the [0, 1] 	GRID corpus (mixed	GRID corpus (mixed-speech)
range. Finally the computed TBM 	GRID corpus (mixed	GRID corpus (mixed-speech)
m̂ and the noisy spectrogram 	GRID corpus (mixed	GRID corpus (mixed-speech)
y are element-wise multiplied to 	GRID corpus (mixed	GRID corpus (mixed-speech)
ob- tain the estimated clean 	GRID corpus (mixed	GRID corpus (mixed-speech)
spectrogram ŝm = m̂ ◦ 	GRID corpus (mixed	GRID corpus (mixed-speech)
y, where 	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
 = [y1	GRID corpus (mixed	GRID corpus (mixed-speech)
, . . . 	GRID corpus (mixed	GRID corpus (mixed-speech)
yT ], yt ∈ Rd.  The model parameters are estimated	GRID corpus (mixed	GRID corpus (mixed-speech)
 to minimize the loss	GRID corpus (mixed	GRID corpus (mixed-speech)
:  Jvl2m = ∑T	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
t=1  ∑d f=1−mt[f ] · log(m̂t[f	GRID corpus (mixed	GRID corpus (mixed-speech)
 ])− (1−mt[f ]) · log(1	GRID corpus (mixed	GRID corpus (mixed-speech)
− m̂t[f ])  2.2. VL2M ref model	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
VL2M generates T-F masks that 	GRID corpus (mixed	GRID corpus (mixed-speech)
are independent of the acous- 	GRID corpus (mixed	GRID corpus (mixed-speech)
tic context. We may want 	GRID corpus (mixed	GRID corpus (mixed-speech)
to refine the masking by 	GRID corpus (mixed	GRID corpus (mixed-speech)
including such context. This is 	GRID corpus (mixed	GRID corpus (mixed-speech)
what the novel VL2M ref 	GRID corpus (mixed	GRID corpus (mixed-speech)
does (Fig. 1b). The computed 	GRID corpus (mixed	GRID corpus (mixed-speech)
TBM m̂ and the input 	GRID corpus (mixed	GRID corpus (mixed-speech)
spectrogram y are the input 	GRID corpus (mixed	GRID corpus (mixed-speech)
to a function that outputs 	GRID corpus (mixed	GRID corpus (mixed-speech)
an Ideal Amplitude Mask (IAM) 	GRID corpus (mixed	GRID corpus (mixed-speech)
p (known as FFT-MASK 	GRID corpus (mixed	GRID corpus (mixed-speech)
in [8]). Given the target 	GRID corpus (mixed	GRID corpus (mixed-speech)
clean spectrogram s and the 	GRID corpus (mixed	GRID corpus (mixed-speech)
noisy spectrogram y, the IAM 	GRID corpus (mixed	GRID corpus (mixed-speech)
is defined as:  pt[f ] = st[f	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
yt[f ]  Note that although IAM generation	GRID corpus (mixed	GRID corpus (mixed-speech)
 requires the mixed-speech spectrogram, separate	GRID corpus (mixed	GRID corpus (mixed-speech)
 spectrograms for each concurrent speakers	GRID corpus (mixed	GRID corpus (mixed-speech)
 are not required	GRID corpus (mixed	GRID corpus (mixed-speech)
.  The target speaker’s spectrogram s	GRID corpus (mixed	GRID corpus (mixed-speech)
 is reconstructed by multiplying the	GRID corpus (mixed	GRID corpus (mixed-speech)
 input spectrogram with the estimated	GRID corpus (mixed	GRID corpus (mixed-speech)
 IAM. Values greater than 10	GRID corpus (mixed	GRID corpus (mixed-speech)
 in the IAM are clipped	GRID corpus (mixed	GRID corpus (mixed-speech)
 to 10 in order to	GRID corpus (mixed	GRID corpus (mixed-speech)
 obtain better numerical stability as	GRID corpus (mixed	GRID corpus (mixed-speech)
 suggested in [8	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
v: video input y: noisy 	GRID corpus (mixed	GRID corpus (mixed-speech)
spectrogram sm: clean spectrogram TBM 	GRID corpus (mixed	GRID corpus (mixed-speech)
s: clean spectrogram IAM m: 	GRID corpus (mixed	GRID corpus (mixed-speech)
TBM p: IAM  STACKED	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
BLSTM  m	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
sm  v	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
y  (a) VL2M	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
v VL2M m  y BLSTM	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
BLSTM  Fusion layer	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
BLSTM p  s	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
b) VL2M ref  v	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
y  p STACKED	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
BLSTM  s	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
c) Audio-Visual concat  sm	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
y  p STACKED	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
BLSTM  s	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
v VL2M m  (d) Audio-Visual concat-ref	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
Fig. 1. Model architectures.  The model performs a function	GRID corpus (mixed	GRID corpus (mixed-speech)
 Fmr(v, y) = p̂ that	GRID corpus (mixed	GRID corpus (mixed-speech)
 con- sists of a VL2M	GRID corpus (mixed	GRID corpus (mixed-speech)
 component plus three different BLSTMs	GRID corpus (mixed	GRID corpus (mixed-speech)
 Gm, Gy and H	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
Gm(Fvl2m(v)) = rm receives the 	GRID corpus (mixed	GRID corpus (mixed-speech)
VL2M mask m̂ as in- 	GRID corpus (mixed	GRID corpus (mixed-speech)
put, and Gy(y) = ry 	GRID corpus (mixed	GRID corpus (mixed-speech)
is fed with the noisy 	GRID corpus (mixed	GRID corpus (mixed-speech)
spectrogram. Their output rm, 	GRID corpus (mixed	GRID corpus (mixed-speech)
ry ∈ Rz are fused 	GRID corpus (mixed	GRID corpus (mixed-speech)
in a joint audio-visual represen- 	GRID corpus (mixed	GRID corpus (mixed-speech)
tation 	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
 = [h1	GRID corpus (mixed	GRID corpus (mixed-speech)
, . . . ,	GRID corpus (mixed	GRID corpus (mixed-speech)
hT ], where ht is 	GRID corpus (mixed	GRID corpus (mixed-speech)
a linear combination of rmt 	GRID corpus (mixed	GRID corpus (mixed-speech)
and ryt : ht = 	GRID corpus (mixed	GRID corpus (mixed-speech)
Whm ·rmt +Why ·ryt +bh. 	GRID corpus (mixed	GRID corpus (mixed-speech)
h is the input of 	GRID corpus (mixed	GRID corpus (mixed-speech)
the third BLSTM H (	GRID corpus (mixed	GRID corpus (mixed-speech)
h) = p̂, where p̂ 	GRID corpus (mixed	GRID corpus (mixed-speech)
lays in the [0,10] range. 	GRID corpus (mixed	GRID corpus (mixed-speech)
The loss function is:  Jmr	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
T∑ t=1  d∑ f=1	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
p̂t[f ] · yt[f ]− 	GRID corpus (mixed	GRID corpus (mixed-speech)
st[f ])2  2.3. Audio-Visual concat model	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
The third model (Fig. 1c) 	GRID corpus (mixed	GRID corpus (mixed-speech)
performs early fusion of audio- 	GRID corpus (mixed	GRID corpus (mixed-speech)
visual features. This model consists 	GRID corpus (mixed	GRID corpus (mixed-speech)
of a single stacked BLSTM 	GRID corpus (mixed	GRID corpus (mixed-speech)
that computes the IAM mask 	GRID corpus (mixed	GRID corpus (mixed-speech)
p̂ from the concate- 	GRID corpus (mixed	GRID corpus (mixed-speech)
nated [v,y]. The training loss 	GRID corpus (mixed	GRID corpus (mixed-speech)
is the same Jmr used 	GRID corpus (mixed	GRID corpus (mixed-speech)
to train VL2M ref. This 	GRID corpus (mixed	GRID corpus (mixed-speech)
model can be regarded as 	GRID corpus (mixed	GRID corpus (mixed-speech)
a simplification of VL2M ref, 	GRID corpus (mixed	GRID corpus (mixed-speech)
where the VL2M operation is 	GRID corpus (mixed	GRID corpus (mixed-speech)
not performed.  2.4. Audio-Visual concat-ref model	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
The fourth model (Fig. 1d) 	GRID corpus (mixed	GRID corpus (mixed-speech)
is an improved version of 	GRID corpus (mixed	GRID corpus (mixed-speech)
the model described in section 2	GRID corpus (mixed	GRID corpus (mixed-speech)
.3. The only difference is 	GRID corpus (mixed	GRID corpus (mixed-speech)
the input of the stacked 	GRID corpus (mixed	GRID corpus (mixed-speech)
BLSTM that is replaced 	GRID corpus (mixed	GRID corpus (mixed-speech)
by [̂sm,y] where ŝm is 	GRID corpus (mixed	GRID corpus (mixed-speech)
the denoised spectrogram returned by 	GRID corpus (mixed	GRID corpus (mixed-speech)
VL2M operation.  3. EXPERIMENTAL SETUP	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
3.1. Dataset  All experiments were carried out	GRID corpus (mixed	GRID corpus (mixed-speech)
 using the GRID [9] and	GRID corpus (mixed	GRID corpus (mixed-speech)
 TCD-TIMIT [10] audio-visual datasets. For	GRID corpus (mixed	GRID corpus (mixed-speech)
 each of them, we created	GRID corpus (mixed	GRID corpus (mixed-speech)
 a mixed-speech version	GRID corpus (mixed	GRID corpus (mixed-speech)
.  Regarding the GRID corpus, for	GRID corpus (mixed	GRID corpus (mixed-speech)
 each of the 33 speakers	GRID corpus (mixed	GRID corpus (mixed-speech)
 (one had to be discarded	GRID corpus (mixed	GRID corpus (mixed-speech)
) we first randomly selected 200 ut- terances (out of 1000	GRID corpus (mixed	GRID corpus (mixed-speech)
). Then, for each utterance, 	GRID corpus (mixed	GRID corpus (mixed-speech)
we created 3 different audio-mixed 	GRID corpus (mixed	GRID corpus (mixed-speech)
samples. Each audio-mixed sample was 	GRID corpus (mixed	GRID corpus (mixed-speech)
created by mixing the chosen 	GRID corpus (mixed	GRID corpus (mixed-speech)
utterance with one utter- ance 	GRID corpus (mixed	GRID corpus (mixed-speech)
from a different speaker.  That resulted in 600 audio-mixed	GRID corpus (mixed	GRID corpus (mixed-speech)
 samples per speaker	GRID corpus (mixed	GRID corpus (mixed-speech)
.  The resulting dataset was split	GRID corpus (mixed	GRID corpus (mixed-speech)
 into disjoint sets of 25/4/4	GRID corpus (mixed	GRID corpus (mixed-speech)
 speakers for training/validation/testing respectively	GRID corpus (mixed	GRID corpus (mixed-speech)
.  The TCD-TIMIT corpus consists of	GRID corpus (mixed	GRID corpus (mixed-speech)
 59 speakers (we ex- cluded	GRID corpus (mixed	GRID corpus (mixed-speech)
 3 professionally-trained lipspeakers) and 98	GRID corpus (mixed	GRID corpus (mixed-speech)
 utterances per speaker. The mixed-speech	GRID corpus (mixed	GRID corpus (mixed-speech)
 version was created following the	GRID corpus (mixed	GRID corpus (mixed-speech)
 same procedure as for GRID	GRID corpus (mixed	GRID corpus (mixed-speech)
, with one difference. Con- 	GRID corpus (mixed	GRID corpus (mixed-speech)
trary to GRID, TCD-TIMIT utterances 	GRID corpus (mixed	GRID corpus (mixed-speech)
have different dura- tion. Thus 2 utterances were mixed only if	GRID corpus (mixed	GRID corpus (mixed-speech)
 their duration dif- ference did	GRID corpus (mixed	GRID corpus (mixed-speech)
 not exceed 2 seconds. For	GRID corpus (mixed	GRID corpus (mixed-speech)
 each utterance pair, we forced	GRID corpus (mixed	GRID corpus (mixed-speech)
 the non-target speaker’s utterance to	GRID corpus (mixed	GRID corpus (mixed-speech)
 match the du- ration of	GRID corpus (mixed	GRID corpus (mixed-speech)
 the target speaker utterance. If	GRID corpus (mixed	GRID corpus (mixed-speech)
 it was longer, the utterance	GRID corpus (mixed	GRID corpus (mixed-speech)
 was cut at its end	GRID corpus (mixed	GRID corpus (mixed-speech)
, whereas if it was 	GRID corpus (mixed	GRID corpus (mixed-speech)
shorter, silence samples were equally 	GRID corpus (mixed	GRID corpus (mixed-speech)
added at its start and 	GRID corpus (mixed	GRID corpus (mixed-speech)
end.  The resulting dataset was split	GRID corpus (mixed	GRID corpus (mixed-speech)
 into disjoint sets of 51/4/4	GRID corpus (mixed	GRID corpus (mixed-speech)
 speakers for training/validation/testing respectively	GRID corpus (mixed	GRID corpus (mixed-speech)
.  3.2. LSTM training	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
In all experiments, the models 	GRID corpus (mixed	GRID corpus (mixed-speech)
were trained using the Adam 	GRID corpus (mixed	GRID corpus (mixed-speech)
optimizer [26]. Early stopping was 	GRID corpus (mixed	GRID corpus (mixed-speech)
applied when the error on 	GRID corpus (mixed	GRID corpus (mixed-speech)
the validation set did not 	GRID corpus (mixed	GRID corpus (mixed-speech)
decrease over 5 consecutive epochs.  VL2M, AV concat and AV	GRID corpus (mixed	GRID corpus (mixed-speech)
 concat-ref had 5, 3 and	GRID corpus (mixed	GRID corpus (mixed-speech)
 3 stacked BLSTM layers respectively	GRID corpus (mixed	GRID corpus (mixed-speech)
. All BLSTMs had 250 	GRID corpus (mixed	GRID corpus (mixed-speech)
units. Hyper-parameters selection was performed 	GRID corpus (mixed	GRID corpus (mixed-speech)
by using random search with 	GRID corpus (mixed	GRID corpus (mixed-speech)
a limited number of samples, 	GRID corpus (mixed	GRID corpus (mixed-speech)
therefore all the reported results 	GRID corpus (mixed	GRID corpus (mixed-speech)
may improve through a deeper 	GRID corpus (mixed	GRID corpus (mixed-speech)
hyper- parameters validation phase.  VL2M ref and AV concat-ref	GRID corpus (mixed	GRID corpus (mixed-speech)
 training was performed in 2	GRID corpus (mixed	GRID corpus (mixed-speech)
 steps. We first pre-trained the	GRID corpus (mixed	GRID corpus (mixed-speech)
 models using the oracle TBM	GRID corpus (mixed	GRID corpus (mixed-speech)
 m. Then we substituted the	GRID corpus (mixed	GRID corpus (mixed-speech)
 oracle masks with the VL2M	GRID corpus (mixed	GRID corpus (mixed-speech)
 component and retrained the models	GRID corpus (mixed	GRID corpus (mixed-speech)
 while freezing the pa- rameters	GRID corpus (mixed	GRID corpus (mixed-speech)
 of the VL2M component	GRID corpus (mixed	GRID corpus (mixed-speech)
.  3.3. Audio pre- and post-processing	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
The original waveforms were resampled 	GRID corpus (mixed	GRID corpus (mixed-speech)
to 16 kHz. Short- Time 	GRID corpus (mixed	GRID corpus (mixed-speech)
Fourier Transform (STFT) x was 	GRID corpus (mixed	GRID corpus (mixed-speech)
computed using FFT size of 512, Hann window of length 25	GRID corpus (mixed	GRID corpus (mixed-speech)
 ms (400 samples), and hop	GRID corpus (mixed	GRID corpus (mixed-speech)
 length of 10 ms (160	GRID corpus (mixed	GRID corpus (mixed-speech)
 samples). The input spectro- gram	GRID corpus (mixed	GRID corpus (mixed-speech)
 was obtained taking the STFT	GRID corpus (mixed	GRID corpus (mixed-speech)
 magnitude and perform- ing power-law	GRID corpus (mixed	GRID corpus (mixed-speech)
 compression |x|p with p	GRID corpus (mixed	GRID corpus (mixed-speech)
 = 0.3. Finally we applied	GRID corpus (mixed	GRID corpus (mixed-speech)
 per-speaker 0-mean 1-std normalization	GRID corpus (mixed	GRID corpus (mixed-speech)
.  In the post-processing stage, the	GRID corpus (mixed	GRID corpus (mixed-speech)
 enhanced waveform gen- erated by	GRID corpus (mixed	GRID corpus (mixed-speech)
 the speech enhancement models was	GRID corpus (mixed	GRID corpus (mixed-speech)
 reconstructed	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
SDR PESQ ViSQOL  Noisy −1.06 1.81 2.11 VL2M	GRID corpus (mixed	GRID corpus (mixed-speech)
 3.17 1.51 1.16 VL2M ref	GRID corpus (mixed	GRID corpus (mixed-speech)
 6.50 2.58 2.99 AV concat	GRID corpus (mixed	GRID corpus (mixed-speech)
 6.31 2.49 2.83 AV c-ref	GRID corpus (mixed	GRID corpus (mixed-speech)
 6.17 2.58 2.96	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
Table 1. GRID results - 	GRID corpus (mixed	GRID corpus (mixed-speech)
speaker-dependent. The “Noisy” row refers 	GRID corpus (mixed	GRID corpus (mixed-speech)
to the metric values of 	GRID corpus (mixed	GRID corpus (mixed-speech)
the input mixed-speech signal.  2 Speakers 3 Speakers SDR	GRID corpus (mixed	GRID corpus (mixed-speech)
 PESQ ViSQOL SDR PESQ ViSQOL	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
Noisy 0.21 1.94 2.58 −5.34 1	GRID corpus (mixed	GRID corpus (mixed-speech)
.43 1.62 VL2M 3.02 1.81 1	GRID corpus (mixed	GRID corpus (mixed-speech)
.70 −2.03 1.43 1.25 VL2M 	GRID corpus (mixed	GRID corpus (mixed-speech)
ref 6.52 2.53 3.02 2.83 2	GRID corpus (mixed	GRID corpus (mixed-speech)
.19 2.53 AV concat 7.37 2	GRID corpus (mixed	GRID corpus (mixed-speech)
.65 3.03 3.02 2.24 2.49 	GRID corpus (mixed	GRID corpus (mixed-speech)
AV c-ref 8.05 2.70 3.07 4	GRID corpus (mixed	GRID corpus (mixed-speech)
.02 2.33 2.64  Table 2. GRID results	GRID corpus (mixed	GRID corpus (mixed-speech)
 - speaker-independent	GRID corpus (mixed	GRID corpus (mixed-speech)
.  by applying the inverse STFT	GRID corpus (mixed	GRID corpus (mixed-speech)
 to the estimated clean spectro	GRID corpus (mixed	GRID corpus (mixed-speech)
- gram and using the 	GRID corpus (mixed	GRID corpus (mixed-speech)
phase of the noisy input 	GRID corpus (mixed	GRID corpus (mixed-speech)
signal.  3.4. Video pre-processing	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
Face landmarks were extracted from 	GRID corpus (mixed	GRID corpus (mixed-speech)
video using the Dlib [7] 	GRID corpus (mixed	GRID corpus (mixed-speech)
implementation of the face landmark 	GRID corpus (mixed	GRID corpus (mixed-speech)
estimator described in [6]. It 	GRID corpus (mixed	GRID corpus (mixed-speech)
returns 68 x-y points, for 	GRID corpus (mixed	GRID corpus (mixed-speech)
an overall 136 values. We 	GRID corpus (mixed	GRID corpus (mixed-speech)
upsampled from 25/29.97 fps (GRID/TCD-TIMIT) 	GRID corpus (mixed	GRID corpus (mixed-speech)
to 100 fps to match 	GRID corpus (mixed	GRID corpus (mixed-speech)
the frame rate of the 	GRID corpus (mixed	GRID corpus (mixed-speech)
audio spectrogram. Upsampling was carried 	GRID corpus (mixed	GRID corpus (mixed-speech)
out through linear interpolation over 	GRID corpus (mixed	GRID corpus (mixed-speech)
time.  The final video feature vector	GRID corpus (mixed	GRID corpus (mixed-speech)
 v was obtained by com	GRID corpus (mixed	GRID corpus (mixed-speech)
- puting the per-speaker normalized 	GRID corpus (mixed	GRID corpus (mixed-speech)
motion vector of the face 	GRID corpus (mixed	GRID corpus (mixed-speech)
landmarks by simply subtracting every 	GRID corpus (mixed	GRID corpus (mixed-speech)
frame with the previ- ous 	GRID corpus (mixed	GRID corpus (mixed-speech)
one. The motion vector of 	GRID corpus (mixed	GRID corpus (mixed-speech)
the first frame was set 	GRID corpus (mixed	GRID corpus (mixed-speech)
to zero.  4. RESULTS	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
In order to compare our 	GRID corpus (mixed	GRID corpus (mixed-speech)
models to previous works in 	GRID corpus (mixed	GRID corpus (mixed-speech)
both speech enhancement and separation, 	GRID corpus (mixed	GRID corpus (mixed-speech)
we evaluated the perfor- mance 	GRID corpus (mixed	GRID corpus (mixed-speech)
of the proposed models using 	GRID corpus (mixed	GRID corpus (mixed-speech)
both speech separation  2 Speakers 3 Speakers SDR	GRID corpus (mixed	GRID corpus (mixed-speech)
 PESQ ViSQOL SDR PESQ ViSQOL	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
Noisy 0.21 2.22 2.74 −3.42 1	GRID corpus (mixed	GRID corpus (mixed-speech)
.92 2.04 VL2M 2.88 2.25 2	GRID corpus (mixed	GRID corpus (mixed-speech)
.62 −0.51 1.99 1.98 VL2M 	GRID corpus (mixed	GRID corpus (mixed-speech)
ref 9.24 2.81 3.09 5.27 2	GRID corpus (mixed	GRID corpus (mixed-speech)
.44 2.54 AV concat 9.56 2	GRID corpus (mixed	GRID corpus (mixed-speech)
.80 3.09 5.15 2.41 2.52 	GRID corpus (mixed	GRID corpus (mixed-speech)
AV c-ref 10.55 3.03 3.21 5	GRID corpus (mixed	GRID corpus (mixed-speech)
.37 2.45 2.58  Table 3. TCD-TIMIT results	GRID corpus (mixed	GRID corpus (mixed-speech)
 - speaker-independent	GRID corpus (mixed	GRID corpus (mixed-speech)
.  and enhancement metrics. Specifically, we	GRID corpus (mixed	GRID corpus (mixed-speech)
 measured the ca- pability of	GRID corpus (mixed	GRID corpus (mixed-speech)
 separating the target utterance from	GRID corpus (mixed	GRID corpus (mixed-speech)
 the concurrent utterance with the	GRID corpus (mixed	GRID corpus (mixed-speech)
 source-to-distortion ratio (SDR) [27, 28	GRID corpus (mixed	GRID corpus (mixed-speech)
]. While the quality of 	GRID corpus (mixed	GRID corpus (mixed-speech)
estimated target speech was measured 	GRID corpus (mixed	GRID corpus (mixed-speech)
with the perceptual PESQ [29] 	GRID corpus (mixed	GRID corpus (mixed-speech)
and ViSQOL [30] metrics. For 	GRID corpus (mixed	GRID corpus (mixed-speech)
PESQ we used the narrow 	GRID corpus (mixed	GRID corpus (mixed-speech)
band mode while for ViSQOL 	GRID corpus (mixed	GRID corpus (mixed-speech)
we used the wide band 	GRID corpus (mixed	GRID corpus (mixed-speech)
mode.  As a very first experiment	GRID corpus (mixed	GRID corpus (mixed-speech)
 we compared landmark posi- tion	GRID corpus (mixed	GRID corpus (mixed-speech)
 vs. landmark motion vectors. It	GRID corpus (mixed	GRID corpus (mixed-speech)
 turned out that landmark positions	GRID corpus (mixed	GRID corpus (mixed-speech)
 performed poorly, thus all results	GRID corpus (mixed	GRID corpus (mixed-speech)
 reported here refer to landmark	GRID corpus (mixed	GRID corpus (mixed-speech)
 motion vectors only	GRID corpus (mixed	GRID corpus (mixed-speech)
.  We then carried out some	GRID corpus (mixed	GRID corpus (mixed-speech)
 speaker-dependent experiments to compare our	GRID corpus (mixed	GRID corpus (mixed-speech)
 models with previous studies as	GRID corpus (mixed	GRID corpus (mixed-speech)
, to the best of 	GRID corpus (mixed	GRID corpus (mixed-speech)
our knowledge, there are no 	GRID corpus (mixed	GRID corpus (mixed-speech)
reported results of speaker- independent 	GRID corpus (mixed	GRID corpus (mixed-speech)
systems trained and tested on 	GRID corpus (mixed	GRID corpus (mixed-speech)
GRID and TCD- TIMIT to 	GRID corpus (mixed	GRID corpus (mixed-speech)
compare with. Table 1 reports 	GRID corpus (mixed	GRID corpus (mixed-speech)
the test-set evalua- tion of 	GRID corpus (mixed	GRID corpus (mixed-speech)
speaker-dependent models on the GRID 	GRID corpus (mixed	GRID corpus (mixed-speech)
corpus with landmark motion vectors. 	GRID corpus (mixed	GRID corpus (mixed-speech)
Results are comparable with previ- 	GRID corpus (mixed	GRID corpus (mixed-speech)
ous state-of-the-art studies in an 	GRID corpus (mixed	GRID corpus (mixed-speech)
almost identical setting [15, 17].  Table 2 and 3 show	GRID corpus (mixed	GRID corpus (mixed-speech)
 speaker-independent test-set results on the	GRID corpus (mixed	GRID corpus (mixed-speech)
 GRID and TCD-TIMIT datasets respectively	GRID corpus (mixed	GRID corpus (mixed-speech)
. V2ML performs significantly worse 	GRID corpus (mixed	GRID corpus (mixed-speech)
than the other three models 	GRID corpus (mixed	GRID corpus (mixed-speech)
in- dicating that a successful 	GRID corpus (mixed	GRID corpus (mixed-speech)
mask generation has to depend 	GRID corpus (mixed	GRID corpus (mixed-speech)
on the acoustic context. The 	GRID corpus (mixed	GRID corpus (mixed-speech)
performance of the three models 	GRID corpus (mixed	GRID corpus (mixed-speech)
in the speaker-independent setting is 	GRID corpus (mixed	GRID corpus (mixed-speech)
comparable to that in the 	GRID corpus (mixed	GRID corpus (mixed-speech)
speaker-dependent setting.  AV concat-ref outperforms V2ML ref	GRID corpus (mixed	GRID corpus (mixed-speech)
 and AV concat for both	GRID corpus (mixed	GRID corpus (mixed-speech)
 datasets. This supports the utility	GRID corpus (mixed	GRID corpus (mixed-speech)
 of a refinement strat- egy	GRID corpus (mixed	GRID corpus (mixed-speech)
 and suggests that the refinement	GRID corpus (mixed	GRID corpus (mixed-speech)
 is more effective when it	GRID corpus (mixed	GRID corpus (mixed-speech)
 directly refines the estimated clean	GRID corpus (mixed	GRID corpus (mixed-speech)
 spectrogram, rather than refining the	GRID corpus (mixed	GRID corpus (mixed-speech)
 estimated mask	GRID corpus (mixed	GRID corpus (mixed-speech)
.  Finally, we evaluated the systems	GRID corpus (mixed	GRID corpus (mixed-speech)
 in a more challenging testing	GRID corpus (mixed	GRID corpus (mixed-speech)
 condition where the target utterance	GRID corpus (mixed	GRID corpus (mixed-speech)
 was mixed with 2 utterances	GRID corpus (mixed	GRID corpus (mixed-speech)
 from 2 competing speakers. Despite	GRID corpus (mixed	GRID corpus (mixed-speech)
 the model was trained with	GRID corpus (mixed	GRID corpus (mixed-speech)
 mixtures of two speakers, the	GRID corpus (mixed	GRID corpus (mixed-speech)
 decrease of performance was not	GRID corpus (mixed	GRID corpus (mixed-speech)
 dramatic	GRID corpus (mixed	GRID corpus (mixed-speech)
.  Code and some testing examples	GRID corpus (mixed	GRID corpus (mixed-speech)
 of our models are avail	GRID corpus (mixed	GRID corpus (mixed-speech)
- able at https://goo.gl/3h1NgE.  5. CONCLUSION	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
This paper proposes the use 	GRID corpus (mixed	GRID corpus (mixed-speech)
of face landmark motion vec- 	GRID corpus (mixed	GRID corpus (mixed-speech)
tors for audio-visual speech enhancement 	GRID corpus (mixed	GRID corpus (mixed-speech)
in a single-channel multi-talker scenario. 	GRID corpus (mixed	GRID corpus (mixed-speech)
Different models are tested where 	GRID corpus (mixed	GRID corpus (mixed-speech)
land- mark motion vectors are 	GRID corpus (mixed	GRID corpus (mixed-speech)
used to generate time-frequency (T- 	GRID corpus (mixed	GRID corpus (mixed-speech)
F) masks that extract the 	GRID corpus (mixed	GRID corpus (mixed-speech)
target speaker’s spectrogram from the 	GRID corpus (mixed	GRID corpus (mixed-speech)
acoustic mixed-speech spectrogram.  To the best of our	GRID corpus (mixed	GRID corpus (mixed-speech)
 knowledge, some of the proposed	GRID corpus (mixed	GRID corpus (mixed-speech)
 mod- els are the first	GRID corpus (mixed	GRID corpus (mixed-speech)
 models trained and evaluated on	GRID corpus (mixed	GRID corpus (mixed-speech)
 the limited size GRID and	GRID corpus (mixed	GRID corpus (mixed-speech)
 TCD-TIMIT datasets that accomplish speaker	GRID corpus (mixed	GRID corpus (mixed-speech)
- independent speech enhancement in 	GRID corpus (mixed	GRID corpus (mixed-speech)
the multi-talker setting, with a 	GRID corpus (mixed	GRID corpus (mixed-speech)
quality of enhancement comparable to 	GRID corpus (mixed	GRID corpus (mixed-speech)
that achieved in a speaker-dependent 	GRID corpus (mixed	GRID corpus (mixed-speech)
setting.  https://goo.gl/3h1NgE	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
6. REFERENCES  [1] E. Colin Cherry, “Some	GRID corpus (mixed	GRID corpus (mixed-speech)
 experiments on the recognition of	GRID corpus (mixed	GRID corpus (mixed-speech)
 speech, with one and with	GRID corpus (mixed	GRID corpus (mixed-speech)
 two ears,” The Journal of	GRID corpus (mixed	GRID corpus (mixed-speech)
 the Acoustical Society of America	GRID corpus (mixed	GRID corpus (mixed-speech)
, vol. 25, no. 5, 	GRID corpus (mixed	GRID corpus (mixed-speech)
pp. 975–979, 1953.  [2] Josh H McDermott, “The	GRID corpus (mixed	GRID corpus (mixed-speech)
 cocktail party problem,” Current Biology	GRID corpus (mixed	GRID corpus (mixed-speech)
, vol. 19, no. 22, 	GRID corpus (mixed	GRID corpus (mixed-speech)
pp. R1024–R1027, 2009.  [3] Elana Zion Golumbic, Gregory	GRID corpus (mixed	GRID corpus (mixed-speech)
 B. Cogan, Charles E. Schroeder	GRID corpus (mixed	GRID corpus (mixed-speech)
, and David Poeppel, “Visual 	GRID corpus (mixed	GRID corpus (mixed-speech)
input enhances selective speech envelope 	GRID corpus (mixed	GRID corpus (mixed-speech)
tracking in auditory cortex at 	GRID corpus (mixed	GRID corpus (mixed-speech)
a “cocktail party”,” Journal of 	GRID corpus (mixed	GRID corpus (mixed-speech)
Neu- roscience, vol. 33, no. 4, pp. 1417–1426, 2013	GRID corpus (mixed	GRID corpus (mixed-speech)
.  [4] Wei Ji Ma, Xiang	GRID corpus (mixed	GRID corpus (mixed-speech)
 Zhou, Lars A. Ross, John	GRID corpus (mixed	GRID corpus (mixed-speech)
 J. Foxe, and Lucas C	GRID corpus (mixed	GRID corpus (mixed-speech)
. Parra, “Lip-reading aids word 	GRID corpus (mixed	GRID corpus (mixed-speech)
recognition most in moderate noise: 	GRID corpus (mixed	GRID corpus (mixed-speech)
A bayesian explanation using high-dimensional 	GRID corpus (mixed	GRID corpus (mixed-speech)
feature space,” PLOS ONE, vol. 4, no. 3, pp. 1–14, 03	GRID corpus (mixed	GRID corpus (mixed-speech)
 2009	GRID corpus (mixed	GRID corpus (mixed-speech)
.  [5] Albert S Bregman, Auditory	GRID corpus (mixed	GRID corpus (mixed-speech)
 scene analysis: The perceptual organi	GRID corpus (mixed	GRID corpus (mixed-speech)
- zation of sound, MIT 	GRID corpus (mixed	GRID corpus (mixed-speech)
press, 1994.  [6] Vahid Kazemi and Josephine	GRID corpus (mixed	GRID corpus (mixed-speech)
 Sullivan, “One millisecond face align	GRID corpus (mixed	GRID corpus (mixed-speech)
- ment with an ensemble 	GRID corpus (mixed	GRID corpus (mixed-speech)
of regression trees,” in The 	GRID corpus (mixed	GRID corpus (mixed-speech)
IEEE Conference on Computer Vision 	GRID corpus (mixed	GRID corpus (mixed-speech)
and Pattern Recognition (CVPR), June 2014	GRID corpus (mixed	GRID corpus (mixed-speech)
.  [7] Davis E. King, “Dlib-ml	GRID corpus (mixed	GRID corpus (mixed-speech)
: A machine learning toolkit,” 	GRID corpus (mixed	GRID corpus (mixed-speech)
Journal of Machine Learning Research, 	GRID corpus (mixed	GRID corpus (mixed-speech)
vol. 10, pp. 1755–1758, 2009.  [8] Yuxuan Wang, Arun Narayanan	GRID corpus (mixed	GRID corpus (mixed-speech)
, and DeLiang Wang, “On 	GRID corpus (mixed	GRID corpus (mixed-speech)
Training Targets for Supervised Speech 	GRID corpus (mixed	GRID corpus (mixed-speech)
Separation,” IEEE/ACM Transactions on Audio, 	GRID corpus (mixed	GRID corpus (mixed-speech)
Speech, and Language Processing, vol. 22, no. 12, pp. 1849–1858, Dec	GRID corpus (mixed	GRID corpus (mixed-speech)
. 2014.  [9] Martin Cooke, Jon Barker	GRID corpus (mixed	GRID corpus (mixed-speech)
, Stuart Cunningham, and Xu 	GRID corpus (mixed	GRID corpus (mixed-speech)
Shao, “An audio-visual corpus for 	GRID corpus (mixed	GRID corpus (mixed-speech)
speech perception and automatic speech 	GRID corpus (mixed	GRID corpus (mixed-speech)
recognition,” The Journal of the 	GRID corpus (mixed	GRID corpus (mixed-speech)
Acoustical Society of America, vol. 120, no. 5, pp. 2421–2424, Nov	GRID corpus (mixed	GRID corpus (mixed-speech)
. 2006.  [10] Naomi Harte and Eoin	GRID corpus (mixed	GRID corpus (mixed-speech)
 Gillen, “TCD-TIMIT: An Audio-Visual Cor	GRID corpus (mixed	GRID corpus (mixed-speech)
- pus of Continuous Speech,” 	GRID corpus (mixed	GRID corpus (mixed-speech)
IEEE Transactions on Multimedia, vol. 17, no. 5, pp. 603–615, May	GRID corpus (mixed	GRID corpus (mixed-speech)
 2015	GRID corpus (mixed	GRID corpus (mixed-speech)
.  [11] Z. Chen, Y. Luo	GRID corpus (mixed	GRID corpus (mixed-speech)
, and N. Mesgarani, “Deep 	GRID corpus (mixed	GRID corpus (mixed-speech)
attractor network for single-microphone speaker 	GRID corpus (mixed	GRID corpus (mixed-speech)
separation,” in 2017 IEEE International 	GRID corpus (mixed	GRID corpus (mixed-speech)
Conference on Acoustics, Speech and 	GRID corpus (mixed	GRID corpus (mixed-speech)
Signal Processing (ICASSP), March 2017, 	GRID corpus (mixed	GRID corpus (mixed-speech)
pp. 246–250.  [12] Yusuf Isik, Jonathan Le	GRID corpus (mixed	GRID corpus (mixed-speech)
 Roux, Zhuo Chen, Shinji Watanabe	GRID corpus (mixed	GRID corpus (mixed-speech)
, and John R. 	GRID corpus (mixed	GRID corpus (mixed-speech)
Hershey, “Single-channel multi-speaker separation using 	GRID corpus (mixed	GRID corpus (mixed-speech)
deep clustering,” in Interspeech, 2016.  [13] Morten Kolbaek, Dong Yu	GRID corpus (mixed	GRID corpus (mixed-speech)
, Zheng-Hua Tan, Jesper Jensen, 	GRID corpus (mixed	GRID corpus (mixed-speech)
Morten Kolbaek, Dong Yu, Zheng-Hua 	GRID corpus (mixed	GRID corpus (mixed-speech)
Tan, and Jesper Jensen, “Multitalker 	GRID corpus (mixed	GRID corpus (mixed-speech)
speech separation with utterance-level permutation 	GRID corpus (mixed	GRID corpus (mixed-speech)
invariant training of deep recurrent 	GRID corpus (mixed	GRID corpus (mixed-speech)
neural networks,” IEEE/ACM Trans. Audio, 	GRID corpus (mixed	GRID corpus (mixed-speech)
Speech and Lang. Proc., vol. 25, no. 10, pp. 1901–1913, Oct	GRID corpus (mixed	GRID corpus (mixed-speech)
. 2017.  [14] Bertrand Rivet, Wenwu Wang	GRID corpus (mixed	GRID corpus (mixed-speech)
, Syed Mohsen Naqvi, and 	GRID corpus (mixed	GRID corpus (mixed-speech)
Jonathon Chambers, “Audiovisual Speech Source 	GRID corpus (mixed	GRID corpus (mixed-speech)
Separation: An overview of key 	GRID corpus (mixed	GRID corpus (mixed-speech)
methodologies,” IEEE Signal Processing Magazine, 	GRID corpus (mixed	GRID corpus (mixed-speech)
vol. 31, no. 3, pp. 125	GRID corpus (mixed	GRID corpus (mixed-speech)
–134, May 2014.  [15] Aviv Gabbay, Ariel Ephrat	GRID corpus (mixed	GRID corpus (mixed-speech)
, Tavi Halperin, and Shmuel 	GRID corpus (mixed	GRID corpus (mixed-speech)
Peleg, “Seeing through noise: Visually 	GRID corpus (mixed	GRID corpus (mixed-speech)
driven speaker separation and enhancement,” 	GRID corpus (mixed	GRID corpus (mixed-speech)
in ICASSP. 2018, pp. 3051–3055, 	GRID corpus (mixed	GRID corpus (mixed-speech)
IEEE.  [16] Ariel Ephrat, Tavi Halperin	GRID corpus (mixed	GRID corpus (mixed-speech)
, and Shmuel Peleg, “Improved 	GRID corpus (mixed	GRID corpus (mixed-speech)
speech reconstruction from silent video,” 	GRID corpus (mixed	GRID corpus (mixed-speech)
ICCV 2017 Workshop on Computer 	GRID corpus (mixed	GRID corpus (mixed-speech)
Vision for Audio-Visual Media, 2017.  [17] Aviv Gabbay, Asaph Shamir	GRID corpus (mixed	GRID corpus (mixed-speech)
, and Shmuel Peleg, “Visual 	GRID corpus (mixed	GRID corpus (mixed-speech)
speech en- hancement,” in Interspeech. 2018, pp. 1170–1174, ISCA	GRID corpus (mixed	GRID corpus (mixed-speech)
.  [18] Jen-Cheng Hou, Syu-Siang Wang	GRID corpus (mixed	GRID corpus (mixed-speech)
, Ying-Hui Lai, Yu Tsao, 	GRID corpus (mixed	GRID corpus (mixed-speech)
Hsiu-Wen Chang, and Hsin-Min 	GRID corpus (mixed	GRID corpus (mixed-speech)
Wang, “Audio-Visual Speech Enhancement Us- 	GRID corpus (mixed	GRID corpus (mixed-speech)
ing Multimodal Deep Convolutional Neural 	GRID corpus (mixed	GRID corpus (mixed-speech)
Networks,” IEEE Trans- actions on 	GRID corpus (mixed	GRID corpus (mixed-speech)
Emerging Topics in Computational Intelligence, 	GRID corpus (mixed	GRID corpus (mixed-speech)
vol. 2, no. 2, pp. 117	GRID corpus (mixed	GRID corpus (mixed-speech)
–128, Apr. 2018.  [19] Jen-Cheng Hou, Syu-Siang Wang	GRID corpus (mixed	GRID corpus (mixed-speech)
, Ying-Hui Lai, Jen-Chun Lin, 	GRID corpus (mixed	GRID corpus (mixed-speech)
Yu Tsao, Hsiu-Wen Chang, and 	GRID corpus (mixed	GRID corpus (mixed-speech)
Hsin-Min Wang, “Audio-visual speech enhancement 	GRID corpus (mixed	GRID corpus (mixed-speech)
using deep neural networks,” in 2016 Asia- Pacific Signal and Information	GRID corpus (mixed	GRID corpus (mixed-speech)
 Processing Association Annual Sum- mit	GRID corpus (mixed	GRID corpus (mixed-speech)
 and Conference (APSIPA), Jeju, South	GRID corpus (mixed	GRID corpus (mixed-speech)
 Korea, Dec. 2016, pp. 1–6	GRID corpus (mixed	GRID corpus (mixed-speech)
, IEEE.  [20] Ariel Ephrat, Inbar Mosseri	GRID corpus (mixed	GRID corpus (mixed-speech)
, Oran Lang, Tali Dekel, 	GRID corpus (mixed	GRID corpus (mixed-speech)
Kevin Wilson, Avinatan Hassidim, William 	GRID corpus (mixed	GRID corpus (mixed-speech)
T. Freeman, and Michael 	GRID corpus (mixed	GRID corpus (mixed-speech)
Rubinstein, “Looking to Listen at 	GRID corpus (mixed	GRID corpus (mixed-speech)
the Cocktail Party: A Speaker-Independent 	GRID corpus (mixed	GRID corpus (mixed-speech)
Audio-Visual Model for Speech Separation,” 	GRID corpus (mixed	GRID corpus (mixed-speech)
ACM Transactions on Graphics, vol. 37, no. 4, pp. 1–11, July	GRID corpus (mixed	GRID corpus (mixed-speech)
 2018, arXiv: 1804.03619	GRID corpus (mixed	GRID corpus (mixed-speech)
.  [21] T. Afouras, J. S	GRID corpus (mixed	GRID corpus (mixed-speech)
. Chung, and A. 	GRID corpus (mixed	GRID corpus (mixed-speech)
Zisserman, “The conversation: Deep audio-visual 	GRID corpus (mixed	GRID corpus (mixed-speech)
speech enhancement,” in Interspeech, 2018.  [22] Andrew Owens and Alexei	GRID corpus (mixed	GRID corpus (mixed-speech)
 A Efros, “Audio-visual scene analysis	GRID corpus (mixed	GRID corpus (mixed-speech)
 with self-supervised multisensory features,” European	GRID corpus (mixed	GRID corpus (mixed-speech)
 Conference on Computer Vision (ECCV	GRID corpus (mixed	GRID corpus (mixed-speech)
), 2018.  [23] Michael C. Anzalone, Lauren	GRID corpus (mixed	GRID corpus (mixed-speech)
 Calandruccio, Karen A. Doherty, and	GRID corpus (mixed	GRID corpus (mixed-speech)
 Laurel H. Carney, “Determination of	GRID corpus (mixed	GRID corpus (mixed-speech)
 the potential benefit of time	GRID corpus (mixed	GRID corpus (mixed-speech)
- frequency gain manipulation,” Ear 	GRID corpus (mixed	GRID corpus (mixed-speech)
Hear, vol. 27, no. 5, 	GRID corpus (mixed	GRID corpus (mixed-speech)
pp. 480–492, Oct 2006, 16957499[pmid].  [24] Ulrik Kjems, Jesper B	GRID corpus (mixed	GRID corpus (mixed-speech)
. Boldt, Michael S. Pedersen, 	GRID corpus (mixed	GRID corpus (mixed-speech)
Thomas Lunner, and DeLiang 	GRID corpus (mixed	GRID corpus (mixed-speech)
Wang, “Role of mask pattern 	GRID corpus (mixed	GRID corpus (mixed-speech)
in intelligibility of ideal binary-masked 	GRID corpus (mixed	GRID corpus (mixed-speech)
noisy speech,” The Journal of 	GRID corpus (mixed	GRID corpus (mixed-speech)
the Acoustical Society of America, 	GRID corpus (mixed	GRID corpus (mixed-speech)
vol. 126, no. 3, pp. 1415	GRID corpus (mixed	GRID corpus (mixed-speech)
–1426, 2009.  [25] A. Graves, A. Mohamed	GRID corpus (mixed	GRID corpus (mixed-speech)
, and G. Hinton, “Speech 	GRID corpus (mixed	GRID corpus (mixed-speech)
recognition with deep recurrent neural 	GRID corpus (mixed	GRID corpus (mixed-speech)
networks,” in 2013 IEEE International 	GRID corpus (mixed	GRID corpus (mixed-speech)
Con- ference on Acoustics, Speech 	GRID corpus (mixed	GRID corpus (mixed-speech)
and Signal Processing, May 2013, 	GRID corpus (mixed	GRID corpus (mixed-speech)
pp. 6645–6649.  [26] Diederik P Kingma and	GRID corpus (mixed	GRID corpus (mixed-speech)
 Jimmy Ba, “Adam: A method	GRID corpus (mixed	GRID corpus (mixed-speech)
 for stochastic optimization,” arXiv preprint	GRID corpus (mixed	GRID corpus (mixed-speech)
 arXiv:1412.6980, 2014	GRID corpus (mixed	GRID corpus (mixed-speech)
.  [27] E. Vincent, R. Gribonval	GRID corpus (mixed	GRID corpus (mixed-speech)
, and C. Fevotte, “Performance 	GRID corpus (mixed	GRID corpus (mixed-speech)
measure- ment in blind audio 	GRID corpus (mixed	GRID corpus (mixed-speech)
source separation,” IEEE Transactions on 	GRID corpus (mixed	GRID corpus (mixed-speech)
Audio, Speech and Language Processing, 	GRID corpus (mixed	GRID corpus (mixed-speech)
vol. 14, no. 4, pp. 1462	GRID corpus (mixed	GRID corpus (mixed-speech)
–1469, July 2006.  [28] Colin Raffel, Brian McFee	GRID corpus (mixed	GRID corpus (mixed-speech)
, Eric J Humphrey, Justin 	GRID corpus (mixed	GRID corpus (mixed-speech)
Salamon, Oriol Nieto, Dawen Liang, 	GRID corpus (mixed	GRID corpus (mixed-speech)
Daniel PW Ellis, and C 	GRID corpus (mixed	GRID corpus (mixed-speech)
Colin Raffel, “mir eval: A 	GRID corpus (mixed	GRID corpus (mixed-speech)
transparent implementation of common mir 	GRID corpus (mixed	GRID corpus (mixed-speech)
metrics,” in In Proceed- ings 	GRID corpus (mixed	GRID corpus (mixed-speech)
of the 15th International Society 	GRID corpus (mixed	GRID corpus (mixed-speech)
for Music Information Retrieval Conference, 	GRID corpus (mixed	GRID corpus (mixed-speech)
ISMIR. Citeseer, 2014.  [29] A.W. Rix, J.G. Beerends	GRID corpus (mixed	GRID corpus (mixed-speech)
, M.P. Hollier, and A.P. 	GRID corpus (mixed	GRID corpus (mixed-speech)
Hekstra, “Perceptual evaluation of speech 	GRID corpus (mixed	GRID corpus (mixed-speech)
quality (PESQ)-a new method for 	GRID corpus (mixed	GRID corpus (mixed-speech)
speech qual- ity assessment of 	GRID corpus (mixed	GRID corpus (mixed-speech)
telephone networks and codecs,” in 2001 IEEE In- ternational Conference on	GRID corpus (mixed	GRID corpus (mixed-speech)
 Acoustics, Speech, and Signal Processing	GRID corpus (mixed	GRID corpus (mixed-speech)
. Proceedings (Cat. No.01CH37221), Salt 	GRID corpus (mixed	GRID corpus (mixed-speech)
Lake City, UT, USA, 2001, 	GRID corpus (mixed	GRID corpus (mixed-speech)
vol. 2, pp. 749–752, IEEE.  [30] A. Hines, J. Skoglund	GRID corpus (mixed	GRID corpus (mixed-speech)
, A. Kokaram, and N. 	GRID corpus (mixed	GRID corpus (mixed-speech)
Harte, “ViSQOL: The Virtual Speech 	GRID corpus (mixed	GRID corpus (mixed-speech)
Quality Objective Listener,” in IWAENC 2012	GRID corpus (mixed	GRID corpus (mixed-speech)
; Inter- national Workshop on 	GRID corpus (mixed	GRID corpus (mixed-speech)
Acoustic Signal Enhancement, Sept. 2012, 	GRID corpus (mixed	GRID corpus (mixed-speech)
pp. 1	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
4	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
1  Introduction	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
1.1  Related work	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
2  MODEL ARCHITECTURES	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
2.1  VL2M model	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
2.2  VL2M_ref model	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
2.3  Audio-Visual concat model	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
2.4  Audio-Visual concat-ref model	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
3  Experimental setup	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
3.1  Dataset	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
3.2  LSTM training	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
3.3  Audio pre- and post-processing	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
3.4  Video pre-processing	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
4  Results	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
5  Conclusion	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
6  References	GRID corpus (mixed	GRID corpus (mixed-speech)
	GRID corpus (mixed	GRID corpus (mixed-speech)
evaluated on the limited size GRID and TCD-TIMIT datasets, that achieve	GRID	GRID corpus (mixed-speech)
trained and evaluated on the GRID [9] and TCD-TIMIT [10] datasets	GRID	GRID corpus (mixed-speech)
were carried out using the GRID [9] and TCD-TIMIT [10] audio-visual	GRID	GRID corpus (mixed-speech)
Regarding the GRID corpus, for each of the	GRID	GRID corpus (mixed-speech)
the same procedure as for GRID, with one difference. Con- trary	GRID	GRID corpus (mixed-speech)
to GRID, TCD-TIMIT utterances have different dura	GRID	GRID corpus (mixed-speech)
Table 1. GRID results - speaker-dependent. The “Noisy	GRID	GRID corpus (mixed-speech)
Table 2. GRID results - speaker-independent	GRID	GRID corpus (mixed-speech)
We upsampled from 25/29.97 fps (GRID	GRID	GRID corpus (mixed-speech)
systems trained and tested on GRID and TCD- TIMIT to compare	GRID	GRID corpus (mixed-speech)
of speaker-dependent models on the GRID corpus with landmark motion vectors	GRID	GRID corpus (mixed-speech)
speaker-independent test-set results on the GRID and TCD-TIMIT datasets respectively. V2ML	GRID	GRID corpus (mixed-speech)
evaluated on the limited size GRID and TCD-TIMIT datasets that accomplish	GRID	GRID corpus (mixed-speech)
We conduct experiments on the WIKIBIO dataset which contains over 700k	WIKIBIO	WikiBio
FBI Agent), etc. We utilize WIKIBIO dataset proposed by Le- bret	WIKIBIO	WikiBio
structured table. (3) Experiments on WIKIBIO dataset show that our model	WIKIBIO	WikiBio
Table 1: Statistics of WIKIBIO dataset	WIKIBIO	WikiBio
and testing set based on WIKIBIO by randomly shuf- fling the	WIKIBIO	WikiBio
We conduct experiments on the WIKIBIO dataset which contains over 700k	WIKIBIO	WikiBio
FBI Agent), etc. We utilize WIKIBIO dataset proposed by Le- bret	WIKIBIO	WikiBio
structured table. (3) Experiments on WIKIBIO dataset show that our model	WIKIBIO	WikiBio
Table 1: Statistics of WIKIBIO dataset	WIKIBIO	WikiBio
and testing set based on WIKIBIO by randomly shuf- fling the	WIKIBIO	WikiBio
new dataset for text generation, WIKIBIO, a corpus of 728,321 articles	WIKIBIO	WikiBio
GNE: a deep learning framework 		BioGRID(yeast)
for gene network inference by 		BioGRID(yeast)
aggregating biological 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
KC et al. BMC Systems 		BioGRID(yeast)
Biology 2019, 13(Suppl 2):38 https://doi.org/10.1186/s12918-019-0694-y  RESEARCH Open Access		BioGRID(yeast)
		BioGRID(yeast)
GNE: a deep learning framework 		BioGRID(yeast)
for gene network inference by 		BioGRID(yeast)
aggregating biological information Kishan KC1*, 		BioGRID(yeast)
Rui Li1, Feng Cui2, Qi 		BioGRID(yeast)
Yu1 and Anne R. Haake1  From The 17th Asia Pacific		BioGRID(yeast)
 Bioinformatics Conference (APBC 2019) Wuhan		BioGRID(yeast)
, China. 14–16 January 2019  Abstract		BioGRID(yeast)
		BioGRID(yeast)
Background: The topological landscape of 		BioGRID(yeast)
gene interaction networks provides a 		BioGRID(yeast)
rich source of information for 		BioGRID(yeast)
inferring functional patterns of genes 		BioGRID(yeast)
or proteins. However, it is 		BioGRID(yeast)
still a challenging task to 		BioGRID(yeast)
aggregate heterogeneous biological information such 		BioGRID(yeast)
as gene expression and gene 		BioGRID(yeast)
interactions to achieve more accurate 		BioGRID(yeast)
inference for prediction and discovery 		BioGRID(yeast)
of new gene interactions. In 		BioGRID(yeast)
particular, how to generate a 		BioGRID(yeast)
unified vector representation to integrate 		BioGRID(yeast)
diverse input data is a 		BioGRID(yeast)
key challenge addressed here.  Results: We propose a scalable		BioGRID(yeast)
 and robust deep learning framework		BioGRID(yeast)
 to learn embedded representations to		BioGRID(yeast)
 unify known gene interactions and		BioGRID(yeast)
 gene expression for gene interaction		BioGRID(yeast)
 predictions. These low- dimensional embeddings		BioGRID(yeast)
 derive deeper insights into the		BioGRID(yeast)
 structure of rapidly accumulating and		BioGRID(yeast)
 diverse gene interaction networks and		BioGRID(yeast)
 greatly simplify downstream modeling. We		BioGRID(yeast)
 compare the predictive power of		BioGRID(yeast)
 our deep embeddings to the		BioGRID(yeast)
 strong baselines. The results suggest		BioGRID(yeast)
 that our deep embeddings achieve		BioGRID(yeast)
 significantly more accurate predictions. Moreover		BioGRID(yeast)
, a set of novel 		BioGRID(yeast)
gene interaction predictions are validated 		BioGRID(yeast)
by up-to-date literature-based database entries.  Conclusion: The proposed model demonstrates		BioGRID(yeast)
 the importance of integrating heterogeneous		BioGRID(yeast)
 information about genes for gene		BioGRID(yeast)
 network inference. GNE is freely		BioGRID(yeast)
 available under the GNU General		BioGRID(yeast)
 Public License and can be		BioGRID(yeast)
 downloaded from GitHub (https://github.com/kckishan/GNE		BioGRID(yeast)
).  Keywords: Gene interaction networks, Gene		BioGRID(yeast)
 expression, Network embedding, Heterogeneous data		BioGRID(yeast)
 integration, Deep learning		BioGRID(yeast)
		BioGRID(yeast)
Background A comprehensive study of 		BioGRID(yeast)
gene interactions (GIs) provides means 		BioGRID(yeast)
to identify the functional relationship 		BioGRID(yeast)
between genes and their corresponding 		BioGRID(yeast)
products, as well as insights 		BioGRID(yeast)
into underlying biological phenomena that 		BioGRID(yeast)
are critical to understanding phenotypes 		BioGRID(yeast)
in health and dis- ease 		BioGRID(yeast)
conditions [1–3]. Since advancements in 		BioGRID(yeast)
measure- ment technologies have led 		BioGRID(yeast)
to numerous high-throughput datasets, there 		BioGRID(yeast)
is a great value in 		BioGRID(yeast)
developing efficient com- putational methods 		BioGRID(yeast)
capable of automatically extracting  *Correspondence: kk3671@rit.edu 1Golisano College of		BioGRID(yeast)
 Computing and Information Sciences, Rochester		BioGRID(yeast)
 Institute of Technology, 20 Lomb		BioGRID(yeast)
 Memorial Drive, 14623 Rochester, New		BioGRID(yeast)
 York, USA Full list of		BioGRID(yeast)
 author information is available at		BioGRID(yeast)
 the end of the article		BioGRID(yeast)
		BioGRID(yeast)
and aggregating meaningful information from 		BioGRID(yeast)
heteroge- neous datasets to infer 		BioGRID(yeast)
gene interactions.  Although a wide variety of		BioGRID(yeast)
 machine learning models have been		BioGRID(yeast)
 developed to analyze high-throughput datasets		BioGRID(yeast)
 for GI prediction [4], there		BioGRID(yeast)
 are still some major chal		BioGRID(yeast)
- lenges, such as efficient 		BioGRID(yeast)
analysis of large heterogeneous datasets, 		BioGRID(yeast)
integration of biological information, and 		BioGRID(yeast)
effec- tive feature engineering. To 		BioGRID(yeast)
address these challenges, we propose 		BioGRID(yeast)
a novel deep learning framework 		BioGRID(yeast)
to integrate diverse biological information 		BioGRID(yeast)
for GI network inference.  Our proposed method frames GI		BioGRID(yeast)
 network inference as a problem		BioGRID(yeast)
 of network embedding. In particular		BioGRID(yeast)
, we rep- resent gene 		BioGRID(yeast)
interactions as a network of 		BioGRID(yeast)
genes and their interactions and 		BioGRID(yeast)
create a deep learning framework 		BioGRID(yeast)
to automatically learn an informative 		BioGRID(yeast)
representation which  © The Author(s). 2019 Open		BioGRID(yeast)
 Access This article is distributed		BioGRID(yeast)
 under the terms of the		BioGRID(yeast)
 Creative Commons Attribution 4.0 International		BioGRID(yeast)
 License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted		BioGRID(yeast)
 use, distribution, and reproduction in		BioGRID(yeast)
 any medium, provided you give		BioGRID(yeast)
 appropriate credit to the original		BioGRID(yeast)
 author(s) and the source, provide		BioGRID(yeast)
 a link to the Creative		BioGRID(yeast)
 Commons license, and indicate if		BioGRID(yeast)
 changes were made. The Creative		BioGRID(yeast)
 Commons Public Domain Dedication waiver		BioGRID(yeast)
 (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data		BioGRID(yeast)
 made available in this article		BioGRID(yeast)
, unless otherwise stated.  http://crossmark.crossref.org/dialog/?doi=10.1186/s12918-019-0694-y&domain=pdf https://github.com/kckishan/GNE mailto: kk3671@rit.edu http://creativecommons.org/licenses/by/4.0		BioGRID(yeast)
/ 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
1		BioGRID(yeast)
		BioGRID(yeast)
0		BioGRID(yeast)
		BioGRID(yeast)
KC et al. BMC Systems 		BioGRID(yeast)
Biology 2019, 13(Suppl 2):38 Page 2 of 14		BioGRID(yeast)
		BioGRID(yeast)
integrates both the topological property 		BioGRID(yeast)
and the gene expression property. 		BioGRID(yeast)
A key insight behind our 		BioGRID(yeast)
gene net- work embedding method 		BioGRID(yeast)
is the “guilt by association” 		BioGRID(yeast)
assumption [5], that is, genes 		BioGRID(yeast)
that are co-localized or have 		BioGRID(yeast)
similar topological roles in the 		BioGRID(yeast)
interaction net- work are likely 		BioGRID(yeast)
to be functionally correlated. This 		BioGRID(yeast)
insight not only allows us 		BioGRID(yeast)
to discover similar genes and 		BioGRID(yeast)
pro- teins but also to 		BioGRID(yeast)
infer the properties of unknown 		BioGRID(yeast)
ones. Our network embedding generates 		BioGRID(yeast)
a lower-dimensional vector representation of 		BioGRID(yeast)
the gene topological character- istics. 		BioGRID(yeast)
The relationships between genes including 		BioGRID(yeast)
higher- order topological properties are 		BioGRID(yeast)
captured by the distances between 		BioGRID(yeast)
genes in the embedding space. 		BioGRID(yeast)
The new low- dimensional representation 		BioGRID(yeast)
of a GI network can 		BioGRID(yeast)
be used for various downstream 		BioGRID(yeast)
tasks, such as gene function 		BioGRID(yeast)
pre- diction, gene interaction prediction, 		BioGRID(yeast)
and gene ontology reconstruction [6].  Furthermore, since the network embedding		BioGRID(yeast)
 method can only preserve the		BioGRID(yeast)
 topological properties of a GI		BioGRID(yeast)
 network, and fails to generalize		BioGRID(yeast)
 for genes with no interaction		BioGRID(yeast)
 infor- mation, our scalable deep		BioGRID(yeast)
 learning method also integrates heterogeneous		BioGRID(yeast)
 gene information, such as expression		BioGRID(yeast)
 data from high throughput technologies		BioGRID(yeast)
, into the GI net- 		BioGRID(yeast)
work inference. Our method projects 		BioGRID(yeast)
genes with similar attributes closer 		BioGRID(yeast)
to each other in the 		BioGRID(yeast)
embedding space, even if they 		BioGRID(yeast)
may not have similar topological 		BioGRID(yeast)
properties. The results show that 		BioGRID(yeast)
by integrating additional gene infor- 		BioGRID(yeast)
mation in the network embedding 		BioGRID(yeast)
process, the prediction performance is 		BioGRID(yeast)
improved significantly.  GI prediction is a long-standing		BioGRID(yeast)
 problem. The pro- posed machine		BioGRID(yeast)
 learning methods include statistical corre		BioGRID(yeast)
- lation, mutual information [7], 		BioGRID(yeast)
dimensionality reduction [8], and network-based 		BioGRID(yeast)
methods (e.g. common neighbor- hood, 		BioGRID(yeast)
network embedding) [4, 9]. Among 		BioGRID(yeast)
these methods, some methods such 		BioGRID(yeast)
as statistical correlation and mutual 		BioGRID(yeast)
information consider only gene expression 		BioGRID(yeast)
whereas other methods use only 		BioGRID(yeast)
topological properties to predict GIs.  Network-based methods have been proposed		BioGRID(yeast)
 to leverage topological properties of		BioGRID(yeast)
 GI networks [10]. Neighborhood-based methods		BioGRID(yeast)
 quantify the proximity between genes		BioGRID(yeast)
, based on common neighbors 		BioGRID(yeast)
in GI network [11]. The 		BioGRID(yeast)
proximity scores assigned to a 		BioGRID(yeast)
pair of genes rely on 		BioGRID(yeast)
the number of neighbors that 		BioGRID(yeast)
the pair has in common. 		BioGRID(yeast)
Adjacency matrix, representing the interaction 		BioGRID(yeast)
network, or proximity matrix, obtained 		BioGRID(yeast)
from neighborhood-based methods, are processed 		BioGRID(yeast)
with network embedding methods to 		BioGRID(yeast)
learn embeddings that preserve the 		BioGRID(yeast)
structural properties of the network. 		BioGRID(yeast)
Structure-preserving network embedding methods such 		BioGRID(yeast)
as Isomap [12] are proposed 		BioGRID(yeast)
as a dimensionality reduc- tion 		BioGRID(yeast)
technique. Since the goal of 		BioGRID(yeast)
these methods is solely for 		BioGRID(yeast)
graph reconstruction, the embedding space 		BioGRID(yeast)
may not be suitable for 		BioGRID(yeast)
GI network inference. Besides, these 		BioGRID(yeast)
meth- ods construct the graphs 		BioGRID(yeast)
from the data features where  proximity between genes is well		BioGRID(yeast)
 defined in the original feature		BioGRID(yeast)
 space [9]. On the other		BioGRID(yeast)
 hand, in GI networks, gene		BioGRID(yeast)
 proximities are not explicitly defined		BioGRID(yeast)
, and they depend on 		BioGRID(yeast)
the specific analytic tasks and 		BioGRID(yeast)
application scenarios.  Our deep learning method allows		BioGRID(yeast)
 incorporating gene expression data with		BioGRID(yeast)
 GI network topological structure information		BioGRID(yeast)
 to preserve both topological and		BioGRID(yeast)
 attribute proximity in the low-dimensional		BioGRID(yeast)
 representation for GI predictions. Moreover		BioGRID(yeast)
, the scalable architecture enables 		BioGRID(yeast)
us to incorporate additional attributes. 		BioGRID(yeast)
Topological prop- erties of GI 		BioGRID(yeast)
network and expression profiles are 		BioGRID(yeast)
trans- formed into two separate 		BioGRID(yeast)
embeddings: ID embedding (which preserves 		BioGRID(yeast)
the topological structure proximity) and 		BioGRID(yeast)
attribute embedding (which preserves the 		BioGRID(yeast)
attribute prox- imity) respectively. With 		BioGRID(yeast)
a multilayer neural network, we 		BioGRID(yeast)
then aggregate the complex statistical 		BioGRID(yeast)
relationships between topology and attribute 		BioGRID(yeast)
information to improve GI predictions.  In summary, our contributions are		BioGRID(yeast)
 as follows		BioGRID(yeast)
:  • We propose a novel		BioGRID(yeast)
 deep learning framework to learn		BioGRID(yeast)
 lower dimensional representations while preserving		BioGRID(yeast)
 topological and attribute proximity of		BioGRID(yeast)
 GI networks		BioGRID(yeast)
.  • We evaluate the prediction		BioGRID(yeast)
 performance on the datasets of		BioGRID(yeast)
 two organisms based on the		BioGRID(yeast)
 embedded representation and achieve significantly		BioGRID(yeast)
 better predictions than the strong		BioGRID(yeast)
 baselines		BioGRID(yeast)
.  • Our method can predict		BioGRID(yeast)
 new gene interactions which are		BioGRID(yeast)
 validated on an up-to-date GI		BioGRID(yeast)
 database		BioGRID(yeast)
.  Methods Preliminaries We formally define		BioGRID(yeast)
 the problem of gene network		BioGRID(yeast)
 infer- ence as a network		BioGRID(yeast)
 embedding problem using the concepts		BioGRID(yeast)
 of topological and attribute proximity		BioGRID(yeast)
 as demonstrated in Fig. 1		BioGRID(yeast)
.  Definition 1 (Gene network) Gene		BioGRID(yeast)
 network can be rep- resented		BioGRID(yeast)
 as a network structure, which		BioGRID(yeast)
 represents the inter- actions between		BioGRID(yeast)
 genes within an organism. The		BioGRID(yeast)
 interaction between genes corresponds to		BioGRID(yeast)
 either a physical interaction through		BioGRID(yeast)
 their gene products, e.g., proteins		BioGRID(yeast)
, or one of the 		BioGRID(yeast)
genes alters or affects the 		BioGRID(yeast)
activity of other gene of 		BioGRID(yeast)
interest. We denote gene network 		BioGRID(yeast)
as G = (V , 		BioGRID(yeast)
E, A) where 		BioGRID(yeast)
V = {vi} denotes genes 		BioGRID(yeast)
or proteins, E = {eij} 		BioGRID(yeast)
denotes edges that correspond to 		BioGRID(yeast)
interactions between genes vi and 		BioGRID(yeast)
vj, and A = {Ai} 		BioGRID(yeast)
represents the attributes of gene 		BioGRID(yeast)
vi. Edge eij is associated 		BioGRID(yeast)
with a weight wij ≥ 0 indicating the strength of the		BioGRID(yeast)
 connection between gene vi and		BioGRID(yeast)
 vj. If gene vi and		BioGRID(yeast)
 vj is not linked by		BioGRID(yeast)
 an edge, wij = 0		BioGRID(yeast)
. We name interactions with 		BioGRID(yeast)
wij > 0 as positive 		BioGRID(yeast)
interactions and wij = 0 		BioGRID(yeast)
as negative interactions. In this 		BioGRID(yeast)
paper, we consider weights wij 		BioGRID(yeast)
to be binary, indicating whether 		BioGRID(yeast)
genes vi and vj interact 		BioGRID(yeast)
or 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
KC et al. BMC Systems 		BioGRID(yeast)
Biology 2019, 13(Suppl 2):38 Page 3 of 14		BioGRID(yeast)
		BioGRID(yeast)
Fig. 1 An illustration of 		BioGRID(yeast)
gene network embedding (GNE). GNE 		BioGRID(yeast)
integrates gene interaction network and 		BioGRID(yeast)
gene expression data to learn 		BioGRID(yeast)
a lower-dimensional representation.The nodes represent 		BioGRID(yeast)
genes, and the genes with 		BioGRID(yeast)
the same color have similar 		BioGRID(yeast)
expression profiles. GNE groups genes 		BioGRID(yeast)
with similar network topology, which 		BioGRID(yeast)
are connected or have a 		BioGRID(yeast)
similar neighborhood in the graph, 		BioGRID(yeast)
and attribute similarity (similar expression 		BioGRID(yeast)
profiles) in the embedded space  Genes directly connected with a		BioGRID(yeast)
 gene vi in gene network		BioGRID(yeast)
 denote the local network structure		BioGRID(yeast)
 of gene vi. We define		BioGRID(yeast)
 local network structures as the		BioGRID(yeast)
 first-order proximity of a gene		BioGRID(yeast)
.  Definition 2 (First-order proximity) The		BioGRID(yeast)
 first-order proximity in a gene		BioGRID(yeast)
 network is the pairwise interactions		BioGRID(yeast)
 between genes. Weight wij indicates		BioGRID(yeast)
 the first-order proxim- ity between		BioGRID(yeast)
 gene vi and vj. If		BioGRID(yeast)
 there is no interaction between		BioGRID(yeast)
 gene vi and vj, their		BioGRID(yeast)
 first-order proximity wij is 0		BioGRID(yeast)
.  Genes are likely to be		BioGRID(yeast)
 involved in the same cellular		BioGRID(yeast)
 func- tions if they are		BioGRID(yeast)
 connected in the gene network		BioGRID(yeast)
. On the other hand, 		BioGRID(yeast)
even if two genes are 		BioGRID(yeast)
not connected, they may be 		BioGRID(yeast)
still related in some cellular 		BioGRID(yeast)
functions. This indicates the need 		BioGRID(yeast)
for an additional notion of 		BioGRID(yeast)
proximity to preserve the network 		BioGRID(yeast)
structure. Studies suggest that genes 		BioGRID(yeast)
that share a similar neighborhood 		BioGRID(yeast)
are also likely to be 		BioGRID(yeast)
related [6]. Thus, we introduce 		BioGRID(yeast)
second-order proximity that characterizes the 		BioGRID(yeast)
global network structure of the 		BioGRID(yeast)
genes.  Definition 3 (Second-order proximity) Second		BioGRID(yeast)
 order proximity denotes the similarity		BioGRID(yeast)
 between the neighbor- hood of		BioGRID(yeast)
 genes. Let Ni = {si,1		BioGRID(yeast)
,. . . , si,i−1, 		BioGRID(yeast)
si,i+1,. . . , si,M−1} 		BioGRID(yeast)
denotes the first-order proximity of 		BioGRID(yeast)
gene vi, where si,j is 		BioGRID(yeast)
wij if there is direct 		BioGRID(yeast)
connection between gene vi and 		BioGRID(yeast)
gene vj, otherwise 0. Then, 		BioGRID(yeast)
the second order proximity is 		BioGRID(yeast)
the sim- ilarity between Ni 		BioGRID(yeast)
and Nj. If there is 		BioGRID(yeast)
no path to reach gene 		BioGRID(yeast)
vi from gene vj, the 		BioGRID(yeast)
second proximity between these genes 		BioGRID(yeast)
is 0.  Integrating first-order and second-order proximities		BioGRID(yeast)
 simultaneously can help to preserve		BioGRID(yeast)
 the topological proper- ties of		BioGRID(yeast)
 the gene network. To generate		BioGRID(yeast)
 a more comprehensive representation of		BioGRID(yeast)
 the genes, it is crucial		BioGRID(yeast)
 to integrate gene expression data		BioGRID(yeast)
 as gene attributes with their		BioGRID(yeast)
 topological properties. Besides preserving topological		BioGRID(yeast)
 properties, gene expression provides additional		BioGRID(yeast)
 information to predict the network		BioGRID(yeast)
 structure		BioGRID(yeast)
.  Definition 4 (Attribute proximity) Attribute		BioGRID(yeast)
 proxim- ity denotes the similarity		BioGRID(yeast)
 between the expression of genes		BioGRID(yeast)
.  We thus investigate both topological		BioGRID(yeast)
 and attribute prox- imity for		BioGRID(yeast)
 gene network embedding, which is		BioGRID(yeast)
 defined as follows		BioGRID(yeast)
:  Definition 5 (Gene network embedding		BioGRID(yeast)
) Given a gene network 		BioGRID(yeast)
denoted as G = (V , E, A), gene network embedding		BioGRID(yeast)
 aims to learn a function		BioGRID(yeast)
 f that maps gene network		BioGRID(yeast)
 structure and their attribute information		BioGRID(yeast)
 to a d- dimensional space		BioGRID(yeast)
 where a gene is represented		BioGRID(yeast)
 by a vector yi		BioGRID(yeast)
 ∈ Rd where d		BioGRID(yeast)
 � M. The low dimensional		BioGRID(yeast)
 vectors yi and yj for		BioGRID(yeast)
 genes vi and vj preserve		BioGRID(yeast)
 their relationships in terms of		BioGRID(yeast)
 the network topological structure and		BioGRID(yeast)
 attribute proximity		BioGRID(yeast)
.  Gene network embedding (GNE) model		BioGRID(yeast)
 Our deep learning framework as		BioGRID(yeast)
 shown in Fig. 2 jointly		BioGRID(yeast)
 utilizes gene network structure and		BioGRID(yeast)
 gene expression data to learn		BioGRID(yeast)
 a unified representation for the		BioGRID(yeast)
 genes. Embedding of a gene		BioGRID(yeast)
 network projects genes into a		BioGRID(yeast)
 lower dimensional space, known as		BioGRID(yeast)
 the embedding space, in which		BioGRID(yeast)
 each gene is represented by		BioGRID(yeast)
 a vector. The embeddings preserve		BioGRID(yeast)
 both the gene network structure		BioGRID(yeast)
 and statistical relationships of gene		BioGRID(yeast)
 expression. We list the variables		BioGRID(yeast)
 to specify our framework in		BioGRID(yeast)
 Table 1		BioGRID(yeast)
.  Gene network structure modeling GNE		BioGRID(yeast)
 framework preserves first-order and second-order		BioGRID(yeast)
 proximity of genes in the		BioGRID(yeast)
 gene network. The key idea		BioGRID(yeast)
 of network structure modeling is		BioGRID(yeast)
 to estimate the pairwise proximity		BioGRID(yeast)
 of genes in terms of		BioGRID(yeast)
 the network structure. If two		BioGRID(yeast)
 genes are connected or share		BioGRID(yeast)
 similar neighborhood genes, they tend		BioGRID(yeast)
 to be related and should		BioGRID(yeast)
 be placed closer to each		BioGRID(yeast)
 other in the embedding space		BioGRID(yeast)
. Inspired by the Skip-gram 		BioGRID(yeast)
model [13], we use one 		BioGRID(yeast)
hot encoded represen- tation to 		BioGRID(yeast)
represent topological information of a 		BioGRID(yeast)
gene. 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
KC et al. BMC Systems 		BioGRID(yeast)
Biology 2019, 13(Suppl 2):38 Page 4 of 14		BioGRID(yeast)
		BioGRID(yeast)
Fig. 2 Overview of Gene 		BioGRID(yeast)
Network Embedding (GNE) Framework for 		BioGRID(yeast)
gene interaction prediction. On the 		BioGRID(yeast)
left,one-hot encoded representation of gene 		BioGRID(yeast)
is encoded to dense vector 		BioGRID(yeast)
v(s)i of dimension d × 1 which captures topological properties and		BioGRID(yeast)
 expression vector of gene is		BioGRID(yeast)
 transformed to v(a)i of dimension		BioGRID(yeast)
 d × 1 which aggregates		BioGRID(yeast)
 the attribute information (Step 1		BioGRID(yeast)
). Next, concatenation of two 		BioGRID(yeast)
embedded vectors (creates vector with 		BioGRID(yeast)
dimension 2d × 1) allows 		BioGRID(yeast)
to combine strength of both 		BioGRID(yeast)
network structure and attribute modeling. 		BioGRID(yeast)
Then, nonlinear transformation of concatenated 		BioGRID(yeast)
vector enables GNE to capture 		BioGRID(yeast)
complex statistical relationships between network 		BioGRID(yeast)
structure and attribute information and 		BioGRID(yeast)
learn better representations (Step 2). 		BioGRID(yeast)
Finally, these learned representation of 		BioGRID(yeast)
dimension d × 1 is 		BioGRID(yeast)
transformed into a probability vector 		BioGRID(yeast)
of length M × 1 		BioGRID(yeast)
in output layer, which contains 		BioGRID(yeast)
the predictive probability of gene 		BioGRID(yeast)
vi to all the genes 		BioGRID(yeast)
in the network. Conditional probability 		BioGRID(yeast)
p(vj|vi) on output layer indicates 		BioGRID(yeast)
the likelihood that gene vj 		BioGRID(yeast)
is connected with gene vi (		BioGRID(yeast)
Step 3)  gene vi in the network		BioGRID(yeast)
 is represented as an M-dimensional		BioGRID(yeast)
 vector where only the ith		BioGRID(yeast)
 component of the vector is		BioGRID(yeast)
 1		BioGRID(yeast)
.  To model topological similarity, we		BioGRID(yeast)
 define the condi- tional probability		BioGRID(yeast)
 of gene vj on gene		BioGRID(yeast)
 vi using a softmax function		BioGRID(yeast)
 as		BioGRID(yeast)
:  p(vj|vi) = exp(f (vi, vj))∑M		BioGRID(yeast)
 j′=1 exp(f (vi, vj		BioGRID(yeast)
′))  (1		BioGRID(yeast)
)  Table 1 Terms and Notations		BioGRID(yeast)
		BioGRID(yeast)
Symbol Definitions  M Total number of genes		BioGRID(yeast)
 in gene network		BioGRID(yeast)
		BioGRID(yeast)
E Number of expression values 		BioGRID(yeast)
for each gene  Ni Set of the neighbor		BioGRID(yeast)
 genes of gene vi v(s)i		BioGRID(yeast)
 Topological representation of gene vi		BioGRID(yeast)
 v(a)i Attribute representation of gene		BioGRID(yeast)
 vi ṽi Neighborhood representation of		BioGRID(yeast)
 gene vi vi Concatenated representation		BioGRID(yeast)
 of topological		BioGRID(yeast)
		BioGRID(yeast)
properties and expression data  k Number of hidden layers		BioGRID(yeast)
 to transform concatenated representation into		BioGRID(yeast)
 embedding space		BioGRID(yeast)
		BioGRID(yeast)
h(k) Output of kth hidden 		BioGRID(yeast)
layer  Wk Weight matrix for kth		BioGRID(yeast)
 hidden layer		BioGRID(yeast)
		BioGRID(yeast)
Wid Weight matrix for topological 		BioGRID(yeast)
structure embedding  Watt Weight matrix for attribute		BioGRID(yeast)
 embedding		BioGRID(yeast)
		BioGRID(yeast)
Wout Weight matrix for output 		BioGRID(yeast)
layer  which measures the likelihood of		BioGRID(yeast)
 gene vi being connected with		BioGRID(yeast)
 vj. Let function f represents		BioGRID(yeast)
 the mapping of two genes		BioGRID(yeast)
 vi and vj to their		BioGRID(yeast)
 estimated proximity score. Let p(N		BioGRID(yeast)
 |v) be the likelihood of		BioGRID(yeast)
 observing a neighborhood N for		BioGRID(yeast)
 a gene v. By assuming		BioGRID(yeast)
 conditional independence, we can factorize		BioGRID(yeast)
 the likelihood so that the		BioGRID(yeast)
 likelihood of observing a neighborhood		BioGRID(yeast)
 gene is independent of observ		BioGRID(yeast)
- ing any other neighborhood 		BioGRID(yeast)
gene, given a gene vi:  p(Ni|vi		BioGRID(yeast)
) = ∏  vj∈Ni p(vj|vi) (2		BioGRID(yeast)
)  where Ni represents the neighborhood		BioGRID(yeast)
 genes of the gene vi		BioGRID(yeast)
. Global structure proximity for 		BioGRID(yeast)
a gene vi can be 		BioGRID(yeast)
pre- served by maximizing the 		BioGRID(yeast)
conditional probability over all genes 		BioGRID(yeast)
in the neighborhood. Hence, we 		BioGRID(yeast)
can define the likelihood function 		BioGRID(yeast)
that preserve global structure proximity 		BioGRID(yeast)
as:  L = M		BioGRID(yeast)
∏  i=1 p(Ni|vi		BioGRID(yeast)
) =  M		BioGRID(yeast)
∏  i=1		BioGRID(yeast)
		BioGRID(yeast)
vj∈Ni p(vj|vi) (3)  Let v(s)i denotes the dense		BioGRID(yeast)
 vector generated from one-hot gene		BioGRID(yeast)
 ID vector, which represents topological		BioGRID(yeast)
 informa- tion of that gene		BioGRID(yeast)
. GNE follows direct encoding 		BioGRID(yeast)
methods [13, 14] to map 		BioGRID(yeast)
genes to vector embeddings, simply 		BioGRID(yeast)
known as embedding 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
KC et al. BMC Systems 		BioGRID(yeast)
Biology 2019, 13(Suppl 2):38 Page 5 of 14		BioGRID(yeast)
		BioGRID(yeast)
v(s)i = Widvi (4) where 		BioGRID(yeast)
Wid ∈ Rd×M is a 		BioGRID(yeast)
matrix containing the embedding vectors 		BioGRID(yeast)
for all genes and 		BioGRID(yeast)
vi ∈ IM is a 		BioGRID(yeast)
one-hot indica- tor vector indicating 		BioGRID(yeast)
the column of Wid corresponding 		BioGRID(yeast)
to gene vi.  Gene expression modeling GNE encodes		BioGRID(yeast)
 the expression data from microarray		BioGRID(yeast)
 exper- iments to the dense		BioGRID(yeast)
 representation using a non-linear transformation		BioGRID(yeast)
. The amount of mRNA 		BioGRID(yeast)
produced during transcription measured over 		BioGRID(yeast)
a number of experiments helps 		BioGRID(yeast)
to identify similarly expressed genes. 		BioGRID(yeast)
Since expres- sion data have 		BioGRID(yeast)
inherent noise [15], transforming expres- 		BioGRID(yeast)
sion data using a non-linear 		BioGRID(yeast)
transformation can be helpful to 		BioGRID(yeast)
uncover the underlying representation. Let 		BioGRID(yeast)
xi be the vector of 		BioGRID(yeast)
expression values of gene vi 		BioGRID(yeast)
measured over E experiments. Using 		BioGRID(yeast)
non-linear transformation, we can capture 		BioGRID(yeast)
the non-linearities of expression data 		BioGRID(yeast)
of gene vi as:  v(a)i = δa(Watt · xi		BioGRID(yeast)
) (5) where v(a)i represents 		BioGRID(yeast)
the lower dimensional attribute rep- 		BioGRID(yeast)
resentation vector for gene vi. 		BioGRID(yeast)
Watt , and δa represents 		BioGRID(yeast)
the weight matrix, and activation 		BioGRID(yeast)
function of attribute transformation layer 		BioGRID(yeast)
respectively.  We use the deep model		BioGRID(yeast)
 to approximate the attribute proximity		BioGRID(yeast)
 by capturing complex statistical relationships		BioGRID(yeast)
 between attributes and introducing non-linearities		BioGRID(yeast)
, simi- lar to structural 		BioGRID(yeast)
embedding.  GNE integration GNE models the		BioGRID(yeast)
 integration of network structure and		BioGRID(yeast)
 attribute information to learn more		BioGRID(yeast)
 comprehensive embeddings for gene networks		BioGRID(yeast)
. GNE takes two inputs: 		BioGRID(yeast)
one for topological information of 		BioGRID(yeast)
a gene as one hot 		BioGRID(yeast)
gene ID vector and another 		BioGRID(yeast)
for its expression as an 		BioGRID(yeast)
attribute vector. Each input is 		BioGRID(yeast)
encoded to its respective embed- 		BioGRID(yeast)
dings. One hot representation for 		BioGRID(yeast)
a gene vi is projected 		BioGRID(yeast)
to the dense vector v(s)i 		BioGRID(yeast)
which captures the topological properties. 		BioGRID(yeast)
Non-linear transformation of attribute vec- 		BioGRID(yeast)
tor generates compact representation vector 		BioGRID(yeast)
v(a)i . Previous work [16] 		BioGRID(yeast)
combines heterogeneous information using the 		BioGRID(yeast)
late fusion approach. However, the 		BioGRID(yeast)
late fusion approach is the 		BioGRID(yeast)
approach of learning separate models 		BioGRID(yeast)
for hetero- geneous information and 		BioGRID(yeast)
integrating the representations learned from 		BioGRID(yeast)
separate models. On the other 		BioGRID(yeast)
hand, the early fusion combines 		BioGRID(yeast)
heterogeneous information and train the 		BioGRID(yeast)
model on combined representations [17]. 		BioGRID(yeast)
We thus propose to use 		BioGRID(yeast)
the early fusion approach to 		BioGRID(yeast)
combine them by concatenating. As 		BioGRID(yeast)
a result, learning from topo- 		BioGRID(yeast)
logical and attribute information can 		BioGRID(yeast)
complement each other, allowing the 		BioGRID(yeast)
model to learn their complex 		BioGRID(yeast)
statistical relationships as well. Embeddings 		BioGRID(yeast)
from topological and attribute information 		BioGRID(yeast)
are concatenated into a vector 		BioGRID(yeast)
as:  vi		BioGRID(yeast)
		BioGRID(yeast)
v(s)i λv (a) i  ] (6		BioGRID(yeast)
)  where λ is the importance		BioGRID(yeast)
 of gene expression information relative		BioGRID(yeast)
 to topological information		BioGRID(yeast)
.  The concatenated vectors are fed		BioGRID(yeast)
 into a multilayer perceptron with		BioGRID(yeast)
 k hidden layers. The hidden		BioGRID(yeast)
 represen- tations from each hidden		BioGRID(yeast)
 layer in GNE are denoted		BioGRID(yeast)
 as h(0)i , h		BioGRID(yeast)
		BioGRID(yeast)
1) i , ....., h  (k) i , which can		BioGRID(yeast)
 be defined as		BioGRID(yeast)
		BioGRID(yeast)
h(0)i = δ (  W0vi + b(0		BioGRID(yeast)
		BioGRID(yeast)
h(k)i = δk (  Wkh(k−1)i + b(k) ) (7		BioGRID(yeast)
)  where δk represents the activation		BioGRID(yeast)
 function of layer k. h(0)i		BioGRID(yeast)
 represents initial representation and h(k)i		BioGRID(yeast)
 represents final representation of the		BioGRID(yeast)
 input gene vi. Transformation of		BioGRID(yeast)
 input data using multiple non-linear		BioGRID(yeast)
 layers has shown to improve		BioGRID(yeast)
 the representation of input data		BioGRID(yeast)
 [18]. Moreover, stacking multiple layers		BioGRID(yeast)
 of non-linear transformations can help		BioGRID(yeast)
 to learn high-order statistical relationships		BioGRID(yeast)
 between topological properties and attributes		BioGRID(yeast)
.  At last, final representation h(k)i		BioGRID(yeast)
 of a gene vi from		BioGRID(yeast)
 the last hidden layer is		BioGRID(yeast)
 transformed to probability vector, which		BioGRID(yeast)
 contains the conditional probability of		BioGRID(yeast)
 all other genes to vi		BioGRID(yeast)
:  oi = [ p(v1|vi), p(v2|vi		BioGRID(yeast)
), . . . , 		BioGRID(yeast)
p(vM|vi)  ] (8		BioGRID(yeast)
)  where p(vj|vi) represents the probability		BioGRID(yeast)
 of gene vi being related		BioGRID(yeast)
 to gene vj and oi		BioGRID(yeast)
 represents the output probabil- ity		BioGRID(yeast)
 vector with the conditional probability		BioGRID(yeast)
 of gene vi being connected		BioGRID(yeast)
 to all other genes		BioGRID(yeast)
.  Weight matrix Wout between the		BioGRID(yeast)
 last hidden layer and the		BioGRID(yeast)
 output layer corresponds to the		BioGRID(yeast)
 abstractive represen- tation of neighborhood		BioGRID(yeast)
 of genes. A jth row		BioGRID(yeast)
 from Wout refers to the		BioGRID(yeast)
 compact representation of neighborhood of		BioGRID(yeast)
 gene vj, which can be		BioGRID(yeast)
 denoted as ṽj. The proximity		BioGRID(yeast)
 score between gene vi and		BioGRID(yeast)
 vj can be defined as		BioGRID(yeast)
:  f (vi, vj) = ṽj		BioGRID(yeast)
 · h(k)i (9) which can		BioGRID(yeast)
 be replaced into Eq. 1		BioGRID(yeast)
 to calculate the condi- tional		BioGRID(yeast)
 probability		BioGRID(yeast)
:  p(vj|vi) = exp		BioGRID(yeast)
		BioGRID(yeast)
ṽj · 		BioGRID(yeast)
		BioGRID(yeast)
(		BioGRID(yeast)
		BioGRID(yeast)
)		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
M j′=1 exp  ( ṽj′ · h(k)i		BioGRID(yeast)
		BioGRID(yeast)
10)  Our model learns two latent		BioGRID(yeast)
 representations h(k)i and ṽi for		BioGRID(yeast)
 a gene vi where h(k)i		BioGRID(yeast)
 is the representation of gene		BioGRID(yeast)
 as a node and ṽi		BioGRID(yeast)
 is the representation of the		BioGRID(yeast)
 gene vi as a neighbor		BioGRID(yeast)
. Neighborhood representation ṽi can 		BioGRID(yeast)
be com- bined with node 		BioGRID(yeast)
representation h(k)i by addition [19, 20		BioGRID(yeast)
] to get final representation 		BioGRID(yeast)
for a gene as:  yi = h(k)i + ṽi		BioGRID(yeast)
 (11) which returns us better		BioGRID(yeast)
 performance results		BioGRID(yeast)
		BioGRID(yeast)
KC et al. BMC Systems 		BioGRID(yeast)
Biology 2019, 13(Suppl 2):38 Page 6 of 14		BioGRID(yeast)
		BioGRID(yeast)
For an edge connecting gene 		BioGRID(yeast)
vi and vj, we create 		BioGRID(yeast)
fea- ture vector by combining 		BioGRID(yeast)
embeddings of those genes using 		BioGRID(yeast)
Hadamard product. Empirical evaluation shows 		BioGRID(yeast)
features created with Hadamard product 		BioGRID(yeast)
gives better performance over 		BioGRID(yeast)
concatenation [14]. Then, we train 		BioGRID(yeast)
a logistic classi- fier on 		BioGRID(yeast)
these features to classify whether 		BioGRID(yeast)
genes vi and vj interact 		BioGRID(yeast)
or not.  Parameter optimization To optimize GNE		BioGRID(yeast)
, the goal is to 		BioGRID(yeast)
maximize objective func- tion mentioned 		BioGRID(yeast)
in Eq. 10 as a 		BioGRID(yeast)
function of all parame- ters. 		BioGRID(yeast)
Let � be the parameters 		BioGRID(yeast)
of GNE which includes {Wid, 		BioGRID(yeast)
Watt , Wout , �h} 		BioGRID(yeast)
and �h represents weight matrices 		BioGRID(yeast)
Wk of hidden layers. We 		BioGRID(yeast)
train our model to maximize 		BioGRID(yeast)
the objective function with respect 		BioGRID(yeast)
to all parameters � :  argmax		BioGRID(yeast)
		BioGRID(yeast)
M∑  i=1		BioGRID(yeast)
		BioGRID(yeast)
vj ∈ Ni log  exp		BioGRID(yeast)
		BioGRID(yeast)
ṽj · h(k)i )  ∑M j′=1 exp		BioGRID(yeast)
		BioGRID(yeast)
ṽj′ · 		BioGRID(yeast)
		BioGRID(yeast)
(		BioGRID(yeast)
		BioGRID(yeast)
)		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
12)  Maximizing this objective function with		BioGRID(yeast)
 respect to � is computationally		BioGRID(yeast)
 expensive, which requires the calculation		BioGRID(yeast)
 of partition function		BioGRID(yeast)
		BioGRID(yeast)
M j′=1 exp  ( ṽj′ · h(k)i		BioGRID(yeast)
		BioGRID(yeast)
for each gene.  To calculate a single probability		BioGRID(yeast)
, we need to aggregate 		BioGRID(yeast)
all genes in the network. 		BioGRID(yeast)
To address this problem, we 		BioGRID(yeast)
adopt the approach of negative 		BioGRID(yeast)
sampling [13] which samples the 		BioGRID(yeast)
negative interactions, interactions with no 		BioGRID(yeast)
evidence of their existence, according 		BioGRID(yeast)
to some noise distribution for 		BioGRID(yeast)
each edge eij. This approach 		BioGRID(yeast)
allows us to sample a 		BioGRID(yeast)
small subset of genes from 		BioGRID(yeast)
the network as negative samples 		BioGRID(yeast)
for a gene, considering that 		BioGRID(yeast)
the genes on selected subset 		BioGRID(yeast)
don’t fall in the neighborhood 		BioGRID(yeast)
Ni of the gene. Above 		BioGRID(yeast)
objective function enhances the similarity 		BioGRID(yeast)
of a gene viwith its 		BioGRID(yeast)
neigh- borhood genes vj ∈ 		BioGRID(yeast)
Ni and weakens the similarity 		BioGRID(yeast)
with genes not in its 		BioGRID(yeast)
neighborhood genes vj /∈ Ni. 		BioGRID(yeast)
It is inap- propriate to 		BioGRID(yeast)
assume that the two genes 		BioGRID(yeast)
in the network are not 		BioGRID(yeast)
related if they are not 		BioGRID(yeast)
connected. It may be the 		BioGRID(yeast)
case that there is not 		BioGRID(yeast)
enough experimental evidence to support 		BioGRID(yeast)
that they are related yet. 		BioGRID(yeast)
Thus, forcing the dissimilarity of 		BioGRID(yeast)
a gene with all other 		BioGRID(yeast)
genes, not in its neighborhood 		BioGRID(yeast)
Ni seems to be inappropriate.  We adopt Adaptive Moment Estimation		BioGRID(yeast)
 (Adam) opti- mization [21], which		BioGRID(yeast)
 is an extension to stochastic		BioGRID(yeast)
 gra- dient descent, for optimizing		BioGRID(yeast)
 Eq. 12. Adam computes the		BioGRID(yeast)
 adaptive learning rate for each		BioGRID(yeast)
 parameter by per- forming smaller		BioGRID(yeast)
 updates for the frequent parameters		BioGRID(yeast)
 and larger updates for the		BioGRID(yeast)
 infrequent parameters. The Adam method		BioGRID(yeast)
 provides the ability of AdaGrad		BioGRID(yeast)
 [22] to deal with sparse		BioGRID(yeast)
 gradients and also the ability		BioGRID(yeast)
 of RMSProp [23] to deal		BioGRID(yeast)
 with non-stationary objectives. In each		BioGRID(yeast)
 step, Adam algorithm samples mini-batch		BioGRID(yeast)
 of interactions and then		BioGRID(yeast)
		BioGRID(yeast)
updates GNE’s parameters. To address 		BioGRID(yeast)
the issue of over- fitting, 		BioGRID(yeast)
regularization like dropout [24] and 		BioGRID(yeast)
batch normal- ization [25] is 		BioGRID(yeast)
added to hidden layers. Proper 		BioGRID(yeast)
optimization of GNE gives the 		BioGRID(yeast)
final representation for each gene.  Experimental setup We evaluate our		BioGRID(yeast)
 model using two real organism		BioGRID(yeast)
 datasets. We take gene interaction		BioGRID(yeast)
 network data from the BioGRID		BioGRID(yeast)
 database [26] and gene expression		BioGRID(yeast)
 data from DREAM5 challenge [7		BioGRID(yeast)
]. We use two interaction 		BioGRID(yeast)
datasets from BioGRID database (2017 		BioGRID(yeast)
released version 3.4.153 and 2018 		BioGRID(yeast)
released version 3.4.158) to evaluate 		BioGRID(yeast)
the predictive performance of our 		BioGRID(yeast)
model. Self-interactions and redun- dant 		BioGRID(yeast)
interactions are removed from interaction 		BioGRID(yeast)
datasets. The statistics of the 		BioGRID(yeast)
datasets are shown in Table 2		BioGRID(yeast)
.  We evaluate the learned embeddings		BioGRID(yeast)
 to infer gene net- work		BioGRID(yeast)
 structure. We randomly hold out		BioGRID(yeast)
 a fraction of inter- actions		BioGRID(yeast)
 as the validation set for		BioGRID(yeast)
 hyper-parameter tuning. Then, we divide		BioGRID(yeast)
 the remaining interactions randomly into		BioGRID(yeast)
 training and testing dataset with		BioGRID(yeast)
 the equal number of interactions		BioGRID(yeast)
. Since the validation set 		BioGRID(yeast)
and the test set con- 		BioGRID(yeast)
tains only positive interactions, we 		BioGRID(yeast)
randomly sample an equal number 		BioGRID(yeast)
of gene pairs from the 		BioGRID(yeast)
network, consider- ing the missing 		BioGRID(yeast)
edge between the gene pairs 		BioGRID(yeast)
represents the absence of interactions. 		BioGRID(yeast)
Given the gene network G 		BioGRID(yeast)
with a fraction of missing 		BioGRID(yeast)
interactions, the task is to 		BioGRID(yeast)
predict these missing interactions.  We compare the GNE model		BioGRID(yeast)
 with five competing methods. Correlation		BioGRID(yeast)
 directly predicts the interactions between		BioGRID(yeast)
 genes based on the correlation		BioGRID(yeast)
 of expression pro- files. Then		BioGRID(yeast)
, the following three baselines (		BioGRID(yeast)
Isomap, LINE, and node2vec) are 		BioGRID(yeast)
network embedding methods. Specif- ically, 		BioGRID(yeast)
node2vec is the strong baseline 		BioGRID(yeast)
for structural net- work embedding. 		BioGRID(yeast)
We evaluate the performance of 		BioGRID(yeast)
GNE against the following methods:  • Correlation [27] It computes		BioGRID(yeast)
 Pearson’s correlation coefficient between all		BioGRID(yeast)
 genes and the interactions are		BioGRID(yeast)
 ranked via correlation scores, i.e		BioGRID(yeast)
., highly correlated gene pairs 		BioGRID(yeast)
receive higher confidence.  • Isomap [10] It computes		BioGRID(yeast)
 all-pairs shortest-path distances to create		BioGRID(yeast)
 a distance matrix and performs		BioGRID(yeast)
 singular-value decomposition of that matrix		BioGRID(yeast)
 to learn a lower-dimensional representation		BioGRID(yeast)
. Genes separated  Table 2 Statistics of the		BioGRID(yeast)
 interaction datasets from BioGRID and		BioGRID(yeast)
 the gene expression data from		BioGRID(yeast)
 DREAM5 challenge		BioGRID(yeast)
		BioGRID(yeast)
Interactions) Expression data  Datasets #(Genes) 2017 version 2018		BioGRID(yeast)
 version #(Experiments		BioGRID(yeast)
)  Yeast 5950 544,652 557,487 536		BioGRID(yeast)
		BioGRID(yeast)
E. coli 4511 148,340 159,523 805		BioGRID(yeast)
		BioGRID(yeast)
KC et al. BMC Systems 		BioGRID(yeast)
Biology 2019, 13(Suppl 2):38 Page 7 of 14		BioGRID(yeast)
		BioGRID(yeast)
by the distance less than 		BioGRID(yeast)
threshold � in embedding space 		BioGRID(yeast)
are considered to have the 		BioGRID(yeast)
connection with each other and 		BioGRID(yeast)
the reliability index, a likelihood 		BioGRID(yeast)
indicating the interaction between two 		BioGRID(yeast)
genes, is computed using 		BioGRID(yeast)
FSWeight [28].  • LINE [16] Two separate		BioGRID(yeast)
 embeddings are learned by preserving		BioGRID(yeast)
 first-order and second-order proximity of		BioGRID(yeast)
 the network structure respectively. Then		BioGRID(yeast)
, these embeddings are concatenated 		BioGRID(yeast)
to get final representations for 		BioGRID(yeast)
each node.  • node2vec [14] It learns		BioGRID(yeast)
 the embeddings of the node		BioGRID(yeast)
 by applying Skip-gram model to		BioGRID(yeast)
 node sequences generated by a		BioGRID(yeast)
 biased random walk. We tuned		BioGRID(yeast)
 two hyper-parameters p and q		BioGRID(yeast)
 that control the random walk		BioGRID(yeast)
.  Note that the competing methods		BioGRID(yeast)
 such as Isomap, LINE, and		BioGRID(yeast)
 node2vec are designed to capture		BioGRID(yeast)
 only the topological properties of		BioGRID(yeast)
 the network. For the fair		BioGRID(yeast)
 com- parison with GNE that		BioGRID(yeast)
 additionally integrates expression data, we		BioGRID(yeast)
 concatenate attribute feature vector with		BioGRID(yeast)
 learned gene representation to extend		BioGRID(yeast)
 baselines by including the gene		BioGRID(yeast)
 expression. We name these variants		BioGRID(yeast)
 as Isomap+, LINE+, and node2vec		BioGRID(yeast)
+.  We have implemented GNE with		BioGRID(yeast)
 TensorFlow frame- work [29]. The		BioGRID(yeast)
 parameter settings for GNE are		BioGRID(yeast)
 deter- mined by its performance		BioGRID(yeast)
 on the validation set. We		BioGRID(yeast)
 randomly initialize GNE’s parameters, optimizing		BioGRID(yeast)
 with mini-batch Adam. We test		BioGRID(yeast)
 the batch size of [8		BioGRID(yeast)
, 16, 32, 64, 128, 256		BioGRID(yeast)
] and learning rate 		BioGRID(yeast)
of [0.1, 0.01, 0.005, 0.002, 0		BioGRID(yeast)
.001, 0.0001]. We test the 		BioGRID(yeast)
number of negative samples to 		BioGRID(yeast)
be [2, 5, 10, 15, 20		BioGRID(yeast)
] as suggested by [13]. 		BioGRID(yeast)
We test the embedding dimension 		BioGRID(yeast)
d of [32, 64, 128, 256		BioGRID(yeast)
] for all methods. Also, 		BioGRID(yeast)
we evaluate model’s performance with 		BioGRID(yeast)
respect to differ- ent values 		BioGRID(yeast)
of λ [0, 0.2, 0.4, 0		BioGRID(yeast)
.6, 0.8, 1], which is 		BioGRID(yeast)
discussed in more detail later. 		BioGRID(yeast)
The parameters are selected based 		BioGRID(yeast)
on empirical evaluation and Table 3 summarizes the optimal parameters tuned		BioGRID(yeast)
 on validation data sets		BioGRID(yeast)
.  To capture the non-linearity of		BioGRID(yeast)
 gene expression data, we choose		BioGRID(yeast)
 Exponential Linear Unit (ELU) [30		BioGRID(yeast)
] activation function, which corresponds 		BioGRID(yeast)
to δa in Eq. 5. 		BioGRID(yeast)
Also, ELU acti- vation avoids 		BioGRID(yeast)
vanishing gradient problem and provides 		BioGRID(yeast)
improved learning characteristics in comparison 		BioGRID(yeast)
to other methods. We use 		BioGRID(yeast)
a single hidden layer (		BioGRID(yeast)
k = 1) with hyperbolic 		BioGRID(yeast)
tangent activation (Tanh) to model 		BioGRID(yeast)
complex statistical relationships between topological 		BioGRID(yeast)
properties  and attributes of the gene		BioGRID(yeast)
. The choice of ELU 		BioGRID(yeast)
for attribute transformation and Tanh 		BioGRID(yeast)
for hidden layer shows better 		BioGRID(yeast)
performance upon empirical evaluation.  We use the area under		BioGRID(yeast)
 the ROC curve (AUROC) and		BioGRID(yeast)
 area under the precision-recall curve		BioGRID(yeast)
 (AUPR) [31] to eval- uate		BioGRID(yeast)
 the rankings generated by the		BioGRID(yeast)
 model for interactions in the		BioGRID(yeast)
 test set. These metrics are		BioGRID(yeast)
 widely used in evaluating the		BioGRID(yeast)
 ranked list of predictions in		BioGRID(yeast)
 gene interaction [4		BioGRID(yeast)
].  Results and discussion We evaluate		BioGRID(yeast)
 the ability of our GNE		BioGRID(yeast)
 model to predict gene interaction		BioGRID(yeast)
 of two real organisms. We		BioGRID(yeast)
 present empirical results of our		BioGRID(yeast)
 proposed method against other methods		BioGRID(yeast)
.  Analysis of gene embeddings We		BioGRID(yeast)
 visualize the embedding vectors of		BioGRID(yeast)
 genes learned by GNE. We		BioGRID(yeast)
 take the learned embeddings, which		BioGRID(yeast)
 specifi- cally model the interactions		BioGRID(yeast)
 by preserving topological and attribute		BioGRID(yeast)
 similarity. We embed these embeddings		BioGRID(yeast)
 into a 2D space using		BioGRID(yeast)
 t-SNE package [32] and visualize		BioGRID(yeast)
 them (Fig. 3). For comparison		BioGRID(yeast)
, we also visualize the 		BioGRID(yeast)
embeddings learned by structure-preserving deep 		BioGRID(yeast)
learning methods, such as LINE, 		BioGRID(yeast)
and node2vec.  In E. coli, a substantial		BioGRID(yeast)
 fraction of functionally related genes		BioGRID(yeast)
 are organized into operons, which		BioGRID(yeast)
 are the group of genes		BioGRID(yeast)
 that interact with each other		BioGRID(yeast)
 and are co-regulated [33]. Since		BioGRID(yeast)
 this concept fits well with		BioGRID(yeast)
 the topological and attribute proximity		BioGRID(yeast)
 implemented in GNE, we expect		BioGRID(yeast)
 GNE to place genes within		BioGRID(yeast)
 an operon close to each		BioGRID(yeast)
 other in the embedding space		BioGRID(yeast)
. To evaluate this, we 		BioGRID(yeast)
collect information about operons of 		BioGRID(yeast)
E. coli from the DOOR 		BioGRID(yeast)
database and visualize the embeddings 		BioGRID(yeast)
of genes within these operons (		BioGRID(yeast)
Fig. 3).  Figure 3 reveals the clustering		BioGRID(yeast)
 structure that corre- sponds to		BioGRID(yeast)
 the operons on E. coli		BioGRID(yeast)
. For example, operon with 		BioGRID(yeast)
operon id 3306 consists of 		BioGRID(yeast)
seven genes: rsxA, rsxB, rsx, 		BioGRID(yeast)
rsxD, rsxG, rsxE, and nth 		BioGRID(yeast)
that are involved in electron 		BioGRID(yeast)
transport. GNE infers similar representations 		BioGRID(yeast)
for these genes, resulting in 		BioGRID(yeast)
localized projection in the 2D 		BioGRID(yeast)
space. Similarly, other operons also 		BioGRID(yeast)
show similar patterns (Fig. 3).  To test if the pattern		BioGRID(yeast)
 in Fig. 3 holds across		BioGRID(yeast)
 all operons, we compute the		BioGRID(yeast)
 average Euclidean distance between each		BioGRID(yeast)
 gene’s vector representation and vector		BioGRID(yeast)
 representations of other genes within		BioGRID(yeast)
 the same operon. Genes within		BioGRID(yeast)
 the same operon have significantly		BioGRID(yeast)
 similar vector representa- tion yi		BioGRID(yeast)
 than expected by chance (p-value		BioGRID(yeast)
 = 1.75e − 127, 2-sample		BioGRID(yeast)
 KS test		BioGRID(yeast)
).  Table 3 Optimal parameter settings		BioGRID(yeast)
 for GNE model		BioGRID(yeast)
		BioGRID(yeast)
Dataset Learning rate Batch size 		BioGRID(yeast)
Embedding dimension (d) Epoch Negative 		BioGRID(yeast)
samples  Yeast 0.005 256 128 20		BioGRID(yeast)
 10		BioGRID(yeast)
		BioGRID(yeast)
E. coli 0.002 128 128 20 10		BioGRID(yeast)
		BioGRID(yeast)
KC et al. BMC Systems 		BioGRID(yeast)
Biology 2019, 13(Suppl 2):38 Page 8 of 14		BioGRID(yeast)
		BioGRID(yeast)
Fig. 3 Visualization of learned 		BioGRID(yeast)
embeddings for genes on E. 		BioGRID(yeast)
coli. Genes are mapped to 		BioGRID(yeast)
the 2D space using the 		BioGRID(yeast)
t-SNE package [32] with learned 		BioGRID(yeast)
gene representations (yi , 		BioGRID(yeast)
i = 1, 2, . . . , M		BioGRID(yeast)
) from different methods: a 		BioGRID(yeast)
GNE, b LINE, and c 		BioGRID(yeast)
node2vec as input. Operons 3203, 3274, 3279, 3306, and 3736 of		BioGRID(yeast)
 E. coli are visualized and		BioGRID(yeast)
 show clustering patterns. Best viewed		BioGRID(yeast)
 on screen		BioGRID(yeast)
		BioGRID(yeast)
Thus, the analysis here indicates 		BioGRID(yeast)
that GNE can learn similar 		BioGRID(yeast)
representations for genes with similar 		BioGRID(yeast)
topological properties and expression.  Gene interaction prediction We randomly		BioGRID(yeast)
 remove 50% of interactions from		BioGRID(yeast)
 the net- work and compare		BioGRID(yeast)
 various methods to evaluate their		BioGRID(yeast)
 pre- dictions for 50% missing		BioGRID(yeast)
 interactions. Table 4 shows the		BioGRID(yeast)
 performance of GNE and other		BioGRID(yeast)
 methods on gene interac- tion		BioGRID(yeast)
 prediction across different datasets. As		BioGRID(yeast)
 our method significantly outperforms other		BioGRID(yeast)
 competing methods, it indicates the		BioGRID(yeast)
 informativeness of gene expression in		BioGRID(yeast)
 pre- dicting missing interactions. Also		BioGRID(yeast)
, our model is capable 		BioGRID(yeast)
of integrating attributes with topological 		BioGRID(yeast)
properties to learn better representations.  Table 4 Area under ROC		BioGRID(yeast)
 curve (AUROC) and Area under		BioGRID(yeast)
 PR curve (AUPR) for gene		BioGRID(yeast)
 Interaction Prediction		BioGRID(yeast)
		BioGRID(yeast)
Methods Yeast E. coli  AUROC AUPR AUROC AUPR		BioGRID(yeast)
		BioGRID(yeast)
Correlation 0.582 0.579 0.537 0.557  Isomap 0.507 0.588 0.559 0.672		BioGRID(yeast)
		BioGRID(yeast)
LINE 0.726 0.686 0.897 0.851  node2vec 0.739 0.708 0.912 0.862		BioGRID(yeast)
		BioGRID(yeast)
Isomap+ 0.653 0.652 0.644 0.649  LINE+ 0.745 0.713 0.899 0.856		BioGRID(yeast)
		BioGRID(yeast)
node2vec+ 0.751 0.716 0.871 0.826  GNE (Topology) 0.787 0.784 0.930		BioGRID(yeast)
 0.931		BioGRID(yeast)
		BioGRID(yeast)
GNE (our model) 0.825* 0.821* 0		BioGRID(yeast)
.940* 0.939*  + indicates the concatenation of		BioGRID(yeast)
 expression data with learned embeddings		BioGRID(yeast)
 to create final representation		BioGRID(yeast)
. * denotes that GNE 		BioGRID(yeast)
significantly outperforms node2vec at 0.01 		BioGRID(yeast)
level paired t-test. Note that 		BioGRID(yeast)
method that achieves the best 		BioGRID(yeast)
performance is bold faced  We compare our model with		BioGRID(yeast)
 a correlation-based method, that takes		BioGRID(yeast)
 only expression data into account		BioGRID(yeast)
. Our model shows significant 		BioGRID(yeast)
improvement of 0.243 (AUROC), 0.242 (		BioGRID(yeast)
AUPR) on yeast and 0.403 (		BioGRID(yeast)
AUROC), 0.382 (AUPR) on E. 		BioGRID(yeast)
coli over correlation-based methods. This 		BioGRID(yeast)
improve- ment suggests the significance 		BioGRID(yeast)
of the topological proper- ties 		BioGRID(yeast)
of the gene network.  The network embedding method, Isomap		BioGRID(yeast)
, performs poorly in comparison 		BioGRID(yeast)
to correlation-based methods on yeast 		BioGRID(yeast)
because of its limitation on 		BioGRID(yeast)
network inference. Deep learning based 		BioGRID(yeast)
network embedding methods such as 		BioGRID(yeast)
LINE, and node2vec show the 		BioGRID(yeast)
significant gain over Isomap and 		BioGRID(yeast)
correlation-based methods. node2vec out- performs 		BioGRID(yeast)
LINE across two datasets. Moreover, 		BioGRID(yeast)
GNE trained only with topological 		BioGRID(yeast)
properties outperforms these structured-based deep 		BioGRID(yeast)
learning methods (Table 4). However, 		BioGRID(yeast)
these methods don’t consider the 		BioGRID(yeast)
attributes of the gene that 		BioGRID(yeast)
we suggest to contain useful 		BioGRID(yeast)
informa- tion for gene interaction 		BioGRID(yeast)
prediction. By adding expres- sion 		BioGRID(yeast)
data with topological properties, GNE 		BioGRID(yeast)
outperforms structure-preserving deep embedding methods 		BioGRID(yeast)
across both datasets.  Focusing on the results corresponding		BioGRID(yeast)
 to the integra- tion of		BioGRID(yeast)
 expression data with topological properties		BioGRID(yeast)
, we find that the 		BioGRID(yeast)
method of integrating the expression 		BioGRID(yeast)
data plays an essential role 		BioGRID(yeast)
in the performance. Performance of 		BioGRID(yeast)
node2vec+ (LINE+, Isomap+) shows little 		BioGRID(yeast)
improvement with the integration of 		BioGRID(yeast)
expression data on yeast. How- 		BioGRID(yeast)
ever, node2vec+ (LINE+, Isomap+) has 		BioGRID(yeast)
no improvement or decline in 		BioGRID(yeast)
performance on E. coli. The 		BioGRID(yeast)
decline in per- formance indicates 		BioGRID(yeast)
that merely concatenating the expres- 		BioGRID(yeast)
sion vector with learned representations 		BioGRID(yeast)
for the gene is insufficient 		BioGRID(yeast)
to capture the rich information 		BioGRID(yeast)
in expression data. The late 		BioGRID(yeast)
fusion approach of combining the 		BioGRID(yeast)
embed- ding vector corresponding to 		BioGRID(yeast)
the topological 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
KC et al. BMC Systems 		BioGRID(yeast)
Biology 2019, 13(Suppl 2):38 Page 9 of 14		BioGRID(yeast)
		BioGRID(yeast)
of the gene network and 		BioGRID(yeast)
the feature vector represent- ing 		BioGRID(yeast)
expression data has no significant 		BioGRID(yeast)
improvement in the performance (except 		BioGRID(yeast)
Isomap). In contrast, our model 		BioGRID(yeast)
incorporates gene expression data with 		BioGRID(yeast)
topological prop- erties by the 		BioGRID(yeast)
early fusion method and shows 		BioGRID(yeast)
significant improvement over other methods.  Impact of network sparsity We		BioGRID(yeast)
 investigate the robustness of our		BioGRID(yeast)
 model to network sparsity. We		BioGRID(yeast)
 hold out 10% interactions as		BioGRID(yeast)
 the test set and change		BioGRID(yeast)
 the sparsity of the remaining		BioGRID(yeast)
 network by randomly removing a		BioGRID(yeast)
 portion of remaining interactions. Then		BioGRID(yeast)
, we train GNE to 		BioGRID(yeast)
predict interactions in the test 		BioGRID(yeast)
set and eval- uate the 		BioGRID(yeast)
change in performance to network 		BioGRID(yeast)
sparsity. We evaluate two versions 		BioGRID(yeast)
of our implementations: GNE with 		BioGRID(yeast)
only topological properties and GNE 		BioGRID(yeast)
with topological properties and expression 		BioGRID(yeast)
data. The result is shown 		BioGRID(yeast)
in Fig. 4.  Figure 4 shows that our		BioGRID(yeast)
 method’s performance improves with an		BioGRID(yeast)
 increase in the number of		BioGRID(yeast)
 training interactions across datasets. Also		BioGRID(yeast)
, our method’s performance improves 		BioGRID(yeast)
when expression data is integrated 		BioGRID(yeast)
with the topological structure. Specifically, 		BioGRID(yeast)
GNE trained on 10% of 		BioGRID(yeast)
total inter- actions and attributes 		BioGRID(yeast)
of yeast shows a significant 		BioGRID(yeast)
gain of 0.172 AUROC (from 0		BioGRID(yeast)
.503 to 0.675) over GNE 		BioGRID(yeast)
trained only with 10% of 		BioGRID(yeast)
total interactions as shown in 		BioGRID(yeast)
Fig. 4. Similarly, GNE improves 		BioGRID(yeast)
the AUROC from 0.497 to 0		BioGRID(yeast)
.816 for E. coli with 		BioGRID(yeast)
the same setup as shown 		BioGRID(yeast)
in Fig. 4. The integration 		BioGRID(yeast)
of gene expression data results 		BioGRID(yeast)
in less improve- ment when 		BioGRID(yeast)
we train GNE on a 		BioGRID(yeast)
relatively large number of interactions.  Moreover, the performance of GNE		BioGRID(yeast)
 trained with 50% of total		BioGRID(yeast)
 interactions and expression data is		BioGRID(yeast)
 comparable to be trained with		BioGRID(yeast)
 80% of total interactions without		BioGRID(yeast)
 gene expression data as shown		BioGRID(yeast)
 in Fig. 4. The integration		BioGRID(yeast)
 of expression data with topological		BioGRID(yeast)
 properties into GNE model has		BioGRID(yeast)
 more improvement on E. coli		BioGRID(yeast)
 than yeast when we train		BioGRID(yeast)
 with 10% of total interactions		BioGRID(yeast)
 for each dataset		BioGRID(yeast)
.  The reason for this is		BioGRID(yeast)
 likely the difference in the		BioGRID(yeast)
 number of available interactions for		BioGRID(yeast)
 yeast and E. coli (Table		BioGRID(yeast)
 2). This indicates the informativeness		BioGRID(yeast)
 of gene expression when we		BioGRID(yeast)
 have few interactions and supports		BioGRID(yeast)
 the idea that the integration		BioGRID(yeast)
 of expression data with topological		BioGRID(yeast)
 properties improves gene interaction prediction		BioGRID(yeast)
.  Impact of λ GNE involves		BioGRID(yeast)
 the parameter λ that controls		BioGRID(yeast)
 the impor- tance of gene		BioGRID(yeast)
 expression information relative to topolog		BioGRID(yeast)
- ical properties of gene 		BioGRID(yeast)
network as shown in Eq. 6		BioGRID(yeast)
. We examine how the 		BioGRID(yeast)
choice of the parameter λ 		BioGRID(yeast)
affects our method’s performance. Figure 5 shows the comparison of our		BioGRID(yeast)
 method’s performance with different values		BioGRID(yeast)
 of λ when GNE is		BioGRID(yeast)
 trained on varying percentage of		BioGRID(yeast)
 total interactions		BioGRID(yeast)
.  We evaluate the impact of		BioGRID(yeast)
 λ on range [0, 0.2		BioGRID(yeast)
, 0.4, 0.6, 0.8, 1]. 		BioGRID(yeast)
When λ becomes 0, the 		BioGRID(yeast)
learned representations model only topological 		BioGRID(yeast)
properties. In contrast, setting the 		BioGRID(yeast)
high value for λ makes 		BioGRID(yeast)
GNE learn only from attributes 		BioGRID(yeast)
and degrades its performance. Therefore, 		BioGRID(yeast)
our model performs well when 		BioGRID(yeast)
λ is within [0, 1].  Figure 5 shows that the		BioGRID(yeast)
 integration of expression data improves		BioGRID(yeast)
 the performance of GNE to		BioGRID(yeast)
 predict gene interac- tions. Impact		BioGRID(yeast)
 of λ depends on the		BioGRID(yeast)
 number of interactions used to		BioGRID(yeast)
 train GNE. If GNE is		BioGRID(yeast)
 trained with few interactions, integration		BioGRID(yeast)
 of expression data with topological		BioGRID(yeast)
 properties plays a vital role		BioGRID(yeast)
 in predicting missing interactions. As		BioGRID(yeast)
 the number of training interactions		BioGRID(yeast)
 increases, integration of expression data		BioGRID(yeast)
 has less impact but still		BioGRID(yeast)
 improves the performance over only		BioGRID(yeast)
 topological properties		BioGRID(yeast)
.  Figures 4 and 5 demonstrate		BioGRID(yeast)
 that the expression data contributes		BioGRID(yeast)
 the increase in AUROC by		BioGRID(yeast)
 nearly 0.14 when interactions are		BioGRID(yeast)
 less than 40% for yeast		BioGRID(yeast)
 and about 0.32 when interactions		BioGRID(yeast)
 are less than 10% for		BioGRID(yeast)
 E. coli. More topo- logical		BioGRID(yeast)
 properties and attributes are required		BioGRID(yeast)
 for yeast than E. coli		BioGRID(yeast)
. It may be related 		BioGRID(yeast)
to the fact that yeast 		BioGRID(yeast)
is a more complex species 		BioGRID(yeast)
than E. coli. Moreover, we 		BioGRID(yeast)
can spec- ulate that more 		BioGRID(yeast)
topological properties and attributes are  a b		BioGRID(yeast)
		BioGRID(yeast)
Fig. 4 AUROC comparison of 		BioGRID(yeast)
GNE’s performance with respect to 		BioGRID(yeast)
network sparsity. a yeast b 		BioGRID(yeast)
E. coli. Integration of expression 		BioGRID(yeast)
data with topological properties of 		BioGRID(yeast)
the gene network improves the 		BioGRID(yeast)
performance for both 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
KC et al. BMC Systems 		BioGRID(yeast)
Biology 2019, 13(Suppl 2):38 Page 10 of 14		BioGRID(yeast)
		BioGRID(yeast)
a b  Fig. 5 Impact of λ		BioGRID(yeast)
 on GNE’s performance trained with		BioGRID(yeast)
 different percentages of interactions. a		BioGRID(yeast)
 yeast b E. coli. Different		BioGRID(yeast)
 lines indicate performance of GNE		BioGRID(yeast)
 trained with different percentages of		BioGRID(yeast)
 interactions		BioGRID(yeast)
		BioGRID(yeast)
required for higher eukaryotes like 		BioGRID(yeast)
humans. In humans, GNE that 		BioGRID(yeast)
integrates topological properties with attributes 		BioGRID(yeast)
may be more successful than 		BioGRID(yeast)
the methods that only use 		BioGRID(yeast)
either topological properties or attributes.  This demonstrates the sensitivity of		BioGRID(yeast)
 GNE to parame- ter λ		BioGRID(yeast)
. This parameter λ has 		BioGRID(yeast)
a considerable impact on  our method’s performance and should		BioGRID(yeast)
 be appropriately selected		BioGRID(yeast)
.  Investigation of GNE’s predictions We		BioGRID(yeast)
 investigate the predictive ability of		BioGRID(yeast)
 our model in iden- tifying		BioGRID(yeast)
 new gene interactions. For this		BioGRID(yeast)
 aim, we consider		BioGRID(yeast)
		BioGRID(yeast)
a b  c d		BioGRID(yeast)
		BioGRID(yeast)
Fig. 6 Temporal holdout validation 		BioGRID(yeast)
in predicting new interactions. Performance 		BioGRID(yeast)
is measured by the area 		BioGRID(yeast)
under the ROC curve and 		BioGRID(yeast)
the area under the precision-recall 		BioGRID(yeast)
curve. Shown are the performance 		BioGRID(yeast)
of each method based on 		BioGRID(yeast)
the AUROC (a, b) and 		BioGRID(yeast)
AUPR (c, d) for yeast 		BioGRID(yeast)
and E. coli. The limit 		BioGRID(yeast)
of the y-axis is adjusted 		BioGRID(yeast)
to [0.5, 1.0] for the 		BioGRID(yeast)
precision-recall curve to make the 		BioGRID(yeast)
difference in performance more visible. 		BioGRID(yeast)
GNE outperforms LINE and 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
KC et al. BMC Systems 		BioGRID(yeast)
Biology 2019, 13(Suppl 2):38 Page 11 of 14		BioGRID(yeast)
		BioGRID(yeast)
Table 5 AUROC and AUPR 		BioGRID(yeast)
comparision for temporal holdout validation  Methods Yeast E. coli		BioGRID(yeast)
		BioGRID(yeast)
AUROC AUPR AUROC AUPR  LINE 0.620 0.611 0.569 0.598		BioGRID(yeast)
		BioGRID(yeast)
node2vec 0.640 0.609 0.587 0.599  GNE (our model) 0.710 0.683		BioGRID(yeast)
 0.653 0.658		BioGRID(yeast)
		BioGRID(yeast)
Note that method that achieves 		BioGRID(yeast)
the best performance is bold 		BioGRID(yeast)
faced  two versions of BioGRID interaction		BioGRID(yeast)
 datasets at two dif- ferent		BioGRID(yeast)
 time points (2017 and 2018		BioGRID(yeast)
 version), where the older version		BioGRID(yeast)
 is used for training and		BioGRID(yeast)
 the newer one is used		BioGRID(yeast)
 for testing the model (temporal		BioGRID(yeast)
 holdout validation). The 2018 version		BioGRID(yeast)
 contains 12,835 new interactions for		BioGRID(yeast)
 yeast and 11,185 new interactions		BioGRID(yeast)
 for E. coli than the		BioGRID(yeast)
 2017 ver- sion. GNE’s performance		BioGRID(yeast)
 trained with 50% and 80		BioGRID(yeast)
% of total interactions are 		BioGRID(yeast)
comparable for both yeast and 		BioGRID(yeast)
E. coli (Figs. 4 and 5		BioGRID(yeast)
). We thus train our 		BioGRID(yeast)
model with 50% of total 		BioGRID(yeast)
interactions from the 2017 version 		BioGRID(yeast)
to learn the embed- dings 		BioGRID(yeast)
for genes and demonstrate the 		BioGRID(yeast)
impact of integrating expression data 		BioGRID(yeast)
with topological properties. We create 		BioGRID(yeast)
the test set with new 		BioGRID(yeast)
interactions from the 2018 version 		BioGRID(yeast)
of BioGRID as positive interactions 		BioGRID(yeast)
and the equal number of 		BioGRID(yeast)
negative interactions randomly sampled. We 		BioGRID(yeast)
make pre- dictions for these 		BioGRID(yeast)
interactions using learned embeddings and 		BioGRID(yeast)
create a list of (Gene 		BioGRID(yeast)
vi, Gene vj, probability), ranked 		BioGRID(yeast)
by the predicted probability. We 		BioGRID(yeast)
consider predicted gene pairs with 		BioGRID(yeast)
the probabilities of 0.5 or 		BioGRID(yeast)
higher but are miss- ing 		BioGRID(yeast)
from BioGRID for further investigation 		BioGRID(yeast)
as we discuss later in 		BioGRID(yeast)
this section.  The temporal holdout performance of		BioGRID(yeast)
 our model in comparison to		BioGRID(yeast)
 other methods is shown in		BioGRID(yeast)
 Fig. 6. We		BioGRID(yeast)
		BioGRID(yeast)
observe that GNE outperforms both 		BioGRID(yeast)
node2vec and LINE in temporal 		BioGRID(yeast)
holdout validation across both yeast 		BioGRID(yeast)
and E. coli datasets, indicating 		BioGRID(yeast)
GNE can accurately predict new 		BioGRID(yeast)
genetic interactions. Table 5 shows 		BioGRID(yeast)
that GNE achieves substantial improvement 		BioGRID(yeast)
of 7.0 (AUROC), 7.4 (AUPR) 		BioGRID(yeast)
on yeast and 6.6 (AUROC), 5		BioGRID(yeast)
.9 (AUPR) on E. coli 		BioGRID(yeast)
datasets.  Table 6 shows the top		BioGRID(yeast)
 5 interactions with the sig		BioGRID(yeast)
- nificant increase in predicted 		BioGRID(yeast)
probability for both yeast and 		BioGRID(yeast)
E. coli after expression data 		BioGRID(yeast)
is integrated. We also provide 		BioGRID(yeast)
literature evidence with experimental evidence 		BioGRID(yeast)
code obtained from the BioGRID 		BioGRID(yeast)
database [26] sup- porting these 		BioGRID(yeast)
predictions. BioGRID compiles interaction data 		BioGRID(yeast)
from numerous publications through comprehen- 		BioGRID(yeast)
sive curation efforts. Taking new 		BioGRID(yeast)
interactions added to BioGRID (version 3		BioGRID(yeast)
.4.158) into consideration, we evalu- 		BioGRID(yeast)
ate the probability of these 		BioGRID(yeast)
interactions predicted by GNE trained 		BioGRID(yeast)
with and without expression data. 		BioGRID(yeast)
Specifically, integration of expression data 		BioGRID(yeast)
increases the probability of 8331 (		BioGRID(yeast)
out of 11,185) interactions for 		BioGRID(yeast)
E. coli (improving AUROC from 0		BioGRID(yeast)
.606 to 0.662) and 6,010 (		BioGRID(yeast)
out of 12,835) interactions for 		BioGRID(yeast)
yeast (improving AUROC from 0.685 		BioGRID(yeast)
to 0.707). Integration of topology 		BioGRID(yeast)
and expression data sig- nificantly 		BioGRID(yeast)
increases the probabilities of true 		BioGRID(yeast)
interactions between genes (Table 6).  To further evaluate GNE’s predictions		BioGRID(yeast)
, we consider the new 		BioGRID(yeast)
version of BioGRID (version 3.4.162) 		BioGRID(yeast)
and evaluate 2609 yeast gene 		BioGRID(yeast)
pairs (Additional file 1: Table 		BioGRID(yeast)
S1) and 871 E. coli 		BioGRID(yeast)
gene pairs (Additional file 2: 		BioGRID(yeast)
Table S2) predicted by GNE 		BioGRID(yeast)
with the probabilities of 0.5 		BioGRID(yeast)
or higher. We find that 128 (5%) yeast gene pairs and		BioGRID(yeast)
 78 (9%) E. coli gene		BioGRID(yeast)
 pairs are true interactions that		BioGRID(yeast)
 have been added to the		BioGRID(yeast)
 lat- est release of BioGRID		BioGRID(yeast)
. We then evaluate the 		BioGRID(yeast)
predictive ability of GNE by 		BioGRID(yeast)
calculating the percentage of true 		BioGRID(yeast)
inter- actions with regard to 		BioGRID(yeast)
different probability bins (Fig. 7). 		BioGRID(yeast)
Sixteen percent of predicted yeast 		BioGRID(yeast)
gene pairs and 17.5%  Table 6 New gene interactions		BioGRID(yeast)
 that are assigned high probability		BioGRID(yeast)
 by GNE		BioGRID(yeast)
		BioGRID(yeast)
Organism Probability  Gene i Gene j Experimental		BioGRID(yeast)
 evidence code Topology Topology		BioGRID(yeast)
 + Expression		BioGRID(yeast)
		BioGRID(yeast)
Yeast  0.287 0.677 TFC8 DHH1 Affinity		BioGRID(yeast)
 Capture-RNA [35		BioGRID(yeast)
]  0.394 0.730 SYH1 DHH1 Affinity		BioGRID(yeast)
 Capture-RNA [35		BioGRID(yeast)
]  0.413 0.746 CPR7 DHH1 Affinity		BioGRID(yeast)
 Capture-RNA [35		BioGRID(yeast)
]  0.253 0.551 MRP10 DHH1 Affinity		BioGRID(yeast)
 Capture-RNA [35		BioGRID(yeast)
]  0.542 0.835 RPS13 ULP2 Affinity		BioGRID(yeast)
 Capture-MS [36		BioGRID(yeast)
]  E. coli		BioGRID(yeast)
		BioGRID(yeast)
0.014 0.944 ATPB RFBC Affinity 		BioGRID(yeast)
Capture-MS [37]  0.012 0.941 NARQ CYDB Affinity		BioGRID(yeast)
 Capture-MS [37		BioGRID(yeast)
]  0.013 0.937 PCNB PAND Affinity		BioGRID(yeast)
 Capture-MS [37		BioGRID(yeast)
]  0.015 0.939 FLIF CHEY Affinity		BioGRID(yeast)
 Capture-MS [37		BioGRID(yeast)
]  0.017 0.938 YCHM PROB Affinity		BioGRID(yeast)
 Capture-MS [37		BioGRID(yeast)
]  New gene interactions on 2018		BioGRID(yeast)
 version that are assigned high		BioGRID(yeast)
 probability by GNE after integration		BioGRID(yeast)
 of expression data. We provide		BioGRID(yeast)
 probability predicted by GNE (with/without		BioGRID(yeast)
 expression data) for new interactions		BioGRID(yeast)
 in the 2018 version and		BioGRID(yeast)
 evidence supporting the existence of		BioGRID(yeast)
 predicted interactions		BioGRID(yeast)
		BioGRID(yeast)
KC et al. BMC Systems 		BioGRID(yeast)
Biology 2019, 13(Suppl 2):38 Page 12 of 14		BioGRID(yeast)
		BioGRID(yeast)
Fig. 7 The percentage of 		BioGRID(yeast)
true interactions from GNE’s predictions 		BioGRID(yeast)
with different probability bins. a 		BioGRID(yeast)
yeast b E. coli. We 		BioGRID(yeast)
divide the gene pairs based 		BioGRID(yeast)
on their predicted probabilities to 		BioGRID(yeast)
different probability ranges (as shown 		BioGRID(yeast)
in the x-axis) and identify 		BioGRID(yeast)
the number of predicted true 		BioGRID(yeast)
interactions in each range. Each 		BioGRID(yeast)
bar indicates the percentage of 		BioGRID(yeast)
true interactions out of predicted 		BioGRID(yeast)
gene pairs in that probability 		BioGRID(yeast)
range  of predicted E. coli gene		BioGRID(yeast)
 pairs with the probability higher		BioGRID(yeast)
 than 0.9 are true interactions		BioGRID(yeast)
. This suggests that gene 		BioGRID(yeast)
pairs with high probability predicted 		BioGRID(yeast)
by GNE are more likely 		BioGRID(yeast)
to be true interactions.  To support our finding that		BioGRID(yeast)
 GNE predicted gene pairs have		BioGRID(yeast)
 high value, we manually check		BioGRID(yeast)
 gene pairs that have high		BioGRID(yeast)
 predicted probability but are missing		BioGRID(yeast)
 from the latest BioGRID release		BioGRID(yeast)
. We find that these 		BioGRID(yeast)
gene pairs interact with the 		BioGRID(yeast)
same set of other genes. 		BioGRID(yeast)
For example, GNE pre- dicts 		BioGRID(yeast)
the interaction between YDR311W and 		BioGRID(yeast)
YGL122C with the probability of 0		BioGRID(yeast)
.968. Mining BioGRID database, we 		BioGRID(yeast)
find that these genes interact 		BioGRID(yeast)
with the same set of 374 genes. Similarly, E. coli genes		BioGRID(yeast)
 DAMX and FLIL with the		BioGRID(yeast)
 predicted probability of 0.998 share		BioGRID(yeast)
 320 interacting genes. In this		BioGRID(yeast)
 way, we identify all interacting		BioGRID(yeast)
 genes shared by each of		BioGRID(yeast)
 the predicted gene pairs in		BioGRID(yeast)
 yeast and E. coli (Additional		BioGRID(yeast)
 file 1: Table S1 and		BioGRID(yeast)
 Additional file 2		BioGRID(yeast)
:  Table S2). Figure 8 shows		BioGRID(yeast)
 the average number of inter		BioGRID(yeast)
- acting genes shared by 		BioGRID(yeast)
a gene pair. In general, 		BioGRID(yeast)
gene pairs with a high 		BioGRID(yeast)
GNE probability tend to have 		BioGRID(yeast)
a large number of interacting 		BioGRID(yeast)
genes. For example, gene pairs 		BioGRID(yeast)
with the probability greater than 0		BioGRID(yeast)
.9 have, on average, 82 		BioGRID(yeast)
common interacting genes for yeast 		BioGRID(yeast)
and 58 for E. coli. 		BioGRID(yeast)
Two sample t-test analysis has 		BioGRID(yeast)
shown that there is a 		BioGRID(yeast)
significant difference in the number 		BioGRID(yeast)
of shared inter- acting genes 		BioGRID(yeast)
with respect to different probability 		BioGRID(yeast)
bins (Table 7).  Moreover, we search the literature		BioGRID(yeast)
 to see if we can		BioGRID(yeast)
 find supporting evidence for predicted		BioGRID(yeast)
 interactions. We find literature evidence		BioGRID(yeast)
 for an interaction between YCL032W		BioGRID(yeast)
 (STE50) and YDL035C (GPR1), which		BioGRID(yeast)
 has the probability of 0.98		BioGRID(yeast)
 predicted by GNE. STE50 is		BioGRID(yeast)
 an adaptor that links G-protein		BioGRID(yeast)
 complex in cell signalling, and		BioGRID(yeast)
 GPR1 is a G- protein		BioGRID(yeast)
 coupled receptor. Both STE50 and		BioGRID(yeast)
 GPR1 share a		BioGRID(yeast)
		BioGRID(yeast)
Fig. 8 The average number 		BioGRID(yeast)
of common interacting genes between 		BioGRID(yeast)
the gene pairs predicted by 		BioGRID(yeast)
GNE. a yeast b E. 		BioGRID(yeast)
coli. We divide gene pairs 		BioGRID(yeast)
into different probability groups based 		BioGRID(yeast)
on predicted probabilities by GNE 		BioGRID(yeast)
and compute the number of 		BioGRID(yeast)
common interacting genes shared by 		BioGRID(yeast)
these gene pairs. We categorize 		BioGRID(yeast)
these gene pairs into different 		BioGRID(yeast)
probability ranges (as shown in 		BioGRID(yeast)
the x-axis). Each bar represents 		BioGRID(yeast)
the average number of common 		BioGRID(yeast)
interacting genes shared by gene 		BioGRID(yeast)
pairs in each probability 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
KC et al. BMC Systems 		BioGRID(yeast)
Biology 2019, 13(Suppl 2):38 Page 13 of 14		BioGRID(yeast)
		BioGRID(yeast)
Table 7 Results of two-sample 		BioGRID(yeast)
t-test  Probability bin for Sample A		BioGRID(yeast)
		BioGRID(yeast)
Probability bin for Sample B  p-value for yeast		BioGRID(yeast)
		BioGRID(yeast)
p-value for E. coli  0.5 - 0.6 0.6		BioGRID(yeast)
 - 0.7 9.2e − 14		BioGRID(yeast)
 6.9e − 02 0.5		BioGRID(yeast)
 - 0.6 0.7 - 0.8		BioGRID(yeast)
 3.02e − 51 1.23e		BioGRID(yeast)
 − 05 0.5 - 0.6		BioGRID(yeast)
 0.8 - 0.9 6.1e		BioGRID(yeast)
 − 117 7.4e − 14		BioGRID(yeast)
 0.5 - 0.6 0.9		BioGRID(yeast)
 - 1.0 2.1e − 177		BioGRID(yeast)
 3.7e − 39 0.6		BioGRID(yeast)
 - 0.7 0.7 - 0.8		BioGRID(yeast)
 8.2e − 17 1.1e		BioGRID(yeast)
 − 02 0.6 - 0.7		BioGRID(yeast)
 0.8 - 0.9 3.5e		BioGRID(yeast)
 − 69 9.2e − 09		BioGRID(yeast)
 0.6 - 0.7 0.9		BioGRID(yeast)
 - 1.0 2.1e − 128		BioGRID(yeast)
 1.9e − 30 0.7		BioGRID(yeast)
 - 0.8 0.8 - 0.9		BioGRID(yeast)
 4.7e − 28 4.8e		BioGRID(yeast)
 − 04 0.7 - 0.8		BioGRID(yeast)
 0.9 - 1.0 6.2e		BioGRID(yeast)
 − 87 7.4e − 23		BioGRID(yeast)
 0.8 - 0.9 0.9		BioGRID(yeast)
 - 1.0 4.3e − 35		BioGRID(yeast)
 5.1e − 13 We divide		BioGRID(yeast)
 gene pairs into different probability		BioGRID(yeast)
 groups based on predicted probabilities		BioGRID(yeast)
 by GNE and compute the		BioGRID(yeast)
 number of common interacting genes		BioGRID(yeast)
 shared by these gene pairs		BioGRID(yeast)
. Significance test shows there 		BioGRID(yeast)
is the significant difference between 		BioGRID(yeast)
average number of shared genes 		BioGRID(yeast)
in different probability bins  common function of cell signalling		BioGRID(yeast)
 via G-protein. Besides, STE50p interacts		BioGRID(yeast)
 with STE11p in the two-hybrid		BioGRID(yeast)
 system, which is a cell-based		BioGRID(yeast)
 system examining protein-protein interactions [34		BioGRID(yeast)
]. Also, BioGRID has evidence 		BioGRID(yeast)
of 30 physi- cal and 4 genetic associations between STE50 and		BioGRID(yeast)
 STE11. Thus, STE50 is highly		BioGRID(yeast)
 likely to interact with STE11		BioGRID(yeast)
, which in turn interacts 		BioGRID(yeast)
with GPR1.  This analysis demonstrates the potential		BioGRID(yeast)
 of our method in the		BioGRID(yeast)
 discovery of gene interactions. Also		BioGRID(yeast)
, GNE can help the 		BioGRID(yeast)
curator to identify interactions with 		BioGRID(yeast)
strong potential that need to 		BioGRID(yeast)
be looked at with experimental 		BioGRID(yeast)
validation or within the literature.  Conclusion We developed a novel		BioGRID(yeast)
 deep learning framework, namely GNE		BioGRID(yeast)
 to perform gene network embedding		BioGRID(yeast)
. Specifi- cally, we design 		BioGRID(yeast)
deep neural network architecture to 		BioGRID(yeast)
model the complex statistical relationships 		BioGRID(yeast)
between gene interaction network and 		BioGRID(yeast)
expression data. GNE is flexible 		BioGRID(yeast)
to the addition of different 		BioGRID(yeast)
types and num- ber of 		BioGRID(yeast)
attributes. The features learned by 		BioGRID(yeast)
GNE allow us to use 		BioGRID(yeast)
out-of-the-box machine learning classifiers like 		BioGRID(yeast)
Logistic Regression to predict gene 		BioGRID(yeast)
interactions accurately.  GNE relies on a deep		BioGRID(yeast)
 learning technique that can learn		BioGRID(yeast)
 the underlying patterns of gene		BioGRID(yeast)
 interactions by integrat- ing heterogeneous		BioGRID(yeast)
 data and extracts features that		BioGRID(yeast)
 are more informative for interaction		BioGRID(yeast)
 prediction. Experimental results show that		BioGRID(yeast)
 GNE achieve better performance in		BioGRID(yeast)
 gene interaction prediction over other		BioGRID(yeast)
 baseline approaches in both yeast		BioGRID(yeast)
 and E. coli organisms. Also		BioGRID(yeast)
, GNE can help the 		BioGRID(yeast)
curator to identify the interactions 		BioGRID(yeast)
that need to be looked 		BioGRID(yeast)
at.  As future work, we aim		BioGRID(yeast)
 to study the impact of		BioGRID(yeast)
 inte- grating other sources of		BioGRID(yeast)
 information about gene such as		BioGRID(yeast)
 transcription factor binding sites, functional		BioGRID(yeast)
 annota- tions (from gene ontology		BioGRID(yeast)
), gene sequences, metabolic pathways, 		BioGRID(yeast)
etc. into GNE in predicting 		BioGRID(yeast)
gene interaction.  Additional files		BioGRID(yeast)
		BioGRID(yeast)
Additional file 1: Table S1. 		BioGRID(yeast)
Includes yeast gene pairs predicted 		BioGRID(yeast)
by GNE with probabilities of 0		BioGRID(yeast)
.5 or higher. (XLSX 109 		BioGRID(yeast)
kb)  Additional file 2: Table S2		BioGRID(yeast)
. Includes E. coli gene 		BioGRID(yeast)
pairs predicted by GNE with 		BioGRID(yeast)
probabilities of 0.5 or higher. 		BioGRID(yeast)
Rows marked with yellow color 		BioGRID(yeast)
indicate predicted interaction is true 		BioGRID(yeast)
based on latest version 3.4.162 		BioGRID(yeast)
of BioGRID interaction dataset released 		BioGRID(yeast)
on June 2018. (XLSX 43.7 		BioGRID(yeast)
kb)  Abbreviations AUPR: Area under the		BioGRID(yeast)
 PR curve; AUROC: Area under		BioGRID(yeast)
 the ROC curve; DOOR: Database		BioGRID(yeast)
 of Prokaryotic Operons; DREAM: Dialogue		BioGRID(yeast)
 on reverse Engineering assessment and		BioGRID(yeast)
 methods; ELU: Exponential linear unit		BioGRID(yeast)
; GI: Genetic interaction; GNE: 		BioGRID(yeast)
Gene network embedding; ID: Identifier; 		BioGRID(yeast)
KS: Kolmogrov-Smirnov; LINE: Large-scale information 		BioGRID(yeast)
network embedding; PR: Precision recall; 		BioGRID(yeast)
ROC: Receiver operator characterstic; t-SNE: 		BioGRID(yeast)
t-Distributed stochastic neighbor embedding  Acknowledgements Not applicable		BioGRID(yeast)
.  Funding This work was supported		BioGRID(yeast)
 by the National Science Foundation		BioGRID(yeast)
 [1062422 to A.H.] and the		BioGRID(yeast)
 National Institutes of Health [R15GM116102		BioGRID(yeast)
 to F.C.]. Publication of this		BioGRID(yeast)
 article was sponsored by NSF		BioGRID(yeast)
 grant (1062422		BioGRID(yeast)
).  Availability of data and materials		BioGRID(yeast)
 The datasets analysed during the		BioGRID(yeast)
 current study are publicly available		BioGRID(yeast)
. The gene expression data 		BioGRID(yeast)
was downloaded from DREAM5 Network 		BioGRID(yeast)
Challenge https:// www.synapse.org/#!Synapse:syn2787209/wiki/70351. The interaction 		BioGRID(yeast)
datasets for yeast and E. 		BioGRID(yeast)
coli were downloaded from BioGRID 		BioGRID(yeast)
https://thebiogrid.org.  About this supplement This article		BioGRID(yeast)
 has been published as part		BioGRID(yeast)
 of BMC Systems Biology Volume		BioGRID(yeast)
 13 Supplement 2, 2019: Selected		BioGRID(yeast)
 articles from the 17th Asia		BioGRID(yeast)
 Pacific Bioinformatics Conference (APBC 2019		BioGRID(yeast)
): systems biology. The full 		BioGRID(yeast)
contents of the supplement are 		BioGRID(yeast)
available online at https://bmcsystbiol.biomedcentral.com/articles/ supplements/volume-13-supplement-2.  Authors’ contributions KK, RL, FC		BioGRID(yeast)
, and AH designed the 		BioGRID(yeast)
research. KK performed the research 		BioGRID(yeast)
and wrote the manuscript. RL, 		BioGRID(yeast)
FC, QY, and AH improved 		BioGRID(yeast)
the draft. All authors read 		BioGRID(yeast)
and approved the final manuscript.  Ethics approval and consent to		BioGRID(yeast)
 participate Not applicable		BioGRID(yeast)
.  Consent for publication Not applicable		BioGRID(yeast)
.  Competing interests The authors declare		BioGRID(yeast)
 that they have no competing		BioGRID(yeast)
 interests		BioGRID(yeast)
.  Publisher’s Note Springer Nature remains		BioGRID(yeast)
 neutral with regard to jurisdictional		BioGRID(yeast)
 claims in published maps and		BioGRID(yeast)
 institutional affiliations		BioGRID(yeast)
.  Author details 1Golisano College of		BioGRID(yeast)
 Computing and Information Sciences, Rochester		BioGRID(yeast)
 Institute of Technology, 20 Lomb		BioGRID(yeast)
 Memorial Drive, 14623 Rochester, New		BioGRID(yeast)
 York, USA. 2Thomas H. Gosnell		BioGRID(yeast)
 School of Life Sciences, Rochester		BioGRID(yeast)
 Institute of Technology, 84 Lomb		BioGRID(yeast)
 Memorial Drive, 14623 Rochester, New		BioGRID(yeast)
 York, USA		BioGRID(yeast)
.  Published: 5 April 2019		BioGRID(yeast)
		BioGRID(yeast)
https://doi.org/10.1186/s12918-019-0694-y https://doi.org/10.1186/s12918-019-0694-y https://www.synapse.org/#!Synapse:syn2787209/wiki/70351 https://www.synapse.org/#!Synapse:syn2787209/wiki/70351 https://thebiogrid.org 		BioGRID(yeast)
https://bmcsystbiol.biomedcentral.com/articles/supplements/volume-13-supplement-2 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
13		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
2		BioGRID(yeast)
		BioGRID(yeast)
KC et al. BMC Systems 		BioGRID(yeast)
Biology 2019, 13(Suppl 2):38 Page 14 of 14		BioGRID(yeast)
		BioGRID(yeast)
References 1. Mani R, Onge 		BioGRID(yeast)
RPS, Hartman JL, Giaever G, 		BioGRID(yeast)
Roth FP. Defining genetic  interaction. Proc Natl Acad Sci		BioGRID(yeast)
. 2008;105(9):3461–6. 2. Boucher B, 		BioGRID(yeast)
Jenna S. Genetic interaction networks: 		BioGRID(yeast)
better understand to  better predict. Front Genet. 2013;4:290		BioGRID(yeast)
. 3. Lage K. Protein–protein 		BioGRID(yeast)
interactions and genetic diseases: the  interactome. Biochim Biophys Acta (BBA		BioGRID(yeast)
) - Mol Basis Dis. 2014		BioGRID(yeast)
;1842(10): 1971–80.  4. Madhukar NS, Elemento O		BioGRID(yeast)
, Pandey G. Prediction of 		BioGRID(yeast)
genetic interactions using machine learning 		BioGRID(yeast)
and network properties. Front Bioeng 		BioGRID(yeast)
Biotechnol. 2015;3:172.  5. Oliver S. Proteomics: guilt-by-association		BioGRID(yeast)
 goes global. Nature. 2000;403(6770):601		BioGRID(yeast)
.  6. Cho H, Berger B		BioGRID(yeast)
, Peng J. Compact integration 		BioGRID(yeast)
of multi-network topology for functional 		BioGRID(yeast)
analysis of genes. Cell Syst. 2016		BioGRID(yeast)
;3(6):540–8.  7. Marbach D, Costello JC		BioGRID(yeast)
, Küffner R, Vega NM, 		BioGRID(yeast)
Prill RJ, Camacho DM, Allison 		BioGRID(yeast)
KR, Aderhold A, Bonneau R, 		BioGRID(yeast)
Chen Y, et al. Wisdom 		BioGRID(yeast)
of crowds for robust gene 		BioGRID(yeast)
network inference. Nat Methods. 2012;9(8):796.  8. Li R, KC K		BioGRID(yeast)
, Cui F, Haake AR. 		BioGRID(yeast)
Sparse covariance modeling in high 		BioGRID(yeast)
dimensions with gaussian processes. In: 		BioGRID(yeast)
Proceedings of The 32nd Conference 		BioGRID(yeast)
on Neural Information Processing Systems (		BioGRID(yeast)
NIPS). 2018.  9. Cui P, Wang X		BioGRID(yeast)
, Pei J, Zhu W. 		BioGRID(yeast)
A survey on network embedding. 		BioGRID(yeast)
IEEE Trans Knowl Data Eng. 2018		BioGRID(yeast)
. arXiv preprint arXiv:1711.08752. IEEE.  10. Lei Y-K, You Z-H		BioGRID(yeast)
, Ji Z, Zhu L, 		BioGRID(yeast)
Huang D-S. Assessing and predicting 		BioGRID(yeast)
protein interactions by combining manifold 		BioGRID(yeast)
embedding with multiple information integration. 		BioGRID(yeast)
In: BMC Bioinformatics, vol. 13. 		BioGRID(yeast)
BioMed Central; 2012. p. 3.  11. Alanis-Lobato G, Cannistraci CV		BioGRID(yeast)
, Ravasi T. Exploitation of 		BioGRID(yeast)
genetic interaction network topology for 		BioGRID(yeast)
the prediction of epistatic behavior. 		BioGRID(yeast)
Genomics. 2013;102(4):202–8.  12. Tenenbaum JB, De Silva		BioGRID(yeast)
 V, Langford JC. A global		BioGRID(yeast)
 geometric framework for nonlinear dimensionality		BioGRID(yeast)
 reduction. science. 2000;290(5500):2319–23		BioGRID(yeast)
.  13. Mikolov T, Sutskever I		BioGRID(yeast)
, Chen K, Corrado GS, 		BioGRID(yeast)
Dean J. Distributed representations of 		BioGRID(yeast)
words and phrases and their 		BioGRID(yeast)
compositionality. In: Advances in Neural 		BioGRID(yeast)
Information Processing Systems. 2013. p. 3111		BioGRID(yeast)
–3119.  14. Grover A, Leskovec J		BioGRID(yeast)
. node2vec: Scalable feature learning 		BioGRID(yeast)
for networks. In: Proceedings of 		BioGRID(yeast)
the 22nd ACM SIGKDD International 		BioGRID(yeast)
Conference on Knowledge Discovery and 		BioGRID(yeast)
Data Mining. ACM; 2016. p. 855		BioGRID(yeast)
–864.  15. Tu Y, Stolovitzky G		BioGRID(yeast)
, Klein U. Quantitative noise 		BioGRID(yeast)
analysis for gene expression microarray 		BioGRID(yeast)
experiments. Proc Natl Acad Sci. 2002		BioGRID(yeast)
;99(22): 14031–6.  16. Tang J, Qu M		BioGRID(yeast)
, Wang M, Zhang M, 		BioGRID(yeast)
Yan J, Mei Q. Line: 		BioGRID(yeast)
Large-scale information network embedding. In: 		BioGRID(yeast)
Proceedings of the 24th International 		BioGRID(yeast)
Conference on World Wide Web. 		BioGRID(yeast)
International World Wide Web Conferences 		BioGRID(yeast)
Steering Committee; 2015. p. 1067–1077.  17. Snoek CG, Worring M		BioGRID(yeast)
, Smeulders AW. Early versus 		BioGRID(yeast)
late fusion in semantic video 		BioGRID(yeast)
analysis. In: Proceedings of the 		BioGRID(yeast)
13th Annual ACM International Conference 		BioGRID(yeast)
on Multimedia. 2005. p. 399–402. 		BioGRID(yeast)
ACM.  18. He K, Zhang X		BioGRID(yeast)
, Ren S, Sun J. 		BioGRID(yeast)
Deep residual learning for image 		BioGRID(yeast)
recognition. In: Proceedings of the 		BioGRID(yeast)
IEEE Conference on Computer Vision 		BioGRID(yeast)
and Pattern Recognition. 2016. p. 770		BioGRID(yeast)
–778.  19. Pennington J, Socher R		BioGRID(yeast)
, Manning C. Glove: Global 		BioGRID(yeast)
vectors for word representation. In: 		BioGRID(yeast)
Proceedings of the 2014 Conference 		BioGRID(yeast)
on Empirical Methods in Natural 		BioGRID(yeast)
Language Processing (EMNLP). 2014. p. 1532		BioGRID(yeast)
–1543.  20. Levy O, Goldberg Y		BioGRID(yeast)
, Dagan I. Improving distributional 		BioGRID(yeast)
similarity with lessons learned from 		BioGRID(yeast)
word embeddings. Trans Assoc Comput 		BioGRID(yeast)
Linguist. 2015;3:211–25.  21. Kingma DP, Ba J		BioGRID(yeast)
. Adam: A method for 		BioGRID(yeast)
stochastic optimization. In: International Conference 		BioGRID(yeast)
on Learning Representations; 2015.  22. Duchi J, Hazan E		BioGRID(yeast)
, Singer Y. Adaptive subgradient 		BioGRID(yeast)
methods for online learning and 		BioGRID(yeast)
stochastic optimization. J Mach Learn 		BioGRID(yeast)
Res. 2011;12(Jul): 2121–59.  23. Tieleman T, Hinton G		BioGRID(yeast)
. Lecture 6.5-rmsprop, coursera: Neural 		BioGRID(yeast)
networks for machine learning. University 		BioGRID(yeast)
of Toronto, Technical Report. 2012.  24. Srivastava N, Hinton G		BioGRID(yeast)
, Krizhevsky A, Sutskever I, 		BioGRID(yeast)
Salakhutdinov R. Dropout: A simple 		BioGRID(yeast)
way to prevent neural networks 		BioGRID(yeast)
from overfitting. J Mach Learn 		BioGRID(yeast)
Res. 2014;15(1):1929–58.  25. Ioffe S, Szegedy C		BioGRID(yeast)
. Batch normalization: Accelerating deep 		BioGRID(yeast)
network training by reducing internal 		BioGRID(yeast)
covariate shift. In: International Conference 		BioGRID(yeast)
on Machine Learning; 2015. p. 448		BioGRID(yeast)
–56.  26. Stark C, Breitkreutz B-J		BioGRID(yeast)
, Reguly T, Boucher L, 		BioGRID(yeast)
Breitkreutz A, Tyers M. Biogrid: 		BioGRID(yeast)
a general repository for interaction 		BioGRID(yeast)
datasets. Nucleic Acids Res. 2006;34(suppl_1):535–9.  27. Butte A-J, Kohane I-S		BioGRID(yeast)
. Mutual information relevance networks: 		BioGRID(yeast)
functional genomic clustering using pairwise 		BioGRID(yeast)
entropy measurements. In: Biocomputing 2000. 		BioGRID(yeast)
World Scientific; 1999. p. 418–429.  28. Chua HN, Sung W-K		BioGRID(yeast)
, Wong L. Exploiting indirect 		BioGRID(yeast)
neighbours and topological weight to 		BioGRID(yeast)
predict protein function from protein–protein 		BioGRID(yeast)
interactions. Bioinformatics. 2006;22(13):1623–30.  29. Abadi M, Agarwal A		BioGRID(yeast)
, Barham P, Brevdo E, 		BioGRID(yeast)
Chen Z, Citro C, Corrado 		BioGRID(yeast)
GS, Davis A, Dean J, 		BioGRID(yeast)
Devin M, Ghemawat S, Goodfellow 		BioGRID(yeast)
I, Harp A, Irving G, 		BioGRID(yeast)
Isard M, Jia Y, Jozefowicz 		BioGRID(yeast)
R, Kaiser L, Kudlur M, 		BioGRID(yeast)
Levenberg J, Mané D., Monga 		BioGRID(yeast)
R, Moore S, Murray D, 		BioGRID(yeast)
Olah C, Schuster M, Shlens 		BioGRID(yeast)
J, Steiner B, Sutskever I, 		BioGRID(yeast)
Talwar K, Tucker P, Vanhoucke 		BioGRID(yeast)
V, Vasudevan V, Viégas F., 		BioGRID(yeast)
Vinyals O, Warden P, Wattenberg 		BioGRID(yeast)
M, Wicke M, Yu Y, 		BioGRID(yeast)
Zheng X. TensorFlow: Large-Scale Machine 		BioGRID(yeast)
Learning on Heterogeneous Systems. Software 		BioGRID(yeast)
available from tensorflow.org. 2015. https://www.tensorflow.org/ 		BioGRID(yeast)
Accessed 21 Dec 2016.  30. Clevert D, Unterthiner T		BioGRID(yeast)
, Hochreiter S. Fast and 		BioGRID(yeast)
accurate deep network learning by 		BioGRID(yeast)
exponential linear units (elus). In: 		BioGRID(yeast)
International Conference on Learning Representations; 2016		BioGRID(yeast)
.  31. Davis J, Goadrich M		BioGRID(yeast)
. The relationship between precision-recall 		BioGRID(yeast)
and roc curves. In: Proceedings 		BioGRID(yeast)
of the 23rd International Conference 		BioGRID(yeast)
on Machine Learning. ACM; 2006. 		BioGRID(yeast)
p. 233–240.  32. Maaten L. v. d		BioGRID(yeast)
., Hinton G. Visualizing data 		BioGRID(yeast)
using t-sne. J Mach Learn 		BioGRID(yeast)
Res. 2008;9(Nov):2579–605.  33. Mao F, Dam P		BioGRID(yeast)
, Chou J, Olman V, 		BioGRID(yeast)
Xu Y. Door: a database 		BioGRID(yeast)
for prokaryotic operons. Nucleic Acids 		BioGRID(yeast)
Res. 2008;37(suppl_1):459–63.  34. Gustin MC, Albertyn J		BioGRID(yeast)
, Alexander M, Davenport K. 		BioGRID(yeast)
Map kinase pathways in the 		BioGRID(yeast)
yeastsaccharomyces cerevisiae. Microbiol Mol Biol 		BioGRID(yeast)
Rev. 1998;62(4): 1264–300.  35. Miller JE, Zhang L		BioGRID(yeast)
, Jiang H, Li Y, 		BioGRID(yeast)
Pugh BF, Reese JC. Genome-wide 		BioGRID(yeast)
mapping of decay factor–mrna interactions 		BioGRID(yeast)
in yeast identifies nutrient-responsive transcripts 		BioGRID(yeast)
as targets of the deadenylase 		BioGRID(yeast)
ccr4. G3: Genes, Genomes, Genet. 2018		BioGRID(yeast)
;8(1):315–30.  36. Liang J, Singh N		BioGRID(yeast)
, Carlson CR, Albuquerque CP, 		BioGRID(yeast)
Corbett KD, Zhou H. Recruitment 		BioGRID(yeast)
of a sumo isopeptidase to 		BioGRID(yeast)
rdna stabilizes silencing complexes by 		BioGRID(yeast)
opposing sumo targeted ubiquitin ligase 		BioGRID(yeast)
activity. Genes Dev. 2017;31(8):802–15.  37. Babu M, Bundalovic-Torma C		BioGRID(yeast)
, Calmettes C, Phanse S, 		BioGRID(yeast)
Zhang Q, Jiang Y, Minic 		BioGRID(yeast)
Z, Kim S, Mehla J, 		BioGRID(yeast)
Gagarinova A, et al. Global 		BioGRID(yeast)
landscape of cell envelope protein 		BioGRID(yeast)
complexes in escherichia coli. Nat 		BioGRID(yeast)
Biotechnol. 2018;36(1):103.  https://www.tensorflow.org		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
Gene network embedding (GNE) 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
Gene network structure 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
Gene expression 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
GNE 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
Parameter 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
Experimental 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
Results and 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
Analysis of gene 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
Gene interaction 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
Impact of network 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
Impact 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
Investigation of GNE's 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
Additional 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
Additional file 1		BioGRID(yeast)
		BioGRID(yeast)
Additional file 2		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
Availability of data and 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
About this 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
Authors' 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
Ethics approval and consent to 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
Consent for 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
Competing 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
Publisher's 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
Author 		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
		BioGRID(yeast)
0.243 (AUROC), 0.242 (AUPR) on yeast and 0.403 (AUROC), 0.382 (AUPR	yeast	BioGRID(yeast)
comparison to correlation-based methods on yeast because of its limitation on	yeast	BioGRID(yeast)
integration of expression data on yeast	yeast	BioGRID(yeast)
inter- actions and attributes of yeast shows a significant gain of	yeast	BioGRID(yeast)
improvement on E. coli than yeast when we train with 10	yeast	BioGRID(yeast)
number of available interactions for yeast and E. coli (Table 2	yeast	BioGRID(yeast)
are less than 40% for yeast and about 0.32 when interactions	yeast	BioGRID(yeast)
and attributes are required for yeast than E. coli. It may	yeast	BioGRID(yeast)
related to the fact that yeast is a more complex species	yeast	BioGRID(yeast)
respect to network sparsity. a yeast b E. coli. Integration of	yeast	BioGRID(yeast)
different percentages of interactions. a yeast b E. coli. Different lines	yeast	BioGRID(yeast)
and AUPR (c, d) for yeast and E. coli. The limit	yeast	BioGRID(yeast)
contains 12,835 new interactions for yeast and 11,185 new interactions for	yeast	BioGRID(yeast)
interactions are comparable for both yeast and E. coli (Figs. 4	yeast	BioGRID(yeast)
temporal holdout validation across both yeast and E. coli datasets, indicating	yeast	BioGRID(yeast)
7.0 (AUROC), 7.4 (AUPR) on yeast and 6.6 (AUROC), 5.9 (AUPR	yeast	BioGRID(yeast)
in predicted probability for both yeast and E. coli after expression	yeast	BioGRID(yeast)
out of 12,835) interactions for yeast (improving AUROC from 0.685 to	yeast	BioGRID(yeast)
version 3.4.162) and evaluate 2609 yeast gene pairs (Additional file 1	yeast	BioGRID(yeast)
We find that 128 (5%) yeast gene pairs and 78 (9	yeast	BioGRID(yeast)
7). Sixteen percent of predicted yeast gene pairs and 17.5	yeast	BioGRID(yeast)
with different probability bins. a yeast b E. coli. We divide	yeast	BioGRID(yeast)
the predicted gene pairs in yeast and E. coli (Additional file	yeast	BioGRID(yeast)
82 common interacting genes for yeast and 58 for E. coli	yeast	BioGRID(yeast)
pairs predicted by GNE. a yeast b E. coli. We divide	yeast	BioGRID(yeast)
p-value for yeast	yeast	BioGRID(yeast)
other baseline approaches in both yeast and E. coli organisms. Also	yeast	BioGRID(yeast)
file 1: Table S1. Includes yeast gene pairs predicted by GNE	yeast	BioGRID(yeast)
www.synapse.org/#!Synapse:syn2787209/wiki/70351. The interaction datasets for yeast and E. coli were downloaded	yeast	BioGRID(yeast)
of decay factor–mrna interactions in yeast identifies nutrient-responsive transcripts as targets	yeast	BioGRID(yeast)
interaction network data from the BioGRID database [26] and gene expression	BioGRID	BioGRID(yeast)
use two interaction datasets from BioGRID database (2017 released version 3.4.153	BioGRID	BioGRID(yeast)
of the interaction datasets from BioGRID and the gene expression data	BioGRID	BioGRID(yeast)
two versions of BioGRID interaction datasets at two dif	BioGRID	BioGRID(yeast)
from the 2018 version of BioGRID as positive interactions and the	BioGRID	BioGRID(yeast)
but are miss- ing from BioGRID for further investigation as we	BioGRID	BioGRID(yeast)
evidence code obtained from the BioGRID database [26] sup- porting these	BioGRID	BioGRID(yeast)
predictions. BioGRID compiles interaction data from numerous	BioGRID	BioGRID(yeast)
Taking new interactions added to BioGRID (version 3.4.158) into consideration, we	BioGRID	BioGRID(yeast)
consider the new version of BioGRID (version 3.4.162) and evaluate 2609	BioGRID	BioGRID(yeast)
the lat- est release of BioGRID	BioGRID	BioGRID(yeast)
are missing from the latest BioGRID release. We find that these	BioGRID	BioGRID(yeast)
the probability of 0.968. Mining BioGRID database, we find that these	BioGRID	BioGRID(yeast)
examining protein-protein interactions [34]. Also, BioGRID has evidence of 30 physi	BioGRID	BioGRID(yeast)
on latest version 3.4.162 of BioGRID interaction dataset released on June	BioGRID	BioGRID(yeast)
E. coli were downloaded from BioGRID https://thebiogrid.org	BioGRID	BioGRID(yeast)
GNE: a deep learning framework 		BioGRID (human)
for gene network inference by 		BioGRID (human)
aggregating biological 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
KC et al. BMC Systems 		BioGRID (human)
Biology 2019, 13(Suppl 2):38 https://doi.org/10.1186/s12918-019-0694-y  RESEARCH Open Access		BioGRID (human)
		BioGRID (human)
GNE: a deep learning framework 		BioGRID (human)
for gene network inference by 		BioGRID (human)
aggregating biological information Kishan KC1*, 		BioGRID (human)
Rui Li1, Feng Cui2, Qi 		BioGRID (human)
Yu1 and Anne R. Haake1  From The 17th Asia Pacific		BioGRID (human)
 Bioinformatics Conference (APBC 2019) Wuhan		BioGRID (human)
, China. 14–16 January 2019  Abstract		BioGRID (human)
		BioGRID (human)
Background: The topological landscape of 		BioGRID (human)
gene interaction networks provides a 		BioGRID (human)
rich source of information for 		BioGRID (human)
inferring functional patterns of genes 		BioGRID (human)
or proteins. However, it is 		BioGRID (human)
still a challenging task to 		BioGRID (human)
aggregate heterogeneous biological information such 		BioGRID (human)
as gene expression and gene 		BioGRID (human)
interactions to achieve more accurate 		BioGRID (human)
inference for prediction and discovery 		BioGRID (human)
of new gene interactions. In 		BioGRID (human)
particular, how to generate a 		BioGRID (human)
unified vector representation to integrate 		BioGRID (human)
diverse input data is a 		BioGRID (human)
key challenge addressed here.  Results: We propose a scalable		BioGRID (human)
 and robust deep learning framework		BioGRID (human)
 to learn embedded representations to		BioGRID (human)
 unify known gene interactions and		BioGRID (human)
 gene expression for gene interaction		BioGRID (human)
 predictions. These low- dimensional embeddings		BioGRID (human)
 derive deeper insights into the		BioGRID (human)
 structure of rapidly accumulating and		BioGRID (human)
 diverse gene interaction networks and		BioGRID (human)
 greatly simplify downstream modeling. We		BioGRID (human)
 compare the predictive power of		BioGRID (human)
 our deep embeddings to the		BioGRID (human)
 strong baselines. The results suggest		BioGRID (human)
 that our deep embeddings achieve		BioGRID (human)
 significantly more accurate predictions. Moreover		BioGRID (human)
, a set of novel 		BioGRID (human)
gene interaction predictions are validated 		BioGRID (human)
by up-to-date literature-based database entries.  Conclusion: The proposed model demonstrates		BioGRID (human)
 the importance of integrating heterogeneous		BioGRID (human)
 information about genes for gene		BioGRID (human)
 network inference. GNE is freely		BioGRID (human)
 available under the GNU General		BioGRID (human)
 Public License and can be		BioGRID (human)
 downloaded from GitHub (https://github.com/kckishan/GNE		BioGRID (human)
).  Keywords: Gene interaction networks, Gene		BioGRID (human)
 expression, Network embedding, Heterogeneous data		BioGRID (human)
 integration, Deep learning		BioGRID (human)
		BioGRID (human)
Background A comprehensive study of 		BioGRID (human)
gene interactions (GIs) provides means 		BioGRID (human)
to identify the functional relationship 		BioGRID (human)
between genes and their corresponding 		BioGRID (human)
products, as well as insights 		BioGRID (human)
into underlying biological phenomena that 		BioGRID (human)
are critical to understanding phenotypes 		BioGRID (human)
in health and dis- ease 		BioGRID (human)
conditions [1–3]. Since advancements in 		BioGRID (human)
measure- ment technologies have led 		BioGRID (human)
to numerous high-throughput datasets, there 		BioGRID (human)
is a great value in 		BioGRID (human)
developing efficient com- putational methods 		BioGRID (human)
capable of automatically extracting  *Correspondence: kk3671@rit.edu 1Golisano College of		BioGRID (human)
 Computing and Information Sciences, Rochester		BioGRID (human)
 Institute of Technology, 20 Lomb		BioGRID (human)
 Memorial Drive, 14623 Rochester, New		BioGRID (human)
 York, USA Full list of		BioGRID (human)
 author information is available at		BioGRID (human)
 the end of the article		BioGRID (human)
		BioGRID (human)
and aggregating meaningful information from 		BioGRID (human)
heteroge- neous datasets to infer 		BioGRID (human)
gene interactions.  Although a wide variety of		BioGRID (human)
 machine learning models have been		BioGRID (human)
 developed to analyze high-throughput datasets		BioGRID (human)
 for GI prediction [4], there		BioGRID (human)
 are still some major chal		BioGRID (human)
- lenges, such as efficient 		BioGRID (human)
analysis of large heterogeneous datasets, 		BioGRID (human)
integration of biological information, and 		BioGRID (human)
effec- tive feature engineering. To 		BioGRID (human)
address these challenges, we propose 		BioGRID (human)
a novel deep learning framework 		BioGRID (human)
to integrate diverse biological information 		BioGRID (human)
for GI network inference.  Our proposed method frames GI		BioGRID (human)
 network inference as a problem		BioGRID (human)
 of network embedding. In particular		BioGRID (human)
, we rep- resent gene 		BioGRID (human)
interactions as a network of 		BioGRID (human)
genes and their interactions and 		BioGRID (human)
create a deep learning framework 		BioGRID (human)
to automatically learn an informative 		BioGRID (human)
representation which  © The Author(s). 2019 Open		BioGRID (human)
 Access This article is distributed		BioGRID (human)
 under the terms of the		BioGRID (human)
 Creative Commons Attribution 4.0 International		BioGRID (human)
 License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted		BioGRID (human)
 use, distribution, and reproduction in		BioGRID (human)
 any medium, provided you give		BioGRID (human)
 appropriate credit to the original		BioGRID (human)
 author(s) and the source, provide		BioGRID (human)
 a link to the Creative		BioGRID (human)
 Commons license, and indicate if		BioGRID (human)
 changes were made. The Creative		BioGRID (human)
 Commons Public Domain Dedication waiver		BioGRID (human)
 (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data		BioGRID (human)
 made available in this article		BioGRID (human)
, unless otherwise stated.  http://crossmark.crossref.org/dialog/?doi=10.1186/s12918-019-0694-y&domain=pdf https://github.com/kckishan/GNE mailto: kk3671@rit.edu http://creativecommons.org/licenses/by/4.0		BioGRID (human)
/ 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
1		BioGRID (human)
		BioGRID (human)
0		BioGRID (human)
		BioGRID (human)
KC et al. BMC Systems 		BioGRID (human)
Biology 2019, 13(Suppl 2):38 Page 2 of 14		BioGRID (human)
		BioGRID (human)
integrates both the topological property 		BioGRID (human)
and the gene expression property. 		BioGRID (human)
A key insight behind our 		BioGRID (human)
gene net- work embedding method 		BioGRID (human)
is the “guilt by association” 		BioGRID (human)
assumption [5], that is, genes 		BioGRID (human)
that are co-localized or have 		BioGRID (human)
similar topological roles in the 		BioGRID (human)
interaction net- work are likely 		BioGRID (human)
to be functionally correlated. This 		BioGRID (human)
insight not only allows us 		BioGRID (human)
to discover similar genes and 		BioGRID (human)
pro- teins but also to 		BioGRID (human)
infer the properties of unknown 		BioGRID (human)
ones. Our network embedding generates 		BioGRID (human)
a lower-dimensional vector representation of 		BioGRID (human)
the gene topological character- istics. 		BioGRID (human)
The relationships between genes including 		BioGRID (human)
higher- order topological properties are 		BioGRID (human)
captured by the distances between 		BioGRID (human)
genes in the embedding space. 		BioGRID (human)
The new low- dimensional representation 		BioGRID (human)
of a GI network can 		BioGRID (human)
be used for various downstream 		BioGRID (human)
tasks, such as gene function 		BioGRID (human)
pre- diction, gene interaction prediction, 		BioGRID (human)
and gene ontology reconstruction [6].  Furthermore, since the network embedding		BioGRID (human)
 method can only preserve the		BioGRID (human)
 topological properties of a GI		BioGRID (human)
 network, and fails to generalize		BioGRID (human)
 for genes with no interaction		BioGRID (human)
 infor- mation, our scalable deep		BioGRID (human)
 learning method also integrates heterogeneous		BioGRID (human)
 gene information, such as expression		BioGRID (human)
 data from high throughput technologies		BioGRID (human)
, into the GI net- 		BioGRID (human)
work inference. Our method projects 		BioGRID (human)
genes with similar attributes closer 		BioGRID (human)
to each other in the 		BioGRID (human)
embedding space, even if they 		BioGRID (human)
may not have similar topological 		BioGRID (human)
properties. The results show that 		BioGRID (human)
by integrating additional gene infor- 		BioGRID (human)
mation in the network embedding 		BioGRID (human)
process, the prediction performance is 		BioGRID (human)
improved significantly.  GI prediction is a long-standing		BioGRID (human)
 problem. The pro- posed machine		BioGRID (human)
 learning methods include statistical corre		BioGRID (human)
- lation, mutual information [7], 		BioGRID (human)
dimensionality reduction [8], and network-based 		BioGRID (human)
methods (e.g. common neighbor- hood, 		BioGRID (human)
network embedding) [4, 9]. Among 		BioGRID (human)
these methods, some methods such 		BioGRID (human)
as statistical correlation and mutual 		BioGRID (human)
information consider only gene expression 		BioGRID (human)
whereas other methods use only 		BioGRID (human)
topological properties to predict GIs.  Network-based methods have been proposed		BioGRID (human)
 to leverage topological properties of		BioGRID (human)
 GI networks [10]. Neighborhood-based methods		BioGRID (human)
 quantify the proximity between genes		BioGRID (human)
, based on common neighbors 		BioGRID (human)
in GI network [11]. The 		BioGRID (human)
proximity scores assigned to a 		BioGRID (human)
pair of genes rely on 		BioGRID (human)
the number of neighbors that 		BioGRID (human)
the pair has in common. 		BioGRID (human)
Adjacency matrix, representing the interaction 		BioGRID (human)
network, or proximity matrix, obtained 		BioGRID (human)
from neighborhood-based methods, are processed 		BioGRID (human)
with network embedding methods to 		BioGRID (human)
learn embeddings that preserve the 		BioGRID (human)
structural properties of the network. 		BioGRID (human)
Structure-preserving network embedding methods such 		BioGRID (human)
as Isomap [12] are proposed 		BioGRID (human)
as a dimensionality reduc- tion 		BioGRID (human)
technique. Since the goal of 		BioGRID (human)
these methods is solely for 		BioGRID (human)
graph reconstruction, the embedding space 		BioGRID (human)
may not be suitable for 		BioGRID (human)
GI network inference. Besides, these 		BioGRID (human)
meth- ods construct the graphs 		BioGRID (human)
from the data features where  proximity between genes is well		BioGRID (human)
 defined in the original feature		BioGRID (human)
 space [9]. On the other		BioGRID (human)
 hand, in GI networks, gene		BioGRID (human)
 proximities are not explicitly defined		BioGRID (human)
, and they depend on 		BioGRID (human)
the specific analytic tasks and 		BioGRID (human)
application scenarios.  Our deep learning method allows		BioGRID (human)
 incorporating gene expression data with		BioGRID (human)
 GI network topological structure information		BioGRID (human)
 to preserve both topological and		BioGRID (human)
 attribute proximity in the low-dimensional		BioGRID (human)
 representation for GI predictions. Moreover		BioGRID (human)
, the scalable architecture enables 		BioGRID (human)
us to incorporate additional attributes. 		BioGRID (human)
Topological prop- erties of GI 		BioGRID (human)
network and expression profiles are 		BioGRID (human)
trans- formed into two separate 		BioGRID (human)
embeddings: ID embedding (which preserves 		BioGRID (human)
the topological structure proximity) and 		BioGRID (human)
attribute embedding (which preserves the 		BioGRID (human)
attribute prox- imity) respectively. With 		BioGRID (human)
a multilayer neural network, we 		BioGRID (human)
then aggregate the complex statistical 		BioGRID (human)
relationships between topology and attribute 		BioGRID (human)
information to improve GI predictions.  In summary, our contributions are		BioGRID (human)
 as follows		BioGRID (human)
:  • We propose a novel		BioGRID (human)
 deep learning framework to learn		BioGRID (human)
 lower dimensional representations while preserving		BioGRID (human)
 topological and attribute proximity of		BioGRID (human)
 GI networks		BioGRID (human)
.  • We evaluate the prediction		BioGRID (human)
 performance on the datasets of		BioGRID (human)
 two organisms based on the		BioGRID (human)
 embedded representation and achieve significantly		BioGRID (human)
 better predictions than the strong		BioGRID (human)
 baselines		BioGRID (human)
.  • Our method can predict		BioGRID (human)
 new gene interactions which are		BioGRID (human)
 validated on an up-to-date GI		BioGRID (human)
 database		BioGRID (human)
.  Methods Preliminaries We formally define		BioGRID (human)
 the problem of gene network		BioGRID (human)
 infer- ence as a network		BioGRID (human)
 embedding problem using the concepts		BioGRID (human)
 of topological and attribute proximity		BioGRID (human)
 as demonstrated in Fig. 1		BioGRID (human)
.  Definition 1 (Gene network) Gene		BioGRID (human)
 network can be rep- resented		BioGRID (human)
 as a network structure, which		BioGRID (human)
 represents the inter- actions between		BioGRID (human)
 genes within an organism. The		BioGRID (human)
 interaction between genes corresponds to		BioGRID (human)
 either a physical interaction through		BioGRID (human)
 their gene products, e.g., proteins		BioGRID (human)
, or one of the 		BioGRID (human)
genes alters or affects the 		BioGRID (human)
activity of other gene of 		BioGRID (human)
interest. We denote gene network 		BioGRID (human)
as G = (V , 		BioGRID (human)
E, A) where 		BioGRID (human)
V = {vi} denotes genes 		BioGRID (human)
or proteins, E = {eij} 		BioGRID (human)
denotes edges that correspond to 		BioGRID (human)
interactions between genes vi and 		BioGRID (human)
vj, and A = {Ai} 		BioGRID (human)
represents the attributes of gene 		BioGRID (human)
vi. Edge eij is associated 		BioGRID (human)
with a weight wij ≥ 0 indicating the strength of the		BioGRID (human)
 connection between gene vi and		BioGRID (human)
 vj. If gene vi and		BioGRID (human)
 vj is not linked by		BioGRID (human)
 an edge, wij = 0		BioGRID (human)
. We name interactions with 		BioGRID (human)
wij > 0 as positive 		BioGRID (human)
interactions and wij = 0 		BioGRID (human)
as negative interactions. In this 		BioGRID (human)
paper, we consider weights wij 		BioGRID (human)
to be binary, indicating whether 		BioGRID (human)
genes vi and vj interact 		BioGRID (human)
or 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
KC et al. BMC Systems 		BioGRID (human)
Biology 2019, 13(Suppl 2):38 Page 3 of 14		BioGRID (human)
		BioGRID (human)
Fig. 1 An illustration of 		BioGRID (human)
gene network embedding (GNE). GNE 		BioGRID (human)
integrates gene interaction network and 		BioGRID (human)
gene expression data to learn 		BioGRID (human)
a lower-dimensional representation.The nodes represent 		BioGRID (human)
genes, and the genes with 		BioGRID (human)
the same color have similar 		BioGRID (human)
expression profiles. GNE groups genes 		BioGRID (human)
with similar network topology, which 		BioGRID (human)
are connected or have a 		BioGRID (human)
similar neighborhood in the graph, 		BioGRID (human)
and attribute similarity (similar expression 		BioGRID (human)
profiles) in the embedded space  Genes directly connected with a		BioGRID (human)
 gene vi in gene network		BioGRID (human)
 denote the local network structure		BioGRID (human)
 of gene vi. We define		BioGRID (human)
 local network structures as the		BioGRID (human)
 first-order proximity of a gene		BioGRID (human)
.  Definition 2 (First-order proximity) The		BioGRID (human)
 first-order proximity in a gene		BioGRID (human)
 network is the pairwise interactions		BioGRID (human)
 between genes. Weight wij indicates		BioGRID (human)
 the first-order proxim- ity between		BioGRID (human)
 gene vi and vj. If		BioGRID (human)
 there is no interaction between		BioGRID (human)
 gene vi and vj, their		BioGRID (human)
 first-order proximity wij is 0		BioGRID (human)
.  Genes are likely to be		BioGRID (human)
 involved in the same cellular		BioGRID (human)
 func- tions if they are		BioGRID (human)
 connected in the gene network		BioGRID (human)
. On the other hand, 		BioGRID (human)
even if two genes are 		BioGRID (human)
not connected, they may be 		BioGRID (human)
still related in some cellular 		BioGRID (human)
functions. This indicates the need 		BioGRID (human)
for an additional notion of 		BioGRID (human)
proximity to preserve the network 		BioGRID (human)
structure. Studies suggest that genes 		BioGRID (human)
that share a similar neighborhood 		BioGRID (human)
are also likely to be 		BioGRID (human)
related [6]. Thus, we introduce 		BioGRID (human)
second-order proximity that characterizes the 		BioGRID (human)
global network structure of the 		BioGRID (human)
genes.  Definition 3 (Second-order proximity) Second		BioGRID (human)
 order proximity denotes the similarity		BioGRID (human)
 between the neighbor- hood of		BioGRID (human)
 genes. Let Ni = {si,1		BioGRID (human)
,. . . , si,i−1, 		BioGRID (human)
si,i+1,. . . , si,M−1} 		BioGRID (human)
denotes the first-order proximity of 		BioGRID (human)
gene vi, where si,j is 		BioGRID (human)
wij if there is direct 		BioGRID (human)
connection between gene vi and 		BioGRID (human)
gene vj, otherwise 0. Then, 		BioGRID (human)
the second order proximity is 		BioGRID (human)
the sim- ilarity between Ni 		BioGRID (human)
and Nj. If there is 		BioGRID (human)
no path to reach gene 		BioGRID (human)
vi from gene vj, the 		BioGRID (human)
second proximity between these genes 		BioGRID (human)
is 0.  Integrating first-order and second-order proximities		BioGRID (human)
 simultaneously can help to preserve		BioGRID (human)
 the topological proper- ties of		BioGRID (human)
 the gene network. To generate		BioGRID (human)
 a more comprehensive representation of		BioGRID (human)
 the genes, it is crucial		BioGRID (human)
 to integrate gene expression data		BioGRID (human)
 as gene attributes with their		BioGRID (human)
 topological properties. Besides preserving topological		BioGRID (human)
 properties, gene expression provides additional		BioGRID (human)
 information to predict the network		BioGRID (human)
 structure		BioGRID (human)
.  Definition 4 (Attribute proximity) Attribute		BioGRID (human)
 proxim- ity denotes the similarity		BioGRID (human)
 between the expression of genes		BioGRID (human)
.  We thus investigate both topological		BioGRID (human)
 and attribute prox- imity for		BioGRID (human)
 gene network embedding, which is		BioGRID (human)
 defined as follows		BioGRID (human)
:  Definition 5 (Gene network embedding		BioGRID (human)
) Given a gene network 		BioGRID (human)
denoted as G = (V , E, A), gene network embedding		BioGRID (human)
 aims to learn a function		BioGRID (human)
 f that maps gene network		BioGRID (human)
 structure and their attribute information		BioGRID (human)
 to a d- dimensional space		BioGRID (human)
 where a gene is represented		BioGRID (human)
 by a vector yi		BioGRID (human)
 ∈ Rd where d		BioGRID (human)
 � M. The low dimensional		BioGRID (human)
 vectors yi and yj for		BioGRID (human)
 genes vi and vj preserve		BioGRID (human)
 their relationships in terms of		BioGRID (human)
 the network topological structure and		BioGRID (human)
 attribute proximity		BioGRID (human)
.  Gene network embedding (GNE) model		BioGRID (human)
 Our deep learning framework as		BioGRID (human)
 shown in Fig. 2 jointly		BioGRID (human)
 utilizes gene network structure and		BioGRID (human)
 gene expression data to learn		BioGRID (human)
 a unified representation for the		BioGRID (human)
 genes. Embedding of a gene		BioGRID (human)
 network projects genes into a		BioGRID (human)
 lower dimensional space, known as		BioGRID (human)
 the embedding space, in which		BioGRID (human)
 each gene is represented by		BioGRID (human)
 a vector. The embeddings preserve		BioGRID (human)
 both the gene network structure		BioGRID (human)
 and statistical relationships of gene		BioGRID (human)
 expression. We list the variables		BioGRID (human)
 to specify our framework in		BioGRID (human)
 Table 1		BioGRID (human)
.  Gene network structure modeling GNE		BioGRID (human)
 framework preserves first-order and second-order		BioGRID (human)
 proximity of genes in the		BioGRID (human)
 gene network. The key idea		BioGRID (human)
 of network structure modeling is		BioGRID (human)
 to estimate the pairwise proximity		BioGRID (human)
 of genes in terms of		BioGRID (human)
 the network structure. If two		BioGRID (human)
 genes are connected or share		BioGRID (human)
 similar neighborhood genes, they tend		BioGRID (human)
 to be related and should		BioGRID (human)
 be placed closer to each		BioGRID (human)
 other in the embedding space		BioGRID (human)
. Inspired by the Skip-gram 		BioGRID (human)
model [13], we use one 		BioGRID (human)
hot encoded represen- tation to 		BioGRID (human)
represent topological information of a 		BioGRID (human)
gene. 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
KC et al. BMC Systems 		BioGRID (human)
Biology 2019, 13(Suppl 2):38 Page 4 of 14		BioGRID (human)
		BioGRID (human)
Fig. 2 Overview of Gene 		BioGRID (human)
Network Embedding (GNE) Framework for 		BioGRID (human)
gene interaction prediction. On the 		BioGRID (human)
left,one-hot encoded representation of gene 		BioGRID (human)
is encoded to dense vector 		BioGRID (human)
v(s)i of dimension d × 1 which captures topological properties and		BioGRID (human)
 expression vector of gene is		BioGRID (human)
 transformed to v(a)i of dimension		BioGRID (human)
 d × 1 which aggregates		BioGRID (human)
 the attribute information (Step 1		BioGRID (human)
). Next, concatenation of two 		BioGRID (human)
embedded vectors (creates vector with 		BioGRID (human)
dimension 2d × 1) allows 		BioGRID (human)
to combine strength of both 		BioGRID (human)
network structure and attribute modeling. 		BioGRID (human)
Then, nonlinear transformation of concatenated 		BioGRID (human)
vector enables GNE to capture 		BioGRID (human)
complex statistical relationships between network 		BioGRID (human)
structure and attribute information and 		BioGRID (human)
learn better representations (Step 2). 		BioGRID (human)
Finally, these learned representation of 		BioGRID (human)
dimension d × 1 is 		BioGRID (human)
transformed into a probability vector 		BioGRID (human)
of length M × 1 		BioGRID (human)
in output layer, which contains 		BioGRID (human)
the predictive probability of gene 		BioGRID (human)
vi to all the genes 		BioGRID (human)
in the network. Conditional probability 		BioGRID (human)
p(vj|vi) on output layer indicates 		BioGRID (human)
the likelihood that gene vj 		BioGRID (human)
is connected with gene vi (		BioGRID (human)
Step 3)  gene vi in the network		BioGRID (human)
 is represented as an M-dimensional		BioGRID (human)
 vector where only the ith		BioGRID (human)
 component of the vector is		BioGRID (human)
 1		BioGRID (human)
.  To model topological similarity, we		BioGRID (human)
 define the condi- tional probability		BioGRID (human)
 of gene vj on gene		BioGRID (human)
 vi using a softmax function		BioGRID (human)
 as		BioGRID (human)
:  p(vj|vi) = exp(f (vi, vj))∑M		BioGRID (human)
 j′=1 exp(f (vi, vj		BioGRID (human)
′))  (1		BioGRID (human)
)  Table 1 Terms and Notations		BioGRID (human)
		BioGRID (human)
Symbol Definitions  M Total number of genes		BioGRID (human)
 in gene network		BioGRID (human)
		BioGRID (human)
E Number of expression values 		BioGRID (human)
for each gene  Ni Set of the neighbor		BioGRID (human)
 genes of gene vi v(s)i		BioGRID (human)
 Topological representation of gene vi		BioGRID (human)
 v(a)i Attribute representation of gene		BioGRID (human)
 vi ṽi Neighborhood representation of		BioGRID (human)
 gene vi vi Concatenated representation		BioGRID (human)
 of topological		BioGRID (human)
		BioGRID (human)
properties and expression data  k Number of hidden layers		BioGRID (human)
 to transform concatenated representation into		BioGRID (human)
 embedding space		BioGRID (human)
		BioGRID (human)
h(k) Output of kth hidden 		BioGRID (human)
layer  Wk Weight matrix for kth		BioGRID (human)
 hidden layer		BioGRID (human)
		BioGRID (human)
Wid Weight matrix for topological 		BioGRID (human)
structure embedding  Watt Weight matrix for attribute		BioGRID (human)
 embedding		BioGRID (human)
		BioGRID (human)
Wout Weight matrix for output 		BioGRID (human)
layer  which measures the likelihood of		BioGRID (human)
 gene vi being connected with		BioGRID (human)
 vj. Let function f represents		BioGRID (human)
 the mapping of two genes		BioGRID (human)
 vi and vj to their		BioGRID (human)
 estimated proximity score. Let p(N		BioGRID (human)
 |v) be the likelihood of		BioGRID (human)
 observing a neighborhood N for		BioGRID (human)
 a gene v. By assuming		BioGRID (human)
 conditional independence, we can factorize		BioGRID (human)
 the likelihood so that the		BioGRID (human)
 likelihood of observing a neighborhood		BioGRID (human)
 gene is independent of observ		BioGRID (human)
- ing any other neighborhood 		BioGRID (human)
gene, given a gene vi:  p(Ni|vi		BioGRID (human)
) = ∏  vj∈Ni p(vj|vi) (2		BioGRID (human)
)  where Ni represents the neighborhood		BioGRID (human)
 genes of the gene vi		BioGRID (human)
. Global structure proximity for 		BioGRID (human)
a gene vi can be 		BioGRID (human)
pre- served by maximizing the 		BioGRID (human)
conditional probability over all genes 		BioGRID (human)
in the neighborhood. Hence, we 		BioGRID (human)
can define the likelihood function 		BioGRID (human)
that preserve global structure proximity 		BioGRID (human)
as:  L = M		BioGRID (human)
∏  i=1 p(Ni|vi		BioGRID (human)
) =  M		BioGRID (human)
∏  i=1		BioGRID (human)
		BioGRID (human)
vj∈Ni p(vj|vi) (3)  Let v(s)i denotes the dense		BioGRID (human)
 vector generated from one-hot gene		BioGRID (human)
 ID vector, which represents topological		BioGRID (human)
 informa- tion of that gene		BioGRID (human)
. GNE follows direct encoding 		BioGRID (human)
methods [13, 14] to map 		BioGRID (human)
genes to vector embeddings, simply 		BioGRID (human)
known as embedding 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
KC et al. BMC Systems 		BioGRID (human)
Biology 2019, 13(Suppl 2):38 Page 5 of 14		BioGRID (human)
		BioGRID (human)
v(s)i = Widvi (4) where 		BioGRID (human)
Wid ∈ Rd×M is a 		BioGRID (human)
matrix containing the embedding vectors 		BioGRID (human)
for all genes and 		BioGRID (human)
vi ∈ IM is a 		BioGRID (human)
one-hot indica- tor vector indicating 		BioGRID (human)
the column of Wid corresponding 		BioGRID (human)
to gene vi.  Gene expression modeling GNE encodes		BioGRID (human)
 the expression data from microarray		BioGRID (human)
 exper- iments to the dense		BioGRID (human)
 representation using a non-linear transformation		BioGRID (human)
. The amount of mRNA 		BioGRID (human)
produced during transcription measured over 		BioGRID (human)
a number of experiments helps 		BioGRID (human)
to identify similarly expressed genes. 		BioGRID (human)
Since expres- sion data have 		BioGRID (human)
inherent noise [15], transforming expres- 		BioGRID (human)
sion data using a non-linear 		BioGRID (human)
transformation can be helpful to 		BioGRID (human)
uncover the underlying representation. Let 		BioGRID (human)
xi be the vector of 		BioGRID (human)
expression values of gene vi 		BioGRID (human)
measured over E experiments. Using 		BioGRID (human)
non-linear transformation, we can capture 		BioGRID (human)
the non-linearities of expression data 		BioGRID (human)
of gene vi as:  v(a)i = δa(Watt · xi		BioGRID (human)
) (5) where v(a)i represents 		BioGRID (human)
the lower dimensional attribute rep- 		BioGRID (human)
resentation vector for gene vi. 		BioGRID (human)
Watt , and δa represents 		BioGRID (human)
the weight matrix, and activation 		BioGRID (human)
function of attribute transformation layer 		BioGRID (human)
respectively.  We use the deep model		BioGRID (human)
 to approximate the attribute proximity		BioGRID (human)
 by capturing complex statistical relationships		BioGRID (human)
 between attributes and introducing non-linearities		BioGRID (human)
, simi- lar to structural 		BioGRID (human)
embedding.  GNE integration GNE models the		BioGRID (human)
 integration of network structure and		BioGRID (human)
 attribute information to learn more		BioGRID (human)
 comprehensive embeddings for gene networks		BioGRID (human)
. GNE takes two inputs: 		BioGRID (human)
one for topological information of 		BioGRID (human)
a gene as one hot 		BioGRID (human)
gene ID vector and another 		BioGRID (human)
for its expression as an 		BioGRID (human)
attribute vector. Each input is 		BioGRID (human)
encoded to its respective embed- 		BioGRID (human)
dings. One hot representation for 		BioGRID (human)
a gene vi is projected 		BioGRID (human)
to the dense vector v(s)i 		BioGRID (human)
which captures the topological properties. 		BioGRID (human)
Non-linear transformation of attribute vec- 		BioGRID (human)
tor generates compact representation vector 		BioGRID (human)
v(a)i . Previous work [16] 		BioGRID (human)
combines heterogeneous information using the 		BioGRID (human)
late fusion approach. However, the 		BioGRID (human)
late fusion approach is the 		BioGRID (human)
approach of learning separate models 		BioGRID (human)
for hetero- geneous information and 		BioGRID (human)
integrating the representations learned from 		BioGRID (human)
separate models. On the other 		BioGRID (human)
hand, the early fusion combines 		BioGRID (human)
heterogeneous information and train the 		BioGRID (human)
model on combined representations [17]. 		BioGRID (human)
We thus propose to use 		BioGRID (human)
the early fusion approach to 		BioGRID (human)
combine them by concatenating. As 		BioGRID (human)
a result, learning from topo- 		BioGRID (human)
logical and attribute information can 		BioGRID (human)
complement each other, allowing the 		BioGRID (human)
model to learn their complex 		BioGRID (human)
statistical relationships as well. Embeddings 		BioGRID (human)
from topological and attribute information 		BioGRID (human)
are concatenated into a vector 		BioGRID (human)
as:  vi		BioGRID (human)
		BioGRID (human)
v(s)i λv (a) i  ] (6		BioGRID (human)
)  where λ is the importance		BioGRID (human)
 of gene expression information relative		BioGRID (human)
 to topological information		BioGRID (human)
.  The concatenated vectors are fed		BioGRID (human)
 into a multilayer perceptron with		BioGRID (human)
 k hidden layers. The hidden		BioGRID (human)
 represen- tations from each hidden		BioGRID (human)
 layer in GNE are denoted		BioGRID (human)
 as h(0)i , h		BioGRID (human)
		BioGRID (human)
1) i , ....., h  (k) i , which can		BioGRID (human)
 be defined as		BioGRID (human)
		BioGRID (human)
h(0)i = δ (  W0vi + b(0		BioGRID (human)
		BioGRID (human)
h(k)i = δk (  Wkh(k−1)i + b(k) ) (7		BioGRID (human)
)  where δk represents the activation		BioGRID (human)
 function of layer k. h(0)i		BioGRID (human)
 represents initial representation and h(k)i		BioGRID (human)
 represents final representation of the		BioGRID (human)
 input gene vi. Transformation of		BioGRID (human)
 input data using multiple non-linear		BioGRID (human)
 layers has shown to improve		BioGRID (human)
 the representation of input data		BioGRID (human)
 [18]. Moreover, stacking multiple layers		BioGRID (human)
 of non-linear transformations can help		BioGRID (human)
 to learn high-order statistical relationships		BioGRID (human)
 between topological properties and attributes		BioGRID (human)
.  At last, final representation h(k)i		BioGRID (human)
 of a gene vi from		BioGRID (human)
 the last hidden layer is		BioGRID (human)
 transformed to probability vector, which		BioGRID (human)
 contains the conditional probability of		BioGRID (human)
 all other genes to vi		BioGRID (human)
:  oi = [ p(v1|vi), p(v2|vi		BioGRID (human)
), . . . , 		BioGRID (human)
p(vM|vi)  ] (8		BioGRID (human)
)  where p(vj|vi) represents the probability		BioGRID (human)
 of gene vi being related		BioGRID (human)
 to gene vj and oi		BioGRID (human)
 represents the output probabil- ity		BioGRID (human)
 vector with the conditional probability		BioGRID (human)
 of gene vi being connected		BioGRID (human)
 to all other genes		BioGRID (human)
.  Weight matrix Wout between the		BioGRID (human)
 last hidden layer and the		BioGRID (human)
 output layer corresponds to the		BioGRID (human)
 abstractive represen- tation of neighborhood		BioGRID (human)
 of genes. A jth row		BioGRID (human)
 from Wout refers to the		BioGRID (human)
 compact representation of neighborhood of		BioGRID (human)
 gene vj, which can be		BioGRID (human)
 denoted as ṽj. The proximity		BioGRID (human)
 score between gene vi and		BioGRID (human)
 vj can be defined as		BioGRID (human)
:  f (vi, vj) = ṽj		BioGRID (human)
 · h(k)i (9) which can		BioGRID (human)
 be replaced into Eq. 1		BioGRID (human)
 to calculate the condi- tional		BioGRID (human)
 probability		BioGRID (human)
:  p(vj|vi) = exp		BioGRID (human)
		BioGRID (human)
ṽj · 		BioGRID (human)
		BioGRID (human)
(		BioGRID (human)
		BioGRID (human)
)		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
M j′=1 exp  ( ṽj′ · h(k)i		BioGRID (human)
		BioGRID (human)
10)  Our model learns two latent		BioGRID (human)
 representations h(k)i and ṽi for		BioGRID (human)
 a gene vi where h(k)i		BioGRID (human)
 is the representation of gene		BioGRID (human)
 as a node and ṽi		BioGRID (human)
 is the representation of the		BioGRID (human)
 gene vi as a neighbor		BioGRID (human)
. Neighborhood representation ṽi can 		BioGRID (human)
be com- bined with node 		BioGRID (human)
representation h(k)i by addition [19, 20		BioGRID (human)
] to get final representation 		BioGRID (human)
for a gene as:  yi = h(k)i + ṽi		BioGRID (human)
 (11) which returns us better		BioGRID (human)
 performance results		BioGRID (human)
		BioGRID (human)
KC et al. BMC Systems 		BioGRID (human)
Biology 2019, 13(Suppl 2):38 Page 6 of 14		BioGRID (human)
		BioGRID (human)
For an edge connecting gene 		BioGRID (human)
vi and vj, we create 		BioGRID (human)
fea- ture vector by combining 		BioGRID (human)
embeddings of those genes using 		BioGRID (human)
Hadamard product. Empirical evaluation shows 		BioGRID (human)
features created with Hadamard product 		BioGRID (human)
gives better performance over 		BioGRID (human)
concatenation [14]. Then, we train 		BioGRID (human)
a logistic classi- fier on 		BioGRID (human)
these features to classify whether 		BioGRID (human)
genes vi and vj interact 		BioGRID (human)
or not.  Parameter optimization To optimize GNE		BioGRID (human)
, the goal is to 		BioGRID (human)
maximize objective func- tion mentioned 		BioGRID (human)
in Eq. 10 as a 		BioGRID (human)
function of all parame- ters. 		BioGRID (human)
Let � be the parameters 		BioGRID (human)
of GNE which includes {Wid, 		BioGRID (human)
Watt , Wout , �h} 		BioGRID (human)
and �h represents weight matrices 		BioGRID (human)
Wk of hidden layers. We 		BioGRID (human)
train our model to maximize 		BioGRID (human)
the objective function with respect 		BioGRID (human)
to all parameters � :  argmax		BioGRID (human)
		BioGRID (human)
M∑  i=1		BioGRID (human)
		BioGRID (human)
vj ∈ Ni log  exp		BioGRID (human)
		BioGRID (human)
ṽj · h(k)i )  ∑M j′=1 exp		BioGRID (human)
		BioGRID (human)
ṽj′ · 		BioGRID (human)
		BioGRID (human)
(		BioGRID (human)
		BioGRID (human)
)		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
12)  Maximizing this objective function with		BioGRID (human)
 respect to � is computationally		BioGRID (human)
 expensive, which requires the calculation		BioGRID (human)
 of partition function		BioGRID (human)
		BioGRID (human)
M j′=1 exp  ( ṽj′ · h(k)i		BioGRID (human)
		BioGRID (human)
for each gene.  To calculate a single probability		BioGRID (human)
, we need to aggregate 		BioGRID (human)
all genes in the network. 		BioGRID (human)
To address this problem, we 		BioGRID (human)
adopt the approach of negative 		BioGRID (human)
sampling [13] which samples the 		BioGRID (human)
negative interactions, interactions with no 		BioGRID (human)
evidence of their existence, according 		BioGRID (human)
to some noise distribution for 		BioGRID (human)
each edge eij. This approach 		BioGRID (human)
allows us to sample a 		BioGRID (human)
small subset of genes from 		BioGRID (human)
the network as negative samples 		BioGRID (human)
for a gene, considering that 		BioGRID (human)
the genes on selected subset 		BioGRID (human)
don’t fall in the neighborhood 		BioGRID (human)
Ni of the gene. Above 		BioGRID (human)
objective function enhances the similarity 		BioGRID (human)
of a gene viwith its 		BioGRID (human)
neigh- borhood genes vj ∈ 		BioGRID (human)
Ni and weakens the similarity 		BioGRID (human)
with genes not in its 		BioGRID (human)
neighborhood genes vj /∈ Ni. 		BioGRID (human)
It is inap- propriate to 		BioGRID (human)
assume that the two genes 		BioGRID (human)
in the network are not 		BioGRID (human)
related if they are not 		BioGRID (human)
connected. It may be the 		BioGRID (human)
case that there is not 		BioGRID (human)
enough experimental evidence to support 		BioGRID (human)
that they are related yet. 		BioGRID (human)
Thus, forcing the dissimilarity of 		BioGRID (human)
a gene with all other 		BioGRID (human)
genes, not in its neighborhood 		BioGRID (human)
Ni seems to be inappropriate.  We adopt Adaptive Moment Estimation		BioGRID (human)
 (Adam) opti- mization [21], which		BioGRID (human)
 is an extension to stochastic		BioGRID (human)
 gra- dient descent, for optimizing		BioGRID (human)
 Eq. 12. Adam computes the		BioGRID (human)
 adaptive learning rate for each		BioGRID (human)
 parameter by per- forming smaller		BioGRID (human)
 updates for the frequent parameters		BioGRID (human)
 and larger updates for the		BioGRID (human)
 infrequent parameters. The Adam method		BioGRID (human)
 provides the ability of AdaGrad		BioGRID (human)
 [22] to deal with sparse		BioGRID (human)
 gradients and also the ability		BioGRID (human)
 of RMSProp [23] to deal		BioGRID (human)
 with non-stationary objectives. In each		BioGRID (human)
 step, Adam algorithm samples mini-batch		BioGRID (human)
 of interactions and then		BioGRID (human)
		BioGRID (human)
updates GNE’s parameters. To address 		BioGRID (human)
the issue of over- fitting, 		BioGRID (human)
regularization like dropout [24] and 		BioGRID (human)
batch normal- ization [25] is 		BioGRID (human)
added to hidden layers. Proper 		BioGRID (human)
optimization of GNE gives the 		BioGRID (human)
final representation for each gene.  Experimental setup We evaluate our		BioGRID (human)
 model using two real organism		BioGRID (human)
 datasets. We take gene interaction		BioGRID (human)
 network data from the BioGRID		BioGRID (human)
 database [26] and gene expression		BioGRID (human)
 data from DREAM5 challenge [7		BioGRID (human)
]. We use two interaction 		BioGRID (human)
datasets from BioGRID database (2017 		BioGRID (human)
released version 3.4.153 and 2018 		BioGRID (human)
released version 3.4.158) to evaluate 		BioGRID (human)
the predictive performance of our 		BioGRID (human)
model. Self-interactions and redun- dant 		BioGRID (human)
interactions are removed from interaction 		BioGRID (human)
datasets. The statistics of the 		BioGRID (human)
datasets are shown in Table 2		BioGRID (human)
.  We evaluate the learned embeddings		BioGRID (human)
 to infer gene net- work		BioGRID (human)
 structure. We randomly hold out		BioGRID (human)
 a fraction of inter- actions		BioGRID (human)
 as the validation set for		BioGRID (human)
 hyper-parameter tuning. Then, we divide		BioGRID (human)
 the remaining interactions randomly into		BioGRID (human)
 training and testing dataset with		BioGRID (human)
 the equal number of interactions		BioGRID (human)
. Since the validation set 		BioGRID (human)
and the test set con- 		BioGRID (human)
tains only positive interactions, we 		BioGRID (human)
randomly sample an equal number 		BioGRID (human)
of gene pairs from the 		BioGRID (human)
network, consider- ing the missing 		BioGRID (human)
edge between the gene pairs 		BioGRID (human)
represents the absence of interactions. 		BioGRID (human)
Given the gene network G 		BioGRID (human)
with a fraction of missing 		BioGRID (human)
interactions, the task is to 		BioGRID (human)
predict these missing interactions.  We compare the GNE model		BioGRID (human)
 with five competing methods. Correlation		BioGRID (human)
 directly predicts the interactions between		BioGRID (human)
 genes based on the correlation		BioGRID (human)
 of expression pro- files. Then		BioGRID (human)
, the following three baselines (		BioGRID (human)
Isomap, LINE, and node2vec) are 		BioGRID (human)
network embedding methods. Specif- ically, 		BioGRID (human)
node2vec is the strong baseline 		BioGRID (human)
for structural net- work embedding. 		BioGRID (human)
We evaluate the performance of 		BioGRID (human)
GNE against the following methods:  • Correlation [27] It computes		BioGRID (human)
 Pearson’s correlation coefficient between all		BioGRID (human)
 genes and the interactions are		BioGRID (human)
 ranked via correlation scores, i.e		BioGRID (human)
., highly correlated gene pairs 		BioGRID (human)
receive higher confidence.  • Isomap [10] It computes		BioGRID (human)
 all-pairs shortest-path distances to create		BioGRID (human)
 a distance matrix and performs		BioGRID (human)
 singular-value decomposition of that matrix		BioGRID (human)
 to learn a lower-dimensional representation		BioGRID (human)
. Genes separated  Table 2 Statistics of the		BioGRID (human)
 interaction datasets from BioGRID and		BioGRID (human)
 the gene expression data from		BioGRID (human)
 DREAM5 challenge		BioGRID (human)
		BioGRID (human)
Interactions) Expression data  Datasets #(Genes) 2017 version 2018		BioGRID (human)
 version #(Experiments		BioGRID (human)
)  Yeast 5950 544,652 557,487 536		BioGRID (human)
		BioGRID (human)
E. coli 4511 148,340 159,523 805		BioGRID (human)
		BioGRID (human)
KC et al. BMC Systems 		BioGRID (human)
Biology 2019, 13(Suppl 2):38 Page 7 of 14		BioGRID (human)
		BioGRID (human)
by the distance less than 		BioGRID (human)
threshold � in embedding space 		BioGRID (human)
are considered to have the 		BioGRID (human)
connection with each other and 		BioGRID (human)
the reliability index, a likelihood 		BioGRID (human)
indicating the interaction between two 		BioGRID (human)
genes, is computed using 		BioGRID (human)
FSWeight [28].  • LINE [16] Two separate		BioGRID (human)
 embeddings are learned by preserving		BioGRID (human)
 first-order and second-order proximity of		BioGRID (human)
 the network structure respectively. Then		BioGRID (human)
, these embeddings are concatenated 		BioGRID (human)
to get final representations for 		BioGRID (human)
each node.  • node2vec [14] It learns		BioGRID (human)
 the embeddings of the node		BioGRID (human)
 by applying Skip-gram model to		BioGRID (human)
 node sequences generated by a		BioGRID (human)
 biased random walk. We tuned		BioGRID (human)
 two hyper-parameters p and q		BioGRID (human)
 that control the random walk		BioGRID (human)
.  Note that the competing methods		BioGRID (human)
 such as Isomap, LINE, and		BioGRID (human)
 node2vec are designed to capture		BioGRID (human)
 only the topological properties of		BioGRID (human)
 the network. For the fair		BioGRID (human)
 com- parison with GNE that		BioGRID (human)
 additionally integrates expression data, we		BioGRID (human)
 concatenate attribute feature vector with		BioGRID (human)
 learned gene representation to extend		BioGRID (human)
 baselines by including the gene		BioGRID (human)
 expression. We name these variants		BioGRID (human)
 as Isomap+, LINE+, and node2vec		BioGRID (human)
+.  We have implemented GNE with		BioGRID (human)
 TensorFlow frame- work [29]. The		BioGRID (human)
 parameter settings for GNE are		BioGRID (human)
 deter- mined by its performance		BioGRID (human)
 on the validation set. We		BioGRID (human)
 randomly initialize GNE’s parameters, optimizing		BioGRID (human)
 with mini-batch Adam. We test		BioGRID (human)
 the batch size of [8		BioGRID (human)
, 16, 32, 64, 128, 256		BioGRID (human)
] and learning rate 		BioGRID (human)
of [0.1, 0.01, 0.005, 0.002, 0		BioGRID (human)
.001, 0.0001]. We test the 		BioGRID (human)
number of negative samples to 		BioGRID (human)
be [2, 5, 10, 15, 20		BioGRID (human)
] as suggested by [13]. 		BioGRID (human)
We test the embedding dimension 		BioGRID (human)
d of [32, 64, 128, 256		BioGRID (human)
] for all methods. Also, 		BioGRID (human)
we evaluate model’s performance with 		BioGRID (human)
respect to differ- ent values 		BioGRID (human)
of λ [0, 0.2, 0.4, 0		BioGRID (human)
.6, 0.8, 1], which is 		BioGRID (human)
discussed in more detail later. 		BioGRID (human)
The parameters are selected based 		BioGRID (human)
on empirical evaluation and Table 3 summarizes the optimal parameters tuned		BioGRID (human)
 on validation data sets		BioGRID (human)
.  To capture the non-linearity of		BioGRID (human)
 gene expression data, we choose		BioGRID (human)
 Exponential Linear Unit (ELU) [30		BioGRID (human)
] activation function, which corresponds 		BioGRID (human)
to δa in Eq. 5. 		BioGRID (human)
Also, ELU acti- vation avoids 		BioGRID (human)
vanishing gradient problem and provides 		BioGRID (human)
improved learning characteristics in comparison 		BioGRID (human)
to other methods. We use 		BioGRID (human)
a single hidden layer (		BioGRID (human)
k = 1) with hyperbolic 		BioGRID (human)
tangent activation (Tanh) to model 		BioGRID (human)
complex statistical relationships between topological 		BioGRID (human)
properties  and attributes of the gene		BioGRID (human)
. The choice of ELU 		BioGRID (human)
for attribute transformation and Tanh 		BioGRID (human)
for hidden layer shows better 		BioGRID (human)
performance upon empirical evaluation.  We use the area under		BioGRID (human)
 the ROC curve (AUROC) and		BioGRID (human)
 area under the precision-recall curve		BioGRID (human)
 (AUPR) [31] to eval- uate		BioGRID (human)
 the rankings generated by the		BioGRID (human)
 model for interactions in the		BioGRID (human)
 test set. These metrics are		BioGRID (human)
 widely used in evaluating the		BioGRID (human)
 ranked list of predictions in		BioGRID (human)
 gene interaction [4		BioGRID (human)
].  Results and discussion We evaluate		BioGRID (human)
 the ability of our GNE		BioGRID (human)
 model to predict gene interaction		BioGRID (human)
 of two real organisms. We		BioGRID (human)
 present empirical results of our		BioGRID (human)
 proposed method against other methods		BioGRID (human)
.  Analysis of gene embeddings We		BioGRID (human)
 visualize the embedding vectors of		BioGRID (human)
 genes learned by GNE. We		BioGRID (human)
 take the learned embeddings, which		BioGRID (human)
 specifi- cally model the interactions		BioGRID (human)
 by preserving topological and attribute		BioGRID (human)
 similarity. We embed these embeddings		BioGRID (human)
 into a 2D space using		BioGRID (human)
 t-SNE package [32] and visualize		BioGRID (human)
 them (Fig. 3). For comparison		BioGRID (human)
, we also visualize the 		BioGRID (human)
embeddings learned by structure-preserving deep 		BioGRID (human)
learning methods, such as LINE, 		BioGRID (human)
and node2vec.  In E. coli, a substantial		BioGRID (human)
 fraction of functionally related genes		BioGRID (human)
 are organized into operons, which		BioGRID (human)
 are the group of genes		BioGRID (human)
 that interact with each other		BioGRID (human)
 and are co-regulated [33]. Since		BioGRID (human)
 this concept fits well with		BioGRID (human)
 the topological and attribute proximity		BioGRID (human)
 implemented in GNE, we expect		BioGRID (human)
 GNE to place genes within		BioGRID (human)
 an operon close to each		BioGRID (human)
 other in the embedding space		BioGRID (human)
. To evaluate this, we 		BioGRID (human)
collect information about operons of 		BioGRID (human)
E. coli from the DOOR 		BioGRID (human)
database and visualize the embeddings 		BioGRID (human)
of genes within these operons (		BioGRID (human)
Fig. 3).  Figure 3 reveals the clustering		BioGRID (human)
 structure that corre- sponds to		BioGRID (human)
 the operons on E. coli		BioGRID (human)
. For example, operon with 		BioGRID (human)
operon id 3306 consists of 		BioGRID (human)
seven genes: rsxA, rsxB, rsx, 		BioGRID (human)
rsxD, rsxG, rsxE, and nth 		BioGRID (human)
that are involved in electron 		BioGRID (human)
transport. GNE infers similar representations 		BioGRID (human)
for these genes, resulting in 		BioGRID (human)
localized projection in the 2D 		BioGRID (human)
space. Similarly, other operons also 		BioGRID (human)
show similar patterns (Fig. 3).  To test if the pattern		BioGRID (human)
 in Fig. 3 holds across		BioGRID (human)
 all operons, we compute the		BioGRID (human)
 average Euclidean distance between each		BioGRID (human)
 gene’s vector representation and vector		BioGRID (human)
 representations of other genes within		BioGRID (human)
 the same operon. Genes within		BioGRID (human)
 the same operon have significantly		BioGRID (human)
 similar vector representa- tion yi		BioGRID (human)
 than expected by chance (p-value		BioGRID (human)
 = 1.75e − 127, 2-sample		BioGRID (human)
 KS test		BioGRID (human)
).  Table 3 Optimal parameter settings		BioGRID (human)
 for GNE model		BioGRID (human)
		BioGRID (human)
Dataset Learning rate Batch size 		BioGRID (human)
Embedding dimension (d) Epoch Negative 		BioGRID (human)
samples  Yeast 0.005 256 128 20		BioGRID (human)
 10		BioGRID (human)
		BioGRID (human)
E. coli 0.002 128 128 20 10		BioGRID (human)
		BioGRID (human)
KC et al. BMC Systems 		BioGRID (human)
Biology 2019, 13(Suppl 2):38 Page 8 of 14		BioGRID (human)
		BioGRID (human)
Fig. 3 Visualization of learned 		BioGRID (human)
embeddings for genes on E. 		BioGRID (human)
coli. Genes are mapped to 		BioGRID (human)
the 2D space using the 		BioGRID (human)
t-SNE package [32] with learned 		BioGRID (human)
gene representations (yi , 		BioGRID (human)
i = 1, 2, . . . , M		BioGRID (human)
) from different methods: a 		BioGRID (human)
GNE, b LINE, and c 		BioGRID (human)
node2vec as input. Operons 3203, 3274, 3279, 3306, and 3736 of		BioGRID (human)
 E. coli are visualized and		BioGRID (human)
 show clustering patterns. Best viewed		BioGRID (human)
 on screen		BioGRID (human)
		BioGRID (human)
Thus, the analysis here indicates 		BioGRID (human)
that GNE can learn similar 		BioGRID (human)
representations for genes with similar 		BioGRID (human)
topological properties and expression.  Gene interaction prediction We randomly		BioGRID (human)
 remove 50% of interactions from		BioGRID (human)
 the net- work and compare		BioGRID (human)
 various methods to evaluate their		BioGRID (human)
 pre- dictions for 50% missing		BioGRID (human)
 interactions. Table 4 shows the		BioGRID (human)
 performance of GNE and other		BioGRID (human)
 methods on gene interac- tion		BioGRID (human)
 prediction across different datasets. As		BioGRID (human)
 our method significantly outperforms other		BioGRID (human)
 competing methods, it indicates the		BioGRID (human)
 informativeness of gene expression in		BioGRID (human)
 pre- dicting missing interactions. Also		BioGRID (human)
, our model is capable 		BioGRID (human)
of integrating attributes with topological 		BioGRID (human)
properties to learn better representations.  Table 4 Area under ROC		BioGRID (human)
 curve (AUROC) and Area under		BioGRID (human)
 PR curve (AUPR) for gene		BioGRID (human)
 Interaction Prediction		BioGRID (human)
		BioGRID (human)
Methods Yeast E. coli  AUROC AUPR AUROC AUPR		BioGRID (human)
		BioGRID (human)
Correlation 0.582 0.579 0.537 0.557  Isomap 0.507 0.588 0.559 0.672		BioGRID (human)
		BioGRID (human)
LINE 0.726 0.686 0.897 0.851  node2vec 0.739 0.708 0.912 0.862		BioGRID (human)
		BioGRID (human)
Isomap+ 0.653 0.652 0.644 0.649  LINE+ 0.745 0.713 0.899 0.856		BioGRID (human)
		BioGRID (human)
node2vec+ 0.751 0.716 0.871 0.826  GNE (Topology) 0.787 0.784 0.930		BioGRID (human)
 0.931		BioGRID (human)
		BioGRID (human)
GNE (our model) 0.825* 0.821* 0		BioGRID (human)
.940* 0.939*  + indicates the concatenation of		BioGRID (human)
 expression data with learned embeddings		BioGRID (human)
 to create final representation		BioGRID (human)
. * denotes that GNE 		BioGRID (human)
significantly outperforms node2vec at 0.01 		BioGRID (human)
level paired t-test. Note that 		BioGRID (human)
method that achieves the best 		BioGRID (human)
performance is bold faced  We compare our model with		BioGRID (human)
 a correlation-based method, that takes		BioGRID (human)
 only expression data into account		BioGRID (human)
. Our model shows significant 		BioGRID (human)
improvement of 0.243 (AUROC), 0.242 (		BioGRID (human)
AUPR) on yeast and 0.403 (		BioGRID (human)
AUROC), 0.382 (AUPR) on E. 		BioGRID (human)
coli over correlation-based methods. This 		BioGRID (human)
improve- ment suggests the significance 		BioGRID (human)
of the topological proper- ties 		BioGRID (human)
of the gene network.  The network embedding method, Isomap		BioGRID (human)
, performs poorly in comparison 		BioGRID (human)
to correlation-based methods on yeast 		BioGRID (human)
because of its limitation on 		BioGRID (human)
network inference. Deep learning based 		BioGRID (human)
network embedding methods such as 		BioGRID (human)
LINE, and node2vec show the 		BioGRID (human)
significant gain over Isomap and 		BioGRID (human)
correlation-based methods. node2vec out- performs 		BioGRID (human)
LINE across two datasets. Moreover, 		BioGRID (human)
GNE trained only with topological 		BioGRID (human)
properties outperforms these structured-based deep 		BioGRID (human)
learning methods (Table 4). However, 		BioGRID (human)
these methods don’t consider the 		BioGRID (human)
attributes of the gene that 		BioGRID (human)
we suggest to contain useful 		BioGRID (human)
informa- tion for gene interaction 		BioGRID (human)
prediction. By adding expres- sion 		BioGRID (human)
data with topological properties, GNE 		BioGRID (human)
outperforms structure-preserving deep embedding methods 		BioGRID (human)
across both datasets.  Focusing on the results corresponding		BioGRID (human)
 to the integra- tion of		BioGRID (human)
 expression data with topological properties		BioGRID (human)
, we find that the 		BioGRID (human)
method of integrating the expression 		BioGRID (human)
data plays an essential role 		BioGRID (human)
in the performance. Performance of 		BioGRID (human)
node2vec+ (LINE+, Isomap+) shows little 		BioGRID (human)
improvement with the integration of 		BioGRID (human)
expression data on yeast. How- 		BioGRID (human)
ever, node2vec+ (LINE+, Isomap+) has 		BioGRID (human)
no improvement or decline in 		BioGRID (human)
performance on E. coli. The 		BioGRID (human)
decline in per- formance indicates 		BioGRID (human)
that merely concatenating the expres- 		BioGRID (human)
sion vector with learned representations 		BioGRID (human)
for the gene is insufficient 		BioGRID (human)
to capture the rich information 		BioGRID (human)
in expression data. The late 		BioGRID (human)
fusion approach of combining the 		BioGRID (human)
embed- ding vector corresponding to 		BioGRID (human)
the topological 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
KC et al. BMC Systems 		BioGRID (human)
Biology 2019, 13(Suppl 2):38 Page 9 of 14		BioGRID (human)
		BioGRID (human)
of the gene network and 		BioGRID (human)
the feature vector represent- ing 		BioGRID (human)
expression data has no significant 		BioGRID (human)
improvement in the performance (except 		BioGRID (human)
Isomap). In contrast, our model 		BioGRID (human)
incorporates gene expression data with 		BioGRID (human)
topological prop- erties by the 		BioGRID (human)
early fusion method and shows 		BioGRID (human)
significant improvement over other methods.  Impact of network sparsity We		BioGRID (human)
 investigate the robustness of our		BioGRID (human)
 model to network sparsity. We		BioGRID (human)
 hold out 10% interactions as		BioGRID (human)
 the test set and change		BioGRID (human)
 the sparsity of the remaining		BioGRID (human)
 network by randomly removing a		BioGRID (human)
 portion of remaining interactions. Then		BioGRID (human)
, we train GNE to 		BioGRID (human)
predict interactions in the test 		BioGRID (human)
set and eval- uate the 		BioGRID (human)
change in performance to network 		BioGRID (human)
sparsity. We evaluate two versions 		BioGRID (human)
of our implementations: GNE with 		BioGRID (human)
only topological properties and GNE 		BioGRID (human)
with topological properties and expression 		BioGRID (human)
data. The result is shown 		BioGRID (human)
in Fig. 4.  Figure 4 shows that our		BioGRID (human)
 method’s performance improves with an		BioGRID (human)
 increase in the number of		BioGRID (human)
 training interactions across datasets. Also		BioGRID (human)
, our method’s performance improves 		BioGRID (human)
when expression data is integrated 		BioGRID (human)
with the topological structure. Specifically, 		BioGRID (human)
GNE trained on 10% of 		BioGRID (human)
total inter- actions and attributes 		BioGRID (human)
of yeast shows a significant 		BioGRID (human)
gain of 0.172 AUROC (from 0		BioGRID (human)
.503 to 0.675) over GNE 		BioGRID (human)
trained only with 10% of 		BioGRID (human)
total interactions as shown in 		BioGRID (human)
Fig. 4. Similarly, GNE improves 		BioGRID (human)
the AUROC from 0.497 to 0		BioGRID (human)
.816 for E. coli with 		BioGRID (human)
the same setup as shown 		BioGRID (human)
in Fig. 4. The integration 		BioGRID (human)
of gene expression data results 		BioGRID (human)
in less improve- ment when 		BioGRID (human)
we train GNE on a 		BioGRID (human)
relatively large number of interactions.  Moreover, the performance of GNE		BioGRID (human)
 trained with 50% of total		BioGRID (human)
 interactions and expression data is		BioGRID (human)
 comparable to be trained with		BioGRID (human)
 80% of total interactions without		BioGRID (human)
 gene expression data as shown		BioGRID (human)
 in Fig. 4. The integration		BioGRID (human)
 of expression data with topological		BioGRID (human)
 properties into GNE model has		BioGRID (human)
 more improvement on E. coli		BioGRID (human)
 than yeast when we train		BioGRID (human)
 with 10% of total interactions		BioGRID (human)
 for each dataset		BioGRID (human)
.  The reason for this is		BioGRID (human)
 likely the difference in the		BioGRID (human)
 number of available interactions for		BioGRID (human)
 yeast and E. coli (Table		BioGRID (human)
 2). This indicates the informativeness		BioGRID (human)
 of gene expression when we		BioGRID (human)
 have few interactions and supports		BioGRID (human)
 the idea that the integration		BioGRID (human)
 of expression data with topological		BioGRID (human)
 properties improves gene interaction prediction		BioGRID (human)
.  Impact of λ GNE involves		BioGRID (human)
 the parameter λ that controls		BioGRID (human)
 the impor- tance of gene		BioGRID (human)
 expression information relative to topolog		BioGRID (human)
- ical properties of gene 		BioGRID (human)
network as shown in Eq. 6		BioGRID (human)
. We examine how the 		BioGRID (human)
choice of the parameter λ 		BioGRID (human)
affects our method’s performance. Figure 5 shows the comparison of our		BioGRID (human)
 method’s performance with different values		BioGRID (human)
 of λ when GNE is		BioGRID (human)
 trained on varying percentage of		BioGRID (human)
 total interactions		BioGRID (human)
.  We evaluate the impact of		BioGRID (human)
 λ on range [0, 0.2		BioGRID (human)
, 0.4, 0.6, 0.8, 1]. 		BioGRID (human)
When λ becomes 0, the 		BioGRID (human)
learned representations model only topological 		BioGRID (human)
properties. In contrast, setting the 		BioGRID (human)
high value for λ makes 		BioGRID (human)
GNE learn only from attributes 		BioGRID (human)
and degrades its performance. Therefore, 		BioGRID (human)
our model performs well when 		BioGRID (human)
λ is within [0, 1].  Figure 5 shows that the		BioGRID (human)
 integration of expression data improves		BioGRID (human)
 the performance of GNE to		BioGRID (human)
 predict gene interac- tions. Impact		BioGRID (human)
 of λ depends on the		BioGRID (human)
 number of interactions used to		BioGRID (human)
 train GNE. If GNE is		BioGRID (human)
 trained with few interactions, integration		BioGRID (human)
 of expression data with topological		BioGRID (human)
 properties plays a vital role		BioGRID (human)
 in predicting missing interactions. As		BioGRID (human)
 the number of training interactions		BioGRID (human)
 increases, integration of expression data		BioGRID (human)
 has less impact but still		BioGRID (human)
 improves the performance over only		BioGRID (human)
 topological properties		BioGRID (human)
.  Figures 4 and 5 demonstrate		BioGRID (human)
 that the expression data contributes		BioGRID (human)
 the increase in AUROC by		BioGRID (human)
 nearly 0.14 when interactions are		BioGRID (human)
 less than 40% for yeast		BioGRID (human)
 and about 0.32 when interactions		BioGRID (human)
 are less than 10% for		BioGRID (human)
 E. coli. More topo- logical		BioGRID (human)
 properties and attributes are required		BioGRID (human)
 for yeast than E. coli		BioGRID (human)
. It may be related 		BioGRID (human)
to the fact that yeast 		BioGRID (human)
is a more complex species 		BioGRID (human)
than E. coli. Moreover, we 		BioGRID (human)
can spec- ulate that more 		BioGRID (human)
topological properties and attributes are  a b		BioGRID (human)
		BioGRID (human)
Fig. 4 AUROC comparison of 		BioGRID (human)
GNE’s performance with respect to 		BioGRID (human)
network sparsity. a yeast b 		BioGRID (human)
E. coli. Integration of expression 		BioGRID (human)
data with topological properties of 		BioGRID (human)
the gene network improves the 		BioGRID (human)
performance for both 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
KC et al. BMC Systems 		BioGRID (human)
Biology 2019, 13(Suppl 2):38 Page 10 of 14		BioGRID (human)
		BioGRID (human)
a b  Fig. 5 Impact of λ		BioGRID (human)
 on GNE’s performance trained with		BioGRID (human)
 different percentages of interactions. a		BioGRID (human)
 yeast b E. coli. Different		BioGRID (human)
 lines indicate performance of GNE		BioGRID (human)
 trained with different percentages of		BioGRID (human)
 interactions		BioGRID (human)
		BioGRID (human)
required for higher eukaryotes like 		BioGRID (human)
humans. In humans, GNE that 		BioGRID (human)
integrates topological properties with attributes 		BioGRID (human)
may be more successful than 		BioGRID (human)
the methods that only use 		BioGRID (human)
either topological properties or attributes.  This demonstrates the sensitivity of		BioGRID (human)
 GNE to parame- ter λ		BioGRID (human)
. This parameter λ has 		BioGRID (human)
a considerable impact on  our method’s performance and should		BioGRID (human)
 be appropriately selected		BioGRID (human)
.  Investigation of GNE’s predictions We		BioGRID (human)
 investigate the predictive ability of		BioGRID (human)
 our model in iden- tifying		BioGRID (human)
 new gene interactions. For this		BioGRID (human)
 aim, we consider		BioGRID (human)
		BioGRID (human)
a b  c d		BioGRID (human)
		BioGRID (human)
Fig. 6 Temporal holdout validation 		BioGRID (human)
in predicting new interactions. Performance 		BioGRID (human)
is measured by the area 		BioGRID (human)
under the ROC curve and 		BioGRID (human)
the area under the precision-recall 		BioGRID (human)
curve. Shown are the performance 		BioGRID (human)
of each method based on 		BioGRID (human)
the AUROC (a, b) and 		BioGRID (human)
AUPR (c, d) for yeast 		BioGRID (human)
and E. coli. The limit 		BioGRID (human)
of the y-axis is adjusted 		BioGRID (human)
to [0.5, 1.0] for the 		BioGRID (human)
precision-recall curve to make the 		BioGRID (human)
difference in performance more visible. 		BioGRID (human)
GNE outperforms LINE and 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
KC et al. BMC Systems 		BioGRID (human)
Biology 2019, 13(Suppl 2):38 Page 11 of 14		BioGRID (human)
		BioGRID (human)
Table 5 AUROC and AUPR 		BioGRID (human)
comparision for temporal holdout validation  Methods Yeast E. coli		BioGRID (human)
		BioGRID (human)
AUROC AUPR AUROC AUPR  LINE 0.620 0.611 0.569 0.598		BioGRID (human)
		BioGRID (human)
node2vec 0.640 0.609 0.587 0.599  GNE (our model) 0.710 0.683		BioGRID (human)
 0.653 0.658		BioGRID (human)
		BioGRID (human)
Note that method that achieves 		BioGRID (human)
the best performance is bold 		BioGRID (human)
faced  two versions of BioGRID interaction		BioGRID (human)
 datasets at two dif- ferent		BioGRID (human)
 time points (2017 and 2018		BioGRID (human)
 version), where the older version		BioGRID (human)
 is used for training and		BioGRID (human)
 the newer one is used		BioGRID (human)
 for testing the model (temporal		BioGRID (human)
 holdout validation). The 2018 version		BioGRID (human)
 contains 12,835 new interactions for		BioGRID (human)
 yeast and 11,185 new interactions		BioGRID (human)
 for E. coli than the		BioGRID (human)
 2017 ver- sion. GNE’s performance		BioGRID (human)
 trained with 50% and 80		BioGRID (human)
% of total interactions are 		BioGRID (human)
comparable for both yeast and 		BioGRID (human)
E. coli (Figs. 4 and 5		BioGRID (human)
). We thus train our 		BioGRID (human)
model with 50% of total 		BioGRID (human)
interactions from the 2017 version 		BioGRID (human)
to learn the embed- dings 		BioGRID (human)
for genes and demonstrate the 		BioGRID (human)
impact of integrating expression data 		BioGRID (human)
with topological properties. We create 		BioGRID (human)
the test set with new 		BioGRID (human)
interactions from the 2018 version 		BioGRID (human)
of BioGRID as positive interactions 		BioGRID (human)
and the equal number of 		BioGRID (human)
negative interactions randomly sampled. We 		BioGRID (human)
make pre- dictions for these 		BioGRID (human)
interactions using learned embeddings and 		BioGRID (human)
create a list of (Gene 		BioGRID (human)
vi, Gene vj, probability), ranked 		BioGRID (human)
by the predicted probability. We 		BioGRID (human)
consider predicted gene pairs with 		BioGRID (human)
the probabilities of 0.5 or 		BioGRID (human)
higher but are miss- ing 		BioGRID (human)
from BioGRID for further investigation 		BioGRID (human)
as we discuss later in 		BioGRID (human)
this section.  The temporal holdout performance of		BioGRID (human)
 our model in comparison to		BioGRID (human)
 other methods is shown in		BioGRID (human)
 Fig. 6. We		BioGRID (human)
		BioGRID (human)
observe that GNE outperforms both 		BioGRID (human)
node2vec and LINE in temporal 		BioGRID (human)
holdout validation across both yeast 		BioGRID (human)
and E. coli datasets, indicating 		BioGRID (human)
GNE can accurately predict new 		BioGRID (human)
genetic interactions. Table 5 shows 		BioGRID (human)
that GNE achieves substantial improvement 		BioGRID (human)
of 7.0 (AUROC), 7.4 (AUPR) 		BioGRID (human)
on yeast and 6.6 (AUROC), 5		BioGRID (human)
.9 (AUPR) on E. coli 		BioGRID (human)
datasets.  Table 6 shows the top		BioGRID (human)
 5 interactions with the sig		BioGRID (human)
- nificant increase in predicted 		BioGRID (human)
probability for both yeast and 		BioGRID (human)
E. coli after expression data 		BioGRID (human)
is integrated. We also provide 		BioGRID (human)
literature evidence with experimental evidence 		BioGRID (human)
code obtained from the BioGRID 		BioGRID (human)
database [26] sup- porting these 		BioGRID (human)
predictions. BioGRID compiles interaction data 		BioGRID (human)
from numerous publications through comprehen- 		BioGRID (human)
sive curation efforts. Taking new 		BioGRID (human)
interactions added to BioGRID (version 3		BioGRID (human)
.4.158) into consideration, we evalu- 		BioGRID (human)
ate the probability of these 		BioGRID (human)
interactions predicted by GNE trained 		BioGRID (human)
with and without expression data. 		BioGRID (human)
Specifically, integration of expression data 		BioGRID (human)
increases the probability of 8331 (		BioGRID (human)
out of 11,185) interactions for 		BioGRID (human)
E. coli (improving AUROC from 0		BioGRID (human)
.606 to 0.662) and 6,010 (		BioGRID (human)
out of 12,835) interactions for 		BioGRID (human)
yeast (improving AUROC from 0.685 		BioGRID (human)
to 0.707). Integration of topology 		BioGRID (human)
and expression data sig- nificantly 		BioGRID (human)
increases the probabilities of true 		BioGRID (human)
interactions between genes (Table 6).  To further evaluate GNE’s predictions		BioGRID (human)
, we consider the new 		BioGRID (human)
version of BioGRID (version 3.4.162) 		BioGRID (human)
and evaluate 2609 yeast gene 		BioGRID (human)
pairs (Additional file 1: Table 		BioGRID (human)
S1) and 871 E. coli 		BioGRID (human)
gene pairs (Additional file 2: 		BioGRID (human)
Table S2) predicted by GNE 		BioGRID (human)
with the probabilities of 0.5 		BioGRID (human)
or higher. We find that 128 (5%) yeast gene pairs and		BioGRID (human)
 78 (9%) E. coli gene		BioGRID (human)
 pairs are true interactions that		BioGRID (human)
 have been added to the		BioGRID (human)
 lat- est release of BioGRID		BioGRID (human)
. We then evaluate the 		BioGRID (human)
predictive ability of GNE by 		BioGRID (human)
calculating the percentage of true 		BioGRID (human)
inter- actions with regard to 		BioGRID (human)
different probability bins (Fig. 7). 		BioGRID (human)
Sixteen percent of predicted yeast 		BioGRID (human)
gene pairs and 17.5%  Table 6 New gene interactions		BioGRID (human)
 that are assigned high probability		BioGRID (human)
 by GNE		BioGRID (human)
		BioGRID (human)
Organism Probability  Gene i Gene j Experimental		BioGRID (human)
 evidence code Topology Topology		BioGRID (human)
 + Expression		BioGRID (human)
		BioGRID (human)
Yeast  0.287 0.677 TFC8 DHH1 Affinity		BioGRID (human)
 Capture-RNA [35		BioGRID (human)
]  0.394 0.730 SYH1 DHH1 Affinity		BioGRID (human)
 Capture-RNA [35		BioGRID (human)
]  0.413 0.746 CPR7 DHH1 Affinity		BioGRID (human)
 Capture-RNA [35		BioGRID (human)
]  0.253 0.551 MRP10 DHH1 Affinity		BioGRID (human)
 Capture-RNA [35		BioGRID (human)
]  0.542 0.835 RPS13 ULP2 Affinity		BioGRID (human)
 Capture-MS [36		BioGRID (human)
]  E. coli		BioGRID (human)
		BioGRID (human)
0.014 0.944 ATPB RFBC Affinity 		BioGRID (human)
Capture-MS [37]  0.012 0.941 NARQ CYDB Affinity		BioGRID (human)
 Capture-MS [37		BioGRID (human)
]  0.013 0.937 PCNB PAND Affinity		BioGRID (human)
 Capture-MS [37		BioGRID (human)
]  0.015 0.939 FLIF CHEY Affinity		BioGRID (human)
 Capture-MS [37		BioGRID (human)
]  0.017 0.938 YCHM PROB Affinity		BioGRID (human)
 Capture-MS [37		BioGRID (human)
]  New gene interactions on 2018		BioGRID (human)
 version that are assigned high		BioGRID (human)
 probability by GNE after integration		BioGRID (human)
 of expression data. We provide		BioGRID (human)
 probability predicted by GNE (with/without		BioGRID (human)
 expression data) for new interactions		BioGRID (human)
 in the 2018 version and		BioGRID (human)
 evidence supporting the existence of		BioGRID (human)
 predicted interactions		BioGRID (human)
		BioGRID (human)
KC et al. BMC Systems 		BioGRID (human)
Biology 2019, 13(Suppl 2):38 Page 12 of 14		BioGRID (human)
		BioGRID (human)
Fig. 7 The percentage of 		BioGRID (human)
true interactions from GNE’s predictions 		BioGRID (human)
with different probability bins. a 		BioGRID (human)
yeast b E. coli. We 		BioGRID (human)
divide the gene pairs based 		BioGRID (human)
on their predicted probabilities to 		BioGRID (human)
different probability ranges (as shown 		BioGRID (human)
in the x-axis) and identify 		BioGRID (human)
the number of predicted true 		BioGRID (human)
interactions in each range. Each 		BioGRID (human)
bar indicates the percentage of 		BioGRID (human)
true interactions out of predicted 		BioGRID (human)
gene pairs in that probability 		BioGRID (human)
range  of predicted E. coli gene		BioGRID (human)
 pairs with the probability higher		BioGRID (human)
 than 0.9 are true interactions		BioGRID (human)
. This suggests that gene 		BioGRID (human)
pairs with high probability predicted 		BioGRID (human)
by GNE are more likely 		BioGRID (human)
to be true interactions.  To support our finding that		BioGRID (human)
 GNE predicted gene pairs have		BioGRID (human)
 high value, we manually check		BioGRID (human)
 gene pairs that have high		BioGRID (human)
 predicted probability but are missing		BioGRID (human)
 from the latest BioGRID release		BioGRID (human)
. We find that these 		BioGRID (human)
gene pairs interact with the 		BioGRID (human)
same set of other genes. 		BioGRID (human)
For example, GNE pre- dicts 		BioGRID (human)
the interaction between YDR311W and 		BioGRID (human)
YGL122C with the probability of 0		BioGRID (human)
.968. Mining BioGRID database, we 		BioGRID (human)
find that these genes interact 		BioGRID (human)
with the same set of 374 genes. Similarly, E. coli genes		BioGRID (human)
 DAMX and FLIL with the		BioGRID (human)
 predicted probability of 0.998 share		BioGRID (human)
 320 interacting genes. In this		BioGRID (human)
 way, we identify all interacting		BioGRID (human)
 genes shared by each of		BioGRID (human)
 the predicted gene pairs in		BioGRID (human)
 yeast and E. coli (Additional		BioGRID (human)
 file 1: Table S1 and		BioGRID (human)
 Additional file 2		BioGRID (human)
:  Table S2). Figure 8 shows		BioGRID (human)
 the average number of inter		BioGRID (human)
- acting genes shared by 		BioGRID (human)
a gene pair. In general, 		BioGRID (human)
gene pairs with a high 		BioGRID (human)
GNE probability tend to have 		BioGRID (human)
a large number of interacting 		BioGRID (human)
genes. For example, gene pairs 		BioGRID (human)
with the probability greater than 0		BioGRID (human)
.9 have, on average, 82 		BioGRID (human)
common interacting genes for yeast 		BioGRID (human)
and 58 for E. coli. 		BioGRID (human)
Two sample t-test analysis has 		BioGRID (human)
shown that there is a 		BioGRID (human)
significant difference in the number 		BioGRID (human)
of shared inter- acting genes 		BioGRID (human)
with respect to different probability 		BioGRID (human)
bins (Table 7).  Moreover, we search the literature		BioGRID (human)
 to see if we can		BioGRID (human)
 find supporting evidence for predicted		BioGRID (human)
 interactions. We find literature evidence		BioGRID (human)
 for an interaction between YCL032W		BioGRID (human)
 (STE50) and YDL035C (GPR1), which		BioGRID (human)
 has the probability of 0.98		BioGRID (human)
 predicted by GNE. STE50 is		BioGRID (human)
 an adaptor that links G-protein		BioGRID (human)
 complex in cell signalling, and		BioGRID (human)
 GPR1 is a G- protein		BioGRID (human)
 coupled receptor. Both STE50 and		BioGRID (human)
 GPR1 share a		BioGRID (human)
		BioGRID (human)
Fig. 8 The average number 		BioGRID (human)
of common interacting genes between 		BioGRID (human)
the gene pairs predicted by 		BioGRID (human)
GNE. a yeast b E. 		BioGRID (human)
coli. We divide gene pairs 		BioGRID (human)
into different probability groups based 		BioGRID (human)
on predicted probabilities by GNE 		BioGRID (human)
and compute the number of 		BioGRID (human)
common interacting genes shared by 		BioGRID (human)
these gene pairs. We categorize 		BioGRID (human)
these gene pairs into different 		BioGRID (human)
probability ranges (as shown in 		BioGRID (human)
the x-axis). Each bar represents 		BioGRID (human)
the average number of common 		BioGRID (human)
interacting genes shared by gene 		BioGRID (human)
pairs in each probability 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
KC et al. BMC Systems 		BioGRID (human)
Biology 2019, 13(Suppl 2):38 Page 13 of 14		BioGRID (human)
		BioGRID (human)
Table 7 Results of two-sample 		BioGRID (human)
t-test  Probability bin for Sample A		BioGRID (human)
		BioGRID (human)
Probability bin for Sample B  p-value for yeast		BioGRID (human)
		BioGRID (human)
p-value for E. coli  0.5 - 0.6 0.6		BioGRID (human)
 - 0.7 9.2e − 14		BioGRID (human)
 6.9e − 02 0.5		BioGRID (human)
 - 0.6 0.7 - 0.8		BioGRID (human)
 3.02e − 51 1.23e		BioGRID (human)
 − 05 0.5 - 0.6		BioGRID (human)
 0.8 - 0.9 6.1e		BioGRID (human)
 − 117 7.4e − 14		BioGRID (human)
 0.5 - 0.6 0.9		BioGRID (human)
 - 1.0 2.1e − 177		BioGRID (human)
 3.7e − 39 0.6		BioGRID (human)
 - 0.7 0.7 - 0.8		BioGRID (human)
 8.2e − 17 1.1e		BioGRID (human)
 − 02 0.6 - 0.7		BioGRID (human)
 0.8 - 0.9 3.5e		BioGRID (human)
 − 69 9.2e − 09		BioGRID (human)
 0.6 - 0.7 0.9		BioGRID (human)
 - 1.0 2.1e − 128		BioGRID (human)
 1.9e − 30 0.7		BioGRID (human)
 - 0.8 0.8 - 0.9		BioGRID (human)
 4.7e − 28 4.8e		BioGRID (human)
 − 04 0.7 - 0.8		BioGRID (human)
 0.9 - 1.0 6.2e		BioGRID (human)
 − 87 7.4e − 23		BioGRID (human)
 0.8 - 0.9 0.9		BioGRID (human)
 - 1.0 4.3e − 35		BioGRID (human)
 5.1e − 13 We divide		BioGRID (human)
 gene pairs into different probability		BioGRID (human)
 groups based on predicted probabilities		BioGRID (human)
 by GNE and compute the		BioGRID (human)
 number of common interacting genes		BioGRID (human)
 shared by these gene pairs		BioGRID (human)
. Significance test shows there 		BioGRID (human)
is the significant difference between 		BioGRID (human)
average number of shared genes 		BioGRID (human)
in different probability bins  common function of cell signalling		BioGRID (human)
 via G-protein. Besides, STE50p interacts		BioGRID (human)
 with STE11p in the two-hybrid		BioGRID (human)
 system, which is a cell-based		BioGRID (human)
 system examining protein-protein interactions [34		BioGRID (human)
]. Also, BioGRID has evidence 		BioGRID (human)
of 30 physi- cal and 4 genetic associations between STE50 and		BioGRID (human)
 STE11. Thus, STE50 is highly		BioGRID (human)
 likely to interact with STE11		BioGRID (human)
, which in turn interacts 		BioGRID (human)
with GPR1.  This analysis demonstrates the potential		BioGRID (human)
 of our method in the		BioGRID (human)
 discovery of gene interactions. Also		BioGRID (human)
, GNE can help the 		BioGRID (human)
curator to identify interactions with 		BioGRID (human)
strong potential that need to 		BioGRID (human)
be looked at with experimental 		BioGRID (human)
validation or within the literature.  Conclusion We developed a novel		BioGRID (human)
 deep learning framework, namely GNE		BioGRID (human)
 to perform gene network embedding		BioGRID (human)
. Specifi- cally, we design 		BioGRID (human)
deep neural network architecture to 		BioGRID (human)
model the complex statistical relationships 		BioGRID (human)
between gene interaction network and 		BioGRID (human)
expression data. GNE is flexible 		BioGRID (human)
to the addition of different 		BioGRID (human)
types and num- ber of 		BioGRID (human)
attributes. The features learned by 		BioGRID (human)
GNE allow us to use 		BioGRID (human)
out-of-the-box machine learning classifiers like 		BioGRID (human)
Logistic Regression to predict gene 		BioGRID (human)
interactions accurately.  GNE relies on a deep		BioGRID (human)
 learning technique that can learn		BioGRID (human)
 the underlying patterns of gene		BioGRID (human)
 interactions by integrat- ing heterogeneous		BioGRID (human)
 data and extracts features that		BioGRID (human)
 are more informative for interaction		BioGRID (human)
 prediction. Experimental results show that		BioGRID (human)
 GNE achieve better performance in		BioGRID (human)
 gene interaction prediction over other		BioGRID (human)
 baseline approaches in both yeast		BioGRID (human)
 and E. coli organisms. Also		BioGRID (human)
, GNE can help the 		BioGRID (human)
curator to identify the interactions 		BioGRID (human)
that need to be looked 		BioGRID (human)
at.  As future work, we aim		BioGRID (human)
 to study the impact of		BioGRID (human)
 inte- grating other sources of		BioGRID (human)
 information about gene such as		BioGRID (human)
 transcription factor binding sites, functional		BioGRID (human)
 annota- tions (from gene ontology		BioGRID (human)
), gene sequences, metabolic pathways, 		BioGRID (human)
etc. into GNE in predicting 		BioGRID (human)
gene interaction.  Additional files		BioGRID (human)
		BioGRID (human)
Additional file 1: Table S1. 		BioGRID (human)
Includes yeast gene pairs predicted 		BioGRID (human)
by GNE with probabilities of 0		BioGRID (human)
.5 or higher. (XLSX 109 		BioGRID (human)
kb)  Additional file 2: Table S2		BioGRID (human)
. Includes E. coli gene 		BioGRID (human)
pairs predicted by GNE with 		BioGRID (human)
probabilities of 0.5 or higher. 		BioGRID (human)
Rows marked with yellow color 		BioGRID (human)
indicate predicted interaction is true 		BioGRID (human)
based on latest version 3.4.162 		BioGRID (human)
of BioGRID interaction dataset released 		BioGRID (human)
on June 2018. (XLSX 43.7 		BioGRID (human)
kb)  Abbreviations AUPR: Area under the		BioGRID (human)
 PR curve; AUROC: Area under		BioGRID (human)
 the ROC curve; DOOR: Database		BioGRID (human)
 of Prokaryotic Operons; DREAM: Dialogue		BioGRID (human)
 on reverse Engineering assessment and		BioGRID (human)
 methods; ELU: Exponential linear unit		BioGRID (human)
; GI: Genetic interaction; GNE: 		BioGRID (human)
Gene network embedding; ID: Identifier; 		BioGRID (human)
KS: Kolmogrov-Smirnov; LINE: Large-scale information 		BioGRID (human)
network embedding; PR: Precision recall; 		BioGRID (human)
ROC: Receiver operator characterstic; t-SNE: 		BioGRID (human)
t-Distributed stochastic neighbor embedding  Acknowledgements Not applicable		BioGRID (human)
.  Funding This work was supported		BioGRID (human)
 by the National Science Foundation		BioGRID (human)
 [1062422 to A.H.] and the		BioGRID (human)
 National Institutes of Health [R15GM116102		BioGRID (human)
 to F.C.]. Publication of this		BioGRID (human)
 article was sponsored by NSF		BioGRID (human)
 grant (1062422		BioGRID (human)
).  Availability of data and materials		BioGRID (human)
 The datasets analysed during the		BioGRID (human)
 current study are publicly available		BioGRID (human)
. The gene expression data 		BioGRID (human)
was downloaded from DREAM5 Network 		BioGRID (human)
Challenge https:// www.synapse.org/#!Synapse:syn2787209/wiki/70351. The interaction 		BioGRID (human)
datasets for yeast and E. 		BioGRID (human)
coli were downloaded from BioGRID 		BioGRID (human)
https://thebiogrid.org.  About this supplement This article		BioGRID (human)
 has been published as part		BioGRID (human)
 of BMC Systems Biology Volume		BioGRID (human)
 13 Supplement 2, 2019: Selected		BioGRID (human)
 articles from the 17th Asia		BioGRID (human)
 Pacific Bioinformatics Conference (APBC 2019		BioGRID (human)
): systems biology. The full 		BioGRID (human)
contents of the supplement are 		BioGRID (human)
available online at https://bmcsystbiol.biomedcentral.com/articles/ supplements/volume-13-supplement-2.  Authors’ contributions KK, RL, FC		BioGRID (human)
, and AH designed the 		BioGRID (human)
research. KK performed the research 		BioGRID (human)
and wrote the manuscript. RL, 		BioGRID (human)
FC, QY, and AH improved 		BioGRID (human)
the draft. All authors read 		BioGRID (human)
and approved the final manuscript.  Ethics approval and consent to		BioGRID (human)
 participate Not applicable		BioGRID (human)
.  Consent for publication Not applicable		BioGRID (human)
.  Competing interests The authors declare		BioGRID (human)
 that they have no competing		BioGRID (human)
 interests		BioGRID (human)
.  Publisher’s Note Springer Nature remains		BioGRID (human)
 neutral with regard to jurisdictional		BioGRID (human)
 claims in published maps and		BioGRID (human)
 institutional affiliations		BioGRID (human)
.  Author details 1Golisano College of		BioGRID (human)
 Computing and Information Sciences, Rochester		BioGRID (human)
 Institute of Technology, 20 Lomb		BioGRID (human)
 Memorial Drive, 14623 Rochester, New		BioGRID (human)
 York, USA. 2Thomas H. Gosnell		BioGRID (human)
 School of Life Sciences, Rochester		BioGRID (human)
 Institute of Technology, 84 Lomb		BioGRID (human)
 Memorial Drive, 14623 Rochester, New		BioGRID (human)
 York, USA		BioGRID (human)
.  Published: 5 April 2019		BioGRID (human)
		BioGRID (human)
https://doi.org/10.1186/s12918-019-0694-y https://doi.org/10.1186/s12918-019-0694-y https://www.synapse.org/#!Synapse:syn2787209/wiki/70351 https://www.synapse.org/#!Synapse:syn2787209/wiki/70351 https://thebiogrid.org 		BioGRID (human)
https://bmcsystbiol.biomedcentral.com/articles/supplements/volume-13-supplement-2 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
13		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
2		BioGRID (human)
		BioGRID (human)
KC et al. BMC Systems 		BioGRID (human)
Biology 2019, 13(Suppl 2):38 Page 14 of 14		BioGRID (human)
		BioGRID (human)
References 1. Mani R, Onge 		BioGRID (human)
RPS, Hartman JL, Giaever G, 		BioGRID (human)
Roth FP. Defining genetic  interaction. Proc Natl Acad Sci		BioGRID (human)
. 2008;105(9):3461–6. 2. Boucher B, 		BioGRID (human)
Jenna S. Genetic interaction networks: 		BioGRID (human)
better understand to  better predict. Front Genet. 2013;4:290		BioGRID (human)
. 3. Lage K. Protein–protein 		BioGRID (human)
interactions and genetic diseases: the  interactome. Biochim Biophys Acta (BBA		BioGRID (human)
) - Mol Basis Dis. 2014		BioGRID (human)
;1842(10): 1971–80.  4. Madhukar NS, Elemento O		BioGRID (human)
, Pandey G. Prediction of 		BioGRID (human)
genetic interactions using machine learning 		BioGRID (human)
and network properties. Front Bioeng 		BioGRID (human)
Biotechnol. 2015;3:172.  5. Oliver S. Proteomics: guilt-by-association		BioGRID (human)
 goes global. Nature. 2000;403(6770):601		BioGRID (human)
.  6. Cho H, Berger B		BioGRID (human)
, Peng J. Compact integration 		BioGRID (human)
of multi-network topology for functional 		BioGRID (human)
analysis of genes. Cell Syst. 2016		BioGRID (human)
;3(6):540–8.  7. Marbach D, Costello JC		BioGRID (human)
, Küffner R, Vega NM, 		BioGRID (human)
Prill RJ, Camacho DM, Allison 		BioGRID (human)
KR, Aderhold A, Bonneau R, 		BioGRID (human)
Chen Y, et al. Wisdom 		BioGRID (human)
of crowds for robust gene 		BioGRID (human)
network inference. Nat Methods. 2012;9(8):796.  8. Li R, KC K		BioGRID (human)
, Cui F, Haake AR. 		BioGRID (human)
Sparse covariance modeling in high 		BioGRID (human)
dimensions with gaussian processes. In: 		BioGRID (human)
Proceedings of The 32nd Conference 		BioGRID (human)
on Neural Information Processing Systems (		BioGRID (human)
NIPS). 2018.  9. Cui P, Wang X		BioGRID (human)
, Pei J, Zhu W. 		BioGRID (human)
A survey on network embedding. 		BioGRID (human)
IEEE Trans Knowl Data Eng. 2018		BioGRID (human)
. arXiv preprint arXiv:1711.08752. IEEE.  10. Lei Y-K, You Z-H		BioGRID (human)
, Ji Z, Zhu L, 		BioGRID (human)
Huang D-S. Assessing and predicting 		BioGRID (human)
protein interactions by combining manifold 		BioGRID (human)
embedding with multiple information integration. 		BioGRID (human)
In: BMC Bioinformatics, vol. 13. 		BioGRID (human)
BioMed Central; 2012. p. 3.  11. Alanis-Lobato G, Cannistraci CV		BioGRID (human)
, Ravasi T. Exploitation of 		BioGRID (human)
genetic interaction network topology for 		BioGRID (human)
the prediction of epistatic behavior. 		BioGRID (human)
Genomics. 2013;102(4):202–8.  12. Tenenbaum JB, De Silva		BioGRID (human)
 V, Langford JC. A global		BioGRID (human)
 geometric framework for nonlinear dimensionality		BioGRID (human)
 reduction. science. 2000;290(5500):2319–23		BioGRID (human)
.  13. Mikolov T, Sutskever I		BioGRID (human)
, Chen K, Corrado GS, 		BioGRID (human)
Dean J. Distributed representations of 		BioGRID (human)
words and phrases and their 		BioGRID (human)
compositionality. In: Advances in Neural 		BioGRID (human)
Information Processing Systems. 2013. p. 3111		BioGRID (human)
–3119.  14. Grover A, Leskovec J		BioGRID (human)
. node2vec: Scalable feature learning 		BioGRID (human)
for networks. In: Proceedings of 		BioGRID (human)
the 22nd ACM SIGKDD International 		BioGRID (human)
Conference on Knowledge Discovery and 		BioGRID (human)
Data Mining. ACM; 2016. p. 855		BioGRID (human)
–864.  15. Tu Y, Stolovitzky G		BioGRID (human)
, Klein U. Quantitative noise 		BioGRID (human)
analysis for gene expression microarray 		BioGRID (human)
experiments. Proc Natl Acad Sci. 2002		BioGRID (human)
;99(22): 14031–6.  16. Tang J, Qu M		BioGRID (human)
, Wang M, Zhang M, 		BioGRID (human)
Yan J, Mei Q. Line: 		BioGRID (human)
Large-scale information network embedding. In: 		BioGRID (human)
Proceedings of the 24th International 		BioGRID (human)
Conference on World Wide Web. 		BioGRID (human)
International World Wide Web Conferences 		BioGRID (human)
Steering Committee; 2015. p. 1067–1077.  17. Snoek CG, Worring M		BioGRID (human)
, Smeulders AW. Early versus 		BioGRID (human)
late fusion in semantic video 		BioGRID (human)
analysis. In: Proceedings of the 		BioGRID (human)
13th Annual ACM International Conference 		BioGRID (human)
on Multimedia. 2005. p. 399–402. 		BioGRID (human)
ACM.  18. He K, Zhang X		BioGRID (human)
, Ren S, Sun J. 		BioGRID (human)
Deep residual learning for image 		BioGRID (human)
recognition. In: Proceedings of the 		BioGRID (human)
IEEE Conference on Computer Vision 		BioGRID (human)
and Pattern Recognition. 2016. p. 770		BioGRID (human)
–778.  19. Pennington J, Socher R		BioGRID (human)
, Manning C. Glove: Global 		BioGRID (human)
vectors for word representation. In: 		BioGRID (human)
Proceedings of the 2014 Conference 		BioGRID (human)
on Empirical Methods in Natural 		BioGRID (human)
Language Processing (EMNLP). 2014. p. 1532		BioGRID (human)
–1543.  20. Levy O, Goldberg Y		BioGRID (human)
, Dagan I. Improving distributional 		BioGRID (human)
similarity with lessons learned from 		BioGRID (human)
word embeddings. Trans Assoc Comput 		BioGRID (human)
Linguist. 2015;3:211–25.  21. Kingma DP, Ba J		BioGRID (human)
. Adam: A method for 		BioGRID (human)
stochastic optimization. In: International Conference 		BioGRID (human)
on Learning Representations; 2015.  22. Duchi J, Hazan E		BioGRID (human)
, Singer Y. Adaptive subgradient 		BioGRID (human)
methods for online learning and 		BioGRID (human)
stochastic optimization. J Mach Learn 		BioGRID (human)
Res. 2011;12(Jul): 2121–59.  23. Tieleman T, Hinton G		BioGRID (human)
. Lecture 6.5-rmsprop, coursera: Neural 		BioGRID (human)
networks for machine learning. University 		BioGRID (human)
of Toronto, Technical Report. 2012.  24. Srivastava N, Hinton G		BioGRID (human)
, Krizhevsky A, Sutskever I, 		BioGRID (human)
Salakhutdinov R. Dropout: A simple 		BioGRID (human)
way to prevent neural networks 		BioGRID (human)
from overfitting. J Mach Learn 		BioGRID (human)
Res. 2014;15(1):1929–58.  25. Ioffe S, Szegedy C		BioGRID (human)
. Batch normalization: Accelerating deep 		BioGRID (human)
network training by reducing internal 		BioGRID (human)
covariate shift. In: International Conference 		BioGRID (human)
on Machine Learning; 2015. p. 448		BioGRID (human)
–56.  26. Stark C, Breitkreutz B-J		BioGRID (human)
, Reguly T, Boucher L, 		BioGRID (human)
Breitkreutz A, Tyers M. Biogrid: 		BioGRID (human)
a general repository for interaction 		BioGRID (human)
datasets. Nucleic Acids Res. 2006;34(suppl_1):535–9.  27. Butte A-J, Kohane I-S		BioGRID (human)
. Mutual information relevance networks: 		BioGRID (human)
functional genomic clustering using pairwise 		BioGRID (human)
entropy measurements. In: Biocomputing 2000. 		BioGRID (human)
World Scientific; 1999. p. 418–429.  28. Chua HN, Sung W-K		BioGRID (human)
, Wong L. Exploiting indirect 		BioGRID (human)
neighbours and topological weight to 		BioGRID (human)
predict protein function from protein–protein 		BioGRID (human)
interactions. Bioinformatics. 2006;22(13):1623–30.  29. Abadi M, Agarwal A		BioGRID (human)
, Barham P, Brevdo E, 		BioGRID (human)
Chen Z, Citro C, Corrado 		BioGRID (human)
GS, Davis A, Dean J, 		BioGRID (human)
Devin M, Ghemawat S, Goodfellow 		BioGRID (human)
I, Harp A, Irving G, 		BioGRID (human)
Isard M, Jia Y, Jozefowicz 		BioGRID (human)
R, Kaiser L, Kudlur M, 		BioGRID (human)
Levenberg J, Mané D., Monga 		BioGRID (human)
R, Moore S, Murray D, 		BioGRID (human)
Olah C, Schuster M, Shlens 		BioGRID (human)
J, Steiner B, Sutskever I, 		BioGRID (human)
Talwar K, Tucker P, Vanhoucke 		BioGRID (human)
V, Vasudevan V, Viégas F., 		BioGRID (human)
Vinyals O, Warden P, Wattenberg 		BioGRID (human)
M, Wicke M, Yu Y, 		BioGRID (human)
Zheng X. TensorFlow: Large-Scale Machine 		BioGRID (human)
Learning on Heterogeneous Systems. Software 		BioGRID (human)
available from tensorflow.org. 2015. https://www.tensorflow.org/ 		BioGRID (human)
Accessed 21 Dec 2016.  30. Clevert D, Unterthiner T		BioGRID (human)
, Hochreiter S. Fast and 		BioGRID (human)
accurate deep network learning by 		BioGRID (human)
exponential linear units (elus). In: 		BioGRID (human)
International Conference on Learning Representations; 2016		BioGRID (human)
.  31. Davis J, Goadrich M		BioGRID (human)
. The relationship between precision-recall 		BioGRID (human)
and roc curves. In: Proceedings 		BioGRID (human)
of the 23rd International Conference 		BioGRID (human)
on Machine Learning. ACM; 2006. 		BioGRID (human)
p. 233–240.  32. Maaten L. v. d		BioGRID (human)
., Hinton G. Visualizing data 		BioGRID (human)
using t-sne. J Mach Learn 		BioGRID (human)
Res. 2008;9(Nov):2579–605.  33. Mao F, Dam P		BioGRID (human)
, Chou J, Olman V, 		BioGRID (human)
Xu Y. Door: a database 		BioGRID (human)
for prokaryotic operons. Nucleic Acids 		BioGRID (human)
Res. 2008;37(suppl_1):459–63.  34. Gustin MC, Albertyn J		BioGRID (human)
, Alexander M, Davenport K. 		BioGRID (human)
Map kinase pathways in the 		BioGRID (human)
yeastsaccharomyces cerevisiae. Microbiol Mol Biol 		BioGRID (human)
Rev. 1998;62(4): 1264–300.  35. Miller JE, Zhang L		BioGRID (human)
, Jiang H, Li Y, 		BioGRID (human)
Pugh BF, Reese JC. Genome-wide 		BioGRID (human)
mapping of decay factor–mrna interactions 		BioGRID (human)
in yeast identifies nutrient-responsive transcripts 		BioGRID (human)
as targets of the deadenylase 		BioGRID (human)
ccr4. G3: Genes, Genomes, Genet. 2018		BioGRID (human)
;8(1):315–30.  36. Liang J, Singh N		BioGRID (human)
, Carlson CR, Albuquerque CP, 		BioGRID (human)
Corbett KD, Zhou H. Recruitment 		BioGRID (human)
of a sumo isopeptidase to 		BioGRID (human)
rdna stabilizes silencing complexes by 		BioGRID (human)
opposing sumo targeted ubiquitin ligase 		BioGRID (human)
activity. Genes Dev. 2017;31(8):802–15.  37. Babu M, Bundalovic-Torma C		BioGRID (human)
, Calmettes C, Phanse S, 		BioGRID (human)
Zhang Q, Jiang Y, Minic 		BioGRID (human)
Z, Kim S, Mehla J, 		BioGRID (human)
Gagarinova A, et al. Global 		BioGRID (human)
landscape of cell envelope protein 		BioGRID (human)
complexes in escherichia coli. Nat 		BioGRID (human)
Biotechnol. 2018;36(1):103.  https://www.tensorflow.org		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
Gene network embedding (GNE) 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
Gene network structure 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
Gene expression 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
GNE 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
Parameter 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
Experimental 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
Results and 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
Analysis of gene 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
Gene interaction 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
Impact of network 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
Impact 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
Investigation of GNE's 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
Additional 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
Additional file 1		BioGRID (human)
		BioGRID (human)
Additional file 2		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
Availability of data and 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
About this 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
Authors' 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
Ethics approval and consent to 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
Consent for 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
Competing 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
Publisher's 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
Author 		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
		BioGRID (human)
interaction network data from the BioGRID database [26] and gene expression	BioGRID	BioGRID (human)
use two interaction datasets from BioGRID database (2017 released version 3.4.153	BioGRID	BioGRID (human)
of the interaction datasets from BioGRID and the gene expression data	BioGRID	BioGRID (human)
two versions of BioGRID interaction datasets at two dif	BioGRID	BioGRID (human)
from the 2018 version of BioGRID as positive interactions and the	BioGRID	BioGRID (human)
but are miss- ing from BioGRID for further investigation as we	BioGRID	BioGRID (human)
evidence code obtained from the BioGRID database [26] sup- porting these	BioGRID	BioGRID (human)
predictions. BioGRID compiles interaction data from numerous	BioGRID	BioGRID (human)
Taking new interactions added to BioGRID (version 3.4.158) into consideration, we	BioGRID	BioGRID (human)
consider the new version of BioGRID (version 3.4.162) and evaluate 2609	BioGRID	BioGRID (human)
the lat- est release of BioGRID	BioGRID	BioGRID (human)
are missing from the latest BioGRID release. We find that these	BioGRID	BioGRID (human)
the probability of 0.968. Mining BioGRID database, we find that these	BioGRID	BioGRID (human)
examining protein-protein interactions [34]. Also, BioGRID has evidence of 30 physi	BioGRID	BioGRID (human)
on latest version 3.4.162 of BioGRID interaction dataset released on June	BioGRID	BioGRID (human)
E. coli were downloaded from BioGRID https://thebiogrid.org	BioGRID	BioGRID (human)
mel-spectrogram generation. Experiments on the LJSpeech dataset show that our parallel	LJSpeech	LJSpeech
We conduct experiments on the LJSpeech dataset to test FastSpeech. The	LJSpeech	LJSpeech
We conduct experiments on LJSpeech dataset [10], which contains 13,100	LJSpeech	LJSpeech
a duration predictor. Experiments on LJSpeech dataset demonstrate that our proposed	LJSpeech	LJSpeech
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 1		EYEDIAP (screen target)
		EYEDIAP (screen target)
Recurrent CNN for 3D Gaze 		EYEDIAP (screen target)
Estimation using Appearance and Shape 		EYEDIAP (screen target)
Cues  Cristina Palmero1,2		EYEDIAP (screen target)
		EYEDIAP (screen target)
crpalmec7@alumnes.ub.edu  1 Dept. Mathematics and Informatics		EYEDIAP (screen target)
 Universitat de Barcelona, Spain		EYEDIAP (screen target)
		EYEDIAP (screen target)
Javier Selva1  javier.selva.castello@est.fib.upc.edu		EYEDIAP (screen target)
		EYEDIAP (screen target)
2 Computer Vision Center Campus 		EYEDIAP (screen target)
UAB, Bellaterra, Spain  Mohammad Ali Bagheri3,4		EYEDIAP (screen target)
		EYEDIAP (screen target)
mohammadali.bagheri@ucalgary.ca  3 Dept. Electrical and Computer		EYEDIAP (screen target)
 Eng. University of Calgary, Canada		EYEDIAP (screen target)
		EYEDIAP (screen target)
Sergio Escalera1,2  sergio@maia.ub.es		EYEDIAP (screen target)
		EYEDIAP (screen target)
4 Dept. Engineering University of 		EYEDIAP (screen target)
Larestan, Iran  Abstract		EYEDIAP (screen target)
		EYEDIAP (screen target)
Gaze behavior is an important 		EYEDIAP (screen target)
non-verbal cue in social signal 		EYEDIAP (screen target)
processing and human- computer interaction. 		EYEDIAP (screen target)
In this paper, we tackle 		EYEDIAP (screen target)
the problem of person- and 		EYEDIAP (screen target)
head pose- independent 3D gaze 		EYEDIAP (screen target)
estimation from remote cameras, using 		EYEDIAP (screen target)
a multi-modal recurrent convolutional neural 		EYEDIAP (screen target)
network (CNN). We propose to 		EYEDIAP (screen target)
combine face, eyes region, and 		EYEDIAP (screen target)
face landmarks as individual streams 		EYEDIAP (screen target)
in a CNN to estimate 		EYEDIAP (screen target)
gaze in still images. Then, 		EYEDIAP (screen target)
we exploit the dynamic nature 		EYEDIAP (screen target)
of gaze by feeding the 		EYEDIAP (screen target)
learned features of all the 		EYEDIAP (screen target)
frames in a sequence to 		EYEDIAP (screen target)
a many-to-one recurrent module that 		EYEDIAP (screen target)
predicts the 3D gaze vector 		EYEDIAP (screen target)
of the last frame. Our 		EYEDIAP (screen target)
multi-modal static solution is evaluated 		EYEDIAP (screen target)
on a wide range of 		EYEDIAP (screen target)
head poses and gaze directions, 		EYEDIAP (screen target)
achieving a significant improvement of 14		EYEDIAP (screen target)
.6% over the state of 		EYEDIAP (screen target)
the art on EYEDIAP dataset, 		EYEDIAP (screen target)
further improved by 4% when 		EYEDIAP (screen target)
the temporal modality is included.  1 Introduction Eyes and their		EYEDIAP (screen target)
 movements are considered an important		EYEDIAP (screen target)
 cue in non-verbal behavior analysis		EYEDIAP (screen target)
, being involved in many 		EYEDIAP (screen target)
cognitive processes and reflecting our 		EYEDIAP (screen target)
internal state [17]. More specifically, 		EYEDIAP (screen target)
eye gaze behavior, as an 		EYEDIAP (screen target)
indicator of human visual attention, 		EYEDIAP (screen target)
has been widely studied to 		EYEDIAP (screen target)
assess communication skills [28] and 		EYEDIAP (screen target)
to identify possible behavioral 		EYEDIAP (screen target)
disorders [9]. Therefore, gaze estimation 		EYEDIAP (screen target)
has become an established line 		EYEDIAP (screen target)
of research in computer vision, 		EYEDIAP (screen target)
being a key feature in 		EYEDIAP (screen target)
human-computer interaction (HCI) and usability 		EYEDIAP (screen target)
research [12, 20].  Recent gaze estimation research has		EYEDIAP (screen target)
 focused on facilitating its use		EYEDIAP (screen target)
 in general everyday applications under		EYEDIAP (screen target)
 real-world conditions, using off-the-shelf remote		EYEDIAP (screen target)
 RGB cameras and re- moving		EYEDIAP (screen target)
 the need of personal calibration		EYEDIAP (screen target)
 [26]. In this setting, appearance-based		EYEDIAP (screen target)
 methods, which learn a mapping		EYEDIAP (screen target)
 from images to gaze directions		EYEDIAP (screen target)
, are the preferred 		EYEDIAP (screen target)
choice [25]. How- ever, they 		EYEDIAP (screen target)
need large amounts of training 		EYEDIAP (screen target)
data to be able to 		EYEDIAP (screen target)
generalize well to in-the-wild situations, 		EYEDIAP (screen target)
which are characterized by significant 		EYEDIAP (screen target)
variability in head poses, face 		EYEDIAP (screen target)
appearances and lighting conditions. In 		EYEDIAP (screen target)
recent years, CNNs have been 		EYEDIAP (screen target)
reported to outperform classical methods. 		EYEDIAP (screen target)
However, most existing approaches have 		EYEDIAP (screen target)
only been tested in restricted 		EYEDIAP (screen target)
HCI tasks,  c© 2018. The copyright of		EYEDIAP (screen target)
 this document resides with its		EYEDIAP (screen target)
 authors. It may be distributed		EYEDIAP (screen target)
 unchanged freely in print or		EYEDIAP (screen target)
 electronic forms		EYEDIAP (screen target)
.  ar X		EYEDIAP (screen target)
		EYEDIAP (screen target)
iv :1  80 5		EYEDIAP (screen target)
.  03 06		EYEDIAP (screen target)
		EYEDIAP (screen target)
4v 3		EYEDIAP (screen target)
		EYEDIAP (screen target)
cs  .C V		EYEDIAP (screen target)
		EYEDIAP (screen target)
1  7		EYEDIAP (screen target)
		EYEDIAP (screen target)
Se  p		EYEDIAP (screen target)
		EYEDIAP (screen target)
20  18		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Liversedge and Findlay} 2000		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Rutter and Durkin} 1987		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Guillon, Hadjikhani, Baduel, 		EYEDIAP (screen target)
and Rog{é}} 2014  Citation Citation {Jacob and Karn		EYEDIAP (screen target)
} 2003  Citation Citation {Majaranta and Bulling		EYEDIAP (screen target)
} 2014  Citation Citation {Palmero, van Dam		EYEDIAP (screen target)
, Escalera, Kelia, Lichtert, Noldus, 		EYEDIAP (screen target)
Spink, and van Wieringen} 2018  Citation Citation {Ono, Okabe, and		EYEDIAP (screen target)
 Sato} 2006		EYEDIAP (screen target)
		EYEDIAP (screen target)
2 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (screen target)
ESTIMATION  Method 3D gaze direction		EYEDIAP (screen target)
		EYEDIAP (screen target)
Unrestricted gaze target  Full face		EYEDIAP (screen target)
		EYEDIAP (screen target)
Eye region  Facial landmarks		EYEDIAP (screen target)
		EYEDIAP (screen target)
Sequential information  Zhang et al. (1) [42		EYEDIAP (screen target)
] 3 7 7 3 7 7 Krafka et al. [16		EYEDIAP (screen target)
] 7 7 3 3 7 7 Zhang et al. (2		EYEDIAP (screen target)
) [43] 3 7 3 7 7 7 Deng and Zhu		EYEDIAP (screen target)
 [4] 3 3 3 3		EYEDIAP (screen target)
 7 7 Ours 3 3		EYEDIAP (screen target)
 3 3 3 3		EYEDIAP (screen target)
		EYEDIAP (screen target)
Table 1: Characteristics of recent 		EYEDIAP (screen target)
related work on person- and 		EYEDIAP (screen target)
head pose-independent appearance-based gaze estimation 		EYEDIAP (screen target)
methods using CNNs.  where users look at the		EYEDIAP (screen target)
 screen or mobile phone, showing		EYEDIAP (screen target)
 a low head pose variability		EYEDIAP (screen target)
. It is yet unclear 		EYEDIAP (screen target)
how these methods would perform 		EYEDIAP (screen target)
in a wider range of 		EYEDIAP (screen target)
head poses.  On a different note, until		EYEDIAP (screen target)
 very recently, the majority of		EYEDIAP (screen target)
 methods only used static eye		EYEDIAP (screen target)
 region appearance as input. State-of-the-art		EYEDIAP (screen target)
 approaches have demonstrated that using		EYEDIAP (screen target)
 the face along with a		EYEDIAP (screen target)
 higher resolution image of the		EYEDIAP (screen target)
 eyes [16], or even just		EYEDIAP (screen target)
 the face itself [43], increases		EYEDIAP (screen target)
 performance. Indeed, the whole-face image		EYEDIAP (screen target)
 encodes more information than eyes		EYEDIAP (screen target)
 alone, such as illumination and		EYEDIAP (screen target)
 head pose. Nevertheless, gaze behavior		EYEDIAP (screen target)
 is not static. Eye and		EYEDIAP (screen target)
 head movements allow us to		EYEDIAP (screen target)
 direct our gaze to target		EYEDIAP (screen target)
 locations of interest. It has		EYEDIAP (screen target)
 been demonstrated that humans can		EYEDIAP (screen target)
 better predict gaze when being		EYEDIAP (screen target)
 shown image sequences of other		EYEDIAP (screen target)
 people moving their eyes [1		EYEDIAP (screen target)
]. However, it is still 		EYEDIAP (screen target)
an open question whether this 		EYEDIAP (screen target)
se- quential information can increase 		EYEDIAP (screen target)
the performance of automatic methods.  In this work, we show		EYEDIAP (screen target)
 that the combination of multiple		EYEDIAP (screen target)
 cues benefits the gaze estimation		EYEDIAP (screen target)
 task. In particular, we use		EYEDIAP (screen target)
 face, eye region and facial		EYEDIAP (screen target)
 landmarks from still images. Facial		EYEDIAP (screen target)
 landmarks model the global shape		EYEDIAP (screen target)
 of the face and come		EYEDIAP (screen target)
 at no cost, since face		EYEDIAP (screen target)
 alignment is a common pre-processing		EYEDIAP (screen target)
 step in many facial image		EYEDIAP (screen target)
 analysis approaches. Furthermore, we present		EYEDIAP (screen target)
 a subject-independent, free-head recurrent 3D		EYEDIAP (screen target)
 gaze regression network to leverage		EYEDIAP (screen target)
 the temporal information of image		EYEDIAP (screen target)
 sequences. The static streams of		EYEDIAP (screen target)
 each frame are combined in		EYEDIAP (screen target)
 a late-fusion fashion using a		EYEDIAP (screen target)
 multi-stream CNN. Then, all feature		EYEDIAP (screen target)
 vectors are input to a		EYEDIAP (screen target)
 many-to-one recurrent module that predicts		EYEDIAP (screen target)
 the gaze vector of the		EYEDIAP (screen target)
 last sequence frame		EYEDIAP (screen target)
.  In summary, our contributions are		EYEDIAP (screen target)
 two-fold. First, we present a		EYEDIAP (screen target)
 Recurrent-CNN net- work architecture that		EYEDIAP (screen target)
 combines appearance, shape and temporal		EYEDIAP (screen target)
 information for 3D gaze estimation		EYEDIAP (screen target)
. Second, we test static 		EYEDIAP (screen target)
and temporal versions of our 		EYEDIAP (screen target)
solution on the EYEDIAP 		EYEDIAP (screen target)
dataset [7] in a wide 		EYEDIAP (screen target)
range of head poses and 		EYEDIAP (screen target)
gaze directions, showing consistent perfor- 		EYEDIAP (screen target)
mance improvements compared to related 		EYEDIAP (screen target)
appearance-based methods. To the best 		EYEDIAP (screen target)
of our knowledge, this is 		EYEDIAP (screen target)
the first third-person, remote camera-based 		EYEDIAP (screen target)
approach that uses tempo- ral 		EYEDIAP (screen target)
information for this task. Table 1 outlines our main method characteristics		EYEDIAP (screen target)
 compared to related work. Models		EYEDIAP (screen target)
 and code are publicly available		EYEDIAP (screen target)
 at https://github.com/ crisie/RecurrentGaze		EYEDIAP (screen target)
.  2 Related work Gaze estimation		EYEDIAP (screen target)
 methods are typically categorized as		EYEDIAP (screen target)
 model-based or appearance-based [5, 10		EYEDIAP (screen target)
, 15]. Model-based approaches use 		EYEDIAP (screen target)
a geometric model of the 		EYEDIAP (screen target)
eye, usually requir- ing either 		EYEDIAP (screen target)
high resolution images or a 		EYEDIAP (screen target)
person-specific calibration stage to estimate 		EYEDIAP (screen target)
personal eye parameters [22, 33, 34, 37, 41]. In contrast, appearance-based		EYEDIAP (screen target)
 methods learn a di- rect		EYEDIAP (screen target)
 mapping from intensity images or		EYEDIAP (screen target)
 extracted eye features to gaze		EYEDIAP (screen target)
 directions, thus being		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Zhang, Sugano, Fritz, 		EYEDIAP (screen target)
and Bulling} 2015  Citation Citation {Krafka, Khosla, Kellnhofer		EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 		EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Deng and Zhu		EYEDIAP (screen target)
} 2017  Citation Citation {Krafka, Khosla, Kellnhofer		EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 		EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Anderson, Risko, and		EYEDIAP (screen target)
 Kingstone} 2016		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Funesprotect unhbox voidb@x 		EYEDIAP (screen target)
penalty @M  {}Mora, Monay, and Odobez} 2014		EYEDIAP (screen target)
{}  Citation Citation {Ferhat and Vilari{ñ}o		EYEDIAP (screen target)
} 2016  Citation Citation {Hansen and Ji		EYEDIAP (screen target)
} 2010  Citation Citation {Kar and Corcoran		EYEDIAP (screen target)
} 2017  Citation Citation {Morimoto, Amir, and		EYEDIAP (screen target)
 Flickner} 2002		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Venkateswarlu etprotect unhbox 		EYEDIAP (screen target)
voidb@x penalty @M  {}al.} 2003		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Wang and Ji} 2017		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Wood and Bulling} 2014		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Yoo and Chung} 2005		EYEDIAP (screen target)
		EYEDIAP (screen target)
https://github.com/crisie/RecurrentGaze 		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 3		EYEDIAP (screen target)
		EYEDIAP (screen target)
potentially applicable to relatively low 		EYEDIAP (screen target)
resolution images and mid-distance scenarios. 		EYEDIAP (screen target)
Dif- ferent mapping functions have 		EYEDIAP (screen target)
been explored, such as neural 		EYEDIAP (screen target)
networks [2], adaptive linear regression (		EYEDIAP (screen target)
ALR) [19], local interpolation [32], 		EYEDIAP (screen target)
gaussian processes [30, 35], random 		EYEDIAP (screen target)
forests [11, 31], or k-nearest 		EYEDIAP (screen target)
neighbors [40]. Main challenges of 		EYEDIAP (screen target)
appearance-based methods for 3D gaze 		EYEDIAP (screen target)
estimation are head pose, illumination 		EYEDIAP (screen target)
and subject invariance without user-specific 		EYEDIAP (screen target)
calibration. To handle these issues, 		EYEDIAP (screen target)
some works proposed compensation 		EYEDIAP (screen target)
methods [18] and warping strategies 		EYEDIAP (screen target)
that synthesize a canonical, frontal 		EYEDIAP (screen target)
looking view of the 		EYEDIAP (screen target)
face [6, 13, 21]. Hybrid 		EYEDIAP (screen target)
approaches based on analysis-by-synthesis have 		EYEDIAP (screen target)
also been evaluated [39].  Currently, data-driven methods are considered		EYEDIAP (screen target)
 the state of the art		EYEDIAP (screen target)
 for person- and head pose-independent		EYEDIAP (screen target)
 appearance-based gaze estimation. Consequently, a		EYEDIAP (screen target)
 number of gaze es- timation		EYEDIAP (screen target)
 datasets have been introduced in		EYEDIAP (screen target)
 recent years, either in controlled		EYEDIAP (screen target)
 [29] or semi- controlled settings		EYEDIAP (screen target)
 [8], in the wild [16		EYEDIAP (screen target)
, 42], or consisting of 		EYEDIAP (screen target)
synthetic data [31, 38, 40]. 		EYEDIAP (screen target)
Zhang et al. [42] showed 		EYEDIAP (screen target)
that CNNs can outperform other 		EYEDIAP (screen target)
mapping methods, using a multi- 		EYEDIAP (screen target)
modal CNN to learn the 		EYEDIAP (screen target)
mapping from 3D head poses 		EYEDIAP (screen target)
and eye images to 3D 		EYEDIAP (screen target)
gaze directions. Krafka et 		EYEDIAP (screen target)
al. [16] proposed a multi-stream 		EYEDIAP (screen target)
CNN for 2D gaze estimation, 		EYEDIAP (screen target)
using individual eye, whole-face image 		EYEDIAP (screen target)
and the face grid as 		EYEDIAP (screen target)
input. As this method was 		EYEDIAP (screen target)
limited to 2D screen mapping, 		EYEDIAP (screen target)
Zhang et al. [43] later 		EYEDIAP (screen target)
explored the potential of just 		EYEDIAP (screen target)
using whole-face images as input 		EYEDIAP (screen target)
to estimate 3D gaze directions. 		EYEDIAP (screen target)
Using a spatial weights CNN, 		EYEDIAP (screen target)
they demonstrated their method to 		EYEDIAP (screen target)
be more robust to facial 		EYEDIAP (screen target)
appearance variation caused by head 		EYEDIAP (screen target)
pose and illumina- tion than 		EYEDIAP (screen target)
eye-only methods. While the method 		EYEDIAP (screen target)
was evaluated in the wild, 		EYEDIAP (screen target)
the subjects were only interacting 		EYEDIAP (screen target)
with a mobile device, thus 		EYEDIAP (screen target)
restricting the head pose range. 		EYEDIAP (screen target)
Deng and Zhu [4] presented 		EYEDIAP (screen target)
a two-stream CNN to disjointly 		EYEDIAP (screen target)
model head pose from face 		EYEDIAP (screen target)
images and eye- ball movement 		EYEDIAP (screen target)
from eye region images. Both 		EYEDIAP (screen target)
were then aggregated into 3D 		EYEDIAP (screen target)
gaze direction using a gaze 		EYEDIAP (screen target)
transform layer. The decomposition was 		EYEDIAP (screen target)
aimed to avoid head-correlation over- 		EYEDIAP (screen target)
fitting of previous data-driven approaches. 		EYEDIAP (screen target)
They evaluated their approach in 		EYEDIAP (screen target)
the wild with a wider 		EYEDIAP (screen target)
range of head poses, obtaining 		EYEDIAP (screen target)
better performance than previous eye-based 		EYEDIAP (screen target)
methods. However, they did not 		EYEDIAP (screen target)
test it on public annotated 		EYEDIAP (screen target)
benchmark datasets.  In this paper, we propose		EYEDIAP (screen target)
 a multi-stream recurrent CNN network		EYEDIAP (screen target)
 for person- and head pose-independent		EYEDIAP (screen target)
 3D gaze estimation for a		EYEDIAP (screen target)
 mid-distance scenario. We evaluate it		EYEDIAP (screen target)
 on a wider range of		EYEDIAP (screen target)
 head poses and gaze directions		EYEDIAP (screen target)
 than screen-targeted approaches. As opposed		EYEDIAP (screen target)
 to previous methods, we also		EYEDIAP (screen target)
 rely on temporal information inherent		EYEDIAP (screen target)
 in sequential data		EYEDIAP (screen target)
.  3 Methodology		EYEDIAP (screen target)
		EYEDIAP (screen target)
In this section, we present 		EYEDIAP (screen target)
our approach for 3D gaze 		EYEDIAP (screen target)
regression based on appearance and 		EYEDIAP (screen target)
shape cues for still images 		EYEDIAP (screen target)
and image sequences. First, we 		EYEDIAP (screen target)
introduce the data modalities and 		EYEDIAP (screen target)
formulate the problem. Then, we 		EYEDIAP (screen target)
detail the normalization procedure prior 		EYEDIAP (screen target)
to the regression stage. Finally, 		EYEDIAP (screen target)
we explain the global network 		EYEDIAP (screen target)
topology as well as the 		EYEDIAP (screen target)
implementation details. An overview of 		EYEDIAP (screen target)
the system architecture is depicted 		EYEDIAP (screen target)
in Figure 1.  3.1 Multi-modal gaze regression		EYEDIAP (screen target)
		EYEDIAP (screen target)
Let us represent gaze direction 		EYEDIAP (screen target)
as a 3D unit vector 		EYEDIAP (screen target)
g = [gx,gy,gz]T ∈R3 in 		EYEDIAP (screen target)
the Camera Coor- dinate System (		EYEDIAP (screen target)
CCS), whose origin is the 		EYEDIAP (screen target)
central point between eyeball centers. 		EYEDIAP (screen target)
Assuming a calibrated camera, and 		EYEDIAP (screen target)
a known head position and 		EYEDIAP (screen target)
orientation, our goal is to 		EYEDIAP (screen target)
estimate g from a sequence 		EYEDIAP (screen target)
of images {I(i) | 		EYEDIAP (screen target)
I ∈ RW×H×3} as a 		EYEDIAP (screen target)
regression problem.  Citation Citation {Baluja and Pomerleau		EYEDIAP (screen target)
} 1994  Citation Citation {Lu, Sugano, Okabe		EYEDIAP (screen target)
, and Sato} 2011{}  Citation Citation {Tan, Kriegman, and		EYEDIAP (screen target)
 Ahuja} 2002		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Sugano, Matsushita, and 		EYEDIAP (screen target)
Sato} 2013  Citation Citation {Williams, Blake, and		EYEDIAP (screen target)
 Cipolla} 2006		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Huang, Veeraraghavan, and 		EYEDIAP (screen target)
Sabharwal} 2017  Citation Citation {Sugano, Matsushita, and		EYEDIAP (screen target)
 Sato} 2014		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Wood, Baltru{²}aitis, Morency, 		EYEDIAP (screen target)
Robinson, and Bulling} 2016{}  Citation Citation {Lu, Okabe, Sugano		EYEDIAP (screen target)
, and Sato} 2011{}  Citation Citation {Funes-Mora and Odobez		EYEDIAP (screen target)
} 2016  Citation Citation {Jeni and Cohn		EYEDIAP (screen target)
} 2016  Citation Citation {Mora and Odobez		EYEDIAP (screen target)
} 2012  Citation Citation {Wood, Baltru{²}aitis, Morency		EYEDIAP (screen target)
, Robinson, and Bulling} 2016{}  Citation Citation {Smith, Yin, Feiner		EYEDIAP (screen target)
, and Nayar} 2013  Citation Citation {Funesprotect unhbox voidb@x		EYEDIAP (screen target)
 penalty @M		EYEDIAP (screen target)
		EYEDIAP (screen target)
Mora, Monay, and Odobez} 2014{}  Citation Citation {Krafka, Khosla, Kellnhofer		EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 		EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (screen target)
, and Bulling} 2015  Citation Citation {Sugano, Matsushita, and		EYEDIAP (screen target)
 Sato} 2014		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Wood, Baltrusaitis, Zhang, 		EYEDIAP (screen target)
Sugano, Robinson, and Bulling} 2015  Citation Citation {Wood, Baltru{²}aitis, Morency		EYEDIAP (screen target)
, Robinson, and Bulling} 2016{}  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (screen target)
, and Bulling} 2015  Citation Citation {Krafka, Khosla, Kellnhofer		EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 		EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Deng and Zhu		EYEDIAP (screen target)
} 2017		EYEDIAP (screen target)
		EYEDIAP (screen target)
4 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (screen target)
ESTIMATION  Conv		EYEDIAP (screen target)
		EYEDIAP (screen target)
C on ca t  x y z x y		EYEDIAP (screen target)
 z x y z		EYEDIAP (screen target)
		EYEDIAP (screen target)
Individual Fusion Temporal  Individual Fusion		EYEDIAP (screen target)
		EYEDIAP (screen target)
Input 		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
Individual Fusion  Normalization		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
 .Conv		EYEDIAP (screen target)
		EYEDIAP (screen target)
Conv .  Conv		EYEDIAP (screen target)
		EYEDIAP (screen target)
Conv .  FC		EYEDIAP (screen target)
		EYEDIAP (screen target)
FC FC RNN  RNN		EYEDIAP (screen target)
		EYEDIAP (screen target)
RNN FC  Ti m e		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
Figure 1: Overview of the 		EYEDIAP (screen target)
proposed network. A multi-stream CNN 		EYEDIAP (screen target)
jointly models full-face, eye region 		EYEDIAP (screen target)
appearance and face landmarks from 		EYEDIAP (screen target)
still images. The combined extracted 		EYEDIAP (screen target)
fea- tures from each frame 		EYEDIAP (screen target)
are fed into a recurrent 		EYEDIAP (screen target)
module to predict last frame’s 		EYEDIAP (screen target)
gaze direction.  Gazing to a specific target		EYEDIAP (screen target)
 is achieved by a combination		EYEDIAP (screen target)
 of eye and head movements		EYEDIAP (screen target)
, which are highly coordinated. 		EYEDIAP (screen target)
Consequently, the apparent direction of 		EYEDIAP (screen target)
gaze is influenced not only 		EYEDIAP (screen target)
by the location of the 		EYEDIAP (screen target)
irises within the eyelid aperture, 		EYEDIAP (screen target)
but also by the position 		EYEDIAP (screen target)
and orientation of the face 		EYEDIAP (screen target)
with respect to the camera. 		EYEDIAP (screen target)
Known as the Wollaston 		EYEDIAP (screen target)
effect [36], the exact same 		EYEDIAP (screen target)
set of eyes may appear 		EYEDIAP (screen target)
to be looking in different 		EYEDIAP (screen target)
directions due to the surrounding 		EYEDIAP (screen target)
facial cues. It is therefore 		EYEDIAP (screen target)
reasonable to state that eye 		EYEDIAP (screen target)
images are not sufficient to 		EYEDIAP (screen target)
estimate gaze direction. Instead, whole-face 		EYEDIAP (screen target)
images can encode head pose 		EYEDIAP (screen target)
or illumination-specific information across larger 		EYEDIAP (screen target)
areas than those available just 		EYEDIAP (screen target)
in the eyes region [16, 43		EYEDIAP (screen target)
].  The drawback of appearance-only methods		EYEDIAP (screen target)
 is that global structure information		EYEDIAP (screen target)
 is not explicitly considered. In		EYEDIAP (screen target)
 that sense, facial landmarks can		EYEDIAP (screen target)
 be used as global shape		EYEDIAP (screen target)
 cues to en- code spatial		EYEDIAP (screen target)
 relationships and geometric constraints. Current		EYEDIAP (screen target)
 state-of-the-art face alignment approaches are		EYEDIAP (screen target)
 robust enough to handle large		EYEDIAP (screen target)
 appearance variability, extreme head poses		EYEDIAP (screen target)
 and occlusions, being especially useful		EYEDIAP (screen target)
 when the dataset used for		EYEDIAP (screen target)
 gaze estimation does not contain		EYEDIAP (screen target)
 such variability. Facial landmarks are		EYEDIAP (screen target)
 mainly correlated with head orientation		EYEDIAP (screen target)
, eye position, eyelid openness, 		EYEDIAP (screen target)
and eyebrow movement, which are 		EYEDIAP (screen target)
valuable features for our task.  Therefore, in our approach we		EYEDIAP (screen target)
 jointly model appearance and shape		EYEDIAP (screen target)
 cues (see Figure 1). The		EYEDIAP (screen target)
 former is represented by a		EYEDIAP (screen target)
 whole-face image IF , along		EYEDIAP (screen target)
 with a higher resolution image		EYEDIAP (screen target)
 of the eyes IE to		EYEDIAP (screen target)
 identify subtle changes. Due to		EYEDIAP (screen target)
 dealing with wide head pose		EYEDIAP (screen target)
 ranges, some eye images may		EYEDIAP (screen target)
 not depict the whole eye		EYEDIAP (screen target)
, containing mostly background or 		EYEDIAP (screen target)
other surrounding facial parts instead. 		EYEDIAP (screen target)
For that reason, and contrary 		EYEDIAP (screen target)
to previous approaches that only 		EYEDIAP (screen target)
use one eye image [31, 42		EYEDIAP (screen target)
], we use a single 		EYEDIAP (screen target)
image composed of two patches 		EYEDIAP (screen target)
of centered left and right 		EYEDIAP (screen target)
eyes. Finally, the shape cue 		EYEDIAP (screen target)
is represented by 3D face 		EYEDIAP (screen target)
landmarks obtained from a 68-landmark 		EYEDIAP (screen target)
model, denoted by 		EYEDIAP (screen target)
L = {(lx, ly, 		EYEDIAP (screen target)
		EYEDIAP (screen target)
)		EYEDIAP (screen target)
		EYEDIAP (screen target)
 | ∀c ∈ [1, ...,68		EYEDIAP (screen target)
]}.  In this work we also		EYEDIAP (screen target)
 consider the dynamic component of		EYEDIAP (screen target)
 gaze. We leverage the se		EYEDIAP (screen target)
- quential information of eye 		EYEDIAP (screen target)
and head movements such that, 		EYEDIAP (screen target)
given appearance and shape features 		EYEDIAP (screen target)
of consecutive frames, it is 		EYEDIAP (screen target)
possible to better predict the 		EYEDIAP (screen target)
gaze direction of the cur- 		EYEDIAP (screen target)
rent frame. Therefore, the 3D 		EYEDIAP (screen target)
gaze estimation task for a 1		EYEDIAP (screen target)
-frame sequence is formulated  Citation Citation {Wollaston etprotect unhbox		EYEDIAP (screen target)
 voidb@x penalty @M		EYEDIAP (screen target)
		EYEDIAP (screen target)
al.} 1824  Citation Citation {Krafka, Khosla, Kellnhofer		EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 		EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Sugano, Matsushita, and		EYEDIAP (screen target)
 Sato} 2014		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Zhang, Sugano, Fritz, 		EYEDIAP (screen target)
and Bulling} 2015		EYEDIAP (screen target)
		EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 5		EYEDIAP (screen target)
		EYEDIAP (screen target)
as g(i) = f ( {IF (i)},{IE (i)},{L(i		EYEDIAP (screen target)
)}  ) , where i denotes		EYEDIAP (screen target)
 the i-th frame, and f		EYEDIAP (screen target)
 is the regression		EYEDIAP (screen target)
		EYEDIAP (screen target)
function.  3.2 Data normalization Prior to		EYEDIAP (screen target)
 gaze regression, a normalization step		EYEDIAP (screen target)
 in the 3D space and		EYEDIAP (screen target)
 the 2D image, similar to		EYEDIAP (screen target)
 [31], is carried out. This		EYEDIAP (screen target)
 is performed to reduce the		EYEDIAP (screen target)
 appearance variability and to allow		EYEDIAP (screen target)
 the gaze estimation model to		EYEDIAP (screen target)
 be applied regardless of the		EYEDIAP (screen target)
 original camera configuration		EYEDIAP (screen target)
.  Let H ∈ R3x3 be		EYEDIAP (screen target)
 the head rotation matrix, and		EYEDIAP (screen target)
 p = [px, py, pz]T		EYEDIAP (screen target)
 ∈ R3 the reference face		EYEDIAP (screen target)
 location with respect to the		EYEDIAP (screen target)
 original CCS. The goal is		EYEDIAP (screen target)
 to find the conversion matrix		EYEDIAP (screen target)
 M = SR such that		EYEDIAP (screen target)
 (a) the X-axes of the		EYEDIAP (screen target)
 virtual camera and the head		EYEDIAP (screen target)
 become parallel using the rotation		EYEDIAP (screen target)
 matrix R, and (b) the		EYEDIAP (screen target)
 virtual camera looks at the		EYEDIAP (screen target)
 reference location from a fixed		EYEDIAP (screen target)
 distance dn using the Z-direction		EYEDIAP (screen target)
 scaling matrix S = diag(1,1,dn/‖p		EYEDIAP (screen target)
‖). R is computed as 		EYEDIAP (screen target)
a = p̂×HT e1, 		EYEDIAP (screen target)
b = â× p̂, 		EYEDIAP (screen target)
R = [â, b̂, p̂]T , where e1 denotes the first		EYEDIAP (screen target)
 orthonormal basis and		EYEDIAP (screen target)
 〈 ·̂ 〉 is the		EYEDIAP (screen target)
 unit vector		EYEDIAP (screen target)
.  This normalization translates into the		EYEDIAP (screen target)
 image space as a cropped		EYEDIAP (screen target)
 image patch of size Wn×Hn		EYEDIAP (screen target)
 centered at p where head		EYEDIAP (screen target)
 roll rotation has been removed		EYEDIAP (screen target)
. This is done by 		EYEDIAP (screen target)
applying a perspective warping to 		EYEDIAP (screen target)
the input image I using 		EYEDIAP (screen target)
the transformation matrix W = 		EYEDIAP (screen target)
CoMCn−1, where Co and Cn 		EYEDIAP (screen target)
are the original and virtual 		EYEDIAP (screen target)
camera matrices, respectively.  The 3D gaze vector is		EYEDIAP (screen target)
 also normalized as gn =Rg		EYEDIAP (screen target)
. After image normalization, the 		EYEDIAP (screen target)
line of sight can be 		EYEDIAP (screen target)
represented in a 2D space. 		EYEDIAP (screen target)
Therefore, gn is further transformed 		EYEDIAP (screen target)
to spherical coor- dinates (θ ,		EYEDIAP (screen target)
φ) assuming unit length, where 		EYEDIAP (screen target)
θ and φ denote the 		EYEDIAP (screen target)
horizontal and vertical direc- tion 		EYEDIAP (screen target)
angles, respectively. This 2D angle 		EYEDIAP (screen target)
representation, delimited in the 		EYEDIAP (screen target)
range [−π/2,π/2], is computed as 		EYEDIAP (screen target)
θ = arctan(gx/gz) and 		EYEDIAP (screen target)
φ = arcsin(−gy), such that (0,		EYEDIAP (screen target)
0) represents looking straight ahead 		EYEDIAP (screen target)
to the CCS origin.  3.3 Recurrent Convolutional Neural Network		EYEDIAP (screen target)
 We propose a Recurrent CNN		EYEDIAP (screen target)
 Regression Network for 3D gaze		EYEDIAP (screen target)
 estimation. The network is divided		EYEDIAP (screen target)
 in 3 modules: (1) Individual		EYEDIAP (screen target)
, (2) Fusion, and (3) 		EYEDIAP (screen target)
Temporal.  First, the Individual module learns		EYEDIAP (screen target)
 features from each appearance cue		EYEDIAP (screen target)
 separately. It consists of a		EYEDIAP (screen target)
 two-stream CNN, one devoted to		EYEDIAP (screen target)
 the normalized face image stream		EYEDIAP (screen target)
 and the other to the		EYEDIAP (screen target)
 joint normalized eyes image. Next		EYEDIAP (screen target)
, the Fusion module combines 		EYEDIAP (screen target)
the extracted features of each 		EYEDIAP (screen target)
appearance stream in a single 		EYEDIAP (screen target)
vector along with the normalized 		EYEDIAP (screen target)
landmark coordinates. Then, it learns 		EYEDIAP (screen target)
a joint representation between modalities 		EYEDIAP (screen target)
in a late-fusion fashion. Both 		EYEDIAP (screen target)
Individual and Fusion modules, further 		EYEDIAP (screen target)
referred to as Static model, 		EYEDIAP (screen target)
are applied to each frame 		EYEDIAP (screen target)
of the sequence. Finally, the 		EYEDIAP (screen target)
resulting feature vectors of each 		EYEDIAP (screen target)
frame are input to the 		EYEDIAP (screen target)
Temporal module based on a 		EYEDIAP (screen target)
many-to-one recurrent network. This module 		EYEDIAP (screen target)
leverages sequential information to predict 		EYEDIAP (screen target)
the normalized 2D gaze angles 		EYEDIAP (screen target)
of the last frame of 		EYEDIAP (screen target)
the sequence using a linear 		EYEDIAP (screen target)
regression layer added on top 		EYEDIAP (screen target)
of it.  3.4 Implementation details 3.4.1 Network		EYEDIAP (screen target)
 details		EYEDIAP (screen target)
		EYEDIAP (screen target)
Each stream of the Individual 		EYEDIAP (screen target)
module is based on the 		EYEDIAP (screen target)
VGG-16 deep network [27], consisting 		EYEDIAP (screen target)
of 13 convolutional layers, 5 		EYEDIAP (screen target)
max pooling layers, and 1 		EYEDIAP (screen target)
fully connected (FC) layer with 		EYEDIAP (screen target)
Rec- tified Linear Unit (ReLU) 		EYEDIAP (screen target)
activations. The full-face stream follows 		EYEDIAP (screen target)
the same configuration  Citation Citation {Sugano, Matsushita, and		EYEDIAP (screen target)
 Sato} 2014		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Parkhi, Vedaldi, and 		EYEDIAP (screen target)
Zisserman} 2015		EYEDIAP (screen target)
		EYEDIAP (screen target)
6 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (screen target)
ESTIMATION  as the base network, having		EYEDIAP (screen target)
 an input of 224×224 pixels		EYEDIAP (screen target)
 and a 4096D FC layer		EYEDIAP (screen target)
. In contrast, the input 		EYEDIAP (screen target)
joint eye image is smaller, 		EYEDIAP (screen target)
with a final size of 120		EYEDIAP (screen target)
×48 pixels, so the number 		EYEDIAP (screen target)
of pa- rameters is decreased 		EYEDIAP (screen target)
proportionally. In this case, its 		EYEDIAP (screen target)
last FC layer produces a 		EYEDIAP (screen target)
1536D vector. A 204D landmark 		EYEDIAP (screen target)
coordinates vector is concatenated to 		EYEDIAP (screen target)
the output of the FC 		EYEDIAP (screen target)
layer of each stream, resulting 		EYEDIAP (screen target)
in a 5836D feature vector. 		EYEDIAP (screen target)
Consequently, the Fusion module consists 		EYEDIAP (screen target)
of 2 5836D FC layers 		EYEDIAP (screen target)
with ReLU activations and 2 		EYEDIAP (screen target)
dropout layers between FCs as 		EYEDIAP (screen target)
regularization. Finally, to model the 		EYEDIAP (screen target)
temporal dependencies, we use a 		EYEDIAP (screen target)
single GRU layer with 128 		EYEDIAP (screen target)
units.  The network is trained in		EYEDIAP (screen target)
 a stage-wise fashion. First, we		EYEDIAP (screen target)
 train the Static model and		EYEDIAP (screen target)
 the final regression layer end-to-end		EYEDIAP (screen target)
 on each individual frame of		EYEDIAP (screen target)
 the training data. The convolutional		EYEDIAP (screen target)
 blocks are pre-trained with the		EYEDIAP (screen target)
 VGG-Face dataset [27], whereas the		EYEDIAP (screen target)
 FCs are trained from scratch		EYEDIAP (screen target)
. Second, the training data 		EYEDIAP (screen target)
is re-arranged by means of 		EYEDIAP (screen target)
a sliding window with stride 1 to build input sequences. Each		EYEDIAP (screen target)
 sequence is composed of s		EYEDIAP (screen target)
 = 4 consecutive frames, whose		EYEDIAP (screen target)
 gaze direction target is the		EYEDIAP (screen target)
 gaze direction of the last		EYEDIAP (screen target)
 frame of the sequence( {I(i−s+1		EYEDIAP (screen target)
), . . . ,I(i)}, 		EYEDIAP (screen target)
g(i)  ) . Using this re-arranged		EYEDIAP (screen target)
 training data, we extract features		EYEDIAP (screen target)
 of each		EYEDIAP (screen target)
		EYEDIAP (screen target)
frame of the sequence from 		EYEDIAP (screen target)
a frozen Individual module, fine-tune 		EYEDIAP (screen target)
the Fusion layers, and train 		EYEDIAP (screen target)
both, the Temporal module and 		EYEDIAP (screen target)
a new final regression layer 		EYEDIAP (screen target)
from scratch. This way, the 		EYEDIAP (screen target)
network can exploit the temporal 		EYEDIAP (screen target)
information to further refine the 		EYEDIAP (screen target)
fusion weights.  We trained the model using		EYEDIAP (screen target)
 ADAM optimizer with an initial		EYEDIAP (screen target)
 learning rate of 0.0001, dropout		EYEDIAP (screen target)
 of 0.3, and batch size		EYEDIAP (screen target)
 of 64 frames. The number		EYEDIAP (screen target)
 of epochs was experimentally set		EYEDIAP (screen target)
 to 21 for the first		EYEDIAP (screen target)
 training stage and 10 for		EYEDIAP (screen target)
 the second. We use the		EYEDIAP (screen target)
 average Euclidean distance between the		EYEDIAP (screen target)
 predicted and ground-truth 3D gaze		EYEDIAP (screen target)
 vectors as loss function		EYEDIAP (screen target)
.  3.4.2 Input pre-processing		EYEDIAP (screen target)
		EYEDIAP (screen target)
For this work we use 		EYEDIAP (screen target)
head pose and eye locations 		EYEDIAP (screen target)
in the 3D scene provided 		EYEDIAP (screen target)
by the dataset. The 3D 		EYEDIAP (screen target)
landmarks are extracted using the 		EYEDIAP (screen target)
state-of-the-art method of Bulat and 		EYEDIAP (screen target)
Tzimiropou- los [3], which is 		EYEDIAP (screen target)
based on stacked hourglass 		EYEDIAP (screen target)
networks [24].  During training, the original image		EYEDIAP (screen target)
 is pre-processed to get the		EYEDIAP (screen target)
 two normalized input images. The		EYEDIAP (screen target)
 normalized whole-face patch is centered		EYEDIAP (screen target)
 0.1 meters ahead of the		EYEDIAP (screen target)
 head center in the head		EYEDIAP (screen target)
 coordinate system, and Cn is		EYEDIAP (screen target)
 defined such that the image		EYEDIAP (screen target)
 has size of 250× 250		EYEDIAP (screen target)
 pixels. The difference between this		EYEDIAP (screen target)
 size and the final input		EYEDIAP (screen target)
 size allows us to perform		EYEDIAP (screen target)
 random cropping and zooming to		EYEDIAP (screen target)
 augment the data (explained in		EYEDIAP (screen target)
 Section 4.1). Similarly, each normalized		EYEDIAP (screen target)
 eye patch is centered in		EYEDIAP (screen target)
 their respective eye center locations		EYEDIAP (screen target)
. In this case, the 		EYEDIAP (screen target)
virtual camera matrix is defined 		EYEDIAP (screen target)
so that the image is 		EYEDIAP (screen target)
cropped to 70×58, while in 		EYEDIAP (screen target)
practice the final patches have 		EYEDIAP (screen target)
size of 60×48. Landmarks are 		EYEDIAP (screen target)
normalized using the same procedure 		EYEDIAP (screen target)
and further pre-processed with mean 		EYEDIAP (screen target)
subtraction and min-max normalization per 		EYEDIAP (screen target)
axis. Finally, we divide them 		EYEDIAP (screen target)
by a scaling factor w 		EYEDIAP (screen target)
such that all coordinates are 		EYEDIAP (screen target)
in the range [0,w]. This 		EYEDIAP (screen target)
way, all concatenated feature values 		EYEDIAP (screen target)
are in a similar range. 		EYEDIAP (screen target)
After inference, the predicted normalized 		EYEDIAP (screen target)
2D angles are de-normalized back 		EYEDIAP (screen target)
to the original 3D space.  4 Experiments In this section		EYEDIAP (screen target)
, we evaluate the cross-subject 		EYEDIAP (screen target)
3D gaze estimation task on 		EYEDIAP (screen target)
a wide range of head 		EYEDIAP (screen target)
poses and gaze directions. Furthermore, 		EYEDIAP (screen target)
we validate the effectiveness of 		EYEDIAP (screen target)
the proposed architecture comparing both 		EYEDIAP (screen target)
static and temporal approaches. We 		EYEDIAP (screen target)
report the error in terms 		EYEDIAP (screen target)
of mean angular error between 		EYEDIAP (screen target)
predicted and ground-truth 3D gaze 		EYEDIAP (screen target)
vectors. Note that due to 		EYEDIAP (screen target)
the requirements of the temporal 		EYEDIAP (screen target)
model not all the frames 		EYEDIAP (screen target)
obtain a prediction. Therefore, for 		EYEDIAP (screen target)
a  Citation Citation {Parkhi, Vedaldi, and		EYEDIAP (screen target)
 Zisserman} 2015		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Bulat and Tzimiropoulos} 2017		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Newell, Yang, and 		EYEDIAP (screen target)
Deng} 2016		EYEDIAP (screen target)
		EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 7		EYEDIAP (screen target)
		EYEDIAP (screen target)
60 30 0 30 60  60		EYEDIAP (screen target)
		EYEDIAP (screen target)
30  0		EYEDIAP (screen target)
		EYEDIAP (screen target)
30  60		EYEDIAP (screen target)
		EYEDIAP (screen target)
100  101		EYEDIAP (screen target)
		EYEDIAP (screen target)
102  60 30 0 30 60		EYEDIAP (screen target)
		EYEDIAP (screen target)
60  30		EYEDIAP (screen target)
		EYEDIAP (screen target)
0  30		EYEDIAP (screen target)
		EYEDIAP (screen target)
60  100		EYEDIAP (screen target)
		EYEDIAP (screen target)
101  102		EYEDIAP (screen target)
		EYEDIAP (screen target)
103  60 30 0 30 60		EYEDIAP (screen target)
		EYEDIAP (screen target)
60  30		EYEDIAP (screen target)
		EYEDIAP (screen target)
0  30		EYEDIAP (screen target)
		EYEDIAP (screen target)
60  100		EYEDIAP (screen target)
		EYEDIAP (screen target)
101  102		EYEDIAP (screen target)
		EYEDIAP (screen target)
60 30 0 30 60  60		EYEDIAP (screen target)
		EYEDIAP (screen target)
30  0		EYEDIAP (screen target)
		EYEDIAP (screen target)
30  60		EYEDIAP (screen target)
		EYEDIAP (screen target)
100  101		EYEDIAP (screen target)
		EYEDIAP (screen target)
102  103		EYEDIAP (screen target)
		EYEDIAP (screen target)
a) g (FT ) (b) 		EYEDIAP (screen target)
h (FT ) (c) g (		EYEDIAP (screen target)
CS) (d) h (CS)  Figure 2: Ground-truth eye gaze		EYEDIAP (screen target)
 g and head orientation h		EYEDIAP (screen target)
 distribution on the filtered EYE		EYEDIAP (screen target)
- DIAP dataset for CS 		EYEDIAP (screen target)
and FT settings, in terms 		EYEDIAP (screen target)
of x- and y- angles.  fair comparison, the reported results		EYEDIAP (screen target)
 for static models disregard such		EYEDIAP (screen target)
 frames when temporal models are		EYEDIAP (screen target)
 included in the comparison		EYEDIAP (screen target)
.  4.1 Training data		EYEDIAP (screen target)
		EYEDIAP (screen target)
There are few publicly available 		EYEDIAP (screen target)
datasets devoted to 3D gaze 		EYEDIAP (screen target)
estimation and most of them 		EYEDIAP (screen target)
focus on HCI with a 		EYEDIAP (screen target)
limited range of head pose 		EYEDIAP (screen target)
and gaze directions. Therefore, we 		EYEDIAP (screen target)
use VGA videos from the 		EYEDIAP (screen target)
publicly-available EYEDIAP dataset [7] to 		EYEDIAP (screen target)
perform the experimental evaluation, as 		EYEDIAP (screen target)
it is currently the only 		EYEDIAP (screen target)
one containing video sequences with 		EYEDIAP (screen target)
a wide range of head 		EYEDIAP (screen target)
poses and showing the full 		EYEDIAP (screen target)
face. This dataset consists of 3		EYEDIAP (screen target)
-minute videos of 16 subjects 		EYEDIAP (screen target)
looking at two types of 		EYEDIAP (screen target)
targets: continuous screen targets on 		EYEDIAP (screen target)
a fixed monitor (CS), and 		EYEDIAP (screen target)
floating physical targets (FT ). 		EYEDIAP (screen target)
The videos are further divided 		EYEDIAP (screen target)
into static (S) and moving (		EYEDIAP (screen target)
M) head pose for each 		EYEDIAP (screen target)
of the subjects. Subjects 12-16 		EYEDIAP (screen target)
were recorded with 2 different 		EYEDIAP (screen target)
lighting conditions.  For evaluation, we filtered out		EYEDIAP (screen target)
 those frames that fulfilled at		EYEDIAP (screen target)
 least one of the following		EYEDIAP (screen target)
 conditions: (1) face or landmarks		EYEDIAP (screen target)
 not detected; (2) subject not		EYEDIAP (screen target)
 looking at the target; (3		EYEDIAP (screen target)
) 3D head pose, eyes 		EYEDIAP (screen target)
or target location not properly 		EYEDIAP (screen target)
recovered; and (4) eyeball rotations 		EYEDIAP (screen target)
violating physical 		EYEDIAP (screen target)
constraints (|θ | ≤ 40		EYEDIAP (screen target)
◦, |φ | ≤ 30		EYEDIAP (screen target)
◦) [23]. Note that we 		EYEDIAP (screen target)
purposely do not filter eye 		EYEDIAP (screen target)
blinking moments to learn their 		EYEDIAP (screen target)
dynamics with the temporal model, 		EYEDIAP (screen target)
which may produce some outliers 		EYEDIAP (screen target)
with a higher prediction error 		EYEDIAP (screen target)
due to a less accurate 		EYEDIAP (screen target)
ground truth. Figure 2 shows 		EYEDIAP (screen target)
the distribution of gaze directions 		EYEDIAP (screen target)
and head poses for both 		EYEDIAP (screen target)
filtered CS and FT cases.  We applied data augmentation to		EYEDIAP (screen target)
 the training set with the		EYEDIAP (screen target)
 following random transforma- tions: horizontal		EYEDIAP (screen target)
 flip, shifts of up to		EYEDIAP (screen target)
 5 pixels, zoom of up		EYEDIAP (screen target)
 to 2%, brightness changes by		EYEDIAP (screen target)
 a factor in the range		EYEDIAP (screen target)
 [0.4,1.75], and additive Gaussian noise		EYEDIAP (screen target)
 with σ2 = 0.03		EYEDIAP (screen target)
.  4.2 Evaluation of static modalities		EYEDIAP (screen target)
		EYEDIAP (screen target)
First, we evaluate the contribution 		EYEDIAP (screen target)
of each static modality on 		EYEDIAP (screen target)
the FT scenario. We divided 		EYEDIAP (screen target)
the 16 participants into 4 		EYEDIAP (screen target)
groups, such that appearance variability 		EYEDIAP (screen target)
was maximized while maintaining a 		EYEDIAP (screen target)
similar number of training samples 		EYEDIAP (screen target)
per group. Each static model 		EYEDIAP (screen target)
was trained end-to-end performing 4-fold 		EYEDIAP (screen target)
cross-validation using different combinations of 		EYEDIAP (screen target)
input modal- ities. Since the 		EYEDIAP (screen target)
number of fusion units depends 		EYEDIAP (screen target)
on the number of input 		EYEDIAP (screen target)
modalities, we also compare different 		EYEDIAP (screen target)
fusion layer sizes. The effect 		EYEDIAP (screen target)
of data normalization is also 		EYEDIAP (screen target)
evaluated by training a not-normalized 		EYEDIAP (screen target)
face model where the input 		EYEDIAP (screen target)
image is the face bounding 		EYEDIAP (screen target)
box with square size the 		EYEDIAP (screen target)
maximum distance between 2D landmarks.  Citation Citation {Funesprotect unhbox voidb@x		EYEDIAP (screen target)
 penalty @M		EYEDIAP (screen target)
		EYEDIAP (screen target)
Mora, Monay, and Odobez} 2014{}  Citation Citation {MSC		EYEDIAP (screen target)
		EYEDIAP (screen target)
8 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (screen target)
ESTIMATION  0 1 2 3 4		EYEDIAP (screen target)
 5 6 7 8 9		EYEDIAP (screen target)
		EYEDIAP (screen target)
10 11  An gl		EYEDIAP (screen target)
		EYEDIAP (screen target)
e  er		EYEDIAP (screen target)
		EYEDIAP (screen target)
ro r (  de gr		EYEDIAP (screen target)
		EYEDIAP (screen target)
ee s)  6.9 6.43 5.58 5.71 5.59		EYEDIAP (screen target)
 5.55 5.52		EYEDIAP (screen target)
		EYEDIAP (screen target)
OF-4096 NE-1536 NF-4096  NF-5632 NFL-4300		EYEDIAP (screen target)
		EYEDIAP (screen target)
NFE-5632 NFEL-5836  Figure 3: Performance evaluation of		EYEDIAP (screen target)
 the Static network using different		EYEDIAP (screen target)
 input modali- ties (O		EYEDIAP (screen target)
 - Not normalized, N		EYEDIAP (screen target)
 - Normalized, F - Face		EYEDIAP (screen target)
, E - Eyes, 		EYEDIAP (screen target)
L - 3D Landmarks) and 		EYEDIAP (screen target)
size of fusion layers on 		EYEDIAP (screen target)
the FT scenario.  Floating Target Screen Target 0		EYEDIAP (screen target)
 1 2 3 4 5		EYEDIAP (screen target)
 6 7 8 9		EYEDIAP (screen target)
		EYEDIAP (screen target)
10 11  An gl		EYEDIAP (screen target)
		EYEDIAP (screen target)
e  er		EYEDIAP (screen target)
		EYEDIAP (screen target)
ro r (  de gr		EYEDIAP (screen target)
		EYEDIAP (screen target)
ee s)  6.36 5.43 5.19 4.2 3.38		EYEDIAP (screen target)
 3.4		EYEDIAP (screen target)
		EYEDIAP (screen target)
MPIIGaze Static Temporal  Figure 4: Performance comparison among		EYEDIAP (screen target)
 MPIIGaze method [42] and our		EYEDIAP (screen target)
 Static and Temporal versions of		EYEDIAP (screen target)
 the proposed network for FT		EYEDIAP (screen target)
 and CS scenarios		EYEDIAP (screen target)
.  As shown in Figure 3		EYEDIAP (screen target)
, all models that take 		EYEDIAP (screen target)
normalized full-face information as input 		EYEDIAP (screen target)
achieve better performance than the 		EYEDIAP (screen target)
eyes-only model. More specifically, the 		EYEDIAP (screen target)
combination of face, eyes and 		EYEDIAP (screen target)
landmarks outperforms all the other 		EYEDIAP (screen target)
combinations by a small but 		EYEDIAP (screen target)
significant margin (paired Wilcoxon test, 		EYEDIAP (screen target)
p < 0.0001). The standard 		EYEDIAP (screen target)
deviation of the best-performing model 		EYEDIAP (screen target)
is reduced compared to the 		EYEDIAP (screen target)
face and eyes model, suggesting 		EYEDIAP (screen target)
a regularizing effect due to 		EYEDIAP (screen target)
the addition of landmarks. The 		EYEDIAP (screen target)
not-normalized face-only model shows the 		EYEDIAP (screen target)
largest error, proving the impact 		EYEDIAP (screen target)
of normalization to reduce the 		EYEDIAP (screen target)
appearance variability. Furthermore, our results 		EYEDIAP (screen target)
indicate that the increase of 		EYEDIAP (screen target)
fusion units is not correlated 		EYEDIAP (screen target)
with a better performance.  4.3 Static gaze regression: comparison		EYEDIAP (screen target)
 with existing methods		EYEDIAP (screen target)
		EYEDIAP (screen target)
We compare our best-performing static 		EYEDIAP (screen target)
model with three baselines. Head: 		EYEDIAP (screen target)
Treating the head pose directly 		EYEDIAP (screen target)
as gaze direction. PR-ALR: Method 		EYEDIAP (screen target)
that relies on RGB-D data 		EYEDIAP (screen target)
to rectify the eye images 		EYEDIAP (screen target)
viewpoint into a canonical head 		EYEDIAP (screen target)
pose using a 3DMM. It 		EYEDIAP (screen target)
then learns an RGB gaze 		EYEDIAP (screen target)
appearance model using ALR [21]. 		EYEDIAP (screen target)
Predicted 3D vectors for FT-S 		EYEDIAP (screen target)
scenario are provided by EYEDIAP 		EYEDIAP (screen target)
dataset. MPIIGaze:. State-of-the-art full-face 3D 		EYEDIAP (screen target)
gaze estimation method [42]. They 		EYEDIAP (screen target)
use an Alexnet-based CNN model 		EYEDIAP (screen target)
with spatial weights to enhance 		EYEDIAP (screen target)
information in different facial regions. 		EYEDIAP (screen target)
We fine-tuned it with the 		EYEDIAP (screen target)
filtered EYEDIAP subsets using our 		EYEDIAP (screen target)
training parameters and normalization procedure.  In addition to the aforementioned		EYEDIAP (screen target)
 FT-based evaluation setup, we also		EYEDIAP (screen target)
 evaluate our method on the		EYEDIAP (screen target)
 CS scenario. In this case		EYEDIAP (screen target)
 there are only 14 participants		EYEDIAP (screen target)
 available, so we divided them		EYEDIAP (screen target)
 in 5 groups and performed		EYEDIAP (screen target)
 5-fold cross-validation. In Figure 4		EYEDIAP (screen target)
 we compare our method to		EYEDIAP (screen target)
 MPIIGaze, achieving a statistically significant		EYEDIAP (screen target)
 improvement of 14.6% and 19.5		EYEDIAP (screen target)
% on FT and CS 		EYEDIAP (screen target)
scenarios, respectively (paired Wilcoxon test, 		EYEDIAP (screen target)
p < 0.0001). We can 		EYEDIAP (screen target)
observe that a re- stricted 		EYEDIAP (screen target)
gaze target benefits the performance 		EYEDIAP (screen target)
of all methods, compared to 		EYEDIAP (screen target)
a more challenging unrestricted setting 		EYEDIAP (screen target)
with a wider range of 		EYEDIAP (screen target)
head poses and gaze directions.  Table 2 provides a detailed		EYEDIAP (screen target)
 comparison on every participant, performing		EYEDIAP (screen target)
 leave-one-out cross-validation on the FT		EYEDIAP (screen target)
 scenario for static and moving		EYEDIAP (screen target)
 head separately. Results show that		EYEDIAP (screen target)
, as expected, facial appearance 		EYEDIAP (screen target)
and head pose have a 		EYEDIAP (screen target)
noticeable impact on gaze accuracy, 		EYEDIAP (screen target)
with average error differences of 		EYEDIAP (screen target)
up to 7.7◦ among participants.  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (screen target)
, and Bulling} 2015  Citation Citation {Mora and Odobez		EYEDIAP (screen target)
} 2012  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (screen target)
, and Bulling} 2015		EYEDIAP (screen target)
		EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 9		EYEDIAP (screen target)
		EYEDIAP (screen target)
Method 1 2 3 4 5 6 7 8 9 10		EYEDIAP (screen target)
 11 12 13 14 15		EYEDIAP (screen target)
 16 Avg. Head 23.5 22.1		EYEDIAP (screen target)
 20.3 23.6 23.2 23.2 23.6		EYEDIAP (screen target)
 21.2 26.7 23.6 23.1 24.4		EYEDIAP (screen target)
 23.3 24.0 24.5 22.8 23.3		EYEDIAP (screen target)
 PR-ALR 12.3 12.0 12.4 11.3		EYEDIAP (screen target)
 15.5 12.9 17.9 11.8 17.3		EYEDIAP (screen target)
 13.4 13.4 14.3 15.2 13.6		EYEDIAP (screen target)
 14.4 14.6 13.9 MPIIGaze 5.3		EYEDIAP (screen target)
 5.1 5.7 4.7 7.3 15.1		EYEDIAP (screen target)
 10.8 5.7 9.9 7.1 5.0		EYEDIAP (screen target)
 5.7 7.4 3.8 4.8 5.5		EYEDIAP (screen target)
 6.8 Static 3.9 4.1 4.2		EYEDIAP (screen target)
 3.9 6.0 6.4 7.2 3.6		EYEDIAP (screen target)
 7.1 5.0 5.7 6.7 3.9		EYEDIAP (screen target)
 4.7 5.1 4.2 5.1 Temporal		EYEDIAP (screen target)
 4.0 4.9 4.3 4.1 6.1		EYEDIAP (screen target)
 6.5 6.6 3.9 7.8 6.1		EYEDIAP (screen target)
 4.7 5.6 4.7 3.5 5.9		EYEDIAP (screen target)
 4.6 5.2 Head 19.3 14.2		EYEDIAP (screen target)
 16.4 19.9 16.8 21.9 16.1		EYEDIAP (screen target)
 24.2 20.3 19.9 18.8 22.3		EYEDIAP (screen target)
 18.1 14.9 16.2 19.3 18.7		EYEDIAP (screen target)
 MPIIGaze 7.6 6.2 5.7 8.7		EYEDIAP (screen target)
 10.1 12.0 12.2 6.1 8.3		EYEDIAP (screen target)
 5.9 6.1 6.2 7.4 4.7		EYEDIAP (screen target)
 4.4 6.0 7.3 Static 5.8		EYEDIAP (screen target)
 5.7 4.4 7.5 6.7 8.8		EYEDIAP (screen target)
 11.6 5.5 8.3 5.5 5.2		EYEDIAP (screen target)
 6.3 5.3 3.9 4.3 5.6		EYEDIAP (screen target)
 6.3 Temporal 6.1 5.6 4.5		EYEDIAP (screen target)
 7.5 6.4 8.2 12.0 5.0		EYEDIAP (screen target)
 7.5 5.4 5.0 5.8 6.6		EYEDIAP (screen target)
 4.0 4.5 5.8 6.2		EYEDIAP (screen target)
		EYEDIAP (screen target)
Table 2: Gaze angular error 		EYEDIAP (screen target)
comparison for static (top half) 		EYEDIAP (screen target)
and moving (bottom half) head 		EYEDIAP (screen target)
pose for each subject in 		EYEDIAP (screen target)
the FT scenario. Best results 		EYEDIAP (screen target)
in bold.  −80 −40 0 40 80−80		EYEDIAP (screen target)
		EYEDIAP (screen target)
40  0		EYEDIAP (screen target)
		EYEDIAP (screen target)
40  80		EYEDIAP (screen target)
		EYEDIAP (screen target)
0  5		EYEDIAP (screen target)
		EYEDIAP (screen target)
10  15		EYEDIAP (screen target)
		EYEDIAP (screen target)
20  25		EYEDIAP (screen target)
		EYEDIAP (screen target)
30  35		EYEDIAP (screen target)
		EYEDIAP (screen target)
80 −40 0 40 80−80  −40		EYEDIAP (screen target)
		EYEDIAP (screen target)
0  40		EYEDIAP (screen target)
		EYEDIAP (screen target)
80  −10		EYEDIAP (screen target)
		EYEDIAP (screen target)
8  −6		EYEDIAP (screen target)
		EYEDIAP (screen target)
4  −2		EYEDIAP (screen target)
		EYEDIAP (screen target)
0  2		EYEDIAP (screen target)
		EYEDIAP (screen target)
4  6		EYEDIAP (screen target)
		EYEDIAP (screen target)
8  10		EYEDIAP (screen target)
		EYEDIAP (screen target)
80 −40 0 40 80−80  −40		EYEDIAP (screen target)
		EYEDIAP (screen target)
0  40		EYEDIAP (screen target)
		EYEDIAP (screen target)
80  0		EYEDIAP (screen target)
		EYEDIAP (screen target)
5  10		EYEDIAP (screen target)
		EYEDIAP (screen target)
15  20		EYEDIAP (screen target)
		EYEDIAP (screen target)
25  30		EYEDIAP (screen target)
		EYEDIAP (screen target)
35  −80 −40 0 40 80−80		EYEDIAP (screen target)
		EYEDIAP (screen target)
40  0		EYEDIAP (screen target)
		EYEDIAP (screen target)
40  80		EYEDIAP (screen target)
		EYEDIAP (screen target)
10  −8		EYEDIAP (screen target)
		EYEDIAP (screen target)
6  −4		EYEDIAP (screen target)
		EYEDIAP (screen target)
2  0		EYEDIAP (screen target)
		EYEDIAP (screen target)
2  4		EYEDIAP (screen target)
		EYEDIAP (screen target)
6  8		EYEDIAP (screen target)
		EYEDIAP (screen target)
10  (a) Gaze space (b) Head		EYEDIAP (screen target)
 orientation space		EYEDIAP (screen target)
		EYEDIAP (screen target)
Figure 5: Angular error distribution 		EYEDIAP (screen target)
across gaze (a) and head 		EYEDIAP (screen target)
orientation (b) spaces in the 		EYEDIAP (screen target)
FT setting, in terms of 		EYEDIAP (screen target)
x- and y- angles. For 		EYEDIAP (screen target)
each space, we depict the 		EYEDIAP (screen target)
Static model performance (left) and 		EYEDIAP (screen target)
the contribution of the Temporal 		EYEDIAP (screen target)
model versus Static (right). In 		EYEDIAP (screen target)
the latter, positive difference means 		EYEDIAP (screen target)
higher improvement of the Temporal 		EYEDIAP (screen target)
model.  4.4 Evaluation of the temporal		EYEDIAP (screen target)
 network		EYEDIAP (screen target)
		EYEDIAP (screen target)
In this section, we evaluate 		EYEDIAP (screen target)
the contribution of adding the 		EYEDIAP (screen target)
temporal module to the static 		EYEDIAP (screen target)
model. To do so, we 		EYEDIAP (screen target)
trained a lower-dimensional version of 		EYEDIAP (screen target)
the static network with compa- 		EYEDIAP (screen target)
rable performance to the original, 		EYEDIAP (screen target)
reducing the number of units 		EYEDIAP (screen target)
of the second fusion layer 		EYEDIAP (screen target)
to 2918. Results are reported 		EYEDIAP (screen target)
in Figure 4 and Table 2		EYEDIAP (screen target)
. One can observe that 		EYEDIAP (screen target)
using sequential information is helpful 		EYEDIAP (screen target)
on the FT scenario, outperforming 		EYEDIAP (screen target)
the static model by a 		EYEDIAP (screen target)
statistically significant 4.4% (paired Wilcoxon 		EYEDIAP (screen target)
test, p < 0.0001). This 		EYEDIAP (screen target)
contribution is more noticeable in 		EYEDIAP (screen target)
the moving head setting, proving 		EYEDIAP (screen target)
that the temporal model can 		EYEDIAP (screen target)
benefit from head motion information. 		EYEDIAP (screen target)
In contrast, such information seems 		EYEDIAP (screen target)
to be less meaningful in 		EYEDIAP (screen target)
the CS scenario, where the 		EYEDIAP (screen target)
obtained error is already very 		EYEDIAP (screen target)
low for a cross-subject setting 		EYEDIAP (screen target)
and the amount of head 		EYEDIAP (screen target)
movement declines.  Figure 5 further explores the		EYEDIAP (screen target)
 error distribution of the static		EYEDIAP (screen target)
 network and the impact of		EYEDIAP (screen target)
 sequential information. We can observe		EYEDIAP (screen target)
 that the accuracy of the		EYEDIAP (screen target)
 static model drops with extreme		EYEDIAP (screen target)
 head poses and gaze directions		EYEDIAP (screen target)
, which can also be 		EYEDIAP (screen target)
correlated to having less data 		EYEDIAP (screen target)
in those areas. Compared to 		EYEDIAP (screen target)
the static model, the temporal 		EYEDIAP (screen target)
model particularly benefits gaze targets 		EYEDIAP (screen target)
from mid-range upwards. Its contribution 		EYEDIAP (screen target)
is less clear for extreme 		EYEDIAP (screen target)
targets, probably again due to 		EYEDIAP (screen target)
data imbalance.  Finally, we evaluated the effect		EYEDIAP (screen target)
 of different recurrent architectures for		EYEDIAP (screen target)
 the temporal model. In particular		EYEDIAP (screen target)
, we tested 1 (128 		EYEDIAP (screen target)
units) and 2 (256-128 units) 		EYEDIAP (screen target)
LSTM and GRU lay- ers, 		EYEDIAP (screen target)
with 1 GRU layer obtaining 		EYEDIAP (screen target)
slightly superior results (up to 0		EYEDIAP (screen target)
.12◦). We also assessed the 		EYEDIAP (screen target)
effect of sequence length fixing 		EYEDIAP (screen target)
s in the range {4,7,10}, 		EYEDIAP (screen target)
with s = 7 performing 		EYEDIAP (screen target)
worse than the other two (		EYEDIAP (screen target)
up to 0		EYEDIAP (screen target)
		EYEDIAP (screen target)
14		EYEDIAP (screen target)
		EYEDIAP (screen target)
10 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (screen target)
ESTIMATION  5 Conclusions In this work		EYEDIAP (screen target)
, we studied the combination 		EYEDIAP (screen target)
of full-face and eye images 		EYEDIAP (screen target)
along with facial land- marks 		EYEDIAP (screen target)
for person- and head pose-independent 		EYEDIAP (screen target)
3D gaze estimation. Consequently, we 		EYEDIAP (screen target)
pro- posed a multi-stream recurrent 		EYEDIAP (screen target)
CNN network that leverages the 		EYEDIAP (screen target)
sequential information of eye and 		EYEDIAP (screen target)
head movements. Both static and 		EYEDIAP (screen target)
temporal versions of our approach 		EYEDIAP (screen target)
significantly outperform current state-of-the-art 3D 		EYEDIAP (screen target)
gaze estimation methods on a 		EYEDIAP (screen target)
wide range of head poses 		EYEDIAP (screen target)
and gaze directions. We showed 		EYEDIAP (screen target)
that adding geometry features to 		EYEDIAP (screen target)
appearance-based methods has a regularizing 		EYEDIAP (screen target)
effect on the accuracy. Adding 		EYEDIAP (screen target)
sequential information further benefits the 		EYEDIAP (screen target)
final performance compared to static-only 		EYEDIAP (screen target)
input, especially from mid-range up- 		EYEDIAP (screen target)
wards and in those cases 		EYEDIAP (screen target)
where head motion is present. 		EYEDIAP (screen target)
The effect in very extreme 		EYEDIAP (screen target)
head poses is not clear 		EYEDIAP (screen target)
due to data imbalance, suggesting 		EYEDIAP (screen target)
the importance of learning from 		EYEDIAP (screen target)
a con- tinuous, balanced dataset 		EYEDIAP (screen target)
including all head poses and 		EYEDIAP (screen target)
gaze directions of interest. To 		EYEDIAP (screen target)
the best of our knowledge, 		EYEDIAP (screen target)
this is the first attempt 		EYEDIAP (screen target)
to exploit the temporal modality 		EYEDIAP (screen target)
in the context of gaze 		EYEDIAP (screen target)
estimation from remote cameras. As 		EYEDIAP (screen target)
future work, we will further 		EYEDIAP (screen target)
explore extracting meaningful temporal representations 		EYEDIAP (screen target)
of gaze dynamics, considering 3DCNNs 		EYEDIAP (screen target)
as well as the encoding 		EYEDIAP (screen target)
of deep features around particular 		EYEDIAP (screen target)
tracked face landmarks [14].  Acknowledgements This work has been		EYEDIAP (screen target)
 partially supported by the Spanish		EYEDIAP (screen target)
 project TIN2016-74946-P (MINECO/ FEDER, UE		EYEDIAP (screen target)
), CERCA Programme / Generalitat 		EYEDIAP (screen target)
de Catalunya, and the FP7 		EYEDIAP (screen target)
people program (Marie Curie Actions), 		EYEDIAP (screen target)
REA grant agreement no FP7-607139 (		EYEDIAP (screen target)
iCARE - Improving Children Auditory 		EYEDIAP (screen target)
REhabilitation). We gratefully acknowledge the 		EYEDIAP (screen target)
support of NVIDIA Corporation with 		EYEDIAP (screen target)
the donation of the GPU 		EYEDIAP (screen target)
used for this research. Portions 		EYEDIAP (screen target)
of the research in this 		EYEDIAP (screen target)
pa- per used the EYEDIAP 		EYEDIAP (screen target)
dataset made available by the 		EYEDIAP (screen target)
Idiap Research Institute, Martigny, Switzerland.  References [1] Nicola C Anderson		EYEDIAP (screen target)
, Evan F Risko, and 		EYEDIAP (screen target)
Alan Kingstone. Motion influences gaze 		EYEDIAP (screen target)
di-  rection discrimination and disambiguates contradictory		EYEDIAP (screen target)
 luminance cues. Psychonomic bulletin		EYEDIAP (screen target)
 & review, 23(3):817–823, 2016		EYEDIAP (screen target)
.  [2] Shumeet Baluja and Dean		EYEDIAP (screen target)
 Pomerleau. Non-intrusive gaze tracking using		EYEDIAP (screen target)
 artificial neu- ral networks. In		EYEDIAP (screen target)
 Advances in Neural Information Processing		EYEDIAP (screen target)
 Systems, pages 753–760, 1994		EYEDIAP (screen target)
.  [3] Adrian Bulat and Georgios		EYEDIAP (screen target)
 Tzimiropoulos. How far are we		EYEDIAP (screen target)
 from solving the 2d		EYEDIAP (screen target)
 & 3d face alignment problem		EYEDIAP (screen target)
? (and a dataset of 230,		EYEDIAP (screen target)
000 3d facial landmarks). In 		EYEDIAP (screen target)
Interna- tional Conference on Computer 		EYEDIAP (screen target)
Vision, 2017.  [4] Haoping Deng and Wangjiang		EYEDIAP (screen target)
 Zhu. Monocular free-head 3d gaze		EYEDIAP (screen target)
 tracking with deep learning and		EYEDIAP (screen target)
 geometry constraints. In Computer Vision		EYEDIAP (screen target)
 (ICCV), 2017 IEEE Interna- tional		EYEDIAP (screen target)
 Conference on, pages 3162–3171. IEEE		EYEDIAP (screen target)
, 2017.  [5] Onur Ferhat and Fernando		EYEDIAP (screen target)
 Vilariño. Low cost eye tracking		EYEDIAP (screen target)
. Computational intelligence and neuroscience, 2016		EYEDIAP (screen target)
:17, 2016.  Citation Citation {Jung, Lee, Yim		EYEDIAP (screen target)
, Park, and Kim} 2015		EYEDIAP (screen target)
		EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 11		EYEDIAP (screen target)
		EYEDIAP (screen target)
6] Kenneth A Funes-Mora and 		EYEDIAP (screen target)
Jean-Marc Odobez. Gaze estimation in 		EYEDIAP (screen target)
the 3D space using RGB-D 		EYEDIAP (screen target)
sensors. International Journal of Computer 		EYEDIAP (screen target)
Vision, 118(2):194–216, 2016.  [7] Kenneth Alberto Funes Mora		EYEDIAP (screen target)
, Florent Monay, and Jean-Marc 		EYEDIAP (screen target)
Odobez. Eyediap: A database for 		EYEDIAP (screen target)
the development and evaluation of 		EYEDIAP (screen target)
gaze estimation algorithms from rgb 		EYEDIAP (screen target)
and rgb-d cameras. In Proceedings 		EYEDIAP (screen target)
of the ACM Symposium on 		EYEDIAP (screen target)
Eye Tracking Research and Applications. 		EYEDIAP (screen target)
ACM, March 2014. doi: 10.1145/2578153.2578190.  [8] Kenneth Alberto Funes Mora		EYEDIAP (screen target)
, Florent Monay, and Jean-Marc 		EYEDIAP (screen target)
Odobez. Eyediap: A database for 		EYEDIAP (screen target)
the development and evaluation of 		EYEDIAP (screen target)
gaze estimation algorithms from rgb 		EYEDIAP (screen target)
and rgb-d cameras. In Proceedings 		EYEDIAP (screen target)
of the Symposium on Eye 		EYEDIAP (screen target)
Tracking Research and Applications, pages 255		EYEDIAP (screen target)
–258. ACM, 2014.  [9] Quentin Guillon, Nouchine Hadjikhani		EYEDIAP (screen target)
, Sophie Baduel, and Bernadette 		EYEDIAP (screen target)
Rogé. Visual social attention in 		EYEDIAP (screen target)
autism spectrum disorder: Insights from 		EYEDIAP (screen target)
eye tracking studies. Neu- 		EYEDIAP (screen target)
roscience & Biobehavioral Reviews, 42:279–297, 2014		EYEDIAP (screen target)
.  [10] Dan Witzner Hansen and		EYEDIAP (screen target)
 Qiang Ji. In the eye		EYEDIAP (screen target)
 of the beholder: A survey		EYEDIAP (screen target)
 of models for eyes and		EYEDIAP (screen target)
 gaze. IEEE transactions on pattern		EYEDIAP (screen target)
 analysis and machine intelligence, 32(3		EYEDIAP (screen target)
): 478–500, 2010.  [11] Qiong Huang, Ashok Veeraraghavan		EYEDIAP (screen target)
, and Ashutosh Sabharwal. Tabletgaze: 		EYEDIAP (screen target)
dataset and analysis for unconstrained 		EYEDIAP (screen target)
appearance-based gaze estimation in mobile 		EYEDIAP (screen target)
tablets. Machine Vision and Applications, 28		EYEDIAP (screen target)
(5-6):445–461, 2017.  [12] Robert JK Jacob and		EYEDIAP (screen target)
 Keith S Karn. Eye tracking		EYEDIAP (screen target)
 in human-computer interaction and usability		EYEDIAP (screen target)
 research: Ready to deliver the		EYEDIAP (screen target)
 promises. In The mind’s eye		EYEDIAP (screen target)
, pages 573–605. Elsevier, 2003.  [13] László A Jeni and		EYEDIAP (screen target)
 Jeffrey F Cohn. Person-independent 3d		EYEDIAP (screen target)
 gaze estimation using face frontalization		EYEDIAP (screen target)
. In Proceedings of the 		EYEDIAP (screen target)
IEEE Conference on Computer Vision 		EYEDIAP (screen target)
and Pattern Recognition Workshops, pages 87		EYEDIAP (screen target)
–95, 2016.  [14] Heechul Jung, Sihaeng Lee		EYEDIAP (screen target)
, Junho Yim, Sunjeong Park, 		EYEDIAP (screen target)
and Junmo Kim. Joint fine- 		EYEDIAP (screen target)
tuning in deep neural networks 		EYEDIAP (screen target)
for facial expression recognition. In 		EYEDIAP (screen target)
Computer Vision (ICCV), 2015 IEEE 		EYEDIAP (screen target)
International Conference on, pages 2983–2991. 		EYEDIAP (screen target)
IEEE, 2015.  [15] Anuradha Kar and Peter		EYEDIAP (screen target)
 Corcoran. A review and analysis		EYEDIAP (screen target)
 of eye-gaze estimation sys- tems		EYEDIAP (screen target)
, algorithms and performance evaluation 		EYEDIAP (screen target)
methods in consumer platforms. IEEE 		EYEDIAP (screen target)
Access, 5:16495–16519, 2017.  [16] Kyle Krafka, Aditya Khosla		EYEDIAP (screen target)
, Petr Kellnhofer, Harini Kannan, 		EYEDIAP (screen target)
Suchendra Bhandarkar, Wojciech Matusik, and 		EYEDIAP (screen target)
Antonio Torralba. Eye tracking for 		EYEDIAP (screen target)
everyone. In Computer Vision and 		EYEDIAP (screen target)
Pattern Recognition (CVPR), 2016 IEEE 		EYEDIAP (screen target)
Conference on, pages 2176–2184. IEEE, 2016		EYEDIAP (screen target)
.  [17] Simon P Liversedge and		EYEDIAP (screen target)
 John M Findlay. Saccadic eye		EYEDIAP (screen target)
 movements and cognition. Trends in		EYEDIAP (screen target)
 cognitive sciences, 4(1):6–14, 2000		EYEDIAP (screen target)
.  [18] Feng Lu, Takahiro Okabe		EYEDIAP (screen target)
, Yusuke Sugano, and Yoichi 		EYEDIAP (screen target)
Sato. A head pose-free approach 		EYEDIAP (screen target)
for appearance-based gaze estimation. In 		EYEDIAP (screen target)
BMVC, pages 1–11, 2011		EYEDIAP (screen target)
		EYEDIAP (screen target)
12 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (screen target)
ESTIMATION  [19] Feng Lu, Yusuke Sugano		EYEDIAP (screen target)
, Takahiro Okabe, and Yoichi 		EYEDIAP (screen target)
Sato. Inferring human gaze from 		EYEDIAP (screen target)
appearance via adaptive linear regression. 		EYEDIAP (screen target)
In Computer Vision (ICCV), 2011 		EYEDIAP (screen target)
IEEE International Conference on, pages 153		EYEDIAP (screen target)
–160. IEEE, 2011.  [20] Päivi Majaranta and Andreas		EYEDIAP (screen target)
 Bulling. Eye tracking and eye-based		EYEDIAP (screen target)
 human–computer interaction. In Advances in		EYEDIAP (screen target)
 physiological computing, pages 39–65. Springer		EYEDIAP (screen target)
, 2014.  [21] Kenneth Alberto Funes Mora		EYEDIAP (screen target)
 and Jean-Marc Odobez. Gaze estimation		EYEDIAP (screen target)
 from multi- modal kinect data		EYEDIAP (screen target)
. In Computer Vision and 		EYEDIAP (screen target)
Pattern Recognition Workshops (CVPRW), 2012 		EYEDIAP (screen target)
IEEE Computer Society Conference on, 		EYEDIAP (screen target)
pages 25–30. IEEE, 2012.  [22] Carlos Hitoshi Morimoto, Arnon		EYEDIAP (screen target)
 Amir, and Myron Flickner. Detecting		EYEDIAP (screen target)
 eye position and gaze from		EYEDIAP (screen target)
 a single camera and 2		EYEDIAP (screen target)
 light sources. In Pattern Recognition		EYEDIAP (screen target)
, 2002. Proceedings. 16th International 		EYEDIAP (screen target)
Conference on, volume 4, pages 314		EYEDIAP (screen target)
–317. IEEE, 2002.  [23] IMO MSC. Circ. 982		EYEDIAP (screen target)
 (2000) guidelines on ergonomic criteria		EYEDIAP (screen target)
 for bridge equipment and layout		EYEDIAP (screen target)
.  [24] Alejandro Newell, Kaiyu Yang		EYEDIAP (screen target)
, and Jia Deng. Stacked 		EYEDIAP (screen target)
hourglass networks for hu- man 		EYEDIAP (screen target)
pose estimation. In European Conference 		EYEDIAP (screen target)
on Computer Vision, pages 483–499. 		EYEDIAP (screen target)
Springer, 2016.  [25] Yasuhiro Ono, Takahiro Okabe		EYEDIAP (screen target)
, and Yoichi Sato. Gaze 		EYEDIAP (screen target)
estimation from low resolution images. 		EYEDIAP (screen target)
In Pacific-Rim Symposium on Image 		EYEDIAP (screen target)
and Video Technology, pages 178–188. 		EYEDIAP (screen target)
Springer, 2006.  [26] Cristina Palmero, Elisabeth A		EYEDIAP (screen target)
. van Dam, Sergio Escalera, 		EYEDIAP (screen target)
Mike Kelia, Guido F. Lichtert, 		EYEDIAP (screen target)
Lucas P.J.J Noldus, Andrew J. 		EYEDIAP (screen target)
Spink, and Astrid van Wieringen. 		EYEDIAP (screen target)
Automatic mutual gaze detection in 		EYEDIAP (screen target)
face-to-face dyadic interaction videos. In 		EYEDIAP (screen target)
Proceedings of Measuring Behavior, pages 158		EYEDIAP (screen target)
–163, 2018.  [27] Omkar M. Parkhi, Andrea		EYEDIAP (screen target)
 Vedaldi, and Andrew Zisserman. Deep		EYEDIAP (screen target)
 face recognition. In British Machine		EYEDIAP (screen target)
 Vision Conference, 2015		EYEDIAP (screen target)
.  [28] Derek R Rutter and		EYEDIAP (screen target)
 Kevin Durkin. Turn-taking in mother–infant		EYEDIAP (screen target)
 interaction: An exam- ination of		EYEDIAP (screen target)
 vocalizations and gaze. Developmental psychology		EYEDIAP (screen target)
, 23(1):54, 1987.  [29] Brian A Smith, Qi		EYEDIAP (screen target)
 Yin, Steven K Feiner, and		EYEDIAP (screen target)
 Shree K Nayar. Gaze locking		EYEDIAP (screen target)
: passive eye contact detection 		EYEDIAP (screen target)
for human-object interaction. In Proceedings 		EYEDIAP (screen target)
of the 26th annual ACM 		EYEDIAP (screen target)
symposium on User interface software 		EYEDIAP (screen target)
and technology, pages 271–280. ACM, 2013		EYEDIAP (screen target)
.  [30] Yusuke Sugano, Yasuyuki Matsushita		EYEDIAP (screen target)
, and Yoichi Sato. Appearance-based 		EYEDIAP (screen target)
gaze es- timation using visual 		EYEDIAP (screen target)
saliency. IEEE transactions on pattern 		EYEDIAP (screen target)
analysis and machine intelligence, 35(2):329–341, 2013		EYEDIAP (screen target)
.  [31] Yusuke Sugano, Yasuyuki Matsushita		EYEDIAP (screen target)
, and Yoichi Sato. Learning-by-synthesis 		EYEDIAP (screen target)
for appearance-based 3d gaze estimation. 		EYEDIAP (screen target)
In Computer Vision and Pattern 		EYEDIAP (screen target)
Recognition (CVPR), 2014 IEEE Conference 		EYEDIAP (screen target)
on, pages 1821–1828. IEEE, 2014.  [32] Kar-Han Tan, David J		EYEDIAP (screen target)
 Kriegman, and Narendra Ahuja. Appearance-based		EYEDIAP (screen target)
 eye gaze es- timation. In		EYEDIAP (screen target)
 Applications of Computer Vision, 2002.(WACV		EYEDIAP (screen target)
 2002). Proceedings. Sixth IEEE Workshop		EYEDIAP (screen target)
 on, pages 191–195. IEEE, 2002		EYEDIAP (screen target)
		EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 13		EYEDIAP (screen target)
		EYEDIAP (screen target)
33] Ronda Venkateswarlu et al. 		EYEDIAP (screen target)
Eye gaze estimation from a 		EYEDIAP (screen target)
single image of one eye. 		EYEDIAP (screen target)
In Computer Vision, 2003. Proceedings. 		EYEDIAP (screen target)
Ninth IEEE International Conference on, 		EYEDIAP (screen target)
pages 136–143. IEEE, 2003.  [34] Kang Wang and Qiang		EYEDIAP (screen target)
 Ji. Real time eye gaze		EYEDIAP (screen target)
 tracking with 3d deformable eye-face		EYEDIAP (screen target)
 model. In Proceedings of the		EYEDIAP (screen target)
 IEEE Conference on Computer Vision		EYEDIAP (screen target)
 and Pattern Recog- nition, pages		EYEDIAP (screen target)
 1003–1011, 2017		EYEDIAP (screen target)
.  [35] Oliver Williams, Andrew Blake		EYEDIAP (screen target)
, and Roberto Cipolla. Sparse 		EYEDIAP (screen target)
and semi-supervised visual mapping with 		EYEDIAP (screen target)
the sˆ 3gp. In Computer 		EYEDIAP (screen target)
Vision and Pattern Recognition, 2006 		EYEDIAP (screen target)
IEEE Computer Society Conference on, 		EYEDIAP (screen target)
volume 1, pages 230–237. IEEE, 2006		EYEDIAP (screen target)
.  [36] William Hyde Wollaston et		EYEDIAP (screen target)
 al. Xiii. on the apparent		EYEDIAP (screen target)
 direction of eyes in a		EYEDIAP (screen target)
 portrait. Philosophical Transactions of the		EYEDIAP (screen target)
 Royal Society of London, 114:247–256		EYEDIAP (screen target)
, 1824.  [37] Erroll Wood and Andreas		EYEDIAP (screen target)
 Bulling. Eyetab: Model-based gaze estimation		EYEDIAP (screen target)
 on unmodi- fied tablet computers		EYEDIAP (screen target)
. In Proceedings of the 		EYEDIAP (screen target)
Symposium on Eye Tracking Research 		EYEDIAP (screen target)
and Applications, pages 207–210. ACM, 2014		EYEDIAP (screen target)
.  [38] Erroll Wood, Tadas Baltrusaitis		EYEDIAP (screen target)
, Xucong Zhang, Yusuke Sugano, 		EYEDIAP (screen target)
Peter Robinson, and Andreas Bulling. 		EYEDIAP (screen target)
Rendering of eyes for eye-shape 		EYEDIAP (screen target)
registration and gaze estimation. In 		EYEDIAP (screen target)
Proceedings of the IEEE International 		EYEDIAP (screen target)
Conference on Computer Vision, pages 3756		EYEDIAP (screen target)
– 3764, 2015.  [39] Erroll Wood, Tadas Baltrušaitis		EYEDIAP (screen target)
, Louis-Philippe Morency, Peter Robinson, 		EYEDIAP (screen target)
and Andreas Bulling. A 3d 		EYEDIAP (screen target)
morphable eye region model for 		EYEDIAP (screen target)
gaze estimation. In European Confer- 		EYEDIAP (screen target)
ence on Computer Vision, pages 297		EYEDIAP (screen target)
–313. Springer, 2016.  [40] Erroll Wood, Tadas Baltrušaitis		EYEDIAP (screen target)
, Louis-Philippe Morency, Peter Robinson, 		EYEDIAP (screen target)
and Andreas Bulling. Learning an 		EYEDIAP (screen target)
appearance-based gaze estimator from one 		EYEDIAP (screen target)
million synthesised images. In Proceedings 		EYEDIAP (screen target)
of the Ninth Biennial ACM 		EYEDIAP (screen target)
Symposium on Eye Tracking Re- 		EYEDIAP (screen target)
search & Applications, pages 131–138. 		EYEDIAP (screen target)
ACM, 2016.  [41] Dong Hyun Yoo and		EYEDIAP (screen target)
 Myung Jin Chung. A novel		EYEDIAP (screen target)
 non-intrusive eye gaze estimation using		EYEDIAP (screen target)
 cross-ratio under large head motion		EYEDIAP (screen target)
. Computer Vision and Image 		EYEDIAP (screen target)
Understanding, 98(1):25–51, 2005.  [42] Xucong Zhang, Yusuke Sugano		EYEDIAP (screen target)
, Mario Fritz, and Andreas 		EYEDIAP (screen target)
Bulling. Appearance-based gaze estimation in 		EYEDIAP (screen target)
the wild. In Proceedings of 		EYEDIAP (screen target)
the IEEE Conference on Computer 		EYEDIAP (screen target)
Vision and Pattern Recognition, pages 4511		EYEDIAP (screen target)
–4520, 2015.  [43] Xucong Zhang, Yusuke Sugano		EYEDIAP (screen target)
, Mario Fritz, and Andreas 		EYEDIAP (screen target)
Bulling. It’s written all over 		EYEDIAP (screen target)
your face: Full-face appearance-based gaze 		EYEDIAP (screen target)
estimation. In Proc. IEEE International 		EYEDIAP (screen target)
Conference on Computer Vision and 		EYEDIAP (screen target)
Pattern Recognition Workshops (CVPRW), 2017		EYEDIAP (screen target)
		EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 1	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Recurrent CNN for 3D Gaze 	target)	EYEDIAP (screen target)
Estimation using Appearance and Shape 	target)	EYEDIAP (screen target)
Cues  Cristina Palmero1,2	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
crpalmec7@alumnes.ub.edu  1 Dept. Mathematics and Informatics	target)	EYEDIAP (screen target)
 Universitat de Barcelona, Spain	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Javier Selva1  javier.selva.castello@est.fib.upc.edu	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
2 Computer Vision Center Campus 	target)	EYEDIAP (screen target)
UAB, Bellaterra, Spain  Mohammad Ali Bagheri3,4	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
mohammadali.bagheri@ucalgary.ca  3 Dept. Electrical and Computer	target)	EYEDIAP (screen target)
 Eng. University of Calgary, Canada	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Sergio Escalera1,2  sergio@maia.ub.es	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
4 Dept. Engineering University of 	target)	EYEDIAP (screen target)
Larestan, Iran  Abstract	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Gaze behavior is an important 	target)	EYEDIAP (screen target)
non-verbal cue in social signal 	target)	EYEDIAP (screen target)
processing and human- computer interaction. 	target)	EYEDIAP (screen target)
In this paper, we tackle 	target)	EYEDIAP (screen target)
the problem of person- and 	target)	EYEDIAP (screen target)
head pose- independent 3D gaze 	target)	EYEDIAP (screen target)
estimation from remote cameras, using 	target)	EYEDIAP (screen target)
a multi-modal recurrent convolutional neural 	target)	EYEDIAP (screen target)
network (CNN). We propose to 	target)	EYEDIAP (screen target)
combine face, eyes region, and 	target)	EYEDIAP (screen target)
face landmarks as individual streams 	target)	EYEDIAP (screen target)
in a CNN to estimate 	target)	EYEDIAP (screen target)
gaze in still images. Then, 	target)	EYEDIAP (screen target)
we exploit the dynamic nature 	target)	EYEDIAP (screen target)
of gaze by feeding the 	target)	EYEDIAP (screen target)
learned features of all the 	target)	EYEDIAP (screen target)
frames in a sequence to 	target)	EYEDIAP (screen target)
a many-to-one recurrent module that 	target)	EYEDIAP (screen target)
predicts the 3D gaze vector 	target)	EYEDIAP (screen target)
of the last frame. Our 	target)	EYEDIAP (screen target)
multi-modal static solution is evaluated 	target)	EYEDIAP (screen target)
on a wide range of 	target)	EYEDIAP (screen target)
head poses and gaze directions, 	target)	EYEDIAP (screen target)
achieving a significant improvement of 14	target)	EYEDIAP (screen target)
.6% over the state of 	target)	EYEDIAP (screen target)
the art on EYEDIAP dataset, 	target)	EYEDIAP (screen target)
further improved by 4% when 	target)	EYEDIAP (screen target)
the temporal modality is included.  1 Introduction Eyes and their	target)	EYEDIAP (screen target)
 movements are considered an important	target)	EYEDIAP (screen target)
 cue in non-verbal behavior analysis	target)	EYEDIAP (screen target)
, being involved in many 	target)	EYEDIAP (screen target)
cognitive processes and reflecting our 	target)	EYEDIAP (screen target)
internal state [17]. More specifically, 	target)	EYEDIAP (screen target)
eye gaze behavior, as an 	target)	EYEDIAP (screen target)
indicator of human visual attention, 	target)	EYEDIAP (screen target)
has been widely studied to 	target)	EYEDIAP (screen target)
assess communication skills [28] and 	target)	EYEDIAP (screen target)
to identify possible behavioral 	target)	EYEDIAP (screen target)
disorders [9]. Therefore, gaze estimation 	target)	EYEDIAP (screen target)
has become an established line 	target)	EYEDIAP (screen target)
of research in computer vision, 	target)	EYEDIAP (screen target)
being a key feature in 	target)	EYEDIAP (screen target)
human-computer interaction (HCI) and usability 	target)	EYEDIAP (screen target)
research [12, 20].  Recent gaze estimation research has	target)	EYEDIAP (screen target)
 focused on facilitating its use	target)	EYEDIAP (screen target)
 in general everyday applications under	target)	EYEDIAP (screen target)
 real-world conditions, using off-the-shelf remote	target)	EYEDIAP (screen target)
 RGB cameras and re- moving	target)	EYEDIAP (screen target)
 the need of personal calibration	target)	EYEDIAP (screen target)
 [26]. In this setting, appearance-based	target)	EYEDIAP (screen target)
 methods, which learn a mapping	target)	EYEDIAP (screen target)
 from images to gaze directions	target)	EYEDIAP (screen target)
, are the preferred 	target)	EYEDIAP (screen target)
choice [25]. How- ever, they 	target)	EYEDIAP (screen target)
need large amounts of training 	target)	EYEDIAP (screen target)
data to be able to 	target)	EYEDIAP (screen target)
generalize well to in-the-wild situations, 	target)	EYEDIAP (screen target)
which are characterized by significant 	target)	EYEDIAP (screen target)
variability in head poses, face 	target)	EYEDIAP (screen target)
appearances and lighting conditions. In 	target)	EYEDIAP (screen target)
recent years, CNNs have been 	target)	EYEDIAP (screen target)
reported to outperform classical methods. 	target)	EYEDIAP (screen target)
However, most existing approaches have 	target)	EYEDIAP (screen target)
only been tested in restricted 	target)	EYEDIAP (screen target)
HCI tasks,  c© 2018. The copyright of	target)	EYEDIAP (screen target)
 this document resides with its	target)	EYEDIAP (screen target)
 authors. It may be distributed	target)	EYEDIAP (screen target)
 unchanged freely in print or	target)	EYEDIAP (screen target)
 electronic forms	target)	EYEDIAP (screen target)
.  ar X	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
iv :1  80 5	target)	EYEDIAP (screen target)
.  03 06	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
4v 3	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
cs  .C V	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
1  7	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Se  p	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
20  18	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Liversedge and Findlay} 2000	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Rutter and Durkin} 1987	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Guillon, Hadjikhani, Baduel, 	target)	EYEDIAP (screen target)
and Rog{é}} 2014  Citation Citation {Jacob and Karn	target)	EYEDIAP (screen target)
} 2003  Citation Citation {Majaranta and Bulling	target)	EYEDIAP (screen target)
} 2014  Citation Citation {Palmero, van Dam	target)	EYEDIAP (screen target)
, Escalera, Kelia, Lichtert, Noldus, 	target)	EYEDIAP (screen target)
Spink, and van Wieringen} 2018  Citation Citation {Ono, Okabe, and	target)	EYEDIAP (screen target)
 Sato} 2006	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
2 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (screen target)
ESTIMATION  Method 3D gaze direction	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Unrestricted gaze target  Full face	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Eye region  Facial landmarks	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Sequential information  Zhang et al. (1) [42	target)	EYEDIAP (screen target)
] 3 7 7 3 7 7 Krafka et al. [16	target)	EYEDIAP (screen target)
] 7 7 3 3 7 7 Zhang et al. (2	target)	EYEDIAP (screen target)
) [43] 3 7 3 7 7 7 Deng and Zhu	target)	EYEDIAP (screen target)
 [4] 3 3 3 3	target)	EYEDIAP (screen target)
 7 7 Ours 3 3	target)	EYEDIAP (screen target)
 3 3 3 3	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Table 1: Characteristics of recent 	target)	EYEDIAP (screen target)
related work on person- and 	target)	EYEDIAP (screen target)
head pose-independent appearance-based gaze estimation 	target)	EYEDIAP (screen target)
methods using CNNs.  where users look at the	target)	EYEDIAP (screen target)
 screen or mobile phone, showing	target)	EYEDIAP (screen target)
 a low head pose variability	target)	EYEDIAP (screen target)
. It is yet unclear 	target)	EYEDIAP (screen target)
how these methods would perform 	target)	EYEDIAP (screen target)
in a wider range of 	target)	EYEDIAP (screen target)
head poses.  On a different note, until	target)	EYEDIAP (screen target)
 very recently, the majority of	target)	EYEDIAP (screen target)
 methods only used static eye	target)	EYEDIAP (screen target)
 region appearance as input. State-of-the-art	target)	EYEDIAP (screen target)
 approaches have demonstrated that using	target)	EYEDIAP (screen target)
 the face along with a	target)	EYEDIAP (screen target)
 higher resolution image of the	target)	EYEDIAP (screen target)
 eyes [16], or even just	target)	EYEDIAP (screen target)
 the face itself [43], increases	target)	EYEDIAP (screen target)
 performance. Indeed, the whole-face image	target)	EYEDIAP (screen target)
 encodes more information than eyes	target)	EYEDIAP (screen target)
 alone, such as illumination and	target)	EYEDIAP (screen target)
 head pose. Nevertheless, gaze behavior	target)	EYEDIAP (screen target)
 is not static. Eye and	target)	EYEDIAP (screen target)
 head movements allow us to	target)	EYEDIAP (screen target)
 direct our gaze to target	target)	EYEDIAP (screen target)
 locations of interest. It has	target)	EYEDIAP (screen target)
 been demonstrated that humans can	target)	EYEDIAP (screen target)
 better predict gaze when being	target)	EYEDIAP (screen target)
 shown image sequences of other	target)	EYEDIAP (screen target)
 people moving their eyes [1	target)	EYEDIAP (screen target)
]. However, it is still 	target)	EYEDIAP (screen target)
an open question whether this 	target)	EYEDIAP (screen target)
se- quential information can increase 	target)	EYEDIAP (screen target)
the performance of automatic methods.  In this work, we show	target)	EYEDIAP (screen target)
 that the combination of multiple	target)	EYEDIAP (screen target)
 cues benefits the gaze estimation	target)	EYEDIAP (screen target)
 task. In particular, we use	target)	EYEDIAP (screen target)
 face, eye region and facial	target)	EYEDIAP (screen target)
 landmarks from still images. Facial	target)	EYEDIAP (screen target)
 landmarks model the global shape	target)	EYEDIAP (screen target)
 of the face and come	target)	EYEDIAP (screen target)
 at no cost, since face	target)	EYEDIAP (screen target)
 alignment is a common pre-processing	target)	EYEDIAP (screen target)
 step in many facial image	target)	EYEDIAP (screen target)
 analysis approaches. Furthermore, we present	target)	EYEDIAP (screen target)
 a subject-independent, free-head recurrent 3D	target)	EYEDIAP (screen target)
 gaze regression network to leverage	target)	EYEDIAP (screen target)
 the temporal information of image	target)	EYEDIAP (screen target)
 sequences. The static streams of	target)	EYEDIAP (screen target)
 each frame are combined in	target)	EYEDIAP (screen target)
 a late-fusion fashion using a	target)	EYEDIAP (screen target)
 multi-stream CNN. Then, all feature	target)	EYEDIAP (screen target)
 vectors are input to a	target)	EYEDIAP (screen target)
 many-to-one recurrent module that predicts	target)	EYEDIAP (screen target)
 the gaze vector of the	target)	EYEDIAP (screen target)
 last sequence frame	target)	EYEDIAP (screen target)
.  In summary, our contributions are	target)	EYEDIAP (screen target)
 two-fold. First, we present a	target)	EYEDIAP (screen target)
 Recurrent-CNN net- work architecture that	target)	EYEDIAP (screen target)
 combines appearance, shape and temporal	target)	EYEDIAP (screen target)
 information for 3D gaze estimation	target)	EYEDIAP (screen target)
. Second, we test static 	target)	EYEDIAP (screen target)
and temporal versions of our 	target)	EYEDIAP (screen target)
solution on the EYEDIAP 	target)	EYEDIAP (screen target)
dataset [7] in a wide 	target)	EYEDIAP (screen target)
range of head poses and 	target)	EYEDIAP (screen target)
gaze directions, showing consistent perfor- 	target)	EYEDIAP (screen target)
mance improvements compared to related 	target)	EYEDIAP (screen target)
appearance-based methods. To the best 	target)	EYEDIAP (screen target)
of our knowledge, this is 	target)	EYEDIAP (screen target)
the first third-person, remote camera-based 	target)	EYEDIAP (screen target)
approach that uses tempo- ral 	target)	EYEDIAP (screen target)
information for this task. Table 1 outlines our main method characteristics	target)	EYEDIAP (screen target)
 compared to related work. Models	target)	EYEDIAP (screen target)
 and code are publicly available	target)	EYEDIAP (screen target)
 at https://github.com/ crisie/RecurrentGaze	target)	EYEDIAP (screen target)
.  2 Related work Gaze estimation	target)	EYEDIAP (screen target)
 methods are typically categorized as	target)	EYEDIAP (screen target)
 model-based or appearance-based [5, 10	target)	EYEDIAP (screen target)
, 15]. Model-based approaches use 	target)	EYEDIAP (screen target)
a geometric model of the 	target)	EYEDIAP (screen target)
eye, usually requir- ing either 	target)	EYEDIAP (screen target)
high resolution images or a 	target)	EYEDIAP (screen target)
person-specific calibration stage to estimate 	target)	EYEDIAP (screen target)
personal eye parameters [22, 33, 34, 37, 41]. In contrast, appearance-based	target)	EYEDIAP (screen target)
 methods learn a di- rect	target)	EYEDIAP (screen target)
 mapping from intensity images or	target)	EYEDIAP (screen target)
 extracted eye features to gaze	target)	EYEDIAP (screen target)
 directions, thus being	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Zhang, Sugano, Fritz, 	target)	EYEDIAP (screen target)
and Bulling} 2015  Citation Citation {Krafka, Khosla, Kellnhofer	target)	EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 	target)	EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Deng and Zhu	target)	EYEDIAP (screen target)
} 2017  Citation Citation {Krafka, Khosla, Kellnhofer	target)	EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 	target)	EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Anderson, Risko, and	target)	EYEDIAP (screen target)
 Kingstone} 2016	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Funesprotect unhbox voidb@x 	target)	EYEDIAP (screen target)
penalty @M  {}Mora, Monay, and Odobez} 2014	target)	EYEDIAP (screen target)
{}  Citation Citation {Ferhat and Vilari{ñ}o	target)	EYEDIAP (screen target)
} 2016  Citation Citation {Hansen and Ji	target)	EYEDIAP (screen target)
} 2010  Citation Citation {Kar and Corcoran	target)	EYEDIAP (screen target)
} 2017  Citation Citation {Morimoto, Amir, and	target)	EYEDIAP (screen target)
 Flickner} 2002	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Venkateswarlu etprotect unhbox 	target)	EYEDIAP (screen target)
voidb@x penalty @M  {}al.} 2003	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Wang and Ji} 2017	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Wood and Bulling} 2014	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Yoo and Chung} 2005	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
https://github.com/crisie/RecurrentGaze 	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 3	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
potentially applicable to relatively low 	target)	EYEDIAP (screen target)
resolution images and mid-distance scenarios. 	target)	EYEDIAP (screen target)
Dif- ferent mapping functions have 	target)	EYEDIAP (screen target)
been explored, such as neural 	target)	EYEDIAP (screen target)
networks [2], adaptive linear regression (	target)	EYEDIAP (screen target)
ALR) [19], local interpolation [32], 	target)	EYEDIAP (screen target)
gaussian processes [30, 35], random 	target)	EYEDIAP (screen target)
forests [11, 31], or k-nearest 	target)	EYEDIAP (screen target)
neighbors [40]. Main challenges of 	target)	EYEDIAP (screen target)
appearance-based methods for 3D gaze 	target)	EYEDIAP (screen target)
estimation are head pose, illumination 	target)	EYEDIAP (screen target)
and subject invariance without user-specific 	target)	EYEDIAP (screen target)
calibration. To handle these issues, 	target)	EYEDIAP (screen target)
some works proposed compensation 	target)	EYEDIAP (screen target)
methods [18] and warping strategies 	target)	EYEDIAP (screen target)
that synthesize a canonical, frontal 	target)	EYEDIAP (screen target)
looking view of the 	target)	EYEDIAP (screen target)
face [6, 13, 21]. Hybrid 	target)	EYEDIAP (screen target)
approaches based on analysis-by-synthesis have 	target)	EYEDIAP (screen target)
also been evaluated [39].  Currently, data-driven methods are considered	target)	EYEDIAP (screen target)
 the state of the art	target)	EYEDIAP (screen target)
 for person- and head pose-independent	target)	EYEDIAP (screen target)
 appearance-based gaze estimation. Consequently, a	target)	EYEDIAP (screen target)
 number of gaze es- timation	target)	EYEDIAP (screen target)
 datasets have been introduced in	target)	EYEDIAP (screen target)
 recent years, either in controlled	target)	EYEDIAP (screen target)
 [29] or semi- controlled settings	target)	EYEDIAP (screen target)
 [8], in the wild [16	target)	EYEDIAP (screen target)
, 42], or consisting of 	target)	EYEDIAP (screen target)
synthetic data [31, 38, 40]. 	target)	EYEDIAP (screen target)
Zhang et al. [42] showed 	target)	EYEDIAP (screen target)
that CNNs can outperform other 	target)	EYEDIAP (screen target)
mapping methods, using a multi- 	target)	EYEDIAP (screen target)
modal CNN to learn the 	target)	EYEDIAP (screen target)
mapping from 3D head poses 	target)	EYEDIAP (screen target)
and eye images to 3D 	target)	EYEDIAP (screen target)
gaze directions. Krafka et 	target)	EYEDIAP (screen target)
al. [16] proposed a multi-stream 	target)	EYEDIAP (screen target)
CNN for 2D gaze estimation, 	target)	EYEDIAP (screen target)
using individual eye, whole-face image 	target)	EYEDIAP (screen target)
and the face grid as 	target)	EYEDIAP (screen target)
input. As this method was 	target)	EYEDIAP (screen target)
limited to 2D screen mapping, 	target)	EYEDIAP (screen target)
Zhang et al. [43] later 	target)	EYEDIAP (screen target)
explored the potential of just 	target)	EYEDIAP (screen target)
using whole-face images as input 	target)	EYEDIAP (screen target)
to estimate 3D gaze directions. 	target)	EYEDIAP (screen target)
Using a spatial weights CNN, 	target)	EYEDIAP (screen target)
they demonstrated their method to 	target)	EYEDIAP (screen target)
be more robust to facial 	target)	EYEDIAP (screen target)
appearance variation caused by head 	target)	EYEDIAP (screen target)
pose and illumina- tion than 	target)	EYEDIAP (screen target)
eye-only methods. While the method 	target)	EYEDIAP (screen target)
was evaluated in the wild, 	target)	EYEDIAP (screen target)
the subjects were only interacting 	target)	EYEDIAP (screen target)
with a mobile device, thus 	target)	EYEDIAP (screen target)
restricting the head pose range. 	target)	EYEDIAP (screen target)
Deng and Zhu [4] presented 	target)	EYEDIAP (screen target)
a two-stream CNN to disjointly 	target)	EYEDIAP (screen target)
model head pose from face 	target)	EYEDIAP (screen target)
images and eye- ball movement 	target)	EYEDIAP (screen target)
from eye region images. Both 	target)	EYEDIAP (screen target)
were then aggregated into 3D 	target)	EYEDIAP (screen target)
gaze direction using a gaze 	target)	EYEDIAP (screen target)
transform layer. The decomposition was 	target)	EYEDIAP (screen target)
aimed to avoid head-correlation over- 	target)	EYEDIAP (screen target)
fitting of previous data-driven approaches. 	target)	EYEDIAP (screen target)
They evaluated their approach in 	target)	EYEDIAP (screen target)
the wild with a wider 	target)	EYEDIAP (screen target)
range of head poses, obtaining 	target)	EYEDIAP (screen target)
better performance than previous eye-based 	target)	EYEDIAP (screen target)
methods. However, they did not 	target)	EYEDIAP (screen target)
test it on public annotated 	target)	EYEDIAP (screen target)
benchmark datasets.  In this paper, we propose	target)	EYEDIAP (screen target)
 a multi-stream recurrent CNN network	target)	EYEDIAP (screen target)
 for person- and head pose-independent	target)	EYEDIAP (screen target)
 3D gaze estimation for a	target)	EYEDIAP (screen target)
 mid-distance scenario. We evaluate it	target)	EYEDIAP (screen target)
 on a wider range of	target)	EYEDIAP (screen target)
 head poses and gaze directions	target)	EYEDIAP (screen target)
 than screen-targeted approaches. As opposed	target)	EYEDIAP (screen target)
 to previous methods, we also	target)	EYEDIAP (screen target)
 rely on temporal information inherent	target)	EYEDIAP (screen target)
 in sequential data	target)	EYEDIAP (screen target)
.  3 Methodology	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
In this section, we present 	target)	EYEDIAP (screen target)
our approach for 3D gaze 	target)	EYEDIAP (screen target)
regression based on appearance and 	target)	EYEDIAP (screen target)
shape cues for still images 	target)	EYEDIAP (screen target)
and image sequences. First, we 	target)	EYEDIAP (screen target)
introduce the data modalities and 	target)	EYEDIAP (screen target)
formulate the problem. Then, we 	target)	EYEDIAP (screen target)
detail the normalization procedure prior 	target)	EYEDIAP (screen target)
to the regression stage. Finally, 	target)	EYEDIAP (screen target)
we explain the global network 	target)	EYEDIAP (screen target)
topology as well as the 	target)	EYEDIAP (screen target)
implementation details. An overview of 	target)	EYEDIAP (screen target)
the system architecture is depicted 	target)	EYEDIAP (screen target)
in Figure 1.  3.1 Multi-modal gaze regression	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Let us represent gaze direction 	target)	EYEDIAP (screen target)
as a 3D unit vector 	target)	EYEDIAP (screen target)
g = [gx,gy,gz]T ∈R3 in 	target)	EYEDIAP (screen target)
the Camera Coor- dinate System (	target)	EYEDIAP (screen target)
CCS), whose origin is the 	target)	EYEDIAP (screen target)
central point between eyeball centers. 	target)	EYEDIAP (screen target)
Assuming a calibrated camera, and 	target)	EYEDIAP (screen target)
a known head position and 	target)	EYEDIAP (screen target)
orientation, our goal is to 	target)	EYEDIAP (screen target)
estimate g from a sequence 	target)	EYEDIAP (screen target)
of images {I(i) | 	target)	EYEDIAP (screen target)
I ∈ RW×H×3} as a 	target)	EYEDIAP (screen target)
regression problem.  Citation Citation {Baluja and Pomerleau	target)	EYEDIAP (screen target)
} 1994  Citation Citation {Lu, Sugano, Okabe	target)	EYEDIAP (screen target)
, and Sato} 2011{}  Citation Citation {Tan, Kriegman, and	target)	EYEDIAP (screen target)
 Ahuja} 2002	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Sugano, Matsushita, and 	target)	EYEDIAP (screen target)
Sato} 2013  Citation Citation {Williams, Blake, and	target)	EYEDIAP (screen target)
 Cipolla} 2006	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Huang, Veeraraghavan, and 	target)	EYEDIAP (screen target)
Sabharwal} 2017  Citation Citation {Sugano, Matsushita, and	target)	EYEDIAP (screen target)
 Sato} 2014	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Wood, Baltru{²}aitis, Morency, 	target)	EYEDIAP (screen target)
Robinson, and Bulling} 2016{}  Citation Citation {Lu, Okabe, Sugano	target)	EYEDIAP (screen target)
, and Sato} 2011{}  Citation Citation {Funes-Mora and Odobez	target)	EYEDIAP (screen target)
} 2016  Citation Citation {Jeni and Cohn	target)	EYEDIAP (screen target)
} 2016  Citation Citation {Mora and Odobez	target)	EYEDIAP (screen target)
} 2012  Citation Citation {Wood, Baltru{²}aitis, Morency	target)	EYEDIAP (screen target)
, Robinson, and Bulling} 2016{}  Citation Citation {Smith, Yin, Feiner	target)	EYEDIAP (screen target)
, and Nayar} 2013  Citation Citation {Funesprotect unhbox voidb@x	target)	EYEDIAP (screen target)
 penalty @M	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Mora, Monay, and Odobez} 2014{}  Citation Citation {Krafka, Khosla, Kellnhofer	target)	EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 	target)	EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (screen target)
, and Bulling} 2015  Citation Citation {Sugano, Matsushita, and	target)	EYEDIAP (screen target)
 Sato} 2014	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Wood, Baltrusaitis, Zhang, 	target)	EYEDIAP (screen target)
Sugano, Robinson, and Bulling} 2015  Citation Citation {Wood, Baltru{²}aitis, Morency	target)	EYEDIAP (screen target)
, Robinson, and Bulling} 2016{}  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (screen target)
, and Bulling} 2015  Citation Citation {Krafka, Khosla, Kellnhofer	target)	EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 	target)	EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Deng and Zhu	target)	EYEDIAP (screen target)
} 2017	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
4 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (screen target)
ESTIMATION  Conv	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
C on ca t  x y z x y	target)	EYEDIAP (screen target)
 z x y z	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Individual Fusion Temporal  Individual Fusion	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Input 	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Individual Fusion  Normalization	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
 .Conv	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Conv .  Conv	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Conv .  FC	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
FC FC RNN  RNN	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
RNN FC  Ti m e	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Figure 1: Overview of the 	target)	EYEDIAP (screen target)
proposed network. A multi-stream CNN 	target)	EYEDIAP (screen target)
jointly models full-face, eye region 	target)	EYEDIAP (screen target)
appearance and face landmarks from 	target)	EYEDIAP (screen target)
still images. The combined extracted 	target)	EYEDIAP (screen target)
fea- tures from each frame 	target)	EYEDIAP (screen target)
are fed into a recurrent 	target)	EYEDIAP (screen target)
module to predict last frame’s 	target)	EYEDIAP (screen target)
gaze direction.  Gazing to a specific target	target)	EYEDIAP (screen target)
 is achieved by a combination	target)	EYEDIAP (screen target)
 of eye and head movements	target)	EYEDIAP (screen target)
, which are highly coordinated. 	target)	EYEDIAP (screen target)
Consequently, the apparent direction of 	target)	EYEDIAP (screen target)
gaze is influenced not only 	target)	EYEDIAP (screen target)
by the location of the 	target)	EYEDIAP (screen target)
irises within the eyelid aperture, 	target)	EYEDIAP (screen target)
but also by the position 	target)	EYEDIAP (screen target)
and orientation of the face 	target)	EYEDIAP (screen target)
with respect to the camera. 	target)	EYEDIAP (screen target)
Known as the Wollaston 	target)	EYEDIAP (screen target)
effect [36], the exact same 	target)	EYEDIAP (screen target)
set of eyes may appear 	target)	EYEDIAP (screen target)
to be looking in different 	target)	EYEDIAP (screen target)
directions due to the surrounding 	target)	EYEDIAP (screen target)
facial cues. It is therefore 	target)	EYEDIAP (screen target)
reasonable to state that eye 	target)	EYEDIAP (screen target)
images are not sufficient to 	target)	EYEDIAP (screen target)
estimate gaze direction. Instead, whole-face 	target)	EYEDIAP (screen target)
images can encode head pose 	target)	EYEDIAP (screen target)
or illumination-specific information across larger 	target)	EYEDIAP (screen target)
areas than those available just 	target)	EYEDIAP (screen target)
in the eyes region [16, 43	target)	EYEDIAP (screen target)
].  The drawback of appearance-only methods	target)	EYEDIAP (screen target)
 is that global structure information	target)	EYEDIAP (screen target)
 is not explicitly considered. In	target)	EYEDIAP (screen target)
 that sense, facial landmarks can	target)	EYEDIAP (screen target)
 be used as global shape	target)	EYEDIAP (screen target)
 cues to en- code spatial	target)	EYEDIAP (screen target)
 relationships and geometric constraints. Current	target)	EYEDIAP (screen target)
 state-of-the-art face alignment approaches are	target)	EYEDIAP (screen target)
 robust enough to handle large	target)	EYEDIAP (screen target)
 appearance variability, extreme head poses	target)	EYEDIAP (screen target)
 and occlusions, being especially useful	target)	EYEDIAP (screen target)
 when the dataset used for	target)	EYEDIAP (screen target)
 gaze estimation does not contain	target)	EYEDIAP (screen target)
 such variability. Facial landmarks are	target)	EYEDIAP (screen target)
 mainly correlated with head orientation	target)	EYEDIAP (screen target)
, eye position, eyelid openness, 	target)	EYEDIAP (screen target)
and eyebrow movement, which are 	target)	EYEDIAP (screen target)
valuable features for our task.  Therefore, in our approach we	target)	EYEDIAP (screen target)
 jointly model appearance and shape	target)	EYEDIAP (screen target)
 cues (see Figure 1). The	target)	EYEDIAP (screen target)
 former is represented by a	target)	EYEDIAP (screen target)
 whole-face image IF , along	target)	EYEDIAP (screen target)
 with a higher resolution image	target)	EYEDIAP (screen target)
 of the eyes IE to	target)	EYEDIAP (screen target)
 identify subtle changes. Due to	target)	EYEDIAP (screen target)
 dealing with wide head pose	target)	EYEDIAP (screen target)
 ranges, some eye images may	target)	EYEDIAP (screen target)
 not depict the whole eye	target)	EYEDIAP (screen target)
, containing mostly background or 	target)	EYEDIAP (screen target)
other surrounding facial parts instead. 	target)	EYEDIAP (screen target)
For that reason, and contrary 	target)	EYEDIAP (screen target)
to previous approaches that only 	target)	EYEDIAP (screen target)
use one eye image [31, 42	target)	EYEDIAP (screen target)
], we use a single 	target)	EYEDIAP (screen target)
image composed of two patches 	target)	EYEDIAP (screen target)
of centered left and right 	target)	EYEDIAP (screen target)
eyes. Finally, the shape cue 	target)	EYEDIAP (screen target)
is represented by 3D face 	target)	EYEDIAP (screen target)
landmarks obtained from a 68-landmark 	target)	EYEDIAP (screen target)
model, denoted by 	target)	EYEDIAP (screen target)
L = {(lx, ly, 	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
)	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
 | ∀c ∈ [1, ...,68	target)	EYEDIAP (screen target)
]}.  In this work we also	target)	EYEDIAP (screen target)
 consider the dynamic component of	target)	EYEDIAP (screen target)
 gaze. We leverage the se	target)	EYEDIAP (screen target)
- quential information of eye 	target)	EYEDIAP (screen target)
and head movements such that, 	target)	EYEDIAP (screen target)
given appearance and shape features 	target)	EYEDIAP (screen target)
of consecutive frames, it is 	target)	EYEDIAP (screen target)
possible to better predict the 	target)	EYEDIAP (screen target)
gaze direction of the cur- 	target)	EYEDIAP (screen target)
rent frame. Therefore, the 3D 	target)	EYEDIAP (screen target)
gaze estimation task for a 1	target)	EYEDIAP (screen target)
-frame sequence is formulated  Citation Citation {Wollaston etprotect unhbox	target)	EYEDIAP (screen target)
 voidb@x penalty @M	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
al.} 1824  Citation Citation {Krafka, Khosla, Kellnhofer	target)	EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 	target)	EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Sugano, Matsushita, and	target)	EYEDIAP (screen target)
 Sato} 2014	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Zhang, Sugano, Fritz, 	target)	EYEDIAP (screen target)
and Bulling} 2015	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 5	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
as g(i) = f ( {IF (i)},{IE (i)},{L(i	target)	EYEDIAP (screen target)
)}  ) , where i denotes	target)	EYEDIAP (screen target)
 the i-th frame, and f	target)	EYEDIAP (screen target)
 is the regression	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
function.  3.2 Data normalization Prior to	target)	EYEDIAP (screen target)
 gaze regression, a normalization step	target)	EYEDIAP (screen target)
 in the 3D space and	target)	EYEDIAP (screen target)
 the 2D image, similar to	target)	EYEDIAP (screen target)
 [31], is carried out. This	target)	EYEDIAP (screen target)
 is performed to reduce the	target)	EYEDIAP (screen target)
 appearance variability and to allow	target)	EYEDIAP (screen target)
 the gaze estimation model to	target)	EYEDIAP (screen target)
 be applied regardless of the	target)	EYEDIAP (screen target)
 original camera configuration	target)	EYEDIAP (screen target)
.  Let H ∈ R3x3 be	target)	EYEDIAP (screen target)
 the head rotation matrix, and	target)	EYEDIAP (screen target)
 p = [px, py, pz]T	target)	EYEDIAP (screen target)
 ∈ R3 the reference face	target)	EYEDIAP (screen target)
 location with respect to the	target)	EYEDIAP (screen target)
 original CCS. The goal is	target)	EYEDIAP (screen target)
 to find the conversion matrix	target)	EYEDIAP (screen target)
 M = SR such that	target)	EYEDIAP (screen target)
 (a) the X-axes of the	target)	EYEDIAP (screen target)
 virtual camera and the head	target)	EYEDIAP (screen target)
 become parallel using the rotation	target)	EYEDIAP (screen target)
 matrix R, and (b) the	target)	EYEDIAP (screen target)
 virtual camera looks at the	target)	EYEDIAP (screen target)
 reference location from a fixed	target)	EYEDIAP (screen target)
 distance dn using the Z-direction	target)	EYEDIAP (screen target)
 scaling matrix S = diag(1,1,dn/‖p	target)	EYEDIAP (screen target)
‖). R is computed as 	target)	EYEDIAP (screen target)
a = p̂×HT e1, 	target)	EYEDIAP (screen target)
b = â× p̂, 	target)	EYEDIAP (screen target)
R = [â, b̂, p̂]T , where e1 denotes the first	target)	EYEDIAP (screen target)
 orthonormal basis and	target)	EYEDIAP (screen target)
 〈 ·̂ 〉 is the	target)	EYEDIAP (screen target)
 unit vector	target)	EYEDIAP (screen target)
.  This normalization translates into the	target)	EYEDIAP (screen target)
 image space as a cropped	target)	EYEDIAP (screen target)
 image patch of size Wn×Hn	target)	EYEDIAP (screen target)
 centered at p where head	target)	EYEDIAP (screen target)
 roll rotation has been removed	target)	EYEDIAP (screen target)
. This is done by 	target)	EYEDIAP (screen target)
applying a perspective warping to 	target)	EYEDIAP (screen target)
the input image I using 	target)	EYEDIAP (screen target)
the transformation matrix W = 	target)	EYEDIAP (screen target)
CoMCn−1, where Co and Cn 	target)	EYEDIAP (screen target)
are the original and virtual 	target)	EYEDIAP (screen target)
camera matrices, respectively.  The 3D gaze vector is	target)	EYEDIAP (screen target)
 also normalized as gn =Rg	target)	EYEDIAP (screen target)
. After image normalization, the 	target)	EYEDIAP (screen target)
line of sight can be 	target)	EYEDIAP (screen target)
represented in a 2D space. 	target)	EYEDIAP (screen target)
Therefore, gn is further transformed 	target)	EYEDIAP (screen target)
to spherical coor- dinates (θ ,	target)	EYEDIAP (screen target)
φ) assuming unit length, where 	target)	EYEDIAP (screen target)
θ and φ denote the 	target)	EYEDIAP (screen target)
horizontal and vertical direc- tion 	target)	EYEDIAP (screen target)
angles, respectively. This 2D angle 	target)	EYEDIAP (screen target)
representation, delimited in the 	target)	EYEDIAP (screen target)
range [−π/2,π/2], is computed as 	target)	EYEDIAP (screen target)
θ = arctan(gx/gz) and 	target)	EYEDIAP (screen target)
φ = arcsin(−gy), such that (0,	target)	EYEDIAP (screen target)
0) represents looking straight ahead 	target)	EYEDIAP (screen target)
to the CCS origin.  3.3 Recurrent Convolutional Neural Network	target)	EYEDIAP (screen target)
 We propose a Recurrent CNN	target)	EYEDIAP (screen target)
 Regression Network for 3D gaze	target)	EYEDIAP (screen target)
 estimation. The network is divided	target)	EYEDIAP (screen target)
 in 3 modules: (1) Individual	target)	EYEDIAP (screen target)
, (2) Fusion, and (3) 	target)	EYEDIAP (screen target)
Temporal.  First, the Individual module learns	target)	EYEDIAP (screen target)
 features from each appearance cue	target)	EYEDIAP (screen target)
 separately. It consists of a	target)	EYEDIAP (screen target)
 two-stream CNN, one devoted to	target)	EYEDIAP (screen target)
 the normalized face image stream	target)	EYEDIAP (screen target)
 and the other to the	target)	EYEDIAP (screen target)
 joint normalized eyes image. Next	target)	EYEDIAP (screen target)
, the Fusion module combines 	target)	EYEDIAP (screen target)
the extracted features of each 	target)	EYEDIAP (screen target)
appearance stream in a single 	target)	EYEDIAP (screen target)
vector along with the normalized 	target)	EYEDIAP (screen target)
landmark coordinates. Then, it learns 	target)	EYEDIAP (screen target)
a joint representation between modalities 	target)	EYEDIAP (screen target)
in a late-fusion fashion. Both 	target)	EYEDIAP (screen target)
Individual and Fusion modules, further 	target)	EYEDIAP (screen target)
referred to as Static model, 	target)	EYEDIAP (screen target)
are applied to each frame 	target)	EYEDIAP (screen target)
of the sequence. Finally, the 	target)	EYEDIAP (screen target)
resulting feature vectors of each 	target)	EYEDIAP (screen target)
frame are input to the 	target)	EYEDIAP (screen target)
Temporal module based on a 	target)	EYEDIAP (screen target)
many-to-one recurrent network. This module 	target)	EYEDIAP (screen target)
leverages sequential information to predict 	target)	EYEDIAP (screen target)
the normalized 2D gaze angles 	target)	EYEDIAP (screen target)
of the last frame of 	target)	EYEDIAP (screen target)
the sequence using a linear 	target)	EYEDIAP (screen target)
regression layer added on top 	target)	EYEDIAP (screen target)
of it.  3.4 Implementation details 3.4.1 Network	target)	EYEDIAP (screen target)
 details	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Each stream of the Individual 	target)	EYEDIAP (screen target)
module is based on the 	target)	EYEDIAP (screen target)
VGG-16 deep network [27], consisting 	target)	EYEDIAP (screen target)
of 13 convolutional layers, 5 	target)	EYEDIAP (screen target)
max pooling layers, and 1 	target)	EYEDIAP (screen target)
fully connected (FC) layer with 	target)	EYEDIAP (screen target)
Rec- tified Linear Unit (ReLU) 	target)	EYEDIAP (screen target)
activations. The full-face stream follows 	target)	EYEDIAP (screen target)
the same configuration  Citation Citation {Sugano, Matsushita, and	target)	EYEDIAP (screen target)
 Sato} 2014	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Parkhi, Vedaldi, and 	target)	EYEDIAP (screen target)
Zisserman} 2015	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
6 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (screen target)
ESTIMATION  as the base network, having	target)	EYEDIAP (screen target)
 an input of 224×224 pixels	target)	EYEDIAP (screen target)
 and a 4096D FC layer	target)	EYEDIAP (screen target)
. In contrast, the input 	target)	EYEDIAP (screen target)
joint eye image is smaller, 	target)	EYEDIAP (screen target)
with a final size of 120	target)	EYEDIAP (screen target)
×48 pixels, so the number 	target)	EYEDIAP (screen target)
of pa- rameters is decreased 	target)	EYEDIAP (screen target)
proportionally. In this case, its 	target)	EYEDIAP (screen target)
last FC layer produces a 	target)	EYEDIAP (screen target)
1536D vector. A 204D landmark 	target)	EYEDIAP (screen target)
coordinates vector is concatenated to 	target)	EYEDIAP (screen target)
the output of the FC 	target)	EYEDIAP (screen target)
layer of each stream, resulting 	target)	EYEDIAP (screen target)
in a 5836D feature vector. 	target)	EYEDIAP (screen target)
Consequently, the Fusion module consists 	target)	EYEDIAP (screen target)
of 2 5836D FC layers 	target)	EYEDIAP (screen target)
with ReLU activations and 2 	target)	EYEDIAP (screen target)
dropout layers between FCs as 	target)	EYEDIAP (screen target)
regularization. Finally, to model the 	target)	EYEDIAP (screen target)
temporal dependencies, we use a 	target)	EYEDIAP (screen target)
single GRU layer with 128 	target)	EYEDIAP (screen target)
units.  The network is trained in	target)	EYEDIAP (screen target)
 a stage-wise fashion. First, we	target)	EYEDIAP (screen target)
 train the Static model and	target)	EYEDIAP (screen target)
 the final regression layer end-to-end	target)	EYEDIAP (screen target)
 on each individual frame of	target)	EYEDIAP (screen target)
 the training data. The convolutional	target)	EYEDIAP (screen target)
 blocks are pre-trained with the	target)	EYEDIAP (screen target)
 VGG-Face dataset [27], whereas the	target)	EYEDIAP (screen target)
 FCs are trained from scratch	target)	EYEDIAP (screen target)
. Second, the training data 	target)	EYEDIAP (screen target)
is re-arranged by means of 	target)	EYEDIAP (screen target)
a sliding window with stride 1 to build input sequences. Each	target)	EYEDIAP (screen target)
 sequence is composed of s	target)	EYEDIAP (screen target)
 = 4 consecutive frames, whose	target)	EYEDIAP (screen target)
 gaze direction target is the	target)	EYEDIAP (screen target)
 gaze direction of the last	target)	EYEDIAP (screen target)
 frame of the sequence( {I(i−s+1	target)	EYEDIAP (screen target)
), . . . ,I(i)}, 	target)	EYEDIAP (screen target)
g(i)  ) . Using this re-arranged	target)	EYEDIAP (screen target)
 training data, we extract features	target)	EYEDIAP (screen target)
 of each	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
frame of the sequence from 	target)	EYEDIAP (screen target)
a frozen Individual module, fine-tune 	target)	EYEDIAP (screen target)
the Fusion layers, and train 	target)	EYEDIAP (screen target)
both, the Temporal module and 	target)	EYEDIAP (screen target)
a new final regression layer 	target)	EYEDIAP (screen target)
from scratch. This way, the 	target)	EYEDIAP (screen target)
network can exploit the temporal 	target)	EYEDIAP (screen target)
information to further refine the 	target)	EYEDIAP (screen target)
fusion weights.  We trained the model using	target)	EYEDIAP (screen target)
 ADAM optimizer with an initial	target)	EYEDIAP (screen target)
 learning rate of 0.0001, dropout	target)	EYEDIAP (screen target)
 of 0.3, and batch size	target)	EYEDIAP (screen target)
 of 64 frames. The number	target)	EYEDIAP (screen target)
 of epochs was experimentally set	target)	EYEDIAP (screen target)
 to 21 for the first	target)	EYEDIAP (screen target)
 training stage and 10 for	target)	EYEDIAP (screen target)
 the second. We use the	target)	EYEDIAP (screen target)
 average Euclidean distance between the	target)	EYEDIAP (screen target)
 predicted and ground-truth 3D gaze	target)	EYEDIAP (screen target)
 vectors as loss function	target)	EYEDIAP (screen target)
.  3.4.2 Input pre-processing	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
For this work we use 	target)	EYEDIAP (screen target)
head pose and eye locations 	target)	EYEDIAP (screen target)
in the 3D scene provided 	target)	EYEDIAP (screen target)
by the dataset. The 3D 	target)	EYEDIAP (screen target)
landmarks are extracted using the 	target)	EYEDIAP (screen target)
state-of-the-art method of Bulat and 	target)	EYEDIAP (screen target)
Tzimiropou- los [3], which is 	target)	EYEDIAP (screen target)
based on stacked hourglass 	target)	EYEDIAP (screen target)
networks [24].  During training, the original image	target)	EYEDIAP (screen target)
 is pre-processed to get the	target)	EYEDIAP (screen target)
 two normalized input images. The	target)	EYEDIAP (screen target)
 normalized whole-face patch is centered	target)	EYEDIAP (screen target)
 0.1 meters ahead of the	target)	EYEDIAP (screen target)
 head center in the head	target)	EYEDIAP (screen target)
 coordinate system, and Cn is	target)	EYEDIAP (screen target)
 defined such that the image	target)	EYEDIAP (screen target)
 has size of 250× 250	target)	EYEDIAP (screen target)
 pixels. The difference between this	target)	EYEDIAP (screen target)
 size and the final input	target)	EYEDIAP (screen target)
 size allows us to perform	target)	EYEDIAP (screen target)
 random cropping and zooming to	target)	EYEDIAP (screen target)
 augment the data (explained in	target)	EYEDIAP (screen target)
 Section 4.1). Similarly, each normalized	target)	EYEDIAP (screen target)
 eye patch is centered in	target)	EYEDIAP (screen target)
 their respective eye center locations	target)	EYEDIAP (screen target)
. In this case, the 	target)	EYEDIAP (screen target)
virtual camera matrix is defined 	target)	EYEDIAP (screen target)
so that the image is 	target)	EYEDIAP (screen target)
cropped to 70×58, while in 	target)	EYEDIAP (screen target)
practice the final patches have 	target)	EYEDIAP (screen target)
size of 60×48. Landmarks are 	target)	EYEDIAP (screen target)
normalized using the same procedure 	target)	EYEDIAP (screen target)
and further pre-processed with mean 	target)	EYEDIAP (screen target)
subtraction and min-max normalization per 	target)	EYEDIAP (screen target)
axis. Finally, we divide them 	target)	EYEDIAP (screen target)
by a scaling factor w 	target)	EYEDIAP (screen target)
such that all coordinates are 	target)	EYEDIAP (screen target)
in the range [0,w]. This 	target)	EYEDIAP (screen target)
way, all concatenated feature values 	target)	EYEDIAP (screen target)
are in a similar range. 	target)	EYEDIAP (screen target)
After inference, the predicted normalized 	target)	EYEDIAP (screen target)
2D angles are de-normalized back 	target)	EYEDIAP (screen target)
to the original 3D space.  4 Experiments In this section	target)	EYEDIAP (screen target)
, we evaluate the cross-subject 	target)	EYEDIAP (screen target)
3D gaze estimation task on 	target)	EYEDIAP (screen target)
a wide range of head 	target)	EYEDIAP (screen target)
poses and gaze directions. Furthermore, 	target)	EYEDIAP (screen target)
we validate the effectiveness of 	target)	EYEDIAP (screen target)
the proposed architecture comparing both 	target)	EYEDIAP (screen target)
static and temporal approaches. We 	target)	EYEDIAP (screen target)
report the error in terms 	target)	EYEDIAP (screen target)
of mean angular error between 	target)	EYEDIAP (screen target)
predicted and ground-truth 3D gaze 	target)	EYEDIAP (screen target)
vectors. Note that due to 	target)	EYEDIAP (screen target)
the requirements of the temporal 	target)	EYEDIAP (screen target)
model not all the frames 	target)	EYEDIAP (screen target)
obtain a prediction. Therefore, for 	target)	EYEDIAP (screen target)
a  Citation Citation {Parkhi, Vedaldi, and	target)	EYEDIAP (screen target)
 Zisserman} 2015	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Bulat and Tzimiropoulos} 2017	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Newell, Yang, and 	target)	EYEDIAP (screen target)
Deng} 2016	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 7	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
60 30 0 30 60  60	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
30  0	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
30  60	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
100  101	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
102  60 30 0 30 60	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
60  30	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
0  30	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
60  100	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
101  102	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
103  60 30 0 30 60	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
60  30	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
0  30	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
60  100	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
101  102	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
60 30 0 30 60  60	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
30  0	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
30  60	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
100  101	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
102  103	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
a) g (FT ) (b) 	target)	EYEDIAP (screen target)
h (FT ) (c) g (	target)	EYEDIAP (screen target)
CS) (d) h (CS)  Figure 2: Ground-truth eye gaze	target)	EYEDIAP (screen target)
 g and head orientation h	target)	EYEDIAP (screen target)
 distribution on the filtered EYE	target)	EYEDIAP (screen target)
- DIAP dataset for CS 	target)	EYEDIAP (screen target)
and FT settings, in terms 	target)	EYEDIAP (screen target)
of x- and y- angles.  fair comparison, the reported results	target)	EYEDIAP (screen target)
 for static models disregard such	target)	EYEDIAP (screen target)
 frames when temporal models are	target)	EYEDIAP (screen target)
 included in the comparison	target)	EYEDIAP (screen target)
.  4.1 Training data	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
There are few publicly available 	target)	EYEDIAP (screen target)
datasets devoted to 3D gaze 	target)	EYEDIAP (screen target)
estimation and most of them 	target)	EYEDIAP (screen target)
focus on HCI with a 	target)	EYEDIAP (screen target)
limited range of head pose 	target)	EYEDIAP (screen target)
and gaze directions. Therefore, we 	target)	EYEDIAP (screen target)
use VGA videos from the 	target)	EYEDIAP (screen target)
publicly-available EYEDIAP dataset [7] to 	target)	EYEDIAP (screen target)
perform the experimental evaluation, as 	target)	EYEDIAP (screen target)
it is currently the only 	target)	EYEDIAP (screen target)
one containing video sequences with 	target)	EYEDIAP (screen target)
a wide range of head 	target)	EYEDIAP (screen target)
poses and showing the full 	target)	EYEDIAP (screen target)
face. This dataset consists of 3	target)	EYEDIAP (screen target)
-minute videos of 16 subjects 	target)	EYEDIAP (screen target)
looking at two types of 	target)	EYEDIAP (screen target)
targets: continuous screen targets on 	target)	EYEDIAP (screen target)
a fixed monitor (CS), and 	target)	EYEDIAP (screen target)
floating physical targets (FT ). 	target)	EYEDIAP (screen target)
The videos are further divided 	target)	EYEDIAP (screen target)
into static (S) and moving (	target)	EYEDIAP (screen target)
M) head pose for each 	target)	EYEDIAP (screen target)
of the subjects. Subjects 12-16 	target)	EYEDIAP (screen target)
were recorded with 2 different 	target)	EYEDIAP (screen target)
lighting conditions.  For evaluation, we filtered out	target)	EYEDIAP (screen target)
 those frames that fulfilled at	target)	EYEDIAP (screen target)
 least one of the following	target)	EYEDIAP (screen target)
 conditions: (1) face or landmarks	target)	EYEDIAP (screen target)
 not detected; (2) subject not	target)	EYEDIAP (screen target)
 looking at the target; (3	target)	EYEDIAP (screen target)
) 3D head pose, eyes 	target)	EYEDIAP (screen target)
or target location not properly 	target)	EYEDIAP (screen target)
recovered; and (4) eyeball rotations 	target)	EYEDIAP (screen target)
violating physical 	target)	EYEDIAP (screen target)
constraints (|θ | ≤ 40	target)	EYEDIAP (screen target)
◦, |φ | ≤ 30	target)	EYEDIAP (screen target)
◦) [23]. Note that we 	target)	EYEDIAP (screen target)
purposely do not filter eye 	target)	EYEDIAP (screen target)
blinking moments to learn their 	target)	EYEDIAP (screen target)
dynamics with the temporal model, 	target)	EYEDIAP (screen target)
which may produce some outliers 	target)	EYEDIAP (screen target)
with a higher prediction error 	target)	EYEDIAP (screen target)
due to a less accurate 	target)	EYEDIAP (screen target)
ground truth. Figure 2 shows 	target)	EYEDIAP (screen target)
the distribution of gaze directions 	target)	EYEDIAP (screen target)
and head poses for both 	target)	EYEDIAP (screen target)
filtered CS and FT cases.  We applied data augmentation to	target)	EYEDIAP (screen target)
 the training set with the	target)	EYEDIAP (screen target)
 following random transforma- tions: horizontal	target)	EYEDIAP (screen target)
 flip, shifts of up to	target)	EYEDIAP (screen target)
 5 pixels, zoom of up	target)	EYEDIAP (screen target)
 to 2%, brightness changes by	target)	EYEDIAP (screen target)
 a factor in the range	target)	EYEDIAP (screen target)
 [0.4,1.75], and additive Gaussian noise	target)	EYEDIAP (screen target)
 with σ2 = 0.03	target)	EYEDIAP (screen target)
.  4.2 Evaluation of static modalities	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
First, we evaluate the contribution 	target)	EYEDIAP (screen target)
of each static modality on 	target)	EYEDIAP (screen target)
the FT scenario. We divided 	target)	EYEDIAP (screen target)
the 16 participants into 4 	target)	EYEDIAP (screen target)
groups, such that appearance variability 	target)	EYEDIAP (screen target)
was maximized while maintaining a 	target)	EYEDIAP (screen target)
similar number of training samples 	target)	EYEDIAP (screen target)
per group. Each static model 	target)	EYEDIAP (screen target)
was trained end-to-end performing 4-fold 	target)	EYEDIAP (screen target)
cross-validation using different combinations of 	target)	EYEDIAP (screen target)
input modal- ities. Since the 	target)	EYEDIAP (screen target)
number of fusion units depends 	target)	EYEDIAP (screen target)
on the number of input 	target)	EYEDIAP (screen target)
modalities, we also compare different 	target)	EYEDIAP (screen target)
fusion layer sizes. The effect 	target)	EYEDIAP (screen target)
of data normalization is also 	target)	EYEDIAP (screen target)
evaluated by training a not-normalized 	target)	EYEDIAP (screen target)
face model where the input 	target)	EYEDIAP (screen target)
image is the face bounding 	target)	EYEDIAP (screen target)
box with square size the 	target)	EYEDIAP (screen target)
maximum distance between 2D landmarks.  Citation Citation {Funesprotect unhbox voidb@x	target)	EYEDIAP (screen target)
 penalty @M	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Mora, Monay, and Odobez} 2014{}  Citation Citation {MSC	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
8 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (screen target)
ESTIMATION  0 1 2 3 4	target)	EYEDIAP (screen target)
 5 6 7 8 9	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
10 11  An gl	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
e  er	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
ro r (  de gr	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
ee s)  6.9 6.43 5.58 5.71 5.59	target)	EYEDIAP (screen target)
 5.55 5.52	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
OF-4096 NE-1536 NF-4096  NF-5632 NFL-4300	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
NFE-5632 NFEL-5836  Figure 3: Performance evaluation of	target)	EYEDIAP (screen target)
 the Static network using different	target)	EYEDIAP (screen target)
 input modali- ties (O	target)	EYEDIAP (screen target)
 - Not normalized, N	target)	EYEDIAP (screen target)
 - Normalized, F - Face	target)	EYEDIAP (screen target)
, E - Eyes, 	target)	EYEDIAP (screen target)
L - 3D Landmarks) and 	target)	EYEDIAP (screen target)
size of fusion layers on 	target)	EYEDIAP (screen target)
the FT scenario.  Floating Target Screen Target 0	target)	EYEDIAP (screen target)
 1 2 3 4 5	target)	EYEDIAP (screen target)
 6 7 8 9	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
10 11  An gl	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
e  er	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
ro r (  de gr	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
ee s)  6.36 5.43 5.19 4.2 3.38	target)	EYEDIAP (screen target)
 3.4	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
MPIIGaze Static Temporal  Figure 4: Performance comparison among	target)	EYEDIAP (screen target)
 MPIIGaze method [42] and our	target)	EYEDIAP (screen target)
 Static and Temporal versions of	target)	EYEDIAP (screen target)
 the proposed network for FT	target)	EYEDIAP (screen target)
 and CS scenarios	target)	EYEDIAP (screen target)
.  As shown in Figure 3	target)	EYEDIAP (screen target)
, all models that take 	target)	EYEDIAP (screen target)
normalized full-face information as input 	target)	EYEDIAP (screen target)
achieve better performance than the 	target)	EYEDIAP (screen target)
eyes-only model. More specifically, the 	target)	EYEDIAP (screen target)
combination of face, eyes and 	target)	EYEDIAP (screen target)
landmarks outperforms all the other 	target)	EYEDIAP (screen target)
combinations by a small but 	target)	EYEDIAP (screen target)
significant margin (paired Wilcoxon test, 	target)	EYEDIAP (screen target)
p < 0.0001). The standard 	target)	EYEDIAP (screen target)
deviation of the best-performing model 	target)	EYEDIAP (screen target)
is reduced compared to the 	target)	EYEDIAP (screen target)
face and eyes model, suggesting 	target)	EYEDIAP (screen target)
a regularizing effect due to 	target)	EYEDIAP (screen target)
the addition of landmarks. The 	target)	EYEDIAP (screen target)
not-normalized face-only model shows the 	target)	EYEDIAP (screen target)
largest error, proving the impact 	target)	EYEDIAP (screen target)
of normalization to reduce the 	target)	EYEDIAP (screen target)
appearance variability. Furthermore, our results 	target)	EYEDIAP (screen target)
indicate that the increase of 	target)	EYEDIAP (screen target)
fusion units is not correlated 	target)	EYEDIAP (screen target)
with a better performance.  4.3 Static gaze regression: comparison	target)	EYEDIAP (screen target)
 with existing methods	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
We compare our best-performing static 	target)	EYEDIAP (screen target)
model with three baselines. Head: 	target)	EYEDIAP (screen target)
Treating the head pose directly 	target)	EYEDIAP (screen target)
as gaze direction. PR-ALR: Method 	target)	EYEDIAP (screen target)
that relies on RGB-D data 	target)	EYEDIAP (screen target)
to rectify the eye images 	target)	EYEDIAP (screen target)
viewpoint into a canonical head 	target)	EYEDIAP (screen target)
pose using a 3DMM. It 	target)	EYEDIAP (screen target)
then learns an RGB gaze 	target)	EYEDIAP (screen target)
appearance model using ALR [21]. 	target)	EYEDIAP (screen target)
Predicted 3D vectors for FT-S 	target)	EYEDIAP (screen target)
scenario are provided by EYEDIAP 	target)	EYEDIAP (screen target)
dataset. MPIIGaze:. State-of-the-art full-face 3D 	target)	EYEDIAP (screen target)
gaze estimation method [42]. They 	target)	EYEDIAP (screen target)
use an Alexnet-based CNN model 	target)	EYEDIAP (screen target)
with spatial weights to enhance 	target)	EYEDIAP (screen target)
information in different facial regions. 	target)	EYEDIAP (screen target)
We fine-tuned it with the 	target)	EYEDIAP (screen target)
filtered EYEDIAP subsets using our 	target)	EYEDIAP (screen target)
training parameters and normalization procedure.  In addition to the aforementioned	target)	EYEDIAP (screen target)
 FT-based evaluation setup, we also	target)	EYEDIAP (screen target)
 evaluate our method on the	target)	EYEDIAP (screen target)
 CS scenario. In this case	target)	EYEDIAP (screen target)
 there are only 14 participants	target)	EYEDIAP (screen target)
 available, so we divided them	target)	EYEDIAP (screen target)
 in 5 groups and performed	target)	EYEDIAP (screen target)
 5-fold cross-validation. In Figure 4	target)	EYEDIAP (screen target)
 we compare our method to	target)	EYEDIAP (screen target)
 MPIIGaze, achieving a statistically significant	target)	EYEDIAP (screen target)
 improvement of 14.6% and 19.5	target)	EYEDIAP (screen target)
% on FT and CS 	target)	EYEDIAP (screen target)
scenarios, respectively (paired Wilcoxon test, 	target)	EYEDIAP (screen target)
p < 0.0001). We can 	target)	EYEDIAP (screen target)
observe that a re- stricted 	target)	EYEDIAP (screen target)
gaze target benefits the performance 	target)	EYEDIAP (screen target)
of all methods, compared to 	target)	EYEDIAP (screen target)
a more challenging unrestricted setting 	target)	EYEDIAP (screen target)
with a wider range of 	target)	EYEDIAP (screen target)
head poses and gaze directions.  Table 2 provides a detailed	target)	EYEDIAP (screen target)
 comparison on every participant, performing	target)	EYEDIAP (screen target)
 leave-one-out cross-validation on the FT	target)	EYEDIAP (screen target)
 scenario for static and moving	target)	EYEDIAP (screen target)
 head separately. Results show that	target)	EYEDIAP (screen target)
, as expected, facial appearance 	target)	EYEDIAP (screen target)
and head pose have a 	target)	EYEDIAP (screen target)
noticeable impact on gaze accuracy, 	target)	EYEDIAP (screen target)
with average error differences of 	target)	EYEDIAP (screen target)
up to 7.7◦ among participants.  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (screen target)
, and Bulling} 2015  Citation Citation {Mora and Odobez	target)	EYEDIAP (screen target)
} 2012  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (screen target)
, and Bulling} 2015	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 9	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Method 1 2 3 4 5 6 7 8 9 10	target)	EYEDIAP (screen target)
 11 12 13 14 15	target)	EYEDIAP (screen target)
 16 Avg. Head 23.5 22.1	target)	EYEDIAP (screen target)
 20.3 23.6 23.2 23.2 23.6	target)	EYEDIAP (screen target)
 21.2 26.7 23.6 23.1 24.4	target)	EYEDIAP (screen target)
 23.3 24.0 24.5 22.8 23.3	target)	EYEDIAP (screen target)
 PR-ALR 12.3 12.0 12.4 11.3	target)	EYEDIAP (screen target)
 15.5 12.9 17.9 11.8 17.3	target)	EYEDIAP (screen target)
 13.4 13.4 14.3 15.2 13.6	target)	EYEDIAP (screen target)
 14.4 14.6 13.9 MPIIGaze 5.3	target)	EYEDIAP (screen target)
 5.1 5.7 4.7 7.3 15.1	target)	EYEDIAP (screen target)
 10.8 5.7 9.9 7.1 5.0	target)	EYEDIAP (screen target)
 5.7 7.4 3.8 4.8 5.5	target)	EYEDIAP (screen target)
 6.8 Static 3.9 4.1 4.2	target)	EYEDIAP (screen target)
 3.9 6.0 6.4 7.2 3.6	target)	EYEDIAP (screen target)
 7.1 5.0 5.7 6.7 3.9	target)	EYEDIAP (screen target)
 4.7 5.1 4.2 5.1 Temporal	target)	EYEDIAP (screen target)
 4.0 4.9 4.3 4.1 6.1	target)	EYEDIAP (screen target)
 6.5 6.6 3.9 7.8 6.1	target)	EYEDIAP (screen target)
 4.7 5.6 4.7 3.5 5.9	target)	EYEDIAP (screen target)
 4.6 5.2 Head 19.3 14.2	target)	EYEDIAP (screen target)
 16.4 19.9 16.8 21.9 16.1	target)	EYEDIAP (screen target)
 24.2 20.3 19.9 18.8 22.3	target)	EYEDIAP (screen target)
 18.1 14.9 16.2 19.3 18.7	target)	EYEDIAP (screen target)
 MPIIGaze 7.6 6.2 5.7 8.7	target)	EYEDIAP (screen target)
 10.1 12.0 12.2 6.1 8.3	target)	EYEDIAP (screen target)
 5.9 6.1 6.2 7.4 4.7	target)	EYEDIAP (screen target)
 4.4 6.0 7.3 Static 5.8	target)	EYEDIAP (screen target)
 5.7 4.4 7.5 6.7 8.8	target)	EYEDIAP (screen target)
 11.6 5.5 8.3 5.5 5.2	target)	EYEDIAP (screen target)
 6.3 5.3 3.9 4.3 5.6	target)	EYEDIAP (screen target)
 6.3 Temporal 6.1 5.6 4.5	target)	EYEDIAP (screen target)
 7.5 6.4 8.2 12.0 5.0	target)	EYEDIAP (screen target)
 7.5 5.4 5.0 5.8 6.6	target)	EYEDIAP (screen target)
 4.0 4.5 5.8 6.2	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Table 2: Gaze angular error 	target)	EYEDIAP (screen target)
comparison for static (top half) 	target)	EYEDIAP (screen target)
and moving (bottom half) head 	target)	EYEDIAP (screen target)
pose for each subject in 	target)	EYEDIAP (screen target)
the FT scenario. Best results 	target)	EYEDIAP (screen target)
in bold.  −80 −40 0 40 80−80	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
40  0	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
40  80	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
0  5	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
10  15	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
20  25	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
30  35	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
80 −40 0 40 80−80  −40	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
0  40	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
80  −10	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
8  −6	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
4  −2	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
0  2	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
4  6	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
8  10	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
80 −40 0 40 80−80  −40	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
0  40	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
80  0	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
5  10	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
15  20	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
25  30	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
35  −80 −40 0 40 80−80	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
40  0	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
40  80	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
10  −8	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
6  −4	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
2  0	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
2  4	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
6  8	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
10  (a) Gaze space (b) Head	target)	EYEDIAP (screen target)
 orientation space	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Figure 5: Angular error distribution 	target)	EYEDIAP (screen target)
across gaze (a) and head 	target)	EYEDIAP (screen target)
orientation (b) spaces in the 	target)	EYEDIAP (screen target)
FT setting, in terms of 	target)	EYEDIAP (screen target)
x- and y- angles. For 	target)	EYEDIAP (screen target)
each space, we depict the 	target)	EYEDIAP (screen target)
Static model performance (left) and 	target)	EYEDIAP (screen target)
the contribution of the Temporal 	target)	EYEDIAP (screen target)
model versus Static (right). In 	target)	EYEDIAP (screen target)
the latter, positive difference means 	target)	EYEDIAP (screen target)
higher improvement of the Temporal 	target)	EYEDIAP (screen target)
model.  4.4 Evaluation of the temporal	target)	EYEDIAP (screen target)
 network	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
In this section, we evaluate 	target)	EYEDIAP (screen target)
the contribution of adding the 	target)	EYEDIAP (screen target)
temporal module to the static 	target)	EYEDIAP (screen target)
model. To do so, we 	target)	EYEDIAP (screen target)
trained a lower-dimensional version of 	target)	EYEDIAP (screen target)
the static network with compa- 	target)	EYEDIAP (screen target)
rable performance to the original, 	target)	EYEDIAP (screen target)
reducing the number of units 	target)	EYEDIAP (screen target)
of the second fusion layer 	target)	EYEDIAP (screen target)
to 2918. Results are reported 	target)	EYEDIAP (screen target)
in Figure 4 and Table 2	target)	EYEDIAP (screen target)
. One can observe that 	target)	EYEDIAP (screen target)
using sequential information is helpful 	target)	EYEDIAP (screen target)
on the FT scenario, outperforming 	target)	EYEDIAP (screen target)
the static model by a 	target)	EYEDIAP (screen target)
statistically significant 4.4% (paired Wilcoxon 	target)	EYEDIAP (screen target)
test, p < 0.0001). This 	target)	EYEDIAP (screen target)
contribution is more noticeable in 	target)	EYEDIAP (screen target)
the moving head setting, proving 	target)	EYEDIAP (screen target)
that the temporal model can 	target)	EYEDIAP (screen target)
benefit from head motion information. 	target)	EYEDIAP (screen target)
In contrast, such information seems 	target)	EYEDIAP (screen target)
to be less meaningful in 	target)	EYEDIAP (screen target)
the CS scenario, where the 	target)	EYEDIAP (screen target)
obtained error is already very 	target)	EYEDIAP (screen target)
low for a cross-subject setting 	target)	EYEDIAP (screen target)
and the amount of head 	target)	EYEDIAP (screen target)
movement declines.  Figure 5 further explores the	target)	EYEDIAP (screen target)
 error distribution of the static	target)	EYEDIAP (screen target)
 network and the impact of	target)	EYEDIAP (screen target)
 sequential information. We can observe	target)	EYEDIAP (screen target)
 that the accuracy of the	target)	EYEDIAP (screen target)
 static model drops with extreme	target)	EYEDIAP (screen target)
 head poses and gaze directions	target)	EYEDIAP (screen target)
, which can also be 	target)	EYEDIAP (screen target)
correlated to having less data 	target)	EYEDIAP (screen target)
in those areas. Compared to 	target)	EYEDIAP (screen target)
the static model, the temporal 	target)	EYEDIAP (screen target)
model particularly benefits gaze targets 	target)	EYEDIAP (screen target)
from mid-range upwards. Its contribution 	target)	EYEDIAP (screen target)
is less clear for extreme 	target)	EYEDIAP (screen target)
targets, probably again due to 	target)	EYEDIAP (screen target)
data imbalance.  Finally, we evaluated the effect	target)	EYEDIAP (screen target)
 of different recurrent architectures for	target)	EYEDIAP (screen target)
 the temporal model. In particular	target)	EYEDIAP (screen target)
, we tested 1 (128 	target)	EYEDIAP (screen target)
units) and 2 (256-128 units) 	target)	EYEDIAP (screen target)
LSTM and GRU lay- ers, 	target)	EYEDIAP (screen target)
with 1 GRU layer obtaining 	target)	EYEDIAP (screen target)
slightly superior results (up to 0	target)	EYEDIAP (screen target)
.12◦). We also assessed the 	target)	EYEDIAP (screen target)
effect of sequence length fixing 	target)	EYEDIAP (screen target)
s in the range {4,7,10}, 	target)	EYEDIAP (screen target)
with s = 7 performing 	target)	EYEDIAP (screen target)
worse than the other two (	target)	EYEDIAP (screen target)
up to 0	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
14	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
10 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (screen target)
ESTIMATION  5 Conclusions In this work	target)	EYEDIAP (screen target)
, we studied the combination 	target)	EYEDIAP (screen target)
of full-face and eye images 	target)	EYEDIAP (screen target)
along with facial land- marks 	target)	EYEDIAP (screen target)
for person- and head pose-independent 	target)	EYEDIAP (screen target)
3D gaze estimation. Consequently, we 	target)	EYEDIAP (screen target)
pro- posed a multi-stream recurrent 	target)	EYEDIAP (screen target)
CNN network that leverages the 	target)	EYEDIAP (screen target)
sequential information of eye and 	target)	EYEDIAP (screen target)
head movements. Both static and 	target)	EYEDIAP (screen target)
temporal versions of our approach 	target)	EYEDIAP (screen target)
significantly outperform current state-of-the-art 3D 	target)	EYEDIAP (screen target)
gaze estimation methods on a 	target)	EYEDIAP (screen target)
wide range of head poses 	target)	EYEDIAP (screen target)
and gaze directions. We showed 	target)	EYEDIAP (screen target)
that adding geometry features to 	target)	EYEDIAP (screen target)
appearance-based methods has a regularizing 	target)	EYEDIAP (screen target)
effect on the accuracy. Adding 	target)	EYEDIAP (screen target)
sequential information further benefits the 	target)	EYEDIAP (screen target)
final performance compared to static-only 	target)	EYEDIAP (screen target)
input, especially from mid-range up- 	target)	EYEDIAP (screen target)
wards and in those cases 	target)	EYEDIAP (screen target)
where head motion is present. 	target)	EYEDIAP (screen target)
The effect in very extreme 	target)	EYEDIAP (screen target)
head poses is not clear 	target)	EYEDIAP (screen target)
due to data imbalance, suggesting 	target)	EYEDIAP (screen target)
the importance of learning from 	target)	EYEDIAP (screen target)
a con- tinuous, balanced dataset 	target)	EYEDIAP (screen target)
including all head poses and 	target)	EYEDIAP (screen target)
gaze directions of interest. To 	target)	EYEDIAP (screen target)
the best of our knowledge, 	target)	EYEDIAP (screen target)
this is the first attempt 	target)	EYEDIAP (screen target)
to exploit the temporal modality 	target)	EYEDIAP (screen target)
in the context of gaze 	target)	EYEDIAP (screen target)
estimation from remote cameras. As 	target)	EYEDIAP (screen target)
future work, we will further 	target)	EYEDIAP (screen target)
explore extracting meaningful temporal representations 	target)	EYEDIAP (screen target)
of gaze dynamics, considering 3DCNNs 	target)	EYEDIAP (screen target)
as well as the encoding 	target)	EYEDIAP (screen target)
of deep features around particular 	target)	EYEDIAP (screen target)
tracked face landmarks [14].  Acknowledgements This work has been	target)	EYEDIAP (screen target)
 partially supported by the Spanish	target)	EYEDIAP (screen target)
 project TIN2016-74946-P (MINECO/ FEDER, UE	target)	EYEDIAP (screen target)
), CERCA Programme / Generalitat 	target)	EYEDIAP (screen target)
de Catalunya, and the FP7 	target)	EYEDIAP (screen target)
people program (Marie Curie Actions), 	target)	EYEDIAP (screen target)
REA grant agreement no FP7-607139 (	target)	EYEDIAP (screen target)
iCARE - Improving Children Auditory 	target)	EYEDIAP (screen target)
REhabilitation). We gratefully acknowledge the 	target)	EYEDIAP (screen target)
support of NVIDIA Corporation with 	target)	EYEDIAP (screen target)
the donation of the GPU 	target)	EYEDIAP (screen target)
used for this research. Portions 	target)	EYEDIAP (screen target)
of the research in this 	target)	EYEDIAP (screen target)
pa- per used the EYEDIAP 	target)	EYEDIAP (screen target)
dataset made available by the 	target)	EYEDIAP (screen target)
Idiap Research Institute, Martigny, Switzerland.  References [1] Nicola C Anderson	target)	EYEDIAP (screen target)
, Evan F Risko, and 	target)	EYEDIAP (screen target)
Alan Kingstone. Motion influences gaze 	target)	EYEDIAP (screen target)
di-  rection discrimination and disambiguates contradictory	target)	EYEDIAP (screen target)
 luminance cues. Psychonomic bulletin	target)	EYEDIAP (screen target)
 & review, 23(3):817–823, 2016	target)	EYEDIAP (screen target)
.  [2] Shumeet Baluja and Dean	target)	EYEDIAP (screen target)
 Pomerleau. Non-intrusive gaze tracking using	target)	EYEDIAP (screen target)
 artificial neu- ral networks. In	target)	EYEDIAP (screen target)
 Advances in Neural Information Processing	target)	EYEDIAP (screen target)
 Systems, pages 753–760, 1994	target)	EYEDIAP (screen target)
.  [3] Adrian Bulat and Georgios	target)	EYEDIAP (screen target)
 Tzimiropoulos. How far are we	target)	EYEDIAP (screen target)
 from solving the 2d	target)	EYEDIAP (screen target)
 & 3d face alignment problem	target)	EYEDIAP (screen target)
? (and a dataset of 230,	target)	EYEDIAP (screen target)
000 3d facial landmarks). In 	target)	EYEDIAP (screen target)
Interna- tional Conference on Computer 	target)	EYEDIAP (screen target)
Vision, 2017.  [4] Haoping Deng and Wangjiang	target)	EYEDIAP (screen target)
 Zhu. Monocular free-head 3d gaze	target)	EYEDIAP (screen target)
 tracking with deep learning and	target)	EYEDIAP (screen target)
 geometry constraints. In Computer Vision	target)	EYEDIAP (screen target)
 (ICCV), 2017 IEEE Interna- tional	target)	EYEDIAP (screen target)
 Conference on, pages 3162–3171. IEEE	target)	EYEDIAP (screen target)
, 2017.  [5] Onur Ferhat and Fernando	target)	EYEDIAP (screen target)
 Vilariño. Low cost eye tracking	target)	EYEDIAP (screen target)
. Computational intelligence and neuroscience, 2016	target)	EYEDIAP (screen target)
:17, 2016.  Citation Citation {Jung, Lee, Yim	target)	EYEDIAP (screen target)
, Park, and Kim} 2015	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 11	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
6] Kenneth A Funes-Mora and 	target)	EYEDIAP (screen target)
Jean-Marc Odobez. Gaze estimation in 	target)	EYEDIAP (screen target)
the 3D space using RGB-D 	target)	EYEDIAP (screen target)
sensors. International Journal of Computer 	target)	EYEDIAP (screen target)
Vision, 118(2):194–216, 2016.  [7] Kenneth Alberto Funes Mora	target)	EYEDIAP (screen target)
, Florent Monay, and Jean-Marc 	target)	EYEDIAP (screen target)
Odobez. Eyediap: A database for 	target)	EYEDIAP (screen target)
the development and evaluation of 	target)	EYEDIAP (screen target)
gaze estimation algorithms from rgb 	target)	EYEDIAP (screen target)
and rgb-d cameras. In Proceedings 	target)	EYEDIAP (screen target)
of the ACM Symposium on 	target)	EYEDIAP (screen target)
Eye Tracking Research and Applications. 	target)	EYEDIAP (screen target)
ACM, March 2014. doi: 10.1145/2578153.2578190.  [8] Kenneth Alberto Funes Mora	target)	EYEDIAP (screen target)
, Florent Monay, and Jean-Marc 	target)	EYEDIAP (screen target)
Odobez. Eyediap: A database for 	target)	EYEDIAP (screen target)
the development and evaluation of 	target)	EYEDIAP (screen target)
gaze estimation algorithms from rgb 	target)	EYEDIAP (screen target)
and rgb-d cameras. In Proceedings 	target)	EYEDIAP (screen target)
of the Symposium on Eye 	target)	EYEDIAP (screen target)
Tracking Research and Applications, pages 255	target)	EYEDIAP (screen target)
–258. ACM, 2014.  [9] Quentin Guillon, Nouchine Hadjikhani	target)	EYEDIAP (screen target)
, Sophie Baduel, and Bernadette 	target)	EYEDIAP (screen target)
Rogé. Visual social attention in 	target)	EYEDIAP (screen target)
autism spectrum disorder: Insights from 	target)	EYEDIAP (screen target)
eye tracking studies. Neu- 	target)	EYEDIAP (screen target)
roscience & Biobehavioral Reviews, 42:279–297, 2014	target)	EYEDIAP (screen target)
.  [10] Dan Witzner Hansen and	target)	EYEDIAP (screen target)
 Qiang Ji. In the eye	target)	EYEDIAP (screen target)
 of the beholder: A survey	target)	EYEDIAP (screen target)
 of models for eyes and	target)	EYEDIAP (screen target)
 gaze. IEEE transactions on pattern	target)	EYEDIAP (screen target)
 analysis and machine intelligence, 32(3	target)	EYEDIAP (screen target)
): 478–500, 2010.  [11] Qiong Huang, Ashok Veeraraghavan	target)	EYEDIAP (screen target)
, and Ashutosh Sabharwal. Tabletgaze: 	target)	EYEDIAP (screen target)
dataset and analysis for unconstrained 	target)	EYEDIAP (screen target)
appearance-based gaze estimation in mobile 	target)	EYEDIAP (screen target)
tablets. Machine Vision and Applications, 28	target)	EYEDIAP (screen target)
(5-6):445–461, 2017.  [12] Robert JK Jacob and	target)	EYEDIAP (screen target)
 Keith S Karn. Eye tracking	target)	EYEDIAP (screen target)
 in human-computer interaction and usability	target)	EYEDIAP (screen target)
 research: Ready to deliver the	target)	EYEDIAP (screen target)
 promises. In The mind’s eye	target)	EYEDIAP (screen target)
, pages 573–605. Elsevier, 2003.  [13] László A Jeni and	target)	EYEDIAP (screen target)
 Jeffrey F Cohn. Person-independent 3d	target)	EYEDIAP (screen target)
 gaze estimation using face frontalization	target)	EYEDIAP (screen target)
. In Proceedings of the 	target)	EYEDIAP (screen target)
IEEE Conference on Computer Vision 	target)	EYEDIAP (screen target)
and Pattern Recognition Workshops, pages 87	target)	EYEDIAP (screen target)
–95, 2016.  [14] Heechul Jung, Sihaeng Lee	target)	EYEDIAP (screen target)
, Junho Yim, Sunjeong Park, 	target)	EYEDIAP (screen target)
and Junmo Kim. Joint fine- 	target)	EYEDIAP (screen target)
tuning in deep neural networks 	target)	EYEDIAP (screen target)
for facial expression recognition. In 	target)	EYEDIAP (screen target)
Computer Vision (ICCV), 2015 IEEE 	target)	EYEDIAP (screen target)
International Conference on, pages 2983–2991. 	target)	EYEDIAP (screen target)
IEEE, 2015.  [15] Anuradha Kar and Peter	target)	EYEDIAP (screen target)
 Corcoran. A review and analysis	target)	EYEDIAP (screen target)
 of eye-gaze estimation sys- tems	target)	EYEDIAP (screen target)
, algorithms and performance evaluation 	target)	EYEDIAP (screen target)
methods in consumer platforms. IEEE 	target)	EYEDIAP (screen target)
Access, 5:16495–16519, 2017.  [16] Kyle Krafka, Aditya Khosla	target)	EYEDIAP (screen target)
, Petr Kellnhofer, Harini Kannan, 	target)	EYEDIAP (screen target)
Suchendra Bhandarkar, Wojciech Matusik, and 	target)	EYEDIAP (screen target)
Antonio Torralba. Eye tracking for 	target)	EYEDIAP (screen target)
everyone. In Computer Vision and 	target)	EYEDIAP (screen target)
Pattern Recognition (CVPR), 2016 IEEE 	target)	EYEDIAP (screen target)
Conference on, pages 2176–2184. IEEE, 2016	target)	EYEDIAP (screen target)
.  [17] Simon P Liversedge and	target)	EYEDIAP (screen target)
 John M Findlay. Saccadic eye	target)	EYEDIAP (screen target)
 movements and cognition. Trends in	target)	EYEDIAP (screen target)
 cognitive sciences, 4(1):6–14, 2000	target)	EYEDIAP (screen target)
.  [18] Feng Lu, Takahiro Okabe	target)	EYEDIAP (screen target)
, Yusuke Sugano, and Yoichi 	target)	EYEDIAP (screen target)
Sato. A head pose-free approach 	target)	EYEDIAP (screen target)
for appearance-based gaze estimation. In 	target)	EYEDIAP (screen target)
BMVC, pages 1–11, 2011	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
12 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (screen target)
ESTIMATION  [19] Feng Lu, Yusuke Sugano	target)	EYEDIAP (screen target)
, Takahiro Okabe, and Yoichi 	target)	EYEDIAP (screen target)
Sato. Inferring human gaze from 	target)	EYEDIAP (screen target)
appearance via adaptive linear regression. 	target)	EYEDIAP (screen target)
In Computer Vision (ICCV), 2011 	target)	EYEDIAP (screen target)
IEEE International Conference on, pages 153	target)	EYEDIAP (screen target)
–160. IEEE, 2011.  [20] Päivi Majaranta and Andreas	target)	EYEDIAP (screen target)
 Bulling. Eye tracking and eye-based	target)	EYEDIAP (screen target)
 human–computer interaction. In Advances in	target)	EYEDIAP (screen target)
 physiological computing, pages 39–65. Springer	target)	EYEDIAP (screen target)
, 2014.  [21] Kenneth Alberto Funes Mora	target)	EYEDIAP (screen target)
 and Jean-Marc Odobez. Gaze estimation	target)	EYEDIAP (screen target)
 from multi- modal kinect data	target)	EYEDIAP (screen target)
. In Computer Vision and 	target)	EYEDIAP (screen target)
Pattern Recognition Workshops (CVPRW), 2012 	target)	EYEDIAP (screen target)
IEEE Computer Society Conference on, 	target)	EYEDIAP (screen target)
pages 25–30. IEEE, 2012.  [22] Carlos Hitoshi Morimoto, Arnon	target)	EYEDIAP (screen target)
 Amir, and Myron Flickner. Detecting	target)	EYEDIAP (screen target)
 eye position and gaze from	target)	EYEDIAP (screen target)
 a single camera and 2	target)	EYEDIAP (screen target)
 light sources. In Pattern Recognition	target)	EYEDIAP (screen target)
, 2002. Proceedings. 16th International 	target)	EYEDIAP (screen target)
Conference on, volume 4, pages 314	target)	EYEDIAP (screen target)
–317. IEEE, 2002.  [23] IMO MSC. Circ. 982	target)	EYEDIAP (screen target)
 (2000) guidelines on ergonomic criteria	target)	EYEDIAP (screen target)
 for bridge equipment and layout	target)	EYEDIAP (screen target)
.  [24] Alejandro Newell, Kaiyu Yang	target)	EYEDIAP (screen target)
, and Jia Deng. Stacked 	target)	EYEDIAP (screen target)
hourglass networks for hu- man 	target)	EYEDIAP (screen target)
pose estimation. In European Conference 	target)	EYEDIAP (screen target)
on Computer Vision, pages 483–499. 	target)	EYEDIAP (screen target)
Springer, 2016.  [25] Yasuhiro Ono, Takahiro Okabe	target)	EYEDIAP (screen target)
, and Yoichi Sato. Gaze 	target)	EYEDIAP (screen target)
estimation from low resolution images. 	target)	EYEDIAP (screen target)
In Pacific-Rim Symposium on Image 	target)	EYEDIAP (screen target)
and Video Technology, pages 178–188. 	target)	EYEDIAP (screen target)
Springer, 2006.  [26] Cristina Palmero, Elisabeth A	target)	EYEDIAP (screen target)
. van Dam, Sergio Escalera, 	target)	EYEDIAP (screen target)
Mike Kelia, Guido F. Lichtert, 	target)	EYEDIAP (screen target)
Lucas P.J.J Noldus, Andrew J. 	target)	EYEDIAP (screen target)
Spink, and Astrid van Wieringen. 	target)	EYEDIAP (screen target)
Automatic mutual gaze detection in 	target)	EYEDIAP (screen target)
face-to-face dyadic interaction videos. In 	target)	EYEDIAP (screen target)
Proceedings of Measuring Behavior, pages 158	target)	EYEDIAP (screen target)
–163, 2018.  [27] Omkar M. Parkhi, Andrea	target)	EYEDIAP (screen target)
 Vedaldi, and Andrew Zisserman. Deep	target)	EYEDIAP (screen target)
 face recognition. In British Machine	target)	EYEDIAP (screen target)
 Vision Conference, 2015	target)	EYEDIAP (screen target)
.  [28] Derek R Rutter and	target)	EYEDIAP (screen target)
 Kevin Durkin. Turn-taking in mother–infant	target)	EYEDIAP (screen target)
 interaction: An exam- ination of	target)	EYEDIAP (screen target)
 vocalizations and gaze. Developmental psychology	target)	EYEDIAP (screen target)
, 23(1):54, 1987.  [29] Brian A Smith, Qi	target)	EYEDIAP (screen target)
 Yin, Steven K Feiner, and	target)	EYEDIAP (screen target)
 Shree K Nayar. Gaze locking	target)	EYEDIAP (screen target)
: passive eye contact detection 	target)	EYEDIAP (screen target)
for human-object interaction. In Proceedings 	target)	EYEDIAP (screen target)
of the 26th annual ACM 	target)	EYEDIAP (screen target)
symposium on User interface software 	target)	EYEDIAP (screen target)
and technology, pages 271–280. ACM, 2013	target)	EYEDIAP (screen target)
.  [30] Yusuke Sugano, Yasuyuki Matsushita	target)	EYEDIAP (screen target)
, and Yoichi Sato. Appearance-based 	target)	EYEDIAP (screen target)
gaze es- timation using visual 	target)	EYEDIAP (screen target)
saliency. IEEE transactions on pattern 	target)	EYEDIAP (screen target)
analysis and machine intelligence, 35(2):329–341, 2013	target)	EYEDIAP (screen target)
.  [31] Yusuke Sugano, Yasuyuki Matsushita	target)	EYEDIAP (screen target)
, and Yoichi Sato. Learning-by-synthesis 	target)	EYEDIAP (screen target)
for appearance-based 3d gaze estimation. 	target)	EYEDIAP (screen target)
In Computer Vision and Pattern 	target)	EYEDIAP (screen target)
Recognition (CVPR), 2014 IEEE Conference 	target)	EYEDIAP (screen target)
on, pages 1821–1828. IEEE, 2014.  [32] Kar-Han Tan, David J	target)	EYEDIAP (screen target)
 Kriegman, and Narendra Ahuja. Appearance-based	target)	EYEDIAP (screen target)
 eye gaze es- timation. In	target)	EYEDIAP (screen target)
 Applications of Computer Vision, 2002.(WACV	target)	EYEDIAP (screen target)
 2002). Proceedings. Sixth IEEE Workshop	target)	EYEDIAP (screen target)
 on, pages 191–195. IEEE, 2002	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 13	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
33] Ronda Venkateswarlu et al. 	target)	EYEDIAP (screen target)
Eye gaze estimation from a 	target)	EYEDIAP (screen target)
single image of one eye. 	target)	EYEDIAP (screen target)
In Computer Vision, 2003. Proceedings. 	target)	EYEDIAP (screen target)
Ninth IEEE International Conference on, 	target)	EYEDIAP (screen target)
pages 136–143. IEEE, 2003.  [34] Kang Wang and Qiang	target)	EYEDIAP (screen target)
 Ji. Real time eye gaze	target)	EYEDIAP (screen target)
 tracking with 3d deformable eye-face	target)	EYEDIAP (screen target)
 model. In Proceedings of the	target)	EYEDIAP (screen target)
 IEEE Conference on Computer Vision	target)	EYEDIAP (screen target)
 and Pattern Recog- nition, pages	target)	EYEDIAP (screen target)
 1003–1011, 2017	target)	EYEDIAP (screen target)
.  [35] Oliver Williams, Andrew Blake	target)	EYEDIAP (screen target)
, and Roberto Cipolla. Sparse 	target)	EYEDIAP (screen target)
and semi-supervised visual mapping with 	target)	EYEDIAP (screen target)
the sˆ 3gp. In Computer 	target)	EYEDIAP (screen target)
Vision and Pattern Recognition, 2006 	target)	EYEDIAP (screen target)
IEEE Computer Society Conference on, 	target)	EYEDIAP (screen target)
volume 1, pages 230–237. IEEE, 2006	target)	EYEDIAP (screen target)
.  [36] William Hyde Wollaston et	target)	EYEDIAP (screen target)
 al. Xiii. on the apparent	target)	EYEDIAP (screen target)
 direction of eyes in a	target)	EYEDIAP (screen target)
 portrait. Philosophical Transactions of the	target)	EYEDIAP (screen target)
 Royal Society of London, 114:247–256	target)	EYEDIAP (screen target)
, 1824.  [37] Erroll Wood and Andreas	target)	EYEDIAP (screen target)
 Bulling. Eyetab: Model-based gaze estimation	target)	EYEDIAP (screen target)
 on unmodi- fied tablet computers	target)	EYEDIAP (screen target)
. In Proceedings of the 	target)	EYEDIAP (screen target)
Symposium on Eye Tracking Research 	target)	EYEDIAP (screen target)
and Applications, pages 207–210. ACM, 2014	target)	EYEDIAP (screen target)
.  [38] Erroll Wood, Tadas Baltrusaitis	target)	EYEDIAP (screen target)
, Xucong Zhang, Yusuke Sugano, 	target)	EYEDIAP (screen target)
Peter Robinson, and Andreas Bulling. 	target)	EYEDIAP (screen target)
Rendering of eyes for eye-shape 	target)	EYEDIAP (screen target)
registration and gaze estimation. In 	target)	EYEDIAP (screen target)
Proceedings of the IEEE International 	target)	EYEDIAP (screen target)
Conference on Computer Vision, pages 3756	target)	EYEDIAP (screen target)
– 3764, 2015.  [39] Erroll Wood, Tadas Baltrušaitis	target)	EYEDIAP (screen target)
, Louis-Philippe Morency, Peter Robinson, 	target)	EYEDIAP (screen target)
and Andreas Bulling. A 3d 	target)	EYEDIAP (screen target)
morphable eye region model for 	target)	EYEDIAP (screen target)
gaze estimation. In European Confer- 	target)	EYEDIAP (screen target)
ence on Computer Vision, pages 297	target)	EYEDIAP (screen target)
–313. Springer, 2016.  [40] Erroll Wood, Tadas Baltrušaitis	target)	EYEDIAP (screen target)
, Louis-Philippe Morency, Peter Robinson, 	target)	EYEDIAP (screen target)
and Andreas Bulling. Learning an 	target)	EYEDIAP (screen target)
appearance-based gaze estimator from one 	target)	EYEDIAP (screen target)
million synthesised images. In Proceedings 	target)	EYEDIAP (screen target)
of the Ninth Biennial ACM 	target)	EYEDIAP (screen target)
Symposium on Eye Tracking Re- 	target)	EYEDIAP (screen target)
search & Applications, pages 131–138. 	target)	EYEDIAP (screen target)
ACM, 2016.  [41] Dong Hyun Yoo and	target)	EYEDIAP (screen target)
 Myung Jin Chung. A novel	target)	EYEDIAP (screen target)
 non-intrusive eye gaze estimation using	target)	EYEDIAP (screen target)
 cross-ratio under large head motion	target)	EYEDIAP (screen target)
. Computer Vision and Image 	target)	EYEDIAP (screen target)
Understanding, 98(1):25–51, 2005.  [42] Xucong Zhang, Yusuke Sugano	target)	EYEDIAP (screen target)
, Mario Fritz, and Andreas 	target)	EYEDIAP (screen target)
Bulling. Appearance-based gaze estimation in 	target)	EYEDIAP (screen target)
the wild. In Proceedings of 	target)	EYEDIAP (screen target)
the IEEE Conference on Computer 	target)	EYEDIAP (screen target)
Vision and Pattern Recognition, pages 4511	target)	EYEDIAP (screen target)
–4520, 2015.  [43] Xucong Zhang, Yusuke Sugano	target)	EYEDIAP (screen target)
, Mario Fritz, and Andreas 	target)	EYEDIAP (screen target)
Bulling. It’s written all over 	target)	EYEDIAP (screen target)
your face: Full-face appearance-based gaze 	target)	EYEDIAP (screen target)
estimation. In Proc. IEEE International 	target)	EYEDIAP (screen target)
Conference on Computer Vision and 	target)	EYEDIAP (screen target)
Pattern Recognition Workshops (CVPRW), 2017	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	(screen	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 1	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Recurrent CNN for 3D Gaze 	(screen	EYEDIAP (screen target)
Estimation using Appearance and Shape 	(screen	EYEDIAP (screen target)
Cues  Cristina Palmero1,2	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
crpalmec7@alumnes.ub.edu  1 Dept. Mathematics and Informatics	(screen	EYEDIAP (screen target)
 Universitat de Barcelona, Spain	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Javier Selva1  javier.selva.castello@est.fib.upc.edu	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
2 Computer Vision Center Campus 	(screen	EYEDIAP (screen target)
UAB, Bellaterra, Spain  Mohammad Ali Bagheri3,4	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
mohammadali.bagheri@ucalgary.ca  3 Dept. Electrical and Computer	(screen	EYEDIAP (screen target)
 Eng. University of Calgary, Canada	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Sergio Escalera1,2  sergio@maia.ub.es	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
4 Dept. Engineering University of 	(screen	EYEDIAP (screen target)
Larestan, Iran  Abstract	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Gaze behavior is an important 	(screen	EYEDIAP (screen target)
non-verbal cue in social signal 	(screen	EYEDIAP (screen target)
processing and human- computer interaction. 	(screen	EYEDIAP (screen target)
In this paper, we tackle 	(screen	EYEDIAP (screen target)
the problem of person- and 	(screen	EYEDIAP (screen target)
head pose- independent 3D gaze 	(screen	EYEDIAP (screen target)
estimation from remote cameras, using 	(screen	EYEDIAP (screen target)
a multi-modal recurrent convolutional neural 	(screen	EYEDIAP (screen target)
network (CNN). We propose to 	(screen	EYEDIAP (screen target)
combine face, eyes region, and 	(screen	EYEDIAP (screen target)
face landmarks as individual streams 	(screen	EYEDIAP (screen target)
in a CNN to estimate 	(screen	EYEDIAP (screen target)
gaze in still images. Then, 	(screen	EYEDIAP (screen target)
we exploit the dynamic nature 	(screen	EYEDIAP (screen target)
of gaze by feeding the 	(screen	EYEDIAP (screen target)
learned features of all the 	(screen	EYEDIAP (screen target)
frames in a sequence to 	(screen	EYEDIAP (screen target)
a many-to-one recurrent module that 	(screen	EYEDIAP (screen target)
predicts the 3D gaze vector 	(screen	EYEDIAP (screen target)
of the last frame. Our 	(screen	EYEDIAP (screen target)
multi-modal static solution is evaluated 	(screen	EYEDIAP (screen target)
on a wide range of 	(screen	EYEDIAP (screen target)
head poses and gaze directions, 	(screen	EYEDIAP (screen target)
achieving a significant improvement of 14	(screen	EYEDIAP (screen target)
.6% over the state of 	(screen	EYEDIAP (screen target)
the art on EYEDIAP dataset, 	(screen	EYEDIAP (screen target)
further improved by 4% when 	(screen	EYEDIAP (screen target)
the temporal modality is included.  1 Introduction Eyes and their	(screen	EYEDIAP (screen target)
 movements are considered an important	(screen	EYEDIAP (screen target)
 cue in non-verbal behavior analysis	(screen	EYEDIAP (screen target)
, being involved in many 	(screen	EYEDIAP (screen target)
cognitive processes and reflecting our 	(screen	EYEDIAP (screen target)
internal state [17]. More specifically, 	(screen	EYEDIAP (screen target)
eye gaze behavior, as an 	(screen	EYEDIAP (screen target)
indicator of human visual attention, 	(screen	EYEDIAP (screen target)
has been widely studied to 	(screen	EYEDIAP (screen target)
assess communication skills [28] and 	(screen	EYEDIAP (screen target)
to identify possible behavioral 	(screen	EYEDIAP (screen target)
disorders [9]. Therefore, gaze estimation 	(screen	EYEDIAP (screen target)
has become an established line 	(screen	EYEDIAP (screen target)
of research in computer vision, 	(screen	EYEDIAP (screen target)
being a key feature in 	(screen	EYEDIAP (screen target)
human-computer interaction (HCI) and usability 	(screen	EYEDIAP (screen target)
research [12, 20].  Recent gaze estimation research has	(screen	EYEDIAP (screen target)
 focused on facilitating its use	(screen	EYEDIAP (screen target)
 in general everyday applications under	(screen	EYEDIAP (screen target)
 real-world conditions, using off-the-shelf remote	(screen	EYEDIAP (screen target)
 RGB cameras and re- moving	(screen	EYEDIAP (screen target)
 the need of personal calibration	(screen	EYEDIAP (screen target)
 [26]. In this setting, appearance-based	(screen	EYEDIAP (screen target)
 methods, which learn a mapping	(screen	EYEDIAP (screen target)
 from images to gaze directions	(screen	EYEDIAP (screen target)
, are the preferred 	(screen	EYEDIAP (screen target)
choice [25]. How- ever, they 	(screen	EYEDIAP (screen target)
need large amounts of training 	(screen	EYEDIAP (screen target)
data to be able to 	(screen	EYEDIAP (screen target)
generalize well to in-the-wild situations, 	(screen	EYEDIAP (screen target)
which are characterized by significant 	(screen	EYEDIAP (screen target)
variability in head poses, face 	(screen	EYEDIAP (screen target)
appearances and lighting conditions. In 	(screen	EYEDIAP (screen target)
recent years, CNNs have been 	(screen	EYEDIAP (screen target)
reported to outperform classical methods. 	(screen	EYEDIAP (screen target)
However, most existing approaches have 	(screen	EYEDIAP (screen target)
only been tested in restricted 	(screen	EYEDIAP (screen target)
HCI tasks,  c© 2018. The copyright of	(screen	EYEDIAP (screen target)
 this document resides with its	(screen	EYEDIAP (screen target)
 authors. It may be distributed	(screen	EYEDIAP (screen target)
 unchanged freely in print or	(screen	EYEDIAP (screen target)
 electronic forms	(screen	EYEDIAP (screen target)
.  ar X	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
iv :1  80 5	(screen	EYEDIAP (screen target)
.  03 06	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
4v 3	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
cs  .C V	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
1  7	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Se  p	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
20  18	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Liversedge and Findlay} 2000	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Rutter and Durkin} 1987	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Guillon, Hadjikhani, Baduel, 	(screen	EYEDIAP (screen target)
and Rog{é}} 2014  Citation Citation {Jacob and Karn	(screen	EYEDIAP (screen target)
} 2003  Citation Citation {Majaranta and Bulling	(screen	EYEDIAP (screen target)
} 2014  Citation Citation {Palmero, van Dam	(screen	EYEDIAP (screen target)
, Escalera, Kelia, Lichtert, Noldus, 	(screen	EYEDIAP (screen target)
Spink, and van Wieringen} 2018  Citation Citation {Ono, Okabe, and	(screen	EYEDIAP (screen target)
 Sato} 2006	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
2 PALMERO ET AL.: MULTI-MODAL 	(screen	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	(screen	EYEDIAP (screen target)
ESTIMATION  Method 3D gaze direction	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Unrestricted gaze target  Full face	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Eye region  Facial landmarks	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Sequential information  Zhang et al. (1) [42	(screen	EYEDIAP (screen target)
] 3 7 7 3 7 7 Krafka et al. [16	(screen	EYEDIAP (screen target)
] 7 7 3 3 7 7 Zhang et al. (2	(screen	EYEDIAP (screen target)
) [43] 3 7 3 7 7 7 Deng and Zhu	(screen	EYEDIAP (screen target)
 [4] 3 3 3 3	(screen	EYEDIAP (screen target)
 7 7 Ours 3 3	(screen	EYEDIAP (screen target)
 3 3 3 3	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Table 1: Characteristics of recent 	(screen	EYEDIAP (screen target)
related work on person- and 	(screen	EYEDIAP (screen target)
head pose-independent appearance-based gaze estimation 	(screen	EYEDIAP (screen target)
methods using CNNs.  where users look at the	(screen	EYEDIAP (screen target)
 screen or mobile phone, showing	(screen	EYEDIAP (screen target)
 a low head pose variability	(screen	EYEDIAP (screen target)
. It is yet unclear 	(screen	EYEDIAP (screen target)
how these methods would perform 	(screen	EYEDIAP (screen target)
in a wider range of 	(screen	EYEDIAP (screen target)
head poses.  On a different note, until	(screen	EYEDIAP (screen target)
 very recently, the majority of	(screen	EYEDIAP (screen target)
 methods only used static eye	(screen	EYEDIAP (screen target)
 region appearance as input. State-of-the-art	(screen	EYEDIAP (screen target)
 approaches have demonstrated that using	(screen	EYEDIAP (screen target)
 the face along with a	(screen	EYEDIAP (screen target)
 higher resolution image of the	(screen	EYEDIAP (screen target)
 eyes [16], or even just	(screen	EYEDIAP (screen target)
 the face itself [43], increases	(screen	EYEDIAP (screen target)
 performance. Indeed, the whole-face image	(screen	EYEDIAP (screen target)
 encodes more information than eyes	(screen	EYEDIAP (screen target)
 alone, such as illumination and	(screen	EYEDIAP (screen target)
 head pose. Nevertheless, gaze behavior	(screen	EYEDIAP (screen target)
 is not static. Eye and	(screen	EYEDIAP (screen target)
 head movements allow us to	(screen	EYEDIAP (screen target)
 direct our gaze to target	(screen	EYEDIAP (screen target)
 locations of interest. It has	(screen	EYEDIAP (screen target)
 been demonstrated that humans can	(screen	EYEDIAP (screen target)
 better predict gaze when being	(screen	EYEDIAP (screen target)
 shown image sequences of other	(screen	EYEDIAP (screen target)
 people moving their eyes [1	(screen	EYEDIAP (screen target)
]. However, it is still 	(screen	EYEDIAP (screen target)
an open question whether this 	(screen	EYEDIAP (screen target)
se- quential information can increase 	(screen	EYEDIAP (screen target)
the performance of automatic methods.  In this work, we show	(screen	EYEDIAP (screen target)
 that the combination of multiple	(screen	EYEDIAP (screen target)
 cues benefits the gaze estimation	(screen	EYEDIAP (screen target)
 task. In particular, we use	(screen	EYEDIAP (screen target)
 face, eye region and facial	(screen	EYEDIAP (screen target)
 landmarks from still images. Facial	(screen	EYEDIAP (screen target)
 landmarks model the global shape	(screen	EYEDIAP (screen target)
 of the face and come	(screen	EYEDIAP (screen target)
 at no cost, since face	(screen	EYEDIAP (screen target)
 alignment is a common pre-processing	(screen	EYEDIAP (screen target)
 step in many facial image	(screen	EYEDIAP (screen target)
 analysis approaches. Furthermore, we present	(screen	EYEDIAP (screen target)
 a subject-independent, free-head recurrent 3D	(screen	EYEDIAP (screen target)
 gaze regression network to leverage	(screen	EYEDIAP (screen target)
 the temporal information of image	(screen	EYEDIAP (screen target)
 sequences. The static streams of	(screen	EYEDIAP (screen target)
 each frame are combined in	(screen	EYEDIAP (screen target)
 a late-fusion fashion using a	(screen	EYEDIAP (screen target)
 multi-stream CNN. Then, all feature	(screen	EYEDIAP (screen target)
 vectors are input to a	(screen	EYEDIAP (screen target)
 many-to-one recurrent module that predicts	(screen	EYEDIAP (screen target)
 the gaze vector of the	(screen	EYEDIAP (screen target)
 last sequence frame	(screen	EYEDIAP (screen target)
.  In summary, our contributions are	(screen	EYEDIAP (screen target)
 two-fold. First, we present a	(screen	EYEDIAP (screen target)
 Recurrent-CNN net- work architecture that	(screen	EYEDIAP (screen target)
 combines appearance, shape and temporal	(screen	EYEDIAP (screen target)
 information for 3D gaze estimation	(screen	EYEDIAP (screen target)
. Second, we test static 	(screen	EYEDIAP (screen target)
and temporal versions of our 	(screen	EYEDIAP (screen target)
solution on the EYEDIAP 	(screen	EYEDIAP (screen target)
dataset [7] in a wide 	(screen	EYEDIAP (screen target)
range of head poses and 	(screen	EYEDIAP (screen target)
gaze directions, showing consistent perfor- 	(screen	EYEDIAP (screen target)
mance improvements compared to related 	(screen	EYEDIAP (screen target)
appearance-based methods. To the best 	(screen	EYEDIAP (screen target)
of our knowledge, this is 	(screen	EYEDIAP (screen target)
the first third-person, remote camera-based 	(screen	EYEDIAP (screen target)
approach that uses tempo- ral 	(screen	EYEDIAP (screen target)
information for this task. Table 1 outlines our main method characteristics	(screen	EYEDIAP (screen target)
 compared to related work. Models	(screen	EYEDIAP (screen target)
 and code are publicly available	(screen	EYEDIAP (screen target)
 at https://github.com/ crisie/RecurrentGaze	(screen	EYEDIAP (screen target)
.  2 Related work Gaze estimation	(screen	EYEDIAP (screen target)
 methods are typically categorized as	(screen	EYEDIAP (screen target)
 model-based or appearance-based [5, 10	(screen	EYEDIAP (screen target)
, 15]. Model-based approaches use 	(screen	EYEDIAP (screen target)
a geometric model of the 	(screen	EYEDIAP (screen target)
eye, usually requir- ing either 	(screen	EYEDIAP (screen target)
high resolution images or a 	(screen	EYEDIAP (screen target)
person-specific calibration stage to estimate 	(screen	EYEDIAP (screen target)
personal eye parameters [22, 33, 34, 37, 41]. In contrast, appearance-based	(screen	EYEDIAP (screen target)
 methods learn a di- rect	(screen	EYEDIAP (screen target)
 mapping from intensity images or	(screen	EYEDIAP (screen target)
 extracted eye features to gaze	(screen	EYEDIAP (screen target)
 directions, thus being	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Zhang, Sugano, Fritz, 	(screen	EYEDIAP (screen target)
and Bulling} 2015  Citation Citation {Krafka, Khosla, Kellnhofer	(screen	EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 	(screen	EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	(screen	EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Deng and Zhu	(screen	EYEDIAP (screen target)
} 2017  Citation Citation {Krafka, Khosla, Kellnhofer	(screen	EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 	(screen	EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	(screen	EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Anderson, Risko, and	(screen	EYEDIAP (screen target)
 Kingstone} 2016	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Funesprotect unhbox voidb@x 	(screen	EYEDIAP (screen target)
penalty @M  {}Mora, Monay, and Odobez} 2014	(screen	EYEDIAP (screen target)
{}  Citation Citation {Ferhat and Vilari{ñ}o	(screen	EYEDIAP (screen target)
} 2016  Citation Citation {Hansen and Ji	(screen	EYEDIAP (screen target)
} 2010  Citation Citation {Kar and Corcoran	(screen	EYEDIAP (screen target)
} 2017  Citation Citation {Morimoto, Amir, and	(screen	EYEDIAP (screen target)
 Flickner} 2002	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Venkateswarlu etprotect unhbox 	(screen	EYEDIAP (screen target)
voidb@x penalty @M  {}al.} 2003	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Wang and Ji} 2017	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Wood and Bulling} 2014	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Yoo and Chung} 2005	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
https://github.com/crisie/RecurrentGaze 	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	(screen	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 3	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
potentially applicable to relatively low 	(screen	EYEDIAP (screen target)
resolution images and mid-distance scenarios. 	(screen	EYEDIAP (screen target)
Dif- ferent mapping functions have 	(screen	EYEDIAP (screen target)
been explored, such as neural 	(screen	EYEDIAP (screen target)
networks [2], adaptive linear regression (	(screen	EYEDIAP (screen target)
ALR) [19], local interpolation [32], 	(screen	EYEDIAP (screen target)
gaussian processes [30, 35], random 	(screen	EYEDIAP (screen target)
forests [11, 31], or k-nearest 	(screen	EYEDIAP (screen target)
neighbors [40]. Main challenges of 	(screen	EYEDIAP (screen target)
appearance-based methods for 3D gaze 	(screen	EYEDIAP (screen target)
estimation are head pose, illumination 	(screen	EYEDIAP (screen target)
and subject invariance without user-specific 	(screen	EYEDIAP (screen target)
calibration. To handle these issues, 	(screen	EYEDIAP (screen target)
some works proposed compensation 	(screen	EYEDIAP (screen target)
methods [18] and warping strategies 	(screen	EYEDIAP (screen target)
that synthesize a canonical, frontal 	(screen	EYEDIAP (screen target)
looking view of the 	(screen	EYEDIAP (screen target)
face [6, 13, 21]. Hybrid 	(screen	EYEDIAP (screen target)
approaches based on analysis-by-synthesis have 	(screen	EYEDIAP (screen target)
also been evaluated [39].  Currently, data-driven methods are considered	(screen	EYEDIAP (screen target)
 the state of the art	(screen	EYEDIAP (screen target)
 for person- and head pose-independent	(screen	EYEDIAP (screen target)
 appearance-based gaze estimation. Consequently, a	(screen	EYEDIAP (screen target)
 number of gaze es- timation	(screen	EYEDIAP (screen target)
 datasets have been introduced in	(screen	EYEDIAP (screen target)
 recent years, either in controlled	(screen	EYEDIAP (screen target)
 [29] or semi- controlled settings	(screen	EYEDIAP (screen target)
 [8], in the wild [16	(screen	EYEDIAP (screen target)
, 42], or consisting of 	(screen	EYEDIAP (screen target)
synthetic data [31, 38, 40]. 	(screen	EYEDIAP (screen target)
Zhang et al. [42] showed 	(screen	EYEDIAP (screen target)
that CNNs can outperform other 	(screen	EYEDIAP (screen target)
mapping methods, using a multi- 	(screen	EYEDIAP (screen target)
modal CNN to learn the 	(screen	EYEDIAP (screen target)
mapping from 3D head poses 	(screen	EYEDIAP (screen target)
and eye images to 3D 	(screen	EYEDIAP (screen target)
gaze directions. Krafka et 	(screen	EYEDIAP (screen target)
al. [16] proposed a multi-stream 	(screen	EYEDIAP (screen target)
CNN for 2D gaze estimation, 	(screen	EYEDIAP (screen target)
using individual eye, whole-face image 	(screen	EYEDIAP (screen target)
and the face grid as 	(screen	EYEDIAP (screen target)
input. As this method was 	(screen	EYEDIAP (screen target)
limited to 2D screen mapping, 	(screen	EYEDIAP (screen target)
Zhang et al. [43] later 	(screen	EYEDIAP (screen target)
explored the potential of just 	(screen	EYEDIAP (screen target)
using whole-face images as input 	(screen	EYEDIAP (screen target)
to estimate 3D gaze directions. 	(screen	EYEDIAP (screen target)
Using a spatial weights CNN, 	(screen	EYEDIAP (screen target)
they demonstrated their method to 	(screen	EYEDIAP (screen target)
be more robust to facial 	(screen	EYEDIAP (screen target)
appearance variation caused by head 	(screen	EYEDIAP (screen target)
pose and illumina- tion than 	(screen	EYEDIAP (screen target)
eye-only methods. While the method 	(screen	EYEDIAP (screen target)
was evaluated in the wild, 	(screen	EYEDIAP (screen target)
the subjects were only interacting 	(screen	EYEDIAP (screen target)
with a mobile device, thus 	(screen	EYEDIAP (screen target)
restricting the head pose range. 	(screen	EYEDIAP (screen target)
Deng and Zhu [4] presented 	(screen	EYEDIAP (screen target)
a two-stream CNN to disjointly 	(screen	EYEDIAP (screen target)
model head pose from face 	(screen	EYEDIAP (screen target)
images and eye- ball movement 	(screen	EYEDIAP (screen target)
from eye region images. Both 	(screen	EYEDIAP (screen target)
were then aggregated into 3D 	(screen	EYEDIAP (screen target)
gaze direction using a gaze 	(screen	EYEDIAP (screen target)
transform layer. The decomposition was 	(screen	EYEDIAP (screen target)
aimed to avoid head-correlation over- 	(screen	EYEDIAP (screen target)
fitting of previous data-driven approaches. 	(screen	EYEDIAP (screen target)
They evaluated their approach in 	(screen	EYEDIAP (screen target)
the wild with a wider 	(screen	EYEDIAP (screen target)
range of head poses, obtaining 	(screen	EYEDIAP (screen target)
better performance than previous eye-based 	(screen	EYEDIAP (screen target)
methods. However, they did not 	(screen	EYEDIAP (screen target)
test it on public annotated 	(screen	EYEDIAP (screen target)
benchmark datasets.  In this paper, we propose	(screen	EYEDIAP (screen target)
 a multi-stream recurrent CNN network	(screen	EYEDIAP (screen target)
 for person- and head pose-independent	(screen	EYEDIAP (screen target)
 3D gaze estimation for a	(screen	EYEDIAP (screen target)
 mid-distance scenario. We evaluate it	(screen	EYEDIAP (screen target)
 on a wider range of	(screen	EYEDIAP (screen target)
 head poses and gaze directions	(screen	EYEDIAP (screen target)
 than screen-targeted approaches. As opposed	(screen	EYEDIAP (screen target)
 to previous methods, we also	(screen	EYEDIAP (screen target)
 rely on temporal information inherent	(screen	EYEDIAP (screen target)
 in sequential data	(screen	EYEDIAP (screen target)
.  3 Methodology	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
In this section, we present 	(screen	EYEDIAP (screen target)
our approach for 3D gaze 	(screen	EYEDIAP (screen target)
regression based on appearance and 	(screen	EYEDIAP (screen target)
shape cues for still images 	(screen	EYEDIAP (screen target)
and image sequences. First, we 	(screen	EYEDIAP (screen target)
introduce the data modalities and 	(screen	EYEDIAP (screen target)
formulate the problem. Then, we 	(screen	EYEDIAP (screen target)
detail the normalization procedure prior 	(screen	EYEDIAP (screen target)
to the regression stage. Finally, 	(screen	EYEDIAP (screen target)
we explain the global network 	(screen	EYEDIAP (screen target)
topology as well as the 	(screen	EYEDIAP (screen target)
implementation details. An overview of 	(screen	EYEDIAP (screen target)
the system architecture is depicted 	(screen	EYEDIAP (screen target)
in Figure 1.  3.1 Multi-modal gaze regression	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Let us represent gaze direction 	(screen	EYEDIAP (screen target)
as a 3D unit vector 	(screen	EYEDIAP (screen target)
g = [gx,gy,gz]T ∈R3 in 	(screen	EYEDIAP (screen target)
the Camera Coor- dinate System (	(screen	EYEDIAP (screen target)
CCS), whose origin is the 	(screen	EYEDIAP (screen target)
central point between eyeball centers. 	(screen	EYEDIAP (screen target)
Assuming a calibrated camera, and 	(screen	EYEDIAP (screen target)
a known head position and 	(screen	EYEDIAP (screen target)
orientation, our goal is to 	(screen	EYEDIAP (screen target)
estimate g from a sequence 	(screen	EYEDIAP (screen target)
of images {I(i) | 	(screen	EYEDIAP (screen target)
I ∈ RW×H×3} as a 	(screen	EYEDIAP (screen target)
regression problem.  Citation Citation {Baluja and Pomerleau	(screen	EYEDIAP (screen target)
} 1994  Citation Citation {Lu, Sugano, Okabe	(screen	EYEDIAP (screen target)
, and Sato} 2011{}  Citation Citation {Tan, Kriegman, and	(screen	EYEDIAP (screen target)
 Ahuja} 2002	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Sugano, Matsushita, and 	(screen	EYEDIAP (screen target)
Sato} 2013  Citation Citation {Williams, Blake, and	(screen	EYEDIAP (screen target)
 Cipolla} 2006	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Huang, Veeraraghavan, and 	(screen	EYEDIAP (screen target)
Sabharwal} 2017  Citation Citation {Sugano, Matsushita, and	(screen	EYEDIAP (screen target)
 Sato} 2014	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Wood, Baltru{²}aitis, Morency, 	(screen	EYEDIAP (screen target)
Robinson, and Bulling} 2016{}  Citation Citation {Lu, Okabe, Sugano	(screen	EYEDIAP (screen target)
, and Sato} 2011{}  Citation Citation {Funes-Mora and Odobez	(screen	EYEDIAP (screen target)
} 2016  Citation Citation {Jeni and Cohn	(screen	EYEDIAP (screen target)
} 2016  Citation Citation {Mora and Odobez	(screen	EYEDIAP (screen target)
} 2012  Citation Citation {Wood, Baltru{²}aitis, Morency	(screen	EYEDIAP (screen target)
, Robinson, and Bulling} 2016{}  Citation Citation {Smith, Yin, Feiner	(screen	EYEDIAP (screen target)
, and Nayar} 2013  Citation Citation {Funesprotect unhbox voidb@x	(screen	EYEDIAP (screen target)
 penalty @M	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Mora, Monay, and Odobez} 2014{}  Citation Citation {Krafka, Khosla, Kellnhofer	(screen	EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 	(screen	EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	(screen	EYEDIAP (screen target)
, and Bulling} 2015  Citation Citation {Sugano, Matsushita, and	(screen	EYEDIAP (screen target)
 Sato} 2014	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Wood, Baltrusaitis, Zhang, 	(screen	EYEDIAP (screen target)
Sugano, Robinson, and Bulling} 2015  Citation Citation {Wood, Baltru{²}aitis, Morency	(screen	EYEDIAP (screen target)
, Robinson, and Bulling} 2016{}  Citation Citation {Zhang, Sugano, Fritz	(screen	EYEDIAP (screen target)
, and Bulling} 2015  Citation Citation {Krafka, Khosla, Kellnhofer	(screen	EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 	(screen	EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	(screen	EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Deng and Zhu	(screen	EYEDIAP (screen target)
} 2017	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
4 PALMERO ET AL.: MULTI-MODAL 	(screen	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	(screen	EYEDIAP (screen target)
ESTIMATION  Conv	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
C on ca t  x y z x y	(screen	EYEDIAP (screen target)
 z x y z	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Individual Fusion Temporal  Individual Fusion	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Input 	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Individual Fusion  Normalization	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
 .Conv	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Conv .  Conv	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Conv .  FC	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
FC FC RNN  RNN	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
RNN FC  Ti m e	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Figure 1: Overview of the 	(screen	EYEDIAP (screen target)
proposed network. A multi-stream CNN 	(screen	EYEDIAP (screen target)
jointly models full-face, eye region 	(screen	EYEDIAP (screen target)
appearance and face landmarks from 	(screen	EYEDIAP (screen target)
still images. The combined extracted 	(screen	EYEDIAP (screen target)
fea- tures from each frame 	(screen	EYEDIAP (screen target)
are fed into a recurrent 	(screen	EYEDIAP (screen target)
module to predict last frame’s 	(screen	EYEDIAP (screen target)
gaze direction.  Gazing to a specific target	(screen	EYEDIAP (screen target)
 is achieved by a combination	(screen	EYEDIAP (screen target)
 of eye and head movements	(screen	EYEDIAP (screen target)
, which are highly coordinated. 	(screen	EYEDIAP (screen target)
Consequently, the apparent direction of 	(screen	EYEDIAP (screen target)
gaze is influenced not only 	(screen	EYEDIAP (screen target)
by the location of the 	(screen	EYEDIAP (screen target)
irises within the eyelid aperture, 	(screen	EYEDIAP (screen target)
but also by the position 	(screen	EYEDIAP (screen target)
and orientation of the face 	(screen	EYEDIAP (screen target)
with respect to the camera. 	(screen	EYEDIAP (screen target)
Known as the Wollaston 	(screen	EYEDIAP (screen target)
effect [36], the exact same 	(screen	EYEDIAP (screen target)
set of eyes may appear 	(screen	EYEDIAP (screen target)
to be looking in different 	(screen	EYEDIAP (screen target)
directions due to the surrounding 	(screen	EYEDIAP (screen target)
facial cues. It is therefore 	(screen	EYEDIAP (screen target)
reasonable to state that eye 	(screen	EYEDIAP (screen target)
images are not sufficient to 	(screen	EYEDIAP (screen target)
estimate gaze direction. Instead, whole-face 	(screen	EYEDIAP (screen target)
images can encode head pose 	(screen	EYEDIAP (screen target)
or illumination-specific information across larger 	(screen	EYEDIAP (screen target)
areas than those available just 	(screen	EYEDIAP (screen target)
in the eyes region [16, 43	(screen	EYEDIAP (screen target)
].  The drawback of appearance-only methods	(screen	EYEDIAP (screen target)
 is that global structure information	(screen	EYEDIAP (screen target)
 is not explicitly considered. In	(screen	EYEDIAP (screen target)
 that sense, facial landmarks can	(screen	EYEDIAP (screen target)
 be used as global shape	(screen	EYEDIAP (screen target)
 cues to en- code spatial	(screen	EYEDIAP (screen target)
 relationships and geometric constraints. Current	(screen	EYEDIAP (screen target)
 state-of-the-art face alignment approaches are	(screen	EYEDIAP (screen target)
 robust enough to handle large	(screen	EYEDIAP (screen target)
 appearance variability, extreme head poses	(screen	EYEDIAP (screen target)
 and occlusions, being especially useful	(screen	EYEDIAP (screen target)
 when the dataset used for	(screen	EYEDIAP (screen target)
 gaze estimation does not contain	(screen	EYEDIAP (screen target)
 such variability. Facial landmarks are	(screen	EYEDIAP (screen target)
 mainly correlated with head orientation	(screen	EYEDIAP (screen target)
, eye position, eyelid openness, 	(screen	EYEDIAP (screen target)
and eyebrow movement, which are 	(screen	EYEDIAP (screen target)
valuable features for our task.  Therefore, in our approach we	(screen	EYEDIAP (screen target)
 jointly model appearance and shape	(screen	EYEDIAP (screen target)
 cues (see Figure 1). The	(screen	EYEDIAP (screen target)
 former is represented by a	(screen	EYEDIAP (screen target)
 whole-face image IF , along	(screen	EYEDIAP (screen target)
 with a higher resolution image	(screen	EYEDIAP (screen target)
 of the eyes IE to	(screen	EYEDIAP (screen target)
 identify subtle changes. Due to	(screen	EYEDIAP (screen target)
 dealing with wide head pose	(screen	EYEDIAP (screen target)
 ranges, some eye images may	(screen	EYEDIAP (screen target)
 not depict the whole eye	(screen	EYEDIAP (screen target)
, containing mostly background or 	(screen	EYEDIAP (screen target)
other surrounding facial parts instead. 	(screen	EYEDIAP (screen target)
For that reason, and contrary 	(screen	EYEDIAP (screen target)
to previous approaches that only 	(screen	EYEDIAP (screen target)
use one eye image [31, 42	(screen	EYEDIAP (screen target)
], we use a single 	(screen	EYEDIAP (screen target)
image composed of two patches 	(screen	EYEDIAP (screen target)
of centered left and right 	(screen	EYEDIAP (screen target)
eyes. Finally, the shape cue 	(screen	EYEDIAP (screen target)
is represented by 3D face 	(screen	EYEDIAP (screen target)
landmarks obtained from a 68-landmark 	(screen	EYEDIAP (screen target)
model, denoted by 	(screen	EYEDIAP (screen target)
L = {(lx, ly, 	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
)	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
 | ∀c ∈ [1, ...,68	(screen	EYEDIAP (screen target)
]}.  In this work we also	(screen	EYEDIAP (screen target)
 consider the dynamic component of	(screen	EYEDIAP (screen target)
 gaze. We leverage the se	(screen	EYEDIAP (screen target)
- quential information of eye 	(screen	EYEDIAP (screen target)
and head movements such that, 	(screen	EYEDIAP (screen target)
given appearance and shape features 	(screen	EYEDIAP (screen target)
of consecutive frames, it is 	(screen	EYEDIAP (screen target)
possible to better predict the 	(screen	EYEDIAP (screen target)
gaze direction of the cur- 	(screen	EYEDIAP (screen target)
rent frame. Therefore, the 3D 	(screen	EYEDIAP (screen target)
gaze estimation task for a 1	(screen	EYEDIAP (screen target)
-frame sequence is formulated  Citation Citation {Wollaston etprotect unhbox	(screen	EYEDIAP (screen target)
 voidb@x penalty @M	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
al.} 1824  Citation Citation {Krafka, Khosla, Kellnhofer	(screen	EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 	(screen	EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	(screen	EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Sugano, Matsushita, and	(screen	EYEDIAP (screen target)
 Sato} 2014	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Zhang, Sugano, Fritz, 	(screen	EYEDIAP (screen target)
and Bulling} 2015	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	(screen	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 5	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
as g(i) = f ( {IF (i)},{IE (i)},{L(i	(screen	EYEDIAP (screen target)
)}  ) , where i denotes	(screen	EYEDIAP (screen target)
 the i-th frame, and f	(screen	EYEDIAP (screen target)
 is the regression	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
function.  3.2 Data normalization Prior to	(screen	EYEDIAP (screen target)
 gaze regression, a normalization step	(screen	EYEDIAP (screen target)
 in the 3D space and	(screen	EYEDIAP (screen target)
 the 2D image, similar to	(screen	EYEDIAP (screen target)
 [31], is carried out. This	(screen	EYEDIAP (screen target)
 is performed to reduce the	(screen	EYEDIAP (screen target)
 appearance variability and to allow	(screen	EYEDIAP (screen target)
 the gaze estimation model to	(screen	EYEDIAP (screen target)
 be applied regardless of the	(screen	EYEDIAP (screen target)
 original camera configuration	(screen	EYEDIAP (screen target)
.  Let H ∈ R3x3 be	(screen	EYEDIAP (screen target)
 the head rotation matrix, and	(screen	EYEDIAP (screen target)
 p = [px, py, pz]T	(screen	EYEDIAP (screen target)
 ∈ R3 the reference face	(screen	EYEDIAP (screen target)
 location with respect to the	(screen	EYEDIAP (screen target)
 original CCS. The goal is	(screen	EYEDIAP (screen target)
 to find the conversion matrix	(screen	EYEDIAP (screen target)
 M = SR such that	(screen	EYEDIAP (screen target)
 (a) the X-axes of the	(screen	EYEDIAP (screen target)
 virtual camera and the head	(screen	EYEDIAP (screen target)
 become parallel using the rotation	(screen	EYEDIAP (screen target)
 matrix R, and (b) the	(screen	EYEDIAP (screen target)
 virtual camera looks at the	(screen	EYEDIAP (screen target)
 reference location from a fixed	(screen	EYEDIAP (screen target)
 distance dn using the Z-direction	(screen	EYEDIAP (screen target)
 scaling matrix S = diag(1,1,dn/‖p	(screen	EYEDIAP (screen target)
‖). R is computed as 	(screen	EYEDIAP (screen target)
a = p̂×HT e1, 	(screen	EYEDIAP (screen target)
b = â× p̂, 	(screen	EYEDIAP (screen target)
R = [â, b̂, p̂]T , where e1 denotes the first	(screen	EYEDIAP (screen target)
 orthonormal basis and	(screen	EYEDIAP (screen target)
 〈 ·̂ 〉 is the	(screen	EYEDIAP (screen target)
 unit vector	(screen	EYEDIAP (screen target)
.  This normalization translates into the	(screen	EYEDIAP (screen target)
 image space as a cropped	(screen	EYEDIAP (screen target)
 image patch of size Wn×Hn	(screen	EYEDIAP (screen target)
 centered at p where head	(screen	EYEDIAP (screen target)
 roll rotation has been removed	(screen	EYEDIAP (screen target)
. This is done by 	(screen	EYEDIAP (screen target)
applying a perspective warping to 	(screen	EYEDIAP (screen target)
the input image I using 	(screen	EYEDIAP (screen target)
the transformation matrix W = 	(screen	EYEDIAP (screen target)
CoMCn−1, where Co and Cn 	(screen	EYEDIAP (screen target)
are the original and virtual 	(screen	EYEDIAP (screen target)
camera matrices, respectively.  The 3D gaze vector is	(screen	EYEDIAP (screen target)
 also normalized as gn =Rg	(screen	EYEDIAP (screen target)
. After image normalization, the 	(screen	EYEDIAP (screen target)
line of sight can be 	(screen	EYEDIAP (screen target)
represented in a 2D space. 	(screen	EYEDIAP (screen target)
Therefore, gn is further transformed 	(screen	EYEDIAP (screen target)
to spherical coor- dinates (θ ,	(screen	EYEDIAP (screen target)
φ) assuming unit length, where 	(screen	EYEDIAP (screen target)
θ and φ denote the 	(screen	EYEDIAP (screen target)
horizontal and vertical direc- tion 	(screen	EYEDIAP (screen target)
angles, respectively. This 2D angle 	(screen	EYEDIAP (screen target)
representation, delimited in the 	(screen	EYEDIAP (screen target)
range [−π/2,π/2], is computed as 	(screen	EYEDIAP (screen target)
θ = arctan(gx/gz) and 	(screen	EYEDIAP (screen target)
φ = arcsin(−gy), such that (0,	(screen	EYEDIAP (screen target)
0) represents looking straight ahead 	(screen	EYEDIAP (screen target)
to the CCS origin.  3.3 Recurrent Convolutional Neural Network	(screen	EYEDIAP (screen target)
 We propose a Recurrent CNN	(screen	EYEDIAP (screen target)
 Regression Network for 3D gaze	(screen	EYEDIAP (screen target)
 estimation. The network is divided	(screen	EYEDIAP (screen target)
 in 3 modules: (1) Individual	(screen	EYEDIAP (screen target)
, (2) Fusion, and (3) 	(screen	EYEDIAP (screen target)
Temporal.  First, the Individual module learns	(screen	EYEDIAP (screen target)
 features from each appearance cue	(screen	EYEDIAP (screen target)
 separately. It consists of a	(screen	EYEDIAP (screen target)
 two-stream CNN, one devoted to	(screen	EYEDIAP (screen target)
 the normalized face image stream	(screen	EYEDIAP (screen target)
 and the other to the	(screen	EYEDIAP (screen target)
 joint normalized eyes image. Next	(screen	EYEDIAP (screen target)
, the Fusion module combines 	(screen	EYEDIAP (screen target)
the extracted features of each 	(screen	EYEDIAP (screen target)
appearance stream in a single 	(screen	EYEDIAP (screen target)
vector along with the normalized 	(screen	EYEDIAP (screen target)
landmark coordinates. Then, it learns 	(screen	EYEDIAP (screen target)
a joint representation between modalities 	(screen	EYEDIAP (screen target)
in a late-fusion fashion. Both 	(screen	EYEDIAP (screen target)
Individual and Fusion modules, further 	(screen	EYEDIAP (screen target)
referred to as Static model, 	(screen	EYEDIAP (screen target)
are applied to each frame 	(screen	EYEDIAP (screen target)
of the sequence. Finally, the 	(screen	EYEDIAP (screen target)
resulting feature vectors of each 	(screen	EYEDIAP (screen target)
frame are input to the 	(screen	EYEDIAP (screen target)
Temporal module based on a 	(screen	EYEDIAP (screen target)
many-to-one recurrent network. This module 	(screen	EYEDIAP (screen target)
leverages sequential information to predict 	(screen	EYEDIAP (screen target)
the normalized 2D gaze angles 	(screen	EYEDIAP (screen target)
of the last frame of 	(screen	EYEDIAP (screen target)
the sequence using a linear 	(screen	EYEDIAP (screen target)
regression layer added on top 	(screen	EYEDIAP (screen target)
of it.  3.4 Implementation details 3.4.1 Network	(screen	EYEDIAP (screen target)
 details	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Each stream of the Individual 	(screen	EYEDIAP (screen target)
module is based on the 	(screen	EYEDIAP (screen target)
VGG-16 deep network [27], consisting 	(screen	EYEDIAP (screen target)
of 13 convolutional layers, 5 	(screen	EYEDIAP (screen target)
max pooling layers, and 1 	(screen	EYEDIAP (screen target)
fully connected (FC) layer with 	(screen	EYEDIAP (screen target)
Rec- tified Linear Unit (ReLU) 	(screen	EYEDIAP (screen target)
activations. The full-face stream follows 	(screen	EYEDIAP (screen target)
the same configuration  Citation Citation {Sugano, Matsushita, and	(screen	EYEDIAP (screen target)
 Sato} 2014	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Parkhi, Vedaldi, and 	(screen	EYEDIAP (screen target)
Zisserman} 2015	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
6 PALMERO ET AL.: MULTI-MODAL 	(screen	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	(screen	EYEDIAP (screen target)
ESTIMATION  as the base network, having	(screen	EYEDIAP (screen target)
 an input of 224×224 pixels	(screen	EYEDIAP (screen target)
 and a 4096D FC layer	(screen	EYEDIAP (screen target)
. In contrast, the input 	(screen	EYEDIAP (screen target)
joint eye image is smaller, 	(screen	EYEDIAP (screen target)
with a final size of 120	(screen	EYEDIAP (screen target)
×48 pixels, so the number 	(screen	EYEDIAP (screen target)
of pa- rameters is decreased 	(screen	EYEDIAP (screen target)
proportionally. In this case, its 	(screen	EYEDIAP (screen target)
last FC layer produces a 	(screen	EYEDIAP (screen target)
1536D vector. A 204D landmark 	(screen	EYEDIAP (screen target)
coordinates vector is concatenated to 	(screen	EYEDIAP (screen target)
the output of the FC 	(screen	EYEDIAP (screen target)
layer of each stream, resulting 	(screen	EYEDIAP (screen target)
in a 5836D feature vector. 	(screen	EYEDIAP (screen target)
Consequently, the Fusion module consists 	(screen	EYEDIAP (screen target)
of 2 5836D FC layers 	(screen	EYEDIAP (screen target)
with ReLU activations and 2 	(screen	EYEDIAP (screen target)
dropout layers between FCs as 	(screen	EYEDIAP (screen target)
regularization. Finally, to model the 	(screen	EYEDIAP (screen target)
temporal dependencies, we use a 	(screen	EYEDIAP (screen target)
single GRU layer with 128 	(screen	EYEDIAP (screen target)
units.  The network is trained in	(screen	EYEDIAP (screen target)
 a stage-wise fashion. First, we	(screen	EYEDIAP (screen target)
 train the Static model and	(screen	EYEDIAP (screen target)
 the final regression layer end-to-end	(screen	EYEDIAP (screen target)
 on each individual frame of	(screen	EYEDIAP (screen target)
 the training data. The convolutional	(screen	EYEDIAP (screen target)
 blocks are pre-trained with the	(screen	EYEDIAP (screen target)
 VGG-Face dataset [27], whereas the	(screen	EYEDIAP (screen target)
 FCs are trained from scratch	(screen	EYEDIAP (screen target)
. Second, the training data 	(screen	EYEDIAP (screen target)
is re-arranged by means of 	(screen	EYEDIAP (screen target)
a sliding window with stride 1 to build input sequences. Each	(screen	EYEDIAP (screen target)
 sequence is composed of s	(screen	EYEDIAP (screen target)
 = 4 consecutive frames, whose	(screen	EYEDIAP (screen target)
 gaze direction target is the	(screen	EYEDIAP (screen target)
 gaze direction of the last	(screen	EYEDIAP (screen target)
 frame of the sequence( {I(i−s+1	(screen	EYEDIAP (screen target)
), . . . ,I(i)}, 	(screen	EYEDIAP (screen target)
g(i)  ) . Using this re-arranged	(screen	EYEDIAP (screen target)
 training data, we extract features	(screen	EYEDIAP (screen target)
 of each	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
frame of the sequence from 	(screen	EYEDIAP (screen target)
a frozen Individual module, fine-tune 	(screen	EYEDIAP (screen target)
the Fusion layers, and train 	(screen	EYEDIAP (screen target)
both, the Temporal module and 	(screen	EYEDIAP (screen target)
a new final regression layer 	(screen	EYEDIAP (screen target)
from scratch. This way, the 	(screen	EYEDIAP (screen target)
network can exploit the temporal 	(screen	EYEDIAP (screen target)
information to further refine the 	(screen	EYEDIAP (screen target)
fusion weights.  We trained the model using	(screen	EYEDIAP (screen target)
 ADAM optimizer with an initial	(screen	EYEDIAP (screen target)
 learning rate of 0.0001, dropout	(screen	EYEDIAP (screen target)
 of 0.3, and batch size	(screen	EYEDIAP (screen target)
 of 64 frames. The number	(screen	EYEDIAP (screen target)
 of epochs was experimentally set	(screen	EYEDIAP (screen target)
 to 21 for the first	(screen	EYEDIAP (screen target)
 training stage and 10 for	(screen	EYEDIAP (screen target)
 the second. We use the	(screen	EYEDIAP (screen target)
 average Euclidean distance between the	(screen	EYEDIAP (screen target)
 predicted and ground-truth 3D gaze	(screen	EYEDIAP (screen target)
 vectors as loss function	(screen	EYEDIAP (screen target)
.  3.4.2 Input pre-processing	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
For this work we use 	(screen	EYEDIAP (screen target)
head pose and eye locations 	(screen	EYEDIAP (screen target)
in the 3D scene provided 	(screen	EYEDIAP (screen target)
by the dataset. The 3D 	(screen	EYEDIAP (screen target)
landmarks are extracted using the 	(screen	EYEDIAP (screen target)
state-of-the-art method of Bulat and 	(screen	EYEDIAP (screen target)
Tzimiropou- los [3], which is 	(screen	EYEDIAP (screen target)
based on stacked hourglass 	(screen	EYEDIAP (screen target)
networks [24].  During training, the original image	(screen	EYEDIAP (screen target)
 is pre-processed to get the	(screen	EYEDIAP (screen target)
 two normalized input images. The	(screen	EYEDIAP (screen target)
 normalized whole-face patch is centered	(screen	EYEDIAP (screen target)
 0.1 meters ahead of the	(screen	EYEDIAP (screen target)
 head center in the head	(screen	EYEDIAP (screen target)
 coordinate system, and Cn is	(screen	EYEDIAP (screen target)
 defined such that the image	(screen	EYEDIAP (screen target)
 has size of 250× 250	(screen	EYEDIAP (screen target)
 pixels. The difference between this	(screen	EYEDIAP (screen target)
 size and the final input	(screen	EYEDIAP (screen target)
 size allows us to perform	(screen	EYEDIAP (screen target)
 random cropping and zooming to	(screen	EYEDIAP (screen target)
 augment the data (explained in	(screen	EYEDIAP (screen target)
 Section 4.1). Similarly, each normalized	(screen	EYEDIAP (screen target)
 eye patch is centered in	(screen	EYEDIAP (screen target)
 their respective eye center locations	(screen	EYEDIAP (screen target)
. In this case, the 	(screen	EYEDIAP (screen target)
virtual camera matrix is defined 	(screen	EYEDIAP (screen target)
so that the image is 	(screen	EYEDIAP (screen target)
cropped to 70×58, while in 	(screen	EYEDIAP (screen target)
practice the final patches have 	(screen	EYEDIAP (screen target)
size of 60×48. Landmarks are 	(screen	EYEDIAP (screen target)
normalized using the same procedure 	(screen	EYEDIAP (screen target)
and further pre-processed with mean 	(screen	EYEDIAP (screen target)
subtraction and min-max normalization per 	(screen	EYEDIAP (screen target)
axis. Finally, we divide them 	(screen	EYEDIAP (screen target)
by a scaling factor w 	(screen	EYEDIAP (screen target)
such that all coordinates are 	(screen	EYEDIAP (screen target)
in the range [0,w]. This 	(screen	EYEDIAP (screen target)
way, all concatenated feature values 	(screen	EYEDIAP (screen target)
are in a similar range. 	(screen	EYEDIAP (screen target)
After inference, the predicted normalized 	(screen	EYEDIAP (screen target)
2D angles are de-normalized back 	(screen	EYEDIAP (screen target)
to the original 3D space.  4 Experiments In this section	(screen	EYEDIAP (screen target)
, we evaluate the cross-subject 	(screen	EYEDIAP (screen target)
3D gaze estimation task on 	(screen	EYEDIAP (screen target)
a wide range of head 	(screen	EYEDIAP (screen target)
poses and gaze directions. Furthermore, 	(screen	EYEDIAP (screen target)
we validate the effectiveness of 	(screen	EYEDIAP (screen target)
the proposed architecture comparing both 	(screen	EYEDIAP (screen target)
static and temporal approaches. We 	(screen	EYEDIAP (screen target)
report the error in terms 	(screen	EYEDIAP (screen target)
of mean angular error between 	(screen	EYEDIAP (screen target)
predicted and ground-truth 3D gaze 	(screen	EYEDIAP (screen target)
vectors. Note that due to 	(screen	EYEDIAP (screen target)
the requirements of the temporal 	(screen	EYEDIAP (screen target)
model not all the frames 	(screen	EYEDIAP (screen target)
obtain a prediction. Therefore, for 	(screen	EYEDIAP (screen target)
a  Citation Citation {Parkhi, Vedaldi, and	(screen	EYEDIAP (screen target)
 Zisserman} 2015	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Bulat and Tzimiropoulos} 2017	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Newell, Yang, and 	(screen	EYEDIAP (screen target)
Deng} 2016	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	(screen	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 7	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
60 30 0 30 60  60	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
30  0	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
30  60	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
100  101	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
102  60 30 0 30 60	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
60  30	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
0  30	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
60  100	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
101  102	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
103  60 30 0 30 60	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
60  30	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
0  30	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
60  100	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
101  102	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
60 30 0 30 60  60	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
30  0	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
30  60	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
100  101	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
102  103	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
a) g (FT ) (b) 	(screen	EYEDIAP (screen target)
h (FT ) (c) g (	(screen	EYEDIAP (screen target)
CS) (d) h (CS)  Figure 2: Ground-truth eye gaze	(screen	EYEDIAP (screen target)
 g and head orientation h	(screen	EYEDIAP (screen target)
 distribution on the filtered EYE	(screen	EYEDIAP (screen target)
- DIAP dataset for CS 	(screen	EYEDIAP (screen target)
and FT settings, in terms 	(screen	EYEDIAP (screen target)
of x- and y- angles.  fair comparison, the reported results	(screen	EYEDIAP (screen target)
 for static models disregard such	(screen	EYEDIAP (screen target)
 frames when temporal models are	(screen	EYEDIAP (screen target)
 included in the comparison	(screen	EYEDIAP (screen target)
.  4.1 Training data	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
There are few publicly available 	(screen	EYEDIAP (screen target)
datasets devoted to 3D gaze 	(screen	EYEDIAP (screen target)
estimation and most of them 	(screen	EYEDIAP (screen target)
focus on HCI with a 	(screen	EYEDIAP (screen target)
limited range of head pose 	(screen	EYEDIAP (screen target)
and gaze directions. Therefore, we 	(screen	EYEDIAP (screen target)
use VGA videos from the 	(screen	EYEDIAP (screen target)
publicly-available EYEDIAP dataset [7] to 	(screen	EYEDIAP (screen target)
perform the experimental evaluation, as 	(screen	EYEDIAP (screen target)
it is currently the only 	(screen	EYEDIAP (screen target)
one containing video sequences with 	(screen	EYEDIAP (screen target)
a wide range of head 	(screen	EYEDIAP (screen target)
poses and showing the full 	(screen	EYEDIAP (screen target)
face. This dataset consists of 3	(screen	EYEDIAP (screen target)
-minute videos of 16 subjects 	(screen	EYEDIAP (screen target)
looking at two types of 	(screen	EYEDIAP (screen target)
targets: continuous screen targets on 	(screen	EYEDIAP (screen target)
a fixed monitor (CS), and 	(screen	EYEDIAP (screen target)
floating physical targets (FT ). 	(screen	EYEDIAP (screen target)
The videos are further divided 	(screen	EYEDIAP (screen target)
into static (S) and moving (	(screen	EYEDIAP (screen target)
M) head pose for each 	(screen	EYEDIAP (screen target)
of the subjects. Subjects 12-16 	(screen	EYEDIAP (screen target)
were recorded with 2 different 	(screen	EYEDIAP (screen target)
lighting conditions.  For evaluation, we filtered out	(screen	EYEDIAP (screen target)
 those frames that fulfilled at	(screen	EYEDIAP (screen target)
 least one of the following	(screen	EYEDIAP (screen target)
 conditions: (1) face or landmarks	(screen	EYEDIAP (screen target)
 not detected; (2) subject not	(screen	EYEDIAP (screen target)
 looking at the target; (3	(screen	EYEDIAP (screen target)
) 3D head pose, eyes 	(screen	EYEDIAP (screen target)
or target location not properly 	(screen	EYEDIAP (screen target)
recovered; and (4) eyeball rotations 	(screen	EYEDIAP (screen target)
violating physical 	(screen	EYEDIAP (screen target)
constraints (|θ | ≤ 40	(screen	EYEDIAP (screen target)
◦, |φ | ≤ 30	(screen	EYEDIAP (screen target)
◦) [23]. Note that we 	(screen	EYEDIAP (screen target)
purposely do not filter eye 	(screen	EYEDIAP (screen target)
blinking moments to learn their 	(screen	EYEDIAP (screen target)
dynamics with the temporal model, 	(screen	EYEDIAP (screen target)
which may produce some outliers 	(screen	EYEDIAP (screen target)
with a higher prediction error 	(screen	EYEDIAP (screen target)
due to a less accurate 	(screen	EYEDIAP (screen target)
ground truth. Figure 2 shows 	(screen	EYEDIAP (screen target)
the distribution of gaze directions 	(screen	EYEDIAP (screen target)
and head poses for both 	(screen	EYEDIAP (screen target)
filtered CS and FT cases.  We applied data augmentation to	(screen	EYEDIAP (screen target)
 the training set with the	(screen	EYEDIAP (screen target)
 following random transforma- tions: horizontal	(screen	EYEDIAP (screen target)
 flip, shifts of up to	(screen	EYEDIAP (screen target)
 5 pixels, zoom of up	(screen	EYEDIAP (screen target)
 to 2%, brightness changes by	(screen	EYEDIAP (screen target)
 a factor in the range	(screen	EYEDIAP (screen target)
 [0.4,1.75], and additive Gaussian noise	(screen	EYEDIAP (screen target)
 with σ2 = 0.03	(screen	EYEDIAP (screen target)
.  4.2 Evaluation of static modalities	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
First, we evaluate the contribution 	(screen	EYEDIAP (screen target)
of each static modality on 	(screen	EYEDIAP (screen target)
the FT scenario. We divided 	(screen	EYEDIAP (screen target)
the 16 participants into 4 	(screen	EYEDIAP (screen target)
groups, such that appearance variability 	(screen	EYEDIAP (screen target)
was maximized while maintaining a 	(screen	EYEDIAP (screen target)
similar number of training samples 	(screen	EYEDIAP (screen target)
per group. Each static model 	(screen	EYEDIAP (screen target)
was trained end-to-end performing 4-fold 	(screen	EYEDIAP (screen target)
cross-validation using different combinations of 	(screen	EYEDIAP (screen target)
input modal- ities. Since the 	(screen	EYEDIAP (screen target)
number of fusion units depends 	(screen	EYEDIAP (screen target)
on the number of input 	(screen	EYEDIAP (screen target)
modalities, we also compare different 	(screen	EYEDIAP (screen target)
fusion layer sizes. The effect 	(screen	EYEDIAP (screen target)
of data normalization is also 	(screen	EYEDIAP (screen target)
evaluated by training a not-normalized 	(screen	EYEDIAP (screen target)
face model where the input 	(screen	EYEDIAP (screen target)
image is the face bounding 	(screen	EYEDIAP (screen target)
box with square size the 	(screen	EYEDIAP (screen target)
maximum distance between 2D landmarks.  Citation Citation {Funesprotect unhbox voidb@x	(screen	EYEDIAP (screen target)
 penalty @M	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Mora, Monay, and Odobez} 2014{}  Citation Citation {MSC	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
8 PALMERO ET AL.: MULTI-MODAL 	(screen	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	(screen	EYEDIAP (screen target)
ESTIMATION  0 1 2 3 4	(screen	EYEDIAP (screen target)
 5 6 7 8 9	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
10 11  An gl	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
e  er	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
ro r (  de gr	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
ee s)  6.9 6.43 5.58 5.71 5.59	(screen	EYEDIAP (screen target)
 5.55 5.52	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
OF-4096 NE-1536 NF-4096  NF-5632 NFL-4300	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
NFE-5632 NFEL-5836  Figure 3: Performance evaluation of	(screen	EYEDIAP (screen target)
 the Static network using different	(screen	EYEDIAP (screen target)
 input modali- ties (O	(screen	EYEDIAP (screen target)
 - Not normalized, N	(screen	EYEDIAP (screen target)
 - Normalized, F - Face	(screen	EYEDIAP (screen target)
, E - Eyes, 	(screen	EYEDIAP (screen target)
L - 3D Landmarks) and 	(screen	EYEDIAP (screen target)
size of fusion layers on 	(screen	EYEDIAP (screen target)
the FT scenario.  Floating Target Screen Target 0	(screen	EYEDIAP (screen target)
 1 2 3 4 5	(screen	EYEDIAP (screen target)
 6 7 8 9	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
10 11  An gl	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
e  er	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
ro r (  de gr	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
ee s)  6.36 5.43 5.19 4.2 3.38	(screen	EYEDIAP (screen target)
 3.4	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
MPIIGaze Static Temporal  Figure 4: Performance comparison among	(screen	EYEDIAP (screen target)
 MPIIGaze method [42] and our	(screen	EYEDIAP (screen target)
 Static and Temporal versions of	(screen	EYEDIAP (screen target)
 the proposed network for FT	(screen	EYEDIAP (screen target)
 and CS scenarios	(screen	EYEDIAP (screen target)
.  As shown in Figure 3	(screen	EYEDIAP (screen target)
, all models that take 	(screen	EYEDIAP (screen target)
normalized full-face information as input 	(screen	EYEDIAP (screen target)
achieve better performance than the 	(screen	EYEDIAP (screen target)
eyes-only model. More specifically, the 	(screen	EYEDIAP (screen target)
combination of face, eyes and 	(screen	EYEDIAP (screen target)
landmarks outperforms all the other 	(screen	EYEDIAP (screen target)
combinations by a small but 	(screen	EYEDIAP (screen target)
significant margin (paired Wilcoxon test, 	(screen	EYEDIAP (screen target)
p < 0.0001). The standard 	(screen	EYEDIAP (screen target)
deviation of the best-performing model 	(screen	EYEDIAP (screen target)
is reduced compared to the 	(screen	EYEDIAP (screen target)
face and eyes model, suggesting 	(screen	EYEDIAP (screen target)
a regularizing effect due to 	(screen	EYEDIAP (screen target)
the addition of landmarks. The 	(screen	EYEDIAP (screen target)
not-normalized face-only model shows the 	(screen	EYEDIAP (screen target)
largest error, proving the impact 	(screen	EYEDIAP (screen target)
of normalization to reduce the 	(screen	EYEDIAP (screen target)
appearance variability. Furthermore, our results 	(screen	EYEDIAP (screen target)
indicate that the increase of 	(screen	EYEDIAP (screen target)
fusion units is not correlated 	(screen	EYEDIAP (screen target)
with a better performance.  4.3 Static gaze regression: comparison	(screen	EYEDIAP (screen target)
 with existing methods	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
We compare our best-performing static 	(screen	EYEDIAP (screen target)
model with three baselines. Head: 	(screen	EYEDIAP (screen target)
Treating the head pose directly 	(screen	EYEDIAP (screen target)
as gaze direction. PR-ALR: Method 	(screen	EYEDIAP (screen target)
that relies on RGB-D data 	(screen	EYEDIAP (screen target)
to rectify the eye images 	(screen	EYEDIAP (screen target)
viewpoint into a canonical head 	(screen	EYEDIAP (screen target)
pose using a 3DMM. It 	(screen	EYEDIAP (screen target)
then learns an RGB gaze 	(screen	EYEDIAP (screen target)
appearance model using ALR [21]. 	(screen	EYEDIAP (screen target)
Predicted 3D vectors for FT-S 	(screen	EYEDIAP (screen target)
scenario are provided by EYEDIAP 	(screen	EYEDIAP (screen target)
dataset. MPIIGaze:. State-of-the-art full-face 3D 	(screen	EYEDIAP (screen target)
gaze estimation method [42]. They 	(screen	EYEDIAP (screen target)
use an Alexnet-based CNN model 	(screen	EYEDIAP (screen target)
with spatial weights to enhance 	(screen	EYEDIAP (screen target)
information in different facial regions. 	(screen	EYEDIAP (screen target)
We fine-tuned it with the 	(screen	EYEDIAP (screen target)
filtered EYEDIAP subsets using our 	(screen	EYEDIAP (screen target)
training parameters and normalization procedure.  In addition to the aforementioned	(screen	EYEDIAP (screen target)
 FT-based evaluation setup, we also	(screen	EYEDIAP (screen target)
 evaluate our method on the	(screen	EYEDIAP (screen target)
 CS scenario. In this case	(screen	EYEDIAP (screen target)
 there are only 14 participants	(screen	EYEDIAP (screen target)
 available, so we divided them	(screen	EYEDIAP (screen target)
 in 5 groups and performed	(screen	EYEDIAP (screen target)
 5-fold cross-validation. In Figure 4	(screen	EYEDIAP (screen target)
 we compare our method to	(screen	EYEDIAP (screen target)
 MPIIGaze, achieving a statistically significant	(screen	EYEDIAP (screen target)
 improvement of 14.6% and 19.5	(screen	EYEDIAP (screen target)
% on FT and CS 	(screen	EYEDIAP (screen target)
scenarios, respectively (paired Wilcoxon test, 	(screen	EYEDIAP (screen target)
p < 0.0001). We can 	(screen	EYEDIAP (screen target)
observe that a re- stricted 	(screen	EYEDIAP (screen target)
gaze target benefits the performance 	(screen	EYEDIAP (screen target)
of all methods, compared to 	(screen	EYEDIAP (screen target)
a more challenging unrestricted setting 	(screen	EYEDIAP (screen target)
with a wider range of 	(screen	EYEDIAP (screen target)
head poses and gaze directions.  Table 2 provides a detailed	(screen	EYEDIAP (screen target)
 comparison on every participant, performing	(screen	EYEDIAP (screen target)
 leave-one-out cross-validation on the FT	(screen	EYEDIAP (screen target)
 scenario for static and moving	(screen	EYEDIAP (screen target)
 head separately. Results show that	(screen	EYEDIAP (screen target)
, as expected, facial appearance 	(screen	EYEDIAP (screen target)
and head pose have a 	(screen	EYEDIAP (screen target)
noticeable impact on gaze accuracy, 	(screen	EYEDIAP (screen target)
with average error differences of 	(screen	EYEDIAP (screen target)
up to 7.7◦ among participants.  Citation Citation {Zhang, Sugano, Fritz	(screen	EYEDIAP (screen target)
, and Bulling} 2015  Citation Citation {Mora and Odobez	(screen	EYEDIAP (screen target)
} 2012  Citation Citation {Zhang, Sugano, Fritz	(screen	EYEDIAP (screen target)
, and Bulling} 2015	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	(screen	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 9	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Method 1 2 3 4 5 6 7 8 9 10	(screen	EYEDIAP (screen target)
 11 12 13 14 15	(screen	EYEDIAP (screen target)
 16 Avg. Head 23.5 22.1	(screen	EYEDIAP (screen target)
 20.3 23.6 23.2 23.2 23.6	(screen	EYEDIAP (screen target)
 21.2 26.7 23.6 23.1 24.4	(screen	EYEDIAP (screen target)
 23.3 24.0 24.5 22.8 23.3	(screen	EYEDIAP (screen target)
 PR-ALR 12.3 12.0 12.4 11.3	(screen	EYEDIAP (screen target)
 15.5 12.9 17.9 11.8 17.3	(screen	EYEDIAP (screen target)
 13.4 13.4 14.3 15.2 13.6	(screen	EYEDIAP (screen target)
 14.4 14.6 13.9 MPIIGaze 5.3	(screen	EYEDIAP (screen target)
 5.1 5.7 4.7 7.3 15.1	(screen	EYEDIAP (screen target)
 10.8 5.7 9.9 7.1 5.0	(screen	EYEDIAP (screen target)
 5.7 7.4 3.8 4.8 5.5	(screen	EYEDIAP (screen target)
 6.8 Static 3.9 4.1 4.2	(screen	EYEDIAP (screen target)
 3.9 6.0 6.4 7.2 3.6	(screen	EYEDIAP (screen target)
 7.1 5.0 5.7 6.7 3.9	(screen	EYEDIAP (screen target)
 4.7 5.1 4.2 5.1 Temporal	(screen	EYEDIAP (screen target)
 4.0 4.9 4.3 4.1 6.1	(screen	EYEDIAP (screen target)
 6.5 6.6 3.9 7.8 6.1	(screen	EYEDIAP (screen target)
 4.7 5.6 4.7 3.5 5.9	(screen	EYEDIAP (screen target)
 4.6 5.2 Head 19.3 14.2	(screen	EYEDIAP (screen target)
 16.4 19.9 16.8 21.9 16.1	(screen	EYEDIAP (screen target)
 24.2 20.3 19.9 18.8 22.3	(screen	EYEDIAP (screen target)
 18.1 14.9 16.2 19.3 18.7	(screen	EYEDIAP (screen target)
 MPIIGaze 7.6 6.2 5.7 8.7	(screen	EYEDIAP (screen target)
 10.1 12.0 12.2 6.1 8.3	(screen	EYEDIAP (screen target)
 5.9 6.1 6.2 7.4 4.7	(screen	EYEDIAP (screen target)
 4.4 6.0 7.3 Static 5.8	(screen	EYEDIAP (screen target)
 5.7 4.4 7.5 6.7 8.8	(screen	EYEDIAP (screen target)
 11.6 5.5 8.3 5.5 5.2	(screen	EYEDIAP (screen target)
 6.3 5.3 3.9 4.3 5.6	(screen	EYEDIAP (screen target)
 6.3 Temporal 6.1 5.6 4.5	(screen	EYEDIAP (screen target)
 7.5 6.4 8.2 12.0 5.0	(screen	EYEDIAP (screen target)
 7.5 5.4 5.0 5.8 6.6	(screen	EYEDIAP (screen target)
 4.0 4.5 5.8 6.2	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Table 2: Gaze angular error 	(screen	EYEDIAP (screen target)
comparison for static (top half) 	(screen	EYEDIAP (screen target)
and moving (bottom half) head 	(screen	EYEDIAP (screen target)
pose for each subject in 	(screen	EYEDIAP (screen target)
the FT scenario. Best results 	(screen	EYEDIAP (screen target)
in bold.  −80 −40 0 40 80−80	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
40  0	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
40  80	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
0  5	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
10  15	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
20  25	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
30  35	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
80 −40 0 40 80−80  −40	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
0  40	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
80  −10	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
8  −6	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
4  −2	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
0  2	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
4  6	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
8  10	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
80 −40 0 40 80−80  −40	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
0  40	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
80  0	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
5  10	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
15  20	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
25  30	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
35  −80 −40 0 40 80−80	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
40  0	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
40  80	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
10  −8	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
6  −4	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
2  0	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
2  4	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
6  8	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
10  (a) Gaze space (b) Head	(screen	EYEDIAP (screen target)
 orientation space	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Figure 5: Angular error distribution 	(screen	EYEDIAP (screen target)
across gaze (a) and head 	(screen	EYEDIAP (screen target)
orientation (b) spaces in the 	(screen	EYEDIAP (screen target)
FT setting, in terms of 	(screen	EYEDIAP (screen target)
x- and y- angles. For 	(screen	EYEDIAP (screen target)
each space, we depict the 	(screen	EYEDIAP (screen target)
Static model performance (left) and 	(screen	EYEDIAP (screen target)
the contribution of the Temporal 	(screen	EYEDIAP (screen target)
model versus Static (right). In 	(screen	EYEDIAP (screen target)
the latter, positive difference means 	(screen	EYEDIAP (screen target)
higher improvement of the Temporal 	(screen	EYEDIAP (screen target)
model.  4.4 Evaluation of the temporal	(screen	EYEDIAP (screen target)
 network	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
In this section, we evaluate 	(screen	EYEDIAP (screen target)
the contribution of adding the 	(screen	EYEDIAP (screen target)
temporal module to the static 	(screen	EYEDIAP (screen target)
model. To do so, we 	(screen	EYEDIAP (screen target)
trained a lower-dimensional version of 	(screen	EYEDIAP (screen target)
the static network with compa- 	(screen	EYEDIAP (screen target)
rable performance to the original, 	(screen	EYEDIAP (screen target)
reducing the number of units 	(screen	EYEDIAP (screen target)
of the second fusion layer 	(screen	EYEDIAP (screen target)
to 2918. Results are reported 	(screen	EYEDIAP (screen target)
in Figure 4 and Table 2	(screen	EYEDIAP (screen target)
. One can observe that 	(screen	EYEDIAP (screen target)
using sequential information is helpful 	(screen	EYEDIAP (screen target)
on the FT scenario, outperforming 	(screen	EYEDIAP (screen target)
the static model by a 	(screen	EYEDIAP (screen target)
statistically significant 4.4% (paired Wilcoxon 	(screen	EYEDIAP (screen target)
test, p < 0.0001). This 	(screen	EYEDIAP (screen target)
contribution is more noticeable in 	(screen	EYEDIAP (screen target)
the moving head setting, proving 	(screen	EYEDIAP (screen target)
that the temporal model can 	(screen	EYEDIAP (screen target)
benefit from head motion information. 	(screen	EYEDIAP (screen target)
In contrast, such information seems 	(screen	EYEDIAP (screen target)
to be less meaningful in 	(screen	EYEDIAP (screen target)
the CS scenario, where the 	(screen	EYEDIAP (screen target)
obtained error is already very 	(screen	EYEDIAP (screen target)
low for a cross-subject setting 	(screen	EYEDIAP (screen target)
and the amount of head 	(screen	EYEDIAP (screen target)
movement declines.  Figure 5 further explores the	(screen	EYEDIAP (screen target)
 error distribution of the static	(screen	EYEDIAP (screen target)
 network and the impact of	(screen	EYEDIAP (screen target)
 sequential information. We can observe	(screen	EYEDIAP (screen target)
 that the accuracy of the	(screen	EYEDIAP (screen target)
 static model drops with extreme	(screen	EYEDIAP (screen target)
 head poses and gaze directions	(screen	EYEDIAP (screen target)
, which can also be 	(screen	EYEDIAP (screen target)
correlated to having less data 	(screen	EYEDIAP (screen target)
in those areas. Compared to 	(screen	EYEDIAP (screen target)
the static model, the temporal 	(screen	EYEDIAP (screen target)
model particularly benefits gaze targets 	(screen	EYEDIAP (screen target)
from mid-range upwards. Its contribution 	(screen	EYEDIAP (screen target)
is less clear for extreme 	(screen	EYEDIAP (screen target)
targets, probably again due to 	(screen	EYEDIAP (screen target)
data imbalance.  Finally, we evaluated the effect	(screen	EYEDIAP (screen target)
 of different recurrent architectures for	(screen	EYEDIAP (screen target)
 the temporal model. In particular	(screen	EYEDIAP (screen target)
, we tested 1 (128 	(screen	EYEDIAP (screen target)
units) and 2 (256-128 units) 	(screen	EYEDIAP (screen target)
LSTM and GRU lay- ers, 	(screen	EYEDIAP (screen target)
with 1 GRU layer obtaining 	(screen	EYEDIAP (screen target)
slightly superior results (up to 0	(screen	EYEDIAP (screen target)
.12◦). We also assessed the 	(screen	EYEDIAP (screen target)
effect of sequence length fixing 	(screen	EYEDIAP (screen target)
s in the range {4,7,10}, 	(screen	EYEDIAP (screen target)
with s = 7 performing 	(screen	EYEDIAP (screen target)
worse than the other two (	(screen	EYEDIAP (screen target)
up to 0	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
14	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
10 PALMERO ET AL.: MULTI-MODAL 	(screen	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	(screen	EYEDIAP (screen target)
ESTIMATION  5 Conclusions In this work	(screen	EYEDIAP (screen target)
, we studied the combination 	(screen	EYEDIAP (screen target)
of full-face and eye images 	(screen	EYEDIAP (screen target)
along with facial land- marks 	(screen	EYEDIAP (screen target)
for person- and head pose-independent 	(screen	EYEDIAP (screen target)
3D gaze estimation. Consequently, we 	(screen	EYEDIAP (screen target)
pro- posed a multi-stream recurrent 	(screen	EYEDIAP (screen target)
CNN network that leverages the 	(screen	EYEDIAP (screen target)
sequential information of eye and 	(screen	EYEDIAP (screen target)
head movements. Both static and 	(screen	EYEDIAP (screen target)
temporal versions of our approach 	(screen	EYEDIAP (screen target)
significantly outperform current state-of-the-art 3D 	(screen	EYEDIAP (screen target)
gaze estimation methods on a 	(screen	EYEDIAP (screen target)
wide range of head poses 	(screen	EYEDIAP (screen target)
and gaze directions. We showed 	(screen	EYEDIAP (screen target)
that adding geometry features to 	(screen	EYEDIAP (screen target)
appearance-based methods has a regularizing 	(screen	EYEDIAP (screen target)
effect on the accuracy. Adding 	(screen	EYEDIAP (screen target)
sequential information further benefits the 	(screen	EYEDIAP (screen target)
final performance compared to static-only 	(screen	EYEDIAP (screen target)
input, especially from mid-range up- 	(screen	EYEDIAP (screen target)
wards and in those cases 	(screen	EYEDIAP (screen target)
where head motion is present. 	(screen	EYEDIAP (screen target)
The effect in very extreme 	(screen	EYEDIAP (screen target)
head poses is not clear 	(screen	EYEDIAP (screen target)
due to data imbalance, suggesting 	(screen	EYEDIAP (screen target)
the importance of learning from 	(screen	EYEDIAP (screen target)
a con- tinuous, balanced dataset 	(screen	EYEDIAP (screen target)
including all head poses and 	(screen	EYEDIAP (screen target)
gaze directions of interest. To 	(screen	EYEDIAP (screen target)
the best of our knowledge, 	(screen	EYEDIAP (screen target)
this is the first attempt 	(screen	EYEDIAP (screen target)
to exploit the temporal modality 	(screen	EYEDIAP (screen target)
in the context of gaze 	(screen	EYEDIAP (screen target)
estimation from remote cameras. As 	(screen	EYEDIAP (screen target)
future work, we will further 	(screen	EYEDIAP (screen target)
explore extracting meaningful temporal representations 	(screen	EYEDIAP (screen target)
of gaze dynamics, considering 3DCNNs 	(screen	EYEDIAP (screen target)
as well as the encoding 	(screen	EYEDIAP (screen target)
of deep features around particular 	(screen	EYEDIAP (screen target)
tracked face landmarks [14].  Acknowledgements This work has been	(screen	EYEDIAP (screen target)
 partially supported by the Spanish	(screen	EYEDIAP (screen target)
 project TIN2016-74946-P (MINECO/ FEDER, UE	(screen	EYEDIAP (screen target)
), CERCA Programme / Generalitat 	(screen	EYEDIAP (screen target)
de Catalunya, and the FP7 	(screen	EYEDIAP (screen target)
people program (Marie Curie Actions), 	(screen	EYEDIAP (screen target)
REA grant agreement no FP7-607139 (	(screen	EYEDIAP (screen target)
iCARE - Improving Children Auditory 	(screen	EYEDIAP (screen target)
REhabilitation). We gratefully acknowledge the 	(screen	EYEDIAP (screen target)
support of NVIDIA Corporation with 	(screen	EYEDIAP (screen target)
the donation of the GPU 	(screen	EYEDIAP (screen target)
used for this research. Portions 	(screen	EYEDIAP (screen target)
of the research in this 	(screen	EYEDIAP (screen target)
pa- per used the EYEDIAP 	(screen	EYEDIAP (screen target)
dataset made available by the 	(screen	EYEDIAP (screen target)
Idiap Research Institute, Martigny, Switzerland.  References [1] Nicola C Anderson	(screen	EYEDIAP (screen target)
, Evan F Risko, and 	(screen	EYEDIAP (screen target)
Alan Kingstone. Motion influences gaze 	(screen	EYEDIAP (screen target)
di-  rection discrimination and disambiguates contradictory	(screen	EYEDIAP (screen target)
 luminance cues. Psychonomic bulletin	(screen	EYEDIAP (screen target)
 & review, 23(3):817–823, 2016	(screen	EYEDIAP (screen target)
.  [2] Shumeet Baluja and Dean	(screen	EYEDIAP (screen target)
 Pomerleau. Non-intrusive gaze tracking using	(screen	EYEDIAP (screen target)
 artificial neu- ral networks. In	(screen	EYEDIAP (screen target)
 Advances in Neural Information Processing	(screen	EYEDIAP (screen target)
 Systems, pages 753–760, 1994	(screen	EYEDIAP (screen target)
.  [3] Adrian Bulat and Georgios	(screen	EYEDIAP (screen target)
 Tzimiropoulos. How far are we	(screen	EYEDIAP (screen target)
 from solving the 2d	(screen	EYEDIAP (screen target)
 & 3d face alignment problem	(screen	EYEDIAP (screen target)
? (and a dataset of 230,	(screen	EYEDIAP (screen target)
000 3d facial landmarks). In 	(screen	EYEDIAP (screen target)
Interna- tional Conference on Computer 	(screen	EYEDIAP (screen target)
Vision, 2017.  [4] Haoping Deng and Wangjiang	(screen	EYEDIAP (screen target)
 Zhu. Monocular free-head 3d gaze	(screen	EYEDIAP (screen target)
 tracking with deep learning and	(screen	EYEDIAP (screen target)
 geometry constraints. In Computer Vision	(screen	EYEDIAP (screen target)
 (ICCV), 2017 IEEE Interna- tional	(screen	EYEDIAP (screen target)
 Conference on, pages 3162–3171. IEEE	(screen	EYEDIAP (screen target)
, 2017.  [5] Onur Ferhat and Fernando	(screen	EYEDIAP (screen target)
 Vilariño. Low cost eye tracking	(screen	EYEDIAP (screen target)
. Computational intelligence and neuroscience, 2016	(screen	EYEDIAP (screen target)
:17, 2016.  Citation Citation {Jung, Lee, Yim	(screen	EYEDIAP (screen target)
, Park, and Kim} 2015	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	(screen	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 11	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
6] Kenneth A Funes-Mora and 	(screen	EYEDIAP (screen target)
Jean-Marc Odobez. Gaze estimation in 	(screen	EYEDIAP (screen target)
the 3D space using RGB-D 	(screen	EYEDIAP (screen target)
sensors. International Journal of Computer 	(screen	EYEDIAP (screen target)
Vision, 118(2):194–216, 2016.  [7] Kenneth Alberto Funes Mora	(screen	EYEDIAP (screen target)
, Florent Monay, and Jean-Marc 	(screen	EYEDIAP (screen target)
Odobez. Eyediap: A database for 	(screen	EYEDIAP (screen target)
the development and evaluation of 	(screen	EYEDIAP (screen target)
gaze estimation algorithms from rgb 	(screen	EYEDIAP (screen target)
and rgb-d cameras. In Proceedings 	(screen	EYEDIAP (screen target)
of the ACM Symposium on 	(screen	EYEDIAP (screen target)
Eye Tracking Research and Applications. 	(screen	EYEDIAP (screen target)
ACM, March 2014. doi: 10.1145/2578153.2578190.  [8] Kenneth Alberto Funes Mora	(screen	EYEDIAP (screen target)
, Florent Monay, and Jean-Marc 	(screen	EYEDIAP (screen target)
Odobez. Eyediap: A database for 	(screen	EYEDIAP (screen target)
the development and evaluation of 	(screen	EYEDIAP (screen target)
gaze estimation algorithms from rgb 	(screen	EYEDIAP (screen target)
and rgb-d cameras. In Proceedings 	(screen	EYEDIAP (screen target)
of the Symposium on Eye 	(screen	EYEDIAP (screen target)
Tracking Research and Applications, pages 255	(screen	EYEDIAP (screen target)
–258. ACM, 2014.  [9] Quentin Guillon, Nouchine Hadjikhani	(screen	EYEDIAP (screen target)
, Sophie Baduel, and Bernadette 	(screen	EYEDIAP (screen target)
Rogé. Visual social attention in 	(screen	EYEDIAP (screen target)
autism spectrum disorder: Insights from 	(screen	EYEDIAP (screen target)
eye tracking studies. Neu- 	(screen	EYEDIAP (screen target)
roscience & Biobehavioral Reviews, 42:279–297, 2014	(screen	EYEDIAP (screen target)
.  [10] Dan Witzner Hansen and	(screen	EYEDIAP (screen target)
 Qiang Ji. In the eye	(screen	EYEDIAP (screen target)
 of the beholder: A survey	(screen	EYEDIAP (screen target)
 of models for eyes and	(screen	EYEDIAP (screen target)
 gaze. IEEE transactions on pattern	(screen	EYEDIAP (screen target)
 analysis and machine intelligence, 32(3	(screen	EYEDIAP (screen target)
): 478–500, 2010.  [11] Qiong Huang, Ashok Veeraraghavan	(screen	EYEDIAP (screen target)
, and Ashutosh Sabharwal. Tabletgaze: 	(screen	EYEDIAP (screen target)
dataset and analysis for unconstrained 	(screen	EYEDIAP (screen target)
appearance-based gaze estimation in mobile 	(screen	EYEDIAP (screen target)
tablets. Machine Vision and Applications, 28	(screen	EYEDIAP (screen target)
(5-6):445–461, 2017.  [12] Robert JK Jacob and	(screen	EYEDIAP (screen target)
 Keith S Karn. Eye tracking	(screen	EYEDIAP (screen target)
 in human-computer interaction and usability	(screen	EYEDIAP (screen target)
 research: Ready to deliver the	(screen	EYEDIAP (screen target)
 promises. In The mind’s eye	(screen	EYEDIAP (screen target)
, pages 573–605. Elsevier, 2003.  [13] László A Jeni and	(screen	EYEDIAP (screen target)
 Jeffrey F Cohn. Person-independent 3d	(screen	EYEDIAP (screen target)
 gaze estimation using face frontalization	(screen	EYEDIAP (screen target)
. In Proceedings of the 	(screen	EYEDIAP (screen target)
IEEE Conference on Computer Vision 	(screen	EYEDIAP (screen target)
and Pattern Recognition Workshops, pages 87	(screen	EYEDIAP (screen target)
–95, 2016.  [14] Heechul Jung, Sihaeng Lee	(screen	EYEDIAP (screen target)
, Junho Yim, Sunjeong Park, 	(screen	EYEDIAP (screen target)
and Junmo Kim. Joint fine- 	(screen	EYEDIAP (screen target)
tuning in deep neural networks 	(screen	EYEDIAP (screen target)
for facial expression recognition. In 	(screen	EYEDIAP (screen target)
Computer Vision (ICCV), 2015 IEEE 	(screen	EYEDIAP (screen target)
International Conference on, pages 2983–2991. 	(screen	EYEDIAP (screen target)
IEEE, 2015.  [15] Anuradha Kar and Peter	(screen	EYEDIAP (screen target)
 Corcoran. A review and analysis	(screen	EYEDIAP (screen target)
 of eye-gaze estimation sys- tems	(screen	EYEDIAP (screen target)
, algorithms and performance evaluation 	(screen	EYEDIAP (screen target)
methods in consumer platforms. IEEE 	(screen	EYEDIAP (screen target)
Access, 5:16495–16519, 2017.  [16] Kyle Krafka, Aditya Khosla	(screen	EYEDIAP (screen target)
, Petr Kellnhofer, Harini Kannan, 	(screen	EYEDIAP (screen target)
Suchendra Bhandarkar, Wojciech Matusik, and 	(screen	EYEDIAP (screen target)
Antonio Torralba. Eye tracking for 	(screen	EYEDIAP (screen target)
everyone. In Computer Vision and 	(screen	EYEDIAP (screen target)
Pattern Recognition (CVPR), 2016 IEEE 	(screen	EYEDIAP (screen target)
Conference on, pages 2176–2184. IEEE, 2016	(screen	EYEDIAP (screen target)
.  [17] Simon P Liversedge and	(screen	EYEDIAP (screen target)
 John M Findlay. Saccadic eye	(screen	EYEDIAP (screen target)
 movements and cognition. Trends in	(screen	EYEDIAP (screen target)
 cognitive sciences, 4(1):6–14, 2000	(screen	EYEDIAP (screen target)
.  [18] Feng Lu, Takahiro Okabe	(screen	EYEDIAP (screen target)
, Yusuke Sugano, and Yoichi 	(screen	EYEDIAP (screen target)
Sato. A head pose-free approach 	(screen	EYEDIAP (screen target)
for appearance-based gaze estimation. In 	(screen	EYEDIAP (screen target)
BMVC, pages 1–11, 2011	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
12 PALMERO ET AL.: MULTI-MODAL 	(screen	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	(screen	EYEDIAP (screen target)
ESTIMATION  [19] Feng Lu, Yusuke Sugano	(screen	EYEDIAP (screen target)
, Takahiro Okabe, and Yoichi 	(screen	EYEDIAP (screen target)
Sato. Inferring human gaze from 	(screen	EYEDIAP (screen target)
appearance via adaptive linear regression. 	(screen	EYEDIAP (screen target)
In Computer Vision (ICCV), 2011 	(screen	EYEDIAP (screen target)
IEEE International Conference on, pages 153	(screen	EYEDIAP (screen target)
–160. IEEE, 2011.  [20] Päivi Majaranta and Andreas	(screen	EYEDIAP (screen target)
 Bulling. Eye tracking and eye-based	(screen	EYEDIAP (screen target)
 human–computer interaction. In Advances in	(screen	EYEDIAP (screen target)
 physiological computing, pages 39–65. Springer	(screen	EYEDIAP (screen target)
, 2014.  [21] Kenneth Alberto Funes Mora	(screen	EYEDIAP (screen target)
 and Jean-Marc Odobez. Gaze estimation	(screen	EYEDIAP (screen target)
 from multi- modal kinect data	(screen	EYEDIAP (screen target)
. In Computer Vision and 	(screen	EYEDIAP (screen target)
Pattern Recognition Workshops (CVPRW), 2012 	(screen	EYEDIAP (screen target)
IEEE Computer Society Conference on, 	(screen	EYEDIAP (screen target)
pages 25–30. IEEE, 2012.  [22] Carlos Hitoshi Morimoto, Arnon	(screen	EYEDIAP (screen target)
 Amir, and Myron Flickner. Detecting	(screen	EYEDIAP (screen target)
 eye position and gaze from	(screen	EYEDIAP (screen target)
 a single camera and 2	(screen	EYEDIAP (screen target)
 light sources. In Pattern Recognition	(screen	EYEDIAP (screen target)
, 2002. Proceedings. 16th International 	(screen	EYEDIAP (screen target)
Conference on, volume 4, pages 314	(screen	EYEDIAP (screen target)
–317. IEEE, 2002.  [23] IMO MSC. Circ. 982	(screen	EYEDIAP (screen target)
 (2000) guidelines on ergonomic criteria	(screen	EYEDIAP (screen target)
 for bridge equipment and layout	(screen	EYEDIAP (screen target)
.  [24] Alejandro Newell, Kaiyu Yang	(screen	EYEDIAP (screen target)
, and Jia Deng. Stacked 	(screen	EYEDIAP (screen target)
hourglass networks for hu- man 	(screen	EYEDIAP (screen target)
pose estimation. In European Conference 	(screen	EYEDIAP (screen target)
on Computer Vision, pages 483–499. 	(screen	EYEDIAP (screen target)
Springer, 2016.  [25] Yasuhiro Ono, Takahiro Okabe	(screen	EYEDIAP (screen target)
, and Yoichi Sato. Gaze 	(screen	EYEDIAP (screen target)
estimation from low resolution images. 	(screen	EYEDIAP (screen target)
In Pacific-Rim Symposium on Image 	(screen	EYEDIAP (screen target)
and Video Technology, pages 178–188. 	(screen	EYEDIAP (screen target)
Springer, 2006.  [26] Cristina Palmero, Elisabeth A	(screen	EYEDIAP (screen target)
. van Dam, Sergio Escalera, 	(screen	EYEDIAP (screen target)
Mike Kelia, Guido F. Lichtert, 	(screen	EYEDIAP (screen target)
Lucas P.J.J Noldus, Andrew J. 	(screen	EYEDIAP (screen target)
Spink, and Astrid van Wieringen. 	(screen	EYEDIAP (screen target)
Automatic mutual gaze detection in 	(screen	EYEDIAP (screen target)
face-to-face dyadic interaction videos. In 	(screen	EYEDIAP (screen target)
Proceedings of Measuring Behavior, pages 158	(screen	EYEDIAP (screen target)
–163, 2018.  [27] Omkar M. Parkhi, Andrea	(screen	EYEDIAP (screen target)
 Vedaldi, and Andrew Zisserman. Deep	(screen	EYEDIAP (screen target)
 face recognition. In British Machine	(screen	EYEDIAP (screen target)
 Vision Conference, 2015	(screen	EYEDIAP (screen target)
.  [28] Derek R Rutter and	(screen	EYEDIAP (screen target)
 Kevin Durkin. Turn-taking in mother–infant	(screen	EYEDIAP (screen target)
 interaction: An exam- ination of	(screen	EYEDIAP (screen target)
 vocalizations and gaze. Developmental psychology	(screen	EYEDIAP (screen target)
, 23(1):54, 1987.  [29] Brian A Smith, Qi	(screen	EYEDIAP (screen target)
 Yin, Steven K Feiner, and	(screen	EYEDIAP (screen target)
 Shree K Nayar. Gaze locking	(screen	EYEDIAP (screen target)
: passive eye contact detection 	(screen	EYEDIAP (screen target)
for human-object interaction. In Proceedings 	(screen	EYEDIAP (screen target)
of the 26th annual ACM 	(screen	EYEDIAP (screen target)
symposium on User interface software 	(screen	EYEDIAP (screen target)
and technology, pages 271–280. ACM, 2013	(screen	EYEDIAP (screen target)
.  [30] Yusuke Sugano, Yasuyuki Matsushita	(screen	EYEDIAP (screen target)
, and Yoichi Sato. Appearance-based 	(screen	EYEDIAP (screen target)
gaze es- timation using visual 	(screen	EYEDIAP (screen target)
saliency. IEEE transactions on pattern 	(screen	EYEDIAP (screen target)
analysis and machine intelligence, 35(2):329–341, 2013	(screen	EYEDIAP (screen target)
.  [31] Yusuke Sugano, Yasuyuki Matsushita	(screen	EYEDIAP (screen target)
, and Yoichi Sato. Learning-by-synthesis 	(screen	EYEDIAP (screen target)
for appearance-based 3d gaze estimation. 	(screen	EYEDIAP (screen target)
In Computer Vision and Pattern 	(screen	EYEDIAP (screen target)
Recognition (CVPR), 2014 IEEE Conference 	(screen	EYEDIAP (screen target)
on, pages 1821–1828. IEEE, 2014.  [32] Kar-Han Tan, David J	(screen	EYEDIAP (screen target)
 Kriegman, and Narendra Ahuja. Appearance-based	(screen	EYEDIAP (screen target)
 eye gaze es- timation. In	(screen	EYEDIAP (screen target)
 Applications of Computer Vision, 2002.(WACV	(screen	EYEDIAP (screen target)
 2002). Proceedings. Sixth IEEE Workshop	(screen	EYEDIAP (screen target)
 on, pages 191–195. IEEE, 2002	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	(screen	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 13	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
33] Ronda Venkateswarlu et al. 	(screen	EYEDIAP (screen target)
Eye gaze estimation from a 	(screen	EYEDIAP (screen target)
single image of one eye. 	(screen	EYEDIAP (screen target)
In Computer Vision, 2003. Proceedings. 	(screen	EYEDIAP (screen target)
Ninth IEEE International Conference on, 	(screen	EYEDIAP (screen target)
pages 136–143. IEEE, 2003.  [34] Kang Wang and Qiang	(screen	EYEDIAP (screen target)
 Ji. Real time eye gaze	(screen	EYEDIAP (screen target)
 tracking with 3d deformable eye-face	(screen	EYEDIAP (screen target)
 model. In Proceedings of the	(screen	EYEDIAP (screen target)
 IEEE Conference on Computer Vision	(screen	EYEDIAP (screen target)
 and Pattern Recog- nition, pages	(screen	EYEDIAP (screen target)
 1003–1011, 2017	(screen	EYEDIAP (screen target)
.  [35] Oliver Williams, Andrew Blake	(screen	EYEDIAP (screen target)
, and Roberto Cipolla. Sparse 	(screen	EYEDIAP (screen target)
and semi-supervised visual mapping with 	(screen	EYEDIAP (screen target)
the sˆ 3gp. In Computer 	(screen	EYEDIAP (screen target)
Vision and Pattern Recognition, 2006 	(screen	EYEDIAP (screen target)
IEEE Computer Society Conference on, 	(screen	EYEDIAP (screen target)
volume 1, pages 230–237. IEEE, 2006	(screen	EYEDIAP (screen target)
.  [36] William Hyde Wollaston et	(screen	EYEDIAP (screen target)
 al. Xiii. on the apparent	(screen	EYEDIAP (screen target)
 direction of eyes in a	(screen	EYEDIAP (screen target)
 portrait. Philosophical Transactions of the	(screen	EYEDIAP (screen target)
 Royal Society of London, 114:247–256	(screen	EYEDIAP (screen target)
, 1824.  [37] Erroll Wood and Andreas	(screen	EYEDIAP (screen target)
 Bulling. Eyetab: Model-based gaze estimation	(screen	EYEDIAP (screen target)
 on unmodi- fied tablet computers	(screen	EYEDIAP (screen target)
. In Proceedings of the 	(screen	EYEDIAP (screen target)
Symposium on Eye Tracking Research 	(screen	EYEDIAP (screen target)
and Applications, pages 207–210. ACM, 2014	(screen	EYEDIAP (screen target)
.  [38] Erroll Wood, Tadas Baltrusaitis	(screen	EYEDIAP (screen target)
, Xucong Zhang, Yusuke Sugano, 	(screen	EYEDIAP (screen target)
Peter Robinson, and Andreas Bulling. 	(screen	EYEDIAP (screen target)
Rendering of eyes for eye-shape 	(screen	EYEDIAP (screen target)
registration and gaze estimation. In 	(screen	EYEDIAP (screen target)
Proceedings of the IEEE International 	(screen	EYEDIAP (screen target)
Conference on Computer Vision, pages 3756	(screen	EYEDIAP (screen target)
– 3764, 2015.  [39] Erroll Wood, Tadas Baltrušaitis	(screen	EYEDIAP (screen target)
, Louis-Philippe Morency, Peter Robinson, 	(screen	EYEDIAP (screen target)
and Andreas Bulling. A 3d 	(screen	EYEDIAP (screen target)
morphable eye region model for 	(screen	EYEDIAP (screen target)
gaze estimation. In European Confer- 	(screen	EYEDIAP (screen target)
ence on Computer Vision, pages 297	(screen	EYEDIAP (screen target)
–313. Springer, 2016.  [40] Erroll Wood, Tadas Baltrušaitis	(screen	EYEDIAP (screen target)
, Louis-Philippe Morency, Peter Robinson, 	(screen	EYEDIAP (screen target)
and Andreas Bulling. Learning an 	(screen	EYEDIAP (screen target)
appearance-based gaze estimator from one 	(screen	EYEDIAP (screen target)
million synthesised images. In Proceedings 	(screen	EYEDIAP (screen target)
of the Ninth Biennial ACM 	(screen	EYEDIAP (screen target)
Symposium on Eye Tracking Re- 	(screen	EYEDIAP (screen target)
search & Applications, pages 131–138. 	(screen	EYEDIAP (screen target)
ACM, 2016.  [41] Dong Hyun Yoo and	(screen	EYEDIAP (screen target)
 Myung Jin Chung. A novel	(screen	EYEDIAP (screen target)
 non-intrusive eye gaze estimation using	(screen	EYEDIAP (screen target)
 cross-ratio under large head motion	(screen	EYEDIAP (screen target)
. Computer Vision and Image 	(screen	EYEDIAP (screen target)
Understanding, 98(1):25–51, 2005.  [42] Xucong Zhang, Yusuke Sugano	(screen	EYEDIAP (screen target)
, Mario Fritz, and Andreas 	(screen	EYEDIAP (screen target)
Bulling. Appearance-based gaze estimation in 	(screen	EYEDIAP (screen target)
the wild. In Proceedings of 	(screen	EYEDIAP (screen target)
the IEEE Conference on Computer 	(screen	EYEDIAP (screen target)
Vision and Pattern Recognition, pages 4511	(screen	EYEDIAP (screen target)
–4520, 2015.  [43] Xucong Zhang, Yusuke Sugano	(screen	EYEDIAP (screen target)
, Mario Fritz, and Andreas 	(screen	EYEDIAP (screen target)
Bulling. It’s written all over 	(screen	EYEDIAP (screen target)
your face: Full-face appearance-based gaze 	(screen	EYEDIAP (screen target)
estimation. In Proc. IEEE International 	(screen	EYEDIAP (screen target)
Conference on Computer Vision and 	(screen	EYEDIAP (screen target)
Pattern Recognition Workshops (CVPRW), 2017	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
state of the art on EYEDIAP dataset, further improved by 4	EYEDIAP	EYEDIAP (screen target)
of our solution on the EYEDIAP dataset [7] in a wide	EYEDIAP	EYEDIAP (screen target)
VGA videos from the publicly-available EYEDIAP dataset [7] to perform the	EYEDIAP	EYEDIAP (screen target)
FT-S scenario are provided by EYEDIAP dataset. MPIIGaze:. State-of-the-art full-face 3D	EYEDIAP	EYEDIAP (screen target)
fine-tuned it with the filtered EYEDIAP subsets using our training parameters	EYEDIAP	EYEDIAP (screen target)
this pa- per used the EYEDIAP dataset made available by the	EYEDIAP	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 1		EYEDIAP (screen target)
		EYEDIAP (screen target)
Recurrent CNN for 3D Gaze 		EYEDIAP (screen target)
Estimation using Appearance and Shape 		EYEDIAP (screen target)
Cues  Cristina Palmero1,2		EYEDIAP (screen target)
		EYEDIAP (screen target)
crpalmec7@alumnes.ub.edu  1 Dept. Mathematics and Informatics		EYEDIAP (screen target)
 Universitat de Barcelona, Spain		EYEDIAP (screen target)
		EYEDIAP (screen target)
Javier Selva1  javier.selva.castello@est.fib.upc.edu		EYEDIAP (screen target)
		EYEDIAP (screen target)
2 Computer Vision Center Campus 		EYEDIAP (screen target)
UAB, Bellaterra, Spain  Mohammad Ali Bagheri3,4		EYEDIAP (screen target)
		EYEDIAP (screen target)
mohammadali.bagheri@ucalgary.ca  3 Dept. Electrical and Computer		EYEDIAP (screen target)
 Eng. University of Calgary, Canada		EYEDIAP (screen target)
		EYEDIAP (screen target)
Sergio Escalera1,2  sergio@maia.ub.es		EYEDIAP (screen target)
		EYEDIAP (screen target)
4 Dept. Engineering University of 		EYEDIAP (screen target)
Larestan, Iran  Abstract		EYEDIAP (screen target)
		EYEDIAP (screen target)
Gaze behavior is an important 		EYEDIAP (screen target)
non-verbal cue in social signal 		EYEDIAP (screen target)
processing and human- computer interaction. 		EYEDIAP (screen target)
In this paper, we tackle 		EYEDIAP (screen target)
the problem of person- and 		EYEDIAP (screen target)
head pose- independent 3D gaze 		EYEDIAP (screen target)
estimation from remote cameras, using 		EYEDIAP (screen target)
a multi-modal recurrent convolutional neural 		EYEDIAP (screen target)
network (CNN). We propose to 		EYEDIAP (screen target)
combine face, eyes region, and 		EYEDIAP (screen target)
face landmarks as individual streams 		EYEDIAP (screen target)
in a CNN to estimate 		EYEDIAP (screen target)
gaze in still images. Then, 		EYEDIAP (screen target)
we exploit the dynamic nature 		EYEDIAP (screen target)
of gaze by feeding the 		EYEDIAP (screen target)
learned features of all the 		EYEDIAP (screen target)
frames in a sequence to 		EYEDIAP (screen target)
a many-to-one recurrent module that 		EYEDIAP (screen target)
predicts the 3D gaze vector 		EYEDIAP (screen target)
of the last frame. Our 		EYEDIAP (screen target)
multi-modal static solution is evaluated 		EYEDIAP (screen target)
on a wide range of 		EYEDIAP (screen target)
head poses and gaze directions, 		EYEDIAP (screen target)
achieving a significant improvement of 14		EYEDIAP (screen target)
.6% over the state of 		EYEDIAP (screen target)
the art on EYEDIAP dataset, 		EYEDIAP (screen target)
further improved by 4% when 		EYEDIAP (screen target)
the temporal modality is included.  1 Introduction Eyes and their		EYEDIAP (screen target)
 movements are considered an important		EYEDIAP (screen target)
 cue in non-verbal behavior analysis		EYEDIAP (screen target)
, being involved in many 		EYEDIAP (screen target)
cognitive processes and reflecting our 		EYEDIAP (screen target)
internal state [17]. More specifically, 		EYEDIAP (screen target)
eye gaze behavior, as an 		EYEDIAP (screen target)
indicator of human visual attention, 		EYEDIAP (screen target)
has been widely studied to 		EYEDIAP (screen target)
assess communication skills [28] and 		EYEDIAP (screen target)
to identify possible behavioral 		EYEDIAP (screen target)
disorders [9]. Therefore, gaze estimation 		EYEDIAP (screen target)
has become an established line 		EYEDIAP (screen target)
of research in computer vision, 		EYEDIAP (screen target)
being a key feature in 		EYEDIAP (screen target)
human-computer interaction (HCI) and usability 		EYEDIAP (screen target)
research [12, 20].  Recent gaze estimation research has		EYEDIAP (screen target)
 focused on facilitating its use		EYEDIAP (screen target)
 in general everyday applications under		EYEDIAP (screen target)
 real-world conditions, using off-the-shelf remote		EYEDIAP (screen target)
 RGB cameras and re- moving		EYEDIAP (screen target)
 the need of personal calibration		EYEDIAP (screen target)
 [26]. In this setting, appearance-based		EYEDIAP (screen target)
 methods, which learn a mapping		EYEDIAP (screen target)
 from images to gaze directions		EYEDIAP (screen target)
, are the preferred 		EYEDIAP (screen target)
choice [25]. How- ever, they 		EYEDIAP (screen target)
need large amounts of training 		EYEDIAP (screen target)
data to be able to 		EYEDIAP (screen target)
generalize well to in-the-wild situations, 		EYEDIAP (screen target)
which are characterized by significant 		EYEDIAP (screen target)
variability in head poses, face 		EYEDIAP (screen target)
appearances and lighting conditions. In 		EYEDIAP (screen target)
recent years, CNNs have been 		EYEDIAP (screen target)
reported to outperform classical methods. 		EYEDIAP (screen target)
However, most existing approaches have 		EYEDIAP (screen target)
only been tested in restricted 		EYEDIAP (screen target)
HCI tasks,  c© 2018. The copyright of		EYEDIAP (screen target)
 this document resides with its		EYEDIAP (screen target)
 authors. It may be distributed		EYEDIAP (screen target)
 unchanged freely in print or		EYEDIAP (screen target)
 electronic forms		EYEDIAP (screen target)
.  ar X		EYEDIAP (screen target)
		EYEDIAP (screen target)
iv :1  80 5		EYEDIAP (screen target)
.  03 06		EYEDIAP (screen target)
		EYEDIAP (screen target)
4v 3		EYEDIAP (screen target)
		EYEDIAP (screen target)
cs  .C V		EYEDIAP (screen target)
		EYEDIAP (screen target)
1  7		EYEDIAP (screen target)
		EYEDIAP (screen target)
Se  p		EYEDIAP (screen target)
		EYEDIAP (screen target)
20  18		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Liversedge and Findlay} 2000		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Rutter and Durkin} 1987		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Guillon, Hadjikhani, Baduel, 		EYEDIAP (screen target)
and Rog{é}} 2014  Citation Citation {Jacob and Karn		EYEDIAP (screen target)
} 2003  Citation Citation {Majaranta and Bulling		EYEDIAP (screen target)
} 2014  Citation Citation {Palmero, van Dam		EYEDIAP (screen target)
, Escalera, Kelia, Lichtert, Noldus, 		EYEDIAP (screen target)
Spink, and van Wieringen} 2018  Citation Citation {Ono, Okabe, and		EYEDIAP (screen target)
 Sato} 2006		EYEDIAP (screen target)
		EYEDIAP (screen target)
2 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (screen target)
ESTIMATION  Method 3D gaze direction		EYEDIAP (screen target)
		EYEDIAP (screen target)
Unrestricted gaze target  Full face		EYEDIAP (screen target)
		EYEDIAP (screen target)
Eye region  Facial landmarks		EYEDIAP (screen target)
		EYEDIAP (screen target)
Sequential information  Zhang et al. (1) [42		EYEDIAP (screen target)
] 3 7 7 3 7 7 Krafka et al. [16		EYEDIAP (screen target)
] 7 7 3 3 7 7 Zhang et al. (2		EYEDIAP (screen target)
) [43] 3 7 3 7 7 7 Deng and Zhu		EYEDIAP (screen target)
 [4] 3 3 3 3		EYEDIAP (screen target)
 7 7 Ours 3 3		EYEDIAP (screen target)
 3 3 3 3		EYEDIAP (screen target)
		EYEDIAP (screen target)
Table 1: Characteristics of recent 		EYEDIAP (screen target)
related work on person- and 		EYEDIAP (screen target)
head pose-independent appearance-based gaze estimation 		EYEDIAP (screen target)
methods using CNNs.  where users look at the		EYEDIAP (screen target)
 screen or mobile phone, showing		EYEDIAP (screen target)
 a low head pose variability		EYEDIAP (screen target)
. It is yet unclear 		EYEDIAP (screen target)
how these methods would perform 		EYEDIAP (screen target)
in a wider range of 		EYEDIAP (screen target)
head poses.  On a different note, until		EYEDIAP (screen target)
 very recently, the majority of		EYEDIAP (screen target)
 methods only used static eye		EYEDIAP (screen target)
 region appearance as input. State-of-the-art		EYEDIAP (screen target)
 approaches have demonstrated that using		EYEDIAP (screen target)
 the face along with a		EYEDIAP (screen target)
 higher resolution image of the		EYEDIAP (screen target)
 eyes [16], or even just		EYEDIAP (screen target)
 the face itself [43], increases		EYEDIAP (screen target)
 performance. Indeed, the whole-face image		EYEDIAP (screen target)
 encodes more information than eyes		EYEDIAP (screen target)
 alone, such as illumination and		EYEDIAP (screen target)
 head pose. Nevertheless, gaze behavior		EYEDIAP (screen target)
 is not static. Eye and		EYEDIAP (screen target)
 head movements allow us to		EYEDIAP (screen target)
 direct our gaze to target		EYEDIAP (screen target)
 locations of interest. It has		EYEDIAP (screen target)
 been demonstrated that humans can		EYEDIAP (screen target)
 better predict gaze when being		EYEDIAP (screen target)
 shown image sequences of other		EYEDIAP (screen target)
 people moving their eyes [1		EYEDIAP (screen target)
]. However, it is still 		EYEDIAP (screen target)
an open question whether this 		EYEDIAP (screen target)
se- quential information can increase 		EYEDIAP (screen target)
the performance of automatic methods.  In this work, we show		EYEDIAP (screen target)
 that the combination of multiple		EYEDIAP (screen target)
 cues benefits the gaze estimation		EYEDIAP (screen target)
 task. In particular, we use		EYEDIAP (screen target)
 face, eye region and facial		EYEDIAP (screen target)
 landmarks from still images. Facial		EYEDIAP (screen target)
 landmarks model the global shape		EYEDIAP (screen target)
 of the face and come		EYEDIAP (screen target)
 at no cost, since face		EYEDIAP (screen target)
 alignment is a common pre-processing		EYEDIAP (screen target)
 step in many facial image		EYEDIAP (screen target)
 analysis approaches. Furthermore, we present		EYEDIAP (screen target)
 a subject-independent, free-head recurrent 3D		EYEDIAP (screen target)
 gaze regression network to leverage		EYEDIAP (screen target)
 the temporal information of image		EYEDIAP (screen target)
 sequences. The static streams of		EYEDIAP (screen target)
 each frame are combined in		EYEDIAP (screen target)
 a late-fusion fashion using a		EYEDIAP (screen target)
 multi-stream CNN. Then, all feature		EYEDIAP (screen target)
 vectors are input to a		EYEDIAP (screen target)
 many-to-one recurrent module that predicts		EYEDIAP (screen target)
 the gaze vector of the		EYEDIAP (screen target)
 last sequence frame		EYEDIAP (screen target)
.  In summary, our contributions are		EYEDIAP (screen target)
 two-fold. First, we present a		EYEDIAP (screen target)
 Recurrent-CNN net- work architecture that		EYEDIAP (screen target)
 combines appearance, shape and temporal		EYEDIAP (screen target)
 information for 3D gaze estimation		EYEDIAP (screen target)
. Second, we test static 		EYEDIAP (screen target)
and temporal versions of our 		EYEDIAP (screen target)
solution on the EYEDIAP 		EYEDIAP (screen target)
dataset [7] in a wide 		EYEDIAP (screen target)
range of head poses and 		EYEDIAP (screen target)
gaze directions, showing consistent perfor- 		EYEDIAP (screen target)
mance improvements compared to related 		EYEDIAP (screen target)
appearance-based methods. To the best 		EYEDIAP (screen target)
of our knowledge, this is 		EYEDIAP (screen target)
the first third-person, remote camera-based 		EYEDIAP (screen target)
approach that uses tempo- ral 		EYEDIAP (screen target)
information for this task. Table 1 outlines our main method characteristics		EYEDIAP (screen target)
 compared to related work. Models		EYEDIAP (screen target)
 and code are publicly available		EYEDIAP (screen target)
 at https://github.com/ crisie/RecurrentGaze		EYEDIAP (screen target)
.  2 Related work Gaze estimation		EYEDIAP (screen target)
 methods are typically categorized as		EYEDIAP (screen target)
 model-based or appearance-based [5, 10		EYEDIAP (screen target)
, 15]. Model-based approaches use 		EYEDIAP (screen target)
a geometric model of the 		EYEDIAP (screen target)
eye, usually requir- ing either 		EYEDIAP (screen target)
high resolution images or a 		EYEDIAP (screen target)
person-specific calibration stage to estimate 		EYEDIAP (screen target)
personal eye parameters [22, 33, 34, 37, 41]. In contrast, appearance-based		EYEDIAP (screen target)
 methods learn a di- rect		EYEDIAP (screen target)
 mapping from intensity images or		EYEDIAP (screen target)
 extracted eye features to gaze		EYEDIAP (screen target)
 directions, thus being		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Zhang, Sugano, Fritz, 		EYEDIAP (screen target)
and Bulling} 2015  Citation Citation {Krafka, Khosla, Kellnhofer		EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 		EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Deng and Zhu		EYEDIAP (screen target)
} 2017  Citation Citation {Krafka, Khosla, Kellnhofer		EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 		EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Anderson, Risko, and		EYEDIAP (screen target)
 Kingstone} 2016		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Funesprotect unhbox voidb@x 		EYEDIAP (screen target)
penalty @M  {}Mora, Monay, and Odobez} 2014		EYEDIAP (screen target)
{}  Citation Citation {Ferhat and Vilari{ñ}o		EYEDIAP (screen target)
} 2016  Citation Citation {Hansen and Ji		EYEDIAP (screen target)
} 2010  Citation Citation {Kar and Corcoran		EYEDIAP (screen target)
} 2017  Citation Citation {Morimoto, Amir, and		EYEDIAP (screen target)
 Flickner} 2002		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Venkateswarlu etprotect unhbox 		EYEDIAP (screen target)
voidb@x penalty @M  {}al.} 2003		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Wang and Ji} 2017		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Wood and Bulling} 2014		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Yoo and Chung} 2005		EYEDIAP (screen target)
		EYEDIAP (screen target)
https://github.com/crisie/RecurrentGaze 		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 3		EYEDIAP (screen target)
		EYEDIAP (screen target)
potentially applicable to relatively low 		EYEDIAP (screen target)
resolution images and mid-distance scenarios. 		EYEDIAP (screen target)
Dif- ferent mapping functions have 		EYEDIAP (screen target)
been explored, such as neural 		EYEDIAP (screen target)
networks [2], adaptive linear regression (		EYEDIAP (screen target)
ALR) [19], local interpolation [32], 		EYEDIAP (screen target)
gaussian processes [30, 35], random 		EYEDIAP (screen target)
forests [11, 31], or k-nearest 		EYEDIAP (screen target)
neighbors [40]. Main challenges of 		EYEDIAP (screen target)
appearance-based methods for 3D gaze 		EYEDIAP (screen target)
estimation are head pose, illumination 		EYEDIAP (screen target)
and subject invariance without user-specific 		EYEDIAP (screen target)
calibration. To handle these issues, 		EYEDIAP (screen target)
some works proposed compensation 		EYEDIAP (screen target)
methods [18] and warping strategies 		EYEDIAP (screen target)
that synthesize a canonical, frontal 		EYEDIAP (screen target)
looking view of the 		EYEDIAP (screen target)
face [6, 13, 21]. Hybrid 		EYEDIAP (screen target)
approaches based on analysis-by-synthesis have 		EYEDIAP (screen target)
also been evaluated [39].  Currently, data-driven methods are considered		EYEDIAP (screen target)
 the state of the art		EYEDIAP (screen target)
 for person- and head pose-independent		EYEDIAP (screen target)
 appearance-based gaze estimation. Consequently, a		EYEDIAP (screen target)
 number of gaze es- timation		EYEDIAP (screen target)
 datasets have been introduced in		EYEDIAP (screen target)
 recent years, either in controlled		EYEDIAP (screen target)
 [29] or semi- controlled settings		EYEDIAP (screen target)
 [8], in the wild [16		EYEDIAP (screen target)
, 42], or consisting of 		EYEDIAP (screen target)
synthetic data [31, 38, 40]. 		EYEDIAP (screen target)
Zhang et al. [42] showed 		EYEDIAP (screen target)
that CNNs can outperform other 		EYEDIAP (screen target)
mapping methods, using a multi- 		EYEDIAP (screen target)
modal CNN to learn the 		EYEDIAP (screen target)
mapping from 3D head poses 		EYEDIAP (screen target)
and eye images to 3D 		EYEDIAP (screen target)
gaze directions. Krafka et 		EYEDIAP (screen target)
al. [16] proposed a multi-stream 		EYEDIAP (screen target)
CNN for 2D gaze estimation, 		EYEDIAP (screen target)
using individual eye, whole-face image 		EYEDIAP (screen target)
and the face grid as 		EYEDIAP (screen target)
input. As this method was 		EYEDIAP (screen target)
limited to 2D screen mapping, 		EYEDIAP (screen target)
Zhang et al. [43] later 		EYEDIAP (screen target)
explored the potential of just 		EYEDIAP (screen target)
using whole-face images as input 		EYEDIAP (screen target)
to estimate 3D gaze directions. 		EYEDIAP (screen target)
Using a spatial weights CNN, 		EYEDIAP (screen target)
they demonstrated their method to 		EYEDIAP (screen target)
be more robust to facial 		EYEDIAP (screen target)
appearance variation caused by head 		EYEDIAP (screen target)
pose and illumina- tion than 		EYEDIAP (screen target)
eye-only methods. While the method 		EYEDIAP (screen target)
was evaluated in the wild, 		EYEDIAP (screen target)
the subjects were only interacting 		EYEDIAP (screen target)
with a mobile device, thus 		EYEDIAP (screen target)
restricting the head pose range. 		EYEDIAP (screen target)
Deng and Zhu [4] presented 		EYEDIAP (screen target)
a two-stream CNN to disjointly 		EYEDIAP (screen target)
model head pose from face 		EYEDIAP (screen target)
images and eye- ball movement 		EYEDIAP (screen target)
from eye region images. Both 		EYEDIAP (screen target)
were then aggregated into 3D 		EYEDIAP (screen target)
gaze direction using a gaze 		EYEDIAP (screen target)
transform layer. The decomposition was 		EYEDIAP (screen target)
aimed to avoid head-correlation over- 		EYEDIAP (screen target)
fitting of previous data-driven approaches. 		EYEDIAP (screen target)
They evaluated their approach in 		EYEDIAP (screen target)
the wild with a wider 		EYEDIAP (screen target)
range of head poses, obtaining 		EYEDIAP (screen target)
better performance than previous eye-based 		EYEDIAP (screen target)
methods. However, they did not 		EYEDIAP (screen target)
test it on public annotated 		EYEDIAP (screen target)
benchmark datasets.  In this paper, we propose		EYEDIAP (screen target)
 a multi-stream recurrent CNN network		EYEDIAP (screen target)
 for person- and head pose-independent		EYEDIAP (screen target)
 3D gaze estimation for a		EYEDIAP (screen target)
 mid-distance scenario. We evaluate it		EYEDIAP (screen target)
 on a wider range of		EYEDIAP (screen target)
 head poses and gaze directions		EYEDIAP (screen target)
 than screen-targeted approaches. As opposed		EYEDIAP (screen target)
 to previous methods, we also		EYEDIAP (screen target)
 rely on temporal information inherent		EYEDIAP (screen target)
 in sequential data		EYEDIAP (screen target)
.  3 Methodology		EYEDIAP (screen target)
		EYEDIAP (screen target)
In this section, we present 		EYEDIAP (screen target)
our approach for 3D gaze 		EYEDIAP (screen target)
regression based on appearance and 		EYEDIAP (screen target)
shape cues for still images 		EYEDIAP (screen target)
and image sequences. First, we 		EYEDIAP (screen target)
introduce the data modalities and 		EYEDIAP (screen target)
formulate the problem. Then, we 		EYEDIAP (screen target)
detail the normalization procedure prior 		EYEDIAP (screen target)
to the regression stage. Finally, 		EYEDIAP (screen target)
we explain the global network 		EYEDIAP (screen target)
topology as well as the 		EYEDIAP (screen target)
implementation details. An overview of 		EYEDIAP (screen target)
the system architecture is depicted 		EYEDIAP (screen target)
in Figure 1.  3.1 Multi-modal gaze regression		EYEDIAP (screen target)
		EYEDIAP (screen target)
Let us represent gaze direction 		EYEDIAP (screen target)
as a 3D unit vector 		EYEDIAP (screen target)
g = [gx,gy,gz]T ∈R3 in 		EYEDIAP (screen target)
the Camera Coor- dinate System (		EYEDIAP (screen target)
CCS), whose origin is the 		EYEDIAP (screen target)
central point between eyeball centers. 		EYEDIAP (screen target)
Assuming a calibrated camera, and 		EYEDIAP (screen target)
a known head position and 		EYEDIAP (screen target)
orientation, our goal is to 		EYEDIAP (screen target)
estimate g from a sequence 		EYEDIAP (screen target)
of images {I(i) | 		EYEDIAP (screen target)
I ∈ RW×H×3} as a 		EYEDIAP (screen target)
regression problem.  Citation Citation {Baluja and Pomerleau		EYEDIAP (screen target)
} 1994  Citation Citation {Lu, Sugano, Okabe		EYEDIAP (screen target)
, and Sato} 2011{}  Citation Citation {Tan, Kriegman, and		EYEDIAP (screen target)
 Ahuja} 2002		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Sugano, Matsushita, and 		EYEDIAP (screen target)
Sato} 2013  Citation Citation {Williams, Blake, and		EYEDIAP (screen target)
 Cipolla} 2006		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Huang, Veeraraghavan, and 		EYEDIAP (screen target)
Sabharwal} 2017  Citation Citation {Sugano, Matsushita, and		EYEDIAP (screen target)
 Sato} 2014		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Wood, Baltru{²}aitis, Morency, 		EYEDIAP (screen target)
Robinson, and Bulling} 2016{}  Citation Citation {Lu, Okabe, Sugano		EYEDIAP (screen target)
, and Sato} 2011{}  Citation Citation {Funes-Mora and Odobez		EYEDIAP (screen target)
} 2016  Citation Citation {Jeni and Cohn		EYEDIAP (screen target)
} 2016  Citation Citation {Mora and Odobez		EYEDIAP (screen target)
} 2012  Citation Citation {Wood, Baltru{²}aitis, Morency		EYEDIAP (screen target)
, Robinson, and Bulling} 2016{}  Citation Citation {Smith, Yin, Feiner		EYEDIAP (screen target)
, and Nayar} 2013  Citation Citation {Funesprotect unhbox voidb@x		EYEDIAP (screen target)
 penalty @M		EYEDIAP (screen target)
		EYEDIAP (screen target)
Mora, Monay, and Odobez} 2014{}  Citation Citation {Krafka, Khosla, Kellnhofer		EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 		EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (screen target)
, and Bulling} 2015  Citation Citation {Sugano, Matsushita, and		EYEDIAP (screen target)
 Sato} 2014		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Wood, Baltrusaitis, Zhang, 		EYEDIAP (screen target)
Sugano, Robinson, and Bulling} 2015  Citation Citation {Wood, Baltru{²}aitis, Morency		EYEDIAP (screen target)
, Robinson, and Bulling} 2016{}  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (screen target)
, and Bulling} 2015  Citation Citation {Krafka, Khosla, Kellnhofer		EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 		EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Deng and Zhu		EYEDIAP (screen target)
} 2017		EYEDIAP (screen target)
		EYEDIAP (screen target)
4 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (screen target)
ESTIMATION  Conv		EYEDIAP (screen target)
		EYEDIAP (screen target)
C on ca t  x y z x y		EYEDIAP (screen target)
 z x y z		EYEDIAP (screen target)
		EYEDIAP (screen target)
Individual Fusion Temporal  Individual Fusion		EYEDIAP (screen target)
		EYEDIAP (screen target)
Input 		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
Individual Fusion  Normalization		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
 .Conv		EYEDIAP (screen target)
		EYEDIAP (screen target)
Conv .  Conv		EYEDIAP (screen target)
		EYEDIAP (screen target)
Conv .  FC		EYEDIAP (screen target)
		EYEDIAP (screen target)
FC FC RNN  RNN		EYEDIAP (screen target)
		EYEDIAP (screen target)
RNN FC  Ti m e		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
		EYEDIAP (screen target)
Figure 1: Overview of the 		EYEDIAP (screen target)
proposed network. A multi-stream CNN 		EYEDIAP (screen target)
jointly models full-face, eye region 		EYEDIAP (screen target)
appearance and face landmarks from 		EYEDIAP (screen target)
still images. The combined extracted 		EYEDIAP (screen target)
fea- tures from each frame 		EYEDIAP (screen target)
are fed into a recurrent 		EYEDIAP (screen target)
module to predict last frame’s 		EYEDIAP (screen target)
gaze direction.  Gazing to a specific target		EYEDIAP (screen target)
 is achieved by a combination		EYEDIAP (screen target)
 of eye and head movements		EYEDIAP (screen target)
, which are highly coordinated. 		EYEDIAP (screen target)
Consequently, the apparent direction of 		EYEDIAP (screen target)
gaze is influenced not only 		EYEDIAP (screen target)
by the location of the 		EYEDIAP (screen target)
irises within the eyelid aperture, 		EYEDIAP (screen target)
but also by the position 		EYEDIAP (screen target)
and orientation of the face 		EYEDIAP (screen target)
with respect to the camera. 		EYEDIAP (screen target)
Known as the Wollaston 		EYEDIAP (screen target)
effect [36], the exact same 		EYEDIAP (screen target)
set of eyes may appear 		EYEDIAP (screen target)
to be looking in different 		EYEDIAP (screen target)
directions due to the surrounding 		EYEDIAP (screen target)
facial cues. It is therefore 		EYEDIAP (screen target)
reasonable to state that eye 		EYEDIAP (screen target)
images are not sufficient to 		EYEDIAP (screen target)
estimate gaze direction. Instead, whole-face 		EYEDIAP (screen target)
images can encode head pose 		EYEDIAP (screen target)
or illumination-specific information across larger 		EYEDIAP (screen target)
areas than those available just 		EYEDIAP (screen target)
in the eyes region [16, 43		EYEDIAP (screen target)
].  The drawback of appearance-only methods		EYEDIAP (screen target)
 is that global structure information		EYEDIAP (screen target)
 is not explicitly considered. In		EYEDIAP (screen target)
 that sense, facial landmarks can		EYEDIAP (screen target)
 be used as global shape		EYEDIAP (screen target)
 cues to en- code spatial		EYEDIAP (screen target)
 relationships and geometric constraints. Current		EYEDIAP (screen target)
 state-of-the-art face alignment approaches are		EYEDIAP (screen target)
 robust enough to handle large		EYEDIAP (screen target)
 appearance variability, extreme head poses		EYEDIAP (screen target)
 and occlusions, being especially useful		EYEDIAP (screen target)
 when the dataset used for		EYEDIAP (screen target)
 gaze estimation does not contain		EYEDIAP (screen target)
 such variability. Facial landmarks are		EYEDIAP (screen target)
 mainly correlated with head orientation		EYEDIAP (screen target)
, eye position, eyelid openness, 		EYEDIAP (screen target)
and eyebrow movement, which are 		EYEDIAP (screen target)
valuable features for our task.  Therefore, in our approach we		EYEDIAP (screen target)
 jointly model appearance and shape		EYEDIAP (screen target)
 cues (see Figure 1). The		EYEDIAP (screen target)
 former is represented by a		EYEDIAP (screen target)
 whole-face image IF , along		EYEDIAP (screen target)
 with a higher resolution image		EYEDIAP (screen target)
 of the eyes IE to		EYEDIAP (screen target)
 identify subtle changes. Due to		EYEDIAP (screen target)
 dealing with wide head pose		EYEDIAP (screen target)
 ranges, some eye images may		EYEDIAP (screen target)
 not depict the whole eye		EYEDIAP (screen target)
, containing mostly background or 		EYEDIAP (screen target)
other surrounding facial parts instead. 		EYEDIAP (screen target)
For that reason, and contrary 		EYEDIAP (screen target)
to previous approaches that only 		EYEDIAP (screen target)
use one eye image [31, 42		EYEDIAP (screen target)
], we use a single 		EYEDIAP (screen target)
image composed of two patches 		EYEDIAP (screen target)
of centered left and right 		EYEDIAP (screen target)
eyes. Finally, the shape cue 		EYEDIAP (screen target)
is represented by 3D face 		EYEDIAP (screen target)
landmarks obtained from a 68-landmark 		EYEDIAP (screen target)
model, denoted by 		EYEDIAP (screen target)
L = {(lx, ly, 		EYEDIAP (screen target)
		EYEDIAP (screen target)
)		EYEDIAP (screen target)
		EYEDIAP (screen target)
 | ∀c ∈ [1, ...,68		EYEDIAP (screen target)
]}.  In this work we also		EYEDIAP (screen target)
 consider the dynamic component of		EYEDIAP (screen target)
 gaze. We leverage the se		EYEDIAP (screen target)
- quential information of eye 		EYEDIAP (screen target)
and head movements such that, 		EYEDIAP (screen target)
given appearance and shape features 		EYEDIAP (screen target)
of consecutive frames, it is 		EYEDIAP (screen target)
possible to better predict the 		EYEDIAP (screen target)
gaze direction of the cur- 		EYEDIAP (screen target)
rent frame. Therefore, the 3D 		EYEDIAP (screen target)
gaze estimation task for a 1		EYEDIAP (screen target)
-frame sequence is formulated  Citation Citation {Wollaston etprotect unhbox		EYEDIAP (screen target)
 voidb@x penalty @M		EYEDIAP (screen target)
		EYEDIAP (screen target)
al.} 1824  Citation Citation {Krafka, Khosla, Kellnhofer		EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 		EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Sugano, Matsushita, and		EYEDIAP (screen target)
 Sato} 2014		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Zhang, Sugano, Fritz, 		EYEDIAP (screen target)
and Bulling} 2015		EYEDIAP (screen target)
		EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 5		EYEDIAP (screen target)
		EYEDIAP (screen target)
as g(i) = f ( {IF (i)},{IE (i)},{L(i		EYEDIAP (screen target)
)}  ) , where i denotes		EYEDIAP (screen target)
 the i-th frame, and f		EYEDIAP (screen target)
 is the regression		EYEDIAP (screen target)
		EYEDIAP (screen target)
function.  3.2 Data normalization Prior to		EYEDIAP (screen target)
 gaze regression, a normalization step		EYEDIAP (screen target)
 in the 3D space and		EYEDIAP (screen target)
 the 2D image, similar to		EYEDIAP (screen target)
 [31], is carried out. This		EYEDIAP (screen target)
 is performed to reduce the		EYEDIAP (screen target)
 appearance variability and to allow		EYEDIAP (screen target)
 the gaze estimation model to		EYEDIAP (screen target)
 be applied regardless of the		EYEDIAP (screen target)
 original camera configuration		EYEDIAP (screen target)
.  Let H ∈ R3x3 be		EYEDIAP (screen target)
 the head rotation matrix, and		EYEDIAP (screen target)
 p = [px, py, pz]T		EYEDIAP (screen target)
 ∈ R3 the reference face		EYEDIAP (screen target)
 location with respect to the		EYEDIAP (screen target)
 original CCS. The goal is		EYEDIAP (screen target)
 to find the conversion matrix		EYEDIAP (screen target)
 M = SR such that		EYEDIAP (screen target)
 (a) the X-axes of the		EYEDIAP (screen target)
 virtual camera and the head		EYEDIAP (screen target)
 become parallel using the rotation		EYEDIAP (screen target)
 matrix R, and (b) the		EYEDIAP (screen target)
 virtual camera looks at the		EYEDIAP (screen target)
 reference location from a fixed		EYEDIAP (screen target)
 distance dn using the Z-direction		EYEDIAP (screen target)
 scaling matrix S = diag(1,1,dn/‖p		EYEDIAP (screen target)
‖). R is computed as 		EYEDIAP (screen target)
a = p̂×HT e1, 		EYEDIAP (screen target)
b = â× p̂, 		EYEDIAP (screen target)
R = [â, b̂, p̂]T , where e1 denotes the first		EYEDIAP (screen target)
 orthonormal basis and		EYEDIAP (screen target)
 〈 ·̂ 〉 is the		EYEDIAP (screen target)
 unit vector		EYEDIAP (screen target)
.  This normalization translates into the		EYEDIAP (screen target)
 image space as a cropped		EYEDIAP (screen target)
 image patch of size Wn×Hn		EYEDIAP (screen target)
 centered at p where head		EYEDIAP (screen target)
 roll rotation has been removed		EYEDIAP (screen target)
. This is done by 		EYEDIAP (screen target)
applying a perspective warping to 		EYEDIAP (screen target)
the input image I using 		EYEDIAP (screen target)
the transformation matrix W = 		EYEDIAP (screen target)
CoMCn−1, where Co and Cn 		EYEDIAP (screen target)
are the original and virtual 		EYEDIAP (screen target)
camera matrices, respectively.  The 3D gaze vector is		EYEDIAP (screen target)
 also normalized as gn =Rg		EYEDIAP (screen target)
. After image normalization, the 		EYEDIAP (screen target)
line of sight can be 		EYEDIAP (screen target)
represented in a 2D space. 		EYEDIAP (screen target)
Therefore, gn is further transformed 		EYEDIAP (screen target)
to spherical coor- dinates (θ ,		EYEDIAP (screen target)
φ) assuming unit length, where 		EYEDIAP (screen target)
θ and φ denote the 		EYEDIAP (screen target)
horizontal and vertical direc- tion 		EYEDIAP (screen target)
angles, respectively. This 2D angle 		EYEDIAP (screen target)
representation, delimited in the 		EYEDIAP (screen target)
range [−π/2,π/2], is computed as 		EYEDIAP (screen target)
θ = arctan(gx/gz) and 		EYEDIAP (screen target)
φ = arcsin(−gy), such that (0,		EYEDIAP (screen target)
0) represents looking straight ahead 		EYEDIAP (screen target)
to the CCS origin.  3.3 Recurrent Convolutional Neural Network		EYEDIAP (screen target)
 We propose a Recurrent CNN		EYEDIAP (screen target)
 Regression Network for 3D gaze		EYEDIAP (screen target)
 estimation. The network is divided		EYEDIAP (screen target)
 in 3 modules: (1) Individual		EYEDIAP (screen target)
, (2) Fusion, and (3) 		EYEDIAP (screen target)
Temporal.  First, the Individual module learns		EYEDIAP (screen target)
 features from each appearance cue		EYEDIAP (screen target)
 separately. It consists of a		EYEDIAP (screen target)
 two-stream CNN, one devoted to		EYEDIAP (screen target)
 the normalized face image stream		EYEDIAP (screen target)
 and the other to the		EYEDIAP (screen target)
 joint normalized eyes image. Next		EYEDIAP (screen target)
, the Fusion module combines 		EYEDIAP (screen target)
the extracted features of each 		EYEDIAP (screen target)
appearance stream in a single 		EYEDIAP (screen target)
vector along with the normalized 		EYEDIAP (screen target)
landmark coordinates. Then, it learns 		EYEDIAP (screen target)
a joint representation between modalities 		EYEDIAP (screen target)
in a late-fusion fashion. Both 		EYEDIAP (screen target)
Individual and Fusion modules, further 		EYEDIAP (screen target)
referred to as Static model, 		EYEDIAP (screen target)
are applied to each frame 		EYEDIAP (screen target)
of the sequence. Finally, the 		EYEDIAP (screen target)
resulting feature vectors of each 		EYEDIAP (screen target)
frame are input to the 		EYEDIAP (screen target)
Temporal module based on a 		EYEDIAP (screen target)
many-to-one recurrent network. This module 		EYEDIAP (screen target)
leverages sequential information to predict 		EYEDIAP (screen target)
the normalized 2D gaze angles 		EYEDIAP (screen target)
of the last frame of 		EYEDIAP (screen target)
the sequence using a linear 		EYEDIAP (screen target)
regression layer added on top 		EYEDIAP (screen target)
of it.  3.4 Implementation details 3.4.1 Network		EYEDIAP (screen target)
 details		EYEDIAP (screen target)
		EYEDIAP (screen target)
Each stream of the Individual 		EYEDIAP (screen target)
module is based on the 		EYEDIAP (screen target)
VGG-16 deep network [27], consisting 		EYEDIAP (screen target)
of 13 convolutional layers, 5 		EYEDIAP (screen target)
max pooling layers, and 1 		EYEDIAP (screen target)
fully connected (FC) layer with 		EYEDIAP (screen target)
Rec- tified Linear Unit (ReLU) 		EYEDIAP (screen target)
activations. The full-face stream follows 		EYEDIAP (screen target)
the same configuration  Citation Citation {Sugano, Matsushita, and		EYEDIAP (screen target)
 Sato} 2014		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Parkhi, Vedaldi, and 		EYEDIAP (screen target)
Zisserman} 2015		EYEDIAP (screen target)
		EYEDIAP (screen target)
6 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (screen target)
ESTIMATION  as the base network, having		EYEDIAP (screen target)
 an input of 224×224 pixels		EYEDIAP (screen target)
 and a 4096D FC layer		EYEDIAP (screen target)
. In contrast, the input 		EYEDIAP (screen target)
joint eye image is smaller, 		EYEDIAP (screen target)
with a final size of 120		EYEDIAP (screen target)
×48 pixels, so the number 		EYEDIAP (screen target)
of pa- rameters is decreased 		EYEDIAP (screen target)
proportionally. In this case, its 		EYEDIAP (screen target)
last FC layer produces a 		EYEDIAP (screen target)
1536D vector. A 204D landmark 		EYEDIAP (screen target)
coordinates vector is concatenated to 		EYEDIAP (screen target)
the output of the FC 		EYEDIAP (screen target)
layer of each stream, resulting 		EYEDIAP (screen target)
in a 5836D feature vector. 		EYEDIAP (screen target)
Consequently, the Fusion module consists 		EYEDIAP (screen target)
of 2 5836D FC layers 		EYEDIAP (screen target)
with ReLU activations and 2 		EYEDIAP (screen target)
dropout layers between FCs as 		EYEDIAP (screen target)
regularization. Finally, to model the 		EYEDIAP (screen target)
temporal dependencies, we use a 		EYEDIAP (screen target)
single GRU layer with 128 		EYEDIAP (screen target)
units.  The network is trained in		EYEDIAP (screen target)
 a stage-wise fashion. First, we		EYEDIAP (screen target)
 train the Static model and		EYEDIAP (screen target)
 the final regression layer end-to-end		EYEDIAP (screen target)
 on each individual frame of		EYEDIAP (screen target)
 the training data. The convolutional		EYEDIAP (screen target)
 blocks are pre-trained with the		EYEDIAP (screen target)
 VGG-Face dataset [27], whereas the		EYEDIAP (screen target)
 FCs are trained from scratch		EYEDIAP (screen target)
. Second, the training data 		EYEDIAP (screen target)
is re-arranged by means of 		EYEDIAP (screen target)
a sliding window with stride 1 to build input sequences. Each		EYEDIAP (screen target)
 sequence is composed of s		EYEDIAP (screen target)
 = 4 consecutive frames, whose		EYEDIAP (screen target)
 gaze direction target is the		EYEDIAP (screen target)
 gaze direction of the last		EYEDIAP (screen target)
 frame of the sequence( {I(i−s+1		EYEDIAP (screen target)
), . . . ,I(i)}, 		EYEDIAP (screen target)
g(i)  ) . Using this re-arranged		EYEDIAP (screen target)
 training data, we extract features		EYEDIAP (screen target)
 of each		EYEDIAP (screen target)
		EYEDIAP (screen target)
frame of the sequence from 		EYEDIAP (screen target)
a frozen Individual module, fine-tune 		EYEDIAP (screen target)
the Fusion layers, and train 		EYEDIAP (screen target)
both, the Temporal module and 		EYEDIAP (screen target)
a new final regression layer 		EYEDIAP (screen target)
from scratch. This way, the 		EYEDIAP (screen target)
network can exploit the temporal 		EYEDIAP (screen target)
information to further refine the 		EYEDIAP (screen target)
fusion weights.  We trained the model using		EYEDIAP (screen target)
 ADAM optimizer with an initial		EYEDIAP (screen target)
 learning rate of 0.0001, dropout		EYEDIAP (screen target)
 of 0.3, and batch size		EYEDIAP (screen target)
 of 64 frames. The number		EYEDIAP (screen target)
 of epochs was experimentally set		EYEDIAP (screen target)
 to 21 for the first		EYEDIAP (screen target)
 training stage and 10 for		EYEDIAP (screen target)
 the second. We use the		EYEDIAP (screen target)
 average Euclidean distance between the		EYEDIAP (screen target)
 predicted and ground-truth 3D gaze		EYEDIAP (screen target)
 vectors as loss function		EYEDIAP (screen target)
.  3.4.2 Input pre-processing		EYEDIAP (screen target)
		EYEDIAP (screen target)
For this work we use 		EYEDIAP (screen target)
head pose and eye locations 		EYEDIAP (screen target)
in the 3D scene provided 		EYEDIAP (screen target)
by the dataset. The 3D 		EYEDIAP (screen target)
landmarks are extracted using the 		EYEDIAP (screen target)
state-of-the-art method of Bulat and 		EYEDIAP (screen target)
Tzimiropou- los [3], which is 		EYEDIAP (screen target)
based on stacked hourglass 		EYEDIAP (screen target)
networks [24].  During training, the original image		EYEDIAP (screen target)
 is pre-processed to get the		EYEDIAP (screen target)
 two normalized input images. The		EYEDIAP (screen target)
 normalized whole-face patch is centered		EYEDIAP (screen target)
 0.1 meters ahead of the		EYEDIAP (screen target)
 head center in the head		EYEDIAP (screen target)
 coordinate system, and Cn is		EYEDIAP (screen target)
 defined such that the image		EYEDIAP (screen target)
 has size of 250× 250		EYEDIAP (screen target)
 pixels. The difference between this		EYEDIAP (screen target)
 size and the final input		EYEDIAP (screen target)
 size allows us to perform		EYEDIAP (screen target)
 random cropping and zooming to		EYEDIAP (screen target)
 augment the data (explained in		EYEDIAP (screen target)
 Section 4.1). Similarly, each normalized		EYEDIAP (screen target)
 eye patch is centered in		EYEDIAP (screen target)
 their respective eye center locations		EYEDIAP (screen target)
. In this case, the 		EYEDIAP (screen target)
virtual camera matrix is defined 		EYEDIAP (screen target)
so that the image is 		EYEDIAP (screen target)
cropped to 70×58, while in 		EYEDIAP (screen target)
practice the final patches have 		EYEDIAP (screen target)
size of 60×48. Landmarks are 		EYEDIAP (screen target)
normalized using the same procedure 		EYEDIAP (screen target)
and further pre-processed with mean 		EYEDIAP (screen target)
subtraction and min-max normalization per 		EYEDIAP (screen target)
axis. Finally, we divide them 		EYEDIAP (screen target)
by a scaling factor w 		EYEDIAP (screen target)
such that all coordinates are 		EYEDIAP (screen target)
in the range [0,w]. This 		EYEDIAP (screen target)
way, all concatenated feature values 		EYEDIAP (screen target)
are in a similar range. 		EYEDIAP (screen target)
After inference, the predicted normalized 		EYEDIAP (screen target)
2D angles are de-normalized back 		EYEDIAP (screen target)
to the original 3D space.  4 Experiments In this section		EYEDIAP (screen target)
, we evaluate the cross-subject 		EYEDIAP (screen target)
3D gaze estimation task on 		EYEDIAP (screen target)
a wide range of head 		EYEDIAP (screen target)
poses and gaze directions. Furthermore, 		EYEDIAP (screen target)
we validate the effectiveness of 		EYEDIAP (screen target)
the proposed architecture comparing both 		EYEDIAP (screen target)
static and temporal approaches. We 		EYEDIAP (screen target)
report the error in terms 		EYEDIAP (screen target)
of mean angular error between 		EYEDIAP (screen target)
predicted and ground-truth 3D gaze 		EYEDIAP (screen target)
vectors. Note that due to 		EYEDIAP (screen target)
the requirements of the temporal 		EYEDIAP (screen target)
model not all the frames 		EYEDIAP (screen target)
obtain a prediction. Therefore, for 		EYEDIAP (screen target)
a  Citation Citation {Parkhi, Vedaldi, and		EYEDIAP (screen target)
 Zisserman} 2015		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Bulat and Tzimiropoulos} 2017		EYEDIAP (screen target)
		EYEDIAP (screen target)
Citation Citation {Newell, Yang, and 		EYEDIAP (screen target)
Deng} 2016		EYEDIAP (screen target)
		EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 7		EYEDIAP (screen target)
		EYEDIAP (screen target)
60 30 0 30 60  60		EYEDIAP (screen target)
		EYEDIAP (screen target)
30  0		EYEDIAP (screen target)
		EYEDIAP (screen target)
30  60		EYEDIAP (screen target)
		EYEDIAP (screen target)
100  101		EYEDIAP (screen target)
		EYEDIAP (screen target)
102  60 30 0 30 60		EYEDIAP (screen target)
		EYEDIAP (screen target)
60  30		EYEDIAP (screen target)
		EYEDIAP (screen target)
0  30		EYEDIAP (screen target)
		EYEDIAP (screen target)
60  100		EYEDIAP (screen target)
		EYEDIAP (screen target)
101  102		EYEDIAP (screen target)
		EYEDIAP (screen target)
103  60 30 0 30 60		EYEDIAP (screen target)
		EYEDIAP (screen target)
60  30		EYEDIAP (screen target)
		EYEDIAP (screen target)
0  30		EYEDIAP (screen target)
		EYEDIAP (screen target)
60  100		EYEDIAP (screen target)
		EYEDIAP (screen target)
101  102		EYEDIAP (screen target)
		EYEDIAP (screen target)
60 30 0 30 60  60		EYEDIAP (screen target)
		EYEDIAP (screen target)
30  0		EYEDIAP (screen target)
		EYEDIAP (screen target)
30  60		EYEDIAP (screen target)
		EYEDIAP (screen target)
100  101		EYEDIAP (screen target)
		EYEDIAP (screen target)
102  103		EYEDIAP (screen target)
		EYEDIAP (screen target)
a) g (FT ) (b) 		EYEDIAP (screen target)
h (FT ) (c) g (		EYEDIAP (screen target)
CS) (d) h (CS)  Figure 2: Ground-truth eye gaze		EYEDIAP (screen target)
 g and head orientation h		EYEDIAP (screen target)
 distribution on the filtered EYE		EYEDIAP (screen target)
- DIAP dataset for CS 		EYEDIAP (screen target)
and FT settings, in terms 		EYEDIAP (screen target)
of x- and y- angles.  fair comparison, the reported results		EYEDIAP (screen target)
 for static models disregard such		EYEDIAP (screen target)
 frames when temporal models are		EYEDIAP (screen target)
 included in the comparison		EYEDIAP (screen target)
.  4.1 Training data		EYEDIAP (screen target)
		EYEDIAP (screen target)
There are few publicly available 		EYEDIAP (screen target)
datasets devoted to 3D gaze 		EYEDIAP (screen target)
estimation and most of them 		EYEDIAP (screen target)
focus on HCI with a 		EYEDIAP (screen target)
limited range of head pose 		EYEDIAP (screen target)
and gaze directions. Therefore, we 		EYEDIAP (screen target)
use VGA videos from the 		EYEDIAP (screen target)
publicly-available EYEDIAP dataset [7] to 		EYEDIAP (screen target)
perform the experimental evaluation, as 		EYEDIAP (screen target)
it is currently the only 		EYEDIAP (screen target)
one containing video sequences with 		EYEDIAP (screen target)
a wide range of head 		EYEDIAP (screen target)
poses and showing the full 		EYEDIAP (screen target)
face. This dataset consists of 3		EYEDIAP (screen target)
-minute videos of 16 subjects 		EYEDIAP (screen target)
looking at two types of 		EYEDIAP (screen target)
targets: continuous screen targets on 		EYEDIAP (screen target)
a fixed monitor (CS), and 		EYEDIAP (screen target)
floating physical targets (FT ). 		EYEDIAP (screen target)
The videos are further divided 		EYEDIAP (screen target)
into static (S) and moving (		EYEDIAP (screen target)
M) head pose for each 		EYEDIAP (screen target)
of the subjects. Subjects 12-16 		EYEDIAP (screen target)
were recorded with 2 different 		EYEDIAP (screen target)
lighting conditions.  For evaluation, we filtered out		EYEDIAP (screen target)
 those frames that fulfilled at		EYEDIAP (screen target)
 least one of the following		EYEDIAP (screen target)
 conditions: (1) face or landmarks		EYEDIAP (screen target)
 not detected; (2) subject not		EYEDIAP (screen target)
 looking at the target; (3		EYEDIAP (screen target)
) 3D head pose, eyes 		EYEDIAP (screen target)
or target location not properly 		EYEDIAP (screen target)
recovered; and (4) eyeball rotations 		EYEDIAP (screen target)
violating physical 		EYEDIAP (screen target)
constraints (|θ | ≤ 40		EYEDIAP (screen target)
◦, |φ | ≤ 30		EYEDIAP (screen target)
◦) [23]. Note that we 		EYEDIAP (screen target)
purposely do not filter eye 		EYEDIAP (screen target)
blinking moments to learn their 		EYEDIAP (screen target)
dynamics with the temporal model, 		EYEDIAP (screen target)
which may produce some outliers 		EYEDIAP (screen target)
with a higher prediction error 		EYEDIAP (screen target)
due to a less accurate 		EYEDIAP (screen target)
ground truth. Figure 2 shows 		EYEDIAP (screen target)
the distribution of gaze directions 		EYEDIAP (screen target)
and head poses for both 		EYEDIAP (screen target)
filtered CS and FT cases.  We applied data augmentation to		EYEDIAP (screen target)
 the training set with the		EYEDIAP (screen target)
 following random transforma- tions: horizontal		EYEDIAP (screen target)
 flip, shifts of up to		EYEDIAP (screen target)
 5 pixels, zoom of up		EYEDIAP (screen target)
 to 2%, brightness changes by		EYEDIAP (screen target)
 a factor in the range		EYEDIAP (screen target)
 [0.4,1.75], and additive Gaussian noise		EYEDIAP (screen target)
 with σ2 = 0.03		EYEDIAP (screen target)
.  4.2 Evaluation of static modalities		EYEDIAP (screen target)
		EYEDIAP (screen target)
First, we evaluate the contribution 		EYEDIAP (screen target)
of each static modality on 		EYEDIAP (screen target)
the FT scenario. We divided 		EYEDIAP (screen target)
the 16 participants into 4 		EYEDIAP (screen target)
groups, such that appearance variability 		EYEDIAP (screen target)
was maximized while maintaining a 		EYEDIAP (screen target)
similar number of training samples 		EYEDIAP (screen target)
per group. Each static model 		EYEDIAP (screen target)
was trained end-to-end performing 4-fold 		EYEDIAP (screen target)
cross-validation using different combinations of 		EYEDIAP (screen target)
input modal- ities. Since the 		EYEDIAP (screen target)
number of fusion units depends 		EYEDIAP (screen target)
on the number of input 		EYEDIAP (screen target)
modalities, we also compare different 		EYEDIAP (screen target)
fusion layer sizes. The effect 		EYEDIAP (screen target)
of data normalization is also 		EYEDIAP (screen target)
evaluated by training a not-normalized 		EYEDIAP (screen target)
face model where the input 		EYEDIAP (screen target)
image is the face bounding 		EYEDIAP (screen target)
box with square size the 		EYEDIAP (screen target)
maximum distance between 2D landmarks.  Citation Citation {Funesprotect unhbox voidb@x		EYEDIAP (screen target)
 penalty @M		EYEDIAP (screen target)
		EYEDIAP (screen target)
Mora, Monay, and Odobez} 2014{}  Citation Citation {MSC		EYEDIAP (screen target)
		EYEDIAP (screen target)
8 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (screen target)
ESTIMATION  0 1 2 3 4		EYEDIAP (screen target)
 5 6 7 8 9		EYEDIAP (screen target)
		EYEDIAP (screen target)
10 11  An gl		EYEDIAP (screen target)
		EYEDIAP (screen target)
e  er		EYEDIAP (screen target)
		EYEDIAP (screen target)
ro r (  de gr		EYEDIAP (screen target)
		EYEDIAP (screen target)
ee s)  6.9 6.43 5.58 5.71 5.59		EYEDIAP (screen target)
 5.55 5.52		EYEDIAP (screen target)
		EYEDIAP (screen target)
OF-4096 NE-1536 NF-4096  NF-5632 NFL-4300		EYEDIAP (screen target)
		EYEDIAP (screen target)
NFE-5632 NFEL-5836  Figure 3: Performance evaluation of		EYEDIAP (screen target)
 the Static network using different		EYEDIAP (screen target)
 input modali- ties (O		EYEDIAP (screen target)
 - Not normalized, N		EYEDIAP (screen target)
 - Normalized, F - Face		EYEDIAP (screen target)
, E - Eyes, 		EYEDIAP (screen target)
L - 3D Landmarks) and 		EYEDIAP (screen target)
size of fusion layers on 		EYEDIAP (screen target)
the FT scenario.  Floating Target Screen Target 0		EYEDIAP (screen target)
 1 2 3 4 5		EYEDIAP (screen target)
 6 7 8 9		EYEDIAP (screen target)
		EYEDIAP (screen target)
10 11  An gl		EYEDIAP (screen target)
		EYEDIAP (screen target)
e  er		EYEDIAP (screen target)
		EYEDIAP (screen target)
ro r (  de gr		EYEDIAP (screen target)
		EYEDIAP (screen target)
ee s)  6.36 5.43 5.19 4.2 3.38		EYEDIAP (screen target)
 3.4		EYEDIAP (screen target)
		EYEDIAP (screen target)
MPIIGaze Static Temporal  Figure 4: Performance comparison among		EYEDIAP (screen target)
 MPIIGaze method [42] and our		EYEDIAP (screen target)
 Static and Temporal versions of		EYEDIAP (screen target)
 the proposed network for FT		EYEDIAP (screen target)
 and CS scenarios		EYEDIAP (screen target)
.  As shown in Figure 3		EYEDIAP (screen target)
, all models that take 		EYEDIAP (screen target)
normalized full-face information as input 		EYEDIAP (screen target)
achieve better performance than the 		EYEDIAP (screen target)
eyes-only model. More specifically, the 		EYEDIAP (screen target)
combination of face, eyes and 		EYEDIAP (screen target)
landmarks outperforms all the other 		EYEDIAP (screen target)
combinations by a small but 		EYEDIAP (screen target)
significant margin (paired Wilcoxon test, 		EYEDIAP (screen target)
p < 0.0001). The standard 		EYEDIAP (screen target)
deviation of the best-performing model 		EYEDIAP (screen target)
is reduced compared to the 		EYEDIAP (screen target)
face and eyes model, suggesting 		EYEDIAP (screen target)
a regularizing effect due to 		EYEDIAP (screen target)
the addition of landmarks. The 		EYEDIAP (screen target)
not-normalized face-only model shows the 		EYEDIAP (screen target)
largest error, proving the impact 		EYEDIAP (screen target)
of normalization to reduce the 		EYEDIAP (screen target)
appearance variability. Furthermore, our results 		EYEDIAP (screen target)
indicate that the increase of 		EYEDIAP (screen target)
fusion units is not correlated 		EYEDIAP (screen target)
with a better performance.  4.3 Static gaze regression: comparison		EYEDIAP (screen target)
 with existing methods		EYEDIAP (screen target)
		EYEDIAP (screen target)
We compare our best-performing static 		EYEDIAP (screen target)
model with three baselines. Head: 		EYEDIAP (screen target)
Treating the head pose directly 		EYEDIAP (screen target)
as gaze direction. PR-ALR: Method 		EYEDIAP (screen target)
that relies on RGB-D data 		EYEDIAP (screen target)
to rectify the eye images 		EYEDIAP (screen target)
viewpoint into a canonical head 		EYEDIAP (screen target)
pose using a 3DMM. It 		EYEDIAP (screen target)
then learns an RGB gaze 		EYEDIAP (screen target)
appearance model using ALR [21]. 		EYEDIAP (screen target)
Predicted 3D vectors for FT-S 		EYEDIAP (screen target)
scenario are provided by EYEDIAP 		EYEDIAP (screen target)
dataset. MPIIGaze:. State-of-the-art full-face 3D 		EYEDIAP (screen target)
gaze estimation method [42]. They 		EYEDIAP (screen target)
use an Alexnet-based CNN model 		EYEDIAP (screen target)
with spatial weights to enhance 		EYEDIAP (screen target)
information in different facial regions. 		EYEDIAP (screen target)
We fine-tuned it with the 		EYEDIAP (screen target)
filtered EYEDIAP subsets using our 		EYEDIAP (screen target)
training parameters and normalization procedure.  In addition to the aforementioned		EYEDIAP (screen target)
 FT-based evaluation setup, we also		EYEDIAP (screen target)
 evaluate our method on the		EYEDIAP (screen target)
 CS scenario. In this case		EYEDIAP (screen target)
 there are only 14 participants		EYEDIAP (screen target)
 available, so we divided them		EYEDIAP (screen target)
 in 5 groups and performed		EYEDIAP (screen target)
 5-fold cross-validation. In Figure 4		EYEDIAP (screen target)
 we compare our method to		EYEDIAP (screen target)
 MPIIGaze, achieving a statistically significant		EYEDIAP (screen target)
 improvement of 14.6% and 19.5		EYEDIAP (screen target)
% on FT and CS 		EYEDIAP (screen target)
scenarios, respectively (paired Wilcoxon test, 		EYEDIAP (screen target)
p < 0.0001). We can 		EYEDIAP (screen target)
observe that a re- stricted 		EYEDIAP (screen target)
gaze target benefits the performance 		EYEDIAP (screen target)
of all methods, compared to 		EYEDIAP (screen target)
a more challenging unrestricted setting 		EYEDIAP (screen target)
with a wider range of 		EYEDIAP (screen target)
head poses and gaze directions.  Table 2 provides a detailed		EYEDIAP (screen target)
 comparison on every participant, performing		EYEDIAP (screen target)
 leave-one-out cross-validation on the FT		EYEDIAP (screen target)
 scenario for static and moving		EYEDIAP (screen target)
 head separately. Results show that		EYEDIAP (screen target)
, as expected, facial appearance 		EYEDIAP (screen target)
and head pose have a 		EYEDIAP (screen target)
noticeable impact on gaze accuracy, 		EYEDIAP (screen target)
with average error differences of 		EYEDIAP (screen target)
up to 7.7◦ among participants.  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (screen target)
, and Bulling} 2015  Citation Citation {Mora and Odobez		EYEDIAP (screen target)
} 2012  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (screen target)
, and Bulling} 2015		EYEDIAP (screen target)
		EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 9		EYEDIAP (screen target)
		EYEDIAP (screen target)
Method 1 2 3 4 5 6 7 8 9 10		EYEDIAP (screen target)
 11 12 13 14 15		EYEDIAP (screen target)
 16 Avg. Head 23.5 22.1		EYEDIAP (screen target)
 20.3 23.6 23.2 23.2 23.6		EYEDIAP (screen target)
 21.2 26.7 23.6 23.1 24.4		EYEDIAP (screen target)
 23.3 24.0 24.5 22.8 23.3		EYEDIAP (screen target)
 PR-ALR 12.3 12.0 12.4 11.3		EYEDIAP (screen target)
 15.5 12.9 17.9 11.8 17.3		EYEDIAP (screen target)
 13.4 13.4 14.3 15.2 13.6		EYEDIAP (screen target)
 14.4 14.6 13.9 MPIIGaze 5.3		EYEDIAP (screen target)
 5.1 5.7 4.7 7.3 15.1		EYEDIAP (screen target)
 10.8 5.7 9.9 7.1 5.0		EYEDIAP (screen target)
 5.7 7.4 3.8 4.8 5.5		EYEDIAP (screen target)
 6.8 Static 3.9 4.1 4.2		EYEDIAP (screen target)
 3.9 6.0 6.4 7.2 3.6		EYEDIAP (screen target)
 7.1 5.0 5.7 6.7 3.9		EYEDIAP (screen target)
 4.7 5.1 4.2 5.1 Temporal		EYEDIAP (screen target)
 4.0 4.9 4.3 4.1 6.1		EYEDIAP (screen target)
 6.5 6.6 3.9 7.8 6.1		EYEDIAP (screen target)
 4.7 5.6 4.7 3.5 5.9		EYEDIAP (screen target)
 4.6 5.2 Head 19.3 14.2		EYEDIAP (screen target)
 16.4 19.9 16.8 21.9 16.1		EYEDIAP (screen target)
 24.2 20.3 19.9 18.8 22.3		EYEDIAP (screen target)
 18.1 14.9 16.2 19.3 18.7		EYEDIAP (screen target)
 MPIIGaze 7.6 6.2 5.7 8.7		EYEDIAP (screen target)
 10.1 12.0 12.2 6.1 8.3		EYEDIAP (screen target)
 5.9 6.1 6.2 7.4 4.7		EYEDIAP (screen target)
 4.4 6.0 7.3 Static 5.8		EYEDIAP (screen target)
 5.7 4.4 7.5 6.7 8.8		EYEDIAP (screen target)
 11.6 5.5 8.3 5.5 5.2		EYEDIAP (screen target)
 6.3 5.3 3.9 4.3 5.6		EYEDIAP (screen target)
 6.3 Temporal 6.1 5.6 4.5		EYEDIAP (screen target)
 7.5 6.4 8.2 12.0 5.0		EYEDIAP (screen target)
 7.5 5.4 5.0 5.8 6.6		EYEDIAP (screen target)
 4.0 4.5 5.8 6.2		EYEDIAP (screen target)
		EYEDIAP (screen target)
Table 2: Gaze angular error 		EYEDIAP (screen target)
comparison for static (top half) 		EYEDIAP (screen target)
and moving (bottom half) head 		EYEDIAP (screen target)
pose for each subject in 		EYEDIAP (screen target)
the FT scenario. Best results 		EYEDIAP (screen target)
in bold.  −80 −40 0 40 80−80		EYEDIAP (screen target)
		EYEDIAP (screen target)
40  0		EYEDIAP (screen target)
		EYEDIAP (screen target)
40  80		EYEDIAP (screen target)
		EYEDIAP (screen target)
0  5		EYEDIAP (screen target)
		EYEDIAP (screen target)
10  15		EYEDIAP (screen target)
		EYEDIAP (screen target)
20  25		EYEDIAP (screen target)
		EYEDIAP (screen target)
30  35		EYEDIAP (screen target)
		EYEDIAP (screen target)
80 −40 0 40 80−80  −40		EYEDIAP (screen target)
		EYEDIAP (screen target)
0  40		EYEDIAP (screen target)
		EYEDIAP (screen target)
80  −10		EYEDIAP (screen target)
		EYEDIAP (screen target)
8  −6		EYEDIAP (screen target)
		EYEDIAP (screen target)
4  −2		EYEDIAP (screen target)
		EYEDIAP (screen target)
0  2		EYEDIAP (screen target)
		EYEDIAP (screen target)
4  6		EYEDIAP (screen target)
		EYEDIAP (screen target)
8  10		EYEDIAP (screen target)
		EYEDIAP (screen target)
80 −40 0 40 80−80  −40		EYEDIAP (screen target)
		EYEDIAP (screen target)
0  40		EYEDIAP (screen target)
		EYEDIAP (screen target)
80  0		EYEDIAP (screen target)
		EYEDIAP (screen target)
5  10		EYEDIAP (screen target)
		EYEDIAP (screen target)
15  20		EYEDIAP (screen target)
		EYEDIAP (screen target)
25  30		EYEDIAP (screen target)
		EYEDIAP (screen target)
35  −80 −40 0 40 80−80		EYEDIAP (screen target)
		EYEDIAP (screen target)
40  0		EYEDIAP (screen target)
		EYEDIAP (screen target)
40  80		EYEDIAP (screen target)
		EYEDIAP (screen target)
10  −8		EYEDIAP (screen target)
		EYEDIAP (screen target)
6  −4		EYEDIAP (screen target)
		EYEDIAP (screen target)
2  0		EYEDIAP (screen target)
		EYEDIAP (screen target)
2  4		EYEDIAP (screen target)
		EYEDIAP (screen target)
6  8		EYEDIAP (screen target)
		EYEDIAP (screen target)
10  (a) Gaze space (b) Head		EYEDIAP (screen target)
 orientation space		EYEDIAP (screen target)
		EYEDIAP (screen target)
Figure 5: Angular error distribution 		EYEDIAP (screen target)
across gaze (a) and head 		EYEDIAP (screen target)
orientation (b) spaces in the 		EYEDIAP (screen target)
FT setting, in terms of 		EYEDIAP (screen target)
x- and y- angles. For 		EYEDIAP (screen target)
each space, we depict the 		EYEDIAP (screen target)
Static model performance (left) and 		EYEDIAP (screen target)
the contribution of the Temporal 		EYEDIAP (screen target)
model versus Static (right). In 		EYEDIAP (screen target)
the latter, positive difference means 		EYEDIAP (screen target)
higher improvement of the Temporal 		EYEDIAP (screen target)
model.  4.4 Evaluation of the temporal		EYEDIAP (screen target)
 network		EYEDIAP (screen target)
		EYEDIAP (screen target)
In this section, we evaluate 		EYEDIAP (screen target)
the contribution of adding the 		EYEDIAP (screen target)
temporal module to the static 		EYEDIAP (screen target)
model. To do so, we 		EYEDIAP (screen target)
trained a lower-dimensional version of 		EYEDIAP (screen target)
the static network with compa- 		EYEDIAP (screen target)
rable performance to the original, 		EYEDIAP (screen target)
reducing the number of units 		EYEDIAP (screen target)
of the second fusion layer 		EYEDIAP (screen target)
to 2918. Results are reported 		EYEDIAP (screen target)
in Figure 4 and Table 2		EYEDIAP (screen target)
. One can observe that 		EYEDIAP (screen target)
using sequential information is helpful 		EYEDIAP (screen target)
on the FT scenario, outperforming 		EYEDIAP (screen target)
the static model by a 		EYEDIAP (screen target)
statistically significant 4.4% (paired Wilcoxon 		EYEDIAP (screen target)
test, p < 0.0001). This 		EYEDIAP (screen target)
contribution is more noticeable in 		EYEDIAP (screen target)
the moving head setting, proving 		EYEDIAP (screen target)
that the temporal model can 		EYEDIAP (screen target)
benefit from head motion information. 		EYEDIAP (screen target)
In contrast, such information seems 		EYEDIAP (screen target)
to be less meaningful in 		EYEDIAP (screen target)
the CS scenario, where the 		EYEDIAP (screen target)
obtained error is already very 		EYEDIAP (screen target)
low for a cross-subject setting 		EYEDIAP (screen target)
and the amount of head 		EYEDIAP (screen target)
movement declines.  Figure 5 further explores the		EYEDIAP (screen target)
 error distribution of the static		EYEDIAP (screen target)
 network and the impact of		EYEDIAP (screen target)
 sequential information. We can observe		EYEDIAP (screen target)
 that the accuracy of the		EYEDIAP (screen target)
 static model drops with extreme		EYEDIAP (screen target)
 head poses and gaze directions		EYEDIAP (screen target)
, which can also be 		EYEDIAP (screen target)
correlated to having less data 		EYEDIAP (screen target)
in those areas. Compared to 		EYEDIAP (screen target)
the static model, the temporal 		EYEDIAP (screen target)
model particularly benefits gaze targets 		EYEDIAP (screen target)
from mid-range upwards. Its contribution 		EYEDIAP (screen target)
is less clear for extreme 		EYEDIAP (screen target)
targets, probably again due to 		EYEDIAP (screen target)
data imbalance.  Finally, we evaluated the effect		EYEDIAP (screen target)
 of different recurrent architectures for		EYEDIAP (screen target)
 the temporal model. In particular		EYEDIAP (screen target)
, we tested 1 (128 		EYEDIAP (screen target)
units) and 2 (256-128 units) 		EYEDIAP (screen target)
LSTM and GRU lay- ers, 		EYEDIAP (screen target)
with 1 GRU layer obtaining 		EYEDIAP (screen target)
slightly superior results (up to 0		EYEDIAP (screen target)
.12◦). We also assessed the 		EYEDIAP (screen target)
effect of sequence length fixing 		EYEDIAP (screen target)
s in the range {4,7,10}, 		EYEDIAP (screen target)
with s = 7 performing 		EYEDIAP (screen target)
worse than the other two (		EYEDIAP (screen target)
up to 0		EYEDIAP (screen target)
		EYEDIAP (screen target)
14		EYEDIAP (screen target)
		EYEDIAP (screen target)
10 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (screen target)
ESTIMATION  5 Conclusions In this work		EYEDIAP (screen target)
, we studied the combination 		EYEDIAP (screen target)
of full-face and eye images 		EYEDIAP (screen target)
along with facial land- marks 		EYEDIAP (screen target)
for person- and head pose-independent 		EYEDIAP (screen target)
3D gaze estimation. Consequently, we 		EYEDIAP (screen target)
pro- posed a multi-stream recurrent 		EYEDIAP (screen target)
CNN network that leverages the 		EYEDIAP (screen target)
sequential information of eye and 		EYEDIAP (screen target)
head movements. Both static and 		EYEDIAP (screen target)
temporal versions of our approach 		EYEDIAP (screen target)
significantly outperform current state-of-the-art 3D 		EYEDIAP (screen target)
gaze estimation methods on a 		EYEDIAP (screen target)
wide range of head poses 		EYEDIAP (screen target)
and gaze directions. We showed 		EYEDIAP (screen target)
that adding geometry features to 		EYEDIAP (screen target)
appearance-based methods has a regularizing 		EYEDIAP (screen target)
effect on the accuracy. Adding 		EYEDIAP (screen target)
sequential information further benefits the 		EYEDIAP (screen target)
final performance compared to static-only 		EYEDIAP (screen target)
input, especially from mid-range up- 		EYEDIAP (screen target)
wards and in those cases 		EYEDIAP (screen target)
where head motion is present. 		EYEDIAP (screen target)
The effect in very extreme 		EYEDIAP (screen target)
head poses is not clear 		EYEDIAP (screen target)
due to data imbalance, suggesting 		EYEDIAP (screen target)
the importance of learning from 		EYEDIAP (screen target)
a con- tinuous, balanced dataset 		EYEDIAP (screen target)
including all head poses and 		EYEDIAP (screen target)
gaze directions of interest. To 		EYEDIAP (screen target)
the best of our knowledge, 		EYEDIAP (screen target)
this is the first attempt 		EYEDIAP (screen target)
to exploit the temporal modality 		EYEDIAP (screen target)
in the context of gaze 		EYEDIAP (screen target)
estimation from remote cameras. As 		EYEDIAP (screen target)
future work, we will further 		EYEDIAP (screen target)
explore extracting meaningful temporal representations 		EYEDIAP (screen target)
of gaze dynamics, considering 3DCNNs 		EYEDIAP (screen target)
as well as the encoding 		EYEDIAP (screen target)
of deep features around particular 		EYEDIAP (screen target)
tracked face landmarks [14].  Acknowledgements This work has been		EYEDIAP (screen target)
 partially supported by the Spanish		EYEDIAP (screen target)
 project TIN2016-74946-P (MINECO/ FEDER, UE		EYEDIAP (screen target)
), CERCA Programme / Generalitat 		EYEDIAP (screen target)
de Catalunya, and the FP7 		EYEDIAP (screen target)
people program (Marie Curie Actions), 		EYEDIAP (screen target)
REA grant agreement no FP7-607139 (		EYEDIAP (screen target)
iCARE - Improving Children Auditory 		EYEDIAP (screen target)
REhabilitation). We gratefully acknowledge the 		EYEDIAP (screen target)
support of NVIDIA Corporation with 		EYEDIAP (screen target)
the donation of the GPU 		EYEDIAP (screen target)
used for this research. Portions 		EYEDIAP (screen target)
of the research in this 		EYEDIAP (screen target)
pa- per used the EYEDIAP 		EYEDIAP (screen target)
dataset made available by the 		EYEDIAP (screen target)
Idiap Research Institute, Martigny, Switzerland.  References [1] Nicola C Anderson		EYEDIAP (screen target)
, Evan F Risko, and 		EYEDIAP (screen target)
Alan Kingstone. Motion influences gaze 		EYEDIAP (screen target)
di-  rection discrimination and disambiguates contradictory		EYEDIAP (screen target)
 luminance cues. Psychonomic bulletin		EYEDIAP (screen target)
 & review, 23(3):817–823, 2016		EYEDIAP (screen target)
.  [2] Shumeet Baluja and Dean		EYEDIAP (screen target)
 Pomerleau. Non-intrusive gaze tracking using		EYEDIAP (screen target)
 artificial neu- ral networks. In		EYEDIAP (screen target)
 Advances in Neural Information Processing		EYEDIAP (screen target)
 Systems, pages 753–760, 1994		EYEDIAP (screen target)
.  [3] Adrian Bulat and Georgios		EYEDIAP (screen target)
 Tzimiropoulos. How far are we		EYEDIAP (screen target)
 from solving the 2d		EYEDIAP (screen target)
 & 3d face alignment problem		EYEDIAP (screen target)
? (and a dataset of 230,		EYEDIAP (screen target)
000 3d facial landmarks). In 		EYEDIAP (screen target)
Interna- tional Conference on Computer 		EYEDIAP (screen target)
Vision, 2017.  [4] Haoping Deng and Wangjiang		EYEDIAP (screen target)
 Zhu. Monocular free-head 3d gaze		EYEDIAP (screen target)
 tracking with deep learning and		EYEDIAP (screen target)
 geometry constraints. In Computer Vision		EYEDIAP (screen target)
 (ICCV), 2017 IEEE Interna- tional		EYEDIAP (screen target)
 Conference on, pages 3162–3171. IEEE		EYEDIAP (screen target)
, 2017.  [5] Onur Ferhat and Fernando		EYEDIAP (screen target)
 Vilariño. Low cost eye tracking		EYEDIAP (screen target)
. Computational intelligence and neuroscience, 2016		EYEDIAP (screen target)
:17, 2016.  Citation Citation {Jung, Lee, Yim		EYEDIAP (screen target)
, Park, and Kim} 2015		EYEDIAP (screen target)
		EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 11		EYEDIAP (screen target)
		EYEDIAP (screen target)
6] Kenneth A Funes-Mora and 		EYEDIAP (screen target)
Jean-Marc Odobez. Gaze estimation in 		EYEDIAP (screen target)
the 3D space using RGB-D 		EYEDIAP (screen target)
sensors. International Journal of Computer 		EYEDIAP (screen target)
Vision, 118(2):194–216, 2016.  [7] Kenneth Alberto Funes Mora		EYEDIAP (screen target)
, Florent Monay, and Jean-Marc 		EYEDIAP (screen target)
Odobez. Eyediap: A database for 		EYEDIAP (screen target)
the development and evaluation of 		EYEDIAP (screen target)
gaze estimation algorithms from rgb 		EYEDIAP (screen target)
and rgb-d cameras. In Proceedings 		EYEDIAP (screen target)
of the ACM Symposium on 		EYEDIAP (screen target)
Eye Tracking Research and Applications. 		EYEDIAP (screen target)
ACM, March 2014. doi: 10.1145/2578153.2578190.  [8] Kenneth Alberto Funes Mora		EYEDIAP (screen target)
, Florent Monay, and Jean-Marc 		EYEDIAP (screen target)
Odobez. Eyediap: A database for 		EYEDIAP (screen target)
the development and evaluation of 		EYEDIAP (screen target)
gaze estimation algorithms from rgb 		EYEDIAP (screen target)
and rgb-d cameras. In Proceedings 		EYEDIAP (screen target)
of the Symposium on Eye 		EYEDIAP (screen target)
Tracking Research and Applications, pages 255		EYEDIAP (screen target)
–258. ACM, 2014.  [9] Quentin Guillon, Nouchine Hadjikhani		EYEDIAP (screen target)
, Sophie Baduel, and Bernadette 		EYEDIAP (screen target)
Rogé. Visual social attention in 		EYEDIAP (screen target)
autism spectrum disorder: Insights from 		EYEDIAP (screen target)
eye tracking studies. Neu- 		EYEDIAP (screen target)
roscience & Biobehavioral Reviews, 42:279–297, 2014		EYEDIAP (screen target)
.  [10] Dan Witzner Hansen and		EYEDIAP (screen target)
 Qiang Ji. In the eye		EYEDIAP (screen target)
 of the beholder: A survey		EYEDIAP (screen target)
 of models for eyes and		EYEDIAP (screen target)
 gaze. IEEE transactions on pattern		EYEDIAP (screen target)
 analysis and machine intelligence, 32(3		EYEDIAP (screen target)
): 478–500, 2010.  [11] Qiong Huang, Ashok Veeraraghavan		EYEDIAP (screen target)
, and Ashutosh Sabharwal. Tabletgaze: 		EYEDIAP (screen target)
dataset and analysis for unconstrained 		EYEDIAP (screen target)
appearance-based gaze estimation in mobile 		EYEDIAP (screen target)
tablets. Machine Vision and Applications, 28		EYEDIAP (screen target)
(5-6):445–461, 2017.  [12] Robert JK Jacob and		EYEDIAP (screen target)
 Keith S Karn. Eye tracking		EYEDIAP (screen target)
 in human-computer interaction and usability		EYEDIAP (screen target)
 research: Ready to deliver the		EYEDIAP (screen target)
 promises. In The mind’s eye		EYEDIAP (screen target)
, pages 573–605. Elsevier, 2003.  [13] László A Jeni and		EYEDIAP (screen target)
 Jeffrey F Cohn. Person-independent 3d		EYEDIAP (screen target)
 gaze estimation using face frontalization		EYEDIAP (screen target)
. In Proceedings of the 		EYEDIAP (screen target)
IEEE Conference on Computer Vision 		EYEDIAP (screen target)
and Pattern Recognition Workshops, pages 87		EYEDIAP (screen target)
–95, 2016.  [14] Heechul Jung, Sihaeng Lee		EYEDIAP (screen target)
, Junho Yim, Sunjeong Park, 		EYEDIAP (screen target)
and Junmo Kim. Joint fine- 		EYEDIAP (screen target)
tuning in deep neural networks 		EYEDIAP (screen target)
for facial expression recognition. In 		EYEDIAP (screen target)
Computer Vision (ICCV), 2015 IEEE 		EYEDIAP (screen target)
International Conference on, pages 2983–2991. 		EYEDIAP (screen target)
IEEE, 2015.  [15] Anuradha Kar and Peter		EYEDIAP (screen target)
 Corcoran. A review and analysis		EYEDIAP (screen target)
 of eye-gaze estimation sys- tems		EYEDIAP (screen target)
, algorithms and performance evaluation 		EYEDIAP (screen target)
methods in consumer platforms. IEEE 		EYEDIAP (screen target)
Access, 5:16495–16519, 2017.  [16] Kyle Krafka, Aditya Khosla		EYEDIAP (screen target)
, Petr Kellnhofer, Harini Kannan, 		EYEDIAP (screen target)
Suchendra Bhandarkar, Wojciech Matusik, and 		EYEDIAP (screen target)
Antonio Torralba. Eye tracking for 		EYEDIAP (screen target)
everyone. In Computer Vision and 		EYEDIAP (screen target)
Pattern Recognition (CVPR), 2016 IEEE 		EYEDIAP (screen target)
Conference on, pages 2176–2184. IEEE, 2016		EYEDIAP (screen target)
.  [17] Simon P Liversedge and		EYEDIAP (screen target)
 John M Findlay. Saccadic eye		EYEDIAP (screen target)
 movements and cognition. Trends in		EYEDIAP (screen target)
 cognitive sciences, 4(1):6–14, 2000		EYEDIAP (screen target)
.  [18] Feng Lu, Takahiro Okabe		EYEDIAP (screen target)
, Yusuke Sugano, and Yoichi 		EYEDIAP (screen target)
Sato. A head pose-free approach 		EYEDIAP (screen target)
for appearance-based gaze estimation. In 		EYEDIAP (screen target)
BMVC, pages 1–11, 2011		EYEDIAP (screen target)
		EYEDIAP (screen target)
12 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (screen target)
ESTIMATION  [19] Feng Lu, Yusuke Sugano		EYEDIAP (screen target)
, Takahiro Okabe, and Yoichi 		EYEDIAP (screen target)
Sato. Inferring human gaze from 		EYEDIAP (screen target)
appearance via adaptive linear regression. 		EYEDIAP (screen target)
In Computer Vision (ICCV), 2011 		EYEDIAP (screen target)
IEEE International Conference on, pages 153		EYEDIAP (screen target)
–160. IEEE, 2011.  [20] Päivi Majaranta and Andreas		EYEDIAP (screen target)
 Bulling. Eye tracking and eye-based		EYEDIAP (screen target)
 human–computer interaction. In Advances in		EYEDIAP (screen target)
 physiological computing, pages 39–65. Springer		EYEDIAP (screen target)
, 2014.  [21] Kenneth Alberto Funes Mora		EYEDIAP (screen target)
 and Jean-Marc Odobez. Gaze estimation		EYEDIAP (screen target)
 from multi- modal kinect data		EYEDIAP (screen target)
. In Computer Vision and 		EYEDIAP (screen target)
Pattern Recognition Workshops (CVPRW), 2012 		EYEDIAP (screen target)
IEEE Computer Society Conference on, 		EYEDIAP (screen target)
pages 25–30. IEEE, 2012.  [22] Carlos Hitoshi Morimoto, Arnon		EYEDIAP (screen target)
 Amir, and Myron Flickner. Detecting		EYEDIAP (screen target)
 eye position and gaze from		EYEDIAP (screen target)
 a single camera and 2		EYEDIAP (screen target)
 light sources. In Pattern Recognition		EYEDIAP (screen target)
, 2002. Proceedings. 16th International 		EYEDIAP (screen target)
Conference on, volume 4, pages 314		EYEDIAP (screen target)
–317. IEEE, 2002.  [23] IMO MSC. Circ. 982		EYEDIAP (screen target)
 (2000) guidelines on ergonomic criteria		EYEDIAP (screen target)
 for bridge equipment and layout		EYEDIAP (screen target)
.  [24] Alejandro Newell, Kaiyu Yang		EYEDIAP (screen target)
, and Jia Deng. Stacked 		EYEDIAP (screen target)
hourglass networks for hu- man 		EYEDIAP (screen target)
pose estimation. In European Conference 		EYEDIAP (screen target)
on Computer Vision, pages 483–499. 		EYEDIAP (screen target)
Springer, 2016.  [25] Yasuhiro Ono, Takahiro Okabe		EYEDIAP (screen target)
, and Yoichi Sato. Gaze 		EYEDIAP (screen target)
estimation from low resolution images. 		EYEDIAP (screen target)
In Pacific-Rim Symposium on Image 		EYEDIAP (screen target)
and Video Technology, pages 178–188. 		EYEDIAP (screen target)
Springer, 2006.  [26] Cristina Palmero, Elisabeth A		EYEDIAP (screen target)
. van Dam, Sergio Escalera, 		EYEDIAP (screen target)
Mike Kelia, Guido F. Lichtert, 		EYEDIAP (screen target)
Lucas P.J.J Noldus, Andrew J. 		EYEDIAP (screen target)
Spink, and Astrid van Wieringen. 		EYEDIAP (screen target)
Automatic mutual gaze detection in 		EYEDIAP (screen target)
face-to-face dyadic interaction videos. In 		EYEDIAP (screen target)
Proceedings of Measuring Behavior, pages 158		EYEDIAP (screen target)
–163, 2018.  [27] Omkar M. Parkhi, Andrea		EYEDIAP (screen target)
 Vedaldi, and Andrew Zisserman. Deep		EYEDIAP (screen target)
 face recognition. In British Machine		EYEDIAP (screen target)
 Vision Conference, 2015		EYEDIAP (screen target)
.  [28] Derek R Rutter and		EYEDIAP (screen target)
 Kevin Durkin. Turn-taking in mother–infant		EYEDIAP (screen target)
 interaction: An exam- ination of		EYEDIAP (screen target)
 vocalizations and gaze. Developmental psychology		EYEDIAP (screen target)
, 23(1):54, 1987.  [29] Brian A Smith, Qi		EYEDIAP (screen target)
 Yin, Steven K Feiner, and		EYEDIAP (screen target)
 Shree K Nayar. Gaze locking		EYEDIAP (screen target)
: passive eye contact detection 		EYEDIAP (screen target)
for human-object interaction. In Proceedings 		EYEDIAP (screen target)
of the 26th annual ACM 		EYEDIAP (screen target)
symposium on User interface software 		EYEDIAP (screen target)
and technology, pages 271–280. ACM, 2013		EYEDIAP (screen target)
.  [30] Yusuke Sugano, Yasuyuki Matsushita		EYEDIAP (screen target)
, and Yoichi Sato. Appearance-based 		EYEDIAP (screen target)
gaze es- timation using visual 		EYEDIAP (screen target)
saliency. IEEE transactions on pattern 		EYEDIAP (screen target)
analysis and machine intelligence, 35(2):329–341, 2013		EYEDIAP (screen target)
.  [31] Yusuke Sugano, Yasuyuki Matsushita		EYEDIAP (screen target)
, and Yoichi Sato. Learning-by-synthesis 		EYEDIAP (screen target)
for appearance-based 3d gaze estimation. 		EYEDIAP (screen target)
In Computer Vision and Pattern 		EYEDIAP (screen target)
Recognition (CVPR), 2014 IEEE Conference 		EYEDIAP (screen target)
on, pages 1821–1828. IEEE, 2014.  [32] Kar-Han Tan, David J		EYEDIAP (screen target)
 Kriegman, and Narendra Ahuja. Appearance-based		EYEDIAP (screen target)
 eye gaze es- timation. In		EYEDIAP (screen target)
 Applications of Computer Vision, 2002.(WACV		EYEDIAP (screen target)
 2002). Proceedings. Sixth IEEE Workshop		EYEDIAP (screen target)
 on, pages 191–195. IEEE, 2002		EYEDIAP (screen target)
		EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 13		EYEDIAP (screen target)
		EYEDIAP (screen target)
33] Ronda Venkateswarlu et al. 		EYEDIAP (screen target)
Eye gaze estimation from a 		EYEDIAP (screen target)
single image of one eye. 		EYEDIAP (screen target)
In Computer Vision, 2003. Proceedings. 		EYEDIAP (screen target)
Ninth IEEE International Conference on, 		EYEDIAP (screen target)
pages 136–143. IEEE, 2003.  [34] Kang Wang and Qiang		EYEDIAP (screen target)
 Ji. Real time eye gaze		EYEDIAP (screen target)
 tracking with 3d deformable eye-face		EYEDIAP (screen target)
 model. In Proceedings of the		EYEDIAP (screen target)
 IEEE Conference on Computer Vision		EYEDIAP (screen target)
 and Pattern Recog- nition, pages		EYEDIAP (screen target)
 1003–1011, 2017		EYEDIAP (screen target)
.  [35] Oliver Williams, Andrew Blake		EYEDIAP (screen target)
, and Roberto Cipolla. Sparse 		EYEDIAP (screen target)
and semi-supervised visual mapping with 		EYEDIAP (screen target)
the sˆ 3gp. In Computer 		EYEDIAP (screen target)
Vision and Pattern Recognition, 2006 		EYEDIAP (screen target)
IEEE Computer Society Conference on, 		EYEDIAP (screen target)
volume 1, pages 230–237. IEEE, 2006		EYEDIAP (screen target)
.  [36] William Hyde Wollaston et		EYEDIAP (screen target)
 al. Xiii. on the apparent		EYEDIAP (screen target)
 direction of eyes in a		EYEDIAP (screen target)
 portrait. Philosophical Transactions of the		EYEDIAP (screen target)
 Royal Society of London, 114:247–256		EYEDIAP (screen target)
, 1824.  [37] Erroll Wood and Andreas		EYEDIAP (screen target)
 Bulling. Eyetab: Model-based gaze estimation		EYEDIAP (screen target)
 on unmodi- fied tablet computers		EYEDIAP (screen target)
. In Proceedings of the 		EYEDIAP (screen target)
Symposium on Eye Tracking Research 		EYEDIAP (screen target)
and Applications, pages 207–210. ACM, 2014		EYEDIAP (screen target)
.  [38] Erroll Wood, Tadas Baltrusaitis		EYEDIAP (screen target)
, Xucong Zhang, Yusuke Sugano, 		EYEDIAP (screen target)
Peter Robinson, and Andreas Bulling. 		EYEDIAP (screen target)
Rendering of eyes for eye-shape 		EYEDIAP (screen target)
registration and gaze estimation. In 		EYEDIAP (screen target)
Proceedings of the IEEE International 		EYEDIAP (screen target)
Conference on Computer Vision, pages 3756		EYEDIAP (screen target)
– 3764, 2015.  [39] Erroll Wood, Tadas Baltrušaitis		EYEDIAP (screen target)
, Louis-Philippe Morency, Peter Robinson, 		EYEDIAP (screen target)
and Andreas Bulling. A 3d 		EYEDIAP (screen target)
morphable eye region model for 		EYEDIAP (screen target)
gaze estimation. In European Confer- 		EYEDIAP (screen target)
ence on Computer Vision, pages 297		EYEDIAP (screen target)
–313. Springer, 2016.  [40] Erroll Wood, Tadas Baltrušaitis		EYEDIAP (screen target)
, Louis-Philippe Morency, Peter Robinson, 		EYEDIAP (screen target)
and Andreas Bulling. Learning an 		EYEDIAP (screen target)
appearance-based gaze estimator from one 		EYEDIAP (screen target)
million synthesised images. In Proceedings 		EYEDIAP (screen target)
of the Ninth Biennial ACM 		EYEDIAP (screen target)
Symposium on Eye Tracking Re- 		EYEDIAP (screen target)
search & Applications, pages 131–138. 		EYEDIAP (screen target)
ACM, 2016.  [41] Dong Hyun Yoo and		EYEDIAP (screen target)
 Myung Jin Chung. A novel		EYEDIAP (screen target)
 non-intrusive eye gaze estimation using		EYEDIAP (screen target)
 cross-ratio under large head motion		EYEDIAP (screen target)
. Computer Vision and Image 		EYEDIAP (screen target)
Understanding, 98(1):25–51, 2005.  [42] Xucong Zhang, Yusuke Sugano		EYEDIAP (screen target)
, Mario Fritz, and Andreas 		EYEDIAP (screen target)
Bulling. Appearance-based gaze estimation in 		EYEDIAP (screen target)
the wild. In Proceedings of 		EYEDIAP (screen target)
the IEEE Conference on Computer 		EYEDIAP (screen target)
Vision and Pattern Recognition, pages 4511		EYEDIAP (screen target)
–4520, 2015.  [43] Xucong Zhang, Yusuke Sugano		EYEDIAP (screen target)
, Mario Fritz, and Andreas 		EYEDIAP (screen target)
Bulling. It’s written all over 		EYEDIAP (screen target)
your face: Full-face appearance-based gaze 		EYEDIAP (screen target)
estimation. In Proc. IEEE International 		EYEDIAP (screen target)
Conference on Computer Vision and 		EYEDIAP (screen target)
Pattern Recognition Workshops (CVPRW), 2017		EYEDIAP (screen target)
		EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 1	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Recurrent CNN for 3D Gaze 	target)	EYEDIAP (screen target)
Estimation using Appearance and Shape 	target)	EYEDIAP (screen target)
Cues  Cristina Palmero1,2	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
crpalmec7@alumnes.ub.edu  1 Dept. Mathematics and Informatics	target)	EYEDIAP (screen target)
 Universitat de Barcelona, Spain	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Javier Selva1  javier.selva.castello@est.fib.upc.edu	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
2 Computer Vision Center Campus 	target)	EYEDIAP (screen target)
UAB, Bellaterra, Spain  Mohammad Ali Bagheri3,4	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
mohammadali.bagheri@ucalgary.ca  3 Dept. Electrical and Computer	target)	EYEDIAP (screen target)
 Eng. University of Calgary, Canada	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Sergio Escalera1,2  sergio@maia.ub.es	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
4 Dept. Engineering University of 	target)	EYEDIAP (screen target)
Larestan, Iran  Abstract	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Gaze behavior is an important 	target)	EYEDIAP (screen target)
non-verbal cue in social signal 	target)	EYEDIAP (screen target)
processing and human- computer interaction. 	target)	EYEDIAP (screen target)
In this paper, we tackle 	target)	EYEDIAP (screen target)
the problem of person- and 	target)	EYEDIAP (screen target)
head pose- independent 3D gaze 	target)	EYEDIAP (screen target)
estimation from remote cameras, using 	target)	EYEDIAP (screen target)
a multi-modal recurrent convolutional neural 	target)	EYEDIAP (screen target)
network (CNN). We propose to 	target)	EYEDIAP (screen target)
combine face, eyes region, and 	target)	EYEDIAP (screen target)
face landmarks as individual streams 	target)	EYEDIAP (screen target)
in a CNN to estimate 	target)	EYEDIAP (screen target)
gaze in still images. Then, 	target)	EYEDIAP (screen target)
we exploit the dynamic nature 	target)	EYEDIAP (screen target)
of gaze by feeding the 	target)	EYEDIAP (screen target)
learned features of all the 	target)	EYEDIAP (screen target)
frames in a sequence to 	target)	EYEDIAP (screen target)
a many-to-one recurrent module that 	target)	EYEDIAP (screen target)
predicts the 3D gaze vector 	target)	EYEDIAP (screen target)
of the last frame. Our 	target)	EYEDIAP (screen target)
multi-modal static solution is evaluated 	target)	EYEDIAP (screen target)
on a wide range of 	target)	EYEDIAP (screen target)
head poses and gaze directions, 	target)	EYEDIAP (screen target)
achieving a significant improvement of 14	target)	EYEDIAP (screen target)
.6% over the state of 	target)	EYEDIAP (screen target)
the art on EYEDIAP dataset, 	target)	EYEDIAP (screen target)
further improved by 4% when 	target)	EYEDIAP (screen target)
the temporal modality is included.  1 Introduction Eyes and their	target)	EYEDIAP (screen target)
 movements are considered an important	target)	EYEDIAP (screen target)
 cue in non-verbal behavior analysis	target)	EYEDIAP (screen target)
, being involved in many 	target)	EYEDIAP (screen target)
cognitive processes and reflecting our 	target)	EYEDIAP (screen target)
internal state [17]. More specifically, 	target)	EYEDIAP (screen target)
eye gaze behavior, as an 	target)	EYEDIAP (screen target)
indicator of human visual attention, 	target)	EYEDIAP (screen target)
has been widely studied to 	target)	EYEDIAP (screen target)
assess communication skills [28] and 	target)	EYEDIAP (screen target)
to identify possible behavioral 	target)	EYEDIAP (screen target)
disorders [9]. Therefore, gaze estimation 	target)	EYEDIAP (screen target)
has become an established line 	target)	EYEDIAP (screen target)
of research in computer vision, 	target)	EYEDIAP (screen target)
being a key feature in 	target)	EYEDIAP (screen target)
human-computer interaction (HCI) and usability 	target)	EYEDIAP (screen target)
research [12, 20].  Recent gaze estimation research has	target)	EYEDIAP (screen target)
 focused on facilitating its use	target)	EYEDIAP (screen target)
 in general everyday applications under	target)	EYEDIAP (screen target)
 real-world conditions, using off-the-shelf remote	target)	EYEDIAP (screen target)
 RGB cameras and re- moving	target)	EYEDIAP (screen target)
 the need of personal calibration	target)	EYEDIAP (screen target)
 [26]. In this setting, appearance-based	target)	EYEDIAP (screen target)
 methods, which learn a mapping	target)	EYEDIAP (screen target)
 from images to gaze directions	target)	EYEDIAP (screen target)
, are the preferred 	target)	EYEDIAP (screen target)
choice [25]. How- ever, they 	target)	EYEDIAP (screen target)
need large amounts of training 	target)	EYEDIAP (screen target)
data to be able to 	target)	EYEDIAP (screen target)
generalize well to in-the-wild situations, 	target)	EYEDIAP (screen target)
which are characterized by significant 	target)	EYEDIAP (screen target)
variability in head poses, face 	target)	EYEDIAP (screen target)
appearances and lighting conditions. In 	target)	EYEDIAP (screen target)
recent years, CNNs have been 	target)	EYEDIAP (screen target)
reported to outperform classical methods. 	target)	EYEDIAP (screen target)
However, most existing approaches have 	target)	EYEDIAP (screen target)
only been tested in restricted 	target)	EYEDIAP (screen target)
HCI tasks,  c© 2018. The copyright of	target)	EYEDIAP (screen target)
 this document resides with its	target)	EYEDIAP (screen target)
 authors. It may be distributed	target)	EYEDIAP (screen target)
 unchanged freely in print or	target)	EYEDIAP (screen target)
 electronic forms	target)	EYEDIAP (screen target)
.  ar X	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
iv :1  80 5	target)	EYEDIAP (screen target)
.  03 06	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
4v 3	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
cs  .C V	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
1  7	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Se  p	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
20  18	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Liversedge and Findlay} 2000	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Rutter and Durkin} 1987	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Guillon, Hadjikhani, Baduel, 	target)	EYEDIAP (screen target)
and Rog{é}} 2014  Citation Citation {Jacob and Karn	target)	EYEDIAP (screen target)
} 2003  Citation Citation {Majaranta and Bulling	target)	EYEDIAP (screen target)
} 2014  Citation Citation {Palmero, van Dam	target)	EYEDIAP (screen target)
, Escalera, Kelia, Lichtert, Noldus, 	target)	EYEDIAP (screen target)
Spink, and van Wieringen} 2018  Citation Citation {Ono, Okabe, and	target)	EYEDIAP (screen target)
 Sato} 2006	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
2 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (screen target)
ESTIMATION  Method 3D gaze direction	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Unrestricted gaze target  Full face	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Eye region  Facial landmarks	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Sequential information  Zhang et al. (1) [42	target)	EYEDIAP (screen target)
] 3 7 7 3 7 7 Krafka et al. [16	target)	EYEDIAP (screen target)
] 7 7 3 3 7 7 Zhang et al. (2	target)	EYEDIAP (screen target)
) [43] 3 7 3 7 7 7 Deng and Zhu	target)	EYEDIAP (screen target)
 [4] 3 3 3 3	target)	EYEDIAP (screen target)
 7 7 Ours 3 3	target)	EYEDIAP (screen target)
 3 3 3 3	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Table 1: Characteristics of recent 	target)	EYEDIAP (screen target)
related work on person- and 	target)	EYEDIAP (screen target)
head pose-independent appearance-based gaze estimation 	target)	EYEDIAP (screen target)
methods using CNNs.  where users look at the	target)	EYEDIAP (screen target)
 screen or mobile phone, showing	target)	EYEDIAP (screen target)
 a low head pose variability	target)	EYEDIAP (screen target)
. It is yet unclear 	target)	EYEDIAP (screen target)
how these methods would perform 	target)	EYEDIAP (screen target)
in a wider range of 	target)	EYEDIAP (screen target)
head poses.  On a different note, until	target)	EYEDIAP (screen target)
 very recently, the majority of	target)	EYEDIAP (screen target)
 methods only used static eye	target)	EYEDIAP (screen target)
 region appearance as input. State-of-the-art	target)	EYEDIAP (screen target)
 approaches have demonstrated that using	target)	EYEDIAP (screen target)
 the face along with a	target)	EYEDIAP (screen target)
 higher resolution image of the	target)	EYEDIAP (screen target)
 eyes [16], or even just	target)	EYEDIAP (screen target)
 the face itself [43], increases	target)	EYEDIAP (screen target)
 performance. Indeed, the whole-face image	target)	EYEDIAP (screen target)
 encodes more information than eyes	target)	EYEDIAP (screen target)
 alone, such as illumination and	target)	EYEDIAP (screen target)
 head pose. Nevertheless, gaze behavior	target)	EYEDIAP (screen target)
 is not static. Eye and	target)	EYEDIAP (screen target)
 head movements allow us to	target)	EYEDIAP (screen target)
 direct our gaze to target	target)	EYEDIAP (screen target)
 locations of interest. It has	target)	EYEDIAP (screen target)
 been demonstrated that humans can	target)	EYEDIAP (screen target)
 better predict gaze when being	target)	EYEDIAP (screen target)
 shown image sequences of other	target)	EYEDIAP (screen target)
 people moving their eyes [1	target)	EYEDIAP (screen target)
]. However, it is still 	target)	EYEDIAP (screen target)
an open question whether this 	target)	EYEDIAP (screen target)
se- quential information can increase 	target)	EYEDIAP (screen target)
the performance of automatic methods.  In this work, we show	target)	EYEDIAP (screen target)
 that the combination of multiple	target)	EYEDIAP (screen target)
 cues benefits the gaze estimation	target)	EYEDIAP (screen target)
 task. In particular, we use	target)	EYEDIAP (screen target)
 face, eye region and facial	target)	EYEDIAP (screen target)
 landmarks from still images. Facial	target)	EYEDIAP (screen target)
 landmarks model the global shape	target)	EYEDIAP (screen target)
 of the face and come	target)	EYEDIAP (screen target)
 at no cost, since face	target)	EYEDIAP (screen target)
 alignment is a common pre-processing	target)	EYEDIAP (screen target)
 step in many facial image	target)	EYEDIAP (screen target)
 analysis approaches. Furthermore, we present	target)	EYEDIAP (screen target)
 a subject-independent, free-head recurrent 3D	target)	EYEDIAP (screen target)
 gaze regression network to leverage	target)	EYEDIAP (screen target)
 the temporal information of image	target)	EYEDIAP (screen target)
 sequences. The static streams of	target)	EYEDIAP (screen target)
 each frame are combined in	target)	EYEDIAP (screen target)
 a late-fusion fashion using a	target)	EYEDIAP (screen target)
 multi-stream CNN. Then, all feature	target)	EYEDIAP (screen target)
 vectors are input to a	target)	EYEDIAP (screen target)
 many-to-one recurrent module that predicts	target)	EYEDIAP (screen target)
 the gaze vector of the	target)	EYEDIAP (screen target)
 last sequence frame	target)	EYEDIAP (screen target)
.  In summary, our contributions are	target)	EYEDIAP (screen target)
 two-fold. First, we present a	target)	EYEDIAP (screen target)
 Recurrent-CNN net- work architecture that	target)	EYEDIAP (screen target)
 combines appearance, shape and temporal	target)	EYEDIAP (screen target)
 information for 3D gaze estimation	target)	EYEDIAP (screen target)
. Second, we test static 	target)	EYEDIAP (screen target)
and temporal versions of our 	target)	EYEDIAP (screen target)
solution on the EYEDIAP 	target)	EYEDIAP (screen target)
dataset [7] in a wide 	target)	EYEDIAP (screen target)
range of head poses and 	target)	EYEDIAP (screen target)
gaze directions, showing consistent perfor- 	target)	EYEDIAP (screen target)
mance improvements compared to related 	target)	EYEDIAP (screen target)
appearance-based methods. To the best 	target)	EYEDIAP (screen target)
of our knowledge, this is 	target)	EYEDIAP (screen target)
the first third-person, remote camera-based 	target)	EYEDIAP (screen target)
approach that uses tempo- ral 	target)	EYEDIAP (screen target)
information for this task. Table 1 outlines our main method characteristics	target)	EYEDIAP (screen target)
 compared to related work. Models	target)	EYEDIAP (screen target)
 and code are publicly available	target)	EYEDIAP (screen target)
 at https://github.com/ crisie/RecurrentGaze	target)	EYEDIAP (screen target)
.  2 Related work Gaze estimation	target)	EYEDIAP (screen target)
 methods are typically categorized as	target)	EYEDIAP (screen target)
 model-based or appearance-based [5, 10	target)	EYEDIAP (screen target)
, 15]. Model-based approaches use 	target)	EYEDIAP (screen target)
a geometric model of the 	target)	EYEDIAP (screen target)
eye, usually requir- ing either 	target)	EYEDIAP (screen target)
high resolution images or a 	target)	EYEDIAP (screen target)
person-specific calibration stage to estimate 	target)	EYEDIAP (screen target)
personal eye parameters [22, 33, 34, 37, 41]. In contrast, appearance-based	target)	EYEDIAP (screen target)
 methods learn a di- rect	target)	EYEDIAP (screen target)
 mapping from intensity images or	target)	EYEDIAP (screen target)
 extracted eye features to gaze	target)	EYEDIAP (screen target)
 directions, thus being	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Zhang, Sugano, Fritz, 	target)	EYEDIAP (screen target)
and Bulling} 2015  Citation Citation {Krafka, Khosla, Kellnhofer	target)	EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 	target)	EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Deng and Zhu	target)	EYEDIAP (screen target)
} 2017  Citation Citation {Krafka, Khosla, Kellnhofer	target)	EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 	target)	EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Anderson, Risko, and	target)	EYEDIAP (screen target)
 Kingstone} 2016	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Funesprotect unhbox voidb@x 	target)	EYEDIAP (screen target)
penalty @M  {}Mora, Monay, and Odobez} 2014	target)	EYEDIAP (screen target)
{}  Citation Citation {Ferhat and Vilari{ñ}o	target)	EYEDIAP (screen target)
} 2016  Citation Citation {Hansen and Ji	target)	EYEDIAP (screen target)
} 2010  Citation Citation {Kar and Corcoran	target)	EYEDIAP (screen target)
} 2017  Citation Citation {Morimoto, Amir, and	target)	EYEDIAP (screen target)
 Flickner} 2002	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Venkateswarlu etprotect unhbox 	target)	EYEDIAP (screen target)
voidb@x penalty @M  {}al.} 2003	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Wang and Ji} 2017	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Wood and Bulling} 2014	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Yoo and Chung} 2005	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
https://github.com/crisie/RecurrentGaze 	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 3	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
potentially applicable to relatively low 	target)	EYEDIAP (screen target)
resolution images and mid-distance scenarios. 	target)	EYEDIAP (screen target)
Dif- ferent mapping functions have 	target)	EYEDIAP (screen target)
been explored, such as neural 	target)	EYEDIAP (screen target)
networks [2], adaptive linear regression (	target)	EYEDIAP (screen target)
ALR) [19], local interpolation [32], 	target)	EYEDIAP (screen target)
gaussian processes [30, 35], random 	target)	EYEDIAP (screen target)
forests [11, 31], or k-nearest 	target)	EYEDIAP (screen target)
neighbors [40]. Main challenges of 	target)	EYEDIAP (screen target)
appearance-based methods for 3D gaze 	target)	EYEDIAP (screen target)
estimation are head pose, illumination 	target)	EYEDIAP (screen target)
and subject invariance without user-specific 	target)	EYEDIAP (screen target)
calibration. To handle these issues, 	target)	EYEDIAP (screen target)
some works proposed compensation 	target)	EYEDIAP (screen target)
methods [18] and warping strategies 	target)	EYEDIAP (screen target)
that synthesize a canonical, frontal 	target)	EYEDIAP (screen target)
looking view of the 	target)	EYEDIAP (screen target)
face [6, 13, 21]. Hybrid 	target)	EYEDIAP (screen target)
approaches based on analysis-by-synthesis have 	target)	EYEDIAP (screen target)
also been evaluated [39].  Currently, data-driven methods are considered	target)	EYEDIAP (screen target)
 the state of the art	target)	EYEDIAP (screen target)
 for person- and head pose-independent	target)	EYEDIAP (screen target)
 appearance-based gaze estimation. Consequently, a	target)	EYEDIAP (screen target)
 number of gaze es- timation	target)	EYEDIAP (screen target)
 datasets have been introduced in	target)	EYEDIAP (screen target)
 recent years, either in controlled	target)	EYEDIAP (screen target)
 [29] or semi- controlled settings	target)	EYEDIAP (screen target)
 [8], in the wild [16	target)	EYEDIAP (screen target)
, 42], or consisting of 	target)	EYEDIAP (screen target)
synthetic data [31, 38, 40]. 	target)	EYEDIAP (screen target)
Zhang et al. [42] showed 	target)	EYEDIAP (screen target)
that CNNs can outperform other 	target)	EYEDIAP (screen target)
mapping methods, using a multi- 	target)	EYEDIAP (screen target)
modal CNN to learn the 	target)	EYEDIAP (screen target)
mapping from 3D head poses 	target)	EYEDIAP (screen target)
and eye images to 3D 	target)	EYEDIAP (screen target)
gaze directions. Krafka et 	target)	EYEDIAP (screen target)
al. [16] proposed a multi-stream 	target)	EYEDIAP (screen target)
CNN for 2D gaze estimation, 	target)	EYEDIAP (screen target)
using individual eye, whole-face image 	target)	EYEDIAP (screen target)
and the face grid as 	target)	EYEDIAP (screen target)
input. As this method was 	target)	EYEDIAP (screen target)
limited to 2D screen mapping, 	target)	EYEDIAP (screen target)
Zhang et al. [43] later 	target)	EYEDIAP (screen target)
explored the potential of just 	target)	EYEDIAP (screen target)
using whole-face images as input 	target)	EYEDIAP (screen target)
to estimate 3D gaze directions. 	target)	EYEDIAP (screen target)
Using a spatial weights CNN, 	target)	EYEDIAP (screen target)
they demonstrated their method to 	target)	EYEDIAP (screen target)
be more robust to facial 	target)	EYEDIAP (screen target)
appearance variation caused by head 	target)	EYEDIAP (screen target)
pose and illumina- tion than 	target)	EYEDIAP (screen target)
eye-only methods. While the method 	target)	EYEDIAP (screen target)
was evaluated in the wild, 	target)	EYEDIAP (screen target)
the subjects were only interacting 	target)	EYEDIAP (screen target)
with a mobile device, thus 	target)	EYEDIAP (screen target)
restricting the head pose range. 	target)	EYEDIAP (screen target)
Deng and Zhu [4] presented 	target)	EYEDIAP (screen target)
a two-stream CNN to disjointly 	target)	EYEDIAP (screen target)
model head pose from face 	target)	EYEDIAP (screen target)
images and eye- ball movement 	target)	EYEDIAP (screen target)
from eye region images. Both 	target)	EYEDIAP (screen target)
were then aggregated into 3D 	target)	EYEDIAP (screen target)
gaze direction using a gaze 	target)	EYEDIAP (screen target)
transform layer. The decomposition was 	target)	EYEDIAP (screen target)
aimed to avoid head-correlation over- 	target)	EYEDIAP (screen target)
fitting of previous data-driven approaches. 	target)	EYEDIAP (screen target)
They evaluated their approach in 	target)	EYEDIAP (screen target)
the wild with a wider 	target)	EYEDIAP (screen target)
range of head poses, obtaining 	target)	EYEDIAP (screen target)
better performance than previous eye-based 	target)	EYEDIAP (screen target)
methods. However, they did not 	target)	EYEDIAP (screen target)
test it on public annotated 	target)	EYEDIAP (screen target)
benchmark datasets.  In this paper, we propose	target)	EYEDIAP (screen target)
 a multi-stream recurrent CNN network	target)	EYEDIAP (screen target)
 for person- and head pose-independent	target)	EYEDIAP (screen target)
 3D gaze estimation for a	target)	EYEDIAP (screen target)
 mid-distance scenario. We evaluate it	target)	EYEDIAP (screen target)
 on a wider range of	target)	EYEDIAP (screen target)
 head poses and gaze directions	target)	EYEDIAP (screen target)
 than screen-targeted approaches. As opposed	target)	EYEDIAP (screen target)
 to previous methods, we also	target)	EYEDIAP (screen target)
 rely on temporal information inherent	target)	EYEDIAP (screen target)
 in sequential data	target)	EYEDIAP (screen target)
.  3 Methodology	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
In this section, we present 	target)	EYEDIAP (screen target)
our approach for 3D gaze 	target)	EYEDIAP (screen target)
regression based on appearance and 	target)	EYEDIAP (screen target)
shape cues for still images 	target)	EYEDIAP (screen target)
and image sequences. First, we 	target)	EYEDIAP (screen target)
introduce the data modalities and 	target)	EYEDIAP (screen target)
formulate the problem. Then, we 	target)	EYEDIAP (screen target)
detail the normalization procedure prior 	target)	EYEDIAP (screen target)
to the regression stage. Finally, 	target)	EYEDIAP (screen target)
we explain the global network 	target)	EYEDIAP (screen target)
topology as well as the 	target)	EYEDIAP (screen target)
implementation details. An overview of 	target)	EYEDIAP (screen target)
the system architecture is depicted 	target)	EYEDIAP (screen target)
in Figure 1.  3.1 Multi-modal gaze regression	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Let us represent gaze direction 	target)	EYEDIAP (screen target)
as a 3D unit vector 	target)	EYEDIAP (screen target)
g = [gx,gy,gz]T ∈R3 in 	target)	EYEDIAP (screen target)
the Camera Coor- dinate System (	target)	EYEDIAP (screen target)
CCS), whose origin is the 	target)	EYEDIAP (screen target)
central point between eyeball centers. 	target)	EYEDIAP (screen target)
Assuming a calibrated camera, and 	target)	EYEDIAP (screen target)
a known head position and 	target)	EYEDIAP (screen target)
orientation, our goal is to 	target)	EYEDIAP (screen target)
estimate g from a sequence 	target)	EYEDIAP (screen target)
of images {I(i) | 	target)	EYEDIAP (screen target)
I ∈ RW×H×3} as a 	target)	EYEDIAP (screen target)
regression problem.  Citation Citation {Baluja and Pomerleau	target)	EYEDIAP (screen target)
} 1994  Citation Citation {Lu, Sugano, Okabe	target)	EYEDIAP (screen target)
, and Sato} 2011{}  Citation Citation {Tan, Kriegman, and	target)	EYEDIAP (screen target)
 Ahuja} 2002	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Sugano, Matsushita, and 	target)	EYEDIAP (screen target)
Sato} 2013  Citation Citation {Williams, Blake, and	target)	EYEDIAP (screen target)
 Cipolla} 2006	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Huang, Veeraraghavan, and 	target)	EYEDIAP (screen target)
Sabharwal} 2017  Citation Citation {Sugano, Matsushita, and	target)	EYEDIAP (screen target)
 Sato} 2014	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Wood, Baltru{²}aitis, Morency, 	target)	EYEDIAP (screen target)
Robinson, and Bulling} 2016{}  Citation Citation {Lu, Okabe, Sugano	target)	EYEDIAP (screen target)
, and Sato} 2011{}  Citation Citation {Funes-Mora and Odobez	target)	EYEDIAP (screen target)
} 2016  Citation Citation {Jeni and Cohn	target)	EYEDIAP (screen target)
} 2016  Citation Citation {Mora and Odobez	target)	EYEDIAP (screen target)
} 2012  Citation Citation {Wood, Baltru{²}aitis, Morency	target)	EYEDIAP (screen target)
, Robinson, and Bulling} 2016{}  Citation Citation {Smith, Yin, Feiner	target)	EYEDIAP (screen target)
, and Nayar} 2013  Citation Citation {Funesprotect unhbox voidb@x	target)	EYEDIAP (screen target)
 penalty @M	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Mora, Monay, and Odobez} 2014{}  Citation Citation {Krafka, Khosla, Kellnhofer	target)	EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 	target)	EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (screen target)
, and Bulling} 2015  Citation Citation {Sugano, Matsushita, and	target)	EYEDIAP (screen target)
 Sato} 2014	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Wood, Baltrusaitis, Zhang, 	target)	EYEDIAP (screen target)
Sugano, Robinson, and Bulling} 2015  Citation Citation {Wood, Baltru{²}aitis, Morency	target)	EYEDIAP (screen target)
, Robinson, and Bulling} 2016{}  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (screen target)
, and Bulling} 2015  Citation Citation {Krafka, Khosla, Kellnhofer	target)	EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 	target)	EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Deng and Zhu	target)	EYEDIAP (screen target)
} 2017	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
4 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (screen target)
ESTIMATION  Conv	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
C on ca t  x y z x y	target)	EYEDIAP (screen target)
 z x y z	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Individual Fusion Temporal  Individual Fusion	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Input 	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Individual Fusion  Normalization	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
 .Conv	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Conv .  Conv	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Conv .  FC	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
FC FC RNN  RNN	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
RNN FC  Ti m e	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Figure 1: Overview of the 	target)	EYEDIAP (screen target)
proposed network. A multi-stream CNN 	target)	EYEDIAP (screen target)
jointly models full-face, eye region 	target)	EYEDIAP (screen target)
appearance and face landmarks from 	target)	EYEDIAP (screen target)
still images. The combined extracted 	target)	EYEDIAP (screen target)
fea- tures from each frame 	target)	EYEDIAP (screen target)
are fed into a recurrent 	target)	EYEDIAP (screen target)
module to predict last frame’s 	target)	EYEDIAP (screen target)
gaze direction.  Gazing to a specific target	target)	EYEDIAP (screen target)
 is achieved by a combination	target)	EYEDIAP (screen target)
 of eye and head movements	target)	EYEDIAP (screen target)
, which are highly coordinated. 	target)	EYEDIAP (screen target)
Consequently, the apparent direction of 	target)	EYEDIAP (screen target)
gaze is influenced not only 	target)	EYEDIAP (screen target)
by the location of the 	target)	EYEDIAP (screen target)
irises within the eyelid aperture, 	target)	EYEDIAP (screen target)
but also by the position 	target)	EYEDIAP (screen target)
and orientation of the face 	target)	EYEDIAP (screen target)
with respect to the camera. 	target)	EYEDIAP (screen target)
Known as the Wollaston 	target)	EYEDIAP (screen target)
effect [36], the exact same 	target)	EYEDIAP (screen target)
set of eyes may appear 	target)	EYEDIAP (screen target)
to be looking in different 	target)	EYEDIAP (screen target)
directions due to the surrounding 	target)	EYEDIAP (screen target)
facial cues. It is therefore 	target)	EYEDIAP (screen target)
reasonable to state that eye 	target)	EYEDIAP (screen target)
images are not sufficient to 	target)	EYEDIAP (screen target)
estimate gaze direction. Instead, whole-face 	target)	EYEDIAP (screen target)
images can encode head pose 	target)	EYEDIAP (screen target)
or illumination-specific information across larger 	target)	EYEDIAP (screen target)
areas than those available just 	target)	EYEDIAP (screen target)
in the eyes region [16, 43	target)	EYEDIAP (screen target)
].  The drawback of appearance-only methods	target)	EYEDIAP (screen target)
 is that global structure information	target)	EYEDIAP (screen target)
 is not explicitly considered. In	target)	EYEDIAP (screen target)
 that sense, facial landmarks can	target)	EYEDIAP (screen target)
 be used as global shape	target)	EYEDIAP (screen target)
 cues to en- code spatial	target)	EYEDIAP (screen target)
 relationships and geometric constraints. Current	target)	EYEDIAP (screen target)
 state-of-the-art face alignment approaches are	target)	EYEDIAP (screen target)
 robust enough to handle large	target)	EYEDIAP (screen target)
 appearance variability, extreme head poses	target)	EYEDIAP (screen target)
 and occlusions, being especially useful	target)	EYEDIAP (screen target)
 when the dataset used for	target)	EYEDIAP (screen target)
 gaze estimation does not contain	target)	EYEDIAP (screen target)
 such variability. Facial landmarks are	target)	EYEDIAP (screen target)
 mainly correlated with head orientation	target)	EYEDIAP (screen target)
, eye position, eyelid openness, 	target)	EYEDIAP (screen target)
and eyebrow movement, which are 	target)	EYEDIAP (screen target)
valuable features for our task.  Therefore, in our approach we	target)	EYEDIAP (screen target)
 jointly model appearance and shape	target)	EYEDIAP (screen target)
 cues (see Figure 1). The	target)	EYEDIAP (screen target)
 former is represented by a	target)	EYEDIAP (screen target)
 whole-face image IF , along	target)	EYEDIAP (screen target)
 with a higher resolution image	target)	EYEDIAP (screen target)
 of the eyes IE to	target)	EYEDIAP (screen target)
 identify subtle changes. Due to	target)	EYEDIAP (screen target)
 dealing with wide head pose	target)	EYEDIAP (screen target)
 ranges, some eye images may	target)	EYEDIAP (screen target)
 not depict the whole eye	target)	EYEDIAP (screen target)
, containing mostly background or 	target)	EYEDIAP (screen target)
other surrounding facial parts instead. 	target)	EYEDIAP (screen target)
For that reason, and contrary 	target)	EYEDIAP (screen target)
to previous approaches that only 	target)	EYEDIAP (screen target)
use one eye image [31, 42	target)	EYEDIAP (screen target)
], we use a single 	target)	EYEDIAP (screen target)
image composed of two patches 	target)	EYEDIAP (screen target)
of centered left and right 	target)	EYEDIAP (screen target)
eyes. Finally, the shape cue 	target)	EYEDIAP (screen target)
is represented by 3D face 	target)	EYEDIAP (screen target)
landmarks obtained from a 68-landmark 	target)	EYEDIAP (screen target)
model, denoted by 	target)	EYEDIAP (screen target)
L = {(lx, ly, 	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
)	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
 | ∀c ∈ [1, ...,68	target)	EYEDIAP (screen target)
]}.  In this work we also	target)	EYEDIAP (screen target)
 consider the dynamic component of	target)	EYEDIAP (screen target)
 gaze. We leverage the se	target)	EYEDIAP (screen target)
- quential information of eye 	target)	EYEDIAP (screen target)
and head movements such that, 	target)	EYEDIAP (screen target)
given appearance and shape features 	target)	EYEDIAP (screen target)
of consecutive frames, it is 	target)	EYEDIAP (screen target)
possible to better predict the 	target)	EYEDIAP (screen target)
gaze direction of the cur- 	target)	EYEDIAP (screen target)
rent frame. Therefore, the 3D 	target)	EYEDIAP (screen target)
gaze estimation task for a 1	target)	EYEDIAP (screen target)
-frame sequence is formulated  Citation Citation {Wollaston etprotect unhbox	target)	EYEDIAP (screen target)
 voidb@x penalty @M	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
al.} 1824  Citation Citation {Krafka, Khosla, Kellnhofer	target)	EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 	target)	EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Sugano, Matsushita, and	target)	EYEDIAP (screen target)
 Sato} 2014	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Zhang, Sugano, Fritz, 	target)	EYEDIAP (screen target)
and Bulling} 2015	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 5	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
as g(i) = f ( {IF (i)},{IE (i)},{L(i	target)	EYEDIAP (screen target)
)}  ) , where i denotes	target)	EYEDIAP (screen target)
 the i-th frame, and f	target)	EYEDIAP (screen target)
 is the regression	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
function.  3.2 Data normalization Prior to	target)	EYEDIAP (screen target)
 gaze regression, a normalization step	target)	EYEDIAP (screen target)
 in the 3D space and	target)	EYEDIAP (screen target)
 the 2D image, similar to	target)	EYEDIAP (screen target)
 [31], is carried out. This	target)	EYEDIAP (screen target)
 is performed to reduce the	target)	EYEDIAP (screen target)
 appearance variability and to allow	target)	EYEDIAP (screen target)
 the gaze estimation model to	target)	EYEDIAP (screen target)
 be applied regardless of the	target)	EYEDIAP (screen target)
 original camera configuration	target)	EYEDIAP (screen target)
.  Let H ∈ R3x3 be	target)	EYEDIAP (screen target)
 the head rotation matrix, and	target)	EYEDIAP (screen target)
 p = [px, py, pz]T	target)	EYEDIAP (screen target)
 ∈ R3 the reference face	target)	EYEDIAP (screen target)
 location with respect to the	target)	EYEDIAP (screen target)
 original CCS. The goal is	target)	EYEDIAP (screen target)
 to find the conversion matrix	target)	EYEDIAP (screen target)
 M = SR such that	target)	EYEDIAP (screen target)
 (a) the X-axes of the	target)	EYEDIAP (screen target)
 virtual camera and the head	target)	EYEDIAP (screen target)
 become parallel using the rotation	target)	EYEDIAP (screen target)
 matrix R, and (b) the	target)	EYEDIAP (screen target)
 virtual camera looks at the	target)	EYEDIAP (screen target)
 reference location from a fixed	target)	EYEDIAP (screen target)
 distance dn using the Z-direction	target)	EYEDIAP (screen target)
 scaling matrix S = diag(1,1,dn/‖p	target)	EYEDIAP (screen target)
‖). R is computed as 	target)	EYEDIAP (screen target)
a = p̂×HT e1, 	target)	EYEDIAP (screen target)
b = â× p̂, 	target)	EYEDIAP (screen target)
R = [â, b̂, p̂]T , where e1 denotes the first	target)	EYEDIAP (screen target)
 orthonormal basis and	target)	EYEDIAP (screen target)
 〈 ·̂ 〉 is the	target)	EYEDIAP (screen target)
 unit vector	target)	EYEDIAP (screen target)
.  This normalization translates into the	target)	EYEDIAP (screen target)
 image space as a cropped	target)	EYEDIAP (screen target)
 image patch of size Wn×Hn	target)	EYEDIAP (screen target)
 centered at p where head	target)	EYEDIAP (screen target)
 roll rotation has been removed	target)	EYEDIAP (screen target)
. This is done by 	target)	EYEDIAP (screen target)
applying a perspective warping to 	target)	EYEDIAP (screen target)
the input image I using 	target)	EYEDIAP (screen target)
the transformation matrix W = 	target)	EYEDIAP (screen target)
CoMCn−1, where Co and Cn 	target)	EYEDIAP (screen target)
are the original and virtual 	target)	EYEDIAP (screen target)
camera matrices, respectively.  The 3D gaze vector is	target)	EYEDIAP (screen target)
 also normalized as gn =Rg	target)	EYEDIAP (screen target)
. After image normalization, the 	target)	EYEDIAP (screen target)
line of sight can be 	target)	EYEDIAP (screen target)
represented in a 2D space. 	target)	EYEDIAP (screen target)
Therefore, gn is further transformed 	target)	EYEDIAP (screen target)
to spherical coor- dinates (θ ,	target)	EYEDIAP (screen target)
φ) assuming unit length, where 	target)	EYEDIAP (screen target)
θ and φ denote the 	target)	EYEDIAP (screen target)
horizontal and vertical direc- tion 	target)	EYEDIAP (screen target)
angles, respectively. This 2D angle 	target)	EYEDIAP (screen target)
representation, delimited in the 	target)	EYEDIAP (screen target)
range [−π/2,π/2], is computed as 	target)	EYEDIAP (screen target)
θ = arctan(gx/gz) and 	target)	EYEDIAP (screen target)
φ = arcsin(−gy), such that (0,	target)	EYEDIAP (screen target)
0) represents looking straight ahead 	target)	EYEDIAP (screen target)
to the CCS origin.  3.3 Recurrent Convolutional Neural Network	target)	EYEDIAP (screen target)
 We propose a Recurrent CNN	target)	EYEDIAP (screen target)
 Regression Network for 3D gaze	target)	EYEDIAP (screen target)
 estimation. The network is divided	target)	EYEDIAP (screen target)
 in 3 modules: (1) Individual	target)	EYEDIAP (screen target)
, (2) Fusion, and (3) 	target)	EYEDIAP (screen target)
Temporal.  First, the Individual module learns	target)	EYEDIAP (screen target)
 features from each appearance cue	target)	EYEDIAP (screen target)
 separately. It consists of a	target)	EYEDIAP (screen target)
 two-stream CNN, one devoted to	target)	EYEDIAP (screen target)
 the normalized face image stream	target)	EYEDIAP (screen target)
 and the other to the	target)	EYEDIAP (screen target)
 joint normalized eyes image. Next	target)	EYEDIAP (screen target)
, the Fusion module combines 	target)	EYEDIAP (screen target)
the extracted features of each 	target)	EYEDIAP (screen target)
appearance stream in a single 	target)	EYEDIAP (screen target)
vector along with the normalized 	target)	EYEDIAP (screen target)
landmark coordinates. Then, it learns 	target)	EYEDIAP (screen target)
a joint representation between modalities 	target)	EYEDIAP (screen target)
in a late-fusion fashion. Both 	target)	EYEDIAP (screen target)
Individual and Fusion modules, further 	target)	EYEDIAP (screen target)
referred to as Static model, 	target)	EYEDIAP (screen target)
are applied to each frame 	target)	EYEDIAP (screen target)
of the sequence. Finally, the 	target)	EYEDIAP (screen target)
resulting feature vectors of each 	target)	EYEDIAP (screen target)
frame are input to the 	target)	EYEDIAP (screen target)
Temporal module based on a 	target)	EYEDIAP (screen target)
many-to-one recurrent network. This module 	target)	EYEDIAP (screen target)
leverages sequential information to predict 	target)	EYEDIAP (screen target)
the normalized 2D gaze angles 	target)	EYEDIAP (screen target)
of the last frame of 	target)	EYEDIAP (screen target)
the sequence using a linear 	target)	EYEDIAP (screen target)
regression layer added on top 	target)	EYEDIAP (screen target)
of it.  3.4 Implementation details 3.4.1 Network	target)	EYEDIAP (screen target)
 details	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Each stream of the Individual 	target)	EYEDIAP (screen target)
module is based on the 	target)	EYEDIAP (screen target)
VGG-16 deep network [27], consisting 	target)	EYEDIAP (screen target)
of 13 convolutional layers, 5 	target)	EYEDIAP (screen target)
max pooling layers, and 1 	target)	EYEDIAP (screen target)
fully connected (FC) layer with 	target)	EYEDIAP (screen target)
Rec- tified Linear Unit (ReLU) 	target)	EYEDIAP (screen target)
activations. The full-face stream follows 	target)	EYEDIAP (screen target)
the same configuration  Citation Citation {Sugano, Matsushita, and	target)	EYEDIAP (screen target)
 Sato} 2014	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Parkhi, Vedaldi, and 	target)	EYEDIAP (screen target)
Zisserman} 2015	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
6 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (screen target)
ESTIMATION  as the base network, having	target)	EYEDIAP (screen target)
 an input of 224×224 pixels	target)	EYEDIAP (screen target)
 and a 4096D FC layer	target)	EYEDIAP (screen target)
. In contrast, the input 	target)	EYEDIAP (screen target)
joint eye image is smaller, 	target)	EYEDIAP (screen target)
with a final size of 120	target)	EYEDIAP (screen target)
×48 pixels, so the number 	target)	EYEDIAP (screen target)
of pa- rameters is decreased 	target)	EYEDIAP (screen target)
proportionally. In this case, its 	target)	EYEDIAP (screen target)
last FC layer produces a 	target)	EYEDIAP (screen target)
1536D vector. A 204D landmark 	target)	EYEDIAP (screen target)
coordinates vector is concatenated to 	target)	EYEDIAP (screen target)
the output of the FC 	target)	EYEDIAP (screen target)
layer of each stream, resulting 	target)	EYEDIAP (screen target)
in a 5836D feature vector. 	target)	EYEDIAP (screen target)
Consequently, the Fusion module consists 	target)	EYEDIAP (screen target)
of 2 5836D FC layers 	target)	EYEDIAP (screen target)
with ReLU activations and 2 	target)	EYEDIAP (screen target)
dropout layers between FCs as 	target)	EYEDIAP (screen target)
regularization. Finally, to model the 	target)	EYEDIAP (screen target)
temporal dependencies, we use a 	target)	EYEDIAP (screen target)
single GRU layer with 128 	target)	EYEDIAP (screen target)
units.  The network is trained in	target)	EYEDIAP (screen target)
 a stage-wise fashion. First, we	target)	EYEDIAP (screen target)
 train the Static model and	target)	EYEDIAP (screen target)
 the final regression layer end-to-end	target)	EYEDIAP (screen target)
 on each individual frame of	target)	EYEDIAP (screen target)
 the training data. The convolutional	target)	EYEDIAP (screen target)
 blocks are pre-trained with the	target)	EYEDIAP (screen target)
 VGG-Face dataset [27], whereas the	target)	EYEDIAP (screen target)
 FCs are trained from scratch	target)	EYEDIAP (screen target)
. Second, the training data 	target)	EYEDIAP (screen target)
is re-arranged by means of 	target)	EYEDIAP (screen target)
a sliding window with stride 1 to build input sequences. Each	target)	EYEDIAP (screen target)
 sequence is composed of s	target)	EYEDIAP (screen target)
 = 4 consecutive frames, whose	target)	EYEDIAP (screen target)
 gaze direction target is the	target)	EYEDIAP (screen target)
 gaze direction of the last	target)	EYEDIAP (screen target)
 frame of the sequence( {I(i−s+1	target)	EYEDIAP (screen target)
), . . . ,I(i)}, 	target)	EYEDIAP (screen target)
g(i)  ) . Using this re-arranged	target)	EYEDIAP (screen target)
 training data, we extract features	target)	EYEDIAP (screen target)
 of each	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
frame of the sequence from 	target)	EYEDIAP (screen target)
a frozen Individual module, fine-tune 	target)	EYEDIAP (screen target)
the Fusion layers, and train 	target)	EYEDIAP (screen target)
both, the Temporal module and 	target)	EYEDIAP (screen target)
a new final regression layer 	target)	EYEDIAP (screen target)
from scratch. This way, the 	target)	EYEDIAP (screen target)
network can exploit the temporal 	target)	EYEDIAP (screen target)
information to further refine the 	target)	EYEDIAP (screen target)
fusion weights.  We trained the model using	target)	EYEDIAP (screen target)
 ADAM optimizer with an initial	target)	EYEDIAP (screen target)
 learning rate of 0.0001, dropout	target)	EYEDIAP (screen target)
 of 0.3, and batch size	target)	EYEDIAP (screen target)
 of 64 frames. The number	target)	EYEDIAP (screen target)
 of epochs was experimentally set	target)	EYEDIAP (screen target)
 to 21 for the first	target)	EYEDIAP (screen target)
 training stage and 10 for	target)	EYEDIAP (screen target)
 the second. We use the	target)	EYEDIAP (screen target)
 average Euclidean distance between the	target)	EYEDIAP (screen target)
 predicted and ground-truth 3D gaze	target)	EYEDIAP (screen target)
 vectors as loss function	target)	EYEDIAP (screen target)
.  3.4.2 Input pre-processing	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
For this work we use 	target)	EYEDIAP (screen target)
head pose and eye locations 	target)	EYEDIAP (screen target)
in the 3D scene provided 	target)	EYEDIAP (screen target)
by the dataset. The 3D 	target)	EYEDIAP (screen target)
landmarks are extracted using the 	target)	EYEDIAP (screen target)
state-of-the-art method of Bulat and 	target)	EYEDIAP (screen target)
Tzimiropou- los [3], which is 	target)	EYEDIAP (screen target)
based on stacked hourglass 	target)	EYEDIAP (screen target)
networks [24].  During training, the original image	target)	EYEDIAP (screen target)
 is pre-processed to get the	target)	EYEDIAP (screen target)
 two normalized input images. The	target)	EYEDIAP (screen target)
 normalized whole-face patch is centered	target)	EYEDIAP (screen target)
 0.1 meters ahead of the	target)	EYEDIAP (screen target)
 head center in the head	target)	EYEDIAP (screen target)
 coordinate system, and Cn is	target)	EYEDIAP (screen target)
 defined such that the image	target)	EYEDIAP (screen target)
 has size of 250× 250	target)	EYEDIAP (screen target)
 pixels. The difference between this	target)	EYEDIAP (screen target)
 size and the final input	target)	EYEDIAP (screen target)
 size allows us to perform	target)	EYEDIAP (screen target)
 random cropping and zooming to	target)	EYEDIAP (screen target)
 augment the data (explained in	target)	EYEDIAP (screen target)
 Section 4.1). Similarly, each normalized	target)	EYEDIAP (screen target)
 eye patch is centered in	target)	EYEDIAP (screen target)
 their respective eye center locations	target)	EYEDIAP (screen target)
. In this case, the 	target)	EYEDIAP (screen target)
virtual camera matrix is defined 	target)	EYEDIAP (screen target)
so that the image is 	target)	EYEDIAP (screen target)
cropped to 70×58, while in 	target)	EYEDIAP (screen target)
practice the final patches have 	target)	EYEDIAP (screen target)
size of 60×48. Landmarks are 	target)	EYEDIAP (screen target)
normalized using the same procedure 	target)	EYEDIAP (screen target)
and further pre-processed with mean 	target)	EYEDIAP (screen target)
subtraction and min-max normalization per 	target)	EYEDIAP (screen target)
axis. Finally, we divide them 	target)	EYEDIAP (screen target)
by a scaling factor w 	target)	EYEDIAP (screen target)
such that all coordinates are 	target)	EYEDIAP (screen target)
in the range [0,w]. This 	target)	EYEDIAP (screen target)
way, all concatenated feature values 	target)	EYEDIAP (screen target)
are in a similar range. 	target)	EYEDIAP (screen target)
After inference, the predicted normalized 	target)	EYEDIAP (screen target)
2D angles are de-normalized back 	target)	EYEDIAP (screen target)
to the original 3D space.  4 Experiments In this section	target)	EYEDIAP (screen target)
, we evaluate the cross-subject 	target)	EYEDIAP (screen target)
3D gaze estimation task on 	target)	EYEDIAP (screen target)
a wide range of head 	target)	EYEDIAP (screen target)
poses and gaze directions. Furthermore, 	target)	EYEDIAP (screen target)
we validate the effectiveness of 	target)	EYEDIAP (screen target)
the proposed architecture comparing both 	target)	EYEDIAP (screen target)
static and temporal approaches. We 	target)	EYEDIAP (screen target)
report the error in terms 	target)	EYEDIAP (screen target)
of mean angular error between 	target)	EYEDIAP (screen target)
predicted and ground-truth 3D gaze 	target)	EYEDIAP (screen target)
vectors. Note that due to 	target)	EYEDIAP (screen target)
the requirements of the temporal 	target)	EYEDIAP (screen target)
model not all the frames 	target)	EYEDIAP (screen target)
obtain a prediction. Therefore, for 	target)	EYEDIAP (screen target)
a  Citation Citation {Parkhi, Vedaldi, and	target)	EYEDIAP (screen target)
 Zisserman} 2015	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Bulat and Tzimiropoulos} 2017	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Citation Citation {Newell, Yang, and 	target)	EYEDIAP (screen target)
Deng} 2016	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 7	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
60 30 0 30 60  60	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
30  0	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
30  60	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
100  101	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
102  60 30 0 30 60	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
60  30	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
0  30	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
60  100	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
101  102	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
103  60 30 0 30 60	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
60  30	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
0  30	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
60  100	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
101  102	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
60 30 0 30 60  60	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
30  0	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
30  60	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
100  101	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
102  103	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
a) g (FT ) (b) 	target)	EYEDIAP (screen target)
h (FT ) (c) g (	target)	EYEDIAP (screen target)
CS) (d) h (CS)  Figure 2: Ground-truth eye gaze	target)	EYEDIAP (screen target)
 g and head orientation h	target)	EYEDIAP (screen target)
 distribution on the filtered EYE	target)	EYEDIAP (screen target)
- DIAP dataset for CS 	target)	EYEDIAP (screen target)
and FT settings, in terms 	target)	EYEDIAP (screen target)
of x- and y- angles.  fair comparison, the reported results	target)	EYEDIAP (screen target)
 for static models disregard such	target)	EYEDIAP (screen target)
 frames when temporal models are	target)	EYEDIAP (screen target)
 included in the comparison	target)	EYEDIAP (screen target)
.  4.1 Training data	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
There are few publicly available 	target)	EYEDIAP (screen target)
datasets devoted to 3D gaze 	target)	EYEDIAP (screen target)
estimation and most of them 	target)	EYEDIAP (screen target)
focus on HCI with a 	target)	EYEDIAP (screen target)
limited range of head pose 	target)	EYEDIAP (screen target)
and gaze directions. Therefore, we 	target)	EYEDIAP (screen target)
use VGA videos from the 	target)	EYEDIAP (screen target)
publicly-available EYEDIAP dataset [7] to 	target)	EYEDIAP (screen target)
perform the experimental evaluation, as 	target)	EYEDIAP (screen target)
it is currently the only 	target)	EYEDIAP (screen target)
one containing video sequences with 	target)	EYEDIAP (screen target)
a wide range of head 	target)	EYEDIAP (screen target)
poses and showing the full 	target)	EYEDIAP (screen target)
face. This dataset consists of 3	target)	EYEDIAP (screen target)
-minute videos of 16 subjects 	target)	EYEDIAP (screen target)
looking at two types of 	target)	EYEDIAP (screen target)
targets: continuous screen targets on 	target)	EYEDIAP (screen target)
a fixed monitor (CS), and 	target)	EYEDIAP (screen target)
floating physical targets (FT ). 	target)	EYEDIAP (screen target)
The videos are further divided 	target)	EYEDIAP (screen target)
into static (S) and moving (	target)	EYEDIAP (screen target)
M) head pose for each 	target)	EYEDIAP (screen target)
of the subjects. Subjects 12-16 	target)	EYEDIAP (screen target)
were recorded with 2 different 	target)	EYEDIAP (screen target)
lighting conditions.  For evaluation, we filtered out	target)	EYEDIAP (screen target)
 those frames that fulfilled at	target)	EYEDIAP (screen target)
 least one of the following	target)	EYEDIAP (screen target)
 conditions: (1) face or landmarks	target)	EYEDIAP (screen target)
 not detected; (2) subject not	target)	EYEDIAP (screen target)
 looking at the target; (3	target)	EYEDIAP (screen target)
) 3D head pose, eyes 	target)	EYEDIAP (screen target)
or target location not properly 	target)	EYEDIAP (screen target)
recovered; and (4) eyeball rotations 	target)	EYEDIAP (screen target)
violating physical 	target)	EYEDIAP (screen target)
constraints (|θ | ≤ 40	target)	EYEDIAP (screen target)
◦, |φ | ≤ 30	target)	EYEDIAP (screen target)
◦) [23]. Note that we 	target)	EYEDIAP (screen target)
purposely do not filter eye 	target)	EYEDIAP (screen target)
blinking moments to learn their 	target)	EYEDIAP (screen target)
dynamics with the temporal model, 	target)	EYEDIAP (screen target)
which may produce some outliers 	target)	EYEDIAP (screen target)
with a higher prediction error 	target)	EYEDIAP (screen target)
due to a less accurate 	target)	EYEDIAP (screen target)
ground truth. Figure 2 shows 	target)	EYEDIAP (screen target)
the distribution of gaze directions 	target)	EYEDIAP (screen target)
and head poses for both 	target)	EYEDIAP (screen target)
filtered CS and FT cases.  We applied data augmentation to	target)	EYEDIAP (screen target)
 the training set with the	target)	EYEDIAP (screen target)
 following random transforma- tions: horizontal	target)	EYEDIAP (screen target)
 flip, shifts of up to	target)	EYEDIAP (screen target)
 5 pixels, zoom of up	target)	EYEDIAP (screen target)
 to 2%, brightness changes by	target)	EYEDIAP (screen target)
 a factor in the range	target)	EYEDIAP (screen target)
 [0.4,1.75], and additive Gaussian noise	target)	EYEDIAP (screen target)
 with σ2 = 0.03	target)	EYEDIAP (screen target)
.  4.2 Evaluation of static modalities	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
First, we evaluate the contribution 	target)	EYEDIAP (screen target)
of each static modality on 	target)	EYEDIAP (screen target)
the FT scenario. We divided 	target)	EYEDIAP (screen target)
the 16 participants into 4 	target)	EYEDIAP (screen target)
groups, such that appearance variability 	target)	EYEDIAP (screen target)
was maximized while maintaining a 	target)	EYEDIAP (screen target)
similar number of training samples 	target)	EYEDIAP (screen target)
per group. Each static model 	target)	EYEDIAP (screen target)
was trained end-to-end performing 4-fold 	target)	EYEDIAP (screen target)
cross-validation using different combinations of 	target)	EYEDIAP (screen target)
input modal- ities. Since the 	target)	EYEDIAP (screen target)
number of fusion units depends 	target)	EYEDIAP (screen target)
on the number of input 	target)	EYEDIAP (screen target)
modalities, we also compare different 	target)	EYEDIAP (screen target)
fusion layer sizes. The effect 	target)	EYEDIAP (screen target)
of data normalization is also 	target)	EYEDIAP (screen target)
evaluated by training a not-normalized 	target)	EYEDIAP (screen target)
face model where the input 	target)	EYEDIAP (screen target)
image is the face bounding 	target)	EYEDIAP (screen target)
box with square size the 	target)	EYEDIAP (screen target)
maximum distance between 2D landmarks.  Citation Citation {Funesprotect unhbox voidb@x	target)	EYEDIAP (screen target)
 penalty @M	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Mora, Monay, and Odobez} 2014{}  Citation Citation {MSC	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
8 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (screen target)
ESTIMATION  0 1 2 3 4	target)	EYEDIAP (screen target)
 5 6 7 8 9	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
10 11  An gl	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
e  er	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
ro r (  de gr	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
ee s)  6.9 6.43 5.58 5.71 5.59	target)	EYEDIAP (screen target)
 5.55 5.52	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
OF-4096 NE-1536 NF-4096  NF-5632 NFL-4300	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
NFE-5632 NFEL-5836  Figure 3: Performance evaluation of	target)	EYEDIAP (screen target)
 the Static network using different	target)	EYEDIAP (screen target)
 input modali- ties (O	target)	EYEDIAP (screen target)
 - Not normalized, N	target)	EYEDIAP (screen target)
 - Normalized, F - Face	target)	EYEDIAP (screen target)
, E - Eyes, 	target)	EYEDIAP (screen target)
L - 3D Landmarks) and 	target)	EYEDIAP (screen target)
size of fusion layers on 	target)	EYEDIAP (screen target)
the FT scenario.  Floating Target Screen Target 0	target)	EYEDIAP (screen target)
 1 2 3 4 5	target)	EYEDIAP (screen target)
 6 7 8 9	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
10 11  An gl	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
e  er	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
ro r (  de gr	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
ee s)  6.36 5.43 5.19 4.2 3.38	target)	EYEDIAP (screen target)
 3.4	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
MPIIGaze Static Temporal  Figure 4: Performance comparison among	target)	EYEDIAP (screen target)
 MPIIGaze method [42] and our	target)	EYEDIAP (screen target)
 Static and Temporal versions of	target)	EYEDIAP (screen target)
 the proposed network for FT	target)	EYEDIAP (screen target)
 and CS scenarios	target)	EYEDIAP (screen target)
.  As shown in Figure 3	target)	EYEDIAP (screen target)
, all models that take 	target)	EYEDIAP (screen target)
normalized full-face information as input 	target)	EYEDIAP (screen target)
achieve better performance than the 	target)	EYEDIAP (screen target)
eyes-only model. More specifically, the 	target)	EYEDIAP (screen target)
combination of face, eyes and 	target)	EYEDIAP (screen target)
landmarks outperforms all the other 	target)	EYEDIAP (screen target)
combinations by a small but 	target)	EYEDIAP (screen target)
significant margin (paired Wilcoxon test, 	target)	EYEDIAP (screen target)
p < 0.0001). The standard 	target)	EYEDIAP (screen target)
deviation of the best-performing model 	target)	EYEDIAP (screen target)
is reduced compared to the 	target)	EYEDIAP (screen target)
face and eyes model, suggesting 	target)	EYEDIAP (screen target)
a regularizing effect due to 	target)	EYEDIAP (screen target)
the addition of landmarks. The 	target)	EYEDIAP (screen target)
not-normalized face-only model shows the 	target)	EYEDIAP (screen target)
largest error, proving the impact 	target)	EYEDIAP (screen target)
of normalization to reduce the 	target)	EYEDIAP (screen target)
appearance variability. Furthermore, our results 	target)	EYEDIAP (screen target)
indicate that the increase of 	target)	EYEDIAP (screen target)
fusion units is not correlated 	target)	EYEDIAP (screen target)
with a better performance.  4.3 Static gaze regression: comparison	target)	EYEDIAP (screen target)
 with existing methods	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
We compare our best-performing static 	target)	EYEDIAP (screen target)
model with three baselines. Head: 	target)	EYEDIAP (screen target)
Treating the head pose directly 	target)	EYEDIAP (screen target)
as gaze direction. PR-ALR: Method 	target)	EYEDIAP (screen target)
that relies on RGB-D data 	target)	EYEDIAP (screen target)
to rectify the eye images 	target)	EYEDIAP (screen target)
viewpoint into a canonical head 	target)	EYEDIAP (screen target)
pose using a 3DMM. It 	target)	EYEDIAP (screen target)
then learns an RGB gaze 	target)	EYEDIAP (screen target)
appearance model using ALR [21]. 	target)	EYEDIAP (screen target)
Predicted 3D vectors for FT-S 	target)	EYEDIAP (screen target)
scenario are provided by EYEDIAP 	target)	EYEDIAP (screen target)
dataset. MPIIGaze:. State-of-the-art full-face 3D 	target)	EYEDIAP (screen target)
gaze estimation method [42]. They 	target)	EYEDIAP (screen target)
use an Alexnet-based CNN model 	target)	EYEDIAP (screen target)
with spatial weights to enhance 	target)	EYEDIAP (screen target)
information in different facial regions. 	target)	EYEDIAP (screen target)
We fine-tuned it with the 	target)	EYEDIAP (screen target)
filtered EYEDIAP subsets using our 	target)	EYEDIAP (screen target)
training parameters and normalization procedure.  In addition to the aforementioned	target)	EYEDIAP (screen target)
 FT-based evaluation setup, we also	target)	EYEDIAP (screen target)
 evaluate our method on the	target)	EYEDIAP (screen target)
 CS scenario. In this case	target)	EYEDIAP (screen target)
 there are only 14 participants	target)	EYEDIAP (screen target)
 available, so we divided them	target)	EYEDIAP (screen target)
 in 5 groups and performed	target)	EYEDIAP (screen target)
 5-fold cross-validation. In Figure 4	target)	EYEDIAP (screen target)
 we compare our method to	target)	EYEDIAP (screen target)
 MPIIGaze, achieving a statistically significant	target)	EYEDIAP (screen target)
 improvement of 14.6% and 19.5	target)	EYEDIAP (screen target)
% on FT and CS 	target)	EYEDIAP (screen target)
scenarios, respectively (paired Wilcoxon test, 	target)	EYEDIAP (screen target)
p < 0.0001). We can 	target)	EYEDIAP (screen target)
observe that a re- stricted 	target)	EYEDIAP (screen target)
gaze target benefits the performance 	target)	EYEDIAP (screen target)
of all methods, compared to 	target)	EYEDIAP (screen target)
a more challenging unrestricted setting 	target)	EYEDIAP (screen target)
with a wider range of 	target)	EYEDIAP (screen target)
head poses and gaze directions.  Table 2 provides a detailed	target)	EYEDIAP (screen target)
 comparison on every participant, performing	target)	EYEDIAP (screen target)
 leave-one-out cross-validation on the FT	target)	EYEDIAP (screen target)
 scenario for static and moving	target)	EYEDIAP (screen target)
 head separately. Results show that	target)	EYEDIAP (screen target)
, as expected, facial appearance 	target)	EYEDIAP (screen target)
and head pose have a 	target)	EYEDIAP (screen target)
noticeable impact on gaze accuracy, 	target)	EYEDIAP (screen target)
with average error differences of 	target)	EYEDIAP (screen target)
up to 7.7◦ among participants.  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (screen target)
, and Bulling} 2015  Citation Citation {Mora and Odobez	target)	EYEDIAP (screen target)
} 2012  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (screen target)
, and Bulling} 2015	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 9	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Method 1 2 3 4 5 6 7 8 9 10	target)	EYEDIAP (screen target)
 11 12 13 14 15	target)	EYEDIAP (screen target)
 16 Avg. Head 23.5 22.1	target)	EYEDIAP (screen target)
 20.3 23.6 23.2 23.2 23.6	target)	EYEDIAP (screen target)
 21.2 26.7 23.6 23.1 24.4	target)	EYEDIAP (screen target)
 23.3 24.0 24.5 22.8 23.3	target)	EYEDIAP (screen target)
 PR-ALR 12.3 12.0 12.4 11.3	target)	EYEDIAP (screen target)
 15.5 12.9 17.9 11.8 17.3	target)	EYEDIAP (screen target)
 13.4 13.4 14.3 15.2 13.6	target)	EYEDIAP (screen target)
 14.4 14.6 13.9 MPIIGaze 5.3	target)	EYEDIAP (screen target)
 5.1 5.7 4.7 7.3 15.1	target)	EYEDIAP (screen target)
 10.8 5.7 9.9 7.1 5.0	target)	EYEDIAP (screen target)
 5.7 7.4 3.8 4.8 5.5	target)	EYEDIAP (screen target)
 6.8 Static 3.9 4.1 4.2	target)	EYEDIAP (screen target)
 3.9 6.0 6.4 7.2 3.6	target)	EYEDIAP (screen target)
 7.1 5.0 5.7 6.7 3.9	target)	EYEDIAP (screen target)
 4.7 5.1 4.2 5.1 Temporal	target)	EYEDIAP (screen target)
 4.0 4.9 4.3 4.1 6.1	target)	EYEDIAP (screen target)
 6.5 6.6 3.9 7.8 6.1	target)	EYEDIAP (screen target)
 4.7 5.6 4.7 3.5 5.9	target)	EYEDIAP (screen target)
 4.6 5.2 Head 19.3 14.2	target)	EYEDIAP (screen target)
 16.4 19.9 16.8 21.9 16.1	target)	EYEDIAP (screen target)
 24.2 20.3 19.9 18.8 22.3	target)	EYEDIAP (screen target)
 18.1 14.9 16.2 19.3 18.7	target)	EYEDIAP (screen target)
 MPIIGaze 7.6 6.2 5.7 8.7	target)	EYEDIAP (screen target)
 10.1 12.0 12.2 6.1 8.3	target)	EYEDIAP (screen target)
 5.9 6.1 6.2 7.4 4.7	target)	EYEDIAP (screen target)
 4.4 6.0 7.3 Static 5.8	target)	EYEDIAP (screen target)
 5.7 4.4 7.5 6.7 8.8	target)	EYEDIAP (screen target)
 11.6 5.5 8.3 5.5 5.2	target)	EYEDIAP (screen target)
 6.3 5.3 3.9 4.3 5.6	target)	EYEDIAP (screen target)
 6.3 Temporal 6.1 5.6 4.5	target)	EYEDIAP (screen target)
 7.5 6.4 8.2 12.0 5.0	target)	EYEDIAP (screen target)
 7.5 5.4 5.0 5.8 6.6	target)	EYEDIAP (screen target)
 4.0 4.5 5.8 6.2	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Table 2: Gaze angular error 	target)	EYEDIAP (screen target)
comparison for static (top half) 	target)	EYEDIAP (screen target)
and moving (bottom half) head 	target)	EYEDIAP (screen target)
pose for each subject in 	target)	EYEDIAP (screen target)
the FT scenario. Best results 	target)	EYEDIAP (screen target)
in bold.  −80 −40 0 40 80−80	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
40  0	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
40  80	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
0  5	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
10  15	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
20  25	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
30  35	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
80 −40 0 40 80−80  −40	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
0  40	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
80  −10	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
8  −6	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
4  −2	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
0  2	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
4  6	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
8  10	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
80 −40 0 40 80−80  −40	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
0  40	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
80  0	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
5  10	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
15  20	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
25  30	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
35  −80 −40 0 40 80−80	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
40  0	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
40  80	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
10  −8	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
6  −4	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
2  0	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
2  4	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
6  8	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
10  (a) Gaze space (b) Head	target)	EYEDIAP (screen target)
 orientation space	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
Figure 5: Angular error distribution 	target)	EYEDIAP (screen target)
across gaze (a) and head 	target)	EYEDIAP (screen target)
orientation (b) spaces in the 	target)	EYEDIAP (screen target)
FT setting, in terms of 	target)	EYEDIAP (screen target)
x- and y- angles. For 	target)	EYEDIAP (screen target)
each space, we depict the 	target)	EYEDIAP (screen target)
Static model performance (left) and 	target)	EYEDIAP (screen target)
the contribution of the Temporal 	target)	EYEDIAP (screen target)
model versus Static (right). In 	target)	EYEDIAP (screen target)
the latter, positive difference means 	target)	EYEDIAP (screen target)
higher improvement of the Temporal 	target)	EYEDIAP (screen target)
model.  4.4 Evaluation of the temporal	target)	EYEDIAP (screen target)
 network	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
In this section, we evaluate 	target)	EYEDIAP (screen target)
the contribution of adding the 	target)	EYEDIAP (screen target)
temporal module to the static 	target)	EYEDIAP (screen target)
model. To do so, we 	target)	EYEDIAP (screen target)
trained a lower-dimensional version of 	target)	EYEDIAP (screen target)
the static network with compa- 	target)	EYEDIAP (screen target)
rable performance to the original, 	target)	EYEDIAP (screen target)
reducing the number of units 	target)	EYEDIAP (screen target)
of the second fusion layer 	target)	EYEDIAP (screen target)
to 2918. Results are reported 	target)	EYEDIAP (screen target)
in Figure 4 and Table 2	target)	EYEDIAP (screen target)
. One can observe that 	target)	EYEDIAP (screen target)
using sequential information is helpful 	target)	EYEDIAP (screen target)
on the FT scenario, outperforming 	target)	EYEDIAP (screen target)
the static model by a 	target)	EYEDIAP (screen target)
statistically significant 4.4% (paired Wilcoxon 	target)	EYEDIAP (screen target)
test, p < 0.0001). This 	target)	EYEDIAP (screen target)
contribution is more noticeable in 	target)	EYEDIAP (screen target)
the moving head setting, proving 	target)	EYEDIAP (screen target)
that the temporal model can 	target)	EYEDIAP (screen target)
benefit from head motion information. 	target)	EYEDIAP (screen target)
In contrast, such information seems 	target)	EYEDIAP (screen target)
to be less meaningful in 	target)	EYEDIAP (screen target)
the CS scenario, where the 	target)	EYEDIAP (screen target)
obtained error is already very 	target)	EYEDIAP (screen target)
low for a cross-subject setting 	target)	EYEDIAP (screen target)
and the amount of head 	target)	EYEDIAP (screen target)
movement declines.  Figure 5 further explores the	target)	EYEDIAP (screen target)
 error distribution of the static	target)	EYEDIAP (screen target)
 network and the impact of	target)	EYEDIAP (screen target)
 sequential information. We can observe	target)	EYEDIAP (screen target)
 that the accuracy of the	target)	EYEDIAP (screen target)
 static model drops with extreme	target)	EYEDIAP (screen target)
 head poses and gaze directions	target)	EYEDIAP (screen target)
, which can also be 	target)	EYEDIAP (screen target)
correlated to having less data 	target)	EYEDIAP (screen target)
in those areas. Compared to 	target)	EYEDIAP (screen target)
the static model, the temporal 	target)	EYEDIAP (screen target)
model particularly benefits gaze targets 	target)	EYEDIAP (screen target)
from mid-range upwards. Its contribution 	target)	EYEDIAP (screen target)
is less clear for extreme 	target)	EYEDIAP (screen target)
targets, probably again due to 	target)	EYEDIAP (screen target)
data imbalance.  Finally, we evaluated the effect	target)	EYEDIAP (screen target)
 of different recurrent architectures for	target)	EYEDIAP (screen target)
 the temporal model. In particular	target)	EYEDIAP (screen target)
, we tested 1 (128 	target)	EYEDIAP (screen target)
units) and 2 (256-128 units) 	target)	EYEDIAP (screen target)
LSTM and GRU lay- ers, 	target)	EYEDIAP (screen target)
with 1 GRU layer obtaining 	target)	EYEDIAP (screen target)
slightly superior results (up to 0	target)	EYEDIAP (screen target)
.12◦). We also assessed the 	target)	EYEDIAP (screen target)
effect of sequence length fixing 	target)	EYEDIAP (screen target)
s in the range {4,7,10}, 	target)	EYEDIAP (screen target)
with s = 7 performing 	target)	EYEDIAP (screen target)
worse than the other two (	target)	EYEDIAP (screen target)
up to 0	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
14	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
10 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (screen target)
ESTIMATION  5 Conclusions In this work	target)	EYEDIAP (screen target)
, we studied the combination 	target)	EYEDIAP (screen target)
of full-face and eye images 	target)	EYEDIAP (screen target)
along with facial land- marks 	target)	EYEDIAP (screen target)
for person- and head pose-independent 	target)	EYEDIAP (screen target)
3D gaze estimation. Consequently, we 	target)	EYEDIAP (screen target)
pro- posed a multi-stream recurrent 	target)	EYEDIAP (screen target)
CNN network that leverages the 	target)	EYEDIAP (screen target)
sequential information of eye and 	target)	EYEDIAP (screen target)
head movements. Both static and 	target)	EYEDIAP (screen target)
temporal versions of our approach 	target)	EYEDIAP (screen target)
significantly outperform current state-of-the-art 3D 	target)	EYEDIAP (screen target)
gaze estimation methods on a 	target)	EYEDIAP (screen target)
wide range of head poses 	target)	EYEDIAP (screen target)
and gaze directions. We showed 	target)	EYEDIAP (screen target)
that adding geometry features to 	target)	EYEDIAP (screen target)
appearance-based methods has a regularizing 	target)	EYEDIAP (screen target)
effect on the accuracy. Adding 	target)	EYEDIAP (screen target)
sequential information further benefits the 	target)	EYEDIAP (screen target)
final performance compared to static-only 	target)	EYEDIAP (screen target)
input, especially from mid-range up- 	target)	EYEDIAP (screen target)
wards and in those cases 	target)	EYEDIAP (screen target)
where head motion is present. 	target)	EYEDIAP (screen target)
The effect in very extreme 	target)	EYEDIAP (screen target)
head poses is not clear 	target)	EYEDIAP (screen target)
due to data imbalance, suggesting 	target)	EYEDIAP (screen target)
the importance of learning from 	target)	EYEDIAP (screen target)
a con- tinuous, balanced dataset 	target)	EYEDIAP (screen target)
including all head poses and 	target)	EYEDIAP (screen target)
gaze directions of interest. To 	target)	EYEDIAP (screen target)
the best of our knowledge, 	target)	EYEDIAP (screen target)
this is the first attempt 	target)	EYEDIAP (screen target)
to exploit the temporal modality 	target)	EYEDIAP (screen target)
in the context of gaze 	target)	EYEDIAP (screen target)
estimation from remote cameras. As 	target)	EYEDIAP (screen target)
future work, we will further 	target)	EYEDIAP (screen target)
explore extracting meaningful temporal representations 	target)	EYEDIAP (screen target)
of gaze dynamics, considering 3DCNNs 	target)	EYEDIAP (screen target)
as well as the encoding 	target)	EYEDIAP (screen target)
of deep features around particular 	target)	EYEDIAP (screen target)
tracked face landmarks [14].  Acknowledgements This work has been	target)	EYEDIAP (screen target)
 partially supported by the Spanish	target)	EYEDIAP (screen target)
 project TIN2016-74946-P (MINECO/ FEDER, UE	target)	EYEDIAP (screen target)
), CERCA Programme / Generalitat 	target)	EYEDIAP (screen target)
de Catalunya, and the FP7 	target)	EYEDIAP (screen target)
people program (Marie Curie Actions), 	target)	EYEDIAP (screen target)
REA grant agreement no FP7-607139 (	target)	EYEDIAP (screen target)
iCARE - Improving Children Auditory 	target)	EYEDIAP (screen target)
REhabilitation). We gratefully acknowledge the 	target)	EYEDIAP (screen target)
support of NVIDIA Corporation with 	target)	EYEDIAP (screen target)
the donation of the GPU 	target)	EYEDIAP (screen target)
used for this research. Portions 	target)	EYEDIAP (screen target)
of the research in this 	target)	EYEDIAP (screen target)
pa- per used the EYEDIAP 	target)	EYEDIAP (screen target)
dataset made available by the 	target)	EYEDIAP (screen target)
Idiap Research Institute, Martigny, Switzerland.  References [1] Nicola C Anderson	target)	EYEDIAP (screen target)
, Evan F Risko, and 	target)	EYEDIAP (screen target)
Alan Kingstone. Motion influences gaze 	target)	EYEDIAP (screen target)
di-  rection discrimination and disambiguates contradictory	target)	EYEDIAP (screen target)
 luminance cues. Psychonomic bulletin	target)	EYEDIAP (screen target)
 & review, 23(3):817–823, 2016	target)	EYEDIAP (screen target)
.  [2] Shumeet Baluja and Dean	target)	EYEDIAP (screen target)
 Pomerleau. Non-intrusive gaze tracking using	target)	EYEDIAP (screen target)
 artificial neu- ral networks. In	target)	EYEDIAP (screen target)
 Advances in Neural Information Processing	target)	EYEDIAP (screen target)
 Systems, pages 753–760, 1994	target)	EYEDIAP (screen target)
.  [3] Adrian Bulat and Georgios	target)	EYEDIAP (screen target)
 Tzimiropoulos. How far are we	target)	EYEDIAP (screen target)
 from solving the 2d	target)	EYEDIAP (screen target)
 & 3d face alignment problem	target)	EYEDIAP (screen target)
? (and a dataset of 230,	target)	EYEDIAP (screen target)
000 3d facial landmarks). In 	target)	EYEDIAP (screen target)
Interna- tional Conference on Computer 	target)	EYEDIAP (screen target)
Vision, 2017.  [4] Haoping Deng and Wangjiang	target)	EYEDIAP (screen target)
 Zhu. Monocular free-head 3d gaze	target)	EYEDIAP (screen target)
 tracking with deep learning and	target)	EYEDIAP (screen target)
 geometry constraints. In Computer Vision	target)	EYEDIAP (screen target)
 (ICCV), 2017 IEEE Interna- tional	target)	EYEDIAP (screen target)
 Conference on, pages 3162–3171. IEEE	target)	EYEDIAP (screen target)
, 2017.  [5] Onur Ferhat and Fernando	target)	EYEDIAP (screen target)
 Vilariño. Low cost eye tracking	target)	EYEDIAP (screen target)
. Computational intelligence and neuroscience, 2016	target)	EYEDIAP (screen target)
:17, 2016.  Citation Citation {Jung, Lee, Yim	target)	EYEDIAP (screen target)
, Park, and Kim} 2015	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 11	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
6] Kenneth A Funes-Mora and 	target)	EYEDIAP (screen target)
Jean-Marc Odobez. Gaze estimation in 	target)	EYEDIAP (screen target)
the 3D space using RGB-D 	target)	EYEDIAP (screen target)
sensors. International Journal of Computer 	target)	EYEDIAP (screen target)
Vision, 118(2):194–216, 2016.  [7] Kenneth Alberto Funes Mora	target)	EYEDIAP (screen target)
, Florent Monay, and Jean-Marc 	target)	EYEDIAP (screen target)
Odobez. Eyediap: A database for 	target)	EYEDIAP (screen target)
the development and evaluation of 	target)	EYEDIAP (screen target)
gaze estimation algorithms from rgb 	target)	EYEDIAP (screen target)
and rgb-d cameras. In Proceedings 	target)	EYEDIAP (screen target)
of the ACM Symposium on 	target)	EYEDIAP (screen target)
Eye Tracking Research and Applications. 	target)	EYEDIAP (screen target)
ACM, March 2014. doi: 10.1145/2578153.2578190.  [8] Kenneth Alberto Funes Mora	target)	EYEDIAP (screen target)
, Florent Monay, and Jean-Marc 	target)	EYEDIAP (screen target)
Odobez. Eyediap: A database for 	target)	EYEDIAP (screen target)
the development and evaluation of 	target)	EYEDIAP (screen target)
gaze estimation algorithms from rgb 	target)	EYEDIAP (screen target)
and rgb-d cameras. In Proceedings 	target)	EYEDIAP (screen target)
of the Symposium on Eye 	target)	EYEDIAP (screen target)
Tracking Research and Applications, pages 255	target)	EYEDIAP (screen target)
–258. ACM, 2014.  [9] Quentin Guillon, Nouchine Hadjikhani	target)	EYEDIAP (screen target)
, Sophie Baduel, and Bernadette 	target)	EYEDIAP (screen target)
Rogé. Visual social attention in 	target)	EYEDIAP (screen target)
autism spectrum disorder: Insights from 	target)	EYEDIAP (screen target)
eye tracking studies. Neu- 	target)	EYEDIAP (screen target)
roscience & Biobehavioral Reviews, 42:279–297, 2014	target)	EYEDIAP (screen target)
.  [10] Dan Witzner Hansen and	target)	EYEDIAP (screen target)
 Qiang Ji. In the eye	target)	EYEDIAP (screen target)
 of the beholder: A survey	target)	EYEDIAP (screen target)
 of models for eyes and	target)	EYEDIAP (screen target)
 gaze. IEEE transactions on pattern	target)	EYEDIAP (screen target)
 analysis and machine intelligence, 32(3	target)	EYEDIAP (screen target)
): 478–500, 2010.  [11] Qiong Huang, Ashok Veeraraghavan	target)	EYEDIAP (screen target)
, and Ashutosh Sabharwal. Tabletgaze: 	target)	EYEDIAP (screen target)
dataset and analysis for unconstrained 	target)	EYEDIAP (screen target)
appearance-based gaze estimation in mobile 	target)	EYEDIAP (screen target)
tablets. Machine Vision and Applications, 28	target)	EYEDIAP (screen target)
(5-6):445–461, 2017.  [12] Robert JK Jacob and	target)	EYEDIAP (screen target)
 Keith S Karn. Eye tracking	target)	EYEDIAP (screen target)
 in human-computer interaction and usability	target)	EYEDIAP (screen target)
 research: Ready to deliver the	target)	EYEDIAP (screen target)
 promises. In The mind’s eye	target)	EYEDIAP (screen target)
, pages 573–605. Elsevier, 2003.  [13] László A Jeni and	target)	EYEDIAP (screen target)
 Jeffrey F Cohn. Person-independent 3d	target)	EYEDIAP (screen target)
 gaze estimation using face frontalization	target)	EYEDIAP (screen target)
. In Proceedings of the 	target)	EYEDIAP (screen target)
IEEE Conference on Computer Vision 	target)	EYEDIAP (screen target)
and Pattern Recognition Workshops, pages 87	target)	EYEDIAP (screen target)
–95, 2016.  [14] Heechul Jung, Sihaeng Lee	target)	EYEDIAP (screen target)
, Junho Yim, Sunjeong Park, 	target)	EYEDIAP (screen target)
and Junmo Kim. Joint fine- 	target)	EYEDIAP (screen target)
tuning in deep neural networks 	target)	EYEDIAP (screen target)
for facial expression recognition. In 	target)	EYEDIAP (screen target)
Computer Vision (ICCV), 2015 IEEE 	target)	EYEDIAP (screen target)
International Conference on, pages 2983–2991. 	target)	EYEDIAP (screen target)
IEEE, 2015.  [15] Anuradha Kar and Peter	target)	EYEDIAP (screen target)
 Corcoran. A review and analysis	target)	EYEDIAP (screen target)
 of eye-gaze estimation sys- tems	target)	EYEDIAP (screen target)
, algorithms and performance evaluation 	target)	EYEDIAP (screen target)
methods in consumer platforms. IEEE 	target)	EYEDIAP (screen target)
Access, 5:16495–16519, 2017.  [16] Kyle Krafka, Aditya Khosla	target)	EYEDIAP (screen target)
, Petr Kellnhofer, Harini Kannan, 	target)	EYEDIAP (screen target)
Suchendra Bhandarkar, Wojciech Matusik, and 	target)	EYEDIAP (screen target)
Antonio Torralba. Eye tracking for 	target)	EYEDIAP (screen target)
everyone. In Computer Vision and 	target)	EYEDIAP (screen target)
Pattern Recognition (CVPR), 2016 IEEE 	target)	EYEDIAP (screen target)
Conference on, pages 2176–2184. IEEE, 2016	target)	EYEDIAP (screen target)
.  [17] Simon P Liversedge and	target)	EYEDIAP (screen target)
 John M Findlay. Saccadic eye	target)	EYEDIAP (screen target)
 movements and cognition. Trends in	target)	EYEDIAP (screen target)
 cognitive sciences, 4(1):6–14, 2000	target)	EYEDIAP (screen target)
.  [18] Feng Lu, Takahiro Okabe	target)	EYEDIAP (screen target)
, Yusuke Sugano, and Yoichi 	target)	EYEDIAP (screen target)
Sato. A head pose-free approach 	target)	EYEDIAP (screen target)
for appearance-based gaze estimation. In 	target)	EYEDIAP (screen target)
BMVC, pages 1–11, 2011	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
12 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (screen target)
ESTIMATION  [19] Feng Lu, Yusuke Sugano	target)	EYEDIAP (screen target)
, Takahiro Okabe, and Yoichi 	target)	EYEDIAP (screen target)
Sato. Inferring human gaze from 	target)	EYEDIAP (screen target)
appearance via adaptive linear regression. 	target)	EYEDIAP (screen target)
In Computer Vision (ICCV), 2011 	target)	EYEDIAP (screen target)
IEEE International Conference on, pages 153	target)	EYEDIAP (screen target)
–160. IEEE, 2011.  [20] Päivi Majaranta and Andreas	target)	EYEDIAP (screen target)
 Bulling. Eye tracking and eye-based	target)	EYEDIAP (screen target)
 human–computer interaction. In Advances in	target)	EYEDIAP (screen target)
 physiological computing, pages 39–65. Springer	target)	EYEDIAP (screen target)
, 2014.  [21] Kenneth Alberto Funes Mora	target)	EYEDIAP (screen target)
 and Jean-Marc Odobez. Gaze estimation	target)	EYEDIAP (screen target)
 from multi- modal kinect data	target)	EYEDIAP (screen target)
. In Computer Vision and 	target)	EYEDIAP (screen target)
Pattern Recognition Workshops (CVPRW), 2012 	target)	EYEDIAP (screen target)
IEEE Computer Society Conference on, 	target)	EYEDIAP (screen target)
pages 25–30. IEEE, 2012.  [22] Carlos Hitoshi Morimoto, Arnon	target)	EYEDIAP (screen target)
 Amir, and Myron Flickner. Detecting	target)	EYEDIAP (screen target)
 eye position and gaze from	target)	EYEDIAP (screen target)
 a single camera and 2	target)	EYEDIAP (screen target)
 light sources. In Pattern Recognition	target)	EYEDIAP (screen target)
, 2002. Proceedings. 16th International 	target)	EYEDIAP (screen target)
Conference on, volume 4, pages 314	target)	EYEDIAP (screen target)
–317. IEEE, 2002.  [23] IMO MSC. Circ. 982	target)	EYEDIAP (screen target)
 (2000) guidelines on ergonomic criteria	target)	EYEDIAP (screen target)
 for bridge equipment and layout	target)	EYEDIAP (screen target)
.  [24] Alejandro Newell, Kaiyu Yang	target)	EYEDIAP (screen target)
, and Jia Deng. Stacked 	target)	EYEDIAP (screen target)
hourglass networks for hu- man 	target)	EYEDIAP (screen target)
pose estimation. In European Conference 	target)	EYEDIAP (screen target)
on Computer Vision, pages 483–499. 	target)	EYEDIAP (screen target)
Springer, 2016.  [25] Yasuhiro Ono, Takahiro Okabe	target)	EYEDIAP (screen target)
, and Yoichi Sato. Gaze 	target)	EYEDIAP (screen target)
estimation from low resolution images. 	target)	EYEDIAP (screen target)
In Pacific-Rim Symposium on Image 	target)	EYEDIAP (screen target)
and Video Technology, pages 178–188. 	target)	EYEDIAP (screen target)
Springer, 2006.  [26] Cristina Palmero, Elisabeth A	target)	EYEDIAP (screen target)
. van Dam, Sergio Escalera, 	target)	EYEDIAP (screen target)
Mike Kelia, Guido F. Lichtert, 	target)	EYEDIAP (screen target)
Lucas P.J.J Noldus, Andrew J. 	target)	EYEDIAP (screen target)
Spink, and Astrid van Wieringen. 	target)	EYEDIAP (screen target)
Automatic mutual gaze detection in 	target)	EYEDIAP (screen target)
face-to-face dyadic interaction videos. In 	target)	EYEDIAP (screen target)
Proceedings of Measuring Behavior, pages 158	target)	EYEDIAP (screen target)
–163, 2018.  [27] Omkar M. Parkhi, Andrea	target)	EYEDIAP (screen target)
 Vedaldi, and Andrew Zisserman. Deep	target)	EYEDIAP (screen target)
 face recognition. In British Machine	target)	EYEDIAP (screen target)
 Vision Conference, 2015	target)	EYEDIAP (screen target)
.  [28] Derek R Rutter and	target)	EYEDIAP (screen target)
 Kevin Durkin. Turn-taking in mother–infant	target)	EYEDIAP (screen target)
 interaction: An exam- ination of	target)	EYEDIAP (screen target)
 vocalizations and gaze. Developmental psychology	target)	EYEDIAP (screen target)
, 23(1):54, 1987.  [29] Brian A Smith, Qi	target)	EYEDIAP (screen target)
 Yin, Steven K Feiner, and	target)	EYEDIAP (screen target)
 Shree K Nayar. Gaze locking	target)	EYEDIAP (screen target)
: passive eye contact detection 	target)	EYEDIAP (screen target)
for human-object interaction. In Proceedings 	target)	EYEDIAP (screen target)
of the 26th annual ACM 	target)	EYEDIAP (screen target)
symposium on User interface software 	target)	EYEDIAP (screen target)
and technology, pages 271–280. ACM, 2013	target)	EYEDIAP (screen target)
.  [30] Yusuke Sugano, Yasuyuki Matsushita	target)	EYEDIAP (screen target)
, and Yoichi Sato. Appearance-based 	target)	EYEDIAP (screen target)
gaze es- timation using visual 	target)	EYEDIAP (screen target)
saliency. IEEE transactions on pattern 	target)	EYEDIAP (screen target)
analysis and machine intelligence, 35(2):329–341, 2013	target)	EYEDIAP (screen target)
.  [31] Yusuke Sugano, Yasuyuki Matsushita	target)	EYEDIAP (screen target)
, and Yoichi Sato. Learning-by-synthesis 	target)	EYEDIAP (screen target)
for appearance-based 3d gaze estimation. 	target)	EYEDIAP (screen target)
In Computer Vision and Pattern 	target)	EYEDIAP (screen target)
Recognition (CVPR), 2014 IEEE Conference 	target)	EYEDIAP (screen target)
on, pages 1821–1828. IEEE, 2014.  [32] Kar-Han Tan, David J	target)	EYEDIAP (screen target)
 Kriegman, and Narendra Ahuja. Appearance-based	target)	EYEDIAP (screen target)
 eye gaze es- timation. In	target)	EYEDIAP (screen target)
 Applications of Computer Vision, 2002.(WACV	target)	EYEDIAP (screen target)
 2002). Proceedings. Sixth IEEE Workshop	target)	EYEDIAP (screen target)
 on, pages 191–195. IEEE, 2002	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 13	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
33] Ronda Venkateswarlu et al. 	target)	EYEDIAP (screen target)
Eye gaze estimation from a 	target)	EYEDIAP (screen target)
single image of one eye. 	target)	EYEDIAP (screen target)
In Computer Vision, 2003. Proceedings. 	target)	EYEDIAP (screen target)
Ninth IEEE International Conference on, 	target)	EYEDIAP (screen target)
pages 136–143. IEEE, 2003.  [34] Kang Wang and Qiang	target)	EYEDIAP (screen target)
 Ji. Real time eye gaze	target)	EYEDIAP (screen target)
 tracking with 3d deformable eye-face	target)	EYEDIAP (screen target)
 model. In Proceedings of the	target)	EYEDIAP (screen target)
 IEEE Conference on Computer Vision	target)	EYEDIAP (screen target)
 and Pattern Recog- nition, pages	target)	EYEDIAP (screen target)
 1003–1011, 2017	target)	EYEDIAP (screen target)
.  [35] Oliver Williams, Andrew Blake	target)	EYEDIAP (screen target)
, and Roberto Cipolla. Sparse 	target)	EYEDIAP (screen target)
and semi-supervised visual mapping with 	target)	EYEDIAP (screen target)
the sˆ 3gp. In Computer 	target)	EYEDIAP (screen target)
Vision and Pattern Recognition, 2006 	target)	EYEDIAP (screen target)
IEEE Computer Society Conference on, 	target)	EYEDIAP (screen target)
volume 1, pages 230–237. IEEE, 2006	target)	EYEDIAP (screen target)
.  [36] William Hyde Wollaston et	target)	EYEDIAP (screen target)
 al. Xiii. on the apparent	target)	EYEDIAP (screen target)
 direction of eyes in a	target)	EYEDIAP (screen target)
 portrait. Philosophical Transactions of the	target)	EYEDIAP (screen target)
 Royal Society of London, 114:247–256	target)	EYEDIAP (screen target)
, 1824.  [37] Erroll Wood and Andreas	target)	EYEDIAP (screen target)
 Bulling. Eyetab: Model-based gaze estimation	target)	EYEDIAP (screen target)
 on unmodi- fied tablet computers	target)	EYEDIAP (screen target)
. In Proceedings of the 	target)	EYEDIAP (screen target)
Symposium on Eye Tracking Research 	target)	EYEDIAP (screen target)
and Applications, pages 207–210. ACM, 2014	target)	EYEDIAP (screen target)
.  [38] Erroll Wood, Tadas Baltrusaitis	target)	EYEDIAP (screen target)
, Xucong Zhang, Yusuke Sugano, 	target)	EYEDIAP (screen target)
Peter Robinson, and Andreas Bulling. 	target)	EYEDIAP (screen target)
Rendering of eyes for eye-shape 	target)	EYEDIAP (screen target)
registration and gaze estimation. In 	target)	EYEDIAP (screen target)
Proceedings of the IEEE International 	target)	EYEDIAP (screen target)
Conference on Computer Vision, pages 3756	target)	EYEDIAP (screen target)
– 3764, 2015.  [39] Erroll Wood, Tadas Baltrušaitis	target)	EYEDIAP (screen target)
, Louis-Philippe Morency, Peter Robinson, 	target)	EYEDIAP (screen target)
and Andreas Bulling. A 3d 	target)	EYEDIAP (screen target)
morphable eye region model for 	target)	EYEDIAP (screen target)
gaze estimation. In European Confer- 	target)	EYEDIAP (screen target)
ence on Computer Vision, pages 297	target)	EYEDIAP (screen target)
–313. Springer, 2016.  [40] Erroll Wood, Tadas Baltrušaitis	target)	EYEDIAP (screen target)
, Louis-Philippe Morency, Peter Robinson, 	target)	EYEDIAP (screen target)
and Andreas Bulling. Learning an 	target)	EYEDIAP (screen target)
appearance-based gaze estimator from one 	target)	EYEDIAP (screen target)
million synthesised images. In Proceedings 	target)	EYEDIAP (screen target)
of the Ninth Biennial ACM 	target)	EYEDIAP (screen target)
Symposium on Eye Tracking Re- 	target)	EYEDIAP (screen target)
search & Applications, pages 131–138. 	target)	EYEDIAP (screen target)
ACM, 2016.  [41] Dong Hyun Yoo and	target)	EYEDIAP (screen target)
 Myung Jin Chung. A novel	target)	EYEDIAP (screen target)
 non-intrusive eye gaze estimation using	target)	EYEDIAP (screen target)
 cross-ratio under large head motion	target)	EYEDIAP (screen target)
. Computer Vision and Image 	target)	EYEDIAP (screen target)
Understanding, 98(1):25–51, 2005.  [42] Xucong Zhang, Yusuke Sugano	target)	EYEDIAP (screen target)
, Mario Fritz, and Andreas 	target)	EYEDIAP (screen target)
Bulling. Appearance-based gaze estimation in 	target)	EYEDIAP (screen target)
the wild. In Proceedings of 	target)	EYEDIAP (screen target)
the IEEE Conference on Computer 	target)	EYEDIAP (screen target)
Vision and Pattern Recognition, pages 4511	target)	EYEDIAP (screen target)
–4520, 2015.  [43] Xucong Zhang, Yusuke Sugano	target)	EYEDIAP (screen target)
, Mario Fritz, and Andreas 	target)	EYEDIAP (screen target)
Bulling. It’s written all over 	target)	EYEDIAP (screen target)
your face: Full-face appearance-based gaze 	target)	EYEDIAP (screen target)
estimation. In Proc. IEEE International 	target)	EYEDIAP (screen target)
Conference on Computer Vision and 	target)	EYEDIAP (screen target)
Pattern Recognition Workshops (CVPRW), 2017	target)	EYEDIAP (screen target)
	target)	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	(screen	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 1	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Recurrent CNN for 3D Gaze 	(screen	EYEDIAP (screen target)
Estimation using Appearance and Shape 	(screen	EYEDIAP (screen target)
Cues  Cristina Palmero1,2	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
crpalmec7@alumnes.ub.edu  1 Dept. Mathematics and Informatics	(screen	EYEDIAP (screen target)
 Universitat de Barcelona, Spain	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Javier Selva1  javier.selva.castello@est.fib.upc.edu	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
2 Computer Vision Center Campus 	(screen	EYEDIAP (screen target)
UAB, Bellaterra, Spain  Mohammad Ali Bagheri3,4	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
mohammadali.bagheri@ucalgary.ca  3 Dept. Electrical and Computer	(screen	EYEDIAP (screen target)
 Eng. University of Calgary, Canada	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Sergio Escalera1,2  sergio@maia.ub.es	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
4 Dept. Engineering University of 	(screen	EYEDIAP (screen target)
Larestan, Iran  Abstract	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Gaze behavior is an important 	(screen	EYEDIAP (screen target)
non-verbal cue in social signal 	(screen	EYEDIAP (screen target)
processing and human- computer interaction. 	(screen	EYEDIAP (screen target)
In this paper, we tackle 	(screen	EYEDIAP (screen target)
the problem of person- and 	(screen	EYEDIAP (screen target)
head pose- independent 3D gaze 	(screen	EYEDIAP (screen target)
estimation from remote cameras, using 	(screen	EYEDIAP (screen target)
a multi-modal recurrent convolutional neural 	(screen	EYEDIAP (screen target)
network (CNN). We propose to 	(screen	EYEDIAP (screen target)
combine face, eyes region, and 	(screen	EYEDIAP (screen target)
face landmarks as individual streams 	(screen	EYEDIAP (screen target)
in a CNN to estimate 	(screen	EYEDIAP (screen target)
gaze in still images. Then, 	(screen	EYEDIAP (screen target)
we exploit the dynamic nature 	(screen	EYEDIAP (screen target)
of gaze by feeding the 	(screen	EYEDIAP (screen target)
learned features of all the 	(screen	EYEDIAP (screen target)
frames in a sequence to 	(screen	EYEDIAP (screen target)
a many-to-one recurrent module that 	(screen	EYEDIAP (screen target)
predicts the 3D gaze vector 	(screen	EYEDIAP (screen target)
of the last frame. Our 	(screen	EYEDIAP (screen target)
multi-modal static solution is evaluated 	(screen	EYEDIAP (screen target)
on a wide range of 	(screen	EYEDIAP (screen target)
head poses and gaze directions, 	(screen	EYEDIAP (screen target)
achieving a significant improvement of 14	(screen	EYEDIAP (screen target)
.6% over the state of 	(screen	EYEDIAP (screen target)
the art on EYEDIAP dataset, 	(screen	EYEDIAP (screen target)
further improved by 4% when 	(screen	EYEDIAP (screen target)
the temporal modality is included.  1 Introduction Eyes and their	(screen	EYEDIAP (screen target)
 movements are considered an important	(screen	EYEDIAP (screen target)
 cue in non-verbal behavior analysis	(screen	EYEDIAP (screen target)
, being involved in many 	(screen	EYEDIAP (screen target)
cognitive processes and reflecting our 	(screen	EYEDIAP (screen target)
internal state [17]. More specifically, 	(screen	EYEDIAP (screen target)
eye gaze behavior, as an 	(screen	EYEDIAP (screen target)
indicator of human visual attention, 	(screen	EYEDIAP (screen target)
has been widely studied to 	(screen	EYEDIAP (screen target)
assess communication skills [28] and 	(screen	EYEDIAP (screen target)
to identify possible behavioral 	(screen	EYEDIAP (screen target)
disorders [9]. Therefore, gaze estimation 	(screen	EYEDIAP (screen target)
has become an established line 	(screen	EYEDIAP (screen target)
of research in computer vision, 	(screen	EYEDIAP (screen target)
being a key feature in 	(screen	EYEDIAP (screen target)
human-computer interaction (HCI) and usability 	(screen	EYEDIAP (screen target)
research [12, 20].  Recent gaze estimation research has	(screen	EYEDIAP (screen target)
 focused on facilitating its use	(screen	EYEDIAP (screen target)
 in general everyday applications under	(screen	EYEDIAP (screen target)
 real-world conditions, using off-the-shelf remote	(screen	EYEDIAP (screen target)
 RGB cameras and re- moving	(screen	EYEDIAP (screen target)
 the need of personal calibration	(screen	EYEDIAP (screen target)
 [26]. In this setting, appearance-based	(screen	EYEDIAP (screen target)
 methods, which learn a mapping	(screen	EYEDIAP (screen target)
 from images to gaze directions	(screen	EYEDIAP (screen target)
, are the preferred 	(screen	EYEDIAP (screen target)
choice [25]. How- ever, they 	(screen	EYEDIAP (screen target)
need large amounts of training 	(screen	EYEDIAP (screen target)
data to be able to 	(screen	EYEDIAP (screen target)
generalize well to in-the-wild situations, 	(screen	EYEDIAP (screen target)
which are characterized by significant 	(screen	EYEDIAP (screen target)
variability in head poses, face 	(screen	EYEDIAP (screen target)
appearances and lighting conditions. In 	(screen	EYEDIAP (screen target)
recent years, CNNs have been 	(screen	EYEDIAP (screen target)
reported to outperform classical methods. 	(screen	EYEDIAP (screen target)
However, most existing approaches have 	(screen	EYEDIAP (screen target)
only been tested in restricted 	(screen	EYEDIAP (screen target)
HCI tasks,  c© 2018. The copyright of	(screen	EYEDIAP (screen target)
 this document resides with its	(screen	EYEDIAP (screen target)
 authors. It may be distributed	(screen	EYEDIAP (screen target)
 unchanged freely in print or	(screen	EYEDIAP (screen target)
 electronic forms	(screen	EYEDIAP (screen target)
.  ar X	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
iv :1  80 5	(screen	EYEDIAP (screen target)
.  03 06	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
4v 3	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
cs  .C V	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
1  7	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Se  p	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
20  18	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Liversedge and Findlay} 2000	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Rutter and Durkin} 1987	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Guillon, Hadjikhani, Baduel, 	(screen	EYEDIAP (screen target)
and Rog{é}} 2014  Citation Citation {Jacob and Karn	(screen	EYEDIAP (screen target)
} 2003  Citation Citation {Majaranta and Bulling	(screen	EYEDIAP (screen target)
} 2014  Citation Citation {Palmero, van Dam	(screen	EYEDIAP (screen target)
, Escalera, Kelia, Lichtert, Noldus, 	(screen	EYEDIAP (screen target)
Spink, and van Wieringen} 2018  Citation Citation {Ono, Okabe, and	(screen	EYEDIAP (screen target)
 Sato} 2006	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
2 PALMERO ET AL.: MULTI-MODAL 	(screen	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	(screen	EYEDIAP (screen target)
ESTIMATION  Method 3D gaze direction	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Unrestricted gaze target  Full face	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Eye region  Facial landmarks	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Sequential information  Zhang et al. (1) [42	(screen	EYEDIAP (screen target)
] 3 7 7 3 7 7 Krafka et al. [16	(screen	EYEDIAP (screen target)
] 7 7 3 3 7 7 Zhang et al. (2	(screen	EYEDIAP (screen target)
) [43] 3 7 3 7 7 7 Deng and Zhu	(screen	EYEDIAP (screen target)
 [4] 3 3 3 3	(screen	EYEDIAP (screen target)
 7 7 Ours 3 3	(screen	EYEDIAP (screen target)
 3 3 3 3	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Table 1: Characteristics of recent 	(screen	EYEDIAP (screen target)
related work on person- and 	(screen	EYEDIAP (screen target)
head pose-independent appearance-based gaze estimation 	(screen	EYEDIAP (screen target)
methods using CNNs.  where users look at the	(screen	EYEDIAP (screen target)
 screen or mobile phone, showing	(screen	EYEDIAP (screen target)
 a low head pose variability	(screen	EYEDIAP (screen target)
. It is yet unclear 	(screen	EYEDIAP (screen target)
how these methods would perform 	(screen	EYEDIAP (screen target)
in a wider range of 	(screen	EYEDIAP (screen target)
head poses.  On a different note, until	(screen	EYEDIAP (screen target)
 very recently, the majority of	(screen	EYEDIAP (screen target)
 methods only used static eye	(screen	EYEDIAP (screen target)
 region appearance as input. State-of-the-art	(screen	EYEDIAP (screen target)
 approaches have demonstrated that using	(screen	EYEDIAP (screen target)
 the face along with a	(screen	EYEDIAP (screen target)
 higher resolution image of the	(screen	EYEDIAP (screen target)
 eyes [16], or even just	(screen	EYEDIAP (screen target)
 the face itself [43], increases	(screen	EYEDIAP (screen target)
 performance. Indeed, the whole-face image	(screen	EYEDIAP (screen target)
 encodes more information than eyes	(screen	EYEDIAP (screen target)
 alone, such as illumination and	(screen	EYEDIAP (screen target)
 head pose. Nevertheless, gaze behavior	(screen	EYEDIAP (screen target)
 is not static. Eye and	(screen	EYEDIAP (screen target)
 head movements allow us to	(screen	EYEDIAP (screen target)
 direct our gaze to target	(screen	EYEDIAP (screen target)
 locations of interest. It has	(screen	EYEDIAP (screen target)
 been demonstrated that humans can	(screen	EYEDIAP (screen target)
 better predict gaze when being	(screen	EYEDIAP (screen target)
 shown image sequences of other	(screen	EYEDIAP (screen target)
 people moving their eyes [1	(screen	EYEDIAP (screen target)
]. However, it is still 	(screen	EYEDIAP (screen target)
an open question whether this 	(screen	EYEDIAP (screen target)
se- quential information can increase 	(screen	EYEDIAP (screen target)
the performance of automatic methods.  In this work, we show	(screen	EYEDIAP (screen target)
 that the combination of multiple	(screen	EYEDIAP (screen target)
 cues benefits the gaze estimation	(screen	EYEDIAP (screen target)
 task. In particular, we use	(screen	EYEDIAP (screen target)
 face, eye region and facial	(screen	EYEDIAP (screen target)
 landmarks from still images. Facial	(screen	EYEDIAP (screen target)
 landmarks model the global shape	(screen	EYEDIAP (screen target)
 of the face and come	(screen	EYEDIAP (screen target)
 at no cost, since face	(screen	EYEDIAP (screen target)
 alignment is a common pre-processing	(screen	EYEDIAP (screen target)
 step in many facial image	(screen	EYEDIAP (screen target)
 analysis approaches. Furthermore, we present	(screen	EYEDIAP (screen target)
 a subject-independent, free-head recurrent 3D	(screen	EYEDIAP (screen target)
 gaze regression network to leverage	(screen	EYEDIAP (screen target)
 the temporal information of image	(screen	EYEDIAP (screen target)
 sequences. The static streams of	(screen	EYEDIAP (screen target)
 each frame are combined in	(screen	EYEDIAP (screen target)
 a late-fusion fashion using a	(screen	EYEDIAP (screen target)
 multi-stream CNN. Then, all feature	(screen	EYEDIAP (screen target)
 vectors are input to a	(screen	EYEDIAP (screen target)
 many-to-one recurrent module that predicts	(screen	EYEDIAP (screen target)
 the gaze vector of the	(screen	EYEDIAP (screen target)
 last sequence frame	(screen	EYEDIAP (screen target)
.  In summary, our contributions are	(screen	EYEDIAP (screen target)
 two-fold. First, we present a	(screen	EYEDIAP (screen target)
 Recurrent-CNN net- work architecture that	(screen	EYEDIAP (screen target)
 combines appearance, shape and temporal	(screen	EYEDIAP (screen target)
 information for 3D gaze estimation	(screen	EYEDIAP (screen target)
. Second, we test static 	(screen	EYEDIAP (screen target)
and temporal versions of our 	(screen	EYEDIAP (screen target)
solution on the EYEDIAP 	(screen	EYEDIAP (screen target)
dataset [7] in a wide 	(screen	EYEDIAP (screen target)
range of head poses and 	(screen	EYEDIAP (screen target)
gaze directions, showing consistent perfor- 	(screen	EYEDIAP (screen target)
mance improvements compared to related 	(screen	EYEDIAP (screen target)
appearance-based methods. To the best 	(screen	EYEDIAP (screen target)
of our knowledge, this is 	(screen	EYEDIAP (screen target)
the first third-person, remote camera-based 	(screen	EYEDIAP (screen target)
approach that uses tempo- ral 	(screen	EYEDIAP (screen target)
information for this task. Table 1 outlines our main method characteristics	(screen	EYEDIAP (screen target)
 compared to related work. Models	(screen	EYEDIAP (screen target)
 and code are publicly available	(screen	EYEDIAP (screen target)
 at https://github.com/ crisie/RecurrentGaze	(screen	EYEDIAP (screen target)
.  2 Related work Gaze estimation	(screen	EYEDIAP (screen target)
 methods are typically categorized as	(screen	EYEDIAP (screen target)
 model-based or appearance-based [5, 10	(screen	EYEDIAP (screen target)
, 15]. Model-based approaches use 	(screen	EYEDIAP (screen target)
a geometric model of the 	(screen	EYEDIAP (screen target)
eye, usually requir- ing either 	(screen	EYEDIAP (screen target)
high resolution images or a 	(screen	EYEDIAP (screen target)
person-specific calibration stage to estimate 	(screen	EYEDIAP (screen target)
personal eye parameters [22, 33, 34, 37, 41]. In contrast, appearance-based	(screen	EYEDIAP (screen target)
 methods learn a di- rect	(screen	EYEDIAP (screen target)
 mapping from intensity images or	(screen	EYEDIAP (screen target)
 extracted eye features to gaze	(screen	EYEDIAP (screen target)
 directions, thus being	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Zhang, Sugano, Fritz, 	(screen	EYEDIAP (screen target)
and Bulling} 2015  Citation Citation {Krafka, Khosla, Kellnhofer	(screen	EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 	(screen	EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	(screen	EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Deng and Zhu	(screen	EYEDIAP (screen target)
} 2017  Citation Citation {Krafka, Khosla, Kellnhofer	(screen	EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 	(screen	EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	(screen	EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Anderson, Risko, and	(screen	EYEDIAP (screen target)
 Kingstone} 2016	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Funesprotect unhbox voidb@x 	(screen	EYEDIAP (screen target)
penalty @M  {}Mora, Monay, and Odobez} 2014	(screen	EYEDIAP (screen target)
{}  Citation Citation {Ferhat and Vilari{ñ}o	(screen	EYEDIAP (screen target)
} 2016  Citation Citation {Hansen and Ji	(screen	EYEDIAP (screen target)
} 2010  Citation Citation {Kar and Corcoran	(screen	EYEDIAP (screen target)
} 2017  Citation Citation {Morimoto, Amir, and	(screen	EYEDIAP (screen target)
 Flickner} 2002	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Venkateswarlu etprotect unhbox 	(screen	EYEDIAP (screen target)
voidb@x penalty @M  {}al.} 2003	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Wang and Ji} 2017	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Wood and Bulling} 2014	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Yoo and Chung} 2005	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
https://github.com/crisie/RecurrentGaze 	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	(screen	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 3	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
potentially applicable to relatively low 	(screen	EYEDIAP (screen target)
resolution images and mid-distance scenarios. 	(screen	EYEDIAP (screen target)
Dif- ferent mapping functions have 	(screen	EYEDIAP (screen target)
been explored, such as neural 	(screen	EYEDIAP (screen target)
networks [2], adaptive linear regression (	(screen	EYEDIAP (screen target)
ALR) [19], local interpolation [32], 	(screen	EYEDIAP (screen target)
gaussian processes [30, 35], random 	(screen	EYEDIAP (screen target)
forests [11, 31], or k-nearest 	(screen	EYEDIAP (screen target)
neighbors [40]. Main challenges of 	(screen	EYEDIAP (screen target)
appearance-based methods for 3D gaze 	(screen	EYEDIAP (screen target)
estimation are head pose, illumination 	(screen	EYEDIAP (screen target)
and subject invariance without user-specific 	(screen	EYEDIAP (screen target)
calibration. To handle these issues, 	(screen	EYEDIAP (screen target)
some works proposed compensation 	(screen	EYEDIAP (screen target)
methods [18] and warping strategies 	(screen	EYEDIAP (screen target)
that synthesize a canonical, frontal 	(screen	EYEDIAP (screen target)
looking view of the 	(screen	EYEDIAP (screen target)
face [6, 13, 21]. Hybrid 	(screen	EYEDIAP (screen target)
approaches based on analysis-by-synthesis have 	(screen	EYEDIAP (screen target)
also been evaluated [39].  Currently, data-driven methods are considered	(screen	EYEDIAP (screen target)
 the state of the art	(screen	EYEDIAP (screen target)
 for person- and head pose-independent	(screen	EYEDIAP (screen target)
 appearance-based gaze estimation. Consequently, a	(screen	EYEDIAP (screen target)
 number of gaze es- timation	(screen	EYEDIAP (screen target)
 datasets have been introduced in	(screen	EYEDIAP (screen target)
 recent years, either in controlled	(screen	EYEDIAP (screen target)
 [29] or semi- controlled settings	(screen	EYEDIAP (screen target)
 [8], in the wild [16	(screen	EYEDIAP (screen target)
, 42], or consisting of 	(screen	EYEDIAP (screen target)
synthetic data [31, 38, 40]. 	(screen	EYEDIAP (screen target)
Zhang et al. [42] showed 	(screen	EYEDIAP (screen target)
that CNNs can outperform other 	(screen	EYEDIAP (screen target)
mapping methods, using a multi- 	(screen	EYEDIAP (screen target)
modal CNN to learn the 	(screen	EYEDIAP (screen target)
mapping from 3D head poses 	(screen	EYEDIAP (screen target)
and eye images to 3D 	(screen	EYEDIAP (screen target)
gaze directions. Krafka et 	(screen	EYEDIAP (screen target)
al. [16] proposed a multi-stream 	(screen	EYEDIAP (screen target)
CNN for 2D gaze estimation, 	(screen	EYEDIAP (screen target)
using individual eye, whole-face image 	(screen	EYEDIAP (screen target)
and the face grid as 	(screen	EYEDIAP (screen target)
input. As this method was 	(screen	EYEDIAP (screen target)
limited to 2D screen mapping, 	(screen	EYEDIAP (screen target)
Zhang et al. [43] later 	(screen	EYEDIAP (screen target)
explored the potential of just 	(screen	EYEDIAP (screen target)
using whole-face images as input 	(screen	EYEDIAP (screen target)
to estimate 3D gaze directions. 	(screen	EYEDIAP (screen target)
Using a spatial weights CNN, 	(screen	EYEDIAP (screen target)
they demonstrated their method to 	(screen	EYEDIAP (screen target)
be more robust to facial 	(screen	EYEDIAP (screen target)
appearance variation caused by head 	(screen	EYEDIAP (screen target)
pose and illumina- tion than 	(screen	EYEDIAP (screen target)
eye-only methods. While the method 	(screen	EYEDIAP (screen target)
was evaluated in the wild, 	(screen	EYEDIAP (screen target)
the subjects were only interacting 	(screen	EYEDIAP (screen target)
with a mobile device, thus 	(screen	EYEDIAP (screen target)
restricting the head pose range. 	(screen	EYEDIAP (screen target)
Deng and Zhu [4] presented 	(screen	EYEDIAP (screen target)
a two-stream CNN to disjointly 	(screen	EYEDIAP (screen target)
model head pose from face 	(screen	EYEDIAP (screen target)
images and eye- ball movement 	(screen	EYEDIAP (screen target)
from eye region images. Both 	(screen	EYEDIAP (screen target)
were then aggregated into 3D 	(screen	EYEDIAP (screen target)
gaze direction using a gaze 	(screen	EYEDIAP (screen target)
transform layer. The decomposition was 	(screen	EYEDIAP (screen target)
aimed to avoid head-correlation over- 	(screen	EYEDIAP (screen target)
fitting of previous data-driven approaches. 	(screen	EYEDIAP (screen target)
They evaluated their approach in 	(screen	EYEDIAP (screen target)
the wild with a wider 	(screen	EYEDIAP (screen target)
range of head poses, obtaining 	(screen	EYEDIAP (screen target)
better performance than previous eye-based 	(screen	EYEDIAP (screen target)
methods. However, they did not 	(screen	EYEDIAP (screen target)
test it on public annotated 	(screen	EYEDIAP (screen target)
benchmark datasets.  In this paper, we propose	(screen	EYEDIAP (screen target)
 a multi-stream recurrent CNN network	(screen	EYEDIAP (screen target)
 for person- and head pose-independent	(screen	EYEDIAP (screen target)
 3D gaze estimation for a	(screen	EYEDIAP (screen target)
 mid-distance scenario. We evaluate it	(screen	EYEDIAP (screen target)
 on a wider range of	(screen	EYEDIAP (screen target)
 head poses and gaze directions	(screen	EYEDIAP (screen target)
 than screen-targeted approaches. As opposed	(screen	EYEDIAP (screen target)
 to previous methods, we also	(screen	EYEDIAP (screen target)
 rely on temporal information inherent	(screen	EYEDIAP (screen target)
 in sequential data	(screen	EYEDIAP (screen target)
.  3 Methodology	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
In this section, we present 	(screen	EYEDIAP (screen target)
our approach for 3D gaze 	(screen	EYEDIAP (screen target)
regression based on appearance and 	(screen	EYEDIAP (screen target)
shape cues for still images 	(screen	EYEDIAP (screen target)
and image sequences. First, we 	(screen	EYEDIAP (screen target)
introduce the data modalities and 	(screen	EYEDIAP (screen target)
formulate the problem. Then, we 	(screen	EYEDIAP (screen target)
detail the normalization procedure prior 	(screen	EYEDIAP (screen target)
to the regression stage. Finally, 	(screen	EYEDIAP (screen target)
we explain the global network 	(screen	EYEDIAP (screen target)
topology as well as the 	(screen	EYEDIAP (screen target)
implementation details. An overview of 	(screen	EYEDIAP (screen target)
the system architecture is depicted 	(screen	EYEDIAP (screen target)
in Figure 1.  3.1 Multi-modal gaze regression	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Let us represent gaze direction 	(screen	EYEDIAP (screen target)
as a 3D unit vector 	(screen	EYEDIAP (screen target)
g = [gx,gy,gz]T ∈R3 in 	(screen	EYEDIAP (screen target)
the Camera Coor- dinate System (	(screen	EYEDIAP (screen target)
CCS), whose origin is the 	(screen	EYEDIAP (screen target)
central point between eyeball centers. 	(screen	EYEDIAP (screen target)
Assuming a calibrated camera, and 	(screen	EYEDIAP (screen target)
a known head position and 	(screen	EYEDIAP (screen target)
orientation, our goal is to 	(screen	EYEDIAP (screen target)
estimate g from a sequence 	(screen	EYEDIAP (screen target)
of images {I(i) | 	(screen	EYEDIAP (screen target)
I ∈ RW×H×3} as a 	(screen	EYEDIAP (screen target)
regression problem.  Citation Citation {Baluja and Pomerleau	(screen	EYEDIAP (screen target)
} 1994  Citation Citation {Lu, Sugano, Okabe	(screen	EYEDIAP (screen target)
, and Sato} 2011{}  Citation Citation {Tan, Kriegman, and	(screen	EYEDIAP (screen target)
 Ahuja} 2002	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Sugano, Matsushita, and 	(screen	EYEDIAP (screen target)
Sato} 2013  Citation Citation {Williams, Blake, and	(screen	EYEDIAP (screen target)
 Cipolla} 2006	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Huang, Veeraraghavan, and 	(screen	EYEDIAP (screen target)
Sabharwal} 2017  Citation Citation {Sugano, Matsushita, and	(screen	EYEDIAP (screen target)
 Sato} 2014	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Wood, Baltru{²}aitis, Morency, 	(screen	EYEDIAP (screen target)
Robinson, and Bulling} 2016{}  Citation Citation {Lu, Okabe, Sugano	(screen	EYEDIAP (screen target)
, and Sato} 2011{}  Citation Citation {Funes-Mora and Odobez	(screen	EYEDIAP (screen target)
} 2016  Citation Citation {Jeni and Cohn	(screen	EYEDIAP (screen target)
} 2016  Citation Citation {Mora and Odobez	(screen	EYEDIAP (screen target)
} 2012  Citation Citation {Wood, Baltru{²}aitis, Morency	(screen	EYEDIAP (screen target)
, Robinson, and Bulling} 2016{}  Citation Citation {Smith, Yin, Feiner	(screen	EYEDIAP (screen target)
, and Nayar} 2013  Citation Citation {Funesprotect unhbox voidb@x	(screen	EYEDIAP (screen target)
 penalty @M	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Mora, Monay, and Odobez} 2014{}  Citation Citation {Krafka, Khosla, Kellnhofer	(screen	EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 	(screen	EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	(screen	EYEDIAP (screen target)
, and Bulling} 2015  Citation Citation {Sugano, Matsushita, and	(screen	EYEDIAP (screen target)
 Sato} 2014	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Wood, Baltrusaitis, Zhang, 	(screen	EYEDIAP (screen target)
Sugano, Robinson, and Bulling} 2015  Citation Citation {Wood, Baltru{²}aitis, Morency	(screen	EYEDIAP (screen target)
, Robinson, and Bulling} 2016{}  Citation Citation {Zhang, Sugano, Fritz	(screen	EYEDIAP (screen target)
, and Bulling} 2015  Citation Citation {Krafka, Khosla, Kellnhofer	(screen	EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 	(screen	EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	(screen	EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Deng and Zhu	(screen	EYEDIAP (screen target)
} 2017	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
4 PALMERO ET AL.: MULTI-MODAL 	(screen	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	(screen	EYEDIAP (screen target)
ESTIMATION  Conv	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
C on ca t  x y z x y	(screen	EYEDIAP (screen target)
 z x y z	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Individual Fusion Temporal  Individual Fusion	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Input 	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Individual Fusion  Normalization	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
 .Conv	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Conv .  Conv	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Conv .  FC	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
FC FC RNN  RNN	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
RNN FC  Ti m e	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Figure 1: Overview of the 	(screen	EYEDIAP (screen target)
proposed network. A multi-stream CNN 	(screen	EYEDIAP (screen target)
jointly models full-face, eye region 	(screen	EYEDIAP (screen target)
appearance and face landmarks from 	(screen	EYEDIAP (screen target)
still images. The combined extracted 	(screen	EYEDIAP (screen target)
fea- tures from each frame 	(screen	EYEDIAP (screen target)
are fed into a recurrent 	(screen	EYEDIAP (screen target)
module to predict last frame’s 	(screen	EYEDIAP (screen target)
gaze direction.  Gazing to a specific target	(screen	EYEDIAP (screen target)
 is achieved by a combination	(screen	EYEDIAP (screen target)
 of eye and head movements	(screen	EYEDIAP (screen target)
, which are highly coordinated. 	(screen	EYEDIAP (screen target)
Consequently, the apparent direction of 	(screen	EYEDIAP (screen target)
gaze is influenced not only 	(screen	EYEDIAP (screen target)
by the location of the 	(screen	EYEDIAP (screen target)
irises within the eyelid aperture, 	(screen	EYEDIAP (screen target)
but also by the position 	(screen	EYEDIAP (screen target)
and orientation of the face 	(screen	EYEDIAP (screen target)
with respect to the camera. 	(screen	EYEDIAP (screen target)
Known as the Wollaston 	(screen	EYEDIAP (screen target)
effect [36], the exact same 	(screen	EYEDIAP (screen target)
set of eyes may appear 	(screen	EYEDIAP (screen target)
to be looking in different 	(screen	EYEDIAP (screen target)
directions due to the surrounding 	(screen	EYEDIAP (screen target)
facial cues. It is therefore 	(screen	EYEDIAP (screen target)
reasonable to state that eye 	(screen	EYEDIAP (screen target)
images are not sufficient to 	(screen	EYEDIAP (screen target)
estimate gaze direction. Instead, whole-face 	(screen	EYEDIAP (screen target)
images can encode head pose 	(screen	EYEDIAP (screen target)
or illumination-specific information across larger 	(screen	EYEDIAP (screen target)
areas than those available just 	(screen	EYEDIAP (screen target)
in the eyes region [16, 43	(screen	EYEDIAP (screen target)
].  The drawback of appearance-only methods	(screen	EYEDIAP (screen target)
 is that global structure information	(screen	EYEDIAP (screen target)
 is not explicitly considered. In	(screen	EYEDIAP (screen target)
 that sense, facial landmarks can	(screen	EYEDIAP (screen target)
 be used as global shape	(screen	EYEDIAP (screen target)
 cues to en- code spatial	(screen	EYEDIAP (screen target)
 relationships and geometric constraints. Current	(screen	EYEDIAP (screen target)
 state-of-the-art face alignment approaches are	(screen	EYEDIAP (screen target)
 robust enough to handle large	(screen	EYEDIAP (screen target)
 appearance variability, extreme head poses	(screen	EYEDIAP (screen target)
 and occlusions, being especially useful	(screen	EYEDIAP (screen target)
 when the dataset used for	(screen	EYEDIAP (screen target)
 gaze estimation does not contain	(screen	EYEDIAP (screen target)
 such variability. Facial landmarks are	(screen	EYEDIAP (screen target)
 mainly correlated with head orientation	(screen	EYEDIAP (screen target)
, eye position, eyelid openness, 	(screen	EYEDIAP (screen target)
and eyebrow movement, which are 	(screen	EYEDIAP (screen target)
valuable features for our task.  Therefore, in our approach we	(screen	EYEDIAP (screen target)
 jointly model appearance and shape	(screen	EYEDIAP (screen target)
 cues (see Figure 1). The	(screen	EYEDIAP (screen target)
 former is represented by a	(screen	EYEDIAP (screen target)
 whole-face image IF , along	(screen	EYEDIAP (screen target)
 with a higher resolution image	(screen	EYEDIAP (screen target)
 of the eyes IE to	(screen	EYEDIAP (screen target)
 identify subtle changes. Due to	(screen	EYEDIAP (screen target)
 dealing with wide head pose	(screen	EYEDIAP (screen target)
 ranges, some eye images may	(screen	EYEDIAP (screen target)
 not depict the whole eye	(screen	EYEDIAP (screen target)
, containing mostly background or 	(screen	EYEDIAP (screen target)
other surrounding facial parts instead. 	(screen	EYEDIAP (screen target)
For that reason, and contrary 	(screen	EYEDIAP (screen target)
to previous approaches that only 	(screen	EYEDIAP (screen target)
use one eye image [31, 42	(screen	EYEDIAP (screen target)
], we use a single 	(screen	EYEDIAP (screen target)
image composed of two patches 	(screen	EYEDIAP (screen target)
of centered left and right 	(screen	EYEDIAP (screen target)
eyes. Finally, the shape cue 	(screen	EYEDIAP (screen target)
is represented by 3D face 	(screen	EYEDIAP (screen target)
landmarks obtained from a 68-landmark 	(screen	EYEDIAP (screen target)
model, denoted by 	(screen	EYEDIAP (screen target)
L = {(lx, ly, 	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
)	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
 | ∀c ∈ [1, ...,68	(screen	EYEDIAP (screen target)
]}.  In this work we also	(screen	EYEDIAP (screen target)
 consider the dynamic component of	(screen	EYEDIAP (screen target)
 gaze. We leverage the se	(screen	EYEDIAP (screen target)
- quential information of eye 	(screen	EYEDIAP (screen target)
and head movements such that, 	(screen	EYEDIAP (screen target)
given appearance and shape features 	(screen	EYEDIAP (screen target)
of consecutive frames, it is 	(screen	EYEDIAP (screen target)
possible to better predict the 	(screen	EYEDIAP (screen target)
gaze direction of the cur- 	(screen	EYEDIAP (screen target)
rent frame. Therefore, the 3D 	(screen	EYEDIAP (screen target)
gaze estimation task for a 1	(screen	EYEDIAP (screen target)
-frame sequence is formulated  Citation Citation {Wollaston etprotect unhbox	(screen	EYEDIAP (screen target)
 voidb@x penalty @M	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
al.} 1824  Citation Citation {Krafka, Khosla, Kellnhofer	(screen	EYEDIAP (screen target)
, Kannan, Bhandarkar, Matusik, and 	(screen	EYEDIAP (screen target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	(screen	EYEDIAP (screen target)
, and Bulling} 2017  Citation Citation {Sugano, Matsushita, and	(screen	EYEDIAP (screen target)
 Sato} 2014	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Zhang, Sugano, Fritz, 	(screen	EYEDIAP (screen target)
and Bulling} 2015	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	(screen	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 5	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
as g(i) = f ( {IF (i)},{IE (i)},{L(i	(screen	EYEDIAP (screen target)
)}  ) , where i denotes	(screen	EYEDIAP (screen target)
 the i-th frame, and f	(screen	EYEDIAP (screen target)
 is the regression	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
function.  3.2 Data normalization Prior to	(screen	EYEDIAP (screen target)
 gaze regression, a normalization step	(screen	EYEDIAP (screen target)
 in the 3D space and	(screen	EYEDIAP (screen target)
 the 2D image, similar to	(screen	EYEDIAP (screen target)
 [31], is carried out. This	(screen	EYEDIAP (screen target)
 is performed to reduce the	(screen	EYEDIAP (screen target)
 appearance variability and to allow	(screen	EYEDIAP (screen target)
 the gaze estimation model to	(screen	EYEDIAP (screen target)
 be applied regardless of the	(screen	EYEDIAP (screen target)
 original camera configuration	(screen	EYEDIAP (screen target)
.  Let H ∈ R3x3 be	(screen	EYEDIAP (screen target)
 the head rotation matrix, and	(screen	EYEDIAP (screen target)
 p = [px, py, pz]T	(screen	EYEDIAP (screen target)
 ∈ R3 the reference face	(screen	EYEDIAP (screen target)
 location with respect to the	(screen	EYEDIAP (screen target)
 original CCS. The goal is	(screen	EYEDIAP (screen target)
 to find the conversion matrix	(screen	EYEDIAP (screen target)
 M = SR such that	(screen	EYEDIAP (screen target)
 (a) the X-axes of the	(screen	EYEDIAP (screen target)
 virtual camera and the head	(screen	EYEDIAP (screen target)
 become parallel using the rotation	(screen	EYEDIAP (screen target)
 matrix R, and (b) the	(screen	EYEDIAP (screen target)
 virtual camera looks at the	(screen	EYEDIAP (screen target)
 reference location from a fixed	(screen	EYEDIAP (screen target)
 distance dn using the Z-direction	(screen	EYEDIAP (screen target)
 scaling matrix S = diag(1,1,dn/‖p	(screen	EYEDIAP (screen target)
‖). R is computed as 	(screen	EYEDIAP (screen target)
a = p̂×HT e1, 	(screen	EYEDIAP (screen target)
b = â× p̂, 	(screen	EYEDIAP (screen target)
R = [â, b̂, p̂]T , where e1 denotes the first	(screen	EYEDIAP (screen target)
 orthonormal basis and	(screen	EYEDIAP (screen target)
 〈 ·̂ 〉 is the	(screen	EYEDIAP (screen target)
 unit vector	(screen	EYEDIAP (screen target)
.  This normalization translates into the	(screen	EYEDIAP (screen target)
 image space as a cropped	(screen	EYEDIAP (screen target)
 image patch of size Wn×Hn	(screen	EYEDIAP (screen target)
 centered at p where head	(screen	EYEDIAP (screen target)
 roll rotation has been removed	(screen	EYEDIAP (screen target)
. This is done by 	(screen	EYEDIAP (screen target)
applying a perspective warping to 	(screen	EYEDIAP (screen target)
the input image I using 	(screen	EYEDIAP (screen target)
the transformation matrix W = 	(screen	EYEDIAP (screen target)
CoMCn−1, where Co and Cn 	(screen	EYEDIAP (screen target)
are the original and virtual 	(screen	EYEDIAP (screen target)
camera matrices, respectively.  The 3D gaze vector is	(screen	EYEDIAP (screen target)
 also normalized as gn =Rg	(screen	EYEDIAP (screen target)
. After image normalization, the 	(screen	EYEDIAP (screen target)
line of sight can be 	(screen	EYEDIAP (screen target)
represented in a 2D space. 	(screen	EYEDIAP (screen target)
Therefore, gn is further transformed 	(screen	EYEDIAP (screen target)
to spherical coor- dinates (θ ,	(screen	EYEDIAP (screen target)
φ) assuming unit length, where 	(screen	EYEDIAP (screen target)
θ and φ denote the 	(screen	EYEDIAP (screen target)
horizontal and vertical direc- tion 	(screen	EYEDIAP (screen target)
angles, respectively. This 2D angle 	(screen	EYEDIAP (screen target)
representation, delimited in the 	(screen	EYEDIAP (screen target)
range [−π/2,π/2], is computed as 	(screen	EYEDIAP (screen target)
θ = arctan(gx/gz) and 	(screen	EYEDIAP (screen target)
φ = arcsin(−gy), such that (0,	(screen	EYEDIAP (screen target)
0) represents looking straight ahead 	(screen	EYEDIAP (screen target)
to the CCS origin.  3.3 Recurrent Convolutional Neural Network	(screen	EYEDIAP (screen target)
 We propose a Recurrent CNN	(screen	EYEDIAP (screen target)
 Regression Network for 3D gaze	(screen	EYEDIAP (screen target)
 estimation. The network is divided	(screen	EYEDIAP (screen target)
 in 3 modules: (1) Individual	(screen	EYEDIAP (screen target)
, (2) Fusion, and (3) 	(screen	EYEDIAP (screen target)
Temporal.  First, the Individual module learns	(screen	EYEDIAP (screen target)
 features from each appearance cue	(screen	EYEDIAP (screen target)
 separately. It consists of a	(screen	EYEDIAP (screen target)
 two-stream CNN, one devoted to	(screen	EYEDIAP (screen target)
 the normalized face image stream	(screen	EYEDIAP (screen target)
 and the other to the	(screen	EYEDIAP (screen target)
 joint normalized eyes image. Next	(screen	EYEDIAP (screen target)
, the Fusion module combines 	(screen	EYEDIAP (screen target)
the extracted features of each 	(screen	EYEDIAP (screen target)
appearance stream in a single 	(screen	EYEDIAP (screen target)
vector along with the normalized 	(screen	EYEDIAP (screen target)
landmark coordinates. Then, it learns 	(screen	EYEDIAP (screen target)
a joint representation between modalities 	(screen	EYEDIAP (screen target)
in a late-fusion fashion. Both 	(screen	EYEDIAP (screen target)
Individual and Fusion modules, further 	(screen	EYEDIAP (screen target)
referred to as Static model, 	(screen	EYEDIAP (screen target)
are applied to each frame 	(screen	EYEDIAP (screen target)
of the sequence. Finally, the 	(screen	EYEDIAP (screen target)
resulting feature vectors of each 	(screen	EYEDIAP (screen target)
frame are input to the 	(screen	EYEDIAP (screen target)
Temporal module based on a 	(screen	EYEDIAP (screen target)
many-to-one recurrent network. This module 	(screen	EYEDIAP (screen target)
leverages sequential information to predict 	(screen	EYEDIAP (screen target)
the normalized 2D gaze angles 	(screen	EYEDIAP (screen target)
of the last frame of 	(screen	EYEDIAP (screen target)
the sequence using a linear 	(screen	EYEDIAP (screen target)
regression layer added on top 	(screen	EYEDIAP (screen target)
of it.  3.4 Implementation details 3.4.1 Network	(screen	EYEDIAP (screen target)
 details	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Each stream of the Individual 	(screen	EYEDIAP (screen target)
module is based on the 	(screen	EYEDIAP (screen target)
VGG-16 deep network [27], consisting 	(screen	EYEDIAP (screen target)
of 13 convolutional layers, 5 	(screen	EYEDIAP (screen target)
max pooling layers, and 1 	(screen	EYEDIAP (screen target)
fully connected (FC) layer with 	(screen	EYEDIAP (screen target)
Rec- tified Linear Unit (ReLU) 	(screen	EYEDIAP (screen target)
activations. The full-face stream follows 	(screen	EYEDIAP (screen target)
the same configuration  Citation Citation {Sugano, Matsushita, and	(screen	EYEDIAP (screen target)
 Sato} 2014	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Parkhi, Vedaldi, and 	(screen	EYEDIAP (screen target)
Zisserman} 2015	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
6 PALMERO ET AL.: MULTI-MODAL 	(screen	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	(screen	EYEDIAP (screen target)
ESTIMATION  as the base network, having	(screen	EYEDIAP (screen target)
 an input of 224×224 pixels	(screen	EYEDIAP (screen target)
 and a 4096D FC layer	(screen	EYEDIAP (screen target)
. In contrast, the input 	(screen	EYEDIAP (screen target)
joint eye image is smaller, 	(screen	EYEDIAP (screen target)
with a final size of 120	(screen	EYEDIAP (screen target)
×48 pixels, so the number 	(screen	EYEDIAP (screen target)
of pa- rameters is decreased 	(screen	EYEDIAP (screen target)
proportionally. In this case, its 	(screen	EYEDIAP (screen target)
last FC layer produces a 	(screen	EYEDIAP (screen target)
1536D vector. A 204D landmark 	(screen	EYEDIAP (screen target)
coordinates vector is concatenated to 	(screen	EYEDIAP (screen target)
the output of the FC 	(screen	EYEDIAP (screen target)
layer of each stream, resulting 	(screen	EYEDIAP (screen target)
in a 5836D feature vector. 	(screen	EYEDIAP (screen target)
Consequently, the Fusion module consists 	(screen	EYEDIAP (screen target)
of 2 5836D FC layers 	(screen	EYEDIAP (screen target)
with ReLU activations and 2 	(screen	EYEDIAP (screen target)
dropout layers between FCs as 	(screen	EYEDIAP (screen target)
regularization. Finally, to model the 	(screen	EYEDIAP (screen target)
temporal dependencies, we use a 	(screen	EYEDIAP (screen target)
single GRU layer with 128 	(screen	EYEDIAP (screen target)
units.  The network is trained in	(screen	EYEDIAP (screen target)
 a stage-wise fashion. First, we	(screen	EYEDIAP (screen target)
 train the Static model and	(screen	EYEDIAP (screen target)
 the final regression layer end-to-end	(screen	EYEDIAP (screen target)
 on each individual frame of	(screen	EYEDIAP (screen target)
 the training data. The convolutional	(screen	EYEDIAP (screen target)
 blocks are pre-trained with the	(screen	EYEDIAP (screen target)
 VGG-Face dataset [27], whereas the	(screen	EYEDIAP (screen target)
 FCs are trained from scratch	(screen	EYEDIAP (screen target)
. Second, the training data 	(screen	EYEDIAP (screen target)
is re-arranged by means of 	(screen	EYEDIAP (screen target)
a sliding window with stride 1 to build input sequences. Each	(screen	EYEDIAP (screen target)
 sequence is composed of s	(screen	EYEDIAP (screen target)
 = 4 consecutive frames, whose	(screen	EYEDIAP (screen target)
 gaze direction target is the	(screen	EYEDIAP (screen target)
 gaze direction of the last	(screen	EYEDIAP (screen target)
 frame of the sequence( {I(i−s+1	(screen	EYEDIAP (screen target)
), . . . ,I(i)}, 	(screen	EYEDIAP (screen target)
g(i)  ) . Using this re-arranged	(screen	EYEDIAP (screen target)
 training data, we extract features	(screen	EYEDIAP (screen target)
 of each	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
frame of the sequence from 	(screen	EYEDIAP (screen target)
a frozen Individual module, fine-tune 	(screen	EYEDIAP (screen target)
the Fusion layers, and train 	(screen	EYEDIAP (screen target)
both, the Temporal module and 	(screen	EYEDIAP (screen target)
a new final regression layer 	(screen	EYEDIAP (screen target)
from scratch. This way, the 	(screen	EYEDIAP (screen target)
network can exploit the temporal 	(screen	EYEDIAP (screen target)
information to further refine the 	(screen	EYEDIAP (screen target)
fusion weights.  We trained the model using	(screen	EYEDIAP (screen target)
 ADAM optimizer with an initial	(screen	EYEDIAP (screen target)
 learning rate of 0.0001, dropout	(screen	EYEDIAP (screen target)
 of 0.3, and batch size	(screen	EYEDIAP (screen target)
 of 64 frames. The number	(screen	EYEDIAP (screen target)
 of epochs was experimentally set	(screen	EYEDIAP (screen target)
 to 21 for the first	(screen	EYEDIAP (screen target)
 training stage and 10 for	(screen	EYEDIAP (screen target)
 the second. We use the	(screen	EYEDIAP (screen target)
 average Euclidean distance between the	(screen	EYEDIAP (screen target)
 predicted and ground-truth 3D gaze	(screen	EYEDIAP (screen target)
 vectors as loss function	(screen	EYEDIAP (screen target)
.  3.4.2 Input pre-processing	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
For this work we use 	(screen	EYEDIAP (screen target)
head pose and eye locations 	(screen	EYEDIAP (screen target)
in the 3D scene provided 	(screen	EYEDIAP (screen target)
by the dataset. The 3D 	(screen	EYEDIAP (screen target)
landmarks are extracted using the 	(screen	EYEDIAP (screen target)
state-of-the-art method of Bulat and 	(screen	EYEDIAP (screen target)
Tzimiropou- los [3], which is 	(screen	EYEDIAP (screen target)
based on stacked hourglass 	(screen	EYEDIAP (screen target)
networks [24].  During training, the original image	(screen	EYEDIAP (screen target)
 is pre-processed to get the	(screen	EYEDIAP (screen target)
 two normalized input images. The	(screen	EYEDIAP (screen target)
 normalized whole-face patch is centered	(screen	EYEDIAP (screen target)
 0.1 meters ahead of the	(screen	EYEDIAP (screen target)
 head center in the head	(screen	EYEDIAP (screen target)
 coordinate system, and Cn is	(screen	EYEDIAP (screen target)
 defined such that the image	(screen	EYEDIAP (screen target)
 has size of 250× 250	(screen	EYEDIAP (screen target)
 pixels. The difference between this	(screen	EYEDIAP (screen target)
 size and the final input	(screen	EYEDIAP (screen target)
 size allows us to perform	(screen	EYEDIAP (screen target)
 random cropping and zooming to	(screen	EYEDIAP (screen target)
 augment the data (explained in	(screen	EYEDIAP (screen target)
 Section 4.1). Similarly, each normalized	(screen	EYEDIAP (screen target)
 eye patch is centered in	(screen	EYEDIAP (screen target)
 their respective eye center locations	(screen	EYEDIAP (screen target)
. In this case, the 	(screen	EYEDIAP (screen target)
virtual camera matrix is defined 	(screen	EYEDIAP (screen target)
so that the image is 	(screen	EYEDIAP (screen target)
cropped to 70×58, while in 	(screen	EYEDIAP (screen target)
practice the final patches have 	(screen	EYEDIAP (screen target)
size of 60×48. Landmarks are 	(screen	EYEDIAP (screen target)
normalized using the same procedure 	(screen	EYEDIAP (screen target)
and further pre-processed with mean 	(screen	EYEDIAP (screen target)
subtraction and min-max normalization per 	(screen	EYEDIAP (screen target)
axis. Finally, we divide them 	(screen	EYEDIAP (screen target)
by a scaling factor w 	(screen	EYEDIAP (screen target)
such that all coordinates are 	(screen	EYEDIAP (screen target)
in the range [0,w]. This 	(screen	EYEDIAP (screen target)
way, all concatenated feature values 	(screen	EYEDIAP (screen target)
are in a similar range. 	(screen	EYEDIAP (screen target)
After inference, the predicted normalized 	(screen	EYEDIAP (screen target)
2D angles are de-normalized back 	(screen	EYEDIAP (screen target)
to the original 3D space.  4 Experiments In this section	(screen	EYEDIAP (screen target)
, we evaluate the cross-subject 	(screen	EYEDIAP (screen target)
3D gaze estimation task on 	(screen	EYEDIAP (screen target)
a wide range of head 	(screen	EYEDIAP (screen target)
poses and gaze directions. Furthermore, 	(screen	EYEDIAP (screen target)
we validate the effectiveness of 	(screen	EYEDIAP (screen target)
the proposed architecture comparing both 	(screen	EYEDIAP (screen target)
static and temporal approaches. We 	(screen	EYEDIAP (screen target)
report the error in terms 	(screen	EYEDIAP (screen target)
of mean angular error between 	(screen	EYEDIAP (screen target)
predicted and ground-truth 3D gaze 	(screen	EYEDIAP (screen target)
vectors. Note that due to 	(screen	EYEDIAP (screen target)
the requirements of the temporal 	(screen	EYEDIAP (screen target)
model not all the frames 	(screen	EYEDIAP (screen target)
obtain a prediction. Therefore, for 	(screen	EYEDIAP (screen target)
a  Citation Citation {Parkhi, Vedaldi, and	(screen	EYEDIAP (screen target)
 Zisserman} 2015	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Bulat and Tzimiropoulos} 2017	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Citation Citation {Newell, Yang, and 	(screen	EYEDIAP (screen target)
Deng} 2016	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	(screen	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 7	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
60 30 0 30 60  60	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
30  0	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
30  60	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
100  101	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
102  60 30 0 30 60	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
60  30	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
0  30	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
60  100	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
101  102	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
103  60 30 0 30 60	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
60  30	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
0  30	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
60  100	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
101  102	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
60 30 0 30 60  60	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
30  0	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
30  60	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
100  101	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
102  103	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
a) g (FT ) (b) 	(screen	EYEDIAP (screen target)
h (FT ) (c) g (	(screen	EYEDIAP (screen target)
CS) (d) h (CS)  Figure 2: Ground-truth eye gaze	(screen	EYEDIAP (screen target)
 g and head orientation h	(screen	EYEDIAP (screen target)
 distribution on the filtered EYE	(screen	EYEDIAP (screen target)
- DIAP dataset for CS 	(screen	EYEDIAP (screen target)
and FT settings, in terms 	(screen	EYEDIAP (screen target)
of x- and y- angles.  fair comparison, the reported results	(screen	EYEDIAP (screen target)
 for static models disregard such	(screen	EYEDIAP (screen target)
 frames when temporal models are	(screen	EYEDIAP (screen target)
 included in the comparison	(screen	EYEDIAP (screen target)
.  4.1 Training data	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
There are few publicly available 	(screen	EYEDIAP (screen target)
datasets devoted to 3D gaze 	(screen	EYEDIAP (screen target)
estimation and most of them 	(screen	EYEDIAP (screen target)
focus on HCI with a 	(screen	EYEDIAP (screen target)
limited range of head pose 	(screen	EYEDIAP (screen target)
and gaze directions. Therefore, we 	(screen	EYEDIAP (screen target)
use VGA videos from the 	(screen	EYEDIAP (screen target)
publicly-available EYEDIAP dataset [7] to 	(screen	EYEDIAP (screen target)
perform the experimental evaluation, as 	(screen	EYEDIAP (screen target)
it is currently the only 	(screen	EYEDIAP (screen target)
one containing video sequences with 	(screen	EYEDIAP (screen target)
a wide range of head 	(screen	EYEDIAP (screen target)
poses and showing the full 	(screen	EYEDIAP (screen target)
face. This dataset consists of 3	(screen	EYEDIAP (screen target)
-minute videos of 16 subjects 	(screen	EYEDIAP (screen target)
looking at two types of 	(screen	EYEDIAP (screen target)
targets: continuous screen targets on 	(screen	EYEDIAP (screen target)
a fixed monitor (CS), and 	(screen	EYEDIAP (screen target)
floating physical targets (FT ). 	(screen	EYEDIAP (screen target)
The videos are further divided 	(screen	EYEDIAP (screen target)
into static (S) and moving (	(screen	EYEDIAP (screen target)
M) head pose for each 	(screen	EYEDIAP (screen target)
of the subjects. Subjects 12-16 	(screen	EYEDIAP (screen target)
were recorded with 2 different 	(screen	EYEDIAP (screen target)
lighting conditions.  For evaluation, we filtered out	(screen	EYEDIAP (screen target)
 those frames that fulfilled at	(screen	EYEDIAP (screen target)
 least one of the following	(screen	EYEDIAP (screen target)
 conditions: (1) face or landmarks	(screen	EYEDIAP (screen target)
 not detected; (2) subject not	(screen	EYEDIAP (screen target)
 looking at the target; (3	(screen	EYEDIAP (screen target)
) 3D head pose, eyes 	(screen	EYEDIAP (screen target)
or target location not properly 	(screen	EYEDIAP (screen target)
recovered; and (4) eyeball rotations 	(screen	EYEDIAP (screen target)
violating physical 	(screen	EYEDIAP (screen target)
constraints (|θ | ≤ 40	(screen	EYEDIAP (screen target)
◦, |φ | ≤ 30	(screen	EYEDIAP (screen target)
◦) [23]. Note that we 	(screen	EYEDIAP (screen target)
purposely do not filter eye 	(screen	EYEDIAP (screen target)
blinking moments to learn their 	(screen	EYEDIAP (screen target)
dynamics with the temporal model, 	(screen	EYEDIAP (screen target)
which may produce some outliers 	(screen	EYEDIAP (screen target)
with a higher prediction error 	(screen	EYEDIAP (screen target)
due to a less accurate 	(screen	EYEDIAP (screen target)
ground truth. Figure 2 shows 	(screen	EYEDIAP (screen target)
the distribution of gaze directions 	(screen	EYEDIAP (screen target)
and head poses for both 	(screen	EYEDIAP (screen target)
filtered CS and FT cases.  We applied data augmentation to	(screen	EYEDIAP (screen target)
 the training set with the	(screen	EYEDIAP (screen target)
 following random transforma- tions: horizontal	(screen	EYEDIAP (screen target)
 flip, shifts of up to	(screen	EYEDIAP (screen target)
 5 pixels, zoom of up	(screen	EYEDIAP (screen target)
 to 2%, brightness changes by	(screen	EYEDIAP (screen target)
 a factor in the range	(screen	EYEDIAP (screen target)
 [0.4,1.75], and additive Gaussian noise	(screen	EYEDIAP (screen target)
 with σ2 = 0.03	(screen	EYEDIAP (screen target)
.  4.2 Evaluation of static modalities	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
First, we evaluate the contribution 	(screen	EYEDIAP (screen target)
of each static modality on 	(screen	EYEDIAP (screen target)
the FT scenario. We divided 	(screen	EYEDIAP (screen target)
the 16 participants into 4 	(screen	EYEDIAP (screen target)
groups, such that appearance variability 	(screen	EYEDIAP (screen target)
was maximized while maintaining a 	(screen	EYEDIAP (screen target)
similar number of training samples 	(screen	EYEDIAP (screen target)
per group. Each static model 	(screen	EYEDIAP (screen target)
was trained end-to-end performing 4-fold 	(screen	EYEDIAP (screen target)
cross-validation using different combinations of 	(screen	EYEDIAP (screen target)
input modal- ities. Since the 	(screen	EYEDIAP (screen target)
number of fusion units depends 	(screen	EYEDIAP (screen target)
on the number of input 	(screen	EYEDIAP (screen target)
modalities, we also compare different 	(screen	EYEDIAP (screen target)
fusion layer sizes. The effect 	(screen	EYEDIAP (screen target)
of data normalization is also 	(screen	EYEDIAP (screen target)
evaluated by training a not-normalized 	(screen	EYEDIAP (screen target)
face model where the input 	(screen	EYEDIAP (screen target)
image is the face bounding 	(screen	EYEDIAP (screen target)
box with square size the 	(screen	EYEDIAP (screen target)
maximum distance between 2D landmarks.  Citation Citation {Funesprotect unhbox voidb@x	(screen	EYEDIAP (screen target)
 penalty @M	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Mora, Monay, and Odobez} 2014{}  Citation Citation {MSC	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
8 PALMERO ET AL.: MULTI-MODAL 	(screen	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	(screen	EYEDIAP (screen target)
ESTIMATION  0 1 2 3 4	(screen	EYEDIAP (screen target)
 5 6 7 8 9	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
10 11  An gl	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
e  er	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
ro r (  de gr	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
ee s)  6.9 6.43 5.58 5.71 5.59	(screen	EYEDIAP (screen target)
 5.55 5.52	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
OF-4096 NE-1536 NF-4096  NF-5632 NFL-4300	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
NFE-5632 NFEL-5836  Figure 3: Performance evaluation of	(screen	EYEDIAP (screen target)
 the Static network using different	(screen	EYEDIAP (screen target)
 input modali- ties (O	(screen	EYEDIAP (screen target)
 - Not normalized, N	(screen	EYEDIAP (screen target)
 - Normalized, F - Face	(screen	EYEDIAP (screen target)
, E - Eyes, 	(screen	EYEDIAP (screen target)
L - 3D Landmarks) and 	(screen	EYEDIAP (screen target)
size of fusion layers on 	(screen	EYEDIAP (screen target)
the FT scenario.  Floating Target Screen Target 0	(screen	EYEDIAP (screen target)
 1 2 3 4 5	(screen	EYEDIAP (screen target)
 6 7 8 9	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
10 11  An gl	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
e  er	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
ro r (  de gr	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
ee s)  6.36 5.43 5.19 4.2 3.38	(screen	EYEDIAP (screen target)
 3.4	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
MPIIGaze Static Temporal  Figure 4: Performance comparison among	(screen	EYEDIAP (screen target)
 MPIIGaze method [42] and our	(screen	EYEDIAP (screen target)
 Static and Temporal versions of	(screen	EYEDIAP (screen target)
 the proposed network for FT	(screen	EYEDIAP (screen target)
 and CS scenarios	(screen	EYEDIAP (screen target)
.  As shown in Figure 3	(screen	EYEDIAP (screen target)
, all models that take 	(screen	EYEDIAP (screen target)
normalized full-face information as input 	(screen	EYEDIAP (screen target)
achieve better performance than the 	(screen	EYEDIAP (screen target)
eyes-only model. More specifically, the 	(screen	EYEDIAP (screen target)
combination of face, eyes and 	(screen	EYEDIAP (screen target)
landmarks outperforms all the other 	(screen	EYEDIAP (screen target)
combinations by a small but 	(screen	EYEDIAP (screen target)
significant margin (paired Wilcoxon test, 	(screen	EYEDIAP (screen target)
p < 0.0001). The standard 	(screen	EYEDIAP (screen target)
deviation of the best-performing model 	(screen	EYEDIAP (screen target)
is reduced compared to the 	(screen	EYEDIAP (screen target)
face and eyes model, suggesting 	(screen	EYEDIAP (screen target)
a regularizing effect due to 	(screen	EYEDIAP (screen target)
the addition of landmarks. The 	(screen	EYEDIAP (screen target)
not-normalized face-only model shows the 	(screen	EYEDIAP (screen target)
largest error, proving the impact 	(screen	EYEDIAP (screen target)
of normalization to reduce the 	(screen	EYEDIAP (screen target)
appearance variability. Furthermore, our results 	(screen	EYEDIAP (screen target)
indicate that the increase of 	(screen	EYEDIAP (screen target)
fusion units is not correlated 	(screen	EYEDIAP (screen target)
with a better performance.  4.3 Static gaze regression: comparison	(screen	EYEDIAP (screen target)
 with existing methods	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
We compare our best-performing static 	(screen	EYEDIAP (screen target)
model with three baselines. Head: 	(screen	EYEDIAP (screen target)
Treating the head pose directly 	(screen	EYEDIAP (screen target)
as gaze direction. PR-ALR: Method 	(screen	EYEDIAP (screen target)
that relies on RGB-D data 	(screen	EYEDIAP (screen target)
to rectify the eye images 	(screen	EYEDIAP (screen target)
viewpoint into a canonical head 	(screen	EYEDIAP (screen target)
pose using a 3DMM. It 	(screen	EYEDIAP (screen target)
then learns an RGB gaze 	(screen	EYEDIAP (screen target)
appearance model using ALR [21]. 	(screen	EYEDIAP (screen target)
Predicted 3D vectors for FT-S 	(screen	EYEDIAP (screen target)
scenario are provided by EYEDIAP 	(screen	EYEDIAP (screen target)
dataset. MPIIGaze:. State-of-the-art full-face 3D 	(screen	EYEDIAP (screen target)
gaze estimation method [42]. They 	(screen	EYEDIAP (screen target)
use an Alexnet-based CNN model 	(screen	EYEDIAP (screen target)
with spatial weights to enhance 	(screen	EYEDIAP (screen target)
information in different facial regions. 	(screen	EYEDIAP (screen target)
We fine-tuned it with the 	(screen	EYEDIAP (screen target)
filtered EYEDIAP subsets using our 	(screen	EYEDIAP (screen target)
training parameters and normalization procedure.  In addition to the aforementioned	(screen	EYEDIAP (screen target)
 FT-based evaluation setup, we also	(screen	EYEDIAP (screen target)
 evaluate our method on the	(screen	EYEDIAP (screen target)
 CS scenario. In this case	(screen	EYEDIAP (screen target)
 there are only 14 participants	(screen	EYEDIAP (screen target)
 available, so we divided them	(screen	EYEDIAP (screen target)
 in 5 groups and performed	(screen	EYEDIAP (screen target)
 5-fold cross-validation. In Figure 4	(screen	EYEDIAP (screen target)
 we compare our method to	(screen	EYEDIAP (screen target)
 MPIIGaze, achieving a statistically significant	(screen	EYEDIAP (screen target)
 improvement of 14.6% and 19.5	(screen	EYEDIAP (screen target)
% on FT and CS 	(screen	EYEDIAP (screen target)
scenarios, respectively (paired Wilcoxon test, 	(screen	EYEDIAP (screen target)
p < 0.0001). We can 	(screen	EYEDIAP (screen target)
observe that a re- stricted 	(screen	EYEDIAP (screen target)
gaze target benefits the performance 	(screen	EYEDIAP (screen target)
of all methods, compared to 	(screen	EYEDIAP (screen target)
a more challenging unrestricted setting 	(screen	EYEDIAP (screen target)
with a wider range of 	(screen	EYEDIAP (screen target)
head poses and gaze directions.  Table 2 provides a detailed	(screen	EYEDIAP (screen target)
 comparison on every participant, performing	(screen	EYEDIAP (screen target)
 leave-one-out cross-validation on the FT	(screen	EYEDIAP (screen target)
 scenario for static and moving	(screen	EYEDIAP (screen target)
 head separately. Results show that	(screen	EYEDIAP (screen target)
, as expected, facial appearance 	(screen	EYEDIAP (screen target)
and head pose have a 	(screen	EYEDIAP (screen target)
noticeable impact on gaze accuracy, 	(screen	EYEDIAP (screen target)
with average error differences of 	(screen	EYEDIAP (screen target)
up to 7.7◦ among participants.  Citation Citation {Zhang, Sugano, Fritz	(screen	EYEDIAP (screen target)
, and Bulling} 2015  Citation Citation {Mora and Odobez	(screen	EYEDIAP (screen target)
} 2012  Citation Citation {Zhang, Sugano, Fritz	(screen	EYEDIAP (screen target)
, and Bulling} 2015	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	(screen	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 9	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Method 1 2 3 4 5 6 7 8 9 10	(screen	EYEDIAP (screen target)
 11 12 13 14 15	(screen	EYEDIAP (screen target)
 16 Avg. Head 23.5 22.1	(screen	EYEDIAP (screen target)
 20.3 23.6 23.2 23.2 23.6	(screen	EYEDIAP (screen target)
 21.2 26.7 23.6 23.1 24.4	(screen	EYEDIAP (screen target)
 23.3 24.0 24.5 22.8 23.3	(screen	EYEDIAP (screen target)
 PR-ALR 12.3 12.0 12.4 11.3	(screen	EYEDIAP (screen target)
 15.5 12.9 17.9 11.8 17.3	(screen	EYEDIAP (screen target)
 13.4 13.4 14.3 15.2 13.6	(screen	EYEDIAP (screen target)
 14.4 14.6 13.9 MPIIGaze 5.3	(screen	EYEDIAP (screen target)
 5.1 5.7 4.7 7.3 15.1	(screen	EYEDIAP (screen target)
 10.8 5.7 9.9 7.1 5.0	(screen	EYEDIAP (screen target)
 5.7 7.4 3.8 4.8 5.5	(screen	EYEDIAP (screen target)
 6.8 Static 3.9 4.1 4.2	(screen	EYEDIAP (screen target)
 3.9 6.0 6.4 7.2 3.6	(screen	EYEDIAP (screen target)
 7.1 5.0 5.7 6.7 3.9	(screen	EYEDIAP (screen target)
 4.7 5.1 4.2 5.1 Temporal	(screen	EYEDIAP (screen target)
 4.0 4.9 4.3 4.1 6.1	(screen	EYEDIAP (screen target)
 6.5 6.6 3.9 7.8 6.1	(screen	EYEDIAP (screen target)
 4.7 5.6 4.7 3.5 5.9	(screen	EYEDIAP (screen target)
 4.6 5.2 Head 19.3 14.2	(screen	EYEDIAP (screen target)
 16.4 19.9 16.8 21.9 16.1	(screen	EYEDIAP (screen target)
 24.2 20.3 19.9 18.8 22.3	(screen	EYEDIAP (screen target)
 18.1 14.9 16.2 19.3 18.7	(screen	EYEDIAP (screen target)
 MPIIGaze 7.6 6.2 5.7 8.7	(screen	EYEDIAP (screen target)
 10.1 12.0 12.2 6.1 8.3	(screen	EYEDIAP (screen target)
 5.9 6.1 6.2 7.4 4.7	(screen	EYEDIAP (screen target)
 4.4 6.0 7.3 Static 5.8	(screen	EYEDIAP (screen target)
 5.7 4.4 7.5 6.7 8.8	(screen	EYEDIAP (screen target)
 11.6 5.5 8.3 5.5 5.2	(screen	EYEDIAP (screen target)
 6.3 5.3 3.9 4.3 5.6	(screen	EYEDIAP (screen target)
 6.3 Temporal 6.1 5.6 4.5	(screen	EYEDIAP (screen target)
 7.5 6.4 8.2 12.0 5.0	(screen	EYEDIAP (screen target)
 7.5 5.4 5.0 5.8 6.6	(screen	EYEDIAP (screen target)
 4.0 4.5 5.8 6.2	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Table 2: Gaze angular error 	(screen	EYEDIAP (screen target)
comparison for static (top half) 	(screen	EYEDIAP (screen target)
and moving (bottom half) head 	(screen	EYEDIAP (screen target)
pose for each subject in 	(screen	EYEDIAP (screen target)
the FT scenario. Best results 	(screen	EYEDIAP (screen target)
in bold.  −80 −40 0 40 80−80	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
40  0	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
40  80	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
0  5	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
10  15	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
20  25	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
30  35	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
80 −40 0 40 80−80  −40	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
0  40	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
80  −10	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
8  −6	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
4  −2	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
0  2	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
4  6	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
8  10	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
80 −40 0 40 80−80  −40	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
0  40	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
80  0	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
5  10	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
15  20	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
25  30	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
35  −80 −40 0 40 80−80	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
40  0	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
40  80	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
10  −8	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
6  −4	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
2  0	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
2  4	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
6  8	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
10  (a) Gaze space (b) Head	(screen	EYEDIAP (screen target)
 orientation space	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
Figure 5: Angular error distribution 	(screen	EYEDIAP (screen target)
across gaze (a) and head 	(screen	EYEDIAP (screen target)
orientation (b) spaces in the 	(screen	EYEDIAP (screen target)
FT setting, in terms of 	(screen	EYEDIAP (screen target)
x- and y- angles. For 	(screen	EYEDIAP (screen target)
each space, we depict the 	(screen	EYEDIAP (screen target)
Static model performance (left) and 	(screen	EYEDIAP (screen target)
the contribution of the Temporal 	(screen	EYEDIAP (screen target)
model versus Static (right). In 	(screen	EYEDIAP (screen target)
the latter, positive difference means 	(screen	EYEDIAP (screen target)
higher improvement of the Temporal 	(screen	EYEDIAP (screen target)
model.  4.4 Evaluation of the temporal	(screen	EYEDIAP (screen target)
 network	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
In this section, we evaluate 	(screen	EYEDIAP (screen target)
the contribution of adding the 	(screen	EYEDIAP (screen target)
temporal module to the static 	(screen	EYEDIAP (screen target)
model. To do so, we 	(screen	EYEDIAP (screen target)
trained a lower-dimensional version of 	(screen	EYEDIAP (screen target)
the static network with compa- 	(screen	EYEDIAP (screen target)
rable performance to the original, 	(screen	EYEDIAP (screen target)
reducing the number of units 	(screen	EYEDIAP (screen target)
of the second fusion layer 	(screen	EYEDIAP (screen target)
to 2918. Results are reported 	(screen	EYEDIAP (screen target)
in Figure 4 and Table 2	(screen	EYEDIAP (screen target)
. One can observe that 	(screen	EYEDIAP (screen target)
using sequential information is helpful 	(screen	EYEDIAP (screen target)
on the FT scenario, outperforming 	(screen	EYEDIAP (screen target)
the static model by a 	(screen	EYEDIAP (screen target)
statistically significant 4.4% (paired Wilcoxon 	(screen	EYEDIAP (screen target)
test, p < 0.0001). This 	(screen	EYEDIAP (screen target)
contribution is more noticeable in 	(screen	EYEDIAP (screen target)
the moving head setting, proving 	(screen	EYEDIAP (screen target)
that the temporal model can 	(screen	EYEDIAP (screen target)
benefit from head motion information. 	(screen	EYEDIAP (screen target)
In contrast, such information seems 	(screen	EYEDIAP (screen target)
to be less meaningful in 	(screen	EYEDIAP (screen target)
the CS scenario, where the 	(screen	EYEDIAP (screen target)
obtained error is already very 	(screen	EYEDIAP (screen target)
low for a cross-subject setting 	(screen	EYEDIAP (screen target)
and the amount of head 	(screen	EYEDIAP (screen target)
movement declines.  Figure 5 further explores the	(screen	EYEDIAP (screen target)
 error distribution of the static	(screen	EYEDIAP (screen target)
 network and the impact of	(screen	EYEDIAP (screen target)
 sequential information. We can observe	(screen	EYEDIAP (screen target)
 that the accuracy of the	(screen	EYEDIAP (screen target)
 static model drops with extreme	(screen	EYEDIAP (screen target)
 head poses and gaze directions	(screen	EYEDIAP (screen target)
, which can also be 	(screen	EYEDIAP (screen target)
correlated to having less data 	(screen	EYEDIAP (screen target)
in those areas. Compared to 	(screen	EYEDIAP (screen target)
the static model, the temporal 	(screen	EYEDIAP (screen target)
model particularly benefits gaze targets 	(screen	EYEDIAP (screen target)
from mid-range upwards. Its contribution 	(screen	EYEDIAP (screen target)
is less clear for extreme 	(screen	EYEDIAP (screen target)
targets, probably again due to 	(screen	EYEDIAP (screen target)
data imbalance.  Finally, we evaluated the effect	(screen	EYEDIAP (screen target)
 of different recurrent architectures for	(screen	EYEDIAP (screen target)
 the temporal model. In particular	(screen	EYEDIAP (screen target)
, we tested 1 (128 	(screen	EYEDIAP (screen target)
units) and 2 (256-128 units) 	(screen	EYEDIAP (screen target)
LSTM and GRU lay- ers, 	(screen	EYEDIAP (screen target)
with 1 GRU layer obtaining 	(screen	EYEDIAP (screen target)
slightly superior results (up to 0	(screen	EYEDIAP (screen target)
.12◦). We also assessed the 	(screen	EYEDIAP (screen target)
effect of sequence length fixing 	(screen	EYEDIAP (screen target)
s in the range {4,7,10}, 	(screen	EYEDIAP (screen target)
with s = 7 performing 	(screen	EYEDIAP (screen target)
worse than the other two (	(screen	EYEDIAP (screen target)
up to 0	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
14	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
10 PALMERO ET AL.: MULTI-MODAL 	(screen	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	(screen	EYEDIAP (screen target)
ESTIMATION  5 Conclusions In this work	(screen	EYEDIAP (screen target)
, we studied the combination 	(screen	EYEDIAP (screen target)
of full-face and eye images 	(screen	EYEDIAP (screen target)
along with facial land- marks 	(screen	EYEDIAP (screen target)
for person- and head pose-independent 	(screen	EYEDIAP (screen target)
3D gaze estimation. Consequently, we 	(screen	EYEDIAP (screen target)
pro- posed a multi-stream recurrent 	(screen	EYEDIAP (screen target)
CNN network that leverages the 	(screen	EYEDIAP (screen target)
sequential information of eye and 	(screen	EYEDIAP (screen target)
head movements. Both static and 	(screen	EYEDIAP (screen target)
temporal versions of our approach 	(screen	EYEDIAP (screen target)
significantly outperform current state-of-the-art 3D 	(screen	EYEDIAP (screen target)
gaze estimation methods on a 	(screen	EYEDIAP (screen target)
wide range of head poses 	(screen	EYEDIAP (screen target)
and gaze directions. We showed 	(screen	EYEDIAP (screen target)
that adding geometry features to 	(screen	EYEDIAP (screen target)
appearance-based methods has a regularizing 	(screen	EYEDIAP (screen target)
effect on the accuracy. Adding 	(screen	EYEDIAP (screen target)
sequential information further benefits the 	(screen	EYEDIAP (screen target)
final performance compared to static-only 	(screen	EYEDIAP (screen target)
input, especially from mid-range up- 	(screen	EYEDIAP (screen target)
wards and in those cases 	(screen	EYEDIAP (screen target)
where head motion is present. 	(screen	EYEDIAP (screen target)
The effect in very extreme 	(screen	EYEDIAP (screen target)
head poses is not clear 	(screen	EYEDIAP (screen target)
due to data imbalance, suggesting 	(screen	EYEDIAP (screen target)
the importance of learning from 	(screen	EYEDIAP (screen target)
a con- tinuous, balanced dataset 	(screen	EYEDIAP (screen target)
including all head poses and 	(screen	EYEDIAP (screen target)
gaze directions of interest. To 	(screen	EYEDIAP (screen target)
the best of our knowledge, 	(screen	EYEDIAP (screen target)
this is the first attempt 	(screen	EYEDIAP (screen target)
to exploit the temporal modality 	(screen	EYEDIAP (screen target)
in the context of gaze 	(screen	EYEDIAP (screen target)
estimation from remote cameras. As 	(screen	EYEDIAP (screen target)
future work, we will further 	(screen	EYEDIAP (screen target)
explore extracting meaningful temporal representations 	(screen	EYEDIAP (screen target)
of gaze dynamics, considering 3DCNNs 	(screen	EYEDIAP (screen target)
as well as the encoding 	(screen	EYEDIAP (screen target)
of deep features around particular 	(screen	EYEDIAP (screen target)
tracked face landmarks [14].  Acknowledgements This work has been	(screen	EYEDIAP (screen target)
 partially supported by the Spanish	(screen	EYEDIAP (screen target)
 project TIN2016-74946-P (MINECO/ FEDER, UE	(screen	EYEDIAP (screen target)
), CERCA Programme / Generalitat 	(screen	EYEDIAP (screen target)
de Catalunya, and the FP7 	(screen	EYEDIAP (screen target)
people program (Marie Curie Actions), 	(screen	EYEDIAP (screen target)
REA grant agreement no FP7-607139 (	(screen	EYEDIAP (screen target)
iCARE - Improving Children Auditory 	(screen	EYEDIAP (screen target)
REhabilitation). We gratefully acknowledge the 	(screen	EYEDIAP (screen target)
support of NVIDIA Corporation with 	(screen	EYEDIAP (screen target)
the donation of the GPU 	(screen	EYEDIAP (screen target)
used for this research. Portions 	(screen	EYEDIAP (screen target)
of the research in this 	(screen	EYEDIAP (screen target)
pa- per used the EYEDIAP 	(screen	EYEDIAP (screen target)
dataset made available by the 	(screen	EYEDIAP (screen target)
Idiap Research Institute, Martigny, Switzerland.  References [1] Nicola C Anderson	(screen	EYEDIAP (screen target)
, Evan F Risko, and 	(screen	EYEDIAP (screen target)
Alan Kingstone. Motion influences gaze 	(screen	EYEDIAP (screen target)
di-  rection discrimination and disambiguates contradictory	(screen	EYEDIAP (screen target)
 luminance cues. Psychonomic bulletin	(screen	EYEDIAP (screen target)
 & review, 23(3):817–823, 2016	(screen	EYEDIAP (screen target)
.  [2] Shumeet Baluja and Dean	(screen	EYEDIAP (screen target)
 Pomerleau. Non-intrusive gaze tracking using	(screen	EYEDIAP (screen target)
 artificial neu- ral networks. In	(screen	EYEDIAP (screen target)
 Advances in Neural Information Processing	(screen	EYEDIAP (screen target)
 Systems, pages 753–760, 1994	(screen	EYEDIAP (screen target)
.  [3] Adrian Bulat and Georgios	(screen	EYEDIAP (screen target)
 Tzimiropoulos. How far are we	(screen	EYEDIAP (screen target)
 from solving the 2d	(screen	EYEDIAP (screen target)
 & 3d face alignment problem	(screen	EYEDIAP (screen target)
? (and a dataset of 230,	(screen	EYEDIAP (screen target)
000 3d facial landmarks). In 	(screen	EYEDIAP (screen target)
Interna- tional Conference on Computer 	(screen	EYEDIAP (screen target)
Vision, 2017.  [4] Haoping Deng and Wangjiang	(screen	EYEDIAP (screen target)
 Zhu. Monocular free-head 3d gaze	(screen	EYEDIAP (screen target)
 tracking with deep learning and	(screen	EYEDIAP (screen target)
 geometry constraints. In Computer Vision	(screen	EYEDIAP (screen target)
 (ICCV), 2017 IEEE Interna- tional	(screen	EYEDIAP (screen target)
 Conference on, pages 3162–3171. IEEE	(screen	EYEDIAP (screen target)
, 2017.  [5] Onur Ferhat and Fernando	(screen	EYEDIAP (screen target)
 Vilariño. Low cost eye tracking	(screen	EYEDIAP (screen target)
. Computational intelligence and neuroscience, 2016	(screen	EYEDIAP (screen target)
:17, 2016.  Citation Citation {Jung, Lee, Yim	(screen	EYEDIAP (screen target)
, Park, and Kim} 2015	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	(screen	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 11	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
6] Kenneth A Funes-Mora and 	(screen	EYEDIAP (screen target)
Jean-Marc Odobez. Gaze estimation in 	(screen	EYEDIAP (screen target)
the 3D space using RGB-D 	(screen	EYEDIAP (screen target)
sensors. International Journal of Computer 	(screen	EYEDIAP (screen target)
Vision, 118(2):194–216, 2016.  [7] Kenneth Alberto Funes Mora	(screen	EYEDIAP (screen target)
, Florent Monay, and Jean-Marc 	(screen	EYEDIAP (screen target)
Odobez. Eyediap: A database for 	(screen	EYEDIAP (screen target)
the development and evaluation of 	(screen	EYEDIAP (screen target)
gaze estimation algorithms from rgb 	(screen	EYEDIAP (screen target)
and rgb-d cameras. In Proceedings 	(screen	EYEDIAP (screen target)
of the ACM Symposium on 	(screen	EYEDIAP (screen target)
Eye Tracking Research and Applications. 	(screen	EYEDIAP (screen target)
ACM, March 2014. doi: 10.1145/2578153.2578190.  [8] Kenneth Alberto Funes Mora	(screen	EYEDIAP (screen target)
, Florent Monay, and Jean-Marc 	(screen	EYEDIAP (screen target)
Odobez. Eyediap: A database for 	(screen	EYEDIAP (screen target)
the development and evaluation of 	(screen	EYEDIAP (screen target)
gaze estimation algorithms from rgb 	(screen	EYEDIAP (screen target)
and rgb-d cameras. In Proceedings 	(screen	EYEDIAP (screen target)
of the Symposium on Eye 	(screen	EYEDIAP (screen target)
Tracking Research and Applications, pages 255	(screen	EYEDIAP (screen target)
–258. ACM, 2014.  [9] Quentin Guillon, Nouchine Hadjikhani	(screen	EYEDIAP (screen target)
, Sophie Baduel, and Bernadette 	(screen	EYEDIAP (screen target)
Rogé. Visual social attention in 	(screen	EYEDIAP (screen target)
autism spectrum disorder: Insights from 	(screen	EYEDIAP (screen target)
eye tracking studies. Neu- 	(screen	EYEDIAP (screen target)
roscience & Biobehavioral Reviews, 42:279–297, 2014	(screen	EYEDIAP (screen target)
.  [10] Dan Witzner Hansen and	(screen	EYEDIAP (screen target)
 Qiang Ji. In the eye	(screen	EYEDIAP (screen target)
 of the beholder: A survey	(screen	EYEDIAP (screen target)
 of models for eyes and	(screen	EYEDIAP (screen target)
 gaze. IEEE transactions on pattern	(screen	EYEDIAP (screen target)
 analysis and machine intelligence, 32(3	(screen	EYEDIAP (screen target)
): 478–500, 2010.  [11] Qiong Huang, Ashok Veeraraghavan	(screen	EYEDIAP (screen target)
, and Ashutosh Sabharwal. Tabletgaze: 	(screen	EYEDIAP (screen target)
dataset and analysis for unconstrained 	(screen	EYEDIAP (screen target)
appearance-based gaze estimation in mobile 	(screen	EYEDIAP (screen target)
tablets. Machine Vision and Applications, 28	(screen	EYEDIAP (screen target)
(5-6):445–461, 2017.  [12] Robert JK Jacob and	(screen	EYEDIAP (screen target)
 Keith S Karn. Eye tracking	(screen	EYEDIAP (screen target)
 in human-computer interaction and usability	(screen	EYEDIAP (screen target)
 research: Ready to deliver the	(screen	EYEDIAP (screen target)
 promises. In The mind’s eye	(screen	EYEDIAP (screen target)
, pages 573–605. Elsevier, 2003.  [13] László A Jeni and	(screen	EYEDIAP (screen target)
 Jeffrey F Cohn. Person-independent 3d	(screen	EYEDIAP (screen target)
 gaze estimation using face frontalization	(screen	EYEDIAP (screen target)
. In Proceedings of the 	(screen	EYEDIAP (screen target)
IEEE Conference on Computer Vision 	(screen	EYEDIAP (screen target)
and Pattern Recognition Workshops, pages 87	(screen	EYEDIAP (screen target)
–95, 2016.  [14] Heechul Jung, Sihaeng Lee	(screen	EYEDIAP (screen target)
, Junho Yim, Sunjeong Park, 	(screen	EYEDIAP (screen target)
and Junmo Kim. Joint fine- 	(screen	EYEDIAP (screen target)
tuning in deep neural networks 	(screen	EYEDIAP (screen target)
for facial expression recognition. In 	(screen	EYEDIAP (screen target)
Computer Vision (ICCV), 2015 IEEE 	(screen	EYEDIAP (screen target)
International Conference on, pages 2983–2991. 	(screen	EYEDIAP (screen target)
IEEE, 2015.  [15] Anuradha Kar and Peter	(screen	EYEDIAP (screen target)
 Corcoran. A review and analysis	(screen	EYEDIAP (screen target)
 of eye-gaze estimation sys- tems	(screen	EYEDIAP (screen target)
, algorithms and performance evaluation 	(screen	EYEDIAP (screen target)
methods in consumer platforms. IEEE 	(screen	EYEDIAP (screen target)
Access, 5:16495–16519, 2017.  [16] Kyle Krafka, Aditya Khosla	(screen	EYEDIAP (screen target)
, Petr Kellnhofer, Harini Kannan, 	(screen	EYEDIAP (screen target)
Suchendra Bhandarkar, Wojciech Matusik, and 	(screen	EYEDIAP (screen target)
Antonio Torralba. Eye tracking for 	(screen	EYEDIAP (screen target)
everyone. In Computer Vision and 	(screen	EYEDIAP (screen target)
Pattern Recognition (CVPR), 2016 IEEE 	(screen	EYEDIAP (screen target)
Conference on, pages 2176–2184. IEEE, 2016	(screen	EYEDIAP (screen target)
.  [17] Simon P Liversedge and	(screen	EYEDIAP (screen target)
 John M Findlay. Saccadic eye	(screen	EYEDIAP (screen target)
 movements and cognition. Trends in	(screen	EYEDIAP (screen target)
 cognitive sciences, 4(1):6–14, 2000	(screen	EYEDIAP (screen target)
.  [18] Feng Lu, Takahiro Okabe	(screen	EYEDIAP (screen target)
, Yusuke Sugano, and Yoichi 	(screen	EYEDIAP (screen target)
Sato. A head pose-free approach 	(screen	EYEDIAP (screen target)
for appearance-based gaze estimation. In 	(screen	EYEDIAP (screen target)
BMVC, pages 1–11, 2011	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
12 PALMERO ET AL.: MULTI-MODAL 	(screen	EYEDIAP (screen target)
RECURRENT CNN FOR 3D GAZE 	(screen	EYEDIAP (screen target)
ESTIMATION  [19] Feng Lu, Yusuke Sugano	(screen	EYEDIAP (screen target)
, Takahiro Okabe, and Yoichi 	(screen	EYEDIAP (screen target)
Sato. Inferring human gaze from 	(screen	EYEDIAP (screen target)
appearance via adaptive linear regression. 	(screen	EYEDIAP (screen target)
In Computer Vision (ICCV), 2011 	(screen	EYEDIAP (screen target)
IEEE International Conference on, pages 153	(screen	EYEDIAP (screen target)
–160. IEEE, 2011.  [20] Päivi Majaranta and Andreas	(screen	EYEDIAP (screen target)
 Bulling. Eye tracking and eye-based	(screen	EYEDIAP (screen target)
 human–computer interaction. In Advances in	(screen	EYEDIAP (screen target)
 physiological computing, pages 39–65. Springer	(screen	EYEDIAP (screen target)
, 2014.  [21] Kenneth Alberto Funes Mora	(screen	EYEDIAP (screen target)
 and Jean-Marc Odobez. Gaze estimation	(screen	EYEDIAP (screen target)
 from multi- modal kinect data	(screen	EYEDIAP (screen target)
. In Computer Vision and 	(screen	EYEDIAP (screen target)
Pattern Recognition Workshops (CVPRW), 2012 	(screen	EYEDIAP (screen target)
IEEE Computer Society Conference on, 	(screen	EYEDIAP (screen target)
pages 25–30. IEEE, 2012.  [22] Carlos Hitoshi Morimoto, Arnon	(screen	EYEDIAP (screen target)
 Amir, and Myron Flickner. Detecting	(screen	EYEDIAP (screen target)
 eye position and gaze from	(screen	EYEDIAP (screen target)
 a single camera and 2	(screen	EYEDIAP (screen target)
 light sources. In Pattern Recognition	(screen	EYEDIAP (screen target)
, 2002. Proceedings. 16th International 	(screen	EYEDIAP (screen target)
Conference on, volume 4, pages 314	(screen	EYEDIAP (screen target)
–317. IEEE, 2002.  [23] IMO MSC. Circ. 982	(screen	EYEDIAP (screen target)
 (2000) guidelines on ergonomic criteria	(screen	EYEDIAP (screen target)
 for bridge equipment and layout	(screen	EYEDIAP (screen target)
.  [24] Alejandro Newell, Kaiyu Yang	(screen	EYEDIAP (screen target)
, and Jia Deng. Stacked 	(screen	EYEDIAP (screen target)
hourglass networks for hu- man 	(screen	EYEDIAP (screen target)
pose estimation. In European Conference 	(screen	EYEDIAP (screen target)
on Computer Vision, pages 483–499. 	(screen	EYEDIAP (screen target)
Springer, 2016.  [25] Yasuhiro Ono, Takahiro Okabe	(screen	EYEDIAP (screen target)
, and Yoichi Sato. Gaze 	(screen	EYEDIAP (screen target)
estimation from low resolution images. 	(screen	EYEDIAP (screen target)
In Pacific-Rim Symposium on Image 	(screen	EYEDIAP (screen target)
and Video Technology, pages 178–188. 	(screen	EYEDIAP (screen target)
Springer, 2006.  [26] Cristina Palmero, Elisabeth A	(screen	EYEDIAP (screen target)
. van Dam, Sergio Escalera, 	(screen	EYEDIAP (screen target)
Mike Kelia, Guido F. Lichtert, 	(screen	EYEDIAP (screen target)
Lucas P.J.J Noldus, Andrew J. 	(screen	EYEDIAP (screen target)
Spink, and Astrid van Wieringen. 	(screen	EYEDIAP (screen target)
Automatic mutual gaze detection in 	(screen	EYEDIAP (screen target)
face-to-face dyadic interaction videos. In 	(screen	EYEDIAP (screen target)
Proceedings of Measuring Behavior, pages 158	(screen	EYEDIAP (screen target)
–163, 2018.  [27] Omkar M. Parkhi, Andrea	(screen	EYEDIAP (screen target)
 Vedaldi, and Andrew Zisserman. Deep	(screen	EYEDIAP (screen target)
 face recognition. In British Machine	(screen	EYEDIAP (screen target)
 Vision Conference, 2015	(screen	EYEDIAP (screen target)
.  [28] Derek R Rutter and	(screen	EYEDIAP (screen target)
 Kevin Durkin. Turn-taking in mother–infant	(screen	EYEDIAP (screen target)
 interaction: An exam- ination of	(screen	EYEDIAP (screen target)
 vocalizations and gaze. Developmental psychology	(screen	EYEDIAP (screen target)
, 23(1):54, 1987.  [29] Brian A Smith, Qi	(screen	EYEDIAP (screen target)
 Yin, Steven K Feiner, and	(screen	EYEDIAP (screen target)
 Shree K Nayar. Gaze locking	(screen	EYEDIAP (screen target)
: passive eye contact detection 	(screen	EYEDIAP (screen target)
for human-object interaction. In Proceedings 	(screen	EYEDIAP (screen target)
of the 26th annual ACM 	(screen	EYEDIAP (screen target)
symposium on User interface software 	(screen	EYEDIAP (screen target)
and technology, pages 271–280. ACM, 2013	(screen	EYEDIAP (screen target)
.  [30] Yusuke Sugano, Yasuyuki Matsushita	(screen	EYEDIAP (screen target)
, and Yoichi Sato. Appearance-based 	(screen	EYEDIAP (screen target)
gaze es- timation using visual 	(screen	EYEDIAP (screen target)
saliency. IEEE transactions on pattern 	(screen	EYEDIAP (screen target)
analysis and machine intelligence, 35(2):329–341, 2013	(screen	EYEDIAP (screen target)
.  [31] Yusuke Sugano, Yasuyuki Matsushita	(screen	EYEDIAP (screen target)
, and Yoichi Sato. Learning-by-synthesis 	(screen	EYEDIAP (screen target)
for appearance-based 3d gaze estimation. 	(screen	EYEDIAP (screen target)
In Computer Vision and Pattern 	(screen	EYEDIAP (screen target)
Recognition (CVPR), 2014 IEEE Conference 	(screen	EYEDIAP (screen target)
on, pages 1821–1828. IEEE, 2014.  [32] Kar-Han Tan, David J	(screen	EYEDIAP (screen target)
 Kriegman, and Narendra Ahuja. Appearance-based	(screen	EYEDIAP (screen target)
 eye gaze es- timation. In	(screen	EYEDIAP (screen target)
 Applications of Computer Vision, 2002.(WACV	(screen	EYEDIAP (screen target)
 2002). Proceedings. Sixth IEEE Workshop	(screen	EYEDIAP (screen target)
 on, pages 191–195. IEEE, 2002	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	(screen	EYEDIAP (screen target)
CNN FOR 3D GAZE ESTIMATION 13	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
33] Ronda Venkateswarlu et al. 	(screen	EYEDIAP (screen target)
Eye gaze estimation from a 	(screen	EYEDIAP (screen target)
single image of one eye. 	(screen	EYEDIAP (screen target)
In Computer Vision, 2003. Proceedings. 	(screen	EYEDIAP (screen target)
Ninth IEEE International Conference on, 	(screen	EYEDIAP (screen target)
pages 136–143. IEEE, 2003.  [34] Kang Wang and Qiang	(screen	EYEDIAP (screen target)
 Ji. Real time eye gaze	(screen	EYEDIAP (screen target)
 tracking with 3d deformable eye-face	(screen	EYEDIAP (screen target)
 model. In Proceedings of the	(screen	EYEDIAP (screen target)
 IEEE Conference on Computer Vision	(screen	EYEDIAP (screen target)
 and Pattern Recog- nition, pages	(screen	EYEDIAP (screen target)
 1003–1011, 2017	(screen	EYEDIAP (screen target)
.  [35] Oliver Williams, Andrew Blake	(screen	EYEDIAP (screen target)
, and Roberto Cipolla. Sparse 	(screen	EYEDIAP (screen target)
and semi-supervised visual mapping with 	(screen	EYEDIAP (screen target)
the sˆ 3gp. In Computer 	(screen	EYEDIAP (screen target)
Vision and Pattern Recognition, 2006 	(screen	EYEDIAP (screen target)
IEEE Computer Society Conference on, 	(screen	EYEDIAP (screen target)
volume 1, pages 230–237. IEEE, 2006	(screen	EYEDIAP (screen target)
.  [36] William Hyde Wollaston et	(screen	EYEDIAP (screen target)
 al. Xiii. on the apparent	(screen	EYEDIAP (screen target)
 direction of eyes in a	(screen	EYEDIAP (screen target)
 portrait. Philosophical Transactions of the	(screen	EYEDIAP (screen target)
 Royal Society of London, 114:247–256	(screen	EYEDIAP (screen target)
, 1824.  [37] Erroll Wood and Andreas	(screen	EYEDIAP (screen target)
 Bulling. Eyetab: Model-based gaze estimation	(screen	EYEDIAP (screen target)
 on unmodi- fied tablet computers	(screen	EYEDIAP (screen target)
. In Proceedings of the 	(screen	EYEDIAP (screen target)
Symposium on Eye Tracking Research 	(screen	EYEDIAP (screen target)
and Applications, pages 207–210. ACM, 2014	(screen	EYEDIAP (screen target)
.  [38] Erroll Wood, Tadas Baltrusaitis	(screen	EYEDIAP (screen target)
, Xucong Zhang, Yusuke Sugano, 	(screen	EYEDIAP (screen target)
Peter Robinson, and Andreas Bulling. 	(screen	EYEDIAP (screen target)
Rendering of eyes for eye-shape 	(screen	EYEDIAP (screen target)
registration and gaze estimation. In 	(screen	EYEDIAP (screen target)
Proceedings of the IEEE International 	(screen	EYEDIAP (screen target)
Conference on Computer Vision, pages 3756	(screen	EYEDIAP (screen target)
– 3764, 2015.  [39] Erroll Wood, Tadas Baltrušaitis	(screen	EYEDIAP (screen target)
, Louis-Philippe Morency, Peter Robinson, 	(screen	EYEDIAP (screen target)
and Andreas Bulling. A 3d 	(screen	EYEDIAP (screen target)
morphable eye region model for 	(screen	EYEDIAP (screen target)
gaze estimation. In European Confer- 	(screen	EYEDIAP (screen target)
ence on Computer Vision, pages 297	(screen	EYEDIAP (screen target)
–313. Springer, 2016.  [40] Erroll Wood, Tadas Baltrušaitis	(screen	EYEDIAP (screen target)
, Louis-Philippe Morency, Peter Robinson, 	(screen	EYEDIAP (screen target)
and Andreas Bulling. Learning an 	(screen	EYEDIAP (screen target)
appearance-based gaze estimator from one 	(screen	EYEDIAP (screen target)
million synthesised images. In Proceedings 	(screen	EYEDIAP (screen target)
of the Ninth Biennial ACM 	(screen	EYEDIAP (screen target)
Symposium on Eye Tracking Re- 	(screen	EYEDIAP (screen target)
search & Applications, pages 131–138. 	(screen	EYEDIAP (screen target)
ACM, 2016.  [41] Dong Hyun Yoo and	(screen	EYEDIAP (screen target)
 Myung Jin Chung. A novel	(screen	EYEDIAP (screen target)
 non-intrusive eye gaze estimation using	(screen	EYEDIAP (screen target)
 cross-ratio under large head motion	(screen	EYEDIAP (screen target)
. Computer Vision and Image 	(screen	EYEDIAP (screen target)
Understanding, 98(1):25–51, 2005.  [42] Xucong Zhang, Yusuke Sugano	(screen	EYEDIAP (screen target)
, Mario Fritz, and Andreas 	(screen	EYEDIAP (screen target)
Bulling. Appearance-based gaze estimation in 	(screen	EYEDIAP (screen target)
the wild. In Proceedings of 	(screen	EYEDIAP (screen target)
the IEEE Conference on Computer 	(screen	EYEDIAP (screen target)
Vision and Pattern Recognition, pages 4511	(screen	EYEDIAP (screen target)
–4520, 2015.  [43] Xucong Zhang, Yusuke Sugano	(screen	EYEDIAP (screen target)
, Mario Fritz, and Andreas 	(screen	EYEDIAP (screen target)
Bulling. It’s written all over 	(screen	EYEDIAP (screen target)
your face: Full-face appearance-based gaze 	(screen	EYEDIAP (screen target)
estimation. In Proc. IEEE International 	(screen	EYEDIAP (screen target)
Conference on Computer Vision and 	(screen	EYEDIAP (screen target)
Pattern Recognition Workshops (CVPRW), 2017	(screen	EYEDIAP (screen target)
	(screen	EYEDIAP (screen target)
state of the art on EYEDIAP dataset, further improved by 4	EYEDIAP	EYEDIAP (screen target)
of our solution on the EYEDIAP dataset [7] in a wide	EYEDIAP	EYEDIAP (screen target)
VGA videos from the publicly-available EYEDIAP dataset [7] to perform the	EYEDIAP	EYEDIAP (screen target)
FT-S scenario are provided by EYEDIAP dataset. MPIIGaze:. State-of-the-art full-face 3D	EYEDIAP	EYEDIAP (screen target)
fine-tuned it with the filtered EYEDIAP subsets using our training parameters	EYEDIAP	EYEDIAP (screen target)
this pa- per used the EYEDIAP dataset made available by the	EYEDIAP	EYEDIAP (screen target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 1		EYEDIAP (floating target)
		EYEDIAP (floating target)
Recurrent CNN for 3D Gaze 		EYEDIAP (floating target)
Estimation using Appearance and Shape 		EYEDIAP (floating target)
Cues  Cristina Palmero1,2		EYEDIAP (floating target)
		EYEDIAP (floating target)
crpalmec7@alumnes.ub.edu  1 Dept. Mathematics and Informatics		EYEDIAP (floating target)
 Universitat de Barcelona, Spain		EYEDIAP (floating target)
		EYEDIAP (floating target)
Javier Selva1  javier.selva.castello@est.fib.upc.edu		EYEDIAP (floating target)
		EYEDIAP (floating target)
2 Computer Vision Center Campus 		EYEDIAP (floating target)
UAB, Bellaterra, Spain  Mohammad Ali Bagheri3,4		EYEDIAP (floating target)
		EYEDIAP (floating target)
mohammadali.bagheri@ucalgary.ca  3 Dept. Electrical and Computer		EYEDIAP (floating target)
 Eng. University of Calgary, Canada		EYEDIAP (floating target)
		EYEDIAP (floating target)
Sergio Escalera1,2  sergio@maia.ub.es		EYEDIAP (floating target)
		EYEDIAP (floating target)
4 Dept. Engineering University of 		EYEDIAP (floating target)
Larestan, Iran  Abstract		EYEDIAP (floating target)
		EYEDIAP (floating target)
Gaze behavior is an important 		EYEDIAP (floating target)
non-verbal cue in social signal 		EYEDIAP (floating target)
processing and human- computer interaction. 		EYEDIAP (floating target)
In this paper, we tackle 		EYEDIAP (floating target)
the problem of person- and 		EYEDIAP (floating target)
head pose- independent 3D gaze 		EYEDIAP (floating target)
estimation from remote cameras, using 		EYEDIAP (floating target)
a multi-modal recurrent convolutional neural 		EYEDIAP (floating target)
network (CNN). We propose to 		EYEDIAP (floating target)
combine face, eyes region, and 		EYEDIAP (floating target)
face landmarks as individual streams 		EYEDIAP (floating target)
in a CNN to estimate 		EYEDIAP (floating target)
gaze in still images. Then, 		EYEDIAP (floating target)
we exploit the dynamic nature 		EYEDIAP (floating target)
of gaze by feeding the 		EYEDIAP (floating target)
learned features of all the 		EYEDIAP (floating target)
frames in a sequence to 		EYEDIAP (floating target)
a many-to-one recurrent module that 		EYEDIAP (floating target)
predicts the 3D gaze vector 		EYEDIAP (floating target)
of the last frame. Our 		EYEDIAP (floating target)
multi-modal static solution is evaluated 		EYEDIAP (floating target)
on a wide range of 		EYEDIAP (floating target)
head poses and gaze directions, 		EYEDIAP (floating target)
achieving a significant improvement of 14		EYEDIAP (floating target)
.6% over the state of 		EYEDIAP (floating target)
the art on EYEDIAP dataset, 		EYEDIAP (floating target)
further improved by 4% when 		EYEDIAP (floating target)
the temporal modality is included.  1 Introduction Eyes and their		EYEDIAP (floating target)
 movements are considered an important		EYEDIAP (floating target)
 cue in non-verbal behavior analysis		EYEDIAP (floating target)
, being involved in many 		EYEDIAP (floating target)
cognitive processes and reflecting our 		EYEDIAP (floating target)
internal state [17]. More specifically, 		EYEDIAP (floating target)
eye gaze behavior, as an 		EYEDIAP (floating target)
indicator of human visual attention, 		EYEDIAP (floating target)
has been widely studied to 		EYEDIAP (floating target)
assess communication skills [28] and 		EYEDIAP (floating target)
to identify possible behavioral 		EYEDIAP (floating target)
disorders [9]. Therefore, gaze estimation 		EYEDIAP (floating target)
has become an established line 		EYEDIAP (floating target)
of research in computer vision, 		EYEDIAP (floating target)
being a key feature in 		EYEDIAP (floating target)
human-computer interaction (HCI) and usability 		EYEDIAP (floating target)
research [12, 20].  Recent gaze estimation research has		EYEDIAP (floating target)
 focused on facilitating its use		EYEDIAP (floating target)
 in general everyday applications under		EYEDIAP (floating target)
 real-world conditions, using off-the-shelf remote		EYEDIAP (floating target)
 RGB cameras and re- moving		EYEDIAP (floating target)
 the need of personal calibration		EYEDIAP (floating target)
 [26]. In this setting, appearance-based		EYEDIAP (floating target)
 methods, which learn a mapping		EYEDIAP (floating target)
 from images to gaze directions		EYEDIAP (floating target)
, are the preferred 		EYEDIAP (floating target)
choice [25]. How- ever, they 		EYEDIAP (floating target)
need large amounts of training 		EYEDIAP (floating target)
data to be able to 		EYEDIAP (floating target)
generalize well to in-the-wild situations, 		EYEDIAP (floating target)
which are characterized by significant 		EYEDIAP (floating target)
variability in head poses, face 		EYEDIAP (floating target)
appearances and lighting conditions. In 		EYEDIAP (floating target)
recent years, CNNs have been 		EYEDIAP (floating target)
reported to outperform classical methods. 		EYEDIAP (floating target)
However, most existing approaches have 		EYEDIAP (floating target)
only been tested in restricted 		EYEDIAP (floating target)
HCI tasks,  c© 2018. The copyright of		EYEDIAP (floating target)
 this document resides with its		EYEDIAP (floating target)
 authors. It may be distributed		EYEDIAP (floating target)
 unchanged freely in print or		EYEDIAP (floating target)
 electronic forms		EYEDIAP (floating target)
.  ar X		EYEDIAP (floating target)
		EYEDIAP (floating target)
iv :1  80 5		EYEDIAP (floating target)
.  03 06		EYEDIAP (floating target)
		EYEDIAP (floating target)
4v 3		EYEDIAP (floating target)
		EYEDIAP (floating target)
cs  .C V		EYEDIAP (floating target)
		EYEDIAP (floating target)
1  7		EYEDIAP (floating target)
		EYEDIAP (floating target)
Se  p		EYEDIAP (floating target)
		EYEDIAP (floating target)
20  18		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Liversedge and Findlay} 2000		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Rutter and Durkin} 1987		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Guillon, Hadjikhani, Baduel, 		EYEDIAP (floating target)
and Rog{é}} 2014  Citation Citation {Jacob and Karn		EYEDIAP (floating target)
} 2003  Citation Citation {Majaranta and Bulling		EYEDIAP (floating target)
} 2014  Citation Citation {Palmero, van Dam		EYEDIAP (floating target)
, Escalera, Kelia, Lichtert, Noldus, 		EYEDIAP (floating target)
Spink, and van Wieringen} 2018  Citation Citation {Ono, Okabe, and		EYEDIAP (floating target)
 Sato} 2006		EYEDIAP (floating target)
		EYEDIAP (floating target)
2 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (floating target)
ESTIMATION  Method 3D gaze direction		EYEDIAP (floating target)
		EYEDIAP (floating target)
Unrestricted gaze target  Full face		EYEDIAP (floating target)
		EYEDIAP (floating target)
Eye region  Facial landmarks		EYEDIAP (floating target)
		EYEDIAP (floating target)
Sequential information  Zhang et al. (1) [42		EYEDIAP (floating target)
] 3 7 7 3 7 7 Krafka et al. [16		EYEDIAP (floating target)
] 7 7 3 3 7 7 Zhang et al. (2		EYEDIAP (floating target)
) [43] 3 7 3 7 7 7 Deng and Zhu		EYEDIAP (floating target)
 [4] 3 3 3 3		EYEDIAP (floating target)
 7 7 Ours 3 3		EYEDIAP (floating target)
 3 3 3 3		EYEDIAP (floating target)
		EYEDIAP (floating target)
Table 1: Characteristics of recent 		EYEDIAP (floating target)
related work on person- and 		EYEDIAP (floating target)
head pose-independent appearance-based gaze estimation 		EYEDIAP (floating target)
methods using CNNs.  where users look at the		EYEDIAP (floating target)
 screen or mobile phone, showing		EYEDIAP (floating target)
 a low head pose variability		EYEDIAP (floating target)
. It is yet unclear 		EYEDIAP (floating target)
how these methods would perform 		EYEDIAP (floating target)
in a wider range of 		EYEDIAP (floating target)
head poses.  On a different note, until		EYEDIAP (floating target)
 very recently, the majority of		EYEDIAP (floating target)
 methods only used static eye		EYEDIAP (floating target)
 region appearance as input. State-of-the-art		EYEDIAP (floating target)
 approaches have demonstrated that using		EYEDIAP (floating target)
 the face along with a		EYEDIAP (floating target)
 higher resolution image of the		EYEDIAP (floating target)
 eyes [16], or even just		EYEDIAP (floating target)
 the face itself [43], increases		EYEDIAP (floating target)
 performance. Indeed, the whole-face image		EYEDIAP (floating target)
 encodes more information than eyes		EYEDIAP (floating target)
 alone, such as illumination and		EYEDIAP (floating target)
 head pose. Nevertheless, gaze behavior		EYEDIAP (floating target)
 is not static. Eye and		EYEDIAP (floating target)
 head movements allow us to		EYEDIAP (floating target)
 direct our gaze to target		EYEDIAP (floating target)
 locations of interest. It has		EYEDIAP (floating target)
 been demonstrated that humans can		EYEDIAP (floating target)
 better predict gaze when being		EYEDIAP (floating target)
 shown image sequences of other		EYEDIAP (floating target)
 people moving their eyes [1		EYEDIAP (floating target)
]. However, it is still 		EYEDIAP (floating target)
an open question whether this 		EYEDIAP (floating target)
se- quential information can increase 		EYEDIAP (floating target)
the performance of automatic methods.  In this work, we show		EYEDIAP (floating target)
 that the combination of multiple		EYEDIAP (floating target)
 cues benefits the gaze estimation		EYEDIAP (floating target)
 task. In particular, we use		EYEDIAP (floating target)
 face, eye region and facial		EYEDIAP (floating target)
 landmarks from still images. Facial		EYEDIAP (floating target)
 landmarks model the global shape		EYEDIAP (floating target)
 of the face and come		EYEDIAP (floating target)
 at no cost, since face		EYEDIAP (floating target)
 alignment is a common pre-processing		EYEDIAP (floating target)
 step in many facial image		EYEDIAP (floating target)
 analysis approaches. Furthermore, we present		EYEDIAP (floating target)
 a subject-independent, free-head recurrent 3D		EYEDIAP (floating target)
 gaze regression network to leverage		EYEDIAP (floating target)
 the temporal information of image		EYEDIAP (floating target)
 sequences. The static streams of		EYEDIAP (floating target)
 each frame are combined in		EYEDIAP (floating target)
 a late-fusion fashion using a		EYEDIAP (floating target)
 multi-stream CNN. Then, all feature		EYEDIAP (floating target)
 vectors are input to a		EYEDIAP (floating target)
 many-to-one recurrent module that predicts		EYEDIAP (floating target)
 the gaze vector of the		EYEDIAP (floating target)
 last sequence frame		EYEDIAP (floating target)
.  In summary, our contributions are		EYEDIAP (floating target)
 two-fold. First, we present a		EYEDIAP (floating target)
 Recurrent-CNN net- work architecture that		EYEDIAP (floating target)
 combines appearance, shape and temporal		EYEDIAP (floating target)
 information for 3D gaze estimation		EYEDIAP (floating target)
. Second, we test static 		EYEDIAP (floating target)
and temporal versions of our 		EYEDIAP (floating target)
solution on the EYEDIAP 		EYEDIAP (floating target)
dataset [7] in a wide 		EYEDIAP (floating target)
range of head poses and 		EYEDIAP (floating target)
gaze directions, showing consistent perfor- 		EYEDIAP (floating target)
mance improvements compared to related 		EYEDIAP (floating target)
appearance-based methods. To the best 		EYEDIAP (floating target)
of our knowledge, this is 		EYEDIAP (floating target)
the first third-person, remote camera-based 		EYEDIAP (floating target)
approach that uses tempo- ral 		EYEDIAP (floating target)
information for this task. Table 1 outlines our main method characteristics		EYEDIAP (floating target)
 compared to related work. Models		EYEDIAP (floating target)
 and code are publicly available		EYEDIAP (floating target)
 at https://github.com/ crisie/RecurrentGaze		EYEDIAP (floating target)
.  2 Related work Gaze estimation		EYEDIAP (floating target)
 methods are typically categorized as		EYEDIAP (floating target)
 model-based or appearance-based [5, 10		EYEDIAP (floating target)
, 15]. Model-based approaches use 		EYEDIAP (floating target)
a geometric model of the 		EYEDIAP (floating target)
eye, usually requir- ing either 		EYEDIAP (floating target)
high resolution images or a 		EYEDIAP (floating target)
person-specific calibration stage to estimate 		EYEDIAP (floating target)
personal eye parameters [22, 33, 34, 37, 41]. In contrast, appearance-based		EYEDIAP (floating target)
 methods learn a di- rect		EYEDIAP (floating target)
 mapping from intensity images or		EYEDIAP (floating target)
 extracted eye features to gaze		EYEDIAP (floating target)
 directions, thus being		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Zhang, Sugano, Fritz, 		EYEDIAP (floating target)
and Bulling} 2015  Citation Citation {Krafka, Khosla, Kellnhofer		EYEDIAP (floating target)
, Kannan, Bhandarkar, Matusik, and 		EYEDIAP (floating target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (floating target)
, and Bulling} 2017  Citation Citation {Deng and Zhu		EYEDIAP (floating target)
} 2017  Citation Citation {Krafka, Khosla, Kellnhofer		EYEDIAP (floating target)
, Kannan, Bhandarkar, Matusik, and 		EYEDIAP (floating target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (floating target)
, and Bulling} 2017  Citation Citation {Anderson, Risko, and		EYEDIAP (floating target)
 Kingstone} 2016		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Funesprotect unhbox voidb@x 		EYEDIAP (floating target)
penalty @M  {}Mora, Monay, and Odobez} 2014		EYEDIAP (floating target)
{}  Citation Citation {Ferhat and Vilari{ñ}o		EYEDIAP (floating target)
} 2016  Citation Citation {Hansen and Ji		EYEDIAP (floating target)
} 2010  Citation Citation {Kar and Corcoran		EYEDIAP (floating target)
} 2017  Citation Citation {Morimoto, Amir, and		EYEDIAP (floating target)
 Flickner} 2002		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Venkateswarlu etprotect unhbox 		EYEDIAP (floating target)
voidb@x penalty @M  {}al.} 2003		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Wang and Ji} 2017		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Wood and Bulling} 2014		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Yoo and Chung} 2005		EYEDIAP (floating target)
		EYEDIAP (floating target)
https://github.com/crisie/RecurrentGaze 		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 3		EYEDIAP (floating target)
		EYEDIAP (floating target)
potentially applicable to relatively low 		EYEDIAP (floating target)
resolution images and mid-distance scenarios. 		EYEDIAP (floating target)
Dif- ferent mapping functions have 		EYEDIAP (floating target)
been explored, such as neural 		EYEDIAP (floating target)
networks [2], adaptive linear regression (		EYEDIAP (floating target)
ALR) [19], local interpolation [32], 		EYEDIAP (floating target)
gaussian processes [30, 35], random 		EYEDIAP (floating target)
forests [11, 31], or k-nearest 		EYEDIAP (floating target)
neighbors [40]. Main challenges of 		EYEDIAP (floating target)
appearance-based methods for 3D gaze 		EYEDIAP (floating target)
estimation are head pose, illumination 		EYEDIAP (floating target)
and subject invariance without user-specific 		EYEDIAP (floating target)
calibration. To handle these issues, 		EYEDIAP (floating target)
some works proposed compensation 		EYEDIAP (floating target)
methods [18] and warping strategies 		EYEDIAP (floating target)
that synthesize a canonical, frontal 		EYEDIAP (floating target)
looking view of the 		EYEDIAP (floating target)
face [6, 13, 21]. Hybrid 		EYEDIAP (floating target)
approaches based on analysis-by-synthesis have 		EYEDIAP (floating target)
also been evaluated [39].  Currently, data-driven methods are considered		EYEDIAP (floating target)
 the state of the art		EYEDIAP (floating target)
 for person- and head pose-independent		EYEDIAP (floating target)
 appearance-based gaze estimation. Consequently, a		EYEDIAP (floating target)
 number of gaze es- timation		EYEDIAP (floating target)
 datasets have been introduced in		EYEDIAP (floating target)
 recent years, either in controlled		EYEDIAP (floating target)
 [29] or semi- controlled settings		EYEDIAP (floating target)
 [8], in the wild [16		EYEDIAP (floating target)
, 42], or consisting of 		EYEDIAP (floating target)
synthetic data [31, 38, 40]. 		EYEDIAP (floating target)
Zhang et al. [42] showed 		EYEDIAP (floating target)
that CNNs can outperform other 		EYEDIAP (floating target)
mapping methods, using a multi- 		EYEDIAP (floating target)
modal CNN to learn the 		EYEDIAP (floating target)
mapping from 3D head poses 		EYEDIAP (floating target)
and eye images to 3D 		EYEDIAP (floating target)
gaze directions. Krafka et 		EYEDIAP (floating target)
al. [16] proposed a multi-stream 		EYEDIAP (floating target)
CNN for 2D gaze estimation, 		EYEDIAP (floating target)
using individual eye, whole-face image 		EYEDIAP (floating target)
and the face grid as 		EYEDIAP (floating target)
input. As this method was 		EYEDIAP (floating target)
limited to 2D screen mapping, 		EYEDIAP (floating target)
Zhang et al. [43] later 		EYEDIAP (floating target)
explored the potential of just 		EYEDIAP (floating target)
using whole-face images as input 		EYEDIAP (floating target)
to estimate 3D gaze directions. 		EYEDIAP (floating target)
Using a spatial weights CNN, 		EYEDIAP (floating target)
they demonstrated their method to 		EYEDIAP (floating target)
be more robust to facial 		EYEDIAP (floating target)
appearance variation caused by head 		EYEDIAP (floating target)
pose and illumina- tion than 		EYEDIAP (floating target)
eye-only methods. While the method 		EYEDIAP (floating target)
was evaluated in the wild, 		EYEDIAP (floating target)
the subjects were only interacting 		EYEDIAP (floating target)
with a mobile device, thus 		EYEDIAP (floating target)
restricting the head pose range. 		EYEDIAP (floating target)
Deng and Zhu [4] presented 		EYEDIAP (floating target)
a two-stream CNN to disjointly 		EYEDIAP (floating target)
model head pose from face 		EYEDIAP (floating target)
images and eye- ball movement 		EYEDIAP (floating target)
from eye region images. Both 		EYEDIAP (floating target)
were then aggregated into 3D 		EYEDIAP (floating target)
gaze direction using a gaze 		EYEDIAP (floating target)
transform layer. The decomposition was 		EYEDIAP (floating target)
aimed to avoid head-correlation over- 		EYEDIAP (floating target)
fitting of previous data-driven approaches. 		EYEDIAP (floating target)
They evaluated their approach in 		EYEDIAP (floating target)
the wild with a wider 		EYEDIAP (floating target)
range of head poses, obtaining 		EYEDIAP (floating target)
better performance than previous eye-based 		EYEDIAP (floating target)
methods. However, they did not 		EYEDIAP (floating target)
test it on public annotated 		EYEDIAP (floating target)
benchmark datasets.  In this paper, we propose		EYEDIAP (floating target)
 a multi-stream recurrent CNN network		EYEDIAP (floating target)
 for person- and head pose-independent		EYEDIAP (floating target)
 3D gaze estimation for a		EYEDIAP (floating target)
 mid-distance scenario. We evaluate it		EYEDIAP (floating target)
 on a wider range of		EYEDIAP (floating target)
 head poses and gaze directions		EYEDIAP (floating target)
 than screen-targeted approaches. As opposed		EYEDIAP (floating target)
 to previous methods, we also		EYEDIAP (floating target)
 rely on temporal information inherent		EYEDIAP (floating target)
 in sequential data		EYEDIAP (floating target)
.  3 Methodology		EYEDIAP (floating target)
		EYEDIAP (floating target)
In this section, we present 		EYEDIAP (floating target)
our approach for 3D gaze 		EYEDIAP (floating target)
regression based on appearance and 		EYEDIAP (floating target)
shape cues for still images 		EYEDIAP (floating target)
and image sequences. First, we 		EYEDIAP (floating target)
introduce the data modalities and 		EYEDIAP (floating target)
formulate the problem. Then, we 		EYEDIAP (floating target)
detail the normalization procedure prior 		EYEDIAP (floating target)
to the regression stage. Finally, 		EYEDIAP (floating target)
we explain the global network 		EYEDIAP (floating target)
topology as well as the 		EYEDIAP (floating target)
implementation details. An overview of 		EYEDIAP (floating target)
the system architecture is depicted 		EYEDIAP (floating target)
in Figure 1.  3.1 Multi-modal gaze regression		EYEDIAP (floating target)
		EYEDIAP (floating target)
Let us represent gaze direction 		EYEDIAP (floating target)
as a 3D unit vector 		EYEDIAP (floating target)
g = [gx,gy,gz]T ∈R3 in 		EYEDIAP (floating target)
the Camera Coor- dinate System (		EYEDIAP (floating target)
CCS), whose origin is the 		EYEDIAP (floating target)
central point between eyeball centers. 		EYEDIAP (floating target)
Assuming a calibrated camera, and 		EYEDIAP (floating target)
a known head position and 		EYEDIAP (floating target)
orientation, our goal is to 		EYEDIAP (floating target)
estimate g from a sequence 		EYEDIAP (floating target)
of images {I(i) | 		EYEDIAP (floating target)
I ∈ RW×H×3} as a 		EYEDIAP (floating target)
regression problem.  Citation Citation {Baluja and Pomerleau		EYEDIAP (floating target)
} 1994  Citation Citation {Lu, Sugano, Okabe		EYEDIAP (floating target)
, and Sato} 2011{}  Citation Citation {Tan, Kriegman, and		EYEDIAP (floating target)
 Ahuja} 2002		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Sugano, Matsushita, and 		EYEDIAP (floating target)
Sato} 2013  Citation Citation {Williams, Blake, and		EYEDIAP (floating target)
 Cipolla} 2006		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Huang, Veeraraghavan, and 		EYEDIAP (floating target)
Sabharwal} 2017  Citation Citation {Sugano, Matsushita, and		EYEDIAP (floating target)
 Sato} 2014		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Wood, Baltru{²}aitis, Morency, 		EYEDIAP (floating target)
Robinson, and Bulling} 2016{}  Citation Citation {Lu, Okabe, Sugano		EYEDIAP (floating target)
, and Sato} 2011{}  Citation Citation {Funes-Mora and Odobez		EYEDIAP (floating target)
} 2016  Citation Citation {Jeni and Cohn		EYEDIAP (floating target)
} 2016  Citation Citation {Mora and Odobez		EYEDIAP (floating target)
} 2012  Citation Citation {Wood, Baltru{²}aitis, Morency		EYEDIAP (floating target)
, Robinson, and Bulling} 2016{}  Citation Citation {Smith, Yin, Feiner		EYEDIAP (floating target)
, and Nayar} 2013  Citation Citation {Funesprotect unhbox voidb@x		EYEDIAP (floating target)
 penalty @M		EYEDIAP (floating target)
		EYEDIAP (floating target)
Mora, Monay, and Odobez} 2014{}  Citation Citation {Krafka, Khosla, Kellnhofer		EYEDIAP (floating target)
, Kannan, Bhandarkar, Matusik, and 		EYEDIAP (floating target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (floating target)
, and Bulling} 2015  Citation Citation {Sugano, Matsushita, and		EYEDIAP (floating target)
 Sato} 2014		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Wood, Baltrusaitis, Zhang, 		EYEDIAP (floating target)
Sugano, Robinson, and Bulling} 2015  Citation Citation {Wood, Baltru{²}aitis, Morency		EYEDIAP (floating target)
, Robinson, and Bulling} 2016{}  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (floating target)
, and Bulling} 2015  Citation Citation {Krafka, Khosla, Kellnhofer		EYEDIAP (floating target)
, Kannan, Bhandarkar, Matusik, and 		EYEDIAP (floating target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (floating target)
, and Bulling} 2017  Citation Citation {Deng and Zhu		EYEDIAP (floating target)
} 2017		EYEDIAP (floating target)
		EYEDIAP (floating target)
4 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (floating target)
ESTIMATION  Conv		EYEDIAP (floating target)
		EYEDIAP (floating target)
C on ca t  x y z x y		EYEDIAP (floating target)
 z x y z		EYEDIAP (floating target)
		EYEDIAP (floating target)
Individual Fusion Temporal  Individual Fusion		EYEDIAP (floating target)
		EYEDIAP (floating target)
Input 		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
Individual Fusion  Normalization		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
 .Conv		EYEDIAP (floating target)
		EYEDIAP (floating target)
Conv .  Conv		EYEDIAP (floating target)
		EYEDIAP (floating target)
Conv .  FC		EYEDIAP (floating target)
		EYEDIAP (floating target)
FC FC RNN  RNN		EYEDIAP (floating target)
		EYEDIAP (floating target)
RNN FC  Ti m e		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
Figure 1: Overview of the 		EYEDIAP (floating target)
proposed network. A multi-stream CNN 		EYEDIAP (floating target)
jointly models full-face, eye region 		EYEDIAP (floating target)
appearance and face landmarks from 		EYEDIAP (floating target)
still images. The combined extracted 		EYEDIAP (floating target)
fea- tures from each frame 		EYEDIAP (floating target)
are fed into a recurrent 		EYEDIAP (floating target)
module to predict last frame’s 		EYEDIAP (floating target)
gaze direction.  Gazing to a specific target		EYEDIAP (floating target)
 is achieved by a combination		EYEDIAP (floating target)
 of eye and head movements		EYEDIAP (floating target)
, which are highly coordinated. 		EYEDIAP (floating target)
Consequently, the apparent direction of 		EYEDIAP (floating target)
gaze is influenced not only 		EYEDIAP (floating target)
by the location of the 		EYEDIAP (floating target)
irises within the eyelid aperture, 		EYEDIAP (floating target)
but also by the position 		EYEDIAP (floating target)
and orientation of the face 		EYEDIAP (floating target)
with respect to the camera. 		EYEDIAP (floating target)
Known as the Wollaston 		EYEDIAP (floating target)
effect [36], the exact same 		EYEDIAP (floating target)
set of eyes may appear 		EYEDIAP (floating target)
to be looking in different 		EYEDIAP (floating target)
directions due to the surrounding 		EYEDIAP (floating target)
facial cues. It is therefore 		EYEDIAP (floating target)
reasonable to state that eye 		EYEDIAP (floating target)
images are not sufficient to 		EYEDIAP (floating target)
estimate gaze direction. Instead, whole-face 		EYEDIAP (floating target)
images can encode head pose 		EYEDIAP (floating target)
or illumination-specific information across larger 		EYEDIAP (floating target)
areas than those available just 		EYEDIAP (floating target)
in the eyes region [16, 43		EYEDIAP (floating target)
].  The drawback of appearance-only methods		EYEDIAP (floating target)
 is that global structure information		EYEDIAP (floating target)
 is not explicitly considered. In		EYEDIAP (floating target)
 that sense, facial landmarks can		EYEDIAP (floating target)
 be used as global shape		EYEDIAP (floating target)
 cues to en- code spatial		EYEDIAP (floating target)
 relationships and geometric constraints. Current		EYEDIAP (floating target)
 state-of-the-art face alignment approaches are		EYEDIAP (floating target)
 robust enough to handle large		EYEDIAP (floating target)
 appearance variability, extreme head poses		EYEDIAP (floating target)
 and occlusions, being especially useful		EYEDIAP (floating target)
 when the dataset used for		EYEDIAP (floating target)
 gaze estimation does not contain		EYEDIAP (floating target)
 such variability. Facial landmarks are		EYEDIAP (floating target)
 mainly correlated with head orientation		EYEDIAP (floating target)
, eye position, eyelid openness, 		EYEDIAP (floating target)
and eyebrow movement, which are 		EYEDIAP (floating target)
valuable features for our task.  Therefore, in our approach we		EYEDIAP (floating target)
 jointly model appearance and shape		EYEDIAP (floating target)
 cues (see Figure 1). The		EYEDIAP (floating target)
 former is represented by a		EYEDIAP (floating target)
 whole-face image IF , along		EYEDIAP (floating target)
 with a higher resolution image		EYEDIAP (floating target)
 of the eyes IE to		EYEDIAP (floating target)
 identify subtle changes. Due to		EYEDIAP (floating target)
 dealing with wide head pose		EYEDIAP (floating target)
 ranges, some eye images may		EYEDIAP (floating target)
 not depict the whole eye		EYEDIAP (floating target)
, containing mostly background or 		EYEDIAP (floating target)
other surrounding facial parts instead. 		EYEDIAP (floating target)
For that reason, and contrary 		EYEDIAP (floating target)
to previous approaches that only 		EYEDIAP (floating target)
use one eye image [31, 42		EYEDIAP (floating target)
], we use a single 		EYEDIAP (floating target)
image composed of two patches 		EYEDIAP (floating target)
of centered left and right 		EYEDIAP (floating target)
eyes. Finally, the shape cue 		EYEDIAP (floating target)
is represented by 3D face 		EYEDIAP (floating target)
landmarks obtained from a 68-landmark 		EYEDIAP (floating target)
model, denoted by 		EYEDIAP (floating target)
L = {(lx, ly, 		EYEDIAP (floating target)
		EYEDIAP (floating target)
)		EYEDIAP (floating target)
		EYEDIAP (floating target)
 | ∀c ∈ [1, ...,68		EYEDIAP (floating target)
]}.  In this work we also		EYEDIAP (floating target)
 consider the dynamic component of		EYEDIAP (floating target)
 gaze. We leverage the se		EYEDIAP (floating target)
- quential information of eye 		EYEDIAP (floating target)
and head movements such that, 		EYEDIAP (floating target)
given appearance and shape features 		EYEDIAP (floating target)
of consecutive frames, it is 		EYEDIAP (floating target)
possible to better predict the 		EYEDIAP (floating target)
gaze direction of the cur- 		EYEDIAP (floating target)
rent frame. Therefore, the 3D 		EYEDIAP (floating target)
gaze estimation task for a 1		EYEDIAP (floating target)
-frame sequence is formulated  Citation Citation {Wollaston etprotect unhbox		EYEDIAP (floating target)
 voidb@x penalty @M		EYEDIAP (floating target)
		EYEDIAP (floating target)
al.} 1824  Citation Citation {Krafka, Khosla, Kellnhofer		EYEDIAP (floating target)
, Kannan, Bhandarkar, Matusik, and 		EYEDIAP (floating target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (floating target)
, and Bulling} 2017  Citation Citation {Sugano, Matsushita, and		EYEDIAP (floating target)
 Sato} 2014		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Zhang, Sugano, Fritz, 		EYEDIAP (floating target)
and Bulling} 2015		EYEDIAP (floating target)
		EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 5		EYEDIAP (floating target)
		EYEDIAP (floating target)
as g(i) = f ( {IF (i)},{IE (i)},{L(i		EYEDIAP (floating target)
)}  ) , where i denotes		EYEDIAP (floating target)
 the i-th frame, and f		EYEDIAP (floating target)
 is the regression		EYEDIAP (floating target)
		EYEDIAP (floating target)
function.  3.2 Data normalization Prior to		EYEDIAP (floating target)
 gaze regression, a normalization step		EYEDIAP (floating target)
 in the 3D space and		EYEDIAP (floating target)
 the 2D image, similar to		EYEDIAP (floating target)
 [31], is carried out. This		EYEDIAP (floating target)
 is performed to reduce the		EYEDIAP (floating target)
 appearance variability and to allow		EYEDIAP (floating target)
 the gaze estimation model to		EYEDIAP (floating target)
 be applied regardless of the		EYEDIAP (floating target)
 original camera configuration		EYEDIAP (floating target)
.  Let H ∈ R3x3 be		EYEDIAP (floating target)
 the head rotation matrix, and		EYEDIAP (floating target)
 p = [px, py, pz]T		EYEDIAP (floating target)
 ∈ R3 the reference face		EYEDIAP (floating target)
 location with respect to the		EYEDIAP (floating target)
 original CCS. The goal is		EYEDIAP (floating target)
 to find the conversion matrix		EYEDIAP (floating target)
 M = SR such that		EYEDIAP (floating target)
 (a) the X-axes of the		EYEDIAP (floating target)
 virtual camera and the head		EYEDIAP (floating target)
 become parallel using the rotation		EYEDIAP (floating target)
 matrix R, and (b) the		EYEDIAP (floating target)
 virtual camera looks at the		EYEDIAP (floating target)
 reference location from a fixed		EYEDIAP (floating target)
 distance dn using the Z-direction		EYEDIAP (floating target)
 scaling matrix S = diag(1,1,dn/‖p		EYEDIAP (floating target)
‖). R is computed as 		EYEDIAP (floating target)
a = p̂×HT e1, 		EYEDIAP (floating target)
b = â× p̂, 		EYEDIAP (floating target)
R = [â, b̂, p̂]T , where e1 denotes the first		EYEDIAP (floating target)
 orthonormal basis and		EYEDIAP (floating target)
 〈 ·̂ 〉 is the		EYEDIAP (floating target)
 unit vector		EYEDIAP (floating target)
.  This normalization translates into the		EYEDIAP (floating target)
 image space as a cropped		EYEDIAP (floating target)
 image patch of size Wn×Hn		EYEDIAP (floating target)
 centered at p where head		EYEDIAP (floating target)
 roll rotation has been removed		EYEDIAP (floating target)
. This is done by 		EYEDIAP (floating target)
applying a perspective warping to 		EYEDIAP (floating target)
the input image I using 		EYEDIAP (floating target)
the transformation matrix W = 		EYEDIAP (floating target)
CoMCn−1, where Co and Cn 		EYEDIAP (floating target)
are the original and virtual 		EYEDIAP (floating target)
camera matrices, respectively.  The 3D gaze vector is		EYEDIAP (floating target)
 also normalized as gn =Rg		EYEDIAP (floating target)
. After image normalization, the 		EYEDIAP (floating target)
line of sight can be 		EYEDIAP (floating target)
represented in a 2D space. 		EYEDIAP (floating target)
Therefore, gn is further transformed 		EYEDIAP (floating target)
to spherical coor- dinates (θ ,		EYEDIAP (floating target)
φ) assuming unit length, where 		EYEDIAP (floating target)
θ and φ denote the 		EYEDIAP (floating target)
horizontal and vertical direc- tion 		EYEDIAP (floating target)
angles, respectively. This 2D angle 		EYEDIAP (floating target)
representation, delimited in the 		EYEDIAP (floating target)
range [−π/2,π/2], is computed as 		EYEDIAP (floating target)
θ = arctan(gx/gz) and 		EYEDIAP (floating target)
φ = arcsin(−gy), such that (0,		EYEDIAP (floating target)
0) represents looking straight ahead 		EYEDIAP (floating target)
to the CCS origin.  3.3 Recurrent Convolutional Neural Network		EYEDIAP (floating target)
 We propose a Recurrent CNN		EYEDIAP (floating target)
 Regression Network for 3D gaze		EYEDIAP (floating target)
 estimation. The network is divided		EYEDIAP (floating target)
 in 3 modules: (1) Individual		EYEDIAP (floating target)
, (2) Fusion, and (3) 		EYEDIAP (floating target)
Temporal.  First, the Individual module learns		EYEDIAP (floating target)
 features from each appearance cue		EYEDIAP (floating target)
 separately. It consists of a		EYEDIAP (floating target)
 two-stream CNN, one devoted to		EYEDIAP (floating target)
 the normalized face image stream		EYEDIAP (floating target)
 and the other to the		EYEDIAP (floating target)
 joint normalized eyes image. Next		EYEDIAP (floating target)
, the Fusion module combines 		EYEDIAP (floating target)
the extracted features of each 		EYEDIAP (floating target)
appearance stream in a single 		EYEDIAP (floating target)
vector along with the normalized 		EYEDIAP (floating target)
landmark coordinates. Then, it learns 		EYEDIAP (floating target)
a joint representation between modalities 		EYEDIAP (floating target)
in a late-fusion fashion. Both 		EYEDIAP (floating target)
Individual and Fusion modules, further 		EYEDIAP (floating target)
referred to as Static model, 		EYEDIAP (floating target)
are applied to each frame 		EYEDIAP (floating target)
of the sequence. Finally, the 		EYEDIAP (floating target)
resulting feature vectors of each 		EYEDIAP (floating target)
frame are input to the 		EYEDIAP (floating target)
Temporal module based on a 		EYEDIAP (floating target)
many-to-one recurrent network. This module 		EYEDIAP (floating target)
leverages sequential information to predict 		EYEDIAP (floating target)
the normalized 2D gaze angles 		EYEDIAP (floating target)
of the last frame of 		EYEDIAP (floating target)
the sequence using a linear 		EYEDIAP (floating target)
regression layer added on top 		EYEDIAP (floating target)
of it.  3.4 Implementation details 3.4.1 Network		EYEDIAP (floating target)
 details		EYEDIAP (floating target)
		EYEDIAP (floating target)
Each stream of the Individual 		EYEDIAP (floating target)
module is based on the 		EYEDIAP (floating target)
VGG-16 deep network [27], consisting 		EYEDIAP (floating target)
of 13 convolutional layers, 5 		EYEDIAP (floating target)
max pooling layers, and 1 		EYEDIAP (floating target)
fully connected (FC) layer with 		EYEDIAP (floating target)
Rec- tified Linear Unit (ReLU) 		EYEDIAP (floating target)
activations. The full-face stream follows 		EYEDIAP (floating target)
the same configuration  Citation Citation {Sugano, Matsushita, and		EYEDIAP (floating target)
 Sato} 2014		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Parkhi, Vedaldi, and 		EYEDIAP (floating target)
Zisserman} 2015		EYEDIAP (floating target)
		EYEDIAP (floating target)
6 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (floating target)
ESTIMATION  as the base network, having		EYEDIAP (floating target)
 an input of 224×224 pixels		EYEDIAP (floating target)
 and a 4096D FC layer		EYEDIAP (floating target)
. In contrast, the input 		EYEDIAP (floating target)
joint eye image is smaller, 		EYEDIAP (floating target)
with a final size of 120		EYEDIAP (floating target)
×48 pixels, so the number 		EYEDIAP (floating target)
of pa- rameters is decreased 		EYEDIAP (floating target)
proportionally. In this case, its 		EYEDIAP (floating target)
last FC layer produces a 		EYEDIAP (floating target)
1536D vector. A 204D landmark 		EYEDIAP (floating target)
coordinates vector is concatenated to 		EYEDIAP (floating target)
the output of the FC 		EYEDIAP (floating target)
layer of each stream, resulting 		EYEDIAP (floating target)
in a 5836D feature vector. 		EYEDIAP (floating target)
Consequently, the Fusion module consists 		EYEDIAP (floating target)
of 2 5836D FC layers 		EYEDIAP (floating target)
with ReLU activations and 2 		EYEDIAP (floating target)
dropout layers between FCs as 		EYEDIAP (floating target)
regularization. Finally, to model the 		EYEDIAP (floating target)
temporal dependencies, we use a 		EYEDIAP (floating target)
single GRU layer with 128 		EYEDIAP (floating target)
units.  The network is trained in		EYEDIAP (floating target)
 a stage-wise fashion. First, we		EYEDIAP (floating target)
 train the Static model and		EYEDIAP (floating target)
 the final regression layer end-to-end		EYEDIAP (floating target)
 on each individual frame of		EYEDIAP (floating target)
 the training data. The convolutional		EYEDIAP (floating target)
 blocks are pre-trained with the		EYEDIAP (floating target)
 VGG-Face dataset [27], whereas the		EYEDIAP (floating target)
 FCs are trained from scratch		EYEDIAP (floating target)
. Second, the training data 		EYEDIAP (floating target)
is re-arranged by means of 		EYEDIAP (floating target)
a sliding window with stride 1 to build input sequences. Each		EYEDIAP (floating target)
 sequence is composed of s		EYEDIAP (floating target)
 = 4 consecutive frames, whose		EYEDIAP (floating target)
 gaze direction target is the		EYEDIAP (floating target)
 gaze direction of the last		EYEDIAP (floating target)
 frame of the sequence( {I(i−s+1		EYEDIAP (floating target)
), . . . ,I(i)}, 		EYEDIAP (floating target)
g(i)  ) . Using this re-arranged		EYEDIAP (floating target)
 training data, we extract features		EYEDIAP (floating target)
 of each		EYEDIAP (floating target)
		EYEDIAP (floating target)
frame of the sequence from 		EYEDIAP (floating target)
a frozen Individual module, fine-tune 		EYEDIAP (floating target)
the Fusion layers, and train 		EYEDIAP (floating target)
both, the Temporal module and 		EYEDIAP (floating target)
a new final regression layer 		EYEDIAP (floating target)
from scratch. This way, the 		EYEDIAP (floating target)
network can exploit the temporal 		EYEDIAP (floating target)
information to further refine the 		EYEDIAP (floating target)
fusion weights.  We trained the model using		EYEDIAP (floating target)
 ADAM optimizer with an initial		EYEDIAP (floating target)
 learning rate of 0.0001, dropout		EYEDIAP (floating target)
 of 0.3, and batch size		EYEDIAP (floating target)
 of 64 frames. The number		EYEDIAP (floating target)
 of epochs was experimentally set		EYEDIAP (floating target)
 to 21 for the first		EYEDIAP (floating target)
 training stage and 10 for		EYEDIAP (floating target)
 the second. We use the		EYEDIAP (floating target)
 average Euclidean distance between the		EYEDIAP (floating target)
 predicted and ground-truth 3D gaze		EYEDIAP (floating target)
 vectors as loss function		EYEDIAP (floating target)
.  3.4.2 Input pre-processing		EYEDIAP (floating target)
		EYEDIAP (floating target)
For this work we use 		EYEDIAP (floating target)
head pose and eye locations 		EYEDIAP (floating target)
in the 3D scene provided 		EYEDIAP (floating target)
by the dataset. The 3D 		EYEDIAP (floating target)
landmarks are extracted using the 		EYEDIAP (floating target)
state-of-the-art method of Bulat and 		EYEDIAP (floating target)
Tzimiropou- los [3], which is 		EYEDIAP (floating target)
based on stacked hourglass 		EYEDIAP (floating target)
networks [24].  During training, the original image		EYEDIAP (floating target)
 is pre-processed to get the		EYEDIAP (floating target)
 two normalized input images. The		EYEDIAP (floating target)
 normalized whole-face patch is centered		EYEDIAP (floating target)
 0.1 meters ahead of the		EYEDIAP (floating target)
 head center in the head		EYEDIAP (floating target)
 coordinate system, and Cn is		EYEDIAP (floating target)
 defined such that the image		EYEDIAP (floating target)
 has size of 250× 250		EYEDIAP (floating target)
 pixels. The difference between this		EYEDIAP (floating target)
 size and the final input		EYEDIAP (floating target)
 size allows us to perform		EYEDIAP (floating target)
 random cropping and zooming to		EYEDIAP (floating target)
 augment the data (explained in		EYEDIAP (floating target)
 Section 4.1). Similarly, each normalized		EYEDIAP (floating target)
 eye patch is centered in		EYEDIAP (floating target)
 their respective eye center locations		EYEDIAP (floating target)
. In this case, the 		EYEDIAP (floating target)
virtual camera matrix is defined 		EYEDIAP (floating target)
so that the image is 		EYEDIAP (floating target)
cropped to 70×58, while in 		EYEDIAP (floating target)
practice the final patches have 		EYEDIAP (floating target)
size of 60×48. Landmarks are 		EYEDIAP (floating target)
normalized using the same procedure 		EYEDIAP (floating target)
and further pre-processed with mean 		EYEDIAP (floating target)
subtraction and min-max normalization per 		EYEDIAP (floating target)
axis. Finally, we divide them 		EYEDIAP (floating target)
by a scaling factor w 		EYEDIAP (floating target)
such that all coordinates are 		EYEDIAP (floating target)
in the range [0,w]. This 		EYEDIAP (floating target)
way, all concatenated feature values 		EYEDIAP (floating target)
are in a similar range. 		EYEDIAP (floating target)
After inference, the predicted normalized 		EYEDIAP (floating target)
2D angles are de-normalized back 		EYEDIAP (floating target)
to the original 3D space.  4 Experiments In this section		EYEDIAP (floating target)
, we evaluate the cross-subject 		EYEDIAP (floating target)
3D gaze estimation task on 		EYEDIAP (floating target)
a wide range of head 		EYEDIAP (floating target)
poses and gaze directions. Furthermore, 		EYEDIAP (floating target)
we validate the effectiveness of 		EYEDIAP (floating target)
the proposed architecture comparing both 		EYEDIAP (floating target)
static and temporal approaches. We 		EYEDIAP (floating target)
report the error in terms 		EYEDIAP (floating target)
of mean angular error between 		EYEDIAP (floating target)
predicted and ground-truth 3D gaze 		EYEDIAP (floating target)
vectors. Note that due to 		EYEDIAP (floating target)
the requirements of the temporal 		EYEDIAP (floating target)
model not all the frames 		EYEDIAP (floating target)
obtain a prediction. Therefore, for 		EYEDIAP (floating target)
a  Citation Citation {Parkhi, Vedaldi, and		EYEDIAP (floating target)
 Zisserman} 2015		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Bulat and Tzimiropoulos} 2017		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Newell, Yang, and 		EYEDIAP (floating target)
Deng} 2016		EYEDIAP (floating target)
		EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 7		EYEDIAP (floating target)
		EYEDIAP (floating target)
60 30 0 30 60  60		EYEDIAP (floating target)
		EYEDIAP (floating target)
30  0		EYEDIAP (floating target)
		EYEDIAP (floating target)
30  60		EYEDIAP (floating target)
		EYEDIAP (floating target)
100  101		EYEDIAP (floating target)
		EYEDIAP (floating target)
102  60 30 0 30 60		EYEDIAP (floating target)
		EYEDIAP (floating target)
60  30		EYEDIAP (floating target)
		EYEDIAP (floating target)
0  30		EYEDIAP (floating target)
		EYEDIAP (floating target)
60  100		EYEDIAP (floating target)
		EYEDIAP (floating target)
101  102		EYEDIAP (floating target)
		EYEDIAP (floating target)
103  60 30 0 30 60		EYEDIAP (floating target)
		EYEDIAP (floating target)
60  30		EYEDIAP (floating target)
		EYEDIAP (floating target)
0  30		EYEDIAP (floating target)
		EYEDIAP (floating target)
60  100		EYEDIAP (floating target)
		EYEDIAP (floating target)
101  102		EYEDIAP (floating target)
		EYEDIAP (floating target)
60 30 0 30 60  60		EYEDIAP (floating target)
		EYEDIAP (floating target)
30  0		EYEDIAP (floating target)
		EYEDIAP (floating target)
30  60		EYEDIAP (floating target)
		EYEDIAP (floating target)
100  101		EYEDIAP (floating target)
		EYEDIAP (floating target)
102  103		EYEDIAP (floating target)
		EYEDIAP (floating target)
a) g (FT ) (b) 		EYEDIAP (floating target)
h (FT ) (c) g (		EYEDIAP (floating target)
CS) (d) h (CS)  Figure 2: Ground-truth eye gaze		EYEDIAP (floating target)
 g and head orientation h		EYEDIAP (floating target)
 distribution on the filtered EYE		EYEDIAP (floating target)
- DIAP dataset for CS 		EYEDIAP (floating target)
and FT settings, in terms 		EYEDIAP (floating target)
of x- and y- angles.  fair comparison, the reported results		EYEDIAP (floating target)
 for static models disregard such		EYEDIAP (floating target)
 frames when temporal models are		EYEDIAP (floating target)
 included in the comparison		EYEDIAP (floating target)
.  4.1 Training data		EYEDIAP (floating target)
		EYEDIAP (floating target)
There are few publicly available 		EYEDIAP (floating target)
datasets devoted to 3D gaze 		EYEDIAP (floating target)
estimation and most of them 		EYEDIAP (floating target)
focus on HCI with a 		EYEDIAP (floating target)
limited range of head pose 		EYEDIAP (floating target)
and gaze directions. Therefore, we 		EYEDIAP (floating target)
use VGA videos from the 		EYEDIAP (floating target)
publicly-available EYEDIAP dataset [7] to 		EYEDIAP (floating target)
perform the experimental evaluation, as 		EYEDIAP (floating target)
it is currently the only 		EYEDIAP (floating target)
one containing video sequences with 		EYEDIAP (floating target)
a wide range of head 		EYEDIAP (floating target)
poses and showing the full 		EYEDIAP (floating target)
face. This dataset consists of 3		EYEDIAP (floating target)
-minute videos of 16 subjects 		EYEDIAP (floating target)
looking at two types of 		EYEDIAP (floating target)
targets: continuous screen targets on 		EYEDIAP (floating target)
a fixed monitor (CS), and 		EYEDIAP (floating target)
floating physical targets (FT ). 		EYEDIAP (floating target)
The videos are further divided 		EYEDIAP (floating target)
into static (S) and moving (		EYEDIAP (floating target)
M) head pose for each 		EYEDIAP (floating target)
of the subjects. Subjects 12-16 		EYEDIAP (floating target)
were recorded with 2 different 		EYEDIAP (floating target)
lighting conditions.  For evaluation, we filtered out		EYEDIAP (floating target)
 those frames that fulfilled at		EYEDIAP (floating target)
 least one of the following		EYEDIAP (floating target)
 conditions: (1) face or landmarks		EYEDIAP (floating target)
 not detected; (2) subject not		EYEDIAP (floating target)
 looking at the target; (3		EYEDIAP (floating target)
) 3D head pose, eyes 		EYEDIAP (floating target)
or target location not properly 		EYEDIAP (floating target)
recovered; and (4) eyeball rotations 		EYEDIAP (floating target)
violating physical 		EYEDIAP (floating target)
constraints (|θ | ≤ 40		EYEDIAP (floating target)
◦, |φ | ≤ 30		EYEDIAP (floating target)
◦) [23]. Note that we 		EYEDIAP (floating target)
purposely do not filter eye 		EYEDIAP (floating target)
blinking moments to learn their 		EYEDIAP (floating target)
dynamics with the temporal model, 		EYEDIAP (floating target)
which may produce some outliers 		EYEDIAP (floating target)
with a higher prediction error 		EYEDIAP (floating target)
due to a less accurate 		EYEDIAP (floating target)
ground truth. Figure 2 shows 		EYEDIAP (floating target)
the distribution of gaze directions 		EYEDIAP (floating target)
and head poses for both 		EYEDIAP (floating target)
filtered CS and FT cases.  We applied data augmentation to		EYEDIAP (floating target)
 the training set with the		EYEDIAP (floating target)
 following random transforma- tions: horizontal		EYEDIAP (floating target)
 flip, shifts of up to		EYEDIAP (floating target)
 5 pixels, zoom of up		EYEDIAP (floating target)
 to 2%, brightness changes by		EYEDIAP (floating target)
 a factor in the range		EYEDIAP (floating target)
 [0.4,1.75], and additive Gaussian noise		EYEDIAP (floating target)
 with σ2 = 0.03		EYEDIAP (floating target)
.  4.2 Evaluation of static modalities		EYEDIAP (floating target)
		EYEDIAP (floating target)
First, we evaluate the contribution 		EYEDIAP (floating target)
of each static modality on 		EYEDIAP (floating target)
the FT scenario. We divided 		EYEDIAP (floating target)
the 16 participants into 4 		EYEDIAP (floating target)
groups, such that appearance variability 		EYEDIAP (floating target)
was maximized while maintaining a 		EYEDIAP (floating target)
similar number of training samples 		EYEDIAP (floating target)
per group. Each static model 		EYEDIAP (floating target)
was trained end-to-end performing 4-fold 		EYEDIAP (floating target)
cross-validation using different combinations of 		EYEDIAP (floating target)
input modal- ities. Since the 		EYEDIAP (floating target)
number of fusion units depends 		EYEDIAP (floating target)
on the number of input 		EYEDIAP (floating target)
modalities, we also compare different 		EYEDIAP (floating target)
fusion layer sizes. The effect 		EYEDIAP (floating target)
of data normalization is also 		EYEDIAP (floating target)
evaluated by training a not-normalized 		EYEDIAP (floating target)
face model where the input 		EYEDIAP (floating target)
image is the face bounding 		EYEDIAP (floating target)
box with square size the 		EYEDIAP (floating target)
maximum distance between 2D landmarks.  Citation Citation {Funesprotect unhbox voidb@x		EYEDIAP (floating target)
 penalty @M		EYEDIAP (floating target)
		EYEDIAP (floating target)
Mora, Monay, and Odobez} 2014{}  Citation Citation {MSC		EYEDIAP (floating target)
		EYEDIAP (floating target)
8 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (floating target)
ESTIMATION  0 1 2 3 4		EYEDIAP (floating target)
 5 6 7 8 9		EYEDIAP (floating target)
		EYEDIAP (floating target)
10 11  An gl		EYEDIAP (floating target)
		EYEDIAP (floating target)
e  er		EYEDIAP (floating target)
		EYEDIAP (floating target)
ro r (  de gr		EYEDIAP (floating target)
		EYEDIAP (floating target)
ee s)  6.9 6.43 5.58 5.71 5.59		EYEDIAP (floating target)
 5.55 5.52		EYEDIAP (floating target)
		EYEDIAP (floating target)
OF-4096 NE-1536 NF-4096  NF-5632 NFL-4300		EYEDIAP (floating target)
		EYEDIAP (floating target)
NFE-5632 NFEL-5836  Figure 3: Performance evaluation of		EYEDIAP (floating target)
 the Static network using different		EYEDIAP (floating target)
 input modali- ties (O		EYEDIAP (floating target)
 - Not normalized, N		EYEDIAP (floating target)
 - Normalized, F - Face		EYEDIAP (floating target)
, E - Eyes, 		EYEDIAP (floating target)
L - 3D Landmarks) and 		EYEDIAP (floating target)
size of fusion layers on 		EYEDIAP (floating target)
the FT scenario.  Floating Target Screen Target 0		EYEDIAP (floating target)
 1 2 3 4 5		EYEDIAP (floating target)
 6 7 8 9		EYEDIAP (floating target)
		EYEDIAP (floating target)
10 11  An gl		EYEDIAP (floating target)
		EYEDIAP (floating target)
e  er		EYEDIAP (floating target)
		EYEDIAP (floating target)
ro r (  de gr		EYEDIAP (floating target)
		EYEDIAP (floating target)
ee s)  6.36 5.43 5.19 4.2 3.38		EYEDIAP (floating target)
 3.4		EYEDIAP (floating target)
		EYEDIAP (floating target)
MPIIGaze Static Temporal  Figure 4: Performance comparison among		EYEDIAP (floating target)
 MPIIGaze method [42] and our		EYEDIAP (floating target)
 Static and Temporal versions of		EYEDIAP (floating target)
 the proposed network for FT		EYEDIAP (floating target)
 and CS scenarios		EYEDIAP (floating target)
.  As shown in Figure 3		EYEDIAP (floating target)
, all models that take 		EYEDIAP (floating target)
normalized full-face information as input 		EYEDIAP (floating target)
achieve better performance than the 		EYEDIAP (floating target)
eyes-only model. More specifically, the 		EYEDIAP (floating target)
combination of face, eyes and 		EYEDIAP (floating target)
landmarks outperforms all the other 		EYEDIAP (floating target)
combinations by a small but 		EYEDIAP (floating target)
significant margin (paired Wilcoxon test, 		EYEDIAP (floating target)
p < 0.0001). The standard 		EYEDIAP (floating target)
deviation of the best-performing model 		EYEDIAP (floating target)
is reduced compared to the 		EYEDIAP (floating target)
face and eyes model, suggesting 		EYEDIAP (floating target)
a regularizing effect due to 		EYEDIAP (floating target)
the addition of landmarks. The 		EYEDIAP (floating target)
not-normalized face-only model shows the 		EYEDIAP (floating target)
largest error, proving the impact 		EYEDIAP (floating target)
of normalization to reduce the 		EYEDIAP (floating target)
appearance variability. Furthermore, our results 		EYEDIAP (floating target)
indicate that the increase of 		EYEDIAP (floating target)
fusion units is not correlated 		EYEDIAP (floating target)
with a better performance.  4.3 Static gaze regression: comparison		EYEDIAP (floating target)
 with existing methods		EYEDIAP (floating target)
		EYEDIAP (floating target)
We compare our best-performing static 		EYEDIAP (floating target)
model with three baselines. Head: 		EYEDIAP (floating target)
Treating the head pose directly 		EYEDIAP (floating target)
as gaze direction. PR-ALR: Method 		EYEDIAP (floating target)
that relies on RGB-D data 		EYEDIAP (floating target)
to rectify the eye images 		EYEDIAP (floating target)
viewpoint into a canonical head 		EYEDIAP (floating target)
pose using a 3DMM. It 		EYEDIAP (floating target)
then learns an RGB gaze 		EYEDIAP (floating target)
appearance model using ALR [21]. 		EYEDIAP (floating target)
Predicted 3D vectors for FT-S 		EYEDIAP (floating target)
scenario are provided by EYEDIAP 		EYEDIAP (floating target)
dataset. MPIIGaze:. State-of-the-art full-face 3D 		EYEDIAP (floating target)
gaze estimation method [42]. They 		EYEDIAP (floating target)
use an Alexnet-based CNN model 		EYEDIAP (floating target)
with spatial weights to enhance 		EYEDIAP (floating target)
information in different facial regions. 		EYEDIAP (floating target)
We fine-tuned it with the 		EYEDIAP (floating target)
filtered EYEDIAP subsets using our 		EYEDIAP (floating target)
training parameters and normalization procedure.  In addition to the aforementioned		EYEDIAP (floating target)
 FT-based evaluation setup, we also		EYEDIAP (floating target)
 evaluate our method on the		EYEDIAP (floating target)
 CS scenario. In this case		EYEDIAP (floating target)
 there are only 14 participants		EYEDIAP (floating target)
 available, so we divided them		EYEDIAP (floating target)
 in 5 groups and performed		EYEDIAP (floating target)
 5-fold cross-validation. In Figure 4		EYEDIAP (floating target)
 we compare our method to		EYEDIAP (floating target)
 MPIIGaze, achieving a statistically significant		EYEDIAP (floating target)
 improvement of 14.6% and 19.5		EYEDIAP (floating target)
% on FT and CS 		EYEDIAP (floating target)
scenarios, respectively (paired Wilcoxon test, 		EYEDIAP (floating target)
p < 0.0001). We can 		EYEDIAP (floating target)
observe that a re- stricted 		EYEDIAP (floating target)
gaze target benefits the performance 		EYEDIAP (floating target)
of all methods, compared to 		EYEDIAP (floating target)
a more challenging unrestricted setting 		EYEDIAP (floating target)
with a wider range of 		EYEDIAP (floating target)
head poses and gaze directions.  Table 2 provides a detailed		EYEDIAP (floating target)
 comparison on every participant, performing		EYEDIAP (floating target)
 leave-one-out cross-validation on the FT		EYEDIAP (floating target)
 scenario for static and moving		EYEDIAP (floating target)
 head separately. Results show that		EYEDIAP (floating target)
, as expected, facial appearance 		EYEDIAP (floating target)
and head pose have a 		EYEDIAP (floating target)
noticeable impact on gaze accuracy, 		EYEDIAP (floating target)
with average error differences of 		EYEDIAP (floating target)
up to 7.7◦ among participants.  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (floating target)
, and Bulling} 2015  Citation Citation {Mora and Odobez		EYEDIAP (floating target)
} 2012  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (floating target)
, and Bulling} 2015		EYEDIAP (floating target)
		EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 9		EYEDIAP (floating target)
		EYEDIAP (floating target)
Method 1 2 3 4 5 6 7 8 9 10		EYEDIAP (floating target)
 11 12 13 14 15		EYEDIAP (floating target)
 16 Avg. Head 23.5 22.1		EYEDIAP (floating target)
 20.3 23.6 23.2 23.2 23.6		EYEDIAP (floating target)
 21.2 26.7 23.6 23.1 24.4		EYEDIAP (floating target)
 23.3 24.0 24.5 22.8 23.3		EYEDIAP (floating target)
 PR-ALR 12.3 12.0 12.4 11.3		EYEDIAP (floating target)
 15.5 12.9 17.9 11.8 17.3		EYEDIAP (floating target)
 13.4 13.4 14.3 15.2 13.6		EYEDIAP (floating target)
 14.4 14.6 13.9 MPIIGaze 5.3		EYEDIAP (floating target)
 5.1 5.7 4.7 7.3 15.1		EYEDIAP (floating target)
 10.8 5.7 9.9 7.1 5.0		EYEDIAP (floating target)
 5.7 7.4 3.8 4.8 5.5		EYEDIAP (floating target)
 6.8 Static 3.9 4.1 4.2		EYEDIAP (floating target)
 3.9 6.0 6.4 7.2 3.6		EYEDIAP (floating target)
 7.1 5.0 5.7 6.7 3.9		EYEDIAP (floating target)
 4.7 5.1 4.2 5.1 Temporal		EYEDIAP (floating target)
 4.0 4.9 4.3 4.1 6.1		EYEDIAP (floating target)
 6.5 6.6 3.9 7.8 6.1		EYEDIAP (floating target)
 4.7 5.6 4.7 3.5 5.9		EYEDIAP (floating target)
 4.6 5.2 Head 19.3 14.2		EYEDIAP (floating target)
 16.4 19.9 16.8 21.9 16.1		EYEDIAP (floating target)
 24.2 20.3 19.9 18.8 22.3		EYEDIAP (floating target)
 18.1 14.9 16.2 19.3 18.7		EYEDIAP (floating target)
 MPIIGaze 7.6 6.2 5.7 8.7		EYEDIAP (floating target)
 10.1 12.0 12.2 6.1 8.3		EYEDIAP (floating target)
 5.9 6.1 6.2 7.4 4.7		EYEDIAP (floating target)
 4.4 6.0 7.3 Static 5.8		EYEDIAP (floating target)
 5.7 4.4 7.5 6.7 8.8		EYEDIAP (floating target)
 11.6 5.5 8.3 5.5 5.2		EYEDIAP (floating target)
 6.3 5.3 3.9 4.3 5.6		EYEDIAP (floating target)
 6.3 Temporal 6.1 5.6 4.5		EYEDIAP (floating target)
 7.5 6.4 8.2 12.0 5.0		EYEDIAP (floating target)
 7.5 5.4 5.0 5.8 6.6		EYEDIAP (floating target)
 4.0 4.5 5.8 6.2		EYEDIAP (floating target)
		EYEDIAP (floating target)
Table 2: Gaze angular error 		EYEDIAP (floating target)
comparison for static (top half) 		EYEDIAP (floating target)
and moving (bottom half) head 		EYEDIAP (floating target)
pose for each subject in 		EYEDIAP (floating target)
the FT scenario. Best results 		EYEDIAP (floating target)
in bold.  −80 −40 0 40 80−80		EYEDIAP (floating target)
		EYEDIAP (floating target)
40  0		EYEDIAP (floating target)
		EYEDIAP (floating target)
40  80		EYEDIAP (floating target)
		EYEDIAP (floating target)
0  5		EYEDIAP (floating target)
		EYEDIAP (floating target)
10  15		EYEDIAP (floating target)
		EYEDIAP (floating target)
20  25		EYEDIAP (floating target)
		EYEDIAP (floating target)
30  35		EYEDIAP (floating target)
		EYEDIAP (floating target)
80 −40 0 40 80−80  −40		EYEDIAP (floating target)
		EYEDIAP (floating target)
0  40		EYEDIAP (floating target)
		EYEDIAP (floating target)
80  −10		EYEDIAP (floating target)
		EYEDIAP (floating target)
8  −6		EYEDIAP (floating target)
		EYEDIAP (floating target)
4  −2		EYEDIAP (floating target)
		EYEDIAP (floating target)
0  2		EYEDIAP (floating target)
		EYEDIAP (floating target)
4  6		EYEDIAP (floating target)
		EYEDIAP (floating target)
8  10		EYEDIAP (floating target)
		EYEDIAP (floating target)
80 −40 0 40 80−80  −40		EYEDIAP (floating target)
		EYEDIAP (floating target)
0  40		EYEDIAP (floating target)
		EYEDIAP (floating target)
80  0		EYEDIAP (floating target)
		EYEDIAP (floating target)
5  10		EYEDIAP (floating target)
		EYEDIAP (floating target)
15  20		EYEDIAP (floating target)
		EYEDIAP (floating target)
25  30		EYEDIAP (floating target)
		EYEDIAP (floating target)
35  −80 −40 0 40 80−80		EYEDIAP (floating target)
		EYEDIAP (floating target)
40  0		EYEDIAP (floating target)
		EYEDIAP (floating target)
40  80		EYEDIAP (floating target)
		EYEDIAP (floating target)
10  −8		EYEDIAP (floating target)
		EYEDIAP (floating target)
6  −4		EYEDIAP (floating target)
		EYEDIAP (floating target)
2  0		EYEDIAP (floating target)
		EYEDIAP (floating target)
2  4		EYEDIAP (floating target)
		EYEDIAP (floating target)
6  8		EYEDIAP (floating target)
		EYEDIAP (floating target)
10  (a) Gaze space (b) Head		EYEDIAP (floating target)
 orientation space		EYEDIAP (floating target)
		EYEDIAP (floating target)
Figure 5: Angular error distribution 		EYEDIAP (floating target)
across gaze (a) and head 		EYEDIAP (floating target)
orientation (b) spaces in the 		EYEDIAP (floating target)
FT setting, in terms of 		EYEDIAP (floating target)
x- and y- angles. For 		EYEDIAP (floating target)
each space, we depict the 		EYEDIAP (floating target)
Static model performance (left) and 		EYEDIAP (floating target)
the contribution of the Temporal 		EYEDIAP (floating target)
model versus Static (right). In 		EYEDIAP (floating target)
the latter, positive difference means 		EYEDIAP (floating target)
higher improvement of the Temporal 		EYEDIAP (floating target)
model.  4.4 Evaluation of the temporal		EYEDIAP (floating target)
 network		EYEDIAP (floating target)
		EYEDIAP (floating target)
In this section, we evaluate 		EYEDIAP (floating target)
the contribution of adding the 		EYEDIAP (floating target)
temporal module to the static 		EYEDIAP (floating target)
model. To do so, we 		EYEDIAP (floating target)
trained a lower-dimensional version of 		EYEDIAP (floating target)
the static network with compa- 		EYEDIAP (floating target)
rable performance to the original, 		EYEDIAP (floating target)
reducing the number of units 		EYEDIAP (floating target)
of the second fusion layer 		EYEDIAP (floating target)
to 2918. Results are reported 		EYEDIAP (floating target)
in Figure 4 and Table 2		EYEDIAP (floating target)
. One can observe that 		EYEDIAP (floating target)
using sequential information is helpful 		EYEDIAP (floating target)
on the FT scenario, outperforming 		EYEDIAP (floating target)
the static model by a 		EYEDIAP (floating target)
statistically significant 4.4% (paired Wilcoxon 		EYEDIAP (floating target)
test, p < 0.0001). This 		EYEDIAP (floating target)
contribution is more noticeable in 		EYEDIAP (floating target)
the moving head setting, proving 		EYEDIAP (floating target)
that the temporal model can 		EYEDIAP (floating target)
benefit from head motion information. 		EYEDIAP (floating target)
In contrast, such information seems 		EYEDIAP (floating target)
to be less meaningful in 		EYEDIAP (floating target)
the CS scenario, where the 		EYEDIAP (floating target)
obtained error is already very 		EYEDIAP (floating target)
low for a cross-subject setting 		EYEDIAP (floating target)
and the amount of head 		EYEDIAP (floating target)
movement declines.  Figure 5 further explores the		EYEDIAP (floating target)
 error distribution of the static		EYEDIAP (floating target)
 network and the impact of		EYEDIAP (floating target)
 sequential information. We can observe		EYEDIAP (floating target)
 that the accuracy of the		EYEDIAP (floating target)
 static model drops with extreme		EYEDIAP (floating target)
 head poses and gaze directions		EYEDIAP (floating target)
, which can also be 		EYEDIAP (floating target)
correlated to having less data 		EYEDIAP (floating target)
in those areas. Compared to 		EYEDIAP (floating target)
the static model, the temporal 		EYEDIAP (floating target)
model particularly benefits gaze targets 		EYEDIAP (floating target)
from mid-range upwards. Its contribution 		EYEDIAP (floating target)
is less clear for extreme 		EYEDIAP (floating target)
targets, probably again due to 		EYEDIAP (floating target)
data imbalance.  Finally, we evaluated the effect		EYEDIAP (floating target)
 of different recurrent architectures for		EYEDIAP (floating target)
 the temporal model. In particular		EYEDIAP (floating target)
, we tested 1 (128 		EYEDIAP (floating target)
units) and 2 (256-128 units) 		EYEDIAP (floating target)
LSTM and GRU lay- ers, 		EYEDIAP (floating target)
with 1 GRU layer obtaining 		EYEDIAP (floating target)
slightly superior results (up to 0		EYEDIAP (floating target)
.12◦). We also assessed the 		EYEDIAP (floating target)
effect of sequence length fixing 		EYEDIAP (floating target)
s in the range {4,7,10}, 		EYEDIAP (floating target)
with s = 7 performing 		EYEDIAP (floating target)
worse than the other two (		EYEDIAP (floating target)
up to 0		EYEDIAP (floating target)
		EYEDIAP (floating target)
14		EYEDIAP (floating target)
		EYEDIAP (floating target)
10 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (floating target)
ESTIMATION  5 Conclusions In this work		EYEDIAP (floating target)
, we studied the combination 		EYEDIAP (floating target)
of full-face and eye images 		EYEDIAP (floating target)
along with facial land- marks 		EYEDIAP (floating target)
for person- and head pose-independent 		EYEDIAP (floating target)
3D gaze estimation. Consequently, we 		EYEDIAP (floating target)
pro- posed a multi-stream recurrent 		EYEDIAP (floating target)
CNN network that leverages the 		EYEDIAP (floating target)
sequential information of eye and 		EYEDIAP (floating target)
head movements. Both static and 		EYEDIAP (floating target)
temporal versions of our approach 		EYEDIAP (floating target)
significantly outperform current state-of-the-art 3D 		EYEDIAP (floating target)
gaze estimation methods on a 		EYEDIAP (floating target)
wide range of head poses 		EYEDIAP (floating target)
and gaze directions. We showed 		EYEDIAP (floating target)
that adding geometry features to 		EYEDIAP (floating target)
appearance-based methods has a regularizing 		EYEDIAP (floating target)
effect on the accuracy. Adding 		EYEDIAP (floating target)
sequential information further benefits the 		EYEDIAP (floating target)
final performance compared to static-only 		EYEDIAP (floating target)
input, especially from mid-range up- 		EYEDIAP (floating target)
wards and in those cases 		EYEDIAP (floating target)
where head motion is present. 		EYEDIAP (floating target)
The effect in very extreme 		EYEDIAP (floating target)
head poses is not clear 		EYEDIAP (floating target)
due to data imbalance, suggesting 		EYEDIAP (floating target)
the importance of learning from 		EYEDIAP (floating target)
a con- tinuous, balanced dataset 		EYEDIAP (floating target)
including all head poses and 		EYEDIAP (floating target)
gaze directions of interest. To 		EYEDIAP (floating target)
the best of our knowledge, 		EYEDIAP (floating target)
this is the first attempt 		EYEDIAP (floating target)
to exploit the temporal modality 		EYEDIAP (floating target)
in the context of gaze 		EYEDIAP (floating target)
estimation from remote cameras. As 		EYEDIAP (floating target)
future work, we will further 		EYEDIAP (floating target)
explore extracting meaningful temporal representations 		EYEDIAP (floating target)
of gaze dynamics, considering 3DCNNs 		EYEDIAP (floating target)
as well as the encoding 		EYEDIAP (floating target)
of deep features around particular 		EYEDIAP (floating target)
tracked face landmarks [14].  Acknowledgements This work has been		EYEDIAP (floating target)
 partially supported by the Spanish		EYEDIAP (floating target)
 project TIN2016-74946-P (MINECO/ FEDER, UE		EYEDIAP (floating target)
), CERCA Programme / Generalitat 		EYEDIAP (floating target)
de Catalunya, and the FP7 		EYEDIAP (floating target)
people program (Marie Curie Actions), 		EYEDIAP (floating target)
REA grant agreement no FP7-607139 (		EYEDIAP (floating target)
iCARE - Improving Children Auditory 		EYEDIAP (floating target)
REhabilitation). We gratefully acknowledge the 		EYEDIAP (floating target)
support of NVIDIA Corporation with 		EYEDIAP (floating target)
the donation of the GPU 		EYEDIAP (floating target)
used for this research. Portions 		EYEDIAP (floating target)
of the research in this 		EYEDIAP (floating target)
pa- per used the EYEDIAP 		EYEDIAP (floating target)
dataset made available by the 		EYEDIAP (floating target)
Idiap Research Institute, Martigny, Switzerland.  References [1] Nicola C Anderson		EYEDIAP (floating target)
, Evan F Risko, and 		EYEDIAP (floating target)
Alan Kingstone. Motion influences gaze 		EYEDIAP (floating target)
di-  rection discrimination and disambiguates contradictory		EYEDIAP (floating target)
 luminance cues. Psychonomic bulletin		EYEDIAP (floating target)
 & review, 23(3):817–823, 2016		EYEDIAP (floating target)
.  [2] Shumeet Baluja and Dean		EYEDIAP (floating target)
 Pomerleau. Non-intrusive gaze tracking using		EYEDIAP (floating target)
 artificial neu- ral networks. In		EYEDIAP (floating target)
 Advances in Neural Information Processing		EYEDIAP (floating target)
 Systems, pages 753–760, 1994		EYEDIAP (floating target)
.  [3] Adrian Bulat and Georgios		EYEDIAP (floating target)
 Tzimiropoulos. How far are we		EYEDIAP (floating target)
 from solving the 2d		EYEDIAP (floating target)
 & 3d face alignment problem		EYEDIAP (floating target)
? (and a dataset of 230,		EYEDIAP (floating target)
000 3d facial landmarks). In 		EYEDIAP (floating target)
Interna- tional Conference on Computer 		EYEDIAP (floating target)
Vision, 2017.  [4] Haoping Deng and Wangjiang		EYEDIAP (floating target)
 Zhu. Monocular free-head 3d gaze		EYEDIAP (floating target)
 tracking with deep learning and		EYEDIAP (floating target)
 geometry constraints. In Computer Vision		EYEDIAP (floating target)
 (ICCV), 2017 IEEE Interna- tional		EYEDIAP (floating target)
 Conference on, pages 3162–3171. IEEE		EYEDIAP (floating target)
, 2017.  [5] Onur Ferhat and Fernando		EYEDIAP (floating target)
 Vilariño. Low cost eye tracking		EYEDIAP (floating target)
. Computational intelligence and neuroscience, 2016		EYEDIAP (floating target)
:17, 2016.  Citation Citation {Jung, Lee, Yim		EYEDIAP (floating target)
, Park, and Kim} 2015		EYEDIAP (floating target)
		EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 11		EYEDIAP (floating target)
		EYEDIAP (floating target)
6] Kenneth A Funes-Mora and 		EYEDIAP (floating target)
Jean-Marc Odobez. Gaze estimation in 		EYEDIAP (floating target)
the 3D space using RGB-D 		EYEDIAP (floating target)
sensors. International Journal of Computer 		EYEDIAP (floating target)
Vision, 118(2):194–216, 2016.  [7] Kenneth Alberto Funes Mora		EYEDIAP (floating target)
, Florent Monay, and Jean-Marc 		EYEDIAP (floating target)
Odobez. Eyediap: A database for 		EYEDIAP (floating target)
the development and evaluation of 		EYEDIAP (floating target)
gaze estimation algorithms from rgb 		EYEDIAP (floating target)
and rgb-d cameras. In Proceedings 		EYEDIAP (floating target)
of the ACM Symposium on 		EYEDIAP (floating target)
Eye Tracking Research and Applications. 		EYEDIAP (floating target)
ACM, March 2014. doi: 10.1145/2578153.2578190.  [8] Kenneth Alberto Funes Mora		EYEDIAP (floating target)
, Florent Monay, and Jean-Marc 		EYEDIAP (floating target)
Odobez. Eyediap: A database for 		EYEDIAP (floating target)
the development and evaluation of 		EYEDIAP (floating target)
gaze estimation algorithms from rgb 		EYEDIAP (floating target)
and rgb-d cameras. In Proceedings 		EYEDIAP (floating target)
of the Symposium on Eye 		EYEDIAP (floating target)
Tracking Research and Applications, pages 255		EYEDIAP (floating target)
–258. ACM, 2014.  [9] Quentin Guillon, Nouchine Hadjikhani		EYEDIAP (floating target)
, Sophie Baduel, and Bernadette 		EYEDIAP (floating target)
Rogé. Visual social attention in 		EYEDIAP (floating target)
autism spectrum disorder: Insights from 		EYEDIAP (floating target)
eye tracking studies. Neu- 		EYEDIAP (floating target)
roscience & Biobehavioral Reviews, 42:279–297, 2014		EYEDIAP (floating target)
.  [10] Dan Witzner Hansen and		EYEDIAP (floating target)
 Qiang Ji. In the eye		EYEDIAP (floating target)
 of the beholder: A survey		EYEDIAP (floating target)
 of models for eyes and		EYEDIAP (floating target)
 gaze. IEEE transactions on pattern		EYEDIAP (floating target)
 analysis and machine intelligence, 32(3		EYEDIAP (floating target)
): 478–500, 2010.  [11] Qiong Huang, Ashok Veeraraghavan		EYEDIAP (floating target)
, and Ashutosh Sabharwal. Tabletgaze: 		EYEDIAP (floating target)
dataset and analysis for unconstrained 		EYEDIAP (floating target)
appearance-based gaze estimation in mobile 		EYEDIAP (floating target)
tablets. Machine Vision and Applications, 28		EYEDIAP (floating target)
(5-6):445–461, 2017.  [12] Robert JK Jacob and		EYEDIAP (floating target)
 Keith S Karn. Eye tracking		EYEDIAP (floating target)
 in human-computer interaction and usability		EYEDIAP (floating target)
 research: Ready to deliver the		EYEDIAP (floating target)
 promises. In The mind’s eye		EYEDIAP (floating target)
, pages 573–605. Elsevier, 2003.  [13] László A Jeni and		EYEDIAP (floating target)
 Jeffrey F Cohn. Person-independent 3d		EYEDIAP (floating target)
 gaze estimation using face frontalization		EYEDIAP (floating target)
. In Proceedings of the 		EYEDIAP (floating target)
IEEE Conference on Computer Vision 		EYEDIAP (floating target)
and Pattern Recognition Workshops, pages 87		EYEDIAP (floating target)
–95, 2016.  [14] Heechul Jung, Sihaeng Lee		EYEDIAP (floating target)
, Junho Yim, Sunjeong Park, 		EYEDIAP (floating target)
and Junmo Kim. Joint fine- 		EYEDIAP (floating target)
tuning in deep neural networks 		EYEDIAP (floating target)
for facial expression recognition. In 		EYEDIAP (floating target)
Computer Vision (ICCV), 2015 IEEE 		EYEDIAP (floating target)
International Conference on, pages 2983–2991. 		EYEDIAP (floating target)
IEEE, 2015.  [15] Anuradha Kar and Peter		EYEDIAP (floating target)
 Corcoran. A review and analysis		EYEDIAP (floating target)
 of eye-gaze estimation sys- tems		EYEDIAP (floating target)
, algorithms and performance evaluation 		EYEDIAP (floating target)
methods in consumer platforms. IEEE 		EYEDIAP (floating target)
Access, 5:16495–16519, 2017.  [16] Kyle Krafka, Aditya Khosla		EYEDIAP (floating target)
, Petr Kellnhofer, Harini Kannan, 		EYEDIAP (floating target)
Suchendra Bhandarkar, Wojciech Matusik, and 		EYEDIAP (floating target)
Antonio Torralba. Eye tracking for 		EYEDIAP (floating target)
everyone. In Computer Vision and 		EYEDIAP (floating target)
Pattern Recognition (CVPR), 2016 IEEE 		EYEDIAP (floating target)
Conference on, pages 2176–2184. IEEE, 2016		EYEDIAP (floating target)
.  [17] Simon P Liversedge and		EYEDIAP (floating target)
 John M Findlay. Saccadic eye		EYEDIAP (floating target)
 movements and cognition. Trends in		EYEDIAP (floating target)
 cognitive sciences, 4(1):6–14, 2000		EYEDIAP (floating target)
.  [18] Feng Lu, Takahiro Okabe		EYEDIAP (floating target)
, Yusuke Sugano, and Yoichi 		EYEDIAP (floating target)
Sato. A head pose-free approach 		EYEDIAP (floating target)
for appearance-based gaze estimation. In 		EYEDIAP (floating target)
BMVC, pages 1–11, 2011		EYEDIAP (floating target)
		EYEDIAP (floating target)
12 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (floating target)
ESTIMATION  [19] Feng Lu, Yusuke Sugano		EYEDIAP (floating target)
, Takahiro Okabe, and Yoichi 		EYEDIAP (floating target)
Sato. Inferring human gaze from 		EYEDIAP (floating target)
appearance via adaptive linear regression. 		EYEDIAP (floating target)
In Computer Vision (ICCV), 2011 		EYEDIAP (floating target)
IEEE International Conference on, pages 153		EYEDIAP (floating target)
–160. IEEE, 2011.  [20] Päivi Majaranta and Andreas		EYEDIAP (floating target)
 Bulling. Eye tracking and eye-based		EYEDIAP (floating target)
 human–computer interaction. In Advances in		EYEDIAP (floating target)
 physiological computing, pages 39–65. Springer		EYEDIAP (floating target)
, 2014.  [21] Kenneth Alberto Funes Mora		EYEDIAP (floating target)
 and Jean-Marc Odobez. Gaze estimation		EYEDIAP (floating target)
 from multi- modal kinect data		EYEDIAP (floating target)
. In Computer Vision and 		EYEDIAP (floating target)
Pattern Recognition Workshops (CVPRW), 2012 		EYEDIAP (floating target)
IEEE Computer Society Conference on, 		EYEDIAP (floating target)
pages 25–30. IEEE, 2012.  [22] Carlos Hitoshi Morimoto, Arnon		EYEDIAP (floating target)
 Amir, and Myron Flickner. Detecting		EYEDIAP (floating target)
 eye position and gaze from		EYEDIAP (floating target)
 a single camera and 2		EYEDIAP (floating target)
 light sources. In Pattern Recognition		EYEDIAP (floating target)
, 2002. Proceedings. 16th International 		EYEDIAP (floating target)
Conference on, volume 4, pages 314		EYEDIAP (floating target)
–317. IEEE, 2002.  [23] IMO MSC. Circ. 982		EYEDIAP (floating target)
 (2000) guidelines on ergonomic criteria		EYEDIAP (floating target)
 for bridge equipment and layout		EYEDIAP (floating target)
.  [24] Alejandro Newell, Kaiyu Yang		EYEDIAP (floating target)
, and Jia Deng. Stacked 		EYEDIAP (floating target)
hourglass networks for hu- man 		EYEDIAP (floating target)
pose estimation. In European Conference 		EYEDIAP (floating target)
on Computer Vision, pages 483–499. 		EYEDIAP (floating target)
Springer, 2016.  [25] Yasuhiro Ono, Takahiro Okabe		EYEDIAP (floating target)
, and Yoichi Sato. Gaze 		EYEDIAP (floating target)
estimation from low resolution images. 		EYEDIAP (floating target)
In Pacific-Rim Symposium on Image 		EYEDIAP (floating target)
and Video Technology, pages 178–188. 		EYEDIAP (floating target)
Springer, 2006.  [26] Cristina Palmero, Elisabeth A		EYEDIAP (floating target)
. van Dam, Sergio Escalera, 		EYEDIAP (floating target)
Mike Kelia, Guido F. Lichtert, 		EYEDIAP (floating target)
Lucas P.J.J Noldus, Andrew J. 		EYEDIAP (floating target)
Spink, and Astrid van Wieringen. 		EYEDIAP (floating target)
Automatic mutual gaze detection in 		EYEDIAP (floating target)
face-to-face dyadic interaction videos. In 		EYEDIAP (floating target)
Proceedings of Measuring Behavior, pages 158		EYEDIAP (floating target)
–163, 2018.  [27] Omkar M. Parkhi, Andrea		EYEDIAP (floating target)
 Vedaldi, and Andrew Zisserman. Deep		EYEDIAP (floating target)
 face recognition. In British Machine		EYEDIAP (floating target)
 Vision Conference, 2015		EYEDIAP (floating target)
.  [28] Derek R Rutter and		EYEDIAP (floating target)
 Kevin Durkin. Turn-taking in mother–infant		EYEDIAP (floating target)
 interaction: An exam- ination of		EYEDIAP (floating target)
 vocalizations and gaze. Developmental psychology		EYEDIAP (floating target)
, 23(1):54, 1987.  [29] Brian A Smith, Qi		EYEDIAP (floating target)
 Yin, Steven K Feiner, and		EYEDIAP (floating target)
 Shree K Nayar. Gaze locking		EYEDIAP (floating target)
: passive eye contact detection 		EYEDIAP (floating target)
for human-object interaction. In Proceedings 		EYEDIAP (floating target)
of the 26th annual ACM 		EYEDIAP (floating target)
symposium on User interface software 		EYEDIAP (floating target)
and technology, pages 271–280. ACM, 2013		EYEDIAP (floating target)
.  [30] Yusuke Sugano, Yasuyuki Matsushita		EYEDIAP (floating target)
, and Yoichi Sato. Appearance-based 		EYEDIAP (floating target)
gaze es- timation using visual 		EYEDIAP (floating target)
saliency. IEEE transactions on pattern 		EYEDIAP (floating target)
analysis and machine intelligence, 35(2):329–341, 2013		EYEDIAP (floating target)
.  [31] Yusuke Sugano, Yasuyuki Matsushita		EYEDIAP (floating target)
, and Yoichi Sato. Learning-by-synthesis 		EYEDIAP (floating target)
for appearance-based 3d gaze estimation. 		EYEDIAP (floating target)
In Computer Vision and Pattern 		EYEDIAP (floating target)
Recognition (CVPR), 2014 IEEE Conference 		EYEDIAP (floating target)
on, pages 1821–1828. IEEE, 2014.  [32] Kar-Han Tan, David J		EYEDIAP (floating target)
 Kriegman, and Narendra Ahuja. Appearance-based		EYEDIAP (floating target)
 eye gaze es- timation. In		EYEDIAP (floating target)
 Applications of Computer Vision, 2002.(WACV		EYEDIAP (floating target)
 2002). Proceedings. Sixth IEEE Workshop		EYEDIAP (floating target)
 on, pages 191–195. IEEE, 2002		EYEDIAP (floating target)
		EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 13		EYEDIAP (floating target)
		EYEDIAP (floating target)
33] Ronda Venkateswarlu et al. 		EYEDIAP (floating target)
Eye gaze estimation from a 		EYEDIAP (floating target)
single image of one eye. 		EYEDIAP (floating target)
In Computer Vision, 2003. Proceedings. 		EYEDIAP (floating target)
Ninth IEEE International Conference on, 		EYEDIAP (floating target)
pages 136–143. IEEE, 2003.  [34] Kang Wang and Qiang		EYEDIAP (floating target)
 Ji. Real time eye gaze		EYEDIAP (floating target)
 tracking with 3d deformable eye-face		EYEDIAP (floating target)
 model. In Proceedings of the		EYEDIAP (floating target)
 IEEE Conference on Computer Vision		EYEDIAP (floating target)
 and Pattern Recog- nition, pages		EYEDIAP (floating target)
 1003–1011, 2017		EYEDIAP (floating target)
.  [35] Oliver Williams, Andrew Blake		EYEDIAP (floating target)
, and Roberto Cipolla. Sparse 		EYEDIAP (floating target)
and semi-supervised visual mapping with 		EYEDIAP (floating target)
the sˆ 3gp. In Computer 		EYEDIAP (floating target)
Vision and Pattern Recognition, 2006 		EYEDIAP (floating target)
IEEE Computer Society Conference on, 		EYEDIAP (floating target)
volume 1, pages 230–237. IEEE, 2006		EYEDIAP (floating target)
.  [36] William Hyde Wollaston et		EYEDIAP (floating target)
 al. Xiii. on the apparent		EYEDIAP (floating target)
 direction of eyes in a		EYEDIAP (floating target)
 portrait. Philosophical Transactions of the		EYEDIAP (floating target)
 Royal Society of London, 114:247–256		EYEDIAP (floating target)
, 1824.  [37] Erroll Wood and Andreas		EYEDIAP (floating target)
 Bulling. Eyetab: Model-based gaze estimation		EYEDIAP (floating target)
 on unmodi- fied tablet computers		EYEDIAP (floating target)
. In Proceedings of the 		EYEDIAP (floating target)
Symposium on Eye Tracking Research 		EYEDIAP (floating target)
and Applications, pages 207–210. ACM, 2014		EYEDIAP (floating target)
.  [38] Erroll Wood, Tadas Baltrusaitis		EYEDIAP (floating target)
, Xucong Zhang, Yusuke Sugano, 		EYEDIAP (floating target)
Peter Robinson, and Andreas Bulling. 		EYEDIAP (floating target)
Rendering of eyes for eye-shape 		EYEDIAP (floating target)
registration and gaze estimation. In 		EYEDIAP (floating target)
Proceedings of the IEEE International 		EYEDIAP (floating target)
Conference on Computer Vision, pages 3756		EYEDIAP (floating target)
– 3764, 2015.  [39] Erroll Wood, Tadas Baltrušaitis		EYEDIAP (floating target)
, Louis-Philippe Morency, Peter Robinson, 		EYEDIAP (floating target)
and Andreas Bulling. A 3d 		EYEDIAP (floating target)
morphable eye region model for 		EYEDIAP (floating target)
gaze estimation. In European Confer- 		EYEDIAP (floating target)
ence on Computer Vision, pages 297		EYEDIAP (floating target)
–313. Springer, 2016.  [40] Erroll Wood, Tadas Baltrušaitis		EYEDIAP (floating target)
, Louis-Philippe Morency, Peter Robinson, 		EYEDIAP (floating target)
and Andreas Bulling. Learning an 		EYEDIAP (floating target)
appearance-based gaze estimator from one 		EYEDIAP (floating target)
million synthesised images. In Proceedings 		EYEDIAP (floating target)
of the Ninth Biennial ACM 		EYEDIAP (floating target)
Symposium on Eye Tracking Re- 		EYEDIAP (floating target)
search & Applications, pages 131–138. 		EYEDIAP (floating target)
ACM, 2016.  [41] Dong Hyun Yoo and		EYEDIAP (floating target)
 Myung Jin Chung. A novel		EYEDIAP (floating target)
 non-intrusive eye gaze estimation using		EYEDIAP (floating target)
 cross-ratio under large head motion		EYEDIAP (floating target)
. Computer Vision and Image 		EYEDIAP (floating target)
Understanding, 98(1):25–51, 2005.  [42] Xucong Zhang, Yusuke Sugano		EYEDIAP (floating target)
, Mario Fritz, and Andreas 		EYEDIAP (floating target)
Bulling. Appearance-based gaze estimation in 		EYEDIAP (floating target)
the wild. In Proceedings of 		EYEDIAP (floating target)
the IEEE Conference on Computer 		EYEDIAP (floating target)
Vision and Pattern Recognition, pages 4511		EYEDIAP (floating target)
–4520, 2015.  [43] Xucong Zhang, Yusuke Sugano		EYEDIAP (floating target)
, Mario Fritz, and Andreas 		EYEDIAP (floating target)
Bulling. It’s written all over 		EYEDIAP (floating target)
your face: Full-face appearance-based gaze 		EYEDIAP (floating target)
estimation. In Proc. IEEE International 		EYEDIAP (floating target)
Conference on Computer Vision and 		EYEDIAP (floating target)
Pattern Recognition Workshops (CVPRW), 2017		EYEDIAP (floating target)
		EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 1	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Recurrent CNN for 3D Gaze 	target)	EYEDIAP (floating target)
Estimation using Appearance and Shape 	target)	EYEDIAP (floating target)
Cues  Cristina Palmero1,2	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
crpalmec7@alumnes.ub.edu  1 Dept. Mathematics and Informatics	target)	EYEDIAP (floating target)
 Universitat de Barcelona, Spain	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Javier Selva1  javier.selva.castello@est.fib.upc.edu	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
2 Computer Vision Center Campus 	target)	EYEDIAP (floating target)
UAB, Bellaterra, Spain  Mohammad Ali Bagheri3,4	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
mohammadali.bagheri@ucalgary.ca  3 Dept. Electrical and Computer	target)	EYEDIAP (floating target)
 Eng. University of Calgary, Canada	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Sergio Escalera1,2  sergio@maia.ub.es	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
4 Dept. Engineering University of 	target)	EYEDIAP (floating target)
Larestan, Iran  Abstract	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Gaze behavior is an important 	target)	EYEDIAP (floating target)
non-verbal cue in social signal 	target)	EYEDIAP (floating target)
processing and human- computer interaction. 	target)	EYEDIAP (floating target)
In this paper, we tackle 	target)	EYEDIAP (floating target)
the problem of person- and 	target)	EYEDIAP (floating target)
head pose- independent 3D gaze 	target)	EYEDIAP (floating target)
estimation from remote cameras, using 	target)	EYEDIAP (floating target)
a multi-modal recurrent convolutional neural 	target)	EYEDIAP (floating target)
network (CNN). We propose to 	target)	EYEDIAP (floating target)
combine face, eyes region, and 	target)	EYEDIAP (floating target)
face landmarks as individual streams 	target)	EYEDIAP (floating target)
in a CNN to estimate 	target)	EYEDIAP (floating target)
gaze in still images. Then, 	target)	EYEDIAP (floating target)
we exploit the dynamic nature 	target)	EYEDIAP (floating target)
of gaze by feeding the 	target)	EYEDIAP (floating target)
learned features of all the 	target)	EYEDIAP (floating target)
frames in a sequence to 	target)	EYEDIAP (floating target)
a many-to-one recurrent module that 	target)	EYEDIAP (floating target)
predicts the 3D gaze vector 	target)	EYEDIAP (floating target)
of the last frame. Our 	target)	EYEDIAP (floating target)
multi-modal static solution is evaluated 	target)	EYEDIAP (floating target)
on a wide range of 	target)	EYEDIAP (floating target)
head poses and gaze directions, 	target)	EYEDIAP (floating target)
achieving a significant improvement of 14	target)	EYEDIAP (floating target)
.6% over the state of 	target)	EYEDIAP (floating target)
the art on EYEDIAP dataset, 	target)	EYEDIAP (floating target)
further improved by 4% when 	target)	EYEDIAP (floating target)
the temporal modality is included.  1 Introduction Eyes and their	target)	EYEDIAP (floating target)
 movements are considered an important	target)	EYEDIAP (floating target)
 cue in non-verbal behavior analysis	target)	EYEDIAP (floating target)
, being involved in many 	target)	EYEDIAP (floating target)
cognitive processes and reflecting our 	target)	EYEDIAP (floating target)
internal state [17]. More specifically, 	target)	EYEDIAP (floating target)
eye gaze behavior, as an 	target)	EYEDIAP (floating target)
indicator of human visual attention, 	target)	EYEDIAP (floating target)
has been widely studied to 	target)	EYEDIAP (floating target)
assess communication skills [28] and 	target)	EYEDIAP (floating target)
to identify possible behavioral 	target)	EYEDIAP (floating target)
disorders [9]. Therefore, gaze estimation 	target)	EYEDIAP (floating target)
has become an established line 	target)	EYEDIAP (floating target)
of research in computer vision, 	target)	EYEDIAP (floating target)
being a key feature in 	target)	EYEDIAP (floating target)
human-computer interaction (HCI) and usability 	target)	EYEDIAP (floating target)
research [12, 20].  Recent gaze estimation research has	target)	EYEDIAP (floating target)
 focused on facilitating its use	target)	EYEDIAP (floating target)
 in general everyday applications under	target)	EYEDIAP (floating target)
 real-world conditions, using off-the-shelf remote	target)	EYEDIAP (floating target)
 RGB cameras and re- moving	target)	EYEDIAP (floating target)
 the need of personal calibration	target)	EYEDIAP (floating target)
 [26]. In this setting, appearance-based	target)	EYEDIAP (floating target)
 methods, which learn a mapping	target)	EYEDIAP (floating target)
 from images to gaze directions	target)	EYEDIAP (floating target)
, are the preferred 	target)	EYEDIAP (floating target)
choice [25]. How- ever, they 	target)	EYEDIAP (floating target)
need large amounts of training 	target)	EYEDIAP (floating target)
data to be able to 	target)	EYEDIAP (floating target)
generalize well to in-the-wild situations, 	target)	EYEDIAP (floating target)
which are characterized by significant 	target)	EYEDIAP (floating target)
variability in head poses, face 	target)	EYEDIAP (floating target)
appearances and lighting conditions. In 	target)	EYEDIAP (floating target)
recent years, CNNs have been 	target)	EYEDIAP (floating target)
reported to outperform classical methods. 	target)	EYEDIAP (floating target)
However, most existing approaches have 	target)	EYEDIAP (floating target)
only been tested in restricted 	target)	EYEDIAP (floating target)
HCI tasks,  c© 2018. The copyright of	target)	EYEDIAP (floating target)
 this document resides with its	target)	EYEDIAP (floating target)
 authors. It may be distributed	target)	EYEDIAP (floating target)
 unchanged freely in print or	target)	EYEDIAP (floating target)
 electronic forms	target)	EYEDIAP (floating target)
.  ar X	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
iv :1  80 5	target)	EYEDIAP (floating target)
.  03 06	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
4v 3	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
cs  .C V	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
1  7	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Se  p	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
20  18	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Liversedge and Findlay} 2000	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Rutter and Durkin} 1987	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Guillon, Hadjikhani, Baduel, 	target)	EYEDIAP (floating target)
and Rog{é}} 2014  Citation Citation {Jacob and Karn	target)	EYEDIAP (floating target)
} 2003  Citation Citation {Majaranta and Bulling	target)	EYEDIAP (floating target)
} 2014  Citation Citation {Palmero, van Dam	target)	EYEDIAP (floating target)
, Escalera, Kelia, Lichtert, Noldus, 	target)	EYEDIAP (floating target)
Spink, and van Wieringen} 2018  Citation Citation {Ono, Okabe, and	target)	EYEDIAP (floating target)
 Sato} 2006	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
2 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (floating target)
ESTIMATION  Method 3D gaze direction	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Unrestricted gaze target  Full face	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Eye region  Facial landmarks	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Sequential information  Zhang et al. (1) [42	target)	EYEDIAP (floating target)
] 3 7 7 3 7 7 Krafka et al. [16	target)	EYEDIAP (floating target)
] 7 7 3 3 7 7 Zhang et al. (2	target)	EYEDIAP (floating target)
) [43] 3 7 3 7 7 7 Deng and Zhu	target)	EYEDIAP (floating target)
 [4] 3 3 3 3	target)	EYEDIAP (floating target)
 7 7 Ours 3 3	target)	EYEDIAP (floating target)
 3 3 3 3	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Table 1: Characteristics of recent 	target)	EYEDIAP (floating target)
related work on person- and 	target)	EYEDIAP (floating target)
head pose-independent appearance-based gaze estimation 	target)	EYEDIAP (floating target)
methods using CNNs.  where users look at the	target)	EYEDIAP (floating target)
 screen or mobile phone, showing	target)	EYEDIAP (floating target)
 a low head pose variability	target)	EYEDIAP (floating target)
. It is yet unclear 	target)	EYEDIAP (floating target)
how these methods would perform 	target)	EYEDIAP (floating target)
in a wider range of 	target)	EYEDIAP (floating target)
head poses.  On a different note, until	target)	EYEDIAP (floating target)
 very recently, the majority of	target)	EYEDIAP (floating target)
 methods only used static eye	target)	EYEDIAP (floating target)
 region appearance as input. State-of-the-art	target)	EYEDIAP (floating target)
 approaches have demonstrated that using	target)	EYEDIAP (floating target)
 the face along with a	target)	EYEDIAP (floating target)
 higher resolution image of the	target)	EYEDIAP (floating target)
 eyes [16], or even just	target)	EYEDIAP (floating target)
 the face itself [43], increases	target)	EYEDIAP (floating target)
 performance. Indeed, the whole-face image	target)	EYEDIAP (floating target)
 encodes more information than eyes	target)	EYEDIAP (floating target)
 alone, such as illumination and	target)	EYEDIAP (floating target)
 head pose. Nevertheless, gaze behavior	target)	EYEDIAP (floating target)
 is not static. Eye and	target)	EYEDIAP (floating target)
 head movements allow us to	target)	EYEDIAP (floating target)
 direct our gaze to target	target)	EYEDIAP (floating target)
 locations of interest. It has	target)	EYEDIAP (floating target)
 been demonstrated that humans can	target)	EYEDIAP (floating target)
 better predict gaze when being	target)	EYEDIAP (floating target)
 shown image sequences of other	target)	EYEDIAP (floating target)
 people moving their eyes [1	target)	EYEDIAP (floating target)
]. However, it is still 	target)	EYEDIAP (floating target)
an open question whether this 	target)	EYEDIAP (floating target)
se- quential information can increase 	target)	EYEDIAP (floating target)
the performance of automatic methods.  In this work, we show	target)	EYEDIAP (floating target)
 that the combination of multiple	target)	EYEDIAP (floating target)
 cues benefits the gaze estimation	target)	EYEDIAP (floating target)
 task. In particular, we use	target)	EYEDIAP (floating target)
 face, eye region and facial	target)	EYEDIAP (floating target)
 landmarks from still images. Facial	target)	EYEDIAP (floating target)
 landmarks model the global shape	target)	EYEDIAP (floating target)
 of the face and come	target)	EYEDIAP (floating target)
 at no cost, since face	target)	EYEDIAP (floating target)
 alignment is a common pre-processing	target)	EYEDIAP (floating target)
 step in many facial image	target)	EYEDIAP (floating target)
 analysis approaches. Furthermore, we present	target)	EYEDIAP (floating target)
 a subject-independent, free-head recurrent 3D	target)	EYEDIAP (floating target)
 gaze regression network to leverage	target)	EYEDIAP (floating target)
 the temporal information of image	target)	EYEDIAP (floating target)
 sequences. The static streams of	target)	EYEDIAP (floating target)
 each frame are combined in	target)	EYEDIAP (floating target)
 a late-fusion fashion using a	target)	EYEDIAP (floating target)
 multi-stream CNN. Then, all feature	target)	EYEDIAP (floating target)
 vectors are input to a	target)	EYEDIAP (floating target)
 many-to-one recurrent module that predicts	target)	EYEDIAP (floating target)
 the gaze vector of the	target)	EYEDIAP (floating target)
 last sequence frame	target)	EYEDIAP (floating target)
.  In summary, our contributions are	target)	EYEDIAP (floating target)
 two-fold. First, we present a	target)	EYEDIAP (floating target)
 Recurrent-CNN net- work architecture that	target)	EYEDIAP (floating target)
 combines appearance, shape and temporal	target)	EYEDIAP (floating target)
 information for 3D gaze estimation	target)	EYEDIAP (floating target)
. Second, we test static 	target)	EYEDIAP (floating target)
and temporal versions of our 	target)	EYEDIAP (floating target)
solution on the EYEDIAP 	target)	EYEDIAP (floating target)
dataset [7] in a wide 	target)	EYEDIAP (floating target)
range of head poses and 	target)	EYEDIAP (floating target)
gaze directions, showing consistent perfor- 	target)	EYEDIAP (floating target)
mance improvements compared to related 	target)	EYEDIAP (floating target)
appearance-based methods. To the best 	target)	EYEDIAP (floating target)
of our knowledge, this is 	target)	EYEDIAP (floating target)
the first third-person, remote camera-based 	target)	EYEDIAP (floating target)
approach that uses tempo- ral 	target)	EYEDIAP (floating target)
information for this task. Table 1 outlines our main method characteristics	target)	EYEDIAP (floating target)
 compared to related work. Models	target)	EYEDIAP (floating target)
 and code are publicly available	target)	EYEDIAP (floating target)
 at https://github.com/ crisie/RecurrentGaze	target)	EYEDIAP (floating target)
.  2 Related work Gaze estimation	target)	EYEDIAP (floating target)
 methods are typically categorized as	target)	EYEDIAP (floating target)
 model-based or appearance-based [5, 10	target)	EYEDIAP (floating target)
, 15]. Model-based approaches use 	target)	EYEDIAP (floating target)
a geometric model of the 	target)	EYEDIAP (floating target)
eye, usually requir- ing either 	target)	EYEDIAP (floating target)
high resolution images or a 	target)	EYEDIAP (floating target)
person-specific calibration stage to estimate 	target)	EYEDIAP (floating target)
personal eye parameters [22, 33, 34, 37, 41]. In contrast, appearance-based	target)	EYEDIAP (floating target)
 methods learn a di- rect	target)	EYEDIAP (floating target)
 mapping from intensity images or	target)	EYEDIAP (floating target)
 extracted eye features to gaze	target)	EYEDIAP (floating target)
 directions, thus being	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Zhang, Sugano, Fritz, 	target)	EYEDIAP (floating target)
and Bulling} 2015  Citation Citation {Krafka, Khosla, Kellnhofer	target)	EYEDIAP (floating target)
, Kannan, Bhandarkar, Matusik, and 	target)	EYEDIAP (floating target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (floating target)
, and Bulling} 2017  Citation Citation {Deng and Zhu	target)	EYEDIAP (floating target)
} 2017  Citation Citation {Krafka, Khosla, Kellnhofer	target)	EYEDIAP (floating target)
, Kannan, Bhandarkar, Matusik, and 	target)	EYEDIAP (floating target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (floating target)
, and Bulling} 2017  Citation Citation {Anderson, Risko, and	target)	EYEDIAP (floating target)
 Kingstone} 2016	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Funesprotect unhbox voidb@x 	target)	EYEDIAP (floating target)
penalty @M  {}Mora, Monay, and Odobez} 2014	target)	EYEDIAP (floating target)
{}  Citation Citation {Ferhat and Vilari{ñ}o	target)	EYEDIAP (floating target)
} 2016  Citation Citation {Hansen and Ji	target)	EYEDIAP (floating target)
} 2010  Citation Citation {Kar and Corcoran	target)	EYEDIAP (floating target)
} 2017  Citation Citation {Morimoto, Amir, and	target)	EYEDIAP (floating target)
 Flickner} 2002	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Venkateswarlu etprotect unhbox 	target)	EYEDIAP (floating target)
voidb@x penalty @M  {}al.} 2003	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Wang and Ji} 2017	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Wood and Bulling} 2014	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Yoo and Chung} 2005	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
https://github.com/crisie/RecurrentGaze 	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 3	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
potentially applicable to relatively low 	target)	EYEDIAP (floating target)
resolution images and mid-distance scenarios. 	target)	EYEDIAP (floating target)
Dif- ferent mapping functions have 	target)	EYEDIAP (floating target)
been explored, such as neural 	target)	EYEDIAP (floating target)
networks [2], adaptive linear regression (	target)	EYEDIAP (floating target)
ALR) [19], local interpolation [32], 	target)	EYEDIAP (floating target)
gaussian processes [30, 35], random 	target)	EYEDIAP (floating target)
forests [11, 31], or k-nearest 	target)	EYEDIAP (floating target)
neighbors [40]. Main challenges of 	target)	EYEDIAP (floating target)
appearance-based methods for 3D gaze 	target)	EYEDIAP (floating target)
estimation are head pose, illumination 	target)	EYEDIAP (floating target)
and subject invariance without user-specific 	target)	EYEDIAP (floating target)
calibration. To handle these issues, 	target)	EYEDIAP (floating target)
some works proposed compensation 	target)	EYEDIAP (floating target)
methods [18] and warping strategies 	target)	EYEDIAP (floating target)
that synthesize a canonical, frontal 	target)	EYEDIAP (floating target)
looking view of the 	target)	EYEDIAP (floating target)
face [6, 13, 21]. Hybrid 	target)	EYEDIAP (floating target)
approaches based on analysis-by-synthesis have 	target)	EYEDIAP (floating target)
also been evaluated [39].  Currently, data-driven methods are considered	target)	EYEDIAP (floating target)
 the state of the art	target)	EYEDIAP (floating target)
 for person- and head pose-independent	target)	EYEDIAP (floating target)
 appearance-based gaze estimation. Consequently, a	target)	EYEDIAP (floating target)
 number of gaze es- timation	target)	EYEDIAP (floating target)
 datasets have been introduced in	target)	EYEDIAP (floating target)
 recent years, either in controlled	target)	EYEDIAP (floating target)
 [29] or semi- controlled settings	target)	EYEDIAP (floating target)
 [8], in the wild [16	target)	EYEDIAP (floating target)
, 42], or consisting of 	target)	EYEDIAP (floating target)
synthetic data [31, 38, 40]. 	target)	EYEDIAP (floating target)
Zhang et al. [42] showed 	target)	EYEDIAP (floating target)
that CNNs can outperform other 	target)	EYEDIAP (floating target)
mapping methods, using a multi- 	target)	EYEDIAP (floating target)
modal CNN to learn the 	target)	EYEDIAP (floating target)
mapping from 3D head poses 	target)	EYEDIAP (floating target)
and eye images to 3D 	target)	EYEDIAP (floating target)
gaze directions. Krafka et 	target)	EYEDIAP (floating target)
al. [16] proposed a multi-stream 	target)	EYEDIAP (floating target)
CNN for 2D gaze estimation, 	target)	EYEDIAP (floating target)
using individual eye, whole-face image 	target)	EYEDIAP (floating target)
and the face grid as 	target)	EYEDIAP (floating target)
input. As this method was 	target)	EYEDIAP (floating target)
limited to 2D screen mapping, 	target)	EYEDIAP (floating target)
Zhang et al. [43] later 	target)	EYEDIAP (floating target)
explored the potential of just 	target)	EYEDIAP (floating target)
using whole-face images as input 	target)	EYEDIAP (floating target)
to estimate 3D gaze directions. 	target)	EYEDIAP (floating target)
Using a spatial weights CNN, 	target)	EYEDIAP (floating target)
they demonstrated their method to 	target)	EYEDIAP (floating target)
be more robust to facial 	target)	EYEDIAP (floating target)
appearance variation caused by head 	target)	EYEDIAP (floating target)
pose and illumina- tion than 	target)	EYEDIAP (floating target)
eye-only methods. While the method 	target)	EYEDIAP (floating target)
was evaluated in the wild, 	target)	EYEDIAP (floating target)
the subjects were only interacting 	target)	EYEDIAP (floating target)
with a mobile device, thus 	target)	EYEDIAP (floating target)
restricting the head pose range. 	target)	EYEDIAP (floating target)
Deng and Zhu [4] presented 	target)	EYEDIAP (floating target)
a two-stream CNN to disjointly 	target)	EYEDIAP (floating target)
model head pose from face 	target)	EYEDIAP (floating target)
images and eye- ball movement 	target)	EYEDIAP (floating target)
from eye region images. Both 	target)	EYEDIAP (floating target)
were then aggregated into 3D 	target)	EYEDIAP (floating target)
gaze direction using a gaze 	target)	EYEDIAP (floating target)
transform layer. The decomposition was 	target)	EYEDIAP (floating target)
aimed to avoid head-correlation over- 	target)	EYEDIAP (floating target)
fitting of previous data-driven approaches. 	target)	EYEDIAP (floating target)
They evaluated their approach in 	target)	EYEDIAP (floating target)
the wild with a wider 	target)	EYEDIAP (floating target)
range of head poses, obtaining 	target)	EYEDIAP (floating target)
better performance than previous eye-based 	target)	EYEDIAP (floating target)
methods. However, they did not 	target)	EYEDIAP (floating target)
test it on public annotated 	target)	EYEDIAP (floating target)
benchmark datasets.  In this paper, we propose	target)	EYEDIAP (floating target)
 a multi-stream recurrent CNN network	target)	EYEDIAP (floating target)
 for person- and head pose-independent	target)	EYEDIAP (floating target)
 3D gaze estimation for a	target)	EYEDIAP (floating target)
 mid-distance scenario. We evaluate it	target)	EYEDIAP (floating target)
 on a wider range of	target)	EYEDIAP (floating target)
 head poses and gaze directions	target)	EYEDIAP (floating target)
 than screen-targeted approaches. As opposed	target)	EYEDIAP (floating target)
 to previous methods, we also	target)	EYEDIAP (floating target)
 rely on temporal information inherent	target)	EYEDIAP (floating target)
 in sequential data	target)	EYEDIAP (floating target)
.  3 Methodology	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
In this section, we present 	target)	EYEDIAP (floating target)
our approach for 3D gaze 	target)	EYEDIAP (floating target)
regression based on appearance and 	target)	EYEDIAP (floating target)
shape cues for still images 	target)	EYEDIAP (floating target)
and image sequences. First, we 	target)	EYEDIAP (floating target)
introduce the data modalities and 	target)	EYEDIAP (floating target)
formulate the problem. Then, we 	target)	EYEDIAP (floating target)
detail the normalization procedure prior 	target)	EYEDIAP (floating target)
to the regression stage. Finally, 	target)	EYEDIAP (floating target)
we explain the global network 	target)	EYEDIAP (floating target)
topology as well as the 	target)	EYEDIAP (floating target)
implementation details. An overview of 	target)	EYEDIAP (floating target)
the system architecture is depicted 	target)	EYEDIAP (floating target)
in Figure 1.  3.1 Multi-modal gaze regression	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Let us represent gaze direction 	target)	EYEDIAP (floating target)
as a 3D unit vector 	target)	EYEDIAP (floating target)
g = [gx,gy,gz]T ∈R3 in 	target)	EYEDIAP (floating target)
the Camera Coor- dinate System (	target)	EYEDIAP (floating target)
CCS), whose origin is the 	target)	EYEDIAP (floating target)
central point between eyeball centers. 	target)	EYEDIAP (floating target)
Assuming a calibrated camera, and 	target)	EYEDIAP (floating target)
a known head position and 	target)	EYEDIAP (floating target)
orientation, our goal is to 	target)	EYEDIAP (floating target)
estimate g from a sequence 	target)	EYEDIAP (floating target)
of images {I(i) | 	target)	EYEDIAP (floating target)
I ∈ RW×H×3} as a 	target)	EYEDIAP (floating target)
regression problem.  Citation Citation {Baluja and Pomerleau	target)	EYEDIAP (floating target)
} 1994  Citation Citation {Lu, Sugano, Okabe	target)	EYEDIAP (floating target)
, and Sato} 2011{}  Citation Citation {Tan, Kriegman, and	target)	EYEDIAP (floating target)
 Ahuja} 2002	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Sugano, Matsushita, and 	target)	EYEDIAP (floating target)
Sato} 2013  Citation Citation {Williams, Blake, and	target)	EYEDIAP (floating target)
 Cipolla} 2006	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Huang, Veeraraghavan, and 	target)	EYEDIAP (floating target)
Sabharwal} 2017  Citation Citation {Sugano, Matsushita, and	target)	EYEDIAP (floating target)
 Sato} 2014	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Wood, Baltru{²}aitis, Morency, 	target)	EYEDIAP (floating target)
Robinson, and Bulling} 2016{}  Citation Citation {Lu, Okabe, Sugano	target)	EYEDIAP (floating target)
, and Sato} 2011{}  Citation Citation {Funes-Mora and Odobez	target)	EYEDIAP (floating target)
} 2016  Citation Citation {Jeni and Cohn	target)	EYEDIAP (floating target)
} 2016  Citation Citation {Mora and Odobez	target)	EYEDIAP (floating target)
} 2012  Citation Citation {Wood, Baltru{²}aitis, Morency	target)	EYEDIAP (floating target)
, Robinson, and Bulling} 2016{}  Citation Citation {Smith, Yin, Feiner	target)	EYEDIAP (floating target)
, and Nayar} 2013  Citation Citation {Funesprotect unhbox voidb@x	target)	EYEDIAP (floating target)
 penalty @M	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Mora, Monay, and Odobez} 2014{}  Citation Citation {Krafka, Khosla, Kellnhofer	target)	EYEDIAP (floating target)
, Kannan, Bhandarkar, Matusik, and 	target)	EYEDIAP (floating target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (floating target)
, and Bulling} 2015  Citation Citation {Sugano, Matsushita, and	target)	EYEDIAP (floating target)
 Sato} 2014	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Wood, Baltrusaitis, Zhang, 	target)	EYEDIAP (floating target)
Sugano, Robinson, and Bulling} 2015  Citation Citation {Wood, Baltru{²}aitis, Morency	target)	EYEDIAP (floating target)
, Robinson, and Bulling} 2016{}  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (floating target)
, and Bulling} 2015  Citation Citation {Krafka, Khosla, Kellnhofer	target)	EYEDIAP (floating target)
, Kannan, Bhandarkar, Matusik, and 	target)	EYEDIAP (floating target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (floating target)
, and Bulling} 2017  Citation Citation {Deng and Zhu	target)	EYEDIAP (floating target)
} 2017	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
4 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (floating target)
ESTIMATION  Conv	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
C on ca t  x y z x y	target)	EYEDIAP (floating target)
 z x y z	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Individual Fusion Temporal  Individual Fusion	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Input 	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Individual Fusion  Normalization	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
 .Conv	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Conv .  Conv	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Conv .  FC	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
FC FC RNN  RNN	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
RNN FC  Ti m e	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Figure 1: Overview of the 	target)	EYEDIAP (floating target)
proposed network. A multi-stream CNN 	target)	EYEDIAP (floating target)
jointly models full-face, eye region 	target)	EYEDIAP (floating target)
appearance and face landmarks from 	target)	EYEDIAP (floating target)
still images. The combined extracted 	target)	EYEDIAP (floating target)
fea- tures from each frame 	target)	EYEDIAP (floating target)
are fed into a recurrent 	target)	EYEDIAP (floating target)
module to predict last frame’s 	target)	EYEDIAP (floating target)
gaze direction.  Gazing to a specific target	target)	EYEDIAP (floating target)
 is achieved by a combination	target)	EYEDIAP (floating target)
 of eye and head movements	target)	EYEDIAP (floating target)
, which are highly coordinated. 	target)	EYEDIAP (floating target)
Consequently, the apparent direction of 	target)	EYEDIAP (floating target)
gaze is influenced not only 	target)	EYEDIAP (floating target)
by the location of the 	target)	EYEDIAP (floating target)
irises within the eyelid aperture, 	target)	EYEDIAP (floating target)
but also by the position 	target)	EYEDIAP (floating target)
and orientation of the face 	target)	EYEDIAP (floating target)
with respect to the camera. 	target)	EYEDIAP (floating target)
Known as the Wollaston 	target)	EYEDIAP (floating target)
effect [36], the exact same 	target)	EYEDIAP (floating target)
set of eyes may appear 	target)	EYEDIAP (floating target)
to be looking in different 	target)	EYEDIAP (floating target)
directions due to the surrounding 	target)	EYEDIAP (floating target)
facial cues. It is therefore 	target)	EYEDIAP (floating target)
reasonable to state that eye 	target)	EYEDIAP (floating target)
images are not sufficient to 	target)	EYEDIAP (floating target)
estimate gaze direction. Instead, whole-face 	target)	EYEDIAP (floating target)
images can encode head pose 	target)	EYEDIAP (floating target)
or illumination-specific information across larger 	target)	EYEDIAP (floating target)
areas than those available just 	target)	EYEDIAP (floating target)
in the eyes region [16, 43	target)	EYEDIAP (floating target)
].  The drawback of appearance-only methods	target)	EYEDIAP (floating target)
 is that global structure information	target)	EYEDIAP (floating target)
 is not explicitly considered. In	target)	EYEDIAP (floating target)
 that sense, facial landmarks can	target)	EYEDIAP (floating target)
 be used as global shape	target)	EYEDIAP (floating target)
 cues to en- code spatial	target)	EYEDIAP (floating target)
 relationships and geometric constraints. Current	target)	EYEDIAP (floating target)
 state-of-the-art face alignment approaches are	target)	EYEDIAP (floating target)
 robust enough to handle large	target)	EYEDIAP (floating target)
 appearance variability, extreme head poses	target)	EYEDIAP (floating target)
 and occlusions, being especially useful	target)	EYEDIAP (floating target)
 when the dataset used for	target)	EYEDIAP (floating target)
 gaze estimation does not contain	target)	EYEDIAP (floating target)
 such variability. Facial landmarks are	target)	EYEDIAP (floating target)
 mainly correlated with head orientation	target)	EYEDIAP (floating target)
, eye position, eyelid openness, 	target)	EYEDIAP (floating target)
and eyebrow movement, which are 	target)	EYEDIAP (floating target)
valuable features for our task.  Therefore, in our approach we	target)	EYEDIAP (floating target)
 jointly model appearance and shape	target)	EYEDIAP (floating target)
 cues (see Figure 1). The	target)	EYEDIAP (floating target)
 former is represented by a	target)	EYEDIAP (floating target)
 whole-face image IF , along	target)	EYEDIAP (floating target)
 with a higher resolution image	target)	EYEDIAP (floating target)
 of the eyes IE to	target)	EYEDIAP (floating target)
 identify subtle changes. Due to	target)	EYEDIAP (floating target)
 dealing with wide head pose	target)	EYEDIAP (floating target)
 ranges, some eye images may	target)	EYEDIAP (floating target)
 not depict the whole eye	target)	EYEDIAP (floating target)
, containing mostly background or 	target)	EYEDIAP (floating target)
other surrounding facial parts instead. 	target)	EYEDIAP (floating target)
For that reason, and contrary 	target)	EYEDIAP (floating target)
to previous approaches that only 	target)	EYEDIAP (floating target)
use one eye image [31, 42	target)	EYEDIAP (floating target)
], we use a single 	target)	EYEDIAP (floating target)
image composed of two patches 	target)	EYEDIAP (floating target)
of centered left and right 	target)	EYEDIAP (floating target)
eyes. Finally, the shape cue 	target)	EYEDIAP (floating target)
is represented by 3D face 	target)	EYEDIAP (floating target)
landmarks obtained from a 68-landmark 	target)	EYEDIAP (floating target)
model, denoted by 	target)	EYEDIAP (floating target)
L = {(lx, ly, 	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
)	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
 | ∀c ∈ [1, ...,68	target)	EYEDIAP (floating target)
]}.  In this work we also	target)	EYEDIAP (floating target)
 consider the dynamic component of	target)	EYEDIAP (floating target)
 gaze. We leverage the se	target)	EYEDIAP (floating target)
- quential information of eye 	target)	EYEDIAP (floating target)
and head movements such that, 	target)	EYEDIAP (floating target)
given appearance and shape features 	target)	EYEDIAP (floating target)
of consecutive frames, it is 	target)	EYEDIAP (floating target)
possible to better predict the 	target)	EYEDIAP (floating target)
gaze direction of the cur- 	target)	EYEDIAP (floating target)
rent frame. Therefore, the 3D 	target)	EYEDIAP (floating target)
gaze estimation task for a 1	target)	EYEDIAP (floating target)
-frame sequence is formulated  Citation Citation {Wollaston etprotect unhbox	target)	EYEDIAP (floating target)
 voidb@x penalty @M	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
al.} 1824  Citation Citation {Krafka, Khosla, Kellnhofer	target)	EYEDIAP (floating target)
, Kannan, Bhandarkar, Matusik, and 	target)	EYEDIAP (floating target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (floating target)
, and Bulling} 2017  Citation Citation {Sugano, Matsushita, and	target)	EYEDIAP (floating target)
 Sato} 2014	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Zhang, Sugano, Fritz, 	target)	EYEDIAP (floating target)
and Bulling} 2015	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 5	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
as g(i) = f ( {IF (i)},{IE (i)},{L(i	target)	EYEDIAP (floating target)
)}  ) , where i denotes	target)	EYEDIAP (floating target)
 the i-th frame, and f	target)	EYEDIAP (floating target)
 is the regression	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
function.  3.2 Data normalization Prior to	target)	EYEDIAP (floating target)
 gaze regression, a normalization step	target)	EYEDIAP (floating target)
 in the 3D space and	target)	EYEDIAP (floating target)
 the 2D image, similar to	target)	EYEDIAP (floating target)
 [31], is carried out. This	target)	EYEDIAP (floating target)
 is performed to reduce the	target)	EYEDIAP (floating target)
 appearance variability and to allow	target)	EYEDIAP (floating target)
 the gaze estimation model to	target)	EYEDIAP (floating target)
 be applied regardless of the	target)	EYEDIAP (floating target)
 original camera configuration	target)	EYEDIAP (floating target)
.  Let H ∈ R3x3 be	target)	EYEDIAP (floating target)
 the head rotation matrix, and	target)	EYEDIAP (floating target)
 p = [px, py, pz]T	target)	EYEDIAP (floating target)
 ∈ R3 the reference face	target)	EYEDIAP (floating target)
 location with respect to the	target)	EYEDIAP (floating target)
 original CCS. The goal is	target)	EYEDIAP (floating target)
 to find the conversion matrix	target)	EYEDIAP (floating target)
 M = SR such that	target)	EYEDIAP (floating target)
 (a) the X-axes of the	target)	EYEDIAP (floating target)
 virtual camera and the head	target)	EYEDIAP (floating target)
 become parallel using the rotation	target)	EYEDIAP (floating target)
 matrix R, and (b) the	target)	EYEDIAP (floating target)
 virtual camera looks at the	target)	EYEDIAP (floating target)
 reference location from a fixed	target)	EYEDIAP (floating target)
 distance dn using the Z-direction	target)	EYEDIAP (floating target)
 scaling matrix S = diag(1,1,dn/‖p	target)	EYEDIAP (floating target)
‖). R is computed as 	target)	EYEDIAP (floating target)
a = p̂×HT e1, 	target)	EYEDIAP (floating target)
b = â× p̂, 	target)	EYEDIAP (floating target)
R = [â, b̂, p̂]T , where e1 denotes the first	target)	EYEDIAP (floating target)
 orthonormal basis and	target)	EYEDIAP (floating target)
 〈 ·̂ 〉 is the	target)	EYEDIAP (floating target)
 unit vector	target)	EYEDIAP (floating target)
.  This normalization translates into the	target)	EYEDIAP (floating target)
 image space as a cropped	target)	EYEDIAP (floating target)
 image patch of size Wn×Hn	target)	EYEDIAP (floating target)
 centered at p where head	target)	EYEDIAP (floating target)
 roll rotation has been removed	target)	EYEDIAP (floating target)
. This is done by 	target)	EYEDIAP (floating target)
applying a perspective warping to 	target)	EYEDIAP (floating target)
the input image I using 	target)	EYEDIAP (floating target)
the transformation matrix W = 	target)	EYEDIAP (floating target)
CoMCn−1, where Co and Cn 	target)	EYEDIAP (floating target)
are the original and virtual 	target)	EYEDIAP (floating target)
camera matrices, respectively.  The 3D gaze vector is	target)	EYEDIAP (floating target)
 also normalized as gn =Rg	target)	EYEDIAP (floating target)
. After image normalization, the 	target)	EYEDIAP (floating target)
line of sight can be 	target)	EYEDIAP (floating target)
represented in a 2D space. 	target)	EYEDIAP (floating target)
Therefore, gn is further transformed 	target)	EYEDIAP (floating target)
to spherical coor- dinates (θ ,	target)	EYEDIAP (floating target)
φ) assuming unit length, where 	target)	EYEDIAP (floating target)
θ and φ denote the 	target)	EYEDIAP (floating target)
horizontal and vertical direc- tion 	target)	EYEDIAP (floating target)
angles, respectively. This 2D angle 	target)	EYEDIAP (floating target)
representation, delimited in the 	target)	EYEDIAP (floating target)
range [−π/2,π/2], is computed as 	target)	EYEDIAP (floating target)
θ = arctan(gx/gz) and 	target)	EYEDIAP (floating target)
φ = arcsin(−gy), such that (0,	target)	EYEDIAP (floating target)
0) represents looking straight ahead 	target)	EYEDIAP (floating target)
to the CCS origin.  3.3 Recurrent Convolutional Neural Network	target)	EYEDIAP (floating target)
 We propose a Recurrent CNN	target)	EYEDIAP (floating target)
 Regression Network for 3D gaze	target)	EYEDIAP (floating target)
 estimation. The network is divided	target)	EYEDIAP (floating target)
 in 3 modules: (1) Individual	target)	EYEDIAP (floating target)
, (2) Fusion, and (3) 	target)	EYEDIAP (floating target)
Temporal.  First, the Individual module learns	target)	EYEDIAP (floating target)
 features from each appearance cue	target)	EYEDIAP (floating target)
 separately. It consists of a	target)	EYEDIAP (floating target)
 two-stream CNN, one devoted to	target)	EYEDIAP (floating target)
 the normalized face image stream	target)	EYEDIAP (floating target)
 and the other to the	target)	EYEDIAP (floating target)
 joint normalized eyes image. Next	target)	EYEDIAP (floating target)
, the Fusion module combines 	target)	EYEDIAP (floating target)
the extracted features of each 	target)	EYEDIAP (floating target)
appearance stream in a single 	target)	EYEDIAP (floating target)
vector along with the normalized 	target)	EYEDIAP (floating target)
landmark coordinates. Then, it learns 	target)	EYEDIAP (floating target)
a joint representation between modalities 	target)	EYEDIAP (floating target)
in a late-fusion fashion. Both 	target)	EYEDIAP (floating target)
Individual and Fusion modules, further 	target)	EYEDIAP (floating target)
referred to as Static model, 	target)	EYEDIAP (floating target)
are applied to each frame 	target)	EYEDIAP (floating target)
of the sequence. Finally, the 	target)	EYEDIAP (floating target)
resulting feature vectors of each 	target)	EYEDIAP (floating target)
frame are input to the 	target)	EYEDIAP (floating target)
Temporal module based on a 	target)	EYEDIAP (floating target)
many-to-one recurrent network. This module 	target)	EYEDIAP (floating target)
leverages sequential information to predict 	target)	EYEDIAP (floating target)
the normalized 2D gaze angles 	target)	EYEDIAP (floating target)
of the last frame of 	target)	EYEDIAP (floating target)
the sequence using a linear 	target)	EYEDIAP (floating target)
regression layer added on top 	target)	EYEDIAP (floating target)
of it.  3.4 Implementation details 3.4.1 Network	target)	EYEDIAP (floating target)
 details	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Each stream of the Individual 	target)	EYEDIAP (floating target)
module is based on the 	target)	EYEDIAP (floating target)
VGG-16 deep network [27], consisting 	target)	EYEDIAP (floating target)
of 13 convolutional layers, 5 	target)	EYEDIAP (floating target)
max pooling layers, and 1 	target)	EYEDIAP (floating target)
fully connected (FC) layer with 	target)	EYEDIAP (floating target)
Rec- tified Linear Unit (ReLU) 	target)	EYEDIAP (floating target)
activations. The full-face stream follows 	target)	EYEDIAP (floating target)
the same configuration  Citation Citation {Sugano, Matsushita, and	target)	EYEDIAP (floating target)
 Sato} 2014	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Parkhi, Vedaldi, and 	target)	EYEDIAP (floating target)
Zisserman} 2015	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
6 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (floating target)
ESTIMATION  as the base network, having	target)	EYEDIAP (floating target)
 an input of 224×224 pixels	target)	EYEDIAP (floating target)
 and a 4096D FC layer	target)	EYEDIAP (floating target)
. In contrast, the input 	target)	EYEDIAP (floating target)
joint eye image is smaller, 	target)	EYEDIAP (floating target)
with a final size of 120	target)	EYEDIAP (floating target)
×48 pixels, so the number 	target)	EYEDIAP (floating target)
of pa- rameters is decreased 	target)	EYEDIAP (floating target)
proportionally. In this case, its 	target)	EYEDIAP (floating target)
last FC layer produces a 	target)	EYEDIAP (floating target)
1536D vector. A 204D landmark 	target)	EYEDIAP (floating target)
coordinates vector is concatenated to 	target)	EYEDIAP (floating target)
the output of the FC 	target)	EYEDIAP (floating target)
layer of each stream, resulting 	target)	EYEDIAP (floating target)
in a 5836D feature vector. 	target)	EYEDIAP (floating target)
Consequently, the Fusion module consists 	target)	EYEDIAP (floating target)
of 2 5836D FC layers 	target)	EYEDIAP (floating target)
with ReLU activations and 2 	target)	EYEDIAP (floating target)
dropout layers between FCs as 	target)	EYEDIAP (floating target)
regularization. Finally, to model the 	target)	EYEDIAP (floating target)
temporal dependencies, we use a 	target)	EYEDIAP (floating target)
single GRU layer with 128 	target)	EYEDIAP (floating target)
units.  The network is trained in	target)	EYEDIAP (floating target)
 a stage-wise fashion. First, we	target)	EYEDIAP (floating target)
 train the Static model and	target)	EYEDIAP (floating target)
 the final regression layer end-to-end	target)	EYEDIAP (floating target)
 on each individual frame of	target)	EYEDIAP (floating target)
 the training data. The convolutional	target)	EYEDIAP (floating target)
 blocks are pre-trained with the	target)	EYEDIAP (floating target)
 VGG-Face dataset [27], whereas the	target)	EYEDIAP (floating target)
 FCs are trained from scratch	target)	EYEDIAP (floating target)
. Second, the training data 	target)	EYEDIAP (floating target)
is re-arranged by means of 	target)	EYEDIAP (floating target)
a sliding window with stride 1 to build input sequences. Each	target)	EYEDIAP (floating target)
 sequence is composed of s	target)	EYEDIAP (floating target)
 = 4 consecutive frames, whose	target)	EYEDIAP (floating target)
 gaze direction target is the	target)	EYEDIAP (floating target)
 gaze direction of the last	target)	EYEDIAP (floating target)
 frame of the sequence( {I(i−s+1	target)	EYEDIAP (floating target)
), . . . ,I(i)}, 	target)	EYEDIAP (floating target)
g(i)  ) . Using this re-arranged	target)	EYEDIAP (floating target)
 training data, we extract features	target)	EYEDIAP (floating target)
 of each	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
frame of the sequence from 	target)	EYEDIAP (floating target)
a frozen Individual module, fine-tune 	target)	EYEDIAP (floating target)
the Fusion layers, and train 	target)	EYEDIAP (floating target)
both, the Temporal module and 	target)	EYEDIAP (floating target)
a new final regression layer 	target)	EYEDIAP (floating target)
from scratch. This way, the 	target)	EYEDIAP (floating target)
network can exploit the temporal 	target)	EYEDIAP (floating target)
information to further refine the 	target)	EYEDIAP (floating target)
fusion weights.  We trained the model using	target)	EYEDIAP (floating target)
 ADAM optimizer with an initial	target)	EYEDIAP (floating target)
 learning rate of 0.0001, dropout	target)	EYEDIAP (floating target)
 of 0.3, and batch size	target)	EYEDIAP (floating target)
 of 64 frames. The number	target)	EYEDIAP (floating target)
 of epochs was experimentally set	target)	EYEDIAP (floating target)
 to 21 for the first	target)	EYEDIAP (floating target)
 training stage and 10 for	target)	EYEDIAP (floating target)
 the second. We use the	target)	EYEDIAP (floating target)
 average Euclidean distance between the	target)	EYEDIAP (floating target)
 predicted and ground-truth 3D gaze	target)	EYEDIAP (floating target)
 vectors as loss function	target)	EYEDIAP (floating target)
.  3.4.2 Input pre-processing	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
For this work we use 	target)	EYEDIAP (floating target)
head pose and eye locations 	target)	EYEDIAP (floating target)
in the 3D scene provided 	target)	EYEDIAP (floating target)
by the dataset. The 3D 	target)	EYEDIAP (floating target)
landmarks are extracted using the 	target)	EYEDIAP (floating target)
state-of-the-art method of Bulat and 	target)	EYEDIAP (floating target)
Tzimiropou- los [3], which is 	target)	EYEDIAP (floating target)
based on stacked hourglass 	target)	EYEDIAP (floating target)
networks [24].  During training, the original image	target)	EYEDIAP (floating target)
 is pre-processed to get the	target)	EYEDIAP (floating target)
 two normalized input images. The	target)	EYEDIAP (floating target)
 normalized whole-face patch is centered	target)	EYEDIAP (floating target)
 0.1 meters ahead of the	target)	EYEDIAP (floating target)
 head center in the head	target)	EYEDIAP (floating target)
 coordinate system, and Cn is	target)	EYEDIAP (floating target)
 defined such that the image	target)	EYEDIAP (floating target)
 has size of 250× 250	target)	EYEDIAP (floating target)
 pixels. The difference between this	target)	EYEDIAP (floating target)
 size and the final input	target)	EYEDIAP (floating target)
 size allows us to perform	target)	EYEDIAP (floating target)
 random cropping and zooming to	target)	EYEDIAP (floating target)
 augment the data (explained in	target)	EYEDIAP (floating target)
 Section 4.1). Similarly, each normalized	target)	EYEDIAP (floating target)
 eye patch is centered in	target)	EYEDIAP (floating target)
 their respective eye center locations	target)	EYEDIAP (floating target)
. In this case, the 	target)	EYEDIAP (floating target)
virtual camera matrix is defined 	target)	EYEDIAP (floating target)
so that the image is 	target)	EYEDIAP (floating target)
cropped to 70×58, while in 	target)	EYEDIAP (floating target)
practice the final patches have 	target)	EYEDIAP (floating target)
size of 60×48. Landmarks are 	target)	EYEDIAP (floating target)
normalized using the same procedure 	target)	EYEDIAP (floating target)
and further pre-processed with mean 	target)	EYEDIAP (floating target)
subtraction and min-max normalization per 	target)	EYEDIAP (floating target)
axis. Finally, we divide them 	target)	EYEDIAP (floating target)
by a scaling factor w 	target)	EYEDIAP (floating target)
such that all coordinates are 	target)	EYEDIAP (floating target)
in the range [0,w]. This 	target)	EYEDIAP (floating target)
way, all concatenated feature values 	target)	EYEDIAP (floating target)
are in a similar range. 	target)	EYEDIAP (floating target)
After inference, the predicted normalized 	target)	EYEDIAP (floating target)
2D angles are de-normalized back 	target)	EYEDIAP (floating target)
to the original 3D space.  4 Experiments In this section	target)	EYEDIAP (floating target)
, we evaluate the cross-subject 	target)	EYEDIAP (floating target)
3D gaze estimation task on 	target)	EYEDIAP (floating target)
a wide range of head 	target)	EYEDIAP (floating target)
poses and gaze directions. Furthermore, 	target)	EYEDIAP (floating target)
we validate the effectiveness of 	target)	EYEDIAP (floating target)
the proposed architecture comparing both 	target)	EYEDIAP (floating target)
static and temporal approaches. We 	target)	EYEDIAP (floating target)
report the error in terms 	target)	EYEDIAP (floating target)
of mean angular error between 	target)	EYEDIAP (floating target)
predicted and ground-truth 3D gaze 	target)	EYEDIAP (floating target)
vectors. Note that due to 	target)	EYEDIAP (floating target)
the requirements of the temporal 	target)	EYEDIAP (floating target)
model not all the frames 	target)	EYEDIAP (floating target)
obtain a prediction. Therefore, for 	target)	EYEDIAP (floating target)
a  Citation Citation {Parkhi, Vedaldi, and	target)	EYEDIAP (floating target)
 Zisserman} 2015	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Bulat and Tzimiropoulos} 2017	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Newell, Yang, and 	target)	EYEDIAP (floating target)
Deng} 2016	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 7	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
60 30 0 30 60  60	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
30  0	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
30  60	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
100  101	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
102  60 30 0 30 60	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
60  30	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
0  30	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
60  100	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
101  102	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
103  60 30 0 30 60	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
60  30	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
0  30	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
60  100	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
101  102	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
60 30 0 30 60  60	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
30  0	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
30  60	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
100  101	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
102  103	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
a) g (FT ) (b) 	target)	EYEDIAP (floating target)
h (FT ) (c) g (	target)	EYEDIAP (floating target)
CS) (d) h (CS)  Figure 2: Ground-truth eye gaze	target)	EYEDIAP (floating target)
 g and head orientation h	target)	EYEDIAP (floating target)
 distribution on the filtered EYE	target)	EYEDIAP (floating target)
- DIAP dataset for CS 	target)	EYEDIAP (floating target)
and FT settings, in terms 	target)	EYEDIAP (floating target)
of x- and y- angles.  fair comparison, the reported results	target)	EYEDIAP (floating target)
 for static models disregard such	target)	EYEDIAP (floating target)
 frames when temporal models are	target)	EYEDIAP (floating target)
 included in the comparison	target)	EYEDIAP (floating target)
.  4.1 Training data	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
There are few publicly available 	target)	EYEDIAP (floating target)
datasets devoted to 3D gaze 	target)	EYEDIAP (floating target)
estimation and most of them 	target)	EYEDIAP (floating target)
focus on HCI with a 	target)	EYEDIAP (floating target)
limited range of head pose 	target)	EYEDIAP (floating target)
and gaze directions. Therefore, we 	target)	EYEDIAP (floating target)
use VGA videos from the 	target)	EYEDIAP (floating target)
publicly-available EYEDIAP dataset [7] to 	target)	EYEDIAP (floating target)
perform the experimental evaluation, as 	target)	EYEDIAP (floating target)
it is currently the only 	target)	EYEDIAP (floating target)
one containing video sequences with 	target)	EYEDIAP (floating target)
a wide range of head 	target)	EYEDIAP (floating target)
poses and showing the full 	target)	EYEDIAP (floating target)
face. This dataset consists of 3	target)	EYEDIAP (floating target)
-minute videos of 16 subjects 	target)	EYEDIAP (floating target)
looking at two types of 	target)	EYEDIAP (floating target)
targets: continuous screen targets on 	target)	EYEDIAP (floating target)
a fixed monitor (CS), and 	target)	EYEDIAP (floating target)
floating physical targets (FT ). 	target)	EYEDIAP (floating target)
The videos are further divided 	target)	EYEDIAP (floating target)
into static (S) and moving (	target)	EYEDIAP (floating target)
M) head pose for each 	target)	EYEDIAP (floating target)
of the subjects. Subjects 12-16 	target)	EYEDIAP (floating target)
were recorded with 2 different 	target)	EYEDIAP (floating target)
lighting conditions.  For evaluation, we filtered out	target)	EYEDIAP (floating target)
 those frames that fulfilled at	target)	EYEDIAP (floating target)
 least one of the following	target)	EYEDIAP (floating target)
 conditions: (1) face or landmarks	target)	EYEDIAP (floating target)
 not detected; (2) subject not	target)	EYEDIAP (floating target)
 looking at the target; (3	target)	EYEDIAP (floating target)
) 3D head pose, eyes 	target)	EYEDIAP (floating target)
or target location not properly 	target)	EYEDIAP (floating target)
recovered; and (4) eyeball rotations 	target)	EYEDIAP (floating target)
violating physical 	target)	EYEDIAP (floating target)
constraints (|θ | ≤ 40	target)	EYEDIAP (floating target)
◦, |φ | ≤ 30	target)	EYEDIAP (floating target)
◦) [23]. Note that we 	target)	EYEDIAP (floating target)
purposely do not filter eye 	target)	EYEDIAP (floating target)
blinking moments to learn their 	target)	EYEDIAP (floating target)
dynamics with the temporal model, 	target)	EYEDIAP (floating target)
which may produce some outliers 	target)	EYEDIAP (floating target)
with a higher prediction error 	target)	EYEDIAP (floating target)
due to a less accurate 	target)	EYEDIAP (floating target)
ground truth. Figure 2 shows 	target)	EYEDIAP (floating target)
the distribution of gaze directions 	target)	EYEDIAP (floating target)
and head poses for both 	target)	EYEDIAP (floating target)
filtered CS and FT cases.  We applied data augmentation to	target)	EYEDIAP (floating target)
 the training set with the	target)	EYEDIAP (floating target)
 following random transforma- tions: horizontal	target)	EYEDIAP (floating target)
 flip, shifts of up to	target)	EYEDIAP (floating target)
 5 pixels, zoom of up	target)	EYEDIAP (floating target)
 to 2%, brightness changes by	target)	EYEDIAP (floating target)
 a factor in the range	target)	EYEDIAP (floating target)
 [0.4,1.75], and additive Gaussian noise	target)	EYEDIAP (floating target)
 with σ2 = 0.03	target)	EYEDIAP (floating target)
.  4.2 Evaluation of static modalities	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
First, we evaluate the contribution 	target)	EYEDIAP (floating target)
of each static modality on 	target)	EYEDIAP (floating target)
the FT scenario. We divided 	target)	EYEDIAP (floating target)
the 16 participants into 4 	target)	EYEDIAP (floating target)
groups, such that appearance variability 	target)	EYEDIAP (floating target)
was maximized while maintaining a 	target)	EYEDIAP (floating target)
similar number of training samples 	target)	EYEDIAP (floating target)
per group. Each static model 	target)	EYEDIAP (floating target)
was trained end-to-end performing 4-fold 	target)	EYEDIAP (floating target)
cross-validation using different combinations of 	target)	EYEDIAP (floating target)
input modal- ities. Since the 	target)	EYEDIAP (floating target)
number of fusion units depends 	target)	EYEDIAP (floating target)
on the number of input 	target)	EYEDIAP (floating target)
modalities, we also compare different 	target)	EYEDIAP (floating target)
fusion layer sizes. The effect 	target)	EYEDIAP (floating target)
of data normalization is also 	target)	EYEDIAP (floating target)
evaluated by training a not-normalized 	target)	EYEDIAP (floating target)
face model where the input 	target)	EYEDIAP (floating target)
image is the face bounding 	target)	EYEDIAP (floating target)
box with square size the 	target)	EYEDIAP (floating target)
maximum distance between 2D landmarks.  Citation Citation {Funesprotect unhbox voidb@x	target)	EYEDIAP (floating target)
 penalty @M	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Mora, Monay, and Odobez} 2014{}  Citation Citation {MSC	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
8 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (floating target)
ESTIMATION  0 1 2 3 4	target)	EYEDIAP (floating target)
 5 6 7 8 9	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
10 11  An gl	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
e  er	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
ro r (  de gr	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
ee s)  6.9 6.43 5.58 5.71 5.59	target)	EYEDIAP (floating target)
 5.55 5.52	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
OF-4096 NE-1536 NF-4096  NF-5632 NFL-4300	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
NFE-5632 NFEL-5836  Figure 3: Performance evaluation of	target)	EYEDIAP (floating target)
 the Static network using different	target)	EYEDIAP (floating target)
 input modali- ties (O	target)	EYEDIAP (floating target)
 - Not normalized, N	target)	EYEDIAP (floating target)
 - Normalized, F - Face	target)	EYEDIAP (floating target)
, E - Eyes, 	target)	EYEDIAP (floating target)
L - 3D Landmarks) and 	target)	EYEDIAP (floating target)
size of fusion layers on 	target)	EYEDIAP (floating target)
the FT scenario.  Floating Target Screen Target 0	target)	EYEDIAP (floating target)
 1 2 3 4 5	target)	EYEDIAP (floating target)
 6 7 8 9	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
10 11  An gl	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
e  er	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
ro r (  de gr	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
ee s)  6.36 5.43 5.19 4.2 3.38	target)	EYEDIAP (floating target)
 3.4	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
MPIIGaze Static Temporal  Figure 4: Performance comparison among	target)	EYEDIAP (floating target)
 MPIIGaze method [42] and our	target)	EYEDIAP (floating target)
 Static and Temporal versions of	target)	EYEDIAP (floating target)
 the proposed network for FT	target)	EYEDIAP (floating target)
 and CS scenarios	target)	EYEDIAP (floating target)
.  As shown in Figure 3	target)	EYEDIAP (floating target)
, all models that take 	target)	EYEDIAP (floating target)
normalized full-face information as input 	target)	EYEDIAP (floating target)
achieve better performance than the 	target)	EYEDIAP (floating target)
eyes-only model. More specifically, the 	target)	EYEDIAP (floating target)
combination of face, eyes and 	target)	EYEDIAP (floating target)
landmarks outperforms all the other 	target)	EYEDIAP (floating target)
combinations by a small but 	target)	EYEDIAP (floating target)
significant margin (paired Wilcoxon test, 	target)	EYEDIAP (floating target)
p < 0.0001). The standard 	target)	EYEDIAP (floating target)
deviation of the best-performing model 	target)	EYEDIAP (floating target)
is reduced compared to the 	target)	EYEDIAP (floating target)
face and eyes model, suggesting 	target)	EYEDIAP (floating target)
a regularizing effect due to 	target)	EYEDIAP (floating target)
the addition of landmarks. The 	target)	EYEDIAP (floating target)
not-normalized face-only model shows the 	target)	EYEDIAP (floating target)
largest error, proving the impact 	target)	EYEDIAP (floating target)
of normalization to reduce the 	target)	EYEDIAP (floating target)
appearance variability. Furthermore, our results 	target)	EYEDIAP (floating target)
indicate that the increase of 	target)	EYEDIAP (floating target)
fusion units is not correlated 	target)	EYEDIAP (floating target)
with a better performance.  4.3 Static gaze regression: comparison	target)	EYEDIAP (floating target)
 with existing methods	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
We compare our best-performing static 	target)	EYEDIAP (floating target)
model with three baselines. Head: 	target)	EYEDIAP (floating target)
Treating the head pose directly 	target)	EYEDIAP (floating target)
as gaze direction. PR-ALR: Method 	target)	EYEDIAP (floating target)
that relies on RGB-D data 	target)	EYEDIAP (floating target)
to rectify the eye images 	target)	EYEDIAP (floating target)
viewpoint into a canonical head 	target)	EYEDIAP (floating target)
pose using a 3DMM. It 	target)	EYEDIAP (floating target)
then learns an RGB gaze 	target)	EYEDIAP (floating target)
appearance model using ALR [21]. 	target)	EYEDIAP (floating target)
Predicted 3D vectors for FT-S 	target)	EYEDIAP (floating target)
scenario are provided by EYEDIAP 	target)	EYEDIAP (floating target)
dataset. MPIIGaze:. State-of-the-art full-face 3D 	target)	EYEDIAP (floating target)
gaze estimation method [42]. They 	target)	EYEDIAP (floating target)
use an Alexnet-based CNN model 	target)	EYEDIAP (floating target)
with spatial weights to enhance 	target)	EYEDIAP (floating target)
information in different facial regions. 	target)	EYEDIAP (floating target)
We fine-tuned it with the 	target)	EYEDIAP (floating target)
filtered EYEDIAP subsets using our 	target)	EYEDIAP (floating target)
training parameters and normalization procedure.  In addition to the aforementioned	target)	EYEDIAP (floating target)
 FT-based evaluation setup, we also	target)	EYEDIAP (floating target)
 evaluate our method on the	target)	EYEDIAP (floating target)
 CS scenario. In this case	target)	EYEDIAP (floating target)
 there are only 14 participants	target)	EYEDIAP (floating target)
 available, so we divided them	target)	EYEDIAP (floating target)
 in 5 groups and performed	target)	EYEDIAP (floating target)
 5-fold cross-validation. In Figure 4	target)	EYEDIAP (floating target)
 we compare our method to	target)	EYEDIAP (floating target)
 MPIIGaze, achieving a statistically significant	target)	EYEDIAP (floating target)
 improvement of 14.6% and 19.5	target)	EYEDIAP (floating target)
% on FT and CS 	target)	EYEDIAP (floating target)
scenarios, respectively (paired Wilcoxon test, 	target)	EYEDIAP (floating target)
p < 0.0001). We can 	target)	EYEDIAP (floating target)
observe that a re- stricted 	target)	EYEDIAP (floating target)
gaze target benefits the performance 	target)	EYEDIAP (floating target)
of all methods, compared to 	target)	EYEDIAP (floating target)
a more challenging unrestricted setting 	target)	EYEDIAP (floating target)
with a wider range of 	target)	EYEDIAP (floating target)
head poses and gaze directions.  Table 2 provides a detailed	target)	EYEDIAP (floating target)
 comparison on every participant, performing	target)	EYEDIAP (floating target)
 leave-one-out cross-validation on the FT	target)	EYEDIAP (floating target)
 scenario for static and moving	target)	EYEDIAP (floating target)
 head separately. Results show that	target)	EYEDIAP (floating target)
, as expected, facial appearance 	target)	EYEDIAP (floating target)
and head pose have a 	target)	EYEDIAP (floating target)
noticeable impact on gaze accuracy, 	target)	EYEDIAP (floating target)
with average error differences of 	target)	EYEDIAP (floating target)
up to 7.7◦ among participants.  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (floating target)
, and Bulling} 2015  Citation Citation {Mora and Odobez	target)	EYEDIAP (floating target)
} 2012  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (floating target)
, and Bulling} 2015	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 9	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Method 1 2 3 4 5 6 7 8 9 10	target)	EYEDIAP (floating target)
 11 12 13 14 15	target)	EYEDIAP (floating target)
 16 Avg. Head 23.5 22.1	target)	EYEDIAP (floating target)
 20.3 23.6 23.2 23.2 23.6	target)	EYEDIAP (floating target)
 21.2 26.7 23.6 23.1 24.4	target)	EYEDIAP (floating target)
 23.3 24.0 24.5 22.8 23.3	target)	EYEDIAP (floating target)
 PR-ALR 12.3 12.0 12.4 11.3	target)	EYEDIAP (floating target)
 15.5 12.9 17.9 11.8 17.3	target)	EYEDIAP (floating target)
 13.4 13.4 14.3 15.2 13.6	target)	EYEDIAP (floating target)
 14.4 14.6 13.9 MPIIGaze 5.3	target)	EYEDIAP (floating target)
 5.1 5.7 4.7 7.3 15.1	target)	EYEDIAP (floating target)
 10.8 5.7 9.9 7.1 5.0	target)	EYEDIAP (floating target)
 5.7 7.4 3.8 4.8 5.5	target)	EYEDIAP (floating target)
 6.8 Static 3.9 4.1 4.2	target)	EYEDIAP (floating target)
 3.9 6.0 6.4 7.2 3.6	target)	EYEDIAP (floating target)
 7.1 5.0 5.7 6.7 3.9	target)	EYEDIAP (floating target)
 4.7 5.1 4.2 5.1 Temporal	target)	EYEDIAP (floating target)
 4.0 4.9 4.3 4.1 6.1	target)	EYEDIAP (floating target)
 6.5 6.6 3.9 7.8 6.1	target)	EYEDIAP (floating target)
 4.7 5.6 4.7 3.5 5.9	target)	EYEDIAP (floating target)
 4.6 5.2 Head 19.3 14.2	target)	EYEDIAP (floating target)
 16.4 19.9 16.8 21.9 16.1	target)	EYEDIAP (floating target)
 24.2 20.3 19.9 18.8 22.3	target)	EYEDIAP (floating target)
 18.1 14.9 16.2 19.3 18.7	target)	EYEDIAP (floating target)
 MPIIGaze 7.6 6.2 5.7 8.7	target)	EYEDIAP (floating target)
 10.1 12.0 12.2 6.1 8.3	target)	EYEDIAP (floating target)
 5.9 6.1 6.2 7.4 4.7	target)	EYEDIAP (floating target)
 4.4 6.0 7.3 Static 5.8	target)	EYEDIAP (floating target)
 5.7 4.4 7.5 6.7 8.8	target)	EYEDIAP (floating target)
 11.6 5.5 8.3 5.5 5.2	target)	EYEDIAP (floating target)
 6.3 5.3 3.9 4.3 5.6	target)	EYEDIAP (floating target)
 6.3 Temporal 6.1 5.6 4.5	target)	EYEDIAP (floating target)
 7.5 6.4 8.2 12.0 5.0	target)	EYEDIAP (floating target)
 7.5 5.4 5.0 5.8 6.6	target)	EYEDIAP (floating target)
 4.0 4.5 5.8 6.2	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Table 2: Gaze angular error 	target)	EYEDIAP (floating target)
comparison for static (top half) 	target)	EYEDIAP (floating target)
and moving (bottom half) head 	target)	EYEDIAP (floating target)
pose for each subject in 	target)	EYEDIAP (floating target)
the FT scenario. Best results 	target)	EYEDIAP (floating target)
in bold.  −80 −40 0 40 80−80	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
40  0	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
40  80	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
0  5	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
10  15	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
20  25	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
30  35	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
80 −40 0 40 80−80  −40	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
0  40	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
80  −10	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
8  −6	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
4  −2	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
0  2	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
4  6	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
8  10	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
80 −40 0 40 80−80  −40	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
0  40	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
80  0	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
5  10	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
15  20	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
25  30	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
35  −80 −40 0 40 80−80	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
40  0	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
40  80	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
10  −8	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
6  −4	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
2  0	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
2  4	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
6  8	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
10  (a) Gaze space (b) Head	target)	EYEDIAP (floating target)
 orientation space	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Figure 5: Angular error distribution 	target)	EYEDIAP (floating target)
across gaze (a) and head 	target)	EYEDIAP (floating target)
orientation (b) spaces in the 	target)	EYEDIAP (floating target)
FT setting, in terms of 	target)	EYEDIAP (floating target)
x- and y- angles. For 	target)	EYEDIAP (floating target)
each space, we depict the 	target)	EYEDIAP (floating target)
Static model performance (left) and 	target)	EYEDIAP (floating target)
the contribution of the Temporal 	target)	EYEDIAP (floating target)
model versus Static (right). In 	target)	EYEDIAP (floating target)
the latter, positive difference means 	target)	EYEDIAP (floating target)
higher improvement of the Temporal 	target)	EYEDIAP (floating target)
model.  4.4 Evaluation of the temporal	target)	EYEDIAP (floating target)
 network	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
In this section, we evaluate 	target)	EYEDIAP (floating target)
the contribution of adding the 	target)	EYEDIAP (floating target)
temporal module to the static 	target)	EYEDIAP (floating target)
model. To do so, we 	target)	EYEDIAP (floating target)
trained a lower-dimensional version of 	target)	EYEDIAP (floating target)
the static network with compa- 	target)	EYEDIAP (floating target)
rable performance to the original, 	target)	EYEDIAP (floating target)
reducing the number of units 	target)	EYEDIAP (floating target)
of the second fusion layer 	target)	EYEDIAP (floating target)
to 2918. Results are reported 	target)	EYEDIAP (floating target)
in Figure 4 and Table 2	target)	EYEDIAP (floating target)
. One can observe that 	target)	EYEDIAP (floating target)
using sequential information is helpful 	target)	EYEDIAP (floating target)
on the FT scenario, outperforming 	target)	EYEDIAP (floating target)
the static model by a 	target)	EYEDIAP (floating target)
statistically significant 4.4% (paired Wilcoxon 	target)	EYEDIAP (floating target)
test, p < 0.0001). This 	target)	EYEDIAP (floating target)
contribution is more noticeable in 	target)	EYEDIAP (floating target)
the moving head setting, proving 	target)	EYEDIAP (floating target)
that the temporal model can 	target)	EYEDIAP (floating target)
benefit from head motion information. 	target)	EYEDIAP (floating target)
In contrast, such information seems 	target)	EYEDIAP (floating target)
to be less meaningful in 	target)	EYEDIAP (floating target)
the CS scenario, where the 	target)	EYEDIAP (floating target)
obtained error is already very 	target)	EYEDIAP (floating target)
low for a cross-subject setting 	target)	EYEDIAP (floating target)
and the amount of head 	target)	EYEDIAP (floating target)
movement declines.  Figure 5 further explores the	target)	EYEDIAP (floating target)
 error distribution of the static	target)	EYEDIAP (floating target)
 network and the impact of	target)	EYEDIAP (floating target)
 sequential information. We can observe	target)	EYEDIAP (floating target)
 that the accuracy of the	target)	EYEDIAP (floating target)
 static model drops with extreme	target)	EYEDIAP (floating target)
 head poses and gaze directions	target)	EYEDIAP (floating target)
, which can also be 	target)	EYEDIAP (floating target)
correlated to having less data 	target)	EYEDIAP (floating target)
in those areas. Compared to 	target)	EYEDIAP (floating target)
the static model, the temporal 	target)	EYEDIAP (floating target)
model particularly benefits gaze targets 	target)	EYEDIAP (floating target)
from mid-range upwards. Its contribution 	target)	EYEDIAP (floating target)
is less clear for extreme 	target)	EYEDIAP (floating target)
targets, probably again due to 	target)	EYEDIAP (floating target)
data imbalance.  Finally, we evaluated the effect	target)	EYEDIAP (floating target)
 of different recurrent architectures for	target)	EYEDIAP (floating target)
 the temporal model. In particular	target)	EYEDIAP (floating target)
, we tested 1 (128 	target)	EYEDIAP (floating target)
units) and 2 (256-128 units) 	target)	EYEDIAP (floating target)
LSTM and GRU lay- ers, 	target)	EYEDIAP (floating target)
with 1 GRU layer obtaining 	target)	EYEDIAP (floating target)
slightly superior results (up to 0	target)	EYEDIAP (floating target)
.12◦). We also assessed the 	target)	EYEDIAP (floating target)
effect of sequence length fixing 	target)	EYEDIAP (floating target)
s in the range {4,7,10}, 	target)	EYEDIAP (floating target)
with s = 7 performing 	target)	EYEDIAP (floating target)
worse than the other two (	target)	EYEDIAP (floating target)
up to 0	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
14	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
10 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (floating target)
ESTIMATION  5 Conclusions In this work	target)	EYEDIAP (floating target)
, we studied the combination 	target)	EYEDIAP (floating target)
of full-face and eye images 	target)	EYEDIAP (floating target)
along with facial land- marks 	target)	EYEDIAP (floating target)
for person- and head pose-independent 	target)	EYEDIAP (floating target)
3D gaze estimation. Consequently, we 	target)	EYEDIAP (floating target)
pro- posed a multi-stream recurrent 	target)	EYEDIAP (floating target)
CNN network that leverages the 	target)	EYEDIAP (floating target)
sequential information of eye and 	target)	EYEDIAP (floating target)
head movements. Both static and 	target)	EYEDIAP (floating target)
temporal versions of our approach 	target)	EYEDIAP (floating target)
significantly outperform current state-of-the-art 3D 	target)	EYEDIAP (floating target)
gaze estimation methods on a 	target)	EYEDIAP (floating target)
wide range of head poses 	target)	EYEDIAP (floating target)
and gaze directions. We showed 	target)	EYEDIAP (floating target)
that adding geometry features to 	target)	EYEDIAP (floating target)
appearance-based methods has a regularizing 	target)	EYEDIAP (floating target)
effect on the accuracy. Adding 	target)	EYEDIAP (floating target)
sequential information further benefits the 	target)	EYEDIAP (floating target)
final performance compared to static-only 	target)	EYEDIAP (floating target)
input, especially from mid-range up- 	target)	EYEDIAP (floating target)
wards and in those cases 	target)	EYEDIAP (floating target)
where head motion is present. 	target)	EYEDIAP (floating target)
The effect in very extreme 	target)	EYEDIAP (floating target)
head poses is not clear 	target)	EYEDIAP (floating target)
due to data imbalance, suggesting 	target)	EYEDIAP (floating target)
the importance of learning from 	target)	EYEDIAP (floating target)
a con- tinuous, balanced dataset 	target)	EYEDIAP (floating target)
including all head poses and 	target)	EYEDIAP (floating target)
gaze directions of interest. To 	target)	EYEDIAP (floating target)
the best of our knowledge, 	target)	EYEDIAP (floating target)
this is the first attempt 	target)	EYEDIAP (floating target)
to exploit the temporal modality 	target)	EYEDIAP (floating target)
in the context of gaze 	target)	EYEDIAP (floating target)
estimation from remote cameras. As 	target)	EYEDIAP (floating target)
future work, we will further 	target)	EYEDIAP (floating target)
explore extracting meaningful temporal representations 	target)	EYEDIAP (floating target)
of gaze dynamics, considering 3DCNNs 	target)	EYEDIAP (floating target)
as well as the encoding 	target)	EYEDIAP (floating target)
of deep features around particular 	target)	EYEDIAP (floating target)
tracked face landmarks [14].  Acknowledgements This work has been	target)	EYEDIAP (floating target)
 partially supported by the Spanish	target)	EYEDIAP (floating target)
 project TIN2016-74946-P (MINECO/ FEDER, UE	target)	EYEDIAP (floating target)
), CERCA Programme / Generalitat 	target)	EYEDIAP (floating target)
de Catalunya, and the FP7 	target)	EYEDIAP (floating target)
people program (Marie Curie Actions), 	target)	EYEDIAP (floating target)
REA grant agreement no FP7-607139 (	target)	EYEDIAP (floating target)
iCARE - Improving Children Auditory 	target)	EYEDIAP (floating target)
REhabilitation). We gratefully acknowledge the 	target)	EYEDIAP (floating target)
support of NVIDIA Corporation with 	target)	EYEDIAP (floating target)
the donation of the GPU 	target)	EYEDIAP (floating target)
used for this research. Portions 	target)	EYEDIAP (floating target)
of the research in this 	target)	EYEDIAP (floating target)
pa- per used the EYEDIAP 	target)	EYEDIAP (floating target)
dataset made available by the 	target)	EYEDIAP (floating target)
Idiap Research Institute, Martigny, Switzerland.  References [1] Nicola C Anderson	target)	EYEDIAP (floating target)
, Evan F Risko, and 	target)	EYEDIAP (floating target)
Alan Kingstone. Motion influences gaze 	target)	EYEDIAP (floating target)
di-  rection discrimination and disambiguates contradictory	target)	EYEDIAP (floating target)
 luminance cues. Psychonomic bulletin	target)	EYEDIAP (floating target)
 & review, 23(3):817–823, 2016	target)	EYEDIAP (floating target)
.  [2] Shumeet Baluja and Dean	target)	EYEDIAP (floating target)
 Pomerleau. Non-intrusive gaze tracking using	target)	EYEDIAP (floating target)
 artificial neu- ral networks. In	target)	EYEDIAP (floating target)
 Advances in Neural Information Processing	target)	EYEDIAP (floating target)
 Systems, pages 753–760, 1994	target)	EYEDIAP (floating target)
.  [3] Adrian Bulat and Georgios	target)	EYEDIAP (floating target)
 Tzimiropoulos. How far are we	target)	EYEDIAP (floating target)
 from solving the 2d	target)	EYEDIAP (floating target)
 & 3d face alignment problem	target)	EYEDIAP (floating target)
? (and a dataset of 230,	target)	EYEDIAP (floating target)
000 3d facial landmarks). In 	target)	EYEDIAP (floating target)
Interna- tional Conference on Computer 	target)	EYEDIAP (floating target)
Vision, 2017.  [4] Haoping Deng and Wangjiang	target)	EYEDIAP (floating target)
 Zhu. Monocular free-head 3d gaze	target)	EYEDIAP (floating target)
 tracking with deep learning and	target)	EYEDIAP (floating target)
 geometry constraints. In Computer Vision	target)	EYEDIAP (floating target)
 (ICCV), 2017 IEEE Interna- tional	target)	EYEDIAP (floating target)
 Conference on, pages 3162–3171. IEEE	target)	EYEDIAP (floating target)
, 2017.  [5] Onur Ferhat and Fernando	target)	EYEDIAP (floating target)
 Vilariño. Low cost eye tracking	target)	EYEDIAP (floating target)
. Computational intelligence and neuroscience, 2016	target)	EYEDIAP (floating target)
:17, 2016.  Citation Citation {Jung, Lee, Yim	target)	EYEDIAP (floating target)
, Park, and Kim} 2015	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 11	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
6] Kenneth A Funes-Mora and 	target)	EYEDIAP (floating target)
Jean-Marc Odobez. Gaze estimation in 	target)	EYEDIAP (floating target)
the 3D space using RGB-D 	target)	EYEDIAP (floating target)
sensors. International Journal of Computer 	target)	EYEDIAP (floating target)
Vision, 118(2):194–216, 2016.  [7] Kenneth Alberto Funes Mora	target)	EYEDIAP (floating target)
, Florent Monay, and Jean-Marc 	target)	EYEDIAP (floating target)
Odobez. Eyediap: A database for 	target)	EYEDIAP (floating target)
the development and evaluation of 	target)	EYEDIAP (floating target)
gaze estimation algorithms from rgb 	target)	EYEDIAP (floating target)
and rgb-d cameras. In Proceedings 	target)	EYEDIAP (floating target)
of the ACM Symposium on 	target)	EYEDIAP (floating target)
Eye Tracking Research and Applications. 	target)	EYEDIAP (floating target)
ACM, March 2014. doi: 10.1145/2578153.2578190.  [8] Kenneth Alberto Funes Mora	target)	EYEDIAP (floating target)
, Florent Monay, and Jean-Marc 	target)	EYEDIAP (floating target)
Odobez. Eyediap: A database for 	target)	EYEDIAP (floating target)
the development and evaluation of 	target)	EYEDIAP (floating target)
gaze estimation algorithms from rgb 	target)	EYEDIAP (floating target)
and rgb-d cameras. In Proceedings 	target)	EYEDIAP (floating target)
of the Symposium on Eye 	target)	EYEDIAP (floating target)
Tracking Research and Applications, pages 255	target)	EYEDIAP (floating target)
–258. ACM, 2014.  [9] Quentin Guillon, Nouchine Hadjikhani	target)	EYEDIAP (floating target)
, Sophie Baduel, and Bernadette 	target)	EYEDIAP (floating target)
Rogé. Visual social attention in 	target)	EYEDIAP (floating target)
autism spectrum disorder: Insights from 	target)	EYEDIAP (floating target)
eye tracking studies. Neu- 	target)	EYEDIAP (floating target)
roscience & Biobehavioral Reviews, 42:279–297, 2014	target)	EYEDIAP (floating target)
.  [10] Dan Witzner Hansen and	target)	EYEDIAP (floating target)
 Qiang Ji. In the eye	target)	EYEDIAP (floating target)
 of the beholder: A survey	target)	EYEDIAP (floating target)
 of models for eyes and	target)	EYEDIAP (floating target)
 gaze. IEEE transactions on pattern	target)	EYEDIAP (floating target)
 analysis and machine intelligence, 32(3	target)	EYEDIAP (floating target)
): 478–500, 2010.  [11] Qiong Huang, Ashok Veeraraghavan	target)	EYEDIAP (floating target)
, and Ashutosh Sabharwal. Tabletgaze: 	target)	EYEDIAP (floating target)
dataset and analysis for unconstrained 	target)	EYEDIAP (floating target)
appearance-based gaze estimation in mobile 	target)	EYEDIAP (floating target)
tablets. Machine Vision and Applications, 28	target)	EYEDIAP (floating target)
(5-6):445–461, 2017.  [12] Robert JK Jacob and	target)	EYEDIAP (floating target)
 Keith S Karn. Eye tracking	target)	EYEDIAP (floating target)
 in human-computer interaction and usability	target)	EYEDIAP (floating target)
 research: Ready to deliver the	target)	EYEDIAP (floating target)
 promises. In The mind’s eye	target)	EYEDIAP (floating target)
, pages 573–605. Elsevier, 2003.  [13] László A Jeni and	target)	EYEDIAP (floating target)
 Jeffrey F Cohn. Person-independent 3d	target)	EYEDIAP (floating target)
 gaze estimation using face frontalization	target)	EYEDIAP (floating target)
. In Proceedings of the 	target)	EYEDIAP (floating target)
IEEE Conference on Computer Vision 	target)	EYEDIAP (floating target)
and Pattern Recognition Workshops, pages 87	target)	EYEDIAP (floating target)
–95, 2016.  [14] Heechul Jung, Sihaeng Lee	target)	EYEDIAP (floating target)
, Junho Yim, Sunjeong Park, 	target)	EYEDIAP (floating target)
and Junmo Kim. Joint fine- 	target)	EYEDIAP (floating target)
tuning in deep neural networks 	target)	EYEDIAP (floating target)
for facial expression recognition. In 	target)	EYEDIAP (floating target)
Computer Vision (ICCV), 2015 IEEE 	target)	EYEDIAP (floating target)
International Conference on, pages 2983–2991. 	target)	EYEDIAP (floating target)
IEEE, 2015.  [15] Anuradha Kar and Peter	target)	EYEDIAP (floating target)
 Corcoran. A review and analysis	target)	EYEDIAP (floating target)
 of eye-gaze estimation sys- tems	target)	EYEDIAP (floating target)
, algorithms and performance evaluation 	target)	EYEDIAP (floating target)
methods in consumer platforms. IEEE 	target)	EYEDIAP (floating target)
Access, 5:16495–16519, 2017.  [16] Kyle Krafka, Aditya Khosla	target)	EYEDIAP (floating target)
, Petr Kellnhofer, Harini Kannan, 	target)	EYEDIAP (floating target)
Suchendra Bhandarkar, Wojciech Matusik, and 	target)	EYEDIAP (floating target)
Antonio Torralba. Eye tracking for 	target)	EYEDIAP (floating target)
everyone. In Computer Vision and 	target)	EYEDIAP (floating target)
Pattern Recognition (CVPR), 2016 IEEE 	target)	EYEDIAP (floating target)
Conference on, pages 2176–2184. IEEE, 2016	target)	EYEDIAP (floating target)
.  [17] Simon P Liversedge and	target)	EYEDIAP (floating target)
 John M Findlay. Saccadic eye	target)	EYEDIAP (floating target)
 movements and cognition. Trends in	target)	EYEDIAP (floating target)
 cognitive sciences, 4(1):6–14, 2000	target)	EYEDIAP (floating target)
.  [18] Feng Lu, Takahiro Okabe	target)	EYEDIAP (floating target)
, Yusuke Sugano, and Yoichi 	target)	EYEDIAP (floating target)
Sato. A head pose-free approach 	target)	EYEDIAP (floating target)
for appearance-based gaze estimation. In 	target)	EYEDIAP (floating target)
BMVC, pages 1–11, 2011	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
12 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (floating target)
ESTIMATION  [19] Feng Lu, Yusuke Sugano	target)	EYEDIAP (floating target)
, Takahiro Okabe, and Yoichi 	target)	EYEDIAP (floating target)
Sato. Inferring human gaze from 	target)	EYEDIAP (floating target)
appearance via adaptive linear regression. 	target)	EYEDIAP (floating target)
In Computer Vision (ICCV), 2011 	target)	EYEDIAP (floating target)
IEEE International Conference on, pages 153	target)	EYEDIAP (floating target)
–160. IEEE, 2011.  [20] Päivi Majaranta and Andreas	target)	EYEDIAP (floating target)
 Bulling. Eye tracking and eye-based	target)	EYEDIAP (floating target)
 human–computer interaction. In Advances in	target)	EYEDIAP (floating target)
 physiological computing, pages 39–65. Springer	target)	EYEDIAP (floating target)
, 2014.  [21] Kenneth Alberto Funes Mora	target)	EYEDIAP (floating target)
 and Jean-Marc Odobez. Gaze estimation	target)	EYEDIAP (floating target)
 from multi- modal kinect data	target)	EYEDIAP (floating target)
. In Computer Vision and 	target)	EYEDIAP (floating target)
Pattern Recognition Workshops (CVPRW), 2012 	target)	EYEDIAP (floating target)
IEEE Computer Society Conference on, 	target)	EYEDIAP (floating target)
pages 25–30. IEEE, 2012.  [22] Carlos Hitoshi Morimoto, Arnon	target)	EYEDIAP (floating target)
 Amir, and Myron Flickner. Detecting	target)	EYEDIAP (floating target)
 eye position and gaze from	target)	EYEDIAP (floating target)
 a single camera and 2	target)	EYEDIAP (floating target)
 light sources. In Pattern Recognition	target)	EYEDIAP (floating target)
, 2002. Proceedings. 16th International 	target)	EYEDIAP (floating target)
Conference on, volume 4, pages 314	target)	EYEDIAP (floating target)
–317. IEEE, 2002.  [23] IMO MSC. Circ. 982	target)	EYEDIAP (floating target)
 (2000) guidelines on ergonomic criteria	target)	EYEDIAP (floating target)
 for bridge equipment and layout	target)	EYEDIAP (floating target)
.  [24] Alejandro Newell, Kaiyu Yang	target)	EYEDIAP (floating target)
, and Jia Deng. Stacked 	target)	EYEDIAP (floating target)
hourglass networks for hu- man 	target)	EYEDIAP (floating target)
pose estimation. In European Conference 	target)	EYEDIAP (floating target)
on Computer Vision, pages 483–499. 	target)	EYEDIAP (floating target)
Springer, 2016.  [25] Yasuhiro Ono, Takahiro Okabe	target)	EYEDIAP (floating target)
, and Yoichi Sato. Gaze 	target)	EYEDIAP (floating target)
estimation from low resolution images. 	target)	EYEDIAP (floating target)
In Pacific-Rim Symposium on Image 	target)	EYEDIAP (floating target)
and Video Technology, pages 178–188. 	target)	EYEDIAP (floating target)
Springer, 2006.  [26] Cristina Palmero, Elisabeth A	target)	EYEDIAP (floating target)
. van Dam, Sergio Escalera, 	target)	EYEDIAP (floating target)
Mike Kelia, Guido F. Lichtert, 	target)	EYEDIAP (floating target)
Lucas P.J.J Noldus, Andrew J. 	target)	EYEDIAP (floating target)
Spink, and Astrid van Wieringen. 	target)	EYEDIAP (floating target)
Automatic mutual gaze detection in 	target)	EYEDIAP (floating target)
face-to-face dyadic interaction videos. In 	target)	EYEDIAP (floating target)
Proceedings of Measuring Behavior, pages 158	target)	EYEDIAP (floating target)
–163, 2018.  [27] Omkar M. Parkhi, Andrea	target)	EYEDIAP (floating target)
 Vedaldi, and Andrew Zisserman. Deep	target)	EYEDIAP (floating target)
 face recognition. In British Machine	target)	EYEDIAP (floating target)
 Vision Conference, 2015	target)	EYEDIAP (floating target)
.  [28] Derek R Rutter and	target)	EYEDIAP (floating target)
 Kevin Durkin. Turn-taking in mother–infant	target)	EYEDIAP (floating target)
 interaction: An exam- ination of	target)	EYEDIAP (floating target)
 vocalizations and gaze. Developmental psychology	target)	EYEDIAP (floating target)
, 23(1):54, 1987.  [29] Brian A Smith, Qi	target)	EYEDIAP (floating target)
 Yin, Steven K Feiner, and	target)	EYEDIAP (floating target)
 Shree K Nayar. Gaze locking	target)	EYEDIAP (floating target)
: passive eye contact detection 	target)	EYEDIAP (floating target)
for human-object interaction. In Proceedings 	target)	EYEDIAP (floating target)
of the 26th annual ACM 	target)	EYEDIAP (floating target)
symposium on User interface software 	target)	EYEDIAP (floating target)
and technology, pages 271–280. ACM, 2013	target)	EYEDIAP (floating target)
.  [30] Yusuke Sugano, Yasuyuki Matsushita	target)	EYEDIAP (floating target)
, and Yoichi Sato. Appearance-based 	target)	EYEDIAP (floating target)
gaze es- timation using visual 	target)	EYEDIAP (floating target)
saliency. IEEE transactions on pattern 	target)	EYEDIAP (floating target)
analysis and machine intelligence, 35(2):329–341, 2013	target)	EYEDIAP (floating target)
.  [31] Yusuke Sugano, Yasuyuki Matsushita	target)	EYEDIAP (floating target)
, and Yoichi Sato. Learning-by-synthesis 	target)	EYEDIAP (floating target)
for appearance-based 3d gaze estimation. 	target)	EYEDIAP (floating target)
In Computer Vision and Pattern 	target)	EYEDIAP (floating target)
Recognition (CVPR), 2014 IEEE Conference 	target)	EYEDIAP (floating target)
on, pages 1821–1828. IEEE, 2014.  [32] Kar-Han Tan, David J	target)	EYEDIAP (floating target)
 Kriegman, and Narendra Ahuja. Appearance-based	target)	EYEDIAP (floating target)
 eye gaze es- timation. In	target)	EYEDIAP (floating target)
 Applications of Computer Vision, 2002.(WACV	target)	EYEDIAP (floating target)
 2002). Proceedings. Sixth IEEE Workshop	target)	EYEDIAP (floating target)
 on, pages 191–195. IEEE, 2002	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 13	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
33] Ronda Venkateswarlu et al. 	target)	EYEDIAP (floating target)
Eye gaze estimation from a 	target)	EYEDIAP (floating target)
single image of one eye. 	target)	EYEDIAP (floating target)
In Computer Vision, 2003. Proceedings. 	target)	EYEDIAP (floating target)
Ninth IEEE International Conference on, 	target)	EYEDIAP (floating target)
pages 136–143. IEEE, 2003.  [34] Kang Wang and Qiang	target)	EYEDIAP (floating target)
 Ji. Real time eye gaze	target)	EYEDIAP (floating target)
 tracking with 3d deformable eye-face	target)	EYEDIAP (floating target)
 model. In Proceedings of the	target)	EYEDIAP (floating target)
 IEEE Conference on Computer Vision	target)	EYEDIAP (floating target)
 and Pattern Recog- nition, pages	target)	EYEDIAP (floating target)
 1003–1011, 2017	target)	EYEDIAP (floating target)
.  [35] Oliver Williams, Andrew Blake	target)	EYEDIAP (floating target)
, and Roberto Cipolla. Sparse 	target)	EYEDIAP (floating target)
and semi-supervised visual mapping with 	target)	EYEDIAP (floating target)
the sˆ 3gp. In Computer 	target)	EYEDIAP (floating target)
Vision and Pattern Recognition, 2006 	target)	EYEDIAP (floating target)
IEEE Computer Society Conference on, 	target)	EYEDIAP (floating target)
volume 1, pages 230–237. IEEE, 2006	target)	EYEDIAP (floating target)
.  [36] William Hyde Wollaston et	target)	EYEDIAP (floating target)
 al. Xiii. on the apparent	target)	EYEDIAP (floating target)
 direction of eyes in a	target)	EYEDIAP (floating target)
 portrait. Philosophical Transactions of the	target)	EYEDIAP (floating target)
 Royal Society of London, 114:247–256	target)	EYEDIAP (floating target)
, 1824.  [37] Erroll Wood and Andreas	target)	EYEDIAP (floating target)
 Bulling. Eyetab: Model-based gaze estimation	target)	EYEDIAP (floating target)
 on unmodi- fied tablet computers	target)	EYEDIAP (floating target)
. In Proceedings of the 	target)	EYEDIAP (floating target)
Symposium on Eye Tracking Research 	target)	EYEDIAP (floating target)
and Applications, pages 207–210. ACM, 2014	target)	EYEDIAP (floating target)
.  [38] Erroll Wood, Tadas Baltrusaitis	target)	EYEDIAP (floating target)
, Xucong Zhang, Yusuke Sugano, 	target)	EYEDIAP (floating target)
Peter Robinson, and Andreas Bulling. 	target)	EYEDIAP (floating target)
Rendering of eyes for eye-shape 	target)	EYEDIAP (floating target)
registration and gaze estimation. In 	target)	EYEDIAP (floating target)
Proceedings of the IEEE International 	target)	EYEDIAP (floating target)
Conference on Computer Vision, pages 3756	target)	EYEDIAP (floating target)
– 3764, 2015.  [39] Erroll Wood, Tadas Baltrušaitis	target)	EYEDIAP (floating target)
, Louis-Philippe Morency, Peter Robinson, 	target)	EYEDIAP (floating target)
and Andreas Bulling. A 3d 	target)	EYEDIAP (floating target)
morphable eye region model for 	target)	EYEDIAP (floating target)
gaze estimation. In European Confer- 	target)	EYEDIAP (floating target)
ence on Computer Vision, pages 297	target)	EYEDIAP (floating target)
–313. Springer, 2016.  [40] Erroll Wood, Tadas Baltrušaitis	target)	EYEDIAP (floating target)
, Louis-Philippe Morency, Peter Robinson, 	target)	EYEDIAP (floating target)
and Andreas Bulling. Learning an 	target)	EYEDIAP (floating target)
appearance-based gaze estimator from one 	target)	EYEDIAP (floating target)
million synthesised images. In Proceedings 	target)	EYEDIAP (floating target)
of the Ninth Biennial ACM 	target)	EYEDIAP (floating target)
Symposium on Eye Tracking Re- 	target)	EYEDIAP (floating target)
search & Applications, pages 131–138. 	target)	EYEDIAP (floating target)
ACM, 2016.  [41] Dong Hyun Yoo and	target)	EYEDIAP (floating target)
 Myung Jin Chung. A novel	target)	EYEDIAP (floating target)
 non-intrusive eye gaze estimation using	target)	EYEDIAP (floating target)
 cross-ratio under large head motion	target)	EYEDIAP (floating target)
. Computer Vision and Image 	target)	EYEDIAP (floating target)
Understanding, 98(1):25–51, 2005.  [42] Xucong Zhang, Yusuke Sugano	target)	EYEDIAP (floating target)
, Mario Fritz, and Andreas 	target)	EYEDIAP (floating target)
Bulling. Appearance-based gaze estimation in 	target)	EYEDIAP (floating target)
the wild. In Proceedings of 	target)	EYEDIAP (floating target)
the IEEE Conference on Computer 	target)	EYEDIAP (floating target)
Vision and Pattern Recognition, pages 4511	target)	EYEDIAP (floating target)
–4520, 2015.  [43] Xucong Zhang, Yusuke Sugano	target)	EYEDIAP (floating target)
, Mario Fritz, and Andreas 	target)	EYEDIAP (floating target)
Bulling. It’s written all over 	target)	EYEDIAP (floating target)
your face: Full-face appearance-based gaze 	target)	EYEDIAP (floating target)
estimation. In Proc. IEEE International 	target)	EYEDIAP (floating target)
Conference on Computer Vision and 	target)	EYEDIAP (floating target)
Pattern Recognition Workshops (CVPRW), 2017	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
state of the art on EYEDIAP dataset, further improved by 4	EYEDIAP	EYEDIAP (floating target)
of our solution on the EYEDIAP dataset [7] in a wide	EYEDIAP	EYEDIAP (floating target)
VGA videos from the publicly-available EYEDIAP dataset [7] to perform the	EYEDIAP	EYEDIAP (floating target)
FT-S scenario are provided by EYEDIAP dataset. MPIIGaze:. State-of-the-art full-face 3D	EYEDIAP	EYEDIAP (floating target)
fine-tuned it with the filtered EYEDIAP subsets using our training parameters	EYEDIAP	EYEDIAP (floating target)
this pa- per used the EYEDIAP dataset made available by the	EYEDIAP	EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 1		EYEDIAP (floating target)
		EYEDIAP (floating target)
Recurrent CNN for 3D Gaze 		EYEDIAP (floating target)
Estimation using Appearance and Shape 		EYEDIAP (floating target)
Cues  Cristina Palmero1,2		EYEDIAP (floating target)
		EYEDIAP (floating target)
crpalmec7@alumnes.ub.edu  1 Dept. Mathematics and Informatics		EYEDIAP (floating target)
 Universitat de Barcelona, Spain		EYEDIAP (floating target)
		EYEDIAP (floating target)
Javier Selva1  javier.selva.castello@est.fib.upc.edu		EYEDIAP (floating target)
		EYEDIAP (floating target)
2 Computer Vision Center Campus 		EYEDIAP (floating target)
UAB, Bellaterra, Spain  Mohammad Ali Bagheri3,4		EYEDIAP (floating target)
		EYEDIAP (floating target)
mohammadali.bagheri@ucalgary.ca  3 Dept. Electrical and Computer		EYEDIAP (floating target)
 Eng. University of Calgary, Canada		EYEDIAP (floating target)
		EYEDIAP (floating target)
Sergio Escalera1,2  sergio@maia.ub.es		EYEDIAP (floating target)
		EYEDIAP (floating target)
4 Dept. Engineering University of 		EYEDIAP (floating target)
Larestan, Iran  Abstract		EYEDIAP (floating target)
		EYEDIAP (floating target)
Gaze behavior is an important 		EYEDIAP (floating target)
non-verbal cue in social signal 		EYEDIAP (floating target)
processing and human- computer interaction. 		EYEDIAP (floating target)
In this paper, we tackle 		EYEDIAP (floating target)
the problem of person- and 		EYEDIAP (floating target)
head pose- independent 3D gaze 		EYEDIAP (floating target)
estimation from remote cameras, using 		EYEDIAP (floating target)
a multi-modal recurrent convolutional neural 		EYEDIAP (floating target)
network (CNN). We propose to 		EYEDIAP (floating target)
combine face, eyes region, and 		EYEDIAP (floating target)
face landmarks as individual streams 		EYEDIAP (floating target)
in a CNN to estimate 		EYEDIAP (floating target)
gaze in still images. Then, 		EYEDIAP (floating target)
we exploit the dynamic nature 		EYEDIAP (floating target)
of gaze by feeding the 		EYEDIAP (floating target)
learned features of all the 		EYEDIAP (floating target)
frames in a sequence to 		EYEDIAP (floating target)
a many-to-one recurrent module that 		EYEDIAP (floating target)
predicts the 3D gaze vector 		EYEDIAP (floating target)
of the last frame. Our 		EYEDIAP (floating target)
multi-modal static solution is evaluated 		EYEDIAP (floating target)
on a wide range of 		EYEDIAP (floating target)
head poses and gaze directions, 		EYEDIAP (floating target)
achieving a significant improvement of 14		EYEDIAP (floating target)
.6% over the state of 		EYEDIAP (floating target)
the art on EYEDIAP dataset, 		EYEDIAP (floating target)
further improved by 4% when 		EYEDIAP (floating target)
the temporal modality is included.  1 Introduction Eyes and their		EYEDIAP (floating target)
 movements are considered an important		EYEDIAP (floating target)
 cue in non-verbal behavior analysis		EYEDIAP (floating target)
, being involved in many 		EYEDIAP (floating target)
cognitive processes and reflecting our 		EYEDIAP (floating target)
internal state [17]. More specifically, 		EYEDIAP (floating target)
eye gaze behavior, as an 		EYEDIAP (floating target)
indicator of human visual attention, 		EYEDIAP (floating target)
has been widely studied to 		EYEDIAP (floating target)
assess communication skills [28] and 		EYEDIAP (floating target)
to identify possible behavioral 		EYEDIAP (floating target)
disorders [9]. Therefore, gaze estimation 		EYEDIAP (floating target)
has become an established line 		EYEDIAP (floating target)
of research in computer vision, 		EYEDIAP (floating target)
being a key feature in 		EYEDIAP (floating target)
human-computer interaction (HCI) and usability 		EYEDIAP (floating target)
research [12, 20].  Recent gaze estimation research has		EYEDIAP (floating target)
 focused on facilitating its use		EYEDIAP (floating target)
 in general everyday applications under		EYEDIAP (floating target)
 real-world conditions, using off-the-shelf remote		EYEDIAP (floating target)
 RGB cameras and re- moving		EYEDIAP (floating target)
 the need of personal calibration		EYEDIAP (floating target)
 [26]. In this setting, appearance-based		EYEDIAP (floating target)
 methods, which learn a mapping		EYEDIAP (floating target)
 from images to gaze directions		EYEDIAP (floating target)
, are the preferred 		EYEDIAP (floating target)
choice [25]. How- ever, they 		EYEDIAP (floating target)
need large amounts of training 		EYEDIAP (floating target)
data to be able to 		EYEDIAP (floating target)
generalize well to in-the-wild situations, 		EYEDIAP (floating target)
which are characterized by significant 		EYEDIAP (floating target)
variability in head poses, face 		EYEDIAP (floating target)
appearances and lighting conditions. In 		EYEDIAP (floating target)
recent years, CNNs have been 		EYEDIAP (floating target)
reported to outperform classical methods. 		EYEDIAP (floating target)
However, most existing approaches have 		EYEDIAP (floating target)
only been tested in restricted 		EYEDIAP (floating target)
HCI tasks,  c© 2018. The copyright of		EYEDIAP (floating target)
 this document resides with its		EYEDIAP (floating target)
 authors. It may be distributed		EYEDIAP (floating target)
 unchanged freely in print or		EYEDIAP (floating target)
 electronic forms		EYEDIAP (floating target)
.  ar X		EYEDIAP (floating target)
		EYEDIAP (floating target)
iv :1  80 5		EYEDIAP (floating target)
.  03 06		EYEDIAP (floating target)
		EYEDIAP (floating target)
4v 3		EYEDIAP (floating target)
		EYEDIAP (floating target)
cs  .C V		EYEDIAP (floating target)
		EYEDIAP (floating target)
1  7		EYEDIAP (floating target)
		EYEDIAP (floating target)
Se  p		EYEDIAP (floating target)
		EYEDIAP (floating target)
20  18		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Liversedge and Findlay} 2000		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Rutter and Durkin} 1987		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Guillon, Hadjikhani, Baduel, 		EYEDIAP (floating target)
and Rog{é}} 2014  Citation Citation {Jacob and Karn		EYEDIAP (floating target)
} 2003  Citation Citation {Majaranta and Bulling		EYEDIAP (floating target)
} 2014  Citation Citation {Palmero, van Dam		EYEDIAP (floating target)
, Escalera, Kelia, Lichtert, Noldus, 		EYEDIAP (floating target)
Spink, and van Wieringen} 2018  Citation Citation {Ono, Okabe, and		EYEDIAP (floating target)
 Sato} 2006		EYEDIAP (floating target)
		EYEDIAP (floating target)
2 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (floating target)
ESTIMATION  Method 3D gaze direction		EYEDIAP (floating target)
		EYEDIAP (floating target)
Unrestricted gaze target  Full face		EYEDIAP (floating target)
		EYEDIAP (floating target)
Eye region  Facial landmarks		EYEDIAP (floating target)
		EYEDIAP (floating target)
Sequential information  Zhang et al. (1) [42		EYEDIAP (floating target)
] 3 7 7 3 7 7 Krafka et al. [16		EYEDIAP (floating target)
] 7 7 3 3 7 7 Zhang et al. (2		EYEDIAP (floating target)
) [43] 3 7 3 7 7 7 Deng and Zhu		EYEDIAP (floating target)
 [4] 3 3 3 3		EYEDIAP (floating target)
 7 7 Ours 3 3		EYEDIAP (floating target)
 3 3 3 3		EYEDIAP (floating target)
		EYEDIAP (floating target)
Table 1: Characteristics of recent 		EYEDIAP (floating target)
related work on person- and 		EYEDIAP (floating target)
head pose-independent appearance-based gaze estimation 		EYEDIAP (floating target)
methods using CNNs.  where users look at the		EYEDIAP (floating target)
 screen or mobile phone, showing		EYEDIAP (floating target)
 a low head pose variability		EYEDIAP (floating target)
. It is yet unclear 		EYEDIAP (floating target)
how these methods would perform 		EYEDIAP (floating target)
in a wider range of 		EYEDIAP (floating target)
head poses.  On a different note, until		EYEDIAP (floating target)
 very recently, the majority of		EYEDIAP (floating target)
 methods only used static eye		EYEDIAP (floating target)
 region appearance as input. State-of-the-art		EYEDIAP (floating target)
 approaches have demonstrated that using		EYEDIAP (floating target)
 the face along with a		EYEDIAP (floating target)
 higher resolution image of the		EYEDIAP (floating target)
 eyes [16], or even just		EYEDIAP (floating target)
 the face itself [43], increases		EYEDIAP (floating target)
 performance. Indeed, the whole-face image		EYEDIAP (floating target)
 encodes more information than eyes		EYEDIAP (floating target)
 alone, such as illumination and		EYEDIAP (floating target)
 head pose. Nevertheless, gaze behavior		EYEDIAP (floating target)
 is not static. Eye and		EYEDIAP (floating target)
 head movements allow us to		EYEDIAP (floating target)
 direct our gaze to target		EYEDIAP (floating target)
 locations of interest. It has		EYEDIAP (floating target)
 been demonstrated that humans can		EYEDIAP (floating target)
 better predict gaze when being		EYEDIAP (floating target)
 shown image sequences of other		EYEDIAP (floating target)
 people moving their eyes [1		EYEDIAP (floating target)
]. However, it is still 		EYEDIAP (floating target)
an open question whether this 		EYEDIAP (floating target)
se- quential information can increase 		EYEDIAP (floating target)
the performance of automatic methods.  In this work, we show		EYEDIAP (floating target)
 that the combination of multiple		EYEDIAP (floating target)
 cues benefits the gaze estimation		EYEDIAP (floating target)
 task. In particular, we use		EYEDIAP (floating target)
 face, eye region and facial		EYEDIAP (floating target)
 landmarks from still images. Facial		EYEDIAP (floating target)
 landmarks model the global shape		EYEDIAP (floating target)
 of the face and come		EYEDIAP (floating target)
 at no cost, since face		EYEDIAP (floating target)
 alignment is a common pre-processing		EYEDIAP (floating target)
 step in many facial image		EYEDIAP (floating target)
 analysis approaches. Furthermore, we present		EYEDIAP (floating target)
 a subject-independent, free-head recurrent 3D		EYEDIAP (floating target)
 gaze regression network to leverage		EYEDIAP (floating target)
 the temporal information of image		EYEDIAP (floating target)
 sequences. The static streams of		EYEDIAP (floating target)
 each frame are combined in		EYEDIAP (floating target)
 a late-fusion fashion using a		EYEDIAP (floating target)
 multi-stream CNN. Then, all feature		EYEDIAP (floating target)
 vectors are input to a		EYEDIAP (floating target)
 many-to-one recurrent module that predicts		EYEDIAP (floating target)
 the gaze vector of the		EYEDIAP (floating target)
 last sequence frame		EYEDIAP (floating target)
.  In summary, our contributions are		EYEDIAP (floating target)
 two-fold. First, we present a		EYEDIAP (floating target)
 Recurrent-CNN net- work architecture that		EYEDIAP (floating target)
 combines appearance, shape and temporal		EYEDIAP (floating target)
 information for 3D gaze estimation		EYEDIAP (floating target)
. Second, we test static 		EYEDIAP (floating target)
and temporal versions of our 		EYEDIAP (floating target)
solution on the EYEDIAP 		EYEDIAP (floating target)
dataset [7] in a wide 		EYEDIAP (floating target)
range of head poses and 		EYEDIAP (floating target)
gaze directions, showing consistent perfor- 		EYEDIAP (floating target)
mance improvements compared to related 		EYEDIAP (floating target)
appearance-based methods. To the best 		EYEDIAP (floating target)
of our knowledge, this is 		EYEDIAP (floating target)
the first third-person, remote camera-based 		EYEDIAP (floating target)
approach that uses tempo- ral 		EYEDIAP (floating target)
information for this task. Table 1 outlines our main method characteristics		EYEDIAP (floating target)
 compared to related work. Models		EYEDIAP (floating target)
 and code are publicly available		EYEDIAP (floating target)
 at https://github.com/ crisie/RecurrentGaze		EYEDIAP (floating target)
.  2 Related work Gaze estimation		EYEDIAP (floating target)
 methods are typically categorized as		EYEDIAP (floating target)
 model-based or appearance-based [5, 10		EYEDIAP (floating target)
, 15]. Model-based approaches use 		EYEDIAP (floating target)
a geometric model of the 		EYEDIAP (floating target)
eye, usually requir- ing either 		EYEDIAP (floating target)
high resolution images or a 		EYEDIAP (floating target)
person-specific calibration stage to estimate 		EYEDIAP (floating target)
personal eye parameters [22, 33, 34, 37, 41]. In contrast, appearance-based		EYEDIAP (floating target)
 methods learn a di- rect		EYEDIAP (floating target)
 mapping from intensity images or		EYEDIAP (floating target)
 extracted eye features to gaze		EYEDIAP (floating target)
 directions, thus being		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Zhang, Sugano, Fritz, 		EYEDIAP (floating target)
and Bulling} 2015  Citation Citation {Krafka, Khosla, Kellnhofer		EYEDIAP (floating target)
, Kannan, Bhandarkar, Matusik, and 		EYEDIAP (floating target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (floating target)
, and Bulling} 2017  Citation Citation {Deng and Zhu		EYEDIAP (floating target)
} 2017  Citation Citation {Krafka, Khosla, Kellnhofer		EYEDIAP (floating target)
, Kannan, Bhandarkar, Matusik, and 		EYEDIAP (floating target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (floating target)
, and Bulling} 2017  Citation Citation {Anderson, Risko, and		EYEDIAP (floating target)
 Kingstone} 2016		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Funesprotect unhbox voidb@x 		EYEDIAP (floating target)
penalty @M  {}Mora, Monay, and Odobez} 2014		EYEDIAP (floating target)
{}  Citation Citation {Ferhat and Vilari{ñ}o		EYEDIAP (floating target)
} 2016  Citation Citation {Hansen and Ji		EYEDIAP (floating target)
} 2010  Citation Citation {Kar and Corcoran		EYEDIAP (floating target)
} 2017  Citation Citation {Morimoto, Amir, and		EYEDIAP (floating target)
 Flickner} 2002		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Venkateswarlu etprotect unhbox 		EYEDIAP (floating target)
voidb@x penalty @M  {}al.} 2003		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Wang and Ji} 2017		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Wood and Bulling} 2014		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Yoo and Chung} 2005		EYEDIAP (floating target)
		EYEDIAP (floating target)
https://github.com/crisie/RecurrentGaze 		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 3		EYEDIAP (floating target)
		EYEDIAP (floating target)
potentially applicable to relatively low 		EYEDIAP (floating target)
resolution images and mid-distance scenarios. 		EYEDIAP (floating target)
Dif- ferent mapping functions have 		EYEDIAP (floating target)
been explored, such as neural 		EYEDIAP (floating target)
networks [2], adaptive linear regression (		EYEDIAP (floating target)
ALR) [19], local interpolation [32], 		EYEDIAP (floating target)
gaussian processes [30, 35], random 		EYEDIAP (floating target)
forests [11, 31], or k-nearest 		EYEDIAP (floating target)
neighbors [40]. Main challenges of 		EYEDIAP (floating target)
appearance-based methods for 3D gaze 		EYEDIAP (floating target)
estimation are head pose, illumination 		EYEDIAP (floating target)
and subject invariance without user-specific 		EYEDIAP (floating target)
calibration. To handle these issues, 		EYEDIAP (floating target)
some works proposed compensation 		EYEDIAP (floating target)
methods [18] and warping strategies 		EYEDIAP (floating target)
that synthesize a canonical, frontal 		EYEDIAP (floating target)
looking view of the 		EYEDIAP (floating target)
face [6, 13, 21]. Hybrid 		EYEDIAP (floating target)
approaches based on analysis-by-synthesis have 		EYEDIAP (floating target)
also been evaluated [39].  Currently, data-driven methods are considered		EYEDIAP (floating target)
 the state of the art		EYEDIAP (floating target)
 for person- and head pose-independent		EYEDIAP (floating target)
 appearance-based gaze estimation. Consequently, a		EYEDIAP (floating target)
 number of gaze es- timation		EYEDIAP (floating target)
 datasets have been introduced in		EYEDIAP (floating target)
 recent years, either in controlled		EYEDIAP (floating target)
 [29] or semi- controlled settings		EYEDIAP (floating target)
 [8], in the wild [16		EYEDIAP (floating target)
, 42], or consisting of 		EYEDIAP (floating target)
synthetic data [31, 38, 40]. 		EYEDIAP (floating target)
Zhang et al. [42] showed 		EYEDIAP (floating target)
that CNNs can outperform other 		EYEDIAP (floating target)
mapping methods, using a multi- 		EYEDIAP (floating target)
modal CNN to learn the 		EYEDIAP (floating target)
mapping from 3D head poses 		EYEDIAP (floating target)
and eye images to 3D 		EYEDIAP (floating target)
gaze directions. Krafka et 		EYEDIAP (floating target)
al. [16] proposed a multi-stream 		EYEDIAP (floating target)
CNN for 2D gaze estimation, 		EYEDIAP (floating target)
using individual eye, whole-face image 		EYEDIAP (floating target)
and the face grid as 		EYEDIAP (floating target)
input. As this method was 		EYEDIAP (floating target)
limited to 2D screen mapping, 		EYEDIAP (floating target)
Zhang et al. [43] later 		EYEDIAP (floating target)
explored the potential of just 		EYEDIAP (floating target)
using whole-face images as input 		EYEDIAP (floating target)
to estimate 3D gaze directions. 		EYEDIAP (floating target)
Using a spatial weights CNN, 		EYEDIAP (floating target)
they demonstrated their method to 		EYEDIAP (floating target)
be more robust to facial 		EYEDIAP (floating target)
appearance variation caused by head 		EYEDIAP (floating target)
pose and illumina- tion than 		EYEDIAP (floating target)
eye-only methods. While the method 		EYEDIAP (floating target)
was evaluated in the wild, 		EYEDIAP (floating target)
the subjects were only interacting 		EYEDIAP (floating target)
with a mobile device, thus 		EYEDIAP (floating target)
restricting the head pose range. 		EYEDIAP (floating target)
Deng and Zhu [4] presented 		EYEDIAP (floating target)
a two-stream CNN to disjointly 		EYEDIAP (floating target)
model head pose from face 		EYEDIAP (floating target)
images and eye- ball movement 		EYEDIAP (floating target)
from eye region images. Both 		EYEDIAP (floating target)
were then aggregated into 3D 		EYEDIAP (floating target)
gaze direction using a gaze 		EYEDIAP (floating target)
transform layer. The decomposition was 		EYEDIAP (floating target)
aimed to avoid head-correlation over- 		EYEDIAP (floating target)
fitting of previous data-driven approaches. 		EYEDIAP (floating target)
They evaluated their approach in 		EYEDIAP (floating target)
the wild with a wider 		EYEDIAP (floating target)
range of head poses, obtaining 		EYEDIAP (floating target)
better performance than previous eye-based 		EYEDIAP (floating target)
methods. However, they did not 		EYEDIAP (floating target)
test it on public annotated 		EYEDIAP (floating target)
benchmark datasets.  In this paper, we propose		EYEDIAP (floating target)
 a multi-stream recurrent CNN network		EYEDIAP (floating target)
 for person- and head pose-independent		EYEDIAP (floating target)
 3D gaze estimation for a		EYEDIAP (floating target)
 mid-distance scenario. We evaluate it		EYEDIAP (floating target)
 on a wider range of		EYEDIAP (floating target)
 head poses and gaze directions		EYEDIAP (floating target)
 than screen-targeted approaches. As opposed		EYEDIAP (floating target)
 to previous methods, we also		EYEDIAP (floating target)
 rely on temporal information inherent		EYEDIAP (floating target)
 in sequential data		EYEDIAP (floating target)
.  3 Methodology		EYEDIAP (floating target)
		EYEDIAP (floating target)
In this section, we present 		EYEDIAP (floating target)
our approach for 3D gaze 		EYEDIAP (floating target)
regression based on appearance and 		EYEDIAP (floating target)
shape cues for still images 		EYEDIAP (floating target)
and image sequences. First, we 		EYEDIAP (floating target)
introduce the data modalities and 		EYEDIAP (floating target)
formulate the problem. Then, we 		EYEDIAP (floating target)
detail the normalization procedure prior 		EYEDIAP (floating target)
to the regression stage. Finally, 		EYEDIAP (floating target)
we explain the global network 		EYEDIAP (floating target)
topology as well as the 		EYEDIAP (floating target)
implementation details. An overview of 		EYEDIAP (floating target)
the system architecture is depicted 		EYEDIAP (floating target)
in Figure 1.  3.1 Multi-modal gaze regression		EYEDIAP (floating target)
		EYEDIAP (floating target)
Let us represent gaze direction 		EYEDIAP (floating target)
as a 3D unit vector 		EYEDIAP (floating target)
g = [gx,gy,gz]T ∈R3 in 		EYEDIAP (floating target)
the Camera Coor- dinate System (		EYEDIAP (floating target)
CCS), whose origin is the 		EYEDIAP (floating target)
central point between eyeball centers. 		EYEDIAP (floating target)
Assuming a calibrated camera, and 		EYEDIAP (floating target)
a known head position and 		EYEDIAP (floating target)
orientation, our goal is to 		EYEDIAP (floating target)
estimate g from a sequence 		EYEDIAP (floating target)
of images {I(i) | 		EYEDIAP (floating target)
I ∈ RW×H×3} as a 		EYEDIAP (floating target)
regression problem.  Citation Citation {Baluja and Pomerleau		EYEDIAP (floating target)
} 1994  Citation Citation {Lu, Sugano, Okabe		EYEDIAP (floating target)
, and Sato} 2011{}  Citation Citation {Tan, Kriegman, and		EYEDIAP (floating target)
 Ahuja} 2002		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Sugano, Matsushita, and 		EYEDIAP (floating target)
Sato} 2013  Citation Citation {Williams, Blake, and		EYEDIAP (floating target)
 Cipolla} 2006		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Huang, Veeraraghavan, and 		EYEDIAP (floating target)
Sabharwal} 2017  Citation Citation {Sugano, Matsushita, and		EYEDIAP (floating target)
 Sato} 2014		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Wood, Baltru{²}aitis, Morency, 		EYEDIAP (floating target)
Robinson, and Bulling} 2016{}  Citation Citation {Lu, Okabe, Sugano		EYEDIAP (floating target)
, and Sato} 2011{}  Citation Citation {Funes-Mora and Odobez		EYEDIAP (floating target)
} 2016  Citation Citation {Jeni and Cohn		EYEDIAP (floating target)
} 2016  Citation Citation {Mora and Odobez		EYEDIAP (floating target)
} 2012  Citation Citation {Wood, Baltru{²}aitis, Morency		EYEDIAP (floating target)
, Robinson, and Bulling} 2016{}  Citation Citation {Smith, Yin, Feiner		EYEDIAP (floating target)
, and Nayar} 2013  Citation Citation {Funesprotect unhbox voidb@x		EYEDIAP (floating target)
 penalty @M		EYEDIAP (floating target)
		EYEDIAP (floating target)
Mora, Monay, and Odobez} 2014{}  Citation Citation {Krafka, Khosla, Kellnhofer		EYEDIAP (floating target)
, Kannan, Bhandarkar, Matusik, and 		EYEDIAP (floating target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (floating target)
, and Bulling} 2015  Citation Citation {Sugano, Matsushita, and		EYEDIAP (floating target)
 Sato} 2014		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Wood, Baltrusaitis, Zhang, 		EYEDIAP (floating target)
Sugano, Robinson, and Bulling} 2015  Citation Citation {Wood, Baltru{²}aitis, Morency		EYEDIAP (floating target)
, Robinson, and Bulling} 2016{}  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (floating target)
, and Bulling} 2015  Citation Citation {Krafka, Khosla, Kellnhofer		EYEDIAP (floating target)
, Kannan, Bhandarkar, Matusik, and 		EYEDIAP (floating target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (floating target)
, and Bulling} 2017  Citation Citation {Deng and Zhu		EYEDIAP (floating target)
} 2017		EYEDIAP (floating target)
		EYEDIAP (floating target)
4 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (floating target)
ESTIMATION  Conv		EYEDIAP (floating target)
		EYEDIAP (floating target)
C on ca t  x y z x y		EYEDIAP (floating target)
 z x y z		EYEDIAP (floating target)
		EYEDIAP (floating target)
Individual Fusion Temporal  Individual Fusion		EYEDIAP (floating target)
		EYEDIAP (floating target)
Input 		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
Individual Fusion  Normalization		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
 .Conv		EYEDIAP (floating target)
		EYEDIAP (floating target)
Conv .  Conv		EYEDIAP (floating target)
		EYEDIAP (floating target)
Conv .  FC		EYEDIAP (floating target)
		EYEDIAP (floating target)
FC FC RNN  RNN		EYEDIAP (floating target)
		EYEDIAP (floating target)
RNN FC  Ti m e		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
		EYEDIAP (floating target)
Figure 1: Overview of the 		EYEDIAP (floating target)
proposed network. A multi-stream CNN 		EYEDIAP (floating target)
jointly models full-face, eye region 		EYEDIAP (floating target)
appearance and face landmarks from 		EYEDIAP (floating target)
still images. The combined extracted 		EYEDIAP (floating target)
fea- tures from each frame 		EYEDIAP (floating target)
are fed into a recurrent 		EYEDIAP (floating target)
module to predict last frame’s 		EYEDIAP (floating target)
gaze direction.  Gazing to a specific target		EYEDIAP (floating target)
 is achieved by a combination		EYEDIAP (floating target)
 of eye and head movements		EYEDIAP (floating target)
, which are highly coordinated. 		EYEDIAP (floating target)
Consequently, the apparent direction of 		EYEDIAP (floating target)
gaze is influenced not only 		EYEDIAP (floating target)
by the location of the 		EYEDIAP (floating target)
irises within the eyelid aperture, 		EYEDIAP (floating target)
but also by the position 		EYEDIAP (floating target)
and orientation of the face 		EYEDIAP (floating target)
with respect to the camera. 		EYEDIAP (floating target)
Known as the Wollaston 		EYEDIAP (floating target)
effect [36], the exact same 		EYEDIAP (floating target)
set of eyes may appear 		EYEDIAP (floating target)
to be looking in different 		EYEDIAP (floating target)
directions due to the surrounding 		EYEDIAP (floating target)
facial cues. It is therefore 		EYEDIAP (floating target)
reasonable to state that eye 		EYEDIAP (floating target)
images are not sufficient to 		EYEDIAP (floating target)
estimate gaze direction. Instead, whole-face 		EYEDIAP (floating target)
images can encode head pose 		EYEDIAP (floating target)
or illumination-specific information across larger 		EYEDIAP (floating target)
areas than those available just 		EYEDIAP (floating target)
in the eyes region [16, 43		EYEDIAP (floating target)
].  The drawback of appearance-only methods		EYEDIAP (floating target)
 is that global structure information		EYEDIAP (floating target)
 is not explicitly considered. In		EYEDIAP (floating target)
 that sense, facial landmarks can		EYEDIAP (floating target)
 be used as global shape		EYEDIAP (floating target)
 cues to en- code spatial		EYEDIAP (floating target)
 relationships and geometric constraints. Current		EYEDIAP (floating target)
 state-of-the-art face alignment approaches are		EYEDIAP (floating target)
 robust enough to handle large		EYEDIAP (floating target)
 appearance variability, extreme head poses		EYEDIAP (floating target)
 and occlusions, being especially useful		EYEDIAP (floating target)
 when the dataset used for		EYEDIAP (floating target)
 gaze estimation does not contain		EYEDIAP (floating target)
 such variability. Facial landmarks are		EYEDIAP (floating target)
 mainly correlated with head orientation		EYEDIAP (floating target)
, eye position, eyelid openness, 		EYEDIAP (floating target)
and eyebrow movement, which are 		EYEDIAP (floating target)
valuable features for our task.  Therefore, in our approach we		EYEDIAP (floating target)
 jointly model appearance and shape		EYEDIAP (floating target)
 cues (see Figure 1). The		EYEDIAP (floating target)
 former is represented by a		EYEDIAP (floating target)
 whole-face image IF , along		EYEDIAP (floating target)
 with a higher resolution image		EYEDIAP (floating target)
 of the eyes IE to		EYEDIAP (floating target)
 identify subtle changes. Due to		EYEDIAP (floating target)
 dealing with wide head pose		EYEDIAP (floating target)
 ranges, some eye images may		EYEDIAP (floating target)
 not depict the whole eye		EYEDIAP (floating target)
, containing mostly background or 		EYEDIAP (floating target)
other surrounding facial parts instead. 		EYEDIAP (floating target)
For that reason, and contrary 		EYEDIAP (floating target)
to previous approaches that only 		EYEDIAP (floating target)
use one eye image [31, 42		EYEDIAP (floating target)
], we use a single 		EYEDIAP (floating target)
image composed of two patches 		EYEDIAP (floating target)
of centered left and right 		EYEDIAP (floating target)
eyes. Finally, the shape cue 		EYEDIAP (floating target)
is represented by 3D face 		EYEDIAP (floating target)
landmarks obtained from a 68-landmark 		EYEDIAP (floating target)
model, denoted by 		EYEDIAP (floating target)
L = {(lx, ly, 		EYEDIAP (floating target)
		EYEDIAP (floating target)
)		EYEDIAP (floating target)
		EYEDIAP (floating target)
 | ∀c ∈ [1, ...,68		EYEDIAP (floating target)
]}.  In this work we also		EYEDIAP (floating target)
 consider the dynamic component of		EYEDIAP (floating target)
 gaze. We leverage the se		EYEDIAP (floating target)
- quential information of eye 		EYEDIAP (floating target)
and head movements such that, 		EYEDIAP (floating target)
given appearance and shape features 		EYEDIAP (floating target)
of consecutive frames, it is 		EYEDIAP (floating target)
possible to better predict the 		EYEDIAP (floating target)
gaze direction of the cur- 		EYEDIAP (floating target)
rent frame. Therefore, the 3D 		EYEDIAP (floating target)
gaze estimation task for a 1		EYEDIAP (floating target)
-frame sequence is formulated  Citation Citation {Wollaston etprotect unhbox		EYEDIAP (floating target)
 voidb@x penalty @M		EYEDIAP (floating target)
		EYEDIAP (floating target)
al.} 1824  Citation Citation {Krafka, Khosla, Kellnhofer		EYEDIAP (floating target)
, Kannan, Bhandarkar, Matusik, and 		EYEDIAP (floating target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (floating target)
, and Bulling} 2017  Citation Citation {Sugano, Matsushita, and		EYEDIAP (floating target)
 Sato} 2014		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Zhang, Sugano, Fritz, 		EYEDIAP (floating target)
and Bulling} 2015		EYEDIAP (floating target)
		EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 5		EYEDIAP (floating target)
		EYEDIAP (floating target)
as g(i) = f ( {IF (i)},{IE (i)},{L(i		EYEDIAP (floating target)
)}  ) , where i denotes		EYEDIAP (floating target)
 the i-th frame, and f		EYEDIAP (floating target)
 is the regression		EYEDIAP (floating target)
		EYEDIAP (floating target)
function.  3.2 Data normalization Prior to		EYEDIAP (floating target)
 gaze regression, a normalization step		EYEDIAP (floating target)
 in the 3D space and		EYEDIAP (floating target)
 the 2D image, similar to		EYEDIAP (floating target)
 [31], is carried out. This		EYEDIAP (floating target)
 is performed to reduce the		EYEDIAP (floating target)
 appearance variability and to allow		EYEDIAP (floating target)
 the gaze estimation model to		EYEDIAP (floating target)
 be applied regardless of the		EYEDIAP (floating target)
 original camera configuration		EYEDIAP (floating target)
.  Let H ∈ R3x3 be		EYEDIAP (floating target)
 the head rotation matrix, and		EYEDIAP (floating target)
 p = [px, py, pz]T		EYEDIAP (floating target)
 ∈ R3 the reference face		EYEDIAP (floating target)
 location with respect to the		EYEDIAP (floating target)
 original CCS. The goal is		EYEDIAP (floating target)
 to find the conversion matrix		EYEDIAP (floating target)
 M = SR such that		EYEDIAP (floating target)
 (a) the X-axes of the		EYEDIAP (floating target)
 virtual camera and the head		EYEDIAP (floating target)
 become parallel using the rotation		EYEDIAP (floating target)
 matrix R, and (b) the		EYEDIAP (floating target)
 virtual camera looks at the		EYEDIAP (floating target)
 reference location from a fixed		EYEDIAP (floating target)
 distance dn using the Z-direction		EYEDIAP (floating target)
 scaling matrix S = diag(1,1,dn/‖p		EYEDIAP (floating target)
‖). R is computed as 		EYEDIAP (floating target)
a = p̂×HT e1, 		EYEDIAP (floating target)
b = â× p̂, 		EYEDIAP (floating target)
R = [â, b̂, p̂]T , where e1 denotes the first		EYEDIAP (floating target)
 orthonormal basis and		EYEDIAP (floating target)
 〈 ·̂ 〉 is the		EYEDIAP (floating target)
 unit vector		EYEDIAP (floating target)
.  This normalization translates into the		EYEDIAP (floating target)
 image space as a cropped		EYEDIAP (floating target)
 image patch of size Wn×Hn		EYEDIAP (floating target)
 centered at p where head		EYEDIAP (floating target)
 roll rotation has been removed		EYEDIAP (floating target)
. This is done by 		EYEDIAP (floating target)
applying a perspective warping to 		EYEDIAP (floating target)
the input image I using 		EYEDIAP (floating target)
the transformation matrix W = 		EYEDIAP (floating target)
CoMCn−1, where Co and Cn 		EYEDIAP (floating target)
are the original and virtual 		EYEDIAP (floating target)
camera matrices, respectively.  The 3D gaze vector is		EYEDIAP (floating target)
 also normalized as gn =Rg		EYEDIAP (floating target)
. After image normalization, the 		EYEDIAP (floating target)
line of sight can be 		EYEDIAP (floating target)
represented in a 2D space. 		EYEDIAP (floating target)
Therefore, gn is further transformed 		EYEDIAP (floating target)
to spherical coor- dinates (θ ,		EYEDIAP (floating target)
φ) assuming unit length, where 		EYEDIAP (floating target)
θ and φ denote the 		EYEDIAP (floating target)
horizontal and vertical direc- tion 		EYEDIAP (floating target)
angles, respectively. This 2D angle 		EYEDIAP (floating target)
representation, delimited in the 		EYEDIAP (floating target)
range [−π/2,π/2], is computed as 		EYEDIAP (floating target)
θ = arctan(gx/gz) and 		EYEDIAP (floating target)
φ = arcsin(−gy), such that (0,		EYEDIAP (floating target)
0) represents looking straight ahead 		EYEDIAP (floating target)
to the CCS origin.  3.3 Recurrent Convolutional Neural Network		EYEDIAP (floating target)
 We propose a Recurrent CNN		EYEDIAP (floating target)
 Regression Network for 3D gaze		EYEDIAP (floating target)
 estimation. The network is divided		EYEDIAP (floating target)
 in 3 modules: (1) Individual		EYEDIAP (floating target)
, (2) Fusion, and (3) 		EYEDIAP (floating target)
Temporal.  First, the Individual module learns		EYEDIAP (floating target)
 features from each appearance cue		EYEDIAP (floating target)
 separately. It consists of a		EYEDIAP (floating target)
 two-stream CNN, one devoted to		EYEDIAP (floating target)
 the normalized face image stream		EYEDIAP (floating target)
 and the other to the		EYEDIAP (floating target)
 joint normalized eyes image. Next		EYEDIAP (floating target)
, the Fusion module combines 		EYEDIAP (floating target)
the extracted features of each 		EYEDIAP (floating target)
appearance stream in a single 		EYEDIAP (floating target)
vector along with the normalized 		EYEDIAP (floating target)
landmark coordinates. Then, it learns 		EYEDIAP (floating target)
a joint representation between modalities 		EYEDIAP (floating target)
in a late-fusion fashion. Both 		EYEDIAP (floating target)
Individual and Fusion modules, further 		EYEDIAP (floating target)
referred to as Static model, 		EYEDIAP (floating target)
are applied to each frame 		EYEDIAP (floating target)
of the sequence. Finally, the 		EYEDIAP (floating target)
resulting feature vectors of each 		EYEDIAP (floating target)
frame are input to the 		EYEDIAP (floating target)
Temporal module based on a 		EYEDIAP (floating target)
many-to-one recurrent network. This module 		EYEDIAP (floating target)
leverages sequential information to predict 		EYEDIAP (floating target)
the normalized 2D gaze angles 		EYEDIAP (floating target)
of the last frame of 		EYEDIAP (floating target)
the sequence using a linear 		EYEDIAP (floating target)
regression layer added on top 		EYEDIAP (floating target)
of it.  3.4 Implementation details 3.4.1 Network		EYEDIAP (floating target)
 details		EYEDIAP (floating target)
		EYEDIAP (floating target)
Each stream of the Individual 		EYEDIAP (floating target)
module is based on the 		EYEDIAP (floating target)
VGG-16 deep network [27], consisting 		EYEDIAP (floating target)
of 13 convolutional layers, 5 		EYEDIAP (floating target)
max pooling layers, and 1 		EYEDIAP (floating target)
fully connected (FC) layer with 		EYEDIAP (floating target)
Rec- tified Linear Unit (ReLU) 		EYEDIAP (floating target)
activations. The full-face stream follows 		EYEDIAP (floating target)
the same configuration  Citation Citation {Sugano, Matsushita, and		EYEDIAP (floating target)
 Sato} 2014		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Parkhi, Vedaldi, and 		EYEDIAP (floating target)
Zisserman} 2015		EYEDIAP (floating target)
		EYEDIAP (floating target)
6 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (floating target)
ESTIMATION  as the base network, having		EYEDIAP (floating target)
 an input of 224×224 pixels		EYEDIAP (floating target)
 and a 4096D FC layer		EYEDIAP (floating target)
. In contrast, the input 		EYEDIAP (floating target)
joint eye image is smaller, 		EYEDIAP (floating target)
with a final size of 120		EYEDIAP (floating target)
×48 pixels, so the number 		EYEDIAP (floating target)
of pa- rameters is decreased 		EYEDIAP (floating target)
proportionally. In this case, its 		EYEDIAP (floating target)
last FC layer produces a 		EYEDIAP (floating target)
1536D vector. A 204D landmark 		EYEDIAP (floating target)
coordinates vector is concatenated to 		EYEDIAP (floating target)
the output of the FC 		EYEDIAP (floating target)
layer of each stream, resulting 		EYEDIAP (floating target)
in a 5836D feature vector. 		EYEDIAP (floating target)
Consequently, the Fusion module consists 		EYEDIAP (floating target)
of 2 5836D FC layers 		EYEDIAP (floating target)
with ReLU activations and 2 		EYEDIAP (floating target)
dropout layers between FCs as 		EYEDIAP (floating target)
regularization. Finally, to model the 		EYEDIAP (floating target)
temporal dependencies, we use a 		EYEDIAP (floating target)
single GRU layer with 128 		EYEDIAP (floating target)
units.  The network is trained in		EYEDIAP (floating target)
 a stage-wise fashion. First, we		EYEDIAP (floating target)
 train the Static model and		EYEDIAP (floating target)
 the final regression layer end-to-end		EYEDIAP (floating target)
 on each individual frame of		EYEDIAP (floating target)
 the training data. The convolutional		EYEDIAP (floating target)
 blocks are pre-trained with the		EYEDIAP (floating target)
 VGG-Face dataset [27], whereas the		EYEDIAP (floating target)
 FCs are trained from scratch		EYEDIAP (floating target)
. Second, the training data 		EYEDIAP (floating target)
is re-arranged by means of 		EYEDIAP (floating target)
a sliding window with stride 1 to build input sequences. Each		EYEDIAP (floating target)
 sequence is composed of s		EYEDIAP (floating target)
 = 4 consecutive frames, whose		EYEDIAP (floating target)
 gaze direction target is the		EYEDIAP (floating target)
 gaze direction of the last		EYEDIAP (floating target)
 frame of the sequence( {I(i−s+1		EYEDIAP (floating target)
), . . . ,I(i)}, 		EYEDIAP (floating target)
g(i)  ) . Using this re-arranged		EYEDIAP (floating target)
 training data, we extract features		EYEDIAP (floating target)
 of each		EYEDIAP (floating target)
		EYEDIAP (floating target)
frame of the sequence from 		EYEDIAP (floating target)
a frozen Individual module, fine-tune 		EYEDIAP (floating target)
the Fusion layers, and train 		EYEDIAP (floating target)
both, the Temporal module and 		EYEDIAP (floating target)
a new final regression layer 		EYEDIAP (floating target)
from scratch. This way, the 		EYEDIAP (floating target)
network can exploit the temporal 		EYEDIAP (floating target)
information to further refine the 		EYEDIAP (floating target)
fusion weights.  We trained the model using		EYEDIAP (floating target)
 ADAM optimizer with an initial		EYEDIAP (floating target)
 learning rate of 0.0001, dropout		EYEDIAP (floating target)
 of 0.3, and batch size		EYEDIAP (floating target)
 of 64 frames. The number		EYEDIAP (floating target)
 of epochs was experimentally set		EYEDIAP (floating target)
 to 21 for the first		EYEDIAP (floating target)
 training stage and 10 for		EYEDIAP (floating target)
 the second. We use the		EYEDIAP (floating target)
 average Euclidean distance between the		EYEDIAP (floating target)
 predicted and ground-truth 3D gaze		EYEDIAP (floating target)
 vectors as loss function		EYEDIAP (floating target)
.  3.4.2 Input pre-processing		EYEDIAP (floating target)
		EYEDIAP (floating target)
For this work we use 		EYEDIAP (floating target)
head pose and eye locations 		EYEDIAP (floating target)
in the 3D scene provided 		EYEDIAP (floating target)
by the dataset. The 3D 		EYEDIAP (floating target)
landmarks are extracted using the 		EYEDIAP (floating target)
state-of-the-art method of Bulat and 		EYEDIAP (floating target)
Tzimiropou- los [3], which is 		EYEDIAP (floating target)
based on stacked hourglass 		EYEDIAP (floating target)
networks [24].  During training, the original image		EYEDIAP (floating target)
 is pre-processed to get the		EYEDIAP (floating target)
 two normalized input images. The		EYEDIAP (floating target)
 normalized whole-face patch is centered		EYEDIAP (floating target)
 0.1 meters ahead of the		EYEDIAP (floating target)
 head center in the head		EYEDIAP (floating target)
 coordinate system, and Cn is		EYEDIAP (floating target)
 defined such that the image		EYEDIAP (floating target)
 has size of 250× 250		EYEDIAP (floating target)
 pixels. The difference between this		EYEDIAP (floating target)
 size and the final input		EYEDIAP (floating target)
 size allows us to perform		EYEDIAP (floating target)
 random cropping and zooming to		EYEDIAP (floating target)
 augment the data (explained in		EYEDIAP (floating target)
 Section 4.1). Similarly, each normalized		EYEDIAP (floating target)
 eye patch is centered in		EYEDIAP (floating target)
 their respective eye center locations		EYEDIAP (floating target)
. In this case, the 		EYEDIAP (floating target)
virtual camera matrix is defined 		EYEDIAP (floating target)
so that the image is 		EYEDIAP (floating target)
cropped to 70×58, while in 		EYEDIAP (floating target)
practice the final patches have 		EYEDIAP (floating target)
size of 60×48. Landmarks are 		EYEDIAP (floating target)
normalized using the same procedure 		EYEDIAP (floating target)
and further pre-processed with mean 		EYEDIAP (floating target)
subtraction and min-max normalization per 		EYEDIAP (floating target)
axis. Finally, we divide them 		EYEDIAP (floating target)
by a scaling factor w 		EYEDIAP (floating target)
such that all coordinates are 		EYEDIAP (floating target)
in the range [0,w]. This 		EYEDIAP (floating target)
way, all concatenated feature values 		EYEDIAP (floating target)
are in a similar range. 		EYEDIAP (floating target)
After inference, the predicted normalized 		EYEDIAP (floating target)
2D angles are de-normalized back 		EYEDIAP (floating target)
to the original 3D space.  4 Experiments In this section		EYEDIAP (floating target)
, we evaluate the cross-subject 		EYEDIAP (floating target)
3D gaze estimation task on 		EYEDIAP (floating target)
a wide range of head 		EYEDIAP (floating target)
poses and gaze directions. Furthermore, 		EYEDIAP (floating target)
we validate the effectiveness of 		EYEDIAP (floating target)
the proposed architecture comparing both 		EYEDIAP (floating target)
static and temporal approaches. We 		EYEDIAP (floating target)
report the error in terms 		EYEDIAP (floating target)
of mean angular error between 		EYEDIAP (floating target)
predicted and ground-truth 3D gaze 		EYEDIAP (floating target)
vectors. Note that due to 		EYEDIAP (floating target)
the requirements of the temporal 		EYEDIAP (floating target)
model not all the frames 		EYEDIAP (floating target)
obtain a prediction. Therefore, for 		EYEDIAP (floating target)
a  Citation Citation {Parkhi, Vedaldi, and		EYEDIAP (floating target)
 Zisserman} 2015		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Bulat and Tzimiropoulos} 2017		EYEDIAP (floating target)
		EYEDIAP (floating target)
Citation Citation {Newell, Yang, and 		EYEDIAP (floating target)
Deng} 2016		EYEDIAP (floating target)
		EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 7		EYEDIAP (floating target)
		EYEDIAP (floating target)
60 30 0 30 60  60		EYEDIAP (floating target)
		EYEDIAP (floating target)
30  0		EYEDIAP (floating target)
		EYEDIAP (floating target)
30  60		EYEDIAP (floating target)
		EYEDIAP (floating target)
100  101		EYEDIAP (floating target)
		EYEDIAP (floating target)
102  60 30 0 30 60		EYEDIAP (floating target)
		EYEDIAP (floating target)
60  30		EYEDIAP (floating target)
		EYEDIAP (floating target)
0  30		EYEDIAP (floating target)
		EYEDIAP (floating target)
60  100		EYEDIAP (floating target)
		EYEDIAP (floating target)
101  102		EYEDIAP (floating target)
		EYEDIAP (floating target)
103  60 30 0 30 60		EYEDIAP (floating target)
		EYEDIAP (floating target)
60  30		EYEDIAP (floating target)
		EYEDIAP (floating target)
0  30		EYEDIAP (floating target)
		EYEDIAP (floating target)
60  100		EYEDIAP (floating target)
		EYEDIAP (floating target)
101  102		EYEDIAP (floating target)
		EYEDIAP (floating target)
60 30 0 30 60  60		EYEDIAP (floating target)
		EYEDIAP (floating target)
30  0		EYEDIAP (floating target)
		EYEDIAP (floating target)
30  60		EYEDIAP (floating target)
		EYEDIAP (floating target)
100  101		EYEDIAP (floating target)
		EYEDIAP (floating target)
102  103		EYEDIAP (floating target)
		EYEDIAP (floating target)
a) g (FT ) (b) 		EYEDIAP (floating target)
h (FT ) (c) g (		EYEDIAP (floating target)
CS) (d) h (CS)  Figure 2: Ground-truth eye gaze		EYEDIAP (floating target)
 g and head orientation h		EYEDIAP (floating target)
 distribution on the filtered EYE		EYEDIAP (floating target)
- DIAP dataset for CS 		EYEDIAP (floating target)
and FT settings, in terms 		EYEDIAP (floating target)
of x- and y- angles.  fair comparison, the reported results		EYEDIAP (floating target)
 for static models disregard such		EYEDIAP (floating target)
 frames when temporal models are		EYEDIAP (floating target)
 included in the comparison		EYEDIAP (floating target)
.  4.1 Training data		EYEDIAP (floating target)
		EYEDIAP (floating target)
There are few publicly available 		EYEDIAP (floating target)
datasets devoted to 3D gaze 		EYEDIAP (floating target)
estimation and most of them 		EYEDIAP (floating target)
focus on HCI with a 		EYEDIAP (floating target)
limited range of head pose 		EYEDIAP (floating target)
and gaze directions. Therefore, we 		EYEDIAP (floating target)
use VGA videos from the 		EYEDIAP (floating target)
publicly-available EYEDIAP dataset [7] to 		EYEDIAP (floating target)
perform the experimental evaluation, as 		EYEDIAP (floating target)
it is currently the only 		EYEDIAP (floating target)
one containing video sequences with 		EYEDIAP (floating target)
a wide range of head 		EYEDIAP (floating target)
poses and showing the full 		EYEDIAP (floating target)
face. This dataset consists of 3		EYEDIAP (floating target)
-minute videos of 16 subjects 		EYEDIAP (floating target)
looking at two types of 		EYEDIAP (floating target)
targets: continuous screen targets on 		EYEDIAP (floating target)
a fixed monitor (CS), and 		EYEDIAP (floating target)
floating physical targets (FT ). 		EYEDIAP (floating target)
The videos are further divided 		EYEDIAP (floating target)
into static (S) and moving (		EYEDIAP (floating target)
M) head pose for each 		EYEDIAP (floating target)
of the subjects. Subjects 12-16 		EYEDIAP (floating target)
were recorded with 2 different 		EYEDIAP (floating target)
lighting conditions.  For evaluation, we filtered out		EYEDIAP (floating target)
 those frames that fulfilled at		EYEDIAP (floating target)
 least one of the following		EYEDIAP (floating target)
 conditions: (1) face or landmarks		EYEDIAP (floating target)
 not detected; (2) subject not		EYEDIAP (floating target)
 looking at the target; (3		EYEDIAP (floating target)
) 3D head pose, eyes 		EYEDIAP (floating target)
or target location not properly 		EYEDIAP (floating target)
recovered; and (4) eyeball rotations 		EYEDIAP (floating target)
violating physical 		EYEDIAP (floating target)
constraints (|θ | ≤ 40		EYEDIAP (floating target)
◦, |φ | ≤ 30		EYEDIAP (floating target)
◦) [23]. Note that we 		EYEDIAP (floating target)
purposely do not filter eye 		EYEDIAP (floating target)
blinking moments to learn their 		EYEDIAP (floating target)
dynamics with the temporal model, 		EYEDIAP (floating target)
which may produce some outliers 		EYEDIAP (floating target)
with a higher prediction error 		EYEDIAP (floating target)
due to a less accurate 		EYEDIAP (floating target)
ground truth. Figure 2 shows 		EYEDIAP (floating target)
the distribution of gaze directions 		EYEDIAP (floating target)
and head poses for both 		EYEDIAP (floating target)
filtered CS and FT cases.  We applied data augmentation to		EYEDIAP (floating target)
 the training set with the		EYEDIAP (floating target)
 following random transforma- tions: horizontal		EYEDIAP (floating target)
 flip, shifts of up to		EYEDIAP (floating target)
 5 pixels, zoom of up		EYEDIAP (floating target)
 to 2%, brightness changes by		EYEDIAP (floating target)
 a factor in the range		EYEDIAP (floating target)
 [0.4,1.75], and additive Gaussian noise		EYEDIAP (floating target)
 with σ2 = 0.03		EYEDIAP (floating target)
.  4.2 Evaluation of static modalities		EYEDIAP (floating target)
		EYEDIAP (floating target)
First, we evaluate the contribution 		EYEDIAP (floating target)
of each static modality on 		EYEDIAP (floating target)
the FT scenario. We divided 		EYEDIAP (floating target)
the 16 participants into 4 		EYEDIAP (floating target)
groups, such that appearance variability 		EYEDIAP (floating target)
was maximized while maintaining a 		EYEDIAP (floating target)
similar number of training samples 		EYEDIAP (floating target)
per group. Each static model 		EYEDIAP (floating target)
was trained end-to-end performing 4-fold 		EYEDIAP (floating target)
cross-validation using different combinations of 		EYEDIAP (floating target)
input modal- ities. Since the 		EYEDIAP (floating target)
number of fusion units depends 		EYEDIAP (floating target)
on the number of input 		EYEDIAP (floating target)
modalities, we also compare different 		EYEDIAP (floating target)
fusion layer sizes. The effect 		EYEDIAP (floating target)
of data normalization is also 		EYEDIAP (floating target)
evaluated by training a not-normalized 		EYEDIAP (floating target)
face model where the input 		EYEDIAP (floating target)
image is the face bounding 		EYEDIAP (floating target)
box with square size the 		EYEDIAP (floating target)
maximum distance between 2D landmarks.  Citation Citation {Funesprotect unhbox voidb@x		EYEDIAP (floating target)
 penalty @M		EYEDIAP (floating target)
		EYEDIAP (floating target)
Mora, Monay, and Odobez} 2014{}  Citation Citation {MSC		EYEDIAP (floating target)
		EYEDIAP (floating target)
8 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (floating target)
ESTIMATION  0 1 2 3 4		EYEDIAP (floating target)
 5 6 7 8 9		EYEDIAP (floating target)
		EYEDIAP (floating target)
10 11  An gl		EYEDIAP (floating target)
		EYEDIAP (floating target)
e  er		EYEDIAP (floating target)
		EYEDIAP (floating target)
ro r (  de gr		EYEDIAP (floating target)
		EYEDIAP (floating target)
ee s)  6.9 6.43 5.58 5.71 5.59		EYEDIAP (floating target)
 5.55 5.52		EYEDIAP (floating target)
		EYEDIAP (floating target)
OF-4096 NE-1536 NF-4096  NF-5632 NFL-4300		EYEDIAP (floating target)
		EYEDIAP (floating target)
NFE-5632 NFEL-5836  Figure 3: Performance evaluation of		EYEDIAP (floating target)
 the Static network using different		EYEDIAP (floating target)
 input modali- ties (O		EYEDIAP (floating target)
 - Not normalized, N		EYEDIAP (floating target)
 - Normalized, F - Face		EYEDIAP (floating target)
, E - Eyes, 		EYEDIAP (floating target)
L - 3D Landmarks) and 		EYEDIAP (floating target)
size of fusion layers on 		EYEDIAP (floating target)
the FT scenario.  Floating Target Screen Target 0		EYEDIAP (floating target)
 1 2 3 4 5		EYEDIAP (floating target)
 6 7 8 9		EYEDIAP (floating target)
		EYEDIAP (floating target)
10 11  An gl		EYEDIAP (floating target)
		EYEDIAP (floating target)
e  er		EYEDIAP (floating target)
		EYEDIAP (floating target)
ro r (  de gr		EYEDIAP (floating target)
		EYEDIAP (floating target)
ee s)  6.36 5.43 5.19 4.2 3.38		EYEDIAP (floating target)
 3.4		EYEDIAP (floating target)
		EYEDIAP (floating target)
MPIIGaze Static Temporal  Figure 4: Performance comparison among		EYEDIAP (floating target)
 MPIIGaze method [42] and our		EYEDIAP (floating target)
 Static and Temporal versions of		EYEDIAP (floating target)
 the proposed network for FT		EYEDIAP (floating target)
 and CS scenarios		EYEDIAP (floating target)
.  As shown in Figure 3		EYEDIAP (floating target)
, all models that take 		EYEDIAP (floating target)
normalized full-face information as input 		EYEDIAP (floating target)
achieve better performance than the 		EYEDIAP (floating target)
eyes-only model. More specifically, the 		EYEDIAP (floating target)
combination of face, eyes and 		EYEDIAP (floating target)
landmarks outperforms all the other 		EYEDIAP (floating target)
combinations by a small but 		EYEDIAP (floating target)
significant margin (paired Wilcoxon test, 		EYEDIAP (floating target)
p < 0.0001). The standard 		EYEDIAP (floating target)
deviation of the best-performing model 		EYEDIAP (floating target)
is reduced compared to the 		EYEDIAP (floating target)
face and eyes model, suggesting 		EYEDIAP (floating target)
a regularizing effect due to 		EYEDIAP (floating target)
the addition of landmarks. The 		EYEDIAP (floating target)
not-normalized face-only model shows the 		EYEDIAP (floating target)
largest error, proving the impact 		EYEDIAP (floating target)
of normalization to reduce the 		EYEDIAP (floating target)
appearance variability. Furthermore, our results 		EYEDIAP (floating target)
indicate that the increase of 		EYEDIAP (floating target)
fusion units is not correlated 		EYEDIAP (floating target)
with a better performance.  4.3 Static gaze regression: comparison		EYEDIAP (floating target)
 with existing methods		EYEDIAP (floating target)
		EYEDIAP (floating target)
We compare our best-performing static 		EYEDIAP (floating target)
model with three baselines. Head: 		EYEDIAP (floating target)
Treating the head pose directly 		EYEDIAP (floating target)
as gaze direction. PR-ALR: Method 		EYEDIAP (floating target)
that relies on RGB-D data 		EYEDIAP (floating target)
to rectify the eye images 		EYEDIAP (floating target)
viewpoint into a canonical head 		EYEDIAP (floating target)
pose using a 3DMM. It 		EYEDIAP (floating target)
then learns an RGB gaze 		EYEDIAP (floating target)
appearance model using ALR [21]. 		EYEDIAP (floating target)
Predicted 3D vectors for FT-S 		EYEDIAP (floating target)
scenario are provided by EYEDIAP 		EYEDIAP (floating target)
dataset. MPIIGaze:. State-of-the-art full-face 3D 		EYEDIAP (floating target)
gaze estimation method [42]. They 		EYEDIAP (floating target)
use an Alexnet-based CNN model 		EYEDIAP (floating target)
with spatial weights to enhance 		EYEDIAP (floating target)
information in different facial regions. 		EYEDIAP (floating target)
We fine-tuned it with the 		EYEDIAP (floating target)
filtered EYEDIAP subsets using our 		EYEDIAP (floating target)
training parameters and normalization procedure.  In addition to the aforementioned		EYEDIAP (floating target)
 FT-based evaluation setup, we also		EYEDIAP (floating target)
 evaluate our method on the		EYEDIAP (floating target)
 CS scenario. In this case		EYEDIAP (floating target)
 there are only 14 participants		EYEDIAP (floating target)
 available, so we divided them		EYEDIAP (floating target)
 in 5 groups and performed		EYEDIAP (floating target)
 5-fold cross-validation. In Figure 4		EYEDIAP (floating target)
 we compare our method to		EYEDIAP (floating target)
 MPIIGaze, achieving a statistically significant		EYEDIAP (floating target)
 improvement of 14.6% and 19.5		EYEDIAP (floating target)
% on FT and CS 		EYEDIAP (floating target)
scenarios, respectively (paired Wilcoxon test, 		EYEDIAP (floating target)
p < 0.0001). We can 		EYEDIAP (floating target)
observe that a re- stricted 		EYEDIAP (floating target)
gaze target benefits the performance 		EYEDIAP (floating target)
of all methods, compared to 		EYEDIAP (floating target)
a more challenging unrestricted setting 		EYEDIAP (floating target)
with a wider range of 		EYEDIAP (floating target)
head poses and gaze directions.  Table 2 provides a detailed		EYEDIAP (floating target)
 comparison on every participant, performing		EYEDIAP (floating target)
 leave-one-out cross-validation on the FT		EYEDIAP (floating target)
 scenario for static and moving		EYEDIAP (floating target)
 head separately. Results show that		EYEDIAP (floating target)
, as expected, facial appearance 		EYEDIAP (floating target)
and head pose have a 		EYEDIAP (floating target)
noticeable impact on gaze accuracy, 		EYEDIAP (floating target)
with average error differences of 		EYEDIAP (floating target)
up to 7.7◦ among participants.  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (floating target)
, and Bulling} 2015  Citation Citation {Mora and Odobez		EYEDIAP (floating target)
} 2012  Citation Citation {Zhang, Sugano, Fritz		EYEDIAP (floating target)
, and Bulling} 2015		EYEDIAP (floating target)
		EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 9		EYEDIAP (floating target)
		EYEDIAP (floating target)
Method 1 2 3 4 5 6 7 8 9 10		EYEDIAP (floating target)
 11 12 13 14 15		EYEDIAP (floating target)
 16 Avg. Head 23.5 22.1		EYEDIAP (floating target)
 20.3 23.6 23.2 23.2 23.6		EYEDIAP (floating target)
 21.2 26.7 23.6 23.1 24.4		EYEDIAP (floating target)
 23.3 24.0 24.5 22.8 23.3		EYEDIAP (floating target)
 PR-ALR 12.3 12.0 12.4 11.3		EYEDIAP (floating target)
 15.5 12.9 17.9 11.8 17.3		EYEDIAP (floating target)
 13.4 13.4 14.3 15.2 13.6		EYEDIAP (floating target)
 14.4 14.6 13.9 MPIIGaze 5.3		EYEDIAP (floating target)
 5.1 5.7 4.7 7.3 15.1		EYEDIAP (floating target)
 10.8 5.7 9.9 7.1 5.0		EYEDIAP (floating target)
 5.7 7.4 3.8 4.8 5.5		EYEDIAP (floating target)
 6.8 Static 3.9 4.1 4.2		EYEDIAP (floating target)
 3.9 6.0 6.4 7.2 3.6		EYEDIAP (floating target)
 7.1 5.0 5.7 6.7 3.9		EYEDIAP (floating target)
 4.7 5.1 4.2 5.1 Temporal		EYEDIAP (floating target)
 4.0 4.9 4.3 4.1 6.1		EYEDIAP (floating target)
 6.5 6.6 3.9 7.8 6.1		EYEDIAP (floating target)
 4.7 5.6 4.7 3.5 5.9		EYEDIAP (floating target)
 4.6 5.2 Head 19.3 14.2		EYEDIAP (floating target)
 16.4 19.9 16.8 21.9 16.1		EYEDIAP (floating target)
 24.2 20.3 19.9 18.8 22.3		EYEDIAP (floating target)
 18.1 14.9 16.2 19.3 18.7		EYEDIAP (floating target)
 MPIIGaze 7.6 6.2 5.7 8.7		EYEDIAP (floating target)
 10.1 12.0 12.2 6.1 8.3		EYEDIAP (floating target)
 5.9 6.1 6.2 7.4 4.7		EYEDIAP (floating target)
 4.4 6.0 7.3 Static 5.8		EYEDIAP (floating target)
 5.7 4.4 7.5 6.7 8.8		EYEDIAP (floating target)
 11.6 5.5 8.3 5.5 5.2		EYEDIAP (floating target)
 6.3 5.3 3.9 4.3 5.6		EYEDIAP (floating target)
 6.3 Temporal 6.1 5.6 4.5		EYEDIAP (floating target)
 7.5 6.4 8.2 12.0 5.0		EYEDIAP (floating target)
 7.5 5.4 5.0 5.8 6.6		EYEDIAP (floating target)
 4.0 4.5 5.8 6.2		EYEDIAP (floating target)
		EYEDIAP (floating target)
Table 2: Gaze angular error 		EYEDIAP (floating target)
comparison for static (top half) 		EYEDIAP (floating target)
and moving (bottom half) head 		EYEDIAP (floating target)
pose for each subject in 		EYEDIAP (floating target)
the FT scenario. Best results 		EYEDIAP (floating target)
in bold.  −80 −40 0 40 80−80		EYEDIAP (floating target)
		EYEDIAP (floating target)
40  0		EYEDIAP (floating target)
		EYEDIAP (floating target)
40  80		EYEDIAP (floating target)
		EYEDIAP (floating target)
0  5		EYEDIAP (floating target)
		EYEDIAP (floating target)
10  15		EYEDIAP (floating target)
		EYEDIAP (floating target)
20  25		EYEDIAP (floating target)
		EYEDIAP (floating target)
30  35		EYEDIAP (floating target)
		EYEDIAP (floating target)
80 −40 0 40 80−80  −40		EYEDIAP (floating target)
		EYEDIAP (floating target)
0  40		EYEDIAP (floating target)
		EYEDIAP (floating target)
80  −10		EYEDIAP (floating target)
		EYEDIAP (floating target)
8  −6		EYEDIAP (floating target)
		EYEDIAP (floating target)
4  −2		EYEDIAP (floating target)
		EYEDIAP (floating target)
0  2		EYEDIAP (floating target)
		EYEDIAP (floating target)
4  6		EYEDIAP (floating target)
		EYEDIAP (floating target)
8  10		EYEDIAP (floating target)
		EYEDIAP (floating target)
80 −40 0 40 80−80  −40		EYEDIAP (floating target)
		EYEDIAP (floating target)
0  40		EYEDIAP (floating target)
		EYEDIAP (floating target)
80  0		EYEDIAP (floating target)
		EYEDIAP (floating target)
5  10		EYEDIAP (floating target)
		EYEDIAP (floating target)
15  20		EYEDIAP (floating target)
		EYEDIAP (floating target)
25  30		EYEDIAP (floating target)
		EYEDIAP (floating target)
35  −80 −40 0 40 80−80		EYEDIAP (floating target)
		EYEDIAP (floating target)
40  0		EYEDIAP (floating target)
		EYEDIAP (floating target)
40  80		EYEDIAP (floating target)
		EYEDIAP (floating target)
10  −8		EYEDIAP (floating target)
		EYEDIAP (floating target)
6  −4		EYEDIAP (floating target)
		EYEDIAP (floating target)
2  0		EYEDIAP (floating target)
		EYEDIAP (floating target)
2  4		EYEDIAP (floating target)
		EYEDIAP (floating target)
6  8		EYEDIAP (floating target)
		EYEDIAP (floating target)
10  (a) Gaze space (b) Head		EYEDIAP (floating target)
 orientation space		EYEDIAP (floating target)
		EYEDIAP (floating target)
Figure 5: Angular error distribution 		EYEDIAP (floating target)
across gaze (a) and head 		EYEDIAP (floating target)
orientation (b) spaces in the 		EYEDIAP (floating target)
FT setting, in terms of 		EYEDIAP (floating target)
x- and y- angles. For 		EYEDIAP (floating target)
each space, we depict the 		EYEDIAP (floating target)
Static model performance (left) and 		EYEDIAP (floating target)
the contribution of the Temporal 		EYEDIAP (floating target)
model versus Static (right). In 		EYEDIAP (floating target)
the latter, positive difference means 		EYEDIAP (floating target)
higher improvement of the Temporal 		EYEDIAP (floating target)
model.  4.4 Evaluation of the temporal		EYEDIAP (floating target)
 network		EYEDIAP (floating target)
		EYEDIAP (floating target)
In this section, we evaluate 		EYEDIAP (floating target)
the contribution of adding the 		EYEDIAP (floating target)
temporal module to the static 		EYEDIAP (floating target)
model. To do so, we 		EYEDIAP (floating target)
trained a lower-dimensional version of 		EYEDIAP (floating target)
the static network with compa- 		EYEDIAP (floating target)
rable performance to the original, 		EYEDIAP (floating target)
reducing the number of units 		EYEDIAP (floating target)
of the second fusion layer 		EYEDIAP (floating target)
to 2918. Results are reported 		EYEDIAP (floating target)
in Figure 4 and Table 2		EYEDIAP (floating target)
. One can observe that 		EYEDIAP (floating target)
using sequential information is helpful 		EYEDIAP (floating target)
on the FT scenario, outperforming 		EYEDIAP (floating target)
the static model by a 		EYEDIAP (floating target)
statistically significant 4.4% (paired Wilcoxon 		EYEDIAP (floating target)
test, p < 0.0001). This 		EYEDIAP (floating target)
contribution is more noticeable in 		EYEDIAP (floating target)
the moving head setting, proving 		EYEDIAP (floating target)
that the temporal model can 		EYEDIAP (floating target)
benefit from head motion information. 		EYEDIAP (floating target)
In contrast, such information seems 		EYEDIAP (floating target)
to be less meaningful in 		EYEDIAP (floating target)
the CS scenario, where the 		EYEDIAP (floating target)
obtained error is already very 		EYEDIAP (floating target)
low for a cross-subject setting 		EYEDIAP (floating target)
and the amount of head 		EYEDIAP (floating target)
movement declines.  Figure 5 further explores the		EYEDIAP (floating target)
 error distribution of the static		EYEDIAP (floating target)
 network and the impact of		EYEDIAP (floating target)
 sequential information. We can observe		EYEDIAP (floating target)
 that the accuracy of the		EYEDIAP (floating target)
 static model drops with extreme		EYEDIAP (floating target)
 head poses and gaze directions		EYEDIAP (floating target)
, which can also be 		EYEDIAP (floating target)
correlated to having less data 		EYEDIAP (floating target)
in those areas. Compared to 		EYEDIAP (floating target)
the static model, the temporal 		EYEDIAP (floating target)
model particularly benefits gaze targets 		EYEDIAP (floating target)
from mid-range upwards. Its contribution 		EYEDIAP (floating target)
is less clear for extreme 		EYEDIAP (floating target)
targets, probably again due to 		EYEDIAP (floating target)
data imbalance.  Finally, we evaluated the effect		EYEDIAP (floating target)
 of different recurrent architectures for		EYEDIAP (floating target)
 the temporal model. In particular		EYEDIAP (floating target)
, we tested 1 (128 		EYEDIAP (floating target)
units) and 2 (256-128 units) 		EYEDIAP (floating target)
LSTM and GRU lay- ers, 		EYEDIAP (floating target)
with 1 GRU layer obtaining 		EYEDIAP (floating target)
slightly superior results (up to 0		EYEDIAP (floating target)
.12◦). We also assessed the 		EYEDIAP (floating target)
effect of sequence length fixing 		EYEDIAP (floating target)
s in the range {4,7,10}, 		EYEDIAP (floating target)
with s = 7 performing 		EYEDIAP (floating target)
worse than the other two (		EYEDIAP (floating target)
up to 0		EYEDIAP (floating target)
		EYEDIAP (floating target)
14		EYEDIAP (floating target)
		EYEDIAP (floating target)
10 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (floating target)
ESTIMATION  5 Conclusions In this work		EYEDIAP (floating target)
, we studied the combination 		EYEDIAP (floating target)
of full-face and eye images 		EYEDIAP (floating target)
along with facial land- marks 		EYEDIAP (floating target)
for person- and head pose-independent 		EYEDIAP (floating target)
3D gaze estimation. Consequently, we 		EYEDIAP (floating target)
pro- posed a multi-stream recurrent 		EYEDIAP (floating target)
CNN network that leverages the 		EYEDIAP (floating target)
sequential information of eye and 		EYEDIAP (floating target)
head movements. Both static and 		EYEDIAP (floating target)
temporal versions of our approach 		EYEDIAP (floating target)
significantly outperform current state-of-the-art 3D 		EYEDIAP (floating target)
gaze estimation methods on a 		EYEDIAP (floating target)
wide range of head poses 		EYEDIAP (floating target)
and gaze directions. We showed 		EYEDIAP (floating target)
that adding geometry features to 		EYEDIAP (floating target)
appearance-based methods has a regularizing 		EYEDIAP (floating target)
effect on the accuracy. Adding 		EYEDIAP (floating target)
sequential information further benefits the 		EYEDIAP (floating target)
final performance compared to static-only 		EYEDIAP (floating target)
input, especially from mid-range up- 		EYEDIAP (floating target)
wards and in those cases 		EYEDIAP (floating target)
where head motion is present. 		EYEDIAP (floating target)
The effect in very extreme 		EYEDIAP (floating target)
head poses is not clear 		EYEDIAP (floating target)
due to data imbalance, suggesting 		EYEDIAP (floating target)
the importance of learning from 		EYEDIAP (floating target)
a con- tinuous, balanced dataset 		EYEDIAP (floating target)
including all head poses and 		EYEDIAP (floating target)
gaze directions of interest. To 		EYEDIAP (floating target)
the best of our knowledge, 		EYEDIAP (floating target)
this is the first attempt 		EYEDIAP (floating target)
to exploit the temporal modality 		EYEDIAP (floating target)
in the context of gaze 		EYEDIAP (floating target)
estimation from remote cameras. As 		EYEDIAP (floating target)
future work, we will further 		EYEDIAP (floating target)
explore extracting meaningful temporal representations 		EYEDIAP (floating target)
of gaze dynamics, considering 3DCNNs 		EYEDIAP (floating target)
as well as the encoding 		EYEDIAP (floating target)
of deep features around particular 		EYEDIAP (floating target)
tracked face landmarks [14].  Acknowledgements This work has been		EYEDIAP (floating target)
 partially supported by the Spanish		EYEDIAP (floating target)
 project TIN2016-74946-P (MINECO/ FEDER, UE		EYEDIAP (floating target)
), CERCA Programme / Generalitat 		EYEDIAP (floating target)
de Catalunya, and the FP7 		EYEDIAP (floating target)
people program (Marie Curie Actions), 		EYEDIAP (floating target)
REA grant agreement no FP7-607139 (		EYEDIAP (floating target)
iCARE - Improving Children Auditory 		EYEDIAP (floating target)
REhabilitation). We gratefully acknowledge the 		EYEDIAP (floating target)
support of NVIDIA Corporation with 		EYEDIAP (floating target)
the donation of the GPU 		EYEDIAP (floating target)
used for this research. Portions 		EYEDIAP (floating target)
of the research in this 		EYEDIAP (floating target)
pa- per used the EYEDIAP 		EYEDIAP (floating target)
dataset made available by the 		EYEDIAP (floating target)
Idiap Research Institute, Martigny, Switzerland.  References [1] Nicola C Anderson		EYEDIAP (floating target)
, Evan F Risko, and 		EYEDIAP (floating target)
Alan Kingstone. Motion influences gaze 		EYEDIAP (floating target)
di-  rection discrimination and disambiguates contradictory		EYEDIAP (floating target)
 luminance cues. Psychonomic bulletin		EYEDIAP (floating target)
 & review, 23(3):817–823, 2016		EYEDIAP (floating target)
.  [2] Shumeet Baluja and Dean		EYEDIAP (floating target)
 Pomerleau. Non-intrusive gaze tracking using		EYEDIAP (floating target)
 artificial neu- ral networks. In		EYEDIAP (floating target)
 Advances in Neural Information Processing		EYEDIAP (floating target)
 Systems, pages 753–760, 1994		EYEDIAP (floating target)
.  [3] Adrian Bulat and Georgios		EYEDIAP (floating target)
 Tzimiropoulos. How far are we		EYEDIAP (floating target)
 from solving the 2d		EYEDIAP (floating target)
 & 3d face alignment problem		EYEDIAP (floating target)
? (and a dataset of 230,		EYEDIAP (floating target)
000 3d facial landmarks). In 		EYEDIAP (floating target)
Interna- tional Conference on Computer 		EYEDIAP (floating target)
Vision, 2017.  [4] Haoping Deng and Wangjiang		EYEDIAP (floating target)
 Zhu. Monocular free-head 3d gaze		EYEDIAP (floating target)
 tracking with deep learning and		EYEDIAP (floating target)
 geometry constraints. In Computer Vision		EYEDIAP (floating target)
 (ICCV), 2017 IEEE Interna- tional		EYEDIAP (floating target)
 Conference on, pages 3162–3171. IEEE		EYEDIAP (floating target)
, 2017.  [5] Onur Ferhat and Fernando		EYEDIAP (floating target)
 Vilariño. Low cost eye tracking		EYEDIAP (floating target)
. Computational intelligence and neuroscience, 2016		EYEDIAP (floating target)
:17, 2016.  Citation Citation {Jung, Lee, Yim		EYEDIAP (floating target)
, Park, and Kim} 2015		EYEDIAP (floating target)
		EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 11		EYEDIAP (floating target)
		EYEDIAP (floating target)
6] Kenneth A Funes-Mora and 		EYEDIAP (floating target)
Jean-Marc Odobez. Gaze estimation in 		EYEDIAP (floating target)
the 3D space using RGB-D 		EYEDIAP (floating target)
sensors. International Journal of Computer 		EYEDIAP (floating target)
Vision, 118(2):194–216, 2016.  [7] Kenneth Alberto Funes Mora		EYEDIAP (floating target)
, Florent Monay, and Jean-Marc 		EYEDIAP (floating target)
Odobez. Eyediap: A database for 		EYEDIAP (floating target)
the development and evaluation of 		EYEDIAP (floating target)
gaze estimation algorithms from rgb 		EYEDIAP (floating target)
and rgb-d cameras. In Proceedings 		EYEDIAP (floating target)
of the ACM Symposium on 		EYEDIAP (floating target)
Eye Tracking Research and Applications. 		EYEDIAP (floating target)
ACM, March 2014. doi: 10.1145/2578153.2578190.  [8] Kenneth Alberto Funes Mora		EYEDIAP (floating target)
, Florent Monay, and Jean-Marc 		EYEDIAP (floating target)
Odobez. Eyediap: A database for 		EYEDIAP (floating target)
the development and evaluation of 		EYEDIAP (floating target)
gaze estimation algorithms from rgb 		EYEDIAP (floating target)
and rgb-d cameras. In Proceedings 		EYEDIAP (floating target)
of the Symposium on Eye 		EYEDIAP (floating target)
Tracking Research and Applications, pages 255		EYEDIAP (floating target)
–258. ACM, 2014.  [9] Quentin Guillon, Nouchine Hadjikhani		EYEDIAP (floating target)
, Sophie Baduel, and Bernadette 		EYEDIAP (floating target)
Rogé. Visual social attention in 		EYEDIAP (floating target)
autism spectrum disorder: Insights from 		EYEDIAP (floating target)
eye tracking studies. Neu- 		EYEDIAP (floating target)
roscience & Biobehavioral Reviews, 42:279–297, 2014		EYEDIAP (floating target)
.  [10] Dan Witzner Hansen and		EYEDIAP (floating target)
 Qiang Ji. In the eye		EYEDIAP (floating target)
 of the beholder: A survey		EYEDIAP (floating target)
 of models for eyes and		EYEDIAP (floating target)
 gaze. IEEE transactions on pattern		EYEDIAP (floating target)
 analysis and machine intelligence, 32(3		EYEDIAP (floating target)
): 478–500, 2010.  [11] Qiong Huang, Ashok Veeraraghavan		EYEDIAP (floating target)
, and Ashutosh Sabharwal. Tabletgaze: 		EYEDIAP (floating target)
dataset and analysis for unconstrained 		EYEDIAP (floating target)
appearance-based gaze estimation in mobile 		EYEDIAP (floating target)
tablets. Machine Vision and Applications, 28		EYEDIAP (floating target)
(5-6):445–461, 2017.  [12] Robert JK Jacob and		EYEDIAP (floating target)
 Keith S Karn. Eye tracking		EYEDIAP (floating target)
 in human-computer interaction and usability		EYEDIAP (floating target)
 research: Ready to deliver the		EYEDIAP (floating target)
 promises. In The mind’s eye		EYEDIAP (floating target)
, pages 573–605. Elsevier, 2003.  [13] László A Jeni and		EYEDIAP (floating target)
 Jeffrey F Cohn. Person-independent 3d		EYEDIAP (floating target)
 gaze estimation using face frontalization		EYEDIAP (floating target)
. In Proceedings of the 		EYEDIAP (floating target)
IEEE Conference on Computer Vision 		EYEDIAP (floating target)
and Pattern Recognition Workshops, pages 87		EYEDIAP (floating target)
–95, 2016.  [14] Heechul Jung, Sihaeng Lee		EYEDIAP (floating target)
, Junho Yim, Sunjeong Park, 		EYEDIAP (floating target)
and Junmo Kim. Joint fine- 		EYEDIAP (floating target)
tuning in deep neural networks 		EYEDIAP (floating target)
for facial expression recognition. In 		EYEDIAP (floating target)
Computer Vision (ICCV), 2015 IEEE 		EYEDIAP (floating target)
International Conference on, pages 2983–2991. 		EYEDIAP (floating target)
IEEE, 2015.  [15] Anuradha Kar and Peter		EYEDIAP (floating target)
 Corcoran. A review and analysis		EYEDIAP (floating target)
 of eye-gaze estimation sys- tems		EYEDIAP (floating target)
, algorithms and performance evaluation 		EYEDIAP (floating target)
methods in consumer platforms. IEEE 		EYEDIAP (floating target)
Access, 5:16495–16519, 2017.  [16] Kyle Krafka, Aditya Khosla		EYEDIAP (floating target)
, Petr Kellnhofer, Harini Kannan, 		EYEDIAP (floating target)
Suchendra Bhandarkar, Wojciech Matusik, and 		EYEDIAP (floating target)
Antonio Torralba. Eye tracking for 		EYEDIAP (floating target)
everyone. In Computer Vision and 		EYEDIAP (floating target)
Pattern Recognition (CVPR), 2016 IEEE 		EYEDIAP (floating target)
Conference on, pages 2176–2184. IEEE, 2016		EYEDIAP (floating target)
.  [17] Simon P Liversedge and		EYEDIAP (floating target)
 John M Findlay. Saccadic eye		EYEDIAP (floating target)
 movements and cognition. Trends in		EYEDIAP (floating target)
 cognitive sciences, 4(1):6–14, 2000		EYEDIAP (floating target)
.  [18] Feng Lu, Takahiro Okabe		EYEDIAP (floating target)
, Yusuke Sugano, and Yoichi 		EYEDIAP (floating target)
Sato. A head pose-free approach 		EYEDIAP (floating target)
for appearance-based gaze estimation. In 		EYEDIAP (floating target)
BMVC, pages 1–11, 2011		EYEDIAP (floating target)
		EYEDIAP (floating target)
12 PALMERO ET AL.: MULTI-MODAL 		EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 		EYEDIAP (floating target)
ESTIMATION  [19] Feng Lu, Yusuke Sugano		EYEDIAP (floating target)
, Takahiro Okabe, and Yoichi 		EYEDIAP (floating target)
Sato. Inferring human gaze from 		EYEDIAP (floating target)
appearance via adaptive linear regression. 		EYEDIAP (floating target)
In Computer Vision (ICCV), 2011 		EYEDIAP (floating target)
IEEE International Conference on, pages 153		EYEDIAP (floating target)
–160. IEEE, 2011.  [20] Päivi Majaranta and Andreas		EYEDIAP (floating target)
 Bulling. Eye tracking and eye-based		EYEDIAP (floating target)
 human–computer interaction. In Advances in		EYEDIAP (floating target)
 physiological computing, pages 39–65. Springer		EYEDIAP (floating target)
, 2014.  [21] Kenneth Alberto Funes Mora		EYEDIAP (floating target)
 and Jean-Marc Odobez. Gaze estimation		EYEDIAP (floating target)
 from multi- modal kinect data		EYEDIAP (floating target)
. In Computer Vision and 		EYEDIAP (floating target)
Pattern Recognition Workshops (CVPRW), 2012 		EYEDIAP (floating target)
IEEE Computer Society Conference on, 		EYEDIAP (floating target)
pages 25–30. IEEE, 2012.  [22] Carlos Hitoshi Morimoto, Arnon		EYEDIAP (floating target)
 Amir, and Myron Flickner. Detecting		EYEDIAP (floating target)
 eye position and gaze from		EYEDIAP (floating target)
 a single camera and 2		EYEDIAP (floating target)
 light sources. In Pattern Recognition		EYEDIAP (floating target)
, 2002. Proceedings. 16th International 		EYEDIAP (floating target)
Conference on, volume 4, pages 314		EYEDIAP (floating target)
–317. IEEE, 2002.  [23] IMO MSC. Circ. 982		EYEDIAP (floating target)
 (2000) guidelines on ergonomic criteria		EYEDIAP (floating target)
 for bridge equipment and layout		EYEDIAP (floating target)
.  [24] Alejandro Newell, Kaiyu Yang		EYEDIAP (floating target)
, and Jia Deng. Stacked 		EYEDIAP (floating target)
hourglass networks for hu- man 		EYEDIAP (floating target)
pose estimation. In European Conference 		EYEDIAP (floating target)
on Computer Vision, pages 483–499. 		EYEDIAP (floating target)
Springer, 2016.  [25] Yasuhiro Ono, Takahiro Okabe		EYEDIAP (floating target)
, and Yoichi Sato. Gaze 		EYEDIAP (floating target)
estimation from low resolution images. 		EYEDIAP (floating target)
In Pacific-Rim Symposium on Image 		EYEDIAP (floating target)
and Video Technology, pages 178–188. 		EYEDIAP (floating target)
Springer, 2006.  [26] Cristina Palmero, Elisabeth A		EYEDIAP (floating target)
. van Dam, Sergio Escalera, 		EYEDIAP (floating target)
Mike Kelia, Guido F. Lichtert, 		EYEDIAP (floating target)
Lucas P.J.J Noldus, Andrew J. 		EYEDIAP (floating target)
Spink, and Astrid van Wieringen. 		EYEDIAP (floating target)
Automatic mutual gaze detection in 		EYEDIAP (floating target)
face-to-face dyadic interaction videos. In 		EYEDIAP (floating target)
Proceedings of Measuring Behavior, pages 158		EYEDIAP (floating target)
–163, 2018.  [27] Omkar M. Parkhi, Andrea		EYEDIAP (floating target)
 Vedaldi, and Andrew Zisserman. Deep		EYEDIAP (floating target)
 face recognition. In British Machine		EYEDIAP (floating target)
 Vision Conference, 2015		EYEDIAP (floating target)
.  [28] Derek R Rutter and		EYEDIAP (floating target)
 Kevin Durkin. Turn-taking in mother–infant		EYEDIAP (floating target)
 interaction: An exam- ination of		EYEDIAP (floating target)
 vocalizations and gaze. Developmental psychology		EYEDIAP (floating target)
, 23(1):54, 1987.  [29] Brian A Smith, Qi		EYEDIAP (floating target)
 Yin, Steven K Feiner, and		EYEDIAP (floating target)
 Shree K Nayar. Gaze locking		EYEDIAP (floating target)
: passive eye contact detection 		EYEDIAP (floating target)
for human-object interaction. In Proceedings 		EYEDIAP (floating target)
of the 26th annual ACM 		EYEDIAP (floating target)
symposium on User interface software 		EYEDIAP (floating target)
and technology, pages 271–280. ACM, 2013		EYEDIAP (floating target)
.  [30] Yusuke Sugano, Yasuyuki Matsushita		EYEDIAP (floating target)
, and Yoichi Sato. Appearance-based 		EYEDIAP (floating target)
gaze es- timation using visual 		EYEDIAP (floating target)
saliency. IEEE transactions on pattern 		EYEDIAP (floating target)
analysis and machine intelligence, 35(2):329–341, 2013		EYEDIAP (floating target)
.  [31] Yusuke Sugano, Yasuyuki Matsushita		EYEDIAP (floating target)
, and Yoichi Sato. Learning-by-synthesis 		EYEDIAP (floating target)
for appearance-based 3d gaze estimation. 		EYEDIAP (floating target)
In Computer Vision and Pattern 		EYEDIAP (floating target)
Recognition (CVPR), 2014 IEEE Conference 		EYEDIAP (floating target)
on, pages 1821–1828. IEEE, 2014.  [32] Kar-Han Tan, David J		EYEDIAP (floating target)
 Kriegman, and Narendra Ahuja. Appearance-based		EYEDIAP (floating target)
 eye gaze es- timation. In		EYEDIAP (floating target)
 Applications of Computer Vision, 2002.(WACV		EYEDIAP (floating target)
 2002). Proceedings. Sixth IEEE Workshop		EYEDIAP (floating target)
 on, pages 191–195. IEEE, 2002		EYEDIAP (floating target)
		EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 		EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 13		EYEDIAP (floating target)
		EYEDIAP (floating target)
33] Ronda Venkateswarlu et al. 		EYEDIAP (floating target)
Eye gaze estimation from a 		EYEDIAP (floating target)
single image of one eye. 		EYEDIAP (floating target)
In Computer Vision, 2003. Proceedings. 		EYEDIAP (floating target)
Ninth IEEE International Conference on, 		EYEDIAP (floating target)
pages 136–143. IEEE, 2003.  [34] Kang Wang and Qiang		EYEDIAP (floating target)
 Ji. Real time eye gaze		EYEDIAP (floating target)
 tracking with 3d deformable eye-face		EYEDIAP (floating target)
 model. In Proceedings of the		EYEDIAP (floating target)
 IEEE Conference on Computer Vision		EYEDIAP (floating target)
 and Pattern Recog- nition, pages		EYEDIAP (floating target)
 1003–1011, 2017		EYEDIAP (floating target)
.  [35] Oliver Williams, Andrew Blake		EYEDIAP (floating target)
, and Roberto Cipolla. Sparse 		EYEDIAP (floating target)
and semi-supervised visual mapping with 		EYEDIAP (floating target)
the sˆ 3gp. In Computer 		EYEDIAP (floating target)
Vision and Pattern Recognition, 2006 		EYEDIAP (floating target)
IEEE Computer Society Conference on, 		EYEDIAP (floating target)
volume 1, pages 230–237. IEEE, 2006		EYEDIAP (floating target)
.  [36] William Hyde Wollaston et		EYEDIAP (floating target)
 al. Xiii. on the apparent		EYEDIAP (floating target)
 direction of eyes in a		EYEDIAP (floating target)
 portrait. Philosophical Transactions of the		EYEDIAP (floating target)
 Royal Society of London, 114:247–256		EYEDIAP (floating target)
, 1824.  [37] Erroll Wood and Andreas		EYEDIAP (floating target)
 Bulling. Eyetab: Model-based gaze estimation		EYEDIAP (floating target)
 on unmodi- fied tablet computers		EYEDIAP (floating target)
. In Proceedings of the 		EYEDIAP (floating target)
Symposium on Eye Tracking Research 		EYEDIAP (floating target)
and Applications, pages 207–210. ACM, 2014		EYEDIAP (floating target)
.  [38] Erroll Wood, Tadas Baltrusaitis		EYEDIAP (floating target)
, Xucong Zhang, Yusuke Sugano, 		EYEDIAP (floating target)
Peter Robinson, and Andreas Bulling. 		EYEDIAP (floating target)
Rendering of eyes for eye-shape 		EYEDIAP (floating target)
registration and gaze estimation. In 		EYEDIAP (floating target)
Proceedings of the IEEE International 		EYEDIAP (floating target)
Conference on Computer Vision, pages 3756		EYEDIAP (floating target)
– 3764, 2015.  [39] Erroll Wood, Tadas Baltrušaitis		EYEDIAP (floating target)
, Louis-Philippe Morency, Peter Robinson, 		EYEDIAP (floating target)
and Andreas Bulling. A 3d 		EYEDIAP (floating target)
morphable eye region model for 		EYEDIAP (floating target)
gaze estimation. In European Confer- 		EYEDIAP (floating target)
ence on Computer Vision, pages 297		EYEDIAP (floating target)
–313. Springer, 2016.  [40] Erroll Wood, Tadas Baltrušaitis		EYEDIAP (floating target)
, Louis-Philippe Morency, Peter Robinson, 		EYEDIAP (floating target)
and Andreas Bulling. Learning an 		EYEDIAP (floating target)
appearance-based gaze estimator from one 		EYEDIAP (floating target)
million synthesised images. In Proceedings 		EYEDIAP (floating target)
of the Ninth Biennial ACM 		EYEDIAP (floating target)
Symposium on Eye Tracking Re- 		EYEDIAP (floating target)
search & Applications, pages 131–138. 		EYEDIAP (floating target)
ACM, 2016.  [41] Dong Hyun Yoo and		EYEDIAP (floating target)
 Myung Jin Chung. A novel		EYEDIAP (floating target)
 non-intrusive eye gaze estimation using		EYEDIAP (floating target)
 cross-ratio under large head motion		EYEDIAP (floating target)
. Computer Vision and Image 		EYEDIAP (floating target)
Understanding, 98(1):25–51, 2005.  [42] Xucong Zhang, Yusuke Sugano		EYEDIAP (floating target)
, Mario Fritz, and Andreas 		EYEDIAP (floating target)
Bulling. Appearance-based gaze estimation in 		EYEDIAP (floating target)
the wild. In Proceedings of 		EYEDIAP (floating target)
the IEEE Conference on Computer 		EYEDIAP (floating target)
Vision and Pattern Recognition, pages 4511		EYEDIAP (floating target)
–4520, 2015.  [43] Xucong Zhang, Yusuke Sugano		EYEDIAP (floating target)
, Mario Fritz, and Andreas 		EYEDIAP (floating target)
Bulling. It’s written all over 		EYEDIAP (floating target)
your face: Full-face appearance-based gaze 		EYEDIAP (floating target)
estimation. In Proc. IEEE International 		EYEDIAP (floating target)
Conference on Computer Vision and 		EYEDIAP (floating target)
Pattern Recognition Workshops (CVPRW), 2017		EYEDIAP (floating target)
		EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 1	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Recurrent CNN for 3D Gaze 	target)	EYEDIAP (floating target)
Estimation using Appearance and Shape 	target)	EYEDIAP (floating target)
Cues  Cristina Palmero1,2	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
crpalmec7@alumnes.ub.edu  1 Dept. Mathematics and Informatics	target)	EYEDIAP (floating target)
 Universitat de Barcelona, Spain	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Javier Selva1  javier.selva.castello@est.fib.upc.edu	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
2 Computer Vision Center Campus 	target)	EYEDIAP (floating target)
UAB, Bellaterra, Spain  Mohammad Ali Bagheri3,4	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
mohammadali.bagheri@ucalgary.ca  3 Dept. Electrical and Computer	target)	EYEDIAP (floating target)
 Eng. University of Calgary, Canada	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Sergio Escalera1,2  sergio@maia.ub.es	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
4 Dept. Engineering University of 	target)	EYEDIAP (floating target)
Larestan, Iran  Abstract	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Gaze behavior is an important 	target)	EYEDIAP (floating target)
non-verbal cue in social signal 	target)	EYEDIAP (floating target)
processing and human- computer interaction. 	target)	EYEDIAP (floating target)
In this paper, we tackle 	target)	EYEDIAP (floating target)
the problem of person- and 	target)	EYEDIAP (floating target)
head pose- independent 3D gaze 	target)	EYEDIAP (floating target)
estimation from remote cameras, using 	target)	EYEDIAP (floating target)
a multi-modal recurrent convolutional neural 	target)	EYEDIAP (floating target)
network (CNN). We propose to 	target)	EYEDIAP (floating target)
combine face, eyes region, and 	target)	EYEDIAP (floating target)
face landmarks as individual streams 	target)	EYEDIAP (floating target)
in a CNN to estimate 	target)	EYEDIAP (floating target)
gaze in still images. Then, 	target)	EYEDIAP (floating target)
we exploit the dynamic nature 	target)	EYEDIAP (floating target)
of gaze by feeding the 	target)	EYEDIAP (floating target)
learned features of all the 	target)	EYEDIAP (floating target)
frames in a sequence to 	target)	EYEDIAP (floating target)
a many-to-one recurrent module that 	target)	EYEDIAP (floating target)
predicts the 3D gaze vector 	target)	EYEDIAP (floating target)
of the last frame. Our 	target)	EYEDIAP (floating target)
multi-modal static solution is evaluated 	target)	EYEDIAP (floating target)
on a wide range of 	target)	EYEDIAP (floating target)
head poses and gaze directions, 	target)	EYEDIAP (floating target)
achieving a significant improvement of 14	target)	EYEDIAP (floating target)
.6% over the state of 	target)	EYEDIAP (floating target)
the art on EYEDIAP dataset, 	target)	EYEDIAP (floating target)
further improved by 4% when 	target)	EYEDIAP (floating target)
the temporal modality is included.  1 Introduction Eyes and their	target)	EYEDIAP (floating target)
 movements are considered an important	target)	EYEDIAP (floating target)
 cue in non-verbal behavior analysis	target)	EYEDIAP (floating target)
, being involved in many 	target)	EYEDIAP (floating target)
cognitive processes and reflecting our 	target)	EYEDIAP (floating target)
internal state [17]. More specifically, 	target)	EYEDIAP (floating target)
eye gaze behavior, as an 	target)	EYEDIAP (floating target)
indicator of human visual attention, 	target)	EYEDIAP (floating target)
has been widely studied to 	target)	EYEDIAP (floating target)
assess communication skills [28] and 	target)	EYEDIAP (floating target)
to identify possible behavioral 	target)	EYEDIAP (floating target)
disorders [9]. Therefore, gaze estimation 	target)	EYEDIAP (floating target)
has become an established line 	target)	EYEDIAP (floating target)
of research in computer vision, 	target)	EYEDIAP (floating target)
being a key feature in 	target)	EYEDIAP (floating target)
human-computer interaction (HCI) and usability 	target)	EYEDIAP (floating target)
research [12, 20].  Recent gaze estimation research has	target)	EYEDIAP (floating target)
 focused on facilitating its use	target)	EYEDIAP (floating target)
 in general everyday applications under	target)	EYEDIAP (floating target)
 real-world conditions, using off-the-shelf remote	target)	EYEDIAP (floating target)
 RGB cameras and re- moving	target)	EYEDIAP (floating target)
 the need of personal calibration	target)	EYEDIAP (floating target)
 [26]. In this setting, appearance-based	target)	EYEDIAP (floating target)
 methods, which learn a mapping	target)	EYEDIAP (floating target)
 from images to gaze directions	target)	EYEDIAP (floating target)
, are the preferred 	target)	EYEDIAP (floating target)
choice [25]. How- ever, they 	target)	EYEDIAP (floating target)
need large amounts of training 	target)	EYEDIAP (floating target)
data to be able to 	target)	EYEDIAP (floating target)
generalize well to in-the-wild situations, 	target)	EYEDIAP (floating target)
which are characterized by significant 	target)	EYEDIAP (floating target)
variability in head poses, face 	target)	EYEDIAP (floating target)
appearances and lighting conditions. In 	target)	EYEDIAP (floating target)
recent years, CNNs have been 	target)	EYEDIAP (floating target)
reported to outperform classical methods. 	target)	EYEDIAP (floating target)
However, most existing approaches have 	target)	EYEDIAP (floating target)
only been tested in restricted 	target)	EYEDIAP (floating target)
HCI tasks,  c© 2018. The copyright of	target)	EYEDIAP (floating target)
 this document resides with its	target)	EYEDIAP (floating target)
 authors. It may be distributed	target)	EYEDIAP (floating target)
 unchanged freely in print or	target)	EYEDIAP (floating target)
 electronic forms	target)	EYEDIAP (floating target)
.  ar X	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
iv :1  80 5	target)	EYEDIAP (floating target)
.  03 06	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
4v 3	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
cs  .C V	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
1  7	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Se  p	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
20  18	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Liversedge and Findlay} 2000	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Rutter and Durkin} 1987	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Guillon, Hadjikhani, Baduel, 	target)	EYEDIAP (floating target)
and Rog{é}} 2014  Citation Citation {Jacob and Karn	target)	EYEDIAP (floating target)
} 2003  Citation Citation {Majaranta and Bulling	target)	EYEDIAP (floating target)
} 2014  Citation Citation {Palmero, van Dam	target)	EYEDIAP (floating target)
, Escalera, Kelia, Lichtert, Noldus, 	target)	EYEDIAP (floating target)
Spink, and van Wieringen} 2018  Citation Citation {Ono, Okabe, and	target)	EYEDIAP (floating target)
 Sato} 2006	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
2 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (floating target)
ESTIMATION  Method 3D gaze direction	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Unrestricted gaze target  Full face	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Eye region  Facial landmarks	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Sequential information  Zhang et al. (1) [42	target)	EYEDIAP (floating target)
] 3 7 7 3 7 7 Krafka et al. [16	target)	EYEDIAP (floating target)
] 7 7 3 3 7 7 Zhang et al. (2	target)	EYEDIAP (floating target)
) [43] 3 7 3 7 7 7 Deng and Zhu	target)	EYEDIAP (floating target)
 [4] 3 3 3 3	target)	EYEDIAP (floating target)
 7 7 Ours 3 3	target)	EYEDIAP (floating target)
 3 3 3 3	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Table 1: Characteristics of recent 	target)	EYEDIAP (floating target)
related work on person- and 	target)	EYEDIAP (floating target)
head pose-independent appearance-based gaze estimation 	target)	EYEDIAP (floating target)
methods using CNNs.  where users look at the	target)	EYEDIAP (floating target)
 screen or mobile phone, showing	target)	EYEDIAP (floating target)
 a low head pose variability	target)	EYEDIAP (floating target)
. It is yet unclear 	target)	EYEDIAP (floating target)
how these methods would perform 	target)	EYEDIAP (floating target)
in a wider range of 	target)	EYEDIAP (floating target)
head poses.  On a different note, until	target)	EYEDIAP (floating target)
 very recently, the majority of	target)	EYEDIAP (floating target)
 methods only used static eye	target)	EYEDIAP (floating target)
 region appearance as input. State-of-the-art	target)	EYEDIAP (floating target)
 approaches have demonstrated that using	target)	EYEDIAP (floating target)
 the face along with a	target)	EYEDIAP (floating target)
 higher resolution image of the	target)	EYEDIAP (floating target)
 eyes [16], or even just	target)	EYEDIAP (floating target)
 the face itself [43], increases	target)	EYEDIAP (floating target)
 performance. Indeed, the whole-face image	target)	EYEDIAP (floating target)
 encodes more information than eyes	target)	EYEDIAP (floating target)
 alone, such as illumination and	target)	EYEDIAP (floating target)
 head pose. Nevertheless, gaze behavior	target)	EYEDIAP (floating target)
 is not static. Eye and	target)	EYEDIAP (floating target)
 head movements allow us to	target)	EYEDIAP (floating target)
 direct our gaze to target	target)	EYEDIAP (floating target)
 locations of interest. It has	target)	EYEDIAP (floating target)
 been demonstrated that humans can	target)	EYEDIAP (floating target)
 better predict gaze when being	target)	EYEDIAP (floating target)
 shown image sequences of other	target)	EYEDIAP (floating target)
 people moving their eyes [1	target)	EYEDIAP (floating target)
]. However, it is still 	target)	EYEDIAP (floating target)
an open question whether this 	target)	EYEDIAP (floating target)
se- quential information can increase 	target)	EYEDIAP (floating target)
the performance of automatic methods.  In this work, we show	target)	EYEDIAP (floating target)
 that the combination of multiple	target)	EYEDIAP (floating target)
 cues benefits the gaze estimation	target)	EYEDIAP (floating target)
 task. In particular, we use	target)	EYEDIAP (floating target)
 face, eye region and facial	target)	EYEDIAP (floating target)
 landmarks from still images. Facial	target)	EYEDIAP (floating target)
 landmarks model the global shape	target)	EYEDIAP (floating target)
 of the face and come	target)	EYEDIAP (floating target)
 at no cost, since face	target)	EYEDIAP (floating target)
 alignment is a common pre-processing	target)	EYEDIAP (floating target)
 step in many facial image	target)	EYEDIAP (floating target)
 analysis approaches. Furthermore, we present	target)	EYEDIAP (floating target)
 a subject-independent, free-head recurrent 3D	target)	EYEDIAP (floating target)
 gaze regression network to leverage	target)	EYEDIAP (floating target)
 the temporal information of image	target)	EYEDIAP (floating target)
 sequences. The static streams of	target)	EYEDIAP (floating target)
 each frame are combined in	target)	EYEDIAP (floating target)
 a late-fusion fashion using a	target)	EYEDIAP (floating target)
 multi-stream CNN. Then, all feature	target)	EYEDIAP (floating target)
 vectors are input to a	target)	EYEDIAP (floating target)
 many-to-one recurrent module that predicts	target)	EYEDIAP (floating target)
 the gaze vector of the	target)	EYEDIAP (floating target)
 last sequence frame	target)	EYEDIAP (floating target)
.  In summary, our contributions are	target)	EYEDIAP (floating target)
 two-fold. First, we present a	target)	EYEDIAP (floating target)
 Recurrent-CNN net- work architecture that	target)	EYEDIAP (floating target)
 combines appearance, shape and temporal	target)	EYEDIAP (floating target)
 information for 3D gaze estimation	target)	EYEDIAP (floating target)
. Second, we test static 	target)	EYEDIAP (floating target)
and temporal versions of our 	target)	EYEDIAP (floating target)
solution on the EYEDIAP 	target)	EYEDIAP (floating target)
dataset [7] in a wide 	target)	EYEDIAP (floating target)
range of head poses and 	target)	EYEDIAP (floating target)
gaze directions, showing consistent perfor- 	target)	EYEDIAP (floating target)
mance improvements compared to related 	target)	EYEDIAP (floating target)
appearance-based methods. To the best 	target)	EYEDIAP (floating target)
of our knowledge, this is 	target)	EYEDIAP (floating target)
the first third-person, remote camera-based 	target)	EYEDIAP (floating target)
approach that uses tempo- ral 	target)	EYEDIAP (floating target)
information for this task. Table 1 outlines our main method characteristics	target)	EYEDIAP (floating target)
 compared to related work. Models	target)	EYEDIAP (floating target)
 and code are publicly available	target)	EYEDIAP (floating target)
 at https://github.com/ crisie/RecurrentGaze	target)	EYEDIAP (floating target)
.  2 Related work Gaze estimation	target)	EYEDIAP (floating target)
 methods are typically categorized as	target)	EYEDIAP (floating target)
 model-based or appearance-based [5, 10	target)	EYEDIAP (floating target)
, 15]. Model-based approaches use 	target)	EYEDIAP (floating target)
a geometric model of the 	target)	EYEDIAP (floating target)
eye, usually requir- ing either 	target)	EYEDIAP (floating target)
high resolution images or a 	target)	EYEDIAP (floating target)
person-specific calibration stage to estimate 	target)	EYEDIAP (floating target)
personal eye parameters [22, 33, 34, 37, 41]. In contrast, appearance-based	target)	EYEDIAP (floating target)
 methods learn a di- rect	target)	EYEDIAP (floating target)
 mapping from intensity images or	target)	EYEDIAP (floating target)
 extracted eye features to gaze	target)	EYEDIAP (floating target)
 directions, thus being	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Zhang, Sugano, Fritz, 	target)	EYEDIAP (floating target)
and Bulling} 2015  Citation Citation {Krafka, Khosla, Kellnhofer	target)	EYEDIAP (floating target)
, Kannan, Bhandarkar, Matusik, and 	target)	EYEDIAP (floating target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (floating target)
, and Bulling} 2017  Citation Citation {Deng and Zhu	target)	EYEDIAP (floating target)
} 2017  Citation Citation {Krafka, Khosla, Kellnhofer	target)	EYEDIAP (floating target)
, Kannan, Bhandarkar, Matusik, and 	target)	EYEDIAP (floating target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (floating target)
, and Bulling} 2017  Citation Citation {Anderson, Risko, and	target)	EYEDIAP (floating target)
 Kingstone} 2016	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Funesprotect unhbox voidb@x 	target)	EYEDIAP (floating target)
penalty @M  {}Mora, Monay, and Odobez} 2014	target)	EYEDIAP (floating target)
{}  Citation Citation {Ferhat and Vilari{ñ}o	target)	EYEDIAP (floating target)
} 2016  Citation Citation {Hansen and Ji	target)	EYEDIAP (floating target)
} 2010  Citation Citation {Kar and Corcoran	target)	EYEDIAP (floating target)
} 2017  Citation Citation {Morimoto, Amir, and	target)	EYEDIAP (floating target)
 Flickner} 2002	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Venkateswarlu etprotect unhbox 	target)	EYEDIAP (floating target)
voidb@x penalty @M  {}al.} 2003	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Wang and Ji} 2017	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Wood and Bulling} 2014	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Yoo and Chung} 2005	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
https://github.com/crisie/RecurrentGaze 	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 3	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
potentially applicable to relatively low 	target)	EYEDIAP (floating target)
resolution images and mid-distance scenarios. 	target)	EYEDIAP (floating target)
Dif- ferent mapping functions have 	target)	EYEDIAP (floating target)
been explored, such as neural 	target)	EYEDIAP (floating target)
networks [2], adaptive linear regression (	target)	EYEDIAP (floating target)
ALR) [19], local interpolation [32], 	target)	EYEDIAP (floating target)
gaussian processes [30, 35], random 	target)	EYEDIAP (floating target)
forests [11, 31], or k-nearest 	target)	EYEDIAP (floating target)
neighbors [40]. Main challenges of 	target)	EYEDIAP (floating target)
appearance-based methods for 3D gaze 	target)	EYEDIAP (floating target)
estimation are head pose, illumination 	target)	EYEDIAP (floating target)
and subject invariance without user-specific 	target)	EYEDIAP (floating target)
calibration. To handle these issues, 	target)	EYEDIAP (floating target)
some works proposed compensation 	target)	EYEDIAP (floating target)
methods [18] and warping strategies 	target)	EYEDIAP (floating target)
that synthesize a canonical, frontal 	target)	EYEDIAP (floating target)
looking view of the 	target)	EYEDIAP (floating target)
face [6, 13, 21]. Hybrid 	target)	EYEDIAP (floating target)
approaches based on analysis-by-synthesis have 	target)	EYEDIAP (floating target)
also been evaluated [39].  Currently, data-driven methods are considered	target)	EYEDIAP (floating target)
 the state of the art	target)	EYEDIAP (floating target)
 for person- and head pose-independent	target)	EYEDIAP (floating target)
 appearance-based gaze estimation. Consequently, a	target)	EYEDIAP (floating target)
 number of gaze es- timation	target)	EYEDIAP (floating target)
 datasets have been introduced in	target)	EYEDIAP (floating target)
 recent years, either in controlled	target)	EYEDIAP (floating target)
 [29] or semi- controlled settings	target)	EYEDIAP (floating target)
 [8], in the wild [16	target)	EYEDIAP (floating target)
, 42], or consisting of 	target)	EYEDIAP (floating target)
synthetic data [31, 38, 40]. 	target)	EYEDIAP (floating target)
Zhang et al. [42] showed 	target)	EYEDIAP (floating target)
that CNNs can outperform other 	target)	EYEDIAP (floating target)
mapping methods, using a multi- 	target)	EYEDIAP (floating target)
modal CNN to learn the 	target)	EYEDIAP (floating target)
mapping from 3D head poses 	target)	EYEDIAP (floating target)
and eye images to 3D 	target)	EYEDIAP (floating target)
gaze directions. Krafka et 	target)	EYEDIAP (floating target)
al. [16] proposed a multi-stream 	target)	EYEDIAP (floating target)
CNN for 2D gaze estimation, 	target)	EYEDIAP (floating target)
using individual eye, whole-face image 	target)	EYEDIAP (floating target)
and the face grid as 	target)	EYEDIAP (floating target)
input. As this method was 	target)	EYEDIAP (floating target)
limited to 2D screen mapping, 	target)	EYEDIAP (floating target)
Zhang et al. [43] later 	target)	EYEDIAP (floating target)
explored the potential of just 	target)	EYEDIAP (floating target)
using whole-face images as input 	target)	EYEDIAP (floating target)
to estimate 3D gaze directions. 	target)	EYEDIAP (floating target)
Using a spatial weights CNN, 	target)	EYEDIAP (floating target)
they demonstrated their method to 	target)	EYEDIAP (floating target)
be more robust to facial 	target)	EYEDIAP (floating target)
appearance variation caused by head 	target)	EYEDIAP (floating target)
pose and illumina- tion than 	target)	EYEDIAP (floating target)
eye-only methods. While the method 	target)	EYEDIAP (floating target)
was evaluated in the wild, 	target)	EYEDIAP (floating target)
the subjects were only interacting 	target)	EYEDIAP (floating target)
with a mobile device, thus 	target)	EYEDIAP (floating target)
restricting the head pose range. 	target)	EYEDIAP (floating target)
Deng and Zhu [4] presented 	target)	EYEDIAP (floating target)
a two-stream CNN to disjointly 	target)	EYEDIAP (floating target)
model head pose from face 	target)	EYEDIAP (floating target)
images and eye- ball movement 	target)	EYEDIAP (floating target)
from eye region images. Both 	target)	EYEDIAP (floating target)
were then aggregated into 3D 	target)	EYEDIAP (floating target)
gaze direction using a gaze 	target)	EYEDIAP (floating target)
transform layer. The decomposition was 	target)	EYEDIAP (floating target)
aimed to avoid head-correlation over- 	target)	EYEDIAP (floating target)
fitting of previous data-driven approaches. 	target)	EYEDIAP (floating target)
They evaluated their approach in 	target)	EYEDIAP (floating target)
the wild with a wider 	target)	EYEDIAP (floating target)
range of head poses, obtaining 	target)	EYEDIAP (floating target)
better performance than previous eye-based 	target)	EYEDIAP (floating target)
methods. However, they did not 	target)	EYEDIAP (floating target)
test it on public annotated 	target)	EYEDIAP (floating target)
benchmark datasets.  In this paper, we propose	target)	EYEDIAP (floating target)
 a multi-stream recurrent CNN network	target)	EYEDIAP (floating target)
 for person- and head pose-independent	target)	EYEDIAP (floating target)
 3D gaze estimation for a	target)	EYEDIAP (floating target)
 mid-distance scenario. We evaluate it	target)	EYEDIAP (floating target)
 on a wider range of	target)	EYEDIAP (floating target)
 head poses and gaze directions	target)	EYEDIAP (floating target)
 than screen-targeted approaches. As opposed	target)	EYEDIAP (floating target)
 to previous methods, we also	target)	EYEDIAP (floating target)
 rely on temporal information inherent	target)	EYEDIAP (floating target)
 in sequential data	target)	EYEDIAP (floating target)
.  3 Methodology	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
In this section, we present 	target)	EYEDIAP (floating target)
our approach for 3D gaze 	target)	EYEDIAP (floating target)
regression based on appearance and 	target)	EYEDIAP (floating target)
shape cues for still images 	target)	EYEDIAP (floating target)
and image sequences. First, we 	target)	EYEDIAP (floating target)
introduce the data modalities and 	target)	EYEDIAP (floating target)
formulate the problem. Then, we 	target)	EYEDIAP (floating target)
detail the normalization procedure prior 	target)	EYEDIAP (floating target)
to the regression stage. Finally, 	target)	EYEDIAP (floating target)
we explain the global network 	target)	EYEDIAP (floating target)
topology as well as the 	target)	EYEDIAP (floating target)
implementation details. An overview of 	target)	EYEDIAP (floating target)
the system architecture is depicted 	target)	EYEDIAP (floating target)
in Figure 1.  3.1 Multi-modal gaze regression	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Let us represent gaze direction 	target)	EYEDIAP (floating target)
as a 3D unit vector 	target)	EYEDIAP (floating target)
g = [gx,gy,gz]T ∈R3 in 	target)	EYEDIAP (floating target)
the Camera Coor- dinate System (	target)	EYEDIAP (floating target)
CCS), whose origin is the 	target)	EYEDIAP (floating target)
central point between eyeball centers. 	target)	EYEDIAP (floating target)
Assuming a calibrated camera, and 	target)	EYEDIAP (floating target)
a known head position and 	target)	EYEDIAP (floating target)
orientation, our goal is to 	target)	EYEDIAP (floating target)
estimate g from a sequence 	target)	EYEDIAP (floating target)
of images {I(i) | 	target)	EYEDIAP (floating target)
I ∈ RW×H×3} as a 	target)	EYEDIAP (floating target)
regression problem.  Citation Citation {Baluja and Pomerleau	target)	EYEDIAP (floating target)
} 1994  Citation Citation {Lu, Sugano, Okabe	target)	EYEDIAP (floating target)
, and Sato} 2011{}  Citation Citation {Tan, Kriegman, and	target)	EYEDIAP (floating target)
 Ahuja} 2002	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Sugano, Matsushita, and 	target)	EYEDIAP (floating target)
Sato} 2013  Citation Citation {Williams, Blake, and	target)	EYEDIAP (floating target)
 Cipolla} 2006	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Huang, Veeraraghavan, and 	target)	EYEDIAP (floating target)
Sabharwal} 2017  Citation Citation {Sugano, Matsushita, and	target)	EYEDIAP (floating target)
 Sato} 2014	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Wood, Baltru{²}aitis, Morency, 	target)	EYEDIAP (floating target)
Robinson, and Bulling} 2016{}  Citation Citation {Lu, Okabe, Sugano	target)	EYEDIAP (floating target)
, and Sato} 2011{}  Citation Citation {Funes-Mora and Odobez	target)	EYEDIAP (floating target)
} 2016  Citation Citation {Jeni and Cohn	target)	EYEDIAP (floating target)
} 2016  Citation Citation {Mora and Odobez	target)	EYEDIAP (floating target)
} 2012  Citation Citation {Wood, Baltru{²}aitis, Morency	target)	EYEDIAP (floating target)
, Robinson, and Bulling} 2016{}  Citation Citation {Smith, Yin, Feiner	target)	EYEDIAP (floating target)
, and Nayar} 2013  Citation Citation {Funesprotect unhbox voidb@x	target)	EYEDIAP (floating target)
 penalty @M	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Mora, Monay, and Odobez} 2014{}  Citation Citation {Krafka, Khosla, Kellnhofer	target)	EYEDIAP (floating target)
, Kannan, Bhandarkar, Matusik, and 	target)	EYEDIAP (floating target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (floating target)
, and Bulling} 2015  Citation Citation {Sugano, Matsushita, and	target)	EYEDIAP (floating target)
 Sato} 2014	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Wood, Baltrusaitis, Zhang, 	target)	EYEDIAP (floating target)
Sugano, Robinson, and Bulling} 2015  Citation Citation {Wood, Baltru{²}aitis, Morency	target)	EYEDIAP (floating target)
, Robinson, and Bulling} 2016{}  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (floating target)
, and Bulling} 2015  Citation Citation {Krafka, Khosla, Kellnhofer	target)	EYEDIAP (floating target)
, Kannan, Bhandarkar, Matusik, and 	target)	EYEDIAP (floating target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (floating target)
, and Bulling} 2017  Citation Citation {Deng and Zhu	target)	EYEDIAP (floating target)
} 2017	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
4 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (floating target)
ESTIMATION  Conv	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
C on ca t  x y z x y	target)	EYEDIAP (floating target)
 z x y z	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Individual Fusion Temporal  Individual Fusion	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Input 	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Individual Fusion  Normalization	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
 .Conv	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Conv .  Conv	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Conv .  FC	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
FC FC RNN  RNN	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
RNN FC  Ti m e	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Figure 1: Overview of the 	target)	EYEDIAP (floating target)
proposed network. A multi-stream CNN 	target)	EYEDIAP (floating target)
jointly models full-face, eye region 	target)	EYEDIAP (floating target)
appearance and face landmarks from 	target)	EYEDIAP (floating target)
still images. The combined extracted 	target)	EYEDIAP (floating target)
fea- tures from each frame 	target)	EYEDIAP (floating target)
are fed into a recurrent 	target)	EYEDIAP (floating target)
module to predict last frame’s 	target)	EYEDIAP (floating target)
gaze direction.  Gazing to a specific target	target)	EYEDIAP (floating target)
 is achieved by a combination	target)	EYEDIAP (floating target)
 of eye and head movements	target)	EYEDIAP (floating target)
, which are highly coordinated. 	target)	EYEDIAP (floating target)
Consequently, the apparent direction of 	target)	EYEDIAP (floating target)
gaze is influenced not only 	target)	EYEDIAP (floating target)
by the location of the 	target)	EYEDIAP (floating target)
irises within the eyelid aperture, 	target)	EYEDIAP (floating target)
but also by the position 	target)	EYEDIAP (floating target)
and orientation of the face 	target)	EYEDIAP (floating target)
with respect to the camera. 	target)	EYEDIAP (floating target)
Known as the Wollaston 	target)	EYEDIAP (floating target)
effect [36], the exact same 	target)	EYEDIAP (floating target)
set of eyes may appear 	target)	EYEDIAP (floating target)
to be looking in different 	target)	EYEDIAP (floating target)
directions due to the surrounding 	target)	EYEDIAP (floating target)
facial cues. It is therefore 	target)	EYEDIAP (floating target)
reasonable to state that eye 	target)	EYEDIAP (floating target)
images are not sufficient to 	target)	EYEDIAP (floating target)
estimate gaze direction. Instead, whole-face 	target)	EYEDIAP (floating target)
images can encode head pose 	target)	EYEDIAP (floating target)
or illumination-specific information across larger 	target)	EYEDIAP (floating target)
areas than those available just 	target)	EYEDIAP (floating target)
in the eyes region [16, 43	target)	EYEDIAP (floating target)
].  The drawback of appearance-only methods	target)	EYEDIAP (floating target)
 is that global structure information	target)	EYEDIAP (floating target)
 is not explicitly considered. In	target)	EYEDIAP (floating target)
 that sense, facial landmarks can	target)	EYEDIAP (floating target)
 be used as global shape	target)	EYEDIAP (floating target)
 cues to en- code spatial	target)	EYEDIAP (floating target)
 relationships and geometric constraints. Current	target)	EYEDIAP (floating target)
 state-of-the-art face alignment approaches are	target)	EYEDIAP (floating target)
 robust enough to handle large	target)	EYEDIAP (floating target)
 appearance variability, extreme head poses	target)	EYEDIAP (floating target)
 and occlusions, being especially useful	target)	EYEDIAP (floating target)
 when the dataset used for	target)	EYEDIAP (floating target)
 gaze estimation does not contain	target)	EYEDIAP (floating target)
 such variability. Facial landmarks are	target)	EYEDIAP (floating target)
 mainly correlated with head orientation	target)	EYEDIAP (floating target)
, eye position, eyelid openness, 	target)	EYEDIAP (floating target)
and eyebrow movement, which are 	target)	EYEDIAP (floating target)
valuable features for our task.  Therefore, in our approach we	target)	EYEDIAP (floating target)
 jointly model appearance and shape	target)	EYEDIAP (floating target)
 cues (see Figure 1). The	target)	EYEDIAP (floating target)
 former is represented by a	target)	EYEDIAP (floating target)
 whole-face image IF , along	target)	EYEDIAP (floating target)
 with a higher resolution image	target)	EYEDIAP (floating target)
 of the eyes IE to	target)	EYEDIAP (floating target)
 identify subtle changes. Due to	target)	EYEDIAP (floating target)
 dealing with wide head pose	target)	EYEDIAP (floating target)
 ranges, some eye images may	target)	EYEDIAP (floating target)
 not depict the whole eye	target)	EYEDIAP (floating target)
, containing mostly background or 	target)	EYEDIAP (floating target)
other surrounding facial parts instead. 	target)	EYEDIAP (floating target)
For that reason, and contrary 	target)	EYEDIAP (floating target)
to previous approaches that only 	target)	EYEDIAP (floating target)
use one eye image [31, 42	target)	EYEDIAP (floating target)
], we use a single 	target)	EYEDIAP (floating target)
image composed of two patches 	target)	EYEDIAP (floating target)
of centered left and right 	target)	EYEDIAP (floating target)
eyes. Finally, the shape cue 	target)	EYEDIAP (floating target)
is represented by 3D face 	target)	EYEDIAP (floating target)
landmarks obtained from a 68-landmark 	target)	EYEDIAP (floating target)
model, denoted by 	target)	EYEDIAP (floating target)
L = {(lx, ly, 	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
)	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
 | ∀c ∈ [1, ...,68	target)	EYEDIAP (floating target)
]}.  In this work we also	target)	EYEDIAP (floating target)
 consider the dynamic component of	target)	EYEDIAP (floating target)
 gaze. We leverage the se	target)	EYEDIAP (floating target)
- quential information of eye 	target)	EYEDIAP (floating target)
and head movements such that, 	target)	EYEDIAP (floating target)
given appearance and shape features 	target)	EYEDIAP (floating target)
of consecutive frames, it is 	target)	EYEDIAP (floating target)
possible to better predict the 	target)	EYEDIAP (floating target)
gaze direction of the cur- 	target)	EYEDIAP (floating target)
rent frame. Therefore, the 3D 	target)	EYEDIAP (floating target)
gaze estimation task for a 1	target)	EYEDIAP (floating target)
-frame sequence is formulated  Citation Citation {Wollaston etprotect unhbox	target)	EYEDIAP (floating target)
 voidb@x penalty @M	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
al.} 1824  Citation Citation {Krafka, Khosla, Kellnhofer	target)	EYEDIAP (floating target)
, Kannan, Bhandarkar, Matusik, and 	target)	EYEDIAP (floating target)
Torralba} 2016  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (floating target)
, and Bulling} 2017  Citation Citation {Sugano, Matsushita, and	target)	EYEDIAP (floating target)
 Sato} 2014	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Zhang, Sugano, Fritz, 	target)	EYEDIAP (floating target)
and Bulling} 2015	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 5	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
as g(i) = f ( {IF (i)},{IE (i)},{L(i	target)	EYEDIAP (floating target)
)}  ) , where i denotes	target)	EYEDIAP (floating target)
 the i-th frame, and f	target)	EYEDIAP (floating target)
 is the regression	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
function.  3.2 Data normalization Prior to	target)	EYEDIAP (floating target)
 gaze regression, a normalization step	target)	EYEDIAP (floating target)
 in the 3D space and	target)	EYEDIAP (floating target)
 the 2D image, similar to	target)	EYEDIAP (floating target)
 [31], is carried out. This	target)	EYEDIAP (floating target)
 is performed to reduce the	target)	EYEDIAP (floating target)
 appearance variability and to allow	target)	EYEDIAP (floating target)
 the gaze estimation model to	target)	EYEDIAP (floating target)
 be applied regardless of the	target)	EYEDIAP (floating target)
 original camera configuration	target)	EYEDIAP (floating target)
.  Let H ∈ R3x3 be	target)	EYEDIAP (floating target)
 the head rotation matrix, and	target)	EYEDIAP (floating target)
 p = [px, py, pz]T	target)	EYEDIAP (floating target)
 ∈ R3 the reference face	target)	EYEDIAP (floating target)
 location with respect to the	target)	EYEDIAP (floating target)
 original CCS. The goal is	target)	EYEDIAP (floating target)
 to find the conversion matrix	target)	EYEDIAP (floating target)
 M = SR such that	target)	EYEDIAP (floating target)
 (a) the X-axes of the	target)	EYEDIAP (floating target)
 virtual camera and the head	target)	EYEDIAP (floating target)
 become parallel using the rotation	target)	EYEDIAP (floating target)
 matrix R, and (b) the	target)	EYEDIAP (floating target)
 virtual camera looks at the	target)	EYEDIAP (floating target)
 reference location from a fixed	target)	EYEDIAP (floating target)
 distance dn using the Z-direction	target)	EYEDIAP (floating target)
 scaling matrix S = diag(1,1,dn/‖p	target)	EYEDIAP (floating target)
‖). R is computed as 	target)	EYEDIAP (floating target)
a = p̂×HT e1, 	target)	EYEDIAP (floating target)
b = â× p̂, 	target)	EYEDIAP (floating target)
R = [â, b̂, p̂]T , where e1 denotes the first	target)	EYEDIAP (floating target)
 orthonormal basis and	target)	EYEDIAP (floating target)
 〈 ·̂ 〉 is the	target)	EYEDIAP (floating target)
 unit vector	target)	EYEDIAP (floating target)
.  This normalization translates into the	target)	EYEDIAP (floating target)
 image space as a cropped	target)	EYEDIAP (floating target)
 image patch of size Wn×Hn	target)	EYEDIAP (floating target)
 centered at p where head	target)	EYEDIAP (floating target)
 roll rotation has been removed	target)	EYEDIAP (floating target)
. This is done by 	target)	EYEDIAP (floating target)
applying a perspective warping to 	target)	EYEDIAP (floating target)
the input image I using 	target)	EYEDIAP (floating target)
the transformation matrix W = 	target)	EYEDIAP (floating target)
CoMCn−1, where Co and Cn 	target)	EYEDIAP (floating target)
are the original and virtual 	target)	EYEDIAP (floating target)
camera matrices, respectively.  The 3D gaze vector is	target)	EYEDIAP (floating target)
 also normalized as gn =Rg	target)	EYEDIAP (floating target)
. After image normalization, the 	target)	EYEDIAP (floating target)
line of sight can be 	target)	EYEDIAP (floating target)
represented in a 2D space. 	target)	EYEDIAP (floating target)
Therefore, gn is further transformed 	target)	EYEDIAP (floating target)
to spherical coor- dinates (θ ,	target)	EYEDIAP (floating target)
φ) assuming unit length, where 	target)	EYEDIAP (floating target)
θ and φ denote the 	target)	EYEDIAP (floating target)
horizontal and vertical direc- tion 	target)	EYEDIAP (floating target)
angles, respectively. This 2D angle 	target)	EYEDIAP (floating target)
representation, delimited in the 	target)	EYEDIAP (floating target)
range [−π/2,π/2], is computed as 	target)	EYEDIAP (floating target)
θ = arctan(gx/gz) and 	target)	EYEDIAP (floating target)
φ = arcsin(−gy), such that (0,	target)	EYEDIAP (floating target)
0) represents looking straight ahead 	target)	EYEDIAP (floating target)
to the CCS origin.  3.3 Recurrent Convolutional Neural Network	target)	EYEDIAP (floating target)
 We propose a Recurrent CNN	target)	EYEDIAP (floating target)
 Regression Network for 3D gaze	target)	EYEDIAP (floating target)
 estimation. The network is divided	target)	EYEDIAP (floating target)
 in 3 modules: (1) Individual	target)	EYEDIAP (floating target)
, (2) Fusion, and (3) 	target)	EYEDIAP (floating target)
Temporal.  First, the Individual module learns	target)	EYEDIAP (floating target)
 features from each appearance cue	target)	EYEDIAP (floating target)
 separately. It consists of a	target)	EYEDIAP (floating target)
 two-stream CNN, one devoted to	target)	EYEDIAP (floating target)
 the normalized face image stream	target)	EYEDIAP (floating target)
 and the other to the	target)	EYEDIAP (floating target)
 joint normalized eyes image. Next	target)	EYEDIAP (floating target)
, the Fusion module combines 	target)	EYEDIAP (floating target)
the extracted features of each 	target)	EYEDIAP (floating target)
appearance stream in a single 	target)	EYEDIAP (floating target)
vector along with the normalized 	target)	EYEDIAP (floating target)
landmark coordinates. Then, it learns 	target)	EYEDIAP (floating target)
a joint representation between modalities 	target)	EYEDIAP (floating target)
in a late-fusion fashion. Both 	target)	EYEDIAP (floating target)
Individual and Fusion modules, further 	target)	EYEDIAP (floating target)
referred to as Static model, 	target)	EYEDIAP (floating target)
are applied to each frame 	target)	EYEDIAP (floating target)
of the sequence. Finally, the 	target)	EYEDIAP (floating target)
resulting feature vectors of each 	target)	EYEDIAP (floating target)
frame are input to the 	target)	EYEDIAP (floating target)
Temporal module based on a 	target)	EYEDIAP (floating target)
many-to-one recurrent network. This module 	target)	EYEDIAP (floating target)
leverages sequential information to predict 	target)	EYEDIAP (floating target)
the normalized 2D gaze angles 	target)	EYEDIAP (floating target)
of the last frame of 	target)	EYEDIAP (floating target)
the sequence using a linear 	target)	EYEDIAP (floating target)
regression layer added on top 	target)	EYEDIAP (floating target)
of it.  3.4 Implementation details 3.4.1 Network	target)	EYEDIAP (floating target)
 details	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Each stream of the Individual 	target)	EYEDIAP (floating target)
module is based on the 	target)	EYEDIAP (floating target)
VGG-16 deep network [27], consisting 	target)	EYEDIAP (floating target)
of 13 convolutional layers, 5 	target)	EYEDIAP (floating target)
max pooling layers, and 1 	target)	EYEDIAP (floating target)
fully connected (FC) layer with 	target)	EYEDIAP (floating target)
Rec- tified Linear Unit (ReLU) 	target)	EYEDIAP (floating target)
activations. The full-face stream follows 	target)	EYEDIAP (floating target)
the same configuration  Citation Citation {Sugano, Matsushita, and	target)	EYEDIAP (floating target)
 Sato} 2014	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Parkhi, Vedaldi, and 	target)	EYEDIAP (floating target)
Zisserman} 2015	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
6 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (floating target)
ESTIMATION  as the base network, having	target)	EYEDIAP (floating target)
 an input of 224×224 pixels	target)	EYEDIAP (floating target)
 and a 4096D FC layer	target)	EYEDIAP (floating target)
. In contrast, the input 	target)	EYEDIAP (floating target)
joint eye image is smaller, 	target)	EYEDIAP (floating target)
with a final size of 120	target)	EYEDIAP (floating target)
×48 pixels, so the number 	target)	EYEDIAP (floating target)
of pa- rameters is decreased 	target)	EYEDIAP (floating target)
proportionally. In this case, its 	target)	EYEDIAP (floating target)
last FC layer produces a 	target)	EYEDIAP (floating target)
1536D vector. A 204D landmark 	target)	EYEDIAP (floating target)
coordinates vector is concatenated to 	target)	EYEDIAP (floating target)
the output of the FC 	target)	EYEDIAP (floating target)
layer of each stream, resulting 	target)	EYEDIAP (floating target)
in a 5836D feature vector. 	target)	EYEDIAP (floating target)
Consequently, the Fusion module consists 	target)	EYEDIAP (floating target)
of 2 5836D FC layers 	target)	EYEDIAP (floating target)
with ReLU activations and 2 	target)	EYEDIAP (floating target)
dropout layers between FCs as 	target)	EYEDIAP (floating target)
regularization. Finally, to model the 	target)	EYEDIAP (floating target)
temporal dependencies, we use a 	target)	EYEDIAP (floating target)
single GRU layer with 128 	target)	EYEDIAP (floating target)
units.  The network is trained in	target)	EYEDIAP (floating target)
 a stage-wise fashion. First, we	target)	EYEDIAP (floating target)
 train the Static model and	target)	EYEDIAP (floating target)
 the final regression layer end-to-end	target)	EYEDIAP (floating target)
 on each individual frame of	target)	EYEDIAP (floating target)
 the training data. The convolutional	target)	EYEDIAP (floating target)
 blocks are pre-trained with the	target)	EYEDIAP (floating target)
 VGG-Face dataset [27], whereas the	target)	EYEDIAP (floating target)
 FCs are trained from scratch	target)	EYEDIAP (floating target)
. Second, the training data 	target)	EYEDIAP (floating target)
is re-arranged by means of 	target)	EYEDIAP (floating target)
a sliding window with stride 1 to build input sequences. Each	target)	EYEDIAP (floating target)
 sequence is composed of s	target)	EYEDIAP (floating target)
 = 4 consecutive frames, whose	target)	EYEDIAP (floating target)
 gaze direction target is the	target)	EYEDIAP (floating target)
 gaze direction of the last	target)	EYEDIAP (floating target)
 frame of the sequence( {I(i−s+1	target)	EYEDIAP (floating target)
), . . . ,I(i)}, 	target)	EYEDIAP (floating target)
g(i)  ) . Using this re-arranged	target)	EYEDIAP (floating target)
 training data, we extract features	target)	EYEDIAP (floating target)
 of each	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
frame of the sequence from 	target)	EYEDIAP (floating target)
a frozen Individual module, fine-tune 	target)	EYEDIAP (floating target)
the Fusion layers, and train 	target)	EYEDIAP (floating target)
both, the Temporal module and 	target)	EYEDIAP (floating target)
a new final regression layer 	target)	EYEDIAP (floating target)
from scratch. This way, the 	target)	EYEDIAP (floating target)
network can exploit the temporal 	target)	EYEDIAP (floating target)
information to further refine the 	target)	EYEDIAP (floating target)
fusion weights.  We trained the model using	target)	EYEDIAP (floating target)
 ADAM optimizer with an initial	target)	EYEDIAP (floating target)
 learning rate of 0.0001, dropout	target)	EYEDIAP (floating target)
 of 0.3, and batch size	target)	EYEDIAP (floating target)
 of 64 frames. The number	target)	EYEDIAP (floating target)
 of epochs was experimentally set	target)	EYEDIAP (floating target)
 to 21 for the first	target)	EYEDIAP (floating target)
 training stage and 10 for	target)	EYEDIAP (floating target)
 the second. We use the	target)	EYEDIAP (floating target)
 average Euclidean distance between the	target)	EYEDIAP (floating target)
 predicted and ground-truth 3D gaze	target)	EYEDIAP (floating target)
 vectors as loss function	target)	EYEDIAP (floating target)
.  3.4.2 Input pre-processing	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
For this work we use 	target)	EYEDIAP (floating target)
head pose and eye locations 	target)	EYEDIAP (floating target)
in the 3D scene provided 	target)	EYEDIAP (floating target)
by the dataset. The 3D 	target)	EYEDIAP (floating target)
landmarks are extracted using the 	target)	EYEDIAP (floating target)
state-of-the-art method of Bulat and 	target)	EYEDIAP (floating target)
Tzimiropou- los [3], which is 	target)	EYEDIAP (floating target)
based on stacked hourglass 	target)	EYEDIAP (floating target)
networks [24].  During training, the original image	target)	EYEDIAP (floating target)
 is pre-processed to get the	target)	EYEDIAP (floating target)
 two normalized input images. The	target)	EYEDIAP (floating target)
 normalized whole-face patch is centered	target)	EYEDIAP (floating target)
 0.1 meters ahead of the	target)	EYEDIAP (floating target)
 head center in the head	target)	EYEDIAP (floating target)
 coordinate system, and Cn is	target)	EYEDIAP (floating target)
 defined such that the image	target)	EYEDIAP (floating target)
 has size of 250× 250	target)	EYEDIAP (floating target)
 pixels. The difference between this	target)	EYEDIAP (floating target)
 size and the final input	target)	EYEDIAP (floating target)
 size allows us to perform	target)	EYEDIAP (floating target)
 random cropping and zooming to	target)	EYEDIAP (floating target)
 augment the data (explained in	target)	EYEDIAP (floating target)
 Section 4.1). Similarly, each normalized	target)	EYEDIAP (floating target)
 eye patch is centered in	target)	EYEDIAP (floating target)
 their respective eye center locations	target)	EYEDIAP (floating target)
. In this case, the 	target)	EYEDIAP (floating target)
virtual camera matrix is defined 	target)	EYEDIAP (floating target)
so that the image is 	target)	EYEDIAP (floating target)
cropped to 70×58, while in 	target)	EYEDIAP (floating target)
practice the final patches have 	target)	EYEDIAP (floating target)
size of 60×48. Landmarks are 	target)	EYEDIAP (floating target)
normalized using the same procedure 	target)	EYEDIAP (floating target)
and further pre-processed with mean 	target)	EYEDIAP (floating target)
subtraction and min-max normalization per 	target)	EYEDIAP (floating target)
axis. Finally, we divide them 	target)	EYEDIAP (floating target)
by a scaling factor w 	target)	EYEDIAP (floating target)
such that all coordinates are 	target)	EYEDIAP (floating target)
in the range [0,w]. This 	target)	EYEDIAP (floating target)
way, all concatenated feature values 	target)	EYEDIAP (floating target)
are in a similar range. 	target)	EYEDIAP (floating target)
After inference, the predicted normalized 	target)	EYEDIAP (floating target)
2D angles are de-normalized back 	target)	EYEDIAP (floating target)
to the original 3D space.  4 Experiments In this section	target)	EYEDIAP (floating target)
, we evaluate the cross-subject 	target)	EYEDIAP (floating target)
3D gaze estimation task on 	target)	EYEDIAP (floating target)
a wide range of head 	target)	EYEDIAP (floating target)
poses and gaze directions. Furthermore, 	target)	EYEDIAP (floating target)
we validate the effectiveness of 	target)	EYEDIAP (floating target)
the proposed architecture comparing both 	target)	EYEDIAP (floating target)
static and temporal approaches. We 	target)	EYEDIAP (floating target)
report the error in terms 	target)	EYEDIAP (floating target)
of mean angular error between 	target)	EYEDIAP (floating target)
predicted and ground-truth 3D gaze 	target)	EYEDIAP (floating target)
vectors. Note that due to 	target)	EYEDIAP (floating target)
the requirements of the temporal 	target)	EYEDIAP (floating target)
model not all the frames 	target)	EYEDIAP (floating target)
obtain a prediction. Therefore, for 	target)	EYEDIAP (floating target)
a  Citation Citation {Parkhi, Vedaldi, and	target)	EYEDIAP (floating target)
 Zisserman} 2015	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Bulat and Tzimiropoulos} 2017	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Citation Citation {Newell, Yang, and 	target)	EYEDIAP (floating target)
Deng} 2016	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 7	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
60 30 0 30 60  60	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
30  0	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
30  60	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
100  101	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
102  60 30 0 30 60	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
60  30	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
0  30	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
60  100	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
101  102	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
103  60 30 0 30 60	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
60  30	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
0  30	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
60  100	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
101  102	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
60 30 0 30 60  60	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
30  0	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
30  60	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
100  101	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
102  103	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
a) g (FT ) (b) 	target)	EYEDIAP (floating target)
h (FT ) (c) g (	target)	EYEDIAP (floating target)
CS) (d) h (CS)  Figure 2: Ground-truth eye gaze	target)	EYEDIAP (floating target)
 g and head orientation h	target)	EYEDIAP (floating target)
 distribution on the filtered EYE	target)	EYEDIAP (floating target)
- DIAP dataset for CS 	target)	EYEDIAP (floating target)
and FT settings, in terms 	target)	EYEDIAP (floating target)
of x- and y- angles.  fair comparison, the reported results	target)	EYEDIAP (floating target)
 for static models disregard such	target)	EYEDIAP (floating target)
 frames when temporal models are	target)	EYEDIAP (floating target)
 included in the comparison	target)	EYEDIAP (floating target)
.  4.1 Training data	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
There are few publicly available 	target)	EYEDIAP (floating target)
datasets devoted to 3D gaze 	target)	EYEDIAP (floating target)
estimation and most of them 	target)	EYEDIAP (floating target)
focus on HCI with a 	target)	EYEDIAP (floating target)
limited range of head pose 	target)	EYEDIAP (floating target)
and gaze directions. Therefore, we 	target)	EYEDIAP (floating target)
use VGA videos from the 	target)	EYEDIAP (floating target)
publicly-available EYEDIAP dataset [7] to 	target)	EYEDIAP (floating target)
perform the experimental evaluation, as 	target)	EYEDIAP (floating target)
it is currently the only 	target)	EYEDIAP (floating target)
one containing video sequences with 	target)	EYEDIAP (floating target)
a wide range of head 	target)	EYEDIAP (floating target)
poses and showing the full 	target)	EYEDIAP (floating target)
face. This dataset consists of 3	target)	EYEDIAP (floating target)
-minute videos of 16 subjects 	target)	EYEDIAP (floating target)
looking at two types of 	target)	EYEDIAP (floating target)
targets: continuous screen targets on 	target)	EYEDIAP (floating target)
a fixed monitor (CS), and 	target)	EYEDIAP (floating target)
floating physical targets (FT ). 	target)	EYEDIAP (floating target)
The videos are further divided 	target)	EYEDIAP (floating target)
into static (S) and moving (	target)	EYEDIAP (floating target)
M) head pose for each 	target)	EYEDIAP (floating target)
of the subjects. Subjects 12-16 	target)	EYEDIAP (floating target)
were recorded with 2 different 	target)	EYEDIAP (floating target)
lighting conditions.  For evaluation, we filtered out	target)	EYEDIAP (floating target)
 those frames that fulfilled at	target)	EYEDIAP (floating target)
 least one of the following	target)	EYEDIAP (floating target)
 conditions: (1) face or landmarks	target)	EYEDIAP (floating target)
 not detected; (2) subject not	target)	EYEDIAP (floating target)
 looking at the target; (3	target)	EYEDIAP (floating target)
) 3D head pose, eyes 	target)	EYEDIAP (floating target)
or target location not properly 	target)	EYEDIAP (floating target)
recovered; and (4) eyeball rotations 	target)	EYEDIAP (floating target)
violating physical 	target)	EYEDIAP (floating target)
constraints (|θ | ≤ 40	target)	EYEDIAP (floating target)
◦, |φ | ≤ 30	target)	EYEDIAP (floating target)
◦) [23]. Note that we 	target)	EYEDIAP (floating target)
purposely do not filter eye 	target)	EYEDIAP (floating target)
blinking moments to learn their 	target)	EYEDIAP (floating target)
dynamics with the temporal model, 	target)	EYEDIAP (floating target)
which may produce some outliers 	target)	EYEDIAP (floating target)
with a higher prediction error 	target)	EYEDIAP (floating target)
due to a less accurate 	target)	EYEDIAP (floating target)
ground truth. Figure 2 shows 	target)	EYEDIAP (floating target)
the distribution of gaze directions 	target)	EYEDIAP (floating target)
and head poses for both 	target)	EYEDIAP (floating target)
filtered CS and FT cases.  We applied data augmentation to	target)	EYEDIAP (floating target)
 the training set with the	target)	EYEDIAP (floating target)
 following random transforma- tions: horizontal	target)	EYEDIAP (floating target)
 flip, shifts of up to	target)	EYEDIAP (floating target)
 5 pixels, zoom of up	target)	EYEDIAP (floating target)
 to 2%, brightness changes by	target)	EYEDIAP (floating target)
 a factor in the range	target)	EYEDIAP (floating target)
 [0.4,1.75], and additive Gaussian noise	target)	EYEDIAP (floating target)
 with σ2 = 0.03	target)	EYEDIAP (floating target)
.  4.2 Evaluation of static modalities	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
First, we evaluate the contribution 	target)	EYEDIAP (floating target)
of each static modality on 	target)	EYEDIAP (floating target)
the FT scenario. We divided 	target)	EYEDIAP (floating target)
the 16 participants into 4 	target)	EYEDIAP (floating target)
groups, such that appearance variability 	target)	EYEDIAP (floating target)
was maximized while maintaining a 	target)	EYEDIAP (floating target)
similar number of training samples 	target)	EYEDIAP (floating target)
per group. Each static model 	target)	EYEDIAP (floating target)
was trained end-to-end performing 4-fold 	target)	EYEDIAP (floating target)
cross-validation using different combinations of 	target)	EYEDIAP (floating target)
input modal- ities. Since the 	target)	EYEDIAP (floating target)
number of fusion units depends 	target)	EYEDIAP (floating target)
on the number of input 	target)	EYEDIAP (floating target)
modalities, we also compare different 	target)	EYEDIAP (floating target)
fusion layer sizes. The effect 	target)	EYEDIAP (floating target)
of data normalization is also 	target)	EYEDIAP (floating target)
evaluated by training a not-normalized 	target)	EYEDIAP (floating target)
face model where the input 	target)	EYEDIAP (floating target)
image is the face bounding 	target)	EYEDIAP (floating target)
box with square size the 	target)	EYEDIAP (floating target)
maximum distance between 2D landmarks.  Citation Citation {Funesprotect unhbox voidb@x	target)	EYEDIAP (floating target)
 penalty @M	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Mora, Monay, and Odobez} 2014{}  Citation Citation {MSC	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
8 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (floating target)
ESTIMATION  0 1 2 3 4	target)	EYEDIAP (floating target)
 5 6 7 8 9	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
10 11  An gl	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
e  er	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
ro r (  de gr	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
ee s)  6.9 6.43 5.58 5.71 5.59	target)	EYEDIAP (floating target)
 5.55 5.52	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
OF-4096 NE-1536 NF-4096  NF-5632 NFL-4300	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
NFE-5632 NFEL-5836  Figure 3: Performance evaluation of	target)	EYEDIAP (floating target)
 the Static network using different	target)	EYEDIAP (floating target)
 input modali- ties (O	target)	EYEDIAP (floating target)
 - Not normalized, N	target)	EYEDIAP (floating target)
 - Normalized, F - Face	target)	EYEDIAP (floating target)
, E - Eyes, 	target)	EYEDIAP (floating target)
L - 3D Landmarks) and 	target)	EYEDIAP (floating target)
size of fusion layers on 	target)	EYEDIAP (floating target)
the FT scenario.  Floating Target Screen Target 0	target)	EYEDIAP (floating target)
 1 2 3 4 5	target)	EYEDIAP (floating target)
 6 7 8 9	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
10 11  An gl	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
e  er	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
ro r (  de gr	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
ee s)  6.36 5.43 5.19 4.2 3.38	target)	EYEDIAP (floating target)
 3.4	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
MPIIGaze Static Temporal  Figure 4: Performance comparison among	target)	EYEDIAP (floating target)
 MPIIGaze method [42] and our	target)	EYEDIAP (floating target)
 Static and Temporal versions of	target)	EYEDIAP (floating target)
 the proposed network for FT	target)	EYEDIAP (floating target)
 and CS scenarios	target)	EYEDIAP (floating target)
.  As shown in Figure 3	target)	EYEDIAP (floating target)
, all models that take 	target)	EYEDIAP (floating target)
normalized full-face information as input 	target)	EYEDIAP (floating target)
achieve better performance than the 	target)	EYEDIAP (floating target)
eyes-only model. More specifically, the 	target)	EYEDIAP (floating target)
combination of face, eyes and 	target)	EYEDIAP (floating target)
landmarks outperforms all the other 	target)	EYEDIAP (floating target)
combinations by a small but 	target)	EYEDIAP (floating target)
significant margin (paired Wilcoxon test, 	target)	EYEDIAP (floating target)
p < 0.0001). The standard 	target)	EYEDIAP (floating target)
deviation of the best-performing model 	target)	EYEDIAP (floating target)
is reduced compared to the 	target)	EYEDIAP (floating target)
face and eyes model, suggesting 	target)	EYEDIAP (floating target)
a regularizing effect due to 	target)	EYEDIAP (floating target)
the addition of landmarks. The 	target)	EYEDIAP (floating target)
not-normalized face-only model shows the 	target)	EYEDIAP (floating target)
largest error, proving the impact 	target)	EYEDIAP (floating target)
of normalization to reduce the 	target)	EYEDIAP (floating target)
appearance variability. Furthermore, our results 	target)	EYEDIAP (floating target)
indicate that the increase of 	target)	EYEDIAP (floating target)
fusion units is not correlated 	target)	EYEDIAP (floating target)
with a better performance.  4.3 Static gaze regression: comparison	target)	EYEDIAP (floating target)
 with existing methods	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
We compare our best-performing static 	target)	EYEDIAP (floating target)
model with three baselines. Head: 	target)	EYEDIAP (floating target)
Treating the head pose directly 	target)	EYEDIAP (floating target)
as gaze direction. PR-ALR: Method 	target)	EYEDIAP (floating target)
that relies on RGB-D data 	target)	EYEDIAP (floating target)
to rectify the eye images 	target)	EYEDIAP (floating target)
viewpoint into a canonical head 	target)	EYEDIAP (floating target)
pose using a 3DMM. It 	target)	EYEDIAP (floating target)
then learns an RGB gaze 	target)	EYEDIAP (floating target)
appearance model using ALR [21]. 	target)	EYEDIAP (floating target)
Predicted 3D vectors for FT-S 	target)	EYEDIAP (floating target)
scenario are provided by EYEDIAP 	target)	EYEDIAP (floating target)
dataset. MPIIGaze:. State-of-the-art full-face 3D 	target)	EYEDIAP (floating target)
gaze estimation method [42]. They 	target)	EYEDIAP (floating target)
use an Alexnet-based CNN model 	target)	EYEDIAP (floating target)
with spatial weights to enhance 	target)	EYEDIAP (floating target)
information in different facial regions. 	target)	EYEDIAP (floating target)
We fine-tuned it with the 	target)	EYEDIAP (floating target)
filtered EYEDIAP subsets using our 	target)	EYEDIAP (floating target)
training parameters and normalization procedure.  In addition to the aforementioned	target)	EYEDIAP (floating target)
 FT-based evaluation setup, we also	target)	EYEDIAP (floating target)
 evaluate our method on the	target)	EYEDIAP (floating target)
 CS scenario. In this case	target)	EYEDIAP (floating target)
 there are only 14 participants	target)	EYEDIAP (floating target)
 available, so we divided them	target)	EYEDIAP (floating target)
 in 5 groups and performed	target)	EYEDIAP (floating target)
 5-fold cross-validation. In Figure 4	target)	EYEDIAP (floating target)
 we compare our method to	target)	EYEDIAP (floating target)
 MPIIGaze, achieving a statistically significant	target)	EYEDIAP (floating target)
 improvement of 14.6% and 19.5	target)	EYEDIAP (floating target)
% on FT and CS 	target)	EYEDIAP (floating target)
scenarios, respectively (paired Wilcoxon test, 	target)	EYEDIAP (floating target)
p < 0.0001). We can 	target)	EYEDIAP (floating target)
observe that a re- stricted 	target)	EYEDIAP (floating target)
gaze target benefits the performance 	target)	EYEDIAP (floating target)
of all methods, compared to 	target)	EYEDIAP (floating target)
a more challenging unrestricted setting 	target)	EYEDIAP (floating target)
with a wider range of 	target)	EYEDIAP (floating target)
head poses and gaze directions.  Table 2 provides a detailed	target)	EYEDIAP (floating target)
 comparison on every participant, performing	target)	EYEDIAP (floating target)
 leave-one-out cross-validation on the FT	target)	EYEDIAP (floating target)
 scenario for static and moving	target)	EYEDIAP (floating target)
 head separately. Results show that	target)	EYEDIAP (floating target)
, as expected, facial appearance 	target)	EYEDIAP (floating target)
and head pose have a 	target)	EYEDIAP (floating target)
noticeable impact on gaze accuracy, 	target)	EYEDIAP (floating target)
with average error differences of 	target)	EYEDIAP (floating target)
up to 7.7◦ among participants.  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (floating target)
, and Bulling} 2015  Citation Citation {Mora and Odobez	target)	EYEDIAP (floating target)
} 2012  Citation Citation {Zhang, Sugano, Fritz	target)	EYEDIAP (floating target)
, and Bulling} 2015	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 9	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Method 1 2 3 4 5 6 7 8 9 10	target)	EYEDIAP (floating target)
 11 12 13 14 15	target)	EYEDIAP (floating target)
 16 Avg. Head 23.5 22.1	target)	EYEDIAP (floating target)
 20.3 23.6 23.2 23.2 23.6	target)	EYEDIAP (floating target)
 21.2 26.7 23.6 23.1 24.4	target)	EYEDIAP (floating target)
 23.3 24.0 24.5 22.8 23.3	target)	EYEDIAP (floating target)
 PR-ALR 12.3 12.0 12.4 11.3	target)	EYEDIAP (floating target)
 15.5 12.9 17.9 11.8 17.3	target)	EYEDIAP (floating target)
 13.4 13.4 14.3 15.2 13.6	target)	EYEDIAP (floating target)
 14.4 14.6 13.9 MPIIGaze 5.3	target)	EYEDIAP (floating target)
 5.1 5.7 4.7 7.3 15.1	target)	EYEDIAP (floating target)
 10.8 5.7 9.9 7.1 5.0	target)	EYEDIAP (floating target)
 5.7 7.4 3.8 4.8 5.5	target)	EYEDIAP (floating target)
 6.8 Static 3.9 4.1 4.2	target)	EYEDIAP (floating target)
 3.9 6.0 6.4 7.2 3.6	target)	EYEDIAP (floating target)
 7.1 5.0 5.7 6.7 3.9	target)	EYEDIAP (floating target)
 4.7 5.1 4.2 5.1 Temporal	target)	EYEDIAP (floating target)
 4.0 4.9 4.3 4.1 6.1	target)	EYEDIAP (floating target)
 6.5 6.6 3.9 7.8 6.1	target)	EYEDIAP (floating target)
 4.7 5.6 4.7 3.5 5.9	target)	EYEDIAP (floating target)
 4.6 5.2 Head 19.3 14.2	target)	EYEDIAP (floating target)
 16.4 19.9 16.8 21.9 16.1	target)	EYEDIAP (floating target)
 24.2 20.3 19.9 18.8 22.3	target)	EYEDIAP (floating target)
 18.1 14.9 16.2 19.3 18.7	target)	EYEDIAP (floating target)
 MPIIGaze 7.6 6.2 5.7 8.7	target)	EYEDIAP (floating target)
 10.1 12.0 12.2 6.1 8.3	target)	EYEDIAP (floating target)
 5.9 6.1 6.2 7.4 4.7	target)	EYEDIAP (floating target)
 4.4 6.0 7.3 Static 5.8	target)	EYEDIAP (floating target)
 5.7 4.4 7.5 6.7 8.8	target)	EYEDIAP (floating target)
 11.6 5.5 8.3 5.5 5.2	target)	EYEDIAP (floating target)
 6.3 5.3 3.9 4.3 5.6	target)	EYEDIAP (floating target)
 6.3 Temporal 6.1 5.6 4.5	target)	EYEDIAP (floating target)
 7.5 6.4 8.2 12.0 5.0	target)	EYEDIAP (floating target)
 7.5 5.4 5.0 5.8 6.6	target)	EYEDIAP (floating target)
 4.0 4.5 5.8 6.2	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Table 2: Gaze angular error 	target)	EYEDIAP (floating target)
comparison for static (top half) 	target)	EYEDIAP (floating target)
and moving (bottom half) head 	target)	EYEDIAP (floating target)
pose for each subject in 	target)	EYEDIAP (floating target)
the FT scenario. Best results 	target)	EYEDIAP (floating target)
in bold.  −80 −40 0 40 80−80	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
40  0	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
40  80	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
0  5	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
10  15	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
20  25	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
30  35	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
80 −40 0 40 80−80  −40	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
0  40	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
80  −10	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
8  −6	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
4  −2	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
0  2	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
4  6	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
8  10	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
80 −40 0 40 80−80  −40	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
0  40	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
80  0	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
5  10	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
15  20	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
25  30	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
35  −80 −40 0 40 80−80	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
40  0	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
40  80	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
10  −8	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
6  −4	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
2  0	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
2  4	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
6  8	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
10  (a) Gaze space (b) Head	target)	EYEDIAP (floating target)
 orientation space	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
Figure 5: Angular error distribution 	target)	EYEDIAP (floating target)
across gaze (a) and head 	target)	EYEDIAP (floating target)
orientation (b) spaces in the 	target)	EYEDIAP (floating target)
FT setting, in terms of 	target)	EYEDIAP (floating target)
x- and y- angles. For 	target)	EYEDIAP (floating target)
each space, we depict the 	target)	EYEDIAP (floating target)
Static model performance (left) and 	target)	EYEDIAP (floating target)
the contribution of the Temporal 	target)	EYEDIAP (floating target)
model versus Static (right). In 	target)	EYEDIAP (floating target)
the latter, positive difference means 	target)	EYEDIAP (floating target)
higher improvement of the Temporal 	target)	EYEDIAP (floating target)
model.  4.4 Evaluation of the temporal	target)	EYEDIAP (floating target)
 network	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
In this section, we evaluate 	target)	EYEDIAP (floating target)
the contribution of adding the 	target)	EYEDIAP (floating target)
temporal module to the static 	target)	EYEDIAP (floating target)
model. To do so, we 	target)	EYEDIAP (floating target)
trained a lower-dimensional version of 	target)	EYEDIAP (floating target)
the static network with compa- 	target)	EYEDIAP (floating target)
rable performance to the original, 	target)	EYEDIAP (floating target)
reducing the number of units 	target)	EYEDIAP (floating target)
of the second fusion layer 	target)	EYEDIAP (floating target)
to 2918. Results are reported 	target)	EYEDIAP (floating target)
in Figure 4 and Table 2	target)	EYEDIAP (floating target)
. One can observe that 	target)	EYEDIAP (floating target)
using sequential information is helpful 	target)	EYEDIAP (floating target)
on the FT scenario, outperforming 	target)	EYEDIAP (floating target)
the static model by a 	target)	EYEDIAP (floating target)
statistically significant 4.4% (paired Wilcoxon 	target)	EYEDIAP (floating target)
test, p < 0.0001). This 	target)	EYEDIAP (floating target)
contribution is more noticeable in 	target)	EYEDIAP (floating target)
the moving head setting, proving 	target)	EYEDIAP (floating target)
that the temporal model can 	target)	EYEDIAP (floating target)
benefit from head motion information. 	target)	EYEDIAP (floating target)
In contrast, such information seems 	target)	EYEDIAP (floating target)
to be less meaningful in 	target)	EYEDIAP (floating target)
the CS scenario, where the 	target)	EYEDIAP (floating target)
obtained error is already very 	target)	EYEDIAP (floating target)
low for a cross-subject setting 	target)	EYEDIAP (floating target)
and the amount of head 	target)	EYEDIAP (floating target)
movement declines.  Figure 5 further explores the	target)	EYEDIAP (floating target)
 error distribution of the static	target)	EYEDIAP (floating target)
 network and the impact of	target)	EYEDIAP (floating target)
 sequential information. We can observe	target)	EYEDIAP (floating target)
 that the accuracy of the	target)	EYEDIAP (floating target)
 static model drops with extreme	target)	EYEDIAP (floating target)
 head poses and gaze directions	target)	EYEDIAP (floating target)
, which can also be 	target)	EYEDIAP (floating target)
correlated to having less data 	target)	EYEDIAP (floating target)
in those areas. Compared to 	target)	EYEDIAP (floating target)
the static model, the temporal 	target)	EYEDIAP (floating target)
model particularly benefits gaze targets 	target)	EYEDIAP (floating target)
from mid-range upwards. Its contribution 	target)	EYEDIAP (floating target)
is less clear for extreme 	target)	EYEDIAP (floating target)
targets, probably again due to 	target)	EYEDIAP (floating target)
data imbalance.  Finally, we evaluated the effect	target)	EYEDIAP (floating target)
 of different recurrent architectures for	target)	EYEDIAP (floating target)
 the temporal model. In particular	target)	EYEDIAP (floating target)
, we tested 1 (128 	target)	EYEDIAP (floating target)
units) and 2 (256-128 units) 	target)	EYEDIAP (floating target)
LSTM and GRU lay- ers, 	target)	EYEDIAP (floating target)
with 1 GRU layer obtaining 	target)	EYEDIAP (floating target)
slightly superior results (up to 0	target)	EYEDIAP (floating target)
.12◦). We also assessed the 	target)	EYEDIAP (floating target)
effect of sequence length fixing 	target)	EYEDIAP (floating target)
s in the range {4,7,10}, 	target)	EYEDIAP (floating target)
with s = 7 performing 	target)	EYEDIAP (floating target)
worse than the other two (	target)	EYEDIAP (floating target)
up to 0	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
14	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
10 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (floating target)
ESTIMATION  5 Conclusions In this work	target)	EYEDIAP (floating target)
, we studied the combination 	target)	EYEDIAP (floating target)
of full-face and eye images 	target)	EYEDIAP (floating target)
along with facial land- marks 	target)	EYEDIAP (floating target)
for person- and head pose-independent 	target)	EYEDIAP (floating target)
3D gaze estimation. Consequently, we 	target)	EYEDIAP (floating target)
pro- posed a multi-stream recurrent 	target)	EYEDIAP (floating target)
CNN network that leverages the 	target)	EYEDIAP (floating target)
sequential information of eye and 	target)	EYEDIAP (floating target)
head movements. Both static and 	target)	EYEDIAP (floating target)
temporal versions of our approach 	target)	EYEDIAP (floating target)
significantly outperform current state-of-the-art 3D 	target)	EYEDIAP (floating target)
gaze estimation methods on a 	target)	EYEDIAP (floating target)
wide range of head poses 	target)	EYEDIAP (floating target)
and gaze directions. We showed 	target)	EYEDIAP (floating target)
that adding geometry features to 	target)	EYEDIAP (floating target)
appearance-based methods has a regularizing 	target)	EYEDIAP (floating target)
effect on the accuracy. Adding 	target)	EYEDIAP (floating target)
sequential information further benefits the 	target)	EYEDIAP (floating target)
final performance compared to static-only 	target)	EYEDIAP (floating target)
input, especially from mid-range up- 	target)	EYEDIAP (floating target)
wards and in those cases 	target)	EYEDIAP (floating target)
where head motion is present. 	target)	EYEDIAP (floating target)
The effect in very extreme 	target)	EYEDIAP (floating target)
head poses is not clear 	target)	EYEDIAP (floating target)
due to data imbalance, suggesting 	target)	EYEDIAP (floating target)
the importance of learning from 	target)	EYEDIAP (floating target)
a con- tinuous, balanced dataset 	target)	EYEDIAP (floating target)
including all head poses and 	target)	EYEDIAP (floating target)
gaze directions of interest. To 	target)	EYEDIAP (floating target)
the best of our knowledge, 	target)	EYEDIAP (floating target)
this is the first attempt 	target)	EYEDIAP (floating target)
to exploit the temporal modality 	target)	EYEDIAP (floating target)
in the context of gaze 	target)	EYEDIAP (floating target)
estimation from remote cameras. As 	target)	EYEDIAP (floating target)
future work, we will further 	target)	EYEDIAP (floating target)
explore extracting meaningful temporal representations 	target)	EYEDIAP (floating target)
of gaze dynamics, considering 3DCNNs 	target)	EYEDIAP (floating target)
as well as the encoding 	target)	EYEDIAP (floating target)
of deep features around particular 	target)	EYEDIAP (floating target)
tracked face landmarks [14].  Acknowledgements This work has been	target)	EYEDIAP (floating target)
 partially supported by the Spanish	target)	EYEDIAP (floating target)
 project TIN2016-74946-P (MINECO/ FEDER, UE	target)	EYEDIAP (floating target)
), CERCA Programme / Generalitat 	target)	EYEDIAP (floating target)
de Catalunya, and the FP7 	target)	EYEDIAP (floating target)
people program (Marie Curie Actions), 	target)	EYEDIAP (floating target)
REA grant agreement no FP7-607139 (	target)	EYEDIAP (floating target)
iCARE - Improving Children Auditory 	target)	EYEDIAP (floating target)
REhabilitation). We gratefully acknowledge the 	target)	EYEDIAP (floating target)
support of NVIDIA Corporation with 	target)	EYEDIAP (floating target)
the donation of the GPU 	target)	EYEDIAP (floating target)
used for this research. Portions 	target)	EYEDIAP (floating target)
of the research in this 	target)	EYEDIAP (floating target)
pa- per used the EYEDIAP 	target)	EYEDIAP (floating target)
dataset made available by the 	target)	EYEDIAP (floating target)
Idiap Research Institute, Martigny, Switzerland.  References [1] Nicola C Anderson	target)	EYEDIAP (floating target)
, Evan F Risko, and 	target)	EYEDIAP (floating target)
Alan Kingstone. Motion influences gaze 	target)	EYEDIAP (floating target)
di-  rection discrimination and disambiguates contradictory	target)	EYEDIAP (floating target)
 luminance cues. Psychonomic bulletin	target)	EYEDIAP (floating target)
 & review, 23(3):817–823, 2016	target)	EYEDIAP (floating target)
.  [2] Shumeet Baluja and Dean	target)	EYEDIAP (floating target)
 Pomerleau. Non-intrusive gaze tracking using	target)	EYEDIAP (floating target)
 artificial neu- ral networks. In	target)	EYEDIAP (floating target)
 Advances in Neural Information Processing	target)	EYEDIAP (floating target)
 Systems, pages 753–760, 1994	target)	EYEDIAP (floating target)
.  [3] Adrian Bulat and Georgios	target)	EYEDIAP (floating target)
 Tzimiropoulos. How far are we	target)	EYEDIAP (floating target)
 from solving the 2d	target)	EYEDIAP (floating target)
 & 3d face alignment problem	target)	EYEDIAP (floating target)
? (and a dataset of 230,	target)	EYEDIAP (floating target)
000 3d facial landmarks). In 	target)	EYEDIAP (floating target)
Interna- tional Conference on Computer 	target)	EYEDIAP (floating target)
Vision, 2017.  [4] Haoping Deng and Wangjiang	target)	EYEDIAP (floating target)
 Zhu. Monocular free-head 3d gaze	target)	EYEDIAP (floating target)
 tracking with deep learning and	target)	EYEDIAP (floating target)
 geometry constraints. In Computer Vision	target)	EYEDIAP (floating target)
 (ICCV), 2017 IEEE Interna- tional	target)	EYEDIAP (floating target)
 Conference on, pages 3162–3171. IEEE	target)	EYEDIAP (floating target)
, 2017.  [5] Onur Ferhat and Fernando	target)	EYEDIAP (floating target)
 Vilariño. Low cost eye tracking	target)	EYEDIAP (floating target)
. Computational intelligence and neuroscience, 2016	target)	EYEDIAP (floating target)
:17, 2016.  Citation Citation {Jung, Lee, Yim	target)	EYEDIAP (floating target)
, Park, and Kim} 2015	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 11	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
6] Kenneth A Funes-Mora and 	target)	EYEDIAP (floating target)
Jean-Marc Odobez. Gaze estimation in 	target)	EYEDIAP (floating target)
the 3D space using RGB-D 	target)	EYEDIAP (floating target)
sensors. International Journal of Computer 	target)	EYEDIAP (floating target)
Vision, 118(2):194–216, 2016.  [7] Kenneth Alberto Funes Mora	target)	EYEDIAP (floating target)
, Florent Monay, and Jean-Marc 	target)	EYEDIAP (floating target)
Odobez. Eyediap: A database for 	target)	EYEDIAP (floating target)
the development and evaluation of 	target)	EYEDIAP (floating target)
gaze estimation algorithms from rgb 	target)	EYEDIAP (floating target)
and rgb-d cameras. In Proceedings 	target)	EYEDIAP (floating target)
of the ACM Symposium on 	target)	EYEDIAP (floating target)
Eye Tracking Research and Applications. 	target)	EYEDIAP (floating target)
ACM, March 2014. doi: 10.1145/2578153.2578190.  [8] Kenneth Alberto Funes Mora	target)	EYEDIAP (floating target)
, Florent Monay, and Jean-Marc 	target)	EYEDIAP (floating target)
Odobez. Eyediap: A database for 	target)	EYEDIAP (floating target)
the development and evaluation of 	target)	EYEDIAP (floating target)
gaze estimation algorithms from rgb 	target)	EYEDIAP (floating target)
and rgb-d cameras. In Proceedings 	target)	EYEDIAP (floating target)
of the Symposium on Eye 	target)	EYEDIAP (floating target)
Tracking Research and Applications, pages 255	target)	EYEDIAP (floating target)
–258. ACM, 2014.  [9] Quentin Guillon, Nouchine Hadjikhani	target)	EYEDIAP (floating target)
, Sophie Baduel, and Bernadette 	target)	EYEDIAP (floating target)
Rogé. Visual social attention in 	target)	EYEDIAP (floating target)
autism spectrum disorder: Insights from 	target)	EYEDIAP (floating target)
eye tracking studies. Neu- 	target)	EYEDIAP (floating target)
roscience & Biobehavioral Reviews, 42:279–297, 2014	target)	EYEDIAP (floating target)
.  [10] Dan Witzner Hansen and	target)	EYEDIAP (floating target)
 Qiang Ji. In the eye	target)	EYEDIAP (floating target)
 of the beholder: A survey	target)	EYEDIAP (floating target)
 of models for eyes and	target)	EYEDIAP (floating target)
 gaze. IEEE transactions on pattern	target)	EYEDIAP (floating target)
 analysis and machine intelligence, 32(3	target)	EYEDIAP (floating target)
): 478–500, 2010.  [11] Qiong Huang, Ashok Veeraraghavan	target)	EYEDIAP (floating target)
, and Ashutosh Sabharwal. Tabletgaze: 	target)	EYEDIAP (floating target)
dataset and analysis for unconstrained 	target)	EYEDIAP (floating target)
appearance-based gaze estimation in mobile 	target)	EYEDIAP (floating target)
tablets. Machine Vision and Applications, 28	target)	EYEDIAP (floating target)
(5-6):445–461, 2017.  [12] Robert JK Jacob and	target)	EYEDIAP (floating target)
 Keith S Karn. Eye tracking	target)	EYEDIAP (floating target)
 in human-computer interaction and usability	target)	EYEDIAP (floating target)
 research: Ready to deliver the	target)	EYEDIAP (floating target)
 promises. In The mind’s eye	target)	EYEDIAP (floating target)
, pages 573–605. Elsevier, 2003.  [13] László A Jeni and	target)	EYEDIAP (floating target)
 Jeffrey F Cohn. Person-independent 3d	target)	EYEDIAP (floating target)
 gaze estimation using face frontalization	target)	EYEDIAP (floating target)
. In Proceedings of the 	target)	EYEDIAP (floating target)
IEEE Conference on Computer Vision 	target)	EYEDIAP (floating target)
and Pattern Recognition Workshops, pages 87	target)	EYEDIAP (floating target)
–95, 2016.  [14] Heechul Jung, Sihaeng Lee	target)	EYEDIAP (floating target)
, Junho Yim, Sunjeong Park, 	target)	EYEDIAP (floating target)
and Junmo Kim. Joint fine- 	target)	EYEDIAP (floating target)
tuning in deep neural networks 	target)	EYEDIAP (floating target)
for facial expression recognition. In 	target)	EYEDIAP (floating target)
Computer Vision (ICCV), 2015 IEEE 	target)	EYEDIAP (floating target)
International Conference on, pages 2983–2991. 	target)	EYEDIAP (floating target)
IEEE, 2015.  [15] Anuradha Kar and Peter	target)	EYEDIAP (floating target)
 Corcoran. A review and analysis	target)	EYEDIAP (floating target)
 of eye-gaze estimation sys- tems	target)	EYEDIAP (floating target)
, algorithms and performance evaluation 	target)	EYEDIAP (floating target)
methods in consumer platforms. IEEE 	target)	EYEDIAP (floating target)
Access, 5:16495–16519, 2017.  [16] Kyle Krafka, Aditya Khosla	target)	EYEDIAP (floating target)
, Petr Kellnhofer, Harini Kannan, 	target)	EYEDIAP (floating target)
Suchendra Bhandarkar, Wojciech Matusik, and 	target)	EYEDIAP (floating target)
Antonio Torralba. Eye tracking for 	target)	EYEDIAP (floating target)
everyone. In Computer Vision and 	target)	EYEDIAP (floating target)
Pattern Recognition (CVPR), 2016 IEEE 	target)	EYEDIAP (floating target)
Conference on, pages 2176–2184. IEEE, 2016	target)	EYEDIAP (floating target)
.  [17] Simon P Liversedge and	target)	EYEDIAP (floating target)
 John M Findlay. Saccadic eye	target)	EYEDIAP (floating target)
 movements and cognition. Trends in	target)	EYEDIAP (floating target)
 cognitive sciences, 4(1):6–14, 2000	target)	EYEDIAP (floating target)
.  [18] Feng Lu, Takahiro Okabe	target)	EYEDIAP (floating target)
, Yusuke Sugano, and Yoichi 	target)	EYEDIAP (floating target)
Sato. A head pose-free approach 	target)	EYEDIAP (floating target)
for appearance-based gaze estimation. In 	target)	EYEDIAP (floating target)
BMVC, pages 1–11, 2011	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
12 PALMERO ET AL.: MULTI-MODAL 	target)	EYEDIAP (floating target)
RECURRENT CNN FOR 3D GAZE 	target)	EYEDIAP (floating target)
ESTIMATION  [19] Feng Lu, Yusuke Sugano	target)	EYEDIAP (floating target)
, Takahiro Okabe, and Yoichi 	target)	EYEDIAP (floating target)
Sato. Inferring human gaze from 	target)	EYEDIAP (floating target)
appearance via adaptive linear regression. 	target)	EYEDIAP (floating target)
In Computer Vision (ICCV), 2011 	target)	EYEDIAP (floating target)
IEEE International Conference on, pages 153	target)	EYEDIAP (floating target)
–160. IEEE, 2011.  [20] Päivi Majaranta and Andreas	target)	EYEDIAP (floating target)
 Bulling. Eye tracking and eye-based	target)	EYEDIAP (floating target)
 human–computer interaction. In Advances in	target)	EYEDIAP (floating target)
 physiological computing, pages 39–65. Springer	target)	EYEDIAP (floating target)
, 2014.  [21] Kenneth Alberto Funes Mora	target)	EYEDIAP (floating target)
 and Jean-Marc Odobez. Gaze estimation	target)	EYEDIAP (floating target)
 from multi- modal kinect data	target)	EYEDIAP (floating target)
. In Computer Vision and 	target)	EYEDIAP (floating target)
Pattern Recognition Workshops (CVPRW), 2012 	target)	EYEDIAP (floating target)
IEEE Computer Society Conference on, 	target)	EYEDIAP (floating target)
pages 25–30. IEEE, 2012.  [22] Carlos Hitoshi Morimoto, Arnon	target)	EYEDIAP (floating target)
 Amir, and Myron Flickner. Detecting	target)	EYEDIAP (floating target)
 eye position and gaze from	target)	EYEDIAP (floating target)
 a single camera and 2	target)	EYEDIAP (floating target)
 light sources. In Pattern Recognition	target)	EYEDIAP (floating target)
, 2002. Proceedings. 16th International 	target)	EYEDIAP (floating target)
Conference on, volume 4, pages 314	target)	EYEDIAP (floating target)
–317. IEEE, 2002.  [23] IMO MSC. Circ. 982	target)	EYEDIAP (floating target)
 (2000) guidelines on ergonomic criteria	target)	EYEDIAP (floating target)
 for bridge equipment and layout	target)	EYEDIAP (floating target)
.  [24] Alejandro Newell, Kaiyu Yang	target)	EYEDIAP (floating target)
, and Jia Deng. Stacked 	target)	EYEDIAP (floating target)
hourglass networks for hu- man 	target)	EYEDIAP (floating target)
pose estimation. In European Conference 	target)	EYEDIAP (floating target)
on Computer Vision, pages 483–499. 	target)	EYEDIAP (floating target)
Springer, 2016.  [25] Yasuhiro Ono, Takahiro Okabe	target)	EYEDIAP (floating target)
, and Yoichi Sato. Gaze 	target)	EYEDIAP (floating target)
estimation from low resolution images. 	target)	EYEDIAP (floating target)
In Pacific-Rim Symposium on Image 	target)	EYEDIAP (floating target)
and Video Technology, pages 178–188. 	target)	EYEDIAP (floating target)
Springer, 2006.  [26] Cristina Palmero, Elisabeth A	target)	EYEDIAP (floating target)
. van Dam, Sergio Escalera, 	target)	EYEDIAP (floating target)
Mike Kelia, Guido F. Lichtert, 	target)	EYEDIAP (floating target)
Lucas P.J.J Noldus, Andrew J. 	target)	EYEDIAP (floating target)
Spink, and Astrid van Wieringen. 	target)	EYEDIAP (floating target)
Automatic mutual gaze detection in 	target)	EYEDIAP (floating target)
face-to-face dyadic interaction videos. In 	target)	EYEDIAP (floating target)
Proceedings of Measuring Behavior, pages 158	target)	EYEDIAP (floating target)
–163, 2018.  [27] Omkar M. Parkhi, Andrea	target)	EYEDIAP (floating target)
 Vedaldi, and Andrew Zisserman. Deep	target)	EYEDIAP (floating target)
 face recognition. In British Machine	target)	EYEDIAP (floating target)
 Vision Conference, 2015	target)	EYEDIAP (floating target)
.  [28] Derek R Rutter and	target)	EYEDIAP (floating target)
 Kevin Durkin. Turn-taking in mother–infant	target)	EYEDIAP (floating target)
 interaction: An exam- ination of	target)	EYEDIAP (floating target)
 vocalizations and gaze. Developmental psychology	target)	EYEDIAP (floating target)
, 23(1):54, 1987.  [29] Brian A Smith, Qi	target)	EYEDIAP (floating target)
 Yin, Steven K Feiner, and	target)	EYEDIAP (floating target)
 Shree K Nayar. Gaze locking	target)	EYEDIAP (floating target)
: passive eye contact detection 	target)	EYEDIAP (floating target)
for human-object interaction. In Proceedings 	target)	EYEDIAP (floating target)
of the 26th annual ACM 	target)	EYEDIAP (floating target)
symposium on User interface software 	target)	EYEDIAP (floating target)
and technology, pages 271–280. ACM, 2013	target)	EYEDIAP (floating target)
.  [30] Yusuke Sugano, Yasuyuki Matsushita	target)	EYEDIAP (floating target)
, and Yoichi Sato. Appearance-based 	target)	EYEDIAP (floating target)
gaze es- timation using visual 	target)	EYEDIAP (floating target)
saliency. IEEE transactions on pattern 	target)	EYEDIAP (floating target)
analysis and machine intelligence, 35(2):329–341, 2013	target)	EYEDIAP (floating target)
.  [31] Yusuke Sugano, Yasuyuki Matsushita	target)	EYEDIAP (floating target)
, and Yoichi Sato. Learning-by-synthesis 	target)	EYEDIAP (floating target)
for appearance-based 3d gaze estimation. 	target)	EYEDIAP (floating target)
In Computer Vision and Pattern 	target)	EYEDIAP (floating target)
Recognition (CVPR), 2014 IEEE Conference 	target)	EYEDIAP (floating target)
on, pages 1821–1828. IEEE, 2014.  [32] Kar-Han Tan, David J	target)	EYEDIAP (floating target)
 Kriegman, and Narendra Ahuja. Appearance-based	target)	EYEDIAP (floating target)
 eye gaze es- timation. In	target)	EYEDIAP (floating target)
 Applications of Computer Vision, 2002.(WACV	target)	EYEDIAP (floating target)
 2002). Proceedings. Sixth IEEE Workshop	target)	EYEDIAP (floating target)
 on, pages 191–195. IEEE, 2002	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
PALMERO ET AL.: MULTI-MODAL RECURRENT 	target)	EYEDIAP (floating target)
CNN FOR 3D GAZE ESTIMATION 13	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
33] Ronda Venkateswarlu et al. 	target)	EYEDIAP (floating target)
Eye gaze estimation from a 	target)	EYEDIAP (floating target)
single image of one eye. 	target)	EYEDIAP (floating target)
In Computer Vision, 2003. Proceedings. 	target)	EYEDIAP (floating target)
Ninth IEEE International Conference on, 	target)	EYEDIAP (floating target)
pages 136–143. IEEE, 2003.  [34] Kang Wang and Qiang	target)	EYEDIAP (floating target)
 Ji. Real time eye gaze	target)	EYEDIAP (floating target)
 tracking with 3d deformable eye-face	target)	EYEDIAP (floating target)
 model. In Proceedings of the	target)	EYEDIAP (floating target)
 IEEE Conference on Computer Vision	target)	EYEDIAP (floating target)
 and Pattern Recog- nition, pages	target)	EYEDIAP (floating target)
 1003–1011, 2017	target)	EYEDIAP (floating target)
.  [35] Oliver Williams, Andrew Blake	target)	EYEDIAP (floating target)
, and Roberto Cipolla. Sparse 	target)	EYEDIAP (floating target)
and semi-supervised visual mapping with 	target)	EYEDIAP (floating target)
the sˆ 3gp. In Computer 	target)	EYEDIAP (floating target)
Vision and Pattern Recognition, 2006 	target)	EYEDIAP (floating target)
IEEE Computer Society Conference on, 	target)	EYEDIAP (floating target)
volume 1, pages 230–237. IEEE, 2006	target)	EYEDIAP (floating target)
.  [36] William Hyde Wollaston et	target)	EYEDIAP (floating target)
 al. Xiii. on the apparent	target)	EYEDIAP (floating target)
 direction of eyes in a	target)	EYEDIAP (floating target)
 portrait. Philosophical Transactions of the	target)	EYEDIAP (floating target)
 Royal Society of London, 114:247–256	target)	EYEDIAP (floating target)
, 1824.  [37] Erroll Wood and Andreas	target)	EYEDIAP (floating target)
 Bulling. Eyetab: Model-based gaze estimation	target)	EYEDIAP (floating target)
 on unmodi- fied tablet computers	target)	EYEDIAP (floating target)
. In Proceedings of the 	target)	EYEDIAP (floating target)
Symposium on Eye Tracking Research 	target)	EYEDIAP (floating target)
and Applications, pages 207–210. ACM, 2014	target)	EYEDIAP (floating target)
.  [38] Erroll Wood, Tadas Baltrusaitis	target)	EYEDIAP (floating target)
, Xucong Zhang, Yusuke Sugano, 	target)	EYEDIAP (floating target)
Peter Robinson, and Andreas Bulling. 	target)	EYEDIAP (floating target)
Rendering of eyes for eye-shape 	target)	EYEDIAP (floating target)
registration and gaze estimation. In 	target)	EYEDIAP (floating target)
Proceedings of the IEEE International 	target)	EYEDIAP (floating target)
Conference on Computer Vision, pages 3756	target)	EYEDIAP (floating target)
– 3764, 2015.  [39] Erroll Wood, Tadas Baltrušaitis	target)	EYEDIAP (floating target)
, Louis-Philippe Morency, Peter Robinson, 	target)	EYEDIAP (floating target)
and Andreas Bulling. A 3d 	target)	EYEDIAP (floating target)
morphable eye region model for 	target)	EYEDIAP (floating target)
gaze estimation. In European Confer- 	target)	EYEDIAP (floating target)
ence on Computer Vision, pages 297	target)	EYEDIAP (floating target)
–313. Springer, 2016.  [40] Erroll Wood, Tadas Baltrušaitis	target)	EYEDIAP (floating target)
, Louis-Philippe Morency, Peter Robinson, 	target)	EYEDIAP (floating target)
and Andreas Bulling. Learning an 	target)	EYEDIAP (floating target)
appearance-based gaze estimator from one 	target)	EYEDIAP (floating target)
million synthesised images. In Proceedings 	target)	EYEDIAP (floating target)
of the Ninth Biennial ACM 	target)	EYEDIAP (floating target)
Symposium on Eye Tracking Re- 	target)	EYEDIAP (floating target)
search & Applications, pages 131–138. 	target)	EYEDIAP (floating target)
ACM, 2016.  [41] Dong Hyun Yoo and	target)	EYEDIAP (floating target)
 Myung Jin Chung. A novel	target)	EYEDIAP (floating target)
 non-intrusive eye gaze estimation using	target)	EYEDIAP (floating target)
 cross-ratio under large head motion	target)	EYEDIAP (floating target)
. Computer Vision and Image 	target)	EYEDIAP (floating target)
Understanding, 98(1):25–51, 2005.  [42] Xucong Zhang, Yusuke Sugano	target)	EYEDIAP (floating target)
, Mario Fritz, and Andreas 	target)	EYEDIAP (floating target)
Bulling. Appearance-based gaze estimation in 	target)	EYEDIAP (floating target)
the wild. In Proceedings of 	target)	EYEDIAP (floating target)
the IEEE Conference on Computer 	target)	EYEDIAP (floating target)
Vision and Pattern Recognition, pages 4511	target)	EYEDIAP (floating target)
–4520, 2015.  [43] Xucong Zhang, Yusuke Sugano	target)	EYEDIAP (floating target)
, Mario Fritz, and Andreas 	target)	EYEDIAP (floating target)
Bulling. It’s written all over 	target)	EYEDIAP (floating target)
your face: Full-face appearance-based gaze 	target)	EYEDIAP (floating target)
estimation. In Proc. IEEE International 	target)	EYEDIAP (floating target)
Conference on Computer Vision and 	target)	EYEDIAP (floating target)
Pattern Recognition Workshops (CVPRW), 2017	target)	EYEDIAP (floating target)
	target)	EYEDIAP (floating target)
state of the art on EYEDIAP dataset, further improved by 4	EYEDIAP	EYEDIAP (floating target)
of our solution on the EYEDIAP dataset [7] in a wide	EYEDIAP	EYEDIAP (floating target)
VGA videos from the publicly-available EYEDIAP dataset [7] to perform the	EYEDIAP	EYEDIAP (floating target)
FT-S scenario are provided by EYEDIAP dataset. MPIIGaze:. State-of-the-art full-face 3D	EYEDIAP	EYEDIAP (floating target)
fine-tuned it with the filtered EYEDIAP subsets using our training parameters	EYEDIAP	EYEDIAP (floating target)
this pa- per used the EYEDIAP dataset made available by the	EYEDIAP	EYEDIAP (floating target)
sub- mission to SemEval 2017 RumourEval	RumourEval	RumourEval
curacy of 0.784 on the RumourEval test set outperforming all other	RumourEval	RumourEval
et al., 2016). However the RumourEval task is different as it	RumourEval	RumourEval
system for subtask-A: SDQC for RumourEval, task- 8 of SemEval 2017	RumourEval	RumourEval
The objective of subtask-A of RumourEval was to identify the stance	RumourEval	RumourEval
the evaluation metric for this RumourEval subtask. However, since a majority	RumourEval	RumourEval
mance for subtask A of RumourEval	RumourEval	RumourEval
sion for subtask A of RumourEval in which the participants were	RumourEval	RumourEval
Kanade dataset (CK+), and delivered 88.7	Kanade	Cohn-Kanade
Kanade dataset (CK+) [10]. By using	Kanade	Cohn-Kanade
Kanade dataset [10]. The CK+ dataset	Kanade	Cohn-Kanade
5] Y-I Tian, Takeo Kanade, and Jeffrey F Cohn, “Recognizing	Kanade	Cohn-Kanade
Lucey, Jeffrey F Cohn, Takeo Kanade, Jason Saragih, Zara Am- badar	Kanade	Cohn-Kanade
Kanade dataset (CK+): A complete dataset	Kanade	Cohn-Kanade
facial expressions in the extended Cohn	Cohn	Cohn-Kanade
for classification in the extended Cohn	Cohn	Cohn-Kanade
our experiments on the extended Cohn	Cohn	Cohn-Kanade
Takeo Kanade, and Jeffrey F Cohn, “Recognizing action units for facial	Cohn	Cohn-Kanade
10] Patrick Lucey, Jeffrey F Cohn, Takeo Kanade, Jason Saragih, Zara	Cohn	Cohn-Kanade
and Iain Matthews, “The extended Cohn	Cohn	Cohn-Kanade
facial expressions in the extended Cohn-Kanade dataset (CK+), and delivered 88.7	Cohn-Kanade	Cohn-Kanade
for classification in the extended Cohn-Kanade dataset (CK+) [10]. By using	Cohn-Kanade	Cohn-Kanade
our experiments on the extended Cohn-Kanade dataset [10]. The CK+ dataset	Cohn-Kanade	Cohn-Kanade
and Iain Matthews, “The extended Cohn-Kanade dataset (CK+): A complete dataset	Cohn-Kanade	Cohn-Kanade
Baldwin (2011). They released the LexNorm corpus, consisting of 549 Tweets	LexNorm	LexNorm
the highest accuracy on the LexNorm dataset. They rerank the results	LexNorm	LexNorm
to the annotation of the LexNorm corpus. Annotation included 1-N and	LexNorm	LexNorm
Words Lang. Caps Multiword %normalized LexNorm1	LexNorm	LexNorm
40,560 en some no 10.5 LexNorm2015 Baldwin et al. (2015b) 44,385	LexNorm	LexNorm
Note that we use the LexNorm1	LexNorm	LexNorm
improvements compared to the original LexNorm corpus. The GhentNorm corpus is	LexNorm	LexNorm
3 is taken from the LexNorm2015 corpus. This annotation also include	LexNorm	LexNorm
on pronunciation. Similar to the LexNorm 2015 annotation, the phrasal abbreviation	LexNorm	LexNorm
LexNorm1.2: for testing on the LexNorm corpus, we use 2,000 Tweets	LexNorm	LexNorm
LexNorm2015	LexNorm	LexNorm
performs especially well on the LexNorm2015 corpus. This is due to	LexNorm	LexNorm
fea- ture groups on the LexNorm2015 develop- ment data	LexNorm	LexNorm
cands LiLiu LexNorm2015 GhentNorm 1 95.6 97.6 98.3	LexNorm	LexNorm
unique correct candidates for the LexNorm2015 corpus. Furthermore, this graph shows	LexNorm	LexNorm
benchmarks. First we consider the LexNorm corpus, for which most previous	LexNorm	LexNorm
we test our performance on LexNorm2015, which was used in the	LexNorm	LexNorm
the ablation experiments on the LexNorm2015 corpus. In this setup, the	LexNorm	LexNorm
LiLiu LexNorm2015	LexNorm	LexNorm
of training data on the LexNorm2015 and LiLiu dataset	LexNorm	LexNorm
two largest datasets: LiLiu and LexNorm2015	LexNorm	LexNorm
higher F1 scores on the LexNorm2015 dataset are probably due to	LexNorm	LexNorm
train and run on the LexNorm2015 dataset, as well as the	LexNorm	LexNorm
different Aspell modes on the LexNorm2015 dataset, using our standard splits	LexNorm	LexNorm
Prev. source Eval Prev. MoNoise LexNorm1	LexNorm	LexNorm
2015) acc 87.58 5 87.63 LexNorm2015 Jin (2015) F1 84.21 86.39	LexNorm	LexNorm
with previous work. For the LexNorm1	LexNorm	LexNorm
benchmarks. The difference on the LexNorm dataset is rather small; however	LexNorm	LexNorm
The performance gap on the LexNorm2015 dataset is a bit bigger	LexNorm	LexNorm
Recall Precision F1 score LexNorm1	LexNorm	LexNorm
.2 74.45 77.56 75.97 LexNorm2015 80.26 93.53 86.39 GhentNorm 28.81	LexNorm	LexNorm
slightly adapted version of the lexnorm1	lexnorm	LexNorm
the widely-used UNBC-McMaster Shoulder- Pain dataset, achieving the state-of-the-art performance. A	dataset	UNBC-McMaster ShoulderPain dataset
one video from the Shoulder-Pain dataset [1] which provides per-frame observer-rated	dataset	UNBC-McMaster ShoulderPain dataset
Particularly, Shoulder-Pain is the only dataset available for visual analysis with	dataset	UNBC-McMaster ShoulderPain dataset
intensities (see Fig. 1), the dataset is small, the label is	dataset	UNBC-McMaster ShoulderPain dataset
Although the small dataset problem prevents us from di	dataset	UNBC-McMaster ShoulderPain dataset
intensity visually using the Shoulder-Pain dataset only: Or- dinal Support Vector	dataset	UNBC-McMaster ShoulderPain dataset
12]1 trained using the CASIA-WebFace dataset con- taning 0.5 million face	dataset	UNBC-McMaster ShoulderPain dataset
discrete values in the Shoulder-Pain dataset, it is natural to regularize	dataset	UNBC-McMaster ShoulderPain dataset
Metrics Labels in the Shoulder-Pain dataset are highly imbalanced, as 91.35	dataset	UNBC-McMaster ShoulderPain dataset
MSE (wMSE) to address the dataset imbalance issue. For example, the	dataset	UNBC-McMaster ShoulderPain dataset
see’ is a totally balanced dataset	dataset	UNBC-McMaster ShoulderPain dataset
our network on the Shoulder-Pain dataset [1] that con- tains 200	dataset	UNBC-McMaster ShoulderPain dataset
the pain intensity estimation. The dataset comes with four types of	dataset	UNBC-McMaster ShoulderPain dataset
is trained on CASIA- WebFace dataset [16], which contains 494,414 training	dataset	UNBC-McMaster ShoulderPain dataset
on the images of Shoulder-Pain dataset	dataset	UNBC-McMaster ShoulderPain dataset
related works on the Shoulder-Pain dataset for the estimation of pain	dataset	UNBC-McMaster ShoulderPain dataset
way to address over-fitting small dataset	dataset	UNBC-McMaster ShoulderPain dataset
25 times on the Shoulder-Pain dataset which contains 25 sub- jects	dataset	UNBC-McMaster ShoulderPain dataset
tested on the UNBC-McMaster Shoulder-Pain dataset and achieves state-of-the-art performance on	dataset	UNBC-McMaster ShoulderPain dataset
challenges and a mul- timodal dataset	dataset	UNBC-McMaster ShoulderPain dataset
and verified on the widely-used UNBC-McMaster Shoulder- Pain dataset, achieving the	UNBC-McMaster	UNBC-McMaster ShoulderPain dataset
layer is tested on the UNBC-McMaster Shoulder-Pain dataset and achieves state-of-the-art	UNBC-McMaster	UNBC-McMaster ShoulderPain dataset
and verified on the widely-used UNBC	UNBC	UNBC-McMaster ShoulderPain dataset
layer is tested on the UNBC	UNBC	UNBC-McMaster ShoulderPain dataset
is tested on the WireFrame dataset [1] and the YorkUrban dataset	dataset	wireframe dataset
4.5 percents on the WireFrame dataset	dataset	wireframe dataset
segment maps in the training dataset to their attraction field maps	dataset	wireframe dataset
is tested on the WireFrame dataset [1] and the YorkUrban dataset	dataset	wireframe dataset
by 4.5% on the WireFrame dataset	dataset	wireframe dataset
human-level performance on the BSDS500 dataset [26]. Followed by this breakthrough	dataset	wireframe dataset
goal by proposing a large-scale dataset with high quality line segment	dataset	wireframe dataset
LSD benchmarks, the Wire- Frame dataset (with 4.5% significant improvement) and	dataset	wireframe dataset
the YorkUrban dataset	dataset	wireframe dataset
field representation on the WireFrame dataset [1]. We first compute the	dataset	wireframe dataset
the protocol provided in the dataset	dataset	wireframe dataset
N} the provided training dataset consisting ofN pairs of raw	dataset	wireframe dataset
N} be the dual training dataset	dataset	wireframe dataset
N} the final training dataset	dataset	wireframe dataset
train- ing set of Wireframe dataset [1]. Similar to [1], we	dataset	wireframe dataset
existing methods on the WireFrame dataset [1] and YorkUr- ban dataset	dataset	wireframe dataset
we train on the Wreframe dataset [1], it is necessary to	dataset	wireframe dataset
proposed method on its testing dataset, which includes 462 images for	dataset	wireframe dataset
ods on the WireFrame [1] dataset	dataset	wireframe dataset
state-of-the-art approaches on the WireFrame dataset and York Urban dataset. The	dataset	wireframe dataset
ond (FPS) on the WireFrame dataset	dataset	wireframe dataset
dataset York Urban	dataset	wireframe dataset
dataset FPS	dataset	wireframe dataset
proposed method on the Wireframe dataset [1] and YorkUrban dataset [2	dataset	wireframe dataset
performance. Due to the YorkUrban dataset aiming at Man- hattan frame	dataset	wireframe dataset
of all methods on this dataset decreased	dataset	wireframe dataset
abovementioned approaches on the Wireframe dataset	dataset	wireframe dataset
with different methods on Wireframe dataset (see Figure 6), YorkUrban dataset	dataset	wireframe dataset
segment detection on Wireframe [1] dataset with different approaches LSD [10	dataset	wireframe dataset
segment detection on YorkUrban [2] dataset with different approaches LSD [10	dataset	wireframe dataset
used LSD benchmarks, the WireFrame dataset [1] and the YorkUrban dataset	dataset	wireframe dataset
wireframe 4https://github.com/NamgyuCho	wireframe	wireframe dataset
line segments fast. The deep wireframe parser [1] spends much time	wireframe	wireframe dataset
wireframe https://github.com/NamgyuCho/Linelet-code-and-YorkUrban-LineSegment-DB https://github.com/NamgyuCho/Linelet-code-and-YorkUrban-LineSegment-DB http://www.elderlab.yorku.ca/resources/ http://www.ipol.im/pub/art/2012/gjmr-lsd	wireframe	wireframe dataset
cinic-10	cinic-10	CINIC-10
cinic-10	cinic-10	CINIC-10
cinic-10	cinic-10	CINIC-10
cinic-10	cinic-10	CINIC-10
10 Is Not ImageNet or CIFAR-10	10	CINIC-10
10 Is Not ImageNet or CIFAR-10	10	CINIC-10
10 dataset as a plug-in extended	10	CINIC-10
10	10	CINIC-10
10 with images selected and downsampled	10	CINIC-10
this reason, CIFAR-10 and CIFAR- 100 (Krizhevsky, 2009) have become the	10	CINIC-10
In CIFAR-10, each of the 10 classes has 6,000 examples. The	10	CINIC-10
100 classes of CIFAR-100 only have	10	CINIC-10
100 is arguably more difficult than	10	CINIC-10
10	10	CINIC-10
10	10	CINIC-10
10	10	CINIC-10
10 Is Not ImageNet or CIFAR-10	10	CINIC-10
10 via the addition of downsampled	10	CINIC-10
10 has the following desirable properties	10	CINIC-10
10 can be used as a	10	CINIC-10
10	10	CINIC-10
10 consists of images from both	10	CINIC-10
10 dataset. This process is detailed	10	CINIC-10
10 The original CIFAR-10 is processed	10	CINIC-10
10	10	CINIC-10
10 class (airplane, automobile etc	10	CINIC-10
10 data: 20,000 images per set	10	CINIC-10
10 data among all three sets	10	CINIC-10
10 test set contains the entirety	10	CINIC-10
10 test set, as well as	10	CINIC-10
10 train set. CINIC-10’s train and	10	CINIC-10
10 can be fully recovered from	10	CINIC-10
10 by the filename	10	CINIC-10
10	10	CINIC-10
10 is listed in imagenet- contributors.csv	10	CINIC-10
10 by augmenting the CIFAR-10 data	10	CINIC-10
10 classes (airplane, automobile, etc.). synset	10	CINIC-10
10 data and the remaining from	10	CINIC-10
10 is with a PyTorch5 data	10	CINIC-10
10 (Section 4.2	10	CINIC-10
10 and ImageNet contributors is given	10	CINIC-10
0 50 100 150 200 250	10	CINIC-10
10	10	CINIC-10
10 (blue) and ImageNet contributions (red	10	CINIC-10
10	10	CINIC-10
10 images will note that these	10	CINIC-10
10 (cows and goats in the	10	CINIC-10
10, automobile	10	CINIC-10
10, airplane	10	CINIC-10
10	10	CINIC-10
106 were copied and tested	10	CINIC-10
10, bird	10	CINIC-10
10, cat	10	CINIC-10
10 benchmarks	10	CINIC-10
10, deer	10	CINIC-10
10, dog	10	CINIC-10
10, frog	10	CINIC-10
10, horse	10	CINIC-10
Figure 10	10	CINIC-10
10, ship	10	CINIC-10
10, truck	10	CINIC-10
10 in this technical report. It	10	CINIC-10
10 with downsam- pled images sourced	10	CINIC-10
10 (and more challenging) but not	10	CINIC-10
10 Github repository	10	CINIC-10
10	10	CINIC-10
Vision (IJCV), 115(3):211–252, 2015. doi: 10	10	CINIC-10
1007	10	CINIC-10
10	10	CINIC-10
CINIC	CINIC	CINIC-10
CINIC	CINIC	CINIC-10
technical report we introduce the CINIC	CINIC	CINIC-10
existing benchmarking datasets, we present CINIC	CINIC	CINIC-10
-10: CINIC	CINIC	CINIC-10
addition of downsampled ImageNet images. CINIC	CINIC	CINIC-10
as in CIFAR, meaning that CINIC	CINIC	CINIC-10
CINIC	CINIC	CINIC-10
our method of constructing the CINIC	CINIC	CINIC-10
among all three sets. The CINIC	CINIC	CINIC-10
from the CIFAR-10 train set. CINIC	CINIC	CINIC-10
can be fully recovered from CINIC	CINIC	CINIC-10
The mapping from sysnsets to CINIC	CINIC	CINIC-10
from each synset-group to compile CINIC	CINIC	CINIC-10
The simplest way to use CINIC	CINIC	CINIC-10
examples for each class in CINIC	CINIC	CINIC-10
show randomly selected samples from CINIC	CINIC	CINIC-10
Figure 2: CINIC	CINIC	CINIC-10
Figure 3: CINIC	CINIC	CINIC-10
1 gives benchmark results on CINIC	CINIC	CINIC-10
Figure 4: CINIC	CINIC	CINIC-10
Figure 5: CINIC	CINIC	CINIC-10
Table 1: CINIC	CINIC	CINIC-10
Figure 6: CINIC	CINIC	CINIC-10
Figure 7: CINIC	CINIC	CINIC-10
Figure 8: CINIC	CINIC	CINIC-10
Figure 9: CINIC	CINIC	CINIC-10
Figure 10: CINIC	CINIC	CINIC-10
Figure 11: CINIC	CINIC	CINIC-10
We presented CINIC	CINIC	CINIC-10
Antoniou, and Amos J. Storkey. CINIC	CINIC	CINIC-10
CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-10	CINIC-10
CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-10	CINIC-10
technical report we introduce the CINIC-10 dataset as a plug-in extended	CINIC-10	CINIC-10
existing benchmarking datasets, we present CINIC-10	CINIC-10	CINIC-10
: CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-10	CINIC-10
addition of downsampled ImageNet images. CINIC-10 has the following desirable properties	CINIC-10	CINIC-10
as in CIFAR, meaning that CINIC-10 can be used as a	CINIC-10	CINIC-10
CINIC-10 consists of images from both	CINIC-10	CINIC-10
our method of constructing the CINIC-10 dataset. This process is detailed	CINIC-10	CINIC-10
among all three sets. The CINIC-10 test set contains the entirety	CINIC-10	CINIC-10
from the CIFAR-10 train set. CINIC-10	CINIC-10	CINIC-10
can be fully recovered from CINIC-10 by the filename	CINIC-10	CINIC-10
The mapping from sysnsets to CINIC-10 is listed in imagenet- contributors.csv	CINIC-10	CINIC-10
from each synset-group to compile CINIC-10 by augmenting the CIFAR-10 data	CINIC-10	CINIC-10
The simplest way to use CINIC-10 is with a PyTorch5 data	CINIC-10	CINIC-10
examples for each class in CINIC-10 (Section 4.2	CINIC-10	CINIC-10
show randomly selected samples from CINIC-10	CINIC-10	CINIC-10
Figure 2: CINIC-10, automobile	CINIC-10	CINIC-10
Figure 3: CINIC-10, airplane	CINIC-10	CINIC-10
1 gives benchmark results on CINIC-10	CINIC-10	CINIC-10
Figure 4: CINIC-10, bird	CINIC-10	CINIC-10
Figure 5: CINIC-10, cat	CINIC-10	CINIC-10
Table 1: CINIC-10 benchmarks	CINIC-10	CINIC-10
Figure 6: CINIC-10, deer	CINIC-10	CINIC-10
Figure 7: CINIC-10, dog	CINIC-10	CINIC-10
Figure 8: CINIC-10, frog	CINIC-10	CINIC-10
Figure 9: CINIC-10, horse	CINIC-10	CINIC-10
Figure 10: CINIC-10, ship	CINIC-10	CINIC-10
Figure 11: CINIC-10, truck	CINIC-10	CINIC-10
We presented CINIC-10 in this technical report. It	CINIC-10	CINIC-10
Antoniou, and Amos J. Storkey. CINIC-10 Github repository	CINIC-10	CINIC-10
CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-	CINIC-10
CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-	CINIC-10
technical report we introduce the CINIC-10 dataset as a plug-in extended	CINIC-	CINIC-10
existing benchmarking datasets, we present CINIC-10	CINIC-	CINIC-10
: CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-	CINIC-10
addition of downsampled ImageNet images. CINIC-10 has the following desirable properties	CINIC-	CINIC-10
as in CIFAR, meaning that CINIC-10 can be used as a	CINIC-	CINIC-10
CINIC-10 consists of images from both	CINIC-	CINIC-10
our method of constructing the CINIC-10 dataset. This process is detailed	CINIC-	CINIC-10
among all three sets. The CINIC-10 test set contains the entirety	CINIC-	CINIC-10
from the CIFAR-10 train set. CINIC-10	CINIC-	CINIC-10
can be fully recovered from CINIC-10 by the filename	CINIC-	CINIC-10
The mapping from sysnsets to CINIC-10 is listed in imagenet- contributors.csv	CINIC-	CINIC-10
from each synset-group to compile CINIC-10 by augmenting the CIFAR-10 data	CINIC-	CINIC-10
The simplest way to use CINIC-10 is with a PyTorch5 data	CINIC-	CINIC-10
examples for each class in CINIC-10 (Section 4.2	CINIC-	CINIC-10
show randomly selected samples from CINIC-10	CINIC-	CINIC-10
Figure 2: CINIC-10, automobile	CINIC-	CINIC-10
Figure 3: CINIC-10, airplane	CINIC-	CINIC-10
1 gives benchmark results on CINIC-10	CINIC-	CINIC-10
Figure 4: CINIC-10, bird	CINIC-	CINIC-10
Figure 5: CINIC-10, cat	CINIC-	CINIC-10
Table 1: CINIC-10 benchmarks	CINIC-	CINIC-10
Figure 6: CINIC-10, deer	CINIC-	CINIC-10
Figure 7: CINIC-10, dog	CINIC-	CINIC-10
Figure 8: CINIC-10, frog	CINIC-	CINIC-10
Figure 9: CINIC-10, horse	CINIC-	CINIC-10
Figure 10: CINIC-10, ship	CINIC-	CINIC-10
Figure 11: CINIC-10, truck	CINIC-	CINIC-10
We presented CINIC-10 in this technical report. It	CINIC-	CINIC-10
Antoniou, and Amos J. Storkey. CINIC-10 Github repository	CINIC-	CINIC-10
cinic-10	cinic-10	CINIC-10
cinic-10	cinic-10	CINIC-10
cinic-10	cinic-10	CINIC-10
cinic-10	cinic-10	CINIC-10
10 Is Not ImageNet or CIFAR-10	10	CINIC-10
10 Is Not ImageNet or CIFAR-10	10	CINIC-10
10 dataset as a plug-in extended	10	CINIC-10
10	10	CINIC-10
10 with images selected and downsampled	10	CINIC-10
this reason, CIFAR-10 and CIFAR- 100 (Krizhevsky, 2009) have become the	10	CINIC-10
In CIFAR-10, each of the 10 classes has 6,000 examples. The	10	CINIC-10
100 classes of CIFAR-100 only have	10	CINIC-10
100 is arguably more difficult than	10	CINIC-10
10	10	CINIC-10
10	10	CINIC-10
10	10	CINIC-10
10 Is Not ImageNet or CIFAR-10	10	CINIC-10
10 via the addition of downsampled	10	CINIC-10
10 has the following desirable properties	10	CINIC-10
10 can be used as a	10	CINIC-10
10	10	CINIC-10
10 consists of images from both	10	CINIC-10
10 dataset. This process is detailed	10	CINIC-10
10 The original CIFAR-10 is processed	10	CINIC-10
10	10	CINIC-10
10 class (airplane, automobile etc	10	CINIC-10
10 data: 20,000 images per set	10	CINIC-10
10 data among all three sets	10	CINIC-10
10 test set contains the entirety	10	CINIC-10
10 test set, as well as	10	CINIC-10
10 train set. CINIC-10’s train and	10	CINIC-10
10 can be fully recovered from	10	CINIC-10
10 by the filename	10	CINIC-10
10	10	CINIC-10
10 is listed in imagenet- contributors.csv	10	CINIC-10
10 by augmenting the CIFAR-10 data	10	CINIC-10
10 classes (airplane, automobile, etc.). synset	10	CINIC-10
10 data and the remaining from	10	CINIC-10
10 is with a PyTorch5 data	10	CINIC-10
10 (Section 4.2	10	CINIC-10
10 and ImageNet contributors is given	10	CINIC-10
0 50 100 150 200 250	10	CINIC-10
10	10	CINIC-10
10 (blue) and ImageNet contributions (red	10	CINIC-10
10	10	CINIC-10
10 images will note that these	10	CINIC-10
10 (cows and goats in the	10	CINIC-10
10, automobile	10	CINIC-10
10, airplane	10	CINIC-10
10	10	CINIC-10
106 were copied and tested	10	CINIC-10
10, bird	10	CINIC-10
10, cat	10	CINIC-10
10 benchmarks	10	CINIC-10
10, deer	10	CINIC-10
10, dog	10	CINIC-10
10, frog	10	CINIC-10
10, horse	10	CINIC-10
Figure 10	10	CINIC-10
10, ship	10	CINIC-10
10, truck	10	CINIC-10
10 in this technical report. It	10	CINIC-10
10 with downsam- pled images sourced	10	CINIC-10
10 (and more challenging) but not	10	CINIC-10
10 Github repository	10	CINIC-10
10	10	CINIC-10
Vision (IJCV), 115(3):211–252, 2015. doi: 10	10	CINIC-10
1007	10	CINIC-10
10	10	CINIC-10
CINIC	CINIC	CINIC-10
CINIC	CINIC	CINIC-10
technical report we introduce the CINIC	CINIC	CINIC-10
existing benchmarking datasets, we present CINIC	CINIC	CINIC-10
-10: CINIC	CINIC	CINIC-10
addition of downsampled ImageNet images. CINIC	CINIC	CINIC-10
as in CIFAR, meaning that CINIC	CINIC	CINIC-10
CINIC	CINIC	CINIC-10
our method of constructing the CINIC	CINIC	CINIC-10
among all three sets. The CINIC	CINIC	CINIC-10
from the CIFAR-10 train set. CINIC	CINIC	CINIC-10
can be fully recovered from CINIC	CINIC	CINIC-10
The mapping from sysnsets to CINIC	CINIC	CINIC-10
from each synset-group to compile CINIC	CINIC	CINIC-10
The simplest way to use CINIC	CINIC	CINIC-10
examples for each class in CINIC	CINIC	CINIC-10
show randomly selected samples from CINIC	CINIC	CINIC-10
Figure 2: CINIC	CINIC	CINIC-10
Figure 3: CINIC	CINIC	CINIC-10
1 gives benchmark results on CINIC	CINIC	CINIC-10
Figure 4: CINIC	CINIC	CINIC-10
Figure 5: CINIC	CINIC	CINIC-10
Table 1: CINIC	CINIC	CINIC-10
Figure 6: CINIC	CINIC	CINIC-10
Figure 7: CINIC	CINIC	CINIC-10
Figure 8: CINIC	CINIC	CINIC-10
Figure 9: CINIC	CINIC	CINIC-10
Figure 10: CINIC	CINIC	CINIC-10
Figure 11: CINIC	CINIC	CINIC-10
We presented CINIC	CINIC	CINIC-10
Antoniou, and Amos J. Storkey. CINIC	CINIC	CINIC-10
CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-10	CINIC-10
CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-10	CINIC-10
technical report we introduce the CINIC-10 dataset as a plug-in extended	CINIC-10	CINIC-10
existing benchmarking datasets, we present CINIC-10	CINIC-10	CINIC-10
: CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-10	CINIC-10
addition of downsampled ImageNet images. CINIC-10 has the following desirable properties	CINIC-10	CINIC-10
as in CIFAR, meaning that CINIC-10 can be used as a	CINIC-10	CINIC-10
CINIC-10 consists of images from both	CINIC-10	CINIC-10
our method of constructing the CINIC-10 dataset. This process is detailed	CINIC-10	CINIC-10
among all three sets. The CINIC-10 test set contains the entirety	CINIC-10	CINIC-10
from the CIFAR-10 train set. CINIC-10	CINIC-10	CINIC-10
can be fully recovered from CINIC-10 by the filename	CINIC-10	CINIC-10
The mapping from sysnsets to CINIC-10 is listed in imagenet- contributors.csv	CINIC-10	CINIC-10
from each synset-group to compile CINIC-10 by augmenting the CIFAR-10 data	CINIC-10	CINIC-10
The simplest way to use CINIC-10 is with a PyTorch5 data	CINIC-10	CINIC-10
examples for each class in CINIC-10 (Section 4.2	CINIC-10	CINIC-10
show randomly selected samples from CINIC-10	CINIC-10	CINIC-10
Figure 2: CINIC-10, automobile	CINIC-10	CINIC-10
Figure 3: CINIC-10, airplane	CINIC-10	CINIC-10
1 gives benchmark results on CINIC-10	CINIC-10	CINIC-10
Figure 4: CINIC-10, bird	CINIC-10	CINIC-10
Figure 5: CINIC-10, cat	CINIC-10	CINIC-10
Table 1: CINIC-10 benchmarks	CINIC-10	CINIC-10
Figure 6: CINIC-10, deer	CINIC-10	CINIC-10
Figure 7: CINIC-10, dog	CINIC-10	CINIC-10
Figure 8: CINIC-10, frog	CINIC-10	CINIC-10
Figure 9: CINIC-10, horse	CINIC-10	CINIC-10
Figure 10: CINIC-10, ship	CINIC-10	CINIC-10
Figure 11: CINIC-10, truck	CINIC-10	CINIC-10
We presented CINIC-10 in this technical report. It	CINIC-10	CINIC-10
Antoniou, and Amos J. Storkey. CINIC-10 Github repository	CINIC-10	CINIC-10
CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-	CINIC-10
CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-	CINIC-10
technical report we introduce the CINIC-10 dataset as a plug-in extended	CINIC-	CINIC-10
existing benchmarking datasets, we present CINIC-10	CINIC-	CINIC-10
: CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-	CINIC-10
addition of downsampled ImageNet images. CINIC-10 has the following desirable properties	CINIC-	CINIC-10
as in CIFAR, meaning that CINIC-10 can be used as a	CINIC-	CINIC-10
CINIC-10 consists of images from both	CINIC-	CINIC-10
our method of constructing the CINIC-10 dataset. This process is detailed	CINIC-	CINIC-10
among all three sets. The CINIC-10 test set contains the entirety	CINIC-	CINIC-10
from the CIFAR-10 train set. CINIC-10	CINIC-	CINIC-10
can be fully recovered from CINIC-10 by the filename	CINIC-	CINIC-10
The mapping from sysnsets to CINIC-10 is listed in imagenet- contributors.csv	CINIC-	CINIC-10
from each synset-group to compile CINIC-10 by augmenting the CIFAR-10 data	CINIC-	CINIC-10
The simplest way to use CINIC-10 is with a PyTorch5 data	CINIC-	CINIC-10
examples for each class in CINIC-10 (Section 4.2	CINIC-	CINIC-10
show randomly selected samples from CINIC-10	CINIC-	CINIC-10
Figure 2: CINIC-10, automobile	CINIC-	CINIC-10
Figure 3: CINIC-10, airplane	CINIC-	CINIC-10
1 gives benchmark results on CINIC-10	CINIC-	CINIC-10
Figure 4: CINIC-10, bird	CINIC-	CINIC-10
Figure 5: CINIC-10, cat	CINIC-	CINIC-10
Table 1: CINIC-10 benchmarks	CINIC-	CINIC-10
Figure 6: CINIC-10, deer	CINIC-	CINIC-10
Figure 7: CINIC-10, dog	CINIC-	CINIC-10
Figure 8: CINIC-10, frog	CINIC-	CINIC-10
Figure 9: CINIC-10, horse	CINIC-	CINIC-10
Figure 10: CINIC-10, ship	CINIC-	CINIC-10
Figure 11: CINIC-10, truck	CINIC-	CINIC-10
We presented CINIC-10 in this technical report. It	CINIC-	CINIC-10
Antoniou, and Amos J. Storkey. CINIC-10 Github repository	CINIC-	CINIC-10
cinic-10	cinic-10	CINIC-10
cinic-10	cinic-10	CINIC-10
cinic-10	cinic-10	CINIC-10
cinic-10	cinic-10	CINIC-10
10 Is Not ImageNet or CIFAR-10	10	CINIC-10
10 Is Not ImageNet or CIFAR-10	10	CINIC-10
10 dataset as a plug-in extended	10	CINIC-10
10	10	CINIC-10
10 with images selected and downsampled	10	CINIC-10
this reason, CIFAR-10 and CIFAR- 100 (Krizhevsky, 2009) have become the	10	CINIC-10
In CIFAR-10, each of the 10 classes has 6,000 examples. The	10	CINIC-10
100 classes of CIFAR-100 only have	10	CINIC-10
100 is arguably more difficult than	10	CINIC-10
10	10	CINIC-10
10	10	CINIC-10
10	10	CINIC-10
10 Is Not ImageNet or CIFAR-10	10	CINIC-10
10 via the addition of downsampled	10	CINIC-10
10 has the following desirable properties	10	CINIC-10
10 can be used as a	10	CINIC-10
10	10	CINIC-10
10 consists of images from both	10	CINIC-10
10 dataset. This process is detailed	10	CINIC-10
10 The original CIFAR-10 is processed	10	CINIC-10
10	10	CINIC-10
10 class (airplane, automobile etc	10	CINIC-10
10 data: 20,000 images per set	10	CINIC-10
10 data among all three sets	10	CINIC-10
10 test set contains the entirety	10	CINIC-10
10 test set, as well as	10	CINIC-10
10 train set. CINIC-10’s train and	10	CINIC-10
10 can be fully recovered from	10	CINIC-10
10 by the filename	10	CINIC-10
10	10	CINIC-10
10 is listed in imagenet- contributors.csv	10	CINIC-10
10 by augmenting the CIFAR-10 data	10	CINIC-10
10 classes (airplane, automobile, etc.). synset	10	CINIC-10
10 data and the remaining from	10	CINIC-10
10 is with a PyTorch5 data	10	CINIC-10
10 (Section 4.2	10	CINIC-10
10 and ImageNet contributors is given	10	CINIC-10
0 50 100 150 200 250	10	CINIC-10
10	10	CINIC-10
10 (blue) and ImageNet contributions (red	10	CINIC-10
10	10	CINIC-10
10 images will note that these	10	CINIC-10
10 (cows and goats in the	10	CINIC-10
10, automobile	10	CINIC-10
10, airplane	10	CINIC-10
10	10	CINIC-10
106 were copied and tested	10	CINIC-10
10, bird	10	CINIC-10
10, cat	10	CINIC-10
10 benchmarks	10	CINIC-10
10, deer	10	CINIC-10
10, dog	10	CINIC-10
10, frog	10	CINIC-10
10, horse	10	CINIC-10
Figure 10	10	CINIC-10
10, ship	10	CINIC-10
10, truck	10	CINIC-10
10 in this technical report. It	10	CINIC-10
10 with downsam- pled images sourced	10	CINIC-10
10 (and more challenging) but not	10	CINIC-10
10 Github repository	10	CINIC-10
10	10	CINIC-10
Vision (IJCV), 115(3):211–252, 2015. doi: 10	10	CINIC-10
1007	10	CINIC-10
10	10	CINIC-10
CINIC	CINIC	CINIC-10
CINIC	CINIC	CINIC-10
technical report we introduce the CINIC	CINIC	CINIC-10
existing benchmarking datasets, we present CINIC	CINIC	CINIC-10
-10: CINIC	CINIC	CINIC-10
addition of downsampled ImageNet images. CINIC	CINIC	CINIC-10
as in CIFAR, meaning that CINIC	CINIC	CINIC-10
CINIC	CINIC	CINIC-10
our method of constructing the CINIC	CINIC	CINIC-10
among all three sets. The CINIC	CINIC	CINIC-10
from the CIFAR-10 train set. CINIC	CINIC	CINIC-10
can be fully recovered from CINIC	CINIC	CINIC-10
The mapping from sysnsets to CINIC	CINIC	CINIC-10
from each synset-group to compile CINIC	CINIC	CINIC-10
The simplest way to use CINIC	CINIC	CINIC-10
examples for each class in CINIC	CINIC	CINIC-10
show randomly selected samples from CINIC	CINIC	CINIC-10
Figure 2: CINIC	CINIC	CINIC-10
Figure 3: CINIC	CINIC	CINIC-10
1 gives benchmark results on CINIC	CINIC	CINIC-10
Figure 4: CINIC	CINIC	CINIC-10
Figure 5: CINIC	CINIC	CINIC-10
Table 1: CINIC	CINIC	CINIC-10
Figure 6: CINIC	CINIC	CINIC-10
Figure 7: CINIC	CINIC	CINIC-10
Figure 8: CINIC	CINIC	CINIC-10
Figure 9: CINIC	CINIC	CINIC-10
Figure 10: CINIC	CINIC	CINIC-10
Figure 11: CINIC	CINIC	CINIC-10
We presented CINIC	CINIC	CINIC-10
Antoniou, and Amos J. Storkey. CINIC	CINIC	CINIC-10
CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-10	CINIC-10
CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-10	CINIC-10
technical report we introduce the CINIC-10 dataset as a plug-in extended	CINIC-10	CINIC-10
existing benchmarking datasets, we present CINIC-10	CINIC-10	CINIC-10
: CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-10	CINIC-10
addition of downsampled ImageNet images. CINIC-10 has the following desirable properties	CINIC-10	CINIC-10
as in CIFAR, meaning that CINIC-10 can be used as a	CINIC-10	CINIC-10
CINIC-10 consists of images from both	CINIC-10	CINIC-10
our method of constructing the CINIC-10 dataset. This process is detailed	CINIC-10	CINIC-10
among all three sets. The CINIC-10 test set contains the entirety	CINIC-10	CINIC-10
from the CIFAR-10 train set. CINIC-10	CINIC-10	CINIC-10
can be fully recovered from CINIC-10 by the filename	CINIC-10	CINIC-10
The mapping from sysnsets to CINIC-10 is listed in imagenet- contributors.csv	CINIC-10	CINIC-10
from each synset-group to compile CINIC-10 by augmenting the CIFAR-10 data	CINIC-10	CINIC-10
The simplest way to use CINIC-10 is with a PyTorch5 data	CINIC-10	CINIC-10
examples for each class in CINIC-10 (Section 4.2	CINIC-10	CINIC-10
show randomly selected samples from CINIC-10	CINIC-10	CINIC-10
Figure 2: CINIC-10, automobile	CINIC-10	CINIC-10
Figure 3: CINIC-10, airplane	CINIC-10	CINIC-10
1 gives benchmark results on CINIC-10	CINIC-10	CINIC-10
Figure 4: CINIC-10, bird	CINIC-10	CINIC-10
Figure 5: CINIC-10, cat	CINIC-10	CINIC-10
Table 1: CINIC-10 benchmarks	CINIC-10	CINIC-10
Figure 6: CINIC-10, deer	CINIC-10	CINIC-10
Figure 7: CINIC-10, dog	CINIC-10	CINIC-10
Figure 8: CINIC-10, frog	CINIC-10	CINIC-10
Figure 9: CINIC-10, horse	CINIC-10	CINIC-10
Figure 10: CINIC-10, ship	CINIC-10	CINIC-10
Figure 11: CINIC-10, truck	CINIC-10	CINIC-10
We presented CINIC-10 in this technical report. It	CINIC-10	CINIC-10
Antoniou, and Amos J. Storkey. CINIC-10 Github repository	CINIC-10	CINIC-10
CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-	CINIC-10
CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-	CINIC-10
technical report we introduce the CINIC-10 dataset as a plug-in extended	CINIC-	CINIC-10
existing benchmarking datasets, we present CINIC-10	CINIC-	CINIC-10
: CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-	CINIC-10
addition of downsampled ImageNet images. CINIC-10 has the following desirable properties	CINIC-	CINIC-10
as in CIFAR, meaning that CINIC-10 can be used as a	CINIC-	CINIC-10
CINIC-10 consists of images from both	CINIC-	CINIC-10
our method of constructing the CINIC-10 dataset. This process is detailed	CINIC-	CINIC-10
among all three sets. The CINIC-10 test set contains the entirety	CINIC-	CINIC-10
from the CIFAR-10 train set. CINIC-10	CINIC-	CINIC-10
can be fully recovered from CINIC-10 by the filename	CINIC-	CINIC-10
The mapping from sysnsets to CINIC-10 is listed in imagenet- contributors.csv	CINIC-	CINIC-10
from each synset-group to compile CINIC-10 by augmenting the CIFAR-10 data	CINIC-	CINIC-10
The simplest way to use CINIC-10 is with a PyTorch5 data	CINIC-	CINIC-10
examples for each class in CINIC-10 (Section 4.2	CINIC-	CINIC-10
show randomly selected samples from CINIC-10	CINIC-	CINIC-10
Figure 2: CINIC-10, automobile	CINIC-	CINIC-10
Figure 3: CINIC-10, airplane	CINIC-	CINIC-10
1 gives benchmark results on CINIC-10	CINIC-	CINIC-10
Figure 4: CINIC-10, bird	CINIC-	CINIC-10
Figure 5: CINIC-10, cat	CINIC-	CINIC-10
Table 1: CINIC-10 benchmarks	CINIC-	CINIC-10
Figure 6: CINIC-10, deer	CINIC-	CINIC-10
Figure 7: CINIC-10, dog	CINIC-	CINIC-10
Figure 8: CINIC-10, frog	CINIC-	CINIC-10
Figure 9: CINIC-10, horse	CINIC-	CINIC-10
Figure 10: CINIC-10, ship	CINIC-	CINIC-10
Figure 11: CINIC-10, truck	CINIC-	CINIC-10
We presented CINIC-10 in this technical report. It	CINIC-	CINIC-10
Antoniou, and Amos J. Storkey. CINIC-10 Github repository	CINIC-	CINIC-10
cinic-10	cinic-10	CINIC-10
cinic-10	cinic-10	CINIC-10
cinic-10	cinic-10	CINIC-10
cinic-10	cinic-10	CINIC-10
10 Is Not ImageNet or CIFAR-10	10	CINIC-10
10 Is Not ImageNet or CIFAR-10	10	CINIC-10
10 dataset as a plug-in extended	10	CINIC-10
10	10	CINIC-10
10 with images selected and downsampled	10	CINIC-10
this reason, CIFAR-10 and CIFAR- 100 (Krizhevsky, 2009) have become the	10	CINIC-10
In CIFAR-10, each of the 10 classes has 6,000 examples. The	10	CINIC-10
100 classes of CIFAR-100 only have	10	CINIC-10
100 is arguably more difficult than	10	CINIC-10
10	10	CINIC-10
10	10	CINIC-10
10	10	CINIC-10
10 Is Not ImageNet or CIFAR-10	10	CINIC-10
10 via the addition of downsampled	10	CINIC-10
10 has the following desirable properties	10	CINIC-10
10 can be used as a	10	CINIC-10
10	10	CINIC-10
10 consists of images from both	10	CINIC-10
10 dataset. This process is detailed	10	CINIC-10
10 The original CIFAR-10 is processed	10	CINIC-10
10	10	CINIC-10
10 class (airplane, automobile etc	10	CINIC-10
10 data: 20,000 images per set	10	CINIC-10
10 data among all three sets	10	CINIC-10
10 test set contains the entirety	10	CINIC-10
10 test set, as well as	10	CINIC-10
10 train set. CINIC-10’s train and	10	CINIC-10
10 can be fully recovered from	10	CINIC-10
10 by the filename	10	CINIC-10
10	10	CINIC-10
10 is listed in imagenet- contributors.csv	10	CINIC-10
10 by augmenting the CIFAR-10 data	10	CINIC-10
10 classes (airplane, automobile, etc.). synset	10	CINIC-10
10 data and the remaining from	10	CINIC-10
10 is with a PyTorch5 data	10	CINIC-10
10 (Section 4.2	10	CINIC-10
10 and ImageNet contributors is given	10	CINIC-10
0 50 100 150 200 250	10	CINIC-10
10	10	CINIC-10
10 (blue) and ImageNet contributions (red	10	CINIC-10
10	10	CINIC-10
10 images will note that these	10	CINIC-10
10 (cows and goats in the	10	CINIC-10
10, automobile	10	CINIC-10
10, airplane	10	CINIC-10
10	10	CINIC-10
106 were copied and tested	10	CINIC-10
10, bird	10	CINIC-10
10, cat	10	CINIC-10
10 benchmarks	10	CINIC-10
10, deer	10	CINIC-10
10, dog	10	CINIC-10
10, frog	10	CINIC-10
10, horse	10	CINIC-10
Figure 10	10	CINIC-10
10, ship	10	CINIC-10
10, truck	10	CINIC-10
10 in this technical report. It	10	CINIC-10
10 with downsam- pled images sourced	10	CINIC-10
10 (and more challenging) but not	10	CINIC-10
10 Github repository	10	CINIC-10
10	10	CINIC-10
Vision (IJCV), 115(3):211–252, 2015. doi: 10	10	CINIC-10
1007	10	CINIC-10
10	10	CINIC-10
CINIC	CINIC	CINIC-10
CINIC	CINIC	CINIC-10
technical report we introduce the CINIC	CINIC	CINIC-10
existing benchmarking datasets, we present CINIC	CINIC	CINIC-10
-10: CINIC	CINIC	CINIC-10
addition of downsampled ImageNet images. CINIC	CINIC	CINIC-10
as in CIFAR, meaning that CINIC	CINIC	CINIC-10
CINIC	CINIC	CINIC-10
our method of constructing the CINIC	CINIC	CINIC-10
among all three sets. The CINIC	CINIC	CINIC-10
from the CIFAR-10 train set. CINIC	CINIC	CINIC-10
can be fully recovered from CINIC	CINIC	CINIC-10
The mapping from sysnsets to CINIC	CINIC	CINIC-10
from each synset-group to compile CINIC	CINIC	CINIC-10
The simplest way to use CINIC	CINIC	CINIC-10
examples for each class in CINIC	CINIC	CINIC-10
show randomly selected samples from CINIC	CINIC	CINIC-10
Figure 2: CINIC	CINIC	CINIC-10
Figure 3: CINIC	CINIC	CINIC-10
1 gives benchmark results on CINIC	CINIC	CINIC-10
Figure 4: CINIC	CINIC	CINIC-10
Figure 5: CINIC	CINIC	CINIC-10
Table 1: CINIC	CINIC	CINIC-10
Figure 6: CINIC	CINIC	CINIC-10
Figure 7: CINIC	CINIC	CINIC-10
Figure 8: CINIC	CINIC	CINIC-10
Figure 9: CINIC	CINIC	CINIC-10
Figure 10: CINIC	CINIC	CINIC-10
Figure 11: CINIC	CINIC	CINIC-10
We presented CINIC	CINIC	CINIC-10
Antoniou, and Amos J. Storkey. CINIC	CINIC	CINIC-10
CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-10	CINIC-10
CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-10	CINIC-10
technical report we introduce the CINIC-10 dataset as a plug-in extended	CINIC-10	CINIC-10
existing benchmarking datasets, we present CINIC-10	CINIC-10	CINIC-10
: CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-10	CINIC-10
addition of downsampled ImageNet images. CINIC-10 has the following desirable properties	CINIC-10	CINIC-10
as in CIFAR, meaning that CINIC-10 can be used as a	CINIC-10	CINIC-10
CINIC-10 consists of images from both	CINIC-10	CINIC-10
our method of constructing the CINIC-10 dataset. This process is detailed	CINIC-10	CINIC-10
among all three sets. The CINIC-10 test set contains the entirety	CINIC-10	CINIC-10
from the CIFAR-10 train set. CINIC-10	CINIC-10	CINIC-10
can be fully recovered from CINIC-10 by the filename	CINIC-10	CINIC-10
The mapping from sysnsets to CINIC-10 is listed in imagenet- contributors.csv	CINIC-10	CINIC-10
from each synset-group to compile CINIC-10 by augmenting the CIFAR-10 data	CINIC-10	CINIC-10
The simplest way to use CINIC-10 is with a PyTorch5 data	CINIC-10	CINIC-10
examples for each class in CINIC-10 (Section 4.2	CINIC-10	CINIC-10
show randomly selected samples from CINIC-10	CINIC-10	CINIC-10
Figure 2: CINIC-10, automobile	CINIC-10	CINIC-10
Figure 3: CINIC-10, airplane	CINIC-10	CINIC-10
1 gives benchmark results on CINIC-10	CINIC-10	CINIC-10
Figure 4: CINIC-10, bird	CINIC-10	CINIC-10
Figure 5: CINIC-10, cat	CINIC-10	CINIC-10
Table 1: CINIC-10 benchmarks	CINIC-10	CINIC-10
Figure 6: CINIC-10, deer	CINIC-10	CINIC-10
Figure 7: CINIC-10, dog	CINIC-10	CINIC-10
Figure 8: CINIC-10, frog	CINIC-10	CINIC-10
Figure 9: CINIC-10, horse	CINIC-10	CINIC-10
Figure 10: CINIC-10, ship	CINIC-10	CINIC-10
Figure 11: CINIC-10, truck	CINIC-10	CINIC-10
We presented CINIC-10 in this technical report. It	CINIC-10	CINIC-10
Antoniou, and Amos J. Storkey. CINIC-10 Github repository	CINIC-10	CINIC-10
CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-	CINIC-10
CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-	CINIC-10
technical report we introduce the CINIC-10 dataset as a plug-in extended	CINIC-	CINIC-10
existing benchmarking datasets, we present CINIC-10	CINIC-	CINIC-10
: CINIC-10 Is Not ImageNet or CIFAR-10	CINIC-	CINIC-10
addition of downsampled ImageNet images. CINIC-10 has the following desirable properties	CINIC-	CINIC-10
as in CIFAR, meaning that CINIC-10 can be used as a	CINIC-	CINIC-10
CINIC-10 consists of images from both	CINIC-	CINIC-10
our method of constructing the CINIC-10 dataset. This process is detailed	CINIC-	CINIC-10
among all three sets. The CINIC-10 test set contains the entirety	CINIC-	CINIC-10
from the CIFAR-10 train set. CINIC-10	CINIC-	CINIC-10
can be fully recovered from CINIC-10 by the filename	CINIC-	CINIC-10
The mapping from sysnsets to CINIC-10 is listed in imagenet- contributors.csv	CINIC-	CINIC-10
from each synset-group to compile CINIC-10 by augmenting the CIFAR-10 data	CINIC-	CINIC-10
The simplest way to use CINIC-10 is with a PyTorch5 data	CINIC-	CINIC-10
examples for each class in CINIC-10 (Section 4.2	CINIC-	CINIC-10
show randomly selected samples from CINIC-10	CINIC-	CINIC-10
Figure 2: CINIC-10, automobile	CINIC-	CINIC-10
Figure 3: CINIC-10, airplane	CINIC-	CINIC-10
1 gives benchmark results on CINIC-10	CINIC-	CINIC-10
Figure 4: CINIC-10, bird	CINIC-	CINIC-10
Figure 5: CINIC-10, cat	CINIC-	CINIC-10
Table 1: CINIC-10 benchmarks	CINIC-	CINIC-10
Figure 6: CINIC-10, deer	CINIC-	CINIC-10
Figure 7: CINIC-10, dog	CINIC-	CINIC-10
Figure 8: CINIC-10, frog	CINIC-	CINIC-10
Figure 9: CINIC-10, horse	CINIC-	CINIC-10
Figure 10: CINIC-10, ship	CINIC-	CINIC-10
Figure 11: CINIC-10, truck	CINIC-	CINIC-10
We presented CINIC-10 in this technical report. It	CINIC-	CINIC-10
Antoniou, and Amos J. Storkey. CINIC-10 Github repository	CINIC-	CINIC-10
images by multiple aesthetic evaluators. AVA	AVA	AVA
Methods AVA	AVA	AVA
aesthetic evaluators (see Table 1,2). AVA Dataset [31]: The Aesthetic Visual	AVA	AVA
Analysis (AVA) dataset is by far the	AVA	AVA
semantic tags provided in the AVA data for analysis4, covering a	AVA	AVA
the standard test partition of AVA (denoted as Val100) for evaluation	AVA	AVA
to nine classes in the AVA dataset, i.e.,, Landscape, Seascape, Cityscape	AVA	AVA
are either elaboratively trained, i.e.,, AVA	AVA	AVA
Methods AVA	AVA	AVA
Marchesotti, and Florent Perronnin. 2012. AVA	AVA	AVA
1,2). AVA Dataset [31]: The Aesthetic Visual Analysis (AVA) dataset is by far	A\S+ V\S+ A\S+	AVA
we take models trained on COCO 	COCO 	COCO 2015
COCO 78	COCO 	COCO 2015
COCO 79	COCO 	COCO 2015
COCO 81	COCO 	COCO 2015
trainval. ”07+12+COCO”: first train on COCO 	COCO 	COCO 2015
COCO 75	COCO 	COCO 2015
COCO 77	COCO 	COCO 2015
COCO 80	COCO 	COCO 2015
trainval. ”07++12+COCO”: first train on COCO 	COCO 	COCO 2015
3.4 COCO  To further validate the SSD	COCO 	COCO 2015
SSD512 architec- tures on the COCO 	COCO 	COCO 2015
dataset. Since objects in COCO 	COCO 	COCO 2015
Table 5: COCO 	COCO 	COCO 2015
show some detection examples on COCO 	COCO 	COCO 2015
network architecture we used for COCO 	COCO 	COCO 2015
Fig. 5: Detection examples on COCO 	COCO 	COCO 2015
Method VOC2007 test VOC2012 test COCO 	COCO 	COCO 2015
COCO 07	COCO 	COCO 2015
COCO 	COCO 	COCO 2015
region proposal networks. In: NIPS. (2015	2015	COCO 2015
R.: Fast R-CNN. In: ICCV. (2015) 7. Erhan, D., Szegedy, C	2015	COCO 2015
arXiv preprint arXiv:1412.1441 v3 (2015) 9. He, K., Zhang, X	2015	COCO 2015
In: CVPR. (2015) 11. Hariharan, B., Arbeláez, P	2015	COCO 2015
and fine-grained localization. In: CVPR. (2015) 12. Liu, W., Rabinovich, A	2015	COCO 2015
scene cnns. In: ICLR. (2015) 14. Howard, A.G.: Some improvements	2015	COCO 2015
nition. In: NIPS. (2015) 16. Russakovsky, O., Deng, J	2015	COCO 2015
scale visual recognition challenge. IJCV (2015	2015	COCO 2015
fully connected crfs. In: ICLR. (2015	2015	COCO 2015
results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that	COCO	COCO 2015
leading results on PASCAL VOC, COCO, and ILSVRC detection all based	COCO	COCO 2015
size evaluated on PASCAL VOC, COCO, and ILSVRC and are compared	COCO	COCO 2015
we take models trained on COCO trainval35k as described in Sec	COCO	COCO 2015
COCO 78.8 84.3 82.0 77.7 68.9	COCO	COCO 2015
COCO 79.6 80.9 86.3 79.0 76.2	COCO	COCO 2015
COCO 81.6 86.6 88.3 82.4 76.0	COCO	COCO 2015
trainval. ”07+12+COCO”: first train on COCO trainval35k then fine-tune on 07+12	COCO	COCO 2015
fine-tuned from models trained on COCO, our SSD512 achieves 80.0% mAP	COCO	COCO 2015
COCO 75.9 87.4 83.6 76.8 62.9	COCO	COCO 2015
COCO 77.5 90.2 83.3 76.3 63.0	COCO	COCO 2015
COCO 80.0 90.7 86.8 80.5 67.8	COCO	COCO 2015
trainval. ”07++12+COCO”: first train on COCO trainval35k then fine-tune on 07++12	COCO	COCO 2015
3.4 COCO	COCO	COCO 2015
SSD512 architec- tures on the COCO dataset. Since objects in COCO	COCO	COCO 2015
Table 5: COCO test-dev2015 detection results	COCO	COCO 2015
show some detection examples on COCO test-dev with the SSD512 model	COCO	COCO 2015
network architecture we used for COCO to the ILSVRC DET dataset	COCO	COCO 2015
Fig. 5: Detection examples on COCO test-dev with SSD512 model. We	COCO	COCO 2015
Method VOC2007 test VOC2012 test COCO test-dev2015	COCO	COCO 2015
COCO 07++12 07++12+COCO trainval35k 0.5 0.5	COCO	COCO 2015
accuracy on PASCAL VOC and COCO, while being 3× faster. Our	COCO	COCO 2015
25. COCO	COCO	COCO 2015
i.e. Extended Cohn-Kanade (CKP) and MMI Facial Expression Databse are used	MMI	MMI
accuracy of 99.2%. For the MMI dataset, currently the best accuracy	MMI	MMI
for CKP and 98.63% for MMI, therefore performing better than the	MMI	MMI
1: Example images from the MMI (top) and CKP (bottom). The	MMI	MMI
Section 4.2) and on the MMI Dataset (Section 4.1). Typical pictures	MMI	MMI
4 DATASETS 4.1 MMI Dataset	MMI	MMI
The MMI dataset has been introduced by	MMI	MMI
shows the differences within the MMI dataset. The six used emotions	MMI	MMI
4.3 Comparison In the MMI Dataset (Fig. 2) the emotion	MMI	MMI
data is taken from the MMI set	MMI	MMI
MMI: The MMI Database contains videos of peo	MMI	MMI
only ones to evaluate the MMI database on Emotions instead of	MMI	MMI
in emotion recognition on the MMI database (Section 4.1	MMI	MMI
experienced when dealing with the MMI Dataset. Since the first two	MMI	MMI
10-fold cross- validation on the MMI Dataset. The lowest accuracy is	MMI	MMI
dataset and 98.36% on the MMI dataset. This shows that the	MMI	MMI
and Dr. Valstar for the MMI data-base	MMI	MMI
4.1 MMI Dataset	MMI	MMI
Allen Institute for Artificial Intelligence (AI2) for a re- cent Kaggle	AI2	AI2 Kaggle Dataset
Table 2: Performance on the AI2 Kaggle questions, measured by precision-at-one	AI2	AI2 Kaggle Dataset
al. (2016) also tackle the AI2 Kag- gle question set with	AI2	AI2 Kaggle Dataset
scores used in the winning Kaggle system from user Cardal	Kaggle	AI2 Kaggle Dataset
AI2) for a re- cent Kaggle challenge. The training set contained	Kaggle	AI2 Kaggle Dataset
the top-performing systems of the Kaggle chal- lenge. These consisted of	Kaggle	AI2 Kaggle Dataset
2: Performance on the AI2 Kaggle questions, measured by precision-at-one (P@1	Kaggle	AI2 Kaggle Dataset
systems that competed in the Kaggle challenge, our system comes in	Kaggle	AI2 Kaggle Dataset
Allen Institute for Artificial Intelligence (AI2) for a re- cent Kaggle	AI2	AI2 Kaggle Dataset
Table 2: Performance on the AI2 Kaggle questions, measured by precision-at-one	AI2	AI2 Kaggle Dataset
al. (2016) also tackle the AI2 Kag- gle question set with	AI2	AI2 Kaggle Dataset
scores used in the winning Kaggle system from user Cardal	Kaggle	AI2 Kaggle Dataset
AI2) for a re- cent Kaggle challenge. The training set contained	Kaggle	AI2 Kaggle Dataset
the top-performing systems of the Kaggle chal- lenge. These consisted of	Kaggle	AI2 Kaggle Dataset
2: Performance on the AI2 Kaggle questions, measured by precision-at-one (P@1	Kaggle	AI2 Kaggle Dataset
systems that competed in the Kaggle challenge, our system comes in	Kaggle	AI2 Kaggle Dataset
Allen Institute for Artificial Intelligence (AI2) for a re- cent Kaggle	AI2	AI2 Kaggle Dataset
Table 2: Performance on the AI2 Kaggle questions, measured by precision-at-one	AI2	AI2 Kaggle Dataset
al. (2016) also tackle the AI2 Kag- gle question set with	AI2	AI2 Kaggle Dataset
scores used in the winning Kaggle system from user Cardal	Kaggle	AI2 Kaggle Dataset
AI2) for a re- cent Kaggle challenge. The training set contained	Kaggle	AI2 Kaggle Dataset
the top-performing systems of the Kaggle chal- lenge. These consisted of	Kaggle	AI2 Kaggle Dataset
2: Performance on the AI2 Kaggle questions, measured by precision-at-one (P@1	Kaggle	AI2 Kaggle Dataset
systems that competed in the Kaggle challenge, our system comes in	Kaggle	AI2 Kaggle Dataset
Allen Institute for Artificial Intelligence (AI2) for a re- cent Kaggle	AI2	AI2 Kaggle Dataset
Table 2: Performance on the AI2 Kaggle questions, measured by precision-at-one	AI2	AI2 Kaggle Dataset
al. (2016) also tackle the AI2 Kag- gle question set with	AI2	AI2 Kaggle Dataset
scores used in the winning Kaggle system from user Cardal	Kaggle	AI2 Kaggle Dataset
AI2) for a re- cent Kaggle challenge. The training set contained	Kaggle	AI2 Kaggle Dataset
the top-performing systems of the Kaggle chal- lenge. These consisted of	Kaggle	AI2 Kaggle Dataset
2: Performance on the AI2 Kaggle questions, measured by precision-at-one (P@1	Kaggle	AI2 Kaggle Dataset
systems that competed in the Kaggle challenge, our system comes in	Kaggle	AI2 Kaggle Dataset
proof as- sistants. We construct CoqGym, a large-scale dataset and learning	CoqGym	CoqGym
that AS- Tactic trained on CoqGym can generate effective tactics and	CoqGym	CoqGym
CoqGym	CoqGym	CoqGym
CoqGym https://github.com/princeton-vl/CoqGym	CoqGym	CoqGym
CoqGym	CoqGym	CoqGym
learning envi- ronment We construct CoqGym, a dataset and learning environment	CoqGym	CoqGym
The learning environment of CoqGym is designed for train- ing	CoqGym	CoqGym
amenable to learning, we augment CoqGym with shorter proofs. They are	CoqGym	CoqGym
Experimental results on CoqGym show that ASTactic can generate	CoqGym	CoqGym
two- fold. First, we build CoqGym	CoqGym	CoqGym
oped a similar tool in CoqGym, but we aim for a	CoqGym	CoqGym
theorem in discrete geometry, whereas CoqGym covers more diverse domains including	CoqGym	CoqGym
4. Constructing CoqGym CoqGym includes a large-scale dataset of	CoqGym	CoqGym
specific domains. The projects in CoqGym include the Coq standard library	CoqGym	CoqGym
work (Huang et al., 2019), CoqGym supplies the complete environment for	CoqGym	CoqGym
exposing its internals. In constructing CoqGym, we modify Coq and use	CoqGym	CoqGym
Dataset statistics CoqGym has 70,856 human-written proofs from	CoqGym	CoqGym
training and testing. Statistics of CoqGym show that many valid tactics	CoqGym	CoqGym
its number of occurrences in CoqGym and manually include the common	CoqGym	CoqGym
the proof steps extracted from CoqGym	CoqGym	CoqGym
the 13,137 testing theorems in CoqGym	CoqGym	CoqGym
in Coq. We have constructed CoqGym	CoqGym	CoqGym
of AST. Experimental results on CoqGym confirm the effectiveness of our	CoqGym	CoqGym
We investigate this approach on COMPLEXQUESTIONS, a dataset designed to focus	COMPLEXQUESTIONS	COMPLEXQUESTIONS
We test this model on COMPLEXQUESTIONS (Bao et al., 2016), a	COMPLEXQUESTIONS	COMPLEXQUESTIONS
obtain rea- sonable performance on COMPLEXQUESTIONS, and analyze the types of	COMPLEXQUESTIONS	COMPLEXQUESTIONS
We convert COMPLEXQUESTIONS into the aforementioned format, and	COMPLEXQUESTIONS	COMPLEXQUESTIONS
Table 1 illustrates that COMPLEXQUESTIONS is dominated by N-ARY questions	COMPLEXQUESTIONS	COMPLEXQUESTIONS
ally return a set, in COMPLEXQUESTIONS 87% of the answers are	COMPLEXQUESTIONS	COMPLEXQUESTIONS
most common non-stop words in COMPLEXQUESTIONS	COMPLEXQUESTIONS	COMPLEXQUESTIONS
COMPLEXQUESTIONS contains 1,300 training examples and	COMPLEXQUESTIONS	COMPLEXQUESTIONS
semantic parsing models on both COMPLEXQUESTIONS and WEBQUES- TIONS. For these	COMPLEXQUESTIONS	COMPLEXQUESTIONS
when it was trained on COMPLEXQUESTIONS, WE- BQUESTIONS, and SIMPLEQUESTIONS. In	COMPLEXQUESTIONS	COMPLEXQUESTIONS
fraction of the questions in COMPLEXQUESTIONS are N-ARY). Interestingly, WEBQA performs	COMPLEXQUESTIONS	COMPLEXQUESTIONS
such a QA system on COMPLEXQUESTIONS and find that it obtains	COMPLEXQUESTIONS	COMPLEXQUESTIONS
brain tissue segmentation challenges, iSEG 2017 and MRBrainS 2013, with the	2017	iSEG 2017 Challenge
brain tissue segmentation challenges, iSEG 2017 and MRBrainS 2013. HyperDenseNet yielded	2017	iSEG 2017 Challenge
1. iSEG 2017 focuses on 6-month infant data	2017	iSEG 2017 Challenge
2D FCNN Chen et al., 2017 [19] T1,T1-IR,FLAIR Brain tissue 3D	2017	iSEG 2017 Challenge
FCNN Dolz et al., 2017 [17] T1,T2 Infant brain tissue	2017	iSEG 2017 Challenge
3D FCNN Fidon et al., 2017 [6] T1,T1c,T2,FLAIR Brain tumor CNN	2017	iSEG 2017 Challenge
Kamnitsas et al., 2017 [5] T1,T1c,T2,FLAIR	2017	iSEG 2017 Challenge
Kamnitsas et al., 2017 [22] MPRAGE,FLAIR,T2,PD Traumatic brain injuries	2017	iSEG 2017 Challenge
FCNN(Adversarial Training) Valverde et al., 2017 [23] T1, T2,FLAIR Multiple-sclerosis 3D	2017	iSEG 2017 Challenge
out in conjunction with MICCAI 2017, with a total of 21	2017	iSEG 2017 Challenge
2017 organizers used three metrics to	2017	iSEG 2017 Challenge
splitting the 10 available iSEG- 2017 volumes into training, validation and	2017	iSEG 2017 Challenge
2017 challenge data. The first point	2017	iSEG 2017 Challenge
2017 challenge data. The first point	2017	iSEG 2017 Challenge
2017 data for HyperDenseNet and the	2017	iSEG 2017 Challenge
2017 Challenge website for first (http://iseg2017.web.unc.edu/rules/results	2017	iSEG 2017 Challenge
2017 for 6-month in- fant brain	2017	iSEG 2017 Challenge
analysis, vol. 36, pp. 61–78, 2017	2017	iSEG 2017 Challenge
national Conference on MICCAI. Springer, 2017, pp. 285–293	2017	iSEG 2017 Challenge
MRI segmentation,” arXiv preprint arXiv:1712.05319, 2017	2017	iSEG 2017 Challenge
from 3D MR images,” NeuroImage, 2017	2017	iSEG 2017 Challenge
International Conference on IPMI. Springer, 2017, pp. 597–609	2017	iSEG 2017 Challenge
NeuroImage, vol. 155, pp. 159–168, 2017	2017	iSEG 2017 Challenge
neonatal brain MRI segmentation,” NeuroImage, 2017	2017	iSEG 2017 Challenge
MRI: A large-scale study,” NeuroImage, 2017	2017	iSEG 2017 Challenge
and random walk,” Medical Physics, 2017	2017	iSEG 2017 Challenge
Pattern Analysis and Machine Intelligence, 2017	2017	iSEG 2017 Challenge
connections on learning.” in AAAI, 2017, pp. 4278–4284	2017	iSEG 2017 Challenge
Proceedings of the IEEE CVPR, 2017	2017	iSEG 2017 Challenge
segmentation from CT volumes,” arXiv:1709.07330, 2017	2017	iSEG 2017 Challenge
Confer- ence on MICCAI. Springer, 2017, pp. 287–295	2017	iSEG 2017 Challenge
Computer Vision and Pattern Recognition, 2017, pp. 4373–4382	2017	iSEG 2017 Challenge
in Multimedia and Expo (ICME), 2017 IEEE International Conference on. IEEE	2017	iSEG 2017 Challenge
, 2017, pp. 355–360	2017	iSEG 2017 Challenge
mobile devices,” arXiv preprint arXiv:1707.01083, 2017	2017	iSEG 2017 Challenge
2017 challenge,” IEEE Transactions on Medical	2017	iSEG 2017 Challenge
medical images,” arXiv preprint arXiv:1711.06853, 2017	2017	iSEG 2017 Challenge
multi-modal brain tissue segmentation challenges, iSEG 2017 and MRBrainS 2013, with	iSEG	iSEG 2017 Challenge
multi-modal brain tissue segmentation challenges, iSEG 2017 and MRBrainS 2013. HyperDenseNet	iSEG	iSEG 2017 Challenge
1. iSEG 2017 focuses on 6-month infant	iSEG	iSEG 2017 Challenge
challenges: infant brain tissue segmentation, iSEG [49], and adult brain tissue	iSEG	iSEG 2017 Challenge
results, com- piled by the iSEG challenge organizers on testing data	iSEG	iSEG 2017 Challenge
3.1 iSEG Challenge The focus of this	iSEG	iSEG 2017 Challenge
3.1.1 Evaluation The iSEG	iSEG	iSEG 2017 Challenge
by splitting the 10 available iSEG	iSEG	iSEG 2017 Challenge
the first round of the iSEG Challenge, as well as to	iSEG	iSEG 2017 Challenge
TABLE 5 Results on the iSEG	iSEG	iSEG 2017 Challenge
an updated ranking, see the iSEG	iSEG	iSEG 2017 Challenge
SKKU teams participated in both iSEG and MRBrains2013 challenges. While these	iSEG	iSEG 2017 Challenge
out- performed HyperDenseNet in the iSEG challenge, the performance of our	iSEG	iSEG 2017 Challenge
the two modalities used in iSEG, these results suggest that Hyper	iSEG	iSEG 2017 Challenge
with two modalities, for both iSEG and MRBrainS chal- lenges. As	iSEG	iSEG 2017 Challenge
for Hyper- DenseNet trained on iSEG (top row of Fig 8	iSEG	iSEG 2017 Challenge
of two highly competitive challenges, iSEG	iSEG	iSEG 2017 Challenge
would like to thank both iSEG and MRBrainS organizers for providing	iSEG	iSEG 2017 Challenge
in HyperDenseNet trained on the iSEG (top) and MRBrainS (from 2nd	iSEG	iSEG 2017 Challenge
iSEG Images were acquired at the	iSEG	iSEG 2017 Challenge
3.1 iSEG Challenge	iSEG	iSEG 2017 Challenge
3.1 iSEG Challenge The focus of this challenge	Challenge	iSEG 2017 Challenge
Challenge results: Table 5 compares the	Challenge	iSEG 2017 Challenge
first round of the iSEG Challenge, as well as to all	Challenge	iSEG 2017 Challenge
3.2 MRBrainS Challenge The MRBrainS challenge was initially	Challenge	iSEG 2017 Challenge
updated ranking, see the iSEG-2017 Challenge website for first (http://iseg2017.web.unc.edu/rules/results/) and	Challenge	iSEG 2017 Challenge
Challenge results: The MRBrainS challenge organiz	Challenge	iSEG 2017 Challenge
updated ranking, see the MRBrainS Challenge website (http://mrbrains13.isi.uu.nl/results.php	Challenge	iSEG 2017 Challenge
machine learning approach,” MICCAI Grand Challenge	Challenge	iSEG 2017 Challenge
ceedings of the MICCAI Grand Challenge	Challenge	iSEG 2017 Challenge
3.1 iSEG Challenge	Challenge	iSEG 2017 Challenge
3.2 MRBrainS Challenge	Challenge	iSEG 2017 Challenge
MRI), employing the publicly available CHAOS dataset. Results show that the	CHAOS	CHAOS MRI Dataset
bined Healthy Abdominal Organ Segmentation (CHAOS) Challenge 2 [52], [53], [54	CHAOS	CHAOS MRI Dataset
other state-of-the-art architectures on the CHAOS dataset. The values show the	CHAOS	CHAOS MRI Dataset
Combined Healthy Abdominal Organ Segmentation (CHAOS) Challenge. We provided extensive ex	CHAOS	CHAOS MRI Dataset
on several subjects on the CHAOS Challenge dataset. The proposed multi-scale	CHAOS	CHAOS MRI Dataset
1) Dataset	Dataset	CHAOS MRI Dataset
IV-A1 Dataset	Dataset	CHAOS MRI Dataset
segmentation on magnetic resonance imaging (MRI	MRI	CHAOS MRI Dataset
segmentation on magnetic resonance imaging (MRI), employing the publicly available CHAOS	MRI	CHAOS MRI Dataset
1) Dataset: The abdominal MRI dataset from the Com- bined	MRI	CHAOS MRI Dataset
segmentation of abdominal organs on MRI (T1-DUAL in phase). This dataset	MRI	CHAOS MRI Dataset
acquired by a 1.5T Philips MRI, producing 12 bit DICOM images	MRI	CHAOS MRI Dataset
Chaos dataset (multi-organ segmentation on MRI task). The values show the	MRI	CHAOS MRI Dataset
approach we conducted experiments on MRI scans (T1-DUAL) from the Combined	MRI	CHAOS MRI Dataset
Deep learning techniques for automatic MRI cardiac multi-structures segmentation and diagnosis	MRI	CHAOS MRI Dataset
of subcortical brain structures on MRI for radiotherapy and radiosurgery: a	MRI	CHAOS MRI Dataset
networks for subcortical segmentation in MRI	MRI	CHAOS MRI Dataset
of bladder cancer structures in MRI with progressive dilated convolutional networks	MRI	CHAOS MRI Dataset
hybrid transportation modes from sparse GPS data using a moving window	GPS	GPS
hybrid transportation modes from sparse GPS data using a moving window	GPS	GPS
the data collected by using GPS devices so that the cost	GPS	GPS
solve a classification problem of GPS data into different transportation modes	GPS	GPS
framework was tested using coarse-grained GPS data, which has been avoided	GPS	GPS
data. Among these practices are GPS	GPS	GPS
surveys, where participants carry a GPS device for a certain duration	GPS	GPS
infer the transportation mode from GPS data. This inference could largely	GPS	GPS
and acceleration and length between GPS fixes. However, none of the	GPS	GPS
low each other in a GPS sequence and that every two	GPS	GPS
GPS consec- utive fixes are analysed	GPS	GPS
solve a classification problem of GPS data into different transporta- tion	GPS	GPS
the data-related issues by collecting GPS data for a rec- ommended	GPS	GPS
framework. We also describe the GPS data collected for this re	GPS	GPS
to be used to classify GPS points into transportation modes (Mitchell	GPS	GPS
modes from the collected sparse GPS data, without information or assumptions	GPS	GPS
and acceleration values calculated from GPS data. Due to its high	GPS	GPS
research. Lastly, we describe the GPS data collected for this research	GPS	GPS
In order to de-construct a GPS track, some definitions have been	GPS	GPS
route between any two consecutive GPS points is called a segment	GPS	GPS
fer the transportation mode from GPS data collected by travel sur	GPS	GPS
Stenneth et al., 2011); strictly GPS devices alone (Chung & Shalaby	GPS	GPS
the temporal granularity of the GPS data (also called the epoch	GPS	GPS
on battery restrictions on current GPS devices or smart phones, but	GPS	GPS
these data-related issues by collecting GPS data for this study for	GPS	GPS
corresponding sample size procedures for GPS	GPS	GPS
are calculated from the collected GPS data	GPS	GPS
the least num- ber of GPS fixes since nearly half of	GPS	GPS
which causes the loss of GPS coverage. In this case, a	GPS	GPS
this dataset. Outliers due to GPS errors are accounted for and	GPS	GPS
Sampling for this kind of GPS	GPS	GPS
framework used to classify the GPS segments into transportation modes. The	GPS	GPS
infer the transportation mode from GPS data has extended from logical	GPS	GPS
the process by segmenting the GPS track into trips, based on	GPS	GPS
dependant on segmentation classify each GPS segment individually into a transportation	GPS	GPS
based on SVMs to classify GPS segments into respective transportation modes	GPS	GPS
error in training (due to GPS errors), and since the data	GPS	GPS
e.g. the sequence of a GPS trajectory’s move- ments: such as	GPS	GPS
The loss of GPS coverage due to indoor activity	GPS	GPS
to study se- quences of GPS trajectory movements rather than each	GPS	GPS
the learning process to consequent GPS data that represent the variabil	GPS	GPS
there will not be any GPS fixes attainable at several areas	GPS	GPS
inferring transportation mode from sparse GPS data. We first provide the	GPS	GPS
the transportation mode from sparse GPS data without any extra information	GPS	GPS
work. Finally, different rate of GPS data collection could be compared	GPS	GPS
M. (2009). National travel survey GPS feasibility study: Final report. Department	GPS	GPS
A., & Cheng, T. (2010). GPS data collection setting for pedestrian	GPS	GPS
for studying transportation modes from GPS data. In Proceedings of Transport	GPS	GPS
A trip reconstruction tool for GPS	GPS	GPS
Extracting places and activities from GPS traces	GPS	GPS
Axhausen, K. W. (2009). Processing GPS raw data without additional information	GPS	GPS
Deducing mode and purpose from GPS data. In The 87th annual	GPS	GPS
Xu, Y. (2010). Effective GPS	GPS	GPS
Understanding transportation modes based on GPS data for Web applications. ACM	GPS	GPS
Learning transportation mode from raw GPS data for geographic applications on	GPS	GPS
hybrid transportation modes from sparse GPS data using a moving window	GPS	GPS
Product-Aware Answer Generation in E-Commerce Question-Answering	Answer	JD Product Question Answer
Product-Aware Answer Generation in E-CommerceQuestion-Answering	Answer	JD Product Question Answer
and Rui Yan. 2019. Product-Aware Answer Generation in E-Commerce Question- Answering	Answer	JD Product Question Answer
Answer generator. (1) Review reader: (See	Answer	JD Product Question Answer
Answer Decoder	Answer	JD Product Question Answer
Generated Answer	Answer	JD Product Question Answer
Generated Answer	Answer	JD Product Question Answer
Answer Encoder	Answer	JD Product Question Answer
Ground Truth Answer	Answer	JD Product Question Answer
Ground Truth Answer	Answer	JD Product Question Answer
ing of Candidate Extraction and Answer Selection for Reading Comprehension. In	Answer	JD Product Question Answer
Ming Zhou. 2018. S-Net: From Answer Extraction to Answer Synthesis for	Answer	JD Product Question Answer
and Wai Lam. 2018. Aware Answer Prediction for Product-Related Questions Incorporating	Answer	JD Product Question Answer
Product-Aware Answer Generation in E-Commerce Question	Question	JD Product Question Answer
CCS CONCEPTS • Information systems→ Question answering	Question	JD Product Question Answer
KEYWORDS Question answering, e-commerce, product-aware answer genera	Question	JD Product Question Answer
Product-Aware Answer Generation in E-Commerce Question	Question	JD Product Question Answer
Question hidden states	Question	JD Product Question Answer
Question aware attention Question aware attention	Question	JD Product Question Answer
Question Encoder	Question	JD Product Question Answer
Question Aware Review Representation	Question	JD Product Question Answer
maps, shown in Figure 2. Question of the left figure in	Question	JD Product Question Answer
2017. An Abstractive approach to Question Answering. arXiv preprint arXiv:1711.06238 (2017	Question	JD Product Question Answer
Ester. 2011. AQA: Aspect-based Opinion Question Answering. (2011), 89–96	Question	JD Product Question Answer
Summarizing Answers in Non-Factoid Community Question	Question	JD Product Question Answer
for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce. In	Question	JD Product Question Answer
Zhaochun Ren JD	JD	JD Product Question Answer
Yihong Zhao JD	JD	JD Product Question Answer
Dawei Yin JD	JD	JD Product Question Answer
performed during an internship at JD	JD	JD Product Question Answer
Product	Product	JD Product Question Answer
Product	Product	JD Product Question Answer
Yin, and Rui Yan. 2019. Product	Product	JD Product Question Answer
ing comprehension, and sequence-to-sequence architecture. Product	Product	JD Product Question Answer
2016. Addressing Complex and Subjective Product	Product	JD Product Question Answer
2018. Aware Answer Prediction for Product	Product	JD Product Question Answer
New Orleans, Louisiana, June 5, 2018	2018	SLAM 2018
2018 Association for Computational Linguistics	2018	SLAM 2018
SLAM 2018 focuses on predicting a student’s	2018	SLAM 2018
The SLAM 2018 Shared Task is primarily cen	2018	SLAM 2018
acquisi- tion (Settles et al., 2018) of non-native learners of English	2018	SLAM 2018
M. Hagiwara, and N. Madnani. 2018	2018	SLAM 2018
SLAM 2018 focuses on predicting a	SLAM	SLAM 2018
The SLAM 2018 Shared Task is primarily	SLAM	SLAM 2018
SLAM 2018 focuses on predicting a student’s	SLAM 2018	SLAM 2018
The SLAM 2018 Shared Task is primarily cen	SLAM 2018	SLAM 2018
SLAM 2018 focuses on predicting a student’s	SLAM 	SLAM 2018
The SLAM 2018 Shared Task is primarily cen	SLAM 	SLAM 2018
Street Image Crawler A city street	Street	Google Street Images
was built using the Google Street View API V3.0 along with	Street	Google Street Images
Street Image Crawler	Street	Google Street Images
crawler was built using the Google Street View API V3.0 along	Google	Google Street Images
ShapeNet-Core [5], Shapenet- Part [42], ModelNet40 [40] and 3DMatch benchmark [45	ModelNet	ModelNet40
by transfer learning on the ModelNet40 dataset. Networks are trained out	ModelNet	ModelNet40
ShapeNet-Core [5], Shapenet- Part [42], ModelNet40 [40] and 3DMatch benchmark [45	ModelNet40	ModelNet40
by transfer learning on the ModelNet40 dataset. Networks are trained out	ModelNet40	ModelNet40
