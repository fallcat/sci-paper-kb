We O
use O
Ko- O
dak24 O
(http://r0k.us/graphics/kodak/), O
BSD68 B-DAT
[53], O
and O
Urban100 O
[11] O
for O

and O
FFDNet O
[20]. O
Kodak24 O
(http://r0k.us/graphics/kodak/), O
BSD68 B-DAT
[53], O
and O
Urban100 O
[11] O
are O

Method O
Kodak24 O
BSD68 B-DAT
Urban10010 O
30 O
50 O
70 O
10 O

Method O
Kodak24 O
BSD68 B-DAT
Urban10010 O
30 O
50 O
70 O
10 O

BSD68 B-DAT

BSD68 B-DAT

images O
from O
Berkeley O
segmentation O
dataset O
(BSD68) B-DAT
[12] O
and O
the O
other O
one O

use O
color O
version O
of O
the O
BSD68 B-DAT
dataset O
for O
testing O
and O
the O

OF O
DIFFERENT O
METHODS O
ON O
THE O
BSD68 B-DAT
DATASET. O
THE O
BEST O
RESULTS O
ARE O

of O
different O
methods O
on O
the O
BSD68 B-DAT
dataset O
are O
shown O
in O
Table O

are O
evaluated O
on O
the O
gray/color O
BSD68 B-DAT
dataset O

results O
of O
one O
image O
from O
BSD68 B-DAT
with O
noise O
level O
50 O

and O
TNRD O
for O
comparison. O
The O
BSD68 B-DAT
dataset O

15, O
25 O
AND O
50 O
ON O
BSD68 B-DAT
DATASET, O
SINGLE O
IMAGE O
SUPER-RESOLUTION O
WITH O

BSD68 B-DAT
25 O
28.57 O
/ O
0.8017 O
28.92 O

Method O
Set5 O
Set14 O
BSD100 B-DAT

Method O
Set5 O
Set14 O
BSD100 B-DAT

The O
way O
of O
embedding O
upscaling B-DAT
feature O
in O
the O
last O
few O

BSD100, B-DAT
Urban100 O
and O
Manga109, O
each O
of O

such O
as O
Set5, O
Set14 O
and O
BSD100) B-DAT
with O
rich O

Method O
Set5 O
Set14 O
BSD100 B-DAT
Urban100 O
Manga109 O

Method O
Set5 O
Set14 O
BSD100 B-DAT
Urban100 O
Manga109 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
traction O
module, O
which O
consists O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
is O
optimized O
by O
stochastic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
plified O
residual O
blocks O
with O
local-source O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-1 B-DAT
Fg O

- B-DAT

- B-DAT
NL O

- B-DAT
NL O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-1 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sentations. O
Thus, O
we O
set O
α O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
spectively. O
f(·) O
and O
δ(·) O
are O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ture, O
and O
embed O
RL-NL O
modules O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tains O
the O
convolution O
layers O
with O

- B-DAT
ble O
1 O
we O
can O
see O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

43074 O
from O
BSD100 B-DAT
102061 I-DAT
from O
BSD100 O

43074 O
from O
BSD100 B-DAT
69015 I-DAT
from O
BSD100 O

208001 O
from O
BSD100 B-DAT
baboon I-DAT
from O
Set14 O

GT175032 O
from O
BSD100 B-DAT
Comparison I-DAT
of O

163085 O
from O
BSD100 B-DAT
(b) I-DAT
detail O
influence O

Set5 O
Set14 O
BSD100 B-DAT

78004 O
from O
BSD100 B-DAT
Fig. I-DAT
5: O
The O
influence O
of O

RCAN O
EnhanceNet O
SRGAN126007 O
from O
BSD100 B-DAT

RCAN O
EnhanceNet O
SRGAN16077 O
from O
BSD100 B-DAT

RCAN O
EnhanceNet O
SRGAN302008 O
from O
BSD100 B-DAT

RCAN O
EnhanceNet O
SRGAN105025 O
from O
BSD100 B-DAT

Set5 O
[42], O
Set14 O
[43], O
BSD100 B-DAT
[44], O
Urban100 O
[45], O
and O
the O

43074 O
from O
BSD100 B-DAT

102061 O
from O
BSD100 B-DAT

43074 O
from O
BSD100 B-DAT

69015 O
from O
BSD100 B-DAT

208001 O
from O
BSD100 B-DAT

GT175032 O
from O
BSD100 B-DAT

163085 O
from O
BSD100 B-DAT

zebra O
from O
Set14 O
175043 O
from O
BSD100 B-DAT

Set5 O
Set14 O
BSD100 B-DAT
Urban100 O
Manga109 O
PSNR/SSIM O
PSNR/SSIM O
PSNR/SSIM O

78004 O
from O
BSD100 B-DAT

RCAN O
EnhanceNet O
SRGAN126007 O
from O
BSD100 B-DAT
ESRGAN(ours O

RCAN O
EnhanceNet O
SRGAN16077 O
from O
BSD100 B-DAT
ESRGAN(ours O

RCAN O
EnhanceNet O
SRGAN302008 O
from O
BSD100 B-DAT
ESRGAN(ours O

RCAN O
EnhanceNet O
SRGAN105025 O
from O
BSD100 B-DAT
ESRGAN(ours O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
GAN) O
[1] O
is O
a O
seminal O

- B-DAT

- B-DAT
lar, O
we O
introduce O
the O
Residual-in-Residual O

- B-DAT
vide O
stronger O
supervision O
for O
brightness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lem, O
has O
attracted O
increasing O
attention O

- B-DAT
panies. O
SISR O
aims O
at O
recovering O

- B-DAT

- B-DAT

- B-DAT
perous O
development. O
Various O
network O
architecture O

- B-DAT

- B-DAT
Noise O
Ratio O
(PSNR) O
value O
[5,6,7,1,8,9,10,11,12 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uation O
of O
human O
observers O
[1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mize O
super-resolution O
model O
in O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
RGAN, O
consistently O
outperforms O
state-of-the-art O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
struction O
style O
and O
smoothness. O
Another O

- B-DAT
pate O
in O
region O
1 O
and O

- B-DAT

- B-DAT

- B-DAT
tures, O
such O
as O
a O
deeper O

- B-DAT
work O
[9], O
deep O
back O
projection O

- B-DAT
provement. O
Zhang O
et O
al. O
[11 O

- B-DAT
ing O
the O
state-of-the-art O
PSNR O
performance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity O
[29,14], O
perceptual O
loss O
[13 O

- B-DAT
imizing O
the O
error O
in O
a O

- B-DAT
pearance. O
Ledig O
et O
al. O
[1 O

- B-DAT
jadi O
et O
al. O
[16] O
develop O

- B-DAT

- B-DAT
veloping O
more O
effective O
GAN O
frameworks O

- B-DAT
tor O
includes O
gradient O
clipping O
[32 O

- B-DAT
ated O
data O
are O
real, O
but O

- B-DAT
sures, O
e.g., O
PSNR O
and O
SSIM O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lenge O
[3]. O
In O
a O
recent O

- B-DAT
tion, O
we O
first O
describe O
our O

- B-DAT
tion O
is O
done O
in O
the O

- B-DAT
ers; O
2) O
replace O
the O
original O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
putational O
complexity O
in O
different O
PSNR-oriented O

- B-DAT
ing O
dataset O
during O
testing. O
When O

- B-DAT
alization O
ability. O
We O
empirically O
observe O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
residual O
network. O
However, O
our O

- B-DAT
erage O
Discriminator O
RaD O
[2], O
denoted O

- B-DAT

- B-DAT
mulated O
as O
DRa(xr, O
xf O

- B-DAT

- B-DAT

- B-DAT
tures O
before O
activation O
rather O
than O

- B-DAT

-543 B-DAT
layer O
is O
merely O
11.17%. O
The O

- B-DAT

- B-DAT

- B-DAT
tance O
between O
recovered O
image O
G(xi O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nition O
[38], O
which O
focuses O
on O

- B-DAT
ing O
perceptual O
loss O
that O
focuses O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tures O
and O
similarly, O
22 O
represents O

- B-DAT

-22 B-DAT
b) O
activation O
map O
of O

-54 B-DAT

- B-DAT
boon’. O
With O
the O
network O
going O

- B-DAT

- B-DAT
ceptual O
quality, O
we O
propose O
a O

- B-DAT
tion. O
Specifically, O
we O
first O
train O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
parameters O
of O
these O
two O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
polated O
image O
is O
either O
too O

- B-DAT
rameter O
λ O
and O
η O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
oriented O
model O
with O
the O
L1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els O
on O
widely O
used O
benchmark O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
the-art O
PSNR-oriented O
methods O
including O
SRCNN O

- B-DAT

- B-DAT
tures, O
e.g., O
animal O
fur, O
building O

- B-DAT
pleasant O
artifacts, O
e.g., O
artifacts O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults, O
and O
than O
previous O
GAN-based O

- B-DAT
tures O
in O
building O
(see O
image O

- B-DAT

- B-DAT
mance O
without O
artifacts. O
It O
does O

- B-DAT
ment O
can O
be O
observed O
from O

- B-DAT
tures O
before O
activation O
can O
result O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gies O
in O
balancing O
the O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
oriented O
method O
outputs O
cartoon-style O
blurry O

- B-DAT

- B-DAT

- B-DAT
pirically O
make O
some O
modifications O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
terpolation O
between O
the O
results O
of O

- B-DAT

- B-DAT

- B-DAT
ceptual O
quality O
than O
previous O
SR O

- B-DAT

- B-DAT
dition, O
useful O
techniques O
including O
residual O

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In: O
CVPR. O
(2018 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
performance O
on O
imagenet O
classification O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
representations. O
In: O
International O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tics. O
(2010 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
duce O
several O
useful O
techniques O
that O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
a O
very O
deep O
network O

- B-DAT

- B-DAT
Residual O
Dense O
Block O
(RRDB), O
which O

- B-DAT
ers O
[47,28]. O
He O
et O
al O

- B-DAT

- B-DAT
tially. O
It O
is O
worth O
noting O

- B-DAT
tion O
(multiplying O
0.1 O
for O
all O

- B-DAT
tremely O
bad O
local O
minimum O
with O

- B-DAT
tion O
(×0.1) O
helps O
the O
network O

- B-DAT
tion O
achieves O
a O
higher O
PSNR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rithms: O
average O
PSNR/SSIM O
on O
Y O

Bicubic O
- B-DAT
28.42/0.8104 O
26.00/0.7027 O
25.96/0.6675 O
23.14/0.6577 O
24.89/0.7866 O

- B-DAT

- B-DAT
verse O
natural O
textures. O
We O
employ O

- B-DAT
over, O
the O
deeper O
model O
achieves O

- B-DAT

- B-DAT

- B-DAT

the O
problem O
of O
estimating O
an O
upscaling B-DAT
function O
u O
: O
X O

progressive O
solution O
to O
learn O
the O
upscaling B-DAT
function O
u. O
In O
the O
following O

where O
ϕ2 O
denotes O
an O
upscaling B-DAT
operator O
by O
a O
factor O
of O

in O
the O
challenge O
target O
4× O
upscaling B-DAT
but O
consider O
unknown O
degradation. O
Given O

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

datasets O
Set5 O
[5], O
Set14 O
[40], O
BSD100 B-DAT
[1], O
Urban100 O
[17], O
and O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
have O
achieved O
impressive O
results O

- B-DAT
ditional O
error O
measures O
and O
perceptual O

- B-DAT
sults O
for O
large O
upsampling O
factors O

- B-DAT

- B-DAT
lows O
to O
scale O
well O
to O

- B-DAT

- B-DAT
taneously. O
In O
particular O
ProSR O
ranks O

- B-DAT
lenge O
[34]. O
Compared O
to O
the O

- B-DAT

- B-DAT
cessing O
has O
recently O
sparked O
increased O

- B-DAT
resolution. O
In O
particular, O
approaches O
to O

- B-DAT

- B-DAT
resolution O
(HR) O
images O
based O
on O

- B-DAT
scaling O
function O
is O
a O
deep O

- B-DAT
lowing O
direct O
approaches. O
The O
first O

- B-DAT
ginning O
and O
then O
essentially O
learns O

- B-DAT

- B-DAT
nation O
of O
upsampling O
layers. O
Thus O

- B-DAT

- B-DAT
istic O
results, O
we O
adopt O
the O

- B-DAT
tain O
a O
multi-scale O
generator O
with O

- B-DAT
riculum O
learning, O
which O
is O
known O

- B-DAT
pling O
factors) O
to O
hard O
(large O

- B-DAT

- B-DAT
egy O
not O
only O
improves O
results O

- B-DAT
lizes O
the O
GAN O
training O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cally O
been O
tackled O
using O
statistical O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cently, O
Dong O
et O
al. O
[6 O

- B-DAT

- B-DAT
ing O
techniques. O
Since O
then, O
deep O

- B-DAT

- B-DAT

- B-DAT
struction O
techniques O
[7, O
20, O
23 O

- B-DAT
age O
to O
the O
desired O
spatial O

- B-DAT
processing O
step. O
Thus, O
the O
CNN O

- B-DAT

- B-DAT
putationally O
expensive O
[30]. O
To O
overcome O

- B-DAT

- B-DAT
scribed O
by O
LapSRN O
by O
Lai O

- B-DAT
sampling O
follows O
the O
principle O
of O

- B-DAT
puted O
at O
each O
scale, O
this O

- B-DAT
pervision. O
Lai O
et O
al. O
improved O

- B-DAT

- B-DAT
erable O
gap O
between O
the O
top-performing O

- B-DAT
tion O
difficulty. O
Furthermore, O
the O
recursive O

- B-DAT
criminator O
along O
with O
a O
progressive O

- B-DAT
niques O
optimize O
the O
reconstruction O
error O

- B-DAT

- B-DAT
construction O
errors, O
they O
are O
unable O

- B-DAT
ally O
plausible O
high-frequencies O
details. O
To O

- B-DAT
versary O
to O
steer O
the O
reconstruction O

- B-DAT
ifold O
of O
natural O
solutions. O
Based O

- B-DAT
riculum O
learning. O
With O
this, O
our O

- B-DAT
sample O
perceptually O
pleasing O
SR O
images O

- B-DAT

- B-DAT

- B-DAT
scaling O
function O
u O
for O
large O

- B-DAT
ing: O
the O
larger O
the O
ratio O

- B-DAT

- B-DAT
resolution O
in O
Section O
3.1 O
and O

- B-DAT

- B-DAT
forming O
a O
2× O
upsampling O
of O

- B-DAT

- B-DAT
sign O
more O
DCUs O
in O
the O

- B-DAT
sumption O
but O
also O
increases O
the O

- B-DAT
ric O
variant O
in O
terms O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ture O
space. O
A O
schematic O
illustration O

- B-DAT
sampling O
architecture O
is O
detailed O
in O

- B-DAT
mid O
principle O
like O
in O
[21 O

- B-DAT

- B-DAT

- B-DAT
fied O
densely O
connected O
block O
followed O

- B-DAT

- B-DAT
CONV(1,1)-BN-RELU-CONV(3,3). O
Following O
recent O
practice O
in O

- B-DAT

-1 B-DAT

- B-DAT
CONV(3,3 O

- B-DAT

- B-DAT

- B-DAT
agation O
as O
shown O
in O
Figure O

- B-DAT
cess O
at O
applying O
GANs O
to O

- B-DAT
scale O
upsampling O
at O
relatively O
low O

- B-DAT
der O
to O
enable O
multi-scale O
GAN-enhanced O

- B-DAT
mension O
of O
the O
input O
image O

- B-DAT
modate O
the O
multi-scale O
outputs O
from O

- B-DAT
work O
is O
fully O
convolutional O
and O

- B-DAT
tures O
similar O
to O
PatchGAN O
[18 O

- B-DAT
erates O
on O
the O
residual O
between O

- B-DAT
sampled O
image. O
This O
allows O
both O

- B-DAT
nator O
to O
concentrate O
only O
on O

- B-DAT
ation O
which O
are O
not O
already O

- B-DAT
tual O
errors. O
This O
can O
also O

- B-DAT
dependent O
baseline O
from O
the O
discriminator O

- B-DAT

- B-DAT

- B-DAT
rent O
scale O
(u0, O
v0, O
r0 O

- B-DAT
midal O
network O
shown O
in O
Figure O

- B-DAT
vious O
level. O
A O
similar O
idea O

- B-DAT
sult O
we O
incrementally O
add O
training O

- B-DAT

- B-DAT
formance O
gain O
for O
all O
included O

- B-DAT
scale O
and O
simple O
multi-scale O
training O

- B-DAT
ities O
in O
GAN O
training O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
posed O
components O
using O
a O
small O

- B-DAT

- B-DAT

- B-DAT
tions O
are O
conducted O
on O
the O

- B-DAT
struction O
quality O
stemming O
from O
each O

- B-DAT

- B-DAT
nection O
from O
the O
LR O
input O

- B-DAT
ing, O
we O
describe O
the O
individual O

- B-DAT

- B-DAT
vantage O
of O
the O
proposed O
asymmetric O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ric O
pyramid O
model O
to O
8 O

- B-DAT
efit O
of O
curriculum O
learning O
over O

- B-DAT

- B-DAT
struction O
quality O
and O
outperforms O
simultaneous O

- B-DAT
neous O
training, O
since O
the O
2 O

- B-DAT
tion O
and O
hence O
less O
time O

- B-DAT
tures O

- B-DAT
pled O
LR O
image. O
Thus O
the O

- B-DAT
ble O
3. O
Therefore, O
we O
conclude O

- B-DAT

- B-DAT
pared O
to O
fixed O
interpolated O
results O

- B-DAT
sampling O
kernel O
to O
create O
the O

- B-DAT
duce O
undesired O
artefacts O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
proaches O

- B-DAT
son, O
we O
benchmark O
against O
VDSR O

- B-DAT
SRN O
[21], O
MsLapSRN O
[22], O
EDSR O

- B-DAT

simultaneous O
-0 B-DAT

- B-DAT

ours O
- B-DAT
27.44 O
- O
- O
28.41 O
- O
alt O
- O
27.32 O

- B-DAT

- B-DAT
proaches, O
we O
divide O
them O
into O

- B-DAT
ingly, O
we O
provide O
two O
models O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tween O
our O
results O
and O
the O

- B-DAT

- B-DAT

- B-DAT
imise O
the O
ℓ1 O
loss O
or O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mental O
material O

- B-DAT
erage O
0.8s, O
2.1s O
and O
4.4s O

- B-DAT

- B-DAT

- B-DAT
scaling, O
where O
the O
low O
resolution O

- B-DAT
idation O
set. O
Our O
model O
ranks O

- B-DAT

- B-DAT
ferent O
to O
the O
bicubic O
8 O

- B-DAT
nario, O
we O
also O
participated O
in O

- B-DAT

- B-DAT
ther O
improvement O
can O
be O
achieved O

- B-DAT
ing O
and O
extended O
training O
data O

- B-DAT
eling O
power, O
we O
have O
proposed O

- B-DAT
ity. O
Furthermore O
we O
leverage O
a O

- B-DAT

- B-DAT
formance O
for O
all O
scales. O
Our O

- B-DAT

- B-DAT
the-art O
benchmark O
in O
terms O
of O

- B-DAT
performs O
existing O
methods O
by O
a O

- B-DAT
plied O
to O
GAN-extended O
method O
for O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
scale O
model O
that O
could O
yield O

24.57 O
24.65 O
22.06 O
26.52 O
SRDenseNet O
- B-DAT
- O
- O
- O
28.50 O
27.53 O

26.05 O
- B-DAT
- O
- O
- O
ProSRs O
(ours) O
33.36 O
32.02 O
31.42 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cup O
and O
Y. O
W. O
Teh O

- B-DAT
national O
Conference O
on O
Machine O
Learning O

- B-DAT
riculum O
learning. O
In O
Proceedings O
of O

- B-DAT
tional O
conference O
on O
machine O
learning O

- B-DAT
Morel. O
Low-complexity O
single-image O
super-resolution O
based O

super-resolution. B-DAT
In O
Com- O
puter O
Vision O
- O
ECCV O
2014 O
- O
13th O
European O

-12, B-DAT
2014, O
Proceedings, O
Part O
IV, O
pages O

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
ference O
on O
Computer O
Vision, O
pages O

- B-DAT

- B-DAT
works O
for O
image O
super-resolution. O
In O

- B-DAT
ference O
on, O
pages O
1157–1164. O
IEEE O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-8, B-DAT
2013, O
pages O
3336–3343, O
2013. O
2 O

- B-DAT

- B-DAT
based O
super-resolution. O
IEEE O
Computer O
Graphics O

- B-DAT
plications, O
22(2):56–65, O
2002. O
2 O

- B-DAT

2009, O
Kyoto, O
Japan, O
September O
27 O
- B-DAT
October O
4, O
2009, O
pages O
349–356 O

- B-DAT

- B-DAT

- B-DAT
ing O
for O
image O
recognition. O
In O

- B-DAT
ference O
on O
computer O
vision O
and O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
to-image O
translation O
with O
conditional O
adversarial O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT
resolution. O
In O
IEEE O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ative O
adversarial O
network. O
arXiv O
preprint O

- B-DAT

- B-DAT
ley. O
Least O
squares O
generative O
adversarial O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pling. O
ACM O
Trans. O
Graph., O
27(5):153:1–153:7 O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

-26 B-DAT
June O
2008, O
Anchorage, O
Alaska, O
USA O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT
shops, O
July O
2017. O
1, O
5 O

- B-DAT
borhood O
regression O
for O
fast O
example-based O

- B-DAT

-8, B-DAT
2013, O
pages O
1920–1927, O
2013. O
2 O

- B-DAT

- B-DAT
resolution O
as O
sparse O
representation O
of O

-26 B-DAT
June O
2008, O
Anchorage, O
Alaska, O
USA O

- B-DAT
age O
super-resolution O
with O
a O
parameter O

- B-DAT

- B-DAT
up O
using O
sparse-representations. O
In O
Curves O

and O
Surfaces O
- B-DAT
7th O
International O
Conference, O
Avignon, O
France O

-30, B-DAT
2010, O
Revised O
Selected O
Papers, O
pages O

- B-DAT

- B-DAT

the O
original O
LR O
inputs O
and O
upscaling B-DAT
spatial O
reso- O
lution O
at O
the O

upscaling B-DAT
strategy O
has O
been O
demon- O
strated O

upscaling B-DAT
SR O
methods O
(e.g., O
DRRN O
[5 O

shallow O
feature O
extraction O
HSF O
(·), O
upscaling B-DAT
module O
HUP O
(·), O
and O
reconstruction O

upscaling B-DAT
layer, O
whose O
weight O
set O
is O

upscaling, B-DAT
whose O
kernel O
size O
is O
1×1 O

is O
set O
as O
16. O
For O
upscaling B-DAT
module O
HUP O
(·), O
we O
follow O

- B-DAT

- B-DAT
portance O
for O
image O
super-resolution O
(SR O

- B-DAT
resolution O
inputs O
and O
features O
contain O

- B-DAT

- B-DAT
tion, O
which O
is O
treated O
equally O

- B-DAT
resentational O
ability O
of O
CNNs. O
To O

- B-DAT
tions. O
Meanwhile, O
RIR O
allows O
abundant O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
given O
its O
low-resolution O
(LR O

- B-DAT

- B-DAT
tions, O
ranging O
from O
security O
and O

- B-DAT

- B-DAT
merous O
learning O
based O
methods O
have O

- B-DAT
layer O
CNN O
for O
image O
SR O

- B-DAT

- B-DAT
work O
depth O
was O
demonstrated O
to O

- B-DAT
nition O
tasks, O
especially O
when O
He O

- B-DAT

- B-DAT

- B-DAT
wise O
features O
equally, O
which O
lacks O

- B-DAT
formation O
(e.g., O
low- O
and O
high-frequency O

- B-DAT

- B-DAT
sible. O
The O
LR O
images O
contain O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ual O
channel O
attention O
networks O
(RCAN O

- B-DAT

- B-DAT
ture O
to O
construct O
very O
deep O

- B-DAT
tions O
in O
RIR O
help O
to O

- B-DAT

- B-DAT
tional O
ability O
of O
the O
network O

- B-DAT
nity O
[1–11,22]. O
Attention O
mechanism O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vious O
works. O
By O
introducing O
residual O

- B-DAT
icant O
improvement O
in O
accuracy. O
Tai O

- B-DAT
lution O
at O
the O
network O
tail O

- B-DAT

- B-DAT
bines O
automated O
texture O
synthesis O
and O

- B-DAT
gree, O
their O
predicted O
results O
may O

- B-DAT
cant O
improvement. O
However, O
most O
of O

- B-DAT

- B-DAT
inative O
ability O
for O
different O
types O

-1 B-DAT
RG-g O
RG-G O

- B-DAT

-1 B-DAT
RCAB-b O
RCAB-B O

- B-DAT

- B-DAT
fication O
with O
a O
trunk-and-mask O
attention O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tain O
significant O
performance O
improvement O
for O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
posed O
RIR O
achieves O
the O
largest O

- B-DAT

- B-DAT
tively O

- B-DAT

- B-DAT

- B-DAT
strated O
to O
be O
more O
efficient O

- B-DAT

- B-DAT
ial O
losses O
[8, O
21]. O
To O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
frequency O
parts O
seem O
to O
be O

- B-DAT

- B-DAT
quently, O
the O
output O
after O
convolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
by O
global O
average O
pooling O

- B-DAT
wise O
features O
can O
be O
emphasized O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tively. O
WD O
is O
the O
weight O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
Fg,bXg,b O

- B-DAT
hance O
the O
discriminative O
ability O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
table O
performance O
improvements O
over O
previous O

- B-DAT
sions O
about O
the O
effects O
of O

- B-DAT
downscaling O
and O
channel-upscaling, O
whose O
kernel O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ation O
metric, O
and O
training O
settings O

- B-DAT

-1 B-DAT
and O
top-5 O
recognition O
errors) O
comparisons O

- B-DAT

- B-DAT
tion O
(CA) O
based O
on O
the O

- B-DAT
formance. O
It’s O
hard O
to O
obtain O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
CNN O
[2], O
SCN O
[3], O
VDSR O

- B-DAT

- B-DAT
ensemble O
strategy O
to O
further O
improve O

- B-DAT

- B-DAT
parisons O
for O
×2, O
×3, O
×4 O

- B-DAT

- B-DAT

- B-DAT
age O
“img O
004”, O
we O
observe O

- B-DAT
CNN O
cannot O
recover O
lines. O
Other O

- B-DAT
Cooking”, O
the O
cropped O
part O
is O

- B-DAT
ful O
representational O
ability O
can O
extract O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults O
with O
7 O
state-of-the-art O
methods O

- B-DAT

- B-DAT
ing O
details O
in O
images O
“img O

- B-DAT
tive O
components. O
These O
comparisons O
indicate O

- B-DAT

-1 B-DAT
error O
0.506 O
0.477 O
0.437 O
0.454 O

-5 B-DAT
error O
0.266 O
0.242 O
0.196 O
0.224 O

- B-DAT

- B-DAT

-50 B-DAT
[20] O
as O
the O
evaluation O
model O

- B-DAT

- B-DAT
idation O
dataset O
for O
evaluation. O
The O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
and O
top-5 O
errors. O
These O
comparisons O

- B-DAT

- B-DAT
nections, O
making O
the O
main O
network O

- B-DAT

- B-DAT

- B-DAT
terdependencies O
among O
channels. O
Extensive O
experiments O

-14 B-DAT

-1 B-DAT

-0484, B-DAT
and O
U.S. O
Army O
Research O
Office O

-17 B-DAT

-1 B-DAT

-0367 B-DAT

- B-DAT

- B-DAT

- B-DAT
lutional O
networks. O
TPAMI O
(2016 O

- B-DAT

- B-DAT
resolution O
with O
sparse O
prior. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
with O
deep O
laplacian O
pyramid O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In: O
CVPR. O
(2018 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
chines. O
In: O
ICML. O
(2010 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
representations. O
In: O
Proc. O
7th O
Int O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formation O
flows O
are O
solely O
feedforward O

- B-DAT

- B-DAT
plored. O
In O
this O
paper, O
we O

- B-DAT
rate O
image O
SR, O
in O
which O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tures O
captured O
under O
large O
receptive O

- B-DAT

- B-DAT
ciently O
selects O
and O
further O
enhances O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-the-art O
SR O
methods O
in O
terms O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
image O
SR O
methods O

- B-DAT

- B-DAT
tures O
solely O
flow O
from O
the O

- B-DAT

- B-DAT
tures O
extracted O
from O
the O
top O

- B-DAT

- B-DAT
agating O
high-level O
features O
to O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
features, O
we O
employ O
multiple O

- B-DAT

- B-DAT
tures O
to O
shallow O
layers. O
However O

- B-DAT

- B-DAT

- B-DAT
level O
information O
to O
refine O
low-level O

- B-DAT

- B-DAT

- B-DAT
posed O
GMFN O
shows O
better O
visual O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tensive O
experiments O
demonstrate O
the O
superiority O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ploys O
16 O
RDBs O

- B-DAT
level O
features O
for O
refining O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
tasks O
(e.g. O
classification O
[29 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ent O
direction, O
[12, O
31] O
applied O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
less, O
we O
argue O
that O
such O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ent O
receptive O
fields, O
every O
piece O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
back O
module O
to O
adaptively O
eliminate O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
able O
contextual O
knowledge O
from O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
cost O
was O
quadratically O
saved O

- B-DAT
gether. O
However, O
these O
networks O
require O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pendent O
convolutional O
neural O
network O
which O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
jacent O
time O
steps O
is O
achieved O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
RDB-BRDB O

- B-DAT

-1 B-DAT
RDB-b O
RDB-B O

- B-DAT

- B-DAT

L O
BF O
-, B-DAT

L O
bF O
- B-DAT
, O
t O

- B-DAT

- B-DAT

- B-DAT
tures O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
RDN O
[33], O
the O
number O

- B-DAT

- B-DAT
volutional O
layer. O
Then, O
a O
3×3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lowing O
RDB. O
The O
placement O
of O

- B-DAT
cording O
to O
the O
relative O
hierarchical O

- B-DAT

- B-DAT

- B-DAT
cilitates O
the O
refinement O
processes O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lected O
indexes O
of O
the O
deepest O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ative O
hierarchical O
relationship O
among O
multiple O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
information O
captured O
under O
different O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
network O
is O
set O
to O
C0 O

- B-DAT
ment O
training O
images O
with O
scaling O

- B-DAT
tion O
bicubic. O
The O
SR O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formance O
of O
various O
single-to-multiple O
anti-feedback O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dex O
sets O
SM O
and O
DN O

- B-DAT

- B-DAT

- B-DAT
to-multiple O
feedback O
manner O
[12, O
31 O

- B-DAT
to-single O
feedback O
manners O
perform O
better O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fining O
low-level O
features. O
However, O
excessively O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
features. O
If O
the O
high-level O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trate O
the O
effectiveness O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bine O
various O
M O
to O
achieve O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ther O
hinder O
the O
reconstruction O
ability O

- B-DAT
tively O
selects O
the O
high O
frequency O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tively O
accesses O
to O
high-level O
information O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nections O
by O
setting O
M O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
son O
results2 O
in O
Tab. O
1 O

- B-DAT
struct O
a O
faithful O
SR O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ogy O
Department O
(No.2018GZ0178 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ding. O
In O
BMVC, O
2012 O

- B-DAT
volutional O
network O
for O
image O
super-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
projection O
networks O
for O
super-resolution. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
back O
network O
for O
image O
super-resolution O

- B-DAT

- B-DAT
layer O
recurrent O
connections O
for O
scene O

- B-DAT

- B-DAT

- B-DAT
masaki, O
and O
Kiyoharu O
Aizawa. O
Sketch-based O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
forward O
networks O
(FF O

-1 B-DAT

- B-DAT
ers O
are O
set O
to O
128 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults O
shown O
Figure O
6 O
indicate O

- B-DAT
tively. O
The O
performance O
evaluated O
on O

- B-DAT
served O
that O
with O
the O
help O

- B-DAT
nificantly O
improved O
compared O
with O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods, O
but O
it O
holds O
relatively O

-16, B-DAT
we O
provide O
more O
qualitative O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rately O
restored O
the O
letter O
"M O

- B-DAT

- B-DAT
coveres O
two O
horizontal O
lines O
as O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

layer O
in O
LR O
space O
for O
upscaling B-DAT

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

- B-DAT

- B-DAT
cently O
achieved O
great O
success O
for O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tract O
abundant O
local O
features O
via O

- B-DAT
tional O
layers. O
RDB O
further O
allows O

- B-DAT
taining O
dense O
local O
features, O
we O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
graded O
low-resolution O
(LR) O
measurement. O
SISR O

- B-DAT
lance O
imaging O
[42], O
medical O
imaging O

- B-DAT
eration O
[9]. O
While O
image O
SR O

- B-DAT

- B-DAT
cedure, O
since O
there O
exists O
a O

- B-DAT
based O
[40], O
reconstruction-based O
[37], O
and O

- B-DAT

- B-DAT

- B-DAT
ules, O
the O
networks O
for O
image O

- B-DAT
ory O
block O
was O
proposed O
to O

- B-DAT

- B-DAT
gles O
of O
view, O
and O
aspect O

- B-DAT
tion. O
While, O
most O
deep O
learning O

- B-DAT

- B-DAT
inal O
LR O
image O
to O
the O

- B-DAT
processing O
step O
not O
only O
increases O

- B-DAT
ing O
to O
our O
experiments O
(see O

- B-DAT
archical O
features O
from O
the O
original O

- B-DAT
posed O
residual O
dense O
block O
(Fig O

- B-DAT
tical O
for O
a O
very O
deep O

- B-DAT
ual O
dense O
block O
(RDB) O
as O

- B-DAT
sion O
(LFF) O
with O
local O
residual O

- B-DAT
catenating O
the O
states O
of O
preceding O

- B-DAT
ing O
layers O
within O
the O
current O

- B-DAT

- B-DAT
sion O
[15 O

- B-DAT

- B-DAT
work O
(RDN) O
for O
high-quality O
image O

- B-DAT
tiguous O
memory O
(CM) O
mechanism, O
but O

- B-DAT
lize O
all O
the O
layers O
within O

- B-DAT
tions. O
The O
accumulated O
features O
are O

- B-DAT
ods O
in O
computer O
vision O
[36 O

- B-DAT
ited O
space, O
we O
only O
discuss O

- B-DAT

- B-DAT

- B-DAT
ther O
improved O
mainly O
by O
increasing O

- B-DAT
ing O
network O
weights. O
VDSR O
[10 O

- B-DAT
creased O
the O
network O
depth O
by O

- B-DAT
duced O
recursive O
learning O
in O
a O

- B-DAT
rameter O
sharing. O
Tai O
et O
al O

- B-DAT
inal O
LR O
images O
to O
the O

- B-DAT

- B-DAT
creases O
computation O
complexity O
quadratically O
[4 O

- B-DAT

- B-DAT
tures O
from O
the O
interpolated O
LR O

- B-DAT

- B-DAT

- B-DAT
PCN O
[22], O
where O
an O
efficient O

- B-DAT

- B-DAT

- B-DAT
ods O
extracted O
features O
in O
the O

- B-DAT
nal O
LR O
features O
with O
transposed O

- B-DAT

- B-DAT

- B-DAT
lows O
direct O
connections O
between O
any O

- B-DAT
troduced O
among O
memory O
blocks O
[26 O

- B-DAT

- B-DAT
duced O
by O
a O
very O
deep O

- B-DAT
tion O
tasks O
(e.g., O
image O
SR O

- B-DAT

- B-DAT
tracts O
features O
F−1 O
from O
the O

- B-DAT
ond O
shallow O
feature O
extraction O
layer O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
layers O
within O
the O
block O

- B-DAT
ture. O
More O
details O
about O
RDB O

- B-DAT
cludes O
global O
feature O
fusion O
(GFF O

- B-DAT

- B-DAT

- B-DAT
nected O
layers, O
local O
feature O
fusion O

- B-DAT
ual O
learning, O
leading O
to O
a O

- B-DAT
nism O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sists O
of O
G O
(also O
known O

- B-DAT

- B-DAT

- B-DAT
tional O
layers O
1 O

- B-DAT

- B-DAT

- B-DAT
ing O
RDB O
and O
each O
layer O

- B-DAT
sequent O
layers, O
which O
not O
only O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ber. O
On O
the O
other O
hand O

- B-DAT
duce O
a O
1 O
× O
1 O

- B-DAT

- B-DAT
comes O
larger, O
very O
deep O
dense O

- B-DAT
tional O
layers O
in O
one O
RDB O

- B-DAT

- B-DAT
mance. O
We O
introduce O
more O
results O

- B-DAT
ing, O
we O
refer O
to O
this O

- B-DAT
maps O
produced O
by O
residual O
dense O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tively O
fused O
to O
form O
FGF O

- B-DAT

- B-DAT
quency O
information. O
However, O
in O
the O

- B-DAT
sequent O
layers. O
The O
local O
feature O

- B-DAT

- B-DAT
tion, O
MemNet O
extracts O
features O
in O

- B-DAT

- B-DAT
nition). O
While O
RDN O
is O
designed O

- B-DAT

- B-DAT
ual O
learning, O
which O
would O
be O

- B-DAT

- B-DAT
cal O
features, O
which O
are O
neglected O

- B-DAT
ferences O
between O
SRDenseNet O
[31] O
and O

- B-DAT
troduces O
the O
basic O
dense O
block O

- B-DAT
bilizes O
the O
training O
of O
wide O

- B-DAT
tract O
global O
features, O
because O
our O

- B-DAT
formance O
and O
convergence O
[17]. O
As O

- B-DAT
formation O
from O
their O
preceding O
layers O

- B-DAT
ers O
within O
one O
RDB. O
Furthermore O

- B-DAT
nections O
among O
memory O
blocks O
in O

- B-DAT

- B-DAT
sults O
are O
evaluated O
with O
PSNR O

- B-DAT
tion O
models O
to O
simulate O
LR O

- B-DAT
bic O
downsampling O
by O
adopting O
the O

- B-DAT
ing O
90◦. O
1,000 O
iterations O
of O

- B-DAT

- B-DAT
ery O
200 O
epochs. O
Training O
a O

- B-DAT
rameters: O
the O
number O
of O
RDB O

- B-DAT
mance O
of O
SRCNN O
[3] O
as O

- B-DAT
cal O
residual O
learning O
(LRL), O
and O

- B-DAT
portant, O
our O
RDN O
allows O
deeper O

- B-DAT
ture O
fusion O
(LFF) O
is O
needed O

- B-DAT
erly, O
so O
LFF O
isn’t O
removed O

- B-DAT
strates O
that O
stacking O
many O
basic O

- B-DAT
sulting O
in O
RDN O
CM1LRL0GFF0, O
RDN O

- B-DAT
ponent O
can O
efficiently O
improve O
the O

- B-DAT
line. O
This O
is O
mainly O
because O

- B-DAT
ing O
in O
RDN O
CM1LRL1GFF0, O
RDN O

- B-DAT
bination O
in O
Table O
1). O
It O

- B-DAT
ponents O
simultaneously O
(denote O
as O
RDN O

- B-DAT
sistent O
with O
the O
analyses O
above O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
SR O
methods: O
SRCNN O
[3 O

- B-DAT

- B-DAT
ther O
improve O
our O
RDN O
and O

- B-DAT

- B-DAT
rable O
or O
even O
better O
results O

- B-DAT
DenseNet O
[31] O
and O
MemNet O
[26 O

- B-DAT
els, O
our O
RDN O
also O
achieves O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put O
patch O
size. O
Moreover, O
our O

- B-DAT

- B-DAT
ods O
would O
produce O
noticeable O
artifacts O

- B-DAT
pared O
methods O
fail O
to O
recover O

- B-DAT
cover O
it O
obviously. O
This O
is O

- B-DAT
archical O
features O
through O
dense O
feature O

- B-DAT
CNN O
[3], O
FSRCNN O
[4], O
VDSR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults O
in O
Figs. O
7 O
and O

- B-DAT
covers O
sharper O
edges. O
This O
comparison O

- B-DAT
tracting O
hierarchical O
features O
from O
the O

- B-DAT
ods O
[3, O
10, O
38]. O
However O

- B-DAT
parison O
indicates O
that O
RDN O
is O

- B-DAT
tion O
models O
demonstrate O
the O
effectiveness O

- B-DAT
ing O
factor O
×3. O
The O
SR O

- B-DAT
ban100 O
and O
“img O
099” O
from O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferent O
or O
unknown O
degradation O
models O

- B-DAT
lizes O
the O
training O
wider O
network O

- B-DAT
ual O
leaning O
(LRL) O
further O
improves O

- B-DAT
world O
data. O
Extensive O
benchmark O
evaluations O

- B-DAT
strate O
that O
our O
RDN O
achieves O

- B-DAT

- B-DAT

- B-DAT
art O
methods O

-14 B-DAT

-1 B-DAT

- B-DAT
0484, O
and O
U.S. O
Army O
Research O

-17 B-DAT

- B-DAT
1-0367 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
deep O
convolutional O
networks O

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT

- B-DAT
mization. O
In O
ICLR, O
2014. O
5 O

- B-DAT
resolution. O
In O
CVPR, O
2017. O
1 O

- B-DAT
ham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT

- B-DAT
supervised O
nets. O
In O
AISTATS, O
2015 O

- B-DAT

- B-DAT
cal O
statistics. O
In O
ICCV, O
2001 O

- B-DAT
masaki, O
and O
K. O
Aizawa. O
Sketch-based O

- B-DAT
ing O
manga109 O
dataset. O
Multimedia O
Tools O

- B-DAT

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT
tia, O
A. O
M. O
S. O
M O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
hazing O
network. O
In O
CVPR, O
2018 O

- B-DAT

- B-DAT
raining O
using O
a O
multi-stream O
dense O

- B-DAT

- B-DAT
resolution O
with O
non-local O
means O
and O

- B-DAT
sion. O
TIP, O
2012. O
1 O

- B-DAT
lutional O
super-resolution O
network O
for O
multiple O

- B-DAT

- B-DAT

- B-DAT
nition O
problem. O
TIP, O
2012. O
1 O

patch O
size O
based O
on O
the O
upscaling B-DAT
factor. O
The O
settings O
of O
input O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

- B-DAT

- B-DAT

- B-DAT
plored O
the O
power O
of O
deep O

- B-DAT
construction O
performance. O
However, O
the O
feedback O

- B-DAT
nism, O
which O
commonly O
exists O
in O

- B-DAT

- B-DAT
level O
representations O
with O
high-level O
information O

- B-DAT
ically, O
we O
use O
hidden O
states O

- B-DAT
ful O
high-level O
representations. O
The O
proposed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
task, O
which O
aims O
to O

- B-DAT

- B-DAT

- B-DAT
herently O
ill-posed O
since O
multiple O
HR O

- B-DAT
merous O
image O
SR O
methods O
have O

- B-DAT
ing O
interpolation-based O
methods[45], O
reconstruction-based O
methods[42 O

- B-DAT

- B-DAT
lutional O
Neural O
Network O
(CNN) O
to O

- B-DAT
tention O
in O
recent O
years O
due O

- B-DAT
posed O
network. O
Blue O
arrows O
represent O

- B-DAT

- B-DAT
ing O
more O
contextual O
information O
with O

- B-DAT
ishing/exploding O
problems O
caused O
by O
simply O

- B-DAT
ters O
increases. O
A O
large-capacity O
network O

- B-DAT

- B-DAT
current O
Neural O
Network O
(RNN). O
Similar O

- B-DAT
tional O
deep O
learning O
based O
methods O

- B-DAT
ward O
manner. O
However, O
the O
feedforward O

- B-DAT

- B-DAT

- B-DAT
down O
manner, O
carrying O
high-level O
information O

- B-DAT
vious O
layers O
and O
refining O
low-level O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
structed O
by O
multiple O
sets O
of O

- B-DAT
and O
down-sampling O
layers O
with O
dense O

- B-DAT

- B-DAT
covery O
difficulty. O
Such O
curriculum O
learning O

- B-DAT
tion O
models. O
Experimental O
results O
demonstrate O

- B-DAT
ority O
of O
our O
proposed O
SRFBN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work O
(SRFBN), O
which O
employs O
a O

- B-DAT
nism. O
High-level O
information O
is O
provided O

- B-DAT

- B-DAT
while, O
such O
recurrent O
structure O
with O

- B-DAT
tions O
provides O
strong O
early O
reconstruction O

- B-DAT
ficiently O
handles O
feedback O
information O
flows O

- B-DAT

- B-DAT
and O
down- O
sampling O
layers, O
and O

- B-DAT

- B-DAT
ing O
reconstruction O
difficulty O
are O
fed O

- B-DAT

- B-DAT

- B-DAT
ious O
computer O
vision O
tasks O
including O

- B-DAT

- B-DAT

- B-DAT
mation O
usage O
in O
LR O
images O

- B-DAT
provement O
in O
image O
SR. O
SRResNet[21 O

- B-DAT
plied O
residual O
skip O
connections O
from O

- B-DAT
work O
architectures O
use O
or O
combine O

- B-DAT

- B-DAT

- B-DAT
textual O
information O
due O
to O
the O

- B-DAT

- B-DAT
ing O
layers, O
and O
thus O
further O

- B-DAT
ity O
of O
the O
network. O
To O

- B-DAT
resolution O
feedback O
network O
(SRFBN), O
in O

- B-DAT

- B-DAT
down O
manner O
to O
correct O
low-level O

- B-DAT
textual O
information O

- B-DAT
capacity O
networks O
occupy O
huge O
amount O

- B-DAT
rent O
structure O
was O
employed[19, O
31 O

- B-DAT

- B-DAT

- B-DAT
and O
down-projection O
units O
to O
achieve O

- B-DAT
tion O
between O
two O
recurrent O
states O

- B-DAT

- B-DAT
ever, O
the O
flow O
of O
information O

- B-DAT

- B-DAT
tion O
of O
an O
input O
image O

- B-DAT
tional O
recurrent O
neural O
network. O
However O

- B-DAT

- B-DAT
ficiently O
flows O
across O
hierarchical O
layers O

- B-DAT
rior O
reconstruction O
performance O
than O
ConvLSTM1 O

- B-DAT
ficient O
strategy O
to O
improve O
the O

- B-DAT

- B-DAT
diction, O
they O
enforce O
a O
curriculum O

- B-DAT
creases O
during O
the O
training O
process O

- B-DAT
mid O
in O
previously O
trained O
networks O

- B-DAT
cess, O
we O
enforce O
a O
curriculum O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
effect O
process O
helps O
to O
achieve O

- B-DAT

- B-DAT
eration O
(to O
force O
the O
network O

- B-DAT
tion O
of O
high-level O
information), O
(2 O

- B-DAT

- B-DAT
formation, O
which O
is O
needed O
to O

- B-DAT
folded O
to O
T O
iterations, O
in O

- B-DAT
rally O
ordered O
from O
1 O
to O

- B-DAT

- B-DAT
tains O
three O
parts: O
an O
LR O

- B-DAT
sampled O
image O
to O
bypass O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
layer O
and O
a O
deconvolutional O

- B-DAT
traction O
block. O
F O
tin O
are O

- B-DAT

- B-DAT

- B-DAT
tained O
by O

- B-DAT

- B-DAT

- B-DAT
sentations O
F O
tin, O
and O
then O

- B-DAT

- B-DAT
tion O
block. O
The O
FB O
contains O

- B-DAT
tially O
with O
dense O
skip O
connections O

- B-DAT
jection O
group, O
which O
can O
project O

- B-DAT
tures O
F O
tin O
by O
feedback O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ingly, O
Ltg O
can O
be O
obtained O

- B-DAT

- B-DAT
tion O
group O
and O
map O
the O

- B-DAT
work. O
T O
target O
HR O
images O

- B-DAT
work. O
(I1HR, O
I O

- B-DAT
tion O
in O
the O
network O
can O

- B-DAT
put O
at O
the O
t-th O
iterations O

- B-DAT

- B-DAT
and O
down-sampling O
operations. O
For O
×2 O

- B-DAT
ating O
LR O
images O
from O
ground O

- B-DAT
ify O
the O
effectiveness O
of O
our O

- B-DAT
degradation O
models O
as O
[47] O
do O

- B-DAT
periments, O
we O
use O
7x7 O
sized O

- B-DAT
sampling O
followed O
by O
adding O
Gaussian O

- B-DAT
ing O
rate O
0.0001. O
The O
learning O

- B-DAT
ery O
200 O
epochs. O
We O
implement O

- B-DAT
ber O
of O
iterations O
(denoted O
as O

- B-DAT
jection O
groups O
in O
the O
feedback O

- B-DAT
iments. O
We O
first O
investigate O
the O

- B-DAT
out O
feedback O
connections O
(T=1). O
Besides O

- B-DAT
sults. O
It O
is O
worth O
noticing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
forward O
one O
in O
this O
subsection O

- B-DAT
put O
to O
low-level O
representations O
and O

- B-DAT
erty), O
denoted O
as O
SRFBN-L-FF. O
SRFBN-L O

- B-DAT

- B-DAT
FF O
both O
have O
four O
iterations O

- B-DAT
ate O
SR O
images O
from O
both O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
from O
which O
we O
conclude O

- B-DAT
trast O
to O
feedforward O
network. O
The O

- B-DAT
rent O
structure. O
Except O
for O
the O

- B-DAT
tive O
experiments O
to O
verify O
other O

- B-DAT
ation O
except O
the O
first O
iteration O

- B-DAT
put O
to O
low-level O
representations O
and O

- B-DAT
erty), O
denoted O
as O
SRFBN-L-FF. O
SRFBN-L O

- B-DAT

- B-DAT
FF O
both O
have O
four O
iterations O

- B-DAT
ate O
SR O
images O
from O
both O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ation, O
from O
which O
we O
conclude O

- B-DAT
trast O
to O
feedforward O
network. O
The O

- B-DAT
current O
structure. O
Except O
the O
above O

- B-DAT
tive O
experiments O
to O
verify O
other O

- B-DAT
ation O
except O
the O
first O
iteration O

- B-DAT
works O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trated O
in O
Fig. O
5. O
Each O

- B-DAT

- B-DAT
covering O
the O
residual O
image. O
In O

- B-DAT
inal O
input O
image[16] O
and O
to O

- B-DAT

- B-DAT
ponents O
(i.e. O
edges O
and O
contours O

- B-DAT
age. O
To O
some O
extent, O
this O

- B-DAT
tion O
ability O
than O
the O
feedforward O

- B-DAT
vation O
is O
that O
the O
feedback O

- B-DAT
sentations O
in O
contrast O
to O
feedforward O

- B-DAT
tions O
and O
then O
the O
smooth O

- B-DAT
strate O
that O
the O
feedforward O
network O

- B-DAT
formation O
through O
layers, O
while O
the O

- B-DAT
lowed O
to O
devote O
most O
of O

- B-DAT

- B-DAT

- B-DAT
sentations O
at O
the O
initial O
iteration O

- B-DAT

- B-DAT

- B-DAT
quent O
iterations O
to O
generate O
better O

- B-DAT
culty. O
For O
example, O
to O
guide O

- B-DAT
works O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trated O
in O
Fig. O
5. O
Each O

- B-DAT

- B-DAT
work O
with O
global O
residual O
skip O

- B-DAT
ering O
the O
residual O
image. O
In O

- B-DAT
put O
image[16] O
and O
to O
predict O

- B-DAT

- B-DAT
servations. O
First, O
compared O
with O
the O

- B-DAT
tions O
in O
contrast O
to O
feedforward O

- B-DAT
cantly O
from O
the O
first O
iteration O

- B-DAT

- B-DAT

- B-DAT
ing O
high-level O
information O
at O
the O

- B-DAT

- B-DAT
back O
network O
will O
urge O
previous O

- B-DAT
tions O
to O
generate O
better O
representations O

- B-DAT
culty. O
For O
example, O
to O
guide O

- B-DAT
gle O
downsampling O
operator O
at O
early O

- B-DAT

- B-DAT
tion O
model. O
The O
results O
shown O

- B-DAT
riculum O
learning O
strategy O
well O
assists O

- B-DAT

- B-DAT
work O
pretrained O
on O
the O
BI O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
son O
results O
are O
given O
in O

- B-DAT

- B-DAT
rameters O
fewer O
than O
1000K. O
This O

- B-DAT
struction O
performance. O
Meanwhile, O
in O
comparison O

- B-DAT
DBPN O
and O
EDSR, O
our O
proposed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods: O
SRCNN[7], O
VDSR[18], O
DRRN[31], O
SRDenseNet[36 O

- B-DAT

- B-DAT

- B-DAT
perform O
almost O
all O
comparative O
methods O

- B-DAT

- B-DAT
ages O
(DIV2K+Flickr2K+ImageNet O
vs. O
DIV2K+Flickr2K). O
However O

- B-DAT
trast O
to O
them. O
In O
addition O

- B-DAT
age O
from O
Manga109, O
DRRN O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
curriculum O
learning O
strategy O
for O

- B-DAT
tion O
models, O
and O
fine-tuned O
based O

- B-DAT
CNN O
C[43], O
SRMD(NF)[44], O
and O
RDN[47 O

- B-DAT

- B-DAT
most O
all O
quantative O
results O
over O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O

- B-DAT

- B-DAT

- B-DAT
mark O
datasets. O
Compared O
with O
other O

- B-DAT
posed O
SRFBN O
could O
alleviate O
the O

- B-DAT
isions, O
we O
further O
indicate O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dation O
models. O
The O
comprehensive O
experimental O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sored O
by O
National O
Natural O
Science O

- B-DAT
tion O
of O
Sichuan O
Science O
and O

- B-DAT

- B-DAT
son O
Weston. O
Curriculum O
learning. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
with O
feedback O
convolutional O
neural O

- B-DAT
tendra O
Malik. O
Human O
pose O
estimation O

- B-DAT

- B-DAT

- B-DAT
works. O
TPAMI, O
2016. O
2, O
7 O

- B-DAT

- B-DAT
down O
influences O
in O
sensory O
processing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mance O
on O
imagenet O
classification. O
In O

- B-DAT
ian O
Q O
Weinberger. O
Densely O
connected O

- B-DAT
works. O
In O
CVPR, O
2016. O
2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rate O
single O
image O
super-resolution O
via O

- B-DAT
tion O
network. O
In O
CVPR, O
2018 O

- B-DAT
tween O
figure O
and O
background O
by O

- B-DAT
ture, O
1998. O
2 O

- B-DAT

- B-DAT
works. O
In O
CVPR, O
2016. O
1 O

- B-DAT
recursive O
convolutional O
network O
for O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tex. O
arXiv O
preprint O
arXiv:1604.03640, O
2016 O

- B-DAT

- B-DAT
dra O
Malik. O
A O
database O
of O

- B-DAT

- B-DAT
timedia O
Tools O
and O
Applications, O
2017 O

- B-DAT

- B-DAT

- B-DAT
back O
for O
crowd O
counting O
convolutional O

- B-DAT

- B-DAT
resolution O
via O
deep O
recursive O
residual O

- B-DAT
net: O
A O
persistent O
memory O
network O

- B-DAT
chored O
neighborhood O
regression O
for O
fast O

- B-DAT

- B-DAT

- B-DAT
resolution. O
In O
ACCV, O
2015. O
1 O

- B-DAT

- B-DAT
tion. O
In O
CVPR, O
2016. O
7 O

- B-DAT

- B-DAT
hanced O
super-resolution O
generative O
adversarial O
networks O

- B-DAT
der O
Sorkinehornung, O
Olga O
Sorkinehornung, O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
back O
networks. O
In O
CVPR, O
2017 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lation O
algorithm O
via O
directional O
filtering O

- B-DAT

- B-DAT

- B-DAT
tion. O
We O
still O
use O
SRFBN-L O

- B-DAT
and O
down-sampling O
layers O
(UDSL), O
(2 O

- B-DAT
and O
down-sampling O
layers O
with O
3 O

- B-DAT
ers O
(with O
one O
padding O
and O

- B-DAT
and O
down-sampling O
operations O
carrying O
large O

- B-DAT
tion O
and O
are O
effective O
for O

- B-DAT

- B-DAT
ter O
adding O
DSC O
to O
the O

- B-DAT
and O
down-sampling O
layers O
(UDSL), O
and O

- B-DAT
sides, O
high-level O
information O
is O
directly O

- B-DAT

- B-DAT

- B-DAT
ization O

- B-DAT
textual O
information O
for O
the O
next O

- B-DAT
parison O
with O
other O
basic O
blocks O

- B-DAT
nism O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tations O
to O
the O
initial O
feature O

- B-DAT

- B-DAT
formation, O
surely O
are O
corrected O
using O

- B-DAT

- B-DAT

- B-DAT
erage O
feature O
map O
at O
each O

- B-DAT

- B-DAT
back) O
and O
SRFBN-L-FF O
(feedforward). O
As O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trum O
map O
through O
discrete O
Fourier O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eration O
t O
grows, O
the O
feedforward O

- B-DAT
ers O
mid-frequency O
and O
high-frequency O
components O

- B-DAT
developed O
information. O
For O
the O
feedback O

- B-DAT
nism O
(t O
>1), O
mid-frequency O
and O

- B-DAT

- B-DAT
tion O
of O
the O
average O
feature O

- B-DAT
ture O
design, O
we O
compare O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
network O
with O
moderate O
parameters O

- B-DAT
cause O
MemNet O
only O
reveals O
the O

- B-DAT

- B-DAT

- B-DAT
tary O
materials. O
Our O
SRFBN-S O
(T=4 O

- B-DAT
son. O
In O
Tab. O
8, O
our O

- B-DAT

- B-DAT
sults O
than O
MemNet O
with O
71 O

- B-DAT

- B-DAT
parison O
shows O
the O
effectiveness O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ban100 O
with O
scale O
factor O
×4 O

- B-DAT

- B-DAT
torch O
for O
fair O
comparison. O
The O

- B-DAT
works O
is O
evaluated O
on O
the O

- B-DAT
tel O
i7 O
CPU O
(16G O
RAM O

- B-DAT
ing O
their O
official O
codes. O
Tab O

- B-DAT

- B-DAT
son O
with O
other O
networks. O
This O

- B-DAT
tiveness O
of O
our O
proposed O
networks O

- B-DAT
volutional O
layers O
with O
77% O
fewer O

-22, B-DAT
we O
provide O
more O
visual O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
images. O
We O
propose O

- B-DAT

- B-DAT
and O
down- O
sampling O
layers, O
providing O

- B-DAT
connected O
up- O
and O
down-sampling O
stages O

- B-DAT
resolution O
components. O
We O
show O
that O

- B-DAT
and O
down- O
sampling O
stages O
(Dense O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
proach O
is O
to O
construct O
an O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work O
[6, O
7, O
38, O
25 O

- B-DAT
ing O
with O
one O
or O
more O

- B-DAT
lution O
and O
finally O
construct O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
SRN O
[25] O
(15.25 O
dB), O
EDSR O

- B-DAT
fectively O
by O
one O
of O
the O

- B-DAT

- B-DAT
tion O
error O
then O
fuses O
it O

- B-DAT
fect O
[4]. O
Moreover, O
this O
method O

- B-DAT
erator, O
leading O
to O
variability O
in O

- B-DAT

- B-DAT

- B-DAT
and O
down- O
sampling: O
Deep O
Back-Projection O

- B-DAT
butions: O
(1) O
Error O
feedback. O
We O

- B-DAT
correcting O
feedback O
mechanism O
for O
SR O

- B-DAT
and O
down-projection O
errors O
to O
guide O

- B-DAT
tion O
for O
obtaining O
better O
results O

- B-DAT
and O
down-sampling O
stages. O
Feed-forward O
architectures O

- B-DAT
way O
mapping, O
only O
map O
rich O

- B-DAT

- B-DAT
(blue O
box) O
and O
down-sampling O
(gold O

- B-DAT
tures O
using O
upsampling O
layers O
but O

- B-DAT
(blue O
box) O
and O
down-sampling O
(gold O

- B-DAT
ity O
enables O
the O
networks O
to O

- B-DAT

- B-DAT
tion O
directly O
utilizes O
different O
types O

- B-DAT

- B-DAT

- B-DAT
and O
down-sampling O
stage O
to O
encourage O

- B-DAT

- B-DAT
tion O
as O
the O
upsampling O
operator O

- B-DAT
tion O
(MR) O
image. O
This O
schema O

- B-DAT
CNN O
[6] O
to O
learn O
MR-to-HR O

- B-DAT

- B-DAT
ple O
convolutional O
layers. O
Later, O
the O

- B-DAT
ploited O
residual O
learning O
[22, O
43 O

- B-DAT
posed O
by O
FSRCNN O
[7] O
and O

- B-DAT
tion O
and O
replace O
predefined O
operators O

- B-DAT
longs O
to O
this O
type. O
However O

- B-DAT
portunities O
to O
propose O
lighter O
networks O

- B-DAT

- B-DAT
late O
the O
reconstruction O
error O
to O

- B-DAT
ables O
the O
networks O
to O
preserve O

- B-DAT
ing O
various O
up- O
and O
down-sampling O

- B-DAT
ating O
deeper O
features O

- B-DAT

- B-DAT
to-target O
space O
in O
one O
step O

- B-DAT
pose O
the O
prediction O
process O
into O

- B-DAT

- B-DAT
back O
procedure O
has O
been O
implemented O

- B-DAT
tion. O
PredNet O
[32] O
is O
an O

- B-DAT
tion, O
Li O
et O
al. O
[29 O

- B-DAT
back O
procedures O
have O
not O
been O

- B-DAT
ial O
Networks O
(GANs) O
[10] O
has O

- B-DAT
age O
reconstruction O
problems O
[28, O
37 O

- B-DAT

- B-DAT

- B-DAT
works. O
Ledig O
et O
al. O
[28 O

- B-DAT
ered O
as O
a O
single O
upsampling O

- B-DAT
ral O
image O
manifold O
that O
is O

- B-DAT

- B-DAT
ages O
by O
specifically O
formulating O
a O

- B-DAT

- B-DAT

- B-DAT
erative O
procedure O
to O
minimize O
the O

- B-DAT
projection O
[51, O
11, O
8, O
46 O

- B-DAT

- B-DAT
ror O
iteratively O
[4]. O
Timofte O
et O

- B-DAT
projection O
can O
improve O
the O
quality O

- B-DAT

- B-DAT
known. O
Most O
of O
the O
previous O

- B-DAT

- B-DAT

- B-DAT
and O
down-sampling O
stages O
to O
learn O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
projection O
unit O
projects O
it O
back O

- B-DAT
serve O
the O
HR O
components O
by O

- B-DAT
and O
down- O
sampling O
operators O
and O

- B-DAT
struct O
numerous O
LR O
and O
HR O

- B-DAT

- B-DAT

- B-DAT
end O
training O
of O
the O
SR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
spectively, O
the O
up- O
and O
down-sampling O

- B-DAT

- B-DAT

- B-DAT
and O
down-projection O
unit O
in O
the O

- B-DAT
nating O
between O
H O
and O
L O

- B-DAT
derstood O
as O
a O
self-correcting O
procedure O

- B-DAT
jection O
error O
to O
the O
sampling O

- B-DAT
sized O
filter O
is O
avoided O
because O

- B-DAT
gence O
speed O
and O
might O
produce O

- B-DAT

- B-DAT
ever, O
iterative O
utilization O
of O
our O

- B-DAT
works O

- B-DAT

- B-DAT
gradient O
problem, O
produce O
improved O
feature O

- B-DAT
age O
feature O
reuse. O
Inspired O
by O

- B-DAT

- B-DAT
mensional O
reduction O
[42, O
12] O
before O

- B-DAT

- B-DAT
and O
down-projection O
unit, O
re- O
spectively O

- B-DAT
ture O
maps O
effectively, O
as O
shown O

- B-DAT

- B-DAT
jection, O
and O
reconstruction, O
as O
described O

- B-DAT
and O
down-projection O
unit O
in O
the O

- B-DAT

- B-DAT
and O
down-projections O
units, O
respectively) O
are O

- B-DAT

- B-DAT
tures O
extraction O
and O
nR O
is O

- B-DAT

- B-DAT
traction O
is O
a O
sequence O
of O

- B-DAT

- B-DAT

- B-DAT
work O
architecture O
is O
modular. O
We O

- B-DAT
traction O
stage O
(2 O
layers), O
and O

- B-DAT

- B-DAT

- B-DAT
tion O
unit O
is O
various O
with O

- B-DAT
nally, O
the O
8× O
enlargement O
use O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puted O
by O

- B-DAT
volutional O
layers O
are O
followed O
by O

- B-DAT
tion.2 O
To O
produce O
LR O
images O

- B-DAT
ing O
Caffe, O
MATLAB O
R2017a O
on O

- B-DAT
tion. O
The O
input O
and O
output O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formance, O
S O
networks O
can O
achieve O

- B-DAT
SRN, O
respectively. O
The O
M O
network O

- B-DAT

- B-DAT

- B-DAT
tal, O
the O
M O
network O
use O

- B-DAT
SRN, O
and O
DRRN, O
respectively O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
largement. O
S O
(T O
= O
2 O

- B-DAT

- B-DAT
rameters O
on O
4× O
and O
8 O

- B-DAT
DBPN O
has O
about O
76% O
fewer O

- B-DAT

- B-DAT
dence O
show O
that O
our O
networks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tures O
generated O
from O
the O
projection O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tively O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pare O
our O
network O
with O
eight O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rithms: O
A+ O
[45], O
SRCNN O
[6 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tics. O
Set5, O
Set14 O
and O
BSDS100 O

- B-DAT

- B-DAT
vide O
each O
image O
in O
Urban100 O

- B-DAT

- B-DAT

- B-DAT
put O
image. O
It O
takes O
less O

- B-DAT
DBPN O
outperforms O
the O
existing O
methods O

- B-DAT

- B-DAT
vious O
statement O
is O
strengthened O
by O

- B-DAT
ban100 O
dataset O
which O
consist O
of O

- B-DAT

- B-DAT
ods O
by O
a O
large O
margin O

- B-DAT

- B-DAT

- B-DAT
ter O
than O
EDSR. O
The O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
which O
predict O
the O
SR O

- B-DAT

- B-DAT
tures O
using O
multiple O
up- O
and O

- B-DAT

- B-DAT

- B-DAT
and O
down-scaling O
steps O
to O
guide O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work O
successfully O
outperforms O
other O
state-of-the-art O

- B-DAT
ods O
on O
large O
scaling O
factors O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
Conference O
(BMVC), O
2012. O
6 O

- B-DAT
man O
pose O
estimation O
with O
iterative O

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT
projection O
for O
single O
image O
super O

- B-DAT
tive O
image O
models O
using O
a O

- B-DAT
tems, O
pages O
1486–1494, O
2015. O
1 O

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
ference O
on O
Computer O
Vision, O
pages O

- B-DAT
projection O
for O
adaptive O
image O
enlargement O

- B-DAT
cessing O
(ICIP), O
2009 O
16th O
IEEE O

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
nets. O
In O
Advances O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
for O
image O
recognition. O
arXiv O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision, O
pages O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
sion O
and O
Pattern O
Recognition O
(CVPR O

- B-DAT
istration. O
CVGIP: O
Graphical O
models O
and O

- B-DAT
ment: O
Resolution, O
occlusion, O
and O
transparency O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT
volutional O
network O
for O
image O
super-resolution O

- B-DAT
ings O
of O
the O
IEEE O
Conference O

- B-DAT
resolution. O
In O
IEEE O
Conferene O
on O

- B-DAT
tern O
Recognition, O
2017. O
1, O
2 O

- B-DAT
sion O
offered O
by O
feedforward O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
adversarial O
network. O
In O
IEEE O

- B-DAT
tation. O
In O
Proceedings O
of O
the O

- B-DAT
resolution O
via O
deep O
draft-ensemble O
learning O

- B-DAT

- B-DAT
ing O
networks O
for O
video O
prediction O

- B-DAT
masaki, O
and O
K. O
Aizawa. O
Sketch-based O

- B-DAT
ing O
manga109 O
dataset. O
Multimedia O
Tools O

- B-DAT
sentation O
learning O
with O
deep O
convolutional O

- B-DAT
sarial O
networks. O
arXiv O
preprint O
arXiv:1511.06434 O

- B-DAT

- B-DAT
tion. O
In O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT
back O
for O
faster O
r-cnn. O
In O

- B-DAT
tion, O
2017. O
1 O

- B-DAT

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT
nition O
Workshops O
(CVPRW), O
2017 O
IEEE O

- B-DAT

- B-DAT
prove O
example-based O
single O
image O
super O

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT
level O
vision O
tasks O
and O
3d O

- B-DAT
tural O
similarity. O
Image O
Processing, O
IEEE O

- B-DAT

- B-DAT

- B-DAT
erative O
projection O
reconstruction O
for O
fast O

reconstruct O
high-resolution O
images O
of O
different O
upscaling B-DAT
factors O
in O
a O
single O
model O

demonstrated O
in O
Fig. O
4. O
For O
upscaling B-DAT
×4, O
if O
we O
use O
a O

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

- B-DAT

- B-DAT

- B-DAT
hanced O
deep O
super-resolution O
network O
(EDSR O

- B-DAT
mance O
exceeding O
those O
of O
current O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods. O
The O
significant O
performance O
improvement O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
on O
benchmark O
datasets O
and O

- B-DAT
ning O
the O
NTIRE2017 O
Super-Resolution O
Challenge O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nificantly O
improved O
performance O
in O
terms O

- B-DAT

- B-DAT
noise O
ratio O
(PSNR) O
in O
the O

- B-DAT
works O
exhibit O
limitations O
in O
terms O

- B-DAT

- B-DAT

- B-DAT
resolution O
of O
different O
scale O
factors O

- B-DAT
lems O
without O
considering O
and O
utilizing O

- B-DAT
quire O
many O
scale-specific O
networks O
that O

- B-DAT

- B-DAT

- B-DAT
dancy O
among O
scale-specific O
models. O
Nonetheless O

- B-DAT

- B-DAT
pling O
method O
[5, O
22, O
14 O

- B-DAT
ploys O
the O
ResNet O
architecture O
from O

- B-DAT
posed O
to O
solve O
higher-level O
computer O

- B-DAT

- B-DAT

- B-DAT
chitecture, O
we O
first O
optimize O
it O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els. O
Furthermore, O
we O
propose O
a O

- B-DAT

- B-DAT

- B-DAT
rameters O
compared O
with O
multiple O
single-scale O

- B-DAT
and O
multi-scale O
super-resolution O
networks O
show O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dicting O
detailed, O
realistic O
textures. O
Previous O

- B-DAT
struct O
better O
high-resolution O
images O

- B-DAT
tween O
ILR O
and O
IHR O
image O

- B-DAT
ods O
rely O
on O
techniques O
ranging O

- B-DAT
ding O
[3, O
2, O
7, O
21 O

- B-DAT
proaches O
utilize O
image O
self-similarities O
to O

- B-DAT
nal O
databases O
[8, O
6, O
29 O

- B-DAT
works O
has O
led O
to O
dramatic O

- B-DAT

- B-DAT
ing O
much O
deeper O
network O
architectures O

- B-DAT
perior O
performance. O
In O
particular, O
they O

- B-DAT
connection O
and O
recursive O
convolution O
alleviate O

- B-DAT

- B-DAT
work. O
Similarly O
to O
[20], O
Mao O

- B-DAT

- B-DAT

- B-DAT
rithms, O
an O
input O
image O
is O

- B-DAT
lation O
before O
they O
fed O
into O

- B-DAT
sampling O
modules O
at O
the O
very O

- B-DAT
sible O
as O
shown O
in O
[5 O

- B-DAT
cause O
the O
size O
of O
features O

- B-DAT

- B-DAT
scale O
training O
and O
computational O
efficiency O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
thermore, O
we O
develop O
an O
appropriate O

- B-DAT
and O
multi-scale O
mod- O
els O

- B-DAT

- B-DAT
hibiting O
improved O
computational O
efficiency. O
In O

- B-DAT
ing O
sections, O
we O
suggest O
a O

- B-DAT

- B-DAT

- B-DAT
scale O
architecture O
(MDSR) O
that O
reconstructs O

- B-DAT

- B-DAT
level O
to O
high-level O
tasks. O
Although O

- B-DAT
fully O
applied O
the O
ResNet O
architecture O

- B-DAT

- B-DAT
mance O
by O
employing O
better O
ResNet O

- B-DAT
work O
model O
from O
original O
ResNet O

- B-DAT
tion O
increases O
the O
performance O
substantially O

- B-DAT
duced O
since O
the O
batch O
normalization O

- B-DAT

- B-DAT

- B-DAT
work O
model O
is O
to O
increase O

- B-DAT
nels) O
F O
occupies O
roughly O
O(BF O

- B-DAT
imize O
the O
model O
capacity O
when O

- B-DAT
tational O
resources O

- B-DAT
cedure O
numerically O
unstable. O
A O
similar O

- B-DAT
ing O
procedure O
greatly O
when O
using O

- B-DAT
ous O
convolution O
layer O
for O
the O

- B-DAT

- B-DAT
vation O
layers O
outside O
the O
residual O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
at O
multiple O
scales O
is O

- B-DAT

- B-DAT
ther O
explore O
this O
idea O
by O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
specific O
processing O
modules O
to O
handle O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ule O
consists O
of O
two O
residual O

- B-DAT

- B-DAT

- B-DAT
tive O
field O
is O
covered O
in O

- B-DAT

- B-DAT

- B-DAT
ules O
are O
located O
in O
parallel O

- B-DAT

- B-DAT
tion. O
The O
architecture O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els O
for O
3 O
different O
scales O

- B-DAT

- B-DAT

- B-DAT
hibits O
comparable O
performance O
as O
the O

- B-DAT

- B-DAT

- B-DAT

Residual O
scaling O
- B-DAT
- O
0.1 O

- B-DAT

- B-DAT
ual O
blocks O
are O
lighter O
than O

- B-DAT

- B-DAT
specific O
EDSRs. O
The O
detailed O
performance O

- B-DAT

- B-DAT
formances O
on O
the O
validation O
dataset O

- B-DAT

- B-DAT
ing O
the O
mean O
RGB O
value O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works O
as O
described O
in O
Sec O

- B-DAT
trained O
network O
for O
other O
scales O

- B-DAT

- B-DAT
specific O
residual O
blocks O
and O
upsampling O

- B-DAT
spond O
to O
different O
scales O
other O

- B-DAT
imizing O
L2 O
is O
generally O
preferred O

- B-DAT
pirically O
found O
that O
L1 O
loss O

- B-DAT
spectively. O
The O
source O
code O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
super-resolved O
images O

- B-DAT

- B-DAT

- B-DAT
rate O
models. O
It O
is O
beneficial O

- B-DAT

- B-DAT
dividually O
trained O
models. O
We O
denote O

- B-DAT

- B-DAT

- B-DAT
inal O
one O
trained O
with O
L2 O

- B-DAT

- B-DAT
els O
require O
much O
less O
GPU O

- B-DAT
sults O
in O
an O
individual O
experiment O

- B-DAT
per O
[14]. O
In O
our O
experiments O

- B-DAT

0.9542 O
37.53 O
/ O
0.9587 O
- B-DAT
/ O
- O
38.11 O
/ O
0.9601 O

0.9090 O
33.66 O
/ O
0.9213 O
- B-DAT
/ O
- O
34.65 O
/ O
0.9282 O

0.9063 O
33.03 O
/ O
0.9124 O
- B-DAT
/ O
- O
33.92 O
/ O
0.9195 O

0.8209 O
29.77 O
/ O
0.8314 O
- B-DAT
/ O
- O
30.52 O
/ O
0.8462 O

0.8879 O
31.90 O
/ O
0.8960 O
- B-DAT
/ O
- O
32.32 O
/ O
0.9013 O

0.7863 O
28.82 O
/ O
0.7976 O
- B-DAT
/ O
- O
29.25 O
/ O
0.8093 O

0.8946 O
30.76 O
/ O
0.9140 O
- B-DAT
/ O
- O
32.93 O
/ O
0.9351 O

0.7989 O
27.14 O
/ O
0.8279 O
- B-DAT
/ O
- O
28.80 O
/ O
0.8653 O

0.9581 O
33.66 O
/ O
0.9625 O
- B-DAT
/ O
- O
35.03 O
/ O
0.9695 O

0.9138 O
30.09 O
/ O
0.9208 O
- B-DAT
/ O
- O
31.26 O
/ O
0.9340 O

0.8753 O
28.17 O
/ O
0.8841 O
- B-DAT
/ O
- O
29.25 O
/ O
0.9017 O

- B-DAT

- B-DAT
vided O
in O
the O
last O
two O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sure O
PSNR O
and O
SSIM O
on O

- B-DAT
LAB O
[18] O
functions O
for O
evaluation O

- B-DAT
nificant O
improvement O
compared O
to O
the O

- B-DAT

- B-DAT

- B-DAT
puts O
compared O
with O
the O
previous O

- B-DAT
ticipating O
in O
the O
NTIRE2017 O
Super-Resolution O

- B-DAT
resolution O
system O
with O
the O
highest O

- B-DAT
graders O
(bicubic, O
unknown) O
with O
three O

- B-DAT
ditions. O
Some O
results O
of O
our O

- B-DAT
ods O
successfully O
reconstruct O
high-resolution O
images O

- B-DAT

- B-DAT
ventional O
ResNet O
architecture, O
we O
achieve O

- B-DAT
ual O
scaling O
techniques O
to O
stably O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT

- B-DAT

- B-DAT
age O
Processing, O
21(7):3194–3205, O
2012. O
2 O

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT

- B-DAT
mization. O
In O
ICLR O
2014. O
5 O

- B-DAT

- B-DAT

- B-DAT
ative O
adversarial O
network. O
arXiv:1609.04802, O
2016 O

- B-DAT

- B-DAT
ing O
very O
deep O
convolutional O
encoder-decoder O

- B-DAT
cal O
statistics. O
In O
ICCV O
2001 O

- B-DAT

- B-DAT

- B-DAT
tional O
networks O
for O
biomedical O
image O

- B-DAT
CAI O
2015. O
2 O

- B-DAT
tion O
by O
locally O
linear O
embedding O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT
v4, O
inception-resnet O
and O
the O
impact O

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
actions O
on O
Image O
Processing, O
21(8):3467–3478 O

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT

- B-DAT

- B-DAT
tional O
Conference O
on O
Curves O
and O

- B-DAT

- B-DAT
gorithm O
via O
directional O
filtering O
and O

- B-DAT
actions O
on O
Image O
Processing, O
15(8):2226–2238 O

2) O
33.03/0.9124 O
33.08/0.9126 O
33.09/0.9129 O
34.04/0.9205 O
BSD100 B-DAT
(2) O
31.90/0.8960 O
31.90/0.8956 O
31.92/0.8965 O
32.26/0.9006 O

BSD100 B-DAT
(3) O
28.82/0.7976 O
28.84/0.7976 O
28.86/0.7987 O
29.18/0.8071 O

BSD100 B-DAT
(4) O
27.29/0.7251 O
27.29/0.7247 O
27.32/0.7266 O
27.66/0.7380 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ous O
image O
restoration O
tasks. O
However O

- B-DAT
pose O
a O
novel O
feature O
space O

- B-DAT
rithm O
that O
outperforms O
the O
existing O

- B-DAT
mance O
of O
a O
learning O
algorithm O

- B-DAT

- B-DAT

- B-DAT
strate O
that O
the O
proposed O
feature O

- B-DAT
performs O
the O
existing O
state-of-the-art O
approaches O

- B-DAT
over, O
our O
algorithm O
was O
ranked O

-10 B-DAT
times O
faster O
computational O
time O
compared O

- B-DAT
cessing O
applications. O
Over O
the O
last O

- B-DAT

- B-DAT

- B-DAT
proaches O
[22], O
and O
sparse O
dictionary O

- B-DAT

- B-DAT

- B-DAT
over, O
these O
algorithms O
are O
usually O

- B-DAT
ative O
manner, O
so O
they O
require O

- B-DAT
sources O

- B-DAT
level O
computer O
vision O
problems O
[24 O

- B-DAT
ing O
and O
super-resolution O
tasks, O
many O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
itation O
of O
the O
state-of-the-art O
CNN O

- B-DAT
posed O
network O
architectures O
are O
motivated O

- B-DAT
sistent O
homology O
analysis O
[11] O
on O

- B-DAT
age O
processing O
tasks. O
Specifically, O
we O

- B-DAT
ual O
manifold O
is O
topologically O
simpler O

- B-DAT
age O
manifold, O
which O
may O
have O

- B-DAT
tion. O
Specifically, O
our O
design O
goal O

- B-DAT
tures O
while O
preserving O
the O
directional O

- B-DAT
lowing. O
First, O
a O
novel O
network O

- B-DAT
fold O
simplification O
is O
proposed. O
Second O

- B-DAT
putational O
topology O
tool O
called O
the O

- B-DAT
form O
to O
simplify O
topological O
structures O

COPY O
ch(LR) O
Label O
Input O
- B-DAT
WT(HR) O
Input O
- O
PS(HR O

Long O
bypass O
layer O
LongBypass(2) O
- B-DAT
- O
Repeat O
1st O
module O
6 O
times O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vanced O
algorithms O
in O
this O
field O

- B-DAT
and O
inter- O
correlations O
of O
the O

- B-DAT
mance O
to O
BM3D O
using O
multi-layer O

- B-DAT
able O
nonlinear O
reaction O
diffusion O
(TNRD O

- B-DAT
ters O
and O
influence O
functions O
by O

- B-DAT
timization O
approach. O
Recently, O
based O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
restoration O
problems O
[21 O

- B-DAT
proach O
is O
using O
a O
skipped O

- B-DAT
cept O
was O
first O
introduced O
by O

- B-DAT
tion. O
In O
low-level O
computer O
vision O

- B-DAT

- B-DAT
ping. O
In O
another O
implementation, O
the O

- B-DAT
formed O
into O
the O
difference O
between O

- B-DAT
lem: O
minf∈F O
L(f), O
where O
L(f O

- B-DAT
notes O
the O
risk. O
A O
major O

- B-DAT
formance. O
Specifically, O
with O
probability O

- B-DAT
pared O
to O
shallow O
ones. O
However O

- B-DAT

- B-DAT
ity O
penalty O
reduces O
much O
more O

- B-DAT
mization. O
One O
of O
the O
most O

- B-DAT
work, O
by O
reducing O
the O
complexity O

- B-DAT
ture O
spaces O
for O
the O
input O

- B-DAT
form O
is O
given O
by O
Y O

- B-DAT
sults O
in O
the O
dimensional O
reduction O

- B-DAT
tion. O
Indeed, O
this O
property O
of O

- B-DAT

- B-DAT
gebraic O
topology, O
Betti O
numbers O
(βm O

- B-DAT
ber O
of O
m-dimensional O
holes O
of O

- B-DAT
ing O
the O
changes O
of O
Betti O

- B-DAT
come O
a O
single O
cluster O
(Fig O

- B-DAT
resented O
as O
a O
slow O
decrease O

- B-DAT

- B-DAT
uration O
over O
� O
distance O
filtration O

- B-DAT
ture O
and O
the O
recent O
persistent O

- B-DAT

- B-DAT

- B-DAT
lationship O
between O
these O
newly O
processed O

- B-DAT
sian O
denoising, O
40 O
× O
40 O

- B-DAT
ing O
because O
it O
is O
helpful O

- B-DAT
leviating O
the O
gradient O
vanishing O
problem O

- B-DAT
malization, O
and O
ReLU O
and O
the O

- B-DAT
ing O
the O
convolution, O
we O
used O

- B-DAT
ceptive O
field O
can O
be O
reduced O

- B-DAT
mary O
denoising O
architecture. O
Depending O
on O

- B-DAT
tended O
denoising O
network O
structures O
with O

- B-DAT
form O
were O
used O
for O
manifold O

- B-DAT
known O
decimation O
scheme, O
however, O
we O

- B-DAT
pixel O
shuffling O
scheme O
[25] O
as O

- B-DAT

- B-DAT
fling O
transform O
does O
not O
reduce O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
task. O
The O
training O
step O

- B-DAT
ter O
the O
first O
two O
layers O

- B-DAT
construct O
the O
bicubic O
x2 O
downsampled O

- B-DAT
nection O
allows O
faster O
computation O
and O

- B-DAT
pass O
connection O

- B-DAT
minance O
channel, O
because O
RGB O
based O

- B-DAT
fect O
of O
data O
augmentation O

- B-DAT
able O
Berkeley O
segmentation O
(BSD500) O
[4 O

- B-DAT
ing O
task. O
In O
addition, O
we O

- B-DAT
fitting, O
we O
re-generated O
the O
Gaussian O

- B-DAT
ing O
and O
validation, O
Gaussian O
noises O

- B-DAT
tion O
using O
image O
flipping, O
rotation O

- B-DAT
tialized O
using O
the O
Xavier O
method O

- B-DAT
sion O
loss O
across O
four O
wavelet O

- B-DAT

- B-DAT
ble O
learning, O
we O
employed O
the O

- B-DAT
date O
parameter O
are O
bounded O
by O

- B-DAT
box O
(beta.20) O
[29] O
in O
MATLAB O

- B-DAT
Works, O
Natick). O
We O
used O
a O

-4770 B-DAT
CPU O
(3.40GHz). O
The O
Gaussian O
denoising O

- B-DAT
work O
took O
about O
two O
days O

- B-DAT

- B-DAT
epoch O
system O
that O
repeats O
forward O

- B-DAT
work O
with O
the O
bicubic O
x2 O

- B-DAT

- B-DAT

- B-DAT
mance O
and O
manifold O
simplification, O
we O

- B-DAT
ogy O
of O
the O
input O
and O

- B-DAT

- B-DAT
noising O
performance, O
we O
used O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
noising O
methods O
in O
terms O
of O

- B-DAT
bara O
and O
House, O
we O
attained O

- B-DAT
posed O
method O
showed O
superior O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
composition, O
additional O
comparative O
studies O
with O

- B-DAT
line O
network O
were O
performed. O
Here O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fling O
scheme O
in O
[25] O
except O

- B-DAT
polated. O
Accordingly, O
the O
networks O
using O

- B-DAT
actly O
same O
architecture O
except O
the O

- B-DAT
plification. O
Here, O
the O
Gaussian O
denoising O

- B-DAT
bara O
image O
was O
used O
for O

- B-DAT

-3 B-DAT
Proposed-P O
Proposed O
Set5 O
(2) O
37.53/0.9586 O

-3, B-DAT
Proposed-P(primary) O
networks O
are O
291 O
dataset[18 O

- B-DAT

- B-DAT
stored O
RGB O
was O
used O
to O

- B-DAT
ues O

- B-DAT
lar, O
our O
networks O
were O
competitive O

-67 B-DAT
seconds O
computational O
time O
by O
the O

-5 B-DAT
seconds O
for O
each O
frame. O
Since O

- B-DAT

- B-DAT
known O
decimation O
dataset O
where O
we O

- B-DAT
ifold O
simplification O
from O
the O
residual O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eas. O
We O
provide O
more O
comparative O

- B-DAT
mentary O
material O

- B-DAT
ogy O
analysis, O
we O
showed O
that O

- B-DAT
ing O
as O
well O
as O
the O

- B-DAT
ing O
and O
NTIRE O
SISR O
competition O

- B-DAT
mance O
and O
speed. O
Moreover, O
we O

- B-DAT

- B-DAT
ing O
Foundation, O
Grant O
number O
NRF-2013M3A9B2076548 O

- B-DAT
puter O
Vision, O
76(2):123–139, O
2008. O
1 O

- B-DAT
sion O
and O
Pattern O
Recognition O
(CVPR O

- B-DAT
mized O
reaction O
diffusion O
processes O
for O

- B-DAT

- B-DAT
den O
Markov O
models. O
IEEE O
Transactions O

- B-DAT

- B-DAT

- B-DAT
laborative O
filtering. O
IEEE O
Transactions O
on O

- B-DAT
cessing, O
16(8):2080–2095, O
2007. O
1 O

- B-DAT
ume O
61. O
SIAM, O
1992. O
3 O

- B-DAT
tralized O
sparse O
representation O
for O
image O

- B-DAT

- B-DAT

- B-DAT
a O
survey. O
Contemporary O
Mathematics, O
453:257–282 O

- B-DAT
ual O
learning O
for O
image O
recognition O

- B-DAT

- B-DAT
tional O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT

- B-DAT
tern O
Recognition O
(CVPR), O
pages O
5197–5206 O

- B-DAT
erating O
deep O
network O
training O
by O

- B-DAT

- B-DAT
works. O
arXiv O
preprint O
arXiv:1511.04587, O
2015 O

- B-DAT
agenet O
classification O
with O
deep O
convolutional O

- B-DAT
ing O
Systems, O
pages O
1097–1105, O
2012 O

- B-DAT
demic O
press, O
1999. O
3 O

- B-DAT

- B-DAT
based O
image O
restoration. O
Multiscale O
Modeling O

- B-DAT
ulation, O
4(2):460–489, O
2005. O
1 O

- B-DAT
moncelli. O
Image O
denoising O
using O
scale O

- B-DAT

- B-DAT
volutional O
networks O
for O
biomedical O
image O

- B-DAT
tion. O
In O
International O
Conference O
on O

- B-DAT
age O
Computing O
and O
Computer-Assisted O
Intervention O

- B-DAT

- B-DAT
gle O
image O
and O
video O
super-resolution O

- B-DAT

- B-DAT
tion O
(CVPR), O
pages O
1874–1883, O
2016 O

- B-DAT
ory, O
volume O
1. O
Wiley O
New O

- B-DAT

-1995 B-DAT

-001 B-DAT

- B-DAT
Hall, O
1995. O
7 O

- B-DAT
celli. O
Image O
quality O
assessment: O
from O

- B-DAT

- B-DAT
ror O
as O
the O
loss, O
the O

- B-DAT

- B-DAT
variant O
under O
batch O
normalization O
is O

- B-DAT

- B-DAT
culated O
the O
barcodes O
of O
the O

- B-DAT
box O
called O
JAVAPLEX O
(http://appliedtopology.github.io/ O
javaplex O

- B-DAT
posed O
of O
residual O
image O
patches O

- B-DAT
ical O
complexity O
in O
the O
image O

- B-DAT
thogonal O
Haar O
wavelet O
transform. O
The O

- B-DAT
form O
which O
further O
reduces O
the O

- B-DAT
resolution O
datasets O
(the O
right O
column O

- B-DAT
codes, O
the O
input O
manifold O
of O

- B-DAT
ages O
had O
simpler O
topology O
than O

- B-DAT
noising O
, O
(b) O
super-resolution O
(bicubic O

- B-DAT
resolution O
(unknown O
decimation) O
tasks. O
(Right O

- B-DAT
ogy O
analysis O
of O
input O
manifold O

- B-DAT

- B-DAT

- B-DAT
tion) O
tasks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ditional O
simpler O
label O
manifold O
from O

- B-DAT
noising O
and O
SISR O
reconstruction O
from O

4 O
on O
datasets O
Set5, O
Set14, O
BSD100 B-DAT

results O
of O
“barbara” O
(Set14) O
with O
upscaling B-DAT
factor O
×4 O

results O
of O
“barbara” O
(Set14) O
with O
upscaling B-DAT
factor O
×4 O

results O
of O
“barbara” O
(Set14) O
with O
upscaling B-DAT
factor O
×4 O

results O
of O
“barbara” O
(Set14) O
with O
upscaling B-DAT
factor O
×4 O

i.e., O
Set5 O
[7], O
Set14 O
[56], O
BSD100 B-DAT
[38], O
and O
Urban100 O
[23], O
because O

4 O
on O
datasets O
Set5, O
Set14, O
BSD100 B-DAT
and O
Urban100. O
Red O
color O
indicates O

BSD100 B-DAT
×2 O
- O
31.90 O
/ O
0.8960 O

PSNR O
values O
on O
Set5 O
and O
BSD100, B-DAT
and O
is O
comparable O
to O
SR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works O
(CNNs) O
generally O
enlarge O
the O

- B-DAT

- B-DAT

- B-DAT
thermore, O
another O
convolutional O
layer O
is O

- B-DAT
crease O
the O
channels O
of O
feature O

- B-DAT
network, O
inverse O
wavelet O
transform O
is O

- B-DAT
construct O
the O
high O
resolution O
feature O

- B-DAT
tering O
and O
subsampling, O
and O
can O

- B-DAT

- B-DAT

- B-DAT
tion O
from O
both O
prior O
modeling O

- B-DAT
tional O
neural O
networks O
(CNNs) O
have O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eral O
representative O
image O
restoration O
tasks O

- B-DAT

- B-DAT
ing O
[57], O
image O
deblurring O
[58 O

-3 B-DAT
10-2 O
10-1 O
100 O
101 O

- B-DAT
PCN O
[45], O
VDSR O
[29], O
DnCNN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
more O
complex O
image O
restoration O

- B-DAT
ping O
from O
degraded O
observation O
to O

- B-DAT
tional O
network O
(FCN) O
by O
removing O

- B-DAT
formance O
by O
taking O
more O
spatial O

- B-DAT
ever, O
for O
FCN O
without O
pooling O

- B-DAT
ing O
filters O
with O
larger O
size O

- B-DAT
ently O
suffers O
from O
gridding O
effect O

- B-DAT
large O
receptive O
field O
while O
avoiding O

- B-DAT
tational O
burden O
and O
the O
potential O

- B-DAT
trates O
the O
receptive O
field, O
run O

- B-DAT
RCNN O
[14] O
has O
relatively O
larger O

- B-DAT

- B-DAT
off O
between O
performance O
and O
efficiency O

- B-DAT

- B-DAT
tracting O
subnetwork O
and O
an O
expanding O

- B-DAT
ture O
maps O
[12, O
13], O
which O

- B-DAT
tailed O
texture. O
In O
the O
expanding O

- B-DAT
wise O
summation O
is O
adopted O
for O

- B-DAT
over, O
dilated O
filtering O
can O
also O

- B-DAT
larging O
receptive O
field. O
Experiments O
on O

- B-DAT
tiveness O
and O
efficiency O
of O
our O

- B-DAT
ure O
1, O
MWCNN O
is O
moderately O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ically, O
more O
discussions O
are O
given O

- B-DAT
ing O
[25]. O
These O
early O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cently, O
multi-layer O
perception O
(MLP) O
has O

- B-DAT
corporating O
residual O
learning O
with O
batch O

- B-DAT
ditional O
non-CNN O
based O
methods. O
Mao O

- B-DAT
gest O
to O
add O
symmetric O
skip O

- B-DAT
proving O
denoising O
performance. O
For O
better O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
CNN O
[16], O
which O
adopts O
a O

- B-DAT

- B-DAT
work O
[29], O
residual O
units O
[32 O

- B-DAT
tional O
cost O
or O
loss O
of O

- B-DAT

- B-DAT
tween O
receptive O
field O
size O
and O

- B-DAT
erative O
adversarial O
networks O
(GANs) O
have O

- B-DAT
duced O
to O
improve O
the O
visual O

- B-DAT
fers O
from O
blocking O
effect O
and O

- B-DAT

- B-DAT

- B-DAT
ing. O
For O
example, O
both O
DnCNN O

- B-DAT
noisers O
can O
also O
serve O
as O

- B-DAT

- B-DAT

- B-DAT
ers O
[58]. O
Romano O
et O
al O

- B-DAT
ing O
CNN O
on O
wavelet O
subbands O

- B-DAT

- B-DAT
lutional O
framelets O
[21, O
54] O
have O

- B-DAT

- B-DAT
composition. O
Deep O
convolutional O
framelets O
independently O

- B-DAT

- B-DAT
form, O
our O
MWCNN O
can O
embed O

- B-DAT
text O
and O
inter-subband O
dependency O

- B-DAT

- B-DAT

- B-DAT
chitecture. O
Finally, O
discussion O
is O
given O

- B-DAT
nection O
of O
MWCNN O
with O
dilated O

- B-DAT

- B-DAT
age O
x O
[36]. O
The O
convolution O

- B-DAT
nal O
property O
of O
DWT, O
the O

- B-DAT

- B-DAT
cessed O
with O
DWT O
to O
produce O

- B-DAT

- B-DAT
sition O
and O
reconstruction O
of O
an O

- B-DAT
ers. O
In O
the O
decomposition O
stage O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
construction O
result O
at O
the O
current O

- B-DAT

- B-DAT
quired O
to O
process O
the O
decomposition O

- B-DAT
lored O
to O
specific O
task. O
In O

- B-DAT

- B-DAT

- B-DAT
eralization O
of O
multi-level O
WPT, O
and O

- B-DAT
sampling O
operations O
safely O
without O
information O

- B-DAT
over, O
compared O
with O
conventional O
CNN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ations. O
As O
to O
the O
last O

- B-DAT
ing O
subnetwork. O
Generally, O
MWCNN O
modifies O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sponds O
to O
a O
multi-channel O
feature O

- B-DAT

- B-DAT
Net[41], O
while O
DWT O
and O
IWT O

- B-DAT
Net, O
the O
downsampling O
has O
no O

- B-DAT
nels, O
and O
the O
subsequent O
convolution O

- B-DAT
crease O
feature O
map O
channels. O
(iii O

- B-DAT
wise O
summation O
is O
used O
to O

- B-DAT
ventional O
U-Net O
concatenation O
is O
adopted O

- B-DAT
tion, O
Haar O
wavelet O
is O
adopted O

- B-DAT
ered O
in O
our O
experiments O

- B-DAT

- B-DAT
responding O
ground-truth O
image. O
The O
objective O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
convolution O
in O
MWCNN, O
respectively. O
When O

- B-DAT
bands O
are O
taken O
into O
account O

- B-DAT
mation O
loss O
caused O
by O
conventional O

- B-DAT
lated O
filtering O
with O
factor O
2 O

- B-DAT
fined O
analogously. O
We O
also O
have O

- B-DAT
lated O
filtering O
and O
MWCNN O
for O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
field O
of O
MWCNN. O
One O

- B-DAT
mance O
quantitatively O
and O
qualitatively O

- B-DAT
sion O
artifacts O
removal. O
Comparison O
of O

- B-DAT

- B-DAT
lowing O
[57], O
we O
consider O
three O

- B-DAT
ing O
four O
compression O
quality O
settings O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tel(R) O
Core(TM) O
i7-5820K O
CPU O
3.30GHz O

- B-DAT
ing O
methods O
are O
only O
tested O

- B-DAT
ban100 O
[23]. O
Table O
1 O
lists O

- B-DAT
ure O
5 O
shows O
the O
denoising O

- B-DAT

0.9027 O
32.77 O
/ O
0.9008 O
- B-DAT
- O
33.15 O
/ O
0.9088 O
25 O
29.97 O

0.8618 O
30.38 O
/ O
0.8601 O
- B-DAT
- O
30.79 O
/ O
0.8711 O
50 O
26.72 O

0.8906 O
31.63 O
/ O
0.8881 O
- B-DAT
- O
31.86 O
/ O
0.8947 O
25 O
28.57 O

0.8278 O
29.15 O
/ O
0.8249 O
- B-DAT
- O
29.41 O
/ O
0.8360 O
50 O
25.62 O

0.9250 O
32.49 O
/ O
0.9244 O
- B-DAT
- O
33.17 O
/ O
0.9357 O
25 O
29.70 O

0.8792 O
29.82 O
/ O
0.8839 O
- B-DAT
- O
30.66 O
/ O
0.9026 O
50 O
25.94 O

0.9593 O
37.66 O
/ O
0.9599 O
- B-DAT
37.52 O
/ O
0.9590 O
37.74 O

0.9222 O
33.82 O
/ O
0.9230 O
- B-DAT
- O
34.03 O
/ O
0.9244 O
34.09 O

0.9118 O
32.94 O
/ O
0.9144 O
- B-DAT
33.08 O
/ O
0.9130 O
33.23 O

0.8349 O
29.61 O
/ O
0.8341 O
- B-DAT
- O
29.96 O
/ O
0.8349 O
30.00 O

BSD100 O
×2 O
- B-DAT
31.90 O
/ O
0.8960 O
31.85 O

0.8942 O
31.98 O
/ O
0.8974 O
- B-DAT
31.80 O
/ O
0.8950 O
32.05 O

0.8995 O
32.23 O
/ O
0.8999 O
×3 O
- B-DAT
28.82 O
/ O
0.7976 O
28.80 O

0.7963 O
28.92 O
/ O
0.7993 O
- B-DAT
- O
28.95 O
/ O
0.8004 O
28.96 O

0.7987 O
29.12 O
/ O
0.8060 O
×4 O
- B-DAT
27.29 O
/ O
0.7251 O
27.23 O

Urban100 O
×2 O
- B-DAT
30.76 O
/ O
0.9140 O
30.75 O

0.9133 O
30.91 O
/ O
0.9159 O
- B-DAT
30.41 O
/ O
0.9100 O
31.23 O

0.9169 O
32.30 O
/ O
0.9296 O
×3 O
- B-DAT
27.14 O
/ O
0.8279 O
27.15 O

0.8276 O
27.31 O
/ O
0.8303 O
- B-DAT
- O
27.53 O
/ O
0.8378 O
27.56 O

0.8334 O
28.13 O
/ O
0.8514 O
×4 O
- B-DAT
25.18 O
/ O
0.7524 O
25.20 O

0.8837 O
32.91 O
/ O
0.8861 O
- B-DAT
33.43 O
/ O
0.8930 O
40 O
32.43 O

0.8911 O
33.34 O
/ O
0.8953 O
- B-DAT
33.77 O
/ O
0.9003 O
- O
34.27 O

0.9059 O
32.98 O
/ O
0.9090 O
- B-DAT
33.45 O
/ O
0.9153 O
40 O
32.35 O

0.9173 O
33.63 O
/ O
0.9198 O
- B-DAT
33.96 O
/ O
0.9247 O
- O
34.45 O

- B-DAT

- B-DAT
peting O
methods O
on O
the O
four O

- B-DAT
forms O
favorably O
in O
terms O
of O

- B-DAT
dexes. O
Compared O
with O
VDSR, O
our O

- B-DAT
perform O
VDSR, O
and O
also O
is O

- B-DAT
ResNet O
on O
Set14. O
Figure O
6 O

-1 B-DAT
method O
by O
0.37dB O

- B-DAT
overlapped O
8 O
× O
8 O
blocks O

- B-DAT
troducing O
the O
blocking O
artifact. O
The O

- B-DAT
mined O
by O
a O
quality O
factorQ O

- B-DAT
sider O
[18, O
19] O
due O
to O

- B-DAT
peting O
methods O
on O
Classic5 O
and O

- B-DAT
Net O
[48]) O
for O
the O
quality O

- B-DAT

- B-DAT

- B-DAT
ing O
library O
is O
adopted O
to O

- B-DAT
based O
methods O
with O
source O
codes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
s, O
MWCNN O
is O
moderately O
slower O

- B-DAT
stead O
of O
the O
increase O
of O

- B-DAT
ness O
of O
MWCNN O
should O
be O

- B-DAT
amples, O
we O
compare O
the O
PSNR O

- B-DAT

- B-DAT
fault O
MWCNN O
with O
Haar O
wavelet O

-2 B-DAT
wavelet, O
and O
(iii) O
MWCN- O
N O

-2 B-DAT
in O
expanding O
subnetwork. O
Then, O
abla O

- B-DAT
tion O
experiments O
are O
provided O
for O

- B-DAT
ness O
of O
additionally O
embedded O
wavelet O

- B-DAT
Net O
with O
same O
architecture O
to O

- B-DAT

- B-DAT
ing O
sum O
connection O
instead O
of O

- B-DAT
Net+D: O
adopting O
learnable O
conventional O
downsamping O

- B-DAT

- B-DAT

- B-DAT
ing O
library O
is O
adopted O
to O

- B-DAT
based O
methods O
with O
source O
codes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
s, O
MWCNN O
is O
moderately O
slower O

- B-DAT
stead O
of O
the O
increase O
of O

- B-DAT
ness O
of O
MWCNN O
should O
be O

- B-DAT
amples, O
we O
compare O
the O
PSNR O

- B-DAT

- B-DAT
fault O
MWCNN O
with O
Haar O
wavelet O

-2 B-DAT
wavelet, O
and O
(iii) O
MWCN- O
N O

-2 B-DAT
in O
expanding O
subnetwork. O
Then, O
abla O

- B-DAT
tion O
experiments O
are O
provided O
for O

- B-DAT
ness O
of O
additionally O
embedded O
wavelet O

- B-DAT
Net O
with O
same O
architecture O
to O

- B-DAT

- B-DAT
ing O
sum O
connection O
instead O
of O

- B-DAT
Net+D: O
adopting O
learnable O
conventional O
downsamping O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
library O
is O
adopted O
to O

- B-DAT
based O
methods O
with O
source O
codes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
s, O
MWCNN O
is O
moderately O
slower O

- B-DAT
stead O
of O
the O
increase O
of O

- B-DAT
ness O
of O
MWCNN O
should O
be O

- B-DAT
amples, O
we O
compare O
the O
PSNR O

- B-DAT

- B-DAT
fault O
MWCNN O
with O
Haar O
wavelet O

-2 B-DAT
wavelet, O
and O
(iii) O
MWCN- O
N O

-2 B-DAT
in O
expanding O
subnetwork. O
Then, O
abla O

- B-DAT
tion O
experiments O
are O
provided O
for O

- B-DAT
ness O
of O
additionally O
embedded O
wavelet O

- B-DAT
Net O
with O
same O
architecture O
to O

- B-DAT

- B-DAT
ing O
sum O
connection O
instead O
of O

- B-DAT
Net+D: O
adopting O
learnable O
conventional O
downsamping O

- B-DAT

- B-DAT
ing O
library O
is O
adopted O
to O

- B-DAT
based O
methods O
with O
source O
codes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fectiveness O
of O
MWCNN O
should O
be O

- B-DAT
ration O
of O
CNN O
and O
DWT O

- B-DAT

- B-DAT
fault O
MWCNN O
with O
Haar O
wavelet O

-2 B-DAT
wavelet, O
and O
(iii) O
MWCNN O
(HD O

-2 B-DAT
in O
expanding O
subnetwork. O
Then, O
ablation O

- B-DAT
periments O
are O
provided O
for O
verifying O

- B-DAT
ditionally O
embedded O
wavelet: O
(i) O
the O

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT
U-Net O
[41] O
U-Net+S O
U-Net+D O
DCF O

0.355 O
26.65 O
/ O
0.354 O
- B-DAT
/ O
- O
27.42 O
/ O
0.343 O

0.097 O
29.57 O
/ O
0.104 O
- B-DAT
/ O
- O
30.01 O
/ O
0.088 O

0.120 O
29.38 O
/ O
0.155 O
- B-DAT
/ O
- O
29.69 O
/ O
0.112 O

- B-DAT

-2 B-DAT

- B-DAT

- B-DAT
dicate O
that O
using O
sum O
connection O

- B-DAT
frequency O
localization O
property O
in O
wavelet O

- B-DAT
dent O
processing O
of O
subbands O
harms O

- B-DAT
pared O
to O
MWCNN O
(DB2) O
and O

- B-DAT
uation. O
MWCNN O
(Haar) O
has O
similar O

- B-DAT

- B-DAT
tween O
performance O
and O
efficiency O

- B-DAT
position, O
where O
different O
CNNs O
are O

- B-DAT
band. O
However, O
the O
results O
in O

- B-DAT
pendent O
processing O
of O
subbands O
is O

- B-DAT

- B-DAT
ier O
computational O
burden. O
Thus, O
a O

- B-DAT
NNs O
with O
different O
levels O
on O

-1 B-DAT
MWCNN-2 O
MWCNN-3 O
MWCNN-4 O

-1 B-DAT
∼MWCNN-4). O
It O
can O
be O
observed O

-3 B-DAT
with O
24-layer O
architecture O
performs O
much O

-1 B-DAT
and O
MWCNN-2, O
while O
MWCNN-4 O
only O

-3 B-DAT
in O
terms O
of O
the O
PSNR O

-3 B-DAT
is O
also O
moderate O
compared O
with O

-3 B-DAT
as O
the O
default O
setting O

- B-DAT

- B-DAT

- B-DAT
fectiveness O
and O
efficiency O
of O
MWCNN O

- B-DAT
eral O
restoration O
tasks O
such O
as O

- B-DAT

- B-DAT

- B-DAT
gle O
image O
super-resolution: O
Dataset O
and O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
shops O
(CVPRW), O
pages O
1122–1131. O
IEEE O

- B-DAT
demic O
Press, O
2001 O

- B-DAT
ing O
for O
image O
restoration: O
Persistent O

- B-DAT

- B-DAT
ifold O
simplification. O
In O
IEEE O
Conference O

- B-DAT
tion. O
IEEE O
Signal O
Processing O
Magazine O

- B-DAT
Morel. O
Low-complexity O
single-image O
super-resolution O
based O

- B-DAT
noising: O
Can O
plain O
neural O
networks O

- B-DAT
tion, O
pages O
2392–2399, O
2012 O

- B-DAT
olding O
for O
image O
denoising O
and O

- B-DAT
tions O
on O
Image O
Processing, O
9(9):1532–1546 O

- B-DAT
tion. O
IEEE O
Transactions O
on O
Pattern O

- B-DAT
age O
denoising O
by O
sparse O
3-d O

- B-DAT

- B-DAT
tive O
filtering. O
IEEE O
Transactions O
on O

- B-DAT

- B-DAT
ization O
and O
signal O
analysis. O
IEEE O

- B-DAT
tion O
Theory, O
36(5):961–1005, O
1990 O

- B-DAT
ference O
on O
Computer O
Vision, O
pages O

- B-DAT
sion O
artifacts O
reduction O
by O
a O

- B-DAT

- B-DAT
tion, O
pages O
2862–2869, O
2014 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
shops O
(CVPRW), O
2017 O

- B-DAT

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
lutional O
networks. O
In O
Advances O
in O

- B-DAT
cessing O
Systems, O
pages O
769–776, O
2009 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lishing O
Company, O
Incorporated, O
2012 O

- B-DAT

- B-DAT
volutional O
network O
for O
image O
super-resolution O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT
timization. O
In O
International O
Conference O
for O

- B-DAT
sentations, O
2015 O

- B-DAT
resolution. O
IEEE O
Conference O
on O
Computer O

- B-DAT
tern O
Recognition, O
2017 O

- B-DAT

- B-DAT

- B-DAT
ative O
adversarial O
network. O
IEEE O
Conference O

- B-DAT

- B-DAT
cessing, O
1(2):244–250, O
1992 O

- B-DAT
ing O
convolutional O
networks O
for O
content-weighted O

- B-DAT
pression. O
IEEE O
Conference O
on O
Computer O

- B-DAT
position: O
the O
wavelet O
representation. O
IEEE O

- B-DAT

- B-DAT
ric O
skip O
connections. O
In O
Advances O

- B-DAT
cessing O
Systems, O
pages O
2802–2810, O
2016 O

- B-DAT
cal O
statistics. O
In O
IEEE O
Conference O

- B-DAT
ence O
Computer O
Vision, O
volume O
2 O

- B-DAT
ing O
for O
image O
quality O
assessment O

- B-DAT

- B-DAT
tional O
networks O
for O
biomedical O
image O

- B-DAT
ternational O
Conference O
on O
Medical O
Image O

- B-DAT

- B-DAT

- B-DAT
ized O
deep O
image O
to O
image O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
preserving O
image O
super-resolution O
via O
contextualized O

- B-DAT
task O
learning. O
IEEE O
Transactions O
on O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
national O
conference O
on O
Multimedia, O
pages O

- B-DAT
mentation. O
arXiv O
preprint O
arXiv:1702.08502, O
2017 O

- B-DAT

- B-DAT

- B-DAT
ing O
with O
deep O
neural O
networks O

- B-DAT
dustrial O
and O
Applied O
Mathematics, O
2018 O

- B-DAT

- B-DAT
lated O
convolutions. O
arXiv O
preprint O
arXiv:1511.07122 O

- B-DAT

- B-DAT

- B-DAT
yond O
a O
gaussian O
denoiser: O
Residual O

subset O
of O
10 O
images O
from O
BSD100 B-DAT

BSD100 B-DAT

tests O
conducted O
on O
Set5, O
Set14, O
BSD100 B-DAT

on O
performance O
(PSNR O
[dB] O
on O
BSD100 B-DAT

Set5, O
Set14 O
and O
BSD100. O
On O
BSD100 B-DAT

and O
especially O
on O
the O
large O
BSD100 B-DAT

Set5 O
Set14 O
BSD100 B-DAT
Figure I-DAT
9: O
Color-coded O
distribution O
of O

Set5 O
Set14 O
BSD100 B-DAT
Figure I-DAT
10: O
Average O
rank O
on O

Set5, O
Set14, O
BSD100 B-DAT

for O
five O
random O
samples O
of O
BSD100 B-DAT

when O
we O
super-resolve O
at O
large O
upscaling B-DAT
factors? O
The O
behavior O
of O
optimization-based O

photo-realistic O
natural O
images O
for O
4× O
upscaling B-DAT
factors. O
To O
achieve O
this, O
we O

guishable O
from O
original O
(right). O
[4× O
upscaling B-DAT

is O
particularly O
pronounced O
for O
high O
upscaling B-DAT
factors, O
for O
which O
texture O
detail O

are O
shown O
in O
brackets. O
[4× O
upscaling B-DAT

super- O
resolved O
with O
a O
4× O
upscaling B-DAT
factor O
is O
shown O
in O
Figure O

the O
network O
to O
learn O
the O
upscaling B-DAT
filters O
directly O
can O
further O
increase O

was O
also O
shown O
that O
learning O
upscaling B-DAT
filters O
is O
beneficial O
in O
terms O

super-resolves O
face O
images O
with O
large O
upscaling B-DAT
factors O
(8×). O
GANs O
were O
also O

for O
image O
SR O
with O
high O
upscaling B-DAT
factors O
(4×) O
as O
measured O
by O

photo-realistic O
SR O
images O
with O
high O
upscaling B-DAT
factors O
(4 O

losses O
in O
that O
category∗. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

HR O
image O
(right: O
i,j). O
[4× O
upscaling B-DAT

SSIM, O
MOS) O
in O
bold. O
[4× O
upscaling B-DAT

that O
SRGAN O
reconstructions O
for O
large O
upscaling B-DAT
factors O
(4×) O
are, O
by O
a O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
IEEE O

and O
SRGAN O
with O
a O
4× O
upscaling B-DAT
factor O
for O
Set5 O
(Section O
A.4 O

low-/high-resolution O
images O
and O
reconstructions O
(4× O
upscaling) B-DAT
obtained O
with O
different O
methods O
(bicubic O

image O
with O
resolution O
64×64 O
with O
upscaling B-DAT
factor O
4×. O
The O
measurements O
are O

for O
another O
100k O
iterations. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

all O
available O
individual O
ratings. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

Set5 O
[3], O
Set14 O
[69] O
and O
BSD100, B-DAT
the O
testing O
set O
of O
BSD300 O

image O
on O
Set5, O
Set14 O
and O
BSD100 B-DAT

SRResNet-MSE, O
SRResNet-VGG22∗ O
(∗not O
rated O
on O
BSD100), B-DAT
SRGAN-MSE∗, O
SRGAN-VGG22∗, O
SRGAN- O
VGG54 O
and O

subset O
of O
10 O
images O
from O
BSD100 B-DAT
by O
adding O
a O
method’s O
images O

distribution O
of O
MOS O
scores O
on O
BSD100 B-DAT

and O
all O
reference O
methods O
on O
BSD100 B-DAT

2) O
are O
highly O
significant O
on O
BSD100, B-DAT
except O
SRCNN O
vs. O
SelfExSR. O
The O

BSD100 B-DAT
PSNR O
25.02 O
25.94 O
26.68 O
26.83 O

tests O
conducted O
on O
Set5, O
Set14, O
BSD100 B-DAT
are O
summarized O
in O
Section O
A.3 O

five O
randomly O
selected O
images O
from O
BSD100 B-DAT
(Section O
A.6 O

on O
performance O
(PSNR O
[dB] O
on O
BSD100 B-DAT
for O
4× O
SR) O
and O
inference O

depth. O
PSNR O
(left) O
calculated O
on O
BSD100 B-DAT

images O
from O
Set5, O
Set14 O
and O
BSD100 B-DAT

. O
On O
BSD100 B-DAT
nine O
versions O
of O
each O
image O

and O
especially O
on O
the O
large O
BSD100 B-DAT
data O
set O
confirm O
that O
SRGAN O

Set5 O
Set14 O
BSD100 B-DAT

MOS O
scores O
on O
Set5, O
Set14, O
BSD100 B-DAT

Set5 O
Set14 O
BSD100 B-DAT

Average O
rank O
on O
Set5, O
Set14, O
BSD100 B-DAT
by O
averaging O
the O
ranks O
over O

A.6. O
BSD100 B-DAT
(five O
random O
samples) O
- O
Visual O

for O
five O
random O
samples O
of O
BSD100 B-DAT
using O
bicubic O
interpolation, O
SRResNet O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks, O
one O
central O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
(SR). O
To O
our O
knowledge O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
guishable O
from O
original O
(right). O
[4 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
inal O
image O
means O
that O
the O

- B-DAT
realistic O
as O
defined O
by O
Ferwerda O

- B-DAT

- B-DAT

- B-DAT
ing O
high-level O
feature O
maps O
of O

- B-DAT

- B-DAT
resolved O
with O
a O
4× O
upscaling O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
image O
informa- O
tion O

- B-DAT

- B-DAT
proaches O
to O
the O
SR O
problem O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
[27], O
trees O
[46] O
or O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-the-art O
SR O
performance. O
Subsequently, O
it O

- B-DAT

- B-DAT

- B-DAT
ciently O
train O
these O
deeper O
network O

- B-DAT
normalization O
[32] O
is O
often O
used O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
results. O
Another O
powerful O
design O

- B-DAT

- B-DAT
connections O
relieve O
the O
network O
architecture O

- B-DAT
tentially O
non-trivial O
to O
represent O
with O

- B-DAT

- B-DAT

- B-DAT
ing O
pixel-wise O
averages O
of O
plausible O

- B-DAT

- B-DAT
ity O
[42, O
33, O
13, O
5 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
perceptually O
more O
convincing O
solutions O

- B-DAT
ure O
2. O
We O
illustrate O
the O

- B-DAT
ure O
3 O
where O
multiple O
potential O

- B-DAT
tion. O
Yu O
and O
Porikli O
[66 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions. O
Similar O
to O
this O
work O

- B-DAT
trained O
VGG O
network O
instead O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity. O
The O
GAN O
procedure O
encourages O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mark O
datasets O
as O
well O
as O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
counterpart O
IHR. O
The O
high-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nels O
as O
in O
the O
VGG O

- B-DAT
ical O
for O
the O
performance O
of O

- B-DAT
tent O
loss O
lSRX O
and O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
solutions O
with O
overly O
smooth O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stead O
of O
log[1−DθD O
(GθG(ILR))] O
[22 O

- B-DAT
mark O
datasets O
Set5 O
[3], O
Set14 O

- B-DAT
and O
high-resolution O
images. O
This O
corresponds O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bor, O
bicubic, O
SRCNN O
[9] O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gral O
score O
from O
1 O
(bad O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG54 O
and O
the O
original O
HR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ResNet O
and O
the O
adversarial O
networks O

- B-DAT
SRGAN- O
Set5 O
MSE O
VGG22 O
MSE O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
MSE-based O
reconstructions, O
to O
those O
competing O

- B-DAT

- B-DAT
formed O
other O
SRGAN O
and O
SRResNet O

- B-DAT

- B-DAT
GAN O
to O
NN, O
bicubic O
interpolation O

- B-DAT

- B-DAT

- B-DAT
art O
methods. O
Quantitative O
results O
are O

- B-DAT
resolved O
with O
SRResNet O
and O
SRGAN O

- B-DAT
realistic O
image O
SR. O
All O
differences O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentary O
material). O
We O
further O
found O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Net O
that O
sets O
a O
new O

- B-DAT
sure. O
We O
have O
highlighted O
some O

- B-DAT

- B-DAT
ial O
loss O
by O
training O
a O

- B-DAT

- B-DAT

- B-DAT
the-art O
reference O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Stevenson. O
Super-Resolution B-DAT
from O
Image O
Sequences O
- O
A O
Review. O
Midwest O
Symposium O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
by O
adaptive O
sparse O
domain O

- B-DAT
ization. O
IEEE O
Transactions O
on O
Image O

- B-DAT

- B-DAT
resolution. O
IEEE O
Computer O
Graphics O
and O

- B-DAT
level O
vision. O
International O
Journal O
of O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

- B-DAT
line O
at O
http://torch.ch/blog/2016/02/04/resnets. O
html. O
2016 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In O
European O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT
tion O
with O
deep O
convolutional O
neural O

- B-DAT

- B-DAT

- B-DAT
mentation O
algorithms O
and O
measuring O
ecological O

- B-DAT

- B-DAT

- B-DAT
sive O
survey. O
In O
Machine O
Vision O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
for O
fast O
example-based O
super-resolution O

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
(ACCV O

- B-DAT

- B-DAT

- B-DAT
Resolution O
via O
Deep O
and O
Shallow O

- B-DAT

- B-DAT

- B-DAT
ence O
on O
Signals, O
Systems O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
derstanding O
Neural O
Networks O
Through O
Deep O

International O
Conference O
on O
Machine O
Learning O
- B-DAT
Deep O
Learning O
Workshop O
2015, O
page O

- B-DAT

- B-DAT
resolution O
by O
retrieving O
web O
images O

- B-DAT
volutional O
networks. O
In O
European O
Conference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG22, O
SRGAN-VGG54) O
described O
in O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
off O
between O
accuracy O
and O
speed O

-100 B-DAT
thousand O
update O
iterations O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

A.4. O
Set5 O
- B-DAT
Visual O
Results O

A.5. O
Set14 O
- B-DAT
Visual O
Results O

A.6. O
BSD100 O
(five O
random O
samples) O
- B-DAT
Visual O
Results O

- B-DAT

- B-DAT

- B-DAT
fully O
applied O
to O
single-image O
super-resolution O

- B-DAT
world O
applications O
due O
to O
the O

- B-DAT

- B-DAT
ture O
that O
implements O
a O
cascading O

- B-DAT
work O
to O
further O
improve O
efficiency O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lution O
limitations, O
and O
could O
be O

- B-DAT

- B-DAT

- B-DAT
vided O
outstanding O
performance O
in O
SISR O

- B-DAT

- B-DAT
tical O
for O
real-world O
applications. O
One O

- B-DAT

- B-DAT

- B-DAT
ple, O
DRCN O
[21] O
uses O
a O

- B-DAT
els O
decrease O
the O
number O
of O

- B-DAT
ducing O
the O
number O
of O
parameters O

- B-DAT

- B-DAT
sider O
a O
situation O
where O
an O

- B-DAT

- B-DAT
lenging O
and O
necessary O
step O
that O

- B-DAT
mand O
for O
streaming O
media O
has O

- B-DAT
ing O
lossy O
compression O
techniques O
before O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
M). O
We O
first O
build O
our O

- B-DAT

- B-DAT
ing O
the O
FSRCNN O
[7], O
CARN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
approaches O
have O
been O
applied O

- B-DAT
based O
SISR O
in O
section O
2.1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rameters O
by O
engaging O
in O
redundant O

- B-DAT

(- B-DAT

- B-DAT

(- B-DAT

- B-DAT

- B-DAT
erations O
in O
(a) O
and O
(b O

- B-DAT

- B-DAT
ter O
category, O
SqueezeNet O
[19] O
builds O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ber O
of O
operations O
compared O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ary O
layers O
are O
cascaded O
into O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ual O
block, O
the O
first O
residual O

- B-DAT
rameter O
of O
the O
convolution O
layer O

- B-DAT
trated O
in O
block O
(c) O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
anism. O
As O
shown O
in O
Fig O

- B-DAT
cading O
on O
both O
the O
local O

- B-DAT

- B-DAT
resentations. O
2) O
Multi-level O
cascading O
connection O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
features. O
This O
facilitates O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
instead O
of O
depthwise O
convolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
scheme O
connects O
all O
blocks O

- B-DAT

- B-DAT
ations, O
while O
we O
gather O
it O

- B-DAT
catenated O
at O
the O
end O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
on O
two O
commonly-used O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ters O
than O
ours. O
The O
CARN-M O

- B-DAT

- B-DAT

- B-DAT
ple, O
CARN O
outperforms O
its O
most O

- B-DAT

- B-DAT
ble O
results O
against O
computationally-expensive O
models O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tiple O
scales O
using O
a O
single O

- B-DAT

(- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
the O
multi-scale O
learning O
to O

- B-DAT

- B-DAT
off O
between O
performance O
vs. O
parameters O

- B-DAT

- B-DAT

- B-DAT

337K O
311.0G O
37.66/0.9590 O
33.38/0.9136 O
31.91/0.8962 O
- B-DAT
LapSRN O
[24] O
813K O
29.9G O
37.52/0.9590 O

974K O
225.7G O
37.89/0.9598 O
33.61/0.9160 O
32.08/0.8984 O
- B-DAT
CARN O
(ours) O
1,592K O
222.8G O
37.76/0.9590 O

- B-DAT

337K O
311.0G O
33.74/0.9226 O
29.90/0.8322 O
28.82/0.7980 O
- B-DAT
DRRN O
[35] O
297K O
6,796.9G O
34.03/0.9244 O

1,159K O
120.0G O
34.27/0.9257 O
30.30/0.8399 O
28.97/0.8025 O
- B-DAT
CARN O
(ours) O
1,592K O
118.8G O
34.29/0.9255 O

- B-DAT

337K O
311.0G O
31.55/0.8856 O
28.15/0.7680 O
27.32/0.7253 O
- B-DAT
LapSRN O
[24] O
813K O
149.4G O
31.54/0.8850 O

1,417K O
83.1G O
32.00/0.8931 O
28.49/0.7783 O
27.44/0.7325 O
- B-DAT
SRDenseNet O
[37] O
2,015K O
389.9G O
32.02/0.8934 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cading. O
The O
network O
topologies O
are O

- B-DAT

- B-DAT
tively O
carries O
mid- O
to O
high-level O

- B-DAT

- B-DAT

- B-DAT
resentations, O
the O
CARN O
model O
can O

- B-DAT

- B-DAT

- B-DAT
NG O
without O
global O
cascading. O
CARN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gation, O
and O
thus O
lead O
to O

- B-DAT

- B-DAT
nections O
inside O
the O
residual O
blocks O

- B-DAT
nation O
and O
1×1 O
convolutions, O
it O

- B-DAT

- B-DAT
tions O
in O
the O
cascading O
connection O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eters, O
and O
PSNR O
vs. O
operations O

- B-DAT

- B-DAT
ically. O
For O
example, O
the O
G64 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
form O
SISR O
accurately O
and O
efficiently O

- B-DAT

- B-DAT

- B-DAT
ent. O
Our O
experiments O
show O
that O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gence O
33(5), O
898–916 O
(2011 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
British O
Machine O

- B-DAT

- B-DAT

- B-DAT
scale O
hierarchical O
image O
database. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
(ICCV) O
(2015 O

- B-DAT
telligence O
and O
Statistics O
(2010 O

- B-DAT
works O
with O
pruning, O
trained O
quantization O

- B-DAT
level O
performance O
on O
imagenet O
classification O

- B-DAT
dreetto, O
M., O
Adam, O
H.: O
Mobilenets O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
and O
Pattern O
Recognition O
(CVPR O

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks. O
In: O
Proceedings O

- B-DAT
tion O
Processing O
Systems O
(NIPS) O
(2012 O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ters O
24(8), O
1208–1212 O
(2017 O

- B-DAT

- B-DAT
mentation. O
In: O
Proceedings O
of O
the O

- B-DAT

- B-DAT
ple O
convolution O
neural O
networks. O
In O

- B-DAT

- B-DAT
cal O
image O
segmentation. O
In: O
Proceedings O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work. O
In: O
Proceedings O
of O
the O

- B-DAT
nition O
(CVPR) O
(2017 O

- B-DAT

- B-DAT
tions. O
In: O
Proceedings O
of O
the O

- B-DAT

- B-DAT
resentation. O
IEEE O
transactions O
on O
image O

- B-DAT

the O
block O
state O
for O
the O
BSD100 B-DAT

from O
an O
image O
of O
the O
BSD100 B-DAT

PSNR O
values O
measured O
for O
the O
BSD100 B-DAT

PSNR, O
and O
SSIM O
for O
the O
BSD100 B-DAT

Method O
# O
params O
Set5 O
Set14 O
BSD100 B-DAT

and O
SSIM O
values O
for O
the O
BSD100 B-DAT

factor O
of O
2 O
on O
the O
BSD100 B-DAT

as O
nearest-neighbor, O
bilinear, O
and O
bicubic O
upscaling B-DAT

the O
proposed O
methods O
for O
an O
upscaling B-DAT
factor O
of O
2 O
on O
the O

the O
com- O
putational O
complexity O
than O
upscaling B-DAT
at O
the O
initial O
stage O
[3,6,12 O

a O
recur- O
sive O
manner, O
and O
upscaling B-DAT

image O
is O
obtained O
from O
the O
upscaling B-DAT
module O

initial O
feature O
extraction, O
RRB, O
and O
upscaling B-DAT
parts. O
On O
the O
other O
hand O

status O
are O
inputted O
to O
the O
upscaling B-DAT
part. O
We O
investigate O
the O
effectiveness O

23]. O
For O
instance, O
in O
the O
upscaling B-DAT
part O
by O
a O
factor O
of O

is O
not O
used O
in O
the O
upscaling B-DAT
part O

the O
frequency O
of O
the O
progressive O
upscaling B-DAT

of O
times O
to O
employ O
the O
upscaling B-DAT
part, O
it O
is O
beneficial O
to O

BSRN O
model, O
one O
of O
the O
upscaling B-DAT
paths O
(i.e., O
×2, O
×3, O
and O

single-scale O
BSRN O
models O
having O
an O
upscaling B-DAT
factor O
of O
4, O
which O
are O

on O
the O
latter O
part O
(i.e., O
upscaling B-DAT
part) O
to O
generate O
good O
quality O

i.e., O
how O
many O
times O
the O
upscaling B-DAT
part O
is O
employed), O
which O
is O

In O
our O
proposed O
model, O
the O
upscaling B-DAT
part O
spends O
most O
of O
the O

average O
processing O
time O
spent O
on O
upscaling B-DAT
an O
image O
by O
a O
factor O

super-resolved O
image O
with O
the O
given O
upscaling B-DAT
factor O
for O
each O
method O
is O

Deep O
residual O
network O
with O
enhanced O
upscaling B-DAT
module O
for O
super-resolution. O
In: O
Proceedings O

including O
Set5 O
[5], O
Set14 O
[29], O
BSD100 B-DAT
[20], O
and O
Urban100 O
[11 O

the O
block O
state O
for O
the O
BSD100 B-DAT
dataset O
[20 O

from O
an O
image O
of O
the O
BSD100 B-DAT
dataset O
[20]. O
Enlarged O
versions O
of O

PSNR O
values O
measured O
for O
the O
BSD100 B-DAT
dataset O
[20]. O
Overall, O
both O
the O

PSNR, O
and O
SSIM O
for O
the O
BSD100 B-DAT
dataset O
[20 O

Method O
# O
params O
Set5 O
Set14 O
BSD100 B-DAT
Urban100 O
PSNR O
/ O
SSIM O
PSNR O

the O
Set5 O
[5], O
Set14 O
[29], O
BSD100 B-DAT
[20], O
and O
Urban100 O
[11] O
datasets O

and O
SSIM O
values O
for O
the O
BSD100 B-DAT
dataset O
[20] O
for O
various O
values O

factor O
of O
2 O
on O
the O
BSD100 B-DAT
dataset O
over O
the O
LapSRN O
model O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
plexities, O
thus O
some O
recursive O
parameter-sharing O

- B-DAT

- B-DAT

- B-DAT
work. O
By O
taking O
advantage O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
field. O
For O
example, O
Dong O

- B-DAT

- B-DAT
volutional O
neural O
network O
(SRCNN) O
model O

- B-DAT
formance O
in O
comparison O
to O
the O

- B-DAT

- B-DAT
nections O
and O
various O
optimization O
techniques O

- B-DAT

- B-DAT

- B-DAT
matically O
increases O
the O
number O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
hinder O
them O
from O
fully O

- B-DAT
vided O
to O
the O
recurrent O
unit O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
block O
state-based O
recursive O
network O

- B-DAT

- B-DAT
arate O
information O
storage O
to O
keep O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
the O
BSRN O
model O
can O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
CNN, O
which O
enhances O
the O
interpolated O

- B-DAT

- B-DAT
lutional O
layers O
to O
improve O
the O

- B-DAT

- B-DAT
tion O
layer O
for O
16 O
times O

- B-DAT

- B-DAT
processing O
part O

- B-DAT

- B-DAT
ple, O
Lai O
et O
al. O
[18 O

- B-DAT

- B-DAT

- B-DAT

C O
- B-DAT

C O
- B-DAT

C O
- B-DAT

C O
- B-DAT

- B-DAT

- B-DAT

- B-DAT
putational O
complexity O
than O
upscaling O
at O

- B-DAT
ploying O
multiple O
residual O
connections O
is O

- B-DAT
ages O
[15,24]. O
Third, O
obtaining O
multiple O

- B-DAT
resolution O
model O
and O
combining O
them O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vided O
into O
three O
parts: O
initial O

- B-DAT
sive O
manner, O
and O
upscaling. O
Fig O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ther O
processed O
via O
a O
recursive O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nates O
two O
input O
matrices O
along O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mance: O
increasing O
the O
number O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put, O
where O
the O
later O
outputs O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
by-pixel O
L1 O
loss, O
i.e O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els. O
The O
single-scale O
models O
are O

- B-DAT

- B-DAT

- B-DAT
ods O
across O
different O
scales. O
The O

- B-DAT

- B-DAT
domly O
cropped O
from O
the O
training O

- B-DAT

- B-DAT
scale O
BSRN O
model. O
For O
data O

- B-DAT

- B-DAT
resolved O
images O
are O
obtained O
from O

- B-DAT
eters. O
To O
prevent O
the O
vanishing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bers O
of O
the O
convolutional O
channels O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
termediate O
features O
are O
largely O
different O

- B-DAT
cally O
change, O
even O
though O
the O

- B-DAT

- B-DAT
gressively O
improved O
features O
and O
highly O

- B-DAT
ploying O
the O
block O
state O
(Fig O

- B-DAT

- B-DAT
space O
operation O
and O
increased O
spatial O

- B-DAT

- B-DAT

- B-DAT
ation. O
To O
verify O
this, O
we O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uated O
on O
the O
Set5 O
[5 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sharing O
parts. O
The O
VDSR, O
LapSRN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ber O
of O
model O
parameters O
required O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ters O
small O
enough O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ample, O
our O
method O
successfully O
upscales O

- B-DAT
resolved O
images O

- B-DAT

- B-DAT
plained O
the O
benefits O
and O
efficiency O

- B-DAT
dition, O
comparison O
with O
the O
other O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mawat, O
S., O
Irving, O
G., O
Isard O

- B-DAT

- B-DAT
chine O
learning. O
In: O
Proceedings O
of O

- B-DAT
resolution: O
Dataset O
and O
study. O
In O

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
British O
Machine O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
via O
dual-state O
recurrent O
networks O

- B-DAT
ence O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks. O
In: O
Proceedings O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
puter O
Vision. O
pp. O
416–423 O
(2001 O

- B-DAT

- B-DAT

- B-DAT
ings O
of O
the O
IEEE O
International O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work. O
In: O
Proceedings O
of O
the O

- B-DAT
ing O
13(4), O
600–612 O
(2004 O

- B-DAT

- B-DAT

- B-DAT
computing O
74(17), O
3193–3203 O
(2011 O

- B-DAT

- B-DAT
representations. O
In: O
Proceedings O
of O
the O

- B-DAT

- B-DAT

21]. O
The O
Set5, O
Set14, O
and O
BSD100 B-DAT

33.52 O
33.45 O
33.58 O
33.56 O
33.57 O
BSD100 B-DAT
32 I-DAT

a) O
BSD100 B-DAT
(b) I-DAT
Urban100 O

Method O
# O
params. O
Set5 O
Set14 O
BSD100 B-DAT

Method O
# O
params. O
Set5 O
Set14 O
BSD100 B-DAT

For O
the O
upscaling B-DAT
part, O
we O
use O
the O
sub-pixel O

layers O
except O
those O
for O
the O
upscaling B-DAT
part. O
All O
our O
networks O
are O

methods, O
since O
MSRN O
uses O
different O
upscaling B-DAT
methods O
using O
more O
parameters O
for O

benchmarks: O
Set5 O
[2], O
Set14 O
[32], O
BSD100 B-DAT
[20], O
Ur- O
ban100 O
[11], O
Manga109 O

21]. O
The O
Set5, O
Set14, O
and O
BSD100 B-DAT
datasets O
consist O
of O
natural O
images O

33.52 O
33.45 O
33.58 O
33.56 O
33.57 O
BSD100 B-DAT
32.17 O
32.17 O
32.17 O
32.17 O
32.17 O

a) O
BSD100 B-DAT

Method O
# O
params. O
Set5 O
Set14 O
BSD100 B-DAT
Urban100 O
Manga109PSNR O
/ O
SSIM O
PSNR O

Method O
# O
params. O
Set5 O
Set14 O
BSD100 B-DAT
Urban100PSNR O
/ O
SSIM O
PSNR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ral O
networks O
that O
stands O
out O

- B-DAT
tention O
mechanisms O
to O
single O
image O

- B-DAT

- B-DAT

- B-DAT
wise O
and O
spatial O
attention O
mechanisms O

- B-DAT

- B-DAT
imental O
analysis O
of O
different O
attention O

- B-DAT
formance O
in O
comparison O
to O
existing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O

- B-DAT

- B-DAT

- B-DAT
resolution O
(LR) O
image. O
It O
is O

- B-DAT

- B-DAT

- B-DAT
proved O
performance, O
this O
also O
has O

- B-DAT

- B-DAT
mation O
equally, O
which O
may O
not O

- B-DAT
table O
network O
structures O
in O
various O

- B-DAT
lems O
[8, O
30]. O
It O
allows O

- B-DAT
tracted O
feature O
maps, O
so O
that O

- B-DAT
ploy O
attention O
mechanisms. O
Zhang O
et O

- B-DAT
level O
vision O
problem O
[8] O
without O

- B-DAT

- B-DAT

- B-DAT
timized O
for O
SR, O
are O
attached O

- B-DAT

- B-DAT
RAM). O
The O
proposed O
RAM O
exploits O

- B-DAT
and O
intra- O
channel O
relationship O
by O

- B-DAT
spectively. O
We O
demonstrate O
both O
the O

- B-DAT
ciency O
of O
our O
proposed O
method O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ual O
attention O
module O
(RAM) O
based O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tiveness O
and O
efficiency O
of O
our O

- B-DAT

- B-DAT

- B-DAT
ual O
(CSAR) O
block O
[9]. O
Targeting O

- B-DAT
tion O
module O
that O
can O
be O

- B-DAT
nels O
(CA) O
or O
spatial O
regions O

- B-DAT
tention O
map O
M, O
having O
a O

- B-DAT
erated O
attention O
map O
is O
normalized O

- B-DAT

- B-DAT
matically O
in O
Table O
1, O
which O

- B-DAT

- B-DAT

- B-DAT
nism O
aims O
to O
recalibrate O
filter O

- B-DAT
channel O
correlation, O
i.e., O
CA. O
The O

- B-DAT
plied O
in O
the O
squeeze O
process O

- B-DAT
channel O
relation O
for O
refining O
feature O

- B-DAT
ploits O
both O
inter-channel O
and O
inter-spatial O

- B-DAT
ing O
is O
additionally O
performed O
in O

- B-DAT
cess. O
For O
the O
SA O
module O

- B-DAT
tially O
performs O
CA O
and O
then O

- B-DAT
volutions, O
where O
the O
first O
one O

- B-DAT
tio. O
While O
CBAM O
combines O
the O

- B-DAT
spired O
by O
EDSR O
[19], O
is O

CSAR O
[9] O
- B-DAT
M O
= O
conv1×1(conv1×1(X)) O
RAM O

RCAB O
[33] O
- B-DAT
CBAM O
[30] O
X̂ O
= O
fSA(fCA(X O

- B-DAT

- B-DAT

- B-DAT
scaling O
part. O
Let O
ILR O
and O

- B-DAT
tion O
3.2. O
F0 O
is O
updated O

- B-DAT
tion, O
and O
then O
the O
updated O

- B-DAT

- B-DAT

- B-DAT
struction O

- B-DAT
ing O
and O
reconstruction, O
respectively, O
and O

- B-DAT

- B-DAT
ditionally O
propose O
a O
way O
to O

- B-DAT
pose O
residual O
attention O
module O
(RAM O

- B-DAT

- B-DAT
lution, O
ReLU, O
and O
convolution, O
and O

- B-DAT
posed O
FA O
mechanism O

- B-DAT

- B-DAT
puter O
vision O
problems O
such O
as O

- B-DAT
ject O
detection O
without O
modification. O
However O

- B-DAT
mately O
aims O
at O
restoring O
high-frequency O

- B-DAT
ages, O
it O
is O
more O
reasonable O

- B-DAT
mined O
using O
high-frequency O
statistics O
about O

- B-DAT
ters O
will O
extract O
the O
edge O

- B-DAT
tion. O
From O
the O
viewpoint O
of O

- B-DAT
nels O
varies O
by O
the O
spatial O

- B-DAT
frequency O
components O
such O
as O
sky O

- B-DAT
like O
CBAM O
[30], O
which O
performs O

- B-DAT
formation O
per O
channel O
to O
preserve O

- B-DAT

- B-DAT
teristics. O
In O
addition, O
for O
the O

- B-DAT
trast O
to O
other O
SA O
mechanisms O

- B-DAT

- B-DAT

- B-DAT
volution O

- B-DAT
anisms O
exploit O
information O
from O
inter-channel O

- B-DAT
channel O
relationship, O
respectively. O
Therefore, O
in O

- B-DAT

- B-DAT
ered O
in O
detail O
in O
Section O

- B-DAT
ban100 O
[11], O
Manga109 O
[21]. O
The O

- B-DAT

- B-DAT
ferent O
characteristics O
from O
natural O
ones O

- B-DAT
to-noise O
ration O
(PSNR) O
and O
structural O

- B-DAT
dex O
on O
the O
Y O
channel O

- B-DAT

- B-DAT
domly O
crop O
a O
48×48 O
patch O

- B-DAT
lected O
16 O
LR O
training O
images O

- B-DAT

- B-DAT
ing O
images O
for O
each O
RGB O

- B-DAT
ment O
our O
networks O
using O
the O

- B-DAT
ploying O
each O
mechanism O
increases O
the O

- B-DAT
rameters, O
and O
as O
the O
network O

- B-DAT
ment O
with O
the O
easiest O
case O

- B-DAT
acteristics O
to O
check O
the O
generalization O

- B-DAT
wise O
and O
spatial O
information O
is O

- B-DAT
nism O
leads O
to O
performance O
improvement O

- B-DAT
age) O
only O
by O
adding O
9K O

- B-DAT
ages O
and O
the O
images O
in O

- B-DAT
tics O
of O
computer-generated O
images, O
which O

- B-DAT
ent O
from O
those O
in O
the O

- B-DAT
mance O
improvement O
is O
achieved O
only O

- B-DAT
pare O
it O
with O
the O
other O

- B-DAT
lustrated O
in O
Section O
2. O
For O

- B-DAT
mance. O
Overall, O
all O
the O
cases O

- B-DAT
ters O
than O
our O
network O
(+257K O

- B-DAT
ods O
of O
the O
squeeze O
process O

- B-DAT
served O
that O
our O
method O
extracting O

- B-DAT

- B-DAT
ing O
images, O
i.e., O
Urban100 O
and O

- B-DAT

- B-DAT
wise O
convolution O
is O
an O
effective O

- B-DAT
ditionally O
used, O
but O
at O
the O

- B-DAT
ure O
4. O
We O
have O
three O

- B-DAT
nisms O
showing O
higher O
performance O
have O

- B-DAT
ing O
the O
corresponding O
blocks O
only O

- B-DAT
nism. O
3) O
Our O
FA O
mechanism O

- B-DAT
ods, O
and O
those O
marked O
with O

- B-DAT
els O
with O
varying O
the O
number O

- B-DAT
scale O
SR O
are O
marked O
with O

- B-DAT
scale O
SR O
are O
marked O
with O

- B-DAT
tive O
(red O
color) O
and O
negative O

- B-DAT

- B-DAT
ground) O
and O
the O
high-frequency O
components O

- B-DAT
frequency O
component O
values O
to O
zero O

- B-DAT
tention” O
to O
the O
high-frequency O
components O

- B-DAT

- B-DAT
termined O
by O
the O
number O
of O

- B-DAT
tional O
parameters O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ness O
of O
RAM O
on O
large O

- B-DAT
responds O
to O
the O
opposite O
case O

- B-DAT
RAM O
R10C64, O
64×64 O
patches O
and O

- B-DAT

- B-DAT
rized O
in O
Table O
3, O
Table O

- B-DAT
ods O
widens O
further O
in O
Urban100 O

- B-DAT

- B-DAT
sual O
results O
of O
challenging O
images O

- B-DAT
mance O
with O
fewer O
parameters O
than O

- B-DAT
strates O
the O
efficiency O
of O
ours O

- B-DAT

- B-DAT
anisms O
are O
integrated O
in O
our O

- B-DAT
strated O
that O
our O
attention O
methods O

- B-DAT
tion O
mechanisms O
for O
SR O
and O

- B-DAT

- B-DAT

- B-DAT
work. O
In O
Proceedings O
of O
the O

- B-DAT
puter O
Vision O
(ECCV), O
2018. O
1 O

- B-DAT
Morel. O
Low-complexity O
single-image O
super-resolution O
based O

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
European O
Conference O

- B-DAT

- B-DAT

- B-DAT
works. O
In O
Proceedings O
of O
the O

- B-DAT
jection O
networks O
for O
super-resolution. O
In O

- B-DAT
tion O
(CVPR), O
2018. O
1, O
7 O

- B-DAT
cient O
convolutional O
neural O
networks O
for O

- B-DAT
cations. O
arXiv O
preprint O
arXiv:1704.04861, O
2017 O

- B-DAT

- B-DAT

- B-DAT
works. O
In O
Proceedings O
of O
the O

- B-DAT

- B-DAT
resolution. O
arXiv O
preprint O
arXiv:1809.11130, O
2018 O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
(CVPR), O
2016. O
1 O

- B-DAT
mization. O
arXiv O
preprint O
arXiv:1412.6980, O
2014 O

- B-DAT
resolution. O
In O
Proceedings O
of O
the O

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT
tive O
adversarial O
network. O
In O
Proceedings O

- B-DAT
ence O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cal O
statistics. O
In O
Proceedings O
of O

- B-DAT
ference O
on O
Computer O
Vision O
(ICCV O

- B-DAT
masaki, O
and O
K. O
Aizawa. O
Sketch-based O

- B-DAT
ing O
manga109 O
dataset. O
Multimedia O
Tools O

- B-DAT
ceedings O
of O
the O
Advances O
in O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
IEEE O
International O

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT

- B-DAT
lutional O
block O
attention O
module. O
In O

- B-DAT
pean O
Conference O
on O
Computer O
Vision O

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT

- B-DAT

- B-DAT
tional O
Conference O
on O
Curves O
and O

- B-DAT

- B-DAT
ual O
dense O
network O
for O
image O

- B-DAT

- B-DAT
ings O
of O
the O
IEEE O
Conference O

result O
produced O
by O
ENet-PAT O
at O
4x B-DAT
super-resolution O
on O
an O
image O
from O

fully O
convolutional O
network O
architecture O
for O
4x B-DAT
super-resolution O
which O
only O
learns O
the O

an O
image O
from O
ImageNet O
for O
4x B-DAT
super-resolution. O
Despite O
reaching O
state-of-the-art O
results O

trained O
with O
different O
losses O
at O
4x B-DAT
super-resolution O
on O
images O
from O
ImageNet O

different O
combinations O
of O
losses O
at O
4x B-DAT
super O
resolution. O
ENet-E O
yields O
the O

methods O
with O
our O
results O
at O
4x B-DAT
super-resolution O
on O
an O
image O
from O

PSNR O
for O
different O
methods O
at O
4x B-DAT
super-resolution. O
ENet-E O
achieves O
state-of-the-art O
results O

both O
ENet-E O
and O
ENet-PAT O
at O
4x B-DAT
super- O
resolution O
side-by-side, O
and O
were O

an O
image O
from O
BSD100 O
at O
4x B-DAT
super-resolution. O
ENet- O
PAT’s O
result O
looks O

on O
average O
per O
image O
at O
4x B-DAT
super-resolution O

adversarial O
discrimina- O
tive O
network O
at O
4x B-DAT
super-resolution. O
As O
in O
the O
generative O

on O
average O
per O
image O
at O
4x B-DAT
super-resolution O
on O
Set5/Set14, O
though O
EnhanceNet O

the O
result O
of O
ENet-PAT O
at O
4x B-DAT
super-resolution O
with O
the O
current O
state O

super-resolution O
in O
Fig. O
4. O
Although O
4x B-DAT
super-resolution O
is O
a O
greatly O
more O

are O
lost O
completely O
in O
the O
4x B-DAT
downsampled O
image O
are O
more O
accurate O

image O
with O
sharper O
textures O
at O
4x B-DAT
super-resolution O
that O
even O
out- O
performs O

on O
images O
from O
ImageNet O
at O
4x B-DAT
super-resolution. O
Computing O
the O
texture O
matching O

edges O
and O
overly O
smooth O
textures O
(4x B-DAT
super-resolution). O
Furthermore, O
these O
models O
are O

that O
the O
network O
produces O
at O
4x B-DAT
super-resolution. O
While O
ENet-E O
significantly O
sharpens O

downsampled O
input O
2x O
downsampled O
input O
4x B-DAT
downsampled O
input O
IHR O

VDSR O
[7] O
2x O
DRCN O
[8] O
4x B-DAT
ENet-PAT O
IHR O

missing) O
with O
our O
model O
at O
4x B-DAT
super-resolution O
(93.75% O
of O
all O
pixels O

Bruna O
et O
al. O
[2] O
at O
4x B-DAT
super-resolution. O
ENet-PAT O
produces O
images O
with O

different O
methods O
at O
2x O
and O
4x B-DAT
super-resolution. O
Similar O
to O
PSNR, O
ENet-PAT O

IFC O
for O
different O
methods O
at O
4x B-DAT
super-resolution. O
Best O
performance O
shown O
in O

an O
image O
from O
ImageNet O
at O
4x B-DAT
super-resolution. O
While O
producing O
an O
overall O

on O
images O
of O
faces O
at O
4x B-DAT
super O
resolution. O
ENet-PAT O
produces O
artifacts O

25.64 O
25.94 O
24.93 O
26.53 O
25.77 O
BSD100 B-DAT
25 I-DAT

BSD100 B-DAT
25 I-DAT

24] O
on O
an O
image O
from O
BSD100 B-DAT

on O
an O
image O
in O
the O
BSD100 B-DAT

BSD100 B-DAT
29 I-DAT

BSD100 B-DAT
0 I-DAT

BSD100 B-DAT
0 I-DAT

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
TOG O

rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

25.64 O
25.94 O
24.93 O
26.53 O
25.77 O
BSD100 B-DAT
25.96 O
27.50 O
24.73 O
25.71 O
24.19 O

BSD100 B-DAT
25.96 O
26.75 O
26.82 O
26.84 O
26.90 O

24] O
on O
an O
image O
from O
BSD100 B-DAT
at O
4x O
super-resolution. O
ENet- O
PAT’s O

9ms O
(Set5), O
18ms O
(Set14), O
12ms O
(BSD100) B-DAT
and O
59ms O
(Urban100) O
on O
average O

case O
on O
an O
image O
from O
BSD100 B-DAT

on O
an O
image O
in O
the O
BSD100 B-DAT
dataset O
that O
is O
shown O
in O

BSD100 B-DAT
29.56 O
31.16 O
26.75 O
31.18 O
31.36 O

BSD100 B-DAT
0.8431 O
0.8840 O
0.8863 O
0.8855 O
0.8879 O

BSD100 B-DAT
0.6675 O
0.7054 O
0.7087 O
0.7106 O
0.7101 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put. O
Traditionally, O
the O
performance O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
quality. O
As O
a O
result O

- B-DAT
rics O
tend O
to O
produce O
over-smoothed O

- B-DAT
frequency O
textures O
and O
do O
not O

- B-DAT
thesis O
in O
combination O
with O
a O

- B-DAT
accurate O
reproduction O
of O
ground O
truth O

- B-DAT
ing. O
By O
using O
feed-forward O
fully O

- B-DAT
works O
in O
an O
adversarial O
training O

- B-DAT
nificant O
boost O
in O
image O
quality O

- B-DAT
fectiveness O
of O
our O
approach, O
yielding O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cent O
years. O
The O
problem O
is O

- B-DAT

- B-DAT
ferent O
HR O
images O
can O
give O

- B-DAT

- B-DAT

- B-DAT
lem O
becomes O
worse, O
rendering O
SISR O

- B-DAT
lem. O
Despite O
considerable O
progress O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
are O
still O
far O
from O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ploy: O
most O
systems O
minimize O
the O

- B-DAT

- B-DAT
construction O
from O
the O
LR O
observation O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
network O
architecture, O
we O

- B-DAT
bination O
with O
adversarial O
training O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
perceptual O
metrics O

- B-DAT
zos O
[11] O
are O
based O
on O

- B-DAT

- B-DAT
lahi O
and O
Moeslund O
[37] O
and O

- B-DAT
based O
models O
that O
either O
exploit O

- B-DAT
ent O
scales O
within O
a O
single O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works O
to O
the O
task O
of O

- B-DAT
gration O
to O
learn O
a O
mapping O

- B-DAT

- B-DAT
tion, O
the O
results O
tend O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ject O
recognition O
system O
like O
VGG O

- B-DAT

- B-DAT

- B-DAT
tion O
with O
an O
adversarial O
network O

- B-DAT
sually O
implausible O
artifacts O
without O
the O

- B-DAT

- B-DAT

- B-DAT
ation O
d O
is O
non-injective O
and O

- B-DAT

- B-DAT
ity O
in O
SISR: O
since O
downsampling O

- B-DAT

- B-DAT
fore, O
even O
state-of-the-art O
models O
learn O

- B-DAT
ple O
in O
Fig. O
2, O
where O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lutional O
layers O
enables O
training O
of O

- B-DAT
put O
image O
of O
arbitrary O
size O

- B-DAT

- B-DAT

- B-DAT
ported O
to O
produce O
checkerboard O
artifacts O

- B-DAT
zontal O
bars O
of O
1×2 O
pixels O

- B-DAT
not O
be O
distinguished O
anymore O
since O

- B-DAT

- B-DAT
pling O
of O
the O
feature O
activations O

- B-DAT

- B-DAT
volution O
layer O
after O
all O
upsampling O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puting O
distances O
in O
image O
space O

- B-DAT

- B-DAT

- B-DAT
tion O
of O
the O
popular O
VGG-19 O

- B-DAT
ally O
decrease O
the O
spatial O
dimension O

- B-DAT
tract O
higher-level O
features O
in O
higher O

- B-DAT

- B-DAT

- B-DAT
atively O
by O
matching O
statistics O
extracted O

- B-DAT

- B-DAT
tween O
the O
feature O
activations O
φ(I O

- B-DAT

- B-DAT
ages O
[24, O
53], O
however O
a O

- B-DAT
resolution O
textures O
during O
inference, O
we O

- B-DAT
ture O
loss O
LT O
patch-wise O
during O

- B-DAT
fore O
learns O
to O
produce O
images O

- B-DAT
tures O
as O
the O
high-resolution O
images O

- B-DAT

- B-DAT

- B-DAT
tures. O
Empirically, O
we O
found O
a O

- B-DAT
ation O
and O
the O
overall O
perceptual O

- B-DAT
imize O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
inative O
network O
as O
we O
found O

- B-DAT
tor O
from O
overpowering O
the O
generator O

- B-DAT
ing O
learning O
strategy O
yields O
better O

- B-DAT
vious O
training O
batch O
and O
only O

- B-DAT
ther O
details O
are O
specified O
in O

- B-DAT
ously O
introduced O
loss O
functions. O
After O

- B-DAT
tive O
and O
quantitative O
evaluation O
of O

- B-DAT
tors O
are O
given O
in O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formations, O
the O
network O
is O
given O

- B-DAT
alistic O
textures O
when O
trained O
with O

- B-DAT

- B-DAT
work O
sometimes O
produces O
unpleasing O
high-frequency O

- B-DAT

- B-DAT

- B-DAT
PAT O
produces O
perceptually O
more O
realistic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
struction, O
but O
even O
the O
state-of-the-art O

- B-DAT

- B-DAT
teristics O
as O
previous O
approaches. O
The O

- B-DAT
age O
than O
ENet-E. O
On O
the O

- B-DAT

- B-DAT
alistic O
textures. O
Comparisons O
with O
further O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pose O
to O
use O
the O
performance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nition O
models O
as O
a O
metric O

- B-DAT

- B-DAT
plement O
pixel-based O
benchmarks O
such O
as O

- B-DAT

- B-DAT

- B-DAT
ric O
similar O
to O
ours O
to O

- B-DAT

-50 B-DAT
[6, O
20] O
as O
this O
class O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lenge O
(ILSVRC) O
[44]. O
For O
the O

- B-DAT

- B-DAT
bels. O
The O
original O
images O
are O

-1 B-DAT
and O
top-5 O
errors O
as O
well O

- B-DAT
fications. O
The O
results O
are O
shown O

- B-DAT
ison, O
some O
of O
the O
results O

- B-DAT
formance O
followed O
by O
DRCN O
[26 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tual O
quality O
which O
is O
reflected O

- B-DAT
ject O
recognition O
benchmark O
matches O
human O

- B-DAT
ter O
than O
PSNR O
does. O
The O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mance O
roughly O
coincides O
with O
the O

- B-DAT
age O
quality O
in O
this O
benchmark O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
imize O
the O
Euclidean O
loss, O
we O

- B-DAT

- B-DAT
erated O
by O
ENet-PAT O
which O
have O

- B-DAT
ages O
upsampled O
with O
bicubic O
interpolation O

- B-DAT

- B-DAT

- B-DAT
resolution O
side-by-side, O
and O
were O
asked O

- B-DAT
age O
produced O
by O
ENet-PAT O
91.0 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
error O
0.506 O
0.477 O
0.454 O
0.449 O

-5 B-DAT
error O
0.266 O
0.242 O
0.224 O
0.214 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
PAT’s O
result O
looks O
more O
natural O

- B-DAT
lutional O
network O
at O
test O
time O

- B-DAT
gence O
rates O
depend O
on O
the O

- B-DAT
tions. O
Although O
not O
optimized O
for O

- B-DAT

- B-DAT
ducing O
state-of-the-art O
results O
by O
both O

- B-DAT
itative O
measures O
by O
training O
with O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tic, O
they O
do O
not O
match O

- B-DAT
wise O
basis. O
Furthermore, O
the O
adversarial O

- B-DAT
eas. O
This O
is O
a O
result O

- B-DAT
work O
and O
apply O
shrinking O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tails O
and O
additional O
comparisons. O
A O

- B-DAT
tation O
of O
ENet-PAT O
can O
be O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
image O
models O
using O
a O

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
ceptual O
similarity O
metrics O
based O
on O

- B-DAT

- B-DAT

- B-DAT
based O
super-resolution. O
IEEE O
CG&A, O
22(2):56–65 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
nets. O
In O
NIPS O

- B-DAT
tional O
neural O
networks O
for O
direct O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
istration. O
CVGIP: O
Graphical O
models O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
sparse O
regression O
and O
natural O

- B-DAT
ceptual O
image O
quality O
assessment O
using O

- B-DAT
cian O
pyramid. O
Electronic O
Imaging, O
2016(16):1–6 O

- B-DAT
jani, O
J. O
Totz, O
Z. O
Wang O

- B-DAT

- B-DAT
age O
super-resolution O
using O
a O
generative O

- B-DAT
resolved O
faces O
for O
improved O
face O

- B-DAT
lance O
video. O
In O
ICB, O
2007 O

- B-DAT
manan, O
P. O
Dollár, O
and O
C O

- B-DAT
mon O
objects O
in O
context. O
In O

- B-DAT
strained O
sparse O
coding O
for O
single O

- B-DAT

- B-DAT
earities O
improve O
neural O
network O
acoustic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
checkerboard/, O
2016 O

- B-DAT

- B-DAT

- B-DAT
sentation O
learning O
with O
deep O
convolutional O

- B-DAT
sarial O
networks. O
In O
ICLR, O
2016 O

- B-DAT
tion O
based O
noise O
removal O
algorithms O

- B-DAT

- B-DAT

- B-DAT
mation O
fidelity O
criterion O
for O
image O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT
structure O
preserving O
image O
super O
resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ture O
networks: O
Feed-forward O
synthesis O
of O

- B-DAT
ized O
images. O
In O
ICML, O
2016 O

- B-DAT

- B-DAT
ment O
photographs. O
In O
ECCV, O
2016 O

- B-DAT
similarities O
for O
single O
frame O
super-resolution O

- B-DAT

- B-DAT
resolution: O
a O
benchmark. O
In O
ECCV O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
as O
sparse O
representation O
of O

- B-DAT
age O
super-resolution O
via O
sparse O
representation O

- B-DAT

- B-DAT
inative O
generative O
networks. O
In O
ECCV O

- B-DAT
age O
super-resolution O
by O
retrieving O
web O

- B-DAT

- B-DAT

- B-DAT
tion. O
In O
ECCV, O
2016 O

- B-DAT
fold. O
In O
ECCV, O
2016 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cally O
similar O
textures O
between O
Iest O

- B-DAT
tween O
faithful O
texture O
generation O
and O

- B-DAT

- B-DAT

-4 B-DAT

- B-DAT

-128 B-DAT

- B-DAT
age O
since O
the O
network O
is O

- B-DAT
pleasant O
results O

- B-DAT
sarial O
network O
used O
for O
the O

- B-DAT
mon O
design O
patterns O
[13] O
and O

- B-DAT
tive O
network O
at O
4x O
super-resolution O

- B-DAT
tions O
[11] O
and O
strided O
convolutions O

- B-DAT
duce O
a O
classification O
label O
between O

- B-DAT
put O
which O
renders O
training O
more O

- B-DAT

- B-DAT
ual O
image O
that O
cancel O
out O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
which O
leads O
to O
loss O

- B-DAT

- B-DAT
EA O
and O
ENet-EAT O
are O
shown O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tor. O
ENet-PAT O
is O
the O
only O

- B-DAT
tails O
and O
it O
is O
visually O

- B-DAT
hanceNet O
is O
even O
faster O
than O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pare O
the O
result O
of O
ENet-PAT O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
manding O
task O
than O
2x O
super-resolution O

- B-DAT
parable O
in O
quality. O
Small O
details O

- B-DAT

- B-DAT
performs O
the O
current O
state O
of O

- B-DAT

- B-DAT
PAT’s O
result O
and O
looks O
very O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceptual O
quality O
of O
ENet-PAT’s O
results O

- B-DAT

- B-DAT

-4 B-DAT
ENet-PAT-128 O
ENet-PAT-16 O
(default) O
IHR O

- B-DAT

- B-DAT

- B-DAT

-4 B-DAT

- B-DAT

-128 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
jority O
of O
subjects O
in O
our O

- B-DAT

- B-DAT

- B-DAT
PAT O
trained O
on O
MSCOCO O
struggles O

- B-DAT
cally O
looking O
faces O
at O
high O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
PAT-F O
has O
significantly O
better O
performance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT

- B-DAT
mization. O
2015 O

- B-DAT
earities O
improve O
neural O
network O
acoustic O

- B-DAT

- B-DAT

- B-DAT
sentation O
learning O
with O
deep O
convolutional O

- B-DAT
sarial O
networks. O
In O
ICLR, O
2016 O

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT
mation O
fidelity O
criterion O
for O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
inative O
generative O
networks. O
In O
ECCV O

and O
27.42dB O
on O
Set5, O
Set14, O
BSD100 B-DAT

and O
27.42dB O
on O
Set5, O
Set14, O
BSD100 B-DAT
and O
Urban100, O
respectively. O
As O
a O

datasets O
Set5 O
[3], O
Set14 O
[54], O
BSD100 B-DAT
[33] O
and O
Urban100 O
[19]. O
The O

BSD100 B-DAT
×3 O
27.20 O
/ O
0.738 O
28.40 O

- B-DAT

- B-DAT
age O
super-resolution O
(SISR). O
However, O
existing O

- B-DAT

- B-DAT

- B-DAT
age O
is O
bicubicly O
downsampled O
from O

- B-DAT

- B-DAT
over, O
they O
lack O
scalability O
in O

- B-DAT
blindly O
deal O
with O
multiple O
degradations O

- B-DAT
ity O
stretching O
strategy O
that O
enables O

- B-DAT

- B-DAT
put. O
Consequently, O
the O
super-resolver O
can O

- B-DAT

- B-DAT
duce O
favorable O
results O
on O
multiple O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put. O
As O
a O
classical O
problem O

- B-DAT
lenging O
research O
topic O
in O
the O

- B-DAT

- B-DAT
nel O
k O
and O
a O
latent O

- B-DAT
pling O
operation O
with O
scale O
factor O

- B-DAT
tive O
white O
Gaussian O
noise O
(AWGN O

- B-DAT
gories, O
i.e., O
interpolation-based O
methods, O
model-based O

- B-DAT
timization O
methods O
and O
discriminative O
learning O

- B-DAT

- B-DAT

- B-DAT
linear O
and O
bicubic O
interpolators O
are O

- B-DAT
age O
priors O
(e.g., O
the O
non-local O

- B-DAT

- B-DAT
based O
optimization O
methods O
are O
flexible O

- B-DAT
ative O
high-quality O
HR O
images, O
but O

- B-DAT

- B-DAT
tegration O
of O
convolutional O
neural O
network O

- B-DAT

- B-DAT
ciency O
to O
some O
extent, O
it O

- B-DAT
backs O
of O
model-based O
optimization O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
networks O
[16, O
18, O
21 O

- B-DAT
els O
based O
on O
discriminative O
CNN O

- B-DAT
els O
are O
specialized O
for O
a O

- B-DAT
ously O
when O
the O
assumed O
degradation O

- B-DAT
ever, O
little O
work O
has O
been O

- B-DAT
ing O
questions, O
which O
are O
the O

- B-DAT
thetic O
data O
to O
train O
a O

- B-DAT
ing O
these O
two O
questions O

- B-DAT

- B-DAT
resolution O
network. O
In O
view O
of O

- B-DAT
sionality O
stretching O
strategy O
which O
facilitates O

- B-DAT
edge, O
there O
is O
no O
attempt O

- B-DAT

- B-DAT
binations O
of O
blur O
kernels O
and O

- B-DAT
AWGN), O
we O
can O
select O
the O

- B-DAT
sult. O
It O
turns O
out O
that O

- B-DAT
sults O
on O
real O
LR O
images O

- B-DAT

- B-DAT
tion O
and O
works O
for O
multiple O

- B-DAT

- B-DAT

- B-DAT
resolution O
network O
learned O
from O
synthetic O

- B-DAT
of-the-art O
SISR O
methods O
on O
synthetic O

- B-DAT

- B-DAT

- B-DAT
work O
(SRCNN) O
was O
proposed. O
In O

- B-DAT
resolution O
and O
empirically O
showed O
that O

- B-DAT
ment O
of O
CNN O
super-resolvers. O
To O

- B-DAT
resolution O
(VDSR) O
method O
with O
residual O

- B-DAT

- B-DAT
ically O
demonstrated O
that O
a O
single O

- B-DAT
tiple O
scales O
super-resolution, O
image O
deblocking O

- B-DAT
put, O
which O
not O
only O
suffers O

- B-DAT
rectly O
manipulating O
the O
LR O
input O

- B-DAT
ing O
operation O
at O
the O
end O

- B-DAT

- B-DAT
scale O
the O
LR O
feature O
maps O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
layers O
are O
used O
to O

- B-DAT

- B-DAT

- B-DAT
teresting O
line O
of O
CNN-based O
methods O

- B-DAT
yond O
bicubic O
degradation O
adopt O
a O

- B-DAT

- B-DAT

- B-DAT
ever, O
manually O
selecting O
the O
hyper-parameters O

- B-DAT
able O
to O
learn O
a O
single O

- B-DAT

- B-DAT
ertheless, O
our O
method O
is O
general O

- B-DAT
cussion O
on O
blur O
kernel O
k O

- B-DAT
lar O
choice O
is O
isotropic O
Gaussian O

- B-DAT
cal O
and O
theoretical O
analyses O
have O

- B-DAT
ticated O
image O
priors O
[12]. O
Specifically O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
pre-processing O
step O
tends O
to O

- B-DAT

- B-DAT
mance O
[43]. O
Thus, O
it O
would O

- B-DAT

- B-DAT
pler O
since O
when O
k O
is O

- B-DAT

- B-DAT
tion O
model. O
It O
should O
be O

- B-DAT
lenging O
task O
since O
the O
degradation O

- B-DAT
ample). O
One O
relevant O
work O
is O

- B-DAT

- B-DAT
timization O
method O
and O
thus O
suffers O

- B-DAT
pects. O
First, O
our O
method O
considers O

- B-DAT
dation O
model. O
Second, O
our O
method O

- B-DAT

- B-DAT
essarily O
derived O
under O
the O
traditional O

- B-DAT
nections O
between O
the O
MAP O
principle O

- B-DAT
anism O
of O
CNN. O
Consequently, O
more O

- B-DAT
tecture O
design O
can O
be O
obtained O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fore, O
the O
MAP O
solution O
of O

- B-DAT

- B-DAT
lated O
as O

- B-DAT
dation O
process, O
accurate O
modeling O
of O

- B-DAT
based O
SISR O
methods O
with O
bicubic O

- B-DAT
form O
generic O
image O
super-resolution O
with O

- B-DAT
dicates O
that O
the O
parameters O
of O

- B-DAT
solve O
this O
problem O

- B-DAT
nel O
is O
first O
projected O
onto O

- B-DAT

- B-DAT
ear O
space O
by O
the O
PCA O

- B-DAT
nique. O
After O
that, O
the O
concatenated O

- B-DAT
dation O
maps O
M O
of O
size O

- B-DAT

- B-DAT
ing O
CNN O
possible O
to O
handle O

- B-DAT
ant O
degradations O
by O
considering O
the O

- B-DAT

- B-DAT

- B-DAT
plex O
architectural O
engineering. O
Typically, O
to O

- B-DAT

- B-DAT

- B-DAT
fied O
Linear O
Units O
(ReLU) O
[26 O

- B-DAT

- B-DAT
volutional O
layer O
to O
convert O
multiple O

- B-DAT
tional O
layers O
is O
set O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sign O
due O
to O
the O
following O

- B-DAT
ever, O
such O
blind O
model O
does O

- B-DAT
pected. O
First, O
the O
performance O
deteriorates O

- B-DAT
ing O
the O
blur O
kernel O
to O

- B-DAT
ferent O
HR O
images O
with O
pixel O

- B-DAT
vate O
the O
pixel-wise O
average O
problem O

- B-DAT

- B-DAT
ization O
ability O
and O
performs O
poorly O

- B-DAT

- B-DAT

- B-DAT
ity, O
one O
can O
treat O
the O

- B-DAT
nel O
and O
noise O
level O
as O

- B-DAT
tion O
maps, O
the O
non-blind O
model O

- B-DAT
tween O
data O
fidelity O
term O
and O

- B-DAT

- B-DAT
cally, O
the O
kernel O
width O
ranges O

- B-DAT
ity O
density O
function O
N O
(0,Σ O

- B-DAT
nel O
is O
determined O
by O
rotation O

- B-DAT
tion O
angle O
range O
to O
[0 O

- B-DAT
out O
the O
paper, O
it O
is O

- B-DAT
rect O
downsampler. O
Alternatively, O
we O
can O

- B-DAT
pler O
↓d, O
we O
can O
find O

- B-DAT

- B-DAT

- B-DAT
tor O
3 O
and O
PCA O
eigenvectors O

- B-DAT

- B-DAT
nel O
and O
a O
noise O
level O

- B-DAT
dation O
maps) O
for O
each O
epoch O

- B-DAT

- B-DAT

- B-DAT
tained O
by O
fine-tuning O
SRMD, O
its O

- B-DAT
cal O
GPU. O
The O
training O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
used O
datasets. O
As O
one O
can O

- B-DAT
rable O
results O
with O
VDSR O
at O

- B-DAT
forms O
VDSR O
at O
large O
scale O

- B-DAT
geNet O
dataset O
[26] O
to O
train O

- B-DAT
MDNF O
on O
scale O
factor O
4 O

- B-DAT
form O
other O
competing O
methods. O
The O

- B-DAT
plicit O
prior O
learning O
and O
thus O

- B-DAT
ment. O
This O
also O
can O
explain O

- B-DAT
parison, O
the O
run O
time O
of O

- B-DAT
ods. O
One O
can O
see O
that O

- B-DAT
petitive O
performance O
against O
other O
methods O

- B-DAT
tion O
settings O
are O
given O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferent O
degradations O
on O
Set5 O
are O

- B-DAT
ticular, O
the O
PSNR O
gain O
of O

- B-DAT

- B-DAT
Noise O
PSNR O
(×2/×3/×4)Width O
sampler O
Level O

- B-DAT

- B-DAT
pler. O
The O
visual O
comparison O
is O

- B-DAT
tially O
variant O
blur O
kernels O
and O

- B-DAT
cally O
downsampled O
from O
HR O
images O

- B-DAT
nels O
and O
corrupted O
by O
AWGN O

- B-DAT

- B-DAT
parison O

- B-DAT
sian O
kernels O
in O
training, O
it O

- B-DAT
ing. O
To O
find O
the O
degradation O

- B-DAT
cally, O
the O
kernel O
width O
is O

- B-DAT

- B-DAT
pression O
artifacts, O
Waifu2x O
[49] O
is O

- B-DAT
son. O
For O
image O
“Chip” O
which O

- B-DAT

- B-DAT
satisfying O
artifacts O
but O
also O
produce O

- B-DAT
ure O
9, O
we O
can O
see O

- B-DAT
duce O
over-smoothed O
results, O
whereas O
SRMD O

- B-DAT

- B-DAT
dations O
via O
a O
single O
model O

- B-DAT
based O
SISR O
methods, O
the O
proposed O

- B-DAT

- B-DAT
ically, O
degradation O
maps O
are O
obtained O

- B-DAT
sionality O
stretching O
of O
the O
degradation O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tially O
variant O
degradations. O
Moreover, O
the O

- B-DAT
struct O
visually O
plausible O
HR O
images O

- B-DAT
posed O
super-resolver O
offers O
a O
feasible O

- B-DAT
tical O
CNN-based O
SISR O
applications O

- B-DAT

- B-DAT
ity O
Enhancement O
of O
Surveillance O
Images O

- B-DAT
ration O
for O
providing O
us O
the O

- B-DAT
search O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
shops, O
volume O
3, O
pages O
126–135 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
Conference, O
2012 O

- B-DAT
shift O
priors O
for O
image O
restoration O

- B-DAT
formation O
Processing O
Systems, O
2017 O

- B-DAT
tion O
diffusion O
processes O
for O
effective O

- B-DAT
tion, O
pages O
5261–5269, O
2015 O

- B-DAT

- B-DAT

- B-DAT
pean O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
ference O
on O
Computer O
Vision, O
pages O

- B-DAT
ized O
sparse O
representation O
for O
image O

- B-DAT
actions O
on O
Image O
Processing, O
22(4):1620–1630 O

- B-DAT
resolution. O
In O
IEEE O
International O
Conference O

- B-DAT
resolution O
via O
BM3D O
sparse O
coding O

- B-DAT
resolution O
and O
texture O
synthesis. O
Advances O

- B-DAT
dom O
Fields O
for O
Vision O
and O

- B-DAT

- B-DAT
puter O
Vision, O
pages O
349–356, O
2009 O

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
nets. O
In O
Advances O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
network O
for O
image O
super-resolution O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT
timization. O
In O
International O
Conference O
for O

- B-DAT
sentations, O
2015 O

- B-DAT
resolution. O
In O
IEEE O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
network. O
In O
IEEE O

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT
tion O
Workshops, O
pages O
136–144, O
July O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
tional O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT
tional O
neural O
networks. O
In O
Advances O

- B-DAT
tioned O
regression O
models O
for O
non-blind O

- B-DAT
resolution. O
In O
IEEE O
International O
Conference O

- B-DAT
curate O
image O
super O
resolution. O
IEEE O

- B-DAT
putational O
Imaging, O
3(1):110–125, O
2017 O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
preserving O
image O
super-resolution O
via O
contextualized O

- B-DAT
task O
learning. O
IEEE O
Transactions O
on O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
tional O
Conference O
on O
Computer O
Vision O

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT

- B-DAT
ral O
networks O
for O
matlab. O
In O

- B-DAT

- B-DAT

- B-DAT
tural O
similarity. O
IEEE O
Transactions O
on O

- B-DAT

- B-DAT
resolution: O
A O
benchmark. O
In O
European O

- B-DAT
puter O
Vision, O
pages O
372–386, O
2014 O

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT

- B-DAT
cessing, O
26(12):5895–5907, O
2017 O

- B-DAT

- B-DAT

- B-DAT
gle O
image O
super-resolution O
under O
internet O

- B-DAT
ference O
on O
Multimedia, O
pages O
677–687 O

- B-DAT
yond O
a O
gaussian O
denoiser: O
Residual O

- B-DAT
ing, O
pages O
3142–3155, O
2017 O

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
and O

4 O
on O
datasets O
Set5, O
Set14, O
BSD100 B-DAT

20] O
for O
testing O
with O
three O
upscaling B-DAT
factors: O
×2, O
×3 O
and O
×4 O

training O
images O
for O
all O
three O
upscaling B-DAT
factors: O
×2, O
×3 O
and O
×4 O

model O
for O
all O
these O
three O
upscaling B-DAT
factors O
as O
in O
[21, O
37 O

best O
result O
across O
all O
the O
upscaling B-DAT
factors O
and O
datasets. O
Visual O
results O

image O
super-resolution O
results O
with O
×4 O
upscaling B-DAT

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests O

sets: O
Set5 O
[1], O
Set14 O
[50], O
BSD100 B-DAT
[30] O
and O
Urban100 O
[20] O
for O

4 O
on O
datasets O
Set5, O
Set14, O
BSD100 B-DAT
and O
Urban100. O
The O
best O
performance O

BSD100 B-DAT
×2 O
31.36/0.8879 O
31.90/0.8960 O
31.85/0.8942 O
31.80/0.895 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

the O
advantage O
of O
RNN O
architecture O
- B-DAT
the O
correlation O
information O
is O
propagated O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
based O
image O
restoration O
approaches. O
The O

- B-DAT

- B-DAT

- B-DAT
sity O
[28, O
46]. O
Alternatively, O
similar O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
is O
adopted O
to O
save O

- B-DAT
sides O
CNNs, O
RNNs O
have O
also O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
methods O
based O
on O
low-rankness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
vision O
tasks. O
However, O
unlike O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-3 B-DAT
and O
reduce O
it O
by O
half O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
local O
modules, O
we O
implement O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-6 B-DAT
from O
NLRN O
with O
unrolling O
length O

- B-DAT

- B-DAT
of-the-art O
network O
models O
on O
Set12 O

- B-DAT
plexities O
are O
also O
compared O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
layers) O
of O
NLRN. O
The O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

4 O
on O
datasets O
Set5, O
Set14, O
BSD100 B-DAT

23]. O
By O
fol- O
lowing O
[23], O
BSD100 B-DAT

SR O
results O
of O
“8023” O
from O
BSD100 B-DAT

of O
the O
art O
for O
large O
upscaling B-DAT
factors O
(×4). O
EDSR O
[16 O

for O
bench- O
mark. O
Moreover, O
the O
BSD100 B-DAT
[18], O
consisting O
of O
100 O
nat O

BSD100 B-DAT
×2 O
29.56/0.8431 O
31.36/0.8879 O
31.90/0.8960 O
31.85/0.8942 O

4 O
on O
datasets O
Set5, O
Set14, O
BSD100 B-DAT
and O
Urban100. O
Red O
color O
indicates O

23]. O
By O
fol- O
lowing O
[23], O
BSD100 B-DAT
is O
not O
evaluated. O
Its O
obvious O

SR O
results O
of O
“8023” O
from O
BSD100 B-DAT
with O
scale O
factor O
×4. O
The O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ual O
Learning O
and O
Convolutional O
Sparse O

- B-DAT

- B-DAT
Threshold O
Algorithm O
(LISTA). O
We O
extend O

- B-DAT
volutional O
version O
and O
build O
the O

- B-DAT
tional O
sparse O
codings O
of O
input O

- B-DAT

- B-DAT
mark O
datasets O
demonstrate O
the O
effectiveness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
arts, O
e.g., O
DRRN O
(52 O
layers O

- B-DAT
sults O
are O
available O
at O
https://github.com/axzml O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rithms, O
especially O
the O
current O
leading O

- B-DAT

- B-DAT
ods O
[26, O
4, O
13, O
14 O

- B-DAT
lem O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sent O
our O
models. O
RL-CSC O
with O

- B-DAT
petitive O
performance O
with O
MemNet O
[24 O

- B-DAT
ters, O
the O
performance O
of O
RL-CSC O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
methods O
[32]. O
Inspired O
by O

- B-DAT

- B-DAT
ers O
named O
VDSR, O
which O
shows O

- B-DAT

- B-DAT
gradient O
problem O
when O
the O
network O

- B-DAT

- B-DAT
work O
(DRCN) O
[14] O
with O
a O

- B-DAT
nary O
success O
of O
ResNet O
[9 O

- B-DAT

- B-DAT

- B-DAT
cursive O
manner, O
leading O
to O
a O

- B-DAT
decoding O
network O
named O
RED-Net O
was O

- B-DAT
tioned O
models O
usually O
lack O
convincing O

- B-DAT
plored, O
i.e., O
what O
role O
each O

- B-DAT
resentation O
with O
strong O
theoretical O
support O

- B-DAT

- B-DAT

- B-DAT
ment O
when O
the O
number O
of O

- B-DAT
posed O
CSC O
based O
SR O
(CSC-SR O

- B-DAT

- B-DAT
tional O
sparse O
coding O
methods. O
In O

- B-DAT
putationally O
efficient O
CSC O
model, O
Sreter O

- B-DAT
duced O
a O
convolutional O
recurrent O
sparse O

- B-DAT

- B-DAT
tending O
the O
LISTA O
method O
to O

- B-DAT
ing O
tasks O

- B-DAT

- B-DAT

- B-DAT
posed O
in O
the O
field O
of O

- B-DAT
tion, O
our O
model, O
termed O
as O

- B-DAT

- B-DAT
defined O
interpretability O

- B-DAT
CSC O
(30 O
layers) O
has O
achieved O

- B-DAT

- B-DAT
mizes O
the O
objective O
function O
(1 O

- B-DAT
ting O
term O
and O
an O
`1-norm O

- B-DAT

- B-DAT
ization O
coefficient O
λ O
is O
used O

- B-DAT

- B-DAT
ative O
Shrinkage O
Thresholding O
Algorithm O
(ISTA O

- B-DAT

- B-DAT

- B-DAT
dress O
this O
issue, O
Gregor O
and O

- B-DAT
gorithm O
termed O
as O
Learned O
ISTA O

- B-DAT
proximate O
estimates O
of O
sparse O
code O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ters O
We, O
G O
and O
θ O

- B-DAT

- B-DAT
vide O
the O
whole O
image O
into O

- B-DAT
actly O
the O
same O
, O
is O

- B-DAT
able O
for O
this O
issue, O
as O

- B-DAT
ternating O
Direction O
Method O
of O
Multipliers O

- B-DAT

- B-DAT

- B-DAT
ory O
burden O
issue O
of O
ADMM O

- B-DAT

- B-DAT
put O
Interpolated O
LR O
(ILR) O
image O

- B-DAT
gency O
speed O
and O
the O
reconstruction O

- B-DAT
lated O
Low-Resolution O
(ILR) O
image O
Iy O

- B-DAT
dicts O
the O
output O
HR O
image O

- B-DAT
verting O
one O
of O
the O
inputs O

- B-DAT
tive O
tool O
to O
learn O
the O

- B-DAT

- B-DAT
portant O
facts: O
(1) O
the O
expressiveness O

- B-DAT

- B-DAT

- B-DAT
frequency O
information O
reconstruction O

- B-DAT
structed O
by O
the O
addition O
of O

- B-DAT

- B-DAT
tion, O
W1 O
and O
S O
for O

- B-DAT
ery O
recursion. O
When O
K O
recursions O

- B-DAT
ing O
process, O
the O
depth O
d O

- B-DAT
ploited O
in O
our O
training O
process O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dient O
descent O
(SGD) O
is O
used O

- B-DAT
ment O
our O
model O
using O
the O

- B-DAT

- B-DAT
fied O
structures O
of O
these O
models O

- B-DAT

- B-DAT

- B-DAT
put) O
and O
a O
pre-activation O
structure O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eter O
settings O
for O
better O
performance O

- B-DAT

- B-DAT

- B-DAT
tween O
SCN O
[29] O
and O
RL-CSC O

- B-DAT

- B-DAT
ing O
linear O
layers, O
so O
more O

- B-DAT

- B-DAT
works. O
With O
the O
help O
of O

- B-DAT

- B-DAT
fers O
with O
DRCN O
[14] O
in O

- B-DAT
ual O
Learning O
[23] O
(LRL) O
and O

- B-DAT

- B-DAT
sides, O
DRCN O
is O
not O
easy O

- B-DAT

- B-DAT

- B-DAT
ate O
predictions) O
is O
used O
to O

- B-DAT

- B-DAT

- B-DAT
troduction O
to O
the O
datasets O
used O

- B-DAT
isons O
with O
state-of-the-arts O
are O
presented O

- B-DAT

- B-DAT
tion O
Dataset O
[18]. O
During O
testing O

- B-DAT
mark. O
Moreover, O
the O
BSD100 O
[18 O

- B-DAT
ural O
images O
are O
used O
for O

- B-DAT

- B-DAT

- B-DAT
minance) O
of O
transformed O
YCbCr O
space O

- B-DAT
cludes O
flipping O
(horizontally O
and O
vertically O

- B-DAT
formed O
on O
each O
image O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tioned O
into O
33× O
33 O
patches O

- B-DAT

- B-DAT
CSC O
is O
30 O
according O
to O

- B-DAT
mized O
using O
SGD O
with O
mini-batch O

- B-DAT

- B-DAT

- B-DAT
ther O
improvements O
of O
the O
loss O

- B-DAT
dient O
clipping O
strategy O
stated O
in O

- B-DAT
ter. O
A O
NVIDIA O
Titan O
Xp O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bic O
interpolation O
to O
the O
original O

- B-DAT
els O
near O
borders O
before O
evaluation O

- B-DAT
mark O
testing O
sets, O
and O
results O

- B-DAT

- B-DAT
lowing O
[23], O
BSD100 O
is O
not O

- B-DAT

- B-DAT

- B-DAT
formance O
(K O
= O
15 O
33.98dB O

- B-DAT
ter. O
Similar O
conclusions O
are O
observed O

- B-DAT
tempt O
to O
combine O
the O
powerful O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ficulties, O
but O
helps O
network O
converge O

- B-DAT

- B-DAT
tings O
as O
stated O
in O
Section O

- B-DAT
ters. O
Specifically, O
two O
types O
of O

- B-DAT
plied O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
terpart. O
Tests O
on O
Set5 O
with O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
datas O
are O
derived O
from O

- B-DAT

- B-DAT
fault O
settings O
given O
by O
the O

- B-DAT

- B-DAT
Memory O
(OOM) O
issue. O
The O
reason O

- B-DAT

- B-DAT

- B-DAT
terpretability. O
We O
extend O
the O
LISTA O

- B-DAT
volutional O
version O
and O
build O
the O

- B-DAT
sions O
without O
introducing O
any O
new O

- B-DAT
sults O
with O
state-of-the-arts O
and O
demonstrate O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
deep O
convolutional O
networks O

- B-DAT

- B-DAT
nary O
Learning: O
A O
Comparative O
Review O

- B-DAT

Delving O
Deep O
into O
Rec- B-DAT
tifiers O
- O
Surpassing O
Human-Level O
Performance O
on O
ImageNet O

- B-DAT
ing O
for O
Image O
Recognition. O
In O

- B-DAT
ble O
convolutional O
sparse O
coding. O
CVPR O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
CVPR O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT
lutional O
Network O
for O
Image O
Super-Resolution O

- B-DAT
ham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT

- B-DAT
ing O
a O
Generative O
Adversarial O
Network O

- B-DAT

- B-DAT
ing O
very O
deep O
convolutional O
encoder-decoder O

- B-DAT
cal O
statistics. O
volume O
2, O
pages O

- B-DAT
Vito, O
Z. O
Lin, O
A. O
Desmaison O

- B-DAT
matic O
differentiation O
in O
pytorch. O
In O

- B-DAT

- B-DAT

- B-DAT
ing. O
In O
ICASSP, O
pages O
2191–2195 O

- B-DAT

- B-DAT
tent O
memory O
network O
for O
image O

- B-DAT

- B-DAT

Challenge O
on O
Single O
Image O
Super-Resolution B-DAT
- O
Methods O
and O
Results. O
CVPR O
Workshops O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution: O
A O
benchmark. O
In O
ECCV O

- B-DAT
age O
super-resolution O
via O
sparse O
representation O

- B-DAT

- B-DAT
convolutional O
networks. O
CVPR, O
2010. O
2 O

- B-DAT

- B-DAT

A O
Survey O
of O
Sparse O
Representation O
- B-DAT
Algorithms O
and O
Applications. O
IEEE O
Access O

branch O
to O
reconstruct O
the O
HR O
4x B-DAT
image O

P40 O
GPU. O
We O
tested O
on O
BSD100 B-DAT
[32] I-DAT
for O
scale O
x4. O
As O

SCALE O
X4 O
ON O
SET5, O
SET14, O
BSD100 B-DAT

high-resolution O
(HR) O
images. O
When O
the O
upscaling B-DAT
factor O
is O
large O

extra O
computation. O
Moreover, O
for O
large O
upscaling B-DAT
factors, O
our O

high O
quality O
images O
for O
higher O
upscaling B-DAT
factors O

model O
could O
do O
multi O
scale O
upscaling B-DAT
task O
via O
Laplacian O
Pyramid O

upscaling B-DAT
factors O
and O
decreasing O
parameters O
by O

We O
first O
upscaling B-DAT
our O
low-resolution O
image O
via O
a O

Berkeley O
segmentation O
dataset O
[32] O
(BSD100) B-DAT
and O
a O
dataset O
of O

P40 O
GPU. O
We O
tested O
on O
BSD100 B-DAT

SCALE O
X4 O
ON O
SET5, O
SET14, O
BSD100 B-DAT
AND O
URBAN100 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-the-art O
methods O
in O
scale O
x4 O

-5 B-DAT

- B-DAT

- B-DAT

-11 B-DAT

- B-DAT
learning-based O
super O
resolution O
methods O
have O

- B-DAT

- B-DAT

- B-DAT
resolution O
simultaneously O
in O
one O
feed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
quality O
high-resolution O
image, O
the O
process O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-2798 B-DAT

- B-DAT

-8 B-DAT

- B-DAT

- B-DAT

-2873 B-DAT

- B-DAT

- B-DAT

-3478 B-DAT

- B-DAT

- B-DAT

-126 B-DAT

- B-DAT
based O
single O
image O
super O
resolu-tion[C]//Proceedings O

- B-DAT
1873 O

- B-DAT

- B-DAT

-5206 B-DAT

- B-DAT

-1105 B-DAT

-551 B-DAT

- B-DAT

-2324 B-DAT

- B-DAT

-307 B-DAT

- B-DAT

- B-DAT

-778 B-DAT

- B-DAT

- B-DAT

- B-DAT
1654 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1645 B-DAT

- B-DAT

- B-DAT

-4547 B-DAT

- B-DAT

-4817 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-711 B-DAT

- B-DAT

- B-DAT

- B-DAT
2423 O

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adver-sarial O

- B-DAT

- B-DAT

-4500 B-DAT

-612 B-DAT

-456 B-DAT

- B-DAT

- B-DAT

- B-DAT
image O
super-resolution O
based O
on O
nonnegative O

- B-DAT

- B-DAT
representations[C]//International O
conference O
on O
curves O
and O

-730 B-DAT

-423 B-DAT

- B-DAT

-407 B-DAT

these O
datasets, O
Set5, O
Set14 O
and O
BSD100 B-DAT

The O
“8023” O
image O
from O
the O
BSD100 B-DAT

testing O
the O
im- O
ages O
in O
BSD100 B-DAT

the O
average O
inference O
time O
for O
upscaling B-DAT
3× O
on O
Set5. O
The O
IDN O

restoration O
per- O
formance O
with O
larger O
upscaling B-DAT
factors, O
the O
recent O
SR O
meth O

the O
original O
HR O
images O
with O
upscaling B-DAT
factor O
m O
(m O
= O
2 O

the O
Set14 O
dataset O
with O
an O
upscaling B-DAT
factor O
4 O

the O
BSD100 O
dataset O
with O
an O
upscaling B-DAT
factor O
4 O

the O
Urban100 O
dataset O
with O
an O
upscaling B-DAT
factor O
4 O

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

datasets: O
Set5 O
[1], O
Set14 O
[27], O
BSD100 B-DAT
[18], O
Urban100 O
[10]. O
Among O
these O

datasets, O
Set5, O
Set14 O
and O
BSD100 B-DAT
consist O
of O
natural O
scenes O
and O

BSD100 B-DAT
×2 O
29.56/0.8431 O
31.90/0.8960 O
31.85/0.8942 O
31.80/0.8952 O

BSD100 B-DAT
×2 O
5.619 O
7.494 O
7.577 O
7.715 O

BSD100 B-DAT
×2 O
0.071 O
0.983 O
0.018 O
4.430 O

The O
“8023” O
image O
from O
the O
BSD100 B-DAT
dataset O
with O
an O
upscaling O
factor O

testing O
the O
im- O
ages O
in O
BSD100 B-DAT
and O
Urban100 O
datasets, O
we O
divide O

- B-DAT

- B-DAT
age O
super-resolution. O
However, O
as O
the O

- B-DAT

- B-DAT

- B-DAT
ods O
have O
been O
faced O
with O

- B-DAT
pact O
convolutional O
network O
to O
directly O

- B-DAT
tion O
block, O
the O
local O
long O

- B-DAT

- B-DAT
fectively O
extracted. O
Specifically, O
the O
proposed O

- B-DAT
quential O
blocks. O
In O
addition, O
the O

- B-DAT
tion. O
Experimental O
results O
demonstrate O
that O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cially O
in O
terms O
of O
time O

- B-DAT

- B-DAT

- B-DAT
lem O
in O
low-level O
computer O
vision O

- B-DAT

- B-DAT

- B-DAT
age. O
Actually, O
an O
infinite O
number O

- B-DAT

- B-DAT
ists. O
In O
order O
to O
mitigate O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formance O
with O
larger O
upscaling O
factors O

- B-DAT
ods O
fall O
into O
the O
example-based O

- B-DAT
ral O
network O
(CNN), O
many O
CNN-based O

- B-DAT
mance. O
Kim O
et O
al. O
propose O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ment O
of O
enormous O
parameters O
of O

- B-DAT
der O
to O
achieve O
better O
performance O

- B-DAT
ory O
consumption, O
which O
are O
less O

- B-DAT
caded O
network O
topologies, O
e.g., O
VDSR O

- B-DAT
brating O
channel-wise O
features O
responses O
can O

- B-DAT
mation O
distillation O
network O
(IDN) O
with O

- B-DAT
ters O
and O
computational O
complexity O
as O

- B-DAT
formation O
distillation O
blocks O
(DBlocks) O
are O

- B-DAT
gressively O
distill O
residual O
information. O
Finally O

- B-DAT
tion O
Block O
(RBlock) O
aggregates O
the O

- B-DAT

- B-DAT
tion O
block, O
which O
contains O
an O

- B-DAT
pression O
unit. O
The O
enhancement O
unit O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pressive O
power, O
we O
send O
a O

- B-DAT

- B-DAT

- B-DAT
ture O
maps O
into O
two O
parts O

- B-DAT
path O
features O
and O
another O
expresses O

- B-DAT

- B-DAT

- B-DAT
mary, O
the O
enhancement O
unit O
is O

- B-DAT
sentation O
power O
of O
the O
network O

- B-DAT
ber O
of O
convolutional O
layer O

- B-DAT

- B-DAT

- B-DAT
tains O
better O
reconstruction O
accuracy O

- B-DAT

- B-DAT
ied O
in O
these O
years. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
similarity O
property O
and O
extract O
example O

- B-DAT
terns O
and O
textures O
but O
lacks O

- B-DAT
tory O
prediction O
for O
images O
of O

- B-DAT

- B-DAT
spective O
deformation O

- B-DAT

- B-DAT

- B-DAT
pact O
dictionary O
or O
manifold O
space O

- B-DAT
dom O
forest O
[20] O
and O
sparse O

- B-DAT
mal O
for O
generating O
high-quality O
SR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
to O
accelerate O
SRCNN O
in O

- B-DAT
signed O
by O
Kim O
et O
al O

- B-DAT
tion O
with O
skip O
connection O
to O

- B-DAT
decoder O
networks O
and O
symmetric O
skip O

- B-DAT

- B-DAT
work O
(LapSRN) O
to O
address O
the O

- B-DAT

- B-DAT
ages. O
Tai O
et O
al. O
[22 O

- B-DAT
work O
to O
effectively O
build O
a O

- B-DAT
racy. O
The O
authors O
also O
present O

- B-DAT

- B-DAT

- B-DAT
sistent O
memory O
network O
(MemNet) O
[23 O

- B-DAT
tion O
task, O
which O
tackles O
the O

- B-DAT

- B-DAT
pose O
a O
novel O
combination O
of O

- B-DAT
construction O
block O
(RBlock). O
Here, O
we O

- B-DAT
formation O
distillation O
blocks O
by O
using O

- B-DAT

- B-DAT

- B-DAT
tively. O
Finally, O
we O
take O
a O

- B-DAT

- B-DAT
age O
restoration O
as O
defined O
below O

- B-DAT
mulated O
as O
follows O

- B-DAT

- B-DAT
lutions O
and O
another O
is O
the O

- B-DAT

- B-DAT
lows O

- B-DAT
while O
is O
the O
input O
of O

- B-DAT

- B-DAT
eration O
respectively. O
Specifically, O
we O
know O

- B-DAT

- B-DAT
path O
information O
as O
the O
input O

- B-DAT

- B-DAT
ations O
of O
the O
below O
module O

- B-DAT
path O
information O
and O
the O
local O

- B-DAT

- B-DAT
mulated O
as O

- B-DAT

- B-DAT

- B-DAT
lized O
without O
exception O
by O
a O

- B-DAT
tage O
of O
a O
1× O
1 O

- B-DAT
tation O
Dataset O
(BSD) O
[18] O
as O

- B-DAT
tation O
in O
three O
ways: O
(1 O

- B-DAT
channel, O
while O
color O
components O
are O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferent O
scaling O
factors O

- B-DAT
sample O
the O
original O
HR O
images O

- B-DAT
erate O
the O
corresponding O
LR O
images O

- B-DAT

- B-DAT

- B-DAT
ters O
will O
generate O
the O
output O

- B-DAT

- B-DAT
mum O
size O
of O
the O
sub-image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ble O
1 O

- B-DAT

- B-DAT

- B-DAT
volution O
layer O
[6, O
24] O
in O

- B-DAT

- B-DAT

- B-DAT
ters O
as O
the O
initial O
values O

- B-DAT
terfly” O
image O
from O
Set5 O
dataset O

- B-DAT
ture O
information O
and O
its O
normalized O

pixel O
value O
ranges O
from O
-0 B-DAT

- B-DAT
ter O
visualizing O
the O
intermediary O
of O

- B-DAT
age O
feature O
map O
can O
roughly O

- B-DAT
ment O
unit O
and O
compression O
unit O

- B-DAT
ing O
above-mentioned O
method. O
As O
illustrated O

- B-DAT
figures O
show O
that O
the O
later O

- B-DAT
creasing O
the O
pixel O
values O
to O

- B-DAT
atively O
clear O
contour O
profile. O
In O

- B-DAT
ure O
obviously O
surpasses O
the O
former O

- B-DAT
paring O
Figure O
5(a) O
with O
Figure O

- B-DAT
ure O
5(b) O
and O
the O
third O

- B-DAT
age. O
The O
bias O
term O
of O

- B-DAT
matically O
adjust O
the O
central O
value O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods, O
including O
bicubic, O
SRCNN O
[3 O

- B-DAT

- B-DAT

- B-DAT
vorably O
against O
state-of-the-art O
results O
on O

- B-DAT
man O
perception O
of O
image O
super-resolution O

- B-DAT
bara” O
image O
has O
serious O
artifacts O

- B-DAT
viously O
see O
that O
the O
proposed O

- B-DAT
tively O
clear O
in O
the O
proposed O

- B-DAT
put O
so O
that O
more O
information O

- B-DAT
age. O
The O
algorithms O
that O
take O

- B-DAT
chine O
with O
4.2GHz O
Intel O
i7 O

- B-DAT
ages O
in O
BSD100 O
and O
Urban100 O

- B-DAT
ages O
into O
several O
parts O
and O

- B-DAT
worthy O
that O
the O
proposed O
IDN O

- B-DAT
ban100 O
dataset O

- B-DAT
cient O
features O
for O
the O
reconstruction O

- B-DAT
posed O
approach O
achieves O
competitive O
results O

- B-DAT
mark O
datasets O
in O
terms O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pact O
network O
will O
be O
more O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
rable O
convolutions. O
In O
CVPR, O
pages O

- B-DAT
based O
super-resolution. O
IEEE O
Computer O
Graphics O

- B-DAT
plications, O
22(2):56–65, O
2002. O
2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works. O
In O
arXiv:1709.01507, O
2017. O
2 O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
ishick, O
S. O
Guadarrama, O
and O
T O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT
tional O
network O
for O
image O
super-resolution O

- B-DAT
resolution. O
In O
CVPR, O
pages O
624–632 O

- B-DAT

- B-DAT
ing O
very O
deep O
convolutional O
encoder-decoder O

- B-DAT
cal O
statistics. O
In O
CVPR, O
pages O

- B-DAT

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT
tent O
memory O
network O
for O
image O

- B-DAT
resolution O
as O
sparse O
representation O
of O

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT

- B-DAT

PSNR O
and O
SSIM O
results O
on O
BSD100 B-DAT

30 O
layer O
network. O
Set5 O
Set14 O
BSD100 B-DAT
s I-DAT
= O
2 O
s O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
Proc O

three O
dataset: O
Set5, O
Set14 O
and O
BSD100 B-DAT

and O
0.1dB. O
The O
results O
on O
BSD100, B-DAT
as O
shown O
in O
Table O
5 O

PSNR O
and O
SSIM O
results O
on O
BSD100 B-DAT
for O
super-resolution. O
PSNR O

30 O
layer O
network. O
Set5 O
Set14 O
BSD100 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work O
for O
image O
restoration O
such O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
layers O
act O
as O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
Recently, O
deep O
neural O
networks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lutional O
neural O
network O
(CNN)-based O
framework O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
layers, O
as O
shown O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works O
[11], O
we O
add O
skip O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT

- B-DAT

-2 B-DAT

- B-DAT

-4 B-DAT

- B-DAT

-4 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion. O
For O
our O
method, O
which O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-200 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-4 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

and×4 O
on O
datasets O
Set5, O
Set14, O
BSD100 B-DAT

row O
shows O
image O
“108005” O
from O
BSD100 B-DAT

BSD100 B-DAT
[28] O
and O
Urban100 O
[15] O
are O

BSD100 B-DAT
×2 O
29.56/0.8431 O
31.36/0.8879 O
31.90/0.8960 O
31.85/0.8942 O

and×4 O
on O
datasets O
Set5, O
Set14, O
BSD100 B-DAT
and O
Urban100. O
Dataset O
Quality O
JPEG O

row O
shows O
image O
“108005” O
from O
BSD100 B-DAT
with O
scale O
factor O
×3. O
Only O

- B-DAT
age O
restoration. O
However, O
as O
the O

- B-DAT

- B-DAT
tle O
influence O
on O
the O
subsequent O

- B-DAT
tive O
learning O
process. O
The O
recursive O

- B-DAT

- B-DAT
tive O
fields. O
The O
representations O
and O

- B-DAT
vious O
states O
should O
be O
reserved O

- B-DAT
resolution O
and O
JPEG O
deblocking. O
Comprehensive O

- B-DAT
iments O
demonstrate O
the O
necessity O
of O

- B-DAT

- B-DAT

- B-DAT
quality O
version O
of O
x, O
D O

- B-DAT
der O
Grant O
Nos. O
91420201, O
61472187 O

- B-DAT

- B-DAT
search O
Fund. O
Jian O
Yang O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
noising O
aims O
to O
recover O
a O

- B-DAT
servation, O
which O
commonly O
assumes O
additive O

- B-DAT
sian O
noise O
with O
a O
standard O

- B-DAT

- B-DAT
resolution O
recovers O
a O
high-resolution O
(HR O

- B-DAT

- B-DAT

- B-DAT
trol O
the O
parameter O
number O
of O

- B-DAT
Recursive O
Convolutional O
Network O
(DRCN) O
[21 O

- B-DAT
gate O
training O
difficulty, O
Mao O
et O

- B-DAT

- B-DAT

- B-DAT
over, O
Zhang O
et O
al. O
[40 O

- B-DAT
path O
feed-forward O
architecture, O
where O
one O

- B-DAT
fluenced O
by O
its O
direct O
former O

- B-DAT

- B-DAT
ory. O
Some O
variants O
of O
CNNs O

- B-DAT

- B-DAT
cific O
prior O
state, O
namely O
restricted O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tent O
memory O
network O
(MemNet), O
which O

- B-DAT
ory O
block O
to O
explicitly O
mine O

- B-DAT
tion O
Net O
(FENet) O
first O
extracts O

- B-DAT

- B-DAT
tains O
a O
recursive O
unit O
and O

- B-DAT
science O
[6, O
25] O
that O
recursive O

- B-DAT
ist O
in O
the O
neocortex, O
the O

- B-DAT

- B-DAT
tive O
fields O
(blue O
circles O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ated O
from O
the O
previous O
memory O

- B-DAT

- B-DAT
ther, O
we O
present O
an O
extended O

- B-DAT

- B-DAT

- B-DAT
term O
memory O
should O
be O
reserved O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
even O
using O
a O
single O

- B-DAT
veal O
that O
the O
network O
depth O

- B-DAT
tion O
and O
achieved O
comparable O
performance O

- B-DAT
resolution O
convolutional O
neural O
network O
(SRCNN O

- B-DAT
dicts O
the O
nonlinear O
LR-HR O
mapping O

- B-DAT
volutional O
network, O
which O
significantly O
outperforms O

- B-DAT
cal O
shallow O
methods. O
The O
authors O

- B-DAT
tended O
CNN O
model, O
named O
Artifacts O

- B-DAT
tional O
Neural O
Networks O
(ARCNN) O
[7 O

- B-DAT

- B-DAT
ural O
sparsity O
of O
images O
[36 O

- B-DAT

- B-DAT
edge O
in O
the O
JPEG O
compression O

- B-DAT

- B-DAT

- B-DAT
ers O
to O
exploit O
large O
contextual O

- B-DAT
ing O
and O
adjustable O
gradient O
clipping O

- B-DAT
ization O
into O
a O
DnCNN O
model O

- B-DAT
age O
restoration O
tasks. O
To O
reduce O

- B-DAT

- B-DAT
connection O
to O
mitigate O
the O
training O

- B-DAT

- B-DAT
posed O
LapSRN O
to O
address O
the O

- B-DAT
curacy O
for O
SISR, O
which O
operates O

- B-DAT

- B-DAT
ages. O
Tai O
et O
al. O
[34 O

- B-DAT
work O
(DRRN) O
to O
address O
the O

- B-DAT

- B-DAT
lutional O
layer O
is O
used O
in O

- B-DAT
ture O
mapping, O
we O
have O

- B-DAT

- B-DAT

- B-DAT
ory O
block O
respectively. O
Finally, O
instead O

- B-DAT

- B-DAT

- B-DAT
age, O
our O
model O
uses O
a O

- B-DAT
notes O
the O
function O
of O
our O

- B-DAT
ber O
of O
training O
patches O
and O

- B-DAT
quality O
patch O
of O
the O
low-quality O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
block. O
Specifically, O
each O
residual O

- B-DAT

- B-DAT

- B-DAT
erate O
multi-level O
representations O
under O
different O

- B-DAT

- B-DAT
ory. O
Supposing O
there O
are O
R O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cursive O
unit. O
These O
representations O
are O

- B-DAT

- B-DAT

- B-DAT
vious O
memory O
blocks O
can O
be O

- B-DAT

- B-DAT
volutional O
layer O
(parameterized O
by O
W O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put O
from O
the O
ensemble O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
can O
get O
lost O
at O

- B-DAT
ward O
CNN O
process, O
and O
dense O

- B-DAT
ous O
layers O
can O
compensate O
such O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferent O
networks. O
(b) O
We O
convert O

- B-DAT

- B-DAT

- B-DAT
tral O
densities O
by O
integrating O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions, O
the O
latter O
layer O
absorbs O

- B-DAT

- B-DAT

- B-DAT
work O
– O
a O
very O
deep O

- B-DAT
works, O
inspired O
by O
LSTM, O
Highway O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tween O
MemNet O
and O
DRCN O
[21 O

- B-DAT
ule O
is O
a O
memory O
block O

- B-DAT
ules O
in O
DRCN, O
which O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
egy, O
which O
is O
imperative O
for O

- B-DAT
nected O
principle. O
In O
general, O
DenseNet O

- B-DAT
tion. O
In O
addition, O
DenseNet O
adopts O

- B-DAT

- B-DAT
tions O
in O
MemNet O
indeed O
play O

- B-DAT

- B-DAT

- B-DAT
nections. O
Average O
PSNR/SSIMs O
for O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ories O
from O
the O
last O
recursion O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tors O
are O
evaluated, O
including O
×2 O

- B-DAT
noising O
is O
used. O
As O
in O

- B-DAT
ing O
time O
and O
storage O
complexities O

- B-DAT

- B-DAT

- B-DAT
ent O
noise O
levels O
are O
all O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sions, O
are O
constructed O
(i.e., O
M6R6 O

- B-DAT
supervised O
MemNet, O
6 O
predictions O
are O

- B-DAT
tions, O
and O
is O
empirically O
set O

- B-DAT
mized O
via O
the O
mini-batch O
stochastic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nections. O
The O
reason O
is O
that O

- B-DAT

- B-DAT

- B-DAT
responding O
weights O
from O
all O
filters O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
we O
normalize O
the O
norms O

- B-DAT
ture O
map O
index O
l. O
We O

- B-DAT
ory O
block O
number O
increases. O
(3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity O
of O
our O
persistent O
memory O

- B-DAT
ent O
among O
different O
work, O
we O

- B-DAT
crease O
the O
parameters O
(filter O
number O

- B-DAT

- B-DAT
mance. O
With O
more O
training O
images O

- B-DAT
nificantly O
outperforms O
the O
state O
of O

- B-DAT

- B-DAT
lem O
in O
networks, O
we O
intend O

- B-DAT
plexity O
and O
accuracy. O
Fig. O
6 O

- B-DAT
notes O
the O
prediction O
of O
the O

- B-DAT
sult O
at O
the O
3rd O
prediction O

- B-DAT
creasing O
model O
complexity O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nal O
image O
is O
resized O
to O

- B-DAT
mance. O
However, O
in O
our O
MemNet O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rectly O
recovers O
the O
pillar. O
Please O

- B-DAT
isons O
for O
SISR. O
SRCNN O
[8 O

- B-DAT
sults O
on O
Classic5 O
and O
LIVE1 O

- B-DAT
erated O
by O
their O
corresponding O
public O

- B-DAT
work O
structures: O
M4R6, O
M6R6, O
M6R8 O

- B-DAT
posed O
deepest O
network O
M10R10 O
achieves O

- B-DAT

- B-DAT

- B-DAT
ory O
network O
(MemNet) O
is O
proposed O

- B-DAT

- B-DAT
ous O
CNN O
architectures. O
In O
each O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ous O
memory O
blocks O
are O
sent O

- B-DAT
resolution O
and O
JPEG O
deblocking O
simultaneously O

- B-DAT
hensive O
benchmark O
evaluations O
well O
demonstrate O

- B-DAT
riority O
of O
our O
MemNet O
over O

- B-DAT

- B-DAT

- B-DAT
ative O
neighbor O
embedding. O
In O
BMVC O

- B-DAT

- B-DAT
age O
denoising O
by O
sparse O
3-D O

- B-DAT

- B-DAT
bridge, O
MA: O
MIT O
Press, O
2001 O

- B-DAT
tifacts O
reduction O
by O
a O
deep O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
volutional O
networks. O
In O
NIPS, O
2008 O

- B-DAT

- B-DAT
ing O
of O
non-parametric O
image O
restoration O

- B-DAT
shick, O
S. O
Guadarrama, O
and O
T O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT
tional O
network O
for O
image O
super-resolution O

- B-DAT

- B-DAT
based O
learning O
applied O
to O
document O

- B-DAT
ings O
of O
the O
IEEE, O
1998 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ric O
skip O
connections. O
In O
NIPS O

- B-DAT
cal O
statistics. O
In O
ICCV, O
2001 O

- B-DAT
stricted O
boltzmann O
machines. O
In O
ICML O

- B-DAT

- B-DAT

- B-DAT
compressed O
images. O
In O
CVPR, O
2016 O

- B-DAT

- B-DAT

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT
up O
using O
sparse-representations. O
Curves O
and O

- B-DAT
yond O
a O
gaussian O
denoiser: O
Residual O

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
TOG O

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
strated O
high-quality O
reconstruction O
for O
single-image O

- B-DAT
resolution. O
In O
this O
paper, O
we O

- B-DAT

- B-DAT
construct O
the O
sub-band O
residuals O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
als, O
and O
uses O
transposed O
convolutions O

- B-DAT
lation O
as O
the O
pre-processing O
step O

- B-DAT
duces O
the O
computational O
complexity. O
We O

- B-DAT

- B-DAT
thermore, O
our O
network O
generates O
multi-scale O

- B-DAT

- B-DAT
tion, O
thereby O
facilitates O
resource-aware O
applications O

- B-DAT
tensive O
quantitative O
and O
qualitative O
evaluations O

- B-DAT
mark O
datasets O
show O
that O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
have O
demonstrated O
the O
state-of-the-art O

- B-DAT
dom O
forest O
[26 O

- B-DAT

- B-DAT
ear O
LR-to-HR O
mapping. O
The O
network O

- B-DAT

- B-DAT
ture O
[17]. O
While O
these O
models O

- B-DAT
sults, O
there O
are O
three O
main O

- B-DAT

- B-DAT
lution O
before O
applying O
the O
network O

- B-DAT
processing O
step O
increases O
unnecessary O
computational O

- B-DAT

- B-DAT
erator O
with O
sub-pixel O
convolution O
[28 O

- B-DAT
volution O
[8] O
(also O
named O
as O

- B-DAT
ture O
the O
underlying O
multi-modal O
distributions O

- B-DAT
smooth O
and O
not O
close O
to O

- B-DAT
ural O
images. O
Third, O
most O
methods O

- B-DAT
isting O
methods O
cannot O
generate O
intermediate O

- B-DAT
ent O
desired O
upsampling O
scales O
and O

- B-DAT

- B-DAT
work O
takes O
an O
LR O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
layer O
for O
upsampling O
the O

- B-DAT
band O
residuals O
(the O
differences O
between O

- B-DAT
age O
and O
the O
ground O
truth O

- B-DAT
tion O
operations. O
While O
the O
proposed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pling O
filters O
with O
deep O
convolutional O

- B-DAT
band O
residuals. O
The O
deep O
supervision O

- B-DAT
tal O
results O
demonstrate O
that O
our O

- B-DAT
eral O
CNN O
based O
super-resolution O
models O

- B-DAT
CNN O
[8], O
our O
LapSRN O
achieves O

- B-DAT

- B-DAT

- B-DAT
ble O
to O
a O
wide O
range O

- B-DAT
aware O
adaptability. O
For O
example, O
the O

- B-DAT
ing O
on O
the O
available O
computational O

- B-DAT
ios O
with O
limited O
computing O
resources O

- B-DAT
putation O
of O
residuals O
at O
finer O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
on O
recent O
example-based O
approaches O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mid O
of O
the O
low-resolution O
input O

- B-DAT
ternal O
image O
databases, O
the O
number O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ically O
slow O
due O
to O
the O

- B-DAT

- B-DAT
ods O
learn O
the O
LR-HR O
mapping O

- B-DAT
rithms, O
such O
as O
nearest O
neighbor O

- B-DAT
ding O
[2, O
5], O
kernel O
ridge O

- B-DAT
resentation O
[37, O
38, O
39]. O
Instead O

- B-DAT
ods O
partition O
the O
image O
database O

- B-DAT

- B-DAT
ear O
regressors O
for O
each O
cluster O

- B-DAT

- B-DAT
CNN O
[7] O
jointly O
optimize O
all O

- B-DAT
linear O
mapping O
in O
the O
image O

- B-DAT

- B-DAT
sample O
images O
to O
the O
desired O

- B-DAT
cursive O
layers O
(DRCN) O
to O
reduce O

- B-DAT

- B-DAT
work O
[28] O
extracts O
feature O
maps O

- B-DAT
places O
the O
bicubic O
upsampling O
operation O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
for O
learning O
and O
predicting O

- B-DAT
CNN, O
VDSR, O
DRCN O
and O
our O

- B-DAT

- B-DAT
ods O
and O
the O
proposed O
framework O

- B-DAT

- B-DAT
sampling O
filters O
with O
convolutional O
and O

- B-DAT
lutional O
layers. O
Using O
the O
learned O

- B-DAT
stead O
of O
the O
`2 O
loss O

- B-DAT
construction O
accuracy. O
Third, O
as O
the O

- B-DAT
gressively O
reconstructs O
HR O
images, O
the O

- B-DAT

- B-DAT
tic O
segmentation O
[11, O
25]. O
Denton O

- B-DAT
GAN) O
to O
generate O
realistic O
images O

- B-DAT
resolution O
model O
that O
predicts O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
GAN, O
the O
convolutional O
layers O
at O

- B-DAT

- B-DAT
tations O
at O
lower O
levels. O
The O

- B-DAT

- B-DAT

- B-DAT
GAN O
are O
independently O
trained. O
On O

- B-DAT

- B-DAT

- B-DAT
isons O
with O
LAPGAN O
in O
the O

- B-DAT
ial O
loss O
for O
photo-realistic O
SR O

- B-DAT
versarial O
loss O
in O
the O
supplementary O

- B-DAT
ual O
images O
at O
log2 O
S O

- B-DAT

- B-DAT
resolving O
an O
LR O
image O
at O

- B-DAT
struction. O
Feature O
extraction. O
At O
level O

- B-DAT
posed O
convolutional O
layer O
to O
upsample O

- B-DAT
tures O
by O
a O
scale O
of O

- B-DAT
volutional O
layer O
is O
connected O
to O

- B-DAT
traction O
at O
the O
coarse O
resolution O

- B-DAT
tional O
layer. O
In O
contrast O
to O

- B-DAT

- B-DAT
sampled O
by O
a O
scale O
of O

- B-DAT
linear O
kernel O
and O
allow O
it O

- B-DAT

- B-DAT
ual O
image O
from O
the O
feature O

- B-DAT

- B-DAT
ilar O
structure O
at O
each O
level O

- B-DAT
work O
parameters O
to O
be O
optimized O

- B-DAT

- B-DAT
age O
ŷ O
= O
f O
(x;θ O

- B-DAT
scaled O
LR O
image O
by O
xs O

- B-DAT
tion O
and O
the O
corresponding O
ground O

- B-DAT

- B-DAT

- B-DAT
ing O
to O
predict O
sub-band O
residual O

- B-DAT

- B-DAT

- B-DAT
sults O
in O
one O
feed-forward O
pass O

- B-DAT

- B-DAT
volutional O
and O
transposed O
convolutional O
layers O

- B-DAT

- B-DAT

- B-DAT
scale O
between O
[0.5,1.0]. O
(2) O
Rotation O

- B-DAT
age O
by O
90◦, O
180◦, O
or O

- B-DAT
izontally O
or O
vertically O
with O
a O

- B-DAT
verges O
faster O
and O
achieves O
improved O

- B-DAT
tions, O
and O
residual O
learning. O
We O

- B-DAT
mance O
(PSNR) O
drop O
on O
both O

- B-DAT
mentum O
parameter O
to O
0.9 O
and O

- B-DAT
SRN O
with O
state-of-the-art O
algorithms O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
performs O
SRCNN O
within O
10 O
epochs O

- B-DAT
trated O
in O
Figure O
2, O
the O

- B-DAT
posed O
network. O
(a) O
HR O
image O

- B-DAT

- B-DAT
formance O
with O
SRCNN. O
In O
Figure O

- B-DAT
struct O
by O
the O
proposed O
algorithm O

- B-DAT
tively O
clean O
and O
sharp O
details O

- B-DAT
ment O
(e.g. O
0.7 O
dB O
on O

- B-DAT
ent O
depth, O
d O
= O
3,5,10,15 O

- B-DAT
offs O
between O
performance O
and O
speed O

- B-DAT
mance O
and O
speed. O
We O
show O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
SR O
algorithms: O
A+ O
[30 O

- B-DAT
BAN100 O
[15] O
and O
MANGA109 O
[23 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
BAN100 O
contains O
challenging O
urban O
scenes O

- B-DAT
ods O
on O
most O
datasets. O
In O

- B-DAT

- B-DAT
less, O
our O
8× O
model O
provides O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pling O
for O
pre-processing O
generate O
results O

- B-DAT
tifacts O
[7, O
17, O
26, O
30 O

- B-DAT
tively O
suppresses O
such O
artifacts O
through O

- B-DAT
struction O
and O
the O
robust O
loss O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
RCNN, O
RFL O
and O
VDSR O
using O

- B-DAT
upsampled O
images O
[7, O
17, O
30 O

- B-DAT

- B-DAT
pling O
[8]. O
The O
state-of-the-art O
methods O

- B-DAT

- B-DAT
structs O
high-quality O
HR O
images O
at O

- B-DAT
ods O
in O
the O
supplementary O
material O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
struct O
these O
models O
in O
MatConvNet O

- B-DAT

-210 B-DAT

-110010 B-DAT
1102 O

- B-DAT

- B-DAT

- B-DAT
RCNN. O
We O
present O
detailed O
evaluations O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
torical O
photographs O
with O
JPEG O
compression O

- B-DAT

- B-DAT
sampling O
kernels O
are O
available. O
As O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ply O
super-resolution O
frame O
by O
frame O

- B-DAT
proach O
achieve O
real-time O
performance O
(i.e O

- B-DAT
lucinate” O
fine O
details. O
As O
shown O

- B-DAT

- B-DAT
herence O
or O
motion O
blur O
are O

- B-DAT

- B-DAT

- B-DAT
ric O
SR O
methods O
[7, O
8 O

- B-DAT
duce O
the O
number O
of O
parameters O

- B-DAT
curate O
single-image O
super-resolution. O
Our O
model O

- B-DAT
sively O
predicts O
high-frequency O
residuals O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mizing O
the O
network O
with O
a O

- B-DAT
posed O
LapSRN O
alleviates O
issues O
with O

- B-DAT
tions O
on O
benchmark O
datasets O
demonstrate O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gorithms O
in O
terms O
of O
visual O

- B-DAT
search O
under O
Grant O
N00014-16-1-2314 O

- B-DAT
Morel. O
Low-complexity O
single-image O
super-resolution O
based O

- B-DAT
ods. O
IJCV, O
61(3):211–231, O
2005. O
4 O

- B-DAT
tions, O
31(4):532–540, O
1983. O
3 O

- B-DAT

- B-DAT
works. O
In O
NIPS, O
2015. O
3 O

- B-DAT
resolution O
using O
deep O
convolutional O
networks O

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT

- B-DAT
based O
super-resolution. O
IEEE, O
Computer O
Graphics O

- B-DAT
plications, O
22(2):56–65, O
2002. O
2 O

- B-DAT
tion O
and O
refinement O
for O
semantic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sis/synthesis. O
In O
SIGGRAPH, O
1995. O
3 O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT
tional O
network O
for O
image O
super-resolution O

- B-DAT

- B-DAT

- B-DAT
ham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT

- B-DAT
supervised O
nets, O
2015. O
In O
International O

- B-DAT
ficial O
Intelligence O
and O
Statistics, O
2015 O

- B-DAT
resolution O
via O
deep O
draft-ensemble O
learning O

- B-DAT

- B-DAT
timedia O
Tools O
and O
Applications, O
pages O

- B-DAT
ters: O
Edge-aware O
image O
processing O
with O

- B-DAT
ing O
to O
refine O
object O
segments O

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT
mation O
fidelity O
criterion O
for O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution: O
a O
benchmark. O
In O
ECCV O

- B-DAT

- B-DAT
resolution O
as O
sparse O
representation O
of O

- B-DAT
resolution O
via O
sparse O
representation. O
TIP O

- B-DAT

- B-DAT

datasets O
(i.e., O
Set5 O
and O
Set14, O
BSD100 B-DAT

AND O
4 O
ON O
SET5, O
SET14, O
BSD100 B-DAT

BSD100 B-DAT
3 I-DAT
28.50 O
/ O
0.7881 O
28.82 O

Gaussian O
denoising, O
SISR O
with O
multiple O
upscaling B-DAT
factors, O
and O
JPEG O
deblocking O
with O

noise O
level, O
SISR O
with O
multiple O
upscaling B-DAT
factors, O
and O
JPEG O
deblocking O
with O

levels, O
down-sampled O
images O
with O
multiple O
upscaling B-DAT
factors, O
and O
JPEG O
images O
with O

model O
for O
all O
the O
three O
upscaling B-DAT
factors O
(i.e., O
2, O
3 O
and O

butterfly” O
from O
Set5 O
dataset O
with O
upscaling B-DAT
factor O
3 O

image O
from O
Urban100 O
dataset O
with O
upscaling B-DAT
factor O
4 O

bicubically O
interpolated O
low-resolution O
images O
with O
upscaling B-DAT
factor O
2 O
(upper O
middle) O
and O

datasets O
(i.e., O
Set5 O
and O
Set14, O
BSD100 B-DAT
and O
Urban100 O
[40]) O
used O
in O

AND O
4 O
ON O
SET5, O
SET14, O
BSD100 B-DAT
AND O
URBAN100 O
DATASETS, O
JPEG O

BSD100 B-DAT
3 O
28.50 O
/ O
0.7881 O
28.82 O

- B-DAT

- B-DAT

- B-DAT
works, O
Residual O
Learning, O
Batch O
Normalization O

- B-DAT

- B-DAT
stitute O
of O
Technology, O
Harbin O
150001 O

- B-DAT

- B-DAT
nic O
University, O
Hong O
Kong O
(e-mail O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
backs. O
First, O
those O
methods O
generally O

- B-DAT
timization O
problem O
in O
the O
testing O

- B-DAT

- B-DAT
based O
methods O
can O
hardly O
achieve O

- B-DAT

- B-DAT
mance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
larization O
and O
learning O
methods O
for O

- B-DAT

- B-DAT
allel O
computation O
on O
modern O
powerful O

- B-DAT
noised O
image O
x̂, O
the O
proposed O

- B-DAT
tion O
technique O
is O
further O
introduced O

- B-DAT

- B-DAT
age O
deblocking O
problem O
can O
be O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
results O
when O
extended O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
remarks O
are O
given O
in O

- B-DAT

- B-DAT
age O
denoising. O
In O
[25], O
stacked O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tween O
depth O
and O
width O
[19 O

- B-DAT

- B-DAT

- B-DAT
resolution O
[31] O
and O
color O
image O

- B-DAT

- B-DAT
scent O
(SGD) O
has O
been O
widely O

- B-DAT

- B-DAT
linearity O
inputs O
during O
training. O
Batch O

- B-DAT
rating O
a O
normalization O
step O
and O

- B-DAT

- B-DAT
tion O
for O
CNN-based O
image O
denoising O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
filters O
to O
be O
3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ping O
is O
more O
like O
an O

- B-DAT

- B-DAT
rithms O
and O
network O
architecture. O
Note O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tributed O
to O
the O
internal O
covariate O

- B-DAT
serve O
that, O
with O
batch O
normalization O

- B-DAT

- B-DAT
malization O
offers O
some O
merits O
for O

- B-DAT
ating O
internal O
covariate O
shift O
problem O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eter, O
fk O
∗ O
x O
stands O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stage O
TNRD O
from O
three O
aspects O

- B-DAT

- B-DAT

- B-DAT
sian O
distributed O
(or O
the O
noise O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing, O
we O
set O
the O
range O

- B-DAT

- B-DAT
B. O
We O
use O
color O
version O

- B-DAT
noising O
tasks, O
as O
in O
[35 O

- B-DAT
resolution O
image O
with O
downscaling O
factors O

- B-DAT

- B-DAT

-3 B-DAT

-3, B-DAT
we O
adopt O
different O
test O
set O

- B-DAT

- B-DAT

- B-DAT

-3 B-DAT

- B-DAT

- B-DAT
ments O
are O
carried O
out O
in O

- B-DAT

- B-DAT

- B-DAT

15 O
31.07 O
31.37 O
31.21 O
- B-DAT
31.24 O
31.42 O
31.73 O
31.61 O
σ O

50 O
25.62 O
25.87 O
25.67 O
26.03 O
- B-DAT
25.97 O
26.23 O
26.23 O

- B-DAT

- B-DAT
B/CDnCNN-B O
and O
DnCNN-3 O
on O
GPU O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
training O
based O
methods O
(i.e O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tures O
meet O
well O
with O
the O

- B-DAT

-5 B-DAT
illustrate O
the O
visual O
results O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
7. O
One O
can O
see O
that O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
v5 O
deep O
learning O
library O
to O

-3 B-DAT
model O
is O
trained O
for O
three O

-3 B-DAT
with O
the O
specific O
state-of-the-art O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

203.1 O
25.4 O
1.42 O
2.11 O
/ O
- B-DAT
0.45 O
/ O
0.010 O
0.74 O

- B-DAT

- B-DAT

- B-DAT

-3 B-DAT
is O
compared O
with O
two O
state-of-the-art O

- B-DAT

- B-DAT

-3 B-DAT
model O
for O
the O
three O
different O

-3 B-DAT
outperforms O
AR-CNN O
by O
about O
0.3dB O

-3 B-DAT
and O
VDSR O
can O
produce O
sharp O

- B-DAT
ods. O
As O
one O
can O
see O

-3 B-DAT
can O
recover O
the O
straight O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

0.8861 O
40 O
33.34 O
/ O
0.8953 O
- B-DAT
33.77 O
/ O
0.9003 O

0.9090 O
40 O
33.63 O
/ O
0.9198 O
- B-DAT
33.96 O
/ O
0.9247 O

- B-DAT

- B-DAT
3 O
can O
produce O
visually O
pleasant O

- B-DAT
mance. O
Unlike O
traditional O
discriminative O
models O

- B-DAT

- B-DAT
scaling O
factors, O
and O
JPEG O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
larization O
method O
for O
total O
variation-based O

- B-DAT
nition, O
2007, O
pp. O
1–8 O

- B-DAT
agation O
with O
learned O
higher-order O
Markov O

- B-DAT
tion,” O
in O
IEEE O
Conference O
on O

- B-DAT
inative O
non-blind O
deblurring,” O
in O
IEEE O

- B-DAT

- B-DAT
mation O
Processing O
Systems, O
2012, O
pp O

- B-DAT

-3 B-DAT
/ O
30.02dB O

- B-DAT

- B-DAT

-3 B-DAT
/ O
32.73dB O

- B-DAT

- B-DAT

-3 B-DAT
/ O
29.70dB O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

LR O
images O
directly O
and O
learned O
upscaling B-DAT
filters O
in O
the O
last O
layer O

the O
same O
as O
the O
SR O
upscaling B-DAT
factor O

of O
our O
DSRN O
with O
×2 O
upscaling B-DAT
on O
Set5 O
dataset O

performance O
drop O
across O
all O
three O
upscaling B-DAT
scales O
when O
changing O
from O
shared O

on O
Set O
14 O
with O
×3 O
upscaling B-DAT

recent O
SR O
methods O
for O
×3 O
upscaling B-DAT
on O
Set O
14 O

images O
on O
Set14 O
with O
x3 O
upscaling B-DAT
among O
differ- O
ent O
SR O
approaches O

Fast O
and O
ac- O
curate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
erate O
at O
a O
fixed O
spatial O

- B-DAT
resolution O
(LR) O
and O
high-resolution O
(HR O

- B-DAT
layed O
feedback. O
Extensive O
quantitative O
and O

- B-DAT
uations O
on O
benchmark O
datasets O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formance O
has O
been O
significantly O
improved O

- B-DAT
velopments O
in O
deep O
neural O
networks O

- B-DAT
ing O
[16] O
have O
been O
widely O

- B-DAT
tently O
observed. O
The O
first O
is O

- B-DAT
ping O
from O
LR O
to O
HR O

- B-DAT
work O
depth O
enlarges O
the O
size O

-15 B-DAT

-1 B-DAT

-0317 B-DAT

- B-DAT
struct O
missing O
HR O
components. O
The O

- B-DAT
ing O
gradients, O
facilitating O
the O
training O

- B-DAT
troduces O
more O
parameters, O
and O
thus O

- B-DAT

- B-DAT

- B-DAT
tional O
Network O
(DRCN) O
[21] O
shares O

- B-DAT
ent O
residual O
units O
and O
achieves O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ral O
networks O
(RNNs). O
Specifically, O
Liao O

- B-DAT

- B-DAT
work O
(ResNet) O
[16] O
is O
equivalent O

- B-DAT
spired O
by O
their O
findings, O
we O

- B-DAT
olution O
(bicubic O
interpolation O
is O
first O

- B-DAT

- B-DAT
nite O
unfolding O
in O
time O
of O

- B-DAT
tioning O
that O
we O
follow O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ventional O
RNN O
model O
is O
generally O

- B-DAT

- B-DAT
dering O
our O
model O
a O
Dual-State O

- B-DAT

- B-DAT
tions. O
This O
provides O
information O
flow O

- B-DAT
ery O
single O
unrolling O
time. O
In O

- B-DAT
age O
Restoration O
and O
Enhancement O
workshop O

- B-DAT

- B-DAT
sive O
experimental O
results O
validate O
that O

- B-DAT
CNN, O
to O
predict O
the O
nonlinear O

- B-DAT

- B-DAT
strated O
superior O
performance O
to O
many O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mization O
techniques O
[34, O
28, O
11 O

- B-DAT
posed O
a O
sparse O
coding O
network O

- B-DAT

- B-DAT
end, O
demonstrating O
the O
benefit O
of O

- B-DAT
work O
in O
[42 O

- B-DAT
ageNet O
challenges O
[9], O
Kim O
et O

- B-DAT
ents. O
However, O
as O
the O
model O

- B-DAT
rameters O
increases. O
To O
control O
the O

- B-DAT
culty O
of O
training. O
Tai O
et O

- B-DAT
ual O
SR O
learning O
algorithms O
are O

- B-DAT
ual O
learning O
or O
local O
residual O

- B-DAT
rameter O
efficient O
via O
recursive O
learning O

- B-DAT
works O
(DenseNet) O
[17] O
instead O
of O

- B-DAT
put O
images, O
Shi O
et O
al O

- B-DAT
duces O
the O
computation O
cost. O
Similarly O

- B-DAT
bination O
with O
smaller O
filter O
sizes O

- B-DAT
Resolution O
Network O
(LapSRN) O
[22] O
works O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ant, O
which O
learns O
different O
scaled O

- B-DAT
allel O
via O
weight O
sharing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
posed O
nature O
of O
single O
image O

- B-DAT
tions O
and O
poor O
subjective O
scores O

- B-DAT
back, O
Generative O
Adversarial O
Networks O
have O

- B-DAT
uation O
by O
mean-opinion-score O
showed O
huge O

- B-DAT

- B-DAT
vide O
a O
better O
understanding O
of O

- B-DAT
cations O

- B-DAT

- B-DAT
tem. O
Then, O
based O
on O
this O

- B-DAT
ment O
of O
SR O
models O
with O

- B-DAT

- B-DAT
rent O
states. O
Depending O
on O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put, O
and O
recurrent O
states O
are O

- B-DAT
tively. O
The O
arrow O
link O
indicates O

- B-DAT
tion O
on O
this O
general O
formulation O

- B-DAT
rection O
to O
a O
fixed O
length O

- B-DAT

- B-DAT

- B-DAT
independent, O
which O
means O
these O
parameters O

- B-DAT
out O
any O
down-sampling O
or O
up-sampling O

- B-DAT
sions O
remain O
the O
same O
across O

- B-DAT
ventional O
residual O
block, O
which O
contains O

- B-DAT
tional O
layers O
with O
skip O
connections O

- B-DAT
rameters O

- B-DAT
ventional O
ResNet O
is O
that O
the O

- B-DAT

- B-DAT
gle O
convolutional O
layer O
to O
the O

- B-DAT
press O
frecurrent. O
The O
graph O
is O

- B-DAT
over, O
unlike O
the O
ResNet O
where O

- B-DAT
tion O
comes O
from O
the O
previous O

- B-DAT
rolled O
state O
s0. O
Figure O
1(e O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
state O
design, O
which O
adopts O
two O

- B-DAT

- B-DAT

- B-DAT
tion O
from O
both O
the O
LR O

- B-DAT
tively. O
Four O
colored O
arrows O
indicate O

- B-DAT

- B-DAT

- B-DAT
tion O
flows O
between O
sl O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

of O
a O
single-state B-DAT
RNN. O
(c) O
- O
(e) O
The O
required O
recurrent O
function O

- B-DAT

- B-DAT
tom O
one O
is O
LR. O
This O

- B-DAT
cialization O
for O
different O
resolutions O
and O

- B-DAT

- B-DAT

- B-DAT
sampling O
transition. O
The O
strides O
in O

- B-DAT

- B-DAT

- B-DAT
ing O
a O
prediction O
at O
every O

- B-DAT
terized O
by O
a O
single O
convolutional O

- B-DAT
ing O
the O
prediction O
only O
at O

- B-DAT
over, O
the O
model O
predicts O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lowing O
datasets: O
Set5 O
[4], O
Set14 O

- B-DAT
ban100 O
[19]. O
The O
training O
data O

- B-DAT
dom O
flipping O
along O
the O
vertical O

- B-DAT

- B-DAT

- B-DAT
vided O
training O
and O
validation O
sets O

- B-DAT
tioned O
data O
augmentations O
except O
random O

- B-DAT

- B-DAT
lution O
filters. O
Due O
to O
our O

- B-DAT

- B-DAT
sions O
as O
the O
LR O
and O

- B-DAT

- B-DAT
volution O
is O
applied O

- B-DAT
form O
distribution O
using O
the O
method O

- B-DAT
mentum O
0.95 O
as O
our O
optimizer O

- B-DAT
ing O
rate O
from O
{0.1,0.03,0.01} O
and O

- B-DAT
ing O
rate O
annealing O
is O
driven O

- B-DAT

- B-DAT
posed O
the O
use O
of O
unshared O

- B-DAT
folding O
time O
to O
resolve O
this O

- B-DAT
nary O
ReLU O
as O
the O
activation O

- B-DAT
imum O
effective O
depth O
of O
the O

- B-DAT
work O
is O
2T O
+ O
4 O

- B-DAT
ers O
in O
a O
residual O
block O

- B-DAT
iary O
input O
and O
output O
layers O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity O
and O
computation O
cost. O
We O

- B-DAT
pirical O
results O
are O
shown O
in O

- B-DAT
tioning O
that O
we O
also O
experimented O

- B-DAT
ing O
to O
be O
crucial O
for O

- B-DAT
forms O
much O
more O
poorly O
than O

- B-DAT

- B-DAT
part. O
Specifically, O
we O
observe O
around O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
state O
baseline O
and O
the O
DSRN O

- B-DAT
tion, O
comparing O
our O
models O
with O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eral O
public O
benchmark O
datasets O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentation O
Dataset O
[2], O
while O
our O

- B-DAT
tive O
performance O
across O
all O
datasets O

- B-DAT
cently O
developed O
DIV2K O
dataset O
and O

- B-DAT
ranking O
algorithms O
in O
Table O
2 O

- B-DAT
petitive O
performance O
with O
the O
best O

- B-DAT
state O
recurrent O
structure O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolved O
images O
on O
Set14 O
with O

- B-DAT
ent O
SR O
approaches. O
For O
these O

- B-DAT
tures O
and O
is O
less O
prone O

- B-DAT
trate O
the O
parameters-to-PSNR O
relationship O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mance, O
and O
has O
modest O
inference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
folding O
of O
a O
single-state O
RNN O

- B-DAT
tions. O
Based O
on O
this, O
we O

- B-DAT
ering O
a O
dual-state O
design; O
the O

- B-DAT
posed O
DSRN O
operate O
at O
different O

- B-DAT

- B-DAT
iments O
on O
benchmark O
datasets O
have O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tour O
detection O
and O
hierarchical O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
based O
on O
nonnegative O
neighbor O

- B-DAT
resolution O
through O
neighbor O
embedding. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
database. O
In O
CVPR, O
pages O

- B-DAT
ing O
a O
deep O
convolutional O
network O

- B-DAT
resolution. O
In O
ECCV, O
2014. O
1 O

- B-DAT

- B-DAT

- B-DAT
ual O
networks O
for O
image O
super-resolution O

- B-DAT

- B-DAT

- B-DAT
TATS, O
2010. O
5 O

- B-DAT

- B-DAT
berger. O
Deep O
networks O
with O
stochastic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works. O
In O
CVPR, O
2016. O
1 O

- B-DAT
recursive O
convolutional O
network O
for O
image O

- B-DAT
resolution. O
In O
CVPR, O
2016. O
1 O

- B-DAT

- B-DAT
ningham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT
hanced O
deep O
residual O
networks O
for O

- B-DAT
resolution. O
In O
CVPR O
Workshops, O
2017 O

- B-DAT

- B-DAT
ing O
a O
mixture O
of O
deep O

- B-DAT
resolution. O
In O
ACCV, O
pages O
145–156 O

- B-DAT

- B-DAT
tion O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT
hancenet: O
Single O
image O
super-resolution O
through O

- B-DAT
tomated O
texture O
synthesis. O
In O
ICCV O

- B-DAT
curate O
image O
upscaling O
with O
super-resolution O

- B-DAT
ment O
using O
natural O
scene O
statistics O

- B-DAT

- B-DAT
gle O
image O
and O
video O
super-resolution O

- B-DAT

- B-DAT

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT

- B-DAT
resolution O
using O
dense O
skip O
connections O

- B-DAT
works O
behave O
like O
ensembles O
of O

- B-DAT
works. O
In O
NIPS, O
2016. O
1 O

- B-DAT
celli. O
Image O
quality O
assessment: O
from O

- B-DAT

- B-DAT

- B-DAT
lution. O
In O
CVPRW, O
pages O
1–8 O

- B-DAT
resolution. O
TIP, O
21(8):3467–3478, O
2012. O
2 O

- B-DAT

- B-DAT
lective O
tensor O
factorization O
in O
deep O

- B-DAT

- B-DAT

- B-DAT
ity O
measures O
of O
recurrent O
neural O

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
a O
deeply-recursive O
convolutional O
network O

- B-DAT
sions). O
Increasing O
recursion O
depth O
can O

- B-DAT
mance O
without O
introducing O
new O
parameters O

- B-DAT
ploding/vanishing O
gradients. O
To O
ease O
the O

- B-DAT
ing, O
we O
propose O
two O
extensions O

- B-DAT

- B-DAT

- B-DAT
ods O
by O
a O
large O
margin O

- B-DAT

- B-DAT
frequency O
components. O
For O
example, O
if O

- B-DAT
tern O
with O
smoothed O
edges O
contained O

- B-DAT
priately O
sharpened. O
As O
SR O
is O

- B-DAT

- B-DAT
ious O
computer O
vision O
tasks O
often O

- B-DAT
tive O
fields O
(224x224 O
common O
in O

- B-DAT
volutional O
(conv.) O
layer O
with O
filter O

- B-DAT
mediate O
representation O
can O
be O
used O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
network O
(DRCN). O
DRCN O
repeatedly O

- B-DAT
sions O
are O
performed. O
Our O
network O

- B-DAT

- B-DAT
dient O
descent O
method O
does O
not O

- B-DAT

- B-DAT

- B-DAT
sions. O
As O
each O
recursion O
leads O

- B-DAT

- B-DAT

- B-DAT
put) O
and O
a O
high-resolution O
image O

- B-DAT
ever, O
is O
likely O
to O
be O

- B-DAT
construction. O
This O
is O
particularly O
effective O

- B-DAT
resolution O
method O
deeply O
recursive O
in O

- B-DAT
sive O
network O
in O
two O
ways O

- B-DAT

- B-DAT
connection. O
Our O
method O
demonstrates O
state-of-the-art O

- B-DAT
formance O
in O
common O
benchmarks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ping O
from O
LR O
to O
HR O

- B-DAT
tention O
to O
find O
better O
regression O

- B-DAT
tional O
neural O
network O
(CNN) O
[5 O

- B-DAT

- B-DAT
lutional O
neural O
network O
(SRCNN) O
[5 O

- B-DAT
sibility O
of O
an O
end-to-end O
approach O

- B-DAT
creases O
the O
number O
of O
parameters O

- B-DAT
lutional O
network O
that O
models O
long-range O

- B-DAT
quential O
data, O
have O
seen O
limited O

- B-DAT
lutional O
network O
in O
a O
separate O

- B-DAT

- B-DAT

- B-DAT
fitting. O
To O
overcome O
overfitting, O
Liang O

- B-DAT

- B-DAT
folded O
layers. O
They O
show O
that O

- B-DAT
tectures O

- B-DAT
mance O
for O
super-resolution. O
We O
apply O

-1 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ence O
net O
solves O
the O
task O

- B-DAT
ture O
maps O
in O
the O
inference O

- B-DAT
ate O
representation O
used O
to O
pass O

- B-DAT
resent O
its O
feature O
maps O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cursion O
applies O
the O
same O
convolution O

- B-DAT
sive O
layer O
represent O
the O
high-resolution O

- B-DAT

- B-DAT

- B-DAT
tion O
net O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formative O
than O
the O
raw O
intensities O

- B-DAT

- B-DAT
terpolated O
input O
image O
(to O
the O

- B-DAT
net O
functions: O
embedding, O
inference O
and O

- B-DAT
spectively. O
Our O
model O
is O
the O

- B-DAT
putes O
the O
matrix O
output O
H0 O

- B-DAT
ence O
net O
f2. O
Hidden O
layer O

- B-DAT
position O
of O
the O
same O
elementary O

- B-DAT

- B-DAT

- B-DAT
bedding O
net. O
The O
formula O
is O

- B-DAT

- B-DAT
dance O
with O
the O
limited O
success O

- B-DAT
havior. O
Long O
term O
components O
approach O

- B-DAT
ing O
an O
exact O
copy O
of O

- B-DAT
cursive O
layer O
needs O
to O
keep O

- B-DAT

- B-DAT

- B-DAT
timal O
recursion O
issues, O
we O
propose O

- B-DAT
construction O
net O
now O
outputs O
D O

- B-DAT
tions O
are O
simultaneously O
supervised O
during O

- B-DAT
ing. O
The O
optimal O
weights O
are O

- B-DAT
diate O
layers O
for O
a O
convolutional O

- B-DAT
tion O
error O
while O
improving O
the O

- B-DAT
nificant O
differences O
between O
our O
recursive-supervision O

- B-DAT

- B-DAT
ciate O
a O
unique O
classifier O
for O

- B-DAT
ditional O
layer, O
a O
new O
classifier O

- B-DAT
ference O
is O
that O
Lee O
et O

- B-DAT
sifiers O
during O
testing. O
However, O
an O

- B-DAT
diate O
predictions O
significantly O
boosts O
the O

- B-DAT

- B-DAT
ing/exploding O
gradients O
along O
one O
backpropagation O

- B-DAT

- B-DAT
sion: O
skip-connection. O
For O
SR, O
input O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eral O
super-resolution O
methods O
[28, O
29 O

- B-DAT

- B-DAT
tion O
under O
recursive-supervision O
(Figure O
3(a O

- B-DAT

- B-DAT
connection O
can O
take O
various O
functional O

- B-DAT
ple, O
input O
can O
be O
concatenated O

- B-DAT

- B-DAT
ing O
set O
is O
minimized. O
This O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gresses, O
α O
decays O
to O
boost O

- B-DAT
put O

- B-DAT
jective O
using O
mini-batch O
gradient O
descent O

- B-DAT
propagation O
(LeCun O
et O
al. O
[15 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mark O
[29, O
28, O
5]. O
Dataset O

- B-DAT
ages O
failed O
by O
existing O
methods O

- B-DAT
folded, O
the O
longest O
chain O
from O

- B-DAT
mentum O
parameter O
to O
0.9 O
and O

- B-DAT
ing O
images O
are O
split O
into O

- B-DAT

- B-DAT

- B-DAT
lutions, O
we O
set O
all O
weights O

- B-DAT

- B-DAT
rameters O
except O
the O
weights O
used O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cause O
human O
vision O
is O
much O

- B-DAT
tensity O
than O
in O
color O

- B-DAT

- B-DAT
parison, O
however, O
we O
also O
crop O

- B-DAT
isting O
methods O
use O
slightly O
different O

- B-DAT
uation O
on O
several O
datasets. O
Our O

- B-DAT
isting O
methods O
in O
all O
datasets O

- B-DAT
tive O
to O
patterns. O
In O
contrast O

- B-DAT

- B-DAT

- B-DAT
ploiting O
a O
large O
image O
context O

- B-DAT

- B-DAT
connection. O
We O
have O
demonstrated O
that O

- B-DAT
forms O
existing O
methods O
by O
a O

- B-DAT
der O
to O
use O
image-level O
context O

- B-DAT

- B-DAT
works, O
IEEE O
Transactions O
on, O
5(2 O

- B-DAT

- B-DAT
projection O
residuals. O
In O
International O
Conference O

- B-DAT

- B-DAT
resolution O
using O
deep O
convolutional O
networks O

- B-DAT
ing O
low-level O
vision. O
IJCV, O
2000 O

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
transformed O
self-exemplars. O
In O

- B-DAT
istration. O
CVGIP: O
Graphical O
models O
and O

- B-DAT

- B-DAT

- B-DAT
ize O
recurrent O
networks O
of O
rectified O

- B-DAT
based O
learning O
applied O
to O
document O

- B-DAT
ings O
of O
the O
IEEE, O
86(11 O

- B-DAT
supervised O
nets. O
arXiv O
preprint O
arXiv:1409.5185 O

- B-DAT
tional O
networks O
for O
semantic O
segmentation O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cal O
statistics. O
In O
ICCV, O
2001 O

- B-DAT
ternational O
Conference O
on O
Machine O
Learning O

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT

- B-DAT

- B-DAT
cation. O
In O
NIPS, O
2012. O
2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
via O
sparse O
representation. O
TIP O

- B-DAT
up O
using O
sparse-representations. O
In O
Curves O

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Trans O

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
marks O
(i.e. O
Set5, O
Set14, O
B100 O

- B-DAT
CNN, O
ANR, O
Zeyde, O
Yang) O
and O

- B-DAT
ments. O
The O
techniques O
are O
widely O

- B-DAT
of-the-art O
results O
outperforming O
A+ O
by O

- B-DAT
age O
PSNR O
whilst O
maintaining O
a O

- B-DAT

- B-DAT

- B-DAT
quencies O
details O
from O
a O
single O

- B-DAT

- B-DAT

- B-DAT
respond O
to O
the O
same O
LR O

- B-DAT
lem, O
the O
SR O
literature O
proposes O

- B-DAT

- B-DAT
ods O
[24], O
reconstruction-based O
methods O
[3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nal O
images. O
Most O
recent O
methods O

- B-DAT
based O
SR. O
We O
apply O
them O

- B-DAT
chored O
Neighborhood O
Regression O
(ANR) O
method O

- B-DAT

- B-DAT
based O
single O
image O
super-resolution O
methods O

- B-DAT
cant O
improvements O
on O
standard O
benchmarks O

- B-DAT
bine O
the O
techniques O
to O
derive O

- B-DAT
ments O
when O
starting O
from O
the O

review O
the O
anchored O
regression O
baseline O
- B-DAT
the O
A+ O
method O
[26]. O
Then O

- B-DAT
bically) O
downscaled O
to O
the O
corresponding O

- B-DAT

- B-DAT

- B-DAT
tion O
we O
first O
describe O
the O

- B-DAT
ods O
we O
use O
or O
compare O

- B-DAT
lar O
images: O
one O
medium O
size O

- B-DAT
ing O
results. O
The O
images O
in O

- B-DAT
ety O
of O
real-life O
scenes O
and O

- B-DAT

- B-DAT
similarity O
(S) O
experiments O
on O
the O

- B-DAT
resentation O
of O
the O
LR-HR O
priors/training O

- B-DAT

- B-DAT
fte O
et O
al. O
[25] O
relaxes O

- B-DAT
tion O
of O
patches O
from O
Yang O

- B-DAT

- B-DAT
SVD O
[1]) O
as O
the O
ANR O

- B-DAT
tionary O
atoms, O
called O
anchors. O
For O

- B-DAT
proves O
with O
the O
number O
of O

- B-DAT
trix O
multiplication O
(application O
of O
the O

- B-DAT

- B-DAT

- B-DAT
els O
for O
LR O
and O
9 O

- B-DAT
mance O
from O
32.39dB O
with O
0.5 O

- B-DAT
sions O
of O
the O
training O
images/patches O

- B-DAT
inal O
images O
by O
90◦, O
180 O

- B-DAT

- B-DAT

- B-DAT
gressors O
varies O
from O
31.83dB O
when O

- B-DAT
ples O
to O
32.39dB O
for O
0.5 O

- B-DAT
ing O
samples O
but O
on O
the O

- B-DAT
ples/anchoring O
points) O
is O
increased, O
the O

- B-DAT
chored O
methods O
(such O
as O
ANR O

- B-DAT
tize O
the O
LR O
feature O
space O

- B-DAT
sors, O
while O
it O
reaches O
32.92dB O

- B-DAT
chor O
is O
applied O
to O
reconstruct O

- B-DAT
tures O
are O
high O
dimensional O
(30 O

- B-DAT
tures O
such O
as O
kd-trees, O
forests O

-4 B-DAT
times O
in O
[20, O
22 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
struction O
consistent O
with O
the O
LR O

- B-DAT
sampling. O
Knowing O
the O
degradation O
operators O

- B-DAT
timated O
[18]. O
Assuming O
the O
degradation O

- B-DAT
pending O
on O
the O
settings O
as O

- B-DAT

- B-DAT
ence O
A+ O
is O
1.18dB O
better O

- B-DAT
ter O
than O
the O
baseline O
Yang O

- B-DAT
tion O
operators O
are O
unknown O
and O

- B-DAT
cise, O
therefore O
our O
reported O
results O

- B-DAT
resolution O
becomes O
more O
accurate, O
since O

- B-DAT
ble O
HR O
solutions O
for O
each O

- B-DAT
ally O
refine O
the O
contents O
up O

- B-DAT
plexity O
depends O
on O
the O
number O

- B-DAT
resolving O
the O
LR O
image O
in O

- B-DAT
get O
the O
HR O
image O
for O

- B-DAT
tal O
approach O
has O
a O
loose O

- B-DAT

- B-DAT

- B-DAT
put O
image O
is O
enhanced O
by O

- B-DAT
sults O
at O
pixel O
level. O
Therefore O

- B-DAT
tion O
(E) O
gives O
a O
0.05dB O

- B-DAT
cade. O
The O
running O
time O
is O

- B-DAT
mations. O
In O
Table O
3 O
we O

- B-DAT

- B-DAT

- B-DAT
ies O
built O
from O
the O
input O

- B-DAT
text. O
Exponents O
are O
Glasner O
et O

- B-DAT
els O
adapted O
to O
each O
new O

- B-DAT
naries O
proved O
better O
in O
terms O

- B-DAT
ies O
can O
be O
better O
than O

- B-DAT
ternal O
and O
internal O
dictionaries O

- B-DAT

- B-DAT
main O
specific O
models O
and O
Sun O

- B-DAT
ber O
that O
does O
not O
increase O

- B-DAT
text O
we O
compute O
a O
regressor O

- B-DAT
tion. O
For O
patches O
of O
comparable O

- B-DAT
chors O
and O
then O
the O
regressor O

- B-DAT
prove O
from O
32.39dB O
to O
32.55dB O

- B-DAT
provements O
achieved O
using O
reasoning O
with O

- B-DAT

- B-DAT
resolution O
method. O
If O
we O
start O

- B-DAT
archical O
search O
structure, O
we O
achieve O

- B-DAT
tion O
time. O
The O
full O
setup O

- B-DAT
cations O
×2, O
×3, O
×4. O
Figs O

- B-DAT
ent O
techniques O
is O
additive, O
each O

- B-DAT
formance. O
These O
techniques O
are O
general O

- B-DAT

- B-DAT

- B-DAT
formance O
scale O
the O
results O
(Set5,×3 O

- B-DAT
ods O
A+, O
ANR, O
Zeyde, O
and O

- B-DAT
bines O
A+ O
with O
A O
and O

- B-DAT
nification O
factors O
×2, O
×3, O
and O

- B-DAT
parison O
with O
the O
baseline O
A O

- B-DAT
ding O
with O
Locally O
Linear O
Embedding O

- B-DAT
porting O
improved O
results O
also O
for O

- B-DAT
dation O
operators O
usually O
are O
not O

- B-DAT
mate O
in O
practice. O
A+B O
just O

- B-DAT
ample, O
the O
clarity O
and O
sharpness O

- B-DAT
formance O
of O
example-based O
super-resolution. O
Combined O

- B-DAT
invasive O
techniques O
such O
as O
augmentation O

- B-DAT
chors O
in O
the O
IA O
method O

- B-DAT
nitude O
more O
regressors O
than O
the O

- B-DAT
ning O
time. O
Another O
technique, O
often O

- B-DAT
caded O
application O
of O
the O
core O

- B-DAT

- B-DAT
wards O
HR O
restoration. O
Using O
the O

- B-DAT

- B-DAT

- B-DAT
the-art O
methods O
such O
as O
A O

- B-DAT

- B-DAT
resolution O
methods. O
The O
proposed O
techniques O

- B-DAT

- B-DAT
beri O
Morel. O
Low-complexity O
single-image O
super-resolution O

- B-DAT

- B-DAT
lutional O
nets. O
In O
BMVC, O
2014 O

- B-DAT
gressors O
for O
image O
super-resolution. O
Computer O

- B-DAT
rum, O
34(2):95–104, O
2015. O
1 O

- B-DAT

- B-DAT
resolution O
using O
deep O
convolutional O
networks O

- B-DAT
actions O
on O
Pattern O
Analysis O
and O

- B-DAT
tralized O
sparse O
representation O
for O
image O

- B-DAT

- B-DAT

- B-DAT
based O
super-resolution. O
IEEE O
Computer O
Graphics O

- B-DAT
plications, O
22(2):56–65, O
2002. O
1 O

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
June O

- B-DAT
tration. O
CVGIP, O
53(3):231–239, O
1991. O
4 O

- B-DAT

- B-DAT

- B-DAT
based O
learning O
applied O
to O
document O

- B-DAT
ings O
of O
the O
IEEE, O
1998 O

- B-DAT
cal O
statistics. O
In O
ICCV, O
2001 O

- B-DAT
resolution. O
In O
ICCV, O
2013. O
4 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Hidalgo, O
and O
B. O
Rosenhahn. O
Fast O

- B-DAT

- B-DAT
alizing O
the O
nonlocal-means O
to O
super-resolution O

- B-DAT
tion. O
Image O
Processing, O
IEEE O
Transactions O

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT

- B-DAT
lucination O
for O
image O
super-resolution. O
In O

- B-DAT
cal O
Imaging, O
Processing O
and O
Analysis O

- B-DAT
demic O
Press, O
2000. O
1 O

- B-DAT
borhood O
regression O
for O
fast O
example-based O

- B-DAT

- B-DAT
ors O
for O
post-processing O
demosaiced O
images O

- B-DAT
similarities O
for O
single O
frame O
super-resolution O

- B-DAT
ume O
6494, O
pages O
497–510, O
2011 O

- B-DAT

- B-DAT

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT
cess., O
19(11):2861–2873, O
2010. O
1 O

- B-DAT
resolution O
as O
sparse O
representation O
of O

- B-DAT

- B-DAT

- B-DAT
ing O
multiple O
linear O
mappings O
for O

- B-DAT

- B-DAT
resolution O
using O
deformable O
patches. O
In O

images O
were O
generated O
from O
the O
BSD100 B-DAT

Each O
LR O
image O
from O
the O
BSD100 B-DAT

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

BSD100 B-DAT
×2 O
31.36 O
/ O
0.8879 O
31.90 O

images O
were O
generated O
from O
the O
BSD100 B-DAT
dataset O
using O
random O
downscaling O
kernels O

Each O
LR O
image O
from O
the O
BSD100 B-DAT
dataset O
was O
randomly O
de- O
graded O

created O
a O
new O
dataset O
from O
BSD100 B-DAT
[13] O
by O
downscaling O
the O
HR O

chose O
for O
each O
image O
from O
BSD100 B-DAT
[13] O
a O
ran- O
dom O
type O

- B-DAT

- B-DAT

- B-DAT
Resolution O
(SR) O
performance O
in O
the O

- B-DAT
ever, O
being O
supervised, O
these O
SR O

- B-DAT
resolution O
(LR) O
images O
from O
their O

- B-DAT

- B-DAT

- B-DAT
ever, O
rarely O
obey O
these O
restrictions O

- B-DAT
sults O
by O
SotA O
(State O
of O

- B-DAT
troduce O
“Zero-Shot” O
SR, O
which O
exploits O

- B-DAT
age, O
and O
train O
a O
small O

- B-DAT

- B-DAT
ological O
data, O
and O
other O
images O

- B-DAT
cess O
is O
unknown O
or O
non-ideal O

- B-DAT

- B-DAT
ous O
unsupervised O
SR O
methods. O
To O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
(supervised O
[21] O
or O
unsupervised O

- B-DAT
riods O
of O
time O
(days O
or O

- B-DAT
mance O
deteriorates O
significantly O
once O
these O

- B-DAT
quality O
natural O
images, O
from O
which O

- B-DAT

- B-DAT
scaling O
kernel O
(usually O
a O
Bicubic O

- B-DAT
tracting O
artifacts O
(sensor O
noise, O
non-ideal O

- B-DAT
pression, O
etc.), O
and O
for O
a O

- B-DAT

- B-DAT
ally O
×2, O
×3 O
or O
×4 O

- B-DAT
ideal O
(non-bicubic) O
downscaling O
kernel, O
or O

- B-DAT
facts. O
Fig. O
1 O
further O
shows O

- B-DAT

- B-DAT

- B-DAT
ing O
on O
any O
prior O
image O

- B-DAT
ploit O
the O
internal O
recurrence O
of O

- B-DAT

- B-DAT

- B-DAT
form O
SR O
on O
real O
images O

- B-DAT
known O
and O
non-ideal O
(see O
example O

- B-DAT

- B-DAT
trained O
SotA O
SR O
methods O
by O

- B-DAT
nally O
supplied O
examples O
(even O
if O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ages O
[4, O
23]. O
This O
formed O

- B-DAT
pervised O
image O
enhancement O
methods, O
including O

- B-DAT
vised O
SR O
[4, O
5, O
6 O

- B-DAT

- B-DAT
ing O
kernel O
is O
unknown), O
Blind-Deblurring O

- B-DAT
Dehazing O
[2], O
and O
more. O
While O

- B-DAT

- B-DAT
ject O
to O
the O
above-mentioned O
supervised O

- B-DAT
age O
patches, O
of O
predefined O
size O

- B-DAT
nearest-neighbours O
search. O
As O
such, O
they O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
without O
being O
restricted O
by O

- B-DAT

- B-DAT
itations O
of O
patch-based O
methods. O
We O

- B-DAT

- B-DAT

- B-DAT
age O
and O
its O
downscaled O
versions O

- B-DAT

- B-DAT
duce O
the O
HR O
output. O
This O

- B-DAT
based O
SR O
by O
a O
large O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
is O
available O
and O
provided O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
est O
amounts O
of O
computational O
resources O

- B-DAT

- B-DAT
ideal’ O
conditions, O
and O
competitive O
results O

- B-DAT
tions O
for O
which O
SotA O
supervised O

- B-DAT
ages O
have O
strong O
internal O
data O

- B-DAT
peat O
many O
times O
inside O
a O

- B-DAT
vation O
was O
empirically O
verified O
by O

- B-DAT

- B-DAT
conies, O
since O
evidence O
to O
their O

- B-DAT
specific O
information O
when O
relying O
on O

- B-DAT
ages. O
While O
the O
strong O
internal O

- B-DAT

- B-DAT
fied O
here O
using O
a O
‘fractal-like O

- B-DAT
power O
was O
analyzed O
and O
shown O

- B-DAT
ternal O
entropy O
of O
patches O
inside O

- B-DAT

- B-DAT
ther O
shown O
to O
be O
particularly O

- B-DAT
tainty O
and O
image O
degradations O
(see O

- B-DAT

- B-DAT
formation. O
Simple O
unsupervised O
internal-SR O
[4 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
struct O
the O
desired O
HR O
output O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put O
training O
instances. O
The O
resulting O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
HR O
training O
example O
pairs. O
This O

- B-DAT
ternal O
collection O
of O
LR-HR O
image O

- B-DAT

- B-DAT
tremely O
deep O
and O
very O
complex O

- B-DAT

- B-DAT
pler O
image-specific O
network O

- B-DAT
vations O
on O
each O
layer. O
The O

- B-DAT

- B-DAT
ods O
[9, O
8, O
3], O
we O

- B-DAT
dependent O
of O
the O
size O
of O

- B-DAT

- B-DAT

- B-DAT
less O
the O
sampled O
image-pair O
is O

- B-DAT

- B-DAT

- B-DAT
father. O
The O
closer O
the O
size-ratio O

- B-DAT

- B-DAT
pled. O
This O
reflects O
the O
higher O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
runtime O
per O
image O
is O

- B-DAT

-80 B-DAT
GPU). O
This O
runtime O
is O
inde O

- B-DAT
pendent O
of O
the O
image O
size O

- B-DAT

- B-DAT
tained O
when O
using O
a O
gradual O

- B-DAT
ample, O
a O
gradual O
increase O
using O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
kernel, O
high-quality O
imaging O
conditions O

- B-DAT
vised O
SR O
methods O
achieve O
an O

- B-DAT
ferent O
lens O
types O
and O
PSFs O

- B-DAT
ing O
conditions O
(e.g., O
subtle O
involuntary O

- B-DAT
sults O
in O
different O
downscaling O
kernels O

- B-DAT
acteristics, O
various O
compression O
artifacts, O
etc O

- B-DAT
figurations/settings. O
Moreover, O
a O
single O
supervised O

- B-DAT
tions/settings. O
To O
obtain O
good O
performance O

- B-DAT

- B-DAT
dations/settings O
of O
the O
test O
image O

- B-DAT
vided, O
the O
bicubic O
kernel O
serves O

- B-DAT

- B-DAT
off O
between O
speed O
and O
quality O

- B-DAT

- B-DAT

- B-DAT
sion O
artifacts, O
etc.) O
We O
found O

- B-DAT
deviation O
of∼5 O
grayscales), O
improves O
the O

- B-DAT

- B-DAT
scale O
information O
(the O
noise), O
while O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ideal O
kernels O
(see O
examples O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Results O
Our O
method O
(ZSSR O
- B-DAT
‘Zero-Shot O
SR’) O
is O
primarily O
aimed O

- B-DAT

- B-DAT

- B-DAT
petitive O
results O
against O
externally-supervised O
methods O

- B-DAT

- B-DAT
ing O
method O
SelfExSR O
[6] O
by O

- B-DAT
ated O
using O
the O
‘ideal’ O
supervised O

- B-DAT
ple O
is O
shown O
in O
Fig O

- B-DAT
ical O
natural O
image, O
further O
analysis O

- B-DAT
ence O
for O
internal O
learning O
(via O

- B-DAT

- B-DAT
nally O
learned O
data O
recurrence O
(ZSSR O

- B-DAT
vantageous O
in O
image O
area O
with O

- B-DAT
res) O
examples O
of O
themselves O
elsewhere O

- B-DAT
age O
(at O
a O
different O
location/scale O

- B-DAT
Learning O
with O
External-Learning O
in O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion. O
Each O
LR O
image O
from O

- B-DAT
graded O
using O
one O
of O
3 O

- B-DAT
plied O
to O
those O
images, O
without O

- B-DAT
polation O
outperforms O
current O
SotA O
SR O

- B-DAT
sian O
kernels. O
For O
each O
image O

- B-DAT

- B-DAT
nel. O
Table O
2 O
compares O
our O

- B-DAT
ing O
externally-supervised O
SR O
methods O
[12 O

- B-DAT

- B-DAT
scaling O
kernel. O
For O
this O
mode O

- B-DAT
parametric O
downscaling O
kernel O
which O
maximizes O

- B-DAT
larity O
of O
patches O
across O
scales O

- B-DAT
ate O
the O
LR O
image. O
Such O

- B-DAT

- B-DAT
age O
(whether O
estimated O
or O
real O

- B-DAT
els O
that O
favor O
Internal-SR O
(i.e O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
selves O
elsewhere O
inside O
the O
same O

- B-DAT
tion/scale O

- B-DAT

- B-DAT
wscaling O
model O
is O
more O
important O

- B-DAT

- B-DAT

- B-DAT
dom O
type O
of O
degradation O
out O

- B-DAT
ble O
3 O
shows O
that O
ZSSR O

- B-DAT
rent O
SotA O
SR O
methods O

- B-DAT

- B-DAT
ploits O
the O
power O
of O
Deep O

- B-DAT

- B-DAT

- B-DAT
cess O
is O
non-ideal, O
unknown, O
and O

- B-DAT
age O
(i.e., O
image-specific O
settings). O
In O

- B-DAT

- B-DAT
ideal’ O
settings, O
our O
method O
substantially O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pean O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
gle O
image. O
In O
International O
Conference O

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
istration. O
CVGIP: O
Graphical O
Model O
and O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT
tion O
(CVPR) O
Workshops, O
pages O
1646–1654 O

- B-DAT

- B-DAT
tional O
network O
for O
image O
super-resolution O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
ham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cal O
statistics. O
In O
Proc. O
8th O

- B-DAT
ume O
2, O
pages O
416–423, O
July O

- B-DAT
resolution. O
In O
International O
Conference O
on O

- B-DAT
sion O
(ECCV). O
2014. O
3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
(ICML). O
3 O

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT
shops, O
July O
2017. O
6 O

- B-DAT
prove O
example-based O
single O
image O
super O

- B-DAT
tion O
(CVPR), O
June O
2016. O
5 O

- B-DAT
chored O
neighborhood O
regression O
for O
fast O

- B-DAT

- B-DAT

which O
learns O
an O
array O
of O
upscaling B-DAT
filters O
to O
upscale O
the O
final O

SR O
pipeline O
with O
more O
complex O
upscaling B-DAT
filters O
specifically O
trained O
for O
each O

different O
methods O
when O
performing O
SR O
upscaling B-DAT
with O
a O
scale O
factor O
of O

Learning O
upscaling B-DAT
filters O
was O
briefly O
suggested O
in O

convolution O
layer O
to O
learn O
the O
upscaling B-DAT
operation O
for O
image O
and O
video O

In O
our O
network, O
upscaling B-DAT
is O
handled O
by O
the O
last O

L O
layers, O
we O
learn O
nL−1 O
upscaling B-DAT
filters O
for O
the O
nL−1 O
feature O

maps O
as O
opposed O
to O
one O
upscaling B-DAT
filter O
for O
the O
input O
image O

to O
a O
single O
fixed O
filter O
upscaling B-DAT
at O
the O
first O
layer. O
This O

refer O
to O
r O
as O
the O
upscaling B-DAT
ratio. O
In O
general, O
both O
ILR O

in O
Fig. O
1, O
to O
avoid O
upscaling B-DAT
ILR O

feature O
maps O
directly O
with O
one O
upscaling B-DAT

implementations O
using O
various O
forms O
of O
upscaling B-DAT
before O
convolution O

luminance O
changes O
[31]. O
For O
each O
upscaling B-DAT
factor, O
we O
train O
a O
specific O

trained O
on O
ImageNet O
with O
an O
upscaling B-DAT
factor O
of O
3: O
(a) O
shows O

Monarch” O
from O
Set14 O
with O
an O
upscaling B-DAT
factor O
of O
3. O
PSNR O
values O

IHR, O
where O
r O
is O
the O
upscaling B-DAT
factor. O
To O
synthesize O
the O
low-resolution O

and O
sub-sample O
it O
by O
the O
upscaling B-DAT
factor. O
The O
sub-images O
are O
extracted O

images O
from O
ImageNet O
[30] O
for O
upscaling B-DAT
factor O
of O
3. O
We O
use O

384022” O
from O
BSD500 O
with O
an O
upscaling B-DAT
factor O
of O
3. O
PSNR O
values O

worse O
results O
than O
an O
adaptive O
upscaling B-DAT
for O
SISR O
and O
requires O
more O

bench O
mark O
data O
set O
with O
upscaling B-DAT
factor O
of O
4 O
shows O
that O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
IEEE O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ture O
where O
the O
feature O
maps O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
the O
non-invertible O
low-pass O
filtering O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
endorfer O
et O
al. O
[27] O
suggested O

- B-DAT
work O
(CNN) O
inspired O
by O
sparse-coding O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gorithms, O
especially O
their O
computational O
and O

- B-DAT
els O
to O
learn O
nonlinear O
relationships O

- B-DAT

- B-DAT

- B-DAT
creasing O
the O
resolution O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rectly O
fed O
to O
the O
network O

- B-DAT
tion O
and O
filter O
size O
reduction O

- B-DAT

- B-DAT
time O
as O
shown O
in O
Sec O

- B-DAT
work O
implicitly O
learns O
the O
processing O

- B-DAT
ing O
[7, O
3, O
31]. O
We O

- B-DAT

- B-DAT

IHR O
using O
a O
Gaussian O
filter O
- B-DAT
thus O
simulating O
the O
camera’s O
point O

spread O
function O
- B-DAT
then O
downsample O
the O
image O
by O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
scaling O
factor O
of O
3. O
The O

- B-DAT

- B-DAT

- B-DAT
tern, O
according O
to O
its O
location O

- B-DAT

- B-DAT
ranges O
the O
elements O
of O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
shuffle O
the O
training O
data O
to O

- B-DAT
ing O
the O
original O
data O
including O

- B-DAT
sampled O
data, O
super-resolved O
data, O
overall O

- B-DAT

- B-DAT
mark O
datasets O
including O
the O
Timofte O

- B-DAT
imately O
10 O
seconds O
in O
length O

- B-DAT

-5 B-DAT

-5 B-DAT
model O
[7], O
(b) O
shows O
weights O

- B-DAT

- B-DAT

-5 B-DAT

- B-DAT
5 O
model O
and O
the O
equations O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pixel O
convolution O
layer O
as O
well O

- B-DAT

-1 B-DAT

-5 B-DAT
model O
[6]. O
Here, O
we O
follow O

- B-DAT

-5 B-DAT

-5 B-DAT
ImageNet O
model O
from O
[7] O
in O

- B-DAT
ilarity O
to O
designed O
features O
including O

- B-DAT

- B-DAT
geNet O
images. O
Results O
in O
Tab O

- B-DAT

- B-DAT

- B-DAT

-5 B-DAT

-5 B-DAT
ImageNet O
model O
in O
this O
section O

-5 B-DAT

-5 B-DAT
ImageNet O
model, O
whilst O
being O
close O

- B-DAT

- B-DAT
put O
image O
to O
HR O
space O

- B-DAT
resolved O
images O
is O
given O
in O

- B-DAT

- B-DAT

-5 B-DAT

-5 B-DAT
ImageNet O
model. O
The O
improvement O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cluding O
our O
own, O
a O
python/theano O

-5 B-DAT

-5 B-DAT
ImageNet O
model, O
the O
number O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
esting O
to O
explore O
ensemble O
prediction O

- B-DAT
resolution O
using O
videos O
from O
the O

-5 B-DAT

-5 B-DAT
ImageNet O
model O
takes O
0.435s O
per O

-5 B-DAT

-5 B-DAT
ImageNet O
model O
takes O
0.434s O
per O

- B-DAT
CNN O
9-5-5 O
ImageNet O
model O
[7 O

- B-DAT

-5 B-DAT

-5 B-DAT
ImageNet O
model O
[7] O
and O
ESPCN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
complexity. O
To O
address O
the O

- B-DAT

- B-DAT

-5 B-DAT

-5 B-DAT
ImageNet O
model O
[7] O
and O
ESPCN O

- B-DAT

- B-DAT

-3 B-DAT

-3 B-DAT
vs O
9-5-5). O
This O
makes O
our O

- B-DAT
implicit O
redundancy O
that O
can O
be O

- B-DAT
resolution O
as O
has O
been O
shown O

- B-DAT

- B-DAT
mation O
from O
videos O
for O
human O

- B-DAT

- B-DAT

- B-DAT

Sequences O
- B-DAT
A O
Review. O
Midwest O
Symposium O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
by O
adaptive O
sparse O
domain O

- B-DAT
ization. O
IEEE O
Transactions O
on O
Image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
invariant O
group-sparse O
regularization. O
In O
IEEE O

- B-DAT
ence O
on O
Computer O
Vision O
(ICCV O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
with O
deep O
convolutional O
neural O

- B-DAT
bard, O
and O
L. O
D. O
Jackel O

- B-DAT
propagation O
network. O
In O
Advances O
in O

- B-DAT
resolution O
with O
fast O
approximate O
convolutional O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ping O
of O
rural O
land O
cover O

- B-DAT

- B-DAT

- B-DAT
sion O
for O
fast O
example-based O
super-resolution O

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
(ACCV O

- B-DAT

- B-DAT

- B-DAT
sketch O
synthesis. O
In O
IEEE O
Conference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
networks. O
In O
Computer O
Vision–ECCV O

- B-DAT
national O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Bischof, O
"Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests," O
in O
Proceedings O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
second- O
order O
gradients O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
second-order O
gradients O
and O
their O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
patches, O
with O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Although O
the O
𝑙0 O
- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-2 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

orientation, O
with O
a O
value O
between O
-90 B-DAT

-1 B-DAT

-2 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
show O
that O
the O
proposed O
GWRR O

-1 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT
and O
Table-3 O
validate O
that O
using O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT
and O
Table O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-3 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
FARF O
FARF* O
SRCNN O

-2 B-DAT

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT
summarizes O
the O
performances O
of O
our O

-3 B-DAT
gives O
more O
details O
of O
the O

- B-DAT
FARF O

-3 B-DAT

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT
also O
shows O
that O
the O
fine-tuned O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-2014 B-DAT

-2015 B-DAT

-2016 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-918 B-DAT

- B-DAT

- B-DAT

-1927 B-DAT

-1588, B-DAT
1997 O

- B-DAT

- B-DAT

-730 B-DAT

- B-DAT

-126 B-DAT

-10 B-DAT

-3311 B-DAT

- B-DAT

-3799 B-DAT

- B-DAT

-26, B-DAT
2017 O

-167, B-DAT
1998 O

- B-DAT

- B-DAT

-297, B-DAT
1995 O

-156 B-DAT

-32, B-DAT
2001 O

-1874 B-DAT

- B-DAT

- B-DAT

- B-DAT

-333 B-DAT

-423 B-DAT

-1692 B-DAT

- B-DAT

-192, B-DAT
2015 O

-515 B-DAT

-424 B-DAT

-156 B-DAT

-1232, B-DAT
2001 O

- B-DAT

-157 B-DAT

- B-DAT

-2873, B-DAT
2010 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-568 B-DAT

- B-DAT
resolution," O
IEEE O
Transactions O
on O
Image O

-861, B-DAT
2015 O

- B-DAT

-4 B-DAT

- B-DAT

-26, B-DAT
2017 O

- B-DAT

-595 B-DAT

- B-DAT
Resolution," O
IEEE O
Transactions O
on O
Multimedia O

-417, B-DAT
2016 O

- B-DAT

-199 B-DAT

- B-DAT

-307, B-DAT
2016 O

- B-DAT

-1654 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
1527, O
2001 O

- B-DAT

- B-DAT

-896, B-DAT
2008 O

- B-DAT

-2238, B-DAT
2006 O

- B-DAT

-013011 B-DAT

-20, B-DAT
2010 O

- B-DAT

- B-DAT

-2072 B-DAT

- B-DAT

- B-DAT

-2357 B-DAT

- B-DAT

- B-DAT

- B-DAT

-180 B-DAT

-2774, B-DAT
2013 O

-1728, B-DAT
2012 O

- B-DAT

- B-DAT

- B-DAT
Based O
Super-Resolution", O
IEEE O
Transactions O
on O

- B-DAT

- B-DAT

- B-DAT
2929, O
2013 O

-352, B-DAT
November O
2009 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

the O
Set5 O
dataset O
with O
an O
upscaling B-DAT
factor O
3). O
The O
proposed O
method O

kernel, O
sub-sample O
it O
by O
the O
upscaling B-DAT
factor, O
and O
upscale O
it O
by O

larger O
Set14 O
set O
[51]. O
The O
upscaling B-DAT
factor O
is O
3. O
We O
use O

on O
the O
ImageNet O
by O
an O
upscaling B-DAT
factor O
3. O
Please O
refer O
to O

our O
published O
implementation O
for O
upscaling B-DAT
factors O
2 O
and O
4. O
Interestingly O

trained O
on O
ImageNet O
with O
an O
upscaling B-DAT
factor O
3. O
The O
filters O
are O

test O
on O
Set5 O
with O
an O
upscaling B-DAT
factor O
3. O
The O
results O
observed O

4.1. O
The O
results O
with O
an O
upscaling B-DAT
factor O
3 O
on O
Set5 O
are O

on O
the O
ImageNet. O
For O
each O
upscaling B-DAT
factor O
∈ O
{2, O
3, O
4 O

to O
evaluate O
the O
performance O
of O
upscaling B-DAT
factors O
2, O
3, O
and O
4 O

108 O
backpropagations. O
Specifically, O
for O
the O
upscaling B-DAT
factor O
3, O
the O
average O
gains O

of O
different O
approaches O
by O
an O
upscaling B-DAT
factor O
3. O
As O
can O
be O

for O
fair O
quantitative O
comparison. O
The O
upscaling B-DAT
factor O
is O
3 O
and O
the O

only O
evaluate O
the O
performance O
of O
upscaling B-DAT
factor O
3. O
Comparisons. O
We O
compare O

network O
to O
cope O
with O
different O
upscaling B-DAT
factors O

Fattal, O
R.: O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

H.: O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In: O
IEEE O

image O
from O
Set5 O
with O
an O
upscaling B-DAT
factor O
3 O

image O
from O
Set14 O
with O
an O
upscaling B-DAT
factor O
3 O

image O
from O
Set14 O
with O
an O
upscaling B-DAT
factor O
3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
offs O
between O
performance O
and O
speed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
image, O
is O
a O
classical O

- B-DAT

- B-DAT
tiplicity O
of O
solutions O
exist O
for O

- B-DAT

- B-DAT
verse O
problem, O
of O
which O
solution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
exemplar O
pairs O
[2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
structing O
high-resolution O
patches. O
The O
overlapping O

- B-DAT

- B-DAT

- B-DAT
work O
[27] O
(more O
details O
in O

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
images. O
Our O
method O

- B-DAT
tally O
from O
existing O
external O
example-based O

- B-DAT
processing O

- B-DAT

- B-DAT
volutional O
Neural O
Network O
(SRCNN)1. O
The O

- B-DAT
ture O
is O
intentionally O
designed O
with O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
example-based O
methods. O
Furthermore, O
the O

- B-DAT

- B-DAT
work O
for O
image O
super-resolution. O
The O

- B-DAT
rectly O
learns O
an O
end-to-end O
mapping O

- B-DAT
and O
high-resolution O
images, O
with O
little O

- B-DAT
processing O
beyond O
the O
optimization O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution, O
and O
can O
achieve O
good O

- B-DAT

- B-DAT
linear O
mapping O
layers. O
Secondly, O
we O

- B-DAT
strate O
that O
performance O
can O
be O

- B-DAT

- B-DAT
ber O
of O
recently O
published O
methods O

- B-DAT

- B-DAT

- B-DAT
olution O
algorithms O
can O
be O
categorized O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
similarity O
property O
and O
generate O
exemplar O

- B-DAT
nal O
example-based O
methods O
[2], O
[4 O

- B-DAT
resolution O
patches O
from O
external O
datasets O

- B-DAT

- B-DAT
tionaries O
are O
directly O
presented O
as O

- B-DAT

- B-DAT

- B-DAT
sponding O
high-resolution O
patch O
used O
for O

- B-DAT
nique O
as O
an O
alternative O
to O

- B-DAT
borhood O
regression O
[41], O
[42] O
are O

- B-DAT
coding-based O
method O
and O
its O
several O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tioned O
methods O
first O
transform O
the O

- B-DAT
ferent O
color O
space O
(YCbCr O
or O

- B-DAT

- B-DAT
fully O
applied O
to O
other O
computer O

- B-DAT

- B-DAT
ceptron O
(MLP), O
whose O
all O
layers O

- B-DAT

- B-DAT

- B-DAT
work O
is O
applied O
for O
natural O

- B-DAT
moving O
noisy O
patterns O
(dirt/rain) O
[12 O

- B-DAT

- B-DAT

- B-DAT
resolution O
pipeline O
under O
the O
notion O

- B-DAT
based O
approach O
[16]. O
The O
deep O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
end O
mapping. O
Further, O
the O
SRCNN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
extracts O
(overlapping) O
patches O
from O

- B-DAT
resolution O
image O
Y O
and O
represents O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
neural O
network. O
An O
overview O

- B-DAT

- B-DAT
spectively, O
and O
’∗’ O
denotes O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tors O
is O
conceptually O
a O
representation O

- B-DAT

- B-DAT

- B-DAT
plexity O
of O
the O
model O
(n2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
patch). O
Motivated O
by O
this O

- B-DAT
lutional O
layer O
to O
produce O
the O

- B-DAT

- B-DAT

- B-DAT
tion O
and O
representation) O
becomes O
purely O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution) O
dictionary. O
If O
the O
dictionary O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
based O
SR O
method O
can O
be O

- B-DAT
volutional O
neural O
network O
(with O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
parameters. O
For O
example, O
we O
can O

- B-DAT
resolution O
patch O
(to O
the O
extreme O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
quires O
the O
estimation O
of O
network O

- B-DAT
imizing O
the O
loss O
between O
the O

- B-DAT
resolution O
images O
X. O
Given O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
crafted” O
methods. O
Despite O
that O
the O

- B-DAT
tory O
performance O
when O
the O
model O

- B-DAT
scent O
with O
the O
standard O
backpropagation O

- B-DAT
ular, O
the O
weight O
matrices O
are O

- B-DAT
erations, O
η O
is O
the O
learning O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ping O
and O
require O
some O
averaging O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
layers O
have O
no O
padding O

- B-DAT

- B-DAT
age O
[26]. O
We O
have O
also O

- B-DAT
tions O
between O
super-resolution O
performance O
and O

- B-DAT
quently, O
we O
compare O
our O
method O

- B-DAT

- B-DAT
the-arts O
both O
quantitatively O
and O
qualitatively O

- B-DAT

- B-DAT
4.4, O
so O
c O
= O
1 O

- B-DAT
tion O
training O
partition. O
The O
size O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nal O
images O
with O
a O
stride O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
geNet O
is O
about O
the O
same O

- B-DAT

- B-DAT
formance O
may O
be O
further O
boosted O

- B-DAT

- B-DAT
tured O
sufficient O
variability O
of O
natural O

- B-DAT

- B-DAT

Laplacian/Gaussian O
filters, O
the O
filters O
a O
- B-DAT
e O
are O
like O
edge O
detectors O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
crease O
the O
network O
width6, O
i.e O

- B-DAT
coding-based O
method O
(31.42 O
dB O

-1 B-DAT

-5 B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-7 B-DAT

- B-DAT
ing O
[17]. O
The O
term O
‘width O

-3 B-DAT

-5 B-DAT

-5 B-DAT

-5 B-DAT

-3 B-DAT

- B-DAT
5 O
and O
9-5-5 O
on O
Set5 O

-1 B-DAT

-5, B-DAT
9-3-5, O
and O
9-5-5 O
is O
8,032 O

-5 B-DAT

-5 B-DAT
is O
almost O
twice O
of O
9-3-5 O

- B-DAT

- B-DAT

-1 B-DAT

-1 B-DAT

-5, B-DAT
9-3-1-5, O
9-5-1-5, O
which O
add O
an O

-1 B-DAT

-5, B-DAT
9-3-5, O
and O
9-5-5, O
respectively. O
The O

- B-DAT
ditional O
layer O
are O
the O
same O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lution O
is O
found O
not O
as O

-1 B-DAT

-5 B-DAT
network, O
then O
the O
performance O
degrades O

- B-DAT

- B-DAT
ure O
9(a)). O
If O
we O
go O

- B-DAT

-1 B-DAT

-5 B-DAT
vs. O
9-1-1-5 O

-3 B-DAT

-5 B-DAT
vs. O
9-3-1-5 O

-5 B-DAT

-5 B-DAT
vs. O
9-5-1-5 O

- B-DAT

- B-DAT

-1 B-DAT

-5, B-DAT
then O
we O
have O
to O
set O

-3 B-DAT

- B-DAT
3-5 O
and O
9-3-3-3. O
However, O
from O

-3 B-DAT

-1 B-DAT

-5 B-DAT
network O

- B-DAT

- B-DAT

- B-DAT
vestigations O
to O
better O
understand O
gradients O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-1 B-DAT

-5 B-DAT
(n22 O
= O
32) O
and O
9-1-1-1-5 O

-3 B-DAT

-3 B-DAT

-5 B-DAT
and O
9-3-3-3 O

- B-DAT
speed O
trade-off: O
a O
three-layer O
network O

- B-DAT
of-the-art O
SR O
methods O

SC O
- B-DAT
sparse O
coding-based O
method O
of O
Yang O

et O
al. O
[50] O
• O
NE+LLE O
- B-DAT
neighbour O
embedding O
+ O
locally O
linear O

embedding O
method O
[4] O
• O
ANR O
- B-DAT
Anchored O
Neighbourhood O
Regression O

method O
[41] O
• O
A+ O
- B-DAT
Adjusted O
Anchored O
Neighbourhood O
Regres O

method O
[42], O
and O
• O
KK O
- B-DAT
the O
method O
described O
in O
[25 O

- B-DAT
based O
methods, O
according O
to O
the O

- B-DAT
sampled O
using O
the O
same O
bicubic O

- B-DAT
terion O
(IFC) O
[38], O
noise O
quality O

- B-DAT

- B-DAT

- B-DAT
scale O
structure O
similarity O
index O
(MSSSIM O

ANR O
- B-DAT
31.92 O
dB O

A+ O
- B-DAT
32.59 O
dB O

SC O
- B-DAT
31.42 O
dB O

Bicubic O
- B-DAT
30.39 O
dB O

NE+LLE O
- B-DAT
31.84 O
dB O

KK O
- B-DAT
32.28 O
dB O

- B-DAT
CNN O
outperforms O
existing O
state-of-the-art O
methods O

- B-DAT

- B-DAT

- B-DAT
cific O
network O
(9-5-5) O
using O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentation, O
whereas O
ours O
are O
in O

- B-DAT

-1 B-DAT

-5, B-DAT
9-3-5, O
and O
9-5-5. O
It O
is O

- B-DAT
1-5 O
network O
is O
the O
fastest O

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-5 B-DAT
network. O
Note O
the O
speed O
gap O

- B-DAT
pletely O
feed-forward. O
The O
9-5-5 O
network O

- B-DAT

- B-DAT

- B-DAT
terpolation. O
It O
is O
interesting O
to O

- B-DAT

- B-DAT
out O
altering O
the O
learning O
mechanism O

- B-DAT
sign. O
In O
particular, O
it O
can O

- B-DAT
nels O
simultaneously O
by O
setting O
the O

- B-DAT

- B-DAT

-5 B-DAT

- B-DAT

A+ O
[41] O
SRCNN O
2 O
33.66 O
- B-DAT
35.77 O
36.20 O
35.83 O
36.54 O
36.66 O

31.92 O
32.59 O
32.75 O
4 O
28.42 O
- B-DAT
29.61 O
30.03 O
29.69 O
30.28 O
30.49 O

2 O
0.9299 O
- B-DAT
0.9490 O
0.9511 O
0.9499 O
0.9544 O
0.9542 O

0.8968 O
0.9088 O
0.9090 O
4 O
0.8104 O
- B-DAT
0.8402 O
0.8541 O
0.8419 O
0.8603 O
0.8628 O

2 O
6.10 O
- B-DAT
7.84 O
6.87 O
8.09 O
8.48 O
8.05 O

4.52 O
4.84 O
4.58 O
4 O
2.35 O
- B-DAT
2.94 O
2.81 O
3.02 O
3.26 O
3.01 O

2 O
36.73 O
- B-DAT
42.90 O
39.49 O
43.28 O
44.58 O
41.13 O

33.10 O
34.48 O
33.21 O
4 O
21.42 O
- B-DAT
25.56 O
24.99 O
25.72 O
26.97 O
25.96 O

2 O
50.06 O
- B-DAT
58.45 O
57.15 O
58.61 O
60.06 O
59.49 O

46.02 O
47.17 O
47.10 O
4 O
37.21 O
- B-DAT
39.85 O
40.40 O
40.01 O
41.03 O
41.13 O

2 O
0.9915 O
- B-DAT
0.9953 O
0.9953 O
0.9954 O
0.9960 O
0.9959 O

0.9844 O
0.9867 O
0.9866 O
4 O
0.9516 O
- B-DAT
0.9666 O
0.9695 O
0.9672 O
0.9720 O
0.9725 O

A+ O
[41] O
SRCNN O
2 O
30.23 O
- B-DAT
31.76 O
32.11 O
31.80 O
32.28 O
32.45 O

28.65 O
29.13 O
29.30 O
4 O
26.00 O
- B-DAT
26.81 O
27.14 O
26.85 O
27.32 O
27.50 O

2 O
0.8687 O
- B-DAT
0.8993 O
0.9026 O
0.9004 O
0.9056 O
0.9067 O

0.8093 O
0.8188 O
0.8215 O
4 O
0.7019 O
- B-DAT
0.7331 O
0.7419 O
0.7352 O
0.7491 O
0.7513 O

2 O
6.09 O
- B-DAT
7.59 O
6.83 O
7.81 O
8.11 O
7.76 O

4.23 O
4.45 O
4.26 O
4 O
2.23 O
- B-DAT
2.71 O
2.57 O
2.78 O
2.94 O
2.74 O

2 O
40.98 O
- B-DAT
41.34 O
38.86 O
41.79 O
42.61 O
38.95 O

37.22 O
38.24 O
35.25 O
4 O
26.15 O
- B-DAT
31.17 O
29.18 O
31.27 O
32.31 O
30.46 O

2 O
47.64 O
- B-DAT
54.47 O
53.85 O
54.57 O
55.62 O
55.39 O

43.36 O
44.25 O
44.32 O
4 O
35.71 O
- B-DAT
37.75 O
38.26 O
37.85 O
38.72 O
38.87 O

2 O
0.9813 O
- B-DAT
0.9886 O
0.9890 O
0.9888 O
0.9896 O
0.9897 O

0.9647 O
0.9669 O
0.9675 O
4 O
0.9134 O
- B-DAT
0.9317 O
0.9338 O
0.9326 O
0.9371 O
0.9376 O

A+ O
[41] O
SRCNN O
2 O
28.38 O
- B-DAT
29.67 O
30.02 O
29.72 O
30.14 O
30.29 O

26.72 O
27.05 O
27.18 O
4 O
24.65 O
- B-DAT
25.21 O
25.38 O
25.25 O
25.51 O
25.60 O

2 O
0.8524 O
- B-DAT
0.8886 O
0.8935 O
0.8900 O
0.8966 O
0.8977 O

0.7843 O
0.7945 O
0.7971 O
4 O
0.6727 O
- B-DAT
0.7037 O
0.7093 O
0.7060 O
0.7171 O
0.7184 O

2 O
5.30 O
- B-DAT
7.10 O
6.33 O
7.28 O
7.51 O
7.21 O

3.91 O
4.07 O
3.91 O
4 O
1.95 O
- B-DAT
2.45 O
2.24 O
2.51 O
2.62 O
2.45 O

2 O
36.84 O
- B-DAT
41.52 O
38.54 O
41.72 O
42.37 O
39.66 O

34.81 O
35.58 O
34.72 O
4 O
21.72 O
- B-DAT
25.15 O
24.87 O
25.27 O
26.01 O
25.65 O

2 O
46.15 O
- B-DAT
52.56 O
52.21 O
52.69 O
53.56 O
53.58 O

41.53 O
42.19 O
42.29 O
4 O
34.86 O
- B-DAT
36.52 O
36.80 O
36.64 O
37.18 O
37.24 O

2 O
0.9780 O
- B-DAT
0.9869 O
0.9876 O
0.9872 O
0.9883 O
0.9883 O

0.9581 O
0.9609 O
0.9614 O
4 O
0.9005 O
- B-DAT
0.9203 O
0.9215 O
0.9214 O
0.9256 O
0.9261 O

-1 B-DAT

-5 B-DAT

-3 B-DAT

-5 B-DAT

-5 B-DAT

-5 B-DAT

- B-DAT
of-the-art O
super-resolution O
quality, O
whilst O
maintains O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-art O
color O
SR O
method O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nel O
when O
training O
is O
performed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
correlation O
among O
each O
other. O
The O

- B-DAT

- B-DAT
channel O
network O
(“Y O
only”). O
It O

- B-DAT

- B-DAT
work O
is O
not O
that O
significant O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
images, O
with O
little O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vantages O
of O
simplicity O
and O
robustness O

- B-DAT

- B-DAT
blurring O
or O
simultaneous O
SR+denoising. O
One O

- B-DAT

- B-DAT
complexity O
single-image O
super-resolution O
based O
on O

- B-DAT

- B-DAT
bor O
embedding. O
In: O
IEEE O
Conference O

- B-DAT

- B-DAT

- B-DAT
resolution. O
IEEE O
Transactions O
on O
Image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
linear O
structure O
within O
convolutional O

- B-DAT
tems O
(2014 O

- B-DAT
tional O
network O
for O
image O
super-resolution O

- B-DAT
ence O
on O
Computer O
Vision, O
pp O

- B-DAT
tional O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT

- B-DAT
resolution. O
Computer O
Graphics O
and O
Applications O

- B-DAT
level O
vision. O
International O
Journal O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
lutional O
neural O
networks O
with O
low O

- B-DAT
tems. O
pp. O
769–776 O
(2008 O

- B-DAT

- B-DAT

- B-DAT
ten O
zip O
code O
recognition. O
Neural O

- B-DAT

- B-DAT
rithms. O
In: O
Advances O
in O
Neural O

- B-DAT
mentation O
algorithms O
and O
measuring O
ecological O

- B-DAT

- B-DAT

- B-DAT
tion. O
In: O
IEEE O
International O
Conference O

- B-DAT
chine O
learning O
approach O
for O
non-blind O

- B-DAT

- B-DAT
tics. O
IEEE O
Transactions O
on O
Image O

- B-DAT
tation O
by O
joint O
identification-verification. O
In O

- B-DAT
quality O
object O
detection. O
arXiv O
preprint O

- B-DAT

- B-DAT

- B-DAT
ternational O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
ilarity O
for O
image O
quality O
assessment O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tionary O
training O
for O
image O
super-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
sparse-representations. O
In: O
Curves O
and O

- B-DAT

- B-DAT
CNNs O
for O
fine-grained O
category O
detection O

- B-DAT
ence O
on O
Computer O
Vision. O
pp O

- B-DAT
tion O
Engineering O
from O
Beijing O
Institute O

- B-DAT
nology, O
China, O
in O
2011. O
He O

- B-DAT
sity O
of O
Hong O
Kong. O
His O

- B-DAT

- B-DAT
versity O
of O
London O
in O
2010 O

- B-DAT
toral O
researcher O
at O
Vision O
Semantics O

- B-DAT
inghua O
University O
in O
2007, O
and O

- B-DAT
ence O
on O
Computer O
Vision O
and O

- B-DAT
tion O
(CVPR) O
2009. O
He O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sachusetts O
Institute O
of O
Technology, O
Cambridge O

- B-DAT
cessing. O
He O
received O
the O
Best O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Bischof, O
"Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests," O
in O
Proceedings O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

- B-DAT

-2 B-DAT

-3 B-DAT

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT
and O
Table O

- B-DAT

- B-DAT

-2 B-DAT
and O
Table-3 O

-1 B-DAT
3 O
2.449±0.029 O
2.236±0.015 O
2.206±0.027 O
(10 O

-3 B-DAT
3 O
6.264±0.042 O
5.952±0.323 O
4.622±0.299 O
(26 O

-2 B-DAT
3 O
6.501±0.199 O
6.308±0.330 O
6.272±0.332 O
(04 O

-2 B-DAT
3 O
6.889±0.199 O
5.196±0.127 O
4.864±0.267 O
(29 O

-2 B-DAT
3 O
3.418±0.171 O
3.377±0.164 O
2.969±0.120 O
(13 O

-1 B-DAT
3 O
1.026±0.158 O
0.391±0.007 O
0.293±0.004 O
(71 O

-2 B-DAT
3 O
6.527±0.203 O
6.520±0.188 O
6.285±0.101 O
(04 O

-2 B-DAT

- B-DAT

-4 B-DAT

-3 B-DAT

-2 B-DAT

-1 B-DAT

-1 B-DAT

-3 B-DAT

- B-DAT

-2, B-DAT
the O
number O
hyperplane(s) O
#ℋ O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
JMPF O
JMPF O

-4 B-DAT

- B-DAT

- B-DAT

- B-DAT

-4 B-DAT
and O
Tables-5, O
where O
JMPF O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-100 B-DAT
dataset O
[22] O
are O
used, O
so O

- B-DAT
JMPF O
JMPF O

- B-DAT
JMPF O
JMPF O

- B-DAT
JMPF O
JMPF O

-5 B-DAT

- B-DAT

- B-DAT

- B-DAT

-4 B-DAT
tabulates O
the O
performances, O
in O
terms O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-4 B-DAT

-5 B-DAT
provides O
more O
details O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
2929, O
2013 O

- B-DAT

- B-DAT

-1927 B-DAT

-1588, B-DAT
1997 O

- B-DAT

- B-DAT

-730 B-DAT

- B-DAT

-126 B-DAT

-10 B-DAT

-3311 B-DAT

- B-DAT

-3799 B-DAT

- B-DAT

-918 B-DAT

-167, B-DAT
1998 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-023008, B-DAT
2017 O

-156 B-DAT

-32, B-DAT
2001 O

-1874 B-DAT

- B-DAT

- B-DAT

-1873 B-DAT

- B-DAT

- B-DAT

-333 B-DAT

-423 B-DAT

-1692 B-DAT

- B-DAT

-26, B-DAT
2017 O

-515 B-DAT

-424 B-DAT

-156 B-DAT

-1232, B-DAT
2001 O

- B-DAT

-157 B-DAT

- B-DAT

-2873, B-DAT
2010 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-568 B-DAT

- B-DAT
resolution," O
IEEE O
Transactions O
on O
Image O

-861, B-DAT
2015 O

- B-DAT

-4 B-DAT

- B-DAT

-192, B-DAT
2015 O

- B-DAT

-595 B-DAT

- B-DAT
Resolution," O
IEEE O
Transactions O
on O
Multimedia O

-417, B-DAT
2016 O

- B-DAT

-199 B-DAT

- B-DAT

-407 B-DAT

- B-DAT

-1654 B-DAT

- B-DAT

- B-DAT

- B-DAT

-3245, B-DAT
2015 O

- B-DAT

- B-DAT

-333 B-DAT

- B-DAT

- B-DAT

-104 B-DAT

- B-DAT

- B-DAT

-297, B-DAT
1995 O

- B-DAT

- B-DAT
1527, O
2001 O

- B-DAT

- B-DAT

-896, B-DAT
2008 O

- B-DAT

-2238, B-DAT
2006 O

- B-DAT

-013011 B-DAT

-20, B-DAT
2010 O

-1874, B-DAT
2008 O

-954 B-DAT

- B-DAT

-294, B-DAT
2014 O

BSD100 B-DAT

and O
our O
model O
for O
the O
BSD100 B-DAT

value. O
For O
instance, O
for O
the O
BSD100 B-DAT

PI O
values O
measured O
for O
the O
BSD100 B-DAT

input O
images O
are O
from O
the O
BSD100 B-DAT

for O
10 O
images O
of O
the O
BSD100 B-DAT

selected O
ten O
images O
in O
the O
BSD100 B-DAT

ground-truth O
images O
are O
from O
the O
BSD100 B-DAT

BSD100 B-DAT

BSD100 B-DAT

BSD100 B-DAT

BSD100 B-DAT

ground-truth O
images O
are O
from O
the O
BSD100 B-DAT

a O
deep O
network O
for O
multi-pass O
upscaling B-DAT
in O
company O
with O
a O
discriminator O

deep O
learning-based O
super-resolution O
model, O
enhanced O
upscaling B-DAT
super-resolution O
(EUSR) O
[14], O
from O
the O

to O
prop- O
erly O
regularize O
the O
upscaling B-DAT
modules O
to O
keep O
balance O
of O

pass O
perceptual O
super-resolution O
with O
enhanced O
upscaling B-DAT
(4PP-EUSR),” O
which O
is O
based O
on O

our O
model O
employs O
so-called O
“multi-pass O
upscaling B-DAT

low-resolution O
image O
through O
the O
multiple O
upscaling B-DAT
paths O
in O
our O
model O
are O

base O
deep O
learning O
model, O
multi-pass O
upscaling B-DAT
for O
training, O
structure O
of O
the O

which O
consists O
of O
so-called O
“enhanced O
upscaling B-DAT
modules” O
and O
performed O
well O
in O

nents O
(Fig. O
2): O
a O
multi-scale O
upscaling B-DAT
model, O
employing O
the O
model O
in O

three O
upscaled O
images O
via O
multi-pass O
upscaling B-DAT
(Section O
3.2). O
The O
discriminator O
tries O

3.1 O
Enhanced O
upscaling B-DAT
super-resolution O

shared O
feature O
extraction, O
and O
enhanced O
upscaling B-DAT

features O
are O
upscaled O
via O
“enhanced O
upscaling B-DAT
modules,” O
where O
each O
module O
increases O

one, O
two, O
and O
three O
enhanced O
upscaling B-DAT
modules, O
respectively. O
The O
configurable O
parameters O

residual O
blocks O
in O
the O
enhanced O
upscaling B-DAT
modules. O
We O
consider O
EUSR O
as O

our O
base O
upscaling B-DAT
model O
because O
it O
is O
one O

3.2 O
Multi-pass O
upscaling B-DAT

our O
model O
utilizes O
all O
these O
upscaling B-DAT
paths O
to O
produce O
three O
output O

output O
images O
have O
the O
same O
upscaling B-DAT
factor O
of O
4 O
for O
a O

Enhanced O
upscaling B-DAT
moduleEUM O

Fig. O
4. O
Multi-pass O
upscaling B-DAT
process, O
which O
produces O
three O
upscaled O

the O
other O
hand, O
our O
multi-pass O
upscaling B-DAT
extends O
it O
with O
a O
different O

three O
images O
obtained O
from O
different O
upscaling B-DAT
paths O
are O
used O
for O
training O

that O
may O
occur O
during O
direct O
upscaling B-DAT
via O
the O
×4 O
path, O
two-pass O

upscaling B-DAT
via O
the O
×2 O
path, O
and O

upscaling B-DAT
via O
the O
×8 O
path O
and O

the O
model O
to O
handle O
various O
upscaling B-DAT
scenarios O

ground-truth O
images. O
This O
helps O
our O
upscaling B-DAT
model O
generating O
more O
natural O
images O

networks: O
EUSR O
[14] O
as O
an O
upscaling B-DAT
model O
and O
SRGAN O
[17] O
as O

two O
newly O
proposed O
components: O
multi-pass O
upscaling B-DAT
and O
qualitative O
score O
predictors. O
In O

Thanks O
to O
the O
multi-pass O
upscaling, B-DAT
the O
proposed O
model O
can O
learn O

various O
upscaling B-DAT
patterns, O
which O
will O
be O
further O

the O
residual O
module O
and O
the O
upscaling B-DAT
part O
of O
the O
EUSR O
model O

images. O
Then, O
one O
of O
the O
upscaling B-DAT
paths O
(i.e., O
×2, O
×4, O
and O

effective O
batch O
sizes O
of O
the O
upscaling B-DAT
and O
discrimina- O
tive O
models O
are O

the O
outputs O
obtained O
from O
different O
upscaling B-DAT
paths, O
comparing O
the O
per- O
formance O

trained O
with O
and O
without O
multi-pass O
upscaling, B-DAT
inves- O
tigating O
the O
roles O
of O

interpolation. O
It O
is O
a O
traditional O
upscaling B-DAT
method, O
which O
inter- O
polates O
pixel O

which O
supports O
multiple O
factors O
of O
upscaling B-DAT

5.2 O
Comparing O
upscaling B-DAT
paths O

images O
by O
utilizing O
all O
the O
upscaling B-DAT
paths: O
by O
passing O
through O
the O

10. O
Images O
reconstructed O
by O
different O
upscaling B-DAT
paths O
of O
our O
model. O
The O

results O
obtained O
from O
the O
different O
upscaling B-DAT
paths O
to O
examine O
what O
aspects O

the O
performance O
of O
the O
three O
upscaling B-DAT
paths O
of O
our O
model. O
While O

PI O
values. O
This O
implies O
that O
upscaling B-DAT
using O
the O
×2 O
path O
or O

different O
de- O
pending O
on O
the O
upscaling B-DAT
paths, O
although O
the O
overall O
patterns O

obtained O
by O
the O
two- O
pass O
upscaling B-DAT
using O
the O
×2 O
path O
contains O

two O
passes, O
thus O
the O
two-pass O
upscaling B-DAT
is O
not O
fully O
optimized. O
Second O

trained O
with O
and O
without O
multi-pass O
upscaling B-DAT
for O
the O
Set5 O
[2], O
Set14 O

These O
results O
show O
that O
each O
upscaling B-DAT
path O
of O
our O
model O
learns O

the O
shared O
part O
of O
the O
upscaling B-DAT
paths O
(i.e., O
the O
intermediate O
residual O

5.3 O
Effectiveness O
of O
multi-pass O
upscaling B-DAT

The O
4PP-EUSR O
model O
employs O
multi-pass O
upscaling B-DAT
as O
aforementioned O
in O
Sec- O
tion O

trained O
with O
and O
without O
multi-pass O
upscaling B-DAT

demonstrates O
that O
employing O
multi- O
pass O
upscaling B-DAT
is O
beneficial O
to O
enhance O
both O

The O
model O
trained O
with O
multi-pass O
upscaling B-DAT
shows O
larger O
PSNR O
and O
SSIM O

This O
confirms O
that O
the O
multi-pass O
upscaling B-DAT
can O
improve O
the O
overall O
quality O

Deep O
residual O
network O
with O
enhanced O
upscaling B-DAT
module O
for O
super-resolution. O
In: O
Proceedings O

Set5 O
[2], O
Set14 O
[40], O
and O
BSD100 B-DAT
[21] O
datasets. O
Each O
dataset O
contains O

Set5 O
[2], O
Set14 O
[40], O
and O
BSD100 B-DAT
[21] O
datasets. O
The O
models O
are O

BSD100 B-DAT
PSNR O
(dB) O
SSIM O
NIQE O
SR O

and O
our O
model O
for O
the O
BSD100 B-DAT
dataset O
[21 O

value. O
For O
instance, O
for O
the O
BSD100 B-DAT
dataset, O
the O
PI O
values O
of O

PI O
values O
measured O
for O
the O
BSD100 B-DAT
dataset. O
It O
confirms O
that O
our O

input O
images O
are O
from O
the O
BSD100 B-DAT
dataset O
[21 O

for O
10 O
images O
of O
the O
BSD100 B-DAT
dataset O
[21 O

selected O
ten O
images O
in O
the O
BSD100 B-DAT
dataset. O
We O
employ O
15 O
participants O

ground-truth O
images O
are O
from O
the O
BSD100 B-DAT
dataset O
[21 O

Set5 O
[2], O
Set14 O
[40], O
and O
BSD100 B-DAT
[21] O
datasets O

BSD100 B-DAT
PSNR O
(dB) O
SSIM O
NIQE O
SR O

Set5 O
[2], O
Set14 O
[40], O
and O
BSD100 B-DAT
[21] O
datasets O

BSD100 B-DAT
PSNR O
(dB) O
SSIM O
NIQE O
SR O

Set5 O
[2], O
Set14 O
[40], O
and O
BSD100 B-DAT
[21] O
datasets O

BSD100 B-DAT
PSNR O
(dB) O
SSIM O
NIQE O
SR O

Set5 O
[2], O
Set14 O
[40], O
and O
BSD100 B-DAT
[21] O
datasets O

BSD100 B-DAT
PSNR O
(dB) O
SSIM O
NIQE O
SR O

ground-truth O
images O
are O
from O
the O
BSD100 B-DAT
dataset O
[21 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
per, O
we O
propose O
a O
novel O

- B-DAT

- B-DAT
tional O
quantitative O
performance. O
The O
proposed O

- B-DAT

- B-DAT
work O
and O
two O
quantitative O
score O

- B-DAT

- B-DAT
age O
quality O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
troduced O
convolutional O
layers O
and O
showed O

- B-DAT

- B-DAT

- B-DAT
truth O
(b) O
Upscaled O
by O
bicubic O

- B-DAT
ation O
(d) O
Upscaled O
with O
perceptual O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ror O
[39]. O
They O
mainly O
aim O

- B-DAT
tained O
images, O
which O
can O
be O

- B-DAT

- B-DAT

- B-DAT
tural O
similarity O
(SSIM) O
[37]. O
Fig O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uralness O
of O
the O
output O
images O

- B-DAT
posed O
a O
super-resolution O
model O
named O

- B-DAT
tures O
of O
VGG19 O
[31] O
when O

- B-DAT

- B-DAT
though O
these O
approaches O
improve O
naturalness O

- B-DAT
proved O
results. O
In O
addition, O
it O

- B-DAT

- B-DAT
tor O
relies O
on O
just O
finding O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
quality. O
For O
example, O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
erly O
regularize O
the O
upscaling O
modules O

- B-DAT

- B-DAT
pass O
perceptual O
super-resolution O
with O
enhanced O

- B-DAT

- B-DAT
ing O
the O
aforementioned O
issues O
via O

- B-DAT

- B-DAT

- B-DAT
scaled O
images O
produced O
by O
passing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uralness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
challenge O
[34]: O
the O
enhanced O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ization O
and O
blending O
outputs O
generated O

- B-DAT
gested O
a O
multi-scale O
super-resolution O
method O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
model O
based O
on O
residual O

- B-DAT

- B-DAT
nism O
into O
the O
super-resolution O
task O

- B-DAT

- B-DAT
age O
classifiers. O
In O
the O
former O

- B-DAT
guish O
the O
ground-truth O
images O
from O

- B-DAT

- B-DAT
scaled O
images O
properly. O
When O
an O

- B-DAT

- B-DAT
termediate O
layers O
of O
the O
classifier O

- B-DAT

- B-DAT
lating O
losses O
of O
their O
super-resolution O

- B-DAT

- B-DAT

- B-DAT
nents O
(Fig. O
2): O
a O
multi-scale O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
3.1) O
generates O
three O
upscaled O

- B-DAT

- B-DAT

- B-DAT
tion O
3.3). O
The O
two O
qualitative O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
traction O
part O
extracts O
low-level O
features O

- B-DAT

- B-DAT

- B-DAT
ing O
paths O
have O
one, O
two O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
entiate O
them O
from O
the O
ground-truth O

- B-DAT
inator O
network O
consists O
of O
several O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
connected O
layer O
with O
the O
softmax O

- B-DAT
ploy O
the O
AVA O
dataset O
[26 O

- B-DAT
ferent O
objectives: O
EUSR O
is O
for O

- B-DAT
ter O
perceptual O
quality. O
Our O
proposed O

- B-DAT
titative O
and O
perceptual O
quality, O
with O

- B-DAT

- B-DAT

- B-DAT
titative O
and O
perceptual O
quality. O
While O

- B-DAT

- B-DAT
inforces O
it O
to O
focus O
on O

- B-DAT
oughly O
investigate O
this O
in O
Section O

- B-DAT

- B-DAT
ally O
improved O
images, O
since O
they O

- B-DAT

- B-DAT
itative O
score O
predictors, O
and O
training O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stead, O
we O
set O
the O
input O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tor O
network O
using O
the O
two O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
jective O
of O
the O
super-resolution O
task O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
diate O
output O
is O
1,280 O
[30 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
step, O
two O
input O
image O

- B-DAT
tive O
models O
are O
six O
and O

- B-DAT

- B-DAT

- B-DAT
mance O
of O
our O
method O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
paring O
the O
outputs O
obtained O
from O

- B-DAT
formance O
of O
our O
method O
trained O

- B-DAT

- B-DAT
tigating O
the O
roles O
of O
loss O

- B-DAT

- B-DAT
cluding O
PSNR, O
SSIM O
[37], O
NIQE O

- B-DAT

- B-DAT

- B-DAT
ity O
metrics O
are O
calculated O
on O

- B-DAT
isting O
studies O
[17,14,19]. O
In O
addition O

- B-DAT

- B-DAT
polates O
pixel O
values O
based O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
MSE O
model O
is O
trained O
with O

- B-DAT

- B-DAT

- B-DAT

-128 B-DAT
layer O
of O
VGG19. O
Their O
results O

- B-DAT
thors’ O
supplementary O
material2 O

- B-DAT
Net, O
but O
does O
not O
employ O

- B-DAT

- B-DAT
ensemble” O
strategy, O
which O
obtains O
eight O

- B-DAT

- B-DAT
scale O
super-resolution O
and O
consists O
of O

- B-DAT
plained O
in O
Section O
3.1. O
We O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-128 B-DAT
layer O
of O
VGG19), O
and O
SRGAN-VGG54 O

- B-DAT

-512 B-DAT
layer O
of O
VGG19). O
The O
compared O

- B-DAT

- B-DAT

- B-DAT
based O
loss O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tween O
the O
VGG19 O
features O
for O

- B-DAT

- B-DAT
ber O
of O
model O
parameters, O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
though O
all O
the O
models O
except O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uated O
on O
the O
three O
datasets O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
scaled O
images O
have O
poor O
perceptual O

- B-DAT
tual O
quality O
(i.e., O
SRGAN O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pares O
the O
baselines O
and O
our O

- B-DAT

- B-DAT
tures O
are O
expected. O
First, O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tative O
quality O
than O
all O
the O

- B-DAT
sidering O
both O
the O
quantitative O
and O

- B-DAT
ploying O
only O
the O
reconstruction O
loss O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

P O
- B-DAT

- B-DAT

- B-DAT
formance O
of O
the O
super-resolution O
methods O

- B-DAT

- B-DAT
jective O
tests O
in O
the O
recommendation O

- B-DAT

-13 B-DAT
[36]. O
As O
for O
the O
evaluation O

- B-DAT

- B-DAT

- B-DAT
VGG22 O
get O
the O
lowest O
opinion O

- B-DAT

- B-DAT
tive O
and O
perceptual O
quality O
in O

- B-DAT

- B-DAT

- B-DAT
ing O
paths O
of O
the O
4PP-EUSR O

- B-DAT
pending O
on O
the O
upscaling O
paths O

- B-DAT

- B-DAT
pass O
upscaling O
using O
the O
×2 O

- B-DAT

- B-DAT
son O
is O
due O
to O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put O
is O
not O
necessarily O
avoided O

- B-DAT
resolution O
and O
thus O
the O
model O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
3.2. O
To O
investigate O
its O

- B-DAT

- B-DAT
pass O
upscaling O
is O
beneficial O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions. O
The O
input O
and O
ground-truth O

- B-DAT

- B-DAT
ity O
(i.e., O
larger O
PSNR O
values O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ful O
to O
construct O
dispersed O
high-frequency O

- B-DAT
cally, O
we O
alter O
the O
weight O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rea, O
under O
the O
“ICT O
Consilience O

-2018 B-DAT

-2017 B-DAT

-0 B-DAT

-01015 B-DAT

-16 B-DAT

-0004, B-DAT
Development O
of O
Intelligent O

- B-DAT

- B-DAT
derstanding O

- B-DAT
mawat, O
S., O
Irving, O
G., O
Isard O

- B-DAT

- B-DAT
chine O
learning. O
In: O
Proceedings O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
British O
Machine O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
performance O
on O
ImageNet O
classification O

- B-DAT
national O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks. O
In: O
Proceedings O

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT

- B-DAT
space O
projection O
and O
neighbor O
embedding O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puter O
Vision. O
pp. O
416–423 O
(2001 O

- B-DAT

- B-DAT

- B-DAT
mation O
with O
non-aligned O
data. O
arXiv:1803.02077 O

- B-DAT

- B-DAT

- B-DAT
thetic O
visual O
analysis. O
In: O
Proceedings O

- B-DAT
nition O
challenge. O
International O
Journal O
of O

- B-DAT

- B-DAT

- B-DAT
ings O
of O
the O
IEEE O
International O

- B-DAT
verted O
residuals O
and O
linear O
bottlenecks O

- B-DAT

- B-DAT
tion O
architecture O
for O
computer O
vision O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

-13 B-DAT

- B-DAT
ing O
13(4), O
600–612 O
(2004 O

- B-DAT

- B-DAT

- B-DAT
computing O
74(17), O
3193–3203 O
(2011 O

- B-DAT

- B-DAT

- B-DAT
representations. O
In: O
Proceedings O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

subset O
of O
10 O
images O
from O
BSD100 B-DAT

BSD100 B-DAT

tests O
conducted O
on O
Set5, O
Set14, O
BSD100 B-DAT

on O
performance O
(PSNR O
[dB] O
on O
BSD100 B-DAT

Set5, O
Set14 O
and O
BSD100. O
On O
BSD100 B-DAT

and O
especially O
on O
the O
large O
BSD100 B-DAT

Set5 O
Set14 O
BSD100 B-DAT
Figure I-DAT
9: O
Color-coded O
distribution O
of O

Set5 O
Set14 O
BSD100 B-DAT
Figure I-DAT
10: O
Average O
rank O
on O

Set5, O
Set14, O
BSD100 B-DAT

for O
five O
random O
samples O
of O
BSD100 B-DAT

when O
we O
super-resolve O
at O
large O
upscaling B-DAT
factors? O
The O
behavior O
of O
optimization-based O

photo-realistic O
natural O
images O
for O
4× O
upscaling B-DAT
factors. O
To O
achieve O
this, O
we O

guishable O
from O
original O
(right). O
[4× O
upscaling B-DAT

is O
particularly O
pronounced O
for O
high O
upscaling B-DAT
factors, O
for O
which O
texture O
detail O

are O
shown O
in O
brackets. O
[4× O
upscaling B-DAT

super- O
resolved O
with O
a O
4× O
upscaling B-DAT
factor O
is O
shown O
in O
Figure O

the O
network O
to O
learn O
the O
upscaling B-DAT
filters O
directly O
can O
further O
increase O

was O
also O
shown O
that O
learning O
upscaling B-DAT
filters O
is O
beneficial O
in O
terms O

super-resolves O
face O
images O
with O
large O
upscaling B-DAT
factors O
(8×). O
GANs O
were O
also O

for O
image O
SR O
with O
high O
upscaling B-DAT
factors O
(4×) O
as O
measured O
by O

photo-realistic O
SR O
images O
with O
high O
upscaling B-DAT
factors O
(4 O

losses O
in O
that O
category∗. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

HR O
image O
(right: O
i,j). O
[4× O
upscaling B-DAT

SSIM, O
MOS) O
in O
bold. O
[4× O
upscaling B-DAT

that O
SRGAN O
reconstructions O
for O
large O
upscaling B-DAT
factors O
(4×) O
are, O
by O
a O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
IEEE O

and O
SRGAN O
with O
a O
4× O
upscaling B-DAT
factor O
for O
Set5 O
(Section O
A.4 O

low-/high-resolution O
images O
and O
reconstructions O
(4× O
upscaling) B-DAT
obtained O
with O
different O
methods O
(bicubic O

image O
with O
resolution O
64×64 O
with O
upscaling B-DAT
factor O
4×. O
The O
measurements O
are O

for O
another O
100k O
iterations. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

all O
available O
individual O
ratings. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

Set5 O
[3], O
Set14 O
[69] O
and O
BSD100, B-DAT
the O
testing O
set O
of O
BSD300 O

image O
on O
Set5, O
Set14 O
and O
BSD100 B-DAT

SRResNet-MSE, O
SRResNet-VGG22∗ O
(∗not O
rated O
on O
BSD100), B-DAT
SRGAN-MSE∗, O
SRGAN-VGG22∗, O
SRGAN- O
VGG54 O
and O

subset O
of O
10 O
images O
from O
BSD100 B-DAT
by O
adding O
a O
method’s O
images O

distribution O
of O
MOS O
scores O
on O
BSD100 B-DAT

and O
all O
reference O
methods O
on O
BSD100 B-DAT

2) O
are O
highly O
significant O
on O
BSD100, B-DAT
except O
SRCNN O
vs. O
SelfExSR. O
The O

BSD100 B-DAT
PSNR O
25.02 O
25.94 O
26.68 O
26.83 O

tests O
conducted O
on O
Set5, O
Set14, O
BSD100 B-DAT
are O
summarized O
in O
Section O
A.3 O

five O
randomly O
selected O
images O
from O
BSD100 B-DAT
(Section O
A.6 O

on O
performance O
(PSNR O
[dB] O
on O
BSD100 B-DAT
for O
4× O
SR) O
and O
inference O

depth. O
PSNR O
(left) O
calculated O
on O
BSD100 B-DAT

images O
from O
Set5, O
Set14 O
and O
BSD100 B-DAT

. O
On O
BSD100 B-DAT
nine O
versions O
of O
each O
image O

and O
especially O
on O
the O
large O
BSD100 B-DAT
data O
set O
confirm O
that O
SRGAN O

Set5 O
Set14 O
BSD100 B-DAT

MOS O
scores O
on O
Set5, O
Set14, O
BSD100 B-DAT

Set5 O
Set14 O
BSD100 B-DAT

Average O
rank O
on O
Set5, O
Set14, O
BSD100 B-DAT
by O
averaging O
the O
ranks O
over O

A.6. O
BSD100 B-DAT
(five O
random O
samples) O
- O
Visual O

for O
five O
random O
samples O
of O
BSD100 B-DAT
using O
bicubic O
interpolation, O
SRResNet O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks, O
one O
central O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
(SR). O
To O
our O
knowledge O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
guishable O
from O
original O
(right). O
[4 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
inal O
image O
means O
that O
the O

- B-DAT
realistic O
as O
defined O
by O
Ferwerda O

- B-DAT

- B-DAT

- B-DAT
ing O
high-level O
feature O
maps O
of O

- B-DAT

- B-DAT
resolved O
with O
a O
4× O
upscaling O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
image O
informa- O
tion O

- B-DAT

- B-DAT
proaches O
to O
the O
SR O
problem O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
[27], O
trees O
[46] O
or O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-the-art O
SR O
performance. O
Subsequently, O
it O

- B-DAT

- B-DAT

- B-DAT
ciently O
train O
these O
deeper O
network O

- B-DAT
normalization O
[32] O
is O
often O
used O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
results. O
Another O
powerful O
design O

- B-DAT

- B-DAT
connections O
relieve O
the O
network O
architecture O

- B-DAT
tentially O
non-trivial O
to O
represent O
with O

- B-DAT

- B-DAT

- B-DAT
ing O
pixel-wise O
averages O
of O
plausible O

- B-DAT

- B-DAT
ity O
[42, O
33, O
13, O
5 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
perceptually O
more O
convincing O
solutions O

- B-DAT
ure O
2. O
We O
illustrate O
the O

- B-DAT
ure O
3 O
where O
multiple O
potential O

- B-DAT
tion. O
Yu O
and O
Porikli O
[66 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions. O
Similar O
to O
this O
work O

- B-DAT
trained O
VGG O
network O
instead O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity. O
The O
GAN O
procedure O
encourages O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mark O
datasets O
as O
well O
as O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
counterpart O
IHR. O
The O
high-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nels O
as O
in O
the O
VGG O

- B-DAT
ical O
for O
the O
performance O
of O

- B-DAT
tent O
loss O
lSRX O
and O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
solutions O
with O
overly O
smooth O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stead O
of O
log[1−DθD O
(GθG(ILR))] O
[22 O

- B-DAT
mark O
datasets O
Set5 O
[3], O
Set14 O

- B-DAT
and O
high-resolution O
images. O
This O
corresponds O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bor, O
bicubic, O
SRCNN O
[9] O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gral O
score O
from O
1 O
(bad O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG54 O
and O
the O
original O
HR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ResNet O
and O
the O
adversarial O
networks O

- B-DAT
SRGAN- O
Set5 O
MSE O
VGG22 O
MSE O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
MSE-based O
reconstructions, O
to O
those O
competing O

- B-DAT

- B-DAT
formed O
other O
SRGAN O
and O
SRResNet O

- B-DAT

- B-DAT
GAN O
to O
NN, O
bicubic O
interpolation O

- B-DAT

- B-DAT

- B-DAT
art O
methods. O
Quantitative O
results O
are O

- B-DAT
resolved O
with O
SRResNet O
and O
SRGAN O

- B-DAT
realistic O
image O
SR. O
All O
differences O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentary O
material). O
We O
further O
found O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Net O
that O
sets O
a O
new O

- B-DAT
sure. O
We O
have O
highlighted O
some O

- B-DAT

- B-DAT
ial O
loss O
by O
training O
a O

- B-DAT

- B-DAT

- B-DAT
the-art O
reference O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Stevenson. O
Super-Resolution B-DAT
from O
Image O
Sequences O
- O
A O
Review. O
Midwest O
Symposium O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
by O
adaptive O
sparse O
domain O

- B-DAT
ization. O
IEEE O
Transactions O
on O
Image O

- B-DAT

- B-DAT
resolution. O
IEEE O
Computer O
Graphics O
and O

- B-DAT
level O
vision. O
International O
Journal O
of O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

- B-DAT
line O
at O
http://torch.ch/blog/2016/02/04/resnets. O
html. O
2016 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In O
European O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT
tion O
with O
deep O
convolutional O
neural O

- B-DAT

- B-DAT

- B-DAT
mentation O
algorithms O
and O
measuring O
ecological O

- B-DAT

- B-DAT

- B-DAT
sive O
survey. O
In O
Machine O
Vision O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
for O
fast O
example-based O
super-resolution O

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
(ACCV O

- B-DAT

- B-DAT

- B-DAT
Resolution O
via O
Deep O
and O
Shallow O

- B-DAT

- B-DAT

- B-DAT
ence O
on O
Signals, O
Systems O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
derstanding O
Neural O
Networks O
Through O
Deep O

International O
Conference O
on O
Machine O
Learning O
- B-DAT
Deep O
Learning O
Workshop O
2015, O
page O

- B-DAT

- B-DAT
resolution O
by O
retrieving O
web O
images O

- B-DAT
volutional O
networks. O
In O
European O
Conference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG22, O
SRGAN-VGG54) O
described O
in O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
off O
between O
accuracy O
and O
speed O

-100 B-DAT
thousand O
update O
iterations O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

A.4. O
Set5 O
- B-DAT
Visual O
Results O

A.5. O
Set14 O
- B-DAT
Visual O
Results O

A.6. O
BSD100 O
(five O
random O
samples) O
- B-DAT
Visual O
Results O

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

Set5 O
[2], O
Set14 O
[50] O
and O
BSD100 B-DAT
[33 O

- B-DAT

Dong2 O
Chen O
Change O
Loy1 O
1CUHK O
- B-DAT
SenseTime O
Joint O
Lab, O
The O
Chinese O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
per, O
we O
show O
that O
it O

- B-DAT
ful O
to O
semantic O
classes. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
the O
same O
loss O
function O

- B-DAT
put O
image O
of O
arbitrary O
size O

- B-DAT

- B-DAT
work O
equipped O
with O
SFT O
can O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions O
exist O
for O
any O
given O

- B-DAT

- B-DAT
ping O
functions O
from O
external O
low O

- B-DAT
and O
high-resolution O
ex- O
emplar O
pairs O

- B-DAT
ifold, O
new O
losses O
are O
proposed O

- B-DAT

- B-DAT

- B-DAT
resolution O
model O
in O
a O
feature O

- B-DAT
resolution O
images O
look O
very O
similar O

- B-DAT
tions O
the O
overall O
visual O
quality O

- B-DAT
cantly O
improved O

- B-DAT
ent O
class O
is O
non-trivial. O
The O

- B-DAT
ceptual O
and O
adversarial O
losses O
(without O

- B-DAT
ine O
closely, O
these O
details O
are O

- B-DAT
tion, O
existing O
methods O
struggle O
in O

- B-DAT
ing, O
plant), O
is O
crucial O
for O

- B-DAT
ical O
prior O
using O
the O
same O

- B-DAT
ing O
pairs O
using O
two O
different O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ously O
documented O
by O
Timofte O
et O

- B-DAT
ized O
models O
for O
each O
semantic O

- B-DAT

- B-DAT

- B-DAT
age O
super-resolution O
with O
CNN. O
This O

- B-DAT
ing O
especially O
when O
multiple O
segments O

- B-DAT

- B-DAT
ther O
incorporated O
into O
the O
reconstruction O

- B-DAT
tic O
segmentation O
maps O
as O
the O

- B-DAT
periments O
embrace O
this O
choice O
and O

- B-DAT

- B-DAT

- B-DAT
cient. O
Combining O
LR O
images O
with O

- B-DAT
puts, O
or O
concatenating O
segmentation O
maps O

- B-DAT
tial O
Feature O
Transform O
(SFT) O
that O

- B-DAT
ically, O
an O
SFT O
layer O
is O

- B-DAT
tion O
probability O
maps, O
based O
on O

- B-DAT
tially O
on O
feature O
maps O
of O

- B-DAT

- B-DAT

- B-DAT
struction O
of O
an O
HR O
image O

- B-DAT
forming O
the O
intermediate O
features O
of O

- B-DAT

- B-DAT

- B-DAT
tiveness O
of O
our O
approach, O
named O

- B-DAT

- B-DAT
vided O
in O
Sec. O
4 O

- B-DAT

- B-DAT
duced O
prior O
information O
to O
help O

- B-DAT

- B-DAT
formance. O
Dong O
et O
al. O
[10 O

- B-DAT
ies O
to O
better O
recover O
local O

- B-DAT
tion O
framework. O
Sun O
et O
al O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
[49, O
50, O
45, O
46 O

- B-DAT
stantiation O
of O
learning-based O
methods, O
Dong O

- B-DAT
pose O
SRCNN O
for O
learning O
the O

- B-DAT
ages O
in O
an O
end-to-end O
manner O

- B-DAT
mid O
structure O
[26], O
residual O
blocks O

- B-DAT
ing O
[23, O
43], O
and O
densely O

- B-DAT
scale O
guidance O
structure O
has O
also O

- B-DAT

- B-DAT

- B-DAT
age O
of O
many O
plausible O
solutions O

- B-DAT

- B-DAT
velop O
a O
similar O
approach O
and O

- B-DAT
ture O
matching O
loss, O
partly O
reducing O

- B-DAT
facts. O
We O
use O
the O
same O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
egorical O
classes O
to O
co-exist O
in O

- B-DAT
fective O
layer O
that O
enables O
an O

- B-DAT
malizing O
feature O
statistics O
[19]. O
Conditional O

- B-DAT
place O
parameters O
for O
feature-wise O
affine O

- B-DAT
ing O
[6] O
and O
visual O
reasoning O

- B-DAT

- B-DAT
mation O
(e.g., O
semantic O
segmentation O
maps O

- B-DAT
cepts O
a O
single O
linguistic O
input O

- B-DAT

- B-DAT
quire O
adaptive O
processing O
at O
different O

- B-DAT
posed O
SFT O
layer O
addresses O
this O

- B-DAT

- B-DAT

- B-DAT
ceptual O
factors O
in O
neural O
style O

- B-DAT

- B-DAT
tion O
instead O
of O
simple O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ilar O
as O
possible O
to O
the O

- B-DAT
based O
methods O
use O
feed-forward O
networks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
structed O
images. O
However, O
the O
generated O

- B-DAT
ing O
which O
region O
belongs O
to O

- B-DAT
eficial O
for O
generating O
richer O
and O

- B-DAT
mantic O
segmentation O
probability O
maps O
P O

- B-DAT
ors O
in O
SR, O
we O
reformulate O

- B-DAT
ping O
functionM O
that O
outputs O
a O

- B-DAT
rameter O
pair O
adaptively O
influences O
the O

- B-DAT
ture O
maps O
in O
an O
SR O

- B-DAT
tionM O
: O
Ψ O
7→ O
(γ,β O

- B-DAT
cific O
layer O

- B-DAT

- B-DAT
cation, O
i.e., O
Hadamard O
product. O
Since O

- B-DAT

- B-DAT

- B-DAT
ers O
in O
an O
SR O
network O

- B-DAT
timized O
end-to-end O
with O
the O
SR O

- B-DAT
ate O
conditions O
that O
can O
be O

- B-DAT

- B-DAT

- B-DAT
tation O
maps O
obtained O
from O
LR O

- B-DAT
ing O
factor O
of O
×4 O
from O

- B-DAT
tained O
even O
on O
LR O
images O

- B-DAT

- B-DAT
mentation O
model O
[32, O
31]. O
Some O

- B-DAT
responding O
segmentation O
results O
are O
depicted O

- B-DAT
tation O
community. O
During O
testing, O
classes O

- B-DAT

- B-DAT
GAN, O
i.e., O
treating O
all O
classes O

- B-DAT
egorical O
priors O
to O
an O
SR O

- B-DAT
put O
LR O
image O
as O
a O

- B-DAT
catenate O
the O
probability O
maps O
with O

- B-DAT

- B-DAT
sis O
network O
[29]1. O
This O
method O

- B-DAT

- B-DAT

- B-DAT

d O
- B-DAT

- B-DAT
ages. O
Second O
row: O
segmentation O
results O

- B-DAT

- B-DAT
mentation O

- B-DAT

- B-DAT
pose O
the O
LR O
image O
based O

- B-DAT
bining O
the O
output O
of O
each O

- B-DAT

- B-DAT
tively. O
They O
are O
jointly O
trained O

- B-DAT
plying O
affine O
transformation. O
Skip O
connection O

- B-DAT

- B-DAT
volution O
layer. O
The O
upsampling O
operation O

- B-DAT
porary O
models O
such O
as O
DRRN O

- B-DAT

- B-DAT
work O
of O
strided O
convolutions O
to O

- B-DAT
tial O
dimensions. O
The O
full O
architecture O

- B-DAT
vided O
in O
the O
supplementary O
material O

- B-DAT
ture O
maps, O
we O
use O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
courage O
the O
generator O
to O
favor O

- B-DAT
door O
scenes O
since O
their O
textures O

- B-DAT

- B-DAT
ground’ O
category O
is O
used O
to O

- B-DAT
tialized O
the O
SR O
network O
by O

- B-DAT

- B-DAT
ceptual O
loss O
and O
GAN O
loss O

- B-DAT
nel. O
The O
mini-batch O
size O
was O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceptual O
quality. O
Our O
proposed O
SFT-GAN O

- B-DAT
ticular, O
we O
collected O
a O
new O

- B-DAT
ages O
from O
search O
engines O
using O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els O
including O
PSNR-oriented O
methods, O
such O

- B-DAT

- B-DAT
hanceNet O
[38]. O
More O
results O
are O

- B-DAT
mentary O
material. O
For O
SRGAN, O
we O

- B-DAT

- B-DAT

- B-DAT
ing O
sharp O
edges, O
PSNR-oriented O
methods O

- B-DAT

- B-DAT
ate O
monotonous O
and O
unnatural O
textures O

-1 B-DAT
Rank-2 O
Rank-3 O
Rank-4 O

- B-DAT

- B-DAT
forms O
PSNR-oriented O
methods O
by O
a O

- B-DAT
ing O
images. O
To O
better O
compare O

- B-DAT
oriented O
baselines O
and O
GAN-based O
approaches O

- B-DAT

- B-DAT
quested O
to O
rank O
4 O
versions O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
domized O
fashion. O
In O
the O
second O

- B-DAT

- B-DAT

- B-DAT
cilitate O
the O
comparison). O
Each O
pair O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ages O
comparable O
to O
HR O
causing O

- B-DAT
lation O
parameters. O
Our O
method O
modulates O

- B-DAT
vestigate O
the O
relationship O
between O
the O

- B-DAT
ture O
modulation O
parameters, O
as O
depicted O

- B-DAT
tionship O
with O
probability O
maps O
P O

- B-DAT
age O
where O
building O
and O
grass O

- B-DAT

- B-DAT

- B-DAT
formation. O
From O
the O
heat O
map O

- B-DAT
ing O
and O
grass O
textures O
simultaneously O

- B-DAT
biguity, O
the O
probability O
maps O
are O

- B-DAT
formation. O
In O
Fig. O
9, O
the O

- B-DAT
tures O
generated O
by O
SFT-GAN O
become O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentation O
results O
are O
not O
available O

- B-DAT

- B-DAT
mentation O
probability O
maps, O
our O
model O

- B-DAT
GAN O
and O
produces O
comparative O
results O

- B-DAT
itatively O
compare O
with O
several O
alternatives O

- B-DAT
mentation O
probability O
maps O
with O
the O

- B-DAT
ditional O
bias O
at O
the O
input O

- B-DAT
rately O
using O
a O
specific O
model O

- B-DAT
GAN O
yields O
outputs O
that O
are O

- B-DAT
essary O
condition O
for O
class-specific O
texture O

- B-DAT
positional O
mapping O
produces O
good O
results O

- B-DAT
rameter O
efficient O
(×2.5 O
parameters O
as O

- B-DAT
putationally O
inefficient O
as O
we O
need O

- B-DAT
tions O
where O
multiple O
categorical O
classes O

- B-DAT

- B-DAT

Comparison O
with O
other O
conditioning O
methods O
- B-DAT
input O
concatenation, O
compositional O
mapping O
and O

- B-DAT
tic O
to O
spatial O
information. O
For O

- B-DAT
ture O
and O
thus O
noisy O
bricks O

- B-DAT

- B-DAT

- B-DAT
erating O
distinct O
and O
rich O
textures O

- B-DAT
gions O
in O
a O
super-resolved O
image O

- B-DAT

- B-DAT
ally O
pleasing O
textures, O
outperforming O
previous O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sider O
priors O
of O
finer O
categories O

- B-DAT
ward O
challenging O
requirements O
for O
segmentation O

- B-DAT
comings. O
Furthermore, O
segmentation O
and O
SR O

- B-DAT

- B-DAT
Morel. O
Low-complexity O
single-image O
super-resolution O
based O

- B-DAT

- B-DAT

- B-DAT
guage. O
arXiv O
preprint O
arXiv:1707.00683, O
2017 O

- B-DAT

- B-DAT
resolution O
using O
deep O
convolutional O
networks O

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
time O
with O
adaptive O
instance O
normalization O

- B-DAT
resolution O
by O
deep O
multi-scale O
guidance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mization. O
arXiv O
preprint O
arXiv:1412.6980, O
2014 O

- B-DAT
resolution. O
In O
CVPR, O
2017. O
1 O

- B-DAT

- B-DAT

- B-DAT
ative O
adversarial O
network. O
In O
CVPR O

- B-DAT

- B-DAT

- B-DAT
manan, O
P. O
Dollár, O
and O
C O

- B-DAT
mon O
objects O
in O
context. O
In O

- B-DAT
ing O
markov O
random O
field O
for O

- B-DAT
cal O
statistics. O
In O
ICCV, O
2001 O

- B-DAT
ors. O
arXiv O
preprint O
arXiv:1707.03017, O
2017 O

- B-DAT
ditioning O
layer. O
arXiv O
preprint O
arXiv:1709.07871 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cination O
for O
image O
super-resolution. O
In O

- B-DAT

- B-DAT
borhood O
regression O
for O
fast O
example-based O

- B-DAT

- B-DAT

- B-DAT
resolution: O
When O
and O
where O
is O

- B-DAT

- B-DAT
resolution O
as O
sparse O
representation O
of O

- B-DAT

- B-DAT

- B-DAT
ralba. O
Scene O
parsing O
through O
ADE20K O

subset O
of O
10 O
images O
from O
BSD100 B-DAT

BSD100 B-DAT

tests O
conducted O
on O
Set5, O
Set14, O
BSD100 B-DAT

on O
performance O
(PSNR O
[dB] O
on O
BSD100 B-DAT

Set5, O
Set14 O
and O
BSD100. O
On O
BSD100 B-DAT

and O
especially O
on O
the O
large O
BSD100 B-DAT

Set5 O
Set14 O
BSD100 B-DAT
Figure I-DAT
9: O
Color-coded O
distribution O
of O

Set5 O
Set14 O
BSD100 B-DAT
Figure I-DAT
10: O
Average O
rank O
on O

Set5, O
Set14, O
BSD100 B-DAT

for O
five O
random O
samples O
of O
BSD100 B-DAT

when O
we O
super-resolve O
at O
large O
upscaling B-DAT
factors? O
The O
behavior O
of O
optimization-based O

photo-realistic O
natural O
images O
for O
4× O
upscaling B-DAT
factors. O
To O
achieve O
this, O
we O

guishable O
from O
original O
(right). O
[4× O
upscaling B-DAT

is O
particularly O
pronounced O
for O
high O
upscaling B-DAT
factors, O
for O
which O
texture O
detail O

are O
shown O
in O
brackets. O
[4× O
upscaling B-DAT

super- O
resolved O
with O
a O
4× O
upscaling B-DAT
factor O
is O
shown O
in O
Figure O

the O
network O
to O
learn O
the O
upscaling B-DAT
filters O
directly O
can O
further O
increase O

was O
also O
shown O
that O
learning O
upscaling B-DAT
filters O
is O
beneficial O
in O
terms O

super-resolves O
face O
images O
with O
large O
upscaling B-DAT
factors O
(8×). O
GANs O
were O
also O

for O
image O
SR O
with O
high O
upscaling B-DAT
factors O
(4×) O
as O
measured O
by O

photo-realistic O
SR O
images O
with O
high O
upscaling B-DAT
factors O
(4 O

losses O
in O
that O
category∗. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

HR O
image O
(right: O
i,j). O
[4× O
upscaling B-DAT

SSIM, O
MOS) O
in O
bold. O
[4× O
upscaling B-DAT

that O
SRGAN O
reconstructions O
for O
large O
upscaling B-DAT
factors O
(4×) O
are, O
by O
a O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
IEEE O

and O
SRGAN O
with O
a O
4× O
upscaling B-DAT
factor O
for O
Set5 O
(Section O
A.4 O

low-/high-resolution O
images O
and O
reconstructions O
(4× O
upscaling) B-DAT
obtained O
with O
different O
methods O
(bicubic O

image O
with O
resolution O
64×64 O
with O
upscaling B-DAT
factor O
4×. O
The O
measurements O
are O

for O
another O
100k O
iterations. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

all O
available O
individual O
ratings. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

Set5 O
[3], O
Set14 O
[69] O
and O
BSD100, B-DAT
the O
testing O
set O
of O
BSD300 O

image O
on O
Set5, O
Set14 O
and O
BSD100 B-DAT

SRResNet-MSE, O
SRResNet-VGG22∗ O
(∗not O
rated O
on O
BSD100), B-DAT
SRGAN-MSE∗, O
SRGAN-VGG22∗, O
SRGAN- O
VGG54 O
and O

subset O
of O
10 O
images O
from O
BSD100 B-DAT
by O
adding O
a O
method’s O
images O

distribution O
of O
MOS O
scores O
on O
BSD100 B-DAT

and O
all O
reference O
methods O
on O
BSD100 B-DAT

2) O
are O
highly O
significant O
on O
BSD100, B-DAT
except O
SRCNN O
vs. O
SelfExSR. O
The O

BSD100 B-DAT
PSNR O
25.02 O
25.94 O
26.68 O
26.83 O

tests O
conducted O
on O
Set5, O
Set14, O
BSD100 B-DAT
are O
summarized O
in O
Section O
A.3 O

five O
randomly O
selected O
images O
from O
BSD100 B-DAT
(Section O
A.6 O

on O
performance O
(PSNR O
[dB] O
on O
BSD100 B-DAT
for O
4× O
SR) O
and O
inference O

depth. O
PSNR O
(left) O
calculated O
on O
BSD100 B-DAT

images O
from O
Set5, O
Set14 O
and O
BSD100 B-DAT

. O
On O
BSD100 B-DAT
nine O
versions O
of O
each O
image O

and O
especially O
on O
the O
large O
BSD100 B-DAT
data O
set O
confirm O
that O
SRGAN O

Set5 O
Set14 O
BSD100 B-DAT

MOS O
scores O
on O
Set5, O
Set14, O
BSD100 B-DAT

Set5 O
Set14 O
BSD100 B-DAT

Average O
rank O
on O
Set5, O
Set14, O
BSD100 B-DAT
by O
averaging O
the O
ranks O
over O

A.6. O
BSD100 B-DAT
(five O
random O
samples) O
- O
Visual O

for O
five O
random O
samples O
of O
BSD100 B-DAT
using O
bicubic O
interpolation, O
SRResNet O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks, O
one O
central O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
(SR). O
To O
our O
knowledge O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
guishable O
from O
original O
(right). O
[4 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
inal O
image O
means O
that O
the O

- B-DAT
realistic O
as O
defined O
by O
Ferwerda O

- B-DAT

- B-DAT

- B-DAT
ing O
high-level O
feature O
maps O
of O

- B-DAT

- B-DAT
resolved O
with O
a O
4× O
upscaling O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
image O
informa- O
tion O

- B-DAT

- B-DAT
proaches O
to O
the O
SR O
problem O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
[27], O
trees O
[46] O
or O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-the-art O
SR O
performance. O
Subsequently, O
it O

- B-DAT

- B-DAT

- B-DAT
ciently O
train O
these O
deeper O
network O

- B-DAT
normalization O
[32] O
is O
often O
used O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
results. O
Another O
powerful O
design O

- B-DAT

- B-DAT
connections O
relieve O
the O
network O
architecture O

- B-DAT
tentially O
non-trivial O
to O
represent O
with O

- B-DAT

- B-DAT

- B-DAT
ing O
pixel-wise O
averages O
of O
plausible O

- B-DAT

- B-DAT
ity O
[42, O
33, O
13, O
5 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
perceptually O
more O
convincing O
solutions O

- B-DAT
ure O
2. O
We O
illustrate O
the O

- B-DAT
ure O
3 O
where O
multiple O
potential O

- B-DAT
tion. O
Yu O
and O
Porikli O
[66 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions. O
Similar O
to O
this O
work O

- B-DAT
trained O
VGG O
network O
instead O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity. O
The O
GAN O
procedure O
encourages O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mark O
datasets O
as O
well O
as O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
counterpart O
IHR. O
The O
high-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nels O
as O
in O
the O
VGG O

- B-DAT
ical O
for O
the O
performance O
of O

- B-DAT
tent O
loss O
lSRX O
and O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
solutions O
with O
overly O
smooth O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stead O
of O
log[1−DθD O
(GθG(ILR))] O
[22 O

- B-DAT
mark O
datasets O
Set5 O
[3], O
Set14 O

- B-DAT
and O
high-resolution O
images. O
This O
corresponds O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bor, O
bicubic, O
SRCNN O
[9] O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gral O
score O
from O
1 O
(bad O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG54 O
and O
the O
original O
HR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ResNet O
and O
the O
adversarial O
networks O

- B-DAT
SRGAN- O
Set5 O
MSE O
VGG22 O
MSE O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
MSE-based O
reconstructions, O
to O
those O
competing O

- B-DAT

- B-DAT
formed O
other O
SRGAN O
and O
SRResNet O

- B-DAT

- B-DAT
GAN O
to O
NN, O
bicubic O
interpolation O

- B-DAT

- B-DAT

- B-DAT
art O
methods. O
Quantitative O
results O
are O

- B-DAT
resolved O
with O
SRResNet O
and O
SRGAN O

- B-DAT
realistic O
image O
SR. O
All O
differences O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentary O
material). O
We O
further O
found O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Net O
that O
sets O
a O
new O

- B-DAT
sure. O
We O
have O
highlighted O
some O

- B-DAT

- B-DAT
ial O
loss O
by O
training O
a O

- B-DAT

- B-DAT

- B-DAT
the-art O
reference O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Stevenson. O
Super-Resolution B-DAT
from O
Image O
Sequences O
- O
A O
Review. O
Midwest O
Symposium O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
by O
adaptive O
sparse O
domain O

- B-DAT
ization. O
IEEE O
Transactions O
on O
Image O

- B-DAT

- B-DAT
resolution. O
IEEE O
Computer O
Graphics O
and O

- B-DAT
level O
vision. O
International O
Journal O
of O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

- B-DAT
line O
at O
http://torch.ch/blog/2016/02/04/resnets. O
html. O
2016 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In O
European O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT
tion O
with O
deep O
convolutional O
neural O

- B-DAT

- B-DAT

- B-DAT
mentation O
algorithms O
and O
measuring O
ecological O

- B-DAT

- B-DAT

- B-DAT
sive O
survey. O
In O
Machine O
Vision O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
for O
fast O
example-based O
super-resolution O

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
(ACCV O

- B-DAT

- B-DAT

- B-DAT
Resolution O
via O
Deep O
and O
Shallow O

- B-DAT

- B-DAT

- B-DAT
ence O
on O
Signals, O
Systems O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
derstanding O
Neural O
Networks O
Through O
Deep O

International O
Conference O
on O
Machine O
Learning O
- B-DAT
Deep O
Learning O
Workshop O
2015, O
page O

- B-DAT

- B-DAT
resolution O
by O
retrieving O
web O
images O

- B-DAT
volutional O
networks. O
In O
European O
Conference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG22, O
SRGAN-VGG54) O
described O
in O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
off O
between O
accuracy O
and O
speed O

-100 B-DAT
thousand O
update O
iterations O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

A.4. O
Set5 O
- B-DAT
Visual O
Results O

A.5. O
Set14 O
- B-DAT
Visual O
Results O

A.6. O
BSD100 O
(five O
random O
samples) O
- B-DAT
Visual O
Results O

subset O
of O
10 O
images O
from O
BSD100 B-DAT

BSD100 B-DAT

tests O
conducted O
on O
Set5, O
Set14, O
BSD100 B-DAT

on O
performance O
(PSNR O
[dB] O
on O
BSD100 B-DAT

Set5, O
Set14 O
and O
BSD100. O
On O
BSD100 B-DAT

and O
especially O
on O
the O
large O
BSD100 B-DAT

Set5 O
Set14 O
BSD100 B-DAT
Figure I-DAT
9: O
Color-coded O
distribution O
of O

Set5 O
Set14 O
BSD100 B-DAT
Figure I-DAT
10: O
Average O
rank O
on O

Set5, O
Set14, O
BSD100 B-DAT

for O
five O
random O
samples O
of O
BSD100 B-DAT

when O
we O
super-resolve O
at O
large O
upscaling B-DAT
factors? O
The O
behavior O
of O
optimization-based O

photo-realistic O
natural O
images O
for O
4× O
upscaling B-DAT
factors. O
To O
achieve O
this, O
we O

guishable O
from O
original O
(right). O
[4× O
upscaling B-DAT

is O
particularly O
pronounced O
for O
high O
upscaling B-DAT
factors, O
for O
which O
texture O
detail O

are O
shown O
in O
brackets. O
[4× O
upscaling B-DAT

super- O
resolved O
with O
a O
4× O
upscaling B-DAT
factor O
is O
shown O
in O
Figure O

the O
network O
to O
learn O
the O
upscaling B-DAT
filters O
directly O
can O
further O
increase O

was O
also O
shown O
that O
learning O
upscaling B-DAT
filters O
is O
beneficial O
in O
terms O

super-resolves O
face O
images O
with O
large O
upscaling B-DAT
factors O
(8×). O
GANs O
were O
also O

for O
image O
SR O
with O
high O
upscaling B-DAT
factors O
(4×) O
as O
measured O
by O

photo-realistic O
SR O
images O
with O
high O
upscaling B-DAT
factors O
(4 O

losses O
in O
that O
category∗. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

HR O
image O
(right: O
i,j). O
[4× O
upscaling B-DAT

SSIM, O
MOS) O
in O
bold. O
[4× O
upscaling B-DAT

that O
SRGAN O
reconstructions O
for O
large O
upscaling B-DAT
factors O
(4×) O
are, O
by O
a O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
IEEE O

and O
SRGAN O
with O
a O
4× O
upscaling B-DAT
factor O
for O
Set5 O
(Section O
A.4 O

low-/high-resolution O
images O
and O
reconstructions O
(4× O
upscaling) B-DAT
obtained O
with O
different O
methods O
(bicubic O

image O
with O
resolution O
64×64 O
with O
upscaling B-DAT
factor O
4×. O
The O
measurements O
are O

for O
another O
100k O
iterations. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

all O
available O
individual O
ratings. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

Set5 O
[3], O
Set14 O
[69] O
and O
BSD100, B-DAT
the O
testing O
set O
of O
BSD300 O

image O
on O
Set5, O
Set14 O
and O
BSD100 B-DAT

SRResNet-MSE, O
SRResNet-VGG22∗ O
(∗not O
rated O
on O
BSD100), B-DAT
SRGAN-MSE∗, O
SRGAN-VGG22∗, O
SRGAN- O
VGG54 O
and O

subset O
of O
10 O
images O
from O
BSD100 B-DAT
by O
adding O
a O
method’s O
images O

distribution O
of O
MOS O
scores O
on O
BSD100 B-DAT

and O
all O
reference O
methods O
on O
BSD100 B-DAT

2) O
are O
highly O
significant O
on O
BSD100, B-DAT
except O
SRCNN O
vs. O
SelfExSR. O
The O

BSD100 B-DAT
PSNR O
25.02 O
25.94 O
26.68 O
26.83 O

tests O
conducted O
on O
Set5, O
Set14, O
BSD100 B-DAT
are O
summarized O
in O
Section O
A.3 O

five O
randomly O
selected O
images O
from O
BSD100 B-DAT
(Section O
A.6 O

on O
performance O
(PSNR O
[dB] O
on O
BSD100 B-DAT
for O
4× O
SR) O
and O
inference O

depth. O
PSNR O
(left) O
calculated O
on O
BSD100 B-DAT

images O
from O
Set5, O
Set14 O
and O
BSD100 B-DAT

. O
On O
BSD100 B-DAT
nine O
versions O
of O
each O
image O

and O
especially O
on O
the O
large O
BSD100 B-DAT
data O
set O
confirm O
that O
SRGAN O

Set5 O
Set14 O
BSD100 B-DAT

MOS O
scores O
on O
Set5, O
Set14, O
BSD100 B-DAT

Set5 O
Set14 O
BSD100 B-DAT

Average O
rank O
on O
Set5, O
Set14, O
BSD100 B-DAT
by O
averaging O
the O
ranks O
over O

A.6. O
BSD100 B-DAT
(five O
random O
samples) O
- O
Visual O

for O
five O
random O
samples O
of O
BSD100 B-DAT
using O
bicubic O
interpolation, O
SRResNet O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks, O
one O
central O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
(SR). O
To O
our O
knowledge O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
guishable O
from O
original O
(right). O
[4 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
inal O
image O
means O
that O
the O

- B-DAT
realistic O
as O
defined O
by O
Ferwerda O

- B-DAT

- B-DAT

- B-DAT
ing O
high-level O
feature O
maps O
of O

- B-DAT

- B-DAT
resolved O
with O
a O
4× O
upscaling O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
image O
informa- O
tion O

- B-DAT

- B-DAT
proaches O
to O
the O
SR O
problem O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
[27], O
trees O
[46] O
or O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-the-art O
SR O
performance. O
Subsequently, O
it O

- B-DAT

- B-DAT

- B-DAT
ciently O
train O
these O
deeper O
network O

- B-DAT
normalization O
[32] O
is O
often O
used O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
results. O
Another O
powerful O
design O

- B-DAT

- B-DAT
connections O
relieve O
the O
network O
architecture O

- B-DAT
tentially O
non-trivial O
to O
represent O
with O

- B-DAT

- B-DAT

- B-DAT
ing O
pixel-wise O
averages O
of O
plausible O

- B-DAT

- B-DAT
ity O
[42, O
33, O
13, O
5 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
perceptually O
more O
convincing O
solutions O

- B-DAT
ure O
2. O
We O
illustrate O
the O

- B-DAT
ure O
3 O
where O
multiple O
potential O

- B-DAT
tion. O
Yu O
and O
Porikli O
[66 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions. O
Similar O
to O
this O
work O

- B-DAT
trained O
VGG O
network O
instead O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity. O
The O
GAN O
procedure O
encourages O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mark O
datasets O
as O
well O
as O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
counterpart O
IHR. O
The O
high-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nels O
as O
in O
the O
VGG O

- B-DAT
ical O
for O
the O
performance O
of O

- B-DAT
tent O
loss O
lSRX O
and O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
solutions O
with O
overly O
smooth O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stead O
of O
log[1−DθD O
(GθG(ILR))] O
[22 O

- B-DAT
mark O
datasets O
Set5 O
[3], O
Set14 O

- B-DAT
and O
high-resolution O
images. O
This O
corresponds O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bor, O
bicubic, O
SRCNN O
[9] O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gral O
score O
from O
1 O
(bad O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG54 O
and O
the O
original O
HR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ResNet O
and O
the O
adversarial O
networks O

- B-DAT
SRGAN- O
Set5 O
MSE O
VGG22 O
MSE O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
MSE-based O
reconstructions, O
to O
those O
competing O

- B-DAT

- B-DAT
formed O
other O
SRGAN O
and O
SRResNet O

- B-DAT

- B-DAT
GAN O
to O
NN, O
bicubic O
interpolation O

- B-DAT

- B-DAT

- B-DAT
art O
methods. O
Quantitative O
results O
are O

- B-DAT
resolved O
with O
SRResNet O
and O
SRGAN O

- B-DAT
realistic O
image O
SR. O
All O
differences O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentary O
material). O
We O
further O
found O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Net O
that O
sets O
a O
new O

- B-DAT
sure. O
We O
have O
highlighted O
some O

- B-DAT

- B-DAT
ial O
loss O
by O
training O
a O

- B-DAT

- B-DAT

- B-DAT
the-art O
reference O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Stevenson. O
Super-Resolution B-DAT
from O
Image O
Sequences O
- O
A O
Review. O
Midwest O
Symposium O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
by O
adaptive O
sparse O
domain O

- B-DAT
ization. O
IEEE O
Transactions O
on O
Image O

- B-DAT

- B-DAT
resolution. O
IEEE O
Computer O
Graphics O
and O

- B-DAT
level O
vision. O
International O
Journal O
of O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

- B-DAT
line O
at O
http://torch.ch/blog/2016/02/04/resnets. O
html. O
2016 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In O
European O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT
tion O
with O
deep O
convolutional O
neural O

- B-DAT

- B-DAT

- B-DAT
mentation O
algorithms O
and O
measuring O
ecological O

- B-DAT

- B-DAT

- B-DAT
sive O
survey. O
In O
Machine O
Vision O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
for O
fast O
example-based O
super-resolution O

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
(ACCV O

- B-DAT

- B-DAT

- B-DAT
Resolution O
via O
Deep O
and O
Shallow O

- B-DAT

- B-DAT

- B-DAT
ence O
on O
Signals, O
Systems O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
derstanding O
Neural O
Networks O
Through O
Deep O

International O
Conference O
on O
Machine O
Learning O
- B-DAT
Deep O
Learning O
Workshop O
2015, O
page O

- B-DAT

- B-DAT
resolution O
by O
retrieving O
web O
images O

- B-DAT
volutional O
networks. O
In O
European O
Conference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG22, O
SRGAN-VGG54) O
described O
in O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
off O
between O
accuracy O
and O
speed O

-100 B-DAT
thousand O
update O
iterations O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

A.4. O
Set5 O
- B-DAT
Visual O
Results O

A.5. O
Set14 O
- B-DAT
Visual O
Results O

A.6. O
BSD100 O
(five O
random O
samples) O
- B-DAT
Visual O
Results O

BSD100 B-DAT

BSD100 B-DAT

on O
an O
image O
from O
the O
BSD100 B-DAT

Fattal, O
R.: O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

H.: O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In: O
Proceedings O

BSD100 B-DAT
mean O

Set5 O
[60], O
Set14 O
[61], O
and O
BSD100 B-DAT
[41] O
datasets. O
We O
report O
PSNR O

BSD100 B-DAT
mean O

on O
an O
image O
from O
the O
BSD100 B-DAT
dataset. O
We O
report O
PSNR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
a O
per-pixel O
loss O
between O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tracted O
from O
pretrained O
networks. O
We O

- B-DAT
proaches, O
and O
propose O
the O
use O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pared O
to O
the O
optimization-based O
method O

- B-DAT
itative O
results O
but O
is O
three O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ples O
from O
image O
processing O
include O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mantic O
segmentation O
and O
depth O
estimation O

- B-DAT
forward O
convolutional O
neural O
network O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stead O
on O
differences O
between O
high-level O

- B-DAT
mizing O
a O
loss O
function. O
This O

- B-DAT

- B-DAT
forward O
transformation O
networks O
for O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sure O
image O
similarities O
more O
robustly O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cally O
similar O
to O
the O
input O

- B-DAT
resolution O
fine O
details O
must O
be O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fer O
of O
semantic O
knowledge O
from O

- B-DAT

- B-DAT
mization O
problem O
from O
[10]; O
our O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
forward O
image O
transformation O
tasks O
have O

- B-DAT
tional O
neural O
networks O
with O
per-pixel O

- B-DAT

- B-DAT
ing O
with O
a O
per-pixel O
classification O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ful O
output O
image O
using O
a O

- B-DAT

- B-DAT
pixel O
regression O
[4,5] O
or O
classification O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
to O
generate O
images O
where O

- B-DAT
level O
features O
extracted O
from O
a O

- B-DAT
stand O
the O
functions O
encoded O
in O

- B-DAT

- B-DAT
formation O
retained O
by O
different O
network O

- B-DAT

- B-DAT

- B-DAT

-16 B-DAT

- B-DAT
quality O
results, O
but O
is O
computationally O

- B-DAT
mization O
problem O
requires O
a O
forward O

- B-DAT

- B-DAT
work O
to O
quickly O
approximate O
solutions O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
niques O
into O
prediction-based O
methods O
(bilinear O

- B-DAT
based O
methods O
[25,26], O
statistical O
methods O

- B-DAT

- B-DAT
mance O
on O
single-image O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formation O
network O
fW O
and O
a O

- B-DAT
lutional O
neural O
network O
parameterized O
by O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trained O
for O
image O
classification O
have O

- B-DAT
fication O
as O
a O
fixed O
loss O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
image, O
and O
the O
style O

- B-DAT

- B-DAT

- B-DAT
chitecture O
of O
[44]. O
All O
non-residual O

- B-DAT
put O
layer, O
which O
instead O
uses O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sample O
the O
low-resolution O
input O
before O

- B-DAT

-16 B-DAT
loss O
network O
φ. O
As O
we O

-2 B-DAT
convolutions O
to O
downsample O
the O
input O

- B-DAT

- B-DAT

- B-DAT
lutional O
layer O
increases O
the O
effective O

- B-DAT

- B-DAT
trained O
for O
image O
classification, O
meaning O

- B-DAT

- B-DAT

- B-DAT

-16 B-DAT
loss O
network O
φ. O
The O
images O

- B-DAT
put O
image O
ŷ O
when O
it O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
[48,49] O
and O
make O
use O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
with O
convolutional O
neural O
networks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions. O
Our O
networks O
are O
trained O

- B-DAT
mization O
is O
performed O
using O
L-BFGS O

- B-DAT

- B-DAT
timization O
converges O
to O
satisfactory O
results O

- B-DAT

-16 B-DAT
loss O
network O
φ O

- B-DAT

-16 B-DAT
loss O
network O
φ. O
Our O
implementation O

- B-DAT

- B-DAT

- B-DAT

-1832 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-16 B-DAT
loss O
network O
has O
features O
which O

-16 B-DAT
features, O
and O
in O
doing O
so O

- B-DAT
tion O
5. O
The O
baseline O
performs O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put O
image O
from O
a O
low-resolution O

- B-DAT

- B-DAT
lem, O
since O
for O
each O
low-resolution O

- B-DAT

- B-DAT
ages O
that O
could O
have O
generated O

- B-DAT

- B-DAT
resolution O
image O
may O
have O
little O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
since O
larger O
factors O
require O

- B-DAT

- B-DAT
sessment O
of O
visual O
quality O
[55,56,57,58,59 O

- B-DAT
level O
differences O
between O
pixels O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mize O
feature O
reconstruction O
loss. O
We O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-16 B-DAT
loss O
network O
φ. O
We O
train O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formance. O
SRCNN O
is O
a O
three-layer O

- B-DAT

- B-DAT
CNN O
is O
not O
trained O
for O

- B-DAT

- B-DAT
ally O
feasible O
for O
our O
models O

- B-DAT
works O
for O
×4 O
and O
×8 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mation O
tasks O
and O
optimization-based O
methods O

- B-DAT

- B-DAT
image O
super-resolution O
where O
we O
show O

- B-DAT
tion. O
We O
also O
plan O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lutional O
networks. O
(2015 O

- B-DAT

- B-DAT

- B-DAT
ing O
them. O
In: O
Proceedings O
of O

- B-DAT
works: O
Visualising O
image O
classification O
models O

- B-DAT

- B-DAT
tation. O
arXiv O
preprint O
arXiv:1505.04366 O
(2015 O

- B-DAT

- B-DAT
ings O
of O
the O
IEEE O
International O

- B-DAT
gus, O
R.: O
Intriguing O
properties O
of O

- B-DAT
tional O
Conference O
on, O
IEEE O
(2012 O

- B-DAT
ject O
detection O
features. O
In: O
Proceedings O

- B-DAT
works. O
arXiv O
preprint O
arXiv:1506.02753 O
(2015 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puter O
Graphics O
and O
Applications, O
IEEE O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision, O
2009 O
IEEE O
12th O

- B-DAT

- B-DAT

- B-DAT
ple O
regression. O
In: O
Proceedings O
of O

- B-DAT

- B-DAT

- B-DAT
sentation O
of O
raw O
image O
patches O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
with O
deep O
convolutional O
generative O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-192376 B-DAT
(2011 O

- B-DAT

- B-DAT

- B-DAT
ence O
image O
quality O
assessment O
algorithms O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
representations. O
In: O
Curves O
and O
Surfaces O

- B-DAT

- B-DAT

Dilated O
Conv O
+ O
Residual O
Blocks O
4x B-DAT
NN O

BSD100 B-DAT
29 I-DAT

BSD100 B-DAT
25 I-DAT

BSD100 B-DAT
22 I-DAT

BSD100 B-DAT
0 I-DAT

BSD100 B-DAT
0 I-DAT

BSD100 B-DAT
0 I-DAT

BSD100 B-DAT
29.56 O
28.30 O
32.37 O
23.97 O
28.12 O

BSD100 B-DAT
25.96 O
24.93 O
27.79 O
20.78 O
24.25 O

BSD100 B-DAT
22.11 O
- O
- O
18.65 O
21.63 O

BSD100 B-DAT
0.843 O
0.873 O
0.902 O
0.909 O
0.932 O

BSD100 B-DAT
0.668 O
0.627 O
0.744 O
0.773 O
0.851 O

BSD100 B-DAT
0.532 O
- O
- O
0.663 O
0752 O

factor O
SISR O
over O
Set5, O
Set14, O
BSD100, B-DAT
and O
Celeb-HQ O
datasets O
with O
bicubic O

models O
over O
datasets O
Set5, O
Set14, O
BSD100, B-DAT
and O
Celeb-HQ. O
Statistics O
for O
competing O

- B-DAT

- B-DAT

- B-DAT
ous O
demand O
for O
higher-resolution O
images O

- B-DAT

- B-DAT
resolution O
(SISR). O
The O
SISR O
problem O

- B-DAT

- B-DAT

- B-DAT
fectiveness O
for O
different O
scale O
factors O

- B-DAT
pared O
to O
basic O
interpolation O
schemes O

- B-DAT
pared O
with O
current O
state-of-the-art O
techniques O

- B-DAT
construction O
improves O
the O
quality O
of O

- B-DAT

- B-DAT

- B-DAT
resolution O
(HR) O
image O
from O
one O

- B-DAT
resolution O
(LR) O
images. O
SR O
plays O

- B-DAT

- B-DAT

- B-DAT
tions, O
only O
a O
single O
instance O

- B-DAT

- B-DAT

- B-DAT
posed O
inverse O
problem O
[6] O
that O

- B-DAT
formation O
to O
restrict O
the O
solution O

- B-DAT
nique O
introduced O
by O
Nazeri O
et O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
creasing O
the O
resolution O
of O
a O

- B-DAT
ery O
of O
pixel O
intensities O
in O

- B-DAT
els. O
The O
missing O
pixel O
intensities O

- B-DAT
ing O
regions O
of O
an O
image O

- B-DAT
ing O
task O
is O
modelled O
as O

- B-DAT
age. O
The O
pipeline O
involves O
first O

- B-DAT
struction O
of O
the O
HR O
image O

- B-DAT

- B-DAT
sampled O
by O
a O
factor O
of O

- B-DAT

- B-DAT
tinguished O
anymore O
as O
the O
problem O

- B-DAT

- B-DAT
construction O
of O
a O
high-resolution O
image O

- B-DAT

- B-DAT
ments O
of O
information O
using O
bilinear O

- B-DAT
tinctive O
features O
in O
the O
original O

- B-DAT

- B-DAT
pling O
by O
a O
factor O
of O

- B-DAT
tra O
empty O
row O
and O
column O

- B-DAT
polation O
and O
bicubic O
interpolation O
[3 O

- B-DAT
pling O
[5]. O
Edge-based O
methods O
learn O

- B-DAT
file O
[39] O
to O
reconstruct O
the O

- B-DAT
tion O
[36] O
to O
predict O
HR O

- B-DAT

- B-DAT
age O
itself O
[19, O
10] O
to O

- B-DAT

- B-DAT
works O
(CNN) O
with O
a O
per-pixel O

- B-DAT

- B-DAT
cently O
Johnson O
et O
al. O
[21 O

- B-DAT

- B-DAT
ing O
a O
perceptual O
loss. O
In O

- B-DAT

- B-DAT

- B-DAT
tion O
loss O
and O
Style O
reconstruction O

- B-DAT
of-the-art O
results O
on O
SISR O
for O

- B-DAT

- B-DAT
ducing O
realistically O
synthesized O
high-frequency O
textures O

- B-DAT

- B-DAT

- B-DAT
work O
to O
image O
super-resolution O
tasks O

- B-DAT
neously O
improves O
structure, O
texture, O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
couples O
SISR O
into O
two O
separate O

- B-DAT
tor O
for O
the O
edge O
enhancement O

- B-DAT
tures O
to O
the O
method O
proposed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
criminator O
follows O
the O
architecture O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
images. O
Their O
corresponding O
edge O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
maps O
in O
the O
intermediate O

- B-DAT
tor O
to O
produce O
results O
with O

- B-DAT
criminator. O
Spectral O
normalization O
(SN) O
[28 O

- B-DAT
bilizes O
training O
by O
scaling O
down O

- B-DAT
ter O
and O
gradient O
values. O
We O

- B-DAT
ments O

- B-DAT
ally O
strided O
convolution O
kernel. O
This O

- B-DAT

- B-DAT

- B-DAT
19 O
trained O
on O
the O
ImageNet O

-19 B-DAT

- B-DAT
mediate O
feature O
maps. O
The O
Gram O

- B-DAT
fully O
mitigate O
the O
“checkerboard” O
artifact O

- B-DAT
pose O
convolutions O
[31]. O
For O
both O

-19 B-DAT

- B-DAT
imize O
the O
reconstruction, O
style, O
perceptual O

- B-DAT

- B-DAT

- B-DAT
tector O
[1]. O
We O
can O
control O

- B-DAT
rameter O
σ. O
For O
our O
purposes O

- B-DAT
els O
of O
both O
stages O
were O

- B-DAT
tinue O
training O
until O
convergence. O
We O

- B-DAT
licly O
available O
datasets O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ization O
purposes, O
the O
LR O
image O

- B-DAT
neighbor O
interpolation. O
All O
HR O
images O

- B-DAT
pared O
against O
bicubic O
interpolation O
and O

- B-DAT
duces O
blurry O
results O
around O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Celeb-HQ B-DAT
33.25 O
- O
- O
31.33 O
32.12 O

Celeb-HQ B-DAT
29.59 O
- O
- O
27.94 O
28.23 O

Set5 O
23.80 O
- B-DAT
- O
19.32 O
23.73 O

Set14 O
22.37 O
- B-DAT
- O
18.47 O
21.44 O

BSD100 O
22.11 O
- B-DAT
- O
18.65 O
21.63 O

Celeb-HQ B-DAT
26.66 O
- O
- O
25.46 O
25.56 O

Celeb-HQ B-DAT
0.967 O
- O
- O
0.957 O
0.968 O

Celeb-HQ B-DAT
0.834 O
- O
- O
0.910 O
0.912 O

Set5 O
0.646 O
- B-DAT
- O
0.801 O
0.904 O

Set14 O
0.552 O
- B-DAT
- O
0.708 O
0.793 O

BSD100 O
0.532 O
- B-DAT
- O
0.663 O
0752 O

Celeb-HQ B-DAT
0.782 O
- O
- O
0.841 O
0.857 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Resolution O
task. O
We O
measure O
precision O

- B-DAT
ous O
scale O
factors O
of O
SISR O

- B-DAT
tion O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

eb O
- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
efit O
of O
this O
approach O
over O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
posed O
model O
learns O
to O
fill O

- B-DAT

- B-DAT
ent O
scales O
of O
SISR. O
Quantitative O

- B-DAT
ness O
of O
the O
structure-guided O
inpainting O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dard O
benchmarks O

- B-DAT

- B-DAT
ing O
algorithms. O
A O
better O
approach O

- B-DAT
age O
contents O
and O
structures O
and O

- B-DAT

- B-DAT
resolution O
process. O
Our O
source O
code O

- B-DAT

- B-DAT

- B-DAT
ration O
with O
the O
donation O
of O

- B-DAT

- B-DAT

- B-DAT
resolution O
through O
neighbor O
embedding. O
In O

- B-DAT
ings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT
ing O
a O
deep O
convolutional O
network O

- B-DAT
resolution. O
In O
European O
conference O
on O

- B-DAT
sion, O
pages O
184–199. O
Springer, O
2014 O

- B-DAT
sions. O
Journal O
of O
applied O
meteorology O

- B-DAT

- B-DAT
nition, O
pages O
117–130. O
Springer, O
2007 O

- B-DAT
vances O
and O
challenges O
in O
super-resolution O

- B-DAT
tional O
Journal O
of O
Imaging O
Systems O

- B-DAT
far. O
Fast O
and O
robust O
multiframe O

- B-DAT
tics. O
ACM O
transactions O
on O
graphics O

- B-DAT
ing O
from O
local O
self-examples. O
ACM O

- B-DAT

- B-DAT

- B-DAT
thesis O
using O
convolutional O
neural O
networks O

- B-DAT
vances O
in O
Neural O
Information O
Processing O

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT
lenge O
on O
Perceptual O
Image O
Restoration O

- B-DAT
ulation O
(PIRM) O
at O
the O
15th O

- B-DAT

- B-DAT

- B-DAT
gio. O
Generative O
adversarial O
nets. O
In O

- B-DAT
ral O
information O
processing O
systems, O
pages O

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
IEEE O
conference O

- B-DAT
ual O
learning O
for O
image O
recognition O

- B-DAT
age O
super-resolution O
from O
transformed O
self-exemplars O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision, O
Kyoto O

- B-DAT
to-image O
translation O
with O
conditional O
adversarial O

- B-DAT
works. O
In O
Proceedings O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sive O
growing O
of O
GANs O
for O

- B-DAT
ing O
Representations, O
2018. O
4 O

- B-DAT

- B-DAT
ningham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT
sion O
and O
Pattern O
Recognition O
(CVPR O

- B-DAT
hanced O
deep O
residual O
networks O
for O

- B-DAT
resolution. O
In O
Proceedings O
of O
the O

- B-DAT
works. O
In O
International O
Conference O
on O

- B-DAT
resentations, O
2018. O
3 O

- B-DAT
painting O
with O
adversarial O
edge O
learning O

- B-DAT

- B-DAT
crimination. O
In O
Proceedings O
of O
the O

- B-DAT
ference O
on O
Computer O
Vision O
(ECCV O

- B-DAT
nition O
challenge. O
International O
Journal O
of O

- B-DAT
hancenet: O
Single O
image O
super-resolution O
through O

- B-DAT
tomated O
texture O
synthesis. O
In O
The O

- B-DAT
age/video O
upsampling. O
In O
ACM O
Transactions O

- B-DAT

- B-DAT
gle O
image O
and O
video O
super-resolution O

- B-DAT
cient O
sub-pixel O
convolutional O
neural O
network O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
lutional O
networks O
for O
large-scale O
image O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT
sion O
and O
Pattern O
Recognition O
(CVPR O

- B-DAT

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision, O
pages O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
versarial O
networks. O
In O
The O
IEEE O

- B-DAT
ence O
on O
Computer O
Vision O
(ICCV O

com- O
pression O
artifacts O
reduction, O
and O
Set5 B-DAT
[5] O
and O
Set14 O
[58] O
for O

super-resolution O
(×4) O
over O
the O
classic O
Set5 B-DAT
[5] O
and O
Set14 O
[58] O
datasets O

- B-DAT
ing O
a O
wide O
range O
of O

- B-DAT
age, O
the O
model O
synthesizes O
a O

- B-DAT

- B-DAT

- B-DAT
compression O
artifact O
reduction O
and O
single O

- B-DAT
olution. O
We O
demonstrate O
that O
our O

- B-DAT
performs O
state-of-the-art O
methods O
on O
all O

- B-DAT

- B-DAT
terpretable, O
which O
we O
demonstrate O
by O

- B-DAT

- B-DAT
gineering O
trade-offs O
entail O
that O
consumer O

- B-DAT

- B-DAT
ited O
in O
resolution O
and O
further O

- B-DAT
sion O
artifacts O
introduced O
for O
the O

- B-DAT
sion O
and O
storage. O
Scientific O
applications O

- B-DAT
tions O
of O
light, O
lenses O
and O

- B-DAT

- B-DAT
ments O
has O
been O
a O
long-standing O

- B-DAT

- B-DAT

- B-DAT
matically O
as O
inverse O
problems O
[48 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
porary O
techniques O
to O
inverse O
problems O

- B-DAT
ularization O
techniques O
which O
are O
amenable O

- B-DAT

- B-DAT

- B-DAT
tion O
of O
filter O
flow O
introduced O

- B-DAT
age O
are O
linearly O
combined O
to O

- B-DAT

- B-DAT

- B-DAT
ing O
an O
appropriately O
regularized/constrained O
flow O

- B-DAT
ages, O
we O
focus O
on O
applications O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lems, O
yielding O
state-of-the-art O
performance O
for O

- B-DAT

- B-DAT
resolution. O
Given O
a O
corrupted O
input O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
niques O
that O
learn O
to O
predict O

- B-DAT
labeled O
video O
data O
[15, O
16 O

- B-DAT
struction O
tasks O
we O
consider O
such O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gression O
models O
makes O
it O
hard O

- B-DAT
bustness O
in O
the O
presence O
of O

- B-DAT
fer O
reliability O
needed O
for O
researchers O

- B-DAT
based O
approaches O
in O
this O
regard O

- B-DAT
ing O
an O
explicit O
description O
of O

- B-DAT

- B-DAT

- B-DAT
ments O
on O
three O
different O
low-level O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ous O
low-level O
image O
reconstruction O
tasks O

- B-DAT

- B-DAT
tally O
that O
predictive O
filter O
flow O

- B-DAT

- B-DAT

- B-DAT
art O
methods O
remarkably O
on O
the O

- B-DAT
uniform O
motion O
blur O
removal, O
compression O

- B-DAT
tion O
and O
single O
image O
super-resolution O

- B-DAT
posing O
additional O
constraints O
on O
certain O

- B-DAT
standing O
a O
wide O
variety O
of O

- B-DAT

- B-DAT
ever, O
filter O
flow O
as O
originally O

- B-DAT

- B-DAT
mal O
filter O
flow O
is O
compute O

- B-DAT
ing O
spatially O
variant O
filtering O
over O

- B-DAT

- B-DAT
tremely O
challenging O
yet O
practically O
significant O

- B-DAT
moving O
blur O
caused O
by O
object O

- B-DAT
cally O
at O
patch O
level, O
and O

- B-DAT
formation O
about O
smooth O
motion O
by O

- B-DAT
fine O
discretized O
set O
of O
linear O

- B-DAT
invertible) O
transforms O
[51], O
i.e., O
downsampling O

- B-DAT
noising O
problems O
[6, O
20]. O
Recent O

- B-DAT

- B-DAT
resolution O
image O
from O
a O
single O

- B-DAT

- B-DAT

- B-DAT
tions O
exists O
for O
any O
given O

- B-DAT

- B-DAT
ods O
adopt O
an O
example-based O
strategy O

- B-DAT
timization O
solver, O
others O
are O
based O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
torized O
version O
of O
the O
source O

- B-DAT
volution O
corresponds O
to O
T O
being O

- B-DAT

- B-DAT
tice, O
particularly O
when O
the O
filters O

- B-DAT
tially O

- B-DAT
ing O
such O
a O
function O
fw O

- B-DAT
ence O
between O
a O
recovered O
image O

- B-DAT
sured O
by O
some O
loss O

- B-DAT

- B-DAT

- B-DAT
tural O
constraint O
that O
each O
output O

- B-DAT

- B-DAT
uct O
of O
this O
vector O
with O

- B-DAT

- B-DAT
ture O
activations O
at O
a O
single O

- B-DAT
tures O
[27, O
45, O
17 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uniform O
motion O
blur O
removal, O
meaning O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
suming O
there O
is O
no O
lighting O

- B-DAT
dicted O
filter O
weights. O
For O
other O

- B-DAT

- B-DAT

- B-DAT
cally O
without O
manual O
labeling. O
Given O

- B-DAT
ity O
images, O
we O
can O
automatically O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work O
with O
3×3 O
convolutional O
layers O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mation, O
while O
the O
second O
stream O

- B-DAT
volution O
layer O
and O
ReLU O
layer O

- B-DAT
ter O
Flow O
is O
self-supervised O
so O

- B-DAT
ited O
amount O
of O
image O
pairs O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
image O
regression O
CNNs, O
we O
also O

- B-DAT

- B-DAT
cients O

- B-DAT

- B-DAT
ary O
effects O
seen O
during O
training O

- B-DAT

- B-DAT
ing O
[24], O
with O
initial O
learning O

- B-DAT

- B-DAT
tensities. O
We O
train O
our O
model O

- B-DAT
dred O
epochs2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sections, O
respectively O

- B-DAT

- B-DAT
ing O
images. O
We O
evaluate O
each O

- B-DAT
uniform O
motion O
blur O
removal O
over O

- B-DAT
pression O
artifacts O
reduction, O
and O
Set5 O

- B-DAT

- B-DAT
Signal-to-Noise-Ratio O
(PSNR) O
and O
Structural O
Similarity O

- B-DAT
dex O
(SSIM) O
[52] O
over O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uniform O
motion O
blur O
dataset O
[2 O

- B-DAT
defined O
set O
of O
blur O
kernels O

- B-DAT

- B-DAT

- B-DAT
art O
methods O
over O
the O
released O

- B-DAT
els O
based O
on O
the O
proposed O

- B-DAT

- B-DAT
puts O
the O
quality O
images O
directly O

- B-DAT

- B-DAT
terpart O
based O
on O
the O
learned O

- B-DAT

- B-DAT
tecture O
is O
higher O
fidelity O
than O

- B-DAT

- B-DAT
mize O
artifacts, O
e.g., O
aliasing O
and O

- B-DAT
sults O
in O
Fig. O
2, O
along O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
provements O
when O
training O
models O
to O

- B-DAT
pixel O
kernels. O
This O
suggests O
that O

- B-DAT
put O
to O
the O
model. O
However O

- B-DAT
provement O
with O
additional O
iterations O
(results O

- B-DAT
ciently O
different O
than O
the O
blurred O

- B-DAT
lution O
could O
be O
inserting O
adversarial O

- B-DAT

- B-DAT

- B-DAT
ate O
JPEG O
compressed O
image O
patches O

- B-DAT
compressed O
ones O
on O
the O
fly O

- B-DAT
light O
areas O
where O
most O
apparent O

- B-DAT

- B-DAT
tion O
over O
LIVE1 O
dataset O
[52 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
compressed O
with O
a O
range O

- B-DAT
solute O
measurements O
by O
PSNR O
and O

- B-DAT
izable O
and O
stable O
performance. O
Basically O

- B-DAT
ages O
compressed O
with O
the O
same O

- B-DAT
pression O
quality O
factors O
measured O
by O

- B-DAT
inal O
JPEG O
compression O
is O
plotted O

- B-DAT

- B-DAT

- B-DAT
tions O

- B-DAT
tween O
CNN O
and O
PFF. O
The O

- B-DAT
duces O
the O
best O
visual O
quality O

- B-DAT
and O
low-frequency O
details O

- B-DAT

- B-DAT

- B-DAT
ages O
4× O
larger. O
To O
generate O

- B-DAT
inal O
image, O
we O
downsample O
14 O

- B-DAT

- B-DAT
sampled O
image O
from O
the O
low-resolution O

- B-DAT

- B-DAT

- B-DAT
aries O
and O
delivers O
an O
anti-aliasing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rics O
used O
here O
are O
PSNR O

- B-DAT
cially, O
the O
filter O
maps O
demonstrate O

- B-DAT

- B-DAT
depth O
understanding O

- B-DAT
dicted O
filter O
flows O
for O
different O

- B-DAT

- B-DAT

- B-DAT
tion O
along O
lines O
of O
different O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
troid O
for O
the O
filter O
at O

- B-DAT
put O
by O
our O
model O
generally O

- B-DAT
tage O
over O
other O
CNN-based O
models O

- B-DAT

- B-DAT

- B-DAT
diate O
layers O
of O
a O
CNN O

- B-DAT
fined O
semantics O
that O
can O
be O

- B-DAT
work O
generates O
space-variant O
per-pixel O
filters O

- B-DAT

- B-DAT
of-the-art O
methods O

- B-DAT

- B-DAT
els O
over O
patches, O
However, O
we O

- B-DAT

- B-DAT

- B-DAT
tions O
to O
reconstruct O
high-frequency O
detail O

- B-DAT

- B-DAT
entific O
applications O
such O
as O
medical O

- B-DAT
pared O
to O
physical O
models O
of O

-1618806 B-DAT

-1253538, B-DAT
DBI-1262547 O
and O
a O
hardware O
donation O

- B-DAT

- B-DAT

- B-DAT
ring O
by O
reblurring. O
In O
Proceedings O

- B-DAT
timation. O
In O
2017 O
12th O
IEEE O

- B-DAT

- B-DAT
ral O
computation, O
7(6):1129–1159, O
1995. O
1 O

- B-DAT
Morel. O
Low-complexity O
single-image O
super-resolution O
based O

- B-DAT
sion O
Conference, O
2012. O
4, O
7 O

- B-DAT

- B-DAT
nition, O
2005. O
CVPR O
2005. O
IEEE O

- B-DAT
ence O
on, O
volume O
2, O
pages O

- B-DAT

- B-DAT
tional O
Joint O
Conference O
on, O
pages O

- B-DAT

- B-DAT
tern O
Recognition, O
2004. O
CVPR O
2004 O

- B-DAT

- B-DAT
sion O
artifacts O
reduction O
by O
a O

- B-DAT
puter O
Vision, O
pages O
576–584, O
2015 O

- B-DAT

- B-DAT

- B-DAT
ence O
of O
interpretable O
machine O
learning O

- B-DAT
adaptive O
dct O
for O
high-quality O
denoising O

- B-DAT
cue. O
In O
European O
Conference O
on O

- B-DAT

- B-DAT
ing O
for O
image O
recognition. O
In O

- B-DAT
ference O
on O
computer O
vision O
and O

- B-DAT
tional O
neural O
networks O
for O
direct O

- B-DAT
ings O
of O
BMVC, O
volume O
10 O

- B-DAT
lutional O
networks. O
In O
Advances O
in O

- B-DAT
cessing O
Systems, O
pages O
769–776, O
2009 O

- B-DAT
sics: O
Unsupervised O
learning O
of O
optical O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
for O
parsimonious O
pixel O
labeling O

- B-DAT
spective O
understanding O
in O
the O
loop O

- B-DAT
tion O
(CVPR), O
2018. O
11 O

- B-DAT
ples O
in O
the O
physical O
world O

- B-DAT

- B-DAT

- B-DAT
tive O
adversarial O
network. O
In O
CVPR O

- B-DAT
tation. O
In O
Proceedings O
of O
the O

- B-DAT
czewicz. O
Adaptive O
deblocking O
filter. O
IEEE O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uating O
segmentation O
algorithms O
and O
measuring O

- B-DAT
ings. O
Eighth O
IEEE O
International O
Conference O

-10 B-DAT

- B-DAT

- B-DAT
cessing O
magazine, O
20(3):21–36, O
2003. O
1 O

- B-DAT

- B-DAT
tion O
(CVPR), O
2009. O
1, O
2 O

- B-DAT

- B-DAT
blurring O
from O
a O
single O
image O

- B-DAT
ics O
(tog), O
volume O
27, O
page O

- B-DAT
niques O
for O
compression O
artifact O
removal O

- B-DAT

- B-DAT
tional O
neural O
network O
for O
non-uniform O

- B-DAT
sion O
artifacts O
removal O
using O
convolutional O

- B-DAT
borhood O
regression O
for O
fast O
example-based O

- B-DAT

- B-DAT

- B-DAT
ing O
machine O
learning O
models O
interpretable O

- B-DAT
ume O
12, O
pages O
163–172. O
Citeseer O

- B-DAT
celli. O
Image O
quality O
assessment: O
from O

- B-DAT
ing, O
13(4):600–612, O
2004. O
4, O
6 O

- B-DAT

- B-DAT
puter O
vision, O
98(2):168–186, O
2012. O
3 O

- B-DAT
tation O
for O
natural O
image O
deblurring O

- B-DAT
tion, O
pages O
1107–1114, O
2013. O
5 O

- B-DAT

- B-DAT
resolution: O
A O
benchmark. O
In O
European O

- B-DAT
puter O
Vision, O
pages O
372–386. O
Springer O

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT

- B-DAT

- B-DAT
ual O
dense O
network O
for O
image O

- B-DAT

- B-DAT
alizations O
to O
understand O
the O
predicted O

- B-DAT
blurred O
image O
to O
the O
same O

- B-DAT

- B-DAT
sults O
for O
all O
the O
three O

- B-DAT

- B-DAT
nents O
by O
PCA O
shown O
in O

- B-DAT
ize O
the O
per-pixel O
loading O
factors O

- B-DAT
cipal O
component. O
We O
run O
PCA O

- B-DAT
pixel O
loading O
factors O
as O
a O

- B-DAT
sualization O
technique, O
we O
can O
know O

- B-DAT
ergy O
(stated O
in O
the O
main O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
terested O
in O
studying O
if O
we O

- B-DAT
eratively O
running O
the O
model, O
i.e O

- B-DAT
age O
as O
input O
to O
the O

- B-DAT
prisingly, O
we O
do O
not O
observe O

- B-DAT
ing O
explicitly O
with O
recurrent O
loops O

- B-DAT

- B-DAT

- B-DAT
tively. O
From O
these O
comparisons O
and O

- B-DAT

- B-DAT

- B-DAT
in O

- B-DAT
tion O
to O
and O
how O
it O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

widely O
used O
benchmark O
datasets O
– O
Set5 B-DAT
[42], O
Set14 O
[43], O
BSD100 O
[44 O

networks O
(PSNR O
is O
evaluated O
on O
Set5 B-DAT
with O
RGB O
channels O

Set5 B-DAT
Set14 O
BSD100 O
Urban100 O
Manga109 O
PSNR/SSIM O

The O
training O
curves O
(evaluated O
on O
Set5 B-DAT
with O
RGB O
channels) O
are O
shown O

size O
(PSNR O
is O
evaluated O
on O
Set5 B-DAT
with O
RGB O
channels O

networks O
(PSNR O
is O
evaluated O
on O
Set5 B-DAT

Set5 B-DAT

The O
training O
curves O
(evaluated O
on O
Set5 B-DAT

size O
(PSNR O
is O
evaluated O
on O
Set5 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
GAN) O
[1] O
is O
a O
seminal O

- B-DAT

- B-DAT
lar, O
we O
introduce O
the O
Residual-in-Residual O

- B-DAT
vide O
stronger O
supervision O
for O
brightness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lem, O
has O
attracted O
increasing O
attention O

- B-DAT
panies. O
SISR O
aims O
at O
recovering O

- B-DAT

- B-DAT

- B-DAT
perous O
development. O
Various O
network O
architecture O

- B-DAT

- B-DAT
Noise O
Ratio O
(PSNR) O
value O
[5,6,7,1,8,9,10,11,12 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uation O
of O
human O
observers O
[1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mize O
super-resolution O
model O
in O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
RGAN, O
consistently O
outperforms O
state-of-the-art O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
struction O
style O
and O
smoothness. O
Another O

- B-DAT
pate O
in O
region O
1 O
and O

- B-DAT

- B-DAT

- B-DAT
tures, O
such O
as O
a O
deeper O

- B-DAT
work O
[9], O
deep O
back O
projection O

- B-DAT
provement. O
Zhang O
et O
al. O
[11 O

- B-DAT
ing O
the O
state-of-the-art O
PSNR O
performance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity O
[29,14], O
perceptual O
loss O
[13 O

- B-DAT
imizing O
the O
error O
in O
a O

- B-DAT
pearance. O
Ledig O
et O
al. O
[1 O

- B-DAT
jadi O
et O
al. O
[16] O
develop O

- B-DAT

- B-DAT
veloping O
more O
effective O
GAN O
frameworks O

- B-DAT
tor O
includes O
gradient O
clipping O
[32 O

- B-DAT
ated O
data O
are O
real, O
but O

- B-DAT
sures, O
e.g., O
PSNR O
and O
SSIM O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lenge O
[3]. O
In O
a O
recent O

- B-DAT
tion, O
we O
first O
describe O
our O

- B-DAT
tion O
is O
done O
in O
the O

- B-DAT
ers; O
2) O
replace O
the O
original O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
putational O
complexity O
in O
different O
PSNR-oriented O

- B-DAT
ing O
dataset O
during O
testing. O
When O

- B-DAT
alization O
ability. O
We O
empirically O
observe O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
residual O
network. O
However, O
our O

- B-DAT
erage O
Discriminator O
RaD O
[2], O
denoted O

- B-DAT

- B-DAT
mulated O
as O
DRa(xr, O
xf O

- B-DAT

- B-DAT

- B-DAT
tures O
before O
activation O
rather O
than O

- B-DAT

-543 B-DAT
layer O
is O
merely O
11.17%. O
The O

- B-DAT

- B-DAT

- B-DAT
tance O
between O
recovered O
image O
G(xi O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nition O
[38], O
which O
focuses O
on O

- B-DAT
ing O
perceptual O
loss O
that O
focuses O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tures O
and O
similarly, O
22 O
represents O

- B-DAT

-22 B-DAT
b) O
activation O
map O
of O

-54 B-DAT

- B-DAT
boon’. O
With O
the O
network O
going O

- B-DAT

- B-DAT
ceptual O
quality, O
we O
propose O
a O

- B-DAT
tion. O
Specifically, O
we O
first O
train O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
parameters O
of O
these O
two O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
polated O
image O
is O
either O
too O

- B-DAT
rameter O
λ O
and O
η O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
oriented O
model O
with O
the O
L1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els O
on O
widely O
used O
benchmark O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
the-art O
PSNR-oriented O
methods O
including O
SRCNN O

- B-DAT

- B-DAT
tures, O
e.g., O
animal O
fur, O
building O

- B-DAT
pleasant O
artifacts, O
e.g., O
artifacts O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults, O
and O
than O
previous O
GAN-based O

- B-DAT
tures O
in O
building O
(see O
image O

- B-DAT

- B-DAT
mance O
without O
artifacts. O
It O
does O

- B-DAT
ment O
can O
be O
observed O
from O

- B-DAT
tures O
before O
activation O
can O
result O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gies O
in O
balancing O
the O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
oriented O
method O
outputs O
cartoon-style O
blurry O

- B-DAT

- B-DAT

- B-DAT
pirically O
make O
some O
modifications O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
terpolation O
between O
the O
results O
of O

- B-DAT

- B-DAT

- B-DAT
ceptual O
quality O
than O
previous O
SR O

- B-DAT

- B-DAT
dition, O
useful O
techniques O
including O
residual O

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In: O
CVPR. O
(2018 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
performance O
on O
imagenet O
classification O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
representations. O
In: O
International O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tics. O
(2010 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
duce O
several O
useful O
techniques O
that O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
a O
very O
deep O
network O

- B-DAT

- B-DAT
Residual O
Dense O
Block O
(RRDB), O
which O

- B-DAT
ers O
[47,28]. O
He O
et O
al O

- B-DAT

- B-DAT
tially. O
It O
is O
worth O
noting O

- B-DAT
tion O
(multiplying O
0.1 O
for O
all O

- B-DAT
tremely O
bad O
local O
minimum O
with O

- B-DAT
tion O
(×0.1) O
helps O
the O
network O

- B-DAT
tion O
achieves O
a O
higher O
PSNR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rithms: O
average O
PSNR/SSIM O
on O
Y O

Bicubic O
- B-DAT
28.42/0.8104 O
26.00/0.7027 O
25.96/0.6675 O
23.14/0.6577 O
24.89/0.7866 O

- B-DAT

- B-DAT
verse O
natural O
textures. O
We O
employ O

- B-DAT
over, O
the O
deeper O
model O
achieves O

- B-DAT

- B-DAT

- B-DAT

adopt O
5 O
standard O
benchmark O
datasets: O
Set5, B-DAT
Set14 O

variants O
trained O
and O
tested O
on O
Set5 B-DAT
dataset O

Base O
reaches O
PSNR=32.00 O
dB O
on O
Set5 B-DAT
(×4). O
Results O
from O
Ra O
to O

best O
PSNR O
(dB) O
values O
on O
Set5 B-DAT
(4×) O
in O
5.6× O
105 O
iterations O

datasets O
(e.g., O
such O
as O
Set5, B-DAT
Set14 O
and O
BSD100) O
with O
rich O

Method O
Set5 B-DAT
Set14 O
BSD100 O
Urban100 O
Manga109 O

Method O
Set5 B-DAT
Set14 O
BSD100 O
Urban100 O
Manga109 O

Computational O
and O
parameter O
comparison O
(2× O
Set5 B-DAT

variants O
trained O
and O
tested O
on O
Set5 B-DAT

Method O
Set5 B-DAT

Method O
Set5 B-DAT

The O
way O
of O
embedding O
upscaling B-DAT
feature O
in O
the O
last O
few O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
traction O
module, O
which O
consists O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
is O
optimized O
by O
stochastic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
plified O
residual O
blocks O
with O
local-source O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-1 B-DAT
Fg O

- B-DAT

- B-DAT
NL O

- B-DAT
NL O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-1 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sentations. O
Thus, O
we O
set O
α O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
spectively. O
f(·) O
and O
δ(·) O
are O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ture, O
and O
embed O
RL-NL O
modules O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tains O
the O
convolution O
layers O
with O

- B-DAT
ble O
1 O
we O
can O
see O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

best O
PSNR O
(dB) O
values O
on O
Set5 B-DAT
(2×) O
in O
5×104 O
iterations O

PSNR O
on O
Set5 B-DAT
(2×) O
37.45 O
37.77 O
37.81 O
37.87 O

use O
five O
standard O
benchmark O
datasets: O
Set5 B-DAT
[36], O
Set14 O
[37], O
B100 O
[38 O

removed, O
the O
PSNR O
value O
on O
Set5 B-DAT
(×2) O
is O
relatively O
low, O
no O

Method O
Scale O
Set5 B-DAT
Set14 O
B100 O
Urban100 O
Manga109 O

Method O
Scale O
Set5 B-DAT
Set14 O
B100 O
Urban100 O
Manga109 O

a) O
Results O
on O
Set5 B-DAT
(4×) O
0 O
0.5 O
1 O
1.5 O

b) O
Results O
on O
Set5 B-DAT
(8×) O
Fig. O
8. O
Performance O
and O

parameters. O
Results O
are O
evaluated O
on O
Set5 B-DAT

Method O
Scale O
Set5 B-DAT

Method O
Scale O
Set5 B-DAT

parameters. O
Results O
are O
evaluated O
on O
Set5 B-DAT
4.5 I-DAT
Object O
Recognition O
Performance O

the O
original O
LR O
inputs O
and O
upscaling B-DAT
spatial O
reso- O
lution O
at O
the O

upscaling B-DAT
strategy O
has O
been O
demon- O
strated O

upscaling B-DAT
SR O
methods O
(e.g., O
DRRN O
[5 O

shallow O
feature O
extraction O
HSF O
(·), O
upscaling B-DAT
module O
HUP O
(·), O
and O
reconstruction O

upscaling B-DAT
layer, O
whose O
weight O
set O
is O

upscaling, B-DAT
whose O
kernel O
size O
is O
1×1 O

is O
set O
as O
16. O
For O
upscaling B-DAT
module O
HUP O
(·), O
we O
follow O

- B-DAT

- B-DAT
portance O
for O
image O
super-resolution O
(SR O

- B-DAT
resolution O
inputs O
and O
features O
contain O

- B-DAT

- B-DAT
tion, O
which O
is O
treated O
equally O

- B-DAT
resentational O
ability O
of O
CNNs. O
To O

- B-DAT
tions. O
Meanwhile, O
RIR O
allows O
abundant O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
given O
its O
low-resolution O
(LR O

- B-DAT

- B-DAT
tions, O
ranging O
from O
security O
and O

- B-DAT

- B-DAT
merous O
learning O
based O
methods O
have O

- B-DAT
layer O
CNN O
for O
image O
SR O

- B-DAT

- B-DAT
work O
depth O
was O
demonstrated O
to O

- B-DAT
nition O
tasks, O
especially O
when O
He O

- B-DAT

- B-DAT

- B-DAT
wise O
features O
equally, O
which O
lacks O

- B-DAT
formation O
(e.g., O
low- O
and O
high-frequency O

- B-DAT

- B-DAT
sible. O
The O
LR O
images O
contain O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ual O
channel O
attention O
networks O
(RCAN O

- B-DAT

- B-DAT
ture O
to O
construct O
very O
deep O

- B-DAT
tions O
in O
RIR O
help O
to O

- B-DAT

- B-DAT
tional O
ability O
of O
the O
network O

- B-DAT
nity O
[1–11,22]. O
Attention O
mechanism O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vious O
works. O
By O
introducing O
residual O

- B-DAT
icant O
improvement O
in O
accuracy. O
Tai O

- B-DAT
lution O
at O
the O
network O
tail O

- B-DAT

- B-DAT
bines O
automated O
texture O
synthesis O
and O

- B-DAT
gree, O
their O
predicted O
results O
may O

- B-DAT
cant O
improvement. O
However, O
most O
of O

- B-DAT

- B-DAT
inative O
ability O
for O
different O
types O

-1 B-DAT
RG-g O
RG-G O

- B-DAT

-1 B-DAT
RCAB-b O
RCAB-B O

- B-DAT

- B-DAT
fication O
with O
a O
trunk-and-mask O
attention O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tain O
significant O
performance O
improvement O
for O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
posed O
RIR O
achieves O
the O
largest O

- B-DAT

- B-DAT
tively O

- B-DAT

- B-DAT

- B-DAT
strated O
to O
be O
more O
efficient O

- B-DAT

- B-DAT
ial O
losses O
[8, O
21]. O
To O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
frequency O
parts O
seem O
to O
be O

- B-DAT

- B-DAT
quently, O
the O
output O
after O
convolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
by O
global O
average O
pooling O

- B-DAT
wise O
features O
can O
be O
emphasized O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tively. O
WD O
is O
the O
weight O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
Fg,bXg,b O

- B-DAT
hance O
the O
discriminative O
ability O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
table O
performance O
improvements O
over O
previous O

- B-DAT
sions O
about O
the O
effects O
of O

- B-DAT
downscaling O
and O
channel-upscaling, O
whose O
kernel O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ation O
metric, O
and O
training O
settings O

- B-DAT

-1 B-DAT
and O
top-5 O
recognition O
errors) O
comparisons O

- B-DAT

- B-DAT
tion O
(CA) O
based O
on O
the O

- B-DAT
formance. O
It’s O
hard O
to O
obtain O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
CNN O
[2], O
SCN O
[3], O
VDSR O

- B-DAT

- B-DAT
ensemble O
strategy O
to O
further O
improve O

- B-DAT

- B-DAT
parisons O
for O
×2, O
×3, O
×4 O

- B-DAT

- B-DAT

- B-DAT
age O
“img O
004”, O
we O
observe O

- B-DAT
CNN O
cannot O
recover O
lines. O
Other O

- B-DAT
Cooking”, O
the O
cropped O
part O
is O

- B-DAT
ful O
representational O
ability O
can O
extract O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults O
with O
7 O
state-of-the-art O
methods O

- B-DAT

- B-DAT
ing O
details O
in O
images O
“img O

- B-DAT
tive O
components. O
These O
comparisons O
indicate O

- B-DAT

-1 B-DAT
error O
0.506 O
0.477 O
0.437 O
0.454 O

-5 B-DAT
error O
0.266 O
0.242 O
0.196 O
0.224 O

- B-DAT

- B-DAT

-50 B-DAT
[20] O
as O
the O
evaluation O
model O

- B-DAT

- B-DAT
idation O
dataset O
for O
evaluation. O
The O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
and O
top-5 O
errors. O
These O
comparisons O

- B-DAT

- B-DAT
nections, O
making O
the O
main O
network O

- B-DAT

- B-DAT

- B-DAT
terdependencies O
among O
channels. O
Extensive O
experiments O

-14 B-DAT

-1 B-DAT

-0484, B-DAT
and O
U.S. O
Army O
Research O
Office O

-17 B-DAT

-1 B-DAT

-0367 B-DAT

- B-DAT

- B-DAT

- B-DAT
lutional O
networks. O
TPAMI O
(2016 O

- B-DAT

- B-DAT
resolution O
with O
sparse O
prior. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
with O
deep O
laplacian O
pyramid O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In: O
CVPR. O
(2018 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
chines. O
In: O
ICML. O
(2010 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
representations. O
In: O
Proc. O
7th O
Int O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

employ O
five O
standard O
benchmark O
datasets: O
Set5 B-DAT
[1], O
Set14 O
[30], O
B100 O
[21 O

Set5 B-DAT
×2 O
33.66/0.9299 O
36.66/0.9542 O
37.53/0.9590 O
37.74/0.9591 O

butterfly’ O
from O
Set5 B-DAT

butterfly’ O
from O
Set5 B-DAT
HR/PSNR I-DAT
Bicubic/22.10 O
VDSR/27.28 O
DRRN/27.60 O
NLRN/28.08 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formation O
flows O
are O
solely O
feedforward O

- B-DAT

- B-DAT
plored. O
In O
this O
paper, O
we O

- B-DAT
rate O
image O
SR, O
in O
which O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tures O
captured O
under O
large O
receptive O

- B-DAT

- B-DAT
ciently O
selects O
and O
further O
enhances O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-the-art O
SR O
methods O
in O
terms O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
image O
SR O
methods O

- B-DAT

- B-DAT
tures O
solely O
flow O
from O
the O

- B-DAT

- B-DAT
tures O
extracted O
from O
the O
top O

- B-DAT

- B-DAT
agating O
high-level O
features O
to O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
features, O
we O
employ O
multiple O

- B-DAT

- B-DAT
tures O
to O
shallow O
layers. O
However O

- B-DAT

- B-DAT

- B-DAT
level O
information O
to O
refine O
low-level O

- B-DAT

- B-DAT

- B-DAT
posed O
GMFN O
shows O
better O
visual O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tensive O
experiments O
demonstrate O
the O
superiority O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ploys O
16 O
RDBs O

- B-DAT
level O
features O
for O
refining O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
tasks O
(e.g. O
classification O
[29 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ent O
direction, O
[12, O
31] O
applied O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
less, O
we O
argue O
that O
such O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ent O
receptive O
fields, O
every O
piece O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
back O
module O
to O
adaptively O
eliminate O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
able O
contextual O
knowledge O
from O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
cost O
was O
quadratically O
saved O

- B-DAT
gether. O
However, O
these O
networks O
require O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pendent O
convolutional O
neural O
network O
which O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
jacent O
time O
steps O
is O
achieved O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
RDB-BRDB O

- B-DAT

-1 B-DAT
RDB-b O
RDB-B O

- B-DAT

- B-DAT

L O
BF O
-, B-DAT

L O
bF O
- B-DAT
, O
t O

- B-DAT

- B-DAT

- B-DAT
tures O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
RDN O
[33], O
the O
number O

- B-DAT

- B-DAT
volutional O
layer. O
Then, O
a O
3×3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lowing O
RDB. O
The O
placement O
of O

- B-DAT
cording O
to O
the O
relative O
hierarchical O

- B-DAT

- B-DAT

- B-DAT
cilitates O
the O
refinement O
processes O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lected O
indexes O
of O
the O
deepest O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ative O
hierarchical O
relationship O
among O
multiple O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
information O
captured O
under O
different O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
network O
is O
set O
to O
C0 O

- B-DAT
ment O
training O
images O
with O
scaling O

- B-DAT
tion O
bicubic. O
The O
SR O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formance O
of O
various O
single-to-multiple O
anti-feedback O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dex O
sets O
SM O
and O
DN O

- B-DAT

- B-DAT

- B-DAT
to-multiple O
feedback O
manner O
[12, O
31 O

- B-DAT
to-single O
feedback O
manners O
perform O
better O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fining O
low-level O
features. O
However, O
excessively O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
features. O
If O
the O
high-level O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trate O
the O
effectiveness O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bine O
various O
M O
to O
achieve O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ther O
hinder O
the O
reconstruction O
ability O

- B-DAT
tively O
selects O
the O
high O
frequency O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tively O
accesses O
to O
high-level O
information O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nections O
by O
setting O
M O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
son O
results2 O
in O
Tab. O
1 O

- B-DAT
struct O
a O
faithful O
SR O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ogy O
Department O
(No.2018GZ0178 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ding. O
In O
BMVC, O
2012 O

- B-DAT
volutional O
network O
for O
image O
super-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
projection O
networks O
for O
super-resolution. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
back O
network O
for O
image O
super-resolution O

- B-DAT

- B-DAT
layer O
recurrent O
connections O
for O
scene O

- B-DAT

- B-DAT

- B-DAT
masaki, O
and O
Kiyoharu O
Aizawa. O
Sketch-based O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
forward O
networks O
(FF O

-1 B-DAT

- B-DAT
ers O
are O
set O
to O
128 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults O
shown O
Figure O
6 O
indicate O

- B-DAT
tively. O
The O
performance O
evaluated O
on O

- B-DAT
served O
that O
with O
the O
help O

- B-DAT
nificantly O
improved O
compared O
with O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods, O
but O
it O
holds O
relatively O

-16, B-DAT
we O
provide O
more O
qualitative O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rately O
restored O
the O
letter O
"M O

- B-DAT

- B-DAT
coveres O
two O
horizontal O
lines O
as O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

use O
five O
standard O
benchmark O
datasets: O
Set5 B-DAT
[1], O
Set14 O
[33], O
B100 O
[18 O

the O
best O
performance O
(PSNR) O
on O
Set5 B-DAT
with O
scaling O
factor O
×2 O
in O

based O
on O
the O
PSNR O
on O
Set5 B-DAT
with O
scaling O
factor O
×2 O
in O

Set5 B-DAT
×2 O
33.66/0.9299 O
36.66/0.9542 O
37.52/0.9591 O
37.74/0.9591 O

PSNR O
and O
SSIM O
results O
on O
Set5, B-DAT
Set14, O
B100, O
Urban100 O

Set5 B-DAT
BD O
28.78/0.8308 O
32.21/0.9001 O
32.05/0.8944 O
26.23/0.8124 O

the O
best O
performance O
(PSNR) O
on O
Set5 B-DAT

based O
on O
the O
PSNR O
on O
Set5 B-DAT

Set5 B-DAT

layer O
in O
LR O
space O
for O
upscaling B-DAT

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

- B-DAT

- B-DAT
cently O
achieved O
great O
success O
for O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tract O
abundant O
local O
features O
via O

- B-DAT
tional O
layers. O
RDB O
further O
allows O

- B-DAT
taining O
dense O
local O
features, O
we O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
graded O
low-resolution O
(LR) O
measurement. O
SISR O

- B-DAT
lance O
imaging O
[42], O
medical O
imaging O

- B-DAT
eration O
[9]. O
While O
image O
SR O

- B-DAT

- B-DAT
cedure, O
since O
there O
exists O
a O

- B-DAT
based O
[40], O
reconstruction-based O
[37], O
and O

- B-DAT

- B-DAT

- B-DAT
ules, O
the O
networks O
for O
image O

- B-DAT
ory O
block O
was O
proposed O
to O

- B-DAT

- B-DAT
gles O
of O
view, O
and O
aspect O

- B-DAT
tion. O
While, O
most O
deep O
learning O

- B-DAT

- B-DAT
inal O
LR O
image O
to O
the O

- B-DAT
processing O
step O
not O
only O
increases O

- B-DAT
ing O
to O
our O
experiments O
(see O

- B-DAT
archical O
features O
from O
the O
original O

- B-DAT
posed O
residual O
dense O
block O
(Fig O

- B-DAT
tical O
for O
a O
very O
deep O

- B-DAT
ual O
dense O
block O
(RDB) O
as O

- B-DAT
sion O
(LFF) O
with O
local O
residual O

- B-DAT
catenating O
the O
states O
of O
preceding O

- B-DAT
ing O
layers O
within O
the O
current O

- B-DAT

- B-DAT
sion O
[15 O

- B-DAT

- B-DAT
work O
(RDN) O
for O
high-quality O
image O

- B-DAT
tiguous O
memory O
(CM) O
mechanism, O
but O

- B-DAT
lize O
all O
the O
layers O
within O

- B-DAT
tions. O
The O
accumulated O
features O
are O

- B-DAT
ods O
in O
computer O
vision O
[36 O

- B-DAT
ited O
space, O
we O
only O
discuss O

- B-DAT

- B-DAT

- B-DAT
ther O
improved O
mainly O
by O
increasing O

- B-DAT
ing O
network O
weights. O
VDSR O
[10 O

- B-DAT
creased O
the O
network O
depth O
by O

- B-DAT
duced O
recursive O
learning O
in O
a O

- B-DAT
rameter O
sharing. O
Tai O
et O
al O

- B-DAT
inal O
LR O
images O
to O
the O

- B-DAT

- B-DAT
creases O
computation O
complexity O
quadratically O
[4 O

- B-DAT

- B-DAT
tures O
from O
the O
interpolated O
LR O

- B-DAT

- B-DAT

- B-DAT
PCN O
[22], O
where O
an O
efficient O

- B-DAT

- B-DAT

- B-DAT
ods O
extracted O
features O
in O
the O

- B-DAT
nal O
LR O
features O
with O
transposed O

- B-DAT

- B-DAT

- B-DAT
lows O
direct O
connections O
between O
any O

- B-DAT
troduced O
among O
memory O
blocks O
[26 O

- B-DAT

- B-DAT
duced O
by O
a O
very O
deep O

- B-DAT
tion O
tasks O
(e.g., O
image O
SR O

- B-DAT

- B-DAT
tracts O
features O
F−1 O
from O
the O

- B-DAT
ond O
shallow O
feature O
extraction O
layer O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
layers O
within O
the O
block O

- B-DAT
ture. O
More O
details O
about O
RDB O

- B-DAT
cludes O
global O
feature O
fusion O
(GFF O

- B-DAT

- B-DAT

- B-DAT
nected O
layers, O
local O
feature O
fusion O

- B-DAT
ual O
learning, O
leading O
to O
a O

- B-DAT
nism O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sists O
of O
G O
(also O
known O

- B-DAT

- B-DAT

- B-DAT
tional O
layers O
1 O

- B-DAT

- B-DAT

- B-DAT
ing O
RDB O
and O
each O
layer O

- B-DAT
sequent O
layers, O
which O
not O
only O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ber. O
On O
the O
other O
hand O

- B-DAT
duce O
a O
1 O
× O
1 O

- B-DAT

- B-DAT
comes O
larger, O
very O
deep O
dense O

- B-DAT
tional O
layers O
in O
one O
RDB O

- B-DAT

- B-DAT
mance. O
We O
introduce O
more O
results O

- B-DAT
ing, O
we O
refer O
to O
this O

- B-DAT
maps O
produced O
by O
residual O
dense O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tively O
fused O
to O
form O
FGF O

- B-DAT

- B-DAT
quency O
information. O
However, O
in O
the O

- B-DAT
sequent O
layers. O
The O
local O
feature O

- B-DAT

- B-DAT
tion, O
MemNet O
extracts O
features O
in O

- B-DAT

- B-DAT
nition). O
While O
RDN O
is O
designed O

- B-DAT

- B-DAT
ual O
learning, O
which O
would O
be O

- B-DAT

- B-DAT
cal O
features, O
which O
are O
neglected O

- B-DAT
ferences O
between O
SRDenseNet O
[31] O
and O

- B-DAT
troduces O
the O
basic O
dense O
block O

- B-DAT
bilizes O
the O
training O
of O
wide O

- B-DAT
tract O
global O
features, O
because O
our O

- B-DAT
formance O
and O
convergence O
[17]. O
As O

- B-DAT
formation O
from O
their O
preceding O
layers O

- B-DAT
ers O
within O
one O
RDB. O
Furthermore O

- B-DAT
nections O
among O
memory O
blocks O
in O

- B-DAT

- B-DAT
sults O
are O
evaluated O
with O
PSNR O

- B-DAT
tion O
models O
to O
simulate O
LR O

- B-DAT
bic O
downsampling O
by O
adopting O
the O

- B-DAT
ing O
90◦. O
1,000 O
iterations O
of O

- B-DAT

- B-DAT
ery O
200 O
epochs. O
Training O
a O

- B-DAT
rameters: O
the O
number O
of O
RDB O

- B-DAT
mance O
of O
SRCNN O
[3] O
as O

- B-DAT
cal O
residual O
learning O
(LRL), O
and O

- B-DAT
portant, O
our O
RDN O
allows O
deeper O

- B-DAT
ture O
fusion O
(LFF) O
is O
needed O

- B-DAT
erly, O
so O
LFF O
isn’t O
removed O

- B-DAT
strates O
that O
stacking O
many O
basic O

- B-DAT
sulting O
in O
RDN O
CM1LRL0GFF0, O
RDN O

- B-DAT
ponent O
can O
efficiently O
improve O
the O

- B-DAT
line. O
This O
is O
mainly O
because O

- B-DAT
ing O
in O
RDN O
CM1LRL1GFF0, O
RDN O

- B-DAT
bination O
in O
Table O
1). O
It O

- B-DAT
ponents O
simultaneously O
(denote O
as O
RDN O

- B-DAT
sistent O
with O
the O
analyses O
above O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
SR O
methods: O
SRCNN O
[3 O

- B-DAT

- B-DAT
ther O
improve O
our O
RDN O
and O

- B-DAT

- B-DAT
rable O
or O
even O
better O
results O

- B-DAT
DenseNet O
[31] O
and O
MemNet O
[26 O

- B-DAT
els, O
our O
RDN O
also O
achieves O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put O
patch O
size. O
Moreover, O
our O

- B-DAT

- B-DAT
ods O
would O
produce O
noticeable O
artifacts O

- B-DAT
pared O
methods O
fail O
to O
recover O

- B-DAT
cover O
it O
obviously. O
This O
is O

- B-DAT
archical O
features O
through O
dense O
feature O

- B-DAT
CNN O
[3], O
FSRCNN O
[4], O
VDSR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults O
in O
Figs. O
7 O
and O

- B-DAT
covers O
sharper O
edges. O
This O
comparison O

- B-DAT
tracting O
hierarchical O
features O
from O
the O

- B-DAT
ods O
[3, O
10, O
38]. O
However O

- B-DAT
parison O
indicates O
that O
RDN O
is O

- B-DAT
tion O
models O
demonstrate O
the O
effectiveness O

- B-DAT
ing O
factor O
×3. O
The O
SR O

- B-DAT
ban100 O
and O
“img O
099” O
from O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferent O
or O
unknown O
degradation O
models O

- B-DAT
lizes O
the O
training O
wider O
network O

- B-DAT
ual O
leaning O
(LRL) O
further O
improves O

- B-DAT
world O
data. O
Extensive O
benchmark O
evaluations O

- B-DAT
strate O
that O
our O
RDN O
achieves O

- B-DAT

- B-DAT

- B-DAT
art O
methods O

-14 B-DAT

-1 B-DAT

- B-DAT
0484, O
and O
U.S. O
Army O
Research O

-17 B-DAT

- B-DAT
1-0367 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
deep O
convolutional O
networks O

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT

- B-DAT
mization. O
In O
ICLR, O
2014. O
5 O

- B-DAT
resolution. O
In O
CVPR, O
2017. O
1 O

- B-DAT
ham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT

- B-DAT
supervised O
nets. O
In O
AISTATS, O
2015 O

- B-DAT

- B-DAT
cal O
statistics. O
In O
ICCV, O
2001 O

- B-DAT
masaki, O
and O
K. O
Aizawa. O
Sketch-based O

- B-DAT
ing O
manga109 O
dataset. O
Multimedia O
Tools O

- B-DAT

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT
tia, O
A. O
M. O
S. O
M O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
hazing O
network. O
In O
CVPR, O
2018 O

- B-DAT

- B-DAT
raining O
using O
a O
multi-stream O
dense O

- B-DAT

- B-DAT
resolution O
with O
non-local O
means O
and O

- B-DAT
sion. O
TIP, O
2012. O
1 O

- B-DAT
lutional O
super-resolution O
network O
for O
multiple O

- B-DAT

- B-DAT

- B-DAT
nition O
problem. O
TIP, O
2012. O
1 O

on O
five O
standard O
benchmark O
datasets: O
Set5 B-DAT

of O
T O
and O
G O
on O
Set5 B-DAT
with O
scaling O
factor O
×4 O

The O
impact O
of O
feedback O
on O
Set5 B-DAT
with O
scale O
factor O
×4 O

from O
32.11dB O
to O
31.82dB O
on O
Set5 B-DAT
with O
scale O
factor O
×4. O
By O

from O
32.11dB O
to O
31.82dB O
on O
Set5 B-DAT
with O
scale O
factor O
×4. O
By O

PSNR O
values O
are O
evaluated O
on O
Set5 B-DAT

parameters. O
Results O
are O
evaluated O
on O
Set5 B-DAT
with O
scale O
factor O
×4. O
Red O

PSNR O
values O
are O
evaluated O
on O
Set5 B-DAT

parameters. O
Results O
are O
evaluated O
on O
Set5 B-DAT
with O
scale O
factor O
×4. O
Red O

Set5 B-DAT
×2 O
33.66/0.9299 O
36.66/0.9542 O
37.53/0.9590 O
37.74/0.9591 O

Set5 B-DAT
BD O
28.34/0.8161 O
31.63/0.8888 O
33.30/0.9159 O
33.38/0.9182 O

butterfly O
from O
Set5 B-DAT

with O
scale O
factor O
×4 O
on O
Set5 B-DAT

with O
scale O
factor O
×4 O
on O
Set5 B-DAT

Params. O
Set5 B-DAT
Set14 O
B100 O
Urban100 O
Manga109 O
MemNet-Pytorch O

of O
T O
and O
G O
on O
Set5 B-DAT

The O
impact O
of O
feedback O
on O
Set5 B-DAT

from O
32.11dB O
to O
31.82dB O
on O
Set5 B-DAT

from O
32.11dB O
to O
31.82dB O
on O
Set5 B-DAT

parameters. O
Results O
are O
evaluated O
on O
Set5 B-DAT

parameters. O
Results O
are O
evaluated O
on O
Set5 B-DAT

Set5 B-DAT

butterfly O
from O
Set5 B-DAT
HR I-DAT
Bicubic O
SRCNN O
VDSR O

Params. O
Set5 B-DAT

patch O
size O
based O
on O
the O
upscaling B-DAT
factor. O
The O
settings O
of O
input O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

- B-DAT

- B-DAT

- B-DAT
plored O
the O
power O
of O
deep O

- B-DAT
construction O
performance. O
However, O
the O
feedback O

- B-DAT
nism, O
which O
commonly O
exists O
in O

- B-DAT

- B-DAT
level O
representations O
with O
high-level O
information O

- B-DAT
ically, O
we O
use O
hidden O
states O

- B-DAT
ful O
high-level O
representations. O
The O
proposed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
task, O
which O
aims O
to O

- B-DAT

- B-DAT

- B-DAT
herently O
ill-posed O
since O
multiple O
HR O

- B-DAT
merous O
image O
SR O
methods O
have O

- B-DAT
ing O
interpolation-based O
methods[45], O
reconstruction-based O
methods[42 O

- B-DAT

- B-DAT
lutional O
Neural O
Network O
(CNN) O
to O

- B-DAT
tention O
in O
recent O
years O
due O

- B-DAT
posed O
network. O
Blue O
arrows O
represent O

- B-DAT

- B-DAT
ing O
more O
contextual O
information O
with O

- B-DAT
ishing/exploding O
problems O
caused O
by O
simply O

- B-DAT
ters O
increases. O
A O
large-capacity O
network O

- B-DAT

- B-DAT
current O
Neural O
Network O
(RNN). O
Similar O

- B-DAT
tional O
deep O
learning O
based O
methods O

- B-DAT
ward O
manner. O
However, O
the O
feedforward O

- B-DAT

- B-DAT

- B-DAT
down O
manner, O
carrying O
high-level O
information O

- B-DAT
vious O
layers O
and O
refining O
low-level O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
structed O
by O
multiple O
sets O
of O

- B-DAT
and O
down-sampling O
layers O
with O
dense O

- B-DAT

- B-DAT
covery O
difficulty. O
Such O
curriculum O
learning O

- B-DAT
tion O
models. O
Experimental O
results O
demonstrate O

- B-DAT
ority O
of O
our O
proposed O
SRFBN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work O
(SRFBN), O
which O
employs O
a O

- B-DAT
nism. O
High-level O
information O
is O
provided O

- B-DAT

- B-DAT
while, O
such O
recurrent O
structure O
with O

- B-DAT
tions O
provides O
strong O
early O
reconstruction O

- B-DAT
ficiently O
handles O
feedback O
information O
flows O

- B-DAT

- B-DAT
and O
down- O
sampling O
layers, O
and O

- B-DAT

- B-DAT
ing O
reconstruction O
difficulty O
are O
fed O

- B-DAT

- B-DAT

- B-DAT
ious O
computer O
vision O
tasks O
including O

- B-DAT

- B-DAT

- B-DAT
mation O
usage O
in O
LR O
images O

- B-DAT
provement O
in O
image O
SR. O
SRResNet[21 O

- B-DAT
plied O
residual O
skip O
connections O
from O

- B-DAT
work O
architectures O
use O
or O
combine O

- B-DAT

- B-DAT

- B-DAT
textual O
information O
due O
to O
the O

- B-DAT

- B-DAT
ing O
layers, O
and O
thus O
further O

- B-DAT
ity O
of O
the O
network. O
To O

- B-DAT
resolution O
feedback O
network O
(SRFBN), O
in O

- B-DAT

- B-DAT
down O
manner O
to O
correct O
low-level O

- B-DAT
textual O
information O

- B-DAT
capacity O
networks O
occupy O
huge O
amount O

- B-DAT
rent O
structure O
was O
employed[19, O
31 O

- B-DAT

- B-DAT

- B-DAT
and O
down-projection O
units O
to O
achieve O

- B-DAT
tion O
between O
two O
recurrent O
states O

- B-DAT

- B-DAT
ever, O
the O
flow O
of O
information O

- B-DAT

- B-DAT
tion O
of O
an O
input O
image O

- B-DAT
tional O
recurrent O
neural O
network. O
However O

- B-DAT

- B-DAT
ficiently O
flows O
across O
hierarchical O
layers O

- B-DAT
rior O
reconstruction O
performance O
than O
ConvLSTM1 O

- B-DAT
ficient O
strategy O
to O
improve O
the O

- B-DAT

- B-DAT
diction, O
they O
enforce O
a O
curriculum O

- B-DAT
creases O
during O
the O
training O
process O

- B-DAT
mid O
in O
previously O
trained O
networks O

- B-DAT
cess, O
we O
enforce O
a O
curriculum O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
effect O
process O
helps O
to O
achieve O

- B-DAT

- B-DAT
eration O
(to O
force O
the O
network O

- B-DAT
tion O
of O
high-level O
information), O
(2 O

- B-DAT

- B-DAT
formation, O
which O
is O
needed O
to O

- B-DAT
folded O
to O
T O
iterations, O
in O

- B-DAT
rally O
ordered O
from O
1 O
to O

- B-DAT

- B-DAT
tains O
three O
parts: O
an O
LR O

- B-DAT
sampled O
image O
to O
bypass O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
layer O
and O
a O
deconvolutional O

- B-DAT
traction O
block. O
F O
tin O
are O

- B-DAT

- B-DAT

- B-DAT
tained O
by O

- B-DAT

- B-DAT

- B-DAT
sentations O
F O
tin, O
and O
then O

- B-DAT

- B-DAT
tion O
block. O
The O
FB O
contains O

- B-DAT
tially O
with O
dense O
skip O
connections O

- B-DAT
jection O
group, O
which O
can O
project O

- B-DAT
tures O
F O
tin O
by O
feedback O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ingly, O
Ltg O
can O
be O
obtained O

- B-DAT

- B-DAT
tion O
group O
and O
map O
the O

- B-DAT
work. O
T O
target O
HR O
images O

- B-DAT
work. O
(I1HR, O
I O

- B-DAT
tion O
in O
the O
network O
can O

- B-DAT
put O
at O
the O
t-th O
iterations O

- B-DAT

- B-DAT
and O
down-sampling O
operations. O
For O
×2 O

- B-DAT
ating O
LR O
images O
from O
ground O

- B-DAT
ify O
the O
effectiveness O
of O
our O

- B-DAT
degradation O
models O
as O
[47] O
do O

- B-DAT
periments, O
we O
use O
7x7 O
sized O

- B-DAT
sampling O
followed O
by O
adding O
Gaussian O

- B-DAT
ing O
rate O
0.0001. O
The O
learning O

- B-DAT
ery O
200 O
epochs. O
We O
implement O

- B-DAT
ber O
of O
iterations O
(denoted O
as O

- B-DAT
jection O
groups O
in O
the O
feedback O

- B-DAT
iments. O
We O
first O
investigate O
the O

- B-DAT
out O
feedback O
connections O
(T=1). O
Besides O

- B-DAT
sults. O
It O
is O
worth O
noticing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
forward O
one O
in O
this O
subsection O

- B-DAT
put O
to O
low-level O
representations O
and O

- B-DAT
erty), O
denoted O
as O
SRFBN-L-FF. O
SRFBN-L O

- B-DAT

- B-DAT
FF O
both O
have O
four O
iterations O

- B-DAT
ate O
SR O
images O
from O
both O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
from O
which O
we O
conclude O

- B-DAT
trast O
to O
feedforward O
network. O
The O

- B-DAT
rent O
structure. O
Except O
for O
the O

- B-DAT
tive O
experiments O
to O
verify O
other O

- B-DAT
ation O
except O
the O
first O
iteration O

- B-DAT
put O
to O
low-level O
representations O
and O

- B-DAT
erty), O
denoted O
as O
SRFBN-L-FF. O
SRFBN-L O

- B-DAT

- B-DAT
FF O
both O
have O
four O
iterations O

- B-DAT
ate O
SR O
images O
from O
both O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ation, O
from O
which O
we O
conclude O

- B-DAT
trast O
to O
feedforward O
network. O
The O

- B-DAT
current O
structure. O
Except O
the O
above O

- B-DAT
tive O
experiments O
to O
verify O
other O

- B-DAT
ation O
except O
the O
first O
iteration O

- B-DAT
works O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trated O
in O
Fig. O
5. O
Each O

- B-DAT

- B-DAT
covering O
the O
residual O
image. O
In O

- B-DAT
inal O
input O
image[16] O
and O
to O

- B-DAT

- B-DAT
ponents O
(i.e. O
edges O
and O
contours O

- B-DAT
age. O
To O
some O
extent, O
this O

- B-DAT
tion O
ability O
than O
the O
feedforward O

- B-DAT
vation O
is O
that O
the O
feedback O

- B-DAT
sentations O
in O
contrast O
to O
feedforward O

- B-DAT
tions O
and O
then O
the O
smooth O

- B-DAT
strate O
that O
the O
feedforward O
network O

- B-DAT
formation O
through O
layers, O
while O
the O

- B-DAT
lowed O
to O
devote O
most O
of O

- B-DAT

- B-DAT

- B-DAT
sentations O
at O
the O
initial O
iteration O

- B-DAT

- B-DAT

- B-DAT
quent O
iterations O
to O
generate O
better O

- B-DAT
culty. O
For O
example, O
to O
guide O

- B-DAT
works O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trated O
in O
Fig. O
5. O
Each O

- B-DAT

- B-DAT
work O
with O
global O
residual O
skip O

- B-DAT
ering O
the O
residual O
image. O
In O

- B-DAT
put O
image[16] O
and O
to O
predict O

- B-DAT

- B-DAT
servations. O
First, O
compared O
with O
the O

- B-DAT
tions O
in O
contrast O
to O
feedforward O

- B-DAT
cantly O
from O
the O
first O
iteration O

- B-DAT

- B-DAT

- B-DAT
ing O
high-level O
information O
at O
the O

- B-DAT

- B-DAT
back O
network O
will O
urge O
previous O

- B-DAT
tions O
to O
generate O
better O
representations O

- B-DAT
culty. O
For O
example, O
to O
guide O

- B-DAT
gle O
downsampling O
operator O
at O
early O

- B-DAT

- B-DAT
tion O
model. O
The O
results O
shown O

- B-DAT
riculum O
learning O
strategy O
well O
assists O

- B-DAT

- B-DAT
work O
pretrained O
on O
the O
BI O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
son O
results O
are O
given O
in O

- B-DAT

- B-DAT
rameters O
fewer O
than O
1000K. O
This O

- B-DAT
struction O
performance. O
Meanwhile, O
in O
comparison O

- B-DAT
DBPN O
and O
EDSR, O
our O
proposed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods: O
SRCNN[7], O
VDSR[18], O
DRRN[31], O
SRDenseNet[36 O

- B-DAT

- B-DAT

- B-DAT
perform O
almost O
all O
comparative O
methods O

- B-DAT

- B-DAT
ages O
(DIV2K+Flickr2K+ImageNet O
vs. O
DIV2K+Flickr2K). O
However O

- B-DAT
trast O
to O
them. O
In O
addition O

- B-DAT
age O
from O
Manga109, O
DRRN O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
curriculum O
learning O
strategy O
for O

- B-DAT
tion O
models, O
and O
fine-tuned O
based O

- B-DAT
CNN O
C[43], O
SRMD(NF)[44], O
and O
RDN[47 O

- B-DAT

- B-DAT
most O
all O
quantative O
results O
over O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O

- B-DAT

- B-DAT

- B-DAT
mark O
datasets. O
Compared O
with O
other O

- B-DAT
posed O
SRFBN O
could O
alleviate O
the O

- B-DAT
isions, O
we O
further O
indicate O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dation O
models. O
The O
comprehensive O
experimental O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sored O
by O
National O
Natural O
Science O

- B-DAT
tion O
of O
Sichuan O
Science O
and O

- B-DAT

- B-DAT
son O
Weston. O
Curriculum O
learning. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
with O
feedback O
convolutional O
neural O

- B-DAT
tendra O
Malik. O
Human O
pose O
estimation O

- B-DAT

- B-DAT

- B-DAT
works. O
TPAMI, O
2016. O
2, O
7 O

- B-DAT

- B-DAT
down O
influences O
in O
sensory O
processing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mance O
on O
imagenet O
classification. O
In O

- B-DAT
ian O
Q O
Weinberger. O
Densely O
connected O

- B-DAT
works. O
In O
CVPR, O
2016. O
2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rate O
single O
image O
super-resolution O
via O

- B-DAT
tion O
network. O
In O
CVPR, O
2018 O

- B-DAT
tween O
figure O
and O
background O
by O

- B-DAT
ture, O
1998. O
2 O

- B-DAT

- B-DAT
works. O
In O
CVPR, O
2016. O
1 O

- B-DAT
recursive O
convolutional O
network O
for O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tex. O
arXiv O
preprint O
arXiv:1604.03640, O
2016 O

- B-DAT

- B-DAT
dra O
Malik. O
A O
database O
of O

- B-DAT

- B-DAT
timedia O
Tools O
and O
Applications, O
2017 O

- B-DAT

- B-DAT

- B-DAT
back O
for O
crowd O
counting O
convolutional O

- B-DAT

- B-DAT
resolution O
via O
deep O
recursive O
residual O

- B-DAT
net: O
A O
persistent O
memory O
network O

- B-DAT
chored O
neighborhood O
regression O
for O
fast O

- B-DAT

- B-DAT

- B-DAT
resolution. O
In O
ACCV, O
2015. O
1 O

- B-DAT

- B-DAT
tion. O
In O
CVPR, O
2016. O
7 O

- B-DAT

- B-DAT
hanced O
super-resolution O
generative O
adversarial O
networks O

- B-DAT
der O
Sorkinehornung, O
Olga O
Sorkinehornung, O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
back O
networks. O
In O
CVPR, O
2017 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lation O
algorithm O
via O
directional O
filtering O

- B-DAT

- B-DAT

- B-DAT
tion. O
We O
still O
use O
SRFBN-L O

- B-DAT
and O
down-sampling O
layers O
(UDSL), O
(2 O

- B-DAT
and O
down-sampling O
layers O
with O
3 O

- B-DAT
ers O
(with O
one O
padding O
and O

- B-DAT
and O
down-sampling O
operations O
carrying O
large O

- B-DAT
tion O
and O
are O
effective O
for O

- B-DAT

- B-DAT
ter O
adding O
DSC O
to O
the O

- B-DAT
and O
down-sampling O
layers O
(UDSL), O
and O

- B-DAT
sides, O
high-level O
information O
is O
directly O

- B-DAT

- B-DAT

- B-DAT
ization O

- B-DAT
textual O
information O
for O
the O
next O

- B-DAT
parison O
with O
other O
basic O
blocks O

- B-DAT
nism O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tations O
to O
the O
initial O
feature O

- B-DAT

- B-DAT
formation, O
surely O
are O
corrected O
using O

- B-DAT

- B-DAT

- B-DAT
erage O
feature O
map O
at O
each O

- B-DAT

- B-DAT
back) O
and O
SRFBN-L-FF O
(feedforward). O
As O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trum O
map O
through O
discrete O
Fourier O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eration O
t O
grows, O
the O
feedforward O

- B-DAT
ers O
mid-frequency O
and O
high-frequency O
components O

- B-DAT
developed O
information. O
For O
the O
feedback O

- B-DAT
nism O
(t O
>1), O
mid-frequency O
and O

- B-DAT

- B-DAT
tion O
of O
the O
average O
feature O

- B-DAT
ture O
design, O
we O
compare O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
network O
with O
moderate O
parameters O

- B-DAT
cause O
MemNet O
only O
reveals O
the O

- B-DAT

- B-DAT

- B-DAT
tary O
materials. O
Our O
SRFBN-S O
(T=4 O

- B-DAT
son. O
In O
Tab. O
8, O
our O

- B-DAT

- B-DAT
sults O
than O
MemNet O
with O
71 O

- B-DAT

- B-DAT
parison O
shows O
the O
effectiveness O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ban100 O
with O
scale O
factor O
×4 O

- B-DAT

- B-DAT
torch O
for O
fair O
comparison. O
The O

- B-DAT
works O
is O
evaluated O
on O
the O

- B-DAT
tel O
i7 O
CPU O
(16G O
RAM O

- B-DAT
ing O
their O
official O
codes. O
Tab O

- B-DAT

- B-DAT
son O
with O
other O
networks. O
This O

- B-DAT
tiveness O
of O
our O
proposed O
networks O

- B-DAT
volutional O
layers O
with O
77% O
fewer O

-22, B-DAT
we O
provide O
more O
visual O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

DRRN O
[43], O
LapSRN O
[25]) O
on O
Set5 B-DAT
dataset O
for O
4× O
enlargement O

depth O
analysis O
of O
DBPN O
on O
Set5 B-DAT
dataset O
for O
8× O
en- O
largement O

higher O
than O
DBPN-L O
on O
the O
Set5 B-DAT
and O
Set14, O
respectively. O
On O
8 O

higher O
that O
DBPN-L O
on O
the O
Set5 B-DAT
and O
Set14, O
respec- O
tively O

The O
results O
are O
evaluated O
with O
Set5 B-DAT
dataset O
for O
4× O
enlargement O

The O
results O
are O
evaluated O
with O
Set5 B-DAT
dataset O
for O
8× O
enlargement O

Set5 B-DAT
Set14 O

extensive O
experiments O
using O
5 O
datasets: O
Set5 B-DAT
[2], O
Set14 O
[50], O
BSDS100 O
[1 O

dataset O
has O
different O
characteris- O
tics. O
Set5, B-DAT
Set14 O
and O
BSDS100 O
consist O
of O

Set5 B-DAT
Set14 O
BSDS100 O
Urban100 O
Manga109 O

DRRN O
[43], O
LapSRN O
[25]) O
on O
Set5 B-DAT

depth O
analysis O
of O
DBPN O
on O
Set5 B-DAT

higher O
than O
DBPN-L O
on O
the O
Set5 B-DAT

higher O
that O
DBPN-L O
on O
the O
Set5 B-DAT

The O
results O
are O
evaluated O
with O
Set5 B-DAT

The O
results O
are O
evaluated O
with O
Set5 B-DAT

Set5 B-DAT

Set5 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
images. O
We O
propose O

- B-DAT

- B-DAT
and O
down- O
sampling O
layers, O
providing O

- B-DAT
connected O
up- O
and O
down-sampling O
stages O

- B-DAT
resolution O
components. O
We O
show O
that O

- B-DAT
and O
down- O
sampling O
stages O
(Dense O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
proach O
is O
to O
construct O
an O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work O
[6, O
7, O
38, O
25 O

- B-DAT
ing O
with O
one O
or O
more O

- B-DAT
lution O
and O
finally O
construct O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
SRN O
[25] O
(15.25 O
dB), O
EDSR O

- B-DAT
fectively O
by O
one O
of O
the O

- B-DAT

- B-DAT
tion O
error O
then O
fuses O
it O

- B-DAT
fect O
[4]. O
Moreover, O
this O
method O

- B-DAT
erator, O
leading O
to O
variability O
in O

- B-DAT

- B-DAT

- B-DAT
and O
down- O
sampling: O
Deep O
Back-Projection O

- B-DAT
butions: O
(1) O
Error O
feedback. O
We O

- B-DAT
correcting O
feedback O
mechanism O
for O
SR O

- B-DAT
and O
down-projection O
errors O
to O
guide O

- B-DAT
tion O
for O
obtaining O
better O
results O

- B-DAT
and O
down-sampling O
stages. O
Feed-forward O
architectures O

- B-DAT
way O
mapping, O
only O
map O
rich O

- B-DAT

- B-DAT
(blue O
box) O
and O
down-sampling O
(gold O

- B-DAT
tures O
using O
upsampling O
layers O
but O

- B-DAT
(blue O
box) O
and O
down-sampling O
(gold O

- B-DAT
ity O
enables O
the O
networks O
to O

- B-DAT

- B-DAT
tion O
directly O
utilizes O
different O
types O

- B-DAT

- B-DAT

- B-DAT
and O
down-sampling O
stage O
to O
encourage O

- B-DAT

- B-DAT
tion O
as O
the O
upsampling O
operator O

- B-DAT
tion O
(MR) O
image. O
This O
schema O

- B-DAT
CNN O
[6] O
to O
learn O
MR-to-HR O

- B-DAT

- B-DAT
ple O
convolutional O
layers. O
Later, O
the O

- B-DAT
ploited O
residual O
learning O
[22, O
43 O

- B-DAT
posed O
by O
FSRCNN O
[7] O
and O

- B-DAT
tion O
and O
replace O
predefined O
operators O

- B-DAT
longs O
to O
this O
type. O
However O

- B-DAT
portunities O
to O
propose O
lighter O
networks O

- B-DAT

- B-DAT
late O
the O
reconstruction O
error O
to O

- B-DAT
ables O
the O
networks O
to O
preserve O

- B-DAT
ing O
various O
up- O
and O
down-sampling O

- B-DAT
ating O
deeper O
features O

- B-DAT

- B-DAT
to-target O
space O
in O
one O
step O

- B-DAT
pose O
the O
prediction O
process O
into O

- B-DAT

- B-DAT
back O
procedure O
has O
been O
implemented O

- B-DAT
tion. O
PredNet O
[32] O
is O
an O

- B-DAT
tion, O
Li O
et O
al. O
[29 O

- B-DAT
back O
procedures O
have O
not O
been O

- B-DAT
ial O
Networks O
(GANs) O
[10] O
has O

- B-DAT
age O
reconstruction O
problems O
[28, O
37 O

- B-DAT

- B-DAT

- B-DAT
works. O
Ledig O
et O
al. O
[28 O

- B-DAT
ered O
as O
a O
single O
upsampling O

- B-DAT
ral O
image O
manifold O
that O
is O

- B-DAT

- B-DAT
ages O
by O
specifically O
formulating O
a O

- B-DAT

- B-DAT

- B-DAT
erative O
procedure O
to O
minimize O
the O

- B-DAT
projection O
[51, O
11, O
8, O
46 O

- B-DAT

- B-DAT
ror O
iteratively O
[4]. O
Timofte O
et O

- B-DAT
projection O
can O
improve O
the O
quality O

- B-DAT

- B-DAT
known. O
Most O
of O
the O
previous O

- B-DAT

- B-DAT

- B-DAT
and O
down-sampling O
stages O
to O
learn O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
projection O
unit O
projects O
it O
back O

- B-DAT
serve O
the O
HR O
components O
by O

- B-DAT
and O
down- O
sampling O
operators O
and O

- B-DAT
struct O
numerous O
LR O
and O
HR O

- B-DAT

- B-DAT

- B-DAT
end O
training O
of O
the O
SR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
spectively, O
the O
up- O
and O
down-sampling O

- B-DAT

- B-DAT

- B-DAT
and O
down-projection O
unit O
in O
the O

- B-DAT
nating O
between O
H O
and O
L O

- B-DAT
derstood O
as O
a O
self-correcting O
procedure O

- B-DAT
jection O
error O
to O
the O
sampling O

- B-DAT
sized O
filter O
is O
avoided O
because O

- B-DAT
gence O
speed O
and O
might O
produce O

- B-DAT

- B-DAT
ever, O
iterative O
utilization O
of O
our O

- B-DAT
works O

- B-DAT

- B-DAT
gradient O
problem, O
produce O
improved O
feature O

- B-DAT
age O
feature O
reuse. O
Inspired O
by O

- B-DAT

- B-DAT
mensional O
reduction O
[42, O
12] O
before O

- B-DAT

- B-DAT
and O
down-projection O
unit, O
re- O
spectively O

- B-DAT
ture O
maps O
effectively, O
as O
shown O

- B-DAT

- B-DAT
jection, O
and O
reconstruction, O
as O
described O

- B-DAT
and O
down-projection O
unit O
in O
the O

- B-DAT

- B-DAT
and O
down-projections O
units, O
respectively) O
are O

- B-DAT

- B-DAT
tures O
extraction O
and O
nR O
is O

- B-DAT

- B-DAT
traction O
is O
a O
sequence O
of O

- B-DAT

- B-DAT

- B-DAT
work O
architecture O
is O
modular. O
We O

- B-DAT
traction O
stage O
(2 O
layers), O
and O

- B-DAT

- B-DAT

- B-DAT
tion O
unit O
is O
various O
with O

- B-DAT
nally, O
the O
8× O
enlargement O
use O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puted O
by O

- B-DAT
volutional O
layers O
are O
followed O
by O

- B-DAT
tion.2 O
To O
produce O
LR O
images O

- B-DAT
ing O
Caffe, O
MATLAB O
R2017a O
on O

- B-DAT
tion. O
The O
input O
and O
output O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formance, O
S O
networks O
can O
achieve O

- B-DAT
SRN, O
respectively. O
The O
M O
network O

- B-DAT

- B-DAT

- B-DAT
tal, O
the O
M O
network O
use O

- B-DAT
SRN, O
and O
DRRN, O
respectively O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
largement. O
S O
(T O
= O
2 O

- B-DAT

- B-DAT
rameters O
on O
4× O
and O
8 O

- B-DAT
DBPN O
has O
about O
76% O
fewer O

- B-DAT

- B-DAT
dence O
show O
that O
our O
networks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tures O
generated O
from O
the O
projection O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tively O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pare O
our O
network O
with O
eight O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rithms: O
A+ O
[45], O
SRCNN O
[6 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tics. O
Set5, O
Set14 O
and O
BSDS100 O

- B-DAT

- B-DAT
vide O
each O
image O
in O
Urban100 O

- B-DAT

- B-DAT

- B-DAT
put O
image. O
It O
takes O
less O

- B-DAT
DBPN O
outperforms O
the O
existing O
methods O

- B-DAT

- B-DAT
vious O
statement O
is O
strengthened O
by O

- B-DAT
ban100 O
dataset O
which O
consist O
of O

- B-DAT

- B-DAT
ods O
by O
a O
large O
margin O

- B-DAT

- B-DAT

- B-DAT
ter O
than O
EDSR. O
The O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
which O
predict O
the O
SR O

- B-DAT

- B-DAT
tures O
using O
multiple O
up- O
and O

- B-DAT

- B-DAT

- B-DAT
and O
down-scaling O
steps O
to O
guide O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work O
successfully O
outperforms O
other O
state-of-the-art O

- B-DAT
ods O
on O
large O
scaling O
factors O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
Conference O
(BMVC), O
2012. O
6 O

- B-DAT
man O
pose O
estimation O
with O
iterative O

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT
projection O
for O
single O
image O
super O

- B-DAT
tive O
image O
models O
using O
a O

- B-DAT
tems, O
pages O
1486–1494, O
2015. O
1 O

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
ference O
on O
Computer O
Vision, O
pages O

- B-DAT
projection O
for O
adaptive O
image O
enlargement O

- B-DAT
cessing O
(ICIP), O
2009 O
16th O
IEEE O

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
nets. O
In O
Advances O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
for O
image O
recognition. O
arXiv O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision, O
pages O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
sion O
and O
Pattern O
Recognition O
(CVPR O

- B-DAT
istration. O
CVGIP: O
Graphical O
models O
and O

- B-DAT
ment: O
Resolution, O
occlusion, O
and O
transparency O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT
volutional O
network O
for O
image O
super-resolution O

- B-DAT
ings O
of O
the O
IEEE O
Conference O

- B-DAT
resolution. O
In O
IEEE O
Conferene O
on O

- B-DAT
tern O
Recognition, O
2017. O
1, O
2 O

- B-DAT
sion O
offered O
by O
feedforward O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
adversarial O
network. O
In O
IEEE O

- B-DAT
tation. O
In O
Proceedings O
of O
the O

- B-DAT
resolution O
via O
deep O
draft-ensemble O
learning O

- B-DAT

- B-DAT
ing O
networks O
for O
video O
prediction O

- B-DAT
masaki, O
and O
K. O
Aizawa. O
Sketch-based O

- B-DAT
ing O
manga109 O
dataset. O
Multimedia O
Tools O

- B-DAT
sentation O
learning O
with O
deep O
convolutional O

- B-DAT
sarial O
networks. O
arXiv O
preprint O
arXiv:1511.06434 O

- B-DAT

- B-DAT
tion. O
In O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT
back O
for O
faster O
r-cnn. O
In O

- B-DAT
tion, O
2017. O
1 O

- B-DAT

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT
nition O
Workshops O
(CVPRW), O
2017 O
IEEE O

- B-DAT

- B-DAT
prove O
example-based O
single O
image O
super O

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT
level O
vision O
tasks O
and O
3d O

- B-DAT
tural O
similarity. O
Image O
Processing, O
IEEE O

- B-DAT

- B-DAT

- B-DAT
erative O
projection O
reconstruction O
for O
fast O

on O
four O
standard O
benchmark O
datasets: O
Set5 B-DAT
[2], O
Set14 O
[33], O
B100 O
[17 O

0.9606 O
38.17 O
/ O
0.9605 O
Set5 B-DAT
×3 O
30.39 O
/ O
0.8682 O
32.58 O

reconstruct O
high-resolution O
images O
of O
different O
upscaling B-DAT
factors O
in O
a O
single O
model O

demonstrated O
in O
Fig. O
4. O
For O
upscaling B-DAT
×4, O
if O
we O
use O
a O

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

- B-DAT

- B-DAT

- B-DAT
hanced O
deep O
super-resolution O
network O
(EDSR O

- B-DAT
mance O
exceeding O
those O
of O
current O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods. O
The O
significant O
performance O
improvement O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
on O
benchmark O
datasets O
and O

- B-DAT
ning O
the O
NTIRE2017 O
Super-Resolution O
Challenge O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nificantly O
improved O
performance O
in O
terms O

- B-DAT

- B-DAT
noise O
ratio O
(PSNR) O
in O
the O

- B-DAT
works O
exhibit O
limitations O
in O
terms O

- B-DAT

- B-DAT

- B-DAT
resolution O
of O
different O
scale O
factors O

- B-DAT
lems O
without O
considering O
and O
utilizing O

- B-DAT
quire O
many O
scale-specific O
networks O
that O

- B-DAT

- B-DAT

- B-DAT
dancy O
among O
scale-specific O
models. O
Nonetheless O

- B-DAT

- B-DAT
pling O
method O
[5, O
22, O
14 O

- B-DAT
ploys O
the O
ResNet O
architecture O
from O

- B-DAT
posed O
to O
solve O
higher-level O
computer O

- B-DAT

- B-DAT

- B-DAT
chitecture, O
we O
first O
optimize O
it O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els. O
Furthermore, O
we O
propose O
a O

- B-DAT

- B-DAT

- B-DAT
rameters O
compared O
with O
multiple O
single-scale O

- B-DAT
and O
multi-scale O
super-resolution O
networks O
show O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dicting O
detailed, O
realistic O
textures. O
Previous O

- B-DAT
struct O
better O
high-resolution O
images O

- B-DAT
tween O
ILR O
and O
IHR O
image O

- B-DAT
ods O
rely O
on O
techniques O
ranging O

- B-DAT
ding O
[3, O
2, O
7, O
21 O

- B-DAT
proaches O
utilize O
image O
self-similarities O
to O

- B-DAT
nal O
databases O
[8, O
6, O
29 O

- B-DAT
works O
has O
led O
to O
dramatic O

- B-DAT

- B-DAT
ing O
much O
deeper O
network O
architectures O

- B-DAT
perior O
performance. O
In O
particular, O
they O

- B-DAT
connection O
and O
recursive O
convolution O
alleviate O

- B-DAT

- B-DAT
work. O
Similarly O
to O
[20], O
Mao O

- B-DAT

- B-DAT

- B-DAT
rithms, O
an O
input O
image O
is O

- B-DAT
lation O
before O
they O
fed O
into O

- B-DAT
sampling O
modules O
at O
the O
very O

- B-DAT
sible O
as O
shown O
in O
[5 O

- B-DAT
cause O
the O
size O
of O
features O

- B-DAT

- B-DAT
scale O
training O
and O
computational O
efficiency O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
thermore, O
we O
develop O
an O
appropriate O

- B-DAT
and O
multi-scale O
mod- O
els O

- B-DAT

- B-DAT
hibiting O
improved O
computational O
efficiency. O
In O

- B-DAT
ing O
sections, O
we O
suggest O
a O

- B-DAT

- B-DAT

- B-DAT
scale O
architecture O
(MDSR) O
that O
reconstructs O

- B-DAT

- B-DAT
level O
to O
high-level O
tasks. O
Although O

- B-DAT
fully O
applied O
the O
ResNet O
architecture O

- B-DAT

- B-DAT
mance O
by O
employing O
better O
ResNet O

- B-DAT
work O
model O
from O
original O
ResNet O

- B-DAT
tion O
increases O
the O
performance O
substantially O

- B-DAT
duced O
since O
the O
batch O
normalization O

- B-DAT

- B-DAT

- B-DAT
work O
model O
is O
to O
increase O

- B-DAT
nels) O
F O
occupies O
roughly O
O(BF O

- B-DAT
imize O
the O
model O
capacity O
when O

- B-DAT
tational O
resources O

- B-DAT
cedure O
numerically O
unstable. O
A O
similar O

- B-DAT
ing O
procedure O
greatly O
when O
using O

- B-DAT
ous O
convolution O
layer O
for O
the O

- B-DAT

- B-DAT
vation O
layers O
outside O
the O
residual O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
at O
multiple O
scales O
is O

- B-DAT

- B-DAT
ther O
explore O
this O
idea O
by O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
specific O
processing O
modules O
to O
handle O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ule O
consists O
of O
two O
residual O

- B-DAT

- B-DAT

- B-DAT
tive O
field O
is O
covered O
in O

- B-DAT

- B-DAT

- B-DAT
ules O
are O
located O
in O
parallel O

- B-DAT

- B-DAT
tion. O
The O
architecture O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els O
for O
3 O
different O
scales O

- B-DAT

- B-DAT

- B-DAT
hibits O
comparable O
performance O
as O
the O

- B-DAT

- B-DAT

- B-DAT

Residual O
scaling O
- B-DAT
- O
0.1 O

- B-DAT

- B-DAT
ual O
blocks O
are O
lighter O
than O

- B-DAT

- B-DAT
specific O
EDSRs. O
The O
detailed O
performance O

- B-DAT

- B-DAT
formances O
on O
the O
validation O
dataset O

- B-DAT

- B-DAT
ing O
the O
mean O
RGB O
value O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works O
as O
described O
in O
Sec O

- B-DAT
trained O
network O
for O
other O
scales O

- B-DAT

- B-DAT
specific O
residual O
blocks O
and O
upsampling O

- B-DAT
spond O
to O
different O
scales O
other O

- B-DAT
imizing O
L2 O
is O
generally O
preferred O

- B-DAT
pirically O
found O
that O
L1 O
loss O

- B-DAT
spectively. O
The O
source O
code O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
super-resolved O
images O

- B-DAT

- B-DAT

- B-DAT
rate O
models. O
It O
is O
beneficial O

- B-DAT

- B-DAT
dividually O
trained O
models. O
We O
denote O

- B-DAT

- B-DAT

- B-DAT
inal O
one O
trained O
with O
L2 O

- B-DAT

- B-DAT
els O
require O
much O
less O
GPU O

- B-DAT
sults O
in O
an O
individual O
experiment O

- B-DAT
per O
[14]. O
In O
our O
experiments O

- B-DAT

0.9542 O
37.53 O
/ O
0.9587 O
- B-DAT
/ O
- O
38.11 O
/ O
0.9601 O

0.9090 O
33.66 O
/ O
0.9213 O
- B-DAT
/ O
- O
34.65 O
/ O
0.9282 O

0.9063 O
33.03 O
/ O
0.9124 O
- B-DAT
/ O
- O
33.92 O
/ O
0.9195 O

0.8209 O
29.77 O
/ O
0.8314 O
- B-DAT
/ O
- O
30.52 O
/ O
0.8462 O

0.8879 O
31.90 O
/ O
0.8960 O
- B-DAT
/ O
- O
32.32 O
/ O
0.9013 O

0.7863 O
28.82 O
/ O
0.7976 O
- B-DAT
/ O
- O
29.25 O
/ O
0.8093 O

0.8946 O
30.76 O
/ O
0.9140 O
- B-DAT
/ O
- O
32.93 O
/ O
0.9351 O

0.7989 O
27.14 O
/ O
0.8279 O
- B-DAT
/ O
- O
28.80 O
/ O
0.8653 O

0.9581 O
33.66 O
/ O
0.9625 O
- B-DAT
/ O
- O
35.03 O
/ O
0.9695 O

0.9138 O
30.09 O
/ O
0.9208 O
- B-DAT
/ O
- O
31.26 O
/ O
0.9340 O

0.8753 O
28.17 O
/ O
0.8841 O
- B-DAT
/ O
- O
29.25 O
/ O
0.9017 O

- B-DAT

- B-DAT
vided O
in O
the O
last O
two O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sure O
PSNR O
and O
SSIM O
on O

- B-DAT
LAB O
[18] O
functions O
for O
evaluation O

- B-DAT
nificant O
improvement O
compared O
to O
the O

- B-DAT

- B-DAT

- B-DAT
puts O
compared O
with O
the O
previous O

- B-DAT
ticipating O
in O
the O
NTIRE2017 O
Super-Resolution O

- B-DAT
resolution O
system O
with O
the O
highest O

- B-DAT
graders O
(bicubic, O
unknown) O
with O
three O

- B-DAT
ditions. O
Some O
results O
of O
our O

- B-DAT
ods O
successfully O
reconstruct O
high-resolution O
images O

- B-DAT

- B-DAT
ventional O
ResNet O
architecture, O
we O
achieve O

- B-DAT
ual O
scaling O
techniques O
to O
stably O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT

- B-DAT

- B-DAT
age O
Processing, O
21(7):3194–3205, O
2012. O
2 O

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT

- B-DAT
mization. O
In O
ICLR O
2014. O
5 O

- B-DAT

- B-DAT

- B-DAT
ative O
adversarial O
network. O
arXiv:1609.04802, O
2016 O

- B-DAT

- B-DAT
ing O
very O
deep O
convolutional O
encoder-decoder O

- B-DAT
cal O
statistics. O
In O
ICCV O
2001 O

- B-DAT

- B-DAT

- B-DAT
tional O
networks O
for O
biomedical O
image O

- B-DAT
CAI O
2015. O
2 O

- B-DAT
tion O
by O
locally O
linear O
embedding O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT
v4, O
inception-resnet O
and O
the O
impact O

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
actions O
on O
Image O
Processing, O
21(8):3467–3478 O

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT

- B-DAT

- B-DAT
tional O
Conference O
on O
Curves O
and O

- B-DAT

- B-DAT
gorithm O
via O
directional O
filtering O
and O

- B-DAT
actions O
on O
Image O
Processing, O
15(8):2226–2238 O

scale) O
VDSR O
DnCNN-3 O
Proposed-P O
Proposed O
Set5 B-DAT
(2) O
37.53/0.9586 O
37.53/0.9582 O
37.57/0.9586 O
38.06/0.9602 O

Set5 B-DAT
(3) O
33.66/0.9213 O
33.73/0.9212 O
33.86/0.9228 O
34.45/0.9272 O

Set5 B-DAT
(4) O
31.35/0.8838 O
31.40/0.8837 O
31.52/0.8864 O
32.23/0.8952 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ous O
image O
restoration O
tasks. O
However O

- B-DAT
pose O
a O
novel O
feature O
space O

- B-DAT
rithm O
that O
outperforms O
the O
existing O

- B-DAT
mance O
of O
a O
learning O
algorithm O

- B-DAT

- B-DAT

- B-DAT
strate O
that O
the O
proposed O
feature O

- B-DAT
performs O
the O
existing O
state-of-the-art O
approaches O

- B-DAT
over, O
our O
algorithm O
was O
ranked O

-10 B-DAT
times O
faster O
computational O
time O
compared O

- B-DAT
cessing O
applications. O
Over O
the O
last O

- B-DAT

- B-DAT

- B-DAT
proaches O
[22], O
and O
sparse O
dictionary O

- B-DAT

- B-DAT

- B-DAT
over, O
these O
algorithms O
are O
usually O

- B-DAT
ative O
manner, O
so O
they O
require O

- B-DAT
sources O

- B-DAT
level O
computer O
vision O
problems O
[24 O

- B-DAT
ing O
and O
super-resolution O
tasks, O
many O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
itation O
of O
the O
state-of-the-art O
CNN O

- B-DAT
posed O
network O
architectures O
are O
motivated O

- B-DAT
sistent O
homology O
analysis O
[11] O
on O

- B-DAT
age O
processing O
tasks. O
Specifically, O
we O

- B-DAT
ual O
manifold O
is O
topologically O
simpler O

- B-DAT
age O
manifold, O
which O
may O
have O

- B-DAT
tion. O
Specifically, O
our O
design O
goal O

- B-DAT
tures O
while O
preserving O
the O
directional O

- B-DAT
lowing. O
First, O
a O
novel O
network O

- B-DAT
fold O
simplification O
is O
proposed. O
Second O

- B-DAT
putational O
topology O
tool O
called O
the O

- B-DAT
form O
to O
simplify O
topological O
structures O

COPY O
ch(LR) O
Label O
Input O
- B-DAT
WT(HR) O
Input O
- O
PS(HR O

Long O
bypass O
layer O
LongBypass(2) O
- B-DAT
- O
Repeat O
1st O
module O
6 O
times O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vanced O
algorithms O
in O
this O
field O

- B-DAT
and O
inter- O
correlations O
of O
the O

- B-DAT
mance O
to O
BM3D O
using O
multi-layer O

- B-DAT
able O
nonlinear O
reaction O
diffusion O
(TNRD O

- B-DAT
ters O
and O
influence O
functions O
by O

- B-DAT
timization O
approach. O
Recently, O
based O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
restoration O
problems O
[21 O

- B-DAT
proach O
is O
using O
a O
skipped O

- B-DAT
cept O
was O
first O
introduced O
by O

- B-DAT
tion. O
In O
low-level O
computer O
vision O

- B-DAT

- B-DAT
ping. O
In O
another O
implementation, O
the O

- B-DAT
formed O
into O
the O
difference O
between O

- B-DAT
lem: O
minf∈F O
L(f), O
where O
L(f O

- B-DAT
notes O
the O
risk. O
A O
major O

- B-DAT
formance. O
Specifically, O
with O
probability O

- B-DAT
pared O
to O
shallow O
ones. O
However O

- B-DAT

- B-DAT
ity O
penalty O
reduces O
much O
more O

- B-DAT
mization. O
One O
of O
the O
most O

- B-DAT
work, O
by O
reducing O
the O
complexity O

- B-DAT
ture O
spaces O
for O
the O
input O

- B-DAT
form O
is O
given O
by O
Y O

- B-DAT
sults O
in O
the O
dimensional O
reduction O

- B-DAT
tion. O
Indeed, O
this O
property O
of O

- B-DAT

- B-DAT
gebraic O
topology, O
Betti O
numbers O
(βm O

- B-DAT
ber O
of O
m-dimensional O
holes O
of O

- B-DAT
ing O
the O
changes O
of O
Betti O

- B-DAT
come O
a O
single O
cluster O
(Fig O

- B-DAT
resented O
as O
a O
slow O
decrease O

- B-DAT

- B-DAT
uration O
over O
� O
distance O
filtration O

- B-DAT
ture O
and O
the O
recent O
persistent O

- B-DAT

- B-DAT

- B-DAT
lationship O
between O
these O
newly O
processed O

- B-DAT
sian O
denoising, O
40 O
× O
40 O

- B-DAT
ing O
because O
it O
is O
helpful O

- B-DAT
leviating O
the O
gradient O
vanishing O
problem O

- B-DAT
malization, O
and O
ReLU O
and O
the O

- B-DAT
ing O
the O
convolution, O
we O
used O

- B-DAT
ceptive O
field O
can O
be O
reduced O

- B-DAT
mary O
denoising O
architecture. O
Depending O
on O

- B-DAT
tended O
denoising O
network O
structures O
with O

- B-DAT
form O
were O
used O
for O
manifold O

- B-DAT
known O
decimation O
scheme, O
however, O
we O

- B-DAT
pixel O
shuffling O
scheme O
[25] O
as O

- B-DAT

- B-DAT
fling O
transform O
does O
not O
reduce O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
task. O
The O
training O
step O

- B-DAT
ter O
the O
first O
two O
layers O

- B-DAT
construct O
the O
bicubic O
x2 O
downsampled O

- B-DAT
nection O
allows O
faster O
computation O
and O

- B-DAT
pass O
connection O

- B-DAT
minance O
channel, O
because O
RGB O
based O

- B-DAT
fect O
of O
data O
augmentation O

- B-DAT
able O
Berkeley O
segmentation O
(BSD500) O
[4 O

- B-DAT
ing O
task. O
In O
addition, O
we O

- B-DAT
fitting, O
we O
re-generated O
the O
Gaussian O

- B-DAT
ing O
and O
validation, O
Gaussian O
noises O

- B-DAT
tion O
using O
image O
flipping, O
rotation O

- B-DAT
tialized O
using O
the O
Xavier O
method O

- B-DAT
sion O
loss O
across O
four O
wavelet O

- B-DAT

- B-DAT
ble O
learning, O
we O
employed O
the O

- B-DAT
date O
parameter O
are O
bounded O
by O

- B-DAT
box O
(beta.20) O
[29] O
in O
MATLAB O

- B-DAT
Works, O
Natick). O
We O
used O
a O

-4770 B-DAT
CPU O
(3.40GHz). O
The O
Gaussian O
denoising O

- B-DAT
work O
took O
about O
two O
days O

- B-DAT

- B-DAT
epoch O
system O
that O
repeats O
forward O

- B-DAT
work O
with O
the O
bicubic O
x2 O

- B-DAT

- B-DAT

- B-DAT
mance O
and O
manifold O
simplification, O
we O

- B-DAT
ogy O
of O
the O
input O
and O

- B-DAT

- B-DAT
noising O
performance, O
we O
used O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
noising O
methods O
in O
terms O
of O

- B-DAT
bara O
and O
House, O
we O
attained O

- B-DAT
posed O
method O
showed O
superior O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
composition, O
additional O
comparative O
studies O
with O

- B-DAT
line O
network O
were O
performed. O
Here O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fling O
scheme O
in O
[25] O
except O

- B-DAT
polated. O
Accordingly, O
the O
networks O
using O

- B-DAT
actly O
same O
architecture O
except O
the O

- B-DAT
plification. O
Here, O
the O
Gaussian O
denoising O

- B-DAT
bara O
image O
was O
used O
for O

- B-DAT

-3 B-DAT
Proposed-P O
Proposed O
Set5 O
(2) O
37.53/0.9586 O

-3, B-DAT
Proposed-P(primary) O
networks O
are O
291 O
dataset[18 O

- B-DAT

- B-DAT
stored O
RGB O
was O
used O
to O

- B-DAT
ues O

- B-DAT
lar, O
our O
networks O
were O
competitive O

-67 B-DAT
seconds O
computational O
time O
by O
the O

-5 B-DAT
seconds O
for O
each O
frame. O
Since O

- B-DAT

- B-DAT
known O
decimation O
dataset O
where O
we O

- B-DAT
ifold O
simplification O
from O
the O
residual O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eas. O
We O
provide O
more O
comparative O

- B-DAT
mentary O
material O

- B-DAT
ogy O
analysis, O
we O
showed O
that O

- B-DAT
ing O
as O
well O
as O
the O

- B-DAT
ing O
and O
NTIRE O
SISR O
competition O

- B-DAT
mance O
and O
speed. O
Moreover, O
we O

- B-DAT

- B-DAT
ing O
Foundation, O
Grant O
number O
NRF-2013M3A9B2076548 O

- B-DAT
puter O
Vision, O
76(2):123–139, O
2008. O
1 O

- B-DAT
sion O
and O
Pattern O
Recognition O
(CVPR O

- B-DAT
mized O
reaction O
diffusion O
processes O
for O

- B-DAT

- B-DAT
den O
Markov O
models. O
IEEE O
Transactions O

- B-DAT

- B-DAT

- B-DAT
laborative O
filtering. O
IEEE O
Transactions O
on O

- B-DAT
cessing, O
16(8):2080–2095, O
2007. O
1 O

- B-DAT
ume O
61. O
SIAM, O
1992. O
3 O

- B-DAT
tralized O
sparse O
representation O
for O
image O

- B-DAT

- B-DAT

- B-DAT
a O
survey. O
Contemporary O
Mathematics, O
453:257–282 O

- B-DAT
ual O
learning O
for O
image O
recognition O

- B-DAT

- B-DAT
tional O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT

- B-DAT
tern O
Recognition O
(CVPR), O
pages O
5197–5206 O

- B-DAT
erating O
deep O
network O
training O
by O

- B-DAT

- B-DAT
works. O
arXiv O
preprint O
arXiv:1511.04587, O
2015 O

- B-DAT
agenet O
classification O
with O
deep O
convolutional O

- B-DAT
ing O
Systems, O
pages O
1097–1105, O
2012 O

- B-DAT
demic O
press, O
1999. O
3 O

- B-DAT

- B-DAT
based O
image O
restoration. O
Multiscale O
Modeling O

- B-DAT
ulation, O
4(2):460–489, O
2005. O
1 O

- B-DAT
moncelli. O
Image O
denoising O
using O
scale O

- B-DAT

- B-DAT
volutional O
networks O
for O
biomedical O
image O

- B-DAT
tion. O
In O
International O
Conference O
on O

- B-DAT
age O
Computing O
and O
Computer-Assisted O
Intervention O

- B-DAT

- B-DAT
gle O
image O
and O
video O
super-resolution O

- B-DAT

- B-DAT
tion O
(CVPR), O
pages O
1874–1883, O
2016 O

- B-DAT
ory, O
volume O
1. O
Wiley O
New O

- B-DAT

-1995 B-DAT

-001 B-DAT

- B-DAT
Hall, O
1995. O
7 O

- B-DAT
celli. O
Image O
quality O
assessment: O
from O

- B-DAT

- B-DAT
ror O
as O
the O
loss, O
the O

- B-DAT

- B-DAT
variant O
under O
batch O
normalization O
is O

- B-DAT

- B-DAT
culated O
the O
barcodes O
of O
the O

- B-DAT
box O
called O
JAVAPLEX O
(http://appliedtopology.github.io/ O
javaplex O

- B-DAT
posed O
of O
residual O
image O
patches O

- B-DAT
ical O
complexity O
in O
the O
image O

- B-DAT
thogonal O
Haar O
wavelet O
transform. O
The O

- B-DAT
form O
which O
further O
reduces O
the O

- B-DAT
resolution O
datasets O
(the O
right O
column O

- B-DAT
codes, O
the O
input O
manifold O
of O

- B-DAT
ages O
had O
simpler O
topology O
than O

- B-DAT
noising O
, O
(b) O
super-resolution O
(bicubic O

- B-DAT
resolution O
(unknown O
decimation) O
tasks. O
(Right O

- B-DAT
ogy O
analysis O
of O
input O
manifold O

- B-DAT

- B-DAT

- B-DAT
tion) O
tasks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ditional O
simpler O
label O
manifold O
from O

- B-DAT
noising O
and O
SISR O
reconstruction O
from O

use O
four O
benchmark O
datasets, O
including O
Set5 B-DAT
[5], O
Set14 O
[29], O
BSD100 O
[20 O

Scale O
Method O
# O
params O
Set5 B-DAT
Set14 O
BSD100 O
Urban100 O
PSNR O

model O
eval- O
uated O
on O
the O
Set5 B-DAT
[5], O
Set14 O
[29], O
BSD100 O
[20 O

Scale O
Method O
# O
params O
Set5 B-DAT

as O
nearest-neighbor, O
bilinear, O
and O
bicubic O
upscaling B-DAT

the O
proposed O
methods O
for O
an O
upscaling B-DAT
factor O
of O
2 O
on O
the O

the O
com- O
putational O
complexity O
than O
upscaling B-DAT
at O
the O
initial O
stage O
[3,6,12 O

a O
recur- O
sive O
manner, O
and O
upscaling B-DAT

image O
is O
obtained O
from O
the O
upscaling B-DAT
module O

initial O
feature O
extraction, O
RRB, O
and O
upscaling B-DAT
parts. O
On O
the O
other O
hand O

status O
are O
inputted O
to O
the O
upscaling B-DAT
part. O
We O
investigate O
the O
effectiveness O

23]. O
For O
instance, O
in O
the O
upscaling B-DAT
part O
by O
a O
factor O
of O

is O
not O
used O
in O
the O
upscaling B-DAT
part O

the O
frequency O
of O
the O
progressive O
upscaling B-DAT

of O
times O
to O
employ O
the O
upscaling B-DAT
part, O
it O
is O
beneficial O
to O

BSRN O
model, O
one O
of O
the O
upscaling B-DAT
paths O
(i.e., O
×2, O
×3, O
and O

single-scale O
BSRN O
models O
having O
an O
upscaling B-DAT
factor O
of O
4, O
which O
are O

on O
the O
latter O
part O
(i.e., O
upscaling B-DAT
part) O
to O
generate O
good O
quality O

i.e., O
how O
many O
times O
the O
upscaling B-DAT
part O
is O
employed), O
which O
is O

In O
our O
proposed O
model, O
the O
upscaling B-DAT
part O
spends O
most O
of O
the O

average O
processing O
time O
spent O
on O
upscaling B-DAT
an O
image O
by O
a O
factor O

super-resolved O
image O
with O
the O
given O
upscaling B-DAT
factor O
for O
each O
method O
is O

Deep O
residual O
network O
with O
enhanced O
upscaling B-DAT
module O
for O
super-resolution. O
In: O
Proceedings O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
plexities, O
thus O
some O
recursive O
parameter-sharing O

- B-DAT

- B-DAT

- B-DAT
work. O
By O
taking O
advantage O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
field. O
For O
example, O
Dong O

- B-DAT

- B-DAT
volutional O
neural O
network O
(SRCNN) O
model O

- B-DAT
formance O
in O
comparison O
to O
the O

- B-DAT

- B-DAT
nections O
and O
various O
optimization O
techniques O

- B-DAT

- B-DAT

- B-DAT
matically O
increases O
the O
number O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
hinder O
them O
from O
fully O

- B-DAT
vided O
to O
the O
recurrent O
unit O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
block O
state-based O
recursive O
network O

- B-DAT

- B-DAT
arate O
information O
storage O
to O
keep O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
the O
BSRN O
model O
can O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
CNN, O
which O
enhances O
the O
interpolated O

- B-DAT

- B-DAT
lutional O
layers O
to O
improve O
the O

- B-DAT

- B-DAT
tion O
layer O
for O
16 O
times O

- B-DAT

- B-DAT
processing O
part O

- B-DAT

- B-DAT
ple, O
Lai O
et O
al. O
[18 O

- B-DAT

- B-DAT

- B-DAT

C O
- B-DAT

C O
- B-DAT

C O
- B-DAT

C O
- B-DAT

- B-DAT

- B-DAT

- B-DAT
putational O
complexity O
than O
upscaling O
at O

- B-DAT
ploying O
multiple O
residual O
connections O
is O

- B-DAT
ages O
[15,24]. O
Third, O
obtaining O
multiple O

- B-DAT
resolution O
model O
and O
combining O
them O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vided O
into O
three O
parts: O
initial O

- B-DAT
sive O
manner, O
and O
upscaling. O
Fig O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ther O
processed O
via O
a O
recursive O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nates O
two O
input O
matrices O
along O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mance: O
increasing O
the O
number O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put, O
where O
the O
later O
outputs O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
by-pixel O
L1 O
loss, O
i.e O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els. O
The O
single-scale O
models O
are O

- B-DAT

- B-DAT

- B-DAT
ods O
across O
different O
scales. O
The O

- B-DAT

- B-DAT
domly O
cropped O
from O
the O
training O

- B-DAT

- B-DAT
scale O
BSRN O
model. O
For O
data O

- B-DAT

- B-DAT
resolved O
images O
are O
obtained O
from O

- B-DAT
eters. O
To O
prevent O
the O
vanishing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bers O
of O
the O
convolutional O
channels O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
termediate O
features O
are O
largely O
different O

- B-DAT
cally O
change, O
even O
though O
the O

- B-DAT

- B-DAT
gressively O
improved O
features O
and O
highly O

- B-DAT
ploying O
the O
block O
state O
(Fig O

- B-DAT

- B-DAT
space O
operation O
and O
increased O
spatial O

- B-DAT

- B-DAT

- B-DAT
ation. O
To O
verify O
this, O
we O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uated O
on O
the O
Set5 O
[5 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sharing O
parts. O
The O
VDSR, O
LapSRN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ber O
of O
model O
parameters O
required O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ters O
small O
enough O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ample, O
our O
method O
successfully O
upscales O

- B-DAT
resolved O
images O

- B-DAT

- B-DAT
plained O
the O
benefits O
and O
efficiency O

- B-DAT
dition, O
comparison O
with O
the O
other O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mawat, O
S., O
Irving, O
G., O
Isard O

- B-DAT

- B-DAT
chine O
learning. O
In: O
Proceedings O
of O

- B-DAT
resolution: O
Dataset O
and O
study. O
In O

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
British O
Machine O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
via O
dual-state O
recurrent O
networks O

- B-DAT
ence O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks. O
In: O
Proceedings O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
puter O
Vision. O
pp. O
416–423 O
(2001 O

- B-DAT

- B-DAT

- B-DAT
ings O
of O
the O
IEEE O
International O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work. O
In: O
Proceedings O
of O
the O

- B-DAT
ing O
13(4), O
600–612 O
(2004 O

- B-DAT

- B-DAT

- B-DAT
computing O
74(17), O
3193–3203 O
(2011 O

- B-DAT

- B-DAT
representations. O
In: O
Proceedings O
of O
the O

- B-DAT

- B-DAT

standard O
benchmark O
datasets O
such O
as O
Set5 B-DAT
[3], O
Set14 O
[39], O
B100 O
[29 O

Scale O
Model O
Params O
MultAdds O
Set5 B-DAT
Set14 O
B100 O
Urban100 O

Scale O
Model O
Params O
MultAdds O
Set5 B-DAT

- B-DAT

- B-DAT

- B-DAT
fully O
applied O
to O
single-image O
super-resolution O

- B-DAT
world O
applications O
due O
to O
the O

- B-DAT

- B-DAT
ture O
that O
implements O
a O
cascading O

- B-DAT
work O
to O
further O
improve O
efficiency O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lution O
limitations, O
and O
could O
be O

- B-DAT

- B-DAT

- B-DAT
vided O
outstanding O
performance O
in O
SISR O

- B-DAT

- B-DAT
tical O
for O
real-world O
applications. O
One O

- B-DAT

- B-DAT

- B-DAT
ple, O
DRCN O
[21] O
uses O
a O

- B-DAT
els O
decrease O
the O
number O
of O

- B-DAT
ducing O
the O
number O
of O
parameters O

- B-DAT

- B-DAT
sider O
a O
situation O
where O
an O

- B-DAT

- B-DAT
lenging O
and O
necessary O
step O
that O

- B-DAT
mand O
for O
streaming O
media O
has O

- B-DAT
ing O
lossy O
compression O
techniques O
before O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
M). O
We O
first O
build O
our O

- B-DAT

- B-DAT
ing O
the O
FSRCNN O
[7], O
CARN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
approaches O
have O
been O
applied O

- B-DAT
based O
SISR O
in O
section O
2.1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rameters O
by O
engaging O
in O
redundant O

- B-DAT

(- B-DAT

- B-DAT

(- B-DAT

- B-DAT

- B-DAT
erations O
in O
(a) O
and O
(b O

- B-DAT

- B-DAT
ter O
category, O
SqueezeNet O
[19] O
builds O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ber O
of O
operations O
compared O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ary O
layers O
are O
cascaded O
into O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ual O
block, O
the O
first O
residual O

- B-DAT
rameter O
of O
the O
convolution O
layer O

- B-DAT
trated O
in O
block O
(c) O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
anism. O
As O
shown O
in O
Fig O

- B-DAT
cading O
on O
both O
the O
local O

- B-DAT

- B-DAT
resentations. O
2) O
Multi-level O
cascading O
connection O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
features. O
This O
facilitates O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
instead O
of O
depthwise O
convolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
scheme O
connects O
all O
blocks O

- B-DAT

- B-DAT
ations, O
while O
we O
gather O
it O

- B-DAT
catenated O
at O
the O
end O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
on O
two O
commonly-used O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ters O
than O
ours. O
The O
CARN-M O

- B-DAT

- B-DAT

- B-DAT
ple, O
CARN O
outperforms O
its O
most O

- B-DAT

- B-DAT
ble O
results O
against O
computationally-expensive O
models O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tiple O
scales O
using O
a O
single O

- B-DAT

(- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
the O
multi-scale O
learning O
to O

- B-DAT

- B-DAT
off O
between O
performance O
vs. O
parameters O

- B-DAT

- B-DAT

- B-DAT

337K O
311.0G O
37.66/0.9590 O
33.38/0.9136 O
31.91/0.8962 O
- B-DAT
LapSRN O
[24] O
813K O
29.9G O
37.52/0.9590 O

974K O
225.7G O
37.89/0.9598 O
33.61/0.9160 O
32.08/0.8984 O
- B-DAT
CARN O
(ours) O
1,592K O
222.8G O
37.76/0.9590 O

- B-DAT

337K O
311.0G O
33.74/0.9226 O
29.90/0.8322 O
28.82/0.7980 O
- B-DAT
DRRN O
[35] O
297K O
6,796.9G O
34.03/0.9244 O

1,159K O
120.0G O
34.27/0.9257 O
30.30/0.8399 O
28.97/0.8025 O
- B-DAT
CARN O
(ours) O
1,592K O
118.8G O
34.29/0.9255 O

- B-DAT

337K O
311.0G O
31.55/0.8856 O
28.15/0.7680 O
27.32/0.7253 O
- B-DAT
LapSRN O
[24] O
813K O
149.4G O
31.54/0.8850 O

1,417K O
83.1G O
32.00/0.8931 O
28.49/0.7783 O
27.44/0.7325 O
- B-DAT
SRDenseNet O
[37] O
2,015K O
389.9G O
32.02/0.8934 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cading. O
The O
network O
topologies O
are O

- B-DAT

- B-DAT
tively O
carries O
mid- O
to O
high-level O

- B-DAT

- B-DAT

- B-DAT
resentations, O
the O
CARN O
model O
can O

- B-DAT

- B-DAT

- B-DAT
NG O
without O
global O
cascading. O
CARN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gation, O
and O
thus O
lead O
to O

- B-DAT

- B-DAT
nections O
inside O
the O
residual O
blocks O

- B-DAT
nation O
and O
1×1 O
convolutions, O
it O

- B-DAT

- B-DAT
tions O
in O
the O
cascading O
connection O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eters, O
and O
PSNR O
vs. O
operations O

- B-DAT

- B-DAT
ically. O
For O
example, O
the O
G64 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
form O
SISR O
accurately O
and O
efficiently O

- B-DAT

- B-DAT

- B-DAT
ent. O
Our O
experiments O
show O
that O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gence O
33(5), O
898–916 O
(2011 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
British O
Machine O

- B-DAT

- B-DAT

- B-DAT
scale O
hierarchical O
image O
database. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
(ICCV) O
(2015 O

- B-DAT
telligence O
and O
Statistics O
(2010 O

- B-DAT
works O
with O
pruning, O
trained O
quantization O

- B-DAT
level O
performance O
on O
imagenet O
classification O

- B-DAT
dreetto, O
M., O
Adam, O
H.: O
Mobilenets O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
and O
Pattern O
Recognition O
(CVPR O

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks. O
In: O
Proceedings O

- B-DAT
tion O
Processing O
Systems O
(NIPS) O
(2012 O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ters O
24(8), O
1208–1212 O
(2017 O

- B-DAT

- B-DAT
mentation. O
In: O
Proceedings O
of O
the O

- B-DAT

- B-DAT
ple O
convolution O
neural O
networks. O
In O

- B-DAT

- B-DAT
cal O
image O
segmentation. O
In: O
Proceedings O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work. O
In: O
Proceedings O
of O
the O

- B-DAT
nition O
(CVPR) O
(2017 O

- B-DAT

- B-DAT
tions. O
In: O
Proceedings O
of O
the O

- B-DAT

- B-DAT
resentation. O
IEEE O
transactions O
on O
image O

- B-DAT

commonly O
used O
in O
SR O
benchmarks: O
Set5 B-DAT
[2], O
Set14 O
[32], O
BSD100 O
[20 O

ban100 O
[11], O
Manga109 O
[21]. O
The O
Set5, B-DAT
Set14, O
and O
BSD100 O
datasets O
consist O

slightly O
better O
(+0.05 O
dB O
for O
Set5) B-DAT
than O
the O
baseline. O
We O
finally O

1371K O
1381K O
1379K O
1505K O
1646K O
Set5 B-DAT
37.90 O
37.93 O
37.95 O
37.98 O
37.96 O

Method O
# O
params. O
Set5 B-DAT
Set14 O
BSD100 O
Urban100 O
Manga109PSNR O

Scale O
Method O
# O
params. O
Set5 B-DAT
Set14 O
BSD100 O
Urban100PSNR O
/ O
SSIM O

1371K O
1381K O
1379K O
1505K O
1646K O
Set5 B-DAT
37 I-DAT

Method O
# O
params. O
Set5 B-DAT

Scale O
Method O
# O
params. O
Set5 B-DAT

For O
the O
upscaling B-DAT
part, O
we O
use O
the O
sub-pixel O

layers O
except O
those O
for O
the O
upscaling B-DAT
part. O
All O
our O
networks O
are O

methods, O
since O
MSRN O
uses O
different O
upscaling B-DAT
methods O
using O
more O
parameters O
for O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ral O
networks O
that O
stands O
out O

- B-DAT
tention O
mechanisms O
to O
single O
image O

- B-DAT

- B-DAT

- B-DAT
wise O
and O
spatial O
attention O
mechanisms O

- B-DAT

- B-DAT
imental O
analysis O
of O
different O
attention O

- B-DAT
formance O
in O
comparison O
to O
existing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O

- B-DAT

- B-DAT

- B-DAT
resolution O
(LR) O
image. O
It O
is O

- B-DAT

- B-DAT

- B-DAT
proved O
performance, O
this O
also O
has O

- B-DAT

- B-DAT
mation O
equally, O
which O
may O
not O

- B-DAT
table O
network O
structures O
in O
various O

- B-DAT
lems O
[8, O
30]. O
It O
allows O

- B-DAT
tracted O
feature O
maps, O
so O
that O

- B-DAT
ploy O
attention O
mechanisms. O
Zhang O
et O

- B-DAT
level O
vision O
problem O
[8] O
without O

- B-DAT

- B-DAT

- B-DAT
timized O
for O
SR, O
are O
attached O

- B-DAT

- B-DAT
RAM). O
The O
proposed O
RAM O
exploits O

- B-DAT
and O
intra- O
channel O
relationship O
by O

- B-DAT
spectively. O
We O
demonstrate O
both O
the O

- B-DAT
ciency O
of O
our O
proposed O
method O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ual O
attention O
module O
(RAM) O
based O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tiveness O
and O
efficiency O
of O
our O

- B-DAT

- B-DAT

- B-DAT
ual O
(CSAR) O
block O
[9]. O
Targeting O

- B-DAT
tion O
module O
that O
can O
be O

- B-DAT
nels O
(CA) O
or O
spatial O
regions O

- B-DAT
tention O
map O
M, O
having O
a O

- B-DAT
erated O
attention O
map O
is O
normalized O

- B-DAT

- B-DAT
matically O
in O
Table O
1, O
which O

- B-DAT

- B-DAT

- B-DAT
nism O
aims O
to O
recalibrate O
filter O

- B-DAT
channel O
correlation, O
i.e., O
CA. O
The O

- B-DAT
plied O
in O
the O
squeeze O
process O

- B-DAT
channel O
relation O
for O
refining O
feature O

- B-DAT
ploits O
both O
inter-channel O
and O
inter-spatial O

- B-DAT
ing O
is O
additionally O
performed O
in O

- B-DAT
cess. O
For O
the O
SA O
module O

- B-DAT
tially O
performs O
CA O
and O
then O

- B-DAT
volutions, O
where O
the O
first O
one O

- B-DAT
tio. O
While O
CBAM O
combines O
the O

- B-DAT
spired O
by O
EDSR O
[19], O
is O

CSAR O
[9] O
- B-DAT
M O
= O
conv1×1(conv1×1(X)) O
RAM O

RCAB O
[33] O
- B-DAT
CBAM O
[30] O
X̂ O
= O
fSA(fCA(X O

- B-DAT

- B-DAT

- B-DAT
scaling O
part. O
Let O
ILR O
and O

- B-DAT
tion O
3.2. O
F0 O
is O
updated O

- B-DAT
tion, O
and O
then O
the O
updated O

- B-DAT

- B-DAT

- B-DAT
struction O

- B-DAT
ing O
and O
reconstruction, O
respectively, O
and O

- B-DAT

- B-DAT
ditionally O
propose O
a O
way O
to O

- B-DAT
pose O
residual O
attention O
module O
(RAM O

- B-DAT

- B-DAT
lution, O
ReLU, O
and O
convolution, O
and O

- B-DAT
posed O
FA O
mechanism O

- B-DAT

- B-DAT
puter O
vision O
problems O
such O
as O

- B-DAT
ject O
detection O
without O
modification. O
However O

- B-DAT
mately O
aims O
at O
restoring O
high-frequency O

- B-DAT
ages, O
it O
is O
more O
reasonable O

- B-DAT
mined O
using O
high-frequency O
statistics O
about O

- B-DAT
ters O
will O
extract O
the O
edge O

- B-DAT
tion. O
From O
the O
viewpoint O
of O

- B-DAT
nels O
varies O
by O
the O
spatial O

- B-DAT
frequency O
components O
such O
as O
sky O

- B-DAT
like O
CBAM O
[30], O
which O
performs O

- B-DAT
formation O
per O
channel O
to O
preserve O

- B-DAT

- B-DAT
teristics. O
In O
addition, O
for O
the O

- B-DAT
trast O
to O
other O
SA O
mechanisms O

- B-DAT

- B-DAT

- B-DAT
volution O

- B-DAT
anisms O
exploit O
information O
from O
inter-channel O

- B-DAT
channel O
relationship, O
respectively. O
Therefore, O
in O

- B-DAT

- B-DAT
ered O
in O
detail O
in O
Section O

- B-DAT
ban100 O
[11], O
Manga109 O
[21]. O
The O

- B-DAT

- B-DAT
ferent O
characteristics O
from O
natural O
ones O

- B-DAT
to-noise O
ration O
(PSNR) O
and O
structural O

- B-DAT
dex O
on O
the O
Y O
channel O

- B-DAT

- B-DAT
domly O
crop O
a O
48×48 O
patch O

- B-DAT
lected O
16 O
LR O
training O
images O

- B-DAT

- B-DAT
ing O
images O
for O
each O
RGB O

- B-DAT
ment O
our O
networks O
using O
the O

- B-DAT
ploying O
each O
mechanism O
increases O
the O

- B-DAT
rameters, O
and O
as O
the O
network O

- B-DAT
ment O
with O
the O
easiest O
case O

- B-DAT
acteristics O
to O
check O
the O
generalization O

- B-DAT
wise O
and O
spatial O
information O
is O

- B-DAT
nism O
leads O
to O
performance O
improvement O

- B-DAT
age) O
only O
by O
adding O
9K O

- B-DAT
ages O
and O
the O
images O
in O

- B-DAT
tics O
of O
computer-generated O
images, O
which O

- B-DAT
ent O
from O
those O
in O
the O

- B-DAT
mance O
improvement O
is O
achieved O
only O

- B-DAT
pare O
it O
with O
the O
other O

- B-DAT
lustrated O
in O
Section O
2. O
For O

- B-DAT
mance. O
Overall, O
all O
the O
cases O

- B-DAT
ters O
than O
our O
network O
(+257K O

- B-DAT
ods O
of O
the O
squeeze O
process O

- B-DAT
served O
that O
our O
method O
extracting O

- B-DAT

- B-DAT
ing O
images, O
i.e., O
Urban100 O
and O

- B-DAT

- B-DAT
wise O
convolution O
is O
an O
effective O

- B-DAT
ditionally O
used, O
but O
at O
the O

- B-DAT
ure O
4. O
We O
have O
three O

- B-DAT
nisms O
showing O
higher O
performance O
have O

- B-DAT
ing O
the O
corresponding O
blocks O
only O

- B-DAT
nism. O
3) O
Our O
FA O
mechanism O

- B-DAT
ods, O
and O
those O
marked O
with O

- B-DAT
els O
with O
varying O
the O
number O

- B-DAT
scale O
SR O
are O
marked O
with O

- B-DAT
scale O
SR O
are O
marked O
with O

- B-DAT
tive O
(red O
color) O
and O
negative O

- B-DAT

- B-DAT
ground) O
and O
the O
high-frequency O
components O

- B-DAT
frequency O
component O
values O
to O
zero O

- B-DAT
tention” O
to O
the O
high-frequency O
components O

- B-DAT

- B-DAT
termined O
by O
the O
number O
of O

- B-DAT
tional O
parameters O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ness O
of O
RAM O
on O
large O

- B-DAT
responds O
to O
the O
opposite O
case O

- B-DAT
RAM O
R10C64, O
64×64 O
patches O
and O

- B-DAT

- B-DAT
rized O
in O
Table O
3, O
Table O

- B-DAT
ods O
widens O
further O
in O
Urban100 O

- B-DAT

- B-DAT
sual O
results O
of O
challenging O
images O

- B-DAT
mance O
with O
fewer O
parameters O
than O

- B-DAT
strates O
the O
efficiency O
of O
ours O

- B-DAT

- B-DAT
anisms O
are O
integrated O
in O
our O

- B-DAT
strated O
that O
our O
attention O
methods O

- B-DAT
tion O
mechanisms O
for O
SR O
and O

- B-DAT

- B-DAT

- B-DAT
work. O
In O
Proceedings O
of O
the O

- B-DAT
puter O
Vision O
(ECCV), O
2018. O
1 O

- B-DAT
Morel. O
Low-complexity O
single-image O
super-resolution O
based O

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
European O
Conference O

- B-DAT

- B-DAT

- B-DAT
works. O
In O
Proceedings O
of O
the O

- B-DAT
jection O
networks O
for O
super-resolution. O
In O

- B-DAT
tion O
(CVPR), O
2018. O
1, O
7 O

- B-DAT
cient O
convolutional O
neural O
networks O
for O

- B-DAT
cations. O
arXiv O
preprint O
arXiv:1704.04861, O
2017 O

- B-DAT

- B-DAT

- B-DAT
works. O
In O
Proceedings O
of O
the O

- B-DAT

- B-DAT
resolution. O
arXiv O
preprint O
arXiv:1809.11130, O
2018 O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
(CVPR), O
2016. O
1 O

- B-DAT
mization. O
arXiv O
preprint O
arXiv:1412.6980, O
2014 O

- B-DAT
resolution. O
In O
Proceedings O
of O
the O

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT
tive O
adversarial O
network. O
In O
Proceedings O

- B-DAT
ence O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cal O
statistics. O
In O
Proceedings O
of O

- B-DAT
ference O
on O
Computer O
Vision O
(ICCV O

- B-DAT
masaki, O
and O
K. O
Aizawa. O
Sketch-based O

- B-DAT
ing O
manga109 O
dataset. O
Multimedia O
Tools O

- B-DAT
ceedings O
of O
the O
Advances O
in O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
IEEE O
International O

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT

- B-DAT
lutional O
block O
attention O
module. O
In O

- B-DAT
pean O
Conference O
on O
Computer O
Vision O

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT

- B-DAT

- B-DAT
tional O
Conference O
on O
Curves O
and O

- B-DAT

- B-DAT
ual O
dense O
network O
for O
image O

- B-DAT

- B-DAT
ings O
of O
the O
IEEE O
Conference O

and O
time O
are O
evaluated O
on O
Set5 B-DAT
with O
the O
scale O
factor O
×4 O

MWCNN O
on O
four O
datasets, O
i.e., O
Set5 B-DAT
[7], O
Set14 O
[56], O
BSD100 O
[38 O

3 O
and O
4 O
on O
datasets O
Set5, B-DAT
Set14, O
BSD100 O
and O
Urban100. O
Red O

Set5 B-DAT
×2 O
37.17 O
/ O
0.9583 O
37.53 O

about O
0.4dB O
by O
PSNR O
on O
Set5 B-DAT
and O
Set14. O
On O
Urban100, O
our O

slightly O
higher O
PSNR O
values O
on O
Set5 B-DAT
and O
BSD100, O
and O
is O
comparable O

and O
time O
are O
evaluated O
on O
Set5 B-DAT

about O
0.4dB O
by O
PSNR O
on O
Set5 B-DAT

slightly O
higher O
PSNR O
values O
on O
Set5 B-DAT

results O
of O
“barbara” O
(Set14) O
with O
upscaling B-DAT
factor O
×4 O

results O
of O
“barbara” O
(Set14) O
with O
upscaling B-DAT
factor O
×4 O

results O
of O
“barbara” O
(Set14) O
with O
upscaling B-DAT
factor O
×4 O

results O
of O
“barbara” O
(Set14) O
with O
upscaling B-DAT
factor O
×4 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works O
(CNNs) O
generally O
enlarge O
the O

- B-DAT

- B-DAT

- B-DAT
thermore, O
another O
convolutional O
layer O
is O

- B-DAT
crease O
the O
channels O
of O
feature O

- B-DAT
network, O
inverse O
wavelet O
transform O
is O

- B-DAT
construct O
the O
high O
resolution O
feature O

- B-DAT
tering O
and O
subsampling, O
and O
can O

- B-DAT

- B-DAT

- B-DAT
tion O
from O
both O
prior O
modeling O

- B-DAT
tional O
neural O
networks O
(CNNs) O
have O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eral O
representative O
image O
restoration O
tasks O

- B-DAT

- B-DAT
ing O
[57], O
image O
deblurring O
[58 O

-3 B-DAT
10-2 O
10-1 O
100 O
101 O

- B-DAT
PCN O
[45], O
VDSR O
[29], O
DnCNN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
more O
complex O
image O
restoration O

- B-DAT
ping O
from O
degraded O
observation O
to O

- B-DAT
tional O
network O
(FCN) O
by O
removing O

- B-DAT
formance O
by O
taking O
more O
spatial O

- B-DAT
ever, O
for O
FCN O
without O
pooling O

- B-DAT
ing O
filters O
with O
larger O
size O

- B-DAT
ently O
suffers O
from O
gridding O
effect O

- B-DAT
large O
receptive O
field O
while O
avoiding O

- B-DAT
tational O
burden O
and O
the O
potential O

- B-DAT
trates O
the O
receptive O
field, O
run O

- B-DAT
RCNN O
[14] O
has O
relatively O
larger O

- B-DAT

- B-DAT
off O
between O
performance O
and O
efficiency O

- B-DAT

- B-DAT
tracting O
subnetwork O
and O
an O
expanding O

- B-DAT
ture O
maps O
[12, O
13], O
which O

- B-DAT
tailed O
texture. O
In O
the O
expanding O

- B-DAT
wise O
summation O
is O
adopted O
for O

- B-DAT
over, O
dilated O
filtering O
can O
also O

- B-DAT
larging O
receptive O
field. O
Experiments O
on O

- B-DAT
tiveness O
and O
efficiency O
of O
our O

- B-DAT
ure O
1, O
MWCNN O
is O
moderately O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ically, O
more O
discussions O
are O
given O

- B-DAT
ing O
[25]. O
These O
early O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cently, O
multi-layer O
perception O
(MLP) O
has O

- B-DAT
corporating O
residual O
learning O
with O
batch O

- B-DAT
ditional O
non-CNN O
based O
methods. O
Mao O

- B-DAT
gest O
to O
add O
symmetric O
skip O

- B-DAT
proving O
denoising O
performance. O
For O
better O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
CNN O
[16], O
which O
adopts O
a O

- B-DAT

- B-DAT
work O
[29], O
residual O
units O
[32 O

- B-DAT
tional O
cost O
or O
loss O
of O

- B-DAT

- B-DAT
tween O
receptive O
field O
size O
and O

- B-DAT
erative O
adversarial O
networks O
(GANs) O
have O

- B-DAT
duced O
to O
improve O
the O
visual O

- B-DAT
fers O
from O
blocking O
effect O
and O

- B-DAT

- B-DAT

- B-DAT
ing. O
For O
example, O
both O
DnCNN O

- B-DAT
noisers O
can O
also O
serve O
as O

- B-DAT

- B-DAT

- B-DAT
ers O
[58]. O
Romano O
et O
al O

- B-DAT
ing O
CNN O
on O
wavelet O
subbands O

- B-DAT

- B-DAT
lutional O
framelets O
[21, O
54] O
have O

- B-DAT

- B-DAT
composition. O
Deep O
convolutional O
framelets O
independently O

- B-DAT

- B-DAT
form, O
our O
MWCNN O
can O
embed O

- B-DAT
text O
and O
inter-subband O
dependency O

- B-DAT

- B-DAT

- B-DAT
chitecture. O
Finally, O
discussion O
is O
given O

- B-DAT
nection O
of O
MWCNN O
with O
dilated O

- B-DAT

- B-DAT
age O
x O
[36]. O
The O
convolution O

- B-DAT
nal O
property O
of O
DWT, O
the O

- B-DAT

- B-DAT
cessed O
with O
DWT O
to O
produce O

- B-DAT

- B-DAT
sition O
and O
reconstruction O
of O
an O

- B-DAT
ers. O
In O
the O
decomposition O
stage O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
construction O
result O
at O
the O
current O

- B-DAT

- B-DAT
quired O
to O
process O
the O
decomposition O

- B-DAT
lored O
to O
specific O
task. O
In O

- B-DAT

- B-DAT

- B-DAT
eralization O
of O
multi-level O
WPT, O
and O

- B-DAT
sampling O
operations O
safely O
without O
information O

- B-DAT
over, O
compared O
with O
conventional O
CNN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ations. O
As O
to O
the O
last O

- B-DAT
ing O
subnetwork. O
Generally, O
MWCNN O
modifies O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sponds O
to O
a O
multi-channel O
feature O

- B-DAT

- B-DAT
Net[41], O
while O
DWT O
and O
IWT O

- B-DAT
Net, O
the O
downsampling O
has O
no O

- B-DAT
nels, O
and O
the O
subsequent O
convolution O

- B-DAT
crease O
feature O
map O
channels. O
(iii O

- B-DAT
wise O
summation O
is O
used O
to O

- B-DAT
ventional O
U-Net O
concatenation O
is O
adopted O

- B-DAT
tion, O
Haar O
wavelet O
is O
adopted O

- B-DAT
ered O
in O
our O
experiments O

- B-DAT

- B-DAT
responding O
ground-truth O
image. O
The O
objective O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
convolution O
in O
MWCNN, O
respectively. O
When O

- B-DAT
bands O
are O
taken O
into O
account O

- B-DAT
mation O
loss O
caused O
by O
conventional O

- B-DAT
lated O
filtering O
with O
factor O
2 O

- B-DAT
fined O
analogously. O
We O
also O
have O

- B-DAT
lated O
filtering O
and O
MWCNN O
for O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
field O
of O
MWCNN. O
One O

- B-DAT
mance O
quantitatively O
and O
qualitatively O

- B-DAT
sion O
artifacts O
removal. O
Comparison O
of O

- B-DAT

- B-DAT
lowing O
[57], O
we O
consider O
three O

- B-DAT
ing O
four O
compression O
quality O
settings O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tel(R) O
Core(TM) O
i7-5820K O
CPU O
3.30GHz O

- B-DAT
ing O
methods O
are O
only O
tested O

- B-DAT
ban100 O
[23]. O
Table O
1 O
lists O

- B-DAT
ure O
5 O
shows O
the O
denoising O

- B-DAT

0.9027 O
32.77 O
/ O
0.9008 O
- B-DAT
- O
33.15 O
/ O
0.9088 O
25 O
29.97 O

0.8618 O
30.38 O
/ O
0.8601 O
- B-DAT
- O
30.79 O
/ O
0.8711 O
50 O
26.72 O

0.8906 O
31.63 O
/ O
0.8881 O
- B-DAT
- O
31.86 O
/ O
0.8947 O
25 O
28.57 O

0.8278 O
29.15 O
/ O
0.8249 O
- B-DAT
- O
29.41 O
/ O
0.8360 O
50 O
25.62 O

0.9250 O
32.49 O
/ O
0.9244 O
- B-DAT
- O
33.17 O
/ O
0.9357 O
25 O
29.70 O

0.8792 O
29.82 O
/ O
0.8839 O
- B-DAT
- O
30.66 O
/ O
0.9026 O
50 O
25.94 O

0.9593 O
37.66 O
/ O
0.9599 O
- B-DAT
37.52 O
/ O
0.9590 O
37.74 O

0.9222 O
33.82 O
/ O
0.9230 O
- B-DAT
- O
34.03 O
/ O
0.9244 O
34.09 O

0.9118 O
32.94 O
/ O
0.9144 O
- B-DAT
33.08 O
/ O
0.9130 O
33.23 O

0.8349 O
29.61 O
/ O
0.8341 O
- B-DAT
- O
29.96 O
/ O
0.8349 O
30.00 O

BSD100 O
×2 O
- B-DAT
31.90 O
/ O
0.8960 O
31.85 O

0.8942 O
31.98 O
/ O
0.8974 O
- B-DAT
31.80 O
/ O
0.8950 O
32.05 O

0.8995 O
32.23 O
/ O
0.8999 O
×3 O
- B-DAT
28.82 O
/ O
0.7976 O
28.80 O

0.7963 O
28.92 O
/ O
0.7993 O
- B-DAT
- O
28.95 O
/ O
0.8004 O
28.96 O

0.7987 O
29.12 O
/ O
0.8060 O
×4 O
- B-DAT
27.29 O
/ O
0.7251 O
27.23 O

Urban100 O
×2 O
- B-DAT
30.76 O
/ O
0.9140 O
30.75 O

0.9133 O
30.91 O
/ O
0.9159 O
- B-DAT
30.41 O
/ O
0.9100 O
31.23 O

0.9169 O
32.30 O
/ O
0.9296 O
×3 O
- B-DAT
27.14 O
/ O
0.8279 O
27.15 O

0.8276 O
27.31 O
/ O
0.8303 O
- B-DAT
- O
27.53 O
/ O
0.8378 O
27.56 O

0.8334 O
28.13 O
/ O
0.8514 O
×4 O
- B-DAT
25.18 O
/ O
0.7524 O
25.20 O

0.8837 O
32.91 O
/ O
0.8861 O
- B-DAT
33.43 O
/ O
0.8930 O
40 O
32.43 O

0.8911 O
33.34 O
/ O
0.8953 O
- B-DAT
33.77 O
/ O
0.9003 O
- O
34.27 O

0.9059 O
32.98 O
/ O
0.9090 O
- B-DAT
33.45 O
/ O
0.9153 O
40 O
32.35 O

0.9173 O
33.63 O
/ O
0.9198 O
- B-DAT
33.96 O
/ O
0.9247 O
- O
34.45 O

- B-DAT

- B-DAT
peting O
methods O
on O
the O
four O

- B-DAT
forms O
favorably O
in O
terms O
of O

- B-DAT
dexes. O
Compared O
with O
VDSR, O
our O

- B-DAT
perform O
VDSR, O
and O
also O
is O

- B-DAT
ResNet O
on O
Set14. O
Figure O
6 O

-1 B-DAT
method O
by O
0.37dB O

- B-DAT
overlapped O
8 O
× O
8 O
blocks O

- B-DAT
troducing O
the O
blocking O
artifact. O
The O

- B-DAT
mined O
by O
a O
quality O
factorQ O

- B-DAT
sider O
[18, O
19] O
due O
to O

- B-DAT
peting O
methods O
on O
Classic5 O
and O

- B-DAT
Net O
[48]) O
for O
the O
quality O

- B-DAT

- B-DAT

- B-DAT
ing O
library O
is O
adopted O
to O

- B-DAT
based O
methods O
with O
source O
codes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
s, O
MWCNN O
is O
moderately O
slower O

- B-DAT
stead O
of O
the O
increase O
of O

- B-DAT
ness O
of O
MWCNN O
should O
be O

- B-DAT
amples, O
we O
compare O
the O
PSNR O

- B-DAT

- B-DAT
fault O
MWCNN O
with O
Haar O
wavelet O

-2 B-DAT
wavelet, O
and O
(iii) O
MWCN- O
N O

-2 B-DAT
in O
expanding O
subnetwork. O
Then, O
abla O

- B-DAT
tion O
experiments O
are O
provided O
for O

- B-DAT
ness O
of O
additionally O
embedded O
wavelet O

- B-DAT
Net O
with O
same O
architecture O
to O

- B-DAT

- B-DAT
ing O
sum O
connection O
instead O
of O

- B-DAT
Net+D: O
adopting O
learnable O
conventional O
downsamping O

- B-DAT

- B-DAT

- B-DAT
ing O
library O
is O
adopted O
to O

- B-DAT
based O
methods O
with O
source O
codes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
s, O
MWCNN O
is O
moderately O
slower O

- B-DAT
stead O
of O
the O
increase O
of O

- B-DAT
ness O
of O
MWCNN O
should O
be O

- B-DAT
amples, O
we O
compare O
the O
PSNR O

- B-DAT

- B-DAT
fault O
MWCNN O
with O
Haar O
wavelet O

-2 B-DAT
wavelet, O
and O
(iii) O
MWCN- O
N O

-2 B-DAT
in O
expanding O
subnetwork. O
Then, O
abla O

- B-DAT
tion O
experiments O
are O
provided O
for O

- B-DAT
ness O
of O
additionally O
embedded O
wavelet O

- B-DAT
Net O
with O
same O
architecture O
to O

- B-DAT

- B-DAT
ing O
sum O
connection O
instead O
of O

- B-DAT
Net+D: O
adopting O
learnable O
conventional O
downsamping O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
library O
is O
adopted O
to O

- B-DAT
based O
methods O
with O
source O
codes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
s, O
MWCNN O
is O
moderately O
slower O

- B-DAT
stead O
of O
the O
increase O
of O

- B-DAT
ness O
of O
MWCNN O
should O
be O

- B-DAT
amples, O
we O
compare O
the O
PSNR O

- B-DAT

- B-DAT
fault O
MWCNN O
with O
Haar O
wavelet O

-2 B-DAT
wavelet, O
and O
(iii) O
MWCN- O
N O

-2 B-DAT
in O
expanding O
subnetwork. O
Then, O
abla O

- B-DAT
tion O
experiments O
are O
provided O
for O

- B-DAT
ness O
of O
additionally O
embedded O
wavelet O

- B-DAT
Net O
with O
same O
architecture O
to O

- B-DAT

- B-DAT
ing O
sum O
connection O
instead O
of O

- B-DAT
Net+D: O
adopting O
learnable O
conventional O
downsamping O

- B-DAT

- B-DAT
ing O
library O
is O
adopted O
to O

- B-DAT
based O
methods O
with O
source O
codes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fectiveness O
of O
MWCNN O
should O
be O

- B-DAT
ration O
of O
CNN O
and O
DWT O

- B-DAT

- B-DAT
fault O
MWCNN O
with O
Haar O
wavelet O

-2 B-DAT
wavelet, O
and O
(iii) O
MWCNN O
(HD O

-2 B-DAT
in O
expanding O
subnetwork. O
Then, O
ablation O

- B-DAT
periments O
are O
provided O
for O
verifying O

- B-DAT
ditionally O
embedded O
wavelet: O
(i) O
the O

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT
U-Net O
[41] O
U-Net+S O
U-Net+D O
DCF O

0.355 O
26.65 O
/ O
0.354 O
- B-DAT
/ O
- O
27.42 O
/ O
0.343 O

0.097 O
29.57 O
/ O
0.104 O
- B-DAT
/ O
- O
30.01 O
/ O
0.088 O

0.120 O
29.38 O
/ O
0.155 O
- B-DAT
/ O
- O
29.69 O
/ O
0.112 O

- B-DAT

-2 B-DAT

- B-DAT

- B-DAT
dicate O
that O
using O
sum O
connection O

- B-DAT
frequency O
localization O
property O
in O
wavelet O

- B-DAT
dent O
processing O
of O
subbands O
harms O

- B-DAT
pared O
to O
MWCNN O
(DB2) O
and O

- B-DAT
uation. O
MWCNN O
(Haar) O
has O
similar O

- B-DAT

- B-DAT
tween O
performance O
and O
efficiency O

- B-DAT
position, O
where O
different O
CNNs O
are O

- B-DAT
band. O
However, O
the O
results O
in O

- B-DAT
pendent O
processing O
of O
subbands O
is O

- B-DAT

- B-DAT
ier O
computational O
burden. O
Thus, O
a O

- B-DAT
NNs O
with O
different O
levels O
on O

-1 B-DAT
MWCNN-2 O
MWCNN-3 O
MWCNN-4 O

-1 B-DAT
∼MWCNN-4). O
It O
can O
be O
observed O

-3 B-DAT
with O
24-layer O
architecture O
performs O
much O

-1 B-DAT
and O
MWCNN-2, O
while O
MWCNN-4 O
only O

-3 B-DAT
in O
terms O
of O
the O
PSNR O

-3 B-DAT
is O
also O
moderate O
compared O
with O

-3 B-DAT
as O
the O
default O
setting O

- B-DAT

- B-DAT

- B-DAT
fectiveness O
and O
efficiency O
of O
MWCNN O

- B-DAT
eral O
restoration O
tasks O
such O
as O

- B-DAT

- B-DAT

- B-DAT
gle O
image O
super-resolution: O
Dataset O
and O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
shops O
(CVPRW), O
pages O
1122–1131. O
IEEE O

- B-DAT
demic O
Press, O
2001 O

- B-DAT
ing O
for O
image O
restoration: O
Persistent O

- B-DAT

- B-DAT
ifold O
simplification. O
In O
IEEE O
Conference O

- B-DAT
tion. O
IEEE O
Signal O
Processing O
Magazine O

- B-DAT
Morel. O
Low-complexity O
single-image O
super-resolution O
based O

- B-DAT
noising: O
Can O
plain O
neural O
networks O

- B-DAT
tion, O
pages O
2392–2399, O
2012 O

- B-DAT
olding O
for O
image O
denoising O
and O

- B-DAT
tions O
on O
Image O
Processing, O
9(9):1532–1546 O

- B-DAT
tion. O
IEEE O
Transactions O
on O
Pattern O

- B-DAT
age O
denoising O
by O
sparse O
3-d O

- B-DAT

- B-DAT
tive O
filtering. O
IEEE O
Transactions O
on O

- B-DAT

- B-DAT
ization O
and O
signal O
analysis. O
IEEE O

- B-DAT
tion O
Theory, O
36(5):961–1005, O
1990 O

- B-DAT
ference O
on O
Computer O
Vision, O
pages O

- B-DAT
sion O
artifacts O
reduction O
by O
a O

- B-DAT

- B-DAT
tion, O
pages O
2862–2869, O
2014 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
shops O
(CVPRW), O
2017 O

- B-DAT

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
lutional O
networks. O
In O
Advances O
in O

- B-DAT
cessing O
Systems, O
pages O
769–776, O
2009 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lishing O
Company, O
Incorporated, O
2012 O

- B-DAT

- B-DAT
volutional O
network O
for O
image O
super-resolution O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT
timization. O
In O
International O
Conference O
for O

- B-DAT
sentations, O
2015 O

- B-DAT
resolution. O
IEEE O
Conference O
on O
Computer O

- B-DAT
tern O
Recognition, O
2017 O

- B-DAT

- B-DAT

- B-DAT
ative O
adversarial O
network. O
IEEE O
Conference O

- B-DAT

- B-DAT
cessing, O
1(2):244–250, O
1992 O

- B-DAT
ing O
convolutional O
networks O
for O
content-weighted O

- B-DAT
pression. O
IEEE O
Conference O
on O
Computer O

- B-DAT
position: O
the O
wavelet O
representation. O
IEEE O

- B-DAT

- B-DAT
ric O
skip O
connections. O
In O
Advances O

- B-DAT
cessing O
Systems, O
pages O
2802–2810, O
2016 O

- B-DAT
cal O
statistics. O
In O
IEEE O
Conference O

- B-DAT
ence O
Computer O
Vision, O
volume O
2 O

- B-DAT
ing O
for O
image O
quality O
assessment O

- B-DAT

- B-DAT
tional O
networks O
for O
biomedical O
image O

- B-DAT
ternational O
Conference O
on O
Medical O
Image O

- B-DAT

- B-DAT

- B-DAT
ized O
deep O
image O
to O
image O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
preserving O
image O
super-resolution O
via O
contextualized O

- B-DAT
task O
learning. O
IEEE O
Transactions O
on O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
national O
conference O
on O
Multimedia, O
pages O

- B-DAT
mentation. O
arXiv O
preprint O
arXiv:1702.08502, O
2017 O

- B-DAT

- B-DAT

- B-DAT
ing O
with O
deep O
neural O
networks O

- B-DAT
dustrial O
and O
Applied O
Mathematics, O
2018 O

- B-DAT

- B-DAT
lated O
convolutions. O
arXiv O
preprint O
arXiv:1511.07122 O

- B-DAT

- B-DAT

- B-DAT
yond O
a O
gaussian O
denoiser: O
Residual O

widely O
used O
bench- O
mark O
datasets O
Set5 B-DAT
[3], O
Set14 O
[69] O
and O
BSD100 O

versions O
of O
each O
image O
on O
Set5, B-DAT
Set14 O
and O
BSD100: O
nearest O
neighbor O

and O
the O
adversarial O
networks O
on O
Set5 B-DAT
and O
Set14 O
benchmark O
data. O
MOS O

SRResNet- O
SRGAN- O
Set5 B-DAT
MSE O
VGG22 O
MSE O
VGG22 O
VGG54 O

respect O
to O
MOS O
score O
on O
Set5 B-DAT

Set5 B-DAT
nearest O
bicubic O
SRCNN O
SelfExSR O
DRCN O

the O
MOS O
tests O
conducted O
on O
Set5, B-DAT
Set14, O
BSD100 O
are O
summarized O
in O

a O
4× O
upscaling O
factor O
for O
Set5 B-DAT
(Section O
A.4), O
Set14 O
(Section O
A.5 O

downsampled O
versions O
of O
images O
from O
Set5, B-DAT
Set14 O
and O
BSD100. O
On O
BSD100 O

rated O
by O
each O
rater. O
On O
Set5 B-DAT
and O
Set14 O
the O
raters O
also O

ordinal O
ranking. O
While O
results O
on O
Set5 B-DAT
are O
somewhat O
inconclusive O
due O
to O

Set5 B-DAT
Set14 O
BSD100 O

distribution O
of O
MOS O
scores O
on O
Set5, B-DAT
Set14, O
BSD100. O
Mean O
shown O
as O

Set5 B-DAT
Set14 O
BSD100 O

Figure O
10: O
Average O
rank O
on O
Set5, B-DAT
Set14, O
BSD100 O
by O
averaging O
the O

A.4. O
Set5 B-DAT
- O
Visual O
Results O

Figure O
11: O
Results O
for O
Set5 B-DAT
using O
bicubic O
interpolation, O
SRResNet O
and O

and O
the O
adversarial O
networks O
on O
Set5 B-DAT

SRResNet- O
SRGAN- O
Set5 B-DAT

Set5 B-DAT

rated O
by O
each O
rater. O
On O
Set5 B-DAT

ordinal O
ranking. O
While O
results O
on O
Set5 B-DAT

Set5 B-DAT

Set5 B-DAT

Figure O
11: O
Results O
for O
Set5 B-DAT

when O
we O
super-resolve O
at O
large O
upscaling B-DAT
factors? O
The O
behavior O
of O
optimization-based O

photo-realistic O
natural O
images O
for O
4× O
upscaling B-DAT
factors. O
To O
achieve O
this, O
we O

guishable O
from O
original O
(right). O
[4× O
upscaling B-DAT

is O
particularly O
pronounced O
for O
high O
upscaling B-DAT
factors, O
for O
which O
texture O
detail O

are O
shown O
in O
brackets. O
[4× O
upscaling B-DAT

super- O
resolved O
with O
a O
4× O
upscaling B-DAT
factor O
is O
shown O
in O
Figure O

the O
network O
to O
learn O
the O
upscaling B-DAT
filters O
directly O
can O
further O
increase O

was O
also O
shown O
that O
learning O
upscaling B-DAT
filters O
is O
beneficial O
in O
terms O

super-resolves O
face O
images O
with O
large O
upscaling B-DAT
factors O
(8×). O
GANs O
were O
also O

for O
image O
SR O
with O
high O
upscaling B-DAT
factors O
(4×) O
as O
measured O
by O

photo-realistic O
SR O
images O
with O
high O
upscaling B-DAT
factors O
(4 O

losses O
in O
that O
category∗. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

HR O
image O
(right: O
i,j). O
[4× O
upscaling B-DAT

SSIM, O
MOS) O
in O
bold. O
[4× O
upscaling B-DAT

that O
SRGAN O
reconstructions O
for O
large O
upscaling B-DAT
factors O
(4×) O
are, O
by O
a O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
IEEE O

and O
SRGAN O
with O
a O
4× O
upscaling B-DAT
factor O
for O
Set5 O
(Section O
A.4 O

low-/high-resolution O
images O
and O
reconstructions O
(4× O
upscaling) B-DAT
obtained O
with O
different O
methods O
(bicubic O

image O
with O
resolution O
64×64 O
with O
upscaling B-DAT
factor O
4×. O
The O
measurements O
are O

for O
another O
100k O
iterations. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

all O
available O
individual O
ratings. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks, O
one O
central O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
(SR). O
To O
our O
knowledge O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
guishable O
from O
original O
(right). O
[4 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
inal O
image O
means O
that O
the O

- B-DAT
realistic O
as O
defined O
by O
Ferwerda O

- B-DAT

- B-DAT

- B-DAT
ing O
high-level O
feature O
maps O
of O

- B-DAT

- B-DAT
resolved O
with O
a O
4× O
upscaling O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
image O
informa- O
tion O

- B-DAT

- B-DAT
proaches O
to O
the O
SR O
problem O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
[27], O
trees O
[46] O
or O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-the-art O
SR O
performance. O
Subsequently, O
it O

- B-DAT

- B-DAT

- B-DAT
ciently O
train O
these O
deeper O
network O

- B-DAT
normalization O
[32] O
is O
often O
used O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
results. O
Another O
powerful O
design O

- B-DAT

- B-DAT
connections O
relieve O
the O
network O
architecture O

- B-DAT
tentially O
non-trivial O
to O
represent O
with O

- B-DAT

- B-DAT

- B-DAT
ing O
pixel-wise O
averages O
of O
plausible O

- B-DAT

- B-DAT
ity O
[42, O
33, O
13, O
5 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
perceptually O
more O
convincing O
solutions O

- B-DAT
ure O
2. O
We O
illustrate O
the O

- B-DAT
ure O
3 O
where O
multiple O
potential O

- B-DAT
tion. O
Yu O
and O
Porikli O
[66 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions. O
Similar O
to O
this O
work O

- B-DAT
trained O
VGG O
network O
instead O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity. O
The O
GAN O
procedure O
encourages O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mark O
datasets O
as O
well O
as O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
counterpart O
IHR. O
The O
high-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nels O
as O
in O
the O
VGG O

- B-DAT
ical O
for O
the O
performance O
of O

- B-DAT
tent O
loss O
lSRX O
and O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
solutions O
with O
overly O
smooth O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stead O
of O
log[1−DθD O
(GθG(ILR))] O
[22 O

- B-DAT
mark O
datasets O
Set5 O
[3], O
Set14 O

- B-DAT
and O
high-resolution O
images. O
This O
corresponds O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bor, O
bicubic, O
SRCNN O
[9] O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gral O
score O
from O
1 O
(bad O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG54 O
and O
the O
original O
HR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ResNet O
and O
the O
adversarial O
networks O

- B-DAT
SRGAN- O
Set5 O
MSE O
VGG22 O
MSE O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
MSE-based O
reconstructions, O
to O
those O
competing O

- B-DAT

- B-DAT
formed O
other O
SRGAN O
and O
SRResNet O

- B-DAT

- B-DAT
GAN O
to O
NN, O
bicubic O
interpolation O

- B-DAT

- B-DAT

- B-DAT
art O
methods. O
Quantitative O
results O
are O

- B-DAT
resolved O
with O
SRResNet O
and O
SRGAN O

- B-DAT
realistic O
image O
SR. O
All O
differences O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentary O
material). O
We O
further O
found O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Net O
that O
sets O
a O
new O

- B-DAT
sure. O
We O
have O
highlighted O
some O

- B-DAT

- B-DAT
ial O
loss O
by O
training O
a O

- B-DAT

- B-DAT

- B-DAT
the-art O
reference O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Stevenson. O
Super-Resolution B-DAT
from O
Image O
Sequences O
- O
A O
Review. O
Midwest O
Symposium O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
by O
adaptive O
sparse O
domain O

- B-DAT
ization. O
IEEE O
Transactions O
on O
Image O

- B-DAT

- B-DAT
resolution. O
IEEE O
Computer O
Graphics O
and O

- B-DAT
level O
vision. O
International O
Journal O
of O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

- B-DAT
line O
at O
http://torch.ch/blog/2016/02/04/resnets. O
html. O
2016 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In O
European O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT
tion O
with O
deep O
convolutional O
neural O

- B-DAT

- B-DAT

- B-DAT
mentation O
algorithms O
and O
measuring O
ecological O

- B-DAT

- B-DAT

- B-DAT
sive O
survey. O
In O
Machine O
Vision O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
for O
fast O
example-based O
super-resolution O

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
(ACCV O

- B-DAT

- B-DAT

- B-DAT
Resolution O
via O
Deep O
and O
Shallow O

- B-DAT

- B-DAT

- B-DAT
ence O
on O
Signals, O
Systems O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
derstanding O
Neural O
Networks O
Through O
Deep O

International O
Conference O
on O
Machine O
Learning O
- B-DAT
Deep O
Learning O
Workshop O
2015, O
page O

- B-DAT

- B-DAT
resolution O
by O
retrieving O
web O
images O

- B-DAT
volutional O
networks. O
In O
European O
Conference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG22, O
SRGAN-VGG54) O
described O
in O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
off O
between O
accuracy O
and O
speed O

-100 B-DAT
thousand O
update O
iterations O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

A.4. O
Set5 O
- B-DAT
Visual O
Results O

A.5. O
Set14 O
- B-DAT
Visual O
Results O

A.6. O
BSD100 O
(five O
random O
samples) O
- B-DAT
Visual O
Results O

29.96dB, O
28.95dB O
and O
27.42dB O
on O
Set5, B-DAT
Set14, O
BSD100 O
and O
Urban100, O
respectively O

with O
dif- O
ferent O
degradations O
on O
Set5 B-DAT
are O
provided O
in O
Table O
2 O

for O
bicubic O
degradation O
on O
datasets O
Set5 B-DAT
[3], O
Set14 O
[54], O
BSD100 O
[33 O

0.959 O
37.79 O
/ O
0.960 O
Set5 B-DAT
×3 O
30.39 O
/ O
0.868 O
32.74 O

methods O
with O
different O
degradations O
on O
Set5 B-DAT

comparison O
on O
image O
“Butterfly” O
from O
Set5 B-DAT

with O
dif- O
ferent O
degradations O
on O
Set5 B-DAT

- B-DAT

- B-DAT
age O
super-resolution O
(SISR). O
However, O
existing O

- B-DAT

- B-DAT

- B-DAT
age O
is O
bicubicly O
downsampled O
from O

- B-DAT

- B-DAT
over, O
they O
lack O
scalability O
in O

- B-DAT
blindly O
deal O
with O
multiple O
degradations O

- B-DAT
ity O
stretching O
strategy O
that O
enables O

- B-DAT

- B-DAT
put. O
Consequently, O
the O
super-resolver O
can O

- B-DAT

- B-DAT
duce O
favorable O
results O
on O
multiple O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put. O
As O
a O
classical O
problem O

- B-DAT
lenging O
research O
topic O
in O
the O

- B-DAT

- B-DAT
nel O
k O
and O
a O
latent O

- B-DAT
pling O
operation O
with O
scale O
factor O

- B-DAT
tive O
white O
Gaussian O
noise O
(AWGN O

- B-DAT
gories, O
i.e., O
interpolation-based O
methods, O
model-based O

- B-DAT
timization O
methods O
and O
discriminative O
learning O

- B-DAT

- B-DAT

- B-DAT
linear O
and O
bicubic O
interpolators O
are O

- B-DAT
age O
priors O
(e.g., O
the O
non-local O

- B-DAT

- B-DAT
based O
optimization O
methods O
are O
flexible O

- B-DAT
ative O
high-quality O
HR O
images, O
but O

- B-DAT

- B-DAT
tegration O
of O
convolutional O
neural O
network O

- B-DAT

- B-DAT
ciency O
to O
some O
extent, O
it O

- B-DAT
backs O
of O
model-based O
optimization O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
networks O
[16, O
18, O
21 O

- B-DAT
els O
based O
on O
discriminative O
CNN O

- B-DAT
els O
are O
specialized O
for O
a O

- B-DAT
ously O
when O
the O
assumed O
degradation O

- B-DAT
ever, O
little O
work O
has O
been O

- B-DAT
ing O
questions, O
which O
are O
the O

- B-DAT
thetic O
data O
to O
train O
a O

- B-DAT
ing O
these O
two O
questions O

- B-DAT

- B-DAT
resolution O
network. O
In O
view O
of O

- B-DAT
sionality O
stretching O
strategy O
which O
facilitates O

- B-DAT
edge, O
there O
is O
no O
attempt O

- B-DAT

- B-DAT
binations O
of O
blur O
kernels O
and O

- B-DAT
AWGN), O
we O
can O
select O
the O

- B-DAT
sult. O
It O
turns O
out O
that O

- B-DAT
sults O
on O
real O
LR O
images O

- B-DAT

- B-DAT
tion O
and O
works O
for O
multiple O

- B-DAT

- B-DAT

- B-DAT
resolution O
network O
learned O
from O
synthetic O

- B-DAT
of-the-art O
SISR O
methods O
on O
synthetic O

- B-DAT

- B-DAT

- B-DAT
work O
(SRCNN) O
was O
proposed. O
In O

- B-DAT
resolution O
and O
empirically O
showed O
that O

- B-DAT
ment O
of O
CNN O
super-resolvers. O
To O

- B-DAT
resolution O
(VDSR) O
method O
with O
residual O

- B-DAT

- B-DAT
ically O
demonstrated O
that O
a O
single O

- B-DAT
tiple O
scales O
super-resolution, O
image O
deblocking O

- B-DAT
put, O
which O
not O
only O
suffers O

- B-DAT
rectly O
manipulating O
the O
LR O
input O

- B-DAT
ing O
operation O
at O
the O
end O

- B-DAT

- B-DAT
scale O
the O
LR O
feature O
maps O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
layers O
are O
used O
to O

- B-DAT

- B-DAT

- B-DAT
teresting O
line O
of O
CNN-based O
methods O

- B-DAT
yond O
bicubic O
degradation O
adopt O
a O

- B-DAT

- B-DAT

- B-DAT
ever, O
manually O
selecting O
the O
hyper-parameters O

- B-DAT
able O
to O
learn O
a O
single O

- B-DAT

- B-DAT
ertheless, O
our O
method O
is O
general O

- B-DAT
cussion O
on O
blur O
kernel O
k O

- B-DAT
lar O
choice O
is O
isotropic O
Gaussian O

- B-DAT
cal O
and O
theoretical O
analyses O
have O

- B-DAT
ticated O
image O
priors O
[12]. O
Specifically O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
pre-processing O
step O
tends O
to O

- B-DAT

- B-DAT
mance O
[43]. O
Thus, O
it O
would O

- B-DAT

- B-DAT
pler O
since O
when O
k O
is O

- B-DAT

- B-DAT
tion O
model. O
It O
should O
be O

- B-DAT
lenging O
task O
since O
the O
degradation O

- B-DAT
ample). O
One O
relevant O
work O
is O

- B-DAT

- B-DAT
timization O
method O
and O
thus O
suffers O

- B-DAT
pects. O
First, O
our O
method O
considers O

- B-DAT
dation O
model. O
Second, O
our O
method O

- B-DAT

- B-DAT
essarily O
derived O
under O
the O
traditional O

- B-DAT
nections O
between O
the O
MAP O
principle O

- B-DAT
anism O
of O
CNN. O
Consequently, O
more O

- B-DAT
tecture O
design O
can O
be O
obtained O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fore, O
the O
MAP O
solution O
of O

- B-DAT

- B-DAT
lated O
as O

- B-DAT
dation O
process, O
accurate O
modeling O
of O

- B-DAT
based O
SISR O
methods O
with O
bicubic O

- B-DAT
form O
generic O
image O
super-resolution O
with O

- B-DAT
dicates O
that O
the O
parameters O
of O

- B-DAT
solve O
this O
problem O

- B-DAT
nel O
is O
first O
projected O
onto O

- B-DAT

- B-DAT
ear O
space O
by O
the O
PCA O

- B-DAT
nique. O
After O
that, O
the O
concatenated O

- B-DAT
dation O
maps O
M O
of O
size O

- B-DAT

- B-DAT
ing O
CNN O
possible O
to O
handle O

- B-DAT
ant O
degradations O
by O
considering O
the O

- B-DAT

- B-DAT

- B-DAT
plex O
architectural O
engineering. O
Typically, O
to O

- B-DAT

- B-DAT

- B-DAT
fied O
Linear O
Units O
(ReLU) O
[26 O

- B-DAT

- B-DAT
volutional O
layer O
to O
convert O
multiple O

- B-DAT
tional O
layers O
is O
set O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sign O
due O
to O
the O
following O

- B-DAT
ever, O
such O
blind O
model O
does O

- B-DAT
pected. O
First, O
the O
performance O
deteriorates O

- B-DAT
ing O
the O
blur O
kernel O
to O

- B-DAT
ferent O
HR O
images O
with O
pixel O

- B-DAT
vate O
the O
pixel-wise O
average O
problem O

- B-DAT

- B-DAT
ization O
ability O
and O
performs O
poorly O

- B-DAT

- B-DAT

- B-DAT
ity, O
one O
can O
treat O
the O

- B-DAT
nel O
and O
noise O
level O
as O

- B-DAT
tion O
maps, O
the O
non-blind O
model O

- B-DAT
tween O
data O
fidelity O
term O
and O

- B-DAT

- B-DAT
cally, O
the O
kernel O
width O
ranges O

- B-DAT
ity O
density O
function O
N O
(0,Σ O

- B-DAT
nel O
is O
determined O
by O
rotation O

- B-DAT
tion O
angle O
range O
to O
[0 O

- B-DAT
out O
the O
paper, O
it O
is O

- B-DAT
rect O
downsampler. O
Alternatively, O
we O
can O

- B-DAT
pler O
↓d, O
we O
can O
find O

- B-DAT

- B-DAT

- B-DAT
tor O
3 O
and O
PCA O
eigenvectors O

- B-DAT

- B-DAT
nel O
and O
a O
noise O
level O

- B-DAT
dation O
maps) O
for O
each O
epoch O

- B-DAT

- B-DAT

- B-DAT
tained O
by O
fine-tuning O
SRMD, O
its O

- B-DAT
cal O
GPU. O
The O
training O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
used O
datasets. O
As O
one O
can O

- B-DAT
rable O
results O
with O
VDSR O
at O

- B-DAT
forms O
VDSR O
at O
large O
scale O

- B-DAT
geNet O
dataset O
[26] O
to O
train O

- B-DAT
MDNF O
on O
scale O
factor O
4 O

- B-DAT
form O
other O
competing O
methods. O
The O

- B-DAT
plicit O
prior O
learning O
and O
thus O

- B-DAT
ment. O
This O
also O
can O
explain O

- B-DAT
parison, O
the O
run O
time O
of O

- B-DAT
ods. O
One O
can O
see O
that O

- B-DAT
petitive O
performance O
against O
other O
methods O

- B-DAT
tion O
settings O
are O
given O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferent O
degradations O
on O
Set5 O
are O

- B-DAT
ticular, O
the O
PSNR O
gain O
of O

- B-DAT

- B-DAT
Noise O
PSNR O
(×2/×3/×4)Width O
sampler O
Level O

- B-DAT

- B-DAT
pler. O
The O
visual O
comparison O
is O

- B-DAT
tially O
variant O
blur O
kernels O
and O

- B-DAT
cally O
downsampled O
from O
HR O
images O

- B-DAT
nels O
and O
corrupted O
by O
AWGN O

- B-DAT

- B-DAT
parison O

- B-DAT
sian O
kernels O
in O
training, O
it O

- B-DAT
ing. O
To O
find O
the O
degradation O

- B-DAT
cally, O
the O
kernel O
width O
is O

- B-DAT

- B-DAT
pression O
artifacts, O
Waifu2x O
[49] O
is O

- B-DAT
son. O
For O
image O
“Chip” O
which O

- B-DAT

- B-DAT
satisfying O
artifacts O
but O
also O
produce O

- B-DAT
ure O
9, O
we O
can O
see O

- B-DAT
duce O
over-smoothed O
results, O
whereas O
SRMD O

- B-DAT

- B-DAT
dations O
via O
a O
single O
model O

- B-DAT
based O
SISR O
methods, O
the O
proposed O

- B-DAT

- B-DAT
ically, O
degradation O
maps O
are O
obtained O

- B-DAT
sionality O
stretching O
of O
the O
degradation O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tially O
variant O
degradations. O
Moreover, O
the O

- B-DAT
struct O
visually O
plausible O
HR O
images O

- B-DAT
posed O
super-resolver O
offers O
a O
feasible O

- B-DAT
tical O
CNN-based O
SISR O
applications O

- B-DAT

- B-DAT
ity O
Enhancement O
of O
Surveillance O
Images O

- B-DAT
ration O
for O
providing O
us O
the O

- B-DAT
search O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
shops, O
volume O
3, O
pages O
126–135 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
Conference, O
2012 O

- B-DAT
shift O
priors O
for O
image O
restoration O

- B-DAT
formation O
Processing O
Systems, O
2017 O

- B-DAT
tion O
diffusion O
processes O
for O
effective O

- B-DAT
tion, O
pages O
5261–5269, O
2015 O

- B-DAT

- B-DAT

- B-DAT
pean O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
ference O
on O
Computer O
Vision, O
pages O

- B-DAT
ized O
sparse O
representation O
for O
image O

- B-DAT
actions O
on O
Image O
Processing, O
22(4):1620–1630 O

- B-DAT
resolution. O
In O
IEEE O
International O
Conference O

- B-DAT
resolution O
via O
BM3D O
sparse O
coding O

- B-DAT
resolution O
and O
texture O
synthesis. O
Advances O

- B-DAT
dom O
Fields O
for O
Vision O
and O

- B-DAT

- B-DAT
puter O
Vision, O
pages O
349–356, O
2009 O

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
nets. O
In O
Advances O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
network O
for O
image O
super-resolution O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT
timization. O
In O
International O
Conference O
for O

- B-DAT
sentations, O
2015 O

- B-DAT
resolution. O
In O
IEEE O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
network. O
In O
IEEE O

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT
tion O
Workshops, O
pages O
136–144, O
July O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
tional O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT
tional O
neural O
networks. O
In O
Advances O

- B-DAT
tioned O
regression O
models O
for O
non-blind O

- B-DAT
resolution. O
In O
IEEE O
International O
Conference O

- B-DAT
curate O
image O
super O
resolution. O
IEEE O

- B-DAT
putational O
Imaging, O
3(1):110–125, O
2017 O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
preserving O
image O
super-resolution O
via O
contextualized O

- B-DAT
task O
learning. O
IEEE O
Transactions O
on O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
tional O
Conference O
on O
Computer O
Vision O

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT

- B-DAT
ral O
networks O
for O
matlab. O
In O

- B-DAT

- B-DAT

- B-DAT
tural O
similarity. O
IEEE O
Transactions O
on O

- B-DAT

- B-DAT
resolution: O
A O
benchmark. O
In O
European O

- B-DAT
puter O
Vision, O
pages O
372–386, O
2014 O

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT

- B-DAT
cessing, O
26(12):5895–5907, O
2017 O

- B-DAT

- B-DAT

- B-DAT
gle O
image O
super-resolution O
under O
internet O

- B-DAT
ference O
on O
Multimedia, O
pages O
677–687 O

- B-DAT
yond O
a O
gaussian O
denoiser: O
Residual O

- B-DAT
ing, O
pages O
3142–3155, O
2017 O

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
and O

We O
adopt O
four O
benchmark O
sets: O
Set5 B-DAT
[1], O
Set14 O
[50], O
BSD100 O
[30 O

3 O
and O
×4 O
on O
datasets O
Set5, B-DAT
Set14, O
BSD100 O
and O
Urban100. O
The O

Set5 B-DAT
×2 O
36.66/0.9542 O
37.53/0.9587 O
37.63/0.9588 O
37.52/0.959 O

20] O
for O
testing O
with O
three O
upscaling B-DAT
factors: O
×2, O
×3 O
and O
×4 O

training O
images O
for O
all O
three O
upscaling B-DAT
factors: O
×2, O
×3 O
and O
×4 O

model O
for O
all O
these O
three O
upscaling B-DAT
factors O
as O
in O
[21, O
37 O

best O
result O
across O
all O
the O
upscaling B-DAT
factors O
and O
datasets. O
Visual O
results O

image O
super-resolution O
results O
with O
×4 O
upscaling B-DAT

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

the O
advantage O
of O
RNN O
architecture O
- B-DAT
the O
correlation O
information O
is O
propagated O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
based O
image O
restoration O
approaches. O
The O

- B-DAT

- B-DAT

- B-DAT
sity O
[28, O
46]. O
Alternatively, O
similar O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
is O
adopted O
to O
save O

- B-DAT
sides O
CNNs, O
RNNs O
have O
also O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
methods O
based O
on O
low-rankness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
vision O
tasks. O
However, O
unlike O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-3 B-DAT
and O
reduce O
it O
by O
half O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
local O
modules, O
we O
implement O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-6 B-DAT
from O
NLRN O
with O
unrolling O
length O

- B-DAT

- B-DAT
of-the-art O
network O
models O
on O
Set12 O

- B-DAT
plexities O
are O
also O
compared O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
layers) O
of O
NLRN. O
The O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

branch O
to O
reconstruct O
the O
HR O
4x B-DAT
image O

Set5 B-DAT
[30] O
and O
Set14 O
[31]. O
The O

comparing O
PSNR, O
in O
Set5 B-DAT
our O
model O
is O
0.22dB O
higher O

models O
for O
scale O
x4 O
on O
Set5 B-DAT
and O

Set5 B-DAT
Set14 O

Set5 B-DAT
Set14 O

including O
Set5 B-DAT
[30] O
and O
Set14 O
[31]. O
We O

Set5 B-DAT
Set14 O
Bsd100 O
Urban100 O

comparing O
PSNR, O
in O
Set5 B-DAT

models O
for O
scale O
x4 O
on O
Set5 B-DAT

Set5 B-DAT

Set5 B-DAT

Set5 B-DAT

high-resolution O
(HR) O
images. O
When O
the O
upscaling B-DAT
factor O
is O
large O

extra O
computation. O
Moreover, O
for O
large O
upscaling B-DAT
factors, O
our O

high O
quality O
images O
for O
higher O
upscaling B-DAT
factors O

model O
could O
do O
multi O
scale O
upscaling B-DAT
task O
via O
Laplacian O
Pyramid O

upscaling B-DAT
factors O
and O
decreasing O
parameters O
by O

We O
first O
upscaling B-DAT
our O
low-resolution O
image O
via O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-the-art O
methods O
in O
scale O
x4 O

-5 B-DAT

- B-DAT

- B-DAT

-11 B-DAT

- B-DAT
learning-based O
super O
resolution O
methods O
have O

- B-DAT

- B-DAT

- B-DAT
resolution O
simultaneously O
in O
one O
feed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
quality O
high-resolution O
image, O
the O
process O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-2798 B-DAT

- B-DAT

-8 B-DAT

- B-DAT

- B-DAT

-2873 B-DAT

- B-DAT

- B-DAT

-3478 B-DAT

- B-DAT

- B-DAT

-126 B-DAT

- B-DAT
based O
single O
image O
super O
resolu-tion[C]//Proceedings O

- B-DAT
1873 O

- B-DAT

- B-DAT

-5206 B-DAT

- B-DAT

-1105 B-DAT

-551 B-DAT

- B-DAT

-2324 B-DAT

- B-DAT

-307 B-DAT

- B-DAT

- B-DAT

-778 B-DAT

- B-DAT

- B-DAT

- B-DAT
1654 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1645 B-DAT

- B-DAT

- B-DAT

-4547 B-DAT

- B-DAT

-4817 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-711 B-DAT

- B-DAT

- B-DAT

- B-DAT
2423 O

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adver-sarial O

- B-DAT

- B-DAT

-4500 B-DAT

-612 B-DAT

-456 B-DAT

- B-DAT

- B-DAT

- B-DAT
image O
super-resolution O
based O
on O
nonnegative O

- B-DAT

- B-DAT
representations[C]//International O
conference O
on O
curves O
and O

-730 B-DAT

-423 B-DAT

- B-DAT

-407 B-DAT

for O
scale O
factor O
×3 O
on O
Set5 B-DAT
[1]. O
The O
number O
of O
layers O

testing, O
we O
choose O
the O
dataset O
Set5 B-DAT
[1], O
and O
Set14 O
[35] O
which O

Set5 B-DAT
×2 O
33.66/0.9299 O
36.66/0.9542 O
37.53/0.9587 O
37.63/0.9588 O

3 O
and O
×4 O
on O
datasets O
Set5, B-DAT
Set14, O
BSD100 O
and O
Urban100. O
Red O

Set5 B-DAT
×2 O
6.083 O
8.036 O
7.811 O
8.569 O

3 O
and O
×4 O
on O
datasets O
Set5, B-DAT
Set14 O
and O
Urban100. O
Red O
color O

The O
models O
are O
tested O
under O
Set5 B-DAT
with O
scale O
factor O
×3 O

non-residual O
coun- O
terpart. O
Tests O
on O
Set5 B-DAT
with O
scale O
factor O
×3 O

The O
tests O
are O
conducted O
on O
Set5 B-DAT
with O
scale O
factor O
×3 O

for O
scale O
factor O
×3 O
on O
Set5 B-DAT

The O
models O
are O
tested O
under O
Set5 B-DAT

non-residual O
coun- O
terpart. O
Tests O
on O
Set5 B-DAT

The O
tests O
are O
conducted O
on O
Set5 B-DAT

of O
the O
art O
for O
large O
upscaling B-DAT
factors O
(×4). O
EDSR O
[16 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ual O
Learning O
and O
Convolutional O
Sparse O

- B-DAT

- B-DAT
Threshold O
Algorithm O
(LISTA). O
We O
extend O

- B-DAT
volutional O
version O
and O
build O
the O

- B-DAT
tional O
sparse O
codings O
of O
input O

- B-DAT

- B-DAT
mark O
datasets O
demonstrate O
the O
effectiveness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
arts, O
e.g., O
DRRN O
(52 O
layers O

- B-DAT
sults O
are O
available O
at O
https://github.com/axzml O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rithms, O
especially O
the O
current O
leading O

- B-DAT

- B-DAT
ods O
[26, O
4, O
13, O
14 O

- B-DAT
lem O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sent O
our O
models. O
RL-CSC O
with O

- B-DAT
petitive O
performance O
with O
MemNet O
[24 O

- B-DAT
ters, O
the O
performance O
of O
RL-CSC O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
methods O
[32]. O
Inspired O
by O

- B-DAT

- B-DAT
ers O
named O
VDSR, O
which O
shows O

- B-DAT

- B-DAT
gradient O
problem O
when O
the O
network O

- B-DAT

- B-DAT
work O
(DRCN) O
[14] O
with O
a O

- B-DAT
nary O
success O
of O
ResNet O
[9 O

- B-DAT

- B-DAT

- B-DAT
cursive O
manner, O
leading O
to O
a O

- B-DAT
decoding O
network O
named O
RED-Net O
was O

- B-DAT
tioned O
models O
usually O
lack O
convincing O

- B-DAT
plored, O
i.e., O
what O
role O
each O

- B-DAT
resentation O
with O
strong O
theoretical O
support O

- B-DAT

- B-DAT

- B-DAT
ment O
when O
the O
number O
of O

- B-DAT
posed O
CSC O
based O
SR O
(CSC-SR O

- B-DAT

- B-DAT
tional O
sparse O
coding O
methods. O
In O

- B-DAT
putationally O
efficient O
CSC O
model, O
Sreter O

- B-DAT
duced O
a O
convolutional O
recurrent O
sparse O

- B-DAT

- B-DAT
tending O
the O
LISTA O
method O
to O

- B-DAT
ing O
tasks O

- B-DAT

- B-DAT

- B-DAT
posed O
in O
the O
field O
of O

- B-DAT
tion, O
our O
model, O
termed O
as O

- B-DAT

- B-DAT
defined O
interpretability O

- B-DAT
CSC O
(30 O
layers) O
has O
achieved O

- B-DAT

- B-DAT
mizes O
the O
objective O
function O
(1 O

- B-DAT
ting O
term O
and O
an O
`1-norm O

- B-DAT

- B-DAT
ization O
coefficient O
λ O
is O
used O

- B-DAT

- B-DAT
ative O
Shrinkage O
Thresholding O
Algorithm O
(ISTA O

- B-DAT

- B-DAT

- B-DAT
dress O
this O
issue, O
Gregor O
and O

- B-DAT
gorithm O
termed O
as O
Learned O
ISTA O

- B-DAT
proximate O
estimates O
of O
sparse O
code O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ters O
We, O
G O
and O
θ O

- B-DAT

- B-DAT
vide O
the O
whole O
image O
into O

- B-DAT
actly O
the O
same O
, O
is O

- B-DAT
able O
for O
this O
issue, O
as O

- B-DAT
ternating O
Direction O
Method O
of O
Multipliers O

- B-DAT

- B-DAT

- B-DAT
ory O
burden O
issue O
of O
ADMM O

- B-DAT

- B-DAT
put O
Interpolated O
LR O
(ILR) O
image O

- B-DAT
gency O
speed O
and O
the O
reconstruction O

- B-DAT
lated O
Low-Resolution O
(ILR) O
image O
Iy O

- B-DAT
dicts O
the O
output O
HR O
image O

- B-DAT
verting O
one O
of O
the O
inputs O

- B-DAT
tive O
tool O
to O
learn O
the O

- B-DAT

- B-DAT
portant O
facts: O
(1) O
the O
expressiveness O

- B-DAT

- B-DAT

- B-DAT
frequency O
information O
reconstruction O

- B-DAT
structed O
by O
the O
addition O
of O

- B-DAT

- B-DAT
tion, O
W1 O
and O
S O
for O

- B-DAT
ery O
recursion. O
When O
K O
recursions O

- B-DAT
ing O
process, O
the O
depth O
d O

- B-DAT
ploited O
in O
our O
training O
process O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dient O
descent O
(SGD) O
is O
used O

- B-DAT
ment O
our O
model O
using O
the O

- B-DAT

- B-DAT
fied O
structures O
of O
these O
models O

- B-DAT

- B-DAT

- B-DAT
put) O
and O
a O
pre-activation O
structure O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eter O
settings O
for O
better O
performance O

- B-DAT

- B-DAT

- B-DAT
tween O
SCN O
[29] O
and O
RL-CSC O

- B-DAT

- B-DAT
ing O
linear O
layers, O
so O
more O

- B-DAT

- B-DAT
works. O
With O
the O
help O
of O

- B-DAT

- B-DAT
fers O
with O
DRCN O
[14] O
in O

- B-DAT
ual O
Learning O
[23] O
(LRL) O
and O

- B-DAT

- B-DAT
sides, O
DRCN O
is O
not O
easy O

- B-DAT

- B-DAT

- B-DAT
ate O
predictions) O
is O
used O
to O

- B-DAT

- B-DAT

- B-DAT
troduction O
to O
the O
datasets O
used O

- B-DAT
isons O
with O
state-of-the-arts O
are O
presented O

- B-DAT

- B-DAT
tion O
Dataset O
[18]. O
During O
testing O

- B-DAT
mark. O
Moreover, O
the O
BSD100 O
[18 O

- B-DAT
ural O
images O
are O
used O
for O

- B-DAT

- B-DAT

- B-DAT
minance) O
of O
transformed O
YCbCr O
space O

- B-DAT
cludes O
flipping O
(horizontally O
and O
vertically O

- B-DAT
formed O
on O
each O
image O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tioned O
into O
33× O
33 O
patches O

- B-DAT

- B-DAT
CSC O
is O
30 O
according O
to O

- B-DAT
mized O
using O
SGD O
with O
mini-batch O

- B-DAT

- B-DAT

- B-DAT
ther O
improvements O
of O
the O
loss O

- B-DAT
dient O
clipping O
strategy O
stated O
in O

- B-DAT
ter. O
A O
NVIDIA O
Titan O
Xp O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bic O
interpolation O
to O
the O
original O

- B-DAT
els O
near O
borders O
before O
evaluation O

- B-DAT
mark O
testing O
sets, O
and O
results O

- B-DAT

- B-DAT
lowing O
[23], O
BSD100 O
is O
not O

- B-DAT

- B-DAT

- B-DAT
formance O
(K O
= O
15 O
33.98dB O

- B-DAT
ter. O
Similar O
conclusions O
are O
observed O

- B-DAT
tempt O
to O
combine O
the O
powerful O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ficulties, O
but O
helps O
network O
converge O

- B-DAT

- B-DAT
tings O
as O
stated O
in O
Section O

- B-DAT
ters. O
Specifically, O
two O
types O
of O

- B-DAT
plied O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
terpart. O
Tests O
on O
Set5 O
with O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
datas O
are O
derived O
from O

- B-DAT

- B-DAT
fault O
settings O
given O
by O
the O

- B-DAT

- B-DAT
Memory O
(OOM) O
issue. O
The O
reason O

- B-DAT

- B-DAT

- B-DAT
terpretability. O
We O
extend O
the O
LISTA O

- B-DAT
volutional O
version O
and O
build O
the O

- B-DAT
sions O
without O
introducing O
any O
new O

- B-DAT
sults O
with O
state-of-the-arts O
and O
demonstrate O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
deep O
convolutional O
networks O

- B-DAT

- B-DAT
nary O
Learning: O
A O
Comparative O
Review O

- B-DAT

Delving O
Deep O
into O
Rec- B-DAT
tifiers O
- O
Surpassing O
Human-Level O
Performance O
on O
ImageNet O

- B-DAT
ing O
for O
Image O
Recognition. O
In O

- B-DAT
ble O
convolutional O
sparse O
coding. O
CVPR O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
CVPR O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT
lutional O
Network O
for O
Image O
Super-Resolution O

- B-DAT
ham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT

- B-DAT
ing O
a O
Generative O
Adversarial O
Network O

- B-DAT

- B-DAT
ing O
very O
deep O
convolutional O
encoder-decoder O

- B-DAT
cal O
statistics. O
volume O
2, O
pages O

- B-DAT
Vito, O
Z. O
Lin, O
A. O
Desmaison O

- B-DAT
matic O
differentiation O
in O
pytorch. O
In O

- B-DAT

- B-DAT

- B-DAT
ing. O
In O
ICASSP, O
pages O
2191–2195 O

- B-DAT

- B-DAT
tent O
memory O
network O
for O
image O

- B-DAT

- B-DAT

Challenge O
on O
Single O
Image O
Super-Resolution B-DAT
- O
Methods O
and O
Results. O
CVPR O
Workshops O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution: O
A O
benchmark. O
In O
ECCV O

- B-DAT
age O
super-resolution O
via O
sparse O
representation O

- B-DAT

- B-DAT
convolutional O
networks. O
CVPR, O
2010. O
2 O

- B-DAT

- B-DAT

A O
Survey O
of O
Sparse O
Representation O
- B-DAT
Algorithms O
and O
Applications. O
IEEE O
Access O

time O
for O
upscaling O
3× O
on O
Set5 B-DAT

four O
widely O
used O
benchmark O
datasets: O
Set5 B-DAT
[1], O
Set14 O
[27], O
BSD100 O
[18 O

Urban100 O
[10]. O
Among O
these O
datasets, O
Set5, B-DAT
Set14 O
and O
BSD100 O
consist O
of O

the O
“but- O
terfly” O
image O
from O
Set5 B-DAT
dataset O

Set5 B-DAT
×2 O
33.66/0.9299 O
37.53/0.9587 O
37.63/0.9588 O
37.52/0.9591 O

Set5 B-DAT
×2 O
6.083 O
8.580 O
8.783 O
9.010 O

Set5 B-DAT
×2 O
0.054 O
0.735 O
0.032 O
4.343 O

the O
“but- O
terfly” O
image O
from O
Set5 B-DAT

the O
average O
inference O
time O
for O
upscaling B-DAT
3× O
on O
Set5. O
The O
IDN O

restoration O
per- O
formance O
with O
larger O
upscaling B-DAT
factors, O
the O
recent O
SR O
meth O

the O
original O
HR O
images O
with O
upscaling B-DAT
factor O
m O
(m O
= O
2 O

the O
Set14 O
dataset O
with O
an O
upscaling B-DAT
factor O
4 O

the O
BSD100 O
dataset O
with O
an O
upscaling B-DAT
factor O
4 O

the O
Urban100 O
dataset O
with O
an O
upscaling B-DAT
factor O
4 O

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

- B-DAT

- B-DAT
age O
super-resolution. O
However, O
as O
the O

- B-DAT

- B-DAT

- B-DAT
ods O
have O
been O
faced O
with O

- B-DAT
pact O
convolutional O
network O
to O
directly O

- B-DAT
tion O
block, O
the O
local O
long O

- B-DAT

- B-DAT
fectively O
extracted. O
Specifically, O
the O
proposed O

- B-DAT
quential O
blocks. O
In O
addition, O
the O

- B-DAT
tion. O
Experimental O
results O
demonstrate O
that O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cially O
in O
terms O
of O
time O

- B-DAT

- B-DAT

- B-DAT
lem O
in O
low-level O
computer O
vision O

- B-DAT

- B-DAT

- B-DAT
age. O
Actually, O
an O
infinite O
number O

- B-DAT

- B-DAT
ists. O
In O
order O
to O
mitigate O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formance O
with O
larger O
upscaling O
factors O

- B-DAT
ods O
fall O
into O
the O
example-based O

- B-DAT
ral O
network O
(CNN), O
many O
CNN-based O

- B-DAT
mance. O
Kim O
et O
al. O
propose O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ment O
of O
enormous O
parameters O
of O

- B-DAT
der O
to O
achieve O
better O
performance O

- B-DAT
ory O
consumption, O
which O
are O
less O

- B-DAT
caded O
network O
topologies, O
e.g., O
VDSR O

- B-DAT
brating O
channel-wise O
features O
responses O
can O

- B-DAT
mation O
distillation O
network O
(IDN) O
with O

- B-DAT
ters O
and O
computational O
complexity O
as O

- B-DAT
formation O
distillation O
blocks O
(DBlocks) O
are O

- B-DAT
gressively O
distill O
residual O
information. O
Finally O

- B-DAT
tion O
Block O
(RBlock) O
aggregates O
the O

- B-DAT

- B-DAT
tion O
block, O
which O
contains O
an O

- B-DAT
pression O
unit. O
The O
enhancement O
unit O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pressive O
power, O
we O
send O
a O

- B-DAT

- B-DAT

- B-DAT
ture O
maps O
into O
two O
parts O

- B-DAT
path O
features O
and O
another O
expresses O

- B-DAT

- B-DAT

- B-DAT
mary, O
the O
enhancement O
unit O
is O

- B-DAT
sentation O
power O
of O
the O
network O

- B-DAT
ber O
of O
convolutional O
layer O

- B-DAT

- B-DAT

- B-DAT
tains O
better O
reconstruction O
accuracy O

- B-DAT

- B-DAT
ied O
in O
these O
years. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
similarity O
property O
and O
extract O
example O

- B-DAT
terns O
and O
textures O
but O
lacks O

- B-DAT
tory O
prediction O
for O
images O
of O

- B-DAT

- B-DAT
spective O
deformation O

- B-DAT

- B-DAT

- B-DAT
pact O
dictionary O
or O
manifold O
space O

- B-DAT
dom O
forest O
[20] O
and O
sparse O

- B-DAT
mal O
for O
generating O
high-quality O
SR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
to O
accelerate O
SRCNN O
in O

- B-DAT
signed O
by O
Kim O
et O
al O

- B-DAT
tion O
with O
skip O
connection O
to O

- B-DAT
decoder O
networks O
and O
symmetric O
skip O

- B-DAT

- B-DAT
work O
(LapSRN) O
to O
address O
the O

- B-DAT

- B-DAT
ages. O
Tai O
et O
al. O
[22 O

- B-DAT
work O
to O
effectively O
build O
a O

- B-DAT
racy. O
The O
authors O
also O
present O

- B-DAT

- B-DAT

- B-DAT
sistent O
memory O
network O
(MemNet) O
[23 O

- B-DAT
tion O
task, O
which O
tackles O
the O

- B-DAT

- B-DAT
pose O
a O
novel O
combination O
of O

- B-DAT
construction O
block O
(RBlock). O
Here, O
we O

- B-DAT
formation O
distillation O
blocks O
by O
using O

- B-DAT

- B-DAT

- B-DAT
tively. O
Finally, O
we O
take O
a O

- B-DAT

- B-DAT
age O
restoration O
as O
defined O
below O

- B-DAT
mulated O
as O
follows O

- B-DAT

- B-DAT
lutions O
and O
another O
is O
the O

- B-DAT

- B-DAT
lows O

- B-DAT
while O
is O
the O
input O
of O

- B-DAT

- B-DAT
eration O
respectively. O
Specifically, O
we O
know O

- B-DAT

- B-DAT
path O
information O
as O
the O
input O

- B-DAT

- B-DAT
ations O
of O
the O
below O
module O

- B-DAT
path O
information O
and O
the O
local O

- B-DAT

- B-DAT
mulated O
as O

- B-DAT

- B-DAT

- B-DAT
lized O
without O
exception O
by O
a O

- B-DAT
tage O
of O
a O
1× O
1 O

- B-DAT
tation O
Dataset O
(BSD) O
[18] O
as O

- B-DAT
tation O
in O
three O
ways: O
(1 O

- B-DAT
channel, O
while O
color O
components O
are O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferent O
scaling O
factors O

- B-DAT
sample O
the O
original O
HR O
images O

- B-DAT
erate O
the O
corresponding O
LR O
images O

- B-DAT

- B-DAT

- B-DAT
ters O
will O
generate O
the O
output O

- B-DAT

- B-DAT
mum O
size O
of O
the O
sub-image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ble O
1 O

- B-DAT

- B-DAT

- B-DAT
volution O
layer O
[6, O
24] O
in O

- B-DAT

- B-DAT

- B-DAT
ters O
as O
the O
initial O
values O

- B-DAT
terfly” O
image O
from O
Set5 O
dataset O

- B-DAT
ture O
information O
and O
its O
normalized O

pixel O
value O
ranges O
from O
-0 B-DAT

- B-DAT
ter O
visualizing O
the O
intermediary O
of O

- B-DAT
age O
feature O
map O
can O
roughly O

- B-DAT
ment O
unit O
and O
compression O
unit O

- B-DAT
ing O
above-mentioned O
method. O
As O
illustrated O

- B-DAT
figures O
show O
that O
the O
later O

- B-DAT
creasing O
the O
pixel O
values O
to O

- B-DAT
atively O
clear O
contour O
profile. O
In O

- B-DAT
ure O
obviously O
surpasses O
the O
former O

- B-DAT
paring O
Figure O
5(a) O
with O
Figure O

- B-DAT
ure O
5(b) O
and O
the O
third O

- B-DAT
age. O
The O
bias O
term O
of O

- B-DAT
matically O
adjust O
the O
central O
value O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods, O
including O
bicubic, O
SRCNN O
[3 O

- B-DAT

- B-DAT

- B-DAT
vorably O
against O
state-of-the-art O
results O
on O

- B-DAT
man O
perception O
of O
image O
super-resolution O

- B-DAT
bara” O
image O
has O
serious O
artifacts O

- B-DAT
viously O
see O
that O
the O
proposed O

- B-DAT
tively O
clear O
in O
the O
proposed O

- B-DAT
put O
so O
that O
more O
information O

- B-DAT
age. O
The O
algorithms O
that O
take O

- B-DAT
chine O
with O
4.2GHz O
Intel O
i7 O

- B-DAT
ages O
in O
BSD100 O
and O
Urban100 O

- B-DAT
ages O
into O
several O
parts O
and O

- B-DAT
worthy O
that O
the O
proposed O
IDN O

- B-DAT
ban100 O
dataset O

- B-DAT
cient O
features O
for O
the O
reconstruction O

- B-DAT
posed O
approach O
achieves O
competitive O
results O

- B-DAT
mark O
datasets O
in O
terms O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pact O
network O
will O
be O
more O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
rable O
convolutions. O
In O
CVPR, O
pages O

- B-DAT
based O
super-resolution. O
IEEE O
Computer O
Graphics O

- B-DAT
plications, O
22(2):56–65, O
2002. O
2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works. O
In O
arXiv:1709.01507, O
2017. O
2 O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
ishick, O
S. O
Guadarrama, O
and O
T O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT
tional O
network O
for O
image O
super-resolution O

- B-DAT
resolution. O
In O
CVPR, O
pages O
624–632 O

- B-DAT

- B-DAT
ing O
very O
deep O
convolutional O
encoder-decoder O

- B-DAT
cal O
statistics. O
In O
CVPR, O
pages O

- B-DAT

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT
tent O
memory O
network O
for O
image O

- B-DAT
resolution O
as O
sparse O
representation O
of O

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT

- B-DAT

3 O
and O
×4 O
on O
dataset O
Set5 B-DAT

For O
testing, O
four O
benchmark O
datasets, O
Set5 B-DAT
[1], O
Set14 O
[39 O

networks O
for O
scale O
factor×3 O
on O
Set5 B-DAT

Set5 B-DAT
×2 O
33.66/0.9299 O
36.66/0.9542 O
37.53/0.9587 O
37.63/0.9588 O

scale O
factor×2,×3 O
and×4 O
on O
datasets O
Set5, B-DAT
Set14, O
BSD100 O
and O
Urban100. O
Dataset O

block) O
for O
scale O
×3 O
on O
Set5, B-DAT
in O
which O
the O
colorbar O
indicates O

performance O
of O
these O
networks O
on O
Set5 B-DAT
with O
scale O
factor O
×3. O
It O

performance O
of O
these O
networks O
on O
Set5 B-DAT

- B-DAT
age O
restoration. O
However, O
as O
the O

- B-DAT

- B-DAT
tle O
influence O
on O
the O
subsequent O

- B-DAT
tive O
learning O
process. O
The O
recursive O

- B-DAT

- B-DAT
tive O
fields. O
The O
representations O
and O

- B-DAT
vious O
states O
should O
be O
reserved O

- B-DAT
resolution O
and O
JPEG O
deblocking. O
Comprehensive O

- B-DAT
iments O
demonstrate O
the O
necessity O
of O

- B-DAT

- B-DAT

- B-DAT
quality O
version O
of O
x, O
D O

- B-DAT
der O
Grant O
Nos. O
91420201, O
61472187 O

- B-DAT

- B-DAT
search O
Fund. O
Jian O
Yang O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
noising O
aims O
to O
recover O
a O

- B-DAT
servation, O
which O
commonly O
assumes O
additive O

- B-DAT
sian O
noise O
with O
a O
standard O

- B-DAT

- B-DAT
resolution O
recovers O
a O
high-resolution O
(HR O

- B-DAT

- B-DAT

- B-DAT
trol O
the O
parameter O
number O
of O

- B-DAT
Recursive O
Convolutional O
Network O
(DRCN) O
[21 O

- B-DAT
gate O
training O
difficulty, O
Mao O
et O

- B-DAT

- B-DAT

- B-DAT
over, O
Zhang O
et O
al. O
[40 O

- B-DAT
path O
feed-forward O
architecture, O
where O
one O

- B-DAT
fluenced O
by O
its O
direct O
former O

- B-DAT

- B-DAT
ory. O
Some O
variants O
of O
CNNs O

- B-DAT

- B-DAT
cific O
prior O
state, O
namely O
restricted O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tent O
memory O
network O
(MemNet), O
which O

- B-DAT
ory O
block O
to O
explicitly O
mine O

- B-DAT
tion O
Net O
(FENet) O
first O
extracts O

- B-DAT

- B-DAT
tains O
a O
recursive O
unit O
and O

- B-DAT
science O
[6, O
25] O
that O
recursive O

- B-DAT
ist O
in O
the O
neocortex, O
the O

- B-DAT

- B-DAT
tive O
fields O
(blue O
circles O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ated O
from O
the O
previous O
memory O

- B-DAT

- B-DAT
ther, O
we O
present O
an O
extended O

- B-DAT

- B-DAT

- B-DAT
term O
memory O
should O
be O
reserved O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
even O
using O
a O
single O

- B-DAT
veal O
that O
the O
network O
depth O

- B-DAT
tion O
and O
achieved O
comparable O
performance O

- B-DAT
resolution O
convolutional O
neural O
network O
(SRCNN O

- B-DAT
dicts O
the O
nonlinear O
LR-HR O
mapping O

- B-DAT
volutional O
network, O
which O
significantly O
outperforms O

- B-DAT
cal O
shallow O
methods. O
The O
authors O

- B-DAT
tended O
CNN O
model, O
named O
Artifacts O

- B-DAT
tional O
Neural O
Networks O
(ARCNN) O
[7 O

- B-DAT

- B-DAT
ural O
sparsity O
of O
images O
[36 O

- B-DAT

- B-DAT
edge O
in O
the O
JPEG O
compression O

- B-DAT

- B-DAT

- B-DAT
ers O
to O
exploit O
large O
contextual O

- B-DAT
ing O
and O
adjustable O
gradient O
clipping O

- B-DAT
ization O
into O
a O
DnCNN O
model O

- B-DAT
age O
restoration O
tasks. O
To O
reduce O

- B-DAT

- B-DAT
connection O
to O
mitigate O
the O
training O

- B-DAT

- B-DAT
posed O
LapSRN O
to O
address O
the O

- B-DAT
curacy O
for O
SISR, O
which O
operates O

- B-DAT

- B-DAT
ages. O
Tai O
et O
al. O
[34 O

- B-DAT
work O
(DRRN) O
to O
address O
the O

- B-DAT

- B-DAT
lutional O
layer O
is O
used O
in O

- B-DAT
ture O
mapping, O
we O
have O

- B-DAT

- B-DAT

- B-DAT
ory O
block O
respectively. O
Finally, O
instead O

- B-DAT

- B-DAT

- B-DAT
age, O
our O
model O
uses O
a O

- B-DAT
notes O
the O
function O
of O
our O

- B-DAT
ber O
of O
training O
patches O
and O

- B-DAT
quality O
patch O
of O
the O
low-quality O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
block. O
Specifically, O
each O
residual O

- B-DAT

- B-DAT

- B-DAT
erate O
multi-level O
representations O
under O
different O

- B-DAT

- B-DAT
ory. O
Supposing O
there O
are O
R O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cursive O
unit. O
These O
representations O
are O

- B-DAT

- B-DAT

- B-DAT
vious O
memory O
blocks O
can O
be O

- B-DAT

- B-DAT
volutional O
layer O
(parameterized O
by O
W O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put O
from O
the O
ensemble O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
can O
get O
lost O
at O

- B-DAT
ward O
CNN O
process, O
and O
dense O

- B-DAT
ous O
layers O
can O
compensate O
such O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferent O
networks. O
(b) O
We O
convert O

- B-DAT

- B-DAT

- B-DAT
tral O
densities O
by O
integrating O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions, O
the O
latter O
layer O
absorbs O

- B-DAT

- B-DAT

- B-DAT
work O
– O
a O
very O
deep O

- B-DAT
works, O
inspired O
by O
LSTM, O
Highway O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tween O
MemNet O
and O
DRCN O
[21 O

- B-DAT
ule O
is O
a O
memory O
block O

- B-DAT
ules O
in O
DRCN, O
which O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
egy, O
which O
is O
imperative O
for O

- B-DAT
nected O
principle. O
In O
general, O
DenseNet O

- B-DAT
tion. O
In O
addition, O
DenseNet O
adopts O

- B-DAT

- B-DAT
tions O
in O
MemNet O
indeed O
play O

- B-DAT

- B-DAT

- B-DAT
nections. O
Average O
PSNR/SSIMs O
for O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ories O
from O
the O
last O
recursion O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tors O
are O
evaluated, O
including O
×2 O

- B-DAT
noising O
is O
used. O
As O
in O

- B-DAT
ing O
time O
and O
storage O
complexities O

- B-DAT

- B-DAT

- B-DAT
ent O
noise O
levels O
are O
all O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sions, O
are O
constructed O
(i.e., O
M6R6 O

- B-DAT
supervised O
MemNet, O
6 O
predictions O
are O

- B-DAT
tions, O
and O
is O
empirically O
set O

- B-DAT
mized O
via O
the O
mini-batch O
stochastic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nections. O
The O
reason O
is O
that O

- B-DAT

- B-DAT

- B-DAT
responding O
weights O
from O
all O
filters O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
we O
normalize O
the O
norms O

- B-DAT
ture O
map O
index O
l. O
We O

- B-DAT
ory O
block O
number O
increases. O
(3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity O
of O
our O
persistent O
memory O

- B-DAT
ent O
among O
different O
work, O
we O

- B-DAT
crease O
the O
parameters O
(filter O
number O

- B-DAT

- B-DAT
mance. O
With O
more O
training O
images O

- B-DAT
nificantly O
outperforms O
the O
state O
of O

- B-DAT

- B-DAT
lem O
in O
networks, O
we O
intend O

- B-DAT
plexity O
and O
accuracy. O
Fig. O
6 O

- B-DAT
notes O
the O
prediction O
of O
the O

- B-DAT
sult O
at O
the O
3rd O
prediction O

- B-DAT
creasing O
model O
complexity O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nal O
image O
is O
resized O
to O

- B-DAT
mance. O
However, O
in O
our O
MemNet O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rectly O
recovers O
the O
pillar. O
Please O

- B-DAT
isons O
for O
SISR. O
SRCNN O
[8 O

- B-DAT
sults O
on O
Classic5 O
and O
LIVE1 O

- B-DAT
erated O
by O
their O
corresponding O
public O

- B-DAT
work O
structures: O
M4R6, O
M6R6, O
M6R8 O

- B-DAT
posed O
deepest O
network O
M10R10 O
achieves O

- B-DAT

- B-DAT

- B-DAT
ory O
network O
(MemNet) O
is O
proposed O

- B-DAT

- B-DAT
ous O
CNN O
architectures. O
In O
each O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ous O
memory O
blocks O
are O
sent O

- B-DAT
resolution O
and O
JPEG O
deblocking O
simultaneously O

- B-DAT
hensive O
benchmark O
evaluations O
well O
demonstrate O

- B-DAT
riority O
of O
our O
MemNet O
over O

- B-DAT

- B-DAT

- B-DAT
ative O
neighbor O
embedding. O
In O
BMVC O

- B-DAT

- B-DAT
age O
denoising O
by O
sparse O
3-D O

- B-DAT

- B-DAT
bridge, O
MA: O
MIT O
Press, O
2001 O

- B-DAT
tifacts O
reduction O
by O
a O
deep O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
volutional O
networks. O
In O
NIPS, O
2008 O

- B-DAT

- B-DAT
ing O
of O
non-parametric O
image O
restoration O

- B-DAT
shick, O
S. O
Guadarrama, O
and O
T O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT
tional O
network O
for O
image O
super-resolution O

- B-DAT

- B-DAT
based O
learning O
applied O
to O
document O

- B-DAT
ings O
of O
the O
IEEE, O
1998 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ric O
skip O
connections. O
In O
NIPS O

- B-DAT
cal O
statistics. O
In O
ICCV, O
2001 O

- B-DAT
stricted O
boltzmann O
machines. O
In O
ICML O

- B-DAT

- B-DAT

- B-DAT
compressed O
images. O
In O
CVPR, O
2016 O

- B-DAT

- B-DAT

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT
up O
using O
sparse-representations. O
Curves O
and O

- B-DAT
yond O
a O
gaussian O
denoiser: O
Residual O

result O
produced O
by O
ENet-PAT O
at O
4x B-DAT
super-resolution O
on O
an O
image O
from O

fully O
convolutional O
network O
architecture O
for O
4x B-DAT
super-resolution O
which O
only O
learns O
the O

an O
image O
from O
ImageNet O
for O
4x B-DAT
super-resolution. O
Despite O
reaching O
state-of-the-art O
results O

trained O
with O
different O
losses O
at O
4x B-DAT
super-resolution O
on O
images O
from O
ImageNet O

different O
combinations O
of O
losses O
at O
4x B-DAT
super O
resolution. O
ENet-E O
yields O
the O

methods O
with O
our O
results O
at O
4x B-DAT
super-resolution O
on O
an O
image O
from O

PSNR O
for O
different O
methods O
at O
4x B-DAT
super-resolution. O
ENet-E O
achieves O
state-of-the-art O
results O

both O
ENet-E O
and O
ENet-PAT O
at O
4x B-DAT
super- O
resolution O
side-by-side, O
and O
were O

an O
image O
from O
BSD100 O
at O
4x B-DAT
super-resolution. O
ENet- O
PAT’s O
result O
looks O

on O
average O
per O
image O
at O
4x B-DAT
super-resolution O

adversarial O
discrimina- O
tive O
network O
at O
4x B-DAT
super-resolution. O
As O
in O
the O
generative O

on O
average O
per O
image O
at O
4x B-DAT
super-resolution O
on O
Set5/Set14, O
though O
EnhanceNet O

the O
result O
of O
ENet-PAT O
at O
4x B-DAT
super-resolution O
with O
the O
current O
state O

super-resolution O
in O
Fig. O
4. O
Although O
4x B-DAT
super-resolution O
is O
a O
greatly O
more O

are O
lost O
completely O
in O
the O
4x B-DAT
downsampled O
image O
are O
more O
accurate O

image O
with O
sharper O
textures O
at O
4x B-DAT
super-resolution O
that O
even O
out- O
performs O

on O
images O
from O
ImageNet O
at O
4x B-DAT
super-resolution. O
Computing O
the O
texture O
matching O

edges O
and O
overly O
smooth O
textures O
(4x B-DAT
super-resolution). O
Furthermore, O
these O
models O
are O

that O
the O
network O
produces O
at O
4x B-DAT
super-resolution. O
While O
ENet-E O
significantly O
sharpens O

downsampled O
input O
2x O
downsampled O
input O
4x B-DAT
downsampled O
input O
IHR O

VDSR O
[7] O
2x O
DRCN O
[8] O
4x B-DAT
ENet-PAT O
IHR O

missing) O
with O
our O
model O
at O
4x B-DAT
super-resolution O
(93.75% O
of O
all O
pixels O

Bruna O
et O
al. O
[2] O
at O
4x B-DAT
super-resolution. O
ENet-PAT O
produces O
images O
with O

different O
methods O
at O
2x O
and O
4x B-DAT
super-resolution. O
Similar O
to O
PSNR, O
ENet-PAT O

IFC O
for O
different O
methods O
at O
4x B-DAT
super-resolution. O
Best O
performance O
shown O
in O

an O
image O
from O
ImageNet O
at O
4x B-DAT
super-resolution. O
While O
producing O
an O
overall O

on O
images O
of O
faces O
at O
4x B-DAT
super O
resolution. O
ENet-PAT O
produces O
artifacts O

ENet-P O
ENet-EA O
ENet-PA O
ENet-EAT O
ENet-PAT O
Set5 B-DAT
28.42 O
31.74 O
28.28 O
28.15 O
27.20 O

Set5 B-DAT
28.42 O
30.14 O
30.28 O
30.31 O
30.48 O

and O
processes O
images O
in O
9ms O
(Set5), B-DAT
18ms O
(Set14), O
12ms O
(BSD100) O
and O

image O
at O
4x O
super-resolution O
on O
Set5 B-DAT

on O
the O
butterfly O
image O
of O
Set5 B-DAT

Set5 B-DAT
33.66 O
36.54 O
30.14 O
36.49 O
36.66 O

Set5 B-DAT
0.9299 O
0.9537 O
0.9544 O
0.9537 O
0.9542 O

Set5 B-DAT
0.8104 O
0.8548 O
0.8603 O
0.8619 O
0.8628 O

Set5 B-DAT
2.329 O
3.191 O
3.248 O
3.166 O
2.991 O

ENet-P O
ENet-EA O
ENet-PA O
ENet-EAT O
ENet-PAT O
Set5 B-DAT
28 I-DAT

Set5 B-DAT
28 I-DAT

Set5 B-DAT
33 I-DAT

Set5 B-DAT
0 I-DAT

Set5 B-DAT
0 I-DAT

Set5 B-DAT
2 I-DAT

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
TOG O

rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put. O
Traditionally, O
the O
performance O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
quality. O
As O
a O
result O

- B-DAT
rics O
tend O
to O
produce O
over-smoothed O

- B-DAT
frequency O
textures O
and O
do O
not O

- B-DAT
thesis O
in O
combination O
with O
a O

- B-DAT
accurate O
reproduction O
of O
ground O
truth O

- B-DAT
ing. O
By O
using O
feed-forward O
fully O

- B-DAT
works O
in O
an O
adversarial O
training O

- B-DAT
nificant O
boost O
in O
image O
quality O

- B-DAT
fectiveness O
of O
our O
approach, O
yielding O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cent O
years. O
The O
problem O
is O

- B-DAT

- B-DAT
ferent O
HR O
images O
can O
give O

- B-DAT

- B-DAT

- B-DAT
lem O
becomes O
worse, O
rendering O
SISR O

- B-DAT
lem. O
Despite O
considerable O
progress O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
are O
still O
far O
from O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ploy: O
most O
systems O
minimize O
the O

- B-DAT

- B-DAT
construction O
from O
the O
LR O
observation O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
network O
architecture, O
we O

- B-DAT
bination O
with O
adversarial O
training O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
perceptual O
metrics O

- B-DAT
zos O
[11] O
are O
based O
on O

- B-DAT

- B-DAT
lahi O
and O
Moeslund O
[37] O
and O

- B-DAT
based O
models O
that O
either O
exploit O

- B-DAT
ent O
scales O
within O
a O
single O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works O
to O
the O
task O
of O

- B-DAT
gration O
to O
learn O
a O
mapping O

- B-DAT

- B-DAT
tion, O
the O
results O
tend O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ject O
recognition O
system O
like O
VGG O

- B-DAT

- B-DAT

- B-DAT
tion O
with O
an O
adversarial O
network O

- B-DAT
sually O
implausible O
artifacts O
without O
the O

- B-DAT

- B-DAT

- B-DAT
ation O
d O
is O
non-injective O
and O

- B-DAT

- B-DAT
ity O
in O
SISR: O
since O
downsampling O

- B-DAT

- B-DAT
fore, O
even O
state-of-the-art O
models O
learn O

- B-DAT
ple O
in O
Fig. O
2, O
where O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lutional O
layers O
enables O
training O
of O

- B-DAT
put O
image O
of O
arbitrary O
size O

- B-DAT

- B-DAT

- B-DAT
ported O
to O
produce O
checkerboard O
artifacts O

- B-DAT
zontal O
bars O
of O
1×2 O
pixels O

- B-DAT
not O
be O
distinguished O
anymore O
since O

- B-DAT

- B-DAT
pling O
of O
the O
feature O
activations O

- B-DAT

- B-DAT
volution O
layer O
after O
all O
upsampling O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puting O
distances O
in O
image O
space O

- B-DAT

- B-DAT

- B-DAT
tion O
of O
the O
popular O
VGG-19 O

- B-DAT
ally O
decrease O
the O
spatial O
dimension O

- B-DAT
tract O
higher-level O
features O
in O
higher O

- B-DAT

- B-DAT

- B-DAT
atively O
by O
matching O
statistics O
extracted O

- B-DAT

- B-DAT
tween O
the O
feature O
activations O
φ(I O

- B-DAT

- B-DAT
ages O
[24, O
53], O
however O
a O

- B-DAT
resolution O
textures O
during O
inference, O
we O

- B-DAT
ture O
loss O
LT O
patch-wise O
during O

- B-DAT
fore O
learns O
to O
produce O
images O

- B-DAT
tures O
as O
the O
high-resolution O
images O

- B-DAT

- B-DAT

- B-DAT
tures. O
Empirically, O
we O
found O
a O

- B-DAT
ation O
and O
the O
overall O
perceptual O

- B-DAT
imize O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
inative O
network O
as O
we O
found O

- B-DAT
tor O
from O
overpowering O
the O
generator O

- B-DAT
ing O
learning O
strategy O
yields O
better O

- B-DAT
vious O
training O
batch O
and O
only O

- B-DAT
ther O
details O
are O
specified O
in O

- B-DAT
ously O
introduced O
loss O
functions. O
After O

- B-DAT
tive O
and O
quantitative O
evaluation O
of O

- B-DAT
tors O
are O
given O
in O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formations, O
the O
network O
is O
given O

- B-DAT
alistic O
textures O
when O
trained O
with O

- B-DAT

- B-DAT
work O
sometimes O
produces O
unpleasing O
high-frequency O

- B-DAT

- B-DAT

- B-DAT
PAT O
produces O
perceptually O
more O
realistic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
struction, O
but O
even O
the O
state-of-the-art O

- B-DAT

- B-DAT
teristics O
as O
previous O
approaches. O
The O

- B-DAT
age O
than O
ENet-E. O
On O
the O

- B-DAT

- B-DAT
alistic O
textures. O
Comparisons O
with O
further O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pose O
to O
use O
the O
performance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nition O
models O
as O
a O
metric O

- B-DAT

- B-DAT
plement O
pixel-based O
benchmarks O
such O
as O

- B-DAT

- B-DAT

- B-DAT
ric O
similar O
to O
ours O
to O

- B-DAT

-50 B-DAT
[6, O
20] O
as O
this O
class O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lenge O
(ILSVRC) O
[44]. O
For O
the O

- B-DAT

- B-DAT
bels. O
The O
original O
images O
are O

-1 B-DAT
and O
top-5 O
errors O
as O
well O

- B-DAT
fications. O
The O
results O
are O
shown O

- B-DAT
ison, O
some O
of O
the O
results O

- B-DAT
formance O
followed O
by O
DRCN O
[26 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tual O
quality O
which O
is O
reflected O

- B-DAT
ject O
recognition O
benchmark O
matches O
human O

- B-DAT
ter O
than O
PSNR O
does. O
The O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mance O
roughly O
coincides O
with O
the O

- B-DAT
age O
quality O
in O
this O
benchmark O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
imize O
the O
Euclidean O
loss, O
we O

- B-DAT

- B-DAT
erated O
by O
ENet-PAT O
which O
have O

- B-DAT
ages O
upsampled O
with O
bicubic O
interpolation O

- B-DAT

- B-DAT

- B-DAT
resolution O
side-by-side, O
and O
were O
asked O

- B-DAT
age O
produced O
by O
ENet-PAT O
91.0 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
error O
0.506 O
0.477 O
0.454 O
0.449 O

-5 B-DAT
error O
0.266 O
0.242 O
0.224 O
0.214 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
PAT’s O
result O
looks O
more O
natural O

- B-DAT
lutional O
network O
at O
test O
time O

- B-DAT
gence O
rates O
depend O
on O
the O

- B-DAT
tions. O
Although O
not O
optimized O
for O

- B-DAT

- B-DAT
ducing O
state-of-the-art O
results O
by O
both O

- B-DAT
itative O
measures O
by O
training O
with O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tic, O
they O
do O
not O
match O

- B-DAT
wise O
basis. O
Furthermore, O
the O
adversarial O

- B-DAT
eas. O
This O
is O
a O
result O

- B-DAT
work O
and O
apply O
shrinking O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tails O
and O
additional O
comparisons. O
A O

- B-DAT
tation O
of O
ENet-PAT O
can O
be O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
image O
models O
using O
a O

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
ceptual O
similarity O
metrics O
based O
on O

- B-DAT

- B-DAT

- B-DAT
based O
super-resolution. O
IEEE O
CG&A, O
22(2):56–65 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
nets. O
In O
NIPS O

- B-DAT
tional O
neural O
networks O
for O
direct O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
istration. O
CVGIP: O
Graphical O
models O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
sparse O
regression O
and O
natural O

- B-DAT
ceptual O
image O
quality O
assessment O
using O

- B-DAT
cian O
pyramid. O
Electronic O
Imaging, O
2016(16):1–6 O

- B-DAT
jani, O
J. O
Totz, O
Z. O
Wang O

- B-DAT

- B-DAT
age O
super-resolution O
using O
a O
generative O

- B-DAT
resolved O
faces O
for O
improved O
face O

- B-DAT
lance O
video. O
In O
ICB, O
2007 O

- B-DAT
manan, O
P. O
Dollár, O
and O
C O

- B-DAT
mon O
objects O
in O
context. O
In O

- B-DAT
strained O
sparse O
coding O
for O
single O

- B-DAT

- B-DAT
earities O
improve O
neural O
network O
acoustic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
checkerboard/, O
2016 O

- B-DAT

- B-DAT

- B-DAT
sentation O
learning O
with O
deep O
convolutional O

- B-DAT
sarial O
networks. O
In O
ICLR, O
2016 O

- B-DAT
tion O
based O
noise O
removal O
algorithms O

- B-DAT

- B-DAT

- B-DAT
mation O
fidelity O
criterion O
for O
image O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT
structure O
preserving O
image O
super O
resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ture O
networks: O
Feed-forward O
synthesis O
of O

- B-DAT
ized O
images. O
In O
ICML, O
2016 O

- B-DAT

- B-DAT
ment O
photographs. O
In O
ECCV, O
2016 O

- B-DAT
similarities O
for O
single O
frame O
super-resolution O

- B-DAT

- B-DAT
resolution: O
a O
benchmark. O
In O
ECCV O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
as O
sparse O
representation O
of O

- B-DAT
age O
super-resolution O
via O
sparse O
representation O

- B-DAT

- B-DAT
inative O
generative O
networks. O
In O
ECCV O

- B-DAT
age O
super-resolution O
by O
retrieving O
web O

- B-DAT

- B-DAT

- B-DAT
tion. O
In O
ECCV, O
2016 O

- B-DAT
fold. O
In O
ECCV, O
2016 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cally O
similar O
textures O
between O
Iest O

- B-DAT
tween O
faithful O
texture O
generation O
and O

- B-DAT

- B-DAT

-4 B-DAT

- B-DAT

-128 B-DAT

- B-DAT
age O
since O
the O
network O
is O

- B-DAT
pleasant O
results O

- B-DAT
sarial O
network O
used O
for O
the O

- B-DAT
mon O
design O
patterns O
[13] O
and O

- B-DAT
tive O
network O
at O
4x O
super-resolution O

- B-DAT
tions O
[11] O
and O
strided O
convolutions O

- B-DAT
duce O
a O
classification O
label O
between O

- B-DAT
put O
which O
renders O
training O
more O

- B-DAT

- B-DAT
ual O
image O
that O
cancel O
out O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
which O
leads O
to O
loss O

- B-DAT

- B-DAT
EA O
and O
ENet-EAT O
are O
shown O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tor. O
ENet-PAT O
is O
the O
only O

- B-DAT
tails O
and O
it O
is O
visually O

- B-DAT
hanceNet O
is O
even O
faster O
than O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pare O
the O
result O
of O
ENet-PAT O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
manding O
task O
than O
2x O
super-resolution O

- B-DAT
parable O
in O
quality. O
Small O
details O

- B-DAT

- B-DAT
performs O
the O
current O
state O
of O

- B-DAT

- B-DAT
PAT’s O
result O
and O
looks O
very O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceptual O
quality O
of O
ENet-PAT’s O
results O

- B-DAT

- B-DAT

-4 B-DAT
ENet-PAT-128 O
ENet-PAT-16 O
(default) O
IHR O

- B-DAT

- B-DAT

- B-DAT

-4 B-DAT

- B-DAT

-128 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
jority O
of O
subjects O
in O
our O

- B-DAT

- B-DAT

- B-DAT
PAT O
trained O
on O
MSCOCO O
struggles O

- B-DAT
cally O
looking O
faces O
at O
high O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
PAT-F O
has O
significantly O
better O
performance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT

- B-DAT
mization. O
2015 O

- B-DAT
earities O
improve O
neural O
network O
acoustic O

- B-DAT

- B-DAT

- B-DAT
sentation O
learning O
with O
deep O
convolutional O

- B-DAT
sarial O
networks. O
In O
ICLR, O
2016 O

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT
mation O
fidelity O
criterion O
for O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
inative O
generative O
networks. O
In O
ECCV O

Set5 B-DAT
×2 O
33.66/0.9299 O
36.54/0.9544 O
36.66/0.9542 O
36.54/0.9537 O

3 O
and O
×4 O
on O
datasets O
Set5, B-DAT
Set14, O
B100 O
and O
Urban100. O
Red O

we O
use O
four O
datasets. O
Datasets O
Set5 B-DAT
[19] O
and O
Set14 O
[32] O
are O

factor O
×3 O
on O
the O
dataset O
Set5 B-DAT

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
a O
deeply-recursive O
convolutional O
network O

- B-DAT
sions). O
Increasing O
recursion O
depth O
can O

- B-DAT
mance O
without O
introducing O
new O
parameters O

- B-DAT
ploding/vanishing O
gradients. O
To O
ease O
the O

- B-DAT
ing, O
we O
propose O
two O
extensions O

- B-DAT

- B-DAT

- B-DAT
ods O
by O
a O
large O
margin O

- B-DAT

- B-DAT
frequency O
components. O
For O
example, O
if O

- B-DAT
tern O
with O
smoothed O
edges O
contained O

- B-DAT
priately O
sharpened. O
As O
SR O
is O

- B-DAT

- B-DAT
ious O
computer O
vision O
tasks O
often O

- B-DAT
tive O
fields O
(224x224 O
common O
in O

- B-DAT
volutional O
(conv.) O
layer O
with O
filter O

- B-DAT
mediate O
representation O
can O
be O
used O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
network O
(DRCN). O
DRCN O
repeatedly O

- B-DAT
sions O
are O
performed. O
Our O
network O

- B-DAT

- B-DAT
dient O
descent O
method O
does O
not O

- B-DAT

- B-DAT

- B-DAT
sions. O
As O
each O
recursion O
leads O

- B-DAT

- B-DAT

- B-DAT
put) O
and O
a O
high-resolution O
image O

- B-DAT
ever, O
is O
likely O
to O
be O

- B-DAT
construction. O
This O
is O
particularly O
effective O

- B-DAT
resolution O
method O
deeply O
recursive O
in O

- B-DAT
sive O
network O
in O
two O
ways O

- B-DAT

- B-DAT
connection. O
Our O
method O
demonstrates O
state-of-the-art O

- B-DAT
formance O
in O
common O
benchmarks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ping O
from O
LR O
to O
HR O

- B-DAT
tention O
to O
find O
better O
regression O

- B-DAT
tional O
neural O
network O
(CNN) O
[5 O

- B-DAT

- B-DAT
lutional O
neural O
network O
(SRCNN) O
[5 O

- B-DAT
sibility O
of O
an O
end-to-end O
approach O

- B-DAT
creases O
the O
number O
of O
parameters O

- B-DAT
lutional O
network O
that O
models O
long-range O

- B-DAT
quential O
data, O
have O
seen O
limited O

- B-DAT
lutional O
network O
in O
a O
separate O

- B-DAT

- B-DAT

- B-DAT
fitting. O
To O
overcome O
overfitting, O
Liang O

- B-DAT

- B-DAT
folded O
layers. O
They O
show O
that O

- B-DAT
tectures O

- B-DAT
mance O
for O
super-resolution. O
We O
apply O

-1 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ence O
net O
solves O
the O
task O

- B-DAT
ture O
maps O
in O
the O
inference O

- B-DAT
ate O
representation O
used O
to O
pass O

- B-DAT
resent O
its O
feature O
maps O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cursion O
applies O
the O
same O
convolution O

- B-DAT
sive O
layer O
represent O
the O
high-resolution O

- B-DAT

- B-DAT

- B-DAT
tion O
net O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formative O
than O
the O
raw O
intensities O

- B-DAT

- B-DAT
terpolated O
input O
image O
(to O
the O

- B-DAT
net O
functions: O
embedding, O
inference O
and O

- B-DAT
spectively. O
Our O
model O
is O
the O

- B-DAT
putes O
the O
matrix O
output O
H0 O

- B-DAT
ence O
net O
f2. O
Hidden O
layer O

- B-DAT
position O
of O
the O
same O
elementary O

- B-DAT

- B-DAT

- B-DAT
bedding O
net. O
The O
formula O
is O

- B-DAT

- B-DAT
dance O
with O
the O
limited O
success O

- B-DAT
havior. O
Long O
term O
components O
approach O

- B-DAT
ing O
an O
exact O
copy O
of O

- B-DAT
cursive O
layer O
needs O
to O
keep O

- B-DAT

- B-DAT

- B-DAT
timal O
recursion O
issues, O
we O
propose O

- B-DAT
construction O
net O
now O
outputs O
D O

- B-DAT
tions O
are O
simultaneously O
supervised O
during O

- B-DAT
ing. O
The O
optimal O
weights O
are O

- B-DAT
diate O
layers O
for O
a O
convolutional O

- B-DAT
tion O
error O
while O
improving O
the O

- B-DAT
nificant O
differences O
between O
our O
recursive-supervision O

- B-DAT

- B-DAT
ciate O
a O
unique O
classifier O
for O

- B-DAT
ditional O
layer, O
a O
new O
classifier O

- B-DAT
ference O
is O
that O
Lee O
et O

- B-DAT
sifiers O
during O
testing. O
However, O
an O

- B-DAT
diate O
predictions O
significantly O
boosts O
the O

- B-DAT

- B-DAT
ing/exploding O
gradients O
along O
one O
backpropagation O

- B-DAT

- B-DAT
sion: O
skip-connection. O
For O
SR, O
input O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eral O
super-resolution O
methods O
[28, O
29 O

- B-DAT

- B-DAT
tion O
under O
recursive-supervision O
(Figure O
3(a O

- B-DAT

- B-DAT
connection O
can O
take O
various O
functional O

- B-DAT
ple, O
input O
can O
be O
concatenated O

- B-DAT

- B-DAT
ing O
set O
is O
minimized. O
This O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gresses, O
α O
decays O
to O
boost O

- B-DAT
put O

- B-DAT
jective O
using O
mini-batch O
gradient O
descent O

- B-DAT
propagation O
(LeCun O
et O
al. O
[15 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mark O
[29, O
28, O
5]. O
Dataset O

- B-DAT
ages O
failed O
by O
existing O
methods O

- B-DAT
folded, O
the O
longest O
chain O
from O

- B-DAT
mentum O
parameter O
to O
0.9 O
and O

- B-DAT
ing O
images O
are O
split O
into O

- B-DAT

- B-DAT

- B-DAT
lutions, O
we O
set O
all O
weights O

- B-DAT

- B-DAT
rameters O
except O
the O
weights O
used O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cause O
human O
vision O
is O
much O

- B-DAT
tensity O
than O
in O
color O

- B-DAT

- B-DAT
parison, O
however, O
we O
also O
crop O

- B-DAT
isting O
methods O
use O
slightly O
different O

- B-DAT
uation O
on O
several O
datasets. O
Our O

- B-DAT
isting O
methods O
in O
all O
datasets O

- B-DAT
tive O
to O
patterns. O
In O
contrast O

- B-DAT

- B-DAT

- B-DAT
ploiting O
a O
large O
image O
context O

- B-DAT

- B-DAT
connection. O
We O
have O
demonstrated O
that O

- B-DAT
forms O
existing O
methods O
by O
a O

- B-DAT
der O
to O
use O
image-level O
context O

- B-DAT

- B-DAT
works, O
IEEE O
Transactions O
on, O
5(2 O

- B-DAT

- B-DAT
projection O
residuals. O
In O
International O
Conference O

- B-DAT

- B-DAT
resolution O
using O
deep O
convolutional O
networks O

- B-DAT
ing O
low-level O
vision. O
IJCV, O
2000 O

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
transformed O
self-exemplars. O
In O

- B-DAT
istration. O
CVGIP: O
Graphical O
models O
and O

- B-DAT

- B-DAT

- B-DAT
ize O
recurrent O
networks O
of O
rectified O

- B-DAT
based O
learning O
applied O
to O
document O

- B-DAT
ings O
of O
the O
IEEE, O
86(11 O

- B-DAT
supervised O
nets. O
arXiv O
preprint O
arXiv:1409.5185 O

- B-DAT
tional O
networks O
for O
semantic O
segmentation O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cal O
statistics. O
In O
ICCV, O
2001 O

- B-DAT
ternational O
Conference O
on O
Machine O
Learning O

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT

- B-DAT

- B-DAT
cation. O
In O
NIPS, O
2012. O
2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
via O
sparse O
representation. O
TIP O

- B-DAT
up O
using O
sparse-representations. O
In O
Curves O

ARFL+ O
[26] O
on O
three O
dataset: O
Set5, B-DAT
Set14 O
and O
BSD100. O
The O
scaling O

PSNR O
and O
SSIM O
results O
on O
Set5 B-DAT

The O
evaluation O
on O
Set5 B-DAT
is O
shown O
in O
Table O
3 O

as O
significant O
as O
that O
on O
Set5, B-DAT
but O
we O
can O
still O
observe O

is O
similar O
than O
that O
on O
Set5 B-DAT

a O
single O
30 O
layer O
network. O
Set5 B-DAT
Set14 O
BSD100 O

The O
evaluation O
on O
Set5 B-DAT

a O
single O
30 O
layer O
network. O
Set5 B-DAT

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
Proc O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work O
for O
image O
restoration O
such O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
layers O
act O
as O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
Recently, O
deep O
neural O
networks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lutional O
neural O
network O
(CNN)-based O
framework O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
layers, O
as O
shown O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works O
[11], O
we O
add O
skip O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT

- B-DAT

-2 B-DAT

- B-DAT

-4 B-DAT

- B-DAT

-4 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion. O
For O
our O
method, O
which O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-200 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-4 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

for O
single O
image O
super-resolution O
on O
Set5 B-DAT

Table O
5 O
shows O
results O
on O
Set5 B-DAT
[4]. O
Again, O
we O
can O
observe O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
chine O
learning O
and O
its O
applications O

- B-DAT

- B-DAT
erations. O
In O
order O
to O
draw O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
doing O
compression O
of O
high O
image O

- B-DAT
fect O
of O
white O
balancing. O
Further O

- B-DAT
uate O
them O
on O
the O
benchmark O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
based O
on O
Wiener O
filter O

- B-DAT

-30, B-DAT
2006 O

- B-DAT

- B-DAT
Gaussian O
noise O
modeling O
and O
fitting O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion. O
In O
CVPR, O
pages O
3431–3440 O

- B-DAT

- B-DAT

- B-DAT
decoder O
networks O
with O
symmetric O
skip O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion. O
IEEE O
T. O
Image O
Process O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

the O
four O
testing O
datasets O
(i.e., O
Set5 B-DAT
and O
Set14, O
BSD100 O
and O
Urban100 O

Set5 B-DAT
3 O
33.18 O
/ O
0.9152 O
33.67 O

super-resolution O
results O
of O
“butterfly” O
from O
Set5 B-DAT
dataset O
with O
upscaling O
factor O
3 O

the O
four O
testing O
datasets O
(i.e., O
Set5 B-DAT

Set5 B-DAT
3 I-DAT
33.18 O
/ O
0.9152 O
33.67 O

super-resolution O
results O
of O
“butterfly” O
from O
Set5 B-DAT

Gaussian O
denoising, O
SISR O
with O
multiple O
upscaling B-DAT
factors, O
and O
JPEG O
deblocking O
with O

noise O
level, O
SISR O
with O
multiple O
upscaling B-DAT
factors, O
and O
JPEG O
deblocking O
with O

levels, O
down-sampled O
images O
with O
multiple O
upscaling B-DAT
factors, O
and O
JPEG O
images O
with O

model O
for O
all O
the O
three O
upscaling B-DAT
factors O
(i.e., O
2, O
3 O
and O

butterfly” O
from O
Set5 O
dataset O
with O
upscaling B-DAT
factor O
3 O

image O
from O
Urban100 O
dataset O
with O
upscaling B-DAT
factor O
4 O

bicubically O
interpolated O
low-resolution O
images O
with O
upscaling B-DAT
factor O
2 O
(upper O
middle) O
and O

- B-DAT

- B-DAT

- B-DAT
works, O
Residual O
Learning, O
Batch O
Normalization O

- B-DAT

- B-DAT
stitute O
of O
Technology, O
Harbin O
150001 O

- B-DAT

- B-DAT
nic O
University, O
Hong O
Kong O
(e-mail O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
backs. O
First, O
those O
methods O
generally O

- B-DAT
timization O
problem O
in O
the O
testing O

- B-DAT

- B-DAT
based O
methods O
can O
hardly O
achieve O

- B-DAT

- B-DAT
mance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
larization O
and O
learning O
methods O
for O

- B-DAT

- B-DAT
allel O
computation O
on O
modern O
powerful O

- B-DAT
noised O
image O
x̂, O
the O
proposed O

- B-DAT
tion O
technique O
is O
further O
introduced O

- B-DAT

- B-DAT
age O
deblocking O
problem O
can O
be O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
results O
when O
extended O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
remarks O
are O
given O
in O

- B-DAT

- B-DAT
age O
denoising. O
In O
[25], O
stacked O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tween O
depth O
and O
width O
[19 O

- B-DAT

- B-DAT

- B-DAT
resolution O
[31] O
and O
color O
image O

- B-DAT

- B-DAT
scent O
(SGD) O
has O
been O
widely O

- B-DAT

- B-DAT
linearity O
inputs O
during O
training. O
Batch O

- B-DAT
rating O
a O
normalization O
step O
and O

- B-DAT

- B-DAT
tion O
for O
CNN-based O
image O
denoising O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
filters O
to O
be O
3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ping O
is O
more O
like O
an O

- B-DAT

- B-DAT
rithms O
and O
network O
architecture. O
Note O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tributed O
to O
the O
internal O
covariate O

- B-DAT
serve O
that, O
with O
batch O
normalization O

- B-DAT

- B-DAT
malization O
offers O
some O
merits O
for O

- B-DAT
ating O
internal O
covariate O
shift O
problem O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eter, O
fk O
∗ O
x O
stands O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stage O
TNRD O
from O
three O
aspects O

- B-DAT

- B-DAT

- B-DAT
sian O
distributed O
(or O
the O
noise O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing, O
we O
set O
the O
range O

- B-DAT

- B-DAT
B. O
We O
use O
color O
version O

- B-DAT
noising O
tasks, O
as O
in O
[35 O

- B-DAT
resolution O
image O
with O
downscaling O
factors O

- B-DAT

- B-DAT

-3 B-DAT

-3, B-DAT
we O
adopt O
different O
test O
set O

- B-DAT

- B-DAT

- B-DAT

-3 B-DAT

- B-DAT

- B-DAT
ments O
are O
carried O
out O
in O

- B-DAT

- B-DAT

- B-DAT

15 O
31.07 O
31.37 O
31.21 O
- B-DAT
31.24 O
31.42 O
31.73 O
31.61 O
σ O

50 O
25.62 O
25.87 O
25.67 O
26.03 O
- B-DAT
25.97 O
26.23 O
26.23 O

- B-DAT

- B-DAT
B/CDnCNN-B O
and O
DnCNN-3 O
on O
GPU O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
training O
based O
methods O
(i.e O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tures O
meet O
well O
with O
the O

- B-DAT

-5 B-DAT
illustrate O
the O
visual O
results O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
7. O
One O
can O
see O
that O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
v5 O
deep O
learning O
library O
to O

-3 B-DAT
model O
is O
trained O
for O
three O

-3 B-DAT
with O
the O
specific O
state-of-the-art O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

203.1 O
25.4 O
1.42 O
2.11 O
/ O
- B-DAT
0.45 O
/ O
0.010 O
0.74 O

- B-DAT

- B-DAT

- B-DAT

-3 B-DAT
is O
compared O
with O
two O
state-of-the-art O

- B-DAT

- B-DAT

-3 B-DAT
model O
for O
the O
three O
different O

-3 B-DAT
outperforms O
AR-CNN O
by O
about O
0.3dB O

-3 B-DAT
and O
VDSR O
can O
produce O
sharp O

- B-DAT
ods. O
As O
one O
can O
see O

-3 B-DAT
can O
recover O
the O
straight O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

0.8861 O
40 O
33.34 O
/ O
0.8953 O
- B-DAT
33.77 O
/ O
0.9003 O

0.9090 O
40 O
33.63 O
/ O
0.9198 O
- B-DAT
33.96 O
/ O
0.9247 O

- B-DAT

- B-DAT
3 O
can O
produce O
visually O
pleasant O

- B-DAT
mance. O
Unlike O
traditional O
discriminative O
models O

- B-DAT

- B-DAT
scaling O
factors, O
and O
JPEG O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
larization O
method O
for O
total O
variation-based O

- B-DAT
nition, O
2007, O
pp. O
1–8 O

- B-DAT
agation O
with O
learned O
higher-order O
Markov O

- B-DAT
tion,” O
in O
IEEE O
Conference O
on O

- B-DAT
inative O
non-blind O
deblurring,” O
in O
IEEE O

- B-DAT

- B-DAT
mation O
Processing O
Systems, O
2012, O
pp O

- B-DAT

-3 B-DAT
/ O
30.02dB O

- B-DAT

- B-DAT

-3 B-DAT
/ O
32.73dB O

- B-DAT

- B-DAT

-3 B-DAT
/ O
29.70dB O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

on O
the O
fol- O
lowing O
datasets: O
Set5 B-DAT
[4], O
Set14 O
[46], O
B100 O
[30 O

DSRN O
with O
×2 O
upscaling O
on O
Set5 B-DAT
dataset O

DSRN O
with O
×2 O
upscaling O
on O
Set5 B-DAT

LR O
images O
directly O
and O
learned O
upscaling B-DAT
filters O
in O
the O
last O
layer O

the O
same O
as O
the O
SR O
upscaling B-DAT
factor O

of O
our O
DSRN O
with O
×2 O
upscaling B-DAT
on O
Set5 O
dataset O

performance O
drop O
across O
all O
three O
upscaling B-DAT
scales O
when O
changing O
from O
shared O

on O
Set O
14 O
with O
×3 O
upscaling B-DAT

recent O
SR O
methods O
for O
×3 O
upscaling B-DAT
on O
Set O
14 O

images O
on O
Set14 O
with O
x3 O
upscaling B-DAT
among O
differ- O
ent O
SR O
approaches O

Fast O
and O
ac- O
curate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
erate O
at O
a O
fixed O
spatial O

- B-DAT
resolution O
(LR) O
and O
high-resolution O
(HR O

- B-DAT
layed O
feedback. O
Extensive O
quantitative O
and O

- B-DAT
uations O
on O
benchmark O
datasets O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formance O
has O
been O
significantly O
improved O

- B-DAT
velopments O
in O
deep O
neural O
networks O

- B-DAT
ing O
[16] O
have O
been O
widely O

- B-DAT
tently O
observed. O
The O
first O
is O

- B-DAT
ping O
from O
LR O
to O
HR O

- B-DAT
work O
depth O
enlarges O
the O
size O

-15 B-DAT

-1 B-DAT

-0317 B-DAT

- B-DAT
struct O
missing O
HR O
components. O
The O

- B-DAT
ing O
gradients, O
facilitating O
the O
training O

- B-DAT
troduces O
more O
parameters, O
and O
thus O

- B-DAT

- B-DAT

- B-DAT
tional O
Network O
(DRCN) O
[21] O
shares O

- B-DAT
ent O
residual O
units O
and O
achieves O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ral O
networks O
(RNNs). O
Specifically, O
Liao O

- B-DAT

- B-DAT
work O
(ResNet) O
[16] O
is O
equivalent O

- B-DAT
spired O
by O
their O
findings, O
we O

- B-DAT
olution O
(bicubic O
interpolation O
is O
first O

- B-DAT

- B-DAT
nite O
unfolding O
in O
time O
of O

- B-DAT
tioning O
that O
we O
follow O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ventional O
RNN O
model O
is O
generally O

- B-DAT

- B-DAT
dering O
our O
model O
a O
Dual-State O

- B-DAT

- B-DAT
tions. O
This O
provides O
information O
flow O

- B-DAT
ery O
single O
unrolling O
time. O
In O

- B-DAT
age O
Restoration O
and O
Enhancement O
workshop O

- B-DAT

- B-DAT
sive O
experimental O
results O
validate O
that O

- B-DAT
CNN, O
to O
predict O
the O
nonlinear O

- B-DAT

- B-DAT
strated O
superior O
performance O
to O
many O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mization O
techniques O
[34, O
28, O
11 O

- B-DAT
posed O
a O
sparse O
coding O
network O

- B-DAT

- B-DAT
end, O
demonstrating O
the O
benefit O
of O

- B-DAT
work O
in O
[42 O

- B-DAT
ageNet O
challenges O
[9], O
Kim O
et O

- B-DAT
ents. O
However, O
as O
the O
model O

- B-DAT
rameters O
increases. O
To O
control O
the O

- B-DAT
culty O
of O
training. O
Tai O
et O

- B-DAT
ual O
SR O
learning O
algorithms O
are O

- B-DAT
ual O
learning O
or O
local O
residual O

- B-DAT
rameter O
efficient O
via O
recursive O
learning O

- B-DAT
works O
(DenseNet) O
[17] O
instead O
of O

- B-DAT
put O
images, O
Shi O
et O
al O

- B-DAT
duces O
the O
computation O
cost. O
Similarly O

- B-DAT
bination O
with O
smaller O
filter O
sizes O

- B-DAT
Resolution O
Network O
(LapSRN) O
[22] O
works O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ant, O
which O
learns O
different O
scaled O

- B-DAT
allel O
via O
weight O
sharing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
posed O
nature O
of O
single O
image O

- B-DAT
tions O
and O
poor O
subjective O
scores O

- B-DAT
back, O
Generative O
Adversarial O
Networks O
have O

- B-DAT
uation O
by O
mean-opinion-score O
showed O
huge O

- B-DAT

- B-DAT
vide O
a O
better O
understanding O
of O

- B-DAT
cations O

- B-DAT

- B-DAT
tem. O
Then, O
based O
on O
this O

- B-DAT
ment O
of O
SR O
models O
with O

- B-DAT

- B-DAT
rent O
states. O
Depending O
on O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put, O
and O
recurrent O
states O
are O

- B-DAT
tively. O
The O
arrow O
link O
indicates O

- B-DAT
tion O
on O
this O
general O
formulation O

- B-DAT
rection O
to O
a O
fixed O
length O

- B-DAT

- B-DAT

- B-DAT
independent, O
which O
means O
these O
parameters O

- B-DAT
out O
any O
down-sampling O
or O
up-sampling O

- B-DAT
sions O
remain O
the O
same O
across O

- B-DAT
ventional O
residual O
block, O
which O
contains O

- B-DAT
tional O
layers O
with O
skip O
connections O

- B-DAT
rameters O

- B-DAT
ventional O
ResNet O
is O
that O
the O

- B-DAT

- B-DAT
gle O
convolutional O
layer O
to O
the O

- B-DAT
press O
frecurrent. O
The O
graph O
is O

- B-DAT
over, O
unlike O
the O
ResNet O
where O

- B-DAT
tion O
comes O
from O
the O
previous O

- B-DAT
rolled O
state O
s0. O
Figure O
1(e O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
state O
design, O
which O
adopts O
two O

- B-DAT

- B-DAT

- B-DAT
tion O
from O
both O
the O
LR O

- B-DAT
tively. O
Four O
colored O
arrows O
indicate O

- B-DAT

- B-DAT

- B-DAT
tion O
flows O
between O
sl O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

of O
a O
single-state B-DAT
RNN. O
(c) O
- O
(e) O
The O
required O
recurrent O
function O

- B-DAT

- B-DAT
tom O
one O
is O
LR. O
This O

- B-DAT
cialization O
for O
different O
resolutions O
and O

- B-DAT

- B-DAT

- B-DAT
sampling O
transition. O
The O
strides O
in O

- B-DAT

- B-DAT

- B-DAT
ing O
a O
prediction O
at O
every O

- B-DAT
terized O
by O
a O
single O
convolutional O

- B-DAT
ing O
the O
prediction O
only O
at O

- B-DAT
over, O
the O
model O
predicts O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lowing O
datasets: O
Set5 O
[4], O
Set14 O

- B-DAT
ban100 O
[19]. O
The O
training O
data O

- B-DAT
dom O
flipping O
along O
the O
vertical O

- B-DAT

- B-DAT

- B-DAT
vided O
training O
and O
validation O
sets O

- B-DAT
tioned O
data O
augmentations O
except O
random O

- B-DAT

- B-DAT
lution O
filters. O
Due O
to O
our O

- B-DAT

- B-DAT
sions O
as O
the O
LR O
and O

- B-DAT

- B-DAT
volution O
is O
applied O

- B-DAT
form O
distribution O
using O
the O
method O

- B-DAT
mentum O
0.95 O
as O
our O
optimizer O

- B-DAT
ing O
rate O
from O
{0.1,0.03,0.01} O
and O

- B-DAT
ing O
rate O
annealing O
is O
driven O

- B-DAT

- B-DAT
posed O
the O
use O
of O
unshared O

- B-DAT
folding O
time O
to O
resolve O
this O

- B-DAT
nary O
ReLU O
as O
the O
activation O

- B-DAT
imum O
effective O
depth O
of O
the O

- B-DAT
work O
is O
2T O
+ O
4 O

- B-DAT
ers O
in O
a O
residual O
block O

- B-DAT
iary O
input O
and O
output O
layers O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity O
and O
computation O
cost. O
We O

- B-DAT
pirical O
results O
are O
shown O
in O

- B-DAT
tioning O
that O
we O
also O
experimented O

- B-DAT
ing O
to O
be O
crucial O
for O

- B-DAT
forms O
much O
more O
poorly O
than O

- B-DAT

- B-DAT
part. O
Specifically, O
we O
observe O
around O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
state O
baseline O
and O
the O
DSRN O

- B-DAT
tion, O
comparing O
our O
models O
with O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eral O
public O
benchmark O
datasets O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentation O
Dataset O
[2], O
while O
our O

- B-DAT
tive O
performance O
across O
all O
datasets O

- B-DAT
cently O
developed O
DIV2K O
dataset O
and O

- B-DAT
ranking O
algorithms O
in O
Table O
2 O

- B-DAT
petitive O
performance O
with O
the O
best O

- B-DAT
state O
recurrent O
structure O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolved O
images O
on O
Set14 O
with O

- B-DAT
ent O
SR O
approaches. O
For O
these O

- B-DAT
tures O
and O
is O
less O
prone O

- B-DAT
trate O
the O
parameters-to-PSNR O
relationship O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mance, O
and O
has O
modest O
inference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
folding O
of O
a O
single-state O
RNN O

- B-DAT
tions. O
Based O
on O
this, O
we O

- B-DAT
ering O
a O
dual-state O
design; O
the O

- B-DAT
posed O
DSRN O
operate O
at O
different O

- B-DAT

- B-DAT
iments O
on O
benchmark O
datasets O
have O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tour O
detection O
and O
hierarchical O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
based O
on O
nonnegative O
neighbor O

- B-DAT
resolution O
through O
neighbor O
embedding. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
database. O
In O
CVPR, O
pages O

- B-DAT
ing O
a O
deep O
convolutional O
network O

- B-DAT
resolution. O
In O
ECCV, O
2014. O
1 O

- B-DAT

- B-DAT

- B-DAT
ual O
networks O
for O
image O
super-resolution O

- B-DAT

- B-DAT

- B-DAT
TATS, O
2010. O
5 O

- B-DAT

- B-DAT
berger. O
Deep O
networks O
with O
stochastic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works. O
In O
CVPR, O
2016. O
1 O

- B-DAT
recursive O
convolutional O
network O
for O
image O

- B-DAT
resolution. O
In O
CVPR, O
2016. O
1 O

- B-DAT

- B-DAT
ningham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT
hanced O
deep O
residual O
networks O
for O

- B-DAT
resolution. O
In O
CVPR O
Workshops, O
2017 O

- B-DAT

- B-DAT
ing O
a O
mixture O
of O
deep O

- B-DAT
resolution. O
In O
ACCV, O
pages O
145–156 O

- B-DAT

- B-DAT
tion O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT
hancenet: O
Single O
image O
super-resolution O
through O

- B-DAT
tomated O
texture O
synthesis. O
In O
ICCV O

- B-DAT
curate O
image O
upscaling O
with O
super-resolution O

- B-DAT
ment O
using O
natural O
scene O
statistics O

- B-DAT

- B-DAT
gle O
image O
and O
video O
super-resolution O

- B-DAT

- B-DAT

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT

- B-DAT
resolution O
using O
dense O
skip O
connections O

- B-DAT
works O
behave O
like O
ensembles O
of O

- B-DAT
works. O
In O
NIPS, O
2016. O
1 O

- B-DAT
celli. O
Image O
quality O
assessment: O
from O

- B-DAT

- B-DAT

- B-DAT
lution. O
In O
CVPRW, O
pages O
1–8 O

- B-DAT
resolution. O
TIP, O
21(8):3467–3478, O
2012. O
2 O

- B-DAT

- B-DAT
lective O
tensor O
factorization O
in O
deep O

- B-DAT

- B-DAT

- B-DAT
ity O
measures O
of O
recurrent O
neural O

Set5 B-DAT
×2 O
36.66 O
/ O
0.9542 O
37.53 O

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

- B-DAT

- B-DAT

- B-DAT
Resolution O
(SR) O
performance O
in O
the O

- B-DAT
ever, O
being O
supervised, O
these O
SR O

- B-DAT
resolution O
(LR) O
images O
from O
their O

- B-DAT

- B-DAT

- B-DAT
ever, O
rarely O
obey O
these O
restrictions O

- B-DAT
sults O
by O
SotA O
(State O
of O

- B-DAT
troduce O
“Zero-Shot” O
SR, O
which O
exploits O

- B-DAT
age, O
and O
train O
a O
small O

- B-DAT

- B-DAT
ological O
data, O
and O
other O
images O

- B-DAT
cess O
is O
unknown O
or O
non-ideal O

- B-DAT

- B-DAT
ous O
unsupervised O
SR O
methods. O
To O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
(supervised O
[21] O
or O
unsupervised O

- B-DAT
riods O
of O
time O
(days O
or O

- B-DAT
mance O
deteriorates O
significantly O
once O
these O

- B-DAT
quality O
natural O
images, O
from O
which O

- B-DAT

- B-DAT
scaling O
kernel O
(usually O
a O
Bicubic O

- B-DAT
tracting O
artifacts O
(sensor O
noise, O
non-ideal O

- B-DAT
pression, O
etc.), O
and O
for O
a O

- B-DAT

- B-DAT
ally O
×2, O
×3 O
or O
×4 O

- B-DAT
ideal O
(non-bicubic) O
downscaling O
kernel, O
or O

- B-DAT
facts. O
Fig. O
1 O
further O
shows O

- B-DAT

- B-DAT

- B-DAT
ing O
on O
any O
prior O
image O

- B-DAT
ploit O
the O
internal O
recurrence O
of O

- B-DAT

- B-DAT

- B-DAT
form O
SR O
on O
real O
images O

- B-DAT
known O
and O
non-ideal O
(see O
example O

- B-DAT

- B-DAT
trained O
SotA O
SR O
methods O
by O

- B-DAT
nally O
supplied O
examples O
(even O
if O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ages O
[4, O
23]. O
This O
formed O

- B-DAT
pervised O
image O
enhancement O
methods, O
including O

- B-DAT
vised O
SR O
[4, O
5, O
6 O

- B-DAT

- B-DAT
ing O
kernel O
is O
unknown), O
Blind-Deblurring O

- B-DAT
Dehazing O
[2], O
and O
more. O
While O

- B-DAT

- B-DAT
ject O
to O
the O
above-mentioned O
supervised O

- B-DAT
age O
patches, O
of O
predefined O
size O

- B-DAT
nearest-neighbours O
search. O
As O
such, O
they O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
without O
being O
restricted O
by O

- B-DAT

- B-DAT
itations O
of O
patch-based O
methods. O
We O

- B-DAT

- B-DAT

- B-DAT
age O
and O
its O
downscaled O
versions O

- B-DAT

- B-DAT
duce O
the O
HR O
output. O
This O

- B-DAT
based O
SR O
by O
a O
large O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
is O
available O
and O
provided O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
est O
amounts O
of O
computational O
resources O

- B-DAT

- B-DAT
ideal’ O
conditions, O
and O
competitive O
results O

- B-DAT
tions O
for O
which O
SotA O
supervised O

- B-DAT
ages O
have O
strong O
internal O
data O

- B-DAT
peat O
many O
times O
inside O
a O

- B-DAT
vation O
was O
empirically O
verified O
by O

- B-DAT

- B-DAT
conies, O
since O
evidence O
to O
their O

- B-DAT
specific O
information O
when O
relying O
on O

- B-DAT
ages. O
While O
the O
strong O
internal O

- B-DAT

- B-DAT
fied O
here O
using O
a O
‘fractal-like O

- B-DAT
power O
was O
analyzed O
and O
shown O

- B-DAT
ternal O
entropy O
of O
patches O
inside O

- B-DAT

- B-DAT
ther O
shown O
to O
be O
particularly O

- B-DAT
tainty O
and O
image O
degradations O
(see O

- B-DAT

- B-DAT
formation. O
Simple O
unsupervised O
internal-SR O
[4 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
struct O
the O
desired O
HR O
output O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put O
training O
instances. O
The O
resulting O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
HR O
training O
example O
pairs. O
This O

- B-DAT
ternal O
collection O
of O
LR-HR O
image O

- B-DAT

- B-DAT
tremely O
deep O
and O
very O
complex O

- B-DAT

- B-DAT
pler O
image-specific O
network O

- B-DAT
vations O
on O
each O
layer. O
The O

- B-DAT

- B-DAT
ods O
[9, O
8, O
3], O
we O

- B-DAT
dependent O
of O
the O
size O
of O

- B-DAT

- B-DAT

- B-DAT
less O
the O
sampled O
image-pair O
is O

- B-DAT

- B-DAT

- B-DAT
father. O
The O
closer O
the O
size-ratio O

- B-DAT

- B-DAT
pled. O
This O
reflects O
the O
higher O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
runtime O
per O
image O
is O

- B-DAT

-80 B-DAT
GPU). O
This O
runtime O
is O
inde O

- B-DAT
pendent O
of O
the O
image O
size O

- B-DAT

- B-DAT
tained O
when O
using O
a O
gradual O

- B-DAT
ample, O
a O
gradual O
increase O
using O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
kernel, O
high-quality O
imaging O
conditions O

- B-DAT
vised O
SR O
methods O
achieve O
an O

- B-DAT
ferent O
lens O
types O
and O
PSFs O

- B-DAT
ing O
conditions O
(e.g., O
subtle O
involuntary O

- B-DAT
sults O
in O
different O
downscaling O
kernels O

- B-DAT
acteristics, O
various O
compression O
artifacts, O
etc O

- B-DAT
figurations/settings. O
Moreover, O
a O
single O
supervised O

- B-DAT
tions/settings. O
To O
obtain O
good O
performance O

- B-DAT

- B-DAT
dations/settings O
of O
the O
test O
image O

- B-DAT
vided, O
the O
bicubic O
kernel O
serves O

- B-DAT

- B-DAT
off O
between O
speed O
and O
quality O

- B-DAT

- B-DAT

- B-DAT
sion O
artifacts, O
etc.) O
We O
found O

- B-DAT
deviation O
of∼5 O
grayscales), O
improves O
the O

- B-DAT

- B-DAT
scale O
information O
(the O
noise), O
while O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ideal O
kernels O
(see O
examples O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Results O
Our O
method O
(ZSSR O
- B-DAT
‘Zero-Shot O
SR’) O
is O
primarily O
aimed O

- B-DAT

- B-DAT

- B-DAT
petitive O
results O
against O
externally-supervised O
methods O

- B-DAT

- B-DAT
ing O
method O
SelfExSR O
[6] O
by O

- B-DAT
ated O
using O
the O
‘ideal’ O
supervised O

- B-DAT
ple O
is O
shown O
in O
Fig O

- B-DAT
ical O
natural O
image, O
further O
analysis O

- B-DAT
ence O
for O
internal O
learning O
(via O

- B-DAT

- B-DAT
nally O
learned O
data O
recurrence O
(ZSSR O

- B-DAT
vantageous O
in O
image O
area O
with O

- B-DAT
res) O
examples O
of O
themselves O
elsewhere O

- B-DAT
age O
(at O
a O
different O
location/scale O

- B-DAT
Learning O
with O
External-Learning O
in O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion. O
Each O
LR O
image O
from O

- B-DAT
graded O
using O
one O
of O
3 O

- B-DAT
plied O
to O
those O
images, O
without O

- B-DAT
polation O
outperforms O
current O
SotA O
SR O

- B-DAT
sian O
kernels. O
For O
each O
image O

- B-DAT

- B-DAT
nel. O
Table O
2 O
compares O
our O

- B-DAT
ing O
externally-supervised O
SR O
methods O
[12 O

- B-DAT

- B-DAT
scaling O
kernel. O
For O
this O
mode O

- B-DAT
parametric O
downscaling O
kernel O
which O
maximizes O

- B-DAT
larity O
of O
patches O
across O
scales O

- B-DAT
ate O
the O
LR O
image. O
Such O

- B-DAT

- B-DAT
age O
(whether O
estimated O
or O
real O

- B-DAT
els O
that O
favor O
Internal-SR O
(i.e O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
selves O
elsewhere O
inside O
the O
same O

- B-DAT
tion/scale O

- B-DAT

- B-DAT
wscaling O
model O
is O
more O
important O

- B-DAT

- B-DAT

- B-DAT
dom O
type O
of O
degradation O
out O

- B-DAT
ble O
3 O
shows O
that O
ZSSR O

- B-DAT
rent O
SotA O
SR O
methods O

- B-DAT

- B-DAT
ploits O
the O
power O
of O
Deep O

- B-DAT

- B-DAT

- B-DAT
cess O
is O
non-ideal, O
unknown, O
and O

- B-DAT
age O
(i.e., O
image-specific O
settings). O
In O

- B-DAT

- B-DAT
ideal’ O
settings, O
our O
method O
substantially O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pean O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
gle O
image. O
In O
International O
Conference O

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
istration. O
CVGIP: O
Graphical O
Model O
and O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT
tion O
(CVPR) O
Workshops, O
pages O
1646–1654 O

- B-DAT

- B-DAT
tional O
network O
for O
image O
super-resolution O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
ham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cal O
statistics. O
In O
Proc. O
8th O

- B-DAT
ume O
2, O
pages O
416–423, O
July O

- B-DAT
resolution. O
In O
International O
Conference O
on O

- B-DAT
sion O
(ECCV). O
2014. O
3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
(ICML). O
3 O

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT
shops, O
July O
2017. O
6 O

- B-DAT
prove O
example-based O
single O
image O
super O

- B-DAT
tion O
(CVPR), O
June O
2016. O
5 O

- B-DAT
chored O
neighborhood O
regression O
for O
fast O

- B-DAT

- B-DAT

we O
use O
three O
dataset, O
i.e. O
Set5 B-DAT
[15] O
(5 O
images), O
Set14 O
[16 O

dB) O
and O
SSIM O
on O
the O
Set5 B-DAT
[15], O
Set14 O
[16] O
and O
BSD200 O

Set5 B-DAT
×2 O
33.66/0.9299 O
36.55/0.9544 O
36.87/0.9556 O
36.34/0.9521 O

2, O
3 O
and O
4 O
on O
Set5 B-DAT
dataset, O
respectively. O
Over O
the O
three O

2, O
3 O
and O
4 O
on O
Set5 B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
tend O
to O
redundantly O
recur O

- B-DAT

- B-DAT
posed O
deep O
learning O
based O
restoration O

- B-DAT

- B-DAT
pose O
a O
dilated O
convolution O
based O

- B-DAT

- B-DAT

- B-DAT
nates O
all O
these O
features O
to O

- B-DAT

- B-DAT

- B-DAT
ception O
module, O
the O
proposed O
end-to-end O

- B-DAT
resolution O
network O
can O
take O
advantage O

- B-DAT

- B-DAT
tion O
to O
improve O
image O
super-resolution O

- B-DAT
imental O
results O
show O
that O
our O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
phasis O
because O
of O
the O
requirement O

- B-DAT
nition O
(UHD) O
TVs. O
However, O
most O

- B-DAT
tent O
from O
lower O
resolutions O
[2 O

- B-DAT

- B-DAT

- B-DAT
tion O
module. O
Our O
proposed O
new O

- B-DAT

- B-DAT
pled O
dictionaries O
to O
learn O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
both O
high O
level O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tablish O
the O
relationship O
between O
the O

- B-DAT

- B-DAT
strate O
that O
a O
convolutional O
neural O

- B-DAT

- B-DAT

- B-DAT
resolution O
image O
to O
the O
high-resolution O

- B-DAT
quency O
details O
instead O
of O
the O

- B-DAT

- B-DAT
tion O
is O
also O
important. O
The O

- B-DAT

- B-DAT
tion O
result O
in O
traditional O
methods O

- B-DAT
scale O
information O
has O
been O
little O

- B-DAT

- B-DAT
ploit O
the O
scale O
information O
since O

- B-DAT

- B-DAT
ception O
module O
to O
learn O
multi-scale O

- B-DAT
ison O
between O
the O
proposed O
dilated O

- B-DAT
tion O
module O
and O
the O
original O

- B-DAT
ent O
scale O
dilated O
convolution O
that O

- B-DAT
scale O
image O
information. O
Furthermore, O
we O

- B-DAT

- B-DAT
performs O
many O
state-of-the-art O
methods O

- B-DAT
ter O
to O
clarify O
that O
no O

- B-DAT

- B-DAT
volution, O
F O
and O
k O
is O

- B-DAT

- B-DAT
tively. O
It O
shows O
that O
filters O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
the-art O
performance O
for O
classification O
and O

- B-DAT

- B-DAT
pose O
a O
dilated O
convolution O
based O

- B-DAT

- B-DAT
formance O

- B-DAT
ers. O
Then, O
we O
concatenate O
all O

- B-DAT

- B-DAT

- B-DAT
terpolation O
result. O
Therefore, O
it O
can O

- B-DAT

- B-DAT
erate O
on O
the O
same O
scale O

- B-DAT
responding O
scale O
image O
information. O
Furthermore O

- B-DAT
put, O
so O
we O
can O
fuse O

- B-DAT

- B-DAT
struction. O
Since O
residual O
learning O
have O

- B-DAT
age O
high O
frequency O
details O
instead O

- B-DAT

- B-DAT
sult O
as O
a O
”low-resolution” O
image O

- B-DAT
ception O
module O
combines O
different O
scale O

- B-DAT
scale O
information, O
in O
the O
next O

- B-DAT
tion O
layer O
after O
the O
inception O

- B-DAT
ules O
following O
with O
a O
common O

- B-DAT
struction O
phase, O
we O
use O
a O

- B-DAT

- B-DAT

- B-DAT
fied O
Linear O
Unit O
(PReLU) O
as O

- B-DAT

- B-DAT
tion O
layer O
in O
our O
very O

- B-DAT
tion O
(Adam) O
[14] O
to O
optimize O

- B-DAT
provement O
in O
performance O
can O
be O

- B-DAT
Resolution O
Network O
(MSSRNet O

-100 B-DAT
dataset, O
which O
contains O
100 O
bmp-format O

- B-DAT
ules O
to O
make O
the O
network O

- B-DAT
tialized O
using O
the O
initializer O
proposed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nential O
decay O
rates O
for O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lar O
deep O
learning O
based O
single O

- B-DAT

- B-DAT
ods O
are O
the O
same O
as O

- B-DAT
performs O
all O
previous O
methods O
in O

- B-DAT
sample O
factor O
2, O
3 O
and O

- B-DAT

- B-DAT
ter O
performance, O
we O
can O
increase O

- B-DAT
tive O
and O
qualitative O
results O

- B-DAT

- B-DAT

- B-DAT
resolution. O
Experimental O
results O
show O
that O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sic O
Research O
Development O
Program O
of O

- B-DAT
manet, O
Scott O
Reed, O
Dragomir O
Anguelov O

- B-DAT
tion, O
2015, O
pp. O
1–9 O

- B-DAT
los O
K O
Katsaggelos, O
“Video O
super-resolution O

- B-DAT
lutional O
neural O
networks,” O
IEEE O
Transactions O

- B-DAT
putational O
Imaging, O
vol. O
2, O
no O

- B-DAT

- B-DAT
tion,” O
IEEE O
transactions O
on O
image O

- B-DAT
resolution O
from O
a O
single O
image O

- B-DAT
aoou O
Tang, O
“Learning O
a O
deep O

- B-DAT

- B-DAT
aoou O
Tang, O
“Image O
super-resolution O
using O

- B-DAT
lutional O
networks,” O
IEEE O
transactions O
on O

- B-DAT
ysis O
and O
machine O
intelligence, O
vol O

- B-DAT
aoou O
Tang, O
“Compression O
artifacts O
reduction O

- B-DAT
ternational O
Conference O
on O
Computer O
Vision O

- B-DAT
celerating O
the O
super-resolution O
convolutional O
neural O

- B-DAT
work,” O
in O
European O
Conference O
on O

- B-DAT
curate O
image O
super-resolution O
using O
very O

- B-DAT
lutional O
networks,” O
in O
Proceedings O
of O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT
rithm O
for O
signal O
analysis O
with O

- B-DAT
ding O
the O
a O
trous O
and O

- B-DAT
actions O
on O
signal O
processing, O
vol O

- B-DAT

- B-DAT
ture O
learning, O
purity O
trees, O
and O

- B-DAT

- B-DAT

- B-DAT
image O
super-resolution O
based O
on O
nonnegative O

- B-DAT
sion O
Conference. O
2012, O
pp. O
135.1–135.10 O

- B-DAT

- B-DAT

- B-DAT
dra O
Malik, O
“A O
database O
of O

- B-DAT
ages O
and O
its O
application O
to O

- B-DAT
gorithms O
and O
measuring O
ecological O
statistics O

- B-DAT
puter O
Vision, O
2001. O
ICCV O
2001 O

- B-DAT
level O
performance O
on O
imagenet O
classification O

- B-DAT
ceedings O
of O
the O
IEEE O
international O

- B-DAT
puter O
vision, O
2015, O
pp. O
1026–1034 O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision. O
Springer, O
2014, O
pp O

- B-DAT
level O
performance O
on O
imagenet O
classification O

- B-DAT
ceedings O
of O
the O
IEEE O
international O

- B-DAT
puter O
vision, O
2015, O
pp. O
1026–1034 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

standard O
SR O
bench- O
marks O
(i.e. O
Set5, B-DAT
Set14, O
B100) O
and O
methods O
(i.e O

al. O
[32]. O
Results O
reported O
on O
Set5, B-DAT
×3. O
Details O
in O
Section O
4.1 O

Zeyde, O
or O
Yang O
methods O
on O
Set5 B-DAT
test O
images O
for O
magnification O
factor O

and O
qualitative O
evaluation O
3 O
datasets O
Set5, B-DAT
Set14, O
and O
B100 O
are O
used O

one O
of O
the O
training O
images. O
Set5 B-DAT
is O
used O
for O
reporting O
results O

Set5 B-DAT

larger, O
more O
diverse O
set O
than O
Set5 B-DAT

on O
average O
than O
those O
in O
Set5 B-DAT

PSNR O
performance O
of O
A+ O
on O
(Set5, B-DAT
×3) O
im- O
proves O
with O
the O

0.5 O
million O
to O
32.55dB O
on O
Set5 B-DAT
and O
magnification O
×3. O
Inspired O
by O

Figure O
4. O
Performance O
(Set5, B-DAT
×3) O
improves O
with O
the O
number O

the O
A+ O
method O
on O
the O
Set5 B-DAT
images. O
The O
performance O
of O
A O

show O
in O
Fig. O
3 O
on O
Set5, B-DAT
×3 O
how O
the O
performance O
of O

is O
the O
local O
regression. O
On O
Set5 B-DAT
the O
PSNR O
is O
32.17dB O
for O

Figure O
5. O
Performance O
(A+A O
on O
Set5, B-DAT
×3) O
depends O
on O
the O
number O

improves O
the O
super-resolution O
PSNR O
results O
(Set5, B-DAT
×3 O

the O
number O
of O
cascade O
stages O
(Set5, B-DAT
×3 O

improve O
the O
super-resolution O
PSNR O
results O
(Set5, B-DAT
×3 O

improves O
the O
super-resolution O
PSNR O
results O
(Set5, B-DAT
×3 O

On O
Set5 B-DAT
(see O
Fig. O
6 O
and O
Table O

improves O
the O
super-resolution O
PSNR O
results O
(Set5, B-DAT
×3 O

from O
32.39dB O
to O
32.55dB O
on O
Set5, B-DAT
while O
the O
running O
time O
only O

achieve O
0.33dB O
improvement O
over O
A+ O
(Set5, B-DAT
×3) O
without O
an O
increase O
in O

Improve O
A+. O
PSNR O
gains O
for O
Set5, B-DAT
×3 O

per- O
formance O
scale O
the O
results O
(Set5 B-DAT

the O
experiments O
until O
now O
used O
Set5 B-DAT
and O
L20 O
and O
magnification O
factor O

Table O
5. O
Average O
PSNR O
on O
Set5, B-DAT
Set14, O
and O
B100 O
and O
the O

Set5 B-DAT
x3 O
30.39 O
31.84 O
31.90 O
31.92 O

PSNR O
performance O
on O
Set5, B-DAT
Set14, O
and O
B100, O
and O
for O

Zeyde, O
or O
Yang O
methods O
on O
Set5 B-DAT

one O
of O
the O
training O
images. O
Set5 B-DAT

0.5 O
million O
to O
32.55dB O
on O
Set5 B-DAT

the O
A+ O
method O
on O
the O
Set5 B-DAT

is O
the O
local O
regression. O
On O
Set5 B-DAT

the O
experiments O
until O
now O
used O
Set5 B-DAT

Set5 B-DAT

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Trans O

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
marks O
(i.e. O
Set5, O
Set14, O
B100 O

- B-DAT
CNN, O
ANR, O
Zeyde, O
Yang) O
and O

- B-DAT
ments. O
The O
techniques O
are O
widely O

- B-DAT
of-the-art O
results O
outperforming O
A+ O
by O

- B-DAT
age O
PSNR O
whilst O
maintaining O
a O

- B-DAT

- B-DAT

- B-DAT
quencies O
details O
from O
a O
single O

- B-DAT

- B-DAT

- B-DAT
respond O
to O
the O
same O
LR O

- B-DAT
lem, O
the O
SR O
literature O
proposes O

- B-DAT

- B-DAT
ods O
[24], O
reconstruction-based O
methods O
[3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nal O
images. O
Most O
recent O
methods O

- B-DAT
based O
SR. O
We O
apply O
them O

- B-DAT
chored O
Neighborhood O
Regression O
(ANR) O
method O

- B-DAT

- B-DAT
based O
single O
image O
super-resolution O
methods O

- B-DAT
cant O
improvements O
on O
standard O
benchmarks O

- B-DAT
bine O
the O
techniques O
to O
derive O

- B-DAT
ments O
when O
starting O
from O
the O

review O
the O
anchored O
regression O
baseline O
- B-DAT
the O
A+ O
method O
[26]. O
Then O

- B-DAT
bically) O
downscaled O
to O
the O
corresponding O

- B-DAT

- B-DAT

- B-DAT
tion O
we O
first O
describe O
the O

- B-DAT
ods O
we O
use O
or O
compare O

- B-DAT
lar O
images: O
one O
medium O
size O

- B-DAT
ing O
results. O
The O
images O
in O

- B-DAT
ety O
of O
real-life O
scenes O
and O

- B-DAT

- B-DAT
similarity O
(S) O
experiments O
on O
the O

- B-DAT
resentation O
of O
the O
LR-HR O
priors/training O

- B-DAT

- B-DAT
fte O
et O
al. O
[25] O
relaxes O

- B-DAT
tion O
of O
patches O
from O
Yang O

- B-DAT

- B-DAT
SVD O
[1]) O
as O
the O
ANR O

- B-DAT
tionary O
atoms, O
called O
anchors. O
For O

- B-DAT
proves O
with O
the O
number O
of O

- B-DAT
trix O
multiplication O
(application O
of O
the O

- B-DAT

- B-DAT

- B-DAT
els O
for O
LR O
and O
9 O

- B-DAT
mance O
from O
32.39dB O
with O
0.5 O

- B-DAT
sions O
of O
the O
training O
images/patches O

- B-DAT
inal O
images O
by O
90◦, O
180 O

- B-DAT

- B-DAT

- B-DAT
gressors O
varies O
from O
31.83dB O
when O

- B-DAT
ples O
to O
32.39dB O
for O
0.5 O

- B-DAT
ing O
samples O
but O
on O
the O

- B-DAT
ples/anchoring O
points) O
is O
increased, O
the O

- B-DAT
chored O
methods O
(such O
as O
ANR O

- B-DAT
tize O
the O
LR O
feature O
space O

- B-DAT
sors, O
while O
it O
reaches O
32.92dB O

- B-DAT
chor O
is O
applied O
to O
reconstruct O

- B-DAT
tures O
are O
high O
dimensional O
(30 O

- B-DAT
tures O
such O
as O
kd-trees, O
forests O

-4 B-DAT
times O
in O
[20, O
22 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
struction O
consistent O
with O
the O
LR O

- B-DAT
sampling. O
Knowing O
the O
degradation O
operators O

- B-DAT
timated O
[18]. O
Assuming O
the O
degradation O

- B-DAT
pending O
on O
the O
settings O
as O

- B-DAT

- B-DAT
ence O
A+ O
is O
1.18dB O
better O

- B-DAT
ter O
than O
the O
baseline O
Yang O

- B-DAT
tion O
operators O
are O
unknown O
and O

- B-DAT
cise, O
therefore O
our O
reported O
results O

- B-DAT
resolution O
becomes O
more O
accurate, O
since O

- B-DAT
ble O
HR O
solutions O
for O
each O

- B-DAT
ally O
refine O
the O
contents O
up O

- B-DAT
plexity O
depends O
on O
the O
number O

- B-DAT
resolving O
the O
LR O
image O
in O

- B-DAT
get O
the O
HR O
image O
for O

- B-DAT
tal O
approach O
has O
a O
loose O

- B-DAT

- B-DAT

- B-DAT
put O
image O
is O
enhanced O
by O

- B-DAT
sults O
at O
pixel O
level. O
Therefore O

- B-DAT
tion O
(E) O
gives O
a O
0.05dB O

- B-DAT
cade. O
The O
running O
time O
is O

- B-DAT
mations. O
In O
Table O
3 O
we O

- B-DAT

- B-DAT

- B-DAT
ies O
built O
from O
the O
input O

- B-DAT
text. O
Exponents O
are O
Glasner O
et O

- B-DAT
els O
adapted O
to O
each O
new O

- B-DAT
naries O
proved O
better O
in O
terms O

- B-DAT
ies O
can O
be O
better O
than O

- B-DAT
ternal O
and O
internal O
dictionaries O

- B-DAT

- B-DAT
main O
specific O
models O
and O
Sun O

- B-DAT
ber O
that O
does O
not O
increase O

- B-DAT
text O
we O
compute O
a O
regressor O

- B-DAT
tion. O
For O
patches O
of O
comparable O

- B-DAT
chors O
and O
then O
the O
regressor O

- B-DAT
prove O
from O
32.39dB O
to O
32.55dB O

- B-DAT
provements O
achieved O
using O
reasoning O
with O

- B-DAT

- B-DAT
resolution O
method. O
If O
we O
start O

- B-DAT
archical O
search O
structure, O
we O
achieve O

- B-DAT
tion O
time. O
The O
full O
setup O

- B-DAT
cations O
×2, O
×3, O
×4. O
Figs O

- B-DAT
ent O
techniques O
is O
additive, O
each O

- B-DAT
formance. O
These O
techniques O
are O
general O

- B-DAT

- B-DAT

- B-DAT
formance O
scale O
the O
results O
(Set5,×3 O

- B-DAT
ods O
A+, O
ANR, O
Zeyde, O
and O

- B-DAT
bines O
A+ O
with O
A O
and O

- B-DAT
nification O
factors O
×2, O
×3, O
and O

- B-DAT
parison O
with O
the O
baseline O
A O

- B-DAT
ding O
with O
Locally O
Linear O
Embedding O

- B-DAT
porting O
improved O
results O
also O
for O

- B-DAT
dation O
operators O
usually O
are O
not O

- B-DAT
mate O
in O
practice. O
A+B O
just O

- B-DAT
ample, O
the O
clarity O
and O
sharpness O

- B-DAT
formance O
of O
example-based O
super-resolution. O
Combined O

- B-DAT
invasive O
techniques O
such O
as O
augmentation O

- B-DAT
chors O
in O
the O
IA O
method O

- B-DAT
nitude O
more O
regressors O
than O
the O

- B-DAT
ning O
time. O
Another O
technique, O
often O

- B-DAT
caded O
application O
of O
the O
core O

- B-DAT

- B-DAT
wards O
HR O
restoration. O
Using O
the O

- B-DAT

- B-DAT

- B-DAT
the-art O
methods O
such O
as O
A O

- B-DAT

- B-DAT
resolution O
methods. O
The O
proposed O
techniques O

- B-DAT

- B-DAT
beri O
Morel. O
Low-complexity O
single-image O
super-resolution O

- B-DAT

- B-DAT
lutional O
nets. O
In O
BMVC, O
2014 O

- B-DAT
gressors O
for O
image O
super-resolution. O
Computer O

- B-DAT
rum, O
34(2):95–104, O
2015. O
1 O

- B-DAT

- B-DAT
resolution O
using O
deep O
convolutional O
networks O

- B-DAT
actions O
on O
Pattern O
Analysis O
and O

- B-DAT
tralized O
sparse O
representation O
for O
image O

- B-DAT

- B-DAT

- B-DAT
based O
super-resolution. O
IEEE O
Computer O
Graphics O

- B-DAT
plications, O
22(2):56–65, O
2002. O
1 O

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
June O

- B-DAT
tration. O
CVGIP, O
53(3):231–239, O
1991. O
4 O

- B-DAT

- B-DAT

- B-DAT
based O
learning O
applied O
to O
document O

- B-DAT
ings O
of O
the O
IEEE, O
1998 O

- B-DAT
cal O
statistics. O
In O
ICCV, O
2001 O

- B-DAT
resolution. O
In O
ICCV, O
2013. O
4 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Hidalgo, O
and O
B. O
Rosenhahn. O
Fast O

- B-DAT

- B-DAT
alizing O
the O
nonlocal-means O
to O
super-resolution O

- B-DAT
tion. O
Image O
Processing, O
IEEE O
Transactions O

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT

- B-DAT
lucination O
for O
image O
super-resolution. O
In O

- B-DAT
cal O
Imaging, O
Processing O
and O
Analysis O

- B-DAT
demic O
Press, O
2000. O
1 O

- B-DAT
borhood O
regression O
for O
fast O
example-based O

- B-DAT

- B-DAT
ors O
for O
post-processing O
demosaiced O
images O

- B-DAT
similarities O
for O
single O
frame O
super-resolution O

- B-DAT
ume O
6494, O
pages O
497–510, O
2011 O

- B-DAT

- B-DAT

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT
cess., O
19(11):2861–2873, O
2010. O
1 O

- B-DAT
resolution O
as O
sparse O
representation O
of O

- B-DAT

- B-DAT

- B-DAT
ing O
multiple O
linear O
mappings O
for O

- B-DAT

- B-DAT
resolution O
using O
deformable O
patches. O
In O

the O
super-resolution O
performance O
on O
the O
Set5 B-DAT
[2], O
Set14 O
[40], O
and O
BSD100 O

our O
model O
evaluated O
on O
the O
Set5 B-DAT
[2], O
Set14 O
[40], O
and O
BSD100 O

Set5 B-DAT
PSNR O
(dB) O
SSIM O
NIQE O
SR O

The O
results O
are O
for O
the O
Set5 B-DAT
[2], O
Set14 O
[40], O
and O
BSD100 O

Set5 B-DAT
PSNR O
(dB) O
SSIM O
NIQE O
SR O

without O
multi-pass O
upscaling O
for O
the O
Set5 B-DAT
[2], O
Set14 O
[40], O
and O
BSD100 O

Set5 B-DAT
PSNR O
(dB) O
SSIM O
NIQE O
SR O

models O
are O
evaluated O
on O
the O
Set5 B-DAT
[2], O
Set14 O
[40], O
and O
BSD100 O

Set5 B-DAT
PSNR O
(dB) O
SSIM O
NIQE O
SR O

models O
are O
evaluated O
on O
the O
Set5 B-DAT
[2], O
Set14 O
[40], O
and O
BSD100 O

Set5 B-DAT
PSNR O
(dB) O
SSIM O
NIQE O
SR O

Set5 B-DAT

Set5 B-DAT

Set5 B-DAT

Set5 B-DAT

Set5 B-DAT

a O
deep O
network O
for O
multi-pass O
upscaling B-DAT
in O
company O
with O
a O
discriminator O

deep O
learning-based O
super-resolution O
model, O
enhanced O
upscaling B-DAT
super-resolution O
(EUSR) O
[14], O
from O
the O

to O
prop- O
erly O
regularize O
the O
upscaling B-DAT
modules O
to O
keep O
balance O
of O

pass O
perceptual O
super-resolution O
with O
enhanced O
upscaling B-DAT
(4PP-EUSR),” O
which O
is O
based O
on O

our O
model O
employs O
so-called O
“multi-pass O
upscaling B-DAT

low-resolution O
image O
through O
the O
multiple O
upscaling B-DAT
paths O
in O
our O
model O
are O

base O
deep O
learning O
model, O
multi-pass O
upscaling B-DAT
for O
training, O
structure O
of O
the O

which O
consists O
of O
so-called O
“enhanced O
upscaling B-DAT
modules” O
and O
performed O
well O
in O

nents O
(Fig. O
2): O
a O
multi-scale O
upscaling B-DAT
model, O
employing O
the O
model O
in O

three O
upscaled O
images O
via O
multi-pass O
upscaling B-DAT
(Section O
3.2). O
The O
discriminator O
tries O

3.1 O
Enhanced O
upscaling B-DAT
super-resolution O

shared O
feature O
extraction, O
and O
enhanced O
upscaling B-DAT

features O
are O
upscaled O
via O
“enhanced O
upscaling B-DAT
modules,” O
where O
each O
module O
increases O

one, O
two, O
and O
three O
enhanced O
upscaling B-DAT
modules, O
respectively. O
The O
configurable O
parameters O

residual O
blocks O
in O
the O
enhanced O
upscaling B-DAT
modules. O
We O
consider O
EUSR O
as O

our O
base O
upscaling B-DAT
model O
because O
it O
is O
one O

3.2 O
Multi-pass O
upscaling B-DAT

our O
model O
utilizes O
all O
these O
upscaling B-DAT
paths O
to O
produce O
three O
output O

output O
images O
have O
the O
same O
upscaling B-DAT
factor O
of O
4 O
for O
a O

Enhanced O
upscaling B-DAT
moduleEUM O

Fig. O
4. O
Multi-pass O
upscaling B-DAT
process, O
which O
produces O
three O
upscaled O

the O
other O
hand, O
our O
multi-pass O
upscaling B-DAT
extends O
it O
with O
a O
different O

three O
images O
obtained O
from O
different O
upscaling B-DAT
paths O
are O
used O
for O
training O

that O
may O
occur O
during O
direct O
upscaling B-DAT
via O
the O
×4 O
path, O
two-pass O

upscaling B-DAT
via O
the O
×2 O
path, O
and O

upscaling B-DAT
via O
the O
×8 O
path O
and O

the O
model O
to O
handle O
various O
upscaling B-DAT
scenarios O

ground-truth O
images. O
This O
helps O
our O
upscaling B-DAT
model O
generating O
more O
natural O
images O

networks: O
EUSR O
[14] O
as O
an O
upscaling B-DAT
model O
and O
SRGAN O
[17] O
as O

two O
newly O
proposed O
components: O
multi-pass O
upscaling B-DAT
and O
qualitative O
score O
predictors. O
In O

Thanks O
to O
the O
multi-pass O
upscaling, B-DAT
the O
proposed O
model O
can O
learn O

various O
upscaling B-DAT
patterns, O
which O
will O
be O
further O

the O
residual O
module O
and O
the O
upscaling B-DAT
part O
of O
the O
EUSR O
model O

images. O
Then, O
one O
of O
the O
upscaling B-DAT
paths O
(i.e., O
×2, O
×4, O
and O

effective O
batch O
sizes O
of O
the O
upscaling B-DAT
and O
discrimina- O
tive O
models O
are O

the O
outputs O
obtained O
from O
different O
upscaling B-DAT
paths, O
comparing O
the O
per- O
formance O

trained O
with O
and O
without O
multi-pass O
upscaling, B-DAT
inves- O
tigating O
the O
roles O
of O

interpolation. O
It O
is O
a O
traditional O
upscaling B-DAT
method, O
which O
inter- O
polates O
pixel O

which O
supports O
multiple O
factors O
of O
upscaling B-DAT

5.2 O
Comparing O
upscaling B-DAT
paths O

images O
by O
utilizing O
all O
the O
upscaling B-DAT
paths: O
by O
passing O
through O
the O

10. O
Images O
reconstructed O
by O
different O
upscaling B-DAT
paths O
of O
our O
model. O
The O

results O
obtained O
from O
the O
different O
upscaling B-DAT
paths O
to O
examine O
what O
aspects O

the O
performance O
of O
the O
three O
upscaling B-DAT
paths O
of O
our O
model. O
While O

PI O
values. O
This O
implies O
that O
upscaling B-DAT
using O
the O
×2 O
path O
or O

different O
de- O
pending O
on O
the O
upscaling B-DAT
paths, O
although O
the O
overall O
patterns O

obtained O
by O
the O
two- O
pass O
upscaling B-DAT
using O
the O
×2 O
path O
contains O

two O
passes, O
thus O
the O
two-pass O
upscaling B-DAT
is O
not O
fully O
optimized. O
Second O

trained O
with O
and O
without O
multi-pass O
upscaling B-DAT
for O
the O
Set5 O
[2], O
Set14 O

These O
results O
show O
that O
each O
upscaling B-DAT
path O
of O
our O
model O
learns O

the O
shared O
part O
of O
the O
upscaling B-DAT
paths O
(i.e., O
the O
intermediate O
residual O

5.3 O
Effectiveness O
of O
multi-pass O
upscaling B-DAT

The O
4PP-EUSR O
model O
employs O
multi-pass O
upscaling B-DAT
as O
aforementioned O
in O
Sec- O
tion O

trained O
with O
and O
without O
multi-pass O
upscaling B-DAT

demonstrates O
that O
employing O
multi- O
pass O
upscaling B-DAT
is O
beneficial O
to O
enhance O
both O

The O
model O
trained O
with O
multi-pass O
upscaling B-DAT
shows O
larger O
PSNR O
and O
SSIM O

This O
confirms O
that O
the O
multi-pass O
upscaling B-DAT
can O
improve O
the O
overall O
quality O

Deep O
residual O
network O
with O
enhanced O
upscaling B-DAT
module O
for O
super-resolution. O
In: O
Proceedings O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
per, O
we O
propose O
a O
novel O

- B-DAT

- B-DAT
tional O
quantitative O
performance. O
The O
proposed O

- B-DAT

- B-DAT
work O
and O
two O
quantitative O
score O

- B-DAT

- B-DAT
age O
quality O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
troduced O
convolutional O
layers O
and O
showed O

- B-DAT

- B-DAT

- B-DAT
truth O
(b) O
Upscaled O
by O
bicubic O

- B-DAT
ation O
(d) O
Upscaled O
with O
perceptual O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ror O
[39]. O
They O
mainly O
aim O

- B-DAT
tained O
images, O
which O
can O
be O

- B-DAT

- B-DAT

- B-DAT
tural O
similarity O
(SSIM) O
[37]. O
Fig O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uralness O
of O
the O
output O
images O

- B-DAT
posed O
a O
super-resolution O
model O
named O

- B-DAT
tures O
of O
VGG19 O
[31] O
when O

- B-DAT

- B-DAT
though O
these O
approaches O
improve O
naturalness O

- B-DAT
proved O
results. O
In O
addition, O
it O

- B-DAT

- B-DAT
tor O
relies O
on O
just O
finding O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
quality. O
For O
example, O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
erly O
regularize O
the O
upscaling O
modules O

- B-DAT

- B-DAT
pass O
perceptual O
super-resolution O
with O
enhanced O

- B-DAT

- B-DAT
ing O
the O
aforementioned O
issues O
via O

- B-DAT

- B-DAT

- B-DAT
scaled O
images O
produced O
by O
passing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uralness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
challenge O
[34]: O
the O
enhanced O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ization O
and O
blending O
outputs O
generated O

- B-DAT
gested O
a O
multi-scale O
super-resolution O
method O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
model O
based O
on O
residual O

- B-DAT

- B-DAT
nism O
into O
the O
super-resolution O
task O

- B-DAT

- B-DAT
age O
classifiers. O
In O
the O
former O

- B-DAT
guish O
the O
ground-truth O
images O
from O

- B-DAT

- B-DAT
scaled O
images O
properly. O
When O
an O

- B-DAT

- B-DAT
termediate O
layers O
of O
the O
classifier O

- B-DAT

- B-DAT
lating O
losses O
of O
their O
super-resolution O

- B-DAT

- B-DAT

- B-DAT
nents O
(Fig. O
2): O
a O
multi-scale O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
3.1) O
generates O
three O
upscaled O

- B-DAT

- B-DAT

- B-DAT
tion O
3.3). O
The O
two O
qualitative O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
traction O
part O
extracts O
low-level O
features O

- B-DAT

- B-DAT

- B-DAT
ing O
paths O
have O
one, O
two O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
entiate O
them O
from O
the O
ground-truth O

- B-DAT
inator O
network O
consists O
of O
several O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
connected O
layer O
with O
the O
softmax O

- B-DAT
ploy O
the O
AVA O
dataset O
[26 O

- B-DAT
ferent O
objectives: O
EUSR O
is O
for O

- B-DAT
ter O
perceptual O
quality. O
Our O
proposed O

- B-DAT
titative O
and O
perceptual O
quality, O
with O

- B-DAT

- B-DAT

- B-DAT
titative O
and O
perceptual O
quality. O
While O

- B-DAT

- B-DAT
inforces O
it O
to O
focus O
on O

- B-DAT
oughly O
investigate O
this O
in O
Section O

- B-DAT

- B-DAT
ally O
improved O
images, O
since O
they O

- B-DAT

- B-DAT
itative O
score O
predictors, O
and O
training O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stead, O
we O
set O
the O
input O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tor O
network O
using O
the O
two O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
jective O
of O
the O
super-resolution O
task O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
diate O
output O
is O
1,280 O
[30 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
step, O
two O
input O
image O

- B-DAT
tive O
models O
are O
six O
and O

- B-DAT

- B-DAT

- B-DAT
mance O
of O
our O
method O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
paring O
the O
outputs O
obtained O
from O

- B-DAT
formance O
of O
our O
method O
trained O

- B-DAT

- B-DAT
tigating O
the O
roles O
of O
loss O

- B-DAT

- B-DAT
cluding O
PSNR, O
SSIM O
[37], O
NIQE O

- B-DAT

- B-DAT

- B-DAT
ity O
metrics O
are O
calculated O
on O

- B-DAT
isting O
studies O
[17,14,19]. O
In O
addition O

- B-DAT

- B-DAT
polates O
pixel O
values O
based O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
MSE O
model O
is O
trained O
with O

- B-DAT

- B-DAT

- B-DAT

-128 B-DAT
layer O
of O
VGG19. O
Their O
results O

- B-DAT
thors’ O
supplementary O
material2 O

- B-DAT
Net, O
but O
does O
not O
employ O

- B-DAT

- B-DAT
ensemble” O
strategy, O
which O
obtains O
eight O

- B-DAT

- B-DAT
scale O
super-resolution O
and O
consists O
of O

- B-DAT
plained O
in O
Section O
3.1. O
We O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-128 B-DAT
layer O
of O
VGG19), O
and O
SRGAN-VGG54 O

- B-DAT

-512 B-DAT
layer O
of O
VGG19). O
The O
compared O

- B-DAT

- B-DAT

- B-DAT
based O
loss O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tween O
the O
VGG19 O
features O
for O

- B-DAT

- B-DAT
ber O
of O
model O
parameters, O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
though O
all O
the O
models O
except O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uated O
on O
the O
three O
datasets O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
scaled O
images O
have O
poor O
perceptual O

- B-DAT
tual O
quality O
(i.e., O
SRGAN O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pares O
the O
baselines O
and O
our O

- B-DAT

- B-DAT
tures O
are O
expected. O
First, O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tative O
quality O
than O
all O
the O

- B-DAT
sidering O
both O
the O
quantitative O
and O

- B-DAT
ploying O
only O
the O
reconstruction O
loss O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

P O
- B-DAT

- B-DAT

- B-DAT
formance O
of O
the O
super-resolution O
methods O

- B-DAT

- B-DAT
jective O
tests O
in O
the O
recommendation O

- B-DAT

-13 B-DAT
[36]. O
As O
for O
the O
evaluation O

- B-DAT

- B-DAT

- B-DAT
VGG22 O
get O
the O
lowest O
opinion O

- B-DAT

- B-DAT
tive O
and O
perceptual O
quality O
in O

- B-DAT

- B-DAT

- B-DAT
ing O
paths O
of O
the O
4PP-EUSR O

- B-DAT
pending O
on O
the O
upscaling O
paths O

- B-DAT

- B-DAT
pass O
upscaling O
using O
the O
×2 O

- B-DAT

- B-DAT
son O
is O
due O
to O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put O
is O
not O
necessarily O
avoided O

- B-DAT
resolution O
and O
thus O
the O
model O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
3.2. O
To O
investigate O
its O

- B-DAT

- B-DAT
pass O
upscaling O
is O
beneficial O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions. O
The O
input O
and O
ground-truth O

- B-DAT

- B-DAT
ity O
(i.e., O
larger O
PSNR O
values O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ful O
to O
construct O
dispersed O
high-frequency O

- B-DAT
cally, O
we O
alter O
the O
weight O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rea, O
under O
the O
“ICT O
Consilience O

-2018 B-DAT

-2017 B-DAT

-0 B-DAT

-01015 B-DAT

-16 B-DAT

-0004, B-DAT
Development O
of O
Intelligent O

- B-DAT

- B-DAT
derstanding O

- B-DAT
mawat, O
S., O
Irving, O
G., O
Isard O

- B-DAT

- B-DAT
chine O
learning. O
In: O
Proceedings O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
British O
Machine O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
performance O
on O
ImageNet O
classification O

- B-DAT
national O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks. O
In: O
Proceedings O

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT

- B-DAT
space O
projection O
and O
neighbor O
embedding O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puter O
Vision. O
pp. O
416–423 O
(2001 O

- B-DAT

- B-DAT

- B-DAT
mation O
with O
non-aligned O
data. O
arXiv:1803.02077 O

- B-DAT

- B-DAT

- B-DAT
thetic O
visual O
analysis. O
In: O
Proceedings O

- B-DAT
nition O
challenge. O
International O
Journal O
of O

- B-DAT

- B-DAT

- B-DAT
ings O
of O
the O
IEEE O
International O

- B-DAT
verted O
residuals O
and O
linear O
bottlenecks O

- B-DAT

- B-DAT
tion O
architecture O
for O
computer O
vision O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

-13 B-DAT

- B-DAT
ing O
13(4), O
600–612 O
(2004 O

- B-DAT

- B-DAT

- B-DAT
computing O
74(17), O
3193–3203 O
(2011 O

- B-DAT

- B-DAT

- B-DAT
representations. O
In: O
Proceedings O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

and O
VDSR O
[15], O
on O
dataset O
Set5 B-DAT
[1] O
and O
Set14 O
[32]. O
To O

For O
the O
F1 O
setting O
on O
Set5 B-DAT
and O
Set14, O
our O
method O
pro O

For O
the O
F1 O
setting O
on O
Set5 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
per, O
we O
show O
that O
proper O

- B-DAT
pensation O
is O
crucial O
for O
achieving O

- B-DAT

- B-DAT
iments O
show O
the O
suitability O
of O

- B-DAT

- B-DAT

- B-DAT
porates O
the O
SPMC O
layer O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
image O
SR O
where O
details O
have O

- B-DAT

- B-DAT
struct O
accurate O
correspondence; O
and O
(2 O

- B-DAT

- B-DAT
secutive O
frames O
increases O
the O
difficulty O

- B-DAT
sponding O
image O
regions, O
subtle O
sub-pixel O

- B-DAT
ily O
benefits O
restoration O
of O
details O

- B-DAT

- B-DAT

- B-DAT
struct O
the O
HR O
output O
based O

- B-DAT
work. O
Most O
of O
these O
methods O

- B-DAT
by-case O
parameter-tuning O
and O
costly O
computation O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
(SPMC) O
strategy, O
which O
is O

- B-DAT
age O
detail O
fusion O
from O
multiple O

- B-DAT
cess O
of O
video O
SR. O
We O

- B-DAT
based O
video O
SR O
systems O
can O

- B-DAT

- B-DAT
herent O
in O
input O
frames, O
or O

- B-DAT

- B-DAT
meaningful O
property O
of O
SR O
systems O

- B-DAT

- B-DAT
plied. O
For O
example, O
ESPCN O
[26 O

- B-DAT
put, O
once O
trained O

- B-DAT

- B-DAT
plied O
for O
arbitrary O
scaling O
factors O

- B-DAT

- B-DAT

- B-DAT
CNN O
[3], O
a O
majority O
of O

- B-DAT
resolution O
images O
as O
input, O
and O

- B-DAT

- B-DAT
volution O
layer O
instead O

- B-DAT

- B-DAT

- B-DAT
tained O
under O
different O
flow O
parameters O

- B-DAT
timation O
is O
separated O
from O
training O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dicted O
affine O
transformation O
parameters. O
Based O

- B-DAT
Net O
[13] O
used O
a O
similar O

- B-DAT
dence. O
Yu O
et O
al. O
[30 O

- B-DAT

- B-DAT

- B-DAT
size. O
The O
transpose O
ST O
corresponds O

- B-DAT

- B-DAT
duces O
the O
green O
signal O
through O

- B-DAT
mation O
operators, O
respectively. O
ni O
is O

- B-DAT
duce O
the O
warped O
image. O
However O

- B-DAT
ally O
makes O
use O
of O
flow O

- B-DAT

- B-DAT
construction O
error O

- B-DAT
forward O
generation O
process O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
between O
frames; O
aligning O
frames O

- B-DAT
tion; O
and O
finally O
increasing O
image O

LI O
- B-DAT

F O
- B-DAT

- B-DAT

- B-DAT
pensation O
transformer O
(MCT) O
module O
from O

- B-DAT
eters O
and O
accordingly O
less O
computation O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
factor. O
The O
layer O
contains O

- B-DAT
ordinates O
are O
first O
calculated O
according O

- B-DAT
ordinates O
as O
operator O
WF O
;α O

- B-DAT
structed O
in O
the O
enlarged O
image O

- B-DAT

- B-DAT
tive O
with O
respect O
to O
each O

- B-DAT
ilar O
derivatives O
can O
be O
derived O

- B-DAT
lation O
kernel, O
because O
of O
its O

- B-DAT
lowing O
back-propagating O
loss O
to O
flow O

- B-DAT
pensation O
and O
resolution O
enhancement. O
Note O

- B-DAT
works O
with O
almost O
no O
additional O

- B-DAT
sated O
frames O
{JHi O
} O
expressed O

- B-DAT

- B-DAT

- B-DAT
upsampling, O
{JHi O
} O
is O
sparse O

- B-DAT

- B-DAT
erence O
frame O
as O
the O
guidance O

- B-DAT
age O
structures. O
On O
the O
other O

- B-DAT

- B-DAT
ing O
information O
in O
other O
frames O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
to O
1/4 O
of O
it O

- B-DAT
formation O
can O
be O
effectively O
aggregated O

- B-DAT

- B-DAT

- B-DAT
tion O
layers O
are O
with O
kernel O

- B-DAT
tified O
Linear O
Units O
(ReLU) O
are O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vised O
warping O
loss O
is O
used O

- B-DAT
olution, O
aligned O
with O
ILi O

- B-DAT
ularization O
weight. O
We O
set O
λ1 O

- B-DAT
responding O
to O
reference O
frame O
IL0 O

- B-DAT

- B-DAT

- B-DAT
taining O
rich O
fine O
details. O
To O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
of O
[19, O
20, O
23 O

- B-DAT
lation. O
LR O
input O
is O
obtained O

- B-DAT
ule O
(clipped O
by O
global O
norm O

- B-DAT
cess. O
At O
each O
iteration, O
we O

- B-DAT
tive O
frames O
(e.g. O
NF O

- B-DAT
domly O
crop O
a O
100×100 O
image O

- B-DAT
ing O
factor. O
Above O
parameters O
are O

- B-DAT
erations, O
we O
fix O
the O
parameters O

- B-DAT
ing O
only O
loss O
LSR O
in O

- B-DAT

- B-DAT

- B-DAT
duces O
multiple O
outputs O
(one O
for O

- B-DAT
cedure. O
An O
example O
is O
shown O

- B-DAT
bic O
×4 O
for O
reference O
frame O

- B-DAT
responding O
to O
three O
time O
steps O

- B-DAT

- B-DAT
age O
in O
our O
method, O
the O

- B-DAT
ing O
all O
input O
frames O
with O

- B-DAT
ically, O
Fig. O
5(f)-(h) O
are O
outputs O

- B-DAT
ever, O
if O
we O
only O
use O

- B-DAT

- B-DAT
sults O
are O
almost O
the O
same O

- B-DAT
ternal O
examples O
because O
if O
the O

0.92 O
31.85 O
/ O
0.92 O
- B-DAT
33.39 O
/ O
0.94 O
36.71 O

0.82 O
29.42 O
/ O
0.87 O
- B-DAT
28.55 O
/ O
0.85 O
31.92 O

0.92 O
31.82 O
/ O
0.92 O
- B-DAT
35.44 O
/ O
0.95 O
36.62 O

0.82 O
29.55 O
/ O
0.87 O
- B-DAT
30.73 O
/ O
0.88 O
32.10 O

Vid4×3 O
25.64 O
/ O
0.80 O
- B-DAT
25.31 O
/ O
0.76 O
27.25 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tails, O
the O
emphasis O
on O
the O

- B-DAT
tail O
recovery O
and O
slightly O
degrade O

- B-DAT

- B-DAT
tively O
estimates O
motion O
flow, O
blur O

- B-DAT
ditional O
and O
CNN-based O
methods. O
We O

- B-DAT
cent O
deep-learning-based O
method O
VSRnet O
[14 O

- B-DAT
son. O
We O
use O
author-provided O
implementation O

- B-DAT

- B-DAT
video O
dataset O
VID4 O
[20]. O
The O

- B-DAT
duces O
comparable O
or O
slightly O
lower O

- B-DAT

- B-DAT

- B-DAT

0.96 O
37.35 O
/ O
0.96 O
- B-DAT
Set O
5 O
(×3) O
32.75 O

0.92 O
33.45 O
/ O
0.92 O
- B-DAT
Set O
5 O
(×4) O
30.49 O

0.91 O
32.70 O
/ O
0.91 O
- B-DAT
Set O
14 O
(×3) O
29.30 O

0.83 O
29.36 O
/ O
0.83 O
- B-DAT
Set O
14 O
(×4) O
27.45 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ings O
for O
other O
methods O
(F31 O

- B-DAT
Rnet O
[14] O
requires O
≈40s O
for O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pensation O
layer O
that O
can O
better O

- B-DAT

- B-DAT
ment. O
We O
have O
conducted O
extensive O

- B-DAT
date O
the O
effectiveness O
of O
each O

- B-DAT

- B-DAT
itatively O
and O
quantitatively, O
at O
the O

- B-DAT

- B-DAT
bic O
×4. O
(b)-(d) O
Output O
from O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT

- B-DAT

- B-DAT
tion O
algorithm O
for O
pure O
translational O

- B-DAT

- B-DAT
ing, O
10(8):1187–1193, O
2001 O

- B-DAT

- B-DAT
sorflow.org O

- B-DAT
ume O
9, O
pages O
249–256, O
2010 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
learning-detection. O
IEEE O
Trans. O
Pattern O
Anal O

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT

- B-DAT
jani, O
J. O
Totz, O
Z. O
Wang O

- B-DAT

- B-DAT
age O
super-resolution O
using O
a O
generative O

- B-DAT
resolution O
via O
deep O
draft-ensemble O
learning O

- B-DAT
ing O
for O
stereo O
matching. O
In O

- B-DAT

- B-DAT

- B-DAT
ing O
very O
deep O
convolutional O
encoder-decoder O

- B-DAT
lutional O
networks O
for O
disparity, O
optical O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT
resolution O
without O
explicit O
subpixel O
motion O

- B-DAT
ness O
constancy O
and O
motion O
smoothness O

- B-DAT
volutional O
neural O
network O
to O
compare O

- B-DAT
nal O
of O
Machine O
Learning O
Research O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

images O
and O
two O
test O
datasets O
Set5 B-DAT
and O
Set14. O
Many O
recent O
state-of-the-art O

the O
trained O
models O
on O
the O
Set5 B-DAT
and O
Set14 O
dataset O

two O
different O
test O
data O
sets: O
Set5 B-DAT
and O
Set14. O
Following O
previous O
works O

Set5 B-DAT
Bicubic O
K-SVD O
[58] O
ANR O
[54 O

3 O
and O
×4 O
on O
the O
Set5 B-DAT
dataset. O
All O
the O
methods O
use O

images O
and O
two O
test O
datasets O
Set5 B-DAT

the O
trained O
models O
on O
the O
Set5 B-DAT

two O
different O
test O
data O
sets: O
Set5 B-DAT

Set5 B-DAT

3 O
and O
×4 O
on O
the O
Set5 B-DAT

trained O
diffusion O
models O
for O
three O
upscaling B-DAT
factors×2,×3 O
and×4, O
using O
exactly O
the O

image, O
and O
a O
regular O
bicubic O
upscaling B-DAT
method O
is O
applied O
to O
the O

run O
time O
(s) O
performance O
for O
upscaling B-DAT
factors O
×2, O
×3 O
and O
×4 O

image O
from O
Set14 O
with O
an O
upscaling B-DAT
factor O
×3. O
Note O
the O
differences O

with O
repeated O
hexagons O
for O
the O
upscaling B-DAT
factor O
×3 O

and O
the O
trained O
model O
for O
upscaling B-DAT
factor O
×3 O
will O
also O
lead O

to O
the O
SISR O
problem O
of O
upscaling B-DAT
factor O
×2. O
It O
is O
generally O

the O
noise O
levels O
or O
all O
upscaling B-DAT
factors O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
Proc O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tal O
operations O
in O
image O
processing O

- B-DAT

- B-DAT
tion O
are O
non-local O
methods O
based O

- B-DAT

- B-DAT
the-art O
techniques O
mainly O
concentrate O
on O

- B-DAT
sian O
denoising, O
BM3D O
[15] O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
posed O
CSF O
model O
offers O
high O

- B-DAT
erate O
fast O
and O
effective O
models O

- B-DAT
tural O
simplicity O
of O
these O
models O

- B-DAT
proach O
for O
various O
problems O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ström O
[43], O
which O
introduces O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tigated. O
Researchers O
also O
propose O
to O

- B-DAT

- B-DAT
order O
diffusion O
models O
[28], O
[17 O

- B-DAT

- B-DAT

- B-DAT
tics/regularization O
As O
shown O
in O
[51 O

- B-DAT
tion O
between O
anisotropic O
diffusion O
models O

- B-DAT
imation O
of O
the O
gradient O
operators O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fore. O
If O
ignoring O
the O
coupled O

- B-DAT

- B-DAT
vector O
product O
∇xu O
can O
be O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
including O
Gaussian O
image O
denoising,single O

- B-DAT
tions O
between O
the O
proposed O
model O

- B-DAT
porate O
a O
reaction O
term O
in O

- B-DAT
tion O
specific O
reaction O
terms O
ψ(u O

- B-DAT
sion O
model O
Note O
that O
the O

- B-DAT
ferentiable. O
In O
order O
to O
handle O

- B-DAT
differentiable O
data O
term, O
e.g., O
the O

- B-DAT
vestigated O
in O
Section O
6, O
we O

- B-DAT

- B-DAT
tional. O
Therefore, O
Eq. O
(6) O
can O

- B-DAT

-1 B-DAT
– O
f O

-1 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
defined O
image O
invariants. O
Therefore, O
this O

- B-DAT
forward O
network. O
We O
refer O
to O

- B-DAT

- B-DAT

- B-DAT
cessing O
task, O
and O
then O
exploit O

- B-DAT

output O
of O
the O
diffusion O
process O
- B-DAT
uT O
is O
optimized. O
We O
call O

- B-DAT
imization O
problem O
with O
respect O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ators O
Kti O
are O
defined O
as O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
with O
respect O
to O
Θt O

- B-DAT

- B-DAT
mental O
material O

- B-DAT

- B-DAT

- B-DAT
work O
of O
Timofte O
et O
al O

- B-DAT
pare O
single O
image O
super O
resolution O

- B-DAT

- B-DAT

- B-DAT

-5 B-DAT
sub-images O
of O
size O
150 O

- B-DAT

- B-DAT

-2680 B-DAT
@ O
2.80GHz O
(eight O
parallel O
threads O

our O
learned O
TNRD55×5 O
works. O
(b) O
- B-DAT
(e) O
are O
intermediate O
results O
at O

stage O
1 O
- B-DAT
4, O
and O
(f) O
is O
the O

- B-DAT

-5 B-DAT
times O
faster O
than O
our O
CPU O

- B-DAT
ing O
problem O
with O
standard O
deviation O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

far O
the O
best- B-DAT
reported O
algorithm O
- O
WNNM. O
It O
turns O
out O
that O

- B-DAT
tained O
the O
following O
results: O
(A.I O

- B-DAT

- B-DAT

- B-DAT
ing O
of O
the O
influence O
functions O

- B-DAT
tions O
of O
the O
TNRD55×5 O
model O

- B-DAT

- B-DAT

- B-DAT
ably O
it O
is O
stimulated O
by O

- B-DAT
tion O
(b), O
can O
adaptively O
switch O

- B-DAT

- B-DAT

- B-DAT
volving O
the O
learned O
influence O
functions O

- B-DAT
tions O

- B-DAT

- B-DAT
sion O
models. O
(a) O
is O
generated O

- B-DAT
porate O
a O
reaction O
term O
in O

- B-DAT

- B-DAT
work O
is O
attributed O
to O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
order O
derivative O
filters, O
as O
well O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fitting. O
However, O
our O
current O
CPU O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
denoising O
algorithms. O
In O
experiments O

- B-DAT
sulting O
TNRD7×7 O
model O
achieves O
the O

TNRD57×7 O
model O
outperforms O
the O
benchmark O
- B-DAT
BM3D O
method O
by O
0.35dB O
in O

also O
surpasses O
the O
best-reported B-DAT
algorithm O
- O
WNNM O
method, O
which O
is O
quite O

- B-DAT

- B-DAT

e.g., O
start O
Matlab O
with O
- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-2680 B-DAT
v2 O
@ O
2.80GHz; O
(2) O
the O

- B-DAT

- B-DAT
putation O
of O
the O
CSF O
model O

- B-DAT

In O
contrast, O
another O
non-local B-DAT
model O
- O
WNNM O
achieves O
compelling O
denoising O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ple O
in O
Figure O
11 O
on O

- B-DAT

- B-DAT
polation O
of O
the O
LR O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

investigate O
the O
JPEG O
deblocking O
problem O
- B-DAT
suppressing O
the O
block O
artifacts O
in O

- B-DAT

- B-DAT

- B-DAT

PSNR O
Time O
baby O
2 O
37.07 O
- B-DAT
38.25 O
8.21 O
38.44 O
1.39 O
38.30 O

38.51 O
1.52 O
bird O
2 O
36.81 O
- B-DAT
39.93 O
2.67 O
40.04 O
0.44 O
40.64 O

41.29 O
0.59 O
butterfly O
2 O
27.43 O
- B-DAT
30.65 O
2.14 O
30.48 O
0.38 O
32.20 O

33.16 O
0.56 O
head O
2 O
34.86 O
- B-DAT
35.59 O
2.46 O
35.66 O
0.41 O
35.64 O

35.71 O
0.60 O
woman O
2 O
32.14 O
- B-DAT
34.49 O
2.45 O
34.55 O
0.43 O
34.94 O

35.50 O
0.57 O
average O
2 O
33.66 O
- B-DAT
35.78 O
3.59 O
35.83 O
0.61 O
36.34 O

36.83 O
0.77 O
baby O
3 O
33.91 O
- B-DAT
35.08 O
3.77 O
35.13 O
0.79 O
35.01 O

35.28 O
1.52 O
bird O
3 O
32.58 O
- B-DAT
34.57 O
1.34 O
34.60 O
0.27 O
34.91 O

36.11 O
0.59 O
butterfly O
3 O
24.04 O
- B-DAT
25.94 O
1.08 O
25.90 O
0.24 O
27.58 O

28.90 O
0.56 O
head O
3 O
32.88 O
- B-DAT
33.56 O
1.35 O
33.63 O
0.24 O
33.55 O

33.78 O
0.60 O
woman O
3 O
28.56 O
- B-DAT
30.37 O
1.14 O
30.33 O
0.24 O
30.92 O

31.77 O
0.57 O
average O
3 O
30.39 O
- B-DAT
31.90 O
1.74 O
31.92 O
0.35 O
32.39 O

33.17 O
0.77 O
baby O
4 O
31.78 O
- B-DAT
33.06 O
2.63 O
33.03 O
0.59 O
32.98 O

33.29 O
1.52 O
bird O
4 O
30.18 O
- B-DAT
31.71 O
0.70 O
31.82 O
0.18 O
31.98 O

32.98 O
0.59 O
butterfly O
4 O
22.10 O
- B-DAT
23.57 O
0.54 O
23.52 O
0.14 O
25.07 O

26.22 O
0.56 O
head O
4 O
31.59 O
- B-DAT
32.21 O
0.66 O
32.27 O
0.16 O
32.19 O

32.57 O
0.60 O
woman O
4 O
26.46 O
- B-DAT
27.89 O
0.72 O
27.80 O
0.23 O
28.21 O

29.17 O
0.57 O
average O
4 O
28.42 O
- B-DAT
29.69 O
1.05 O
29.69 O
0.26 O
30.09 O

- B-DAT

- B-DAT
mance O
evaluation. O
We O
distorted O
the O

- B-DAT

- B-DAT

Time O
PSNR O
Time O
baboon O
23.21 O
- B-DAT
23.52 O
3.54 O
23.56 O
0.77 O
23.60 O

0.75 O
23.62 O
1.30 O
barbara O
26.25 O
- B-DAT
26.76 O
6.24 O
26.69 O
1.23 O
26.66 O

1.18 O
26.25 O
1.75 O
bridge O
24.40 O
- B-DAT
25.02 O
3.98 O
25.01 O
0.80 O
25.07 O

0.81 O
25.29 O
1.19 O
coastguard O
26.55 O
- B-DAT
27.15 O
1.54 O
27.08 O
0.36 O
27.20 O

0.35 O
27.12 O
0.65 O
comic O
23.12 O
- B-DAT
23.96 O
1.37 O
24.04 O
0.27 O
24.39 O

0.34 O
24.67 O
0.65 O
face O
32.82 O
- B-DAT
33.53 O
1.10 O
33.62 O
0.24 O
33.58 O

0.29 O
33.82 O
0.57 O
flowers O
27.23 O
- B-DAT
28.43 O
2.66 O
28.49 O
0.57 O
28.97 O

0.61 O
29.55 O
0.90 O
foreman O
31.18 O
- B-DAT
33.19 O
1.54 O
33.23 O
0.30 O
33.35 O

0.36 O
34.65 O
0.65 O
lenna O
31.68 O
- B-DAT
33.00 O
3.89 O
33.08 O
0.79 O
33.39 O

0.77 O
33.77 O
1.19 O
man O
27.01 O
- B-DAT
27.90 O
3.81 O
27.92 O
0.76 O
28.18 O

0.80 O
28.52 O
1.17 O
monarch O
29.43 O
- B-DAT
31.10 O
6.13 O
31.09 O
1.13 O
32.39 O

1.12 O
33.61 O
1.66 O
pepper O
32.39 O
- B-DAT
34.07 O
3.84 O
33.82 O
0.80 O
34.35 O

0.82 O
35.06 O
1.20 O
ppt3 O
23.71 O
- B-DAT
25.23 O
4.53 O
25.03 O
1.01 O
26.02 O

0.98 O
27.08 O
1.48 O
zebra O
26.63 O
- B-DAT
28.49 O
3.36 O
28.43 O
0.69 O
28.87 O

29.40 O
1.04 O
average O
performance O
27.54 O
- B-DAT
28.67 O
3.40 O
28.65 O
0.69 O
29.00 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferent O
compression O
parameter O
q. O
We O

- B-DAT
ing, O
4 O
stages O
are O
already O

in O
terms O
of O
run O
time) O
- B-DAT
SADCT O
consumes O
about O
56.5s11. O
Furthermore O

- B-DAT
SR[9] O
SADCT[22] O
RTF[33] O
TNRD O

- B-DAT
work O
for O
effective O
image O
restoration O

- B-DAT

- B-DAT

- B-DAT
ity, O
cf. O
Fig O
5 O

- B-DAT

to O
define O
the O
ground O
truth O
- B-DAT
the O
expected O
output O
of O
the O

- B-DAT
fusion O
network O
during O
training. O
For O

- B-DAT

- B-DAT
backward O
steps O
provide O
good O
approximation O

- B-DAT
ing O
to O
consider O
learned O
nonlinear O

- B-DAT
dard O
CNs O
could O
lead O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
wise O
training O
of O
deep O
networks O

- B-DAT

- B-DAT
cies O
with O
gradient O
descent O
is O

- B-DAT

- B-DAT
sion O
via O
a O
learned O
dictionary O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
parametric O
image O
restoration O
models: O
A O

- B-DAT

- B-DAT

- B-DAT
ization O
and O
diffusion O
approach O
to O

- B-DAT

- B-DAT
tive O
nonlocal O
sparsity-based O
modeling. O
IEEE O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
plied O
physics O
from O
Nanjing O
University O

- B-DAT
nautics O
and O
Astronautics, O
China, O
a O

- B-DAT
gree O
in O
computer O
science O
from O

- B-DAT

- B-DAT
gree O
in O
Computer O
Engineering O
from O

- B-DAT
versity O
of O
Technology O
in O
2004 O

- B-DAT
tively. O
In O
2013 O
he O
received O

- B-DAT
ation O
for O
pattern O
recognition O
(DAGM O

- B-DAT
fessur O
”Mobile O
Computer O
Vision”) O
and O

images O
and O
two O
test O
datasets O
Set5 B-DAT
and O
Set14 O
which O
provides O
5 O

of O
the O
image O
benchmark O
as O
Set5 B-DAT
vs O
SuperTexture O

SRCNN O
(ImageNet) O
ESPCN O
(ImageNet O
relu) O
Set5 B-DAT
3 O
32.39 O
32.39 O
32.55 O
32.52 O

Scale O
Bicubic O
SRCNN O
TNRD O
ESPCN O
Set5 B-DAT
3 O
30.39 O
32.75 O
33.17 O
33.13 O

3 O
26.74 O
27.98 O
28.07 O
28.11 O
Set5 B-DAT
4 O
28.42 O
30.49 O
30.85 O
30.90 O

images O
and O
two O
test O
datasets O
Set5 B-DAT

of O
the O
image O
benchmark O
as O
Set5 B-DAT

SRCNN O
(ImageNet) O
ESPCN O
(ImageNet O
relu) O
Set5 B-DAT
3 I-DAT
32.39 O
32.39 O
32.55 O
32.52 O
33.00 O

Scale O
Bicubic O
SRCNN O
TNRD O
ESPCN O
Set5 B-DAT
3 I-DAT
30.39 O
32.75 O
33.17 O
33.13 O
Set14 O

3 O
26.74 O
27.98 O
28.07 O
28.11 O
Set5 B-DAT
4 I-DAT
28.42 O
30.49 O
30.85 O
30.90 O
Set14 O

which O
learns O
an O
array O
of O
upscaling B-DAT
filters O
to O
upscale O
the O
final O

SR O
pipeline O
with O
more O
complex O
upscaling B-DAT
filters O
specifically O
trained O
for O
each O

different O
methods O
when O
performing O
SR O
upscaling B-DAT
with O
a O
scale O
factor O
of O

Learning O
upscaling B-DAT
filters O
was O
briefly O
suggested O
in O

convolution O
layer O
to O
learn O
the O
upscaling B-DAT
operation O
for O
image O
and O
video O

In O
our O
network, O
upscaling B-DAT
is O
handled O
by O
the O
last O

L O
layers, O
we O
learn O
nL−1 O
upscaling B-DAT
filters O
for O
the O
nL−1 O
feature O

maps O
as O
opposed O
to O
one O
upscaling B-DAT
filter O
for O
the O
input O
image O

to O
a O
single O
fixed O
filter O
upscaling B-DAT
at O
the O
first O
layer. O
This O

refer O
to O
r O
as O
the O
upscaling B-DAT
ratio. O
In O
general, O
both O
ILR O

in O
Fig. O
1, O
to O
avoid O
upscaling B-DAT
ILR O

feature O
maps O
directly O
with O
one O
upscaling B-DAT

implementations O
using O
various O
forms O
of O
upscaling B-DAT
before O
convolution O

luminance O
changes O
[31]. O
For O
each O
upscaling B-DAT
factor, O
we O
train O
a O
specific O

trained O
on O
ImageNet O
with O
an O
upscaling B-DAT
factor O
of O
3: O
(a) O
shows O

Monarch” O
from O
Set14 O
with O
an O
upscaling B-DAT
factor O
of O
3. O
PSNR O
values O

IHR, O
where O
r O
is O
the O
upscaling B-DAT
factor. O
To O
synthesize O
the O
low-resolution O

and O
sub-sample O
it O
by O
the O
upscaling B-DAT
factor. O
The O
sub-images O
are O
extracted O

images O
from O
ImageNet O
[30] O
for O
upscaling B-DAT
factor O
of O
3. O
We O
use O

384022” O
from O
BSD500 O
with O
an O
upscaling B-DAT
factor O
of O
3. O
PSNR O
values O

worse O
results O
than O
an O
adaptive O
upscaling B-DAT
for O
SISR O
and O
requires O
more O

bench O
mark O
data O
set O
with O
upscaling B-DAT
factor O
of O
4 O
shows O
that O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
IEEE O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ture O
where O
the O
feature O
maps O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
the O
non-invertible O
low-pass O
filtering O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
endorfer O
et O
al. O
[27] O
suggested O

- B-DAT
work O
(CNN) O
inspired O
by O
sparse-coding O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gorithms, O
especially O
their O
computational O
and O

- B-DAT
els O
to O
learn O
nonlinear O
relationships O

- B-DAT

- B-DAT

- B-DAT
creasing O
the O
resolution O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rectly O
fed O
to O
the O
network O

- B-DAT
tion O
and O
filter O
size O
reduction O

- B-DAT

- B-DAT
time O
as O
shown O
in O
Sec O

- B-DAT
work O
implicitly O
learns O
the O
processing O

- B-DAT
ing O
[7, O
3, O
31]. O
We O

- B-DAT

- B-DAT

IHR O
using O
a O
Gaussian O
filter O
- B-DAT
thus O
simulating O
the O
camera’s O
point O

spread O
function O
- B-DAT
then O
downsample O
the O
image O
by O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
scaling O
factor O
of O
3. O
The O

- B-DAT

- B-DAT

- B-DAT
tern, O
according O
to O
its O
location O

- B-DAT

- B-DAT
ranges O
the O
elements O
of O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
shuffle O
the O
training O
data O
to O

- B-DAT
ing O
the O
original O
data O
including O

- B-DAT
sampled O
data, O
super-resolved O
data, O
overall O

- B-DAT

- B-DAT
mark O
datasets O
including O
the O
Timofte O

- B-DAT
imately O
10 O
seconds O
in O
length O

- B-DAT

-5 B-DAT

-5 B-DAT
model O
[7], O
(b) O
shows O
weights O

- B-DAT

- B-DAT

-5 B-DAT

- B-DAT
5 O
model O
and O
the O
equations O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pixel O
convolution O
layer O
as O
well O

- B-DAT

-1 B-DAT

-5 B-DAT
model O
[6]. O
Here, O
we O
follow O

- B-DAT

-5 B-DAT

-5 B-DAT
ImageNet O
model O
from O
[7] O
in O

- B-DAT
ilarity O
to O
designed O
features O
including O

- B-DAT

- B-DAT
geNet O
images. O
Results O
in O
Tab O

- B-DAT

- B-DAT

- B-DAT

-5 B-DAT

-5 B-DAT
ImageNet O
model O
in O
this O
section O

-5 B-DAT

-5 B-DAT
ImageNet O
model, O
whilst O
being O
close O

- B-DAT

- B-DAT
put O
image O
to O
HR O
space O

- B-DAT
resolved O
images O
is O
given O
in O

- B-DAT

- B-DAT

-5 B-DAT

-5 B-DAT
ImageNet O
model. O
The O
improvement O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cluding O
our O
own, O
a O
python/theano O

-5 B-DAT

-5 B-DAT
ImageNet O
model, O
the O
number O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
esting O
to O
explore O
ensemble O
prediction O

- B-DAT
resolution O
using O
videos O
from O
the O

-5 B-DAT

-5 B-DAT
ImageNet O
model O
takes O
0.435s O
per O

-5 B-DAT

-5 B-DAT
ImageNet O
model O
takes O
0.434s O
per O

- B-DAT
CNN O
9-5-5 O
ImageNet O
model O
[7 O

- B-DAT

-5 B-DAT

-5 B-DAT
ImageNet O
model O
[7] O
and O
ESPCN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
complexity. O
To O
address O
the O

- B-DAT

- B-DAT

-5 B-DAT

-5 B-DAT
ImageNet O
model O
[7] O
and O
ESPCN O

- B-DAT

- B-DAT

-3 B-DAT

-3 B-DAT
vs O
9-5-5). O
This O
makes O
our O

- B-DAT
implicit O
redundancy O
that O
can O
be O

- B-DAT
resolution O
as O
has O
been O
shown O

- B-DAT

- B-DAT
mation O
from O
videos O
for O
human O

- B-DAT

- B-DAT

- B-DAT

Sequences O
- B-DAT
A O
Review. O
Midwest O
Symposium O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
by O
adaptive O
sparse O
domain O

- B-DAT
ization. O
IEEE O
Transactions O
on O
Image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
invariant O
group-sparse O
regularization. O
In O
IEEE O

- B-DAT
ence O
on O
Computer O
Vision O
(ICCV O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
with O
deep O
convolutional O
neural O

- B-DAT
bard, O
and O
L. O
D. O
Jackel O

- B-DAT
propagation O
network. O
In O
Advances O
in O

- B-DAT
resolution O
with O
fast O
approximate O
convolutional O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ping O
of O
rural O
land O
cover O

- B-DAT

- B-DAT

- B-DAT
sion O
for O
fast O
example-based O
super-resolution O

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
(ACCV O

- B-DAT

- B-DAT

- B-DAT
sketch O
synthesis. O
In O
IEEE O
Conference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
networks. O
In O
Computer O
Vision–ECCV O

- B-DAT
national O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

provided O
in O
Section O
4.4.1 O
(the O
Set5 B-DAT
dataset O
with O
an O
upscaling O
factor O

extend O
the O
original O
experiments O
from O
Set5 B-DAT
[2] O
and O
Set14 O
[51] O
test O

32. O
We O
use O
the O
Set5 B-DAT
[2] O
as O
the O
validation O
set O

on O
ImageNet O
and O
test O
on O
Set5 B-DAT
with O
an O
upscaling O
factor O
3 O

is O
conducted O
on O
the O
Set5 B-DAT
dataset. O
n1 O
= O
128 O
n1 O

an O
upscaling O
factor O
3 O
on O
Set5 B-DAT
are O
32.57 O
dB, O
which O
is O

9-3- O
5 O
and O
9-5-5 O
on O
Set5 B-DAT
with O
8 O
× O
108 O
backpropagations O

bicubic O
kernel. O
Test O
set. O
The O
Set5 B-DAT
[2] O
(5 O
images), O
Set14 O
[51 O

of O
other O
methods O
on O
the O
Set5 B-DAT
dataset O

testing O
is O
conducted O
on O
the O
Set5 B-DAT
[2]. O
The O
network O
settings O
are O

result O
of O
DNC O
on O
the O
Set5 B-DAT
dataset O

dB) O
and O
MSSIM O
on O
the O
Set5 B-DAT
dataset O

strategies O
on O
the O
Set5 B-DAT
dataset O

14. O
The O
“butterfly” O
image O
from O
Set5 B-DAT
with O
an O
upscaling O
factor O
3 O

provided O
in O
Section O
4.4.1 O
(the O
Set5 B-DAT

on O
ImageNet O
and O
test O
on O
Set5 B-DAT

is O
conducted O
on O
the O
Set5 B-DAT

an O
upscaling O
factor O
3 O
on O
Set5 B-DAT

9-3- O
5 O
and O
9-5-5 O
on O
Set5 B-DAT

of O
other O
methods O
on O
the O
Set5 B-DAT

result O
of O
DNC O
on O
the O
Set5 B-DAT

dB) O
and O
MSSIM O
on O
the O
Set5 B-DAT

strategies O
on O
the O
Set5 B-DAT

14. O
The O
“butterfly” O
image O
from O
Set5 B-DAT

the O
Set5 O
dataset O
with O
an O
upscaling B-DAT
factor O
3). O
The O
proposed O
method O

kernel, O
sub-sample O
it O
by O
the O
upscaling B-DAT
factor, O
and O
upscale O
it O
by O

larger O
Set14 O
set O
[51]. O
The O
upscaling B-DAT
factor O
is O
3. O
We O
use O

on O
the O
ImageNet O
by O
an O
upscaling B-DAT
factor O
3. O
Please O
refer O
to O

our O
published O
implementation O
for O
upscaling B-DAT
factors O
2 O
and O
4. O
Interestingly O

trained O
on O
ImageNet O
with O
an O
upscaling B-DAT
factor O
3. O
The O
filters O
are O

test O
on O
Set5 O
with O
an O
upscaling B-DAT
factor O
3. O
The O
results O
observed O

4.1. O
The O
results O
with O
an O
upscaling B-DAT
factor O
3 O
on O
Set5 O
are O

on O
the O
ImageNet. O
For O
each O
upscaling B-DAT
factor O
∈ O
{2, O
3, O
4 O

to O
evaluate O
the O
performance O
of O
upscaling B-DAT
factors O
2, O
3, O
and O
4 O

108 O
backpropagations. O
Specifically, O
for O
the O
upscaling B-DAT
factor O
3, O
the O
average O
gains O

of O
different O
approaches O
by O
an O
upscaling B-DAT
factor O
3. O
As O
can O
be O

for O
fair O
quantitative O
comparison. O
The O
upscaling B-DAT
factor O
is O
3 O
and O
the O

only O
evaluate O
the O
performance O
of O
upscaling B-DAT
factor O
3. O
Comparisons. O
We O
compare O

network O
to O
cope O
with O
different O
upscaling B-DAT
factors O

Fattal, O
R.: O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

H.: O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In: O
IEEE O

image O
from O
Set5 O
with O
an O
upscaling B-DAT
factor O
3 O

image O
from O
Set14 O
with O
an O
upscaling B-DAT
factor O
3 O

image O
from O
Set14 O
with O
an O
upscaling B-DAT
factor O
3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
offs O
between O
performance O
and O
speed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
image, O
is O
a O
classical O

- B-DAT

- B-DAT
tiplicity O
of O
solutions O
exist O
for O

- B-DAT

- B-DAT
verse O
problem, O
of O
which O
solution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
exemplar O
pairs O
[2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
structing O
high-resolution O
patches. O
The O
overlapping O

- B-DAT

- B-DAT

- B-DAT
work O
[27] O
(more O
details O
in O

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
images. O
Our O
method O

- B-DAT
tally O
from O
existing O
external O
example-based O

- B-DAT
processing O

- B-DAT

- B-DAT
volutional O
Neural O
Network O
(SRCNN)1. O
The O

- B-DAT
ture O
is O
intentionally O
designed O
with O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
example-based O
methods. O
Furthermore, O
the O

- B-DAT

- B-DAT
work O
for O
image O
super-resolution. O
The O

- B-DAT
rectly O
learns O
an O
end-to-end O
mapping O

- B-DAT
and O
high-resolution O
images, O
with O
little O

- B-DAT
processing O
beyond O
the O
optimization O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution, O
and O
can O
achieve O
good O

- B-DAT

- B-DAT
linear O
mapping O
layers. O
Secondly, O
we O

- B-DAT
strate O
that O
performance O
can O
be O

- B-DAT

- B-DAT
ber O
of O
recently O
published O
methods O

- B-DAT

- B-DAT

- B-DAT
olution O
algorithms O
can O
be O
categorized O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
similarity O
property O
and O
generate O
exemplar O

- B-DAT
nal O
example-based O
methods O
[2], O
[4 O

- B-DAT
resolution O
patches O
from O
external O
datasets O

- B-DAT

- B-DAT
tionaries O
are O
directly O
presented O
as O

- B-DAT

- B-DAT

- B-DAT
sponding O
high-resolution O
patch O
used O
for O

- B-DAT
nique O
as O
an O
alternative O
to O

- B-DAT
borhood O
regression O
[41], O
[42] O
are O

- B-DAT
coding-based O
method O
and O
its O
several O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tioned O
methods O
first O
transform O
the O

- B-DAT
ferent O
color O
space O
(YCbCr O
or O

- B-DAT

- B-DAT
fully O
applied O
to O
other O
computer O

- B-DAT

- B-DAT
ceptron O
(MLP), O
whose O
all O
layers O

- B-DAT

- B-DAT

- B-DAT
work O
is O
applied O
for O
natural O

- B-DAT
moving O
noisy O
patterns O
(dirt/rain) O
[12 O

- B-DAT

- B-DAT

- B-DAT
resolution O
pipeline O
under O
the O
notion O

- B-DAT
based O
approach O
[16]. O
The O
deep O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
end O
mapping. O
Further, O
the O
SRCNN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
extracts O
(overlapping) O
patches O
from O

- B-DAT
resolution O
image O
Y O
and O
represents O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
neural O
network. O
An O
overview O

- B-DAT

- B-DAT
spectively, O
and O
’∗’ O
denotes O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tors O
is O
conceptually O
a O
representation O

- B-DAT

- B-DAT

- B-DAT
plexity O
of O
the O
model O
(n2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
patch). O
Motivated O
by O
this O

- B-DAT
lutional O
layer O
to O
produce O
the O

- B-DAT

- B-DAT

- B-DAT
tion O
and O
representation) O
becomes O
purely O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution) O
dictionary. O
If O
the O
dictionary O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
based O
SR O
method O
can O
be O

- B-DAT
volutional O
neural O
network O
(with O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
parameters. O
For O
example, O
we O
can O

- B-DAT
resolution O
patch O
(to O
the O
extreme O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
quires O
the O
estimation O
of O
network O

- B-DAT
imizing O
the O
loss O
between O
the O

- B-DAT
resolution O
images O
X. O
Given O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
crafted” O
methods. O
Despite O
that O
the O

- B-DAT
tory O
performance O
when O
the O
model O

- B-DAT
scent O
with O
the O
standard O
backpropagation O

- B-DAT
ular, O
the O
weight O
matrices O
are O

- B-DAT
erations, O
η O
is O
the O
learning O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ping O
and O
require O
some O
averaging O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
layers O
have O
no O
padding O

- B-DAT

- B-DAT
age O
[26]. O
We O
have O
also O

- B-DAT
tions O
between O
super-resolution O
performance O
and O

- B-DAT
quently, O
we O
compare O
our O
method O

- B-DAT

- B-DAT
the-arts O
both O
quantitatively O
and O
qualitatively O

- B-DAT

- B-DAT
4.4, O
so O
c O
= O
1 O

- B-DAT
tion O
training O
partition. O
The O
size O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nal O
images O
with O
a O
stride O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
geNet O
is O
about O
the O
same O

- B-DAT

- B-DAT
formance O
may O
be O
further O
boosted O

- B-DAT

- B-DAT
tured O
sufficient O
variability O
of O
natural O

- B-DAT

- B-DAT

Laplacian/Gaussian O
filters, O
the O
filters O
a O
- B-DAT
e O
are O
like O
edge O
detectors O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
crease O
the O
network O
width6, O
i.e O

- B-DAT
coding-based O
method O
(31.42 O
dB O

-1 B-DAT

-5 B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-7 B-DAT

- B-DAT
ing O
[17]. O
The O
term O
‘width O

-3 B-DAT

-5 B-DAT

-5 B-DAT

-5 B-DAT

-3 B-DAT

- B-DAT
5 O
and O
9-5-5 O
on O
Set5 O

-1 B-DAT

-5, B-DAT
9-3-5, O
and O
9-5-5 O
is O
8,032 O

-5 B-DAT

-5 B-DAT
is O
almost O
twice O
of O
9-3-5 O

- B-DAT

- B-DAT

-1 B-DAT

-1 B-DAT

-5, B-DAT
9-3-1-5, O
9-5-1-5, O
which O
add O
an O

-1 B-DAT

-5, B-DAT
9-3-5, O
and O
9-5-5, O
respectively. O
The O

- B-DAT
ditional O
layer O
are O
the O
same O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lution O
is O
found O
not O
as O

-1 B-DAT

-5 B-DAT
network, O
then O
the O
performance O
degrades O

- B-DAT

- B-DAT
ure O
9(a)). O
If O
we O
go O

- B-DAT

-1 B-DAT

-5 B-DAT
vs. O
9-1-1-5 O

-3 B-DAT

-5 B-DAT
vs. O
9-3-1-5 O

-5 B-DAT

-5 B-DAT
vs. O
9-5-1-5 O

- B-DAT

- B-DAT

-1 B-DAT

-5, B-DAT
then O
we O
have O
to O
set O

-3 B-DAT

- B-DAT
3-5 O
and O
9-3-3-3. O
However, O
from O

-3 B-DAT

-1 B-DAT

-5 B-DAT
network O

- B-DAT

- B-DAT

- B-DAT
vestigations O
to O
better O
understand O
gradients O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-1 B-DAT

-5 B-DAT
(n22 O
= O
32) O
and O
9-1-1-1-5 O

-3 B-DAT

-3 B-DAT

-5 B-DAT
and O
9-3-3-3 O

- B-DAT
speed O
trade-off: O
a O
three-layer O
network O

- B-DAT
of-the-art O
SR O
methods O

SC O
- B-DAT
sparse O
coding-based O
method O
of O
Yang O

et O
al. O
[50] O
• O
NE+LLE O
- B-DAT
neighbour O
embedding O
+ O
locally O
linear O

embedding O
method O
[4] O
• O
ANR O
- B-DAT
Anchored O
Neighbourhood O
Regression O

method O
[41] O
• O
A+ O
- B-DAT
Adjusted O
Anchored O
Neighbourhood O
Regres O

method O
[42], O
and O
• O
KK O
- B-DAT
the O
method O
described O
in O
[25 O

- B-DAT
based O
methods, O
according O
to O
the O

- B-DAT
sampled O
using O
the O
same O
bicubic O

- B-DAT
terion O
(IFC) O
[38], O
noise O
quality O

- B-DAT

- B-DAT

- B-DAT
scale O
structure O
similarity O
index O
(MSSSIM O

ANR O
- B-DAT
31.92 O
dB O

A+ O
- B-DAT
32.59 O
dB O

SC O
- B-DAT
31.42 O
dB O

Bicubic O
- B-DAT
30.39 O
dB O

NE+LLE O
- B-DAT
31.84 O
dB O

KK O
- B-DAT
32.28 O
dB O

- B-DAT
CNN O
outperforms O
existing O
state-of-the-art O
methods O

- B-DAT

- B-DAT

- B-DAT
cific O
network O
(9-5-5) O
using O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentation, O
whereas O
ours O
are O
in O

- B-DAT

-1 B-DAT

-5, B-DAT
9-3-5, O
and O
9-5-5. O
It O
is O

- B-DAT
1-5 O
network O
is O
the O
fastest O

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-5 B-DAT
network. O
Note O
the O
speed O
gap O

- B-DAT
pletely O
feed-forward. O
The O
9-5-5 O
network O

- B-DAT

- B-DAT

- B-DAT
terpolation. O
It O
is O
interesting O
to O

- B-DAT

- B-DAT
out O
altering O
the O
learning O
mechanism O

- B-DAT
sign. O
In O
particular, O
it O
can O

- B-DAT
nels O
simultaneously O
by O
setting O
the O

- B-DAT

- B-DAT

-5 B-DAT

- B-DAT

A+ O
[41] O
SRCNN O
2 O
33.66 O
- B-DAT
35.77 O
36.20 O
35.83 O
36.54 O
36.66 O

31.92 O
32.59 O
32.75 O
4 O
28.42 O
- B-DAT
29.61 O
30.03 O
29.69 O
30.28 O
30.49 O

2 O
0.9299 O
- B-DAT
0.9490 O
0.9511 O
0.9499 O
0.9544 O
0.9542 O

0.8968 O
0.9088 O
0.9090 O
4 O
0.8104 O
- B-DAT
0.8402 O
0.8541 O
0.8419 O
0.8603 O
0.8628 O

2 O
6.10 O
- B-DAT
7.84 O
6.87 O
8.09 O
8.48 O
8.05 O

4.52 O
4.84 O
4.58 O
4 O
2.35 O
- B-DAT
2.94 O
2.81 O
3.02 O
3.26 O
3.01 O

2 O
36.73 O
- B-DAT
42.90 O
39.49 O
43.28 O
44.58 O
41.13 O

33.10 O
34.48 O
33.21 O
4 O
21.42 O
- B-DAT
25.56 O
24.99 O
25.72 O
26.97 O
25.96 O

2 O
50.06 O
- B-DAT
58.45 O
57.15 O
58.61 O
60.06 O
59.49 O

46.02 O
47.17 O
47.10 O
4 O
37.21 O
- B-DAT
39.85 O
40.40 O
40.01 O
41.03 O
41.13 O

2 O
0.9915 O
- B-DAT
0.9953 O
0.9953 O
0.9954 O
0.9960 O
0.9959 O

0.9844 O
0.9867 O
0.9866 O
4 O
0.9516 O
- B-DAT
0.9666 O
0.9695 O
0.9672 O
0.9720 O
0.9725 O

A+ O
[41] O
SRCNN O
2 O
30.23 O
- B-DAT
31.76 O
32.11 O
31.80 O
32.28 O
32.45 O

28.65 O
29.13 O
29.30 O
4 O
26.00 O
- B-DAT
26.81 O
27.14 O
26.85 O
27.32 O
27.50 O

2 O
0.8687 O
- B-DAT
0.8993 O
0.9026 O
0.9004 O
0.9056 O
0.9067 O

0.8093 O
0.8188 O
0.8215 O
4 O
0.7019 O
- B-DAT
0.7331 O
0.7419 O
0.7352 O
0.7491 O
0.7513 O

2 O
6.09 O
- B-DAT
7.59 O
6.83 O
7.81 O
8.11 O
7.76 O

4.23 O
4.45 O
4.26 O
4 O
2.23 O
- B-DAT
2.71 O
2.57 O
2.78 O
2.94 O
2.74 O

2 O
40.98 O
- B-DAT
41.34 O
38.86 O
41.79 O
42.61 O
38.95 O

37.22 O
38.24 O
35.25 O
4 O
26.15 O
- B-DAT
31.17 O
29.18 O
31.27 O
32.31 O
30.46 O

2 O
47.64 O
- B-DAT
54.47 O
53.85 O
54.57 O
55.62 O
55.39 O

43.36 O
44.25 O
44.32 O
4 O
35.71 O
- B-DAT
37.75 O
38.26 O
37.85 O
38.72 O
38.87 O

2 O
0.9813 O
- B-DAT
0.9886 O
0.9890 O
0.9888 O
0.9896 O
0.9897 O

0.9647 O
0.9669 O
0.9675 O
4 O
0.9134 O
- B-DAT
0.9317 O
0.9338 O
0.9326 O
0.9371 O
0.9376 O

A+ O
[41] O
SRCNN O
2 O
28.38 O
- B-DAT
29.67 O
30.02 O
29.72 O
30.14 O
30.29 O

26.72 O
27.05 O
27.18 O
4 O
24.65 O
- B-DAT
25.21 O
25.38 O
25.25 O
25.51 O
25.60 O

2 O
0.8524 O
- B-DAT
0.8886 O
0.8935 O
0.8900 O
0.8966 O
0.8977 O

0.7843 O
0.7945 O
0.7971 O
4 O
0.6727 O
- B-DAT
0.7037 O
0.7093 O
0.7060 O
0.7171 O
0.7184 O

2 O
5.30 O
- B-DAT
7.10 O
6.33 O
7.28 O
7.51 O
7.21 O

3.91 O
4.07 O
3.91 O
4 O
1.95 O
- B-DAT
2.45 O
2.24 O
2.51 O
2.62 O
2.45 O

2 O
36.84 O
- B-DAT
41.52 O
38.54 O
41.72 O
42.37 O
39.66 O

34.81 O
35.58 O
34.72 O
4 O
21.72 O
- B-DAT
25.15 O
24.87 O
25.27 O
26.01 O
25.65 O

2 O
46.15 O
- B-DAT
52.56 O
52.21 O
52.69 O
53.56 O
53.58 O

41.53 O
42.19 O
42.29 O
4 O
34.86 O
- B-DAT
36.52 O
36.80 O
36.64 O
37.18 O
37.24 O

2 O
0.9780 O
- B-DAT
0.9869 O
0.9876 O
0.9872 O
0.9883 O
0.9883 O

0.9581 O
0.9609 O
0.9614 O
4 O
0.9005 O
- B-DAT
0.9203 O
0.9215 O
0.9214 O
0.9256 O
0.9261 O

-1 B-DAT

-5 B-DAT

-3 B-DAT

-5 B-DAT

-5 B-DAT

-5 B-DAT

- B-DAT
of-the-art O
super-resolution O
quality, O
whilst O
maintains O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-art O
color O
SR O
method O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nel O
when O
training O
is O
performed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
correlation O
among O
each O
other. O
The O

- B-DAT

- B-DAT
channel O
network O
(“Y O
only”). O
It O

- B-DAT

- B-DAT
work O
is O
not O
that O
significant O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
images, O
with O
little O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vantages O
of O
simplicity O
and O
robustness O

- B-DAT

- B-DAT
blurring O
or O
simultaneous O
SR+denoising. O
One O

- B-DAT

- B-DAT
complexity O
single-image O
super-resolution O
based O
on O

- B-DAT

- B-DAT
bor O
embedding. O
In: O
IEEE O
Conference O

- B-DAT

- B-DAT

- B-DAT
resolution. O
IEEE O
Transactions O
on O
Image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
linear O
structure O
within O
convolutional O

- B-DAT
tems O
(2014 O

- B-DAT
tional O
network O
for O
image O
super-resolution O

- B-DAT
ence O
on O
Computer O
Vision, O
pp O

- B-DAT
tional O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT

- B-DAT
resolution. O
Computer O
Graphics O
and O
Applications O

- B-DAT
level O
vision. O
International O
Journal O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
lutional O
neural O
networks O
with O
low O

- B-DAT
tems. O
pp. O
769–776 O
(2008 O

- B-DAT

- B-DAT

- B-DAT
ten O
zip O
code O
recognition. O
Neural O

- B-DAT

- B-DAT
rithms. O
In: O
Advances O
in O
Neural O

- B-DAT
mentation O
algorithms O
and O
measuring O
ecological O

- B-DAT

- B-DAT

- B-DAT
tion. O
In: O
IEEE O
International O
Conference O

- B-DAT
chine O
learning O
approach O
for O
non-blind O

- B-DAT

- B-DAT
tics. O
IEEE O
Transactions O
on O
Image O

- B-DAT
tation O
by O
joint O
identification-verification. O
In O

- B-DAT
quality O
object O
detection. O
arXiv O
preprint O

- B-DAT

- B-DAT

- B-DAT
ternational O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
ilarity O
for O
image O
quality O
assessment O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tionary O
training O
for O
image O
super-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
sparse-representations. O
In: O
Curves O
and O

- B-DAT

- B-DAT
CNNs O
for O
fine-grained O
category O
detection O

- B-DAT
ence O
on O
Computer O
Vision. O
pp O

- B-DAT
tion O
Engineering O
from O
Beijing O
Institute O

- B-DAT
nology, O
China, O
in O
2011. O
He O

- B-DAT
sity O
of O
Hong O
Kong. O
His O

- B-DAT

- B-DAT
versity O
of O
London O
in O
2010 O

- B-DAT
toral O
researcher O
at O
Vision O
Semantics O

- B-DAT
inghua O
University O
in O
2007, O
and O

- B-DAT
ence O
on O
Computer O
Vision O
and O

- B-DAT
tion O
(CVPR) O
2009. O
He O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sachusetts O
Institute O
of O
Technology, O
Cambridge O

- B-DAT
cessing. O
He O
received O
the O
Best O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Set5 B-DAT

on O
some O
images O
from O
the O
Set5 B-DAT
dataset, O
with O
magnification O
factor O

Set5 B-DAT

magnification O
factors O
(×3) O
on O
dataset O
Set5 B-DAT

on O
some O
images O
from O
the O
Set5 B-DAT

Bischof, O
"Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests," O
in O
Proceedings O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
second- O
order O
gradients O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
second-order O
gradients O
and O
their O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
patches, O
with O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Although O
the O
𝑙0 O
- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-2 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

orientation, O
with O
a O
value O
between O
-90 B-DAT

-1 B-DAT

-2 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
show O
that O
the O
proposed O
GWRR O

-1 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT
and O
Table-3 O
validate O
that O
using O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT
and O
Table O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-3 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
FARF O
FARF* O
SRCNN O

-2 B-DAT

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT
summarizes O
the O
performances O
of O
our O

-3 B-DAT
gives O
more O
details O
of O
the O

- B-DAT
FARF O

-3 B-DAT

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT
also O
shows O
that O
the O
fine-tuned O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-2014 B-DAT

-2015 B-DAT

-2016 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-918 B-DAT

- B-DAT

- B-DAT

-1927 B-DAT

-1588, B-DAT
1997 O

- B-DAT

- B-DAT

-730 B-DAT

- B-DAT

-126 B-DAT

-10 B-DAT

-3311 B-DAT

- B-DAT

-3799 B-DAT

- B-DAT

-26, B-DAT
2017 O

-167, B-DAT
1998 O

- B-DAT

- B-DAT

-297, B-DAT
1995 O

-156 B-DAT

-32, B-DAT
2001 O

-1874 B-DAT

- B-DAT

- B-DAT

- B-DAT

-333 B-DAT

-423 B-DAT

-1692 B-DAT

- B-DAT

-192, B-DAT
2015 O

-515 B-DAT

-424 B-DAT

-156 B-DAT

-1232, B-DAT
2001 O

- B-DAT

-157 B-DAT

- B-DAT

-2873, B-DAT
2010 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-568 B-DAT

- B-DAT
resolution," O
IEEE O
Transactions O
on O
Image O

-861, B-DAT
2015 O

- B-DAT

-4 B-DAT

- B-DAT

-26, B-DAT
2017 O

- B-DAT

-595 B-DAT

- B-DAT
Resolution," O
IEEE O
Transactions O
on O
Multimedia O

-417, B-DAT
2016 O

- B-DAT

-199 B-DAT

- B-DAT

-307, B-DAT
2016 O

- B-DAT

-1654 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
1527, O
2001 O

- B-DAT

- B-DAT

-896, B-DAT
2008 O

- B-DAT

-2238, B-DAT
2006 O

- B-DAT

-013011 B-DAT

-20, B-DAT
2010 O

- B-DAT

- B-DAT

-2072 B-DAT

- B-DAT

- B-DAT

-2357 B-DAT

- B-DAT

- B-DAT

- B-DAT

-180 B-DAT

-2774, B-DAT
2013 O

-1728, B-DAT
2012 O

- B-DAT

- B-DAT

- B-DAT
Based O
Super-Resolution", O
IEEE O
Transactions O
on O

- B-DAT

- B-DAT

- B-DAT
2929, O
2013 O

-352, B-DAT
November O
2009 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Set5 B-DAT

Set5 B-DAT

Set5 B-DAT

Set5 B-DAT

state-of-the-art O
methods O
on O
the O
dataset O
Set5 B-DAT

factors. O
For O
the O
Set5 B-DAT
and O
Set14 O
datasets, O
with O
different O

of O
the O
performances O
in O
datasets O
Set5 B-DAT

7: O
Super-resolved O
(×3) O
images O
from O
Set5 B-DAT

factors. O
For O
the O
Set5 B-DAT

Bischof, O
"Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests," O
in O
Proceedings O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

- B-DAT

-2 B-DAT

-3 B-DAT

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT
and O
Table O

- B-DAT

- B-DAT

-2 B-DAT
and O
Table-3 O

-1 B-DAT
3 O
2.449±0.029 O
2.236±0.015 O
2.206±0.027 O
(10 O

-3 B-DAT
3 O
6.264±0.042 O
5.952±0.323 O
4.622±0.299 O
(26 O

-2 B-DAT
3 O
6.501±0.199 O
6.308±0.330 O
6.272±0.332 O
(04 O

-2 B-DAT
3 O
6.889±0.199 O
5.196±0.127 O
4.864±0.267 O
(29 O

-2 B-DAT
3 O
3.418±0.171 O
3.377±0.164 O
2.969±0.120 O
(13 O

-1 B-DAT
3 O
1.026±0.158 O
0.391±0.007 O
0.293±0.004 O
(71 O

-2 B-DAT
3 O
6.527±0.203 O
6.520±0.188 O
6.285±0.101 O
(04 O

-2 B-DAT

- B-DAT

-4 B-DAT

-3 B-DAT

-2 B-DAT

-1 B-DAT

-1 B-DAT

-3 B-DAT

- B-DAT

-2, B-DAT
the O
number O
hyperplane(s) O
#ℋ O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
JMPF O
JMPF O

-4 B-DAT

- B-DAT

- B-DAT

- B-DAT

-4 B-DAT
and O
Tables-5, O
where O
JMPF O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-100 B-DAT
dataset O
[22] O
are O
used, O
so O

- B-DAT
JMPF O
JMPF O

- B-DAT
JMPF O
JMPF O

- B-DAT
JMPF O
JMPF O

-5 B-DAT

- B-DAT

- B-DAT

- B-DAT

-4 B-DAT
tabulates O
the O
performances, O
in O
terms O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-4 B-DAT

-5 B-DAT
provides O
more O
details O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
2929, O
2013 O

- B-DAT

- B-DAT

-1927 B-DAT

-1588, B-DAT
1997 O

- B-DAT

- B-DAT

-730 B-DAT

- B-DAT

-126 B-DAT

-10 B-DAT

-3311 B-DAT

- B-DAT

-3799 B-DAT

- B-DAT

-918 B-DAT

-167, B-DAT
1998 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-023008, B-DAT
2017 O

-156 B-DAT

-32, B-DAT
2001 O

-1874 B-DAT

- B-DAT

- B-DAT

-1873 B-DAT

- B-DAT

- B-DAT

-333 B-DAT

-423 B-DAT

-1692 B-DAT

- B-DAT

-26, B-DAT
2017 O

-515 B-DAT

-424 B-DAT

-156 B-DAT

-1232, B-DAT
2001 O

- B-DAT

-157 B-DAT

- B-DAT

-2873, B-DAT
2010 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-568 B-DAT

- B-DAT
resolution," O
IEEE O
Transactions O
on O
Image O

-861, B-DAT
2015 O

- B-DAT

-4 B-DAT

- B-DAT

-192, B-DAT
2015 O

- B-DAT

-595 B-DAT

- B-DAT
Resolution," O
IEEE O
Transactions O
on O
Multimedia O

-417, B-DAT
2016 O

- B-DAT

-199 B-DAT

- B-DAT

-407 B-DAT

- B-DAT

-1654 B-DAT

- B-DAT

- B-DAT

- B-DAT

-3245, B-DAT
2015 O

- B-DAT

- B-DAT

-333 B-DAT

- B-DAT

- B-DAT

-104 B-DAT

- B-DAT

- B-DAT

-297, B-DAT
1995 O

- B-DAT

- B-DAT
1527, O
2001 O

- B-DAT

- B-DAT

-896, B-DAT
2008 O

- B-DAT

-2238, B-DAT
2006 O

- B-DAT

-013011 B-DAT

-20, B-DAT
2010 O

-1874, B-DAT
2008 O

-954 B-DAT

- B-DAT

-294, B-DAT
2014 O

on O
standard O
benchmarks O
such O
as O
Set5 B-DAT
[2], O
Set14 O
[50] O
and O
BSD100 O

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

- B-DAT

Dong2 O
Chen O
Change O
Loy1 O
1CUHK O
- B-DAT
SenseTime O
Joint O
Lab, O
The O
Chinese O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
per, O
we O
show O
that O
it O

- B-DAT
ful O
to O
semantic O
classes. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
the O
same O
loss O
function O

- B-DAT
put O
image O
of O
arbitrary O
size O

- B-DAT

- B-DAT
work O
equipped O
with O
SFT O
can O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions O
exist O
for O
any O
given O

- B-DAT

- B-DAT
ping O
functions O
from O
external O
low O

- B-DAT
and O
high-resolution O
ex- O
emplar O
pairs O

- B-DAT
ifold, O
new O
losses O
are O
proposed O

- B-DAT

- B-DAT

- B-DAT
resolution O
model O
in O
a O
feature O

- B-DAT
resolution O
images O
look O
very O
similar O

- B-DAT
tions O
the O
overall O
visual O
quality O

- B-DAT
cantly O
improved O

- B-DAT
ent O
class O
is O
non-trivial. O
The O

- B-DAT
ceptual O
and O
adversarial O
losses O
(without O

- B-DAT
ine O
closely, O
these O
details O
are O

- B-DAT
tion, O
existing O
methods O
struggle O
in O

- B-DAT
ing, O
plant), O
is O
crucial O
for O

- B-DAT
ical O
prior O
using O
the O
same O

- B-DAT
ing O
pairs O
using O
two O
different O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ously O
documented O
by O
Timofte O
et O

- B-DAT
ized O
models O
for O
each O
semantic O

- B-DAT

- B-DAT

- B-DAT
age O
super-resolution O
with O
CNN. O
This O

- B-DAT
ing O
especially O
when O
multiple O
segments O

- B-DAT

- B-DAT
ther O
incorporated O
into O
the O
reconstruction O

- B-DAT
tic O
segmentation O
maps O
as O
the O

- B-DAT
periments O
embrace O
this O
choice O
and O

- B-DAT

- B-DAT

- B-DAT
cient. O
Combining O
LR O
images O
with O

- B-DAT
puts, O
or O
concatenating O
segmentation O
maps O

- B-DAT
tial O
Feature O
Transform O
(SFT) O
that O

- B-DAT
ically, O
an O
SFT O
layer O
is O

- B-DAT
tion O
probability O
maps, O
based O
on O

- B-DAT
tially O
on O
feature O
maps O
of O

- B-DAT

- B-DAT

- B-DAT
struction O
of O
an O
HR O
image O

- B-DAT
forming O
the O
intermediate O
features O
of O

- B-DAT

- B-DAT

- B-DAT
tiveness O
of O
our O
approach, O
named O

- B-DAT

- B-DAT
vided O
in O
Sec. O
4 O

- B-DAT

- B-DAT
duced O
prior O
information O
to O
help O

- B-DAT

- B-DAT
formance. O
Dong O
et O
al. O
[10 O

- B-DAT
ies O
to O
better O
recover O
local O

- B-DAT
tion O
framework. O
Sun O
et O
al O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
[49, O
50, O
45, O
46 O

- B-DAT
stantiation O
of O
learning-based O
methods, O
Dong O

- B-DAT
pose O
SRCNN O
for O
learning O
the O

- B-DAT
ages O
in O
an O
end-to-end O
manner O

- B-DAT
mid O
structure O
[26], O
residual O
blocks O

- B-DAT
ing O
[23, O
43], O
and O
densely O

- B-DAT
scale O
guidance O
structure O
has O
also O

- B-DAT

- B-DAT

- B-DAT
age O
of O
many O
plausible O
solutions O

- B-DAT

- B-DAT
velop O
a O
similar O
approach O
and O

- B-DAT
ture O
matching O
loss, O
partly O
reducing O

- B-DAT
facts. O
We O
use O
the O
same O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
egorical O
classes O
to O
co-exist O
in O

- B-DAT
fective O
layer O
that O
enables O
an O

- B-DAT
malizing O
feature O
statistics O
[19]. O
Conditional O

- B-DAT
place O
parameters O
for O
feature-wise O
affine O

- B-DAT
ing O
[6] O
and O
visual O
reasoning O

- B-DAT

- B-DAT
mation O
(e.g., O
semantic O
segmentation O
maps O

- B-DAT
cepts O
a O
single O
linguistic O
input O

- B-DAT

- B-DAT
quire O
adaptive O
processing O
at O
different O

- B-DAT
posed O
SFT O
layer O
addresses O
this O

- B-DAT

- B-DAT

- B-DAT
ceptual O
factors O
in O
neural O
style O

- B-DAT

- B-DAT
tion O
instead O
of O
simple O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ilar O
as O
possible O
to O
the O

- B-DAT
based O
methods O
use O
feed-forward O
networks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
structed O
images. O
However, O
the O
generated O

- B-DAT
ing O
which O
region O
belongs O
to O

- B-DAT
eficial O
for O
generating O
richer O
and O

- B-DAT
mantic O
segmentation O
probability O
maps O
P O

- B-DAT
ors O
in O
SR, O
we O
reformulate O

- B-DAT
ping O
functionM O
that O
outputs O
a O

- B-DAT
rameter O
pair O
adaptively O
influences O
the O

- B-DAT
ture O
maps O
in O
an O
SR O

- B-DAT
tionM O
: O
Ψ O
7→ O
(γ,β O

- B-DAT
cific O
layer O

- B-DAT

- B-DAT
cation, O
i.e., O
Hadamard O
product. O
Since O

- B-DAT

- B-DAT

- B-DAT
ers O
in O
an O
SR O
network O

- B-DAT
timized O
end-to-end O
with O
the O
SR O

- B-DAT
ate O
conditions O
that O
can O
be O

- B-DAT

- B-DAT

- B-DAT
tation O
maps O
obtained O
from O
LR O

- B-DAT
ing O
factor O
of O
×4 O
from O

- B-DAT
tained O
even O
on O
LR O
images O

- B-DAT

- B-DAT
mentation O
model O
[32, O
31]. O
Some O

- B-DAT
responding O
segmentation O
results O
are O
depicted O

- B-DAT
tation O
community. O
During O
testing, O
classes O

- B-DAT

- B-DAT
GAN, O
i.e., O
treating O
all O
classes O

- B-DAT
egorical O
priors O
to O
an O
SR O

- B-DAT
put O
LR O
image O
as O
a O

- B-DAT
catenate O
the O
probability O
maps O
with O

- B-DAT

- B-DAT
sis O
network O
[29]1. O
This O
method O

- B-DAT

- B-DAT

- B-DAT

d O
- B-DAT

- B-DAT
ages. O
Second O
row: O
segmentation O
results O

- B-DAT

- B-DAT
mentation O

- B-DAT

- B-DAT
pose O
the O
LR O
image O
based O

- B-DAT
bining O
the O
output O
of O
each O

- B-DAT

- B-DAT
tively. O
They O
are O
jointly O
trained O

- B-DAT
plying O
affine O
transformation. O
Skip O
connection O

- B-DAT

- B-DAT
volution O
layer. O
The O
upsampling O
operation O

- B-DAT
porary O
models O
such O
as O
DRRN O

- B-DAT

- B-DAT
work O
of O
strided O
convolutions O
to O

- B-DAT
tial O
dimensions. O
The O
full O
architecture O

- B-DAT
vided O
in O
the O
supplementary O
material O

- B-DAT
ture O
maps, O
we O
use O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
courage O
the O
generator O
to O
favor O

- B-DAT
door O
scenes O
since O
their O
textures O

- B-DAT

- B-DAT
ground’ O
category O
is O
used O
to O

- B-DAT
tialized O
the O
SR O
network O
by O

- B-DAT

- B-DAT
ceptual O
loss O
and O
GAN O
loss O

- B-DAT
nel. O
The O
mini-batch O
size O
was O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceptual O
quality. O
Our O
proposed O
SFT-GAN O

- B-DAT
ticular, O
we O
collected O
a O
new O

- B-DAT
ages O
from O
search O
engines O
using O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els O
including O
PSNR-oriented O
methods, O
such O

- B-DAT

- B-DAT
hanceNet O
[38]. O
More O
results O
are O

- B-DAT
mentary O
material. O
For O
SRGAN, O
we O

- B-DAT

- B-DAT

- B-DAT
ing O
sharp O
edges, O
PSNR-oriented O
methods O

- B-DAT

- B-DAT
ate O
monotonous O
and O
unnatural O
textures O

-1 B-DAT
Rank-2 O
Rank-3 O
Rank-4 O

- B-DAT

- B-DAT
forms O
PSNR-oriented O
methods O
by O
a O

- B-DAT
ing O
images. O
To O
better O
compare O

- B-DAT
oriented O
baselines O
and O
GAN-based O
approaches O

- B-DAT

- B-DAT
quested O
to O
rank O
4 O
versions O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
domized O
fashion. O
In O
the O
second O

- B-DAT

- B-DAT

- B-DAT
cilitate O
the O
comparison). O
Each O
pair O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ages O
comparable O
to O
HR O
causing O

- B-DAT
lation O
parameters. O
Our O
method O
modulates O

- B-DAT
vestigate O
the O
relationship O
between O
the O

- B-DAT
ture O
modulation O
parameters, O
as O
depicted O

- B-DAT
tionship O
with O
probability O
maps O
P O

- B-DAT
age O
where O
building O
and O
grass O

- B-DAT

- B-DAT

- B-DAT
formation. O
From O
the O
heat O
map O

- B-DAT
ing O
and O
grass O
textures O
simultaneously O

- B-DAT
biguity, O
the O
probability O
maps O
are O

- B-DAT
formation. O
In O
Fig. O
9, O
the O

- B-DAT
tures O
generated O
by O
SFT-GAN O
become O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentation O
results O
are O
not O
available O

- B-DAT

- B-DAT
mentation O
probability O
maps, O
our O
model O

- B-DAT
GAN O
and O
produces O
comparative O
results O

- B-DAT
itatively O
compare O
with O
several O
alternatives O

- B-DAT
mentation O
probability O
maps O
with O
the O

- B-DAT
ditional O
bias O
at O
the O
input O

- B-DAT
rately O
using O
a O
specific O
model O

- B-DAT
GAN O
yields O
outputs O
that O
are O

- B-DAT
essary O
condition O
for O
class-specific O
texture O

- B-DAT
positional O
mapping O
produces O
good O
results O

- B-DAT
rameter O
efficient O
(×2.5 O
parameters O
as O

- B-DAT
putationally O
inefficient O
as O
we O
need O

- B-DAT
tions O
where O
multiple O
categorical O
classes O

- B-DAT

- B-DAT

Comparison O
with O
other O
conditioning O
methods O
- B-DAT
input O
concatenation, O
compositional O
mapping O
and O

- B-DAT
tic O
to O
spatial O
information. O
For O

- B-DAT
ture O
and O
thus O
noisy O
bricks O

- B-DAT

- B-DAT

- B-DAT
erating O
distinct O
and O
rich O
textures O

- B-DAT
gions O
in O
a O
super-resolved O
image O

- B-DAT

- B-DAT
ally O
pleasing O
textures, O
outperforming O
previous O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sider O
priors O
of O
finer O
categories O

- B-DAT
ward O
challenging O
requirements O
for O
segmentation O

- B-DAT
comings. O
Furthermore, O
segmentation O
and O
SR O

- B-DAT

- B-DAT
Morel. O
Low-complexity O
single-image O
super-resolution O
based O

- B-DAT

- B-DAT

- B-DAT
guage. O
arXiv O
preprint O
arXiv:1707.00683, O
2017 O

- B-DAT

- B-DAT
resolution O
using O
deep O
convolutional O
networks O

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
time O
with O
adaptive O
instance O
normalization O

- B-DAT
resolution O
by O
deep O
multi-scale O
guidance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mization. O
arXiv O
preprint O
arXiv:1412.6980, O
2014 O

- B-DAT
resolution. O
In O
CVPR, O
2017. O
1 O

- B-DAT

- B-DAT

- B-DAT
ative O
adversarial O
network. O
In O
CVPR O

- B-DAT

- B-DAT

- B-DAT
manan, O
P. O
Dollár, O
and O
C O

- B-DAT
mon O
objects O
in O
context. O
In O

- B-DAT
ing O
markov O
random O
field O
for O

- B-DAT
cal O
statistics. O
In O
ICCV, O
2001 O

- B-DAT
ors. O
arXiv O
preprint O
arXiv:1707.03017, O
2017 O

- B-DAT
ditioning O
layer. O
arXiv O
preprint O
arXiv:1709.07871 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cination O
for O
image O
super-resolution. O
In O

- B-DAT

- B-DAT
borhood O
regression O
for O
fast O
example-based O

- B-DAT

- B-DAT

- B-DAT
resolution: O
When O
and O
where O
is O

- B-DAT

- B-DAT
resolution O
as O
sparse O
representation O
of O

- B-DAT

- B-DAT

- B-DAT
ralba. O
Scene O
parsing O
through O
ADE20K O

widely O
used O
bench- O
mark O
datasets O
Set5 B-DAT
[3], O
Set14 O
[69] O
and O
BSD100 O

versions O
of O
each O
image O
on O
Set5, B-DAT
Set14 O
and O
BSD100: O
nearest O
neighbor O

and O
the O
adversarial O
networks O
on O
Set5 B-DAT
and O
Set14 O
benchmark O
data. O
MOS O

SRResNet- O
SRGAN- O
Set5 B-DAT
MSE O
VGG22 O
MSE O
VGG22 O
VGG54 O

respect O
to O
MOS O
score O
on O
Set5 B-DAT

Set5 B-DAT
nearest O
bicubic O
SRCNN O
SelfExSR O
DRCN O

the O
MOS O
tests O
conducted O
on O
Set5, B-DAT
Set14, O
BSD100 O
are O
summarized O
in O

a O
4× O
upscaling O
factor O
for O
Set5 B-DAT
(Section O
A.4), O
Set14 O
(Section O
A.5 O

downsampled O
versions O
of O
images O
from O
Set5, B-DAT
Set14 O
and O
BSD100. O
On O
BSD100 O

rated O
by O
each O
rater. O
On O
Set5 B-DAT
and O
Set14 O
the O
raters O
also O

ordinal O
ranking. O
While O
results O
on O
Set5 B-DAT
are O
somewhat O
inconclusive O
due O
to O

Set5 B-DAT
Set14 O
BSD100 O

distribution O
of O
MOS O
scores O
on O
Set5, B-DAT
Set14, O
BSD100. O
Mean O
shown O
as O

Set5 B-DAT
Set14 O
BSD100 O

Figure O
10: O
Average O
rank O
on O
Set5, B-DAT
Set14, O
BSD100 O
by O
averaging O
the O

A.4. O
Set5 B-DAT
- O
Visual O
Results O

Figure O
11: O
Results O
for O
Set5 B-DAT
using O
bicubic O
interpolation, O
SRResNet O
and O

and O
the O
adversarial O
networks O
on O
Set5 B-DAT

SRResNet- O
SRGAN- O
Set5 B-DAT

Set5 B-DAT

rated O
by O
each O
rater. O
On O
Set5 B-DAT

ordinal O
ranking. O
While O
results O
on O
Set5 B-DAT

Set5 B-DAT

Set5 B-DAT

Figure O
11: O
Results O
for O
Set5 B-DAT

when O
we O
super-resolve O
at O
large O
upscaling B-DAT
factors? O
The O
behavior O
of O
optimization-based O

photo-realistic O
natural O
images O
for O
4× O
upscaling B-DAT
factors. O
To O
achieve O
this, O
we O

guishable O
from O
original O
(right). O
[4× O
upscaling B-DAT

is O
particularly O
pronounced O
for O
high O
upscaling B-DAT
factors, O
for O
which O
texture O
detail O

are O
shown O
in O
brackets. O
[4× O
upscaling B-DAT

super- O
resolved O
with O
a O
4× O
upscaling B-DAT
factor O
is O
shown O
in O
Figure O

the O
network O
to O
learn O
the O
upscaling B-DAT
filters O
directly O
can O
further O
increase O

was O
also O
shown O
that O
learning O
upscaling B-DAT
filters O
is O
beneficial O
in O
terms O

super-resolves O
face O
images O
with O
large O
upscaling B-DAT
factors O
(8×). O
GANs O
were O
also O

for O
image O
SR O
with O
high O
upscaling B-DAT
factors O
(4×) O
as O
measured O
by O

photo-realistic O
SR O
images O
with O
high O
upscaling B-DAT
factors O
(4 O

losses O
in O
that O
category∗. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

HR O
image O
(right: O
i,j). O
[4× O
upscaling B-DAT

SSIM, O
MOS) O
in O
bold. O
[4× O
upscaling B-DAT

that O
SRGAN O
reconstructions O
for O
large O
upscaling B-DAT
factors O
(4×) O
are, O
by O
a O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
IEEE O

and O
SRGAN O
with O
a O
4× O
upscaling B-DAT
factor O
for O
Set5 O
(Section O
A.4 O

low-/high-resolution O
images O
and O
reconstructions O
(4× O
upscaling) B-DAT
obtained O
with O
different O
methods O
(bicubic O

image O
with O
resolution O
64×64 O
with O
upscaling B-DAT
factor O
4×. O
The O
measurements O
are O

for O
another O
100k O
iterations. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

all O
available O
individual O
ratings. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks, O
one O
central O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
(SR). O
To O
our O
knowledge O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
guishable O
from O
original O
(right). O
[4 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
inal O
image O
means O
that O
the O

- B-DAT
realistic O
as O
defined O
by O
Ferwerda O

- B-DAT

- B-DAT

- B-DAT
ing O
high-level O
feature O
maps O
of O

- B-DAT

- B-DAT
resolved O
with O
a O
4× O
upscaling O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
image O
informa- O
tion O

- B-DAT

- B-DAT
proaches O
to O
the O
SR O
problem O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
[27], O
trees O
[46] O
or O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-the-art O
SR O
performance. O
Subsequently, O
it O

- B-DAT

- B-DAT

- B-DAT
ciently O
train O
these O
deeper O
network O

- B-DAT
normalization O
[32] O
is O
often O
used O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
results. O
Another O
powerful O
design O

- B-DAT

- B-DAT
connections O
relieve O
the O
network O
architecture O

- B-DAT
tentially O
non-trivial O
to O
represent O
with O

- B-DAT

- B-DAT

- B-DAT
ing O
pixel-wise O
averages O
of O
plausible O

- B-DAT

- B-DAT
ity O
[42, O
33, O
13, O
5 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
perceptually O
more O
convincing O
solutions O

- B-DAT
ure O
2. O
We O
illustrate O
the O

- B-DAT
ure O
3 O
where O
multiple O
potential O

- B-DAT
tion. O
Yu O
and O
Porikli O
[66 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions. O
Similar O
to O
this O
work O

- B-DAT
trained O
VGG O
network O
instead O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity. O
The O
GAN O
procedure O
encourages O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mark O
datasets O
as O
well O
as O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
counterpart O
IHR. O
The O
high-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nels O
as O
in O
the O
VGG O

- B-DAT
ical O
for O
the O
performance O
of O

- B-DAT
tent O
loss O
lSRX O
and O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
solutions O
with O
overly O
smooth O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stead O
of O
log[1−DθD O
(GθG(ILR))] O
[22 O

- B-DAT
mark O
datasets O
Set5 O
[3], O
Set14 O

- B-DAT
and O
high-resolution O
images. O
This O
corresponds O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bor, O
bicubic, O
SRCNN O
[9] O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gral O
score O
from O
1 O
(bad O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG54 O
and O
the O
original O
HR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ResNet O
and O
the O
adversarial O
networks O

- B-DAT
SRGAN- O
Set5 O
MSE O
VGG22 O
MSE O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
MSE-based O
reconstructions, O
to O
those O
competing O

- B-DAT

- B-DAT
formed O
other O
SRGAN O
and O
SRResNet O

- B-DAT

- B-DAT
GAN O
to O
NN, O
bicubic O
interpolation O

- B-DAT

- B-DAT

- B-DAT
art O
methods. O
Quantitative O
results O
are O

- B-DAT
resolved O
with O
SRResNet O
and O
SRGAN O

- B-DAT
realistic O
image O
SR. O
All O
differences O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentary O
material). O
We O
further O
found O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Net O
that O
sets O
a O
new O

- B-DAT
sure. O
We O
have O
highlighted O
some O

- B-DAT

- B-DAT
ial O
loss O
by O
training O
a O

- B-DAT

- B-DAT

- B-DAT
the-art O
reference O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Stevenson. O
Super-Resolution B-DAT
from O
Image O
Sequences O
- O
A O
Review. O
Midwest O
Symposium O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
by O
adaptive O
sparse O
domain O

- B-DAT
ization. O
IEEE O
Transactions O
on O
Image O

- B-DAT

- B-DAT
resolution. O
IEEE O
Computer O
Graphics O
and O

- B-DAT
level O
vision. O
International O
Journal O
of O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

- B-DAT
line O
at O
http://torch.ch/blog/2016/02/04/resnets. O
html. O
2016 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In O
European O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT
tion O
with O
deep O
convolutional O
neural O

- B-DAT

- B-DAT

- B-DAT
mentation O
algorithms O
and O
measuring O
ecological O

- B-DAT

- B-DAT

- B-DAT
sive O
survey. O
In O
Machine O
Vision O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
for O
fast O
example-based O
super-resolution O

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
(ACCV O

- B-DAT

- B-DAT

- B-DAT
Resolution O
via O
Deep O
and O
Shallow O

- B-DAT

- B-DAT

- B-DAT
ence O
on O
Signals, O
Systems O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
derstanding O
Neural O
Networks O
Through O
Deep O

International O
Conference O
on O
Machine O
Learning O
- B-DAT
Deep O
Learning O
Workshop O
2015, O
page O

- B-DAT

- B-DAT
resolution O
by O
retrieving O
web O
images O

- B-DAT
volutional O
networks. O
In O
European O
Conference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG22, O
SRGAN-VGG54) O
described O
in O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
off O
between O
accuracy O
and O
speed O

-100 B-DAT
thousand O
update O
iterations O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

A.4. O
Set5 O
- B-DAT
Visual O
Results O

A.5. O
Set14 O
- B-DAT
Visual O
Results O

A.6. O
BSD100 O
(five O
random O
samples) O
- B-DAT
Visual O
Results O

on O
the O
two O
common O
datasets O
Set5 B-DAT
[3] O
and O
Set14 O
[34] O
for O

Set5 B-DAT
[3] O
Set14 O
[34] O
Method O
scale O

- B-DAT

- B-DAT
smoothed O
version O
of O
the O
natural O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
Kernel-Blind O
Deconvolution. O
Kernel-blind O
deconvolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
or O
kernel-blind O
deconvo- O
lution. O
These O

- B-DAT
or O
kernel-blind O
case, O
but O
not O

- B-DAT

- B-DAT
smoothed O
probability O
distribution O
of O
natural O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-6 B-DAT
[28]* O
32.36 O
26.34 O
21.43 O
17.33 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
Kernel-Blind O
Deblurring O
(NA+KE). O
Gradient O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
kernel-) O
blind O
deblurring O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
Kernel-Blind O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
We O
evaluate O
our O
method O

- B-DAT

-3 B-DAT
[37] O
have O
a O
single O
model O

- B-DAT

-3 B-DAT
[37] O
35.20 O
31.58 O
29.30 O
26.30 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-153324 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-7, B-DAT
2012, O
pages O
1–10, O
2012 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work O
to O
solve O
them O
all—solving O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
patterned O
color O
images. O
In O
Acoustics O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
Kernel-Blind O

- B-DAT

Dilated O
Conv O
+ O
Residual O
Blocks O
4x B-DAT
NN O

Set5, B-DAT
Set14, O
BSDS100, O
Urban100 O
[18]. O
Standard O

Set5 B-DAT
33.66 O
33.89 O
38.20 O
27.32 O
33.60 O

Set5 B-DAT
28.42 O
28.56 O
32.62 O
24.22 O
28.59 O

Set5 B-DAT
23.80 O
- O
- O
19.32 O
23.73 O

Set5 B-DAT
0.930 O
0.928 O
0.961 O
0.974 O
0.985 O

Set5 B-DAT
0.810 O
0.809 O
0.898 O
0.929 O
0.965 O

Set5 B-DAT
0.646 O
- O
- O
0.801 O
0.904 O

for×2,×4, O
and×8 O
factor O
SISR O
over O
Set5, B-DAT
Set14, O
BSD100, O
and O
Celeb-HQ O
datasets O

art O
SISR O
models O
over O
datasets O
Set5, B-DAT
Set14, O
BSD100, O
and O
Celeb-HQ. O
Statistics O

Set5 B-DAT
33 I-DAT

Set5 B-DAT
28 I-DAT

Set5 B-DAT
23 I-DAT

Set5 B-DAT
0 I-DAT

Set5 B-DAT
0 I-DAT

Set5 B-DAT
0 I-DAT

- B-DAT

- B-DAT

- B-DAT
ous O
demand O
for O
higher-resolution O
images O

- B-DAT

- B-DAT
resolution O
(SISR). O
The O
SISR O
problem O

- B-DAT

- B-DAT

- B-DAT
fectiveness O
for O
different O
scale O
factors O

- B-DAT
pared O
to O
basic O
interpolation O
schemes O

- B-DAT
pared O
with O
current O
state-of-the-art O
techniques O

- B-DAT
construction O
improves O
the O
quality O
of O

- B-DAT

- B-DAT

- B-DAT
resolution O
(HR) O
image O
from O
one O

- B-DAT
resolution O
(LR) O
images. O
SR O
plays O

- B-DAT

- B-DAT

- B-DAT
tions, O
only O
a O
single O
instance O

- B-DAT

- B-DAT

- B-DAT
posed O
inverse O
problem O
[6] O
that O

- B-DAT
formation O
to O
restrict O
the O
solution O

- B-DAT
nique O
introduced O
by O
Nazeri O
et O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
creasing O
the O
resolution O
of O
a O

- B-DAT
ery O
of O
pixel O
intensities O
in O

- B-DAT
els. O
The O
missing O
pixel O
intensities O

- B-DAT
ing O
regions O
of O
an O
image O

- B-DAT
ing O
task O
is O
modelled O
as O

- B-DAT
age. O
The O
pipeline O
involves O
first O

- B-DAT
struction O
of O
the O
HR O
image O

- B-DAT

- B-DAT
sampled O
by O
a O
factor O
of O

- B-DAT

- B-DAT
tinguished O
anymore O
as O
the O
problem O

- B-DAT

- B-DAT
construction O
of O
a O
high-resolution O
image O

- B-DAT

- B-DAT
ments O
of O
information O
using O
bilinear O

- B-DAT
tinctive O
features O
in O
the O
original O

- B-DAT

- B-DAT
pling O
by O
a O
factor O
of O

- B-DAT
tra O
empty O
row O
and O
column O

- B-DAT
polation O
and O
bicubic O
interpolation O
[3 O

- B-DAT
pling O
[5]. O
Edge-based O
methods O
learn O

- B-DAT
file O
[39] O
to O
reconstruct O
the O

- B-DAT
tion O
[36] O
to O
predict O
HR O

- B-DAT

- B-DAT
age O
itself O
[19, O
10] O
to O

- B-DAT

- B-DAT
works O
(CNN) O
with O
a O
per-pixel O

- B-DAT

- B-DAT
cently O
Johnson O
et O
al. O
[21 O

- B-DAT

- B-DAT
ing O
a O
perceptual O
loss. O
In O

- B-DAT

- B-DAT

- B-DAT
tion O
loss O
and O
Style O
reconstruction O

- B-DAT
of-the-art O
results O
on O
SISR O
for O

- B-DAT

- B-DAT
ducing O
realistically O
synthesized O
high-frequency O
textures O

- B-DAT

- B-DAT

- B-DAT
work O
to O
image O
super-resolution O
tasks O

- B-DAT
neously O
improves O
structure, O
texture, O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
couples O
SISR O
into O
two O
separate O

- B-DAT
tor O
for O
the O
edge O
enhancement O

- B-DAT
tures O
to O
the O
method O
proposed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
criminator O
follows O
the O
architecture O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
images. O
Their O
corresponding O
edge O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
maps O
in O
the O
intermediate O

- B-DAT
tor O
to O
produce O
results O
with O

- B-DAT
criminator. O
Spectral O
normalization O
(SN) O
[28 O

- B-DAT
bilizes O
training O
by O
scaling O
down O

- B-DAT
ter O
and O
gradient O
values. O
We O

- B-DAT
ments O

- B-DAT
ally O
strided O
convolution O
kernel. O
This O

- B-DAT

- B-DAT

- B-DAT
19 O
trained O
on O
the O
ImageNet O

-19 B-DAT

- B-DAT
mediate O
feature O
maps. O
The O
Gram O

- B-DAT
fully O
mitigate O
the O
“checkerboard” O
artifact O

- B-DAT
pose O
convolutions O
[31]. O
For O
both O

-19 B-DAT

- B-DAT
imize O
the O
reconstruction, O
style, O
perceptual O

- B-DAT

- B-DAT

- B-DAT
tector O
[1]. O
We O
can O
control O

- B-DAT
rameter O
σ. O
For O
our O
purposes O

- B-DAT
els O
of O
both O
stages O
were O

- B-DAT
tinue O
training O
until O
convergence. O
We O

- B-DAT
licly O
available O
datasets O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ization O
purposes, O
the O
LR O
image O

- B-DAT
neighbor O
interpolation. O
All O
HR O
images O

- B-DAT
pared O
against O
bicubic O
interpolation O
and O

- B-DAT
duces O
blurry O
results O
around O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Celeb-HQ B-DAT
33.25 O
- O
- O
31.33 O
32.12 O

Celeb-HQ B-DAT
29.59 O
- O
- O
27.94 O
28.23 O

Set5 O
23.80 O
- B-DAT
- O
19.32 O
23.73 O

Set14 O
22.37 O
- B-DAT
- O
18.47 O
21.44 O

BSD100 O
22.11 O
- B-DAT
- O
18.65 O
21.63 O

Celeb-HQ B-DAT
26.66 O
- O
- O
25.46 O
25.56 O

Celeb-HQ B-DAT
0.967 O
- O
- O
0.957 O
0.968 O

Celeb-HQ B-DAT
0.834 O
- O
- O
0.910 O
0.912 O

Set5 O
0.646 O
- B-DAT
- O
0.801 O
0.904 O

Set14 O
0.552 O
- B-DAT
- O
0.708 O
0.793 O

BSD100 O
0.532 O
- B-DAT
- O
0.663 O
0752 O

Celeb-HQ B-DAT
0.782 O
- O
- O
0.841 O
0.857 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Resolution O
task. O
We O
measure O
precision O

- B-DAT
ous O
scale O
factors O
of O
SISR O

- B-DAT
tion O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

eb O
- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
efit O
of O
this O
approach O
over O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
posed O
model O
learns O
to O
fill O

- B-DAT

- B-DAT
ent O
scales O
of O
SISR. O
Quantitative O

- B-DAT
ness O
of O
the O
structure-guided O
inpainting O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dard O
benchmarks O

- B-DAT

- B-DAT
ing O
algorithms. O
A O
better O
approach O

- B-DAT
age O
contents O
and O
structures O
and O

- B-DAT

- B-DAT
resolution O
process. O
Our O
source O
code O

- B-DAT

- B-DAT

- B-DAT
ration O
with O
the O
donation O
of O

- B-DAT

- B-DAT

- B-DAT
resolution O
through O
neighbor O
embedding. O
In O

- B-DAT
ings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT
ing O
a O
deep O
convolutional O
network O

- B-DAT
resolution. O
In O
European O
conference O
on O

- B-DAT
sion, O
pages O
184–199. O
Springer, O
2014 O

- B-DAT
sions. O
Journal O
of O
applied O
meteorology O

- B-DAT

- B-DAT
nition, O
pages O
117–130. O
Springer, O
2007 O

- B-DAT
vances O
and O
challenges O
in O
super-resolution O

- B-DAT
tional O
Journal O
of O
Imaging O
Systems O

- B-DAT
far. O
Fast O
and O
robust O
multiframe O

- B-DAT
tics. O
ACM O
transactions O
on O
graphics O

- B-DAT
ing O
from O
local O
self-examples. O
ACM O

- B-DAT

- B-DAT

- B-DAT
thesis O
using O
convolutional O
neural O
networks O

- B-DAT
vances O
in O
Neural O
Information O
Processing O

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT
lenge O
on O
Perceptual O
Image O
Restoration O

- B-DAT
ulation O
(PIRM) O
at O
the O
15th O

- B-DAT

- B-DAT

- B-DAT
gio. O
Generative O
adversarial O
nets. O
In O

- B-DAT
ral O
information O
processing O
systems, O
pages O

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
IEEE O
conference O

- B-DAT
ual O
learning O
for O
image O
recognition O

- B-DAT
age O
super-resolution O
from O
transformed O
self-exemplars O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision, O
Kyoto O

- B-DAT
to-image O
translation O
with O
conditional O
adversarial O

- B-DAT
works. O
In O
Proceedings O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sive O
growing O
of O
GANs O
for O

- B-DAT
ing O
Representations, O
2018. O
4 O

- B-DAT

- B-DAT
ningham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT
sion O
and O
Pattern O
Recognition O
(CVPR O

- B-DAT
hanced O
deep O
residual O
networks O
for O

- B-DAT
resolution. O
In O
Proceedings O
of O
the O

- B-DAT
works. O
In O
International O
Conference O
on O

- B-DAT
resentations, O
2018. O
3 O

- B-DAT
painting O
with O
adversarial O
edge O
learning O

- B-DAT

- B-DAT
crimination. O
In O
Proceedings O
of O
the O

- B-DAT
ference O
on O
Computer O
Vision O
(ECCV O

- B-DAT
nition O
challenge. O
International O
Journal O
of O

- B-DAT
hancenet: O
Single O
image O
super-resolution O
through O

- B-DAT
tomated O
texture O
synthesis. O
In O
The O

- B-DAT
age/video O
upsampling. O
In O
ACM O
Transactions O

- B-DAT

- B-DAT
gle O
image O
and O
video O
super-resolution O

- B-DAT
cient O
sub-pixel O
convolutional O
neural O
network O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
lutional O
networks O
for O
large-scale O
image O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT
sion O
and O
Pattern O
Recognition O
(CVPR O

- B-DAT

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision, O
pages O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
versarial O
networks. O
In O
The O
IEEE O

- B-DAT
ence O
on O
Computer O
Vision O
(ICCV O

widely O
used O
bench- O
mark O
datasets O
Set5 B-DAT
[3], O
Set14 O
[69] O
and O
BSD100 O

versions O
of O
each O
image O
on O
Set5, B-DAT
Set14 O
and O
BSD100: O
nearest O
neighbor O

and O
the O
adversarial O
networks O
on O
Set5 B-DAT
and O
Set14 O
benchmark O
data. O
MOS O

SRResNet- O
SRGAN- O
Set5 B-DAT
MSE O
VGG22 O
MSE O
VGG22 O
VGG54 O

respect O
to O
MOS O
score O
on O
Set5 B-DAT

Set5 B-DAT
nearest O
bicubic O
SRCNN O
SelfExSR O
DRCN O

the O
MOS O
tests O
conducted O
on O
Set5, B-DAT
Set14, O
BSD100 O
are O
summarized O
in O

a O
4× O
upscaling O
factor O
for O
Set5 B-DAT
(Section O
A.4), O
Set14 O
(Section O
A.5 O

downsampled O
versions O
of O
images O
from O
Set5, B-DAT
Set14 O
and O
BSD100. O
On O
BSD100 O

rated O
by O
each O
rater. O
On O
Set5 B-DAT
and O
Set14 O
the O
raters O
also O

ordinal O
ranking. O
While O
results O
on O
Set5 B-DAT
are O
somewhat O
inconclusive O
due O
to O

Set5 B-DAT
Set14 O
BSD100 O

distribution O
of O
MOS O
scores O
on O
Set5, B-DAT
Set14, O
BSD100. O
Mean O
shown O
as O

Set5 B-DAT
Set14 O
BSD100 O

Figure O
10: O
Average O
rank O
on O
Set5, B-DAT
Set14, O
BSD100 O
by O
averaging O
the O

A.4. O
Set5 B-DAT
- O
Visual O
Results O

Figure O
11: O
Results O
for O
Set5 B-DAT
using O
bicubic O
interpolation, O
SRResNet O
and O

and O
the O
adversarial O
networks O
on O
Set5 B-DAT

SRResNet- O
SRGAN- O
Set5 B-DAT

Set5 B-DAT

rated O
by O
each O
rater. O
On O
Set5 B-DAT

ordinal O
ranking. O
While O
results O
on O
Set5 B-DAT

Set5 B-DAT

Set5 B-DAT

Figure O
11: O
Results O
for O
Set5 B-DAT

when O
we O
super-resolve O
at O
large O
upscaling B-DAT
factors? O
The O
behavior O
of O
optimization-based O

photo-realistic O
natural O
images O
for O
4× O
upscaling B-DAT
factors. O
To O
achieve O
this, O
we O

guishable O
from O
original O
(right). O
[4× O
upscaling B-DAT

is O
particularly O
pronounced O
for O
high O
upscaling B-DAT
factors, O
for O
which O
texture O
detail O

are O
shown O
in O
brackets. O
[4× O
upscaling B-DAT

super- O
resolved O
with O
a O
4× O
upscaling B-DAT
factor O
is O
shown O
in O
Figure O

the O
network O
to O
learn O
the O
upscaling B-DAT
filters O
directly O
can O
further O
increase O

was O
also O
shown O
that O
learning O
upscaling B-DAT
filters O
is O
beneficial O
in O
terms O

super-resolves O
face O
images O
with O
large O
upscaling B-DAT
factors O
(8×). O
GANs O
were O
also O

for O
image O
SR O
with O
high O
upscaling B-DAT
factors O
(4×) O
as O
measured O
by O

photo-realistic O
SR O
images O
with O
high O
upscaling B-DAT
factors O
(4 O

losses O
in O
that O
category∗. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

HR O
image O
(right: O
i,j). O
[4× O
upscaling B-DAT

SSIM, O
MOS) O
in O
bold. O
[4× O
upscaling B-DAT

that O
SRGAN O
reconstructions O
for O
large O
upscaling B-DAT
factors O
(4×) O
are, O
by O
a O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
IEEE O

and O
SRGAN O
with O
a O
4× O
upscaling B-DAT
factor O
for O
Set5 O
(Section O
A.4 O

low-/high-resolution O
images O
and O
reconstructions O
(4× O
upscaling) B-DAT
obtained O
with O
different O
methods O
(bicubic O

image O
with O
resolution O
64×64 O
with O
upscaling B-DAT
factor O
4×. O
The O
measurements O
are O

for O
another O
100k O
iterations. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

all O
available O
individual O
ratings. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks, O
one O
central O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
(SR). O
To O
our O
knowledge O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
guishable O
from O
original O
(right). O
[4 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
inal O
image O
means O
that O
the O

- B-DAT
realistic O
as O
defined O
by O
Ferwerda O

- B-DAT

- B-DAT

- B-DAT
ing O
high-level O
feature O
maps O
of O

- B-DAT

- B-DAT
resolved O
with O
a O
4× O
upscaling O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
image O
informa- O
tion O

- B-DAT

- B-DAT
proaches O
to O
the O
SR O
problem O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
[27], O
trees O
[46] O
or O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-the-art O
SR O
performance. O
Subsequently, O
it O

- B-DAT

- B-DAT

- B-DAT
ciently O
train O
these O
deeper O
network O

- B-DAT
normalization O
[32] O
is O
often O
used O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
results. O
Another O
powerful O
design O

- B-DAT

- B-DAT
connections O
relieve O
the O
network O
architecture O

- B-DAT
tentially O
non-trivial O
to O
represent O
with O

- B-DAT

- B-DAT

- B-DAT
ing O
pixel-wise O
averages O
of O
plausible O

- B-DAT

- B-DAT
ity O
[42, O
33, O
13, O
5 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
perceptually O
more O
convincing O
solutions O

- B-DAT
ure O
2. O
We O
illustrate O
the O

- B-DAT
ure O
3 O
where O
multiple O
potential O

- B-DAT
tion. O
Yu O
and O
Porikli O
[66 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions. O
Similar O
to O
this O
work O

- B-DAT
trained O
VGG O
network O
instead O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity. O
The O
GAN O
procedure O
encourages O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mark O
datasets O
as O
well O
as O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
counterpart O
IHR. O
The O
high-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nels O
as O
in O
the O
VGG O

- B-DAT
ical O
for O
the O
performance O
of O

- B-DAT
tent O
loss O
lSRX O
and O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
solutions O
with O
overly O
smooth O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stead O
of O
log[1−DθD O
(GθG(ILR))] O
[22 O

- B-DAT
mark O
datasets O
Set5 O
[3], O
Set14 O

- B-DAT
and O
high-resolution O
images. O
This O
corresponds O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bor, O
bicubic, O
SRCNN O
[9] O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gral O
score O
from O
1 O
(bad O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG54 O
and O
the O
original O
HR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ResNet O
and O
the O
adversarial O
networks O

- B-DAT
SRGAN- O
Set5 O
MSE O
VGG22 O
MSE O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
MSE-based O
reconstructions, O
to O
those O
competing O

- B-DAT

- B-DAT
formed O
other O
SRGAN O
and O
SRResNet O

- B-DAT

- B-DAT
GAN O
to O
NN, O
bicubic O
interpolation O

- B-DAT

- B-DAT

- B-DAT
art O
methods. O
Quantitative O
results O
are O

- B-DAT
resolved O
with O
SRResNet O
and O
SRGAN O

- B-DAT
realistic O
image O
SR. O
All O
differences O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentary O
material). O
We O
further O
found O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Net O
that O
sets O
a O
new O

- B-DAT
sure. O
We O
have O
highlighted O
some O

- B-DAT

- B-DAT
ial O
loss O
by O
training O
a O

- B-DAT

- B-DAT

- B-DAT
the-art O
reference O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Stevenson. O
Super-Resolution B-DAT
from O
Image O
Sequences O
- O
A O
Review. O
Midwest O
Symposium O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
by O
adaptive O
sparse O
domain O

- B-DAT
ization. O
IEEE O
Transactions O
on O
Image O

- B-DAT

- B-DAT
resolution. O
IEEE O
Computer O
Graphics O
and O

- B-DAT
level O
vision. O
International O
Journal O
of O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

- B-DAT
line O
at O
http://torch.ch/blog/2016/02/04/resnets. O
html. O
2016 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In O
European O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT
tion O
with O
deep O
convolutional O
neural O

- B-DAT

- B-DAT

- B-DAT
mentation O
algorithms O
and O
measuring O
ecological O

- B-DAT

- B-DAT

- B-DAT
sive O
survey. O
In O
Machine O
Vision O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
for O
fast O
example-based O
super-resolution O

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
(ACCV O

- B-DAT

- B-DAT

- B-DAT
Resolution O
via O
Deep O
and O
Shallow O

- B-DAT

- B-DAT

- B-DAT
ence O
on O
Signals, O
Systems O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
derstanding O
Neural O
Networks O
Through O
Deep O

International O
Conference O
on O
Machine O
Learning O
- B-DAT
Deep O
Learning O
Workshop O
2015, O
page O

- B-DAT

- B-DAT
resolution O
by O
retrieving O
web O
images O

- B-DAT
volutional O
networks. O
In O
European O
Conference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG22, O
SRGAN-VGG54) O
described O
in O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
off O
between O
accuracy O
and O
speed O

-100 B-DAT
thousand O
update O
iterations O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

A.4. O
Set5 O
- B-DAT
Visual O
Results O

A.5. O
Set14 O
- B-DAT
Visual O
Results O

A.6. O
BSD100 O
(five O
random O
samples) O
- B-DAT
Visual O
Results O

Ground O
Truth O
This O
image O
Set5 B-DAT
mean O

4 O
super-resolution O
on O
images O
from O
Set5 B-DAT
(top) O
and O
Set14 O
(bottom). O
We O

all O
models O
on O
the O
standard O
Set5 B-DAT
[60], O
Set14 O
[61], O
and O
BSD100 O

Ground O
Truth O
This O
image O
Set5 B-DAT
mean O
Set14 O
mean O

Ground O
Truth O
This O
image O
Set5 B-DAT

Ground O
Truth O
This O
image O
Set5 B-DAT

Fattal, O
R.: O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

H.: O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In: O
Proceedings O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
a O
per-pixel O
loss O
between O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tracted O
from O
pretrained O
networks. O
We O

- B-DAT
proaches, O
and O
propose O
the O
use O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pared O
to O
the O
optimization-based O
method O

- B-DAT
itative O
results O
but O
is O
three O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ples O
from O
image O
processing O
include O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mantic O
segmentation O
and O
depth O
estimation O

- B-DAT
forward O
convolutional O
neural O
network O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stead O
on O
differences O
between O
high-level O

- B-DAT
mizing O
a O
loss O
function. O
This O

- B-DAT

- B-DAT
forward O
transformation O
networks O
for O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sure O
image O
similarities O
more O
robustly O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cally O
similar O
to O
the O
input O

- B-DAT
resolution O
fine O
details O
must O
be O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fer O
of O
semantic O
knowledge O
from O

- B-DAT

- B-DAT
mization O
problem O
from O
[10]; O
our O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
forward O
image O
transformation O
tasks O
have O

- B-DAT
tional O
neural O
networks O
with O
per-pixel O

- B-DAT

- B-DAT
ing O
with O
a O
per-pixel O
classification O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ful O
output O
image O
using O
a O

- B-DAT

- B-DAT
pixel O
regression O
[4,5] O
or O
classification O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
to O
generate O
images O
where O

- B-DAT
level O
features O
extracted O
from O
a O

- B-DAT
stand O
the O
functions O
encoded O
in O

- B-DAT

- B-DAT
formation O
retained O
by O
different O
network O

- B-DAT

- B-DAT

- B-DAT

-16 B-DAT

- B-DAT
quality O
results, O
but O
is O
computationally O

- B-DAT
mization O
problem O
requires O
a O
forward O

- B-DAT

- B-DAT
work O
to O
quickly O
approximate O
solutions O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
niques O
into O
prediction-based O
methods O
(bilinear O

- B-DAT
based O
methods O
[25,26], O
statistical O
methods O

- B-DAT

- B-DAT
mance O
on O
single-image O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formation O
network O
fW O
and O
a O

- B-DAT
lutional O
neural O
network O
parameterized O
by O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trained O
for O
image O
classification O
have O

- B-DAT
fication O
as O
a O
fixed O
loss O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
image, O
and O
the O
style O

- B-DAT

- B-DAT

- B-DAT
chitecture O
of O
[44]. O
All O
non-residual O

- B-DAT
put O
layer, O
which O
instead O
uses O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sample O
the O
low-resolution O
input O
before O

- B-DAT

-16 B-DAT
loss O
network O
φ. O
As O
we O

-2 B-DAT
convolutions O
to O
downsample O
the O
input O

- B-DAT

- B-DAT

- B-DAT
lutional O
layer O
increases O
the O
effective O

- B-DAT

- B-DAT
trained O
for O
image O
classification, O
meaning O

- B-DAT

- B-DAT

- B-DAT

-16 B-DAT
loss O
network O
φ. O
The O
images O

- B-DAT
put O
image O
ŷ O
when O
it O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
[48,49] O
and O
make O
use O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
with O
convolutional O
neural O
networks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions. O
Our O
networks O
are O
trained O

- B-DAT
mization O
is O
performed O
using O
L-BFGS O

- B-DAT

- B-DAT
timization O
converges O
to O
satisfactory O
results O

- B-DAT

-16 B-DAT
loss O
network O
φ O

- B-DAT

-16 B-DAT
loss O
network O
φ. O
Our O
implementation O

- B-DAT

- B-DAT

- B-DAT

-1832 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-16 B-DAT
loss O
network O
has O
features O
which O

-16 B-DAT
features, O
and O
in O
doing O
so O

- B-DAT
tion O
5. O
The O
baseline O
performs O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put O
image O
from O
a O
low-resolution O

- B-DAT

- B-DAT
lem, O
since O
for O
each O
low-resolution O

- B-DAT

- B-DAT
ages O
that O
could O
have O
generated O

- B-DAT

- B-DAT
resolution O
image O
may O
have O
little O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
since O
larger O
factors O
require O

- B-DAT

- B-DAT
sessment O
of O
visual O
quality O
[55,56,57,58,59 O

- B-DAT
level O
differences O
between O
pixels O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mize O
feature O
reconstruction O
loss. O
We O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-16 B-DAT
loss O
network O
φ. O
We O
train O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formance. O
SRCNN O
is O
a O
three-layer O

- B-DAT

- B-DAT
CNN O
is O
not O
trained O
for O

- B-DAT

- B-DAT
ally O
feasible O
for O
our O
models O

- B-DAT
works O
for O
×4 O
and O
×8 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mation O
tasks O
and O
optimization-based O
methods O

- B-DAT

- B-DAT
image O
super-resolution O
where O
we O
show O

- B-DAT
tion. O
We O
also O
plan O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lutional O
networks. O
(2015 O

- B-DAT

- B-DAT

- B-DAT
ing O
them. O
In: O
Proceedings O
of O

- B-DAT
works: O
Visualising O
image O
classification O
models O

- B-DAT

- B-DAT
tation. O
arXiv O
preprint O
arXiv:1505.04366 O
(2015 O

- B-DAT

- B-DAT
ings O
of O
the O
IEEE O
International O

- B-DAT
gus, O
R.: O
Intriguing O
properties O
of O

- B-DAT
tional O
Conference O
on, O
IEEE O
(2012 O

- B-DAT
ject O
detection O
features. O
In: O
Proceedings O

- B-DAT
works. O
arXiv O
preprint O
arXiv:1506.02753 O
(2015 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puter O
Graphics O
and O
Applications, O
IEEE O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision, O
2009 O
IEEE O
12th O

- B-DAT

- B-DAT

- B-DAT
ple O
regression. O
In: O
Proceedings O
of O

- B-DAT

- B-DAT

- B-DAT
sentation O
of O
raw O
image O
patches O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
with O
deep O
convolutional O
generative O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-192376 B-DAT
(2011 O

- B-DAT

- B-DAT

- B-DAT
ence O
image O
quality O
assessment O
algorithms O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
representations. O
In: O
Curves O
and O
Surfaces O

- B-DAT

- B-DAT

widely O
used O
bench- O
mark O
datasets O
Set5 B-DAT
[3], O
Set14 O
[69] O
and O
BSD100 O

versions O
of O
each O
image O
on O
Set5, B-DAT
Set14 O
and O
BSD100: O
nearest O
neighbor O

and O
the O
adversarial O
networks O
on O
Set5 B-DAT
and O
Set14 O
benchmark O
data. O
MOS O

SRResNet- O
SRGAN- O
Set5 B-DAT
MSE O
VGG22 O
MSE O
VGG22 O
VGG54 O

respect O
to O
MOS O
score O
on O
Set5 B-DAT

Set5 B-DAT
nearest O
bicubic O
SRCNN O
SelfExSR O
DRCN O

the O
MOS O
tests O
conducted O
on O
Set5, B-DAT
Set14, O
BSD100 O
are O
summarized O
in O

a O
4× O
upscaling O
factor O
for O
Set5 B-DAT
(Section O
A.4), O
Set14 O
(Section O
A.5 O

downsampled O
versions O
of O
images O
from O
Set5, B-DAT
Set14 O
and O
BSD100. O
On O
BSD100 O

rated O
by O
each O
rater. O
On O
Set5 B-DAT
and O
Set14 O
the O
raters O
also O

ordinal O
ranking. O
While O
results O
on O
Set5 B-DAT
are O
somewhat O
inconclusive O
due O
to O

Set5 B-DAT
Set14 O
BSD100 O

distribution O
of O
MOS O
scores O
on O
Set5, B-DAT
Set14, O
BSD100. O
Mean O
shown O
as O

Set5 B-DAT
Set14 O
BSD100 O

Figure O
10: O
Average O
rank O
on O
Set5, B-DAT
Set14, O
BSD100 O
by O
averaging O
the O

A.4. O
Set5 B-DAT
- O
Visual O
Results O

Figure O
11: O
Results O
for O
Set5 B-DAT
using O
bicubic O
interpolation, O
SRResNet O
and O

and O
the O
adversarial O
networks O
on O
Set5 B-DAT

SRResNet- O
SRGAN- O
Set5 B-DAT

Set5 B-DAT

rated O
by O
each O
rater. O
On O
Set5 B-DAT

ordinal O
ranking. O
While O
results O
on O
Set5 B-DAT

Set5 B-DAT

Set5 B-DAT

Figure O
11: O
Results O
for O
Set5 B-DAT

when O
we O
super-resolve O
at O
large O
upscaling B-DAT
factors? O
The O
behavior O
of O
optimization-based O

photo-realistic O
natural O
images O
for O
4× O
upscaling B-DAT
factors. O
To O
achieve O
this, O
we O

guishable O
from O
original O
(right). O
[4× O
upscaling B-DAT

is O
particularly O
pronounced O
for O
high O
upscaling B-DAT
factors, O
for O
which O
texture O
detail O

are O
shown O
in O
brackets. O
[4× O
upscaling B-DAT

super- O
resolved O
with O
a O
4× O
upscaling B-DAT
factor O
is O
shown O
in O
Figure O

the O
network O
to O
learn O
the O
upscaling B-DAT
filters O
directly O
can O
further O
increase O

was O
also O
shown O
that O
learning O
upscaling B-DAT
filters O
is O
beneficial O
in O
terms O

super-resolves O
face O
images O
with O
large O
upscaling B-DAT
factors O
(8×). O
GANs O
were O
also O

for O
image O
SR O
with O
high O
upscaling B-DAT
factors O
(4×) O
as O
measured O
by O

photo-realistic O
SR O
images O
with O
high O
upscaling B-DAT
factors O
(4 O

losses O
in O
that O
category∗. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

HR O
image O
(right: O
i,j). O
[4× O
upscaling B-DAT

SSIM, O
MOS) O
in O
bold. O
[4× O
upscaling B-DAT

that O
SRGAN O
reconstructions O
for O
large O
upscaling B-DAT
factors O
(4×) O
are, O
by O
a O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
IEEE O

and O
SRGAN O
with O
a O
4× O
upscaling B-DAT
factor O
for O
Set5 O
(Section O
A.4 O

low-/high-resolution O
images O
and O
reconstructions O
(4× O
upscaling) B-DAT
obtained O
with O
different O
methods O
(bicubic O

image O
with O
resolution O
64×64 O
with O
upscaling B-DAT
factor O
4×. O
The O
measurements O
are O

for O
another O
100k O
iterations. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

all O
available O
individual O
ratings. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks, O
one O
central O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
(SR). O
To O
our O
knowledge O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
guishable O
from O
original O
(right). O
[4 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
inal O
image O
means O
that O
the O

- B-DAT
realistic O
as O
defined O
by O
Ferwerda O

- B-DAT

- B-DAT

- B-DAT
ing O
high-level O
feature O
maps O
of O

- B-DAT

- B-DAT
resolved O
with O
a O
4× O
upscaling O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
image O
informa- O
tion O

- B-DAT

- B-DAT
proaches O
to O
the O
SR O
problem O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
[27], O
trees O
[46] O
or O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-the-art O
SR O
performance. O
Subsequently, O
it O

- B-DAT

- B-DAT

- B-DAT
ciently O
train O
these O
deeper O
network O

- B-DAT
normalization O
[32] O
is O
often O
used O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
results. O
Another O
powerful O
design O

- B-DAT

- B-DAT
connections O
relieve O
the O
network O
architecture O

- B-DAT
tentially O
non-trivial O
to O
represent O
with O

- B-DAT

- B-DAT

- B-DAT
ing O
pixel-wise O
averages O
of O
plausible O

- B-DAT

- B-DAT
ity O
[42, O
33, O
13, O
5 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
perceptually O
more O
convincing O
solutions O

- B-DAT
ure O
2. O
We O
illustrate O
the O

- B-DAT
ure O
3 O
where O
multiple O
potential O

- B-DAT
tion. O
Yu O
and O
Porikli O
[66 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions. O
Similar O
to O
this O
work O

- B-DAT
trained O
VGG O
network O
instead O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity. O
The O
GAN O
procedure O
encourages O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mark O
datasets O
as O
well O
as O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
counterpart O
IHR. O
The O
high-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nels O
as O
in O
the O
VGG O

- B-DAT
ical O
for O
the O
performance O
of O

- B-DAT
tent O
loss O
lSRX O
and O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
solutions O
with O
overly O
smooth O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stead O
of O
log[1−DθD O
(GθG(ILR))] O
[22 O

- B-DAT
mark O
datasets O
Set5 O
[3], O
Set14 O

- B-DAT
and O
high-resolution O
images. O
This O
corresponds O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bor, O
bicubic, O
SRCNN O
[9] O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gral O
score O
from O
1 O
(bad O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG54 O
and O
the O
original O
HR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ResNet O
and O
the O
adversarial O
networks O

- B-DAT
SRGAN- O
Set5 O
MSE O
VGG22 O
MSE O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
MSE-based O
reconstructions, O
to O
those O
competing O

- B-DAT

- B-DAT
formed O
other O
SRGAN O
and O
SRResNet O

- B-DAT

- B-DAT
GAN O
to O
NN, O
bicubic O
interpolation O

- B-DAT

- B-DAT

- B-DAT
art O
methods. O
Quantitative O
results O
are O

- B-DAT
resolved O
with O
SRResNet O
and O
SRGAN O

- B-DAT
realistic O
image O
SR. O
All O
differences O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentary O
material). O
We O
further O
found O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Net O
that O
sets O
a O
new O

- B-DAT
sure. O
We O
have O
highlighted O
some O

- B-DAT

- B-DAT
ial O
loss O
by O
training O
a O

- B-DAT

- B-DAT

- B-DAT
the-art O
reference O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Stevenson. O
Super-Resolution B-DAT
from O
Image O
Sequences O
- O
A O
Review. O
Midwest O
Symposium O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
by O
adaptive O
sparse O
domain O

- B-DAT
ization. O
IEEE O
Transactions O
on O
Image O

- B-DAT

- B-DAT
resolution. O
IEEE O
Computer O
Graphics O
and O

- B-DAT
level O
vision. O
International O
Journal O
of O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

- B-DAT
line O
at O
http://torch.ch/blog/2016/02/04/resnets. O
html. O
2016 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In O
European O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT
tion O
with O
deep O
convolutional O
neural O

- B-DAT

- B-DAT

- B-DAT
mentation O
algorithms O
and O
measuring O
ecological O

- B-DAT

- B-DAT

- B-DAT
sive O
survey. O
In O
Machine O
Vision O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
for O
fast O
example-based O
super-resolution O

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
(ACCV O

- B-DAT

- B-DAT

- B-DAT
Resolution O
via O
Deep O
and O
Shallow O

- B-DAT

- B-DAT

- B-DAT
ence O
on O
Signals, O
Systems O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
derstanding O
Neural O
Networks O
Through O
Deep O

International O
Conference O
on O
Machine O
Learning O
- B-DAT
Deep O
Learning O
Workshop O
2015, O
page O

- B-DAT

- B-DAT
resolution O
by O
retrieving O
web O
images O

- B-DAT
volutional O
networks. O
In O
European O
Conference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG22, O
SRGAN-VGG54) O
described O
in O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
off O
between O
accuracy O
and O
speed O

-100 B-DAT
thousand O
update O
iterations O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

A.4. O
Set5 O
- B-DAT
Visual O
Results O

A.5. O
Set14 O
- B-DAT
Visual O
Results O

A.6. O
BSD100 O
(five O
random O
samples) O
- B-DAT
Visual O
Results O

test O
datasets, O
i.e., O
Set12 O
[57], O
BSD68 B-DAT
[38], O
and O
Ur- O
ban100 O
[23 O

in O
terms O
of O
PSNR O
on O
BSD68 B-DAT

and O
50 O
on O
datasets O
Set14, O
BSD68 B-DAT
and O
Urban100. O
Red O
color O
indicates O

BSD68 B-DAT
15 O
31.08 O
/ O
0.8722 O
31.42 O

Image O
denoising O
results O
of O
“Test011” O
(BSD68) B-DAT
with O
noise O
level O
50 O

Image O
denoising O
results O
of O
“Test011” O
(BSD68) B-DAT
with O
noise O
level O
50 O

Image O
denoising O
results O
of O
“Test011” O
(BSD68) B-DAT
with O
noise O
level O
50 O

Image O
denoising O
results O
of O
“Test011” O
(BSD68) B-DAT
with O
noise O
level O
50 O

BSD68 B-DAT
26.35 O
/ O
0.142 O
24.32 O

0.082 O
27.74 O
/ O
0.091 O
BSD68 B-DAT
26.16 O
/ O
0.044 O
26.45 O

various O
noise O
levels O
on O
Set12, O
BSD68 B-DAT
and O
Urban100. O
The O
best O
performance O

BSD68 B-DAT
15 O
31.07/0.8717 O
31.37/0.8766 O
31.42/0.8769 O
31.52 O

We O
use O
Ko- O
dak24 O
(http://r0k.us/graphics/kodak/), O
BSD68 B-DAT
[53], O
and O
Urban100 O
[11] O
for O

and O
FFDNet O
[20]. O
Kodak24 O
(http://r0k.us/graphics/kodak/), O
BSD68 B-DAT
[53], O
and O
Urban100 O
[11] O
are O

Method O
Kodak24 O
BSD68 B-DAT
Urban10010 O
30 O
50 O
70 O
10 O

Method O
Kodak24 O
BSD68 B-DAT
Urban10010 O
30 O
50 O
70 O
10 O

BSD68 B-DAT

BSD68 B-DAT

for O
σ O
= O
25 O
and O
BSD68 B-DAT
is O
part O
of O
the O
RED30 O

BSD68 B-DAT
25 O
29.23 O
28.56 O
29.03 O
28.99 O

accuracy O
and O
efficiency O
on O
the O
BSD68 B-DAT
dataset O
with O
σ O
= O
15 O

gain O
on O
average O
for O
the O
BSD68 B-DAT
dataset), O
possibly O
because O
the O
noise O

grayscale O
image O
denoising, O
we O
use O
BSD68 B-DAT
[3] O
and O
Set12 O
datasets O
to O

for O
removing O
real O
noise. O
The O
BSD68 B-DAT
dataset O
consists O
of O
68 O
images O

on O
image O
“102061” O
from O
the O
BSD68 B-DAT
dataset O
with O
noise O
level O
50 O

color O
version O
of O
the O
grayscale O
BSD68 B-DAT
dataset. O
The O
Kodak24 O
dataset O
consists O

report O
the O
PSNR O
results O
on O
BSD68 B-DAT
and O
Set12 O
datasets, O
respectively. O
We O

comparison. O
Their O
PSNR O
results O
on O
BSD68 B-DAT
dataset O
with O
noise O
level O
50 O

range O
of O
noise O
levels O
on O
BSD68 B-DAT

We O
also O
tested O
FFDNet-Clip O
on O
BSD68 B-DAT
dataset O
with O
clipping O
setting, O
it O

RESULTS O
OF O
DIFFERENT O
METHODS O
ON O
BSD68 B-DAT

as O
15) O
are O
evaluated O
on O
BSD68 B-DAT
images O
with O
noise O
level O
ranging O

PSNR O
results O
are O
evaluated O
on O
BSD68 B-DAT

images O
from O
Berkeley O
segmentation O
dataset O
(BSD68) B-DAT
[12] O
and O
the O
other O
one O

use O
color O
version O
of O
the O
BSD68 B-DAT
dataset O
for O
testing O
and O
the O

OF O
DIFFERENT O
METHODS O
ON O
THE O
BSD68 B-DAT
DATASET. O
THE O
BEST O
RESULTS O
ARE O

of O
different O
methods O
on O
the O
BSD68 B-DAT
dataset O
are O
shown O
in O
Table O

are O
evaluated O
on O
the O
gray/color O
BSD68 B-DAT
dataset O

results O
of O
one O
image O
from O
BSD68 B-DAT
with O
noise O
level O
50 O

and O
TNRD O
for O
comparison. O
The O
BSD68 B-DAT
dataset O

15, O
25 O
AND O
50 O
ON O
BSD68 B-DAT
DATASET, O
SINGLE O
IMAGE O
SUPER-RESOLUTION O
WITH O

BSD68 B-DAT
25 O
28.57 O
/ O
0.8017 O
28.92 O

average O
PSNR O
of O
29.15dB O
on O
BSD68 B-DAT
dataset O
[50], O
which O
is O
much O

improve O
the O
PSNR O
results O
of O
BSD68 B-DAT
dataset O
[50] O
but O
can O
slightly O

of O
different O
methods O
on O
(gray) O
BSD68 B-DAT
dataset O

proposed O
CNN O
denoiser O
on O
(color) O
BSD68 B-DAT
dataset O

results O
of O
different O
methods O
on O
BSD68 B-DAT
dataset O
are O
shown O
in O
Table O

Urban100 B-DAT
[11] O
26.05 O
25.50 O
26.72 O
26.85 O

Method O
Scale O
Set5 O
Set14 O
B100 O
Urban100 B-DAT
Manga109PSNR O
SSIM O
PSNR O
SSIM O
PSNR O

51], O
Set14 O
[52], O
B100 O
[53], O
Urban100 B-DAT
[11], O
and O
Manga109 O
[57] O
for O

dak24 O
(http://r0k.us/graphics/kodak/), O
BSD68 O
[53], O
and O
Urban100 B-DAT
[11] O
for O
color O
and O
gray O

Urban100 B-DAT

Urban100 B-DAT

Urban100 B-DAT

Urban100 B-DAT
BD O
23.52/0.6862 O
25.70/0.7770 O
22.04/0.6745 O
26.61/0.8136 O

results O
on O
Set5, O
Set14, O
B100, O
Urban100, B-DAT
and O
Manga109 O
with O
scaling O
factor O

Kodak24 O
(http://r0k.us/graphics/kodak/), O
BSD68 O
[53], O
and O
Urban100 B-DAT
[11] O
are O
used O
for O
gray-scale O

test O
sets O
respectively. O
Gains O
on O
Urban100 B-DAT
become O
larger, O
which O
is O
mainly O

Method O
Kodak24 O
BSD68 O
Urban10010 B-DAT
30 O
50 O
70 O
10 O
30 O

Method O
Kodak24 O
BSD68 O
Urban10010 B-DAT
30 O
50 O
70 O
10 O
30 O

Urban100 B-DAT

Urban100 B-DAT

Urban100 B-DAT

evaluate O
our O
NLRN O
on O
the O
Urban100 B-DAT
dataset O
[20], O
which O
contains O
abundant O

Set14 O
[50], O
BSD100 O
[30] O
and O
Urban100 B-DAT
[20] O
for O
testing O
with O
three O

outperforms O
all O
the O
competitors O
on O
Urban100 B-DAT
and O
yields O
the O
best O
results O

levels O
on O
Set12, O
BSD68 O
and O
Urban100 B-DAT

Urban100 B-DAT
15 O
32.35/0.9220 O
32.97/0.9271 O
31.86/0.9031 O

on O
14 O
images, O
BSD200 O
and O
Urban100 B-DAT

Urban100 B-DAT
30 O
28.75/0.8567 O
29.47/0.8697 O
29.12/0.8674 O
29.10/0.8631 O

datasets O
Set5, O
Set14, O
BSD100 O
and O
Urban100 B-DAT

Urban100 B-DAT
×2 O
29.50/0.8946 O
30.76/0.9140 O
30.75/0.9133 O
30.41/0.910 O

barbara. O
2) O
image O
004 O
in O
Urban100 B-DAT

. O
3) O
image O
019 O
in O
Urban100 B-DAT

. O
4) O
image O
033 O
in O
Urban100 B-DAT

. O
5) O
image O
046 O
in O
Urban100 B-DAT

bottom: O
1) O
image O
005 O
in O
Urban100 B-DAT

. O
2) O
image O
019 O
in O
Urban100 B-DAT

. O
3) O
image O
044 O
in O
Urban100 B-DAT

. O
4) O
image O
062 O
in O
Urban100 B-DAT

. O
5) O
image O
099 O
in O
Urban100 B-DAT

Set12, O
and O
1.2dB O
higher O
on O
Urban100 B-DAT

more O
results O
on O
Set12 O
and O
Urban100 B-DAT

Set14 O
[56], O
BSD100 O
[38], O
and O
Urban100 B-DAT
[23], O
because O
they O
are O
widely O

on O
datasets O
Set14, O
BSD68 O
and O
Urban100 B-DAT

Urban100 B-DAT
15 O
32.34 O
/ O
0.9220 O
31.98 O

datasets O
Set5, O
Set14, O
BSD100 O
and O
Urban100 B-DAT

Urban100 B-DAT
×2 O
- O
30.76 O
/ O
0.9140 O

on O
Set5 O
and O
Set14. O
On O
Urban100, B-DAT
our O
MWCNN O
outperforms O
VDSR O
by O

0.122 O
26.52 O
/ O
0.088 O
Urban100 B-DAT
26.56 O
/ O
0.764 O
24.18 O

Urban100 B-DAT
26.08 O
/ O
0.212 O
27.10 O

PSNR O
and O
SSIM O
[43] O
on O
Urban100 B-DAT
for O
different O
architectures O
on O
gray-scale O

set O
[32], O
and O
(iii) O
the O
Urban100 B-DAT
[16] O
dataset, O
which O
contains O
images O

shows O
the O
results O
on O
the O
Urban100 B-DAT
test O
set O
(σ O
= O
25 O

Table O
2. O
PSNR O
(dB) O
on O
Urban100 B-DAT
for O
gray-scale O
image O
denoising O
for O

values O
on O
an O
image O
from O
Urban100 B-DAT
(σ O
= O
50 O

2 O
shows O
the O
results O
on O
Urban100 B-DAT
with O
σ O
∈ O
{25, O
50 O

0.79dB O
(σ O
= O
70) O
on O
Urban100 B-DAT

Urban100 B-DAT
25 O
29.97 O
29.71 O
29.92 O
29.80 O

accuracy O
of O
DnCNN O
even O
on O
Urban100, B-DAT
whereas O
our O
N3Net O
even O
fares O

NN3D O
is O
very O
effective O
on O
Urban100 B-DAT
where O
self-similarity O
can O
intuitively O
shine O

for O
an O
image O
from O
the O
Urban100 B-DAT
dataset. O
BM3D O
and O
UNLNet O
can O

Table O
13. O
PSNR O
(dB) O
on O
Urban100 B-DAT
for O
different O
architectures O
on O
gray-scale O

of O
100 O
images O
(BSD100), O
and O
Urban100 B-DAT

for O
single O
image O
super-resolution O
on O
Urban100 B-DAT
and O
BSD100. O
WSD-SR O
does O
not O

Urban100 B-DAT
×2 O
26.88 O
29.54 O
30.29 O
31.31 O

values O
on O
four O
images O
from O
Urban100 B-DAT
with O
a O
super-resolution O
factor O
of O

Set5 O
and O
Set14, O
BSD100 O
and O
Urban100 B-DAT
[40]) O
used O
in O
[35 O

Urban100 B-DAT
3 O
26.42 O
/ O
0.8076 O
27.13 O

results O
of O
one O
image O
from O
Urban100 B-DAT
dataset O
with O
upscaling O
factor O
4 O

Dilated O
Conv O
+ O
Residual O
Blocks O
4x B-DAT
NN O

Celeb-HQ B-DAT
[22]. O
High-quality O
version O
of O
the O

Celeb-HQ B-DAT
33.25 O
- O
- O
31.33 O
32.12 O

Celeb-HQ B-DAT
29.59 O
- O
- O
27.94 O
28.23 O

Celeb-HQ B-DAT
26.66 O
- O
- O
25.46 O
25.56 O

Celeb-HQ B-DAT
0.967 O
- O
- O
0.957 O
0.968 O

Celeb-HQ B-DAT
0.834 O
- O
- O
0.910 O
0.912 O

Celeb-HQ B-DAT
0.782 O
- O
- O
0.841 O
0.857 O

over O
Set5, O
Set14, O
BSD100, O
and O
Celeb-HQ B-DAT
datasets O
with O
bicubic O
interpolation, O
ENet O

datasets O
Set5, O
Set14, O
BSD100, O
and O
Celeb-HQ B-DAT

our O
edge O
enhancer O
G1 O
for O
Celeb-HQ B-DAT
and O
Places2 O
datasets O
for O
the O

Celeb B-DAT

Celeb B-DAT

Celeb B-DAT

Celeb B-DAT

Celeb B-DAT

Celeb B-DAT

Celeb B-DAT

over O
Set5, O
Set14, O
BSD100, O
and O
Celeb B-DAT

datasets O
Set5, O
Set14, O
BSD100, O
and O
Celeb B-DAT

our O
edge O
enhancer O
G1 O
for O
Celeb B-DAT

5 O
standard O
benchmark O
datasets: O
Set5, O
Set14 B-DAT

datasets O
(e.g., O
such O
as O
Set5, O
Set14 B-DAT
and O
BSD100) O
with O
rich O

Method O
Set5 O
Set14 B-DAT
BSD100 O
Urban100 O
Manga109 O

Method O
Set5 O
Set14 B-DAT
BSD100 O
Urban100 O
Manga109 O

The O
way O
of O
embedding O
upscaling B-DAT
feature O
in O
the O
last O
few O

datasets O
(e.g., O
such O
as O
Set5, O
Set14 B-DAT

Method O
Set5 O
Set14 B-DAT

Method O
Set5 O
Set14 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
traction O
module, O
which O
consists O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
is O
optimized O
by O
stochastic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
plified O
residual O
blocks O
with O
local-source O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-1 B-DAT
Fg O

- B-DAT

- B-DAT
NL O

- B-DAT
NL O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-1 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sentations. O
Thus, O
we O
set O
α O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
spectively. O
f(·) O
and O
δ(·) O
are O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ture, O
and O
embed O
RL-NL O
modules O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tains O
the O
convolution O
layers O
with O

- B-DAT
ble O
1 O
we O
can O
see O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

benchmark O
datasets O
– O
Set5 O
[42], O
Set14 B-DAT
[43], O
BSD100 O
[44], O
Urban100 O
[45 O

face O
from O
Set14 B-DAT

RCAN O
EnhanceNet O
SRGANbaboon O
from O
Set14 B-DAT

baboon O
from O
Set14 B-DAT

baboon O
from O
Set14 B-DAT

baboon O
from O
Set14 B-DAT
zebra O
from O
Set14 O
175043 O
from O
BSD100 O

Set5 O
Set14 B-DAT
BSD100 O
Urban100 O
Manga109 O
PSNR/SSIM O
PSNR/SSIM O

baboon O
from O
Set14 B-DAT

zebra O
from O
Set14 B-DAT

face O
from O
Set14 B-DAT
HR I-DAT
Bicubic O
SRCNN O
EDSR O

RCAN O
EnhanceNet O
SRGANbaboon O
from O
Set14 B-DAT
43074 I-DAT
from O
BSD100 O

baboon O
from O
Set14 B-DAT
baboon I-DAT
from O
Set14 O

baboon O
from O
Set14 B-DAT
zebra I-DAT
from O
Set14 O
175043 O
from O
BSD100 O

Set5 O
Set14 B-DAT

baboon O
from O
Set14 B-DAT
DIV2K I-DAT
DF2K O
DF2K+OST O

zebra O
from O
Set14 B-DAT
78004 I-DAT
from O
BSD100 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
GAN) O
[1] O
is O
a O
seminal O

- B-DAT

- B-DAT
lar, O
we O
introduce O
the O
Residual-in-Residual O

- B-DAT
vide O
stronger O
supervision O
for O
brightness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lem, O
has O
attracted O
increasing O
attention O

- B-DAT
panies. O
SISR O
aims O
at O
recovering O

- B-DAT

- B-DAT

- B-DAT
perous O
development. O
Various O
network O
architecture O

- B-DAT

- B-DAT
Noise O
Ratio O
(PSNR) O
value O
[5,6,7,1,8,9,10,11,12 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uation O
of O
human O
observers O
[1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mize O
super-resolution O
model O
in O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
RGAN, O
consistently O
outperforms O
state-of-the-art O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
struction O
style O
and O
smoothness. O
Another O

- B-DAT
pate O
in O
region O
1 O
and O

- B-DAT

- B-DAT

- B-DAT
tures, O
such O
as O
a O
deeper O

- B-DAT
work O
[9], O
deep O
back O
projection O

- B-DAT
provement. O
Zhang O
et O
al. O
[11 O

- B-DAT
ing O
the O
state-of-the-art O
PSNR O
performance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity O
[29,14], O
perceptual O
loss O
[13 O

- B-DAT
imizing O
the O
error O
in O
a O

- B-DAT
pearance. O
Ledig O
et O
al. O
[1 O

- B-DAT
jadi O
et O
al. O
[16] O
develop O

- B-DAT

- B-DAT
veloping O
more O
effective O
GAN O
frameworks O

- B-DAT
tor O
includes O
gradient O
clipping O
[32 O

- B-DAT
ated O
data O
are O
real, O
but O

- B-DAT
sures, O
e.g., O
PSNR O
and O
SSIM O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lenge O
[3]. O
In O
a O
recent O

- B-DAT
tion, O
we O
first O
describe O
our O

- B-DAT
tion O
is O
done O
in O
the O

- B-DAT
ers; O
2) O
replace O
the O
original O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
putational O
complexity O
in O
different O
PSNR-oriented O

- B-DAT
ing O
dataset O
during O
testing. O
When O

- B-DAT
alization O
ability. O
We O
empirically O
observe O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
residual O
network. O
However, O
our O

- B-DAT
erage O
Discriminator O
RaD O
[2], O
denoted O

- B-DAT

- B-DAT
mulated O
as O
DRa(xr, O
xf O

- B-DAT

- B-DAT

- B-DAT
tures O
before O
activation O
rather O
than O

- B-DAT

-543 B-DAT
layer O
is O
merely O
11.17%. O
The O

- B-DAT

- B-DAT

- B-DAT
tance O
between O
recovered O
image O
G(xi O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nition O
[38], O
which O
focuses O
on O

- B-DAT
ing O
perceptual O
loss O
that O
focuses O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tures O
and O
similarly, O
22 O
represents O

- B-DAT

-22 B-DAT
b) O
activation O
map O
of O

-54 B-DAT

- B-DAT
boon’. O
With O
the O
network O
going O

- B-DAT

- B-DAT
ceptual O
quality, O
we O
propose O
a O

- B-DAT
tion. O
Specifically, O
we O
first O
train O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
parameters O
of O
these O
two O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
polated O
image O
is O
either O
too O

- B-DAT
rameter O
λ O
and O
η O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
oriented O
model O
with O
the O
L1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els O
on O
widely O
used O
benchmark O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
the-art O
PSNR-oriented O
methods O
including O
SRCNN O

- B-DAT

- B-DAT
tures, O
e.g., O
animal O
fur, O
building O

- B-DAT
pleasant O
artifacts, O
e.g., O
artifacts O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults, O
and O
than O
previous O
GAN-based O

- B-DAT
tures O
in O
building O
(see O
image O

- B-DAT

- B-DAT
mance O
without O
artifacts. O
It O
does O

- B-DAT
ment O
can O
be O
observed O
from O

- B-DAT
tures O
before O
activation O
can O
result O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gies O
in O
balancing O
the O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
oriented O
method O
outputs O
cartoon-style O
blurry O

- B-DAT

- B-DAT

- B-DAT
pirically O
make O
some O
modifications O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
terpolation O
between O
the O
results O
of O

- B-DAT

- B-DAT

- B-DAT
ceptual O
quality O
than O
previous O
SR O

- B-DAT

- B-DAT
dition, O
useful O
techniques O
including O
residual O

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In: O
CVPR. O
(2018 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
performance O
on O
imagenet O
classification O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
representations. O
In: O
International O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tics. O
(2010 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
duce O
several O
useful O
techniques O
that O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
a O
very O
deep O
network O

- B-DAT

- B-DAT
Residual O
Dense O
Block O
(RRDB), O
which O

- B-DAT
ers O
[47,28]. O
He O
et O
al O

- B-DAT

- B-DAT
tially. O
It O
is O
worth O
noting O

- B-DAT
tion O
(multiplying O
0.1 O
for O
all O

- B-DAT
tremely O
bad O
local O
minimum O
with O

- B-DAT
tion O
(×0.1) O
helps O
the O
network O

- B-DAT
tion O
achieves O
a O
higher O
PSNR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rithms: O
average O
PSNR/SSIM O
on O
Y O

Bicubic O
- B-DAT
28.42/0.8104 O
26.00/0.7027 O
25.96/0.6675 O
23.14/0.6577 O
24.89/0.7866 O

- B-DAT

- B-DAT
verse O
natural O
textures. O
We O
employ O

- B-DAT
over, O
the O
deeper O
model O
achieves O

- B-DAT

- B-DAT

- B-DAT

reduction, O
and O
Set5 O
[5] O
and O
Set14 B-DAT
[58] O
for O
single O
image O
super-resolution O

the O
classic O
Set5 O
[5] O
and O
Set14 B-DAT
[58] O
datasets. O
The O
met- O
rics O

- B-DAT
ing O
a O
wide O
range O
of O

- B-DAT
age, O
the O
model O
synthesizes O
a O

- B-DAT

- B-DAT

- B-DAT
compression O
artifact O
reduction O
and O
single O

- B-DAT
olution. O
We O
demonstrate O
that O
our O

- B-DAT
performs O
state-of-the-art O
methods O
on O
all O

- B-DAT

- B-DAT
terpretable, O
which O
we O
demonstrate O
by O

- B-DAT

- B-DAT
gineering O
trade-offs O
entail O
that O
consumer O

- B-DAT

- B-DAT
ited O
in O
resolution O
and O
further O

- B-DAT
sion O
artifacts O
introduced O
for O
the O

- B-DAT
sion O
and O
storage. O
Scientific O
applications O

- B-DAT
tions O
of O
light, O
lenses O
and O

- B-DAT

- B-DAT
ments O
has O
been O
a O
long-standing O

- B-DAT

- B-DAT

- B-DAT
matically O
as O
inverse O
problems O
[48 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
porary O
techniques O
to O
inverse O
problems O

- B-DAT
ularization O
techniques O
which O
are O
amenable O

- B-DAT

- B-DAT

- B-DAT
tion O
of O
filter O
flow O
introduced O

- B-DAT
age O
are O
linearly O
combined O
to O

- B-DAT

- B-DAT

- B-DAT
ing O
an O
appropriately O
regularized/constrained O
flow O

- B-DAT
ages, O
we O
focus O
on O
applications O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lems, O
yielding O
state-of-the-art O
performance O
for O

- B-DAT

- B-DAT
resolution. O
Given O
a O
corrupted O
input O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
niques O
that O
learn O
to O
predict O

- B-DAT
labeled O
video O
data O
[15, O
16 O

- B-DAT
struction O
tasks O
we O
consider O
such O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gression O
models O
makes O
it O
hard O

- B-DAT
bustness O
in O
the O
presence O
of O

- B-DAT
fer O
reliability O
needed O
for O
researchers O

- B-DAT
based O
approaches O
in O
this O
regard O

- B-DAT
ing O
an O
explicit O
description O
of O

- B-DAT

- B-DAT

- B-DAT
ments O
on O
three O
different O
low-level O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ous O
low-level O
image O
reconstruction O
tasks O

- B-DAT

- B-DAT
tally O
that O
predictive O
filter O
flow O

- B-DAT

- B-DAT

- B-DAT
art O
methods O
remarkably O
on O
the O

- B-DAT
uniform O
motion O
blur O
removal, O
compression O

- B-DAT
tion O
and O
single O
image O
super-resolution O

- B-DAT
posing O
additional O
constraints O
on O
certain O

- B-DAT
standing O
a O
wide O
variety O
of O

- B-DAT

- B-DAT
ever, O
filter O
flow O
as O
originally O

- B-DAT

- B-DAT
mal O
filter O
flow O
is O
compute O

- B-DAT
ing O
spatially O
variant O
filtering O
over O

- B-DAT

- B-DAT
tremely O
challenging O
yet O
practically O
significant O

- B-DAT
moving O
blur O
caused O
by O
object O

- B-DAT
cally O
at O
patch O
level, O
and O

- B-DAT
formation O
about O
smooth O
motion O
by O

- B-DAT
fine O
discretized O
set O
of O
linear O

- B-DAT
invertible) O
transforms O
[51], O
i.e., O
downsampling O

- B-DAT
noising O
problems O
[6, O
20]. O
Recent O

- B-DAT

- B-DAT
resolution O
image O
from O
a O
single O

- B-DAT

- B-DAT

- B-DAT
tions O
exists O
for O
any O
given O

- B-DAT

- B-DAT
ods O
adopt O
an O
example-based O
strategy O

- B-DAT
timization O
solver, O
others O
are O
based O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
torized O
version O
of O
the O
source O

- B-DAT
volution O
corresponds O
to O
T O
being O

- B-DAT

- B-DAT
tice, O
particularly O
when O
the O
filters O

- B-DAT
tially O

- B-DAT
ing O
such O
a O
function O
fw O

- B-DAT
ence O
between O
a O
recovered O
image O

- B-DAT
sured O
by O
some O
loss O

- B-DAT

- B-DAT

- B-DAT
tural O
constraint O
that O
each O
output O

- B-DAT

- B-DAT
uct O
of O
this O
vector O
with O

- B-DAT

- B-DAT
ture O
activations O
at O
a O
single O

- B-DAT
tures O
[27, O
45, O
17 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uniform O
motion O
blur O
removal, O
meaning O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
suming O
there O
is O
no O
lighting O

- B-DAT
dicted O
filter O
weights. O
For O
other O

- B-DAT

- B-DAT

- B-DAT
cally O
without O
manual O
labeling. O
Given O

- B-DAT
ity O
images, O
we O
can O
automatically O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work O
with O
3×3 O
convolutional O
layers O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mation, O
while O
the O
second O
stream O

- B-DAT
volution O
layer O
and O
ReLU O
layer O

- B-DAT
ter O
Flow O
is O
self-supervised O
so O

- B-DAT
ited O
amount O
of O
image O
pairs O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
image O
regression O
CNNs, O
we O
also O

- B-DAT

- B-DAT
cients O

- B-DAT

- B-DAT
ary O
effects O
seen O
during O
training O

- B-DAT

- B-DAT
ing O
[24], O
with O
initial O
learning O

- B-DAT

- B-DAT
tensities. O
We O
train O
our O
model O

- B-DAT
dred O
epochs2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sections, O
respectively O

- B-DAT

- B-DAT
ing O
images. O
We O
evaluate O
each O

- B-DAT
uniform O
motion O
blur O
removal O
over O

- B-DAT
pression O
artifacts O
reduction, O
and O
Set5 O

- B-DAT

- B-DAT
Signal-to-Noise-Ratio O
(PSNR) O
and O
Structural O
Similarity O

- B-DAT
dex O
(SSIM) O
[52] O
over O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uniform O
motion O
blur O
dataset O
[2 O

- B-DAT
defined O
set O
of O
blur O
kernels O

- B-DAT

- B-DAT

- B-DAT
art O
methods O
over O
the O
released O

- B-DAT
els O
based O
on O
the O
proposed O

- B-DAT

- B-DAT
puts O
the O
quality O
images O
directly O

- B-DAT

- B-DAT
terpart O
based O
on O
the O
learned O

- B-DAT

- B-DAT
tecture O
is O
higher O
fidelity O
than O

- B-DAT

- B-DAT
mize O
artifacts, O
e.g., O
aliasing O
and O

- B-DAT
sults O
in O
Fig. O
2, O
along O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
provements O
when O
training O
models O
to O

- B-DAT
pixel O
kernels. O
This O
suggests O
that O

- B-DAT
put O
to O
the O
model. O
However O

- B-DAT
provement O
with O
additional O
iterations O
(results O

- B-DAT
ciently O
different O
than O
the O
blurred O

- B-DAT
lution O
could O
be O
inserting O
adversarial O

- B-DAT

- B-DAT

- B-DAT
ate O
JPEG O
compressed O
image O
patches O

- B-DAT
compressed O
ones O
on O
the O
fly O

- B-DAT
light O
areas O
where O
most O
apparent O

- B-DAT

- B-DAT
tion O
over O
LIVE1 O
dataset O
[52 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
compressed O
with O
a O
range O

- B-DAT
solute O
measurements O
by O
PSNR O
and O

- B-DAT
izable O
and O
stable O
performance. O
Basically O

- B-DAT
ages O
compressed O
with O
the O
same O

- B-DAT
pression O
quality O
factors O
measured O
by O

- B-DAT
inal O
JPEG O
compression O
is O
plotted O

- B-DAT

- B-DAT

- B-DAT
tions O

- B-DAT
tween O
CNN O
and O
PFF. O
The O

- B-DAT
duces O
the O
best O
visual O
quality O

- B-DAT
and O
low-frequency O
details O

- B-DAT

- B-DAT

- B-DAT
ages O
4× O
larger. O
To O
generate O

- B-DAT
inal O
image, O
we O
downsample O
14 O

- B-DAT

- B-DAT
sampled O
image O
from O
the O
low-resolution O

- B-DAT

- B-DAT

- B-DAT
aries O
and O
delivers O
an O
anti-aliasing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rics O
used O
here O
are O
PSNR O

- B-DAT
cially, O
the O
filter O
maps O
demonstrate O

- B-DAT

- B-DAT
depth O
understanding O

- B-DAT
dicted O
filter O
flows O
for O
different O

- B-DAT

- B-DAT

- B-DAT
tion O
along O
lines O
of O
different O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
troid O
for O
the O
filter O
at O

- B-DAT
put O
by O
our O
model O
generally O

- B-DAT
tage O
over O
other O
CNN-based O
models O

- B-DAT

- B-DAT

- B-DAT
diate O
layers O
of O
a O
CNN O

- B-DAT
fined O
semantics O
that O
can O
be O

- B-DAT
work O
generates O
space-variant O
per-pixel O
filters O

- B-DAT

- B-DAT
of-the-art O
methods O

- B-DAT

- B-DAT
els O
over O
patches, O
However, O
we O

- B-DAT

- B-DAT

- B-DAT
tions O
to O
reconstruct O
high-frequency O
detail O

- B-DAT

- B-DAT
entific O
applications O
such O
as O
medical O

- B-DAT
pared O
to O
physical O
models O
of O

-1618806 B-DAT

-1253538, B-DAT
DBI-1262547 O
and O
a O
hardware O
donation O

- B-DAT

- B-DAT

- B-DAT
ring O
by O
reblurring. O
In O
Proceedings O

- B-DAT
timation. O
In O
2017 O
12th O
IEEE O

- B-DAT

- B-DAT
ral O
computation, O
7(6):1129–1159, O
1995. O
1 O

- B-DAT
Morel. O
Low-complexity O
single-image O
super-resolution O
based O

- B-DAT
sion O
Conference, O
2012. O
4, O
7 O

- B-DAT

- B-DAT
nition, O
2005. O
CVPR O
2005. O
IEEE O

- B-DAT
ence O
on, O
volume O
2, O
pages O

- B-DAT

- B-DAT
tional O
Joint O
Conference O
on, O
pages O

- B-DAT

- B-DAT
tern O
Recognition, O
2004. O
CVPR O
2004 O

- B-DAT

- B-DAT
sion O
artifacts O
reduction O
by O
a O

- B-DAT
puter O
Vision, O
pages O
576–584, O
2015 O

- B-DAT

- B-DAT

- B-DAT
ence O
of O
interpretable O
machine O
learning O

- B-DAT
adaptive O
dct O
for O
high-quality O
denoising O

- B-DAT
cue. O
In O
European O
Conference O
on O

- B-DAT

- B-DAT
ing O
for O
image O
recognition. O
In O

- B-DAT
ference O
on O
computer O
vision O
and O

- B-DAT
tional O
neural O
networks O
for O
direct O

- B-DAT
ings O
of O
BMVC, O
volume O
10 O

- B-DAT
lutional O
networks. O
In O
Advances O
in O

- B-DAT
cessing O
Systems, O
pages O
769–776, O
2009 O

- B-DAT
sics: O
Unsupervised O
learning O
of O
optical O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
for O
parsimonious O
pixel O
labeling O

- B-DAT
spective O
understanding O
in O
the O
loop O

- B-DAT
tion O
(CVPR), O
2018. O
11 O

- B-DAT
ples O
in O
the O
physical O
world O

- B-DAT

- B-DAT

- B-DAT
tive O
adversarial O
network. O
In O
CVPR O

- B-DAT
tation. O
In O
Proceedings O
of O
the O

- B-DAT
czewicz. O
Adaptive O
deblocking O
filter. O
IEEE O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uating O
segmentation O
algorithms O
and O
measuring O

- B-DAT
ings. O
Eighth O
IEEE O
International O
Conference O

-10 B-DAT

- B-DAT

- B-DAT
cessing O
magazine, O
20(3):21–36, O
2003. O
1 O

- B-DAT

- B-DAT
tion O
(CVPR), O
2009. O
1, O
2 O

- B-DAT

- B-DAT
blurring O
from O
a O
single O
image O

- B-DAT
ics O
(tog), O
volume O
27, O
page O

- B-DAT
niques O
for O
compression O
artifact O
removal O

- B-DAT

- B-DAT
tional O
neural O
network O
for O
non-uniform O

- B-DAT
sion O
artifacts O
removal O
using O
convolutional O

- B-DAT
borhood O
regression O
for O
fast O
example-based O

- B-DAT

- B-DAT

- B-DAT
ing O
machine O
learning O
models O
interpretable O

- B-DAT
ume O
12, O
pages O
163–172. O
Citeseer O

- B-DAT
celli. O
Image O
quality O
assessment: O
from O

- B-DAT
ing, O
13(4):600–612, O
2004. O
4, O
6 O

- B-DAT

- B-DAT
puter O
vision, O
98(2):168–186, O
2012. O
3 O

- B-DAT
tation O
for O
natural O
image O
deblurring O

- B-DAT
tion, O
pages O
1107–1114, O
2013. O
5 O

- B-DAT

- B-DAT
resolution: O
A O
benchmark. O
In O
European O

- B-DAT
puter O
Vision, O
pages O
372–386. O
Springer O

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT

- B-DAT

- B-DAT
ual O
dense O
network O
for O
image O

- B-DAT

- B-DAT
alizations O
to O
understand O
the O
predicted O

- B-DAT
blurred O
image O
to O
the O
same O

- B-DAT

- B-DAT
sults O
for O
all O
the O
three O

- B-DAT

- B-DAT
nents O
by O
PCA O
shown O
in O

- B-DAT
ize O
the O
per-pixel O
loading O
factors O

- B-DAT
cipal O
component. O
We O
run O
PCA O

- B-DAT
pixel O
loading O
factors O
as O
a O

- B-DAT
sualization O
technique, O
we O
can O
know O

- B-DAT
ergy O
(stated O
in O
the O
main O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
terested O
in O
studying O
if O
we O

- B-DAT
eratively O
running O
the O
model, O
i.e O

- B-DAT
age O
as O
input O
to O
the O

- B-DAT
prisingly, O
we O
do O
not O
observe O

- B-DAT
ing O
explicitly O
with O
recurrent O
loops O

- B-DAT

- B-DAT

- B-DAT
tively. O
From O
these O
comparisons O
and O

- B-DAT

- B-DAT

- B-DAT
in O

- B-DAT
tion O
to O
and O
how O
it O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

the O
benchmark O
datasets O
Set5 O
[5], O
Set14 B-DAT
[40], O
BSD100 O
[1], O
Urban100 O
[17 O

refer O
to O
4× O
results O
of O
Set14 B-DAT

Set5 O
Set14 B-DAT
B100 O
U100 O
DIV2K O
average O

model O
B100 O
Set14 B-DAT

the O
problem O
of O
estimating O
an O
upscaling B-DAT
function O
u O
: O
X O

progressive O
solution O
to O
learn O
the O
upscaling B-DAT
function O
u. O
In O
the O
following O

where O
ϕ2 O
denotes O
an O
upscaling B-DAT
operator O
by O
a O
factor O
of O

in O
the O
challenge O
target O
4× O
upscaling B-DAT
but O
consider O
unknown O
degradation. O
Given O

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

Set5 O
Set14 B-DAT

model O
B100 O
Set14 B-DAT
2× I-DAT
4× O
8× O
2× O
4 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
have O
achieved O
impressive O
results O

- B-DAT
ditional O
error O
measures O
and O
perceptual O

- B-DAT
sults O
for O
large O
upsampling O
factors O

- B-DAT

- B-DAT
lows O
to O
scale O
well O
to O

- B-DAT

- B-DAT
taneously. O
In O
particular O
ProSR O
ranks O

- B-DAT
lenge O
[34]. O
Compared O
to O
the O

- B-DAT

- B-DAT
cessing O
has O
recently O
sparked O
increased O

- B-DAT
resolution. O
In O
particular, O
approaches O
to O

- B-DAT

- B-DAT
resolution O
(HR) O
images O
based O
on O

- B-DAT
scaling O
function O
is O
a O
deep O

- B-DAT
lowing O
direct O
approaches. O
The O
first O

- B-DAT
ginning O
and O
then O
essentially O
learns O

- B-DAT

- B-DAT
nation O
of O
upsampling O
layers. O
Thus O

- B-DAT

- B-DAT
istic O
results, O
we O
adopt O
the O

- B-DAT
tain O
a O
multi-scale O
generator O
with O

- B-DAT
riculum O
learning, O
which O
is O
known O

- B-DAT
pling O
factors) O
to O
hard O
(large O

- B-DAT

- B-DAT
egy O
not O
only O
improves O
results O

- B-DAT
lizes O
the O
GAN O
training O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cally O
been O
tackled O
using O
statistical O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cently, O
Dong O
et O
al. O
[6 O

- B-DAT

- B-DAT
ing O
techniques. O
Since O
then, O
deep O

- B-DAT

- B-DAT

- B-DAT
struction O
techniques O
[7, O
20, O
23 O

- B-DAT
age O
to O
the O
desired O
spatial O

- B-DAT
processing O
step. O
Thus, O
the O
CNN O

- B-DAT

- B-DAT
putationally O
expensive O
[30]. O
To O
overcome O

- B-DAT

- B-DAT
scribed O
by O
LapSRN O
by O
Lai O

- B-DAT
sampling O
follows O
the O
principle O
of O

- B-DAT
puted O
at O
each O
scale, O
this O

- B-DAT
pervision. O
Lai O
et O
al. O
improved O

- B-DAT

- B-DAT
erable O
gap O
between O
the O
top-performing O

- B-DAT
tion O
difficulty. O
Furthermore, O
the O
recursive O

- B-DAT
criminator O
along O
with O
a O
progressive O

- B-DAT
niques O
optimize O
the O
reconstruction O
error O

- B-DAT

- B-DAT
construction O
errors, O
they O
are O
unable O

- B-DAT
ally O
plausible O
high-frequencies O
details. O
To O

- B-DAT
versary O
to O
steer O
the O
reconstruction O

- B-DAT
ifold O
of O
natural O
solutions. O
Based O

- B-DAT
riculum O
learning. O
With O
this, O
our O

- B-DAT
sample O
perceptually O
pleasing O
SR O
images O

- B-DAT

- B-DAT

- B-DAT
scaling O
function O
u O
for O
large O

- B-DAT
ing: O
the O
larger O
the O
ratio O

- B-DAT

- B-DAT
resolution O
in O
Section O
3.1 O
and O

- B-DAT

- B-DAT
forming O
a O
2× O
upsampling O
of O

- B-DAT

- B-DAT
sign O
more O
DCUs O
in O
the O

- B-DAT
sumption O
but O
also O
increases O
the O

- B-DAT
ric O
variant O
in O
terms O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ture O
space. O
A O
schematic O
illustration O

- B-DAT
sampling O
architecture O
is O
detailed O
in O

- B-DAT
mid O
principle O
like O
in O
[21 O

- B-DAT

- B-DAT

- B-DAT
fied O
densely O
connected O
block O
followed O

- B-DAT

- B-DAT
CONV(1,1)-BN-RELU-CONV(3,3). O
Following O
recent O
practice O
in O

- B-DAT

-1 B-DAT

- B-DAT
CONV(3,3 O

- B-DAT

- B-DAT

- B-DAT
agation O
as O
shown O
in O
Figure O

- B-DAT
cess O
at O
applying O
GANs O
to O

- B-DAT
scale O
upsampling O
at O
relatively O
low O

- B-DAT
der O
to O
enable O
multi-scale O
GAN-enhanced O

- B-DAT
mension O
of O
the O
input O
image O

- B-DAT
modate O
the O
multi-scale O
outputs O
from O

- B-DAT
work O
is O
fully O
convolutional O
and O

- B-DAT
tures O
similar O
to O
PatchGAN O
[18 O

- B-DAT
erates O
on O
the O
residual O
between O

- B-DAT
sampled O
image. O
This O
allows O
both O

- B-DAT
nator O
to O
concentrate O
only O
on O

- B-DAT
ation O
which O
are O
not O
already O

- B-DAT
tual O
errors. O
This O
can O
also O

- B-DAT
dependent O
baseline O
from O
the O
discriminator O

- B-DAT

- B-DAT

- B-DAT
rent O
scale O
(u0, O
v0, O
r0 O

- B-DAT
midal O
network O
shown O
in O
Figure O

- B-DAT
vious O
level. O
A O
similar O
idea O

- B-DAT
sult O
we O
incrementally O
add O
training O

- B-DAT

- B-DAT
formance O
gain O
for O
all O
included O

- B-DAT
scale O
and O
simple O
multi-scale O
training O

- B-DAT
ities O
in O
GAN O
training O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
posed O
components O
using O
a O
small O

- B-DAT

- B-DAT

- B-DAT
tions O
are O
conducted O
on O
the O

- B-DAT
struction O
quality O
stemming O
from O
each O

- B-DAT

- B-DAT
nection O
from O
the O
LR O
input O

- B-DAT
ing, O
we O
describe O
the O
individual O

- B-DAT

- B-DAT
vantage O
of O
the O
proposed O
asymmetric O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ric O
pyramid O
model O
to O
8 O

- B-DAT
efit O
of O
curriculum O
learning O
over O

- B-DAT

- B-DAT
struction O
quality O
and O
outperforms O
simultaneous O

- B-DAT
neous O
training, O
since O
the O
2 O

- B-DAT
tion O
and O
hence O
less O
time O

- B-DAT
tures O

- B-DAT
pled O
LR O
image. O
Thus O
the O

- B-DAT
ble O
3. O
Therefore, O
we O
conclude O

- B-DAT

- B-DAT
pared O
to O
fixed O
interpolated O
results O

- B-DAT
sampling O
kernel O
to O
create O
the O

- B-DAT
duce O
undesired O
artefacts O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
proaches O

- B-DAT
son, O
we O
benchmark O
against O
VDSR O

- B-DAT
SRN O
[21], O
MsLapSRN O
[22], O
EDSR O

- B-DAT

simultaneous O
-0 B-DAT

- B-DAT

ours O
- B-DAT
27.44 O
- O
- O
28.41 O
- O
alt O
- O
27.32 O

- B-DAT

- B-DAT
proaches, O
we O
divide O
them O
into O

- B-DAT
ingly, O
we O
provide O
two O
models O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tween O
our O
results O
and O
the O

- B-DAT

- B-DAT

- B-DAT
imise O
the O
ℓ1 O
loss O
or O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mental O
material O

- B-DAT
erage O
0.8s, O
2.1s O
and O
4.4s O

- B-DAT

- B-DAT

- B-DAT
scaling, O
where O
the O
low O
resolution O

- B-DAT
idation O
set. O
Our O
model O
ranks O

- B-DAT

- B-DAT
ferent O
to O
the O
bicubic O
8 O

- B-DAT
nario, O
we O
also O
participated O
in O

- B-DAT

- B-DAT
ther O
improvement O
can O
be O
achieved O

- B-DAT
ing O
and O
extended O
training O
data O

- B-DAT
eling O
power, O
we O
have O
proposed O

- B-DAT
ity. O
Furthermore O
we O
leverage O
a O

- B-DAT

- B-DAT
formance O
for O
all O
scales. O
Our O

- B-DAT

- B-DAT
the-art O
benchmark O
in O
terms O
of O

- B-DAT
performs O
existing O
methods O
by O
a O

- B-DAT
plied O
to O
GAN-extended O
method O
for O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
scale O
model O
that O
could O
yield O

24.57 O
24.65 O
22.06 O
26.52 O
SRDenseNet O
- B-DAT
- O
- O
- O
28.50 O
27.53 O

26.05 O
- B-DAT
- O
- O
- O
ProSRs O
(ours) O
33.36 O
32.02 O
31.42 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cup O
and O
Y. O
W. O
Teh O

- B-DAT
national O
Conference O
on O
Machine O
Learning O

- B-DAT
riculum O
learning. O
In O
Proceedings O
of O

- B-DAT
tional O
conference O
on O
machine O
learning O

- B-DAT
Morel. O
Low-complexity O
single-image O
super-resolution O
based O

super-resolution. B-DAT
In O
Com- O
puter O
Vision O
- O
ECCV O
2014 O
- O
13th O
European O

-12, B-DAT
2014, O
Proceedings, O
Part O
IV, O
pages O

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
ference O
on O
Computer O
Vision, O
pages O

- B-DAT

- B-DAT
works O
for O
image O
super-resolution. O
In O

- B-DAT
ference O
on, O
pages O
1157–1164. O
IEEE O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-8, B-DAT
2013, O
pages O
3336–3343, O
2013. O
2 O

- B-DAT

- B-DAT
based O
super-resolution. O
IEEE O
Computer O
Graphics O

- B-DAT
plications, O
22(2):56–65, O
2002. O
2 O

- B-DAT

2009, O
Kyoto, O
Japan, O
September O
27 O
- B-DAT
October O
4, O
2009, O
pages O
349–356 O

- B-DAT

- B-DAT

- B-DAT
ing O
for O
image O
recognition. O
In O

- B-DAT
ference O
on O
computer O
vision O
and O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
to-image O
translation O
with O
conditional O
adversarial O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT
resolution. O
In O
IEEE O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ative O
adversarial O
network. O
arXiv O
preprint O

- B-DAT

- B-DAT
ley. O
Least O
squares O
generative O
adversarial O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pling. O
ACM O
Trans. O
Graph., O
27(5):153:1–153:7 O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

-26 B-DAT
June O
2008, O
Anchorage, O
Alaska, O
USA O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT
shops, O
July O
2017. O
1, O
5 O

- B-DAT
borhood O
regression O
for O
fast O
example-based O

- B-DAT

-8, B-DAT
2013, O
pages O
1920–1927, O
2013. O
2 O

- B-DAT

- B-DAT
resolution O
as O
sparse O
representation O
of O

-26 B-DAT
June O
2008, O
Anchorage, O
Alaska, O
USA O

- B-DAT
age O
super-resolution O
with O
a O
parameter O

- B-DAT

- B-DAT
up O
using O
sparse-representations. O
In O
Curves O

and O
Surfaces O
- B-DAT
7th O
International O
Conference, O
Avignon, O
France O

-30, B-DAT
2010, O
Revised O
Selected O
Papers, O
pages O

- B-DAT

- B-DAT

standard O
benchmark O
datasets: O
Set5 O
[36], O
Set14 B-DAT
[37], O
B100 O
[38], O
Urban100 O
[22 O

Method O
Scale O
Set5 O
Set14 B-DAT
B100 O
Urban100 O
Manga109 O

Method O
Scale O
Set5 O
Set14 B-DAT
B100 O
Urban100 O
Manga109 O

the O
original O
LR O
inputs O
and O
upscaling B-DAT
spatial O
reso- O
lution O
at O
the O

upscaling B-DAT
strategy O
has O
been O
demon- O
strated O

upscaling B-DAT
SR O
methods O
(e.g., O
DRRN O
[5 O

shallow O
feature O
extraction O
HSF O
(·), O
upscaling B-DAT
module O
HUP O
(·), O
and O
reconstruction O

upscaling B-DAT
layer, O
whose O
weight O
set O
is O

upscaling, B-DAT
whose O
kernel O
size O
is O
1×1 O

is O
set O
as O
16. O
For O
upscaling B-DAT
module O
HUP O
(·), O
we O
follow O

Method O
Scale O
Set5 O
Set14 B-DAT

Method O
Scale O
Set5 O
Set14 B-DAT

- B-DAT

- B-DAT
portance O
for O
image O
super-resolution O
(SR O

- B-DAT
resolution O
inputs O
and O
features O
contain O

- B-DAT

- B-DAT
tion, O
which O
is O
treated O
equally O

- B-DAT
resentational O
ability O
of O
CNNs. O
To O

- B-DAT
tions. O
Meanwhile, O
RIR O
allows O
abundant O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
given O
its O
low-resolution O
(LR O

- B-DAT

- B-DAT
tions, O
ranging O
from O
security O
and O

- B-DAT

- B-DAT
merous O
learning O
based O
methods O
have O

- B-DAT
layer O
CNN O
for O
image O
SR O

- B-DAT

- B-DAT
work O
depth O
was O
demonstrated O
to O

- B-DAT
nition O
tasks, O
especially O
when O
He O

- B-DAT

- B-DAT

- B-DAT
wise O
features O
equally, O
which O
lacks O

- B-DAT
formation O
(e.g., O
low- O
and O
high-frequency O

- B-DAT

- B-DAT
sible. O
The O
LR O
images O
contain O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ual O
channel O
attention O
networks O
(RCAN O

- B-DAT

- B-DAT
ture O
to O
construct O
very O
deep O

- B-DAT
tions O
in O
RIR O
help O
to O

- B-DAT

- B-DAT
tional O
ability O
of O
the O
network O

- B-DAT
nity O
[1–11,22]. O
Attention O
mechanism O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vious O
works. O
By O
introducing O
residual O

- B-DAT
icant O
improvement O
in O
accuracy. O
Tai O

- B-DAT
lution O
at O
the O
network O
tail O

- B-DAT

- B-DAT
bines O
automated O
texture O
synthesis O
and O

- B-DAT
gree, O
their O
predicted O
results O
may O

- B-DAT
cant O
improvement. O
However, O
most O
of O

- B-DAT

- B-DAT
inative O
ability O
for O
different O
types O

-1 B-DAT
RG-g O
RG-G O

- B-DAT

-1 B-DAT
RCAB-b O
RCAB-B O

- B-DAT

- B-DAT
fication O
with O
a O
trunk-and-mask O
attention O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tain O
significant O
performance O
improvement O
for O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
posed O
RIR O
achieves O
the O
largest O

- B-DAT

- B-DAT
tively O

- B-DAT

- B-DAT

- B-DAT
strated O
to O
be O
more O
efficient O

- B-DAT

- B-DAT
ial O
losses O
[8, O
21]. O
To O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
frequency O
parts O
seem O
to O
be O

- B-DAT

- B-DAT
quently, O
the O
output O
after O
convolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
by O
global O
average O
pooling O

- B-DAT
wise O
features O
can O
be O
emphasized O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tively. O
WD O
is O
the O
weight O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
Fg,bXg,b O

- B-DAT
hance O
the O
discriminative O
ability O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
table O
performance O
improvements O
over O
previous O

- B-DAT
sions O
about O
the O
effects O
of O

- B-DAT
downscaling O
and O
channel-upscaling, O
whose O
kernel O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ation O
metric, O
and O
training O
settings O

- B-DAT

-1 B-DAT
and O
top-5 O
recognition O
errors) O
comparisons O

- B-DAT

- B-DAT
tion O
(CA) O
based O
on O
the O

- B-DAT
formance. O
It’s O
hard O
to O
obtain O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
CNN O
[2], O
SCN O
[3], O
VDSR O

- B-DAT

- B-DAT
ensemble O
strategy O
to O
further O
improve O

- B-DAT

- B-DAT
parisons O
for O
×2, O
×3, O
×4 O

- B-DAT

- B-DAT

- B-DAT
age O
“img O
004”, O
we O
observe O

- B-DAT
CNN O
cannot O
recover O
lines. O
Other O

- B-DAT
Cooking”, O
the O
cropped O
part O
is O

- B-DAT
ful O
representational O
ability O
can O
extract O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults O
with O
7 O
state-of-the-art O
methods O

- B-DAT

- B-DAT
ing O
details O
in O
images O
“img O

- B-DAT
tive O
components. O
These O
comparisons O
indicate O

- B-DAT

-1 B-DAT
error O
0.506 O
0.477 O
0.437 O
0.454 O

-5 B-DAT
error O
0.266 O
0.242 O
0.196 O
0.224 O

- B-DAT

- B-DAT

-50 B-DAT
[20] O
as O
the O
evaluation O
model O

- B-DAT

- B-DAT
idation O
dataset O
for O
evaluation. O
The O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
and O
top-5 O
errors. O
These O
comparisons O

- B-DAT

- B-DAT
nections, O
making O
the O
main O
network O

- B-DAT

- B-DAT

- B-DAT
terdependencies O
among O
channels. O
Extensive O
experiments O

-14 B-DAT

-1 B-DAT

-0484, B-DAT
and O
U.S. O
Army O
Research O
Office O

-17 B-DAT

-1 B-DAT

-0367 B-DAT

- B-DAT

- B-DAT

- B-DAT
lutional O
networks. O
TPAMI O
(2016 O

- B-DAT

- B-DAT
resolution O
with O
sparse O
prior. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
with O
deep O
laplacian O
pyramid O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In: O
CVPR. O
(2018 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
chines. O
In: O
ICML. O
(2010 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
representations. O
In: O
Proc. O
7th O
Int O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

standard O
benchmark O
datasets: O
Set5 O
[1], O
Set14 B-DAT
[30], O
B100 O
[21], O
Urban100 O
[11 O

Set14 B-DAT
×2 O
30.24/0.8688 O
32.45/0.9067 O
33.05/0.9130 O
33.23/0.9136 O

from O
Set14 B-DAT

from O
Set14 B-DAT
HR/PSNR I-DAT
Bicubic/21.69 O
VDSR/23.02 O
DRRN/23.14 O
NLRN/23.15 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formation O
flows O
are O
solely O
feedforward O

- B-DAT

- B-DAT
plored. O
In O
this O
paper, O
we O

- B-DAT
rate O
image O
SR, O
in O
which O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tures O
captured O
under O
large O
receptive O

- B-DAT

- B-DAT
ciently O
selects O
and O
further O
enhances O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-the-art O
SR O
methods O
in O
terms O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
image O
SR O
methods O

- B-DAT

- B-DAT
tures O
solely O
flow O
from O
the O

- B-DAT

- B-DAT
tures O
extracted O
from O
the O
top O

- B-DAT

- B-DAT
agating O
high-level O
features O
to O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
features, O
we O
employ O
multiple O

- B-DAT

- B-DAT
tures O
to O
shallow O
layers. O
However O

- B-DAT

- B-DAT

- B-DAT
level O
information O
to O
refine O
low-level O

- B-DAT

- B-DAT

- B-DAT
posed O
GMFN O
shows O
better O
visual O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tensive O
experiments O
demonstrate O
the O
superiority O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ploys O
16 O
RDBs O

- B-DAT
level O
features O
for O
refining O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
tasks O
(e.g. O
classification O
[29 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ent O
direction, O
[12, O
31] O
applied O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
less, O
we O
argue O
that O
such O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ent O
receptive O
fields, O
every O
piece O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
back O
module O
to O
adaptively O
eliminate O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
able O
contextual O
knowledge O
from O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
cost O
was O
quadratically O
saved O

- B-DAT
gether. O
However, O
these O
networks O
require O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pendent O
convolutional O
neural O
network O
which O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
jacent O
time O
steps O
is O
achieved O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
RDB-BRDB O

- B-DAT

-1 B-DAT
RDB-b O
RDB-B O

- B-DAT

- B-DAT

L O
BF O
-, B-DAT

L O
bF O
- B-DAT
, O
t O

- B-DAT

- B-DAT

- B-DAT
tures O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
RDN O
[33], O
the O
number O

- B-DAT

- B-DAT
volutional O
layer. O
Then, O
a O
3×3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lowing O
RDB. O
The O
placement O
of O

- B-DAT
cording O
to O
the O
relative O
hierarchical O

- B-DAT

- B-DAT

- B-DAT
cilitates O
the O
refinement O
processes O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lected O
indexes O
of O
the O
deepest O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ative O
hierarchical O
relationship O
among O
multiple O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
information O
captured O
under O
different O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
network O
is O
set O
to O
C0 O

- B-DAT
ment O
training O
images O
with O
scaling O

- B-DAT
tion O
bicubic. O
The O
SR O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formance O
of O
various O
single-to-multiple O
anti-feedback O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dex O
sets O
SM O
and O
DN O

- B-DAT

- B-DAT

- B-DAT
to-multiple O
feedback O
manner O
[12, O
31 O

- B-DAT
to-single O
feedback O
manners O
perform O
better O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fining O
low-level O
features. O
However, O
excessively O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
features. O
If O
the O
high-level O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trate O
the O
effectiveness O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bine O
various O
M O
to O
achieve O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ther O
hinder O
the O
reconstruction O
ability O

- B-DAT
tively O
selects O
the O
high O
frequency O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tively O
accesses O
to O
high-level O
information O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nections O
by O
setting O
M O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
son O
results2 O
in O
Tab. O
1 O

- B-DAT
struct O
a O
faithful O
SR O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ogy O
Department O
(No.2018GZ0178 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ding. O
In O
BMVC, O
2012 O

- B-DAT
volutional O
network O
for O
image O
super-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
projection O
networks O
for O
super-resolution. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
back O
network O
for O
image O
super-resolution O

- B-DAT

- B-DAT
layer O
recurrent O
connections O
for O
scene O

- B-DAT

- B-DAT

- B-DAT
masaki, O
and O
Kiyoharu O
Aizawa. O
Sketch-based O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
forward O
networks O
(FF O

-1 B-DAT

- B-DAT
ers O
are O
set O
to O
128 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults O
shown O
Figure O
6 O
indicate O

- B-DAT
tively. O
The O
performance O
evaluated O
on O

- B-DAT
served O
that O
with O
the O
help O

- B-DAT
nificantly O
improved O
compared O
with O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods, O
but O
it O
holds O
relatively O

-16, B-DAT
we O
provide O
more O
qualitative O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rately O
restored O
the O
letter O
"M O

- B-DAT

- B-DAT
coveres O
two O
horizontal O
lines O
as O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

DBPN-L O
on O
the O
Set5 O
and O
Set14, B-DAT
respectively. O
On O
8×, O
the O
gaps O

DBPN-L O
on O
the O
Set5 O
and O
Set14, B-DAT
respec- O
tively O

Set5 O
Set14 B-DAT

using O
5 O
datasets: O
Set5 O
[2], O
Set14 B-DAT
[50], O
BSDS100 O
[1], O
Urban100 O
[16 O

has O
different O
characteris- O
tics. O
Set5, O
Set14 B-DAT
and O
BSDS100 O
consist O
of O
natural O

Set5 O
Set14 B-DAT
BSDS100 O
Urban100 O
Manga109 O

Set5 O
Set14 B-DAT
Algorithm I-DAT
Scale O
PSNR O
SSIM O
PSNR O

has O
different O
characteris- O
tics. O
Set5, O
Set14 B-DAT

Set5 O
Set14 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
images. O
We O
propose O

- B-DAT

- B-DAT
and O
down- O
sampling O
layers, O
providing O

- B-DAT
connected O
up- O
and O
down-sampling O
stages O

- B-DAT
resolution O
components. O
We O
show O
that O

- B-DAT
and O
down- O
sampling O
stages O
(Dense O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
proach O
is O
to O
construct O
an O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work O
[6, O
7, O
38, O
25 O

- B-DAT
ing O
with O
one O
or O
more O

- B-DAT
lution O
and O
finally O
construct O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
SRN O
[25] O
(15.25 O
dB), O
EDSR O

- B-DAT
fectively O
by O
one O
of O
the O

- B-DAT

- B-DAT
tion O
error O
then O
fuses O
it O

- B-DAT
fect O
[4]. O
Moreover, O
this O
method O

- B-DAT
erator, O
leading O
to O
variability O
in O

- B-DAT

- B-DAT

- B-DAT
and O
down- O
sampling: O
Deep O
Back-Projection O

- B-DAT
butions: O
(1) O
Error O
feedback. O
We O

- B-DAT
correcting O
feedback O
mechanism O
for O
SR O

- B-DAT
and O
down-projection O
errors O
to O
guide O

- B-DAT
tion O
for O
obtaining O
better O
results O

- B-DAT
and O
down-sampling O
stages. O
Feed-forward O
architectures O

- B-DAT
way O
mapping, O
only O
map O
rich O

- B-DAT

- B-DAT
(blue O
box) O
and O
down-sampling O
(gold O

- B-DAT
tures O
using O
upsampling O
layers O
but O

- B-DAT
(blue O
box) O
and O
down-sampling O
(gold O

- B-DAT
ity O
enables O
the O
networks O
to O

- B-DAT

- B-DAT
tion O
directly O
utilizes O
different O
types O

- B-DAT

- B-DAT

- B-DAT
and O
down-sampling O
stage O
to O
encourage O

- B-DAT

- B-DAT
tion O
as O
the O
upsampling O
operator O

- B-DAT
tion O
(MR) O
image. O
This O
schema O

- B-DAT
CNN O
[6] O
to O
learn O
MR-to-HR O

- B-DAT

- B-DAT
ple O
convolutional O
layers. O
Later, O
the O

- B-DAT
ploited O
residual O
learning O
[22, O
43 O

- B-DAT
posed O
by O
FSRCNN O
[7] O
and O

- B-DAT
tion O
and O
replace O
predefined O
operators O

- B-DAT
longs O
to O
this O
type. O
However O

- B-DAT
portunities O
to O
propose O
lighter O
networks O

- B-DAT

- B-DAT
late O
the O
reconstruction O
error O
to O

- B-DAT
ables O
the O
networks O
to O
preserve O

- B-DAT
ing O
various O
up- O
and O
down-sampling O

- B-DAT
ating O
deeper O
features O

- B-DAT

- B-DAT
to-target O
space O
in O
one O
step O

- B-DAT
pose O
the O
prediction O
process O
into O

- B-DAT

- B-DAT
back O
procedure O
has O
been O
implemented O

- B-DAT
tion. O
PredNet O
[32] O
is O
an O

- B-DAT
tion, O
Li O
et O
al. O
[29 O

- B-DAT
back O
procedures O
have O
not O
been O

- B-DAT
ial O
Networks O
(GANs) O
[10] O
has O

- B-DAT
age O
reconstruction O
problems O
[28, O
37 O

- B-DAT

- B-DAT

- B-DAT
works. O
Ledig O
et O
al. O
[28 O

- B-DAT
ered O
as O
a O
single O
upsampling O

- B-DAT
ral O
image O
manifold O
that O
is O

- B-DAT

- B-DAT
ages O
by O
specifically O
formulating O
a O

- B-DAT

- B-DAT

- B-DAT
erative O
procedure O
to O
minimize O
the O

- B-DAT
projection O
[51, O
11, O
8, O
46 O

- B-DAT

- B-DAT
ror O
iteratively O
[4]. O
Timofte O
et O

- B-DAT
projection O
can O
improve O
the O
quality O

- B-DAT

- B-DAT
known. O
Most O
of O
the O
previous O

- B-DAT

- B-DAT

- B-DAT
and O
down-sampling O
stages O
to O
learn O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
projection O
unit O
projects O
it O
back O

- B-DAT
serve O
the O
HR O
components O
by O

- B-DAT
and O
down- O
sampling O
operators O
and O

- B-DAT
struct O
numerous O
LR O
and O
HR O

- B-DAT

- B-DAT

- B-DAT
end O
training O
of O
the O
SR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
spectively, O
the O
up- O
and O
down-sampling O

- B-DAT

- B-DAT

- B-DAT
and O
down-projection O
unit O
in O
the O

- B-DAT
nating O
between O
H O
and O
L O

- B-DAT
derstood O
as O
a O
self-correcting O
procedure O

- B-DAT
jection O
error O
to O
the O
sampling O

- B-DAT
sized O
filter O
is O
avoided O
because O

- B-DAT
gence O
speed O
and O
might O
produce O

- B-DAT

- B-DAT
ever, O
iterative O
utilization O
of O
our O

- B-DAT
works O

- B-DAT

- B-DAT
gradient O
problem, O
produce O
improved O
feature O

- B-DAT
age O
feature O
reuse. O
Inspired O
by O

- B-DAT

- B-DAT
mensional O
reduction O
[42, O
12] O
before O

- B-DAT

- B-DAT
and O
down-projection O
unit, O
re- O
spectively O

- B-DAT
ture O
maps O
effectively, O
as O
shown O

- B-DAT

- B-DAT
jection, O
and O
reconstruction, O
as O
described O

- B-DAT
and O
down-projection O
unit O
in O
the O

- B-DAT

- B-DAT
and O
down-projections O
units, O
respectively) O
are O

- B-DAT

- B-DAT
tures O
extraction O
and O
nR O
is O

- B-DAT

- B-DAT
traction O
is O
a O
sequence O
of O

- B-DAT

- B-DAT

- B-DAT
work O
architecture O
is O
modular. O
We O

- B-DAT
traction O
stage O
(2 O
layers), O
and O

- B-DAT

- B-DAT

- B-DAT
tion O
unit O
is O
various O
with O

- B-DAT
nally, O
the O
8× O
enlargement O
use O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puted O
by O

- B-DAT
volutional O
layers O
are O
followed O
by O

- B-DAT
tion.2 O
To O
produce O
LR O
images O

- B-DAT
ing O
Caffe, O
MATLAB O
R2017a O
on O

- B-DAT
tion. O
The O
input O
and O
output O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formance, O
S O
networks O
can O
achieve O

- B-DAT
SRN, O
respectively. O
The O
M O
network O

- B-DAT

- B-DAT

- B-DAT
tal, O
the O
M O
network O
use O

- B-DAT
SRN, O
and O
DRRN, O
respectively O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
largement. O
S O
(T O
= O
2 O

- B-DAT

- B-DAT
rameters O
on O
4× O
and O
8 O

- B-DAT
DBPN O
has O
about O
76% O
fewer O

- B-DAT

- B-DAT
dence O
show O
that O
our O
networks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tures O
generated O
from O
the O
projection O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tively O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pare O
our O
network O
with O
eight O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rithms: O
A+ O
[45], O
SRCNN O
[6 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tics. O
Set5, O
Set14 O
and O
BSDS100 O

- B-DAT

- B-DAT
vide O
each O
image O
in O
Urban100 O

- B-DAT

- B-DAT

- B-DAT
put O
image. O
It O
takes O
less O

- B-DAT
DBPN O
outperforms O
the O
existing O
methods O

- B-DAT

- B-DAT
vious O
statement O
is O
strengthened O
by O

- B-DAT
ban100 O
dataset O
which O
consist O
of O

- B-DAT

- B-DAT
ods O
by O
a O
large O
margin O

- B-DAT

- B-DAT

- B-DAT
ter O
than O
EDSR. O
The O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
which O
predict O
the O
SR O

- B-DAT

- B-DAT
tures O
using O
multiple O
up- O
and O

- B-DAT

- B-DAT

- B-DAT
and O
down-scaling O
steps O
to O
guide O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work O
successfully O
outperforms O
other O
state-of-the-art O

- B-DAT
ods O
on O
large O
scaling O
factors O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
Conference O
(BMVC), O
2012. O
6 O

- B-DAT
man O
pose O
estimation O
with O
iterative O

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT
projection O
for O
single O
image O
super O

- B-DAT
tive O
image O
models O
using O
a O

- B-DAT
tems, O
pages O
1486–1494, O
2015. O
1 O

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
ference O
on O
Computer O
Vision, O
pages O

- B-DAT
projection O
for O
adaptive O
image O
enlargement O

- B-DAT
cessing O
(ICIP), O
2009 O
16th O
IEEE O

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
nets. O
In O
Advances O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
for O
image O
recognition. O
arXiv O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision, O
pages O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
sion O
and O
Pattern O
Recognition O
(CVPR O

- B-DAT
istration. O
CVGIP: O
Graphical O
models O
and O

- B-DAT
ment: O
Resolution, O
occlusion, O
and O
transparency O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT
volutional O
network O
for O
image O
super-resolution O

- B-DAT
ings O
of O
the O
IEEE O
Conference O

- B-DAT
resolution. O
In O
IEEE O
Conferene O
on O

- B-DAT
tern O
Recognition, O
2017. O
1, O
2 O

- B-DAT
sion O
offered O
by O
feedforward O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
adversarial O
network. O
In O
IEEE O

- B-DAT
tation. O
In O
Proceedings O
of O
the O

- B-DAT
resolution O
via O
deep O
draft-ensemble O
learning O

- B-DAT

- B-DAT
ing O
networks O
for O
video O
prediction O

- B-DAT
masaki, O
and O
K. O
Aizawa. O
Sketch-based O

- B-DAT
ing O
manga109 O
dataset. O
Multimedia O
Tools O

- B-DAT
sentation O
learning O
with O
deep O
convolutional O

- B-DAT
sarial O
networks. O
arXiv O
preprint O
arXiv:1511.06434 O

- B-DAT

- B-DAT
tion. O
In O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT
back O
for O
faster O
r-cnn. O
In O

- B-DAT
tion, O
2017. O
1 O

- B-DAT

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT
nition O
Workshops O
(CVPRW), O
2017 O
IEEE O

- B-DAT

- B-DAT
prove O
example-based O
single O
image O
super O

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT
level O
vision O
tasks O
and O
3d O

- B-DAT
tural O
similarity. O
Image O
Processing, O
IEEE O

- B-DAT

- B-DAT

- B-DAT
erative O
projection O
reconstruction O
for O
fast O

standard O
benchmark O
datasets: O
Set5 O
[1], O
Set14 B-DAT
[33], O
B100 O
[18], O
Urban100 O
[8 O

Set14 B-DAT
×2 O
30.24/0.8688 O
32.45/0.9067 O
33.08/0.9130 O
33.23/0.9136 O

and O
SSIM O
results O
on O
Set5, O
Set14, B-DAT
B100, O
Urban100 O

Set14 B-DAT
BD O
26.38/0.7271 O
28.89/0.8105 O
28.80/0.8074 O
24.44/0.7106 O

layer O
in O
LR O
space O
for O
upscaling B-DAT

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

Set14 B-DAT

- B-DAT

- B-DAT
cently O
achieved O
great O
success O
for O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tract O
abundant O
local O
features O
via O

- B-DAT
tional O
layers. O
RDB O
further O
allows O

- B-DAT
taining O
dense O
local O
features, O
we O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
graded O
low-resolution O
(LR) O
measurement. O
SISR O

- B-DAT
lance O
imaging O
[42], O
medical O
imaging O

- B-DAT
eration O
[9]. O
While O
image O
SR O

- B-DAT

- B-DAT
cedure, O
since O
there O
exists O
a O

- B-DAT
based O
[40], O
reconstruction-based O
[37], O
and O

- B-DAT

- B-DAT

- B-DAT
ules, O
the O
networks O
for O
image O

- B-DAT
ory O
block O
was O
proposed O
to O

- B-DAT

- B-DAT
gles O
of O
view, O
and O
aspect O

- B-DAT
tion. O
While, O
most O
deep O
learning O

- B-DAT

- B-DAT
inal O
LR O
image O
to O
the O

- B-DAT
processing O
step O
not O
only O
increases O

- B-DAT
ing O
to O
our O
experiments O
(see O

- B-DAT
archical O
features O
from O
the O
original O

- B-DAT
posed O
residual O
dense O
block O
(Fig O

- B-DAT
tical O
for O
a O
very O
deep O

- B-DAT
ual O
dense O
block O
(RDB) O
as O

- B-DAT
sion O
(LFF) O
with O
local O
residual O

- B-DAT
catenating O
the O
states O
of O
preceding O

- B-DAT
ing O
layers O
within O
the O
current O

- B-DAT

- B-DAT
sion O
[15 O

- B-DAT

- B-DAT
work O
(RDN) O
for O
high-quality O
image O

- B-DAT
tiguous O
memory O
(CM) O
mechanism, O
but O

- B-DAT
lize O
all O
the O
layers O
within O

- B-DAT
tions. O
The O
accumulated O
features O
are O

- B-DAT
ods O
in O
computer O
vision O
[36 O

- B-DAT
ited O
space, O
we O
only O
discuss O

- B-DAT

- B-DAT

- B-DAT
ther O
improved O
mainly O
by O
increasing O

- B-DAT
ing O
network O
weights. O
VDSR O
[10 O

- B-DAT
creased O
the O
network O
depth O
by O

- B-DAT
duced O
recursive O
learning O
in O
a O

- B-DAT
rameter O
sharing. O
Tai O
et O
al O

- B-DAT
inal O
LR O
images O
to O
the O

- B-DAT

- B-DAT
creases O
computation O
complexity O
quadratically O
[4 O

- B-DAT

- B-DAT
tures O
from O
the O
interpolated O
LR O

- B-DAT

- B-DAT

- B-DAT
PCN O
[22], O
where O
an O
efficient O

- B-DAT

- B-DAT

- B-DAT
ods O
extracted O
features O
in O
the O

- B-DAT
nal O
LR O
features O
with O
transposed O

- B-DAT

- B-DAT

- B-DAT
lows O
direct O
connections O
between O
any O

- B-DAT
troduced O
among O
memory O
blocks O
[26 O

- B-DAT

- B-DAT
duced O
by O
a O
very O
deep O

- B-DAT
tion O
tasks O
(e.g., O
image O
SR O

- B-DAT

- B-DAT
tracts O
features O
F−1 O
from O
the O

- B-DAT
ond O
shallow O
feature O
extraction O
layer O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
layers O
within O
the O
block O

- B-DAT
ture. O
More O
details O
about O
RDB O

- B-DAT
cludes O
global O
feature O
fusion O
(GFF O

- B-DAT

- B-DAT

- B-DAT
nected O
layers, O
local O
feature O
fusion O

- B-DAT
ual O
learning, O
leading O
to O
a O

- B-DAT
nism O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sists O
of O
G O
(also O
known O

- B-DAT

- B-DAT

- B-DAT
tional O
layers O
1 O

- B-DAT

- B-DAT

- B-DAT
ing O
RDB O
and O
each O
layer O

- B-DAT
sequent O
layers, O
which O
not O
only O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ber. O
On O
the O
other O
hand O

- B-DAT
duce O
a O
1 O
× O
1 O

- B-DAT

- B-DAT
comes O
larger, O
very O
deep O
dense O

- B-DAT
tional O
layers O
in O
one O
RDB O

- B-DAT

- B-DAT
mance. O
We O
introduce O
more O
results O

- B-DAT
ing, O
we O
refer O
to O
this O

- B-DAT
maps O
produced O
by O
residual O
dense O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tively O
fused O
to O
form O
FGF O

- B-DAT

- B-DAT
quency O
information. O
However, O
in O
the O

- B-DAT
sequent O
layers. O
The O
local O
feature O

- B-DAT

- B-DAT
tion, O
MemNet O
extracts O
features O
in O

- B-DAT

- B-DAT
nition). O
While O
RDN O
is O
designed O

- B-DAT

- B-DAT
ual O
learning, O
which O
would O
be O

- B-DAT

- B-DAT
cal O
features, O
which O
are O
neglected O

- B-DAT
ferences O
between O
SRDenseNet O
[31] O
and O

- B-DAT
troduces O
the O
basic O
dense O
block O

- B-DAT
bilizes O
the O
training O
of O
wide O

- B-DAT
tract O
global O
features, O
because O
our O

- B-DAT
formance O
and O
convergence O
[17]. O
As O

- B-DAT
formation O
from O
their O
preceding O
layers O

- B-DAT
ers O
within O
one O
RDB. O
Furthermore O

- B-DAT
nections O
among O
memory O
blocks O
in O

- B-DAT

- B-DAT
sults O
are O
evaluated O
with O
PSNR O

- B-DAT
tion O
models O
to O
simulate O
LR O

- B-DAT
bic O
downsampling O
by O
adopting O
the O

- B-DAT
ing O
90◦. O
1,000 O
iterations O
of O

- B-DAT

- B-DAT
ery O
200 O
epochs. O
Training O
a O

- B-DAT
rameters: O
the O
number O
of O
RDB O

- B-DAT
mance O
of O
SRCNN O
[3] O
as O

- B-DAT
cal O
residual O
learning O
(LRL), O
and O

- B-DAT
portant, O
our O
RDN O
allows O
deeper O

- B-DAT
ture O
fusion O
(LFF) O
is O
needed O

- B-DAT
erly, O
so O
LFF O
isn’t O
removed O

- B-DAT
strates O
that O
stacking O
many O
basic O

- B-DAT
sulting O
in O
RDN O
CM1LRL0GFF0, O
RDN O

- B-DAT
ponent O
can O
efficiently O
improve O
the O

- B-DAT
line. O
This O
is O
mainly O
because O

- B-DAT
ing O
in O
RDN O
CM1LRL1GFF0, O
RDN O

- B-DAT
bination O
in O
Table O
1). O
It O

- B-DAT
ponents O
simultaneously O
(denote O
as O
RDN O

- B-DAT
sistent O
with O
the O
analyses O
above O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
SR O
methods: O
SRCNN O
[3 O

- B-DAT

- B-DAT
ther O
improve O
our O
RDN O
and O

- B-DAT

- B-DAT
rable O
or O
even O
better O
results O

- B-DAT
DenseNet O
[31] O
and O
MemNet O
[26 O

- B-DAT
els, O
our O
RDN O
also O
achieves O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put O
patch O
size. O
Moreover, O
our O

- B-DAT

- B-DAT
ods O
would O
produce O
noticeable O
artifacts O

- B-DAT
pared O
methods O
fail O
to O
recover O

- B-DAT
cover O
it O
obviously. O
This O
is O

- B-DAT
archical O
features O
through O
dense O
feature O

- B-DAT
CNN O
[3], O
FSRCNN O
[4], O
VDSR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults O
in O
Figs. O
7 O
and O

- B-DAT
covers O
sharper O
edges. O
This O
comparison O

- B-DAT
tracting O
hierarchical O
features O
from O
the O

- B-DAT
ods O
[3, O
10, O
38]. O
However O

- B-DAT
parison O
indicates O
that O
RDN O
is O

- B-DAT
tion O
models O
demonstrate O
the O
effectiveness O

- B-DAT
ing O
factor O
×3. O
The O
SR O

- B-DAT
ban100 O
and O
“img O
099” O
from O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferent O
or O
unknown O
degradation O
models O

- B-DAT
lizes O
the O
training O
wider O
network O

- B-DAT
ual O
leaning O
(LRL) O
further O
improves O

- B-DAT
world O
data. O
Extensive O
benchmark O
evaluations O

- B-DAT
strate O
that O
our O
RDN O
achieves O

- B-DAT

- B-DAT

- B-DAT
art O
methods O

-14 B-DAT

-1 B-DAT

- B-DAT
0484, O
and O
U.S. O
Army O
Research O

-17 B-DAT

- B-DAT
1-0367 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
deep O
convolutional O
networks O

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT

- B-DAT
mization. O
In O
ICLR, O
2014. O
5 O

- B-DAT
resolution. O
In O
CVPR, O
2017. O
1 O

- B-DAT
ham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT

- B-DAT
supervised O
nets. O
In O
AISTATS, O
2015 O

- B-DAT

- B-DAT
cal O
statistics. O
In O
ICCV, O
2001 O

- B-DAT
masaki, O
and O
K. O
Aizawa. O
Sketch-based O

- B-DAT
ing O
manga109 O
dataset. O
Multimedia O
Tools O

- B-DAT

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT
tia, O
A. O
M. O
S. O
M O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
hazing O
network. O
In O
CVPR, O
2018 O

- B-DAT

- B-DAT
raining O
using O
a O
multi-stream O
dense O

- B-DAT

- B-DAT
resolution O
with O
non-local O
means O
and O

- B-DAT
sion. O
TIP, O
2012. O
1 O

- B-DAT
lutional O
super-resolution O
network O
for O
multiple O

- B-DAT

- B-DAT

- B-DAT
nition O
problem. O
TIP, O
2012. O
1 O

five O
standard O
benchmark O
datasets: O
Set5[3], O
Set14 B-DAT

Set14 B-DAT
×2 O
30.24/0.8688 O
32.45/0.9067 O
33.05/0.9130 O
33.23/0.9136 O

Set14 B-DAT
BD O
26.12/0.7106 O
28.52/0.7924 O
29.67/0.8269 O
29.73/0.8292 O

Params. O
Set5 O
Set14 B-DAT
B100 O
Urban100 O
Manga109 O
MemNet-Pytorch O
677K O

from O
Set14 B-DAT

patch O
size O
based O
on O
the O
upscaling B-DAT
factor. O
The O
settings O
of O
input O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

Set14 B-DAT

Params. O
Set5 O
Set14 B-DAT

from O
Set14 B-DAT
Figure I-DAT
19. O
Visual O
results O
of O

- B-DAT

- B-DAT

- B-DAT
plored O
the O
power O
of O
deep O

- B-DAT
construction O
performance. O
However, O
the O
feedback O

- B-DAT
nism, O
which O
commonly O
exists O
in O

- B-DAT

- B-DAT
level O
representations O
with O
high-level O
information O

- B-DAT
ically, O
we O
use O
hidden O
states O

- B-DAT
ful O
high-level O
representations. O
The O
proposed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
task, O
which O
aims O
to O

- B-DAT

- B-DAT

- B-DAT
herently O
ill-posed O
since O
multiple O
HR O

- B-DAT
merous O
image O
SR O
methods O
have O

- B-DAT
ing O
interpolation-based O
methods[45], O
reconstruction-based O
methods[42 O

- B-DAT

- B-DAT
lutional O
Neural O
Network O
(CNN) O
to O

- B-DAT
tention O
in O
recent O
years O
due O

- B-DAT
posed O
network. O
Blue O
arrows O
represent O

- B-DAT

- B-DAT
ing O
more O
contextual O
information O
with O

- B-DAT
ishing/exploding O
problems O
caused O
by O
simply O

- B-DAT
ters O
increases. O
A O
large-capacity O
network O

- B-DAT

- B-DAT
current O
Neural O
Network O
(RNN). O
Similar O

- B-DAT
tional O
deep O
learning O
based O
methods O

- B-DAT
ward O
manner. O
However, O
the O
feedforward O

- B-DAT

- B-DAT

- B-DAT
down O
manner, O
carrying O
high-level O
information O

- B-DAT
vious O
layers O
and O
refining O
low-level O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
structed O
by O
multiple O
sets O
of O

- B-DAT
and O
down-sampling O
layers O
with O
dense O

- B-DAT

- B-DAT
covery O
difficulty. O
Such O
curriculum O
learning O

- B-DAT
tion O
models. O
Experimental O
results O
demonstrate O

- B-DAT
ority O
of O
our O
proposed O
SRFBN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work O
(SRFBN), O
which O
employs O
a O

- B-DAT
nism. O
High-level O
information O
is O
provided O

- B-DAT

- B-DAT
while, O
such O
recurrent O
structure O
with O

- B-DAT
tions O
provides O
strong O
early O
reconstruction O

- B-DAT
ficiently O
handles O
feedback O
information O
flows O

- B-DAT

- B-DAT
and O
down- O
sampling O
layers, O
and O

- B-DAT

- B-DAT
ing O
reconstruction O
difficulty O
are O
fed O

- B-DAT

- B-DAT

- B-DAT
ious O
computer O
vision O
tasks O
including O

- B-DAT

- B-DAT

- B-DAT
mation O
usage O
in O
LR O
images O

- B-DAT
provement O
in O
image O
SR. O
SRResNet[21 O

- B-DAT
plied O
residual O
skip O
connections O
from O

- B-DAT
work O
architectures O
use O
or O
combine O

- B-DAT

- B-DAT

- B-DAT
textual O
information O
due O
to O
the O

- B-DAT

- B-DAT
ing O
layers, O
and O
thus O
further O

- B-DAT
ity O
of O
the O
network. O
To O

- B-DAT
resolution O
feedback O
network O
(SRFBN), O
in O

- B-DAT

- B-DAT
down O
manner O
to O
correct O
low-level O

- B-DAT
textual O
information O

- B-DAT
capacity O
networks O
occupy O
huge O
amount O

- B-DAT
rent O
structure O
was O
employed[19, O
31 O

- B-DAT

- B-DAT

- B-DAT
and O
down-projection O
units O
to O
achieve O

- B-DAT
tion O
between O
two O
recurrent O
states O

- B-DAT

- B-DAT
ever, O
the O
flow O
of O
information O

- B-DAT

- B-DAT
tion O
of O
an O
input O
image O

- B-DAT
tional O
recurrent O
neural O
network. O
However O

- B-DAT

- B-DAT
ficiently O
flows O
across O
hierarchical O
layers O

- B-DAT
rior O
reconstruction O
performance O
than O
ConvLSTM1 O

- B-DAT
ficient O
strategy O
to O
improve O
the O

- B-DAT

- B-DAT
diction, O
they O
enforce O
a O
curriculum O

- B-DAT
creases O
during O
the O
training O
process O

- B-DAT
mid O
in O
previously O
trained O
networks O

- B-DAT
cess, O
we O
enforce O
a O
curriculum O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
effect O
process O
helps O
to O
achieve O

- B-DAT

- B-DAT
eration O
(to O
force O
the O
network O

- B-DAT
tion O
of O
high-level O
information), O
(2 O

- B-DAT

- B-DAT
formation, O
which O
is O
needed O
to O

- B-DAT
folded O
to O
T O
iterations, O
in O

- B-DAT
rally O
ordered O
from O
1 O
to O

- B-DAT

- B-DAT
tains O
three O
parts: O
an O
LR O

- B-DAT
sampled O
image O
to O
bypass O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
layer O
and O
a O
deconvolutional O

- B-DAT
traction O
block. O
F O
tin O
are O

- B-DAT

- B-DAT

- B-DAT
tained O
by O

- B-DAT

- B-DAT

- B-DAT
sentations O
F O
tin, O
and O
then O

- B-DAT

- B-DAT
tion O
block. O
The O
FB O
contains O

- B-DAT
tially O
with O
dense O
skip O
connections O

- B-DAT
jection O
group, O
which O
can O
project O

- B-DAT
tures O
F O
tin O
by O
feedback O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ingly, O
Ltg O
can O
be O
obtained O

- B-DAT

- B-DAT
tion O
group O
and O
map O
the O

- B-DAT
work. O
T O
target O
HR O
images O

- B-DAT
work. O
(I1HR, O
I O

- B-DAT
tion O
in O
the O
network O
can O

- B-DAT
put O
at O
the O
t-th O
iterations O

- B-DAT

- B-DAT
and O
down-sampling O
operations. O
For O
×2 O

- B-DAT
ating O
LR O
images O
from O
ground O

- B-DAT
ify O
the O
effectiveness O
of O
our O

- B-DAT
degradation O
models O
as O
[47] O
do O

- B-DAT
periments, O
we O
use O
7x7 O
sized O

- B-DAT
sampling O
followed O
by O
adding O
Gaussian O

- B-DAT
ing O
rate O
0.0001. O
The O
learning O

- B-DAT
ery O
200 O
epochs. O
We O
implement O

- B-DAT
ber O
of O
iterations O
(denoted O
as O

- B-DAT
jection O
groups O
in O
the O
feedback O

- B-DAT
iments. O
We O
first O
investigate O
the O

- B-DAT
out O
feedback O
connections O
(T=1). O
Besides O

- B-DAT
sults. O
It O
is O
worth O
noticing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
forward O
one O
in O
this O
subsection O

- B-DAT
put O
to O
low-level O
representations O
and O

- B-DAT
erty), O
denoted O
as O
SRFBN-L-FF. O
SRFBN-L O

- B-DAT

- B-DAT
FF O
both O
have O
four O
iterations O

- B-DAT
ate O
SR O
images O
from O
both O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
from O
which O
we O
conclude O

- B-DAT
trast O
to O
feedforward O
network. O
The O

- B-DAT
rent O
structure. O
Except O
for O
the O

- B-DAT
tive O
experiments O
to O
verify O
other O

- B-DAT
ation O
except O
the O
first O
iteration O

- B-DAT
put O
to O
low-level O
representations O
and O

- B-DAT
erty), O
denoted O
as O
SRFBN-L-FF. O
SRFBN-L O

- B-DAT

- B-DAT
FF O
both O
have O
four O
iterations O

- B-DAT
ate O
SR O
images O
from O
both O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ation, O
from O
which O
we O
conclude O

- B-DAT
trast O
to O
feedforward O
network. O
The O

- B-DAT
current O
structure. O
Except O
the O
above O

- B-DAT
tive O
experiments O
to O
verify O
other O

- B-DAT
ation O
except O
the O
first O
iteration O

- B-DAT
works O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trated O
in O
Fig. O
5. O
Each O

- B-DAT

- B-DAT
covering O
the O
residual O
image. O
In O

- B-DAT
inal O
input O
image[16] O
and O
to O

- B-DAT

- B-DAT
ponents O
(i.e. O
edges O
and O
contours O

- B-DAT
age. O
To O
some O
extent, O
this O

- B-DAT
tion O
ability O
than O
the O
feedforward O

- B-DAT
vation O
is O
that O
the O
feedback O

- B-DAT
sentations O
in O
contrast O
to O
feedforward O

- B-DAT
tions O
and O
then O
the O
smooth O

- B-DAT
strate O
that O
the O
feedforward O
network O

- B-DAT
formation O
through O
layers, O
while O
the O

- B-DAT
lowed O
to O
devote O
most O
of O

- B-DAT

- B-DAT

- B-DAT
sentations O
at O
the O
initial O
iteration O

- B-DAT

- B-DAT

- B-DAT
quent O
iterations O
to O
generate O
better O

- B-DAT
culty. O
For O
example, O
to O
guide O

- B-DAT
works O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trated O
in O
Fig. O
5. O
Each O

- B-DAT

- B-DAT
work O
with O
global O
residual O
skip O

- B-DAT
ering O
the O
residual O
image. O
In O

- B-DAT
put O
image[16] O
and O
to O
predict O

- B-DAT

- B-DAT
servations. O
First, O
compared O
with O
the O

- B-DAT
tions O
in O
contrast O
to O
feedforward O

- B-DAT
cantly O
from O
the O
first O
iteration O

- B-DAT

- B-DAT

- B-DAT
ing O
high-level O
information O
at O
the O

- B-DAT

- B-DAT
back O
network O
will O
urge O
previous O

- B-DAT
tions O
to O
generate O
better O
representations O

- B-DAT
culty. O
For O
example, O
to O
guide O

- B-DAT
gle O
downsampling O
operator O
at O
early O

- B-DAT

- B-DAT
tion O
model. O
The O
results O
shown O

- B-DAT
riculum O
learning O
strategy O
well O
assists O

- B-DAT

- B-DAT
work O
pretrained O
on O
the O
BI O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
son O
results O
are O
given O
in O

- B-DAT

- B-DAT
rameters O
fewer O
than O
1000K. O
This O

- B-DAT
struction O
performance. O
Meanwhile, O
in O
comparison O

- B-DAT
DBPN O
and O
EDSR, O
our O
proposed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods: O
SRCNN[7], O
VDSR[18], O
DRRN[31], O
SRDenseNet[36 O

- B-DAT

- B-DAT

- B-DAT
perform O
almost O
all O
comparative O
methods O

- B-DAT

- B-DAT
ages O
(DIV2K+Flickr2K+ImageNet O
vs. O
DIV2K+Flickr2K). O
However O

- B-DAT
trast O
to O
them. O
In O
addition O

- B-DAT
age O
from O
Manga109, O
DRRN O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
curriculum O
learning O
strategy O
for O

- B-DAT
tion O
models, O
and O
fine-tuned O
based O

- B-DAT
CNN O
C[43], O
SRMD(NF)[44], O
and O
RDN[47 O

- B-DAT

- B-DAT
most O
all O
quantative O
results O
over O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O

- B-DAT

- B-DAT

- B-DAT
mark O
datasets. O
Compared O
with O
other O

- B-DAT
posed O
SRFBN O
could O
alleviate O
the O

- B-DAT
isions, O
we O
further O
indicate O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dation O
models. O
The O
comprehensive O
experimental O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sored O
by O
National O
Natural O
Science O

- B-DAT
tion O
of O
Sichuan O
Science O
and O

- B-DAT

- B-DAT
son O
Weston. O
Curriculum O
learning. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
with O
feedback O
convolutional O
neural O

- B-DAT
tendra O
Malik. O
Human O
pose O
estimation O

- B-DAT

- B-DAT

- B-DAT
works. O
TPAMI, O
2016. O
2, O
7 O

- B-DAT

- B-DAT
down O
influences O
in O
sensory O
processing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mance O
on O
imagenet O
classification. O
In O

- B-DAT
ian O
Q O
Weinberger. O
Densely O
connected O

- B-DAT
works. O
In O
CVPR, O
2016. O
2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rate O
single O
image O
super-resolution O
via O

- B-DAT
tion O
network. O
In O
CVPR, O
2018 O

- B-DAT
tween O
figure O
and O
background O
by O

- B-DAT
ture, O
1998. O
2 O

- B-DAT

- B-DAT
works. O
In O
CVPR, O
2016. O
1 O

- B-DAT
recursive O
convolutional O
network O
for O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tex. O
arXiv O
preprint O
arXiv:1604.03640, O
2016 O

- B-DAT

- B-DAT
dra O
Malik. O
A O
database O
of O

- B-DAT

- B-DAT
timedia O
Tools O
and O
Applications, O
2017 O

- B-DAT

- B-DAT

- B-DAT
back O
for O
crowd O
counting O
convolutional O

- B-DAT

- B-DAT
resolution O
via O
deep O
recursive O
residual O

- B-DAT
net: O
A O
persistent O
memory O
network O

- B-DAT
chored O
neighborhood O
regression O
for O
fast O

- B-DAT

- B-DAT

- B-DAT
resolution. O
In O
ACCV, O
2015. O
1 O

- B-DAT

- B-DAT
tion. O
In O
CVPR, O
2016. O
7 O

- B-DAT

- B-DAT
hanced O
super-resolution O
generative O
adversarial O
networks O

- B-DAT
der O
Sorkinehornung, O
Olga O
Sorkinehornung, O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
back O
networks. O
In O
CVPR, O
2017 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lation O
algorithm O
via O
directional O
filtering O

- B-DAT

- B-DAT

- B-DAT
tion. O
We O
still O
use O
SRFBN-L O

- B-DAT
and O
down-sampling O
layers O
(UDSL), O
(2 O

- B-DAT
and O
down-sampling O
layers O
with O
3 O

- B-DAT
ers O
(with O
one O
padding O
and O

- B-DAT
and O
down-sampling O
operations O
carrying O
large O

- B-DAT
tion O
and O
are O
effective O
for O

- B-DAT

- B-DAT
ter O
adding O
DSC O
to O
the O

- B-DAT
and O
down-sampling O
layers O
(UDSL), O
and O

- B-DAT
sides, O
high-level O
information O
is O
directly O

- B-DAT

- B-DAT

- B-DAT
ization O

- B-DAT
textual O
information O
for O
the O
next O

- B-DAT
parison O
with O
other O
basic O
blocks O

- B-DAT
nism O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tations O
to O
the O
initial O
feature O

- B-DAT

- B-DAT
formation, O
surely O
are O
corrected O
using O

- B-DAT

- B-DAT

- B-DAT
erage O
feature O
map O
at O
each O

- B-DAT

- B-DAT
back) O
and O
SRFBN-L-FF O
(feedforward). O
As O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trum O
map O
through O
discrete O
Fourier O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eration O
t O
grows, O
the O
feedforward O

- B-DAT
ers O
mid-frequency O
and O
high-frequency O
components O

- B-DAT
developed O
information. O
For O
the O
feedback O

- B-DAT
nism O
(t O
>1), O
mid-frequency O
and O

- B-DAT

- B-DAT
tion O
of O
the O
average O
feature O

- B-DAT
ture O
design, O
we O
compare O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
network O
with O
moderate O
parameters O

- B-DAT
cause O
MemNet O
only O
reveals O
the O

- B-DAT

- B-DAT

- B-DAT
tary O
materials. O
Our O
SRFBN-S O
(T=4 O

- B-DAT
son. O
In O
Tab. O
8, O
our O

- B-DAT

- B-DAT
sults O
than O
MemNet O
with O
71 O

- B-DAT

- B-DAT
parison O
shows O
the O
effectiveness O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ban100 O
with O
scale O
factor O
×4 O

- B-DAT

- B-DAT
torch O
for O
fair O
comparison. O
The O

- B-DAT
works O
is O
evaluated O
on O
the O

- B-DAT
tel O
i7 O
CPU O
(16G O
RAM O

- B-DAT
ing O
their O
official O
codes. O
Tab O

- B-DAT

- B-DAT
son O
with O
other O
networks. O
This O

- B-DAT
tiveness O
of O
our O
proposed O
networks O

- B-DAT
volutional O
layers O
with O
77% O
fewer O

-22, B-DAT
we O
provide O
more O
visual O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

standard O
benchmark O
datasets: O
Set5 O
[2], O
Set14 B-DAT
[33], O
B100 O
[17], O
and O
Urban100 O

Set14 B-DAT
×3 O
27.55 O
/ O
0.7742 O
29.13 O

reconstruct O
high-resolution O
images O
of O
different O
upscaling B-DAT
factors O
in O
a O
single O
model O

demonstrated O
in O
Fig. O
4. O
For O
upscaling B-DAT
×4, O
if O
we O
use O
a O

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

- B-DAT

- B-DAT

- B-DAT
hanced O
deep O
super-resolution O
network O
(EDSR O

- B-DAT
mance O
exceeding O
those O
of O
current O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods. O
The O
significant O
performance O
improvement O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
on O
benchmark O
datasets O
and O

- B-DAT
ning O
the O
NTIRE2017 O
Super-Resolution O
Challenge O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nificantly O
improved O
performance O
in O
terms O

- B-DAT

- B-DAT
noise O
ratio O
(PSNR) O
in O
the O

- B-DAT
works O
exhibit O
limitations O
in O
terms O

- B-DAT

- B-DAT

- B-DAT
resolution O
of O
different O
scale O
factors O

- B-DAT
lems O
without O
considering O
and O
utilizing O

- B-DAT
quire O
many O
scale-specific O
networks O
that O

- B-DAT

- B-DAT

- B-DAT
dancy O
among O
scale-specific O
models. O
Nonetheless O

- B-DAT

- B-DAT
pling O
method O
[5, O
22, O
14 O

- B-DAT
ploys O
the O
ResNet O
architecture O
from O

- B-DAT
posed O
to O
solve O
higher-level O
computer O

- B-DAT

- B-DAT

- B-DAT
chitecture, O
we O
first O
optimize O
it O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els. O
Furthermore, O
we O
propose O
a O

- B-DAT

- B-DAT

- B-DAT
rameters O
compared O
with O
multiple O
single-scale O

- B-DAT
and O
multi-scale O
super-resolution O
networks O
show O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dicting O
detailed, O
realistic O
textures. O
Previous O

- B-DAT
struct O
better O
high-resolution O
images O

- B-DAT
tween O
ILR O
and O
IHR O
image O

- B-DAT
ods O
rely O
on O
techniques O
ranging O

- B-DAT
ding O
[3, O
2, O
7, O
21 O

- B-DAT
proaches O
utilize O
image O
self-similarities O
to O

- B-DAT
nal O
databases O
[8, O
6, O
29 O

- B-DAT
works O
has O
led O
to O
dramatic O

- B-DAT

- B-DAT
ing O
much O
deeper O
network O
architectures O

- B-DAT
perior O
performance. O
In O
particular, O
they O

- B-DAT
connection O
and O
recursive O
convolution O
alleviate O

- B-DAT

- B-DAT
work. O
Similarly O
to O
[20], O
Mao O

- B-DAT

- B-DAT

- B-DAT
rithms, O
an O
input O
image O
is O

- B-DAT
lation O
before O
they O
fed O
into O

- B-DAT
sampling O
modules O
at O
the O
very O

- B-DAT
sible O
as O
shown O
in O
[5 O

- B-DAT
cause O
the O
size O
of O
features O

- B-DAT

- B-DAT
scale O
training O
and O
computational O
efficiency O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
thermore, O
we O
develop O
an O
appropriate O

- B-DAT
and O
multi-scale O
mod- O
els O

- B-DAT

- B-DAT
hibiting O
improved O
computational O
efficiency. O
In O

- B-DAT
ing O
sections, O
we O
suggest O
a O

- B-DAT

- B-DAT

- B-DAT
scale O
architecture O
(MDSR) O
that O
reconstructs O

- B-DAT

- B-DAT
level O
to O
high-level O
tasks. O
Although O

- B-DAT
fully O
applied O
the O
ResNet O
architecture O

- B-DAT

- B-DAT
mance O
by O
employing O
better O
ResNet O

- B-DAT
work O
model O
from O
original O
ResNet O

- B-DAT
tion O
increases O
the O
performance O
substantially O

- B-DAT
duced O
since O
the O
batch O
normalization O

- B-DAT

- B-DAT

- B-DAT
work O
model O
is O
to O
increase O

- B-DAT
nels) O
F O
occupies O
roughly O
O(BF O

- B-DAT
imize O
the O
model O
capacity O
when O

- B-DAT
tational O
resources O

- B-DAT
cedure O
numerically O
unstable. O
A O
similar O

- B-DAT
ing O
procedure O
greatly O
when O
using O

- B-DAT
ous O
convolution O
layer O
for O
the O

- B-DAT

- B-DAT
vation O
layers O
outside O
the O
residual O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
at O
multiple O
scales O
is O

- B-DAT

- B-DAT
ther O
explore O
this O
idea O
by O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
specific O
processing O
modules O
to O
handle O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ule O
consists O
of O
two O
residual O

- B-DAT

- B-DAT

- B-DAT
tive O
field O
is O
covered O
in O

- B-DAT

- B-DAT

- B-DAT
ules O
are O
located O
in O
parallel O

- B-DAT

- B-DAT
tion. O
The O
architecture O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els O
for O
3 O
different O
scales O

- B-DAT

- B-DAT

- B-DAT
hibits O
comparable O
performance O
as O
the O

- B-DAT

- B-DAT

- B-DAT

Residual O
scaling O
- B-DAT
- O
0.1 O

- B-DAT

- B-DAT
ual O
blocks O
are O
lighter O
than O

- B-DAT

- B-DAT
specific O
EDSRs. O
The O
detailed O
performance O

- B-DAT

- B-DAT
formances O
on O
the O
validation O
dataset O

- B-DAT

- B-DAT
ing O
the O
mean O
RGB O
value O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works O
as O
described O
in O
Sec O

- B-DAT
trained O
network O
for O
other O
scales O

- B-DAT

- B-DAT
specific O
residual O
blocks O
and O
upsampling O

- B-DAT
spond O
to O
different O
scales O
other O

- B-DAT
imizing O
L2 O
is O
generally O
preferred O

- B-DAT
pirically O
found O
that O
L1 O
loss O

- B-DAT
spectively. O
The O
source O
code O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
super-resolved O
images O

- B-DAT

- B-DAT

- B-DAT
rate O
models. O
It O
is O
beneficial O

- B-DAT

- B-DAT
dividually O
trained O
models. O
We O
denote O

- B-DAT

- B-DAT

- B-DAT
inal O
one O
trained O
with O
L2 O

- B-DAT

- B-DAT
els O
require O
much O
less O
GPU O

- B-DAT
sults O
in O
an O
individual O
experiment O

- B-DAT
per O
[14]. O
In O
our O
experiments O

- B-DAT

0.9542 O
37.53 O
/ O
0.9587 O
- B-DAT
/ O
- O
38.11 O
/ O
0.9601 O

0.9090 O
33.66 O
/ O
0.9213 O
- B-DAT
/ O
- O
34.65 O
/ O
0.9282 O

0.9063 O
33.03 O
/ O
0.9124 O
- B-DAT
/ O
- O
33.92 O
/ O
0.9195 O

0.8209 O
29.77 O
/ O
0.8314 O
- B-DAT
/ O
- O
30.52 O
/ O
0.8462 O

0.8879 O
31.90 O
/ O
0.8960 O
- B-DAT
/ O
- O
32.32 O
/ O
0.9013 O

0.7863 O
28.82 O
/ O
0.7976 O
- B-DAT
/ O
- O
29.25 O
/ O
0.8093 O

0.8946 O
30.76 O
/ O
0.9140 O
- B-DAT
/ O
- O
32.93 O
/ O
0.9351 O

0.7989 O
27.14 O
/ O
0.8279 O
- B-DAT
/ O
- O
28.80 O
/ O
0.8653 O

0.9581 O
33.66 O
/ O
0.9625 O
- B-DAT
/ O
- O
35.03 O
/ O
0.9695 O

0.9138 O
30.09 O
/ O
0.9208 O
- B-DAT
/ O
- O
31.26 O
/ O
0.9340 O

0.8753 O
28.17 O
/ O
0.8841 O
- B-DAT
/ O
- O
29.25 O
/ O
0.9017 O

- B-DAT

- B-DAT
vided O
in O
the O
last O
two O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sure O
PSNR O
and O
SSIM O
on O

- B-DAT
LAB O
[18] O
functions O
for O
evaluation O

- B-DAT
nificant O
improvement O
compared O
to O
the O

- B-DAT

- B-DAT

- B-DAT
puts O
compared O
with O
the O
previous O

- B-DAT
ticipating O
in O
the O
NTIRE2017 O
Super-Resolution O

- B-DAT
resolution O
system O
with O
the O
highest O

- B-DAT
graders O
(bicubic, O
unknown) O
with O
three O

- B-DAT
ditions. O
Some O
results O
of O
our O

- B-DAT
ods O
successfully O
reconstruct O
high-resolution O
images O

- B-DAT

- B-DAT
ventional O
ResNet O
architecture, O
we O
achieve O

- B-DAT
ual O
scaling O
techniques O
to O
stably O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT

- B-DAT

- B-DAT
age O
Processing, O
21(7):3194–3205, O
2012. O
2 O

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT

- B-DAT
mization. O
In O
ICLR O
2014. O
5 O

- B-DAT

- B-DAT

- B-DAT
ative O
adversarial O
network. O
arXiv:1609.04802, O
2016 O

- B-DAT

- B-DAT
ing O
very O
deep O
convolutional O
encoder-decoder O

- B-DAT
cal O
statistics. O
In O
ICCV O
2001 O

- B-DAT

- B-DAT

- B-DAT
tional O
networks O
for O
biomedical O
image O

- B-DAT
CAI O
2015. O
2 O

- B-DAT
tion O
by O
locally O
linear O
embedding O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT
v4, O
inception-resnet O
and O
the O
impact O

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
actions O
on O
Image O
Processing, O
21(8):3467–3478 O

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT

- B-DAT

- B-DAT
tional O
Conference O
on O
Curves O
and O

- B-DAT

- B-DAT
gorithm O
via O
directional O
filtering O
and O

- B-DAT
actions O
on O
Image O
Processing, O
15(8):2226–2238 O

Set14 B-DAT
(2) O
33.03/0.9124 O
33.08/0.9126 O
33.09/0.9129 O
34.04/0.9205 O

3) O
33.66/0.9213 O
33.73/0.9212 O
33.86/0.9228 O
34.45/0.9272 O
Set14 B-DAT
(3) O
29.77/0.8314 O
29.83/0.8321 O
29.88/0.8331 O
30.56/0.8450 O

4) O
31.35/0.8838 O
31.40/0.8837 O
31.52/0.8864 O
32.23/0.8952 O
Set14 B-DAT
(4) O
28.01/0.7674 O
28.07/0.7681 O
28.11/0.7699 O
28.80/0.7856 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ous O
image O
restoration O
tasks. O
However O

- B-DAT
pose O
a O
novel O
feature O
space O

- B-DAT
rithm O
that O
outperforms O
the O
existing O

- B-DAT
mance O
of O
a O
learning O
algorithm O

- B-DAT

- B-DAT

- B-DAT
strate O
that O
the O
proposed O
feature O

- B-DAT
performs O
the O
existing O
state-of-the-art O
approaches O

- B-DAT
over, O
our O
algorithm O
was O
ranked O

-10 B-DAT
times O
faster O
computational O
time O
compared O

- B-DAT
cessing O
applications. O
Over O
the O
last O

- B-DAT

- B-DAT

- B-DAT
proaches O
[22], O
and O
sparse O
dictionary O

- B-DAT

- B-DAT

- B-DAT
over, O
these O
algorithms O
are O
usually O

- B-DAT
ative O
manner, O
so O
they O
require O

- B-DAT
sources O

- B-DAT
level O
computer O
vision O
problems O
[24 O

- B-DAT
ing O
and O
super-resolution O
tasks, O
many O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
itation O
of O
the O
state-of-the-art O
CNN O

- B-DAT
posed O
network O
architectures O
are O
motivated O

- B-DAT
sistent O
homology O
analysis O
[11] O
on O

- B-DAT
age O
processing O
tasks. O
Specifically, O
we O

- B-DAT
ual O
manifold O
is O
topologically O
simpler O

- B-DAT
age O
manifold, O
which O
may O
have O

- B-DAT
tion. O
Specifically, O
our O
design O
goal O

- B-DAT
tures O
while O
preserving O
the O
directional O

- B-DAT
lowing. O
First, O
a O
novel O
network O

- B-DAT
fold O
simplification O
is O
proposed. O
Second O

- B-DAT
putational O
topology O
tool O
called O
the O

- B-DAT
form O
to O
simplify O
topological O
structures O

COPY O
ch(LR) O
Label O
Input O
- B-DAT
WT(HR) O
Input O
- O
PS(HR O

Long O
bypass O
layer O
LongBypass(2) O
- B-DAT
- O
Repeat O
1st O
module O
6 O
times O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vanced O
algorithms O
in O
this O
field O

- B-DAT
and O
inter- O
correlations O
of O
the O

- B-DAT
mance O
to O
BM3D O
using O
multi-layer O

- B-DAT
able O
nonlinear O
reaction O
diffusion O
(TNRD O

- B-DAT
ters O
and O
influence O
functions O
by O

- B-DAT
timization O
approach. O
Recently, O
based O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
restoration O
problems O
[21 O

- B-DAT
proach O
is O
using O
a O
skipped O

- B-DAT
cept O
was O
first O
introduced O
by O

- B-DAT
tion. O
In O
low-level O
computer O
vision O

- B-DAT

- B-DAT
ping. O
In O
another O
implementation, O
the O

- B-DAT
formed O
into O
the O
difference O
between O

- B-DAT
lem: O
minf∈F O
L(f), O
where O
L(f O

- B-DAT
notes O
the O
risk. O
A O
major O

- B-DAT
formance. O
Specifically, O
with O
probability O

- B-DAT
pared O
to O
shallow O
ones. O
However O

- B-DAT

- B-DAT
ity O
penalty O
reduces O
much O
more O

- B-DAT
mization. O
One O
of O
the O
most O

- B-DAT
work, O
by O
reducing O
the O
complexity O

- B-DAT
ture O
spaces O
for O
the O
input O

- B-DAT
form O
is O
given O
by O
Y O

- B-DAT
sults O
in O
the O
dimensional O
reduction O

- B-DAT
tion. O
Indeed, O
this O
property O
of O

- B-DAT

- B-DAT
gebraic O
topology, O
Betti O
numbers O
(βm O

- B-DAT
ber O
of O
m-dimensional O
holes O
of O

- B-DAT
ing O
the O
changes O
of O
Betti O

- B-DAT
come O
a O
single O
cluster O
(Fig O

- B-DAT
resented O
as O
a O
slow O
decrease O

- B-DAT

- B-DAT
uration O
over O
� O
distance O
filtration O

- B-DAT
ture O
and O
the O
recent O
persistent O

- B-DAT

- B-DAT

- B-DAT
lationship O
between O
these O
newly O
processed O

- B-DAT
sian O
denoising, O
40 O
× O
40 O

- B-DAT
ing O
because O
it O
is O
helpful O

- B-DAT
leviating O
the O
gradient O
vanishing O
problem O

- B-DAT
malization, O
and O
ReLU O
and O
the O

- B-DAT
ing O
the O
convolution, O
we O
used O

- B-DAT
ceptive O
field O
can O
be O
reduced O

- B-DAT
mary O
denoising O
architecture. O
Depending O
on O

- B-DAT
tended O
denoising O
network O
structures O
with O

- B-DAT
form O
were O
used O
for O
manifold O

- B-DAT
known O
decimation O
scheme, O
however, O
we O

- B-DAT
pixel O
shuffling O
scheme O
[25] O
as O

- B-DAT

- B-DAT
fling O
transform O
does O
not O
reduce O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
task. O
The O
training O
step O

- B-DAT
ter O
the O
first O
two O
layers O

- B-DAT
construct O
the O
bicubic O
x2 O
downsampled O

- B-DAT
nection O
allows O
faster O
computation O
and O

- B-DAT
pass O
connection O

- B-DAT
minance O
channel, O
because O
RGB O
based O

- B-DAT
fect O
of O
data O
augmentation O

- B-DAT
able O
Berkeley O
segmentation O
(BSD500) O
[4 O

- B-DAT
ing O
task. O
In O
addition, O
we O

- B-DAT
fitting, O
we O
re-generated O
the O
Gaussian O

- B-DAT
ing O
and O
validation, O
Gaussian O
noises O

- B-DAT
tion O
using O
image O
flipping, O
rotation O

- B-DAT
tialized O
using O
the O
Xavier O
method O

- B-DAT
sion O
loss O
across O
four O
wavelet O

- B-DAT

- B-DAT
ble O
learning, O
we O
employed O
the O

- B-DAT
date O
parameter O
are O
bounded O
by O

- B-DAT
box O
(beta.20) O
[29] O
in O
MATLAB O

- B-DAT
Works, O
Natick). O
We O
used O
a O

-4770 B-DAT
CPU O
(3.40GHz). O
The O
Gaussian O
denoising O

- B-DAT
work O
took O
about O
two O
days O

- B-DAT

- B-DAT
epoch O
system O
that O
repeats O
forward O

- B-DAT
work O
with O
the O
bicubic O
x2 O

- B-DAT

- B-DAT

- B-DAT
mance O
and O
manifold O
simplification, O
we O

- B-DAT
ogy O
of O
the O
input O
and O

- B-DAT

- B-DAT
noising O
performance, O
we O
used O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
noising O
methods O
in O
terms O
of O

- B-DAT
bara O
and O
House, O
we O
attained O

- B-DAT
posed O
method O
showed O
superior O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
composition, O
additional O
comparative O
studies O
with O

- B-DAT
line O
network O
were O
performed. O
Here O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fling O
scheme O
in O
[25] O
except O

- B-DAT
polated. O
Accordingly, O
the O
networks O
using O

- B-DAT
actly O
same O
architecture O
except O
the O

- B-DAT
plification. O
Here, O
the O
Gaussian O
denoising O

- B-DAT
bara O
image O
was O
used O
for O

- B-DAT

-3 B-DAT
Proposed-P O
Proposed O
Set5 O
(2) O
37.53/0.9586 O

-3, B-DAT
Proposed-P(primary) O
networks O
are O
291 O
dataset[18 O

- B-DAT

- B-DAT
stored O
RGB O
was O
used O
to O

- B-DAT
ues O

- B-DAT
lar, O
our O
networks O
were O
competitive O

-67 B-DAT
seconds O
computational O
time O
by O
the O

-5 B-DAT
seconds O
for O
each O
frame. O
Since O

- B-DAT

- B-DAT
known O
decimation O
dataset O
where O
we O

- B-DAT
ifold O
simplification O
from O
the O
residual O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eas. O
We O
provide O
more O
comparative O

- B-DAT
mentary O
material O

- B-DAT
ogy O
analysis, O
we O
showed O
that O

- B-DAT
ing O
as O
well O
as O
the O

- B-DAT
ing O
and O
NTIRE O
SISR O
competition O

- B-DAT
mance O
and O
speed. O
Moreover, O
we O

- B-DAT

- B-DAT
ing O
Foundation, O
Grant O
number O
NRF-2013M3A9B2076548 O

- B-DAT
puter O
Vision, O
76(2):123–139, O
2008. O
1 O

- B-DAT
sion O
and O
Pattern O
Recognition O
(CVPR O

- B-DAT
mized O
reaction O
diffusion O
processes O
for O

- B-DAT

- B-DAT
den O
Markov O
models. O
IEEE O
Transactions O

- B-DAT

- B-DAT

- B-DAT
laborative O
filtering. O
IEEE O
Transactions O
on O

- B-DAT
cessing, O
16(8):2080–2095, O
2007. O
1 O

- B-DAT
ume O
61. O
SIAM, O
1992. O
3 O

- B-DAT
tralized O
sparse O
representation O
for O
image O

- B-DAT

- B-DAT

- B-DAT
a O
survey. O
Contemporary O
Mathematics, O
453:257–282 O

- B-DAT
ual O
learning O
for O
image O
recognition O

- B-DAT

- B-DAT
tional O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT

- B-DAT
tern O
Recognition O
(CVPR), O
pages O
5197–5206 O

- B-DAT
erating O
deep O
network O
training O
by O

- B-DAT

- B-DAT
works. O
arXiv O
preprint O
arXiv:1511.04587, O
2015 O

- B-DAT
agenet O
classification O
with O
deep O
convolutional O

- B-DAT
ing O
Systems, O
pages O
1097–1105, O
2012 O

- B-DAT
demic O
press, O
1999. O
3 O

- B-DAT

- B-DAT
based O
image O
restoration. O
Multiscale O
Modeling O

- B-DAT
ulation, O
4(2):460–489, O
2005. O
1 O

- B-DAT
moncelli. O
Image O
denoising O
using O
scale O

- B-DAT

- B-DAT
volutional O
networks O
for O
biomedical O
image O

- B-DAT
tion. O
In O
International O
Conference O
on O

- B-DAT
age O
Computing O
and O
Computer-Assisted O
Intervention O

- B-DAT

- B-DAT
gle O
image O
and O
video O
super-resolution O

- B-DAT

- B-DAT
tion O
(CVPR), O
pages O
1874–1883, O
2016 O

- B-DAT
ory, O
volume O
1. O
Wiley O
New O

- B-DAT

-1995 B-DAT

-001 B-DAT

- B-DAT
Hall, O
1995. O
7 O

- B-DAT
celli. O
Image O
quality O
assessment: O
from O

- B-DAT

- B-DAT
ror O
as O
the O
loss, O
the O

- B-DAT

- B-DAT
variant O
under O
batch O
normalization O
is O

- B-DAT

- B-DAT
culated O
the O
barcodes O
of O
the O

- B-DAT
box O
called O
JAVAPLEX O
(http://appliedtopology.github.io/ O
javaplex O

- B-DAT
posed O
of O
residual O
image O
patches O

- B-DAT
ical O
complexity O
in O
the O
image O

- B-DAT
thogonal O
Haar O
wavelet O
transform. O
The O

- B-DAT
form O
which O
further O
reduces O
the O

- B-DAT
resolution O
datasets O
(the O
right O
column O

- B-DAT
codes, O
the O
input O
manifold O
of O

- B-DAT
ages O
had O
simpler O
topology O
than O

- B-DAT
noising O
, O
(b) O
super-resolution O
(bicubic O

- B-DAT
resolution O
(unknown O
decimation) O
tasks. O
(Right O

- B-DAT
ogy O
analysis O
of O
input O
manifold O

- B-DAT

- B-DAT

- B-DAT
tion) O
tasks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ditional O
simpler O
label O
manifold O
from O

- B-DAT
noising O
and O
SISR O
reconstruction O
from O

datasets O
such O
as O
Set5 O
[3], O
Set14 B-DAT
[39], O
B100 O
[29] O
and O
Urban100 O

of O
operations O
and O
parameters O
on O
Set14 B-DAT
×4 O
dataset. O
The O
x-axis O
and O

of O
the O
parameters O
on O
the O
Set14 B-DAT
×4 O
dataset. O
Here, O
our O
CARN O

qualitative O
comparisons O
over O
three O
datasets O
(Set14, B-DAT
B100 O
and O
Urban100) O
for O
×4 O

Scale O
Model O
Params O
MultAdds O
Set5 O
Set14 B-DAT
B100 O
Urban100 O

cascading O
modules O
measured O
on O
the O
Set14 B-DAT
×4 O
dataset. O
CARN-NL O
represents O
CARN O

We O
evaluate O
all O
models O
on O
Set14 B-DAT
with O
×4 O
scale. O
GConv O
represents O

comic O
from O
Set14 B-DAT

Scale O
Model O
Params O
MultAdds O
Set5 O
Set14 B-DAT

We O
evaluate O
all O
models O
on O
Set14 B-DAT

comic O
from O
Set14 B-DAT
HR I-DAT

- B-DAT

- B-DAT

- B-DAT
fully O
applied O
to O
single-image O
super-resolution O

- B-DAT
world O
applications O
due O
to O
the O

- B-DAT

- B-DAT
ture O
that O
implements O
a O
cascading O

- B-DAT
work O
to O
further O
improve O
efficiency O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lution O
limitations, O
and O
could O
be O

- B-DAT

- B-DAT

- B-DAT
vided O
outstanding O
performance O
in O
SISR O

- B-DAT

- B-DAT
tical O
for O
real-world O
applications. O
One O

- B-DAT

- B-DAT

- B-DAT
ple, O
DRCN O
[21] O
uses O
a O

- B-DAT
els O
decrease O
the O
number O
of O

- B-DAT
ducing O
the O
number O
of O
parameters O

- B-DAT

- B-DAT
sider O
a O
situation O
where O
an O

- B-DAT

- B-DAT
lenging O
and O
necessary O
step O
that O

- B-DAT
mand O
for O
streaming O
media O
has O

- B-DAT
ing O
lossy O
compression O
techniques O
before O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
M). O
We O
first O
build O
our O

- B-DAT

- B-DAT
ing O
the O
FSRCNN O
[7], O
CARN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
approaches O
have O
been O
applied O

- B-DAT
based O
SISR O
in O
section O
2.1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rameters O
by O
engaging O
in O
redundant O

- B-DAT

(- B-DAT

- B-DAT

(- B-DAT

- B-DAT

- B-DAT
erations O
in O
(a) O
and O
(b O

- B-DAT

- B-DAT
ter O
category, O
SqueezeNet O
[19] O
builds O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ber O
of O
operations O
compared O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ary O
layers O
are O
cascaded O
into O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ual O
block, O
the O
first O
residual O

- B-DAT
rameter O
of O
the O
convolution O
layer O

- B-DAT
trated O
in O
block O
(c) O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
anism. O
As O
shown O
in O
Fig O

- B-DAT
cading O
on O
both O
the O
local O

- B-DAT

- B-DAT
resentations. O
2) O
Multi-level O
cascading O
connection O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
features. O
This O
facilitates O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
instead O
of O
depthwise O
convolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
scheme O
connects O
all O
blocks O

- B-DAT

- B-DAT
ations, O
while O
we O
gather O
it O

- B-DAT
catenated O
at O
the O
end O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
on O
two O
commonly-used O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ters O
than O
ours. O
The O
CARN-M O

- B-DAT

- B-DAT

- B-DAT
ple, O
CARN O
outperforms O
its O
most O

- B-DAT

- B-DAT
ble O
results O
against O
computationally-expensive O
models O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tiple O
scales O
using O
a O
single O

- B-DAT

(- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
the O
multi-scale O
learning O
to O

- B-DAT

- B-DAT
off O
between O
performance O
vs. O
parameters O

- B-DAT

- B-DAT

- B-DAT

337K O
311.0G O
37.66/0.9590 O
33.38/0.9136 O
31.91/0.8962 O
- B-DAT
LapSRN O
[24] O
813K O
29.9G O
37.52/0.9590 O

974K O
225.7G O
37.89/0.9598 O
33.61/0.9160 O
32.08/0.8984 O
- B-DAT
CARN O
(ours) O
1,592K O
222.8G O
37.76/0.9590 O

- B-DAT

337K O
311.0G O
33.74/0.9226 O
29.90/0.8322 O
28.82/0.7980 O
- B-DAT
DRRN O
[35] O
297K O
6,796.9G O
34.03/0.9244 O

1,159K O
120.0G O
34.27/0.9257 O
30.30/0.8399 O
28.97/0.8025 O
- B-DAT
CARN O
(ours) O
1,592K O
118.8G O
34.29/0.9255 O

- B-DAT

337K O
311.0G O
31.55/0.8856 O
28.15/0.7680 O
27.32/0.7253 O
- B-DAT
LapSRN O
[24] O
813K O
149.4G O
31.54/0.8850 O

1,417K O
83.1G O
32.00/0.8931 O
28.49/0.7783 O
27.44/0.7325 O
- B-DAT
SRDenseNet O
[37] O
2,015K O
389.9G O
32.02/0.8934 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cading. O
The O
network O
topologies O
are O

- B-DAT

- B-DAT
tively O
carries O
mid- O
to O
high-level O

- B-DAT

- B-DAT

- B-DAT
resentations, O
the O
CARN O
model O
can O

- B-DAT

- B-DAT

- B-DAT
NG O
without O
global O
cascading. O
CARN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gation, O
and O
thus O
lead O
to O

- B-DAT

- B-DAT
nections O
inside O
the O
residual O
blocks O

- B-DAT
nation O
and O
1×1 O
convolutions, O
it O

- B-DAT

- B-DAT
tions O
in O
the O
cascading O
connection O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eters, O
and O
PSNR O
vs. O
operations O

- B-DAT

- B-DAT
ically. O
For O
example, O
the O
G64 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
form O
SISR O
accurately O
and O
efficiently O

- B-DAT

- B-DAT

- B-DAT
ent. O
Our O
experiments O
show O
that O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gence O
33(5), O
898–916 O
(2011 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
British O
Machine O

- B-DAT

- B-DAT

- B-DAT
scale O
hierarchical O
image O
database. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
(ICCV) O
(2015 O

- B-DAT
telligence O
and O
Statistics O
(2010 O

- B-DAT
works O
with O
pruning, O
trained O
quantization O

- B-DAT
level O
performance O
on O
imagenet O
classification O

- B-DAT
dreetto, O
M., O
Adam, O
H.: O
Mobilenets O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
and O
Pattern O
Recognition O
(CVPR O

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks. O
In: O
Proceedings O

- B-DAT
tion O
Processing O
Systems O
(NIPS) O
(2012 O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ters O
24(8), O
1208–1212 O
(2017 O

- B-DAT

- B-DAT
mentation. O
In: O
Proceedings O
of O
the O

- B-DAT

- B-DAT
ple O
convolution O
neural O
networks. O
In O

- B-DAT

- B-DAT
cal O
image O
segmentation. O
In: O
Proceedings O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work. O
In: O
Proceedings O
of O
the O

- B-DAT
nition O
(CVPR) O
(2017 O

- B-DAT

- B-DAT
tions. O
In: O
Proceedings O
of O
the O

- B-DAT

- B-DAT
resentation. O
IEEE O
transactions O
on O
image O

- B-DAT

benchmark O
datasets, O
including O
Set5 O
[5], O
Set14 B-DAT
[29], O
BSD100 O
[20], O
and O
Urban100 O

Scale O
Method O
# O
params O
Set5 O
Set14 B-DAT
BSD100 O
Urban100 O
PSNR O
/ O
SSIM O

uated O
on O
the O
Set5 O
[5], O
Set14 B-DAT
[29], O
BSD100 O
[20], O
and O
Urban100 O

Ours) O
(32.50 O
/ O
0.8640)lenna O
from O
Set14 B-DAT
(×4 O

as O
nearest-neighbor, O
bilinear, O
and O
bicubic O
upscaling B-DAT

the O
proposed O
methods O
for O
an O
upscaling B-DAT
factor O
of O
2 O
on O
the O

the O
com- O
putational O
complexity O
than O
upscaling B-DAT
at O
the O
initial O
stage O
[3,6,12 O

a O
recur- O
sive O
manner, O
and O
upscaling B-DAT

image O
is O
obtained O
from O
the O
upscaling B-DAT
module O

initial O
feature O
extraction, O
RRB, O
and O
upscaling B-DAT
parts. O
On O
the O
other O
hand O

status O
are O
inputted O
to O
the O
upscaling B-DAT
part. O
We O
investigate O
the O
effectiveness O

23]. O
For O
instance, O
in O
the O
upscaling B-DAT
part O
by O
a O
factor O
of O

is O
not O
used O
in O
the O
upscaling B-DAT
part O

the O
frequency O
of O
the O
progressive O
upscaling B-DAT

of O
times O
to O
employ O
the O
upscaling B-DAT
part, O
it O
is O
beneficial O
to O

BSRN O
model, O
one O
of O
the O
upscaling B-DAT
paths O
(i.e., O
×2, O
×3, O
and O

single-scale O
BSRN O
models O
having O
an O
upscaling B-DAT
factor O
of O
4, O
which O
are O

on O
the O
latter O
part O
(i.e., O
upscaling B-DAT
part) O
to O
generate O
good O
quality O

i.e., O
how O
many O
times O
the O
upscaling B-DAT
part O
is O
employed), O
which O
is O

In O
our O
proposed O
model, O
the O
upscaling B-DAT
part O
spends O
most O
of O
the O

average O
processing O
time O
spent O
on O
upscaling B-DAT
an O
image O
by O
a O
factor O

super-resolved O
image O
with O
the O
given O
upscaling B-DAT
factor O
for O
each O
method O
is O

Deep O
residual O
network O
with O
enhanced O
upscaling B-DAT
module O
for O
super-resolution. O
In: O
Proceedings O

Scale O
Method O
# O
params O
Set5 O
Set14 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
plexities, O
thus O
some O
recursive O
parameter-sharing O

- B-DAT

- B-DAT

- B-DAT
work. O
By O
taking O
advantage O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
field. O
For O
example, O
Dong O

- B-DAT

- B-DAT
volutional O
neural O
network O
(SRCNN) O
model O

- B-DAT
formance O
in O
comparison O
to O
the O

- B-DAT

- B-DAT
nections O
and O
various O
optimization O
techniques O

- B-DAT

- B-DAT

- B-DAT
matically O
increases O
the O
number O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
hinder O
them O
from O
fully O

- B-DAT
vided O
to O
the O
recurrent O
unit O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
block O
state-based O
recursive O
network O

- B-DAT

- B-DAT
arate O
information O
storage O
to O
keep O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
the O
BSRN O
model O
can O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
CNN, O
which O
enhances O
the O
interpolated O

- B-DAT

- B-DAT
lutional O
layers O
to O
improve O
the O

- B-DAT

- B-DAT
tion O
layer O
for O
16 O
times O

- B-DAT

- B-DAT
processing O
part O

- B-DAT

- B-DAT
ple, O
Lai O
et O
al. O
[18 O

- B-DAT

- B-DAT

- B-DAT

C O
- B-DAT

C O
- B-DAT

C O
- B-DAT

C O
- B-DAT

- B-DAT

- B-DAT

- B-DAT
putational O
complexity O
than O
upscaling O
at O

- B-DAT
ploying O
multiple O
residual O
connections O
is O

- B-DAT
ages O
[15,24]. O
Third, O
obtaining O
multiple O

- B-DAT
resolution O
model O
and O
combining O
them O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vided O
into O
three O
parts: O
initial O

- B-DAT
sive O
manner, O
and O
upscaling. O
Fig O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ther O
processed O
via O
a O
recursive O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nates O
two O
input O
matrices O
along O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mance: O
increasing O
the O
number O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put, O
where O
the O
later O
outputs O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
by-pixel O
L1 O
loss, O
i.e O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els. O
The O
single-scale O
models O
are O

- B-DAT

- B-DAT

- B-DAT
ods O
across O
different O
scales. O
The O

- B-DAT

- B-DAT
domly O
cropped O
from O
the O
training O

- B-DAT

- B-DAT
scale O
BSRN O
model. O
For O
data O

- B-DAT

- B-DAT
resolved O
images O
are O
obtained O
from O

- B-DAT
eters. O
To O
prevent O
the O
vanishing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bers O
of O
the O
convolutional O
channels O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
termediate O
features O
are O
largely O
different O

- B-DAT
cally O
change, O
even O
though O
the O

- B-DAT

- B-DAT
gressively O
improved O
features O
and O
highly O

- B-DAT
ploying O
the O
block O
state O
(Fig O

- B-DAT

- B-DAT
space O
operation O
and O
increased O
spatial O

- B-DAT

- B-DAT

- B-DAT
ation. O
To O
verify O
this, O
we O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uated O
on O
the O
Set5 O
[5 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sharing O
parts. O
The O
VDSR, O
LapSRN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ber O
of O
model O
parameters O
required O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ters O
small O
enough O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ample, O
our O
method O
successfully O
upscales O

- B-DAT
resolved O
images O

- B-DAT

- B-DAT
plained O
the O
benefits O
and O
efficiency O

- B-DAT
dition, O
comparison O
with O
the O
other O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mawat, O
S., O
Irving, O
G., O
Isard O

- B-DAT

- B-DAT
chine O
learning. O
In: O
Proceedings O
of O

- B-DAT
resolution: O
Dataset O
and O
study. O
In O

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
British O
Machine O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
via O
dual-state O
recurrent O
networks O

- B-DAT
ence O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks. O
In: O
Proceedings O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
puter O
Vision. O
pp. O
416–423 O
(2001 O

- B-DAT

- B-DAT

- B-DAT
ings O
of O
the O
IEEE O
International O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work. O
In: O
Proceedings O
of O
the O

- B-DAT
ing O
13(4), O
600–612 O
(2004 O

- B-DAT

- B-DAT

- B-DAT
computing O
74(17), O
3193–3203 O
(2011 O

- B-DAT

- B-DAT
representations. O
In: O
Proceedings O
of O
the O

- B-DAT

- B-DAT

in O
SR O
benchmarks: O
Set5 O
[2], O
Set14 B-DAT
[32], O
BSD100 O
[20], O
Ur- O
ban100 O

11], O
Manga109 O
[21]. O
The O
Set5, O
Set14, B-DAT
and O
BSD100 O
datasets O
consist O
of O

Set14 B-DAT
33.58 O
33.55 O
33.59 O
33.57 O
33.58 O

Method O
# O
params. O
Set5 O
Set14 B-DAT
BSD100 O
Urban100 O
Manga109PSNR O
/ O
SSIM O

Scale O
Method O
# O
params. O
Set5 O
Set14 B-DAT
BSD100 O
Urban100PSNR O
/ O
SSIM O
PSNR O

For O
the O
upscaling B-DAT
part, O
we O
use O
the O
sub-pixel O

layers O
except O
those O
for O
the O
upscaling B-DAT
part. O
All O
our O
networks O
are O

methods, O
since O
MSRN O
uses O
different O
upscaling B-DAT
methods O
using O
more O
parameters O
for O

Set14 B-DAT
33 I-DAT

Method O
# O
params. O
Set5 O
Set14 B-DAT

Scale O
Method O
# O
params. O
Set5 O
Set14 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ral O
networks O
that O
stands O
out O

- B-DAT
tention O
mechanisms O
to O
single O
image O

- B-DAT

- B-DAT

- B-DAT
wise O
and O
spatial O
attention O
mechanisms O

- B-DAT

- B-DAT
imental O
analysis O
of O
different O
attention O

- B-DAT
formance O
in O
comparison O
to O
existing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O

- B-DAT

- B-DAT

- B-DAT
resolution O
(LR) O
image. O
It O
is O

- B-DAT

- B-DAT

- B-DAT
proved O
performance, O
this O
also O
has O

- B-DAT

- B-DAT
mation O
equally, O
which O
may O
not O

- B-DAT
table O
network O
structures O
in O
various O

- B-DAT
lems O
[8, O
30]. O
It O
allows O

- B-DAT
tracted O
feature O
maps, O
so O
that O

- B-DAT
ploy O
attention O
mechanisms. O
Zhang O
et O

- B-DAT
level O
vision O
problem O
[8] O
without O

- B-DAT

- B-DAT

- B-DAT
timized O
for O
SR, O
are O
attached O

- B-DAT

- B-DAT
RAM). O
The O
proposed O
RAM O
exploits O

- B-DAT
and O
intra- O
channel O
relationship O
by O

- B-DAT
spectively. O
We O
demonstrate O
both O
the O

- B-DAT
ciency O
of O
our O
proposed O
method O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ual O
attention O
module O
(RAM) O
based O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tiveness O
and O
efficiency O
of O
our O

- B-DAT

- B-DAT

- B-DAT
ual O
(CSAR) O
block O
[9]. O
Targeting O

- B-DAT
tion O
module O
that O
can O
be O

- B-DAT
nels O
(CA) O
or O
spatial O
regions O

- B-DAT
tention O
map O
M, O
having O
a O

- B-DAT
erated O
attention O
map O
is O
normalized O

- B-DAT

- B-DAT
matically O
in O
Table O
1, O
which O

- B-DAT

- B-DAT

- B-DAT
nism O
aims O
to O
recalibrate O
filter O

- B-DAT
channel O
correlation, O
i.e., O
CA. O
The O

- B-DAT
plied O
in O
the O
squeeze O
process O

- B-DAT
channel O
relation O
for O
refining O
feature O

- B-DAT
ploits O
both O
inter-channel O
and O
inter-spatial O

- B-DAT
ing O
is O
additionally O
performed O
in O

- B-DAT
cess. O
For O
the O
SA O
module O

- B-DAT
tially O
performs O
CA O
and O
then O

- B-DAT
volutions, O
where O
the O
first O
one O

- B-DAT
tio. O
While O
CBAM O
combines O
the O

- B-DAT
spired O
by O
EDSR O
[19], O
is O

CSAR O
[9] O
- B-DAT
M O
= O
conv1×1(conv1×1(X)) O
RAM O

RCAB O
[33] O
- B-DAT
CBAM O
[30] O
X̂ O
= O
fSA(fCA(X O

- B-DAT

- B-DAT

- B-DAT
scaling O
part. O
Let O
ILR O
and O

- B-DAT
tion O
3.2. O
F0 O
is O
updated O

- B-DAT
tion, O
and O
then O
the O
updated O

- B-DAT

- B-DAT

- B-DAT
struction O

- B-DAT
ing O
and O
reconstruction, O
respectively, O
and O

- B-DAT

- B-DAT
ditionally O
propose O
a O
way O
to O

- B-DAT
pose O
residual O
attention O
module O
(RAM O

- B-DAT

- B-DAT
lution, O
ReLU, O
and O
convolution, O
and O

- B-DAT
posed O
FA O
mechanism O

- B-DAT

- B-DAT
puter O
vision O
problems O
such O
as O

- B-DAT
ject O
detection O
without O
modification. O
However O

- B-DAT
mately O
aims O
at O
restoring O
high-frequency O

- B-DAT
ages, O
it O
is O
more O
reasonable O

- B-DAT
mined O
using O
high-frequency O
statistics O
about O

- B-DAT
ters O
will O
extract O
the O
edge O

- B-DAT
tion. O
From O
the O
viewpoint O
of O

- B-DAT
nels O
varies O
by O
the O
spatial O

- B-DAT
frequency O
components O
such O
as O
sky O

- B-DAT
like O
CBAM O
[30], O
which O
performs O

- B-DAT
formation O
per O
channel O
to O
preserve O

- B-DAT

- B-DAT
teristics. O
In O
addition, O
for O
the O

- B-DAT
trast O
to O
other O
SA O
mechanisms O

- B-DAT

- B-DAT

- B-DAT
volution O

- B-DAT
anisms O
exploit O
information O
from O
inter-channel O

- B-DAT
channel O
relationship, O
respectively. O
Therefore, O
in O

- B-DAT

- B-DAT
ered O
in O
detail O
in O
Section O

- B-DAT
ban100 O
[11], O
Manga109 O
[21]. O
The O

- B-DAT

- B-DAT
ferent O
characteristics O
from O
natural O
ones O

- B-DAT
to-noise O
ration O
(PSNR) O
and O
structural O

- B-DAT
dex O
on O
the O
Y O
channel O

- B-DAT

- B-DAT
domly O
crop O
a O
48×48 O
patch O

- B-DAT
lected O
16 O
LR O
training O
images O

- B-DAT

- B-DAT
ing O
images O
for O
each O
RGB O

- B-DAT
ment O
our O
networks O
using O
the O

- B-DAT
ploying O
each O
mechanism O
increases O
the O

- B-DAT
rameters, O
and O
as O
the O
network O

- B-DAT
ment O
with O
the O
easiest O
case O

- B-DAT
acteristics O
to O
check O
the O
generalization O

- B-DAT
wise O
and O
spatial O
information O
is O

- B-DAT
nism O
leads O
to O
performance O
improvement O

- B-DAT
age) O
only O
by O
adding O
9K O

- B-DAT
ages O
and O
the O
images O
in O

- B-DAT
tics O
of O
computer-generated O
images, O
which O

- B-DAT
ent O
from O
those O
in O
the O

- B-DAT
mance O
improvement O
is O
achieved O
only O

- B-DAT
pare O
it O
with O
the O
other O

- B-DAT
lustrated O
in O
Section O
2. O
For O

- B-DAT
mance. O
Overall, O
all O
the O
cases O

- B-DAT
ters O
than O
our O
network O
(+257K O

- B-DAT
ods O
of O
the O
squeeze O
process O

- B-DAT
served O
that O
our O
method O
extracting O

- B-DAT

- B-DAT
ing O
images, O
i.e., O
Urban100 O
and O

- B-DAT

- B-DAT
wise O
convolution O
is O
an O
effective O

- B-DAT
ditionally O
used, O
but O
at O
the O

- B-DAT
ure O
4. O
We O
have O
three O

- B-DAT
nisms O
showing O
higher O
performance O
have O

- B-DAT
ing O
the O
corresponding O
blocks O
only O

- B-DAT
nism. O
3) O
Our O
FA O
mechanism O

- B-DAT
ods, O
and O
those O
marked O
with O

- B-DAT
els O
with O
varying O
the O
number O

- B-DAT
scale O
SR O
are O
marked O
with O

- B-DAT
scale O
SR O
are O
marked O
with O

- B-DAT
tive O
(red O
color) O
and O
negative O

- B-DAT

- B-DAT
ground) O
and O
the O
high-frequency O
components O

- B-DAT
frequency O
component O
values O
to O
zero O

- B-DAT
tention” O
to O
the O
high-frequency O
components O

- B-DAT

- B-DAT
termined O
by O
the O
number O
of O

- B-DAT
tional O
parameters O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ness O
of O
RAM O
on O
large O

- B-DAT
responds O
to O
the O
opposite O
case O

- B-DAT
RAM O
R10C64, O
64×64 O
patches O
and O

- B-DAT

- B-DAT
rized O
in O
Table O
3, O
Table O

- B-DAT
ods O
widens O
further O
in O
Urban100 O

- B-DAT

- B-DAT
sual O
results O
of O
challenging O
images O

- B-DAT
mance O
with O
fewer O
parameters O
than O

- B-DAT
strates O
the O
efficiency O
of O
ours O

- B-DAT

- B-DAT
anisms O
are O
integrated O
in O
our O

- B-DAT
strated O
that O
our O
attention O
methods O

- B-DAT
tion O
mechanisms O
for O
SR O
and O

- B-DAT

- B-DAT

- B-DAT
work. O
In O
Proceedings O
of O
the O

- B-DAT
puter O
Vision O
(ECCV), O
2018. O
1 O

- B-DAT
Morel. O
Low-complexity O
single-image O
super-resolution O
based O

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
European O
Conference O

- B-DAT

- B-DAT

- B-DAT
works. O
In O
Proceedings O
of O
the O

- B-DAT
jection O
networks O
for O
super-resolution. O
In O

- B-DAT
tion O
(CVPR), O
2018. O
1, O
7 O

- B-DAT
cient O
convolutional O
neural O
networks O
for O

- B-DAT
cations. O
arXiv O
preprint O
arXiv:1704.04861, O
2017 O

- B-DAT

- B-DAT

- B-DAT
works. O
In O
Proceedings O
of O
the O

- B-DAT

- B-DAT
resolution. O
arXiv O
preprint O
arXiv:1809.11130, O
2018 O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
(CVPR), O
2016. O
1 O

- B-DAT
mization. O
arXiv O
preprint O
arXiv:1412.6980, O
2014 O

- B-DAT
resolution. O
In O
Proceedings O
of O
the O

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT
tive O
adversarial O
network. O
In O
Proceedings O

- B-DAT
ence O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cal O
statistics. O
In O
Proceedings O
of O

- B-DAT
ference O
on O
Computer O
Vision O
(ICCV O

- B-DAT
masaki, O
and O
K. O
Aizawa. O
Sketch-based O

- B-DAT
ing O
manga109 O
dataset. O
Multimedia O
Tools O

- B-DAT
ceedings O
of O
the O
Advances O
in O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
IEEE O
International O

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT

- B-DAT
lutional O
block O
attention O
module. O
In O

- B-DAT
pean O
Conference O
on O
Computer O
Vision O

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT

- B-DAT

- B-DAT
tional O
Conference O
on O
Curves O
and O

- B-DAT

- B-DAT
ual O
dense O
network O
for O
image O

- B-DAT

- B-DAT
ings O
of O
the O
IEEE O
Conference O

bench- O
mark O
datasets O
Set5 O
[3], O
Set14 B-DAT
[69] O
and O
BSD100, O
the O
testing O

of O
each O
image O
on O
Set5, O
Set14 B-DAT
and O
BSD100: O
nearest O
neighbor O
(NN O

adversarial O
networks O
on O
Set5 O
and O
Set14 B-DAT
benchmark O
data. O
MOS O
score O
significantly O

Set14 B-DAT
PSNR O
28.49 O
27.19 O
26.92 O
26.44 O

SRGAN O
and O
SRResNet O
variants O
on O
Set14 B-DAT
in O
terms O
of O
MOS. O
We O

Set14 B-DAT
PSNR O
24.64 O
25.99 O
27.18 O
27.45 O

MOS O
tests O
conducted O
on O
Set5, O
Set14, B-DAT
BSD100 O
are O
summarized O
in O
Section O

factor O
for O
Set5 O
(Section O
A.4), O
Set14 B-DAT
(Section O
A.5) O
and O
five O
randomly O

reconstructions O
of O
the O
baboon O
from O
Set14 B-DAT
appear O
closer O
to O
the O
reference O

versions O
of O
images O
from O
Set5, O
Set14 B-DAT
and O
BSD100. O
On O
BSD100 O
nine O

each O
rater. O
On O
Set5 O
and O
Set14 B-DAT
the O
raters O
also O
rated O
three O

comparably O
little O
detail, O
ratings O
on O
Set14 B-DAT
and O
especially O
on O
the O
large O

Set5 O
Set14 B-DAT
BSD100 O

of O
MOS O
scores O
on O
Set5, O
Set14, B-DAT
BSD100. O
Mean O
shown O
as O
red O

Set5 O
Set14 B-DAT
BSD100 O

10: O
Average O
rank O
on O
Set5, O
Set14, B-DAT
BSD100 O
by O
averaging O
the O
ranks O

A.5. O
Set14 B-DAT
- O
Visual O
Results O

Figure O
12: O
Results O
for O
Set14 B-DAT
using O
bicubic O
interpolation, O
SRResNet O
and O

Figure O
13: O
Results O
for O
Set14 B-DAT
using O
bicubic O
interpolation O
, O
SRResNet O

Figure O
14: O
Results O
for O
Set14 B-DAT
using O
bicubic O
interpolation, O
SRResNet O
and O

when O
we O
super-resolve O
at O
large O
upscaling B-DAT
factors? O
The O
behavior O
of O
optimization-based O

photo-realistic O
natural O
images O
for O
4× O
upscaling B-DAT
factors. O
To O
achieve O
this, O
we O

guishable O
from O
original O
(right). O
[4× O
upscaling B-DAT

is O
particularly O
pronounced O
for O
high O
upscaling B-DAT
factors, O
for O
which O
texture O
detail O

are O
shown O
in O
brackets. O
[4× O
upscaling B-DAT

super- O
resolved O
with O
a O
4× O
upscaling B-DAT
factor O
is O
shown O
in O
Figure O

the O
network O
to O
learn O
the O
upscaling B-DAT
filters O
directly O
can O
further O
increase O

was O
also O
shown O
that O
learning O
upscaling B-DAT
filters O
is O
beneficial O
in O
terms O

super-resolves O
face O
images O
with O
large O
upscaling B-DAT
factors O
(8×). O
GANs O
were O
also O

for O
image O
SR O
with O
high O
upscaling B-DAT
factors O
(4×) O
as O
measured O
by O

photo-realistic O
SR O
images O
with O
high O
upscaling B-DAT
factors O
(4 O

losses O
in O
that O
category∗. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

HR O
image O
(right: O
i,j). O
[4× O
upscaling B-DAT

SSIM, O
MOS) O
in O
bold. O
[4× O
upscaling B-DAT

that O
SRGAN O
reconstructions O
for O
large O
upscaling B-DAT
factors O
(4×) O
are, O
by O
a O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
IEEE O

and O
SRGAN O
with O
a O
4× O
upscaling B-DAT
factor O
for O
Set5 O
(Section O
A.4 O

low-/high-resolution O
images O
and O
reconstructions O
(4× O
upscaling) B-DAT
obtained O
with O
different O
methods O
(bicubic O

image O
with O
resolution O
64×64 O
with O
upscaling B-DAT
factor O
4×. O
The O
measurements O
are O

for O
another O
100k O
iterations. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

all O
available O
individual O
ratings. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

of O
each O
image O
on O
Set5, O
Set14 B-DAT

adversarial O
networks O
on O
Set5 O
and O
Set14 B-DAT

Set14 B-DAT

SRGAN O
and O
SRResNet O
variants O
on O
Set14 B-DAT

Set14 B-DAT

reconstructions O
of O
the O
baboon O
from O
Set14 B-DAT

versions O
of O
images O
from O
Set5, O
Set14 B-DAT

each O
rater. O
On O
Set5 O
and O
Set14 B-DAT

comparably O
little O
detail, O
ratings O
on O
Set14 B-DAT

Set5 O
Set14 B-DAT

Set5 O
Set14 B-DAT

Figure O
12: O
Results O
for O
Set14 B-DAT

Figure O
13: O
Results O
for O
Set14 B-DAT

Figure O
14: O
Results O
for O
Set14 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks, O
one O
central O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
(SR). O
To O
our O
knowledge O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
guishable O
from O
original O
(right). O
[4 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
inal O
image O
means O
that O
the O

- B-DAT
realistic O
as O
defined O
by O
Ferwerda O

- B-DAT

- B-DAT

- B-DAT
ing O
high-level O
feature O
maps O
of O

- B-DAT

- B-DAT
resolved O
with O
a O
4× O
upscaling O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
image O
informa- O
tion O

- B-DAT

- B-DAT
proaches O
to O
the O
SR O
problem O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
[27], O
trees O
[46] O
or O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-the-art O
SR O
performance. O
Subsequently, O
it O

- B-DAT

- B-DAT

- B-DAT
ciently O
train O
these O
deeper O
network O

- B-DAT
normalization O
[32] O
is O
often O
used O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
results. O
Another O
powerful O
design O

- B-DAT

- B-DAT
connections O
relieve O
the O
network O
architecture O

- B-DAT
tentially O
non-trivial O
to O
represent O
with O

- B-DAT

- B-DAT

- B-DAT
ing O
pixel-wise O
averages O
of O
plausible O

- B-DAT

- B-DAT
ity O
[42, O
33, O
13, O
5 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
perceptually O
more O
convincing O
solutions O

- B-DAT
ure O
2. O
We O
illustrate O
the O

- B-DAT
ure O
3 O
where O
multiple O
potential O

- B-DAT
tion. O
Yu O
and O
Porikli O
[66 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions. O
Similar O
to O
this O
work O

- B-DAT
trained O
VGG O
network O
instead O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity. O
The O
GAN O
procedure O
encourages O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mark O
datasets O
as O
well O
as O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
counterpart O
IHR. O
The O
high-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nels O
as O
in O
the O
VGG O

- B-DAT
ical O
for O
the O
performance O
of O

- B-DAT
tent O
loss O
lSRX O
and O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
solutions O
with O
overly O
smooth O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stead O
of O
log[1−DθD O
(GθG(ILR))] O
[22 O

- B-DAT
mark O
datasets O
Set5 O
[3], O
Set14 O

- B-DAT
and O
high-resolution O
images. O
This O
corresponds O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bor, O
bicubic, O
SRCNN O
[9] O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gral O
score O
from O
1 O
(bad O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG54 O
and O
the O
original O
HR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ResNet O
and O
the O
adversarial O
networks O

- B-DAT
SRGAN- O
Set5 O
MSE O
VGG22 O
MSE O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
MSE-based O
reconstructions, O
to O
those O
competing O

- B-DAT

- B-DAT
formed O
other O
SRGAN O
and O
SRResNet O

- B-DAT

- B-DAT
GAN O
to O
NN, O
bicubic O
interpolation O

- B-DAT

- B-DAT

- B-DAT
art O
methods. O
Quantitative O
results O
are O

- B-DAT
resolved O
with O
SRResNet O
and O
SRGAN O

- B-DAT
realistic O
image O
SR. O
All O
differences O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentary O
material). O
We O
further O
found O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Net O
that O
sets O
a O
new O

- B-DAT
sure. O
We O
have O
highlighted O
some O

- B-DAT

- B-DAT
ial O
loss O
by O
training O
a O

- B-DAT

- B-DAT

- B-DAT
the-art O
reference O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Stevenson. O
Super-Resolution B-DAT
from O
Image O
Sequences O
- O
A O
Review. O
Midwest O
Symposium O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
by O
adaptive O
sparse O
domain O

- B-DAT
ization. O
IEEE O
Transactions O
on O
Image O

- B-DAT

- B-DAT
resolution. O
IEEE O
Computer O
Graphics O
and O

- B-DAT
level O
vision. O
International O
Journal O
of O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

- B-DAT
line O
at O
http://torch.ch/blog/2016/02/04/resnets. O
html. O
2016 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In O
European O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT
tion O
with O
deep O
convolutional O
neural O

- B-DAT

- B-DAT

- B-DAT
mentation O
algorithms O
and O
measuring O
ecological O

- B-DAT

- B-DAT

- B-DAT
sive O
survey. O
In O
Machine O
Vision O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
for O
fast O
example-based O
super-resolution O

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
(ACCV O

- B-DAT

- B-DAT

- B-DAT
Resolution O
via O
Deep O
and O
Shallow O

- B-DAT

- B-DAT

- B-DAT
ence O
on O
Signals, O
Systems O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
derstanding O
Neural O
Networks O
Through O
Deep O

International O
Conference O
on O
Machine O
Learning O
- B-DAT
Deep O
Learning O
Workshop O
2015, O
page O

- B-DAT

- B-DAT
resolution O
by O
retrieving O
web O
images O

- B-DAT
volutional O
networks. O
In O
European O
Conference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG22, O
SRGAN-VGG54) O
described O
in O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
off O
between O
accuracy O
and O
speed O

-100 B-DAT
thousand O
update O
iterations O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

A.4. O
Set5 O
- B-DAT
Visual O
Results O

A.5. O
Set14 O
- B-DAT
Visual O
Results O

A.6. O
BSD100 O
(five O
random O
samples) O
- B-DAT
Visual O
Results O

result O
produced O
by O
ENet-PAT O
at O
4x B-DAT
super-resolution O
on O
an O
image O
from O

fully O
convolutional O
network O
architecture O
for O
4x B-DAT
super-resolution O
which O
only O
learns O
the O

an O
image O
from O
ImageNet O
for O
4x B-DAT
super-resolution. O
Despite O
reaching O
state-of-the-art O
results O

trained O
with O
different O
losses O
at O
4x B-DAT
super-resolution O
on O
images O
from O
ImageNet O

different O
combinations O
of O
losses O
at O
4x B-DAT
super O
resolution. O
ENet-E O
yields O
the O

methods O
with O
our O
results O
at O
4x B-DAT
super-resolution O
on O
an O
image O
from O

PSNR O
for O
different O
methods O
at O
4x B-DAT
super-resolution. O
ENet-E O
achieves O
state-of-the-art O
results O

both O
ENet-E O
and O
ENet-PAT O
at O
4x B-DAT
super- O
resolution O
side-by-side, O
and O
were O

an O
image O
from O
BSD100 O
at O
4x B-DAT
super-resolution. O
ENet- O
PAT’s O
result O
looks O

on O
average O
per O
image O
at O
4x B-DAT
super-resolution O

adversarial O
discrimina- O
tive O
network O
at O
4x B-DAT
super-resolution. O
As O
in O
the O
generative O

on O
average O
per O
image O
at O
4x B-DAT
super-resolution O
on O
Set5/Set14, O
though O
EnhanceNet O

the O
result O
of O
ENet-PAT O
at O
4x B-DAT
super-resolution O
with O
the O
current O
state O

super-resolution O
in O
Fig. O
4. O
Although O
4x B-DAT
super-resolution O
is O
a O
greatly O
more O

are O
lost O
completely O
in O
the O
4x B-DAT
downsampled O
image O
are O
more O
accurate O

image O
with O
sharper O
textures O
at O
4x B-DAT
super-resolution O
that O
even O
out- O
performs O

on O
images O
from O
ImageNet O
at O
4x B-DAT
super-resolution. O
Computing O
the O
texture O
matching O

edges O
and O
overly O
smooth O
textures O
(4x B-DAT
super-resolution). O
Furthermore, O
these O
models O
are O

that O
the O
network O
produces O
at O
4x B-DAT
super-resolution. O
While O
ENet-E O
significantly O
sharpens O

downsampled O
input O
2x O
downsampled O
input O
4x B-DAT
downsampled O
input O
IHR O

VDSR O
[7] O
2x O
DRCN O
[8] O
4x B-DAT
ENet-PAT O
IHR O

missing) O
with O
our O
model O
at O
4x B-DAT
super-resolution O
(93.75% O
of O
all O
pixels O

Bruna O
et O
al. O
[2] O
at O
4x B-DAT
super-resolution. O
ENet-PAT O
produces O
images O
with O

different O
methods O
at O
2x O
and O
4x B-DAT
super-resolution. O
Similar O
to O
PSNR, O
ENet-PAT O

IFC O
for O
different O
methods O
at O
4x B-DAT
super-resolution. O
Best O
performance O
shown O
in O

an O
image O
from O
ImageNet O
at O
4x B-DAT
super-resolution. O
While O
producing O
an O
overall O

on O
images O
of O
faces O
at O
4x B-DAT
super O
resolution. O
ENet-PAT O
produces O
artifacts O

Set14 B-DAT
26.00 O
28.42 O
25.64 O
25.94 O
24.93 O

on O
the O
zebra O
image O
from O
Set14 B-DAT
which O
is O
particularly O
well-suited O
for O

super-resolution O
on O
an O
image O
from O
Set14 B-DAT

30.62 O
30.90 O
31.53 O
31.35 O
31.74 O
Set14 B-DAT
26.00 O
27.24 O
27.32 O
27.40 O
27.49 O

images O
in O
9ms O
(Set5), O
18ms O
(Set14), B-DAT
12ms O
(BSD100) O
and O
59ms O
(Urban100 O

Set14, B-DAT
though O
EnhanceNet O
runs O
on O
a O

36.88 O
37.63 O
37.53 O
37.32 O
33.89 O
Set14 B-DAT
30.24 O
32.26 O
27.24 O
32.22 O
32.42 O

0.9559 O
0.9588 O
0.9587 O
0.9581 O
0.9276 O
Set14 B-DAT
0.8688 O
0.9040 O
0.9056 O
0.9034 O
0.9063 O

0.8678 O
0.8854 O
0.8838 O
0.8869 O
0.8082 O
Set14 B-DAT
0.7027 O
0.7451 O
0.7491 O
0.7518 O
0.7503 O

3.379 O
3.554 O
3.553 O
3.413 O
2.643 O
Set14 B-DAT
2.237 O
2.919 O
2.751 O
2.893 O
2.751 O

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
TOG O

rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

Set14 B-DAT
26 I-DAT

on O
the O
zebra O
image O
from O
Set14 B-DAT

30.62 O
30.90 O
31.53 O
31.35 O
31.74 O
Set14 B-DAT
26 I-DAT

36.88 O
37.63 O
37.53 O
37.32 O
33.89 O
Set14 B-DAT
30 I-DAT

0.9559 O
0.9588 O
0.9587 O
0.9581 O
0.9276 O
Set14 B-DAT
0 I-DAT

0.8678 O
0.8854 O
0.8838 O
0.8869 O
0.8082 O
Set14 B-DAT
0 I-DAT

3.379 O
3.554 O
3.553 O
3.413 O
2.643 O
Set14 B-DAT
2 I-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put. O
Traditionally, O
the O
performance O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
quality. O
As O
a O
result O

- B-DAT
rics O
tend O
to O
produce O
over-smoothed O

- B-DAT
frequency O
textures O
and O
do O
not O

- B-DAT
thesis O
in O
combination O
with O
a O

- B-DAT
accurate O
reproduction O
of O
ground O
truth O

- B-DAT
ing. O
By O
using O
feed-forward O
fully O

- B-DAT
works O
in O
an O
adversarial O
training O

- B-DAT
nificant O
boost O
in O
image O
quality O

- B-DAT
fectiveness O
of O
our O
approach, O
yielding O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cent O
years. O
The O
problem O
is O

- B-DAT

- B-DAT
ferent O
HR O
images O
can O
give O

- B-DAT

- B-DAT

- B-DAT
lem O
becomes O
worse, O
rendering O
SISR O

- B-DAT
lem. O
Despite O
considerable O
progress O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
are O
still O
far O
from O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ploy: O
most O
systems O
minimize O
the O

- B-DAT

- B-DAT
construction O
from O
the O
LR O
observation O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
network O
architecture, O
we O

- B-DAT
bination O
with O
adversarial O
training O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
perceptual O
metrics O

- B-DAT
zos O
[11] O
are O
based O
on O

- B-DAT

- B-DAT
lahi O
and O
Moeslund O
[37] O
and O

- B-DAT
based O
models O
that O
either O
exploit O

- B-DAT
ent O
scales O
within O
a O
single O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works O
to O
the O
task O
of O

- B-DAT
gration O
to O
learn O
a O
mapping O

- B-DAT

- B-DAT
tion, O
the O
results O
tend O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ject O
recognition O
system O
like O
VGG O

- B-DAT

- B-DAT

- B-DAT
tion O
with O
an O
adversarial O
network O

- B-DAT
sually O
implausible O
artifacts O
without O
the O

- B-DAT

- B-DAT

- B-DAT
ation O
d O
is O
non-injective O
and O

- B-DAT

- B-DAT
ity O
in O
SISR: O
since O
downsampling O

- B-DAT

- B-DAT
fore, O
even O
state-of-the-art O
models O
learn O

- B-DAT
ple O
in O
Fig. O
2, O
where O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lutional O
layers O
enables O
training O
of O

- B-DAT
put O
image O
of O
arbitrary O
size O

- B-DAT

- B-DAT

- B-DAT
ported O
to O
produce O
checkerboard O
artifacts O

- B-DAT
zontal O
bars O
of O
1×2 O
pixels O

- B-DAT
not O
be O
distinguished O
anymore O
since O

- B-DAT

- B-DAT
pling O
of O
the O
feature O
activations O

- B-DAT

- B-DAT
volution O
layer O
after O
all O
upsampling O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puting O
distances O
in O
image O
space O

- B-DAT

- B-DAT

- B-DAT
tion O
of O
the O
popular O
VGG-19 O

- B-DAT
ally O
decrease O
the O
spatial O
dimension O

- B-DAT
tract O
higher-level O
features O
in O
higher O

- B-DAT

- B-DAT

- B-DAT
atively O
by O
matching O
statistics O
extracted O

- B-DAT

- B-DAT
tween O
the O
feature O
activations O
φ(I O

- B-DAT

- B-DAT
ages O
[24, O
53], O
however O
a O

- B-DAT
resolution O
textures O
during O
inference, O
we O

- B-DAT
ture O
loss O
LT O
patch-wise O
during O

- B-DAT
fore O
learns O
to O
produce O
images O

- B-DAT
tures O
as O
the O
high-resolution O
images O

- B-DAT

- B-DAT

- B-DAT
tures. O
Empirically, O
we O
found O
a O

- B-DAT
ation O
and O
the O
overall O
perceptual O

- B-DAT
imize O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
inative O
network O
as O
we O
found O

- B-DAT
tor O
from O
overpowering O
the O
generator O

- B-DAT
ing O
learning O
strategy O
yields O
better O

- B-DAT
vious O
training O
batch O
and O
only O

- B-DAT
ther O
details O
are O
specified O
in O

- B-DAT
ously O
introduced O
loss O
functions. O
After O

- B-DAT
tive O
and O
quantitative O
evaluation O
of O

- B-DAT
tors O
are O
given O
in O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formations, O
the O
network O
is O
given O

- B-DAT
alistic O
textures O
when O
trained O
with O

- B-DAT

- B-DAT
work O
sometimes O
produces O
unpleasing O
high-frequency O

- B-DAT

- B-DAT

- B-DAT
PAT O
produces O
perceptually O
more O
realistic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
struction, O
but O
even O
the O
state-of-the-art O

- B-DAT

- B-DAT
teristics O
as O
previous O
approaches. O
The O

- B-DAT
age O
than O
ENet-E. O
On O
the O

- B-DAT

- B-DAT
alistic O
textures. O
Comparisons O
with O
further O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pose O
to O
use O
the O
performance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nition O
models O
as O
a O
metric O

- B-DAT

- B-DAT
plement O
pixel-based O
benchmarks O
such O
as O

- B-DAT

- B-DAT

- B-DAT
ric O
similar O
to O
ours O
to O

- B-DAT

-50 B-DAT
[6, O
20] O
as O
this O
class O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lenge O
(ILSVRC) O
[44]. O
For O
the O

- B-DAT

- B-DAT
bels. O
The O
original O
images O
are O

-1 B-DAT
and O
top-5 O
errors O
as O
well O

- B-DAT
fications. O
The O
results O
are O
shown O

- B-DAT
ison, O
some O
of O
the O
results O

- B-DAT
formance O
followed O
by O
DRCN O
[26 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tual O
quality O
which O
is O
reflected O

- B-DAT
ject O
recognition O
benchmark O
matches O
human O

- B-DAT
ter O
than O
PSNR O
does. O
The O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mance O
roughly O
coincides O
with O
the O

- B-DAT
age O
quality O
in O
this O
benchmark O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
imize O
the O
Euclidean O
loss, O
we O

- B-DAT

- B-DAT
erated O
by O
ENet-PAT O
which O
have O

- B-DAT
ages O
upsampled O
with O
bicubic O
interpolation O

- B-DAT

- B-DAT

- B-DAT
resolution O
side-by-side, O
and O
were O
asked O

- B-DAT
age O
produced O
by O
ENet-PAT O
91.0 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
error O
0.506 O
0.477 O
0.454 O
0.449 O

-5 B-DAT
error O
0.266 O
0.242 O
0.224 O
0.214 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
PAT’s O
result O
looks O
more O
natural O

- B-DAT
lutional O
network O
at O
test O
time O

- B-DAT
gence O
rates O
depend O
on O
the O

- B-DAT
tions. O
Although O
not O
optimized O
for O

- B-DAT

- B-DAT
ducing O
state-of-the-art O
results O
by O
both O

- B-DAT
itative O
measures O
by O
training O
with O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tic, O
they O
do O
not O
match O

- B-DAT
wise O
basis. O
Furthermore, O
the O
adversarial O

- B-DAT
eas. O
This O
is O
a O
result O

- B-DAT
work O
and O
apply O
shrinking O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tails O
and O
additional O
comparisons. O
A O

- B-DAT
tation O
of O
ENet-PAT O
can O
be O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
image O
models O
using O
a O

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
ceptual O
similarity O
metrics O
based O
on O

- B-DAT

- B-DAT

- B-DAT
based O
super-resolution. O
IEEE O
CG&A, O
22(2):56–65 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
nets. O
In O
NIPS O

- B-DAT
tional O
neural O
networks O
for O
direct O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
istration. O
CVGIP: O
Graphical O
models O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
sparse O
regression O
and O
natural O

- B-DAT
ceptual O
image O
quality O
assessment O
using O

- B-DAT
cian O
pyramid. O
Electronic O
Imaging, O
2016(16):1–6 O

- B-DAT
jani, O
J. O
Totz, O
Z. O
Wang O

- B-DAT

- B-DAT
age O
super-resolution O
using O
a O
generative O

- B-DAT
resolved O
faces O
for O
improved O
face O

- B-DAT
lance O
video. O
In O
ICB, O
2007 O

- B-DAT
manan, O
P. O
Dollár, O
and O
C O

- B-DAT
mon O
objects O
in O
context. O
In O

- B-DAT
strained O
sparse O
coding O
for O
single O

- B-DAT

- B-DAT
earities O
improve O
neural O
network O
acoustic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
checkerboard/, O
2016 O

- B-DAT

- B-DAT

- B-DAT
sentation O
learning O
with O
deep O
convolutional O

- B-DAT
sarial O
networks. O
In O
ICLR, O
2016 O

- B-DAT
tion O
based O
noise O
removal O
algorithms O

- B-DAT

- B-DAT

- B-DAT
mation O
fidelity O
criterion O
for O
image O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT
structure O
preserving O
image O
super O
resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ture O
networks: O
Feed-forward O
synthesis O
of O

- B-DAT
ized O
images. O
In O
ICML, O
2016 O

- B-DAT

- B-DAT
ment O
photographs. O
In O
ECCV, O
2016 O

- B-DAT
similarities O
for O
single O
frame O
super-resolution O

- B-DAT

- B-DAT
resolution: O
a O
benchmark. O
In O
ECCV O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
as O
sparse O
representation O
of O

- B-DAT
age O
super-resolution O
via O
sparse O
representation O

- B-DAT

- B-DAT
inative O
generative O
networks. O
In O
ECCV O

- B-DAT
age O
super-resolution O
by O
retrieving O
web O

- B-DAT

- B-DAT

- B-DAT
tion. O
In O
ECCV, O
2016 O

- B-DAT
fold. O
In O
ECCV, O
2016 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cally O
similar O
textures O
between O
Iest O

- B-DAT
tween O
faithful O
texture O
generation O
and O

- B-DAT

- B-DAT

-4 B-DAT

- B-DAT

-128 B-DAT

- B-DAT
age O
since O
the O
network O
is O

- B-DAT
pleasant O
results O

- B-DAT
sarial O
network O
used O
for O
the O

- B-DAT
mon O
design O
patterns O
[13] O
and O

- B-DAT
tive O
network O
at O
4x O
super-resolution O

- B-DAT
tions O
[11] O
and O
strided O
convolutions O

- B-DAT
duce O
a O
classification O
label O
between O

- B-DAT
put O
which O
renders O
training O
more O

- B-DAT

- B-DAT
ual O
image O
that O
cancel O
out O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
which O
leads O
to O
loss O

- B-DAT

- B-DAT
EA O
and O
ENet-EAT O
are O
shown O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tor. O
ENet-PAT O
is O
the O
only O

- B-DAT
tails O
and O
it O
is O
visually O

- B-DAT
hanceNet O
is O
even O
faster O
than O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pare O
the O
result O
of O
ENet-PAT O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
manding O
task O
than O
2x O
super-resolution O

- B-DAT
parable O
in O
quality. O
Small O
details O

- B-DAT

- B-DAT
performs O
the O
current O
state O
of O

- B-DAT

- B-DAT
PAT’s O
result O
and O
looks O
very O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceptual O
quality O
of O
ENet-PAT’s O
results O

- B-DAT

- B-DAT

-4 B-DAT
ENet-PAT-128 O
ENet-PAT-16 O
(default) O
IHR O

- B-DAT

- B-DAT

- B-DAT

-4 B-DAT

- B-DAT

-128 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
jority O
of O
subjects O
in O
our O

- B-DAT

- B-DAT

- B-DAT
PAT O
trained O
on O
MSCOCO O
struggles O

- B-DAT
cally O
looking O
faces O
at O
high O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
PAT-F O
has O
significantly O
better O
performance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT

- B-DAT
mization. O
2015 O

- B-DAT
earities O
improve O
neural O
network O
acoustic O

- B-DAT

- B-DAT

- B-DAT
sentation O
learning O
with O
deep O
convolutional O

- B-DAT
sarial O
networks. O
In O
ICLR, O
2016 O

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT
mation O
fidelity O
criterion O
for O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
inative O
generative O
networks. O
In O
ECCV O

four O
datasets, O
i.e., O
Set5 O
[7], O
Set14 B-DAT
[56], O
BSD100 O
[38], O
and O
Urban100 O

25 O
and O
50 O
on O
datasets O
Set14, B-DAT
BSD68 O
and O
Urban100. O
Red O
color O

and O
4 O
on O
datasets O
Set5, O
Set14, B-DAT
BSD100 O
and O
Urban100. O
Red O
color O

Set14 B-DAT
×2 O
32.77 O
/ O
0.9109 O
33.03 O

by O
PSNR O
on O
Set5 O
and O
Set14 B-DAT

comparable O
to O
SR- O
ResNet O
on O
Set14 B-DAT

on O
the O
images O
Barbara O
from O
Set14 B-DAT

image O
super-resolution O
results O
of O
“barbara” O
(Set14) B-DAT
with O
upscaling O
factor O
×4 O

image O
super-resolution O
results O
of O
“barbara” O
(Set14) B-DAT
with O
upscaling O
factor O
×4 O

image O
super-resolution O
results O
of O
“barbara” O
(Set14) B-DAT
with O
upscaling O
factor O
×4 O

image O
super-resolution O
results O
of O
“barbara” O
(Set14) B-DAT
with O
upscaling O
factor O
×4 O

results O
of O
“barbara” O
(Set14) O
with O
upscaling B-DAT
factor O
×4 O

results O
of O
“barbara” O
(Set14) O
with O
upscaling B-DAT
factor O
×4 O

results O
of O
“barbara” O
(Set14) O
with O
upscaling B-DAT
factor O
×4 O

results O
of O
“barbara” O
(Set14) O
with O
upscaling B-DAT
factor O
×4 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works O
(CNNs) O
generally O
enlarge O
the O

- B-DAT

- B-DAT

- B-DAT
thermore, O
another O
convolutional O
layer O
is O

- B-DAT
crease O
the O
channels O
of O
feature O

- B-DAT
network, O
inverse O
wavelet O
transform O
is O

- B-DAT
construct O
the O
high O
resolution O
feature O

- B-DAT
tering O
and O
subsampling, O
and O
can O

- B-DAT

- B-DAT

- B-DAT
tion O
from O
both O
prior O
modeling O

- B-DAT
tional O
neural O
networks O
(CNNs) O
have O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eral O
representative O
image O
restoration O
tasks O

- B-DAT

- B-DAT
ing O
[57], O
image O
deblurring O
[58 O

-3 B-DAT
10-2 O
10-1 O
100 O
101 O

- B-DAT
PCN O
[45], O
VDSR O
[29], O
DnCNN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
more O
complex O
image O
restoration O

- B-DAT
ping O
from O
degraded O
observation O
to O

- B-DAT
tional O
network O
(FCN) O
by O
removing O

- B-DAT
formance O
by O
taking O
more O
spatial O

- B-DAT
ever, O
for O
FCN O
without O
pooling O

- B-DAT
ing O
filters O
with O
larger O
size O

- B-DAT
ently O
suffers O
from O
gridding O
effect O

- B-DAT
large O
receptive O
field O
while O
avoiding O

- B-DAT
tational O
burden O
and O
the O
potential O

- B-DAT
trates O
the O
receptive O
field, O
run O

- B-DAT
RCNN O
[14] O
has O
relatively O
larger O

- B-DAT

- B-DAT
off O
between O
performance O
and O
efficiency O

- B-DAT

- B-DAT
tracting O
subnetwork O
and O
an O
expanding O

- B-DAT
ture O
maps O
[12, O
13], O
which O

- B-DAT
tailed O
texture. O
In O
the O
expanding O

- B-DAT
wise O
summation O
is O
adopted O
for O

- B-DAT
over, O
dilated O
filtering O
can O
also O

- B-DAT
larging O
receptive O
field. O
Experiments O
on O

- B-DAT
tiveness O
and O
efficiency O
of O
our O

- B-DAT
ure O
1, O
MWCNN O
is O
moderately O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ically, O
more O
discussions O
are O
given O

- B-DAT
ing O
[25]. O
These O
early O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cently, O
multi-layer O
perception O
(MLP) O
has O

- B-DAT
corporating O
residual O
learning O
with O
batch O

- B-DAT
ditional O
non-CNN O
based O
methods. O
Mao O

- B-DAT
gest O
to O
add O
symmetric O
skip O

- B-DAT
proving O
denoising O
performance. O
For O
better O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
CNN O
[16], O
which O
adopts O
a O

- B-DAT

- B-DAT
work O
[29], O
residual O
units O
[32 O

- B-DAT
tional O
cost O
or O
loss O
of O

- B-DAT

- B-DAT
tween O
receptive O
field O
size O
and O

- B-DAT
erative O
adversarial O
networks O
(GANs) O
have O

- B-DAT
duced O
to O
improve O
the O
visual O

- B-DAT
fers O
from O
blocking O
effect O
and O

- B-DAT

- B-DAT

- B-DAT
ing. O
For O
example, O
both O
DnCNN O

- B-DAT
noisers O
can O
also O
serve O
as O

- B-DAT

- B-DAT

- B-DAT
ers O
[58]. O
Romano O
et O
al O

- B-DAT
ing O
CNN O
on O
wavelet O
subbands O

- B-DAT

- B-DAT
lutional O
framelets O
[21, O
54] O
have O

- B-DAT

- B-DAT
composition. O
Deep O
convolutional O
framelets O
independently O

- B-DAT

- B-DAT
form, O
our O
MWCNN O
can O
embed O

- B-DAT
text O
and O
inter-subband O
dependency O

- B-DAT

- B-DAT

- B-DAT
chitecture. O
Finally, O
discussion O
is O
given O

- B-DAT
nection O
of O
MWCNN O
with O
dilated O

- B-DAT

- B-DAT
age O
x O
[36]. O
The O
convolution O

- B-DAT
nal O
property O
of O
DWT, O
the O

- B-DAT

- B-DAT
cessed O
with O
DWT O
to O
produce O

- B-DAT

- B-DAT
sition O
and O
reconstruction O
of O
an O

- B-DAT
ers. O
In O
the O
decomposition O
stage O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
construction O
result O
at O
the O
current O

- B-DAT

- B-DAT
quired O
to O
process O
the O
decomposition O

- B-DAT
lored O
to O
specific O
task. O
In O

- B-DAT

- B-DAT

- B-DAT
eralization O
of O
multi-level O
WPT, O
and O

- B-DAT
sampling O
operations O
safely O
without O
information O

- B-DAT
over, O
compared O
with O
conventional O
CNN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ations. O
As O
to O
the O
last O

- B-DAT
ing O
subnetwork. O
Generally, O
MWCNN O
modifies O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sponds O
to O
a O
multi-channel O
feature O

- B-DAT

- B-DAT
Net[41], O
while O
DWT O
and O
IWT O

- B-DAT
Net, O
the O
downsampling O
has O
no O

- B-DAT
nels, O
and O
the O
subsequent O
convolution O

- B-DAT
crease O
feature O
map O
channels. O
(iii O

- B-DAT
wise O
summation O
is O
used O
to O

- B-DAT
ventional O
U-Net O
concatenation O
is O
adopted O

- B-DAT
tion, O
Haar O
wavelet O
is O
adopted O

- B-DAT
ered O
in O
our O
experiments O

- B-DAT

- B-DAT
responding O
ground-truth O
image. O
The O
objective O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
convolution O
in O
MWCNN, O
respectively. O
When O

- B-DAT
bands O
are O
taken O
into O
account O

- B-DAT
mation O
loss O
caused O
by O
conventional O

- B-DAT
lated O
filtering O
with O
factor O
2 O

- B-DAT
fined O
analogously. O
We O
also O
have O

- B-DAT
lated O
filtering O
and O
MWCNN O
for O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
field O
of O
MWCNN. O
One O

- B-DAT
mance O
quantitatively O
and O
qualitatively O

- B-DAT
sion O
artifacts O
removal. O
Comparison O
of O

- B-DAT

- B-DAT
lowing O
[57], O
we O
consider O
three O

- B-DAT
ing O
four O
compression O
quality O
settings O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tel(R) O
Core(TM) O
i7-5820K O
CPU O
3.30GHz O

- B-DAT
ing O
methods O
are O
only O
tested O

- B-DAT
ban100 O
[23]. O
Table O
1 O
lists O

- B-DAT
ure O
5 O
shows O
the O
denoising O

- B-DAT

0.9027 O
32.77 O
/ O
0.9008 O
- B-DAT
- O
33.15 O
/ O
0.9088 O
25 O
29.97 O

0.8618 O
30.38 O
/ O
0.8601 O
- B-DAT
- O
30.79 O
/ O
0.8711 O
50 O
26.72 O

0.8906 O
31.63 O
/ O
0.8881 O
- B-DAT
- O
31.86 O
/ O
0.8947 O
25 O
28.57 O

0.8278 O
29.15 O
/ O
0.8249 O
- B-DAT
- O
29.41 O
/ O
0.8360 O
50 O
25.62 O

0.9250 O
32.49 O
/ O
0.9244 O
- B-DAT
- O
33.17 O
/ O
0.9357 O
25 O
29.70 O

0.8792 O
29.82 O
/ O
0.8839 O
- B-DAT
- O
30.66 O
/ O
0.9026 O
50 O
25.94 O

0.9593 O
37.66 O
/ O
0.9599 O
- B-DAT
37.52 O
/ O
0.9590 O
37.74 O

0.9222 O
33.82 O
/ O
0.9230 O
- B-DAT
- O
34.03 O
/ O
0.9244 O
34.09 O

0.9118 O
32.94 O
/ O
0.9144 O
- B-DAT
33.08 O
/ O
0.9130 O
33.23 O

0.8349 O
29.61 O
/ O
0.8341 O
- B-DAT
- O
29.96 O
/ O
0.8349 O
30.00 O

BSD100 O
×2 O
- B-DAT
31.90 O
/ O
0.8960 O
31.85 O

0.8942 O
31.98 O
/ O
0.8974 O
- B-DAT
31.80 O
/ O
0.8950 O
32.05 O

0.8995 O
32.23 O
/ O
0.8999 O
×3 O
- B-DAT
28.82 O
/ O
0.7976 O
28.80 O

0.7963 O
28.92 O
/ O
0.7993 O
- B-DAT
- O
28.95 O
/ O
0.8004 O
28.96 O

0.7987 O
29.12 O
/ O
0.8060 O
×4 O
- B-DAT
27.29 O
/ O
0.7251 O
27.23 O

Urban100 O
×2 O
- B-DAT
30.76 O
/ O
0.9140 O
30.75 O

0.9133 O
30.91 O
/ O
0.9159 O
- B-DAT
30.41 O
/ O
0.9100 O
31.23 O

0.9169 O
32.30 O
/ O
0.9296 O
×3 O
- B-DAT
27.14 O
/ O
0.8279 O
27.15 O

0.8276 O
27.31 O
/ O
0.8303 O
- B-DAT
- O
27.53 O
/ O
0.8378 O
27.56 O

0.8334 O
28.13 O
/ O
0.8514 O
×4 O
- B-DAT
25.18 O
/ O
0.7524 O
25.20 O

0.8837 O
32.91 O
/ O
0.8861 O
- B-DAT
33.43 O
/ O
0.8930 O
40 O
32.43 O

0.8911 O
33.34 O
/ O
0.8953 O
- B-DAT
33.77 O
/ O
0.9003 O
- O
34.27 O

0.9059 O
32.98 O
/ O
0.9090 O
- B-DAT
33.45 O
/ O
0.9153 O
40 O
32.35 O

0.9173 O
33.63 O
/ O
0.9198 O
- B-DAT
33.96 O
/ O
0.9247 O
- O
34.45 O

- B-DAT

- B-DAT
peting O
methods O
on O
the O
four O

- B-DAT
forms O
favorably O
in O
terms O
of O

- B-DAT
dexes. O
Compared O
with O
VDSR, O
our O

- B-DAT
perform O
VDSR, O
and O
also O
is O

- B-DAT
ResNet O
on O
Set14. O
Figure O
6 O

-1 B-DAT
method O
by O
0.37dB O

- B-DAT
overlapped O
8 O
× O
8 O
blocks O

- B-DAT
troducing O
the O
blocking O
artifact. O
The O

- B-DAT
mined O
by O
a O
quality O
factorQ O

- B-DAT
sider O
[18, O
19] O
due O
to O

- B-DAT
peting O
methods O
on O
Classic5 O
and O

- B-DAT
Net O
[48]) O
for O
the O
quality O

- B-DAT

- B-DAT

- B-DAT
ing O
library O
is O
adopted O
to O

- B-DAT
based O
methods O
with O
source O
codes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
s, O
MWCNN O
is O
moderately O
slower O

- B-DAT
stead O
of O
the O
increase O
of O

- B-DAT
ness O
of O
MWCNN O
should O
be O

- B-DAT
amples, O
we O
compare O
the O
PSNR O

- B-DAT

- B-DAT
fault O
MWCNN O
with O
Haar O
wavelet O

-2 B-DAT
wavelet, O
and O
(iii) O
MWCN- O
N O

-2 B-DAT
in O
expanding O
subnetwork. O
Then, O
abla O

- B-DAT
tion O
experiments O
are O
provided O
for O

- B-DAT
ness O
of O
additionally O
embedded O
wavelet O

- B-DAT
Net O
with O
same O
architecture O
to O

- B-DAT

- B-DAT
ing O
sum O
connection O
instead O
of O

- B-DAT
Net+D: O
adopting O
learnable O
conventional O
downsamping O

- B-DAT

- B-DAT

- B-DAT
ing O
library O
is O
adopted O
to O

- B-DAT
based O
methods O
with O
source O
codes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
s, O
MWCNN O
is O
moderately O
slower O

- B-DAT
stead O
of O
the O
increase O
of O

- B-DAT
ness O
of O
MWCNN O
should O
be O

- B-DAT
amples, O
we O
compare O
the O
PSNR O

- B-DAT

- B-DAT
fault O
MWCNN O
with O
Haar O
wavelet O

-2 B-DAT
wavelet, O
and O
(iii) O
MWCN- O
N O

-2 B-DAT
in O
expanding O
subnetwork. O
Then, O
abla O

- B-DAT
tion O
experiments O
are O
provided O
for O

- B-DAT
ness O
of O
additionally O
embedded O
wavelet O

- B-DAT
Net O
with O
same O
architecture O
to O

- B-DAT

- B-DAT
ing O
sum O
connection O
instead O
of O

- B-DAT
Net+D: O
adopting O
learnable O
conventional O
downsamping O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
library O
is O
adopted O
to O

- B-DAT
based O
methods O
with O
source O
codes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
s, O
MWCNN O
is O
moderately O
slower O

- B-DAT
stead O
of O
the O
increase O
of O

- B-DAT
ness O
of O
MWCNN O
should O
be O

- B-DAT
amples, O
we O
compare O
the O
PSNR O

- B-DAT

- B-DAT
fault O
MWCNN O
with O
Haar O
wavelet O

-2 B-DAT
wavelet, O
and O
(iii) O
MWCN- O
N O

-2 B-DAT
in O
expanding O
subnetwork. O
Then, O
abla O

- B-DAT
tion O
experiments O
are O
provided O
for O

- B-DAT
ness O
of O
additionally O
embedded O
wavelet O

- B-DAT
Net O
with O
same O
architecture O
to O

- B-DAT

- B-DAT
ing O
sum O
connection O
instead O
of O

- B-DAT
Net+D: O
adopting O
learnable O
conventional O
downsamping O

- B-DAT

- B-DAT
ing O
library O
is O
adopted O
to O

- B-DAT
based O
methods O
with O
source O
codes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fectiveness O
of O
MWCNN O
should O
be O

- B-DAT
ration O
of O
CNN O
and O
DWT O

- B-DAT

- B-DAT
fault O
MWCNN O
with O
Haar O
wavelet O

-2 B-DAT
wavelet, O
and O
(iii) O
MWCNN O
(HD O

-2 B-DAT
in O
expanding O
subnetwork. O
Then, O
ablation O

- B-DAT
periments O
are O
provided O
for O
verifying O

- B-DAT
ditionally O
embedded O
wavelet: O
(i) O
the O

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT
U-Net O
[41] O
U-Net+S O
U-Net+D O
DCF O

0.355 O
26.65 O
/ O
0.354 O
- B-DAT
/ O
- O
27.42 O
/ O
0.343 O

0.097 O
29.57 O
/ O
0.104 O
- B-DAT
/ O
- O
30.01 O
/ O
0.088 O

0.120 O
29.38 O
/ O
0.155 O
- B-DAT
/ O
- O
29.69 O
/ O
0.112 O

- B-DAT

-2 B-DAT

- B-DAT

- B-DAT
dicate O
that O
using O
sum O
connection O

- B-DAT
frequency O
localization O
property O
in O
wavelet O

- B-DAT
dent O
processing O
of O
subbands O
harms O

- B-DAT
pared O
to O
MWCNN O
(DB2) O
and O

- B-DAT
uation. O
MWCNN O
(Haar) O
has O
similar O

- B-DAT

- B-DAT
tween O
performance O
and O
efficiency O

- B-DAT
position, O
where O
different O
CNNs O
are O

- B-DAT
band. O
However, O
the O
results O
in O

- B-DAT
pendent O
processing O
of O
subbands O
is O

- B-DAT

- B-DAT
ier O
computational O
burden. O
Thus, O
a O

- B-DAT
NNs O
with O
different O
levels O
on O

-1 B-DAT
MWCNN-2 O
MWCNN-3 O
MWCNN-4 O

-1 B-DAT
∼MWCNN-4). O
It O
can O
be O
observed O

-3 B-DAT
with O
24-layer O
architecture O
performs O
much O

-1 B-DAT
and O
MWCNN-2, O
while O
MWCNN-4 O
only O

-3 B-DAT
in O
terms O
of O
the O
PSNR O

-3 B-DAT
is O
also O
moderate O
compared O
with O

-3 B-DAT
as O
the O
default O
setting O

- B-DAT

- B-DAT

- B-DAT
fectiveness O
and O
efficiency O
of O
MWCNN O

- B-DAT
eral O
restoration O
tasks O
such O
as O

- B-DAT

- B-DAT

- B-DAT
gle O
image O
super-resolution: O
Dataset O
and O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
shops O
(CVPRW), O
pages O
1122–1131. O
IEEE O

- B-DAT
demic O
Press, O
2001 O

- B-DAT
ing O
for O
image O
restoration: O
Persistent O

- B-DAT

- B-DAT
ifold O
simplification. O
In O
IEEE O
Conference O

- B-DAT
tion. O
IEEE O
Signal O
Processing O
Magazine O

- B-DAT
Morel. O
Low-complexity O
single-image O
super-resolution O
based O

- B-DAT
noising: O
Can O
plain O
neural O
networks O

- B-DAT
tion, O
pages O
2392–2399, O
2012 O

- B-DAT
olding O
for O
image O
denoising O
and O

- B-DAT
tions O
on O
Image O
Processing, O
9(9):1532–1546 O

- B-DAT
tion. O
IEEE O
Transactions O
on O
Pattern O

- B-DAT
age O
denoising O
by O
sparse O
3-d O

- B-DAT

- B-DAT
tive O
filtering. O
IEEE O
Transactions O
on O

- B-DAT

- B-DAT
ization O
and O
signal O
analysis. O
IEEE O

- B-DAT
tion O
Theory, O
36(5):961–1005, O
1990 O

- B-DAT
ference O
on O
Computer O
Vision, O
pages O

- B-DAT
sion O
artifacts O
reduction O
by O
a O

- B-DAT

- B-DAT
tion, O
pages O
2862–2869, O
2014 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
shops O
(CVPRW), O
2017 O

- B-DAT

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
lutional O
networks. O
In O
Advances O
in O

- B-DAT
cessing O
Systems, O
pages O
769–776, O
2009 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lishing O
Company, O
Incorporated, O
2012 O

- B-DAT

- B-DAT
volutional O
network O
for O
image O
super-resolution O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT
timization. O
In O
International O
Conference O
for O

- B-DAT
sentations, O
2015 O

- B-DAT
resolution. O
IEEE O
Conference O
on O
Computer O

- B-DAT
tern O
Recognition, O
2017 O

- B-DAT

- B-DAT

- B-DAT
ative O
adversarial O
network. O
IEEE O
Conference O

- B-DAT

- B-DAT
cessing, O
1(2):244–250, O
1992 O

- B-DAT
ing O
convolutional O
networks O
for O
content-weighted O

- B-DAT
pression. O
IEEE O
Conference O
on O
Computer O

- B-DAT
position: O
the O
wavelet O
representation. O
IEEE O

- B-DAT

- B-DAT
ric O
skip O
connections. O
In O
Advances O

- B-DAT
cessing O
Systems, O
pages O
2802–2810, O
2016 O

- B-DAT
cal O
statistics. O
In O
IEEE O
Conference O

- B-DAT
ence O
Computer O
Vision, O
volume O
2 O

- B-DAT
ing O
for O
image O
quality O
assessment O

- B-DAT

- B-DAT
tional O
networks O
for O
biomedical O
image O

- B-DAT
ternational O
Conference O
on O
Medical O
Image O

- B-DAT

- B-DAT

- B-DAT
ized O
deep O
image O
to O
image O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
preserving O
image O
super-resolution O
via O
contextualized O

- B-DAT
task O
learning. O
IEEE O
Transactions O
on O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
national O
conference O
on O
Multimedia, O
pages O

- B-DAT
mentation. O
arXiv O
preprint O
arXiv:1702.08502, O
2017 O

- B-DAT

- B-DAT

- B-DAT
ing O
with O
deep O
neural O
networks O

- B-DAT
dustrial O
and O
Applied O
Mathematics, O
2018 O

- B-DAT

- B-DAT
lated O
convolutions. O
arXiv O
preprint O
arXiv:1511.07122 O

- B-DAT

- B-DAT

- B-DAT
yond O
a O
gaussian O
denoiser: O
Residual O

in O
BSD, O
and O
test O
on O
Set14 B-DAT
and O
the O
BSD O
test O
set O

four O
benchmark O
sets: O
Set5 O
[1], O
Set14 B-DAT
[50], O
BSD100 O
[30] O
and O
Urban100 O

and O
×4 O
on O
datasets O
Set5, O
Set14, B-DAT
BSD100 O
and O
Urban100. O
The O
best O

Set14 B-DAT
×2 O
32.45/0.9067 O
33.03/0.9124 O
33.04/0.9118 O
33.08/0.913 O

20] O
for O
testing O
with O
three O
upscaling B-DAT
factors: O
×2, O
×3 O
and O
×4 O

training O
images O
for O
all O
three O
upscaling B-DAT
factors: O
×2, O
×3 O
and O
×4 O

model O
for O
all O
these O
three O
upscaling B-DAT
factors O
as O
in O
[21, O
37 O

best O
result O
across O
all O
the O
upscaling B-DAT
factors O
and O
datasets. O
Visual O
results O

image O
super-resolution O
results O
with O
×4 O
upscaling B-DAT

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests O

in O
BSD, O
and O
test O
on O
Set14 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

the O
advantage O
of O
RNN O
architecture O
- B-DAT
the O
correlation O
information O
is O
propagated O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
based O
image O
restoration O
approaches. O
The O

- B-DAT

- B-DAT

- B-DAT
sity O
[28, O
46]. O
Alternatively, O
similar O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
is O
adopted O
to O
save O

- B-DAT
sides O
CNNs, O
RNNs O
have O
also O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
methods O
based O
on O
low-rankness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
vision O
tasks. O
However, O
unlike O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-3 B-DAT
and O
reduce O
it O
by O
half O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
local O
modules, O
we O
implement O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-6 B-DAT
from O
NLRN O
with O
unrolling O
length O

- B-DAT

- B-DAT
of-the-art O
network O
models O
on O
Set12 O

- B-DAT
plexities O
are O
also O
compared O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
layers) O
of O
NLRN. O
The O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

28.95dB O
and O
27.42dB O
on O
Set5, O
Set14, B-DAT
BSD100 O
and O
Urban100, O
respectively. O
As O

degradation O
on O
datasets O
Set5 O
[3], O
Set14 B-DAT
[54], O
BSD100 O
[33] O
and O
Urban100 O

Set14 B-DAT
×3 O
27.53 O
/ O
0.774 O
29.27 O

- B-DAT

- B-DAT
age O
super-resolution O
(SISR). O
However, O
existing O

- B-DAT

- B-DAT

- B-DAT
age O
is O
bicubicly O
downsampled O
from O

- B-DAT

- B-DAT
over, O
they O
lack O
scalability O
in O

- B-DAT
blindly O
deal O
with O
multiple O
degradations O

- B-DAT
ity O
stretching O
strategy O
that O
enables O

- B-DAT

- B-DAT
put. O
Consequently, O
the O
super-resolver O
can O

- B-DAT

- B-DAT
duce O
favorable O
results O
on O
multiple O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put. O
As O
a O
classical O
problem O

- B-DAT
lenging O
research O
topic O
in O
the O

- B-DAT

- B-DAT
nel O
k O
and O
a O
latent O

- B-DAT
pling O
operation O
with O
scale O
factor O

- B-DAT
tive O
white O
Gaussian O
noise O
(AWGN O

- B-DAT
gories, O
i.e., O
interpolation-based O
methods, O
model-based O

- B-DAT
timization O
methods O
and O
discriminative O
learning O

- B-DAT

- B-DAT

- B-DAT
linear O
and O
bicubic O
interpolators O
are O

- B-DAT
age O
priors O
(e.g., O
the O
non-local O

- B-DAT

- B-DAT
based O
optimization O
methods O
are O
flexible O

- B-DAT
ative O
high-quality O
HR O
images, O
but O

- B-DAT

- B-DAT
tegration O
of O
convolutional O
neural O
network O

- B-DAT

- B-DAT
ciency O
to O
some O
extent, O
it O

- B-DAT
backs O
of O
model-based O
optimization O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
networks O
[16, O
18, O
21 O

- B-DAT
els O
based O
on O
discriminative O
CNN O

- B-DAT
els O
are O
specialized O
for O
a O

- B-DAT
ously O
when O
the O
assumed O
degradation O

- B-DAT
ever, O
little O
work O
has O
been O

- B-DAT
ing O
questions, O
which O
are O
the O

- B-DAT
thetic O
data O
to O
train O
a O

- B-DAT
ing O
these O
two O
questions O

- B-DAT

- B-DAT
resolution O
network. O
In O
view O
of O

- B-DAT
sionality O
stretching O
strategy O
which O
facilitates O

- B-DAT
edge, O
there O
is O
no O
attempt O

- B-DAT

- B-DAT
binations O
of O
blur O
kernels O
and O

- B-DAT
AWGN), O
we O
can O
select O
the O

- B-DAT
sult. O
It O
turns O
out O
that O

- B-DAT
sults O
on O
real O
LR O
images O

- B-DAT

- B-DAT
tion O
and O
works O
for O
multiple O

- B-DAT

- B-DAT

- B-DAT
resolution O
network O
learned O
from O
synthetic O

- B-DAT
of-the-art O
SISR O
methods O
on O
synthetic O

- B-DAT

- B-DAT

- B-DAT
work O
(SRCNN) O
was O
proposed. O
In O

- B-DAT
resolution O
and O
empirically O
showed O
that O

- B-DAT
ment O
of O
CNN O
super-resolvers. O
To O

- B-DAT
resolution O
(VDSR) O
method O
with O
residual O

- B-DAT

- B-DAT
ically O
demonstrated O
that O
a O
single O

- B-DAT
tiple O
scales O
super-resolution, O
image O
deblocking O

- B-DAT
put, O
which O
not O
only O
suffers O

- B-DAT
rectly O
manipulating O
the O
LR O
input O

- B-DAT
ing O
operation O
at O
the O
end O

- B-DAT

- B-DAT
scale O
the O
LR O
feature O
maps O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
layers O
are O
used O
to O

- B-DAT

- B-DAT

- B-DAT
teresting O
line O
of O
CNN-based O
methods O

- B-DAT
yond O
bicubic O
degradation O
adopt O
a O

- B-DAT

- B-DAT

- B-DAT
ever, O
manually O
selecting O
the O
hyper-parameters O

- B-DAT
able O
to O
learn O
a O
single O

- B-DAT

- B-DAT
ertheless, O
our O
method O
is O
general O

- B-DAT
cussion O
on O
blur O
kernel O
k O

- B-DAT
lar O
choice O
is O
isotropic O
Gaussian O

- B-DAT
cal O
and O
theoretical O
analyses O
have O

- B-DAT
ticated O
image O
priors O
[12]. O
Specifically O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
pre-processing O
step O
tends O
to O

- B-DAT

- B-DAT
mance O
[43]. O
Thus, O
it O
would O

- B-DAT

- B-DAT
pler O
since O
when O
k O
is O

- B-DAT

- B-DAT
tion O
model. O
It O
should O
be O

- B-DAT
lenging O
task O
since O
the O
degradation O

- B-DAT
ample). O
One O
relevant O
work O
is O

- B-DAT

- B-DAT
timization O
method O
and O
thus O
suffers O

- B-DAT
pects. O
First, O
our O
method O
considers O

- B-DAT
dation O
model. O
Second, O
our O
method O

- B-DAT

- B-DAT
essarily O
derived O
under O
the O
traditional O

- B-DAT
nections O
between O
the O
MAP O
principle O

- B-DAT
anism O
of O
CNN. O
Consequently, O
more O

- B-DAT
tecture O
design O
can O
be O
obtained O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fore, O
the O
MAP O
solution O
of O

- B-DAT

- B-DAT
lated O
as O

- B-DAT
dation O
process, O
accurate O
modeling O
of O

- B-DAT
based O
SISR O
methods O
with O
bicubic O

- B-DAT
form O
generic O
image O
super-resolution O
with O

- B-DAT
dicates O
that O
the O
parameters O
of O

- B-DAT
solve O
this O
problem O

- B-DAT
nel O
is O
first O
projected O
onto O

- B-DAT

- B-DAT
ear O
space O
by O
the O
PCA O

- B-DAT
nique. O
After O
that, O
the O
concatenated O

- B-DAT
dation O
maps O
M O
of O
size O

- B-DAT

- B-DAT
ing O
CNN O
possible O
to O
handle O

- B-DAT
ant O
degradations O
by O
considering O
the O

- B-DAT

- B-DAT

- B-DAT
plex O
architectural O
engineering. O
Typically, O
to O

- B-DAT

- B-DAT

- B-DAT
fied O
Linear O
Units O
(ReLU) O
[26 O

- B-DAT

- B-DAT
volutional O
layer O
to O
convert O
multiple O

- B-DAT
tional O
layers O
is O
set O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sign O
due O
to O
the O
following O

- B-DAT
ever, O
such O
blind O
model O
does O

- B-DAT
pected. O
First, O
the O
performance O
deteriorates O

- B-DAT
ing O
the O
blur O
kernel O
to O

- B-DAT
ferent O
HR O
images O
with O
pixel O

- B-DAT
vate O
the O
pixel-wise O
average O
problem O

- B-DAT

- B-DAT
ization O
ability O
and O
performs O
poorly O

- B-DAT

- B-DAT

- B-DAT
ity, O
one O
can O
treat O
the O

- B-DAT
nel O
and O
noise O
level O
as O

- B-DAT
tion O
maps, O
the O
non-blind O
model O

- B-DAT
tween O
data O
fidelity O
term O
and O

- B-DAT

- B-DAT
cally, O
the O
kernel O
width O
ranges O

- B-DAT
ity O
density O
function O
N O
(0,Σ O

- B-DAT
nel O
is O
determined O
by O
rotation O

- B-DAT
tion O
angle O
range O
to O
[0 O

- B-DAT
out O
the O
paper, O
it O
is O

- B-DAT
rect O
downsampler. O
Alternatively, O
we O
can O

- B-DAT
pler O
↓d, O
we O
can O
find O

- B-DAT

- B-DAT

- B-DAT
tor O
3 O
and O
PCA O
eigenvectors O

- B-DAT

- B-DAT
nel O
and O
a O
noise O
level O

- B-DAT
dation O
maps) O
for O
each O
epoch O

- B-DAT

- B-DAT

- B-DAT
tained O
by O
fine-tuning O
SRMD, O
its O

- B-DAT
cal O
GPU. O
The O
training O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
used O
datasets. O
As O
one O
can O

- B-DAT
rable O
results O
with O
VDSR O
at O

- B-DAT
forms O
VDSR O
at O
large O
scale O

- B-DAT
geNet O
dataset O
[26] O
to O
train O

- B-DAT
MDNF O
on O
scale O
factor O
4 O

- B-DAT
form O
other O
competing O
methods. O
The O

- B-DAT
plicit O
prior O
learning O
and O
thus O

- B-DAT
ment. O
This O
also O
can O
explain O

- B-DAT
parison, O
the O
run O
time O
of O

- B-DAT
ods. O
One O
can O
see O
that O

- B-DAT
petitive O
performance O
against O
other O
methods O

- B-DAT
tion O
settings O
are O
given O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferent O
degradations O
on O
Set5 O
are O

- B-DAT
ticular, O
the O
PSNR O
gain O
of O

- B-DAT

- B-DAT
Noise O
PSNR O
(×2/×3/×4)Width O
sampler O
Level O

- B-DAT

- B-DAT
pler. O
The O
visual O
comparison O
is O

- B-DAT
tially O
variant O
blur O
kernels O
and O

- B-DAT
cally O
downsampled O
from O
HR O
images O

- B-DAT
nels O
and O
corrupted O
by O
AWGN O

- B-DAT

- B-DAT
parison O

- B-DAT
sian O
kernels O
in O
training, O
it O

- B-DAT
ing. O
To O
find O
the O
degradation O

- B-DAT
cally, O
the O
kernel O
width O
is O

- B-DAT

- B-DAT
pression O
artifacts, O
Waifu2x O
[49] O
is O

- B-DAT
son. O
For O
image O
“Chip” O
which O

- B-DAT

- B-DAT
satisfying O
artifacts O
but O
also O
produce O

- B-DAT
ure O
9, O
we O
can O
see O

- B-DAT
duce O
over-smoothed O
results, O
whereas O
SRMD O

- B-DAT

- B-DAT
dations O
via O
a O
single O
model O

- B-DAT
based O
SISR O
methods, O
the O
proposed O

- B-DAT

- B-DAT
ically, O
degradation O
maps O
are O
obtained O

- B-DAT
sionality O
stretching O
of O
the O
degradation O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tially O
variant O
degradations. O
Moreover, O
the O

- B-DAT
struct O
visually O
plausible O
HR O
images O

- B-DAT
posed O
super-resolver O
offers O
a O
feasible O

- B-DAT
tical O
CNN-based O
SISR O
applications O

- B-DAT

- B-DAT
ity O
Enhancement O
of O
Surveillance O
Images O

- B-DAT
ration O
for O
providing O
us O
the O

- B-DAT
search O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
shops, O
volume O
3, O
pages O
126–135 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
Conference, O
2012 O

- B-DAT
shift O
priors O
for O
image O
restoration O

- B-DAT
formation O
Processing O
Systems, O
2017 O

- B-DAT
tion O
diffusion O
processes O
for O
effective O

- B-DAT
tion, O
pages O
5261–5269, O
2015 O

- B-DAT

- B-DAT

- B-DAT
pean O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
ference O
on O
Computer O
Vision, O
pages O

- B-DAT
ized O
sparse O
representation O
for O
image O

- B-DAT
actions O
on O
Image O
Processing, O
22(4):1620–1630 O

- B-DAT
resolution. O
In O
IEEE O
International O
Conference O

- B-DAT
resolution O
via O
BM3D O
sparse O
coding O

- B-DAT
resolution O
and O
texture O
synthesis. O
Advances O

- B-DAT
dom O
Fields O
for O
Vision O
and O

- B-DAT

- B-DAT
puter O
Vision, O
pages O
349–356, O
2009 O

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
nets. O
In O
Advances O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
network O
for O
image O
super-resolution O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT
timization. O
In O
International O
Conference O
for O

- B-DAT
sentations, O
2015 O

- B-DAT
resolution. O
In O
IEEE O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
network. O
In O
IEEE O

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT
tion O
Workshops, O
pages O
136–144, O
July O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
tional O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT
tional O
neural O
networks. O
In O
Advances O

- B-DAT
tioned O
regression O
models O
for O
non-blind O

- B-DAT
resolution. O
In O
IEEE O
International O
Conference O

- B-DAT
curate O
image O
super O
resolution. O
IEEE O

- B-DAT
putational O
Imaging, O
3(1):110–125, O
2017 O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
preserving O
image O
super-resolution O
via O
contextualized O

- B-DAT
task O
learning. O
IEEE O
Transactions O
on O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
tional O
Conference O
on O
Computer O
Vision O

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT

- B-DAT
ral O
networks O
for O
matlab. O
In O

- B-DAT

- B-DAT

- B-DAT
tural O
similarity. O
IEEE O
Transactions O
on O

- B-DAT

- B-DAT
resolution: O
A O
benchmark. O
In O
European O

- B-DAT
puter O
Vision, O
pages O
372–386, O
2014 O

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT

- B-DAT
cessing, O
26(12):5895–5907, O
2017 O

- B-DAT

- B-DAT

- B-DAT
gle O
image O
super-resolution O
under O
internet O

- B-DAT
ference O
on O
Multimedia, O
pages O
677–687 O

- B-DAT
yond O
a O
gaussian O
denoiser: O
Residual O

- B-DAT
ing, O
pages O
3142–3155, O
2017 O

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
and O

branch O
to O
reconstruct O
the O
HR O
4x B-DAT
image O

Set5 O
[30] O
and O
Set14 B-DAT
[31]. O
The O
results O
are O
shown O

Set14 B-DAT

Set5 O
Set14 B-DAT

Set5 O
Set14 B-DAT

including O
Set5 O
[30] O
and O
Set14 B-DAT
[31]. O
We O
also O
included O
the O

We O
compared O
the O
parameters O
and O
Set14 B-DAT
[31] O
results O
of O

Set5 O
Set14 B-DAT
Bsd100 O
Urban100 O

high-resolution O
(HR) O
images. O
When O
the O
upscaling B-DAT
factor O
is O
large O

extra O
computation. O
Moreover, O
for O
large O
upscaling B-DAT
factors, O
our O

high O
quality O
images O
for O
higher O
upscaling B-DAT
factors O

model O
could O
do O
multi O
scale O
upscaling B-DAT
task O
via O
Laplacian O
Pyramid O

upscaling B-DAT
factors O
and O
decreasing O
parameters O
by O

We O
first O
upscaling B-DAT
our O
low-resolution O
image O
via O
a O

Set5 O
Set14 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-the-art O
methods O
in O
scale O
x4 O

-5 B-DAT

- B-DAT

- B-DAT

-11 B-DAT

- B-DAT
learning-based O
super O
resolution O
methods O
have O

- B-DAT

- B-DAT

- B-DAT
resolution O
simultaneously O
in O
one O
feed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
quality O
high-resolution O
image, O
the O
process O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-2798 B-DAT

- B-DAT

-8 B-DAT

- B-DAT

- B-DAT

-2873 B-DAT

- B-DAT

- B-DAT

-3478 B-DAT

- B-DAT

- B-DAT

-126 B-DAT

- B-DAT
based O
single O
image O
super O
resolu-tion[C]//Proceedings O

- B-DAT
1873 O

- B-DAT

- B-DAT

-5206 B-DAT

- B-DAT

-1105 B-DAT

-551 B-DAT

- B-DAT

-2324 B-DAT

- B-DAT

-307 B-DAT

- B-DAT

- B-DAT

-778 B-DAT

- B-DAT

- B-DAT

- B-DAT
1654 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1645 B-DAT

- B-DAT

- B-DAT

-4547 B-DAT

- B-DAT

-4817 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-711 B-DAT

- B-DAT

- B-DAT

- B-DAT
2423 O

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adver-sarial O

- B-DAT

- B-DAT

-4500 B-DAT

-612 B-DAT

-456 B-DAT

- B-DAT

- B-DAT

- B-DAT
image O
super-resolution O
based O
on O
nonnegative O

- B-DAT

- B-DAT
representations[C]//International O
conference O
on O
curves O
and O

-730 B-DAT

-423 B-DAT

- B-DAT

-407 B-DAT

the O
dataset O
Set5 O
[1], O
and O
Set14 B-DAT
[35] O
which O
are O
widely O
used O

Set14 B-DAT
×2 O
30.24/0.8688 O
32.45/0.9067 O
33.03/0.9124 O
33.04/0.9118 O

and O
×4 O
on O
datasets O
Set5, O
Set14, B-DAT
BSD100 O
and O
Urban100. O
Red O
color O

Set14 B-DAT
×2 O
6.105 O
7.784 O
7.591 O
8.178 O

and O
×4 O
on O
datasets O
Set5, O
Set14 B-DAT
and O
Urban100. O
Red O
color O
indicates O

SR O
results O
of O
“ppt3” O
from O
Set14 B-DAT
with O
scale O
factor O
×3. O
Texts O

of O
the O
art O
for O
large O
upscaling B-DAT
factors O
(×4). O
EDSR O
[16 O

and O
×4 O
on O
datasets O
Set5, O
Set14 B-DAT

SR O
results O
of O
“ppt3” O
from O
Set14 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ual O
Learning O
and O
Convolutional O
Sparse O

- B-DAT

- B-DAT
Threshold O
Algorithm O
(LISTA). O
We O
extend O

- B-DAT
volutional O
version O
and O
build O
the O

- B-DAT
tional O
sparse O
codings O
of O
input O

- B-DAT

- B-DAT
mark O
datasets O
demonstrate O
the O
effectiveness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
arts, O
e.g., O
DRRN O
(52 O
layers O

- B-DAT
sults O
are O
available O
at O
https://github.com/axzml O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rithms, O
especially O
the O
current O
leading O

- B-DAT

- B-DAT
ods O
[26, O
4, O
13, O
14 O

- B-DAT
lem O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sent O
our O
models. O
RL-CSC O
with O

- B-DAT
petitive O
performance O
with O
MemNet O
[24 O

- B-DAT
ters, O
the O
performance O
of O
RL-CSC O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
methods O
[32]. O
Inspired O
by O

- B-DAT

- B-DAT
ers O
named O
VDSR, O
which O
shows O

- B-DAT

- B-DAT
gradient O
problem O
when O
the O
network O

- B-DAT

- B-DAT
work O
(DRCN) O
[14] O
with O
a O

- B-DAT
nary O
success O
of O
ResNet O
[9 O

- B-DAT

- B-DAT

- B-DAT
cursive O
manner, O
leading O
to O
a O

- B-DAT
decoding O
network O
named O
RED-Net O
was O

- B-DAT
tioned O
models O
usually O
lack O
convincing O

- B-DAT
plored, O
i.e., O
what O
role O
each O

- B-DAT
resentation O
with O
strong O
theoretical O
support O

- B-DAT

- B-DAT

- B-DAT
ment O
when O
the O
number O
of O

- B-DAT
posed O
CSC O
based O
SR O
(CSC-SR O

- B-DAT

- B-DAT
tional O
sparse O
coding O
methods. O
In O

- B-DAT
putationally O
efficient O
CSC O
model, O
Sreter O

- B-DAT
duced O
a O
convolutional O
recurrent O
sparse O

- B-DAT

- B-DAT
tending O
the O
LISTA O
method O
to O

- B-DAT
ing O
tasks O

- B-DAT

- B-DAT

- B-DAT
posed O
in O
the O
field O
of O

- B-DAT
tion, O
our O
model, O
termed O
as O

- B-DAT

- B-DAT
defined O
interpretability O

- B-DAT
CSC O
(30 O
layers) O
has O
achieved O

- B-DAT

- B-DAT
mizes O
the O
objective O
function O
(1 O

- B-DAT
ting O
term O
and O
an O
`1-norm O

- B-DAT

- B-DAT
ization O
coefficient O
λ O
is O
used O

- B-DAT

- B-DAT
ative O
Shrinkage O
Thresholding O
Algorithm O
(ISTA O

- B-DAT

- B-DAT

- B-DAT
dress O
this O
issue, O
Gregor O
and O

- B-DAT
gorithm O
termed O
as O
Learned O
ISTA O

- B-DAT
proximate O
estimates O
of O
sparse O
code O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ters O
We, O
G O
and O
θ O

- B-DAT

- B-DAT
vide O
the O
whole O
image O
into O

- B-DAT
actly O
the O
same O
, O
is O

- B-DAT
able O
for O
this O
issue, O
as O

- B-DAT
ternating O
Direction O
Method O
of O
Multipliers O

- B-DAT

- B-DAT

- B-DAT
ory O
burden O
issue O
of O
ADMM O

- B-DAT

- B-DAT
put O
Interpolated O
LR O
(ILR) O
image O

- B-DAT
gency O
speed O
and O
the O
reconstruction O

- B-DAT
lated O
Low-Resolution O
(ILR) O
image O
Iy O

- B-DAT
dicts O
the O
output O
HR O
image O

- B-DAT
verting O
one O
of O
the O
inputs O

- B-DAT
tive O
tool O
to O
learn O
the O

- B-DAT

- B-DAT
portant O
facts: O
(1) O
the O
expressiveness O

- B-DAT

- B-DAT

- B-DAT
frequency O
information O
reconstruction O

- B-DAT
structed O
by O
the O
addition O
of O

- B-DAT

- B-DAT
tion, O
W1 O
and O
S O
for O

- B-DAT
ery O
recursion. O
When O
K O
recursions O

- B-DAT
ing O
process, O
the O
depth O
d O

- B-DAT
ploited O
in O
our O
training O
process O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dient O
descent O
(SGD) O
is O
used O

- B-DAT
ment O
our O
model O
using O
the O

- B-DAT

- B-DAT
fied O
structures O
of O
these O
models O

- B-DAT

- B-DAT

- B-DAT
put) O
and O
a O
pre-activation O
structure O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eter O
settings O
for O
better O
performance O

- B-DAT

- B-DAT

- B-DAT
tween O
SCN O
[29] O
and O
RL-CSC O

- B-DAT

- B-DAT
ing O
linear O
layers, O
so O
more O

- B-DAT

- B-DAT
works. O
With O
the O
help O
of O

- B-DAT

- B-DAT
fers O
with O
DRCN O
[14] O
in O

- B-DAT
ual O
Learning O
[23] O
(LRL) O
and O

- B-DAT

- B-DAT
sides, O
DRCN O
is O
not O
easy O

- B-DAT

- B-DAT

- B-DAT
ate O
predictions) O
is O
used O
to O

- B-DAT

- B-DAT

- B-DAT
troduction O
to O
the O
datasets O
used O

- B-DAT
isons O
with O
state-of-the-arts O
are O
presented O

- B-DAT

- B-DAT
tion O
Dataset O
[18]. O
During O
testing O

- B-DAT
mark. O
Moreover, O
the O
BSD100 O
[18 O

- B-DAT
ural O
images O
are O
used O
for O

- B-DAT

- B-DAT

- B-DAT
minance) O
of O
transformed O
YCbCr O
space O

- B-DAT
cludes O
flipping O
(horizontally O
and O
vertically O

- B-DAT
formed O
on O
each O
image O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tioned O
into O
33× O
33 O
patches O

- B-DAT

- B-DAT
CSC O
is O
30 O
according O
to O

- B-DAT
mized O
using O
SGD O
with O
mini-batch O

- B-DAT

- B-DAT

- B-DAT
ther O
improvements O
of O
the O
loss O

- B-DAT
dient O
clipping O
strategy O
stated O
in O

- B-DAT
ter. O
A O
NVIDIA O
Titan O
Xp O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bic O
interpolation O
to O
the O
original O

- B-DAT
els O
near O
borders O
before O
evaluation O

- B-DAT
mark O
testing O
sets, O
and O
results O

- B-DAT

- B-DAT
lowing O
[23], O
BSD100 O
is O
not O

- B-DAT

- B-DAT

- B-DAT
formance O
(K O
= O
15 O
33.98dB O

- B-DAT
ter. O
Similar O
conclusions O
are O
observed O

- B-DAT
tempt O
to O
combine O
the O
powerful O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ficulties, O
but O
helps O
network O
converge O

- B-DAT

- B-DAT
tings O
as O
stated O
in O
Section O

- B-DAT
ters. O
Specifically, O
two O
types O
of O

- B-DAT
plied O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
terpart. O
Tests O
on O
Set5 O
with O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
datas O
are O
derived O
from O

- B-DAT

- B-DAT
fault O
settings O
given O
by O
the O

- B-DAT

- B-DAT
Memory O
(OOM) O
issue. O
The O
reason O

- B-DAT

- B-DAT

- B-DAT
terpretability. O
We O
extend O
the O
LISTA O

- B-DAT
volutional O
version O
and O
build O
the O

- B-DAT
sions O
without O
introducing O
any O
new O

- B-DAT
sults O
with O
state-of-the-arts O
and O
demonstrate O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
deep O
convolutional O
networks O

- B-DAT

- B-DAT
nary O
Learning: O
A O
Comparative O
Review O

- B-DAT

Delving O
Deep O
into O
Rec- B-DAT
tifiers O
- O
Surpassing O
Human-Level O
Performance O
on O
ImageNet O

- B-DAT
ing O
for O
Image O
Recognition. O
In O

- B-DAT
ble O
convolutional O
sparse O
coding. O
CVPR O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
CVPR O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT
lutional O
Network O
for O
Image O
Super-Resolution O

- B-DAT
ham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT

- B-DAT
ing O
a O
Generative O
Adversarial O
Network O

- B-DAT

- B-DAT
ing O
very O
deep O
convolutional O
encoder-decoder O

- B-DAT
cal O
statistics. O
volume O
2, O
pages O

- B-DAT
Vito, O
Z. O
Lin, O
A. O
Desmaison O

- B-DAT
matic O
differentiation O
in O
pytorch. O
In O

- B-DAT

- B-DAT

- B-DAT
ing. O
In O
ICASSP, O
pages O
2191–2195 O

- B-DAT

- B-DAT
tent O
memory O
network O
for O
image O

- B-DAT

- B-DAT

Challenge O
on O
Single O
Image O
Super-Resolution B-DAT
- O
Methods O
and O
Results. O
CVPR O
Workshops O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution: O
A O
benchmark. O
In O
ECCV O

- B-DAT
age O
super-resolution O
via O
sparse O
representation O

- B-DAT

- B-DAT
convolutional O
networks. O
CVPR, O
2010. O
2 O

- B-DAT

- B-DAT

A O
Survey O
of O
Sparse O
Representation O
- B-DAT
Algorithms O
and O
Applications. O
IEEE O
Access O

four O
benchmark O
datasets, O
Set5 O
[1], O
Set14 B-DAT
[39 O

Set14 B-DAT
×2 O
30.24/0.8688 O
32.45/0.9067 O
33.03/0.9124 O
33.04/0.9118 O

factor×2,×3 O
and×4 O
on O
datasets O
Set5, O
Set14, B-DAT
BSD100 O
and O
Urban100. O
Dataset O
Quality O

- B-DAT
age O
restoration. O
However, O
as O
the O

- B-DAT

- B-DAT
tle O
influence O
on O
the O
subsequent O

- B-DAT
tive O
learning O
process. O
The O
recursive O

- B-DAT

- B-DAT
tive O
fields. O
The O
representations O
and O

- B-DAT
vious O
states O
should O
be O
reserved O

- B-DAT
resolution O
and O
JPEG O
deblocking. O
Comprehensive O

- B-DAT
iments O
demonstrate O
the O
necessity O
of O

- B-DAT

- B-DAT

- B-DAT
quality O
version O
of O
x, O
D O

- B-DAT
der O
Grant O
Nos. O
91420201, O
61472187 O

- B-DAT

- B-DAT
search O
Fund. O
Jian O
Yang O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
noising O
aims O
to O
recover O
a O

- B-DAT
servation, O
which O
commonly O
assumes O
additive O

- B-DAT
sian O
noise O
with O
a O
standard O

- B-DAT

- B-DAT
resolution O
recovers O
a O
high-resolution O
(HR O

- B-DAT

- B-DAT

- B-DAT
trol O
the O
parameter O
number O
of O

- B-DAT
Recursive O
Convolutional O
Network O
(DRCN) O
[21 O

- B-DAT
gate O
training O
difficulty, O
Mao O
et O

- B-DAT

- B-DAT

- B-DAT
over, O
Zhang O
et O
al. O
[40 O

- B-DAT
path O
feed-forward O
architecture, O
where O
one O

- B-DAT
fluenced O
by O
its O
direct O
former O

- B-DAT

- B-DAT
ory. O
Some O
variants O
of O
CNNs O

- B-DAT

- B-DAT
cific O
prior O
state, O
namely O
restricted O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tent O
memory O
network O
(MemNet), O
which O

- B-DAT
ory O
block O
to O
explicitly O
mine O

- B-DAT
tion O
Net O
(FENet) O
first O
extracts O

- B-DAT

- B-DAT
tains O
a O
recursive O
unit O
and O

- B-DAT
science O
[6, O
25] O
that O
recursive O

- B-DAT
ist O
in O
the O
neocortex, O
the O

- B-DAT

- B-DAT
tive O
fields O
(blue O
circles O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ated O
from O
the O
previous O
memory O

- B-DAT

- B-DAT
ther, O
we O
present O
an O
extended O

- B-DAT

- B-DAT

- B-DAT
term O
memory O
should O
be O
reserved O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
even O
using O
a O
single O

- B-DAT
veal O
that O
the O
network O
depth O

- B-DAT
tion O
and O
achieved O
comparable O
performance O

- B-DAT
resolution O
convolutional O
neural O
network O
(SRCNN O

- B-DAT
dicts O
the O
nonlinear O
LR-HR O
mapping O

- B-DAT
volutional O
network, O
which O
significantly O
outperforms O

- B-DAT
cal O
shallow O
methods. O
The O
authors O

- B-DAT
tended O
CNN O
model, O
named O
Artifacts O

- B-DAT
tional O
Neural O
Networks O
(ARCNN) O
[7 O

- B-DAT

- B-DAT
ural O
sparsity O
of O
images O
[36 O

- B-DAT

- B-DAT
edge O
in O
the O
JPEG O
compression O

- B-DAT

- B-DAT

- B-DAT
ers O
to O
exploit O
large O
contextual O

- B-DAT
ing O
and O
adjustable O
gradient O
clipping O

- B-DAT
ization O
into O
a O
DnCNN O
model O

- B-DAT
age O
restoration O
tasks. O
To O
reduce O

- B-DAT

- B-DAT
connection O
to O
mitigate O
the O
training O

- B-DAT

- B-DAT
posed O
LapSRN O
to O
address O
the O

- B-DAT
curacy O
for O
SISR, O
which O
operates O

- B-DAT

- B-DAT
ages. O
Tai O
et O
al. O
[34 O

- B-DAT
work O
(DRRN) O
to O
address O
the O

- B-DAT

- B-DAT
lutional O
layer O
is O
used O
in O

- B-DAT
ture O
mapping, O
we O
have O

- B-DAT

- B-DAT

- B-DAT
ory O
block O
respectively. O
Finally, O
instead O

- B-DAT

- B-DAT

- B-DAT
age, O
our O
model O
uses O
a O

- B-DAT
notes O
the O
function O
of O
our O

- B-DAT
ber O
of O
training O
patches O
and O

- B-DAT
quality O
patch O
of O
the O
low-quality O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
block. O
Specifically, O
each O
residual O

- B-DAT

- B-DAT

- B-DAT
erate O
multi-level O
representations O
under O
different O

- B-DAT

- B-DAT
ory. O
Supposing O
there O
are O
R O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cursive O
unit. O
These O
representations O
are O

- B-DAT

- B-DAT

- B-DAT
vious O
memory O
blocks O
can O
be O

- B-DAT

- B-DAT
volutional O
layer O
(parameterized O
by O
W O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put O
from O
the O
ensemble O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
can O
get O
lost O
at O

- B-DAT
ward O
CNN O
process, O
and O
dense O

- B-DAT
ous O
layers O
can O
compensate O
such O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferent O
networks. O
(b) O
We O
convert O

- B-DAT

- B-DAT

- B-DAT
tral O
densities O
by O
integrating O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions, O
the O
latter O
layer O
absorbs O

- B-DAT

- B-DAT

- B-DAT
work O
– O
a O
very O
deep O

- B-DAT
works, O
inspired O
by O
LSTM, O
Highway O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tween O
MemNet O
and O
DRCN O
[21 O

- B-DAT
ule O
is O
a O
memory O
block O

- B-DAT
ules O
in O
DRCN, O
which O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
egy, O
which O
is O
imperative O
for O

- B-DAT
nected O
principle. O
In O
general, O
DenseNet O

- B-DAT
tion. O
In O
addition, O
DenseNet O
adopts O

- B-DAT

- B-DAT
tions O
in O
MemNet O
indeed O
play O

- B-DAT

- B-DAT

- B-DAT
nections. O
Average O
PSNR/SSIMs O
for O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ories O
from O
the O
last O
recursion O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tors O
are O
evaluated, O
including O
×2 O

- B-DAT
noising O
is O
used. O
As O
in O

- B-DAT
ing O
time O
and O
storage O
complexities O

- B-DAT

- B-DAT

- B-DAT
ent O
noise O
levels O
are O
all O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sions, O
are O
constructed O
(i.e., O
M6R6 O

- B-DAT
supervised O
MemNet, O
6 O
predictions O
are O

- B-DAT
tions, O
and O
is O
empirically O
set O

- B-DAT
mized O
via O
the O
mini-batch O
stochastic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nections. O
The O
reason O
is O
that O

- B-DAT

- B-DAT

- B-DAT
responding O
weights O
from O
all O
filters O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
we O
normalize O
the O
norms O

- B-DAT
ture O
map O
index O
l. O
We O

- B-DAT
ory O
block O
number O
increases. O
(3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity O
of O
our O
persistent O
memory O

- B-DAT
ent O
among O
different O
work, O
we O

- B-DAT
crease O
the O
parameters O
(filter O
number O

- B-DAT

- B-DAT
mance. O
With O
more O
training O
images O

- B-DAT
nificantly O
outperforms O
the O
state O
of O

- B-DAT

- B-DAT
lem O
in O
networks, O
we O
intend O

- B-DAT
plexity O
and O
accuracy. O
Fig. O
6 O

- B-DAT
notes O
the O
prediction O
of O
the O

- B-DAT
sult O
at O
the O
3rd O
prediction O

- B-DAT
creasing O
model O
complexity O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nal O
image O
is O
resized O
to O

- B-DAT
mance. O
However, O
in O
our O
MemNet O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rectly O
recovers O
the O
pillar. O
Please O

- B-DAT
isons O
for O
SISR. O
SRCNN O
[8 O

- B-DAT
sults O
on O
Classic5 O
and O
LIVE1 O

- B-DAT
erated O
by O
their O
corresponding O
public O

- B-DAT
work O
structures: O
M4R6, O
M6R6, O
M6R8 O

- B-DAT
posed O
deepest O
network O
M10R10 O
achieves O

- B-DAT

- B-DAT

- B-DAT
ory O
network O
(MemNet) O
is O
proposed O

- B-DAT

- B-DAT
ous O
CNN O
architectures. O
In O
each O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ous O
memory O
blocks O
are O
sent O

- B-DAT
resolution O
and O
JPEG O
deblocking O
simultaneously O

- B-DAT
hensive O
benchmark O
evaluations O
well O
demonstrate O

- B-DAT
riority O
of O
our O
MemNet O
over O

- B-DAT

- B-DAT

- B-DAT
ative O
neighbor O
embedding. O
In O
BMVC O

- B-DAT

- B-DAT
age O
denoising O
by O
sparse O
3-D O

- B-DAT

- B-DAT
bridge, O
MA: O
MIT O
Press, O
2001 O

- B-DAT
tifacts O
reduction O
by O
a O
deep O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
volutional O
networks. O
In O
NIPS, O
2008 O

- B-DAT

- B-DAT
ing O
of O
non-parametric O
image O
restoration O

- B-DAT
shick, O
S. O
Guadarrama, O
and O
T O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT
tional O
network O
for O
image O
super-resolution O

- B-DAT

- B-DAT
based O
learning O
applied O
to O
document O

- B-DAT
ings O
of O
the O
IEEE, O
1998 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ric O
skip O
connections. O
In O
NIPS O

- B-DAT
cal O
statistics. O
In O
ICCV, O
2001 O

- B-DAT
stricted O
boltzmann O
machines. O
In O
ICML O

- B-DAT

- B-DAT

- B-DAT
compressed O
images. O
In O
CVPR, O
2016 O

- B-DAT

- B-DAT

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT
up O
using O
sparse-representations. O
Curves O
and O

- B-DAT
yond O
a O
gaussian O
denoiser: O
Residual O

used O
benchmark O
datasets: O
Set5 O
[1], O
Set14 B-DAT
[27], O
BSD100 O
[18], O
Urban100 O
[10 O

]. O
Among O
these O
datasets, O
Set5, O
Set14 B-DAT
and O
BSD100 O
consist O
of O
natural O

Set14 B-DAT
×2 O
30.24/0.8688 O
33.03/0.9124 O
33.04/0.9118 O
32.99/0.9124 O

Set14 B-DAT
×2 O
6.105 O
8.159 O
8.370 O
8.501 O

Set14 B-DAT
×2 O
0.113 O
1.579 O
0.035 O
8.540 O

The O
“barbara” O
image O
from O
the O
Set14 B-DAT
dataset O
with O
an O
upscaling O
factor O

the O
average O
inference O
time O
for O
upscaling B-DAT
3× O
on O
Set5. O
The O
IDN O

restoration O
per- O
formance O
with O
larger O
upscaling B-DAT
factors, O
the O
recent O
SR O
meth O

the O
original O
HR O
images O
with O
upscaling B-DAT
factor O
m O
(m O
= O
2 O

the O
Set14 O
dataset O
with O
an O
upscaling B-DAT
factor O
4 O

the O
BSD100 O
dataset O
with O
an O
upscaling B-DAT
factor O
4 O

the O
Urban100 O
dataset O
with O
an O
upscaling B-DAT
factor O
4 O

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

10]. O
Among O
these O
datasets, O
Set5, O
Set14 B-DAT

The O
“barbara” O
image O
from O
the O
Set14 B-DAT

- B-DAT

- B-DAT
age O
super-resolution. O
However, O
as O
the O

- B-DAT

- B-DAT

- B-DAT
ods O
have O
been O
faced O
with O

- B-DAT
pact O
convolutional O
network O
to O
directly O

- B-DAT
tion O
block, O
the O
local O
long O

- B-DAT

- B-DAT
fectively O
extracted. O
Specifically, O
the O
proposed O

- B-DAT
quential O
blocks. O
In O
addition, O
the O

- B-DAT
tion. O
Experimental O
results O
demonstrate O
that O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cially O
in O
terms O
of O
time O

- B-DAT

- B-DAT

- B-DAT
lem O
in O
low-level O
computer O
vision O

- B-DAT

- B-DAT

- B-DAT
age. O
Actually, O
an O
infinite O
number O

- B-DAT

- B-DAT
ists. O
In O
order O
to O
mitigate O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formance O
with O
larger O
upscaling O
factors O

- B-DAT
ods O
fall O
into O
the O
example-based O

- B-DAT
ral O
network O
(CNN), O
many O
CNN-based O

- B-DAT
mance. O
Kim O
et O
al. O
propose O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ment O
of O
enormous O
parameters O
of O

- B-DAT
der O
to O
achieve O
better O
performance O

- B-DAT
ory O
consumption, O
which O
are O
less O

- B-DAT
caded O
network O
topologies, O
e.g., O
VDSR O

- B-DAT
brating O
channel-wise O
features O
responses O
can O

- B-DAT
mation O
distillation O
network O
(IDN) O
with O

- B-DAT
ters O
and O
computational O
complexity O
as O

- B-DAT
formation O
distillation O
blocks O
(DBlocks) O
are O

- B-DAT
gressively O
distill O
residual O
information. O
Finally O

- B-DAT
tion O
Block O
(RBlock) O
aggregates O
the O

- B-DAT

- B-DAT
tion O
block, O
which O
contains O
an O

- B-DAT
pression O
unit. O
The O
enhancement O
unit O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pressive O
power, O
we O
send O
a O

- B-DAT

- B-DAT

- B-DAT
ture O
maps O
into O
two O
parts O

- B-DAT
path O
features O
and O
another O
expresses O

- B-DAT

- B-DAT

- B-DAT
mary, O
the O
enhancement O
unit O
is O

- B-DAT
sentation O
power O
of O
the O
network O

- B-DAT
ber O
of O
convolutional O
layer O

- B-DAT

- B-DAT

- B-DAT
tains O
better O
reconstruction O
accuracy O

- B-DAT

- B-DAT
ied O
in O
these O
years. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
similarity O
property O
and O
extract O
example O

- B-DAT
terns O
and O
textures O
but O
lacks O

- B-DAT
tory O
prediction O
for O
images O
of O

- B-DAT

- B-DAT
spective O
deformation O

- B-DAT

- B-DAT

- B-DAT
pact O
dictionary O
or O
manifold O
space O

- B-DAT
dom O
forest O
[20] O
and O
sparse O

- B-DAT
mal O
for O
generating O
high-quality O
SR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
to O
accelerate O
SRCNN O
in O

- B-DAT
signed O
by O
Kim O
et O
al O

- B-DAT
tion O
with O
skip O
connection O
to O

- B-DAT
decoder O
networks O
and O
symmetric O
skip O

- B-DAT

- B-DAT
work O
(LapSRN) O
to O
address O
the O

- B-DAT

- B-DAT
ages. O
Tai O
et O
al. O
[22 O

- B-DAT
work O
to O
effectively O
build O
a O

- B-DAT
racy. O
The O
authors O
also O
present O

- B-DAT

- B-DAT

- B-DAT
sistent O
memory O
network O
(MemNet) O
[23 O

- B-DAT
tion O
task, O
which O
tackles O
the O

- B-DAT

- B-DAT
pose O
a O
novel O
combination O
of O

- B-DAT
construction O
block O
(RBlock). O
Here, O
we O

- B-DAT
formation O
distillation O
blocks O
by O
using O

- B-DAT

- B-DAT

- B-DAT
tively. O
Finally, O
we O
take O
a O

- B-DAT

- B-DAT
age O
restoration O
as O
defined O
below O

- B-DAT
mulated O
as O
follows O

- B-DAT

- B-DAT
lutions O
and O
another O
is O
the O

- B-DAT

- B-DAT
lows O

- B-DAT
while O
is O
the O
input O
of O

- B-DAT

- B-DAT
eration O
respectively. O
Specifically, O
we O
know O

- B-DAT

- B-DAT
path O
information O
as O
the O
input O

- B-DAT

- B-DAT
ations O
of O
the O
below O
module O

- B-DAT
path O
information O
and O
the O
local O

- B-DAT

- B-DAT
mulated O
as O

- B-DAT

- B-DAT

- B-DAT
lized O
without O
exception O
by O
a O

- B-DAT
tage O
of O
a O
1× O
1 O

- B-DAT
tation O
Dataset O
(BSD) O
[18] O
as O

- B-DAT
tation O
in O
three O
ways: O
(1 O

- B-DAT
channel, O
while O
color O
components O
are O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferent O
scaling O
factors O

- B-DAT
sample O
the O
original O
HR O
images O

- B-DAT
erate O
the O
corresponding O
LR O
images O

- B-DAT

- B-DAT

- B-DAT
ters O
will O
generate O
the O
output O

- B-DAT

- B-DAT
mum O
size O
of O
the O
sub-image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ble O
1 O

- B-DAT

- B-DAT

- B-DAT
volution O
layer O
[6, O
24] O
in O

- B-DAT

- B-DAT

- B-DAT
ters O
as O
the O
initial O
values O

- B-DAT
terfly” O
image O
from O
Set5 O
dataset O

- B-DAT
ture O
information O
and O
its O
normalized O

pixel O
value O
ranges O
from O
-0 B-DAT

- B-DAT
ter O
visualizing O
the O
intermediary O
of O

- B-DAT
age O
feature O
map O
can O
roughly O

- B-DAT
ment O
unit O
and O
compression O
unit O

- B-DAT
ing O
above-mentioned O
method. O
As O
illustrated O

- B-DAT
figures O
show O
that O
the O
later O

- B-DAT
creasing O
the O
pixel O
values O
to O

- B-DAT
atively O
clear O
contour O
profile. O
In O

- B-DAT
ure O
obviously O
surpasses O
the O
former O

- B-DAT
paring O
Figure O
5(a) O
with O
Figure O

- B-DAT
ure O
5(b) O
and O
the O
third O

- B-DAT
age. O
The O
bias O
term O
of O

- B-DAT
matically O
adjust O
the O
central O
value O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods, O
including O
bicubic, O
SRCNN O
[3 O

- B-DAT

- B-DAT

- B-DAT
vorably O
against O
state-of-the-art O
results O
on O

- B-DAT
man O
perception O
of O
image O
super-resolution O

- B-DAT
bara” O
image O
has O
serious O
artifacts O

- B-DAT
viously O
see O
that O
the O
proposed O

- B-DAT
tively O
clear O
in O
the O
proposed O

- B-DAT
put O
so O
that O
more O
information O

- B-DAT
age. O
The O
algorithms O
that O
take O

- B-DAT
chine O
with O
4.2GHz O
Intel O
i7 O

- B-DAT
ages O
in O
BSD100 O
and O
Urban100 O

- B-DAT
ages O
into O
several O
parts O
and O

- B-DAT
worthy O
that O
the O
proposed O
IDN O

- B-DAT
ban100 O
dataset O

- B-DAT
cient O
features O
for O
the O
reconstruction O

- B-DAT
posed O
approach O
achieves O
competitive O
results O

- B-DAT
mark O
datasets O
in O
terms O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pact O
network O
will O
be O
more O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
rable O
convolutions. O
In O
CVPR, O
pages O

- B-DAT
based O
super-resolution. O
IEEE O
Computer O
Graphics O

- B-DAT
plications, O
22(2):56–65, O
2002. O
2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works. O
In O
arXiv:1709.01507, O
2017. O
2 O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
ishick, O
S. O
Guadarrama, O
and O
T O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT
tional O
network O
for O
image O
super-resolution O

- B-DAT
resolution. O
In O
CVPR, O
pages O
624–632 O

- B-DAT

- B-DAT
ing O
very O
deep O
convolutional O
encoder-decoder O

- B-DAT
cal O
statistics. O
In O
CVPR, O
pages O

- B-DAT

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT
tent O
memory O
network O
for O
image O

- B-DAT
resolution O
as O
sparse O
representation O
of O

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT

- B-DAT

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
TOG O

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
strated O
high-quality O
reconstruction O
for O
single-image O

- B-DAT
resolution. O
In O
this O
paper, O
we O

- B-DAT

- B-DAT
construct O
the O
sub-band O
residuals O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
als, O
and O
uses O
transposed O
convolutions O

- B-DAT
lation O
as O
the O
pre-processing O
step O

- B-DAT
duces O
the O
computational O
complexity. O
We O

- B-DAT

- B-DAT
thermore, O
our O
network O
generates O
multi-scale O

- B-DAT

- B-DAT
tion, O
thereby O
facilitates O
resource-aware O
applications O

- B-DAT
tensive O
quantitative O
and O
qualitative O
evaluations O

- B-DAT
mark O
datasets O
show O
that O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
have O
demonstrated O
the O
state-of-the-art O

- B-DAT
dom O
forest O
[26 O

- B-DAT

- B-DAT
ear O
LR-to-HR O
mapping. O
The O
network O

- B-DAT

- B-DAT
ture O
[17]. O
While O
these O
models O

- B-DAT
sults, O
there O
are O
three O
main O

- B-DAT

- B-DAT
lution O
before O
applying O
the O
network O

- B-DAT
processing O
step O
increases O
unnecessary O
computational O

- B-DAT

- B-DAT
erator O
with O
sub-pixel O
convolution O
[28 O

- B-DAT
volution O
[8] O
(also O
named O
as O

- B-DAT
ture O
the O
underlying O
multi-modal O
distributions O

- B-DAT
smooth O
and O
not O
close O
to O

- B-DAT
ural O
images. O
Third, O
most O
methods O

- B-DAT
isting O
methods O
cannot O
generate O
intermediate O

- B-DAT
ent O
desired O
upsampling O
scales O
and O

- B-DAT

- B-DAT
work O
takes O
an O
LR O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
layer O
for O
upsampling O
the O

- B-DAT
band O
residuals O
(the O
differences O
between O

- B-DAT
age O
and O
the O
ground O
truth O

- B-DAT
tion O
operations. O
While O
the O
proposed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pling O
filters O
with O
deep O
convolutional O

- B-DAT
band O
residuals. O
The O
deep O
supervision O

- B-DAT
tal O
results O
demonstrate O
that O
our O

- B-DAT
eral O
CNN O
based O
super-resolution O
models O

- B-DAT
CNN O
[8], O
our O
LapSRN O
achieves O

- B-DAT

- B-DAT

- B-DAT
ble O
to O
a O
wide O
range O

- B-DAT
aware O
adaptability. O
For O
example, O
the O

- B-DAT
ing O
on O
the O
available O
computational O

- B-DAT
ios O
with O
limited O
computing O
resources O

- B-DAT
putation O
of O
residuals O
at O
finer O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
on O
recent O
example-based O
approaches O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mid O
of O
the O
low-resolution O
input O

- B-DAT
ternal O
image O
databases, O
the O
number O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ically O
slow O
due O
to O
the O

- B-DAT

- B-DAT
ods O
learn O
the O
LR-HR O
mapping O

- B-DAT
rithms, O
such O
as O
nearest O
neighbor O

- B-DAT
ding O
[2, O
5], O
kernel O
ridge O

- B-DAT
resentation O
[37, O
38, O
39]. O
Instead O

- B-DAT
ods O
partition O
the O
image O
database O

- B-DAT

- B-DAT
ear O
regressors O
for O
each O
cluster O

- B-DAT

- B-DAT
CNN O
[7] O
jointly O
optimize O
all O

- B-DAT
linear O
mapping O
in O
the O
image O

- B-DAT

- B-DAT
sample O
images O
to O
the O
desired O

- B-DAT
cursive O
layers O
(DRCN) O
to O
reduce O

- B-DAT

- B-DAT
work O
[28] O
extracts O
feature O
maps O

- B-DAT
places O
the O
bicubic O
upsampling O
operation O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
for O
learning O
and O
predicting O

- B-DAT
CNN, O
VDSR, O
DRCN O
and O
our O

- B-DAT

- B-DAT
ods O
and O
the O
proposed O
framework O

- B-DAT

- B-DAT
sampling O
filters O
with O
convolutional O
and O

- B-DAT
lutional O
layers. O
Using O
the O
learned O

- B-DAT
stead O
of O
the O
`2 O
loss O

- B-DAT
construction O
accuracy. O
Third, O
as O
the O

- B-DAT
gressively O
reconstructs O
HR O
images, O
the O

- B-DAT

- B-DAT
tic O
segmentation O
[11, O
25]. O
Denton O

- B-DAT
GAN) O
to O
generate O
realistic O
images O

- B-DAT
resolution O
model O
that O
predicts O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
GAN, O
the O
convolutional O
layers O
at O

- B-DAT

- B-DAT
tations O
at O
lower O
levels. O
The O

- B-DAT

- B-DAT

- B-DAT
GAN O
are O
independently O
trained. O
On O

- B-DAT

- B-DAT

- B-DAT
isons O
with O
LAPGAN O
in O
the O

- B-DAT
ial O
loss O
for O
photo-realistic O
SR O

- B-DAT
versarial O
loss O
in O
the O
supplementary O

- B-DAT
ual O
images O
at O
log2 O
S O

- B-DAT

- B-DAT
resolving O
an O
LR O
image O
at O

- B-DAT
struction. O
Feature O
extraction. O
At O
level O

- B-DAT
posed O
convolutional O
layer O
to O
upsample O

- B-DAT
tures O
by O
a O
scale O
of O

- B-DAT
volutional O
layer O
is O
connected O
to O

- B-DAT
traction O
at O
the O
coarse O
resolution O

- B-DAT
tional O
layer. O
In O
contrast O
to O

- B-DAT

- B-DAT
sampled O
by O
a O
scale O
of O

- B-DAT
linear O
kernel O
and O
allow O
it O

- B-DAT

- B-DAT
ual O
image O
from O
the O
feature O

- B-DAT

- B-DAT
ilar O
structure O
at O
each O
level O

- B-DAT
work O
parameters O
to O
be O
optimized O

- B-DAT

- B-DAT
age O
ŷ O
= O
f O
(x;θ O

- B-DAT
scaled O
LR O
image O
by O
xs O

- B-DAT
tion O
and O
the O
corresponding O
ground O

- B-DAT

- B-DAT

- B-DAT
ing O
to O
predict O
sub-band O
residual O

- B-DAT

- B-DAT

- B-DAT
sults O
in O
one O
feed-forward O
pass O

- B-DAT

- B-DAT
volutional O
and O
transposed O
convolutional O
layers O

- B-DAT

- B-DAT

- B-DAT
scale O
between O
[0.5,1.0]. O
(2) O
Rotation O

- B-DAT
age O
by O
90◦, O
180◦, O
or O

- B-DAT
izontally O
or O
vertically O
with O
a O

- B-DAT
verges O
faster O
and O
achieves O
improved O

- B-DAT
tions, O
and O
residual O
learning. O
We O

- B-DAT
mance O
(PSNR) O
drop O
on O
both O

- B-DAT
mentum O
parameter O
to O
0.9 O
and O

- B-DAT
SRN O
with O
state-of-the-art O
algorithms O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
performs O
SRCNN O
within O
10 O
epochs O

- B-DAT
trated O
in O
Figure O
2, O
the O

- B-DAT
posed O
network. O
(a) O
HR O
image O

- B-DAT

- B-DAT
formance O
with O
SRCNN. O
In O
Figure O

- B-DAT
struct O
by O
the O
proposed O
algorithm O

- B-DAT
tively O
clean O
and O
sharp O
details O

- B-DAT
ment O
(e.g. O
0.7 O
dB O
on O

- B-DAT
ent O
depth, O
d O
= O
3,5,10,15 O

- B-DAT
offs O
between O
performance O
and O
speed O

- B-DAT
mance O
and O
speed. O
We O
show O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
SR O
algorithms: O
A+ O
[30 O

- B-DAT
BAN100 O
[15] O
and O
MANGA109 O
[23 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
BAN100 O
contains O
challenging O
urban O
scenes O

- B-DAT
ods O
on O
most O
datasets. O
In O

- B-DAT

- B-DAT
less, O
our O
8× O
model O
provides O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pling O
for O
pre-processing O
generate O
results O

- B-DAT
tifacts O
[7, O
17, O
26, O
30 O

- B-DAT
tively O
suppresses O
such O
artifacts O
through O

- B-DAT
struction O
and O
the O
robust O
loss O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
RCNN, O
RFL O
and O
VDSR O
using O

- B-DAT
upsampled O
images O
[7, O
17, O
30 O

- B-DAT

- B-DAT
pling O
[8]. O
The O
state-of-the-art O
methods O

- B-DAT

- B-DAT
structs O
high-quality O
HR O
images O
at O

- B-DAT
ods O
in O
the O
supplementary O
material O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
struct O
these O
models O
in O
MatConvNet O

- B-DAT

-210 B-DAT

-110010 B-DAT
1102 O

- B-DAT

- B-DAT

- B-DAT
RCNN. O
We O
present O
detailed O
evaluations O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
torical O
photographs O
with O
JPEG O
compression O

- B-DAT

- B-DAT
sampling O
kernels O
are O
available. O
As O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ply O
super-resolution O
frame O
by O
frame O

- B-DAT
proach O
achieve O
real-time O
performance O
(i.e O

- B-DAT
lucinate” O
fine O
details. O
As O
shown O

- B-DAT

- B-DAT
herence O
or O
motion O
blur O
are O

- B-DAT

- B-DAT

- B-DAT
ric O
SR O
methods O
[7, O
8 O

- B-DAT
duce O
the O
number O
of O
parameters O

- B-DAT
curate O
single-image O
super-resolution. O
Our O
model O

- B-DAT
sively O
predicts O
high-frequency O
residuals O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mizing O
the O
network O
with O
a O

- B-DAT
posed O
LapSRN O
alleviates O
issues O
with O

- B-DAT
tions O
on O
benchmark O
datasets O
demonstrate O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gorithms O
in O
terms O
of O
visual O

- B-DAT
search O
under O
Grant O
N00014-16-1-2314 O

- B-DAT
Morel. O
Low-complexity O
single-image O
super-resolution O
based O

- B-DAT
ods. O
IJCV, O
61(3):211–231, O
2005. O
4 O

- B-DAT
tions, O
31(4):532–540, O
1983. O
3 O

- B-DAT

- B-DAT
works. O
In O
NIPS, O
2015. O
3 O

- B-DAT
resolution O
using O
deep O
convolutional O
networks O

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT

- B-DAT
based O
super-resolution. O
IEEE, O
Computer O
Graphics O

- B-DAT
plications, O
22(2):56–65, O
2002. O
2 O

- B-DAT
tion O
and O
refinement O
for O
semantic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sis/synthesis. O
In O
SIGGRAPH, O
1995. O
3 O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT
tional O
network O
for O
image O
super-resolution O

- B-DAT

- B-DAT

- B-DAT
ham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT

- B-DAT
supervised O
nets, O
2015. O
In O
International O

- B-DAT
ficial O
Intelligence O
and O
Statistics, O
2015 O

- B-DAT
resolution O
via O
deep O
draft-ensemble O
learning O

- B-DAT

- B-DAT
timedia O
Tools O
and O
Applications, O
pages O

- B-DAT
ters: O
Edge-aware O
image O
processing O
with O

- B-DAT
ing O
to O
refine O
object O
segments O

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT
mation O
fidelity O
criterion O
for O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution: O
a O
benchmark. O
In O
ECCV O

- B-DAT

- B-DAT
resolution O
as O
sparse O
representation O
of O

- B-DAT
resolution O
via O
sparse O
representation. O
TIP O

- B-DAT

- B-DAT

fol- O
lowing O
datasets: O
Set5 O
[4], O
Set14 B-DAT
[46], O
B100 O
[30] O
and O
Ur O

of O
super- O
resolved O
images O
on O
Set14 B-DAT
with O
x3 O
upscaling O
among O
differ O

LR O
images O
directly O
and O
learned O
upscaling B-DAT
filters O
in O
the O
last O
layer O

the O
same O
as O
the O
SR O
upscaling B-DAT
factor O

of O
our O
DSRN O
with O
×2 O
upscaling B-DAT
on O
Set5 O
dataset O

performance O
drop O
across O
all O
three O
upscaling B-DAT
scales O
when O
changing O
from O
shared O

on O
Set O
14 O
with O
×3 O
upscaling B-DAT

recent O
SR O
methods O
for O
×3 O
upscaling B-DAT
on O
Set O
14 O

images O
on O
Set14 O
with O
x3 O
upscaling B-DAT
among O
differ- O
ent O
SR O
approaches O

Fast O
and O
ac- O
curate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

of O
super- O
resolved O
images O
on O
Set14 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
erate O
at O
a O
fixed O
spatial O

- B-DAT
resolution O
(LR) O
and O
high-resolution O
(HR O

- B-DAT
layed O
feedback. O
Extensive O
quantitative O
and O

- B-DAT
uations O
on O
benchmark O
datasets O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formance O
has O
been O
significantly O
improved O

- B-DAT
velopments O
in O
deep O
neural O
networks O

- B-DAT
ing O
[16] O
have O
been O
widely O

- B-DAT
tently O
observed. O
The O
first O
is O

- B-DAT
ping O
from O
LR O
to O
HR O

- B-DAT
work O
depth O
enlarges O
the O
size O

-15 B-DAT

-1 B-DAT

-0317 B-DAT

- B-DAT
struct O
missing O
HR O
components. O
The O

- B-DAT
ing O
gradients, O
facilitating O
the O
training O

- B-DAT
troduces O
more O
parameters, O
and O
thus O

- B-DAT

- B-DAT

- B-DAT
tional O
Network O
(DRCN) O
[21] O
shares O

- B-DAT
ent O
residual O
units O
and O
achieves O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ral O
networks O
(RNNs). O
Specifically, O
Liao O

- B-DAT

- B-DAT
work O
(ResNet) O
[16] O
is O
equivalent O

- B-DAT
spired O
by O
their O
findings, O
we O

- B-DAT
olution O
(bicubic O
interpolation O
is O
first O

- B-DAT

- B-DAT
nite O
unfolding O
in O
time O
of O

- B-DAT
tioning O
that O
we O
follow O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ventional O
RNN O
model O
is O
generally O

- B-DAT

- B-DAT
dering O
our O
model O
a O
Dual-State O

- B-DAT

- B-DAT
tions. O
This O
provides O
information O
flow O

- B-DAT
ery O
single O
unrolling O
time. O
In O

- B-DAT
age O
Restoration O
and O
Enhancement O
workshop O

- B-DAT

- B-DAT
sive O
experimental O
results O
validate O
that O

- B-DAT
CNN, O
to O
predict O
the O
nonlinear O

- B-DAT

- B-DAT
strated O
superior O
performance O
to O
many O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mization O
techniques O
[34, O
28, O
11 O

- B-DAT
posed O
a O
sparse O
coding O
network O

- B-DAT

- B-DAT
end, O
demonstrating O
the O
benefit O
of O

- B-DAT
work O
in O
[42 O

- B-DAT
ageNet O
challenges O
[9], O
Kim O
et O

- B-DAT
ents. O
However, O
as O
the O
model O

- B-DAT
rameters O
increases. O
To O
control O
the O

- B-DAT
culty O
of O
training. O
Tai O
et O

- B-DAT
ual O
SR O
learning O
algorithms O
are O

- B-DAT
ual O
learning O
or O
local O
residual O

- B-DAT
rameter O
efficient O
via O
recursive O
learning O

- B-DAT
works O
(DenseNet) O
[17] O
instead O
of O

- B-DAT
put O
images, O
Shi O
et O
al O

- B-DAT
duces O
the O
computation O
cost. O
Similarly O

- B-DAT
bination O
with O
smaller O
filter O
sizes O

- B-DAT
Resolution O
Network O
(LapSRN) O
[22] O
works O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ant, O
which O
learns O
different O
scaled O

- B-DAT
allel O
via O
weight O
sharing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
posed O
nature O
of O
single O
image O

- B-DAT
tions O
and O
poor O
subjective O
scores O

- B-DAT
back, O
Generative O
Adversarial O
Networks O
have O

- B-DAT
uation O
by O
mean-opinion-score O
showed O
huge O

- B-DAT

- B-DAT
vide O
a O
better O
understanding O
of O

- B-DAT
cations O

- B-DAT

- B-DAT
tem. O
Then, O
based O
on O
this O

- B-DAT
ment O
of O
SR O
models O
with O

- B-DAT

- B-DAT
rent O
states. O
Depending O
on O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put, O
and O
recurrent O
states O
are O

- B-DAT
tively. O
The O
arrow O
link O
indicates O

- B-DAT
tion O
on O
this O
general O
formulation O

- B-DAT
rection O
to O
a O
fixed O
length O

- B-DAT

- B-DAT

- B-DAT
independent, O
which O
means O
these O
parameters O

- B-DAT
out O
any O
down-sampling O
or O
up-sampling O

- B-DAT
sions O
remain O
the O
same O
across O

- B-DAT
ventional O
residual O
block, O
which O
contains O

- B-DAT
tional O
layers O
with O
skip O
connections O

- B-DAT
rameters O

- B-DAT
ventional O
ResNet O
is O
that O
the O

- B-DAT

- B-DAT
gle O
convolutional O
layer O
to O
the O

- B-DAT
press O
frecurrent. O
The O
graph O
is O

- B-DAT
over, O
unlike O
the O
ResNet O
where O

- B-DAT
tion O
comes O
from O
the O
previous O

- B-DAT
rolled O
state O
s0. O
Figure O
1(e O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
state O
design, O
which O
adopts O
two O

- B-DAT

- B-DAT

- B-DAT
tion O
from O
both O
the O
LR O

- B-DAT
tively. O
Four O
colored O
arrows O
indicate O

- B-DAT

- B-DAT

- B-DAT
tion O
flows O
between O
sl O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

of O
a O
single-state B-DAT
RNN. O
(c) O
- O
(e) O
The O
required O
recurrent O
function O

- B-DAT

- B-DAT
tom O
one O
is O
LR. O
This O

- B-DAT
cialization O
for O
different O
resolutions O
and O

- B-DAT

- B-DAT

- B-DAT
sampling O
transition. O
The O
strides O
in O

- B-DAT

- B-DAT

- B-DAT
ing O
a O
prediction O
at O
every O

- B-DAT
terized O
by O
a O
single O
convolutional O

- B-DAT
ing O
the O
prediction O
only O
at O

- B-DAT
over, O
the O
model O
predicts O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lowing O
datasets: O
Set5 O
[4], O
Set14 O

- B-DAT
ban100 O
[19]. O
The O
training O
data O

- B-DAT
dom O
flipping O
along O
the O
vertical O

- B-DAT

- B-DAT

- B-DAT
vided O
training O
and O
validation O
sets O

- B-DAT
tioned O
data O
augmentations O
except O
random O

- B-DAT

- B-DAT
lution O
filters. O
Due O
to O
our O

- B-DAT

- B-DAT
sions O
as O
the O
LR O
and O

- B-DAT

- B-DAT
volution O
is O
applied O

- B-DAT
form O
distribution O
using O
the O
method O

- B-DAT
mentum O
0.95 O
as O
our O
optimizer O

- B-DAT
ing O
rate O
from O
{0.1,0.03,0.01} O
and O

- B-DAT
ing O
rate O
annealing O
is O
driven O

- B-DAT

- B-DAT
posed O
the O
use O
of O
unshared O

- B-DAT
folding O
time O
to O
resolve O
this O

- B-DAT
nary O
ReLU O
as O
the O
activation O

- B-DAT
imum O
effective O
depth O
of O
the O

- B-DAT
work O
is O
2T O
+ O
4 O

- B-DAT
ers O
in O
a O
residual O
block O

- B-DAT
iary O
input O
and O
output O
layers O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity O
and O
computation O
cost. O
We O

- B-DAT
pirical O
results O
are O
shown O
in O

- B-DAT
tioning O
that O
we O
also O
experimented O

- B-DAT
ing O
to O
be O
crucial O
for O

- B-DAT
forms O
much O
more O
poorly O
than O

- B-DAT

- B-DAT
part. O
Specifically, O
we O
observe O
around O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
state O
baseline O
and O
the O
DSRN O

- B-DAT
tion, O
comparing O
our O
models O
with O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eral O
public O
benchmark O
datasets O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentation O
Dataset O
[2], O
while O
our O

- B-DAT
tive O
performance O
across O
all O
datasets O

- B-DAT
cently O
developed O
DIV2K O
dataset O
and O

- B-DAT
ranking O
algorithms O
in O
Table O
2 O

- B-DAT
petitive O
performance O
with O
the O
best O

- B-DAT
state O
recurrent O
structure O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolved O
images O
on O
Set14 O
with O

- B-DAT
ent O
SR O
approaches. O
For O
these O

- B-DAT
tures O
and O
is O
less O
prone O

- B-DAT
trate O
the O
parameters-to-PSNR O
relationship O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mance, O
and O
has O
modest O
inference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
folding O
of O
a O
single-state O
RNN O

- B-DAT
tions. O
Based O
on O
this, O
we O

- B-DAT
ering O
a O
dual-state O
design; O
the O

- B-DAT
posed O
DSRN O
operate O
at O
different O

- B-DAT

- B-DAT
iments O
on O
benchmark O
datasets O
have O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tour O
detection O
and O
hierarchical O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
based O
on O
nonnegative O
neighbor O

- B-DAT
resolution O
through O
neighbor O
embedding. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
database. O
In O
CVPR, O
pages O

- B-DAT
ing O
a O
deep O
convolutional O
network O

- B-DAT
resolution. O
In O
ECCV, O
2014. O
1 O

- B-DAT

- B-DAT

- B-DAT
ual O
networks O
for O
image O
super-resolution O

- B-DAT

- B-DAT

- B-DAT
TATS, O
2010. O
5 O

- B-DAT

- B-DAT
berger. O
Deep O
networks O
with O
stochastic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works. O
In O
CVPR, O
2016. O
1 O

- B-DAT
recursive O
convolutional O
network O
for O
image O

- B-DAT
resolution. O
In O
CVPR, O
2016. O
1 O

- B-DAT

- B-DAT
ningham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT
hanced O
deep O
residual O
networks O
for O

- B-DAT
resolution. O
In O
CVPR O
Workshops, O
2017 O

- B-DAT

- B-DAT
ing O
a O
mixture O
of O
deep O

- B-DAT
resolution. O
In O
ACCV, O
pages O
145–156 O

- B-DAT

- B-DAT
tion O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT
hancenet: O
Single O
image O
super-resolution O
through O

- B-DAT
tomated O
texture O
synthesis. O
In O
ICCV O

- B-DAT
curate O
image O
upscaling O
with O
super-resolution O

- B-DAT
ment O
using O
natural O
scene O
statistics O

- B-DAT

- B-DAT
gle O
image O
and O
video O
super-resolution O

- B-DAT

- B-DAT

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT

- B-DAT
resolution O
using O
dense O
skip O
connections O

- B-DAT
works O
behave O
like O
ensembles O
of O

- B-DAT
works. O
In O
NIPS, O
2016. O
1 O

- B-DAT
celli. O
Image O
quality O
assessment: O
from O

- B-DAT

- B-DAT

- B-DAT
lution. O
In O
CVPRW, O
pages O
1–8 O

- B-DAT
resolution. O
TIP, O
21(8):3467–3478, O
2012. O
2 O

- B-DAT

- B-DAT
lective O
tensor O
factorization O
in O
deep O

- B-DAT

- B-DAT

- B-DAT
ity O
measures O
of O
recurrent O
neural O

testing O
datasets O
(i.e., O
Set5 O
and O
Set14, B-DAT
BSD100 O
and O
Urban100 O
[40]) O
used O

Set14 B-DAT
3 O
29.43 O
/ O
0.8232 O
29.77 O

Gaussian O
denoising, O
SISR O
with O
multiple O
upscaling B-DAT
factors, O
and O
JPEG O
deblocking O
with O

noise O
level, O
SISR O
with O
multiple O
upscaling B-DAT
factors, O
and O
JPEG O
deblocking O
with O

levels, O
down-sampled O
images O
with O
multiple O
upscaling B-DAT
factors, O
and O
JPEG O
images O
with O

model O
for O
all O
the O
three O
upscaling B-DAT
factors O
(i.e., O
2, O
3 O
and O

butterfly” O
from O
Set5 O
dataset O
with O
upscaling B-DAT
factor O
3 O

image O
from O
Urban100 O
dataset O
with O
upscaling B-DAT
factor O
4 O

bicubically O
interpolated O
low-resolution O
images O
with O
upscaling B-DAT
factor O
2 O
(upper O
middle) O
and O

Set14 B-DAT
3 I-DAT
29.43 O
/ O
0.8232 O
29.77 O

- B-DAT

- B-DAT

- B-DAT
works, O
Residual O
Learning, O
Batch O
Normalization O

- B-DAT

- B-DAT
stitute O
of O
Technology, O
Harbin O
150001 O

- B-DAT

- B-DAT
nic O
University, O
Hong O
Kong O
(e-mail O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
backs. O
First, O
those O
methods O
generally O

- B-DAT
timization O
problem O
in O
the O
testing O

- B-DAT

- B-DAT
based O
methods O
can O
hardly O
achieve O

- B-DAT

- B-DAT
mance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
larization O
and O
learning O
methods O
for O

- B-DAT

- B-DAT
allel O
computation O
on O
modern O
powerful O

- B-DAT
noised O
image O
x̂, O
the O
proposed O

- B-DAT
tion O
technique O
is O
further O
introduced O

- B-DAT

- B-DAT
age O
deblocking O
problem O
can O
be O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
results O
when O
extended O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
remarks O
are O
given O
in O

- B-DAT

- B-DAT
age O
denoising. O
In O
[25], O
stacked O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tween O
depth O
and O
width O
[19 O

- B-DAT

- B-DAT

- B-DAT
resolution O
[31] O
and O
color O
image O

- B-DAT

- B-DAT
scent O
(SGD) O
has O
been O
widely O

- B-DAT

- B-DAT
linearity O
inputs O
during O
training. O
Batch O

- B-DAT
rating O
a O
normalization O
step O
and O

- B-DAT

- B-DAT
tion O
for O
CNN-based O
image O
denoising O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
filters O
to O
be O
3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ping O
is O
more O
like O
an O

- B-DAT

- B-DAT
rithms O
and O
network O
architecture. O
Note O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tributed O
to O
the O
internal O
covariate O

- B-DAT
serve O
that, O
with O
batch O
normalization O

- B-DAT

- B-DAT
malization O
offers O
some O
merits O
for O

- B-DAT
ating O
internal O
covariate O
shift O
problem O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eter, O
fk O
∗ O
x O
stands O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stage O
TNRD O
from O
three O
aspects O

- B-DAT

- B-DAT

- B-DAT
sian O
distributed O
(or O
the O
noise O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing, O
we O
set O
the O
range O

- B-DAT

- B-DAT
B. O
We O
use O
color O
version O

- B-DAT
noising O
tasks, O
as O
in O
[35 O

- B-DAT
resolution O
image O
with O
downscaling O
factors O

- B-DAT

- B-DAT

-3 B-DAT

-3, B-DAT
we O
adopt O
different O
test O
set O

- B-DAT

- B-DAT

- B-DAT

-3 B-DAT

- B-DAT

- B-DAT
ments O
are O
carried O
out O
in O

- B-DAT

- B-DAT

- B-DAT

15 O
31.07 O
31.37 O
31.21 O
- B-DAT
31.24 O
31.42 O
31.73 O
31.61 O
σ O

50 O
25.62 O
25.87 O
25.67 O
26.03 O
- B-DAT
25.97 O
26.23 O
26.23 O

- B-DAT

- B-DAT
B/CDnCNN-B O
and O
DnCNN-3 O
on O
GPU O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
training O
based O
methods O
(i.e O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tures O
meet O
well O
with O
the O

- B-DAT

-5 B-DAT
illustrate O
the O
visual O
results O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
7. O
One O
can O
see O
that O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
v5 O
deep O
learning O
library O
to O

-3 B-DAT
model O
is O
trained O
for O
three O

-3 B-DAT
with O
the O
specific O
state-of-the-art O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

203.1 O
25.4 O
1.42 O
2.11 O
/ O
- B-DAT
0.45 O
/ O
0.010 O
0.74 O

- B-DAT

- B-DAT

- B-DAT

-3 B-DAT
is O
compared O
with O
two O
state-of-the-art O

- B-DAT

- B-DAT

-3 B-DAT
model O
for O
the O
three O
different O

-3 B-DAT
outperforms O
AR-CNN O
by O
about O
0.3dB O

-3 B-DAT
and O
VDSR O
can O
produce O
sharp O

- B-DAT
ods. O
As O
one O
can O
see O

-3 B-DAT
can O
recover O
the O
straight O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

0.8861 O
40 O
33.34 O
/ O
0.8953 O
- B-DAT
33.77 O
/ O
0.9003 O

0.9090 O
40 O
33.63 O
/ O
0.9198 O
- B-DAT
33.96 O
/ O
0.9247 O

- B-DAT

- B-DAT
3 O
can O
produce O
visually O
pleasant O

- B-DAT
mance. O
Unlike O
traditional O
discriminative O
models O

- B-DAT

- B-DAT
scaling O
factors, O
and O
JPEG O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
larization O
method O
for O
total O
variation-based O

- B-DAT
nition, O
2007, O
pp. O
1–8 O

- B-DAT
agation O
with O
learned O
higher-order O
Markov O

- B-DAT
tion,” O
in O
IEEE O
Conference O
on O

- B-DAT
inative O
non-blind O
deblurring,” O
in O
IEEE O

- B-DAT

- B-DAT
mation O
Processing O
Systems, O
2012, O
pp O

- B-DAT

-3 B-DAT
/ O
30.02dB O

- B-DAT

- B-DAT

-3 B-DAT
/ O
32.73dB O

- B-DAT

- B-DAT

-3 B-DAT
/ O
29.70dB O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

6: O
Super-resolution O
results O
of O
“ppt3” O
(Set14) B-DAT
with O
scale O
factor O
×3. O
Texts O

Set14 B-DAT
×2 O
30.24/0.8688 O
32.28/0.9056 O
32.42/0.9063 O
32.26/0.9040 O

and O
×4 O
on O
datasets O
Set5, O
Set14, B-DAT
B100 O
and O
Urban100. O
Red O
color O

datasets. O
Datasets O
Set5 O
[19] O
and O
Set14 B-DAT
[32] O
are O
often O
used O
for O

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
a O
deeply-recursive O
convolutional O
network O

- B-DAT
sions). O
Increasing O
recursion O
depth O
can O

- B-DAT
mance O
without O
introducing O
new O
parameters O

- B-DAT
ploding/vanishing O
gradients. O
To O
ease O
the O

- B-DAT
ing, O
we O
propose O
two O
extensions O

- B-DAT

- B-DAT

- B-DAT
ods O
by O
a O
large O
margin O

- B-DAT

- B-DAT
frequency O
components. O
For O
example, O
if O

- B-DAT
tern O
with O
smoothed O
edges O
contained O

- B-DAT
priately O
sharpened. O
As O
SR O
is O

- B-DAT

- B-DAT
ious O
computer O
vision O
tasks O
often O

- B-DAT
tive O
fields O
(224x224 O
common O
in O

- B-DAT
volutional O
(conv.) O
layer O
with O
filter O

- B-DAT
mediate O
representation O
can O
be O
used O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
network O
(DRCN). O
DRCN O
repeatedly O

- B-DAT
sions O
are O
performed. O
Our O
network O

- B-DAT

- B-DAT
dient O
descent O
method O
does O
not O

- B-DAT

- B-DAT

- B-DAT
sions. O
As O
each O
recursion O
leads O

- B-DAT

- B-DAT

- B-DAT
put) O
and O
a O
high-resolution O
image O

- B-DAT
ever, O
is O
likely O
to O
be O

- B-DAT
construction. O
This O
is O
particularly O
effective O

- B-DAT
resolution O
method O
deeply O
recursive O
in O

- B-DAT
sive O
network O
in O
two O
ways O

- B-DAT

- B-DAT
connection. O
Our O
method O
demonstrates O
state-of-the-art O

- B-DAT
formance O
in O
common O
benchmarks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ping O
from O
LR O
to O
HR O

- B-DAT
tention O
to O
find O
better O
regression O

- B-DAT
tional O
neural O
network O
(CNN) O
[5 O

- B-DAT

- B-DAT
lutional O
neural O
network O
(SRCNN) O
[5 O

- B-DAT
sibility O
of O
an O
end-to-end O
approach O

- B-DAT
creases O
the O
number O
of O
parameters O

- B-DAT
lutional O
network O
that O
models O
long-range O

- B-DAT
quential O
data, O
have O
seen O
limited O

- B-DAT
lutional O
network O
in O
a O
separate O

- B-DAT

- B-DAT

- B-DAT
fitting. O
To O
overcome O
overfitting, O
Liang O

- B-DAT

- B-DAT
folded O
layers. O
They O
show O
that O

- B-DAT
tectures O

- B-DAT
mance O
for O
super-resolution. O
We O
apply O

-1 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ence O
net O
solves O
the O
task O

- B-DAT
ture O
maps O
in O
the O
inference O

- B-DAT
ate O
representation O
used O
to O
pass O

- B-DAT
resent O
its O
feature O
maps O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cursion O
applies O
the O
same O
convolution O

- B-DAT
sive O
layer O
represent O
the O
high-resolution O

- B-DAT

- B-DAT

- B-DAT
tion O
net O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formative O
than O
the O
raw O
intensities O

- B-DAT

- B-DAT
terpolated O
input O
image O
(to O
the O

- B-DAT
net O
functions: O
embedding, O
inference O
and O

- B-DAT
spectively. O
Our O
model O
is O
the O

- B-DAT
putes O
the O
matrix O
output O
H0 O

- B-DAT
ence O
net O
f2. O
Hidden O
layer O

- B-DAT
position O
of O
the O
same O
elementary O

- B-DAT

- B-DAT

- B-DAT
bedding O
net. O
The O
formula O
is O

- B-DAT

- B-DAT
dance O
with O
the O
limited O
success O

- B-DAT
havior. O
Long O
term O
components O
approach O

- B-DAT
ing O
an O
exact O
copy O
of O

- B-DAT
cursive O
layer O
needs O
to O
keep O

- B-DAT

- B-DAT

- B-DAT
timal O
recursion O
issues, O
we O
propose O

- B-DAT
construction O
net O
now O
outputs O
D O

- B-DAT
tions O
are O
simultaneously O
supervised O
during O

- B-DAT
ing. O
The O
optimal O
weights O
are O

- B-DAT
diate O
layers O
for O
a O
convolutional O

- B-DAT
tion O
error O
while O
improving O
the O

- B-DAT
nificant O
differences O
between O
our O
recursive-supervision O

- B-DAT

- B-DAT
ciate O
a O
unique O
classifier O
for O

- B-DAT
ditional O
layer, O
a O
new O
classifier O

- B-DAT
ference O
is O
that O
Lee O
et O

- B-DAT
sifiers O
during O
testing. O
However, O
an O

- B-DAT
diate O
predictions O
significantly O
boosts O
the O

- B-DAT

- B-DAT
ing/exploding O
gradients O
along O
one O
backpropagation O

- B-DAT

- B-DAT
sion: O
skip-connection. O
For O
SR, O
input O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eral O
super-resolution O
methods O
[28, O
29 O

- B-DAT

- B-DAT
tion O
under O
recursive-supervision O
(Figure O
3(a O

- B-DAT

- B-DAT
connection O
can O
take O
various O
functional O

- B-DAT
ple, O
input O
can O
be O
concatenated O

- B-DAT

- B-DAT
ing O
set O
is O
minimized. O
This O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gresses, O
α O
decays O
to O
boost O

- B-DAT
put O

- B-DAT
jective O
using O
mini-batch O
gradient O
descent O

- B-DAT
propagation O
(LeCun O
et O
al. O
[15 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mark O
[29, O
28, O
5]. O
Dataset O

- B-DAT
ages O
failed O
by O
existing O
methods O

- B-DAT
folded, O
the O
longest O
chain O
from O

- B-DAT
mentum O
parameter O
to O
0.9 O
and O

- B-DAT
ing O
images O
are O
split O
into O

- B-DAT

- B-DAT

- B-DAT
lutions, O
we O
set O
all O
weights O

- B-DAT

- B-DAT
rameters O
except O
the O
weights O
used O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cause O
human O
vision O
is O
much O

- B-DAT
tensity O
than O
in O
color O

- B-DAT

- B-DAT
parison, O
however, O
we O
also O
crop O

- B-DAT
isting O
methods O
use O
slightly O
different O

- B-DAT
uation O
on O
several O
datasets. O
Our O

- B-DAT
isting O
methods O
in O
all O
datasets O

- B-DAT
tive O
to O
patterns. O
In O
contrast O

- B-DAT

- B-DAT

- B-DAT
ploiting O
a O
large O
image O
context O

- B-DAT

- B-DAT
connection. O
We O
have O
demonstrated O
that O

- B-DAT
forms O
existing O
methods O
by O
a O

- B-DAT
der O
to O
use O
image-level O
context O

- B-DAT

- B-DAT
works, O
IEEE O
Transactions O
on, O
5(2 O

- B-DAT

- B-DAT
projection O
residuals. O
In O
International O
Conference O

- B-DAT

- B-DAT
resolution O
using O
deep O
convolutional O
networks O

- B-DAT
ing O
low-level O
vision. O
IJCV, O
2000 O

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
transformed O
self-exemplars. O
In O

- B-DAT
istration. O
CVGIP: O
Graphical O
models O
and O

- B-DAT

- B-DAT

- B-DAT
ize O
recurrent O
networks O
of O
rectified O

- B-DAT
based O
learning O
applied O
to O
document O

- B-DAT
ings O
of O
the O
IEEE, O
86(11 O

- B-DAT
supervised O
nets. O
arXiv O
preprint O
arXiv:1409.5185 O

- B-DAT
tional O
networks O
for O
semantic O
segmentation O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cal O
statistics. O
In O
ICCV, O
2001 O

- B-DAT
ternational O
Conference O
on O
Machine O
Learning O

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT

- B-DAT

- B-DAT
cation. O
In O
NIPS, O
2012. O
2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
via O
sparse O
representation. O
TIP O

- B-DAT
up O
using O
sparse-representations. O
In O
Curves O

Set14 B-DAT
×2 O
32.42 O
/ O
0.9063 O
33.03 O

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

- B-DAT

- B-DAT

- B-DAT
Resolution O
(SR) O
performance O
in O
the O

- B-DAT
ever, O
being O
supervised, O
these O
SR O

- B-DAT
resolution O
(LR) O
images O
from O
their O

- B-DAT

- B-DAT

- B-DAT
ever, O
rarely O
obey O
these O
restrictions O

- B-DAT
sults O
by O
SotA O
(State O
of O

- B-DAT
troduce O
“Zero-Shot” O
SR, O
which O
exploits O

- B-DAT
age, O
and O
train O
a O
small O

- B-DAT

- B-DAT
ological O
data, O
and O
other O
images O

- B-DAT
cess O
is O
unknown O
or O
non-ideal O

- B-DAT

- B-DAT
ous O
unsupervised O
SR O
methods. O
To O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
(supervised O
[21] O
or O
unsupervised O

- B-DAT
riods O
of O
time O
(days O
or O

- B-DAT
mance O
deteriorates O
significantly O
once O
these O

- B-DAT
quality O
natural O
images, O
from O
which O

- B-DAT

- B-DAT
scaling O
kernel O
(usually O
a O
Bicubic O

- B-DAT
tracting O
artifacts O
(sensor O
noise, O
non-ideal O

- B-DAT
pression, O
etc.), O
and O
for O
a O

- B-DAT

- B-DAT
ally O
×2, O
×3 O
or O
×4 O

- B-DAT
ideal O
(non-bicubic) O
downscaling O
kernel, O
or O

- B-DAT
facts. O
Fig. O
1 O
further O
shows O

- B-DAT

- B-DAT

- B-DAT
ing O
on O
any O
prior O
image O

- B-DAT
ploit O
the O
internal O
recurrence O
of O

- B-DAT

- B-DAT

- B-DAT
form O
SR O
on O
real O
images O

- B-DAT
known O
and O
non-ideal O
(see O
example O

- B-DAT

- B-DAT
trained O
SotA O
SR O
methods O
by O

- B-DAT
nally O
supplied O
examples O
(even O
if O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ages O
[4, O
23]. O
This O
formed O

- B-DAT
pervised O
image O
enhancement O
methods, O
including O

- B-DAT
vised O
SR O
[4, O
5, O
6 O

- B-DAT

- B-DAT
ing O
kernel O
is O
unknown), O
Blind-Deblurring O

- B-DAT
Dehazing O
[2], O
and O
more. O
While O

- B-DAT

- B-DAT
ject O
to O
the O
above-mentioned O
supervised O

- B-DAT
age O
patches, O
of O
predefined O
size O

- B-DAT
nearest-neighbours O
search. O
As O
such, O
they O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
without O
being O
restricted O
by O

- B-DAT

- B-DAT
itations O
of O
patch-based O
methods. O
We O

- B-DAT

- B-DAT

- B-DAT
age O
and O
its O
downscaled O
versions O

- B-DAT

- B-DAT
duce O
the O
HR O
output. O
This O

- B-DAT
based O
SR O
by O
a O
large O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
is O
available O
and O
provided O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
est O
amounts O
of O
computational O
resources O

- B-DAT

- B-DAT
ideal’ O
conditions, O
and O
competitive O
results O

- B-DAT
tions O
for O
which O
SotA O
supervised O

- B-DAT
ages O
have O
strong O
internal O
data O

- B-DAT
peat O
many O
times O
inside O
a O

- B-DAT
vation O
was O
empirically O
verified O
by O

- B-DAT

- B-DAT
conies, O
since O
evidence O
to O
their O

- B-DAT
specific O
information O
when O
relying O
on O

- B-DAT
ages. O
While O
the O
strong O
internal O

- B-DAT

- B-DAT
fied O
here O
using O
a O
‘fractal-like O

- B-DAT
power O
was O
analyzed O
and O
shown O

- B-DAT
ternal O
entropy O
of O
patches O
inside O

- B-DAT

- B-DAT
ther O
shown O
to O
be O
particularly O

- B-DAT
tainty O
and O
image O
degradations O
(see O

- B-DAT

- B-DAT
formation. O
Simple O
unsupervised O
internal-SR O
[4 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
struct O
the O
desired O
HR O
output O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put O
training O
instances. O
The O
resulting O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
HR O
training O
example O
pairs. O
This O

- B-DAT
ternal O
collection O
of O
LR-HR O
image O

- B-DAT

- B-DAT
tremely O
deep O
and O
very O
complex O

- B-DAT

- B-DAT
pler O
image-specific O
network O

- B-DAT
vations O
on O
each O
layer. O
The O

- B-DAT

- B-DAT
ods O
[9, O
8, O
3], O
we O

- B-DAT
dependent O
of O
the O
size O
of O

- B-DAT

- B-DAT

- B-DAT
less O
the O
sampled O
image-pair O
is O

- B-DAT

- B-DAT

- B-DAT
father. O
The O
closer O
the O
size-ratio O

- B-DAT

- B-DAT
pled. O
This O
reflects O
the O
higher O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
runtime O
per O
image O
is O

- B-DAT

-80 B-DAT
GPU). O
This O
runtime O
is O
inde O

- B-DAT
pendent O
of O
the O
image O
size O

- B-DAT

- B-DAT
tained O
when O
using O
a O
gradual O

- B-DAT
ample, O
a O
gradual O
increase O
using O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
kernel, O
high-quality O
imaging O
conditions O

- B-DAT
vised O
SR O
methods O
achieve O
an O

- B-DAT
ferent O
lens O
types O
and O
PSFs O

- B-DAT
ing O
conditions O
(e.g., O
subtle O
involuntary O

- B-DAT
sults O
in O
different O
downscaling O
kernels O

- B-DAT
acteristics, O
various O
compression O
artifacts, O
etc O

- B-DAT
figurations/settings. O
Moreover, O
a O
single O
supervised O

- B-DAT
tions/settings. O
To O
obtain O
good O
performance O

- B-DAT

- B-DAT
dations/settings O
of O
the O
test O
image O

- B-DAT
vided, O
the O
bicubic O
kernel O
serves O

- B-DAT

- B-DAT
off O
between O
speed O
and O
quality O

- B-DAT

- B-DAT

- B-DAT
sion O
artifacts, O
etc.) O
We O
found O

- B-DAT
deviation O
of∼5 O
grayscales), O
improves O
the O

- B-DAT

- B-DAT
scale O
information O
(the O
noise), O
while O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ideal O
kernels O
(see O
examples O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Results O
Our O
method O
(ZSSR O
- B-DAT
‘Zero-Shot O
SR’) O
is O
primarily O
aimed O

- B-DAT

- B-DAT

- B-DAT
petitive O
results O
against O
externally-supervised O
methods O

- B-DAT

- B-DAT
ing O
method O
SelfExSR O
[6] O
by O

- B-DAT
ated O
using O
the O
‘ideal’ O
supervised O

- B-DAT
ple O
is O
shown O
in O
Fig O

- B-DAT
ical O
natural O
image, O
further O
analysis O

- B-DAT
ence O
for O
internal O
learning O
(via O

- B-DAT

- B-DAT
nally O
learned O
data O
recurrence O
(ZSSR O

- B-DAT
vantageous O
in O
image O
area O
with O

- B-DAT
res) O
examples O
of O
themselves O
elsewhere O

- B-DAT
age O
(at O
a O
different O
location/scale O

- B-DAT
Learning O
with O
External-Learning O
in O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion. O
Each O
LR O
image O
from O

- B-DAT
graded O
using O
one O
of O
3 O

- B-DAT
plied O
to O
those O
images, O
without O

- B-DAT
polation O
outperforms O
current O
SotA O
SR O

- B-DAT
sian O
kernels. O
For O
each O
image O

- B-DAT

- B-DAT
nel. O
Table O
2 O
compares O
our O

- B-DAT
ing O
externally-supervised O
SR O
methods O
[12 O

- B-DAT

- B-DAT
scaling O
kernel. O
For O
this O
mode O

- B-DAT
parametric O
downscaling O
kernel O
which O
maximizes O

- B-DAT
larity O
of O
patches O
across O
scales O

- B-DAT
ate O
the O
LR O
image. O
Such O

- B-DAT

- B-DAT
age O
(whether O
estimated O
or O
real O

- B-DAT
els O
that O
favor O
Internal-SR O
(i.e O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
selves O
elsewhere O
inside O
the O
same O

- B-DAT
tion/scale O

- B-DAT

- B-DAT
wscaling O
model O
is O
more O
important O

- B-DAT

- B-DAT

- B-DAT
dom O
type O
of O
degradation O
out O

- B-DAT
ble O
3 O
shows O
that O
ZSSR O

- B-DAT
rent O
SotA O
SR O
methods O

- B-DAT

- B-DAT
ploits O
the O
power O
of O
Deep O

- B-DAT

- B-DAT

- B-DAT
cess O
is O
non-ideal, O
unknown, O
and O

- B-DAT
age O
(i.e., O
image-specific O
settings). O
In O

- B-DAT

- B-DAT
ideal’ O
settings, O
our O
method O
substantially O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pean O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
gle O
image. O
In O
International O
Conference O

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
istration. O
CVGIP: O
Graphical O
Model O
and O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT
tion O
(CVPR) O
Workshops, O
pages O
1646–1654 O

- B-DAT

- B-DAT
tional O
network O
for O
image O
super-resolution O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
ham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cal O
statistics. O
In O
Proc. O
8th O

- B-DAT
ume O
2, O
pages O
416–423, O
July O

- B-DAT
resolution. O
In O
International O
Conference O
on O

- B-DAT
sion O
(ECCV). O
2014. O
3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
(ICML). O
3 O

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT
shops, O
July O
2017. O
6 O

- B-DAT
prove O
example-based O
single O
image O
super O

- B-DAT
tion O
(CVPR), O
June O
2016. O
5 O

- B-DAT
chored O
neighborhood O
regression O
for O
fast O

- B-DAT

- B-DAT

SR O
bench- O
marks O
(i.e. O
Set5, O
Set14, B-DAT
B100) O
and O
methods O
(i.e. O
A O

qualitative O
evaluation O
3 O
datasets O
Set5, O
Set14, B-DAT
and O
B100 O
are O
used O
as O

the O
name O
‘Set5’ O
in O
[25]. O
Set14 B-DAT
is O
a O
larger, O
more O
diverse O

ing O
results. O
The O
images O
in O
Set14 B-DAT
are O
larger O
on O
average O
than O

5. O
Average O
PSNR O
on O
Set5, O
Set14, B-DAT
and O
B100 O
and O
the O
improvement O

Set14 B-DAT
x3 O
27.54 O
28.60 O
28.67 O
28.65 O

PSNR O
performance O
on O
Set5, O
Set14, B-DAT
and O
B100, O
and O
for O
mag O

results O
for O
magnification O
×4 O
on O
Set14 B-DAT
for O
our O
IA O
method O
in O

results O
for O
×4. O
Images O
from O
Set14 B-DAT

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Trans O

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

the O
name O
‘Set5’ O
in O
[25]. O
Set14 B-DAT

ing O
results. O
The O
images O
in O
Set14 B-DAT

Set14 B-DAT

results O
for O
magnification O
×4 O
on O
Set14 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
marks O
(i.e. O
Set5, O
Set14, O
B100 O

- B-DAT
CNN, O
ANR, O
Zeyde, O
Yang) O
and O

- B-DAT
ments. O
The O
techniques O
are O
widely O

- B-DAT
of-the-art O
results O
outperforming O
A+ O
by O

- B-DAT
age O
PSNR O
whilst O
maintaining O
a O

- B-DAT

- B-DAT

- B-DAT
quencies O
details O
from O
a O
single O

- B-DAT

- B-DAT

- B-DAT
respond O
to O
the O
same O
LR O

- B-DAT
lem, O
the O
SR O
literature O
proposes O

- B-DAT

- B-DAT
ods O
[24], O
reconstruction-based O
methods O
[3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nal O
images. O
Most O
recent O
methods O

- B-DAT
based O
SR. O
We O
apply O
them O

- B-DAT
chored O
Neighborhood O
Regression O
(ANR) O
method O

- B-DAT

- B-DAT
based O
single O
image O
super-resolution O
methods O

- B-DAT
cant O
improvements O
on O
standard O
benchmarks O

- B-DAT
bine O
the O
techniques O
to O
derive O

- B-DAT
ments O
when O
starting O
from O
the O

review O
the O
anchored O
regression O
baseline O
- B-DAT
the O
A+ O
method O
[26]. O
Then O

- B-DAT
bically) O
downscaled O
to O
the O
corresponding O

- B-DAT

- B-DAT

- B-DAT
tion O
we O
first O
describe O
the O

- B-DAT
ods O
we O
use O
or O
compare O

- B-DAT
lar O
images: O
one O
medium O
size O

- B-DAT
ing O
results. O
The O
images O
in O

- B-DAT
ety O
of O
real-life O
scenes O
and O

- B-DAT

- B-DAT
similarity O
(S) O
experiments O
on O
the O

- B-DAT
resentation O
of O
the O
LR-HR O
priors/training O

- B-DAT

- B-DAT
fte O
et O
al. O
[25] O
relaxes O

- B-DAT
tion O
of O
patches O
from O
Yang O

- B-DAT

- B-DAT
SVD O
[1]) O
as O
the O
ANR O

- B-DAT
tionary O
atoms, O
called O
anchors. O
For O

- B-DAT
proves O
with O
the O
number O
of O

- B-DAT
trix O
multiplication O
(application O
of O
the O

- B-DAT

- B-DAT

- B-DAT
els O
for O
LR O
and O
9 O

- B-DAT
mance O
from O
32.39dB O
with O
0.5 O

- B-DAT
sions O
of O
the O
training O
images/patches O

- B-DAT
inal O
images O
by O
90◦, O
180 O

- B-DAT

- B-DAT

- B-DAT
gressors O
varies O
from O
31.83dB O
when O

- B-DAT
ples O
to O
32.39dB O
for O
0.5 O

- B-DAT
ing O
samples O
but O
on O
the O

- B-DAT
ples/anchoring O
points) O
is O
increased, O
the O

- B-DAT
chored O
methods O
(such O
as O
ANR O

- B-DAT
tize O
the O
LR O
feature O
space O

- B-DAT
sors, O
while O
it O
reaches O
32.92dB O

- B-DAT
chor O
is O
applied O
to O
reconstruct O

- B-DAT
tures O
are O
high O
dimensional O
(30 O

- B-DAT
tures O
such O
as O
kd-trees, O
forests O

-4 B-DAT
times O
in O
[20, O
22 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
struction O
consistent O
with O
the O
LR O

- B-DAT
sampling. O
Knowing O
the O
degradation O
operators O

- B-DAT
timated O
[18]. O
Assuming O
the O
degradation O

- B-DAT
pending O
on O
the O
settings O
as O

- B-DAT

- B-DAT
ence O
A+ O
is O
1.18dB O
better O

- B-DAT
ter O
than O
the O
baseline O
Yang O

- B-DAT
tion O
operators O
are O
unknown O
and O

- B-DAT
cise, O
therefore O
our O
reported O
results O

- B-DAT
resolution O
becomes O
more O
accurate, O
since O

- B-DAT
ble O
HR O
solutions O
for O
each O

- B-DAT
ally O
refine O
the O
contents O
up O

- B-DAT
plexity O
depends O
on O
the O
number O

- B-DAT
resolving O
the O
LR O
image O
in O

- B-DAT
get O
the O
HR O
image O
for O

- B-DAT
tal O
approach O
has O
a O
loose O

- B-DAT

- B-DAT

- B-DAT
put O
image O
is O
enhanced O
by O

- B-DAT
sults O
at O
pixel O
level. O
Therefore O

- B-DAT
tion O
(E) O
gives O
a O
0.05dB O

- B-DAT
cade. O
The O
running O
time O
is O

- B-DAT
mations. O
In O
Table O
3 O
we O

- B-DAT

- B-DAT

- B-DAT
ies O
built O
from O
the O
input O

- B-DAT
text. O
Exponents O
are O
Glasner O
et O

- B-DAT
els O
adapted O
to O
each O
new O

- B-DAT
naries O
proved O
better O
in O
terms O

- B-DAT
ies O
can O
be O
better O
than O

- B-DAT
ternal O
and O
internal O
dictionaries O

- B-DAT

- B-DAT
main O
specific O
models O
and O
Sun O

- B-DAT
ber O
that O
does O
not O
increase O

- B-DAT
text O
we O
compute O
a O
regressor O

- B-DAT
tion. O
For O
patches O
of O
comparable O

- B-DAT
chors O
and O
then O
the O
regressor O

- B-DAT
prove O
from O
32.39dB O
to O
32.55dB O

- B-DAT
provements O
achieved O
using O
reasoning O
with O

- B-DAT

- B-DAT
resolution O
method. O
If O
we O
start O

- B-DAT
archical O
search O
structure, O
we O
achieve O

- B-DAT
tion O
time. O
The O
full O
setup O

- B-DAT
cations O
×2, O
×3, O
×4. O
Figs O

- B-DAT
ent O
techniques O
is O
additive, O
each O

- B-DAT
formance. O
These O
techniques O
are O
general O

- B-DAT

- B-DAT

- B-DAT
formance O
scale O
the O
results O
(Set5,×3 O

- B-DAT
ods O
A+, O
ANR, O
Zeyde, O
and O

- B-DAT
bines O
A+ O
with O
A O
and O

- B-DAT
nification O
factors O
×2, O
×3, O
and O

- B-DAT
parison O
with O
the O
baseline O
A O

- B-DAT
ding O
with O
Locally O
Linear O
Embedding O

- B-DAT
porting O
improved O
results O
also O
for O

- B-DAT
dation O
operators O
usually O
are O
not O

- B-DAT
mate O
in O
practice. O
A+B O
just O

- B-DAT
ample, O
the O
clarity O
and O
sharpness O

- B-DAT
formance O
of O
example-based O
super-resolution. O
Combined O

- B-DAT
invasive O
techniques O
such O
as O
augmentation O

- B-DAT
chors O
in O
the O
IA O
method O

- B-DAT
nitude O
more O
regressors O
than O
the O

- B-DAT
ning O
time. O
Another O
technique, O
often O

- B-DAT
caded O
application O
of O
the O
core O

- B-DAT

- B-DAT
wards O
HR O
restoration. O
Using O
the O

- B-DAT

- B-DAT

- B-DAT
the-art O
methods O
such O
as O
A O

- B-DAT

- B-DAT
resolution O
methods. O
The O
proposed O
techniques O

- B-DAT

- B-DAT
beri O
Morel. O
Low-complexity O
single-image O
super-resolution O

- B-DAT

- B-DAT
lutional O
nets. O
In O
BMVC, O
2014 O

- B-DAT
gressors O
for O
image O
super-resolution. O
Computer O

- B-DAT
rum, O
34(2):95–104, O
2015. O
1 O

- B-DAT

- B-DAT
resolution O
using O
deep O
convolutional O
networks O

- B-DAT
actions O
on O
Pattern O
Analysis O
and O

- B-DAT
tralized O
sparse O
representation O
for O
image O

- B-DAT

- B-DAT

- B-DAT
based O
super-resolution. O
IEEE O
Computer O
Graphics O

- B-DAT
plications, O
22(2):56–65, O
2002. O
1 O

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
June O

- B-DAT
tration. O
CVGIP, O
53(3):231–239, O
1991. O
4 O

- B-DAT

- B-DAT

- B-DAT
based O
learning O
applied O
to O
document O

- B-DAT
ings O
of O
the O
IEEE, O
1998 O

- B-DAT
cal O
statistics. O
In O
ICCV, O
2001 O

- B-DAT
resolution. O
In O
ICCV, O
2013. O
4 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Hidalgo, O
and O
B. O
Rosenhahn. O
Fast O

- B-DAT

- B-DAT
alizing O
the O
nonlocal-means O
to O
super-resolution O

- B-DAT
tion. O
Image O
Processing, O
IEEE O
Transactions O

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT

- B-DAT
lucination O
for O
image O
super-resolution. O
In O

- B-DAT
cal O
Imaging, O
Processing O
and O
Analysis O

- B-DAT
demic O
Press, O
2000. O
1 O

- B-DAT
borhood O
regression O
for O
fast O
example-based O

- B-DAT

- B-DAT
ors O
for O
post-processing O
demosaiced O
images O

- B-DAT
similarities O
for O
single O
frame O
super-resolution O

- B-DAT
ume O
6494, O
pages O
497–510, O
2011 O

- B-DAT

- B-DAT

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT
cess., O
19(11):2861–2873, O
2010. O
1 O

- B-DAT
resolution O
as O
sparse O
representation O
of O

- B-DAT

- B-DAT

- B-DAT
ing O
multiple O
linear O
mappings O
for O

- B-DAT

- B-DAT
resolution O
using O
deformable O
patches. O
In O

i.e. O
Set5 O
[15] O
(5 O
images), O
Set14 B-DAT
[16] O
(14 O
images) O
and O
BSD200 O

SSIM O
on O
the O
Set5 O
[15], O
Set14 B-DAT
[16] O
and O
BSD200 O
[17] O
dataset O

Set14 B-DAT
×2 O
30.23/0.8687 O
32.28/0.9056 O
32.51/0.9074 O
32.18/0.9039 O

- B-DAT

- B-DAT

- B-DAT
age O
tend O
to O
redundantly O
recur O

- B-DAT

- B-DAT
posed O
deep O
learning O
based O
restoration O

- B-DAT

- B-DAT
pose O
a O
dilated O
convolution O
based O

- B-DAT

- B-DAT

- B-DAT
nates O
all O
these O
features O
to O

- B-DAT

- B-DAT

- B-DAT
ception O
module, O
the O
proposed O
end-to-end O

- B-DAT
resolution O
network O
can O
take O
advantage O

- B-DAT

- B-DAT
tion O
to O
improve O
image O
super-resolution O

- B-DAT
imental O
results O
show O
that O
our O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
phasis O
because O
of O
the O
requirement O

- B-DAT
nition O
(UHD) O
TVs. O
However, O
most O

- B-DAT
tent O
from O
lower O
resolutions O
[2 O

- B-DAT

- B-DAT

- B-DAT
tion O
module. O
Our O
proposed O
new O

- B-DAT

- B-DAT
pled O
dictionaries O
to O
learn O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
both O
high O
level O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tablish O
the O
relationship O
between O
the O

- B-DAT

- B-DAT
strate O
that O
a O
convolutional O
neural O

- B-DAT

- B-DAT

- B-DAT
resolution O
image O
to O
the O
high-resolution O

- B-DAT
quency O
details O
instead O
of O
the O

- B-DAT

- B-DAT
tion O
is O
also O
important. O
The O

- B-DAT

- B-DAT
tion O
result O
in O
traditional O
methods O

- B-DAT
scale O
information O
has O
been O
little O

- B-DAT

- B-DAT
ploit O
the O
scale O
information O
since O

- B-DAT

- B-DAT
ception O
module O
to O
learn O
multi-scale O

- B-DAT
ison O
between O
the O
proposed O
dilated O

- B-DAT
tion O
module O
and O
the O
original O

- B-DAT
ent O
scale O
dilated O
convolution O
that O

- B-DAT
scale O
image O
information. O
Furthermore, O
we O

- B-DAT

- B-DAT
performs O
many O
state-of-the-art O
methods O

- B-DAT
ter O
to O
clarify O
that O
no O

- B-DAT

- B-DAT
volution, O
F O
and O
k O
is O

- B-DAT

- B-DAT
tively. O
It O
shows O
that O
filters O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
the-art O
performance O
for O
classification O
and O

- B-DAT

- B-DAT
pose O
a O
dilated O
convolution O
based O

- B-DAT

- B-DAT
formance O

- B-DAT
ers. O
Then, O
we O
concatenate O
all O

- B-DAT

- B-DAT

- B-DAT
terpolation O
result. O
Therefore, O
it O
can O

- B-DAT

- B-DAT
erate O
on O
the O
same O
scale O

- B-DAT
responding O
scale O
image O
information. O
Furthermore O

- B-DAT
put, O
so O
we O
can O
fuse O

- B-DAT

- B-DAT
struction. O
Since O
residual O
learning O
have O

- B-DAT
age O
high O
frequency O
details O
instead O

- B-DAT

- B-DAT
sult O
as O
a O
”low-resolution” O
image O

- B-DAT
ception O
module O
combines O
different O
scale O

- B-DAT
scale O
information, O
in O
the O
next O

- B-DAT
tion O
layer O
after O
the O
inception O

- B-DAT
ules O
following O
with O
a O
common O

- B-DAT
struction O
phase, O
we O
use O
a O

- B-DAT

- B-DAT

- B-DAT
fied O
Linear O
Unit O
(PReLU) O
as O

- B-DAT

- B-DAT
tion O
layer O
in O
our O
very O

- B-DAT
tion O
(Adam) O
[14] O
to O
optimize O

- B-DAT
provement O
in O
performance O
can O
be O

- B-DAT
Resolution O
Network O
(MSSRNet O

-100 B-DAT
dataset, O
which O
contains O
100 O
bmp-format O

- B-DAT
ules O
to O
make O
the O
network O

- B-DAT
tialized O
using O
the O
initializer O
proposed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nential O
decay O
rates O
for O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lar O
deep O
learning O
based O
single O

- B-DAT

- B-DAT
ods O
are O
the O
same O
as O

- B-DAT
performs O
all O
previous O
methods O
in O

- B-DAT
sample O
factor O
2, O
3 O
and O

- B-DAT

- B-DAT
ter O
performance, O
we O
can O
increase O

- B-DAT
tive O
and O
qualitative O
results O

- B-DAT

- B-DAT

- B-DAT
resolution. O
Experimental O
results O
show O
that O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sic O
Research O
Development O
Program O
of O

- B-DAT
manet, O
Scott O
Reed, O
Dragomir O
Anguelov O

- B-DAT
tion, O
2015, O
pp. O
1–9 O

- B-DAT
los O
K O
Katsaggelos, O
“Video O
super-resolution O

- B-DAT
lutional O
neural O
networks,” O
IEEE O
Transactions O

- B-DAT
putational O
Imaging, O
vol. O
2, O
no O

- B-DAT

- B-DAT
tion,” O
IEEE O
transactions O
on O
image O

- B-DAT
resolution O
from O
a O
single O
image O

- B-DAT
aoou O
Tang, O
“Learning O
a O
deep O

- B-DAT

- B-DAT
aoou O
Tang, O
“Image O
super-resolution O
using O

- B-DAT
lutional O
networks,” O
IEEE O
transactions O
on O

- B-DAT
ysis O
and O
machine O
intelligence, O
vol O

- B-DAT
aoou O
Tang, O
“Compression O
artifacts O
reduction O

- B-DAT
ternational O
Conference O
on O
Computer O
Vision O

- B-DAT
celerating O
the O
super-resolution O
convolutional O
neural O

- B-DAT
work,” O
in O
European O
Conference O
on O

- B-DAT
curate O
image O
super-resolution O
using O
very O

- B-DAT
lutional O
networks,” O
in O
Proceedings O
of O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT
rithm O
for O
signal O
analysis O
with O

- B-DAT
ding O
the O
a O
trous O
and O

- B-DAT
actions O
on O
signal O
processing, O
vol O

- B-DAT

- B-DAT
ture O
learning, O
purity O
trees, O
and O

- B-DAT

- B-DAT

- B-DAT
image O
super-resolution O
based O
on O
nonnegative O

- B-DAT
sion O
Conference. O
2012, O
pp. O
135.1–135.10 O

- B-DAT

- B-DAT

- B-DAT
dra O
Malik, O
“A O
database O
of O

- B-DAT
ages O
and O
its O
application O
to O

- B-DAT
gorithms O
and O
measuring O
ecological O
statistics O

- B-DAT
puter O
Vision, O
2001. O
ICCV O
2001 O

- B-DAT
level O
performance O
on O
imagenet O
classification O

- B-DAT
ceedings O
of O
the O
IEEE O
international O

- B-DAT
puter O
vision, O
2015, O
pp. O
1026–1034 O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision. O
Springer, O
2014, O
pp O

- B-DAT
level O
performance O
on O
imagenet O
classification O

- B-DAT
ceedings O
of O
the O
IEEE O
international O

- B-DAT
puter O
vision, O
2015, O
pp. O
1026–1034 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

two O
test O
datasets O
Set5 O
and O
Set14 B-DAT

models O
on O
the O
Set5 O
and O
Set14 B-DAT
dataset O

test O
data O
sets: O
Set5 O
and O
Set14 B-DAT

Set14 B-DAT
Bicubic O
K-SVD O
[58] O
ANR O
[54 O

s) O
per O
image O
on O
the O
Set14 B-DAT
dataset O

for O
the O
“Monarch” O
image O
from O
Set14 B-DAT
with O
an O
upscaling O
factor O
×3 O

trained O
diffusion O
models O
for O
three O
upscaling B-DAT
factors×2,×3 O
and×4, O
using O
exactly O
the O

image, O
and O
a O
regular O
bicubic O
upscaling B-DAT
method O
is O
applied O
to O
the O

run O
time O
(s) O
performance O
for O
upscaling B-DAT
factors O
×2, O
×3 O
and O
×4 O

image O
from O
Set14 O
with O
an O
upscaling B-DAT
factor O
×3. O
Note O
the O
differences O

with O
repeated O
hexagons O
for O
the O
upscaling B-DAT
factor O
×3 O

and O
the O
trained O
model O
for O
upscaling B-DAT
factor O
×3 O
will O
also O
lead O

to O
the O
SISR O
problem O
of O
upscaling B-DAT
factor O
×2. O
It O
is O
generally O

the O
noise O
levels O
or O
all O
upscaling B-DAT
factors O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
Proc O

models O
on O
the O
Set5 O
and O
Set14 B-DAT

Set14 B-DAT

s) O
per O
image O
on O
the O
Set14 B-DAT

for O
the O
“Monarch” O
image O
from O
Set14 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tal O
operations O
in O
image O
processing O

- B-DAT

- B-DAT
tion O
are O
non-local O
methods O
based O

- B-DAT

- B-DAT
the-art O
techniques O
mainly O
concentrate O
on O

- B-DAT
sian O
denoising, O
BM3D O
[15] O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
posed O
CSF O
model O
offers O
high O

- B-DAT
erate O
fast O
and O
effective O
models O

- B-DAT
tural O
simplicity O
of O
these O
models O

- B-DAT
proach O
for O
various O
problems O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ström O
[43], O
which O
introduces O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tigated. O
Researchers O
also O
propose O
to O

- B-DAT

- B-DAT
order O
diffusion O
models O
[28], O
[17 O

- B-DAT

- B-DAT

- B-DAT
tics/regularization O
As O
shown O
in O
[51 O

- B-DAT
tion O
between O
anisotropic O
diffusion O
models O

- B-DAT
imation O
of O
the O
gradient O
operators O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fore. O
If O
ignoring O
the O
coupled O

- B-DAT

- B-DAT
vector O
product O
∇xu O
can O
be O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
including O
Gaussian O
image O
denoising,single O

- B-DAT
tions O
between O
the O
proposed O
model O

- B-DAT
porate O
a O
reaction O
term O
in O

- B-DAT
tion O
specific O
reaction O
terms O
ψ(u O

- B-DAT
sion O
model O
Note O
that O
the O

- B-DAT
ferentiable. O
In O
order O
to O
handle O

- B-DAT
differentiable O
data O
term, O
e.g., O
the O

- B-DAT
vestigated O
in O
Section O
6, O
we O

- B-DAT

- B-DAT
tional. O
Therefore, O
Eq. O
(6) O
can O

- B-DAT

-1 B-DAT
– O
f O

-1 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
defined O
image O
invariants. O
Therefore, O
this O

- B-DAT
forward O
network. O
We O
refer O
to O

- B-DAT

- B-DAT

- B-DAT
cessing O
task, O
and O
then O
exploit O

- B-DAT

output O
of O
the O
diffusion O
process O
- B-DAT
uT O
is O
optimized. O
We O
call O

- B-DAT
imization O
problem O
with O
respect O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ators O
Kti O
are O
defined O
as O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
with O
respect O
to O
Θt O

- B-DAT

- B-DAT
mental O
material O

- B-DAT

- B-DAT

- B-DAT
work O
of O
Timofte O
et O
al O

- B-DAT
pare O
single O
image O
super O
resolution O

- B-DAT

- B-DAT

- B-DAT

-5 B-DAT
sub-images O
of O
size O
150 O

- B-DAT

- B-DAT

-2680 B-DAT
@ O
2.80GHz O
(eight O
parallel O
threads O

our O
learned O
TNRD55×5 O
works. O
(b) O
- B-DAT
(e) O
are O
intermediate O
results O
at O

stage O
1 O
- B-DAT
4, O
and O
(f) O
is O
the O

- B-DAT

-5 B-DAT
times O
faster O
than O
our O
CPU O

- B-DAT
ing O
problem O
with O
standard O
deviation O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

far O
the O
best- B-DAT
reported O
algorithm O
- O
WNNM. O
It O
turns O
out O
that O

- B-DAT
tained O
the O
following O
results: O
(A.I O

- B-DAT

- B-DAT

- B-DAT
ing O
of O
the O
influence O
functions O

- B-DAT
tions O
of O
the O
TNRD55×5 O
model O

- B-DAT

- B-DAT

- B-DAT
ably O
it O
is O
stimulated O
by O

- B-DAT
tion O
(b), O
can O
adaptively O
switch O

- B-DAT

- B-DAT

- B-DAT
volving O
the O
learned O
influence O
functions O

- B-DAT
tions O

- B-DAT

- B-DAT
sion O
models. O
(a) O
is O
generated O

- B-DAT
porate O
a O
reaction O
term O
in O

- B-DAT

- B-DAT
work O
is O
attributed O
to O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
order O
derivative O
filters, O
as O
well O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fitting. O
However, O
our O
current O
CPU O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
denoising O
algorithms. O
In O
experiments O

- B-DAT
sulting O
TNRD7×7 O
model O
achieves O
the O

TNRD57×7 O
model O
outperforms O
the O
benchmark O
- B-DAT
BM3D O
method O
by O
0.35dB O
in O

also O
surpasses O
the O
best-reported B-DAT
algorithm O
- O
WNNM O
method, O
which O
is O
quite O

- B-DAT

- B-DAT

e.g., O
start O
Matlab O
with O
- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-2680 B-DAT
v2 O
@ O
2.80GHz; O
(2) O
the O

- B-DAT

- B-DAT
putation O
of O
the O
CSF O
model O

- B-DAT

In O
contrast, O
another O
non-local B-DAT
model O
- O
WNNM O
achieves O
compelling O
denoising O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ple O
in O
Figure O
11 O
on O

- B-DAT

- B-DAT
polation O
of O
the O
LR O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

investigate O
the O
JPEG O
deblocking O
problem O
- B-DAT
suppressing O
the O
block O
artifacts O
in O

- B-DAT

- B-DAT

- B-DAT

PSNR O
Time O
baby O
2 O
37.07 O
- B-DAT
38.25 O
8.21 O
38.44 O
1.39 O
38.30 O

38.51 O
1.52 O
bird O
2 O
36.81 O
- B-DAT
39.93 O
2.67 O
40.04 O
0.44 O
40.64 O

41.29 O
0.59 O
butterfly O
2 O
27.43 O
- B-DAT
30.65 O
2.14 O
30.48 O
0.38 O
32.20 O

33.16 O
0.56 O
head O
2 O
34.86 O
- B-DAT
35.59 O
2.46 O
35.66 O
0.41 O
35.64 O

35.71 O
0.60 O
woman O
2 O
32.14 O
- B-DAT
34.49 O
2.45 O
34.55 O
0.43 O
34.94 O

35.50 O
0.57 O
average O
2 O
33.66 O
- B-DAT
35.78 O
3.59 O
35.83 O
0.61 O
36.34 O

36.83 O
0.77 O
baby O
3 O
33.91 O
- B-DAT
35.08 O
3.77 O
35.13 O
0.79 O
35.01 O

35.28 O
1.52 O
bird O
3 O
32.58 O
- B-DAT
34.57 O
1.34 O
34.60 O
0.27 O
34.91 O

36.11 O
0.59 O
butterfly O
3 O
24.04 O
- B-DAT
25.94 O
1.08 O
25.90 O
0.24 O
27.58 O

28.90 O
0.56 O
head O
3 O
32.88 O
- B-DAT
33.56 O
1.35 O
33.63 O
0.24 O
33.55 O

33.78 O
0.60 O
woman O
3 O
28.56 O
- B-DAT
30.37 O
1.14 O
30.33 O
0.24 O
30.92 O

31.77 O
0.57 O
average O
3 O
30.39 O
- B-DAT
31.90 O
1.74 O
31.92 O
0.35 O
32.39 O

33.17 O
0.77 O
baby O
4 O
31.78 O
- B-DAT
33.06 O
2.63 O
33.03 O
0.59 O
32.98 O

33.29 O
1.52 O
bird O
4 O
30.18 O
- B-DAT
31.71 O
0.70 O
31.82 O
0.18 O
31.98 O

32.98 O
0.59 O
butterfly O
4 O
22.10 O
- B-DAT
23.57 O
0.54 O
23.52 O
0.14 O
25.07 O

26.22 O
0.56 O
head O
4 O
31.59 O
- B-DAT
32.21 O
0.66 O
32.27 O
0.16 O
32.19 O

32.57 O
0.60 O
woman O
4 O
26.46 O
- B-DAT
27.89 O
0.72 O
27.80 O
0.23 O
28.21 O

29.17 O
0.57 O
average O
4 O
28.42 O
- B-DAT
29.69 O
1.05 O
29.69 O
0.26 O
30.09 O

- B-DAT

- B-DAT
mance O
evaluation. O
We O
distorted O
the O

- B-DAT

- B-DAT

Time O
PSNR O
Time O
baboon O
23.21 O
- B-DAT
23.52 O
3.54 O
23.56 O
0.77 O
23.60 O

0.75 O
23.62 O
1.30 O
barbara O
26.25 O
- B-DAT
26.76 O
6.24 O
26.69 O
1.23 O
26.66 O

1.18 O
26.25 O
1.75 O
bridge O
24.40 O
- B-DAT
25.02 O
3.98 O
25.01 O
0.80 O
25.07 O

0.81 O
25.29 O
1.19 O
coastguard O
26.55 O
- B-DAT
27.15 O
1.54 O
27.08 O
0.36 O
27.20 O

0.35 O
27.12 O
0.65 O
comic O
23.12 O
- B-DAT
23.96 O
1.37 O
24.04 O
0.27 O
24.39 O

0.34 O
24.67 O
0.65 O
face O
32.82 O
- B-DAT
33.53 O
1.10 O
33.62 O
0.24 O
33.58 O

0.29 O
33.82 O
0.57 O
flowers O
27.23 O
- B-DAT
28.43 O
2.66 O
28.49 O
0.57 O
28.97 O

0.61 O
29.55 O
0.90 O
foreman O
31.18 O
- B-DAT
33.19 O
1.54 O
33.23 O
0.30 O
33.35 O

0.36 O
34.65 O
0.65 O
lenna O
31.68 O
- B-DAT
33.00 O
3.89 O
33.08 O
0.79 O
33.39 O

0.77 O
33.77 O
1.19 O
man O
27.01 O
- B-DAT
27.90 O
3.81 O
27.92 O
0.76 O
28.18 O

0.80 O
28.52 O
1.17 O
monarch O
29.43 O
- B-DAT
31.10 O
6.13 O
31.09 O
1.13 O
32.39 O

1.12 O
33.61 O
1.66 O
pepper O
32.39 O
- B-DAT
34.07 O
3.84 O
33.82 O
0.80 O
34.35 O

0.82 O
35.06 O
1.20 O
ppt3 O
23.71 O
- B-DAT
25.23 O
4.53 O
25.03 O
1.01 O
26.02 O

0.98 O
27.08 O
1.48 O
zebra O
26.63 O
- B-DAT
28.49 O
3.36 O
28.43 O
0.69 O
28.87 O

29.40 O
1.04 O
average O
performance O
27.54 O
- B-DAT
28.67 O
3.40 O
28.65 O
0.69 O
29.00 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferent O
compression O
parameter O
q. O
We O

- B-DAT
ing, O
4 O
stages O
are O
already O

in O
terms O
of O
run O
time) O
- B-DAT
SADCT O
consumes O
about O
56.5s11. O
Furthermore O

- B-DAT
SR[9] O
SADCT[22] O
RTF[33] O
TNRD O

- B-DAT
work O
for O
effective O
image O
restoration O

- B-DAT

- B-DAT

- B-DAT
ity, O
cf. O
Fig O
5 O

- B-DAT

to O
define O
the O
ground O
truth O
- B-DAT
the O
expected O
output O
of O
the O

- B-DAT
fusion O
network O
during O
training. O
For O

- B-DAT

- B-DAT
backward O
steps O
provide O
good O
approximation O

- B-DAT
ing O
to O
consider O
learned O
nonlinear O

- B-DAT
dard O
CNs O
could O
lead O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
wise O
training O
of O
deep O
networks O

- B-DAT

- B-DAT
cies O
with O
gradient O
descent O
is O

- B-DAT

- B-DAT
sion O
via O
a O
learned O
dictionary O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
parametric O
image O
restoration O
models: O
A O

- B-DAT

- B-DAT

- B-DAT
ization O
and O
diffusion O
approach O
to O

- B-DAT

- B-DAT
tive O
nonlocal O
sparsity-based O
modeling. O
IEEE O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
plied O
physics O
from O
Nanjing O
University O

- B-DAT
nautics O
and O
Astronautics, O
China, O
a O

- B-DAT
gree O
in O
computer O
science O
from O

- B-DAT

- B-DAT
gree O
in O
Computer O
Engineering O
from O

- B-DAT
versity O
of O
Technology O
in O
2004 O

- B-DAT
tively. O
In O
2013 O
he O
received O

- B-DAT
ation O
for O
pattern O
recognition O
(DAGM O

- B-DAT
fessur O
”Mobile O
Computer O
Vision”) O
and O

run-time O
over O
the O
images O
from O
Set14 B-DAT
run O
on O
a O
single O
CPU O

two O
test O
datasets O
Set5 O
and O
Set14 B-DAT
which O
provides O
5 O
and O
14 O

Baboon”, O
”Comic” O
and O
”Monarch” O
from O
Set14 B-DAT
with O
an O
upscaling O
factor O
of O

32.39 O
32.39 O
32.55 O
32.52 O
33.00 O
Set14 B-DAT
3 O
29.00 O
28.97 O
29.08 O
29.14 O

best O
model’s O
run O
time O
on O
Set144 B-DAT
with O
an O
upscale O
factor O
of O

datasets. O
However, O
the O
use O
of O
Set14 B-DAT
on O
a O
single O
CPU O
core O

super-resolving O
one O
single O
image O
from O
Set14 B-DAT
on O
a O
K2 O
GPU. O
Utilising O

3 O
30.39 O
32.75 O
33.17 O
33.13 O
Set14 B-DAT
3 O
27.54 O
29.30 O
29.46 O
29.49 O

4 O
28.42 O
30.49 O
30.85 O
30.90 O
Set14 B-DAT
4 O
26.00 O
27.50 O
27.68 O
27.73 O

which O
learns O
an O
array O
of O
upscaling B-DAT
filters O
to O
upscale O
the O
final O

SR O
pipeline O
with O
more O
complex O
upscaling B-DAT
filters O
specifically O
trained O
for O
each O

different O
methods O
when O
performing O
SR O
upscaling B-DAT
with O
a O
scale O
factor O
of O

Learning O
upscaling B-DAT
filters O
was O
briefly O
suggested O
in O

convolution O
layer O
to O
learn O
the O
upscaling B-DAT
operation O
for O
image O
and O
video O

In O
our O
network, O
upscaling B-DAT
is O
handled O
by O
the O
last O

L O
layers, O
we O
learn O
nL−1 O
upscaling B-DAT
filters O
for O
the O
nL−1 O
feature O

maps O
as O
opposed O
to O
one O
upscaling B-DAT
filter O
for O
the O
input O
image O

to O
a O
single O
fixed O
filter O
upscaling B-DAT
at O
the O
first O
layer. O
This O

refer O
to O
r O
as O
the O
upscaling B-DAT
ratio. O
In O
general, O
both O
ILR O

in O
Fig. O
1, O
to O
avoid O
upscaling B-DAT
ILR O

feature O
maps O
directly O
with O
one O
upscaling B-DAT

implementations O
using O
various O
forms O
of O
upscaling B-DAT
before O
convolution O

luminance O
changes O
[31]. O
For O
each O
upscaling B-DAT
factor, O
we O
train O
a O
specific O

trained O
on O
ImageNet O
with O
an O
upscaling B-DAT
factor O
of O
3: O
(a) O
shows O

Monarch” O
from O
Set14 O
with O
an O
upscaling B-DAT
factor O
of O
3. O
PSNR O
values O

IHR, O
where O
r O
is O
the O
upscaling B-DAT
factor. O
To O
synthesize O
the O
low-resolution O

and O
sub-sample O
it O
by O
the O
upscaling B-DAT
factor. O
The O
sub-images O
are O
extracted O

images O
from O
ImageNet O
[30] O
for O
upscaling B-DAT
factor O
of O
3. O
We O
use O

384022” O
from O
BSD500 O
with O
an O
upscaling B-DAT
factor O
of O
3. O
PSNR O
values O

worse O
results O
than O
an O
adaptive O
upscaling B-DAT
for O
SISR O
and O
requires O
more O

bench O
mark O
data O
set O
with O
upscaling B-DAT
factor O
of O
4 O
shows O
that O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
IEEE O

run-time O
over O
the O
images O
from O
Set14 B-DAT

two O
test O
datasets O
Set5 O
and O
Set14 B-DAT

Baboon”, O
”Comic” O
and O
”Monarch” O
from O
Set14 B-DAT

32.39 O
32.39 O
32.55 O
32.52 O
33.00 O
Set14 B-DAT
3 I-DAT
29.00 O
28.97 O
29.08 O
29.14 O
29.42 O

datasets. O
However, O
the O
use O
of O
Set14 B-DAT

super-resolving O
one O
single O
image O
from O
Set14 B-DAT

3 O
30.39 O
32.75 O
33.17 O
33.13 O
Set14 B-DAT
3 I-DAT
27.54 O
29.30 O
29.46 O
29.49 O
BSD300 O

4 O
28.42 O
30.49 O
30.85 O
30.90 O
Set14 B-DAT
4 I-DAT
26.00 O
27.50 O
27.68 O
27.73 O
BSD300 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ture O
where O
the O
feature O
maps O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
the O
non-invertible O
low-pass O
filtering O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
endorfer O
et O
al. O
[27] O
suggested O

- B-DAT
work O
(CNN) O
inspired O
by O
sparse-coding O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gorithms, O
especially O
their O
computational O
and O

- B-DAT
els O
to O
learn O
nonlinear O
relationships O

- B-DAT

- B-DAT

- B-DAT
creasing O
the O
resolution O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rectly O
fed O
to O
the O
network O

- B-DAT
tion O
and O
filter O
size O
reduction O

- B-DAT

- B-DAT
time O
as O
shown O
in O
Sec O

- B-DAT
work O
implicitly O
learns O
the O
processing O

- B-DAT
ing O
[7, O
3, O
31]. O
We O

- B-DAT

- B-DAT

IHR O
using O
a O
Gaussian O
filter O
- B-DAT
thus O
simulating O
the O
camera’s O
point O

spread O
function O
- B-DAT
then O
downsample O
the O
image O
by O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
scaling O
factor O
of O
3. O
The O

- B-DAT

- B-DAT

- B-DAT
tern, O
according O
to O
its O
location O

- B-DAT

- B-DAT
ranges O
the O
elements O
of O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
shuffle O
the O
training O
data O
to O

- B-DAT
ing O
the O
original O
data O
including O

- B-DAT
sampled O
data, O
super-resolved O
data, O
overall O

- B-DAT

- B-DAT
mark O
datasets O
including O
the O
Timofte O

- B-DAT
imately O
10 O
seconds O
in O
length O

- B-DAT

-5 B-DAT

-5 B-DAT
model O
[7], O
(b) O
shows O
weights O

- B-DAT

- B-DAT

-5 B-DAT

- B-DAT
5 O
model O
and O
the O
equations O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pixel O
convolution O
layer O
as O
well O

- B-DAT

-1 B-DAT

-5 B-DAT
model O
[6]. O
Here, O
we O
follow O

- B-DAT

-5 B-DAT

-5 B-DAT
ImageNet O
model O
from O
[7] O
in O

- B-DAT
ilarity O
to O
designed O
features O
including O

- B-DAT

- B-DAT
geNet O
images. O
Results O
in O
Tab O

- B-DAT

- B-DAT

- B-DAT

-5 B-DAT

-5 B-DAT
ImageNet O
model O
in O
this O
section O

-5 B-DAT

-5 B-DAT
ImageNet O
model, O
whilst O
being O
close O

- B-DAT

- B-DAT
put O
image O
to O
HR O
space O

- B-DAT
resolved O
images O
is O
given O
in O

- B-DAT

- B-DAT

-5 B-DAT

-5 B-DAT
ImageNet O
model. O
The O
improvement O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cluding O
our O
own, O
a O
python/theano O

-5 B-DAT

-5 B-DAT
ImageNet O
model, O
the O
number O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
esting O
to O
explore O
ensemble O
prediction O

- B-DAT
resolution O
using O
videos O
from O
the O

-5 B-DAT

-5 B-DAT
ImageNet O
model O
takes O
0.435s O
per O

-5 B-DAT

-5 B-DAT
ImageNet O
model O
takes O
0.434s O
per O

- B-DAT
CNN O
9-5-5 O
ImageNet O
model O
[7 O

- B-DAT

-5 B-DAT

-5 B-DAT
ImageNet O
model O
[7] O
and O
ESPCN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
complexity. O
To O
address O
the O

- B-DAT

- B-DAT

-5 B-DAT

-5 B-DAT
ImageNet O
model O
[7] O
and O
ESPCN O

- B-DAT

- B-DAT

-3 B-DAT

-3 B-DAT
vs O
9-5-5). O
This O
makes O
our O

- B-DAT
implicit O
redundancy O
that O
can O
be O

- B-DAT
resolution O
as O
has O
been O
shown O

- B-DAT

- B-DAT
mation O
from O
videos O
for O
human O

- B-DAT

- B-DAT

- B-DAT

Sequences O
- B-DAT
A O
Review. O
Midwest O
Symposium O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
by O
adaptive O
sparse O
domain O

- B-DAT
ization. O
IEEE O
Transactions O
on O
Image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
invariant O
group-sparse O
regularization. O
In O
IEEE O

- B-DAT
ence O
on O
Computer O
Vision O
(ICCV O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
with O
deep O
convolutional O
neural O

- B-DAT
bard, O
and O
L. O
D. O
Jackel O

- B-DAT
propagation O
network. O
In O
Advances O
in O

- B-DAT
resolution O
with O
fast O
approximate O
convolutional O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ping O
of O
rural O
land O
cover O

- B-DAT

- B-DAT

- B-DAT
sion O
for O
fast O
example-based O
super-resolution O

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
(ACCV O

- B-DAT

- B-DAT

- B-DAT
sketch O
synthesis. O
In O
IEEE O
Conference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
networks. O
In O
Computer O
Vision–ECCV O

- B-DAT
national O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

performance O
on O
the O
Set5 O
[2], O
Set14 B-DAT
[40], O
and O
BSD100 O
[21] O
datasets O

evaluated O
on O
the O
Set5 O
[2], O
Set14 B-DAT
[40], O
and O
BSD100 O
[21] O
datasets O

Set14 B-DAT
PSNR O
(dB) O
SSIM O
NIQE O
SR O

input O
images O
are O
from O
the O
Set14 B-DAT
dataset O
[40 O

are O
for O
the O
Set5 O
[2], O
Set14 B-DAT
[40], O
and O
BSD100 O
[21] O
datasets O

Set14 B-DAT
PSNR O
(dB) O
SSIM O
NIQE O
SR O

upscaling O
for O
the O
Set5 O
[2], O
Set14 B-DAT
[40], O
and O
BSD100 O
[21] O
datasets O

Set14 B-DAT
PSNR O
(dB) O
SSIM O
NIQE O
SR O

values O
for O
the O
datasets O
except O
Set14 B-DAT

ground-truth O
images O
are O
from O
the O
Set14 B-DAT
dataset O
[40 O

evaluated O
on O
the O
Set5 O
[2], O
Set14 B-DAT
[40], O
and O
BSD100 O
[21] O
datasets O

Set14 B-DAT
PSNR O
(dB) O
SSIM O
NIQE O
SR O

evaluated O
on O
the O
Set5 O
[2], O
Set14 B-DAT
[40], O
and O
BSD100 O
[21] O
datasets O

Set14 B-DAT
PSNR O
(dB) O
SSIM O
NIQE O
SR O

a O
deep O
network O
for O
multi-pass O
upscaling B-DAT
in O
company O
with O
a O
discriminator O

deep O
learning-based O
super-resolution O
model, O
enhanced O
upscaling B-DAT
super-resolution O
(EUSR) O
[14], O
from O
the O

to O
prop- O
erly O
regularize O
the O
upscaling B-DAT
modules O
to O
keep O
balance O
of O

pass O
perceptual O
super-resolution O
with O
enhanced O
upscaling B-DAT
(4PP-EUSR),” O
which O
is O
based O
on O

our O
model O
employs O
so-called O
“multi-pass O
upscaling B-DAT

low-resolution O
image O
through O
the O
multiple O
upscaling B-DAT
paths O
in O
our O
model O
are O

base O
deep O
learning O
model, O
multi-pass O
upscaling B-DAT
for O
training, O
structure O
of O
the O

which O
consists O
of O
so-called O
“enhanced O
upscaling B-DAT
modules” O
and O
performed O
well O
in O

nents O
(Fig. O
2): O
a O
multi-scale O
upscaling B-DAT
model, O
employing O
the O
model O
in O

three O
upscaled O
images O
via O
multi-pass O
upscaling B-DAT
(Section O
3.2). O
The O
discriminator O
tries O

3.1 O
Enhanced O
upscaling B-DAT
super-resolution O

shared O
feature O
extraction, O
and O
enhanced O
upscaling B-DAT

features O
are O
upscaled O
via O
“enhanced O
upscaling B-DAT
modules,” O
where O
each O
module O
increases O

one, O
two, O
and O
three O
enhanced O
upscaling B-DAT
modules, O
respectively. O
The O
configurable O
parameters O

residual O
blocks O
in O
the O
enhanced O
upscaling B-DAT
modules. O
We O
consider O
EUSR O
as O

our O
base O
upscaling B-DAT
model O
because O
it O
is O
one O

3.2 O
Multi-pass O
upscaling B-DAT

our O
model O
utilizes O
all O
these O
upscaling B-DAT
paths O
to O
produce O
three O
output O

output O
images O
have O
the O
same O
upscaling B-DAT
factor O
of O
4 O
for O
a O

Enhanced O
upscaling B-DAT
moduleEUM O

Fig. O
4. O
Multi-pass O
upscaling B-DAT
process, O
which O
produces O
three O
upscaled O

the O
other O
hand, O
our O
multi-pass O
upscaling B-DAT
extends O
it O
with O
a O
different O

three O
images O
obtained O
from O
different O
upscaling B-DAT
paths O
are O
used O
for O
training O

that O
may O
occur O
during O
direct O
upscaling B-DAT
via O
the O
×4 O
path, O
two-pass O

upscaling B-DAT
via O
the O
×2 O
path, O
and O

upscaling B-DAT
via O
the O
×8 O
path O
and O

the O
model O
to O
handle O
various O
upscaling B-DAT
scenarios O

ground-truth O
images. O
This O
helps O
our O
upscaling B-DAT
model O
generating O
more O
natural O
images O

networks: O
EUSR O
[14] O
as O
an O
upscaling B-DAT
model O
and O
SRGAN O
[17] O
as O

two O
newly O
proposed O
components: O
multi-pass O
upscaling B-DAT
and O
qualitative O
score O
predictors. O
In O

Thanks O
to O
the O
multi-pass O
upscaling, B-DAT
the O
proposed O
model O
can O
learn O

various O
upscaling B-DAT
patterns, O
which O
will O
be O
further O

the O
residual O
module O
and O
the O
upscaling B-DAT
part O
of O
the O
EUSR O
model O

images. O
Then, O
one O
of O
the O
upscaling B-DAT
paths O
(i.e., O
×2, O
×4, O
and O

effective O
batch O
sizes O
of O
the O
upscaling B-DAT
and O
discrimina- O
tive O
models O
are O

the O
outputs O
obtained O
from O
different O
upscaling B-DAT
paths, O
comparing O
the O
per- O
formance O

trained O
with O
and O
without O
multi-pass O
upscaling, B-DAT
inves- O
tigating O
the O
roles O
of O

interpolation. O
It O
is O
a O
traditional O
upscaling B-DAT
method, O
which O
inter- O
polates O
pixel O

which O
supports O
multiple O
factors O
of O
upscaling B-DAT

5.2 O
Comparing O
upscaling B-DAT
paths O

images O
by O
utilizing O
all O
the O
upscaling B-DAT
paths: O
by O
passing O
through O
the O

10. O
Images O
reconstructed O
by O
different O
upscaling B-DAT
paths O
of O
our O
model. O
The O

results O
obtained O
from O
the O
different O
upscaling B-DAT
paths O
to O
examine O
what O
aspects O

the O
performance O
of O
the O
three O
upscaling B-DAT
paths O
of O
our O
model. O
While O

PI O
values. O
This O
implies O
that O
upscaling B-DAT
using O
the O
×2 O
path O
or O

different O
de- O
pending O
on O
the O
upscaling B-DAT
paths, O
although O
the O
overall O
patterns O

obtained O
by O
the O
two- O
pass O
upscaling B-DAT
using O
the O
×2 O
path O
contains O

two O
passes, O
thus O
the O
two-pass O
upscaling B-DAT
is O
not O
fully O
optimized. O
Second O

trained O
with O
and O
without O
multi-pass O
upscaling B-DAT
for O
the O
Set5 O
[2], O
Set14 O

These O
results O
show O
that O
each O
upscaling B-DAT
path O
of O
our O
model O
learns O

the O
shared O
part O
of O
the O
upscaling B-DAT
paths O
(i.e., O
the O
intermediate O
residual O

5.3 O
Effectiveness O
of O
multi-pass O
upscaling B-DAT

The O
4PP-EUSR O
model O
employs O
multi-pass O
upscaling B-DAT
as O
aforementioned O
in O
Sec- O
tion O

trained O
with O
and O
without O
multi-pass O
upscaling B-DAT

demonstrates O
that O
employing O
multi- O
pass O
upscaling B-DAT
is O
beneficial O
to O
enhance O
both O

The O
model O
trained O
with O
multi-pass O
upscaling B-DAT
shows O
larger O
PSNR O
and O
SSIM O

This O
confirms O
that O
the O
multi-pass O
upscaling B-DAT
can O
improve O
the O
overall O
quality O

Deep O
residual O
network O
with O
enhanced O
upscaling B-DAT
module O
for O
super-resolution. O
In: O
Proceedings O

Set14 B-DAT

input O
images O
are O
from O
the O
Set14 B-DAT

Set14 B-DAT

Set14 B-DAT

ground-truth O
images O
are O
from O
the O
Set14 B-DAT

Set14 B-DAT

Set14 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
per, O
we O
propose O
a O
novel O

- B-DAT

- B-DAT
tional O
quantitative O
performance. O
The O
proposed O

- B-DAT

- B-DAT
work O
and O
two O
quantitative O
score O

- B-DAT

- B-DAT
age O
quality O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
troduced O
convolutional O
layers O
and O
showed O

- B-DAT

- B-DAT

- B-DAT
truth O
(b) O
Upscaled O
by O
bicubic O

- B-DAT
ation O
(d) O
Upscaled O
with O
perceptual O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ror O
[39]. O
They O
mainly O
aim O

- B-DAT
tained O
images, O
which O
can O
be O

- B-DAT

- B-DAT

- B-DAT
tural O
similarity O
(SSIM) O
[37]. O
Fig O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uralness O
of O
the O
output O
images O

- B-DAT
posed O
a O
super-resolution O
model O
named O

- B-DAT
tures O
of O
VGG19 O
[31] O
when O

- B-DAT

- B-DAT
though O
these O
approaches O
improve O
naturalness O

- B-DAT
proved O
results. O
In O
addition, O
it O

- B-DAT

- B-DAT
tor O
relies O
on O
just O
finding O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
quality. O
For O
example, O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
erly O
regularize O
the O
upscaling O
modules O

- B-DAT

- B-DAT
pass O
perceptual O
super-resolution O
with O
enhanced O

- B-DAT

- B-DAT
ing O
the O
aforementioned O
issues O
via O

- B-DAT

- B-DAT

- B-DAT
scaled O
images O
produced O
by O
passing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uralness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
challenge O
[34]: O
the O
enhanced O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ization O
and O
blending O
outputs O
generated O

- B-DAT
gested O
a O
multi-scale O
super-resolution O
method O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
model O
based O
on O
residual O

- B-DAT

- B-DAT
nism O
into O
the O
super-resolution O
task O

- B-DAT

- B-DAT
age O
classifiers. O
In O
the O
former O

- B-DAT
guish O
the O
ground-truth O
images O
from O

- B-DAT

- B-DAT
scaled O
images O
properly. O
When O
an O

- B-DAT

- B-DAT
termediate O
layers O
of O
the O
classifier O

- B-DAT

- B-DAT
lating O
losses O
of O
their O
super-resolution O

- B-DAT

- B-DAT

- B-DAT
nents O
(Fig. O
2): O
a O
multi-scale O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
3.1) O
generates O
three O
upscaled O

- B-DAT

- B-DAT

- B-DAT
tion O
3.3). O
The O
two O
qualitative O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
traction O
part O
extracts O
low-level O
features O

- B-DAT

- B-DAT

- B-DAT
ing O
paths O
have O
one, O
two O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
entiate O
them O
from O
the O
ground-truth O

- B-DAT
inator O
network O
consists O
of O
several O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
connected O
layer O
with O
the O
softmax O

- B-DAT
ploy O
the O
AVA O
dataset O
[26 O

- B-DAT
ferent O
objectives: O
EUSR O
is O
for O

- B-DAT
ter O
perceptual O
quality. O
Our O
proposed O

- B-DAT
titative O
and O
perceptual O
quality, O
with O

- B-DAT

- B-DAT

- B-DAT
titative O
and O
perceptual O
quality. O
While O

- B-DAT

- B-DAT
inforces O
it O
to O
focus O
on O

- B-DAT
oughly O
investigate O
this O
in O
Section O

- B-DAT

- B-DAT
ally O
improved O
images, O
since O
they O

- B-DAT

- B-DAT
itative O
score O
predictors, O
and O
training O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stead, O
we O
set O
the O
input O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tor O
network O
using O
the O
two O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
jective O
of O
the O
super-resolution O
task O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
diate O
output O
is O
1,280 O
[30 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
step, O
two O
input O
image O

- B-DAT
tive O
models O
are O
six O
and O

- B-DAT

- B-DAT

- B-DAT
mance O
of O
our O
method O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
paring O
the O
outputs O
obtained O
from O

- B-DAT
formance O
of O
our O
method O
trained O

- B-DAT

- B-DAT
tigating O
the O
roles O
of O
loss O

- B-DAT

- B-DAT
cluding O
PSNR, O
SSIM O
[37], O
NIQE O

- B-DAT

- B-DAT

- B-DAT
ity O
metrics O
are O
calculated O
on O

- B-DAT
isting O
studies O
[17,14,19]. O
In O
addition O

- B-DAT

- B-DAT
polates O
pixel O
values O
based O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
MSE O
model O
is O
trained O
with O

- B-DAT

- B-DAT

- B-DAT

-128 B-DAT
layer O
of O
VGG19. O
Their O
results O

- B-DAT
thors’ O
supplementary O
material2 O

- B-DAT
Net, O
but O
does O
not O
employ O

- B-DAT

- B-DAT
ensemble” O
strategy, O
which O
obtains O
eight O

- B-DAT

- B-DAT
scale O
super-resolution O
and O
consists O
of O

- B-DAT
plained O
in O
Section O
3.1. O
We O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-128 B-DAT
layer O
of O
VGG19), O
and O
SRGAN-VGG54 O

- B-DAT

-512 B-DAT
layer O
of O
VGG19). O
The O
compared O

- B-DAT

- B-DAT

- B-DAT
based O
loss O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tween O
the O
VGG19 O
features O
for O

- B-DAT

- B-DAT
ber O
of O
model O
parameters, O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
though O
all O
the O
models O
except O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uated O
on O
the O
three O
datasets O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
scaled O
images O
have O
poor O
perceptual O

- B-DAT
tual O
quality O
(i.e., O
SRGAN O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pares O
the O
baselines O
and O
our O

- B-DAT

- B-DAT
tures O
are O
expected. O
First, O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tative O
quality O
than O
all O
the O

- B-DAT
sidering O
both O
the O
quantitative O
and O

- B-DAT
ploying O
only O
the O
reconstruction O
loss O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

P O
- B-DAT

- B-DAT

- B-DAT
formance O
of O
the O
super-resolution O
methods O

- B-DAT

- B-DAT
jective O
tests O
in O
the O
recommendation O

- B-DAT

-13 B-DAT
[36]. O
As O
for O
the O
evaluation O

- B-DAT

- B-DAT

- B-DAT
VGG22 O
get O
the O
lowest O
opinion O

- B-DAT

- B-DAT
tive O
and O
perceptual O
quality O
in O

- B-DAT

- B-DAT

- B-DAT
ing O
paths O
of O
the O
4PP-EUSR O

- B-DAT
pending O
on O
the O
upscaling O
paths O

- B-DAT

- B-DAT
pass O
upscaling O
using O
the O
×2 O

- B-DAT

- B-DAT
son O
is O
due O
to O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put O
is O
not O
necessarily O
avoided O

- B-DAT
resolution O
and O
thus O
the O
model O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
3.2. O
To O
investigate O
its O

- B-DAT

- B-DAT
pass O
upscaling O
is O
beneficial O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions. O
The O
input O
and O
ground-truth O

- B-DAT

- B-DAT
ity O
(i.e., O
larger O
PSNR O
values O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ful O
to O
construct O
dispersed O
high-frequency O

- B-DAT
cally, O
we O
alter O
the O
weight O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rea, O
under O
the O
“ICT O
Consilience O

-2018 B-DAT

-2017 B-DAT

-0 B-DAT

-01015 B-DAT

-16 B-DAT

-0004, B-DAT
Development O
of O
Intelligent O

- B-DAT

- B-DAT
derstanding O

- B-DAT
mawat, O
S., O
Irving, O
G., O
Isard O

- B-DAT

- B-DAT
chine O
learning. O
In: O
Proceedings O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
British O
Machine O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
performance O
on O
ImageNet O
classification O

- B-DAT
national O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks. O
In: O
Proceedings O

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT

- B-DAT
space O
projection O
and O
neighbor O
embedding O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puter O
Vision. O
pp. O
416–423 O
(2001 O

- B-DAT

- B-DAT

- B-DAT
mation O
with O
non-aligned O
data. O
arXiv:1803.02077 O

- B-DAT

- B-DAT

- B-DAT
thetic O
visual O
analysis. O
In: O
Proceedings O

- B-DAT
nition O
challenge. O
International O
Journal O
of O

- B-DAT

- B-DAT

- B-DAT
ings O
of O
the O
IEEE O
International O

- B-DAT
verted O
residuals O
and O
linear O
bottlenecks O

- B-DAT

- B-DAT
tion O
architecture O
for O
computer O
vision O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

-13 B-DAT

- B-DAT
ing O
13(4), O
600–612 O
(2004 O

- B-DAT

- B-DAT

- B-DAT
computing O
74(17), O
3193–3203 O
(2011 O

- B-DAT

- B-DAT

- B-DAT
representations. O
In: O
Proceedings O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

on O
dataset O
Set5 O
[1] O
and O
Set14 B-DAT
[32]. O
To O
further O
compare O
the O

F1 O
setting O
on O
Set5 O
and O
Set14, B-DAT
our O
method O
pro- O
duces O
comparable O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
per, O
we O
show O
that O
proper O

- B-DAT
pensation O
is O
crucial O
for O
achieving O

- B-DAT

- B-DAT
iments O
show O
the O
suitability O
of O

- B-DAT

- B-DAT

- B-DAT
porates O
the O
SPMC O
layer O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
image O
SR O
where O
details O
have O

- B-DAT

- B-DAT
struct O
accurate O
correspondence; O
and O
(2 O

- B-DAT

- B-DAT
secutive O
frames O
increases O
the O
difficulty O

- B-DAT
sponding O
image O
regions, O
subtle O
sub-pixel O

- B-DAT
ily O
benefits O
restoration O
of O
details O

- B-DAT

- B-DAT

- B-DAT
struct O
the O
HR O
output O
based O

- B-DAT
work. O
Most O
of O
these O
methods O

- B-DAT
by-case O
parameter-tuning O
and O
costly O
computation O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
(SPMC) O
strategy, O
which O
is O

- B-DAT
age O
detail O
fusion O
from O
multiple O

- B-DAT
cess O
of O
video O
SR. O
We O

- B-DAT
based O
video O
SR O
systems O
can O

- B-DAT

- B-DAT
herent O
in O
input O
frames, O
or O

- B-DAT

- B-DAT
meaningful O
property O
of O
SR O
systems O

- B-DAT

- B-DAT
plied. O
For O
example, O
ESPCN O
[26 O

- B-DAT
put, O
once O
trained O

- B-DAT

- B-DAT
plied O
for O
arbitrary O
scaling O
factors O

- B-DAT

- B-DAT

- B-DAT
CNN O
[3], O
a O
majority O
of O

- B-DAT
resolution O
images O
as O
input, O
and O

- B-DAT

- B-DAT
volution O
layer O
instead O

- B-DAT

- B-DAT

- B-DAT
tained O
under O
different O
flow O
parameters O

- B-DAT
timation O
is O
separated O
from O
training O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dicted O
affine O
transformation O
parameters. O
Based O

- B-DAT
Net O
[13] O
used O
a O
similar O

- B-DAT
dence. O
Yu O
et O
al. O
[30 O

- B-DAT

- B-DAT

- B-DAT
size. O
The O
transpose O
ST O
corresponds O

- B-DAT

- B-DAT
duces O
the O
green O
signal O
through O

- B-DAT
mation O
operators, O
respectively. O
ni O
is O

- B-DAT
duce O
the O
warped O
image. O
However O

- B-DAT
ally O
makes O
use O
of O
flow O

- B-DAT

- B-DAT
construction O
error O

- B-DAT
forward O
generation O
process O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
between O
frames; O
aligning O
frames O

- B-DAT
tion; O
and O
finally O
increasing O
image O

LI O
- B-DAT

F O
- B-DAT

- B-DAT

- B-DAT
pensation O
transformer O
(MCT) O
module O
from O

- B-DAT
eters O
and O
accordingly O
less O
computation O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
factor. O
The O
layer O
contains O

- B-DAT
ordinates O
are O
first O
calculated O
according O

- B-DAT
ordinates O
as O
operator O
WF O
;α O

- B-DAT
structed O
in O
the O
enlarged O
image O

- B-DAT

- B-DAT
tive O
with O
respect O
to O
each O

- B-DAT
ilar O
derivatives O
can O
be O
derived O

- B-DAT
lation O
kernel, O
because O
of O
its O

- B-DAT
lowing O
back-propagating O
loss O
to O
flow O

- B-DAT
pensation O
and O
resolution O
enhancement. O
Note O

- B-DAT
works O
with O
almost O
no O
additional O

- B-DAT
sated O
frames O
{JHi O
} O
expressed O

- B-DAT

- B-DAT

- B-DAT
upsampling, O
{JHi O
} O
is O
sparse O

- B-DAT

- B-DAT
erence O
frame O
as O
the O
guidance O

- B-DAT
age O
structures. O
On O
the O
other O

- B-DAT

- B-DAT
ing O
information O
in O
other O
frames O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
to O
1/4 O
of O
it O

- B-DAT
formation O
can O
be O
effectively O
aggregated O

- B-DAT

- B-DAT

- B-DAT
tion O
layers O
are O
with O
kernel O

- B-DAT
tified O
Linear O
Units O
(ReLU) O
are O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vised O
warping O
loss O
is O
used O

- B-DAT
olution, O
aligned O
with O
ILi O

- B-DAT
ularization O
weight. O
We O
set O
λ1 O

- B-DAT
responding O
to O
reference O
frame O
IL0 O

- B-DAT

- B-DAT

- B-DAT
taining O
rich O
fine O
details. O
To O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
of O
[19, O
20, O
23 O

- B-DAT
lation. O
LR O
input O
is O
obtained O

- B-DAT
ule O
(clipped O
by O
global O
norm O

- B-DAT
cess. O
At O
each O
iteration, O
we O

- B-DAT
tive O
frames O
(e.g. O
NF O

- B-DAT
domly O
crop O
a O
100×100 O
image O

- B-DAT
ing O
factor. O
Above O
parameters O
are O

- B-DAT
erations, O
we O
fix O
the O
parameters O

- B-DAT
ing O
only O
loss O
LSR O
in O

- B-DAT

- B-DAT

- B-DAT
duces O
multiple O
outputs O
(one O
for O

- B-DAT
cedure. O
An O
example O
is O
shown O

- B-DAT
bic O
×4 O
for O
reference O
frame O

- B-DAT
responding O
to O
three O
time O
steps O

- B-DAT

- B-DAT
age O
in O
our O
method, O
the O

- B-DAT
ing O
all O
input O
frames O
with O

- B-DAT
ically, O
Fig. O
5(f)-(h) O
are O
outputs O

- B-DAT
ever, O
if O
we O
only O
use O

- B-DAT

- B-DAT
sults O
are O
almost O
the O
same O

- B-DAT
ternal O
examples O
because O
if O
the O

0.92 O
31.85 O
/ O
0.92 O
- B-DAT
33.39 O
/ O
0.94 O
36.71 O

0.82 O
29.42 O
/ O
0.87 O
- B-DAT
28.55 O
/ O
0.85 O
31.92 O

0.92 O
31.82 O
/ O
0.92 O
- B-DAT
35.44 O
/ O
0.95 O
36.62 O

0.82 O
29.55 O
/ O
0.87 O
- B-DAT
30.73 O
/ O
0.88 O
32.10 O

Vid4×3 O
25.64 O
/ O
0.80 O
- B-DAT
25.31 O
/ O
0.76 O
27.25 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tails, O
the O
emphasis O
on O
the O

- B-DAT
tail O
recovery O
and O
slightly O
degrade O

- B-DAT

- B-DAT
tively O
estimates O
motion O
flow, O
blur O

- B-DAT
ditional O
and O
CNN-based O
methods. O
We O

- B-DAT
cent O
deep-learning-based O
method O
VSRnet O
[14 O

- B-DAT
son. O
We O
use O
author-provided O
implementation O

- B-DAT

- B-DAT
video O
dataset O
VID4 O
[20]. O
The O

- B-DAT
duces O
comparable O
or O
slightly O
lower O

- B-DAT

- B-DAT

- B-DAT

0.96 O
37.35 O
/ O
0.96 O
- B-DAT
Set O
5 O
(×3) O
32.75 O

0.92 O
33.45 O
/ O
0.92 O
- B-DAT
Set O
5 O
(×4) O
30.49 O

0.91 O
32.70 O
/ O
0.91 O
- B-DAT
Set O
14 O
(×3) O
29.30 O

0.83 O
29.36 O
/ O
0.83 O
- B-DAT
Set O
14 O
(×4) O
27.45 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ings O
for O
other O
methods O
(F31 O

- B-DAT
Rnet O
[14] O
requires O
≈40s O
for O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pensation O
layer O
that O
can O
better O

- B-DAT

- B-DAT
ment. O
We O
have O
conducted O
extensive O

- B-DAT
date O
the O
effectiveness O
of O
each O

- B-DAT

- B-DAT
itatively O
and O
quantitatively, O
at O
the O

- B-DAT

- B-DAT
bic O
×4. O
(b)-(d) O
Output O
from O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT

- B-DAT

- B-DAT
tion O
algorithm O
for O
pure O
translational O

- B-DAT

- B-DAT
ing, O
10(8):1187–1193, O
2001 O

- B-DAT

- B-DAT
sorflow.org O

- B-DAT
ume O
9, O
pages O
249–256, O
2010 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
learning-detection. O
IEEE O
Trans. O
Pattern O
Anal O

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT

- B-DAT
jani, O
J. O
Totz, O
Z. O
Wang O

- B-DAT

- B-DAT
age O
super-resolution O
using O
a O
generative O

- B-DAT
resolution O
via O
deep O
draft-ensemble O
learning O

- B-DAT
ing O
for O
stereo O
matching. O
In O

- B-DAT

- B-DAT

- B-DAT
ing O
very O
deep O
convolutional O
encoder-decoder O

- B-DAT
lutional O
networks O
for O
disparity, O
optical O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT
resolution O
without O
explicit O
subpixel O
motion O

- B-DAT
ness O
constancy O
and O
motion O
smoothness O

- B-DAT
volutional O
neural O
network O
to O
compare O

- B-DAT
nal O
of O
Machine O
Learning O
Research O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

experiments O
from O
Set5 O
[2] O
and O
Set14 B-DAT
[51] O
test O
images O
to O
BSD200 O

if O
we O
use O
the O
larger O
Set14 B-DAT
set O
[51]. O
The O
upscaling O
factor O

The O
Set5 O
[2] O
(5 O
images), O
Set14 B-DAT
[51] O
(14 O
images) O
and O
BSD200 O

with O
their O
restoration O
performance O
on O
Set14 B-DAT

dB) O
and O
MSSIM O
on O
the O
Set14 B-DAT
dataset O

The O
chart O
is O
based O
on O
Set14 B-DAT
results O
summarized O
in O
Table O
3 O

15. O
The O
“ppt3” O
image O
from O
Set14 B-DAT
with O
an O
upscaling O
factor O
3 O

16. O
The O
“zebra” O
image O
from O
Set14 B-DAT
with O
an O
upscaling O
factor O
3 O

the O
Set5 O
dataset O
with O
an O
upscaling B-DAT
factor O
3). O
The O
proposed O
method O

kernel, O
sub-sample O
it O
by O
the O
upscaling B-DAT
factor, O
and O
upscale O
it O
by O

larger O
Set14 O
set O
[51]. O
The O
upscaling B-DAT
factor O
is O
3. O
We O
use O

on O
the O
ImageNet O
by O
an O
upscaling B-DAT
factor O
3. O
Please O
refer O
to O

our O
published O
implementation O
for O
upscaling B-DAT
factors O
2 O
and O
4. O
Interestingly O

trained O
on O
ImageNet O
with O
an O
upscaling B-DAT
factor O
3. O
The O
filters O
are O

test O
on O
Set5 O
with O
an O
upscaling B-DAT
factor O
3. O
The O
results O
observed O

4.1. O
The O
results O
with O
an O
upscaling B-DAT
factor O
3 O
on O
Set5 O
are O

on O
the O
ImageNet. O
For O
each O
upscaling B-DAT
factor O
∈ O
{2, O
3, O
4 O

to O
evaluate O
the O
performance O
of O
upscaling B-DAT
factors O
2, O
3, O
and O
4 O

108 O
backpropagations. O
Specifically, O
for O
the O
upscaling B-DAT
factor O
3, O
the O
average O
gains O

of O
different O
approaches O
by O
an O
upscaling B-DAT
factor O
3. O
As O
can O
be O

for O
fair O
quantitative O
comparison. O
The O
upscaling B-DAT
factor O
is O
3 O
and O
the O

only O
evaluate O
the O
performance O
of O
upscaling B-DAT
factor O
3. O
Comparisons. O
We O
compare O

network O
to O
cope O
with O
different O
upscaling B-DAT
factors O

Fattal, O
R.: O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

H.: O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In: O
IEEE O

image O
from O
Set5 O
with O
an O
upscaling B-DAT
factor O
3 O

image O
from O
Set14 O
with O
an O
upscaling B-DAT
factor O
3 O

image O
from O
Set14 O
with O
an O
upscaling B-DAT
factor O
3 O

if O
we O
use O
the O
larger O
Set14 B-DAT

dB) O
and O
MSSIM O
on O
the O
Set14 B-DAT

The O
chart O
is O
based O
on O
Set14 B-DAT

15. O
The O
“ppt3” O
image O
from O
Set14 B-DAT

16. O
The O
“zebra” O
image O
from O
Set14 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
offs O
between O
performance O
and O
speed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
image, O
is O
a O
classical O

- B-DAT

- B-DAT
tiplicity O
of O
solutions O
exist O
for O

- B-DAT

- B-DAT
verse O
problem, O
of O
which O
solution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
exemplar O
pairs O
[2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
structing O
high-resolution O
patches. O
The O
overlapping O

- B-DAT

- B-DAT

- B-DAT
work O
[27] O
(more O
details O
in O

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
images. O
Our O
method O

- B-DAT
tally O
from O
existing O
external O
example-based O

- B-DAT
processing O

- B-DAT

- B-DAT
volutional O
Neural O
Network O
(SRCNN)1. O
The O

- B-DAT
ture O
is O
intentionally O
designed O
with O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
example-based O
methods. O
Furthermore, O
the O

- B-DAT

- B-DAT
work O
for O
image O
super-resolution. O
The O

- B-DAT
rectly O
learns O
an O
end-to-end O
mapping O

- B-DAT
and O
high-resolution O
images, O
with O
little O

- B-DAT
processing O
beyond O
the O
optimization O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution, O
and O
can O
achieve O
good O

- B-DAT

- B-DAT
linear O
mapping O
layers. O
Secondly, O
we O

- B-DAT
strate O
that O
performance O
can O
be O

- B-DAT

- B-DAT
ber O
of O
recently O
published O
methods O

- B-DAT

- B-DAT

- B-DAT
olution O
algorithms O
can O
be O
categorized O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
similarity O
property O
and O
generate O
exemplar O

- B-DAT
nal O
example-based O
methods O
[2], O
[4 O

- B-DAT
resolution O
patches O
from O
external O
datasets O

- B-DAT

- B-DAT
tionaries O
are O
directly O
presented O
as O

- B-DAT

- B-DAT

- B-DAT
sponding O
high-resolution O
patch O
used O
for O

- B-DAT
nique O
as O
an O
alternative O
to O

- B-DAT
borhood O
regression O
[41], O
[42] O
are O

- B-DAT
coding-based O
method O
and O
its O
several O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tioned O
methods O
first O
transform O
the O

- B-DAT
ferent O
color O
space O
(YCbCr O
or O

- B-DAT

- B-DAT
fully O
applied O
to O
other O
computer O

- B-DAT

- B-DAT
ceptron O
(MLP), O
whose O
all O
layers O

- B-DAT

- B-DAT

- B-DAT
work O
is O
applied O
for O
natural O

- B-DAT
moving O
noisy O
patterns O
(dirt/rain) O
[12 O

- B-DAT

- B-DAT

- B-DAT
resolution O
pipeline O
under O
the O
notion O

- B-DAT
based O
approach O
[16]. O
The O
deep O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
end O
mapping. O
Further, O
the O
SRCNN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
extracts O
(overlapping) O
patches O
from O

- B-DAT
resolution O
image O
Y O
and O
represents O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
neural O
network. O
An O
overview O

- B-DAT

- B-DAT
spectively, O
and O
’∗’ O
denotes O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tors O
is O
conceptually O
a O
representation O

- B-DAT

- B-DAT

- B-DAT
plexity O
of O
the O
model O
(n2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
patch). O
Motivated O
by O
this O

- B-DAT
lutional O
layer O
to O
produce O
the O

- B-DAT

- B-DAT

- B-DAT
tion O
and O
representation) O
becomes O
purely O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution) O
dictionary. O
If O
the O
dictionary O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
based O
SR O
method O
can O
be O

- B-DAT
volutional O
neural O
network O
(with O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
parameters. O
For O
example, O
we O
can O

- B-DAT
resolution O
patch O
(to O
the O
extreme O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
quires O
the O
estimation O
of O
network O

- B-DAT
imizing O
the O
loss O
between O
the O

- B-DAT
resolution O
images O
X. O
Given O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
crafted” O
methods. O
Despite O
that O
the O

- B-DAT
tory O
performance O
when O
the O
model O

- B-DAT
scent O
with O
the O
standard O
backpropagation O

- B-DAT
ular, O
the O
weight O
matrices O
are O

- B-DAT
erations, O
η O
is O
the O
learning O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ping O
and O
require O
some O
averaging O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
layers O
have O
no O
padding O

- B-DAT

- B-DAT
age O
[26]. O
We O
have O
also O

- B-DAT
tions O
between O
super-resolution O
performance O
and O

- B-DAT
quently, O
we O
compare O
our O
method O

- B-DAT

- B-DAT
the-arts O
both O
quantitatively O
and O
qualitatively O

- B-DAT

- B-DAT
4.4, O
so O
c O
= O
1 O

- B-DAT
tion O
training O
partition. O
The O
size O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nal O
images O
with O
a O
stride O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
geNet O
is O
about O
the O
same O

- B-DAT

- B-DAT
formance O
may O
be O
further O
boosted O

- B-DAT

- B-DAT
tured O
sufficient O
variability O
of O
natural O

- B-DAT

- B-DAT

Laplacian/Gaussian O
filters, O
the O
filters O
a O
- B-DAT
e O
are O
like O
edge O
detectors O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
crease O
the O
network O
width6, O
i.e O

- B-DAT
coding-based O
method O
(31.42 O
dB O

-1 B-DAT

-5 B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-7 B-DAT

- B-DAT
ing O
[17]. O
The O
term O
‘width O

-3 B-DAT

-5 B-DAT

-5 B-DAT

-5 B-DAT

-3 B-DAT

- B-DAT
5 O
and O
9-5-5 O
on O
Set5 O

-1 B-DAT

-5, B-DAT
9-3-5, O
and O
9-5-5 O
is O
8,032 O

-5 B-DAT

-5 B-DAT
is O
almost O
twice O
of O
9-3-5 O

- B-DAT

- B-DAT

-1 B-DAT

-1 B-DAT

-5, B-DAT
9-3-1-5, O
9-5-1-5, O
which O
add O
an O

-1 B-DAT

-5, B-DAT
9-3-5, O
and O
9-5-5, O
respectively. O
The O

- B-DAT
ditional O
layer O
are O
the O
same O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lution O
is O
found O
not O
as O

-1 B-DAT

-5 B-DAT
network, O
then O
the O
performance O
degrades O

- B-DAT

- B-DAT
ure O
9(a)). O
If O
we O
go O

- B-DAT

-1 B-DAT

-5 B-DAT
vs. O
9-1-1-5 O

-3 B-DAT

-5 B-DAT
vs. O
9-3-1-5 O

-5 B-DAT

-5 B-DAT
vs. O
9-5-1-5 O

- B-DAT

- B-DAT

-1 B-DAT

-5, B-DAT
then O
we O
have O
to O
set O

-3 B-DAT

- B-DAT
3-5 O
and O
9-3-3-3. O
However, O
from O

-3 B-DAT

-1 B-DAT

-5 B-DAT
network O

- B-DAT

- B-DAT

- B-DAT
vestigations O
to O
better O
understand O
gradients O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-1 B-DAT

-5 B-DAT
(n22 O
= O
32) O
and O
9-1-1-1-5 O

-3 B-DAT

-3 B-DAT

-5 B-DAT
and O
9-3-3-3 O

- B-DAT
speed O
trade-off: O
a O
three-layer O
network O

- B-DAT
of-the-art O
SR O
methods O

SC O
- B-DAT
sparse O
coding-based O
method O
of O
Yang O

et O
al. O
[50] O
• O
NE+LLE O
- B-DAT
neighbour O
embedding O
+ O
locally O
linear O

embedding O
method O
[4] O
• O
ANR O
- B-DAT
Anchored O
Neighbourhood O
Regression O

method O
[41] O
• O
A+ O
- B-DAT
Adjusted O
Anchored O
Neighbourhood O
Regres O

method O
[42], O
and O
• O
KK O
- B-DAT
the O
method O
described O
in O
[25 O

- B-DAT
based O
methods, O
according O
to O
the O

- B-DAT
sampled O
using O
the O
same O
bicubic O

- B-DAT
terion O
(IFC) O
[38], O
noise O
quality O

- B-DAT

- B-DAT

- B-DAT
scale O
structure O
similarity O
index O
(MSSSIM O

ANR O
- B-DAT
31.92 O
dB O

A+ O
- B-DAT
32.59 O
dB O

SC O
- B-DAT
31.42 O
dB O

Bicubic O
- B-DAT
30.39 O
dB O

NE+LLE O
- B-DAT
31.84 O
dB O

KK O
- B-DAT
32.28 O
dB O

- B-DAT
CNN O
outperforms O
existing O
state-of-the-art O
methods O

- B-DAT

- B-DAT

- B-DAT
cific O
network O
(9-5-5) O
using O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentation, O
whereas O
ours O
are O
in O

- B-DAT

-1 B-DAT

-5, B-DAT
9-3-5, O
and O
9-5-5. O
It O
is O

- B-DAT
1-5 O
network O
is O
the O
fastest O

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-5 B-DAT
network. O
Note O
the O
speed O
gap O

- B-DAT
pletely O
feed-forward. O
The O
9-5-5 O
network O

- B-DAT

- B-DAT

- B-DAT
terpolation. O
It O
is O
interesting O
to O

- B-DAT

- B-DAT
out O
altering O
the O
learning O
mechanism O

- B-DAT
sign. O
In O
particular, O
it O
can O

- B-DAT
nels O
simultaneously O
by O
setting O
the O

- B-DAT

- B-DAT

-5 B-DAT

- B-DAT

A+ O
[41] O
SRCNN O
2 O
33.66 O
- B-DAT
35.77 O
36.20 O
35.83 O
36.54 O
36.66 O

31.92 O
32.59 O
32.75 O
4 O
28.42 O
- B-DAT
29.61 O
30.03 O
29.69 O
30.28 O
30.49 O

2 O
0.9299 O
- B-DAT
0.9490 O
0.9511 O
0.9499 O
0.9544 O
0.9542 O

0.8968 O
0.9088 O
0.9090 O
4 O
0.8104 O
- B-DAT
0.8402 O
0.8541 O
0.8419 O
0.8603 O
0.8628 O

2 O
6.10 O
- B-DAT
7.84 O
6.87 O
8.09 O
8.48 O
8.05 O

4.52 O
4.84 O
4.58 O
4 O
2.35 O
- B-DAT
2.94 O
2.81 O
3.02 O
3.26 O
3.01 O

2 O
36.73 O
- B-DAT
42.90 O
39.49 O
43.28 O
44.58 O
41.13 O

33.10 O
34.48 O
33.21 O
4 O
21.42 O
- B-DAT
25.56 O
24.99 O
25.72 O
26.97 O
25.96 O

2 O
50.06 O
- B-DAT
58.45 O
57.15 O
58.61 O
60.06 O
59.49 O

46.02 O
47.17 O
47.10 O
4 O
37.21 O
- B-DAT
39.85 O
40.40 O
40.01 O
41.03 O
41.13 O

2 O
0.9915 O
- B-DAT
0.9953 O
0.9953 O
0.9954 O
0.9960 O
0.9959 O

0.9844 O
0.9867 O
0.9866 O
4 O
0.9516 O
- B-DAT
0.9666 O
0.9695 O
0.9672 O
0.9720 O
0.9725 O

A+ O
[41] O
SRCNN O
2 O
30.23 O
- B-DAT
31.76 O
32.11 O
31.80 O
32.28 O
32.45 O

28.65 O
29.13 O
29.30 O
4 O
26.00 O
- B-DAT
26.81 O
27.14 O
26.85 O
27.32 O
27.50 O

2 O
0.8687 O
- B-DAT
0.8993 O
0.9026 O
0.9004 O
0.9056 O
0.9067 O

0.8093 O
0.8188 O
0.8215 O
4 O
0.7019 O
- B-DAT
0.7331 O
0.7419 O
0.7352 O
0.7491 O
0.7513 O

2 O
6.09 O
- B-DAT
7.59 O
6.83 O
7.81 O
8.11 O
7.76 O

4.23 O
4.45 O
4.26 O
4 O
2.23 O
- B-DAT
2.71 O
2.57 O
2.78 O
2.94 O
2.74 O

2 O
40.98 O
- B-DAT
41.34 O
38.86 O
41.79 O
42.61 O
38.95 O

37.22 O
38.24 O
35.25 O
4 O
26.15 O
- B-DAT
31.17 O
29.18 O
31.27 O
32.31 O
30.46 O

2 O
47.64 O
- B-DAT
54.47 O
53.85 O
54.57 O
55.62 O
55.39 O

43.36 O
44.25 O
44.32 O
4 O
35.71 O
- B-DAT
37.75 O
38.26 O
37.85 O
38.72 O
38.87 O

2 O
0.9813 O
- B-DAT
0.9886 O
0.9890 O
0.9888 O
0.9896 O
0.9897 O

0.9647 O
0.9669 O
0.9675 O
4 O
0.9134 O
- B-DAT
0.9317 O
0.9338 O
0.9326 O
0.9371 O
0.9376 O

A+ O
[41] O
SRCNN O
2 O
28.38 O
- B-DAT
29.67 O
30.02 O
29.72 O
30.14 O
30.29 O

26.72 O
27.05 O
27.18 O
4 O
24.65 O
- B-DAT
25.21 O
25.38 O
25.25 O
25.51 O
25.60 O

2 O
0.8524 O
- B-DAT
0.8886 O
0.8935 O
0.8900 O
0.8966 O
0.8977 O

0.7843 O
0.7945 O
0.7971 O
4 O
0.6727 O
- B-DAT
0.7037 O
0.7093 O
0.7060 O
0.7171 O
0.7184 O

2 O
5.30 O
- B-DAT
7.10 O
6.33 O
7.28 O
7.51 O
7.21 O

3.91 O
4.07 O
3.91 O
4 O
1.95 O
- B-DAT
2.45 O
2.24 O
2.51 O
2.62 O
2.45 O

2 O
36.84 O
- B-DAT
41.52 O
38.54 O
41.72 O
42.37 O
39.66 O

34.81 O
35.58 O
34.72 O
4 O
21.72 O
- B-DAT
25.15 O
24.87 O
25.27 O
26.01 O
25.65 O

2 O
46.15 O
- B-DAT
52.56 O
52.21 O
52.69 O
53.56 O
53.58 O

41.53 O
42.19 O
42.29 O
4 O
34.86 O
- B-DAT
36.52 O
36.80 O
36.64 O
37.18 O
37.24 O

2 O
0.9780 O
- B-DAT
0.9869 O
0.9876 O
0.9872 O
0.9883 O
0.9883 O

0.9581 O
0.9609 O
0.9614 O
4 O
0.9005 O
- B-DAT
0.9203 O
0.9215 O
0.9214 O
0.9256 O
0.9261 O

-1 B-DAT

-5 B-DAT

-3 B-DAT

-5 B-DAT

-5 B-DAT

-5 B-DAT

- B-DAT
of-the-art O
super-resolution O
quality, O
whilst O
maintains O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-art O
color O
SR O
method O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nel O
when O
training O
is O
performed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
correlation O
among O
each O
other. O
The O

- B-DAT

- B-DAT
channel O
network O
(“Y O
only”). O
It O

- B-DAT

- B-DAT
work O
is O
not O
that O
significant O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
images, O
with O
little O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vantages O
of O
simplicity O
and O
robustness O

- B-DAT

- B-DAT
blurring O
or O
simultaneous O
SR+denoising. O
One O

- B-DAT

- B-DAT
complexity O
single-image O
super-resolution O
based O
on O

- B-DAT

- B-DAT
bor O
embedding. O
In: O
IEEE O
Conference O

- B-DAT

- B-DAT

- B-DAT
resolution. O
IEEE O
Transactions O
on O
Image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
linear O
structure O
within O
convolutional O

- B-DAT
tems O
(2014 O

- B-DAT
tional O
network O
for O
image O
super-resolution O

- B-DAT
ence O
on O
Computer O
Vision, O
pp O

- B-DAT
tional O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT

- B-DAT
resolution. O
Computer O
Graphics O
and O
Applications O

- B-DAT
level O
vision. O
International O
Journal O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
lutional O
neural O
networks O
with O
low O

- B-DAT
tems. O
pp. O
769–776 O
(2008 O

- B-DAT

- B-DAT

- B-DAT
ten O
zip O
code O
recognition. O
Neural O

- B-DAT

- B-DAT
rithms. O
In: O
Advances O
in O
Neural O

- B-DAT
mentation O
algorithms O
and O
measuring O
ecological O

- B-DAT

- B-DAT

- B-DAT
tion. O
In: O
IEEE O
International O
Conference O

- B-DAT
chine O
learning O
approach O
for O
non-blind O

- B-DAT

- B-DAT
tics. O
IEEE O
Transactions O
on O
Image O

- B-DAT
tation O
by O
joint O
identification-verification. O
In O

- B-DAT
quality O
object O
detection. O
arXiv O
preprint O

- B-DAT

- B-DAT

- B-DAT
ternational O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
ilarity O
for O
image O
quality O
assessment O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tionary O
training O
for O
image O
super-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
sparse-representations. O
In: O
Curves O
and O

- B-DAT

- B-DAT
CNNs O
for O
fine-grained O
category O
detection O

- B-DAT
ence O
on O
Computer O
Vision. O
pp O

- B-DAT
tion O
Engineering O
from O
Beijing O
Institute O

- B-DAT
nology, O
China, O
in O
2011. O
He O

- B-DAT
sity O
of O
Hong O
Kong. O
His O

- B-DAT

- B-DAT
versity O
of O
London O
in O
2010 O

- B-DAT
toral O
researcher O
at O
Vision O
Semantics O

- B-DAT
inghua O
University O
in O
2007, O
and O

- B-DAT
ence O
on O
Computer O
Vision O
and O

- B-DAT
tion O
(CVPR) O
2009. O
He O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sachusetts O
Institute O
of O
Technology, O
Cambridge O

- B-DAT
cessing. O
He O
received O
the O
Best O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

resolution O
(3x) O
experiments O
on O
Set14 B-DAT

8 O
are O
obtained O
on O
the O
Set14 B-DAT
dataset, O
and O
2 O

Set14 B-DAT

standard O
super-resolution O
benchmarks O
Set O
5, O
Set14 B-DAT
and O

Set14 B-DAT

Bischof, O
"Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests," O
in O
Proceedings O

8 O
are O
obtained O
on O
the O
Set14 B-DAT

standard O
super-resolution O
benchmarks O
Set O
5, O
Set14 B-DAT

Set14 B-DAT
×2 I-DAT
30.23 O
32.28 O
32.26 O
32.33 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
second- O
order O
gradients O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
second-order O
gradients O
and O
their O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
patches, O
with O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Although O
the O
𝑙0 O
- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-2 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

orientation, O
with O
a O
value O
between O
-90 B-DAT

-1 B-DAT

-2 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
show O
that O
the O
proposed O
GWRR O

-1 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT
and O
Table-3 O
validate O
that O
using O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT
and O
Table O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-3 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
FARF O
FARF* O
SRCNN O

-2 B-DAT

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT
summarizes O
the O
performances O
of O
our O

-3 B-DAT
gives O
more O
details O
of O
the O

- B-DAT
FARF O

-3 B-DAT

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT
also O
shows O
that O
the O
fine-tuned O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-2014 B-DAT

-2015 B-DAT

-2016 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-918 B-DAT

- B-DAT

- B-DAT

-1927 B-DAT

-1588, B-DAT
1997 O

- B-DAT

- B-DAT

-730 B-DAT

- B-DAT

-126 B-DAT

-10 B-DAT

-3311 B-DAT

- B-DAT

-3799 B-DAT

- B-DAT

-26, B-DAT
2017 O

-167, B-DAT
1998 O

- B-DAT

- B-DAT

-297, B-DAT
1995 O

-156 B-DAT

-32, B-DAT
2001 O

-1874 B-DAT

- B-DAT

- B-DAT

- B-DAT

-333 B-DAT

-423 B-DAT

-1692 B-DAT

- B-DAT

-192, B-DAT
2015 O

-515 B-DAT

-424 B-DAT

-156 B-DAT

-1232, B-DAT
2001 O

- B-DAT

-157 B-DAT

- B-DAT

-2873, B-DAT
2010 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-568 B-DAT

- B-DAT
resolution," O
IEEE O
Transactions O
on O
Image O

-861, B-DAT
2015 O

- B-DAT

-4 B-DAT

- B-DAT

-26, B-DAT
2017 O

- B-DAT

-595 B-DAT

- B-DAT
Resolution," O
IEEE O
Transactions O
on O
Multimedia O

-417, B-DAT
2016 O

- B-DAT

-199 B-DAT

- B-DAT

-307, B-DAT
2016 O

- B-DAT

-1654 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
1527, O
2001 O

- B-DAT

- B-DAT

-896, B-DAT
2008 O

- B-DAT

-2238, B-DAT
2006 O

- B-DAT

-013011 B-DAT

-20, B-DAT
2010 O

- B-DAT

- B-DAT

-2072 B-DAT

- B-DAT

- B-DAT

-2357 B-DAT

- B-DAT

- B-DAT

- B-DAT

-180 B-DAT

-2774, B-DAT
2013 O

-1728, B-DAT
2012 O

- B-DAT

- B-DAT

- B-DAT
Based O
Super-Resolution", O
IEEE O
Transactions O
on O

- B-DAT

- B-DAT

- B-DAT
2929, O
2013 O

-352, B-DAT
November O
2009 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Set O
5, O
Set14, B-DAT
and O
B100 O
[20], O
and O
compare O

Set14 B-DAT

factors. O
For O
the O
Set5 O
and O
Set14 B-DAT
datasets, O
with O
different O
magnification O
factors O

Bischof, O
"Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests," O
in O
Proceedings O

factors. O
For O
the O
Set5 O
and O
Set14 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

- B-DAT

-2 B-DAT

-3 B-DAT

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT
and O
Table O

- B-DAT

- B-DAT

-2 B-DAT
and O
Table-3 O

-1 B-DAT
3 O
2.449±0.029 O
2.236±0.015 O
2.206±0.027 O
(10 O

-3 B-DAT
3 O
6.264±0.042 O
5.952±0.323 O
4.622±0.299 O
(26 O

-2 B-DAT
3 O
6.501±0.199 O
6.308±0.330 O
6.272±0.332 O
(04 O

-2 B-DAT
3 O
6.889±0.199 O
5.196±0.127 O
4.864±0.267 O
(29 O

-2 B-DAT
3 O
3.418±0.171 O
3.377±0.164 O
2.969±0.120 O
(13 O

-1 B-DAT
3 O
1.026±0.158 O
0.391±0.007 O
0.293±0.004 O
(71 O

-2 B-DAT
3 O
6.527±0.203 O
6.520±0.188 O
6.285±0.101 O
(04 O

-2 B-DAT

- B-DAT

-4 B-DAT

-3 B-DAT

-2 B-DAT

-1 B-DAT

-1 B-DAT

-3 B-DAT

- B-DAT

-2, B-DAT
the O
number O
hyperplane(s) O
#ℋ O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
JMPF O
JMPF O

-4 B-DAT

- B-DAT

- B-DAT

- B-DAT

-4 B-DAT
and O
Tables-5, O
where O
JMPF O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-100 B-DAT
dataset O
[22] O
are O
used, O
so O

- B-DAT
JMPF O
JMPF O

- B-DAT
JMPF O
JMPF O

- B-DAT
JMPF O
JMPF O

-5 B-DAT

- B-DAT

- B-DAT

- B-DAT

-4 B-DAT
tabulates O
the O
performances, O
in O
terms O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-4 B-DAT

-5 B-DAT
provides O
more O
details O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
2929, O
2013 O

- B-DAT

- B-DAT

-1927 B-DAT

-1588, B-DAT
1997 O

- B-DAT

- B-DAT

-730 B-DAT

- B-DAT

-126 B-DAT

-10 B-DAT

-3311 B-DAT

- B-DAT

-3799 B-DAT

- B-DAT

-918 B-DAT

-167, B-DAT
1998 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-023008, B-DAT
2017 O

-156 B-DAT

-32, B-DAT
2001 O

-1874 B-DAT

- B-DAT

- B-DAT

-1873 B-DAT

- B-DAT

- B-DAT

-333 B-DAT

-423 B-DAT

-1692 B-DAT

- B-DAT

-26, B-DAT
2017 O

-515 B-DAT

-424 B-DAT

-156 B-DAT

-1232, B-DAT
2001 O

- B-DAT

-157 B-DAT

- B-DAT

-2873, B-DAT
2010 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-568 B-DAT

- B-DAT
resolution," O
IEEE O
Transactions O
on O
Image O

-861, B-DAT
2015 O

- B-DAT

-4 B-DAT

- B-DAT

-192, B-DAT
2015 O

- B-DAT

-595 B-DAT

- B-DAT
Resolution," O
IEEE O
Transactions O
on O
Multimedia O

-417, B-DAT
2016 O

- B-DAT

-199 B-DAT

- B-DAT

-407 B-DAT

- B-DAT

-1654 B-DAT

- B-DAT

- B-DAT

- B-DAT

-3245, B-DAT
2015 O

- B-DAT

- B-DAT

-333 B-DAT

- B-DAT

- B-DAT

-104 B-DAT

- B-DAT

- B-DAT

-297, B-DAT
1995 O

- B-DAT

- B-DAT
1527, O
2001 O

- B-DAT

- B-DAT

-896, B-DAT
2008 O

- B-DAT

-2238, B-DAT
2006 O

- B-DAT

-013011 B-DAT

-20, B-DAT
2010 O

-1874, B-DAT
2008 O

-954 B-DAT

- B-DAT

-294, B-DAT
2014 O

common O
datasets O
Set5 O
[3] O
and O
Set14 B-DAT
[34] O
for O
different O
upsampling O
scales O

Set5 O
[3] O
Set14 B-DAT
[34] O
Method O
scale: O
×2 O
×3 O

- B-DAT

- B-DAT
smoothed O
version O
of O
the O
natural O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
Kernel-Blind O
Deconvolution. O
Kernel-blind O
deconvolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
or O
kernel-blind O
deconvo- O
lution. O
These O

- B-DAT
or O
kernel-blind O
case, O
but O
not O

- B-DAT

- B-DAT
smoothed O
probability O
distribution O
of O
natural O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-6 B-DAT
[28]* O
32.36 O
26.34 O
21.43 O
17.33 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
Kernel-Blind O
Deblurring O
(NA+KE). O
Gradient O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
kernel-) O
blind O
deblurring O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
Kernel-Blind O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
We O
evaluate O
our O
method O

- B-DAT

-3 B-DAT
[37] O
have O
a O
single O
model O

- B-DAT

-3 B-DAT
[37] O
35.20 O
31.58 O
29.30 O
26.30 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-153324 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-7, B-DAT
2012, O
pages O
1–10, O
2012 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work O
to O
solve O
them O
all—solving O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
patterned O
color O
images. O
In O
Acoustics O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
Kernel-Blind O

- B-DAT

benchmarks O
such O
as O
Set5 O
[2], O
Set14 B-DAT
[50] O
and O
BSD100 O
[33 O

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

- B-DAT

Dong2 O
Chen O
Change O
Loy1 O
1CUHK O
- B-DAT
SenseTime O
Joint O
Lab, O
The O
Chinese O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
per, O
we O
show O
that O
it O

- B-DAT
ful O
to O
semantic O
classes. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
the O
same O
loss O
function O

- B-DAT
put O
image O
of O
arbitrary O
size O

- B-DAT

- B-DAT
work O
equipped O
with O
SFT O
can O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions O
exist O
for O
any O
given O

- B-DAT

- B-DAT
ping O
functions O
from O
external O
low O

- B-DAT
and O
high-resolution O
ex- O
emplar O
pairs O

- B-DAT
ifold, O
new O
losses O
are O
proposed O

- B-DAT

- B-DAT

- B-DAT
resolution O
model O
in O
a O
feature O

- B-DAT
resolution O
images O
look O
very O
similar O

- B-DAT
tions O
the O
overall O
visual O
quality O

- B-DAT
cantly O
improved O

- B-DAT
ent O
class O
is O
non-trivial. O
The O

- B-DAT
ceptual O
and O
adversarial O
losses O
(without O

- B-DAT
ine O
closely, O
these O
details O
are O

- B-DAT
tion, O
existing O
methods O
struggle O
in O

- B-DAT
ing, O
plant), O
is O
crucial O
for O

- B-DAT
ical O
prior O
using O
the O
same O

- B-DAT
ing O
pairs O
using O
two O
different O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ously O
documented O
by O
Timofte O
et O

- B-DAT
ized O
models O
for O
each O
semantic O

- B-DAT

- B-DAT

- B-DAT
age O
super-resolution O
with O
CNN. O
This O

- B-DAT
ing O
especially O
when O
multiple O
segments O

- B-DAT

- B-DAT
ther O
incorporated O
into O
the O
reconstruction O

- B-DAT
tic O
segmentation O
maps O
as O
the O

- B-DAT
periments O
embrace O
this O
choice O
and O

- B-DAT

- B-DAT

- B-DAT
cient. O
Combining O
LR O
images O
with O

- B-DAT
puts, O
or O
concatenating O
segmentation O
maps O

- B-DAT
tial O
Feature O
Transform O
(SFT) O
that O

- B-DAT
ically, O
an O
SFT O
layer O
is O

- B-DAT
tion O
probability O
maps, O
based O
on O

- B-DAT
tially O
on O
feature O
maps O
of O

- B-DAT

- B-DAT

- B-DAT
struction O
of O
an O
HR O
image O

- B-DAT
forming O
the O
intermediate O
features O
of O

- B-DAT

- B-DAT

- B-DAT
tiveness O
of O
our O
approach, O
named O

- B-DAT

- B-DAT
vided O
in O
Sec. O
4 O

- B-DAT

- B-DAT
duced O
prior O
information O
to O
help O

- B-DAT

- B-DAT
formance. O
Dong O
et O
al. O
[10 O

- B-DAT
ies O
to O
better O
recover O
local O

- B-DAT
tion O
framework. O
Sun O
et O
al O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
[49, O
50, O
45, O
46 O

- B-DAT
stantiation O
of O
learning-based O
methods, O
Dong O

- B-DAT
pose O
SRCNN O
for O
learning O
the O

- B-DAT
ages O
in O
an O
end-to-end O
manner O

- B-DAT
mid O
structure O
[26], O
residual O
blocks O

- B-DAT
ing O
[23, O
43], O
and O
densely O

- B-DAT
scale O
guidance O
structure O
has O
also O

- B-DAT

- B-DAT

- B-DAT
age O
of O
many O
plausible O
solutions O

- B-DAT

- B-DAT
velop O
a O
similar O
approach O
and O

- B-DAT
ture O
matching O
loss, O
partly O
reducing O

- B-DAT
facts. O
We O
use O
the O
same O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
egorical O
classes O
to O
co-exist O
in O

- B-DAT
fective O
layer O
that O
enables O
an O

- B-DAT
malizing O
feature O
statistics O
[19]. O
Conditional O

- B-DAT
place O
parameters O
for O
feature-wise O
affine O

- B-DAT
ing O
[6] O
and O
visual O
reasoning O

- B-DAT

- B-DAT
mation O
(e.g., O
semantic O
segmentation O
maps O

- B-DAT
cepts O
a O
single O
linguistic O
input O

- B-DAT

- B-DAT
quire O
adaptive O
processing O
at O
different O

- B-DAT
posed O
SFT O
layer O
addresses O
this O

- B-DAT

- B-DAT

- B-DAT
ceptual O
factors O
in O
neural O
style O

- B-DAT

- B-DAT
tion O
instead O
of O
simple O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ilar O
as O
possible O
to O
the O

- B-DAT
based O
methods O
use O
feed-forward O
networks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
structed O
images. O
However, O
the O
generated O

- B-DAT
ing O
which O
region O
belongs O
to O

- B-DAT
eficial O
for O
generating O
richer O
and O

- B-DAT
mantic O
segmentation O
probability O
maps O
P O

- B-DAT
ors O
in O
SR, O
we O
reformulate O

- B-DAT
ping O
functionM O
that O
outputs O
a O

- B-DAT
rameter O
pair O
adaptively O
influences O
the O

- B-DAT
ture O
maps O
in O
an O
SR O

- B-DAT
tionM O
: O
Ψ O
7→ O
(γ,β O

- B-DAT
cific O
layer O

- B-DAT

- B-DAT
cation, O
i.e., O
Hadamard O
product. O
Since O

- B-DAT

- B-DAT

- B-DAT
ers O
in O
an O
SR O
network O

- B-DAT
timized O
end-to-end O
with O
the O
SR O

- B-DAT
ate O
conditions O
that O
can O
be O

- B-DAT

- B-DAT

- B-DAT
tation O
maps O
obtained O
from O
LR O

- B-DAT
ing O
factor O
of O
×4 O
from O

- B-DAT
tained O
even O
on O
LR O
images O

- B-DAT

- B-DAT
mentation O
model O
[32, O
31]. O
Some O

- B-DAT
responding O
segmentation O
results O
are O
depicted O

- B-DAT
tation O
community. O
During O
testing, O
classes O

- B-DAT

- B-DAT
GAN, O
i.e., O
treating O
all O
classes O

- B-DAT
egorical O
priors O
to O
an O
SR O

- B-DAT
put O
LR O
image O
as O
a O

- B-DAT
catenate O
the O
probability O
maps O
with O

- B-DAT

- B-DAT
sis O
network O
[29]1. O
This O
method O

- B-DAT

- B-DAT

- B-DAT

d O
- B-DAT

- B-DAT
ages. O
Second O
row: O
segmentation O
results O

- B-DAT

- B-DAT
mentation O

- B-DAT

- B-DAT
pose O
the O
LR O
image O
based O

- B-DAT
bining O
the O
output O
of O
each O

- B-DAT

- B-DAT
tively. O
They O
are O
jointly O
trained O

- B-DAT
plying O
affine O
transformation. O
Skip O
connection O

- B-DAT

- B-DAT
volution O
layer. O
The O
upsampling O
operation O

- B-DAT
porary O
models O
such O
as O
DRRN O

- B-DAT

- B-DAT
work O
of O
strided O
convolutions O
to O

- B-DAT
tial O
dimensions. O
The O
full O
architecture O

- B-DAT
vided O
in O
the O
supplementary O
material O

- B-DAT
ture O
maps, O
we O
use O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
courage O
the O
generator O
to O
favor O

- B-DAT
door O
scenes O
since O
their O
textures O

- B-DAT

- B-DAT
ground’ O
category O
is O
used O
to O

- B-DAT
tialized O
the O
SR O
network O
by O

- B-DAT

- B-DAT
ceptual O
loss O
and O
GAN O
loss O

- B-DAT
nel. O
The O
mini-batch O
size O
was O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceptual O
quality. O
Our O
proposed O
SFT-GAN O

- B-DAT
ticular, O
we O
collected O
a O
new O

- B-DAT
ages O
from O
search O
engines O
using O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els O
including O
PSNR-oriented O
methods, O
such O

- B-DAT

- B-DAT
hanceNet O
[38]. O
More O
results O
are O

- B-DAT
mentary O
material. O
For O
SRGAN, O
we O

- B-DAT

- B-DAT

- B-DAT
ing O
sharp O
edges, O
PSNR-oriented O
methods O

- B-DAT

- B-DAT
ate O
monotonous O
and O
unnatural O
textures O

-1 B-DAT
Rank-2 O
Rank-3 O
Rank-4 O

- B-DAT

- B-DAT
forms O
PSNR-oriented O
methods O
by O
a O

- B-DAT
ing O
images. O
To O
better O
compare O

- B-DAT
oriented O
baselines O
and O
GAN-based O
approaches O

- B-DAT

- B-DAT
quested O
to O
rank O
4 O
versions O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
domized O
fashion. O
In O
the O
second O

- B-DAT

- B-DAT

- B-DAT
cilitate O
the O
comparison). O
Each O
pair O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ages O
comparable O
to O
HR O
causing O

- B-DAT
lation O
parameters. O
Our O
method O
modulates O

- B-DAT
vestigate O
the O
relationship O
between O
the O

- B-DAT
ture O
modulation O
parameters, O
as O
depicted O

- B-DAT
tionship O
with O
probability O
maps O
P O

- B-DAT
age O
where O
building O
and O
grass O

- B-DAT

- B-DAT

- B-DAT
formation. O
From O
the O
heat O
map O

- B-DAT
ing O
and O
grass O
textures O
simultaneously O

- B-DAT
biguity, O
the O
probability O
maps O
are O

- B-DAT
formation. O
In O
Fig. O
9, O
the O

- B-DAT
tures O
generated O
by O
SFT-GAN O
become O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentation O
results O
are O
not O
available O

- B-DAT

- B-DAT
mentation O
probability O
maps, O
our O
model O

- B-DAT
GAN O
and O
produces O
comparative O
results O

- B-DAT
itatively O
compare O
with O
several O
alternatives O

- B-DAT
mentation O
probability O
maps O
with O
the O

- B-DAT
ditional O
bias O
at O
the O
input O

- B-DAT
rately O
using O
a O
specific O
model O

- B-DAT
GAN O
yields O
outputs O
that O
are O

- B-DAT
essary O
condition O
for O
class-specific O
texture O

- B-DAT
positional O
mapping O
produces O
good O
results O

- B-DAT
rameter O
efficient O
(×2.5 O
parameters O
as O

- B-DAT
putationally O
inefficient O
as O
we O
need O

- B-DAT
tions O
where O
multiple O
categorical O
classes O

- B-DAT

- B-DAT

Comparison O
with O
other O
conditioning O
methods O
- B-DAT
input O
concatenation, O
compositional O
mapping O
and O

- B-DAT
tic O
to O
spatial O
information. O
For O

- B-DAT
ture O
and O
thus O
noisy O
bricks O

- B-DAT

- B-DAT

- B-DAT
erating O
distinct O
and O
rich O
textures O

- B-DAT
gions O
in O
a O
super-resolved O
image O

- B-DAT

- B-DAT
ally O
pleasing O
textures, O
outperforming O
previous O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sider O
priors O
of O
finer O
categories O

- B-DAT
ward O
challenging O
requirements O
for O
segmentation O

- B-DAT
comings. O
Furthermore, O
segmentation O
and O
SR O

- B-DAT

- B-DAT
Morel. O
Low-complexity O
single-image O
super-resolution O
based O

- B-DAT

- B-DAT

- B-DAT
guage. O
arXiv O
preprint O
arXiv:1707.00683, O
2017 O

- B-DAT

- B-DAT
resolution O
using O
deep O
convolutional O
networks O

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
time O
with O
adaptive O
instance O
normalization O

- B-DAT
resolution O
by O
deep O
multi-scale O
guidance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mization. O
arXiv O
preprint O
arXiv:1412.6980, O
2014 O

- B-DAT
resolution. O
In O
CVPR, O
2017. O
1 O

- B-DAT

- B-DAT

- B-DAT
ative O
adversarial O
network. O
In O
CVPR O

- B-DAT

- B-DAT

- B-DAT
manan, O
P. O
Dollár, O
and O
C O

- B-DAT
mon O
objects O
in O
context. O
In O

- B-DAT
ing O
markov O
random O
field O
for O

- B-DAT
cal O
statistics. O
In O
ICCV, O
2001 O

- B-DAT
ors. O
arXiv O
preprint O
arXiv:1707.03017, O
2017 O

- B-DAT
ditioning O
layer. O
arXiv O
preprint O
arXiv:1709.07871 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cination O
for O
image O
super-resolution. O
In O

- B-DAT

- B-DAT
borhood O
regression O
for O
fast O
example-based O

- B-DAT

- B-DAT

- B-DAT
resolution: O
When O
and O
where O
is O

- B-DAT

- B-DAT
resolution O
as O
sparse O
representation O
of O

- B-DAT

- B-DAT

- B-DAT
ralba. O
Scene O
parsing O
through O
ADE20K O

bench- O
mark O
datasets O
Set5 O
[3], O
Set14 B-DAT
[69] O
and O
BSD100, O
the O
testing O

of O
each O
image O
on O
Set5, O
Set14 B-DAT
and O
BSD100: O
nearest O
neighbor O
(NN O

adversarial O
networks O
on O
Set5 O
and O
Set14 B-DAT
benchmark O
data. O
MOS O
score O
significantly O

Set14 B-DAT
PSNR O
28.49 O
27.19 O
26.92 O
26.44 O

SRGAN O
and O
SRResNet O
variants O
on O
Set14 B-DAT
in O
terms O
of O
MOS. O
We O

Set14 B-DAT
PSNR O
24.64 O
25.99 O
27.18 O
27.45 O

MOS O
tests O
conducted O
on O
Set5, O
Set14, B-DAT
BSD100 O
are O
summarized O
in O
Section O

factor O
for O
Set5 O
(Section O
A.4), O
Set14 B-DAT
(Section O
A.5) O
and O
five O
randomly O

reconstructions O
of O
the O
baboon O
from O
Set14 B-DAT
appear O
closer O
to O
the O
reference O

versions O
of O
images O
from O
Set5, O
Set14 B-DAT
and O
BSD100. O
On O
BSD100 O
nine O

each O
rater. O
On O
Set5 O
and O
Set14 B-DAT
the O
raters O
also O
rated O
three O

comparably O
little O
detail, O
ratings O
on O
Set14 B-DAT
and O
especially O
on O
the O
large O

Set5 O
Set14 B-DAT
BSD100 O

of O
MOS O
scores O
on O
Set5, O
Set14, B-DAT
BSD100. O
Mean O
shown O
as O
red O

Set5 O
Set14 B-DAT
BSD100 O

10: O
Average O
rank O
on O
Set5, O
Set14, B-DAT
BSD100 O
by O
averaging O
the O
ranks O

A.5. O
Set14 B-DAT
- O
Visual O
Results O

Figure O
12: O
Results O
for O
Set14 B-DAT
using O
bicubic O
interpolation, O
SRResNet O
and O

Figure O
13: O
Results O
for O
Set14 B-DAT
using O
bicubic O
interpolation O
, O
SRResNet O

Figure O
14: O
Results O
for O
Set14 B-DAT
using O
bicubic O
interpolation, O
SRResNet O
and O

when O
we O
super-resolve O
at O
large O
upscaling B-DAT
factors? O
The O
behavior O
of O
optimization-based O

photo-realistic O
natural O
images O
for O
4× O
upscaling B-DAT
factors. O
To O
achieve O
this, O
we O

guishable O
from O
original O
(right). O
[4× O
upscaling B-DAT

is O
particularly O
pronounced O
for O
high O
upscaling B-DAT
factors, O
for O
which O
texture O
detail O

are O
shown O
in O
brackets. O
[4× O
upscaling B-DAT

super- O
resolved O
with O
a O
4× O
upscaling B-DAT
factor O
is O
shown O
in O
Figure O

the O
network O
to O
learn O
the O
upscaling B-DAT
filters O
directly O
can O
further O
increase O

was O
also O
shown O
that O
learning O
upscaling B-DAT
filters O
is O
beneficial O
in O
terms O

super-resolves O
face O
images O
with O
large O
upscaling B-DAT
factors O
(8×). O
GANs O
were O
also O

for O
image O
SR O
with O
high O
upscaling B-DAT
factors O
(4×) O
as O
measured O
by O

photo-realistic O
SR O
images O
with O
high O
upscaling B-DAT
factors O
(4 O

losses O
in O
that O
category∗. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

HR O
image O
(right: O
i,j). O
[4× O
upscaling B-DAT

SSIM, O
MOS) O
in O
bold. O
[4× O
upscaling B-DAT

that O
SRGAN O
reconstructions O
for O
large O
upscaling B-DAT
factors O
(4×) O
are, O
by O
a O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
IEEE O

and O
SRGAN O
with O
a O
4× O
upscaling B-DAT
factor O
for O
Set5 O
(Section O
A.4 O

low-/high-resolution O
images O
and O
reconstructions O
(4× O
upscaling) B-DAT
obtained O
with O
different O
methods O
(bicubic O

image O
with O
resolution O
64×64 O
with O
upscaling B-DAT
factor O
4×. O
The O
measurements O
are O

for O
another O
100k O
iterations. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

all O
available O
individual O
ratings. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

of O
each O
image O
on O
Set5, O
Set14 B-DAT

adversarial O
networks O
on O
Set5 O
and O
Set14 B-DAT

Set14 B-DAT

SRGAN O
and O
SRResNet O
variants O
on O
Set14 B-DAT

Set14 B-DAT

reconstructions O
of O
the O
baboon O
from O
Set14 B-DAT

versions O
of O
images O
from O
Set5, O
Set14 B-DAT

each O
rater. O
On O
Set5 O
and O
Set14 B-DAT

comparably O
little O
detail, O
ratings O
on O
Set14 B-DAT

Set5 O
Set14 B-DAT

Set5 O
Set14 B-DAT

Figure O
12: O
Results O
for O
Set14 B-DAT

Figure O
13: O
Results O
for O
Set14 B-DAT

Figure O
14: O
Results O
for O
Set14 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks, O
one O
central O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
(SR). O
To O
our O
knowledge O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
guishable O
from O
original O
(right). O
[4 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
inal O
image O
means O
that O
the O

- B-DAT
realistic O
as O
defined O
by O
Ferwerda O

- B-DAT

- B-DAT

- B-DAT
ing O
high-level O
feature O
maps O
of O

- B-DAT

- B-DAT
resolved O
with O
a O
4× O
upscaling O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
image O
informa- O
tion O

- B-DAT

- B-DAT
proaches O
to O
the O
SR O
problem O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
[27], O
trees O
[46] O
or O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-the-art O
SR O
performance. O
Subsequently, O
it O

- B-DAT

- B-DAT

- B-DAT
ciently O
train O
these O
deeper O
network O

- B-DAT
normalization O
[32] O
is O
often O
used O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
results. O
Another O
powerful O
design O

- B-DAT

- B-DAT
connections O
relieve O
the O
network O
architecture O

- B-DAT
tentially O
non-trivial O
to O
represent O
with O

- B-DAT

- B-DAT

- B-DAT
ing O
pixel-wise O
averages O
of O
plausible O

- B-DAT

- B-DAT
ity O
[42, O
33, O
13, O
5 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
perceptually O
more O
convincing O
solutions O

- B-DAT
ure O
2. O
We O
illustrate O
the O

- B-DAT
ure O
3 O
where O
multiple O
potential O

- B-DAT
tion. O
Yu O
and O
Porikli O
[66 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions. O
Similar O
to O
this O
work O

- B-DAT
trained O
VGG O
network O
instead O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity. O
The O
GAN O
procedure O
encourages O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mark O
datasets O
as O
well O
as O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
counterpart O
IHR. O
The O
high-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nels O
as O
in O
the O
VGG O

- B-DAT
ical O
for O
the O
performance O
of O

- B-DAT
tent O
loss O
lSRX O
and O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
solutions O
with O
overly O
smooth O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stead O
of O
log[1−DθD O
(GθG(ILR))] O
[22 O

- B-DAT
mark O
datasets O
Set5 O
[3], O
Set14 O

- B-DAT
and O
high-resolution O
images. O
This O
corresponds O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bor, O
bicubic, O
SRCNN O
[9] O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gral O
score O
from O
1 O
(bad O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG54 O
and O
the O
original O
HR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ResNet O
and O
the O
adversarial O
networks O

- B-DAT
SRGAN- O
Set5 O
MSE O
VGG22 O
MSE O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
MSE-based O
reconstructions, O
to O
those O
competing O

- B-DAT

- B-DAT
formed O
other O
SRGAN O
and O
SRResNet O

- B-DAT

- B-DAT
GAN O
to O
NN, O
bicubic O
interpolation O

- B-DAT

- B-DAT

- B-DAT
art O
methods. O
Quantitative O
results O
are O

- B-DAT
resolved O
with O
SRResNet O
and O
SRGAN O

- B-DAT
realistic O
image O
SR. O
All O
differences O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentary O
material). O
We O
further O
found O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Net O
that O
sets O
a O
new O

- B-DAT
sure. O
We O
have O
highlighted O
some O

- B-DAT

- B-DAT
ial O
loss O
by O
training O
a O

- B-DAT

- B-DAT

- B-DAT
the-art O
reference O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Stevenson. O
Super-Resolution B-DAT
from O
Image O
Sequences O
- O
A O
Review. O
Midwest O
Symposium O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
by O
adaptive O
sparse O
domain O

- B-DAT
ization. O
IEEE O
Transactions O
on O
Image O

- B-DAT

- B-DAT
resolution. O
IEEE O
Computer O
Graphics O
and O

- B-DAT
level O
vision. O
International O
Journal O
of O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

- B-DAT
line O
at O
http://torch.ch/blog/2016/02/04/resnets. O
html. O
2016 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In O
European O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT
tion O
with O
deep O
convolutional O
neural O

- B-DAT

- B-DAT

- B-DAT
mentation O
algorithms O
and O
measuring O
ecological O

- B-DAT

- B-DAT

- B-DAT
sive O
survey. O
In O
Machine O
Vision O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
for O
fast O
example-based O
super-resolution O

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
(ACCV O

- B-DAT

- B-DAT

- B-DAT
Resolution O
via O
Deep O
and O
Shallow O

- B-DAT

- B-DAT

- B-DAT
ence O
on O
Signals, O
Systems O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
derstanding O
Neural O
Networks O
Through O
Deep O

International O
Conference O
on O
Machine O
Learning O
- B-DAT
Deep O
Learning O
Workshop O
2015, O
page O

- B-DAT

- B-DAT
resolution O
by O
retrieving O
web O
images O

- B-DAT
volutional O
networks. O
In O
European O
Conference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG22, O
SRGAN-VGG54) O
described O
in O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
off O
between O
accuracy O
and O
speed O

-100 B-DAT
thousand O
update O
iterations O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

A.4. O
Set5 O
- B-DAT
Visual O
Results O

A.5. O
Set14 O
- B-DAT
Visual O
Results O

A.6. O
BSD100 O
(five O
random O
samples) O
- B-DAT
Visual O
Results O

bench- O
mark O
datasets O
Set5 O
[3], O
Set14 B-DAT
[69] O
and O
BSD100, O
the O
testing O

of O
each O
image O
on O
Set5, O
Set14 B-DAT
and O
BSD100: O
nearest O
neighbor O
(NN O

adversarial O
networks O
on O
Set5 O
and O
Set14 B-DAT
benchmark O
data. O
MOS O
score O
significantly O

Set14 B-DAT
PSNR O
28.49 O
27.19 O
26.92 O
26.44 O

SRGAN O
and O
SRResNet O
variants O
on O
Set14 B-DAT
in O
terms O
of O
MOS. O
We O

Set14 B-DAT
PSNR O
24.64 O
25.99 O
27.18 O
27.45 O

MOS O
tests O
conducted O
on O
Set5, O
Set14, B-DAT
BSD100 O
are O
summarized O
in O
Section O

factor O
for O
Set5 O
(Section O
A.4), O
Set14 B-DAT
(Section O
A.5) O
and O
five O
randomly O

reconstructions O
of O
the O
baboon O
from O
Set14 B-DAT
appear O
closer O
to O
the O
reference O

versions O
of O
images O
from O
Set5, O
Set14 B-DAT
and O
BSD100. O
On O
BSD100 O
nine O

each O
rater. O
On O
Set5 O
and O
Set14 B-DAT
the O
raters O
also O
rated O
three O

comparably O
little O
detail, O
ratings O
on O
Set14 B-DAT
and O
especially O
on O
the O
large O

Set5 O
Set14 B-DAT
BSD100 O

of O
MOS O
scores O
on O
Set5, O
Set14, B-DAT
BSD100. O
Mean O
shown O
as O
red O

Set5 O
Set14 B-DAT
BSD100 O

10: O
Average O
rank O
on O
Set5, O
Set14, B-DAT
BSD100 O
by O
averaging O
the O
ranks O

A.5. O
Set14 B-DAT
- O
Visual O
Results O

Figure O
12: O
Results O
for O
Set14 B-DAT
using O
bicubic O
interpolation, O
SRResNet O
and O

Figure O
13: O
Results O
for O
Set14 B-DAT
using O
bicubic O
interpolation O
, O
SRResNet O

Figure O
14: O
Results O
for O
Set14 B-DAT
using O
bicubic O
interpolation, O
SRResNet O
and O

when O
we O
super-resolve O
at O
large O
upscaling B-DAT
factors? O
The O
behavior O
of O
optimization-based O

photo-realistic O
natural O
images O
for O
4× O
upscaling B-DAT
factors. O
To O
achieve O
this, O
we O

guishable O
from O
original O
(right). O
[4× O
upscaling B-DAT

is O
particularly O
pronounced O
for O
high O
upscaling B-DAT
factors, O
for O
which O
texture O
detail O

are O
shown O
in O
brackets. O
[4× O
upscaling B-DAT

super- O
resolved O
with O
a O
4× O
upscaling B-DAT
factor O
is O
shown O
in O
Figure O

the O
network O
to O
learn O
the O
upscaling B-DAT
filters O
directly O
can O
further O
increase O

was O
also O
shown O
that O
learning O
upscaling B-DAT
filters O
is O
beneficial O
in O
terms O

super-resolves O
face O
images O
with O
large O
upscaling B-DAT
factors O
(8×). O
GANs O
were O
also O

for O
image O
SR O
with O
high O
upscaling B-DAT
factors O
(4×) O
as O
measured O
by O

photo-realistic O
SR O
images O
with O
high O
upscaling B-DAT
factors O
(4 O

losses O
in O
that O
category∗. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

HR O
image O
(right: O
i,j). O
[4× O
upscaling B-DAT

SSIM, O
MOS) O
in O
bold. O
[4× O
upscaling B-DAT

that O
SRGAN O
reconstructions O
for O
large O
upscaling B-DAT
factors O
(4×) O
are, O
by O
a O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
IEEE O

and O
SRGAN O
with O
a O
4× O
upscaling B-DAT
factor O
for O
Set5 O
(Section O
A.4 O

low-/high-resolution O
images O
and O
reconstructions O
(4× O
upscaling) B-DAT
obtained O
with O
different O
methods O
(bicubic O

image O
with O
resolution O
64×64 O
with O
upscaling B-DAT
factor O
4×. O
The O
measurements O
are O

for O
another O
100k O
iterations. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

all O
available O
individual O
ratings. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

of O
each O
image O
on O
Set5, O
Set14 B-DAT

adversarial O
networks O
on O
Set5 O
and O
Set14 B-DAT

Set14 B-DAT

SRGAN O
and O
SRResNet O
variants O
on O
Set14 B-DAT

Set14 B-DAT

reconstructions O
of O
the O
baboon O
from O
Set14 B-DAT

versions O
of O
images O
from O
Set5, O
Set14 B-DAT

each O
rater. O
On O
Set5 O
and O
Set14 B-DAT

comparably O
little O
detail, O
ratings O
on O
Set14 B-DAT

Set5 O
Set14 B-DAT

Set5 O
Set14 B-DAT

Figure O
12: O
Results O
for O
Set14 B-DAT

Figure O
13: O
Results O
for O
Set14 B-DAT

Figure O
14: O
Results O
for O
Set14 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks, O
one O
central O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
(SR). O
To O
our O
knowledge O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
guishable O
from O
original O
(right). O
[4 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
inal O
image O
means O
that O
the O

- B-DAT
realistic O
as O
defined O
by O
Ferwerda O

- B-DAT

- B-DAT

- B-DAT
ing O
high-level O
feature O
maps O
of O

- B-DAT

- B-DAT
resolved O
with O
a O
4× O
upscaling O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
image O
informa- O
tion O

- B-DAT

- B-DAT
proaches O
to O
the O
SR O
problem O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
[27], O
trees O
[46] O
or O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-the-art O
SR O
performance. O
Subsequently, O
it O

- B-DAT

- B-DAT

- B-DAT
ciently O
train O
these O
deeper O
network O

- B-DAT
normalization O
[32] O
is O
often O
used O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
results. O
Another O
powerful O
design O

- B-DAT

- B-DAT
connections O
relieve O
the O
network O
architecture O

- B-DAT
tentially O
non-trivial O
to O
represent O
with O

- B-DAT

- B-DAT

- B-DAT
ing O
pixel-wise O
averages O
of O
plausible O

- B-DAT

- B-DAT
ity O
[42, O
33, O
13, O
5 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
perceptually O
more O
convincing O
solutions O

- B-DAT
ure O
2. O
We O
illustrate O
the O

- B-DAT
ure O
3 O
where O
multiple O
potential O

- B-DAT
tion. O
Yu O
and O
Porikli O
[66 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions. O
Similar O
to O
this O
work O

- B-DAT
trained O
VGG O
network O
instead O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity. O
The O
GAN O
procedure O
encourages O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mark O
datasets O
as O
well O
as O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
counterpart O
IHR. O
The O
high-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nels O
as O
in O
the O
VGG O

- B-DAT
ical O
for O
the O
performance O
of O

- B-DAT
tent O
loss O
lSRX O
and O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
solutions O
with O
overly O
smooth O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stead O
of O
log[1−DθD O
(GθG(ILR))] O
[22 O

- B-DAT
mark O
datasets O
Set5 O
[3], O
Set14 O

- B-DAT
and O
high-resolution O
images. O
This O
corresponds O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bor, O
bicubic, O
SRCNN O
[9] O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gral O
score O
from O
1 O
(bad O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG54 O
and O
the O
original O
HR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ResNet O
and O
the O
adversarial O
networks O

- B-DAT
SRGAN- O
Set5 O
MSE O
VGG22 O
MSE O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
MSE-based O
reconstructions, O
to O
those O
competing O

- B-DAT

- B-DAT
formed O
other O
SRGAN O
and O
SRResNet O

- B-DAT

- B-DAT
GAN O
to O
NN, O
bicubic O
interpolation O

- B-DAT

- B-DAT

- B-DAT
art O
methods. O
Quantitative O
results O
are O

- B-DAT
resolved O
with O
SRResNet O
and O
SRGAN O

- B-DAT
realistic O
image O
SR. O
All O
differences O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentary O
material). O
We O
further O
found O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Net O
that O
sets O
a O
new O

- B-DAT
sure. O
We O
have O
highlighted O
some O

- B-DAT

- B-DAT
ial O
loss O
by O
training O
a O

- B-DAT

- B-DAT

- B-DAT
the-art O
reference O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Stevenson. O
Super-Resolution B-DAT
from O
Image O
Sequences O
- O
A O
Review. O
Midwest O
Symposium O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
by O
adaptive O
sparse O
domain O

- B-DAT
ization. O
IEEE O
Transactions O
on O
Image O

- B-DAT

- B-DAT
resolution. O
IEEE O
Computer O
Graphics O
and O

- B-DAT
level O
vision. O
International O
Journal O
of O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

- B-DAT
line O
at O
http://torch.ch/blog/2016/02/04/resnets. O
html. O
2016 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In O
European O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT
tion O
with O
deep O
convolutional O
neural O

- B-DAT

- B-DAT

- B-DAT
mentation O
algorithms O
and O
measuring O
ecological O

- B-DAT

- B-DAT

- B-DAT
sive O
survey. O
In O
Machine O
Vision O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
for O
fast O
example-based O
super-resolution O

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
(ACCV O

- B-DAT

- B-DAT

- B-DAT
Resolution O
via O
Deep O
and O
Shallow O

- B-DAT

- B-DAT

- B-DAT
ence O
on O
Signals, O
Systems O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
derstanding O
Neural O
Networks O
Through O
Deep O

International O
Conference O
on O
Machine O
Learning O
- B-DAT
Deep O
Learning O
Workshop O
2015, O
page O

- B-DAT

- B-DAT
resolution O
by O
retrieving O
web O
images O

- B-DAT
volutional O
networks. O
In O
European O
Conference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG22, O
SRGAN-VGG54) O
described O
in O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
off O
between O
accuracy O
and O
speed O

-100 B-DAT
thousand O
update O
iterations O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

A.4. O
Set5 O
- B-DAT
Visual O
Results O

A.5. O
Set14 O
- B-DAT
Visual O
Results O

A.6. O
BSD100 O
(five O
random O
samples) O
- B-DAT
Visual O
Results O

Dilated O
Conv O
+ O
Residual O
Blocks O
4x B-DAT
NN O

Set5, O
Set14, B-DAT
BSDS100, O
Urban100 O
[18]. O
Standard O
SISR O

Set14 B-DAT
30.24 O
30.45 O
34.02 O
24.86 O
29.24 O

Set14 B-DAT
25.99 O
25.77 O
28.94 O
21.56 O
25.19 O

Set14 B-DAT
22.37 O
- O
- O
18.47 O
21.44 O

Set14 B-DAT
0.869 O
0.862 O
0.920 O
0.930 O
0.954 O

Set14 B-DAT
0.703 O
0.678 O
0.790 O
0.832 O
0.894 O

Set14 B-DAT
0.552 O
- O
- O
0.708 O
0.793 O

and×8 O
factor O
SISR O
over O
Set5, O
Set14, B-DAT
BSD100, O
and O
Celeb-HQ O
datasets O
with O

SISR O
models O
over O
datasets O
Set5, O
Set14, B-DAT
BSD100, O
and O
Celeb-HQ. O
Statistics O
for O

Set14 B-DAT
30 I-DAT

Set14 B-DAT
25 I-DAT

Set14 B-DAT
22 I-DAT

Set14 B-DAT
0 I-DAT

Set14 B-DAT
0 I-DAT

Set14 B-DAT
0 I-DAT

- B-DAT

- B-DAT

- B-DAT
ous O
demand O
for O
higher-resolution O
images O

- B-DAT

- B-DAT
resolution O
(SISR). O
The O
SISR O
problem O

- B-DAT

- B-DAT

- B-DAT
fectiveness O
for O
different O
scale O
factors O

- B-DAT
pared O
to O
basic O
interpolation O
schemes O

- B-DAT
pared O
with O
current O
state-of-the-art O
techniques O

- B-DAT
construction O
improves O
the O
quality O
of O

- B-DAT

- B-DAT

- B-DAT
resolution O
(HR) O
image O
from O
one O

- B-DAT
resolution O
(LR) O
images. O
SR O
plays O

- B-DAT

- B-DAT

- B-DAT
tions, O
only O
a O
single O
instance O

- B-DAT

- B-DAT

- B-DAT
posed O
inverse O
problem O
[6] O
that O

- B-DAT
formation O
to O
restrict O
the O
solution O

- B-DAT
nique O
introduced O
by O
Nazeri O
et O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
creasing O
the O
resolution O
of O
a O

- B-DAT
ery O
of O
pixel O
intensities O
in O

- B-DAT
els. O
The O
missing O
pixel O
intensities O

- B-DAT
ing O
regions O
of O
an O
image O

- B-DAT
ing O
task O
is O
modelled O
as O

- B-DAT
age. O
The O
pipeline O
involves O
first O

- B-DAT
struction O
of O
the O
HR O
image O

- B-DAT

- B-DAT
sampled O
by O
a O
factor O
of O

- B-DAT

- B-DAT
tinguished O
anymore O
as O
the O
problem O

- B-DAT

- B-DAT
construction O
of O
a O
high-resolution O
image O

- B-DAT

- B-DAT
ments O
of O
information O
using O
bilinear O

- B-DAT
tinctive O
features O
in O
the O
original O

- B-DAT

- B-DAT
pling O
by O
a O
factor O
of O

- B-DAT
tra O
empty O
row O
and O
column O

- B-DAT
polation O
and O
bicubic O
interpolation O
[3 O

- B-DAT
pling O
[5]. O
Edge-based O
methods O
learn O

- B-DAT
file O
[39] O
to O
reconstruct O
the O

- B-DAT
tion O
[36] O
to O
predict O
HR O

- B-DAT

- B-DAT
age O
itself O
[19, O
10] O
to O

- B-DAT

- B-DAT
works O
(CNN) O
with O
a O
per-pixel O

- B-DAT

- B-DAT
cently O
Johnson O
et O
al. O
[21 O

- B-DAT

- B-DAT
ing O
a O
perceptual O
loss. O
In O

- B-DAT

- B-DAT

- B-DAT
tion O
loss O
and O
Style O
reconstruction O

- B-DAT
of-the-art O
results O
on O
SISR O
for O

- B-DAT

- B-DAT
ducing O
realistically O
synthesized O
high-frequency O
textures O

- B-DAT

- B-DAT

- B-DAT
work O
to O
image O
super-resolution O
tasks O

- B-DAT
neously O
improves O
structure, O
texture, O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
couples O
SISR O
into O
two O
separate O

- B-DAT
tor O
for O
the O
edge O
enhancement O

- B-DAT
tures O
to O
the O
method O
proposed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
criminator O
follows O
the O
architecture O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
images. O
Their O
corresponding O
edge O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
maps O
in O
the O
intermediate O

- B-DAT
tor O
to O
produce O
results O
with O

- B-DAT
criminator. O
Spectral O
normalization O
(SN) O
[28 O

- B-DAT
bilizes O
training O
by O
scaling O
down O

- B-DAT
ter O
and O
gradient O
values. O
We O

- B-DAT
ments O

- B-DAT
ally O
strided O
convolution O
kernel. O
This O

- B-DAT

- B-DAT

- B-DAT
19 O
trained O
on O
the O
ImageNet O

-19 B-DAT

- B-DAT
mediate O
feature O
maps. O
The O
Gram O

- B-DAT
fully O
mitigate O
the O
“checkerboard” O
artifact O

- B-DAT
pose O
convolutions O
[31]. O
For O
both O

-19 B-DAT

- B-DAT
imize O
the O
reconstruction, O
style, O
perceptual O

- B-DAT

- B-DAT

- B-DAT
tector O
[1]. O
We O
can O
control O

- B-DAT
rameter O
σ. O
For O
our O
purposes O

- B-DAT
els O
of O
both O
stages O
were O

- B-DAT
tinue O
training O
until O
convergence. O
We O

- B-DAT
licly O
available O
datasets O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ization O
purposes, O
the O
LR O
image O

- B-DAT
neighbor O
interpolation. O
All O
HR O
images O

- B-DAT
pared O
against O
bicubic O
interpolation O
and O

- B-DAT
duces O
blurry O
results O
around O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Celeb-HQ B-DAT
33.25 O
- O
- O
31.33 O
32.12 O

Celeb-HQ B-DAT
29.59 O
- O
- O
27.94 O
28.23 O

Set5 O
23.80 O
- B-DAT
- O
19.32 O
23.73 O

Set14 O
22.37 O
- B-DAT
- O
18.47 O
21.44 O

BSD100 O
22.11 O
- B-DAT
- O
18.65 O
21.63 O

Celeb-HQ B-DAT
26.66 O
- O
- O
25.46 O
25.56 O

Celeb-HQ B-DAT
0.967 O
- O
- O
0.957 O
0.968 O

Celeb-HQ B-DAT
0.834 O
- O
- O
0.910 O
0.912 O

Set5 O
0.646 O
- B-DAT
- O
0.801 O
0.904 O

Set14 O
0.552 O
- B-DAT
- O
0.708 O
0.793 O

BSD100 O
0.532 O
- B-DAT
- O
0.663 O
0752 O

Celeb-HQ B-DAT
0.782 O
- O
- O
0.841 O
0.857 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Resolution O
task. O
We O
measure O
precision O

- B-DAT
ous O
scale O
factors O
of O
SISR O

- B-DAT
tion O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

eb O
- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
efit O
of O
this O
approach O
over O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
posed O
model O
learns O
to O
fill O

- B-DAT

- B-DAT
ent O
scales O
of O
SISR. O
Quantitative O

- B-DAT
ness O
of O
the O
structure-guided O
inpainting O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dard O
benchmarks O

- B-DAT

- B-DAT
ing O
algorithms. O
A O
better O
approach O

- B-DAT
age O
contents O
and O
structures O
and O

- B-DAT

- B-DAT
resolution O
process. O
Our O
source O
code O

- B-DAT

- B-DAT

- B-DAT
ration O
with O
the O
donation O
of O

- B-DAT

- B-DAT

- B-DAT
resolution O
through O
neighbor O
embedding. O
In O

- B-DAT
ings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT
ing O
a O
deep O
convolutional O
network O

- B-DAT
resolution. O
In O
European O
conference O
on O

- B-DAT
sion, O
pages O
184–199. O
Springer, O
2014 O

- B-DAT
sions. O
Journal O
of O
applied O
meteorology O

- B-DAT

- B-DAT
nition, O
pages O
117–130. O
Springer, O
2007 O

- B-DAT
vances O
and O
challenges O
in O
super-resolution O

- B-DAT
tional O
Journal O
of O
Imaging O
Systems O

- B-DAT
far. O
Fast O
and O
robust O
multiframe O

- B-DAT
tics. O
ACM O
transactions O
on O
graphics O

- B-DAT
ing O
from O
local O
self-examples. O
ACM O

- B-DAT

- B-DAT

- B-DAT
thesis O
using O
convolutional O
neural O
networks O

- B-DAT
vances O
in O
Neural O
Information O
Processing O

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT
lenge O
on O
Perceptual O
Image O
Restoration O

- B-DAT
ulation O
(PIRM) O
at O
the O
15th O

- B-DAT

- B-DAT

- B-DAT
gio. O
Generative O
adversarial O
nets. O
In O

- B-DAT
ral O
information O
processing O
systems, O
pages O

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
IEEE O
conference O

- B-DAT
ual O
learning O
for O
image O
recognition O

- B-DAT
age O
super-resolution O
from O
transformed O
self-exemplars O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision, O
Kyoto O

- B-DAT
to-image O
translation O
with O
conditional O
adversarial O

- B-DAT
works. O
In O
Proceedings O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sive O
growing O
of O
GANs O
for O

- B-DAT
ing O
Representations, O
2018. O
4 O

- B-DAT

- B-DAT
ningham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT
sion O
and O
Pattern O
Recognition O
(CVPR O

- B-DAT
hanced O
deep O
residual O
networks O
for O

- B-DAT
resolution. O
In O
Proceedings O
of O
the O

- B-DAT
works. O
In O
International O
Conference O
on O

- B-DAT
resentations, O
2018. O
3 O

- B-DAT
painting O
with O
adversarial O
edge O
learning O

- B-DAT

- B-DAT
crimination. O
In O
Proceedings O
of O
the O

- B-DAT
ference O
on O
Computer O
Vision O
(ECCV O

- B-DAT
nition O
challenge. O
International O
Journal O
of O

- B-DAT
hancenet: O
Single O
image O
super-resolution O
through O

- B-DAT
tomated O
texture O
synthesis. O
In O
The O

- B-DAT
age/video O
upsampling. O
In O
ACM O
Transactions O

- B-DAT

- B-DAT
gle O
image O
and O
video O
super-resolution O

- B-DAT
cient O
sub-pixel O
convolutional O
neural O
network O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
lutional O
networks O
for O
large-scale O
image O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT
sion O
and O
Pattern O
Recognition O
(CVPR O

- B-DAT

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision, O
pages O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
versarial O
networks. O
In O
The O
IEEE O

- B-DAT
ence O
on O
Computer O
Vision O
(ICCV O

bench- O
mark O
datasets O
Set5 O
[3], O
Set14 B-DAT
[69] O
and O
BSD100, O
the O
testing O

of O
each O
image O
on O
Set5, O
Set14 B-DAT
and O
BSD100: O
nearest O
neighbor O
(NN O

adversarial O
networks O
on O
Set5 O
and O
Set14 B-DAT
benchmark O
data. O
MOS O
score O
significantly O

Set14 B-DAT
PSNR O
28.49 O
27.19 O
26.92 O
26.44 O

SRGAN O
and O
SRResNet O
variants O
on O
Set14 B-DAT
in O
terms O
of O
MOS. O
We O

Set14 B-DAT
PSNR O
24.64 O
25.99 O
27.18 O
27.45 O

MOS O
tests O
conducted O
on O
Set5, O
Set14, B-DAT
BSD100 O
are O
summarized O
in O
Section O

factor O
for O
Set5 O
(Section O
A.4), O
Set14 B-DAT
(Section O
A.5) O
and O
five O
randomly O

reconstructions O
of O
the O
baboon O
from O
Set14 B-DAT
appear O
closer O
to O
the O
reference O

versions O
of O
images O
from O
Set5, O
Set14 B-DAT
and O
BSD100. O
On O
BSD100 O
nine O

each O
rater. O
On O
Set5 O
and O
Set14 B-DAT
the O
raters O
also O
rated O
three O

comparably O
little O
detail, O
ratings O
on O
Set14 B-DAT
and O
especially O
on O
the O
large O

Set5 O
Set14 B-DAT
BSD100 O

of O
MOS O
scores O
on O
Set5, O
Set14, B-DAT
BSD100. O
Mean O
shown O
as O
red O

Set5 O
Set14 B-DAT
BSD100 O

10: O
Average O
rank O
on O
Set5, O
Set14, B-DAT
BSD100 O
by O
averaging O
the O
ranks O

A.5. O
Set14 B-DAT
- O
Visual O
Results O

Figure O
12: O
Results O
for O
Set14 B-DAT
using O
bicubic O
interpolation, O
SRResNet O
and O

Figure O
13: O
Results O
for O
Set14 B-DAT
using O
bicubic O
interpolation O
, O
SRResNet O

Figure O
14: O
Results O
for O
Set14 B-DAT
using O
bicubic O
interpolation, O
SRResNet O
and O

when O
we O
super-resolve O
at O
large O
upscaling B-DAT
factors? O
The O
behavior O
of O
optimization-based O

photo-realistic O
natural O
images O
for O
4× O
upscaling B-DAT
factors. O
To O
achieve O
this, O
we O

guishable O
from O
original O
(right). O
[4× O
upscaling B-DAT

is O
particularly O
pronounced O
for O
high O
upscaling B-DAT
factors, O
for O
which O
texture O
detail O

are O
shown O
in O
brackets. O
[4× O
upscaling B-DAT

super- O
resolved O
with O
a O
4× O
upscaling B-DAT
factor O
is O
shown O
in O
Figure O

the O
network O
to O
learn O
the O
upscaling B-DAT
filters O
directly O
can O
further O
increase O

was O
also O
shown O
that O
learning O
upscaling B-DAT
filters O
is O
beneficial O
in O
terms O

super-resolves O
face O
images O
with O
large O
upscaling B-DAT
factors O
(8×). O
GANs O
were O
also O

for O
image O
SR O
with O
high O
upscaling B-DAT
factors O
(4×) O
as O
measured O
by O

photo-realistic O
SR O
images O
with O
high O
upscaling B-DAT
factors O
(4 O

losses O
in O
that O
category∗. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

HR O
image O
(right: O
i,j). O
[4× O
upscaling B-DAT

SSIM, O
MOS) O
in O
bold. O
[4× O
upscaling B-DAT

that O
SRGAN O
reconstructions O
for O
large O
upscaling B-DAT
factors O
(4×) O
are, O
by O
a O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
IEEE O

and O
SRGAN O
with O
a O
4× O
upscaling B-DAT
factor O
for O
Set5 O
(Section O
A.4 O

low-/high-resolution O
images O
and O
reconstructions O
(4× O
upscaling) B-DAT
obtained O
with O
different O
methods O
(bicubic O

image O
with O
resolution O
64×64 O
with O
upscaling B-DAT
factor O
4×. O
The O
measurements O
are O

for O
another O
100k O
iterations. O
[4× O
upscaling B-DAT

centered O
around O
value O
i. O
[4× O
upscaling B-DAT

all O
available O
individual O
ratings. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

interpolation, O
SRResNet O
and O
SRGAN. O
[4× O
upscaling B-DAT

of O
each O
image O
on O
Set5, O
Set14 B-DAT

adversarial O
networks O
on O
Set5 O
and O
Set14 B-DAT

Set14 B-DAT

SRGAN O
and O
SRResNet O
variants O
on O
Set14 B-DAT

Set14 B-DAT

reconstructions O
of O
the O
baboon O
from O
Set14 B-DAT

versions O
of O
images O
from O
Set5, O
Set14 B-DAT

each O
rater. O
On O
Set5 O
and O
Set14 B-DAT

comparably O
little O
detail, O
ratings O
on O
Set14 B-DAT

Set5 O
Set14 B-DAT

Set5 O
Set14 B-DAT

Figure O
12: O
Results O
for O
Set14 B-DAT

Figure O
13: O
Results O
for O
Set14 B-DAT

Figure O
14: O
Results O
for O
Set14 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks, O
one O
central O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
(SR). O
To O
our O
knowledge O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
guishable O
from O
original O
(right). O
[4 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
inal O
image O
means O
that O
the O

- B-DAT
realistic O
as O
defined O
by O
Ferwerda O

- B-DAT

- B-DAT

- B-DAT
ing O
high-level O
feature O
maps O
of O

- B-DAT

- B-DAT
resolved O
with O
a O
4× O
upscaling O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
image O
informa- O
tion O

- B-DAT

- B-DAT
proaches O
to O
the O
SR O
problem O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
[27], O
trees O
[46] O
or O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-the-art O
SR O
performance. O
Subsequently, O
it O

- B-DAT

- B-DAT

- B-DAT
ciently O
train O
these O
deeper O
network O

- B-DAT
normalization O
[32] O
is O
often O
used O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
results. O
Another O
powerful O
design O

- B-DAT

- B-DAT
connections O
relieve O
the O
network O
architecture O

- B-DAT
tentially O
non-trivial O
to O
represent O
with O

- B-DAT

- B-DAT

- B-DAT
ing O
pixel-wise O
averages O
of O
plausible O

- B-DAT

- B-DAT
ity O
[42, O
33, O
13, O
5 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
perceptually O
more O
convincing O
solutions O

- B-DAT
ure O
2. O
We O
illustrate O
the O

- B-DAT
ure O
3 O
where O
multiple O
potential O

- B-DAT
tion. O
Yu O
and O
Porikli O
[66 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions. O
Similar O
to O
this O
work O

- B-DAT
trained O
VGG O
network O
instead O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity. O
The O
GAN O
procedure O
encourages O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mark O
datasets O
as O
well O
as O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
counterpart O
IHR. O
The O
high-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nels O
as O
in O
the O
VGG O

- B-DAT
ical O
for O
the O
performance O
of O

- B-DAT
tent O
loss O
lSRX O
and O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
solutions O
with O
overly O
smooth O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stead O
of O
log[1−DθD O
(GθG(ILR))] O
[22 O

- B-DAT
mark O
datasets O
Set5 O
[3], O
Set14 O

- B-DAT
and O
high-resolution O
images. O
This O
corresponds O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bor, O
bicubic, O
SRCNN O
[9] O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gral O
score O
from O
1 O
(bad O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG54 O
and O
the O
original O
HR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ResNet O
and O
the O
adversarial O
networks O

- B-DAT
SRGAN- O
Set5 O
MSE O
VGG22 O
MSE O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
MSE-based O
reconstructions, O
to O
those O
competing O

- B-DAT

- B-DAT
formed O
other O
SRGAN O
and O
SRResNet O

- B-DAT

- B-DAT
GAN O
to O
NN, O
bicubic O
interpolation O

- B-DAT

- B-DAT

- B-DAT
art O
methods. O
Quantitative O
results O
are O

- B-DAT
resolved O
with O
SRResNet O
and O
SRGAN O

- B-DAT
realistic O
image O
SR. O
All O
differences O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentary O
material). O
We O
further O
found O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Net O
that O
sets O
a O
new O

- B-DAT
sure. O
We O
have O
highlighted O
some O

- B-DAT

- B-DAT
ial O
loss O
by O
training O
a O

- B-DAT

- B-DAT

- B-DAT
the-art O
reference O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Stevenson. O
Super-Resolution B-DAT
from O
Image O
Sequences O
- O
A O
Review. O
Midwest O
Symposium O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
by O
adaptive O
sparse O
domain O

- B-DAT
ization. O
IEEE O
Transactions O
on O
Image O

- B-DAT

- B-DAT
resolution. O
IEEE O
Computer O
Graphics O
and O

- B-DAT
level O
vision. O
International O
Journal O
of O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

- B-DAT
line O
at O
http://torch.ch/blog/2016/02/04/resnets. O
html. O
2016 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In O
European O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT
tion O
with O
deep O
convolutional O
neural O

- B-DAT

- B-DAT

- B-DAT
mentation O
algorithms O
and O
measuring O
ecological O

- B-DAT

- B-DAT

- B-DAT
sive O
survey. O
In O
Machine O
Vision O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
for O
fast O
example-based O
super-resolution O

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
(ACCV O

- B-DAT

- B-DAT

- B-DAT
Resolution O
via O
Deep O
and O
Shallow O

- B-DAT

- B-DAT

- B-DAT
ence O
on O
Signals, O
Systems O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
derstanding O
Neural O
Networks O
Through O
Deep O

International O
Conference O
on O
Machine O
Learning O
- B-DAT
Deep O
Learning O
Workshop O
2015, O
page O

- B-DAT

- B-DAT
resolution O
by O
retrieving O
web O
images O

- B-DAT
volutional O
networks. O
In O
European O
Conference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
VGG22, O
SRGAN-VGG54) O
described O
in O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
off O
between O
accuracy O
and O
speed O

-100 B-DAT
thousand O
update O
iterations O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

A.4. O
Set5 O
- B-DAT
Visual O
Results O

A.5. O
Set14 O
- B-DAT
Visual O
Results O

A.6. O
BSD100 O
(five O
random O
samples) O
- B-DAT
Visual O
Results O

The O
way O
of O
embedding O
upscaling B-DAT
feature O
in O
the O
last O
few O

BSD100, O
Urban100 O
and O
Manga109, B-DAT
each O
of O
which O
has O
dif O

for O
datasets(e.g., O
Urban100 O
and O
Manga109) B-DAT
with O
rich O
re O

Method O
Set5 O
Set14 O
BSD100 O
Urban100 O
Manga109 B-DAT

Method O
Set5 O
Set14 O
BSD100 O
Urban100 O
Manga109 B-DAT

0.4 O
dB O
on O
Urban100 O
and O
Manga109 B-DAT
datasets O

Method O
Set5 O
Set14 O
BSD100 O
Urban100 O
Manga109 B-DAT
PSNR/ I-DAT
SSIM O
PSNR/ O
SSIM O
PSNR O

Method O
Set5 O
Set14 O
BSD100 O
Urban100 O
Manga109 B-DAT
PSNR/ I-DAT
SSIM O
PSNR/ O
SSIM O
PSNR O

0.4 O
dB O
on O
Urban100 O
and O
Manga109 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
traction O
module, O
which O
consists O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
is O
optimized O
by O
stochastic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
plified O
residual O
blocks O
with O
local-source O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-1 B-DAT
Fg O

- B-DAT

- B-DAT
NL O

- B-DAT
NL O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-1 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sentations. O
Thus, O
we O
set O
α O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
spectively. O
f(·) O
and O
δ(·) O
are O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ture, O
and O
embed O
RL-NL O
modules O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tains O
the O
convolution O
layers O
with O

- B-DAT
ble O
1 O
we O
can O
see O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Set5 O
Set14 O
BSD100 O
Urban100 O
Manga109 B-DAT
PSNR/SSIM O
PSNR/SSIM O
PSNR/SSIM O
PSNR/SSIM O
PSNR/SSIM O

Set5 O
Set14 O
BSD100 O
Urban100 O
Manga109 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
GAN) O
[1] O
is O
a O
seminal O

- B-DAT

- B-DAT
lar, O
we O
introduce O
the O
Residual-in-Residual O

- B-DAT
vide O
stronger O
supervision O
for O
brightness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lem, O
has O
attracted O
increasing O
attention O

- B-DAT
panies. O
SISR O
aims O
at O
recovering O

- B-DAT

- B-DAT

- B-DAT
perous O
development. O
Various O
network O
architecture O

- B-DAT

- B-DAT
Noise O
Ratio O
(PSNR) O
value O
[5,6,7,1,8,9,10,11,12 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uation O
of O
human O
observers O
[1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mize O
super-resolution O
model O
in O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
RGAN, O
consistently O
outperforms O
state-of-the-art O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
struction O
style O
and O
smoothness. O
Another O

- B-DAT
pate O
in O
region O
1 O
and O

- B-DAT

- B-DAT

- B-DAT
tures, O
such O
as O
a O
deeper O

- B-DAT
work O
[9], O
deep O
back O
projection O

- B-DAT
provement. O
Zhang O
et O
al. O
[11 O

- B-DAT
ing O
the O
state-of-the-art O
PSNR O
performance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity O
[29,14], O
perceptual O
loss O
[13 O

- B-DAT
imizing O
the O
error O
in O
a O

- B-DAT
pearance. O
Ledig O
et O
al. O
[1 O

- B-DAT
jadi O
et O
al. O
[16] O
develop O

- B-DAT

- B-DAT
veloping O
more O
effective O
GAN O
frameworks O

- B-DAT
tor O
includes O
gradient O
clipping O
[32 O

- B-DAT
ated O
data O
are O
real, O
but O

- B-DAT
sures, O
e.g., O
PSNR O
and O
SSIM O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lenge O
[3]. O
In O
a O
recent O

- B-DAT
tion, O
we O
first O
describe O
our O

- B-DAT
tion O
is O
done O
in O
the O

- B-DAT
ers; O
2) O
replace O
the O
original O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
putational O
complexity O
in O
different O
PSNR-oriented O

- B-DAT
ing O
dataset O
during O
testing. O
When O

- B-DAT
alization O
ability. O
We O
empirically O
observe O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
residual O
network. O
However, O
our O

- B-DAT
erage O
Discriminator O
RaD O
[2], O
denoted O

- B-DAT

- B-DAT
mulated O
as O
DRa(xr, O
xf O

- B-DAT

- B-DAT

- B-DAT
tures O
before O
activation O
rather O
than O

- B-DAT

-543 B-DAT
layer O
is O
merely O
11.17%. O
The O

- B-DAT

- B-DAT

- B-DAT
tance O
between O
recovered O
image O
G(xi O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nition O
[38], O
which O
focuses O
on O

- B-DAT
ing O
perceptual O
loss O
that O
focuses O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tures O
and O
similarly, O
22 O
represents O

- B-DAT

-22 B-DAT
b) O
activation O
map O
of O

-54 B-DAT

- B-DAT
boon’. O
With O
the O
network O
going O

- B-DAT

- B-DAT
ceptual O
quality, O
we O
propose O
a O

- B-DAT
tion. O
Specifically, O
we O
first O
train O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
parameters O
of O
these O
two O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
polated O
image O
is O
either O
too O

- B-DAT
rameter O
λ O
and O
η O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
oriented O
model O
with O
the O
L1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els O
on O
widely O
used O
benchmark O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
the-art O
PSNR-oriented O
methods O
including O
SRCNN O

- B-DAT

- B-DAT
tures, O
e.g., O
animal O
fur, O
building O

- B-DAT
pleasant O
artifacts, O
e.g., O
artifacts O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults, O
and O
than O
previous O
GAN-based O

- B-DAT
tures O
in O
building O
(see O
image O

- B-DAT

- B-DAT
mance O
without O
artifacts. O
It O
does O

- B-DAT
ment O
can O
be O
observed O
from O

- B-DAT
tures O
before O
activation O
can O
result O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gies O
in O
balancing O
the O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
oriented O
method O
outputs O
cartoon-style O
blurry O

- B-DAT

- B-DAT

- B-DAT
pirically O
make O
some O
modifications O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
terpolation O
between O
the O
results O
of O

- B-DAT

- B-DAT

- B-DAT
ceptual O
quality O
than O
previous O
SR O

- B-DAT

- B-DAT
dition, O
useful O
techniques O
including O
residual O

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In: O
CVPR. O
(2018 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
performance O
on O
imagenet O
classification O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
representations. O
In: O
International O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tics. O
(2010 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
duce O
several O
useful O
techniques O
that O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
a O
very O
deep O
network O

- B-DAT

- B-DAT
Residual O
Dense O
Block O
(RRDB), O
which O

- B-DAT
ers O
[47,28]. O
He O
et O
al O

- B-DAT

- B-DAT
tially. O
It O
is O
worth O
noting O

- B-DAT
tion O
(multiplying O
0.1 O
for O
all O

- B-DAT
tremely O
bad O
local O
minimum O
with O

- B-DAT
tion O
(×0.1) O
helps O
the O
network O

- B-DAT
tion O
achieves O
a O
higher O
PSNR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rithms: O
average O
PSNR/SSIM O
on O
Y O

Bicubic O
- B-DAT
28.42/0.8104 O
26.00/0.7027 O
25.96/0.6675 O
23.14/0.6577 O
24.89/0.7866 O

- B-DAT

- B-DAT
verse O
natural O
textures. O
We O
employ O

- B-DAT
over, O
the O
deeper O
model O
achieves O

- B-DAT

- B-DAT

- B-DAT

B100 O
[21], O
Urban100 O
[11], O
and O
Manga109 B-DAT
[22]. O
We O
generate O
LR O
images O

Manga109 B-DAT
×2 O
30.30/0.9339 O
35.60/0.9663 O
37.22/0.9750 O
37.60/0.9736 O

EDSR/32.01 O
D-DBPN/31.50 O
RDN/31.95 O
SRFBN/32.08‘ParaisoRoad’ O
from O
Manga109 B-DAT
GMFN(Ours)/32.32 O

EDSR/27.03 O
D-DBPN/26.63 O
RDN/27.14 O
SRFBN/27.04‘ToutaMairimasu’ O
from O
Manga109 B-DAT
GMFN(Ours)/27.47 O

EDSR/29.94 O
D-DBPN/29.21 O
RDN/30.27 O
SRFBN/29.84‘UchiNoNyansDiary’ O
from O
Manga109 B-DAT
GMFN(Ours)/30.90 O

EDSR/32.01 O
D-DBPN/31.50 O
RDN/31.95 O
SRFBN/32.08‘ParaisoRoad’ O
from O
Manga109 B-DAT

EDSR/27.03 O
D-DBPN/26.63 O
RDN/27.14 O
SRFBN/27.04‘ToutaMairimasu’ O
from O
Manga109 B-DAT

EDSR/29.94 O
D-DBPN/29.21 O
RDN/30.27 O
SRFBN/29.84‘UchiNoNyansDiary’ O
from O
Manga109 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formation O
flows O
are O
solely O
feedforward O

- B-DAT

- B-DAT
plored. O
In O
this O
paper, O
we O

- B-DAT
rate O
image O
SR, O
in O
which O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tures O
captured O
under O
large O
receptive O

- B-DAT

- B-DAT
ciently O
selects O
and O
further O
enhances O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-the-art O
SR O
methods O
in O
terms O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
image O
SR O
methods O

- B-DAT

- B-DAT
tures O
solely O
flow O
from O
the O

- B-DAT

- B-DAT
tures O
extracted O
from O
the O
top O

- B-DAT

- B-DAT
agating O
high-level O
features O
to O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
features, O
we O
employ O
multiple O

- B-DAT

- B-DAT
tures O
to O
shallow O
layers. O
However O

- B-DAT

- B-DAT

- B-DAT
level O
information O
to O
refine O
low-level O

- B-DAT

- B-DAT

- B-DAT
posed O
GMFN O
shows O
better O
visual O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tensive O
experiments O
demonstrate O
the O
superiority O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ploys O
16 O
RDBs O

- B-DAT
level O
features O
for O
refining O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
tasks O
(e.g. O
classification O
[29 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ent O
direction, O
[12, O
31] O
applied O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
less, O
we O
argue O
that O
such O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ent O
receptive O
fields, O
every O
piece O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
back O
module O
to O
adaptively O
eliminate O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
able O
contextual O
knowledge O
from O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
cost O
was O
quadratically O
saved O

- B-DAT
gether. O
However, O
these O
networks O
require O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pendent O
convolutional O
neural O
network O
which O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
jacent O
time O
steps O
is O
achieved O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
RDB-BRDB O

- B-DAT

-1 B-DAT
RDB-b O
RDB-B O

- B-DAT

- B-DAT

L O
BF O
-, B-DAT

L O
bF O
- B-DAT
, O
t O

- B-DAT

- B-DAT

- B-DAT
tures O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
RDN O
[33], O
the O
number O

- B-DAT

- B-DAT
volutional O
layer. O
Then, O
a O
3×3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lowing O
RDB. O
The O
placement O
of O

- B-DAT
cording O
to O
the O
relative O
hierarchical O

- B-DAT

- B-DAT

- B-DAT
cilitates O
the O
refinement O
processes O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lected O
indexes O
of O
the O
deepest O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ative O
hierarchical O
relationship O
among O
multiple O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
information O
captured O
under O
different O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
network O
is O
set O
to O
C0 O

- B-DAT
ment O
training O
images O
with O
scaling O

- B-DAT
tion O
bicubic. O
The O
SR O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formance O
of O
various O
single-to-multiple O
anti-feedback O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dex O
sets O
SM O
and O
DN O

- B-DAT

- B-DAT

- B-DAT
to-multiple O
feedback O
manner O
[12, O
31 O

- B-DAT
to-single O
feedback O
manners O
perform O
better O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fining O
low-level O
features. O
However, O
excessively O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
features. O
If O
the O
high-level O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trate O
the O
effectiveness O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bine O
various O
M O
to O
achieve O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ther O
hinder O
the O
reconstruction O
ability O

- B-DAT
tively O
selects O
the O
high O
frequency O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tively O
accesses O
to O
high-level O
information O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nections O
by O
setting O
M O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
son O
results2 O
in O
Tab. O
1 O

- B-DAT
struct O
a O
faithful O
SR O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ogy O
Department O
(No.2018GZ0178 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ding. O
In O
BMVC, O
2012 O

- B-DAT
volutional O
network O
for O
image O
super-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
projection O
networks O
for O
super-resolution. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
back O
network O
for O
image O
super-resolution O

- B-DAT

- B-DAT
layer O
recurrent O
connections O
for O
scene O

- B-DAT

- B-DAT

- B-DAT
masaki, O
and O
Kiyoharu O
Aizawa. O
Sketch-based O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
forward O
networks O
(FF O

-1 B-DAT

- B-DAT
ers O
are O
set O
to O
128 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults O
shown O
Figure O
6 O
indicate O

- B-DAT
tively. O
The O
performance O
evaluated O
on O

- B-DAT
served O
that O
with O
the O
help O

- B-DAT
nificantly O
improved O
compared O
with O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods, O
but O
it O
holds O
relatively O

-16, B-DAT
we O
provide O
more O
qualitative O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rately O
restored O
the O
letter O
"M O

- B-DAT

- B-DAT
coveres O
two O
horizontal O
lines O
as O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

the O
original O
LR O
inputs O
and O
upscaling B-DAT
spatial O
reso- O
lution O
at O
the O

upscaling B-DAT
strategy O
has O
been O
demon- O
strated O

upscaling B-DAT
SR O
methods O
(e.g., O
DRRN O
[5 O

shallow O
feature O
extraction O
HSF O
(·), O
upscaling B-DAT
module O
HUP O
(·), O
and O
reconstruction O

upscaling B-DAT
layer, O
whose O
weight O
set O
is O

upscaling, B-DAT
whose O
kernel O
size O
is O
1×1 O

is O
set O
as O
16. O
For O
upscaling B-DAT
module O
HUP O
(·), O
we O
follow O

B100 O
[38], O
Urban100 O
[22], O
and O
Manga109 B-DAT
[39]. O
We O
conduct O
experiments O
with O

Scale O
Set5 O
Set14 O
B100 O
Urban100 O
Manga109 B-DAT

Manga109 B-DAT
(4×): O
YumeiroCooking O

BI O
model O
on O
Urban100 O
and O
Manga109 B-DAT
datasets. O
The O
best O
results O
are O

becomes O
larger. O
For O
Urban100 O
and O
Manga109, B-DAT
the O
PSNR O
gains O
of O
RCAN O

Manga109 B-DAT
(8×): O
TaiyouNiSmash O

BI O
model O
on O
Urban100 O
and O
Manga109 B-DAT
datasets. O
The O
best O
results O
are O

Scale O
Set5 O
Set14 O
B100 O
Urban100 O
Manga109 B-DAT

Scale O
Set5 O
Set14 O
B100 O
Urban100 O
Manga109 B-DAT
PSNR I-DAT
SSIM O
PSNR O
SSIM O
PSNR O

BI O
model O
on O
Urban100 O
and O
Manga109 B-DAT

BI O
model O
on O
Urban100 O
and O
Manga109 B-DAT

Scale O
Set5 O
Set14 O
B100 O
Urban100 O
Manga109 B-DAT
PSNR I-DAT
SSIM O
PSNR O
SSIM O
PSNR O

- B-DAT

- B-DAT
portance O
for O
image O
super-resolution O
(SR O

- B-DAT
resolution O
inputs O
and O
features O
contain O

- B-DAT

- B-DAT
tion, O
which O
is O
treated O
equally O

- B-DAT
resentational O
ability O
of O
CNNs. O
To O

- B-DAT
tions. O
Meanwhile, O
RIR O
allows O
abundant O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
given O
its O
low-resolution O
(LR O

- B-DAT

- B-DAT
tions, O
ranging O
from O
security O
and O

- B-DAT

- B-DAT
merous O
learning O
based O
methods O
have O

- B-DAT
layer O
CNN O
for O
image O
SR O

- B-DAT

- B-DAT
work O
depth O
was O
demonstrated O
to O

- B-DAT
nition O
tasks, O
especially O
when O
He O

- B-DAT

- B-DAT

- B-DAT
wise O
features O
equally, O
which O
lacks O

- B-DAT
formation O
(e.g., O
low- O
and O
high-frequency O

- B-DAT

- B-DAT
sible. O
The O
LR O
images O
contain O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ual O
channel O
attention O
networks O
(RCAN O

- B-DAT

- B-DAT
ture O
to O
construct O
very O
deep O

- B-DAT
tions O
in O
RIR O
help O
to O

- B-DAT

- B-DAT
tional O
ability O
of O
the O
network O

- B-DAT
nity O
[1–11,22]. O
Attention O
mechanism O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vious O
works. O
By O
introducing O
residual O

- B-DAT
icant O
improvement O
in O
accuracy. O
Tai O

- B-DAT
lution O
at O
the O
network O
tail O

- B-DAT

- B-DAT
bines O
automated O
texture O
synthesis O
and O

- B-DAT
gree, O
their O
predicted O
results O
may O

- B-DAT
cant O
improvement. O
However, O
most O
of O

- B-DAT

- B-DAT
inative O
ability O
for O
different O
types O

-1 B-DAT
RG-g O
RG-G O

- B-DAT

-1 B-DAT
RCAB-b O
RCAB-B O

- B-DAT

- B-DAT
fication O
with O
a O
trunk-and-mask O
attention O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tain O
significant O
performance O
improvement O
for O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
posed O
RIR O
achieves O
the O
largest O

- B-DAT

- B-DAT
tively O

- B-DAT

- B-DAT

- B-DAT
strated O
to O
be O
more O
efficient O

- B-DAT

- B-DAT
ial O
losses O
[8, O
21]. O
To O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
frequency O
parts O
seem O
to O
be O

- B-DAT

- B-DAT
quently, O
the O
output O
after O
convolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
by O
global O
average O
pooling O

- B-DAT
wise O
features O
can O
be O
emphasized O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tively. O
WD O
is O
the O
weight O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
Fg,bXg,b O

- B-DAT
hance O
the O
discriminative O
ability O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
table O
performance O
improvements O
over O
previous O

- B-DAT
sions O
about O
the O
effects O
of O

- B-DAT
downscaling O
and O
channel-upscaling, O
whose O
kernel O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ation O
metric, O
and O
training O
settings O

- B-DAT

-1 B-DAT
and O
top-5 O
recognition O
errors) O
comparisons O

- B-DAT

- B-DAT
tion O
(CA) O
based O
on O
the O

- B-DAT
formance. O
It’s O
hard O
to O
obtain O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
CNN O
[2], O
SCN O
[3], O
VDSR O

- B-DAT

- B-DAT
ensemble O
strategy O
to O
further O
improve O

- B-DAT

- B-DAT
parisons O
for O
×2, O
×3, O
×4 O

- B-DAT

- B-DAT

- B-DAT
age O
“img O
004”, O
we O
observe O

- B-DAT
CNN O
cannot O
recover O
lines. O
Other O

- B-DAT
Cooking”, O
the O
cropped O
part O
is O

- B-DAT
ful O
representational O
ability O
can O
extract O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults O
with O
7 O
state-of-the-art O
methods O

- B-DAT

- B-DAT
ing O
details O
in O
images O
“img O

- B-DAT
tive O
components. O
These O
comparisons O
indicate O

- B-DAT

-1 B-DAT
error O
0.506 O
0.477 O
0.437 O
0.454 O

-5 B-DAT
error O
0.266 O
0.242 O
0.196 O
0.224 O

- B-DAT

- B-DAT

-50 B-DAT
[20] O
as O
the O
evaluation O
model O

- B-DAT

- B-DAT
idation O
dataset O
for O
evaluation. O
The O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
and O
top-5 O
errors. O
These O
comparisons O

- B-DAT

- B-DAT
nections, O
making O
the O
main O
network O

- B-DAT

- B-DAT

- B-DAT
terdependencies O
among O
channels. O
Extensive O
experiments O

-14 B-DAT

-1 B-DAT

-0484, B-DAT
and O
U.S. O
Army O
Research O
Office O

-17 B-DAT

-1 B-DAT

-0367 B-DAT

- B-DAT

- B-DAT

- B-DAT
lutional O
networks. O
TPAMI O
(2016 O

- B-DAT

- B-DAT
resolution O
with O
sparse O
prior. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
with O
deep O
laplacian O
pyramid O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In: O
CVPR. O
(2018 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
chines. O
In: O
ICML. O
(2010 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
representations. O
In: O
Proc. O
7th O
Int O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

patch O
size O
based O
on O
the O
upscaling B-DAT
factor. O
The O
settings O
of O
input O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

Set5[3], O
Set14[41], O
B100[24], O
Urban100[15], O
and O
Manga109 B-DAT

from O
Manga109 B-DAT

the O
‘BokuHaSitatakaKun’ O
im- O
age O
from O
Manga109, B-DAT
DRRN O
and O
MemNet O
even O
split O

Manga109 B-DAT
×2 O
30.30/0.9339 O
35.60/0.9663 O
37.22/0.9750 O
37.60/0.9736 O

Manga109 B-DAT
BD O
25.03/0.7987 O
28.79/0.8851 O
31.66/0.9260 O
31.15/0.9245 O

Params. O
Set5 O
Set14 O
B100 O
Urban100 O
Manga109 B-DAT
MemNet-Pytorch O
677K O
31.75/0.889 O
28.31/0.775 O
27.37/0.729 O

D-DBPN O
especially O
on O
Urban100 O
and O
Manga109 B-DAT
datasets, O
which O
mainly O
contain O
images O

from O
Manga109 B-DAT

from O
Manga109 B-DAT

from O
Manga109 B-DAT

from O
Manga109 B-DAT

from O
Manga109 B-DAT

from O
Manga109 B-DAT
VDSRHR I-DAT
DRRNBicubic O

Manga109 B-DAT

Params. O
Set5 O
Set14 O
B100 O
Urban100 O
Manga109 B-DAT

D-DBPN O
especially O
on O
Urban100 O
and O
Manga109 B-DAT

from O
Manga109 B-DAT
Bicubic I-DAT
VDSR O
DRRN O

from O
Manga109 B-DAT
Figure I-DAT
14. O
Visual O
results O
of O

from O
Manga109 B-DAT
Figure I-DAT
17. O
Visual O
results O
of O

from O
Manga109 B-DAT
Figure I-DAT
18. O
Visual O
results O
of O

from O
Manga109 B-DAT
Figure I-DAT
22. O
Visual O
results O
of O

- B-DAT

- B-DAT

- B-DAT
plored O
the O
power O
of O
deep O

- B-DAT
construction O
performance. O
However, O
the O
feedback O

- B-DAT
nism, O
which O
commonly O
exists O
in O

- B-DAT

- B-DAT
level O
representations O
with O
high-level O
information O

- B-DAT
ically, O
we O
use O
hidden O
states O

- B-DAT
ful O
high-level O
representations. O
The O
proposed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
task, O
which O
aims O
to O

- B-DAT

- B-DAT

- B-DAT
herently O
ill-posed O
since O
multiple O
HR O

- B-DAT
merous O
image O
SR O
methods O
have O

- B-DAT
ing O
interpolation-based O
methods[45], O
reconstruction-based O
methods[42 O

- B-DAT

- B-DAT
lutional O
Neural O
Network O
(CNN) O
to O

- B-DAT
tention O
in O
recent O
years O
due O

- B-DAT
posed O
network. O
Blue O
arrows O
represent O

- B-DAT

- B-DAT
ing O
more O
contextual O
information O
with O

- B-DAT
ishing/exploding O
problems O
caused O
by O
simply O

- B-DAT
ters O
increases. O
A O
large-capacity O
network O

- B-DAT

- B-DAT
current O
Neural O
Network O
(RNN). O
Similar O

- B-DAT
tional O
deep O
learning O
based O
methods O

- B-DAT
ward O
manner. O
However, O
the O
feedforward O

- B-DAT

- B-DAT

- B-DAT
down O
manner, O
carrying O
high-level O
information O

- B-DAT
vious O
layers O
and O
refining O
low-level O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
structed O
by O
multiple O
sets O
of O

- B-DAT
and O
down-sampling O
layers O
with O
dense O

- B-DAT

- B-DAT
covery O
difficulty. O
Such O
curriculum O
learning O

- B-DAT
tion O
models. O
Experimental O
results O
demonstrate O

- B-DAT
ority O
of O
our O
proposed O
SRFBN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work O
(SRFBN), O
which O
employs O
a O

- B-DAT
nism. O
High-level O
information O
is O
provided O

- B-DAT

- B-DAT
while, O
such O
recurrent O
structure O
with O

- B-DAT
tions O
provides O
strong O
early O
reconstruction O

- B-DAT
ficiently O
handles O
feedback O
information O
flows O

- B-DAT

- B-DAT
and O
down- O
sampling O
layers, O
and O

- B-DAT

- B-DAT
ing O
reconstruction O
difficulty O
are O
fed O

- B-DAT

- B-DAT

- B-DAT
ious O
computer O
vision O
tasks O
including O

- B-DAT

- B-DAT

- B-DAT
mation O
usage O
in O
LR O
images O

- B-DAT
provement O
in O
image O
SR. O
SRResNet[21 O

- B-DAT
plied O
residual O
skip O
connections O
from O

- B-DAT
work O
architectures O
use O
or O
combine O

- B-DAT

- B-DAT

- B-DAT
textual O
information O
due O
to O
the O

- B-DAT

- B-DAT
ing O
layers, O
and O
thus O
further O

- B-DAT
ity O
of O
the O
network. O
To O

- B-DAT
resolution O
feedback O
network O
(SRFBN), O
in O

- B-DAT

- B-DAT
down O
manner O
to O
correct O
low-level O

- B-DAT
textual O
information O

- B-DAT
capacity O
networks O
occupy O
huge O
amount O

- B-DAT
rent O
structure O
was O
employed[19, O
31 O

- B-DAT

- B-DAT

- B-DAT
and O
down-projection O
units O
to O
achieve O

- B-DAT
tion O
between O
two O
recurrent O
states O

- B-DAT

- B-DAT
ever, O
the O
flow O
of O
information O

- B-DAT

- B-DAT
tion O
of O
an O
input O
image O

- B-DAT
tional O
recurrent O
neural O
network. O
However O

- B-DAT

- B-DAT
ficiently O
flows O
across O
hierarchical O
layers O

- B-DAT
rior O
reconstruction O
performance O
than O
ConvLSTM1 O

- B-DAT
ficient O
strategy O
to O
improve O
the O

- B-DAT

- B-DAT
diction, O
they O
enforce O
a O
curriculum O

- B-DAT
creases O
during O
the O
training O
process O

- B-DAT
mid O
in O
previously O
trained O
networks O

- B-DAT
cess, O
we O
enforce O
a O
curriculum O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
effect O
process O
helps O
to O
achieve O

- B-DAT

- B-DAT
eration O
(to O
force O
the O
network O

- B-DAT
tion O
of O
high-level O
information), O
(2 O

- B-DAT

- B-DAT
formation, O
which O
is O
needed O
to O

- B-DAT
folded O
to O
T O
iterations, O
in O

- B-DAT
rally O
ordered O
from O
1 O
to O

- B-DAT

- B-DAT
tains O
three O
parts: O
an O
LR O

- B-DAT
sampled O
image O
to O
bypass O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
layer O
and O
a O
deconvolutional O

- B-DAT
traction O
block. O
F O
tin O
are O

- B-DAT

- B-DAT

- B-DAT
tained O
by O

- B-DAT

- B-DAT

- B-DAT
sentations O
F O
tin, O
and O
then O

- B-DAT

- B-DAT
tion O
block. O
The O
FB O
contains O

- B-DAT
tially O
with O
dense O
skip O
connections O

- B-DAT
jection O
group, O
which O
can O
project O

- B-DAT
tures O
F O
tin O
by O
feedback O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ingly, O
Ltg O
can O
be O
obtained O

- B-DAT

- B-DAT
tion O
group O
and O
map O
the O

- B-DAT
work. O
T O
target O
HR O
images O

- B-DAT
work. O
(I1HR, O
I O

- B-DAT
tion O
in O
the O
network O
can O

- B-DAT
put O
at O
the O
t-th O
iterations O

- B-DAT

- B-DAT
and O
down-sampling O
operations. O
For O
×2 O

- B-DAT
ating O
LR O
images O
from O
ground O

- B-DAT
ify O
the O
effectiveness O
of O
our O

- B-DAT
degradation O
models O
as O
[47] O
do O

- B-DAT
periments, O
we O
use O
7x7 O
sized O

- B-DAT
sampling O
followed O
by O
adding O
Gaussian O

- B-DAT
ing O
rate O
0.0001. O
The O
learning O

- B-DAT
ery O
200 O
epochs. O
We O
implement O

- B-DAT
ber O
of O
iterations O
(denoted O
as O

- B-DAT
jection O
groups O
in O
the O
feedback O

- B-DAT
iments. O
We O
first O
investigate O
the O

- B-DAT
out O
feedback O
connections O
(T=1). O
Besides O

- B-DAT
sults. O
It O
is O
worth O
noticing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
forward O
one O
in O
this O
subsection O

- B-DAT
put O
to O
low-level O
representations O
and O

- B-DAT
erty), O
denoted O
as O
SRFBN-L-FF. O
SRFBN-L O

- B-DAT

- B-DAT
FF O
both O
have O
four O
iterations O

- B-DAT
ate O
SR O
images O
from O
both O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
from O
which O
we O
conclude O

- B-DAT
trast O
to O
feedforward O
network. O
The O

- B-DAT
rent O
structure. O
Except O
for O
the O

- B-DAT
tive O
experiments O
to O
verify O
other O

- B-DAT
ation O
except O
the O
first O
iteration O

- B-DAT
put O
to O
low-level O
representations O
and O

- B-DAT
erty), O
denoted O
as O
SRFBN-L-FF. O
SRFBN-L O

- B-DAT

- B-DAT
FF O
both O
have O
four O
iterations O

- B-DAT
ate O
SR O
images O
from O
both O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ation, O
from O
which O
we O
conclude O

- B-DAT
trast O
to O
feedforward O
network. O
The O

- B-DAT
current O
structure. O
Except O
the O
above O

- B-DAT
tive O
experiments O
to O
verify O
other O

- B-DAT
ation O
except O
the O
first O
iteration O

- B-DAT
works O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trated O
in O
Fig. O
5. O
Each O

- B-DAT

- B-DAT
covering O
the O
residual O
image. O
In O

- B-DAT
inal O
input O
image[16] O
and O
to O

- B-DAT

- B-DAT
ponents O
(i.e. O
edges O
and O
contours O

- B-DAT
age. O
To O
some O
extent, O
this O

- B-DAT
tion O
ability O
than O
the O
feedforward O

- B-DAT
vation O
is O
that O
the O
feedback O

- B-DAT
sentations O
in O
contrast O
to O
feedforward O

- B-DAT
tions O
and O
then O
the O
smooth O

- B-DAT
strate O
that O
the O
feedforward O
network O

- B-DAT
formation O
through O
layers, O
while O
the O

- B-DAT
lowed O
to O
devote O
most O
of O

- B-DAT

- B-DAT

- B-DAT
sentations O
at O
the O
initial O
iteration O

- B-DAT

- B-DAT

- B-DAT
quent O
iterations O
to O
generate O
better O

- B-DAT
culty. O
For O
example, O
to O
guide O

- B-DAT
works O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trated O
in O
Fig. O
5. O
Each O

- B-DAT

- B-DAT
work O
with O
global O
residual O
skip O

- B-DAT
ering O
the O
residual O
image. O
In O

- B-DAT
put O
image[16] O
and O
to O
predict O

- B-DAT

- B-DAT
servations. O
First, O
compared O
with O
the O

- B-DAT
tions O
in O
contrast O
to O
feedforward O

- B-DAT
cantly O
from O
the O
first O
iteration O

- B-DAT

- B-DAT

- B-DAT
ing O
high-level O
information O
at O
the O

- B-DAT

- B-DAT
back O
network O
will O
urge O
previous O

- B-DAT
tions O
to O
generate O
better O
representations O

- B-DAT
culty. O
For O
example, O
to O
guide O

- B-DAT
gle O
downsampling O
operator O
at O
early O

- B-DAT

- B-DAT
tion O
model. O
The O
results O
shown O

- B-DAT
riculum O
learning O
strategy O
well O
assists O

- B-DAT

- B-DAT
work O
pretrained O
on O
the O
BI O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
son O
results O
are O
given O
in O

- B-DAT

- B-DAT
rameters O
fewer O
than O
1000K. O
This O

- B-DAT
struction O
performance. O
Meanwhile, O
in O
comparison O

- B-DAT
DBPN O
and O
EDSR, O
our O
proposed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods: O
SRCNN[7], O
VDSR[18], O
DRRN[31], O
SRDenseNet[36 O

- B-DAT

- B-DAT

- B-DAT
perform O
almost O
all O
comparative O
methods O

- B-DAT

- B-DAT
ages O
(DIV2K+Flickr2K+ImageNet O
vs. O
DIV2K+Flickr2K). O
However O

- B-DAT
trast O
to O
them. O
In O
addition O

- B-DAT
age O
from O
Manga109, O
DRRN O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
curriculum O
learning O
strategy O
for O

- B-DAT
tion O
models, O
and O
fine-tuned O
based O

- B-DAT
CNN O
C[43], O
SRMD(NF)[44], O
and O
RDN[47 O

- B-DAT

- B-DAT
most O
all O
quantative O
results O
over O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O

- B-DAT

- B-DAT

- B-DAT
mark O
datasets. O
Compared O
with O
other O

- B-DAT
posed O
SRFBN O
could O
alleviate O
the O

- B-DAT
isions, O
we O
further O
indicate O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dation O
models. O
The O
comprehensive O
experimental O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sored O
by O
National O
Natural O
Science O

- B-DAT
tion O
of O
Sichuan O
Science O
and O

- B-DAT

- B-DAT
son O
Weston. O
Curriculum O
learning. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
with O
feedback O
convolutional O
neural O

- B-DAT
tendra O
Malik. O
Human O
pose O
estimation O

- B-DAT

- B-DAT

- B-DAT
works. O
TPAMI, O
2016. O
2, O
7 O

- B-DAT

- B-DAT
down O
influences O
in O
sensory O
processing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mance O
on O
imagenet O
classification. O
In O

- B-DAT
ian O
Q O
Weinberger. O
Densely O
connected O

- B-DAT
works. O
In O
CVPR, O
2016. O
2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rate O
single O
image O
super-resolution O
via O

- B-DAT
tion O
network. O
In O
CVPR, O
2018 O

- B-DAT
tween O
figure O
and O
background O
by O

- B-DAT
ture, O
1998. O
2 O

- B-DAT

- B-DAT
works. O
In O
CVPR, O
2016. O
1 O

- B-DAT
recursive O
convolutional O
network O
for O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tex. O
arXiv O
preprint O
arXiv:1604.03640, O
2016 O

- B-DAT

- B-DAT
dra O
Malik. O
A O
database O
of O

- B-DAT

- B-DAT
timedia O
Tools O
and O
Applications, O
2017 O

- B-DAT

- B-DAT

- B-DAT
back O
for O
crowd O
counting O
convolutional O

- B-DAT

- B-DAT
resolution O
via O
deep O
recursive O
residual O

- B-DAT
net: O
A O
persistent O
memory O
network O

- B-DAT
chored O
neighborhood O
regression O
for O
fast O

- B-DAT

- B-DAT

- B-DAT
resolution. O
In O
ACCV, O
2015. O
1 O

- B-DAT

- B-DAT
tion. O
In O
CVPR, O
2016. O
7 O

- B-DAT

- B-DAT
hanced O
super-resolution O
generative O
adversarial O
networks O

- B-DAT
der O
Sorkinehornung, O
Olga O
Sorkinehornung, O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
back O
networks. O
In O
CVPR, O
2017 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lation O
algorithm O
via O
directional O
filtering O

- B-DAT

- B-DAT

- B-DAT
tion. O
We O
still O
use O
SRFBN-L O

- B-DAT
and O
down-sampling O
layers O
(UDSL), O
(2 O

- B-DAT
and O
down-sampling O
layers O
with O
3 O

- B-DAT
ers O
(with O
one O
padding O
and O

- B-DAT
and O
down-sampling O
operations O
carrying O
large O

- B-DAT
tion O
and O
are O
effective O
for O

- B-DAT

- B-DAT
ter O
adding O
DSC O
to O
the O

- B-DAT
and O
down-sampling O
layers O
(UDSL), O
and O

- B-DAT
sides, O
high-level O
information O
is O
directly O

- B-DAT

- B-DAT

- B-DAT
ization O

- B-DAT
textual O
information O
for O
the O
next O

- B-DAT
parison O
with O
other O
basic O
blocks O

- B-DAT
nism O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tations O
to O
the O
initial O
feature O

- B-DAT

- B-DAT
formation, O
surely O
are O
corrected O
using O

- B-DAT

- B-DAT

- B-DAT
erage O
feature O
map O
at O
each O

- B-DAT

- B-DAT
back) O
and O
SRFBN-L-FF O
(feedforward). O
As O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trum O
map O
through O
discrete O
Fourier O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eration O
t O
grows, O
the O
feedforward O

- B-DAT
ers O
mid-frequency O
and O
high-frequency O
components O

- B-DAT
developed O
information. O
For O
the O
feedback O

- B-DAT
nism O
(t O
>1), O
mid-frequency O
and O

- B-DAT

- B-DAT
tion O
of O
the O
average O
feature O

- B-DAT
ture O
design, O
we O
compare O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
network O
with O
moderate O
parameters O

- B-DAT
cause O
MemNet O
only O
reveals O
the O

- B-DAT

- B-DAT

- B-DAT
tary O
materials. O
Our O
SRFBN-S O
(T=4 O

- B-DAT
son. O
In O
Tab. O
8, O
our O

- B-DAT

- B-DAT
sults O
than O
MemNet O
with O
71 O

- B-DAT

- B-DAT
parison O
shows O
the O
effectiveness O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ban100 O
with O
scale O
factor O
×4 O

- B-DAT

- B-DAT
torch O
for O
fair O
comparison. O
The O

- B-DAT
works O
is O
evaluated O
on O
the O

- B-DAT
tel O
i7 O
CPU O
(16G O
RAM O

- B-DAT
ing O
their O
official O
codes. O
Tab O

- B-DAT

- B-DAT
son O
with O
other O
networks. O
This O

- B-DAT
tiveness O
of O
our O
proposed O
networks O

- B-DAT
volutional O
layers O
with O
77% O
fewer O

-22, B-DAT
we O
provide O
more O
visual O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

layer O
in O
LR O
space O
for O
upscaling B-DAT

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

B100 O
[18], O
Urban100 O
[8], O
and O
Manga109 B-DAT
[19]. O
The O
SR O
re- O
sults O

Manga109 B-DAT
×2 O
30.80/0.9339 O
35.60/0.9663 O
37.27/0.9740 O
37.60/0.9736 O

and O
Manga109 B-DAT
with O
scaling O
factor O
×3. O
Our O

Manga109 B-DAT
BD O
25.46/0.8149 O
29.64/0.9003 O
29.47/0.8924 O
23.04/0.7927 O

from O
B100 O
and O
“LancelotFullThrottle” O
from O
Manga109 B-DAT
respectively O

and O
Manga109 B-DAT

Manga109 B-DAT

from O
B100 O
and O
“LancelotFullThrottle” O
from O
Manga109 B-DAT

- B-DAT

- B-DAT
cently O
achieved O
great O
success O
for O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tract O
abundant O
local O
features O
via O

- B-DAT
tional O
layers. O
RDB O
further O
allows O

- B-DAT
taining O
dense O
local O
features, O
we O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
graded O
low-resolution O
(LR) O
measurement. O
SISR O

- B-DAT
lance O
imaging O
[42], O
medical O
imaging O

- B-DAT
eration O
[9]. O
While O
image O
SR O

- B-DAT

- B-DAT
cedure, O
since O
there O
exists O
a O

- B-DAT
based O
[40], O
reconstruction-based O
[37], O
and O

- B-DAT

- B-DAT

- B-DAT
ules, O
the O
networks O
for O
image O

- B-DAT
ory O
block O
was O
proposed O
to O

- B-DAT

- B-DAT
gles O
of O
view, O
and O
aspect O

- B-DAT
tion. O
While, O
most O
deep O
learning O

- B-DAT

- B-DAT
inal O
LR O
image O
to O
the O

- B-DAT
processing O
step O
not O
only O
increases O

- B-DAT
ing O
to O
our O
experiments O
(see O

- B-DAT
archical O
features O
from O
the O
original O

- B-DAT
posed O
residual O
dense O
block O
(Fig O

- B-DAT
tical O
for O
a O
very O
deep O

- B-DAT
ual O
dense O
block O
(RDB) O
as O

- B-DAT
sion O
(LFF) O
with O
local O
residual O

- B-DAT
catenating O
the O
states O
of O
preceding O

- B-DAT
ing O
layers O
within O
the O
current O

- B-DAT

- B-DAT
sion O
[15 O

- B-DAT

- B-DAT
work O
(RDN) O
for O
high-quality O
image O

- B-DAT
tiguous O
memory O
(CM) O
mechanism, O
but O

- B-DAT
lize O
all O
the O
layers O
within O

- B-DAT
tions. O
The O
accumulated O
features O
are O

- B-DAT
ods O
in O
computer O
vision O
[36 O

- B-DAT
ited O
space, O
we O
only O
discuss O

- B-DAT

- B-DAT

- B-DAT
ther O
improved O
mainly O
by O
increasing O

- B-DAT
ing O
network O
weights. O
VDSR O
[10 O

- B-DAT
creased O
the O
network O
depth O
by O

- B-DAT
duced O
recursive O
learning O
in O
a O

- B-DAT
rameter O
sharing. O
Tai O
et O
al O

- B-DAT
inal O
LR O
images O
to O
the O

- B-DAT

- B-DAT
creases O
computation O
complexity O
quadratically O
[4 O

- B-DAT

- B-DAT
tures O
from O
the O
interpolated O
LR O

- B-DAT

- B-DAT

- B-DAT
PCN O
[22], O
where O
an O
efficient O

- B-DAT

- B-DAT

- B-DAT
ods O
extracted O
features O
in O
the O

- B-DAT
nal O
LR O
features O
with O
transposed O

- B-DAT

- B-DAT

- B-DAT
lows O
direct O
connections O
between O
any O

- B-DAT
troduced O
among O
memory O
blocks O
[26 O

- B-DAT

- B-DAT
duced O
by O
a O
very O
deep O

- B-DAT
tion O
tasks O
(e.g., O
image O
SR O

- B-DAT

- B-DAT
tracts O
features O
F−1 O
from O
the O

- B-DAT
ond O
shallow O
feature O
extraction O
layer O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
layers O
within O
the O
block O

- B-DAT
ture. O
More O
details O
about O
RDB O

- B-DAT
cludes O
global O
feature O
fusion O
(GFF O

- B-DAT

- B-DAT

- B-DAT
nected O
layers, O
local O
feature O
fusion O

- B-DAT
ual O
learning, O
leading O
to O
a O

- B-DAT
nism O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sists O
of O
G O
(also O
known O

- B-DAT

- B-DAT

- B-DAT
tional O
layers O
1 O

- B-DAT

- B-DAT

- B-DAT
ing O
RDB O
and O
each O
layer O

- B-DAT
sequent O
layers, O
which O
not O
only O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ber. O
On O
the O
other O
hand O

- B-DAT
duce O
a O
1 O
× O
1 O

- B-DAT

- B-DAT
comes O
larger, O
very O
deep O
dense O

- B-DAT
tional O
layers O
in O
one O
RDB O

- B-DAT

- B-DAT
mance. O
We O
introduce O
more O
results O

- B-DAT
ing, O
we O
refer O
to O
this O

- B-DAT
maps O
produced O
by O
residual O
dense O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tively O
fused O
to O
form O
FGF O

- B-DAT

- B-DAT
quency O
information. O
However, O
in O
the O

- B-DAT
sequent O
layers. O
The O
local O
feature O

- B-DAT

- B-DAT
tion, O
MemNet O
extracts O
features O
in O

- B-DAT

- B-DAT
nition). O
While O
RDN O
is O
designed O

- B-DAT

- B-DAT
ual O
learning, O
which O
would O
be O

- B-DAT

- B-DAT
cal O
features, O
which O
are O
neglected O

- B-DAT
ferences O
between O
SRDenseNet O
[31] O
and O

- B-DAT
troduces O
the O
basic O
dense O
block O

- B-DAT
bilizes O
the O
training O
of O
wide O

- B-DAT
tract O
global O
features, O
because O
our O

- B-DAT
formance O
and O
convergence O
[17]. O
As O

- B-DAT
formation O
from O
their O
preceding O
layers O

- B-DAT
ers O
within O
one O
RDB. O
Furthermore O

- B-DAT
nections O
among O
memory O
blocks O
in O

- B-DAT

- B-DAT
sults O
are O
evaluated O
with O
PSNR O

- B-DAT
tion O
models O
to O
simulate O
LR O

- B-DAT
bic O
downsampling O
by O
adopting O
the O

- B-DAT
ing O
90◦. O
1,000 O
iterations O
of O

- B-DAT

- B-DAT
ery O
200 O
epochs. O
Training O
a O

- B-DAT
rameters: O
the O
number O
of O
RDB O

- B-DAT
mance O
of O
SRCNN O
[3] O
as O

- B-DAT
cal O
residual O
learning O
(LRL), O
and O

- B-DAT
portant, O
our O
RDN O
allows O
deeper O

- B-DAT
ture O
fusion O
(LFF) O
is O
needed O

- B-DAT
erly, O
so O
LFF O
isn’t O
removed O

- B-DAT
strates O
that O
stacking O
many O
basic O

- B-DAT
sulting O
in O
RDN O
CM1LRL0GFF0, O
RDN O

- B-DAT
ponent O
can O
efficiently O
improve O
the O

- B-DAT
line. O
This O
is O
mainly O
because O

- B-DAT
ing O
in O
RDN O
CM1LRL1GFF0, O
RDN O

- B-DAT
bination O
in O
Table O
1). O
It O

- B-DAT
ponents O
simultaneously O
(denote O
as O
RDN O

- B-DAT
sistent O
with O
the O
analyses O
above O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
SR O
methods: O
SRCNN O
[3 O

- B-DAT

- B-DAT
ther O
improve O
our O
RDN O
and O

- B-DAT

- B-DAT
rable O
or O
even O
better O
results O

- B-DAT
DenseNet O
[31] O
and O
MemNet O
[26 O

- B-DAT
els, O
our O
RDN O
also O
achieves O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put O
patch O
size. O
Moreover, O
our O

- B-DAT

- B-DAT
ods O
would O
produce O
noticeable O
artifacts O

- B-DAT
pared O
methods O
fail O
to O
recover O

- B-DAT
cover O
it O
obviously. O
This O
is O

- B-DAT
archical O
features O
through O
dense O
feature O

- B-DAT
CNN O
[3], O
FSRCNN O
[4], O
VDSR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults O
in O
Figs. O
7 O
and O

- B-DAT
covers O
sharper O
edges. O
This O
comparison O

- B-DAT
tracting O
hierarchical O
features O
from O
the O

- B-DAT
ods O
[3, O
10, O
38]. O
However O

- B-DAT
parison O
indicates O
that O
RDN O
is O

- B-DAT
tion O
models O
demonstrate O
the O
effectiveness O

- B-DAT
ing O
factor O
×3. O
The O
SR O

- B-DAT
ban100 O
and O
“img O
099” O
from O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferent O
or O
unknown O
degradation O
models O

- B-DAT
lizes O
the O
training O
wider O
network O

- B-DAT
ual O
leaning O
(LRL) O
further O
improves O

- B-DAT
world O
data. O
Extensive O
benchmark O
evaluations O

- B-DAT
strate O
that O
our O
RDN O
achieves O

- B-DAT

- B-DAT

- B-DAT
art O
methods O

-14 B-DAT

-1 B-DAT

- B-DAT
0484, O
and O
U.S. O
Army O
Research O

-17 B-DAT

- B-DAT
1-0367 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
deep O
convolutional O
networks O

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT

- B-DAT
mization. O
In O
ICLR, O
2014. O
5 O

- B-DAT
resolution. O
In O
CVPR, O
2017. O
1 O

- B-DAT
ham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT

- B-DAT
supervised O
nets. O
In O
AISTATS, O
2015 O

- B-DAT

- B-DAT
cal O
statistics. O
In O
ICCV, O
2001 O

- B-DAT
masaki, O
and O
K. O
Aizawa. O
Sketch-based O

- B-DAT
ing O
manga109 O
dataset. O
Multimedia O
Tools O

- B-DAT

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT
tia, O
A. O
M. O
S. O
M O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
hazing O
network. O
In O
CVPR, O
2018 O

- B-DAT

- B-DAT
raining O
using O
a O
multi-stream O
dense O

- B-DAT

- B-DAT
resolution O
with O
non-local O
means O
and O

- B-DAT
sion. O
TIP, O
2012. O
1 O

- B-DAT
lutional O
super-resolution O
network O
for O
multiple O

- B-DAT

- B-DAT

- B-DAT
nition O
problem. O
TIP, O
2012. O
1 O

reconstruct O
high-resolution O
images O
of O
different O
upscaling B-DAT
factors O
in O
a O
single O
model O

demonstrated O
in O
Fig. O
4. O
For O
upscaling B-DAT
×4, O
if O
we O
use O
a O

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

- B-DAT

- B-DAT

- B-DAT
hanced O
deep O
super-resolution O
network O
(EDSR O

- B-DAT
mance O
exceeding O
those O
of O
current O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods. O
The O
significant O
performance O
improvement O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
on O
benchmark O
datasets O
and O

- B-DAT
ning O
the O
NTIRE2017 O
Super-Resolution O
Challenge O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nificantly O
improved O
performance O
in O
terms O

- B-DAT

- B-DAT
noise O
ratio O
(PSNR) O
in O
the O

- B-DAT
works O
exhibit O
limitations O
in O
terms O

- B-DAT

- B-DAT

- B-DAT
resolution O
of O
different O
scale O
factors O

- B-DAT
lems O
without O
considering O
and O
utilizing O

- B-DAT
quire O
many O
scale-specific O
networks O
that O

- B-DAT

- B-DAT

- B-DAT
dancy O
among O
scale-specific O
models. O
Nonetheless O

- B-DAT

- B-DAT
pling O
method O
[5, O
22, O
14 O

- B-DAT
ploys O
the O
ResNet O
architecture O
from O

- B-DAT
posed O
to O
solve O
higher-level O
computer O

- B-DAT

- B-DAT

- B-DAT
chitecture, O
we O
first O
optimize O
it O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els. O
Furthermore, O
we O
propose O
a O

- B-DAT

- B-DAT

- B-DAT
rameters O
compared O
with O
multiple O
single-scale O

- B-DAT
and O
multi-scale O
super-resolution O
networks O
show O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dicting O
detailed, O
realistic O
textures. O
Previous O

- B-DAT
struct O
better O
high-resolution O
images O

- B-DAT
tween O
ILR O
and O
IHR O
image O

- B-DAT
ods O
rely O
on O
techniques O
ranging O

- B-DAT
ding O
[3, O
2, O
7, O
21 O

- B-DAT
proaches O
utilize O
image O
self-similarities O
to O

- B-DAT
nal O
databases O
[8, O
6, O
29 O

- B-DAT
works O
has O
led O
to O
dramatic O

- B-DAT

- B-DAT
ing O
much O
deeper O
network O
architectures O

- B-DAT
perior O
performance. O
In O
particular, O
they O

- B-DAT
connection O
and O
recursive O
convolution O
alleviate O

- B-DAT

- B-DAT
work. O
Similarly O
to O
[20], O
Mao O

- B-DAT

- B-DAT

- B-DAT
rithms, O
an O
input O
image O
is O

- B-DAT
lation O
before O
they O
fed O
into O

- B-DAT
sampling O
modules O
at O
the O
very O

- B-DAT
sible O
as O
shown O
in O
[5 O

- B-DAT
cause O
the O
size O
of O
features O

- B-DAT

- B-DAT
scale O
training O
and O
computational O
efficiency O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
thermore, O
we O
develop O
an O
appropriate O

- B-DAT
and O
multi-scale O
mod- O
els O

- B-DAT

- B-DAT
hibiting O
improved O
computational O
efficiency. O
In O

- B-DAT
ing O
sections, O
we O
suggest O
a O

- B-DAT

- B-DAT

- B-DAT
scale O
architecture O
(MDSR) O
that O
reconstructs O

- B-DAT

- B-DAT
level O
to O
high-level O
tasks. O
Although O

- B-DAT
fully O
applied O
the O
ResNet O
architecture O

- B-DAT

- B-DAT
mance O
by O
employing O
better O
ResNet O

- B-DAT
work O
model O
from O
original O
ResNet O

- B-DAT
tion O
increases O
the O
performance O
substantially O

- B-DAT
duced O
since O
the O
batch O
normalization O

- B-DAT

- B-DAT

- B-DAT
work O
model O
is O
to O
increase O

- B-DAT
nels) O
F O
occupies O
roughly O
O(BF O

- B-DAT
imize O
the O
model O
capacity O
when O

- B-DAT
tational O
resources O

- B-DAT
cedure O
numerically O
unstable. O
A O
similar O

- B-DAT
ing O
procedure O
greatly O
when O
using O

- B-DAT
ous O
convolution O
layer O
for O
the O

- B-DAT

- B-DAT
vation O
layers O
outside O
the O
residual O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
at O
multiple O
scales O
is O

- B-DAT

- B-DAT
ther O
explore O
this O
idea O
by O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
specific O
processing O
modules O
to O
handle O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ule O
consists O
of O
two O
residual O

- B-DAT

- B-DAT

- B-DAT
tive O
field O
is O
covered O
in O

- B-DAT

- B-DAT

- B-DAT
ules O
are O
located O
in O
parallel O

- B-DAT

- B-DAT
tion. O
The O
architecture O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els O
for O
3 O
different O
scales O

- B-DAT

- B-DAT

- B-DAT
hibits O
comparable O
performance O
as O
the O

- B-DAT

- B-DAT

- B-DAT

Residual O
scaling O
- B-DAT
- O
0.1 O

- B-DAT

- B-DAT
ual O
blocks O
are O
lighter O
than O

- B-DAT

- B-DAT
specific O
EDSRs. O
The O
detailed O
performance O

- B-DAT

- B-DAT
formances O
on O
the O
validation O
dataset O

- B-DAT

- B-DAT
ing O
the O
mean O
RGB O
value O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works O
as O
described O
in O
Sec O

- B-DAT
trained O
network O
for O
other O
scales O

- B-DAT

- B-DAT
specific O
residual O
blocks O
and O
upsampling O

- B-DAT
spond O
to O
different O
scales O
other O

- B-DAT
imizing O
L2 O
is O
generally O
preferred O

- B-DAT
pirically O
found O
that O
L1 O
loss O

- B-DAT
spectively. O
The O
source O
code O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
super-resolved O
images O

- B-DAT

- B-DAT

- B-DAT
rate O
models. O
It O
is O
beneficial O

- B-DAT

- B-DAT
dividually O
trained O
models. O
We O
denote O

- B-DAT

- B-DAT

- B-DAT
inal O
one O
trained O
with O
L2 O

- B-DAT

- B-DAT
els O
require O
much O
less O
GPU O

- B-DAT
sults O
in O
an O
individual O
experiment O

- B-DAT
per O
[14]. O
In O
our O
experiments O

- B-DAT

0.9542 O
37.53 O
/ O
0.9587 O
- B-DAT
/ O
- O
38.11 O
/ O
0.9601 O

0.9090 O
33.66 O
/ O
0.9213 O
- B-DAT
/ O
- O
34.65 O
/ O
0.9282 O

0.9063 O
33.03 O
/ O
0.9124 O
- B-DAT
/ O
- O
33.92 O
/ O
0.9195 O

0.8209 O
29.77 O
/ O
0.8314 O
- B-DAT
/ O
- O
30.52 O
/ O
0.8462 O

0.8879 O
31.90 O
/ O
0.8960 O
- B-DAT
/ O
- O
32.32 O
/ O
0.9013 O

0.7863 O
28.82 O
/ O
0.7976 O
- B-DAT
/ O
- O
29.25 O
/ O
0.8093 O

0.8946 O
30.76 O
/ O
0.9140 O
- B-DAT
/ O
- O
32.93 O
/ O
0.9351 O

0.7989 O
27.14 O
/ O
0.8279 O
- B-DAT
/ O
- O
28.80 O
/ O
0.8653 O

0.9581 O
33.66 O
/ O
0.9625 O
- B-DAT
/ O
- O
35.03 O
/ O
0.9695 O

0.9138 O
30.09 O
/ O
0.9208 O
- B-DAT
/ O
- O
31.26 O
/ O
0.9340 O

0.8753 O
28.17 O
/ O
0.8841 O
- B-DAT
/ O
- O
29.25 O
/ O
0.9017 O

- B-DAT

- B-DAT
vided O
in O
the O
last O
two O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sure O
PSNR O
and O
SSIM O
on O

- B-DAT
LAB O
[18] O
functions O
for O
evaluation O

- B-DAT
nificant O
improvement O
compared O
to O
the O

- B-DAT

- B-DAT

- B-DAT
puts O
compared O
with O
the O
previous O

- B-DAT
ticipating O
in O
the O
NTIRE2017 O
Super-Resolution O

- B-DAT
resolution O
system O
with O
the O
highest O

- B-DAT
graders O
(bicubic, O
unknown) O
with O
three O

- B-DAT
ditions. O
Some O
results O
of O
our O

- B-DAT
ods O
successfully O
reconstruct O
high-resolution O
images O

- B-DAT

- B-DAT
ventional O
ResNet O
architecture, O
we O
achieve O

- B-DAT
ual O
scaling O
techniques O
to O
stably O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT

- B-DAT

- B-DAT
age O
Processing, O
21(7):3194–3205, O
2012. O
2 O

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT

- B-DAT
mization. O
In O
ICLR O
2014. O
5 O

- B-DAT

- B-DAT

- B-DAT
ative O
adversarial O
network. O
arXiv:1609.04802, O
2016 O

- B-DAT

- B-DAT
ing O
very O
deep O
convolutional O
encoder-decoder O

- B-DAT
cal O
statistics. O
In O
ICCV O
2001 O

- B-DAT

- B-DAT

- B-DAT
tional O
networks O
for O
biomedical O
image O

- B-DAT
CAI O
2015. O
2 O

- B-DAT
tion O
by O
locally O
linear O
embedding O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT
v4, O
inception-resnet O
and O
the O
impact O

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
actions O
on O
Image O
Processing, O
21(8):3467–3478 O

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT

- B-DAT

- B-DAT
tional O
Conference O
on O
Curves O
and O

- B-DAT

- B-DAT
gorithm O
via O
directional O
filtering O
and O

- B-DAT
actions O
on O
Image O
Processing, O
15(8):2226–2238 O

BSDS100 O
[1], O
Urban100 O
[16] O
and O
Manga109 B-DAT
[33]. O
Each O
dataset O
has O
different O

in O
different O
frequency O
bands; O
and O
Manga109 B-DAT
is O
a O
dataset O
of O
Japanese O

each O
image O
in O
Urban100 O
and O
Manga109 B-DAT
into O
four O
parts O
and O
then O

Interesting O
results O
are O
shown O
on O
Manga109 B-DAT
dataset O
where O
D-DBPN O
obtains O
25.50 O

Set5 O
Set14 O
BSDS100 O
Urban100 O
Manga109 B-DAT

in O
different O
frequency O
bands; O
and O
Manga109 B-DAT

each O
image O
in O
Urban100 O
and O
Manga109 B-DAT

Interesting O
results O
are O
shown O
on O
Manga109 B-DAT

Set5 O
Set14 O
BSDS100 O
Urban100 O
Manga109 B-DAT
Algorithm I-DAT
Scale O
PSNR O
SSIM O
PSNR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
images. O
We O
propose O

- B-DAT

- B-DAT
and O
down- O
sampling O
layers, O
providing O

- B-DAT
connected O
up- O
and O
down-sampling O
stages O

- B-DAT
resolution O
components. O
We O
show O
that O

- B-DAT
and O
down- O
sampling O
stages O
(Dense O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
proach O
is O
to O
construct O
an O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work O
[6, O
7, O
38, O
25 O

- B-DAT
ing O
with O
one O
or O
more O

- B-DAT
lution O
and O
finally O
construct O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
SRN O
[25] O
(15.25 O
dB), O
EDSR O

- B-DAT
fectively O
by O
one O
of O
the O

- B-DAT

- B-DAT
tion O
error O
then O
fuses O
it O

- B-DAT
fect O
[4]. O
Moreover, O
this O
method O

- B-DAT
erator, O
leading O
to O
variability O
in O

- B-DAT

- B-DAT

- B-DAT
and O
down- O
sampling: O
Deep O
Back-Projection O

- B-DAT
butions: O
(1) O
Error O
feedback. O
We O

- B-DAT
correcting O
feedback O
mechanism O
for O
SR O

- B-DAT
and O
down-projection O
errors O
to O
guide O

- B-DAT
tion O
for O
obtaining O
better O
results O

- B-DAT
and O
down-sampling O
stages. O
Feed-forward O
architectures O

- B-DAT
way O
mapping, O
only O
map O
rich O

- B-DAT

- B-DAT
(blue O
box) O
and O
down-sampling O
(gold O

- B-DAT
tures O
using O
upsampling O
layers O
but O

- B-DAT
(blue O
box) O
and O
down-sampling O
(gold O

- B-DAT
ity O
enables O
the O
networks O
to O

- B-DAT

- B-DAT
tion O
directly O
utilizes O
different O
types O

- B-DAT

- B-DAT

- B-DAT
and O
down-sampling O
stage O
to O
encourage O

- B-DAT

- B-DAT
tion O
as O
the O
upsampling O
operator O

- B-DAT
tion O
(MR) O
image. O
This O
schema O

- B-DAT
CNN O
[6] O
to O
learn O
MR-to-HR O

- B-DAT

- B-DAT
ple O
convolutional O
layers. O
Later, O
the O

- B-DAT
ploited O
residual O
learning O
[22, O
43 O

- B-DAT
posed O
by O
FSRCNN O
[7] O
and O

- B-DAT
tion O
and O
replace O
predefined O
operators O

- B-DAT
longs O
to O
this O
type. O
However O

- B-DAT
portunities O
to O
propose O
lighter O
networks O

- B-DAT

- B-DAT
late O
the O
reconstruction O
error O
to O

- B-DAT
ables O
the O
networks O
to O
preserve O

- B-DAT
ing O
various O
up- O
and O
down-sampling O

- B-DAT
ating O
deeper O
features O

- B-DAT

- B-DAT
to-target O
space O
in O
one O
step O

- B-DAT
pose O
the O
prediction O
process O
into O

- B-DAT

- B-DAT
back O
procedure O
has O
been O
implemented O

- B-DAT
tion. O
PredNet O
[32] O
is O
an O

- B-DAT
tion, O
Li O
et O
al. O
[29 O

- B-DAT
back O
procedures O
have O
not O
been O

- B-DAT
ial O
Networks O
(GANs) O
[10] O
has O

- B-DAT
age O
reconstruction O
problems O
[28, O
37 O

- B-DAT

- B-DAT

- B-DAT
works. O
Ledig O
et O
al. O
[28 O

- B-DAT
ered O
as O
a O
single O
upsampling O

- B-DAT
ral O
image O
manifold O
that O
is O

- B-DAT

- B-DAT
ages O
by O
specifically O
formulating O
a O

- B-DAT

- B-DAT

- B-DAT
erative O
procedure O
to O
minimize O
the O

- B-DAT
projection O
[51, O
11, O
8, O
46 O

- B-DAT

- B-DAT
ror O
iteratively O
[4]. O
Timofte O
et O

- B-DAT
projection O
can O
improve O
the O
quality O

- B-DAT

- B-DAT
known. O
Most O
of O
the O
previous O

- B-DAT

- B-DAT

- B-DAT
and O
down-sampling O
stages O
to O
learn O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
projection O
unit O
projects O
it O
back O

- B-DAT
serve O
the O
HR O
components O
by O

- B-DAT
and O
down- O
sampling O
operators O
and O

- B-DAT
struct O
numerous O
LR O
and O
HR O

- B-DAT

- B-DAT

- B-DAT
end O
training O
of O
the O
SR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
spectively, O
the O
up- O
and O
down-sampling O

- B-DAT

- B-DAT

- B-DAT
and O
down-projection O
unit O
in O
the O

- B-DAT
nating O
between O
H O
and O
L O

- B-DAT
derstood O
as O
a O
self-correcting O
procedure O

- B-DAT
jection O
error O
to O
the O
sampling O

- B-DAT
sized O
filter O
is O
avoided O
because O

- B-DAT
gence O
speed O
and O
might O
produce O

- B-DAT

- B-DAT
ever, O
iterative O
utilization O
of O
our O

- B-DAT
works O

- B-DAT

- B-DAT
gradient O
problem, O
produce O
improved O
feature O

- B-DAT
age O
feature O
reuse. O
Inspired O
by O

- B-DAT

- B-DAT
mensional O
reduction O
[42, O
12] O
before O

- B-DAT

- B-DAT
and O
down-projection O
unit, O
re- O
spectively O

- B-DAT
ture O
maps O
effectively, O
as O
shown O

- B-DAT

- B-DAT
jection, O
and O
reconstruction, O
as O
described O

- B-DAT
and O
down-projection O
unit O
in O
the O

- B-DAT

- B-DAT
and O
down-projections O
units, O
respectively) O
are O

- B-DAT

- B-DAT
tures O
extraction O
and O
nR O
is O

- B-DAT

- B-DAT
traction O
is O
a O
sequence O
of O

- B-DAT

- B-DAT

- B-DAT
work O
architecture O
is O
modular. O
We O

- B-DAT
traction O
stage O
(2 O
layers), O
and O

- B-DAT

- B-DAT

- B-DAT
tion O
unit O
is O
various O
with O

- B-DAT
nally, O
the O
8× O
enlargement O
use O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puted O
by O

- B-DAT
volutional O
layers O
are O
followed O
by O

- B-DAT
tion.2 O
To O
produce O
LR O
images O

- B-DAT
ing O
Caffe, O
MATLAB O
R2017a O
on O

- B-DAT
tion. O
The O
input O
and O
output O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formance, O
S O
networks O
can O
achieve O

- B-DAT
SRN, O
respectively. O
The O
M O
network O

- B-DAT

- B-DAT

- B-DAT
tal, O
the O
M O
network O
use O

- B-DAT
SRN, O
and O
DRRN, O
respectively O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
largement. O
S O
(T O
= O
2 O

- B-DAT

- B-DAT
rameters O
on O
4× O
and O
8 O

- B-DAT
DBPN O
has O
about O
76% O
fewer O

- B-DAT

- B-DAT
dence O
show O
that O
our O
networks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tures O
generated O
from O
the O
projection O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tively O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pare O
our O
network O
with O
eight O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rithms: O
A+ O
[45], O
SRCNN O
[6 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tics. O
Set5, O
Set14 O
and O
BSDS100 O

- B-DAT

- B-DAT
vide O
each O
image O
in O
Urban100 O

- B-DAT

- B-DAT

- B-DAT
put O
image. O
It O
takes O
less O

- B-DAT
DBPN O
outperforms O
the O
existing O
methods O

- B-DAT

- B-DAT
vious O
statement O
is O
strengthened O
by O

- B-DAT
ban100 O
dataset O
which O
consist O
of O

- B-DAT

- B-DAT
ods O
by O
a O
large O
margin O

- B-DAT

- B-DAT

- B-DAT
ter O
than O
EDSR. O
The O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
which O
predict O
the O
SR O

- B-DAT

- B-DAT
tures O
using O
multiple O
up- O
and O

- B-DAT

- B-DAT

- B-DAT
and O
down-scaling O
steps O
to O
guide O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work O
successfully O
outperforms O
other O
state-of-the-art O

- B-DAT
ods O
on O
large O
scaling O
factors O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
Conference O
(BMVC), O
2012. O
6 O

- B-DAT
man O
pose O
estimation O
with O
iterative O

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT
projection O
for O
single O
image O
super O

- B-DAT
tive O
image O
models O
using O
a O

- B-DAT
tems, O
pages O
1486–1494, O
2015. O
1 O

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
ference O
on O
Computer O
Vision, O
pages O

- B-DAT
projection O
for O
adaptive O
image O
enlargement O

- B-DAT
cessing O
(ICIP), O
2009 O
16th O
IEEE O

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
nets. O
In O
Advances O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
for O
image O
recognition. O
arXiv O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision, O
pages O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
sion O
and O
Pattern O
Recognition O
(CVPR O

- B-DAT
istration. O
CVGIP: O
Graphical O
models O
and O

- B-DAT
ment: O
Resolution, O
occlusion, O
and O
transparency O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT
volutional O
network O
for O
image O
super-resolution O

- B-DAT
ings O
of O
the O
IEEE O
Conference O

- B-DAT
resolution. O
In O
IEEE O
Conferene O
on O

- B-DAT
tern O
Recognition, O
2017. O
1, O
2 O

- B-DAT
sion O
offered O
by O
feedforward O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
adversarial O
network. O
In O
IEEE O

- B-DAT
tation. O
In O
Proceedings O
of O
the O

- B-DAT
resolution O
via O
deep O
draft-ensemble O
learning O

- B-DAT

- B-DAT
ing O
networks O
for O
video O
prediction O

- B-DAT
masaki, O
and O
K. O
Aizawa. O
Sketch-based O

- B-DAT
ing O
manga109 O
dataset. O
Multimedia O
Tools O

- B-DAT
sentation O
learning O
with O
deep O
convolutional O

- B-DAT
sarial O
networks. O
arXiv O
preprint O
arXiv:1511.06434 O

- B-DAT

- B-DAT
tion. O
In O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT
back O
for O
faster O
r-cnn. O
In O

- B-DAT
tion, O
2017. O
1 O

- B-DAT

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT
nition O
Workshops O
(CVPRW), O
2017 O
IEEE O

- B-DAT

- B-DAT
prove O
example-based O
single O
image O
super O

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT
level O
vision O
tasks O
and O
3d O

- B-DAT
tural O
similarity. O
Image O
Processing, O
IEEE O

- B-DAT

- B-DAT

- B-DAT
erative O
projection O
reconstruction O
for O
fast O

- B-DAT
age O
restoration. O
However, O
as O
the O

- B-DAT

- B-DAT
tle O
influence O
on O
the O
subsequent O

- B-DAT
tive O
learning O
process. O
The O
recursive O

- B-DAT

- B-DAT
tive O
fields. O
The O
representations O
and O

- B-DAT
vious O
states O
should O
be O
reserved O

- B-DAT
resolution O
and O
JPEG O
deblocking. O
Comprehensive O

- B-DAT
iments O
demonstrate O
the O
necessity O
of O

- B-DAT

- B-DAT

- B-DAT
quality O
version O
of O
x, O
D O

- B-DAT
der O
Grant O
Nos. O
91420201, O
61472187 O

- B-DAT

- B-DAT
search O
Fund. O
Jian O
Yang O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
noising O
aims O
to O
recover O
a O

- B-DAT
servation, O
which O
commonly O
assumes O
additive O

- B-DAT
sian O
noise O
with O
a O
standard O

- B-DAT

- B-DAT
resolution O
recovers O
a O
high-resolution O
(HR O

- B-DAT

- B-DAT

- B-DAT
trol O
the O
parameter O
number O
of O

- B-DAT
Recursive O
Convolutional O
Network O
(DRCN) O
[21 O

- B-DAT
gate O
training O
difficulty, O
Mao O
et O

- B-DAT

- B-DAT

- B-DAT
over, O
Zhang O
et O
al. O
[40 O

- B-DAT
path O
feed-forward O
architecture, O
where O
one O

- B-DAT
fluenced O
by O
its O
direct O
former O

- B-DAT

- B-DAT
ory. O
Some O
variants O
of O
CNNs O

- B-DAT

- B-DAT
cific O
prior O
state, O
namely O
restricted O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tent O
memory O
network O
(MemNet), O
which O

- B-DAT
ory O
block O
to O
explicitly O
mine O

- B-DAT
tion O
Net O
(FENet) O
first O
extracts O

- B-DAT

- B-DAT
tains O
a O
recursive O
unit O
and O

- B-DAT
science O
[6, O
25] O
that O
recursive O

- B-DAT
ist O
in O
the O
neocortex, O
the O

- B-DAT

- B-DAT
tive O
fields O
(blue O
circles O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ated O
from O
the O
previous O
memory O

- B-DAT

- B-DAT
ther, O
we O
present O
an O
extended O

- B-DAT

- B-DAT

- B-DAT
term O
memory O
should O
be O
reserved O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
even O
using O
a O
single O

- B-DAT
veal O
that O
the O
network O
depth O

- B-DAT
tion O
and O
achieved O
comparable O
performance O

- B-DAT
resolution O
convolutional O
neural O
network O
(SRCNN O

- B-DAT
dicts O
the O
nonlinear O
LR-HR O
mapping O

- B-DAT
volutional O
network, O
which O
significantly O
outperforms O

- B-DAT
cal O
shallow O
methods. O
The O
authors O

- B-DAT
tended O
CNN O
model, O
named O
Artifacts O

- B-DAT
tional O
Neural O
Networks O
(ARCNN) O
[7 O

- B-DAT

- B-DAT
ural O
sparsity O
of O
images O
[36 O

- B-DAT

- B-DAT
edge O
in O
the O
JPEG O
compression O

- B-DAT

- B-DAT

- B-DAT
ers O
to O
exploit O
large O
contextual O

- B-DAT
ing O
and O
adjustable O
gradient O
clipping O

- B-DAT
ization O
into O
a O
DnCNN O
model O

- B-DAT
age O
restoration O
tasks. O
To O
reduce O

- B-DAT

- B-DAT
connection O
to O
mitigate O
the O
training O

- B-DAT

- B-DAT
posed O
LapSRN O
to O
address O
the O

- B-DAT
curacy O
for O
SISR, O
which O
operates O

- B-DAT

- B-DAT
ages. O
Tai O
et O
al. O
[34 O

- B-DAT
work O
(DRRN) O
to O
address O
the O

- B-DAT

- B-DAT
lutional O
layer O
is O
used O
in O

- B-DAT
ture O
mapping, O
we O
have O

- B-DAT

- B-DAT

- B-DAT
ory O
block O
respectively. O
Finally, O
instead O

- B-DAT

- B-DAT

- B-DAT
age, O
our O
model O
uses O
a O

- B-DAT
notes O
the O
function O
of O
our O

- B-DAT
ber O
of O
training O
patches O
and O

- B-DAT
quality O
patch O
of O
the O
low-quality O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
block. O
Specifically, O
each O
residual O

- B-DAT

- B-DAT

- B-DAT
erate O
multi-level O
representations O
under O
different O

- B-DAT

- B-DAT
ory. O
Supposing O
there O
are O
R O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cursive O
unit. O
These O
representations O
are O

- B-DAT

- B-DAT

- B-DAT
vious O
memory O
blocks O
can O
be O

- B-DAT

- B-DAT
volutional O
layer O
(parameterized O
by O
W O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put O
from O
the O
ensemble O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
can O
get O
lost O
at O

- B-DAT
ward O
CNN O
process, O
and O
dense O

- B-DAT
ous O
layers O
can O
compensate O
such O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferent O
networks. O
(b) O
We O
convert O

- B-DAT

- B-DAT

- B-DAT
tral O
densities O
by O
integrating O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions, O
the O
latter O
layer O
absorbs O

- B-DAT

- B-DAT

- B-DAT
work O
– O
a O
very O
deep O

- B-DAT
works, O
inspired O
by O
LSTM, O
Highway O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tween O
MemNet O
and O
DRCN O
[21 O

- B-DAT
ule O
is O
a O
memory O
block O

- B-DAT
ules O
in O
DRCN, O
which O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
egy, O
which O
is O
imperative O
for O

- B-DAT
nected O
principle. O
In O
general, O
DenseNet O

- B-DAT
tion. O
In O
addition, O
DenseNet O
adopts O

- B-DAT

- B-DAT
tions O
in O
MemNet O
indeed O
play O

- B-DAT

- B-DAT

- B-DAT
nections. O
Average O
PSNR/SSIMs O
for O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ories O
from O
the O
last O
recursion O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tors O
are O
evaluated, O
including O
×2 O

- B-DAT
noising O
is O
used. O
As O
in O

- B-DAT
ing O
time O
and O
storage O
complexities O

- B-DAT

- B-DAT

- B-DAT
ent O
noise O
levels O
are O
all O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sions, O
are O
constructed O
(i.e., O
M6R6 O

- B-DAT
supervised O
MemNet, O
6 O
predictions O
are O

- B-DAT
tions, O
and O
is O
empirically O
set O

- B-DAT
mized O
via O
the O
mini-batch O
stochastic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nections. O
The O
reason O
is O
that O

- B-DAT

- B-DAT

- B-DAT
responding O
weights O
from O
all O
filters O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
we O
normalize O
the O
norms O

- B-DAT
ture O
map O
index O
l. O
We O

- B-DAT
ory O
block O
number O
increases. O
(3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity O
of O
our O
persistent O
memory O

- B-DAT
ent O
among O
different O
work, O
we O

- B-DAT
crease O
the O
parameters O
(filter O
number O

- B-DAT

- B-DAT
mance. O
With O
more O
training O
images O

- B-DAT
nificantly O
outperforms O
the O
state O
of O

- B-DAT

- B-DAT
lem O
in O
networks, O
we O
intend O

- B-DAT
plexity O
and O
accuracy. O
Fig. O
6 O

- B-DAT
notes O
the O
prediction O
of O
the O

- B-DAT
sult O
at O
the O
3rd O
prediction O

- B-DAT
creasing O
model O
complexity O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nal O
image O
is O
resized O
to O

- B-DAT
mance. O
However, O
in O
our O
MemNet O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rectly O
recovers O
the O
pillar. O
Please O

- B-DAT
isons O
for O
SISR. O
SRCNN O
[8 O

- B-DAT
sults O
on O
Classic5 O
and O
LIVE1 O

- B-DAT
erated O
by O
their O
corresponding O
public O

- B-DAT
work O
structures: O
M4R6, O
M6R6, O
M6R8 O

- B-DAT
posed O
deepest O
network O
M10R10 O
achieves O

- B-DAT

- B-DAT

- B-DAT
ory O
network O
(MemNet) O
is O
proposed O

- B-DAT

- B-DAT
ous O
CNN O
architectures. O
In O
each O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ous O
memory O
blocks O
are O
sent O

- B-DAT
resolution O
and O
JPEG O
deblocking O
simultaneously O

- B-DAT
hensive O
benchmark O
evaluations O
well O
demonstrate O

- B-DAT
riority O
of O
our O
MemNet O
over O

- B-DAT

- B-DAT

- B-DAT
ative O
neighbor O
embedding. O
In O
BMVC O

- B-DAT

- B-DAT
age O
denoising O
by O
sparse O
3-D O

- B-DAT

- B-DAT
bridge, O
MA: O
MIT O
Press, O
2001 O

- B-DAT
tifacts O
reduction O
by O
a O
deep O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
volutional O
networks. O
In O
NIPS, O
2008 O

- B-DAT

- B-DAT
ing O
of O
non-parametric O
image O
restoration O

- B-DAT
shick, O
S. O
Guadarrama, O
and O
T O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT
tional O
network O
for O
image O
super-resolution O

- B-DAT

- B-DAT
based O
learning O
applied O
to O
document O

- B-DAT
ings O
of O
the O
IEEE, O
1998 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ric O
skip O
connections. O
In O
NIPS O

- B-DAT
cal O
statistics. O
In O
ICCV, O
2001 O

- B-DAT
stricted O
boltzmann O
machines. O
In O
ICML O

- B-DAT

- B-DAT

- B-DAT
compressed O
images. O
In O
CVPR, O
2016 O

- B-DAT

- B-DAT

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT
up O
using O
sparse-representations. O
Curves O
and O

- B-DAT
yond O
a O
gaussian O
denoiser: O
Residual O

the O
Set5 O
dataset O
with O
an O
upscaling B-DAT
factor O
3). O
The O
proposed O
method O

kernel, O
sub-sample O
it O
by O
the O
upscaling B-DAT
factor, O
and O
upscale O
it O
by O

larger O
Set14 O
set O
[51]. O
The O
upscaling B-DAT
factor O
is O
3. O
We O
use O

on O
the O
ImageNet O
by O
an O
upscaling B-DAT
factor O
3. O
Please O
refer O
to O

our O
published O
implementation O
for O
upscaling B-DAT
factors O
2 O
and O
4. O
Interestingly O

trained O
on O
ImageNet O
with O
an O
upscaling B-DAT
factor O
3. O
The O
filters O
are O

test O
on O
Set5 O
with O
an O
upscaling B-DAT
factor O
3. O
The O
results O
observed O

4.1. O
The O
results O
with O
an O
upscaling B-DAT
factor O
3 O
on O
Set5 O
are O

on O
the O
ImageNet. O
For O
each O
upscaling B-DAT
factor O
∈ O
{2, O
3, O
4 O

to O
evaluate O
the O
performance O
of O
upscaling B-DAT
factors O
2, O
3, O
and O
4 O

108 O
backpropagations. O
Specifically, O
for O
the O
upscaling B-DAT
factor O
3, O
the O
average O
gains O

of O
different O
approaches O
by O
an O
upscaling B-DAT
factor O
3. O
As O
can O
be O

for O
fair O
quantitative O
comparison. O
The O
upscaling B-DAT
factor O
is O
3 O
and O
the O

only O
evaluate O
the O
performance O
of O
upscaling B-DAT
factor O
3. O
Comparisons. O
We O
compare O

network O
to O
cope O
with O
different O
upscaling B-DAT
factors O

Fattal, O
R.: O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

H.: O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In: O
IEEE O

image O
from O
Set5 O
with O
an O
upscaling B-DAT
factor O
3 O

image O
from O
Set14 O
with O
an O
upscaling B-DAT
factor O
3 O

image O
from O
Set14 O
with O
an O
upscaling B-DAT
factor O
3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
offs O
between O
performance O
and O
speed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
image, O
is O
a O
classical O

- B-DAT

- B-DAT
tiplicity O
of O
solutions O
exist O
for O

- B-DAT

- B-DAT
verse O
problem, O
of O
which O
solution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
exemplar O
pairs O
[2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
structing O
high-resolution O
patches. O
The O
overlapping O

- B-DAT

- B-DAT

- B-DAT
work O
[27] O
(more O
details O
in O

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
images. O
Our O
method O

- B-DAT
tally O
from O
existing O
external O
example-based O

- B-DAT
processing O

- B-DAT

- B-DAT
volutional O
Neural O
Network O
(SRCNN)1. O
The O

- B-DAT
ture O
is O
intentionally O
designed O
with O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
example-based O
methods. O
Furthermore, O
the O

- B-DAT

- B-DAT
work O
for O
image O
super-resolution. O
The O

- B-DAT
rectly O
learns O
an O
end-to-end O
mapping O

- B-DAT
and O
high-resolution O
images, O
with O
little O

- B-DAT
processing O
beyond O
the O
optimization O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution, O
and O
can O
achieve O
good O

- B-DAT

- B-DAT
linear O
mapping O
layers. O
Secondly, O
we O

- B-DAT
strate O
that O
performance O
can O
be O

- B-DAT

- B-DAT
ber O
of O
recently O
published O
methods O

- B-DAT

- B-DAT

- B-DAT
olution O
algorithms O
can O
be O
categorized O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
similarity O
property O
and O
generate O
exemplar O

- B-DAT
nal O
example-based O
methods O
[2], O
[4 O

- B-DAT
resolution O
patches O
from O
external O
datasets O

- B-DAT

- B-DAT
tionaries O
are O
directly O
presented O
as O

- B-DAT

- B-DAT

- B-DAT
sponding O
high-resolution O
patch O
used O
for O

- B-DAT
nique O
as O
an O
alternative O
to O

- B-DAT
borhood O
regression O
[41], O
[42] O
are O

- B-DAT
coding-based O
method O
and O
its O
several O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tioned O
methods O
first O
transform O
the O

- B-DAT
ferent O
color O
space O
(YCbCr O
or O

- B-DAT

- B-DAT
fully O
applied O
to O
other O
computer O

- B-DAT

- B-DAT
ceptron O
(MLP), O
whose O
all O
layers O

- B-DAT

- B-DAT

- B-DAT
work O
is O
applied O
for O
natural O

- B-DAT
moving O
noisy O
patterns O
(dirt/rain) O
[12 O

- B-DAT

- B-DAT

- B-DAT
resolution O
pipeline O
under O
the O
notion O

- B-DAT
based O
approach O
[16]. O
The O
deep O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
end O
mapping. O
Further, O
the O
SRCNN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
extracts O
(overlapping) O
patches O
from O

- B-DAT
resolution O
image O
Y O
and O
represents O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
neural O
network. O
An O
overview O

- B-DAT

- B-DAT
spectively, O
and O
’∗’ O
denotes O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tors O
is O
conceptually O
a O
representation O

- B-DAT

- B-DAT

- B-DAT
plexity O
of O
the O
model O
(n2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
patch). O
Motivated O
by O
this O

- B-DAT
lutional O
layer O
to O
produce O
the O

- B-DAT

- B-DAT

- B-DAT
tion O
and O
representation) O
becomes O
purely O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution) O
dictionary. O
If O
the O
dictionary O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
based O
SR O
method O
can O
be O

- B-DAT
volutional O
neural O
network O
(with O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
parameters. O
For O
example, O
we O
can O

- B-DAT
resolution O
patch O
(to O
the O
extreme O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
quires O
the O
estimation O
of O
network O

- B-DAT
imizing O
the O
loss O
between O
the O

- B-DAT
resolution O
images O
X. O
Given O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
crafted” O
methods. O
Despite O
that O
the O

- B-DAT
tory O
performance O
when O
the O
model O

- B-DAT
scent O
with O
the O
standard O
backpropagation O

- B-DAT
ular, O
the O
weight O
matrices O
are O

- B-DAT
erations, O
η O
is O
the O
learning O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ping O
and O
require O
some O
averaging O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
layers O
have O
no O
padding O

- B-DAT

- B-DAT
age O
[26]. O
We O
have O
also O

- B-DAT
tions O
between O
super-resolution O
performance O
and O

- B-DAT
quently, O
we O
compare O
our O
method O

- B-DAT

- B-DAT
the-arts O
both O
quantitatively O
and O
qualitatively O

- B-DAT

- B-DAT
4.4, O
so O
c O
= O
1 O

- B-DAT
tion O
training O
partition. O
The O
size O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nal O
images O
with O
a O
stride O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
geNet O
is O
about O
the O
same O

- B-DAT

- B-DAT
formance O
may O
be O
further O
boosted O

- B-DAT

- B-DAT
tured O
sufficient O
variability O
of O
natural O

- B-DAT

- B-DAT

Laplacian/Gaussian O
filters, O
the O
filters O
a O
- B-DAT
e O
are O
like O
edge O
detectors O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
crease O
the O
network O
width6, O
i.e O

- B-DAT
coding-based O
method O
(31.42 O
dB O

-1 B-DAT

-5 B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-7 B-DAT

- B-DAT
ing O
[17]. O
The O
term O
‘width O

-3 B-DAT

-5 B-DAT

-5 B-DAT

-5 B-DAT

-3 B-DAT

- B-DAT
5 O
and O
9-5-5 O
on O
Set5 O

-1 B-DAT

-5, B-DAT
9-3-5, O
and O
9-5-5 O
is O
8,032 O

-5 B-DAT

-5 B-DAT
is O
almost O
twice O
of O
9-3-5 O

- B-DAT

- B-DAT

-1 B-DAT

-1 B-DAT

-5, B-DAT
9-3-1-5, O
9-5-1-5, O
which O
add O
an O

-1 B-DAT

-5, B-DAT
9-3-5, O
and O
9-5-5, O
respectively. O
The O

- B-DAT
ditional O
layer O
are O
the O
same O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lution O
is O
found O
not O
as O

-1 B-DAT

-5 B-DAT
network, O
then O
the O
performance O
degrades O

- B-DAT

- B-DAT
ure O
9(a)). O
If O
we O
go O

- B-DAT

-1 B-DAT

-5 B-DAT
vs. O
9-1-1-5 O

-3 B-DAT

-5 B-DAT
vs. O
9-3-1-5 O

-5 B-DAT

-5 B-DAT
vs. O
9-5-1-5 O

- B-DAT

- B-DAT

-1 B-DAT

-5, B-DAT
then O
we O
have O
to O
set O

-3 B-DAT

- B-DAT
3-5 O
and O
9-3-3-3. O
However, O
from O

-3 B-DAT

-1 B-DAT

-5 B-DAT
network O

- B-DAT

- B-DAT

- B-DAT
vestigations O
to O
better O
understand O
gradients O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-1 B-DAT

-5 B-DAT
(n22 O
= O
32) O
and O
9-1-1-1-5 O

-3 B-DAT

-3 B-DAT

-5 B-DAT
and O
9-3-3-3 O

- B-DAT
speed O
trade-off: O
a O
three-layer O
network O

- B-DAT
of-the-art O
SR O
methods O

SC O
- B-DAT
sparse O
coding-based O
method O
of O
Yang O

et O
al. O
[50] O
• O
NE+LLE O
- B-DAT
neighbour O
embedding O
+ O
locally O
linear O

embedding O
method O
[4] O
• O
ANR O
- B-DAT
Anchored O
Neighbourhood O
Regression O

method O
[41] O
• O
A+ O
- B-DAT
Adjusted O
Anchored O
Neighbourhood O
Regres O

method O
[42], O
and O
• O
KK O
- B-DAT
the O
method O
described O
in O
[25 O

- B-DAT
based O
methods, O
according O
to O
the O

- B-DAT
sampled O
using O
the O
same O
bicubic O

- B-DAT
terion O
(IFC) O
[38], O
noise O
quality O

- B-DAT

- B-DAT

- B-DAT
scale O
structure O
similarity O
index O
(MSSSIM O

ANR O
- B-DAT
31.92 O
dB O

A+ O
- B-DAT
32.59 O
dB O

SC O
- B-DAT
31.42 O
dB O

Bicubic O
- B-DAT
30.39 O
dB O

NE+LLE O
- B-DAT
31.84 O
dB O

KK O
- B-DAT
32.28 O
dB O

- B-DAT
CNN O
outperforms O
existing O
state-of-the-art O
methods O

- B-DAT

- B-DAT

- B-DAT
cific O
network O
(9-5-5) O
using O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentation, O
whereas O
ours O
are O
in O

- B-DAT

-1 B-DAT

-5, B-DAT
9-3-5, O
and O
9-5-5. O
It O
is O

- B-DAT
1-5 O
network O
is O
the O
fastest O

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-5 B-DAT
network. O
Note O
the O
speed O
gap O

- B-DAT
pletely O
feed-forward. O
The O
9-5-5 O
network O

- B-DAT

- B-DAT

- B-DAT
terpolation. O
It O
is O
interesting O
to O

- B-DAT

- B-DAT
out O
altering O
the O
learning O
mechanism O

- B-DAT
sign. O
In O
particular, O
it O
can O

- B-DAT
nels O
simultaneously O
by O
setting O
the O

- B-DAT

- B-DAT

-5 B-DAT

- B-DAT

A+ O
[41] O
SRCNN O
2 O
33.66 O
- B-DAT
35.77 O
36.20 O
35.83 O
36.54 O
36.66 O

31.92 O
32.59 O
32.75 O
4 O
28.42 O
- B-DAT
29.61 O
30.03 O
29.69 O
30.28 O
30.49 O

2 O
0.9299 O
- B-DAT
0.9490 O
0.9511 O
0.9499 O
0.9544 O
0.9542 O

0.8968 O
0.9088 O
0.9090 O
4 O
0.8104 O
- B-DAT
0.8402 O
0.8541 O
0.8419 O
0.8603 O
0.8628 O

2 O
6.10 O
- B-DAT
7.84 O
6.87 O
8.09 O
8.48 O
8.05 O

4.52 O
4.84 O
4.58 O
4 O
2.35 O
- B-DAT
2.94 O
2.81 O
3.02 O
3.26 O
3.01 O

2 O
36.73 O
- B-DAT
42.90 O
39.49 O
43.28 O
44.58 O
41.13 O

33.10 O
34.48 O
33.21 O
4 O
21.42 O
- B-DAT
25.56 O
24.99 O
25.72 O
26.97 O
25.96 O

2 O
50.06 O
- B-DAT
58.45 O
57.15 O
58.61 O
60.06 O
59.49 O

46.02 O
47.17 O
47.10 O
4 O
37.21 O
- B-DAT
39.85 O
40.40 O
40.01 O
41.03 O
41.13 O

2 O
0.9915 O
- B-DAT
0.9953 O
0.9953 O
0.9954 O
0.9960 O
0.9959 O

0.9844 O
0.9867 O
0.9866 O
4 O
0.9516 O
- B-DAT
0.9666 O
0.9695 O
0.9672 O
0.9720 O
0.9725 O

A+ O
[41] O
SRCNN O
2 O
30.23 O
- B-DAT
31.76 O
32.11 O
31.80 O
32.28 O
32.45 O

28.65 O
29.13 O
29.30 O
4 O
26.00 O
- B-DAT
26.81 O
27.14 O
26.85 O
27.32 O
27.50 O

2 O
0.8687 O
- B-DAT
0.8993 O
0.9026 O
0.9004 O
0.9056 O
0.9067 O

0.8093 O
0.8188 O
0.8215 O
4 O
0.7019 O
- B-DAT
0.7331 O
0.7419 O
0.7352 O
0.7491 O
0.7513 O

2 O
6.09 O
- B-DAT
7.59 O
6.83 O
7.81 O
8.11 O
7.76 O

4.23 O
4.45 O
4.26 O
4 O
2.23 O
- B-DAT
2.71 O
2.57 O
2.78 O
2.94 O
2.74 O

2 O
40.98 O
- B-DAT
41.34 O
38.86 O
41.79 O
42.61 O
38.95 O

37.22 O
38.24 O
35.25 O
4 O
26.15 O
- B-DAT
31.17 O
29.18 O
31.27 O
32.31 O
30.46 O

2 O
47.64 O
- B-DAT
54.47 O
53.85 O
54.57 O
55.62 O
55.39 O

43.36 O
44.25 O
44.32 O
4 O
35.71 O
- B-DAT
37.75 O
38.26 O
37.85 O
38.72 O
38.87 O

2 O
0.9813 O
- B-DAT
0.9886 O
0.9890 O
0.9888 O
0.9896 O
0.9897 O

0.9647 O
0.9669 O
0.9675 O
4 O
0.9134 O
- B-DAT
0.9317 O
0.9338 O
0.9326 O
0.9371 O
0.9376 O

A+ O
[41] O
SRCNN O
2 O
28.38 O
- B-DAT
29.67 O
30.02 O
29.72 O
30.14 O
30.29 O

26.72 O
27.05 O
27.18 O
4 O
24.65 O
- B-DAT
25.21 O
25.38 O
25.25 O
25.51 O
25.60 O

2 O
0.8524 O
- B-DAT
0.8886 O
0.8935 O
0.8900 O
0.8966 O
0.8977 O

0.7843 O
0.7945 O
0.7971 O
4 O
0.6727 O
- B-DAT
0.7037 O
0.7093 O
0.7060 O
0.7171 O
0.7184 O

2 O
5.30 O
- B-DAT
7.10 O
6.33 O
7.28 O
7.51 O
7.21 O

3.91 O
4.07 O
3.91 O
4 O
1.95 O
- B-DAT
2.45 O
2.24 O
2.51 O
2.62 O
2.45 O

2 O
36.84 O
- B-DAT
41.52 O
38.54 O
41.72 O
42.37 O
39.66 O

34.81 O
35.58 O
34.72 O
4 O
21.72 O
- B-DAT
25.15 O
24.87 O
25.27 O
26.01 O
25.65 O

2 O
46.15 O
- B-DAT
52.56 O
52.21 O
52.69 O
53.56 O
53.58 O

41.53 O
42.19 O
42.29 O
4 O
34.86 O
- B-DAT
36.52 O
36.80 O
36.64 O
37.18 O
37.24 O

2 O
0.9780 O
- B-DAT
0.9869 O
0.9876 O
0.9872 O
0.9883 O
0.9883 O

0.9581 O
0.9609 O
0.9614 O
4 O
0.9005 O
- B-DAT
0.9203 O
0.9215 O
0.9214 O
0.9256 O
0.9261 O

-1 B-DAT

-5 B-DAT

-3 B-DAT

-5 B-DAT

-5 B-DAT

-5 B-DAT

- B-DAT
of-the-art O
super-resolution O
quality, O
whilst O
maintains O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-art O
color O
SR O
method O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nel O
when O
training O
is O
performed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
correlation O
among O
each O
other. O
The O

- B-DAT

- B-DAT
channel O
network O
(“Y O
only”). O
It O

- B-DAT

- B-DAT
work O
is O
not O
that O
significant O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
images, O
with O
little O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vantages O
of O
simplicity O
and O
robustness O

- B-DAT

- B-DAT
blurring O
or O
simultaneous O
SR+denoising. O
One O

- B-DAT

- B-DAT
complexity O
single-image O
super-resolution O
based O
on O

- B-DAT

- B-DAT
bor O
embedding. O
In: O
IEEE O
Conference O

- B-DAT

- B-DAT

- B-DAT
resolution. O
IEEE O
Transactions O
on O
Image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
linear O
structure O
within O
convolutional O

- B-DAT
tems O
(2014 O

- B-DAT
tional O
network O
for O
image O
super-resolution O

- B-DAT
ence O
on O
Computer O
Vision, O
pp O

- B-DAT
tional O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT

- B-DAT
resolution. O
Computer O
Graphics O
and O
Applications O

- B-DAT
level O
vision. O
International O
Journal O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
lutional O
neural O
networks O
with O
low O

- B-DAT
tems. O
pp. O
769–776 O
(2008 O

- B-DAT

- B-DAT

- B-DAT
ten O
zip O
code O
recognition. O
Neural O

- B-DAT

- B-DAT
rithms. O
In: O
Advances O
in O
Neural O

- B-DAT
mentation O
algorithms O
and O
measuring O
ecological O

- B-DAT

- B-DAT

- B-DAT
tion. O
In: O
IEEE O
International O
Conference O

- B-DAT
chine O
learning O
approach O
for O
non-blind O

- B-DAT

- B-DAT
tics. O
IEEE O
Transactions O
on O
Image O

- B-DAT
tation O
by O
joint O
identification-verification. O
In O

- B-DAT
quality O
object O
detection. O
arXiv O
preprint O

- B-DAT

- B-DAT

- B-DAT
ternational O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
ilarity O
for O
image O
quality O
assessment O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tionary O
training O
for O
image O
super-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
sparse-representations. O
In: O
Curves O
and O

- B-DAT

- B-DAT
CNNs O
for O
fine-grained O
category O
detection O

- B-DAT
ence O
on O
Computer O
Vision. O
pp O

- B-DAT
tion O
Engineering O
from O
Beijing O
Institute O

- B-DAT
nology, O
China, O
in O
2011. O
He O

- B-DAT
sity O
of O
Hong O
Kong. O
His O

- B-DAT

- B-DAT
versity O
of O
London O
in O
2010 O

- B-DAT
toral O
researcher O
at O
Vision O
Semantics O

- B-DAT
inghua O
University O
in O
2007, O
and O

- B-DAT
ence O
on O
Computer O
Vision O
and O

- B-DAT
tion O
(CVPR) O
2009. O
He O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sachusetts O
Institute O
of O
Technology, O
Cambridge O

- B-DAT
cessing. O
He O
received O
the O
Best O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Set5 O
Set14 O
BSD100 O
Urban100 O
Manga109 B-DAT
PSNR/SSIM O
PSNR/SSIM O
PSNR/SSIM O
PSNR/SSIM O
PSNR/SSIM O

Set5 O
Set14 O
BSD100 O
Urban100 O
Manga109 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
GAN) O
[1] O
is O
a O
seminal O

- B-DAT

- B-DAT
lar, O
we O
introduce O
the O
Residual-in-Residual O

- B-DAT
vide O
stronger O
supervision O
for O
brightness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lem, O
has O
attracted O
increasing O
attention O

- B-DAT
panies. O
SISR O
aims O
at O
recovering O

- B-DAT

- B-DAT

- B-DAT
perous O
development. O
Various O
network O
architecture O

- B-DAT

- B-DAT
Noise O
Ratio O
(PSNR) O
value O
[5,6,7,1,8,9,10,11,12 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uation O
of O
human O
observers O
[1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mize O
super-resolution O
model O
in O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
RGAN, O
consistently O
outperforms O
state-of-the-art O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
struction O
style O
and O
smoothness. O
Another O

- B-DAT
pate O
in O
region O
1 O
and O

- B-DAT

- B-DAT

- B-DAT
tures, O
such O
as O
a O
deeper O

- B-DAT
work O
[9], O
deep O
back O
projection O

- B-DAT
provement. O
Zhang O
et O
al. O
[11 O

- B-DAT
ing O
the O
state-of-the-art O
PSNR O
performance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity O
[29,14], O
perceptual O
loss O
[13 O

- B-DAT
imizing O
the O
error O
in O
a O

- B-DAT
pearance. O
Ledig O
et O
al. O
[1 O

- B-DAT
jadi O
et O
al. O
[16] O
develop O

- B-DAT

- B-DAT
veloping O
more O
effective O
GAN O
frameworks O

- B-DAT
tor O
includes O
gradient O
clipping O
[32 O

- B-DAT
ated O
data O
are O
real, O
but O

- B-DAT
sures, O
e.g., O
PSNR O
and O
SSIM O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lenge O
[3]. O
In O
a O
recent O

- B-DAT
tion, O
we O
first O
describe O
our O

- B-DAT
tion O
is O
done O
in O
the O

- B-DAT
ers; O
2) O
replace O
the O
original O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
putational O
complexity O
in O
different O
PSNR-oriented O

- B-DAT
ing O
dataset O
during O
testing. O
When O

- B-DAT
alization O
ability. O
We O
empirically O
observe O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
residual O
network. O
However, O
our O

- B-DAT
erage O
Discriminator O
RaD O
[2], O
denoted O

- B-DAT

- B-DAT
mulated O
as O
DRa(xr, O
xf O

- B-DAT

- B-DAT

- B-DAT
tures O
before O
activation O
rather O
than O

- B-DAT

-543 B-DAT
layer O
is O
merely O
11.17%. O
The O

- B-DAT

- B-DAT

- B-DAT
tance O
between O
recovered O
image O
G(xi O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nition O
[38], O
which O
focuses O
on O

- B-DAT
ing O
perceptual O
loss O
that O
focuses O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tures O
and O
similarly, O
22 O
represents O

- B-DAT

-22 B-DAT
b) O
activation O
map O
of O

-54 B-DAT

- B-DAT
boon’. O
With O
the O
network O
going O

- B-DAT

- B-DAT
ceptual O
quality, O
we O
propose O
a O

- B-DAT
tion. O
Specifically, O
we O
first O
train O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
parameters O
of O
these O
two O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
polated O
image O
is O
either O
too O

- B-DAT
rameter O
λ O
and O
η O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
oriented O
model O
with O
the O
L1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els O
on O
widely O
used O
benchmark O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
the-art O
PSNR-oriented O
methods O
including O
SRCNN O

- B-DAT

- B-DAT
tures, O
e.g., O
animal O
fur, O
building O

- B-DAT
pleasant O
artifacts, O
e.g., O
artifacts O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults, O
and O
than O
previous O
GAN-based O

- B-DAT
tures O
in O
building O
(see O
image O

- B-DAT

- B-DAT
mance O
without O
artifacts. O
It O
does O

- B-DAT
ment O
can O
be O
observed O
from O

- B-DAT
tures O
before O
activation O
can O
result O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gies O
in O
balancing O
the O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
oriented O
method O
outputs O
cartoon-style O
blurry O

- B-DAT

- B-DAT

- B-DAT
pirically O
make O
some O
modifications O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
terpolation O
between O
the O
results O
of O

- B-DAT

- B-DAT

- B-DAT
ceptual O
quality O
than O
previous O
SR O

- B-DAT

- B-DAT
dition, O
useful O
techniques O
including O
residual O

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In: O
CVPR. O
(2018 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
performance O
on O
imagenet O
classification O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
representations. O
In: O
International O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tics. O
(2010 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
duce O
several O
useful O
techniques O
that O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
a O
very O
deep O
network O

- B-DAT

- B-DAT
Residual O
Dense O
Block O
(RRDB), O
which O

- B-DAT
ers O
[47,28]. O
He O
et O
al O

- B-DAT

- B-DAT
tially. O
It O
is O
worth O
noting O

- B-DAT
tion O
(multiplying O
0.1 O
for O
all O

- B-DAT
tremely O
bad O
local O
minimum O
with O

- B-DAT
tion O
(×0.1) O
helps O
the O
network O

- B-DAT
tion O
achieves O
a O
higher O
PSNR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rithms: O
average O
PSNR/SSIM O
on O
Y O

Bicubic O
- B-DAT
28.42/0.8104 O
26.00/0.7027 O
25.96/0.6675 O
23.14/0.6577 O
24.89/0.7866 O

- B-DAT

- B-DAT
verse O
natural O
textures. O
We O
employ O

- B-DAT
over, O
the O
deeper O
model O
achieves O

- B-DAT

- B-DAT

- B-DAT

Evaluation O
on O
BSD200 B-DAT
For O
the O
BSD O
dataset, O
300 O

BSD200 B-DAT
σ O
= O
10 O
σ O

The O
way O
of O
embedding O
upscaling B-DAT
feature O
in O
the O
last O
few O

BSD100, O
Urban100 B-DAT

SR O
with O
BI O
model O
on O
Urban100 B-DAT

for O
datasets(e.g., O
Urban100 B-DAT

Method O
Set5 O
Set14 O
BSD100 O
Urban100 B-DAT

Method O
Set5 O
Set14 O
BSD100 O
Urban100 B-DAT

up O
to O
0.4 O
dB O
on O
Urban100 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
traction O
module, O
which O
consists O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
is O
optimized O
by O
stochastic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
plified O
residual O
blocks O
with O
local-source O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-1 B-DAT
Fg O

- B-DAT

- B-DAT
NL O

- B-DAT
NL O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-1 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sentations. O
Thus, O
we O
set O
α O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
spectively. O
f(·) O
and O
δ(·) O
are O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ture, O
and O
embed O
RL-NL O
modules O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tains O
the O
convolution O
layers O
with O

- B-DAT
ble O
1 O
we O
can O
see O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

SR O
on O
“img O
092” O
from O
Urban100 B-DAT

BSD100, O
Urban100 B-DAT
and O
Manga109, O
each O
of O
which O

Urban100 B-DAT
(4×): O
img O
067 O

Urban100 B-DAT
(4×): O
img O
076 O

SR O
with O
BI O
model O
on O
Urban100 B-DAT
dataset. O
The O
best O
results O
are O

for O
datasets(e.g., O
Urban100 B-DAT
and O
Manga109) O
with O
rich O
re O

Method O
Set5 O
Set14 O
BSD100 O
Urban100 B-DAT
Manga109 O

Method O
Set5 O
Set14 O
BSD100 O
Urban100 B-DAT
Manga109 O

up O
to O
0.4 O
dB O
on O
Urban100 B-DAT
and O
Manga109 O
datasets O

Set5 O
Set14 O
BSD100 O
Urban100 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
GAN) O
[1] O
is O
a O
seminal O

- B-DAT

- B-DAT
lar, O
we O
introduce O
the O
Residual-in-Residual O

- B-DAT
vide O
stronger O
supervision O
for O
brightness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lem, O
has O
attracted O
increasing O
attention O

- B-DAT
panies. O
SISR O
aims O
at O
recovering O

- B-DAT

- B-DAT

- B-DAT
perous O
development. O
Various O
network O
architecture O

- B-DAT

- B-DAT
Noise O
Ratio O
(PSNR) O
value O
[5,6,7,1,8,9,10,11,12 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uation O
of O
human O
observers O
[1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mize O
super-resolution O
model O
in O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
RGAN, O
consistently O
outperforms O
state-of-the-art O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
struction O
style O
and O
smoothness. O
Another O

- B-DAT
pate O
in O
region O
1 O
and O

- B-DAT

- B-DAT

- B-DAT
tures, O
such O
as O
a O
deeper O

- B-DAT
work O
[9], O
deep O
back O
projection O

- B-DAT
provement. O
Zhang O
et O
al. O
[11 O

- B-DAT
ing O
the O
state-of-the-art O
PSNR O
performance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity O
[29,14], O
perceptual O
loss O
[13 O

- B-DAT
imizing O
the O
error O
in O
a O

- B-DAT
pearance. O
Ledig O
et O
al. O
[1 O

- B-DAT
jadi O
et O
al. O
[16] O
develop O

- B-DAT

- B-DAT
veloping O
more O
effective O
GAN O
frameworks O

- B-DAT
tor O
includes O
gradient O
clipping O
[32 O

- B-DAT
ated O
data O
are O
real, O
but O

- B-DAT
sures, O
e.g., O
PSNR O
and O
SSIM O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lenge O
[3]. O
In O
a O
recent O

- B-DAT
tion, O
we O
first O
describe O
our O

- B-DAT
tion O
is O
done O
in O
the O

- B-DAT
ers; O
2) O
replace O
the O
original O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
putational O
complexity O
in O
different O
PSNR-oriented O

- B-DAT
ing O
dataset O
during O
testing. O
When O

- B-DAT
alization O
ability. O
We O
empirically O
observe O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
residual O
network. O
However, O
our O

- B-DAT
erage O
Discriminator O
RaD O
[2], O
denoted O

- B-DAT

- B-DAT
mulated O
as O
DRa(xr, O
xf O

- B-DAT

- B-DAT

- B-DAT
tures O
before O
activation O
rather O
than O

- B-DAT

-543 B-DAT
layer O
is O
merely O
11.17%. O
The O

- B-DAT

- B-DAT

- B-DAT
tance O
between O
recovered O
image O
G(xi O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nition O
[38], O
which O
focuses O
on O

- B-DAT
ing O
perceptual O
loss O
that O
focuses O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tures O
and O
similarly, O
22 O
represents O

- B-DAT

-22 B-DAT
b) O
activation O
map O
of O

-54 B-DAT

- B-DAT
boon’. O
With O
the O
network O
going O

- B-DAT

- B-DAT
ceptual O
quality, O
we O
propose O
a O

- B-DAT
tion. O
Specifically, O
we O
first O
train O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
parameters O
of O
these O
two O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
polated O
image O
is O
either O
too O

- B-DAT
rameter O
λ O
and O
η O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
oriented O
model O
with O
the O
L1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els O
on O
widely O
used O
benchmark O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
the-art O
PSNR-oriented O
methods O
including O
SRCNN O

- B-DAT

- B-DAT
tures, O
e.g., O
animal O
fur, O
building O

- B-DAT
pleasant O
artifacts, O
e.g., O
artifacts O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults, O
and O
than O
previous O
GAN-based O

- B-DAT
tures O
in O
building O
(see O
image O

- B-DAT

- B-DAT
mance O
without O
artifacts. O
It O
does O

- B-DAT
ment O
can O
be O
observed O
from O

- B-DAT
tures O
before O
activation O
can O
result O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gies O
in O
balancing O
the O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
oriented O
method O
outputs O
cartoon-style O
blurry O

- B-DAT

- B-DAT

- B-DAT
pirically O
make O
some O
modifications O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
terpolation O
between O
the O
results O
of O

- B-DAT

- B-DAT

- B-DAT
ceptual O
quality O
than O
previous O
SR O

- B-DAT

- B-DAT
dition, O
useful O
techniques O
including O
residual O

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In: O
CVPR. O
(2018 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
performance O
on O
imagenet O
classification O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
representations. O
In: O
International O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tics. O
(2010 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
duce O
several O
useful O
techniques O
that O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
a O
very O
deep O
network O

- B-DAT

- B-DAT
Residual O
Dense O
Block O
(RRDB), O
which O

- B-DAT
ers O
[47,28]. O
He O
et O
al O

- B-DAT

- B-DAT
tially. O
It O
is O
worth O
noting O

- B-DAT
tion O
(multiplying O
0.1 O
for O
all O

- B-DAT
tremely O
bad O
local O
minimum O
with O

- B-DAT
tion O
(×0.1) O
helps O
the O
network O

- B-DAT
tion O
achieves O
a O
higher O
PSNR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rithms: O
average O
PSNR/SSIM O
on O
Y O

Bicubic O
- B-DAT
28.42/0.8104 O
26.00/0.7027 O
25.96/0.6675 O
23.14/0.6577 O
24.89/0.7866 O

- B-DAT

- B-DAT
verse O
natural O
textures. O
We O
employ O

- B-DAT
over, O
the O
deeper O
model O
achieves O

- B-DAT

- B-DAT

- B-DAT

42], O
Set14 O
[43], O
BSD100 O
[44], O
Urban100 B-DAT
[45], O
and O
the O
PIRM O
self-validation O

Set5 O
Set14 O
BSD100 O
Urban100 B-DAT
Manga109 O
PSNR/SSIM O
PSNR/SSIM O
PSNR/SSIM O
PSNR/SSIM O

the O
problem O
of O
estimating O
an O
upscaling B-DAT
function O
u O
: O
X O

progressive O
solution O
to O
learn O
the O
upscaling B-DAT
function O
u. O
In O
the O
following O

where O
ϕ2 O
denotes O
an O
upscaling B-DAT
operator O
by O
a O
factor O
of O

in O
the O
challenge O
target O
4× O
upscaling B-DAT
but O
consider O
unknown O
degradation. O
Given O

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
have O
achieved O
impressive O
results O

- B-DAT
ditional O
error O
measures O
and O
perceptual O

- B-DAT
sults O
for O
large O
upsampling O
factors O

- B-DAT

- B-DAT
lows O
to O
scale O
well O
to O

- B-DAT

- B-DAT
taneously. O
In O
particular O
ProSR O
ranks O

- B-DAT
lenge O
[34]. O
Compared O
to O
the O

- B-DAT

- B-DAT
cessing O
has O
recently O
sparked O
increased O

- B-DAT
resolution. O
In O
particular, O
approaches O
to O

- B-DAT

- B-DAT
resolution O
(HR) O
images O
based O
on O

- B-DAT
scaling O
function O
is O
a O
deep O

- B-DAT
lowing O
direct O
approaches. O
The O
first O

- B-DAT
ginning O
and O
then O
essentially O
learns O

- B-DAT

- B-DAT
nation O
of O
upsampling O
layers. O
Thus O

- B-DAT

- B-DAT
istic O
results, O
we O
adopt O
the O

- B-DAT
tain O
a O
multi-scale O
generator O
with O

- B-DAT
riculum O
learning, O
which O
is O
known O

- B-DAT
pling O
factors) O
to O
hard O
(large O

- B-DAT

- B-DAT
egy O
not O
only O
improves O
results O

- B-DAT
lizes O
the O
GAN O
training O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cally O
been O
tackled O
using O
statistical O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cently, O
Dong O
et O
al. O
[6 O

- B-DAT

- B-DAT
ing O
techniques. O
Since O
then, O
deep O

- B-DAT

- B-DAT

- B-DAT
struction O
techniques O
[7, O
20, O
23 O

- B-DAT
age O
to O
the O
desired O
spatial O

- B-DAT
processing O
step. O
Thus, O
the O
CNN O

- B-DAT

- B-DAT
putationally O
expensive O
[30]. O
To O
overcome O

- B-DAT

- B-DAT
scribed O
by O
LapSRN O
by O
Lai O

- B-DAT
sampling O
follows O
the O
principle O
of O

- B-DAT
puted O
at O
each O
scale, O
this O

- B-DAT
pervision. O
Lai O
et O
al. O
improved O

- B-DAT

- B-DAT
erable O
gap O
between O
the O
top-performing O

- B-DAT
tion O
difficulty. O
Furthermore, O
the O
recursive O

- B-DAT
criminator O
along O
with O
a O
progressive O

- B-DAT
niques O
optimize O
the O
reconstruction O
error O

- B-DAT

- B-DAT
construction O
errors, O
they O
are O
unable O

- B-DAT
ally O
plausible O
high-frequencies O
details. O
To O

- B-DAT
versary O
to O
steer O
the O
reconstruction O

- B-DAT
ifold O
of O
natural O
solutions. O
Based O

- B-DAT
riculum O
learning. O
With O
this, O
our O

- B-DAT
sample O
perceptually O
pleasing O
SR O
images O

- B-DAT

- B-DAT

- B-DAT
scaling O
function O
u O
for O
large O

- B-DAT
ing: O
the O
larger O
the O
ratio O

- B-DAT

- B-DAT
resolution O
in O
Section O
3.1 O
and O

- B-DAT

- B-DAT
forming O
a O
2× O
upsampling O
of O

- B-DAT

- B-DAT
sign O
more O
DCUs O
in O
the O

- B-DAT
sumption O
but O
also O
increases O
the O

- B-DAT
ric O
variant O
in O
terms O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ture O
space. O
A O
schematic O
illustration O

- B-DAT
sampling O
architecture O
is O
detailed O
in O

- B-DAT
mid O
principle O
like O
in O
[21 O

- B-DAT

- B-DAT

- B-DAT
fied O
densely O
connected O
block O
followed O

- B-DAT

- B-DAT
CONV(1,1)-BN-RELU-CONV(3,3). O
Following O
recent O
practice O
in O

- B-DAT

-1 B-DAT

- B-DAT
CONV(3,3 O

- B-DAT

- B-DAT

- B-DAT
agation O
as O
shown O
in O
Figure O

- B-DAT
cess O
at O
applying O
GANs O
to O

- B-DAT
scale O
upsampling O
at O
relatively O
low O

- B-DAT
der O
to O
enable O
multi-scale O
GAN-enhanced O

- B-DAT
mension O
of O
the O
input O
image O

- B-DAT
modate O
the O
multi-scale O
outputs O
from O

- B-DAT
work O
is O
fully O
convolutional O
and O

- B-DAT
tures O
similar O
to O
PatchGAN O
[18 O

- B-DAT
erates O
on O
the O
residual O
between O

- B-DAT
sampled O
image. O
This O
allows O
both O

- B-DAT
nator O
to O
concentrate O
only O
on O

- B-DAT
ation O
which O
are O
not O
already O

- B-DAT
tual O
errors. O
This O
can O
also O

- B-DAT
dependent O
baseline O
from O
the O
discriminator O

- B-DAT

- B-DAT

- B-DAT
rent O
scale O
(u0, O
v0, O
r0 O

- B-DAT
midal O
network O
shown O
in O
Figure O

- B-DAT
vious O
level. O
A O
similar O
idea O

- B-DAT
sult O
we O
incrementally O
add O
training O

- B-DAT

- B-DAT
formance O
gain O
for O
all O
included O

- B-DAT
scale O
and O
simple O
multi-scale O
training O

- B-DAT
ities O
in O
GAN O
training O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
posed O
components O
using O
a O
small O

- B-DAT

- B-DAT

- B-DAT
tions O
are O
conducted O
on O
the O

- B-DAT
struction O
quality O
stemming O
from O
each O

- B-DAT

- B-DAT
nection O
from O
the O
LR O
input O

- B-DAT
ing, O
we O
describe O
the O
individual O

- B-DAT

- B-DAT
vantage O
of O
the O
proposed O
asymmetric O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ric O
pyramid O
model O
to O
8 O

- B-DAT
efit O
of O
curriculum O
learning O
over O

- B-DAT

- B-DAT
struction O
quality O
and O
outperforms O
simultaneous O

- B-DAT
neous O
training, O
since O
the O
2 O

- B-DAT
tion O
and O
hence O
less O
time O

- B-DAT
tures O

- B-DAT
pled O
LR O
image. O
Thus O
the O

- B-DAT
ble O
3. O
Therefore, O
we O
conclude O

- B-DAT

- B-DAT
pared O
to O
fixed O
interpolated O
results O

- B-DAT
sampling O
kernel O
to O
create O
the O

- B-DAT
duce O
undesired O
artefacts O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
proaches O

- B-DAT
son, O
we O
benchmark O
against O
VDSR O

- B-DAT
SRN O
[21], O
MsLapSRN O
[22], O
EDSR O

- B-DAT

simultaneous O
-0 B-DAT

- B-DAT

ours O
- B-DAT
27.44 O
- O
- O
28.41 O
- O
alt O
- O
27.32 O

- B-DAT

- B-DAT
proaches, O
we O
divide O
them O
into O

- B-DAT
ingly, O
we O
provide O
two O
models O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tween O
our O
results O
and O
the O

- B-DAT

- B-DAT

- B-DAT
imise O
the O
ℓ1 O
loss O
or O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mental O
material O

- B-DAT
erage O
0.8s, O
2.1s O
and O
4.4s O

- B-DAT

- B-DAT

- B-DAT
scaling, O
where O
the O
low O
resolution O

- B-DAT
idation O
set. O
Our O
model O
ranks O

- B-DAT

- B-DAT
ferent O
to O
the O
bicubic O
8 O

- B-DAT
nario, O
we O
also O
participated O
in O

- B-DAT

- B-DAT
ther O
improvement O
can O
be O
achieved O

- B-DAT
ing O
and O
extended O
training O
data O

- B-DAT
eling O
power, O
we O
have O
proposed O

- B-DAT
ity. O
Furthermore O
we O
leverage O
a O

- B-DAT

- B-DAT
formance O
for O
all O
scales. O
Our O

- B-DAT

- B-DAT
the-art O
benchmark O
in O
terms O
of O

- B-DAT
performs O
existing O
methods O
by O
a O

- B-DAT
plied O
to O
GAN-extended O
method O
for O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
scale O
model O
that O
could O
yield O

24.57 O
24.65 O
22.06 O
26.52 O
SRDenseNet O
- B-DAT
- O
- O
- O
28.50 O
27.53 O

26.05 O
- B-DAT
- O
- O
- O
ProSRs O
(ours) O
33.36 O
32.02 O
31.42 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cup O
and O
Y. O
W. O
Teh O

- B-DAT
national O
Conference O
on O
Machine O
Learning O

- B-DAT
riculum O
learning. O
In O
Proceedings O
of O

- B-DAT
tional O
conference O
on O
machine O
learning O

- B-DAT
Morel. O
Low-complexity O
single-image O
super-resolution O
based O

super-resolution. B-DAT
In O
Com- O
puter O
Vision O
- O
ECCV O
2014 O
- O
13th O
European O

-12, B-DAT
2014, O
Proceedings, O
Part O
IV, O
pages O

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
ference O
on O
Computer O
Vision, O
pages O

- B-DAT

- B-DAT
works O
for O
image O
super-resolution. O
In O

- B-DAT
ference O
on, O
pages O
1157–1164. O
IEEE O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-8, B-DAT
2013, O
pages O
3336–3343, O
2013. O
2 O

- B-DAT

- B-DAT
based O
super-resolution. O
IEEE O
Computer O
Graphics O

- B-DAT
plications, O
22(2):56–65, O
2002. O
2 O

- B-DAT

2009, O
Kyoto, O
Japan, O
September O
27 O
- B-DAT
October O
4, O
2009, O
pages O
349–356 O

- B-DAT

- B-DAT

- B-DAT
ing O
for O
image O
recognition. O
In O

- B-DAT
ference O
on O
computer O
vision O
and O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
to-image O
translation O
with O
conditional O
adversarial O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT
resolution. O
In O
IEEE O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ative O
adversarial O
network. O
arXiv O
preprint O

- B-DAT

- B-DAT
ley. O
Least O
squares O
generative O
adversarial O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pling. O
ACM O
Trans. O
Graph., O
27(5):153:1–153:7 O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

-26 B-DAT
June O
2008, O
Anchorage, O
Alaska, O
USA O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT
shops, O
July O
2017. O
1, O
5 O

- B-DAT
borhood O
regression O
for O
fast O
example-based O

- B-DAT

-8, B-DAT
2013, O
pages O
1920–1927, O
2013. O
2 O

- B-DAT

- B-DAT
resolution O
as O
sparse O
representation O
of O

-26 B-DAT
June O
2008, O
Anchorage, O
Alaska, O
USA O

- B-DAT
age O
super-resolution O
with O
a O
parameter O

- B-DAT

- B-DAT
up O
using O
sparse-representations. O
In O
Curves O

and O
Surfaces O
- B-DAT
7th O
International O
Conference, O
Avignon, O
France O

-30, B-DAT
2010, O
Revised O
Selected O
Papers, O
pages O

- B-DAT

- B-DAT

5], O
Set14 O
[40], O
BSD100 O
[1], O
Urban100 B-DAT
[17], O
and O
the O
DIV2K O
validation O

the O
original O
LR O
inputs O
and O
upscaling B-DAT
spatial O
reso- O
lution O
at O
the O

upscaling B-DAT
strategy O
has O
been O
demon- O
strated O

upscaling B-DAT
SR O
methods O
(e.g., O
DRRN O
[5 O

shallow O
feature O
extraction O
HSF O
(·), O
upscaling B-DAT
module O
HUP O
(·), O
and O
reconstruction O

upscaling B-DAT
layer, O
whose O
weight O
set O
is O

upscaling, B-DAT
whose O
kernel O
size O
is O
1×1 O

is O
set O
as O
16. O
For O
upscaling B-DAT
module O
HUP O
(·), O
we O
follow O

4×) O
on O
“img O
074” O
from O
Urban100 B-DAT
VDSR I-DAT
[4] O
and O
DRCN O
[19 O

Method O
Scale O
Set5 O
Set14 O
B100 O
Urban100 B-DAT

SR O
with O
BI O
model O
on O
Urban100 B-DAT

EDSR O
also O
becomes O
larger. O
For O
Urban100 B-DAT

SR O
with O
BI O
model O
on O
Urban100 B-DAT

Method O
Scale O
Set5 O
Set14 O
B100 O
Urban100 B-DAT

SR O
with O
BD O
model O
on O
Urban100 B-DAT

- B-DAT

- B-DAT
portance O
for O
image O
super-resolution O
(SR O

- B-DAT
resolution O
inputs O
and O
features O
contain O

- B-DAT

- B-DAT
tion, O
which O
is O
treated O
equally O

- B-DAT
resentational O
ability O
of O
CNNs. O
To O

- B-DAT
tions. O
Meanwhile, O
RIR O
allows O
abundant O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
given O
its O
low-resolution O
(LR O

- B-DAT

- B-DAT
tions, O
ranging O
from O
security O
and O

- B-DAT

- B-DAT
merous O
learning O
based O
methods O
have O

- B-DAT
layer O
CNN O
for O
image O
SR O

- B-DAT

- B-DAT
work O
depth O
was O
demonstrated O
to O

- B-DAT
nition O
tasks, O
especially O
when O
He O

- B-DAT

- B-DAT

- B-DAT
wise O
features O
equally, O
which O
lacks O

- B-DAT
formation O
(e.g., O
low- O
and O
high-frequency O

- B-DAT

- B-DAT
sible. O
The O
LR O
images O
contain O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ual O
channel O
attention O
networks O
(RCAN O

- B-DAT

- B-DAT
ture O
to O
construct O
very O
deep O

- B-DAT
tions O
in O
RIR O
help O
to O

- B-DAT

- B-DAT
tional O
ability O
of O
the O
network O

- B-DAT
nity O
[1–11,22]. O
Attention O
mechanism O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vious O
works. O
By O
introducing O
residual O

- B-DAT
icant O
improvement O
in O
accuracy. O
Tai O

- B-DAT
lution O
at O
the O
network O
tail O

- B-DAT

- B-DAT
bines O
automated O
texture O
synthesis O
and O

- B-DAT
gree, O
their O
predicted O
results O
may O

- B-DAT
cant O
improvement. O
However, O
most O
of O

- B-DAT

- B-DAT
inative O
ability O
for O
different O
types O

-1 B-DAT
RG-g O
RG-G O

- B-DAT

-1 B-DAT
RCAB-b O
RCAB-B O

- B-DAT

- B-DAT
fication O
with O
a O
trunk-and-mask O
attention O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tain O
significant O
performance O
improvement O
for O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
posed O
RIR O
achieves O
the O
largest O

- B-DAT

- B-DAT
tively O

- B-DAT

- B-DAT

- B-DAT
strated O
to O
be O
more O
efficient O

- B-DAT

- B-DAT
ial O
losses O
[8, O
21]. O
To O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
frequency O
parts O
seem O
to O
be O

- B-DAT

- B-DAT
quently, O
the O
output O
after O
convolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
by O
global O
average O
pooling O

- B-DAT
wise O
features O
can O
be O
emphasized O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tively. O
WD O
is O
the O
weight O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
Fg,bXg,b O

- B-DAT
hance O
the O
discriminative O
ability O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
table O
performance O
improvements O
over O
previous O

- B-DAT
sions O
about O
the O
effects O
of O

- B-DAT
downscaling O
and O
channel-upscaling, O
whose O
kernel O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ation O
metric, O
and O
training O
settings O

- B-DAT

-1 B-DAT
and O
top-5 O
recognition O
errors) O
comparisons O

- B-DAT

- B-DAT
tion O
(CA) O
based O
on O
the O

- B-DAT
formance. O
It’s O
hard O
to O
obtain O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
CNN O
[2], O
SCN O
[3], O
VDSR O

- B-DAT

- B-DAT
ensemble O
strategy O
to O
further O
improve O

- B-DAT

- B-DAT
parisons O
for O
×2, O
×3, O
×4 O

- B-DAT

- B-DAT

- B-DAT
age O
“img O
004”, O
we O
observe O

- B-DAT
CNN O
cannot O
recover O
lines. O
Other O

- B-DAT
Cooking”, O
the O
cropped O
part O
is O

- B-DAT
ful O
representational O
ability O
can O
extract O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults O
with O
7 O
state-of-the-art O
methods O

- B-DAT

- B-DAT
ing O
details O
in O
images O
“img O

- B-DAT
tive O
components. O
These O
comparisons O
indicate O

- B-DAT

-1 B-DAT
error O
0.506 O
0.477 O
0.437 O
0.454 O

-5 B-DAT
error O
0.266 O
0.242 O
0.196 O
0.224 O

- B-DAT

- B-DAT

-50 B-DAT
[20] O
as O
the O
evaluation O
model O

- B-DAT

- B-DAT
idation O
dataset O
for O
evaluation. O
The O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
and O
top-5 O
errors. O
These O
comparisons O

- B-DAT

- B-DAT
nections, O
making O
the O
main O
network O

- B-DAT

- B-DAT

- B-DAT
terdependencies O
among O
channels. O
Extensive O
experiments O

-14 B-DAT

-1 B-DAT

-0484, B-DAT
and O
U.S. O
Army O
Research O
Office O

-17 B-DAT

-1 B-DAT

-0367 B-DAT

- B-DAT

- B-DAT

- B-DAT
lutional O
networks. O
TPAMI O
(2016 O

- B-DAT

- B-DAT
resolution O
with O
sparse O
prior. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
with O
deep O
laplacian O
pyramid O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In: O
CVPR. O
(2018 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
chines. O
In: O
ICML. O
(2010 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
representations. O
In: O
Proc. O
7th O
Int O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

4×) O
on O
“img O
074” O
from O
Urban100 B-DAT

36], O
Set14 O
[37], O
B100 O
[38], O
Urban100 B-DAT
[22], O
and O
Manga109 O
[39]. O
We O

Method O
Scale O
Set5 O
Set14 O
B100 O
Urban100 B-DAT
Manga109 O

Urban100 B-DAT
(4×): O
img O
004 O

Urban100 B-DAT
(4×): O
img O
073 O

SR O
with O
BI O
model O
on O
Urban100 B-DAT
and O
Manga109 O
datasets. O
The O
best O

EDSR O
also O
becomes O
larger. O
For O
Urban100 B-DAT
and O
Manga109, O
the O
PSNR O
gains O

Urban100 B-DAT
(8×): O
img O
040 O

SR O
with O
BI O
model O
on O
Urban100 B-DAT
and O
Manga109 O
datasets. O
The O
best O

Method O
Scale O
Set5 O
Set14 O
B100 O
Urban100 B-DAT
Manga109 O

Urban100 B-DAT
(3×): O
img O
062 O

Urban100 B-DAT
(3×): O
img O
078 O

SR O
with O
BD O
model O
on O
Urban100 B-DAT
dataset. O
The O
best O
results O
are O

image O
SR O
on O
‘img_092’ O
from O
Urban100 B-DAT

2×105 O
iterations O
and O
evaluated O
on O
Urban100 B-DAT

from O
Urban100 B-DAT
HR/PSNR I-DAT
Bicubic/23.33 O
VDSR/26.81 O
DRRN/26.89 O
NLRN/27.39 O

Urban100 B-DAT
26 I-DAT

105 O
iterations O
and O
evaluated O
on O
Urban100 B-DAT

tively. O
The O
performance O
evaluated O
on O
Urban100 B-DAT

trade-off. O
Models O
are O
evaluated O
on O
Urban100 B-DAT

representative O
state-of-the-art O
methods O
on O
Urban100 B-DAT

img_044’ O
from O
Urban100 B-DAT

img_062’ O
from O
Urban100 B-DAT
HR/PSNR I-DAT
Bicubic/19.86 O
VDSR/20.76 O
DRRN/20.90 O
NLRN/21.37 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formation O
flows O
are O
solely O
feedforward O

- B-DAT

- B-DAT
plored. O
In O
this O
paper, O
we O

- B-DAT
rate O
image O
SR, O
in O
which O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tures O
captured O
under O
large O
receptive O

- B-DAT

- B-DAT
ciently O
selects O
and O
further O
enhances O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-the-art O
SR O
methods O
in O
terms O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
image O
SR O
methods O

- B-DAT

- B-DAT
tures O
solely O
flow O
from O
the O

- B-DAT

- B-DAT
tures O
extracted O
from O
the O
top O

- B-DAT

- B-DAT
agating O
high-level O
features O
to O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
features, O
we O
employ O
multiple O

- B-DAT

- B-DAT
tures O
to O
shallow O
layers. O
However O

- B-DAT

- B-DAT

- B-DAT
level O
information O
to O
refine O
low-level O

- B-DAT

- B-DAT

- B-DAT
posed O
GMFN O
shows O
better O
visual O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tensive O
experiments O
demonstrate O
the O
superiority O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ploys O
16 O
RDBs O

- B-DAT
level O
features O
for O
refining O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
tasks O
(e.g. O
classification O
[29 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ent O
direction, O
[12, O
31] O
applied O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
less, O
we O
argue O
that O
such O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ent O
receptive O
fields, O
every O
piece O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
back O
module O
to O
adaptively O
eliminate O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
able O
contextual O
knowledge O
from O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
cost O
was O
quadratically O
saved O

- B-DAT
gether. O
However, O
these O
networks O
require O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pendent O
convolutional O
neural O
network O
which O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
jacent O
time O
steps O
is O
achieved O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
RDB-BRDB O

- B-DAT

-1 B-DAT
RDB-b O
RDB-B O

- B-DAT

- B-DAT

L O
BF O
-, B-DAT

L O
bF O
- B-DAT
, O
t O

- B-DAT

- B-DAT

- B-DAT
tures O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
RDN O
[33], O
the O
number O

- B-DAT

- B-DAT
volutional O
layer. O
Then, O
a O
3×3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lowing O
RDB. O
The O
placement O
of O

- B-DAT
cording O
to O
the O
relative O
hierarchical O

- B-DAT

- B-DAT

- B-DAT
cilitates O
the O
refinement O
processes O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lected O
indexes O
of O
the O
deepest O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ative O
hierarchical O
relationship O
among O
multiple O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
information O
captured O
under O
different O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
network O
is O
set O
to O
C0 O

- B-DAT
ment O
training O
images O
with O
scaling O

- B-DAT
tion O
bicubic. O
The O
SR O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formance O
of O
various O
single-to-multiple O
anti-feedback O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dex O
sets O
SM O
and O
DN O

- B-DAT

- B-DAT

- B-DAT
to-multiple O
feedback O
manner O
[12, O
31 O

- B-DAT
to-single O
feedback O
manners O
perform O
better O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fining O
low-level O
features. O
However, O
excessively O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
features. O
If O
the O
high-level O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trate O
the O
effectiveness O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bine O
various O
M O
to O
achieve O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ther O
hinder O
the O
reconstruction O
ability O

- B-DAT
tively O
selects O
the O
high O
frequency O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tively O
accesses O
to O
high-level O
information O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nections O
by O
setting O
M O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
son O
results2 O
in O
Tab. O
1 O

- B-DAT
struct O
a O
faithful O
SR O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ogy O
Department O
(No.2018GZ0178 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ding. O
In O
BMVC, O
2012 O

- B-DAT
volutional O
network O
for O
image O
super-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
projection O
networks O
for O
super-resolution. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
back O
network O
for O
image O
super-resolution O

- B-DAT

- B-DAT
layer O
recurrent O
connections O
for O
scene O

- B-DAT

- B-DAT

- B-DAT
masaki, O
and O
Kiyoharu O
Aizawa. O
Sketch-based O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
forward O
networks O
(FF O

-1 B-DAT

- B-DAT
ers O
are O
set O
to O
128 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults O
shown O
Figure O
6 O
indicate O

- B-DAT
tively. O
The O
performance O
evaluated O
on O

- B-DAT
served O
that O
with O
the O
help O

- B-DAT
nificantly O
improved O
compared O
with O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods, O
but O
it O
holds O
relatively O

-16, B-DAT
we O
provide O
more O
qualitative O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rately O
restored O
the O
letter O
"M O

- B-DAT

- B-DAT
coveres O
two O
horizontal O
lines O
as O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

image O
SR O
on O
‘img_092’ O
from O
Urban100 B-DAT
dataset. O
The O
proposed O
GMFN O
accurately O

1], O
Set14 O
[30], O
B100 O
[21], O
Urban100 B-DAT
[11], O
and O
Manga109 O
[22]. O
We O

2×105 O
iterations O
and O
evaluated O
on O
Urban100 B-DAT
dataset O
with O
scale O
factor O
×4 O

Urban100 B-DAT
×2 O
26.88/0.8403 O
29.50/0.8946 O
30.77/0.9140 O
31.23/0.9188 O

from O
Urban100 B-DAT

Urban100 B-DAT
26.00 O
26.13 O
26.07 O
26.14 O

105 O
iterations O
and O
evaluated O
on O
Urban100 B-DAT
dataset. O
We O
set O
M O

tively. O
The O
performance O
evaluated O
on O
Urban100 B-DAT
dataset O
is O
shown O
in O
Figure O

trade-off. O
Models O
are O
evaluated O
on O
Urban100 B-DAT
under O
scale O
factor O
×4 O
on O

representative O
state-of-the-art O
methods O
on O
Urban100 B-DAT
with O
scale O
factor O
×4. O
Figure O

img_044’ O
from O
Urban100 B-DAT
EDSR/33.17 O
D-DBPN/31.35 O
RDN/33.11 O
SRFBN/32.97 O

img_062’ O
from O
Urban100 B-DAT

reconstruct O
high-resolution O
images O
of O
different O
upscaling B-DAT
factors O
in O
a O
single O
model O

demonstrated O
in O
Fig. O
4. O
For O
upscaling B-DAT
×4, O
if O
we O
use O
a O

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

- B-DAT

- B-DAT

- B-DAT
hanced O
deep O
super-resolution O
network O
(EDSR O

- B-DAT
mance O
exceeding O
those O
of O
current O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods. O
The O
significant O
performance O
improvement O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
on O
benchmark O
datasets O
and O

- B-DAT
ning O
the O
NTIRE2017 O
Super-Resolution O
Challenge O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nificantly O
improved O
performance O
in O
terms O

- B-DAT

- B-DAT
noise O
ratio O
(PSNR) O
in O
the O

- B-DAT
works O
exhibit O
limitations O
in O
terms O

- B-DAT

- B-DAT

- B-DAT
resolution O
of O
different O
scale O
factors O

- B-DAT
lems O
without O
considering O
and O
utilizing O

- B-DAT
quire O
many O
scale-specific O
networks O
that O

- B-DAT

- B-DAT

- B-DAT
dancy O
among O
scale-specific O
models. O
Nonetheless O

- B-DAT

- B-DAT
pling O
method O
[5, O
22, O
14 O

- B-DAT
ploys O
the O
ResNet O
architecture O
from O

- B-DAT
posed O
to O
solve O
higher-level O
computer O

- B-DAT

- B-DAT

- B-DAT
chitecture, O
we O
first O
optimize O
it O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els. O
Furthermore, O
we O
propose O
a O

- B-DAT

- B-DAT

- B-DAT
rameters O
compared O
with O
multiple O
single-scale O

- B-DAT
and O
multi-scale O
super-resolution O
networks O
show O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dicting O
detailed, O
realistic O
textures. O
Previous O

- B-DAT
struct O
better O
high-resolution O
images O

- B-DAT
tween O
ILR O
and O
IHR O
image O

- B-DAT
ods O
rely O
on O
techniques O
ranging O

- B-DAT
ding O
[3, O
2, O
7, O
21 O

- B-DAT
proaches O
utilize O
image O
self-similarities O
to O

- B-DAT
nal O
databases O
[8, O
6, O
29 O

- B-DAT
works O
has O
led O
to O
dramatic O

- B-DAT

- B-DAT
ing O
much O
deeper O
network O
architectures O

- B-DAT
perior O
performance. O
In O
particular, O
they O

- B-DAT
connection O
and O
recursive O
convolution O
alleviate O

- B-DAT

- B-DAT
work. O
Similarly O
to O
[20], O
Mao O

- B-DAT

- B-DAT

- B-DAT
rithms, O
an O
input O
image O
is O

- B-DAT
lation O
before O
they O
fed O
into O

- B-DAT
sampling O
modules O
at O
the O
very O

- B-DAT
sible O
as O
shown O
in O
[5 O

- B-DAT
cause O
the O
size O
of O
features O

- B-DAT

- B-DAT
scale O
training O
and O
computational O
efficiency O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
thermore, O
we O
develop O
an O
appropriate O

- B-DAT
and O
multi-scale O
mod- O
els O

- B-DAT

- B-DAT
hibiting O
improved O
computational O
efficiency. O
In O

- B-DAT
ing O
sections, O
we O
suggest O
a O

- B-DAT

- B-DAT

- B-DAT
scale O
architecture O
(MDSR) O
that O
reconstructs O

- B-DAT

- B-DAT
level O
to O
high-level O
tasks. O
Although O

- B-DAT
fully O
applied O
the O
ResNet O
architecture O

- B-DAT

- B-DAT
mance O
by O
employing O
better O
ResNet O

- B-DAT
work O
model O
from O
original O
ResNet O

- B-DAT
tion O
increases O
the O
performance O
substantially O

- B-DAT
duced O
since O
the O
batch O
normalization O

- B-DAT

- B-DAT

- B-DAT
work O
model O
is O
to O
increase O

- B-DAT
nels) O
F O
occupies O
roughly O
O(BF O

- B-DAT
imize O
the O
model O
capacity O
when O

- B-DAT
tational O
resources O

- B-DAT
cedure O
numerically O
unstable. O
A O
similar O

- B-DAT
ing O
procedure O
greatly O
when O
using O

- B-DAT
ous O
convolution O
layer O
for O
the O

- B-DAT

- B-DAT
vation O
layers O
outside O
the O
residual O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
at O
multiple O
scales O
is O

- B-DAT

- B-DAT
ther O
explore O
this O
idea O
by O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
specific O
processing O
modules O
to O
handle O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ule O
consists O
of O
two O
residual O

- B-DAT

- B-DAT

- B-DAT
tive O
field O
is O
covered O
in O

- B-DAT

- B-DAT

- B-DAT
ules O
are O
located O
in O
parallel O

- B-DAT

- B-DAT
tion. O
The O
architecture O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els O
for O
3 O
different O
scales O

- B-DAT

- B-DAT

- B-DAT
hibits O
comparable O
performance O
as O
the O

- B-DAT

- B-DAT

- B-DAT

Residual O
scaling O
- B-DAT
- O
0.1 O

- B-DAT

- B-DAT
ual O
blocks O
are O
lighter O
than O

- B-DAT

- B-DAT
specific O
EDSRs. O
The O
detailed O
performance O

- B-DAT

- B-DAT
formances O
on O
the O
validation O
dataset O

- B-DAT

- B-DAT
ing O
the O
mean O
RGB O
value O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works O
as O
described O
in O
Sec O

- B-DAT
trained O
network O
for O
other O
scales O

- B-DAT

- B-DAT
specific O
residual O
blocks O
and O
upsampling O

- B-DAT
spond O
to O
different O
scales O
other O

- B-DAT
imizing O
L2 O
is O
generally O
preferred O

- B-DAT
pirically O
found O
that O
L1 O
loss O

- B-DAT
spectively. O
The O
source O
code O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
super-resolved O
images O

- B-DAT

- B-DAT

- B-DAT
rate O
models. O
It O
is O
beneficial O

- B-DAT

- B-DAT
dividually O
trained O
models. O
We O
denote O

- B-DAT

- B-DAT

- B-DAT
inal O
one O
trained O
with O
L2 O

- B-DAT

- B-DAT
els O
require O
much O
less O
GPU O

- B-DAT
sults O
in O
an O
individual O
experiment O

- B-DAT
per O
[14]. O
In O
our O
experiments O

- B-DAT

0.9542 O
37.53 O
/ O
0.9587 O
- B-DAT
/ O
- O
38.11 O
/ O
0.9601 O

0.9090 O
33.66 O
/ O
0.9213 O
- B-DAT
/ O
- O
34.65 O
/ O
0.9282 O

0.9063 O
33.03 O
/ O
0.9124 O
- B-DAT
/ O
- O
33.92 O
/ O
0.9195 O

0.8209 O
29.77 O
/ O
0.8314 O
- B-DAT
/ O
- O
30.52 O
/ O
0.8462 O

0.8879 O
31.90 O
/ O
0.8960 O
- B-DAT
/ O
- O
32.32 O
/ O
0.9013 O

0.7863 O
28.82 O
/ O
0.7976 O
- B-DAT
/ O
- O
29.25 O
/ O
0.8093 O

0.8946 O
30.76 O
/ O
0.9140 O
- B-DAT
/ O
- O
32.93 O
/ O
0.9351 O

0.7989 O
27.14 O
/ O
0.8279 O
- B-DAT
/ O
- O
28.80 O
/ O
0.8653 O

0.9581 O
33.66 O
/ O
0.9625 O
- B-DAT
/ O
- O
35.03 O
/ O
0.9695 O

0.9138 O
30.09 O
/ O
0.9208 O
- B-DAT
/ O
- O
31.26 O
/ O
0.9340 O

0.8753 O
28.17 O
/ O
0.8841 O
- B-DAT
/ O
- O
29.25 O
/ O
0.9017 O

- B-DAT

- B-DAT
vided O
in O
the O
last O
two O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sure O
PSNR O
and O
SSIM O
on O

- B-DAT
LAB O
[18] O
functions O
for O
evaluation O

- B-DAT
nificant O
improvement O
compared O
to O
the O

- B-DAT

- B-DAT

- B-DAT
puts O
compared O
with O
the O
previous O

- B-DAT
ticipating O
in O
the O
NTIRE2017 O
Super-Resolution O

- B-DAT
resolution O
system O
with O
the O
highest O

- B-DAT
graders O
(bicubic, O
unknown) O
with O
three O

- B-DAT
ditions. O
Some O
results O
of O
our O

- B-DAT
ods O
successfully O
reconstruct O
high-resolution O
images O

- B-DAT

- B-DAT
ventional O
ResNet O
architecture, O
we O
achieve O

- B-DAT
ual O
scaling O
techniques O
to O
stably O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT

- B-DAT

- B-DAT
age O
Processing, O
21(7):3194–3205, O
2012. O
2 O

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT

- B-DAT
mization. O
In O
ICLR O
2014. O
5 O

- B-DAT

- B-DAT

- B-DAT
ative O
adversarial O
network. O
arXiv:1609.04802, O
2016 O

- B-DAT

- B-DAT
ing O
very O
deep O
convolutional O
encoder-decoder O

- B-DAT
cal O
statistics. O
In O
ICCV O
2001 O

- B-DAT

- B-DAT

- B-DAT
tional O
networks O
for O
biomedical O
image O

- B-DAT
CAI O
2015. O
2 O

- B-DAT
tion O
by O
locally O
linear O
embedding O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT
v4, O
inception-resnet O
and O
the O
impact O

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
actions O
on O
Image O
Processing, O
21(8):3467–3478 O

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT

- B-DAT

- B-DAT
tional O
Conference O
on O
Curves O
and O

- B-DAT

- B-DAT
gorithm O
via O
directional O
filtering O
and O

- B-DAT
actions O
on O
Image O
Processing, O
15(8):2226–2238 O

Set14 O
[33], O
B100 O
[17], O
and O
Urban100 B-DAT
[10 O

img034 O
from O
Urban100 B-DAT
[10 O

img062 O
from O
Urban100 B-DAT
[10 O

Urban100 B-DAT
×3 O
24.46 O
/ O
0.7349 O
26.03 O

layer O
in O
LR O
space O
for O
upscaling B-DAT

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

training. O
As O
most O
images O
in O
Urban100 B-DAT

B100 O
and O
“img O
043” O
from O
Urban100 B-DAT

Urban100 B-DAT

ban100 O
and O
“img O
099” O
from O
Urban100 B-DAT

- B-DAT

- B-DAT
cently O
achieved O
great O
success O
for O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tract O
abundant O
local O
features O
via O

- B-DAT
tional O
layers. O
RDB O
further O
allows O

- B-DAT
taining O
dense O
local O
features, O
we O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
graded O
low-resolution O
(LR) O
measurement. O
SISR O

- B-DAT
lance O
imaging O
[42], O
medical O
imaging O

- B-DAT
eration O
[9]. O
While O
image O
SR O

- B-DAT

- B-DAT
cedure, O
since O
there O
exists O
a O

- B-DAT
based O
[40], O
reconstruction-based O
[37], O
and O

- B-DAT

- B-DAT

- B-DAT
ules, O
the O
networks O
for O
image O

- B-DAT
ory O
block O
was O
proposed O
to O

- B-DAT

- B-DAT
gles O
of O
view, O
and O
aspect O

- B-DAT
tion. O
While, O
most O
deep O
learning O

- B-DAT

- B-DAT
inal O
LR O
image O
to O
the O

- B-DAT
processing O
step O
not O
only O
increases O

- B-DAT
ing O
to O
our O
experiments O
(see O

- B-DAT
archical O
features O
from O
the O
original O

- B-DAT
posed O
residual O
dense O
block O
(Fig O

- B-DAT
tical O
for O
a O
very O
deep O

- B-DAT
ual O
dense O
block O
(RDB) O
as O

- B-DAT
sion O
(LFF) O
with O
local O
residual O

- B-DAT
catenating O
the O
states O
of O
preceding O

- B-DAT
ing O
layers O
within O
the O
current O

- B-DAT

- B-DAT
sion O
[15 O

- B-DAT

- B-DAT
work O
(RDN) O
for O
high-quality O
image O

- B-DAT
tiguous O
memory O
(CM) O
mechanism, O
but O

- B-DAT
lize O
all O
the O
layers O
within O

- B-DAT
tions. O
The O
accumulated O
features O
are O

- B-DAT
ods O
in O
computer O
vision O
[36 O

- B-DAT
ited O
space, O
we O
only O
discuss O

- B-DAT

- B-DAT

- B-DAT
ther O
improved O
mainly O
by O
increasing O

- B-DAT
ing O
network O
weights. O
VDSR O
[10 O

- B-DAT
creased O
the O
network O
depth O
by O

- B-DAT
duced O
recursive O
learning O
in O
a O

- B-DAT
rameter O
sharing. O
Tai O
et O
al O

- B-DAT
inal O
LR O
images O
to O
the O

- B-DAT

- B-DAT
creases O
computation O
complexity O
quadratically O
[4 O

- B-DAT

- B-DAT
tures O
from O
the O
interpolated O
LR O

- B-DAT

- B-DAT

- B-DAT
PCN O
[22], O
where O
an O
efficient O

- B-DAT

- B-DAT

- B-DAT
ods O
extracted O
features O
in O
the O

- B-DAT
nal O
LR O
features O
with O
transposed O

- B-DAT

- B-DAT

- B-DAT
lows O
direct O
connections O
between O
any O

- B-DAT
troduced O
among O
memory O
blocks O
[26 O

- B-DAT

- B-DAT
duced O
by O
a O
very O
deep O

- B-DAT
tion O
tasks O
(e.g., O
image O
SR O

- B-DAT

- B-DAT
tracts O
features O
F−1 O
from O
the O

- B-DAT
ond O
shallow O
feature O
extraction O
layer O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
layers O
within O
the O
block O

- B-DAT
ture. O
More O
details O
about O
RDB O

- B-DAT
cludes O
global O
feature O
fusion O
(GFF O

- B-DAT

- B-DAT

- B-DAT
nected O
layers, O
local O
feature O
fusion O

- B-DAT
ual O
learning, O
leading O
to O
a O

- B-DAT
nism O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sists O
of O
G O
(also O
known O

- B-DAT

- B-DAT

- B-DAT
tional O
layers O
1 O

- B-DAT

- B-DAT

- B-DAT
ing O
RDB O
and O
each O
layer O

- B-DAT
sequent O
layers, O
which O
not O
only O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ber. O
On O
the O
other O
hand O

- B-DAT
duce O
a O
1 O
× O
1 O

- B-DAT

- B-DAT
comes O
larger, O
very O
deep O
dense O

- B-DAT
tional O
layers O
in O
one O
RDB O

- B-DAT

- B-DAT
mance. O
We O
introduce O
more O
results O

- B-DAT
ing, O
we O
refer O
to O
this O

- B-DAT
maps O
produced O
by O
residual O
dense O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tively O
fused O
to O
form O
FGF O

- B-DAT

- B-DAT
quency O
information. O
However, O
in O
the O

- B-DAT
sequent O
layers. O
The O
local O
feature O

- B-DAT

- B-DAT
tion, O
MemNet O
extracts O
features O
in O

- B-DAT

- B-DAT
nition). O
While O
RDN O
is O
designed O

- B-DAT

- B-DAT
ual O
learning, O
which O
would O
be O

- B-DAT

- B-DAT
cal O
features, O
which O
are O
neglected O

- B-DAT
ferences O
between O
SRDenseNet O
[31] O
and O

- B-DAT
troduces O
the O
basic O
dense O
block O

- B-DAT
bilizes O
the O
training O
of O
wide O

- B-DAT
tract O
global O
features, O
because O
our O

- B-DAT
formance O
and O
convergence O
[17]. O
As O

- B-DAT
formation O
from O
their O
preceding O
layers O

- B-DAT
ers O
within O
one O
RDB. O
Furthermore O

- B-DAT
nections O
among O
memory O
blocks O
in O

- B-DAT

- B-DAT
sults O
are O
evaluated O
with O
PSNR O

- B-DAT
tion O
models O
to O
simulate O
LR O

- B-DAT
bic O
downsampling O
by O
adopting O
the O

- B-DAT
ing O
90◦. O
1,000 O
iterations O
of O

- B-DAT

- B-DAT
ery O
200 O
epochs. O
Training O
a O

- B-DAT
rameters: O
the O
number O
of O
RDB O

- B-DAT
mance O
of O
SRCNN O
[3] O
as O

- B-DAT
cal O
residual O
learning O
(LRL), O
and O

- B-DAT
portant, O
our O
RDN O
allows O
deeper O

- B-DAT
ture O
fusion O
(LFF) O
is O
needed O

- B-DAT
erly, O
so O
LFF O
isn’t O
removed O

- B-DAT
strates O
that O
stacking O
many O
basic O

- B-DAT
sulting O
in O
RDN O
CM1LRL0GFF0, O
RDN O

- B-DAT
ponent O
can O
efficiently O
improve O
the O

- B-DAT
line. O
This O
is O
mainly O
because O

- B-DAT
ing O
in O
RDN O
CM1LRL1GFF0, O
RDN O

- B-DAT
bination O
in O
Table O
1). O
It O

- B-DAT
ponents O
simultaneously O
(denote O
as O
RDN O

- B-DAT
sistent O
with O
the O
analyses O
above O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
SR O
methods: O
SRCNN O
[3 O

- B-DAT

- B-DAT
ther O
improve O
our O
RDN O
and O

- B-DAT

- B-DAT
rable O
or O
even O
better O
results O

- B-DAT
DenseNet O
[31] O
and O
MemNet O
[26 O

- B-DAT
els, O
our O
RDN O
also O
achieves O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put O
patch O
size. O
Moreover, O
our O

- B-DAT

- B-DAT
ods O
would O
produce O
noticeable O
artifacts O

- B-DAT
pared O
methods O
fail O
to O
recover O

- B-DAT
cover O
it O
obviously. O
This O
is O

- B-DAT
archical O
features O
through O
dense O
feature O

- B-DAT
CNN O
[3], O
FSRCNN O
[4], O
VDSR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults O
in O
Figs. O
7 O
and O

- B-DAT
covers O
sharper O
edges. O
This O
comparison O

- B-DAT
tracting O
hierarchical O
features O
from O
the O

- B-DAT
ods O
[3, O
10, O
38]. O
However O

- B-DAT
parison O
indicates O
that O
RDN O
is O

- B-DAT
tion O
models O
demonstrate O
the O
effectiveness O

- B-DAT
ing O
factor O
×3. O
The O
SR O

- B-DAT
ban100 O
and O
“img O
099” O
from O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferent O
or O
unknown O
degradation O
models O

- B-DAT
lizes O
the O
training O
wider O
network O

- B-DAT
ual O
leaning O
(LRL) O
further O
improves O

- B-DAT
world O
data. O
Extensive O
benchmark O
evaluations O

- B-DAT
strate O
that O
our O
RDN O
achieves O

- B-DAT

- B-DAT

- B-DAT
art O
methods O

-14 B-DAT

-1 B-DAT

- B-DAT
0484, O
and O
U.S. O
Army O
Research O

-17 B-DAT

- B-DAT
1-0367 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
deep O
convolutional O
networks O

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT

- B-DAT
mization. O
In O
ICLR, O
2014. O
5 O

- B-DAT
resolution. O
In O
CVPR, O
2017. O
1 O

- B-DAT
ham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT

- B-DAT
supervised O
nets. O
In O
AISTATS, O
2015 O

- B-DAT

- B-DAT
cal O
statistics. O
In O
ICCV, O
2001 O

- B-DAT
masaki, O
and O
K. O
Aizawa. O
Sketch-based O

- B-DAT
ing O
manga109 O
dataset. O
Multimedia O
Tools O

- B-DAT

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT
tia, O
A. O
M. O
S. O
M O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
hazing O
network. O
In O
CVPR, O
2018 O

- B-DAT

- B-DAT
raining O
using O
a O
multi-stream O
dense O

- B-DAT

- B-DAT
resolution O
with O
non-local O
means O
and O

- B-DAT
sion. O
TIP, O
2012. O
1 O

- B-DAT
lutional O
super-resolution O
network O
for O
multiple O

- B-DAT

- B-DAT

- B-DAT
nition O
problem. O
TIP, O
2012. O
1 O

1], O
Set14 O
[33], O
B100 O
[18], O
Urban100 B-DAT
[8], O
and O
Manga109 O
[19]. O
The O

training. O
As O
most O
images O
in O
Urban100 B-DAT
contain O
self-similar O
structures, O
larger O
input O

Urban100 B-DAT
×2 O
26.88/0.8403 O
29.50/0.8946 O
30.41/0.9101 O
31.23/0.9188 O

B100 O
and O
“img O
043” O
from O
Urban100 B-DAT
respectively O

results O
on O
Set5, O
Set14, O
B100, O
Urban100 B-DAT

Urban100 B-DAT
BD O
23.52/0.6862 O
25.84/0.7856 O
25.70/0.7770 O
22.04/0.6745 O

ban100 O
and O
“img O
099” O
from O
Urban100 B-DAT
respectively O

patch O
size O
based O
on O
the O
upscaling B-DAT
factor. O
The O
settings O
of O
input O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

from O
Urban100 B-DAT
Bicubic I-DAT
VDSRHR O
DRRN O

Urban100 B-DAT

Params. O
Set5 O
Set14 O
B100 O
Urban100 B-DAT

contrast O
to O
D-DBPN O
especially O
on O
Urban100 B-DAT

Average O
running O
time O
comparison O
on O
Urban100 B-DAT

from O
Urban100 B-DAT
HR I-DAT
Bicubic O
VDSR O
DRRN O

from O
Urban100 B-DAT

from O
Urban100 B-DAT

from O
Urban100 B-DAT

from O
Urban100 B-DAT
Figure I-DAT
20. O
Visual O
results O
of O

from O
Urban100 B-DAT
Figure I-DAT
21. O
Visual O
results O
of O

- B-DAT

- B-DAT

- B-DAT
plored O
the O
power O
of O
deep O

- B-DAT
construction O
performance. O
However, O
the O
feedback O

- B-DAT
nism, O
which O
commonly O
exists O
in O

- B-DAT

- B-DAT
level O
representations O
with O
high-level O
information O

- B-DAT
ically, O
we O
use O
hidden O
states O

- B-DAT
ful O
high-level O
representations. O
The O
proposed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
task, O
which O
aims O
to O

- B-DAT

- B-DAT

- B-DAT
herently O
ill-posed O
since O
multiple O
HR O

- B-DAT
merous O
image O
SR O
methods O
have O

- B-DAT
ing O
interpolation-based O
methods[45], O
reconstruction-based O
methods[42 O

- B-DAT

- B-DAT
lutional O
Neural O
Network O
(CNN) O
to O

- B-DAT
tention O
in O
recent O
years O
due O

- B-DAT
posed O
network. O
Blue O
arrows O
represent O

- B-DAT

- B-DAT
ing O
more O
contextual O
information O
with O

- B-DAT
ishing/exploding O
problems O
caused O
by O
simply O

- B-DAT
ters O
increases. O
A O
large-capacity O
network O

- B-DAT

- B-DAT
current O
Neural O
Network O
(RNN). O
Similar O

- B-DAT
tional O
deep O
learning O
based O
methods O

- B-DAT
ward O
manner. O
However, O
the O
feedforward O

- B-DAT

- B-DAT

- B-DAT
down O
manner, O
carrying O
high-level O
information O

- B-DAT
vious O
layers O
and O
refining O
low-level O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
structed O
by O
multiple O
sets O
of O

- B-DAT
and O
down-sampling O
layers O
with O
dense O

- B-DAT

- B-DAT
covery O
difficulty. O
Such O
curriculum O
learning O

- B-DAT
tion O
models. O
Experimental O
results O
demonstrate O

- B-DAT
ority O
of O
our O
proposed O
SRFBN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work O
(SRFBN), O
which O
employs O
a O

- B-DAT
nism. O
High-level O
information O
is O
provided O

- B-DAT

- B-DAT
while, O
such O
recurrent O
structure O
with O

- B-DAT
tions O
provides O
strong O
early O
reconstruction O

- B-DAT
ficiently O
handles O
feedback O
information O
flows O

- B-DAT

- B-DAT
and O
down- O
sampling O
layers, O
and O

- B-DAT

- B-DAT
ing O
reconstruction O
difficulty O
are O
fed O

- B-DAT

- B-DAT

- B-DAT
ious O
computer O
vision O
tasks O
including O

- B-DAT

- B-DAT

- B-DAT
mation O
usage O
in O
LR O
images O

- B-DAT
provement O
in O
image O
SR. O
SRResNet[21 O

- B-DAT
plied O
residual O
skip O
connections O
from O

- B-DAT
work O
architectures O
use O
or O
combine O

- B-DAT

- B-DAT

- B-DAT
textual O
information O
due O
to O
the O

- B-DAT

- B-DAT
ing O
layers, O
and O
thus O
further O

- B-DAT
ity O
of O
the O
network. O
To O

- B-DAT
resolution O
feedback O
network O
(SRFBN), O
in O

- B-DAT

- B-DAT
down O
manner O
to O
correct O
low-level O

- B-DAT
textual O
information O

- B-DAT
capacity O
networks O
occupy O
huge O
amount O

- B-DAT
rent O
structure O
was O
employed[19, O
31 O

- B-DAT

- B-DAT

- B-DAT
and O
down-projection O
units O
to O
achieve O

- B-DAT
tion O
between O
two O
recurrent O
states O

- B-DAT

- B-DAT
ever, O
the O
flow O
of O
information O

- B-DAT

- B-DAT
tion O
of O
an O
input O
image O

- B-DAT
tional O
recurrent O
neural O
network. O
However O

- B-DAT

- B-DAT
ficiently O
flows O
across O
hierarchical O
layers O

- B-DAT
rior O
reconstruction O
performance O
than O
ConvLSTM1 O

- B-DAT
ficient O
strategy O
to O
improve O
the O

- B-DAT

- B-DAT
diction, O
they O
enforce O
a O
curriculum O

- B-DAT
creases O
during O
the O
training O
process O

- B-DAT
mid O
in O
previously O
trained O
networks O

- B-DAT
cess, O
we O
enforce O
a O
curriculum O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
effect O
process O
helps O
to O
achieve O

- B-DAT

- B-DAT
eration O
(to O
force O
the O
network O

- B-DAT
tion O
of O
high-level O
information), O
(2 O

- B-DAT

- B-DAT
formation, O
which O
is O
needed O
to O

- B-DAT
folded O
to O
T O
iterations, O
in O

- B-DAT
rally O
ordered O
from O
1 O
to O

- B-DAT

- B-DAT
tains O
three O
parts: O
an O
LR O

- B-DAT
sampled O
image O
to O
bypass O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
layer O
and O
a O
deconvolutional O

- B-DAT
traction O
block. O
F O
tin O
are O

- B-DAT

- B-DAT

- B-DAT
tained O
by O

- B-DAT

- B-DAT

- B-DAT
sentations O
F O
tin, O
and O
then O

- B-DAT

- B-DAT
tion O
block. O
The O
FB O
contains O

- B-DAT
tially O
with O
dense O
skip O
connections O

- B-DAT
jection O
group, O
which O
can O
project O

- B-DAT
tures O
F O
tin O
by O
feedback O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ingly, O
Ltg O
can O
be O
obtained O

- B-DAT

- B-DAT
tion O
group O
and O
map O
the O

- B-DAT
work. O
T O
target O
HR O
images O

- B-DAT
work. O
(I1HR, O
I O

- B-DAT
tion O
in O
the O
network O
can O

- B-DAT
put O
at O
the O
t-th O
iterations O

- B-DAT

- B-DAT
and O
down-sampling O
operations. O
For O
×2 O

- B-DAT
ating O
LR O
images O
from O
ground O

- B-DAT
ify O
the O
effectiveness O
of O
our O

- B-DAT
degradation O
models O
as O
[47] O
do O

- B-DAT
periments, O
we O
use O
7x7 O
sized O

- B-DAT
sampling O
followed O
by O
adding O
Gaussian O

- B-DAT
ing O
rate O
0.0001. O
The O
learning O

- B-DAT
ery O
200 O
epochs. O
We O
implement O

- B-DAT
ber O
of O
iterations O
(denoted O
as O

- B-DAT
jection O
groups O
in O
the O
feedback O

- B-DAT
iments. O
We O
first O
investigate O
the O

- B-DAT
out O
feedback O
connections O
(T=1). O
Besides O

- B-DAT
sults. O
It O
is O
worth O
noticing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
forward O
one O
in O
this O
subsection O

- B-DAT
put O
to O
low-level O
representations O
and O

- B-DAT
erty), O
denoted O
as O
SRFBN-L-FF. O
SRFBN-L O

- B-DAT

- B-DAT
FF O
both O
have O
four O
iterations O

- B-DAT
ate O
SR O
images O
from O
both O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
from O
which O
we O
conclude O

- B-DAT
trast O
to O
feedforward O
network. O
The O

- B-DAT
rent O
structure. O
Except O
for O
the O

- B-DAT
tive O
experiments O
to O
verify O
other O

- B-DAT
ation O
except O
the O
first O
iteration O

- B-DAT
put O
to O
low-level O
representations O
and O

- B-DAT
erty), O
denoted O
as O
SRFBN-L-FF. O
SRFBN-L O

- B-DAT

- B-DAT
FF O
both O
have O
four O
iterations O

- B-DAT
ate O
SR O
images O
from O
both O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ation, O
from O
which O
we O
conclude O

- B-DAT
trast O
to O
feedforward O
network. O
The O

- B-DAT
current O
structure. O
Except O
the O
above O

- B-DAT
tive O
experiments O
to O
verify O
other O

- B-DAT
ation O
except O
the O
first O
iteration O

- B-DAT
works O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trated O
in O
Fig. O
5. O
Each O

- B-DAT

- B-DAT
covering O
the O
residual O
image. O
In O

- B-DAT
inal O
input O
image[16] O
and O
to O

- B-DAT

- B-DAT
ponents O
(i.e. O
edges O
and O
contours O

- B-DAT
age. O
To O
some O
extent, O
this O

- B-DAT
tion O
ability O
than O
the O
feedforward O

- B-DAT
vation O
is O
that O
the O
feedback O

- B-DAT
sentations O
in O
contrast O
to O
feedforward O

- B-DAT
tions O
and O
then O
the O
smooth O

- B-DAT
strate O
that O
the O
feedforward O
network O

- B-DAT
formation O
through O
layers, O
while O
the O

- B-DAT
lowed O
to O
devote O
most O
of O

- B-DAT

- B-DAT

- B-DAT
sentations O
at O
the O
initial O
iteration O

- B-DAT

- B-DAT

- B-DAT
quent O
iterations O
to O
generate O
better O

- B-DAT
culty. O
For O
example, O
to O
guide O

- B-DAT
works O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trated O
in O
Fig. O
5. O
Each O

- B-DAT

- B-DAT
work O
with O
global O
residual O
skip O

- B-DAT
ering O
the O
residual O
image. O
In O

- B-DAT
put O
image[16] O
and O
to O
predict O

- B-DAT

- B-DAT
servations. O
First, O
compared O
with O
the O

- B-DAT
tions O
in O
contrast O
to O
feedforward O

- B-DAT
cantly O
from O
the O
first O
iteration O

- B-DAT

- B-DAT

- B-DAT
ing O
high-level O
information O
at O
the O

- B-DAT

- B-DAT
back O
network O
will O
urge O
previous O

- B-DAT
tions O
to O
generate O
better O
representations O

- B-DAT
culty. O
For O
example, O
to O
guide O

- B-DAT
gle O
downsampling O
operator O
at O
early O

- B-DAT

- B-DAT
tion O
model. O
The O
results O
shown O

- B-DAT
riculum O
learning O
strategy O
well O
assists O

- B-DAT

- B-DAT
work O
pretrained O
on O
the O
BI O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
son O
results O
are O
given O
in O

- B-DAT

- B-DAT
rameters O
fewer O
than O
1000K. O
This O

- B-DAT
struction O
performance. O
Meanwhile, O
in O
comparison O

- B-DAT
DBPN O
and O
EDSR, O
our O
proposed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods: O
SRCNN[7], O
VDSR[18], O
DRRN[31], O
SRDenseNet[36 O

- B-DAT

- B-DAT

- B-DAT
perform O
almost O
all O
comparative O
methods O

- B-DAT

- B-DAT
ages O
(DIV2K+Flickr2K+ImageNet O
vs. O
DIV2K+Flickr2K). O
However O

- B-DAT
trast O
to O
them. O
In O
addition O

- B-DAT
age O
from O
Manga109, O
DRRN O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
curriculum O
learning O
strategy O
for O

- B-DAT
tion O
models, O
and O
fine-tuned O
based O

- B-DAT
CNN O
C[43], O
SRMD(NF)[44], O
and O
RDN[47 O

- B-DAT

- B-DAT
most O
all O
quantative O
results O
over O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O

- B-DAT

- B-DAT

- B-DAT
mark O
datasets. O
Compared O
with O
other O

- B-DAT
posed O
SRFBN O
could O
alleviate O
the O

- B-DAT
isions, O
we O
further O
indicate O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dation O
models. O
The O
comprehensive O
experimental O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sored O
by O
National O
Natural O
Science O

- B-DAT
tion O
of O
Sichuan O
Science O
and O

- B-DAT

- B-DAT
son O
Weston. O
Curriculum O
learning. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
with O
feedback O
convolutional O
neural O

- B-DAT
tendra O
Malik. O
Human O
pose O
estimation O

- B-DAT

- B-DAT

- B-DAT
works. O
TPAMI, O
2016. O
2, O
7 O

- B-DAT

- B-DAT
down O
influences O
in O
sensory O
processing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mance O
on O
imagenet O
classification. O
In O

- B-DAT
ian O
Q O
Weinberger. O
Densely O
connected O

- B-DAT
works. O
In O
CVPR, O
2016. O
2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rate O
single O
image O
super-resolution O
via O

- B-DAT
tion O
network. O
In O
CVPR, O
2018 O

- B-DAT
tween O
figure O
and O
background O
by O

- B-DAT
ture, O
1998. O
2 O

- B-DAT

- B-DAT
works. O
In O
CVPR, O
2016. O
1 O

- B-DAT
recursive O
convolutional O
network O
for O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tex. O
arXiv O
preprint O
arXiv:1604.03640, O
2016 O

- B-DAT

- B-DAT
dra O
Malik. O
A O
database O
of O

- B-DAT

- B-DAT
timedia O
Tools O
and O
Applications, O
2017 O

- B-DAT

- B-DAT

- B-DAT
back O
for O
crowd O
counting O
convolutional O

- B-DAT

- B-DAT
resolution O
via O
deep O
recursive O
residual O

- B-DAT
net: O
A O
persistent O
memory O
network O

- B-DAT
chored O
neighborhood O
regression O
for O
fast O

- B-DAT

- B-DAT

- B-DAT
resolution. O
In O
ACCV, O
2015. O
1 O

- B-DAT

- B-DAT
tion. O
In O
CVPR, O
2016. O
7 O

- B-DAT

- B-DAT
hanced O
super-resolution O
generative O
adversarial O
networks O

- B-DAT
der O
Sorkinehornung, O
Olga O
Sorkinehornung, O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
back O
networks. O
In O
CVPR, O
2017 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lation O
algorithm O
via O
directional O
filtering O

- B-DAT

- B-DAT

- B-DAT
tion. O
We O
still O
use O
SRFBN-L O

- B-DAT
and O
down-sampling O
layers O
(UDSL), O
(2 O

- B-DAT
and O
down-sampling O
layers O
with O
3 O

- B-DAT
ers O
(with O
one O
padding O
and O

- B-DAT
and O
down-sampling O
operations O
carrying O
large O

- B-DAT
tion O
and O
are O
effective O
for O

- B-DAT

- B-DAT
ter O
adding O
DSC O
to O
the O

- B-DAT
and O
down-sampling O
layers O
(UDSL), O
and O

- B-DAT
sides, O
high-level O
information O
is O
directly O

- B-DAT

- B-DAT

- B-DAT
ization O

- B-DAT
textual O
information O
for O
the O
next O

- B-DAT
parison O
with O
other O
basic O
blocks O

- B-DAT
nism O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tations O
to O
the O
initial O
feature O

- B-DAT

- B-DAT
formation, O
surely O
are O
corrected O
using O

- B-DAT

- B-DAT

- B-DAT
erage O
feature O
map O
at O
each O

- B-DAT

- B-DAT
back) O
and O
SRFBN-L-FF O
(feedforward). O
As O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trum O
map O
through O
discrete O
Fourier O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eration O
t O
grows, O
the O
feedforward O

- B-DAT
ers O
mid-frequency O
and O
high-frequency O
components O

- B-DAT
developed O
information. O
For O
the O
feedback O

- B-DAT
nism O
(t O
>1), O
mid-frequency O
and O

- B-DAT

- B-DAT
tion O
of O
the O
average O
feature O

- B-DAT
ture O
design, O
we O
compare O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
network O
with O
moderate O
parameters O

- B-DAT
cause O
MemNet O
only O
reveals O
the O

- B-DAT

- B-DAT

- B-DAT
tary O
materials. O
Our O
SRFBN-S O
(T=4 O

- B-DAT
son. O
In O
Tab. O
8, O
our O

- B-DAT

- B-DAT
sults O
than O
MemNet O
with O
71 O

- B-DAT

- B-DAT
parison O
shows O
the O
effectiveness O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ban100 O
with O
scale O
factor O
×4 O

- B-DAT

- B-DAT
torch O
for O
fair O
comparison. O
The O

- B-DAT
works O
is O
evaluated O
on O
the O

- B-DAT
tel O
i7 O
CPU O
(16G O
RAM O

- B-DAT
ing O
their O
official O
codes. O
Tab O

- B-DAT

- B-DAT
son O
with O
other O
networks. O
This O

- B-DAT
tiveness O
of O
our O
proposed O
networks O

- B-DAT
volutional O
layers O
with O
77% O
fewer O

-22, B-DAT
we O
provide O
more O
visual O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

benchmark O
datasets: O
Set5[3], O
Set14[41], O
B100[24], O
Urban100 B-DAT

from O
Urban100 B-DAT

for O
the O
‘img O
092’ O
from O
Urban100, B-DAT
the O
texture O
direction O
of O
the O

Urban100 B-DAT
×2 O
26.88/0.8403 O
29.50/0.8946 O
30.77/0.9140 O
31.23/0.9188 O

Urban100 B-DAT
BD O
23.20/0.6661 O
25.31/0.7612 O
26.75/0.8145 O
26.77/0.8154 O

RDN O
SRFBN O
(Ours) O
img_044 O
from O
Urban100 B-DAT

Params. O
Set5 O
Set14 O
B100 O
Urban100 B-DAT
Manga109 O
MemNet-Pytorch O
677K O
31.75/0.889 O
28.31/0.775 O

contrast O
to O
D-DBPN O
especially O
on O
Urban100 B-DAT
and O
Manga109 O
datasets, O
which O
mainly O

Average O
running O
time O
comparison O
on O
Urban100 B-DAT
with O
scale O
factor O
4 O
on O

from O
Urban100 B-DAT

from O
Urban100 B-DAT
EDSR O
D-DBPN O
SRFBN O
(Ours O

from O
Urban100 B-DAT
SRMDNF O
RDN O
SRFBN O
(Ours O

from O
Urban100 B-DAT
SRMDNF O
RDN O
SRFBN O
(Ours O

from O
Urban100 B-DAT

from O
Urban100 B-DAT

Berkeley O
segmentation O
(BSD500) O
[4] O
and O
Urban100 B-DAT
[16] I-DAT
datasets. O
Specifically, O
we O
used O

400 O
images O
of O
BSD500 O
and O
Urban100 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ous O
image O
restoration O
tasks. O
However O

- B-DAT
pose O
a O
novel O
feature O
space O

- B-DAT
rithm O
that O
outperforms O
the O
existing O

- B-DAT
mance O
of O
a O
learning O
algorithm O

- B-DAT

- B-DAT

- B-DAT
strate O
that O
the O
proposed O
feature O

- B-DAT
performs O
the O
existing O
state-of-the-art O
approaches O

- B-DAT
over, O
our O
algorithm O
was O
ranked O

-10 B-DAT
times O
faster O
computational O
time O
compared O

- B-DAT
cessing O
applications. O
Over O
the O
last O

- B-DAT

- B-DAT

- B-DAT
proaches O
[22], O
and O
sparse O
dictionary O

- B-DAT

- B-DAT

- B-DAT
over, O
these O
algorithms O
are O
usually O

- B-DAT
ative O
manner, O
so O
they O
require O

- B-DAT
sources O

- B-DAT
level O
computer O
vision O
problems O
[24 O

- B-DAT
ing O
and O
super-resolution O
tasks, O
many O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
itation O
of O
the O
state-of-the-art O
CNN O

- B-DAT
posed O
network O
architectures O
are O
motivated O

- B-DAT
sistent O
homology O
analysis O
[11] O
on O

- B-DAT
age O
processing O
tasks. O
Specifically, O
we O

- B-DAT
ual O
manifold O
is O
topologically O
simpler O

- B-DAT
age O
manifold, O
which O
may O
have O

- B-DAT
tion. O
Specifically, O
our O
design O
goal O

- B-DAT
tures O
while O
preserving O
the O
directional O

- B-DAT
lowing. O
First, O
a O
novel O
network O

- B-DAT
fold O
simplification O
is O
proposed. O
Second O

- B-DAT
putational O
topology O
tool O
called O
the O

- B-DAT
form O
to O
simplify O
topological O
structures O

COPY O
ch(LR) O
Label O
Input O
- B-DAT
WT(HR) O
Input O
- O
PS(HR O

Long O
bypass O
layer O
LongBypass(2) O
- B-DAT
- O
Repeat O
1st O
module O
6 O
times O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vanced O
algorithms O
in O
this O
field O

- B-DAT
and O
inter- O
correlations O
of O
the O

- B-DAT
mance O
to O
BM3D O
using O
multi-layer O

- B-DAT
able O
nonlinear O
reaction O
diffusion O
(TNRD O

- B-DAT
ters O
and O
influence O
functions O
by O

- B-DAT
timization O
approach. O
Recently, O
based O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
restoration O
problems O
[21 O

- B-DAT
proach O
is O
using O
a O
skipped O

- B-DAT
cept O
was O
first O
introduced O
by O

- B-DAT
tion. O
In O
low-level O
computer O
vision O

- B-DAT

- B-DAT
ping. O
In O
another O
implementation, O
the O

- B-DAT
formed O
into O
the O
difference O
between O

- B-DAT
lem: O
minf∈F O
L(f), O
where O
L(f O

- B-DAT
notes O
the O
risk. O
A O
major O

- B-DAT
formance. O
Specifically, O
with O
probability O

- B-DAT
pared O
to O
shallow O
ones. O
However O

- B-DAT

- B-DAT
ity O
penalty O
reduces O
much O
more O

- B-DAT
mization. O
One O
of O
the O
most O

- B-DAT
work, O
by O
reducing O
the O
complexity O

- B-DAT
ture O
spaces O
for O
the O
input O

- B-DAT
form O
is O
given O
by O
Y O

- B-DAT
sults O
in O
the O
dimensional O
reduction O

- B-DAT
tion. O
Indeed, O
this O
property O
of O

- B-DAT

- B-DAT
gebraic O
topology, O
Betti O
numbers O
(βm O

- B-DAT
ber O
of O
m-dimensional O
holes O
of O

- B-DAT
ing O
the O
changes O
of O
Betti O

- B-DAT
come O
a O
single O
cluster O
(Fig O

- B-DAT
resented O
as O
a O
slow O
decrease O

- B-DAT

- B-DAT
uration O
over O
� O
distance O
filtration O

- B-DAT
ture O
and O
the O
recent O
persistent O

- B-DAT

- B-DAT

- B-DAT
lationship O
between O
these O
newly O
processed O

- B-DAT
sian O
denoising, O
40 O
× O
40 O

- B-DAT
ing O
because O
it O
is O
helpful O

- B-DAT
leviating O
the O
gradient O
vanishing O
problem O

- B-DAT
malization, O
and O
ReLU O
and O
the O

- B-DAT
ing O
the O
convolution, O
we O
used O

- B-DAT
ceptive O
field O
can O
be O
reduced O

- B-DAT
mary O
denoising O
architecture. O
Depending O
on O

- B-DAT
tended O
denoising O
network O
structures O
with O

- B-DAT
form O
were O
used O
for O
manifold O

- B-DAT
known O
decimation O
scheme, O
however, O
we O

- B-DAT
pixel O
shuffling O
scheme O
[25] O
as O

- B-DAT

- B-DAT
fling O
transform O
does O
not O
reduce O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
task. O
The O
training O
step O

- B-DAT
ter O
the O
first O
two O
layers O

- B-DAT
construct O
the O
bicubic O
x2 O
downsampled O

- B-DAT
nection O
allows O
faster O
computation O
and O

- B-DAT
pass O
connection O

- B-DAT
minance O
channel, O
because O
RGB O
based O

- B-DAT
fect O
of O
data O
augmentation O

- B-DAT
able O
Berkeley O
segmentation O
(BSD500) O
[4 O

- B-DAT
ing O
task. O
In O
addition, O
we O

- B-DAT
fitting, O
we O
re-generated O
the O
Gaussian O

- B-DAT
ing O
and O
validation, O
Gaussian O
noises O

- B-DAT
tion O
using O
image O
flipping, O
rotation O

- B-DAT
tialized O
using O
the O
Xavier O
method O

- B-DAT
sion O
loss O
across O
four O
wavelet O

- B-DAT

- B-DAT
ble O
learning, O
we O
employed O
the O

- B-DAT
date O
parameter O
are O
bounded O
by O

- B-DAT
box O
(beta.20) O
[29] O
in O
MATLAB O

- B-DAT
Works, O
Natick). O
We O
used O
a O

-4770 B-DAT
CPU O
(3.40GHz). O
The O
Gaussian O
denoising O

- B-DAT
work O
took O
about O
two O
days O

- B-DAT

- B-DAT
epoch O
system O
that O
repeats O
forward O

- B-DAT
work O
with O
the O
bicubic O
x2 O

- B-DAT

- B-DAT

- B-DAT
mance O
and O
manifold O
simplification, O
we O

- B-DAT
ogy O
of O
the O
input O
and O

- B-DAT

- B-DAT
noising O
performance, O
we O
used O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
noising O
methods O
in O
terms O
of O

- B-DAT
bara O
and O
House, O
we O
attained O

- B-DAT
posed O
method O
showed O
superior O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
composition, O
additional O
comparative O
studies O
with O

- B-DAT
line O
network O
were O
performed. O
Here O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fling O
scheme O
in O
[25] O
except O

- B-DAT
polated. O
Accordingly, O
the O
networks O
using O

- B-DAT
actly O
same O
architecture O
except O
the O

- B-DAT
plification. O
Here, O
the O
Gaussian O
denoising O

- B-DAT
bara O
image O
was O
used O
for O

- B-DAT

-3 B-DAT
Proposed-P O
Proposed O
Set5 O
(2) O
37.53/0.9586 O

-3, B-DAT
Proposed-P(primary) O
networks O
are O
291 O
dataset[18 O

- B-DAT

- B-DAT
stored O
RGB O
was O
used O
to O

- B-DAT
ues O

- B-DAT
lar, O
our O
networks O
were O
competitive O

-67 B-DAT
seconds O
computational O
time O
by O
the O

-5 B-DAT
seconds O
for O
each O
frame. O
Since O

- B-DAT

- B-DAT
known O
decimation O
dataset O
where O
we O

- B-DAT
ifold O
simplification O
from O
the O
residual O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eas. O
We O
provide O
more O
comparative O

- B-DAT
mentary O
material O

- B-DAT
ogy O
analysis, O
we O
showed O
that O

- B-DAT
ing O
as O
well O
as O
the O

- B-DAT
ing O
and O
NTIRE O
SISR O
competition O

- B-DAT
mance O
and O
speed. O
Moreover, O
we O

- B-DAT

- B-DAT
ing O
Foundation, O
Grant O
number O
NRF-2013M3A9B2076548 O

- B-DAT
puter O
Vision, O
76(2):123–139, O
2008. O
1 O

- B-DAT
sion O
and O
Pattern O
Recognition O
(CVPR O

- B-DAT
mized O
reaction O
diffusion O
processes O
for O

- B-DAT

- B-DAT
den O
Markov O
models. O
IEEE O
Transactions O

- B-DAT

- B-DAT

- B-DAT
laborative O
filtering. O
IEEE O
Transactions O
on O

- B-DAT
cessing, O
16(8):2080–2095, O
2007. O
1 O

- B-DAT
ume O
61. O
SIAM, O
1992. O
3 O

- B-DAT
tralized O
sparse O
representation O
for O
image O

- B-DAT

- B-DAT

- B-DAT
a O
survey. O
Contemporary O
Mathematics, O
453:257–282 O

- B-DAT
ual O
learning O
for O
image O
recognition O

- B-DAT

- B-DAT
tional O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT

- B-DAT
tern O
Recognition O
(CVPR), O
pages O
5197–5206 O

- B-DAT
erating O
deep O
network O
training O
by O

- B-DAT

- B-DAT
works. O
arXiv O
preprint O
arXiv:1511.04587, O
2015 O

- B-DAT
agenet O
classification O
with O
deep O
convolutional O

- B-DAT
ing O
Systems, O
pages O
1097–1105, O
2012 O

- B-DAT
demic O
press, O
1999. O
3 O

- B-DAT

- B-DAT
based O
image O
restoration. O
Multiscale O
Modeling O

- B-DAT
ulation, O
4(2):460–489, O
2005. O
1 O

- B-DAT
moncelli. O
Image O
denoising O
using O
scale O

- B-DAT

- B-DAT
volutional O
networks O
for O
biomedical O
image O

- B-DAT
tion. O
In O
International O
Conference O
on O

- B-DAT
age O
Computing O
and O
Computer-Assisted O
Intervention O

- B-DAT

- B-DAT
gle O
image O
and O
video O
super-resolution O

- B-DAT

- B-DAT
tion O
(CVPR), O
pages O
1874–1883, O
2016 O

- B-DAT
ory, O
volume O
1. O
Wiley O
New O

- B-DAT

-1995 B-DAT

-001 B-DAT

- B-DAT
Hall, O
1995. O
7 O

- B-DAT
celli. O
Image O
quality O
assessment: O
from O

- B-DAT

- B-DAT
ror O
as O
the O
loss, O
the O

- B-DAT

- B-DAT
variant O
under O
batch O
normalization O
is O

- B-DAT

- B-DAT
culated O
the O
barcodes O
of O
the O

- B-DAT
box O
called O
JAVAPLEX O
(http://appliedtopology.github.io/ O
javaplex O

- B-DAT
posed O
of O
residual O
image O
patches O

- B-DAT
ical O
complexity O
in O
the O
image O

- B-DAT
thogonal O
Haar O
wavelet O
transform. O
The O

- B-DAT
form O
which O
further O
reduces O
the O

- B-DAT
resolution O
datasets O
(the O
right O
column O

- B-DAT
codes, O
the O
input O
manifold O
of O

- B-DAT
ages O
had O
simpler O
topology O
than O

- B-DAT
noising O
, O
(b) O
super-resolution O
(bicubic O

- B-DAT
resolution O
(unknown O
decimation) O
tasks. O
(Right O

- B-DAT
ogy O
analysis O
of O
input O
manifold O

- B-DAT

- B-DAT

- B-DAT
tion) O
tasks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ditional O
simpler O
label O
manifold O
from O

- B-DAT
noising O
and O
SISR O
reconstruction O
from O

Berkeley O
segmentation O
(BSD500) O
[4] O
and O
Urban100 B-DAT

400 O
images O
of O
BSD500 O
and O
Urban100 B-DAT
datasets O
for O
training O
in O
the O

Urban100 B-DAT
(2) O
30.76/0.9140 O
30.75/0.9134 O
30.96/0.9169 O
32.63/0.9330 O

3) O
28.82/0.7976 O
28.84/0.7976 O
28.86/0.7987 O
29.18/0.8071 O
Urban100 B-DAT
(3) O
27.14/0.8279 O
27.15/0.8272 O
27.28/0.8334 O
28.50/0.8587 O

4) O
27.29/0.7251 O
27.29/0.7247 O
27.32/0.7266 O
27.66/0.7380 O
Urban100 B-DAT
(4) O
25.18/0.7524 O
25.21/0.7518 O
25.36/0.7614 O
26.42/0.7940 O

results O
of O
“barbara” O
(Set14) O
with O
upscaling B-DAT
factor O
×4 O

results O
of O
“barbara” O
(Set14) O
with O
upscaling B-DAT
factor O
×4 O

results O
of O
“barbara” O
(Set14) O
with O
upscaling B-DAT
factor O
×4 O

results O
of O
“barbara” O
(Set14) O
with O
upscaling B-DAT
factor O
×4 O

Urban100 B-DAT
15 I-DAT
32.34 O
/ O
0.9220 O
31.98 O

0.122 O
26.52 O
/ O
0.088 O
Urban100 B-DAT
26 I-DAT

Urban100 B-DAT
26 I-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works O
(CNNs) O
generally O
enlarge O
the O

- B-DAT

- B-DAT

- B-DAT
thermore, O
another O
convolutional O
layer O
is O

- B-DAT
crease O
the O
channels O
of O
feature O

- B-DAT
network, O
inverse O
wavelet O
transform O
is O

- B-DAT
construct O
the O
high O
resolution O
feature O

- B-DAT
tering O
and O
subsampling, O
and O
can O

- B-DAT

- B-DAT

- B-DAT
tion O
from O
both O
prior O
modeling O

- B-DAT
tional O
neural O
networks O
(CNNs) O
have O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eral O
representative O
image O
restoration O
tasks O

- B-DAT

- B-DAT
ing O
[57], O
image O
deblurring O
[58 O

-3 B-DAT
10-2 O
10-1 O
100 O
101 O

- B-DAT
PCN O
[45], O
VDSR O
[29], O
DnCNN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
more O
complex O
image O
restoration O

- B-DAT
ping O
from O
degraded O
observation O
to O

- B-DAT
tional O
network O
(FCN) O
by O
removing O

- B-DAT
formance O
by O
taking O
more O
spatial O

- B-DAT
ever, O
for O
FCN O
without O
pooling O

- B-DAT
ing O
filters O
with O
larger O
size O

- B-DAT
ently O
suffers O
from O
gridding O
effect O

- B-DAT
large O
receptive O
field O
while O
avoiding O

- B-DAT
tational O
burden O
and O
the O
potential O

- B-DAT
trates O
the O
receptive O
field, O
run O

- B-DAT
RCNN O
[14] O
has O
relatively O
larger O

- B-DAT

- B-DAT
off O
between O
performance O
and O
efficiency O

- B-DAT

- B-DAT
tracting O
subnetwork O
and O
an O
expanding O

- B-DAT
ture O
maps O
[12, O
13], O
which O

- B-DAT
tailed O
texture. O
In O
the O
expanding O

- B-DAT
wise O
summation O
is O
adopted O
for O

- B-DAT
over, O
dilated O
filtering O
can O
also O

- B-DAT
larging O
receptive O
field. O
Experiments O
on O

- B-DAT
tiveness O
and O
efficiency O
of O
our O

- B-DAT
ure O
1, O
MWCNN O
is O
moderately O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ically, O
more O
discussions O
are O
given O

- B-DAT
ing O
[25]. O
These O
early O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cently, O
multi-layer O
perception O
(MLP) O
has O

- B-DAT
corporating O
residual O
learning O
with O
batch O

- B-DAT
ditional O
non-CNN O
based O
methods. O
Mao O

- B-DAT
gest O
to O
add O
symmetric O
skip O

- B-DAT
proving O
denoising O
performance. O
For O
better O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
CNN O
[16], O
which O
adopts O
a O

- B-DAT

- B-DAT
work O
[29], O
residual O
units O
[32 O

- B-DAT
tional O
cost O
or O
loss O
of O

- B-DAT

- B-DAT
tween O
receptive O
field O
size O
and O

- B-DAT
erative O
adversarial O
networks O
(GANs) O
have O

- B-DAT
duced O
to O
improve O
the O
visual O

- B-DAT
fers O
from O
blocking O
effect O
and O

- B-DAT

- B-DAT

- B-DAT
ing. O
For O
example, O
both O
DnCNN O

- B-DAT
noisers O
can O
also O
serve O
as O

- B-DAT

- B-DAT

- B-DAT
ers O
[58]. O
Romano O
et O
al O

- B-DAT
ing O
CNN O
on O
wavelet O
subbands O

- B-DAT

- B-DAT
lutional O
framelets O
[21, O
54] O
have O

- B-DAT

- B-DAT
composition. O
Deep O
convolutional O
framelets O
independently O

- B-DAT

- B-DAT
form, O
our O
MWCNN O
can O
embed O

- B-DAT
text O
and O
inter-subband O
dependency O

- B-DAT

- B-DAT

- B-DAT
chitecture. O
Finally, O
discussion O
is O
given O

- B-DAT
nection O
of O
MWCNN O
with O
dilated O

- B-DAT

- B-DAT
age O
x O
[36]. O
The O
convolution O

- B-DAT
nal O
property O
of O
DWT, O
the O

- B-DAT

- B-DAT
cessed O
with O
DWT O
to O
produce O

- B-DAT

- B-DAT
sition O
and O
reconstruction O
of O
an O

- B-DAT
ers. O
In O
the O
decomposition O
stage O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
construction O
result O
at O
the O
current O

- B-DAT

- B-DAT
quired O
to O
process O
the O
decomposition O

- B-DAT
lored O
to O
specific O
task. O
In O

- B-DAT

- B-DAT

- B-DAT
eralization O
of O
multi-level O
WPT, O
and O

- B-DAT
sampling O
operations O
safely O
without O
information O

- B-DAT
over, O
compared O
with O
conventional O
CNN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ations. O
As O
to O
the O
last O

- B-DAT
ing O
subnetwork. O
Generally, O
MWCNN O
modifies O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sponds O
to O
a O
multi-channel O
feature O

- B-DAT

- B-DAT
Net[41], O
while O
DWT O
and O
IWT O

- B-DAT
Net, O
the O
downsampling O
has O
no O

- B-DAT
nels, O
and O
the O
subsequent O
convolution O

- B-DAT
crease O
feature O
map O
channels. O
(iii O

- B-DAT
wise O
summation O
is O
used O
to O

- B-DAT
ventional O
U-Net O
concatenation O
is O
adopted O

- B-DAT
tion, O
Haar O
wavelet O
is O
adopted O

- B-DAT
ered O
in O
our O
experiments O

- B-DAT

- B-DAT
responding O
ground-truth O
image. O
The O
objective O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
convolution O
in O
MWCNN, O
respectively. O
When O

- B-DAT
bands O
are O
taken O
into O
account O

- B-DAT
mation O
loss O
caused O
by O
conventional O

- B-DAT
lated O
filtering O
with O
factor O
2 O

- B-DAT
fined O
analogously. O
We O
also O
have O

- B-DAT
lated O
filtering O
and O
MWCNN O
for O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
field O
of O
MWCNN. O
One O

- B-DAT
mance O
quantitatively O
and O
qualitatively O

- B-DAT
sion O
artifacts O
removal. O
Comparison O
of O

- B-DAT

- B-DAT
lowing O
[57], O
we O
consider O
three O

- B-DAT
ing O
four O
compression O
quality O
settings O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tel(R) O
Core(TM) O
i7-5820K O
CPU O
3.30GHz O

- B-DAT
ing O
methods O
are O
only O
tested O

- B-DAT
ban100 O
[23]. O
Table O
1 O
lists O

- B-DAT
ure O
5 O
shows O
the O
denoising O

- B-DAT

0.9027 O
32.77 O
/ O
0.9008 O
- B-DAT
- O
33.15 O
/ O
0.9088 O
25 O
29.97 O

0.8618 O
30.38 O
/ O
0.8601 O
- B-DAT
- O
30.79 O
/ O
0.8711 O
50 O
26.72 O

0.8906 O
31.63 O
/ O
0.8881 O
- B-DAT
- O
31.86 O
/ O
0.8947 O
25 O
28.57 O

0.8278 O
29.15 O
/ O
0.8249 O
- B-DAT
- O
29.41 O
/ O
0.8360 O
50 O
25.62 O

0.9250 O
32.49 O
/ O
0.9244 O
- B-DAT
- O
33.17 O
/ O
0.9357 O
25 O
29.70 O

0.8792 O
29.82 O
/ O
0.8839 O
- B-DAT
- O
30.66 O
/ O
0.9026 O
50 O
25.94 O

0.9593 O
37.66 O
/ O
0.9599 O
- B-DAT
37.52 O
/ O
0.9590 O
37.74 O

0.9222 O
33.82 O
/ O
0.9230 O
- B-DAT
- O
34.03 O
/ O
0.9244 O
34.09 O

0.9118 O
32.94 O
/ O
0.9144 O
- B-DAT
33.08 O
/ O
0.9130 O
33.23 O

0.8349 O
29.61 O
/ O
0.8341 O
- B-DAT
- O
29.96 O
/ O
0.8349 O
30.00 O

BSD100 O
×2 O
- B-DAT
31.90 O
/ O
0.8960 O
31.85 O

0.8942 O
31.98 O
/ O
0.8974 O
- B-DAT
31.80 O
/ O
0.8950 O
32.05 O

0.8995 O
32.23 O
/ O
0.8999 O
×3 O
- B-DAT
28.82 O
/ O
0.7976 O
28.80 O

0.7963 O
28.92 O
/ O
0.7993 O
- B-DAT
- O
28.95 O
/ O
0.8004 O
28.96 O

0.7987 O
29.12 O
/ O
0.8060 O
×4 O
- B-DAT
27.29 O
/ O
0.7251 O
27.23 O

Urban100 O
×2 O
- B-DAT
30.76 O
/ O
0.9140 O
30.75 O

0.9133 O
30.91 O
/ O
0.9159 O
- B-DAT
30.41 O
/ O
0.9100 O
31.23 O

0.9169 O
32.30 O
/ O
0.9296 O
×3 O
- B-DAT
27.14 O
/ O
0.8279 O
27.15 O

0.8276 O
27.31 O
/ O
0.8303 O
- B-DAT
- O
27.53 O
/ O
0.8378 O
27.56 O

0.8334 O
28.13 O
/ O
0.8514 O
×4 O
- B-DAT
25.18 O
/ O
0.7524 O
25.20 O

0.8837 O
32.91 O
/ O
0.8861 O
- B-DAT
33.43 O
/ O
0.8930 O
40 O
32.43 O

0.8911 O
33.34 O
/ O
0.8953 O
- B-DAT
33.77 O
/ O
0.9003 O
- O
34.27 O

0.9059 O
32.98 O
/ O
0.9090 O
- B-DAT
33.45 O
/ O
0.9153 O
40 O
32.35 O

0.9173 O
33.63 O
/ O
0.9198 O
- B-DAT
33.96 O
/ O
0.9247 O
- O
34.45 O

- B-DAT

- B-DAT
peting O
methods O
on O
the O
four O

- B-DAT
forms O
favorably O
in O
terms O
of O

- B-DAT
dexes. O
Compared O
with O
VDSR, O
our O

- B-DAT
perform O
VDSR, O
and O
also O
is O

- B-DAT
ResNet O
on O
Set14. O
Figure O
6 O

-1 B-DAT
method O
by O
0.37dB O

- B-DAT
overlapped O
8 O
× O
8 O
blocks O

- B-DAT
troducing O
the O
blocking O
artifact. O
The O

- B-DAT
mined O
by O
a O
quality O
factorQ O

- B-DAT
sider O
[18, O
19] O
due O
to O

- B-DAT
peting O
methods O
on O
Classic5 O
and O

- B-DAT
Net O
[48]) O
for O
the O
quality O

- B-DAT

- B-DAT

- B-DAT
ing O
library O
is O
adopted O
to O

- B-DAT
based O
methods O
with O
source O
codes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
s, O
MWCNN O
is O
moderately O
slower O

- B-DAT
stead O
of O
the O
increase O
of O

- B-DAT
ness O
of O
MWCNN O
should O
be O

- B-DAT
amples, O
we O
compare O
the O
PSNR O

- B-DAT

- B-DAT
fault O
MWCNN O
with O
Haar O
wavelet O

-2 B-DAT
wavelet, O
and O
(iii) O
MWCN- O
N O

-2 B-DAT
in O
expanding O
subnetwork. O
Then, O
abla O

- B-DAT
tion O
experiments O
are O
provided O
for O

- B-DAT
ness O
of O
additionally O
embedded O
wavelet O

- B-DAT
Net O
with O
same O
architecture O
to O

- B-DAT

- B-DAT
ing O
sum O
connection O
instead O
of O

- B-DAT
Net+D: O
adopting O
learnable O
conventional O
downsamping O

- B-DAT

- B-DAT

- B-DAT
ing O
library O
is O
adopted O
to O

- B-DAT
based O
methods O
with O
source O
codes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
s, O
MWCNN O
is O
moderately O
slower O

- B-DAT
stead O
of O
the O
increase O
of O

- B-DAT
ness O
of O
MWCNN O
should O
be O

- B-DAT
amples, O
we O
compare O
the O
PSNR O

- B-DAT

- B-DAT
fault O
MWCNN O
with O
Haar O
wavelet O

-2 B-DAT
wavelet, O
and O
(iii) O
MWCN- O
N O

-2 B-DAT
in O
expanding O
subnetwork. O
Then, O
abla O

- B-DAT
tion O
experiments O
are O
provided O
for O

- B-DAT
ness O
of O
additionally O
embedded O
wavelet O

- B-DAT
Net O
with O
same O
architecture O
to O

- B-DAT

- B-DAT
ing O
sum O
connection O
instead O
of O

- B-DAT
Net+D: O
adopting O
learnable O
conventional O
downsamping O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
library O
is O
adopted O
to O

- B-DAT
based O
methods O
with O
source O
codes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
s, O
MWCNN O
is O
moderately O
slower O

- B-DAT
stead O
of O
the O
increase O
of O

- B-DAT
ness O
of O
MWCNN O
should O
be O

- B-DAT
amples, O
we O
compare O
the O
PSNR O

- B-DAT

- B-DAT
fault O
MWCNN O
with O
Haar O
wavelet O

-2 B-DAT
wavelet, O
and O
(iii) O
MWCN- O
N O

-2 B-DAT
in O
expanding O
subnetwork. O
Then, O
abla O

- B-DAT
tion O
experiments O
are O
provided O
for O

- B-DAT
ness O
of O
additionally O
embedded O
wavelet O

- B-DAT
Net O
with O
same O
architecture O
to O

- B-DAT

- B-DAT
ing O
sum O
connection O
instead O
of O

- B-DAT
Net+D: O
adopting O
learnable O
conventional O
downsamping O

- B-DAT

- B-DAT
ing O
library O
is O
adopted O
to O

- B-DAT
based O
methods O
with O
source O
codes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fectiveness O
of O
MWCNN O
should O
be O

- B-DAT
ration O
of O
CNN O
and O
DWT O

- B-DAT

- B-DAT
fault O
MWCNN O
with O
Haar O
wavelet O

-2 B-DAT
wavelet, O
and O
(iii) O
MWCNN O
(HD O

-2 B-DAT
in O
expanding O
subnetwork. O
Then, O
ablation O

- B-DAT
periments O
are O
provided O
for O
verifying O

- B-DAT
ditionally O
embedded O
wavelet: O
(i) O
the O

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT
U-Net O
[41] O
U-Net+S O
U-Net+D O
DCF O

0.355 O
26.65 O
/ O
0.354 O
- B-DAT
/ O
- O
27.42 O
/ O
0.343 O

0.097 O
29.57 O
/ O
0.104 O
- B-DAT
/ O
- O
30.01 O
/ O
0.088 O

0.120 O
29.38 O
/ O
0.155 O
- B-DAT
/ O
- O
29.69 O
/ O
0.112 O

- B-DAT

-2 B-DAT

- B-DAT

- B-DAT
dicate O
that O
using O
sum O
connection O

- B-DAT
frequency O
localization O
property O
in O
wavelet O

- B-DAT
dent O
processing O
of O
subbands O
harms O

- B-DAT
pared O
to O
MWCNN O
(DB2) O
and O

- B-DAT
uation. O
MWCNN O
(Haar) O
has O
similar O

- B-DAT

- B-DAT
tween O
performance O
and O
efficiency O

- B-DAT
position, O
where O
different O
CNNs O
are O

- B-DAT
band. O
However, O
the O
results O
in O

- B-DAT
pendent O
processing O
of O
subbands O
is O

- B-DAT

- B-DAT
ier O
computational O
burden. O
Thus, O
a O

- B-DAT
NNs O
with O
different O
levels O
on O

-1 B-DAT
MWCNN-2 O
MWCNN-3 O
MWCNN-4 O

-1 B-DAT
∼MWCNN-4). O
It O
can O
be O
observed O

-3 B-DAT
with O
24-layer O
architecture O
performs O
much O

-1 B-DAT
and O
MWCNN-2, O
while O
MWCNN-4 O
only O

-3 B-DAT
in O
terms O
of O
the O
PSNR O

-3 B-DAT
is O
also O
moderate O
compared O
with O

-3 B-DAT
as O
the O
default O
setting O

- B-DAT

- B-DAT

- B-DAT
fectiveness O
and O
efficiency O
of O
MWCNN O

- B-DAT
eral O
restoration O
tasks O
such O
as O

- B-DAT

- B-DAT

- B-DAT
gle O
image O
super-resolution: O
Dataset O
and O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
shops O
(CVPRW), O
pages O
1122–1131. O
IEEE O

- B-DAT
demic O
Press, O
2001 O

- B-DAT
ing O
for O
image O
restoration: O
Persistent O

- B-DAT

- B-DAT
ifold O
simplification. O
In O
IEEE O
Conference O

- B-DAT
tion. O
IEEE O
Signal O
Processing O
Magazine O

- B-DAT
Morel. O
Low-complexity O
single-image O
super-resolution O
based O

- B-DAT
noising: O
Can O
plain O
neural O
networks O

- B-DAT
tion, O
pages O
2392–2399, O
2012 O

- B-DAT
olding O
for O
image O
denoising O
and O

- B-DAT
tions O
on O
Image O
Processing, O
9(9):1532–1546 O

- B-DAT
tion. O
IEEE O
Transactions O
on O
Pattern O

- B-DAT
age O
denoising O
by O
sparse O
3-d O

- B-DAT

- B-DAT
tive O
filtering. O
IEEE O
Transactions O
on O

- B-DAT

- B-DAT
ization O
and O
signal O
analysis. O
IEEE O

- B-DAT
tion O
Theory, O
36(5):961–1005, O
1990 O

- B-DAT
ference O
on O
Computer O
Vision, O
pages O

- B-DAT
sion O
artifacts O
reduction O
by O
a O

- B-DAT

- B-DAT
tion, O
pages O
2862–2869, O
2014 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
shops O
(CVPRW), O
2017 O

- B-DAT

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
lutional O
networks. O
In O
Advances O
in O

- B-DAT
cessing O
Systems, O
pages O
769–776, O
2009 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lishing O
Company, O
Incorporated, O
2012 O

- B-DAT

- B-DAT
volutional O
network O
for O
image O
super-resolution O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT
timization. O
In O
International O
Conference O
for O

- B-DAT
sentations, O
2015 O

- B-DAT
resolution. O
IEEE O
Conference O
on O
Computer O

- B-DAT
tern O
Recognition, O
2017 O

- B-DAT

- B-DAT

- B-DAT
ative O
adversarial O
network. O
IEEE O
Conference O

- B-DAT

- B-DAT
cessing, O
1(2):244–250, O
1992 O

- B-DAT
ing O
convolutional O
networks O
for O
content-weighted O

- B-DAT
pression. O
IEEE O
Conference O
on O
Computer O

- B-DAT
position: O
the O
wavelet O
representation. O
IEEE O

- B-DAT

- B-DAT
ric O
skip O
connections. O
In O
Advances O

- B-DAT
cessing O
Systems, O
pages O
2802–2810, O
2016 O

- B-DAT
cal O
statistics. O
In O
IEEE O
Conference O

- B-DAT
ence O
Computer O
Vision, O
volume O
2 O

- B-DAT
ing O
for O
image O
quality O
assessment O

- B-DAT

- B-DAT
tional O
networks O
for O
biomedical O
image O

- B-DAT
ternational O
Conference O
on O
Medical O
Image O

- B-DAT

- B-DAT

- B-DAT
ized O
deep O
image O
to O
image O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
preserving O
image O
super-resolution O
via O
contextualized O

- B-DAT
task O
learning. O
IEEE O
Transactions O
on O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
national O
conference O
on O
Multimedia, O
pages O

- B-DAT
mentation. O
arXiv O
preprint O
arXiv:1702.08502, O
2017 O

- B-DAT

- B-DAT

- B-DAT
ing O
with O
deep O
neural O
networks O

- B-DAT
dustrial O
and O
Applied O
Mathematics, O
2018 O

- B-DAT

- B-DAT
lated O
convolutions. O
arXiv O
preprint O
arXiv:1511.07122 O

- B-DAT

- B-DAT

- B-DAT
yond O
a O
gaussian O
denoiser: O
Residual O

Set12, O
and O
1.2dB O
higher O
on O
Urban100 B-DAT

more O
results O
on O
Set12 O
and O
Urban100 B-DAT

Set14 O
[56], O
BSD100 O
[38], O
and O
Urban100 B-DAT
[23], O
because O
they O
are O
widely O

on O
datasets O
Set14, O
BSD68 O
and O
Urban100 B-DAT

Urban100 B-DAT
15 O
32.34 O
/ O
0.9220 O
31.98 O

datasets O
Set5, O
Set14, O
BSD100 O
and O
Urban100 B-DAT

Urban100 B-DAT
×2 O
- O
30.76 O
/ O
0.9140 O

on O
Set5 O
and O
Set14. O
On O
Urban100, B-DAT
our O
MWCNN O
outperforms O
VDSR O
by O

0.122 O
26.52 O
/ O
0.088 O
Urban100 B-DAT
26.56 O
/ O
0.764 O
24.18 O

Urban100 B-DAT
26.08 O
/ O
0.212 O
27.10 O

Params O
MultAdds O
Set5 O
Set14 O
B100 O
Urban100 B-DAT
PSNR/SSIM I-DAT
PSNR/SSIM O
PSNR/SSIM O
PSNR/SSIM O

image034 O
from O
Urban100 B-DAT
HR I-DAT

image067 O
from O
Urban100 B-DAT
HR I-DAT

- B-DAT

- B-DAT

- B-DAT
fully O
applied O
to O
single-image O
super-resolution O

- B-DAT
world O
applications O
due O
to O
the O

- B-DAT

- B-DAT
ture O
that O
implements O
a O
cascading O

- B-DAT
work O
to O
further O
improve O
efficiency O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lution O
limitations, O
and O
could O
be O

- B-DAT

- B-DAT

- B-DAT
vided O
outstanding O
performance O
in O
SISR O

- B-DAT

- B-DAT
tical O
for O
real-world O
applications. O
One O

- B-DAT

- B-DAT

- B-DAT
ple, O
DRCN O
[21] O
uses O
a O

- B-DAT
els O
decrease O
the O
number O
of O

- B-DAT
ducing O
the O
number O
of O
parameters O

- B-DAT

- B-DAT
sider O
a O
situation O
where O
an O

- B-DAT

- B-DAT
lenging O
and O
necessary O
step O
that O

- B-DAT
mand O
for O
streaming O
media O
has O

- B-DAT
ing O
lossy O
compression O
techniques O
before O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
M). O
We O
first O
build O
our O

- B-DAT

- B-DAT
ing O
the O
FSRCNN O
[7], O
CARN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
approaches O
have O
been O
applied O

- B-DAT
based O
SISR O
in O
section O
2.1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rameters O
by O
engaging O
in O
redundant O

- B-DAT

(- B-DAT

- B-DAT

(- B-DAT

- B-DAT

- B-DAT
erations O
in O
(a) O
and O
(b O

- B-DAT

- B-DAT
ter O
category, O
SqueezeNet O
[19] O
builds O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ber O
of O
operations O
compared O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ary O
layers O
are O
cascaded O
into O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ual O
block, O
the O
first O
residual O

- B-DAT
rameter O
of O
the O
convolution O
layer O

- B-DAT
trated O
in O
block O
(c) O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
anism. O
As O
shown O
in O
Fig O

- B-DAT
cading O
on O
both O
the O
local O

- B-DAT

- B-DAT
resentations. O
2) O
Multi-level O
cascading O
connection O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
features. O
This O
facilitates O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
instead O
of O
depthwise O
convolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
scheme O
connects O
all O
blocks O

- B-DAT

- B-DAT
ations, O
while O
we O
gather O
it O

- B-DAT
catenated O
at O
the O
end O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
on O
two O
commonly-used O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ters O
than O
ours. O
The O
CARN-M O

- B-DAT

- B-DAT

- B-DAT
ple, O
CARN O
outperforms O
its O
most O

- B-DAT

- B-DAT
ble O
results O
against O
computationally-expensive O
models O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tiple O
scales O
using O
a O
single O

- B-DAT

(- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
the O
multi-scale O
learning O
to O

- B-DAT

- B-DAT
off O
between O
performance O
vs. O
parameters O

- B-DAT

- B-DAT

- B-DAT

337K O
311.0G O
37.66/0.9590 O
33.38/0.9136 O
31.91/0.8962 O
- B-DAT
LapSRN O
[24] O
813K O
29.9G O
37.52/0.9590 O

974K O
225.7G O
37.89/0.9598 O
33.61/0.9160 O
32.08/0.8984 O
- B-DAT
CARN O
(ours) O
1,592K O
222.8G O
37.76/0.9590 O

- B-DAT

337K O
311.0G O
33.74/0.9226 O
29.90/0.8322 O
28.82/0.7980 O
- B-DAT
DRRN O
[35] O
297K O
6,796.9G O
34.03/0.9244 O

1,159K O
120.0G O
34.27/0.9257 O
30.30/0.8399 O
28.97/0.8025 O
- B-DAT
CARN O
(ours) O
1,592K O
118.8G O
34.29/0.9255 O

- B-DAT

337K O
311.0G O
31.55/0.8856 O
28.15/0.7680 O
27.32/0.7253 O
- B-DAT
LapSRN O
[24] O
813K O
149.4G O
31.54/0.8850 O

1,417K O
83.1G O
32.00/0.8931 O
28.49/0.7783 O
27.44/0.7325 O
- B-DAT
SRDenseNet O
[37] O
2,015K O
389.9G O
32.02/0.8934 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cading. O
The O
network O
topologies O
are O

- B-DAT

- B-DAT
tively O
carries O
mid- O
to O
high-level O

- B-DAT

- B-DAT

- B-DAT
resentations, O
the O
CARN O
model O
can O

- B-DAT

- B-DAT

- B-DAT
NG O
without O
global O
cascading. O
CARN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gation, O
and O
thus O
lead O
to O

- B-DAT

- B-DAT
nections O
inside O
the O
residual O
blocks O

- B-DAT
nation O
and O
1×1 O
convolutions, O
it O

- B-DAT

- B-DAT
tions O
in O
the O
cascading O
connection O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eters, O
and O
PSNR O
vs. O
operations O

- B-DAT

- B-DAT
ically. O
For O
example, O
the O
G64 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
form O
SISR O
accurately O
and O
efficiently O

- B-DAT

- B-DAT

- B-DAT
ent. O
Our O
experiments O
show O
that O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gence O
33(5), O
898–916 O
(2011 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
British O
Machine O

- B-DAT

- B-DAT

- B-DAT
scale O
hierarchical O
image O
database. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
(ICCV) O
(2015 O

- B-DAT
telligence O
and O
Statistics O
(2010 O

- B-DAT
works O
with O
pruning, O
trained O
quantization O

- B-DAT
level O
performance O
on O
imagenet O
classification O

- B-DAT
dreetto, O
M., O
Adam, O
H.: O
Mobilenets O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
and O
Pattern O
Recognition O
(CVPR O

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks. O
In: O
Proceedings O

- B-DAT
tion O
Processing O
Systems O
(NIPS) O
(2012 O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ters O
24(8), O
1208–1212 O
(2017 O

- B-DAT

- B-DAT
mentation. O
In: O
Proceedings O
of O
the O

- B-DAT

- B-DAT
ple O
convolution O
neural O
networks. O
In O

- B-DAT

- B-DAT
cal O
image O
segmentation. O
In: O
Proceedings O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work. O
In: O
Proceedings O
of O
the O

- B-DAT
nition O
(CVPR) O
(2017 O

- B-DAT

- B-DAT
tions. O
In: O
Proceedings O
of O
the O

- B-DAT

- B-DAT
resentation. O
IEEE O
transactions O
on O
image O

- B-DAT

Set14 O
[39], O
B100 O
[29] O
and O
Urban100 B-DAT
[18] O
for O
testing O
and O
benchmarking O

three O
datasets O
(Set14, O
B100 O
and O
Urban100) B-DAT
for O
×4 O
scale. O
It O
can O

Params O
MultAdds O
Set5 O
Set14 O
B100 O
Urban100 B-DAT

image034 O
from O
Urban100 B-DAT

image067 O
from O
Urban100 B-DAT

For O
the O
upscaling B-DAT
part, O
we O
use O
the O
sub-pixel O

layers O
except O
those O
for O
the O
upscaling B-DAT
part. O
All O
our O
networks O
are O

methods, O
since O
MSRN O
uses O
different O
upscaling B-DAT
methods O
using O
more O
parameters O
for O

consist O
of O
natural O
images. O
The O
Urban100 B-DAT

improvement O
is O
more O
prominent O
for O
Urban100 B-DAT

and O
Manga109. O
Since O
Urban100 B-DAT

for O
challeng- O
ing O
images, O
i.e., O
Urban100 B-DAT

Urban100 B-DAT
32 I-DAT

b) O
Urban100 B-DAT
Figure I-DAT
6: O
PSNR O
(dB) O
vs O

meth- O
ods O
widens O
further O
in O
Urban100 B-DAT

params. O
Set5 O
Set14 O
BSD100 O
Urban100 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ral O
networks O
that O
stands O
out O

- B-DAT
tention O
mechanisms O
to O
single O
image O

- B-DAT

- B-DAT

- B-DAT
wise O
and O
spatial O
attention O
mechanisms O

- B-DAT

- B-DAT
imental O
analysis O
of O
different O
attention O

- B-DAT
formance O
in O
comparison O
to O
existing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O

- B-DAT

- B-DAT

- B-DAT
resolution O
(LR) O
image. O
It O
is O

- B-DAT

- B-DAT

- B-DAT
proved O
performance, O
this O
also O
has O

- B-DAT

- B-DAT
mation O
equally, O
which O
may O
not O

- B-DAT
table O
network O
structures O
in O
various O

- B-DAT
lems O
[8, O
30]. O
It O
allows O

- B-DAT
tracted O
feature O
maps, O
so O
that O

- B-DAT
ploy O
attention O
mechanisms. O
Zhang O
et O

- B-DAT
level O
vision O
problem O
[8] O
without O

- B-DAT

- B-DAT

- B-DAT
timized O
for O
SR, O
are O
attached O

- B-DAT

- B-DAT
RAM). O
The O
proposed O
RAM O
exploits O

- B-DAT
and O
intra- O
channel O
relationship O
by O

- B-DAT
spectively. O
We O
demonstrate O
both O
the O

- B-DAT
ciency O
of O
our O
proposed O
method O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ual O
attention O
module O
(RAM) O
based O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tiveness O
and O
efficiency O
of O
our O

- B-DAT

- B-DAT

- B-DAT
ual O
(CSAR) O
block O
[9]. O
Targeting O

- B-DAT
tion O
module O
that O
can O
be O

- B-DAT
nels O
(CA) O
or O
spatial O
regions O

- B-DAT
tention O
map O
M, O
having O
a O

- B-DAT
erated O
attention O
map O
is O
normalized O

- B-DAT

- B-DAT
matically O
in O
Table O
1, O
which O

- B-DAT

- B-DAT

- B-DAT
nism O
aims O
to O
recalibrate O
filter O

- B-DAT
channel O
correlation, O
i.e., O
CA. O
The O

- B-DAT
plied O
in O
the O
squeeze O
process O

- B-DAT
channel O
relation O
for O
refining O
feature O

- B-DAT
ploits O
both O
inter-channel O
and O
inter-spatial O

- B-DAT
ing O
is O
additionally O
performed O
in O

- B-DAT
cess. O
For O
the O
SA O
module O

- B-DAT
tially O
performs O
CA O
and O
then O

- B-DAT
volutions, O
where O
the O
first O
one O

- B-DAT
tio. O
While O
CBAM O
combines O
the O

- B-DAT
spired O
by O
EDSR O
[19], O
is O

CSAR O
[9] O
- B-DAT
M O
= O
conv1×1(conv1×1(X)) O
RAM O

RCAB O
[33] O
- B-DAT
CBAM O
[30] O
X̂ O
= O
fSA(fCA(X O

- B-DAT

- B-DAT

- B-DAT
scaling O
part. O
Let O
ILR O
and O

- B-DAT
tion O
3.2. O
F0 O
is O
updated O

- B-DAT
tion, O
and O
then O
the O
updated O

- B-DAT

- B-DAT

- B-DAT
struction O

- B-DAT
ing O
and O
reconstruction, O
respectively, O
and O

- B-DAT

- B-DAT
ditionally O
propose O
a O
way O
to O

- B-DAT
pose O
residual O
attention O
module O
(RAM O

- B-DAT

- B-DAT
lution, O
ReLU, O
and O
convolution, O
and O

- B-DAT
posed O
FA O
mechanism O

- B-DAT

- B-DAT
puter O
vision O
problems O
such O
as O

- B-DAT
ject O
detection O
without O
modification. O
However O

- B-DAT
mately O
aims O
at O
restoring O
high-frequency O

- B-DAT
ages, O
it O
is O
more O
reasonable O

- B-DAT
mined O
using O
high-frequency O
statistics O
about O

- B-DAT
ters O
will O
extract O
the O
edge O

- B-DAT
tion. O
From O
the O
viewpoint O
of O

- B-DAT
nels O
varies O
by O
the O
spatial O

- B-DAT
frequency O
components O
such O
as O
sky O

- B-DAT
like O
CBAM O
[30], O
which O
performs O

- B-DAT
formation O
per O
channel O
to O
preserve O

- B-DAT

- B-DAT
teristics. O
In O
addition, O
for O
the O

- B-DAT
trast O
to O
other O
SA O
mechanisms O

- B-DAT

- B-DAT

- B-DAT
volution O

- B-DAT
anisms O
exploit O
information O
from O
inter-channel O

- B-DAT
channel O
relationship, O
respectively. O
Therefore, O
in O

- B-DAT

- B-DAT
ered O
in O
detail O
in O
Section O

- B-DAT
ban100 O
[11], O
Manga109 O
[21]. O
The O

- B-DAT

- B-DAT
ferent O
characteristics O
from O
natural O
ones O

- B-DAT
to-noise O
ration O
(PSNR) O
and O
structural O

- B-DAT
dex O
on O
the O
Y O
channel O

- B-DAT

- B-DAT
domly O
crop O
a O
48×48 O
patch O

- B-DAT
lected O
16 O
LR O
training O
images O

- B-DAT

- B-DAT
ing O
images O
for O
each O
RGB O

- B-DAT
ment O
our O
networks O
using O
the O

- B-DAT
ploying O
each O
mechanism O
increases O
the O

- B-DAT
rameters, O
and O
as O
the O
network O

- B-DAT
ment O
with O
the O
easiest O
case O

- B-DAT
acteristics O
to O
check O
the O
generalization O

- B-DAT
wise O
and O
spatial O
information O
is O

- B-DAT
nism O
leads O
to O
performance O
improvement O

- B-DAT
age) O
only O
by O
adding O
9K O

- B-DAT
ages O
and O
the O
images O
in O

- B-DAT
tics O
of O
computer-generated O
images, O
which O

- B-DAT
ent O
from O
those O
in O
the O

- B-DAT
mance O
improvement O
is O
achieved O
only O

- B-DAT
pare O
it O
with O
the O
other O

- B-DAT
lustrated O
in O
Section O
2. O
For O

- B-DAT
mance. O
Overall, O
all O
the O
cases O

- B-DAT
ters O
than O
our O
network O
(+257K O

- B-DAT
ods O
of O
the O
squeeze O
process O

- B-DAT
served O
that O
our O
method O
extracting O

- B-DAT

- B-DAT
ing O
images, O
i.e., O
Urban100 O
and O

- B-DAT

- B-DAT
wise O
convolution O
is O
an O
effective O

- B-DAT
ditionally O
used, O
but O
at O
the O

- B-DAT
ure O
4. O
We O
have O
three O

- B-DAT
nisms O
showing O
higher O
performance O
have O

- B-DAT
ing O
the O
corresponding O
blocks O
only O

- B-DAT
nism. O
3) O
Our O
FA O
mechanism O

- B-DAT
ods, O
and O
those O
marked O
with O

- B-DAT
els O
with O
varying O
the O
number O

- B-DAT
scale O
SR O
are O
marked O
with O

- B-DAT
scale O
SR O
are O
marked O
with O

- B-DAT
tive O
(red O
color) O
and O
negative O

- B-DAT

- B-DAT
ground) O
and O
the O
high-frequency O
components O

- B-DAT
frequency O
component O
values O
to O
zero O

- B-DAT
tention” O
to O
the O
high-frequency O
components O

- B-DAT

- B-DAT
termined O
by O
the O
number O
of O

- B-DAT
tional O
parameters O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ness O
of O
RAM O
on O
large O

- B-DAT
responds O
to O
the O
opposite O
case O

- B-DAT
RAM O
R10C64, O
64×64 O
patches O
and O

- B-DAT

- B-DAT
rized O
in O
Table O
3, O
Table O

- B-DAT
ods O
widens O
further O
in O
Urban100 O

- B-DAT

- B-DAT
sual O
results O
of O
challenging O
images O

- B-DAT
mance O
with O
fewer O
parameters O
than O

- B-DAT
strates O
the O
efficiency O
of O
ours O

- B-DAT

- B-DAT
anisms O
are O
integrated O
in O
our O

- B-DAT
strated O
that O
our O
attention O
methods O

- B-DAT
tion O
mechanisms O
for O
SR O
and O

- B-DAT

- B-DAT

- B-DAT
work. O
In O
Proceedings O
of O
the O

- B-DAT
puter O
Vision O
(ECCV), O
2018. O
1 O

- B-DAT
Morel. O
Low-complexity O
single-image O
super-resolution O
based O

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
European O
Conference O

- B-DAT

- B-DAT

- B-DAT
works. O
In O
Proceedings O
of O
the O

- B-DAT
jection O
networks O
for O
super-resolution. O
In O

- B-DAT
tion O
(CVPR), O
2018. O
1, O
7 O

- B-DAT
cient O
convolutional O
neural O
networks O
for O

- B-DAT
cations. O
arXiv O
preprint O
arXiv:1704.04861, O
2017 O

- B-DAT

- B-DAT

- B-DAT
works. O
In O
Proceedings O
of O
the O

- B-DAT

- B-DAT
resolution. O
arXiv O
preprint O
arXiv:1809.11130, O
2018 O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
(CVPR), O
2016. O
1 O

- B-DAT
mization. O
arXiv O
preprint O
arXiv:1412.6980, O
2014 O

- B-DAT
resolution. O
In O
Proceedings O
of O
the O

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT
tive O
adversarial O
network. O
In O
Proceedings O

- B-DAT
ence O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cal O
statistics. O
In O
Proceedings O
of O

- B-DAT
ference O
on O
Computer O
Vision O
(ICCV O

- B-DAT
masaki, O
and O
K. O
Aizawa. O
Sketch-based O

- B-DAT
ing O
manga109 O
dataset. O
Multimedia O
Tools O

- B-DAT
ceedings O
of O
the O
Advances O
in O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
IEEE O
International O

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT

- B-DAT
lutional O
block O
attention O
module. O
In O

- B-DAT
pean O
Conference O
on O
Computer O
Vision O

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT

- B-DAT

- B-DAT
tional O
Conference O
on O
Curves O
and O

- B-DAT

- B-DAT
ual O
dense O
network O
for O
image O

- B-DAT

- B-DAT
ings O
of O
the O
IEEE O
Conference O

consist O
of O
natural O
images. O
The O
Urban100 B-DAT
dataset O
includes O
images O
related O
to O

improvement O
is O
more O
prominent O
for O
Urban100 B-DAT
and O
Manga109. O
Since O
Urban100 O
contains O

for O
challeng- O
ing O
images, O
i.e., O
Urban100 B-DAT
and O
Manga109. O
Comparing O
the O
SA O

Urban100 B-DAT
32.13 O
32.26 O
32.13 O
32.28 O
32.24 O

b) O
Urban100 B-DAT

meth- O
ods O
widens O
further O
in O
Urban100 B-DAT
and O
Manga109. O
Note O
that O
our O

results O
on O
challenging O
images O
of O
Urban100 B-DAT
[11 O

params. O
Set5 O
Set14 O
BSD100 O
Urban100 B-DAT
Manga109PSNR O
/ O
SSIM O
PSNR O

as O
nearest-neighbor, O
bilinear, O
and O
bicubic O
upscaling B-DAT

the O
proposed O
methods O
for O
an O
upscaling B-DAT
factor O
of O
2 O
on O
the O

the O
com- O
putational O
complexity O
than O
upscaling B-DAT
at O
the O
initial O
stage O
[3,6,12 O

a O
recur- O
sive O
manner, O
and O
upscaling B-DAT

image O
is O
obtained O
from O
the O
upscaling B-DAT
module O

initial O
feature O
extraction, O
RRB, O
and O
upscaling B-DAT
parts. O
On O
the O
other O
hand O

status O
are O
inputted O
to O
the O
upscaling B-DAT
part. O
We O
investigate O
the O
effectiveness O

23]. O
For O
instance, O
in O
the O
upscaling B-DAT
part O
by O
a O
factor O
of O

is O
not O
used O
in O
the O
upscaling B-DAT
part O

the O
frequency O
of O
the O
progressive O
upscaling B-DAT

of O
times O
to O
employ O
the O
upscaling B-DAT
part, O
it O
is O
beneficial O
to O

BSRN O
model, O
one O
of O
the O
upscaling B-DAT
paths O
(i.e., O
×2, O
×3, O
and O

single-scale O
BSRN O
models O
having O
an O
upscaling B-DAT
factor O
of O
4, O
which O
are O

on O
the O
latter O
part O
(i.e., O
upscaling B-DAT
part) O
to O
generate O
good O
quality O

i.e., O
how O
many O
times O
the O
upscaling B-DAT
part O
is O
employed), O
which O
is O

In O
our O
proposed O
model, O
the O
upscaling B-DAT
part O
spends O
most O
of O
the O

average O
processing O
time O
spent O
on O
upscaling B-DAT
an O
image O
by O
a O
factor O

super-resolved O
image O
with O
the O
given O
upscaling B-DAT
factor O
for O
each O
method O
is O

Deep O
residual O
network O
with O
enhanced O
upscaling B-DAT
module O
for O
super-resolution. O
In: O
Proceedings O

factor O
of O
2 O
on O
the O
Urban100 B-DAT

params O
Set5 O
Set14 O
BSD100 O
Urban100 B-DAT

of O
the O
structures O
in O
the O
Urban100 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
plexities, O
thus O
some O
recursive O
parameter-sharing O

- B-DAT

- B-DAT

- B-DAT
work. O
By O
taking O
advantage O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
field. O
For O
example, O
Dong O

- B-DAT

- B-DAT
volutional O
neural O
network O
(SRCNN) O
model O

- B-DAT
formance O
in O
comparison O
to O
the O

- B-DAT

- B-DAT
nections O
and O
various O
optimization O
techniques O

- B-DAT

- B-DAT

- B-DAT
matically O
increases O
the O
number O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
hinder O
them O
from O
fully O

- B-DAT
vided O
to O
the O
recurrent O
unit O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
block O
state-based O
recursive O
network O

- B-DAT

- B-DAT
arate O
information O
storage O
to O
keep O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
the O
BSRN O
model O
can O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
CNN, O
which O
enhances O
the O
interpolated O

- B-DAT

- B-DAT
lutional O
layers O
to O
improve O
the O

- B-DAT

- B-DAT
tion O
layer O
for O
16 O
times O

- B-DAT

- B-DAT
processing O
part O

- B-DAT

- B-DAT
ple, O
Lai O
et O
al. O
[18 O

- B-DAT

- B-DAT

- B-DAT

C O
- B-DAT

C O
- B-DAT

C O
- B-DAT

C O
- B-DAT

- B-DAT

- B-DAT

- B-DAT
putational O
complexity O
than O
upscaling O
at O

- B-DAT
ploying O
multiple O
residual O
connections O
is O

- B-DAT
ages O
[15,24]. O
Third, O
obtaining O
multiple O

- B-DAT
resolution O
model O
and O
combining O
them O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vided O
into O
three O
parts: O
initial O

- B-DAT
sive O
manner, O
and O
upscaling. O
Fig O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ther O
processed O
via O
a O
recursive O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nates O
two O
input O
matrices O
along O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mance: O
increasing O
the O
number O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put, O
where O
the O
later O
outputs O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
by-pixel O
L1 O
loss, O
i.e O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els. O
The O
single-scale O
models O
are O

- B-DAT

- B-DAT

- B-DAT
ods O
across O
different O
scales. O
The O

- B-DAT

- B-DAT
domly O
cropped O
from O
the O
training O

- B-DAT

- B-DAT
scale O
BSRN O
model. O
For O
data O

- B-DAT

- B-DAT
resolved O
images O
are O
obtained O
from O

- B-DAT
eters. O
To O
prevent O
the O
vanishing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bers O
of O
the O
convolutional O
channels O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
termediate O
features O
are O
largely O
different O

- B-DAT
cally O
change, O
even O
though O
the O

- B-DAT

- B-DAT
gressively O
improved O
features O
and O
highly O

- B-DAT
ploying O
the O
block O
state O
(Fig O

- B-DAT

- B-DAT
space O
operation O
and O
increased O
spatial O

- B-DAT

- B-DAT

- B-DAT
ation. O
To O
verify O
this, O
we O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uated O
on O
the O
Set5 O
[5 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sharing O
parts. O
The O
VDSR, O
LapSRN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ber O
of O
model O
parameters O
required O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ters O
small O
enough O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ample, O
our O
method O
successfully O
upscales O

- B-DAT
resolved O
images O

- B-DAT

- B-DAT
plained O
the O
benefits O
and O
efficiency O

- B-DAT
dition, O
comparison O
with O
the O
other O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mawat, O
S., O
Irving, O
G., O
Isard O

- B-DAT

- B-DAT
chine O
learning. O
In: O
Proceedings O
of O

- B-DAT
resolution: O
Dataset O
and O
study. O
In O

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceedings O
of O
the O
British O
Machine O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
via O
dual-state O
recurrent O
networks O

- B-DAT
ence O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
networks. O
In: O
Proceedings O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
puter O
Vision. O
pp. O
416–423 O
(2001 O

- B-DAT

- B-DAT

- B-DAT
ings O
of O
the O
IEEE O
International O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work. O
In: O
Proceedings O
of O
the O

- B-DAT
ing O
13(4), O
600–612 O
(2004 O

- B-DAT

- B-DAT

- B-DAT
computing O
74(17), O
3193–3203 O
(2011 O

- B-DAT

- B-DAT
representations. O
In: O
Proceedings O
of O
the O

- B-DAT

- B-DAT

factor O
of O
2 O
on O
the O
Urban100 B-DAT
dataset O
[11 O

Set14 O
[29], O
BSD100 O
[20], O
and O
Urban100 B-DAT
[11 O

params O
Set5 O
Set14 O
BSD100 O
Urban100 B-DAT
PSNR O
/ O
SSIM O
PSNR O

Set14 O
[29], O
BSD100 O
[20], O
and O
Urban100 B-DAT
[11] O
datasets. O
Red O
and O
blue O

Ours) O
(20.18 O
/ O
0.6594)image024 O
from O
Urban100 B-DAT
(×4 O

Ours) O
(22.17 O
/ O
0.7641)image059 O
from O
Urban100 B-DAT
(×4 O

of O
the O
structures O
in O
the O
Urban100 B-DAT
dataset, O
which O
results O
in O
clearer O

20] O
for O
testing O
with O
three O
upscaling B-DAT
factors: O
×2, O
×3 O
and O
×4 O

training O
images O
for O
all O
three O
upscaling B-DAT
factors: O
×2, O
×3 O
and O
×4 O

model O
for O
all O
these O
three O
upscaling B-DAT
factors O
as O
in O
[21, O
37 O

best O
result O
across O
all O
the O
upscaling B-DAT
factors O
and O
datasets. O
Visual O
results O

image O
super-resolution O
results O
with O
×4 O
upscaling B-DAT

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

Bischof. O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests O

evaluate O
our O
NLRN O
on O
the O
Urban100 B-DAT

outperforms O
all O
the O
competitors O
on O
Urban100 B-DAT

Urban100 B-DAT
15 I-DAT
32.35/0.9220 O
32.97/0.9271 O
31.86/0.9031 O
-/- O
32.68/0.9255 O

Urban100 B-DAT
30 I-DAT
28.75/0.8567 O
29.47/0.8697 O
29.12/0.8674 O
29.10/0.8631 O
29.94/0.8830 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

the O
advantage O
of O
RNN O
architecture O
- B-DAT
the O
correlation O
information O
is O
propagated O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
based O
image O
restoration O
approaches. O
The O

- B-DAT

- B-DAT

- B-DAT
sity O
[28, O
46]. O
Alternatively, O
similar O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
is O
adopted O
to O
save O

- B-DAT
sides O
CNNs, O
RNNs O
have O
also O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
methods O
based O
on O
low-rankness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
vision O
tasks. O
However, O
unlike O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-3 B-DAT
and O
reduce O
it O
by O
half O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
local O
modules, O
we O
implement O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-6 B-DAT
from O
NLRN O
with O
unrolling O
length O

- B-DAT

- B-DAT
of-the-art O
network O
models O
on O
Set12 O

- B-DAT
plexities O
are O
also O
compared O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
layers) O
of O
NLRN. O
The O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

evaluate O
our O
NLRN O
on O
the O
Urban100 B-DAT
dataset O
[20], O
which O
contains O
abundant O

Set14 O
[50], O
BSD100 O
[30] O
and O
Urban100 B-DAT
[20] O
for O
testing O
with O
three O

outperforms O
all O
the O
competitors O
on O
Urban100 B-DAT
and O
yields O
the O
best O
results O

levels O
on O
Set12, O
BSD68 O
and O
Urban100 B-DAT

Urban100 B-DAT
15 O
32.35/0.9220 O
32.97/0.9271 O
31.86/0.9031 O

on O
14 O
images, O
BSD200 O
and O
Urban100 B-DAT

Urban100 B-DAT
30 O
28.75/0.8567 O
29.47/0.8697 O
29.12/0.8674 O
29.10/0.8631 O

datasets O
Set5, O
Set14, O
BSD100 O
and O
Urban100 B-DAT

Urban100 B-DAT
×2 O
29.50/0.8946 O
30.76/0.9140 O
30.75/0.9133 O
30.41/0.910 O

barbara. O
2) O
image O
004 O
in O
Urban100 B-DAT

. O
3) O
image O
019 O
in O
Urban100 B-DAT

. O
4) O
image O
033 O
in O
Urban100 B-DAT

. O
5) O
image O
046 O
in O
Urban100 B-DAT

bottom: O
1) O
image O
005 O
in O
Urban100 B-DAT

. O
2) O
image O
019 O
in O
Urban100 B-DAT

. O
3) O
image O
044 O
in O
Urban100 B-DAT

. O
4) O
image O
062 O
in O
Urban100 B-DAT

. O
5) O
image O
099 O
in O
Urban100 B-DAT

- B-DAT

- B-DAT
age O
super-resolution O
(SISR). O
However, O
existing O

- B-DAT

- B-DAT

- B-DAT
age O
is O
bicubicly O
downsampled O
from O

- B-DAT

- B-DAT
over, O
they O
lack O
scalability O
in O

- B-DAT
blindly O
deal O
with O
multiple O
degradations O

- B-DAT
ity O
stretching O
strategy O
that O
enables O

- B-DAT

- B-DAT
put. O
Consequently, O
the O
super-resolver O
can O

- B-DAT

- B-DAT
duce O
favorable O
results O
on O
multiple O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put. O
As O
a O
classical O
problem O

- B-DAT
lenging O
research O
topic O
in O
the O

- B-DAT

- B-DAT
nel O
k O
and O
a O
latent O

- B-DAT
pling O
operation O
with O
scale O
factor O

- B-DAT
tive O
white O
Gaussian O
noise O
(AWGN O

- B-DAT
gories, O
i.e., O
interpolation-based O
methods, O
model-based O

- B-DAT
timization O
methods O
and O
discriminative O
learning O

- B-DAT

- B-DAT

- B-DAT
linear O
and O
bicubic O
interpolators O
are O

- B-DAT
age O
priors O
(e.g., O
the O
non-local O

- B-DAT

- B-DAT
based O
optimization O
methods O
are O
flexible O

- B-DAT
ative O
high-quality O
HR O
images, O
but O

- B-DAT

- B-DAT
tegration O
of O
convolutional O
neural O
network O

- B-DAT

- B-DAT
ciency O
to O
some O
extent, O
it O

- B-DAT
backs O
of O
model-based O
optimization O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
networks O
[16, O
18, O
21 O

- B-DAT
els O
based O
on O
discriminative O
CNN O

- B-DAT
els O
are O
specialized O
for O
a O

- B-DAT
ously O
when O
the O
assumed O
degradation O

- B-DAT
ever, O
little O
work O
has O
been O

- B-DAT
ing O
questions, O
which O
are O
the O

- B-DAT
thetic O
data O
to O
train O
a O

- B-DAT
ing O
these O
two O
questions O

- B-DAT

- B-DAT
resolution O
network. O
In O
view O
of O

- B-DAT
sionality O
stretching O
strategy O
which O
facilitates O

- B-DAT
edge, O
there O
is O
no O
attempt O

- B-DAT

- B-DAT
binations O
of O
blur O
kernels O
and O

- B-DAT
AWGN), O
we O
can O
select O
the O

- B-DAT
sult. O
It O
turns O
out O
that O

- B-DAT
sults O
on O
real O
LR O
images O

- B-DAT

- B-DAT
tion O
and O
works O
for O
multiple O

- B-DAT

- B-DAT

- B-DAT
resolution O
network O
learned O
from O
synthetic O

- B-DAT
of-the-art O
SISR O
methods O
on O
synthetic O

- B-DAT

- B-DAT

- B-DAT
work O
(SRCNN) O
was O
proposed. O
In O

- B-DAT
resolution O
and O
empirically O
showed O
that O

- B-DAT
ment O
of O
CNN O
super-resolvers. O
To O

- B-DAT
resolution O
(VDSR) O
method O
with O
residual O

- B-DAT

- B-DAT
ically O
demonstrated O
that O
a O
single O

- B-DAT
tiple O
scales O
super-resolution, O
image O
deblocking O

- B-DAT
put, O
which O
not O
only O
suffers O

- B-DAT
rectly O
manipulating O
the O
LR O
input O

- B-DAT
ing O
operation O
at O
the O
end O

- B-DAT

- B-DAT
scale O
the O
LR O
feature O
maps O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
layers O
are O
used O
to O

- B-DAT

- B-DAT

- B-DAT
teresting O
line O
of O
CNN-based O
methods O

- B-DAT
yond O
bicubic O
degradation O
adopt O
a O

- B-DAT

- B-DAT

- B-DAT
ever, O
manually O
selecting O
the O
hyper-parameters O

- B-DAT
able O
to O
learn O
a O
single O

- B-DAT

- B-DAT
ertheless, O
our O
method O
is O
general O

- B-DAT
cussion O
on O
blur O
kernel O
k O

- B-DAT
lar O
choice O
is O
isotropic O
Gaussian O

- B-DAT
cal O
and O
theoretical O
analyses O
have O

- B-DAT
ticated O
image O
priors O
[12]. O
Specifically O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
pre-processing O
step O
tends O
to O

- B-DAT

- B-DAT
mance O
[43]. O
Thus, O
it O
would O

- B-DAT

- B-DAT
pler O
since O
when O
k O
is O

- B-DAT

- B-DAT
tion O
model. O
It O
should O
be O

- B-DAT
lenging O
task O
since O
the O
degradation O

- B-DAT
ample). O
One O
relevant O
work O
is O

- B-DAT

- B-DAT
timization O
method O
and O
thus O
suffers O

- B-DAT
pects. O
First, O
our O
method O
considers O

- B-DAT
dation O
model. O
Second, O
our O
method O

- B-DAT

- B-DAT
essarily O
derived O
under O
the O
traditional O

- B-DAT
nections O
between O
the O
MAP O
principle O

- B-DAT
anism O
of O
CNN. O
Consequently, O
more O

- B-DAT
tecture O
design O
can O
be O
obtained O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fore, O
the O
MAP O
solution O
of O

- B-DAT

- B-DAT
lated O
as O

- B-DAT
dation O
process, O
accurate O
modeling O
of O

- B-DAT
based O
SISR O
methods O
with O
bicubic O

- B-DAT
form O
generic O
image O
super-resolution O
with O

- B-DAT
dicates O
that O
the O
parameters O
of O

- B-DAT
solve O
this O
problem O

- B-DAT
nel O
is O
first O
projected O
onto O

- B-DAT

- B-DAT
ear O
space O
by O
the O
PCA O

- B-DAT
nique. O
After O
that, O
the O
concatenated O

- B-DAT
dation O
maps O
M O
of O
size O

- B-DAT

- B-DAT
ing O
CNN O
possible O
to O
handle O

- B-DAT
ant O
degradations O
by O
considering O
the O

- B-DAT

- B-DAT

- B-DAT
plex O
architectural O
engineering. O
Typically, O
to O

- B-DAT

- B-DAT

- B-DAT
fied O
Linear O
Units O
(ReLU) O
[26 O

- B-DAT

- B-DAT
volutional O
layer O
to O
convert O
multiple O

- B-DAT
tional O
layers O
is O
set O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sign O
due O
to O
the O
following O

- B-DAT
ever, O
such O
blind O
model O
does O

- B-DAT
pected. O
First, O
the O
performance O
deteriorates O

- B-DAT
ing O
the O
blur O
kernel O
to O

- B-DAT
ferent O
HR O
images O
with O
pixel O

- B-DAT
vate O
the O
pixel-wise O
average O
problem O

- B-DAT

- B-DAT
ization O
ability O
and O
performs O
poorly O

- B-DAT

- B-DAT

- B-DAT
ity, O
one O
can O
treat O
the O

- B-DAT
nel O
and O
noise O
level O
as O

- B-DAT
tion O
maps, O
the O
non-blind O
model O

- B-DAT
tween O
data O
fidelity O
term O
and O

- B-DAT

- B-DAT
cally, O
the O
kernel O
width O
ranges O

- B-DAT
ity O
density O
function O
N O
(0,Σ O

- B-DAT
nel O
is O
determined O
by O
rotation O

- B-DAT
tion O
angle O
range O
to O
[0 O

- B-DAT
out O
the O
paper, O
it O
is O

- B-DAT
rect O
downsampler. O
Alternatively, O
we O
can O

- B-DAT
pler O
↓d, O
we O
can O
find O

- B-DAT

- B-DAT

- B-DAT
tor O
3 O
and O
PCA O
eigenvectors O

- B-DAT

- B-DAT
nel O
and O
a O
noise O
level O

- B-DAT
dation O
maps) O
for O
each O
epoch O

- B-DAT

- B-DAT

- B-DAT
tained O
by O
fine-tuning O
SRMD, O
its O

- B-DAT
cal O
GPU. O
The O
training O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
used O
datasets. O
As O
one O
can O

- B-DAT
rable O
results O
with O
VDSR O
at O

- B-DAT
forms O
VDSR O
at O
large O
scale O

- B-DAT
geNet O
dataset O
[26] O
to O
train O

- B-DAT
MDNF O
on O
scale O
factor O
4 O

- B-DAT
form O
other O
competing O
methods. O
The O

- B-DAT
plicit O
prior O
learning O
and O
thus O

- B-DAT
ment. O
This O
also O
can O
explain O

- B-DAT
parison, O
the O
run O
time O
of O

- B-DAT
ods. O
One O
can O
see O
that O

- B-DAT
petitive O
performance O
against O
other O
methods O

- B-DAT
tion O
settings O
are O
given O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferent O
degradations O
on O
Set5 O
are O

- B-DAT
ticular, O
the O
PSNR O
gain O
of O

- B-DAT

- B-DAT
Noise O
PSNR O
(×2/×3/×4)Width O
sampler O
Level O

- B-DAT

- B-DAT
pler. O
The O
visual O
comparison O
is O

- B-DAT
tially O
variant O
blur O
kernels O
and O

- B-DAT
cally O
downsampled O
from O
HR O
images O

- B-DAT
nels O
and O
corrupted O
by O
AWGN O

- B-DAT

- B-DAT
parison O

- B-DAT
sian O
kernels O
in O
training, O
it O

- B-DAT
ing. O
To O
find O
the O
degradation O

- B-DAT
cally, O
the O
kernel O
width O
is O

- B-DAT

- B-DAT
pression O
artifacts, O
Waifu2x O
[49] O
is O

- B-DAT
son. O
For O
image O
“Chip” O
which O

- B-DAT

- B-DAT
satisfying O
artifacts O
but O
also O
produce O

- B-DAT
ure O
9, O
we O
can O
see O

- B-DAT
duce O
over-smoothed O
results, O
whereas O
SRMD O

- B-DAT

- B-DAT
dations O
via O
a O
single O
model O

- B-DAT
based O
SISR O
methods, O
the O
proposed O

- B-DAT

- B-DAT
ically, O
degradation O
maps O
are O
obtained O

- B-DAT
sionality O
stretching O
of O
the O
degradation O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tially O
variant O
degradations. O
Moreover, O
the O

- B-DAT
struct O
visually O
plausible O
HR O
images O

- B-DAT
posed O
super-resolver O
offers O
a O
feasible O

- B-DAT
tical O
CNN-based O
SISR O
applications O

- B-DAT

- B-DAT
ity O
Enhancement O
of O
Surveillance O
Images O

- B-DAT
ration O
for O
providing O
us O
the O

- B-DAT
search O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
shops, O
volume O
3, O
pages O
126–135 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
Conference, O
2012 O

- B-DAT
shift O
priors O
for O
image O
restoration O

- B-DAT
formation O
Processing O
Systems, O
2017 O

- B-DAT
tion O
diffusion O
processes O
for O
effective O

- B-DAT
tion, O
pages O
5261–5269, O
2015 O

- B-DAT

- B-DAT

- B-DAT
pean O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
ference O
on O
Computer O
Vision, O
pages O

- B-DAT
ized O
sparse O
representation O
for O
image O

- B-DAT
actions O
on O
Image O
Processing, O
22(4):1620–1630 O

- B-DAT
resolution. O
In O
IEEE O
International O
Conference O

- B-DAT
resolution O
via O
BM3D O
sparse O
coding O

- B-DAT
resolution O
and O
texture O
synthesis. O
Advances O

- B-DAT
dom O
Fields O
for O
Vision O
and O

- B-DAT

- B-DAT
puter O
Vision, O
pages O
349–356, O
2009 O

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
nets. O
In O
Advances O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
network O
for O
image O
super-resolution O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT
timization. O
In O
International O
Conference O
for O

- B-DAT
sentations, O
2015 O

- B-DAT
resolution. O
In O
IEEE O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
network. O
In O
IEEE O

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT

- B-DAT
tion O
Workshops, O
pages O
136–144, O
July O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT
tional O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT
tional O
neural O
networks. O
In O
Advances O

- B-DAT
tioned O
regression O
models O
for O
non-blind O

- B-DAT
resolution. O
In O
IEEE O
International O
Conference O

- B-DAT
curate O
image O
super O
resolution. O
IEEE O

- B-DAT
putational O
Imaging, O
3(1):110–125, O
2017 O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
preserving O
image O
super-resolution O
via O
contextualized O

- B-DAT
task O
learning. O
IEEE O
Transactions O
on O

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
tional O
Conference O
on O
Computer O
Vision O

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT

- B-DAT
ral O
networks O
for O
matlab. O
In O

- B-DAT

- B-DAT

- B-DAT
tural O
similarity. O
IEEE O
Transactions O
on O

- B-DAT

- B-DAT
resolution: O
A O
benchmark. O
In O
European O

- B-DAT
puter O
Vision, O
pages O
372–386, O
2014 O

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT

- B-DAT
cessing, O
26(12):5895–5907, O
2017 O

- B-DAT

- B-DAT

- B-DAT
gle O
image O
super-resolution O
under O
internet O

- B-DAT
ference O
on O
Multimedia, O
pages O
677–687 O

- B-DAT
yond O
a O
gaussian O
denoiser: O
Residual O

- B-DAT
ing, O
pages O
3142–3155, O
2017 O

- B-DAT

- B-DAT
ence O
on O
Computer O
Vision O
and O

on O
Set5, O
Set14, O
BSD100 O
and O
Urban100, B-DAT
respectively. O
As O
a O
result, O
it O

Set14 O
[54], O
BSD100 O
[33] O
and O
Urban100 B-DAT
[19]. O
The O
best O
two O
results O

Urban100 B-DAT
×3 O
24.46 O
/ O
0.737 O
26.25 O

on O
image O
“Img O
099” O
from O
Urban100 B-DAT

result O
produced O
by O
ENet-PAT O
at O
4x B-DAT
super-resolution O
on O
an O
image O
from O

fully O
convolutional O
network O
architecture O
for O
4x B-DAT
super-resolution O
which O
only O
learns O
the O

an O
image O
from O
ImageNet O
for O
4x B-DAT
super-resolution. O
Despite O
reaching O
state-of-the-art O
results O

trained O
with O
different O
losses O
at O
4x B-DAT
super-resolution O
on O
images O
from O
ImageNet O

different O
combinations O
of O
losses O
at O
4x B-DAT
super O
resolution. O
ENet-E O
yields O
the O

methods O
with O
our O
results O
at O
4x B-DAT
super-resolution O
on O
an O
image O
from O

PSNR O
for O
different O
methods O
at O
4x B-DAT
super-resolution. O
ENet-E O
achieves O
state-of-the-art O
results O

both O
ENet-E O
and O
ENet-PAT O
at O
4x B-DAT
super- O
resolution O
side-by-side, O
and O
were O

an O
image O
from O
BSD100 O
at O
4x B-DAT
super-resolution. O
ENet- O
PAT’s O
result O
looks O

on O
average O
per O
image O
at O
4x B-DAT
super-resolution O

adversarial O
discrimina- O
tive O
network O
at O
4x B-DAT
super-resolution. O
As O
in O
the O
generative O

on O
average O
per O
image O
at O
4x B-DAT
super-resolution O
on O
Set5/Set14, O
though O
EnhanceNet O

the O
result O
of O
ENet-PAT O
at O
4x B-DAT
super-resolution O
with O
the O
current O
state O

super-resolution O
in O
Fig. O
4. O
Although O
4x B-DAT
super-resolution O
is O
a O
greatly O
more O

are O
lost O
completely O
in O
the O
4x B-DAT
downsampled O
image O
are O
more O
accurate O

image O
with O
sharper O
textures O
at O
4x B-DAT
super-resolution O
that O
even O
out- O
performs O

on O
images O
from O
ImageNet O
at O
4x B-DAT
super-resolution. O
Computing O
the O
texture O
matching O

edges O
and O
overly O
smooth O
textures O
(4x B-DAT
super-resolution). O
Furthermore, O
these O
models O
are O

that O
the O
network O
produces O
at O
4x B-DAT
super-resolution. O
While O
ENet-E O
significantly O
sharpens O

downsampled O
input O
2x O
downsampled O
input O
4x B-DAT
downsampled O
input O
IHR O

VDSR O
[7] O
2x O
DRCN O
[8] O
4x B-DAT
ENet-PAT O
IHR O

missing) O
with O
our O
model O
at O
4x B-DAT
super-resolution O
(93.75% O
of O
all O
pixels O

Bruna O
et O
al. O
[2] O
at O
4x B-DAT
super-resolution. O
ENet-PAT O
produces O
images O
with O

different O
methods O
at O
2x O
and O
4x B-DAT
super-resolution. O
Similar O
to O
PSNR, O
ENet-PAT O

IFC O
for O
different O
methods O
at O
4x B-DAT
super-resolution. O
Best O
performance O
shown O
in O

an O
image O
from O
ImageNet O
at O
4x B-DAT
super-resolution. O
While O
producing O
an O
overall O

on O
images O
of O
faces O
at O
4x B-DAT
super O
resolution. O
ENet-PAT O
produces O
artifacts O

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
TOG O

rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

Urban100 B-DAT
23 I-DAT

26.98 O
– O
27.23 O
27.29 O
27.50 O
Urban100 B-DAT
23 I-DAT

31.39 O
31.85 O
31.90 O
31.95 O
28.30 O
Urban100 B-DAT
26 I-DAT

0.8895 O
0.8942 O
0.8960 O
0.8981 O
0.8729 O
Urban100 B-DAT
0 I-DAT

0.7159 O
0.7233 O
0.7251 O
0.7326 O
0.6270 O
Urban100 B-DAT
0 I-DAT

Urban100 B-DAT
2 I-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put. O
Traditionally, O
the O
performance O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
quality. O
As O
a O
result O

- B-DAT
rics O
tend O
to O
produce O
over-smoothed O

- B-DAT
frequency O
textures O
and O
do O
not O

- B-DAT
thesis O
in O
combination O
with O
a O

- B-DAT
accurate O
reproduction O
of O
ground O
truth O

- B-DAT
ing. O
By O
using O
feed-forward O
fully O

- B-DAT
works O
in O
an O
adversarial O
training O

- B-DAT
nificant O
boost O
in O
image O
quality O

- B-DAT
fectiveness O
of O
our O
approach, O
yielding O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cent O
years. O
The O
problem O
is O

- B-DAT

- B-DAT
ferent O
HR O
images O
can O
give O

- B-DAT

- B-DAT

- B-DAT
lem O
becomes O
worse, O
rendering O
SISR O

- B-DAT
lem. O
Despite O
considerable O
progress O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
are O
still O
far O
from O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ploy: O
most O
systems O
minimize O
the O

- B-DAT

- B-DAT
construction O
from O
the O
LR O
observation O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
neural O
network O
architecture, O
we O

- B-DAT
bination O
with O
adversarial O
training O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
perceptual O
metrics O

- B-DAT
zos O
[11] O
are O
based O
on O

- B-DAT

- B-DAT
lahi O
and O
Moeslund O
[37] O
and O

- B-DAT
based O
models O
that O
either O
exploit O

- B-DAT
ent O
scales O
within O
a O
single O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works O
to O
the O
task O
of O

- B-DAT
gration O
to O
learn O
a O
mapping O

- B-DAT

- B-DAT
tion, O
the O
results O
tend O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ject O
recognition O
system O
like O
VGG O

- B-DAT

- B-DAT

- B-DAT
tion O
with O
an O
adversarial O
network O

- B-DAT
sually O
implausible O
artifacts O
without O
the O

- B-DAT

- B-DAT

- B-DAT
ation O
d O
is O
non-injective O
and O

- B-DAT

- B-DAT
ity O
in O
SISR: O
since O
downsampling O

- B-DAT

- B-DAT
fore, O
even O
state-of-the-art O
models O
learn O

- B-DAT
ple O
in O
Fig. O
2, O
where O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lutional O
layers O
enables O
training O
of O

- B-DAT
put O
image O
of O
arbitrary O
size O

- B-DAT

- B-DAT

- B-DAT
ported O
to O
produce O
checkerboard O
artifacts O

- B-DAT
zontal O
bars O
of O
1×2 O
pixels O

- B-DAT
not O
be O
distinguished O
anymore O
since O

- B-DAT

- B-DAT
pling O
of O
the O
feature O
activations O

- B-DAT

- B-DAT
volution O
layer O
after O
all O
upsampling O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puting O
distances O
in O
image O
space O

- B-DAT

- B-DAT

- B-DAT
tion O
of O
the O
popular O
VGG-19 O

- B-DAT
ally O
decrease O
the O
spatial O
dimension O

- B-DAT
tract O
higher-level O
features O
in O
higher O

- B-DAT

- B-DAT

- B-DAT
atively O
by O
matching O
statistics O
extracted O

- B-DAT

- B-DAT
tween O
the O
feature O
activations O
φ(I O

- B-DAT

- B-DAT
ages O
[24, O
53], O
however O
a O

- B-DAT
resolution O
textures O
during O
inference, O
we O

- B-DAT
ture O
loss O
LT O
patch-wise O
during O

- B-DAT
fore O
learns O
to O
produce O
images O

- B-DAT
tures O
as O
the O
high-resolution O
images O

- B-DAT

- B-DAT

- B-DAT
tures. O
Empirically, O
we O
found O
a O

- B-DAT
ation O
and O
the O
overall O
perceptual O

- B-DAT
imize O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
inative O
network O
as O
we O
found O

- B-DAT
tor O
from O
overpowering O
the O
generator O

- B-DAT
ing O
learning O
strategy O
yields O
better O

- B-DAT
vious O
training O
batch O
and O
only O

- B-DAT
ther O
details O
are O
specified O
in O

- B-DAT
ously O
introduced O
loss O
functions. O
After O

- B-DAT
tive O
and O
quantitative O
evaluation O
of O

- B-DAT
tors O
are O
given O
in O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formations, O
the O
network O
is O
given O

- B-DAT
alistic O
textures O
when O
trained O
with O

- B-DAT

- B-DAT
work O
sometimes O
produces O
unpleasing O
high-frequency O

- B-DAT

- B-DAT

- B-DAT
PAT O
produces O
perceptually O
more O
realistic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
struction, O
but O
even O
the O
state-of-the-art O

- B-DAT

- B-DAT
teristics O
as O
previous O
approaches. O
The O

- B-DAT
age O
than O
ENet-E. O
On O
the O

- B-DAT

- B-DAT
alistic O
textures. O
Comparisons O
with O
further O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pose O
to O
use O
the O
performance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nition O
models O
as O
a O
metric O

- B-DAT

- B-DAT
plement O
pixel-based O
benchmarks O
such O
as O

- B-DAT

- B-DAT

- B-DAT
ric O
similar O
to O
ours O
to O

- B-DAT

-50 B-DAT
[6, O
20] O
as O
this O
class O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lenge O
(ILSVRC) O
[44]. O
For O
the O

- B-DAT

- B-DAT
bels. O
The O
original O
images O
are O

-1 B-DAT
and O
top-5 O
errors O
as O
well O

- B-DAT
fications. O
The O
results O
are O
shown O

- B-DAT
ison, O
some O
of O
the O
results O

- B-DAT
formance O
followed O
by O
DRCN O
[26 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tual O
quality O
which O
is O
reflected O

- B-DAT
ject O
recognition O
benchmark O
matches O
human O

- B-DAT
ter O
than O
PSNR O
does. O
The O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mance O
roughly O
coincides O
with O
the O

- B-DAT
age O
quality O
in O
this O
benchmark O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
imize O
the O
Euclidean O
loss, O
we O

- B-DAT

- B-DAT
erated O
by O
ENet-PAT O
which O
have O

- B-DAT
ages O
upsampled O
with O
bicubic O
interpolation O

- B-DAT

- B-DAT

- B-DAT
resolution O
side-by-side, O
and O
were O
asked O

- B-DAT
age O
produced O
by O
ENet-PAT O
91.0 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT
error O
0.506 O
0.477 O
0.454 O
0.449 O

-5 B-DAT
error O
0.266 O
0.242 O
0.224 O
0.214 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
PAT’s O
result O
looks O
more O
natural O

- B-DAT
lutional O
network O
at O
test O
time O

- B-DAT
gence O
rates O
depend O
on O
the O

- B-DAT
tions. O
Although O
not O
optimized O
for O

- B-DAT

- B-DAT
ducing O
state-of-the-art O
results O
by O
both O

- B-DAT
itative O
measures O
by O
training O
with O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tic, O
they O
do O
not O
match O

- B-DAT
wise O
basis. O
Furthermore, O
the O
adversarial O

- B-DAT
eas. O
This O
is O
a O
result O

- B-DAT
work O
and O
apply O
shrinking O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tails O
and O
additional O
comparisons. O
A O

- B-DAT
tation O
of O
ENet-PAT O
can O
be O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
image O
models O
using O
a O

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
ceptual O
similarity O
metrics O
based O
on O

- B-DAT

- B-DAT

- B-DAT
based O
super-resolution. O
IEEE O
CG&A, O
22(2):56–65 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
nets. O
In O
NIPS O

- B-DAT
tional O
neural O
networks O
for O
direct O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
istration. O
CVGIP: O
Graphical O
models O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
sparse O
regression O
and O
natural O

- B-DAT
ceptual O
image O
quality O
assessment O
using O

- B-DAT
cian O
pyramid. O
Electronic O
Imaging, O
2016(16):1–6 O

- B-DAT
jani, O
J. O
Totz, O
Z. O
Wang O

- B-DAT

- B-DAT
age O
super-resolution O
using O
a O
generative O

- B-DAT
resolved O
faces O
for O
improved O
face O

- B-DAT
lance O
video. O
In O
ICB, O
2007 O

- B-DAT
manan, O
P. O
Dollár, O
and O
C O

- B-DAT
mon O
objects O
in O
context. O
In O

- B-DAT
strained O
sparse O
coding O
for O
single O

- B-DAT

- B-DAT
earities O
improve O
neural O
network O
acoustic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
checkerboard/, O
2016 O

- B-DAT

- B-DAT

- B-DAT
sentation O
learning O
with O
deep O
convolutional O

- B-DAT
sarial O
networks. O
In O
ICLR, O
2016 O

- B-DAT
tion O
based O
noise O
removal O
algorithms O

- B-DAT

- B-DAT

- B-DAT
mation O
fidelity O
criterion O
for O
image O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT
structure O
preserving O
image O
super O
resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ture O
networks: O
Feed-forward O
synthesis O
of O

- B-DAT
ized O
images. O
In O
ICML, O
2016 O

- B-DAT

- B-DAT
ment O
photographs. O
In O
ECCV, O
2016 O

- B-DAT
similarities O
for O
single O
frame O
super-resolution O

- B-DAT

- B-DAT
resolution: O
a O
benchmark. O
In O
ECCV O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
as O
sparse O
representation O
of O

- B-DAT
age O
super-resolution O
via O
sparse O
representation O

- B-DAT

- B-DAT
inative O
generative O
networks. O
In O
ECCV O

- B-DAT
age O
super-resolution O
by O
retrieving O
web O

- B-DAT

- B-DAT

- B-DAT
tion. O
In O
ECCV, O
2016 O

- B-DAT
fold. O
In O
ECCV, O
2016 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cally O
similar O
textures O
between O
Iest O

- B-DAT
tween O
faithful O
texture O
generation O
and O

- B-DAT

- B-DAT

-4 B-DAT

- B-DAT

-128 B-DAT

- B-DAT
age O
since O
the O
network O
is O

- B-DAT
pleasant O
results O

- B-DAT
sarial O
network O
used O
for O
the O

- B-DAT
mon O
design O
patterns O
[13] O
and O

- B-DAT
tive O
network O
at O
4x O
super-resolution O

- B-DAT
tions O
[11] O
and O
strided O
convolutions O

- B-DAT
duce O
a O
classification O
label O
between O

- B-DAT
put O
which O
renders O
training O
more O

- B-DAT

- B-DAT
ual O
image O
that O
cancel O
out O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
which O
leads O
to O
loss O

- B-DAT

- B-DAT
EA O
and O
ENet-EAT O
are O
shown O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tor. O
ENet-PAT O
is O
the O
only O

- B-DAT
tails O
and O
it O
is O
visually O

- B-DAT
hanceNet O
is O
even O
faster O
than O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pare O
the O
result O
of O
ENet-PAT O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
manding O
task O
than O
2x O
super-resolution O

- B-DAT
parable O
in O
quality. O
Small O
details O

- B-DAT

- B-DAT
performs O
the O
current O
state O
of O

- B-DAT

- B-DAT
PAT’s O
result O
and O
looks O
very O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ceptual O
quality O
of O
ENet-PAT’s O
results O

- B-DAT

- B-DAT

-4 B-DAT
ENet-PAT-128 O
ENet-PAT-16 O
(default) O
IHR O

- B-DAT

- B-DAT

- B-DAT

-4 B-DAT

- B-DAT

-128 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
jority O
of O
subjects O
in O
our O

- B-DAT

- B-DAT

- B-DAT
PAT O
trained O
on O
MSCOCO O
struggles O

- B-DAT
cally O
looking O
faces O
at O
high O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
PAT-F O
has O
significantly O
better O
performance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT

- B-DAT
mization. O
2015 O

- B-DAT
earities O
improve O
neural O
network O
acoustic O

- B-DAT

- B-DAT

- B-DAT
sentation O
learning O
with O
deep O
convolutional O

- B-DAT
sarial O
networks. O
In O
ICLR, O
2016 O

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT
mation O
fidelity O
criterion O
for O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
inative O
generative O
networks. O
In O
ECCV O

Urban100 B-DAT
23.14 O
25.66 O
23.75 O
23.56 O
22.51 O

26.98 O
– O
27.23 O
27.29 O
27.50 O
Urban100 B-DAT
23.14 O
24.19 O
24.32 O
24.79 O
24.52 O

Set14), O
12ms O
(BSD100) O
and O
59ms O
(Urban100) B-DAT
on O
average O
per O
image O
at O

31.39 O
31.85 O
31.90 O
31.95 O
28.30 O
Urban100 B-DAT
26.88 O
29.11 O
24.19 O
29.54 O
29.50 O

0.8895 O
0.8942 O
0.8960 O
0.8981 O
0.8729 O
Urban100 B-DAT
0.8403 O
0.8706 O
0.8938 O
0.8947 O
0.8946 O

0.7159 O
0.7233 O
0.7251 O
0.7326 O
0.6270 O
Urban100 B-DAT
0.6577 O
0.7096 O
0.7183 O
0.7374 O
0.7221 O

Urban100 B-DAT
2.361 O
3.110 O
3.208 O
3.314 O
2.963 O

of O
the O
art O
for O
large O
upscaling B-DAT
factors O
(×4). O
EDSR O
[16 O

used O
for O
testing. O
Finally, O
the O
Urban100 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ual O
Learning O
and O
Convolutional O
Sparse O

- B-DAT

- B-DAT
Threshold O
Algorithm O
(LISTA). O
We O
extend O

- B-DAT
volutional O
version O
and O
build O
the O

- B-DAT
tional O
sparse O
codings O
of O
input O

- B-DAT

- B-DAT
mark O
datasets O
demonstrate O
the O
effectiveness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
arts, O
e.g., O
DRRN O
(52 O
layers O

- B-DAT
sults O
are O
available O
at O
https://github.com/axzml O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rithms, O
especially O
the O
current O
leading O

- B-DAT

- B-DAT
ods O
[26, O
4, O
13, O
14 O

- B-DAT
lem O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sent O
our O
models. O
RL-CSC O
with O

- B-DAT
petitive O
performance O
with O
MemNet O
[24 O

- B-DAT
ters, O
the O
performance O
of O
RL-CSC O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
methods O
[32]. O
Inspired O
by O

- B-DAT

- B-DAT
ers O
named O
VDSR, O
which O
shows O

- B-DAT

- B-DAT
gradient O
problem O
when O
the O
network O

- B-DAT

- B-DAT
work O
(DRCN) O
[14] O
with O
a O

- B-DAT
nary O
success O
of O
ResNet O
[9 O

- B-DAT

- B-DAT

- B-DAT
cursive O
manner, O
leading O
to O
a O

- B-DAT
decoding O
network O
named O
RED-Net O
was O

- B-DAT
tioned O
models O
usually O
lack O
convincing O

- B-DAT
plored, O
i.e., O
what O
role O
each O

- B-DAT
resentation O
with O
strong O
theoretical O
support O

- B-DAT

- B-DAT

- B-DAT
ment O
when O
the O
number O
of O

- B-DAT
posed O
CSC O
based O
SR O
(CSC-SR O

- B-DAT

- B-DAT
tional O
sparse O
coding O
methods. O
In O

- B-DAT
putationally O
efficient O
CSC O
model, O
Sreter O

- B-DAT
duced O
a O
convolutional O
recurrent O
sparse O

- B-DAT

- B-DAT
tending O
the O
LISTA O
method O
to O

- B-DAT
ing O
tasks O

- B-DAT

- B-DAT

- B-DAT
posed O
in O
the O
field O
of O

- B-DAT
tion, O
our O
model, O
termed O
as O

- B-DAT

- B-DAT
defined O
interpretability O

- B-DAT
CSC O
(30 O
layers) O
has O
achieved O

- B-DAT

- B-DAT
mizes O
the O
objective O
function O
(1 O

- B-DAT
ting O
term O
and O
an O
`1-norm O

- B-DAT

- B-DAT
ization O
coefficient O
λ O
is O
used O

- B-DAT

- B-DAT
ative O
Shrinkage O
Thresholding O
Algorithm O
(ISTA O

- B-DAT

- B-DAT

- B-DAT
dress O
this O
issue, O
Gregor O
and O

- B-DAT
gorithm O
termed O
as O
Learned O
ISTA O

- B-DAT
proximate O
estimates O
of O
sparse O
code O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ters O
We, O
G O
and O
θ O

- B-DAT

- B-DAT
vide O
the O
whole O
image O
into O

- B-DAT
actly O
the O
same O
, O
is O

- B-DAT
able O
for O
this O
issue, O
as O

- B-DAT
ternating O
Direction O
Method O
of O
Multipliers O

- B-DAT

- B-DAT

- B-DAT
ory O
burden O
issue O
of O
ADMM O

- B-DAT

- B-DAT
put O
Interpolated O
LR O
(ILR) O
image O

- B-DAT
gency O
speed O
and O
the O
reconstruction O

- B-DAT
lated O
Low-Resolution O
(ILR) O
image O
Iy O

- B-DAT
dicts O
the O
output O
HR O
image O

- B-DAT
verting O
one O
of O
the O
inputs O

- B-DAT
tive O
tool O
to O
learn O
the O

- B-DAT

- B-DAT
portant O
facts: O
(1) O
the O
expressiveness O

- B-DAT

- B-DAT

- B-DAT
frequency O
information O
reconstruction O

- B-DAT
structed O
by O
the O
addition O
of O

- B-DAT

- B-DAT
tion, O
W1 O
and O
S O
for O

- B-DAT
ery O
recursion. O
When O
K O
recursions O

- B-DAT
ing O
process, O
the O
depth O
d O

- B-DAT
ploited O
in O
our O
training O
process O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dient O
descent O
(SGD) O
is O
used O

- B-DAT
ment O
our O
model O
using O
the O

- B-DAT

- B-DAT
fied O
structures O
of O
these O
models O

- B-DAT

- B-DAT

- B-DAT
put) O
and O
a O
pre-activation O
structure O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eter O
settings O
for O
better O
performance O

- B-DAT

- B-DAT

- B-DAT
tween O
SCN O
[29] O
and O
RL-CSC O

- B-DAT

- B-DAT
ing O
linear O
layers, O
so O
more O

- B-DAT

- B-DAT
works. O
With O
the O
help O
of O

- B-DAT

- B-DAT
fers O
with O
DRCN O
[14] O
in O

- B-DAT
ual O
Learning O
[23] O
(LRL) O
and O

- B-DAT

- B-DAT
sides, O
DRCN O
is O
not O
easy O

- B-DAT

- B-DAT

- B-DAT
ate O
predictions) O
is O
used O
to O

- B-DAT

- B-DAT

- B-DAT
troduction O
to O
the O
datasets O
used O

- B-DAT
isons O
with O
state-of-the-arts O
are O
presented O

- B-DAT

- B-DAT
tion O
Dataset O
[18]. O
During O
testing O

- B-DAT
mark. O
Moreover, O
the O
BSD100 O
[18 O

- B-DAT
ural O
images O
are O
used O
for O

- B-DAT

- B-DAT

- B-DAT
minance) O
of O
transformed O
YCbCr O
space O

- B-DAT
cludes O
flipping O
(horizontally O
and O
vertically O

- B-DAT
formed O
on O
each O
image O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tioned O
into O
33× O
33 O
patches O

- B-DAT

- B-DAT
CSC O
is O
30 O
according O
to O

- B-DAT
mized O
using O
SGD O
with O
mini-batch O

- B-DAT

- B-DAT

- B-DAT
ther O
improvements O
of O
the O
loss O

- B-DAT
dient O
clipping O
strategy O
stated O
in O

- B-DAT
ter. O
A O
NVIDIA O
Titan O
Xp O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
bic O
interpolation O
to O
the O
original O

- B-DAT
els O
near O
borders O
before O
evaluation O

- B-DAT
mark O
testing O
sets, O
and O
results O

- B-DAT

- B-DAT
lowing O
[23], O
BSD100 O
is O
not O

- B-DAT

- B-DAT

- B-DAT
formance O
(K O
= O
15 O
33.98dB O

- B-DAT
ter. O
Similar O
conclusions O
are O
observed O

- B-DAT
tempt O
to O
combine O
the O
powerful O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ficulties, O
but O
helps O
network O
converge O

- B-DAT

- B-DAT
tings O
as O
stated O
in O
Section O

- B-DAT
ters. O
Specifically, O
two O
types O
of O

- B-DAT
plied O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
terpart. O
Tests O
on O
Set5 O
with O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
datas O
are O
derived O
from O

- B-DAT

- B-DAT
fault O
settings O
given O
by O
the O

- B-DAT

- B-DAT
Memory O
(OOM) O
issue. O
The O
reason O

- B-DAT

- B-DAT

- B-DAT
terpretability. O
We O
extend O
the O
LISTA O

- B-DAT
volutional O
version O
and O
build O
the O

- B-DAT
sions O
without O
introducing O
any O
new O

- B-DAT
sults O
with O
state-of-the-arts O
and O
demonstrate O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
deep O
convolutional O
networks O

- B-DAT

- B-DAT
nary O
Learning: O
A O
Comparative O
Review O

- B-DAT

Delving O
Deep O
into O
Rec- B-DAT
tifiers O
- O
Surpassing O
Human-Level O
Performance O
on O
ImageNet O

- B-DAT
ing O
for O
Image O
Recognition. O
In O

- B-DAT
ble O
convolutional O
sparse O
coding. O
CVPR O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
CVPR O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT
lutional O
Network O
for O
Image O
Super-Resolution O

- B-DAT
ham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT

- B-DAT
ing O
a O
Generative O
Adversarial O
Network O

- B-DAT

- B-DAT
ing O
very O
deep O
convolutional O
encoder-decoder O

- B-DAT
cal O
statistics. O
volume O
2, O
pages O

- B-DAT
Vito, O
Z. O
Lin, O
A. O
Desmaison O

- B-DAT
matic O
differentiation O
in O
pytorch. O
In O

- B-DAT

- B-DAT

- B-DAT
ing. O
In O
ICASSP, O
pages O
2191–2195 O

- B-DAT

- B-DAT
tent O
memory O
network O
for O
image O

- B-DAT

- B-DAT

Challenge O
on O
Single O
Image O
Super-Resolution B-DAT
- O
Methods O
and O
Results. O
CVPR O
Workshops O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution: O
A O
benchmark. O
In O
ECCV O

- B-DAT
age O
super-resolution O
via O
sparse O
representation O

- B-DAT

- B-DAT
convolutional O
networks. O
CVPR, O
2010. O
2 O

- B-DAT

- B-DAT

A O
Survey O
of O
Sparse O
Representation O
- B-DAT
Algorithms O
and O
Applications. O
IEEE O
Access O

used O
for O
testing. O
Finally, O
the O
Urban100 B-DAT
of O
100 O
urban O
images O
introduced O

Urban100 B-DAT
×2 O
26.88/0.8403 O
29.50/0.8946 O
30.76/0.9140 O
30.75/0.9133 O

datasets O
Set5, O
Set14, O
BSD100 O
and O
Urban100 B-DAT

Urban100 B-DAT
×2 O
6.245 O
7.989 O
7.937 O
8.645 O

on O
datasets O
Set5, O
Set14 O
and O
Urban100 B-DAT

shows O
image O
“img O
002” O
from O
Urban100 B-DAT

- B-DAT
age O
restoration. O
However, O
as O
the O

- B-DAT

- B-DAT
tle O
influence O
on O
the O
subsequent O

- B-DAT
tive O
learning O
process. O
The O
recursive O

- B-DAT

- B-DAT
tive O
fields. O
The O
representations O
and O

- B-DAT
vious O
states O
should O
be O
reserved O

- B-DAT
resolution O
and O
JPEG O
deblocking. O
Comprehensive O

- B-DAT
iments O
demonstrate O
the O
necessity O
of O

- B-DAT

- B-DAT

- B-DAT
quality O
version O
of O
x, O
D O

- B-DAT
der O
Grant O
Nos. O
91420201, O
61472187 O

- B-DAT

- B-DAT
search O
Fund. O
Jian O
Yang O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
noising O
aims O
to O
recover O
a O

- B-DAT
servation, O
which O
commonly O
assumes O
additive O

- B-DAT
sian O
noise O
with O
a O
standard O

- B-DAT

- B-DAT
resolution O
recovers O
a O
high-resolution O
(HR O

- B-DAT

- B-DAT

- B-DAT
trol O
the O
parameter O
number O
of O

- B-DAT
Recursive O
Convolutional O
Network O
(DRCN) O
[21 O

- B-DAT
gate O
training O
difficulty, O
Mao O
et O

- B-DAT

- B-DAT

- B-DAT
over, O
Zhang O
et O
al. O
[40 O

- B-DAT
path O
feed-forward O
architecture, O
where O
one O

- B-DAT
fluenced O
by O
its O
direct O
former O

- B-DAT

- B-DAT
ory. O
Some O
variants O
of O
CNNs O

- B-DAT

- B-DAT
cific O
prior O
state, O
namely O
restricted O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tent O
memory O
network O
(MemNet), O
which O

- B-DAT
ory O
block O
to O
explicitly O
mine O

- B-DAT
tion O
Net O
(FENet) O
first O
extracts O

- B-DAT

- B-DAT
tains O
a O
recursive O
unit O
and O

- B-DAT
science O
[6, O
25] O
that O
recursive O

- B-DAT
ist O
in O
the O
neocortex, O
the O

- B-DAT

- B-DAT
tive O
fields O
(blue O
circles O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ated O
from O
the O
previous O
memory O

- B-DAT

- B-DAT
ther, O
we O
present O
an O
extended O

- B-DAT

- B-DAT

- B-DAT
term O
memory O
should O
be O
reserved O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
even O
using O
a O
single O

- B-DAT
veal O
that O
the O
network O
depth O

- B-DAT
tion O
and O
achieved O
comparable O
performance O

- B-DAT
resolution O
convolutional O
neural O
network O
(SRCNN O

- B-DAT
dicts O
the O
nonlinear O
LR-HR O
mapping O

- B-DAT
volutional O
network, O
which O
significantly O
outperforms O

- B-DAT
cal O
shallow O
methods. O
The O
authors O

- B-DAT
tended O
CNN O
model, O
named O
Artifacts O

- B-DAT
tional O
Neural O
Networks O
(ARCNN) O
[7 O

- B-DAT

- B-DAT
ural O
sparsity O
of O
images O
[36 O

- B-DAT

- B-DAT
edge O
in O
the O
JPEG O
compression O

- B-DAT

- B-DAT

- B-DAT
ers O
to O
exploit O
large O
contextual O

- B-DAT
ing O
and O
adjustable O
gradient O
clipping O

- B-DAT
ization O
into O
a O
DnCNN O
model O

- B-DAT
age O
restoration O
tasks. O
To O
reduce O

- B-DAT

- B-DAT
connection O
to O
mitigate O
the O
training O

- B-DAT

- B-DAT
posed O
LapSRN O
to O
address O
the O

- B-DAT
curacy O
for O
SISR, O
which O
operates O

- B-DAT

- B-DAT
ages. O
Tai O
et O
al. O
[34 O

- B-DAT
work O
(DRRN) O
to O
address O
the O

- B-DAT

- B-DAT
lutional O
layer O
is O
used O
in O

- B-DAT
ture O
mapping, O
we O
have O

- B-DAT

- B-DAT

- B-DAT
ory O
block O
respectively. O
Finally, O
instead O

- B-DAT

- B-DAT

- B-DAT
age, O
our O
model O
uses O
a O

- B-DAT
notes O
the O
function O
of O
our O

- B-DAT
ber O
of O
training O
patches O
and O

- B-DAT
quality O
patch O
of O
the O
low-quality O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
block. O
Specifically, O
each O
residual O

- B-DAT

- B-DAT

- B-DAT
erate O
multi-level O
representations O
under O
different O

- B-DAT

- B-DAT
ory. O
Supposing O
there O
are O
R O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cursive O
unit. O
These O
representations O
are O

- B-DAT

- B-DAT

- B-DAT
vious O
memory O
blocks O
can O
be O

- B-DAT

- B-DAT
volutional O
layer O
(parameterized O
by O
W O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put O
from O
the O
ensemble O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
can O
get O
lost O
at O

- B-DAT
ward O
CNN O
process, O
and O
dense O

- B-DAT
ous O
layers O
can O
compensate O
such O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferent O
networks. O
(b) O
We O
convert O

- B-DAT

- B-DAT

- B-DAT
tral O
densities O
by O
integrating O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions, O
the O
latter O
layer O
absorbs O

- B-DAT

- B-DAT

- B-DAT
work O
– O
a O
very O
deep O

- B-DAT
works, O
inspired O
by O
LSTM, O
Highway O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tween O
MemNet O
and O
DRCN O
[21 O

- B-DAT
ule O
is O
a O
memory O
block O

- B-DAT
ules O
in O
DRCN, O
which O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
egy, O
which O
is O
imperative O
for O

- B-DAT
nected O
principle. O
In O
general, O
DenseNet O

- B-DAT
tion. O
In O
addition, O
DenseNet O
adopts O

- B-DAT

- B-DAT
tions O
in O
MemNet O
indeed O
play O

- B-DAT

- B-DAT

- B-DAT
nections. O
Average O
PSNR/SSIMs O
for O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ories O
from O
the O
last O
recursion O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tors O
are O
evaluated, O
including O
×2 O

- B-DAT
noising O
is O
used. O
As O
in O

- B-DAT
ing O
time O
and O
storage O
complexities O

- B-DAT

- B-DAT

- B-DAT
ent O
noise O
levels O
are O
all O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sions, O
are O
constructed O
(i.e., O
M6R6 O

- B-DAT
supervised O
MemNet, O
6 O
predictions O
are O

- B-DAT
tions, O
and O
is O
empirically O
set O

- B-DAT
mized O
via O
the O
mini-batch O
stochastic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nections. O
The O
reason O
is O
that O

- B-DAT

- B-DAT

- B-DAT
responding O
weights O
from O
all O
filters O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
we O
normalize O
the O
norms O

- B-DAT
ture O
map O
index O
l. O
We O

- B-DAT
ory O
block O
number O
increases. O
(3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity O
of O
our O
persistent O
memory O

- B-DAT
ent O
among O
different O
work, O
we O

- B-DAT
crease O
the O
parameters O
(filter O
number O

- B-DAT

- B-DAT
mance. O
With O
more O
training O
images O

- B-DAT
nificantly O
outperforms O
the O
state O
of O

- B-DAT

- B-DAT
lem O
in O
networks, O
we O
intend O

- B-DAT
plexity O
and O
accuracy. O
Fig. O
6 O

- B-DAT
notes O
the O
prediction O
of O
the O

- B-DAT
sult O
at O
the O
3rd O
prediction O

- B-DAT
creasing O
model O
complexity O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nal O
image O
is O
resized O
to O

- B-DAT
mance. O
However, O
in O
our O
MemNet O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rectly O
recovers O
the O
pillar. O
Please O

- B-DAT
isons O
for O
SISR. O
SRCNN O
[8 O

- B-DAT
sults O
on O
Classic5 O
and O
LIVE1 O

- B-DAT
erated O
by O
their O
corresponding O
public O

- B-DAT
work O
structures: O
M4R6, O
M6R6, O
M6R8 O

- B-DAT
posed O
deepest O
network O
M10R10 O
achieves O

- B-DAT

- B-DAT

- B-DAT
ory O
network O
(MemNet) O
is O
proposed O

- B-DAT

- B-DAT
ous O
CNN O
architectures. O
In O
each O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ous O
memory O
blocks O
are O
sent O

- B-DAT
resolution O
and O
JPEG O
deblocking O
simultaneously O

- B-DAT
hensive O
benchmark O
evaluations O
well O
demonstrate O

- B-DAT
riority O
of O
our O
MemNet O
over O

- B-DAT

- B-DAT

- B-DAT
ative O
neighbor O
embedding. O
In O
BMVC O

- B-DAT

- B-DAT
age O
denoising O
by O
sparse O
3-D O

- B-DAT

- B-DAT
bridge, O
MA: O
MIT O
Press, O
2001 O

- B-DAT
tifacts O
reduction O
by O
a O
deep O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
volutional O
networks. O
In O
NIPS, O
2008 O

- B-DAT

- B-DAT
ing O
of O
non-parametric O
image O
restoration O

- B-DAT
shick, O
S. O
Guadarrama, O
and O
T O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT
tional O
network O
for O
image O
super-resolution O

- B-DAT

- B-DAT
based O
learning O
applied O
to O
document O

- B-DAT
ings O
of O
the O
IEEE, O
1998 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ric O
skip O
connections. O
In O
NIPS O

- B-DAT
cal O
statistics. O
In O
ICCV, O
2001 O

- B-DAT
stricted O
boltzmann O
machines. O
In O
ICML O

- B-DAT

- B-DAT

- B-DAT
compressed O
images. O
In O
CVPR, O
2016 O

- B-DAT

- B-DAT

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT
up O
using O
sparse-representations. O
Curves O
and O

- B-DAT
yond O
a O
gaussian O
denoiser: O
Residual O

BSD100 O
[28] O
and O
Urban100 B-DAT
[15] O
are O
used. O
Three O
scale O

Urban100 B-DAT
×2 O
26.88/0.8403 O
29.50/0.8946 O
30.76/0.9140 O
30.75/0.9133 O

datasets O
Set5, O
Set14, O
BSD100 O
and O
Urban100 B-DAT

shows O
image O
“img O
002” O
from O
Urban100 B-DAT
with O
scale O
factor O
×4. O
MemNet O

branch O
to O
reconstruct O
the O
HR O
4x B-DAT
image O

high-resolution O
(HR) O
images. O
When O
the O
upscaling B-DAT
factor O
is O
large O

extra O
computation. O
Moreover, O
for O
large O
upscaling B-DAT
factors, O
our O

high O
quality O
images O
for O
higher O
upscaling B-DAT
factors O

model O
could O
do O
multi O
scale O
upscaling B-DAT
task O
via O
Laplacian O
Pyramid O

upscaling B-DAT
factors O
and O
decreasing O
parameters O
by O

We O
first O
upscaling B-DAT
our O
low-resolution O
image O
via O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-the-art O
methods O
in O
scale O
x4 O

-5 B-DAT

- B-DAT

- B-DAT

-11 B-DAT

- B-DAT
learning-based O
super O
resolution O
methods O
have O

- B-DAT

- B-DAT

- B-DAT
resolution O
simultaneously O
in O
one O
feed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
quality O
high-resolution O
image, O
the O
process O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-2798 B-DAT

- B-DAT

-8 B-DAT

- B-DAT

- B-DAT

-2873 B-DAT

- B-DAT

- B-DAT

-3478 B-DAT

- B-DAT

- B-DAT

-126 B-DAT

- B-DAT
based O
single O
image O
super O
resolu-tion[C]//Proceedings O

- B-DAT
1873 O

- B-DAT

- B-DAT

-5206 B-DAT

- B-DAT

-1105 B-DAT

-551 B-DAT

- B-DAT

-2324 B-DAT

- B-DAT

-307 B-DAT

- B-DAT

- B-DAT

-778 B-DAT

- B-DAT

- B-DAT

- B-DAT
1654 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1645 B-DAT

- B-DAT

- B-DAT

-4547 B-DAT

- B-DAT

-4817 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-711 B-DAT

- B-DAT

- B-DAT

- B-DAT
2423 O

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adver-sarial O

- B-DAT

- B-DAT

-4500 B-DAT

-612 B-DAT

-456 B-DAT

- B-DAT

- B-DAT

- B-DAT
image O
super-resolution O
based O
on O
nonnegative O

- B-DAT

- B-DAT
representations[C]//International O
conference O
on O
curves O
and O

-730 B-DAT

-423 B-DAT

- B-DAT

-407 B-DAT

urban O
landscape O
named O
Urban100 B-DAT
[8]. O
All O
the O
RGB O
images O

Set5 O
Set14 O
Bsd100 O
Urban100 B-DAT

the O
average O
inference O
time O
for O
upscaling B-DAT
3× O
on O
Set5. O
The O
IDN O

restoration O
per- O
formance O
with O
larger O
upscaling B-DAT
factors, O
the O
recent O
SR O
meth O

the O
original O
HR O
images O
with O
upscaling B-DAT
factor O
m O
(m O
= O
2 O

the O
Set14 O
dataset O
with O
an O
upscaling B-DAT
factor O
4 O

the O
BSD100 O
dataset O
with O
an O
upscaling B-DAT
factor O
4 O

the O
Urban100 O
dataset O
with O
an O
upscaling B-DAT
factor O
4 O

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

consist O
of O
natural O
scenes O
and O
Urban100 B-DAT

structure O
on O
image O
“img085” O
of O
Urban100 B-DAT

than O
that O
of O
MemNet O
in O
Urban100 B-DAT

The O
“img085” O
image O
from O
the O
Urban100 B-DAT

im- O
ages O
in O
BSD100 O
and O
Urban100 B-DAT

- B-DAT

- B-DAT
age O
super-resolution. O
However, O
as O
the O

- B-DAT

- B-DAT

- B-DAT
ods O
have O
been O
faced O
with O

- B-DAT
pact O
convolutional O
network O
to O
directly O

- B-DAT
tion O
block, O
the O
local O
long O

- B-DAT

- B-DAT
fectively O
extracted. O
Specifically, O
the O
proposed O

- B-DAT
quential O
blocks. O
In O
addition, O
the O

- B-DAT
tion. O
Experimental O
results O
demonstrate O
that O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cially O
in O
terms O
of O
time O

- B-DAT

- B-DAT

- B-DAT
lem O
in O
low-level O
computer O
vision O

- B-DAT

- B-DAT

- B-DAT
age. O
Actually, O
an O
infinite O
number O

- B-DAT

- B-DAT
ists. O
In O
order O
to O
mitigate O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formance O
with O
larger O
upscaling O
factors O

- B-DAT
ods O
fall O
into O
the O
example-based O

- B-DAT
ral O
network O
(CNN), O
many O
CNN-based O

- B-DAT
mance. O
Kim O
et O
al. O
propose O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ment O
of O
enormous O
parameters O
of O

- B-DAT
der O
to O
achieve O
better O
performance O

- B-DAT
ory O
consumption, O
which O
are O
less O

- B-DAT
caded O
network O
topologies, O
e.g., O
VDSR O

- B-DAT
brating O
channel-wise O
features O
responses O
can O

- B-DAT
mation O
distillation O
network O
(IDN) O
with O

- B-DAT
ters O
and O
computational O
complexity O
as O

- B-DAT
formation O
distillation O
blocks O
(DBlocks) O
are O

- B-DAT
gressively O
distill O
residual O
information. O
Finally O

- B-DAT
tion O
Block O
(RBlock) O
aggregates O
the O

- B-DAT

- B-DAT
tion O
block, O
which O
contains O
an O

- B-DAT
pression O
unit. O
The O
enhancement O
unit O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pressive O
power, O
we O
send O
a O

- B-DAT

- B-DAT

- B-DAT
ture O
maps O
into O
two O
parts O

- B-DAT
path O
features O
and O
another O
expresses O

- B-DAT

- B-DAT

- B-DAT
mary, O
the O
enhancement O
unit O
is O

- B-DAT
sentation O
power O
of O
the O
network O

- B-DAT
ber O
of O
convolutional O
layer O

- B-DAT

- B-DAT

- B-DAT
tains O
better O
reconstruction O
accuracy O

- B-DAT

- B-DAT
ied O
in O
these O
years. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
similarity O
property O
and O
extract O
example O

- B-DAT
terns O
and O
textures O
but O
lacks O

- B-DAT
tory O
prediction O
for O
images O
of O

- B-DAT

- B-DAT
spective O
deformation O

- B-DAT

- B-DAT

- B-DAT
pact O
dictionary O
or O
manifold O
space O

- B-DAT
dom O
forest O
[20] O
and O
sparse O

- B-DAT
mal O
for O
generating O
high-quality O
SR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
to O
accelerate O
SRCNN O
in O

- B-DAT
signed O
by O
Kim O
et O
al O

- B-DAT
tion O
with O
skip O
connection O
to O

- B-DAT
decoder O
networks O
and O
symmetric O
skip O

- B-DAT

- B-DAT
work O
(LapSRN) O
to O
address O
the O

- B-DAT

- B-DAT
ages. O
Tai O
et O
al. O
[22 O

- B-DAT
work O
to O
effectively O
build O
a O

- B-DAT
racy. O
The O
authors O
also O
present O

- B-DAT

- B-DAT

- B-DAT
sistent O
memory O
network O
(MemNet) O
[23 O

- B-DAT
tion O
task, O
which O
tackles O
the O

- B-DAT

- B-DAT
pose O
a O
novel O
combination O
of O

- B-DAT
construction O
block O
(RBlock). O
Here, O
we O

- B-DAT
formation O
distillation O
blocks O
by O
using O

- B-DAT

- B-DAT

- B-DAT
tively. O
Finally, O
we O
take O
a O

- B-DAT

- B-DAT
age O
restoration O
as O
defined O
below O

- B-DAT
mulated O
as O
follows O

- B-DAT

- B-DAT
lutions O
and O
another O
is O
the O

- B-DAT

- B-DAT
lows O

- B-DAT
while O
is O
the O
input O
of O

- B-DAT

- B-DAT
eration O
respectively. O
Specifically, O
we O
know O

- B-DAT

- B-DAT
path O
information O
as O
the O
input O

- B-DAT

- B-DAT
ations O
of O
the O
below O
module O

- B-DAT
path O
information O
and O
the O
local O

- B-DAT

- B-DAT
mulated O
as O

- B-DAT

- B-DAT

- B-DAT
lized O
without O
exception O
by O
a O

- B-DAT
tage O
of O
a O
1× O
1 O

- B-DAT
tation O
Dataset O
(BSD) O
[18] O
as O

- B-DAT
tation O
in O
three O
ways: O
(1 O

- B-DAT
channel, O
while O
color O
components O
are O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferent O
scaling O
factors O

- B-DAT
sample O
the O
original O
HR O
images O

- B-DAT
erate O
the O
corresponding O
LR O
images O

- B-DAT

- B-DAT

- B-DAT
ters O
will O
generate O
the O
output O

- B-DAT

- B-DAT
mum O
size O
of O
the O
sub-image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ble O
1 O

- B-DAT

- B-DAT

- B-DAT
volution O
layer O
[6, O
24] O
in O

- B-DAT

- B-DAT

- B-DAT
ters O
as O
the O
initial O
values O

- B-DAT
terfly” O
image O
from O
Set5 O
dataset O

- B-DAT
ture O
information O
and O
its O
normalized O

pixel O
value O
ranges O
from O
-0 B-DAT

- B-DAT
ter O
visualizing O
the O
intermediary O
of O

- B-DAT
age O
feature O
map O
can O
roughly O

- B-DAT
ment O
unit O
and O
compression O
unit O

- B-DAT
ing O
above-mentioned O
method. O
As O
illustrated O

- B-DAT
figures O
show O
that O
the O
later O

- B-DAT
creasing O
the O
pixel O
values O
to O

- B-DAT
atively O
clear O
contour O
profile. O
In O

- B-DAT
ure O
obviously O
surpasses O
the O
former O

- B-DAT
paring O
Figure O
5(a) O
with O
Figure O

- B-DAT
ure O
5(b) O
and O
the O
third O

- B-DAT
age. O
The O
bias O
term O
of O

- B-DAT
matically O
adjust O
the O
central O
value O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods, O
including O
bicubic, O
SRCNN O
[3 O

- B-DAT

- B-DAT

- B-DAT
vorably O
against O
state-of-the-art O
results O
on O

- B-DAT
man O
perception O
of O
image O
super-resolution O

- B-DAT
bara” O
image O
has O
serious O
artifacts O

- B-DAT
viously O
see O
that O
the O
proposed O

- B-DAT
tively O
clear O
in O
the O
proposed O

- B-DAT
put O
so O
that O
more O
information O

- B-DAT
age. O
The O
algorithms O
that O
take O

- B-DAT
chine O
with O
4.2GHz O
Intel O
i7 O

- B-DAT
ages O
in O
BSD100 O
and O
Urban100 O

- B-DAT
ages O
into O
several O
parts O
and O

- B-DAT
worthy O
that O
the O
proposed O
IDN O

- B-DAT
ban100 O
dataset O

- B-DAT
cient O
features O
for O
the O
reconstruction O

- B-DAT
posed O
approach O
achieves O
competitive O
results O

- B-DAT
mark O
datasets O
in O
terms O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pact O
network O
will O
be O
more O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
rable O
convolutions. O
In O
CVPR, O
pages O

- B-DAT
based O
super-resolution. O
IEEE O
Computer O
Graphics O

- B-DAT
plications, O
22(2):56–65, O
2002. O
2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works. O
In O
arXiv:1709.01507, O
2017. O
2 O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
ishick, O
S. O
Guadarrama, O
and O
T O

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT
tional O
network O
for O
image O
super-resolution O

- B-DAT
resolution. O
In O
CVPR, O
pages O
624–632 O

- B-DAT

- B-DAT
ing O
very O
deep O
convolutional O
encoder-decoder O

- B-DAT
cal O
statistics. O
In O
CVPR, O
pages O

- B-DAT

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT

- B-DAT
tent O
memory O
network O
for O
image O

- B-DAT
resolution O
as O
sparse O
representation O
of O

- B-DAT
resolution O
via O
sparse O
representation. O
IEEE O

- B-DAT

- B-DAT

1], O
Set14 O
[27], O
BSD100 O
[18], O
Urban100 B-DAT
[10]. O
Among O
these O
datasets, O
Set5 O

consist O
of O
natural O
scenes O
and O
Urban100 B-DAT
contains O
challenging O
urban O
scenes O
images O

Urban100 B-DAT
×2 O
26.88/0.8403 O
30.76/0.9140 O
30.75/0.9133 O
30.41/0.9103 O

Urban100 B-DAT
×2 O
6.245 O
8.629 O
8.959 O
8.907 O

Urban100 B-DAT
×2 O
0.451 O
5.010 O
0.082 O
26.699 O

structure O
on O
image O
“img085” O
of O
Urban100 B-DAT
dataset O
is O
rela- O
tively O
clear O

than O
that O
of O
MemNet O
in O
Urban100 B-DAT
dataset O
and O
3 O

The O
“img085” O
image O
from O
the O
Urban100 B-DAT
dataset O
with O
an O
upscaling O
factor O

im- O
ages O
in O
BSD100 O
and O
Urban100 B-DAT
datasets, O
we O
divide O
100 O
im O

R. O
Fattal. O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
TOG O

Fast O
and O
accu- O
rate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
strated O
high-quality O
reconstruction O
for O
single-image O

- B-DAT
resolution. O
In O
this O
paper, O
we O

- B-DAT

- B-DAT
construct O
the O
sub-band O
residuals O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
als, O
and O
uses O
transposed O
convolutions O

- B-DAT
lation O
as O
the O
pre-processing O
step O

- B-DAT
duces O
the O
computational O
complexity. O
We O

- B-DAT

- B-DAT
thermore, O
our O
network O
generates O
multi-scale O

- B-DAT

- B-DAT
tion, O
thereby O
facilitates O
resource-aware O
applications O

- B-DAT
tensive O
quantitative O
and O
qualitative O
evaluations O

- B-DAT
mark O
datasets O
show O
that O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
have O
demonstrated O
the O
state-of-the-art O

- B-DAT
dom O
forest O
[26 O

- B-DAT

- B-DAT
ear O
LR-to-HR O
mapping. O
The O
network O

- B-DAT

- B-DAT
ture O
[17]. O
While O
these O
models O

- B-DAT
sults, O
there O
are O
three O
main O

- B-DAT

- B-DAT
lution O
before O
applying O
the O
network O

- B-DAT
processing O
step O
increases O
unnecessary O
computational O

- B-DAT

- B-DAT
erator O
with O
sub-pixel O
convolution O
[28 O

- B-DAT
volution O
[8] O
(also O
named O
as O

- B-DAT
ture O
the O
underlying O
multi-modal O
distributions O

- B-DAT
smooth O
and O
not O
close O
to O

- B-DAT
ural O
images. O
Third, O
most O
methods O

- B-DAT
isting O
methods O
cannot O
generate O
intermediate O

- B-DAT
ent O
desired O
upsampling O
scales O
and O

- B-DAT

- B-DAT
work O
takes O
an O
LR O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
layer O
for O
upsampling O
the O

- B-DAT
band O
residuals O
(the O
differences O
between O

- B-DAT
age O
and O
the O
ground O
truth O

- B-DAT
tion O
operations. O
While O
the O
proposed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pling O
filters O
with O
deep O
convolutional O

- B-DAT
band O
residuals. O
The O
deep O
supervision O

- B-DAT
tal O
results O
demonstrate O
that O
our O

- B-DAT
eral O
CNN O
based O
super-resolution O
models O

- B-DAT
CNN O
[8], O
our O
LapSRN O
achieves O

- B-DAT

- B-DAT

- B-DAT
ble O
to O
a O
wide O
range O

- B-DAT
aware O
adaptability. O
For O
example, O
the O

- B-DAT
ing O
on O
the O
available O
computational O

- B-DAT
ios O
with O
limited O
computing O
resources O

- B-DAT
putation O
of O
residuals O
at O
finer O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
on O
recent O
example-based O
approaches O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mid O
of O
the O
low-resolution O
input O

- B-DAT
ternal O
image O
databases, O
the O
number O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ically O
slow O
due O
to O
the O

- B-DAT

- B-DAT
ods O
learn O
the O
LR-HR O
mapping O

- B-DAT
rithms, O
such O
as O
nearest O
neighbor O

- B-DAT
ding O
[2, O
5], O
kernel O
ridge O

- B-DAT
resentation O
[37, O
38, O
39]. O
Instead O

- B-DAT
ods O
partition O
the O
image O
database O

- B-DAT

- B-DAT
ear O
regressors O
for O
each O
cluster O

- B-DAT

- B-DAT
CNN O
[7] O
jointly O
optimize O
all O

- B-DAT
linear O
mapping O
in O
the O
image O

- B-DAT

- B-DAT
sample O
images O
to O
the O
desired O

- B-DAT
cursive O
layers O
(DRCN) O
to O
reduce O

- B-DAT

- B-DAT
work O
[28] O
extracts O
feature O
maps O

- B-DAT
places O
the O
bicubic O
upsampling O
operation O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
for O
learning O
and O
predicting O

- B-DAT
CNN, O
VDSR, O
DRCN O
and O
our O

- B-DAT

- B-DAT
ods O
and O
the O
proposed O
framework O

- B-DAT

- B-DAT
sampling O
filters O
with O
convolutional O
and O

- B-DAT
lutional O
layers. O
Using O
the O
learned O

- B-DAT
stead O
of O
the O
`2 O
loss O

- B-DAT
construction O
accuracy. O
Third, O
as O
the O

- B-DAT
gressively O
reconstructs O
HR O
images, O
the O

- B-DAT

- B-DAT
tic O
segmentation O
[11, O
25]. O
Denton O

- B-DAT
GAN) O
to O
generate O
realistic O
images O

- B-DAT
resolution O
model O
that O
predicts O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
GAN, O
the O
convolutional O
layers O
at O

- B-DAT

- B-DAT
tations O
at O
lower O
levels. O
The O

- B-DAT

- B-DAT

- B-DAT
GAN O
are O
independently O
trained. O
On O

- B-DAT

- B-DAT

- B-DAT
isons O
with O
LAPGAN O
in O
the O

- B-DAT
ial O
loss O
for O
photo-realistic O
SR O

- B-DAT
versarial O
loss O
in O
the O
supplementary O

- B-DAT
ual O
images O
at O
log2 O
S O

- B-DAT

- B-DAT
resolving O
an O
LR O
image O
at O

- B-DAT
struction. O
Feature O
extraction. O
At O
level O

- B-DAT
posed O
convolutional O
layer O
to O
upsample O

- B-DAT
tures O
by O
a O
scale O
of O

- B-DAT
volutional O
layer O
is O
connected O
to O

- B-DAT
traction O
at O
the O
coarse O
resolution O

- B-DAT
tional O
layer. O
In O
contrast O
to O

- B-DAT

- B-DAT
sampled O
by O
a O
scale O
of O

- B-DAT
linear O
kernel O
and O
allow O
it O

- B-DAT

- B-DAT
ual O
image O
from O
the O
feature O

- B-DAT

- B-DAT
ilar O
structure O
at O
each O
level O

- B-DAT
work O
parameters O
to O
be O
optimized O

- B-DAT

- B-DAT
age O
ŷ O
= O
f O
(x;θ O

- B-DAT
scaled O
LR O
image O
by O
xs O

- B-DAT
tion O
and O
the O
corresponding O
ground O

- B-DAT

- B-DAT

- B-DAT
ing O
to O
predict O
sub-band O
residual O

- B-DAT

- B-DAT

- B-DAT
sults O
in O
one O
feed-forward O
pass O

- B-DAT

- B-DAT
volutional O
and O
transposed O
convolutional O
layers O

- B-DAT

- B-DAT

- B-DAT
scale O
between O
[0.5,1.0]. O
(2) O
Rotation O

- B-DAT
age O
by O
90◦, O
180◦, O
or O

- B-DAT
izontally O
or O
vertically O
with O
a O

- B-DAT
verges O
faster O
and O
achieves O
improved O

- B-DAT
tions, O
and O
residual O
learning. O
We O

- B-DAT
mance O
(PSNR) O
drop O
on O
both O

- B-DAT
mentum O
parameter O
to O
0.9 O
and O

- B-DAT
SRN O
with O
state-of-the-art O
algorithms O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
performs O
SRCNN O
within O
10 O
epochs O

- B-DAT
trated O
in O
Figure O
2, O
the O

- B-DAT
posed O
network. O
(a) O
HR O
image O

- B-DAT

- B-DAT
formance O
with O
SRCNN. O
In O
Figure O

- B-DAT
struct O
by O
the O
proposed O
algorithm O

- B-DAT
tively O
clean O
and O
sharp O
details O

- B-DAT
ment O
(e.g. O
0.7 O
dB O
on O

- B-DAT
ent O
depth, O
d O
= O
3,5,10,15 O

- B-DAT
offs O
between O
performance O
and O
speed O

- B-DAT
mance O
and O
speed. O
We O
show O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
art O
SR O
algorithms: O
A+ O
[30 O

- B-DAT
BAN100 O
[15] O
and O
MANGA109 O
[23 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
BAN100 O
contains O
challenging O
urban O
scenes O

- B-DAT
ods O
on O
most O
datasets. O
In O

- B-DAT

- B-DAT
less, O
our O
8× O
model O
provides O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pling O
for O
pre-processing O
generate O
results O

- B-DAT
tifacts O
[7, O
17, O
26, O
30 O

- B-DAT
tively O
suppresses O
such O
artifacts O
through O

- B-DAT
struction O
and O
the O
robust O
loss O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
RCNN, O
RFL O
and O
VDSR O
using O

- B-DAT
upsampled O
images O
[7, O
17, O
30 O

- B-DAT

- B-DAT
pling O
[8]. O
The O
state-of-the-art O
methods O

- B-DAT

- B-DAT
structs O
high-quality O
HR O
images O
at O

- B-DAT
ods O
in O
the O
supplementary O
material O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
struct O
these O
models O
in O
MatConvNet O

- B-DAT

-210 B-DAT

-110010 B-DAT
1102 O

- B-DAT

- B-DAT

- B-DAT
RCNN. O
We O
present O
detailed O
evaluations O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
torical O
photographs O
with O
JPEG O
compression O

- B-DAT

- B-DAT
sampling O
kernels O
are O
available. O
As O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ply O
super-resolution O
frame O
by O
frame O

- B-DAT
proach O
achieve O
real-time O
performance O
(i.e O

- B-DAT
lucinate” O
fine O
details. O
As O
shown O

- B-DAT

- B-DAT
herence O
or O
motion O
blur O
are O

- B-DAT

- B-DAT

- B-DAT
ric O
SR O
methods O
[7, O
8 O

- B-DAT
duce O
the O
number O
of O
parameters O

- B-DAT
curate O
single-image O
super-resolution. O
Our O
model O

- B-DAT
sively O
predicts O
high-frequency O
residuals O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mizing O
the O
network O
with O
a O

- B-DAT
posed O
LapSRN O
alleviates O
issues O
with O

- B-DAT
tions O
on O
benchmark O
datasets O
demonstrate O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gorithms O
in O
terms O
of O
visual O

- B-DAT
search O
under O
Grant O
N00014-16-1-2314 O

- B-DAT
Morel. O
Low-complexity O
single-image O
super-resolution O
based O

- B-DAT
ods. O
IJCV, O
61(3):211–231, O
2005. O
4 O

- B-DAT
tions, O
31(4):532–540, O
1983. O
3 O

- B-DAT

- B-DAT
works. O
In O
NIPS, O
2015. O
3 O

- B-DAT
resolution O
using O
deep O
convolutional O
networks O

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT

- B-DAT
based O
super-resolution. O
IEEE, O
Computer O
Graphics O

- B-DAT
plications, O
22(2):56–65, O
2002. O
2 O

- B-DAT
tion O
and O
refinement O
for O
semantic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sis/synthesis. O
In O
SIGGRAPH, O
1995. O
3 O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT
tional O
network O
for O
image O
super-resolution O

- B-DAT

- B-DAT

- B-DAT
ham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT

- B-DAT
supervised O
nets, O
2015. O
In O
International O

- B-DAT
ficial O
Intelligence O
and O
Statistics, O
2015 O

- B-DAT
resolution O
via O
deep O
draft-ensemble O
learning O

- B-DAT

- B-DAT
timedia O
Tools O
and O
Applications, O
pages O

- B-DAT
ters: O
Edge-aware O
image O
processing O
with O

- B-DAT
ing O
to O
refine O
object O
segments O

- B-DAT
rate O
image O
upscaling O
with O
super-resolution O

- B-DAT
mation O
fidelity O
criterion O
for O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution: O
a O
benchmark. O
In O
ECCV O

- B-DAT

- B-DAT
resolution O
as O
sparse O
representation O
of O

- B-DAT
resolution O
via O
sparse O
representation. O
TIP O

- B-DAT

- B-DAT

Gaussian O
denoising, O
SISR O
with O
multiple O
upscaling B-DAT
factors, O
and O
JPEG O
deblocking O
with O

noise O
level, O
SISR O
with O
multiple O
upscaling B-DAT
factors, O
and O
JPEG O
deblocking O
with O

levels, O
down-sampled O
images O
with O
multiple O
upscaling B-DAT
factors, O
and O
JPEG O
images O
with O

model O
for O
all O
the O
three O
upscaling B-DAT
factors O
(i.e., O
2, O
3 O
and O

butterfly” O
from O
Set5 O
dataset O
with O
upscaling B-DAT
factor O
3 O

image O
from O
Urban100 O
dataset O
with O
upscaling B-DAT
factor O
4 O

bicubically O
interpolated O
low-resolution O
images O
with O
upscaling B-DAT
factor O
2 O
(upper O
middle) O
and O

Urban100 B-DAT
3 I-DAT
26.42 O
/ O
0.8076 O
27.13 O

results O
of O
one O
image O
from O
Urban100 B-DAT

- B-DAT

- B-DAT

- B-DAT
works, O
Residual O
Learning, O
Batch O
Normalization O

- B-DAT

- B-DAT
stitute O
of O
Technology, O
Harbin O
150001 O

- B-DAT

- B-DAT
nic O
University, O
Hong O
Kong O
(e-mail O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
backs. O
First, O
those O
methods O
generally O

- B-DAT
timization O
problem O
in O
the O
testing O

- B-DAT

- B-DAT
based O
methods O
can O
hardly O
achieve O

- B-DAT

- B-DAT
mance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
larization O
and O
learning O
methods O
for O

- B-DAT

- B-DAT
allel O
computation O
on O
modern O
powerful O

- B-DAT
noised O
image O
x̂, O
the O
proposed O

- B-DAT
tion O
technique O
is O
further O
introduced O

- B-DAT

- B-DAT
age O
deblocking O
problem O
can O
be O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
results O
when O
extended O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
remarks O
are O
given O
in O

- B-DAT

- B-DAT
age O
denoising. O
In O
[25], O
stacked O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tween O
depth O
and O
width O
[19 O

- B-DAT

- B-DAT

- B-DAT
resolution O
[31] O
and O
color O
image O

- B-DAT

- B-DAT
scent O
(SGD) O
has O
been O
widely O

- B-DAT

- B-DAT
linearity O
inputs O
during O
training. O
Batch O

- B-DAT
rating O
a O
normalization O
step O
and O

- B-DAT

- B-DAT
tion O
for O
CNN-based O
image O
denoising O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
filters O
to O
be O
3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ping O
is O
more O
like O
an O

- B-DAT

- B-DAT
rithms O
and O
network O
architecture. O
Note O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tributed O
to O
the O
internal O
covariate O

- B-DAT
serve O
that, O
with O
batch O
normalization O

- B-DAT

- B-DAT
malization O
offers O
some O
merits O
for O

- B-DAT
ating O
internal O
covariate O
shift O
problem O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eter, O
fk O
∗ O
x O
stands O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stage O
TNRD O
from O
three O
aspects O

- B-DAT

- B-DAT

- B-DAT
sian O
distributed O
(or O
the O
noise O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing, O
we O
set O
the O
range O

- B-DAT

- B-DAT
B. O
We O
use O
color O
version O

- B-DAT
noising O
tasks, O
as O
in O
[35 O

- B-DAT
resolution O
image O
with O
downscaling O
factors O

- B-DAT

- B-DAT

-3 B-DAT

-3, B-DAT
we O
adopt O
different O
test O
set O

- B-DAT

- B-DAT

- B-DAT

-3 B-DAT

- B-DAT

- B-DAT
ments O
are O
carried O
out O
in O

- B-DAT

- B-DAT

- B-DAT

15 O
31.07 O
31.37 O
31.21 O
- B-DAT
31.24 O
31.42 O
31.73 O
31.61 O
σ O

50 O
25.62 O
25.87 O
25.67 O
26.03 O
- B-DAT
25.97 O
26.23 O
26.23 O

- B-DAT

- B-DAT
B/CDnCNN-B O
and O
DnCNN-3 O
on O
GPU O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
training O
based O
methods O
(i.e O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tures O
meet O
well O
with O
the O

- B-DAT

-5 B-DAT
illustrate O
the O
visual O
results O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
7. O
One O
can O
see O
that O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
v5 O
deep O
learning O
library O
to O

-3 B-DAT
model O
is O
trained O
for O
three O

-3 B-DAT
with O
the O
specific O
state-of-the-art O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

203.1 O
25.4 O
1.42 O
2.11 O
/ O
- B-DAT
0.45 O
/ O
0.010 O
0.74 O

- B-DAT

- B-DAT

- B-DAT

-3 B-DAT
is O
compared O
with O
two O
state-of-the-art O

- B-DAT

- B-DAT

-3 B-DAT
model O
for O
the O
three O
different O

-3 B-DAT
outperforms O
AR-CNN O
by O
about O
0.3dB O

-3 B-DAT
and O
VDSR O
can O
produce O
sharp O

- B-DAT
ods. O
As O
one O
can O
see O

-3 B-DAT
can O
recover O
the O
straight O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

0.8861 O
40 O
33.34 O
/ O
0.8953 O
- B-DAT
33.77 O
/ O
0.9003 O

0.9090 O
40 O
33.63 O
/ O
0.9198 O
- B-DAT
33.96 O
/ O
0.9247 O

- B-DAT

- B-DAT
3 O
can O
produce O
visually O
pleasant O

- B-DAT
mance. O
Unlike O
traditional O
discriminative O
models O

- B-DAT

- B-DAT
scaling O
factors, O
and O
JPEG O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
larization O
method O
for O
total O
variation-based O

- B-DAT
nition, O
2007, O
pp. O
1–8 O

- B-DAT
agation O
with O
learned O
higher-order O
Markov O

- B-DAT
tion,” O
in O
IEEE O
Conference O
on O

- B-DAT
inative O
non-blind O
deblurring,” O
in O
IEEE O

- B-DAT

- B-DAT
mation O
Processing O
Systems, O
2012, O
pp O

- B-DAT

-3 B-DAT
/ O
30.02dB O

- B-DAT

- B-DAT

-3 B-DAT
/ O
32.73dB O

- B-DAT

- B-DAT

-3 B-DAT
/ O
29.70dB O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Set5 O
and O
Set14, O
BSD100 O
and O
Urban100 B-DAT
[40]) O
used O
in O
[35 O

Urban100 B-DAT
3 O
26.42 O
/ O
0.8076 O
27.13 O

results O
of O
one O
image O
from O
Urban100 B-DAT
dataset O
with O
upscaling O
factor O
4 O

LR O
images O
directly O
and O
learned O
upscaling B-DAT
filters O
in O
the O
last O
layer O

the O
same O
as O
the O
SR O
upscaling B-DAT
factor O

of O
our O
DSRN O
with O
×2 O
upscaling B-DAT
on O
Set5 O
dataset O

performance O
drop O
across O
all O
three O
upscaling B-DAT
scales O
when O
changing O
from O
shared O

on O
Set O
14 O
with O
×3 O
upscaling B-DAT

recent O
SR O
methods O
for O
×3 O
upscaling B-DAT
on O
Set O
14 O

images O
on O
Set14 O
with O
x3 O
upscaling B-DAT
among O
differ- O
ent O
SR O
approaches O

Fast O
and O
ac- O
curate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In O
CVPR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
erate O
at O
a O
fixed O
spatial O

- B-DAT
resolution O
(LR) O
and O
high-resolution O
(HR O

- B-DAT
layed O
feedback. O
Extensive O
quantitative O
and O

- B-DAT
uations O
on O
benchmark O
datasets O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formance O
has O
been O
significantly O
improved O

- B-DAT
velopments O
in O
deep O
neural O
networks O

- B-DAT
ing O
[16] O
have O
been O
widely O

- B-DAT
tently O
observed. O
The O
first O
is O

- B-DAT
ping O
from O
LR O
to O
HR O

- B-DAT
work O
depth O
enlarges O
the O
size O

-15 B-DAT

-1 B-DAT

-0317 B-DAT

- B-DAT
struct O
missing O
HR O
components. O
The O

- B-DAT
ing O
gradients, O
facilitating O
the O
training O

- B-DAT
troduces O
more O
parameters, O
and O
thus O

- B-DAT

- B-DAT

- B-DAT
tional O
Network O
(DRCN) O
[21] O
shares O

- B-DAT
ent O
residual O
units O
and O
achieves O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ral O
networks O
(RNNs). O
Specifically, O
Liao O

- B-DAT

- B-DAT
work O
(ResNet) O
[16] O
is O
equivalent O

- B-DAT
spired O
by O
their O
findings, O
we O

- B-DAT
olution O
(bicubic O
interpolation O
is O
first O

- B-DAT

- B-DAT
nite O
unfolding O
in O
time O
of O

- B-DAT
tioning O
that O
we O
follow O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ventional O
RNN O
model O
is O
generally O

- B-DAT

- B-DAT
dering O
our O
model O
a O
Dual-State O

- B-DAT

- B-DAT
tions. O
This O
provides O
information O
flow O

- B-DAT
ery O
single O
unrolling O
time. O
In O

- B-DAT
age O
Restoration O
and O
Enhancement O
workshop O

- B-DAT

- B-DAT
sive O
experimental O
results O
validate O
that O

- B-DAT
CNN, O
to O
predict O
the O
nonlinear O

- B-DAT

- B-DAT
strated O
superior O
performance O
to O
many O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mization O
techniques O
[34, O
28, O
11 O

- B-DAT
posed O
a O
sparse O
coding O
network O

- B-DAT

- B-DAT
end, O
demonstrating O
the O
benefit O
of O

- B-DAT
work O
in O
[42 O

- B-DAT
ageNet O
challenges O
[9], O
Kim O
et O

- B-DAT
ents. O
However, O
as O
the O
model O

- B-DAT
rameters O
increases. O
To O
control O
the O

- B-DAT
culty O
of O
training. O
Tai O
et O

- B-DAT
ual O
SR O
learning O
algorithms O
are O

- B-DAT
ual O
learning O
or O
local O
residual O

- B-DAT
rameter O
efficient O
via O
recursive O
learning O

- B-DAT
works O
(DenseNet) O
[17] O
instead O
of O

- B-DAT
put O
images, O
Shi O
et O
al O

- B-DAT
duces O
the O
computation O
cost. O
Similarly O

- B-DAT
bination O
with O
smaller O
filter O
sizes O

- B-DAT
Resolution O
Network O
(LapSRN) O
[22] O
works O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ant, O
which O
learns O
different O
scaled O

- B-DAT
allel O
via O
weight O
sharing O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
posed O
nature O
of O
single O
image O

- B-DAT
tions O
and O
poor O
subjective O
scores O

- B-DAT
back, O
Generative O
Adversarial O
Networks O
have O

- B-DAT
uation O
by O
mean-opinion-score O
showed O
huge O

- B-DAT

- B-DAT
vide O
a O
better O
understanding O
of O

- B-DAT
cations O

- B-DAT

- B-DAT
tem. O
Then, O
based O
on O
this O

- B-DAT
ment O
of O
SR O
models O
with O

- B-DAT

- B-DAT
rent O
states. O
Depending O
on O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
put, O
and O
recurrent O
states O
are O

- B-DAT
tively. O
The O
arrow O
link O
indicates O

- B-DAT
tion O
on O
this O
general O
formulation O

- B-DAT
rection O
to O
a O
fixed O
length O

- B-DAT

- B-DAT

- B-DAT
independent, O
which O
means O
these O
parameters O

- B-DAT
out O
any O
down-sampling O
or O
up-sampling O

- B-DAT
sions O
remain O
the O
same O
across O

- B-DAT
ventional O
residual O
block, O
which O
contains O

- B-DAT
tional O
layers O
with O
skip O
connections O

- B-DAT
rameters O

- B-DAT
ventional O
ResNet O
is O
that O
the O

- B-DAT

- B-DAT
gle O
convolutional O
layer O
to O
the O

- B-DAT
press O
frecurrent. O
The O
graph O
is O

- B-DAT
over, O
unlike O
the O
ResNet O
where O

- B-DAT
tion O
comes O
from O
the O
previous O

- B-DAT
rolled O
state O
s0. O
Figure O
1(e O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
state O
design, O
which O
adopts O
two O

- B-DAT

- B-DAT

- B-DAT
tion O
from O
both O
the O
LR O

- B-DAT
tively. O
Four O
colored O
arrows O
indicate O

- B-DAT

- B-DAT

- B-DAT
tion O
flows O
between O
sl O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

of O
a O
single-state B-DAT
RNN. O
(c) O
- O
(e) O
The O
required O
recurrent O
function O

- B-DAT

- B-DAT
tom O
one O
is O
LR. O
This O

- B-DAT
cialization O
for O
different O
resolutions O
and O

- B-DAT

- B-DAT

- B-DAT
sampling O
transition. O
The O
strides O
in O

- B-DAT

- B-DAT

- B-DAT
ing O
a O
prediction O
at O
every O

- B-DAT
terized O
by O
a O
single O
convolutional O

- B-DAT
ing O
the O
prediction O
only O
at O

- B-DAT
over, O
the O
model O
predicts O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lowing O
datasets: O
Set5 O
[4], O
Set14 O

- B-DAT
ban100 O
[19]. O
The O
training O
data O

- B-DAT
dom O
flipping O
along O
the O
vertical O

- B-DAT

- B-DAT

- B-DAT
vided O
training O
and O
validation O
sets O

- B-DAT
tioned O
data O
augmentations O
except O
random O

- B-DAT

- B-DAT
lution O
filters. O
Due O
to O
our O

- B-DAT

- B-DAT
sions O
as O
the O
LR O
and O

- B-DAT

- B-DAT
volution O
is O
applied O

- B-DAT
form O
distribution O
using O
the O
method O

- B-DAT
mentum O
0.95 O
as O
our O
optimizer O

- B-DAT
ing O
rate O
from O
{0.1,0.03,0.01} O
and O

- B-DAT
ing O
rate O
annealing O
is O
driven O

- B-DAT

- B-DAT
posed O
the O
use O
of O
unshared O

- B-DAT
folding O
time O
to O
resolve O
this O

- B-DAT
nary O
ReLU O
as O
the O
activation O

- B-DAT
imum O
effective O
depth O
of O
the O

- B-DAT
work O
is O
2T O
+ O
4 O

- B-DAT
ers O
in O
a O
residual O
block O

- B-DAT
iary O
input O
and O
output O
layers O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity O
and O
computation O
cost. O
We O

- B-DAT
pirical O
results O
are O
shown O
in O

- B-DAT
tioning O
that O
we O
also O
experimented O

- B-DAT
ing O
to O
be O
crucial O
for O

- B-DAT
forms O
much O
more O
poorly O
than O

- B-DAT

- B-DAT
part. O
Specifically, O
we O
observe O
around O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
state O
baseline O
and O
the O
DSRN O

- B-DAT
tion, O
comparing O
our O
models O
with O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
eral O
public O
benchmark O
datasets O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentation O
Dataset O
[2], O
while O
our O

- B-DAT
tive O
performance O
across O
all O
datasets O

- B-DAT
cently O
developed O
DIV2K O
dataset O
and O

- B-DAT
ranking O
algorithms O
in O
Table O
2 O

- B-DAT
petitive O
performance O
with O
the O
best O

- B-DAT
state O
recurrent O
structure O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolved O
images O
on O
Set14 O
with O

- B-DAT
ent O
SR O
approaches. O
For O
these O

- B-DAT
tures O
and O
is O
less O
prone O

- B-DAT
trate O
the O
parameters-to-PSNR O
relationship O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mance, O
and O
has O
modest O
inference O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
folding O
of O
a O
single-state O
RNN O

- B-DAT
tions. O
Based O
on O
this, O
we O

- B-DAT
ering O
a O
dual-state O
design; O
the O

- B-DAT
posed O
DSRN O
operate O
at O
different O

- B-DAT

- B-DAT
iments O
on O
benchmark O
datasets O
have O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tour O
detection O
and O
hierarchical O
image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
based O
on O
nonnegative O
neighbor O

- B-DAT
resolution O
through O
neighbor O
embedding. O
In O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
database. O
In O
CVPR, O
pages O

- B-DAT
ing O
a O
deep O
convolutional O
network O

- B-DAT
resolution. O
In O
ECCV, O
2014. O
1 O

- B-DAT

- B-DAT

- B-DAT
ual O
networks O
for O
image O
super-resolution O

- B-DAT

- B-DAT

- B-DAT
TATS, O
2010. O
5 O

- B-DAT

- B-DAT
berger. O
Deep O
networks O
with O
stochastic O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
works. O
In O
CVPR, O
2016. O
1 O

- B-DAT
recursive O
convolutional O
network O
for O
image O

- B-DAT
resolution. O
In O
CVPR, O
2016. O
1 O

- B-DAT

- B-DAT
ningham, O
A. O
Acosta, O
A. O
Aitken O

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT
hanced O
deep O
residual O
networks O
for O

- B-DAT
resolution. O
In O
CVPR O
Workshops, O
2017 O

- B-DAT

- B-DAT
ing O
a O
mixture O
of O
deep O

- B-DAT
resolution. O
In O
ACCV, O
pages O
145–156 O

- B-DAT

- B-DAT
tion O
using O
very O
deep O
convolutional O

- B-DAT

- B-DAT
hancenet: O
Single O
image O
super-resolution O
through O

- B-DAT
tomated O
texture O
synthesis. O
In O
ICCV O

- B-DAT
curate O
image O
upscaling O
with O
super-resolution O

- B-DAT
ment O
using O
natural O
scene O
statistics O

- B-DAT

- B-DAT
gle O
image O
and O
video O
super-resolution O

- B-DAT

- B-DAT

- B-DAT
resolution: O
Methods O
and O
results. O
In O

- B-DAT

- B-DAT
resolution O
using O
dense O
skip O
connections O

- B-DAT
works O
behave O
like O
ensembles O
of O

- B-DAT
works. O
In O
NIPS, O
2016. O
1 O

- B-DAT
celli. O
Image O
quality O
assessment: O
from O

- B-DAT

- B-DAT

- B-DAT
lution. O
In O
CVPRW, O
pages O
1–8 O

- B-DAT
resolution. O
TIP, O
21(8):3467–3478, O
2012. O
2 O

- B-DAT

- B-DAT
lective O
tensor O
factorization O
in O
deep O

- B-DAT

- B-DAT

- B-DAT
ity O
measures O
of O
recurrent O
neural O

the O
Set5 O
dataset O
with O
an O
upscaling B-DAT
factor O
3). O
The O
proposed O
method O

kernel, O
sub-sample O
it O
by O
the O
upscaling B-DAT
factor, O
and O
upscale O
it O
by O

larger O
Set14 O
set O
[51]. O
The O
upscaling B-DAT
factor O
is O
3. O
We O
use O

on O
the O
ImageNet O
by O
an O
upscaling B-DAT
factor O
3. O
Please O
refer O
to O

our O
published O
implementation O
for O
upscaling B-DAT
factors O
2 O
and O
4. O
Interestingly O

trained O
on O
ImageNet O
with O
an O
upscaling B-DAT
factor O
3. O
The O
filters O
are O

test O
on O
Set5 O
with O
an O
upscaling B-DAT
factor O
3. O
The O
results O
observed O

4.1. O
The O
results O
with O
an O
upscaling B-DAT
factor O
3 O
on O
Set5 O
are O

on O
the O
ImageNet. O
For O
each O
upscaling B-DAT
factor O
∈ O
{2, O
3, O
4 O

to O
evaluate O
the O
performance O
of O
upscaling B-DAT
factors O
2, O
3, O
and O
4 O

108 O
backpropagations. O
Specifically, O
for O
the O
upscaling B-DAT
factor O
3, O
the O
average O
gains O

of O
different O
approaches O
by O
an O
upscaling B-DAT
factor O
3. O
As O
can O
be O

for O
fair O
quantitative O
comparison. O
The O
upscaling B-DAT
factor O
is O
3 O
and O
the O

only O
evaluate O
the O
performance O
of O
upscaling B-DAT
factor O
3. O
Comparisons. O
We O
compare O

network O
to O
cope O
with O
different O
upscaling B-DAT
factors O

Fattal, O
R.: O
Image O
and O
video O
upscaling B-DAT
from O
local O
self-examples. O
ACM O
Transactions O

H.: O
Fast O
and O
accurate O
image O
upscaling B-DAT
with O
super-resolution O
forests. O
In: O
IEEE O

image O
from O
Set5 O
with O
an O
upscaling B-DAT
factor O
3 O

image O
from O
Set14 O
with O
an O
upscaling B-DAT
factor O
3 O

image O
from O
Set14 O
with O
an O
upscaling B-DAT
factor O
3 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
offs O
between O
performance O
and O
speed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
image, O
is O
a O
classical O

- B-DAT

- B-DAT
tiplicity O
of O
solutions O
exist O
for O

- B-DAT

- B-DAT
verse O
problem, O
of O
which O
solution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
exemplar O
pairs O
[2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
structing O
high-resolution O
patches. O
The O
overlapping O

- B-DAT

- B-DAT

- B-DAT
work O
[27] O
(more O
details O
in O

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
images. O
Our O
method O

- B-DAT
tally O
from O
existing O
external O
example-based O

- B-DAT
processing O

- B-DAT

- B-DAT
volutional O
Neural O
Network O
(SRCNN)1. O
The O

- B-DAT
ture O
is O
intentionally O
designed O
with O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
example-based O
methods. O
Furthermore, O
the O

- B-DAT

- B-DAT
work O
for O
image O
super-resolution. O
The O

- B-DAT
rectly O
learns O
an O
end-to-end O
mapping O

- B-DAT
and O
high-resolution O
images, O
with O
little O

- B-DAT
processing O
beyond O
the O
optimization O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution, O
and O
can O
achieve O
good O

- B-DAT

- B-DAT
linear O
mapping O
layers. O
Secondly, O
we O

- B-DAT
strate O
that O
performance O
can O
be O

- B-DAT

- B-DAT
ber O
of O
recently O
published O
methods O

- B-DAT

- B-DAT

- B-DAT
olution O
algorithms O
can O
be O
categorized O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
similarity O
property O
and O
generate O
exemplar O

- B-DAT
nal O
example-based O
methods O
[2], O
[4 O

- B-DAT
resolution O
patches O
from O
external O
datasets O

- B-DAT

- B-DAT
tionaries O
are O
directly O
presented O
as O

- B-DAT

- B-DAT

- B-DAT
sponding O
high-resolution O
patch O
used O
for O

- B-DAT
nique O
as O
an O
alternative O
to O

- B-DAT
borhood O
regression O
[41], O
[42] O
are O

- B-DAT
coding-based O
method O
and O
its O
several O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tioned O
methods O
first O
transform O
the O

- B-DAT
ferent O
color O
space O
(YCbCr O
or O

- B-DAT

- B-DAT
fully O
applied O
to O
other O
computer O

- B-DAT

- B-DAT
ceptron O
(MLP), O
whose O
all O
layers O

- B-DAT

- B-DAT

- B-DAT
work O
is O
applied O
for O
natural O

- B-DAT
moving O
noisy O
patterns O
(dirt/rain) O
[12 O

- B-DAT

- B-DAT

- B-DAT
resolution O
pipeline O
under O
the O
notion O

- B-DAT
based O
approach O
[16]. O
The O
deep O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
end O
mapping. O
Further, O
the O
SRCNN O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
extracts O
(overlapping) O
patches O
from O

- B-DAT
resolution O
image O
Y O
and O
represents O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
neural O
network. O
An O
overview O

- B-DAT

- B-DAT
spectively, O
and O
’∗’ O
denotes O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tors O
is O
conceptually O
a O
representation O

- B-DAT

- B-DAT

- B-DAT
plexity O
of O
the O
model O
(n2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
patch). O
Motivated O
by O
this O

- B-DAT
lutional O
layer O
to O
produce O
the O

- B-DAT

- B-DAT

- B-DAT
tion O
and O
representation) O
becomes O
purely O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution) O
dictionary. O
If O
the O
dictionary O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
based O
SR O
method O
can O
be O

- B-DAT
volutional O
neural O
network O
(with O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
parameters. O
For O
example, O
we O
can O

- B-DAT
resolution O
patch O
(to O
the O
extreme O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
quires O
the O
estimation O
of O
network O

- B-DAT
imizing O
the O
loss O
between O
the O

- B-DAT
resolution O
images O
X. O
Given O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
crafted” O
methods. O
Despite O
that O
the O

- B-DAT
tory O
performance O
when O
the O
model O

- B-DAT
scent O
with O
the O
standard O
backpropagation O

- B-DAT
ular, O
the O
weight O
matrices O
are O

- B-DAT
erations, O
η O
is O
the O
learning O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ping O
and O
require O
some O
averaging O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
volutional O
layers O
have O
no O
padding O

- B-DAT

- B-DAT
age O
[26]. O
We O
have O
also O

- B-DAT
tions O
between O
super-resolution O
performance O
and O

- B-DAT
quently, O
we O
compare O
our O
method O

- B-DAT

- B-DAT
the-arts O
both O
quantitatively O
and O
qualitatively O

- B-DAT

- B-DAT
4.4, O
so O
c O
= O
1 O

- B-DAT
tion O
training O
partition. O
The O
size O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nal O
images O
with O
a O
stride O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
geNet O
is O
about O
the O
same O

- B-DAT

- B-DAT
formance O
may O
be O
further O
boosted O

- B-DAT

- B-DAT
tured O
sufficient O
variability O
of O
natural O

- B-DAT

- B-DAT

Laplacian/Gaussian O
filters, O
the O
filters O
a O
- B-DAT
e O
are O
like O
edge O
detectors O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
crease O
the O
network O
width6, O
i.e O

- B-DAT
coding-based O
method O
(31.42 O
dB O

-1 B-DAT

-5 B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-7 B-DAT

- B-DAT
ing O
[17]. O
The O
term O
‘width O

-3 B-DAT

-5 B-DAT

-5 B-DAT

-5 B-DAT

-3 B-DAT

- B-DAT
5 O
and O
9-5-5 O
on O
Set5 O

-1 B-DAT

-5, B-DAT
9-3-5, O
and O
9-5-5 O
is O
8,032 O

-5 B-DAT

-5 B-DAT
is O
almost O
twice O
of O
9-3-5 O

- B-DAT

- B-DAT

-1 B-DAT

-1 B-DAT

-5, B-DAT
9-3-1-5, O
9-5-1-5, O
which O
add O
an O

-1 B-DAT

-5, B-DAT
9-3-5, O
and O
9-5-5, O
respectively. O
The O

- B-DAT
ditional O
layer O
are O
the O
same O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lution O
is O
found O
not O
as O

-1 B-DAT

-5 B-DAT
network, O
then O
the O
performance O
degrades O

- B-DAT

- B-DAT
ure O
9(a)). O
If O
we O
go O

- B-DAT

-1 B-DAT

-5 B-DAT
vs. O
9-1-1-5 O

-3 B-DAT

-5 B-DAT
vs. O
9-3-1-5 O

-5 B-DAT

-5 B-DAT
vs. O
9-5-1-5 O

- B-DAT

- B-DAT

-1 B-DAT

-5, B-DAT
then O
we O
have O
to O
set O

-3 B-DAT

- B-DAT
3-5 O
and O
9-3-3-3. O
However, O
from O

-3 B-DAT

-1 B-DAT

-5 B-DAT
network O

- B-DAT

- B-DAT

- B-DAT
vestigations O
to O
better O
understand O
gradients O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-1 B-DAT

-5 B-DAT
(n22 O
= O
32) O
and O
9-1-1-1-5 O

-3 B-DAT

-3 B-DAT

-5 B-DAT
and O
9-3-3-3 O

- B-DAT
speed O
trade-off: O
a O
three-layer O
network O

- B-DAT
of-the-art O
SR O
methods O

SC O
- B-DAT
sparse O
coding-based O
method O
of O
Yang O

et O
al. O
[50] O
• O
NE+LLE O
- B-DAT
neighbour O
embedding O
+ O
locally O
linear O

embedding O
method O
[4] O
• O
ANR O
- B-DAT
Anchored O
Neighbourhood O
Regression O

method O
[41] O
• O
A+ O
- B-DAT
Adjusted O
Anchored O
Neighbourhood O
Regres O

method O
[42], O
and O
• O
KK O
- B-DAT
the O
method O
described O
in O
[25 O

- B-DAT
based O
methods, O
according O
to O
the O

- B-DAT
sampled O
using O
the O
same O
bicubic O

- B-DAT
terion O
(IFC) O
[38], O
noise O
quality O

- B-DAT

- B-DAT

- B-DAT
scale O
structure O
similarity O
index O
(MSSSIM O

ANR O
- B-DAT
31.92 O
dB O

A+ O
- B-DAT
32.59 O
dB O

SC O
- B-DAT
31.42 O
dB O

Bicubic O
- B-DAT
30.39 O
dB O

NE+LLE O
- B-DAT
31.84 O
dB O

KK O
- B-DAT
32.28 O
dB O

- B-DAT
CNN O
outperforms O
existing O
state-of-the-art O
methods O

- B-DAT

- B-DAT

- B-DAT
cific O
network O
(9-5-5) O
using O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mentation, O
whereas O
ours O
are O
in O

- B-DAT

-1 B-DAT

-5, B-DAT
9-3-5, O
and O
9-5-5. O
It O
is O

- B-DAT
1-5 O
network O
is O
the O
fastest O

- B-DAT

- B-DAT

- B-DAT

-1 B-DAT

-5 B-DAT
network. O
Note O
the O
speed O
gap O

- B-DAT
pletely O
feed-forward. O
The O
9-5-5 O
network O

- B-DAT

- B-DAT

- B-DAT
terpolation. O
It O
is O
interesting O
to O

- B-DAT

- B-DAT
out O
altering O
the O
learning O
mechanism O

- B-DAT
sign. O
In O
particular, O
it O
can O

- B-DAT
nels O
simultaneously O
by O
setting O
the O

- B-DAT

- B-DAT

-5 B-DAT

- B-DAT

A+ O
[41] O
SRCNN O
2 O
33.66 O
- B-DAT
35.77 O
36.20 O
35.83 O
36.54 O
36.66 O

31.92 O
32.59 O
32.75 O
4 O
28.42 O
- B-DAT
29.61 O
30.03 O
29.69 O
30.28 O
30.49 O

2 O
0.9299 O
- B-DAT
0.9490 O
0.9511 O
0.9499 O
0.9544 O
0.9542 O

0.8968 O
0.9088 O
0.9090 O
4 O
0.8104 O
- B-DAT
0.8402 O
0.8541 O
0.8419 O
0.8603 O
0.8628 O

2 O
6.10 O
- B-DAT
7.84 O
6.87 O
8.09 O
8.48 O
8.05 O

4.52 O
4.84 O
4.58 O
4 O
2.35 O
- B-DAT
2.94 O
2.81 O
3.02 O
3.26 O
3.01 O

2 O
36.73 O
- B-DAT
42.90 O
39.49 O
43.28 O
44.58 O
41.13 O

33.10 O
34.48 O
33.21 O
4 O
21.42 O
- B-DAT
25.56 O
24.99 O
25.72 O
26.97 O
25.96 O

2 O
50.06 O
- B-DAT
58.45 O
57.15 O
58.61 O
60.06 O
59.49 O

46.02 O
47.17 O
47.10 O
4 O
37.21 O
- B-DAT
39.85 O
40.40 O
40.01 O
41.03 O
41.13 O

2 O
0.9915 O
- B-DAT
0.9953 O
0.9953 O
0.9954 O
0.9960 O
0.9959 O

0.9844 O
0.9867 O
0.9866 O
4 O
0.9516 O
- B-DAT
0.9666 O
0.9695 O
0.9672 O
0.9720 O
0.9725 O

A+ O
[41] O
SRCNN O
2 O
30.23 O
- B-DAT
31.76 O
32.11 O
31.80 O
32.28 O
32.45 O

28.65 O
29.13 O
29.30 O
4 O
26.00 O
- B-DAT
26.81 O
27.14 O
26.85 O
27.32 O
27.50 O

2 O
0.8687 O
- B-DAT
0.8993 O
0.9026 O
0.9004 O
0.9056 O
0.9067 O

0.8093 O
0.8188 O
0.8215 O
4 O
0.7019 O
- B-DAT
0.7331 O
0.7419 O
0.7352 O
0.7491 O
0.7513 O

2 O
6.09 O
- B-DAT
7.59 O
6.83 O
7.81 O
8.11 O
7.76 O

4.23 O
4.45 O
4.26 O
4 O
2.23 O
- B-DAT
2.71 O
2.57 O
2.78 O
2.94 O
2.74 O

2 O
40.98 O
- B-DAT
41.34 O
38.86 O
41.79 O
42.61 O
38.95 O

37.22 O
38.24 O
35.25 O
4 O
26.15 O
- B-DAT
31.17 O
29.18 O
31.27 O
32.31 O
30.46 O

2 O
47.64 O
- B-DAT
54.47 O
53.85 O
54.57 O
55.62 O
55.39 O

43.36 O
44.25 O
44.32 O
4 O
35.71 O
- B-DAT
37.75 O
38.26 O
37.85 O
38.72 O
38.87 O

2 O
0.9813 O
- B-DAT
0.9886 O
0.9890 O
0.9888 O
0.9896 O
0.9897 O

0.9647 O
0.9669 O
0.9675 O
4 O
0.9134 O
- B-DAT
0.9317 O
0.9338 O
0.9326 O
0.9371 O
0.9376 O

A+ O
[41] O
SRCNN O
2 O
28.38 O
- B-DAT
29.67 O
30.02 O
29.72 O
30.14 O
30.29 O

26.72 O
27.05 O
27.18 O
4 O
24.65 O
- B-DAT
25.21 O
25.38 O
25.25 O
25.51 O
25.60 O

2 O
0.8524 O
- B-DAT
0.8886 O
0.8935 O
0.8900 O
0.8966 O
0.8977 O

0.7843 O
0.7945 O
0.7971 O
4 O
0.6727 O
- B-DAT
0.7037 O
0.7093 O
0.7060 O
0.7171 O
0.7184 O

2 O
5.30 O
- B-DAT
7.10 O
6.33 O
7.28 O
7.51 O
7.21 O

3.91 O
4.07 O
3.91 O
4 O
1.95 O
- B-DAT
2.45 O
2.24 O
2.51 O
2.62 O
2.45 O

2 O
36.84 O
- B-DAT
41.52 O
38.54 O
41.72 O
42.37 O
39.66 O

34.81 O
35.58 O
34.72 O
4 O
21.72 O
- B-DAT
25.15 O
24.87 O
25.27 O
26.01 O
25.65 O

2 O
46.15 O
- B-DAT
52.56 O
52.21 O
52.69 O
53.56 O
53.58 O

41.53 O
42.19 O
42.29 O
4 O
34.86 O
- B-DAT
36.52 O
36.80 O
36.64 O
37.18 O
37.24 O

2 O
0.9780 O
- B-DAT
0.9869 O
0.9876 O
0.9872 O
0.9883 O
0.9883 O

0.9581 O
0.9609 O
0.9614 O
4 O
0.9005 O
- B-DAT
0.9203 O
0.9215 O
0.9214 O
0.9256 O
0.9261 O

-1 B-DAT

-5 B-DAT

-3 B-DAT

-5 B-DAT

-5 B-DAT

-5 B-DAT

- B-DAT
of-the-art O
super-resolution O
quality, O
whilst O
maintains O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
of-art O
color O
SR O
method O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nel O
when O
training O
is O
performed O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
correlation O
among O
each O
other. O
The O

- B-DAT

- B-DAT
channel O
network O
(“Y O
only”). O
It O

- B-DAT

- B-DAT
work O
is O
not O
that O
significant O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
images, O
with O
little O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vantages O
of O
simplicity O
and O
robustness O

- B-DAT

- B-DAT
blurring O
or O
simultaneous O
SR+denoising. O
One O

- B-DAT

- B-DAT
complexity O
single-image O
super-resolution O
based O
on O

- B-DAT

- B-DAT
bor O
embedding. O
In: O
IEEE O
Conference O

- B-DAT

- B-DAT

- B-DAT
resolution. O
IEEE O
Transactions O
on O
Image O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
linear O
structure O
within O
convolutional O

- B-DAT
tems O
(2014 O

- B-DAT
tional O
network O
for O
image O
super-resolution O

- B-DAT
ence O
on O
Computer O
Vision, O
pp O

- B-DAT
tional O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT

- B-DAT
resolution. O
Computer O
Graphics O
and O
Applications O

- B-DAT
level O
vision. O
International O
Journal O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puter O
Vision O
and O
Pattern O
Recognition O

- B-DAT
lutional O
neural O
networks O
with O
low O

- B-DAT
tems. O
pp. O
769–776 O
(2008 O

- B-DAT

- B-DAT

- B-DAT
ten O
zip O
code O
recognition. O
Neural O

- B-DAT

- B-DAT
rithms. O
In: O
Advances O
in O
Neural O

- B-DAT
mentation O
algorithms O
and O
measuring O
ecological O

- B-DAT

- B-DAT

- B-DAT
tion. O
In: O
IEEE O
International O
Conference O

- B-DAT
chine O
learning O
approach O
for O
non-blind O

- B-DAT

- B-DAT
tics. O
IEEE O
Transactions O
on O
Image O

- B-DAT
tation O
by O
joint O
identification-verification. O
In O

- B-DAT
quality O
object O
detection. O
arXiv O
preprint O

- B-DAT

- B-DAT

- B-DAT
ternational O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
ilarity O
for O
image O
quality O
assessment O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tionary O
training O
for O
image O
super-resolution O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
sparse-representations. O
In: O
Curves O
and O

- B-DAT

- B-DAT
CNNs O
for O
fine-grained O
category O
detection O

- B-DAT
ence O
on O
Computer O
Vision. O
pp O

- B-DAT
tion O
Engineering O
from O
Beijing O
Institute O

- B-DAT
nology, O
China, O
in O
2011. O
He O

- B-DAT
sity O
of O
Hong O
Kong. O
His O

- B-DAT

- B-DAT
versity O
of O
London O
in O
2010 O

- B-DAT
toral O
researcher O
at O
Vision O
Semantics O

- B-DAT
inghua O
University O
in O
2007, O
and O

- B-DAT
ence O
on O
Computer O
Vision O
and O

- B-DAT
tion O
(CVPR) O
2009. O
He O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sachusetts O
Institute O
of O
Technology, O
Cambridge O

- B-DAT
cessing. O
He O
received O
the O
Best O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Set5 O
Set14 O
BSD100 O
Urban100 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
GAN) O
[1] O
is O
a O
seminal O

- B-DAT

- B-DAT
lar, O
we O
introduce O
the O
Residual-in-Residual O

- B-DAT
vide O
stronger O
supervision O
for O
brightness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lem, O
has O
attracted O
increasing O
attention O

- B-DAT
panies. O
SISR O
aims O
at O
recovering O

- B-DAT

- B-DAT

- B-DAT
perous O
development. O
Various O
network O
architecture O

- B-DAT

- B-DAT
Noise O
Ratio O
(PSNR) O
value O
[5,6,7,1,8,9,10,11,12 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
uation O
of O
human O
observers O
[1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
mize O
super-resolution O
model O
in O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
RGAN, O
consistently O
outperforms O
state-of-the-art O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
struction O
style O
and O
smoothness. O
Another O

- B-DAT
pate O
in O
region O
1 O
and O

- B-DAT

- B-DAT

- B-DAT
tures, O
such O
as O
a O
deeper O

- B-DAT
work O
[9], O
deep O
back O
projection O

- B-DAT
provement. O
Zhang O
et O
al. O
[11 O

- B-DAT
ing O
the O
state-of-the-art O
PSNR O
performance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity O
[29,14], O
perceptual O
loss O
[13 O

- B-DAT
imizing O
the O
error O
in O
a O

- B-DAT
pearance. O
Ledig O
et O
al. O
[1 O

- B-DAT
jadi O
et O
al. O
[16] O
develop O

- B-DAT

- B-DAT
veloping O
more O
effective O
GAN O
frameworks O

- B-DAT
tor O
includes O
gradient O
clipping O
[32 O

- B-DAT
ated O
data O
are O
real, O
but O

- B-DAT
sures, O
e.g., O
PSNR O
and O
SSIM O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lenge O
[3]. O
In O
a O
recent O

- B-DAT
tion, O
we O
first O
describe O
our O

- B-DAT
tion O
is O
done O
in O
the O

- B-DAT
ers; O
2) O
replace O
the O
original O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
putational O
complexity O
in O
different O
PSNR-oriented O

- B-DAT
ing O
dataset O
during O
testing. O
When O

- B-DAT
alization O
ability. O
We O
empirically O
observe O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
residual O
network. O
However, O
our O

- B-DAT
erage O
Discriminator O
RaD O
[2], O
denoted O

- B-DAT

- B-DAT
mulated O
as O
DRa(xr, O
xf O

- B-DAT

- B-DAT

- B-DAT
tures O
before O
activation O
rather O
than O

- B-DAT

-543 B-DAT
layer O
is O
merely O
11.17%. O
The O

- B-DAT

- B-DAT

- B-DAT
tance O
between O
recovered O
image O
G(xi O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nition O
[38], O
which O
focuses O
on O

- B-DAT
ing O
perceptual O
loss O
that O
focuses O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tures O
and O
similarly, O
22 O
represents O

- B-DAT

-22 B-DAT
b) O
activation O
map O
of O

-54 B-DAT

- B-DAT
boon’. O
With O
the O
network O
going O

- B-DAT

- B-DAT
ceptual O
quality, O
we O
propose O
a O

- B-DAT
tion. O
Specifically, O
we O
first O
train O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
parameters O
of O
these O
two O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
polated O
image O
is O
either O
too O

- B-DAT
rameter O
λ O
and O
η O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
oriented O
model O
with O
the O
L1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els O
on O
widely O
used O
benchmark O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
the-art O
PSNR-oriented O
methods O
including O
SRCNN O

- B-DAT

- B-DAT
tures, O
e.g., O
animal O
fur, O
building O

- B-DAT
pleasant O
artifacts, O
e.g., O
artifacts O
in O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults, O
and O
than O
previous O
GAN-based O

- B-DAT
tures O
in O
building O
(see O
image O

- B-DAT

- B-DAT
mance O
without O
artifacts. O
It O
does O

- B-DAT
ment O
can O
be O
observed O
from O

- B-DAT
tures O
before O
activation O
can O
result O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gies O
in O
balancing O
the O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
oriented O
method O
outputs O
cartoon-style O
blurry O

- B-DAT

- B-DAT

- B-DAT
pirically O
make O
some O
modifications O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
terpolation O
between O
the O
results O
of O

- B-DAT

- B-DAT

- B-DAT
ceptual O
quality O
than O
previous O
SR O

- B-DAT

- B-DAT
dition, O
useful O
techniques O
including O
residual O

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
a O
generative O
adversarial O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution. O
In: O
CVPR. O
(2018 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
level O
performance O
on O
imagenet O
classification O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
representations. O
In: O
International O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tics. O
(2010 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
duce O
several O
useful O
techniques O
that O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
a O
very O
deep O
network O

- B-DAT

- B-DAT
Residual O
Dense O
Block O
(RRDB), O
which O

- B-DAT
ers O
[47,28]. O
He O
et O
al O

- B-DAT

- B-DAT
tially. O
It O
is O
worth O
noting O

- B-DAT
tion O
(multiplying O
0.1 O
for O
all O

- B-DAT
tremely O
bad O
local O
minimum O
with O

- B-DAT
tion O
(×0.1) O
helps O
the O
network O

- B-DAT
tion O
achieves O
a O
higher O
PSNR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rithms: O
average O
PSNR/SSIM O
on O
Y O

Bicubic O
- B-DAT
28.42/0.8104 O
26.00/0.7027 O
25.96/0.6675 O
23.14/0.6577 O
24.89/0.7866 O

- B-DAT

- B-DAT
verse O
natural O
textures. O
We O
employ O

- B-DAT
over, O
the O
deeper O
model O
achieves O

- B-DAT

- B-DAT

- B-DAT

42], O
Set14 O
[43], O
BSD100 O
[44], O
Urban100 B-DAT
[45], O
and O
the O
PIRM O
self-validation O

Set5 O
Set14 O
BSD100 O
Urban100 B-DAT
Manga109 O
PSNR/SSIM O
PSNR/SSIM O
PSNR/SSIM O
PSNR/SSIM O

BSDS100 O
consist O
of O
natural O
scenes; O
Urban100 B-DAT

di- O
vide O
each O
image O
in O
Urban100 B-DAT

than O
EDSR. O
While O
on O
the O
Urban100 B-DAT

Set5 O
Set14 O
BSDS100 O
Urban100 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
and O
high-resolution O
images. O
We O
propose O

- B-DAT

- B-DAT
and O
down- O
sampling O
layers, O
providing O

- B-DAT
connected O
up- O
and O
down-sampling O
stages O

- B-DAT
resolution O
components. O
We O
show O
that O

- B-DAT
and O
down- O
sampling O
stages O
(Dense O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
proach O
is O
to O
construct O
an O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work O
[6, O
7, O
38, O
25 O

- B-DAT
ing O
with O
one O
or O
more O

- B-DAT
lution O
and O
finally O
construct O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
SRN O
[25] O
(15.25 O
dB), O
EDSR O

- B-DAT
fectively O
by O
one O
of O
the O

- B-DAT

- B-DAT
tion O
error O
then O
fuses O
it O

- B-DAT
fect O
[4]. O
Moreover, O
this O
method O

- B-DAT
erator, O
leading O
to O
variability O
in O

- B-DAT

- B-DAT

- B-DAT
and O
down- O
sampling: O
Deep O
Back-Projection O

- B-DAT
butions: O
(1) O
Error O
feedback. O
We O

- B-DAT
correcting O
feedback O
mechanism O
for O
SR O

- B-DAT
and O
down-projection O
errors O
to O
guide O

- B-DAT
tion O
for O
obtaining O
better O
results O

- B-DAT
and O
down-sampling O
stages. O
Feed-forward O
architectures O

- B-DAT
way O
mapping, O
only O
map O
rich O

- B-DAT

- B-DAT
(blue O
box) O
and O
down-sampling O
(gold O

- B-DAT
tures O
using O
upsampling O
layers O
but O

- B-DAT
(blue O
box) O
and O
down-sampling O
(gold O

- B-DAT
ity O
enables O
the O
networks O
to O

- B-DAT

- B-DAT
tion O
directly O
utilizes O
different O
types O

- B-DAT

- B-DAT

- B-DAT
and O
down-sampling O
stage O
to O
encourage O

- B-DAT

- B-DAT
tion O
as O
the O
upsampling O
operator O

- B-DAT
tion O
(MR) O
image. O
This O
schema O

- B-DAT
CNN O
[6] O
to O
learn O
MR-to-HR O

- B-DAT

- B-DAT
ple O
convolutional O
layers. O
Later, O
the O

- B-DAT
ploited O
residual O
learning O
[22, O
43 O

- B-DAT
posed O
by O
FSRCNN O
[7] O
and O

- B-DAT
tion O
and O
replace O
predefined O
operators O

- B-DAT
longs O
to O
this O
type. O
However O

- B-DAT
portunities O
to O
propose O
lighter O
networks O

- B-DAT

- B-DAT
late O
the O
reconstruction O
error O
to O

- B-DAT
ables O
the O
networks O
to O
preserve O

- B-DAT
ing O
various O
up- O
and O
down-sampling O

- B-DAT
ating O
deeper O
features O

- B-DAT

- B-DAT
to-target O
space O
in O
one O
step O

- B-DAT
pose O
the O
prediction O
process O
into O

- B-DAT

- B-DAT
back O
procedure O
has O
been O
implemented O

- B-DAT
tion. O
PredNet O
[32] O
is O
an O

- B-DAT
tion, O
Li O
et O
al. O
[29 O

- B-DAT
back O
procedures O
have O
not O
been O

- B-DAT
ial O
Networks O
(GANs) O
[10] O
has O

- B-DAT
age O
reconstruction O
problems O
[28, O
37 O

- B-DAT

- B-DAT

- B-DAT
works. O
Ledig O
et O
al. O
[28 O

- B-DAT
ered O
as O
a O
single O
upsampling O

- B-DAT
ral O
image O
manifold O
that O
is O

- B-DAT

- B-DAT
ages O
by O
specifically O
formulating O
a O

- B-DAT

- B-DAT

- B-DAT
erative O
procedure O
to O
minimize O
the O

- B-DAT
projection O
[51, O
11, O
8, O
46 O

- B-DAT

- B-DAT
ror O
iteratively O
[4]. O
Timofte O
et O

- B-DAT
projection O
can O
improve O
the O
quality O

- B-DAT

- B-DAT
known. O
Most O
of O
the O
previous O

- B-DAT

- B-DAT

- B-DAT
and O
down-sampling O
stages O
to O
learn O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
projection O
unit O
projects O
it O
back O

- B-DAT
serve O
the O
HR O
components O
by O

- B-DAT
and O
down- O
sampling O
operators O
and O

- B-DAT
struct O
numerous O
LR O
and O
HR O

- B-DAT

- B-DAT

- B-DAT
end O
training O
of O
the O
SR O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
spectively, O
the O
up- O
and O
down-sampling O

- B-DAT

- B-DAT

- B-DAT
and O
down-projection O
unit O
in O
the O

- B-DAT
nating O
between O
H O
and O
L O

- B-DAT
derstood O
as O
a O
self-correcting O
procedure O

- B-DAT
jection O
error O
to O
the O
sampling O

- B-DAT
sized O
filter O
is O
avoided O
because O

- B-DAT
gence O
speed O
and O
might O
produce O

- B-DAT

- B-DAT
ever, O
iterative O
utilization O
of O
our O

- B-DAT
works O

- B-DAT

- B-DAT
gradient O
problem, O
produce O
improved O
feature O

- B-DAT
age O
feature O
reuse. O
Inspired O
by O

- B-DAT

- B-DAT
mensional O
reduction O
[42, O
12] O
before O

- B-DAT

- B-DAT
and O
down-projection O
unit, O
re- O
spectively O

- B-DAT
ture O
maps O
effectively, O
as O
shown O

- B-DAT

- B-DAT
jection, O
and O
reconstruction, O
as O
described O

- B-DAT
and O
down-projection O
unit O
in O
the O

- B-DAT

- B-DAT
and O
down-projections O
units, O
respectively) O
are O

- B-DAT

- B-DAT
tures O
extraction O
and O
nR O
is O

- B-DAT

- B-DAT
traction O
is O
a O
sequence O
of O

- B-DAT

- B-DAT

- B-DAT
work O
architecture O
is O
modular. O
We O

- B-DAT
traction O
stage O
(2 O
layers), O
and O

- B-DAT

- B-DAT

- B-DAT
tion O
unit O
is O
various O
with O

- B-DAT
nally, O
the O
8× O
enlargement O
use O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
puted O
by O

- B-DAT
volutional O
layers O
are O
followed O
by O

- B-DAT
tion.2 O
To O
produce O
LR O
images O

- B-DAT
ing O
Caffe, O
MATLAB O
R2017a O
on O

- B-DAT
tion. O
The O
input O
and O
output O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
formance, O
S O
networks O
can O
achieve O

- B-DAT
SRN, O
respectively. O
The O
M O
network O

- B-DAT

- B-DAT

- B-DAT
tal, O
the O
M O
network O
use O

- B-DAT
SRN, O
and O
DRRN, O
respectively O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
largement. O
S O
(T O
= O
2 O

- B-DAT

- B-DAT
rameters O
on O
4× O
and O
8 O

- B-DAT
DBPN O
has O
about O
76% O
fewer O

- B-DAT

- B-DAT
dence O
show O
that O
our O
networks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tures O
generated O
from O
the O
projection O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tively O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pare O
our O
network O
with O
eight O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rithms: O
A+ O
[45], O
SRCNN O
[6 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tics. O
Set5, O
Set14 O
and O
BSDS100 O

- B-DAT

- B-DAT
vide O
each O
image O
in O
Urban100 O

- B-DAT

- B-DAT

- B-DAT
put O
image. O
It O
takes O
less O

- B-DAT
DBPN O
outperforms O
the O
existing O
methods O

- B-DAT

- B-DAT
vious O
statement O
is O
strengthened O
by O

- B-DAT
ban100 O
dataset O
which O
consist O
of O

- B-DAT

- B-DAT
ods O
by O
a O
large O
margin O

- B-DAT

- B-DAT

- B-DAT
ter O
than O
EDSR. O
The O
results O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
which O
predict O
the O
SR O

- B-DAT

- B-DAT
tures O
using O
multiple O
up- O
and O

- B-DAT

- B-DAT

- B-DAT
and O
down-scaling O
steps O
to O
guide O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work O
successfully O
outperforms O
other O
state-of-the-art O

- B-DAT
ods O
on O
large O
scaling O
factors O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sion O
Conference O
(BMVC), O
2012. O
6 O

- B-DAT
man O
pose O
estimation O
with O
iterative O

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT
projection O
for O
single O
image O
super O

- B-DAT
tive O
image O
models O
using O
a O

- B-DAT
tems, O
pages O
1486–1494, O
2015. O
1 O

- B-DAT

- B-DAT
resolution O
convolutional O
neural O
network. O
In O

- B-DAT
ference O
on O
Computer O
Vision, O
pages O

- B-DAT
projection O
for O
adaptive O
image O
enlargement O

- B-DAT
cessing O
(ICIP), O
2009 O
16th O
IEEE O

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
nets. O
In O
Advances O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
for O
image O
recognition. O
arXiv O

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision, O
pages O

- B-DAT
resolution O
from O
transformed O
self-exemplars. O
In O

- B-DAT
sion O
and O
Pattern O
Recognition O
(CVPR O

- B-DAT
istration. O
CVGIP: O
Graphical O
models O
and O

- B-DAT
ment: O
Resolution, O
occlusion, O
and O
transparency O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
resolution O
using O
very O
deep O
convolutional O

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT
volutional O
network O
for O
image O
super-resolution O

- B-DAT
ings O
of O
the O
IEEE O
Conference O

- B-DAT
resolution. O
In O
IEEE O
Conferene O
on O

- B-DAT
tern O
Recognition, O
2017. O
1, O
2 O

- B-DAT
sion O
offered O
by O
feedforward O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
adversarial O
network. O
In O
IEEE O

- B-DAT
tation. O
In O
Proceedings O
of O
the O

- B-DAT
resolution O
via O
deep O
draft-ensemble O
learning O

- B-DAT

- B-DAT
ing O
networks O
for O
video O
prediction O

- B-DAT
masaki, O
and O
K. O
Aizawa. O
Sketch-based O

- B-DAT
ing O
manga109 O
dataset. O
Multimedia O
Tools O

- B-DAT
sentation O
learning O
with O
deep O
convolutional O

- B-DAT
sarial O
networks. O
arXiv O
preprint O
arXiv:1511.06434 O

- B-DAT

- B-DAT
tion. O
In O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT
age O
and O
video O
super-resolution O
using O

- B-DAT

- B-DAT
back O
for O
faster O
r-cnn. O
In O

- B-DAT
tion, O
2017. O
1 O

- B-DAT

- B-DAT

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT
nition O
Workshops O
(CVPRW), O
2017 O
IEEE O

- B-DAT

- B-DAT
prove O
example-based O
single O
image O
super O

- B-DAT
ceedings O
of O
the O
IEEE O
Conference O

- B-DAT

- B-DAT
level O
vision O
tasks O
and O
3d O

- B-DAT
tural O
similarity. O
Image O
Processing, O
IEEE O

- B-DAT

- B-DAT

- B-DAT
erative O
projection O
reconstruction O
for O
fast O

2], O
Set14 O
[50], O
BSDS100 O
[1], O
Urban100 B-DAT
[16] O
and O
Manga109 O
[33]. O
Each O

BSDS100 O
consist O
of O
natural O
scenes; O
Urban100 B-DAT
contains O
urban O
scenes O
with O
details O

di- O
vide O
each O
image O
in O
Urban100 B-DAT
and O
Manga109 O
into O
four O
parts O

regular O
patterns O
from O
buildings. O
In O
Urban100, B-DAT
EDSR O
has O
0.54 O
dB O
higher O

than O
EDSR. O
While O
on O
the O
Urban100 B-DAT
dataset, O
D-DBPN O
achieves O
23.25 O
which O

Set5 O
Set14 O
BSDS100 O
Urban100 B-DAT
Manga109 O

Evaluation O
on O
BSD200 B-DAT
For O
the O
BSD O
dataset, O
300 O

BSD200 B-DAT
σ O
= O
10 O
σ O

various O
noise O
levels O
on O
Set12, O
BSD68 B-DAT
and O
Urban100. O
The O
best O
performance O

BSD68 B-DAT
15 O
31.07/0.8717 O
31.37/0.8766 O
31.42/0.8769 O
31.52 O

test O
datasets, O
i.e., O
Set12 O
[57], O
BSD68 B-DAT
[38], O
and O
Ur- O
ban100 O
[23 O

in O
terms O
of O
PSNR O
on O
BSD68 B-DAT

and O
50 O
on O
datasets O
Set14, O
BSD68 B-DAT
and O
Urban100. O
Red O
color O
indicates O

BSD68 B-DAT
15 O
31.08 O
/ O
0.8722 O
31.42 O

Image O
denoising O
results O
of O
“Test011” O
(BSD68) B-DAT
with O
noise O
level O
50 O

Image O
denoising O
results O
of O
“Test011” O
(BSD68) B-DAT
with O
noise O
level O
50 O

Image O
denoising O
results O
of O
“Test011” O
(BSD68) B-DAT
with O
noise O
level O
50 O

Image O
denoising O
results O
of O
“Test011” O
(BSD68) B-DAT
with O
noise O
level O
50 O

BSD68 B-DAT
26.35 O
/ O
0.142 O
24.32 O

0.082 O
27.74 O
/ O
0.091 O
BSD68 B-DAT
26.16 O
/ O
0.044 O
26.45 O

images O
from O
Berkeley O
segmentation O
dataset O
(BSD68) B-DAT
[12] O
and O
the O
other O
one O

use O
color O
version O
of O
the O
BSD68 B-DAT
dataset O
for O
testing O
and O
the O

OF O
DIFFERENT O
METHODS O
ON O
THE O
BSD68 B-DAT
DATASET. O
THE O
BEST O
RESULTS O
ARE O

of O
different O
methods O
on O
the O
BSD68 B-DAT
dataset O
are O
shown O
in O
Table O

are O
evaluated O
on O
the O
gray/color O
BSD68 B-DAT
dataset O

results O
of O
one O
image O
from O
BSD68 B-DAT
with O
noise O
level O
50 O

and O
TNRD O
for O
comparison. O
The O
BSD68 B-DAT
dataset O

15, O
25 O
AND O
50 O
ON O
BSD68 B-DAT
DATASET, O
SINGLE O
IMAGE O
SUPER-RESOLUTION O
WITH O

BSD68 B-DAT
25 O
28.57 O
/ O
0.8017 O
28.92 O

average O
PSNR O
of O
29.15dB O
on O
BSD68 B-DAT
dataset O
[50], O
which O
is O
much O

improve O
the O
PSNR O
results O
of O
BSD68 B-DAT
dataset O
[50] O
but O
can O
slightly O

of O
different O
methods O
on O
(gray) O
BSD68 B-DAT
dataset O

proposed O
CNN O
denoiser O
on O
(color) O
BSD68 B-DAT
dataset O

results O
of O
different O
methods O
on O
BSD68 B-DAT
dataset O
are O
shown O
in O
Table O

accuracy O
and O
efficiency O
on O
the O
BSD68 B-DAT
dataset O
with O
σ O
= O
15 O

gain O
on O
average O
for O
the O
BSD68 B-DAT
dataset), O
possibly O
because O
the O
noise O

grayscale O
image O
denoising, O
we O
use O
BSD68 B-DAT
[3] O
and O
Set12 O
datasets O
to O

for O
removing O
real O
noise. O
The O
BSD68 B-DAT
dataset O
consists O
of O
68 O
images O

on O
image O
“102061” O
from O
the O
BSD68 B-DAT
dataset O
with O
noise O
level O
50 O

color O
version O
of O
the O
grayscale O
BSD68 B-DAT
dataset. O
The O
Kodak24 O
dataset O
consists O

report O
the O
PSNR O
results O
on O
BSD68 B-DAT
and O
Set12 O
datasets, O
respectively. O
We O

comparison. O
Their O
PSNR O
results O
on O
BSD68 B-DAT
dataset O
with O
noise O
level O
50 O

range O
of O
noise O
levels O
on O
BSD68 B-DAT

We O
also O
tested O
FFDNet-Clip O
on O
BSD68 B-DAT
dataset O
with O
clipping O
setting, O
it O

RESULTS O
OF O
DIFFERENT O
METHODS O
ON O
BSD68 B-DAT

as O
15) O
are O
evaluated O
on O
BSD68 B-DAT
images O
with O
noise O
level O
ranging O

PSNR O
results O
are O
evaluated O
on O
BSD68 B-DAT

We O
use O
Ko- O
dak24 O
(http://r0k.us/graphics/kodak/), O
BSD68 B-DAT
[53], O
and O
Urban100 O
[11] O
for O

and O
FFDNet O
[20]. O
Kodak24 O
(http://r0k.us/graphics/kodak/), O
BSD68 B-DAT
[53], O
and O
Urban100 O
[11] O
are O

Method O
Kodak24 O
BSD68 B-DAT
Urban10010 O
30 O
50 O
70 O
10 O

Method O
Kodak24 O
BSD68 B-DAT
Urban10010 O
30 O
50 O
70 O
10 O

BSD68 B-DAT

BSD68 B-DAT

images O
from O
Berkeley O
segmentation O
dataset O
(BSD68) B-DAT
[12] O
and O
the O
other O
one O

use O
color O
version O
of O
the O
BSD68 B-DAT
dataset O
for O
testing O
and O
the O

OF O
DIFFERENT O
METHODS O
ON O
THE O
BSD68 B-DAT
DATASET. O
THE O
BEST O
RESULTS O
ARE O

of O
different O
methods O
on O
the O
BSD68 B-DAT
dataset O
are O
shown O
in O
Table O

are O
evaluated O
on O
the O
gray/color O
BSD68 B-DAT
dataset O

results O
of O
one O
image O
from O
BSD68 B-DAT
with O
noise O
level O
50 O

and O
TNRD O
for O
comparison. O
The O
BSD68 B-DAT
dataset O

15, O
25 O
AND O
50 O
ON O
BSD68 B-DAT
DATASET, O
SINGLE O
IMAGE O
SUPER-RESOLUTION O
WITH O

BSD68 B-DAT
25 O
28.57 O
/ O
0.8017 O
28.92 O

Urban100 B-DAT
[11] O
26.05 O
25.50 O
26.72 O
26.85 O

Method O
Scale O
Set5 O
Set14 O
B100 O
Urban100 B-DAT
Manga109PSNR O
SSIM O
PSNR O
SSIM O
PSNR O

51], O
Set14 O
[52], O
B100 O
[53], O
Urban100 B-DAT
[11], O
and O
Manga109 O
[57] O
for O

dak24 O
(http://r0k.us/graphics/kodak/), O
BSD68 O
[53], O
and O
Urban100 B-DAT
[11] O
for O
color O
and O
gray O

Urban100 B-DAT

Urban100 B-DAT

Urban100 B-DAT

Urban100 B-DAT
BD O
23.52/0.6862 O
25.70/0.7770 O
22.04/0.6745 O
26.61/0.8136 O

results O
on O
Set5, O
Set14, O
B100, O
Urban100, B-DAT
and O
Manga109 O
with O
scaling O
factor O

Kodak24 O
(http://r0k.us/graphics/kodak/), O
BSD68 O
[53], O
and O
Urban100 B-DAT
[11] O
are O
used O
for O
gray-scale O

test O
sets O
respectively. O
Gains O
on O
Urban100 B-DAT
become O
larger, O
which O
is O
mainly O

Method O
Kodak24 O
BSD68 O
Urban10010 B-DAT
30 O
50 O
70 O
10 O
30 O

Method O
Kodak24 O
BSD68 O
Urban10010 B-DAT
30 O
50 O
70 O
10 O
30 O

Urban100 B-DAT

Urban100 B-DAT

Urban100 B-DAT

PSNR O
and O
SSIM O
[43] O
on O
Urban100 B-DAT
for O
different O
architectures O
on O
gray-scale O

set O
[32], O
and O
(iii) O
the O
Urban100 B-DAT
[16] O
dataset, O
which O
contains O
images O

shows O
the O
results O
on O
the O
Urban100 B-DAT
test O
set O
(σ O
= O
25 O

Table O
2. O
PSNR O
(dB) O
on O
Urban100 B-DAT
for O
gray-scale O
image O
denoising O
for O

values O
on O
an O
image O
from O
Urban100 B-DAT
(σ O
= O
50 O

2 O
shows O
the O
results O
on O
Urban100 B-DAT
with O
σ O
∈ O
{25, O
50 O

0.79dB O
(σ O
= O
70) O
on O
Urban100 B-DAT

Urban100 B-DAT
25 O
29.97 O
29.71 O
29.92 O
29.80 O

accuracy O
of O
DnCNN O
even O
on O
Urban100, B-DAT
whereas O
our O
N3Net O
even O
fares O

NN3D O
is O
very O
effective O
on O
Urban100 B-DAT
where O
self-similarity O
can O
intuitively O
shine O

for O
an O
image O
from O
the O
Urban100 B-DAT
dataset. O
BM3D O
and O
UNLNet O
can O

Table O
13. O
PSNR O
(dB) O
on O
Urban100 B-DAT
for O
different O
architectures O
on O
gray-scale O

of O
100 O
images O
(BSD100), O
and O
Urban100 B-DAT

for O
single O
image O
super-resolution O
on O
Urban100 B-DAT
and O
BSD100. O
WSD-SR O
does O
not O

Urban100 B-DAT
×2 O
26.88 O
29.54 O
30.29 O
31.31 O

values O
on O
four O
images O
from O
Urban100 B-DAT
with O
a O
super-resolution O
factor O
of O

We O
use O
Ko- O
dak24 O
(http://r0k.us/graphics/kodak/), O
BSD68 B-DAT
[53], O
and O
Urban100 O
[11] O
for O

and O
FFDNet O
[20]. O
Kodak24 O
(http://r0k.us/graphics/kodak/), O
BSD68 B-DAT
[53], O
and O
Urban100 O
[11] O
are O

Method O
Kodak24 O
BSD68 B-DAT
Urban10010 O
30 O
50 O
70 O
10 O

Method O
Kodak24 O
BSD68 B-DAT
Urban10010 O
30 O
50 O
70 O
10 O

BSD68 B-DAT

BSD68 B-DAT

images O
from O
Berkeley O
segmentation O
dataset O
(BSD68) B-DAT
[12] O
and O
the O
other O
one O

use O
color O
version O
of O
the O
BSD68 B-DAT
dataset O
for O
testing O
and O
the O

OF O
DIFFERENT O
METHODS O
ON O
THE O
BSD68 B-DAT
DATASET. O
THE O
BEST O
RESULTS O
ARE O

of O
different O
methods O
on O
the O
BSD68 B-DAT
dataset O
are O
shown O
in O
Table O

are O
evaluated O
on O
the O
gray/color O
BSD68 B-DAT
dataset O

results O
of O
one O
image O
from O
BSD68 B-DAT
with O
noise O
level O
50 O

and O
TNRD O
for O
comparison. O
The O
BSD68 B-DAT
dataset O

15, O
25 O
AND O
50 O
ON O
BSD68 B-DAT
DATASET, O
SINGLE O
IMAGE O
SUPER-RESOLUTION O
WITH O

BSD68 B-DAT
25 O
28.57 O
/ O
0.8017 O
28.92 O

for O
σ O
= O
25 O
and O
BSD68 B-DAT
is O
part O
of O
the O
RED30 O

BSD68 B-DAT
25 O
29.23 O
28.56 O
29.03 O
28.99 O

Urban100 B-DAT
[11] O
26.05 O
25.50 O
26.72 O
26.85 O

Method O
Scale O
Set5 O
Set14 O
B100 O
Urban100 B-DAT
Manga109PSNR O
SSIM O
PSNR O
SSIM O
PSNR O

51], O
Set14 O
[52], O
B100 O
[53], O
Urban100 B-DAT
[11], O
and O
Manga109 O
[57] O
for O

dak24 O
(http://r0k.us/graphics/kodak/), O
BSD68 O
[53], O
and O
Urban100 B-DAT
[11] O
for O
color O
and O
gray O

Urban100 B-DAT

Urban100 B-DAT

Urban100 B-DAT

Urban100 B-DAT
BD O
23.52/0.6862 O
25.70/0.7770 O
22.04/0.6745 O
26.61/0.8136 O

results O
on O
Set5, O
Set14, O
B100, O
Urban100, B-DAT
and O
Manga109 O
with O
scaling O
factor O

Kodak24 O
(http://r0k.us/graphics/kodak/), O
BSD68 O
[53], O
and O
Urban100 B-DAT
[11] O
are O
used O
for O
gray-scale O

test O
sets O
respectively. O
Gains O
on O
Urban100 B-DAT
become O
larger, O
which O
is O
mainly O

Method O
Kodak24 O
BSD68 O
Urban10010 B-DAT
30 O
50 O
70 O
10 O
30 O

Method O
Kodak24 O
BSD68 O
Urban10010 B-DAT
30 O
50 O
70 O
10 O
30 O

Urban100 B-DAT

Urban100 B-DAT

Urban100 B-DAT

PSNR O
and O
SSIM O
[43] O
on O
Urban100 B-DAT
for O
different O
architectures O
on O
gray-scale O

set O
[32], O
and O
(iii) O
the O
Urban100 B-DAT
[16] O
dataset, O
which O
contains O
images O

shows O
the O
results O
on O
the O
Urban100 B-DAT
test O
set O
(σ O
= O
25 O

Table O
2. O
PSNR O
(dB) O
on O
Urban100 B-DAT
for O
gray-scale O
image O
denoising O
for O

values O
on O
an O
image O
from O
Urban100 B-DAT
(σ O
= O
50 O

2 O
shows O
the O
results O
on O
Urban100 B-DAT
with O
σ O
∈ O
{25, O
50 O

0.79dB O
(σ O
= O
70) O
on O
Urban100 B-DAT

Urban100 B-DAT
25 O
29.97 O
29.71 O
29.92 O
29.80 O

accuracy O
of O
DnCNN O
even O
on O
Urban100, B-DAT
whereas O
our O
N3Net O
even O
fares O

NN3D O
is O
very O
effective O
on O
Urban100 B-DAT
where O
self-similarity O
can O
intuitively O
shine O

for O
an O
image O
from O
the O
Urban100 B-DAT
dataset. O
BM3D O
and O
UNLNet O
can O

Table O
13. O
PSNR O
(dB) O
on O
Urban100 B-DAT
for O
different O
architectures O
on O
gray-scale O

of O
100 O
images O
(BSD100), O
and O
Urban100 B-DAT

for O
single O
image O
super-resolution O
on O
Urban100 B-DAT
and O
BSD100. O
WSD-SR O
does O
not O

Urban100 B-DAT
×2 O
26.88 O
29.54 O
30.29 O
31.31 O

values O
on O
four O
images O
from O
Urban100 B-DAT
with O
a O
super-resolution O
factor O
of O

Set5 O
and O
Set14, O
BSD100 O
and O
Urban100 B-DAT
[40]) O
used O
in O
[35 O

Urban100 B-DAT
3 O
26.42 O
/ O
0.8076 O
27.13 O

results O
of O
one O
image O
from O
Urban100 B-DAT
dataset O
with O
upscaling O
factor O
4 O

Evaluation O
on O
BSD200 B-DAT
For O
the O
BSD O
dataset, O
300 O

BSD200 B-DAT
σ O
= O
10 O
σ O

various O
noise O
levels O
on O
Set12, O
BSD68 B-DAT
and O
Urban100. O
The O
best O
performance O

BSD68 B-DAT
15 O
31.07/0.8717 O
31.37/0.8766 O
31.42/0.8769 O
31.52 O

test O
datasets, O
i.e., O
Set12 O
[57], O
BSD68 B-DAT
[38], O
and O
Ur- O
ban100 O
[23 O

in O
terms O
of O
PSNR O
on O
BSD68 B-DAT

and O
50 O
on O
datasets O
Set14, O
BSD68 B-DAT
and O
Urban100. O
Red O
color O
indicates O

BSD68 B-DAT
15 O
31.08 O
/ O
0.8722 O
31.42 O

Image O
denoising O
results O
of O
“Test011” O
(BSD68) B-DAT
with O
noise O
level O
50 O

Image O
denoising O
results O
of O
“Test011” O
(BSD68) B-DAT
with O
noise O
level O
50 O

Image O
denoising O
results O
of O
“Test011” O
(BSD68) B-DAT
with O
noise O
level O
50 O

Image O
denoising O
results O
of O
“Test011” O
(BSD68) B-DAT
with O
noise O
level O
50 O

BSD68 B-DAT
26.35 O
/ O
0.142 O
24.32 O

0.082 O
27.74 O
/ O
0.091 O
BSD68 B-DAT
26.16 O
/ O
0.044 O
26.45 O

for O
σ O
= O
25 O
and O
BSD68 B-DAT
is O
part O
of O
the O
RED30 O

BSD68 B-DAT
25 O
29.23 O
28.56 O
29.03 O
28.99 O

images O
from O
Berkeley O
segmentation O
dataset O
(BSD68) B-DAT
[12] O
and O
the O
other O
one O

use O
color O
version O
of O
the O
BSD68 B-DAT
dataset O
for O
testing O
and O
the O

OF O
DIFFERENT O
METHODS O
ON O
THE O
BSD68 B-DAT
DATASET. O
THE O
BEST O
RESULTS O
ARE O

of O
different O
methods O
on O
the O
BSD68 B-DAT
dataset O
are O
shown O
in O
Table O

are O
evaluated O
on O
the O
gray/color O
BSD68 B-DAT
dataset O

results O
of O
one O
image O
from O
BSD68 B-DAT
with O
noise O
level O
50 O

and O
TNRD O
for O
comparison. O
The O
BSD68 B-DAT
dataset O

15, O
25 O
AND O
50 O
ON O
BSD68 B-DAT
DATASET, O
SINGLE O
IMAGE O
SUPER-RESOLUTION O
WITH O

BSD68 B-DAT
25 O
28.57 O
/ O
0.8017 O
28.92 O

accuracy O
and O
efficiency O
on O
the O
BSD68 B-DAT
dataset O
with O
σ O
= O
15 O

gain O
on O
average O
for O
the O
BSD68 B-DAT
dataset), O
possibly O
because O
the O
noise O

grayscale O
image O
denoising, O
we O
use O
BSD68 B-DAT
[3] O
and O
Set12 O
datasets O
to O

for O
removing O
real O
noise. O
The O
BSD68 B-DAT
dataset O
consists O
of O
68 O
images O

on O
image O
“102061” O
from O
the O
BSD68 B-DAT
dataset O
with O
noise O
level O
50 O

color O
version O
of O
the O
grayscale O
BSD68 B-DAT
dataset. O
The O
Kodak24 O
dataset O
consists O

report O
the O
PSNR O
results O
on O
BSD68 B-DAT
and O
Set12 O
datasets, O
respectively. O
We O

comparison. O
Their O
PSNR O
results O
on O
BSD68 B-DAT
dataset O
with O
noise O
level O
50 O

range O
of O
noise O
levels O
on O
BSD68 B-DAT

We O
also O
tested O
FFDNet-Clip O
on O
BSD68 B-DAT
dataset O
with O
clipping O
setting, O
it O

RESULTS O
OF O
DIFFERENT O
METHODS O
ON O
BSD68 B-DAT

as O
15) O
are O
evaluated O
on O
BSD68 B-DAT
images O
with O
noise O
level O
ranging O

PSNR O
results O
are O
evaluated O
on O
BSD68 B-DAT

average O
PSNR O
of O
29.15dB O
on O
BSD68 B-DAT
dataset O
[50], O
which O
is O
much O

improve O
the O
PSNR O
results O
of O
BSD68 B-DAT
dataset O
[50] O
but O
can O
slightly O

of O
different O
methods O
on O
(gray) O
BSD68 B-DAT
dataset O

proposed O
CNN O
denoiser O
on O
(color) O
BSD68 B-DAT
dataset O

results O
of O
different O
methods O
on O
BSD68 B-DAT
dataset O
are O
shown O
in O
Table O

Evaluation O
on O
BSD200 B-DAT
For O
the O
BSD O
dataset, O
300 O

BSD200 B-DAT
σ O
= O
10 O
σ O

the O
layer O
(“pool5”) O
before O
the O
1000 B-DAT

58.80 O
4096 O
59.42 O
8192 O
59.69 O
16000 B-DAT
59.83 O
32000 O
59.71 O

in O
MCB O
with O
d O
= O
16000 B-DAT

1604 B-DAT

1602 B-DAT

to O
Visual O
Question O
Answering. O
arXiv: O
1605 B-DAT

1505 B-DAT

1503 B-DAT

The O
Visual O
Question O
Answering O
(VQA) O
real B-DAT

experiments O
on O
the O
open- O
ended O
real B-DAT

we O
also O
report O
our O
multiple-choice O
real B-DAT

the O
Visual7W O
dataset O
and O
the O
VQA B-DAT
challenge O

such O
as O
visual O
question O
answering O
(VQA) B-DAT
and O
visual O
ground- O
ing, O
most O

multimodal O
pooling), O
current O
approaches O
in O
VQA B-DAT
or O
grounding O
rely O
on O
concatenat O

predict O
an- O
swers O
for O
the O
VQA B-DAT
task O
and O
locations O
for O
the O

we O
present O
an O
architecture O
for O
VQA B-DAT
which O
uses O
MCB O
twice, O
once O

additional O
training O
data O
for O
the O
VQA B-DAT
task. O
To O
sum- O
marize, O
MCB O

task O
of O
visual O
question O
answering O
(VQA) B-DAT
or O
visual O
grounding, O
we O
have O

then O
detail O
our O
architectures O
for O
VQA B-DAT
(Sec. O
3.2) O
and O
visual O
grounding O

and O
z O
∈ O
R3000 O
for O
VQA B-DAT

Figure O
3: O
Our O
architecture O
for O
VQA B-DAT

3.2 O
Architectures O
for O
VQA B-DAT

In O
VQA, B-DAT
the O
input O
to O
the O
model O

Yang O
et O
al., O
2015) O
for O
VQA, B-DAT
the O
soft O
attention O
mechanism O
can O

Answer O
Encoding. O
For O
VQA B-DAT
with O
multiple O
choices, O
we O
can O

Figure O
4: O
Our O
architecture O
for O
VQA B-DAT

The O
Visual O
Question O
Answering O
(VQA) B-DAT
real-image O
dataset O
(Antol O
et O
al O

data O
split. O
We O
use O
the O
VQA B-DAT
tool O
provided O
by O
Antol O
et O

and O
how). O
Compared O
to O
the O
VQA B-DAT
dataset, O
Vi- O
sual O
Genome O
represents O

Genome O
are O
larger O
than O
the O
VQA B-DAT
dataset. O
To O
leverage O
the O
Visual O

vocabulary O
space O
created O
from O
the O
VQA B-DAT
dataset, O
leaving O
us O
with O
addi O

Models O
are O
trained O
on O
the O
VQA B-DAT
train O
split O
and O
tested O
on O

2, O
we O
train O
on O
the O
VQA B-DAT
train O
split, O
vali- O
date O
on O

the O
VQA B-DAT
validation O
split, O
and O
report O
results O

on O
the O
VQA B-DAT
test-dev O
split. O
We O
use O
early O

training O
settings O
as O
in O
the O
VQA B-DAT
exper- O
iments. O
We O
use O
the O

For O
VQA B-DAT
multiple O
choice, O
we O
train O
the O

Models O
are O
trained O
on O
the O
VQA B-DAT
train O
split O
and O
tested O
on O

80.9 O
37.5 O
43.5 O
58.2 O
- O
VQA B-DAT
team O
(Antol O
et O
al., O
2015 O

and O
multiple-choice O
(MC) O
results O
on O
VQA B-DAT
test O
set O
(trained O
on O
train+val O

with O
the O
state-of-the- O
art O
on O
VQA B-DAT
test O
set. O
Our O
best O
single O

next O
best O
approach O
on O
the O
VQA B-DAT
open-ended O
task O
and O
0.8 O
points O

maps O
from O
MCB O
model O
on O
VQA B-DAT
images. O
Bottom: O
predicted O
grounding O
from O

gives O
significant O
improvements O
on O
two O
VQA B-DAT
datasets O
compared O
to O
state-of-the-art. O
In O

3.2 O
Architectures O
for O
VQA B-DAT

ended B-DAT
question O
answering, O
we O
present O
an O

our O
experiments O
on O
the O
open- O
ended B-DAT
real-image O
task. O
In O
Table O
4 O

ended B-DAT
models O
and O
take O
the O
argmax O

ended B-DAT
and O
multiple-choice O
(MC) O
results O
on O

ended B-DAT
task O
and O
0.8 O
points O
above O

ended B-DAT
task O
(on O
Test-dev O

the O
visual O
grounding O
task. O
For O
open B-DAT

of O
our O
experiments O
on O
the O
open B-DAT

multiple O
choice, O
we O
train O
the O
open B-DAT

best O
approach O
on O
the O
VQA O
open B-DAT

65.4% O
versus O
64.9% O
on O
the O
open B-DAT

Representation O
learning O
for O
text O
and O
images B-DAT
has O
been O
extensively O
studied O
in O

to O
work O
best O
to O
represent O
images B-DAT
(Don- O
ahue O
et O
al., O
2013 O

consists O
of O
approximately O
200,000 O
MSCOCO O
images B-DAT
(Lin O
et O
al., O
2014), O
with O

3 O
data O
splits: O
train O
(80K O
images), B-DAT
validation O
(40K O
images), O
and O
test O

80K O
images B-DAT

et O
al., O
2016) O
uses O
108,249 O
images B-DAT
from O
the O
intersection O
of O
YFCC100M O

Visual7W O
is O
composed O
of O
47,300 O
images B-DAT
from O
MSCOCO O
and O
there O
are O

augment O
our O
training O
data O
with O
images B-DAT
and O
QA O
pairs O
from O
the O

2015) O
which O
consists O
of O
31K O
images B-DAT
from O
Flickr30k O
dataset O
(Hodosh O
et O

al., O
2014), O
which O
contains O
20K O
images B-DAT
from O
IAPR O
TC-12 O
dataset O
(Grubinger O

from O
MCB O
model O
on O
VQA O
images B-DAT

model O
(right) O
on O
Flickr30k O
Entities O
images B-DAT

Grounding O
of O
textual O
phrases O
in O
images B-DAT
by O
reconstruc- O
tion. O
In O
Proceedings O

semantics O
for O
finding O
and O
describing O
images B-DAT
with O
sentences. O
Transactions O
of O
the O

Compact O
Bilinear O
Pooling O
for O
Visual O
Question B-DAT
Answering O
and O
Visual O
Grounding O

4 O
Evaluation O
on O
Visual O
Question B-DAT
Answering O

The O
Visual O
Question B-DAT
Answering O
(VQA) O
real-image O
dataset O
(Antol O

Hierarchical O
Co-Attention O
for O
Vi- O
sual O
Question B-DAT
Answering. O
In O
Advances O
in O
Neural O

Deep O
Learning O
Approach O
to O
Visual O
Question B-DAT
Answering. O
arXiv: O
1605.02697 O

Ask O
Me O
Anything: O
Free-form O
Visual O
Question B-DAT
Answering O
Based O
on O
Knowledge O
from O

Li O
Fei-Fei. O
2016. O
Visual7W: O
Grounded O
Question B-DAT
Answering O
in O
Images. O
In O
Proceedings O

4 O
Evaluation O
on O
Visual O
Question B-DAT
Answering O

Multimodal O
Compact O
Bilinear O
Pooling O
for O
Visual B-DAT
Question O
Answering O
and O
Visual O
Grounding O

Visual B-DAT
Vector O

Count O
Sketch O
of O
Visual B-DAT
Vector O

Multimodal O
Compact O
Bilinear O
Pooling O
for O
Visual B-DAT
and O
Textual O
Embeddings O

3.3 O
Architecture O
for O
Visual B-DAT
Grounding O

4 O
Evaluation O
on O
Visual B-DAT
Question O
Answering O

The O
Visual B-DAT
Question O
Answering O
(VQA) O
real-image O
dataset O

The O
Visual B-DAT
Genome O
dataset O
(Krishna O
et O
al O

question O
and O
answer O
lengths O
for O
Visual B-DAT
Genome O
are O
larger O
than O
the O

VQA O
dataset. O
To O
leverage O
the O
Visual B-DAT
Genome O
dataset O
as O
additional O
training O

is O
a O
part O
of O
the O
Visual B-DAT
Genome. O
Visual7W O
adds O
a O
7th O

64.24% O
accuracy O
(trained O
on O
train+val). O
Visual B-DAT
inspection O
of O
the O
gen O

and O
QA O
pairs O
from O
the O
Visual B-DAT
Genome O
dataset. O
We O
also O
con O

were O
trained O
with O
data O
from O
Visual B-DAT
Genome, O
and O
some O
were O
trained O

5 O
Evaluation O
on O
Visual B-DAT
Grounding O

and O
Devi O
Parikh. O
2015. O
Vqa: O
Visual B-DAT
question O
answering. O
In O
Proceedings O
of O

Bernstein, O
and O
Li O
Fei-Fei. O
2016. O
Visual B-DAT
genome: O
Connecting O
language O
and O
vi O

A O
Deep O
Learning O
Approach O
to O
Visual B-DAT
Question O
Answering. O
arXiv: O
1605.02697 O

2016. O
Ask O
Me O
Anything: O
Free-form O
Visual B-DAT
Question O
Answering O
Based O
on O
Knowledge O

Multimodal O
Compact O
Bilinear O
Pooling O
for O
Visual B-DAT
and O
Textual O
Embeddings O

3.3 O
Architecture O
for O
Visual B-DAT
Grounding O

4 O
Evaluation O
on O
Visual B-DAT
Question O
Answering O

5 O
Evaluation O
on O
Visual B-DAT
Grounding O

Bilinear O
Pooling O
for O
Visual O
Question O
Answering B-DAT
and O
Visual O
Grounding O

4 O
Evaluation O
on O
Visual O
Question O
Answering B-DAT

The O
Visual O
Question O
Answering B-DAT
(VQA) O
real-image O
dataset O
(Antol O
et O

Co-Attention O
for O
Vi- O
sual O
Question O
Answering B-DAT

Learning O
Approach O
to O
Visual O
Question O
Answering B-DAT

Me O
Anything: O
Free-form O
Visual O
Question O
Answering B-DAT
Based O
on O
Knowledge O
from O
External O

Fei-Fei. O
2016. O
Visual7W: O
Grounded O
Question O
Answering B-DAT
in O
Images. O
In O
Proceedings O
of O

4 O
Evaluation O
on O
Visual O
Question O
Answering B-DAT

the O
Visual7W O
dataset O
and O
the O
VQA B-DAT
challenge O

such O
as O
visual O
question O
answering O
(VQA) B-DAT
and O
visual O
ground- O
ing, O
most O

multimodal O
pooling), O
current O
approaches O
in O
VQA B-DAT
or O
grounding O
rely O
on O
concatenat O

predict O
an- O
swers O
for O
the O
VQA B-DAT
task O
and O
locations O
for O
the O

we O
present O
an O
architecture O
for O
VQA B-DAT
which O
uses O
MCB O
twice, O
once O

additional O
training O
data O
for O
the O
VQA B-DAT
task. O
To O
sum- O
marize, O
MCB O

task O
of O
visual O
question O
answering O
(VQA) B-DAT
or O
visual O
grounding, O
we O
have O

then O
detail O
our O
architectures O
for O
VQA B-DAT
(Sec. O
3.2) O
and O
visual O
grounding O

and O
z O
∈ O
R3000 O
for O
VQA B-DAT

Figure O
3: O
Our O
architecture O
for O
VQA B-DAT

3.2 O
Architectures O
for O
VQA B-DAT

In O
VQA, B-DAT
the O
input O
to O
the O
model O

Yang O
et O
al., O
2015) O
for O
VQA, B-DAT
the O
soft O
attention O
mechanism O
can O

Answer O
Encoding. O
For O
VQA B-DAT
with O
multiple O
choices, O
we O
can O

Figure O
4: O
Our O
architecture O
for O
VQA B-DAT

The O
Visual O
Question O
Answering O
(VQA) B-DAT
real-image O
dataset O
(Antol O
et O
al O

data O
split. O
We O
use O
the O
VQA B-DAT
tool O
provided O
by O
Antol O
et O

and O
how). O
Compared O
to O
the O
VQA B-DAT
dataset, O
Vi- O
sual O
Genome O
represents O

Genome O
are O
larger O
than O
the O
VQA B-DAT
dataset. O
To O
leverage O
the O
Visual O

vocabulary O
space O
created O
from O
the O
VQA B-DAT
dataset, O
leaving O
us O
with O
addi O

Models O
are O
trained O
on O
the O
VQA B-DAT
train O
split O
and O
tested O
on O

2, O
we O
train O
on O
the O
VQA B-DAT
train O
split, O
vali- O
date O
on O

the O
VQA B-DAT
validation O
split, O
and O
report O
results O

on O
the O
VQA B-DAT
test-dev O
split. O
We O
use O
early O

training O
settings O
as O
in O
the O
VQA B-DAT
exper- O
iments. O
We O
use O
the O

For O
VQA B-DAT
multiple O
choice, O
we O
train O
the O

Models O
are O
trained O
on O
the O
VQA B-DAT
train O
split O
and O
tested O
on O

80.9 O
37.5 O
43.5 O
58.2 O
- O
VQA B-DAT
team O
(Antol O
et O
al., O
2015 O

and O
multiple-choice O
(MC) O
results O
on O
VQA B-DAT
test O
set O
(trained O
on O
train+val O

with O
the O
state-of-the- O
art O
on O
VQA B-DAT
test O
set. O
Our O
best O
single O

next O
best O
approach O
on O
the O
VQA B-DAT
open-ended O
task O
and O
0.8 O
points O

maps O
from O
MCB O
model O
on O
VQA B-DAT
images. O
Bottom: O
predicted O
grounding O
from O

gives O
significant O
improvements O
on O
two O
VQA B-DAT
datasets O
compared O
to O
state-of-the-art. O
In O

3.2 O
Architectures O
for O
VQA B-DAT

behavioral O
cloning O
from O
expert O
100 B-DAT

cloning O
from O
expert”) O
already O
achieves O
100 B-DAT

proposed O
CLEVR O
dataset O
[15] O
with O
100, B-DAT

1603 B-DAT

1802 B-DAT

answering. O
(The O
example O
shows O
a O
real B-DAT
structure O
predicted O
by O
our O
model O

the O
VQA O
dataset O
[4] O
with O
real B-DAT
images. O
On O
the O
VQA O
dataset O

to O
ques- O
tion O
answering O
about O
real B-DAT

Visual O
Question O
Answering O
(VQA) B-DAT
requires O
joint O
com- O
prehension O
of O

deep O
networks O
have O
shown O
promising O
VQA B-DAT
perfor- O
mance O
[9], O
there O
is O

success O
of O
state-of-the-art O
approaches O
to O
VQA B-DAT
instead O
comes O
from O
their O
ability O

results O
published O
with O
the O
first O
VQA B-DAT
datasets O

great- O
est O
challenges O
to O
standard O
VQA B-DAT
approaches O
and O
the O
hardest O
reasoning O

large-scale O
datasets: O
CLEVR O
[15] O
and O
VQA B-DAT
[4 O

from O
our O
model O
on O
the O
VQA B-DAT
dataset O

5. O
Evaluation O
on O
the O
VQA B-DAT
dataset O

evaluate O
our O
method O
on O
the O
VQA B-DAT
dataset O
[4] O
with O
real O
images O

. O
On O
the O
VQA B-DAT
dataset, O
although O
there O
are O
no O

in O
Table O
4 O
on O
the O
VQA B-DAT
dataset, O
where O
our O
method O
significantly O

Compared O
with O
MCB O
[9] O
(the O
VQA B-DAT
2016 O
challenge O
winner O
method) O
trained O

of O
our O
method O
on O
the O
VQA B-DAT
test-dev O
set. O
Our O
model O
outperforms O

Parikh. O
Making O
the O
V O
in O
VQA B-DAT
matter: O
Elevating O
the O
role O
of O

requires O
joint O
com- O
prehension O
of O
images B-DAT
and O
text. O
This O
comprehension O
often O

input O
and O
applying O
them O
to O
images B-DAT
in O
order O
to O
solve O
question O

referring O
expressions O
in O
images B-DAT
does O
not O
need O
a O
parser O

CLEVR O
dataset O
[15] O
with O
100,000 O
images B-DAT
and O
853,554 O
questions. O
The O
images O

dataset O
are O
photo- O
realistic O
rendered O
images B-DAT
with O
objects O
of O
different O
shapes O

VQA O
dataset O
[4] O
with O
real O
images B-DAT

End-to-End O
Module O
Networks O
for O
Visual O
Question B-DAT
Answering O

Visual O
Question B-DAT
Answering O
(VQA) O
requires O
joint O
com O

Question B-DAT
attentions O

Question B-DAT
features O

Question B-DAT

Figure O
5: O
Question B-DAT
answering O
examples O
on O
the O
CLEVR O

im- O
age O
understanding O
in O
Visual O
Question B-DAT
Answering. O
CoRR, O
abs/1612.00837, O
2016. O
1 O

Hierarchical O
Co- O
Attention O
for O
Visual O
Question B-DAT
Answering. O
In O
Advances O
in O
Neural O

Reason: O
End-to-End O
Module O
Networks O
for O
Visual B-DAT
Question O
Answering O

Visual B-DAT
Question O
Answering O
(VQA) O
requires O
joint O

to O
each O
individual O
input O
example. O
Visual B-DAT
question O
answering. O
The O
visual O
question O

Method O
Visual B-DAT
feature O
Accuracy O
NMN O
[3] O
LRCN O

Zitnick, O
and O
D. O
Parikh. O
Vqa: O
Visual B-DAT
question O
answering. O
In O
Proceedings O
of O

of O
im- O
age O
understanding O
in O
Visual B-DAT
Question O
Answering. O
CoRR, O
abs/1612.00837, O
2016 O

Parikh. O
Hierarchical O
Co- O
Attention O
for O
Visual B-DAT
Question O
Answering. O
In O
Advances O
in O

Module O
Networks O
for O
Visual O
Question O
Answering B-DAT

Visual O
Question O
Answering B-DAT
(VQA) O
requires O
joint O
com- O
prehension O

age O
understanding O
in O
Visual O
Question O
Answering B-DAT

Co- O
Attention O
for O
Visual O
Question O
Answering B-DAT

Visual O
Question O
Answering O
(VQA) B-DAT
requires O
joint O
com- O
prehension O
of O

deep O
networks O
have O
shown O
promising O
VQA B-DAT
perfor- O
mance O
[9], O
there O
is O

success O
of O
state-of-the-art O
approaches O
to O
VQA B-DAT
instead O
comes O
from O
their O
ability O

results O
published O
with O
the O
first O
VQA B-DAT
datasets O

great- O
est O
challenges O
to O
standard O
VQA B-DAT
approaches O
and O
the O
hardest O
reasoning O

large-scale O
datasets: O
CLEVR O
[15] O
and O
VQA B-DAT
[4 O

from O
our O
model O
on O
the O
VQA B-DAT
dataset O

5. O
Evaluation O
on O
the O
VQA B-DAT
dataset O

evaluate O
our O
method O
on O
the O
VQA B-DAT
dataset O
[4] O
with O
real O
images O

. O
On O
the O
VQA B-DAT
dataset, O
although O
there O
are O
no O

in O
Table O
4 O
on O
the O
VQA B-DAT
dataset, O
where O
our O
method O
significantly O

Compared O
with O
MCB O
[9] O
(the O
VQA B-DAT
2016 O
challenge O
winner O
method) O
trained O

of O
our O
method O
on O
the O
VQA B-DAT
test-dev O
set. O
Our O
model O
outperforms O

Parikh. O
Making O
the O
V O
in O
VQA B-DAT
matter: O
Elevating O
the O
role O
of O

1606 B-DAT

1606 B-DAT

1606 B-DAT

256, O
and O
C O
= O
1000 B-DAT

approach O
to O
question O
answering O
about O
real B-DAT

with O
Joint O
Loss O
Minimization O
for O
VQA B-DAT

a O
single O
step O
prediction O
in O
VQA B-DAT
dataset O

Introduction O
Visual O
Question O
Answering O
(VQA) B-DAT
(Antol O
et O
al. O
2015) O
is O

scene O
classification, O
activity O
recognition, O
etc., O
VQA B-DAT
involves O
various O
recognition O
tasks O
at O

questions. O
Due O
to O
these O
reasons, O
VQA B-DAT
requires O
substantial O
amount O
of O
learning O

to O
improve O
the O
performance O
of O
VQA B-DAT
systems. O
After O
extract O

natural O
as O
many O
questions O
in O
VQA B-DAT
conceptually O
require O
multiple O
steps O
for O

front O
of O
the O
giraffe?” O
the O
VQA B-DAT
model O
should O
be O
able O
to O

deep O
recurrent O
neural O
network O
for O
VQA, B-DAT
which O
is O
composed O
of O
multiple O

We O
show O
that O
the O
VQA B-DAT
model O
involving O
multiple O
reason- O
ing O

a O
single O
step O
prediction O
in O
VQA B-DAT
dataset O

Related O
Work O
The O
VQA B-DAT
problem O
is O
first O
addressed O
by O

a O
shallow O
neural O
network O
for O
VQA, B-DAT
which O
accepts O
the O
CNN O
features O

questions O
as O
its O
inputs. O
Recently, O
VQA B-DAT
algorithms O
are O
often O
formulated O
with O

accuracy. O
These O
approaches O
typically O
pose O
VQA B-DAT
problems O
as O
simple O
classification O
tasks O

Several O
VQA B-DAT
systems O
(Yang O
et O
al. O
2016 O

to O
an O
object O
proposal O
for O
VQA B-DAT
while O
(Xiong, O
Merity, O
and O
Socher O

describes O
the O
general O
formulation O
of O
VQA B-DAT
and O
discusses O
our O
approach O
based O

Problem O
Formulation O
We O
formulate O
VQA B-DAT
problem O
as O
a O
classification O
task O

and O
a O
question O
q, O
a O
VQA B-DAT
model O
predicts O
the O
best O
answer O

which O
are O
frequently O
employed O
for O
VQA B-DAT
problems O
in O
recent O
years O

novel O
neural O
network O
architecture O
for O
VQA B-DAT
and O
propose O
a O
training O
strategy O

the O
proposed O
method O
resembles O
the O
VQA B-DAT
methods O
such O
as O
(Yang O
et O

based O
on O
our O
observations O
in O
VQA B-DAT
problems, O
but O
it O
is O
contradictory O

test O
the O
proposed O
network O
in O
VQA B-DAT
dataset O
(An- O
tol O
et O
al O

Two O
tasks O
are O
defined O
on O
VQA B-DAT
dataset: O
open-ended O
task O
and O
multple-choice O

Single O
model O
performance O
on O
the O
VQA B-DAT
test-dev O
dataset O
of O
all O
compared O

single O
model O
performance O
in O
the O
VQA B-DAT
test-standard. O
Asterisk O
(*) O
denotes O
the O

et O
al. O
2016) O
in O
the O
VQA B-DAT
dataset O
using O
the O
same O
image O

2.3, O
which O
are O
significant O
in O
VQA B-DAT
con- O
text. O
Both O
training O
schemes O

improve O
the O
accuracy O
of O
a O
VQA B-DAT
system O
by O
using O
a O
better O

1, O
and O
evaluated O
both O
in O
VQA B-DAT
test-dev O
and O
test-standard. O
The O
results O

Conclusion O
We O
proposed O
a O
VQA B-DAT
algorithm O
based O
on O
a O
recurrent O

outstanding O
performance O
in O
the O
standard O
VQA B-DAT
dataset O
with- O
out O
data O
augmentation O

as O
a O
general O
framework O
for O
VQA B-DAT
problems O
by O
replacing O
our O
answering O

L.; O
and O
Parikh, O
D. O
2015. O
VQA B-DAT

with O
Joint O
Loss O
Minimization O
for O
VQA B-DAT
Supplementary O
Document O

Full O
and O
Ours O
SS O
on O
VQA B-DAT
validation O
dataset O

ended B-DAT
task O
and O
multple-choice O
task. O
The O

ended B-DAT
question O
without O
knowing O
predefined O
can O

are O
defined O
on O
VQA O
dataset: O
open B-DAT

predict O
an O
answer O
for O
an O
open B-DAT

learning O
to O
capture O
information O
from O
images B-DAT
and O
understand O
questions O

accepts O
the O
CNN O
features O
of O
images B-DAT
and O
the O
Bag- O
of-Words O
(BoW O

on O
the O
joint O
features O
from O
images B-DAT
and O
questions, O
where O
CNNs O
and O

1997) O
are O
employed O
to O
encode O
images B-DAT
and O
questions, O
respectively O
(Ren, O
Kiros O

various O
tasks O
related O
to O
input O
images B-DAT

et O
al. O
2015), O
which O
borrows O
images B-DAT
from O
MSCOCO O
dataset O
(Lin O
et O

encoders. O
After O
rescal- O
ing O
input O
images B-DAT
to O
448× O
448, O
we O
extract O

to O
the O
features O
for O
input O
images B-DAT
and O
questions O
in O
each O
answering O

approach O
to O
answering O
questions O
about O
images B-DAT

Introduction O
Visual O
Question B-DAT
Answering O
(VQA) O
(Antol O
et O
al O

Introduction O
Visual B-DAT
Question O
Answering O
(VQA) O
(Antol O
et O

Zitnick, O
C. O
L. O
2014. O
Microsoft O
COCO B-DAT

Training O
Recurrent O
Answering B-DAT
Units O
with O
Joint O
Loss O
Minimization O

Introduction O
Visual O
Question O
Answering B-DAT
(VQA) O
(Antol O
et O
al. O
2015 O

question O
sentence O
q, O
respec- O
tively. O
Answering B-DAT
module O
takes O
extracted O
image O
and O

Answering B-DAT
Module O
Answering O
module O
is O
a O
recurrent O
neural O

To O
implement O
the O
Answering B-DAT
module, O
we O
need O
to O
learn O

Training O
Recurrent O
Answering B-DAT
Units O
with O
Joint O
Loss O
Minimization O

Answering B-DAT
Module O

with O
Joint O
Loss O
Minimization O
for O
VQA B-DAT

a O
single O
step O
prediction O
in O
VQA B-DAT
dataset O

Introduction O
Visual O
Question O
Answering O
(VQA) B-DAT
(Antol O
et O
al. O
2015) O
is O

scene O
classification, O
activity O
recognition, O
etc., O
VQA B-DAT
involves O
various O
recognition O
tasks O
at O

questions. O
Due O
to O
these O
reasons, O
VQA B-DAT
requires O
substantial O
amount O
of O
learning O

to O
improve O
the O
performance O
of O
VQA B-DAT
systems. O
After O
extract O

natural O
as O
many O
questions O
in O
VQA B-DAT
conceptually O
require O
multiple O
steps O
for O

front O
of O
the O
giraffe?” O
the O
VQA B-DAT
model O
should O
be O
able O
to O

deep O
recurrent O
neural O
network O
for O
VQA, B-DAT
which O
is O
composed O
of O
multiple O

We O
show O
that O
the O
VQA B-DAT
model O
involving O
multiple O
reason- O
ing O

a O
single O
step O
prediction O
in O
VQA B-DAT
dataset O

Related O
Work O
The O
VQA B-DAT
problem O
is O
first O
addressed O
by O

a O
shallow O
neural O
network O
for O
VQA, B-DAT
which O
accepts O
the O
CNN O
features O

questions O
as O
its O
inputs. O
Recently, O
VQA B-DAT
algorithms O
are O
often O
formulated O
with O

accuracy. O
These O
approaches O
typically O
pose O
VQA B-DAT
problems O
as O
simple O
classification O
tasks O

Several O
VQA B-DAT
systems O
(Yang O
et O
al. O
2016 O

to O
an O
object O
proposal O
for O
VQA B-DAT
while O
(Xiong, O
Merity, O
and O
Socher O

describes O
the O
general O
formulation O
of O
VQA B-DAT
and O
discusses O
our O
approach O
based O

Problem O
Formulation O
We O
formulate O
VQA B-DAT
problem O
as O
a O
classification O
task O

and O
a O
question O
q, O
a O
VQA B-DAT
model O
predicts O
the O
best O
answer O

which O
are O
frequently O
employed O
for O
VQA B-DAT
problems O
in O
recent O
years O

novel O
neural O
network O
architecture O
for O
VQA B-DAT
and O
propose O
a O
training O
strategy O

the O
proposed O
method O
resembles O
the O
VQA B-DAT
methods O
such O
as O
(Yang O
et O

based O
on O
our O
observations O
in O
VQA B-DAT
problems, O
but O
it O
is O
contradictory O

test O
the O
proposed O
network O
in O
VQA B-DAT
dataset O
(An- O
tol O
et O
al O

Two O
tasks O
are O
defined O
on O
VQA B-DAT
dataset: O
open-ended O
task O
and O
multple-choice O

Single O
model O
performance O
on O
the O
VQA B-DAT
test-dev O
dataset O
of O
all O
compared O

single O
model O
performance O
in O
the O
VQA B-DAT
test-standard. O
Asterisk O
(*) O
denotes O
the O

et O
al. O
2016) O
in O
the O
VQA B-DAT
dataset O
using O
the O
same O
image O

2.3, O
which O
are O
significant O
in O
VQA B-DAT
con- O
text. O
Both O
training O
schemes O

improve O
the O
accuracy O
of O
a O
VQA B-DAT
system O
by O
using O
a O
better O

1, O
and O
evaluated O
both O
in O
VQA B-DAT
test-dev O
and O
test-standard. O
The O
results O

Conclusion O
We O
proposed O
a O
VQA B-DAT
algorithm O
based O
on O
a O
recurrent O

outstanding O
performance O
in O
the O
standard O
VQA B-DAT
dataset O
with- O
out O
data O
augmentation O

as O
a O
general O
framework O
for O
VQA B-DAT
problems O
by O
replacing O
our O
answering O

L.; O
and O
Parikh, O
D. O
2015. O
VQA B-DAT

with O
Joint O
Loss O
Minimization O
for O
VQA B-DAT
Supplementary O
Document O

Full O
and O
Ours O
SS O
on O
VQA B-DAT
validation O
dataset O

answers. O
We O
use O
the O
top O
1000 B-DAT
most O
frequent O
answers O
as O
the O

1-0679 B-DAT
to O
DB, O
a O
Sloan O
Fellowship O

1606 B-DAT

1606 B-DAT

1604 B-DAT

1606 B-DAT

1602 B-DAT

1602 B-DAT

1409 B-DAT

models O
for O
Visual O
Question O
Answering O
(VQA) B-DAT
that O
generate O
spatial O
maps O
highlighting O

a O
novel O
co-attention O
model O
for O
VQA B-DAT
that O
jointly O
reasons O
about O
image O

improves O
the O
state-of-the-art O
on O
the O
VQA B-DAT
dataset O
from O
60.3% O
to O
60.5 O

further O
improved O
to O
62.1% O
for O
VQA B-DAT
and O
65.4% O
for O
COCO-QA.1 O

1 O
Introduction O
Visual O
Question O
Answering O
(VQA) B-DAT
[2, O
7, O
16, O
17, O
29 O

23–25] O
have O
been O
explored O
for O
VQA, B-DAT
where O
the O
attention O
mechanism O
typically O

far, O
all O
attention O
models O
for O
VQA B-DAT
in O
literature O
have O
focused O
on O

novel O
multi-modal O
attention O
model O
for O
VQA B-DAT
with O
the O
following O
two O
unique O

a O
novel O
co-attention O
mechanism O
for O
VQA B-DAT
that O
jointly O
performs O
question-guided O
visual O

model O
on O
two O
large O
datasets, O
VQA B-DAT
[2] O
and O
COCO-QA O
[17 O

6] O
have O
proposed O
models O
for O
VQA B-DAT

explored O
image O
attention O
models O
for O
VQA B-DAT

generated O
by O
attention O
models O
for O
VQA B-DAT

has O
explored O
question O
attention O
in O
VQA, B-DAT
there O
are O
some O
related O
works O

Following O
[2], O
we O
treat O
VQA B-DAT
as O
a O
classification O
task. O
We O

model O
on O
two O
datasets, O
the O
VQA B-DAT
dataset O
[2] O
and O
the O
COCO-QA O

VQA B-DAT
dataset O
[2] O
is O
the O
largest O

we O
train O
our O
model O
on O
VQA B-DAT
train+val O
and O
report O
the O
test-dev O

and O
test-standard O
results O
from O
the O
VQA B-DAT
evaluation O
server. O
We O
use O
the O

Table O
1: O
Results O
on O
the O
VQA B-DAT
dataset. O
“-” O
indicates O
the O
results O

to O
512 O
and O
1024 O
for O
VQA B-DAT
since O
it O
is O
a O
much O

are O
two O
test O
scenarios O
on O
VQA B-DAT

1 O
shows O
results O
on O
the O
VQA B-DAT
test O
sets O
for O
both O
open-ended O

Similar O
to O
the O
result O
on O
VQA, B-DAT
our O
model O
improves O
the O
state-of-the-art O

w.r.t O
these O
ablations O
on O
the O
VQA B-DAT
validation O
set O
(test O
sets O
are O

of O
previous O
attention O
models O
for O
VQA B-DAT
[23, O
25 O

3: O
Ablation O
study O
on O
the O
VQA B-DAT
dataset O
using O
Oursa+VGG O

three O
columns O
using O
Oursp+VGG) O
and O
VQA B-DAT
(last O
two O
columns O
Oursa+VGG) O
dataset O

three O
columns O
using O
Oursp+VGG) O
and O
VQA B-DAT
(last O
two O
columns O
Oursa+VGG) O
dataset O

ended B-DAT
and O
multiple-choice. O
The O
best O
performing O

ended B-DAT
test O
scenario, O
we O
compare O
our O

ended B-DAT
and O
multiple-choice O
settings. O
We O
can O

ended B-DAT
and O
from O
64.2% O
(FDA O
[11 O

ended B-DAT
questions, O
and O
4.0% O
and O
1.1 O

two O
test O
scenarios O
on O
VQA: O
open B-DAT

used O
as O
our O
baseline. O
For O
open B-DAT

VQA O
test O
sets O
for O
both O
open B-DAT

23]) O
to O
62.1% O
(Oursa+ResNet) O
on O
open B-DAT

3.4% O
and O
1.4% O
improvement O
on O
open B-DAT

Q:do O
the O
doors O
open B-DAT
in O
or O
out? O
A: O
open(in O

Q: O
is O
this O
an O
open B-DAT
market O
at O
night? O
A: O
yes(no O

places O
? O
do O
the O
doors O
open B-DAT
in O
or O
out O
? O
is O

this O
an O
open B-DAT
market O
at O
night O

places O
? O
do O
the O
doors O
open B-DAT
in O
or O
out O
? O
is O

this O
an O
open B-DAT
market O
at O
night O

places O
? O
do O
the O
doors O
open B-DAT
in O
or O
out O
? O
is O

this O
an O
open B-DAT
market O
at O
night O

based O
on O
8,000 O
and O
4,000 O
images B-DAT
respectively. O
There O
are O
four O
types O

attention O
has O
different O
patterns O
across O
images B-DAT

. O
For O
the O
first O
two O
images, B-DAT
the O
attention O
transfers O
from O
objects O

attends O
to O
the O
regions O
in O
images B-DAT
and O
phrases O
in O
the O
questions O

co-attends O
to O
interpretable O
regions O
of O
images B-DAT
and O
questions O
for O
predicting O
the O

approach O
to O
answering O
questions O
about O
images B-DAT

Visual7w: O
Grounded O
question O
answering O
in O
images B-DAT

Hierarchical O
Question-Image B-DAT
Co-Attention O
for O
Visual O
Question O
Answering O

proposed O
attention O
models O
for O
Visual O
Question B-DAT
Answering O
(VQA) O
that O
generate O
spatial O

1 O
Introduction O
Visual O
Question B-DAT
Answering O
(VQA) O
[2, O
7, O
16 O

Question B-DAT
Hierarchy: O
We O
build O
a O
hierarchical O

3.2 O
Question B-DAT
Hierarchy O

Question B-DAT
Attention O
alone, O
where O
no O
image O

Atten O
79.8 O
33.9 O
43.6 O
55.9 O
Question B-DAT
Atten O
79.4 O
33.3 O
41.7 O
54.8 O

3.2 O
Question B-DAT
Hierarchy O

Hierarchical O
Question-Image O
Co-Attention O
for O
Visual B-DAT
Question O
Answering O

have O
proposed O
attention O
models O
for O
Visual B-DAT
Question O
Answering O
(VQA) O
that O
generate O

1 O
Introduction O
Visual B-DAT
Question O
Answering O
(VQA) O
[2, O
7 O

Zitnick, O
and O
Devi O
Parikh. O
Vqa: O
Visual B-DAT
question O
answering. O
In O
ICCV, O
2015 O

David O
A O
Shamma, O
et O
al. O
Visual B-DAT
genome: O
Connecting O
language O
and O
vision O

61.6% O
to O
63.3% O
on O
the O
COCO B-DAT

for O
VQA O
and O
65.4% O
for O
COCO B-DAT

large O
datasets, O
VQA O
[2] O
and O
COCO B-DAT

VQA O
dataset O
[2] O
and O
the O
COCO B-DAT

questions O
and O
answers O
on O
Microsoft O
COCO B-DAT
dataset O
[14]. O
The O
dataset O
contains O

COCO B-DAT

from O
captions O
in O
the O
Microsoft O
COCO B-DAT
dataset O
[14]. O
There O
are O
78,736 O

the O
last O
5 O
epochs. O
For O
COCO B-DAT

15] O
and O
SAN O
[25] O
on O
COCO B-DAT

2 O
shows O
results O
on O
the O
COCO B-DAT

Table O
2: O
Results O
on O
the O
COCO B-DAT

question O
co-attention O
maps O
on O
the O
COCO B-DAT

on O
success O
cases O
in O
the O
COCO B-DAT

on O
failure O
cases O
in O
the O
COCO B-DAT

Question-Image O
Co-Attention O
for O
Visual O
Question O
Answering B-DAT

attention O
models O
for O
Visual O
Question O
Answering B-DAT
(VQA) O
that O
generate O
spatial O
maps O

1 O
Introduction O
Visual O
Question O
Answering B-DAT
(VQA) O
[2, O
7, O
16, O
17 O

models O
for O
Visual O
Question O
Answering O
(VQA) B-DAT
that O
generate O
spatial O
maps O
highlighting O

a O
novel O
co-attention O
model O
for O
VQA B-DAT
that O
jointly O
reasons O
about O
image O

improves O
the O
state-of-the-art O
on O
the O
VQA B-DAT
dataset O
from O
60.3% O
to O
60.5 O

further O
improved O
to O
62.1% O
for O
VQA B-DAT
and O
65.4% O
for O
COCO-QA.1 O

1 O
Introduction O
Visual O
Question O
Answering O
(VQA) B-DAT
[2, O
7, O
16, O
17, O
29 O

23–25] O
have O
been O
explored O
for O
VQA, B-DAT
where O
the O
attention O
mechanism O
typically O

far, O
all O
attention O
models O
for O
VQA B-DAT
in O
literature O
have O
focused O
on O

novel O
multi-modal O
attention O
model O
for O
VQA B-DAT
with O
the O
following O
two O
unique O

a O
novel O
co-attention O
mechanism O
for O
VQA B-DAT
that O
jointly O
performs O
question-guided O
visual O

model O
on O
two O
large O
datasets, O
VQA B-DAT
[2] O
and O
COCO-QA O
[17 O

6] O
have O
proposed O
models O
for O
VQA B-DAT

explored O
image O
attention O
models O
for O
VQA B-DAT

generated O
by O
attention O
models O
for O
VQA B-DAT

has O
explored O
question O
attention O
in O
VQA, B-DAT
there O
are O
some O
related O
works O

Following O
[2], O
we O
treat O
VQA B-DAT
as O
a O
classification O
task. O
We O

model O
on O
two O
datasets, O
the O
VQA B-DAT
dataset O
[2] O
and O
the O
COCO-QA O

VQA B-DAT
dataset O
[2] O
is O
the O
largest O

we O
train O
our O
model O
on O
VQA B-DAT
train+val O
and O
report O
the O
test-dev O

and O
test-standard O
results O
from O
the O
VQA B-DAT
evaluation O
server. O
We O
use O
the O

Table O
1: O
Results O
on O
the O
VQA B-DAT
dataset. O
“-” O
indicates O
the O
results O

to O
512 O
and O
1024 O
for O
VQA B-DAT
since O
it O
is O
a O
much O

are O
two O
test O
scenarios O
on O
VQA B-DAT

1 O
shows O
results O
on O
the O
VQA B-DAT
test O
sets O
for O
both O
open-ended O

Similar O
to O
the O
result O
on O
VQA, B-DAT
our O
model O
improves O
the O
state-of-the-art O

w.r.t O
these O
ablations O
on O
the O
VQA B-DAT
validation O
set O
(test O
sets O
are O

of O
previous O
attention O
models O
for O
VQA B-DAT
[23, O
25 O

3: O
Ablation O
study O
on O
the O
VQA B-DAT
dataset O
using O
Oursa+VGG O

three O
columns O
using O
Oursp+VGG) O
and O
VQA B-DAT
(last O
two O
columns O
Oursa+VGG) O
dataset O

three O
columns O
using O
Oursp+VGG) O
and O
VQA B-DAT
(last O
two O
columns O
Oursa+VGG) O
dataset O

100 B-DAT
layers. O
The O
very O
deep O
neural O

10044009 B-DAT

10060086 B-DAT

1601 B-DAT

1409 B-DAT

1207 B-DAT

1604 B-DAT

1506 B-DAT

1505 B-DAT

1603 B-DAT

81.81 O
38.43 O
48.43 O
MRN O
3 O
1200 B-DAT
33.9M O
61.68 O
82.28 O
38.82 O
49.25 O

may O
help O
to O
solve O
the O
real B-DAT
world O
problems O
which O
need O
the O

We O
choose O
the O
Visual O
QA O
(VQA) B-DAT
dataset O
[2] O
for O
the O
evaluation O

questions O
and O
answers O
of O
the O
VQA B-DAT
dataset O
are O
collected O
via O
Amazon O

that O
TrimZero O
was O
suitable O
for O
VQA B-DAT
tasks. O
Approximately, O
37.5% O
of O
training O

Table O
3: O
The O
VQA B-DAT
test-standard O
results. O
The O
precision O
of O

The O
VQA B-DAT
Challenge, O
which O
released O
the O
VQA O

the O
training O
set O
of O
the O
VQA B-DAT
dataset, O
and O
visualized O
using O
the O

the O
state-of-the-art O
results O
on O
the O
VQA B-DAT
dataset O
for O
both O
Open-Ended O
and O

Zitnick, O
and O
Devi O
Parikh. O
VQA B-DAT

A.1 O
VQA B-DAT
test-dev O
Results O

effects O
of O
various O
options O
for O
VQA B-DAT
test-dev. O
Here, O
the O
model O
of O

Table O
5: O
The O
results O
for O
VQA B-DAT
test-dev. O
The O
precision O
of O
some O

shortcut O
connections O
of O
MRN O
for O
VQA B-DAT
test-dev. O
ResNet-152 O
features O
and O
2k O

A.1 O
VQA B-DAT
test-dev O
Results O

as O
a O
sequence O
of O
three O
images B-DAT

The O
images B-DAT
come O
from O
the O
MS-COCO O
dataset O

and O
81,434 O
for O
test. O
The O
images B-DAT
are O
carefully O
collected O
to O
contain O

three-block O
layered O
MRN. O
The O
original O
images B-DAT
are O
shown O
in O
the O
first O

each O
group. O
The O
next O
three O
images B-DAT
show O
the O
input O
gradients O
of O

effect O
(bright O
color) O
on O
the O
images B-DAT

Networks O
for O
Question B-DAT
Answering. O
arXiv O
preprint O
arXiv:1601.01705, O
2016 O

and O
Devi O
Parikh. O
VQA: O
Visual O
Question B-DAT
Answering. O
In O
International O
Conference O
on O

Dynamic O
Attention O
Model O
for O
Visual O
Question B-DAT
Answering. O
arXiv O
preprint O
arXiv:1604.01485, O
2016 O

LSTM O
and O
normalized O
CNN O
Visual O
Question B-DAT
Answering O
model. O
https://github.com/VT-vision-lab/VQA_LSTM_CNN, O
2015 O

Seo, O
and O
Bohyung O
Han. O
Image O
Question B-DAT
Answering O
us- O
ing O
Convolutional O
Neural O

Models O
and O
Data O
for O
Image O
Question B-DAT
Answering. O
In O
Advances O
in O
Neural O

Ask O
Me O
Anything: O
Free-form O
Visual O
Question B-DAT
Answering O
Based O
on O
Knowledge O
from O

Networks O
for O
Visual O
and O
Textual O
Question B-DAT
Answering. O
arXiv O
preprint O
arXiv:1603.01417, O
2016 O

Stacked O
Attention O
Networks O
for O
Image O
Question B-DAT
Answering. O
arXiv O
preprint O
arXiv:1511.02274, O
2015 O

Question B-DAT
[2] O
48.09 O
75.66 O
36.70 O
27.14 O

Multimodal O
Residual O
Learning O
for O
Visual B-DAT
QA O

the O
state-of-the-art O
results O
on O
the O
Visual B-DAT
QA O
dataset O
for O
both O
Open-Ended O

Visual B-DAT
question-answering O
tasks O
provide O
a O
testbed O

the O
state-of-the-art O
results O
on O
the O
Visual B-DAT
QA O
dataset O
for O
both O
Open-Ended O

4.1 O
Visual B-DAT
QA O
Dataset O

We O
choose O
the O
Visual B-DAT
QA O
(VQA) O
dataset O
[2] O
for O

Visual B-DAT
Features O
The O
ResNet-152 O
visual O
features O

Zitnick, O
and O
Devi O
Parikh. O
VQA: O
Visual B-DAT
Question O
Answering. O
In O
International O
Conference O

Focused O
Dynamic O
Attention O
Model O
for O
Visual B-DAT
Question O
Answering. O
arXiv O
preprint O
arXiv:1604.01485 O

Karpathy O
and O
Li O
Fei-Fei. O
Deep O
Visual B-DAT

Deeper O
LSTM O
and O
normalized O
CNN O
Visual B-DAT
Question O
Answering O
model. O
https://github.com/VT-vision-lab/VQA_LSTM_CNN, O
2015 O

Hengel. O
Ask O
Me O
Anything: O
Free-form O
Visual B-DAT
Question O
Answering O
Based O
on O
Knowledge O

Socher. O
Dynamic O
Memory O
Networks O
for O
Visual B-DAT
and O
Textual O
Question O
Answering. O
arXiv O

4.1 O
Visual B-DAT
QA O
Dataset O

COCO B-DAT
dataset, O
123,287 O
of O
them O
for O

and O
C O
Lawrence O
Zitnick. O
Microsoft O
COCO B-DAT

Networks O
for O
Question O
Answering B-DAT

Devi O
Parikh. O
VQA: O
Visual O
Question O
Answering B-DAT

Attention O
Model O
for O
Visual O
Question O
Answering B-DAT

and O
normalized O
CNN O
Visual O
Question O
Answering B-DAT
model. O
https://github.com/VT-vision-lab/VQA_LSTM_CNN, O
2015 O

Neurons: O
A O
Neural-based O
Approach O
to O
Answering B-DAT
Questions O
about O
Images. O
arXiv O
preprint O

and O
Bohyung O
Han. O
Image O
Question O
Answering B-DAT
us- O
ing O
Convolutional O
Neural O
Network O

and O
Data O
for O
Image O
Question O
Answering B-DAT

Me O
Anything: O
Free-form O
Visual O
Question O
Answering B-DAT
Based O
on O
Knowledge O
from O
External O

for O
Visual O
and O
Textual O
Question O
Answering B-DAT

Attention O
Networks O
for O
Image O
Question O
Answering B-DAT

We O
choose O
the O
Visual O
QA O
(VQA) B-DAT
dataset O
[2] O
for O
the O
evaluation O

questions O
and O
answers O
of O
the O
VQA B-DAT
dataset O
are O
collected O
via O
Amazon O

that O
TrimZero O
was O
suitable O
for O
VQA B-DAT
tasks. O
Approximately, O
37.5% O
of O
training O

Table O
3: O
The O
VQA B-DAT
test-standard O
results. O
The O
precision O
of O

The O
VQA B-DAT
Challenge, O
which O
released O
the O
VQA O

the O
training O
set O
of O
the O
VQA B-DAT
dataset, O
and O
visualized O
using O
the O

the O
state-of-the-art O
results O
on O
the O
VQA B-DAT
dataset O
for O
both O
Open-Ended O
and O

Zitnick, O
and O
Devi O
Parikh. O
VQA B-DAT

A.1 O
VQA B-DAT
test-dev O
Results O

effects O
of O
various O
options O
for O
VQA B-DAT
test-dev. O
Here, O
the O
model O
of O

Table O
5: O
The O
results O
for O
VQA B-DAT
test-dev. O
The O
precision O
of O
some O

shortcut O
connections O
of O
MRN O
for O
VQA B-DAT
test-dev. O
ResNet-152 O
features O
and O
2k O

A.1 O
VQA B-DAT
test-dev O
Results O

generating O
sentences. O
They O
first O
learned O
1000 B-DAT
independent O
detectors O
for O
visual O
words O

3.6 O
-0.6 O
-6.2 O
-0.4 O
2.1 O
1.0 B-DAT

use O
SGD O
with O
mini-batches O
of O
100 B-DAT
image-sentence O
pairs O

the O
context O
words. O
We O
collect O
100, B-DAT

Descent O
(SGD) O
with O
mini-batches O
of O
100 B-DAT
image-QA O
pairs. O
The O
attributes, O
internal O

0.80 O
0.64 O
0.50 O
0.40 O
0.28 O
1.07 B-DAT
9.60 O
Att-SVM+LSTM O
0.69 O
0.52 O
0.38 O

feature O
dimension O
from O
4096 O
to O
1000 B-DAT

generation O
ability. O
We O
randomly O
sample O
1000 B-DAT
results O
from O
the O
COCO O
validation O

TABLE O
4: O
Human O
Evaluation O
on O
1000 B-DAT
sampled O
results O
from O
MS O
COCO O

38] O
VIS O
Input O
Dim O
256 O
1000 B-DAT
1000 O
4096 O
4096 O

RNN O
Dim O
256 O
512 O
1000 B-DAT

that O
said O
answer3 O
,1} O
thus O
100 B-DAT

with O
2 O
hidden O
layers O
and O
1000 B-DAT
hidden O
units O
(dropout O
0.5) O
in O

space O
of O
which O
is O
the O
1000 B-DAT
most O
frequent O
answers O
in O
the O

1505 B-DAT

Int. O
J. O
Comput. O
Vision, O
vol. O
100, B-DAT
no. O
1, O
pp. O
59–77, O
2012 O

98, O
no. O
8, O
pp. O
1485– O
1508, B-DAT
2010 O

1507 B-DAT

1506 B-DAT

1505 B-DAT

1506 B-DAT

The O
figure O
1 O
shows O
an O
real B-DAT
example O
produced O
by O
our O
model O

approach O
to O
question O
answering O
about O
real B-DAT

J. O
Sun, O
“Faster O
r-cnn: O
Towards O
real B-DAT

Visual O
Question O
Answering O
(VQA) B-DAT
is O
a O
more O
recent O

state O
of O
the O
art O
in O
VQA B-DAT
[16], O
[17], O
[18] O
relies O
on O

15] O
also O
have O
suggested O
that O
VQA B-DAT
is O
a O
more O
“AI-complete” O
task O

including O
common O
sense, O
into O
the O
VQA B-DAT
process. O
In O
this O
work, O
we O

art O
is O
61.60%. O
On O
the O
VQA B-DAT
[15] O
evaluation O
server O
(which O
does O

further O
experiments O
on O
two O
additional O
VQA B-DAT
datasets. O
More O
ablation O
models O
of O

the O
first O
to O
study O
the O
VQA B-DAT
problem. O
They O
proposed O
a O
method O

15] O
proposed O
a O
large-scale O
open-ended O
VQA B-DAT
dataset O
based O
on O
COCO, O
which O

is O
called O
VQA B-DAT

47], O
[51], O
[52] O
formulate O
the O
VQA B-DAT
as O
a O
classification O
problem O
and O

QA) O
systems O
[55], O
[56]. O
However, O
VQA B-DAT
systems O
exploiting O
KBs O
are O
still O

a O
formal O
query O
language, O
our O
VQA B-DAT
system O
is O
able O
to O
encode O

issues O
in O
this O
approach O
to O
VQA B-DAT

4 O
A O
VQA B-DAT
MODEL O
WITH O
EXTERNAL O
KNOWLEDGE O
The O

key O
differentiator O
of O
our O
VQA B-DAT
model O
is O
that O
it O
is O

of O
input O
sources O
of O
our O
VQA B-DAT

We O
propose O
to O
train O
a O
VQA B-DAT
model O
by O
maximizing O
the O
prob O

which O
is O
input O
to O
the O
VQA B-DAT
LSTM O
model O
that O
interprets O
the O

We O
want O
our O
VQA B-DAT
model O
to O
be O
able O
to O

test O
images. O
Two O
large- O
scale O
VQA B-DAT
data O
are O
constructed O
both O
based O

descriptions. O
Another O
benchmarked O
dataset O
is O
VQA B-DAT
[15], O
which O
is O
a O
much O

truth O
answers O
for O
the O
actual O
VQA B-DAT
test O
split O
are O
not O
available O

can O
be O
evaluated O
via O
the O
VQA B-DAT
evaluation O
server. O
Hence, O
we O
also O

DAQURA O
Toronto O
All O
Reduced O
COCO-QA O
VQA B-DAT

Toronto O
COCO-QA O
Dataset O
[18] O
and O
VQA B-DAT
dataset O
[15 O

a O
significant O
role O
in O
the O
VQA B-DAT
task. O
The O
Att+Know-LSTM O
model O
does O

5.2.3 O
Results O
on O
VQA B-DAT
Antol O
et O
al. O
[15] O
provide O

the O
VQA B-DAT
dataset O
which O
is O
intended O
to O

There O
are O
several O
splits O
for O
VQA B-DAT
dataset, O
such O
as O
the O
validation O

Table O
11, O
results O
the O
on O
VQA B-DAT
validation O
set, O
we O
see O
that O

Its O
overall O
accuracy O
on O
the O
VQA B-DAT
is O
50.01, O
which O
is O
still O

for O
various O
question O
types O
on O
VQA B-DAT
validation O
set. O
All O
results O
are O

the O
evaluation O
metric O
from O
the O
VQA B-DAT
evaluation O
tools. O
The O
overall O
accuracy O

have O
also O
tested O
on O
the O
VQA B-DAT
test-dev O
and O
test- O
standard O
consisting O

and O
evaluated O
them O
on O
the O
VQA B-DAT
evaluation O
server. O
Table O
12 O
shows O

VQA B-DAT
Answer O
Type O
Overall O
Test-standard O
Yes/No O

TABLE O
12: O
VQA B-DAT
Open-Ended O
evaluation O
server O
results. O
Accura O

to O
extend O
the O
state-of-the-art O
RNN-based O
VQA B-DAT
approach O
so O
as O
to O
incorporate O

is O
the O
state-of-the-art O
on O
three O
VQA B-DAT
datasets O
and O
produces O
the O
best O

results O
on O
the O
VQA B-DAT
evaluation O
server O

VQA B-DAT

All O
results O
are O
from O
the O
VQA B-DAT
dataset. O
More O
results O
can O
be O

4 O
A O
VQA B-DAT
Model O
with O
External O
Knowledge O

5.2.3 O
Results O
on O
VQA B-DAT

ended B-DAT
question O
about O
the O
image O
are O

ended B-DAT
VQA O
dataset O
based O
on O
COCO O

ended B-DAT
Visual O
Question O
Answering”. O
They O
also O

ended, B-DAT
questions O
about O
images. O
The O
knowledge O

an O
image O
and O
a O
free-form, O
open B-DAT

al. O
[15] O
proposed O
a O
large-scale O
open B-DAT

intended O
to O
support O
“free-form O
and O
open B-DAT

TABLE O
11: O
Results O
on O
the O
open B-DAT

information O
required O
to O
answer O
general, O
open B-DAT

nucleus O
for O
a O
web O
of O
open B-DAT
data. O
Springer, O
2007 O

The O
problem O
of O
annotating O
images B-DAT
with O
natural O
language O
at O
the O

based O
on O
co- O
embedding O
of O
images B-DAT
and O
text O
in O
the O
same O

to O
learn O
the O
mapping O
from O
images B-DAT
to O
sentences O
directly. O
Mao O
et O

learn O
a O
bi-directional O
mapping O
between O
images B-DAT
and O
their O
sentence-based O
descriptions, O
which O

in O
the O
generality O
of O
the O
images B-DAT
and O
text O
it O
is O
applicable O

to O
learn O
the O
mapping O
from O
images B-DAT
to O
sentences O
has O
become O
the O

to O
detect O
visual O
words O
from O
images B-DAT

contain O
8,000, O
31,000 O
and O
123,287 O
images B-DAT
respec- O
tively, O
and O
each O
image O

test O
set O
consisting O
of O
40775 O
images B-DAT
(human O
captions O
for O
this O
split O

further O
generated O
captions O
for O
the O
images B-DAT
in O
the O
COCO O
test O
set O

containing O
40,775 O
images B-DAT
and O
evaluated O
them O
on O
the O

and O
uses O
only O
25 O
test O
images B-DAT

both O
based O
on O
MS O
COCO O
images B-DAT

are O
gener- O
ated O
from O
117,684 O
images B-DAT

based O
on O
204,721 O
MS O
COCO O
images B-DAT

. O
We O
randomly O
choose O
5000 O
images B-DAT
from O
the O
validation O
set O
as O

et O
al. O
[79] O
encodes O
both O
images B-DAT
and O
questions O
with O
a O
CNN O

counting O
ability O
in O
very O
clean O
images B-DAT
with O
a O
single O
object O
type O

set O
(we O
randomly O
choose O
5000 O
images B-DAT
from O
the O
validation O
set O
as O

answer O
general, O
open-ended, O
questions O
about O
images B-DAT

T. O
L. O
Berg, O
“Im2text: O
Describing O
images B-DAT
using O
1 O
million O
captioned O
photographs O

semantics O
for O
finding O
and O
describing O
images B-DAT
with O
sentences,” O
Proc. O
Conf. O
Association O

a O
story: O
Generating O
sentences O
from O
images B-DAT

guided O
sentence O
generation O
of O
natural O
images B-DAT

reconstruction O
of O
archi- O
tecture O
from O
images B-DAT

Image O
Captioning O
and O
Visual O
Question B-DAT
Answering O
Based O
on O
Attributes O

Index O
Terms—Image O
Captioning, O
Visual O
Question B-DAT
Answering, O
Concepts O
Learning, O
Recurrent O
Neural O

Question B-DAT
Answering: O
Q: O
Why O
do O
they O

Visual O
Question B-DAT
Answering O
(VQA) O
is O
a O
more O

2.3 O
Visual O
Question B-DAT
Answering O

successfully O
in O
several O
natural O
language O
Question B-DAT
Answering O
(QA) O
systems O
[55], O
[56 O

4.2 O
Question B-DAT

5.2 O
Evaluation O
on O
Visual O
Question B-DAT
Answering O
We O
evaluate O
our O
model O

12,468 O
4,173 O
117,684 O
614,163 O
# O
Question B-DAT
Types O
3 O
3 O
4 O
more O

support O
“free-form O
and O
open-ended O
Visual O
Question B-DAT
Answering”. O
They O
also O
provide O
a O

Our-Baseline O
Our O
Proposal O
Question B-DAT
VggNet O
Att O
Att+Cap O
Att+Know O
A+C+K O

and O
D. O
Parikh, O
“VQA: O
Visual O
Question B-DAT
Answering,” O
in O
Proc. O
IEEE O
Int O

Methods O
for O
Multilingual O
Im- O
age O
Question B-DAT
Answering,” O
in O
Proc. O
Advances O
in O

Kiros, O
and O
R. O
Zemel, O
“Image O
Question B-DAT
Answering: O
A O
Visual O
Semantic O
Embedding O

Ask O
Me O
Anything: O
Free-form O
Visual O
Question B-DAT
Answering O
Based O
on O
Knowledge O
from O

and O
L. O
Fei-Fei, O
“Visual7W: O
Grounded O
Question B-DAT
Answering O
in O
Images,” O
in O
Proc O

Ask, O
Attend O
and O
Answer: O
Exploring O
Question B-DAT

-Guided O
Spatial O
Attention O
for O
Visual O
Question B-DAT
Answer- O
ing,” O
arXiv O
preprint O
arXiv:1511.05234 O

Convolutional O
Neural O
Network O
for O
Visual O
Question B-DAT
Answering,” O
arXiv O
preprint O
arXiv:1511.05960, O
2015 O

Li, O
“Compositional O
Memory O
for O
Visual O
Question B-DAT
Answering,” O
arXiv O
preprint O
arXiv:1511.05676, O
2015 O

D. O
Klein, O
“Deep O
Compo- O
sitional O
Question B-DAT
Answering O
with O
Neural O
Module O
Networks O

Stacked O
Attention O
Networks O
for O
Image O
Question B-DAT
Answering,” O
in O
Proc. O
IEEE O
Conf O

Seo, O
and O
B. O
Han, O
“Image O
Question B-DAT
Answering O
using O
Convolutional O
Neural O
Network O

Semantic O
Parsing O
on O
Freebase O
from O
Question B-DAT

Multimodal O
Knowledge O
Base O
for O
Visual O
Question B-DAT
Answering,” O
arXiv:1507.05670, O
2015 O

VisKE: O
Visual O
Knowledge O
Extraction O
and O
Question B-DAT
Answering O
by O
Visual O
Veri- O
fication O

2.3 O
Visual O
Question B-DAT
Answering O

4.2 O
Question B-DAT

5.2 O
Evaluation O
on O
Visual O
Question B-DAT
Answering O

Image O
Captioning O
and O
Visual B-DAT
Question O
Answering O
Based O
on O
Attributes O

Index O
Terms—Image O
Captioning, O
Visual B-DAT
Question O
Answering, O
Concepts O
Learning, O
Recurrent O

with O
the O
Australian O
Centre O
for O
Visual B-DAT
Technolo- O
gies, O
and O
School O
of O

Visual B-DAT
Question O
Answering O
(VQA) O
is O
a O

2.3 O
Visual B-DAT
Question O
Answering O

TABLE O
5: O
Visual B-DAT
feature O
input O
dimension O
and O
properties O

5.2 O
Evaluation O
on O
Visual B-DAT
Question O
Answering O
We O
evaluate O
our O

to O
support O
“free-form O
and O
open-ended O
Visual B-DAT
Question O
Answering”. O
They O
also O
provide O

Zitnick, O
“Mind’s O
Eye: O
A O
Recurrent O
Visual B-DAT
Representation O
for O
Image O
Caption O
Generation O

Zitnick, O
and O
D. O
Parikh, O
“VQA: O
Visual B-DAT
Question O
Answering,” O
in O
Proc. O
IEEE O

Zemel, O
“Image O
Question O
Answering: O
A O
Visual B-DAT
Semantic O
Embedding O
Model O
and O
a O

Hengel, O
“Ask O
Me O
Anything: O
Free-form O
Visual B-DAT
Question O
Answering O
Based O
on O
Knowledge O

Neural O
Image O
Caption O
Generation O
with O
Visual B-DAT
Attention,” O
in O
Proc. O
Int. O
Conf O

Visual B-DAT
Tur- O
ing O
test O
for O
computer O

Exploring O
Question-Guided O
Spatial O
Attention O
for O
Visual B-DAT
Question O
Answer- O
ing,” O
arXiv O
preprint O

Based O
Convolutional O
Neural O
Network O
for O
Visual B-DAT
Question O
Answering,” O
arXiv O
preprint O
arXiv:1511.05960 O

Y. O
Li, O
“Compositional O
Memory O
for O
Visual B-DAT
Question O
Answering,” O
arXiv O
preprint O
arXiv:1511.05676 O

Large-scale O
Multimodal O
Knowledge O
Base O
for O
Visual B-DAT
Question O
Answering,” O
arXiv:1507.05670, O
2015 O

Listen, O
Use O
Your O
Imagination: O
Leveraging O
Visual B-DAT
Common O
Sense O
for O
Non-Visual O
Tasks O

Divvala, O
and O
A. O
Farhadi, O
“VisKE: O
Visual B-DAT
Knowledge O
Extraction O
and O
Question O
Answering O

by O
Visual B-DAT
Veri- O
fication O
of O
Relation O
Phrases O

and O
M. O
Fritz, O
“Towards O
a O
Visual B-DAT
Turing O
Chal- O
lenge,” O
arXiv:1410.8027, O
2014 O

the O
Aus- O
tralian O
Centre O
for O
Visual B-DAT
Technologies O
(ACVT) O
of O
the O
University O

at O
the O
Australian O
Centre O
for O
Visual B-DAT
Technologies O
(ACVT) O
of O
the O
University O

of O
The O
Australian O
Centre O
for O
Visual B-DAT
Technologies O
(ACVT). O
He O
received O
a O

2.3 O
Visual B-DAT
Question O
Answering O

5.2 O
Evaluation O
on O
Visual B-DAT
Question O
Answering O

of O
70.98% O
on O
the O
Toronto O
COCO B-DAT

al. O
[36] O
won O
the O
2015 O
COCO B-DAT
Captioning O

open-ended O
VQA O
dataset O
based O
on O
COCO, B-DAT
which O
is O
called O
VQA. O
Inspired O

MS O
COCO B-DAT

on O
Flickr8k, O
Flickr30k O
and O
MS O
COCO B-DAT
datasets O
separately. O
Note O
it O
is O

27], O
Flickr30k O
[69] O
and O
Microsoft O
COCO B-DAT
dataset O
[70]. O
These O
datasets O
contain O

split O
for O
Flickr30k O
and O
MS O
COCO, B-DAT
for O
fair O
comparison, O
we O
report O

tested O
on O
the O
actually O
MS O
COCO B-DAT
test O
set O
consisting O
of O
40775 O

and O
evaluated O
them O
on O
the O
COCO B-DAT
evaluation O
server O

sentence O
perplexity O
(PPL). O
For O
MS O
COCO B-DAT
dataset, O
we O
additionally O
evaluate O
our O

and O
our O
baseline O
on O
MS O
COCO B-DAT
dataset. O
‡ O
indicates O
ground O
truth O

COCO B-DAT
word O
dictionaries O
of O
size O
8791 O

on O
Flickr8k, O
Flickr30k O
and O
Microsoft O
COCO B-DAT
dataset. O
It O
is O
not O
surprising O

COCO B-DAT

TABLE O
3: O
COCO B-DAT
evaluation O
server O
results. O
M O
and O

for O
the O
images O
in O
the O
COCO B-DAT
test O
set O
containing O
40,775 O
images O

and O
evaluated O
them O
on O
the O
COCO B-DAT
evaluation O
server. O
These O
results O
are O

sample O
1000 O
results O
from O
the O
COCO B-DAT
validation O
dataset, O
generated O
by O
our O

evaluation O
protocol O
of O
the O
MS O
COCO B-DAT
Captioning O
Challenge O
2015, O
two O
evaluation O

1000 O
sampled O
results O
from O
MS O
COCO B-DAT
validation O
split O

constructed O
both O
based O
on O
MS O
COCO B-DAT
images. O
The O
Toronto O
COCO-QA O
Dataset O

answers O
based O
on O
204,721 O
MS O
COCO B-DAT
images. O
We O
randomly O
choose O
5000 O

DAQURA O
DAQURA O
Toronto O
All O
Reduced O
COCO B-DAT

statistics O
about O
the O
DAQURA, O
Toronto O
COCO B-DAT

has O
been O
fine-tuned O
on O
the O
COCO B-DAT
dataset, O
based O
on O
the O
task O

Toronto O
COCO B-DAT

and O
our O
baseline O
on O
Toronto O
COCO B-DAT

Toronto O
COCO B-DAT

TABLE O
10: O
Toronto O
COCO B-DAT

5.2.2 O
Results O
on O
Toronto O
COCO B-DAT

reports O
the O
results O
on O
Toronto O
COCO B-DAT

not O
surprising O
because O
the O
Toronto O
COCO B-DAT

generated O
automatically O
from O
the O
MS O
COCO B-DAT
captions, O
and O
thus O
the O
fact O

The O
comparison O
on O
the O
Toronto O
COCO B-DAT

labels O
used O
in O
the O
MS O
COCO B-DAT
captions. O
Ren O
et O
al.also O
observed O

and O
C. O
L. O
Zitnick, O
“Microsoft O
COCO B-DAT

5.2.2 O
Results O
on O
Toronto O
COCO B-DAT

Image O
Captioning O
and O
Visual O
Question O
Answering B-DAT
Based O
on O
Attributes O

Index O
Terms—Image O
Captioning, O
Visual O
Question O
Answering, B-DAT
Concepts O
Learning, O
Recurrent O
Neural O
Networks O

Question O
Answering B-DAT

Visual O
Question O
Answering B-DAT
(VQA) O
is O
a O
more O
recent O

2.3 O
Visual O
Question O
Answering B-DAT

in O
several O
natural O
language O
Question O
Answering B-DAT
(QA) O
systems O
[55], O
[56]. O
However O

5.2 O
Evaluation O
on O
Visual O
Question O
Answering B-DAT
We O
evaluate O
our O
model O
on O

free-form O
and O
open-ended O
Visual O
Question O
Answering B-DAT

D. O
Parikh, O
“VQA: O
Visual O
Question O
Answering B-DAT

for O
Multilingual O
Im- O
age O
Question O
Answering B-DAT

Neurons: O
A O
Neural-based O
Approach O
to O
Answering B-DAT
Questions O
about O
Images,” O
in O
Proc O

and O
R. O
Zemel, O
“Image O
Question O
Answering B-DAT

Me O
Anything: O
Free-form O
Visual O
Question O
Answering B-DAT
Based O
on O
Knowledge O
from O
External O

L. O
Fei-Fei, O
“Visual7W: O
Grounded O
Question O
Answering B-DAT
in O
Images,” O
in O
Proc. O
IEEE O

Neural O
Network O
for O
Visual O
Question O
Answering B-DAT

Compositional O
Memory O
for O
Visual O
Question O
Answering B-DAT

Klein, O
“Deep O
Compo- O
sitional O
Question O
Answering B-DAT
with O
Neural O
Module O
Networks,” O
in O

Attention O
Networks O
for O
Image O
Question O
Answering B-DAT

and O
B. O
Han, O
“Image O
Question O
Answering B-DAT
using O
Convolutional O
Neural O
Network O
with O

Knowledge O
Base O
for O
Visual O
Question O
Answering B-DAT

Visual O
Knowledge O
Extraction O
and O
Question O
Answering B-DAT
by O
Visual O
Veri- O
fication O
of O

2.3 O
Visual O
Question O
Answering B-DAT

5.2 O
Evaluation O
on O
Visual O
Question O
Answering B-DAT

Visual O
Question O
Answering O
(VQA) B-DAT
is O
a O
more O
recent O

state O
of O
the O
art O
in O
VQA B-DAT
[16], O
[17], O
[18] O
relies O
on O

15] O
also O
have O
suggested O
that O
VQA B-DAT
is O
a O
more O
“AI-complete” O
task O

including O
common O
sense, O
into O
the O
VQA B-DAT
process. O
In O
this O
work, O
we O

art O
is O
61.60%. O
On O
the O
VQA B-DAT
[15] O
evaluation O
server O
(which O
does O

further O
experiments O
on O
two O
additional O
VQA B-DAT
datasets. O
More O
ablation O
models O
of O

the O
first O
to O
study O
the O
VQA B-DAT
problem. O
They O
proposed O
a O
method O

15] O
proposed O
a O
large-scale O
open-ended O
VQA B-DAT
dataset O
based O
on O
COCO, O
which O

is O
called O
VQA B-DAT

47], O
[51], O
[52] O
formulate O
the O
VQA B-DAT
as O
a O
classification O
problem O
and O

QA) O
systems O
[55], O
[56]. O
However, O
VQA B-DAT
systems O
exploiting O
KBs O
are O
still O

a O
formal O
query O
language, O
our O
VQA B-DAT
system O
is O
able O
to O
encode O

issues O
in O
this O
approach O
to O
VQA B-DAT

4 O
A O
VQA B-DAT
MODEL O
WITH O
EXTERNAL O
KNOWLEDGE O
The O

key O
differentiator O
of O
our O
VQA B-DAT
model O
is O
that O
it O
is O

of O
input O
sources O
of O
our O
VQA B-DAT

We O
propose O
to O
train O
a O
VQA B-DAT
model O
by O
maximizing O
the O
prob O

which O
is O
input O
to O
the O
VQA B-DAT
LSTM O
model O
that O
interprets O
the O

We O
want O
our O
VQA B-DAT
model O
to O
be O
able O
to O

test O
images. O
Two O
large- O
scale O
VQA B-DAT
data O
are O
constructed O
both O
based O

descriptions. O
Another O
benchmarked O
dataset O
is O
VQA B-DAT
[15], O
which O
is O
a O
much O

truth O
answers O
for O
the O
actual O
VQA B-DAT
test O
split O
are O
not O
available O

can O
be O
evaluated O
via O
the O
VQA B-DAT
evaluation O
server. O
Hence, O
we O
also O

DAQURA O
Toronto O
All O
Reduced O
COCO-QA O
VQA B-DAT

Toronto O
COCO-QA O
Dataset O
[18] O
and O
VQA B-DAT
dataset O
[15 O

a O
significant O
role O
in O
the O
VQA B-DAT
task. O
The O
Att+Know-LSTM O
model O
does O

5.2.3 O
Results O
on O
VQA B-DAT
Antol O
et O
al. O
[15] O
provide O

the O
VQA B-DAT
dataset O
which O
is O
intended O
to O

There O
are O
several O
splits O
for O
VQA B-DAT
dataset, O
such O
as O
the O
validation O

Table O
11, O
results O
the O
on O
VQA B-DAT
validation O
set, O
we O
see O
that O

Its O
overall O
accuracy O
on O
the O
VQA B-DAT
is O
50.01, O
which O
is O
still O

for O
various O
question O
types O
on O
VQA B-DAT
validation O
set. O
All O
results O
are O

the O
evaluation O
metric O
from O
the O
VQA B-DAT
evaluation O
tools. O
The O
overall O
accuracy O

have O
also O
tested O
on O
the O
VQA B-DAT
test-dev O
and O
test- O
standard O
consisting O

and O
evaluated O
them O
on O
the O
VQA B-DAT
evaluation O
server. O
Table O
12 O
shows O

VQA B-DAT
Answer O
Type O
Overall O
Test-standard O
Yes/No O

TABLE O
12: O
VQA B-DAT
Open-Ended O
evaluation O
server O
results. O
Accura O

to O
extend O
the O
state-of-the-art O
RNN-based O
VQA B-DAT
approach O
so O
as O
to O
incorporate O

is O
the O
state-of-the-art O
on O
three O
VQA B-DAT
datasets O
and O
produces O
the O
best O

results O
on O
the O
VQA B-DAT
evaluation O
server O

VQA B-DAT

All O
results O
are O
from O
the O
VQA B-DAT
dataset. O
More O
results O
can O
be O

4 O
A O
VQA B-DAT
Model O
with O
External O
Knowledge O

5.2.3 O
Results O
on O
VQA B-DAT

6,16,3,19] O
is O
to O
use O
the O
1,000 B-DAT
most O
common O
answers O
in O
the O

follow O
[6] O
and O
use O
the O
1000 B-DAT
most O
common O
answers O

classification O
into O
one O
of O
the O
1,000 B-DAT
most O
common O
answers. O
The O
implementation O

1506 B-DAT

1105 B-DAT

1409 B-DAT

1601 B-DAT

1503 B-DAT

complex O
reasoning, O
common O
sense, O
and O
real B-DAT

approach O
to O
question O
answering O
about O
real B-DAT

Abstract. O
Visual O
Question O
and O
Answering O
(VQA) B-DAT
problems O
are O
attract- O
ing O
increasing O

from O
multiple O
research O
disciplines. O
Solving O
VQA B-DAT
problems O
requires O
techniques O
from O
both O

mod- O
eling, O
most O
of O
existing O
VQA B-DAT
methods O
adopt O
the O
strategy O
of O

on O
a O
large-scale O
benchmark O
dataset, O
VQA, B-DAT
clearly O
demonstrate O
the O
superior O
performance O

Visual O
question O
answering O
(VQA) B-DAT
is O
an O
active O
research O
direction O

from O
multiple O
communities. O
Generally, O
the O
VQA B-DAT
investigates O
a O
generalization O
of O
traditional O

to O
be O
considered. O
More O
concretely, O
VQA B-DAT
is O
about O
how O
to O
provide O

VQA B-DAT
is O
a O
quite O
challenging O
task O

developing O
modern O
AI O
systems. O
The O
VQA B-DAT
problem O
can O
be O
regarded O
as O

Recently, O
VQA B-DAT
is O
advanced O
significantly O
by O
the O

basket?”. O
Answering O
this O
question O
requires O
VQA B-DAT
methods O
to O
first O
understand O
the O

The O
first O
feasible O
solution O
to O
VQA B-DAT
problems O
was O
provided O
by O
Malinowski O

Fritz O
also O
constructed O
the O
first O
VQA B-DAT
bench- O
mark O
dataset, O
named O
as O

trained O
and O
evaluated O
on O
the O
VQA B-DAT
problem O
[4,5,3]. O
More O
recently, O
Antol O

6] O
published O
the O
currently O
largest O
VQA B-DAT
dataset. O
It O
consists O
of O
three O

and O
real-world O
knowledge, O
making O
the O
VQA B-DAT
dataset O
suitable O
for O
a O
true O

Visual O
Turing O
Test. O
The O
VQA B-DAT
authors O
split O
the O
evaluation O
on O

the O
critical O
factors O
for O
solving O
VQA B-DAT
problems O
well. O
A O
common O
practice O

with O
existing O
VQA B-DAT
methods O
on O
model- O
ing O
image O

and O
thus O
hurt O
the O
overall O
VQA B-DAT
performance O

given O
question. O
Recall O
the O
above O
VQA B-DAT
example. O
To O
answer O
the O
question O

these O
regions O
of O
interest. O
Then O
VQA B-DAT
compliments O
the O
features O
from O
selected O

model O
on O
two O
types O
of O
VQA B-DAT
tasks, O
i.e., O
the O
open-ended O
task O

the O
multiple-choice O
task, O
on O
the O
VQA B-DAT
dataset O
– O
the O
largest O
VQA O

and O
on O
the O
multiple- O
choice O
VQA B-DAT
tasks O

2 O
we O
review O
the O
current O
VQA B-DAT
models, O
and O
compare O
them O
to O

VQA B-DAT
has O
received O
great O
research O
attention O

uses O
attention O
mechanism O
in O
solving O
VQA B-DAT
problems O
is O
the O
ABC-CNN O
model O

the O
different O
sub-tasks O
of O
the O
VQA B-DAT
problem O
(e.g. O
counting, O
locating O
an O

set O
and O
thus O
simplify O
the O
VQA B-DAT
task O
to O
a O
classification O
problem O

4 O
Focused O
Dynamic O
Attention O
for O
VQA B-DAT

for O
two O
images O
from O
the O
VQA B-DAT
dataset. O
Examples O
provided O
by O
[6 O

use O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset O
[6], O
which O
is O
the O

baseline O
models O
provided O
by O
the O
VQA B-DAT
dataset O
authors O
[27], O
which O
currently O

standard O
implementation O
of O
an O
LSTM+CNN O
VQA B-DAT
model. O
It O
uses O
an O
LSTM O

and O
our O
FDA O
model O
on O
VQA B-DAT
test-dev O
and O
test-standard O
data O
for O

VQA B-DAT
Question O
48.09 O
75.66 O
27.14 O
36.70 O

and O
our O
FDA O
model O
on O
VQA B-DAT
test-dev O
and O
test-standard O
data O
for O

VQA B-DAT
Question O
53.68 O
75.71 O
38.64 O
37.05 O

the O
baselines O
provided O
by O
the O
VQA B-DAT
authors O
[6]. O
The O
results O
for O

nificant O
when O
solving O
the O
multiple-choice O
VQA B-DAT
problems. O
From O
Table O
2, O
one O

best O
ever O
performance O
on O
the O
VQA B-DAT
dataset. O
In O
particular, O
it O
improves O

model O
to O
solve O
the O
challenging O
VQA B-DAT
problems. O
FDA O
is O
built O
upon O

improvement O
over O
baselines O
on O
the O
VQA B-DAT
benchmark O
datasets, O
for O
both O
the O

open-ended O
and O
multiple-choices O
VQA B-DAT
tasks. O
Excellent O
performance O
of O
FDA O

attention O
to O
visual O
part O
in O
VQA B-DAT
tasks O
could O
essentially O
improve O
the O

ended B-DAT
task, O
where O
the O
method O
should O

ended B-DAT
task O
and O
the O
multiple-choice O
task O

ended, B-DAT
and O
on O
the O
multiple- O
choice O

ended B-DAT
questions, O
and O
2.42% O
for O
multiple-choice O

ended B-DAT
task, O
where O
the O
method O
should O

ended B-DAT
task. O
Re- O
sults O
from O
most O

ended B-DAT
task O
are O
listed O
in O
Table O

ended B-DAT
and O
multiple-choices O
VQA O
tasks. O
Excellent O

dataset O
on O
two O
tasks: O
an O
open B-DAT

of O
VQA O
tasks, O
i.e., O
the O
open B-DAT

achieve O
state-of-the-art O
accuracy O
on O
the O
open B-DAT

the O
image O
features O
(4.98% O
for O
open B-DAT

An O
open B-DAT

and O
test-standard O
data O
for O
the O
open B-DAT

6]. O
The O
results O
for O
the O
open B-DAT

benchmark O
datasets, O
for O
both O
the O
open B-DAT

repre- O
sentations O
of O
questions O
and O
images, B-DAT
align O
and O
fuse O
them O
in O

as O
DAQUAR, O
which O
contains O
1,449 O
images B-DAT
and O
12,468 O
questions O
generated O
by O

contains O
a O
large O
number O
of O
images B-DAT
(123,287) O
and O
questions O
(117,684), O
but O

each O
one O
of O
the O
204,721 O
images B-DAT
found O
in O
the O
Microsoft O
COCO O

the O
image O
(blue) O
for O
two O
images B-DAT
from O
the O
VQA O
dataset. O
Examples O

each O
one O
of O
the O
204,721 O
images B-DAT
found O
in O
the O
Microsoft O
COCO O

proach O
to O
answering O
questions O
about O
images B-DAT

Dynamic O
Attention O
Model O
for O
Visual O
Question B-DAT
Answering O

Abstract. O
Visual O
Question B-DAT
and O
Answering O
(VQA) O
problems O
are O

Keywords: O
Visual O
Question B-DAT
Answering, O
Attention O

Dynamic O
Attention O
Model O
for O
Visual O
Question B-DAT
Answering O
3 O

Dynamic O
Attention O
(FDA) O
for O
Visual O
Question B-DAT
Answering. O
With O
the O
FDA O
model O

Dynamic O
Attention O
Model O
for O
Visual O
Question B-DAT
Answering O
5 O

4.1 O
Question B-DAT
Understanding O

Question B-DAT

Dynamic O
Attention O
Model O
for O
Visual O
Question B-DAT
Answering O
7 O

experiments O
we O
use O
the O
Visual O
Question B-DAT
Answering O
(VQA) O
dataset O
[6], O
which O

Dynamic O
Attention O
Model O
for O
Visual O
Question B-DAT
Answering O
9 O

VQA O
Question B-DAT
48.09 O
75.66 O
27.14 O
36.70 O

VQA O
Question B-DAT
53.68 O
75.71 O
38.64 O
37.05 O

Question B-DAT

Dynamic O
Attention O
Model O
for O
Visual O
Question B-DAT
Answering O
11 O

Dynamic O
Attention O
Model O
for O
Visual O
Question B-DAT
Answering O
13 O

Dynamic O
Attention O
Model O
for O
Visual O
Question B-DAT
Answering O
15 O

Dynamic O
Attention O
Model O
for O
Visual O
Question B-DAT
Answering O

Focused O
Dynamic O
Attention O
Model O
for O
Visual B-DAT
Question O
Answering O

Abstract. O
Visual B-DAT
Question O
and O
Answering O
(VQA) O
problems O

Keywords: O
Visual B-DAT
Question O
Answering, O
Attention O

Visual B-DAT
question O
answering O
(VQA) O
is O
an O

can O
be O
regarded O
as O
a O
Visual B-DAT
Turing O
Test O

dataset O
suitable O
for O
a O
true O
Visual B-DAT
Turing O
Test. O
The O
VQA O
authors O

Focused O
Dynamic O
Attention O
Model O
for O
Visual B-DAT
Question O
Answering O
3 O

Focused O
Dynamic O
Attention O
(FDA) O
for O
Visual B-DAT
Question O
Answering. O
With O
the O
FDA O

Focused O
Dynamic O
Attention O
Model O
for O
Visual B-DAT
Question O
Answering O
5 O

Focused O
Dynamic O
Attention O
Model O
for O
Visual B-DAT
Question O
Answering O
7 O

all O
experiments O
we O
use O
the O
Visual B-DAT
Question O
Answering O
(VQA) O
dataset O
[6 O

Focused O
Dynamic O
Attention O
Model O
for O
Visual B-DAT
Question O
Answering O
9 O

Focused O
Dynamic O
Attention O
Model O
for O
Visual B-DAT
Question O
Answering O
11 O

Focused O
Dynamic O
Attention O
Model O
for O
Visual B-DAT
Question O
Answering O
13 O

S., O
Hallonquist, O
N., O
Younes, O
L.: O
Visual B-DAT
turing O
test O
for O
computer O
vision O

Zitnick, O
C., O
Parikh, O
D.: O
Vqa: O
Visual B-DAT
question O
answering. O
In: O
The O
IEEE O

Focused O
Dynamic O
Attention O
Model O
for O
Visual B-DAT
Question O
Answering O
15 O

Focused O
Dynamic O
Attention O
Model O
for O
Visual B-DAT
Question O
Answering O

images O
found O
in O
the O
Microsoft O
COCO B-DAT
dataset O
[7]. O
Answering O
the O
614,163 O

model O
used O
in O
ILSVRC O
and O
COCO B-DAT
2015 O
competitions, O
which O
won O
the O

classification, O
ImageNet O
detection, O
ImageNet O
localization, O
COCO B-DAT
detection, O
and O
COCO O
segmentation O
[24 O

images O
found O
in O
the O
Microsoft O
COCO B-DAT
dataset O
[7]. O
Figure O
2 O
shows O

Attention O
Model O
for O
Visual O
Question O
Answering B-DAT

Abstract. O
Visual O
Question O
and O
Answering B-DAT
(VQA) O
problems O
are O
attract- O
ing O

Keywords: O
Visual O
Question O
Answering, B-DAT
Attention O

apples O
are O
in O
the O
basket?”. O
Answering B-DAT
this O
question O
requires O
VQA O
methods O

the O
Microsoft O
COCO O
dataset O
[7]. O
Answering B-DAT
the O
614,163 O
questions O
requires O
complex O

Attention O
Model O
for O
Visual O
Question O
Answering B-DAT
3 O

Attention O
(FDA) O
for O
Visual O
Question O
Answering B-DAT

Attention O
Model O
for O
Visual O
Question O
Answering B-DAT
5 O

Attention O
Model O
for O
Visual O
Question O
Answering B-DAT
7 O

we O
use O
the O
Visual O
Question O
Answering B-DAT
(VQA) O
dataset O
[6], O
which O
is O

Attention O
Model O
for O
Visual O
Question O
Answering B-DAT
9 O

Attention O
Model O
for O
Visual O
Question O
Answering B-DAT
11 O

Attention O
Model O
for O
Visual O
Question O
Answering B-DAT
13 O

Attention O
Model O
for O
Visual O
Question O
Answering B-DAT
15 O

Attention O
Model O
for O
Visual O
Question O
Answering B-DAT

Abstract. O
Visual O
Question O
and O
Answering O
(VQA) B-DAT
problems O
are O
attract- O
ing O
increasing O

from O
multiple O
research O
disciplines. O
Solving O
VQA B-DAT
problems O
requires O
techniques O
from O
both O

mod- O
eling, O
most O
of O
existing O
VQA B-DAT
methods O
adopt O
the O
strategy O
of O

on O
a O
large-scale O
benchmark O
dataset, O
VQA, B-DAT
clearly O
demonstrate O
the O
superior O
performance O

Visual O
question O
answering O
(VQA) B-DAT
is O
an O
active O
research O
direction O

from O
multiple O
communities. O
Generally, O
the O
VQA B-DAT
investigates O
a O
generalization O
of O
traditional O

to O
be O
considered. O
More O
concretely, O
VQA B-DAT
is O
about O
how O
to O
provide O

VQA B-DAT
is O
a O
quite O
challenging O
task O

developing O
modern O
AI O
systems. O
The O
VQA B-DAT
problem O
can O
be O
regarded O
as O

Recently, O
VQA B-DAT
is O
advanced O
significantly O
by O
the O

basket?”. O
Answering O
this O
question O
requires O
VQA B-DAT
methods O
to O
first O
understand O
the O

The O
first O
feasible O
solution O
to O
VQA B-DAT
problems O
was O
provided O
by O
Malinowski O

Fritz O
also O
constructed O
the O
first O
VQA B-DAT
bench- O
mark O
dataset, O
named O
as O

trained O
and O
evaluated O
on O
the O
VQA B-DAT
problem O
[4,5,3]. O
More O
recently, O
Antol O

6] O
published O
the O
currently O
largest O
VQA B-DAT
dataset. O
It O
consists O
of O
three O

and O
real-world O
knowledge, O
making O
the O
VQA B-DAT
dataset O
suitable O
for O
a O
true O

Visual O
Turing O
Test. O
The O
VQA B-DAT
authors O
split O
the O
evaluation O
on O

the O
critical O
factors O
for O
solving O
VQA B-DAT
problems O
well. O
A O
common O
practice O

with O
existing O
VQA B-DAT
methods O
on O
model- O
ing O
image O

and O
thus O
hurt O
the O
overall O
VQA B-DAT
performance O

given O
question. O
Recall O
the O
above O
VQA B-DAT
example. O
To O
answer O
the O
question O

these O
regions O
of O
interest. O
Then O
VQA B-DAT
compliments O
the O
features O
from O
selected O

model O
on O
two O
types O
of O
VQA B-DAT
tasks, O
i.e., O
the O
open-ended O
task O

the O
multiple-choice O
task, O
on O
the O
VQA B-DAT
dataset O
– O
the O
largest O
VQA O

and O
on O
the O
multiple- O
choice O
VQA B-DAT
tasks O

2 O
we O
review O
the O
current O
VQA B-DAT
models, O
and O
compare O
them O
to O

VQA B-DAT
has O
received O
great O
research O
attention O

uses O
attention O
mechanism O
in O
solving O
VQA B-DAT
problems O
is O
the O
ABC-CNN O
model O

the O
different O
sub-tasks O
of O
the O
VQA B-DAT
problem O
(e.g. O
counting, O
locating O
an O

set O
and O
thus O
simplify O
the O
VQA B-DAT
task O
to O
a O
classification O
problem O

4 O
Focused O
Dynamic O
Attention O
for O
VQA B-DAT

for O
two O
images O
from O
the O
VQA B-DAT
dataset. O
Examples O
provided O
by O
[6 O

use O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset O
[6], O
which O
is O
the O

baseline O
models O
provided O
by O
the O
VQA B-DAT
dataset O
authors O
[27], O
which O
currently O

standard O
implementation O
of O
an O
LSTM+CNN O
VQA B-DAT
model. O
It O
uses O
an O
LSTM O

and O
our O
FDA O
model O
on O
VQA B-DAT
test-dev O
and O
test-standard O
data O
for O

VQA B-DAT
Question O
48.09 O
75.66 O
27.14 O
36.70 O

and O
our O
FDA O
model O
on O
VQA B-DAT
test-dev O
and O
test-standard O
data O
for O

VQA B-DAT
Question O
53.68 O
75.71 O
38.64 O
37.05 O

the O
baselines O
provided O
by O
the O
VQA B-DAT
authors O
[6]. O
The O
results O
for O

nificant O
when O
solving O
the O
multiple-choice O
VQA B-DAT
problems. O
From O
Table O
2, O
one O

best O
ever O
performance O
on O
the O
VQA B-DAT
dataset. O
In O
particular, O
it O
improves O

model O
to O
solve O
the O
challenging O
VQA B-DAT
problems. O
FDA O
is O
built O
upon O

improvement O
over O
baselines O
on O
the O
VQA B-DAT
benchmark O
datasets, O
for O
both O
the O

open-ended O
and O
multiple-choices O
VQA B-DAT
tasks. O
Excellent O
performance O
of O
FDA O

attention O
to O
visual O
part O
in O
VQA B-DAT
tasks O
could O
essentially O
improve O
the O

1], O
we O
use O
the O
top O
1000 B-DAT
most O
frequent O
answer O
as O
possible O

size O
is O
fixed O
to O
be O
100 B-DAT

Color, O
followed O
by O
1.3% O
and O
1.0 B-DAT

We O
randomly O
sample O
100 B-DAT
images O
from O
the O
COCO-QA O
test O

1505 B-DAT

1409 B-DAT

1406 B-DAT

1505 B-DAT

1308 B-DAT

1408 B-DAT

1105, B-DAT
2012. O
2 O

1506 B-DAT

1506 B-DAT

1505 B-DAT

1505 B-DAT

110 B-DAT

1409 B-DAT

1409 B-DAT

1502 B-DAT

and O
answers O
not O
only O
for O
real B-DAT
images, O
but O
also O
for O
abstract O

approach O
to O
question O
answering O
about O
real B-DAT

VQA B-DAT
is O
created O
through O
human O
labeling O

accuracy. O
The O
evaluation O
on O
the O
VQA B-DAT
data O
set O
is O
different O
from O

size O
to O
be O
640. O
For O
VQA B-DAT
dataset, O
since O
it O
is O
larger O

DAQUAR-ALL, O
DAQUAR- O
REDUCED, O
COCO-QA O
and O
VQA B-DAT
are O
presented O
in O
Table. O
1 O

VQA B-DAT

Table O
5: O
VQA B-DAT
results O
on O
the O
official O
server O

performance O
of O
various O
models O
on O
VQA, B-DAT
which O
is O
the O
largest O
among O

Table O
6: O
VQA B-DAT
results O
on O
our O
partition, O
in O

large O
data O
sets, O
COCO-QA O
and O
VQA, B-DAT
in O
Ta- O
ble. O
4 O
and O

similar O
trend O
of O
improvements O
on O
VQA B-DAT

that O
the O
Other O
type O
in O
VQA B-DAT
refers O
to O
questions O
that O
usually O

Loca- O
tion O
in O
COCO-QA. O
The O
VQA B-DAT
data O
set O
has O
a O
special O

Similar O
trends O
are O
observed O
on O
VQA B-DAT
(Table. O
6), O
e.g., O
the O
two-layer O

to O
detect O
words O
given O
the O
images, B-DAT
then O
used O
a O
maximum O
entropy O

of O
questions O
and O
answers O
given O
images B-DAT

answers O
not O
only O
for O
real O
images, B-DAT
but O
also O
for O
abstract O
scenes O

to O
predict O
answers O
based O
on O
images B-DAT
and O
questions. O
[19, O
7] O
both O

framework O
to O
generate O
answers O
given O
images B-DAT
and O
questions. O
They O
first O
used O

a O
LSTM O
to O
encoder O
the O
images B-DAT
and O
questions O
and O
then O
used O

to O
get O
the O
representation O
of O
images B-DAT

spatial O
information O
of O
the O
original O
images B-DAT

. O
We O
first O
rescale O
the O
images B-DAT
to O
be O
448×448 O

pixel O
region O
of O
the O
input O
images B-DAT

generated O
on O
795 O
and O
654 O
images B-DAT
respectively. O
The O

images B-DAT
are O
mainly O
indoor O
scenes. O
The O

and O
uses O
only O
25 O
test O
images B-DAT

8, O
000 O
and O
4, O
000 O
images B-DAT
respectively. O
There O
are O
four O
types O

1]. O
The O
data O
set O
uses O
images B-DAT
in O
the O
COCO O
image O
caption O

of O
a O
sample O
set O
of O
images B-DAT
from O
the O
COCO-QA O
test O
set O

For O
each O
example, O
the O
three O
images B-DAT
from O
left O
to O
right O
are O

We O
randomly O
sample O
100 O
images B-DAT
from O
the O
COCO-QA O
test O
set O

approach O
to O
answering O
questions O
about O
images B-DAT

Stacked O
Attention O
Networks O
for O
Image O
Question B-DAT
Answering O

Question B-DAT

3.2. O
Question B-DAT
Model O

Question B-DAT

Question B-DAT

VQA: O
[1] O
Question B-DAT
48.1 O
75.7 O
36.7 O
27.1 O

in O
Table. O
5, O
Q+I O
vs O
Question, B-DAT
and O
LSTM O
Q+I O
vs O
LSTM O

S. O
Chopra, O
and O
J. O
Weston. O
Question B-DAT
answering O
with O
subgraph O
embeddings. O
arXiv O

via O
staged O
query O
graph O
generation: O
Question B-DAT
answering O
with O
knowledge O
base. O
In O

Zitnick, O
and O
D. O
Parikh. O
Vqa: O
Visual B-DAT
question O
answering. O
arXiv O
preprint O
arXiv:1505.00468 O

is O
generated O
based O
on O
the O
COCO B-DAT
caption O
data O
set. O
Given O
a O

COCO B-DAT

21]. O
Based O
on O
the O
Microsoft O
COCO B-DAT
data O
set, O
the O
authors O
first O

set O
uses O
images O
in O
the O
COCO B-DAT
image O
caption O
data O
set O
[16 O

For O
DAQUAR O
and O
COCO B-DAT

results O
on O
DAQUAR-ALL, O
DAQUAR- O
REDUCED, O
COCO B-DAT

racy, O
respectively. O
On O
the O
larger O
COCO B-DAT

Table O
3: O
COCO B-DAT

Table O
4: O
COCO B-DAT

the O
two O
large O
data O
sets, O
COCO B-DAT

respectively. O
We O
observe O
that O
on O
COCO B-DAT

Objects O
and O
Loca- O
tion O
in O
COCO B-DAT

the O
one-layer O
SAN. O
Specifically, O
on O
COCO B-DAT

set O
of O
images O
from O
the O
COCO B-DAT

sample O
100 O
images O
from O
the O
COCO B-DAT

Attention O
Networks O
for O
Image O
Question O
Answering B-DAT

VQA B-DAT
is O
created O
through O
human O
labeling O

accuracy. O
The O
evaluation O
on O
the O
VQA B-DAT
data O
set O
is O
different O
from O

size O
to O
be O
640. O
For O
VQA B-DAT
dataset, O
since O
it O
is O
larger O

DAQUAR-ALL, O
DAQUAR- O
REDUCED, O
COCO-QA O
and O
VQA B-DAT
are O
presented O
in O
Table. O
1 O

VQA B-DAT

Table O
5: O
VQA B-DAT
results O
on O
the O
official O
server O

performance O
of O
various O
models O
on O
VQA, B-DAT
which O
is O
the O
largest O
among O

Table O
6: O
VQA B-DAT
results O
on O
our O
partition, O
in O

large O
data O
sets, O
COCO-QA O
and O
VQA, B-DAT
in O
Ta- O
ble. O
4 O
and O

similar O
trend O
of O
improvements O
on O
VQA B-DAT

that O
the O
Other O
type O
in O
VQA B-DAT
refers O
to O
questions O
that O
usually O

Loca- O
tion O
in O
COCO-QA. O
The O
VQA B-DAT
data O
set O
has O
a O
special O

Similar O
trends O
are O
observed O
on O
VQA B-DAT
(Table. O
6), O
e.g., O
the O
two-layer O

the O
SMem-VQA O
one-hop O
model O
achieves O
100 B-DAT

practice, O
we O
choose O
the O
top O
1000 B-DAT
answers O
in O
train O
and O
val O

whose O
answers O
belong O
to O
these O
1000 B-DAT
answers O
as O
train- O
ing O
data O

size, O
the O
embedding O
dimension O
is O
1000 B-DAT
on O
the O
VQA O
dataset. O
We O

1505 B-DAT

1505 B-DAT

1505 B-DAT

1503 B-DAT

1408 B-DAT

1406 B-DAT

1502 B-DAT

1409 B-DAT

1508 B-DAT

artificial O
intelligence. O
It O
has O
many O
real B-DAT

is O
then O
embedded O
into O
a O
real B-DAT

im- O
provement O
on O
these O
complicated O
real B-DAT

approach O
to O
question O
answering O
about O
real B-DAT

problem O
of O
Visual O
Question O
Answering O
(VQA), B-DAT
which O
requires O
joint O
image O
and O

and O
apply O
it O
to O
the O
VQA B-DAT
task. O
Memory O
networks O
are O
recurrent O

answering O
datasets, O
DAQUAR O
[1] O
and O
VQA B-DAT
[2], O
and O
obtain O
improved O
results O

Visual O
Question O
Answering O
(VQA) B-DAT
is O
an O
emerging O
interdisciplinary O
research O

popular O
image O
captioning O
task O
[6,7,8,9], O
VQA B-DAT
requires O
a O
deeper O
un- O
derstanding O

a O
Spatial O
Memory O
Network O
for O
VQA B-DAT
(SMem-VQA) O
that O
answers O
questions O
about O

model O
on O
examples O
from O
the O
VQA B-DAT
dataset O
[2]. O
In O
the O
first O

of O
the O
early O
works O
[1], O
VQA B-DAT
is O
seen O
as O
a O
Turing O

new O
deep O
learning O
approach O
to O
VQA B-DAT
that O
incorporates O
explicit O
spatial O
attention O

call O
the O
Spatial O
Memory O
Network O
VQA B-DAT
(SMem- O
VQA). O
Our O
approach O
is O

infer O
the O
answer. O
However, O
in O
VQA, B-DAT
the O
knowledge O
is O
in O
the O

with O
spatial O
attention O
for O
the O
VQA B-DAT
task O
which O
allows O
one O
to O

VQA B-DAT

results O
on O
DAQUAR O
[1] O
and O
VQA B-DAT
[2] O
datasets. O
Sec. O
5 O
concludes O

popularity O
of O
visual O
question O
answering O
(VQA), B-DAT
text O
question O
an- O
swering O
(QA O

this O
paper, O
we O
solve O
the O
VQA B-DAT
problem O
using O
a O
multi- O
modal O

VQA B-DAT
is O
related O
to O
image O
captioning O

. O
Several O
early O
papers O
about O
VQA B-DAT
directly O
adapt O
the O
image O
captioning O

models O
to O
solve O
the O
VQA B-DAT
problem O
[10][11] O
by O
gen- O
erating O

results O
on O
the O
more O
common O
VQA B-DAT
benchmark O
[2], O
and O
our O
own O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O
[2] O
on O
arxiv.org O
and O

The O
iBOWIMG O
model O
beats O
most O
VQA B-DAT
models O
consid- O
ered O
in O
the O

our O
proposed O
model O
to O
the O
VQA B-DAT
models O
(namely, O
the O
ACK O
model O

DPPnet O
model O
in O
[27] O
tackles O
VQA B-DAT
by O
learning O
a O
convolutional O
neural O

data O
in O
addition O
to O
the O
VQA B-DAT
dataset O
[2], O
e.g. O
the O
knowledge O

3 O
Spatial O
Memory O
Network O
for O
VQA B-DAT

VQA B-DAT
network, O
illustrated O
in O
Fig. O
2 O

a O
second O
hop O
into O
SMem- O
VQA B-DAT
network O

for O
Visual O
Question O
Answering O
(SMem- O
VQA B-DAT

the O
sequence-based O
LSTM O
for O
the O
VQA B-DAT
task. O
Specifically, O
we O
compute O

standard O
datasets O
(DAQUAR O
[1] O
and O
VQA B-DAT
[2] O
datasets) O
are O
reported O
in O

The O
questions O
in O
the O
public O
VQA B-DAT
datasets O
are O
quite O
varied O
and O

VQA B-DAT
one-hop O
model O
achieves O
100% O
test O

VQA B-DAT
one-hop O
model O
is O
equivalent O
to O

VQA B-DAT
model. O
We O
check O
the O
visualization O

VQA B-DAT
model O
can O
pay O
high O
attention O

VQA B-DAT
one-hop O
model O
achieves O
96% O
test O

VQA B-DAT
One-Hop O
36.03 O
SMem-VQA O
Two-Hop O
40.07 O

VQA B-DAT
model O
can O
infer O
the O
relative O

VQA B-DAT
One-Hop O
and O
Two-Hop O
models O
on O

VQA B-DAT
(top O
row) O
and O
DAQUAR O
(bottom O

VQA B-DAT
model O
on O
the O
DAQUAR O
dataset O

VQA B-DAT
One-Hop O
and O
Two-Hop O
models O
on O

Results O
on O
VQA B-DAT
The O
VQA O
dataset O
is O
a O
recent O
large O

dimension O
is O
1000 O
on O
the O
VQA B-DAT
dataset. O
We O
report O
the O
test-dev O

and O
test-standard O
results O
from O
the O
VQA B-DAT
evaluation O
server. O
The O
server O
evaluation O

test-standard O
results O
on O
the O
Open-Ended O
VQA B-DAT
dataset O
(in O
percentage). O
Models O
with O

data O
in O
addition O
to O
the O
VQA B-DAT
dataset O

VQA B-DAT
One-Hop O
56.56 O
78.98 O
35.93 O
42.09 O

VQA B-DAT
Two-Hop O
57.99 O
80.87 O
37.32 O
43.12 O

For O
the O
VQA B-DAT
dataset, O
we O
use O
the O
simple O

model, O
which O
beats O
most O
existing O
VQA B-DAT
models O
currently O
on O
arxiv.org. O
We O

well O
the O
best O
model O
in O
VQA B-DAT
dataset O
paper O
[2] O
are O
listed O

the O
best O
model O
in O
the O
VQA B-DAT
dataset O
paper. O
• O
ACK O
[26 O

VQA B-DAT
models O
and O
the O
four O
baseline O

models O
on O
VQA B-DAT
dataset O
are O
shown O
in O
Tab O

VQA B-DAT
One-Hop O
model O
obtains O
slightly O
better O

VQA B-DAT
Two-Hop O
model O
achieves O
an O
improvement O

VQA B-DAT
Two-Hop O
model O
also O
shows O
best O

VQA B-DAT
Two-Hop O
model O
has O
slightly O
better O

SMem-VQA B-DAT
Two-Hop O
model O
on O
the O
VQA O
dataset. O
Higher O
values O
in O
the O

into O
our O
model O
on O
the O
VQA B-DAT
dataset, O
but O
the O
result O
does O

VQA B-DAT
One-Hop O
and O
Two-Hop O
models O
on O

the O
VQA B-DAT
dataset O
are O
shown O
in O
Fig O

VQA B-DAT
Two-Hop O
model O
in O
Fig. O
6 O

the O
Spatial O
Memory O
Network O
for O
VQA, B-DAT
a O
memory O
network O
architecture O
with O

on O
the O
challenging O
DAQUAR O
and O
VQA B-DAT
datasets O
showed O
improved O
results O
over O

SMem-VQA B-DAT
model O
and O
exploring O
other O
VQA O
attention O
models O

D., O
Zitnick, O
C.L., O
Parikh, O
D.: O
VQA B-DAT

ended B-DAT
dataset, O
which O
con- O
tains O
a O

use O
the O
full O
release O
(V1.0) O
open B-DAT

SMem-VQA) O
that O
answers O
questions O
about O
images B-DAT
using O
spatial O
inference. O
The O
figure O

information O
and O
thus O
cannot O
distinguish O
images B-DAT
with O
a O
square O
in O
differ O

we O
collect O
all O
the O
cat O
images B-DAT
from O
the O
MS O
COCO O
Detection O

of O
the O
cat O
in O
the O
images B-DAT

We O
select O
2639 O
training O
cat O
images B-DAT
and O
1395 O
testing O
cat O
images O

cat. O
We O
also O
check O
the O
images B-DAT
where O
our O
model O
makes O
mistakes O

mis- O
takes O
mainly O
occur O
in O
images B-DAT
with O
more O
than O
one O
cats O

proach O
to O
answering O
questions O
about O
images B-DAT

Grounded O
question O
an- O
swering O
in O
images B-DAT

sup- O
port O
inference O
from O
rgbd O
images B-DAT

Ask, O
Attend O
and O
Answer: O
Exploring O
Question B-DAT

Question B-DAT
Answering O

address O
the O
problem O
of O
Visual O
Question B-DAT
Answering O
(VQA), O
which O
requires O
joint O

Keywords: O
Visual O
Question B-DAT
Answering, O
Spatial O
Attention, O
Memory O
Net O

Visual O
Question B-DAT
Answering O
(VQA) O
is O
an O
emerging O

recently O
been O
proposed O
for O
text O
Question B-DAT
Answering O
(QA) O
[12,13]. O
Memory O
networks O

Spatial O
Memory O
Network O
for O
Visual O
Question B-DAT
Answering O
(SMem- O
VQA). O
(a) O
Overview O

1] O
12.73 O
Neural-Image-QA O
[10] O
29.27 O
Question B-DAT
LSTM O
[10] O
32.32 O
VIS+LSTM O
[11 O

] O
34.41 O
Question B-DAT
BOW O
[11] O
32.67 O
IMG+BOW O
[11 O

of O
the O
encoding O
phase. O
• O
Question B-DAT
LSTM O
[10]: O
only O
shows O
the O

Question B-DAT
BOW O
[11]: O
only O
uses O
the O

either O
the O
LSTM O
model O
or O
Question B-DAT
BOW O
model O
does O
equally O
well O

A., O
Chopra, O
S., O
Weston, O
J.: O
Question B-DAT
answering O
with O
subgraph O
embeddings. O
arXiv O

Ask, O
Attend O
and O
Answer: O
Exploring O
Question B-DAT

-Guided O
Spatial O
Attention O
for O
Visual O
Question B-DAT
Answering O

Exploring O
Question-Guided O
Spatial O
Attention O
for O
Visual B-DAT

We O
address O
the O
problem O
of O
Visual B-DAT
Question O
Answering O
(VQA), O
which O
requires O

Keywords: O
Visual B-DAT
Question O
Answering, O
Spatial O
Attention, O
Memory O

Visual B-DAT
Question O
Answering O
(VQA) O
is O
an O

proposed O
Spatial O
Memory O
Network O
for O
Visual B-DAT
Question O
Answering O
(SMem- O
VQA). O
(a O

Exploring O
Question-Guided O
Spatial O
Attention O
for O
Visual B-DAT
Question O
Answering O

cat O
images O
from O
the O
MS O
COCO B-DAT
Detection O
dataset O
[30], O
and O
add O

testing O
cat O
images O
from O
MS O
COCO B-DAT
Detection O
dataset O

large O
dataset O
based O
on O
MS O
COCO B-DAT
[30]. O
We O
use O
the O
full O

Question O
Answering B-DAT

the O
problem O
of O
Visual O
Question O
Answering B-DAT
(VQA), O
which O
requires O
joint O
image O

Keywords: O
Visual O
Question O
Answering, B-DAT
Spatial O
Attention, O
Memory O
Net- O
work O

Visual O
Question O
Answering B-DAT
(VQA) O
is O
an O
emerging O
interdisciplinary O

been O
proposed O
for O
text O
Question O
Answering B-DAT
(QA) O
[12,13]. O
Memory O
networks O
combine O

Memory O
Network O
for O
Visual O
Question O
Answering B-DAT
(SMem- O
VQA). O
(a) O
Overview. O
First O

Spatial O
Attention O
for O
Visual O
Question O
Answering B-DAT

problem O
of O
Visual O
Question O
Answering O
(VQA), B-DAT
which O
requires O
joint O
image O
and O

and O
apply O
it O
to O
the O
VQA B-DAT
task. O
Memory O
networks O
are O
recurrent O

answering O
datasets, O
DAQUAR O
[1] O
and O
VQA B-DAT
[2], O
and O
obtain O
improved O
results O

Visual O
Question O
Answering O
(VQA) B-DAT
is O
an O
emerging O
interdisciplinary O
research O

popular O
image O
captioning O
task O
[6,7,8,9], O
VQA B-DAT
requires O
a O
deeper O
un- O
derstanding O

a O
Spatial O
Memory O
Network O
for O
VQA B-DAT
(SMem-VQA) O
that O
answers O
questions O
about O

model O
on O
examples O
from O
the O
VQA B-DAT
dataset O
[2]. O
In O
the O
first O

of O
the O
early O
works O
[1], O
VQA B-DAT
is O
seen O
as O
a O
Turing O

new O
deep O
learning O
approach O
to O
VQA B-DAT
that O
incorporates O
explicit O
spatial O
attention O

call O
the O
Spatial O
Memory O
Network O
VQA B-DAT
(SMem- O
VQA). O
Our O
approach O
is O

infer O
the O
answer. O
However, O
in O
VQA, B-DAT
the O
knowledge O
is O
in O
the O

with O
spatial O
attention O
for O
the O
VQA B-DAT
task O
which O
allows O
one O
to O

VQA B-DAT

results O
on O
DAQUAR O
[1] O
and O
VQA B-DAT
[2] O
datasets. O
Sec. O
5 O
concludes O

popularity O
of O
visual O
question O
answering O
(VQA), B-DAT
text O
question O
an- O
swering O
(QA O

this O
paper, O
we O
solve O
the O
VQA B-DAT
problem O
using O
a O
multi- O
modal O

VQA B-DAT
is O
related O
to O
image O
captioning O

. O
Several O
early O
papers O
about O
VQA B-DAT
directly O
adapt O
the O
image O
captioning O

models O
to O
solve O
the O
VQA B-DAT
problem O
[10][11] O
by O
gen- O
erating O

results O
on O
the O
more O
common O
VQA B-DAT
benchmark O
[2], O
and O
our O
own O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O
[2] O
on O
arxiv.org O
and O

The O
iBOWIMG O
model O
beats O
most O
VQA B-DAT
models O
consid- O
ered O
in O
the O

our O
proposed O
model O
to O
the O
VQA B-DAT
models O
(namely, O
the O
ACK O
model O

DPPnet O
model O
in O
[27] O
tackles O
VQA B-DAT
by O
learning O
a O
convolutional O
neural O

data O
in O
addition O
to O
the O
VQA B-DAT
dataset O
[2], O
e.g. O
the O
knowledge O

3 O
Spatial O
Memory O
Network O
for O
VQA B-DAT

VQA B-DAT
network, O
illustrated O
in O
Fig. O
2 O

a O
second O
hop O
into O
SMem- O
VQA B-DAT
network O

for O
Visual O
Question O
Answering O
(SMem- O
VQA B-DAT

the O
sequence-based O
LSTM O
for O
the O
VQA B-DAT
task. O
Specifically, O
we O
compute O

standard O
datasets O
(DAQUAR O
[1] O
and O
VQA B-DAT
[2] O
datasets) O
are O
reported O
in O

The O
questions O
in O
the O
public O
VQA B-DAT
datasets O
are O
quite O
varied O
and O

VQA B-DAT
one-hop O
model O
achieves O
100% O
test O

VQA B-DAT
one-hop O
model O
is O
equivalent O
to O

VQA B-DAT
model. O
We O
check O
the O
visualization O

VQA B-DAT
model O
can O
pay O
high O
attention O

VQA B-DAT
one-hop O
model O
achieves O
96% O
test O

VQA B-DAT
One-Hop O
36.03 O
SMem-VQA O
Two-Hop O
40.07 O

VQA B-DAT
model O
can O
infer O
the O
relative O

VQA B-DAT
One-Hop O
and O
Two-Hop O
models O
on O

VQA B-DAT
(top O
row) O
and O
DAQUAR O
(bottom O

VQA B-DAT
model O
on O
the O
DAQUAR O
dataset O

VQA B-DAT
One-Hop O
and O
Two-Hop O
models O
on O

Results O
on O
VQA B-DAT
The O
VQA O
dataset O
is O
a O
recent O
large O

dimension O
is O
1000 O
on O
the O
VQA B-DAT
dataset. O
We O
report O
the O
test-dev O

and O
test-standard O
results O
from O
the O
VQA B-DAT
evaluation O
server. O
The O
server O
evaluation O

test-standard O
results O
on O
the O
Open-Ended O
VQA B-DAT
dataset O
(in O
percentage). O
Models O
with O

data O
in O
addition O
to O
the O
VQA B-DAT
dataset O

VQA B-DAT
One-Hop O
56.56 O
78.98 O
35.93 O
42.09 O

VQA B-DAT
Two-Hop O
57.99 O
80.87 O
37.32 O
43.12 O

For O
the O
VQA B-DAT
dataset, O
we O
use O
the O
simple O

model, O
which O
beats O
most O
existing O
VQA B-DAT
models O
currently O
on O
arxiv.org. O
We O

well O
the O
best O
model O
in O
VQA B-DAT
dataset O
paper O
[2] O
are O
listed O

the O
best O
model O
in O
the O
VQA B-DAT
dataset O
paper. O
• O
ACK O
[26 O

VQA B-DAT
models O
and O
the O
four O
baseline O

models O
on O
VQA B-DAT
dataset O
are O
shown O
in O
Tab O

VQA B-DAT
One-Hop O
model O
obtains O
slightly O
better O

VQA B-DAT
Two-Hop O
model O
achieves O
an O
improvement O

VQA B-DAT
Two-Hop O
model O
also O
shows O
best O

VQA B-DAT
Two-Hop O
model O
has O
slightly O
better O

SMem-VQA B-DAT
Two-Hop O
model O
on O
the O
VQA O
dataset. O
Higher O
values O
in O
the O

into O
our O
model O
on O
the O
VQA B-DAT
dataset, O
but O
the O
result O
does O

VQA B-DAT
One-Hop O
and O
Two-Hop O
models O
on O

the O
VQA B-DAT
dataset O
are O
shown O
in O
Fig O

VQA B-DAT
Two-Hop O
model O
in O
Fig. O
6 O

the O
Spatial O
Memory O
Network O
for O
VQA, B-DAT
a O
memory O
network O
architecture O
with O

on O
the O
challenging O
DAQUAR O
and O
VQA B-DAT
datasets O
showed O
improved O
results O
over O

SMem-VQA B-DAT
model O
and O
exploring O
other O
VQA O
attention O
models O

D., O
Zitnick, O
C.L., O
Parikh, O
D.: O
VQA B-DAT

scenes. O
The O
set O
contains O
over O
100 B-DAT
objects O
and O
31 O
animals O
in O

i.e., O
an O
answer O
is O
deemed O
100 B-DAT

MS O
COCO O
dataset O
[32] O
and O
150, B-DAT

a O
scale O
of O
0 O
− O
100) B-DAT
required O
to O
answer O
a O
question O

choose O
the O
top O
K O
= O
1000 B-DAT
most O
frequent O
answers O
as O
possible O

Question O
(BoW O
Q): O
The O
top O
1,000 B-DAT
words O
in O
the O
questions O
are O

are O
concatenated O
to O
get O
a O
1,030 B-DAT

1000 B-DAT

1000 B-DAT

with O
2 O
hidden O
layers O
and O
1000 B-DAT
hidden O
units O
(dropout O
0.5) O
in O

a O
bag-of-words O
representation O
containing O
the O
1,000 B-DAT
most O
popular O
words O
in O
the O

that O
it O
has O
VQA O
accuracy O
1.0 B-DAT
(see O
section O
3 O
for O
accuracy O

Question O
K O
= O
1000 B-DAT
Human O
To O
Be O
Able O
To O

100 B-DAT
A O

100 B-DAT

100 B-DAT

100 B-DAT

can O
see O
that O
K O
= O
1000 B-DAT
performs O
better O
than O
K O

performs O
better O
then O
K O
= O
1000 B-DAT
by O
0.40% O
for O
open-ended O
task O

blue” O
(28881, O
1.16%), O
“4” O
(27174, O
1.09 B-DAT

outside” O
(1846, O
0.07%), O
“hot O
dog” O
(1809, B-DAT
0.07%), O
“night” O
(1805, O
0.07%), O
“trees O

and O
white” O
(1518, O
0.06%), O
“bedroom” O
(1500, B-DAT
0.06%), O
“bat” O
(1494, O
0.06%), O
“glasses O

0.06%), O
“cloudy” O
(1413, O
0.06%), O
“15” O
(1407, B-DAT
0.06%), O
“up” O
(1399, O
0.06%), O
“blonde O

0.05%), O
“many” O
(1211, O
0.05%), O
“zoo” O
(1204, B-DAT
0.05%), O
“suitcase” O
(1199, O
0.05%), O
“old O

0.04%), O
“mountains” O
(1030, O
0.04%), O
“wall” O
(1009, B-DAT
0.04%), O
“ele- O
phants” O
(1006, O
0.04 O

1504 B-DAT

1504 B-DAT

1408 B-DAT

1506 B-DAT

1409 B-DAT

1502 B-DAT

accurate O
natural O
language O
answer. O
Mirroring O
real B-DAT

removing O
the O
need O
to O
parse O
real B-DAT
images. O
Three O
questions O
were O
collected O

We O
begin O
by O
describing O
the O
real B-DAT
images O
and O
abstract O

Scenes. O
The O
VQA O
task O
with O
real B-DAT
images O
requires O
the O
use O
of O

2) O
that O
more O
closely O
mirror O
real B-DAT
images O
than O
previous O
papers O
[57 O

details, O
and O
examples. O
Splits. O
For O
real B-DAT
images, O
we O
follow O
the O
same O

was O
used O
for O
both O
the O
real B-DAT
images O
and O
abstract O
scenes. O
In O

red”, O
“blue”, O
“4”, O
“green” O
for O
real B-DAT
images. O
The O
inclusion O
of O
the O

sample O
of O
60K O
questions O
for O
real B-DAT
images O
(left) O
and O
all O
questions O

the O
questions O
for O
both O
the O
real B-DAT
images O
(left) O
and O
abstract O
scenes O

is O
quite O
similar O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
This O

to O
those O
elicited O
by O
the O
real B-DAT
images. O
There O
exists O
a O
surprising O

with O
different O
word O
lengths O
for O
real B-DAT
images O
and O
abstract O
scenes O

89.32%, O
6.91%, O
and O
2.74% O
for O
real B-DAT
images O
and O
90.51%, O
5.89%, O
and O

sample O
of O
60K O
questions O
for O
real B-DAT
images O
when O
subjects O
provide O
answers O

answers O
in O
our O
dataset O
for O
real B-DAT
images O
and O
3,770 O
for O
abstract O

40.66% O
of O
the O
questions O
on O
real B-DAT
images O
and O
abstract O
scenes O
respectively O

yes/no’ O
answers O
are O
“yes” O
for O
real B-DAT
images O
and O
abstract O
scenes. O
Question O

14.48% O
of O
the O
questions O
on O
real B-DAT
images O
and O
abstract O
scenes O
are O

of O
the O
‘number’ O
answers O
for O
real B-DAT
images O
and O
39.85% O
for O
abstract O

labeled O
as O
confident O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
Inter-human O

confident, O
1 O
= O
confident) O
for O
real B-DAT
images O
and O
abstract O
scenes O
(black O

in O
the O
answers O
for O
both O
real B-DAT
images O
(83.30%) O
and O
abstract O
scenes O

has O
2.70 O
unique O
answers O
for O
real B-DAT
images O
and O
2.39 O
for O
abstract O

subset O
10K O
questions O
from O
the O
real B-DAT
images O
of O
VQA O
trainval) O
asking O

p O
< O
.001) O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
See O

on O
the O
VQA O
test-dev O
for O
real B-DAT
images. O
Q O
= O
Question, O
I O

on O
the O
VQA O
test-dev O
for O
real B-DAT
images. O
As O
expected, O
the O
vision-alone O

for O
different O
question O
types O
on O
real B-DAT
images O
(Q+C O
is O
reported O
on O

on O
the O
VQA O
test-dev O
for O
real B-DAT
images. O
The O
different O
ablated O
versions O

on O
the O
VQA O
test-dev O
for O
real B-DAT
images. O
Q O
= O
Question, O
I O

real B-DAT
| O
oe-abstract O
| O
mc-real O

real B-DAT

real B-DAT

real B-DAT
and O
multiple-choice-real O
are O
shown O
in O

both O
open-ended O
and O
multiple-choice O
tasks O
(real B-DAT
images) O
with O
other O
entries O
(as O

data O
(MS O
COCO O
captions O
for O
real B-DAT
images O
and O
captions O
collected O
by O

p O
< O
.001) O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
This O

and O
Fig. O
17 O
(adjectives) O
for O
real B-DAT
images O
and O
Fig. O
18 O
(nouns O

in O
Fig. O
14 O
(left) O
for O
real B-DAT
images O
and O
Fig. O
14 O
(right O

and O
question O
& O
answers O
for O
real B-DAT
images O
(left) O
and O
abstract O
scenes O

indicating O
the O
normalized O
count O
for O
real B-DAT
images O

indicating O
the O
normalized O
count O
for O
real B-DAT
images O

indicating O
the O
normalized O
count O
for O
real B-DAT
images O

each O
of O
the O
two O
datasets, O
real B-DAT
and O
abstract, O
first O
two O
rows O

first O
five O
words O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
Note O

of O
3,000 O
questions O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
In O

better O
(≈ O
15% O
increase O
for O
real B-DAT
images O
and O
≈ O
11% O
increase O

words O
of O
questions O
in O
the O
real B-DAT
images O
training O
set O
and O
ensure O

type O
is O
also O
computed O
on O
real B-DAT
images O
training O
set. O
“nearest O
neighbor O

sample O
of O
60K O
questions O
for O
real B-DAT
images O
(left) O
and O
all O
questions O

sample O
of O
60K O
questions O
for O
real B-DAT
images O
(top) O
and O
all O
questions O

top O
250 O
answers O
in O
our O
real B-DAT
images O
dataset O
along O
with O
their O

numerous O
representative O
examples O
of O
the O
real B-DAT
image O
dataset O

numerous O
representative O
examples O
of O
the O
real B-DAT
and O
abstract O
scene O
dataset O

VQA B-DAT

and O
open-ended O
Visual O
Question O
Answering O
(VQA B-DAT

a O
system O
that O
succeeds O
at O
VQA B-DAT
typically O
needs O
a O
more O
detailed O

producing O
generic O
image O
captions. O
Moreover, O
VQA B-DAT
is O
amenable O
to O
automatic O
evaluation O

Numerous O
baselines O
and O
methods O
for O
VQA B-DAT
are O
provided O
and O
compared O
with O

human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

open- O
ended O
Visual O
Question O
Answering O
(VQA B-DAT

). O
A O
VQA B-DAT
system O
takes O
as O
input O
an O

Is O
this O
person O
expecting O
company?”). O
VQA B-DAT
[19], O
[36], O
[50], O
[3] O
is O

the O
high-level O
reasoning O
required O
for O
VQA B-DAT
by O
removing O
the O
need O
to O

29]. O
As O
part O
of O
the O
VQA B-DAT
initiative, O
we O
will O
organize O
an O

state-of-the-art O
methods O
and O
best O
practices. O
VQA B-DAT
poses O
a O
rich O
set O
of O

during O
the O
past O
few O
decades. O
VQA B-DAT
provides O
an O
attractive O
balance O
between O

VQA B-DAT
Efforts. O
Several O
recent O
papers O
have O

difficult O
and O
unconstrained O
task, O
our O
VQA B-DAT
dataset O
is O
two O
orders O
of O

1,449 O
images O
respectively). O
The O
proposed O
VQA B-DAT
task O
has O
connections O
to O
other O

These O
approaches O
provide O
inspiration O
for O
VQA B-DAT
techniques. O
One O
key O
concern O
in O

fixed O
set O
of O
loca- O
tions. O
VQA B-DAT
is O
naturally O
grounded O
in O
images O

Describing O
Visual O
Content. O
Related O
to O
VQA B-DAT
are O
the O
tasks O
of O
image O

by O
[53]). O
The O
questions O
in O
VQA B-DAT
require O
detailed O
specific O
information O
about O

3 O
VQA B-DAT
DATASET O
COLLECTION O

describe O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset. O
We O
begin O
by O
describing O

they O
are O
well-suited O
for O
our O
VQA B-DAT
task. O
The O
more O
diverse O
our O

their O
answers. O
Abstract O
Scenes. O
The O
VQA B-DAT
task O
with O
real O
images O
requires O

the O
high-level O
reasoning O
required O
for O
VQA, B-DAT
but O
not O
the O
low-level O
vision O

test-standard, O
test-challenge, O
test-reserve). O
For O
the O
VQA B-DAT
challenge O
(see O
section O
6), O
test-dev O

default’ O
test O
data O
for O
the O
VQA B-DAT
competition. O
When O
comparing O
to O
the O

sentences O
containing O
multiple O
words. O
In O
VQA, B-DAT
most O
answers O
(89.32%) O
are O
single O

4 O
VQA B-DAT
DATASET O
ANALYSIS O
In O
this O
section O

questions O
and O
answers O
in O
the O
VQA B-DAT
train O
dataset. O
To O
gain O
an O

visual O
information O
is O
critical O
to O
VQA B-DAT
and O
that O
commonsense O
information O
alone O

from O
the O
real O
images O
of O
VQA B-DAT
trainval) O
asking O
subjects O

5 O
VQA B-DAT
BASELINES O
AND O
METHODS O
In O
this O

explore O
the O
difficulty O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

novel O
methods. O
We O
train O
on O
VQA B-DAT
train+val. O
Unless O
stated O
otherwise, O
all O

top O
1K O
answers O
of O
the O
VQA B-DAT
train/val O
dataset O

multiple- O
choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
As O

I O
(Fig. O
8), O
selected O
using O
VQA B-DAT
test-dev O
accuracies) O
on O
VQA O
test O

worse O
than O
human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

system O
is O
correct O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

correct” O
implies O
that O
it O
has O
VQA B-DAT
accuracy O
1.0 O
(see O
section O
3 O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
The O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

frequently O
predicted O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

age O
of O
question) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

system O
is O
correct) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

a O
filtered O
version O
of O
the O
VQA B-DAT
train O
+ O
val O
dataset O
in O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

6 O
VQA B-DAT
CHALLENGE O
AND O
WORKSHOP O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O

task O
of O
Visual O
Question O
Answering O
(VQA B-DAT

multiple-choice O
tasks O
in O
the O
respective O
VQA B-DAT
Real O
Image O
Challenge O
leaderboards O
(as O

datasets O
may O
help O
enable O
practical O
VQA B-DAT
applications. O
We O
believe O
VQA O
has O

questions O
IV O
- O
Details O
on O
VQA B-DAT
baselines O
V O
- O
“Age” O
and O

Leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Open-Ended) O
on O

leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Multiple-Choice) O
on O

Additional O
examples O
from O
the O
VQA B-DAT
dataset O

scenes. O
This O
helps O
motivate O
the O
VQA B-DAT
task O
as O
a O
way O
to O

APPENDIX O
IV: O
DETAILS O
ON O
VQA B-DAT
BASELINES O
“per O
Q-type O
prior” O
baseline O

For O
every O
question O
in O
the O
VQA B-DAT
test-standard O
set, O
we O
find O
its O

norm O
I), O
selected O
using O
VQA B-DAT
test- O
dev O
accuracies). O
To O
estimate O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

a O
random O
selection O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

ended B-DAT
Visual O
Question O
Answering O
(VQA). O
Given O

ended B-DAT

ended B-DAT
answers O
contain O
only O
a O
few O

task O
of O
free-form O
and O
open- O
ended B-DAT
Visual O
Question O
Answering O
(VQA). O
A O

ended, B-DAT
natural- O
language O
question O
about O
the O

ended B-DAT
questions O
require O
a O
potentially O
vast O

ended B-DAT
questions O
collected O
for O
images O
via O

ended B-DAT
answering O
task O
and O
a O
multiple O

ended B-DAT
task O
that O
requires O
a O
free-form O

ended B-DAT
questions O
offers O
many O
benefits, O
it O

ended, B-DAT
free-form O
questions O
and O
answers O
provided O

ended B-DAT
questions O
result O
in O
a O
diverse O

ended B-DAT
and O
(ii) O
multiple-choice. O
For O
the O

ended B-DAT
task, O
the O
generated O
answers O
are O

ended B-DAT
task, O
the O
accuracy O
of O
a O

ended B-DAT
answers O
to O
open-ended O
questions. O
The O

ended B-DAT
and O
multiple-choice O
tasks. O
Note O
that O

ended B-DAT
task, O
we O
pick O
the O
most O

ended B-DAT
task O
using O
cosine O
similarity O
in O

ended B-DAT
task, O
we O
pick O
the O
most O

ended B-DAT
task O
using O
cosine O
similarity O
in O

on O
two O
different O
tasks: O
open- O
ended B-DAT
selects O
the O
answer O
with O
highest O

ended B-DAT
and O
multiple- O
choice O
tasks O
on O

ended B-DAT
and O
multiple-choice O
tasks O
on O
the O

ended B-DAT

ended B-DAT
task, O
the O
vision-alone O
model O
(I O

ended B-DAT
(53.68% O
on O
multiple-choice) O
and O
LSTM O

Q O
achieving O
48.76% O
on O
open- O
ended B-DAT
(54.75% O
on O
multiple-choice); O
both O
outperforming O

ended B-DAT

ended) B-DAT
/ O
63.09% O
(multiple-choice). O
We O
can O

ended B-DAT

ended B-DAT
test-dev O
results O
for O
different O
question O

I) O
for O
both O
the O
open- O
ended B-DAT
and O
multiple-choice O
tasks O
on O
the O

ended B-DAT
task O
and O
by O
0.24% O
for O

ended B-DAT
task O
and O
by O
1.24% O
for O

ended B-DAT
task O
and O
by O
1.92% O
for O

ended B-DAT
task O
and O
by O
1.16% O
for O

words O
by O
0.24% O
for O
open- O
ended B-DAT
task O
and O
by O
0.17% O
for O

ended B-DAT
task O
and O
by O
0.02% O
for O

ended B-DAT
and O
multiple-choice O
tasks O
on O
the O

ended B-DAT
task O
and O
by O
1.88% O
for O

ended B-DAT

ended B-DAT
and O
multiple-choice O
tasks O
(real O
images O

ended, B-DAT
natural O

ended B-DAT
and O
multiple-choice O
tasks O
in O
the O

ended B-DAT
and O
not O
task-specific. O
For O
some O

ended B-DAT
questions O
that O
are O
answered O
by O

ended B-DAT
answers O
task O
when O
subjects O
were O

ended B-DAT
answer O
task. O
In O
comparison O
to O

open- O
ended B-DAT
answer, O
the O
multiple-choice O
accuracies O
are O

the O
task O
of O
free-form O
and O
open B-DAT

the O
questions O
and O
answers O
are O
open B-DAT

to O
automatic O
evaluation, O
since O
many O
open B-DAT

is O
still O
a O
difficult O
and O
open B-DAT
research O
problem O
[51], O
[13], O
[22 O

the O
task O
of O
free-form O
and O
open B-DAT

an O
image O
and O
a O
free-form, O
open B-DAT

Fig. O
1: O
Examples O
of O
free-form, O
open B-DAT

paper, O
we O
present O
both O
an O
open B-DAT

task O
[45], O
[33]. O
Unlike O
the O
open B-DAT

answers. O
While O
the O
use O
of O
open B-DAT

contrast, O
our O
proposed O
task O
involves O
open B-DAT

tions: O
(i) O
open B-DAT

and O
(ii) O
multiple-choice. O
For O
the O
open B-DAT

each O
question. O
As O
with O
the O
open B-DAT

recall O
that O
they O
are O
human-provided O
open B-DAT

-ended O
answers O
to O
open B-DAT

answer O
(“yes”) O
for O
both O
the O
open B-DAT

per O
Q-type O
prior: O
For O
the O
open B-DAT

the O
picked O
answer O
for O
the O
open B-DAT

are O
found. O
Next, O
for O
the O
open B-DAT

the O
picked O
answer O
for O
the O
open B-DAT

result O
on O
two O
different O
tasks: O
open B-DAT

of O
our O
methods O
for O
the O
open B-DAT

and O
methods O
for O
both O
the O
open B-DAT

the O
question O
performs O
rather O
poorly O
(open B-DAT

multiple-choice: O
30.53%). O
In O
fact, O
on O
open B-DAT

BoW O
Q O
achieving O
48.09% O
on O
open B-DAT

LSTM O
Q O
achieving O
48.76% O
on O
open B-DAT

outperforming O
the O
nearest O
neighbor O
baseline O
(open B-DAT

VQA O
test- O
standard O
is O
58.16% O
(open B-DAT

on O
multiple-choice O
are O
better O
than O
open B-DAT

norm O
I) O
for O
both O
the O
open B-DAT

the O
performance O
by O
0.16% O
for O
open B-DAT

performs O
better O
by O
0.95% O
for O
open B-DAT

500 O
by O
0.82% O
for O
open B-DAT

1000 O
by O
0.40% O
for O
open B-DAT

questions O
words O
by O
0.24% O
for O
open B-DAT

questions O
words O
by O
0.06% O
for O
open B-DAT

norm O
I) O
for O
the O
open B-DAT

performs O
worse O
by O
1.13% O
for O
open B-DAT

page5. O
Screenshots O
of O
leaderboards O
for O
open B-DAT

norm O
I) O
for O
both O
open B-DAT

Given O
an O
image O
and O
an O
open B-DAT

of O
other O
entries O
for O
the O
open B-DAT

from O
our O
human O
subjects O
were O
open B-DAT

to O
capture O
images O
and O
ask O
open B-DAT

is O
the O
inter-human O
agreement O
for O
open B-DAT

shows O
the O
inter-human O
agreement O
for O
open B-DAT

answer O
task. O
In O
comparison O
to O
open B-DAT

open B-DAT

provide O
a O
dataset O
containing O
∼0.25M O
images, B-DAT
∼0.76M O
questions, O
and O
∼10M O
answers O

free-form, O
open-ended O
questions O
collected O
for O
images B-DAT
via O
Amazon O
Mechanical O
Turk. O
Note O

vision?”). O
Moreover, O
since O
questions O
about O
images B-DAT
often O
tend O
to O
seek O
specific O

large O
dataset O
that O
contains O
204,721 O
images B-DAT
from O
the O
MS O
COCO O
dataset O

The O
MS O
COCO O
dataset O
has O
images B-DAT
depicting O
diverse O
and O
complex O
scenes O

the O
need O
to O
parse O
real O
images B-DAT

250,000 O
vs. O
2,591 O
and O
1,449 O
images B-DAT
respectively). O
The O
proposed O
VQA O
task O

introduced O
a O
dataset O
of O
10k O
images B-DAT
and O
prompted O
captions O
that O
describe O

English O
by O
humans) O
for O
COCO O
images B-DAT

VQA O
is O
naturally O
grounded O
in O
images B-DAT
– O
requiring O
the O
understanding O
of O

both O
text O
(questions) O
and O
vision O
(images B-DAT

begin O
by O
describing O
the O
real O
images B-DAT
and O
abstract O

im- O
ages O
and O
81,434 O
test O
images B-DAT
from O
the O
newly-released O
Microsoft O
Common O

dataset O
was O
gathered O
to O
find O
images B-DAT
containing O
multiple O
objects O
and O
rich O

the O
visual O
complexity O
of O
these O
images, B-DAT
they O
are O
well-suited O
for O
our O

more O
diverse O
our O
collection O
of O
images, B-DAT
the O
more O
diverse, O
comprehensive, O
and O

The O
VQA O
task O
with O
real O
images B-DAT
requires O
the O
use O
of O
complex O

that O
more O
closely O
mirror O
real O
images B-DAT
than O
previous O
papers O
[57], O
[58 O

and O
examples. O
Splits. O
For O
real O
images, B-DAT
we O
follow O
the O
same O
train/val/test O

five O
single-sentence O
captions O
for O
all O
images B-DAT

It O
understands O
a O
lot O
about O
images B-DAT

used O
for O
both O
the O
real O
images B-DAT
and O
abstract O
scenes. O
In O
total O

blue”, O
“4”, O
“green” O
for O
real O
images B-DAT

of O
60K O
questions O
for O
real O
images B-DAT
(left) O
and O
all O
questions O
for O

at O
the O
image) O
for O
204,721 O
images B-DAT
from O
the O
MS O
COCO O
dataset O

questions O
for O
both O
the O
real O
images B-DAT
(left) O
and O
abstract O
scenes O
(right O

quite O
similar O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
This O
helps O

those O
elicited O
by O
the O
real O
images B-DAT

different O
word O
lengths O
for O
real O
images B-DAT
and O
abstract O
scenes O

6.91%, O
and O
2.74% O
for O
real O
images B-DAT
and O
90.51%, O
5.89%, O
and O
2.49 O

elicit O
specific O
information O
from O
the O
images B-DAT

of O
60K O
questions O
for O
real O
images B-DAT
when O
subjects O
provide O
answers O
when O

in O
our O
dataset O
for O
real O
images B-DAT
and O
3,770 O
for O
abstract O
scenes O

of O
the O
questions O
on O
real O
images B-DAT
and O
abstract O
scenes O
respectively. O
Among O

answers O
are O
“yes” O
for O
real O
images B-DAT
and O
abstract O
scenes. O
Question O
types O

of O
the O
questions O
on O
real O
images B-DAT
and O
abstract O
scenes O
are O
‘number O

the O
‘number’ O
answers O
for O
real O
images B-DAT
and O
39.85% O
for O
abstract O
scenes O

as O
confident O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
Inter-human O
Agreement O

1 O
= O
confident) O
for O
real O
images B-DAT
and O
abstract O
scenes O
(black O
lines O

the O
answers O
for O
both O
real O
images B-DAT
(83.30%) O
and O
abstract O
scenes O
(87.49 O

2.70 O
unique O
answers O
for O
real O
images B-DAT
and O
2.39 O
for O
abstract O
scenes O

answers O
provided O
with O
and O
without O
images, B-DAT
we O
show O
the O
distribution O
of O

for O
answers O
with O
and O
without O
images B-DAT

10K O
questions O
from O
the O
real O
images B-DAT
of O
VQA O
trainval) O
asking O
subjects O

of O
3K O
train O
questions O
(1K O
images B-DAT

001) O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
See O
the O

dataset O
for O
the O
MS O
COCO O
images B-DAT
using O
several O
baselines O
and O
novel O

nearest O
neighbor O
questions O
and O
associated O
images B-DAT
from O
the O
training O
set. O
See O

VGGNet O
[48] O
to O
encode O
the O
images B-DAT

the O
VQA O
test-dev O
for O
real O
images B-DAT

the O
VQA O
test-dev O
for O
real O
images B-DAT

different O
question O
types O
on O
real O
images B-DAT
(Q+C O
is O
reported O
on O
val O

the O
VQA O
test-dev O
for O
real O
images B-DAT

the O
VQA O
test-dev O
for O
real O
images B-DAT

open-ended O
and O
multiple-choice O
tasks O
(real O
images) B-DAT
with O
other O
entries O
(as O
of O

a O
dataset O
containing O
over O
250K O
images, B-DAT
760K O
questions, O
and O
around O
10M O

the O
visually O
impaired O
to O
capture O
images B-DAT
and O
ask O
open-ended O
questions O
that O

MS O
COCO O
captions O
for O
real O
images B-DAT
and O
captions O
collected O
by O
us O

001) O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
This O
helps O

Fig. O
17 O
(adjectives) O
for O
real O
images B-DAT
and O
Fig. O
18 O
(nouns), O
Fig O

Fig. O
14 O
(left) O
for O
real O
images B-DAT
and O
Fig. O
14 O
(right) O
for O

question O
& O
answers O
for O
real O
images B-DAT
(left) O
and O
abstract O
scenes O
(right O

the O
normalized O
count O
for O
real O
images B-DAT

the O
normalized O
count O
for O
real O
images B-DAT

the O
normalized O
count O
for O
real O
images B-DAT

five O
words O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
Note O
the O

3,000 O
questions O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
In O
Table O

15% O
increase O
for O
real O
images B-DAT
and O
≈ O
11% O
increase O
for O

of O
questions O
in O
the O
real O
images B-DAT
training O
set O
and O
ensure O
that O

is O
also O
computed O
on O
real O
images B-DAT
training O
set. O
“nearest O
neighbor” O
baseline O

k O
questions O
and O
their O
associated O
images, B-DAT
we O
find O
the O
image O
which O

used O
to O
collect O
questions O
for O
images B-DAT

subjects O
were O
shown O
the O
corresponding O
images B-DAT

of O
60K O
questions O
for O
real O
images B-DAT
(left) O
and O
all O
questions O
for O

of O
60K O
questions O
for O
real O
images B-DAT
(top) O
and O
all O
questions O
for O

250 O
answers O
in O
our O
real O
images B-DAT
dataset O
along O
with O
their O
counts O

for O
the O
MS O
COCO O
[32] O
images, B-DAT
abstract O
scenes, O
and O
multiple-choice O
questions O

approach O
to O
answering O
questions O
about O
images B-DAT

VQA: O
Visual O
Question B-DAT
Answering O
www.visualqa.org O

of O
free-form O
and O
open-ended O
Visual O
Question B-DAT
Answering O
(VQA). O
Given O
an O
image O

free-form O
and O
open- O
ended O
Visual O
Question B-DAT
Answering O
(VQA). O
A O
VQA O
system O

We O
now O
describe O
the O
Visual O
Question B-DAT
Answering O
(VQA) O
dataset. O
We O
begin O

Types O
of O
Question B-DAT

of O
Words O
in O
Question B-DAT

Distribution O
of O
Question B-DAT
Lengths O

real O
images O
and O
abstract O
scenes. O
Question B-DAT
types O
such O
as O
“How O
many O

As O
shown O
in O
Table O
1 O
(Question B-DAT
+ O
Image), O
there O
is O
significant O

Fig. O
2). O
In O
Table O
1 O
(Question), B-DAT
we O
show O
the O
percentage O
of O

Question B-DAT
40.81 O
67.60 O
25.77 O
21.22 O
Real O

Question B-DAT
+ O
Caption* O
57.47 O
78.97 O
39.68 O

Question B-DAT
+ O
Image O
83.30 O
95.77 O
83.39 O

Question B-DAT
43.27 O
66.65 O
28.52 O
23.66 O
Abstract O

Question B-DAT
+ O
Caption* O
54.34 O
74.70 O
41.19 O

Question B-DAT
+ O
Image O
87.49 O
95.96 O
95.04 O

question O
without O
seeing O
the O
image O
(Question), B-DAT
seeing O
just O
a O
caption O
of O

and O
not O
the O
image O
itself O
(Question B-DAT
+ O
Caption), O
and O
seeing O
the O

image O
(Question B-DAT
+ O
Image). O
Results O
are O
shown O

answer O
the O
questions? O
Table O
1 O
(Question B-DAT
+ O
Caption) O
shows O
the O
percentage O

Question B-DAT
Channel: O
This O
channel O
provides O
an O

1) O
Bag-of-Words O
Question B-DAT
(BoW O
Q): O
The O
top O
1,000 O

caption O
embedding O
(Caption). O
For O
BoW O
Question B-DAT
+ O
Caption O
(BoW O
Q O

for O
real O
images. O
Q O
= O
Question, B-DAT
I O
= O
Image, O
C O

Question B-DAT
K O
= O
1000 O
Human O
To O

for O
real O
images. O
Q O
= O
Question, B-DAT
I O
= O
Image. O
See O
text O

introduce O
the O
task O
of O
Visual O
Question B-DAT
Answering O
(VQA). O
Given O
an O
image O

Etzioni. O
Paraphrase-Driven O
Learning O
for O
Open O
Question B-DAT
Answering. O
In O
ACL, O
2013. O
2 O

Zettlemoyer, O
and O
O. O
Etzioni. O
Open O
Question B-DAT
Answering O
over O
Curated O
and O
Extracted O

Fritz. O
A O
Multi-World O
Approach O
to O
Question B-DAT
Answering O
about O
Real-World O
Scenes O
based O

T. O
Mikolov. O
Towards O
AI- O
Complete O
Question B-DAT
Answering: O
A O
Set O
of O
Prerequisite O

VQA: O
Visual B-DAT
Question O
Answering O
www.visualqa.org O

task O
of O
free-form O
and O
open-ended O
Visual B-DAT
Question O
Answering O
(VQA). O
Given O
an O

questions O
and O
answers O
are O
open-ended. O
Visual B-DAT
questions O
selectively O
target O
different O
areas O

of O
free-form O
and O
open- O
ended O
Visual B-DAT
Question O
Answering O
(VQA). O
A O
VQA O

complex O
reasoning O
more O
essential. O
Describing O
Visual B-DAT
Content. O
Related O
to O
VQA O
are O

We O
now O
describe O
the O
Visual B-DAT
Question O
Answering O
(VQA) O
dataset. O
We O

we O
introduce O
the O
task O
of O
Visual B-DAT
Question O
Answering O
(VQA). O
Given O
an O

cloud O
service. O
In O
Mobile O
Cloud O
Visual B-DAT
Media O
Computing, O
pages O
265–290. O
Springer O

D. O
Parikh. O
Zero-Shot O
Learning O
via O
Visual B-DAT
Abstraction. O
In O
ECCV, O
2014. O
2 O

Nearly O
Real- O
time O
Answers O
to O
Visual B-DAT
Questions. O
In O
User O
Interface O
Software O

and O
A. O
Gupta. O
NEIL: O
Extracting O
Visual B-DAT
Knowledge O
from O
Web O
Data. O
In O

Zitnick. O
Mind’s O
Eye: O
A O
Recurrent O
Visual B-DAT
Represen- O
tation O
for O
Image O
Caption O

Long-term O
Recurrent O
Convolutional O
Networks O
for O
Visual B-DAT
Recognition O
and O
Description. O
In O
CVPR O

G. O
Zweig. O
From O
Captions O
to O
Visual B-DAT
Concepts O
and O
Back. O
In O
CVPR O

Hallonquist, O
and O
L. O
Younes. O
A O
Visual B-DAT
Turing O
Test O
for O
Computer O
Vision O

Karpathy O
and O
L. O
Fei-Fei. O
Deep O
Visual B-DAT

and O
R. O
S. O
Zemel. O
Unifying O
Visual B-DAT

Listen, O
Use O
Your O
Imagination: O
Leveraging O
Visual B-DAT
Common O
Sense O
for O
Non-Visual O
Tasks O

Divvala, O
and O
A. O
Farhadi. O
Viske: O
Visual B-DAT
knowledge O
extraction O
and O
question O
answering O

Berg, O
and O
T. O
L. O
Berg. O
Visual B-DAT
madlibs: O
Fill-in-the- O
blank O
description O
generation O

Bringing O
Semantics O
Into O
Focus O
Using O
Visual B-DAT
Abstraction. O
In O
CVPR, O
2013. O
2 O

and O
L. O
Vanderwende. O
Learning O
the O
Visual B-DAT
Interpretation O
of O
Sentences. O
In O
ICCV O

204,721 O
images O
from O
the O
MS O
COCO B-DAT
dataset O
[32] O
and O
a O
newly O

contains O
50,000 O
scenes. O
The O
MS O
COCO B-DAT
dataset O
has O
images O
depicting O
diverse O

to O
English O
by O
humans) O
for O
COCO B-DAT
images. O
[44] O
automatically O
generated O
four O

object, O
count, O
color, O
location) O
using O
COCO B-DAT
captions. O
Text-based O
Q&A O
is O
a O

Common O
Objects O
in O
Context O
(MS O
COCO) B-DAT
[32] O
dataset. O
The O
MS O
COCO O

split O
strategy O
as O
the O
MC O
COCO B-DAT
dataset O
[32] O
(including O
test- O
dev O

abstract O
scenes. O
Captions. O
The O
MS O
COCO B-DAT
dataset O
[32], O
[7] O
already O
contains O

204,721 O
images O
from O
the O
MS O
COCO B-DAT
dataset O
[32] O
and O
150,000 O
questions O

VQA O
dataset O
for O
the O
MS O
COCO B-DAT
images O
using O
several O
baselines O
and O

from O
the O
caption O
data O
(MS O
COCO B-DAT
captions O
for O
real O
images O
and O

VQA O
dataset O
for O
the O
MS O
COCO B-DAT
[32] O
images, O
abstract O
scenes, O
and O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT
captions: O
Data O
collection O
and O
evaluation O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT
Captions: O
Data O
Collection O
and O
Evaluation O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT

VQA: O
Visual O
Question O
Answering B-DAT
www.visualqa.org O

free-form O
and O
open-ended O
Visual O
Question O
Answering B-DAT
(VQA). O
Given O
an O
image O
and O

and O
open- O
ended O
Visual O
Question O
Answering B-DAT
(VQA). O
A O
VQA O
system O
takes O

now O
describe O
the O
Visual O
Question O
Answering B-DAT
(VQA) O
dataset. O
We O
begin O
by O

the O
task O
of O
Visual O
Question O
Answering B-DAT
(VQA). O
Given O
an O
image O
and O

Paraphrase-Driven O
Learning O
for O
Open O
Question O
Answering B-DAT

and O
O. O
Etzioni. O
Open O
Question O
Answering B-DAT
over O
Curated O
and O
Extracted O
Knowledge O

A O
Multi-World O
Approach O
to O
Question O
Answering B-DAT
about O
Real-World O
Scenes O
based O
on O

Parsing O
for O
Understanding O
Events O
and O
Answering B-DAT
Queries. O
IEEE O
MultiMedia, O
2014. O
1 O

Mikolov. O
Towards O
AI- O
Complete O
Question O
Answering B-DAT

VQA B-DAT

and O
open-ended O
Visual O
Question O
Answering O
(VQA B-DAT

a O
system O
that O
succeeds O
at O
VQA B-DAT
typically O
needs O
a O
more O
detailed O

producing O
generic O
image O
captions. O
Moreover, O
VQA B-DAT
is O
amenable O
to O
automatic O
evaluation O

Numerous O
baselines O
and O
methods O
for O
VQA B-DAT
are O
provided O
and O
compared O
with O

human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

open- O
ended O
Visual O
Question O
Answering O
(VQA B-DAT

). O
A O
VQA B-DAT
system O
takes O
as O
input O
an O

Is O
this O
person O
expecting O
company?”). O
VQA B-DAT
[19], O
[36], O
[50], O
[3] O
is O

the O
high-level O
reasoning O
required O
for O
VQA B-DAT
by O
removing O
the O
need O
to O

29]. O
As O
part O
of O
the O
VQA B-DAT
initiative, O
we O
will O
organize O
an O

state-of-the-art O
methods O
and O
best O
practices. O
VQA B-DAT
poses O
a O
rich O
set O
of O

during O
the O
past O
few O
decades. O
VQA B-DAT
provides O
an O
attractive O
balance O
between O

VQA B-DAT
Efforts. O
Several O
recent O
papers O
have O

difficult O
and O
unconstrained O
task, O
our O
VQA B-DAT
dataset O
is O
two O
orders O
of O

1,449 O
images O
respectively). O
The O
proposed O
VQA B-DAT
task O
has O
connections O
to O
other O

These O
approaches O
provide O
inspiration O
for O
VQA B-DAT
techniques. O
One O
key O
concern O
in O

fixed O
set O
of O
loca- O
tions. O
VQA B-DAT
is O
naturally O
grounded O
in O
images O

Describing O
Visual O
Content. O
Related O
to O
VQA B-DAT
are O
the O
tasks O
of O
image O

by O
[53]). O
The O
questions O
in O
VQA B-DAT
require O
detailed O
specific O
information O
about O

3 O
VQA B-DAT
DATASET O
COLLECTION O

describe O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset. O
We O
begin O
by O
describing O

they O
are O
well-suited O
for O
our O
VQA B-DAT
task. O
The O
more O
diverse O
our O

their O
answers. O
Abstract O
Scenes. O
The O
VQA B-DAT
task O
with O
real O
images O
requires O

the O
high-level O
reasoning O
required O
for O
VQA, B-DAT
but O
not O
the O
low-level O
vision O

test-standard, O
test-challenge, O
test-reserve). O
For O
the O
VQA B-DAT
challenge O
(see O
section O
6), O
test-dev O

default’ O
test O
data O
for O
the O
VQA B-DAT
competition. O
When O
comparing O
to O
the O

sentences O
containing O
multiple O
words. O
In O
VQA, B-DAT
most O
answers O
(89.32%) O
are O
single O

4 O
VQA B-DAT
DATASET O
ANALYSIS O
In O
this O
section O

questions O
and O
answers O
in O
the O
VQA B-DAT
train O
dataset. O
To O
gain O
an O

visual O
information O
is O
critical O
to O
VQA B-DAT
and O
that O
commonsense O
information O
alone O

from O
the O
real O
images O
of O
VQA B-DAT
trainval) O
asking O
subjects O

5 O
VQA B-DAT
BASELINES O
AND O
METHODS O
In O
this O

explore O
the O
difficulty O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

novel O
methods. O
We O
train O
on O
VQA B-DAT
train+val. O
Unless O
stated O
otherwise, O
all O

top O
1K O
answers O
of O
the O
VQA B-DAT
train/val O
dataset O

multiple- O
choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
As O

I O
(Fig. O
8), O
selected O
using O
VQA B-DAT
test-dev O
accuracies) O
on O
VQA O
test O

worse O
than O
human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

system O
is O
correct O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

correct” O
implies O
that O
it O
has O
VQA B-DAT
accuracy O
1.0 O
(see O
section O
3 O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
The O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

frequently O
predicted O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

age O
of O
question) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

system O
is O
correct) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

a O
filtered O
version O
of O
the O
VQA B-DAT
train O
+ O
val O
dataset O
in O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

6 O
VQA B-DAT
CHALLENGE O
AND O
WORKSHOP O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O

task O
of O
Visual O
Question O
Answering O
(VQA B-DAT

multiple-choice O
tasks O
in O
the O
respective O
VQA B-DAT
Real O
Image O
Challenge O
leaderboards O
(as O

datasets O
may O
help O
enable O
practical O
VQA B-DAT
applications. O
We O
believe O
VQA O
has O

questions O
IV O
- O
Details O
on O
VQA B-DAT
baselines O
V O
- O
“Age” O
and O

Leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Open-Ended) O
on O

leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Multiple-Choice) O
on O

Additional O
examples O
from O
the O
VQA B-DAT
dataset O

scenes. O
This O
helps O
motivate O
the O
VQA B-DAT
task O
as O
a O
way O
to O

APPENDIX O
IV: O
DETAILS O
ON O
VQA B-DAT
BASELINES O
“per O
Q-type O
prior” O
baseline O

For O
every O
question O
in O
the O
VQA B-DAT
test-standard O
set, O
we O
find O
its O

norm O
I), O
selected O
using O
VQA B-DAT
test- O
dev O
accuracies). O
To O
estimate O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

a O
random O
selection O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

brown O
(score: O
12.89 O
= O
1.01 B-DAT
[image] O
+ O
11.88 O
[word]) O
red O

word]) O
yellow O
(score: O
11.91 O
= O
1.08 B-DAT
[image] O
+ O
10.84 O
[word O

(1.05 B-DAT

1505 B-DAT

1505 B-DAT

1505 B-DAT

1105, B-DAT
2012 O

1409 B-DAT

to O
interact O
with O
people O
in O
real B-DAT
time. O
Aided O
by O
the O
simplicity O

When O
evaluated O
on O
the O
challenging O
VQA B-DAT
dataset O
[2], O
it O
shows O
comparable O

in O
one O
of O
the O
earliest O
VQA B-DAT
papers O
[12], O
the O
simple O
baseline O

the O
recent O
much O
larger O
COCO O
VQA B-DAT
dataset O
[2], O
the O
BOWIMG O
baseline O

the O
Full O
release O
of O
COCO O
VQA B-DAT
dataset O
[2], O
the O
largest O
VQA O

so O
far. O
In O
the O
COCO O
VQA B-DAT
dataset, O
there O
are O
3 O
questions O

the O
evaluation O
standard O
of O
the O
VQA B-DAT
dataset, O
the O
result O
of O
the O

any O
proposed O
VQA B-DAT
models O
should O
report O
accuracy O
on O

Since O
this O
VQA B-DAT
dataset O
is O
rather O
new, O
the O

The O
full O
set O
of O
the O
VQA B-DAT
dataset O
was O
released O
on O
Oct.6 O

2015) O
used O
v0.9 O
version O
of O
VQA B-DAT
with O
their O
own O
split O
of O

2015) O
used O
v0.9 O
version O
of O
VQA B-DAT
dataset. O
So O
these O
are O
not O

neural O
network O
models O
on O
the O
VQA B-DAT
dataset. O
Furthermore, O
due O
to O
its O

the O
attention O
mechanisms O
of O
the O
VQA B-DAT
models O
in O
[13, O
17, O
18 O

the O
strength O
and O
weakness O
of O
VQA B-DAT
model O

an O
interactive O
web O
demo1, O
and O
open B-DAT

question O
relevant O
to O
the O
given O
images B-DAT

pairs O
in O
val2014, O
for O
123,287 O
images B-DAT
overall O
in O
the O
training O
set O

we O
first O
randomly O
split O
the O
images B-DAT
of O
COCO O
val2014 O
into O
70 O

The O
question-answer O
pairs O
from O
the O
images B-DAT
of O
COCO O
train2014 O
+ O
val2014 O

and O
actions O
appearing O
in O
the O
images B-DAT
of O
COCO O
dataset. O
For O
the O

the O
deep O
feature O
of O
the O
images B-DAT
are O
extracted O
beforehand. O
Figure O
4 O

Simple O
Baseline O
for O
Visual O
Question B-DAT
Answering O

2 O
iBOWIMG O
for O
Visual O
Question B-DAT
Answering O

and O
test-standard O
set. O
For O
Open-Ended O
Question B-DAT
track, O
we O
take O
the O
top-1 O

softmax O
output. O
For O
the O
Multiple-Choice O
Question B-DAT
track, O
we O
first O
get O
the O

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT
answering O
is O
essentially O
an O
interactive O

2 O
iBOWIMG O
for O
Visual O
Question B-DAT
Answering O

Simple O
Baseline O
for O
Visual B-DAT
Question O
Answering O

2 O
iBOWIMG O
for O
Visual B-DAT
Question O
Answering O

3.3 O
Understanding O
the O
Visual B-DAT
QA O
model O

two O
different O
questions O
and O
answers. O
Visual B-DAT
features O
from O
CNN O
already O
have O

4 O
Interactive O
Visual B-DAT
QA O
Demo O

Zitnick, O
and O
D. O
Parikh. O
Vqa: O
Visual B-DAT
question O
answering. O
arXiv O
preprint O
arXiv:1505.00468 O

2 O
iBOWIMG O
for O
Visual B-DAT
Question O
Answering O

3.3 O
Understanding O
the O
Visual B-DAT
QA O
model O

4 O
Interactive O
Visual B-DAT
QA O
Demo O

of O
the O
image O
captions O
of O
COCO B-DAT
dataset O
[9]. O
For O
the O
recent O

much O
larger O
COCO B-DAT
VQA O
dataset O
[2], O
the O
BOWIMG O

on O
the O
Full O
release O
of O
COCO B-DAT
VQA O
dataset O
[2], O
the O
largest O

dataset O
so O
far. O
In O
the O
COCO B-DAT
VQA O
dataset, O
there O
are O
3 O

for O
each O
image O
in O
the O
COCO B-DAT
dataset. O
For O
each O
question, O
10 O

the O
image O
set O
in O
the O
COCO B-DAT
dataset O

randomly O
split O
the O
images O
of O
COCO B-DAT
val2014 O
into O
70% O
subset O
A O

pairs O
from O
the O
images O
of O
COCO B-DAT
train2014 O
+ O
val2014 O
subset O
A O

model O
on O
the O
testing O
set O
(COCO B-DAT
test2015) O
to O
the O
evaluation O
server O

appearing O
in O
the O
images O
of O
COCO B-DAT
dataset. O
For O
the O
second O
image O

For O
visual O
question O
answering O
on O
COCO B-DAT
dataset, O
our O
implementation O
of O
a O

Simple O
Baseline O
for O
Visual O
Question O
Answering B-DAT

2 O
iBOWIMG O
for O
Visual O
Question O
Answering B-DAT

2 O
iBOWIMG O
for O
Visual O
Question O
Answering B-DAT

When O
evaluated O
on O
the O
challenging O
VQA B-DAT
dataset O
[2], O
it O
shows O
comparable O

in O
one O
of O
the O
earliest O
VQA B-DAT
papers O
[12], O
the O
simple O
baseline O

the O
recent O
much O
larger O
COCO O
VQA B-DAT
dataset O
[2], O
the O
BOWIMG O
baseline O

the O
Full O
release O
of O
COCO O
VQA B-DAT
dataset O
[2], O
the O
largest O
VQA O

so O
far. O
In O
the O
COCO O
VQA B-DAT
dataset, O
there O
are O
3 O
questions O

the O
evaluation O
standard O
of O
the O
VQA B-DAT
dataset, O
the O
result O
of O
the O

any O
proposed O
VQA B-DAT
models O
should O
report O
accuracy O
on O

Since O
this O
VQA B-DAT
dataset O
is O
rather O
new, O
the O

The O
full O
set O
of O
the O
VQA B-DAT
dataset O
was O
released O
on O
Oct.6 O

2015) O
used O
v0.9 O
version O
of O
VQA B-DAT
with O
their O
own O
split O
of O

2015) O
used O
v0.9 O
version O
of O
VQA B-DAT
dataset. O
So O
these O
are O
not O

neural O
network O
models O
on O
the O
VQA B-DAT
dataset. O
Furthermore, O
due O
to O
its O

the O
attention O
mechanisms O
of O
the O
VQA B-DAT
models O
in O
[13, O
17, O
18 O

the O
strength O
and O
weakness O
of O
VQA B-DAT
model O

truth O
score O
s(q, O
a) O
= O
1.0 B-DAT
if O
the O
answer O
a O
was O

1 B-DAT
0 I-DAT

1509 B-DAT

1606 B-DAT

1606 B-DAT

1602 B-DAT

1606 B-DAT

1606 B-DAT

1607 B-DAT

epochs O
on O
the O
“abstract O
scenes”, O
100 B-DAT
epochs O
on O
the O
“balanced” O
dataset O

abstract B-DAT
scenes” O
multiple-choice O
benchmark, O
and O
from O

abstract B-DAT
scenes”, O
de- O
spite O
this O
being O

abstract B-DAT
scenes” O
benchmark O
[4] O
and O
demonstrate O

abstract B-DAT
scenes” O
dataset. O
The O
candidate O
answers O

abstract B-DAT
scenes” O
from O
Antol O
et O
al O

abstract B-DAT
scenes” O
(left: O
multiple O
choice, O
middle O

abstract B-DAT
scenes” O
dataset O
(left O
and O
middle O

abstract B-DAT
scenes” O
dataset O
(Fig. O
3, O
left O

abstract B-DAT
scenes” O
test O
set O
is O
not O

abstract B-DAT
scenes” O
dataset O

abstract B-DAT
scenes” O
dataset O
in O
Table O
2 O

abstract B-DAT
scenes” O
dataset O
(average O
scores O
in O

abstract B-DAT
scenes” O
dataset O
(top O
row) O
and O

abstract B-DAT
scenes”, O
100 O
epochs O
on O
the O

abstract B-DAT
scenes” O
dataset, O
and O
accuracy O
over O

we O
choose O
to O
focus O
on O
abstract B-DAT
scenes O
? O
Does O
this O
method O

The O
balanced O
dataset O
of O
abstract B-DAT
scenes O
was O
the O
only O
one O

the O
scene O
descriptions O
(provided O
with O
abstract B-DAT
scenes) O
as O
the O
output O
of O

C.1. O
Additional O
results: O
abstract B-DAT
scenes O
dataset O

ended B-DAT
form, O
a O
question O
is O
provided O

ended) B-DAT
and O
“balanced” O
datasets O
(right). O
The O

ended B-DAT
and O
multiple O
choice O
settings. O
The O

ended B-DAT
Method O
Overall O
Yes/no O
Other O
Number O

ended B-DAT
setting, O
the O
output O
space O
is O

ended B-DAT
(O.E.). O
Therefore, O
why O
is O
the O

improve O
visual O
question O
answer- O
ing O
(VQA) B-DAT
with O
structured O
representations O
of O
both O

questions. O
A O
key O
challenge O
in O
VQA B-DAT
is O
to O
require O
joint O
reasoning O

pre- O
dominant O
CNN/LSTM-based O
approach O
to O
VQA B-DAT
is O
limited O
by O
monolithic O
vector O

Multiple O
datasets O
for O
VQA B-DAT
have O
been O
introduced O
with O
either O

perfor- O
mance. O
A O
particularly O
attractive O
VQA B-DAT
dataset O
was O
intro- O
duced O
in O

This O
strongly O
contrasts O
with O
other O
VQA B-DAT
datasets O
of O
real O
images, O
where O

the O
greater O
goal O
of O
general O
VQA B-DAT

additional O
challenge, O
which O
affects O
all O
VQA B-DAT
datasets, O
is O
the O
sparsity O
of O

this O
challenge, O
most O
methods O
for O
VQA B-DAT
process O
the O
question O
through O
a O

and O
question O
for O
VQA, B-DAT
and O
a O
neural O
network O
capable O

this O
information O
accessible O
to O
the O
VQA B-DAT
model. O
This O
rep- O
resentation O
uses O

the O
proposed O
model O
on O
the O
VQA B-DAT
“abstract O
scenes” O
benchmark O
[4] O
and O

time O
on O
the O
task O
of O
VQA B-DAT
– O
precision/recall O
curves O
of O
predicted O

answer. O
Most O
recent O
papers O
on O
VQA B-DAT
propose O
im- O
provements O
and O
variations O

memory O
networks O
(DMN), O
applied O
to O
VQA B-DAT
in O
[27] O
also O
maintain O
a O

Most O
VQA B-DAT
systems O
are O
trained O
end-to-end O
from O

pairs. O
This O
contrasts O
with O
other O
VQA B-DAT
datasets O
where O
blind O
guessing O
can O

VQA B-DAT
score” O
[4], O
which O
is O
a O

its O
predicted O
answers. O
Most O
existing O
VQA B-DAT
methods O
treat O
the O
answering O
as O

Graph O
VQA B-DAT
(full O
model) O
74.94 O
39.1 O

It O
is O
unclear O
whether O
huge O
VQA B-DAT
datasets O
could O
ultimately O
negate O
this O

with O
LSTMs. O
In O
our O
opinion, O
VQA B-DAT
systems O
are O
unlikely O
to O
learn O

without O
rest- O
ing O
entirely O
on O
VQA B-DAT

69.73 O
80.70 O
62.08 O
58.82 O
Graph O
VQA B-DAT
(full O
model) O
74.37 O
79.74 O
68.31 O

References O
[1] O
VQA B-DAT
Challenge O
leaderboard. O
http://visualqa.org O

L. O
Zitnick, O
and O
D. O
Parikh. O
VQA B-DAT

the O
validation O
set O
(measured O
by O
VQA B-DAT
score O
on O
the O
“abstract O
scenes O

its O
an- O
swers. O
A O
practical O
VQA B-DAT
system O
will O
need O
to O
provide O

and O
artificial O
intelligence. O
In O
its O
open B-DAT

scenes” O
(left: O
multiple O
choice, O
middle: O
open B-DAT

leader O
board O
in O
both O
the O
open B-DAT

In O
the O
open B-DAT

setting O
should O
be O
easier O
than O
open B-DAT

18, O
22, O
32] O
or O
synthetic O
images B-DAT
[4, O
31]. O
Our O
experiments O
uses O

on O
clip O
art O
or O
“cartoon” O
images B-DAT
created O
by O
humans O
to O
depict O

other O
VQA O
datasets O
of O
real O
images, B-DAT
where O
a O
correct O
answer O
is O

obvious O
limita- O
tions O
of O
synthetic O
images, B-DAT
improvements O
on O
the O
aforemen- O
tioned O

end-to-end O
from O
ques- O
tions O
and O
images B-DAT
to O
answers, O
with O
the O
exception O

is O
equally O
applicable O
to O
real O
images, B-DAT
with O
the O
object O
list O
replaced O

Its O
direct O
extension O
to O
real O
images B-DAT
will O
be O
addressed O
in O
future O

this O
method O
extend O
to O
real O
images B-DAT
? O
The O
balanced O
dataset O
of O

could O
be O
extended O
to O
real O
images B-DAT
by O
building O
graphs O
of O
the O

images B-DAT
where O
scene O
nodes O
are O
candidates O

Graph-Structured O
Representations O
for O
Visual O
Question B-DAT
Answering O

Introduction O
The O
task O
of O
Visual O
Question B-DAT
Answering O
has O
received O

1) O
Question B-DAT

2) O
Question B-DAT

and O
D. O
Parikh. O
VQA: O
Visual O
Question B-DAT
Answering. O
In O
Proc. O
IEEE O
Int O

Neu- O
ral O
Network O
for O
Visual O
Question B-DAT
Answering. O
arXiv O
preprint O
arXiv:1511.05960, O
2015 O

Li. O
Compositional O
Memory O
for O
Visual O
Question B-DAT
Answering. O
arXiv O
preprint O
arXiv:1511.05676, O
2015 O

Kiros, O
and O
R. O
Zemel. O
Image O
Question B-DAT
Answering: O
A O
Visual O
Semantic O
Embedding O

A. O
van O
den O
Hengel. O
Visual O
Question B-DAT
Answering: O
A O
Survey O
of O
Methods O

Attend O
and O
Answer: O
Explor- O
ing O
Question B-DAT

-Guided O
Spatial O
Attention O
for O
Visual O
Question B-DAT
Answering. O
arXiv O
preprint O
arXiv:1511.05234, O
2015 O

Stacked O
Attention O
Networks O
for O
Image O
Question B-DAT
Answering. O
In O
Proc. O
IEEE O
Conf O

and O
L. O
Fei-Fei. O
Visual7W: O
Grounded O
Question B-DAT
Answering O
in O
Images. O
In O
Proc O

Graph-Structured O
Representations O
for O
Visual B-DAT
Question O
Answering O

den O
Hengel O
Australian O
Centre O
for O
Visual B-DAT
Technologies O

1. O
Introduction O
The O
task O
of O
Visual B-DAT
Question O
Answering O
has O
received O

Zitnick, O
and O
D. O
Parikh. O
VQA: O
Visual B-DAT
Question O
Answering. O
In O
Proc. O
IEEE O

Convolutional O
Neu- O
ral O
Network O
for O
Visual B-DAT
Question O
Answering. O
arXiv O
preprint O
arXiv:1511.05960 O

Y. O
Li. O
Compositional O
Memory O
for O
Visual B-DAT
Question O
Answering. O
arXiv O
preprint O
arXiv:1511.05676 O

Bern- O
stein, O
and O
L. O
Fei-Fei. O
Visual B-DAT
genome: O
Connecting O
language O
and O
vision O

Zemel. O
Image O
Question O
Answering: O
A O
Visual B-DAT
Semantic O
Embedding O
Model O
and O
a O

and O
A. O
van O
den O
Hengel. O
Visual B-DAT
Question O
Answering: O
A O
Survey O
of O

ing O
Question-Guided O
Spatial O
Attention O
for O
Visual B-DAT
Question O
Answering. O
arXiv O
preprint O
arXiv:1511.05234 O

Graph-Structured O
Representations O
for O
Visual O
Question O
Answering B-DAT

The O
task O
of O
Visual O
Question O
Answering B-DAT
has O
received O

D. O
Parikh. O
VQA: O
Visual O
Question O
Answering B-DAT

ral O
Network O
for O
Visual O
Question O
Answering B-DAT

Compositional O
Memory O
for O
Visual O
Question O
Answering B-DAT

and O
R. O
Zemel. O
Image O
Question O
Answering B-DAT

van O
den O
Hengel. O
Visual O
Question O
Answering B-DAT

Spatial O
Attention O
for O
Visual O
Question O
Answering B-DAT

Attention O
Networks O
for O
Image O
Question O
Answering B-DAT

L. O
Fei-Fei. O
Visual7W: O
Grounded O
Question O
Answering B-DAT
in O
Images. O
In O
Proc. O
IEEE O

improve O
visual O
question O
answer- O
ing O
(VQA) B-DAT
with O
structured O
representations O
of O
both O

questions. O
A O
key O
challenge O
in O
VQA B-DAT
is O
to O
require O
joint O
reasoning O

pre- O
dominant O
CNN/LSTM-based O
approach O
to O
VQA B-DAT
is O
limited O
by O
monolithic O
vector O

Multiple O
datasets O
for O
VQA B-DAT
have O
been O
introduced O
with O
either O

perfor- O
mance. O
A O
particularly O
attractive O
VQA B-DAT
dataset O
was O
intro- O
duced O
in O

This O
strongly O
contrasts O
with O
other O
VQA B-DAT
datasets O
of O
real O
images, O
where O

the O
greater O
goal O
of O
general O
VQA B-DAT

additional O
challenge, O
which O
affects O
all O
VQA B-DAT
datasets, O
is O
the O
sparsity O
of O

this O
challenge, O
most O
methods O
for O
VQA B-DAT
process O
the O
question O
through O
a O

and O
question O
for O
VQA, B-DAT
and O
a O
neural O
network O
capable O

this O
information O
accessible O
to O
the O
VQA B-DAT
model. O
This O
rep- O
resentation O
uses O

the O
proposed O
model O
on O
the O
VQA B-DAT
“abstract O
scenes” O
benchmark O
[4] O
and O

time O
on O
the O
task O
of O
VQA B-DAT
– O
precision/recall O
curves O
of O
predicted O

answer. O
Most O
recent O
papers O
on O
VQA B-DAT
propose O
im- O
provements O
and O
variations O

memory O
networks O
(DMN), O
applied O
to O
VQA B-DAT
in O
[27] O
also O
maintain O
a O

Most O
VQA B-DAT
systems O
are O
trained O
end-to-end O
from O

pairs. O
This O
contrasts O
with O
other O
VQA B-DAT
datasets O
where O
blind O
guessing O
can O

VQA B-DAT
score” O
[4], O
which O
is O
a O

its O
predicted O
answers. O
Most O
existing O
VQA B-DAT
methods O
treat O
the O
answering O
as O

Graph O
VQA B-DAT
(full O
model) O
74.94 O
39.1 O

It O
is O
unclear O
whether O
huge O
VQA B-DAT
datasets O
could O
ultimately O
negate O
this O

with O
LSTMs. O
In O
our O
opinion, O
VQA B-DAT
systems O
are O
unlikely O
to O
learn O

without O
rest- O
ing O
entirely O
on O
VQA B-DAT

69.73 O
80.70 O
62.08 O
58.82 O
Graph O
VQA B-DAT
(full O
model) O
74.37 O
79.74 O
68.31 O

References O
[1] O
VQA B-DAT
Challenge O
leaderboard. O
http://visualqa.org O

L. O
Zitnick, O
and O
D. O
Parikh. O
VQA B-DAT

the O
validation O
set O
(measured O
by O
VQA B-DAT
score O
on O
the O
“abstract O
scenes O

its O
an- O
swers. O
A O
practical O
VQA B-DAT
system O
will O
need O
to O
provide O

scenes. O
The O
set O
contains O
over O
100 B-DAT
objects O
and O
31 O
animals O
in O

i.e., O
an O
answer O
is O
deemed O
100 B-DAT

MS O
COCO O
dataset O
[32] O
and O
150, B-DAT

a O
scale O
of O
0 O
− O
100) B-DAT
required O
to O
answer O
a O
question O

choose O
the O
top O
K O
= O
1000 B-DAT
most O
frequent O
answers O
as O
possible O

Question O
(BoW O
Q): O
The O
top O
1,000 B-DAT
words O
in O
the O
questions O
are O

are O
concatenated O
to O
get O
a O
1,030 B-DAT

1000 B-DAT

1000 B-DAT

with O
2 O
hidden O
layers O
and O
1000 B-DAT
hidden O
units O
(dropout O
0.5) O
in O

a O
bag-of-words O
representation O
containing O
the O
1,000 B-DAT
most O
popular O
words O
in O
the O

that O
it O
has O
VQA O
accuracy O
1.0 B-DAT
(see O
section O
3 O
for O
accuracy O

Question O
K O
= O
1000 B-DAT
Human O
To O
Be O
Able O
To O

100 B-DAT
A O

100 B-DAT

100 B-DAT

100 B-DAT

can O
see O
that O
K O
= O
1000 B-DAT
performs O
better O
than O
K O

performs O
better O
then O
K O
= O
1000 B-DAT
by O
0.40% O
for O
open-ended O
task O

blue” O
(28881, O
1.16%), O
“4” O
(27174, O
1.09 B-DAT

outside” O
(1846, O
0.07%), O
“hot O
dog” O
(1809, B-DAT
0.07%), O
“night” O
(1805, O
0.07%), O
“trees O

and O
white” O
(1518, O
0.06%), O
“bedroom” O
(1500, B-DAT
0.06%), O
“bat” O
(1494, O
0.06%), O
“glasses O

0.06%), O
“cloudy” O
(1413, O
0.06%), O
“15” O
(1407, B-DAT
0.06%), O
“up” O
(1399, O
0.06%), O
“blonde O

0.05%), O
“many” O
(1211, O
0.05%), O
“zoo” O
(1204, B-DAT
0.05%), O
“suitcase” O
(1199, O
0.05%), O
“old O

0.04%), O
“mountains” O
(1030, O
0.04%), O
“wall” O
(1009, B-DAT
0.04%), O
“ele- O
phants” O
(1006, O
0.04 O

1504 B-DAT

1504 B-DAT

1408 B-DAT

1506 B-DAT

1409 B-DAT

1502 B-DAT

32] O
and O
a O
newly O
created O
abstract B-DAT
scene O
dataset O
[57], O
[2] O
that O

a O
new O
dataset O
of O
“realistic” O
abstract B-DAT
scenes O
to O
enable O
research O
focused O

output O
answer O
classes. O
[34] O
generates O
abstract B-DAT
scenes O
to O
capture O
visual O
common O

describing O
the O
real O
images O
and O
abstract B-DAT

tasks, O
we O
create O
a O
new O
abstract B-DAT
scenes O
dataset O
[2], O
[57], O
[58 O

winners O
of O
the O
challenge. O
For O
abstract B-DAT
scenes, O
we O
created O
splits O
for O

test-dev, O
test-standard, O
test-challenge, O
test-reserve) O
for O
abstract B-DAT
scenes. O
Captions. O
The O
MS O
COCO O

collected O
five O
single-captions O
for O
all O
abstract B-DAT
scenes O
using O
the O
same O
user O

both O
the O
real O
images O
and O
abstract B-DAT
scenes. O
In O
total, O
three O
questions O

left) O
and O
all O
questions O
for O
abstract B-DAT
scenes O
(right). O
The O
ordering O
of O

1,950,000 O
answers O
for O
50, O
000 O
abstract B-DAT
scenes O

the O
real O
images O
(left) O
and O
abstract B-DAT
scenes O
(right). O
Interestingly, O
the O
distribution O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
This O
helps O
demonstrate O
that O

of O
questions O
elicited O
by O
the O
abstract B-DAT
scenes O
is O
similar O
to O
those O

lengths O
for O
real O
images O
and O
abstract B-DAT
scenes O

90.51%, O
5.89%, O
and O
2.49% O
for O
abstract B-DAT
scenes. O
The O
brevity O
of O
answers O

real O
images O
and O
3,770 O
for O
abstract B-DAT
scenes. O
‘Yes/No’ O
and O
‘Number’ O
Answers O

questions O
on O
real O
images O
and O
abstract B-DAT
scenes O
respectively. O
Among O
these O
‘yes/no O

yes” O
for O
real O
images O
and O
abstract B-DAT
scenes. O
Question O
types O
such O
as O

questions O
on O
real O
images O
and O
abstract B-DAT
scenes O
are O
‘number’ O
questions. O
“2 O

real O
images O
and O
39.85% O
for O
abstract B-DAT
scenes. O
Subject O
Confidence. O
When O
the O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
Inter-human O
Agreement. O
Does O
the O

confident) O
for O
real O
images O
and O
abstract B-DAT
scenes O
(black O
lines). O
Percentage O
of O

both O
real O
images O
(83.30%) O
and O
abstract B-DAT
scenes O
(87.49%). O
Note O
that O
on O

real O
images O
and O
2.39 O
for O
abstract B-DAT
scenes. O
The O
agreement O
is O
significantly O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
See O
the O
appendix O
for O

abstract B-DAT
| O
mc-real O
| O
mc-abstract O

oe-abstract-leaderboard B-DAT
| O
mc-real-leaderboard O
| O
mc- O
abstract O

VI O
- O
Details O
on O
the O
abstract B-DAT
scene O
dataset O
VII O
- O
User O

captions O
collected O
by O
us O
for O
abstract B-DAT
scenes) O
using O
the O
Stanford O
part-of-speech O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
This O
helps O
motivate O
the O

and O
Fig. O
20 O
(adjectives) O
for O
abstract B-DAT
scenes.7 O
The O
left O
side O
shows O

and O
Fig. O
14 O
(right) O
for O
abstract B-DAT
scenes. O
We O
see O
that O
questions O

for O
real O
images O
(left) O
and O
abstract B-DAT
scenes O
(right O

indicating O
the O
normalized O
count O
for O
abstract B-DAT
scenes O

indicating O
the O
normalized O
count O
for O
abstract B-DAT
scenes O

indicating O
the O
normalized O
count O
for O
abstract B-DAT
scenes O

the O
two O
datasets, O
real O
and O
abstract, B-DAT
first O
two O
rows O
are O
the O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
Note O
the O
diversity O
of O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
In O
Table O
6, O
we O

and O
≈ O
11% O
increase O
for O
abstract B-DAT
scenes) O
for O
“other” O
questions. O
Since O

that O
are O
present O
in O
the O
abstract B-DAT
scenes O
dataset. O
For O
more O
examples O

left) O
and O
all O
questions O
for O
abstract B-DAT
scenes O
(right). O
The O
ordering O
of O

top) O
and O
all O
questions O
for O
abstract B-DAT
scenes O
(bottom). O
Each O
column O
corresponds O

the O
objects O
present O
in O
the O
abstract B-DAT
scene O
dataset. O
Right: O
The O
AMT O

interface O
for O
collecting O
abstract B-DAT
scenes. O
The O
light O
green O
circles O

the O
MS O
COCO O
[32] O
images, O
abstract B-DAT
scenes, O
and O
multiple-choice O
questions, O
respectively O

numerous O
representative O
examples O
of O
the O
abstract B-DAT
scene O
dataset O

examples O
of O
the O
real O
and O
abstract B-DAT
scene O
dataset O

ended B-DAT
Visual O
Question O
Answering O
(VQA). O
Given O

ended B-DAT

ended B-DAT
answers O
contain O
only O
a O
few O

task O
of O
free-form O
and O
open- O
ended B-DAT
Visual O
Question O
Answering O
(VQA). O
A O

ended, B-DAT
natural- O
language O
question O
about O
the O

ended B-DAT
questions O
require O
a O
potentially O
vast O

ended B-DAT
questions O
collected O
for O
images O
via O

ended B-DAT
answering O
task O
and O
a O
multiple O

ended B-DAT
task O
that O
requires O
a O
free-form O

ended B-DAT
questions O
offers O
many O
benefits, O
it O

ended, B-DAT
free-form O
questions O
and O
answers O
provided O

ended B-DAT
questions O
result O
in O
a O
diverse O

ended B-DAT
and O
(ii) O
multiple-choice. O
For O
the O

ended B-DAT
task, O
the O
generated O
answers O
are O

ended B-DAT
task, O
the O
accuracy O
of O
a O

ended B-DAT
answers O
to O
open-ended O
questions. O
The O

ended B-DAT
and O
multiple-choice O
tasks. O
Note O
that O

ended B-DAT
task, O
we O
pick O
the O
most O

ended B-DAT
task O
using O
cosine O
similarity O
in O

ended B-DAT
task, O
we O
pick O
the O
most O

ended B-DAT
task O
using O
cosine O
similarity O
in O

on O
two O
different O
tasks: O
open- O
ended B-DAT
selects O
the O
answer O
with O
highest O

ended B-DAT
and O
multiple- O
choice O
tasks O
on O

ended B-DAT
and O
multiple-choice O
tasks O
on O
the O

ended B-DAT

ended B-DAT
task, O
the O
vision-alone O
model O
(I O

ended B-DAT
(53.68% O
on O
multiple-choice) O
and O
LSTM O

Q O
achieving O
48.76% O
on O
open- O
ended B-DAT
(54.75% O
on O
multiple-choice); O
both O
outperforming O

ended B-DAT

ended) B-DAT
/ O
63.09% O
(multiple-choice). O
We O
can O

ended B-DAT

ended B-DAT
test-dev O
results O
for O
different O
question O

I) O
for O
both O
the O
open- O
ended B-DAT
and O
multiple-choice O
tasks O
on O
the O

ended B-DAT
task O
and O
by O
0.24% O
for O

ended B-DAT
task O
and O
by O
1.24% O
for O

ended B-DAT
task O
and O
by O
1.92% O
for O

ended B-DAT
task O
and O
by O
1.16% O
for O

words O
by O
0.24% O
for O
open- O
ended B-DAT
task O
and O
by O
0.17% O
for O

ended B-DAT
task O
and O
by O
0.02% O
for O

ended B-DAT
and O
multiple-choice O
tasks O
on O
the O

ended B-DAT
task O
and O
by O
1.88% O
for O

ended B-DAT

ended B-DAT
and O
multiple-choice O
tasks O
(real O
images O

ended, B-DAT
natural O

ended B-DAT
and O
multiple-choice O
tasks O
in O
the O

ended B-DAT
and O
not O
task-specific. O
For O
some O

ended B-DAT
questions O
that O
are O
answered O
by O

ended B-DAT
answers O
task O
when O
subjects O
were O

ended B-DAT
answer O
task. O
In O
comparison O
to O

open- O
ended B-DAT
answer, O
the O
multiple-choice O
accuracies O
are O

VQA B-DAT

and O
open-ended O
Visual O
Question O
Answering O
(VQA B-DAT

a O
system O
that O
succeeds O
at O
VQA B-DAT
typically O
needs O
a O
more O
detailed O

producing O
generic O
image O
captions. O
Moreover, O
VQA B-DAT
is O
amenable O
to O
automatic O
evaluation O

Numerous O
baselines O
and O
methods O
for O
VQA B-DAT
are O
provided O
and O
compared O
with O

human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

open- O
ended O
Visual O
Question O
Answering O
(VQA B-DAT

). O
A O
VQA B-DAT
system O
takes O
as O
input O
an O

Is O
this O
person O
expecting O
company?”). O
VQA B-DAT
[19], O
[36], O
[50], O
[3] O
is O

the O
high-level O
reasoning O
required O
for O
VQA B-DAT
by O
removing O
the O
need O
to O

29]. O
As O
part O
of O
the O
VQA B-DAT
initiative, O
we O
will O
organize O
an O

state-of-the-art O
methods O
and O
best O
practices. O
VQA B-DAT
poses O
a O
rich O
set O
of O

during O
the O
past O
few O
decades. O
VQA B-DAT
provides O
an O
attractive O
balance O
between O

VQA B-DAT
Efforts. O
Several O
recent O
papers O
have O

difficult O
and O
unconstrained O
task, O
our O
VQA B-DAT
dataset O
is O
two O
orders O
of O

1,449 O
images O
respectively). O
The O
proposed O
VQA B-DAT
task O
has O
connections O
to O
other O

These O
approaches O
provide O
inspiration O
for O
VQA B-DAT
techniques. O
One O
key O
concern O
in O

fixed O
set O
of O
loca- O
tions. O
VQA B-DAT
is O
naturally O
grounded O
in O
images O

Describing O
Visual O
Content. O
Related O
to O
VQA B-DAT
are O
the O
tasks O
of O
image O

by O
[53]). O
The O
questions O
in O
VQA B-DAT
require O
detailed O
specific O
information O
about O

3 O
VQA B-DAT
DATASET O
COLLECTION O

describe O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset. O
We O
begin O
by O
describing O

they O
are O
well-suited O
for O
our O
VQA B-DAT
task. O
The O
more O
diverse O
our O

their O
answers. O
Abstract O
Scenes. O
The O
VQA B-DAT
task O
with O
real O
images O
requires O

the O
high-level O
reasoning O
required O
for O
VQA, B-DAT
but O
not O
the O
low-level O
vision O

test-standard, O
test-challenge, O
test-reserve). O
For O
the O
VQA B-DAT
challenge O
(see O
section O
6), O
test-dev O

default’ O
test O
data O
for O
the O
VQA B-DAT
competition. O
When O
comparing O
to O
the O

sentences O
containing O
multiple O
words. O
In O
VQA, B-DAT
most O
answers O
(89.32%) O
are O
single O

4 O
VQA B-DAT
DATASET O
ANALYSIS O
In O
this O
section O

questions O
and O
answers O
in O
the O
VQA B-DAT
train O
dataset. O
To O
gain O
an O

visual O
information O
is O
critical O
to O
VQA B-DAT
and O
that O
commonsense O
information O
alone O

from O
the O
real O
images O
of O
VQA B-DAT
trainval) O
asking O
subjects O

5 O
VQA B-DAT
BASELINES O
AND O
METHODS O
In O
this O

explore O
the O
difficulty O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

novel O
methods. O
We O
train O
on O
VQA B-DAT
train+val. O
Unless O
stated O
otherwise, O
all O

top O
1K O
answers O
of O
the O
VQA B-DAT
train/val O
dataset O

multiple- O
choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
As O

I O
(Fig. O
8), O
selected O
using O
VQA B-DAT
test-dev O
accuracies) O
on O
VQA O
test O

worse O
than O
human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

system O
is O
correct O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

correct” O
implies O
that O
it O
has O
VQA B-DAT
accuracy O
1.0 O
(see O
section O
3 O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
The O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

frequently O
predicted O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

age O
of O
question) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

system O
is O
correct) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

a O
filtered O
version O
of O
the O
VQA B-DAT
train O
+ O
val O
dataset O
in O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

6 O
VQA B-DAT
CHALLENGE O
AND O
WORKSHOP O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O

task O
of O
Visual O
Question O
Answering O
(VQA B-DAT

multiple-choice O
tasks O
in O
the O
respective O
VQA B-DAT
Real O
Image O
Challenge O
leaderboards O
(as O

datasets O
may O
help O
enable O
practical O
VQA B-DAT
applications. O
We O
believe O
VQA O
has O

questions O
IV O
- O
Details O
on O
VQA B-DAT
baselines O
V O
- O
“Age” O
and O

Leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Open-Ended) O
on O

leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Multiple-Choice) O
on O

Additional O
examples O
from O
the O
VQA B-DAT
dataset O

scenes. O
This O
helps O
motivate O
the O
VQA B-DAT
task O
as O
a O
way O
to O

APPENDIX O
IV: O
DETAILS O
ON O
VQA B-DAT
BASELINES O
“per O
Q-type O
prior” O
baseline O

For O
every O
question O
in O
the O
VQA B-DAT
test-standard O
set, O
we O
find O
its O

norm O
I), O
selected O
using O
VQA B-DAT
test- O
dev O
accuracies). O
To O
estimate O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

a O
random O
selection O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

the O
task O
of O
free-form O
and O
open B-DAT

the O
questions O
and O
answers O
are O
open B-DAT

to O
automatic O
evaluation, O
since O
many O
open B-DAT

is O
still O
a O
difficult O
and O
open B-DAT
research O
problem O
[51], O
[13], O
[22 O

the O
task O
of O
free-form O
and O
open B-DAT

an O
image O
and O
a O
free-form, O
open B-DAT

Fig. O
1: O
Examples O
of O
free-form, O
open B-DAT

paper, O
we O
present O
both O
an O
open B-DAT

task O
[45], O
[33]. O
Unlike O
the O
open B-DAT

answers. O
While O
the O
use O
of O
open B-DAT

contrast, O
our O
proposed O
task O
involves O
open B-DAT

tions: O
(i) O
open B-DAT

and O
(ii) O
multiple-choice. O
For O
the O
open B-DAT

each O
question. O
As O
with O
the O
open B-DAT

recall O
that O
they O
are O
human-provided O
open B-DAT

-ended O
answers O
to O
open B-DAT

answer O
(“yes”) O
for O
both O
the O
open B-DAT

per O
Q-type O
prior: O
For O
the O
open B-DAT

the O
picked O
answer O
for O
the O
open B-DAT

are O
found. O
Next, O
for O
the O
open B-DAT

the O
picked O
answer O
for O
the O
open B-DAT

result O
on O
two O
different O
tasks: O
open B-DAT

of O
our O
methods O
for O
the O
open B-DAT

and O
methods O
for O
both O
the O
open B-DAT

the O
question O
performs O
rather O
poorly O
(open B-DAT

multiple-choice: O
30.53%). O
In O
fact, O
on O
open B-DAT

BoW O
Q O
achieving O
48.09% O
on O
open B-DAT

LSTM O
Q O
achieving O
48.76% O
on O
open B-DAT

outperforming O
the O
nearest O
neighbor O
baseline O
(open B-DAT

VQA O
test- O
standard O
is O
58.16% O
(open B-DAT

on O
multiple-choice O
are O
better O
than O
open B-DAT

norm O
I) O
for O
both O
the O
open B-DAT

the O
performance O
by O
0.16% O
for O
open B-DAT

performs O
better O
by O
0.95% O
for O
open B-DAT

500 O
by O
0.82% O
for O
open B-DAT

1000 O
by O
0.40% O
for O
open B-DAT

questions O
words O
by O
0.24% O
for O
open B-DAT

questions O
words O
by O
0.06% O
for O
open B-DAT

norm O
I) O
for O
the O
open B-DAT

performs O
worse O
by O
1.13% O
for O
open B-DAT

page5. O
Screenshots O
of O
leaderboards O
for O
open B-DAT

norm O
I) O
for O
both O
open B-DAT

Given O
an O
image O
and O
an O
open B-DAT

of O
other O
entries O
for O
the O
open B-DAT

from O
our O
human O
subjects O
were O
open B-DAT

to O
capture O
images O
and O
ask O
open B-DAT

is O
the O
inter-human O
agreement O
for O
open B-DAT

shows O
the O
inter-human O
agreement O
for O
open B-DAT

answer O
task. O
In O
comparison O
to O
open B-DAT

open B-DAT

provide O
a O
dataset O
containing O
∼0.25M O
images, B-DAT
∼0.76M O
questions, O
and O
∼10M O
answers O

free-form, O
open-ended O
questions O
collected O
for O
images B-DAT
via O
Amazon O
Mechanical O
Turk. O
Note O

vision?”). O
Moreover, O
since O
questions O
about O
images B-DAT
often O
tend O
to O
seek O
specific O

large O
dataset O
that O
contains O
204,721 O
images B-DAT
from O
the O
MS O
COCO O
dataset O

The O
MS O
COCO O
dataset O
has O
images B-DAT
depicting O
diverse O
and O
complex O
scenes O

the O
need O
to O
parse O
real O
images B-DAT

250,000 O
vs. O
2,591 O
and O
1,449 O
images B-DAT
respectively). O
The O
proposed O
VQA O
task O

introduced O
a O
dataset O
of O
10k O
images B-DAT
and O
prompted O
captions O
that O
describe O

English O
by O
humans) O
for O
COCO O
images B-DAT

VQA O
is O
naturally O
grounded O
in O
images B-DAT
– O
requiring O
the O
understanding O
of O

both O
text O
(questions) O
and O
vision O
(images B-DAT

begin O
by O
describing O
the O
real O
images B-DAT
and O
abstract O

im- O
ages O
and O
81,434 O
test O
images B-DAT
from O
the O
newly-released O
Microsoft O
Common O

dataset O
was O
gathered O
to O
find O
images B-DAT
containing O
multiple O
objects O
and O
rich O

the O
visual O
complexity O
of O
these O
images, B-DAT
they O
are O
well-suited O
for O
our O

more O
diverse O
our O
collection O
of O
images, B-DAT
the O
more O
diverse, O
comprehensive, O
and O

The O
VQA O
task O
with O
real O
images B-DAT
requires O
the O
use O
of O
complex O

that O
more O
closely O
mirror O
real O
images B-DAT
than O
previous O
papers O
[57], O
[58 O

and O
examples. O
Splits. O
For O
real O
images, B-DAT
we O
follow O
the O
same O
train/val/test O

five O
single-sentence O
captions O
for O
all O
images B-DAT

It O
understands O
a O
lot O
about O
images B-DAT

used O
for O
both O
the O
real O
images B-DAT
and O
abstract O
scenes. O
In O
total O

blue”, O
“4”, O
“green” O
for O
real O
images B-DAT

of O
60K O
questions O
for O
real O
images B-DAT
(left) O
and O
all O
questions O
for O

at O
the O
image) O
for O
204,721 O
images B-DAT
from O
the O
MS O
COCO O
dataset O

questions O
for O
both O
the O
real O
images B-DAT
(left) O
and O
abstract O
scenes O
(right O

quite O
similar O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
This O
helps O

those O
elicited O
by O
the O
real O
images B-DAT

different O
word O
lengths O
for O
real O
images B-DAT
and O
abstract O
scenes O

6.91%, O
and O
2.74% O
for O
real O
images B-DAT
and O
90.51%, O
5.89%, O
and O
2.49 O

elicit O
specific O
information O
from O
the O
images B-DAT

of O
60K O
questions O
for O
real O
images B-DAT
when O
subjects O
provide O
answers O
when O

in O
our O
dataset O
for O
real O
images B-DAT
and O
3,770 O
for O
abstract O
scenes O

of O
the O
questions O
on O
real O
images B-DAT
and O
abstract O
scenes O
respectively. O
Among O

answers O
are O
“yes” O
for O
real O
images B-DAT
and O
abstract O
scenes. O
Question O
types O

of O
the O
questions O
on O
real O
images B-DAT
and O
abstract O
scenes O
are O
‘number O

the O
‘number’ O
answers O
for O
real O
images B-DAT
and O
39.85% O
for O
abstract O
scenes O

as O
confident O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
Inter-human O
Agreement O

1 O
= O
confident) O
for O
real O
images B-DAT
and O
abstract O
scenes O
(black O
lines O

the O
answers O
for O
both O
real O
images B-DAT
(83.30%) O
and O
abstract O
scenes O
(87.49 O

2.70 O
unique O
answers O
for O
real O
images B-DAT
and O
2.39 O
for O
abstract O
scenes O

answers O
provided O
with O
and O
without O
images, B-DAT
we O
show O
the O
distribution O
of O

for O
answers O
with O
and O
without O
images B-DAT

10K O
questions O
from O
the O
real O
images B-DAT
of O
VQA O
trainval) O
asking O
subjects O

of O
3K O
train O
questions O
(1K O
images B-DAT

001) O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
See O
the O

dataset O
for O
the O
MS O
COCO O
images B-DAT
using O
several O
baselines O
and O
novel O

nearest O
neighbor O
questions O
and O
associated O
images B-DAT
from O
the O
training O
set. O
See O

VGGNet O
[48] O
to O
encode O
the O
images B-DAT

the O
VQA O
test-dev O
for O
real O
images B-DAT

the O
VQA O
test-dev O
for O
real O
images B-DAT

different O
question O
types O
on O
real O
images B-DAT
(Q+C O
is O
reported O
on O
val O

the O
VQA O
test-dev O
for O
real O
images B-DAT

the O
VQA O
test-dev O
for O
real O
images B-DAT

open-ended O
and O
multiple-choice O
tasks O
(real O
images) B-DAT
with O
other O
entries O
(as O
of O

a O
dataset O
containing O
over O
250K O
images, B-DAT
760K O
questions, O
and O
around O
10M O

the O
visually O
impaired O
to O
capture O
images B-DAT
and O
ask O
open-ended O
questions O
that O

MS O
COCO O
captions O
for O
real O
images B-DAT
and O
captions O
collected O
by O
us O

001) O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
This O
helps O

Fig. O
17 O
(adjectives) O
for O
real O
images B-DAT
and O
Fig. O
18 O
(nouns), O
Fig O

Fig. O
14 O
(left) O
for O
real O
images B-DAT
and O
Fig. O
14 O
(right) O
for O

question O
& O
answers O
for O
real O
images B-DAT
(left) O
and O
abstract O
scenes O
(right O

the O
normalized O
count O
for O
real O
images B-DAT

the O
normalized O
count O
for O
real O
images B-DAT

the O
normalized O
count O
for O
real O
images B-DAT

five O
words O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
Note O
the O

3,000 O
questions O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
In O
Table O

15% O
increase O
for O
real O
images B-DAT
and O
≈ O
11% O
increase O
for O

of O
questions O
in O
the O
real O
images B-DAT
training O
set O
and O
ensure O
that O

is O
also O
computed O
on O
real O
images B-DAT
training O
set. O
“nearest O
neighbor” O
baseline O

k O
questions O
and O
their O
associated O
images, B-DAT
we O
find O
the O
image O
which O

used O
to O
collect O
questions O
for O
images B-DAT

subjects O
were O
shown O
the O
corresponding O
images B-DAT

of O
60K O
questions O
for O
real O
images B-DAT
(left) O
and O
all O
questions O
for O

of O
60K O
questions O
for O
real O
images B-DAT
(top) O
and O
all O
questions O
for O

250 O
answers O
in O
our O
real O
images B-DAT
dataset O
along O
with O
their O
counts O

for O
the O
MS O
COCO O
[32] O
images, B-DAT
abstract O
scenes, O
and O
multiple-choice O
questions O

approach O
to O
answering O
questions O
about O
images B-DAT

VQA: O
Visual O
Question B-DAT
Answering O
www.visualqa.org O

of O
free-form O
and O
open-ended O
Visual O
Question B-DAT
Answering O
(VQA). O
Given O
an O
image O

free-form O
and O
open- O
ended O
Visual O
Question B-DAT
Answering O
(VQA). O
A O
VQA O
system O

We O
now O
describe O
the O
Visual O
Question B-DAT
Answering O
(VQA) O
dataset. O
We O
begin O

Types O
of O
Question B-DAT

of O
Words O
in O
Question B-DAT

Distribution O
of O
Question B-DAT
Lengths O

real O
images O
and O
abstract O
scenes. O
Question B-DAT
types O
such O
as O
“How O
many O

As O
shown O
in O
Table O
1 O
(Question B-DAT
+ O
Image), O
there O
is O
significant O

Fig. O
2). O
In O
Table O
1 O
(Question), B-DAT
we O
show O
the O
percentage O
of O

Question B-DAT
40.81 O
67.60 O
25.77 O
21.22 O
Real O

Question B-DAT
+ O
Caption* O
57.47 O
78.97 O
39.68 O

Question B-DAT
+ O
Image O
83.30 O
95.77 O
83.39 O

Question B-DAT
43.27 O
66.65 O
28.52 O
23.66 O
Abstract O

Question B-DAT
+ O
Caption* O
54.34 O
74.70 O
41.19 O

Question B-DAT
+ O
Image O
87.49 O
95.96 O
95.04 O

question O
without O
seeing O
the O
image O
(Question), B-DAT
seeing O
just O
a O
caption O
of O

and O
not O
the O
image O
itself O
(Question B-DAT
+ O
Caption), O
and O
seeing O
the O

image O
(Question B-DAT
+ O
Image). O
Results O
are O
shown O

answer O
the O
questions? O
Table O
1 O
(Question B-DAT
+ O
Caption) O
shows O
the O
percentage O

Question B-DAT
Channel: O
This O
channel O
provides O
an O

1) O
Bag-of-Words O
Question B-DAT
(BoW O
Q): O
The O
top O
1,000 O

caption O
embedding O
(Caption). O
For O
BoW O
Question B-DAT
+ O
Caption O
(BoW O
Q O

for O
real O
images. O
Q O
= O
Question, B-DAT
I O
= O
Image, O
C O

Question B-DAT
K O
= O
1000 O
Human O
To O

for O
real O
images. O
Q O
= O
Question, B-DAT
I O
= O
Image. O
See O
text O

introduce O
the O
task O
of O
Visual O
Question B-DAT
Answering O
(VQA). O
Given O
an O
image O

Etzioni. O
Paraphrase-Driven O
Learning O
for O
Open O
Question B-DAT
Answering. O
In O
ACL, O
2013. O
2 O

Zettlemoyer, O
and O
O. O
Etzioni. O
Open O
Question B-DAT
Answering O
over O
Curated O
and O
Extracted O

Fritz. O
A O
Multi-World O
Approach O
to O
Question B-DAT
Answering O
about O
Real-World O
Scenes O
based O

T. O
Mikolov. O
Towards O
AI- O
Complete O
Question B-DAT
Answering: O
A O
Set O
of O
Prerequisite O

VQA: O
Visual B-DAT
Question O
Answering O
www.visualqa.org O

task O
of O
free-form O
and O
open-ended O
Visual B-DAT
Question O
Answering O
(VQA). O
Given O
an O

questions O
and O
answers O
are O
open-ended. O
Visual B-DAT
questions O
selectively O
target O
different O
areas O

of O
free-form O
and O
open- O
ended O
Visual B-DAT
Question O
Answering O
(VQA). O
A O
VQA O

complex O
reasoning O
more O
essential. O
Describing O
Visual B-DAT
Content. O
Related O
to O
VQA O
are O

We O
now O
describe O
the O
Visual B-DAT
Question O
Answering O
(VQA) O
dataset. O
We O

we O
introduce O
the O
task O
of O
Visual B-DAT
Question O
Answering O
(VQA). O
Given O
an O

cloud O
service. O
In O
Mobile O
Cloud O
Visual B-DAT
Media O
Computing, O
pages O
265–290. O
Springer O

D. O
Parikh. O
Zero-Shot O
Learning O
via O
Visual B-DAT
Abstraction. O
In O
ECCV, O
2014. O
2 O

Nearly O
Real- O
time O
Answers O
to O
Visual B-DAT
Questions. O
In O
User O
Interface O
Software O

and O
A. O
Gupta. O
NEIL: O
Extracting O
Visual B-DAT
Knowledge O
from O
Web O
Data. O
In O

Zitnick. O
Mind’s O
Eye: O
A O
Recurrent O
Visual B-DAT
Represen- O
tation O
for O
Image O
Caption O

Long-term O
Recurrent O
Convolutional O
Networks O
for O
Visual B-DAT
Recognition O
and O
Description. O
In O
CVPR O

G. O
Zweig. O
From O
Captions O
to O
Visual B-DAT
Concepts O
and O
Back. O
In O
CVPR O

Hallonquist, O
and O
L. O
Younes. O
A O
Visual B-DAT
Turing O
Test O
for O
Computer O
Vision O

Karpathy O
and O
L. O
Fei-Fei. O
Deep O
Visual B-DAT

and O
R. O
S. O
Zemel. O
Unifying O
Visual B-DAT

Listen, O
Use O
Your O
Imagination: O
Leveraging O
Visual B-DAT
Common O
Sense O
for O
Non-Visual O
Tasks O

Divvala, O
and O
A. O
Farhadi. O
Viske: O
Visual B-DAT
knowledge O
extraction O
and O
question O
answering O

Berg, O
and O
T. O
L. O
Berg. O
Visual B-DAT
madlibs: O
Fill-in-the- O
blank O
description O
generation O

Bringing O
Semantics O
Into O
Focus O
Using O
Visual B-DAT
Abstraction. O
In O
CVPR, O
2013. O
2 O

and O
L. O
Vanderwende. O
Learning O
the O
Visual B-DAT
Interpretation O
of O
Sentences. O
In O
ICCV O

204,721 O
images O
from O
the O
MS O
COCO B-DAT
dataset O
[32] O
and O
a O
newly O

contains O
50,000 O
scenes. O
The O
MS O
COCO B-DAT
dataset O
has O
images O
depicting O
diverse O

to O
English O
by O
humans) O
for O
COCO B-DAT
images. O
[44] O
automatically O
generated O
four O

object, O
count, O
color, O
location) O
using O
COCO B-DAT
captions. O
Text-based O
Q&A O
is O
a O

Common O
Objects O
in O
Context O
(MS O
COCO) B-DAT
[32] O
dataset. O
The O
MS O
COCO O

split O
strategy O
as O
the O
MC O
COCO B-DAT
dataset O
[32] O
(including O
test- O
dev O

abstract O
scenes. O
Captions. O
The O
MS O
COCO B-DAT
dataset O
[32], O
[7] O
already O
contains O

204,721 O
images O
from O
the O
MS O
COCO B-DAT
dataset O
[32] O
and O
150,000 O
questions O

VQA O
dataset O
for O
the O
MS O
COCO B-DAT
images O
using O
several O
baselines O
and O

from O
the O
caption O
data O
(MS O
COCO B-DAT
captions O
for O
real O
images O
and O

VQA O
dataset O
for O
the O
MS O
COCO B-DAT
[32] O
images, O
abstract O
scenes, O
and O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT
captions: O
Data O
collection O
and O
evaluation O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT
Captions: O
Data O
Collection O
and O
Evaluation O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT

VQA: O
Visual O
Question O
Answering B-DAT
www.visualqa.org O

free-form O
and O
open-ended O
Visual O
Question O
Answering B-DAT
(VQA). O
Given O
an O
image O
and O

and O
open- O
ended O
Visual O
Question O
Answering B-DAT
(VQA). O
A O
VQA O
system O
takes O

now O
describe O
the O
Visual O
Question O
Answering B-DAT
(VQA) O
dataset. O
We O
begin O
by O

the O
task O
of O
Visual O
Question O
Answering B-DAT
(VQA). O
Given O
an O
image O
and O

Paraphrase-Driven O
Learning O
for O
Open O
Question O
Answering B-DAT

and O
O. O
Etzioni. O
Open O
Question O
Answering B-DAT
over O
Curated O
and O
Extracted O
Knowledge O

A O
Multi-World O
Approach O
to O
Question O
Answering B-DAT
about O
Real-World O
Scenes O
based O
on O

Parsing O
for O
Understanding O
Events O
and O
Answering B-DAT
Queries. O
IEEE O
MultiMedia, O
2014. O
1 O

Mikolov. O
Towards O
AI- O
Complete O
Question O
Answering B-DAT

VQA B-DAT

and O
open-ended O
Visual O
Question O
Answering O
(VQA B-DAT

a O
system O
that O
succeeds O
at O
VQA B-DAT
typically O
needs O
a O
more O
detailed O

producing O
generic O
image O
captions. O
Moreover, O
VQA B-DAT
is O
amenable O
to O
automatic O
evaluation O

Numerous O
baselines O
and O
methods O
for O
VQA B-DAT
are O
provided O
and O
compared O
with O

human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

open- O
ended O
Visual O
Question O
Answering O
(VQA B-DAT

). O
A O
VQA B-DAT
system O
takes O
as O
input O
an O

Is O
this O
person O
expecting O
company?”). O
VQA B-DAT
[19], O
[36], O
[50], O
[3] O
is O

the O
high-level O
reasoning O
required O
for O
VQA B-DAT
by O
removing O
the O
need O
to O

29]. O
As O
part O
of O
the O
VQA B-DAT
initiative, O
we O
will O
organize O
an O

state-of-the-art O
methods O
and O
best O
practices. O
VQA B-DAT
poses O
a O
rich O
set O
of O

during O
the O
past O
few O
decades. O
VQA B-DAT
provides O
an O
attractive O
balance O
between O

VQA B-DAT
Efforts. O
Several O
recent O
papers O
have O

difficult O
and O
unconstrained O
task, O
our O
VQA B-DAT
dataset O
is O
two O
orders O
of O

1,449 O
images O
respectively). O
The O
proposed O
VQA B-DAT
task O
has O
connections O
to O
other O

These O
approaches O
provide O
inspiration O
for O
VQA B-DAT
techniques. O
One O
key O
concern O
in O

fixed O
set O
of O
loca- O
tions. O
VQA B-DAT
is O
naturally O
grounded O
in O
images O

Describing O
Visual O
Content. O
Related O
to O
VQA B-DAT
are O
the O
tasks O
of O
image O

by O
[53]). O
The O
questions O
in O
VQA B-DAT
require O
detailed O
specific O
information O
about O

3 O
VQA B-DAT
DATASET O
COLLECTION O

describe O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset. O
We O
begin O
by O
describing O

they O
are O
well-suited O
for O
our O
VQA B-DAT
task. O
The O
more O
diverse O
our O

their O
answers. O
Abstract O
Scenes. O
The O
VQA B-DAT
task O
with O
real O
images O
requires O

the O
high-level O
reasoning O
required O
for O
VQA, B-DAT
but O
not O
the O
low-level O
vision O

test-standard, O
test-challenge, O
test-reserve). O
For O
the O
VQA B-DAT
challenge O
(see O
section O
6), O
test-dev O

default’ O
test O
data O
for O
the O
VQA B-DAT
competition. O
When O
comparing O
to O
the O

sentences O
containing O
multiple O
words. O
In O
VQA, B-DAT
most O
answers O
(89.32%) O
are O
single O

4 O
VQA B-DAT
DATASET O
ANALYSIS O
In O
this O
section O

questions O
and O
answers O
in O
the O
VQA B-DAT
train O
dataset. O
To O
gain O
an O

visual O
information O
is O
critical O
to O
VQA B-DAT
and O
that O
commonsense O
information O
alone O

from O
the O
real O
images O
of O
VQA B-DAT
trainval) O
asking O
subjects O

5 O
VQA B-DAT
BASELINES O
AND O
METHODS O
In O
this O

explore O
the O
difficulty O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

novel O
methods. O
We O
train O
on O
VQA B-DAT
train+val. O
Unless O
stated O
otherwise, O
all O

top O
1K O
answers O
of O
the O
VQA B-DAT
train/val O
dataset O

multiple- O
choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
As O

I O
(Fig. O
8), O
selected O
using O
VQA B-DAT
test-dev O
accuracies) O
on O
VQA O
test O

worse O
than O
human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

system O
is O
correct O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

correct” O
implies O
that O
it O
has O
VQA B-DAT
accuracy O
1.0 O
(see O
section O
3 O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
The O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

frequently O
predicted O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

age O
of O
question) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

system O
is O
correct) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

a O
filtered O
version O
of O
the O
VQA B-DAT
train O
+ O
val O
dataset O
in O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

6 O
VQA B-DAT
CHALLENGE O
AND O
WORKSHOP O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O

task O
of O
Visual O
Question O
Answering O
(VQA B-DAT

multiple-choice O
tasks O
in O
the O
respective O
VQA B-DAT
Real O
Image O
Challenge O
leaderboards O
(as O

datasets O
may O
help O
enable O
practical O
VQA B-DAT
applications. O
We O
believe O
VQA O
has O

questions O
IV O
- O
Details O
on O
VQA B-DAT
baselines O
V O
- O
“Age” O
and O

Leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Open-Ended) O
on O

leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Multiple-Choice) O
on O

Additional O
examples O
from O
the O
VQA B-DAT
dataset O

scenes. O
This O
helps O
motivate O
the O
VQA B-DAT
task O
as O
a O
way O
to O

APPENDIX O
IV: O
DETAILS O
ON O
VQA B-DAT
BASELINES O
“per O
Q-type O
prior” O
baseline O

For O
every O
question O
in O
the O
VQA B-DAT
test-standard O
set, O
we O
find O
its O

norm O
I), O
selected O
using O
VQA B-DAT
test- O
dev O
accuracies). O
To O
estimate O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

a O
random O
selection O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

scenes. O
The O
set O
contains O
over O
100 B-DAT
objects O
and O
31 O
animals O
in O

i.e., O
an O
answer O
is O
deemed O
100 B-DAT

MS O
COCO O
dataset O
[32] O
and O
150, B-DAT

a O
scale O
of O
0 O
− O
100) B-DAT
required O
to O
answer O
a O
question O

choose O
the O
top O
K O
= O
1000 B-DAT
most O
frequent O
answers O
as O
possible O

Question O
(BoW O
Q): O
The O
top O
1,000 B-DAT
words O
in O
the O
questions O
are O

are O
concatenated O
to O
get O
a O
1,030 B-DAT

1000 B-DAT

1000 B-DAT

with O
2 O
hidden O
layers O
and O
1000 B-DAT
hidden O
units O
(dropout O
0.5) O
in O

a O
bag-of-words O
representation O
containing O
the O
1,000 B-DAT
most O
popular O
words O
in O
the O

that O
it O
has O
VQA O
accuracy O
1.0 B-DAT
(see O
section O
3 O
for O
accuracy O

Question O
K O
= O
1000 B-DAT
Human O
To O
Be O
Able O
To O

100 B-DAT
A O

100 B-DAT

100 B-DAT

100 B-DAT

can O
see O
that O
K O
= O
1000 B-DAT
performs O
better O
than O
K O

performs O
better O
then O
K O
= O
1000 B-DAT
by O
0.40% O
for O
open-ended O
task O

blue” O
(28881, O
1.16%), O
“4” O
(27174, O
1.09 B-DAT

outside” O
(1846, O
0.07%), O
“hot O
dog” O
(1809, B-DAT
0.07%), O
“night” O
(1805, O
0.07%), O
“trees O

and O
white” O
(1518, O
0.06%), O
“bedroom” O
(1500, B-DAT
0.06%), O
“bat” O
(1494, O
0.06%), O
“glasses O

0.06%), O
“cloudy” O
(1413, O
0.06%), O
“15” O
(1407, B-DAT
0.06%), O
“up” O
(1399, O
0.06%), O
“blonde O

0.05%), O
“many” O
(1211, O
0.05%), O
“zoo” O
(1204, B-DAT
0.05%), O
“suitcase” O
(1199, O
0.05%), O
“old O

0.04%), O
“mountains” O
(1030, O
0.04%), O
“wall” O
(1009, B-DAT
0.04%), O
“ele- O
phants” O
(1006, O
0.04 O

1504 B-DAT

1504 B-DAT

1408 B-DAT

1506 B-DAT

1409 B-DAT

1502 B-DAT

32] O
and O
a O
newly O
created O
abstract B-DAT
scene O
dataset O
[57], O
[2] O
that O

a O
new O
dataset O
of O
“realistic” O
abstract B-DAT
scenes O
to O
enable O
research O
focused O

output O
answer O
classes. O
[34] O
generates O
abstract B-DAT
scenes O
to O
capture O
visual O
common O

describing O
the O
real O
images O
and O
abstract B-DAT

tasks, O
we O
create O
a O
new O
abstract B-DAT
scenes O
dataset O
[2], O
[57], O
[58 O

winners O
of O
the O
challenge. O
For O
abstract B-DAT
scenes, O
we O
created O
splits O
for O

test-dev, O
test-standard, O
test-challenge, O
test-reserve) O
for O
abstract B-DAT
scenes. O
Captions. O
The O
MS O
COCO O

collected O
five O
single-captions O
for O
all O
abstract B-DAT
scenes O
using O
the O
same O
user O

both O
the O
real O
images O
and O
abstract B-DAT
scenes. O
In O
total, O
three O
questions O

left) O
and O
all O
questions O
for O
abstract B-DAT
scenes O
(right). O
The O
ordering O
of O

1,950,000 O
answers O
for O
50, O
000 O
abstract B-DAT
scenes O

the O
real O
images O
(left) O
and O
abstract B-DAT
scenes O
(right). O
Interestingly, O
the O
distribution O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
This O
helps O
demonstrate O
that O

of O
questions O
elicited O
by O
the O
abstract B-DAT
scenes O
is O
similar O
to O
those O

lengths O
for O
real O
images O
and O
abstract B-DAT
scenes O

90.51%, O
5.89%, O
and O
2.49% O
for O
abstract B-DAT
scenes. O
The O
brevity O
of O
answers O

real O
images O
and O
3,770 O
for O
abstract B-DAT
scenes. O
‘Yes/No’ O
and O
‘Number’ O
Answers O

questions O
on O
real O
images O
and O
abstract B-DAT
scenes O
respectively. O
Among O
these O
‘yes/no O

yes” O
for O
real O
images O
and O
abstract B-DAT
scenes. O
Question O
types O
such O
as O

questions O
on O
real O
images O
and O
abstract B-DAT
scenes O
are O
‘number’ O
questions. O
“2 O

real O
images O
and O
39.85% O
for O
abstract B-DAT
scenes. O
Subject O
Confidence. O
When O
the O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
Inter-human O
Agreement. O
Does O
the O

confident) O
for O
real O
images O
and O
abstract B-DAT
scenes O
(black O
lines). O
Percentage O
of O

both O
real O
images O
(83.30%) O
and O
abstract B-DAT
scenes O
(87.49%). O
Note O
that O
on O

real O
images O
and O
2.39 O
for O
abstract B-DAT
scenes. O
The O
agreement O
is O
significantly O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
See O
the O
appendix O
for O

abstract B-DAT
| O
mc-real O
| O
mc-abstract O

oe-abstract-leaderboard B-DAT
| O
mc-real-leaderboard O
| O
mc- O
abstract O

VI O
- O
Details O
on O
the O
abstract B-DAT
scene O
dataset O
VII O
- O
User O

captions O
collected O
by O
us O
for O
abstract B-DAT
scenes) O
using O
the O
Stanford O
part-of-speech O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
This O
helps O
motivate O
the O

and O
Fig. O
20 O
(adjectives) O
for O
abstract B-DAT
scenes.7 O
The O
left O
side O
shows O

and O
Fig. O
14 O
(right) O
for O
abstract B-DAT
scenes. O
We O
see O
that O
questions O

for O
real O
images O
(left) O
and O
abstract B-DAT
scenes O
(right O

indicating O
the O
normalized O
count O
for O
abstract B-DAT
scenes O

indicating O
the O
normalized O
count O
for O
abstract B-DAT
scenes O

indicating O
the O
normalized O
count O
for O
abstract B-DAT
scenes O

the O
two O
datasets, O
real O
and O
abstract, B-DAT
first O
two O
rows O
are O
the O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
Note O
the O
diversity O
of O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
In O
Table O
6, O
we O

and O
≈ O
11% O
increase O
for O
abstract B-DAT
scenes) O
for O
“other” O
questions. O
Since O

that O
are O
present O
in O
the O
abstract B-DAT
scenes O
dataset. O
For O
more O
examples O

left) O
and O
all O
questions O
for O
abstract B-DAT
scenes O
(right). O
The O
ordering O
of O

top) O
and O
all O
questions O
for O
abstract B-DAT
scenes O
(bottom). O
Each O
column O
corresponds O

the O
objects O
present O
in O
the O
abstract B-DAT
scene O
dataset. O
Right: O
The O
AMT O

interface O
for O
collecting O
abstract B-DAT
scenes. O
The O
light O
green O
circles O

the O
MS O
COCO O
[32] O
images, O
abstract B-DAT
scenes, O
and O
multiple-choice O
questions, O
respectively O

numerous O
representative O
examples O
of O
the O
abstract B-DAT
scene O
dataset O

examples O
of O
the O
real O
and O
abstract B-DAT
scene O
dataset O

ended B-DAT
Visual O
Question O
Answering O
(VQA). O
Given O

ended B-DAT

ended B-DAT
answers O
contain O
only O
a O
few O

task O
of O
free-form O
and O
open- O
ended B-DAT
Visual O
Question O
Answering O
(VQA). O
A O

ended, B-DAT
natural- O
language O
question O
about O
the O

ended B-DAT
questions O
require O
a O
potentially O
vast O

ended B-DAT
questions O
collected O
for O
images O
via O

ended B-DAT
answering O
task O
and O
a O
multiple O

ended B-DAT
task O
that O
requires O
a O
free-form O

ended B-DAT
questions O
offers O
many O
benefits, O
it O

ended, B-DAT
free-form O
questions O
and O
answers O
provided O

ended B-DAT
questions O
result O
in O
a O
diverse O

ended B-DAT
and O
(ii) O
multiple-choice. O
For O
the O

ended B-DAT
task, O
the O
generated O
answers O
are O

ended B-DAT
task, O
the O
accuracy O
of O
a O

ended B-DAT
answers O
to O
open-ended O
questions. O
The O

ended B-DAT
and O
multiple-choice O
tasks. O
Note O
that O

ended B-DAT
task, O
we O
pick O
the O
most O

ended B-DAT
task O
using O
cosine O
similarity O
in O

ended B-DAT
task, O
we O
pick O
the O
most O

ended B-DAT
task O
using O
cosine O
similarity O
in O

on O
two O
different O
tasks: O
open- O
ended B-DAT
selects O
the O
answer O
with O
highest O

ended B-DAT
and O
multiple- O
choice O
tasks O
on O

ended B-DAT
and O
multiple-choice O
tasks O
on O
the O

ended B-DAT

ended B-DAT
task, O
the O
vision-alone O
model O
(I O

ended B-DAT
(53.68% O
on O
multiple-choice) O
and O
LSTM O

Q O
achieving O
48.76% O
on O
open- O
ended B-DAT
(54.75% O
on O
multiple-choice); O
both O
outperforming O

ended B-DAT

ended) B-DAT
/ O
63.09% O
(multiple-choice). O
We O
can O

ended B-DAT

ended B-DAT
test-dev O
results O
for O
different O
question O

I) O
for O
both O
the O
open- O
ended B-DAT
and O
multiple-choice O
tasks O
on O
the O

ended B-DAT
task O
and O
by O
0.24% O
for O

ended B-DAT
task O
and O
by O
1.24% O
for O

ended B-DAT
task O
and O
by O
1.92% O
for O

ended B-DAT
task O
and O
by O
1.16% O
for O

words O
by O
0.24% O
for O
open- O
ended B-DAT
task O
and O
by O
0.17% O
for O

ended B-DAT
task O
and O
by O
0.02% O
for O

ended B-DAT
and O
multiple-choice O
tasks O
on O
the O

ended B-DAT
task O
and O
by O
1.88% O
for O

ended B-DAT

ended B-DAT
and O
multiple-choice O
tasks O
(real O
images O

ended, B-DAT
natural O

ended B-DAT
and O
multiple-choice O
tasks O
in O
the O

ended B-DAT
and O
not O
task-specific. O
For O
some O

ended B-DAT
questions O
that O
are O
answered O
by O

ended B-DAT
answers O
task O
when O
subjects O
were O

ended B-DAT
answer O
task. O
In O
comparison O
to O

open- O
ended B-DAT
answer, O
the O
multiple-choice O
accuracies O
are O

VQA B-DAT

and O
open-ended O
Visual O
Question O
Answering O
(VQA B-DAT

a O
system O
that O
succeeds O
at O
VQA B-DAT
typically O
needs O
a O
more O
detailed O

producing O
generic O
image O
captions. O
Moreover, O
VQA B-DAT
is O
amenable O
to O
automatic O
evaluation O

Numerous O
baselines O
and O
methods O
for O
VQA B-DAT
are O
provided O
and O
compared O
with O

human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

open- O
ended O
Visual O
Question O
Answering O
(VQA B-DAT

). O
A O
VQA B-DAT
system O
takes O
as O
input O
an O

Is O
this O
person O
expecting O
company?”). O
VQA B-DAT
[19], O
[36], O
[50], O
[3] O
is O

the O
high-level O
reasoning O
required O
for O
VQA B-DAT
by O
removing O
the O
need O
to O

29]. O
As O
part O
of O
the O
VQA B-DAT
initiative, O
we O
will O
organize O
an O

state-of-the-art O
methods O
and O
best O
practices. O
VQA B-DAT
poses O
a O
rich O
set O
of O

during O
the O
past O
few O
decades. O
VQA B-DAT
provides O
an O
attractive O
balance O
between O

VQA B-DAT
Efforts. O
Several O
recent O
papers O
have O

difficult O
and O
unconstrained O
task, O
our O
VQA B-DAT
dataset O
is O
two O
orders O
of O

1,449 O
images O
respectively). O
The O
proposed O
VQA B-DAT
task O
has O
connections O
to O
other O

These O
approaches O
provide O
inspiration O
for O
VQA B-DAT
techniques. O
One O
key O
concern O
in O

fixed O
set O
of O
loca- O
tions. O
VQA B-DAT
is O
naturally O
grounded O
in O
images O

Describing O
Visual O
Content. O
Related O
to O
VQA B-DAT
are O
the O
tasks O
of O
image O

by O
[53]). O
The O
questions O
in O
VQA B-DAT
require O
detailed O
specific O
information O
about O

3 O
VQA B-DAT
DATASET O
COLLECTION O

describe O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset. O
We O
begin O
by O
describing O

they O
are O
well-suited O
for O
our O
VQA B-DAT
task. O
The O
more O
diverse O
our O

their O
answers. O
Abstract O
Scenes. O
The O
VQA B-DAT
task O
with O
real O
images O
requires O

the O
high-level O
reasoning O
required O
for O
VQA, B-DAT
but O
not O
the O
low-level O
vision O

test-standard, O
test-challenge, O
test-reserve). O
For O
the O
VQA B-DAT
challenge O
(see O
section O
6), O
test-dev O

default’ O
test O
data O
for O
the O
VQA B-DAT
competition. O
When O
comparing O
to O
the O

sentences O
containing O
multiple O
words. O
In O
VQA, B-DAT
most O
answers O
(89.32%) O
are O
single O

4 O
VQA B-DAT
DATASET O
ANALYSIS O
In O
this O
section O

questions O
and O
answers O
in O
the O
VQA B-DAT
train O
dataset. O
To O
gain O
an O

visual O
information O
is O
critical O
to O
VQA B-DAT
and O
that O
commonsense O
information O
alone O

from O
the O
real O
images O
of O
VQA B-DAT
trainval) O
asking O
subjects O

5 O
VQA B-DAT
BASELINES O
AND O
METHODS O
In O
this O

explore O
the O
difficulty O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

novel O
methods. O
We O
train O
on O
VQA B-DAT
train+val. O
Unless O
stated O
otherwise, O
all O

top O
1K O
answers O
of O
the O
VQA B-DAT
train/val O
dataset O

multiple- O
choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
As O

I O
(Fig. O
8), O
selected O
using O
VQA B-DAT
test-dev O
accuracies) O
on O
VQA O
test O

worse O
than O
human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

system O
is O
correct O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

correct” O
implies O
that O
it O
has O
VQA B-DAT
accuracy O
1.0 O
(see O
section O
3 O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
The O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

frequently O
predicted O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

age O
of O
question) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

system O
is O
correct) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

a O
filtered O
version O
of O
the O
VQA B-DAT
train O
+ O
val O
dataset O
in O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

6 O
VQA B-DAT
CHALLENGE O
AND O
WORKSHOP O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O

task O
of O
Visual O
Question O
Answering O
(VQA B-DAT

multiple-choice O
tasks O
in O
the O
respective O
VQA B-DAT
Real O
Image O
Challenge O
leaderboards O
(as O

datasets O
may O
help O
enable O
practical O
VQA B-DAT
applications. O
We O
believe O
VQA O
has O

questions O
IV O
- O
Details O
on O
VQA B-DAT
baselines O
V O
- O
“Age” O
and O

Leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Open-Ended) O
on O

leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Multiple-Choice) O
on O

Additional O
examples O
from O
the O
VQA B-DAT
dataset O

scenes. O
This O
helps O
motivate O
the O
VQA B-DAT
task O
as O
a O
way O
to O

APPENDIX O
IV: O
DETAILS O
ON O
VQA B-DAT
BASELINES O
“per O
Q-type O
prior” O
baseline O

For O
every O
question O
in O
the O
VQA B-DAT
test-standard O
set, O
we O
find O
its O

norm O
I), O
selected O
using O
VQA B-DAT
test- O
dev O
accuracies). O
To O
estimate O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

a O
random O
selection O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

the O
task O
of O
free-form O
and O
open B-DAT

the O
questions O
and O
answers O
are O
open B-DAT

to O
automatic O
evaluation, O
since O
many O
open B-DAT

is O
still O
a O
difficult O
and O
open B-DAT
research O
problem O
[51], O
[13], O
[22 O

the O
task O
of O
free-form O
and O
open B-DAT

an O
image O
and O
a O
free-form, O
open B-DAT

Fig. O
1: O
Examples O
of O
free-form, O
open B-DAT

paper, O
we O
present O
both O
an O
open B-DAT

task O
[45], O
[33]. O
Unlike O
the O
open B-DAT

answers. O
While O
the O
use O
of O
open B-DAT

contrast, O
our O
proposed O
task O
involves O
open B-DAT

tions: O
(i) O
open B-DAT

and O
(ii) O
multiple-choice. O
For O
the O
open B-DAT

each O
question. O
As O
with O
the O
open B-DAT

recall O
that O
they O
are O
human-provided O
open B-DAT

-ended O
answers O
to O
open B-DAT

answer O
(“yes”) O
for O
both O
the O
open B-DAT

per O
Q-type O
prior: O
For O
the O
open B-DAT

the O
picked O
answer O
for O
the O
open B-DAT

are O
found. O
Next, O
for O
the O
open B-DAT

the O
picked O
answer O
for O
the O
open B-DAT

result O
on O
two O
different O
tasks: O
open B-DAT

of O
our O
methods O
for O
the O
open B-DAT

and O
methods O
for O
both O
the O
open B-DAT

the O
question O
performs O
rather O
poorly O
(open B-DAT

multiple-choice: O
30.53%). O
In O
fact, O
on O
open B-DAT

BoW O
Q O
achieving O
48.09% O
on O
open B-DAT

LSTM O
Q O
achieving O
48.76% O
on O
open B-DAT

outperforming O
the O
nearest O
neighbor O
baseline O
(open B-DAT

VQA O
test- O
standard O
is O
58.16% O
(open B-DAT

on O
multiple-choice O
are O
better O
than O
open B-DAT

norm O
I) O
for O
both O
the O
open B-DAT

the O
performance O
by O
0.16% O
for O
open B-DAT

performs O
better O
by O
0.95% O
for O
open B-DAT

500 O
by O
0.82% O
for O
open B-DAT

1000 O
by O
0.40% O
for O
open B-DAT

questions O
words O
by O
0.24% O
for O
open B-DAT

questions O
words O
by O
0.06% O
for O
open B-DAT

norm O
I) O
for O
the O
open B-DAT

performs O
worse O
by O
1.13% O
for O
open B-DAT

page5. O
Screenshots O
of O
leaderboards O
for O
open B-DAT

norm O
I) O
for O
both O
open B-DAT

Given O
an O
image O
and O
an O
open B-DAT

of O
other O
entries O
for O
the O
open B-DAT

from O
our O
human O
subjects O
were O
open B-DAT

to O
capture O
images O
and O
ask O
open B-DAT

is O
the O
inter-human O
agreement O
for O
open B-DAT

shows O
the O
inter-human O
agreement O
for O
open B-DAT

answer O
task. O
In O
comparison O
to O
open B-DAT

open B-DAT

provide O
a O
dataset O
containing O
∼0.25M O
images, B-DAT
∼0.76M O
questions, O
and O
∼10M O
answers O

free-form, O
open-ended O
questions O
collected O
for O
images B-DAT
via O
Amazon O
Mechanical O
Turk. O
Note O

vision?”). O
Moreover, O
since O
questions O
about O
images B-DAT
often O
tend O
to O
seek O
specific O

large O
dataset O
that O
contains O
204,721 O
images B-DAT
from O
the O
MS O
COCO O
dataset O

The O
MS O
COCO O
dataset O
has O
images B-DAT
depicting O
diverse O
and O
complex O
scenes O

the O
need O
to O
parse O
real O
images B-DAT

250,000 O
vs. O
2,591 O
and O
1,449 O
images B-DAT
respectively). O
The O
proposed O
VQA O
task O

introduced O
a O
dataset O
of O
10k O
images B-DAT
and O
prompted O
captions O
that O
describe O

English O
by O
humans) O
for O
COCO O
images B-DAT

VQA O
is O
naturally O
grounded O
in O
images B-DAT
– O
requiring O
the O
understanding O
of O

both O
text O
(questions) O
and O
vision O
(images B-DAT

begin O
by O
describing O
the O
real O
images B-DAT
and O
abstract O

im- O
ages O
and O
81,434 O
test O
images B-DAT
from O
the O
newly-released O
Microsoft O
Common O

dataset O
was O
gathered O
to O
find O
images B-DAT
containing O
multiple O
objects O
and O
rich O

the O
visual O
complexity O
of O
these O
images, B-DAT
they O
are O
well-suited O
for O
our O

more O
diverse O
our O
collection O
of O
images, B-DAT
the O
more O
diverse, O
comprehensive, O
and O

The O
VQA O
task O
with O
real O
images B-DAT
requires O
the O
use O
of O
complex O

that O
more O
closely O
mirror O
real O
images B-DAT
than O
previous O
papers O
[57], O
[58 O

and O
examples. O
Splits. O
For O
real O
images, B-DAT
we O
follow O
the O
same O
train/val/test O

five O
single-sentence O
captions O
for O
all O
images B-DAT

It O
understands O
a O
lot O
about O
images B-DAT

used O
for O
both O
the O
real O
images B-DAT
and O
abstract O
scenes. O
In O
total O

blue”, O
“4”, O
“green” O
for O
real O
images B-DAT

of O
60K O
questions O
for O
real O
images B-DAT
(left) O
and O
all O
questions O
for O

at O
the O
image) O
for O
204,721 O
images B-DAT
from O
the O
MS O
COCO O
dataset O

questions O
for O
both O
the O
real O
images B-DAT
(left) O
and O
abstract O
scenes O
(right O

quite O
similar O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
This O
helps O

those O
elicited O
by O
the O
real O
images B-DAT

different O
word O
lengths O
for O
real O
images B-DAT
and O
abstract O
scenes O

6.91%, O
and O
2.74% O
for O
real O
images B-DAT
and O
90.51%, O
5.89%, O
and O
2.49 O

elicit O
specific O
information O
from O
the O
images B-DAT

of O
60K O
questions O
for O
real O
images B-DAT
when O
subjects O
provide O
answers O
when O

in O
our O
dataset O
for O
real O
images B-DAT
and O
3,770 O
for O
abstract O
scenes O

of O
the O
questions O
on O
real O
images B-DAT
and O
abstract O
scenes O
respectively. O
Among O

answers O
are O
“yes” O
for O
real O
images B-DAT
and O
abstract O
scenes. O
Question O
types O

of O
the O
questions O
on O
real O
images B-DAT
and O
abstract O
scenes O
are O
‘number O

the O
‘number’ O
answers O
for O
real O
images B-DAT
and O
39.85% O
for O
abstract O
scenes O

as O
confident O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
Inter-human O
Agreement O

1 O
= O
confident) O
for O
real O
images B-DAT
and O
abstract O
scenes O
(black O
lines O

the O
answers O
for O
both O
real O
images B-DAT
(83.30%) O
and O
abstract O
scenes O
(87.49 O

2.70 O
unique O
answers O
for O
real O
images B-DAT
and O
2.39 O
for O
abstract O
scenes O

answers O
provided O
with O
and O
without O
images, B-DAT
we O
show O
the O
distribution O
of O

for O
answers O
with O
and O
without O
images B-DAT

10K O
questions O
from O
the O
real O
images B-DAT
of O
VQA O
trainval) O
asking O
subjects O

of O
3K O
train O
questions O
(1K O
images B-DAT

001) O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
See O
the O

dataset O
for O
the O
MS O
COCO O
images B-DAT
using O
several O
baselines O
and O
novel O

nearest O
neighbor O
questions O
and O
associated O
images B-DAT
from O
the O
training O
set. O
See O

VGGNet O
[48] O
to O
encode O
the O
images B-DAT

the O
VQA O
test-dev O
for O
real O
images B-DAT

the O
VQA O
test-dev O
for O
real O
images B-DAT

different O
question O
types O
on O
real O
images B-DAT
(Q+C O
is O
reported O
on O
val O

the O
VQA O
test-dev O
for O
real O
images B-DAT

the O
VQA O
test-dev O
for O
real O
images B-DAT

open-ended O
and O
multiple-choice O
tasks O
(real O
images) B-DAT
with O
other O
entries O
(as O
of O

a O
dataset O
containing O
over O
250K O
images, B-DAT
760K O
questions, O
and O
around O
10M O

the O
visually O
impaired O
to O
capture O
images B-DAT
and O
ask O
open-ended O
questions O
that O

MS O
COCO O
captions O
for O
real O
images B-DAT
and O
captions O
collected O
by O
us O

001) O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
This O
helps O

Fig. O
17 O
(adjectives) O
for O
real O
images B-DAT
and O
Fig. O
18 O
(nouns), O
Fig O

Fig. O
14 O
(left) O
for O
real O
images B-DAT
and O
Fig. O
14 O
(right) O
for O

question O
& O
answers O
for O
real O
images B-DAT
(left) O
and O
abstract O
scenes O
(right O

the O
normalized O
count O
for O
real O
images B-DAT

the O
normalized O
count O
for O
real O
images B-DAT

the O
normalized O
count O
for O
real O
images B-DAT

five O
words O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
Note O
the O

3,000 O
questions O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
In O
Table O

15% O
increase O
for O
real O
images B-DAT
and O
≈ O
11% O
increase O
for O

of O
questions O
in O
the O
real O
images B-DAT
training O
set O
and O
ensure O
that O

is O
also O
computed O
on O
real O
images B-DAT
training O
set. O
“nearest O
neighbor” O
baseline O

k O
questions O
and O
their O
associated O
images, B-DAT
we O
find O
the O
image O
which O

used O
to O
collect O
questions O
for O
images B-DAT

subjects O
were O
shown O
the O
corresponding O
images B-DAT

of O
60K O
questions O
for O
real O
images B-DAT
(left) O
and O
all O
questions O
for O

of O
60K O
questions O
for O
real O
images B-DAT
(top) O
and O
all O
questions O
for O

250 O
answers O
in O
our O
real O
images B-DAT
dataset O
along O
with O
their O
counts O

for O
the O
MS O
COCO O
[32] O
images, B-DAT
abstract O
scenes, O
and O
multiple-choice O
questions O

approach O
to O
answering O
questions O
about O
images B-DAT

VQA: O
Visual O
Question B-DAT
Answering O
www.visualqa.org O

of O
free-form O
and O
open-ended O
Visual O
Question B-DAT
Answering O
(VQA). O
Given O
an O
image O

free-form O
and O
open- O
ended O
Visual O
Question B-DAT
Answering O
(VQA). O
A O
VQA O
system O

We O
now O
describe O
the O
Visual O
Question B-DAT
Answering O
(VQA) O
dataset. O
We O
begin O

Types O
of O
Question B-DAT

of O
Words O
in O
Question B-DAT

Distribution O
of O
Question B-DAT
Lengths O

real O
images O
and O
abstract O
scenes. O
Question B-DAT
types O
such O
as O
“How O
many O

As O
shown O
in O
Table O
1 O
(Question B-DAT
+ O
Image), O
there O
is O
significant O

Fig. O
2). O
In O
Table O
1 O
(Question), B-DAT
we O
show O
the O
percentage O
of O

Question B-DAT
40.81 O
67.60 O
25.77 O
21.22 O
Real O

Question B-DAT
+ O
Caption* O
57.47 O
78.97 O
39.68 O

Question B-DAT
+ O
Image O
83.30 O
95.77 O
83.39 O

Question B-DAT
43.27 O
66.65 O
28.52 O
23.66 O
Abstract O

Question B-DAT
+ O
Caption* O
54.34 O
74.70 O
41.19 O

Question B-DAT
+ O
Image O
87.49 O
95.96 O
95.04 O

question O
without O
seeing O
the O
image O
(Question), B-DAT
seeing O
just O
a O
caption O
of O

and O
not O
the O
image O
itself O
(Question B-DAT
+ O
Caption), O
and O
seeing O
the O

image O
(Question B-DAT
+ O
Image). O
Results O
are O
shown O

answer O
the O
questions? O
Table O
1 O
(Question B-DAT
+ O
Caption) O
shows O
the O
percentage O

Question B-DAT
Channel: O
This O
channel O
provides O
an O

1) O
Bag-of-Words O
Question B-DAT
(BoW O
Q): O
The O
top O
1,000 O

caption O
embedding O
(Caption). O
For O
BoW O
Question B-DAT
+ O
Caption O
(BoW O
Q O

for O
real O
images. O
Q O
= O
Question, B-DAT
I O
= O
Image, O
C O

Question B-DAT
K O
= O
1000 O
Human O
To O

for O
real O
images. O
Q O
= O
Question, B-DAT
I O
= O
Image. O
See O
text O

introduce O
the O
task O
of O
Visual O
Question B-DAT
Answering O
(VQA). O
Given O
an O
image O

Etzioni. O
Paraphrase-Driven O
Learning O
for O
Open O
Question B-DAT
Answering. O
In O
ACL, O
2013. O
2 O

Zettlemoyer, O
and O
O. O
Etzioni. O
Open O
Question B-DAT
Answering O
over O
Curated O
and O
Extracted O

Fritz. O
A O
Multi-World O
Approach O
to O
Question B-DAT
Answering O
about O
Real-World O
Scenes O
based O

T. O
Mikolov. O
Towards O
AI- O
Complete O
Question B-DAT
Answering: O
A O
Set O
of O
Prerequisite O

VQA: O
Visual B-DAT
Question O
Answering O
www.visualqa.org O

task O
of O
free-form O
and O
open-ended O
Visual B-DAT
Question O
Answering O
(VQA). O
Given O
an O

questions O
and O
answers O
are O
open-ended. O
Visual B-DAT
questions O
selectively O
target O
different O
areas O

of O
free-form O
and O
open- O
ended O
Visual B-DAT
Question O
Answering O
(VQA). O
A O
VQA O

complex O
reasoning O
more O
essential. O
Describing O
Visual B-DAT
Content. O
Related O
to O
VQA O
are O

We O
now O
describe O
the O
Visual B-DAT
Question O
Answering O
(VQA) O
dataset. O
We O

we O
introduce O
the O
task O
of O
Visual B-DAT
Question O
Answering O
(VQA). O
Given O
an O

cloud O
service. O
In O
Mobile O
Cloud O
Visual B-DAT
Media O
Computing, O
pages O
265–290. O
Springer O

D. O
Parikh. O
Zero-Shot O
Learning O
via O
Visual B-DAT
Abstraction. O
In O
ECCV, O
2014. O
2 O

Nearly O
Real- O
time O
Answers O
to O
Visual B-DAT
Questions. O
In O
User O
Interface O
Software O

and O
A. O
Gupta. O
NEIL: O
Extracting O
Visual B-DAT
Knowledge O
from O
Web O
Data. O
In O

Zitnick. O
Mind’s O
Eye: O
A O
Recurrent O
Visual B-DAT
Represen- O
tation O
for O
Image O
Caption O

Long-term O
Recurrent O
Convolutional O
Networks O
for O
Visual B-DAT
Recognition O
and O
Description. O
In O
CVPR O

G. O
Zweig. O
From O
Captions O
to O
Visual B-DAT
Concepts O
and O
Back. O
In O
CVPR O

Hallonquist, O
and O
L. O
Younes. O
A O
Visual B-DAT
Turing O
Test O
for O
Computer O
Vision O

Karpathy O
and O
L. O
Fei-Fei. O
Deep O
Visual B-DAT

and O
R. O
S. O
Zemel. O
Unifying O
Visual B-DAT

Listen, O
Use O
Your O
Imagination: O
Leveraging O
Visual B-DAT
Common O
Sense O
for O
Non-Visual O
Tasks O

Divvala, O
and O
A. O
Farhadi. O
Viske: O
Visual B-DAT
knowledge O
extraction O
and O
question O
answering O

Berg, O
and O
T. O
L. O
Berg. O
Visual B-DAT
madlibs: O
Fill-in-the- O
blank O
description O
generation O

Bringing O
Semantics O
Into O
Focus O
Using O
Visual B-DAT
Abstraction. O
In O
CVPR, O
2013. O
2 O

and O
L. O
Vanderwende. O
Learning O
the O
Visual B-DAT
Interpretation O
of O
Sentences. O
In O
ICCV O

204,721 O
images O
from O
the O
MS O
COCO B-DAT
dataset O
[32] O
and O
a O
newly O

contains O
50,000 O
scenes. O
The O
MS O
COCO B-DAT
dataset O
has O
images O
depicting O
diverse O

to O
English O
by O
humans) O
for O
COCO B-DAT
images. O
[44] O
automatically O
generated O
four O

object, O
count, O
color, O
location) O
using O
COCO B-DAT
captions. O
Text-based O
Q&A O
is O
a O

Common O
Objects O
in O
Context O
(MS O
COCO) B-DAT
[32] O
dataset. O
The O
MS O
COCO O

split O
strategy O
as O
the O
MC O
COCO B-DAT
dataset O
[32] O
(including O
test- O
dev O

abstract O
scenes. O
Captions. O
The O
MS O
COCO B-DAT
dataset O
[32], O
[7] O
already O
contains O

204,721 O
images O
from O
the O
MS O
COCO B-DAT
dataset O
[32] O
and O
150,000 O
questions O

VQA O
dataset O
for O
the O
MS O
COCO B-DAT
images O
using O
several O
baselines O
and O

from O
the O
caption O
data O
(MS O
COCO B-DAT
captions O
for O
real O
images O
and O

VQA O
dataset O
for O
the O
MS O
COCO B-DAT
[32] O
images, O
abstract O
scenes, O
and O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT
captions: O
Data O
collection O
and O
evaluation O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT
Captions: O
Data O
Collection O
and O
Evaluation O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT

VQA: O
Visual O
Question O
Answering B-DAT
www.visualqa.org O

free-form O
and O
open-ended O
Visual O
Question O
Answering B-DAT
(VQA). O
Given O
an O
image O
and O

and O
open- O
ended O
Visual O
Question O
Answering B-DAT
(VQA). O
A O
VQA O
system O
takes O

now O
describe O
the O
Visual O
Question O
Answering B-DAT
(VQA) O
dataset. O
We O
begin O
by O

the O
task O
of O
Visual O
Question O
Answering B-DAT
(VQA). O
Given O
an O
image O
and O

Paraphrase-Driven O
Learning O
for O
Open O
Question O
Answering B-DAT

and O
O. O
Etzioni. O
Open O
Question O
Answering B-DAT
over O
Curated O
and O
Extracted O
Knowledge O

A O
Multi-World O
Approach O
to O
Question O
Answering B-DAT
about O
Real-World O
Scenes O
based O
on O

Parsing O
for O
Understanding O
Events O
and O
Answering B-DAT
Queries. O
IEEE O
MultiMedia, O
2014. O
1 O

Mikolov. O
Towards O
AI- O
Complete O
Question O
Answering B-DAT

VQA B-DAT

and O
open-ended O
Visual O
Question O
Answering O
(VQA B-DAT

a O
system O
that O
succeeds O
at O
VQA B-DAT
typically O
needs O
a O
more O
detailed O

producing O
generic O
image O
captions. O
Moreover, O
VQA B-DAT
is O
amenable O
to O
automatic O
evaluation O

Numerous O
baselines O
and O
methods O
for O
VQA B-DAT
are O
provided O
and O
compared O
with O

human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

open- O
ended O
Visual O
Question O
Answering O
(VQA B-DAT

). O
A O
VQA B-DAT
system O
takes O
as O
input O
an O

Is O
this O
person O
expecting O
company?”). O
VQA B-DAT
[19], O
[36], O
[50], O
[3] O
is O

the O
high-level O
reasoning O
required O
for O
VQA B-DAT
by O
removing O
the O
need O
to O

29]. O
As O
part O
of O
the O
VQA B-DAT
initiative, O
we O
will O
organize O
an O

state-of-the-art O
methods O
and O
best O
practices. O
VQA B-DAT
poses O
a O
rich O
set O
of O

during O
the O
past O
few O
decades. O
VQA B-DAT
provides O
an O
attractive O
balance O
between O

VQA B-DAT
Efforts. O
Several O
recent O
papers O
have O

difficult O
and O
unconstrained O
task, O
our O
VQA B-DAT
dataset O
is O
two O
orders O
of O

1,449 O
images O
respectively). O
The O
proposed O
VQA B-DAT
task O
has O
connections O
to O
other O

These O
approaches O
provide O
inspiration O
for O
VQA B-DAT
techniques. O
One O
key O
concern O
in O

fixed O
set O
of O
loca- O
tions. O
VQA B-DAT
is O
naturally O
grounded O
in O
images O

Describing O
Visual O
Content. O
Related O
to O
VQA B-DAT
are O
the O
tasks O
of O
image O

by O
[53]). O
The O
questions O
in O
VQA B-DAT
require O
detailed O
specific O
information O
about O

3 O
VQA B-DAT
DATASET O
COLLECTION O

describe O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset. O
We O
begin O
by O
describing O

they O
are O
well-suited O
for O
our O
VQA B-DAT
task. O
The O
more O
diverse O
our O

their O
answers. O
Abstract O
Scenes. O
The O
VQA B-DAT
task O
with O
real O
images O
requires O

the O
high-level O
reasoning O
required O
for O
VQA, B-DAT
but O
not O
the O
low-level O
vision O

test-standard, O
test-challenge, O
test-reserve). O
For O
the O
VQA B-DAT
challenge O
(see O
section O
6), O
test-dev O

default’ O
test O
data O
for O
the O
VQA B-DAT
competition. O
When O
comparing O
to O
the O

sentences O
containing O
multiple O
words. O
In O
VQA, B-DAT
most O
answers O
(89.32%) O
are O
single O

4 O
VQA B-DAT
DATASET O
ANALYSIS O
In O
this O
section O

questions O
and O
answers O
in O
the O
VQA B-DAT
train O
dataset. O
To O
gain O
an O

visual O
information O
is O
critical O
to O
VQA B-DAT
and O
that O
commonsense O
information O
alone O

from O
the O
real O
images O
of O
VQA B-DAT
trainval) O
asking O
subjects O

5 O
VQA B-DAT
BASELINES O
AND O
METHODS O
In O
this O

explore O
the O
difficulty O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

novel O
methods. O
We O
train O
on O
VQA B-DAT
train+val. O
Unless O
stated O
otherwise, O
all O

top O
1K O
answers O
of O
the O
VQA B-DAT
train/val O
dataset O

multiple- O
choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
As O

I O
(Fig. O
8), O
selected O
using O
VQA B-DAT
test-dev O
accuracies) O
on O
VQA O
test O

worse O
than O
human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

system O
is O
correct O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

correct” O
implies O
that O
it O
has O
VQA B-DAT
accuracy O
1.0 O
(see O
section O
3 O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
The O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

frequently O
predicted O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

age O
of O
question) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

system O
is O
correct) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

a O
filtered O
version O
of O
the O
VQA B-DAT
train O
+ O
val O
dataset O
in O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

6 O
VQA B-DAT
CHALLENGE O
AND O
WORKSHOP O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O

task O
of O
Visual O
Question O
Answering O
(VQA B-DAT

multiple-choice O
tasks O
in O
the O
respective O
VQA B-DAT
Real O
Image O
Challenge O
leaderboards O
(as O

datasets O
may O
help O
enable O
practical O
VQA B-DAT
applications. O
We O
believe O
VQA O
has O

questions O
IV O
- O
Details O
on O
VQA B-DAT
baselines O
V O
- O
“Age” O
and O

Leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Open-Ended) O
on O

leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Multiple-Choice) O
on O

Additional O
examples O
from O
the O
VQA B-DAT
dataset O

scenes. O
This O
helps O
motivate O
the O
VQA B-DAT
task O
as O
a O
way O
to O

APPENDIX O
IV: O
DETAILS O
ON O
VQA B-DAT
BASELINES O
“per O
Q-type O
prior” O
baseline O

For O
every O
question O
in O
the O
VQA B-DAT
test-standard O
set, O
we O
find O
its O

norm O
I), O
selected O
using O
VQA B-DAT
test- O
dev O
accuracies). O
To O
estimate O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

a O
random O
selection O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

scenes. O
The O
set O
contains O
over O
100 B-DAT
objects O
and O
31 O
animals O
in O

i.e., O
an O
answer O
is O
deemed O
100 B-DAT

MS O
COCO O
dataset O
[32] O
and O
150, B-DAT

a O
scale O
of O
0 O
− O
100) B-DAT
required O
to O
answer O
a O
question O

choose O
the O
top O
K O
= O
1000 B-DAT
most O
frequent O
answers O
as O
possible O

Question O
(BoW O
Q): O
The O
top O
1,000 B-DAT
words O
in O
the O
questions O
are O

are O
concatenated O
to O
get O
a O
1,030 B-DAT

1000 B-DAT

1000 B-DAT

with O
2 O
hidden O
layers O
and O
1000 B-DAT
hidden O
units O
(dropout O
0.5) O
in O

a O
bag-of-words O
representation O
containing O
the O
1,000 B-DAT
most O
popular O
words O
in O
the O

that O
it O
has O
VQA O
accuracy O
1.0 B-DAT
(see O
section O
3 O
for O
accuracy O

Question O
K O
= O
1000 B-DAT
Human O
To O
Be O
Able O
To O

100 B-DAT
A O

100 B-DAT

100 B-DAT

100 B-DAT

can O
see O
that O
K O
= O
1000 B-DAT
performs O
better O
than O
K O

performs O
better O
then O
K O
= O
1000 B-DAT
by O
0.40% O
for O
open-ended O
task O

blue” O
(28881, O
1.16%), O
“4” O
(27174, O
1.09 B-DAT

outside” O
(1846, O
0.07%), O
“hot O
dog” O
(1809, B-DAT
0.07%), O
“night” O
(1805, O
0.07%), O
“trees O

and O
white” O
(1518, O
0.06%), O
“bedroom” O
(1500, B-DAT
0.06%), O
“bat” O
(1494, O
0.06%), O
“glasses O

0.06%), O
“cloudy” O
(1413, O
0.06%), O
“15” O
(1407, B-DAT
0.06%), O
“up” O
(1399, O
0.06%), O
“blonde O

0.05%), O
“many” O
(1211, O
0.05%), O
“zoo” O
(1204, B-DAT
0.05%), O
“suitcase” O
(1199, O
0.05%), O
“old O

0.04%), O
“mountains” O
(1030, O
0.04%), O
“wall” O
(1009, B-DAT
0.04%), O
“ele- O
phants” O
(1006, O
0.04 O

1504 B-DAT

1504 B-DAT

1408 B-DAT

1506 B-DAT

1409 B-DAT

1502 B-DAT

32] O
and O
a O
newly O
created O
abstract B-DAT
scene O
dataset O
[57], O
[2] O
that O

a O
new O
dataset O
of O
“realistic” O
abstract B-DAT
scenes O
to O
enable O
research O
focused O

output O
answer O
classes. O
[34] O
generates O
abstract B-DAT
scenes O
to O
capture O
visual O
common O

describing O
the O
real O
images O
and O
abstract B-DAT

tasks, O
we O
create O
a O
new O
abstract B-DAT
scenes O
dataset O
[2], O
[57], O
[58 O

winners O
of O
the O
challenge. O
For O
abstract B-DAT
scenes, O
we O
created O
splits O
for O

test-dev, O
test-standard, O
test-challenge, O
test-reserve) O
for O
abstract B-DAT
scenes. O
Captions. O
The O
MS O
COCO O

collected O
five O
single-captions O
for O
all O
abstract B-DAT
scenes O
using O
the O
same O
user O

both O
the O
real O
images O
and O
abstract B-DAT
scenes. O
In O
total, O
three O
questions O

left) O
and O
all O
questions O
for O
abstract B-DAT
scenes O
(right). O
The O
ordering O
of O

1,950,000 O
answers O
for O
50, O
000 O
abstract B-DAT
scenes O

the O
real O
images O
(left) O
and O
abstract B-DAT
scenes O
(right). O
Interestingly, O
the O
distribution O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
This O
helps O
demonstrate O
that O

of O
questions O
elicited O
by O
the O
abstract B-DAT
scenes O
is O
similar O
to O
those O

lengths O
for O
real O
images O
and O
abstract B-DAT
scenes O

90.51%, O
5.89%, O
and O
2.49% O
for O
abstract B-DAT
scenes. O
The O
brevity O
of O
answers O

real O
images O
and O
3,770 O
for O
abstract B-DAT
scenes. O
‘Yes/No’ O
and O
‘Number’ O
Answers O

questions O
on O
real O
images O
and O
abstract B-DAT
scenes O
respectively. O
Among O
these O
‘yes/no O

yes” O
for O
real O
images O
and O
abstract B-DAT
scenes. O
Question O
types O
such O
as O

questions O
on O
real O
images O
and O
abstract B-DAT
scenes O
are O
‘number’ O
questions. O
“2 O

real O
images O
and O
39.85% O
for O
abstract B-DAT
scenes. O
Subject O
Confidence. O
When O
the O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
Inter-human O
Agreement. O
Does O
the O

confident) O
for O
real O
images O
and O
abstract B-DAT
scenes O
(black O
lines). O
Percentage O
of O

both O
real O
images O
(83.30%) O
and O
abstract B-DAT
scenes O
(87.49%). O
Note O
that O
on O

real O
images O
and O
2.39 O
for O
abstract B-DAT
scenes. O
The O
agreement O
is O
significantly O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
See O
the O
appendix O
for O

abstract B-DAT
| O
mc-real O
| O
mc-abstract O

oe-abstract-leaderboard B-DAT
| O
mc-real-leaderboard O
| O
mc- O
abstract O

VI O
- O
Details O
on O
the O
abstract B-DAT
scene O
dataset O
VII O
- O
User O

captions O
collected O
by O
us O
for O
abstract B-DAT
scenes) O
using O
the O
Stanford O
part-of-speech O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
This O
helps O
motivate O
the O

and O
Fig. O
20 O
(adjectives) O
for O
abstract B-DAT
scenes.7 O
The O
left O
side O
shows O

and O
Fig. O
14 O
(right) O
for O
abstract B-DAT
scenes. O
We O
see O
that O
questions O

for O
real O
images O
(left) O
and O
abstract B-DAT
scenes O
(right O

indicating O
the O
normalized O
count O
for O
abstract B-DAT
scenes O

indicating O
the O
normalized O
count O
for O
abstract B-DAT
scenes O

indicating O
the O
normalized O
count O
for O
abstract B-DAT
scenes O

the O
two O
datasets, O
real O
and O
abstract, B-DAT
first O
two O
rows O
are O
the O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
Note O
the O
diversity O
of O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
In O
Table O
6, O
we O

and O
≈ O
11% O
increase O
for O
abstract B-DAT
scenes) O
for O
“other” O
questions. O
Since O

that O
are O
present O
in O
the O
abstract B-DAT
scenes O
dataset. O
For O
more O
examples O

left) O
and O
all O
questions O
for O
abstract B-DAT
scenes O
(right). O
The O
ordering O
of O

top) O
and O
all O
questions O
for O
abstract B-DAT
scenes O
(bottom). O
Each O
column O
corresponds O

the O
objects O
present O
in O
the O
abstract B-DAT
scene O
dataset. O
Right: O
The O
AMT O

interface O
for O
collecting O
abstract B-DAT
scenes. O
The O
light O
green O
circles O

the O
MS O
COCO O
[32] O
images, O
abstract B-DAT
scenes, O
and O
multiple-choice O
questions, O
respectively O

numerous O
representative O
examples O
of O
the O
abstract B-DAT
scene O
dataset O

examples O
of O
the O
real O
and O
abstract B-DAT
scene O
dataset O

ended B-DAT
Visual O
Question O
Answering O
(VQA). O
Given O

ended B-DAT

ended B-DAT
answers O
contain O
only O
a O
few O

task O
of O
free-form O
and O
open- O
ended B-DAT
Visual O
Question O
Answering O
(VQA). O
A O

ended, B-DAT
natural- O
language O
question O
about O
the O

ended B-DAT
questions O
require O
a O
potentially O
vast O

ended B-DAT
questions O
collected O
for O
images O
via O

ended B-DAT
answering O
task O
and O
a O
multiple O

ended B-DAT
task O
that O
requires O
a O
free-form O

ended B-DAT
questions O
offers O
many O
benefits, O
it O

ended, B-DAT
free-form O
questions O
and O
answers O
provided O

ended B-DAT
questions O
result O
in O
a O
diverse O

ended B-DAT
and O
(ii) O
multiple-choice. O
For O
the O

ended B-DAT
task, O
the O
generated O
answers O
are O

ended B-DAT
task, O
the O
accuracy O
of O
a O

ended B-DAT
answers O
to O
open-ended O
questions. O
The O

ended B-DAT
and O
multiple-choice O
tasks. O
Note O
that O

ended B-DAT
task, O
we O
pick O
the O
most O

ended B-DAT
task O
using O
cosine O
similarity O
in O

ended B-DAT
task, O
we O
pick O
the O
most O

ended B-DAT
task O
using O
cosine O
similarity O
in O

on O
two O
different O
tasks: O
open- O
ended B-DAT
selects O
the O
answer O
with O
highest O

ended B-DAT
and O
multiple- O
choice O
tasks O
on O

ended B-DAT
and O
multiple-choice O
tasks O
on O
the O

ended B-DAT

ended B-DAT
task, O
the O
vision-alone O
model O
(I O

ended B-DAT
(53.68% O
on O
multiple-choice) O
and O
LSTM O

Q O
achieving O
48.76% O
on O
open- O
ended B-DAT
(54.75% O
on O
multiple-choice); O
both O
outperforming O

ended B-DAT

ended) B-DAT
/ O
63.09% O
(multiple-choice). O
We O
can O

ended B-DAT

ended B-DAT
test-dev O
results O
for O
different O
question O

I) O
for O
both O
the O
open- O
ended B-DAT
and O
multiple-choice O
tasks O
on O
the O

ended B-DAT
task O
and O
by O
0.24% O
for O

ended B-DAT
task O
and O
by O
1.24% O
for O

ended B-DAT
task O
and O
by O
1.92% O
for O

ended B-DAT
task O
and O
by O
1.16% O
for O

words O
by O
0.24% O
for O
open- O
ended B-DAT
task O
and O
by O
0.17% O
for O

ended B-DAT
task O
and O
by O
0.02% O
for O

ended B-DAT
and O
multiple-choice O
tasks O
on O
the O

ended B-DAT
task O
and O
by O
1.88% O
for O

ended B-DAT

ended B-DAT
and O
multiple-choice O
tasks O
(real O
images O

ended, B-DAT
natural O

ended B-DAT
and O
multiple-choice O
tasks O
in O
the O

ended B-DAT
and O
not O
task-specific. O
For O
some O

ended B-DAT
questions O
that O
are O
answered O
by O

ended B-DAT
answers O
task O
when O
subjects O
were O

ended B-DAT
answer O
task. O
In O
comparison O
to O

open- O
ended B-DAT
answer, O
the O
multiple-choice O
accuracies O
are O

VQA B-DAT

and O
open-ended O
Visual O
Question O
Answering O
(VQA B-DAT

a O
system O
that O
succeeds O
at O
VQA B-DAT
typically O
needs O
a O
more O
detailed O

producing O
generic O
image O
captions. O
Moreover, O
VQA B-DAT
is O
amenable O
to O
automatic O
evaluation O

Numerous O
baselines O
and O
methods O
for O
VQA B-DAT
are O
provided O
and O
compared O
with O

human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

open- O
ended O
Visual O
Question O
Answering O
(VQA B-DAT

). O
A O
VQA B-DAT
system O
takes O
as O
input O
an O

Is O
this O
person O
expecting O
company?”). O
VQA B-DAT
[19], O
[36], O
[50], O
[3] O
is O

the O
high-level O
reasoning O
required O
for O
VQA B-DAT
by O
removing O
the O
need O
to O

29]. O
As O
part O
of O
the O
VQA B-DAT
initiative, O
we O
will O
organize O
an O

state-of-the-art O
methods O
and O
best O
practices. O
VQA B-DAT
poses O
a O
rich O
set O
of O

during O
the O
past O
few O
decades. O
VQA B-DAT
provides O
an O
attractive O
balance O
between O

VQA B-DAT
Efforts. O
Several O
recent O
papers O
have O

difficult O
and O
unconstrained O
task, O
our O
VQA B-DAT
dataset O
is O
two O
orders O
of O

1,449 O
images O
respectively). O
The O
proposed O
VQA B-DAT
task O
has O
connections O
to O
other O

These O
approaches O
provide O
inspiration O
for O
VQA B-DAT
techniques. O
One O
key O
concern O
in O

fixed O
set O
of O
loca- O
tions. O
VQA B-DAT
is O
naturally O
grounded O
in O
images O

Describing O
Visual O
Content. O
Related O
to O
VQA B-DAT
are O
the O
tasks O
of O
image O

by O
[53]). O
The O
questions O
in O
VQA B-DAT
require O
detailed O
specific O
information O
about O

3 O
VQA B-DAT
DATASET O
COLLECTION O

describe O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset. O
We O
begin O
by O
describing O

they O
are O
well-suited O
for O
our O
VQA B-DAT
task. O
The O
more O
diverse O
our O

their O
answers. O
Abstract O
Scenes. O
The O
VQA B-DAT
task O
with O
real O
images O
requires O

the O
high-level O
reasoning O
required O
for O
VQA, B-DAT
but O
not O
the O
low-level O
vision O

test-standard, O
test-challenge, O
test-reserve). O
For O
the O
VQA B-DAT
challenge O
(see O
section O
6), O
test-dev O

default’ O
test O
data O
for O
the O
VQA B-DAT
competition. O
When O
comparing O
to O
the O

sentences O
containing O
multiple O
words. O
In O
VQA, B-DAT
most O
answers O
(89.32%) O
are O
single O

4 O
VQA B-DAT
DATASET O
ANALYSIS O
In O
this O
section O

questions O
and O
answers O
in O
the O
VQA B-DAT
train O
dataset. O
To O
gain O
an O

visual O
information O
is O
critical O
to O
VQA B-DAT
and O
that O
commonsense O
information O
alone O

from O
the O
real O
images O
of O
VQA B-DAT
trainval) O
asking O
subjects O

5 O
VQA B-DAT
BASELINES O
AND O
METHODS O
In O
this O

explore O
the O
difficulty O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

novel O
methods. O
We O
train O
on O
VQA B-DAT
train+val. O
Unless O
stated O
otherwise, O
all O

top O
1K O
answers O
of O
the O
VQA B-DAT
train/val O
dataset O

multiple- O
choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
As O

I O
(Fig. O
8), O
selected O
using O
VQA B-DAT
test-dev O
accuracies) O
on O
VQA O
test O

worse O
than O
human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

system O
is O
correct O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

correct” O
implies O
that O
it O
has O
VQA B-DAT
accuracy O
1.0 O
(see O
section O
3 O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
The O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

frequently O
predicted O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

age O
of O
question) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

system O
is O
correct) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

a O
filtered O
version O
of O
the O
VQA B-DAT
train O
+ O
val O
dataset O
in O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

6 O
VQA B-DAT
CHALLENGE O
AND O
WORKSHOP O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O

task O
of O
Visual O
Question O
Answering O
(VQA B-DAT

multiple-choice O
tasks O
in O
the O
respective O
VQA B-DAT
Real O
Image O
Challenge O
leaderboards O
(as O

datasets O
may O
help O
enable O
practical O
VQA B-DAT
applications. O
We O
believe O
VQA O
has O

questions O
IV O
- O
Details O
on O
VQA B-DAT
baselines O
V O
- O
“Age” O
and O

Leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Open-Ended) O
on O

leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Multiple-Choice) O
on O

Additional O
examples O
from O
the O
VQA B-DAT
dataset O

scenes. O
This O
helps O
motivate O
the O
VQA B-DAT
task O
as O
a O
way O
to O

APPENDIX O
IV: O
DETAILS O
ON O
VQA B-DAT
BASELINES O
“per O
Q-type O
prior” O
baseline O

For O
every O
question O
in O
the O
VQA B-DAT
test-standard O
set, O
we O
find O
its O

norm O
I), O
selected O
using O
VQA B-DAT
test- O
dev O
accuracies). O
To O
estimate O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

a O
random O
selection O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

the O
task O
of O
free-form O
and O
open B-DAT

the O
questions O
and O
answers O
are O
open B-DAT

to O
automatic O
evaluation, O
since O
many O
open B-DAT

is O
still O
a O
difficult O
and O
open B-DAT
research O
problem O
[51], O
[13], O
[22 O

the O
task O
of O
free-form O
and O
open B-DAT

an O
image O
and O
a O
free-form, O
open B-DAT

Fig. O
1: O
Examples O
of O
free-form, O
open B-DAT

paper, O
we O
present O
both O
an O
open B-DAT

task O
[45], O
[33]. O
Unlike O
the O
open B-DAT

answers. O
While O
the O
use O
of O
open B-DAT

contrast, O
our O
proposed O
task O
involves O
open B-DAT

tions: O
(i) O
open B-DAT

and O
(ii) O
multiple-choice. O
For O
the O
open B-DAT

each O
question. O
As O
with O
the O
open B-DAT

recall O
that O
they O
are O
human-provided O
open B-DAT

-ended O
answers O
to O
open B-DAT

answer O
(“yes”) O
for O
both O
the O
open B-DAT

per O
Q-type O
prior: O
For O
the O
open B-DAT

the O
picked O
answer O
for O
the O
open B-DAT

are O
found. O
Next, O
for O
the O
open B-DAT

the O
picked O
answer O
for O
the O
open B-DAT

result O
on O
two O
different O
tasks: O
open B-DAT

of O
our O
methods O
for O
the O
open B-DAT

and O
methods O
for O
both O
the O
open B-DAT

the O
question O
performs O
rather O
poorly O
(open B-DAT

multiple-choice: O
30.53%). O
In O
fact, O
on O
open B-DAT

BoW O
Q O
achieving O
48.09% O
on O
open B-DAT

LSTM O
Q O
achieving O
48.76% O
on O
open B-DAT

outperforming O
the O
nearest O
neighbor O
baseline O
(open B-DAT

VQA O
test- O
standard O
is O
58.16% O
(open B-DAT

on O
multiple-choice O
are O
better O
than O
open B-DAT

norm O
I) O
for O
both O
the O
open B-DAT

the O
performance O
by O
0.16% O
for O
open B-DAT

performs O
better O
by O
0.95% O
for O
open B-DAT

500 O
by O
0.82% O
for O
open B-DAT

1000 O
by O
0.40% O
for O
open B-DAT

questions O
words O
by O
0.24% O
for O
open B-DAT

questions O
words O
by O
0.06% O
for O
open B-DAT

norm O
I) O
for O
the O
open B-DAT

performs O
worse O
by O
1.13% O
for O
open B-DAT

page5. O
Screenshots O
of O
leaderboards O
for O
open B-DAT

norm O
I) O
for O
both O
open B-DAT

Given O
an O
image O
and O
an O
open B-DAT

of O
other O
entries O
for O
the O
open B-DAT

from O
our O
human O
subjects O
were O
open B-DAT

to O
capture O
images O
and O
ask O
open B-DAT

is O
the O
inter-human O
agreement O
for O
open B-DAT

shows O
the O
inter-human O
agreement O
for O
open B-DAT

answer O
task. O
In O
comparison O
to O
open B-DAT

open B-DAT

provide O
a O
dataset O
containing O
∼0.25M O
images, B-DAT
∼0.76M O
questions, O
and O
∼10M O
answers O

free-form, O
open-ended O
questions O
collected O
for O
images B-DAT
via O
Amazon O
Mechanical O
Turk. O
Note O

vision?”). O
Moreover, O
since O
questions O
about O
images B-DAT
often O
tend O
to O
seek O
specific O

large O
dataset O
that O
contains O
204,721 O
images B-DAT
from O
the O
MS O
COCO O
dataset O

The O
MS O
COCO O
dataset O
has O
images B-DAT
depicting O
diverse O
and O
complex O
scenes O

the O
need O
to O
parse O
real O
images B-DAT

250,000 O
vs. O
2,591 O
and O
1,449 O
images B-DAT
respectively). O
The O
proposed O
VQA O
task O

introduced O
a O
dataset O
of O
10k O
images B-DAT
and O
prompted O
captions O
that O
describe O

English O
by O
humans) O
for O
COCO O
images B-DAT

VQA O
is O
naturally O
grounded O
in O
images B-DAT
– O
requiring O
the O
understanding O
of O

both O
text O
(questions) O
and O
vision O
(images B-DAT

begin O
by O
describing O
the O
real O
images B-DAT
and O
abstract O

im- O
ages O
and O
81,434 O
test O
images B-DAT
from O
the O
newly-released O
Microsoft O
Common O

dataset O
was O
gathered O
to O
find O
images B-DAT
containing O
multiple O
objects O
and O
rich O

the O
visual O
complexity O
of O
these O
images, B-DAT
they O
are O
well-suited O
for O
our O

more O
diverse O
our O
collection O
of O
images, B-DAT
the O
more O
diverse, O
comprehensive, O
and O

The O
VQA O
task O
with O
real O
images B-DAT
requires O
the O
use O
of O
complex O

that O
more O
closely O
mirror O
real O
images B-DAT
than O
previous O
papers O
[57], O
[58 O

and O
examples. O
Splits. O
For O
real O
images, B-DAT
we O
follow O
the O
same O
train/val/test O

five O
single-sentence O
captions O
for O
all O
images B-DAT

It O
understands O
a O
lot O
about O
images B-DAT

used O
for O
both O
the O
real O
images B-DAT
and O
abstract O
scenes. O
In O
total O

blue”, O
“4”, O
“green” O
for O
real O
images B-DAT

of O
60K O
questions O
for O
real O
images B-DAT
(left) O
and O
all O
questions O
for O

at O
the O
image) O
for O
204,721 O
images B-DAT
from O
the O
MS O
COCO O
dataset O

questions O
for O
both O
the O
real O
images B-DAT
(left) O
and O
abstract O
scenes O
(right O

quite O
similar O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
This O
helps O

those O
elicited O
by O
the O
real O
images B-DAT

different O
word O
lengths O
for O
real O
images B-DAT
and O
abstract O
scenes O

6.91%, O
and O
2.74% O
for O
real O
images B-DAT
and O
90.51%, O
5.89%, O
and O
2.49 O

elicit O
specific O
information O
from O
the O
images B-DAT

of O
60K O
questions O
for O
real O
images B-DAT
when O
subjects O
provide O
answers O
when O

in O
our O
dataset O
for O
real O
images B-DAT
and O
3,770 O
for O
abstract O
scenes O

of O
the O
questions O
on O
real O
images B-DAT
and O
abstract O
scenes O
respectively. O
Among O

answers O
are O
“yes” O
for O
real O
images B-DAT
and O
abstract O
scenes. O
Question O
types O

of O
the O
questions O
on O
real O
images B-DAT
and O
abstract O
scenes O
are O
‘number O

the O
‘number’ O
answers O
for O
real O
images B-DAT
and O
39.85% O
for O
abstract O
scenes O

as O
confident O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
Inter-human O
Agreement O

1 O
= O
confident) O
for O
real O
images B-DAT
and O
abstract O
scenes O
(black O
lines O

the O
answers O
for O
both O
real O
images B-DAT
(83.30%) O
and O
abstract O
scenes O
(87.49 O

2.70 O
unique O
answers O
for O
real O
images B-DAT
and O
2.39 O
for O
abstract O
scenes O

answers O
provided O
with O
and O
without O
images, B-DAT
we O
show O
the O
distribution O
of O

for O
answers O
with O
and O
without O
images B-DAT

10K O
questions O
from O
the O
real O
images B-DAT
of O
VQA O
trainval) O
asking O
subjects O

of O
3K O
train O
questions O
(1K O
images B-DAT

001) O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
See O
the O

dataset O
for O
the O
MS O
COCO O
images B-DAT
using O
several O
baselines O
and O
novel O

nearest O
neighbor O
questions O
and O
associated O
images B-DAT
from O
the O
training O
set. O
See O

VGGNet O
[48] O
to O
encode O
the O
images B-DAT

the O
VQA O
test-dev O
for O
real O
images B-DAT

the O
VQA O
test-dev O
for O
real O
images B-DAT

different O
question O
types O
on O
real O
images B-DAT
(Q+C O
is O
reported O
on O
val O

the O
VQA O
test-dev O
for O
real O
images B-DAT

the O
VQA O
test-dev O
for O
real O
images B-DAT

open-ended O
and O
multiple-choice O
tasks O
(real O
images) B-DAT
with O
other O
entries O
(as O
of O

a O
dataset O
containing O
over O
250K O
images, B-DAT
760K O
questions, O
and O
around O
10M O

the O
visually O
impaired O
to O
capture O
images B-DAT
and O
ask O
open-ended O
questions O
that O

MS O
COCO O
captions O
for O
real O
images B-DAT
and O
captions O
collected O
by O
us O

001) O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
This O
helps O

Fig. O
17 O
(adjectives) O
for O
real O
images B-DAT
and O
Fig. O
18 O
(nouns), O
Fig O

Fig. O
14 O
(left) O
for O
real O
images B-DAT
and O
Fig. O
14 O
(right) O
for O

question O
& O
answers O
for O
real O
images B-DAT
(left) O
and O
abstract O
scenes O
(right O

the O
normalized O
count O
for O
real O
images B-DAT

the O
normalized O
count O
for O
real O
images B-DAT

the O
normalized O
count O
for O
real O
images B-DAT

five O
words O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
Note O
the O

3,000 O
questions O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
In O
Table O

15% O
increase O
for O
real O
images B-DAT
and O
≈ O
11% O
increase O
for O

of O
questions O
in O
the O
real O
images B-DAT
training O
set O
and O
ensure O
that O

is O
also O
computed O
on O
real O
images B-DAT
training O
set. O
“nearest O
neighbor” O
baseline O

k O
questions O
and O
their O
associated O
images, B-DAT
we O
find O
the O
image O
which O

used O
to O
collect O
questions O
for O
images B-DAT

subjects O
were O
shown O
the O
corresponding O
images B-DAT

of O
60K O
questions O
for O
real O
images B-DAT
(left) O
and O
all O
questions O
for O

of O
60K O
questions O
for O
real O
images B-DAT
(top) O
and O
all O
questions O
for O

250 O
answers O
in O
our O
real O
images B-DAT
dataset O
along O
with O
their O
counts O

for O
the O
MS O
COCO O
[32] O
images, B-DAT
abstract O
scenes, O
and O
multiple-choice O
questions O

approach O
to O
answering O
questions O
about O
images B-DAT

VQA: O
Visual O
Question B-DAT
Answering O
www.visualqa.org O

of O
free-form O
and O
open-ended O
Visual O
Question B-DAT
Answering O
(VQA). O
Given O
an O
image O

free-form O
and O
open- O
ended O
Visual O
Question B-DAT
Answering O
(VQA). O
A O
VQA O
system O

We O
now O
describe O
the O
Visual O
Question B-DAT
Answering O
(VQA) O
dataset. O
We O
begin O

Types O
of O
Question B-DAT

of O
Words O
in O
Question B-DAT

Distribution O
of O
Question B-DAT
Lengths O

real O
images O
and O
abstract O
scenes. O
Question B-DAT
types O
such O
as O
“How O
many O

As O
shown O
in O
Table O
1 O
(Question B-DAT
+ O
Image), O
there O
is O
significant O

Fig. O
2). O
In O
Table O
1 O
(Question), B-DAT
we O
show O
the O
percentage O
of O

Question B-DAT
40.81 O
67.60 O
25.77 O
21.22 O
Real O

Question B-DAT
+ O
Caption* O
57.47 O
78.97 O
39.68 O

Question B-DAT
+ O
Image O
83.30 O
95.77 O
83.39 O

Question B-DAT
43.27 O
66.65 O
28.52 O
23.66 O
Abstract O

Question B-DAT
+ O
Caption* O
54.34 O
74.70 O
41.19 O

Question B-DAT
+ O
Image O
87.49 O
95.96 O
95.04 O

question O
without O
seeing O
the O
image O
(Question), B-DAT
seeing O
just O
a O
caption O
of O

and O
not O
the O
image O
itself O
(Question B-DAT
+ O
Caption), O
and O
seeing O
the O

image O
(Question B-DAT
+ O
Image). O
Results O
are O
shown O

answer O
the O
questions? O
Table O
1 O
(Question B-DAT
+ O
Caption) O
shows O
the O
percentage O

Question B-DAT
Channel: O
This O
channel O
provides O
an O

1) O
Bag-of-Words O
Question B-DAT
(BoW O
Q): O
The O
top O
1,000 O

caption O
embedding O
(Caption). O
For O
BoW O
Question B-DAT
+ O
Caption O
(BoW O
Q O

for O
real O
images. O
Q O
= O
Question, B-DAT
I O
= O
Image, O
C O

Question B-DAT
K O
= O
1000 O
Human O
To O

for O
real O
images. O
Q O
= O
Question, B-DAT
I O
= O
Image. O
See O
text O

introduce O
the O
task O
of O
Visual O
Question B-DAT
Answering O
(VQA). O
Given O
an O
image O

Etzioni. O
Paraphrase-Driven O
Learning O
for O
Open O
Question B-DAT
Answering. O
In O
ACL, O
2013. O
2 O

Zettlemoyer, O
and O
O. O
Etzioni. O
Open O
Question B-DAT
Answering O
over O
Curated O
and O
Extracted O

Fritz. O
A O
Multi-World O
Approach O
to O
Question B-DAT
Answering O
about O
Real-World O
Scenes O
based O

T. O
Mikolov. O
Towards O
AI- O
Complete O
Question B-DAT
Answering: O
A O
Set O
of O
Prerequisite O

VQA: O
Visual B-DAT
Question O
Answering O
www.visualqa.org O

task O
of O
free-form O
and O
open-ended O
Visual B-DAT
Question O
Answering O
(VQA). O
Given O
an O

questions O
and O
answers O
are O
open-ended. O
Visual B-DAT
questions O
selectively O
target O
different O
areas O

of O
free-form O
and O
open- O
ended O
Visual B-DAT
Question O
Answering O
(VQA). O
A O
VQA O

complex O
reasoning O
more O
essential. O
Describing O
Visual B-DAT
Content. O
Related O
to O
VQA O
are O

We O
now O
describe O
the O
Visual B-DAT
Question O
Answering O
(VQA) O
dataset. O
We O

we O
introduce O
the O
task O
of O
Visual B-DAT
Question O
Answering O
(VQA). O
Given O
an O

cloud O
service. O
In O
Mobile O
Cloud O
Visual B-DAT
Media O
Computing, O
pages O
265–290. O
Springer O

D. O
Parikh. O
Zero-Shot O
Learning O
via O
Visual B-DAT
Abstraction. O
In O
ECCV, O
2014. O
2 O

Nearly O
Real- O
time O
Answers O
to O
Visual B-DAT
Questions. O
In O
User O
Interface O
Software O

and O
A. O
Gupta. O
NEIL: O
Extracting O
Visual B-DAT
Knowledge O
from O
Web O
Data. O
In O

Zitnick. O
Mind’s O
Eye: O
A O
Recurrent O
Visual B-DAT
Represen- O
tation O
for O
Image O
Caption O

Long-term O
Recurrent O
Convolutional O
Networks O
for O
Visual B-DAT
Recognition O
and O
Description. O
In O
CVPR O

G. O
Zweig. O
From O
Captions O
to O
Visual B-DAT
Concepts O
and O
Back. O
In O
CVPR O

Hallonquist, O
and O
L. O
Younes. O
A O
Visual B-DAT
Turing O
Test O
for O
Computer O
Vision O

Karpathy O
and O
L. O
Fei-Fei. O
Deep O
Visual B-DAT

and O
R. O
S. O
Zemel. O
Unifying O
Visual B-DAT

Listen, O
Use O
Your O
Imagination: O
Leveraging O
Visual B-DAT
Common O
Sense O
for O
Non-Visual O
Tasks O

Divvala, O
and O
A. O
Farhadi. O
Viske: O
Visual B-DAT
knowledge O
extraction O
and O
question O
answering O

Berg, O
and O
T. O
L. O
Berg. O
Visual B-DAT
madlibs: O
Fill-in-the- O
blank O
description O
generation O

Bringing O
Semantics O
Into O
Focus O
Using O
Visual B-DAT
Abstraction. O
In O
CVPR, O
2013. O
2 O

and O
L. O
Vanderwende. O
Learning O
the O
Visual B-DAT
Interpretation O
of O
Sentences. O
In O
ICCV O

204,721 O
images O
from O
the O
MS O
COCO B-DAT
dataset O
[32] O
and O
a O
newly O

contains O
50,000 O
scenes. O
The O
MS O
COCO B-DAT
dataset O
has O
images O
depicting O
diverse O

to O
English O
by O
humans) O
for O
COCO B-DAT
images. O
[44] O
automatically O
generated O
four O

object, O
count, O
color, O
location) O
using O
COCO B-DAT
captions. O
Text-based O
Q&A O
is O
a O

Common O
Objects O
in O
Context O
(MS O
COCO) B-DAT
[32] O
dataset. O
The O
MS O
COCO O

split O
strategy O
as O
the O
MC O
COCO B-DAT
dataset O
[32] O
(including O
test- O
dev O

abstract O
scenes. O
Captions. O
The O
MS O
COCO B-DAT
dataset O
[32], O
[7] O
already O
contains O

204,721 O
images O
from O
the O
MS O
COCO B-DAT
dataset O
[32] O
and O
150,000 O
questions O

VQA O
dataset O
for O
the O
MS O
COCO B-DAT
images O
using O
several O
baselines O
and O

from O
the O
caption O
data O
(MS O
COCO B-DAT
captions O
for O
real O
images O
and O

VQA O
dataset O
for O
the O
MS O
COCO B-DAT
[32] O
images, O
abstract O
scenes, O
and O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT
captions: O
Data O
collection O
and O
evaluation O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT
Captions: O
Data O
Collection O
and O
Evaluation O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT

VQA: O
Visual O
Question O
Answering B-DAT
www.visualqa.org O

free-form O
and O
open-ended O
Visual O
Question O
Answering B-DAT
(VQA). O
Given O
an O
image O
and O

and O
open- O
ended O
Visual O
Question O
Answering B-DAT
(VQA). O
A O
VQA O
system O
takes O

now O
describe O
the O
Visual O
Question O
Answering B-DAT
(VQA) O
dataset. O
We O
begin O
by O

the O
task O
of O
Visual O
Question O
Answering B-DAT
(VQA). O
Given O
an O
image O
and O

Paraphrase-Driven O
Learning O
for O
Open O
Question O
Answering B-DAT

and O
O. O
Etzioni. O
Open O
Question O
Answering B-DAT
over O
Curated O
and O
Extracted O
Knowledge O

A O
Multi-World O
Approach O
to O
Question O
Answering B-DAT
about O
Real-World O
Scenes O
based O
on O

Parsing O
for O
Understanding O
Events O
and O
Answering B-DAT
Queries. O
IEEE O
MultiMedia, O
2014. O
1 O

Mikolov. O
Towards O
AI- O
Complete O
Question O
Answering B-DAT

VQA B-DAT

and O
open-ended O
Visual O
Question O
Answering O
(VQA B-DAT

a O
system O
that O
succeeds O
at O
VQA B-DAT
typically O
needs O
a O
more O
detailed O

producing O
generic O
image O
captions. O
Moreover, O
VQA B-DAT
is O
amenable O
to O
automatic O
evaluation O

Numerous O
baselines O
and O
methods O
for O
VQA B-DAT
are O
provided O
and O
compared O
with O

human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

open- O
ended O
Visual O
Question O
Answering O
(VQA B-DAT

). O
A O
VQA B-DAT
system O
takes O
as O
input O
an O

Is O
this O
person O
expecting O
company?”). O
VQA B-DAT
[19], O
[36], O
[50], O
[3] O
is O

the O
high-level O
reasoning O
required O
for O
VQA B-DAT
by O
removing O
the O
need O
to O

29]. O
As O
part O
of O
the O
VQA B-DAT
initiative, O
we O
will O
organize O
an O

state-of-the-art O
methods O
and O
best O
practices. O
VQA B-DAT
poses O
a O
rich O
set O
of O

during O
the O
past O
few O
decades. O
VQA B-DAT
provides O
an O
attractive O
balance O
between O

VQA B-DAT
Efforts. O
Several O
recent O
papers O
have O

difficult O
and O
unconstrained O
task, O
our O
VQA B-DAT
dataset O
is O
two O
orders O
of O

1,449 O
images O
respectively). O
The O
proposed O
VQA B-DAT
task O
has O
connections O
to O
other O

These O
approaches O
provide O
inspiration O
for O
VQA B-DAT
techniques. O
One O
key O
concern O
in O

fixed O
set O
of O
loca- O
tions. O
VQA B-DAT
is O
naturally O
grounded O
in O
images O

Describing O
Visual O
Content. O
Related O
to O
VQA B-DAT
are O
the O
tasks O
of O
image O

by O
[53]). O
The O
questions O
in O
VQA B-DAT
require O
detailed O
specific O
information O
about O

3 O
VQA B-DAT
DATASET O
COLLECTION O

describe O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset. O
We O
begin O
by O
describing O

they O
are O
well-suited O
for O
our O
VQA B-DAT
task. O
The O
more O
diverse O
our O

their O
answers. O
Abstract O
Scenes. O
The O
VQA B-DAT
task O
with O
real O
images O
requires O

the O
high-level O
reasoning O
required O
for O
VQA, B-DAT
but O
not O
the O
low-level O
vision O

test-standard, O
test-challenge, O
test-reserve). O
For O
the O
VQA B-DAT
challenge O
(see O
section O
6), O
test-dev O

default’ O
test O
data O
for O
the O
VQA B-DAT
competition. O
When O
comparing O
to O
the O

sentences O
containing O
multiple O
words. O
In O
VQA, B-DAT
most O
answers O
(89.32%) O
are O
single O

4 O
VQA B-DAT
DATASET O
ANALYSIS O
In O
this O
section O

questions O
and O
answers O
in O
the O
VQA B-DAT
train O
dataset. O
To O
gain O
an O

visual O
information O
is O
critical O
to O
VQA B-DAT
and O
that O
commonsense O
information O
alone O

from O
the O
real O
images O
of O
VQA B-DAT
trainval) O
asking O
subjects O

5 O
VQA B-DAT
BASELINES O
AND O
METHODS O
In O
this O

explore O
the O
difficulty O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

novel O
methods. O
We O
train O
on O
VQA B-DAT
train+val. O
Unless O
stated O
otherwise, O
all O

top O
1K O
answers O
of O
the O
VQA B-DAT
train/val O
dataset O

multiple- O
choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
As O

I O
(Fig. O
8), O
selected O
using O
VQA B-DAT
test-dev O
accuracies) O
on O
VQA O
test O

worse O
than O
human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

system O
is O
correct O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

correct” O
implies O
that O
it O
has O
VQA B-DAT
accuracy O
1.0 O
(see O
section O
3 O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
The O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

frequently O
predicted O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

age O
of O
question) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

system O
is O
correct) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

a O
filtered O
version O
of O
the O
VQA B-DAT
train O
+ O
val O
dataset O
in O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

6 O
VQA B-DAT
CHALLENGE O
AND O
WORKSHOP O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O

task O
of O
Visual O
Question O
Answering O
(VQA B-DAT

multiple-choice O
tasks O
in O
the O
respective O
VQA B-DAT
Real O
Image O
Challenge O
leaderboards O
(as O

datasets O
may O
help O
enable O
practical O
VQA B-DAT
applications. O
We O
believe O
VQA O
has O

questions O
IV O
- O
Details O
on O
VQA B-DAT
baselines O
V O
- O
“Age” O
and O

Leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Open-Ended) O
on O

leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Multiple-Choice) O
on O

Additional O
examples O
from O
the O
VQA B-DAT
dataset O

scenes. O
This O
helps O
motivate O
the O
VQA B-DAT
task O
as O
a O
way O
to O

APPENDIX O
IV: O
DETAILS O
ON O
VQA B-DAT
BASELINES O
“per O
Q-type O
prior” O
baseline O

For O
every O
question O
in O
the O
VQA B-DAT
test-standard O
set, O
we O
find O
its O

norm O
I), O
selected O
using O
VQA B-DAT
test- O
dev O
accuracies). O
To O
estimate O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

a O
random O
selection O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

to O
a O
challenging O
visual-reasoning O
task, O
NLVR2, B-DAT
and O
improve O
the O
previous O
best O

Visual O
Reason- O
ing O
for O
Real O
(NLVR2 B-DAT

Method O
VQA O
GQA O
NLVR2 B-DAT

on O
the O
‘test-standard’ O
splits O
and O
NLVR2 B-DAT
results O
are O
reported O
on O
the O

Hudson O
and O
Manning, O
2019), O
and O
NLVR2 B-DAT

Since O
each O
da- O
tum O
in O
NLVR2 B-DAT
has O
two O
natural O
images O
img0 O

VQA/GQA O
test- O
standard O
sets O
and O
NLVR2 B-DAT
public O
test O
set. O
Be- O
sides O

NLVR2 B-DAT
NLVR2 O
(Suhr O
et O
al., O
2019) O
is O

is O
60.0%. O
8Each O
statement O
in O
NLVR2 B-DAT
is O
related O
to O
multiple O
image O

Method O
VQA O
GQA O
NLVR2 B-DAT

achieves O
accuracy O
of O
74.9% O
on O
NLVR2, B-DAT
all O
results O
without O
LXMERT O
pre O

Method O
VQA O
GQA O
NLVR2 B-DAT

Method O
VQA O
GQA O
NLVR2 B-DAT

datasets. O
The O
2.1% O
improvement O
on O
NLVR2 B-DAT
shows O
the O
stronger O
rep- O
resentations O

data O
(images O
and O
statements) O
in O
NLVR2 B-DAT

margin O
of O
9% O
accuracy O
on O
NLVR2 B-DAT
(and O
15% O
in O
consistency). O
LXMERT O

challenging O
visual O
reasoning O
dataset O
of O
NLVR2 B-DAT

Suhr O
for O
evalua- O
tion O
on O
NLVR2 B-DAT

NLVR2 B-DAT
Since O
the O
previous O
two O
datasets O

challenging O
vi- O
sual O
reasoning O
dataset O
NLVR2 B-DAT
where O
all O
the O
sen- O
tences O

in O
pre-training. O
Each O
datum O
in O
NLVR2 B-DAT
contains O
two O
related O
nat- O
ural O

these O
two O
images O
or O
not. O
NLVR2 B-DAT

B O
Details O
of O
NLVR2 B-DAT
Fine-tuning O

Each O
datum O
in O
NLVR2 B-DAT
consists O
of O
a O
two-image O
pair O

use O
our O
LXMERT O
model O
on O
NLVR2, B-DAT
we O
concatenate O
the O
cross-modality O
representations O

test-standard’ O
split. O
The O
images O
in O
NLVR2 B-DAT
are O
not O
from O
either O
MS O

vision-and-language O
tasks O
including O
VQA, O
VCR, O
NLVR2, B-DAT
and O
Flickr30K O
show O
that O
VisualBERT O

natural O
language O
for O
visual O
reasoning O
(NLVR2, B-DAT
Suhr O
et O
al. O
(2019)), O
and O

Natural O
Language O
for O
Visual O
Reasoning O
(NLVR2 B-DAT

4.3 O
NLVR2 B-DAT

NLVR2 B-DAT
is O
a O
dataset O
for O
joint O

with O
the O
state-of-the-art O
model O
on O
NLVR2 B-DAT

of O
the O
ablation O
models O
on O
NLVR2 B-DAT

conduct O
our O
ablation O
study O
on O
NLVR2 B-DAT
and O
include O
two O
ablation O
models O

Alane O
Suhr O
for O
evaluation O
on O
NLVR2 B-DAT

C O
NLVR2 B-DAT

For O
each O
training O
example O
in O
NLVR2, B-DAT
we O
construct O
a O
sequence O
consisting O

4.3 O
NLVR2 B-DAT

C O
NLVR2 B-DAT

question O
answering O
(image O
QA) O
datasets: O
VQA B-DAT
v2 I-DAT

for O
evaluating O
our O
LXMERT O
framework: O
VQA B-DAT
v2 I-DAT

6Our O
result O
on O
VQA B-DAT
v2 I-DAT

to O
an O
image. O
We O
take O
VQA B-DAT
v2 I-DAT

the O
answer O
bias O
compared O
to O
VQA B-DAT
v1 I-DAT

Fine-tuning O
For O
training O
and O
validating O
VQA B-DAT
v2 I-DAT

test O
our O
model O
on O
the O
VQA B-DAT
v2 I-DAT

answering O
(image O
QA) O
datasets: O
VQA O
v2 B-DAT

evaluating O
our O
LXMERT O
framework: O
VQA O
v2 B-DAT

6Our O
result O
on O
VQA O
v2 B-DAT

an O
image. O
We O
take O
VQA O
v2 B-DAT

For O
training O
and O
validating O
VQA O
v2 B-DAT

our O
model O
on O
the O
VQA O
v2 B-DAT

question O
answering O
(image O
QA) O
datasets: O
VQA B-DAT
v2 I-DAT

for O
evaluating O
our O
LXMERT O
framework: O
VQA B-DAT
v2 I-DAT

6Our O
result O
on O
VQA B-DAT
v2 I-DAT

to O
an O
image. O
We O
take O
VQA B-DAT
v2 I-DAT

Fine-tuning O
For O
training O
and O
validating O
VQA B-DAT
v2 I-DAT

test O
our O
model O
on O
the O
VQA B-DAT
v2 I-DAT

ques- O
tion O
answering O
datasets O
(i.e., O
VQA B-DAT
and O
GQA). O
We O
also O
show O

two O
popular O
visual O
question-answering O
datasets, O
VQA B-DAT
(Antol O
et O
al., O
2015) O
and O

COCO-Cap O
VG-Cap O
VQA B-DAT
GQA O
VG-QA O
All O

question O
answering O
(image O
QA) O
datasets: O
VQA B-DAT
v2.0 O
(Antol O
et O
al., O
2015 O

Method O
VQA B-DAT
GQA O
NLVR2 O

Table O
2: O
Test-set O
results. O
VQA B-DAT

for O
evaluating O
our O
LXMERT O
framework: O
VQA B-DAT
v2.0 O
dataset O
(Goyal O
et O
al O

4.2 O
Implementation O
Details O
On O
VQA B-DAT
and O
GQA, O
we O
fine-tune O
our O

vious O
best O
published O
results O
on O
VQA B-DAT

VQA B-DAT
The O
SotA O
result O
is O
BAN+Counter O

GQA O
method O
is O
higher O
than O
VQA, B-DAT
possibly O
because O
GQA O
requires O
more O

Gao O
et O
al., O
2019b). O
MCAN O
(VQA B-DAT
challenge O
ver- O
sion) O
uses O
stronger O

features O
and O
achieves O
72.8% O
on O
VQA B-DAT
2.0 O
test-standard. O
MUAN O
achieves O
71.1 O

6Our O
result O
on O
VQA B-DAT
v2.0 O
‘test-dev’ O
is O
72.4%. O
7Our O

Method O
VQA B-DAT
GQA O
NLVR2 O

Method O
VQA B-DAT
GQA O
NLVR2 O

Method O
VQA B-DAT
GQA O
NLVR2 O

which O
is O
used O
in O
several O
VQA B-DAT
implementations O
(Anderson O
et O
al., O
2018 O

to O
par- O
ticipate O
in O
the O
VQA B-DAT
and O
GQA O
challenges O
in O
May O

margin O
of O
1.5% O
accuracy O
on O
VQA B-DAT
2.0 O
and O
a O
margin O
of O

the O
top-3 O
on O
both O
the O
VQA B-DAT
and O
GQA O
challenges O

two O
image O
QA O
datasets O
(i.e., O
VQA B-DAT
and O
GQA) O
and O
show O
the O

VQA B-DAT
The O
goal O
of O
visual O
question O

answering O
(VQA) B-DAT
(Antol O
et O
al., O
2015) O
is O

to O
an O
image. O
We O
take O
VQA B-DAT
v2.0 O
dataset O
(Goyal O
et O
al O

the O
answer O
bias O
compared O
to O
VQA B-DAT
v1.0. O
The O
dataset O
contains O
an O

Manning, O
2019) O
is O
same O
as O
VQA B-DAT
(i.e., O
answer O
single-image O
related O
questions O

Fine-tuning O
For O
training O
and O
validating O
VQA B-DAT
v2.0, O
we O
take O
the O
same O

test O
our O
model O
on O
the O
VQA B-DAT
v2.0 O
‘test-dev’ O
and O
‘test- O
standard O

their O
correct O
answer O
from O
the O
VQA B-DAT
v2 I-DAT
dataset O
[14 O

on O
the O
VQA B-DAT
v2 I-DAT
benchmark O
[14]. O
Admittedly, O
a O
large O

the O
dataset, O
referred O
to O
as O
VQA B-DAT
v2 I-DAT
[14]. O
It O
associates O
two O
images O

to O
the O
ques- O
tions O
of O
VQA B-DAT
v2, I-DAT
these O
have O
more O
diverse O
formulations O

phrases, O
whereas O
most O
answers O
in O
VQA B-DAT
v2 I-DAT
are O
usually O
1 O
to O
3-words O

whose O
answers O
overlap O
those O
in O
VQA B-DAT
v2 I-DAT

collecting O
large-scale O
datasets O
such O
as O
VQA B-DAT
v2 I-DAT

The O
images O
used O
by O
the O
VQA B-DAT
v2 I-DAT
dataset O
yield O
in O
that O
case O

training O
ques- O
tion O
in O
the O
VQA B-DAT
v2 I-DAT
dataset O
is O
associated O
with O
one O

occasionally O
the O
case O
in O
the O
VQA B-DAT
v2 I-DAT
dataset. O
Second, O
the O
use O
of O

official O
validation O
set O
of O
the O
VQA B-DAT
v2 I-DAT
dataset O
for O
monitoring, O
and O
identify O

output O
vocabulary O
determined O
on O
the O
VQA B-DAT
v2 I-DAT
dataset. O
This O
amounts O
to O
only O

to O
keep O
balanced O
pairs O
of O
VQA B-DAT
v2 I-DAT
in O
the O
same O
mini-batches. O
Those O

the O
official O
training O
set O
of O
VQA B-DAT
v2 I-DAT
and O
on O
the O
ad- O
ditional O

reported O
on O
the O
validation O
test O
VQA B-DAT
v2 I-DAT
at O
the O
best O
epoch O
(highest O

24] O
increase O
the O
performance O
on O
VQA B-DAT
v2 I-DAT
[14] O
in O
all O
question O
types O

the O
output O
vocabulary O
determined O
on O
VQA B-DAT
v2, I-DAT
and O
which O
use O
an O
image O

also O
used O
in O
VQA B-DAT
v2 I-DAT

485,000 O
over O
the O
650,000 O
of O
VQA B-DAT
v2 I-DAT

COCO O
images O
not O
used O
in O
VQA B-DAT
v2 I-DAT
resulted O

VQA B-DAT
v2 I-DAT
validation O
VQA O
Score O
Accuracy O
over O

single O
network, O
evaluated O
on O
the O
VQA B-DAT
v2 I-DAT
validation O
set. O
We O
evaluate O
variations O

VQA B-DAT
v2 I-DAT
validation O

Visual O
Genome, O
only O
uses O
the O
VQA B-DAT
v2 I-DAT
training O
set O
57.84±0.05 O
77.46 O
36.73 O

single O
network, O
evaluated O
on O
the O
VQA B-DAT
v2 I-DAT
validation O
set. O
The O
ablations O
of O

include O
the O
validation O
split O
of O
VQA B-DAT
v2 I-DAT
for O
training O
and O
use O
its O

the O
valida- O
tion O
split O
of O
VQA B-DAT
v2 I-DAT
for O
training. O
Our O
model O
obtained O

VQA B-DAT
v2 I-DAT
test-dev O
VQA O
v2 O
test-std O

methods. O
Excerpt O
from O
the O
official O
VQA B-DAT
v2 I-DAT
Leaderboard O
[1 O

correct O
answer O
from O
the O
VQA O
v2 B-DAT
dataset O
[14 O

on O
the O
VQA O
v2 B-DAT
benchmark O
[14]. O
Admittedly, O
a O
large O

dataset, O
referred O
to O
as O
VQA O
v2 B-DAT
[14]. O
It O
associates O
two O
images O

the O
ques- O
tions O
of O
VQA O
v2, B-DAT
these O
have O
more O
diverse O
formulations O

whereas O
most O
answers O
in O
VQA O
v2 B-DAT
are O
usually O
1 O
to O
3-words O

answers O
overlap O
those O
in O
VQA O
v2 B-DAT

large-scale O
datasets O
such O
as O
VQA O
v2 B-DAT

images O
used O
by O
the O
VQA O
v2 B-DAT
dataset O
yield O
in O
that O
case O

ques- O
tion O
in O
the O
VQA O
v2 B-DAT
dataset O
is O
associated O
with O
one O

the O
case O
in O
the O
VQA O
v2 B-DAT
dataset. O
Second, O
the O
use O
of O

validation O
set O
of O
the O
VQA O
v2 B-DAT
dataset O
for O
monitoring, O
and O
identify O

vocabulary O
determined O
on O
the O
VQA O
v2 B-DAT
dataset. O
This O
amounts O
to O
only O

keep O
balanced O
pairs O
of O
VQA O
v2 B-DAT
in O
the O
same O
mini-batches. O
Those O

official O
training O
set O
of O
VQA O
v2 B-DAT
and O
on O
the O
ad- O
ditional O

on O
the O
validation O
test O
VQA O
v2 B-DAT
at O
the O
best O
epoch O
(highest O

increase O
the O
performance O
on O
VQA O
v2 B-DAT
[14] O
in O
all O
question O
types O

output O
vocabulary O
determined O
on O
VQA O
v2, B-DAT
and O
which O
use O
an O
image O

also O
used O
in O
VQA O
v2 B-DAT

over O
the O
650,000 O
of O
VQA O
v2 B-DAT

images O
not O
used O
in O
VQA O
v2 B-DAT
resulted O

VQA O
v2 B-DAT
validation O
VQA O
Score O
Accuracy O
over O

network, O
evaluated O
on O
the O
VQA O
v2 B-DAT
validation O
set. O
We O
evaluate O
variations O

VQA O
v2 B-DAT
validation O

Genome, O
only O
uses O
the O
VQA O
v2 B-DAT
training O
set O
57.84±0.05 O
77.46 O
36.73 O

network, O
evaluated O
on O
the O
VQA O
v2 B-DAT
validation O
set. O
The O
ablations O
of O

the O
validation O
split O
of O
VQA O
v2 B-DAT
for O
training O
and O
use O
its O

valida- O
tion O
split O
of O
VQA O
v2 B-DAT
for O
training. O
Our O
model O
obtained O

VQA O
v2 B-DAT
test-dev O
VQA O
v2 O
test-std O

Excerpt O
from O
the O
official O
VQA O
v2 B-DAT
Leaderboard O
[1 O

their O
correct O
answer O
from O
the O
VQA B-DAT
v2 I-DAT
dataset O
[14 O

on O
the O
VQA B-DAT
v2 I-DAT
benchmark O
[14]. O
Admittedly, O
a O
large O

the O
dataset, O
referred O
to O
as O
VQA B-DAT
v2 I-DAT
[14]. O
It O
associates O
two O
images O

to O
the O
ques- O
tions O
of O
VQA B-DAT
v2, I-DAT
these O
have O
more O
diverse O
formulations O

phrases, O
whereas O
most O
answers O
in O
VQA B-DAT
v2 I-DAT
are O
usually O
1 O
to O
3-words O

whose O
answers O
overlap O
those O
in O
VQA B-DAT
v2 I-DAT

collecting O
large-scale O
datasets O
such O
as O
VQA B-DAT
v2 I-DAT

The O
images O
used O
by O
the O
VQA B-DAT
v2 I-DAT
dataset O
yield O
in O
that O
case O

training O
ques- O
tion O
in O
the O
VQA B-DAT
v2 I-DAT
dataset O
is O
associated O
with O
one O

occasionally O
the O
case O
in O
the O
VQA B-DAT
v2 I-DAT
dataset. O
Second, O
the O
use O
of O

official O
validation O
set O
of O
the O
VQA B-DAT
v2 I-DAT
dataset O
for O
monitoring, O
and O
identify O

output O
vocabulary O
determined O
on O
the O
VQA B-DAT
v2 I-DAT
dataset. O
This O
amounts O
to O
only O

to O
keep O
balanced O
pairs O
of O
VQA B-DAT
v2 I-DAT
in O
the O
same O
mini-batches. O
Those O

the O
official O
training O
set O
of O
VQA B-DAT
v2 I-DAT
and O
on O
the O
ad- O
ditional O

reported O
on O
the O
validation O
test O
VQA B-DAT
v2 I-DAT
at O
the O
best O
epoch O
(highest O

24] O
increase O
the O
performance O
on O
VQA B-DAT
v2 I-DAT
[14] O
in O
all O
question O
types O

the O
output O
vocabulary O
determined O
on O
VQA B-DAT
v2, I-DAT
and O
which O
use O
an O
image O

also O
used O
in O
VQA B-DAT
v2 I-DAT

485,000 O
over O
the O
650,000 O
of O
VQA B-DAT
v2 I-DAT

COCO O
images O
not O
used O
in O
VQA B-DAT
v2 I-DAT
resulted O

VQA B-DAT
v2 I-DAT
validation O
VQA O
Score O
Accuracy O
over O

single O
network, O
evaluated O
on O
the O
VQA B-DAT
v2 I-DAT
validation O
set. O
We O
evaluate O
variations O

VQA B-DAT
v2 I-DAT
validation O

Visual O
Genome, O
only O
uses O
the O
VQA B-DAT
v2 I-DAT
training O
set O
57.84±0.05 O
77.46 O
36.73 O

single O
network, O
evaluated O
on O
the O
VQA B-DAT
v2 I-DAT
validation O
set. O
The O
ablations O
of O

include O
the O
validation O
split O
of O
VQA B-DAT
v2 I-DAT
for O
training O
and O
use O
its O

the O
valida- O
tion O
split O
of O
VQA B-DAT
v2 I-DAT
for O
training. O
Our O
model O
obtained O

VQA B-DAT
v2 I-DAT
test-dev O
VQA O
v2 O
test-std O

methods. O
Excerpt O
from O
the O
official O
VQA B-DAT
v2 I-DAT
Leaderboard O
[1 O

model O
for O
visual O
question O
answering O
(VQA), B-DAT
which O
won O
the O
first O
place O

in O
the O
2017 O
VQA B-DAT
Challenge. O
VQA O
is O
a O
task O
of O
significant O

of O
deep O
neural O
networks O
for O
VQA B-DAT
is O
very O
dependent O
on O
choices O

task O
of O
Visual O
Question O
Answering O
(VQA) B-DAT
involves O

a O
relatively O
simple O
model O
for O
VQA B-DAT
that O
achieves O
state-of-the-art O
results. O
It O

task O
of O
visual O
question O
answering O
(VQA) B-DAT
relates O
visual O
concepts O
with O
elements O

their O
correct O
answer O
from O
the O
VQA B-DAT
v2 O
dataset O
[14 O

on O
the O
VQA B-DAT
v2 O
benchmark O
[14]. O
Admittedly, O
a O

a O
task O
as O
complex O
as O
VQA, B-DAT
small O
variations O
of O
hyperparameters O
and O

of O
a O
successful O
model O
for O
VQA B-DAT

basis O
for O
future O
development O
of O
VQA B-DAT
systems O
and O
multimodal O
reasoning O
algorithms O

the O
basis O
of O
many O
modern O
VQA B-DAT
methods O
[33], O
the O
details O
of O

2. O
Background O
The O
task O
of O
VQA B-DAT
has O
gathered O
increasing O
interest O
in O

former. O
This O
is O
partly O
because O
VQA B-DAT
constitutes O
a O
practical O
setting O
to O

computer O
vision. O
The O
task O
of O
VQA B-DAT
is O
extremely O
challenging, O
since O
it O

1). O
The O
increasing O
interest O
in O
VQA B-DAT
parallels O
a O
similar O
trend O
for O

number O
of O
large-scale O
datasets O
for O
VQA B-DAT
have O
been O
created O
(e.g. O
[6 O

and O
ground O
truth O
answers. O
The O
VQA B-DAT

the O
dataset, O
referred O
to O
as O
VQA B-DAT
v2 O
[14]. O
It O
associates O
two O

ba- O
sis O
of O
the O
2017 O
VQA B-DAT
challenge O
[1] O
and O
of O
the O

to O
the O
ques- O
tions O
of O
VQA B-DAT
v2, O
these O
have O
more O
diverse O

phrases, O
whereas O
most O
answers O
in O
VQA B-DAT
v2 O
are O
usually O
1 O
to O

whose O
answers O
overlap O
those O
in O
VQA B-DAT
v2 O

Methods O
The O
prevailing O
approach O
to O
VQA B-DAT
is O
based O
on O
three O
components O

answers. O
Questions O
in O
the O
current O
VQA B-DAT
datasets O
are O
mostly O
visual O
in O

few O
thou- O
sands). O
Second, O
most O
VQA B-DAT
models O
are O
based O
on O
a O

collecting O
large-scale O
datasets O
such O
as O
VQA B-DAT
v2. O
It O
contains O
in O
the O

majority O
of O
methods O
proposed O
for O
VQA B-DAT
in O
the O
past O
few O
years O

The O
images O
used O
by O
the O
VQA B-DAT
v2 O
dataset O
yield O
in O
that O

during O
the O
training O
of O
the O
VQA B-DAT
model. O
The O
features O
can O
therefore O

mechanism O
common O
to O
most O
modern O
VQA B-DAT
models O
(see O
e.g. O
[38, O
34 O

N=3129 O
candidate O
answers. O
We O
treat O
VQA B-DAT
as O
a O
multi-label O
classification O
task O

training O
ques- O
tion O
in O
the O
VQA B-DAT
v2 O
dataset O
is O
associated O
with O

as O
commonly O
used O
in O
other O
VQA B-DAT
models. O
The O
advantage O
of O
the O

occasionally O
the O
case O
in O
the O
VQA B-DAT
v2 O
dataset. O
Second, O
the O
use O

official O
validation O
set O
of O
the O
VQA B-DAT
v2 O
dataset O
for O
monitoring, O
and O

the O
best O
performance O
(highest O
overall O
VQA B-DAT
score). O
The O
train- O
ing O
is O

output O
vocabulary O
determined O
on O
the O
VQA B-DAT
v2 O
dataset. O
This O
amounts O
to O

to O
keep O
balanced O
pairs O
of O
VQA B-DAT
v2 O
in O
the O
same O
mini-batches O

the O
official O
training O
set O
of O
VQA B-DAT
v2 O
and O
on O
the O
ad O

reported O
on O
the O
validation O
test O
VQA B-DAT
v2 O
at O
the O
best O
epoch O

highest O
overall O
VQA B-DAT
score). O
Each O
experiment O
(i.e. O
each O

performance O
metric O
is O
the O
standard O
VQA B-DAT
accuracy O
[6], O
i.e. O
the O
average O

24] O
increase O
the O
performance O
on O
VQA B-DAT
v2 O
[14] O
in O
all O
question O

the O
output O
vocabulary O
determined O
on O
VQA B-DAT
v2, O
and O
which O
use O
an O

image O
also O
used O
in O
VQA B-DAT
v2. O
Note O
that O
the O
+0.67 O

485,000 O
over O
the O
650,000 O
of O
VQA B-DAT
v2). O
Note O
that O
including O
VG O

COCO O
images O
not O
used O
in O
VQA B-DAT
v2 O
resulted O

1 O
are O
inconclusive: O
the O
overall O
VQA B-DAT
score O
is O
virtually O
identi- O
cal O

zero. O
In O
Table O
1, O
the O
VQA B-DAT
score O
remains O
virtually O
identical, O
but O

VQA B-DAT
training O
data. O
On O
the O
other O

suggests O
that O
a O
sufficiently O
large O
VQA B-DAT
training O
set O
may O
remove O
this O

VQA B-DAT
v2 O
validation O
VQA O
Score O
Accuracy O
over O

single O
network, O
evaluated O
on O
the O
VQA B-DAT
v2 O
validation O
set. O
We O
evaluate O

VQA B-DAT
v2 O
validation O

VQA B-DAT
Score O
Accuracy O
over O

Visual O
Genome, O
only O
uses O
the O
VQA B-DAT
v2 O
training O
set O
57.84±0.05 O
77.46 O

single O
network, O
evaluated O
on O
the O
VQA B-DAT
v2 O
validation O
set. O
The O
ablations O

reference O
model O
on O
the O
overall O
VQA B-DAT
score. O
It O
does O
not, O
however O

VQA B-DAT
data O
for O
pretraining O
the O
word O

from O
scratch O
shrinks O
as O
more O
VQA B-DAT

that O
a O
suf- O
ficiently O
large O
VQA B-DAT
training O
set O
would O
remove O
the O

dataset O
to O
be O
learned O
from O
VQA B-DAT

include O
the O
validation O
split O
of O
VQA B-DAT
v2 O
for O
training O
and O
use O

could O
be O
incorporated O
into O
other O
VQA B-DAT
models. O
The O
results O
fol- O
low O

the O
valida- O
tion O
split O
of O
VQA B-DAT
v2 O
for O
training. O
Our O
model O

first O
place O
at O
the O
2017 O
VQA B-DAT
Challenge O
[1]. O
It O
still O
surpasses O

paper O
presented O
a O
model O
for O
VQA B-DAT
based O
on O
a O
deep O

vances O
in O
the O
field O
of O
VQA, B-DAT
which O
remains O
a O
largely O
un O

unprece- O
dented O
in O
scale O
for O
VQA, B-DAT
and O
is O
intended O
as O
a O

of O
various O
components O
of O
a O
VQA B-DAT
model. O
It O
also O
allows O
us O

measured O
that O
gains O
from O
additional O
VQA B-DAT
training O
data O
had O
not O
reached O

VQA B-DAT
datasets O
is O
a O
promising O
direction O

VQA B-DAT
v2 O
test-dev O
VQA O
v2 O
test-std O

methods. O
Excerpt O
from O
the O
official O
VQA B-DAT
v2 O
Leaderboard O
[1 O

works O
on O
compositional O
models O
for O
VQA B-DAT
are O
a O
promising O
direction O
to O

current O
state O
of O
progress O
on O
VQA B-DAT

References O
[1] O
VQA B-DAT
Challenge O
leaderboard. O
http://visualqa.org O

L. O
Zitnick, O
and O
D. O
Parikh. O
VQA B-DAT

Parikh. O
Making O
the O
V O
in O
VQA B-DAT
matter: O
Elevating O
the O
role O
of O

of O
the O
up-down O
model O
on O
VQA B-DAT
v2 I-DAT

test- O
std O
split O
of O
the O
VQA B-DAT
v2 I-DAT

Table O
1. O
Accuracy O
(%) O
on O
VQA B-DAT
v2 I-DAT

from O
65.32% O
to O
66.91% O
on O
VQA B-DAT
v2 I-DAT

and O
72.27% O
on O
test-std O
of O
VQA B-DAT
v2 I-DAT

the O
up-down O
model O
on O
VQA O
v2 B-DAT

std O
split O
of O
the O
VQA O
v2 B-DAT

1. O
Accuracy O
(%) O
on O
VQA O
v2 B-DAT

65.32% O
to O
66.91% O
on O
VQA O
v2 B-DAT

72.27% O
on O
test-std O
of O
VQA O
v2 B-DAT

of O
the O
up-down O
model O
on O
VQA B-DAT
v2 I-DAT

test- O
std O
split O
of O
the O
VQA B-DAT
v2 I-DAT

Table O
1. O
Accuracy O
(%) O
on O
VQA B-DAT
v2 I-DAT

from O
65.32% O
to O
66.91% O
on O
VQA B-DAT
v2 I-DAT

and O
72.27% O
on O
test-std O
of O
VQA B-DAT
v2 I-DAT

the O
Winning O
Entry O
to O
the O
VQA B-DAT
Challenge O
2018 O

FAIR)’s O
A-STAR O
team O
to O
the O
VQA B-DAT
Challenge O
20181 O

of O
the O
up-down O
model O
on O
VQA B-DAT
v2.0 O
dataset O
[6] O
– O
from O

test- O
std O
split O
of O
the O
VQA B-DAT
v2.0 O
dataset. O
Our O
code O
in O

the O
winning O
entry O
to O
the O
VQA B-DAT
Challenge O
2018 O
from O
Face- O
book O

today’s O
Visual O
Question O
Answer- O
ing O
(VQA) B-DAT
models O
fit O
a O
particular O
design O

development O
in O
VQA B-DAT
[2] O
and O
related O
directions O
like O

winning O
entry O
to O
the O
2017 O
VQA B-DAT
challenge. O
The O
key O
idea O
in O

Their O
performance O
reached O
70.34% O
on O
VQA B-DAT
2.0 O
test-std O
split O
with O
an O

3FAIR O
A-STAR’s O
entry O
in O
the O
VQA B-DAT
2018 O
Challenge O
was O
72.25%. O
This O

Table O
1. O
Accuracy O
(%) O
on O
VQA B-DAT
v2.0. O
For O
ease O
of O
presentation O

from O
65.32% O
to O
66.91% O
on O
VQA B-DAT
v2.0 O
test-dev O

a O
single O
ground-truth O
answer O
while O
VQA B-DAT
has O
10, O
we O
simply O
replicated O

data O
format O
compatible O
with O
the O
VQA B-DAT
evaluation O
protocol O

roring O
the O
images O
in O
the O
VQA B-DAT
dataset. O
We O
do O
some O
basic O

only O
on O
images O
from O
the O
VQA B-DAT
dataset O
without O
fine-tuning. O
After O
the O

and O
70.24% O
for O
test-std O
on O
VQA B-DAT
2.0 O

down O
model O
trained O
on O
the O
VQA B-DAT
dataset O
with/without O
data O
augmentation O
and O

and O
72.27% O
on O
test-std O
of O
VQA B-DAT
v2.0 O

Lawrence O
Zitnick, O
and O
D. O
Parikh. O
VQA B-DAT

Parikh. O
Making O
the O
V O
in O
VQA B-DAT
matter: O
Elevating O
the O
role O
of O

model O
on O
visual O
question O
answering O
(VQA B-DAT
2.0) O
and O
Flickr30k O
Entities O
datasets O

a O
large O
and O
highly-competitive O
dataset, O
VQA B-DAT
2.0 O
[8]. O
Our O
model O
achieves O

Visual O
Question O
Answering O
(VQA B-DAT

). O
We O
evaluate O
on O
the O
VQA B-DAT
2.0 O
dataset O
[1, O
8], O
which O

our O
bilinear O
attention O
approach. O
The O
VQA B-DAT
evaluation O
metric O
considers O
inter-human O
variability O

Question O
embedding. O
For O
VQA, B-DAT
we O
get O
a O
question O
embedding O

Classifier. O
For O
VQA, B-DAT
we O
use O
a O
two-layer O
multi-layer O

For O
the O
test O
split O
of O
VQA, B-DAT
both O
train O
and O
validation O
splits O

without O
additional O
hyperparameter O
tuning O
from O
VQA B-DAT
experiments O

6 O
VQA B-DAT
results O
and O
discussions O

in O
Table O
1 O
shows O
2017 O
VQA B-DAT
Challenge O
winner O
architecture O
[2, O
29 O

in O
the O
leaderboard O
of O
both O
VQA B-DAT
Challenge O
2017 O
and O
2018 O
achieving O

Table O
1: O
Validation O
scores O
on O
VQA B-DAT
2.0 O
dataset O
for O
the O
number O

Model O
VQA B-DAT
Score O

Table O
2: O
Validation O
scores O
on O
VQA B-DAT
2.0 O
dataset O
for O
attention O
and O

Model O
nParams O
VQA B-DAT
Score O

test-standard O
scores O
of O
single-model O
on O
VQA B-DAT
2.0 O
dataset O
to O
compare O
state-of O

Parikh. O
Making O
the O
V O
in O
VQA B-DAT
Matter: O
Elevating O
the O
Role O
of O

Anton O
van O
den O
Hengel. O
The O
VQA B-DAT

that O
can O
be O
found O
in O
VQA B-DAT
and O
Visual O
Genome O
datasets O
or O

Test-standard O
scores O
of O
ensemble-model O
on O
VQA B-DAT
2.0 O
dataset O
to O
compare O
state-of-the-arts O

. O
Excerpt O
from O
the O
VQA B-DAT
2.0 O
Leaderboard O
at O
the O
time O

68.91 O
85.54 O
49.00 O
58.99 O
VQA B-DAT

6 O
VQA B-DAT
results O
and O
discussions O

Exper- O
imental O
results O
on O
the O
VQA B-DAT
v2 I-DAT
challenge O
demonstrates O
that O
our O
approach O

visual O
questions O
for O
the O
VQA B-DAT
v2 I-DAT
challenge O
(An- O
tol O
et O
al O

al., O
2018) O
model O
on O
the O
VQA B-DAT
v2 I-DAT
validation O
set O
(Antol O
et O
al O

use O
the O
validation O
set O
for O
VQA B-DAT
v2 I-DAT
to O
tune O
the O
initial O
learning O

VQA O
Dataset O
We O
use O
the O
VQA B-DAT
v2 I-DAT

000 O
question-image O
pairs O
from O
the O
VQA B-DAT
v2 I-DAT
validation O
set O
with O
hu- O
man O

imental O
results O
on O
the O
VQA O
v2 B-DAT
challenge O
demonstrates O
that O
our O
approach O

visual O
questions O
for O
the O
VQA O
v2 B-DAT
challenge O
(An- O
tol O
et O
al O

2018) O
model O
on O
the O
VQA O
v2 B-DAT
validation O
set O
(Antol O
et O
al O

image O
features O
V O
= O
{v1, O
v2, B-DAT
..., O
vK} O
using O
bottom-up O
attention O

v2 B-DAT
data O
(which O
are O
used O
in O

the O
validation O
set O
for O
VQA O
v2 B-DAT
to O
tune O
the O
initial O
learning O

Dataset O
We O
use O
the O
VQA O
v2 B-DAT

question-image O
pairs O
from O
the O
VQA O
v2 B-DAT
validation O
set O
with O
hu- O
man O

Exper- O
imental O
results O
on O
the O
VQA B-DAT
v2 I-DAT
challenge O
demonstrates O
that O
our O
approach O

visual O
questions O
for O
the O
VQA B-DAT
v2 I-DAT
challenge O
(An- O
tol O
et O
al O

al., O
2018) O
model O
on O
the O
VQA B-DAT
v2 I-DAT
validation O
set O
(Antol O
et O
al O

use O
the O
validation O
set O
for O
VQA B-DAT
v2 I-DAT
to O
tune O
the O
initial O
learning O

VQA O
Dataset O
We O
use O
the O
VQA B-DAT
v2 I-DAT

000 O
question-image O
pairs O
from O
the O
VQA B-DAT
v2 I-DAT
validation O
set O
with O
hu- O
man O

Abstract O
Visual O
question O
answering O
(VQA) B-DAT
and O
image O
captioning O
require O
a O

a O
novel O
approach O
to O
improve O
VQA B-DAT
performance O
that O
exploits O
this O
con O

Exper- O
imental O
results O
on O
the O
VQA B-DAT
v2 O
challenge O
demonstrates O
that O
our O

approach O
obtains O
state-of-the-art O
VQA B-DAT
performance O
(e.g. O
68.4% O
on O
the O

recent O
years, O
visual O
question O
answering O
(VQA) B-DAT
(Antol O
et O
al., O
2015) O
and O

and O
NLP O
communities. O
Most O
recent O
VQA B-DAT
research O
(Lu O
et O
al., O
2017 O

However,little O
VQA B-DAT
research O
works O
on O
exploit- O
ing O

be O
very O
useful O
for O
many O
VQA B-DAT
ques- O
tions. O
In O
particular, O
we O

visual O
questions O
for O
the O
VQA B-DAT
v2 O
challenge O
(An- O
tol O
et O

outperforming O
a O
large O
number O
of O
VQA B-DAT
models O
that O
use O
image O
features O

Existing O
work O
using O
captions O
for O
VQA B-DAT
has O
generated O
question- O
agnostic O
captions O

be O
relevant O
to O
the O
given O
VQA B-DAT
question O

relevant O
to O
a O
partic- O
ular O
VQA B-DAT
question. O
Fig. O
1 O
shows O
examples O

caption O
generation O
loss O
and O
the O
VQA B-DAT
answer O
pre- O
diction O
loss. O
A O

the O
corresponding O
captions O
help O
the O
VQA B-DAT
training O
process O

approach O
shows O
significant O
improvements O
on O
VQA B-DAT
accuracy O
over O
our O
baseline O
Up-Down O

al., O
2018) O
model O
on O
the O
VQA B-DAT
v2 O
validation O
set O
(Antol O
et O

methods O
have O
been O
proposed O
for O
VQA, B-DAT
including O
top-down O
(Ren O
et O
al O

pervision. O
However, O
comparatively O
little O
previous O
VQA B-DAT
research O
has O
worked O
on O
enriching O

is O
directly O
relevant O
to O
the O
VQA B-DAT
process O

the O
critical O
objects O
in O
the O
VQA B-DAT
process O
and O
provide O
information O
that O

can O
help O
the O
VQA B-DAT
module O
predict O
the O
answer O

generates O
question-relevant O
captions O
to O
aid O
VQA B-DAT

in O
phase O
1. O
Then, O
the O
VQA B-DAT
model O
is O
fine-tuned O
with O
generated O

in O
Sec. O
3.2. O
Then, O
the O
VQA B-DAT
module O
is O
presented O
in O
Sec O

captions O
to O
im- O
prove O
the O
VQA B-DAT
performance. O
In O
Sec. O
3.4, O
we O

a O
knowledge O
supplement O
to O
aid O
VQA, B-DAT
and O
to O
provide O
additional O
clues O

3.3 O
VQA B-DAT
Module O
This O
section O
describes O
the O

details O
of O
the O
VQA B-DAT
module. O
The O
generated O
captions O
are O

the O
bottom-up O
attention. O
Therefore, O
our O
VQA B-DAT
module O
utilizes O
the O
caption O
embeddings O

the O
top-down O
attention O
weights O
in O
VQA B-DAT
in O
order O
to O
produce O
the O

from O
the O
captions O
into O
the O
VQA B-DAT
process, O
we O
add O
the O
caption O

scores O
in O
the O
gold- O
standard O
VQA B-DAT

selected O
relevant O
cap- O
tions O
for O
VQA B-DAT
based O
on O
word O
similarities O
between O

account O
the O
details O
of O
the O
VQA B-DAT
process. O
In O
contrast, O
during O
training O

tion O
that O
will O
most O
improve O
VQA B-DAT

for O
both O
cap- O
tioning O
and O
VQA B-DAT

image O
captioning O
module O
and O
the O
VQA B-DAT
module O
in O
the O
optimization O
process O

is O
the O
sum O
of O
the O
VQA B-DAT
loss O
and O
the O
captioning O
loss O

use O
the O
validation O
set O
for O
VQA B-DAT
v2 O
to O
tune O
the O
initial O

yielding O
the O
highest O
over- O
all O
VQA B-DAT
score. O
We O
use O
1, O
280 O

and O
attention O
model O
in O
the O
VQA B-DAT
module O
with O
36 O
object O
detection O

al., O
2015) O
and O
pre-train O
the O
VQA B-DAT
and O
caption- O
generation O
modules O
for O

evaluate O
our O
joint O
model O
on O
VQA B-DAT

VQA B-DAT
Dataset O
We O
use O
the O
VQA O
v2.0 O
dataset O
(Antol O
et O
al O

dataset O
is O
used O
in O
the O
VQA B-DAT
2018 O
challenge O
and O
contains O
over O

report O
accuracies O
using O
the O
official O
VQA B-DAT
metric O
using O
soft O
scores, O
which O

main- O
tain O
consistency O
with O
the O
VQA B-DAT
tasks, O
we O
use O
the O
dataset’s O

valida- O
tion. O
Similar O
to O
the O
VQA B-DAT
question O
pre-processing, O
we O
first O
convert O

4.2 O
Results O
on O
VQA B-DAT

the O
experimental O
results O
on O
the O
VQA B-DAT
task O
and O
compare O
our O
results O

more O
helpful O
clues O
for O
the O
VQA B-DAT
process O
than O
the O
question-agnostic O
Up-Down O

2018) O
82.20 O
43.90 O
56.26 O
65.32 O
VQA B-DAT

Comparison O
of O
our O
results O
on O
VQA B-DAT
with O
the O
state-of-the-art O
methods O
on O

that O
are O
helpful O
to O
the O
VQA B-DAT
pro- O
cess. O
We O
use O
w O

adjust- O
ment O
(CAA) O
helps O
the O
VQA B-DAT
module O
focus O
atten- O
tion O
just O

annotated O
important O
objects O
from O
the O
VQA B-DAT

The O
question-relevant O
caption O
helps O
the O
VQA B-DAT
module O
adjust O
the O
visual O
attention O

evaluate O
multimodal O
(visual O
and O
textual) O
VQA B-DAT
explanation O
(Wu O
and O
Mooney, O
2018 O

). O
The O
VQA B-DAT

000 O
question-image O
pairs O
from O
the O
VQA B-DAT
v2 O
validation O
set O
with O
hu O

the O
highly-attended O
objects O
in O
the O
VQA B-DAT
process O
to O
the O
objects O
high O

2, O
000 O
human O
annotations O
in O
VQA B-DAT

to O
human O
attention-annotation O
from O
the O
VQA B-DAT

question-relevant O
image O
captions O
can O
improve O
VQA B-DAT
performance. O
In O
particular, O
we O
present O

additional O
in- O
formation O
to O
aid O
VQA B-DAT

the O
caption O
gen- O
eration O
and O
VQA B-DAT
tasks O
agree O
on O
the O
direction O

current O
state-of-the-art O
single O
model O
for O
VQA B-DAT

Attention O
for O
Image O
Captioning O
and O
VQA B-DAT

Zitnick, O
and O
Devi O
Parikh. O
2015. O
VQA B-DAT

2017. O
Making O
the O
V O
in O
VQA B-DAT
Matter: O
Elevating O
the O
Role O
of O

Cai, O
and O
Jiebo O
Luo. O
2018b. O
VQA B-DAT

in O
Tab. O
1 O
on O
both O
VQA B-DAT
v2 I-DAT

Tab. O
1 O
on O
both O
VQA O
v2 B-DAT

in O
Tab. O
1 O
on O
both O
VQA B-DAT
v2 I-DAT

Towards O
VQA B-DAT
Models O
That O
Can O
Read O

in O
the O
image. O
But O
today’s O
VQA B-DAT
models O
can O
not O
read! O
Our O

questions O
about O
text O
(e.g., O
the O
VQA B-DAT
dataset) O
or O
are O
too O
small O

that O
LoRRA O
outperforms O
existing O
state-of-the-art O
VQA B-DAT
models O
on O
our O
TextVQA O
dataset O

larger O
on O
TextVQA B-DAT
than O
on O
VQA O
2.0, O
suggesting O
that O
TextVQA O
is O

progress O
along O
directions O
complementary O
to O
VQA B-DAT
2.0 O

endowing O
Visual O
Question O
An- O
swering O
(VQA) B-DAT
models O
a O
new O
capability O

VQA B-DAT
has O
witnessed O
tremendous O
progress. O
But O

today’s O
VQA B-DAT
models O
fail O
catastrophically O
on O
questions O

top O
entries O
in O
the O
CVPR O
VQA B-DAT
Challenges O
(2016-18) O
struggle O
to O

dataset. O
TextVQA B-DAT
ques- O
tions O
require O
VQA O
models O
to O
understand O
text O
embedded O

answers O
predicted O
by O
a O
state-of-the-art O
VQA B-DAT
model O
(Pythia O
[17]) O
are O
shown O

in O
red. O
Clearly, O
today’s O
VQA B-DAT
models O
fail O
at O
answering O
questions O

art O
VQA B-DAT
models O
are O
predominantly O
monolithic O
deep O

from O
the O
distant O
supervision O
of O
VQA B-DAT
accuracy O

the O
all- O
encompassing O
task O
of O
VQA B-DAT

Specifically, O
we O
propose O
a O
new O
VQA B-DAT
model O
that O
includes O
OCR O
as O

paradigm O
popu- O
lar O
among O
existing O
VQA B-DAT
models). O
Our O
model O
learns O
this O

an O
initial O
step O
towards O
endowing O
VQA B-DAT
models O
with O
the O
ability O
to O

has O
been O
limited O
progress O
on O
VQA B-DAT
models O
that O
can O
read O
and O

are O
infrequent O
in O
the O
standard O
VQA B-DAT
datasets O
[3, O
10, O
51] O
because O

important O
skill O
that O
cur- O
rent O
VQA B-DAT
models O
lack O

the O
question O
as O
in O
traditional O
VQA, B-DAT
but O
also O
read O
the O
text O

LoRRA O
outperforms O
existing O
state-of-the-art O
VQA B-DAT
mod O

our O
TextVQA B-DAT
as O
well O
as O
VQA O
2.0 O
dataset O

Related O
work O
Visual O
Question O
Answering. O
VQA B-DAT
has O
seen O
numerous O

datasets O
since O
the O
first O
large-scale O
VQA B-DAT
dataset O
was O
introduced O
by O
Antol O

and O
more O
varied O
than O
earlier O
VQA B-DAT
datasets O
such O
as O
DAQUAR O
[31 O

reasoning O
independent O
of O
language, O
non-photo-realistic O
VQA B-DAT
datasets O
have O
been O
introduced O
such O

al. O
[45] O
introduced O
a O
Fact-Based O
VQA B-DAT
dataset O
which O
explicitly O
requires O
exter O

Text O
based O
VQA B-DAT

questions. O
MemexQA O
[16] O
introduces O
a O
VQA B-DAT
task O
which O
involves O
reasoning O
about O

Visual O
Representations O
for O
VQA B-DAT
Models. O
VQA O
mod- O
els O
typically O
use O
some O

extend O
the O
representations O
that O
a O
VQA B-DAT
model O
reasons O
over. O
Specifically, O
in O

contains O
three O
components: O
(i) O
a O
VQA B-DAT
component O
to O
reason O
and O
infer O

the O
OCR O
module O
and O
backbone O
VQA B-DAT
model O
can O
be O
any O
OCR O

model O
and O
any O
recent O
attention-based O
VQA B-DAT
model. O
Our O
approach O
is O
agnostic O

3.1. O
VQA B-DAT
Component O

Similar O
to O
many O
VQA B-DAT
models O
[7, O
17], O
we O
first O

level, O
the O
calculation O
of O
our O
VQA B-DAT
features O
fV O
QA(v, O
q) O
can O

use O
the O
same O
architecture O
as O
VQA B-DAT
component O
to O
get O
combined O
OCR-question O

are O
not O
shared O
with O
the O
VQA B-DAT
model O
component O
above O
but O
they O

fixed O
answer O
space, O
the O
current O
VQA B-DAT
models O
are O
only O
able O
to O

VQA B-DAT
2.0 O
Accuracy O
Model O
test-dev O

Table O
1: O
Single O
model O
VQA B-DAT
2.0 O
and O
VizWiz O
performance O
in O

is O
comparable O
to O
state-of-the-art O
on O
VQA B-DAT
2.0 O

Our O
VQA B-DAT
component O
is O
based O
on O
the O

VQA B-DAT
2018 O
chal- O
lenge O
winner O
entry O

hidden O
dimen- O
sions) O
achieves O
state-of-the-art O
VQA B-DAT
accuracy O
for O
a O
single O

in O
Tab. O
1 O
on O
both O
VQA B-DAT
v2.0 O
dataset O
[9] O
and O
VizWiz O

top- O
down O
attention O
network O
[1] O
(VQA B-DAT
winner O
2017), O
which O
in O
turn O

attention O
mechanism O
similar O
to O
the O
VQA B-DAT
2016 O
winner O
[7], O
which O
relied O

77k O
question O
words O
in O
the O
VQA B-DAT
2.0) O
for O
the O
question O
embedding O

of O
developing O
and O
study- O
ing O
VQA B-DAT
models O
that O
can O
reason O
about O

Dataset O
TextVQA B-DAT
VQA O
VizWiz O

Dataset O
TextVQA B-DAT
VQA O
VizWiz O

DatasetTextVQA B-DAT
VQA O
VizWiz O

TextVQA. B-DAT
We O
show O
comparisons O
with O
VQA O
2.0 O
[10] O
and O
VizWiz O
[13 O

a O
different O
answer. O
Follow- O
ing O
VQA B-DAT
[3, O
10] O
and O
VizWiz O
[13 O

3, O
13] O
when O
collecting O
the O
VQA B-DAT
and O
VizWiz O
datasets. O
In O
addition O

the O
same O
statistics O
for O
the O
VQA B-DAT
2.0 O
and O
the O
VizWiz O
datasets O

which O
is O
higher O
than O
in O
VQA B-DAT
2.0 O
(6.29) O
and O
VizWiz O
(6.68 O

is O
quite O
high O
compared O
to O
VQA B-DAT
2.0 O
(3.4%) O
and O
VizWiz O
(22.8 O

a O
challenge O
that O
most O
existing O
VQA B-DAT
datasets O
do O
not O
typically O
pose O

is O
more O
gradual O
than O
in O
VQA B-DAT
2.0 O
which O
drops O
sharply O
after O

respectively. O
Models O
with O
LoRRA O
outperform O
VQA B-DAT
SoTA O
(Pythia, O
BAN) O
and O
other O

ab- O
lation O
is O
state-of-the-art O
for O
VQA B-DAT
2.0 O
and O
doesn’t O
use O
any O

We O
calculate O
val O
accuracy O
using O
VQA B-DAT
accuracy O
metric O
[10] O
at O
every O

85.01%, O
consistent O
with O
that O
on O
VQA B-DAT
2.0 O
[10] O
and O
VizWiz O
[13 O

occurring O
questions. O
This O
accuracy O
on O
VQA B-DAT
2.0 O
validation O
set O
with O
3129 O

which O
are O
state- O
of-the-art O
on O
VQA B-DAT
2.0 O
and O
VizWiz O
only O
achieve O

demonstrates O
the O
inability O
of O
current O
VQA B-DAT
models O
to O
read O
and O
reason O

that O
LoRRA O
can O
help O
state-of-the-art O
VQA B-DAT
models O
to O
perform O
better O
on O

from O
68.71 O
to O
69.21 O
on O
VQA B-DAT
2.0 O
[9] O
(see O
Tab. O
1 O

reasoning O
in O
this O
more O
general O
VQA B-DAT
benchmark O

of O
the O
underlying O
OCR O
and O
VQA B-DAT
mod- O
ules. O
LoRRA O
significantly O
outperforms O

the O
current O
state-of- O
the-art O
VQA B-DAT
models O
on O
TextVQA. O
Our O
OCR O

well O
as O
on O
enabling O
the O
VQA B-DAT
models O
to O
read O
and O
reason O

Parikh. O
Making O
the O
V O
in O
VQA B-DAT
matter: O
Ele- O
vating O
the O
role O

Note O
that O
unlike O
most O
existing O
VQA B-DAT
models, O
the O
model O
does O
not O

in O
Visual O
Question O
An- O
swering O
(VQA) B-DAT
tasks. O
They O
help O
to O
learn O

generalizes O
some O
of O
the O
latest O
VQA B-DAT
architectures, O
providing O
state-of-the- O
art O
results O

is O
certainly O
Visual O
Question O
Answering O
(VQA) B-DAT
[19, O
2]. O
VQA O
is O
a O

for O
the O
fusion O
problem O
in O
VQA B-DAT
because O
they O
encode O
full O
second-order O

provide O
more O
details O
on O
related O
VQA B-DAT
works O
and O
highlight O
our O
contributions O

two O
spaces, O
the O
goal O
of O
VQA B-DAT
[2, O
19] O
is O
to O
merge O

systems, O
and O
are O
fundamental O
for O
VQA B-DAT
models O
to O
obtain O
the O
best O

Multimodal O
language O
modeling O
[10]. O
In O
VQA, B-DAT
a O
simple O
element-wise O
product O
be O

state-of-the-art O
performances O
on O
the O
well-known O
VQA B-DAT
database O
[2]. O
Despite O
these O
impressive O

New O
fusion O
scheme O
for O
VQA B-DAT
relying O
on O
a O
Tucker O
tensor-based O

Answering O
(VQA). B-DAT
In O
VQA, O
one O
is O
given O
a O
question O

2. O
As O
commonly O
done O
in O
VQA, B-DAT
images O
v O
and O
questions O
q O

linguistic O
information O
is O
crucial O
in O
VQA B-DAT

interactions O
quickly O
become O
intractable O
in O
VQA, B-DAT
because O
the O
size O
of O
the O

4. O
Experiments O
VQA B-DAT
Dataset O
The O
VQA O
dataset O
[2] O
is O
built O
over O

for O
a O
visual O
question, O
the O
VQA B-DAT
accuracy O
is O
given O
by O

with O
recently O
introduced O
techniques O
for O
VQA, B-DAT
which O
are O
de- O
scribed O
below O

Each O
(image,question) O
pair O
in O
the O
VQA B-DAT
dataset O
is O
annotated O
with O
10 O

the O
test-dev O
and O
test-standard O
splits O
VQA B-DAT
dataset; O
(n) O
for O
an O
ensemble O

directly O
translates O
for O
the O
whole O
VQA B-DAT
task O

models O
are O
trained O
on O
the O
VQA B-DAT
train O
split, O
and O
the O
scores O

Figure O
5: O
Accuracy O
on O
VQA B-DAT
val O
in O
function O
of O
to O

the O
R O
spaces O
using O
the O
VQA B-DAT
question O
types. O
We O
first O
train O

the O
VQA B-DAT
task. O
Our O
main O
contribution O
is O

gener- O
alizes O
the O
most O
competitive O
VQA B-DAT
architectures. O
MUTAN O
is O
evaluated O
on O

the O
most O
recent O
VQA B-DAT
dataset, O
reaching O
state- O
of-the-art O

L. O
Zitnick, O
and O
D. O
Parikh. O
VQA B-DAT

the O
images O
before O
training O
our O
VQA B-DAT
models O
as O
follow. O
We O
load O

the O
test-dev O
and O
test-standard O
splits O
VQA B-DAT
dataset; O
(n) O
for O
an O
ensemble O

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2 I-DAT

To O
benchmark O
models O
on O
VQA B-DAT
v2 I-DAT

also O
train O
these O
models O
on O
VQA B-DAT
v2 I-DAT

and O
report O
re- O
sults O
on O
VQA B-DAT
v2 I-DAT

Papers O
report- O
ing O
results O
on O
VQA B-DAT
v2 I-DAT

VQA O
models O
when O
trained O
on O
VQA B-DAT
v2 I-DAT

.0 O
train+val O
and O
tested O
on O
VQA B-DAT
v2 I-DAT

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2 I-DAT

Answering O
Dataset O
and O
Challenge O
(VQA O
v2 B-DAT

To O
benchmark O
models O
on O
VQA O
v2 B-DAT

train O
these O
models O
on O
VQA O
v2 B-DAT

report O
re- O
sults O
on O
VQA O
v2 B-DAT

report- O
ing O
results O
on O
VQA O
v2 B-DAT

models O
when O
trained O
on O
VQA O
v2 B-DAT

train+val O
and O
tested O
on O
VQA O
v2 B-DAT

Answering O
Dataset O
and O
Challenge O
(VQA O
v2 B-DAT

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2 I-DAT

To O
benchmark O
models O
on O
VQA B-DAT
v2 I-DAT

also O
train O
these O
models O
on O
VQA B-DAT
v2 I-DAT

and O
report O
re- O
sults O
on O
VQA B-DAT
v2 I-DAT

Papers O
report- O
ing O
results O
on O
VQA B-DAT
v2 I-DAT

VQA O
models O
when O
trained O
on O
VQA B-DAT
v2 I-DAT

.0 O
train+val O
and O
tested O
on O
VQA B-DAT
v2 I-DAT

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2 I-DAT

Making O
the O
V O
in O
VQA B-DAT
Matter: O
Elevating O
the O
Role O
of O

task O
of O
Visual O
Question O
Answering O
(VQA) B-DAT
and O
make O
vision O
(the O
V O

in O
VQA) B-DAT
matter! O
Specifically, O
we O
balance O
the O

popular O
VQA B-DAT
dataset O
[3] O
by O
collecting O
complementary O

balanced O
than O
the O
origi- O
nal O
VQA B-DAT
dataset O
and O
has O
approximately O
twice O

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2.0 O

benchmark O
a O
number O
of O
state-of-art O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

1: O
Examples O
from O
our O
balanced O
VQA B-DAT
dataset O

28] O
and O
visual O
question O
answering O
(VQA) B-DAT
[3, O
26, O
27, O
10, O
31 O

1]. O
For O
instance, O
in O
the O
VQA B-DAT
[3] O
dataset, O
the O
most O
com O

visual O
priming O
bias’ O
in O
the O
VQA B-DAT
dataset O
– O
specifi- O
cally, O
subjects O

in O
the O
VQA B-DAT
dataset O
starting O
with O
the O
n-gram O

associated O
image O
results O
in O
a O
VQA B-DAT
accuracy O
of O
87 O

role O
of O
image O
understanding O
in O
VQA B-DAT

goal, O
we O
collect O
a O
balanced O
VQA B-DAT
dataset O
with O
significantly O
reduced O
language O

ically, O
we O
create O
a O
balanced O
VQA B-DAT
dataset O
in O
the O
following O
way O

answer) O
triplet O
(I,Q,A) O
from O
the O
VQA B-DAT
dataset, O
we O
ask O
a O
human O

this O
balanced O
dataset O
will O
force O
VQA B-DAT
models O
to O
focus O
on O
visual O

exploiting O
language O
priors, O
en- O
abling O
VQA B-DAT
evaluation O
protocols O
to O
more O
accurately O

Our O
balanced O
VQA B-DAT
dataset O
is O
also O
particularly O
difficult O

of O
VGGNet O
[37] O
features. O
Therefore, O
VQA B-DAT
models O
will O
need O
to O
under O

double O
the O
size O
of O
the O
VQA B-DAT
[3] O
dataset O
– O
with O
approximately O

23]. O
We O
believe O
this O
balanced O
VQA B-DAT
dataset O
is O
a O
better O
dataset O

to O
benchmark O
VQA B-DAT
approaches, O
and O
is O
publicly O
available O

will O
allow O
users O
of O
the O
VQA B-DAT
model O
to O
establish O
greater O
trust O

1) O
We O
balance O
the O
existing O
VQA B-DAT
dataset O
[3] O
by O
collecting O
complementary O

result O
is O
a O
more O
balanced O
VQA B-DAT
dataset, O
which O
is O
also O
approximately O

the O
size O
of O
the O
original O
VQA B-DAT
dataset. O
(2) O
We O
evaluate O
state-of-art O

VQA B-DAT
models O
(with O
publicly O
available O
code O

trained O
on O
the O
existing O
‘unbalanced’ O
VQA B-DAT
dataset O
perform O
poorly O
on O
our O

language O
priors O
in O
the O
existing O
VQA B-DAT
dataset O
to O
achieve O
higher O
accuracy O

builds O
on O
top O
of O
the O
VQA B-DAT
dataset O
from O
Antol O
et O
al O

of O
the O
most O
widely O
used O
VQA B-DAT
datasets. O
We O
reduce O
the O
language O

twice O
the O
size O
of O
the O
VQA B-DAT
dataset. O
We O
benchmark O
one O
‘baseline O

’ O
VQA B-DAT
model O
[24], O
one O
attention-based O
VQA O

the O
winning O
model O
from O
the O
VQA B-DAT
Real O
Open O
Ended O
Chal- O
lenge O

2016 O
[9] O
on O
our O
balanced O
VQA B-DAT
dataset, O
and O
compare O
them O
to O

examples O
from O
our O
proposed O
balanced O
VQA B-DAT
dataset. O
Each O
question O
has O
two O

similar O
images O
for O
questions O
in O
VQA B-DAT

study O
this O
goal O
of O
balancing O
VQA B-DAT
in O
a O
fairly O
restricted O
setting O

from O
clipart O
(part O
of O
the O
VQA B-DAT
abstract O
scenes O
dataset O
[3]). O
Using O

binary O
ones), O
benchmarking O
of O
state-of-art O
VQA B-DAT
models O
on O
the O
balanced O
dataset O

, O
and O
finally O
the O
novel O
VQA B-DAT
model O
with O
counter-example O
based O
explanations O

build O
on O
top O
of O
the O
VQA B-DAT
dataset O
introduced O
by O
An- O
tol O

et O
al. O
[3]. O
VQA B-DAT
real O
images O
dataset O
contains O
just O

has O
spurred O
significant O
progress O
in O
VQA B-DAT
domain, O
as O
discussed O
earlier, O
it O

answer) O
triplet O
(I,Q,A) O
in O
the O
VQA B-DAT
dataset, O
our O
goal O
is O
to O

are O
seman- O
tically O
similar, O
a O
VQA B-DAT
model O
will O
have O
to O
understand O

questions O
from O
the O
original O
(unbalanced) O
VQA B-DAT
dataset O
[3] O
(top) O
and O
from O

all O
the O
questions O
in O
the O
VQA B-DAT
dataset. O
We O
believe O
that O
a O

and O
test O
splits O
of O
the O
VQA B-DAT
dataset. O
AMT O
workers O
picked O
“not O

question, O
image) O
pairs. O
Following O
original O
VQA B-DAT
dataset O
[3], O
we O
divide O
our O

We O
use O
the O
publicly O
released O
VQA B-DAT
evaluation O
script O
in O
our O
experiments O

for O
each O
question O
to O
compute O
VQA B-DAT
accura- O
cies. O
As O
described O
above O

to O
be O
consistent O
with O
the O
VQA B-DAT
dataset O
[3]. O
Note O
that O
while O

type O
in O
our O
new O
balanced O
VQA B-DAT
dataset O
with O
the O
original O
(un O

- O
balanced) O
VQA B-DAT
dataset O
[3]. O
We O
notice O
several O

balanced O
dataset O
compared O
to O
unbalanced O
VQA B-DAT
dataset. O
“baseball” O
is O
now O
slightly O

more O
balanced O
than O
the O
original O
VQA B-DAT
dataset. O
The O
resultant O
impact O
of O

ing O
on O
performance O
of O
state-of-the-art O
VQA B-DAT
models O
is O
dis- O
cussed O
in O

4. O
Benchmarking O
Existing O
VQA B-DAT
Models O
Our O
first O
approach O
to O

training O
a O
VQA B-DAT
model O
that O
empha O

to O
re-train O
the O
existing O
state-of-art O
VQA B-DAT
models O
(with O
code O
publicly O
available O

9]) O
on O
our O
new O
balanced O
VQA B-DAT
dataset. O
Our O
hypothesis O
is O
that O

I) O
[24]: O
This O
was O
the O
VQA B-DAT
model O
introduced O
in O
[3] O
together O

This O
is O
a O
recent O
attention-based O
VQA B-DAT
model O
that O
‘co-attends’ O
to O
both O

real O
images O
track O
of O
the O
VQA B-DAT
Challenge O
2016. O
This O
model O
uses O

utilize O
any O
visual O
information. O
Comparing O
VQA B-DAT
models O
to O
language- O
only O
ablations O

quantifies O
to O
what O
extent O
VQA B-DAT
models O
have O
succeeded O
in O
leveraging O

Table O
1: O
Performance O
of O
VQA B-DAT
models O
when O
trained/tested O
on O
unbalanced/balanced O

VQA B-DAT
datasets. O
UB O
stands O
for O
training O

see O
that O
the O
current O
state-of-art O
VQA B-DAT
models O
trained O
on O
the O
original O

unbalanced) O
VQA B-DAT
dataset O
perform O
signifi- O
cantly O
worse O

evaluating O
on O
the O
original O
unbalanced O
VQA B-DAT
dataset O
(i.e., O
comparing O
UU O
to O

in O
accuracy O
suggests O
that O
current O
VQA B-DAT
models O
are O
data O
starved, O
and O

would O
benefit O
from O
even O
larger O
VQA B-DAT
datasets O

the O
language-bias O
in O
the O
original O
VQA B-DAT
dataset, O
and O
its O
successful O
alleviation O

same O
question. O
To O
be O
successful, O
VQA B-DAT
models O
need O
to O
understand O
the O

an- O
alyze O
the O
performance O
of O
VQA B-DAT
models O
in O
unique O
ways. O
Given O

the O
prediction O
of O
a O
VQA B-DAT
model, O
we O
can O
count O
the O

training O
on O
balanced O
dataset, O
this O
VQA B-DAT
model O
has O
learned O
to O
tell O

room O
for O
improvement O
remains. O
The O
VQA B-DAT
model O
still O
can O
not O
tell O

To O
benchmark O
models O
on O
VQA B-DAT
v2.0 O
dataset, O
we O
also O
train O

these O
models O
on O
VQA B-DAT
v2.0 O
train+val O
and O
report O
re O

- O
sults O
on O
VQA B-DAT
v2.0 O
test-standard O
in O
Table O
2 O

Papers O
report- O
ing O
results O
on O
VQA B-DAT
v2.0 O
dataset O
are O
suggested O
to O

Table O
2: O
Performance O
of O
VQA B-DAT
models O
when O
trained O
on O
VQA O

v2.0 O
train+val O
and O
tested O
on O
VQA B-DAT
v2.0 O
test-standard O
dataset O

models O
when O
trained/tested O
on O
unbalanced/balanced O
VQA B-DAT
datasets. O
UB O
stands O
for O
training O

HieCoAtt). O
This O
suggests O
that O
these O
VQA B-DAT
mod- O
els O
are O
really O
exploiting O

that O
for O
both O
the O
state-of-art O
VQA B-DAT
mod- O
els, O
the O
largest O
source O

the O
results O
announced O
at O
the O
VQA B-DAT
Real O
Open O
Ended O
Challenge O
20162 O

priors O
present O
in O
the O
unbalanced O
VQA B-DAT
dataset O
(particularly O
in O
the O
“yes/no O

similar O
accuracies O
for O
all O
state-of-art O
VQA B-DAT
models, O
rendering O
vastly O
different O
models O

these O
answer-types). O
Benchmarking O
these O
different O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

color O
is O
the O
fire-hydrant?” O
a O
VQA B-DAT
model O
may O
be O
perceived O
as O

step, O
similar O
to O
a O
conventional O
VQA B-DAT
model, O
it O
takes O
in O
an O

similar O
to O
a O
conven- O
tional O
VQA B-DAT
model), O
and O
second, O
it O
explains O

VQA B-DAT
head O
in O
our O
model, O
and O

the O
most O
likely O
counter- O
example. O
VQA B-DAT
Model: O
Using O
a O
VQA O
model’s O

Random O
Distance O
VQA B-DAT
[3] O
Ours O

baseline, O
as O
well O
as O
the O
VQA B-DAT
[3] O
model. O
Interestingly, O
the O
strongest O

terface O
to O
‘balance’ O
the O
popular O
VQA B-DAT
dataset O
[3] O
by O
col- O
lecting O

bal- O
anced O
than O
the O
original O
VQA B-DAT
dataset O
by O
construction, O
but O
also O

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2.0 O

a O
number O
of O
(near) O
state-of-art O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

L. O
Zitnick, O
and O
D. O
Parikh. O
VQA B-DAT

D. O
Parikh. O
Question O
Relevance O
in O
VQA B-DAT

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2 I-DAT

To O
benchmark O
models O
on O
VQA B-DAT
v2 I-DAT

also O
train O
these O
models O
on O
VQA B-DAT
v2 I-DAT

and O
report O
re- O
sults O
on O
VQA B-DAT
v2 I-DAT

Papers O
report- O
ing O
results O
on O
VQA B-DAT
v2 I-DAT

VQA O
models O
when O
trained O
on O
VQA B-DAT
v2 I-DAT

.0 O
train+val O
and O
tested O
on O
VQA B-DAT
v2 I-DAT

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2 I-DAT

Answering O
Dataset O
and O
Challenge O
(VQA O
v2 B-DAT

To O
benchmark O
models O
on O
VQA O
v2 B-DAT

train O
these O
models O
on O
VQA O
v2 B-DAT

report O
re- O
sults O
on O
VQA O
v2 B-DAT

report- O
ing O
results O
on O
VQA O
v2 B-DAT

models O
when O
trained O
on O
VQA O
v2 B-DAT

train+val O
and O
tested O
on O
VQA O
v2 B-DAT

Answering O
Dataset O
and O
Challenge O
(VQA O
v2 B-DAT

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2 I-DAT

To O
benchmark O
models O
on O
VQA B-DAT
v2 I-DAT

also O
train O
these O
models O
on O
VQA B-DAT
v2 I-DAT

and O
report O
re- O
sults O
on O
VQA B-DAT
v2 I-DAT

Papers O
report- O
ing O
results O
on O
VQA B-DAT
v2 I-DAT

VQA O
models O
when O
trained O
on O
VQA B-DAT
v2 I-DAT

.0 O
train+val O
and O
tested O
on O
VQA B-DAT
v2 I-DAT

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2 I-DAT

Making O
the O
V O
in O
VQA B-DAT
Matter: O
Elevating O
the O
Role O
of O

task O
of O
Visual O
Question O
Answering O
(VQA) B-DAT
and O
make O
vision O
(the O
V O

in O
VQA) B-DAT
matter! O
Specifically, O
we O
balance O
the O

popular O
VQA B-DAT
dataset O
[3] O
by O
collecting O
complementary O

balanced O
than O
the O
origi- O
nal O
VQA B-DAT
dataset O
and O
has O
approximately O
twice O

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2.0 O

benchmark O
a O
number O
of O
state-of-art O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

1: O
Examples O
from O
our O
balanced O
VQA B-DAT
dataset O

28] O
and O
visual O
question O
answering O
(VQA) B-DAT
[3, O
26, O
27, O
10, O
31 O

1]. O
For O
instance, O
in O
the O
VQA B-DAT
[3] O
dataset, O
the O
most O
com O

visual O
priming O
bias’ O
in O
the O
VQA B-DAT
dataset O
– O
specifi- O
cally, O
subjects O

in O
the O
VQA B-DAT
dataset O
starting O
with O
the O
n-gram O

associated O
image O
results O
in O
a O
VQA B-DAT
accuracy O
of O
87 O

role O
of O
image O
understanding O
in O
VQA B-DAT

goal, O
we O
collect O
a O
balanced O
VQA B-DAT
dataset O
with O
significantly O
reduced O
language O

ically, O
we O
create O
a O
balanced O
VQA B-DAT
dataset O
in O
the O
following O
way O

answer) O
triplet O
(I,Q,A) O
from O
the O
VQA B-DAT
dataset, O
we O
ask O
a O
human O

this O
balanced O
dataset O
will O
force O
VQA B-DAT
models O
to O
focus O
on O
visual O

exploiting O
language O
priors, O
en- O
abling O
VQA B-DAT
evaluation O
protocols O
to O
more O
accurately O

Our O
balanced O
VQA B-DAT
dataset O
is O
also O
particularly O
difficult O

of O
VGGNet O
[37] O
features. O
Therefore, O
VQA B-DAT
models O
will O
need O
to O
under O

double O
the O
size O
of O
the O
VQA B-DAT
[3] O
dataset O
– O
with O
approximately O

23]. O
We O
believe O
this O
balanced O
VQA B-DAT
dataset O
is O
a O
better O
dataset O

to O
benchmark O
VQA B-DAT
approaches, O
and O
is O
publicly O
available O

will O
allow O
users O
of O
the O
VQA B-DAT
model O
to O
establish O
greater O
trust O

1) O
We O
balance O
the O
existing O
VQA B-DAT
dataset O
[3] O
by O
collecting O
complementary O

result O
is O
a O
more O
balanced O
VQA B-DAT
dataset, O
which O
is O
also O
approximately O

the O
size O
of O
the O
original O
VQA B-DAT
dataset. O
(2) O
We O
evaluate O
state-of-art O

VQA B-DAT
models O
(with O
publicly O
available O
code O

trained O
on O
the O
existing O
‘unbalanced’ O
VQA B-DAT
dataset O
perform O
poorly O
on O
our O

language O
priors O
in O
the O
existing O
VQA B-DAT
dataset O
to O
achieve O
higher O
accuracy O

builds O
on O
top O
of O
the O
VQA B-DAT
dataset O
from O
Antol O
et O
al O

of O
the O
most O
widely O
used O
VQA B-DAT
datasets. O
We O
reduce O
the O
language O

twice O
the O
size O
of O
the O
VQA B-DAT
dataset. O
We O
benchmark O
one O
‘baseline O

’ O
VQA B-DAT
model O
[24], O
one O
attention-based O
VQA O

the O
winning O
model O
from O
the O
VQA B-DAT
Real O
Open O
Ended O
Chal- O
lenge O

2016 O
[9] O
on O
our O
balanced O
VQA B-DAT
dataset, O
and O
compare O
them O
to O

examples O
from O
our O
proposed O
balanced O
VQA B-DAT
dataset. O
Each O
question O
has O
two O

similar O
images O
for O
questions O
in O
VQA B-DAT

study O
this O
goal O
of O
balancing O
VQA B-DAT
in O
a O
fairly O
restricted O
setting O

from O
clipart O
(part O
of O
the O
VQA B-DAT
abstract O
scenes O
dataset O
[3]). O
Using O

binary O
ones), O
benchmarking O
of O
state-of-art O
VQA B-DAT
models O
on O
the O
balanced O
dataset O

, O
and O
finally O
the O
novel O
VQA B-DAT
model O
with O
counter-example O
based O
explanations O

build O
on O
top O
of O
the O
VQA B-DAT
dataset O
introduced O
by O
An- O
tol O

et O
al. O
[3]. O
VQA B-DAT
real O
images O
dataset O
contains O
just O

has O
spurred O
significant O
progress O
in O
VQA B-DAT
domain, O
as O
discussed O
earlier, O
it O

answer) O
triplet O
(I,Q,A) O
in O
the O
VQA B-DAT
dataset, O
our O
goal O
is O
to O

are O
seman- O
tically O
similar, O
a O
VQA B-DAT
model O
will O
have O
to O
understand O

questions O
from O
the O
original O
(unbalanced) O
VQA B-DAT
dataset O
[3] O
(top) O
and O
from O

all O
the O
questions O
in O
the O
VQA B-DAT
dataset. O
We O
believe O
that O
a O

and O
test O
splits O
of O
the O
VQA B-DAT
dataset. O
AMT O
workers O
picked O
“not O

question, O
image) O
pairs. O
Following O
original O
VQA B-DAT
dataset O
[3], O
we O
divide O
our O

We O
use O
the O
publicly O
released O
VQA B-DAT
evaluation O
script O
in O
our O
experiments O

for O
each O
question O
to O
compute O
VQA B-DAT
accura- O
cies. O
As O
described O
above O

to O
be O
consistent O
with O
the O
VQA B-DAT
dataset O
[3]. O
Note O
that O
while O

type O
in O
our O
new O
balanced O
VQA B-DAT
dataset O
with O
the O
original O
(un O

- O
balanced) O
VQA B-DAT
dataset O
[3]. O
We O
notice O
several O

balanced O
dataset O
compared O
to O
unbalanced O
VQA B-DAT
dataset. O
“baseball” O
is O
now O
slightly O

more O
balanced O
than O
the O
original O
VQA B-DAT
dataset. O
The O
resultant O
impact O
of O

ing O
on O
performance O
of O
state-of-the-art O
VQA B-DAT
models O
is O
dis- O
cussed O
in O

4. O
Benchmarking O
Existing O
VQA B-DAT
Models O
Our O
first O
approach O
to O

training O
a O
VQA B-DAT
model O
that O
empha O

to O
re-train O
the O
existing O
state-of-art O
VQA B-DAT
models O
(with O
code O
publicly O
available O

9]) O
on O
our O
new O
balanced O
VQA B-DAT
dataset. O
Our O
hypothesis O
is O
that O

I) O
[24]: O
This O
was O
the O
VQA B-DAT
model O
introduced O
in O
[3] O
together O

This O
is O
a O
recent O
attention-based O
VQA B-DAT
model O
that O
‘co-attends’ O
to O
both O

real O
images O
track O
of O
the O
VQA B-DAT
Challenge O
2016. O
This O
model O
uses O

utilize O
any O
visual O
information. O
Comparing O
VQA B-DAT
models O
to O
language- O
only O
ablations O

quantifies O
to O
what O
extent O
VQA B-DAT
models O
have O
succeeded O
in O
leveraging O

Table O
1: O
Performance O
of O
VQA B-DAT
models O
when O
trained/tested O
on O
unbalanced/balanced O

VQA B-DAT
datasets. O
UB O
stands O
for O
training O

see O
that O
the O
current O
state-of-art O
VQA B-DAT
models O
trained O
on O
the O
original O

unbalanced) O
VQA B-DAT
dataset O
perform O
signifi- O
cantly O
worse O

evaluating O
on O
the O
original O
unbalanced O
VQA B-DAT
dataset O
(i.e., O
comparing O
UU O
to O

in O
accuracy O
suggests O
that O
current O
VQA B-DAT
models O
are O
data O
starved, O
and O

would O
benefit O
from O
even O
larger O
VQA B-DAT
datasets O

the O
language-bias O
in O
the O
original O
VQA B-DAT
dataset, O
and O
its O
successful O
alleviation O

same O
question. O
To O
be O
successful, O
VQA B-DAT
models O
need O
to O
understand O
the O

an- O
alyze O
the O
performance O
of O
VQA B-DAT
models O
in O
unique O
ways. O
Given O

the O
prediction O
of O
a O
VQA B-DAT
model, O
we O
can O
count O
the O

training O
on O
balanced O
dataset, O
this O
VQA B-DAT
model O
has O
learned O
to O
tell O

room O
for O
improvement O
remains. O
The O
VQA B-DAT
model O
still O
can O
not O
tell O

To O
benchmark O
models O
on O
VQA B-DAT
v2.0 O
dataset, O
we O
also O
train O

these O
models O
on O
VQA B-DAT
v2.0 O
train+val O
and O
report O
re O

- O
sults O
on O
VQA B-DAT
v2.0 O
test-standard O
in O
Table O
2 O

Papers O
report- O
ing O
results O
on O
VQA B-DAT
v2.0 O
dataset O
are O
suggested O
to O

Table O
2: O
Performance O
of O
VQA B-DAT
models O
when O
trained O
on O
VQA O

v2.0 O
train+val O
and O
tested O
on O
VQA B-DAT
v2.0 O
test-standard O
dataset O

models O
when O
trained/tested O
on O
unbalanced/balanced O
VQA B-DAT
datasets. O
UB O
stands O
for O
training O

HieCoAtt). O
This O
suggests O
that O
these O
VQA B-DAT
mod- O
els O
are O
really O
exploiting O

that O
for O
both O
the O
state-of-art O
VQA B-DAT
mod- O
els, O
the O
largest O
source O

the O
results O
announced O
at O
the O
VQA B-DAT
Real O
Open O
Ended O
Challenge O
20162 O

priors O
present O
in O
the O
unbalanced O
VQA B-DAT
dataset O
(particularly O
in O
the O
“yes/no O

similar O
accuracies O
for O
all O
state-of-art O
VQA B-DAT
models, O
rendering O
vastly O
different O
models O

these O
answer-types). O
Benchmarking O
these O
different O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

color O
is O
the O
fire-hydrant?” O
a O
VQA B-DAT
model O
may O
be O
perceived O
as O

step, O
similar O
to O
a O
conventional O
VQA B-DAT
model, O
it O
takes O
in O
an O

similar O
to O
a O
conven- O
tional O
VQA B-DAT
model), O
and O
second, O
it O
explains O

VQA B-DAT
head O
in O
our O
model, O
and O

the O
most O
likely O
counter- O
example. O
VQA B-DAT
Model: O
Using O
a O
VQA O
model’s O

Random O
Distance O
VQA B-DAT
[3] O
Ours O

baseline, O
as O
well O
as O
the O
VQA B-DAT
[3] O
model. O
Interestingly, O
the O
strongest O

terface O
to O
‘balance’ O
the O
popular O
VQA B-DAT
dataset O
[3] O
by O
col- O
lecting O

bal- O
anced O
than O
the O
original O
VQA B-DAT
dataset O
by O
construction, O
but O
also O

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2.0 O

a O
number O
of O
(near) O
state-of-art O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

L. O
Zitnick, O
and O
D. O
Parikh. O
VQA B-DAT

D. O
Parikh. O
Question O
Relevance O
in O
VQA B-DAT

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2 I-DAT

To O
benchmark O
models O
on O
VQA B-DAT
v2 I-DAT

also O
train O
these O
models O
on O
VQA B-DAT
v2 I-DAT

and O
report O
re- O
sults O
on O
VQA B-DAT
v2 I-DAT

Papers O
report- O
ing O
results O
on O
VQA B-DAT
v2 I-DAT

VQA O
models O
when O
trained O
on O
VQA B-DAT
v2 I-DAT

.0 O
train+val O
and O
tested O
on O
VQA B-DAT
v2 I-DAT

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2 I-DAT

Answering O
Dataset O
and O
Challenge O
(VQA O
v2 B-DAT

To O
benchmark O
models O
on O
VQA O
v2 B-DAT

train O
these O
models O
on O
VQA O
v2 B-DAT

report O
re- O
sults O
on O
VQA O
v2 B-DAT

report- O
ing O
results O
on O
VQA O
v2 B-DAT

models O
when O
trained O
on O
VQA O
v2 B-DAT

train+val O
and O
tested O
on O
VQA O
v2 B-DAT

Answering O
Dataset O
and O
Challenge O
(VQA O
v2 B-DAT

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2 I-DAT

To O
benchmark O
models O
on O
VQA B-DAT
v2 I-DAT

also O
train O
these O
models O
on O
VQA B-DAT
v2 I-DAT

and O
report O
re- O
sults O
on O
VQA B-DAT
v2 I-DAT

Papers O
report- O
ing O
results O
on O
VQA B-DAT
v2 I-DAT

VQA O
models O
when O
trained O
on O
VQA B-DAT
v2 I-DAT

.0 O
train+val O
and O
tested O
on O
VQA B-DAT
v2 I-DAT

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2 I-DAT

Making O
the O
V O
in O
VQA B-DAT
Matter: O
Elevating O
the O
Role O
of O

task O
of O
Visual O
Question O
Answering O
(VQA) B-DAT
and O
make O
vision O
(the O
V O

in O
VQA) B-DAT
matter! O
Specifically, O
we O
balance O
the O

popular O
VQA B-DAT
dataset O
[3] O
by O
collecting O
complementary O

balanced O
than O
the O
origi- O
nal O
VQA B-DAT
dataset O
and O
has O
approximately O
twice O

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2.0 O

benchmark O
a O
number O
of O
state-of-art O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

1: O
Examples O
from O
our O
balanced O
VQA B-DAT
dataset O

28] O
and O
visual O
question O
answering O
(VQA) B-DAT
[3, O
26, O
27, O
10, O
31 O

1]. O
For O
instance, O
in O
the O
VQA B-DAT
[3] O
dataset, O
the O
most O
com O

visual O
priming O
bias’ O
in O
the O
VQA B-DAT
dataset O
– O
specifi- O
cally, O
subjects O

in O
the O
VQA B-DAT
dataset O
starting O
with O
the O
n-gram O

associated O
image O
results O
in O
a O
VQA B-DAT
accuracy O
of O
87 O

role O
of O
image O
understanding O
in O
VQA B-DAT

goal, O
we O
collect O
a O
balanced O
VQA B-DAT
dataset O
with O
significantly O
reduced O
language O

ically, O
we O
create O
a O
balanced O
VQA B-DAT
dataset O
in O
the O
following O
way O

answer) O
triplet O
(I,Q,A) O
from O
the O
VQA B-DAT
dataset, O
we O
ask O
a O
human O

this O
balanced O
dataset O
will O
force O
VQA B-DAT
models O
to O
focus O
on O
visual O

exploiting O
language O
priors, O
en- O
abling O
VQA B-DAT
evaluation O
protocols O
to O
more O
accurately O

Our O
balanced O
VQA B-DAT
dataset O
is O
also O
particularly O
difficult O

of O
VGGNet O
[37] O
features. O
Therefore, O
VQA B-DAT
models O
will O
need O
to O
under O

double O
the O
size O
of O
the O
VQA B-DAT
[3] O
dataset O
– O
with O
approximately O

23]. O
We O
believe O
this O
balanced O
VQA B-DAT
dataset O
is O
a O
better O
dataset O

to O
benchmark O
VQA B-DAT
approaches, O
and O
is O
publicly O
available O

will O
allow O
users O
of O
the O
VQA B-DAT
model O
to O
establish O
greater O
trust O

1) O
We O
balance O
the O
existing O
VQA B-DAT
dataset O
[3] O
by O
collecting O
complementary O

result O
is O
a O
more O
balanced O
VQA B-DAT
dataset, O
which O
is O
also O
approximately O

the O
size O
of O
the O
original O
VQA B-DAT
dataset. O
(2) O
We O
evaluate O
state-of-art O

VQA B-DAT
models O
(with O
publicly O
available O
code O

trained O
on O
the O
existing O
‘unbalanced’ O
VQA B-DAT
dataset O
perform O
poorly O
on O
our O

language O
priors O
in O
the O
existing O
VQA B-DAT
dataset O
to O
achieve O
higher O
accuracy O

builds O
on O
top O
of O
the O
VQA B-DAT
dataset O
from O
Antol O
et O
al O

of O
the O
most O
widely O
used O
VQA B-DAT
datasets. O
We O
reduce O
the O
language O

twice O
the O
size O
of O
the O
VQA B-DAT
dataset. O
We O
benchmark O
one O
‘baseline O

’ O
VQA B-DAT
model O
[24], O
one O
attention-based O
VQA O

the O
winning O
model O
from O
the O
VQA B-DAT
Real O
Open O
Ended O
Chal- O
lenge O

2016 O
[9] O
on O
our O
balanced O
VQA B-DAT
dataset, O
and O
compare O
them O
to O

examples O
from O
our O
proposed O
balanced O
VQA B-DAT
dataset. O
Each O
question O
has O
two O

similar O
images O
for O
questions O
in O
VQA B-DAT

study O
this O
goal O
of O
balancing O
VQA B-DAT
in O
a O
fairly O
restricted O
setting O

from O
clipart O
(part O
of O
the O
VQA B-DAT
abstract O
scenes O
dataset O
[3]). O
Using O

binary O
ones), O
benchmarking O
of O
state-of-art O
VQA B-DAT
models O
on O
the O
balanced O
dataset O

, O
and O
finally O
the O
novel O
VQA B-DAT
model O
with O
counter-example O
based O
explanations O

build O
on O
top O
of O
the O
VQA B-DAT
dataset O
introduced O
by O
An- O
tol O

et O
al. O
[3]. O
VQA B-DAT
real O
images O
dataset O
contains O
just O

has O
spurred O
significant O
progress O
in O
VQA B-DAT
domain, O
as O
discussed O
earlier, O
it O

answer) O
triplet O
(I,Q,A) O
in O
the O
VQA B-DAT
dataset, O
our O
goal O
is O
to O

are O
seman- O
tically O
similar, O
a O
VQA B-DAT
model O
will O
have O
to O
understand O

questions O
from O
the O
original O
(unbalanced) O
VQA B-DAT
dataset O
[3] O
(top) O
and O
from O

all O
the O
questions O
in O
the O
VQA B-DAT
dataset. O
We O
believe O
that O
a O

and O
test O
splits O
of O
the O
VQA B-DAT
dataset. O
AMT O
workers O
picked O
“not O

question, O
image) O
pairs. O
Following O
original O
VQA B-DAT
dataset O
[3], O
we O
divide O
our O

We O
use O
the O
publicly O
released O
VQA B-DAT
evaluation O
script O
in O
our O
experiments O

for O
each O
question O
to O
compute O
VQA B-DAT
accura- O
cies. O
As O
described O
above O

to O
be O
consistent O
with O
the O
VQA B-DAT
dataset O
[3]. O
Note O
that O
while O

type O
in O
our O
new O
balanced O
VQA B-DAT
dataset O
with O
the O
original O
(un O

- O
balanced) O
VQA B-DAT
dataset O
[3]. O
We O
notice O
several O

balanced O
dataset O
compared O
to O
unbalanced O
VQA B-DAT
dataset. O
“baseball” O
is O
now O
slightly O

more O
balanced O
than O
the O
original O
VQA B-DAT
dataset. O
The O
resultant O
impact O
of O

ing O
on O
performance O
of O
state-of-the-art O
VQA B-DAT
models O
is O
dis- O
cussed O
in O

4. O
Benchmarking O
Existing O
VQA B-DAT
Models O
Our O
first O
approach O
to O

training O
a O
VQA B-DAT
model O
that O
empha O

to O
re-train O
the O
existing O
state-of-art O
VQA B-DAT
models O
(with O
code O
publicly O
available O

9]) O
on O
our O
new O
balanced O
VQA B-DAT
dataset. O
Our O
hypothesis O
is O
that O

I) O
[24]: O
This O
was O
the O
VQA B-DAT
model O
introduced O
in O
[3] O
together O

This O
is O
a O
recent O
attention-based O
VQA B-DAT
model O
that O
‘co-attends’ O
to O
both O

real O
images O
track O
of O
the O
VQA B-DAT
Challenge O
2016. O
This O
model O
uses O

utilize O
any O
visual O
information. O
Comparing O
VQA B-DAT
models O
to O
language- O
only O
ablations O

quantifies O
to O
what O
extent O
VQA B-DAT
models O
have O
succeeded O
in O
leveraging O

Table O
1: O
Performance O
of O
VQA B-DAT
models O
when O
trained/tested O
on O
unbalanced/balanced O

VQA B-DAT
datasets. O
UB O
stands O
for O
training O

see O
that O
the O
current O
state-of-art O
VQA B-DAT
models O
trained O
on O
the O
original O

unbalanced) O
VQA B-DAT
dataset O
perform O
signifi- O
cantly O
worse O

evaluating O
on O
the O
original O
unbalanced O
VQA B-DAT
dataset O
(i.e., O
comparing O
UU O
to O

in O
accuracy O
suggests O
that O
current O
VQA B-DAT
models O
are O
data O
starved, O
and O

would O
benefit O
from O
even O
larger O
VQA B-DAT
datasets O

the O
language-bias O
in O
the O
original O
VQA B-DAT
dataset, O
and O
its O
successful O
alleviation O

same O
question. O
To O
be O
successful, O
VQA B-DAT
models O
need O
to O
understand O
the O

an- O
alyze O
the O
performance O
of O
VQA B-DAT
models O
in O
unique O
ways. O
Given O

the O
prediction O
of O
a O
VQA B-DAT
model, O
we O
can O
count O
the O

training O
on O
balanced O
dataset, O
this O
VQA B-DAT
model O
has O
learned O
to O
tell O

room O
for O
improvement O
remains. O
The O
VQA B-DAT
model O
still O
can O
not O
tell O

To O
benchmark O
models O
on O
VQA B-DAT
v2.0 O
dataset, O
we O
also O
train O

these O
models O
on O
VQA B-DAT
v2.0 O
train+val O
and O
report O
re O

- O
sults O
on O
VQA B-DAT
v2.0 O
test-standard O
in O
Table O
2 O

Papers O
report- O
ing O
results O
on O
VQA B-DAT
v2.0 O
dataset O
are O
suggested O
to O

Table O
2: O
Performance O
of O
VQA B-DAT
models O
when O
trained O
on O
VQA O

v2.0 O
train+val O
and O
tested O
on O
VQA B-DAT
v2.0 O
test-standard O
dataset O

models O
when O
trained/tested O
on O
unbalanced/balanced O
VQA B-DAT
datasets. O
UB O
stands O
for O
training O

HieCoAtt). O
This O
suggests O
that O
these O
VQA B-DAT
mod- O
els O
are O
really O
exploiting O

that O
for O
both O
the O
state-of-art O
VQA B-DAT
mod- O
els, O
the O
largest O
source O

the O
results O
announced O
at O
the O
VQA B-DAT
Real O
Open O
Ended O
Challenge O
20162 O

priors O
present O
in O
the O
unbalanced O
VQA B-DAT
dataset O
(particularly O
in O
the O
“yes/no O

similar O
accuracies O
for O
all O
state-of-art O
VQA B-DAT
models, O
rendering O
vastly O
different O
models O

these O
answer-types). O
Benchmarking O
these O
different O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

color O
is O
the O
fire-hydrant?” O
a O
VQA B-DAT
model O
may O
be O
perceived O
as O

step, O
similar O
to O
a O
conventional O
VQA B-DAT
model, O
it O
takes O
in O
an O

similar O
to O
a O
conven- O
tional O
VQA B-DAT
model), O
and O
second, O
it O
explains O

VQA B-DAT
head O
in O
our O
model, O
and O

the O
most O
likely O
counter- O
example. O
VQA B-DAT
Model: O
Using O
a O
VQA O
model’s O

Random O
Distance O
VQA B-DAT
[3] O
Ours O

baseline, O
as O
well O
as O
the O
VQA B-DAT
[3] O
model. O
Interestingly, O
the O
strongest O

terface O
to O
‘balance’ O
the O
popular O
VQA B-DAT
dataset O
[3] O
by O
col- O
lecting O

bal- O
anced O
than O
the O
original O
VQA B-DAT
dataset O
by O
construction, O
but O
also O

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2.0 O

a O
number O
of O
(near) O
state-of-art O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

L. O
Zitnick, O
and O
D. O
Parikh. O
VQA B-DAT

D. O
Parikh. O
Question O
Relevance O
in O
VQA B-DAT

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2 I-DAT

To O
benchmark O
models O
on O
VQA B-DAT
v2 I-DAT

also O
train O
these O
models O
on O
VQA B-DAT
v2 I-DAT

and O
report O
re- O
sults O
on O
VQA B-DAT
v2 I-DAT

Papers O
report- O
ing O
results O
on O
VQA B-DAT
v2 I-DAT

VQA O
models O
when O
trained O
on O
VQA B-DAT
v2 I-DAT

.0 O
train+val O
and O
tested O
on O
VQA B-DAT
v2 I-DAT

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2 I-DAT

Answering O
Dataset O
and O
Challenge O
(VQA O
v2 B-DAT

To O
benchmark O
models O
on O
VQA O
v2 B-DAT

train O
these O
models O
on O
VQA O
v2 B-DAT

report O
re- O
sults O
on O
VQA O
v2 B-DAT

report- O
ing O
results O
on O
VQA O
v2 B-DAT

models O
when O
trained O
on O
VQA O
v2 B-DAT

train+val O
and O
tested O
on O
VQA O
v2 B-DAT

Answering O
Dataset O
and O
Challenge O
(VQA O
v2 B-DAT

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2 I-DAT

To O
benchmark O
models O
on O
VQA B-DAT
v2 I-DAT

also O
train O
these O
models O
on O
VQA B-DAT
v2 I-DAT

and O
report O
re- O
sults O
on O
VQA B-DAT
v2 I-DAT

Papers O
report- O
ing O
results O
on O
VQA B-DAT
v2 I-DAT

VQA O
models O
when O
trained O
on O
VQA B-DAT
v2 I-DAT

.0 O
train+val O
and O
tested O
on O
VQA B-DAT
v2 I-DAT

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2 I-DAT

Making O
the O
V O
in O
VQA B-DAT
Matter: O
Elevating O
the O
Role O
of O

task O
of O
Visual O
Question O
Answering O
(VQA) B-DAT
and O
make O
vision O
(the O
V O

in O
VQA) B-DAT
matter! O
Specifically, O
we O
balance O
the O

popular O
VQA B-DAT
dataset O
[3] O
by O
collecting O
complementary O

balanced O
than O
the O
origi- O
nal O
VQA B-DAT
dataset O
and O
has O
approximately O
twice O

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2.0 O

benchmark O
a O
number O
of O
state-of-art O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

1: O
Examples O
from O
our O
balanced O
VQA B-DAT
dataset O

28] O
and O
visual O
question O
answering O
(VQA) B-DAT
[3, O
26, O
27, O
10, O
31 O

1]. O
For O
instance, O
in O
the O
VQA B-DAT
[3] O
dataset, O
the O
most O
com O

visual O
priming O
bias’ O
in O
the O
VQA B-DAT
dataset O
– O
specifi- O
cally, O
subjects O

in O
the O
VQA B-DAT
dataset O
starting O
with O
the O
n-gram O

associated O
image O
results O
in O
a O
VQA B-DAT
accuracy O
of O
87 O

role O
of O
image O
understanding O
in O
VQA B-DAT

goal, O
we O
collect O
a O
balanced O
VQA B-DAT
dataset O
with O
significantly O
reduced O
language O

ically, O
we O
create O
a O
balanced O
VQA B-DAT
dataset O
in O
the O
following O
way O

answer) O
triplet O
(I,Q,A) O
from O
the O
VQA B-DAT
dataset, O
we O
ask O
a O
human O

this O
balanced O
dataset O
will O
force O
VQA B-DAT
models O
to O
focus O
on O
visual O

exploiting O
language O
priors, O
en- O
abling O
VQA B-DAT
evaluation O
protocols O
to O
more O
accurately O

Our O
balanced O
VQA B-DAT
dataset O
is O
also O
particularly O
difficult O

of O
VGGNet O
[37] O
features. O
Therefore, O
VQA B-DAT
models O
will O
need O
to O
under O

double O
the O
size O
of O
the O
VQA B-DAT
[3] O
dataset O
– O
with O
approximately O

23]. O
We O
believe O
this O
balanced O
VQA B-DAT
dataset O
is O
a O
better O
dataset O

to O
benchmark O
VQA B-DAT
approaches, O
and O
is O
publicly O
available O

will O
allow O
users O
of O
the O
VQA B-DAT
model O
to O
establish O
greater O
trust O

1) O
We O
balance O
the O
existing O
VQA B-DAT
dataset O
[3] O
by O
collecting O
complementary O

result O
is O
a O
more O
balanced O
VQA B-DAT
dataset, O
which O
is O
also O
approximately O

the O
size O
of O
the O
original O
VQA B-DAT
dataset. O
(2) O
We O
evaluate O
state-of-art O

VQA B-DAT
models O
(with O
publicly O
available O
code O

trained O
on O
the O
existing O
‘unbalanced’ O
VQA B-DAT
dataset O
perform O
poorly O
on O
our O

language O
priors O
in O
the O
existing O
VQA B-DAT
dataset O
to O
achieve O
higher O
accuracy O

builds O
on O
top O
of O
the O
VQA B-DAT
dataset O
from O
Antol O
et O
al O

of O
the O
most O
widely O
used O
VQA B-DAT
datasets. O
We O
reduce O
the O
language O

twice O
the O
size O
of O
the O
VQA B-DAT
dataset. O
We O
benchmark O
one O
‘baseline O

’ O
VQA B-DAT
model O
[24], O
one O
attention-based O
VQA O

the O
winning O
model O
from O
the O
VQA B-DAT
Real O
Open O
Ended O
Chal- O
lenge O

2016 O
[9] O
on O
our O
balanced O
VQA B-DAT
dataset, O
and O
compare O
them O
to O

examples O
from O
our O
proposed O
balanced O
VQA B-DAT
dataset. O
Each O
question O
has O
two O

similar O
images O
for O
questions O
in O
VQA B-DAT

study O
this O
goal O
of O
balancing O
VQA B-DAT
in O
a O
fairly O
restricted O
setting O

from O
clipart O
(part O
of O
the O
VQA B-DAT
abstract O
scenes O
dataset O
[3]). O
Using O

binary O
ones), O
benchmarking O
of O
state-of-art O
VQA B-DAT
models O
on O
the O
balanced O
dataset O

, O
and O
finally O
the O
novel O
VQA B-DAT
model O
with O
counter-example O
based O
explanations O

build O
on O
top O
of O
the O
VQA B-DAT
dataset O
introduced O
by O
An- O
tol O

et O
al. O
[3]. O
VQA B-DAT
real O
images O
dataset O
contains O
just O

has O
spurred O
significant O
progress O
in O
VQA B-DAT
domain, O
as O
discussed O
earlier, O
it O

answer) O
triplet O
(I,Q,A) O
in O
the O
VQA B-DAT
dataset, O
our O
goal O
is O
to O

are O
seman- O
tically O
similar, O
a O
VQA B-DAT
model O
will O
have O
to O
understand O

questions O
from O
the O
original O
(unbalanced) O
VQA B-DAT
dataset O
[3] O
(top) O
and O
from O

all O
the O
questions O
in O
the O
VQA B-DAT
dataset. O
We O
believe O
that O
a O

and O
test O
splits O
of O
the O
VQA B-DAT
dataset. O
AMT O
workers O
picked O
“not O

question, O
image) O
pairs. O
Following O
original O
VQA B-DAT
dataset O
[3], O
we O
divide O
our O

We O
use O
the O
publicly O
released O
VQA B-DAT
evaluation O
script O
in O
our O
experiments O

for O
each O
question O
to O
compute O
VQA B-DAT
accura- O
cies. O
As O
described O
above O

to O
be O
consistent O
with O
the O
VQA B-DAT
dataset O
[3]. O
Note O
that O
while O

type O
in O
our O
new O
balanced O
VQA B-DAT
dataset O
with O
the O
original O
(un O

- O
balanced) O
VQA B-DAT
dataset O
[3]. O
We O
notice O
several O

balanced O
dataset O
compared O
to O
unbalanced O
VQA B-DAT
dataset. O
“baseball” O
is O
now O
slightly O

more O
balanced O
than O
the O
original O
VQA B-DAT
dataset. O
The O
resultant O
impact O
of O

ing O
on O
performance O
of O
state-of-the-art O
VQA B-DAT
models O
is O
dis- O
cussed O
in O

4. O
Benchmarking O
Existing O
VQA B-DAT
Models O
Our O
first O
approach O
to O

training O
a O
VQA B-DAT
model O
that O
empha O

to O
re-train O
the O
existing O
state-of-art O
VQA B-DAT
models O
(with O
code O
publicly O
available O

9]) O
on O
our O
new O
balanced O
VQA B-DAT
dataset. O
Our O
hypothesis O
is O
that O

I) O
[24]: O
This O
was O
the O
VQA B-DAT
model O
introduced O
in O
[3] O
together O

This O
is O
a O
recent O
attention-based O
VQA B-DAT
model O
that O
‘co-attends’ O
to O
both O

real O
images O
track O
of O
the O
VQA B-DAT
Challenge O
2016. O
This O
model O
uses O

utilize O
any O
visual O
information. O
Comparing O
VQA B-DAT
models O
to O
language- O
only O
ablations O

quantifies O
to O
what O
extent O
VQA B-DAT
models O
have O
succeeded O
in O
leveraging O

Table O
1: O
Performance O
of O
VQA B-DAT
models O
when O
trained/tested O
on O
unbalanced/balanced O

VQA B-DAT
datasets. O
UB O
stands O
for O
training O

see O
that O
the O
current O
state-of-art O
VQA B-DAT
models O
trained O
on O
the O
original O

unbalanced) O
VQA B-DAT
dataset O
perform O
signifi- O
cantly O
worse O

evaluating O
on O
the O
original O
unbalanced O
VQA B-DAT
dataset O
(i.e., O
comparing O
UU O
to O

in O
accuracy O
suggests O
that O
current O
VQA B-DAT
models O
are O
data O
starved, O
and O

would O
benefit O
from O
even O
larger O
VQA B-DAT
datasets O

the O
language-bias O
in O
the O
original O
VQA B-DAT
dataset, O
and O
its O
successful O
alleviation O

same O
question. O
To O
be O
successful, O
VQA B-DAT
models O
need O
to O
understand O
the O

an- O
alyze O
the O
performance O
of O
VQA B-DAT
models O
in O
unique O
ways. O
Given O

the O
prediction O
of O
a O
VQA B-DAT
model, O
we O
can O
count O
the O

training O
on O
balanced O
dataset, O
this O
VQA B-DAT
model O
has O
learned O
to O
tell O

room O
for O
improvement O
remains. O
The O
VQA B-DAT
model O
still O
can O
not O
tell O

To O
benchmark O
models O
on O
VQA B-DAT
v2.0 O
dataset, O
we O
also O
train O

these O
models O
on O
VQA B-DAT
v2.0 O
train+val O
and O
report O
re O

- O
sults O
on O
VQA B-DAT
v2.0 O
test-standard O
in O
Table O
2 O

Papers O
report- O
ing O
results O
on O
VQA B-DAT
v2.0 O
dataset O
are O
suggested O
to O

Table O
2: O
Performance O
of O
VQA B-DAT
models O
when O
trained O
on O
VQA O

v2.0 O
train+val O
and O
tested O
on O
VQA B-DAT
v2.0 O
test-standard O
dataset O

models O
when O
trained/tested O
on O
unbalanced/balanced O
VQA B-DAT
datasets. O
UB O
stands O
for O
training O

HieCoAtt). O
This O
suggests O
that O
these O
VQA B-DAT
mod- O
els O
are O
really O
exploiting O

that O
for O
both O
the O
state-of-art O
VQA B-DAT
mod- O
els, O
the O
largest O
source O

the O
results O
announced O
at O
the O
VQA B-DAT
Real O
Open O
Ended O
Challenge O
20162 O

priors O
present O
in O
the O
unbalanced O
VQA B-DAT
dataset O
(particularly O
in O
the O
“yes/no O

similar O
accuracies O
for O
all O
state-of-art O
VQA B-DAT
models, O
rendering O
vastly O
different O
models O

these O
answer-types). O
Benchmarking O
these O
different O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

color O
is O
the O
fire-hydrant?” O
a O
VQA B-DAT
model O
may O
be O
perceived O
as O

step, O
similar O
to O
a O
conventional O
VQA B-DAT
model, O
it O
takes O
in O
an O

similar O
to O
a O
conven- O
tional O
VQA B-DAT
model), O
and O
second, O
it O
explains O

VQA B-DAT
head O
in O
our O
model, O
and O

the O
most O
likely O
counter- O
example. O
VQA B-DAT
Model: O
Using O
a O
VQA O
model’s O

Random O
Distance O
VQA B-DAT
[3] O
Ours O

baseline, O
as O
well O
as O
the O
VQA B-DAT
[3] O
model. O
Interestingly, O
the O
strongest O

terface O
to O
‘balance’ O
the O
popular O
VQA B-DAT
dataset O
[3] O
by O
col- O
lecting O

bal- O
anced O
than O
the O
original O
VQA B-DAT
dataset O
by O
construction, O
but O
also O

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2.0 O

a O
number O
of O
(near) O
state-of-art O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

L. O
Zitnick, O
and O
D. O
Parikh. O
VQA B-DAT

D. O
Parikh. O
Question O
Relevance O
in O
VQA B-DAT

truth O
score O
s(q, O
a) O
= O
1.0 B-DAT
if O
the O
answer O
a O
was O

1 B-DAT
0 I-DAT

1509 B-DAT

1606 B-DAT

1606 B-DAT

1602 B-DAT

1606 B-DAT

1606 B-DAT

1607 B-DAT

epochs O
on O
the O
“abstract O
scenes”, O
100 B-DAT
epochs O
on O
the O
“balanced” O
dataset O

abstract B-DAT
scenes” O
multiple-choice O
benchmark, O
and O
from O

abstract B-DAT
scenes”, O
de- O
spite O
this O
being O

abstract B-DAT
scenes” O
benchmark O
[4] O
and O
demonstrate O

abstract B-DAT
scenes” O
dataset. O
The O
candidate O
answers O

abstract B-DAT
scenes” O
from O
Antol O
et O
al O

abstract B-DAT
scenes” O
(left: O
multiple O
choice, O
middle O

abstract B-DAT
scenes” O
dataset O
(left O
and O
middle O

abstract B-DAT
scenes” O
dataset O
(Fig. O
3, O
left O

abstract B-DAT
scenes” O
test O
set O
is O
not O

abstract B-DAT
scenes” O
dataset O

abstract B-DAT
scenes” O
dataset O
in O
Table O
2 O

abstract B-DAT
scenes” O
dataset O
(average O
scores O
in O

abstract B-DAT
scenes” O
dataset O
(top O
row) O
and O

abstract B-DAT
scenes”, O
100 O
epochs O
on O
the O

abstract B-DAT
scenes” O
dataset, O
and O
accuracy O
over O

we O
choose O
to O
focus O
on O
abstract B-DAT
scenes O
? O
Does O
this O
method O

The O
balanced O
dataset O
of O
abstract B-DAT
scenes O
was O
the O
only O
one O

the O
scene O
descriptions O
(provided O
with O
abstract B-DAT
scenes) O
as O
the O
output O
of O

C.1. O
Additional O
results: O
abstract B-DAT
scenes O
dataset O

improve O
visual O
question O
answer- O
ing O
(VQA) B-DAT
with O
structured O
representations O
of O
both O

questions. O
A O
key O
challenge O
in O
VQA B-DAT
is O
to O
require O
joint O
reasoning O

pre- O
dominant O
CNN/LSTM-based O
approach O
to O
VQA B-DAT
is O
limited O
by O
monolithic O
vector O

Multiple O
datasets O
for O
VQA B-DAT
have O
been O
introduced O
with O
either O

perfor- O
mance. O
A O
particularly O
attractive O
VQA B-DAT
dataset O
was O
intro- O
duced O
in O

This O
strongly O
contrasts O
with O
other O
VQA B-DAT
datasets O
of O
real O
images, O
where O

the O
greater O
goal O
of O
general O
VQA B-DAT

additional O
challenge, O
which O
affects O
all O
VQA B-DAT
datasets, O
is O
the O
sparsity O
of O

this O
challenge, O
most O
methods O
for O
VQA B-DAT
process O
the O
question O
through O
a O

and O
question O
for O
VQA, B-DAT
and O
a O
neural O
network O
capable O

this O
information O
accessible O
to O
the O
VQA B-DAT
model. O
This O
rep- O
resentation O
uses O

the O
proposed O
model O
on O
the O
VQA B-DAT
“abstract O
scenes” O
benchmark O
[4] O
and O

time O
on O
the O
task O
of O
VQA B-DAT
– O
precision/recall O
curves O
of O
predicted O

answer. O
Most O
recent O
papers O
on O
VQA B-DAT
propose O
im- O
provements O
and O
variations O

memory O
networks O
(DMN), O
applied O
to O
VQA B-DAT
in O
[27] O
also O
maintain O
a O

Most O
VQA B-DAT
systems O
are O
trained O
end-to-end O
from O

pairs. O
This O
contrasts O
with O
other O
VQA B-DAT
datasets O
where O
blind O
guessing O
can O

VQA B-DAT
score” O
[4], O
which O
is O
a O

its O
predicted O
answers. O
Most O
existing O
VQA B-DAT
methods O
treat O
the O
answering O
as O

Graph O
VQA B-DAT
(full O
model) O
74.94 O
39.1 O

It O
is O
unclear O
whether O
huge O
VQA B-DAT
datasets O
could O
ultimately O
negate O
this O

with O
LSTMs. O
In O
our O
opinion, O
VQA B-DAT
systems O
are O
unlikely O
to O
learn O

without O
rest- O
ing O
entirely O
on O
VQA B-DAT

69.73 O
80.70 O
62.08 O
58.82 O
Graph O
VQA B-DAT
(full O
model) O
74.37 O
79.74 O
68.31 O

References O
[1] O
VQA B-DAT
Challenge O
leaderboard. O
http://visualqa.org O

L. O
Zitnick, O
and O
D. O
Parikh. O
VQA B-DAT

the O
validation O
set O
(measured O
by O
VQA B-DAT
score O
on O
the O
“abstract O
scenes O

its O
an- O
swers. O
A O
practical O
VQA B-DAT
system O
will O
need O
to O
provide O

choice B-DAT
benchmark, O
and O
from O
34.7% O
to O

choice B-DAT
variant, O
an O
answer O
is O
selected O

choice B-DAT
setting. O
On O
the O
“balanced” O
version O

the O
“abstract O
scenes” O
(left: O
multiple O
choice, B-DAT
middle: O
open-ended) O
and O
“balanced” O
datasets O

both O
the O
open-ended O
and O
multiple O
choice B-DAT
settings. O
The O
advantage O
over O
existing O

Multiple O
choice B-DAT
Open-ended O
Method O
Overall O
Yes/no O
Other O

choice B-DAT
(M.C.) O
setting O
should O
be O
easier O

Graph-Structured O
Representations O
for O
Visual O
Question B-DAT
Answering O

Introduction O
The O
task O
of O
Visual O
Question B-DAT
Answering O
has O
received O

1) O
Question B-DAT

2) O
Question B-DAT

and O
D. O
Parikh. O
VQA: O
Visual O
Question B-DAT
Answering. O
In O
Proc. O
IEEE O
Int O

Neu- O
ral O
Network O
for O
Visual O
Question B-DAT
Answering. O
arXiv O
preprint O
arXiv:1511.05960, O
2015 O

Li. O
Compositional O
Memory O
for O
Visual O
Question B-DAT
Answering. O
arXiv O
preprint O
arXiv:1511.05676, O
2015 O

Kiros, O
and O
R. O
Zemel. O
Image O
Question B-DAT
Answering: O
A O
Visual O
Semantic O
Embedding O

A. O
van O
den O
Hengel. O
Visual O
Question B-DAT
Answering: O
A O
Survey O
of O
Methods O

Attend O
and O
Answer: O
Explor- O
ing O
Question B-DAT

-Guided O
Spatial O
Attention O
for O
Visual O
Question B-DAT
Answering. O
arXiv O
preprint O
arXiv:1511.05234, O
2015 O

Stacked O
Attention O
Networks O
for O
Image O
Question B-DAT
Answering. O
In O
Proc. O
IEEE O
Conf O

and O
L. O
Fei-Fei. O
Visual7W: O
Grounded O
Question B-DAT
Answering O
in O
Images. O
In O
Proc O

Graph-Structured O
Representations O
for O
Visual B-DAT
Question O
Answering O

den O
Hengel O
Australian O
Centre O
for O
Visual B-DAT
Technologies O

1. O
Introduction O
The O
task O
of O
Visual B-DAT
Question O
Answering O
has O
received O

Zitnick, O
and O
D. O
Parikh. O
VQA: O
Visual B-DAT
Question O
Answering. O
In O
Proc. O
IEEE O

Convolutional O
Neu- O
ral O
Network O
for O
Visual B-DAT
Question O
Answering. O
arXiv O
preprint O
arXiv:1511.05960 O

Y. O
Li. O
Compositional O
Memory O
for O
Visual B-DAT
Question O
Answering. O
arXiv O
preprint O
arXiv:1511.05676 O

Bern- O
stein, O
and O
L. O
Fei-Fei. O
Visual B-DAT
genome: O
Connecting O
language O
and O
vision O

Zemel. O
Image O
Question O
Answering: O
A O
Visual B-DAT
Semantic O
Embedding O
Model O
and O
a O

and O
A. O
van O
den O
Hengel. O
Visual B-DAT
Question O
Answering: O
A O
Survey O
of O

ing O
Question-Guided O
Spatial O
Attention O
for O
Visual B-DAT
Question O
Answering. O
arXiv O
preprint O
arXiv:1511.05234 O

capture O
situations O
as O
simple O
as O
multiple B-DAT
object O
instances, O
and O
LSTMs O
process O

74.4% O
on O
the O
“abstract O
scenes” O
multiple B-DAT

a O
short O
phrase. O
In O
the O
multiple B-DAT

good O
?. O
Others O
require O
relating O
multiple B-DAT
facts O
or O
understanding O
complex O
actions O

edge O
types. O
Our O
network O
uses O
multiple B-DAT
layers O
that O
iterate O
over O
the O

71.2% O
to O
74.4% O
in O
the O
multiple B-DAT

node O
of O
both O
graphs. O
Over O
multiple B-DAT
iterations, O
the O
GRU O
updates O
a O

into O
each O
node O
feature O
over O
multiple B-DAT
iterations, O
but O
we O
did O
not O

of O
ground O
truth O
answers O
from O
multiple B-DAT
human O
annotators. O
Let O
us O
refer O

on O
the O
“abstract O
scenes” O
(left: O
multiple B-DAT
choice, O
middle: O
open-ended) O
and O
“balanced O

make O
better O
use O
of O
the O
multiple B-DAT
human-provided O
answers, O
we O
propose O
to O

scenes, O
and O
when O
synonyms O
constitute O
multiple B-DAT
accept- O
able O
answers. O
In O
those O

in O
both O
the O
open-ended O
and O
multiple B-DAT
choice O
settings. O
The O
advantage O
over O

The O
multiple B-DAT

Graph-Structured O
Representations O
for O
Visual O
Question O
Answering B-DAT

The O
task O
of O
Visual O
Question O
Answering B-DAT
has O
received O

D. O
Parikh. O
VQA: O
Visual O
Question O
Answering B-DAT

ral O
Network O
for O
Visual O
Question O
Answering B-DAT

Compositional O
Memory O
for O
Visual O
Question O
Answering B-DAT

and O
R. O
Zemel. O
Image O
Question O
Answering B-DAT

van O
den O
Hengel. O
Visual O
Question O
Answering B-DAT

Spatial O
Attention O
for O
Visual O
Question O
Answering B-DAT

Attention O
Networks O
for O
Image O
Question O
Answering B-DAT

L. O
Fei-Fei. O
Visual7W: O
Grounded O
Question O
Answering B-DAT
in O
Images. O
In O
Proc. O
IEEE O

improve O
visual O
question O
answer- O
ing O
(VQA) B-DAT
with O
structured O
representations O
of O
both O

questions. O
A O
key O
challenge O
in O
VQA B-DAT
is O
to O
require O
joint O
reasoning O

pre- O
dominant O
CNN/LSTM-based O
approach O
to O
VQA B-DAT
is O
limited O
by O
monolithic O
vector O

Multiple O
datasets O
for O
VQA B-DAT
have O
been O
introduced O
with O
either O

perfor- O
mance. O
A O
particularly O
attractive O
VQA B-DAT
dataset O
was O
intro- O
duced O
in O

This O
strongly O
contrasts O
with O
other O
VQA B-DAT
datasets O
of O
real O
images, O
where O

the O
greater O
goal O
of O
general O
VQA B-DAT

additional O
challenge, O
which O
affects O
all O
VQA B-DAT
datasets, O
is O
the O
sparsity O
of O

this O
challenge, O
most O
methods O
for O
VQA B-DAT
process O
the O
question O
through O
a O

and O
question O
for O
VQA, B-DAT
and O
a O
neural O
network O
capable O

this O
information O
accessible O
to O
the O
VQA B-DAT
model. O
This O
rep- O
resentation O
uses O

the O
proposed O
model O
on O
the O
VQA B-DAT
“abstract O
scenes” O
benchmark O
[4] O
and O

time O
on O
the O
task O
of O
VQA B-DAT
– O
precision/recall O
curves O
of O
predicted O

answer. O
Most O
recent O
papers O
on O
VQA B-DAT
propose O
im- O
provements O
and O
variations O

memory O
networks O
(DMN), O
applied O
to O
VQA B-DAT
in O
[27] O
also O
maintain O
a O

Most O
VQA B-DAT
systems O
are O
trained O
end-to-end O
from O

pairs. O
This O
contrasts O
with O
other O
VQA B-DAT
datasets O
where O
blind O
guessing O
can O

VQA B-DAT
score” O
[4], O
which O
is O
a O

its O
predicted O
answers. O
Most O
existing O
VQA B-DAT
methods O
treat O
the O
answering O
as O

Graph O
VQA B-DAT
(full O
model) O
74.94 O
39.1 O

It O
is O
unclear O
whether O
huge O
VQA B-DAT
datasets O
could O
ultimately O
negate O
this O

with O
LSTMs. O
In O
our O
opinion, O
VQA B-DAT
systems O
are O
unlikely O
to O
learn O

without O
rest- O
ing O
entirely O
on O
VQA B-DAT

69.73 O
80.70 O
62.08 O
58.82 O
Graph O
VQA B-DAT
(full O
model) O
74.37 O
79.74 O
68.31 O

References O
[1] O
VQA B-DAT
Challenge O
leaderboard. O
http://visualqa.org O

L. O
Zitnick, O
and O
D. O
Parikh. O
VQA B-DAT

the O
validation O
set O
(measured O
by O
VQA B-DAT
score O
on O
the O
“abstract O
scenes O

its O
an- O
swers. O
A O
practical O
VQA B-DAT
system O
will O
need O
to O
provide O

scenes. O
The O
set O
contains O
over O
100 B-DAT
objects O
and O
31 O
animals O
in O

i.e., O
an O
answer O
is O
deemed O
100 B-DAT

MS O
COCO O
dataset O
[32] O
and O
150, B-DAT

a O
scale O
of O
0 O
− O
100) B-DAT
required O
to O
answer O
a O
question O

choose O
the O
top O
K O
= O
1000 B-DAT
most O
frequent O
answers O
as O
possible O

Question O
(BoW O
Q): O
The O
top O
1,000 B-DAT
words O
in O
the O
questions O
are O

are O
concatenated O
to O
get O
a O
1,030 B-DAT

1000 B-DAT

1000 B-DAT

with O
2 O
hidden O
layers O
and O
1000 B-DAT
hidden O
units O
(dropout O
0.5) O
in O

a O
bag-of-words O
representation O
containing O
the O
1,000 B-DAT
most O
popular O
words O
in O
the O

that O
it O
has O
VQA O
accuracy O
1.0 B-DAT
(see O
section O
3 O
for O
accuracy O

Question O
K O
= O
1000 B-DAT
Human O
To O
Be O
Able O
To O

100 B-DAT
A O

100 B-DAT

100 B-DAT

100 B-DAT

can O
see O
that O
K O
= O
1000 B-DAT
performs O
better O
than O
K O

performs O
better O
then O
K O
= O
1000 B-DAT
by O
0.40% O
for O
open-ended O
task O

blue” O
(28881, O
1.16%), O
“4” O
(27174, O
1.09 B-DAT

outside” O
(1846, O
0.07%), O
“hot O
dog” O
(1809, B-DAT
0.07%), O
“night” O
(1805, O
0.07%), O
“trees O

and O
white” O
(1518, O
0.06%), O
“bedroom” O
(1500, B-DAT
0.06%), O
“bat” O
(1494, O
0.06%), O
“glasses O

0.06%), O
“cloudy” O
(1413, O
0.06%), O
“15” O
(1407, B-DAT
0.06%), O
“up” O
(1399, O
0.06%), O
“blonde O

0.05%), O
“many” O
(1211, O
0.05%), O
“zoo” O
(1204, B-DAT
0.05%), O
“suitcase” O
(1199, O
0.05%), O
“old O

0.04%), O
“mountains” O
(1030, O
0.04%), O
“wall” O
(1009, B-DAT
0.04%), O
“ele- O
phants” O
(1006, O
0.04 O

1504 B-DAT

1504 B-DAT

1408 B-DAT

1506 B-DAT

1409 B-DAT

1502 B-DAT

32] O
and O
a O
newly O
created O
abstract B-DAT
scene O
dataset O
[57], O
[2] O
that O

a O
new O
dataset O
of O
“realistic” O
abstract B-DAT
scenes O
to O
enable O
research O
focused O

output O
answer O
classes. O
[34] O
generates O
abstract B-DAT
scenes O
to O
capture O
visual O
common O

describing O
the O
real O
images O
and O
abstract B-DAT

tasks, O
we O
create O
a O
new O
abstract B-DAT
scenes O
dataset O
[2], O
[57], O
[58 O

winners O
of O
the O
challenge. O
For O
abstract B-DAT
scenes, O
we O
created O
splits O
for O

test-dev, O
test-standard, O
test-challenge, O
test-reserve) O
for O
abstract B-DAT
scenes. O
Captions. O
The O
MS O
COCO O

collected O
five O
single-captions O
for O
all O
abstract B-DAT
scenes O
using O
the O
same O
user O

both O
the O
real O
images O
and O
abstract B-DAT
scenes. O
In O
total, O
three O
questions O

left) O
and O
all O
questions O
for O
abstract B-DAT
scenes O
(right). O
The O
ordering O
of O

1,950,000 O
answers O
for O
50, O
000 O
abstract B-DAT
scenes O

the O
real O
images O
(left) O
and O
abstract B-DAT
scenes O
(right). O
Interestingly, O
the O
distribution O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
This O
helps O
demonstrate O
that O

of O
questions O
elicited O
by O
the O
abstract B-DAT
scenes O
is O
similar O
to O
those O

lengths O
for O
real O
images O
and O
abstract B-DAT
scenes O

90.51%, O
5.89%, O
and O
2.49% O
for O
abstract B-DAT
scenes. O
The O
brevity O
of O
answers O

real O
images O
and O
3,770 O
for O
abstract B-DAT
scenes. O
‘Yes/No’ O
and O
‘Number’ O
Answers O

questions O
on O
real O
images O
and O
abstract B-DAT
scenes O
respectively. O
Among O
these O
‘yes/no O

yes” O
for O
real O
images O
and O
abstract B-DAT
scenes. O
Question O
types O
such O
as O

questions O
on O
real O
images O
and O
abstract B-DAT
scenes O
are O
‘number’ O
questions. O
“2 O

real O
images O
and O
39.85% O
for O
abstract B-DAT
scenes. O
Subject O
Confidence. O
When O
the O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
Inter-human O
Agreement. O
Does O
the O

confident) O
for O
real O
images O
and O
abstract B-DAT
scenes O
(black O
lines). O
Percentage O
of O

both O
real O
images O
(83.30%) O
and O
abstract B-DAT
scenes O
(87.49%). O
Note O
that O
on O

real O
images O
and O
2.39 O
for O
abstract B-DAT
scenes. O
The O
agreement O
is O
significantly O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
See O
the O
appendix O
for O

abstract B-DAT
| O
mc-real O
| O
mc-abstract O

oe-abstract-leaderboard B-DAT
| O
mc-real-leaderboard O
| O
mc- O
abstract O

VI O
- O
Details O
on O
the O
abstract B-DAT
scene O
dataset O
VII O
- O
User O

captions O
collected O
by O
us O
for O
abstract B-DAT
scenes) O
using O
the O
Stanford O
part-of-speech O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
This O
helps O
motivate O
the O

and O
Fig. O
20 O
(adjectives) O
for O
abstract B-DAT
scenes.7 O
The O
left O
side O
shows O

and O
Fig. O
14 O
(right) O
for O
abstract B-DAT
scenes. O
We O
see O
that O
questions O

for O
real O
images O
(left) O
and O
abstract B-DAT
scenes O
(right O

indicating O
the O
normalized O
count O
for O
abstract B-DAT
scenes O

indicating O
the O
normalized O
count O
for O
abstract B-DAT
scenes O

indicating O
the O
normalized O
count O
for O
abstract B-DAT
scenes O

the O
two O
datasets, O
real O
and O
abstract, B-DAT
first O
two O
rows O
are O
the O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
Note O
the O
diversity O
of O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
In O
Table O
6, O
we O

and O
≈ O
11% O
increase O
for O
abstract B-DAT
scenes) O
for O
“other” O
questions. O
Since O

that O
are O
present O
in O
the O
abstract B-DAT
scenes O
dataset. O
For O
more O
examples O

left) O
and O
all O
questions O
for O
abstract B-DAT
scenes O
(right). O
The O
ordering O
of O

top) O
and O
all O
questions O
for O
abstract B-DAT
scenes O
(bottom). O
Each O
column O
corresponds O

the O
objects O
present O
in O
the O
abstract B-DAT
scene O
dataset. O
Right: O
The O
AMT O

interface O
for O
collecting O
abstract B-DAT
scenes. O
The O
light O
green O
circles O

the O
MS O
COCO O
[32] O
images, O
abstract B-DAT
scenes, O
and O
multiple-choice O
questions, O
respectively O

numerous O
representative O
examples O
of O
the O
abstract B-DAT
scene O
dataset O

examples O
of O
the O
real O
and O
abstract B-DAT
scene O
dataset O

VQA B-DAT

and O
open-ended O
Visual O
Question O
Answering O
(VQA B-DAT

a O
system O
that O
succeeds O
at O
VQA B-DAT
typically O
needs O
a O
more O
detailed O

producing O
generic O
image O
captions. O
Moreover, O
VQA B-DAT
is O
amenable O
to O
automatic O
evaluation O

Numerous O
baselines O
and O
methods O
for O
VQA B-DAT
are O
provided O
and O
compared O
with O

human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

open- O
ended O
Visual O
Question O
Answering O
(VQA B-DAT

). O
A O
VQA B-DAT
system O
takes O
as O
input O
an O

Is O
this O
person O
expecting O
company?”). O
VQA B-DAT
[19], O
[36], O
[50], O
[3] O
is O

the O
high-level O
reasoning O
required O
for O
VQA B-DAT
by O
removing O
the O
need O
to O

29]. O
As O
part O
of O
the O
VQA B-DAT
initiative, O
we O
will O
organize O
an O

state-of-the-art O
methods O
and O
best O
practices. O
VQA B-DAT
poses O
a O
rich O
set O
of O

during O
the O
past O
few O
decades. O
VQA B-DAT
provides O
an O
attractive O
balance O
between O

VQA B-DAT
Efforts. O
Several O
recent O
papers O
have O

difficult O
and O
unconstrained O
task, O
our O
VQA B-DAT
dataset O
is O
two O
orders O
of O

1,449 O
images O
respectively). O
The O
proposed O
VQA B-DAT
task O
has O
connections O
to O
other O

These O
approaches O
provide O
inspiration O
for O
VQA B-DAT
techniques. O
One O
key O
concern O
in O

fixed O
set O
of O
loca- O
tions. O
VQA B-DAT
is O
naturally O
grounded O
in O
images O

Describing O
Visual O
Content. O
Related O
to O
VQA B-DAT
are O
the O
tasks O
of O
image O

by O
[53]). O
The O
questions O
in O
VQA B-DAT
require O
detailed O
specific O
information O
about O

3 O
VQA B-DAT
DATASET O
COLLECTION O

describe O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset. O
We O
begin O
by O
describing O

they O
are O
well-suited O
for O
our O
VQA B-DAT
task. O
The O
more O
diverse O
our O

their O
answers. O
Abstract O
Scenes. O
The O
VQA B-DAT
task O
with O
real O
images O
requires O

the O
high-level O
reasoning O
required O
for O
VQA, B-DAT
but O
not O
the O
low-level O
vision O

test-standard, O
test-challenge, O
test-reserve). O
For O
the O
VQA B-DAT
challenge O
(see O
section O
6), O
test-dev O

default’ O
test O
data O
for O
the O
VQA B-DAT
competition. O
When O
comparing O
to O
the O

sentences O
containing O
multiple O
words. O
In O
VQA, B-DAT
most O
answers O
(89.32%) O
are O
single O

4 O
VQA B-DAT
DATASET O
ANALYSIS O
In O
this O
section O

questions O
and O
answers O
in O
the O
VQA B-DAT
train O
dataset. O
To O
gain O
an O

visual O
information O
is O
critical O
to O
VQA B-DAT
and O
that O
commonsense O
information O
alone O

from O
the O
real O
images O
of O
VQA B-DAT
trainval) O
asking O
subjects O

5 O
VQA B-DAT
BASELINES O
AND O
METHODS O
In O
this O

explore O
the O
difficulty O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

novel O
methods. O
We O
train O
on O
VQA B-DAT
train+val. O
Unless O
stated O
otherwise, O
all O

top O
1K O
answers O
of O
the O
VQA B-DAT
train/val O
dataset O

multiple- O
choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
As O

I O
(Fig. O
8), O
selected O
using O
VQA B-DAT
test-dev O
accuracies) O
on O
VQA O
test O

worse O
than O
human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

system O
is O
correct O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

correct” O
implies O
that O
it O
has O
VQA B-DAT
accuracy O
1.0 O
(see O
section O
3 O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
The O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

frequently O
predicted O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

age O
of O
question) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

system O
is O
correct) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

a O
filtered O
version O
of O
the O
VQA B-DAT
train O
+ O
val O
dataset O
in O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

6 O
VQA B-DAT
CHALLENGE O
AND O
WORKSHOP O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O

task O
of O
Visual O
Question O
Answering O
(VQA B-DAT

multiple-choice O
tasks O
in O
the O
respective O
VQA B-DAT
Real O
Image O
Challenge O
leaderboards O
(as O

datasets O
may O
help O
enable O
practical O
VQA B-DAT
applications. O
We O
believe O
VQA O
has O

questions O
IV O
- O
Details O
on O
VQA B-DAT
baselines O
V O
- O
“Age” O
and O

Leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Open-Ended) O
on O

leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Multiple-Choice) O
on O

Additional O
examples O
from O
the O
VQA B-DAT
dataset O

scenes. O
This O
helps O
motivate O
the O
VQA B-DAT
task O
as O
a O
way O
to O

APPENDIX O
IV: O
DETAILS O
ON O
VQA B-DAT
BASELINES O
“per O
Q-type O
prior” O
baseline O

For O
every O
question O
in O
the O
VQA B-DAT
test-standard O
set, O
we O
find O
its O

norm O
I), O
selected O
using O
VQA B-DAT
test- O
dev O
accuracies). O
To O
estimate O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

a O
random O
selection O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

choice B-DAT
format. O
We O
provide O
a O
dataset O

answering O
task O
and O
a O
multiple- O
choice B-DAT
task O
[45], O
[33]. O
Unlike O
the O

choice B-DAT
task O
only O
requires O
an O

choice B-DAT
answers). O
These O
approaches O
provide O
inspiration O

choice B-DAT

choice B-DAT
task, O
18 O
candidate O
answers O
are O

answers O
is O
randomized. O
Example O
multiple O
choice B-DAT
questions O
are O
in O
the O
appendix O

choice B-DAT

choice B-DAT
tasks. O
Note O
that O
“yes” O
is O

choice B-DAT
questions O

choice B-DAT
task, O
we O
pick O
the O
answer O

choice B-DAT
task, O
we O
pick O
the O
answer O

for O
the O
open-ended O
and O
multiple- O
choice B-DAT
tasks O
on O
the O
VQA O
test-dev O

choice B-DAT
picks O
the O
answer O
that O
has O

choice B-DAT
tasks O
on O
the O
VQA O
test-dev O

choice B-DAT

choice) B-DAT
and O
LSTM O
Q O
achieving O
48.76 O

choice B-DAT

choice B-DAT

choice B-DAT

choice B-DAT
are O
better O
than O
open-ended. O
All O

choice B-DAT
tasks O
on O
the O
VQA O
test-dev O

choice B-DAT
task O

choice B-DAT
task O

choice B-DAT
task O

choice B-DAT
task O

choice B-DAT
task O

choice B-DAT
task O

choice B-DAT
tasks O
on O
the O
VQA O
test-dev O

and O
by O
1.88% O
for O
multiple- O
choice B-DAT
task O

choice B-DAT

choice B-DAT
tasks O
(real O
images) O
with O
other O

choice B-DAT
tasks O
in O
the O
respective O
VQA O

choice B-DAT
questions O
IV O
- O
Details O
on O

choice B-DAT
questions O
when O
subjects O
were O
shown O

choice B-DAT
questions, O
we O
collected O
three O
human O

human O
accuracies O
for O
mul- O
tiple O
choice B-DAT
questions. O
Table O
6 O
also O
shows O

choice B-DAT
accuracies O
are O
more O
or O
less O

increase O
in O
accuracy O
using O
multiple O
choice B-DAT
is O
not O
surprising O

plausible, O
answers O
for O
the O
multiple- O
choice B-DAT
task O
and O
to O
assess O
how O

choice B-DAT
answers O

choice B-DAT
questions, O
respectively O

choice B-DAT
questions O
for O
numerous O
representative O
examples O

VQA: O
Visual O
Question B-DAT
Answering O
www.visualqa.org O

of O
free-form O
and O
open-ended O
Visual O
Question B-DAT
Answering O
(VQA). O
Given O
an O
image O

free-form O
and O
open- O
ended O
Visual O
Question B-DAT
Answering O
(VQA). O
A O
VQA O
system O

We O
now O
describe O
the O
Visual O
Question B-DAT
Answering O
(VQA) O
dataset. O
We O
begin O

Types O
of O
Question B-DAT

of O
Words O
in O
Question B-DAT

Distribution O
of O
Question B-DAT
Lengths O

real O
images O
and O
abstract O
scenes. O
Question B-DAT
types O
such O
as O
“How O
many O

As O
shown O
in O
Table O
1 O
(Question B-DAT
+ O
Image), O
there O
is O
significant O

Fig. O
2). O
In O
Table O
1 O
(Question), B-DAT
we O
show O
the O
percentage O
of O

Question B-DAT
40.81 O
67.60 O
25.77 O
21.22 O
Real O

Question B-DAT
+ O
Caption* O
57.47 O
78.97 O
39.68 O

Question B-DAT
+ O
Image O
83.30 O
95.77 O
83.39 O

Question B-DAT
43.27 O
66.65 O
28.52 O
23.66 O
Abstract O

Question B-DAT
+ O
Caption* O
54.34 O
74.70 O
41.19 O

Question B-DAT
+ O
Image O
87.49 O
95.96 O
95.04 O

question O
without O
seeing O
the O
image O
(Question), B-DAT
seeing O
just O
a O
caption O
of O

and O
not O
the O
image O
itself O
(Question B-DAT
+ O
Caption), O
and O
seeing O
the O

image O
(Question B-DAT
+ O
Image). O
Results O
are O
shown O

answer O
the O
questions? O
Table O
1 O
(Question B-DAT
+ O
Caption) O
shows O
the O
percentage O

Question B-DAT
Channel: O
This O
channel O
provides O
an O

1) O
Bag-of-Words O
Question B-DAT
(BoW O
Q): O
The O
top O
1,000 O

caption O
embedding O
(Caption). O
For O
BoW O
Question B-DAT
+ O
Caption O
(BoW O
Q O

for O
real O
images. O
Q O
= O
Question, B-DAT
I O
= O
Image, O
C O

Question B-DAT
K O
= O
1000 O
Human O
To O

for O
real O
images. O
Q O
= O
Question, B-DAT
I O
= O
Image. O
See O
text O

introduce O
the O
task O
of O
Visual O
Question B-DAT
Answering O
(VQA). O
Given O
an O
image O

Etzioni. O
Paraphrase-Driven O
Learning O
for O
Open O
Question B-DAT
Answering. O
In O
ACL, O
2013. O
2 O

Zettlemoyer, O
and O
O. O
Etzioni. O
Open O
Question B-DAT
Answering O
over O
Curated O
and O
Extracted O

Fritz. O
A O
Multi-World O
Approach O
to O
Question B-DAT
Answering O
about O
Real-World O
Scenes O
based O

T. O
Mikolov. O
Towards O
AI- O
Complete O
Question B-DAT
Answering: O
A O
Set O
of O
Prerequisite O

VQA: O
Visual B-DAT
Question O
Answering O
www.visualqa.org O

task O
of O
free-form O
and O
open-ended O
Visual B-DAT
Question O
Answering O
(VQA). O
Given O
an O

questions O
and O
answers O
are O
open-ended. O
Visual B-DAT
questions O
selectively O
target O
different O
areas O

of O
free-form O
and O
open- O
ended O
Visual B-DAT
Question O
Answering O
(VQA). O
A O
VQA O

complex O
reasoning O
more O
essential. O
Describing O
Visual B-DAT
Content. O
Related O
to O
VQA O
are O

We O
now O
describe O
the O
Visual B-DAT
Question O
Answering O
(VQA) O
dataset. O
We O

we O
introduce O
the O
task O
of O
Visual B-DAT
Question O
Answering O
(VQA). O
Given O
an O

cloud O
service. O
In O
Mobile O
Cloud O
Visual B-DAT
Media O
Computing, O
pages O
265–290. O
Springer O

D. O
Parikh. O
Zero-Shot O
Learning O
via O
Visual B-DAT
Abstraction. O
In O
ECCV, O
2014. O
2 O

Nearly O
Real- O
time O
Answers O
to O
Visual B-DAT
Questions. O
In O
User O
Interface O
Software O

and O
A. O
Gupta. O
NEIL: O
Extracting O
Visual B-DAT
Knowledge O
from O
Web O
Data. O
In O

Zitnick. O
Mind’s O
Eye: O
A O
Recurrent O
Visual B-DAT
Represen- O
tation O
for O
Image O
Caption O

Long-term O
Recurrent O
Convolutional O
Networks O
for O
Visual B-DAT
Recognition O
and O
Description. O
In O
CVPR O

G. O
Zweig. O
From O
Captions O
to O
Visual B-DAT
Concepts O
and O
Back. O
In O
CVPR O

Hallonquist, O
and O
L. O
Younes. O
A O
Visual B-DAT
Turing O
Test O
for O
Computer O
Vision O

Karpathy O
and O
L. O
Fei-Fei. O
Deep O
Visual B-DAT

and O
R. O
S. O
Zemel. O
Unifying O
Visual B-DAT

Listen, O
Use O
Your O
Imagination: O
Leveraging O
Visual B-DAT
Common O
Sense O
for O
Non-Visual O
Tasks O

Divvala, O
and O
A. O
Farhadi. O
Viske: O
Visual B-DAT
knowledge O
extraction O
and O
question O
answering O

Berg, O
and O
T. O
L. O
Berg. O
Visual B-DAT
madlibs: O
Fill-in-the- O
blank O
description O
generation O

Bringing O
Semantics O
Into O
Focus O
Using O
Visual B-DAT
Abstraction. O
In O
CVPR, O
2013. O
2 O

and O
L. O
Vanderwende. O
Learning O
the O
Visual B-DAT
Interpretation O
of O
Sentences. O
In O
ICCV O

can O
be O
provided O
in O
a O
multiple B-DAT

open-ended O
answering O
task O
and O
a O
multiple B-DAT

requires O
a O
free-form O
response, O
the O
multiple B-DAT

sentence O
completion O
(e.g., O
[45] O
with O
multiple B-DAT

gathered O
to O
find O
images O
containing O
multiple B-DAT
objects O
and O
rich O
contextual O
information O

tions: O
(i) O
open-ended O
and O
(ii) O
multiple B-DAT

and O
reliable O
for O
sentences O
containing O
multiple B-DAT
words. O
In O
VQA, O
most O
answers O

image O
caption O
evaluation O
[6]. O
For O
multiple B-DAT

the O
answers O
is O
randomized. O
Example O
multiple B-DAT
choice O
questions O
are O
in O
the O

according O
to O
the O
accuracy O
metric, O
multiple B-DAT
options O
could O
have O
a O
non-zero O

answers O
are O
free-form O
and O
not O
multiple B-DAT

for O
both O
the O
open-ended O
and O
multiple B-DAT

of O
the O
choices O
for O
the O
multiple B-DAT

appendix O
for O
details). O
For O
the O
multiple B-DAT

Q-type O
prior” O
baseline, O
for O
the O
multiple B-DAT

methods O
for O
the O
open-ended O
and O
multiple B-DAT

possible O
K O
answers O
and O
multiple B-DAT

for O
both O
the O
open-ended O
and O
multiple B-DAT

rather O
poorly O
(open-ended: O
28.13% O
/ O
multiple B-DAT

48.09% O
on O
open-ended O
(53.68% O
on O
multiple B-DAT

on O
open- O
ended O
(54.75% O
on O
multiple B-DAT

nearest O
neighbor O
baseline O
(open-ended: O
42.70%, O
multiple B-DAT

is O
58.16% O
(open-ended) O
/ O
63.09% O
(multiple B-DAT

a O
general O
trend, O
results O
on O
multiple B-DAT

both O
the O
open- O
ended O
and O
multiple B-DAT

task O
and O
by O
0.24% O
for O
multiple B-DAT

task O
and O
by O
1.24% O
for O
multiple B-DAT

task O
and O
by O
1.92% O
for O
multiple B-DAT

task O
and O
by O
1.16% O
for O
multiple B-DAT

task O
and O
by O
0.17% O
for O
multiple B-DAT

task O
and O
by O
0.02% O
for O
multiple B-DAT

I) O
for O
the O
open-ended O
and O
multiple B-DAT

task O
and O
by O
1.88% O
for O
multiple B-DAT

of O
leaderboards O
for O
open-ended-real O
and O
multiple B-DAT

I) O
for O
both O
open-ended O
and O
multiple B-DAT

entries O
for O
the O
open-ended O
and O
multiple B-DAT

III O
- O
Human O
accuracy O
on O
multiple B-DAT

are O
the O
human O
accuracies O
for O
multiple B-DAT

To O
compute O
human O
accuracy O
for O
multiple B-DAT

to O
open- O
ended O
answer, O
the O
multiple B-DAT

the O
increase O
in O
accuracy O
using O
multiple B-DAT
choice O
is O
not O
surprising O

but O
plausible, O
answers O
for O
the O
multiple B-DAT

collect O
the O
plausible, O
but O
incorrect, O
multiple B-DAT

32] O
images, O
abstract O
scenes, O
and O
multiple B-DAT

Fig. O
29: O
Random O
examples O
of O
multiple B-DAT

204,721 O
images O
from O
the O
MS O
COCO B-DAT
dataset O
[32] O
and O
a O
newly O

contains O
50,000 O
scenes. O
The O
MS O
COCO B-DAT
dataset O
has O
images O
depicting O
diverse O

to O
English O
by O
humans) O
for O
COCO B-DAT
images. O
[44] O
automatically O
generated O
four O

object, O
count, O
color, O
location) O
using O
COCO B-DAT
captions. O
Text-based O
Q&A O
is O
a O

Common O
Objects O
in O
Context O
(MS O
COCO) B-DAT
[32] O
dataset. O
The O
MS O
COCO O

split O
strategy O
as O
the O
MC O
COCO B-DAT
dataset O
[32] O
(including O
test- O
dev O

abstract O
scenes. O
Captions. O
The O
MS O
COCO B-DAT
dataset O
[32], O
[7] O
already O
contains O

204,721 O
images O
from O
the O
MS O
COCO B-DAT
dataset O
[32] O
and O
150,000 O
questions O

VQA O
dataset O
for O
the O
MS O
COCO B-DAT
images O
using O
several O
baselines O
and O

from O
the O
caption O
data O
(MS O
COCO B-DAT
captions O
for O
real O
images O
and O

VQA O
dataset O
for O
the O
MS O
COCO B-DAT
[32] O
images, O
abstract O
scenes, O
and O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT
captions: O
Data O
collection O
and O
evaluation O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT
Captions: O
Data O
Collection O
and O
Evaluation O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT

VQA: O
Visual O
Question O
Answering B-DAT
www.visualqa.org O

free-form O
and O
open-ended O
Visual O
Question O
Answering B-DAT
(VQA). O
Given O
an O
image O
and O

and O
open- O
ended O
Visual O
Question O
Answering B-DAT
(VQA). O
A O
VQA O
system O
takes O

now O
describe O
the O
Visual O
Question O
Answering B-DAT
(VQA) O
dataset. O
We O
begin O
by O

the O
task O
of O
Visual O
Question O
Answering B-DAT
(VQA). O
Given O
an O
image O
and O

Paraphrase-Driven O
Learning O
for O
Open O
Question O
Answering B-DAT

and O
O. O
Etzioni. O
Open O
Question O
Answering B-DAT
over O
Curated O
and O
Extracted O
Knowledge O

A O
Multi-World O
Approach O
to O
Question O
Answering B-DAT
about O
Real-World O
Scenes O
based O
on O

Parsing O
for O
Understanding O
Events O
and O
Answering B-DAT
Queries. O
IEEE O
MultiMedia, O
2014. O
1 O

Mikolov. O
Towards O
AI- O
Complete O
Question O
Answering B-DAT

VQA B-DAT

and O
open-ended O
Visual O
Question O
Answering O
(VQA B-DAT

a O
system O
that O
succeeds O
at O
VQA B-DAT
typically O
needs O
a O
more O
detailed O

producing O
generic O
image O
captions. O
Moreover, O
VQA B-DAT
is O
amenable O
to O
automatic O
evaluation O

Numerous O
baselines O
and O
methods O
for O
VQA B-DAT
are O
provided O
and O
compared O
with O

human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

open- O
ended O
Visual O
Question O
Answering O
(VQA B-DAT

). O
A O
VQA B-DAT
system O
takes O
as O
input O
an O

Is O
this O
person O
expecting O
company?”). O
VQA B-DAT
[19], O
[36], O
[50], O
[3] O
is O

the O
high-level O
reasoning O
required O
for O
VQA B-DAT
by O
removing O
the O
need O
to O

29]. O
As O
part O
of O
the O
VQA B-DAT
initiative, O
we O
will O
organize O
an O

state-of-the-art O
methods O
and O
best O
practices. O
VQA B-DAT
poses O
a O
rich O
set O
of O

during O
the O
past O
few O
decades. O
VQA B-DAT
provides O
an O
attractive O
balance O
between O

VQA B-DAT
Efforts. O
Several O
recent O
papers O
have O

difficult O
and O
unconstrained O
task, O
our O
VQA B-DAT
dataset O
is O
two O
orders O
of O

1,449 O
images O
respectively). O
The O
proposed O
VQA B-DAT
task O
has O
connections O
to O
other O

These O
approaches O
provide O
inspiration O
for O
VQA B-DAT
techniques. O
One O
key O
concern O
in O

fixed O
set O
of O
loca- O
tions. O
VQA B-DAT
is O
naturally O
grounded O
in O
images O

Describing O
Visual O
Content. O
Related O
to O
VQA B-DAT
are O
the O
tasks O
of O
image O

by O
[53]). O
The O
questions O
in O
VQA B-DAT
require O
detailed O
specific O
information O
about O

3 O
VQA B-DAT
DATASET O
COLLECTION O

describe O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset. O
We O
begin O
by O
describing O

they O
are O
well-suited O
for O
our O
VQA B-DAT
task. O
The O
more O
diverse O
our O

their O
answers. O
Abstract O
Scenes. O
The O
VQA B-DAT
task O
with O
real O
images O
requires O

the O
high-level O
reasoning O
required O
for O
VQA, B-DAT
but O
not O
the O
low-level O
vision O

test-standard, O
test-challenge, O
test-reserve). O
For O
the O
VQA B-DAT
challenge O
(see O
section O
6), O
test-dev O

default’ O
test O
data O
for O
the O
VQA B-DAT
competition. O
When O
comparing O
to O
the O

sentences O
containing O
multiple O
words. O
In O
VQA, B-DAT
most O
answers O
(89.32%) O
are O
single O

4 O
VQA B-DAT
DATASET O
ANALYSIS O
In O
this O
section O

questions O
and O
answers O
in O
the O
VQA B-DAT
train O
dataset. O
To O
gain O
an O

visual O
information O
is O
critical O
to O
VQA B-DAT
and O
that O
commonsense O
information O
alone O

from O
the O
real O
images O
of O
VQA B-DAT
trainval) O
asking O
subjects O

5 O
VQA B-DAT
BASELINES O
AND O
METHODS O
In O
this O

explore O
the O
difficulty O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

novel O
methods. O
We O
train O
on O
VQA B-DAT
train+val. O
Unless O
stated O
otherwise, O
all O

top O
1K O
answers O
of O
the O
VQA B-DAT
train/val O
dataset O

multiple- O
choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
As O

I O
(Fig. O
8), O
selected O
using O
VQA B-DAT
test-dev O
accuracies) O
on O
VQA O
test O

worse O
than O
human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

system O
is O
correct O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

correct” O
implies O
that O
it O
has O
VQA B-DAT
accuracy O
1.0 O
(see O
section O
3 O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
The O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

frequently O
predicted O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

age O
of O
question) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

system O
is O
correct) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

a O
filtered O
version O
of O
the O
VQA B-DAT
train O
+ O
val O
dataset O
in O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

6 O
VQA B-DAT
CHALLENGE O
AND O
WORKSHOP O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O

task O
of O
Visual O
Question O
Answering O
(VQA B-DAT

multiple-choice O
tasks O
in O
the O
respective O
VQA B-DAT
Real O
Image O
Challenge O
leaderboards O
(as O

datasets O
may O
help O
enable O
practical O
VQA B-DAT
applications. O
We O
believe O
VQA O
has O

questions O
IV O
- O
Details O
on O
VQA B-DAT
baselines O
V O
- O
“Age” O
and O

Leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Open-Ended) O
on O

leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Multiple-Choice) O
on O

Additional O
examples O
from O
the O
VQA B-DAT
dataset O

scenes. O
This O
helps O
motivate O
the O
VQA B-DAT
task O
as O
a O
way O
to O

APPENDIX O
IV: O
DETAILS O
ON O
VQA B-DAT
BASELINES O
“per O
Q-type O
prior” O
baseline O

For O
every O
question O
in O
the O
VQA B-DAT
test-standard O
set, O
we O
find O
its O

norm O
I), O
selected O
using O
VQA B-DAT
test- O
dev O
accuracies). O
To O
estimate O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

a O
random O
selection O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

scenes. O
The O
set O
contains O
over O
100 B-DAT
objects O
and O
31 O
animals O
in O

i.e., O
an O
answer O
is O
deemed O
100 B-DAT

MS O
COCO O
dataset O
[32] O
and O
150, B-DAT

a O
scale O
of O
0 O
− O
100) B-DAT
required O
to O
answer O
a O
question O

choose O
the O
top O
K O
= O
1000 B-DAT
most O
frequent O
answers O
as O
possible O

Question O
(BoW O
Q): O
The O
top O
1,000 B-DAT
words O
in O
the O
questions O
are O

are O
concatenated O
to O
get O
a O
1,030 B-DAT

1000 B-DAT

1000 B-DAT

with O
2 O
hidden O
layers O
and O
1000 B-DAT
hidden O
units O
(dropout O
0.5) O
in O

a O
bag-of-words O
representation O
containing O
the O
1,000 B-DAT
most O
popular O
words O
in O
the O

that O
it O
has O
VQA O
accuracy O
1.0 B-DAT
(see O
section O
3 O
for O
accuracy O

Question O
K O
= O
1000 B-DAT
Human O
To O
Be O
Able O
To O

100 B-DAT
A O

100 B-DAT

100 B-DAT

100 B-DAT

can O
see O
that O
K O
= O
1000 B-DAT
performs O
better O
than O
K O

performs O
better O
then O
K O
= O
1000 B-DAT
by O
0.40% O
for O
open-ended O
task O

blue” O
(28881, O
1.16%), O
“4” O
(27174, O
1.09 B-DAT

outside” O
(1846, O
0.07%), O
“hot O
dog” O
(1809, B-DAT
0.07%), O
“night” O
(1805, O
0.07%), O
“trees O

and O
white” O
(1518, O
0.06%), O
“bedroom” O
(1500, B-DAT
0.06%), O
“bat” O
(1494, O
0.06%), O
“glasses O

0.06%), O
“cloudy” O
(1413, O
0.06%), O
“15” O
(1407, B-DAT
0.06%), O
“up” O
(1399, O
0.06%), O
“blonde O

0.05%), O
“many” O
(1211, O
0.05%), O
“zoo” O
(1204, B-DAT
0.05%), O
“suitcase” O
(1199, O
0.05%), O
“old O

0.04%), O
“mountains” O
(1030, O
0.04%), O
“wall” O
(1009, B-DAT
0.04%), O
“ele- O
phants” O
(1006, O
0.04 O

1504 B-DAT

1504 B-DAT

1408 B-DAT

1506 B-DAT

1409 B-DAT

1502 B-DAT

32] O
and O
a O
newly O
created O
abstract B-DAT
scene O
dataset O
[57], O
[2] O
that O

a O
new O
dataset O
of O
“realistic” O
abstract B-DAT
scenes O
to O
enable O
research O
focused O

output O
answer O
classes. O
[34] O
generates O
abstract B-DAT
scenes O
to O
capture O
visual O
common O

describing O
the O
real O
images O
and O
abstract B-DAT

tasks, O
we O
create O
a O
new O
abstract B-DAT
scenes O
dataset O
[2], O
[57], O
[58 O

winners O
of O
the O
challenge. O
For O
abstract B-DAT
scenes, O
we O
created O
splits O
for O

test-dev, O
test-standard, O
test-challenge, O
test-reserve) O
for O
abstract B-DAT
scenes. O
Captions. O
The O
MS O
COCO O

collected O
five O
single-captions O
for O
all O
abstract B-DAT
scenes O
using O
the O
same O
user O

both O
the O
real O
images O
and O
abstract B-DAT
scenes. O
In O
total, O
three O
questions O

left) O
and O
all O
questions O
for O
abstract B-DAT
scenes O
(right). O
The O
ordering O
of O

1,950,000 O
answers O
for O
50, O
000 O
abstract B-DAT
scenes O

the O
real O
images O
(left) O
and O
abstract B-DAT
scenes O
(right). O
Interestingly, O
the O
distribution O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
This O
helps O
demonstrate O
that O

of O
questions O
elicited O
by O
the O
abstract B-DAT
scenes O
is O
similar O
to O
those O

lengths O
for O
real O
images O
and O
abstract B-DAT
scenes O

90.51%, O
5.89%, O
and O
2.49% O
for O
abstract B-DAT
scenes. O
The O
brevity O
of O
answers O

real O
images O
and O
3,770 O
for O
abstract B-DAT
scenes. O
‘Yes/No’ O
and O
‘Number’ O
Answers O

questions O
on O
real O
images O
and O
abstract B-DAT
scenes O
respectively. O
Among O
these O
‘yes/no O

yes” O
for O
real O
images O
and O
abstract B-DAT
scenes. O
Question O
types O
such O
as O

questions O
on O
real O
images O
and O
abstract B-DAT
scenes O
are O
‘number’ O
questions. O
“2 O

real O
images O
and O
39.85% O
for O
abstract B-DAT
scenes. O
Subject O
Confidence. O
When O
the O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
Inter-human O
Agreement. O
Does O
the O

confident) O
for O
real O
images O
and O
abstract B-DAT
scenes O
(black O
lines). O
Percentage O
of O

both O
real O
images O
(83.30%) O
and O
abstract B-DAT
scenes O
(87.49%). O
Note O
that O
on O

real O
images O
and O
2.39 O
for O
abstract B-DAT
scenes. O
The O
agreement O
is O
significantly O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
See O
the O
appendix O
for O

abstract B-DAT
| O
mc-real O
| O
mc-abstract O

oe-abstract-leaderboard B-DAT
| O
mc-real-leaderboard O
| O
mc- O
abstract O

VI O
- O
Details O
on O
the O
abstract B-DAT
scene O
dataset O
VII O
- O
User O

captions O
collected O
by O
us O
for O
abstract B-DAT
scenes) O
using O
the O
Stanford O
part-of-speech O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
This O
helps O
motivate O
the O

and O
Fig. O
20 O
(adjectives) O
for O
abstract B-DAT
scenes.7 O
The O
left O
side O
shows O

and O
Fig. O
14 O
(right) O
for O
abstract B-DAT
scenes. O
We O
see O
that O
questions O

for O
real O
images O
(left) O
and O
abstract B-DAT
scenes O
(right O

indicating O
the O
normalized O
count O
for O
abstract B-DAT
scenes O

indicating O
the O
normalized O
count O
for O
abstract B-DAT
scenes O

indicating O
the O
normalized O
count O
for O
abstract B-DAT
scenes O

the O
two O
datasets, O
real O
and O
abstract, B-DAT
first O
two O
rows O
are O
the O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
Note O
the O
diversity O
of O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
In O
Table O
6, O
we O

and O
≈ O
11% O
increase O
for O
abstract B-DAT
scenes) O
for O
“other” O
questions. O
Since O

that O
are O
present O
in O
the O
abstract B-DAT
scenes O
dataset. O
For O
more O
examples O

left) O
and O
all O
questions O
for O
abstract B-DAT
scenes O
(right). O
The O
ordering O
of O

top) O
and O
all O
questions O
for O
abstract B-DAT
scenes O
(bottom). O
Each O
column O
corresponds O

the O
objects O
present O
in O
the O
abstract B-DAT
scene O
dataset. O
Right: O
The O
AMT O

interface O
for O
collecting O
abstract B-DAT
scenes. O
The O
light O
green O
circles O

the O
MS O
COCO O
[32] O
images, O
abstract B-DAT
scenes, O
and O
multiple-choice O
questions, O
respectively O

numerous O
representative O
examples O
of O
the O
abstract B-DAT
scene O
dataset O

examples O
of O
the O
real O
and O
abstract B-DAT
scene O
dataset O

VQA B-DAT

and O
open-ended O
Visual O
Question O
Answering O
(VQA B-DAT

a O
system O
that O
succeeds O
at O
VQA B-DAT
typically O
needs O
a O
more O
detailed O

producing O
generic O
image O
captions. O
Moreover, O
VQA B-DAT
is O
amenable O
to O
automatic O
evaluation O

Numerous O
baselines O
and O
methods O
for O
VQA B-DAT
are O
provided O
and O
compared O
with O

human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

open- O
ended O
Visual O
Question O
Answering O
(VQA B-DAT

). O
A O
VQA B-DAT
system O
takes O
as O
input O
an O

Is O
this O
person O
expecting O
company?”). O
VQA B-DAT
[19], O
[36], O
[50], O
[3] O
is O

the O
high-level O
reasoning O
required O
for O
VQA B-DAT
by O
removing O
the O
need O
to O

29]. O
As O
part O
of O
the O
VQA B-DAT
initiative, O
we O
will O
organize O
an O

state-of-the-art O
methods O
and O
best O
practices. O
VQA B-DAT
poses O
a O
rich O
set O
of O

during O
the O
past O
few O
decades. O
VQA B-DAT
provides O
an O
attractive O
balance O
between O

VQA B-DAT
Efforts. O
Several O
recent O
papers O
have O

difficult O
and O
unconstrained O
task, O
our O
VQA B-DAT
dataset O
is O
two O
orders O
of O

1,449 O
images O
respectively). O
The O
proposed O
VQA B-DAT
task O
has O
connections O
to O
other O

These O
approaches O
provide O
inspiration O
for O
VQA B-DAT
techniques. O
One O
key O
concern O
in O

fixed O
set O
of O
loca- O
tions. O
VQA B-DAT
is O
naturally O
grounded O
in O
images O

Describing O
Visual O
Content. O
Related O
to O
VQA B-DAT
are O
the O
tasks O
of O
image O

by O
[53]). O
The O
questions O
in O
VQA B-DAT
require O
detailed O
specific O
information O
about O

3 O
VQA B-DAT
DATASET O
COLLECTION O

describe O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset. O
We O
begin O
by O
describing O

they O
are O
well-suited O
for O
our O
VQA B-DAT
task. O
The O
more O
diverse O
our O

their O
answers. O
Abstract O
Scenes. O
The O
VQA B-DAT
task O
with O
real O
images O
requires O

the O
high-level O
reasoning O
required O
for O
VQA, B-DAT
but O
not O
the O
low-level O
vision O

test-standard, O
test-challenge, O
test-reserve). O
For O
the O
VQA B-DAT
challenge O
(see O
section O
6), O
test-dev O

default’ O
test O
data O
for O
the O
VQA B-DAT
competition. O
When O
comparing O
to O
the O

sentences O
containing O
multiple O
words. O
In O
VQA, B-DAT
most O
answers O
(89.32%) O
are O
single O

4 O
VQA B-DAT
DATASET O
ANALYSIS O
In O
this O
section O

questions O
and O
answers O
in O
the O
VQA B-DAT
train O
dataset. O
To O
gain O
an O

visual O
information O
is O
critical O
to O
VQA B-DAT
and O
that O
commonsense O
information O
alone O

from O
the O
real O
images O
of O
VQA B-DAT
trainval) O
asking O
subjects O

5 O
VQA B-DAT
BASELINES O
AND O
METHODS O
In O
this O

explore O
the O
difficulty O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

novel O
methods. O
We O
train O
on O
VQA B-DAT
train+val. O
Unless O
stated O
otherwise, O
all O

top O
1K O
answers O
of O
the O
VQA B-DAT
train/val O
dataset O

multiple- O
choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
As O

I O
(Fig. O
8), O
selected O
using O
VQA B-DAT
test-dev O
accuracies) O
on O
VQA O
test O

worse O
than O
human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

system O
is O
correct O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

correct” O
implies O
that O
it O
has O
VQA B-DAT
accuracy O
1.0 O
(see O
section O
3 O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
The O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

frequently O
predicted O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

age O
of O
question) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

system O
is O
correct) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

a O
filtered O
version O
of O
the O
VQA B-DAT
train O
+ O
val O
dataset O
in O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

6 O
VQA B-DAT
CHALLENGE O
AND O
WORKSHOP O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O

task O
of O
Visual O
Question O
Answering O
(VQA B-DAT

multiple-choice O
tasks O
in O
the O
respective O
VQA B-DAT
Real O
Image O
Challenge O
leaderboards O
(as O

datasets O
may O
help O
enable O
practical O
VQA B-DAT
applications. O
We O
believe O
VQA O
has O

questions O
IV O
- O
Details O
on O
VQA B-DAT
baselines O
V O
- O
“Age” O
and O

Leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Open-Ended) O
on O

leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Multiple-Choice) O
on O

Additional O
examples O
from O
the O
VQA B-DAT
dataset O

scenes. O
This O
helps O
motivate O
the O
VQA B-DAT
task O
as O
a O
way O
to O

APPENDIX O
IV: O
DETAILS O
ON O
VQA B-DAT
BASELINES O
“per O
Q-type O
prior” O
baseline O

For O
every O
question O
in O
the O
VQA B-DAT
test-standard O
set, O
we O
find O
its O

norm O
I), O
selected O
using O
VQA B-DAT
test- O
dev O
accuracies). O
To O
estimate O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

a O
random O
selection O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

choice B-DAT
format. O
We O
provide O
a O
dataset O

answering O
task O
and O
a O
multiple- O
choice B-DAT
task O
[45], O
[33]. O
Unlike O
the O

choice B-DAT
task O
only O
requires O
an O

choice B-DAT
answers). O
These O
approaches O
provide O
inspiration O

choice B-DAT

choice B-DAT
task, O
18 O
candidate O
answers O
are O

answers O
is O
randomized. O
Example O
multiple O
choice B-DAT
questions O
are O
in O
the O
appendix O

choice B-DAT

choice B-DAT
tasks. O
Note O
that O
“yes” O
is O

choice B-DAT
questions O

choice B-DAT
task, O
we O
pick O
the O
answer O

choice B-DAT
task, O
we O
pick O
the O
answer O

for O
the O
open-ended O
and O
multiple- O
choice B-DAT
tasks O
on O
the O
VQA O
test-dev O

choice B-DAT
picks O
the O
answer O
that O
has O

choice B-DAT
tasks O
on O
the O
VQA O
test-dev O

choice B-DAT

choice) B-DAT
and O
LSTM O
Q O
achieving O
48.76 O

choice B-DAT

choice B-DAT

choice B-DAT

choice B-DAT
are O
better O
than O
open-ended. O
All O

choice B-DAT
tasks O
on O
the O
VQA O
test-dev O

choice B-DAT
task O

choice B-DAT
task O

choice B-DAT
task O

choice B-DAT
task O

choice B-DAT
task O

choice B-DAT
task O

choice B-DAT
tasks O
on O
the O
VQA O
test-dev O

and O
by O
1.88% O
for O
multiple- O
choice B-DAT
task O

choice B-DAT

choice B-DAT
tasks O
(real O
images) O
with O
other O

choice B-DAT
tasks O
in O
the O
respective O
VQA O

choice B-DAT
questions O
IV O
- O
Details O
on O

choice B-DAT
questions O
when O
subjects O
were O
shown O

choice B-DAT
questions, O
we O
collected O
three O
human O

human O
accuracies O
for O
mul- O
tiple O
choice B-DAT
questions. O
Table O
6 O
also O
shows O

choice B-DAT
accuracies O
are O
more O
or O
less O

increase O
in O
accuracy O
using O
multiple O
choice B-DAT
is O
not O
surprising O

plausible, O
answers O
for O
the O
multiple- O
choice B-DAT
task O
and O
to O
assess O
how O

choice B-DAT
answers O

choice B-DAT
questions, O
respectively O

choice B-DAT
questions O
for O
numerous O
representative O
examples O

VQA: O
Visual O
Question B-DAT
Answering O
www.visualqa.org O

of O
free-form O
and O
open-ended O
Visual O
Question B-DAT
Answering O
(VQA). O
Given O
an O
image O

free-form O
and O
open- O
ended O
Visual O
Question B-DAT
Answering O
(VQA). O
A O
VQA O
system O

We O
now O
describe O
the O
Visual O
Question B-DAT
Answering O
(VQA) O
dataset. O
We O
begin O

Types O
of O
Question B-DAT

of O
Words O
in O
Question B-DAT

Distribution O
of O
Question B-DAT
Lengths O

real O
images O
and O
abstract O
scenes. O
Question B-DAT
types O
such O
as O
“How O
many O

As O
shown O
in O
Table O
1 O
(Question B-DAT
+ O
Image), O
there O
is O
significant O

Fig. O
2). O
In O
Table O
1 O
(Question), B-DAT
we O
show O
the O
percentage O
of O

Question B-DAT
40.81 O
67.60 O
25.77 O
21.22 O
Real O

Question B-DAT
+ O
Caption* O
57.47 O
78.97 O
39.68 O

Question B-DAT
+ O
Image O
83.30 O
95.77 O
83.39 O

Question B-DAT
43.27 O
66.65 O
28.52 O
23.66 O
Abstract O

Question B-DAT
+ O
Caption* O
54.34 O
74.70 O
41.19 O

Question B-DAT
+ O
Image O
87.49 O
95.96 O
95.04 O

question O
without O
seeing O
the O
image O
(Question), B-DAT
seeing O
just O
a O
caption O
of O

and O
not O
the O
image O
itself O
(Question B-DAT
+ O
Caption), O
and O
seeing O
the O

image O
(Question B-DAT
+ O
Image). O
Results O
are O
shown O

answer O
the O
questions? O
Table O
1 O
(Question B-DAT
+ O
Caption) O
shows O
the O
percentage O

Question B-DAT
Channel: O
This O
channel O
provides O
an O

1) O
Bag-of-Words O
Question B-DAT
(BoW O
Q): O
The O
top O
1,000 O

caption O
embedding O
(Caption). O
For O
BoW O
Question B-DAT
+ O
Caption O
(BoW O
Q O

for O
real O
images. O
Q O
= O
Question, B-DAT
I O
= O
Image, O
C O

Question B-DAT
K O
= O
1000 O
Human O
To O

for O
real O
images. O
Q O
= O
Question, B-DAT
I O
= O
Image. O
See O
text O

introduce O
the O
task O
of O
Visual O
Question B-DAT
Answering O
(VQA). O
Given O
an O
image O

Etzioni. O
Paraphrase-Driven O
Learning O
for O
Open O
Question B-DAT
Answering. O
In O
ACL, O
2013. O
2 O

Zettlemoyer, O
and O
O. O
Etzioni. O
Open O
Question B-DAT
Answering O
over O
Curated O
and O
Extracted O

Fritz. O
A O
Multi-World O
Approach O
to O
Question B-DAT
Answering O
about O
Real-World O
Scenes O
based O

T. O
Mikolov. O
Towards O
AI- O
Complete O
Question B-DAT
Answering: O
A O
Set O
of O
Prerequisite O

VQA: O
Visual B-DAT
Question O
Answering O
www.visualqa.org O

task O
of O
free-form O
and O
open-ended O
Visual B-DAT
Question O
Answering O
(VQA). O
Given O
an O

questions O
and O
answers O
are O
open-ended. O
Visual B-DAT
questions O
selectively O
target O
different O
areas O

of O
free-form O
and O
open- O
ended O
Visual B-DAT
Question O
Answering O
(VQA). O
A O
VQA O

complex O
reasoning O
more O
essential. O
Describing O
Visual B-DAT
Content. O
Related O
to O
VQA O
are O

We O
now O
describe O
the O
Visual B-DAT
Question O
Answering O
(VQA) O
dataset. O
We O

we O
introduce O
the O
task O
of O
Visual B-DAT
Question O
Answering O
(VQA). O
Given O
an O

cloud O
service. O
In O
Mobile O
Cloud O
Visual B-DAT
Media O
Computing, O
pages O
265–290. O
Springer O

D. O
Parikh. O
Zero-Shot O
Learning O
via O
Visual B-DAT
Abstraction. O
In O
ECCV, O
2014. O
2 O

Nearly O
Real- O
time O
Answers O
to O
Visual B-DAT
Questions. O
In O
User O
Interface O
Software O

and O
A. O
Gupta. O
NEIL: O
Extracting O
Visual B-DAT
Knowledge O
from O
Web O
Data. O
In O

Zitnick. O
Mind’s O
Eye: O
A O
Recurrent O
Visual B-DAT
Represen- O
tation O
for O
Image O
Caption O

Long-term O
Recurrent O
Convolutional O
Networks O
for O
Visual B-DAT
Recognition O
and O
Description. O
In O
CVPR O

G. O
Zweig. O
From O
Captions O
to O
Visual B-DAT
Concepts O
and O
Back. O
In O
CVPR O

Hallonquist, O
and O
L. O
Younes. O
A O
Visual B-DAT
Turing O
Test O
for O
Computer O
Vision O

Karpathy O
and O
L. O
Fei-Fei. O
Deep O
Visual B-DAT

and O
R. O
S. O
Zemel. O
Unifying O
Visual B-DAT

Listen, O
Use O
Your O
Imagination: O
Leveraging O
Visual B-DAT
Common O
Sense O
for O
Non-Visual O
Tasks O

Divvala, O
and O
A. O
Farhadi. O
Viske: O
Visual B-DAT
knowledge O
extraction O
and O
question O
answering O

Berg, O
and O
T. O
L. O
Berg. O
Visual B-DAT
madlibs: O
Fill-in-the- O
blank O
description O
generation O

Bringing O
Semantics O
Into O
Focus O
Using O
Visual B-DAT
Abstraction. O
In O
CVPR, O
2013. O
2 O

and O
L. O
Vanderwende. O
Learning O
the O
Visual B-DAT
Interpretation O
of O
Sentences. O
In O
ICCV O

can O
be O
provided O
in O
a O
multiple B-DAT

open-ended O
answering O
task O
and O
a O
multiple B-DAT

requires O
a O
free-form O
response, O
the O
multiple B-DAT

sentence O
completion O
(e.g., O
[45] O
with O
multiple B-DAT

gathered O
to O
find O
images O
containing O
multiple B-DAT
objects O
and O
rich O
contextual O
information O

tions: O
(i) O
open-ended O
and O
(ii) O
multiple B-DAT

and O
reliable O
for O
sentences O
containing O
multiple B-DAT
words. O
In O
VQA, O
most O
answers O

image O
caption O
evaluation O
[6]. O
For O
multiple B-DAT

the O
answers O
is O
randomized. O
Example O
multiple B-DAT
choice O
questions O
are O
in O
the O

according O
to O
the O
accuracy O
metric, O
multiple B-DAT
options O
could O
have O
a O
non-zero O

answers O
are O
free-form O
and O
not O
multiple B-DAT

for O
both O
the O
open-ended O
and O
multiple B-DAT

of O
the O
choices O
for O
the O
multiple B-DAT

appendix O
for O
details). O
For O
the O
multiple B-DAT

Q-type O
prior” O
baseline, O
for O
the O
multiple B-DAT

methods O
for O
the O
open-ended O
and O
multiple B-DAT

possible O
K O
answers O
and O
multiple B-DAT

for O
both O
the O
open-ended O
and O
multiple B-DAT

rather O
poorly O
(open-ended: O
28.13% O
/ O
multiple B-DAT

48.09% O
on O
open-ended O
(53.68% O
on O
multiple B-DAT

on O
open- O
ended O
(54.75% O
on O
multiple B-DAT

nearest O
neighbor O
baseline O
(open-ended: O
42.70%, O
multiple B-DAT

is O
58.16% O
(open-ended) O
/ O
63.09% O
(multiple B-DAT

a O
general O
trend, O
results O
on O
multiple B-DAT

both O
the O
open- O
ended O
and O
multiple B-DAT

task O
and O
by O
0.24% O
for O
multiple B-DAT

task O
and O
by O
1.24% O
for O
multiple B-DAT

task O
and O
by O
1.92% O
for O
multiple B-DAT

task O
and O
by O
1.16% O
for O
multiple B-DAT

task O
and O
by O
0.17% O
for O
multiple B-DAT

task O
and O
by O
0.02% O
for O
multiple B-DAT

I) O
for O
the O
open-ended O
and O
multiple B-DAT

task O
and O
by O
1.88% O
for O
multiple B-DAT

of O
leaderboards O
for O
open-ended-real O
and O
multiple B-DAT

I) O
for O
both O
open-ended O
and O
multiple B-DAT

entries O
for O
the O
open-ended O
and O
multiple B-DAT

III O
- O
Human O
accuracy O
on O
multiple B-DAT

are O
the O
human O
accuracies O
for O
multiple B-DAT

To O
compute O
human O
accuracy O
for O
multiple B-DAT

to O
open- O
ended O
answer, O
the O
multiple B-DAT

the O
increase O
in O
accuracy O
using O
multiple B-DAT
choice O
is O
not O
surprising O

but O
plausible, O
answers O
for O
the O
multiple B-DAT

collect O
the O
plausible, O
but O
incorrect, O
multiple B-DAT

32] O
images, O
abstract O
scenes, O
and O
multiple B-DAT

Fig. O
29: O
Random O
examples O
of O
multiple B-DAT

204,721 O
images O
from O
the O
MS O
COCO B-DAT
dataset O
[32] O
and O
a O
newly O

contains O
50,000 O
scenes. O
The O
MS O
COCO B-DAT
dataset O
has O
images O
depicting O
diverse O

to O
English O
by O
humans) O
for O
COCO B-DAT
images. O
[44] O
automatically O
generated O
four O

object, O
count, O
color, O
location) O
using O
COCO B-DAT
captions. O
Text-based O
Q&A O
is O
a O

Common O
Objects O
in O
Context O
(MS O
COCO) B-DAT
[32] O
dataset. O
The O
MS O
COCO O

split O
strategy O
as O
the O
MC O
COCO B-DAT
dataset O
[32] O
(including O
test- O
dev O

abstract O
scenes. O
Captions. O
The O
MS O
COCO B-DAT
dataset O
[32], O
[7] O
already O
contains O

204,721 O
images O
from O
the O
MS O
COCO B-DAT
dataset O
[32] O
and O
150,000 O
questions O

VQA O
dataset O
for O
the O
MS O
COCO B-DAT
images O
using O
several O
baselines O
and O

from O
the O
caption O
data O
(MS O
COCO B-DAT
captions O
for O
real O
images O
and O

VQA O
dataset O
for O
the O
MS O
COCO B-DAT
[32] O
images, O
abstract O
scenes, O
and O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT
captions: O
Data O
collection O
and O
evaluation O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT
Captions: O
Data O
Collection O
and O
Evaluation O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT

VQA: O
Visual O
Question O
Answering B-DAT
www.visualqa.org O

free-form O
and O
open-ended O
Visual O
Question O
Answering B-DAT
(VQA). O
Given O
an O
image O
and O

and O
open- O
ended O
Visual O
Question O
Answering B-DAT
(VQA). O
A O
VQA O
system O
takes O

now O
describe O
the O
Visual O
Question O
Answering B-DAT
(VQA) O
dataset. O
We O
begin O
by O

the O
task O
of O
Visual O
Question O
Answering B-DAT
(VQA). O
Given O
an O
image O
and O

Paraphrase-Driven O
Learning O
for O
Open O
Question O
Answering B-DAT

and O
O. O
Etzioni. O
Open O
Question O
Answering B-DAT
over O
Curated O
and O
Extracted O
Knowledge O

A O
Multi-World O
Approach O
to O
Question O
Answering B-DAT
about O
Real-World O
Scenes O
based O
on O

Parsing O
for O
Understanding O
Events O
and O
Answering B-DAT
Queries. O
IEEE O
MultiMedia, O
2014. O
1 O

Mikolov. O
Towards O
AI- O
Complete O
Question O
Answering B-DAT

VQA B-DAT

and O
open-ended O
Visual O
Question O
Answering O
(VQA B-DAT

a O
system O
that O
succeeds O
at O
VQA B-DAT
typically O
needs O
a O
more O
detailed O

producing O
generic O
image O
captions. O
Moreover, O
VQA B-DAT
is O
amenable O
to O
automatic O
evaluation O

Numerous O
baselines O
and O
methods O
for O
VQA B-DAT
are O
provided O
and O
compared O
with O

human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

open- O
ended O
Visual O
Question O
Answering O
(VQA B-DAT

). O
A O
VQA B-DAT
system O
takes O
as O
input O
an O

Is O
this O
person O
expecting O
company?”). O
VQA B-DAT
[19], O
[36], O
[50], O
[3] O
is O

the O
high-level O
reasoning O
required O
for O
VQA B-DAT
by O
removing O
the O
need O
to O

29]. O
As O
part O
of O
the O
VQA B-DAT
initiative, O
we O
will O
organize O
an O

state-of-the-art O
methods O
and O
best O
practices. O
VQA B-DAT
poses O
a O
rich O
set O
of O

during O
the O
past O
few O
decades. O
VQA B-DAT
provides O
an O
attractive O
balance O
between O

VQA B-DAT
Efforts. O
Several O
recent O
papers O
have O

difficult O
and O
unconstrained O
task, O
our O
VQA B-DAT
dataset O
is O
two O
orders O
of O

1,449 O
images O
respectively). O
The O
proposed O
VQA B-DAT
task O
has O
connections O
to O
other O

These O
approaches O
provide O
inspiration O
for O
VQA B-DAT
techniques. O
One O
key O
concern O
in O

fixed O
set O
of O
loca- O
tions. O
VQA B-DAT
is O
naturally O
grounded O
in O
images O

Describing O
Visual O
Content. O
Related O
to O
VQA B-DAT
are O
the O
tasks O
of O
image O

by O
[53]). O
The O
questions O
in O
VQA B-DAT
require O
detailed O
specific O
information O
about O

3 O
VQA B-DAT
DATASET O
COLLECTION O

describe O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset. O
We O
begin O
by O
describing O

they O
are O
well-suited O
for O
our O
VQA B-DAT
task. O
The O
more O
diverse O
our O

their O
answers. O
Abstract O
Scenes. O
The O
VQA B-DAT
task O
with O
real O
images O
requires O

the O
high-level O
reasoning O
required O
for O
VQA, B-DAT
but O
not O
the O
low-level O
vision O

test-standard, O
test-challenge, O
test-reserve). O
For O
the O
VQA B-DAT
challenge O
(see O
section O
6), O
test-dev O

default’ O
test O
data O
for O
the O
VQA B-DAT
competition. O
When O
comparing O
to O
the O

sentences O
containing O
multiple O
words. O
In O
VQA, B-DAT
most O
answers O
(89.32%) O
are O
single O

4 O
VQA B-DAT
DATASET O
ANALYSIS O
In O
this O
section O

questions O
and O
answers O
in O
the O
VQA B-DAT
train O
dataset. O
To O
gain O
an O

visual O
information O
is O
critical O
to O
VQA B-DAT
and O
that O
commonsense O
information O
alone O

from O
the O
real O
images O
of O
VQA B-DAT
trainval) O
asking O
subjects O

5 O
VQA B-DAT
BASELINES O
AND O
METHODS O
In O
this O

explore O
the O
difficulty O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

novel O
methods. O
We O
train O
on O
VQA B-DAT
train+val. O
Unless O
stated O
otherwise, O
all O

top O
1K O
answers O
of O
the O
VQA B-DAT
train/val O
dataset O

multiple- O
choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
As O

I O
(Fig. O
8), O
selected O
using O
VQA B-DAT
test-dev O
accuracies) O
on O
VQA O
test O

worse O
than O
human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

system O
is O
correct O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

correct” O
implies O
that O
it O
has O
VQA B-DAT
accuracy O
1.0 O
(see O
section O
3 O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
The O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

frequently O
predicted O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

age O
of O
question) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

system O
is O
correct) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

a O
filtered O
version O
of O
the O
VQA B-DAT
train O
+ O
val O
dataset O
in O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

6 O
VQA B-DAT
CHALLENGE O
AND O
WORKSHOP O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O

task O
of O
Visual O
Question O
Answering O
(VQA B-DAT

multiple-choice O
tasks O
in O
the O
respective O
VQA B-DAT
Real O
Image O
Challenge O
leaderboards O
(as O

datasets O
may O
help O
enable O
practical O
VQA B-DAT
applications. O
We O
believe O
VQA O
has O

questions O
IV O
- O
Details O
on O
VQA B-DAT
baselines O
V O
- O
“Age” O
and O

Leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Open-Ended) O
on O

leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Multiple-Choice) O
on O

Additional O
examples O
from O
the O
VQA B-DAT
dataset O

scenes. O
This O
helps O
motivate O
the O
VQA B-DAT
task O
as O
a O
way O
to O

APPENDIX O
IV: O
DETAILS O
ON O
VQA B-DAT
BASELINES O
“per O
Q-type O
prior” O
baseline O

For O
every O
question O
in O
the O
VQA B-DAT
test-standard O
set, O
we O
find O
its O

norm O
I), O
selected O
using O
VQA B-DAT
test- O
dev O
accuracies). O
To O
estimate O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

a O
random O
selection O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

scenes. O
The O
set O
contains O
over O
100 B-DAT
objects O
and O
31 O
animals O
in O

i.e., O
an O
answer O
is O
deemed O
100 B-DAT

MS O
COCO O
dataset O
[32] O
and O
150, B-DAT

a O
scale O
of O
0 O
− O
100) B-DAT
required O
to O
answer O
a O
question O

choose O
the O
top O
K O
= O
1000 B-DAT
most O
frequent O
answers O
as O
possible O

Question O
(BoW O
Q): O
The O
top O
1,000 B-DAT
words O
in O
the O
questions O
are O

are O
concatenated O
to O
get O
a O
1,030 B-DAT

1000 B-DAT

1000 B-DAT

with O
2 O
hidden O
layers O
and O
1000 B-DAT
hidden O
units O
(dropout O
0.5) O
in O

a O
bag-of-words O
representation O
containing O
the O
1,000 B-DAT
most O
popular O
words O
in O
the O

that O
it O
has O
VQA O
accuracy O
1.0 B-DAT
(see O
section O
3 O
for O
accuracy O

Question O
K O
= O
1000 B-DAT
Human O
To O
Be O
Able O
To O

100 B-DAT
A O

100 B-DAT

100 B-DAT

100 B-DAT

can O
see O
that O
K O
= O
1000 B-DAT
performs O
better O
than O
K O

performs O
better O
then O
K O
= O
1000 B-DAT
by O
0.40% O
for O
open-ended O
task O

blue” O
(28881, O
1.16%), O
“4” O
(27174, O
1.09 B-DAT

outside” O
(1846, O
0.07%), O
“hot O
dog” O
(1809, B-DAT
0.07%), O
“night” O
(1805, O
0.07%), O
“trees O

and O
white” O
(1518, O
0.06%), O
“bedroom” O
(1500, B-DAT
0.06%), O
“bat” O
(1494, O
0.06%), O
“glasses O

0.06%), O
“cloudy” O
(1413, O
0.06%), O
“15” O
(1407, B-DAT
0.06%), O
“up” O
(1399, O
0.06%), O
“blonde O

0.05%), O
“many” O
(1211, O
0.05%), O
“zoo” O
(1204, B-DAT
0.05%), O
“suitcase” O
(1199, O
0.05%), O
“old O

0.04%), O
“mountains” O
(1030, O
0.04%), O
“wall” O
(1009, B-DAT
0.04%), O
“ele- O
phants” O
(1006, O
0.04 O

1504 B-DAT

1504 B-DAT

1408 B-DAT

1506 B-DAT

1409 B-DAT

1502 B-DAT

32] O
and O
a O
newly O
created O
abstract B-DAT
scene O
dataset O
[57], O
[2] O
that O

a O
new O
dataset O
of O
“realistic” O
abstract B-DAT
scenes O
to O
enable O
research O
focused O

output O
answer O
classes. O
[34] O
generates O
abstract B-DAT
scenes O
to O
capture O
visual O
common O

describing O
the O
real O
images O
and O
abstract B-DAT

tasks, O
we O
create O
a O
new O
abstract B-DAT
scenes O
dataset O
[2], O
[57], O
[58 O

winners O
of O
the O
challenge. O
For O
abstract B-DAT
scenes, O
we O
created O
splits O
for O

test-dev, O
test-standard, O
test-challenge, O
test-reserve) O
for O
abstract B-DAT
scenes. O
Captions. O
The O
MS O
COCO O

collected O
five O
single-captions O
for O
all O
abstract B-DAT
scenes O
using O
the O
same O
user O

both O
the O
real O
images O
and O
abstract B-DAT
scenes. O
In O
total, O
three O
questions O

left) O
and O
all O
questions O
for O
abstract B-DAT
scenes O
(right). O
The O
ordering O
of O

1,950,000 O
answers O
for O
50, O
000 O
abstract B-DAT
scenes O

the O
real O
images O
(left) O
and O
abstract B-DAT
scenes O
(right). O
Interestingly, O
the O
distribution O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
This O
helps O
demonstrate O
that O

of O
questions O
elicited O
by O
the O
abstract B-DAT
scenes O
is O
similar O
to O
those O

lengths O
for O
real O
images O
and O
abstract B-DAT
scenes O

90.51%, O
5.89%, O
and O
2.49% O
for O
abstract B-DAT
scenes. O
The O
brevity O
of O
answers O

real O
images O
and O
3,770 O
for O
abstract B-DAT
scenes. O
‘Yes/No’ O
and O
‘Number’ O
Answers O

questions O
on O
real O
images O
and O
abstract B-DAT
scenes O
respectively. O
Among O
these O
‘yes/no O

yes” O
for O
real O
images O
and O
abstract B-DAT
scenes. O
Question O
types O
such O
as O

questions O
on O
real O
images O
and O
abstract B-DAT
scenes O
are O
‘number’ O
questions. O
“2 O

real O
images O
and O
39.85% O
for O
abstract B-DAT
scenes. O
Subject O
Confidence. O
When O
the O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
Inter-human O
Agreement. O
Does O
the O

confident) O
for O
real O
images O
and O
abstract B-DAT
scenes O
(black O
lines). O
Percentage O
of O

both O
real O
images O
(83.30%) O
and O
abstract B-DAT
scenes O
(87.49%). O
Note O
that O
on O

real O
images O
and O
2.39 O
for O
abstract B-DAT
scenes. O
The O
agreement O
is O
significantly O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
See O
the O
appendix O
for O

abstract B-DAT
| O
mc-real O
| O
mc-abstract O

oe-abstract-leaderboard B-DAT
| O
mc-real-leaderboard O
| O
mc- O
abstract O

VI O
- O
Details O
on O
the O
abstract B-DAT
scene O
dataset O
VII O
- O
User O

captions O
collected O
by O
us O
for O
abstract B-DAT
scenes) O
using O
the O
Stanford O
part-of-speech O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
This O
helps O
motivate O
the O

and O
Fig. O
20 O
(adjectives) O
for O
abstract B-DAT
scenes.7 O
The O
left O
side O
shows O

and O
Fig. O
14 O
(right) O
for O
abstract B-DAT
scenes. O
We O
see O
that O
questions O

for O
real O
images O
(left) O
and O
abstract B-DAT
scenes O
(right O

indicating O
the O
normalized O
count O
for O
abstract B-DAT
scenes O

indicating O
the O
normalized O
count O
for O
abstract B-DAT
scenes O

indicating O
the O
normalized O
count O
for O
abstract B-DAT
scenes O

the O
two O
datasets, O
real O
and O
abstract, B-DAT
first O
two O
rows O
are O
the O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
Note O
the O
diversity O
of O

for O
both O
real O
images O
and O
abstract B-DAT
scenes. O
In O
Table O
6, O
we O

and O
≈ O
11% O
increase O
for O
abstract B-DAT
scenes) O
for O
“other” O
questions. O
Since O

that O
are O
present O
in O
the O
abstract B-DAT
scenes O
dataset. O
For O
more O
examples O

left) O
and O
all O
questions O
for O
abstract B-DAT
scenes O
(right). O
The O
ordering O
of O

top) O
and O
all O
questions O
for O
abstract B-DAT
scenes O
(bottom). O
Each O
column O
corresponds O

the O
objects O
present O
in O
the O
abstract B-DAT
scene O
dataset. O
Right: O
The O
AMT O

interface O
for O
collecting O
abstract B-DAT
scenes. O
The O
light O
green O
circles O

the O
MS O
COCO O
[32] O
images, O
abstract B-DAT
scenes, O
and O
multiple-choice O
questions, O
respectively O

numerous O
representative O
examples O
of O
the O
abstract B-DAT
scene O
dataset O

examples O
of O
the O
real O
and O
abstract B-DAT
scene O
dataset O

VQA B-DAT

and O
open-ended O
Visual O
Question O
Answering O
(VQA B-DAT

a O
system O
that O
succeeds O
at O
VQA B-DAT
typically O
needs O
a O
more O
detailed O

producing O
generic O
image O
captions. O
Moreover, O
VQA B-DAT
is O
amenable O
to O
automatic O
evaluation O

Numerous O
baselines O
and O
methods O
for O
VQA B-DAT
are O
provided O
and O
compared O
with O

human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

open- O
ended O
Visual O
Question O
Answering O
(VQA B-DAT

). O
A O
VQA B-DAT
system O
takes O
as O
input O
an O

Is O
this O
person O
expecting O
company?”). O
VQA B-DAT
[19], O
[36], O
[50], O
[3] O
is O

the O
high-level O
reasoning O
required O
for O
VQA B-DAT
by O
removing O
the O
need O
to O

29]. O
As O
part O
of O
the O
VQA B-DAT
initiative, O
we O
will O
organize O
an O

state-of-the-art O
methods O
and O
best O
practices. O
VQA B-DAT
poses O
a O
rich O
set O
of O

during O
the O
past O
few O
decades. O
VQA B-DAT
provides O
an O
attractive O
balance O
between O

VQA B-DAT
Efforts. O
Several O
recent O
papers O
have O

difficult O
and O
unconstrained O
task, O
our O
VQA B-DAT
dataset O
is O
two O
orders O
of O

1,449 O
images O
respectively). O
The O
proposed O
VQA B-DAT
task O
has O
connections O
to O
other O

These O
approaches O
provide O
inspiration O
for O
VQA B-DAT
techniques. O
One O
key O
concern O
in O

fixed O
set O
of O
loca- O
tions. O
VQA B-DAT
is O
naturally O
grounded O
in O
images O

Describing O
Visual O
Content. O
Related O
to O
VQA B-DAT
are O
the O
tasks O
of O
image O

by O
[53]). O
The O
questions O
in O
VQA B-DAT
require O
detailed O
specific O
information O
about O

3 O
VQA B-DAT
DATASET O
COLLECTION O

describe O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset. O
We O
begin O
by O
describing O

they O
are O
well-suited O
for O
our O
VQA B-DAT
task. O
The O
more O
diverse O
our O

their O
answers. O
Abstract O
Scenes. O
The O
VQA B-DAT
task O
with O
real O
images O
requires O

the O
high-level O
reasoning O
required O
for O
VQA, B-DAT
but O
not O
the O
low-level O
vision O

test-standard, O
test-challenge, O
test-reserve). O
For O
the O
VQA B-DAT
challenge O
(see O
section O
6), O
test-dev O

default’ O
test O
data O
for O
the O
VQA B-DAT
competition. O
When O
comparing O
to O
the O

sentences O
containing O
multiple O
words. O
In O
VQA, B-DAT
most O
answers O
(89.32%) O
are O
single O

4 O
VQA B-DAT
DATASET O
ANALYSIS O
In O
this O
section O

questions O
and O
answers O
in O
the O
VQA B-DAT
train O
dataset. O
To O
gain O
an O

visual O
information O
is O
critical O
to O
VQA B-DAT
and O
that O
commonsense O
information O
alone O

from O
the O
real O
images O
of O
VQA B-DAT
trainval) O
asking O
subjects O

5 O
VQA B-DAT
BASELINES O
AND O
METHODS O
In O
this O

explore O
the O
difficulty O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

novel O
methods. O
We O
train O
on O
VQA B-DAT
train+val. O
Unless O
stated O
otherwise, O
all O

top O
1K O
answers O
of O
the O
VQA B-DAT
train/val O
dataset O

multiple- O
choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
As O

I O
(Fig. O
8), O
selected O
using O
VQA B-DAT
test-dev O
accuracies) O
on O
VQA O
test O

worse O
than O
human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

system O
is O
correct O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

correct” O
implies O
that O
it O
has O
VQA B-DAT
accuracy O
1.0 O
(see O
section O
3 O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
The O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

frequently O
predicted O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

age O
of O
question) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

system O
is O
correct) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

a O
filtered O
version O
of O
the O
VQA B-DAT
train O
+ O
val O
dataset O
in O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

6 O
VQA B-DAT
CHALLENGE O
AND O
WORKSHOP O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O

task O
of O
Visual O
Question O
Answering O
(VQA B-DAT

multiple-choice O
tasks O
in O
the O
respective O
VQA B-DAT
Real O
Image O
Challenge O
leaderboards O
(as O

datasets O
may O
help O
enable O
practical O
VQA B-DAT
applications. O
We O
believe O
VQA O
has O

questions O
IV O
- O
Details O
on O
VQA B-DAT
baselines O
V O
- O
“Age” O
and O

Leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Open-Ended) O
on O

leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Multiple-Choice) O
on O

Additional O
examples O
from O
the O
VQA B-DAT
dataset O

scenes. O
This O
helps O
motivate O
the O
VQA B-DAT
task O
as O
a O
way O
to O

APPENDIX O
IV: O
DETAILS O
ON O
VQA B-DAT
BASELINES O
“per O
Q-type O
prior” O
baseline O

For O
every O
question O
in O
the O
VQA B-DAT
test-standard O
set, O
we O
find O
its O

norm O
I), O
selected O
using O
VQA B-DAT
test- O
dev O
accuracies). O
To O
estimate O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

a O
random O
selection O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

choice B-DAT
format. O
We O
provide O
a O
dataset O

answering O
task O
and O
a O
multiple- O
choice B-DAT
task O
[45], O
[33]. O
Unlike O
the O

choice B-DAT
task O
only O
requires O
an O

choice B-DAT
answers). O
These O
approaches O
provide O
inspiration O

choice B-DAT

choice B-DAT
task, O
18 O
candidate O
answers O
are O

answers O
is O
randomized. O
Example O
multiple O
choice B-DAT
questions O
are O
in O
the O
appendix O

choice B-DAT

choice B-DAT
tasks. O
Note O
that O
“yes” O
is O

choice B-DAT
questions O

choice B-DAT
task, O
we O
pick O
the O
answer O

choice B-DAT
task, O
we O
pick O
the O
answer O

for O
the O
open-ended O
and O
multiple- O
choice B-DAT
tasks O
on O
the O
VQA O
test-dev O

choice B-DAT
picks O
the O
answer O
that O
has O

choice B-DAT
tasks O
on O
the O
VQA O
test-dev O

choice B-DAT

choice) B-DAT
and O
LSTM O
Q O
achieving O
48.76 O

choice B-DAT

choice B-DAT

choice B-DAT

choice B-DAT
are O
better O
than O
open-ended. O
All O

choice B-DAT
tasks O
on O
the O
VQA O
test-dev O

choice B-DAT
task O

choice B-DAT
task O

choice B-DAT
task O

choice B-DAT
task O

choice B-DAT
task O

choice B-DAT
task O

choice B-DAT
tasks O
on O
the O
VQA O
test-dev O

and O
by O
1.88% O
for O
multiple- O
choice B-DAT
task O

choice B-DAT

choice B-DAT
tasks O
(real O
images) O
with O
other O

choice B-DAT
tasks O
in O
the O
respective O
VQA O

choice B-DAT
questions O
IV O
- O
Details O
on O

choice B-DAT
questions O
when O
subjects O
were O
shown O

choice B-DAT
questions, O
we O
collected O
three O
human O

human O
accuracies O
for O
mul- O
tiple O
choice B-DAT
questions. O
Table O
6 O
also O
shows O

choice B-DAT
accuracies O
are O
more O
or O
less O

increase O
in O
accuracy O
using O
multiple O
choice B-DAT
is O
not O
surprising O

plausible, O
answers O
for O
the O
multiple- O
choice B-DAT
task O
and O
to O
assess O
how O

choice B-DAT
answers O

choice B-DAT
questions, O
respectively O

choice B-DAT
questions O
for O
numerous O
representative O
examples O

VQA: O
Visual O
Question B-DAT
Answering O
www.visualqa.org O

of O
free-form O
and O
open-ended O
Visual O
Question B-DAT
Answering O
(VQA). O
Given O
an O
image O

free-form O
and O
open- O
ended O
Visual O
Question B-DAT
Answering O
(VQA). O
A O
VQA O
system O

We O
now O
describe O
the O
Visual O
Question B-DAT
Answering O
(VQA) O
dataset. O
We O
begin O

Types O
of O
Question B-DAT

of O
Words O
in O
Question B-DAT

Distribution O
of O
Question B-DAT
Lengths O

real O
images O
and O
abstract O
scenes. O
Question B-DAT
types O
such O
as O
“How O
many O

As O
shown O
in O
Table O
1 O
(Question B-DAT
+ O
Image), O
there O
is O
significant O

Fig. O
2). O
In O
Table O
1 O
(Question), B-DAT
we O
show O
the O
percentage O
of O

Question B-DAT
40.81 O
67.60 O
25.77 O
21.22 O
Real O

Question B-DAT
+ O
Caption* O
57.47 O
78.97 O
39.68 O

Question B-DAT
+ O
Image O
83.30 O
95.77 O
83.39 O

Question B-DAT
43.27 O
66.65 O
28.52 O
23.66 O
Abstract O

Question B-DAT
+ O
Caption* O
54.34 O
74.70 O
41.19 O

Question B-DAT
+ O
Image O
87.49 O
95.96 O
95.04 O

question O
without O
seeing O
the O
image O
(Question), B-DAT
seeing O
just O
a O
caption O
of O

and O
not O
the O
image O
itself O
(Question B-DAT
+ O
Caption), O
and O
seeing O
the O

image O
(Question B-DAT
+ O
Image). O
Results O
are O
shown O

answer O
the O
questions? O
Table O
1 O
(Question B-DAT
+ O
Caption) O
shows O
the O
percentage O

Question B-DAT
Channel: O
This O
channel O
provides O
an O

1) O
Bag-of-Words O
Question B-DAT
(BoW O
Q): O
The O
top O
1,000 O

caption O
embedding O
(Caption). O
For O
BoW O
Question B-DAT
+ O
Caption O
(BoW O
Q O

for O
real O
images. O
Q O
= O
Question, B-DAT
I O
= O
Image, O
C O

Question B-DAT
K O
= O
1000 O
Human O
To O

for O
real O
images. O
Q O
= O
Question, B-DAT
I O
= O
Image. O
See O
text O

introduce O
the O
task O
of O
Visual O
Question B-DAT
Answering O
(VQA). O
Given O
an O
image O

Etzioni. O
Paraphrase-Driven O
Learning O
for O
Open O
Question B-DAT
Answering. O
In O
ACL, O
2013. O
2 O

Zettlemoyer, O
and O
O. O
Etzioni. O
Open O
Question B-DAT
Answering O
over O
Curated O
and O
Extracted O

Fritz. O
A O
Multi-World O
Approach O
to O
Question B-DAT
Answering O
about O
Real-World O
Scenes O
based O

T. O
Mikolov. O
Towards O
AI- O
Complete O
Question B-DAT
Answering: O
A O
Set O
of O
Prerequisite O

VQA: O
Visual B-DAT
Question O
Answering O
www.visualqa.org O

task O
of O
free-form O
and O
open-ended O
Visual B-DAT
Question O
Answering O
(VQA). O
Given O
an O

questions O
and O
answers O
are O
open-ended. O
Visual B-DAT
questions O
selectively O
target O
different O
areas O

of O
free-form O
and O
open- O
ended O
Visual B-DAT
Question O
Answering O
(VQA). O
A O
VQA O

complex O
reasoning O
more O
essential. O
Describing O
Visual B-DAT
Content. O
Related O
to O
VQA O
are O

We O
now O
describe O
the O
Visual B-DAT
Question O
Answering O
(VQA) O
dataset. O
We O

we O
introduce O
the O
task O
of O
Visual B-DAT
Question O
Answering O
(VQA). O
Given O
an O

cloud O
service. O
In O
Mobile O
Cloud O
Visual B-DAT
Media O
Computing, O
pages O
265–290. O
Springer O

D. O
Parikh. O
Zero-Shot O
Learning O
via O
Visual B-DAT
Abstraction. O
In O
ECCV, O
2014. O
2 O

Nearly O
Real- O
time O
Answers O
to O
Visual B-DAT
Questions. O
In O
User O
Interface O
Software O

and O
A. O
Gupta. O
NEIL: O
Extracting O
Visual B-DAT
Knowledge O
from O
Web O
Data. O
In O

Zitnick. O
Mind’s O
Eye: O
A O
Recurrent O
Visual B-DAT
Represen- O
tation O
for O
Image O
Caption O

Long-term O
Recurrent O
Convolutional O
Networks O
for O
Visual B-DAT
Recognition O
and O
Description. O
In O
CVPR O

G. O
Zweig. O
From O
Captions O
to O
Visual B-DAT
Concepts O
and O
Back. O
In O
CVPR O

Hallonquist, O
and O
L. O
Younes. O
A O
Visual B-DAT
Turing O
Test O
for O
Computer O
Vision O

Karpathy O
and O
L. O
Fei-Fei. O
Deep O
Visual B-DAT

and O
R. O
S. O
Zemel. O
Unifying O
Visual B-DAT

Listen, O
Use O
Your O
Imagination: O
Leveraging O
Visual B-DAT
Common O
Sense O
for O
Non-Visual O
Tasks O

Divvala, O
and O
A. O
Farhadi. O
Viske: O
Visual B-DAT
knowledge O
extraction O
and O
question O
answering O

Berg, O
and O
T. O
L. O
Berg. O
Visual B-DAT
madlibs: O
Fill-in-the- O
blank O
description O
generation O

Bringing O
Semantics O
Into O
Focus O
Using O
Visual B-DAT
Abstraction. O
In O
CVPR, O
2013. O
2 O

and O
L. O
Vanderwende. O
Learning O
the O
Visual B-DAT
Interpretation O
of O
Sentences. O
In O
ICCV O

can O
be O
provided O
in O
a O
multiple B-DAT

open-ended O
answering O
task O
and O
a O
multiple B-DAT

requires O
a O
free-form O
response, O
the O
multiple B-DAT

sentence O
completion O
(e.g., O
[45] O
with O
multiple B-DAT

gathered O
to O
find O
images O
containing O
multiple B-DAT
objects O
and O
rich O
contextual O
information O

tions: O
(i) O
open-ended O
and O
(ii) O
multiple B-DAT

and O
reliable O
for O
sentences O
containing O
multiple B-DAT
words. O
In O
VQA, O
most O
answers O

image O
caption O
evaluation O
[6]. O
For O
multiple B-DAT

the O
answers O
is O
randomized. O
Example O
multiple B-DAT
choice O
questions O
are O
in O
the O

according O
to O
the O
accuracy O
metric, O
multiple B-DAT
options O
could O
have O
a O
non-zero O

answers O
are O
free-form O
and O
not O
multiple B-DAT

for O
both O
the O
open-ended O
and O
multiple B-DAT

of O
the O
choices O
for O
the O
multiple B-DAT

appendix O
for O
details). O
For O
the O
multiple B-DAT

Q-type O
prior” O
baseline, O
for O
the O
multiple B-DAT

methods O
for O
the O
open-ended O
and O
multiple B-DAT

possible O
K O
answers O
and O
multiple B-DAT

for O
both O
the O
open-ended O
and O
multiple B-DAT

rather O
poorly O
(open-ended: O
28.13% O
/ O
multiple B-DAT

48.09% O
on O
open-ended O
(53.68% O
on O
multiple B-DAT

on O
open- O
ended O
(54.75% O
on O
multiple B-DAT

nearest O
neighbor O
baseline O
(open-ended: O
42.70%, O
multiple B-DAT

is O
58.16% O
(open-ended) O
/ O
63.09% O
(multiple B-DAT

a O
general O
trend, O
results O
on O
multiple B-DAT

both O
the O
open- O
ended O
and O
multiple B-DAT

task O
and O
by O
0.24% O
for O
multiple B-DAT

task O
and O
by O
1.24% O
for O
multiple B-DAT

task O
and O
by O
1.92% O
for O
multiple B-DAT

task O
and O
by O
1.16% O
for O
multiple B-DAT

task O
and O
by O
0.17% O
for O
multiple B-DAT

task O
and O
by O
0.02% O
for O
multiple B-DAT

I) O
for O
the O
open-ended O
and O
multiple B-DAT

task O
and O
by O
1.88% O
for O
multiple B-DAT

of O
leaderboards O
for O
open-ended-real O
and O
multiple B-DAT

I) O
for O
both O
open-ended O
and O
multiple B-DAT

entries O
for O
the O
open-ended O
and O
multiple B-DAT

III O
- O
Human O
accuracy O
on O
multiple B-DAT

are O
the O
human O
accuracies O
for O
multiple B-DAT

To O
compute O
human O
accuracy O
for O
multiple B-DAT

to O
open- O
ended O
answer, O
the O
multiple B-DAT

the O
increase O
in O
accuracy O
using O
multiple B-DAT
choice O
is O
not O
surprising O

but O
plausible, O
answers O
for O
the O
multiple B-DAT

collect O
the O
plausible, O
but O
incorrect, O
multiple B-DAT

32] O
images, O
abstract O
scenes, O
and O
multiple B-DAT

Fig. O
29: O
Random O
examples O
of O
multiple B-DAT

204,721 O
images O
from O
the O
MS O
COCO B-DAT
dataset O
[32] O
and O
a O
newly O

contains O
50,000 O
scenes. O
The O
MS O
COCO B-DAT
dataset O
has O
images O
depicting O
diverse O

to O
English O
by O
humans) O
for O
COCO B-DAT
images. O
[44] O
automatically O
generated O
four O

object, O
count, O
color, O
location) O
using O
COCO B-DAT
captions. O
Text-based O
Q&A O
is O
a O

Common O
Objects O
in O
Context O
(MS O
COCO) B-DAT
[32] O
dataset. O
The O
MS O
COCO O

split O
strategy O
as O
the O
MC O
COCO B-DAT
dataset O
[32] O
(including O
test- O
dev O

abstract O
scenes. O
Captions. O
The O
MS O
COCO B-DAT
dataset O
[32], O
[7] O
already O
contains O

204,721 O
images O
from O
the O
MS O
COCO B-DAT
dataset O
[32] O
and O
150,000 O
questions O

VQA O
dataset O
for O
the O
MS O
COCO B-DAT
images O
using O
several O
baselines O
and O

from O
the O
caption O
data O
(MS O
COCO B-DAT
captions O
for O
real O
images O
and O

VQA O
dataset O
for O
the O
MS O
COCO B-DAT
[32] O
images, O
abstract O
scenes, O
and O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT
captions: O
Data O
collection O
and O
evaluation O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT
Captions: O
Data O
Collection O
and O
Evaluation O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT

VQA: O
Visual O
Question O
Answering B-DAT
www.visualqa.org O

free-form O
and O
open-ended O
Visual O
Question O
Answering B-DAT
(VQA). O
Given O
an O
image O
and O

and O
open- O
ended O
Visual O
Question O
Answering B-DAT
(VQA). O
A O
VQA O
system O
takes O

now O
describe O
the O
Visual O
Question O
Answering B-DAT
(VQA) O
dataset. O
We O
begin O
by O

the O
task O
of O
Visual O
Question O
Answering B-DAT
(VQA). O
Given O
an O
image O
and O

Paraphrase-Driven O
Learning O
for O
Open O
Question O
Answering B-DAT

and O
O. O
Etzioni. O
Open O
Question O
Answering B-DAT
over O
Curated O
and O
Extracted O
Knowledge O

A O
Multi-World O
Approach O
to O
Question O
Answering B-DAT
about O
Real-World O
Scenes O
based O
on O

Parsing O
for O
Understanding O
Events O
and O
Answering B-DAT
Queries. O
IEEE O
MultiMedia, O
2014. O
1 O

Mikolov. O
Towards O
AI- O
Complete O
Question O
Answering B-DAT

VQA B-DAT

and O
open-ended O
Visual O
Question O
Answering O
(VQA B-DAT

a O
system O
that O
succeeds O
at O
VQA B-DAT
typically O
needs O
a O
more O
detailed O

producing O
generic O
image O
captions. O
Moreover, O
VQA B-DAT
is O
amenable O
to O
automatic O
evaluation O

Numerous O
baselines O
and O
methods O
for O
VQA B-DAT
are O
provided O
and O
compared O
with O

human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

open- O
ended O
Visual O
Question O
Answering O
(VQA B-DAT

). O
A O
VQA B-DAT
system O
takes O
as O
input O
an O

Is O
this O
person O
expecting O
company?”). O
VQA B-DAT
[19], O
[36], O
[50], O
[3] O
is O

the O
high-level O
reasoning O
required O
for O
VQA B-DAT
by O
removing O
the O
need O
to O

29]. O
As O
part O
of O
the O
VQA B-DAT
initiative, O
we O
will O
organize O
an O

state-of-the-art O
methods O
and O
best O
practices. O
VQA B-DAT
poses O
a O
rich O
set O
of O

during O
the O
past O
few O
decades. O
VQA B-DAT
provides O
an O
attractive O
balance O
between O

VQA B-DAT
Efforts. O
Several O
recent O
papers O
have O

difficult O
and O
unconstrained O
task, O
our O
VQA B-DAT
dataset O
is O
two O
orders O
of O

1,449 O
images O
respectively). O
The O
proposed O
VQA B-DAT
task O
has O
connections O
to O
other O

These O
approaches O
provide O
inspiration O
for O
VQA B-DAT
techniques. O
One O
key O
concern O
in O

fixed O
set O
of O
loca- O
tions. O
VQA B-DAT
is O
naturally O
grounded O
in O
images O

Describing O
Visual O
Content. O
Related O
to O
VQA B-DAT
are O
the O
tasks O
of O
image O

by O
[53]). O
The O
questions O
in O
VQA B-DAT
require O
detailed O
specific O
information O
about O

3 O
VQA B-DAT
DATASET O
COLLECTION O

describe O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset. O
We O
begin O
by O
describing O

they O
are O
well-suited O
for O
our O
VQA B-DAT
task. O
The O
more O
diverse O
our O

their O
answers. O
Abstract O
Scenes. O
The O
VQA B-DAT
task O
with O
real O
images O
requires O

the O
high-level O
reasoning O
required O
for O
VQA, B-DAT
but O
not O
the O
low-level O
vision O

test-standard, O
test-challenge, O
test-reserve). O
For O
the O
VQA B-DAT
challenge O
(see O
section O
6), O
test-dev O

default’ O
test O
data O
for O
the O
VQA B-DAT
competition. O
When O
comparing O
to O
the O

sentences O
containing O
multiple O
words. O
In O
VQA, B-DAT
most O
answers O
(89.32%) O
are O
single O

4 O
VQA B-DAT
DATASET O
ANALYSIS O
In O
this O
section O

questions O
and O
answers O
in O
the O
VQA B-DAT
train O
dataset. O
To O
gain O
an O

visual O
information O
is O
critical O
to O
VQA B-DAT
and O
that O
commonsense O
information O
alone O

from O
the O
real O
images O
of O
VQA B-DAT
trainval) O
asking O
subjects O

5 O
VQA B-DAT
BASELINES O
AND O
METHODS O
In O
this O

explore O
the O
difficulty O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

novel O
methods. O
We O
train O
on O
VQA B-DAT
train+val. O
Unless O
stated O
otherwise, O
all O

top O
1K O
answers O
of O
the O
VQA B-DAT
train/val O
dataset O

multiple- O
choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
As O

I O
(Fig. O
8), O
selected O
using O
VQA B-DAT
test-dev O
accuracies) O
on O
VQA O
test O

worse O
than O
human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

system O
is O
correct O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

correct” O
implies O
that O
it O
has O
VQA B-DAT
accuracy O
1.0 O
(see O
section O
3 O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
The O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

frequently O
predicted O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

age O
of O
question) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

system O
is O
correct) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

a O
filtered O
version O
of O
the O
VQA B-DAT
train O
+ O
val O
dataset O
in O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

6 O
VQA B-DAT
CHALLENGE O
AND O
WORKSHOP O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O

task O
of O
Visual O
Question O
Answering O
(VQA B-DAT

multiple-choice O
tasks O
in O
the O
respective O
VQA B-DAT
Real O
Image O
Challenge O
leaderboards O
(as O

datasets O
may O
help O
enable O
practical O
VQA B-DAT
applications. O
We O
believe O
VQA O
has O

questions O
IV O
- O
Details O
on O
VQA B-DAT
baselines O
V O
- O
“Age” O
and O

Leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Open-Ended) O
on O

leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Multiple-Choice) O
on O

Additional O
examples O
from O
the O
VQA B-DAT
dataset O

scenes. O
This O
helps O
motivate O
the O
VQA B-DAT
task O
as O
a O
way O
to O

APPENDIX O
IV: O
DETAILS O
ON O
VQA B-DAT
BASELINES O
“per O
Q-type O
prior” O
baseline O

For O
every O
question O
in O
the O
VQA B-DAT
test-standard O
set, O
we O
find O
its O

norm O
I), O
selected O
using O
VQA B-DAT
test- O
dev O
accuracies). O
To O
estimate O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

a O
random O
selection O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

Differential O
Network O
for O
Visual O
Question O
Generation B-DAT

Namboodiri, O
2018). O
However, O
Visual O
Question O
Generation B-DAT
(VQG) O
is O
a O
separate O
task O

Differential O
Network O
for O
Visual O
Question O
Generation B-DAT

our O
experiments O
on O
Visual O
Question O
Generation B-DAT
(VQG) O
dataset O
(Mostafazadeh O
et O
al O

D.2 O
Question O
Generation B-DAT
approaches: O
Sampling O
vs O
Argmax O

Multimodal O
Differential O
Network O
for O
Visual B-DAT
Question I-DAT
Generation I-DAT

Patro O
and O
Namboodiri, O
2018). O
However, O
Visual B-DAT
Question I-DAT
Generation I-DAT
(VQG) O
is O
a O
separate O
task O

our O
Multimodal O
Differential O
Network O
for O
Visual B-DAT
Question I-DAT
Generation I-DAT

We O
conduct O
our O
experiments O
on O
Visual B-DAT
Question I-DAT
Generation I-DAT
(VQG) O
dataset O
(Mostafazadeh O
et O
al O

Multimodal O
Differential O
Network O
for O
Visual B-DAT
Question I-DAT
Generation I-DAT

captioning, O
paragraph O
gen- O
eration, O
Visual O
Question B-DAT
Answering O
(VQA) O
and O
Visual O
Dialog O

and O
Namboodiri, O
2018). O
However, O
Visual B-DAT
Question I-DAT
Generation I-DAT
(VQG) O
is O
a O
separate O

Multimodal O
Differential O
Network O
for O
Visual B-DAT
Question I-DAT
Generation. I-DAT
It O
consists O
of O
a O

4.2 O
Decoder: O
Question B-DAT
Generator O
The O
role O
of O
decoder O

conduct O
our O
experiments O
on O
Visual B-DAT
Question I-DAT
Generation I-DAT
(VQG) O
dataset O
(Mostafazadeh O
et O

Devi O
Parikh. O
2015. O
VQA: O
Visual O
Question B-DAT
An- O
swering. O
In O
International O
Conference O

Human O
Atten- O
tion O
in O
Visual O
Question B-DAT
Answering: O
Do O
Humans O
and O
Deep O

various O
tags O
(Noun, O
Verb, O
and O
Question B-DAT
tags) O
and O
different O
ways O
of O

14: O
Compute O
Decode O
Question B-DAT
Sentence: O
15: O
ŷ O
= O
Generating O

Noun O
tag, O
Verb O
tag O
and O
Question B-DAT
tags O
(What, O
Where O

D.2 O
Question B-DAT
Generation O
approaches: O
Sampling O
vs O
Argmax O

skateboarding O
around O
little O
cones. O
Our O
Question B-DAT

a O
pair O
of O
skis. O
Our O
Question B-DAT

Multimodal O
Differential O
Network O
for O
Visual B-DAT
Question O
Generation O

answering O
(Antol O
et O
al., O
2015). O
Visual B-DAT
Dialog O
(Das O
et O
al., O
2017 O

image O
captioning, O
paragraph O
gen- O
eration, O
Visual B-DAT
Question O
Answering O
(VQA) O
and O
Visual O

been O
many O
works O
for O
solving O
Visual B-DAT
Dialog O
(Chappell O
et O
al., O
2004 O

Patro O
and O
Namboodiri, O
2018). O
However, O
Visual B-DAT
Question O
Generation O
(VQG) O
is O
a O

our O
Multimodal O
Differential O
Network O
for O
Visual B-DAT
Question O
Generation. O
It O
consists O
of O

We O
conduct O
our O
experiments O
on O
Visual B-DAT
Question O
Generation O
(VQG) O
dataset O
(Mostafazadeh O

and O
Devi O
Parikh. O
2015. O
VQA: O
Visual B-DAT
Question O
An- O
swering. O
In O
International O

2016. O
Human O
Atten- O
tion O
in O
Visual B-DAT
Question O
Answering: O
Do O
Humans O
and O

Parikh, O
and O
Dhruv O
Batra. O
2017. O
Visual B-DAT
Dialog. O
In O
IEEE O
Conference O
on O

Hallonquist, O
and O
Laurent O
Younes. O
2015. O
Visual B-DAT
turing O
test O
for O
com- O
puter O

Visual B-DAT
object O
and O
scene O
recognition O
plays O

to O
solve O
the O
task O
of O
visual B-DAT
question I-DAT
generation I-DAT

The O
task O
in O
visual B-DAT
question I-DAT
generation I-DAT
(VQG) O
is O
to O
generate O
a O

Modeling B-DAT
Relationships O
in O
Referential O
Expressions O

with B-DAT
Compositional O
Modular O
Networks O
Ronghang O
Hu1 O
Marcus O
Rohrbach1 O
Jacob O

Andreas1 B-DAT
Trevor O
Darrell1 O
Kate O
Saenko2 O

1University B-DAT
of O
California, O
Berkeley O
2Boston O

University B-DAT

ronghang,rohrbach,jda,trevor}@eecs.berkeley.edu, B-DAT
saenko@bu.edu O
Abstract O

People B-DAT
often O
refer O
to O
entities O

in B-DAT
an O
image O
in O
terms O

of B-DAT
their O
relationships O
with O
other O

entities. B-DAT
For O
example, O
the O
black O

cat B-DAT
sitting O
under O
the O
table O

refers B-DAT
to O
both O
a O
black O

cat B-DAT
entity O
and O
its O
relationship O

with B-DAT
another O
table O
entity. O
Understanding O

these B-DAT
relationships O
is O
essential O
for O

interpreting B-DAT
and O
ground- O
ing O
such O

natural B-DAT
language O
expressions. O
Most O
prior O

work B-DAT
focuses O
on O
either O
grounding O

entire B-DAT
referential O
expressions O
holistically O
to O

one B-DAT
region, O
or O
localizing O
relationships O

based B-DAT
on O
a O
fixed O
set O

of B-DAT
categories. O
In O
this O
paper O

we B-DAT
instead O
present O
a O
modular O

deep B-DAT
architecture O
capable O
of O
analyzing O

referen- B-DAT
tial O
expressions O
into O
their O

component B-DAT
parts, O
identifying O
en- O
tities O

and B-DAT
relationships O
mentioned O
in O
the O

input B-DAT
expression O
and O
grounding O
them O

all B-DAT
in O
the O
scene. O
We O

call B-DAT
this O
approach O
Compositional O
Modular O

Networks B-DAT
(CMNs): O
a O
novel O
archi- O

tecture B-DAT
that O
learns O
linguistic O
analysis O

and B-DAT
visual O
inference O
end-to-end. O
Our O

approach B-DAT
is O
built O
around O
two O

types B-DAT
of O
neu- O
ral O
modules O

that B-DAT
inspect O
local O
regions O
and O

pairwise B-DAT
interac- O
tions O
between O
regions. O

We B-DAT
evaluate O
CMNs O
on O
multiple O

ref- B-DAT
erential O
expression O
datasets, O
outperforming O

state-of-the-art B-DAT
approaches O
on O
all O
tasks. O
1. O
Introduction O

Great B-DAT
progress O
has O
been O
made O

on B-DAT
object O
detection, O
the O
task O

of B-DAT
localizing O
visual O
entities O
belonging O

to B-DAT
a O
pre-defined O
set O
of O

categories B-DAT
[8, O
24, O
23, O
6, O
17 O

]. B-DAT
But O
the O
more O
general O

and B-DAT
challenging O
task O
of O
localizing O

entities B-DAT
based O
on O
arbi- O
trary O

natural B-DAT
language O
expressions O
remains O
far O

from B-DAT
solved. O
This O
task, O
sometimes O

known B-DAT
as O
grounding O
or O
referential O

ex- B-DAT
pression O
comprehension, O
has O
been O

explored B-DAT
by O
recent O
work O
in O

both B-DAT
computer O
vision O
and O
natural O

language B-DAT
processing O
[20, O
11, O
25]. O

Given B-DAT
an O
image O
and O
a O

natural B-DAT
language O
ex- O
pression O
referring O

to B-DAT
a O
visual O
entity, O
such O

as B-DAT
the O
young O
man O
wearing O

green B-DAT
shirt O
and O
riding O
a O

black B-DAT
bicycle, O
these O
ap- O
proaches O

localize B-DAT
the O
image O
region O
corresponding O

to B-DAT
the O
en- O
tity O
that O

the B-DAT
expression O
refers O
to O
with O

a B-DAT
bounding O
box. O
Referential O
expressions O
often O
describe O
relationships O

be B-DAT

- B-DAT
output O
top O
region O
pair O

final B-DAT
scores O
on O
region O
pairs O
input O
image O

expression B-DAT
parsing O
with O
attention O
subj: O

the B-DAT
woman O
holding O
a O
grey O

rel: B-DAT
the O
woman O
holding O
a O

grey B-DAT
umbrella O
obj: O
the O
woman O
holding O
a O

grey B-DAT
umbrella O

input B-DAT
expression O
the O
woman O
holding O

a B-DAT
grey O
umbrella O
qsubj O

candidate B-DAT

localization B-DAT
module O

relationship B-DAT
module O

subject B-DAT
scores O

unary) B-DAT
qrel O
qobj O

localization B-DAT
module O

relationship B-DAT
scores O

pairwise) B-DAT
object O

scores B-DAT
(unary O

) B-DAT
region O
pairs O

Figure B-DAT
1. O
Given O
an O
image O

and B-DAT
an O
expression, O
we O
learn O

to B-DAT
parse O
the O
expression O
into O

vector B-DAT
representation O
of O
subject O
qsubj O
, O
relationship O
qrel O
and O
object O
qobj O

with B-DAT
attention, O
and O
align O
these O

textual B-DAT
compo- O
nents O
to O
image O

regions B-DAT
with O
two O
types O
of O

modules. B-DAT
The O
localization O
module O
outputs O

scores B-DAT
over O
each O
individual O
region O

while B-DAT
the O
rela- O
tionship O
module O

produces B-DAT
scores O
over O
region O
pairs O

. B-DAT
These O
outputs O
are O
integrated O

into B-DAT
final O
scores O
over O
region O

pairs, B-DAT
producing O
the O
top O
region O

pair B-DAT
as O
grounding O
result. O
(Best O

viewed B-DAT
in O
color.) O
tween O
multiple O
entities O
in O
an O

image. B-DAT
In O
Figure O
1, O
for O

ex- B-DAT
ample, O
the O
expression O
the O

woman B-DAT
holding O
a O
grey O
umbrella O

describes B-DAT
a O
woman O
entity O
that O

participates B-DAT
in O
a O
holding O
re O

- B-DAT
lationship O
with O
a O
grey O

umbrella B-DAT
entity. O
Because O
there O
are O

multiple B-DAT
women O
in O
the O
image, O

resolving B-DAT
this O
referential O
ex- O
pression O

requires B-DAT
both O
finding O
a O
bounding O

box B-DAT
that O
contains O
a O
person, O

and B-DAT
ensuring O
that O
this O
bounding O

box B-DAT
relates O
in O
the O
ar O
X O

iv B-DAT
:1 O
61 O
1 O

. B-DAT
09 O
97 O

8v B-DAT
1 O

cs B-DAT
.C O
V O

3 B-DAT
0 O

N B-DAT
ov O

2 B-DAT
01 O
6 O

right B-DAT
way O
to O
other O
objects O

in B-DAT
the O
scene. O
Previous O
work O

on B-DAT
grounding O
referential O
expressions O
either O
(1 O

) B-DAT
treats O
referen- O
tial O
expressions O

holistically, B-DAT
thus O
failing O
to O
model O

explicit B-DAT
correspondence O
between O
textual O
components O

and B-DAT
visual O
en- O
tities O
in O

the B-DAT
image O
[20, O
11, O
25, O
30, O
21], O
or O
else O
(2) O
relies O

on B-DAT
a O
fixed O
set O
of O

entity B-DAT
and O
relationship O
categories O
defined O

a B-DAT
priori O
[18 O

]. B-DAT
In O
this O
paper, O
we O
present O

a B-DAT
joint O
approach O
that O
explic O

- B-DAT
itly O
models O
the O
compositional O

linguistic B-DAT
structure O
of O
referen- O
tial O

expressions B-DAT
and O
their O
groundings, O
but O

which B-DAT
nonetheless O
supports O
interpretation O
of O

arbitrary B-DAT
language. O
We O
focus O
on O

referential B-DAT
expressions O
involving O
inter-object O
relationships O

that B-DAT
can O
be O
represented O
as O

a B-DAT
subject O
entity, O
a O
relationship O

and B-DAT
an O
object O
entity. O
We O

propose B-DAT
Compositional O
Modular O
Net- O
works O
( O

CMNs), B-DAT
an O
end-to-end O
trained O
model O

that B-DAT
learns O
lan- O
guage O
representation O

and B-DAT
image O
region O
localization O
jointly O

as B-DAT
shown O
in O
Figure O
1. O

Our B-DAT
model O
differentiably O
parses O
the O

referential B-DAT
expression O
into O
a O
subject, O

relationship B-DAT
and O
ob- O
ject O
with O

three B-DAT
soft O
attention O
maps, O
and O

aligns B-DAT
the O
extracted O
textual O
representations O

with B-DAT
image O
regions O
using O
a O

modu- B-DAT
lar O
neural O
architecture. O
There O

are B-DAT
two O
types O
of O
modules O

in B-DAT
our O
model, O
one O
used O

for B-DAT
localizing O
specific O
textual O
compo- O

nents B-DAT
by O
outputting O
unary O
scores O

over B-DAT
regions O
for O
that O
com- O

ponent, B-DAT
and O
one O
for O
determining O

the B-DAT
relationship O
between O
two O
pairs O

of B-DAT
bounding O
boxes O
by O
outputting O

pairwise B-DAT
scores O
over O
region-region O
pairs. O

We B-DAT
evaluate O
our O
model O
on O

mul- B-DAT
tiple O
datasets O
containing O
referential O

expressions, B-DAT
and O
show O
that O
our O

model B-DAT
outperforms O
both O
natural O
baselines O

and B-DAT
pre- O
vious O
work. O
2. O
Related O
work O
Grounding O
referential O

expressions. B-DAT
The O
problem O
of O

grounding B-DAT
referential O
expressions O
can O
be O

naturally B-DAT
formu- O
lated O
as O
a O

retrieval B-DAT
problem O
over O
image O

regions B-DAT
[20, O
11, O
25, O
7, O
30, O
21]. O
First, O
a O
set O
of O

candidate B-DAT
regions O
are O
extracted O
(e.g O

. B-DAT
via O
object O
proposal O
methods O

like B-DAT
[28, O
4, O
13, O
33]). O

Next, B-DAT
each O
candidate O
region O
is O

scored B-DAT
by O
a O
model O
with O

respect B-DAT
to O
the O
query O
expression, O

returning B-DAT
the O
highest O
scoring O
candi- O

date B-DAT
as O
the O
grounding O
result. O

In B-DAT
[20, O
11], O
each O
region O

is B-DAT
scored O
based O
on O
its O

local B-DAT
visual O
features O
and O
some O

global B-DAT
contextual O
features O
from O
the O

whole B-DAT
image. O
However, O
local O
visual O

features B-DAT
and O
global O
contextual O
from O

the B-DAT
whole O
image O
are O
often O

insufficient B-DAT
to O
determine O
whether O
a O

region B-DAT
matches O
an O
expression, O
as O

relationships B-DAT
with O
other O
regions O
in O

the B-DAT
im- O
age O
must O
also O

be B-DAT
considered. O
Two O
recent O

methods B-DAT
[30, O
21] O
go O
beyond O

local B-DAT
visual O
features O
in O
a O

single B-DAT
region, O
and O
con- O
sider O

multiple B-DAT
regions O
at O
the O
same O

time. B-DAT
[30] O
adds O
contex- O
tual O

feature B-DAT
extracted O
from O
other O
regions O

in B-DAT
the O
image, O
and O
[21] O

proposes B-DAT
a O
model O
that O
grounds O

a B-DAT
referential O
expression O
into O
a O

pair B-DAT
of O
regions. O
All O
these O

methods B-DAT
represent O
language O
holistically O
using O

a B-DAT
recurrent O
neural O
network: O
either O

gener- B-DAT
atively, O
by O
predicting O
a O

distribution B-DAT
over O
referential O
expres- O

sions B-DAT
[20, O
11, O
30, O
21], O

or B-DAT
discriminatively, O
by O
encoding O
ex- O
pressions O
into O
a O
vector O
representation O

[25, B-DAT
7]. O
This O
makes O
it O

difficult B-DAT
to O
learn O
explicit O
correspondences O

between B-DAT
the O
com- O
ponents O
in O

the B-DAT
textual O
expression O
and O
entities O

in B-DAT
the O
image. O
In O
this O

work, B-DAT
we O
learn O
to O
parse O

the B-DAT
language O
expression O
into O
textual O

components B-DAT
in O
instead O
of O
treating O

it B-DAT
as O
a O
whole, O
and O

align B-DAT
these O
components O
with O
image O

regions B-DAT
end-to-end O

. B-DAT
Handling O
inter-object O
relationships. O
Recently O
work O

by B-DAT
[18] O
trains O
detectors O
based O

on B-DAT
RCNN O
[8] O
and O
uses O

a B-DAT
linguis- O
tic O
prior O
to O

detect B-DAT
visual O
relationships. O
However, O
this O

work B-DAT
relies O
on O
fixed, O
predefined O

categories B-DAT
for O
subjects, O
relations, O
and O

objects, B-DAT
treating O
entities O
like O
“bicycle O

” B-DAT
and O
relationships O
like O

and B-DAT
“riding” O
as O
discrete O
classes. O

Instead B-DAT
of O
building O
upon O
a O

fixed B-DAT
inventory O
of O
classes, O
our O

model B-DAT
handles O
re- O
lationships O
specified O

by B-DAT
arbitrary O
natural O
language O
phrases, O

and B-DAT
jointly O
learns O
expression O
parsing O

and B-DAT
visual O
entity O
lo- O
calization. O

Although B-DAT
[15] O
also O
learns O
language O

parsing B-DAT
and O
perception, O
it O
is O

directly B-DAT
based O
on O
logic O
(λ-calculus) O

and B-DAT
re- O
quires O
additional O
classifiers O

trained B-DAT
for O
each O
predicate O
class. O
Compositional O
structure O
with O
modules. O
Neural O

Mod- B-DAT
ule O
Networks O
[3] O
address O

visual B-DAT
question O
answering O
by O
de O

- B-DAT
composing O
the O
questions O
into O

textual B-DAT
components O
and O
dy- O
namically O

assembling B-DAT
a O
specific O
network O
architecture O

for B-DAT
the O
question O
from O
a O

few B-DAT
network O
modules O
based O
on O

the B-DAT
textual O
components. O
However, O
this O

method B-DAT
relies O
on O
an O
external O

language B-DAT
parser O
for O
textual O
analysis O

instead B-DAT
of O
end-to-end O
learned O
language O

representation, B-DAT
and O
is O
not O
directly O

appli- B-DAT
cable O
to O
the O
task O

of B-DAT
grounding O
referential O
expressions O
into O

bounding B-DAT
boxes, O
since O
it O
does O

not B-DAT
explicitly O
output O
bound- O
ing O

boxes B-DAT
as O
results. O
Recently, O
[2] O

improves B-DAT
over O
[3] O
by O
learning O

to B-DAT
re-rank O
parsing O
outputs O
from O

the B-DAT
external O
parser, O
but O
it O

is B-DAT
still O
not O
end-to-end O
learned O

since B-DAT
the O
parser O
is O
fixed O

and B-DAT
not O
optimized O
for O
the O

task. B-DAT
Inspired O
by O
[3], O
our O

model B-DAT
also O
uses O
a O
modular O

structure, B-DAT
but O
learns O
the O
language O

rep- B-DAT
resentation O
end-to-end O
from O
words. O
3. O
Our O
model O
We O
propose O

Compositional B-DAT
Modular O
Networks O
(CMNs O

) B-DAT
to O
localize O
visual O
entities O
described O

by B-DAT
a O
query O
referential O
expression O

. B-DAT
Our O
model O
is O
compositional O

in B-DAT
the O
sense O
that O
it O

localizes B-DAT
a O
referential O
expression O
by O

grounding B-DAT
the O
compo- O
nents O
in O

the B-DAT
expressions O
and O
exploiting O
their O

interactions, B-DAT
in O
accordance O
with O
the O

principle B-DAT
of O
compositionality O
of O
natu- O

ral B-DAT
language O
– O
the O
meaning O

of B-DAT
a O
complex O
expression O
is O

de- B-DAT
termined O
by O
the O
meanings O

of B-DAT
its O
constituent O
expressions O
and O

the B-DAT
rules O
used O
to O
combine O

them B-DAT
[29]. O
Our O
model O
works O

in B-DAT
a O
retrieval O
setting: O
given O

an B-DAT
image O
I O
, O
a O

referential B-DAT
expression O
Q O
as O
query O

and B-DAT
a O
set O
of O
candidate O

region B-DAT
bounding O
boxes O

B B-DAT
= O
{bi} O
for O
the O

image B-DAT
I O
(e.g. O
extracted O
through O

object B-DAT
pro- O
posal O
methods), O
our O

model B-DAT
outputs O
a O
score O
for O

each B-DAT
bound- O
ing O
box O
bi, O

and B-DAT
returns O
the O
bounding O
box O

with B-DAT
the O
highest O
score O
as O

grounding B-DAT
(localization) O
result. O
Unlike O
state-of-the- O

art B-DAT
methods O
[25, O
7], O
the O

scores B-DAT
for O
each O
region O
bounding O

box B-DAT
bi O
∈ O
B O
are O

not B-DAT
predicted O
only O
from O
the O

local B-DAT
feature O
of O

feature B-DAT
local O

spatial B-DAT
feature O

language B-DAT
representation O
of O

subject B-DAT
or O
object O
qloc O

fully B-DAT
connected O
layer O
element-wise O
multiplication O

l2-normalization B-DAT
concatenate O

fully B-DAT
connected O
layer O
output O
unary O
score O
sloc O

image B-DAT
region O
b O
b2 O
local O

spatial B-DAT
feature O

language B-DAT
representation O

of B-DAT
relationship O
qrel O

fully B-DAT
connected O
layer O
concatenate O

output B-DAT
pairwise O
score O
srel O
image O
region O
b1 O

b1 B-DAT
local O
spatial O

feature B-DAT
image O
region O
b2word O
sequence O
{wt O

} B-DAT
the O
man O
riding O
a O

black B-DAT
bicycle O
word O
embedding O
sequence O
{et O

} B-DAT
concatenated O
state O
{ht O

} B-DAT
fully O

layer B-DAT
1 O
softmax O
CNN O

qsubj B-DAT
weighted O
average O

three B-DAT
attention O
weights O
over O
each O

word B-DAT
{at,subj}, O
{at,rel}, O
{at,obj O

} B-DAT
subject O

at,subj}: B-DAT
the O
man O
riding O
a O

black B-DAT
bicycle O

{at,rel B-DAT

the B-DAT
man O
riding O
a O
black O

bicycle B-DAT

at,obj B-DAT
}: O
the O
man O
riding O
a O

black B-DAT
bicycle O

a) B-DAT
language O
representation O
(b) O
localization O

module B-DAT
(c) O
relationship O
module O
2-layer O
bidirectional O
LSTM O

fully B-DAT
connected O

layer B-DAT
2 O
softmax O
fully O

layer B-DAT
3 O
softmax O
element-wise O

multiplication B-DAT
l2-normalization O

fully B-DAT
connected O
layer O
qrel O
qobj O

Figure B-DAT
2. O
Detailed O
illustration O
of O

our B-DAT
model. O
(a) O
Our O
model O

learns B-DAT
to O
parse O
an O
expression O

into B-DAT
subject, O
relationship O
and O
object O

with B-DAT
attention O
for O
language O
representation O
( O

Sec. B-DAT
3.1). O
(b) O
The O
localization O

module B-DAT
matches O
subject O
or O
object O

with B-DAT
each O
image O
region O
and O

returns B-DAT
a O
unary O
score O
(Sec. O
3 O

.2). B-DAT
(c) O
The O
relationship O
module O

matches B-DAT
a O
relationship O
with O
a O

pair B-DAT
of O
regions O
and O
returns O

a B-DAT
pairwise O
score O
(Sec. O
3.3). O
but O
also O
based O
on O
other O

regions B-DAT
in O
the O
image. O
In O

our B-DAT
model, O
we O
focus O
on O

the B-DAT
relationships O
in O
referential O
expressions O

that B-DAT
can O
be O
represented O
as O

a B-DAT
3-component O
triplet O
(subject, O
relationship O

, B-DAT
object), O
and O
learn O
to O

parse B-DAT
the O
ex- O
pressions O
into O

these B-DAT
components O
with O
attention. O
For O

exam- B-DAT
ple, O
a O
young O
man O

wearing B-DAT
a O
blue O
shirt O
can O

be B-DAT
parsed O
as O
the O
triplet O
( O

a B-DAT
young O
man, O
wearing, O
a O

blue B-DAT
shirt). O
The O
score O
of O

a B-DAT
region O
is O
determined O
by O

simultaneously B-DAT
looking O
at O
whether O
it O

matches B-DAT
the O
description O
of O
the O

subject B-DAT
entity O
and O
whether O
it O

matches B-DAT
the O
relationship O
with O
another O

interacting B-DAT
object O
entity O
mentioned O
in O

the B-DAT
expression. O
Our O
model O
handles O
such O
inter-object O

relationships B-DAT
by O
looking O
at O
pairs O

of B-DAT
regions O
(bi, O
bj). O
For O

referential B-DAT
expres- O
sions O
like O
“the O

red B-DAT
apple O
on O
top O
of O

the B-DAT
bookshelf”, O
we O
want O
to O

find B-DAT
a O
region O
pair O
(bi O

, B-DAT
bj) O
such O
that O
bi O

matches B-DAT
the O
subject O
entity O
“red O

apple” B-DAT
and O
bj O
matches O
the O

object B-DAT
entity O
“book- O
shelf” O
and O

the B-DAT
configuration O
of O
(bi, O
bj) O

matches B-DAT
the O
relation- O
ship O
“on O

top B-DAT
of”. O
To O
achieve O
this O

goal, B-DAT
our O
model O
is O
based O

on B-DAT
a O
compositional O
modular O
structure, O

composed B-DAT
of O
two O
mod- O
ules O

assembled B-DAT
in O
a O
pipeline O
for O

different B-DAT
sub-tasks: O
one O
localization O
module O

floc(·, B-DAT
qloc; O
Θloc) O
for O
deciding O

whether B-DAT
a O
region O
matches O
the O

subject B-DAT
or O
object O
in O
the O

expression, B-DAT
where O
qloc O
is O
the O

textual B-DAT
vector O
representation O
of O
the O

sub- B-DAT
ject O
component O
“red O
apple” O

or B-DAT
the O
object O
component O
“book- O

shelf”, B-DAT
and O
one O
relationship O
module O

frel(·, B-DAT
·, O
qrel; O
Θrel) O
for O

deciding B-DAT
whether O
a O
pair O
of O

regions B-DAT
matches O
the O
relationship O
described O

in B-DAT
the O
expression O
represented O
by O

qrel, B-DAT
the O
textual O
vector O
representation O

of B-DAT
the O
relationship O
“on O
top O

of”. B-DAT
The O
representations O
qsubj O
, O

qrel B-DAT
and O
qobj O
are O
learned O

jointly B-DAT
in O
our O
model O
in O

Sec. B-DAT
3.1. O
We O
define O
the O
pairwise O
score O

spair(bi, B-DAT
bj) O
over O
a O
pair O

of B-DAT

image B-DAT
regions O
(bi, O
bj) O
matching O

an B-DAT
input O
referential O
expres- O
sion O

Q B-DAT
as O
the O
sum O
of O

three B-DAT
components: O
spair(bi, O
bj) O
= O
floc(bi, O
qsubj O

; B-DAT
Θloc O

) B-DAT
+ O
floc(bj O
, O
qobj O

; B-DAT
Θloc O

) B-DAT
+ O
frel(bi, O
bj O
, O
qrel O

; B-DAT
Θrel), O
(1 O

) B-DAT
where O
qsubj O
, O
qobj O
and O

qrel B-DAT
are O
vector O
representations O
of O

sub- B-DAT
ject, O
relationship O
and O
object O

, B-DAT
respectively. O
For O
inference, O
we O
define O
the O

final B-DAT
subject O
unary O
score O
ssubj(bi O

) B-DAT
of O
a O
bounding O
of O

bi B-DAT
corresponding O
to O
the O
subject O
( O

e.g. B-DAT
“the O
red O
apple” O

in B-DAT
“the O
red O
apple O
on O

top B-DAT
of O
the O
book- O
shelf”) O

as B-DAT
the O
score O
of O
the O

best B-DAT
possible O
pair O
(bi, O
bj) O

that B-DAT
matches O
the O
entire O
expression: O
ssubj(bi) O
, O
max O
bj∈B O

spair(bi, B-DAT
bj). O
(2) O
The O
subject O
is O
ultimately O
grounded O

(localized) B-DAT
to O
the O
highest O
scoring O

region B-DAT
as O

b∗subj B-DAT
= O
arg O
max O
bi∈B O
(ssubj(bi)). O
(3 O

) B-DAT
3.1. O
Expression O
parsing O
with O
attention O

Given B-DAT
a O
referential O
expressionQ O
like O

the B-DAT
tall O
woman O
car- O
rying O

a B-DAT
red O
bag, O
how O
can O

we B-DAT
decide O
which O
substrings O
cor- O

responds B-DAT
to O
the O
subject, O
the O

relationship, B-DAT
and O
the O
object, O
and O

extract B-DAT
three O
vector O
representations O
qsubj O
, O
qrel O
and O
qobj O
cor- O
responding O

to B-DAT
these O
three O
components? O
One O

possible B-DAT
ap- O
proach O
is O
to O

use B-DAT
an O
external O
language O
parser O

to B-DAT
parse O
the O
referential O
expression O

into B-DAT
the O
triplet O
format O
(subject O

relationship, B-DAT
object) O
and O
then O
process O

each B-DAT
com- O
ponent O
with O
an O

encoder B-DAT
(e.g. O
a O
recurrent O
neural O

network) B-DAT
to O
extract O
qsubj O
, O

qrel B-DAT
and O
qobj O
. O
However, O

the B-DAT
formal O
represen- O
tations O
of O

language B-DAT
produced O
by O
syntactic O
parsers O

do B-DAT
not O
al- O
ways O
correspond O

to B-DAT
intuitive O
visual O
representations. O
As O

a B-DAT
simple O
example, O
the O
apple O

on B-DAT
top O
of O
the O
bookshelf O

is B-DAT
ana- O
lyzed O
[31] O
as O

having B-DAT
a O
subject O
phrase O
the O

apple, B-DAT
a O
relation- O
ship O
on, O

and B-DAT
an O
object O
phrase O
top O

of B-DAT
the O
bookshelf, O
when O
in O

fact B-DAT
the O
visually O
salient O
objects O

are B-DAT
simply O
the O
apple O
and O

the B-DAT
bookshelf, O
while O
the O
complete O

expression B-DAT
on O
top O
of O
de- O

scribes B-DAT
the O
relationship O
between O
them. O
Therefore, O
in O
this O
work O
we O

learn B-DAT
to O
decompose O
the O
input O

expression B-DAT
Q O
into O
the O
above O

3 B-DAT
components, O
and O
generate O
vector O

representations B-DAT
qsubj O
, O
qrel O
and O

qobj B-DAT
fromQ O
through O
a O
soft O

attention B-DAT
mechanism O
over O
the O
word O

sequence, B-DAT
as O
shown O
in O
Figure O

2 B-DAT
(a). O
For O
a O
referential O

expression B-DAT
Q O
that O
is O
a O

sequence B-DAT
of O
T O
words O
{wt}Tt=1 O

, B-DAT
we O
first O
embed O
each O

word B-DAT
wt O
to O
a O
vector O

et B-DAT
using O
GloVe O
[22], O
and O

then B-DAT
scan O
through O
the O
word O

embedding B-DAT
sequence O
{et}Tt=1 O
with O
a O
2 O

-layer B-DAT
bi- O
directional O
LSTM O

network B-DAT
[26]. O
The O
first O
layer O

takes B-DAT
as O
in- O
put O
the O

sequence B-DAT
{et} O
and O
outputs O
a O

forward B-DAT
hidden O
state O
h O
(1,fw) O

t B-DAT
and O
a O
backward O
hidden O

state B-DAT
h O
(1,bw) O
t O
at O
each O
time O

step, B-DAT
which O
are O
concatenated O
into O

h(1)t B-DAT
. O
The O
second O
layer O

then B-DAT
takes O
the O
first O
layer’s O

output B-DAT
sequence O
{h(1)t O
} O
as O

input B-DAT
and O
outputs O
forward O
and O

backward B-DAT
hidden O
states O
h(2,fw)t O
and O

h(2,bw)t B-DAT
at O
each O
time O
step. O

All B-DAT
the O
hidden O
states O
in O

the B-DAT
first O
layer O
and O
second O

layer B-DAT
are O
concatenated O
into O
a O

single B-DAT
vector O
ht. O
ht O
= O
[ O
h O
(1,fw O

) B-DAT
t O
h O
(1,bw) O
t O
h O

2,fw) B-DAT
t O
h O
(2,bw) O
t O

4) B-DAT
The O
concatenated O
state O
ht O
contains O

information B-DAT
from O
word O
wt O
itself O

and B-DAT
also O
context O
from O
words O

before B-DAT
and O
after O
wt. O
Then O

the B-DAT
attention O
weights O
at,subj O

, B-DAT
at,rel O
and O
at,obj O
for O

subject, B-DAT
relationship, O
object O
over O
each O

word B-DAT
wt O
are O
obtained O
by O

three B-DAT
linear O
predictions O
over O
ht O

followed B-DAT
by O
a O
softmax O
as O

at,subj B-DAT
= O
exp O
( O
βTsubjht O

T B-DAT
τ=1 O
exp O
( O
βTsubjhτ O

5) B-DAT
at,rel O
= O
exp O
( O
βTrelht O

T B-DAT
τ=1 O
exp O
( O
βTrelhτ O

6) B-DAT
at,obj O
= O
exp O
( O
βTobjht O

T B-DAT
τ=1 O
exp O
( O
βTobjhτ O

7) B-DAT
and O
the O
language O
representations O

of B-DAT
the O
subject O
qsubj O
, O

rela- B-DAT
tionship O
qrel O
and O
object O

qobj B-DAT
are O
extracted O
as O
weighed O

aver- B-DAT
age O
of O
word O
embedding O

vectors B-DAT
{et}with O
attention O
weights, O
as O
follows O

: B-DAT
qsubj O

T∑ B-DAT
t=1 O
at,subjet O
(8 O

) B-DAT
qrel O

T∑ B-DAT
t=1 O
at,relet O
(9 O

) B-DAT
qobj O

T∑ B-DAT
t=1 O
at,objet. O
(10 O

) B-DAT
In O
our O
implementation, O
both O
the O

forward B-DAT
and O
the O
back- O
ward O

LSTM B-DAT
in O
each O
layer O
of O

the B-DAT
bi-directional O
LSTM O
net- O
work O

have B-DAT
1000-dimensional O
hidden O
states, O
so O

the B-DAT
final O
ht O
is O
4000-dimensional O

. B-DAT
During O
training, O
dropout O
is O

added B-DAT
on O
top O
of O
ht O

as B-DAT
regularization. O
3.2. O
Localization O
module O

As B-DAT
shown O
in O
Figure O
2 O
( O

b), B-DAT
the O
localization O
module O
floc O

outputs B-DAT
a O
score O
sloc O
= O

floc(b, B-DAT
qloc; O
Θloc) O
representing O
how O

likely B-DAT
a O
region O
bounding O
box O

b B-DAT
matches O
qloc, O
which O
is O

either B-DAT
the O
subject O
textual O
vector O

qsubj B-DAT
in O
Eqn. O
8 O
or O

object B-DAT
textual O
vector O
qobj O
in O

Eqn. B-DAT
10. O
This O
module O
takes O
the O
local O

visual B-DAT
feature O
xvis O
and O
spa O

- B-DAT
tial O
feature O
xspatial O
of O

image B-DAT
region O
b. O
We O
extract O

visual B-DAT
feature O
xv O
from O
image O

region B-DAT
b O
using O
a O
convolutional O

neu- B-DAT
ral O
network O
[27], O
and O

extract B-DAT
a O
5-dimensional O
spatial O
feature O

xs B-DAT
= O
[ O
xmin O
WI O

yminHI B-DAT
, O
xmax O
WI O
, O
ymaxHI O
, O
Sb O
SI O

from B-DAT
b O
using O
the O
same O

representation B-DAT
as O
in O
[20], O

where B-DAT
[xmin, O
ymin, O
xmax, O
ymax] O

and B-DAT
Sb O
are O
bounding O
box O

coordinates B-DAT
and O
area O
of O
b, O

andWI B-DAT
, O
HI O
and O
SI O

are B-DAT
width, O
height O
and O
area O

of B-DAT
the O
image O
I O
. O

Then, B-DAT
xv O
and O
xs O
are O

concatenated B-DAT
into O
a O
vector O

xv,s B-DAT
= O
[xv O
xs] O
as O

representation B-DAT
of O
region O
b. O
Since O
element-wise O
multiplication O
is O
shown O

to B-DAT
be O
a O
pow- O
erful O

way B-DAT
to O
combine O
representations O
from O

different B-DAT
modal- O
ities O
[5], O
we O

adopt B-DAT
it O
here O
to O
obtain O

a B-DAT
joint O
vision O
and O
lan O

- B-DAT
guage O
representation. O
In O
our O

implementation, B-DAT
xv,s O
is O
first O
embedded O

to B-DAT
a O
new O
vector O
x̃v,s O

that B-DAT
has O
the O
same O
dimen- O

sion B-DAT
as O
qloc O
(which O
is O

either B-DAT
qsubj O
in O
Eqn. O
8 O

or B-DAT
qobj O
in O
Eqn. O
10) O

through B-DAT
a O
linear O
transform, O
and O

then B-DAT
element-wise O
multiplied O
with O
qloc O

to B-DAT
obtain O
a O
vector O
zloc, O

which B-DAT
is O
L2- O
normalized O
into O

ẑloc B-DAT
to O
obtain O
a O
more O

robust B-DAT
representation, O
as O
follows: O
x̃v,s O
= O
Wv,sxv,s O
+ O
bv,s O

(11) B-DAT
zloc O
= O
x̃v,s O

� B-DAT
qloc O
(12) O
ẑloc O

= B-DAT
zloc/‖zloc‖2 O
(13 O

) B-DAT
where O
� O
is O
element-wise O
multiplication O

between B-DAT
two O
vec- O
tors. O
Then O

the B-DAT
score O
sloc O
is O
predicted O

linearly B-DAT
from O
ẑloc O
as O

sloc B-DAT
= O
w O
T O

locẑloc B-DAT
+ O
bloc. O
(14) O
The O
parameters O
in O
Θloc O
are O

(Wv,s, B-DAT
bv,s, O
wloc, O
bloc O

3.3. B-DAT
Relationship O
module O
As O
shown O
in O
Figure O
2 O

(c), B-DAT
the O
relationship O
module O
frel O

outputs B-DAT
a O
score O
srel O

= B-DAT
frel(b1, O
b2, O
qrel; O
Θrel O

) B-DAT
representing O
how O
likely O
a O

pair B-DAT
of O
region O
bounding O
boxes O
( O

b1, B-DAT
b2) O
matches O
qrel, O
the O

representation B-DAT
of O
relationship O
in O
the O

expression. B-DAT
In O
our O
implementation, O
we O
use O

the B-DAT
spatial O
features O
xs1 O
and O

xs2 B-DAT
of O
the O
two O
regions O

b1 B-DAT
and O
b2 O
extracted O
in O

the B-DAT
same O
way O
as O
in O

localization B-DAT
module O
(we O
empirically O
find O

that B-DAT
adding O
visual O
features O
of O

b1 B-DAT
and O
b2 O
leads O
to O

no B-DAT
noticeable O
performance O
boost O
while O

slowing B-DAT
training O
significantly). O
Then O
xs1 O

and B-DAT
xs2 O
are O
concatenated O
as O

xs1,s2 B-DAT
= O
[xs1 O
xs2], O
and O

then B-DAT
processed O
in O
a O
similar O

way B-DAT
as O
in O
localization O
mod O

- B-DAT
ule O
to O
obtain O
srel, O

as B-DAT
shown O
below: O
x̃s1,s2 O
= O
Ws1,s2xs1,s2 O
+ O
bs1,s2 O

(15) B-DAT
zrel O
= O
x̃s1,s2 O

� B-DAT
qrel O
(16) O
ẑrel O

= B-DAT
zrel/‖zrel‖2 O
(17) O
srel O

= B-DAT
w O

T B-DAT
relẑrel O
+ O
brel. O
(18) O
The O
parameters O
in O
Θrel O
are O

(Ws1,s2, B-DAT
bs1,s2, O
wrel, O
brel O

). B-DAT
3.4. O
End-to-end O
learning O

During B-DAT
training, O
for O
an O
image O

I B-DAT
, O
a O
referential O
expression O

Q B-DAT
and O
a O
set O
of O

candidate B-DAT
regions O
B O
extracted O
from O

I B-DAT
, O
if O
the O
ground-truth O

regions B-DAT
bsubj O
gt O
of O
the O

subject B-DAT
entity O
and O
bobj O
gt O

of B-DAT
the O
object O
entity O
are O

both B-DAT
available, O
then O
we O
can O

optimize B-DAT
the O
pairwise O
score O
spair O

in B-DAT
Eqn. O
1 O
with O
strong O

supervision B-DAT
using O
softmax O
loss O
Lossstrong. O
Lossstrong O
= O
− O
log O

exp B-DAT
(spair(bsubj O
gt, O
bobj O
gt))∑ O
( O

bi,bj)∈B×B B-DAT
exp O
(spair(bi, O
bj)) O
) O
(19 O

) B-DAT
However, O
it O
is O
often O
hard O

to B-DAT
obtain O
ground-truth O
regions O
for O

both B-DAT
subject O
entity O
and O
object O

entity. B-DAT
For O
referential O
expres- O
sions O

like B-DAT
“a O
red O
vase O
on O

top B-DAT
of O
the O
table”, O
often O

there B-DAT
is O
only O
a O
ground-truth O

bounding B-DAT
box O
annotation O
b1 O
for O

the B-DAT
subject O
(vase) O
in O
the O

expression, B-DAT
but O
no O
bounding O
box O

annotation B-DAT
b2 O
for O
the O
object O

(table), B-DAT
so O
one O
cannot O
directly O

optimize B-DAT
the O
pairwise O
score O
spair(b1 O

, B-DAT
b2). O
To O
address O
this O

issue, B-DAT
we O
treat O
the O
object O

region B-DAT
b2 O
as O
a O
latent O

variable, B-DAT
and O
optimize O
the O
unary O

score B-DAT
ssubj(b1) O
in O
Eqn. O
2. O

Since B-DAT
ssubj(b1) O
is O
ob- O
tained O

by B-DAT
maximizing O
over O
all O
possible O

region B-DAT
b2 O
∈ O
B O
in O

spair(b1, B-DAT
b2), O
this O
can O
be O

regarded B-DAT
as O
a O
weakly O
supervised O

Multiple B-DAT
Instance O
Learning O
(MIL) O
approach O

similar B-DAT
to O
[21]. O
The O
unary O

score B-DAT
ssubj O
can O
be O
optimized O

with B-DAT
weak O
supervi- O
sion O
using O

softmax B-DAT
loss O
Lossweak. O
Lossweak O
= O
− O
log O

exp B-DAT
(ssubj(bsubj O
gt))∑ O
bi∈B O
exp O
( O

ssubj(bi)) B-DAT
) O
(20 O

) B-DAT
The O
whole O
system O
is O
trained O

end-to-end B-DAT
with O
backpropa- O
gation. O
In O

our B-DAT
experiments, O
we O
train O
for O

300000 B-DAT
iterations O

, B-DAT
Method O
Accuracy O
baseline O
(loc O
module O

) B-DAT
46.27% O
our O
full O
model O
99 O

.99% B-DAT
Table O
1. O
Accuracy O
of O
our O

model B-DAT
and O
the O
baseline O
on O

the B-DAT
synthetic O
shape O
dataset. O
See O

Sec. B-DAT
4.1 O
for O
details O

. B-DAT
expression=“the O
green O
square O
right O
of O

a B-DAT
red O
circle” O
baseline O

- B-DAT
sloc O
ssubj O
sobj O

a) B-DAT
(b) O
(c) O
(d) O
Figure O
3. O
For O
the O
image O

in B-DAT
(a) O
and O
the O
expression O

“the B-DAT
green O
square O
right O
of O

a B-DAT
red O
circle”, O
(b) O
baseline O

scores B-DAT
on O
each O
location O
on O

the B-DAT
5 O
by O
5 O
grid O

using B-DAT
localization O
module O
only O
(darker O

is B-DAT
higher), O
and O
(c, O
d O

) B-DAT
scores O
ssubj O
and O
sobj O

using B-DAT
our O
full O
model. O
ssubj O

is B-DAT
highest O
on O
the O
exact O

green B-DAT
square O
that O
is O
on O

the B-DAT
right O
of O
a O
red O

circle, B-DAT
and O
sobj O
is O
highest O

on B-DAT
this O
red O
circle. O
with O
0.95 O
momentum O
and O
an O

initial B-DAT
learning O
rate O
of O
0.005 O

, B-DAT
multiplied O
by O
0.1 O
after O

every B-DAT
120000 O
iterations. O
Each O
batch O

contains B-DAT
one O
image O
with O
all O

referential B-DAT
expressions O
anno- O
tated O
over O

that B-DAT
image. O
Parameters O
in O
the O

localization B-DAT
mod- O
ule, O
the O
relationship O

module B-DAT
and O
the O
language O
representation O

in B-DAT
our O
model O
are O
initialized O

randomly B-DAT
with O
Xavier O
initializer O
[9]. O

Our B-DAT
model O
is O
implemented O
using O

TensorFlow B-DAT
[1] O
and O
we O
plan O

to B-DAT
release O
our O
code O
and O

data B-DAT
to O
facilitate O
reproduc- O
tion O

of B-DAT
our O
results. O
4. O
Experiments O
We O
first O
evaluate O

our B-DAT
model O
on O
a O
synthetic O

dataset B-DAT
to O
ver O

- B-DAT
ify O
its O
ability O
to O
handle O

inter-object B-DAT
relationships O
in O
refer- O
ential O

expressions. B-DAT
Next O
we O
apply O
our O

method B-DAT
to O
real O
im- O
ages O

and B-DAT
expressions O
in O
the O
Visual O

Genome B-DAT
dataset O
[14] O
and O
Google-Ref O

dataset B-DAT
[20]. O
Since O
the O
task O

of B-DAT
answering O
pointing O
questions O
in O

visual B-DAT
question O
answering O
is O
similar O

to B-DAT
grounding O
referential O
expressions, O
we O

also B-DAT
evaluate O
our O
model O
on O

the B-DAT
pointing O
questions O
in O
the O

Visual-7W B-DAT
dataset O
[32 O

]. B-DAT
4.1. O
Analysis O
on O
a O
synthetic O

dataset B-DAT

Inspired B-DAT
by O
[3], O
we O
first O

perform B-DAT
a O
simulation O
exper- O
iment O

on B-DAT
a O
synthetic O
shape O
dataset. O

The B-DAT
dataset O
con- O
sists O
of O
30000 O
images O
with O
simple O
circles, O
squares O

and B-DAT
triangles O
of O
different O
sizes O

and B-DAT
colors O
on O
a O
5 O

by B-DAT
5 O
grid, O
and O
referential O

expressions B-DAT
constructed O
using O
a O
template O

of B-DAT
the O
form O
[subj] O
[relationship O

] B-DAT
[obj], O
where O
[subj] O

and B-DAT
[obj] O
involve O
both O
shape O

classes B-DAT
and O
at- O
tributes O

and B-DAT
[relationship] O
is O
some O
spatial O

relation- B-DAT
ships O
such O
as O
“above”. O

The B-DAT
task O
is O
to O
localize O

the B-DAT
corre- O
sponding O
shape O
region O

described B-DAT
by O
the O
expression O
on O

5 B-DAT
by O
5 O
grid. O
Figure O
3 O
(a) O
shows O
an O
example O
in O

this B-DAT
dataset O
with O
the O
synthetic O

expression B-DAT
“the O
green O
square O
right O

of B-DAT
a O
red O
circle”. O
In O

the B-DAT
synthesizing O
procedure, O
we O
make O

sure B-DAT
that O
the O
shape O
region O

being B-DAT
referred O
to O
cannot O
be O

inferred B-DAT
simply O
from O
[subj] O
as O

there B-DAT
will O
be O
multiple O
matching O

regions, B-DAT
and O
the O
relationship O
with O

another B-DAT
region O
described O
by O
[obj O

] B-DAT
has O
to O
be O
taken O

into B-DAT
consideration. O
On O
this O
dataset, O
we O
train O

our B-DAT
model O
with O
weak O
super O

- B-DAT
vision O
by O
Eqn. O
20 O

using B-DAT
the O
ground-truth O
subject O
region O

bsubj B-DAT
gt O
of O
the O
subject O

shape B-DAT
described O
in O
the O
expression. O

Here B-DAT
the O
candidate O
region O
set O

B B-DAT
are O
the O
25 O
possible O

loca- B-DAT
tions O
on O
the O
5 O

by B-DAT
5 O
grid, O
and O
visual O

features B-DAT
are O
extracted O
from O
the O

corresponding B-DAT
cropped O
image O
region O
with O

a B-DAT
VGG- O
16 O
network O
[27] O

pretrained B-DAT
on O
ImageNET O
classification. O
As O

a B-DAT
comparison, O
we O
also O
train O

a B-DAT
baseline O
model O
using O
only O

the B-DAT
localization O
module, O
with O
a O

softmax B-DAT
loss O
on O
its O
output O

sloc B-DAT
in O
Eqn. O
14 O
over O

all B-DAT
25 O
locations O
on O
the O

grid, B-DAT
and O
language O
representation O
qloc O

obtained B-DAT
by O
scanning O
through O
the O

word B-DAT
embedding O
sequence O
with O
a O

single B-DAT
LSTM O
network O
and O
tak- O

ing B-DAT
the O
hidden O
state O
at O

the B-DAT
last O
time O
step O
same O

as B-DAT
in O
[25, O
10]. O
This O

baseline B-DAT
method O
resembles O
the O
supervised O

version B-DAT
of O
GroundeR O
[25], O
and O

the B-DAT
main O
difference O
between O
this O

base- B-DAT
line O
and O
our O
model O

is B-DAT
that O
the O
baseline O
only O

looks B-DAT
at O
a O
re- O
gion’s O

appearance B-DAT
and O
spatial O
property O
but O

ignores B-DAT
pairwise O
relationship O
with O
other O

regions. B-DAT
We O
evaluate O
with O
the O
accuracy O

on B-DAT
whether O
the O
pre- O
dicted O

subject B-DAT
region O
b∗subj O
matches O
the O

ground-truth B-DAT
region O
bsubj O
gt. O
Table O

1 B-DAT
shows O
the O
results O
on O

this B-DAT
dataset, O
where O
our O
model O

trained B-DAT
with O
weak O
supervision O
(the O

same B-DAT
as O
the O
super- O
vision O

given B-DAT
to O
baseline) O
achieves O
nearly O

perfect B-DAT
accuracy— O
significantly O
outperforming O
the O

baseline B-DAT
using O
a O
localization O
module O

only. B-DAT
Figure O
3 O
shows O
an O

example, B-DAT
where O
the O
base- O
line O

can B-DAT
localize O
green O
squares O
but O

fails B-DAT
to O
distinguish O
the O
exact O

green B-DAT
square O
right O
of O
a O

red B-DAT
circle, O
while O
our O
model O

suc- B-DAT
cessfully O
finds O
the O
subject-object O

pair, B-DAT
although O
it O
has O
never O

seen B-DAT
the O
ground-truth O
location O
for O

the B-DAT
object O
entity O
during O
training O

. B-DAT
4.2. O
Localizing O
relationships O
in O
Visual O

Genome B-DAT

We B-DAT
also O
evaluate O
our O
method O

on B-DAT
the O
Visual O
Genome O

dataset B-DAT
[14], O
which O
contains O
relationship O

expressions B-DAT
anno- O
tated O
over O
pairs O

of B-DAT
objects, O
such O
as O
“computer O

on B-DAT
top O
of O
ta- O
ble” O

and B-DAT
“person O
wearing O
shirt”. O
On O
the O
relationship O
annotations O
in O

Visual B-DAT
Genome, O
given O
an O
image O

and B-DAT
an O
expression O
like O
“man O

wearing B-DAT
hat”, O
we O
evaluate O
our O

method B-DAT
in O
two O
test O
scenarios O

: B-DAT
retrieving O
the O
subject O

region B-DAT
(“man”) O
and O
retrieving O
the O

subject-object B-DAT
pair O
(both O
“man” O

and B-DAT
“hat”). O
In O
our O
experiment, O

we B-DAT
take O
the O
bounding O
boxes O

of B-DAT
all O
the O
annotated O
entities O

in B-DAT
each O
im- O
age O
(around O
35 O
per O
image) O
as O
candidate O
region O

set B-DAT
B O
at O
both O
training O

and B-DAT
test O
time, O
and O
extract O

visual B-DAT
features O
for O
each O
region O

from B-DAT
fc7 O
output O
of O
a O

Faster-RCNN B-DAT
VGG-16 O
network O
[24] O
pretrained O

on B-DAT
MSCOCO O
detection O
dataset O
[16 O

]. B-DAT
The O
Method O
training O
supervision O
P@1-subj O
P@1-pair O

baseline B-DAT
subject-GT O
41.20% O
- O
baseline O

subject-object-GT B-DAT
- O
23.37% O
our O
full O

model B-DAT
subject-GT O
43.81% O
26.56% O
our O

full B-DAT
model O
subject-object-GT O
44.24% O
28.52 O

% B-DAT
Table O
2. O
Performance O
of O
our O

model B-DAT
on O
relationship O
expressions O
in O

Visual B-DAT
Genome O
dataset. O
See O
Sec O

. B-DAT
4.2 O
for O
details. O
input O
images O
are O
first O
forwarded O

through B-DAT
the O
convolutional O
layers O
of O

the B-DAT
network, O
and O
the O
features O

of B-DAT
each O
image O
region O
are O

extracted B-DAT
by O
ROI-pooling O
over O
the O

convolutional B-DAT
feature O
map, O
followed O
by O

subsequent B-DAT
fully O
connected O
layers. O
We O

use B-DAT
the O
same O
training, O
validation O

and B-DAT
test O
split O
as O
in O

[12 B-DAT

]. B-DAT
Since O
there O
are O
ground-truth O
annotations O

for B-DAT
both O
subject O
region O
and O

object B-DAT
region O
in O
this O
dataset O

, B-DAT
we O
experiment O
with O
two O

training B-DAT
supervision O
settings: O
(1) O
weak O

supervision B-DAT
by O
only O
providing O
the O

ground-truth B-DAT
region O
of O
the O
subject O

en- B-DAT
tity O
at O
training O
time O
( O

subject-GT B-DAT
in O
Table O
2) O
and O

optimizing B-DAT
unary O
subject O
score O
ssubj O

with B-DAT
Eqn. O
20 O
and O
(2) O

strong B-DAT
su- O
pervision O
by O
providing O

the B-DAT
ground-truth O
region O
pair O
of O

both B-DAT
subject O
and O
object O
entities O

at B-DAT
training O
time O
(subject-object- O
GT O

in B-DAT
Table O
2) O
and O
optimizing O

pairwise B-DAT
score O
spair O
with O
Eqn. O
19 O

. B-DAT
Similar O
to O
the O
experiment O
on O

the B-DAT
synthetic O
dataset O
in O
Sec O

. B-DAT
4.1, O
we O
also O
train O

a B-DAT
baseline O
model O
that O
only O

looks B-DAT
at O
local O
appearance O
and O

spatial B-DAT
properties O
but O
ignores O
pairwise O

rela- B-DAT
tionships. O
For O
the O
first O

evaluation B-DAT
scenario O
of O
retrieving O
the O

subject B-DAT
region, O
we O
train O
a O

baseline B-DAT
model O
using O
a O
localiza- O

tion B-DAT
module O
only O
by O
optimizing O

its B-DAT
output O
sloc O
for O
ground- O

truth B-DAT
subject O
region O
with O
softmax O

loss B-DAT
(the O
same O
training O
su- O

pervision B-DAT
as O
subject-GT). O
For O
the O

second B-DAT
scenario O
of O
retriev- O
ing O

the B-DAT
subject-object O
pair, O
we O
train O

two B-DAT
such O
baseline O
mod- O
els O

optimized B-DAT
with O
subject O
ground-truth O
and O

object B-DAT
ground- O
truth O
respectively, O
to O

localize B-DAT
of O
the O
subject O
region O

and B-DAT
ob- O
ject O
region O
separately O

with B-DAT
each O
model O
and O
at O

test B-DAT
time O
com- O
bine O
the O

predicted B-DAT
subject O
region O
and O
predicted O

object B-DAT
region O
from O
each O
model O

be B-DAT
the O
subject-object O
pair O
(same O

training B-DAT
supervision O
as O
subject-object-GT). O
We O
evaluate O
with O
top-1 O
precision O

(P@1), B-DAT
which O
is O
the O
percentage O

of B-DAT
test O
instances O
where O
the O

top B-DAT
scoring O
predic- O
tion O
matches O

the B-DAT
ground-truth O
in O
each O
image O

(P@1-subj B-DAT
for O
predicted O
subject O
regions O

matching B-DAT
subject O
ground-truth O
in O
the O

first B-DAT
scenario, O
and O
P@1-pair O
for O

predicted B-DAT
subject O
and O
object O
regions O

both B-DAT
matching O
the O
ground-truth O
in O

the B-DAT
second O
scenario). O
The O
results O

are B-DAT
summarized O
in O
Table O
2 O

, B-DAT
where O
it O
can O
be O

seen B-DAT
that O
our O
full O
model O

outperforms B-DAT
the O
baseline O
using O
only O

localization B-DAT
modules O
in O
both O
evaluation O

scenar- B-DAT
ios. O
Note O
that O
in O

the B-DAT
second O
evaluation O
scenario O
of O

retrieving B-DAT
subject-object O
pairs, O
our O
weakly O

supervised B-DAT
model O
still O
out- O
performs O

the B-DAT
baseline O
trained O
with O
strong O

supervision. B-DAT
Figure O
4 O
shows O
some O
examples O

of B-DAT
our O
model O
trained O
with O

weak B-DAT
supervision O
(subject-GT) O
and O
attention O

weights B-DAT

ground-truth B-DAT
our O
prediction O
attention O
weights O

ground-truth B-DAT
our O
prediction O
attention O
weights O

expression=“tennis B-DAT
player O
wears O
shorts” O
expression=“building O

behind B-DAT
bus” O
expression=“car O
has O
tail O
light” O
expression=“window O

on B-DAT
front O
of O
building O

” B-DAT
expression=“business O
name O
on O
sign” O
expression=“board O

on B-DAT
top O
of O
store O

” B-DAT
expression=“wine O
bottle O
next O
to O
glasses O

” B-DAT
expression=“chairs O
around O
table” O
expression=“marker O
on O
top O
of O
ledge O

” B-DAT
expression=“chair O
next O
to O
table” O
Figure O
4. O
Visualization O
of O
grounded O

relationship B-DAT
expressions O
in O
the O
Visual O

Genome B-DAT
dataset, O
trained O
with O
weak O

supervision B-DAT
(subject-GT). O
In O
each O
example O

, B-DAT
the O
first O
and O
the O

second B-DAT
column O
show O
ground-truth O
region O

pairs B-DAT
and O
our O
predicted O
region O

pairs B-DAT
respectively O
(subject O
in O
red O

solid B-DAT
box O
and O
object O
in O

green B-DAT
dashed O
box). O
The O
third O

column B-DAT
visualizes O
attention O
weights O
in O

Eqn. B-DAT
5–7 O
for O
subject, O
relationship O

and B-DAT
object O
(darker O
is O
higher). O
in O
Eqn. O
5–7. O
It O
can O

be B-DAT
seen O
that O
even O
with O

weak B-DAT
supervi- O
sion, O
our O
model O

still B-DAT
generates O
reasonable O
attention O
weights O

over B-DAT
words O
for O
subject, O
relationship O

and B-DAT
object O

. B-DAT
4.3. O
Grounding O
referential O
expressions O
in O

images B-DAT

We B-DAT
apply O
our O
model O
to O

the B-DAT
Google-Ref O
dataset O
[20], O
a O

benchmark B-DAT
dataset O
for O
grounding O
referential O

expressions. B-DAT
As O
this O
dataset O
does O

not B-DAT
explicitly O
contain O
subject-object O
pair O

annotation B-DAT
for O
the O
referential O
expressions, O

we B-DAT
train O
our O
model O
with O

weak B-DAT
supervision O
(Eqn. O
20) O
by O

optimizing B-DAT
the O
subject O
score O
ssubj O

using B-DAT
the O
expression-level O
region O
ground-truth. O

The B-DAT
candidate O
bounding O
box O
set O

B B-DAT
at O
both O
training O
and O

test B-DAT
time O
are O
all O
the O

annotated B-DAT
entities O
in O
the O
image O
( O

which B-DAT
is O
the O
“Ground-Truth” O
evaluation O

setting B-DAT
in O
[20]). O
As O
in O

Sec. B-DAT
4.2, O
fc7 O
output O
of O

a B-DAT
MSCOCO-pretrained O
Method O
P@1 O
Mao O
et O
al O

. B-DAT
[20] O
60.7% O
Yu O
et O

al. B-DAT
[30] O
64.0% O
Nagaraja O
et O

al. B-DAT
[21] O
68.4% O
baseline O
(loc O

module) B-DAT
66.5% O
our O
model O
(w/ O

external B-DAT
parser) O
53.5% O
our O
full O

model B-DAT
69.3% O
Table O
3. O
Top-1 O
precision O
of O

our B-DAT
model O
and O
previous O
methods O

on B-DAT
Google-Ref O
dataset. O
See O
Sec O

. B-DAT
4.3 O
for O
details. O
Faster-RCNN O
VGG-16 O
network O
is O
used O

for B-DAT
visual O
feature O
extraction. O
Similar O

to B-DAT
Sec. O
4.1, O
we O
also O

train B-DAT
a O
GroundeR- O
like O
[25 O

] B-DAT
baseline O
model O
with O
localization O

module B-DAT
which O
looks O
only O
at O

a B-DAT
region’s O
local O

Method B-DAT
Accuracy O
Zhu O
et O

al. B-DAT
[32] O
56.10% O
baseline O
(loc O

module) B-DAT
71.61% O
our O
model O
(w/ O

external B-DAT
parser) O
61.66% O
our O
full O

model B-DAT
72.53% O
Table O
4. O
Accuracy O
of O
our O

model B-DAT
and O
previous O
methods O
on O

the B-DAT
pointing O
questions O
in O
Visual-7W O

dataset. B-DAT
See O
Sec. O
4.4 O
for O

details B-DAT

. B-DAT
In O
addition, O
instead O
of O
learning O

a B-DAT
linguistic O
analysis O
end- O
to-end O

as B-DAT
in O
Sec. O
3.1, O
we O

also B-DAT
experiment O
with O
parsing O
the O

expression B-DAT
using O
the O
Stanford O
Parser O

[31, B-DAT
19]. O
An O
expres- O
sion O

is B-DAT
parsed O
into O
subject, O
relationship O

and B-DAT
object O
compo- O
nent O
according O

to B-DAT
the O
constituency O
tree, O
and O

the B-DAT
components O
are O
encoded O
into O

vectors B-DAT
qsubj O
, O
qrel O
and O

qobj B-DAT
using O
three O
sep- O
arate O

LSTM B-DAT
encoders, O
similar O
to O
the O

baseline B-DAT
and O
[25 O

]. B-DAT
Following O
[20], O
we O
evaluate O
on O

this B-DAT
dataset O
using O
the O
top-1 O

precision B-DAT
(P@1) O
metric, O
which O
is O

the B-DAT
fraction O
of O
the O
highest O

scoring B-DAT
subject O
region O
matching O
the O

ground-truth B-DAT
for O
the O
expression. O
Table O

3 B-DAT
shows O
the O
performance O
of O

our B-DAT
model, O
baseline O
model O
and O

previous B-DAT
work. O
Note O
that O
all O

the B-DAT
methods O
are O
trained O
with O

the B-DAT
same O
weak O
supervision O
(only O

a B-DAT
ground-truth O
subject O
region). O
It O

can B-DAT
be O
seen O
that O
by O

in- B-DAT
corporating O
inter-object O
relationships, O
our O

full B-DAT
model O
out- O
performs O
the O

baseline B-DAT
using O
only O
localization O
modules O

, B-DAT
and O
works O
better O
than O

previous B-DAT
state-of-the-art O
methods. O
Additionally, O
replacing O
the O
learned O
expression O

parsing B-DAT
and O
language O
representation O
in O

Sec. B-DAT
3.1 O
with O
an O
external O

parser B-DAT
(“our O
model O
w/ O
external O

parser” B-DAT
in O
Table O
3) O
leads O

to B-DAT
a O
significant O
performance O
drop O

. B-DAT
We O
find O
that O
this O

is B-DAT
mainly O
because O
existing O
parsers O

are B-DAT
not O
specifically O
tuned O
for O

the B-DAT
referring O
expression O
task—as O
noted O

in B-DAT
Sec. O
3.1, O
expressions O
like O

chair B-DAT
on O
the O
left O
of O

the B-DAT
table O
are O
parsed O
as O
( O

chair, B-DAT
on, O
the O
left O
of O

the B-DAT
table) O
rather O
than O
the O

desired B-DAT
triplet O
(chair, O
on O
the O

left B-DAT
of, O
the O
table). O
In O

our B-DAT
full O
model, O
the O
language O

repre- B-DAT
sentation O
is O
end-to-end O
optimized O

with B-DAT
other O
parts, O
while O
it O

is B-DAT
hard O
to O
jointly O
optimize O

an B-DAT
external O
language O
parser O

like B-DAT
[31] O
for O
this O
task. O
Figure O
5 O
shows O
some O
example O

results B-DAT
on O
this O
dataset. O
It O

can B-DAT
be O
seen O
that O
although O

weakly B-DAT
supervised, O
our O
model O
not O

only B-DAT
grounds O
the O
subject O
region O

correctly B-DAT
(solid O
box), O
but O
also O

finds B-DAT
reasonable O
regions O
(dashed O
box O

) B-DAT
for O
the O
object O
entity. O
4.4. O
Answering O
pointing O
questions O
in O

Visual-7W B-DAT

Finally, B-DAT
we O
evaluate O
our O
method O

on B-DAT
the O
multiple O
choice O
pointing O

questions B-DAT
(i.e. O
“which” O
questions) O
in O

visual B-DAT
ques- O
tion O
answering O
on O

the B-DAT
Visual-7W O
dataset O
[32]. O
Given O

an B-DAT
image O
and O
a O
question O

like B-DAT
“which O
tomato O
slice O
is O

under B-DAT
the O
knife”, O
the O
task O

is B-DAT
to O
select O
the O
corresponding O

region B-DAT
from O
a O
few O
choice O

regions B-DAT
(4 O
choices O
in O
this O

dataset) B-DAT
as O
answer. O
Since O
this O

task B-DAT
is O
closely O
related O
to O

grounding B-DAT
referential O
ex- O
pressions, O
our O

model B-DAT
can O
be O
trained O
in O

the B-DAT
same O
way O
as O
in O
Sec. O
4.3 O
to O
score O
each O

choice B-DAT
region O
using O
subject O
score O

ssubj B-DAT
and O
pick O
the O
highest O

scoring B-DAT
choice O
as O
answer O

. B-DAT
As O
before, O
we O
train O
our O

model B-DAT
with O
weak O
supervision O
through O

Eqn. B-DAT
20 O
and O
use O
a O

MSCOCO-pretrained B-DAT
Faster- O
RCNN O
VGG-16 O
network O

for B-DAT
visual O
feature O
extraction. O
Here O

we B-DAT
use O
two O
different O
candidate O

bounding B-DAT
box O
setsBsubj O
and O
Bobj O

of B-DAT
the O
subject O
regions O
(the O

choices) B-DAT
and O
the O
object O
re O

- B-DAT
gions, O
where O
Bsubj O
is O

the B-DAT
4 O
choice O
bounding O
boxes, O

and B-DAT
Bobj O
is O
the O
set O

of B-DAT
300 O
proposal O
bounding O
boxes O

extracted B-DAT
using O
RPN O
in O

Faster-RCNN B-DAT
[24]. O
Similar O
to O
Sec. O
4 O

.3, B-DAT
we O
also O
train O
a O

baseline B-DAT
model O
using O
only O
a O

localization B-DAT
module O
to O
score O
each O

choice B-DAT
based O
only O
on O
its O

local B-DAT
appearance O
and O
spatial O
properties, O

and B-DAT
a O
truncated O
model O
that O

uses B-DAT
the O
Stan- O
ford O

parser B-DAT
[31, O
19] O
for O
expression O

parsing B-DAT
and O
language O
representation. O
The O
results O
are O
shown O
in O

Table B-DAT
4. O
It O
can O
be O

seen B-DAT
that O
our O
full O
model O

outperforms B-DAT
the O
baseline O
and O
the O

truncated B-DAT
model O
with O
an O
external O

parser, B-DAT
and O
achieves O
much O
higher O

accuracy B-DAT
than O
previous O
work O
[32 O

]. B-DAT
Figure O
6 O
shows O
some O

question B-DAT
answering O
examples O
on O
this O

dataset. B-DAT
5. O
Conclusion O
We O
have O
proposed O

Compositional B-DAT
Modular O
Networks, O
a O

novel B-DAT
end-to-end O
trainable O
model O
for O

handling B-DAT
relationships O
in O
referential O
expressions. O

Our B-DAT
model O
learns O
to O
parse O

input B-DAT
expressions O
with O
soft O
attention, O

and B-DAT
incorporates O
two O
types O
of O

modules B-DAT
that O
consider O
a O
region’s O

local B-DAT
features O
and O
pair- O
wise O

interaction B-DAT
between O
regions O
respectively. O
The O

model B-DAT
induces O
intuitive O
linguistic O
and O

visual B-DAT
analyses O
of O
referential O
expressions O

from B-DAT
only O
weak O
supervision, O
and O

experimen- B-DAT
tal O
results O
demonstrate O
that O

our B-DAT
approach O
outperforms O
both O
natural O

baselines B-DAT
and O
state-of-the-art O
methods O
on O

multiple B-DAT
datasets. O
Acknowledgements O
This O
work O
was O
supported O

by B-DAT
DARPA, O
AFRL, O
DoD O

MURI B-DAT
award O
N000141110688, O
NSF O
awards O

IIS-1427425, B-DAT
IIS-1212798 O
and O
IIS-1212928, O
NGA O

and B-DAT
the O
Berkeley O
Ar- O
tificial O

Intelligence B-DAT
Research O
(BAIR) O
Lab. O
Jacob O

Andreas B-DAT
is O
supported O
by O
a O

Facebook B-DAT
graduate O
fellowship O
and O
a O

Huawei B-DAT
/ O
Berkeley O
AI O
fellowship. O
References O
[1] O
M. O
Abadi, O
A O

. B-DAT
Agarwal, O
P. O
Barham, O
E. O

Brevdo, B-DAT
Z. O
Chen, O
C. O
Citro, O
G. O
S. O
Corrado O

, B-DAT
A. O
Davis, O
J. O
Dean, O

M. B-DAT
Devin, O
S. O
Ghe- O
mawat, O

I. B-DAT
Goodfellow, O
A. O
Harp, O
G. O

Irving, B-DAT
M. O
Isard, O
Y. O
Jia, O

R. B-DAT
Jozefowicz, O
L. O
Kaiser, O
M. O

Kudlur, B-DAT
J. O
Levenberg, O
D. O
Mané, O

R. B-DAT
Monga, O
S. O
Moore, O
D. O

Murray, B-DAT
C. O
Olah, O
M. O
Schuster, O

J. B-DAT
Shlens, O
B. O
Steiner, O
I. O

Sutskever, B-DAT
K. O
Talwar, O
P. O
Tucker, O

V. B-DAT
Vanhoucke, O
V. O
Vasudevan, O
F. O

Viégas, B-DAT
O. O
Vinyals, O
P. O
War- O

den, B-DAT
M. O
Wattenberg, O
M. O
Wicke, O

Y. B-DAT
Yu, O
and O
X. O
Zheng. O

Tensor- B-DAT
Flow: O
Large-scale O
machine O
learning O

on B-DAT
heterogeneous O
sys- O
tems. O
arXiv:1603.04467, O
2016 O

. B-DAT
5 O

ground-truth B-DAT
our O
prediction O
ground-truth O
our O

prediction B-DAT
ground-truth O
our O
prediction O
expression=“a O

bear B-DAT
lying O
to O
the O
right O

of B-DAT
another O
bear” O
expression=“man O
in O
sunglasses O

walking B-DAT

towards B-DAT
two O
talking O
men” O
expression=“a O

picnic B-DAT
table O
that O
has O
a O

bottle B-DAT
of O
water O
sitting O
on O
it O

” B-DAT
correct O
correct O
correct O

expression=“woman B-DAT
in O
a O
cream O
colored O

wedding B-DAT
dress O
cutting O
cake” O
expression=“a O
man O
going O
before O
a O

lady B-DAT
carrying O
a O
cellphone O

” B-DAT
expression=“pizza O
slice O
not O
eaten O

” B-DAT
correct O
correct O
incorrect O

expression=“a B-DAT
full O
grown O
brown O
bear O

near B-DAT
a O
young O
bear” O
expression=“black O
dog O
standing O
on O
all O

four B-DAT
legs O

” B-DAT
expression=“chair O
being O
sat O
in O
by O

a B-DAT
man O

” B-DAT
correct O
incorrect O
correct O
Figure O
5 O

. B-DAT
Examples O
of O
referential O
expressions O

in B-DAT
the O
Google-Ref O
dataset. O
The O

left B-DAT
column O
shows O
the O
ground-truth O

region B-DAT
and O
the O
right O
column O

shows B-DAT
the O
grounded O
subject O
region O
( O

our B-DAT
prediction) O
in O
solid O
box O

and B-DAT
the O
grounded O
object O
region O

in B-DAT
dashed O
box. O
A O
prediction O

is B-DAT
labeled O
as O
correct O
if O

the B-DAT
predicted O
subject O
region O
matches O

the B-DAT
ground-truth O
region. O
ground-truth O
our O
prediction O
ground-truth O
our O

prediction B-DAT
ground-truth O
our O
prediction O
question=“Which O

wine B-DAT
glass O
is O
in O
the O

man’s B-DAT

hand?” B-DAT
question=“Which O
person O
is O
wearing O

a B-DAT
helmet?” O
question=“Which O
mouse O
is O

on B-DAT
a O
pad O
by O
computer O

?” B-DAT
correct O
correct O
correct O

question=“Which B-DAT
head O
is O
that O
of O

an B-DAT
adult O
giraffe?” O
question=“Which O
pants O
belong O
to O
the O

man B-DAT
closest O
to O
the O
train O

?” B-DAT
question=“Which O
white O
pillow O
is O
leftmost O

on B-DAT
the O
bed O

?” B-DAT
correct O
correct O
correct O

question=“Which B-DAT
red O
shape O
is O
on O

a B-DAT
large O
white O
sign?” O
question=“Which O
is O
not O
a O
pair O

of B-DAT
a O
living O
canine O

?” B-DAT
question=“Which O
hand O
can O
be O
seen O

from B-DAT
under O
the O
umbrella O

?” B-DAT
correct O
incorrect O
correct O
Figure O
6 O

. B-DAT
Example O
pointing O
questions O
in O

the B-DAT
Visual-7W O
dataset. O
The O
left O

column B-DAT
shows O
the O
4 O
multiple O

choices B-DAT
(ground-truth O
answer O
in O
yellow) O

and B-DAT
the O
right O
column O
shows O

the B-DAT
grounded O
subject O
region O
(predicted O

answer) B-DAT
in O
solid O
box O
and O

the B-DAT
grounded O
object O
region O
in O

dashed B-DAT
box. O
A O
prediction O
is O

labeled B-DAT
as O
correct O
if O
the O

predicted B-DAT
subject O
region O
matches O
the O

ground-truth B-DAT

2] B-DAT
J. O
Andreas, O
M. O
Rohrbach, O

T. B-DAT
Darrell, O
and O
D. O
Klein. O

Learning B-DAT
to O
compose O
neural O
networks O

for B-DAT
question O
answering. O
In O
Pro- O

ceedings B-DAT
of O
the O
Conference O
of O

the B-DAT
North O
American O
Chapter O
of O

the B-DAT
Association O
for O
Computational O
Linguistics O
( O

NAACL), B-DAT
2016. O
2 O
[3] O
J. O
Andreas, O
M. O
Rohrbach O

, B-DAT
T. O
Darrell, O
and O
D. O

Klein. B-DAT
Neural O
module O
networks. O
In O

Proceedings B-DAT
of O
the O
IEEE O
Conference O

on B-DAT
Computer O
Vision O
and O
Pattern O

Recognition B-DAT
(CVPR), O
2016. O
2, O
5 O
[4] O
P. O
Arbeláez, O
J. O
Pont-Tuset O

, B-DAT
J. O
Barron, O
F. O
Marques, O

and B-DAT
J. O
Ma- O
lik. O
Multiscale O

combinatorial B-DAT
grouping. O
In O
Proceedings O
of O

the B-DAT
IEEE O
Conference O
on O
Computer O

Vision B-DAT
and O
Pattern O
Recognition O
(CVPR), O
2014 O

. B-DAT
2 O
[5] O
J. O
Ba, O
V. O
Mnih O

, B-DAT
and O
K. O
Kavukcuoglu. O
Multiple O

object B-DAT
recog- O
nition O
with O
visual O

attention. B-DAT
In O
Proceedings O
of O
the O

Inter- B-DAT
national O
Conference O
on O
Learning O

Representations B-DAT
(ICLR), O
2015. O
4 O
[6] O
J. O
Dai, O
Y. O
Li O

, B-DAT
K. O
He, O
and O
J. O

Sun. B-DAT
R-fcn: O
Object O
detection O
via O

region-based B-DAT
fully O
convolutional O
networks. O
In O

Advances B-DAT
in O
Neural O
Information O
Processing O

Systems B-DAT
(NIPS), O
2016. O
1 O
[7] O
A. O
Fukui, O
D. O
H O

. B-DAT
Park, O
D. O
Yang, O
A. O

Rohrbach, B-DAT
T. O
Darrell, O
and O
M. O

Rohrbach. B-DAT
Multimodal O
compact O
bilinear O
pooling O

for B-DAT
visual O
question O
answering O
and O

visual B-DAT
grounding. O
In O
Pro- O
ceedings O

of B-DAT
the O
Conference O
on O
Empirical O

Methods B-DAT
in O
Natural O
Language O
Processing O
( O

EMNLP), B-DAT
2016. O
2 O
[8] O
R. O
Girshick, O
J. O
Donahue O

, B-DAT
T. O
Darrell, O
and O
J. O

Malik. B-DAT
Rich O
fea- O
ture O
hierarchies O

for B-DAT
accurate O
object O
detection O
and O

semantic B-DAT
segmentation. O
In O
Proceedings O
of O

the B-DAT
IEEE O
Conference O
on O
Computer O

Vision B-DAT
and O
Pattern O
Recognition O
(CVPR), O
2014 O

. B-DAT
1, O
2 O
[9] O
X. O
Glorot O
and O
Y O

. B-DAT
Bengio. O
Understanding O
the O
difficulty O

of B-DAT
training O
deep O
feedforward O
neural O

networks. B-DAT
In O
Aistats, O
vol- O
ume O
9, O
pages O
249–256, O
2010. O
5 O

10] B-DAT
R. O
Hu, O
M. O
Rohrbach, O

and B-DAT
T. O
Darrell. O
Segmentation O
from O

nat- B-DAT
ural O
language O
expressions. O
In O

Proceedings B-DAT
of O
the O
European O
Conference O

on B-DAT
Computer O
Vision O
(ECCV), O
2016. O
6 O

11] B-DAT
R. O
Hu, O
H. O
Xu, O

M. B-DAT
Rohrbach, O
J. O
Feng, O
K. O

Saenko, B-DAT
and O
T. O
Dar- O
rell. O

Natural B-DAT
language O
object O
retrieval. O
In O

Proceedings B-DAT
of O
the O
IEEE O
Conference O

on B-DAT
Computer O
Vision O
and O
Pattern O

Recogni- B-DAT
tion O
(CVPR), O
2016. O
1, O
2 O

12] B-DAT
J. O
Johnson, O
A. O
Karpathy, O

and B-DAT
L. O
Fei-Fei. O
Densecap: O
Fully O

convolutional B-DAT
localization O
networks O
for O
dense O

captioning. B-DAT
In O
Proceedings O
of O
the O

IEEE B-DAT
Conference O
on O
Computer O
Vision O

and B-DAT
Pattern O
Recognition O
(CVPR), O
2016. O
6 O

13] B-DAT
P. O
Krähenbühl O
and O
V. O

Koltun. B-DAT
Geodesic O
object O
proposals. O
In O

Proceedings B-DAT
of O
the O
European O
Conference O

on B-DAT
Computer O
Vi- O
sion O
(ECCV), O
2014 O

. B-DAT
2 O
[14] O
R. O
Krishna, O
Y. O
Zhu O

, B-DAT
O. O
Groth, O
J. O
Johnson, O

K. B-DAT
Hata, O
J. O
Kravitz, O
S. O

Chen, B-DAT
Y. O
Kalantidis, O
L.-J. O
Li, O

D. B-DAT
A. O
Shamma, O
et O
al. O

Visual B-DAT
genome: O
Connecting O
language O
and O

vision B-DAT
using O
crowdsourced O
dense O
image O

annotations. B-DAT
arXiv O
preprint O
arXiv:1602.07332, O
2016. O
5, O
6 O

15] B-DAT
J. O
Krishnamurthy O
and O
T. O

Kollar. B-DAT
Jointly O
learning O
to O
parse O

and B-DAT
perceive: O
Connecting O
natural O
language O

to B-DAT
the O
physical O
world. O
Transactions O

of B-DAT
the O
Association O
for O
Computational O

Linguistics, B-DAT
1:193–206, O
2013. O
2 O
[16] O
T.-Y. O
Lin, O
M. O
Maire O

, B-DAT
S. O
Belongie, O
J. O
Hays, O

P. B-DAT
Perona, O
D. O
Ra- O
manan, O

P. B-DAT
Dollár, O
and O
C. O
L. O

Zitnick. B-DAT
Microsoft O
coco: O
Com- O
mon O

objects B-DAT
in O
context. O
In O
Proceedings O

of B-DAT
the O
European O
Con- O
ference O

on B-DAT
Computer O
Vision O
(ECCV), O
2014. O
6 O

17] B-DAT
W. O
Liu, O
D. O
Anguelov, O

D. B-DAT
Erhan, O
C. O
Szegedy, O
and O

S. B-DAT
Reed. O
Ssd: O
Single O
shot O

multibox B-DAT
detector. O
In O
Proceedings O
of O

the B-DAT
European O
Conference O
on O
Computer O

Vision B-DAT
(ECCV), O
2016. O
1 O
[18] O
C. O
Lu, O
R. O
Krishna O

, B-DAT
M. O
Bernstein, O
and O
L. O

Fei-Fei. B-DAT
Visual O
re- O
lationship O
detection O

with B-DAT
language O
priors. O
In O
Proceedings O

of B-DAT
the O
European O
Conference O
on O

Computer B-DAT
Vision O
(ECCV), O
2016. O
2 O
[19] O
C. O
D. O
Manning, O
M O

. B-DAT
Surdeanu, O
J. O
Bauer, O
J. O

Finkel, B-DAT
S. O
J. O
Bethard, O
and O

D. B-DAT
McClosky. O
The O
Stanford O
CoreNLP O

natural B-DAT
language O
processing O
toolkit. O
In O

Association B-DAT
for O
Computa- O
tional O
Linguistics O
( O

ACL) B-DAT
System O
Demonstrations, O
pages O
55– O
60, O
2014. O
8 O

20] B-DAT
J. O
Mao, O
J. O
Huang, O

A. B-DAT
Toshev, O
O. O
Camburu, O
A. O

Yuille, B-DAT
and O
K. O
Murphy. O
Generation O

and B-DAT
comprehension O
of O
unambiguous O
object O

descriptions. B-DAT
In O
Proceedings O
of O
the O

IEEE B-DAT
Conference O
on O
Computer O
Vision O

and B-DAT
Pattern O
Recognition O
(CVPR), O
2016. O
1, O
2, O
4, O
5, O
7, O
8 O

21] B-DAT
V. O
K. O
Nagaraja, O
V. O

I. B-DAT
Morariu, O
and O
L. O
S. O

Davis. B-DAT
Modeling O
con- O
text O
between O

objects B-DAT
for O
referring O
expression O
understanding. O

In B-DAT
Proceedings O
of O
the O
European O

Conference B-DAT
on O
Computer O
Vi- O
sion O
( O

ECCV), B-DAT
2016. O
2, O
5, O
7 O
[22] O
J. O
Pennington, O
R. O
Socher O

, B-DAT
and O
C. O
D. O
Manning. O

Glove: B-DAT
Global O
vectors O
for O
word O

representation. B-DAT
In O
Proceedings O
of O
the O

Con- B-DAT
ference O
on O
Empirical O
Methods O

in B-DAT
Natural O
Language O
Process- O
ing O
( O

EMNLP), B-DAT
2014. O
4 O
[23] O
J. O
Redmon, O
S. O
Divvala O

, B-DAT
R. O
Girshick, O
and O
A. O

Farhadi. B-DAT
You O
only O
look O
once: O

Unified, B-DAT
real-time O
object O
detection. O
In O

Pro- B-DAT
ceedings O
of O
the O
IEEE O

Conference B-DAT
on O
Computer O
Vision O
and O

Pattern B-DAT
Recognition O
(CVPR), O
2015. O
1 O
[24] O
S. O
Ren, O
K. O
He O

, B-DAT
R. O
Girshick, O
and O
J. O

Sun. B-DAT
Faster O
r-cnn: O
Towards O
real-time O

object B-DAT
detection O
with O
region O
proposal O

networks. B-DAT
In O
Advances O
in O
Neural O

Information B-DAT
Processing O
Systems O
(NIPS), O
2015. O
1, O
6, O
8 O

25] B-DAT
A. O
Rohrbach, O
M. O
Rohrbach, O

R. B-DAT
Hu, O
T. O
Darrell, O
and O

B. B-DAT
Schiele. O
Grounding O
of O
textual O

phrases B-DAT
in O
images O
by O
re- O

construction. B-DAT
In O
Proceedings O
of O
the O

European B-DAT
Conference O
on O
Computer O
Vision O
( O

ECCV), B-DAT
2016. O
1, O
2, O
6, O
7, O
8 O

26] B-DAT
M. O
Schuster O
and O
K. O

K. B-DAT
Paliwal. O
Bidirectional O
recurrent O
neural O

networks. B-DAT
IEEE O
Transactions O
on O
Signal O

Processing, B-DAT
45(11):2673–2681, O
1997. O
4 O
[27] O
K. O
Simonyan O
and O
A O

. B-DAT
Zisserman. O
Very O
deep O
convolutional O

networks B-DAT
for O
large-scale O
image O
recognition. O

In B-DAT
Proceedings O
of O
the O
International O

Conference B-DAT
on O
Learning O
Representations O
(ICLR), O
2015 O

. B-DAT
4, O
6 O
[28] O
J. O
R. O
Uijlings, O
K O

. B-DAT
E. O
van O
de O
Sande, O

T. B-DAT
Gevers, O
and O
A. O
W. O

Smeulders. B-DAT
Selective O
search O
for O
object O

recognition. B-DAT
Inter- O
national O
journal O
of O

computer B-DAT
vision, O
104(2):154–171, O
2013. O
2 O
[29] O
M. O
Werning, O
W. O
Hinzen O

, B-DAT
and O
E. O
Machery. O
The O

Oxford B-DAT
hand- O
book O
of O
compositionality. O

Oxford B-DAT
University O
Press, O
2012. O
2 O
[30] O
L. O
Yu, O
P. O
Poirson O

, B-DAT
S. O
Yang, O
A. O
C. O

Berg, B-DAT
and O
T. O
L. O
Berg. O

Mod- B-DAT
eling O
context O
in O
referring O

expressions. B-DAT
In O
Proceedings O
of O
the O

European B-DAT
Conference O
on O
Computer O
Vision O
( O

ECCV), B-DAT
2016. O
2, O
7 O

31] B-DAT
M. O
Zhu, O
Y. O
Zhang, O

W. B-DAT
Chen, O
M. O
Zhang, O
and O

J. B-DAT
Zhu. O
Fast O
and O
accurate O

shift-reduce B-DAT
constituent O
parsing. O
In O
ACL O
(1 O

), B-DAT
pages O
434–443, O
2013. O
4, O
8 O

32] B-DAT
Y. O
Zhu, O
O. O
Groth, O

M. B-DAT
Bernstein, O
and O
L. O
Fei-Fei. O

Visual7w: B-DAT
Grounded O
question O
answering O
in O

images. B-DAT
arXiv O
preprint O
arXiv:1511.03416, O
2015. O
5, O
8 O

33] B-DAT
C. O
L. O
Zitnick O
and O

P. B-DAT
Dollár. O
Edge O
boxes: O
Locating O

object B-DAT
proposals O
from O
edges. O
In O

Proceedings B-DAT
of O
the O
European O
Con- O

ference B-DAT
on O
Computer O
Vision O
(ECCV), O
2014 O

. B-DAT
2 O

ages O
and O
expressions O
in O
the O
Visual B-DAT
Genome I-DAT
dataset O
[14] O
and O
Google-Ref O
dataset O

4.2. O
Localizing O
relationships O
in O
Visual B-DAT
Genome I-DAT

evaluate O
our O
method O
on O
the O
Visual B-DAT
Genome I-DAT
dataset O
[14], O
which O
contains O
relationship O

On O
the O
relationship O
annotations O
in O
Visual B-DAT
Genome, I-DAT
given O
an O
image O
and O
an O

model O
on O
relationship O
expressions O
in O
Visual B-DAT
Genome I-DAT
dataset. O
See O
Sec. O
4.2 O
for O

grounded O
relationship O
expressions O
in O
the O
Visual B-DAT
Genome I-DAT
dataset, O
trained O
with O
weak O
supervision O

and O
expressions O
in O
the O
Visual O
Genome B-DAT
dataset O
[14] O
and O
Google-Ref O
dataset O

4.2. O
Localizing O
relationships O
in O
Visual O
Genome B-DAT

our O
method O
on O
the O
Visual O
Genome B-DAT
dataset O
[14], O
which O
contains O
relationship O

the O
relationship O
annotations O
in O
Visual O
Genome, B-DAT
given O
an O
image O
and O
an O

on O
relationship O
expressions O
in O
Visual O
Genome B-DAT
dataset. O
See O
Sec. O
4.2 O
for O

relationship O
expressions O
in O
the O
Visual O
Genome B-DAT
dataset, O
trained O
with O
weak O
supervision O

on O
fixed, O
predefined O
categories O
for O
subjects, B-DAT
relations, O
and O
objects, O
treating O
entities O

ages O
and O
expressions O
in O
the O
Visual B-DAT
Genome O
dataset O
[14] O
and O
Google-Ref O

the O
pointing O
questions O
in O
the O
Visual B-DAT

4.2. O
Localizing O
relationships O
in O
Visual B-DAT
Genome O

evaluate O
our O
method O
on O
the O
Visual B-DAT
Genome O
dataset O
[14], O
which O
contains O

On O
the O
relationship O
annotations O
in O
Visual B-DAT
Genome, O
given O
an O
image O
and O

model O
on O
relationship O
expressions O
in O
Visual B-DAT
Genome O
dataset. O
See O
Sec. O
4.2 O

grounded O
relationship O
expressions O
in O
the O
Visual B-DAT
Genome O
dataset, O
trained O
with O
weak O

on O
the O
pointing O
questions O
in O
Visual B-DAT

4.4. O
Answering O
pointing O
questions O
in O
Visual B-DAT

ques- O
tion O
answering O
on O
the O
Visual B-DAT

Example O
pointing O
questions O
in O
the O
Visual B-DAT

D. O
A. O
Shamma, O
et O
al. O
Visual B-DAT
genome: O
Connecting O
language O
and O
vision O

M. O
Bernstein, O
and O
L. O
Fei-Fei. O
Visual B-DAT
re- O
lationship O
detection O
with O
language O

on O
fixed, O
predefined O
categories O
for O
subjects, B-DAT
relations, O
and O
objects, O
treating O
entities O

the O
layer O
(“pool5”) O
before O
the O
1000 B-DAT

58.80 O
4096 O
59.42 O
8192 O
59.69 O
16000 B-DAT
59.83 O
32000 O
59.71 O

in O
MCB O
with O
d O
= O
16000 B-DAT

1604 B-DAT

1602 B-DAT

to O
Visual O
Question O
Answering. O
arXiv: O
1605 B-DAT

1505 B-DAT

1503 B-DAT

the O
Visual7W O
dataset O
and O
the O
VQA B-DAT
challenge O

such O
as O
visual O
question O
answering O
(VQA) B-DAT
and O
visual O
ground- O
ing, O
most O

multimodal O
pooling), O
current O
approaches O
in O
VQA B-DAT
or O
grounding O
rely O
on O
concatenat O

predict O
an- O
swers O
for O
the O
VQA B-DAT
task O
and O
locations O
for O
the O

we O
present O
an O
architecture O
for O
VQA B-DAT
which O
uses O
MCB O
twice, O
once O

additional O
training O
data O
for O
the O
VQA B-DAT
task. O
To O
sum- O
marize, O
MCB O

task O
of O
visual O
question O
answering O
(VQA) B-DAT
or O
visual O
grounding, O
we O
have O

then O
detail O
our O
architectures O
for O
VQA B-DAT
(Sec. O
3.2) O
and O
visual O
grounding O

and O
z O
∈ O
R3000 O
for O
VQA B-DAT

Figure O
3: O
Our O
architecture O
for O
VQA B-DAT

3.2 O
Architectures O
for O
VQA B-DAT

In O
VQA, B-DAT
the O
input O
to O
the O
model O

Yang O
et O
al., O
2015) O
for O
VQA, B-DAT
the O
soft O
attention O
mechanism O
can O

Answer O
Encoding. O
For O
VQA B-DAT
with O
multiple O
choices, O
we O
can O

Figure O
4: O
Our O
architecture O
for O
VQA B-DAT

The O
Visual O
Question O
Answering O
(VQA) B-DAT
real-image O
dataset O
(Antol O
et O
al O

data O
split. O
We O
use O
the O
VQA B-DAT
tool O
provided O
by O
Antol O
et O

and O
how). O
Compared O
to O
the O
VQA B-DAT
dataset, O
Vi- O
sual O
Genome O
represents O

Genome O
are O
larger O
than O
the O
VQA B-DAT
dataset. O
To O
leverage O
the O
Visual O

vocabulary O
space O
created O
from O
the O
VQA B-DAT
dataset, O
leaving O
us O
with O
addi O

Models O
are O
trained O
on O
the O
VQA B-DAT
train O
split O
and O
tested O
on O

2, O
we O
train O
on O
the O
VQA B-DAT
train O
split, O
vali- O
date O
on O

the O
VQA B-DAT
validation O
split, O
and O
report O
results O

on O
the O
VQA B-DAT
test-dev O
split. O
We O
use O
early O

training O
settings O
as O
in O
the O
VQA B-DAT
exper- O
iments. O
We O
use O
the O

For O
VQA B-DAT
multiple O
choice, O
we O
train O
the O

Models O
are O
trained O
on O
the O
VQA B-DAT
train O
split O
and O
tested O
on O

80.9 O
37.5 O
43.5 O
58.2 O
- O
VQA B-DAT
team O
(Antol O
et O
al., O
2015 O

and O
multiple-choice O
(MC) O
results O
on O
VQA B-DAT
test O
set O
(trained O
on O
train+val O

with O
the O
state-of-the- O
art O
on O
VQA B-DAT
test O
set. O
Our O
best O
single O

next O
best O
approach O
on O
the O
VQA B-DAT
open-ended O
task O
and O
0.8 O
points O

maps O
from O
MCB O
model O
on O
VQA B-DAT
images. O
Bottom: O
predicted O
grounding O
from O

gives O
significant O
improvements O
on O
two O
VQA B-DAT
datasets O
compared O
to O
state-of-the-art. O
In O

3.2 O
Architectures O
for O
VQA B-DAT

The O
Visual O
Question O
Answering O
(VQA) O
real B-DAT

experiments O
on O
the O
open- O
ended O
real B-DAT

we O
also O
report O
our O
multiple-choice O
real B-DAT

Representation O
learning O
for O
text O
and O
images B-DAT
has O
been O
extensively O
studied O
in O

to O
work O
best O
to O
represent O
images B-DAT
(Don- O
ahue O
et O
al., O
2013 O

consists O
of O
approximately O
200,000 O
MSCOCO O
images B-DAT
(Lin O
et O
al., O
2014), O
with O

3 O
data O
splits: O
train O
(80K O
images), B-DAT
validation O
(40K O
images), O
and O
test O

80K O
images B-DAT

et O
al., O
2016) O
uses O
108,249 O
images B-DAT
from O
the O
intersection O
of O
YFCC100M O

Visual7W O
is O
composed O
of O
47,300 O
images B-DAT
from O
MSCOCO O
and O
there O
are O

augment O
our O
training O
data O
with O
images B-DAT
and O
QA O
pairs O
from O
the O

2015) O
which O
consists O
of O
31K O
images B-DAT
from O
Flickr30k O
dataset O
(Hodosh O
et O

al., O
2014), O
which O
contains O
20K O
images B-DAT
from O
IAPR O
TC-12 O
dataset O
(Grubinger O

from O
MCB O
model O
on O
VQA O
images B-DAT

model O
(right) O
on O
Flickr30k O
Entities O
images B-DAT

Grounding O
of O
textual O
phrases O
in O
images B-DAT
by O
reconstruc- O
tion. O
In O
Proceedings O

semantics O
for O
finding O
and O
describing O
images B-DAT
with O
sentences. O
Transactions O
of O
the O

choice B-DAT
ques- O
tion O
answering O
we O
introduce O

multiple O
variable-length O
answer O
choices, B-DAT
each O
choice O
is O
encoded O
using O
a O
word O

choice B-DAT
real-image O
scores O

choice B-DAT
format O
and O
each O
question O
comes O

For O
VQA O
multiple O
choice, B-DAT
we O
train O
the O
open-ended O
models O

the O
argmax O
over O
the O
multiple O
choice B-DAT

choice B-DAT
QA O
tasks O
accuracy O
(%) O
on O

choice B-DAT
(MC) O
results O
on O
VQA O
test O

results O
for O
the O
Visual7W O
multiple- O
choice B-DAT
QA O
task. O
The O
MCB O
with O

choice B-DAT
task O
(on O
Test- O
dev). O
Even O

Compact O
Bilinear O
Pooling O
for O
Visual O
Question B-DAT
Answering O
and O
Visual O
Grounding O

4 O
Evaluation O
on O
Visual O
Question B-DAT
Answering O

The O
Visual O
Question B-DAT
Answering O
(VQA) O
real-image O
dataset O
(Antol O

Hierarchical O
Co-Attention O
for O
Vi- O
sual O
Question B-DAT
Answering. O
In O
Advances O
in O
Neural O

Deep O
Learning O
Approach O
to O
Visual O
Question B-DAT
Answering. O
arXiv: O
1605.02697 O

Ask O
Me O
Anything: O
Free-form O
Visual O
Question B-DAT
Answering O
Based O
on O
Knowledge O
from O

Li O
Fei-Fei. O
2016. O
Visual7W: O
Grounded O
Question B-DAT
Answering O
in O
Images. O
In O
Proceedings O

4 O
Evaluation O
on O
Visual O
Question B-DAT
Answering O

Multimodal O
Compact O
Bilinear O
Pooling O
for O
Visual B-DAT
Question O
Answering O
and O
Visual O
Grounding O

Visual B-DAT
Vector O

Count O
Sketch O
of O
Visual B-DAT
Vector O

Multimodal O
Compact O
Bilinear O
Pooling O
for O
Visual B-DAT
and O
Textual O
Embeddings O

3.3 O
Architecture O
for O
Visual B-DAT
Grounding O

4 O
Evaluation O
on O
Visual B-DAT
Question O
Answering O

The O
Visual B-DAT
Question O
Answering O
(VQA) O
real-image O
dataset O

The O
Visual B-DAT
Genome O
dataset O
(Krishna O
et O
al O

question O
and O
answer O
lengths O
for O
Visual B-DAT
Genome O
are O
larger O
than O
the O

VQA O
dataset. O
To O
leverage O
the O
Visual B-DAT
Genome O
dataset O
as O
additional O
training O

is O
a O
part O
of O
the O
Visual B-DAT
Genome. O
Visual7W O
adds O
a O
7th O

64.24% O
accuracy O
(trained O
on O
train+val). O
Visual B-DAT
inspection O
of O
the O
gen O

and O
QA O
pairs O
from O
the O
Visual B-DAT
Genome O
dataset. O
We O
also O
con O

were O
trained O
with O
data O
from O
Visual B-DAT
Genome, O
and O
some O
were O
trained O

5 O
Evaluation O
on O
Visual B-DAT
Grounding O

and O
Devi O
Parikh. O
2015. O
Vqa: O
Visual B-DAT
question O
answering. O
In O
Proceedings O
of O

Bernstein, O
and O
Li O
Fei-Fei. O
2016. O
Visual B-DAT
genome: O
Connecting O
language O
and O
vi O

A O
Deep O
Learning O
Approach O
to O
Visual B-DAT
Question O
Answering. O
arXiv: O
1605.02697 O

2016. O
Ask O
Me O
Anything: O
Free-form O
Visual B-DAT
Question O
Answering O
Based O
on O
Knowledge O

Multimodal O
Compact O
Bilinear O
Pooling O
for O
Visual B-DAT
and O
Textual O
Embeddings O

3.3 O
Architecture O
for O
Visual B-DAT
Grounding O

4 O
Evaluation O
on O
Visual B-DAT
Question O
Answering O

5 O
Evaluation O
on O
Visual B-DAT
Grounding O

to O
predict O
the O
answer. O
For O
multiple B-DAT

proposed O
a O
model O
that O
extracts O
multiple B-DAT
co-attentions O
on O
the O
image O
and O

predict O
the O
attention O
weights O
over O
multiple B-DAT
bounding O
box O
proposals. O
Similarly, O
Hu O

also O
ex- O
periment O
with O
generating O
multiple B-DAT
attention O
maps O
to O
allow O
the O

model O
to O
make O
multiple B-DAT
“glimpses” O
which O
are O
concatenated O
before O

Answer O
Encoding. O
For O
VQA O
with O
multiple B-DAT
choices, O
we O
can O
additionally O
embed O

Figure O
4, O
to O
deal O
with O
multiple B-DAT
variable-length O
answer O
choices, O
each O
choice O

and O
an O
image O
along O
with O
multiple B-DAT
proposal O
bounding O
boxes. O
The O
goal O

4, O
we O
also O
report O
our O
multiple B-DAT

in O
Visual7W O
are O
in O
a O
multiple B-DAT

For O
VQA O
multiple B-DAT
choice, O
we O
train O
the O
open-ended O

take O
the O
argmax O
over O
the O
multiple B-DAT
choice O

Table O
4: O
Open-ended O
and O
multiple B-DAT

We O
also O
evaluated O
models O
with O
multiple B-DAT
atten- O
tion O
maps O
or O
channels O

smoothing O
effect O
occurs O
when O
using O
multiple B-DAT
maps O

presents O
results O
for O
the O
Visual7W O
multiple B-DAT

next O
best O
approach O
on O
the O
multiple B-DAT

visual O
grounding O
task. O
We O
present O
multiple B-DAT
ablations O
of O
our O
proposed O
architecture O

our O
architecture O
with O
attention O
and O
multiple B-DAT
MCBs O
gives O
significant O
improvements O
on O

Bilinear O
Pooling O
for O
Visual O
Question O
Answering B-DAT
and O
Visual O
Grounding O

4 O
Evaluation O
on O
Visual O
Question O
Answering B-DAT

The O
Visual O
Question O
Answering B-DAT
(VQA) O
real-image O
dataset O
(Antol O
et O

Co-Attention O
for O
Vi- O
sual O
Question O
Answering B-DAT

Learning O
Approach O
to O
Visual O
Question O
Answering B-DAT

Me O
Anything: O
Free-form O
Visual O
Question O
Answering B-DAT
Based O
on O
Knowledge O
from O
External O

Fei-Fei. O
2016. O
Visual7W: O
Grounded O
Question O
Answering B-DAT
in O
Images. O
In O
Proceedings O
of O

4 O
Evaluation O
on O
Visual O
Question O
Answering B-DAT

the O
Visual7W O
dataset O
and O
the O
VQA B-DAT
challenge O

such O
as O
visual O
question O
answering O
(VQA) B-DAT
and O
visual O
ground- O
ing, O
most O

multimodal O
pooling), O
current O
approaches O
in O
VQA B-DAT
or O
grounding O
rely O
on O
concatenat O

predict O
an- O
swers O
for O
the O
VQA B-DAT
task O
and O
locations O
for O
the O

we O
present O
an O
architecture O
for O
VQA B-DAT
which O
uses O
MCB O
twice, O
once O

additional O
training O
data O
for O
the O
VQA B-DAT
task. O
To O
sum- O
marize, O
MCB O

task O
of O
visual O
question O
answering O
(VQA) B-DAT
or O
visual O
grounding, O
we O
have O

then O
detail O
our O
architectures O
for O
VQA B-DAT
(Sec. O
3.2) O
and O
visual O
grounding O

and O
z O
∈ O
R3000 O
for O
VQA B-DAT

Figure O
3: O
Our O
architecture O
for O
VQA B-DAT

3.2 O
Architectures O
for O
VQA B-DAT

In O
VQA, B-DAT
the O
input O
to O
the O
model O

Yang O
et O
al., O
2015) O
for O
VQA, B-DAT
the O
soft O
attention O
mechanism O
can O

Answer O
Encoding. O
For O
VQA B-DAT
with O
multiple O
choices, O
we O
can O

Figure O
4: O
Our O
architecture O
for O
VQA B-DAT

The O
Visual O
Question O
Answering O
(VQA) B-DAT
real-image O
dataset O
(Antol O
et O
al O

data O
split. O
We O
use O
the O
VQA B-DAT
tool O
provided O
by O
Antol O
et O

and O
how). O
Compared O
to O
the O
VQA B-DAT
dataset, O
Vi- O
sual O
Genome O
represents O

Genome O
are O
larger O
than O
the O
VQA B-DAT
dataset. O
To O
leverage O
the O
Visual O

vocabulary O
space O
created O
from O
the O
VQA B-DAT
dataset, O
leaving O
us O
with O
addi O

Models O
are O
trained O
on O
the O
VQA B-DAT
train O
split O
and O
tested O
on O

2, O
we O
train O
on O
the O
VQA B-DAT
train O
split, O
vali- O
date O
on O

the O
VQA B-DAT
validation O
split, O
and O
report O
results O

on O
the O
VQA B-DAT
test-dev O
split. O
We O
use O
early O

training O
settings O
as O
in O
the O
VQA B-DAT
exper- O
iments. O
We O
use O
the O

For O
VQA B-DAT
multiple O
choice, O
we O
train O
the O

Models O
are O
trained O
on O
the O
VQA B-DAT
train O
split O
and O
tested O
on O

80.9 O
37.5 O
43.5 O
58.2 O
- O
VQA B-DAT
team O
(Antol O
et O
al., O
2015 O

and O
multiple-choice O
(MC) O
results O
on O
VQA B-DAT
test O
set O
(trained O
on O
train+val O

with O
the O
state-of-the- O
art O
on O
VQA B-DAT
test O
set. O
Our O
best O
single O

next O
best O
approach O
on O
the O
VQA B-DAT
open-ended O
task O
and O
0.8 O
points O

maps O
from O
MCB O
model O
on O
VQA B-DAT
images. O
Bottom: O
predicted O
grounding O
from O

gives O
significant O
improvements O
on O
two O
VQA B-DAT
datasets O
compared O
to O
state-of-the-art. O
In O

3.2 O
Architectures O
for O
VQA B-DAT

1606 B-DAT

1606 B-DAT

1606 B-DAT

256, O
and O
C O
= O
1000 B-DAT

with O
Joint O
Loss O
Minimization O
for O
VQA B-DAT

a O
single O
step O
prediction O
in O
VQA B-DAT
dataset O

Introduction O
Visual O
Question O
Answering O
(VQA) B-DAT
(Antol O
et O
al. O
2015) O
is O

scene O
classification, O
activity O
recognition, O
etc., O
VQA B-DAT
involves O
various O
recognition O
tasks O
at O

questions. O
Due O
to O
these O
reasons, O
VQA B-DAT
requires O
substantial O
amount O
of O
learning O

to O
improve O
the O
performance O
of O
VQA B-DAT
systems. O
After O
extract O

natural O
as O
many O
questions O
in O
VQA B-DAT
conceptually O
require O
multiple O
steps O
for O

front O
of O
the O
giraffe?” O
the O
VQA B-DAT
model O
should O
be O
able O
to O

deep O
recurrent O
neural O
network O
for O
VQA, B-DAT
which O
is O
composed O
of O
multiple O

We O
show O
that O
the O
VQA B-DAT
model O
involving O
multiple O
reason- O
ing O

a O
single O
step O
prediction O
in O
VQA B-DAT
dataset O

Related O
Work O
The O
VQA B-DAT
problem O
is O
first O
addressed O
by O

a O
shallow O
neural O
network O
for O
VQA, B-DAT
which O
accepts O
the O
CNN O
features O

questions O
as O
its O
inputs. O
Recently, O
VQA B-DAT
algorithms O
are O
often O
formulated O
with O

accuracy. O
These O
approaches O
typically O
pose O
VQA B-DAT
problems O
as O
simple O
classification O
tasks O

Several O
VQA B-DAT
systems O
(Yang O
et O
al. O
2016 O

to O
an O
object O
proposal O
for O
VQA B-DAT
while O
(Xiong, O
Merity, O
and O
Socher O

describes O
the O
general O
formulation O
of O
VQA B-DAT
and O
discusses O
our O
approach O
based O

Problem O
Formulation O
We O
formulate O
VQA B-DAT
problem O
as O
a O
classification O
task O

and O
a O
question O
q, O
a O
VQA B-DAT
model O
predicts O
the O
best O
answer O

which O
are O
frequently O
employed O
for O
VQA B-DAT
problems O
in O
recent O
years O

novel O
neural O
network O
architecture O
for O
VQA B-DAT
and O
propose O
a O
training O
strategy O

the O
proposed O
method O
resembles O
the O
VQA B-DAT
methods O
such O
as O
(Yang O
et O

based O
on O
our O
observations O
in O
VQA B-DAT
problems, O
but O
it O
is O
contradictory O

test O
the O
proposed O
network O
in O
VQA B-DAT
dataset O
(An- O
tol O
et O
al O

Two O
tasks O
are O
defined O
on O
VQA B-DAT
dataset: O
open-ended O
task O
and O
multple-choice O

Single O
model O
performance O
on O
the O
VQA B-DAT
test-dev O
dataset O
of O
all O
compared O

single O
model O
performance O
in O
the O
VQA B-DAT
test-standard. O
Asterisk O
(*) O
denotes O
the O

et O
al. O
2016) O
in O
the O
VQA B-DAT
dataset O
using O
the O
same O
image O

2.3, O
which O
are O
significant O
in O
VQA B-DAT
con- O
text. O
Both O
training O
schemes O

improve O
the O
accuracy O
of O
a O
VQA B-DAT
system O
by O
using O
a O
better O

1, O
and O
evaluated O
both O
in O
VQA B-DAT
test-dev O
and O
test-standard. O
The O
results O

Conclusion O
We O
proposed O
a O
VQA B-DAT
algorithm O
based O
on O
a O
recurrent O

outstanding O
performance O
in O
the O
standard O
VQA B-DAT
dataset O
with- O
out O
data O
augmentation O

as O
a O
general O
framework O
for O
VQA B-DAT
problems O
by O
replacing O
our O
answering O

L.; O
and O
Parikh, O
D. O
2015. O
VQA B-DAT

with O
Joint O
Loss O
Minimization O
for O
VQA B-DAT
Supplementary O
Document O

Full O
and O
Ours O
SS O
on O
VQA B-DAT
validation O
dataset O

approach O
to O
question O
answering O
about O
real B-DAT

learning O
to O
capture O
information O
from O
images B-DAT
and O
understand O
questions O

accepts O
the O
CNN O
features O
of O
images B-DAT
and O
the O
Bag- O
of-Words O
(BoW O

on O
the O
joint O
features O
from O
images B-DAT
and O
questions, O
where O
CNNs O
and O

1997) O
are O
employed O
to O
encode O
images B-DAT
and O
questions, O
respectively O
(Ren, O
Kiros O

various O
tasks O
related O
to O
input O
images B-DAT

et O
al. O
2015), O
which O
borrows O
images B-DAT
from O
MSCOCO O
dataset O
(Lin O
et O

encoders. O
After O
rescal- O
ing O
input O
images B-DAT
to O
448× O
448, O
we O
extract O

to O
the O
features O
for O
input O
images B-DAT
and O
questions O
in O
each O
answering O

approach O
to O
answering O
questions O
about O
images B-DAT

choice B-DAT
task. O
The O
model O
has O
to O

choice B-DAT
task O
In O
both O
cases, O
the O

Introduction O
Visual O
Question B-DAT
Answering O
(VQA) O
(Antol O
et O
al O

Introduction O
Visual B-DAT
Question O
Answering O
(VQA) O
(Antol O
et O

questions O
in O
VQA O
conceptually O
require O
multiple B-DAT
steps O
for O
reasoning. O
For O
example O

or O
a O
single O
subtask O
in O
multiple B-DAT
steps, O
which O
are O
likely O
to O

simply O
minimizing O
joint O
loss O
from O
multiple B-DAT
steps O
of O
our O
recurrent O
deep O

trained O
with O
joint O
loss O
from O
multiple B-DAT
steps, O
out- O
performs O
other O
training-testing O

VQA, O
which O
is O
composed O
of O
multiple B-DAT
answering O
units O
with O
shared O
parameters O

by O
minimizing O
joint O
loss O
from O
multiple B-DAT
units O

that O
the O
VQA O
model O
involving O
multiple B-DAT
reason- O
ing O
steps O
tends O
to O

motivation, O
neural O
module O
networks O
combine O
multiple B-DAT
module O
networks O
to O
construct O
a O

networks O
are O
sometimes O
trained O
with O
multiple B-DAT
supervisions O
to O
facilitate O
training O
procedure O

where O
losses O
are O
backpropagated O
from O
multiple B-DAT
branches. O
GoogLeNet O
(Szegedy O
et O
al O

its O
hidden O
state. O
By O
concatenating O
multiple B-DAT
answering O
units O
se- O
quentially, O
we O

overfitting O
in O
complex O
models O
among O
multiple B-DAT
answering O
units O

network, O
which O
is O
composed O
of O
multiple B-DAT
answering O
units O
with O
shared O
model O

We O
minimize O
joint O
loss O
from O
multiple B-DAT
steps O
by O
providing O
the O
ground-truth O

we O
observe O
that O
models O
with O
multiple B-DAT
steps O
generally O
overfit O
to O
training O

one O
of O
the O
answers O
from O
multiple B-DAT
answering O
units O
is O
not O
feasible O

ensemble O
of O
the O
answers O
from O
multiple B-DAT
units O
is O
not O
particularly O
better O

of O
18 O
candidate O
answers O
in O
multiple B-DAT

Figure O
3. O
Instead, O
we O
provide O
multiple B-DAT
supervision O
in O
our O
network, O
but O

model O
learned O
from O
all O
the O
multiple B-DAT
answering O
units O
without O
overfitting. O
The O

Zitnick, O
C. O
L. O
2014. O
Microsoft O
COCO B-DAT

Training O
Recurrent O
Answering B-DAT
Units O
with O
Joint O
Loss O
Minimization O

Introduction O
Visual O
Question O
Answering B-DAT
(VQA) O
(Antol O
et O
al. O
2015 O

question O
sentence O
q, O
respec- O
tively. O
Answering B-DAT
module O
takes O
extracted O
image O
and O

Answering B-DAT
Module O
Answering O
module O
is O
a O
recurrent O
neural O

To O
implement O
the O
Answering B-DAT
module, O
we O
need O
to O
learn O

Training O
Recurrent O
Answering B-DAT
Units O
with O
Joint O
Loss O
Minimization O

Answering B-DAT
Module O

with O
Joint O
Loss O
Minimization O
for O
VQA B-DAT

a O
single O
step O
prediction O
in O
VQA B-DAT
dataset O

Introduction O
Visual O
Question O
Answering O
(VQA) B-DAT
(Antol O
et O
al. O
2015) O
is O

scene O
classification, O
activity O
recognition, O
etc., O
VQA B-DAT
involves O
various O
recognition O
tasks O
at O

questions. O
Due O
to O
these O
reasons, O
VQA B-DAT
requires O
substantial O
amount O
of O
learning O

to O
improve O
the O
performance O
of O
VQA B-DAT
systems. O
After O
extract O

natural O
as O
many O
questions O
in O
VQA B-DAT
conceptually O
require O
multiple O
steps O
for O

front O
of O
the O
giraffe?” O
the O
VQA B-DAT
model O
should O
be O
able O
to O

deep O
recurrent O
neural O
network O
for O
VQA, B-DAT
which O
is O
composed O
of O
multiple O

We O
show O
that O
the O
VQA B-DAT
model O
involving O
multiple O
reason- O
ing O

a O
single O
step O
prediction O
in O
VQA B-DAT
dataset O

Related O
Work O
The O
VQA B-DAT
problem O
is O
first O
addressed O
by O

a O
shallow O
neural O
network O
for O
VQA, B-DAT
which O
accepts O
the O
CNN O
features O

questions O
as O
its O
inputs. O
Recently, O
VQA B-DAT
algorithms O
are O
often O
formulated O
with O

accuracy. O
These O
approaches O
typically O
pose O
VQA B-DAT
problems O
as O
simple O
classification O
tasks O

Several O
VQA B-DAT
systems O
(Yang O
et O
al. O
2016 O

to O
an O
object O
proposal O
for O
VQA B-DAT
while O
(Xiong, O
Merity, O
and O
Socher O

describes O
the O
general O
formulation O
of O
VQA B-DAT
and O
discusses O
our O
approach O
based O

Problem O
Formulation O
We O
formulate O
VQA B-DAT
problem O
as O
a O
classification O
task O

and O
a O
question O
q, O
a O
VQA B-DAT
model O
predicts O
the O
best O
answer O

which O
are O
frequently O
employed O
for O
VQA B-DAT
problems O
in O
recent O
years O

novel O
neural O
network O
architecture O
for O
VQA B-DAT
and O
propose O
a O
training O
strategy O

the O
proposed O
method O
resembles O
the O
VQA B-DAT
methods O
such O
as O
(Yang O
et O

based O
on O
our O
observations O
in O
VQA B-DAT
problems, O
but O
it O
is O
contradictory O

test O
the O
proposed O
network O
in O
VQA B-DAT
dataset O
(An- O
tol O
et O
al O

Two O
tasks O
are O
defined O
on O
VQA B-DAT
dataset: O
open-ended O
task O
and O
multple-choice O

Single O
model O
performance O
on O
the O
VQA B-DAT
test-dev O
dataset O
of O
all O
compared O

single O
model O
performance O
in O
the O
VQA B-DAT
test-standard. O
Asterisk O
(*) O
denotes O
the O

et O
al. O
2016) O
in O
the O
VQA B-DAT
dataset O
using O
the O
same O
image O

2.3, O
which O
are O
significant O
in O
VQA B-DAT
con- O
text. O
Both O
training O
schemes O

improve O
the O
accuracy O
of O
a O
VQA B-DAT
system O
by O
using O
a O
better O

1, O
and O
evaluated O
both O
in O
VQA B-DAT
test-dev O
and O
test-standard. O
The O
results O

Conclusion O
We O
proposed O
a O
VQA B-DAT
algorithm O
based O
on O
a O
recurrent O

outstanding O
performance O
in O
the O
standard O
VQA B-DAT
dataset O
with- O
out O
data O
augmentation O

as O
a O
general O
framework O
for O
VQA B-DAT
problems O
by O
replacing O
our O
answering O

L.; O
and O
Parikh, O
D. O
2015. O
VQA B-DAT

with O
Joint O
Loss O
Minimization O
for O
VQA B-DAT
Supplementary O
Document O

Full O
and O
Ours O
SS O
on O
VQA B-DAT
validation O
dataset O

100 B-DAT
layers. O
The O
very O
deep O
neural O

10044009 B-DAT

10060086 B-DAT

1601 B-DAT

1409 B-DAT

1207 B-DAT

1604 B-DAT

1506 B-DAT

1505 B-DAT

1603 B-DAT

81.81 O
38.43 O
48.43 O
MRN O
3 O
1200 B-DAT
33.9M O
61.68 O
82.28 O
38.82 O
49.25 O

We O
choose O
the O
Visual O
QA O
(VQA) B-DAT
dataset O
[2] O
for O
the O
evaluation O

questions O
and O
answers O
of O
the O
VQA B-DAT
dataset O
are O
collected O
via O
Amazon O

that O
TrimZero O
was O
suitable O
for O
VQA B-DAT
tasks. O
Approximately, O
37.5% O
of O
training O

Table O
3: O
The O
VQA B-DAT
test-standard O
results. O
The O
precision O
of O

The O
VQA B-DAT
Challenge, O
which O
released O
the O
VQA O

the O
training O
set O
of O
the O
VQA B-DAT
dataset, O
and O
visualized O
using O
the O

the O
state-of-the-art O
results O
on O
the O
VQA B-DAT
dataset O
for O
both O
Open-Ended O
and O

Zitnick, O
and O
Devi O
Parikh. O
VQA B-DAT

A.1 O
VQA B-DAT
test-dev O
Results O

effects O
of O
various O
options O
for O
VQA B-DAT
test-dev. O
Here, O
the O
model O
of O

Table O
5: O
The O
results O
for O
VQA B-DAT
test-dev. O
The O
precision O
of O
some O

shortcut O
connections O
of O
MRN O
for O
VQA B-DAT
test-dev. O
ResNet-152 O
features O
and O
2k O

A.1 O
VQA B-DAT
test-dev O
Results O

may O
help O
to O
solve O
the O
real B-DAT
world O
problems O
which O
need O
the O

as O
a O
sequence O
of O
three O
images B-DAT

The O
images B-DAT
come O
from O
the O
MS-COCO O
dataset O

and O
81,434 O
for O
test. O
The O
images B-DAT
are O
carefully O
collected O
to O
contain O

three-block O
layered O
MRN. O
The O
original O
images B-DAT
are O
shown O
in O
the O
first O

each O
group. O
The O
next O
three O
images B-DAT
show O
the O
input O
gradients O
of O

effect O
(bright O
color) O
on O
the O
images B-DAT

choice B-DAT
of O
the O
shortcuts O
for O
each O

29] O
assume O
that O
an O
appropriate O
choice B-DAT
of O
weights O
on O
visual O
feature O

joint O
representation. O
We O
justify O
this O
choice B-DAT
in O
Sections O
4 O
and O
5 O

Networks O
for O
Question B-DAT
Answering. O
arXiv O
preprint O
arXiv:1601.01705, O
2016 O

and O
Devi O
Parikh. O
VQA: O
Visual O
Question B-DAT
Answering. O
In O
International O
Conference O
on O

Dynamic O
Attention O
Model O
for O
Visual O
Question B-DAT
Answering. O
arXiv O
preprint O
arXiv:1604.01485, O
2016 O

LSTM O
and O
normalized O
CNN O
Visual O
Question B-DAT
Answering O
model. O
https://github.com/VT-vision-lab/VQA_LSTM_CNN, O
2015 O

Seo, O
and O
Bohyung O
Han. O
Image O
Question B-DAT
Answering O
us- O
ing O
Convolutional O
Neural O

Models O
and O
Data O
for O
Image O
Question B-DAT
Answering. O
In O
Advances O
in O
Neural O

Ask O
Me O
Anything: O
Free-form O
Visual O
Question B-DAT
Answering O
Based O
on O
Knowledge O
from O

Networks O
for O
Visual O
and O
Textual O
Question B-DAT
Answering. O
arXiv O
preprint O
arXiv:1603.01417, O
2016 O

Stacked O
Attention O
Networks O
for O
Image O
Question B-DAT
Answering. O
arXiv O
preprint O
arXiv:1511.02274, O
2015 O

Question B-DAT
[2] O
48.09 O
75.66 O
36.70 O
27.14 O

Multimodal O
Residual O
Learning O
for O
Visual B-DAT
QA O

the O
state-of-the-art O
results O
on O
the O
Visual B-DAT
QA O
dataset O
for O
both O
Open-Ended O

Visual B-DAT
question-answering O
tasks O
provide O
a O
testbed O

the O
state-of-the-art O
results O
on O
the O
Visual B-DAT
QA O
dataset O
for O
both O
Open-Ended O

4.1 O
Visual B-DAT
QA O
Dataset O

We O
choose O
the O
Visual B-DAT
QA O
(VQA) O
dataset O
[2] O
for O

Visual B-DAT
Features O
The O
ResNet-152 O
visual O
features O

Zitnick, O
and O
Devi O
Parikh. O
VQA: O
Visual B-DAT
Question O
Answering. O
In O
International O
Conference O

Focused O
Dynamic O
Attention O
Model O
for O
Visual B-DAT
Question O
Answering. O
arXiv O
preprint O
arXiv:1604.01485 O

Karpathy O
and O
Li O
Fei-Fei. O
Deep O
Visual B-DAT

Deeper O
LSTM O
and O
normalized O
CNN O
Visual B-DAT
Question O
Answering O
model. O
https://github.com/VT-vision-lab/VQA_LSTM_CNN, O
2015 O

Hengel. O
Ask O
Me O
Anything: O
Free-form O
Visual B-DAT
Question O
Answering O
Based O
on O
Knowledge O

Socher. O
Dynamic O
Memory O
Networks O
for O
Visual B-DAT
and O
Textual O
Question O
Answering. O
arXiv O

4.1 O
Visual B-DAT
QA O
Dataset O

consistently O
shows O
state-of-the-art O
results O
across O
multiple B-DAT
visual O
tasks O
including O
image O
classification O

for O
the O
linear O
combination O
of O
multiple B-DAT
visual O
feature O
vectors O
indexing O
spatial O

element-wise O
multiplication, O
instead O
of O
the O
multiple B-DAT
(spatial) O
visual O
features O
approach O
for O

MRN O
consists O
of O
multiple B-DAT
learning O
blocks, O
which O
are O
stacked O

are O
carefully O
collected O
to O
contain O
multiple B-DAT
objects O
and O
natural O
situations, O
which O

using O
question-only O
shortcuts O
in O
the O
multiple B-DAT
blocks O
as O
in O
[29]. O
For O

COCO B-DAT
dataset, O
123,287 O
of O
them O
for O

and O
C O
Lawrence O
Zitnick. O
Microsoft O
COCO B-DAT

Networks O
for O
Question O
Answering B-DAT

Devi O
Parikh. O
VQA: O
Visual O
Question O
Answering B-DAT

Attention O
Model O
for O
Visual O
Question O
Answering B-DAT

and O
normalized O
CNN O
Visual O
Question O
Answering B-DAT
model. O
https://github.com/VT-vision-lab/VQA_LSTM_CNN, O
2015 O

Neurons: O
A O
Neural-based O
Approach O
to O
Answering B-DAT
Questions O
about O
Images. O
arXiv O
preprint O

and O
Bohyung O
Han. O
Image O
Question O
Answering B-DAT
us- O
ing O
Convolutional O
Neural O
Network O

and O
Data O
for O
Image O
Question O
Answering B-DAT

Me O
Anything: O
Free-form O
Visual O
Question O
Answering B-DAT
Based O
on O
Knowledge O
from O
External O

for O
Visual O
and O
Textual O
Question O
Answering B-DAT

Attention O
Networks O
for O
Image O
Question O
Answering B-DAT

We O
choose O
the O
Visual O
QA O
(VQA) B-DAT
dataset O
[2] O
for O
the O
evaluation O

questions O
and O
answers O
of O
the O
VQA B-DAT
dataset O
are O
collected O
via O
Amazon O

that O
TrimZero O
was O
suitable O
for O
VQA B-DAT
tasks. O
Approximately, O
37.5% O
of O
training O

Table O
3: O
The O
VQA B-DAT
test-standard O
results. O
The O
precision O
of O

The O
VQA B-DAT
Challenge, O
which O
released O
the O
VQA O

the O
training O
set O
of O
the O
VQA B-DAT
dataset, O
and O
visualized O
using O
the O

the O
state-of-the-art O
results O
on O
the O
VQA B-DAT
dataset O
for O
both O
Open-Ended O
and O

Zitnick, O
and O
Devi O
Parikh. O
VQA B-DAT

A.1 O
VQA B-DAT
test-dev O
Results O

effects O
of O
various O
options O
for O
VQA B-DAT
test-dev. O
Here, O
the O
model O
of O

Table O
5: O
The O
results O
for O
VQA B-DAT
test-dev. O
The O
precision O
of O
some O

shortcut O
connections O
of O
MRN O
for O
VQA B-DAT
test-dev. O
ResNet-152 O
features O
and O
2k O

A.1 O
VQA B-DAT
test-dev O
Results O

answers. O
We O
use O
the O
top O
1000 B-DAT
most O
frequent O
answers O
as O
the O

1-0679 B-DAT
to O
DB, O
a O
Sloan O
Fellowship O

1606 B-DAT

1606 B-DAT

1604 B-DAT

1606 B-DAT

1602 B-DAT

1602 B-DAT

1409 B-DAT

models O
for O
Visual O
Question O
Answering O
(VQA) B-DAT
that O
generate O
spatial O
maps O
highlighting O

a O
novel O
co-attention O
model O
for O
VQA B-DAT
that O
jointly O
reasons O
about O
image O

improves O
the O
state-of-the-art O
on O
the O
VQA B-DAT
dataset O
from O
60.3% O
to O
60.5 O

further O
improved O
to O
62.1% O
for O
VQA B-DAT
and O
65.4% O
for O
COCO-QA.1 O

1 O
Introduction O
Visual O
Question O
Answering O
(VQA) B-DAT
[2, O
7, O
16, O
17, O
29 O

23–25] O
have O
been O
explored O
for O
VQA, B-DAT
where O
the O
attention O
mechanism O
typically O

far, O
all O
attention O
models O
for O
VQA B-DAT
in O
literature O
have O
focused O
on O

novel O
multi-modal O
attention O
model O
for O
VQA B-DAT
with O
the O
following O
two O
unique O

a O
novel O
co-attention O
mechanism O
for O
VQA B-DAT
that O
jointly O
performs O
question-guided O
visual O

model O
on O
two O
large O
datasets, O
VQA B-DAT
[2] O
and O
COCO-QA O
[17 O

6] O
have O
proposed O
models O
for O
VQA B-DAT

explored O
image O
attention O
models O
for O
VQA B-DAT

generated O
by O
attention O
models O
for O
VQA B-DAT

has O
explored O
question O
attention O
in O
VQA, B-DAT
there O
are O
some O
related O
works O

Following O
[2], O
we O
treat O
VQA B-DAT
as O
a O
classification O
task. O
We O

model O
on O
two O
datasets, O
the O
VQA B-DAT
dataset O
[2] O
and O
the O
COCO-QA O

VQA B-DAT
dataset O
[2] O
is O
the O
largest O

we O
train O
our O
model O
on O
VQA B-DAT
train+val O
and O
report O
the O
test-dev O

and O
test-standard O
results O
from O
the O
VQA B-DAT
evaluation O
server. O
We O
use O
the O

Table O
1: O
Results O
on O
the O
VQA B-DAT
dataset. O
“-” O
indicates O
the O
results O

to O
512 O
and O
1024 O
for O
VQA B-DAT
since O
it O
is O
a O
much O

are O
two O
test O
scenarios O
on O
VQA B-DAT

1 O
shows O
results O
on O
the O
VQA B-DAT
test O
sets O
for O
both O
open-ended O

Similar O
to O
the O
result O
on O
VQA, B-DAT
our O
model O
improves O
the O
state-of-the-art O

w.r.t O
these O
ablations O
on O
the O
VQA B-DAT
validation O
set O
(test O
sets O
are O

of O
previous O
attention O
models O
for O
VQA B-DAT
[23, O
25 O

3: O
Ablation O
study O
on O
the O
VQA B-DAT
dataset O
using O
Oursa+VGG O

three O
columns O
using O
Oursp+VGG) O
and O
VQA B-DAT
(last O
two O
columns O
Oursa+VGG) O
dataset O

three O
columns O
using O
Oursp+VGG) O
and O
VQA B-DAT
(last O
two O
columns O
Oursa+VGG) O
dataset O

based O
on O
8,000 O
and O
4,000 O
images B-DAT
respectively. O
There O
are O
four O
types O

attention O
has O
different O
patterns O
across O
images B-DAT

. O
For O
the O
first O
two O
images, B-DAT
the O
attention O
transfers O
from O
objects O

attends O
to O
the O
regions O
in O
images B-DAT
and O
phrases O
in O
the O
questions O

co-attends O
to O
interpretable O
regions O
of O
images B-DAT
and O
questions O
for O
predicting O
the O

approach O
to O
answering O
questions O
about O
images B-DAT

Visual7w: O
Grounded O
question O
answering O
in O
images B-DAT

to O
the O
question O
and O
answer O
choice B-DAT

choice B-DAT

and O
DMN+ O
[23]. O
For O
multiple O
choice, B-DAT
we O
compare O
with O
Region O
Sel O

choice B-DAT
settings. O
We O
can O
see O
that O

choice B-DAT

choice B-DAT
questions. O
As O
we O
can O
see O

Hierarchical O
Question-Image B-DAT
Co-Attention O
for O
Visual O
Question O
Answering O

proposed O
attention O
models O
for O
Visual O
Question B-DAT
Answering O
(VQA) O
that O
generate O
spatial O

1 O
Introduction O
Visual O
Question B-DAT
Answering O
(VQA) O
[2, O
7, O
16 O

Question B-DAT
Hierarchy: O
We O
build O
a O
hierarchical O

3.2 O
Question B-DAT
Hierarchy O

Question B-DAT
Attention O
alone, O
where O
no O
image O

Atten O
79.8 O
33.9 O
43.6 O
55.9 O
Question B-DAT
Atten O
79.4 O
33.3 O
41.7 O
54.8 O

3.2 O
Question B-DAT
Hierarchy O

Hierarchical O
Question-Image O
Co-Attention O
for O
Visual B-DAT
Question O
Answering O

have O
proposed O
attention O
models O
for O
Visual B-DAT
Question O
Answering O
(VQA) O
that O
generate O

1 O
Introduction O
Visual B-DAT
Question O
Answering O
(VQA) O
[2, O
7 O

Zitnick, O
and O
Devi O
Parikh. O
Vqa: O
Visual B-DAT
question O
answering. O
In O
ICCV, O
2015 O

David O
A O
Shamma, O
et O
al. O
Visual B-DAT
genome: O
Connecting O
language O
and O
vision O

other O
works O
perform O
image O
attention O
multiple B-DAT
times O
in O
a O
stacked O
manner O

stacked O
attention O
network, O
which O
runs O
multiple B-DAT
hops O
to O
infer O
the O
answer O

scenarios O
on O
VQA: O
open-ended O
and O
multiple B-DAT

11] O
and O
DMN+ O
[23]. O
For O
multiple B-DAT
choice, O
we O
compare O
with O
Region O

sets O
for O
both O
open-ended O
and O
multiple B-DAT

11]) O
to O
66.1% O
(Oursa+ResNet) O
on O
multiple B-DAT

and O
4.0% O
and O
1.1% O
on O
multiple B-DAT

61.6% O
to O
63.3% O
on O
the O
COCO B-DAT

for O
VQA O
and O
65.4% O
for O
COCO B-DAT

large O
datasets, O
VQA O
[2] O
and O
COCO B-DAT

VQA O
dataset O
[2] O
and O
the O
COCO B-DAT

questions O
and O
answers O
on O
Microsoft O
COCO B-DAT
dataset O
[14]. O
The O
dataset O
contains O

COCO B-DAT

from O
captions O
in O
the O
Microsoft O
COCO B-DAT
dataset O
[14]. O
There O
are O
78,736 O

the O
last O
5 O
epochs. O
For O
COCO B-DAT

15] O
and O
SAN O
[25] O
on O
COCO B-DAT

2 O
shows O
results O
on O
the O
COCO B-DAT

Table O
2: O
Results O
on O
the O
COCO B-DAT

question O
co-attention O
maps O
on O
the O
COCO B-DAT

on O
success O
cases O
in O
the O
COCO B-DAT

on O
failure O
cases O
in O
the O
COCO B-DAT

Question-Image O
Co-Attention O
for O
Visual O
Question O
Answering B-DAT

attention O
models O
for O
Visual O
Question O
Answering B-DAT
(VQA) O
that O
generate O
spatial O
maps O

1 O
Introduction O
Visual O
Question O
Answering B-DAT
(VQA) O
[2, O
7, O
16, O
17 O

models O
for O
Visual O
Question O
Answering O
(VQA) B-DAT
that O
generate O
spatial O
maps O
highlighting O

a O
novel O
co-attention O
model O
for O
VQA B-DAT
that O
jointly O
reasons O
about O
image O

improves O
the O
state-of-the-art O
on O
the O
VQA B-DAT
dataset O
from O
60.3% O
to O
60.5 O

further O
improved O
to O
62.1% O
for O
VQA B-DAT
and O
65.4% O
for O
COCO-QA.1 O

1 O
Introduction O
Visual O
Question O
Answering O
(VQA) B-DAT
[2, O
7, O
16, O
17, O
29 O

23–25] O
have O
been O
explored O
for O
VQA, B-DAT
where O
the O
attention O
mechanism O
typically O

far, O
all O
attention O
models O
for O
VQA B-DAT
in O
literature O
have O
focused O
on O

novel O
multi-modal O
attention O
model O
for O
VQA B-DAT
with O
the O
following O
two O
unique O

a O
novel O
co-attention O
mechanism O
for O
VQA B-DAT
that O
jointly O
performs O
question-guided O
visual O

model O
on O
two O
large O
datasets, O
VQA B-DAT
[2] O
and O
COCO-QA O
[17 O

6] O
have O
proposed O
models O
for O
VQA B-DAT

explored O
image O
attention O
models O
for O
VQA B-DAT

generated O
by O
attention O
models O
for O
VQA B-DAT

has O
explored O
question O
attention O
in O
VQA, B-DAT
there O
are O
some O
related O
works O

Following O
[2], O
we O
treat O
VQA B-DAT
as O
a O
classification O
task. O
We O

model O
on O
two O
datasets, O
the O
VQA B-DAT
dataset O
[2] O
and O
the O
COCO-QA O

VQA B-DAT
dataset O
[2] O
is O
the O
largest O

we O
train O
our O
model O
on O
VQA B-DAT
train+val O
and O
report O
the O
test-dev O

and O
test-standard O
results O
from O
the O
VQA B-DAT
evaluation O
server. O
We O
use O
the O

Table O
1: O
Results O
on O
the O
VQA B-DAT
dataset. O
“-” O
indicates O
the O
results O

to O
512 O
and O
1024 O
for O
VQA B-DAT
since O
it O
is O
a O
much O

are O
two O
test O
scenarios O
on O
VQA B-DAT

1 O
shows O
results O
on O
the O
VQA B-DAT
test O
sets O
for O
both O
open-ended O

Similar O
to O
the O
result O
on O
VQA, B-DAT
our O
model O
improves O
the O
state-of-the-art O

w.r.t O
these O
ablations O
on O
the O
VQA B-DAT
validation O
set O
(test O
sets O
are O

of O
previous O
attention O
models O
for O
VQA B-DAT
[23, O
25 O

3: O
Ablation O
study O
on O
the O
VQA B-DAT
dataset O
using O
Oursa+VGG O

three O
columns O
using O
Oursp+VGG) O
and O
VQA B-DAT
(last O
two O
columns O
Oursa+VGG) O
dataset O

three O
columns O
using O
Oursp+VGG) O
and O
VQA B-DAT
(last O
two O
columns O
Oursa+VGG) O
dataset O

6,16,3,19] O
is O
to O
use O
the O
1,000 B-DAT
most O
common O
answers O
in O
the O

follow O
[6] O
and O
use O
the O
1000 B-DAT
most O
common O
answers O

classification O
into O
one O
of O
the O
1,000 B-DAT
most O
common O
answers. O
The O
implementation O

1506 B-DAT

1105 B-DAT

1409 B-DAT

1601 B-DAT

1503 B-DAT

Abstract. O
Visual O
Question O
and O
Answering O
(VQA) B-DAT
problems O
are O
attract- O
ing O
increasing O

from O
multiple O
research O
disciplines. O
Solving O
VQA B-DAT
problems O
requires O
techniques O
from O
both O

mod- O
eling, O
most O
of O
existing O
VQA B-DAT
methods O
adopt O
the O
strategy O
of O

on O
a O
large-scale O
benchmark O
dataset, O
VQA, B-DAT
clearly O
demonstrate O
the O
superior O
performance O

Visual O
question O
answering O
(VQA) B-DAT
is O
an O
active O
research O
direction O

from O
multiple O
communities. O
Generally, O
the O
VQA B-DAT
investigates O
a O
generalization O
of O
traditional O

to O
be O
considered. O
More O
concretely, O
VQA B-DAT
is O
about O
how O
to O
provide O

VQA B-DAT
is O
a O
quite O
challenging O
task O

developing O
modern O
AI O
systems. O
The O
VQA B-DAT
problem O
can O
be O
regarded O
as O

Recently, O
VQA B-DAT
is O
advanced O
significantly O
by O
the O

basket?”. O
Answering O
this O
question O
requires O
VQA B-DAT
methods O
to O
first O
understand O
the O

The O
first O
feasible O
solution O
to O
VQA B-DAT
problems O
was O
provided O
by O
Malinowski O

Fritz O
also O
constructed O
the O
first O
VQA B-DAT
bench- O
mark O
dataset, O
named O
as O

trained O
and O
evaluated O
on O
the O
VQA B-DAT
problem O
[4,5,3]. O
More O
recently, O
Antol O

6] O
published O
the O
currently O
largest O
VQA B-DAT
dataset. O
It O
consists O
of O
three O

and O
real-world O
knowledge, O
making O
the O
VQA B-DAT
dataset O
suitable O
for O
a O
true O

Visual O
Turing O
Test. O
The O
VQA B-DAT
authors O
split O
the O
evaluation O
on O

the O
critical O
factors O
for O
solving O
VQA B-DAT
problems O
well. O
A O
common O
practice O

with O
existing O
VQA B-DAT
methods O
on O
model- O
ing O
image O

and O
thus O
hurt O
the O
overall O
VQA B-DAT
performance O

given O
question. O
Recall O
the O
above O
VQA B-DAT
example. O
To O
answer O
the O
question O

these O
regions O
of O
interest. O
Then O
VQA B-DAT
compliments O
the O
features O
from O
selected O

model O
on O
two O
types O
of O
VQA B-DAT
tasks, O
i.e., O
the O
open-ended O
task O

the O
multiple-choice O
task, O
on O
the O
VQA B-DAT
dataset O
– O
the O
largest O
VQA O

and O
on O
the O
multiple- O
choice O
VQA B-DAT
tasks O

2 O
we O
review O
the O
current O
VQA B-DAT
models, O
and O
compare O
them O
to O

VQA B-DAT
has O
received O
great O
research O
attention O

uses O
attention O
mechanism O
in O
solving O
VQA B-DAT
problems O
is O
the O
ABC-CNN O
model O

the O
different O
sub-tasks O
of O
the O
VQA B-DAT
problem O
(e.g. O
counting, O
locating O
an O

set O
and O
thus O
simplify O
the O
VQA B-DAT
task O
to O
a O
classification O
problem O

4 O
Focused O
Dynamic O
Attention O
for O
VQA B-DAT

for O
two O
images O
from O
the O
VQA B-DAT
dataset. O
Examples O
provided O
by O
[6 O

use O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset O
[6], O
which O
is O
the O

baseline O
models O
provided O
by O
the O
VQA B-DAT
dataset O
authors O
[27], O
which O
currently O

standard O
implementation O
of O
an O
LSTM+CNN O
VQA B-DAT
model. O
It O
uses O
an O
LSTM O

and O
our O
FDA O
model O
on O
VQA B-DAT
test-dev O
and O
test-standard O
data O
for O

VQA B-DAT
Question O
48.09 O
75.66 O
27.14 O
36.70 O

and O
our O
FDA O
model O
on O
VQA B-DAT
test-dev O
and O
test-standard O
data O
for O

VQA B-DAT
Question O
53.68 O
75.71 O
38.64 O
37.05 O

the O
baselines O
provided O
by O
the O
VQA B-DAT
authors O
[6]. O
The O
results O
for O

nificant O
when O
solving O
the O
multiple-choice O
VQA B-DAT
problems. O
From O
Table O
2, O
one O

best O
ever O
performance O
on O
the O
VQA B-DAT
dataset. O
In O
particular, O
it O
improves O

model O
to O
solve O
the O
challenging O
VQA B-DAT
problems. O
FDA O
is O
built O
upon O

improvement O
over O
baselines O
on O
the O
VQA B-DAT
benchmark O
datasets, O
for O
both O
the O

open-ended O
and O
multiple-choices O
VQA B-DAT
tasks. O
Excellent O
performance O
of O
FDA O

attention O
to O
visual O
part O
in O
VQA B-DAT
tasks O
could O
essentially O
improve O
the O

complex O
reasoning, O
common O
sense, O
and O
real B-DAT

approach O
to O
question O
answering O
about O
real B-DAT

repre- O
sentations O
of O
questions O
and O
images, B-DAT
align O
and O
fuse O
them O
in O

as O
DAQUAR, O
which O
contains O
1,449 O
images B-DAT
and O
12,468 O
questions O
generated O
by O

contains O
a O
large O
number O
of O
images B-DAT
(123,287) O
and O
questions O
(117,684), O
but O

each O
one O
of O
the O
204,721 O
images B-DAT
found O
in O
the O
Microsoft O
COCO O

the O
image O
(blue) O
for O
two O
images B-DAT
from O
the O
VQA O
dataset. O
Examples O

each O
one O
of O
the O
204,721 O
images B-DAT
found O
in O
the O
Microsoft O
COCO O

proach O
to O
answering O
questions O
about O
images B-DAT

choice B-DAT
task, O
where O
for O
each O
question O

choice B-DAT
task, O
on O
the O
VQA O
dataset O

open-ended, O
and O
on O
the O
multiple- O
choice B-DAT
VQA O
tasks O

choice B-DAT
question). O
We O
believe O
that O
the O

choice B-DAT
task, O
where O
for O
each O
question O

choice B-DAT
task. O
The O
model, O
first O
described O

choice B-DAT
task. O
Results O
from O
most O
recent O

choice B-DAT
task O
are O
given O
in O
Table O

choice B-DAT
VQA O
problems. O
From O
Table O
2 O

Dynamic O
Attention O
Model O
for O
Visual O
Question B-DAT
Answering O

Abstract. O
Visual O
Question B-DAT
and O
Answering O
(VQA) O
problems O
are O

Keywords: O
Visual O
Question B-DAT
Answering, O
Attention O

Dynamic O
Attention O
Model O
for O
Visual O
Question B-DAT
Answering O
3 O

Dynamic O
Attention O
(FDA) O
for O
Visual O
Question B-DAT
Answering. O
With O
the O
FDA O
model O

Dynamic O
Attention O
Model O
for O
Visual O
Question B-DAT
Answering O
5 O

4.1 O
Question B-DAT
Understanding O

Question B-DAT

Dynamic O
Attention O
Model O
for O
Visual O
Question B-DAT
Answering O
7 O

experiments O
we O
use O
the O
Visual O
Question B-DAT
Answering O
(VQA) O
dataset O
[6], O
which O

Dynamic O
Attention O
Model O
for O
Visual O
Question B-DAT
Answering O
9 O

VQA O
Question B-DAT
48.09 O
75.66 O
27.14 O
36.70 O

VQA O
Question B-DAT
53.68 O
75.71 O
38.64 O
37.05 O

Question B-DAT

Dynamic O
Attention O
Model O
for O
Visual O
Question B-DAT
Answering O
11 O

Dynamic O
Attention O
Model O
for O
Visual O
Question B-DAT
Answering O
13 O

Dynamic O
Attention O
Model O
for O
Visual O
Question B-DAT
Answering O
15 O

Dynamic O
Attention O
Model O
for O
Visual O
Question B-DAT
Answering O

Focused O
Dynamic O
Attention O
Model O
for O
Visual B-DAT
Question O
Answering O

Abstract. O
Visual B-DAT
Question O
and O
Answering O
(VQA) O
problems O

Keywords: O
Visual B-DAT
Question O
Answering, O
Attention O

Visual B-DAT
question O
answering O
(VQA) O
is O
an O

can O
be O
regarded O
as O
a O
Visual B-DAT
Turing O
Test O

dataset O
suitable O
for O
a O
true O
Visual B-DAT
Turing O
Test. O
The O
VQA O
authors O

Focused O
Dynamic O
Attention O
Model O
for O
Visual B-DAT
Question O
Answering O
3 O

Focused O
Dynamic O
Attention O
(FDA) O
for O
Visual B-DAT
Question O
Answering. O
With O
the O
FDA O

Focused O
Dynamic O
Attention O
Model O
for O
Visual B-DAT
Question O
Answering O
5 O

Focused O
Dynamic O
Attention O
Model O
for O
Visual B-DAT
Question O
Answering O
7 O

all O
experiments O
we O
use O
the O
Visual B-DAT
Question O
Answering O
(VQA) O
dataset O
[6 O

Focused O
Dynamic O
Attention O
Model O
for O
Visual B-DAT
Question O
Answering O
9 O

Focused O
Dynamic O
Attention O
Model O
for O
Visual B-DAT
Question O
Answering O
11 O

Focused O
Dynamic O
Attention O
Model O
for O
Visual B-DAT
Question O
Answering O
13 O

S., O
Hallonquist, O
N., O
Younes, O
L.: O
Visual B-DAT
turing O
test O
for O
computer O
vision O

Zitnick, O
C., O
Parikh, O
D.: O
Vqa: O
Visual B-DAT
question O
answering. O
In: O
The O
IEEE O

Focused O
Dynamic O
Attention O
Model O
for O
Visual B-DAT
Question O
Answering O
15 O

Focused O
Dynamic O
Attention O
Model O
for O
Visual B-DAT
Question O
Answering O

attract- O
ing O
increasing O
interest O
from O
multiple B-DAT
research O
disciplines. O
Solving O
VQA O
problems O

received O
great O
research O
attention O
from O
multiple B-DAT
communities. O
Generally, O
the O
VQA O
investigates O

language O
an- O
swer, O
and O
a O
multiple B-DAT

the O
open-ended O
task O
and O
the O
multiple B-DAT

the O
open-ended, O
and O
on O
the O
multiple B-DAT

open-ended O
questions, O
and O
2.42% O
for O
multiple B-DAT

A O
multiple B-DAT

the O
test-standard O
split O
for O
the O
multiple B-DAT

and O
test-standard O
data O
for O
the O
multiple B-DAT

and O
the O
results O
for O
the O
multiple B-DAT

sig- O
nificant O
when O
solving O
the O
multiple B-DAT

well O
as O
a O
stack O
of O
multiple B-DAT
LSTM O
layers O
for O
feature O
fusion O

for O
both O
the O
open-ended O
and O
multiple B-DAT

images O
found O
in O
the O
Microsoft O
COCO B-DAT
dataset O
[7]. O
Answering O
the O
614,163 O

model O
used O
in O
ILSVRC O
and O
COCO B-DAT
2015 O
competitions, O
which O
won O
the O

classification, O
ImageNet O
detection, O
ImageNet O
localization, O
COCO B-DAT
detection, O
and O
COCO O
segmentation O
[24 O

images O
found O
in O
the O
Microsoft O
COCO B-DAT
dataset O
[7]. O
Figure O
2 O
shows O

Attention O
Model O
for O
Visual O
Question O
Answering B-DAT

Abstract. O
Visual O
Question O
and O
Answering B-DAT
(VQA) O
problems O
are O
attract- O
ing O

Keywords: O
Visual O
Question O
Answering, B-DAT
Attention O

apples O
are O
in O
the O
basket?”. O
Answering B-DAT
this O
question O
requires O
VQA O
methods O

the O
Microsoft O
COCO O
dataset O
[7]. O
Answering B-DAT
the O
614,163 O
questions O
requires O
complex O

Attention O
Model O
for O
Visual O
Question O
Answering B-DAT
3 O

Attention O
(FDA) O
for O
Visual O
Question O
Answering B-DAT

Attention O
Model O
for O
Visual O
Question O
Answering B-DAT
5 O

Attention O
Model O
for O
Visual O
Question O
Answering B-DAT
7 O

we O
use O
the O
Visual O
Question O
Answering B-DAT
(VQA) O
dataset O
[6], O
which O
is O

Attention O
Model O
for O
Visual O
Question O
Answering B-DAT
9 O

Attention O
Model O
for O
Visual O
Question O
Answering B-DAT
11 O

Attention O
Model O
for O
Visual O
Question O
Answering B-DAT
13 O

Attention O
Model O
for O
Visual O
Question O
Answering B-DAT
15 O

Attention O
Model O
for O
Visual O
Question O
Answering B-DAT

Abstract. O
Visual O
Question O
and O
Answering O
(VQA) B-DAT
problems O
are O
attract- O
ing O
increasing O

from O
multiple O
research O
disciplines. O
Solving O
VQA B-DAT
problems O
requires O
techniques O
from O
both O

mod- O
eling, O
most O
of O
existing O
VQA B-DAT
methods O
adopt O
the O
strategy O
of O

on O
a O
large-scale O
benchmark O
dataset, O
VQA, B-DAT
clearly O
demonstrate O
the O
superior O
performance O

Visual O
question O
answering O
(VQA) B-DAT
is O
an O
active O
research O
direction O

from O
multiple O
communities. O
Generally, O
the O
VQA B-DAT
investigates O
a O
generalization O
of O
traditional O

to O
be O
considered. O
More O
concretely, O
VQA B-DAT
is O
about O
how O
to O
provide O

VQA B-DAT
is O
a O
quite O
challenging O
task O

developing O
modern O
AI O
systems. O
The O
VQA B-DAT
problem O
can O
be O
regarded O
as O

Recently, O
VQA B-DAT
is O
advanced O
significantly O
by O
the O

basket?”. O
Answering O
this O
question O
requires O
VQA B-DAT
methods O
to O
first O
understand O
the O

The O
first O
feasible O
solution O
to O
VQA B-DAT
problems O
was O
provided O
by O
Malinowski O

Fritz O
also O
constructed O
the O
first O
VQA B-DAT
bench- O
mark O
dataset, O
named O
as O

trained O
and O
evaluated O
on O
the O
VQA B-DAT
problem O
[4,5,3]. O
More O
recently, O
Antol O

6] O
published O
the O
currently O
largest O
VQA B-DAT
dataset. O
It O
consists O
of O
three O

and O
real-world O
knowledge, O
making O
the O
VQA B-DAT
dataset O
suitable O
for O
a O
true O

Visual O
Turing O
Test. O
The O
VQA B-DAT
authors O
split O
the O
evaluation O
on O

the O
critical O
factors O
for O
solving O
VQA B-DAT
problems O
well. O
A O
common O
practice O

with O
existing O
VQA B-DAT
methods O
on O
model- O
ing O
image O

and O
thus O
hurt O
the O
overall O
VQA B-DAT
performance O

given O
question. O
Recall O
the O
above O
VQA B-DAT
example. O
To O
answer O
the O
question O

these O
regions O
of O
interest. O
Then O
VQA B-DAT
compliments O
the O
features O
from O
selected O

model O
on O
two O
types O
of O
VQA B-DAT
tasks, O
i.e., O
the O
open-ended O
task O

the O
multiple-choice O
task, O
on O
the O
VQA B-DAT
dataset O
– O
the O
largest O
VQA O

and O
on O
the O
multiple- O
choice O
VQA B-DAT
tasks O

2 O
we O
review O
the O
current O
VQA B-DAT
models, O
and O
compare O
them O
to O

VQA B-DAT
has O
received O
great O
research O
attention O

uses O
attention O
mechanism O
in O
solving O
VQA B-DAT
problems O
is O
the O
ABC-CNN O
model O

the O
different O
sub-tasks O
of O
the O
VQA B-DAT
problem O
(e.g. O
counting, O
locating O
an O

set O
and O
thus O
simplify O
the O
VQA B-DAT
task O
to O
a O
classification O
problem O

4 O
Focused O
Dynamic O
Attention O
for O
VQA B-DAT

for O
two O
images O
from O
the O
VQA B-DAT
dataset. O
Examples O
provided O
by O
[6 O

use O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset O
[6], O
which O
is O
the O

baseline O
models O
provided O
by O
the O
VQA B-DAT
dataset O
authors O
[27], O
which O
currently O

standard O
implementation O
of O
an O
LSTM+CNN O
VQA B-DAT
model. O
It O
uses O
an O
LSTM O

and O
our O
FDA O
model O
on O
VQA B-DAT
test-dev O
and O
test-standard O
data O
for O

VQA B-DAT
Question O
48.09 O
75.66 O
27.14 O
36.70 O

and O
our O
FDA O
model O
on O
VQA B-DAT
test-dev O
and O
test-standard O
data O
for O

VQA B-DAT
Question O
53.68 O
75.71 O
38.64 O
37.05 O

the O
baselines O
provided O
by O
the O
VQA B-DAT
authors O
[6]. O
The O
results O
for O

nificant O
when O
solving O
the O
multiple-choice O
VQA B-DAT
problems. O
From O
Table O
2, O
one O

best O
ever O
performance O
on O
the O
VQA B-DAT
dataset. O
In O
particular, O
it O
improves O

model O
to O
solve O
the O
challenging O
VQA B-DAT
problems. O
FDA O
is O
built O
upon O

improvement O
over O
baselines O
on O
the O
VQA B-DAT
benchmark O
datasets, O
for O
both O
the O

open-ended O
and O
multiple-choices O
VQA B-DAT
tasks. O
Excellent O
performance O
of O
FDA O

attention O
to O
visual O
part O
in O
VQA B-DAT
tasks O
could O
essentially O
improve O
the O

scenes. O
The O
set O
contains O
over O
100 B-DAT
objects O
and O
31 O
animals O
in O

i.e., O
an O
answer O
is O
deemed O
100 B-DAT

MS O
COCO O
dataset O
[32] O
and O
150, B-DAT

a O
scale O
of O
0 O
− O
100) B-DAT
required O
to O
answer O
a O
question O

choose O
the O
top O
K O
= O
1000 B-DAT
most O
frequent O
answers O
as O
possible O

Question O
(BoW O
Q): O
The O
top O
1,000 B-DAT
words O
in O
the O
questions O
are O

are O
concatenated O
to O
get O
a O
1,030 B-DAT

1000 B-DAT

1000 B-DAT

with O
2 O
hidden O
layers O
and O
1000 B-DAT
hidden O
units O
(dropout O
0.5) O
in O

a O
bag-of-words O
representation O
containing O
the O
1,000 B-DAT
most O
popular O
words O
in O
the O

that O
it O
has O
VQA O
accuracy O
1.0 B-DAT
(see O
section O
3 O
for O
accuracy O

Question O
K O
= O
1000 B-DAT
Human O
To O
Be O
Able O
To O

100 B-DAT
A O

100 B-DAT

100 B-DAT

100 B-DAT

can O
see O
that O
K O
= O
1000 B-DAT
performs O
better O
than O
K O

performs O
better O
then O
K O
= O
1000 B-DAT
by O
0.40% O
for O
open-ended O
task O

blue” O
(28881, O
1.16%), O
“4” O
(27174, O
1.09 B-DAT

outside” O
(1846, O
0.07%), O
“hot O
dog” O
(1809, B-DAT
0.07%), O
“night” O
(1805, O
0.07%), O
“trees O

and O
white” O
(1518, O
0.06%), O
“bedroom” O
(1500, B-DAT
0.06%), O
“bat” O
(1494, O
0.06%), O
“glasses O

0.06%), O
“cloudy” O
(1413, O
0.06%), O
“15” O
(1407, B-DAT
0.06%), O
“up” O
(1399, O
0.06%), O
“blonde O

0.05%), O
“many” O
(1211, O
0.05%), O
“zoo” O
(1204, B-DAT
0.05%), O
“suitcase” O
(1199, O
0.05%), O
“old O

0.04%), O
“mountains” O
(1030, O
0.04%), O
“wall” O
(1009, B-DAT
0.04%), O
“ele- O
phants” O
(1006, O
0.04 O

1504 B-DAT

1504 B-DAT

1408 B-DAT

1506 B-DAT

1409 B-DAT

1502 B-DAT

VQA B-DAT

and O
open-ended O
Visual O
Question O
Answering O
(VQA B-DAT

a O
system O
that O
succeeds O
at O
VQA B-DAT
typically O
needs O
a O
more O
detailed O

producing O
generic O
image O
captions. O
Moreover, O
VQA B-DAT
is O
amenable O
to O
automatic O
evaluation O

Numerous O
baselines O
and O
methods O
for O
VQA B-DAT
are O
provided O
and O
compared O
with O

human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

open- O
ended O
Visual O
Question O
Answering O
(VQA B-DAT

). O
A O
VQA B-DAT
system O
takes O
as O
input O
an O

Is O
this O
person O
expecting O
company?”). O
VQA B-DAT
[19], O
[36], O
[50], O
[3] O
is O

the O
high-level O
reasoning O
required O
for O
VQA B-DAT
by O
removing O
the O
need O
to O

29]. O
As O
part O
of O
the O
VQA B-DAT
initiative, O
we O
will O
organize O
an O

state-of-the-art O
methods O
and O
best O
practices. O
VQA B-DAT
poses O
a O
rich O
set O
of O

during O
the O
past O
few O
decades. O
VQA B-DAT
provides O
an O
attractive O
balance O
between O

VQA B-DAT
Efforts. O
Several O
recent O
papers O
have O

difficult O
and O
unconstrained O
task, O
our O
VQA B-DAT
dataset O
is O
two O
orders O
of O

1,449 O
images O
respectively). O
The O
proposed O
VQA B-DAT
task O
has O
connections O
to O
other O

These O
approaches O
provide O
inspiration O
for O
VQA B-DAT
techniques. O
One O
key O
concern O
in O

fixed O
set O
of O
loca- O
tions. O
VQA B-DAT
is O
naturally O
grounded O
in O
images O

Describing O
Visual O
Content. O
Related O
to O
VQA B-DAT
are O
the O
tasks O
of O
image O

by O
[53]). O
The O
questions O
in O
VQA B-DAT
require O
detailed O
specific O
information O
about O

3 O
VQA B-DAT
DATASET O
COLLECTION O

describe O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset. O
We O
begin O
by O
describing O

they O
are O
well-suited O
for O
our O
VQA B-DAT
task. O
The O
more O
diverse O
our O

their O
answers. O
Abstract O
Scenes. O
The O
VQA B-DAT
task O
with O
real O
images O
requires O

the O
high-level O
reasoning O
required O
for O
VQA, B-DAT
but O
not O
the O
low-level O
vision O

test-standard, O
test-challenge, O
test-reserve). O
For O
the O
VQA B-DAT
challenge O
(see O
section O
6), O
test-dev O

default’ O
test O
data O
for O
the O
VQA B-DAT
competition. O
When O
comparing O
to O
the O

sentences O
containing O
multiple O
words. O
In O
VQA, B-DAT
most O
answers O
(89.32%) O
are O
single O

4 O
VQA B-DAT
DATASET O
ANALYSIS O
In O
this O
section O

questions O
and O
answers O
in O
the O
VQA B-DAT
train O
dataset. O
To O
gain O
an O

visual O
information O
is O
critical O
to O
VQA B-DAT
and O
that O
commonsense O
information O
alone O

from O
the O
real O
images O
of O
VQA B-DAT
trainval) O
asking O
subjects O

5 O
VQA B-DAT
BASELINES O
AND O
METHODS O
In O
this O

explore O
the O
difficulty O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

novel O
methods. O
We O
train O
on O
VQA B-DAT
train+val. O
Unless O
stated O
otherwise, O
all O

top O
1K O
answers O
of O
the O
VQA B-DAT
train/val O
dataset O

multiple- O
choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
As O

I O
(Fig. O
8), O
selected O
using O
VQA B-DAT
test-dev O
accuracies) O
on O
VQA O
test O

worse O
than O
human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

system O
is O
correct O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

correct” O
implies O
that O
it O
has O
VQA B-DAT
accuracy O
1.0 O
(see O
section O
3 O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
The O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

frequently O
predicted O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

age O
of O
question) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

system O
is O
correct) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

a O
filtered O
version O
of O
the O
VQA B-DAT
train O
+ O
val O
dataset O
in O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

6 O
VQA B-DAT
CHALLENGE O
AND O
WORKSHOP O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O

task O
of O
Visual O
Question O
Answering O
(VQA B-DAT

multiple-choice O
tasks O
in O
the O
respective O
VQA B-DAT
Real O
Image O
Challenge O
leaderboards O
(as O

datasets O
may O
help O
enable O
practical O
VQA B-DAT
applications. O
We O
believe O
VQA O
has O

questions O
IV O
- O
Details O
on O
VQA B-DAT
baselines O
V O
- O
“Age” O
and O

Leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Open-Ended) O
on O

leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Multiple-Choice) O
on O

Additional O
examples O
from O
the O
VQA B-DAT
dataset O

scenes. O
This O
helps O
motivate O
the O
VQA B-DAT
task O
as O
a O
way O
to O

APPENDIX O
IV: O
DETAILS O
ON O
VQA B-DAT
BASELINES O
“per O
Q-type O
prior” O
baseline O

For O
every O
question O
in O
the O
VQA B-DAT
test-standard O
set, O
we O
find O
its O

norm O
I), O
selected O
using O
VQA B-DAT
test- O
dev O
accuracies). O
To O
estimate O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

a O
random O
selection O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

accurate O
natural O
language O
answer. O
Mirroring O
real B-DAT

removing O
the O
need O
to O
parse O
real B-DAT
images. O
Three O
questions O
were O
collected O

We O
begin O
by O
describing O
the O
real B-DAT
images O
and O
abstract O

Scenes. O
The O
VQA O
task O
with O
real B-DAT
images O
requires O
the O
use O
of O

2) O
that O
more O
closely O
mirror O
real B-DAT
images O
than O
previous O
papers O
[57 O

details, O
and O
examples. O
Splits. O
For O
real B-DAT
images, O
we O
follow O
the O
same O

was O
used O
for O
both O
the O
real B-DAT
images O
and O
abstract O
scenes. O
In O

red”, O
“blue”, O
“4”, O
“green” O
for O
real B-DAT
images. O
The O
inclusion O
of O
the O

sample O
of O
60K O
questions O
for O
real B-DAT
images O
(left) O
and O
all O
questions O

the O
questions O
for O
both O
the O
real B-DAT
images O
(left) O
and O
abstract O
scenes O

is O
quite O
similar O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
This O

to O
those O
elicited O
by O
the O
real B-DAT
images. O
There O
exists O
a O
surprising O

with O
different O
word O
lengths O
for O
real B-DAT
images O
and O
abstract O
scenes O

89.32%, O
6.91%, O
and O
2.74% O
for O
real B-DAT
images O
and O
90.51%, O
5.89%, O
and O

sample O
of O
60K O
questions O
for O
real B-DAT
images O
when O
subjects O
provide O
answers O

answers O
in O
our O
dataset O
for O
real B-DAT
images O
and O
3,770 O
for O
abstract O

40.66% O
of O
the O
questions O
on O
real B-DAT
images O
and O
abstract O
scenes O
respectively O

yes/no’ O
answers O
are O
“yes” O
for O
real B-DAT
images O
and O
abstract O
scenes. O
Question O

14.48% O
of O
the O
questions O
on O
real B-DAT
images O
and O
abstract O
scenes O
are O

of O
the O
‘number’ O
answers O
for O
real B-DAT
images O
and O
39.85% O
for O
abstract O

labeled O
as O
confident O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
Inter-human O

confident, O
1 O
= O
confident) O
for O
real B-DAT
images O
and O
abstract O
scenes O
(black O

in O
the O
answers O
for O
both O
real B-DAT
images O
(83.30%) O
and O
abstract O
scenes O

has O
2.70 O
unique O
answers O
for O
real B-DAT
images O
and O
2.39 O
for O
abstract O

subset O
10K O
questions O
from O
the O
real B-DAT
images O
of O
VQA O
trainval) O
asking O

p O
< O
.001) O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
See O

on O
the O
VQA O
test-dev O
for O
real B-DAT
images. O
Q O
= O
Question, O
I O

on O
the O
VQA O
test-dev O
for O
real B-DAT
images. O
As O
expected, O
the O
vision-alone O

for O
different O
question O
types O
on O
real B-DAT
images O
(Q+C O
is O
reported O
on O

on O
the O
VQA O
test-dev O
for O
real B-DAT
images. O
The O
different O
ablated O
versions O

on O
the O
VQA O
test-dev O
for O
real B-DAT
images. O
Q O
= O
Question, O
I O

real B-DAT
| O
oe-abstract O
| O
mc-real O

real B-DAT

real B-DAT

real B-DAT
and O
multiple-choice-real O
are O
shown O
in O

both O
open-ended O
and O
multiple-choice O
tasks O
(real B-DAT
images) O
with O
other O
entries O
(as O

data O
(MS O
COCO O
captions O
for O
real B-DAT
images O
and O
captions O
collected O
by O

p O
< O
.001) O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
This O

and O
Fig. O
17 O
(adjectives) O
for O
real B-DAT
images O
and O
Fig. O
18 O
(nouns O

in O
Fig. O
14 O
(left) O
for O
real B-DAT
images O
and O
Fig. O
14 O
(right O

and O
question O
& O
answers O
for O
real B-DAT
images O
(left) O
and O
abstract O
scenes O

indicating O
the O
normalized O
count O
for O
real B-DAT
images O

indicating O
the O
normalized O
count O
for O
real B-DAT
images O

indicating O
the O
normalized O
count O
for O
real B-DAT
images O

each O
of O
the O
two O
datasets, O
real B-DAT
and O
abstract, O
first O
two O
rows O

first O
five O
words O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
Note O

of O
3,000 O
questions O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
In O

better O
(≈ O
15% O
increase O
for O
real B-DAT
images O
and O
≈ O
11% O
increase O

words O
of O
questions O
in O
the O
real B-DAT
images O
training O
set O
and O
ensure O

type O
is O
also O
computed O
on O
real B-DAT
images O
training O
set. O
“nearest O
neighbor O

sample O
of O
60K O
questions O
for O
real B-DAT
images O
(left) O
and O
all O
questions O

sample O
of O
60K O
questions O
for O
real B-DAT
images O
(top) O
and O
all O
questions O

top O
250 O
answers O
in O
our O
real B-DAT
images O
dataset O
along O
with O
their O

numerous O
representative O
examples O
of O
the O
real B-DAT
image O
dataset O

numerous O
representative O
examples O
of O
the O
real B-DAT
and O
abstract O
scene O
dataset O

provide O
a O
dataset O
containing O
∼0.25M O
images, B-DAT
∼0.76M O
questions, O
and O
∼10M O
answers O

free-form, O
open-ended O
questions O
collected O
for O
images B-DAT
via O
Amazon O
Mechanical O
Turk. O
Note O

vision?”). O
Moreover, O
since O
questions O
about O
images B-DAT
often O
tend O
to O
seek O
specific O

large O
dataset O
that O
contains O
204,721 O
images B-DAT
from O
the O
MS O
COCO O
dataset O

The O
MS O
COCO O
dataset O
has O
images B-DAT
depicting O
diverse O
and O
complex O
scenes O

the O
need O
to O
parse O
real O
images B-DAT

250,000 O
vs. O
2,591 O
and O
1,449 O
images B-DAT
respectively). O
The O
proposed O
VQA O
task O

introduced O
a O
dataset O
of O
10k O
images B-DAT
and O
prompted O
captions O
that O
describe O

English O
by O
humans) O
for O
COCO O
images B-DAT

VQA O
is O
naturally O
grounded O
in O
images B-DAT
– O
requiring O
the O
understanding O
of O

both O
text O
(questions) O
and O
vision O
(images B-DAT

begin O
by O
describing O
the O
real O
images B-DAT
and O
abstract O

im- O
ages O
and O
81,434 O
test O
images B-DAT
from O
the O
newly-released O
Microsoft O
Common O

dataset O
was O
gathered O
to O
find O
images B-DAT
containing O
multiple O
objects O
and O
rich O

the O
visual O
complexity O
of O
these O
images, B-DAT
they O
are O
well-suited O
for O
our O

more O
diverse O
our O
collection O
of O
images, B-DAT
the O
more O
diverse, O
comprehensive, O
and O

The O
VQA O
task O
with O
real O
images B-DAT
requires O
the O
use O
of O
complex O

that O
more O
closely O
mirror O
real O
images B-DAT
than O
previous O
papers O
[57], O
[58 O

and O
examples. O
Splits. O
For O
real O
images, B-DAT
we O
follow O
the O
same O
train/val/test O

five O
single-sentence O
captions O
for O
all O
images B-DAT

It O
understands O
a O
lot O
about O
images B-DAT

used O
for O
both O
the O
real O
images B-DAT
and O
abstract O
scenes. O
In O
total O

blue”, O
“4”, O
“green” O
for O
real O
images B-DAT

of O
60K O
questions O
for O
real O
images B-DAT
(left) O
and O
all O
questions O
for O

at O
the O
image) O
for O
204,721 O
images B-DAT
from O
the O
MS O
COCO O
dataset O

questions O
for O
both O
the O
real O
images B-DAT
(left) O
and O
abstract O
scenes O
(right O

quite O
similar O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
This O
helps O

those O
elicited O
by O
the O
real O
images B-DAT

different O
word O
lengths O
for O
real O
images B-DAT
and O
abstract O
scenes O

6.91%, O
and O
2.74% O
for O
real O
images B-DAT
and O
90.51%, O
5.89%, O
and O
2.49 O

elicit O
specific O
information O
from O
the O
images B-DAT

of O
60K O
questions O
for O
real O
images B-DAT
when O
subjects O
provide O
answers O
when O

in O
our O
dataset O
for O
real O
images B-DAT
and O
3,770 O
for O
abstract O
scenes O

of O
the O
questions O
on O
real O
images B-DAT
and O
abstract O
scenes O
respectively. O
Among O

answers O
are O
“yes” O
for O
real O
images B-DAT
and O
abstract O
scenes. O
Question O
types O

of O
the O
questions O
on O
real O
images B-DAT
and O
abstract O
scenes O
are O
‘number O

the O
‘number’ O
answers O
for O
real O
images B-DAT
and O
39.85% O
for O
abstract O
scenes O

as O
confident O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
Inter-human O
Agreement O

1 O
= O
confident) O
for O
real O
images B-DAT
and O
abstract O
scenes O
(black O
lines O

the O
answers O
for O
both O
real O
images B-DAT
(83.30%) O
and O
abstract O
scenes O
(87.49 O

2.70 O
unique O
answers O
for O
real O
images B-DAT
and O
2.39 O
for O
abstract O
scenes O

answers O
provided O
with O
and O
without O
images, B-DAT
we O
show O
the O
distribution O
of O

for O
answers O
with O
and O
without O
images B-DAT

10K O
questions O
from O
the O
real O
images B-DAT
of O
VQA O
trainval) O
asking O
subjects O

of O
3K O
train O
questions O
(1K O
images B-DAT

001) O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
See O
the O

dataset O
for O
the O
MS O
COCO O
images B-DAT
using O
several O
baselines O
and O
novel O

nearest O
neighbor O
questions O
and O
associated O
images B-DAT
from O
the O
training O
set. O
See O

VGGNet O
[48] O
to O
encode O
the O
images B-DAT

the O
VQA O
test-dev O
for O
real O
images B-DAT

the O
VQA O
test-dev O
for O
real O
images B-DAT

different O
question O
types O
on O
real O
images B-DAT
(Q+C O
is O
reported O
on O
val O

the O
VQA O
test-dev O
for O
real O
images B-DAT

the O
VQA O
test-dev O
for O
real O
images B-DAT

open-ended O
and O
multiple-choice O
tasks O
(real O
images) B-DAT
with O
other O
entries O
(as O
of O

a O
dataset O
containing O
over O
250K O
images, B-DAT
760K O
questions, O
and O
around O
10M O

the O
visually O
impaired O
to O
capture O
images B-DAT
and O
ask O
open-ended O
questions O
that O

MS O
COCO O
captions O
for O
real O
images B-DAT
and O
captions O
collected O
by O
us O

001) O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
This O
helps O

Fig. O
17 O
(adjectives) O
for O
real O
images B-DAT
and O
Fig. O
18 O
(nouns), O
Fig O

Fig. O
14 O
(left) O
for O
real O
images B-DAT
and O
Fig. O
14 O
(right) O
for O

question O
& O
answers O
for O
real O
images B-DAT
(left) O
and O
abstract O
scenes O
(right O

the O
normalized O
count O
for O
real O
images B-DAT

the O
normalized O
count O
for O
real O
images B-DAT

the O
normalized O
count O
for O
real O
images B-DAT

five O
words O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
Note O
the O

3,000 O
questions O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
In O
Table O

15% O
increase O
for O
real O
images B-DAT
and O
≈ O
11% O
increase O
for O

of O
questions O
in O
the O
real O
images B-DAT
training O
set O
and O
ensure O
that O

is O
also O
computed O
on O
real O
images B-DAT
training O
set. O
“nearest O
neighbor” O
baseline O

k O
questions O
and O
their O
associated O
images, B-DAT
we O
find O
the O
image O
which O

used O
to O
collect O
questions O
for O
images B-DAT

subjects O
were O
shown O
the O
corresponding O
images B-DAT

of O
60K O
questions O
for O
real O
images B-DAT
(left) O
and O
all O
questions O
for O

of O
60K O
questions O
for O
real O
images B-DAT
(top) O
and O
all O
questions O
for O

250 O
answers O
in O
our O
real O
images B-DAT
dataset O
along O
with O
their O
counts O

for O
the O
MS O
COCO O
[32] O
images, B-DAT
abstract O
scenes, O
and O
multiple-choice O
questions O

approach O
to O
answering O
questions O
about O
images B-DAT

choice B-DAT
format. O
We O
provide O
a O
dataset O

answering O
task O
and O
a O
multiple- O
choice B-DAT
task O
[45], O
[33]. O
Unlike O
the O

choice B-DAT
task O
only O
requires O
an O

choice B-DAT
answers). O
These O
approaches O
provide O
inspiration O

choice B-DAT

choice B-DAT
task, O
18 O
candidate O
answers O
are O

answers O
is O
randomized. O
Example O
multiple O
choice B-DAT
questions O
are O
in O
the O
appendix O

choice B-DAT

choice B-DAT
tasks. O
Note O
that O
“yes” O
is O

choice B-DAT
questions O

choice B-DAT
task, O
we O
pick O
the O
answer O

choice B-DAT
task, O
we O
pick O
the O
answer O

for O
the O
open-ended O
and O
multiple- O
choice B-DAT
tasks O
on O
the O
VQA O
test-dev O

choice B-DAT
picks O
the O
answer O
that O
has O

choice B-DAT
tasks O
on O
the O
VQA O
test-dev O

choice B-DAT

choice) B-DAT
and O
LSTM O
Q O
achieving O
48.76 O

choice B-DAT

choice B-DAT

choice B-DAT

choice B-DAT
are O
better O
than O
open-ended. O
All O

choice B-DAT
tasks O
on O
the O
VQA O
test-dev O

choice B-DAT
task O

choice B-DAT
task O

choice B-DAT
task O

choice B-DAT
task O

choice B-DAT
task O

choice B-DAT
task O

choice B-DAT
tasks O
on O
the O
VQA O
test-dev O

and O
by O
1.88% O
for O
multiple- O
choice B-DAT
task O

choice B-DAT

choice B-DAT
tasks O
(real O
images) O
with O
other O

choice B-DAT
tasks O
in O
the O
respective O
VQA O

choice B-DAT
questions O
IV O
- O
Details O
on O

choice B-DAT
questions O
when O
subjects O
were O
shown O

choice B-DAT
questions, O
we O
collected O
three O
human O

human O
accuracies O
for O
mul- O
tiple O
choice B-DAT
questions. O
Table O
6 O
also O
shows O

choice B-DAT
accuracies O
are O
more O
or O
less O

increase O
in O
accuracy O
using O
multiple O
choice B-DAT
is O
not O
surprising O

plausible, O
answers O
for O
the O
multiple- O
choice B-DAT
task O
and O
to O
assess O
how O

choice B-DAT
answers O

choice B-DAT
questions, O
respectively O

choice B-DAT
questions O
for O
numerous O
representative O
examples O

VQA: O
Visual O
Question B-DAT
Answering O
www.visualqa.org O

of O
free-form O
and O
open-ended O
Visual O
Question B-DAT
Answering O
(VQA). O
Given O
an O
image O

free-form O
and O
open- O
ended O
Visual O
Question B-DAT
Answering O
(VQA). O
A O
VQA O
system O

We O
now O
describe O
the O
Visual O
Question B-DAT
Answering O
(VQA) O
dataset. O
We O
begin O

Types O
of O
Question B-DAT

of O
Words O
in O
Question B-DAT

Distribution O
of O
Question B-DAT
Lengths O

real O
images O
and O
abstract O
scenes. O
Question B-DAT
types O
such O
as O
“How O
many O

As O
shown O
in O
Table O
1 O
(Question B-DAT
+ O
Image), O
there O
is O
significant O

Fig. O
2). O
In O
Table O
1 O
(Question), B-DAT
we O
show O
the O
percentage O
of O

Question B-DAT
40.81 O
67.60 O
25.77 O
21.22 O
Real O

Question B-DAT
+ O
Caption* O
57.47 O
78.97 O
39.68 O

Question B-DAT
+ O
Image O
83.30 O
95.77 O
83.39 O

Question B-DAT
43.27 O
66.65 O
28.52 O
23.66 O
Abstract O

Question B-DAT
+ O
Caption* O
54.34 O
74.70 O
41.19 O

Question B-DAT
+ O
Image O
87.49 O
95.96 O
95.04 O

question O
without O
seeing O
the O
image O
(Question), B-DAT
seeing O
just O
a O
caption O
of O

and O
not O
the O
image O
itself O
(Question B-DAT
+ O
Caption), O
and O
seeing O
the O

image O
(Question B-DAT
+ O
Image). O
Results O
are O
shown O

answer O
the O
questions? O
Table O
1 O
(Question B-DAT
+ O
Caption) O
shows O
the O
percentage O

Question B-DAT
Channel: O
This O
channel O
provides O
an O

1) O
Bag-of-Words O
Question B-DAT
(BoW O
Q): O
The O
top O
1,000 O

caption O
embedding O
(Caption). O
For O
BoW O
Question B-DAT
+ O
Caption O
(BoW O
Q O

for O
real O
images. O
Q O
= O
Question, B-DAT
I O
= O
Image, O
C O

Question B-DAT
K O
= O
1000 O
Human O
To O

for O
real O
images. O
Q O
= O
Question, B-DAT
I O
= O
Image. O
See O
text O

introduce O
the O
task O
of O
Visual O
Question B-DAT
Answering O
(VQA). O
Given O
an O
image O

Etzioni. O
Paraphrase-Driven O
Learning O
for O
Open O
Question B-DAT
Answering. O
In O
ACL, O
2013. O
2 O

Zettlemoyer, O
and O
O. O
Etzioni. O
Open O
Question B-DAT
Answering O
over O
Curated O
and O
Extracted O

Fritz. O
A O
Multi-World O
Approach O
to O
Question B-DAT
Answering O
about O
Real-World O
Scenes O
based O

T. O
Mikolov. O
Towards O
AI- O
Complete O
Question B-DAT
Answering: O
A O
Set O
of O
Prerequisite O

VQA: O
Visual B-DAT
Question O
Answering O
www.visualqa.org O

task O
of O
free-form O
and O
open-ended O
Visual B-DAT
Question O
Answering O
(VQA). O
Given O
an O

questions O
and O
answers O
are O
open-ended. O
Visual B-DAT
questions O
selectively O
target O
different O
areas O

of O
free-form O
and O
open- O
ended O
Visual B-DAT
Question O
Answering O
(VQA). O
A O
VQA O

complex O
reasoning O
more O
essential. O
Describing O
Visual B-DAT
Content. O
Related O
to O
VQA O
are O

We O
now O
describe O
the O
Visual B-DAT
Question O
Answering O
(VQA) O
dataset. O
We O

we O
introduce O
the O
task O
of O
Visual B-DAT
Question O
Answering O
(VQA). O
Given O
an O

cloud O
service. O
In O
Mobile O
Cloud O
Visual B-DAT
Media O
Computing, O
pages O
265–290. O
Springer O

D. O
Parikh. O
Zero-Shot O
Learning O
via O
Visual B-DAT
Abstraction. O
In O
ECCV, O
2014. O
2 O

Nearly O
Real- O
time O
Answers O
to O
Visual B-DAT
Questions. O
In O
User O
Interface O
Software O

and O
A. O
Gupta. O
NEIL: O
Extracting O
Visual B-DAT
Knowledge O
from O
Web O
Data. O
In O

Zitnick. O
Mind’s O
Eye: O
A O
Recurrent O
Visual B-DAT
Represen- O
tation O
for O
Image O
Caption O

Long-term O
Recurrent O
Convolutional O
Networks O
for O
Visual B-DAT
Recognition O
and O
Description. O
In O
CVPR O

G. O
Zweig. O
From O
Captions O
to O
Visual B-DAT
Concepts O
and O
Back. O
In O
CVPR O

Hallonquist, O
and O
L. O
Younes. O
A O
Visual B-DAT
Turing O
Test O
for O
Computer O
Vision O

Karpathy O
and O
L. O
Fei-Fei. O
Deep O
Visual B-DAT

and O
R. O
S. O
Zemel. O
Unifying O
Visual B-DAT

Listen, O
Use O
Your O
Imagination: O
Leveraging O
Visual B-DAT
Common O
Sense O
for O
Non-Visual O
Tasks O

Divvala, O
and O
A. O
Farhadi. O
Viske: O
Visual B-DAT
knowledge O
extraction O
and O
question O
answering O

Berg, O
and O
T. O
L. O
Berg. O
Visual B-DAT
madlibs: O
Fill-in-the- O
blank O
description O
generation O

Bringing O
Semantics O
Into O
Focus O
Using O
Visual B-DAT
Abstraction. O
In O
CVPR, O
2013. O
2 O

and O
L. O
Vanderwende. O
Learning O
the O
Visual B-DAT
Interpretation O
of O
Sentences. O
In O
ICCV O

can O
be O
provided O
in O
a O
multiple B-DAT

open-ended O
answering O
task O
and O
a O
multiple B-DAT

requires O
a O
free-form O
response, O
the O
multiple B-DAT

sentence O
completion O
(e.g., O
[45] O
with O
multiple B-DAT

gathered O
to O
find O
images O
containing O
multiple B-DAT
objects O
and O
rich O
contextual O
information O

tions: O
(i) O
open-ended O
and O
(ii) O
multiple B-DAT

and O
reliable O
for O
sentences O
containing O
multiple B-DAT
words. O
In O
VQA, O
most O
answers O

image O
caption O
evaluation O
[6]. O
For O
multiple B-DAT

the O
answers O
is O
randomized. O
Example O
multiple B-DAT
choice O
questions O
are O
in O
the O

according O
to O
the O
accuracy O
metric, O
multiple B-DAT
options O
could O
have O
a O
non-zero O

answers O
are O
free-form O
and O
not O
multiple B-DAT

for O
both O
the O
open-ended O
and O
multiple B-DAT

of O
the O
choices O
for O
the O
multiple B-DAT

appendix O
for O
details). O
For O
the O
multiple B-DAT

Q-type O
prior” O
baseline, O
for O
the O
multiple B-DAT

methods O
for O
the O
open-ended O
and O
multiple B-DAT

possible O
K O
answers O
and O
multiple B-DAT

for O
both O
the O
open-ended O
and O
multiple B-DAT

rather O
poorly O
(open-ended: O
28.13% O
/ O
multiple B-DAT

48.09% O
on O
open-ended O
(53.68% O
on O
multiple B-DAT

on O
open- O
ended O
(54.75% O
on O
multiple B-DAT

nearest O
neighbor O
baseline O
(open-ended: O
42.70%, O
multiple B-DAT

is O
58.16% O
(open-ended) O
/ O
63.09% O
(multiple B-DAT

a O
general O
trend, O
results O
on O
multiple B-DAT

both O
the O
open- O
ended O
and O
multiple B-DAT

task O
and O
by O
0.24% O
for O
multiple B-DAT

task O
and O
by O
1.24% O
for O
multiple B-DAT

task O
and O
by O
1.92% O
for O
multiple B-DAT

task O
and O
by O
1.16% O
for O
multiple B-DAT

task O
and O
by O
0.17% O
for O
multiple B-DAT

task O
and O
by O
0.02% O
for O
multiple B-DAT

I) O
for O
the O
open-ended O
and O
multiple B-DAT

task O
and O
by O
1.88% O
for O
multiple B-DAT

of O
leaderboards O
for O
open-ended-real O
and O
multiple B-DAT

I) O
for O
both O
open-ended O
and O
multiple B-DAT

entries O
for O
the O
open-ended O
and O
multiple B-DAT

III O
- O
Human O
accuracy O
on O
multiple B-DAT

are O
the O
human O
accuracies O
for O
multiple B-DAT

To O
compute O
human O
accuracy O
for O
multiple B-DAT

to O
open- O
ended O
answer, O
the O
multiple B-DAT

the O
increase O
in O
accuracy O
using O
multiple B-DAT
choice O
is O
not O
surprising O

but O
plausible, O
answers O
for O
the O
multiple B-DAT

collect O
the O
plausible, O
but O
incorrect, O
multiple B-DAT

32] O
images, O
abstract O
scenes, O
and O
multiple B-DAT

Fig. O
29: O
Random O
examples O
of O
multiple B-DAT

204,721 O
images O
from O
the O
MS O
COCO B-DAT
dataset O
[32] O
and O
a O
newly O

contains O
50,000 O
scenes. O
The O
MS O
COCO B-DAT
dataset O
has O
images O
depicting O
diverse O

to O
English O
by O
humans) O
for O
COCO B-DAT
images. O
[44] O
automatically O
generated O
four O

object, O
count, O
color, O
location) O
using O
COCO B-DAT
captions. O
Text-based O
Q&A O
is O
a O

Common O
Objects O
in O
Context O
(MS O
COCO) B-DAT
[32] O
dataset. O
The O
MS O
COCO O

split O
strategy O
as O
the O
MC O
COCO B-DAT
dataset O
[32] O
(including O
test- O
dev O

abstract O
scenes. O
Captions. O
The O
MS O
COCO B-DAT
dataset O
[32], O
[7] O
already O
contains O

204,721 O
images O
from O
the O
MS O
COCO B-DAT
dataset O
[32] O
and O
150,000 O
questions O

VQA O
dataset O
for O
the O
MS O
COCO B-DAT
images O
using O
several O
baselines O
and O

from O
the O
caption O
data O
(MS O
COCO B-DAT
captions O
for O
real O
images O
and O

VQA O
dataset O
for O
the O
MS O
COCO B-DAT
[32] O
images, O
abstract O
scenes, O
and O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT
captions: O
Data O
collection O
and O
evaluation O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT
Captions: O
Data O
Collection O
and O
Evaluation O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT

VQA: O
Visual O
Question O
Answering B-DAT
www.visualqa.org O

free-form O
and O
open-ended O
Visual O
Question O
Answering B-DAT
(VQA). O
Given O
an O
image O
and O

and O
open- O
ended O
Visual O
Question O
Answering B-DAT
(VQA). O
A O
VQA O
system O
takes O

now O
describe O
the O
Visual O
Question O
Answering B-DAT
(VQA) O
dataset. O
We O
begin O
by O

the O
task O
of O
Visual O
Question O
Answering B-DAT
(VQA). O
Given O
an O
image O
and O

Paraphrase-Driven O
Learning O
for O
Open O
Question O
Answering B-DAT

and O
O. O
Etzioni. O
Open O
Question O
Answering B-DAT
over O
Curated O
and O
Extracted O
Knowledge O

A O
Multi-World O
Approach O
to O
Question O
Answering B-DAT
about O
Real-World O
Scenes O
based O
on O

Parsing O
for O
Understanding O
Events O
and O
Answering B-DAT
Queries. O
IEEE O
MultiMedia, O
2014. O
1 O

Mikolov. O
Towards O
AI- O
Complete O
Question O
Answering B-DAT

VQA B-DAT

and O
open-ended O
Visual O
Question O
Answering O
(VQA B-DAT

a O
system O
that O
succeeds O
at O
VQA B-DAT
typically O
needs O
a O
more O
detailed O

producing O
generic O
image O
captions. O
Moreover, O
VQA B-DAT
is O
amenable O
to O
automatic O
evaluation O

Numerous O
baselines O
and O
methods O
for O
VQA B-DAT
are O
provided O
and O
compared O
with O

human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

open- O
ended O
Visual O
Question O
Answering O
(VQA B-DAT

). O
A O
VQA B-DAT
system O
takes O
as O
input O
an O

Is O
this O
person O
expecting O
company?”). O
VQA B-DAT
[19], O
[36], O
[50], O
[3] O
is O

the O
high-level O
reasoning O
required O
for O
VQA B-DAT
by O
removing O
the O
need O
to O

29]. O
As O
part O
of O
the O
VQA B-DAT
initiative, O
we O
will O
organize O
an O

state-of-the-art O
methods O
and O
best O
practices. O
VQA B-DAT
poses O
a O
rich O
set O
of O

during O
the O
past O
few O
decades. O
VQA B-DAT
provides O
an O
attractive O
balance O
between O

VQA B-DAT
Efforts. O
Several O
recent O
papers O
have O

difficult O
and O
unconstrained O
task, O
our O
VQA B-DAT
dataset O
is O
two O
orders O
of O

1,449 O
images O
respectively). O
The O
proposed O
VQA B-DAT
task O
has O
connections O
to O
other O

These O
approaches O
provide O
inspiration O
for O
VQA B-DAT
techniques. O
One O
key O
concern O
in O

fixed O
set O
of O
loca- O
tions. O
VQA B-DAT
is O
naturally O
grounded O
in O
images O

Describing O
Visual O
Content. O
Related O
to O
VQA B-DAT
are O
the O
tasks O
of O
image O

by O
[53]). O
The O
questions O
in O
VQA B-DAT
require O
detailed O
specific O
information O
about O

3 O
VQA B-DAT
DATASET O
COLLECTION O

describe O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset. O
We O
begin O
by O
describing O

they O
are O
well-suited O
for O
our O
VQA B-DAT
task. O
The O
more O
diverse O
our O

their O
answers. O
Abstract O
Scenes. O
The O
VQA B-DAT
task O
with O
real O
images O
requires O

the O
high-level O
reasoning O
required O
for O
VQA, B-DAT
but O
not O
the O
low-level O
vision O

test-standard, O
test-challenge, O
test-reserve). O
For O
the O
VQA B-DAT
challenge O
(see O
section O
6), O
test-dev O

default’ O
test O
data O
for O
the O
VQA B-DAT
competition. O
When O
comparing O
to O
the O

sentences O
containing O
multiple O
words. O
In O
VQA, B-DAT
most O
answers O
(89.32%) O
are O
single O

4 O
VQA B-DAT
DATASET O
ANALYSIS O
In O
this O
section O

questions O
and O
answers O
in O
the O
VQA B-DAT
train O
dataset. O
To O
gain O
an O

visual O
information O
is O
critical O
to O
VQA B-DAT
and O
that O
commonsense O
information O
alone O

from O
the O
real O
images O
of O
VQA B-DAT
trainval) O
asking O
subjects O

5 O
VQA B-DAT
BASELINES O
AND O
METHODS O
In O
this O

explore O
the O
difficulty O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

novel O
methods. O
We O
train O
on O
VQA B-DAT
train+val. O
Unless O
stated O
otherwise, O
all O

top O
1K O
answers O
of O
the O
VQA B-DAT
train/val O
dataset O

multiple- O
choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
As O

I O
(Fig. O
8), O
selected O
using O
VQA B-DAT
test-dev O
accuracies) O
on O
VQA O
test O

worse O
than O
human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

system O
is O
correct O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

correct” O
implies O
that O
it O
has O
VQA B-DAT
accuracy O
1.0 O
(see O
section O
3 O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
The O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

frequently O
predicted O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

age O
of O
question) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

system O
is O
correct) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

a O
filtered O
version O
of O
the O
VQA B-DAT
train O
+ O
val O
dataset O
in O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

6 O
VQA B-DAT
CHALLENGE O
AND O
WORKSHOP O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O

task O
of O
Visual O
Question O
Answering O
(VQA B-DAT

multiple-choice O
tasks O
in O
the O
respective O
VQA B-DAT
Real O
Image O
Challenge O
leaderboards O
(as O

datasets O
may O
help O
enable O
practical O
VQA B-DAT
applications. O
We O
believe O
VQA O
has O

questions O
IV O
- O
Details O
on O
VQA B-DAT
baselines O
V O
- O
“Age” O
and O

Leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Open-Ended) O
on O

leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Multiple-Choice) O
on O

Additional O
examples O
from O
the O
VQA B-DAT
dataset O

scenes. O
This O
helps O
motivate O
the O
VQA B-DAT
task O
as O
a O
way O
to O

APPENDIX O
IV: O
DETAILS O
ON O
VQA B-DAT
BASELINES O
“per O
Q-type O
prior” O
baseline O

For O
every O
question O
in O
the O
VQA B-DAT
test-standard O
set, O
we O
find O
its O

norm O
I), O
selected O
using O
VQA B-DAT
test- O
dev O
accuracies). O
To O
estimate O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

a O
random O
selection O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

brown O
(score: O
12.89 O
= O
1.01 B-DAT
[image] O
+ O
11.88 O
[word]) O
red O

word]) O
yellow O
(score: O
11.91 O
= O
1.08 B-DAT
[image] O
+ O
10.84 O
[word O

(1.05 B-DAT

1505 B-DAT

1505 B-DAT

1505 B-DAT

1105, B-DAT
2012 O

1409 B-DAT

When O
evaluated O
on O
the O
challenging O
VQA B-DAT
dataset O
[2], O
it O
shows O
comparable O

in O
one O
of O
the O
earliest O
VQA B-DAT
papers O
[12], O
the O
simple O
baseline O

the O
recent O
much O
larger O
COCO O
VQA B-DAT
dataset O
[2], O
the O
BOWIMG O
baseline O

the O
Full O
release O
of O
COCO O
VQA B-DAT
dataset O
[2], O
the O
largest O
VQA O

so O
far. O
In O
the O
COCO O
VQA B-DAT
dataset, O
there O
are O
3 O
questions O

the O
evaluation O
standard O
of O
the O
VQA B-DAT
dataset, O
the O
result O
of O
the O

any O
proposed O
VQA B-DAT
models O
should O
report O
accuracy O
on O

Since O
this O
VQA B-DAT
dataset O
is O
rather O
new, O
the O

The O
full O
set O
of O
the O
VQA B-DAT
dataset O
was O
released O
on O
Oct.6 O

2015) O
used O
v0.9 O
version O
of O
VQA B-DAT
with O
their O
own O
split O
of O

2015) O
used O
v0.9 O
version O
of O
VQA B-DAT
dataset. O
So O
these O
are O
not O

neural O
network O
models O
on O
the O
VQA B-DAT
dataset. O
Furthermore, O
due O
to O
its O

the O
attention O
mechanisms O
of O
the O
VQA B-DAT
models O
in O
[13, O
17, O
18 O

the O
strength O
and O
weakness O
of O
VQA B-DAT
model O

to O
interact O
with O
people O
in O
real B-DAT
time. O
Aided O
by O
the O
simplicity O

question O
relevant O
to O
the O
given O
images B-DAT

pairs O
in O
val2014, O
for O
123,287 O
images B-DAT
overall O
in O
the O
training O
set O

we O
first O
randomly O
split O
the O
images B-DAT
of O
COCO O
val2014 O
into O
70 O

The O
question-answer O
pairs O
from O
the O
images B-DAT
of O
COCO O
train2014 O
+ O
val2014 O

and O
actions O
appearing O
in O
the O
images B-DAT
of O
COCO O
dataset. O
For O
the O

the O
deep O
feature O
of O
the O
images B-DAT
are O
extracted O
beforehand. O
Figure O
4 O

Simple O
Baseline O
for O
Visual O
Question B-DAT
Answering O

2 O
iBOWIMG O
for O
Visual O
Question B-DAT
Answering O

and O
test-standard O
set. O
For O
Open-Ended O
Question B-DAT
track, O
we O
take O
the O
top-1 O

softmax O
output. O
For O
the O
Multiple-Choice O
Question B-DAT
track, O
we O
first O
get O
the O

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT
answering O
is O
essentially O
an O
interactive O

2 O
iBOWIMG O
for O
Visual O
Question B-DAT
Answering O

Simple O
Baseline O
for O
Visual B-DAT
Question O
Answering O

2 O
iBOWIMG O
for O
Visual B-DAT
Question O
Answering O

3.3 O
Understanding O
the O
Visual B-DAT
QA O
model O

two O
different O
questions O
and O
answers. O
Visual B-DAT
features O
from O
CNN O
already O
have O

4 O
Interactive O
Visual B-DAT
QA O
Demo O

Zitnick, O
and O
D. O
Parikh. O
Vqa: O
Visual B-DAT
question O
answering. O
arXiv O
preprint O
arXiv:1505.00468 O

2 O
iBOWIMG O
for O
Visual B-DAT
Question O
Answering O

3.3 O
Understanding O
the O
Visual B-DAT
QA O
model O

4 O
Interactive O
Visual B-DAT
QA O
Demo O

be O
in O
single O
word O
or O
multiple B-DAT
words. O
Then O
we O
have O
the O

of O
the O
image O
captions O
of O
COCO B-DAT
dataset O
[9]. O
For O
the O
recent O

much O
larger O
COCO B-DAT
VQA O
dataset O
[2], O
the O
BOWIMG O

on O
the O
Full O
release O
of O
COCO B-DAT
VQA O
dataset O
[2], O
the O
largest O

dataset O
so O
far. O
In O
the O
COCO B-DAT
VQA O
dataset, O
there O
are O
3 O

for O
each O
image O
in O
the O
COCO B-DAT
dataset. O
For O
each O
question, O
10 O

the O
image O
set O
in O
the O
COCO B-DAT
dataset O

randomly O
split O
the O
images O
of O
COCO B-DAT
val2014 O
into O
70% O
subset O
A O

pairs O
from O
the O
images O
of O
COCO B-DAT
train2014 O
+ O
val2014 O
subset O
A O

model O
on O
the O
testing O
set O
(COCO B-DAT
test2015) O
to O
the O
evaluation O
server O

appearing O
in O
the O
images O
of O
COCO B-DAT
dataset. O
For O
the O
second O
image O

For O
visual O
question O
answering O
on O
COCO B-DAT
dataset, O
our O
implementation O
of O
a O

Simple O
Baseline O
for O
Visual O
Question O
Answering B-DAT

2 O
iBOWIMG O
for O
Visual O
Question O
Answering B-DAT

2 O
iBOWIMG O
for O
Visual O
Question O
Answering B-DAT

When O
evaluated O
on O
the O
challenging O
VQA B-DAT
dataset O
[2], O
it O
shows O
comparable O

in O
one O
of O
the O
earliest O
VQA B-DAT
papers O
[12], O
the O
simple O
baseline O

the O
recent O
much O
larger O
COCO O
VQA B-DAT
dataset O
[2], O
the O
BOWIMG O
baseline O

the O
Full O
release O
of O
COCO O
VQA B-DAT
dataset O
[2], O
the O
largest O
VQA O

so O
far. O
In O
the O
COCO O
VQA B-DAT
dataset, O
there O
are O
3 O
questions O

the O
evaluation O
standard O
of O
the O
VQA B-DAT
dataset, O
the O
result O
of O
the O

any O
proposed O
VQA B-DAT
models O
should O
report O
accuracy O
on O

Since O
this O
VQA B-DAT
dataset O
is O
rather O
new, O
the O

The O
full O
set O
of O
the O
VQA B-DAT
dataset O
was O
released O
on O
Oct.6 O

2015) O
used O
v0.9 O
version O
of O
VQA B-DAT
with O
their O
own O
split O
of O

2015) O
used O
v0.9 O
version O
of O
VQA B-DAT
dataset. O
So O
these O
are O
not O

neural O
network O
models O
on O
the O
VQA B-DAT
dataset. O
Furthermore, O
due O
to O
its O

the O
attention O
mechanisms O
of O
the O
VQA B-DAT
models O
in O
[13, O
17, O
18 O

the O
strength O
and O
weakness O
of O
VQA B-DAT
model O

1.0 B-DAT
dataset O
is O
also O
built O
on O

state- O
of-the-art O
methods O
for O
VQA O
1.0 B-DAT
and O
VQG-COCO O
dataset. O
We O
perform O

in O
table O
2 O
for O
VQA O
1.0 B-DAT
and O
table O
3 O
for O
VQG-COCO O

1.0 B-DAT
Dataset. O
The O
first O
block O
consists O

only O
for O
50 O
of O
the O
100 B-DAT
questions O
involved O
in O
the O
survey O

We O
provided O
175 O
people O
with O
100 B-DAT
such O
images O
from O
the O
VQG O

ground O
truth O
question). O
For O
the O
100 B-DAT
images, O
on O
an O
average O
59.7 O

1409 B-DAT

1705 B-DAT

1606 B-DAT

Long O
Papers), O
volume O
1, O
pages O
1802 B-DAT

1603 B-DAT

1409 B-DAT

1703 B-DAT

1500 B-DAT
and O
b=1250 O
are O
set O
empir O

approach O
to O
question O
answering O
about O
real B-DAT

a O
nat- O
ural O
extension O
for O
VQA B-DAT

can O
be O
used O
for O
improving O
VQA, B-DAT
Image O
captioning O
and O
Object O
Classification O

gen- O
eration, O
Visual O
Question O
Answering O
(VQA) B-DAT
and O
Visual O
Dialog. O
(Barnard O
et O

et O
al., O
2016) O
for O
solving O
VQA B-DAT
task O
including O
attention-based O
methods O
(Zhu O

also O
used O
the O
questions O
from O
VQA B-DAT
dataset O
(An- O
tol O
et O
al O

uses O
only O
the O
VQG-COCO O
dataset. O
VQA B-DAT

2015) O
to O
extract O
captions O
for O
VQA B-DAT
dataset O
as O
the O
human O
annotated O

get O
good O
results O
on O
the O
VQA B-DAT
dataset O
(as O
shown O
in O
Table O

model O
separately O
for O
VQG-COCO O
and O
VQA B-DAT
dataset O

with O
state- O
of-the-art O
methods O
for O
VQA B-DAT
1.0 O
and O
VQG-COCO O
dataset. O
We O

provided O
in O
table O
2 O
for O
VQA B-DAT
1.0 O
and O
table O
3 O
for O

We O
observe O
that O
for O
the O
VQA B-DAT
dataset O
we O
achieve O
an O
improvement O

Yang O
et O
al., O
2015) O
for O
VQA B-DAT
dataset O
by O
around O
6% O
in O

Table O
2: O
State-of-the-Art O
comparison O
on O
VQA B-DAT

Zitnick, O
and O
Devi O
Parikh. O
2015. O
VQA B-DAT

on O
two O
types O
of O
dataset: O
VQA B-DAT
dataset O
(Antol O
et O
al., O
2015 O

B.1.1 O
VQA B-DAT
dataset O
VQA O
dataset(Antol O
et O
al., O
2015) O
is O

2014) O
to O
extract O
captions O
for O
VQA B-DAT
dataset O

have O
used O
that, O
but, O
for O
VQA B-DAT
dataset O
captions O
were O
not O
available O

gen- O
erated O
descriptive O
sentences O
from O
images B-DAT
with O
the O
help O
of O
Deep O

questions O
when O
considering O
the O
individual O
images B-DAT
are O
also O
shown O

target, O
support- O
ing O
and O
contrasting O
images B-DAT

annotated O
question O
for O
all O
the O
images B-DAT

human O
annotated O
questions O
based O
on O
images B-DAT
of O
MS-COCO O
dataset. O
This O
dataset O

a O
total O
of O
2500 O
training O
images, B-DAT
1250 O
validation O
images, O
and O
1250 O

testing O
images B-DAT

dataset O
is O
also O
built O
on O
images B-DAT
from O
MS-COCO O
dataset. O
It O
con O

tains O
a O
total O
of O
82783 O
images B-DAT
for O
training, O
40504 O
for O
validation O

use O
of O
the O
1250 O
validation O
images B-DAT
to O
tune O
the O
hyperparameters O
and O

175 O
people O
with O
100 O
such O
images B-DAT
from O
the O
VQG- O
COCO O
validation O

dataset O
which O
has O
1250 O
images B-DAT

truth O
question). O
For O
the O
100 O
images, B-DAT
on O
an O
average O
59.7% O
people O

a O
story: O
Generating O
sentences O
from O
images B-DAT

semantics O
for O
finding O
and O
describing O
images B-DAT
with O
sentences. O
Transactions O
of O
the O

Visual7w: O
Grounded O
question O
answering O
in O
images B-DAT

a) O
for O
the O
rest O
of O
images B-DAT

is O
built O
on O
com- O
plex O
images B-DAT
from O
MS-COCO O
dataset. O
It O
contains O

a O
total O
of O
204721 O
images, B-DAT
out O
of O
which O
82783 O
are O

a O
total O
of O
2500 O
training O
images, B-DAT
1250 O
val- O
idation O
images O
and O

1250 O
testing O
images B-DAT

1.8 O
mil- O
lion O
of O
scene O
images B-DAT

to O
gener- O
ate O
captions O
for O
images B-DAT
that O
did O
not O
have O
them O

Multimodal O
Differential O
Network O
for O
Visual O
Question B-DAT
Generation O

captioning, O
paragraph O
gen- O
eration, O
Visual O
Question B-DAT
Answering O
(VQA) O
and O
Visual O
Dialog O

and O
Namboodiri, O
2018). O
However, O
Visual O
Question B-DAT
Generation O
(VQG) O
is O
a O
separate O

Multimodal O
Differential O
Network O
for O
Visual O
Question B-DAT
Generation. O
It O
consists O
of O
a O

4.2 O
Decoder: O
Question B-DAT
Generator O
The O
role O
of O
decoder O

conduct O
our O
experiments O
on O
Visual O
Question B-DAT
Generation O
(VQG) O
dataset O
(Mostafazadeh O
et O

Devi O
Parikh. O
2015. O
VQA: O
Visual O
Question B-DAT
An- O
swering. O
In O
International O
Conference O

Human O
Atten- O
tion O
in O
Visual O
Question B-DAT
Answering: O
Do O
Humans O
and O
Deep O

various O
tags O
(Noun, O
Verb, O
and O
Question B-DAT
tags) O
and O
different O
ways O
of O

14: O
Compute O
Decode O
Question B-DAT
Sentence: O
15: O
ŷ O
= O
Generating O

Noun O
tag, O
Verb O
tag O
and O
Question B-DAT
tags O
(What, O
Where O

D.2 O
Question B-DAT
Generation O
approaches: O
Sampling O
vs O
Argmax O

skateboarding O
around O
little O
cones. O
Our O
Question B-DAT

a O
pair O
of O
skis. O
Our O
Question B-DAT

Multimodal O
Differential O
Network O
for O
Visual B-DAT
Question O
Generation O

answering O
(Antol O
et O
al., O
2015). O
Visual B-DAT
Dialog O
(Das O
et O
al., O
2017 O

image O
captioning, O
paragraph O
gen- O
eration, O
Visual B-DAT
Question O
Answering O
(VQA) O
and O
Visual O

been O
many O
works O
for O
solving O
Visual B-DAT
Dialog O
(Chappell O
et O
al., O
2004 O

Patro O
and O
Namboodiri, O
2018). O
However, O
Visual B-DAT
Question O
Generation O
(VQG) O
is O
a O

our O
Multimodal O
Differential O
Network O
for O
Visual B-DAT
Question O
Generation. O
It O
consists O
of O

We O
conduct O
our O
experiments O
on O
Visual B-DAT
Question O
Generation O
(VQG) O
dataset O
(Mostafazadeh O

and O
Devi O
Parikh. O
2015. O
VQA: O
Visual B-DAT
Question O
An- O
swering. O
In O
International O

2016. O
Human O
Atten- O
tion O
in O
Visual B-DAT
Question O
Answering: O
Do O
Humans O
and O

Parikh, O
and O
Dhruv O
Batra. O
2017. O
Visual B-DAT
Dialog. O
In O
IEEE O
Conference O
on O

Hallonquist, O
and O
Laurent O
Younes. O
2015. O
Visual B-DAT
turing O
test O
for O
com- O
puter O

Visual B-DAT
object O
and O
scene O
recognition O
plays O

COCO B-DAT
dataset O
which O
provide O
a O
comparison O

COCO B-DAT
dataset. O
This O
dataset O
was O
developed O

COCO B-DAT
dataset O
for O
our O
ex- O
periments O

COCO B-DAT
dataset. O
VQA-1.0 O
dataset O
is O
also O

COCO B-DAT
dataset. O
It O
con- O
tains O
a O

COCO B-DAT
and O
VQA O
dataset O

COCO B-DAT
dataset. O
During O
infer- O
ence, O
We O

COCO B-DAT
dataset O
and O
the O
result O
for O

COCO B-DAT
dataset. O
We O
perform O
a O
user O

COCO B-DAT

The O
results O
for O
the O
VQG- O
COCO B-DAT
test O
set O
are O
given O
in O

COCO B-DAT
Dataset O
as O
mentioned O
in O
section O

COCO B-DAT
dataset. O
The O
comparable O
baselines O
for O

COCO B-DAT
dataset O
this O
is O
15% O
for O

COCO B-DAT
dataset, O
we O
improve O
over O
(Mostafazadeh O

State-of-the-Art O
(SOTA) O
comparison O
on O
VQG- O
COCO B-DAT
Dataset. O
The O
first O
block O
consists O

such O
images O
from O
the O
VQG- O
COCO B-DAT
validation O
dataset O
which O
has O
1250 O

CIDER O
metric O
scores O
for O
VQG- O
COCO B-DAT
dataset. O
We O
present O
different O
experiments O

COCO B-DAT
dataset O
which O
provide O
a O
comparison O

COCO B-DAT

COCO B-DAT
dataset. O
Second O
one O
is O
VQG O

- O
COCO B-DAT
dataset O
based O
on O
natural O
question O

COCO B-DAT
dataset. O
It O
contains O
a O
total O

COCO B-DAT
dataset O
is O
associated O
with O
3 O

COCO B-DAT
dataset(?), O
is O
developed O
for O
gen O

COCO B-DAT

COCO B-DAT
Dataset. O
The O
first O
block O
consists O

COCO B-DAT

COCO B-DAT

paragraph O
gen- O
eration, O
Visual O
Question O
Answering B-DAT
(VQA) O
and O
Visual O
Dialog. O
(Barnard O

Atten- O
tion O
in O
Visual O
Question O
Answering B-DAT

a O
nat- O
ural O
extension O
for O
VQA B-DAT

can O
be O
used O
for O
improving O
VQA, B-DAT
Image O
captioning O
and O
Object O
Classification O

gen- O
eration, O
Visual O
Question O
Answering O
(VQA) B-DAT
and O
Visual O
Dialog. O
(Barnard O
et O

et O
al., O
2016) O
for O
solving O
VQA B-DAT
task O
including O
attention-based O
methods O
(Zhu O

also O
used O
the O
questions O
from O
VQA B-DAT
dataset O
(An- O
tol O
et O
al O

uses O
only O
the O
VQG-COCO O
dataset. O
VQA B-DAT

2015) O
to O
extract O
captions O
for O
VQA B-DAT
dataset O
as O
the O
human O
annotated O

get O
good O
results O
on O
the O
VQA B-DAT
dataset O
(as O
shown O
in O
Table O

model O
separately O
for O
VQG-COCO O
and O
VQA B-DAT
dataset O

with O
state- O
of-the-art O
methods O
for O
VQA B-DAT
1.0 O
and O
VQG-COCO O
dataset. O
We O

provided O
in O
table O
2 O
for O
VQA B-DAT
1.0 O
and O
table O
3 O
for O

We O
observe O
that O
for O
the O
VQA B-DAT
dataset O
we O
achieve O
an O
improvement O

Yang O
et O
al., O
2015) O
for O
VQA B-DAT
dataset O
by O
around O
6% O
in O

Table O
2: O
State-of-the-Art O
comparison O
on O
VQA B-DAT

Zitnick, O
and O
Devi O
Parikh. O
2015. O
VQA B-DAT

on O
two O
types O
of O
dataset: O
VQA B-DAT
dataset O
(Antol O
et O
al., O
2015 O

B.1.1 O
VQA B-DAT
dataset O
VQA O
dataset(Antol O
et O
al., O
2015) O
is O

2014) O
to O
extract O
captions O
for O
VQA B-DAT
dataset O

have O
used O
that, O
but, O
for O
VQA B-DAT
dataset O
captions O
were O
not O
available O

performs O
the O
state-of-the-art O
on O
the O
Visual7W B-DAT
dataset O
and O
the O
VQA O
challenge O

The O
Visual7W B-DAT
dataset O
(Zhu O
et O
al., O
2016 O

part O
of O
the O
Visual O
Genome. O
Visual7W B-DAT
adds O
a O
7th O
which O
question O

The O
natural O
language O
answers O
in O
Visual7W B-DAT
are O
in O
a O
multiple-choice O
format O

one O
being O
the O
correct O
answer. O
Visual7W B-DAT
is O
composed O
of O
47,300 O
images O

For O
the O
Visual7W B-DAT
task, O
we O
use O
the O
same O

QA O
tasks O
accuracy O
(%) O
on O
Visual7W B-DAT
test O
set O

answers O
at O
test O
time. O
For O
Visual7W, B-DAT
we O
use O
the O
an- O
swer O

3 O
presents O
results O
for O
the O
Visual7W B-DAT
multiple- O
choice O
QA O
task. O
The O

stein, O
and O
Li O
Fei-Fei. O
2016. O
Visual7W B-DAT

You O
only O
look O
once: O
Unified, O
real B-DAT

Sun. O
Faster O
R-CNN: O
To- O
wards O
real B-DAT

and O
visual O
question O
answer- O
ing O
(VQA) B-DAT
to O
enable O
deeper O
image O
understanding O

applying O
the O
same O
approach O
to O
VQA B-DAT
we O
obtain O
first O
place O
in O

the O
2017 O
VQA B-DAT
Challenge O

and O
visual O
question O
an- O
swering O
(VQA) B-DAT
[12] O
continue O
to O
inspire O
considerable O

34, O
27, O
48, O
46] O
and O
VQA B-DAT
[11, O
28, O
45, O
47, O
51 O

used O
in O
image O
captioning O
and O
VQA B-DAT
are O
of O
the O
top-down O
variety O

method, O
we O
additionally O
present O
a O
VQA B-DAT
model O
using O
the O
same O
bottom-up O

first O
place O
in O
the O
2017 O
VQA B-DAT
Challenge, O
achieving O
70.3% O
overall O
accuracy O

on O
the O
VQA B-DAT
v2.0 O
test-standard O
server. O
Code, O
models O

proposed O
for O
image O
captioning O
and O
VQA B-DAT

question O
in O
the O
case O
of O
VQA B-DAT
[11, O
28, O
45, O
47, O
51 O

additionally O
apply O
our O
method O
to O
VQA, B-DAT
es- O
tablishing O
the O
broad O
applicability O

our O
VQA B-DAT
model O
take O
as O
input O
a O

Section O
3.3 O
we O
outline O
our O
VQA B-DAT
model. O
We O
note O
that O
for O

however, O
that O
in O
captioning O
and O
VQA B-DAT
we O
utilize O
only O
the O
feature O

use O
in O
image O
captioning O
or O
VQA, B-DAT
we O
take O
the O
final O
output O

4. O
Overview O
of O
the O
proposed O
VQA B-DAT
model. O
A O
deep O
neural O
network O

3.3. O
VQA B-DAT
Model O

features O
V O
, O
our O
proposed O
VQA B-DAT
model O
also O
uses O
a O
‘soft O

some O
important O
aspects O
of O
our O
VQA B-DAT
approach O
are O
not O
detailed O
here O

For O
full O
specifics O
of O
the O
VQA B-DAT
model O
including O
a O
detailed O
exploration O

data O
augmentation O
when O
training O
our O
VQA B-DAT
model. O
The O
dataset O
contains O
108K O

When O
training O
the O
VQA B-DAT
model, O
we O
augment O
the O
VQA O

4.1.3 O
VQA B-DAT
v2.0 O
Dataset O

To O
evaluate O
our O
proposed O
VQA B-DAT
model, O
we O
use O
the O
recently O

introduced O
VQA B-DAT
v2.0 O
dataset O
[12], O
which O
attempts O

the O
basis O
of O
the O
2017 O
VQA B-DAT
Challenge2, O
contains O
1.1M O
questions O
with O

vocabulary O
size O
of O
3,129. O
Our O
VQA B-DAT
test O
server O
submissions O
are O
trained O

report O
accuracies O
using O
the O
standard O
VQA B-DAT
metric O
[2], O
which O
takes O
into O

in O
both O
our O
captioning O
and O
VQA B-DAT
experiments O
we O
evaluate O
our O
full O

in O
our O
full O
model. O
In O
VQA B-DAT
experiments, O
we O
encode O
the O
resized O

4. O
Single-model O
performance O
on O
the O
VQA B-DAT
v2.0 O
validation O
set. O
The O
use O

Table O
5. O
VQA B-DAT
v2.0 O
test-standard O
server O
accuracy O
as O

4.4. O
VQA B-DAT
Results O

performance O
of O
our O
full O
Up-Down O
VQA B-DAT
model O
relative O
to O
several O
ResNet O

baselines O
on O
the O
VQA B-DAT
v2.0 O
validation O
set. O
The O
addition O

ensembled O
models O
on O
the O
official O
VQA B-DAT
2.0 O
test-standard O
evaluation O
server, O
along O

first O
place O
in O
the O
2017 O
VQA B-DAT
Challenge O

Figure O
6. O
VQA B-DAT
example O
illustrating O
attention O
output. O
Given O

We O
include O
an O
example O
of O
VQA B-DAT
attention O
in O
Figure O
6 O

L. O
Zitnick, O
and O
D. O
Parikh. O
VQA B-DAT

Parikh. O
Making O
the O
V O
in O
VQA B-DAT
matter: O
Elevating O
the O
role O
of O

in O
both O
our O
captioning O
and O
VQA B-DAT
models, O
image O
features O
are O
fixed O

6.3. O
VQA B-DAT
Model O
In O
the O
VQA O
model, O
we O
use O
300 O
dimension O

dimension O
512. O
We O
train O
the O
VQA B-DAT
model O
using O
AdaDelta O
[50] O
and O

for O
further O
details O
of O
the O
VQA B-DAT
model O
implementation O

Examples O
of O
visual O
question O
answering O
(VQA) B-DAT
failure O
cases. O
Although O
our O
simple O

VQA B-DAT
model O
has O
limited O
reading O
and O

Image O
Captioning O
and O
Visual O
Question O
Answering B-DAT

D. O
Parikh. O
VQA: O
Visual O
Question O
Answering B-DAT

image O
understanding O
in O
Visual O
Question O
Answering B-DAT

model. O
The O
dataset O
contains O
108K O
images B-DAT
densely O
annotated O
with O
scene O
graphs O

attribute O
data. O
We O
reserve O
5K O
images B-DAT
for O
validation, O
and O
5K O
images O

testing, O
treating O
the O
remaining O
98K O
images B-DAT
as O
training O
data. O
As O
approximately O

51K O
Visual O
Genome O
images B-DAT
are O
also O
found O
in O
the O

sets. O
We O
ensure O
that O
any O
images B-DAT
found O
in O
both O
datasets O
are O

This O
split O
contains O
113,287 O
training O
images B-DAT
with O
five O
captions O
each, O
and O

5K O
images B-DAT
re- O
spectively O
for O
validation O
and O

training O
and O
validation O
set O
(123K O
images B-DAT

11.1M O
answers O
relating O
to O
MSCOCO O
images B-DAT

ResNet- O
101 O
encoding O
of O
full O
images, B-DAT
similar O
to O
our O
ResNet O
base O

Visual7w: O
Grounded O
question O
answering O
in O
images B-DAT

the O
training O
data. O
Starting O
from O
2,000 B-DAT
object O
classes O
and O
500 O
attribute O

200 B-DAT
[14]. O
In O
separate O
experiments O
we O

models O
on O
the O
official O
VQA O
2.0 B-DAT
test-standard O
evaluation O
server, O
along O
with O

posterior O
parietal O
cortices. O
Science, O
315(5820):1860–1862, O
2007 B-DAT

brain. O
Nature O
Reviews O
Neuroscience, O
3(3):201–215, O
2002 B-DAT

sum- O
maries. O
In O
ACL O
Workshop, O
2004 B-DAT

of O
machine O
translation. O
In O
ACL, O
2002 B-DAT

the O
art. O
Cog- O
nition, O
80(1):1–46, O
2001 B-DAT

for O
Image O
Captioning O
and O
Visual O
Question B-DAT
Answering O

Question B-DAT

Question B-DAT

and O
D. O
Parikh. O
VQA: O
Visual O
Question B-DAT
Answering. O
In O
ICCV, O
2015. O
6 O

of O
image O
understanding O
in O
Visual O
Question B-DAT
Answering. O
In O
CVPR, O
2017. O
1 O

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT

Question B-DAT

Attention O
for O
Image O
Captioning O
and O
Visual B-DAT
Question O
Answering O

35]. O
We O
then O
train O
on O
Visual B-DAT
Genome O
[21] O
data. O
To O
aid O

4.1.1 O
Visual B-DAT
Genome O
Dataset O

We O
use O
the O
Visual B-DAT
Genome O
[21] O
dataset O
to O
pretrain O

training O
data. O
As O
approximately O
51K O
Visual B-DAT
Genome O
images O
are O
also O
found O

VQA O
v2.0 O
training O
data O
with O
Visual B-DAT
Genome O
question O
and O
answer O
pairs O

additional O
questions O
and O
answers O
from O
Visual B-DAT
Genome. O
To O
evaluate O
answer O
qual O

Zitnick, O
and O
D. O
Parikh. O
VQA: O
Visual B-DAT
Question O
Answering. O
In O
ICCV, O
2015 O

role O
of O
image O
understanding O
in O
Visual B-DAT
Question O
Answering. O
In O
CVPR, O
2017 O

Bern- O
stein, O
and O
L. O
Fei-Fei. O
Visual B-DAT
genome: O
Connecting O
language O
and O
vision O

in O
both O
downstream O
tasks. O
Since O
Visual B-DAT
Genome O
[21] O
contains O
a O
relatively O

4.1.2 O
Microsoft O
COCO B-DAT
Dataset O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT
cap- O
tions: O
Data O
collection O
and O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT

and O
visual O
question O
answer- O
ing O
(VQA) B-DAT
to O
enable O
deeper O
image O
understanding O

applying O
the O
same O
approach O
to O
VQA B-DAT
we O
obtain O
first O
place O
in O

the O
2017 O
VQA B-DAT
Challenge O

and O
visual O
question O
an- O
swering O
(VQA) B-DAT
[12] O
continue O
to O
inspire O
considerable O

34, O
27, O
48, O
46] O
and O
VQA B-DAT
[11, O
28, O
45, O
47, O
51 O

used O
in O
image O
captioning O
and O
VQA B-DAT
are O
of O
the O
top-down O
variety O

method, O
we O
additionally O
present O
a O
VQA B-DAT
model O
using O
the O
same O
bottom-up O

first O
place O
in O
the O
2017 O
VQA B-DAT
Challenge, O
achieving O
70.3% O
overall O
accuracy O

on O
the O
VQA B-DAT
v2.0 O
test-standard O
server. O
Code, O
models O

proposed O
for O
image O
captioning O
and O
VQA B-DAT

question O
in O
the O
case O
of O
VQA B-DAT
[11, O
28, O
45, O
47, O
51 O

additionally O
apply O
our O
method O
to O
VQA, B-DAT
es- O
tablishing O
the O
broad O
applicability O

our O
VQA B-DAT
model O
take O
as O
input O
a O

Section O
3.3 O
we O
outline O
our O
VQA B-DAT
model. O
We O
note O
that O
for O

however, O
that O
in O
captioning O
and O
VQA B-DAT
we O
utilize O
only O
the O
feature O

use O
in O
image O
captioning O
or O
VQA, B-DAT
we O
take O
the O
final O
output O

4. O
Overview O
of O
the O
proposed O
VQA B-DAT
model. O
A O
deep O
neural O
network O

3.3. O
VQA B-DAT
Model O

features O
V O
, O
our O
proposed O
VQA B-DAT
model O
also O
uses O
a O
‘soft O

some O
important O
aspects O
of O
our O
VQA B-DAT
approach O
are O
not O
detailed O
here O

For O
full O
specifics O
of O
the O
VQA B-DAT
model O
including O
a O
detailed O
exploration O

data O
augmentation O
when O
training O
our O
VQA B-DAT
model. O
The O
dataset O
contains O
108K O

When O
training O
the O
VQA B-DAT
model, O
we O
augment O
the O
VQA O

4.1.3 O
VQA B-DAT
v2.0 O
Dataset O

To O
evaluate O
our O
proposed O
VQA B-DAT
model, O
we O
use O
the O
recently O

introduced O
VQA B-DAT
v2.0 O
dataset O
[12], O
which O
attempts O

the O
basis O
of O
the O
2017 O
VQA B-DAT
Challenge2, O
contains O
1.1M O
questions O
with O

vocabulary O
size O
of O
3,129. O
Our O
VQA B-DAT
test O
server O
submissions O
are O
trained O

report O
accuracies O
using O
the O
standard O
VQA B-DAT
metric O
[2], O
which O
takes O
into O

in O
both O
our O
captioning O
and O
VQA B-DAT
experiments O
we O
evaluate O
our O
full O

in O
our O
full O
model. O
In O
VQA B-DAT
experiments, O
we O
encode O
the O
resized O

4. O
Single-model O
performance O
on O
the O
VQA B-DAT
v2.0 O
validation O
set. O
The O
use O

Table O
5. O
VQA B-DAT
v2.0 O
test-standard O
server O
accuracy O
as O

4.4. O
VQA B-DAT
Results O

performance O
of O
our O
full O
Up-Down O
VQA B-DAT
model O
relative O
to O
several O
ResNet O

baselines O
on O
the O
VQA B-DAT
v2.0 O
validation O
set. O
The O
addition O

ensembled O
models O
on O
the O
official O
VQA B-DAT
2.0 O
test-standard O
evaluation O
server, O
along O

first O
place O
in O
the O
2017 O
VQA B-DAT
Challenge O

Figure O
6. O
VQA B-DAT
example O
illustrating O
attention O
output. O
Given O

We O
include O
an O
example O
of O
VQA B-DAT
attention O
in O
Figure O
6 O

L. O
Zitnick, O
and O
D. O
Parikh. O
VQA B-DAT

Parikh. O
Making O
the O
V O
in O
VQA B-DAT
matter: O
Elevating O
the O
role O
of O

in O
both O
our O
captioning O
and O
VQA B-DAT
models, O
image O
features O
are O
fixed O

6.3. O
VQA B-DAT
Model O
In O
the O
VQA O
model, O
we O
use O
300 O
dimension O

dimension O
512. O
We O
train O
the O
VQA B-DAT
model O
using O
AdaDelta O
[50] O
and O

for O
further O
details O
of O
the O
VQA B-DAT
model O
implementation O

Examples O
of O
visual O
question O
answering O
(VQA) B-DAT
failure O
cases. O
Although O
our O
simple O

VQA B-DAT
model O
has O
limited O
reading O
and O

accurate O
natural O
language O
answer. O
Mirroring O
real B-DAT

removing O
the O
need O
to O
parse O
real B-DAT
images. O
Three O
questions O
were O
collected O

We O
begin O
by O
describing O
the O
real B-DAT
images O
and O
abstract O

Scenes. O
The O
VQA O
task O
with O
real B-DAT
images O
requires O
the O
use O
of O

2) O
that O
more O
closely O
mirror O
real B-DAT
images O
than O
previous O
papers O
[57 O

details, O
and O
examples. O
Splits. O
For O
real B-DAT
images, O
we O
follow O
the O
same O

was O
used O
for O
both O
the O
real B-DAT
images O
and O
abstract O
scenes. O
In O

red”, O
“blue”, O
“4”, O
“green” O
for O
real B-DAT
images. O
The O
inclusion O
of O
the O

sample O
of O
60K O
questions O
for O
real B-DAT
images O
(left) O
and O
all O
questions O

the O
questions O
for O
both O
the O
real B-DAT
images O
(left) O
and O
abstract O
scenes O

is O
quite O
similar O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
This O

to O
those O
elicited O
by O
the O
real B-DAT
images. O
There O
exists O
a O
surprising O

with O
different O
word O
lengths O
for O
real B-DAT
images O
and O
abstract O
scenes O

89.32%, O
6.91%, O
and O
2.74% O
for O
real B-DAT
images O
and O
90.51%, O
5.89%, O
and O

sample O
of O
60K O
questions O
for O
real B-DAT
images O
when O
subjects O
provide O
answers O

answers O
in O
our O
dataset O
for O
real B-DAT
images O
and O
3,770 O
for O
abstract O

40.66% O
of O
the O
questions O
on O
real B-DAT
images O
and O
abstract O
scenes O
respectively O

yes/no’ O
answers O
are O
“yes” O
for O
real B-DAT
images O
and O
abstract O
scenes. O
Question O

14.48% O
of O
the O
questions O
on O
real B-DAT
images O
and O
abstract O
scenes O
are O

of O
the O
‘number’ O
answers O
for O
real B-DAT
images O
and O
39.85% O
for O
abstract O

labeled O
as O
confident O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
Inter-human O

confident, O
1 O
= O
confident) O
for O
real B-DAT
images O
and O
abstract O
scenes O
(black O

in O
the O
answers O
for O
both O
real B-DAT
images O
(83.30%) O
and O
abstract O
scenes O

has O
2.70 O
unique O
answers O
for O
real B-DAT
images O
and O
2.39 O
for O
abstract O

subset O
10K O
questions O
from O
the O
real B-DAT
images O
of O
VQA O
trainval) O
asking O

p O
< O
.001) O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
See O

on O
the O
VQA O
test-dev O
for O
real B-DAT
images. O
Q O
= O
Question, O
I O

on O
the O
VQA O
test-dev O
for O
real B-DAT
images. O
As O
expected, O
the O
vision-alone O

for O
different O
question O
types O
on O
real B-DAT
images O
(Q+C O
is O
reported O
on O

on O
the O
VQA O
test-dev O
for O
real B-DAT
images. O
The O
different O
ablated O
versions O

on O
the O
VQA O
test-dev O
for O
real B-DAT
images. O
Q O
= O
Question, O
I O

real B-DAT
| O
oe-abstract O
| O
mc-real O

real B-DAT

real B-DAT

real B-DAT
and O
multiple-choice-real O
are O
shown O
in O

both O
open-ended O
and O
multiple-choice O
tasks O
(real B-DAT
images) O
with O
other O
entries O
(as O

data O
(MS O
COCO O
captions O
for O
real B-DAT
images O
and O
captions O
collected O
by O

p O
< O
.001) O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
This O

and O
Fig. O
17 O
(adjectives) O
for O
real B-DAT
images O
and O
Fig. O
18 O
(nouns O

in O
Fig. O
14 O
(left) O
for O
real B-DAT
images O
and O
Fig. O
14 O
(right O

and O
question O
& O
answers O
for O
real B-DAT
images O
(left) O
and O
abstract O
scenes O

indicating O
the O
normalized O
count O
for O
real B-DAT
images O

indicating O
the O
normalized O
count O
for O
real B-DAT
images O

indicating O
the O
normalized O
count O
for O
real B-DAT
images O

each O
of O
the O
two O
datasets, O
real B-DAT
and O
abstract, O
first O
two O
rows O

first O
five O
words O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
Note O

of O
3,000 O
questions O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
In O

better O
(≈ O
15% O
increase O
for O
real B-DAT
images O
and O
≈ O
11% O
increase O

words O
of O
questions O
in O
the O
real B-DAT
images O
training O
set O
and O
ensure O

type O
is O
also O
computed O
on O
real B-DAT
images O
training O
set. O
“nearest O
neighbor O

sample O
of O
60K O
questions O
for O
real B-DAT
images O
(left) O
and O
all O
questions O

sample O
of O
60K O
questions O
for O
real B-DAT
images O
(top) O
and O
all O
questions O

top O
250 O
answers O
in O
our O
real B-DAT
images O
dataset O
along O
with O
their O

numerous O
representative O
examples O
of O
the O
real B-DAT
image O
dataset O

numerous O
representative O
examples O
of O
the O
real B-DAT
and O
abstract O
scene O
dataset O

VQA B-DAT

and O
open-ended O
Visual O
Question O
Answering O
(VQA B-DAT

a O
system O
that O
succeeds O
at O
VQA B-DAT
typically O
needs O
a O
more O
detailed O

producing O
generic O
image O
captions. O
Moreover, O
VQA B-DAT
is O
amenable O
to O
automatic O
evaluation O

Numerous O
baselines O
and O
methods O
for O
VQA B-DAT
are O
provided O
and O
compared O
with O

human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

open- O
ended O
Visual O
Question O
Answering O
(VQA B-DAT

). O
A O
VQA B-DAT
system O
takes O
as O
input O
an O

Is O
this O
person O
expecting O
company?”). O
VQA B-DAT
[19], O
[36], O
[50], O
[3] O
is O

the O
high-level O
reasoning O
required O
for O
VQA B-DAT
by O
removing O
the O
need O
to O

29]. O
As O
part O
of O
the O
VQA B-DAT
initiative, O
we O
will O
organize O
an O

state-of-the-art O
methods O
and O
best O
practices. O
VQA B-DAT
poses O
a O
rich O
set O
of O

during O
the O
past O
few O
decades. O
VQA B-DAT
provides O
an O
attractive O
balance O
between O

VQA B-DAT
Efforts. O
Several O
recent O
papers O
have O

difficult O
and O
unconstrained O
task, O
our O
VQA B-DAT
dataset O
is O
two O
orders O
of O

1,449 O
images O
respectively). O
The O
proposed O
VQA B-DAT
task O
has O
connections O
to O
other O

These O
approaches O
provide O
inspiration O
for O
VQA B-DAT
techniques. O
One O
key O
concern O
in O

fixed O
set O
of O
loca- O
tions. O
VQA B-DAT
is O
naturally O
grounded O
in O
images O

Describing O
Visual O
Content. O
Related O
to O
VQA B-DAT
are O
the O
tasks O
of O
image O

by O
[53]). O
The O
questions O
in O
VQA B-DAT
require O
detailed O
specific O
information O
about O

3 O
VQA B-DAT
DATASET O
COLLECTION O

describe O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset. O
We O
begin O
by O
describing O

they O
are O
well-suited O
for O
our O
VQA B-DAT
task. O
The O
more O
diverse O
our O

their O
answers. O
Abstract O
Scenes. O
The O
VQA B-DAT
task O
with O
real O
images O
requires O

the O
high-level O
reasoning O
required O
for O
VQA, B-DAT
but O
not O
the O
low-level O
vision O

test-standard, O
test-challenge, O
test-reserve). O
For O
the O
VQA B-DAT
challenge O
(see O
section O
6), O
test-dev O

default’ O
test O
data O
for O
the O
VQA B-DAT
competition. O
When O
comparing O
to O
the O

sentences O
containing O
multiple O
words. O
In O
VQA, B-DAT
most O
answers O
(89.32%) O
are O
single O

4 O
VQA B-DAT
DATASET O
ANALYSIS O
In O
this O
section O

questions O
and O
answers O
in O
the O
VQA B-DAT
train O
dataset. O
To O
gain O
an O

visual O
information O
is O
critical O
to O
VQA B-DAT
and O
that O
commonsense O
information O
alone O

from O
the O
real O
images O
of O
VQA B-DAT
trainval) O
asking O
subjects O

5 O
VQA B-DAT
BASELINES O
AND O
METHODS O
In O
this O

explore O
the O
difficulty O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

novel O
methods. O
We O
train O
on O
VQA B-DAT
train+val. O
Unless O
stated O
otherwise, O
all O

top O
1K O
answers O
of O
the O
VQA B-DAT
train/val O
dataset O

multiple- O
choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
As O

I O
(Fig. O
8), O
selected O
using O
VQA B-DAT
test-dev O
accuracies) O
on O
VQA O
test O

worse O
than O
human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

system O
is O
correct O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

correct” O
implies O
that O
it O
has O
VQA B-DAT
accuracy O
1.0 O
(see O
section O
3 O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
The O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

frequently O
predicted O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

age O
of O
question) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

system O
is O
correct) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

a O
filtered O
version O
of O
the O
VQA B-DAT
train O
+ O
val O
dataset O
in O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

6 O
VQA B-DAT
CHALLENGE O
AND O
WORKSHOP O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O

task O
of O
Visual O
Question O
Answering O
(VQA B-DAT

multiple-choice O
tasks O
in O
the O
respective O
VQA B-DAT
Real O
Image O
Challenge O
leaderboards O
(as O

datasets O
may O
help O
enable O
practical O
VQA B-DAT
applications. O
We O
believe O
VQA O
has O

questions O
IV O
- O
Details O
on O
VQA B-DAT
baselines O
V O
- O
“Age” O
and O

Leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Open-Ended) O
on O

leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Multiple-Choice) O
on O

Additional O
examples O
from O
the O
VQA B-DAT
dataset O

scenes. O
This O
helps O
motivate O
the O
VQA B-DAT
task O
as O
a O
way O
to O

APPENDIX O
IV: O
DETAILS O
ON O
VQA B-DAT
BASELINES O
“per O
Q-type O
prior” O
baseline O

For O
every O
question O
in O
the O
VQA B-DAT
test-standard O
set, O
we O
find O
its O

norm O
I), O
selected O
using O
VQA B-DAT
test- O
dev O
accuracies). O
To O
estimate O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

a O
random O
selection O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

ended B-DAT
Visual O
Question O
Answering O
(VQA). O
Given O

ended B-DAT

ended B-DAT
answers O
contain O
only O
a O
few O

task O
of O
free-form O
and O
open- O
ended B-DAT
Visual O
Question O
Answering O
(VQA). O
A O

ended, B-DAT
natural- O
language O
question O
about O
the O

ended B-DAT
questions O
require O
a O
potentially O
vast O

ended B-DAT
questions O
collected O
for O
images O
via O

ended B-DAT
answering O
task O
and O
a O
multiple O

ended B-DAT
task O
that O
requires O
a O
free-form O

ended B-DAT
questions O
offers O
many O
benefits, O
it O

ended, B-DAT
free-form O
questions O
and O
answers O
provided O

ended B-DAT
questions O
result O
in O
a O
diverse O

ended B-DAT
and O
(ii) O
multiple-choice. O
For O
the O

ended B-DAT
task, O
the O
generated O
answers O
are O

ended B-DAT
task, O
the O
accuracy O
of O
a O

ended B-DAT
answers O
to O
open-ended O
questions. O
The O

ended B-DAT
and O
multiple-choice O
tasks. O
Note O
that O

ended B-DAT
task, O
we O
pick O
the O
most O

ended B-DAT
task O
using O
cosine O
similarity O
in O

ended B-DAT
task, O
we O
pick O
the O
most O

ended B-DAT
task O
using O
cosine O
similarity O
in O

on O
two O
different O
tasks: O
open- O
ended B-DAT
selects O
the O
answer O
with O
highest O

ended B-DAT
and O
multiple- O
choice O
tasks O
on O

ended B-DAT
and O
multiple-choice O
tasks O
on O
the O

ended B-DAT

ended B-DAT
task, O
the O
vision-alone O
model O
(I O

ended B-DAT
(53.68% O
on O
multiple-choice) O
and O
LSTM O

Q O
achieving O
48.76% O
on O
open- O
ended B-DAT
(54.75% O
on O
multiple-choice); O
both O
outperforming O

ended B-DAT

ended) B-DAT
/ O
63.09% O
(multiple-choice). O
We O
can O

ended B-DAT

ended B-DAT
test-dev O
results O
for O
different O
question O

I) O
for O
both O
the O
open- O
ended B-DAT
and O
multiple-choice O
tasks O
on O
the O

ended B-DAT
task O
and O
by O
0.24% O
for O

ended B-DAT
task O
and O
by O
1.24% O
for O

ended B-DAT
task O
and O
by O
1.92% O
for O

ended B-DAT
task O
and O
by O
1.16% O
for O

words O
by O
0.24% O
for O
open- O
ended B-DAT
task O
and O
by O
0.17% O
for O

ended B-DAT
task O
and O
by O
0.02% O
for O

ended B-DAT
and O
multiple-choice O
tasks O
on O
the O

ended B-DAT
task O
and O
by O
1.88% O
for O

ended B-DAT

ended B-DAT
and O
multiple-choice O
tasks O
(real O
images O

ended, B-DAT
natural O

ended B-DAT
and O
multiple-choice O
tasks O
in O
the O

ended B-DAT
and O
not O
task-specific. O
For O
some O

ended B-DAT
questions O
that O
are O
answered O
by O

ended B-DAT
answers O
task O
when O
subjects O
were O

ended B-DAT
answer O
task. O
In O
comparison O
to O

open- O
ended B-DAT
answer, O
the O
multiple-choice O
accuracies O
are O

the O
task O
of O
free-form O
and O
open B-DAT

the O
questions O
and O
answers O
are O
open B-DAT

to O
automatic O
evaluation, O
since O
many O
open B-DAT

is O
still O
a O
difficult O
and O
open B-DAT
research O
problem O
[51], O
[13], O
[22 O

the O
task O
of O
free-form O
and O
open B-DAT

an O
image O
and O
a O
free-form, O
open B-DAT

Fig. O
1: O
Examples O
of O
free-form, O
open B-DAT

paper, O
we O
present O
both O
an O
open B-DAT

task O
[45], O
[33]. O
Unlike O
the O
open B-DAT

answers. O
While O
the O
use O
of O
open B-DAT

contrast, O
our O
proposed O
task O
involves O
open B-DAT

tions: O
(i) O
open B-DAT

and O
(ii) O
multiple-choice. O
For O
the O
open B-DAT

each O
question. O
As O
with O
the O
open B-DAT

recall O
that O
they O
are O
human-provided O
open B-DAT

-ended O
answers O
to O
open B-DAT

answer O
(“yes”) O
for O
both O
the O
open B-DAT

per O
Q-type O
prior: O
For O
the O
open B-DAT

the O
picked O
answer O
for O
the O
open B-DAT

are O
found. O
Next, O
for O
the O
open B-DAT

the O
picked O
answer O
for O
the O
open B-DAT

result O
on O
two O
different O
tasks: O
open B-DAT

of O
our O
methods O
for O
the O
open B-DAT

and O
methods O
for O
both O
the O
open B-DAT

the O
question O
performs O
rather O
poorly O
(open B-DAT

multiple-choice: O
30.53%). O
In O
fact, O
on O
open B-DAT

BoW O
Q O
achieving O
48.09% O
on O
open B-DAT

LSTM O
Q O
achieving O
48.76% O
on O
open B-DAT

outperforming O
the O
nearest O
neighbor O
baseline O
(open B-DAT

VQA O
test- O
standard O
is O
58.16% O
(open B-DAT

on O
multiple-choice O
are O
better O
than O
open B-DAT

norm O
I) O
for O
both O
the O
open B-DAT

the O
performance O
by O
0.16% O
for O
open B-DAT

performs O
better O
by O
0.95% O
for O
open B-DAT

500 O
by O
0.82% O
for O
open B-DAT

1000 O
by O
0.40% O
for O
open B-DAT

questions O
words O
by O
0.24% O
for O
open B-DAT

questions O
words O
by O
0.06% O
for O
open B-DAT

norm O
I) O
for O
the O
open B-DAT

performs O
worse O
by O
1.13% O
for O
open B-DAT

page5. O
Screenshots O
of O
leaderboards O
for O
open B-DAT

norm O
I) O
for O
both O
open B-DAT

Given O
an O
image O
and O
an O
open B-DAT

of O
other O
entries O
for O
the O
open B-DAT

from O
our O
human O
subjects O
were O
open B-DAT

to O
capture O
images O
and O
ask O
open B-DAT

is O
the O
inter-human O
agreement O
for O
open B-DAT

shows O
the O
inter-human O
agreement O
for O
open B-DAT

answer O
task. O
In O
comparison O
to O
open B-DAT

open B-DAT

VQA: O
Visual O
Question O
Answering B-DAT
www.visualqa.org O

free-form O
and O
open-ended O
Visual O
Question O
Answering B-DAT
(VQA). O
Given O
an O
image O
and O

and O
open- O
ended O
Visual O
Question O
Answering B-DAT
(VQA). O
A O
VQA O
system O
takes O

now O
describe O
the O
Visual O
Question O
Answering B-DAT
(VQA) O
dataset. O
We O
begin O
by O

the O
task O
of O
Visual O
Question O
Answering B-DAT
(VQA). O
Given O
an O
image O
and O

Paraphrase-Driven O
Learning O
for O
Open O
Question O
Answering B-DAT

and O
O. O
Etzioni. O
Open O
Question O
Answering B-DAT
over O
Curated O
and O
Extracted O
Knowledge O

A O
Multi-World O
Approach O
to O
Question O
Answering B-DAT
about O
Real-World O
Scenes O
based O
on O

Parsing O
for O
Understanding O
Events O
and O
Answering B-DAT
Queries. O
IEEE O
MultiMedia, O
2014. O
1 O

Mikolov. O
Towards O
AI- O
Complete O
Question O
Answering B-DAT

provide O
a O
dataset O
containing O
∼0.25M O
images, B-DAT
∼0.76M O
questions, O
and O
∼10M O
answers O

free-form, O
open-ended O
questions O
collected O
for O
images B-DAT
via O
Amazon O
Mechanical O
Turk. O
Note O

vision?”). O
Moreover, O
since O
questions O
about O
images B-DAT
often O
tend O
to O
seek O
specific O

large O
dataset O
that O
contains O
204,721 O
images B-DAT
from O
the O
MS O
COCO O
dataset O

The O
MS O
COCO O
dataset O
has O
images B-DAT
depicting O
diverse O
and O
complex O
scenes O

the O
need O
to O
parse O
real O
images B-DAT

250,000 O
vs. O
2,591 O
and O
1,449 O
images B-DAT
respectively). O
The O
proposed O
VQA O
task O

introduced O
a O
dataset O
of O
10k O
images B-DAT
and O
prompted O
captions O
that O
describe O

English O
by O
humans) O
for O
COCO O
images B-DAT

VQA O
is O
naturally O
grounded O
in O
images B-DAT
– O
requiring O
the O
understanding O
of O

both O
text O
(questions) O
and O
vision O
(images B-DAT

begin O
by O
describing O
the O
real O
images B-DAT
and O
abstract O

im- O
ages O
and O
81,434 O
test O
images B-DAT
from O
the O
newly-released O
Microsoft O
Common O

dataset O
was O
gathered O
to O
find O
images B-DAT
containing O
multiple O
objects O
and O
rich O

the O
visual O
complexity O
of O
these O
images, B-DAT
they O
are O
well-suited O
for O
our O

more O
diverse O
our O
collection O
of O
images, B-DAT
the O
more O
diverse, O
comprehensive, O
and O

The O
VQA O
task O
with O
real O
images B-DAT
requires O
the O
use O
of O
complex O

that O
more O
closely O
mirror O
real O
images B-DAT
than O
previous O
papers O
[57], O
[58 O

and O
examples. O
Splits. O
For O
real O
images, B-DAT
we O
follow O
the O
same O
train/val/test O

five O
single-sentence O
captions O
for O
all O
images B-DAT

It O
understands O
a O
lot O
about O
images B-DAT

used O
for O
both O
the O
real O
images B-DAT
and O
abstract O
scenes. O
In O
total O

blue”, O
“4”, O
“green” O
for O
real O
images B-DAT

of O
60K O
questions O
for O
real O
images B-DAT
(left) O
and O
all O
questions O
for O

at O
the O
image) O
for O
204,721 O
images B-DAT
from O
the O
MS O
COCO O
dataset O

questions O
for O
both O
the O
real O
images B-DAT
(left) O
and O
abstract O
scenes O
(right O

quite O
similar O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
This O
helps O

those O
elicited O
by O
the O
real O
images B-DAT

different O
word O
lengths O
for O
real O
images B-DAT
and O
abstract O
scenes O

6.91%, O
and O
2.74% O
for O
real O
images B-DAT
and O
90.51%, O
5.89%, O
and O
2.49 O

elicit O
specific O
information O
from O
the O
images B-DAT

of O
60K O
questions O
for O
real O
images B-DAT
when O
subjects O
provide O
answers O
when O

in O
our O
dataset O
for O
real O
images B-DAT
and O
3,770 O
for O
abstract O
scenes O

of O
the O
questions O
on O
real O
images B-DAT
and O
abstract O
scenes O
respectively. O
Among O

answers O
are O
“yes” O
for O
real O
images B-DAT
and O
abstract O
scenes. O
Question O
types O

of O
the O
questions O
on O
real O
images B-DAT
and O
abstract O
scenes O
are O
‘number O

the O
‘number’ O
answers O
for O
real O
images B-DAT
and O
39.85% O
for O
abstract O
scenes O

as O
confident O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
Inter-human O
Agreement O

1 O
= O
confident) O
for O
real O
images B-DAT
and O
abstract O
scenes O
(black O
lines O

the O
answers O
for O
both O
real O
images B-DAT
(83.30%) O
and O
abstract O
scenes O
(87.49 O

2.70 O
unique O
answers O
for O
real O
images B-DAT
and O
2.39 O
for O
abstract O
scenes O

answers O
provided O
with O
and O
without O
images, B-DAT
we O
show O
the O
distribution O
of O

for O
answers O
with O
and O
without O
images B-DAT

10K O
questions O
from O
the O
real O
images B-DAT
of O
VQA O
trainval) O
asking O
subjects O

of O
3K O
train O
questions O
(1K O
images B-DAT

001) O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
See O
the O

dataset O
for O
the O
MS O
COCO O
images B-DAT
using O
several O
baselines O
and O
novel O

nearest O
neighbor O
questions O
and O
associated O
images B-DAT
from O
the O
training O
set. O
See O

VGGNet O
[48] O
to O
encode O
the O
images B-DAT

the O
VQA O
test-dev O
for O
real O
images B-DAT

the O
VQA O
test-dev O
for O
real O
images B-DAT

different O
question O
types O
on O
real O
images B-DAT
(Q+C O
is O
reported O
on O
val O

the O
VQA O
test-dev O
for O
real O
images B-DAT

the O
VQA O
test-dev O
for O
real O
images B-DAT

open-ended O
and O
multiple-choice O
tasks O
(real O
images) B-DAT
with O
other O
entries O
(as O
of O

a O
dataset O
containing O
over O
250K O
images, B-DAT
760K O
questions, O
and O
around O
10M O

the O
visually O
impaired O
to O
capture O
images B-DAT
and O
ask O
open-ended O
questions O
that O

MS O
COCO O
captions O
for O
real O
images B-DAT
and O
captions O
collected O
by O
us O

001) O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
This O
helps O

Fig. O
17 O
(adjectives) O
for O
real O
images B-DAT
and O
Fig. O
18 O
(nouns), O
Fig O

Fig. O
14 O
(left) O
for O
real O
images B-DAT
and O
Fig. O
14 O
(right) O
for O

question O
& O
answers O
for O
real O
images B-DAT
(left) O
and O
abstract O
scenes O
(right O

the O
normalized O
count O
for O
real O
images B-DAT

the O
normalized O
count O
for O
real O
images B-DAT

the O
normalized O
count O
for O
real O
images B-DAT

five O
words O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
Note O
the O

3,000 O
questions O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
In O
Table O

15% O
increase O
for O
real O
images B-DAT
and O
≈ O
11% O
increase O
for O

of O
questions O
in O
the O
real O
images B-DAT
training O
set O
and O
ensure O
that O

is O
also O
computed O
on O
real O
images B-DAT
training O
set. O
“nearest O
neighbor” O
baseline O

k O
questions O
and O
their O
associated O
images, B-DAT
we O
find O
the O
image O
which O

used O
to O
collect O
questions O
for O
images B-DAT

subjects O
were O
shown O
the O
corresponding O
images B-DAT

of O
60K O
questions O
for O
real O
images B-DAT
(left) O
and O
all O
questions O
for O

of O
60K O
questions O
for O
real O
images B-DAT
(top) O
and O
all O
questions O
for O

250 O
answers O
in O
our O
real O
images B-DAT
dataset O
along O
with O
their O
counts O

for O
the O
MS O
COCO O
[32] O
images, B-DAT
abstract O
scenes, O
and O
multiple-choice O
questions O

approach O
to O
answering O
questions O
about O
images B-DAT

250, B-DAT

4) O
K O
= O
2000 B-DAT

model, O
we O
use O
K O
= O
2000 B-DAT
most O
frequent O
answers O
as O
possible O

can O
see O
that O
K O
= O
2000 B-DAT
performs O
better O
then O
K O

80.64 O
37.44 O
49.10 O
K O
= O
2000 B-DAT
58.15 O
80.56 O
37.04 O
43.79 O
63.86 O

The O
top O
250 B-DAT
answers O
in O
our O
real O
images O

0.13%), O
“sheep” O
(2927, O
0.12%), O
“bear” O
(2803, B-DAT
0.11%), O
“phone” O
(2772, O
0.11%), O
“12 O

” O
(2633, O
0.11%), O
“mo- O
torcycle” O
(2608, B-DAT
0.11%), O
“cake” O
(2602, O
0.1%), O
“wine O

0.1%), O
“beach” O
(2536, O
0.1%), O
“soccer” O
(2504, B-DAT
0.1%), O
“sunny” O
(2475, O
0.1%), O
“zebra O

” O
(2403, B-DAT
0.1%), O
“tan” O
(2402, O
0.1%), O
“brick” O
(2395, O
0.1%), O
“female O

290 B-DAT

Conference O
on O
Management O
of O
Data, O
2008 B-DAT

Reasoning O
Tool-Kit. O
BT O
Technology O
Journal, O
2004 B-DAT

cyclic O
dependency O
network. O
In O
ACL, O
2003 B-DAT

VQA: O
Visual O
Question B-DAT
Answering O
www.visualqa.org O

of O
free-form O
and O
open-ended O
Visual O
Question B-DAT
Answering O
(VQA). O
Given O
an O
image O

free-form O
and O
open- O
ended O
Visual O
Question B-DAT
Answering O
(VQA). O
A O
VQA O
system O

We O
now O
describe O
the O
Visual O
Question B-DAT
Answering O
(VQA) O
dataset. O
We O
begin O

Types O
of O
Question B-DAT

of O
Words O
in O
Question B-DAT

Distribution O
of O
Question B-DAT
Lengths O

real O
images O
and O
abstract O
scenes. O
Question B-DAT
types O
such O
as O
“How O
many O

As O
shown O
in O
Table O
1 O
(Question B-DAT
+ O
Image), O
there O
is O
significant O

Fig. O
2). O
In O
Table O
1 O
(Question), B-DAT
we O
show O
the O
percentage O
of O

Question B-DAT
40.81 O
67.60 O
25.77 O
21.22 O
Real O

Question B-DAT
+ O
Caption* O
57.47 O
78.97 O
39.68 O

Question B-DAT
+ O
Image O
83.30 O
95.77 O
83.39 O

Question B-DAT
43.27 O
66.65 O
28.52 O
23.66 O
Abstract O

Question B-DAT
+ O
Caption* O
54.34 O
74.70 O
41.19 O

Question B-DAT
+ O
Image O
87.49 O
95.96 O
95.04 O

question O
without O
seeing O
the O
image O
(Question), B-DAT
seeing O
just O
a O
caption O
of O

and O
not O
the O
image O
itself O
(Question B-DAT
+ O
Caption), O
and O
seeing O
the O

image O
(Question B-DAT
+ O
Image). O
Results O
are O
shown O

answer O
the O
questions? O
Table O
1 O
(Question B-DAT
+ O
Caption) O
shows O
the O
percentage O

Question B-DAT
Channel: O
This O
channel O
provides O
an O

1) O
Bag-of-Words O
Question B-DAT
(BoW O
Q): O
The O
top O
1,000 O

caption O
embedding O
(Caption). O
For O
BoW O
Question B-DAT
+ O
Caption O
(BoW O
Q O

for O
real O
images. O
Q O
= O
Question, B-DAT
I O
= O
Image, O
C O

Question B-DAT
K O
= O
1000 O
Human O
To O

for O
real O
images. O
Q O
= O
Question, B-DAT
I O
= O
Image. O
See O
text O

introduce O
the O
task O
of O
Visual O
Question B-DAT
Answering O
(VQA). O
Given O
an O
image O

Etzioni. O
Paraphrase-Driven O
Learning O
for O
Open O
Question B-DAT
Answering. O
In O
ACL, O
2013. O
2 O

Zettlemoyer, O
and O
O. O
Etzioni. O
Open O
Question B-DAT
Answering O
over O
Curated O
and O
Extracted O

Fritz. O
A O
Multi-World O
Approach O
to O
Question B-DAT
Answering O
about O
Real-World O
Scenes O
based O

T. O
Mikolov. O
Towards O
AI- O
Complete O
Question B-DAT
Answering: O
A O
Set O
of O
Prerequisite O

VQA: O
Visual B-DAT
Question O
Answering O
www.visualqa.org O

task O
of O
free-form O
and O
open-ended O
Visual B-DAT
Question O
Answering O
(VQA). O
Given O
an O

questions O
and O
answers O
are O
open-ended. O
Visual B-DAT
questions O
selectively O
target O
different O
areas O

of O
free-form O
and O
open- O
ended O
Visual B-DAT
Question O
Answering O
(VQA). O
A O
VQA O

complex O
reasoning O
more O
essential. O
Describing O
Visual B-DAT
Content. O
Related O
to O
VQA O
are O

We O
now O
describe O
the O
Visual B-DAT
Question O
Answering O
(VQA) O
dataset. O
We O

we O
introduce O
the O
task O
of O
Visual B-DAT
Question O
Answering O
(VQA). O
Given O
an O

cloud O
service. O
In O
Mobile O
Cloud O
Visual B-DAT
Media O
Computing, O
pages O
265–290. O
Springer O

D. O
Parikh. O
Zero-Shot O
Learning O
via O
Visual B-DAT
Abstraction. O
In O
ECCV, O
2014. O
2 O

Nearly O
Real- O
time O
Answers O
to O
Visual B-DAT
Questions. O
In O
User O
Interface O
Software O

and O
A. O
Gupta. O
NEIL: O
Extracting O
Visual B-DAT
Knowledge O
from O
Web O
Data. O
In O

Zitnick. O
Mind’s O
Eye: O
A O
Recurrent O
Visual B-DAT
Represen- O
tation O
for O
Image O
Caption O

Long-term O
Recurrent O
Convolutional O
Networks O
for O
Visual B-DAT
Recognition O
and O
Description. O
In O
CVPR O

G. O
Zweig. O
From O
Captions O
to O
Visual B-DAT
Concepts O
and O
Back. O
In O
CVPR O

Hallonquist, O
and O
L. O
Younes. O
A O
Visual B-DAT
Turing O
Test O
for O
Computer O
Vision O

Karpathy O
and O
L. O
Fei-Fei. O
Deep O
Visual B-DAT

and O
R. O
S. O
Zemel. O
Unifying O
Visual B-DAT

Listen, O
Use O
Your O
Imagination: O
Leveraging O
Visual B-DAT
Common O
Sense O
for O
Non-Visual O
Tasks O

Divvala, O
and O
A. O
Farhadi. O
Viske: O
Visual B-DAT
knowledge O
extraction O
and O
question O
answering O

Berg, O
and O
T. O
L. O
Berg. O
Visual B-DAT
madlibs: O
Fill-in-the- O
blank O
description O
generation O

Bringing O
Semantics O
Into O
Focus O
Using O
Visual B-DAT
Abstraction. O
In O
CVPR, O
2013. O
2 O

and O
L. O
Vanderwende. O
Learning O
the O
Visual B-DAT
Interpretation O
of O
Sentences. O
In O
ICCV O

204,721 O
images O
from O
the O
MS O
COCO B-DAT
dataset O
[32] O
and O
a O
newly O

contains O
50,000 O
scenes. O
The O
MS O
COCO B-DAT
dataset O
has O
images O
depicting O
diverse O

to O
English O
by O
humans) O
for O
COCO B-DAT
images. O
[44] O
automatically O
generated O
four O

object, O
count, O
color, O
location) O
using O
COCO B-DAT
captions. O
Text-based O
Q&A O
is O
a O

Common O
Objects O
in O
Context O
(MS O
COCO) B-DAT
[32] O
dataset. O
The O
MS O
COCO O

split O
strategy O
as O
the O
MC O
COCO B-DAT
dataset O
[32] O
(including O
test- O
dev O

abstract O
scenes. O
Captions. O
The O
MS O
COCO B-DAT
dataset O
[32], O
[7] O
already O
contains O

204,721 O
images O
from O
the O
MS O
COCO B-DAT
dataset O
[32] O
and O
150,000 O
questions O

VQA O
dataset O
for O
the O
MS O
COCO B-DAT
images O
using O
several O
baselines O
and O

from O
the O
caption O
data O
(MS O
COCO B-DAT
captions O
for O
real O
images O
and O

VQA O
dataset O
for O
the O
MS O
COCO B-DAT
[32] O
images, O
abstract O
scenes, O
and O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT
captions: O
Data O
collection O
and O
evaluation O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT
Captions: O
Data O
Collection O
and O
Evaluation O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT

VQA B-DAT

and O
open-ended O
Visual O
Question O
Answering O
(VQA B-DAT

a O
system O
that O
succeeds O
at O
VQA B-DAT
typically O
needs O
a O
more O
detailed O

producing O
generic O
image O
captions. O
Moreover, O
VQA B-DAT
is O
amenable O
to O
automatic O
evaluation O

Numerous O
baselines O
and O
methods O
for O
VQA B-DAT
are O
provided O
and O
compared O
with O

human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

open- O
ended O
Visual O
Question O
Answering O
(VQA B-DAT

). O
A O
VQA B-DAT
system O
takes O
as O
input O
an O

Is O
this O
person O
expecting O
company?”). O
VQA B-DAT
[19], O
[36], O
[50], O
[3] O
is O

the O
high-level O
reasoning O
required O
for O
VQA B-DAT
by O
removing O
the O
need O
to O

29]. O
As O
part O
of O
the O
VQA B-DAT
initiative, O
we O
will O
organize O
an O

state-of-the-art O
methods O
and O
best O
practices. O
VQA B-DAT
poses O
a O
rich O
set O
of O

during O
the O
past O
few O
decades. O
VQA B-DAT
provides O
an O
attractive O
balance O
between O

VQA B-DAT
Efforts. O
Several O
recent O
papers O
have O

difficult O
and O
unconstrained O
task, O
our O
VQA B-DAT
dataset O
is O
two O
orders O
of O

1,449 O
images O
respectively). O
The O
proposed O
VQA B-DAT
task O
has O
connections O
to O
other O

These O
approaches O
provide O
inspiration O
for O
VQA B-DAT
techniques. O
One O
key O
concern O
in O

fixed O
set O
of O
loca- O
tions. O
VQA B-DAT
is O
naturally O
grounded O
in O
images O

Describing O
Visual O
Content. O
Related O
to O
VQA B-DAT
are O
the O
tasks O
of O
image O

by O
[53]). O
The O
questions O
in O
VQA B-DAT
require O
detailed O
specific O
information O
about O

3 O
VQA B-DAT
DATASET O
COLLECTION O

describe O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset. O
We O
begin O
by O
describing O

they O
are O
well-suited O
for O
our O
VQA B-DAT
task. O
The O
more O
diverse O
our O

their O
answers. O
Abstract O
Scenes. O
The O
VQA B-DAT
task O
with O
real O
images O
requires O

the O
high-level O
reasoning O
required O
for O
VQA, B-DAT
but O
not O
the O
low-level O
vision O

test-standard, O
test-challenge, O
test-reserve). O
For O
the O
VQA B-DAT
challenge O
(see O
section O
6), O
test-dev O

default’ O
test O
data O
for O
the O
VQA B-DAT
competition. O
When O
comparing O
to O
the O

sentences O
containing O
multiple O
words. O
In O
VQA, B-DAT
most O
answers O
(89.32%) O
are O
single O

4 O
VQA B-DAT
DATASET O
ANALYSIS O
In O
this O
section O

questions O
and O
answers O
in O
the O
VQA B-DAT
train O
dataset. O
To O
gain O
an O

visual O
information O
is O
critical O
to O
VQA B-DAT
and O
that O
commonsense O
information O
alone O

from O
the O
real O
images O
of O
VQA B-DAT
trainval) O
asking O
subjects O

5 O
VQA B-DAT
BASELINES O
AND O
METHODS O
In O
this O

explore O
the O
difficulty O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

novel O
methods. O
We O
train O
on O
VQA B-DAT
train+val. O
Unless O
stated O
otherwise, O
all O

top O
1K O
answers O
of O
the O
VQA B-DAT
train/val O
dataset O

multiple- O
choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
As O

I O
(Fig. O
8), O
selected O
using O
VQA B-DAT
test-dev O
accuracies) O
on O
VQA O
test O

worse O
than O
human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

system O
is O
correct O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

correct” O
implies O
that O
it O
has O
VQA B-DAT
accuracy O
1.0 O
(see O
section O
3 O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
The O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

frequently O
predicted O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

age O
of O
question) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

system O
is O
correct) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

a O
filtered O
version O
of O
the O
VQA B-DAT
train O
+ O
val O
dataset O
in O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

6 O
VQA B-DAT
CHALLENGE O
AND O
WORKSHOP O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O

task O
of O
Visual O
Question O
Answering O
(VQA B-DAT

multiple-choice O
tasks O
in O
the O
respective O
VQA B-DAT
Real O
Image O
Challenge O
leaderboards O
(as O

datasets O
may O
help O
enable O
practical O
VQA B-DAT
applications. O
We O
believe O
VQA O
has O

questions O
IV O
- O
Details O
on O
VQA B-DAT
baselines O
V O
- O
“Age” O
and O

Leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Open-Ended) O
on O

leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Multiple-Choice) O
on O

Additional O
examples O
from O
the O
VQA B-DAT
dataset O

scenes. O
This O
helps O
motivate O
the O
VQA B-DAT
task O
as O
a O
way O
to O

APPENDIX O
IV: O
DETAILS O
ON O
VQA B-DAT
BASELINES O
“per O
Q-type O
prior” O
baseline O

For O
every O
question O
in O
the O
VQA B-DAT
test-standard O
set, O
we O
find O
its O

norm O
I), O
selected O
using O
VQA B-DAT
test- O
dev O
accuracies). O
To O
estimate O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

a O
random O
selection O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

accurate O
natural O
language O
answer. O
Mirroring O
real B-DAT

removing O
the O
need O
to O
parse O
real B-DAT
images. O
Three O
questions O
were O
collected O

We O
begin O
by O
describing O
the O
real B-DAT
images O
and O
abstract O

Scenes. O
The O
VQA O
task O
with O
real B-DAT
images O
requires O
the O
use O
of O

2) O
that O
more O
closely O
mirror O
real B-DAT
images O
than O
previous O
papers O
[57 O

details, O
and O
examples. O
Splits. O
For O
real B-DAT
images, O
we O
follow O
the O
same O

was O
used O
for O
both O
the O
real B-DAT
images O
and O
abstract O
scenes. O
In O

red”, O
“blue”, O
“4”, O
“green” O
for O
real B-DAT
images. O
The O
inclusion O
of O
the O

sample O
of O
60K O
questions O
for O
real B-DAT
images O
(left) O
and O
all O
questions O

the O
questions O
for O
both O
the O
real B-DAT
images O
(left) O
and O
abstract O
scenes O

is O
quite O
similar O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
This O

to O
those O
elicited O
by O
the O
real B-DAT
images. O
There O
exists O
a O
surprising O

with O
different O
word O
lengths O
for O
real B-DAT
images O
and O
abstract O
scenes O

89.32%, O
6.91%, O
and O
2.74% O
for O
real B-DAT
images O
and O
90.51%, O
5.89%, O
and O

sample O
of O
60K O
questions O
for O
real B-DAT
images O
when O
subjects O
provide O
answers O

answers O
in O
our O
dataset O
for O
real B-DAT
images O
and O
3,770 O
for O
abstract O

40.66% O
of O
the O
questions O
on O
real B-DAT
images O
and O
abstract O
scenes O
respectively O

yes/no’ O
answers O
are O
“yes” O
for O
real B-DAT
images O
and O
abstract O
scenes. O
Question O

14.48% O
of O
the O
questions O
on O
real B-DAT
images O
and O
abstract O
scenes O
are O

of O
the O
‘number’ O
answers O
for O
real B-DAT
images O
and O
39.85% O
for O
abstract O

labeled O
as O
confident O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
Inter-human O

confident, O
1 O
= O
confident) O
for O
real B-DAT
images O
and O
abstract O
scenes O
(black O

in O
the O
answers O
for O
both O
real B-DAT
images O
(83.30%) O
and O
abstract O
scenes O

has O
2.70 O
unique O
answers O
for O
real B-DAT
images O
and O
2.39 O
for O
abstract O

subset O
10K O
questions O
from O
the O
real B-DAT
images O
of O
VQA O
trainval) O
asking O

p O
< O
.001) O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
See O

on O
the O
VQA O
test-dev O
for O
real B-DAT
images. O
Q O
= O
Question, O
I O

on O
the O
VQA O
test-dev O
for O
real B-DAT
images. O
As O
expected, O
the O
vision-alone O

for O
different O
question O
types O
on O
real B-DAT
images O
(Q+C O
is O
reported O
on O

on O
the O
VQA O
test-dev O
for O
real B-DAT
images. O
The O
different O
ablated O
versions O

on O
the O
VQA O
test-dev O
for O
real B-DAT
images. O
Q O
= O
Question, O
I O

real B-DAT
| O
oe-abstract O
| O
mc-real O

real B-DAT

real B-DAT

real B-DAT
and O
multiple-choice-real O
are O
shown O
in O

both O
open-ended O
and O
multiple-choice O
tasks O
(real B-DAT
images) O
with O
other O
entries O
(as O

data O
(MS O
COCO O
captions O
for O
real B-DAT
images O
and O
captions O
collected O
by O

p O
< O
.001) O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
This O

and O
Fig. O
17 O
(adjectives) O
for O
real B-DAT
images O
and O
Fig. O
18 O
(nouns O

in O
Fig. O
14 O
(left) O
for O
real B-DAT
images O
and O
Fig. O
14 O
(right O

and O
question O
& O
answers O
for O
real B-DAT
images O
(left) O
and O
abstract O
scenes O

indicating O
the O
normalized O
count O
for O
real B-DAT
images O

indicating O
the O
normalized O
count O
for O
real B-DAT
images O

indicating O
the O
normalized O
count O
for O
real B-DAT
images O

each O
of O
the O
two O
datasets, O
real B-DAT
and O
abstract, O
first O
two O
rows O

first O
five O
words O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
Note O

of O
3,000 O
questions O
for O
both O
real B-DAT
images O
and O
abstract O
scenes. O
In O

better O
(≈ O
15% O
increase O
for O
real B-DAT
images O
and O
≈ O
11% O
increase O

words O
of O
questions O
in O
the O
real B-DAT
images O
training O
set O
and O
ensure O

type O
is O
also O
computed O
on O
real B-DAT
images O
training O
set. O
“nearest O
neighbor O

sample O
of O
60K O
questions O
for O
real B-DAT
images O
(left) O
and O
all O
questions O

sample O
of O
60K O
questions O
for O
real B-DAT
images O
(top) O
and O
all O
questions O

top O
250 O
answers O
in O
our O
real B-DAT
images O
dataset O
along O
with O
their O

numerous O
representative O
examples O
of O
the O
real B-DAT
image O
dataset O

numerous O
representative O
examples O
of O
the O
real B-DAT
and O
abstract O
scene O
dataset O

VQA B-DAT

and O
open-ended O
Visual O
Question O
Answering O
(VQA B-DAT

a O
system O
that O
succeeds O
at O
VQA B-DAT
typically O
needs O
a O
more O
detailed O

producing O
generic O
image O
captions. O
Moreover, O
VQA B-DAT
is O
amenable O
to O
automatic O
evaluation O

Numerous O
baselines O
and O
methods O
for O
VQA B-DAT
are O
provided O
and O
compared O
with O

human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

open- O
ended O
Visual O
Question O
Answering O
(VQA B-DAT

). O
A O
VQA B-DAT
system O
takes O
as O
input O
an O

Is O
this O
person O
expecting O
company?”). O
VQA B-DAT
[19], O
[36], O
[50], O
[3] O
is O

the O
high-level O
reasoning O
required O
for O
VQA B-DAT
by O
removing O
the O
need O
to O

29]. O
As O
part O
of O
the O
VQA B-DAT
initiative, O
we O
will O
organize O
an O

state-of-the-art O
methods O
and O
best O
practices. O
VQA B-DAT
poses O
a O
rich O
set O
of O

during O
the O
past O
few O
decades. O
VQA B-DAT
provides O
an O
attractive O
balance O
between O

VQA B-DAT
Efforts. O
Several O
recent O
papers O
have O

difficult O
and O
unconstrained O
task, O
our O
VQA B-DAT
dataset O
is O
two O
orders O
of O

1,449 O
images O
respectively). O
The O
proposed O
VQA B-DAT
task O
has O
connections O
to O
other O

These O
approaches O
provide O
inspiration O
for O
VQA B-DAT
techniques. O
One O
key O
concern O
in O

fixed O
set O
of O
loca- O
tions. O
VQA B-DAT
is O
naturally O
grounded O
in O
images O

Describing O
Visual O
Content. O
Related O
to O
VQA B-DAT
are O
the O
tasks O
of O
image O

by O
[53]). O
The O
questions O
in O
VQA B-DAT
require O
detailed O
specific O
information O
about O

3 O
VQA B-DAT
DATASET O
COLLECTION O

describe O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset. O
We O
begin O
by O
describing O

they O
are O
well-suited O
for O
our O
VQA B-DAT
task. O
The O
more O
diverse O
our O

their O
answers. O
Abstract O
Scenes. O
The O
VQA B-DAT
task O
with O
real O
images O
requires O

the O
high-level O
reasoning O
required O
for O
VQA, B-DAT
but O
not O
the O
low-level O
vision O

test-standard, O
test-challenge, O
test-reserve). O
For O
the O
VQA B-DAT
challenge O
(see O
section O
6), O
test-dev O

default’ O
test O
data O
for O
the O
VQA B-DAT
competition. O
When O
comparing O
to O
the O

sentences O
containing O
multiple O
words. O
In O
VQA, B-DAT
most O
answers O
(89.32%) O
are O
single O

4 O
VQA B-DAT
DATASET O
ANALYSIS O
In O
this O
section O

questions O
and O
answers O
in O
the O
VQA B-DAT
train O
dataset. O
To O
gain O
an O

visual O
information O
is O
critical O
to O
VQA B-DAT
and O
that O
commonsense O
information O
alone O

from O
the O
real O
images O
of O
VQA B-DAT
trainval) O
asking O
subjects O

5 O
VQA B-DAT
BASELINES O
AND O
METHODS O
In O
this O

explore O
the O
difficulty O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

novel O
methods. O
We O
train O
on O
VQA B-DAT
train+val. O
Unless O
stated O
otherwise, O
all O

top O
1K O
answers O
of O
the O
VQA B-DAT
train/val O
dataset O

multiple- O
choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
As O

I O
(Fig. O
8), O
selected O
using O
VQA B-DAT
test-dev O
accuracies) O
on O
VQA O
test O

worse O
than O
human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

system O
is O
correct O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

correct” O
implies O
that O
it O
has O
VQA B-DAT
accuracy O
1.0 O
(see O
section O
3 O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
The O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

frequently O
predicted O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

age O
of O
question) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

system O
is O
correct) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

a O
filtered O
version O
of O
the O
VQA B-DAT
train O
+ O
val O
dataset O
in O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

6 O
VQA B-DAT
CHALLENGE O
AND O
WORKSHOP O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O

task O
of O
Visual O
Question O
Answering O
(VQA B-DAT

multiple-choice O
tasks O
in O
the O
respective O
VQA B-DAT
Real O
Image O
Challenge O
leaderboards O
(as O

datasets O
may O
help O
enable O
practical O
VQA B-DAT
applications. O
We O
believe O
VQA O
has O

questions O
IV O
- O
Details O
on O
VQA B-DAT
baselines O
V O
- O
“Age” O
and O

Leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Open-Ended) O
on O

leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Multiple-Choice) O
on O

Additional O
examples O
from O
the O
VQA B-DAT
dataset O

scenes. O
This O
helps O
motivate O
the O
VQA B-DAT
task O
as O
a O
way O
to O

APPENDIX O
IV: O
DETAILS O
ON O
VQA B-DAT
BASELINES O
“per O
Q-type O
prior” O
baseline O

For O
every O
question O
in O
the O
VQA B-DAT
test-standard O
set, O
we O
find O
its O

norm O
I), O
selected O
using O
VQA B-DAT
test- O
dev O
accuracies). O
To O
estimate O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

a O
random O
selection O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

ended B-DAT
Visual O
Question O
Answering O
(VQA). O
Given O

ended B-DAT

ended B-DAT
answers O
contain O
only O
a O
few O

task O
of O
free-form O
and O
open- O
ended B-DAT
Visual O
Question O
Answering O
(VQA). O
A O

ended, B-DAT
natural- O
language O
question O
about O
the O

ended B-DAT
questions O
require O
a O
potentially O
vast O

ended B-DAT
questions O
collected O
for O
images O
via O

ended B-DAT
answering O
task O
and O
a O
multiple O

ended B-DAT
task O
that O
requires O
a O
free-form O

ended B-DAT
questions O
offers O
many O
benefits, O
it O

ended, B-DAT
free-form O
questions O
and O
answers O
provided O

ended B-DAT
questions O
result O
in O
a O
diverse O

ended B-DAT
and O
(ii) O
multiple-choice. O
For O
the O

ended B-DAT
task, O
the O
generated O
answers O
are O

ended B-DAT
task, O
the O
accuracy O
of O
a O

ended B-DAT
answers O
to O
open-ended O
questions. O
The O

ended B-DAT
and O
multiple-choice O
tasks. O
Note O
that O

ended B-DAT
task, O
we O
pick O
the O
most O

ended B-DAT
task O
using O
cosine O
similarity O
in O

ended B-DAT
task, O
we O
pick O
the O
most O

ended B-DAT
task O
using O
cosine O
similarity O
in O

on O
two O
different O
tasks: O
open- O
ended B-DAT
selects O
the O
answer O
with O
highest O

ended B-DAT
and O
multiple- O
choice O
tasks O
on O

ended B-DAT
and O
multiple-choice O
tasks O
on O
the O

ended B-DAT

ended B-DAT
task, O
the O
vision-alone O
model O
(I O

ended B-DAT
(53.68% O
on O
multiple-choice) O
and O
LSTM O

Q O
achieving O
48.76% O
on O
open- O
ended B-DAT
(54.75% O
on O
multiple-choice); O
both O
outperforming O

ended B-DAT

ended) B-DAT
/ O
63.09% O
(multiple-choice). O
We O
can O

ended B-DAT

ended B-DAT
test-dev O
results O
for O
different O
question O

I) O
for O
both O
the O
open- O
ended B-DAT
and O
multiple-choice O
tasks O
on O
the O

ended B-DAT
task O
and O
by O
0.24% O
for O

ended B-DAT
task O
and O
by O
1.24% O
for O

ended B-DAT
task O
and O
by O
1.92% O
for O

ended B-DAT
task O
and O
by O
1.16% O
for O

words O
by O
0.24% O
for O
open- O
ended B-DAT
task O
and O
by O
0.17% O
for O

ended B-DAT
task O
and O
by O
0.02% O
for O

ended B-DAT
and O
multiple-choice O
tasks O
on O
the O

ended B-DAT
task O
and O
by O
1.88% O
for O

ended B-DAT

ended B-DAT
and O
multiple-choice O
tasks O
(real O
images O

ended, B-DAT
natural O

ended B-DAT
and O
multiple-choice O
tasks O
in O
the O

ended B-DAT
and O
not O
task-specific. O
For O
some O

ended B-DAT
questions O
that O
are O
answered O
by O

ended B-DAT
answers O
task O
when O
subjects O
were O

ended B-DAT
answer O
task. O
In O
comparison O
to O

open- O
ended B-DAT
answer, O
the O
multiple-choice O
accuracies O
are O

the O
task O
of O
free-form O
and O
open B-DAT

the O
questions O
and O
answers O
are O
open B-DAT

to O
automatic O
evaluation, O
since O
many O
open B-DAT

is O
still O
a O
difficult O
and O
open B-DAT
research O
problem O
[51], O
[13], O
[22 O

the O
task O
of O
free-form O
and O
open B-DAT

an O
image O
and O
a O
free-form, O
open B-DAT

Fig. O
1: O
Examples O
of O
free-form, O
open B-DAT

paper, O
we O
present O
both O
an O
open B-DAT

task O
[45], O
[33]. O
Unlike O
the O
open B-DAT

answers. O
While O
the O
use O
of O
open B-DAT

contrast, O
our O
proposed O
task O
involves O
open B-DAT

tions: O
(i) O
open B-DAT

and O
(ii) O
multiple-choice. O
For O
the O
open B-DAT

each O
question. O
As O
with O
the O
open B-DAT

recall O
that O
they O
are O
human-provided O
open B-DAT

-ended O
answers O
to O
open B-DAT

answer O
(“yes”) O
for O
both O
the O
open B-DAT

per O
Q-type O
prior: O
For O
the O
open B-DAT

the O
picked O
answer O
for O
the O
open B-DAT

are O
found. O
Next, O
for O
the O
open B-DAT

the O
picked O
answer O
for O
the O
open B-DAT

result O
on O
two O
different O
tasks: O
open B-DAT

of O
our O
methods O
for O
the O
open B-DAT

and O
methods O
for O
both O
the O
open B-DAT

the O
question O
performs O
rather O
poorly O
(open B-DAT

multiple-choice: O
30.53%). O
In O
fact, O
on O
open B-DAT

BoW O
Q O
achieving O
48.09% O
on O
open B-DAT

LSTM O
Q O
achieving O
48.76% O
on O
open B-DAT

outperforming O
the O
nearest O
neighbor O
baseline O
(open B-DAT

VQA O
test- O
standard O
is O
58.16% O
(open B-DAT

on O
multiple-choice O
are O
better O
than O
open B-DAT

norm O
I) O
for O
both O
the O
open B-DAT

the O
performance O
by O
0.16% O
for O
open B-DAT

performs O
better O
by O
0.95% O
for O
open B-DAT

500 O
by O
0.82% O
for O
open B-DAT

1000 O
by O
0.40% O
for O
open B-DAT

questions O
words O
by O
0.24% O
for O
open B-DAT

questions O
words O
by O
0.06% O
for O
open B-DAT

norm O
I) O
for O
the O
open B-DAT

performs O
worse O
by O
1.13% O
for O
open B-DAT

page5. O
Screenshots O
of O
leaderboards O
for O
open B-DAT

norm O
I) O
for O
both O
open B-DAT

Given O
an O
image O
and O
an O
open B-DAT

of O
other O
entries O
for O
the O
open B-DAT

from O
our O
human O
subjects O
were O
open B-DAT

to O
capture O
images O
and O
ask O
open B-DAT

is O
the O
inter-human O
agreement O
for O
open B-DAT

shows O
the O
inter-human O
agreement O
for O
open B-DAT

answer O
task. O
In O
comparison O
to O
open B-DAT

open B-DAT

VQA: O
Visual O
Question O
Answering B-DAT
www.visualqa.org O

free-form O
and O
open-ended O
Visual O
Question O
Answering B-DAT
(VQA). O
Given O
an O
image O
and O

and O
open- O
ended O
Visual O
Question O
Answering B-DAT
(VQA). O
A O
VQA O
system O
takes O

now O
describe O
the O
Visual O
Question O
Answering B-DAT
(VQA) O
dataset. O
We O
begin O
by O

the O
task O
of O
Visual O
Question O
Answering B-DAT
(VQA). O
Given O
an O
image O
and O

Paraphrase-Driven O
Learning O
for O
Open O
Question O
Answering B-DAT

and O
O. O
Etzioni. O
Open O
Question O
Answering B-DAT
over O
Curated O
and O
Extracted O
Knowledge O

A O
Multi-World O
Approach O
to O
Question O
Answering B-DAT
about O
Real-World O
Scenes O
based O
on O

Parsing O
for O
Understanding O
Events O
and O
Answering B-DAT
Queries. O
IEEE O
MultiMedia, O
2014. O
1 O

Mikolov. O
Towards O
AI- O
Complete O
Question O
Answering B-DAT

provide O
a O
dataset O
containing O
∼0.25M O
images, B-DAT
∼0.76M O
questions, O
and O
∼10M O
answers O

free-form, O
open-ended O
questions O
collected O
for O
images B-DAT
via O
Amazon O
Mechanical O
Turk. O
Note O

vision?”). O
Moreover, O
since O
questions O
about O
images B-DAT
often O
tend O
to O
seek O
specific O

large O
dataset O
that O
contains O
204,721 O
images B-DAT
from O
the O
MS O
COCO O
dataset O

The O
MS O
COCO O
dataset O
has O
images B-DAT
depicting O
diverse O
and O
complex O
scenes O

the O
need O
to O
parse O
real O
images B-DAT

250,000 O
vs. O
2,591 O
and O
1,449 O
images B-DAT
respectively). O
The O
proposed O
VQA O
task O

introduced O
a O
dataset O
of O
10k O
images B-DAT
and O
prompted O
captions O
that O
describe O

English O
by O
humans) O
for O
COCO O
images B-DAT

VQA O
is O
naturally O
grounded O
in O
images B-DAT
– O
requiring O
the O
understanding O
of O

both O
text O
(questions) O
and O
vision O
(images B-DAT

begin O
by O
describing O
the O
real O
images B-DAT
and O
abstract O

im- O
ages O
and O
81,434 O
test O
images B-DAT
from O
the O
newly-released O
Microsoft O
Common O

dataset O
was O
gathered O
to O
find O
images B-DAT
containing O
multiple O
objects O
and O
rich O

the O
visual O
complexity O
of O
these O
images, B-DAT
they O
are O
well-suited O
for O
our O

more O
diverse O
our O
collection O
of O
images, B-DAT
the O
more O
diverse, O
comprehensive, O
and O

The O
VQA O
task O
with O
real O
images B-DAT
requires O
the O
use O
of O
complex O

that O
more O
closely O
mirror O
real O
images B-DAT
than O
previous O
papers O
[57], O
[58 O

and O
examples. O
Splits. O
For O
real O
images, B-DAT
we O
follow O
the O
same O
train/val/test O

five O
single-sentence O
captions O
for O
all O
images B-DAT

It O
understands O
a O
lot O
about O
images B-DAT

used O
for O
both O
the O
real O
images B-DAT
and O
abstract O
scenes. O
In O
total O

blue”, O
“4”, O
“green” O
for O
real O
images B-DAT

of O
60K O
questions O
for O
real O
images B-DAT
(left) O
and O
all O
questions O
for O

at O
the O
image) O
for O
204,721 O
images B-DAT
from O
the O
MS O
COCO O
dataset O

questions O
for O
both O
the O
real O
images B-DAT
(left) O
and O
abstract O
scenes O
(right O

quite O
similar O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
This O
helps O

those O
elicited O
by O
the O
real O
images B-DAT

different O
word O
lengths O
for O
real O
images B-DAT
and O
abstract O
scenes O

6.91%, O
and O
2.74% O
for O
real O
images B-DAT
and O
90.51%, O
5.89%, O
and O
2.49 O

elicit O
specific O
information O
from O
the O
images B-DAT

of O
60K O
questions O
for O
real O
images B-DAT
when O
subjects O
provide O
answers O
when O

in O
our O
dataset O
for O
real O
images B-DAT
and O
3,770 O
for O
abstract O
scenes O

of O
the O
questions O
on O
real O
images B-DAT
and O
abstract O
scenes O
respectively. O
Among O

answers O
are O
“yes” O
for O
real O
images B-DAT
and O
abstract O
scenes. O
Question O
types O

of O
the O
questions O
on O
real O
images B-DAT
and O
abstract O
scenes O
are O
‘number O

the O
‘number’ O
answers O
for O
real O
images B-DAT
and O
39.85% O
for O
abstract O
scenes O

as O
confident O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
Inter-human O
Agreement O

1 O
= O
confident) O
for O
real O
images B-DAT
and O
abstract O
scenes O
(black O
lines O

the O
answers O
for O
both O
real O
images B-DAT
(83.30%) O
and O
abstract O
scenes O
(87.49 O

2.70 O
unique O
answers O
for O
real O
images B-DAT
and O
2.39 O
for O
abstract O
scenes O

answers O
provided O
with O
and O
without O
images, B-DAT
we O
show O
the O
distribution O
of O

for O
answers O
with O
and O
without O
images B-DAT

10K O
questions O
from O
the O
real O
images B-DAT
of O
VQA O
trainval) O
asking O
subjects O

of O
3K O
train O
questions O
(1K O
images B-DAT

001) O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
See O
the O

dataset O
for O
the O
MS O
COCO O
images B-DAT
using O
several O
baselines O
and O
novel O

nearest O
neighbor O
questions O
and O
associated O
images B-DAT
from O
the O
training O
set. O
See O

VGGNet O
[48] O
to O
encode O
the O
images B-DAT

the O
VQA O
test-dev O
for O
real O
images B-DAT

the O
VQA O
test-dev O
for O
real O
images B-DAT

different O
question O
types O
on O
real O
images B-DAT
(Q+C O
is O
reported O
on O
val O

the O
VQA O
test-dev O
for O
real O
images B-DAT

the O
VQA O
test-dev O
for O
real O
images B-DAT

open-ended O
and O
multiple-choice O
tasks O
(real O
images) B-DAT
with O
other O
entries O
(as O
of O

a O
dataset O
containing O
over O
250K O
images, B-DAT
760K O
questions, O
and O
around O
10M O

the O
visually O
impaired O
to O
capture O
images B-DAT
and O
ask O
open-ended O
questions O
that O

MS O
COCO O
captions O
for O
real O
images B-DAT
and O
captions O
collected O
by O
us O

001) O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
This O
helps O

Fig. O
17 O
(adjectives) O
for O
real O
images B-DAT
and O
Fig. O
18 O
(nouns), O
Fig O

Fig. O
14 O
(left) O
for O
real O
images B-DAT
and O
Fig. O
14 O
(right) O
for O

question O
& O
answers O
for O
real O
images B-DAT
(left) O
and O
abstract O
scenes O
(right O

the O
normalized O
count O
for O
real O
images B-DAT

the O
normalized O
count O
for O
real O
images B-DAT

the O
normalized O
count O
for O
real O
images B-DAT

five O
words O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
Note O
the O

3,000 O
questions O
for O
both O
real O
images B-DAT
and O
abstract O
scenes. O
In O
Table O

15% O
increase O
for O
real O
images B-DAT
and O
≈ O
11% O
increase O
for O

of O
questions O
in O
the O
real O
images B-DAT
training O
set O
and O
ensure O
that O

is O
also O
computed O
on O
real O
images B-DAT
training O
set. O
“nearest O
neighbor” O
baseline O

k O
questions O
and O
their O
associated O
images, B-DAT
we O
find O
the O
image O
which O

used O
to O
collect O
questions O
for O
images B-DAT

subjects O
were O
shown O
the O
corresponding O
images B-DAT

of O
60K O
questions O
for O
real O
images B-DAT
(left) O
and O
all O
questions O
for O

of O
60K O
questions O
for O
real O
images B-DAT
(top) O
and O
all O
questions O
for O

250 O
answers O
in O
our O
real O
images B-DAT
dataset O
along O
with O
their O
counts O

for O
the O
MS O
COCO O
[32] O
images, B-DAT
abstract O
scenes, O
and O
multiple-choice O
questions O

approach O
to O
answering O
questions O
about O
images B-DAT

250, B-DAT

4) O
K O
= O
2000 B-DAT

model, O
we O
use O
K O
= O
2000 B-DAT
most O
frequent O
answers O
as O
possible O

can O
see O
that O
K O
= O
2000 B-DAT
performs O
better O
then O
K O

80.64 O
37.44 O
49.10 O
K O
= O
2000 B-DAT
58.15 O
80.56 O
37.04 O
43.79 O
63.86 O

The O
top O
250 B-DAT
answers O
in O
our O
real O
images O

0.13%), O
“sheep” O
(2927, O
0.12%), O
“bear” O
(2803, B-DAT
0.11%), O
“phone” O
(2772, O
0.11%), O
“12 O

” O
(2633, O
0.11%), O
“mo- O
torcycle” O
(2608, B-DAT
0.11%), O
“cake” O
(2602, O
0.1%), O
“wine O

0.1%), O
“beach” O
(2536, O
0.1%), O
“soccer” O
(2504, B-DAT
0.1%), O
“sunny” O
(2475, O
0.1%), O
“zebra O

” O
(2403, B-DAT
0.1%), O
“tan” O
(2402, O
0.1%), O
“brick” O
(2395, O
0.1%), O
“female O

290 B-DAT

Conference O
on O
Management O
of O
Data, O
2008 B-DAT

Reasoning O
Tool-Kit. O
BT O
Technology O
Journal, O
2004 B-DAT

cyclic O
dependency O
network. O
In O
ACL, O
2003 B-DAT

VQA: O
Visual O
Question B-DAT
Answering O
www.visualqa.org O

of O
free-form O
and O
open-ended O
Visual O
Question B-DAT
Answering O
(VQA). O
Given O
an O
image O

free-form O
and O
open- O
ended O
Visual O
Question B-DAT
Answering O
(VQA). O
A O
VQA O
system O

We O
now O
describe O
the O
Visual O
Question B-DAT
Answering O
(VQA) O
dataset. O
We O
begin O

Types O
of O
Question B-DAT

of O
Words O
in O
Question B-DAT

Distribution O
of O
Question B-DAT
Lengths O

real O
images O
and O
abstract O
scenes. O
Question B-DAT
types O
such O
as O
“How O
many O

As O
shown O
in O
Table O
1 O
(Question B-DAT
+ O
Image), O
there O
is O
significant O

Fig. O
2). O
In O
Table O
1 O
(Question), B-DAT
we O
show O
the O
percentage O
of O

Question B-DAT
40.81 O
67.60 O
25.77 O
21.22 O
Real O

Question B-DAT
+ O
Caption* O
57.47 O
78.97 O
39.68 O

Question B-DAT
+ O
Image O
83.30 O
95.77 O
83.39 O

Question B-DAT
43.27 O
66.65 O
28.52 O
23.66 O
Abstract O

Question B-DAT
+ O
Caption* O
54.34 O
74.70 O
41.19 O

Question B-DAT
+ O
Image O
87.49 O
95.96 O
95.04 O

question O
without O
seeing O
the O
image O
(Question), B-DAT
seeing O
just O
a O
caption O
of O

and O
not O
the O
image O
itself O
(Question B-DAT
+ O
Caption), O
and O
seeing O
the O

image O
(Question B-DAT
+ O
Image). O
Results O
are O
shown O

answer O
the O
questions? O
Table O
1 O
(Question B-DAT
+ O
Caption) O
shows O
the O
percentage O

Question B-DAT
Channel: O
This O
channel O
provides O
an O

1) O
Bag-of-Words O
Question B-DAT
(BoW O
Q): O
The O
top O
1,000 O

caption O
embedding O
(Caption). O
For O
BoW O
Question B-DAT
+ O
Caption O
(BoW O
Q O

for O
real O
images. O
Q O
= O
Question, B-DAT
I O
= O
Image, O
C O

Question B-DAT
K O
= O
1000 O
Human O
To O

for O
real O
images. O
Q O
= O
Question, B-DAT
I O
= O
Image. O
See O
text O

introduce O
the O
task O
of O
Visual O
Question B-DAT
Answering O
(VQA). O
Given O
an O
image O

Etzioni. O
Paraphrase-Driven O
Learning O
for O
Open O
Question B-DAT
Answering. O
In O
ACL, O
2013. O
2 O

Zettlemoyer, O
and O
O. O
Etzioni. O
Open O
Question B-DAT
Answering O
over O
Curated O
and O
Extracted O

Fritz. O
A O
Multi-World O
Approach O
to O
Question B-DAT
Answering O
about O
Real-World O
Scenes O
based O

T. O
Mikolov. O
Towards O
AI- O
Complete O
Question B-DAT
Answering: O
A O
Set O
of O
Prerequisite O

VQA: O
Visual B-DAT
Question O
Answering O
www.visualqa.org O

task O
of O
free-form O
and O
open-ended O
Visual B-DAT
Question O
Answering O
(VQA). O
Given O
an O

questions O
and O
answers O
are O
open-ended. O
Visual B-DAT
questions O
selectively O
target O
different O
areas O

of O
free-form O
and O
open- O
ended O
Visual B-DAT
Question O
Answering O
(VQA). O
A O
VQA O

complex O
reasoning O
more O
essential. O
Describing O
Visual B-DAT
Content. O
Related O
to O
VQA O
are O

We O
now O
describe O
the O
Visual B-DAT
Question O
Answering O
(VQA) O
dataset. O
We O

we O
introduce O
the O
task O
of O
Visual B-DAT
Question O
Answering O
(VQA). O
Given O
an O

cloud O
service. O
In O
Mobile O
Cloud O
Visual B-DAT
Media O
Computing, O
pages O
265–290. O
Springer O

D. O
Parikh. O
Zero-Shot O
Learning O
via O
Visual B-DAT
Abstraction. O
In O
ECCV, O
2014. O
2 O

Nearly O
Real- O
time O
Answers O
to O
Visual B-DAT
Questions. O
In O
User O
Interface O
Software O

and O
A. O
Gupta. O
NEIL: O
Extracting O
Visual B-DAT
Knowledge O
from O
Web O
Data. O
In O

Zitnick. O
Mind’s O
Eye: O
A O
Recurrent O
Visual B-DAT
Represen- O
tation O
for O
Image O
Caption O

Long-term O
Recurrent O
Convolutional O
Networks O
for O
Visual B-DAT
Recognition O
and O
Description. O
In O
CVPR O

G. O
Zweig. O
From O
Captions O
to O
Visual B-DAT
Concepts O
and O
Back. O
In O
CVPR O

Hallonquist, O
and O
L. O
Younes. O
A O
Visual B-DAT
Turing O
Test O
for O
Computer O
Vision O

Karpathy O
and O
L. O
Fei-Fei. O
Deep O
Visual B-DAT

and O
R. O
S. O
Zemel. O
Unifying O
Visual B-DAT

Listen, O
Use O
Your O
Imagination: O
Leveraging O
Visual B-DAT
Common O
Sense O
for O
Non-Visual O
Tasks O

Divvala, O
and O
A. O
Farhadi. O
Viske: O
Visual B-DAT
knowledge O
extraction O
and O
question O
answering O

Berg, O
and O
T. O
L. O
Berg. O
Visual B-DAT
madlibs: O
Fill-in-the- O
blank O
description O
generation O

Bringing O
Semantics O
Into O
Focus O
Using O
Visual B-DAT
Abstraction. O
In O
CVPR, O
2013. O
2 O

and O
L. O
Vanderwende. O
Learning O
the O
Visual B-DAT
Interpretation O
of O
Sentences. O
In O
ICCV O

204,721 O
images O
from O
the O
MS O
COCO B-DAT
dataset O
[32] O
and O
a O
newly O

contains O
50,000 O
scenes. O
The O
MS O
COCO B-DAT
dataset O
has O
images O
depicting O
diverse O

to O
English O
by O
humans) O
for O
COCO B-DAT
images. O
[44] O
automatically O
generated O
four O

object, O
count, O
color, O
location) O
using O
COCO B-DAT
captions. O
Text-based O
Q&A O
is O
a O

Common O
Objects O
in O
Context O
(MS O
COCO) B-DAT
[32] O
dataset. O
The O
MS O
COCO O

split O
strategy O
as O
the O
MC O
COCO B-DAT
dataset O
[32] O
(including O
test- O
dev O

abstract O
scenes. O
Captions. O
The O
MS O
COCO B-DAT
dataset O
[32], O
[7] O
already O
contains O

204,721 O
images O
from O
the O
MS O
COCO B-DAT
dataset O
[32] O
and O
150,000 O
questions O

VQA O
dataset O
for O
the O
MS O
COCO B-DAT
images O
using O
several O
baselines O
and O

from O
the O
caption O
data O
(MS O
COCO B-DAT
captions O
for O
real O
images O
and O

VQA O
dataset O
for O
the O
MS O
COCO B-DAT
[32] O
images, O
abstract O
scenes, O
and O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT
captions: O
Data O
collection O
and O
evaluation O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT
Captions: O
Data O
Collection O
and O
Evaluation O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT

VQA B-DAT

and O
open-ended O
Visual O
Question O
Answering O
(VQA B-DAT

a O
system O
that O
succeeds O
at O
VQA B-DAT
typically O
needs O
a O
more O
detailed O

producing O
generic O
image O
captions. O
Moreover, O
VQA B-DAT
is O
amenable O
to O
automatic O
evaluation O

Numerous O
baselines O
and O
methods O
for O
VQA B-DAT
are O
provided O
and O
compared O
with O

human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

open- O
ended O
Visual O
Question O
Answering O
(VQA B-DAT

). O
A O
VQA B-DAT
system O
takes O
as O
input O
an O

Is O
this O
person O
expecting O
company?”). O
VQA B-DAT
[19], O
[36], O
[50], O
[3] O
is O

the O
high-level O
reasoning O
required O
for O
VQA B-DAT
by O
removing O
the O
need O
to O

29]. O
As O
part O
of O
the O
VQA B-DAT
initiative, O
we O
will O
organize O
an O

state-of-the-art O
methods O
and O
best O
practices. O
VQA B-DAT
poses O
a O
rich O
set O
of O

during O
the O
past O
few O
decades. O
VQA B-DAT
provides O
an O
attractive O
balance O
between O

VQA B-DAT
Efforts. O
Several O
recent O
papers O
have O

difficult O
and O
unconstrained O
task, O
our O
VQA B-DAT
dataset O
is O
two O
orders O
of O

1,449 O
images O
respectively). O
The O
proposed O
VQA B-DAT
task O
has O
connections O
to O
other O

These O
approaches O
provide O
inspiration O
for O
VQA B-DAT
techniques. O
One O
key O
concern O
in O

fixed O
set O
of O
loca- O
tions. O
VQA B-DAT
is O
naturally O
grounded O
in O
images O

Describing O
Visual O
Content. O
Related O
to O
VQA B-DAT
are O
the O
tasks O
of O
image O

by O
[53]). O
The O
questions O
in O
VQA B-DAT
require O
detailed O
specific O
information O
about O

3 O
VQA B-DAT
DATASET O
COLLECTION O

describe O
the O
Visual O
Question O
Answering O
(VQA) B-DAT
dataset. O
We O
begin O
by O
describing O

they O
are O
well-suited O
for O
our O
VQA B-DAT
task. O
The O
more O
diverse O
our O

their O
answers. O
Abstract O
Scenes. O
The O
VQA B-DAT
task O
with O
real O
images O
requires O

the O
high-level O
reasoning O
required O
for O
VQA, B-DAT
but O
not O
the O
low-level O
vision O

test-standard, O
test-challenge, O
test-reserve). O
For O
the O
VQA B-DAT
challenge O
(see O
section O
6), O
test-dev O

default’ O
test O
data O
for O
the O
VQA B-DAT
competition. O
When O
comparing O
to O
the O

sentences O
containing O
multiple O
words. O
In O
VQA, B-DAT
most O
answers O
(89.32%) O
are O
single O

4 O
VQA B-DAT
DATASET O
ANALYSIS O
In O
this O
section O

questions O
and O
answers O
in O
the O
VQA B-DAT
train O
dataset. O
To O
gain O
an O

visual O
information O
is O
critical O
to O
VQA B-DAT
and O
that O
commonsense O
information O
alone O

from O
the O
real O
images O
of O
VQA B-DAT
trainval) O
asking O
subjects O

5 O
VQA B-DAT
BASELINES O
AND O
METHODS O
In O
this O

explore O
the O
difficulty O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

novel O
methods. O
We O
train O
on O
VQA B-DAT
train+val. O
Unless O
stated O
otherwise, O
all O

top O
1K O
answers O
of O
the O
VQA B-DAT
train/val O
dataset O

multiple- O
choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
As O

I O
(Fig. O
8), O
selected O
using O
VQA B-DAT
test-dev O
accuracies) O
on O
VQA O
test O

worse O
than O
human O
performance. O
Our O
VQA B-DAT
demo O
is O
available O
on O
CloudCV O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

system O
is O
correct O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

correct” O
implies O
that O
it O
has O
VQA B-DAT
accuracy O
1.0 O
(see O
section O
3 O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
The O

ground O
truth O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

frequently O
predicted O
answers O
on O
the O
VQA B-DAT
validation O
set O
(plot O
is O
sorted O

age O
of O
question) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

system O
is O
correct) O
on O
the O
VQA B-DAT
valida- O
tion O
set. O
System O
refers O

a O
filtered O
version O
of O
the O
VQA B-DAT
train O
+ O
val O
dataset O
in O

and O
multiple-choice O
tasks O
on O
the O
VQA B-DAT
test-dev O
for O
real O
images. O
Q O

6 O
VQA B-DAT
CHALLENGE O
AND O
WORKSHOP O

papers O
reporting O
results O
on O
the O
VQA B-DAT
dataset O

task O
of O
Visual O
Question O
Answering O
(VQA B-DAT

multiple-choice O
tasks O
in O
the O
respective O
VQA B-DAT
Real O
Image O
Challenge O
leaderboards O
(as O

datasets O
may O
help O
enable O
practical O
VQA B-DAT
applications. O
We O
believe O
VQA O
has O

questions O
IV O
- O
Details O
on O
VQA B-DAT
baselines O
V O
- O
“Age” O
and O

Leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Open-Ended) O
on O

leaderboard O
showing O
test-standard O
accuracies O
for O
VQA B-DAT
Real O
Image O
Challenge O
(Multiple-Choice) O
on O

Additional O
examples O
from O
the O
VQA B-DAT
dataset O

scenes. O
This O
helps O
motivate O
the O
VQA B-DAT
task O
as O
a O
way O
to O

APPENDIX O
IV: O
DETAILS O
ON O
VQA B-DAT
BASELINES O
“per O
Q-type O
prior” O
baseline O

For O
every O
question O
in O
the O
VQA B-DAT
test-standard O
set, O
we O
find O
its O

norm O
I), O
selected O
using O
VQA B-DAT
test- O
dev O
accuracies). O
To O
estimate O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

subset O
of O
questions O
in O
the O
VQA B-DAT
validation O
set O
for O
which O
we O

a O
random O
selection O
of O
the O
VQA B-DAT
dataset O
for O
the O
MS O
COCO O

is O
simply O
not O
possible O
in O
real B-DAT
images. O
The O
novelty O
of O
our O

collection O
inter- O
face, O
application O
to O
real B-DAT
images, O
extension O
to O
all O
questions O

tol O
et O
al. O
[3]. O
VQA O
real B-DAT
images O
dataset O
contains O
just O
over O

the O
winning O
entry O
on O
the O
real B-DAT
images O
track O
of O
the O
VQA O

Making O
the O
V O
in O
VQA B-DAT
Matter: O
Elevating O
the O
Role O
of O

task O
of O
Visual O
Question O
Answering O
(VQA) B-DAT
and O
make O
vision O
(the O
V O

in O
VQA) B-DAT
matter! O
Specifically, O
we O
balance O
the O

popular O
VQA B-DAT
dataset O
[3] O
by O
collecting O
complementary O

balanced O
than O
the O
origi- O
nal O
VQA B-DAT
dataset O
and O
has O
approximately O
twice O

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2.0 O

benchmark O
a O
number O
of O
state-of-art O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

1: O
Examples O
from O
our O
balanced O
VQA B-DAT
dataset O

28] O
and O
visual O
question O
answering O
(VQA) B-DAT
[3, O
26, O
27, O
10, O
31 O

1]. O
For O
instance, O
in O
the O
VQA B-DAT
[3] O
dataset, O
the O
most O
com O

visual O
priming O
bias’ O
in O
the O
VQA B-DAT
dataset O
– O
specifi- O
cally, O
subjects O

in O
the O
VQA B-DAT
dataset O
starting O
with O
the O
n-gram O

associated O
image O
results O
in O
a O
VQA B-DAT
accuracy O
of O
87 O

role O
of O
image O
understanding O
in O
VQA B-DAT

goal, O
we O
collect O
a O
balanced O
VQA B-DAT
dataset O
with O
significantly O
reduced O
language O

ically, O
we O
create O
a O
balanced O
VQA B-DAT
dataset O
in O
the O
following O
way O

answer) O
triplet O
(I,Q,A) O
from O
the O
VQA B-DAT
dataset, O
we O
ask O
a O
human O

this O
balanced O
dataset O
will O
force O
VQA B-DAT
models O
to O
focus O
on O
visual O

exploiting O
language O
priors, O
en- O
abling O
VQA B-DAT
evaluation O
protocols O
to O
more O
accurately O

Our O
balanced O
VQA B-DAT
dataset O
is O
also O
particularly O
difficult O

of O
VGGNet O
[37] O
features. O
Therefore, O
VQA B-DAT
models O
will O
need O
to O
under O

double O
the O
size O
of O
the O
VQA B-DAT
[3] O
dataset O
– O
with O
approximately O

23]. O
We O
believe O
this O
balanced O
VQA B-DAT
dataset O
is O
a O
better O
dataset O

to O
benchmark O
VQA B-DAT
approaches, O
and O
is O
publicly O
available O

will O
allow O
users O
of O
the O
VQA B-DAT
model O
to O
establish O
greater O
trust O

1) O
We O
balance O
the O
existing O
VQA B-DAT
dataset O
[3] O
by O
collecting O
complementary O

result O
is O
a O
more O
balanced O
VQA B-DAT
dataset, O
which O
is O
also O
approximately O

the O
size O
of O
the O
original O
VQA B-DAT
dataset. O
(2) O
We O
evaluate O
state-of-art O

VQA B-DAT
models O
(with O
publicly O
available O
code O

trained O
on O
the O
existing O
‘unbalanced’ O
VQA B-DAT
dataset O
perform O
poorly O
on O
our O

language O
priors O
in O
the O
existing O
VQA B-DAT
dataset O
to O
achieve O
higher O
accuracy O

builds O
on O
top O
of O
the O
VQA B-DAT
dataset O
from O
Antol O
et O
al O

of O
the O
most O
widely O
used O
VQA B-DAT
datasets. O
We O
reduce O
the O
language O

twice O
the O
size O
of O
the O
VQA B-DAT
dataset. O
We O
benchmark O
one O
‘baseline O

’ O
VQA B-DAT
model O
[24], O
one O
attention-based O
VQA O

the O
winning O
model O
from O
the O
VQA B-DAT
Real O
Open O
Ended O
Chal- O
lenge O

2016 O
[9] O
on O
our O
balanced O
VQA B-DAT
dataset, O
and O
compare O
them O
to O

examples O
from O
our O
proposed O
balanced O
VQA B-DAT
dataset. O
Each O
question O
has O
two O

similar O
images O
for O
questions O
in O
VQA B-DAT

study O
this O
goal O
of O
balancing O
VQA B-DAT
in O
a O
fairly O
restricted O
setting O

from O
clipart O
(part O
of O
the O
VQA B-DAT
abstract O
scenes O
dataset O
[3]). O
Using O

binary O
ones), O
benchmarking O
of O
state-of-art O
VQA B-DAT
models O
on O
the O
balanced O
dataset O

, O
and O
finally O
the O
novel O
VQA B-DAT
model O
with O
counter-example O
based O
explanations O

build O
on O
top O
of O
the O
VQA B-DAT
dataset O
introduced O
by O
An- O
tol O

et O
al. O
[3]. O
VQA B-DAT
real O
images O
dataset O
contains O
just O

has O
spurred O
significant O
progress O
in O
VQA B-DAT
domain, O
as O
discussed O
earlier, O
it O

answer) O
triplet O
(I,Q,A) O
in O
the O
VQA B-DAT
dataset, O
our O
goal O
is O
to O

are O
seman- O
tically O
similar, O
a O
VQA B-DAT
model O
will O
have O
to O
understand O

questions O
from O
the O
original O
(unbalanced) O
VQA B-DAT
dataset O
[3] O
(top) O
and O
from O

all O
the O
questions O
in O
the O
VQA B-DAT
dataset. O
We O
believe O
that O
a O

and O
test O
splits O
of O
the O
VQA B-DAT
dataset. O
AMT O
workers O
picked O
“not O

question, O
image) O
pairs. O
Following O
original O
VQA B-DAT
dataset O
[3], O
we O
divide O
our O

We O
use O
the O
publicly O
released O
VQA B-DAT
evaluation O
script O
in O
our O
experiments O

for O
each O
question O
to O
compute O
VQA B-DAT
accura- O
cies. O
As O
described O
above O

to O
be O
consistent O
with O
the O
VQA B-DAT
dataset O
[3]. O
Note O
that O
while O

type O
in O
our O
new O
balanced O
VQA B-DAT
dataset O
with O
the O
original O
(un O

- O
balanced) O
VQA B-DAT
dataset O
[3]. O
We O
notice O
several O

balanced O
dataset O
compared O
to O
unbalanced O
VQA B-DAT
dataset. O
“baseball” O
is O
now O
slightly O

more O
balanced O
than O
the O
original O
VQA B-DAT
dataset. O
The O
resultant O
impact O
of O

ing O
on O
performance O
of O
state-of-the-art O
VQA B-DAT
models O
is O
dis- O
cussed O
in O

4. O
Benchmarking O
Existing O
VQA B-DAT
Models O
Our O
first O
approach O
to O

training O
a O
VQA B-DAT
model O
that O
empha O

to O
re-train O
the O
existing O
state-of-art O
VQA B-DAT
models O
(with O
code O
publicly O
available O

9]) O
on O
our O
new O
balanced O
VQA B-DAT
dataset. O
Our O
hypothesis O
is O
that O

I) O
[24]: O
This O
was O
the O
VQA B-DAT
model O
introduced O
in O
[3] O
together O

This O
is O
a O
recent O
attention-based O
VQA B-DAT
model O
that O
‘co-attends’ O
to O
both O

real O
images O
track O
of O
the O
VQA B-DAT
Challenge O
2016. O
This O
model O
uses O

utilize O
any O
visual O
information. O
Comparing O
VQA B-DAT
models O
to O
language- O
only O
ablations O

quantifies O
to O
what O
extent O
VQA B-DAT
models O
have O
succeeded O
in O
leveraging O

Table O
1: O
Performance O
of O
VQA B-DAT
models O
when O
trained/tested O
on O
unbalanced/balanced O

VQA B-DAT
datasets. O
UB O
stands O
for O
training O

see O
that O
the O
current O
state-of-art O
VQA B-DAT
models O
trained O
on O
the O
original O

unbalanced) O
VQA B-DAT
dataset O
perform O
signifi- O
cantly O
worse O

evaluating O
on O
the O
original O
unbalanced O
VQA B-DAT
dataset O
(i.e., O
comparing O
UU O
to O

in O
accuracy O
suggests O
that O
current O
VQA B-DAT
models O
are O
data O
starved, O
and O

would O
benefit O
from O
even O
larger O
VQA B-DAT
datasets O

the O
language-bias O
in O
the O
original O
VQA B-DAT
dataset, O
and O
its O
successful O
alleviation O

same O
question. O
To O
be O
successful, O
VQA B-DAT
models O
need O
to O
understand O
the O

an- O
alyze O
the O
performance O
of O
VQA B-DAT
models O
in O
unique O
ways. O
Given O

the O
prediction O
of O
a O
VQA B-DAT
model, O
we O
can O
count O
the O

training O
on O
balanced O
dataset, O
this O
VQA B-DAT
model O
has O
learned O
to O
tell O

room O
for O
improvement O
remains. O
The O
VQA B-DAT
model O
still O
can O
not O
tell O

To O
benchmark O
models O
on O
VQA B-DAT
v2.0 O
dataset, O
we O
also O
train O

these O
models O
on O
VQA B-DAT
v2.0 O
train+val O
and O
report O
re O

- O
sults O
on O
VQA B-DAT
v2.0 O
test-standard O
in O
Table O
2 O

Papers O
report- O
ing O
results O
on O
VQA B-DAT
v2.0 O
dataset O
are O
suggested O
to O

Table O
2: O
Performance O
of O
VQA B-DAT
models O
when O
trained O
on O
VQA O

v2.0 O
train+val O
and O
tested O
on O
VQA B-DAT
v2.0 O
test-standard O
dataset O

models O
when O
trained/tested O
on O
unbalanced/balanced O
VQA B-DAT
datasets. O
UB O
stands O
for O
training O

HieCoAtt). O
This O
suggests O
that O
these O
VQA B-DAT
mod- O
els O
are O
really O
exploiting O

that O
for O
both O
the O
state-of-art O
VQA B-DAT
mod- O
els, O
the O
largest O
source O

the O
results O
announced O
at O
the O
VQA B-DAT
Real O
Open O
Ended O
Challenge O
20162 O

priors O
present O
in O
the O
unbalanced O
VQA B-DAT
dataset O
(particularly O
in O
the O
“yes/no O

similar O
accuracies O
for O
all O
state-of-art O
VQA B-DAT
models, O
rendering O
vastly O
different O
models O

these O
answer-types). O
Benchmarking O
these O
different O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

color O
is O
the O
fire-hydrant?” O
a O
VQA B-DAT
model O
may O
be O
perceived O
as O

step, O
similar O
to O
a O
conventional O
VQA B-DAT
model, O
it O
takes O
in O
an O

similar O
to O
a O
conven- O
tional O
VQA B-DAT
model), O
and O
second, O
it O
explains O

VQA B-DAT
head O
in O
our O
model, O
and O

the O
most O
likely O
counter- O
example. O
VQA B-DAT
Model: O
Using O
a O
VQA O
model’s O

Random O
Distance O
VQA B-DAT
[3] O
Ours O

baseline, O
as O
well O
as O
the O
VQA B-DAT
[3] O
model. O
Interestingly, O
the O
strongest O

terface O
to O
‘balance’ O
the O
popular O
VQA B-DAT
dataset O
[3] O
by O
col- O
lecting O

bal- O
anced O
than O
the O
original O
VQA B-DAT
dataset O
by O
construction, O
but O
also O

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2.0 O

a O
number O
of O
(near) O
state-of-art O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

L. O
Zitnick, O
and O
D. O
Parikh. O
VQA B-DAT

D. O
Parikh. O
Question O
Relevance O
in O
VQA B-DAT

Image O
Understanding O
in O
Visual O
Question O
Answering B-DAT

the O
task O
of O
Visual O
Question O
Answering B-DAT
(VQA) O
and O
make O
vision O
(the O

iteration O
of O
the O
Visual O
Question O
Answering B-DAT
Dataset O
and O
Challenge O
(VQA O
v2.0 O

2. O
Related O
Work O
Visual O
Question O
Answering B-DAT

2. O
Answering B-DAT
head: O
The O
second O
component O
is O

the O
task O
of O
Visual O
Question O
Answering B-DAT
and O
elevate O
the O
role O
of O

iteration O
of O
the O
Visual O
Question O
Answering B-DAT
Dataset O
and O
Challenge O
(VQA O
v2.0 O

of O
Visual O
Question O
Answering B-DAT
Models. O
In O
EMNLP, O
2016. O
1 O

D. O
Parikh. O
VQA: O
Visual O
Question O
Answering B-DAT

Pooling O
for O
Vi- O
sual O
Question O
Answering B-DAT
and O
Visual O
Grounding. O
In O
EMNLP O

der O
Maaten. O
Revisiting O
Visual O
Question O
Answering B-DAT
Baselines. O
In O
ECCV, O
2016. O
1 O

and O
C. O
Kanan. O
Visual O
Question O
Answering B-DAT

and O
normalized O
CNN O
Visual O
Question O
Answering B-DAT
model. O
https://github.com/VT-vision-lab/ O
VQA_LSTM_CNN, O
2015. O
2 O

A O
Multi-World O
Approach O
to O
Question O
Answering B-DAT
about O
Real-World O
Scenes O
based O
on O

1 O
Million O
Full-Sentences O
Visual O
Question O
Answering B-DAT
(FSVQA). O
arXiv O
preprint O
arXiv:1609.06657, O
2016 O

Answering B-DAT

Spatial O
Attention O
for O
Visual O
Question O
Answering B-DAT

tention O
Networks O
for O
Image O
Question O
Answering B-DAT

Yin O
and O
Yang: O
Balancing O
and O
Answering B-DAT
Binary O
Visual O
Questions. O
In O
CVPR O

Simple O
Baseline O
for O
Visual O
Question O
Answering B-DAT

dataset O
[3] O
by O
collecting O
complementary O
images B-DAT
such O
that O
every O
question O
in O

rather O
a O
pair O
of O
similar O
images B-DAT
that O
result O
in O
two O
different O

protocol O
for O
identifying O
com- O
plementary O
images B-DAT
enables O
us O
to O
develop O
a O

tower O
in O
the O
picture?” O
on O
images B-DAT
actually O
containing O
clock O
tow- O
ers O

the O
goal O
of O
under- O
standing O
images B-DAT
correctly O
when O
they O
are O
only O

and O
A′) O
for O
two O
different O
images B-DAT
(I O
and O
I O
′ O
respectively O

subtle O
differences O
between O
the O
two O
images B-DAT
to O
pre- O
dict O
the O
answers O

to O
both O
the O
images B-DAT
correctly O

associated O
answers O
on O
the O
∼200k O
images B-DAT
from O
COCO O
[23]. O
We O
believe O

not O
only O
answers O
questions O
about O
images, B-DAT
but O
also O
‘explains’ O
its O
answer O

hard O
negatives” O
i.e., O
examples O
of O
images B-DAT
that O
it O
believes O
are O
similar O

dataset O
[3] O
by O
collecting O
complementary O
images B-DAT
such O
that O
almost O
every O
question O

rather O
a O
pair O
of O
similar O
images B-DAT
that O
result O
in O
two O
different O

addition O
to O
answering O
questions O
about O
images, B-DAT
also O
provides O
a O
counter- O
example O

based O
explanation O
– O
it O
retrieves O
images B-DAT
that O
it O
be- O
lieves O
are O

Each O
question O
has O
two O
similar O
images B-DAT
with O
different O
answers O
to O
the O

create O
two O
similar O
captions O
for O
images, B-DAT
while O
we O
create O
a O
novel O

interface O
to O
collect O
two O
similar O
images B-DAT
for O
questions O
in O
VQA O

AMT) O
interface O
to O
collect O
complementary O
images B-DAT

simply O
not O
possible O
in O
real O
images B-DAT

inter- O
face, O
application O
to O
real O
images, B-DAT
extension O
to O
all O
questions O
(not O

or O
spatial O
maps O
overlaid O
on O
images B-DAT
to O
highlight O
the O
regions O
that O

et O
al. O
[3]. O
VQA O
real O
images B-DAT
dataset O
contains O
just O
over O
204K O

images B-DAT
from O
COCO O
[23], O
614K O
free-form O

to O
collect O
such O
com- O
plementary O
images B-DAT
on O
Amazon O
Mechanical O
Turk O
(AMT O

workers O
are O
shown O
24 O
nearest-neighbor O
images B-DAT
of O
I O
, O
the O
question O

from O
the O
list O
of O
24 O
images B-DAT
for O
whichQ O
“makes O
sense” O
and O

After O
the O
complementary O
images B-DAT
are O
collected, O
we O
con- O
duct O

collect O
answers O
on O
these O
new O
images B-DAT

results O
in O
pairs O
of O
complementary O
images B-DAT
I O
and O
I O
′ O
that O

right O
an- O
swer O
to O
both O
images B-DAT

. O
Example O
complementary O
images B-DAT
are O
shown O
in O
Fig. O
1 O

for O
any O
of O
the O
24 O
images B-DAT
(e.g. O
the O
question O
is O
‘what O

and O
none O
of O
the O
neighboring O
images B-DAT
contain O
a O
woman), O
or O
(2 O

applicable O
to O
some O
neighbor- O
ing O
images, B-DAT
but O
the O
answer O
to O
the O

thus O
the O
nearest O
neigh- O
bor O
images, B-DAT
while O
globally O
similar, O
do O
not O

many O
more O
than O
24 O
neighboring O
images B-DAT
could O
pos- O
sibly O
reduce O
this O

We O
collected O
complementary O
images B-DAT
and O
the O
corre- O
sponding O
new O

we O
collected O
approximately O
195K O
complementary O
images B-DAT
for O
train, O
93K O
complementary O
images O

for O
val, O
and O
191K O
complementary O
images B-DAT
for O
test O
set. O
In O
addition O

winning O
entry O
on O
the O
real O
images B-DAT
track O
of O
the O
VQA O
Challenge O

has O
different O
answers O
on O
different O
images B-DAT

can O
extract O
detailed O
information O
from O
images B-DAT
and O
leverage O
this O
information O
to O

free-form O
natural O
language O
questions O
about O
images B-DAT
accurately. O
As O
ex- O
pected O
from O

since O
there O
are O
pairs O
of O
images B-DAT
very O
similar O
to O
each O
other O

the O
subtle O
differences O
in O
these O
images B-DAT

of O
questions O
where O
both O
complementary O
images B-DAT
(I O
,I O
′) O
received O
correct O

difference O
between O
two O
otherwise O
similar O
images B-DAT

to O
result O
in O
the O
two O
images B-DAT
having O
different O
ground O
truth O
an O

but O
also O
pro- O
vides O
example O
images B-DAT
that O
are O
similar O
to O
the O

picks O
one O
ofK O
nearest O
neighbor O
images B-DAT
of O
I O
, O
INN O

model O
is O
learning O
representations O
of O
images B-DAT
and O
questions. O
It O
is O
a O

component. O
A O
total O
of O
25 O
images B-DAT
– O
the O
original O
image O
I O

and O
24 O
candidate O
images B-DAT
{I1, O
I2, O
..., O
I24} O
are O

inner-product O
values O
forK O
candi- O
date O
images B-DAT
are O
then O
passed O
through O
a O

1, O
2, O
...,K}. O
TheK O
candidate O
images B-DAT
{I1, O
I2, O
..., O
IK} O
are O

higher O
than O
all O
other O
candidate O
images B-DAT
by O
a O
de- O
sired O
margin O

counter-example. O
Distance: O
Sorting O
the O
candidate O
images B-DAT
in O
increasing O
order O
of O
their O

answer O
to O
sort O
the O
candidate O
images B-DAT
in O
as- O
cending O
order O
of O

extract O
meaning- O
ful O
details O
from O
images B-DAT
still O
remain O
elusive O

3] O
by O
col- O
lecting O
‘complementary’ O
images B-DAT

dataset, O
we O
have O
two O
complementary O
images B-DAT
that O
look O
sim- O
ilar, O
but O

Finally, O
our O
framework O
around O
complementary O
images B-DAT
enables O
us O
to O
develop O
a O

produces O
a O
list O
of O
similar O
images B-DAT
that O
it O
considers O
‘counter-examples’, O
i.e O

approach O
to O
answering O
questions O
about O
images B-DAT

Hierarchical O
Image O
Database. O
In O
CVPR, O
2009 B-DAT

of O
Image O
Understanding O
in O
Visual O
Question B-DAT
Answering O

for O
the O
task O
of O
Visual O
Question B-DAT
Answering O
(VQA) O
and O
make O
vision O

2nd O
iteration O
of O
the O
Visual O
Question B-DAT
Answering O
Dataset O
and O
Challenge O
(VQA O

2. O
Related O
Work O
Visual O
Question B-DAT
Answering. O
A O
number O
of O
recent O

Deeper O
LSTM O
Question B-DAT
+ O
norm O
Image O
(d-LSTM+n- O
I O

similar O
architecture O
as O
Deeper O
LSTM O
Question B-DAT
+ O
norm O
Image O
[24] O
except O

for O
the O
task O
of O
Visual O
Question B-DAT
Answering O
and O
elevate O
the O
role O

2nd O
iteration O
of O
the O
Visual O
Question B-DAT
Answering O
Dataset O
and O
Challenge O
(VQA O

of O
Visual O
Question B-DAT
Answering O
Models. O
In O
EMNLP, O
2016 O

and O
D. O
Parikh. O
VQA: O
Visual O
Question B-DAT
Answering. O
In O
ICCV, O
2015. O
1 O

Bilinear O
Pooling O
for O
Vi- O
sual O
Question B-DAT
Answering O
and O
Visual O
Grounding. O
In O

Transparent O
AI O
Systems: O
Interpreting O
Visual O
Question B-DAT
An- O
swering O
Models. O
In O
ICML O

van O
der O
Maaten. O
Revisiting O
Visual O
Question B-DAT
Answering O
Baselines. O
In O
ECCV, O
2016 O

Kafle O
and O
C. O
Kanan. O
Visual O
Question B-DAT
Answering: O
Datasets, O
Algorithms, O
and O
Future O

LSTM O
and O
normalized O
CNN O
Visual O
Question B-DAT
Answering O
model. O
https://github.com/VT-vision-lab/ O
VQA_LSTM_CNN, O
2015 O

Batra, O
and O
D. O
Parikh. O
Hierarchical O
Question B-DAT

-Image O
Co-Attention O
for O
Visual O
Question B-DAT
Answer- O
ing. O
In O
NIPS, O
2016 O

Fritz. O
A O
Multi-World O
Approach O
to O
Question B-DAT
Answering O
about O
Real-World O
Scenes O
based O

D. O
Batra, O
and O
D. O
Parikh. O
Question B-DAT
Relevance O
in O
VQA: O
Identifying O
Non-Visual O

Gray: O
1 O
Million O
Full-Sentences O
Visual O
Question B-DAT
Answering O
(FSVQA). O
arXiv O
preprint O
arXiv:1609.06657 O

Understanding O
Stories O
in O
Movies O
through O
Question B-DAT

Attend O
and O
Answer: O
Explor- O
ing O
Question B-DAT

-Guided O
Spatial O
Attention O
for O
Visual O
Question B-DAT
Answering. O
In O
ECCV, O
2016. O
2 O

At- O
tention O
Networks O
for O
Image O
Question B-DAT
Answering. O
In O
CVPR, O
2016. O
2 O

Madlibs: O
Fill-in-the-blank O
Description O
Generation O
and O
Question B-DAT
An- O
swering. O
In O
ICCV, O
2015 O

Fergus. O
Simple O
Baseline O
for O
Visual O
Question B-DAT
Answering. O
CoRR, O
abs/1512.02167, O
2015. O
1 O

Role O
of O
Image O
Understanding O
in O
Visual B-DAT
Question O
Answering O

priors O
for O
the O
task O
of O
Visual B-DAT
Question O
Answering O
(VQA) O
and O
make O

the O
2nd O
iteration O
of O
the O
Visual B-DAT
Question O
Answering O
Dataset O
and O
Challenge O

2. O
Related O
Work O
Visual B-DAT
Question O
Answering. O
A O
number O
of O

priors O
for O
the O
task O
of O
Visual B-DAT
Question O
Answering O
and O
elevate O
the O

the O
2nd O
iteration O
of O
the O
Visual B-DAT
Question O
Answering O
Dataset O
and O
Challenge O

of O
Visual B-DAT
Question O
Answering O
Models. O
In O
EMNLP O

Zitnick, O
and O
D. O
Parikh. O
VQA: O
Visual B-DAT
Question O
Answering. O
In O
ICCV, O
2015 O

Re- O
current O
Convolutional O
Networks O
for O
Visual B-DAT
Recognition O
and O
Description. O
In O
CVPR O

G. O
Zweig. O
From O
Captions O
to O
Visual B-DAT
Concepts O
and O
Back. O
In O
CVPR O

Vi- O
sual O
Question O
Answering O
and O
Visual B-DAT
Grounding. O
In O
EMNLP, O
2016. O
2 O

Towards O
Transparent O
AI O
Systems: O
Interpreting O
Visual B-DAT
Question O
An- O
swering O
Models. O
In O

L. O
van O
der O
Maaten. O
Revisiting O
Visual B-DAT
Question O
Answering O
Baselines. O
In O
ECCV O

K. O
Kafle O
and O
C. O
Kanan. O
Visual B-DAT
Question O
Answering: O
Datasets, O
Algorithms, O
and O

Karpathy O
and O
L. O
Fei-Fei. O
Deep O
Visual B-DAT

Zhang. O
Multimodal O
Residual O
Learning O
for O
Visual B-DAT
QA. O
In O
NIPS, O
2016. O
2 O

and O
R. O
S. O
Zemel. O
Unifying O
Visual B-DAT

D. O
A. O
Shamma, O
et O
al. O
Visual B-DAT
genome: O
Connecting O
language O
and O
vision O

Deeper O
LSTM O
and O
normalized O
CNN O
Visual B-DAT
Question O
Answering O
model. O
https://github.com/VT-vision-lab/ O
VQA_LSTM_CNN O

Parikh. O
Hierarchical O
Question-Image O
Co-Attention O
for O
Visual B-DAT
Question O
Answer- O
ing. O
In O
NIPS O

Visual B-DAT
And O
False-Premise O
Questions. O
In O
EMNLP O

Why O
did O
you O
say O
that? O
Visual B-DAT
Explanations O
from O
Deep O
Networks O
via O

is O
Gray: O
1 O
Million O
Full-Sentences O
Visual B-DAT
Question O
Answering O
(FSVQA). O
arXiv O
preprint O

ing O
Question-Guided O
Spatial O
Attention O
for O
Visual B-DAT
Question O
Answering. O
In O
ECCV, O
2016 O

Berg, O
and O
T. O
L. O
Berg. O
Visual B-DAT
Madlibs: O
Fill-in-the-blank O
Description O
Generation O
and O

Yang: O
Balancing O
and O
Answering O
Binary O
Visual B-DAT
Questions. O
In O
CVPR, O
2016. O
1 O

R. O
Fergus. O
Simple O
Baseline O
for O
Visual B-DAT
Question O
Answering. O
CoRR, O
abs/1512.02167, O
2015 O

on O
the O
∼200k O
images O
from O
COCO B-DAT
[23]. O
We O
believe O
this O
balanced O

just O
over O
204K O
images O
from O
COCO B-DAT
[23], O
614K O
free-form O
natural O
lan O

may O
be O
no O
image O
in O
COCO B-DAT
where O
the O
answer O
to O
“is O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT

Making O
the O
V O
in O
VQA B-DAT
Matter: O
Elevating O
the O
Role O
of O

task O
of O
Visual O
Question O
Answering O
(VQA) B-DAT
and O
make O
vision O
(the O
V O

in O
VQA) B-DAT
matter! O
Specifically, O
we O
balance O
the O

popular O
VQA B-DAT
dataset O
[3] O
by O
collecting O
complementary O

balanced O
than O
the O
origi- O
nal O
VQA B-DAT
dataset O
and O
has O
approximately O
twice O

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2.0 O

benchmark O
a O
number O
of O
state-of-art O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

1: O
Examples O
from O
our O
balanced O
VQA B-DAT
dataset O

28] O
and O
visual O
question O
answering O
(VQA) B-DAT
[3, O
26, O
27, O
10, O
31 O

1]. O
For O
instance, O
in O
the O
VQA B-DAT
[3] O
dataset, O
the O
most O
com O

visual O
priming O
bias’ O
in O
the O
VQA B-DAT
dataset O
– O
specifi- O
cally, O
subjects O

in O
the O
VQA B-DAT
dataset O
starting O
with O
the O
n-gram O

associated O
image O
results O
in O
a O
VQA B-DAT
accuracy O
of O
87 O

role O
of O
image O
understanding O
in O
VQA B-DAT

goal, O
we O
collect O
a O
balanced O
VQA B-DAT
dataset O
with O
significantly O
reduced O
language O

ically, O
we O
create O
a O
balanced O
VQA B-DAT
dataset O
in O
the O
following O
way O

answer) O
triplet O
(I,Q,A) O
from O
the O
VQA B-DAT
dataset, O
we O
ask O
a O
human O

this O
balanced O
dataset O
will O
force O
VQA B-DAT
models O
to O
focus O
on O
visual O

exploiting O
language O
priors, O
en- O
abling O
VQA B-DAT
evaluation O
protocols O
to O
more O
accurately O

Our O
balanced O
VQA B-DAT
dataset O
is O
also O
particularly O
difficult O

of O
VGGNet O
[37] O
features. O
Therefore, O
VQA B-DAT
models O
will O
need O
to O
under O

double O
the O
size O
of O
the O
VQA B-DAT
[3] O
dataset O
– O
with O
approximately O

23]. O
We O
believe O
this O
balanced O
VQA B-DAT
dataset O
is O
a O
better O
dataset O

to O
benchmark O
VQA B-DAT
approaches, O
and O
is O
publicly O
available O

will O
allow O
users O
of O
the O
VQA B-DAT
model O
to O
establish O
greater O
trust O

1) O
We O
balance O
the O
existing O
VQA B-DAT
dataset O
[3] O
by O
collecting O
complementary O

result O
is O
a O
more O
balanced O
VQA B-DAT
dataset, O
which O
is O
also O
approximately O

the O
size O
of O
the O
original O
VQA B-DAT
dataset. O
(2) O
We O
evaluate O
state-of-art O

VQA B-DAT
models O
(with O
publicly O
available O
code O

trained O
on O
the O
existing O
‘unbalanced’ O
VQA B-DAT
dataset O
perform O
poorly O
on O
our O

language O
priors O
in O
the O
existing O
VQA B-DAT
dataset O
to O
achieve O
higher O
accuracy O

builds O
on O
top O
of O
the O
VQA B-DAT
dataset O
from O
Antol O
et O
al O

of O
the O
most O
widely O
used O
VQA B-DAT
datasets. O
We O
reduce O
the O
language O

twice O
the O
size O
of O
the O
VQA B-DAT
dataset. O
We O
benchmark O
one O
‘baseline O

’ O
VQA B-DAT
model O
[24], O
one O
attention-based O
VQA O

the O
winning O
model O
from O
the O
VQA B-DAT
Real O
Open O
Ended O
Chal- O
lenge O

2016 O
[9] O
on O
our O
balanced O
VQA B-DAT
dataset, O
and O
compare O
them O
to O

examples O
from O
our O
proposed O
balanced O
VQA B-DAT
dataset. O
Each O
question O
has O
two O

similar O
images O
for O
questions O
in O
VQA B-DAT

study O
this O
goal O
of O
balancing O
VQA B-DAT
in O
a O
fairly O
restricted O
setting O

from O
clipart O
(part O
of O
the O
VQA B-DAT
abstract O
scenes O
dataset O
[3]). O
Using O

binary O
ones), O
benchmarking O
of O
state-of-art O
VQA B-DAT
models O
on O
the O
balanced O
dataset O

, O
and O
finally O
the O
novel O
VQA B-DAT
model O
with O
counter-example O
based O
explanations O

build O
on O
top O
of O
the O
VQA B-DAT
dataset O
introduced O
by O
An- O
tol O

et O
al. O
[3]. O
VQA B-DAT
real O
images O
dataset O
contains O
just O

has O
spurred O
significant O
progress O
in O
VQA B-DAT
domain, O
as O
discussed O
earlier, O
it O

answer) O
triplet O
(I,Q,A) O
in O
the O
VQA B-DAT
dataset, O
our O
goal O
is O
to O

are O
seman- O
tically O
similar, O
a O
VQA B-DAT
model O
will O
have O
to O
understand O

questions O
from O
the O
original O
(unbalanced) O
VQA B-DAT
dataset O
[3] O
(top) O
and O
from O

all O
the O
questions O
in O
the O
VQA B-DAT
dataset. O
We O
believe O
that O
a O

and O
test O
splits O
of O
the O
VQA B-DAT
dataset. O
AMT O
workers O
picked O
“not O

question, O
image) O
pairs. O
Following O
original O
VQA B-DAT
dataset O
[3], O
we O
divide O
our O

We O
use O
the O
publicly O
released O
VQA B-DAT
evaluation O
script O
in O
our O
experiments O

for O
each O
question O
to O
compute O
VQA B-DAT
accura- O
cies. O
As O
described O
above O

to O
be O
consistent O
with O
the O
VQA B-DAT
dataset O
[3]. O
Note O
that O
while O

type O
in O
our O
new O
balanced O
VQA B-DAT
dataset O
with O
the O
original O
(un O

- O
balanced) O
VQA B-DAT
dataset O
[3]. O
We O
notice O
several O

balanced O
dataset O
compared O
to O
unbalanced O
VQA B-DAT
dataset. O
“baseball” O
is O
now O
slightly O

more O
balanced O
than O
the O
original O
VQA B-DAT
dataset. O
The O
resultant O
impact O
of O

ing O
on O
performance O
of O
state-of-the-art O
VQA B-DAT
models O
is O
dis- O
cussed O
in O

4. O
Benchmarking O
Existing O
VQA B-DAT
Models O
Our O
first O
approach O
to O

training O
a O
VQA B-DAT
model O
that O
empha O

to O
re-train O
the O
existing O
state-of-art O
VQA B-DAT
models O
(with O
code O
publicly O
available O

9]) O
on O
our O
new O
balanced O
VQA B-DAT
dataset. O
Our O
hypothesis O
is O
that O

I) O
[24]: O
This O
was O
the O
VQA B-DAT
model O
introduced O
in O
[3] O
together O

This O
is O
a O
recent O
attention-based O
VQA B-DAT
model O
that O
‘co-attends’ O
to O
both O

real O
images O
track O
of O
the O
VQA B-DAT
Challenge O
2016. O
This O
model O
uses O

utilize O
any O
visual O
information. O
Comparing O
VQA B-DAT
models O
to O
language- O
only O
ablations O

quantifies O
to O
what O
extent O
VQA B-DAT
models O
have O
succeeded O
in O
leveraging O

Table O
1: O
Performance O
of O
VQA B-DAT
models O
when O
trained/tested O
on O
unbalanced/balanced O

VQA B-DAT
datasets. O
UB O
stands O
for O
training O

see O
that O
the O
current O
state-of-art O
VQA B-DAT
models O
trained O
on O
the O
original O

unbalanced) O
VQA B-DAT
dataset O
perform O
signifi- O
cantly O
worse O

evaluating O
on O
the O
original O
unbalanced O
VQA B-DAT
dataset O
(i.e., O
comparing O
UU O
to O

in O
accuracy O
suggests O
that O
current O
VQA B-DAT
models O
are O
data O
starved, O
and O

would O
benefit O
from O
even O
larger O
VQA B-DAT
datasets O

the O
language-bias O
in O
the O
original O
VQA B-DAT
dataset, O
and O
its O
successful O
alleviation O

same O
question. O
To O
be O
successful, O
VQA B-DAT
models O
need O
to O
understand O
the O

an- O
alyze O
the O
performance O
of O
VQA B-DAT
models O
in O
unique O
ways. O
Given O

the O
prediction O
of O
a O
VQA B-DAT
model, O
we O
can O
count O
the O

training O
on O
balanced O
dataset, O
this O
VQA B-DAT
model O
has O
learned O
to O
tell O

room O
for O
improvement O
remains. O
The O
VQA B-DAT
model O
still O
can O
not O
tell O

To O
benchmark O
models O
on O
VQA B-DAT
v2.0 O
dataset, O
we O
also O
train O

these O
models O
on O
VQA B-DAT
v2.0 O
train+val O
and O
report O
re O

- O
sults O
on O
VQA B-DAT
v2.0 O
test-standard O
in O
Table O
2 O

Papers O
report- O
ing O
results O
on O
VQA B-DAT
v2.0 O
dataset O
are O
suggested O
to O

Table O
2: O
Performance O
of O
VQA B-DAT
models O
when O
trained O
on O
VQA O

v2.0 O
train+val O
and O
tested O
on O
VQA B-DAT
v2.0 O
test-standard O
dataset O

models O
when O
trained/tested O
on O
unbalanced/balanced O
VQA B-DAT
datasets. O
UB O
stands O
for O
training O

HieCoAtt). O
This O
suggests O
that O
these O
VQA B-DAT
mod- O
els O
are O
really O
exploiting O

that O
for O
both O
the O
state-of-art O
VQA B-DAT
mod- O
els, O
the O
largest O
source O

the O
results O
announced O
at O
the O
VQA B-DAT
Real O
Open O
Ended O
Challenge O
20162 O

priors O
present O
in O
the O
unbalanced O
VQA B-DAT
dataset O
(particularly O
in O
the O
“yes/no O

similar O
accuracies O
for O
all O
state-of-art O
VQA B-DAT
models, O
rendering O
vastly O
different O
models O

these O
answer-types). O
Benchmarking O
these O
different O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

color O
is O
the O
fire-hydrant?” O
a O
VQA B-DAT
model O
may O
be O
perceived O
as O

step, O
similar O
to O
a O
conventional O
VQA B-DAT
model, O
it O
takes O
in O
an O

similar O
to O
a O
conven- O
tional O
VQA B-DAT
model), O
and O
second, O
it O
explains O

VQA B-DAT
head O
in O
our O
model, O
and O

the O
most O
likely O
counter- O
example. O
VQA B-DAT
Model: O
Using O
a O
VQA O
model’s O

Random O
Distance O
VQA B-DAT
[3] O
Ours O

baseline, O
as O
well O
as O
the O
VQA B-DAT
[3] O
model. O
Interestingly, O
the O
strongest O

terface O
to O
‘balance’ O
the O
popular O
VQA B-DAT
dataset O
[3] O
by O
col- O
lecting O

bal- O
anced O
than O
the O
original O
VQA B-DAT
dataset O
by O
construction, O
but O
also O

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2.0 O

a O
number O
of O
(near) O
state-of-art O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

L. O
Zitnick, O
and O
D. O
Parikh. O
VQA B-DAT

D. O
Parikh. O
Question O
Relevance O
in O
VQA B-DAT

is O
simply O
not O
possible O
in O
real B-DAT
images. O
The O
novelty O
of O
our O

collection O
inter- O
face, O
application O
to O
real B-DAT
images, O
extension O
to O
all O
questions O

tol O
et O
al. O
[3]. O
VQA O
real B-DAT
images O
dataset O
contains O
just O
over O

the O
winning O
entry O
on O
the O
real B-DAT
images O
track O
of O
the O
VQA O

Making O
the O
V O
in O
VQA B-DAT
Matter: O
Elevating O
the O
Role O
of O

task O
of O
Visual O
Question O
Answering O
(VQA) B-DAT
and O
make O
vision O
(the O
V O

in O
VQA) B-DAT
matter! O
Specifically, O
we O
balance O
the O

popular O
VQA B-DAT
dataset O
[3] O
by O
collecting O
complementary O

balanced O
than O
the O
origi- O
nal O
VQA B-DAT
dataset O
and O
has O
approximately O
twice O

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2.0 O

benchmark O
a O
number O
of O
state-of-art O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

1: O
Examples O
from O
our O
balanced O
VQA B-DAT
dataset O

28] O
and O
visual O
question O
answering O
(VQA) B-DAT
[3, O
26, O
27, O
10, O
31 O

1]. O
For O
instance, O
in O
the O
VQA B-DAT
[3] O
dataset, O
the O
most O
com O

visual O
priming O
bias’ O
in O
the O
VQA B-DAT
dataset O
– O
specifi- O
cally, O
subjects O

in O
the O
VQA B-DAT
dataset O
starting O
with O
the O
n-gram O

associated O
image O
results O
in O
a O
VQA B-DAT
accuracy O
of O
87 O

role O
of O
image O
understanding O
in O
VQA B-DAT

goal, O
we O
collect O
a O
balanced O
VQA B-DAT
dataset O
with O
significantly O
reduced O
language O

ically, O
we O
create O
a O
balanced O
VQA B-DAT
dataset O
in O
the O
following O
way O

answer) O
triplet O
(I,Q,A) O
from O
the O
VQA B-DAT
dataset, O
we O
ask O
a O
human O

this O
balanced O
dataset O
will O
force O
VQA B-DAT
models O
to O
focus O
on O
visual O

exploiting O
language O
priors, O
en- O
abling O
VQA B-DAT
evaluation O
protocols O
to O
more O
accurately O

Our O
balanced O
VQA B-DAT
dataset O
is O
also O
particularly O
difficult O

of O
VGGNet O
[37] O
features. O
Therefore, O
VQA B-DAT
models O
will O
need O
to O
under O

double O
the O
size O
of O
the O
VQA B-DAT
[3] O
dataset O
– O
with O
approximately O

23]. O
We O
believe O
this O
balanced O
VQA B-DAT
dataset O
is O
a O
better O
dataset O

to O
benchmark O
VQA B-DAT
approaches, O
and O
is O
publicly O
available O

will O
allow O
users O
of O
the O
VQA B-DAT
model O
to O
establish O
greater O
trust O

1) O
We O
balance O
the O
existing O
VQA B-DAT
dataset O
[3] O
by O
collecting O
complementary O

result O
is O
a O
more O
balanced O
VQA B-DAT
dataset, O
which O
is O
also O
approximately O

the O
size O
of O
the O
original O
VQA B-DAT
dataset. O
(2) O
We O
evaluate O
state-of-art O

VQA B-DAT
models O
(with O
publicly O
available O
code O

trained O
on O
the O
existing O
‘unbalanced’ O
VQA B-DAT
dataset O
perform O
poorly O
on O
our O

language O
priors O
in O
the O
existing O
VQA B-DAT
dataset O
to O
achieve O
higher O
accuracy O

builds O
on O
top O
of O
the O
VQA B-DAT
dataset O
from O
Antol O
et O
al O

of O
the O
most O
widely O
used O
VQA B-DAT
datasets. O
We O
reduce O
the O
language O

twice O
the O
size O
of O
the O
VQA B-DAT
dataset. O
We O
benchmark O
one O
‘baseline O

’ O
VQA B-DAT
model O
[24], O
one O
attention-based O
VQA O

the O
winning O
model O
from O
the O
VQA B-DAT
Real O
Open O
Ended O
Chal- O
lenge O

2016 O
[9] O
on O
our O
balanced O
VQA B-DAT
dataset, O
and O
compare O
them O
to O

examples O
from O
our O
proposed O
balanced O
VQA B-DAT
dataset. O
Each O
question O
has O
two O

similar O
images O
for O
questions O
in O
VQA B-DAT

study O
this O
goal O
of O
balancing O
VQA B-DAT
in O
a O
fairly O
restricted O
setting O

from O
clipart O
(part O
of O
the O
VQA B-DAT
abstract O
scenes O
dataset O
[3]). O
Using O

binary O
ones), O
benchmarking O
of O
state-of-art O
VQA B-DAT
models O
on O
the O
balanced O
dataset O

, O
and O
finally O
the O
novel O
VQA B-DAT
model O
with O
counter-example O
based O
explanations O

build O
on O
top O
of O
the O
VQA B-DAT
dataset O
introduced O
by O
An- O
tol O

et O
al. O
[3]. O
VQA B-DAT
real O
images O
dataset O
contains O
just O

has O
spurred O
significant O
progress O
in O
VQA B-DAT
domain, O
as O
discussed O
earlier, O
it O

answer) O
triplet O
(I,Q,A) O
in O
the O
VQA B-DAT
dataset, O
our O
goal O
is O
to O

are O
seman- O
tically O
similar, O
a O
VQA B-DAT
model O
will O
have O
to O
understand O

questions O
from O
the O
original O
(unbalanced) O
VQA B-DAT
dataset O
[3] O
(top) O
and O
from O

all O
the O
questions O
in O
the O
VQA B-DAT
dataset. O
We O
believe O
that O
a O

and O
test O
splits O
of O
the O
VQA B-DAT
dataset. O
AMT O
workers O
picked O
“not O

question, O
image) O
pairs. O
Following O
original O
VQA B-DAT
dataset O
[3], O
we O
divide O
our O

We O
use O
the O
publicly O
released O
VQA B-DAT
evaluation O
script O
in O
our O
experiments O

for O
each O
question O
to O
compute O
VQA B-DAT
accura- O
cies. O
As O
described O
above O

to O
be O
consistent O
with O
the O
VQA B-DAT
dataset O
[3]. O
Note O
that O
while O

type O
in O
our O
new O
balanced O
VQA B-DAT
dataset O
with O
the O
original O
(un O

- O
balanced) O
VQA B-DAT
dataset O
[3]. O
We O
notice O
several O

balanced O
dataset O
compared O
to O
unbalanced O
VQA B-DAT
dataset. O
“baseball” O
is O
now O
slightly O

more O
balanced O
than O
the O
original O
VQA B-DAT
dataset. O
The O
resultant O
impact O
of O

ing O
on O
performance O
of O
state-of-the-art O
VQA B-DAT
models O
is O
dis- O
cussed O
in O

4. O
Benchmarking O
Existing O
VQA B-DAT
Models O
Our O
first O
approach O
to O

training O
a O
VQA B-DAT
model O
that O
empha O

to O
re-train O
the O
existing O
state-of-art O
VQA B-DAT
models O
(with O
code O
publicly O
available O

9]) O
on O
our O
new O
balanced O
VQA B-DAT
dataset. O
Our O
hypothesis O
is O
that O

I) O
[24]: O
This O
was O
the O
VQA B-DAT
model O
introduced O
in O
[3] O
together O

This O
is O
a O
recent O
attention-based O
VQA B-DAT
model O
that O
‘co-attends’ O
to O
both O

real O
images O
track O
of O
the O
VQA B-DAT
Challenge O
2016. O
This O
model O
uses O

utilize O
any O
visual O
information. O
Comparing O
VQA B-DAT
models O
to O
language- O
only O
ablations O

quantifies O
to O
what O
extent O
VQA B-DAT
models O
have O
succeeded O
in O
leveraging O

Table O
1: O
Performance O
of O
VQA B-DAT
models O
when O
trained/tested O
on O
unbalanced/balanced O

VQA B-DAT
datasets. O
UB O
stands O
for O
training O

see O
that O
the O
current O
state-of-art O
VQA B-DAT
models O
trained O
on O
the O
original O

unbalanced) O
VQA B-DAT
dataset O
perform O
signifi- O
cantly O
worse O

evaluating O
on O
the O
original O
unbalanced O
VQA B-DAT
dataset O
(i.e., O
comparing O
UU O
to O

in O
accuracy O
suggests O
that O
current O
VQA B-DAT
models O
are O
data O
starved, O
and O

would O
benefit O
from O
even O
larger O
VQA B-DAT
datasets O

the O
language-bias O
in O
the O
original O
VQA B-DAT
dataset, O
and O
its O
successful O
alleviation O

same O
question. O
To O
be O
successful, O
VQA B-DAT
models O
need O
to O
understand O
the O

an- O
alyze O
the O
performance O
of O
VQA B-DAT
models O
in O
unique O
ways. O
Given O

the O
prediction O
of O
a O
VQA B-DAT
model, O
we O
can O
count O
the O

training O
on O
balanced O
dataset, O
this O
VQA B-DAT
model O
has O
learned O
to O
tell O

room O
for O
improvement O
remains. O
The O
VQA B-DAT
model O
still O
can O
not O
tell O

To O
benchmark O
models O
on O
VQA B-DAT
v2.0 O
dataset, O
we O
also O
train O

these O
models O
on O
VQA B-DAT
v2.0 O
train+val O
and O
report O
re O

- O
sults O
on O
VQA B-DAT
v2.0 O
test-standard O
in O
Table O
2 O

Papers O
report- O
ing O
results O
on O
VQA B-DAT
v2.0 O
dataset O
are O
suggested O
to O

Table O
2: O
Performance O
of O
VQA B-DAT
models O
when O
trained O
on O
VQA O

v2.0 O
train+val O
and O
tested O
on O
VQA B-DAT
v2.0 O
test-standard O
dataset O

models O
when O
trained/tested O
on O
unbalanced/balanced O
VQA B-DAT
datasets. O
UB O
stands O
for O
training O

HieCoAtt). O
This O
suggests O
that O
these O
VQA B-DAT
mod- O
els O
are O
really O
exploiting O

that O
for O
both O
the O
state-of-art O
VQA B-DAT
mod- O
els, O
the O
largest O
source O

the O
results O
announced O
at O
the O
VQA B-DAT
Real O
Open O
Ended O
Challenge O
20162 O

priors O
present O
in O
the O
unbalanced O
VQA B-DAT
dataset O
(particularly O
in O
the O
“yes/no O

similar O
accuracies O
for O
all O
state-of-art O
VQA B-DAT
models, O
rendering O
vastly O
different O
models O

these O
answer-types). O
Benchmarking O
these O
different O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

color O
is O
the O
fire-hydrant?” O
a O
VQA B-DAT
model O
may O
be O
perceived O
as O

step, O
similar O
to O
a O
conventional O
VQA B-DAT
model, O
it O
takes O
in O
an O

similar O
to O
a O
conven- O
tional O
VQA B-DAT
model), O
and O
second, O
it O
explains O

VQA B-DAT
head O
in O
our O
model, O
and O

the O
most O
likely O
counter- O
example. O
VQA B-DAT
Model: O
Using O
a O
VQA O
model’s O

Random O
Distance O
VQA B-DAT
[3] O
Ours O

baseline, O
as O
well O
as O
the O
VQA B-DAT
[3] O
model. O
Interestingly, O
the O
strongest O

terface O
to O
‘balance’ O
the O
popular O
VQA B-DAT
dataset O
[3] O
by O
col- O
lecting O

bal- O
anced O
than O
the O
original O
VQA B-DAT
dataset O
by O
construction, O
but O
also O

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2.0 O

a O
number O
of O
(near) O
state-of-art O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

L. O
Zitnick, O
and O
D. O
Parikh. O
VQA B-DAT

D. O
Parikh. O
Question O
Relevance O
in O
VQA B-DAT

Image O
Understanding O
in O
Visual O
Question O
Answering B-DAT

the O
task O
of O
Visual O
Question O
Answering B-DAT
(VQA) O
and O
make O
vision O
(the O

iteration O
of O
the O
Visual O
Question O
Answering B-DAT
Dataset O
and O
Challenge O
(VQA O
v2.0 O

2. O
Related O
Work O
Visual O
Question O
Answering B-DAT

2. O
Answering B-DAT
head: O
The O
second O
component O
is O

the O
task O
of O
Visual O
Question O
Answering B-DAT
and O
elevate O
the O
role O
of O

iteration O
of O
the O
Visual O
Question O
Answering B-DAT
Dataset O
and O
Challenge O
(VQA O
v2.0 O

of O
Visual O
Question O
Answering B-DAT
Models. O
In O
EMNLP, O
2016. O
1 O

D. O
Parikh. O
VQA: O
Visual O
Question O
Answering B-DAT

Pooling O
for O
Vi- O
sual O
Question O
Answering B-DAT
and O
Visual O
Grounding. O
In O
EMNLP O

der O
Maaten. O
Revisiting O
Visual O
Question O
Answering B-DAT
Baselines. O
In O
ECCV, O
2016. O
1 O

and O
C. O
Kanan. O
Visual O
Question O
Answering B-DAT

and O
normalized O
CNN O
Visual O
Question O
Answering B-DAT
model. O
https://github.com/VT-vision-lab/ O
VQA_LSTM_CNN, O
2015. O
2 O

A O
Multi-World O
Approach O
to O
Question O
Answering B-DAT
about O
Real-World O
Scenes O
based O
on O

1 O
Million O
Full-Sentences O
Visual O
Question O
Answering B-DAT
(FSVQA). O
arXiv O
preprint O
arXiv:1609.06657, O
2016 O

Answering B-DAT

Spatial O
Attention O
for O
Visual O
Question O
Answering B-DAT

tention O
Networks O
for O
Image O
Question O
Answering B-DAT

Yin O
and O
Yang: O
Balancing O
and O
Answering B-DAT
Binary O
Visual O
Questions. O
In O
CVPR O

Simple O
Baseline O
for O
Visual O
Question O
Answering B-DAT

dataset O
[3] O
by O
collecting O
complementary O
images B-DAT
such O
that O
every O
question O
in O

rather O
a O
pair O
of O
similar O
images B-DAT
that O
result O
in O
two O
different O

protocol O
for O
identifying O
com- O
plementary O
images B-DAT
enables O
us O
to O
develop O
a O

tower O
in O
the O
picture?” O
on O
images B-DAT
actually O
containing O
clock O
tow- O
ers O

the O
goal O
of O
under- O
standing O
images B-DAT
correctly O
when O
they O
are O
only O

and O
A′) O
for O
two O
different O
images B-DAT
(I O
and O
I O
′ O
respectively O

subtle O
differences O
between O
the O
two O
images B-DAT
to O
pre- O
dict O
the O
answers O

to O
both O
the O
images B-DAT
correctly O

associated O
answers O
on O
the O
∼200k O
images B-DAT
from O
COCO O
[23]. O
We O
believe O

not O
only O
answers O
questions O
about O
images, B-DAT
but O
also O
‘explains’ O
its O
answer O

hard O
negatives” O
i.e., O
examples O
of O
images B-DAT
that O
it O
believes O
are O
similar O

dataset O
[3] O
by O
collecting O
complementary O
images B-DAT
such O
that O
almost O
every O
question O

rather O
a O
pair O
of O
similar O
images B-DAT
that O
result O
in O
two O
different O

addition O
to O
answering O
questions O
about O
images, B-DAT
also O
provides O
a O
counter- O
example O

based O
explanation O
– O
it O
retrieves O
images B-DAT
that O
it O
be- O
lieves O
are O

Each O
question O
has O
two O
similar O
images B-DAT
with O
different O
answers O
to O
the O

create O
two O
similar O
captions O
for O
images, B-DAT
while O
we O
create O
a O
novel O

interface O
to O
collect O
two O
similar O
images B-DAT
for O
questions O
in O
VQA O

AMT) O
interface O
to O
collect O
complementary O
images B-DAT

simply O
not O
possible O
in O
real O
images B-DAT

inter- O
face, O
application O
to O
real O
images, B-DAT
extension O
to O
all O
questions O
(not O

or O
spatial O
maps O
overlaid O
on O
images B-DAT
to O
highlight O
the O
regions O
that O

et O
al. O
[3]. O
VQA O
real O
images B-DAT
dataset O
contains O
just O
over O
204K O

images B-DAT
from O
COCO O
[23], O
614K O
free-form O

to O
collect O
such O
com- O
plementary O
images B-DAT
on O
Amazon O
Mechanical O
Turk O
(AMT O

workers O
are O
shown O
24 O
nearest-neighbor O
images B-DAT
of O
I O
, O
the O
question O

from O
the O
list O
of O
24 O
images B-DAT
for O
whichQ O
“makes O
sense” O
and O

After O
the O
complementary O
images B-DAT
are O
collected, O
we O
con- O
duct O

collect O
answers O
on O
these O
new O
images B-DAT

results O
in O
pairs O
of O
complementary O
images B-DAT
I O
and O
I O
′ O
that O

right O
an- O
swer O
to O
both O
images B-DAT

. O
Example O
complementary O
images B-DAT
are O
shown O
in O
Fig. O
1 O

for O
any O
of O
the O
24 O
images B-DAT
(e.g. O
the O
question O
is O
‘what O

and O
none O
of O
the O
neighboring O
images B-DAT
contain O
a O
woman), O
or O
(2 O

applicable O
to O
some O
neighbor- O
ing O
images, B-DAT
but O
the O
answer O
to O
the O

thus O
the O
nearest O
neigh- O
bor O
images, B-DAT
while O
globally O
similar, O
do O
not O

many O
more O
than O
24 O
neighboring O
images B-DAT
could O
pos- O
sibly O
reduce O
this O

We O
collected O
complementary O
images B-DAT
and O
the O
corre- O
sponding O
new O

we O
collected O
approximately O
195K O
complementary O
images B-DAT
for O
train, O
93K O
complementary O
images O

for O
val, O
and O
191K O
complementary O
images B-DAT
for O
test O
set. O
In O
addition O

winning O
entry O
on O
the O
real O
images B-DAT
track O
of O
the O
VQA O
Challenge O

has O
different O
answers O
on O
different O
images B-DAT

can O
extract O
detailed O
information O
from O
images B-DAT
and O
leverage O
this O
information O
to O

free-form O
natural O
language O
questions O
about O
images B-DAT
accurately. O
As O
ex- O
pected O
from O

since O
there O
are O
pairs O
of O
images B-DAT
very O
similar O
to O
each O
other O

the O
subtle O
differences O
in O
these O
images B-DAT

of O
questions O
where O
both O
complementary O
images B-DAT
(I O
,I O
′) O
received O
correct O

difference O
between O
two O
otherwise O
similar O
images B-DAT

to O
result O
in O
the O
two O
images B-DAT
having O
different O
ground O
truth O
an O

but O
also O
pro- O
vides O
example O
images B-DAT
that O
are O
similar O
to O
the O

picks O
one O
ofK O
nearest O
neighbor O
images B-DAT
of O
I O
, O
INN O

model O
is O
learning O
representations O
of O
images B-DAT
and O
questions. O
It O
is O
a O

component. O
A O
total O
of O
25 O
images B-DAT
– O
the O
original O
image O
I O

and O
24 O
candidate O
images B-DAT
{I1, O
I2, O
..., O
I24} O
are O

inner-product O
values O
forK O
candi- O
date O
images B-DAT
are O
then O
passed O
through O
a O

1, O
2, O
...,K}. O
TheK O
candidate O
images B-DAT
{I1, O
I2, O
..., O
IK} O
are O

higher O
than O
all O
other O
candidate O
images B-DAT
by O
a O
de- O
sired O
margin O

counter-example. O
Distance: O
Sorting O
the O
candidate O
images B-DAT
in O
increasing O
order O
of O
their O

answer O
to O
sort O
the O
candidate O
images B-DAT
in O
as- O
cending O
order O
of O

extract O
meaning- O
ful O
details O
from O
images B-DAT
still O
remain O
elusive O

3] O
by O
col- O
lecting O
‘complementary’ O
images B-DAT

dataset, O
we O
have O
two O
complementary O
images B-DAT
that O
look O
sim- O
ilar, O
but O

Finally, O
our O
framework O
around O
complementary O
images B-DAT
enables O
us O
to O
develop O
a O

produces O
a O
list O
of O
similar O
images B-DAT
that O
it O
considers O
‘counter-examples’, O
i.e O

approach O
to O
answering O
questions O
about O
images B-DAT

Hierarchical O
Image O
Database. O
In O
CVPR, O
2009 B-DAT

of O
Image O
Understanding O
in O
Visual O
Question B-DAT
Answering O

for O
the O
task O
of O
Visual O
Question B-DAT
Answering O
(VQA) O
and O
make O
vision O

2nd O
iteration O
of O
the O
Visual O
Question B-DAT
Answering O
Dataset O
and O
Challenge O
(VQA O

2. O
Related O
Work O
Visual O
Question B-DAT
Answering. O
A O
number O
of O
recent O

Deeper O
LSTM O
Question B-DAT
+ O
norm O
Image O
(d-LSTM+n- O
I O

similar O
architecture O
as O
Deeper O
LSTM O
Question B-DAT
+ O
norm O
Image O
[24] O
except O

for O
the O
task O
of O
Visual O
Question B-DAT
Answering O
and O
elevate O
the O
role O

2nd O
iteration O
of O
the O
Visual O
Question B-DAT
Answering O
Dataset O
and O
Challenge O
(VQA O

of O
Visual O
Question B-DAT
Answering O
Models. O
In O
EMNLP, O
2016 O

and O
D. O
Parikh. O
VQA: O
Visual O
Question B-DAT
Answering. O
In O
ICCV, O
2015. O
1 O

Bilinear O
Pooling O
for O
Vi- O
sual O
Question B-DAT
Answering O
and O
Visual O
Grounding. O
In O

Transparent O
AI O
Systems: O
Interpreting O
Visual O
Question B-DAT
An- O
swering O
Models. O
In O
ICML O

van O
der O
Maaten. O
Revisiting O
Visual O
Question B-DAT
Answering O
Baselines. O
In O
ECCV, O
2016 O

Kafle O
and O
C. O
Kanan. O
Visual O
Question B-DAT
Answering: O
Datasets, O
Algorithms, O
and O
Future O

LSTM O
and O
normalized O
CNN O
Visual O
Question B-DAT
Answering O
model. O
https://github.com/VT-vision-lab/ O
VQA_LSTM_CNN, O
2015 O

Batra, O
and O
D. O
Parikh. O
Hierarchical O
Question B-DAT

-Image O
Co-Attention O
for O
Visual O
Question B-DAT
Answer- O
ing. O
In O
NIPS, O
2016 O

Fritz. O
A O
Multi-World O
Approach O
to O
Question B-DAT
Answering O
about O
Real-World O
Scenes O
based O

D. O
Batra, O
and O
D. O
Parikh. O
Question B-DAT
Relevance O
in O
VQA: O
Identifying O
Non-Visual O

Gray: O
1 O
Million O
Full-Sentences O
Visual O
Question B-DAT
Answering O
(FSVQA). O
arXiv O
preprint O
arXiv:1609.06657 O

Understanding O
Stories O
in O
Movies O
through O
Question B-DAT

Attend O
and O
Answer: O
Explor- O
ing O
Question B-DAT

-Guided O
Spatial O
Attention O
for O
Visual O
Question B-DAT
Answering. O
In O
ECCV, O
2016. O
2 O

At- O
tention O
Networks O
for O
Image O
Question B-DAT
Answering. O
In O
CVPR, O
2016. O
2 O

Madlibs: O
Fill-in-the-blank O
Description O
Generation O
and O
Question B-DAT
An- O
swering. O
In O
ICCV, O
2015 O

Fergus. O
Simple O
Baseline O
for O
Visual O
Question B-DAT
Answering. O
CoRR, O
abs/1512.02167, O
2015. O
1 O

Role O
of O
Image O
Understanding O
in O
Visual B-DAT
Question O
Answering O

priors O
for O
the O
task O
of O
Visual B-DAT
Question O
Answering O
(VQA) O
and O
make O

the O
2nd O
iteration O
of O
the O
Visual B-DAT
Question O
Answering O
Dataset O
and O
Challenge O

2. O
Related O
Work O
Visual B-DAT
Question O
Answering. O
A O
number O
of O

priors O
for O
the O
task O
of O
Visual B-DAT
Question O
Answering O
and O
elevate O
the O

the O
2nd O
iteration O
of O
the O
Visual B-DAT
Question O
Answering O
Dataset O
and O
Challenge O

of O
Visual B-DAT
Question O
Answering O
Models. O
In O
EMNLP O

Zitnick, O
and O
D. O
Parikh. O
VQA: O
Visual B-DAT
Question O
Answering. O
In O
ICCV, O
2015 O

Re- O
current O
Convolutional O
Networks O
for O
Visual B-DAT
Recognition O
and O
Description. O
In O
CVPR O

G. O
Zweig. O
From O
Captions O
to O
Visual B-DAT
Concepts O
and O
Back. O
In O
CVPR O

Vi- O
sual O
Question O
Answering O
and O
Visual B-DAT
Grounding. O
In O
EMNLP, O
2016. O
2 O

Towards O
Transparent O
AI O
Systems: O
Interpreting O
Visual B-DAT
Question O
An- O
swering O
Models. O
In O

L. O
van O
der O
Maaten. O
Revisiting O
Visual B-DAT
Question O
Answering O
Baselines. O
In O
ECCV O

K. O
Kafle O
and O
C. O
Kanan. O
Visual B-DAT
Question O
Answering: O
Datasets, O
Algorithms, O
and O

Karpathy O
and O
L. O
Fei-Fei. O
Deep O
Visual B-DAT

Zhang. O
Multimodal O
Residual O
Learning O
for O
Visual B-DAT
QA. O
In O
NIPS, O
2016. O
2 O

and O
R. O
S. O
Zemel. O
Unifying O
Visual B-DAT

D. O
A. O
Shamma, O
et O
al. O
Visual B-DAT
genome: O
Connecting O
language O
and O
vision O

Deeper O
LSTM O
and O
normalized O
CNN O
Visual B-DAT
Question O
Answering O
model. O
https://github.com/VT-vision-lab/ O
VQA_LSTM_CNN O

Parikh. O
Hierarchical O
Question-Image O
Co-Attention O
for O
Visual B-DAT
Question O
Answer- O
ing. O
In O
NIPS O

Visual B-DAT
And O
False-Premise O
Questions. O
In O
EMNLP O

Why O
did O
you O
say O
that? O
Visual B-DAT
Explanations O
from O
Deep O
Networks O
via O

is O
Gray: O
1 O
Million O
Full-Sentences O
Visual B-DAT
Question O
Answering O
(FSVQA). O
arXiv O
preprint O

ing O
Question-Guided O
Spatial O
Attention O
for O
Visual B-DAT
Question O
Answering. O
In O
ECCV, O
2016 O

Berg, O
and O
T. O
L. O
Berg. O
Visual B-DAT
Madlibs: O
Fill-in-the-blank O
Description O
Generation O
and O

Yang: O
Balancing O
and O
Answering O
Binary O
Visual B-DAT
Questions. O
In O
CVPR, O
2016. O
1 O

R. O
Fergus. O
Simple O
Baseline O
for O
Visual B-DAT
Question O
Answering. O
CoRR, O
abs/1512.02167, O
2015 O

on O
the O
∼200k O
images O
from O
COCO B-DAT
[23]. O
We O
believe O
this O
balanced O

just O
over O
204K O
images O
from O
COCO B-DAT
[23], O
614K O
free-form O
natural O
lan O

may O
be O
no O
image O
in O
COCO B-DAT
where O
the O
answer O
to O
“is O

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT

Making O
the O
V O
in O
VQA B-DAT
Matter: O
Elevating O
the O
Role O
of O

task O
of O
Visual O
Question O
Answering O
(VQA) B-DAT
and O
make O
vision O
(the O
V O

in O
VQA) B-DAT
matter! O
Specifically, O
we O
balance O
the O

popular O
VQA B-DAT
dataset O
[3] O
by O
collecting O
complementary O

balanced O
than O
the O
origi- O
nal O
VQA B-DAT
dataset O
and O
has O
approximately O
twice O

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2.0 O

benchmark O
a O
number O
of O
state-of-art O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

1: O
Examples O
from O
our O
balanced O
VQA B-DAT
dataset O

28] O
and O
visual O
question O
answering O
(VQA) B-DAT
[3, O
26, O
27, O
10, O
31 O

1]. O
For O
instance, O
in O
the O
VQA B-DAT
[3] O
dataset, O
the O
most O
com O

visual O
priming O
bias’ O
in O
the O
VQA B-DAT
dataset O
– O
specifi- O
cally, O
subjects O

in O
the O
VQA B-DAT
dataset O
starting O
with O
the O
n-gram O

associated O
image O
results O
in O
a O
VQA B-DAT
accuracy O
of O
87 O

role O
of O
image O
understanding O
in O
VQA B-DAT

goal, O
we O
collect O
a O
balanced O
VQA B-DAT
dataset O
with O
significantly O
reduced O
language O

ically, O
we O
create O
a O
balanced O
VQA B-DAT
dataset O
in O
the O
following O
way O

answer) O
triplet O
(I,Q,A) O
from O
the O
VQA B-DAT
dataset, O
we O
ask O
a O
human O

this O
balanced O
dataset O
will O
force O
VQA B-DAT
models O
to O
focus O
on O
visual O

exploiting O
language O
priors, O
en- O
abling O
VQA B-DAT
evaluation O
protocols O
to O
more O
accurately O

Our O
balanced O
VQA B-DAT
dataset O
is O
also O
particularly O
difficult O

of O
VGGNet O
[37] O
features. O
Therefore, O
VQA B-DAT
models O
will O
need O
to O
under O

double O
the O
size O
of O
the O
VQA B-DAT
[3] O
dataset O
– O
with O
approximately O

23]. O
We O
believe O
this O
balanced O
VQA B-DAT
dataset O
is O
a O
better O
dataset O

to O
benchmark O
VQA B-DAT
approaches, O
and O
is O
publicly O
available O

will O
allow O
users O
of O
the O
VQA B-DAT
model O
to O
establish O
greater O
trust O

1) O
We O
balance O
the O
existing O
VQA B-DAT
dataset O
[3] O
by O
collecting O
complementary O

result O
is O
a O
more O
balanced O
VQA B-DAT
dataset, O
which O
is O
also O
approximately O

the O
size O
of O
the O
original O
VQA B-DAT
dataset. O
(2) O
We O
evaluate O
state-of-art O

VQA B-DAT
models O
(with O
publicly O
available O
code O

trained O
on O
the O
existing O
‘unbalanced’ O
VQA B-DAT
dataset O
perform O
poorly O
on O
our O

language O
priors O
in O
the O
existing O
VQA B-DAT
dataset O
to O
achieve O
higher O
accuracy O

builds O
on O
top O
of O
the O
VQA B-DAT
dataset O
from O
Antol O
et O
al O

of O
the O
most O
widely O
used O
VQA B-DAT
datasets. O
We O
reduce O
the O
language O

twice O
the O
size O
of O
the O
VQA B-DAT
dataset. O
We O
benchmark O
one O
‘baseline O

’ O
VQA B-DAT
model O
[24], O
one O
attention-based O
VQA O

the O
winning O
model O
from O
the O
VQA B-DAT
Real O
Open O
Ended O
Chal- O
lenge O

2016 O
[9] O
on O
our O
balanced O
VQA B-DAT
dataset, O
and O
compare O
them O
to O

examples O
from O
our O
proposed O
balanced O
VQA B-DAT
dataset. O
Each O
question O
has O
two O

similar O
images O
for O
questions O
in O
VQA B-DAT

study O
this O
goal O
of O
balancing O
VQA B-DAT
in O
a O
fairly O
restricted O
setting O

from O
clipart O
(part O
of O
the O
VQA B-DAT
abstract O
scenes O
dataset O
[3]). O
Using O

binary O
ones), O
benchmarking O
of O
state-of-art O
VQA B-DAT
models O
on O
the O
balanced O
dataset O

, O
and O
finally O
the O
novel O
VQA B-DAT
model O
with O
counter-example O
based O
explanations O

build O
on O
top O
of O
the O
VQA B-DAT
dataset O
introduced O
by O
An- O
tol O

et O
al. O
[3]. O
VQA B-DAT
real O
images O
dataset O
contains O
just O

has O
spurred O
significant O
progress O
in O
VQA B-DAT
domain, O
as O
discussed O
earlier, O
it O

answer) O
triplet O
(I,Q,A) O
in O
the O
VQA B-DAT
dataset, O
our O
goal O
is O
to O

are O
seman- O
tically O
similar, O
a O
VQA B-DAT
model O
will O
have O
to O
understand O

questions O
from O
the O
original O
(unbalanced) O
VQA B-DAT
dataset O
[3] O
(top) O
and O
from O

all O
the O
questions O
in O
the O
VQA B-DAT
dataset. O
We O
believe O
that O
a O

and O
test O
splits O
of O
the O
VQA B-DAT
dataset. O
AMT O
workers O
picked O
“not O

question, O
image) O
pairs. O
Following O
original O
VQA B-DAT
dataset O
[3], O
we O
divide O
our O

We O
use O
the O
publicly O
released O
VQA B-DAT
evaluation O
script O
in O
our O
experiments O

for O
each O
question O
to O
compute O
VQA B-DAT
accura- O
cies. O
As O
described O
above O

to O
be O
consistent O
with O
the O
VQA B-DAT
dataset O
[3]. O
Note O
that O
while O

type O
in O
our O
new O
balanced O
VQA B-DAT
dataset O
with O
the O
original O
(un O

- O
balanced) O
VQA B-DAT
dataset O
[3]. O
We O
notice O
several O

balanced O
dataset O
compared O
to O
unbalanced O
VQA B-DAT
dataset. O
“baseball” O
is O
now O
slightly O

more O
balanced O
than O
the O
original O
VQA B-DAT
dataset. O
The O
resultant O
impact O
of O

ing O
on O
performance O
of O
state-of-the-art O
VQA B-DAT
models O
is O
dis- O
cussed O
in O

4. O
Benchmarking O
Existing O
VQA B-DAT
Models O
Our O
first O
approach O
to O

training O
a O
VQA B-DAT
model O
that O
empha O

to O
re-train O
the O
existing O
state-of-art O
VQA B-DAT
models O
(with O
code O
publicly O
available O

9]) O
on O
our O
new O
balanced O
VQA B-DAT
dataset. O
Our O
hypothesis O
is O
that O

I) O
[24]: O
This O
was O
the O
VQA B-DAT
model O
introduced O
in O
[3] O
together O

This O
is O
a O
recent O
attention-based O
VQA B-DAT
model O
that O
‘co-attends’ O
to O
both O

real O
images O
track O
of O
the O
VQA B-DAT
Challenge O
2016. O
This O
model O
uses O

utilize O
any O
visual O
information. O
Comparing O
VQA B-DAT
models O
to O
language- O
only O
ablations O

quantifies O
to O
what O
extent O
VQA B-DAT
models O
have O
succeeded O
in O
leveraging O

Table O
1: O
Performance O
of O
VQA B-DAT
models O
when O
trained/tested O
on O
unbalanced/balanced O

VQA B-DAT
datasets. O
UB O
stands O
for O
training O

see O
that O
the O
current O
state-of-art O
VQA B-DAT
models O
trained O
on O
the O
original O

unbalanced) O
VQA B-DAT
dataset O
perform O
signifi- O
cantly O
worse O

evaluating O
on O
the O
original O
unbalanced O
VQA B-DAT
dataset O
(i.e., O
comparing O
UU O
to O

in O
accuracy O
suggests O
that O
current O
VQA B-DAT
models O
are O
data O
starved, O
and O

would O
benefit O
from O
even O
larger O
VQA B-DAT
datasets O

the O
language-bias O
in O
the O
original O
VQA B-DAT
dataset, O
and O
its O
successful O
alleviation O

same O
question. O
To O
be O
successful, O
VQA B-DAT
models O
need O
to O
understand O
the O

an- O
alyze O
the O
performance O
of O
VQA B-DAT
models O
in O
unique O
ways. O
Given O

the O
prediction O
of O
a O
VQA B-DAT
model, O
we O
can O
count O
the O

training O
on O
balanced O
dataset, O
this O
VQA B-DAT
model O
has O
learned O
to O
tell O

room O
for O
improvement O
remains. O
The O
VQA B-DAT
model O
still O
can O
not O
tell O

To O
benchmark O
models O
on O
VQA B-DAT
v2.0 O
dataset, O
we O
also O
train O

these O
models O
on O
VQA B-DAT
v2.0 O
train+val O
and O
report O
re O

- O
sults O
on O
VQA B-DAT
v2.0 O
test-standard O
in O
Table O
2 O

Papers O
report- O
ing O
results O
on O
VQA B-DAT
v2.0 O
dataset O
are O
suggested O
to O

Table O
2: O
Performance O
of O
VQA B-DAT
models O
when O
trained O
on O
VQA O

v2.0 O
train+val O
and O
tested O
on O
VQA B-DAT
v2.0 O
test-standard O
dataset O

models O
when O
trained/tested O
on O
unbalanced/balanced O
VQA B-DAT
datasets. O
UB O
stands O
for O
training O

HieCoAtt). O
This O
suggests O
that O
these O
VQA B-DAT
mod- O
els O
are O
really O
exploiting O

that O
for O
both O
the O
state-of-art O
VQA B-DAT
mod- O
els, O
the O
largest O
source O

the O
results O
announced O
at O
the O
VQA B-DAT
Real O
Open O
Ended O
Challenge O
20162 O

priors O
present O
in O
the O
unbalanced O
VQA B-DAT
dataset O
(particularly O
in O
the O
“yes/no O

similar O
accuracies O
for O
all O
state-of-art O
VQA B-DAT
models, O
rendering O
vastly O
different O
models O

these O
answer-types). O
Benchmarking O
these O
different O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

color O
is O
the O
fire-hydrant?” O
a O
VQA B-DAT
model O
may O
be O
perceived O
as O

step, O
similar O
to O
a O
conventional O
VQA B-DAT
model, O
it O
takes O
in O
an O

similar O
to O
a O
conven- O
tional O
VQA B-DAT
model), O
and O
second, O
it O
explains O

VQA B-DAT
head O
in O
our O
model, O
and O

the O
most O
likely O
counter- O
example. O
VQA B-DAT
Model: O
Using O
a O
VQA O
model’s O

Random O
Distance O
VQA B-DAT
[3] O
Ours O

baseline, O
as O
well O
as O
the O
VQA B-DAT
[3] O
model. O
Interestingly, O
the O
strongest O

terface O
to O
‘balance’ O
the O
popular O
VQA B-DAT
dataset O
[3] O
by O
col- O
lecting O

bal- O
anced O
than O
the O
original O
VQA B-DAT
dataset O
by O
construction, O
but O
also O

Question O
Answering O
Dataset O
and O
Challenge O
(VQA B-DAT
v2.0 O

a O
number O
of O
(near) O
state-of-art O
VQA B-DAT
models O
on O
our O
balanced O
dataset O

L. O
Zitnick, O
and O
D. O
Parikh. O
VQA B-DAT

D. O
Parikh. O
Question O
Relevance O
in O
VQA B-DAT

Modeling B-DAT
Relationships O
in O
Referential O
Expressions O

with B-DAT
Compositional O
Modular O
Networks O
Ronghang O
Hu1 O
Marcus O
Rohrbach1 O
Jacob O

Andreas1 B-DAT
Trevor O
Darrell1 O
Kate O
Saenko2 O

1University B-DAT
of O
California, O
Berkeley O
2Boston O

University B-DAT

ronghang,rohrbach,jda,trevor}@eecs.berkeley.edu, B-DAT
saenko@bu.edu O
Abstract O

People B-DAT
often O
refer O
to O
entities O

in B-DAT
an O
image O
in O
terms O

of B-DAT
their O
relationships O
with O
other O

entities. B-DAT
For O
example, O
the O
black O

cat B-DAT
sitting O
under O
the O
table O

refers B-DAT
to O
both O
a O
black O

cat B-DAT
entity O
and O
its O
relationship O

with B-DAT
another O
table O
entity. O
Understanding O

these B-DAT
relationships O
is O
essential O
for O

interpreting B-DAT
and O
ground- O
ing O
such O

natural B-DAT
language O
expressions. O
Most O
prior O

work B-DAT
focuses O
on O
either O
grounding O

entire B-DAT
referential O
expressions O
holistically O
to O

one B-DAT
region, O
or O
localizing O
relationships O

based B-DAT
on O
a O
fixed O
set O

of B-DAT
categories. O
In O
this O
paper O

we B-DAT
instead O
present O
a O
modular O

deep B-DAT
architecture O
capable O
of O
analyzing O

referen- B-DAT
tial O
expressions O
into O
their O

component B-DAT
parts, O
identifying O
en- O
tities O

and B-DAT
relationships O
mentioned O
in O
the O

input B-DAT
expression O
and O
grounding O
them O

all B-DAT
in O
the O
scene. O
We O

call B-DAT
this O
approach O
Compositional O
Modular O

Networks B-DAT
(CMNs): O
a O
novel O
archi- O

tecture B-DAT
that O
learns O
linguistic O
analysis O

and B-DAT
visual O
inference O
end-to-end. O
Our O

approach B-DAT
is O
built O
around O
two O

types B-DAT
of O
neu- O
ral O
modules O

that B-DAT
inspect O
local O
regions O
and O

pairwise B-DAT
interac- O
tions O
between O
regions. O

We B-DAT
evaluate O
CMNs O
on O
multiple O

ref- B-DAT
erential O
expression O
datasets, O
outperforming O

state-of-the-art B-DAT
approaches O
on O
all O
tasks. O
1. O
Introduction O

Great B-DAT
progress O
has O
been O
made O

on B-DAT
object O
detection, O
the O
task O

of B-DAT
localizing O
visual O
entities O
belonging O

to B-DAT
a O
pre-defined O
set O
of O

categories B-DAT
[8, O
24, O
23, O
6, O
17 O

]. B-DAT
But O
the O
more O
general O

and B-DAT
challenging O
task O
of O
localizing O

entities B-DAT
based O
on O
arbi- O
trary O

natural B-DAT
language O
expressions O
remains O
far O

from B-DAT
solved. O
This O
task, O
sometimes O

known B-DAT
as O
grounding O
or O
referential O

ex- B-DAT
pression O
comprehension, O
has O
been O

explored B-DAT
by O
recent O
work O
in O

both B-DAT
computer O
vision O
and O
natural O

language B-DAT
processing O
[20, O
11, O
25]. O

Given B-DAT
an O
image O
and O
a O

natural B-DAT
language O
ex- O
pression O
referring O

to B-DAT
a O
visual O
entity, O
such O

as B-DAT
the O
young O
man O
wearing O

green B-DAT
shirt O
and O
riding O
a O

black B-DAT
bicycle, O
these O
ap- O
proaches O

localize B-DAT
the O
image O
region O
corresponding O

to B-DAT
the O
en- O
tity O
that O

the B-DAT
expression O
refers O
to O
with O

a B-DAT
bounding O
box. O
Referential O
expressions O
often O
describe O
relationships O

be B-DAT

- B-DAT
output O
top O
region O
pair O

final B-DAT
scores O
on O
region O
pairs O
input O
image O

expression B-DAT
parsing O
with O
attention O
subj: O

the B-DAT
woman O
holding O
a O
grey O

rel: B-DAT
the O
woman O
holding O
a O

grey B-DAT
umbrella O
obj: O
the O
woman O
holding O
a O

grey B-DAT
umbrella O

input B-DAT
expression O
the O
woman O
holding O

a B-DAT
grey O
umbrella O
qsubj O

candidate B-DAT

localization B-DAT
module O

relationship B-DAT
module O

subject B-DAT
scores O

unary) B-DAT
qrel O
qobj O

localization B-DAT
module O

relationship B-DAT
scores O

pairwise) B-DAT
object O

scores B-DAT
(unary O

) B-DAT
region O
pairs O

Figure B-DAT
1. O
Given O
an O
image O

and B-DAT
an O
expression, O
we O
learn O

to B-DAT
parse O
the O
expression O
into O

vector B-DAT
representation O
of O
subject O
qsubj O
, O
relationship O
qrel O
and O
object O
qobj O

with B-DAT
attention, O
and O
align O
these O

textual B-DAT
compo- O
nents O
to O
image O

regions B-DAT
with O
two O
types O
of O

modules. B-DAT
The O
localization O
module O
outputs O

scores B-DAT
over O
each O
individual O
region O

while B-DAT
the O
rela- O
tionship O
module O

produces B-DAT
scores O
over O
region O
pairs O

. B-DAT
These O
outputs O
are O
integrated O

into B-DAT
final O
scores O
over O
region O

pairs, B-DAT
producing O
the O
top O
region O

pair B-DAT
as O
grounding O
result. O
(Best O

viewed B-DAT
in O
color.) O
tween O
multiple O
entities O
in O
an O

image. B-DAT
In O
Figure O
1, O
for O

ex- B-DAT
ample, O
the O
expression O
the O

woman B-DAT
holding O
a O
grey O
umbrella O

describes B-DAT
a O
woman O
entity O
that O

participates B-DAT
in O
a O
holding O
re O

- B-DAT
lationship O
with O
a O
grey O

umbrella B-DAT
entity. O
Because O
there O
are O

multiple B-DAT
women O
in O
the O
image, O

resolving B-DAT
this O
referential O
ex- O
pression O

requires B-DAT
both O
finding O
a O
bounding O

box B-DAT
that O
contains O
a O
person, O

and B-DAT
ensuring O
that O
this O
bounding O

box B-DAT
relates O
in O
the O
ar O
X O

iv B-DAT
:1 O
61 O
1 O

. B-DAT
09 O
97 O

8v B-DAT
1 O

cs B-DAT
.C O
V O

3 B-DAT
0 O

N B-DAT
ov O

2 B-DAT
01 O
6 O

right B-DAT
way O
to O
other O
objects O

in B-DAT
the O
scene. O
Previous O
work O

on B-DAT
grounding O
referential O
expressions O
either O
(1 O

) B-DAT
treats O
referen- O
tial O
expressions O

holistically, B-DAT
thus O
failing O
to O
model O

explicit B-DAT
correspondence O
between O
textual O
components O

and B-DAT
visual O
en- O
tities O
in O

the B-DAT
image O
[20, O
11, O
25, O
30, O
21], O
or O
else O
(2) O
relies O

on B-DAT
a O
fixed O
set O
of O

entity B-DAT
and O
relationship O
categories O
defined O

a B-DAT
priori O
[18 O

]. B-DAT
In O
this O
paper, O
we O
present O

a B-DAT
joint O
approach O
that O
explic O

- B-DAT
itly O
models O
the O
compositional O

linguistic B-DAT
structure O
of O
referen- O
tial O

expressions B-DAT
and O
their O
groundings, O
but O

which B-DAT
nonetheless O
supports O
interpretation O
of O

arbitrary B-DAT
language. O
We O
focus O
on O

referential B-DAT
expressions O
involving O
inter-object O
relationships O

that B-DAT
can O
be O
represented O
as O

a B-DAT
subject O
entity, O
a O
relationship O

and B-DAT
an O
object O
entity. O
We O

propose B-DAT
Compositional O
Modular O
Net- O
works O
( O

CMNs), B-DAT
an O
end-to-end O
trained O
model O

that B-DAT
learns O
lan- O
guage O
representation O

and B-DAT
image O
region O
localization O
jointly O

as B-DAT
shown O
in O
Figure O
1. O

Our B-DAT
model O
differentiably O
parses O
the O

referential B-DAT
expression O
into O
a O
subject, O

relationship B-DAT
and O
ob- O
ject O
with O

three B-DAT
soft O
attention O
maps, O
and O

aligns B-DAT
the O
extracted O
textual O
representations O

with B-DAT
image O
regions O
using O
a O

modu- B-DAT
lar O
neural O
architecture. O
There O

are B-DAT
two O
types O
of O
modules O

in B-DAT
our O
model, O
one O
used O

for B-DAT
localizing O
specific O
textual O
compo- O

nents B-DAT
by O
outputting O
unary O
scores O

over B-DAT
regions O
for O
that O
com- O

ponent, B-DAT
and O
one O
for O
determining O

the B-DAT
relationship O
between O
two O
pairs O

of B-DAT
bounding O
boxes O
by O
outputting O

pairwise B-DAT
scores O
over O
region-region O
pairs. O

We B-DAT
evaluate O
our O
model O
on O

mul- B-DAT
tiple O
datasets O
containing O
referential O

expressions, B-DAT
and O
show O
that O
our O

model B-DAT
outperforms O
both O
natural O
baselines O

and B-DAT
pre- O
vious O
work. O
2. O
Related O
work O
Grounding O
referential O

expressions. B-DAT
The O
problem O
of O

grounding B-DAT
referential O
expressions O
can O
be O

naturally B-DAT
formu- O
lated O
as O
a O

retrieval B-DAT
problem O
over O
image O

regions B-DAT
[20, O
11, O
25, O
7, O
30, O
21]. O
First, O
a O
set O
of O

candidate B-DAT
regions O
are O
extracted O
(e.g O

. B-DAT
via O
object O
proposal O
methods O

like B-DAT
[28, O
4, O
13, O
33]). O

Next, B-DAT
each O
candidate O
region O
is O

scored B-DAT
by O
a O
model O
with O

respect B-DAT
to O
the O
query O
expression, O

returning B-DAT
the O
highest O
scoring O
candi- O

date B-DAT
as O
the O
grounding O
result. O

In B-DAT
[20, O
11], O
each O
region O

is B-DAT
scored O
based O
on O
its O

local B-DAT
visual O
features O
and O
some O

global B-DAT
contextual O
features O
from O
the O

whole B-DAT
image. O
However, O
local O
visual O

features B-DAT
and O
global O
contextual O
from O

the B-DAT
whole O
image O
are O
often O

insufficient B-DAT
to O
determine O
whether O
a O

region B-DAT
matches O
an O
expression, O
as O

relationships B-DAT
with O
other O
regions O
in O

the B-DAT
im- O
age O
must O
also O

be B-DAT
considered. O
Two O
recent O

methods B-DAT
[30, O
21] O
go O
beyond O

local B-DAT
visual O
features O
in O
a O

single B-DAT
region, O
and O
con- O
sider O

multiple B-DAT
regions O
at O
the O
same O

time. B-DAT
[30] O
adds O
contex- O
tual O

feature B-DAT
extracted O
from O
other O
regions O

in B-DAT
the O
image, O
and O
[21] O

proposes B-DAT
a O
model O
that O
grounds O

a B-DAT
referential O
expression O
into O
a O

pair B-DAT
of O
regions. O
All O
these O

methods B-DAT
represent O
language O
holistically O
using O

a B-DAT
recurrent O
neural O
network: O
either O

gener- B-DAT
atively, O
by O
predicting O
a O

distribution B-DAT
over O
referential O
expres- O

sions B-DAT
[20, O
11, O
30, O
21], O

or B-DAT
discriminatively, O
by O
encoding O
ex- O
pressions O
into O
a O
vector O
representation O

[25, B-DAT
7]. O
This O
makes O
it O

difficult B-DAT
to O
learn O
explicit O
correspondences O

between B-DAT
the O
com- O
ponents O
in O

the B-DAT
textual O
expression O
and O
entities O

in B-DAT
the O
image. O
In O
this O

work, B-DAT
we O
learn O
to O
parse O

the B-DAT
language O
expression O
into O
textual O

components B-DAT
in O
instead O
of O
treating O

it B-DAT
as O
a O
whole, O
and O

align B-DAT
these O
components O
with O
image O

regions B-DAT
end-to-end O

. B-DAT
Handling O
inter-object O
relationships. O
Recently O
work O

by B-DAT
[18] O
trains O
detectors O
based O

on B-DAT
RCNN O
[8] O
and O
uses O

a B-DAT
linguis- O
tic O
prior O
to O

detect B-DAT
visual O
relationships. O
However, O
this O

work B-DAT
relies O
on O
fixed, O
predefined O

categories B-DAT
for O
subjects, O
relations, O
and O

objects, B-DAT
treating O
entities O
like O
“bicycle O

” B-DAT
and O
relationships O
like O

and B-DAT
“riding” O
as O
discrete O
classes. O

Instead B-DAT
of O
building O
upon O
a O

fixed B-DAT
inventory O
of O
classes, O
our O

model B-DAT
handles O
re- O
lationships O
specified O

by B-DAT
arbitrary O
natural O
language O
phrases, O

and B-DAT
jointly O
learns O
expression O
parsing O

and B-DAT
visual O
entity O
lo- O
calization. O

Although B-DAT
[15] O
also O
learns O
language O

parsing B-DAT
and O
perception, O
it O
is O

directly B-DAT
based O
on O
logic O
(λ-calculus) O

and B-DAT
re- O
quires O
additional O
classifiers O

trained B-DAT
for O
each O
predicate O
class. O
Compositional O
structure O
with O
modules. O
Neural O

Mod- B-DAT
ule O
Networks O
[3] O
address O

visual B-DAT
question O
answering O
by O
de O

- B-DAT
composing O
the O
questions O
into O

textual B-DAT
components O
and O
dy- O
namically O

assembling B-DAT
a O
specific O
network O
architecture O

for B-DAT
the O
question O
from O
a O

few B-DAT
network O
modules O
based O
on O

the B-DAT
textual O
components. O
However, O
this O

method B-DAT
relies O
on O
an O
external O

language B-DAT
parser O
for O
textual O
analysis O

instead B-DAT
of O
end-to-end O
learned O
language O

representation, B-DAT
and O
is O
not O
directly O

appli- B-DAT
cable O
to O
the O
task O

of B-DAT
grounding O
referential O
expressions O
into O

bounding B-DAT
boxes, O
since O
it O
does O

not B-DAT
explicitly O
output O
bound- O
ing O

boxes B-DAT
as O
results. O
Recently, O
[2] O

improves B-DAT
over O
[3] O
by O
learning O

to B-DAT
re-rank O
parsing O
outputs O
from O

the B-DAT
external O
parser, O
but O
it O

is B-DAT
still O
not O
end-to-end O
learned O

since B-DAT
the O
parser O
is O
fixed O

and B-DAT
not O
optimized O
for O
the O

task. B-DAT
Inspired O
by O
[3], O
our O

model B-DAT
also O
uses O
a O
modular O

structure, B-DAT
but O
learns O
the O
language O

rep- B-DAT
resentation O
end-to-end O
from O
words. O
3. O
Our O
model O
We O
propose O

Compositional B-DAT
Modular O
Networks O
(CMNs O

) B-DAT
to O
localize O
visual O
entities O
described O

by B-DAT
a O
query O
referential O
expression O

. B-DAT
Our O
model O
is O
compositional O

in B-DAT
the O
sense O
that O
it O

localizes B-DAT
a O
referential O
expression O
by O

grounding B-DAT
the O
compo- O
nents O
in O

the B-DAT
expressions O
and O
exploiting O
their O

interactions, B-DAT
in O
accordance O
with O
the O

principle B-DAT
of O
compositionality O
of O
natu- O

ral B-DAT
language O
– O
the O
meaning O

of B-DAT
a O
complex O
expression O
is O

de- B-DAT
termined O
by O
the O
meanings O

of B-DAT
its O
constituent O
expressions O
and O

the B-DAT
rules O
used O
to O
combine O

them B-DAT
[29]. O
Our O
model O
works O

in B-DAT
a O
retrieval O
setting: O
given O

an B-DAT
image O
I O
, O
a O

referential B-DAT
expression O
Q O
as O
query O

and B-DAT
a O
set O
of O
candidate O

region B-DAT
bounding O
boxes O

B B-DAT
= O
{bi} O
for O
the O

image B-DAT
I O
(e.g. O
extracted O
through O

object B-DAT
pro- O
posal O
methods), O
our O

model B-DAT
outputs O
a O
score O
for O

each B-DAT
bound- O
ing O
box O
bi, O

and B-DAT
returns O
the O
bounding O
box O

with B-DAT
the O
highest O
score O
as O

grounding B-DAT
(localization) O
result. O
Unlike O
state-of-the- O

art B-DAT
methods O
[25, O
7], O
the O

scores B-DAT
for O
each O
region O
bounding O

box B-DAT
bi O
∈ O
B O
are O

not B-DAT
predicted O
only O
from O
the O

local B-DAT
feature O
of O

feature B-DAT
local O

spatial B-DAT
feature O

language B-DAT
representation O
of O

subject B-DAT
or O
object O
qloc O

fully B-DAT
connected O
layer O
element-wise O
multiplication O

l2-normalization B-DAT
concatenate O

fully B-DAT
connected O
layer O
output O
unary O
score O
sloc O

image B-DAT
region O
b O
b2 O
local O

spatial B-DAT
feature O

language B-DAT
representation O

of B-DAT
relationship O
qrel O

fully B-DAT
connected O
layer O
concatenate O

output B-DAT
pairwise O
score O
srel O
image O
region O
b1 O

b1 B-DAT
local O
spatial O

feature B-DAT
image O
region O
b2word O
sequence O
{wt O

} B-DAT
the O
man O
riding O
a O

black B-DAT
bicycle O
word O
embedding O
sequence O
{et O

} B-DAT
concatenated O
state O
{ht O

} B-DAT
fully O

layer B-DAT
1 O
softmax O
CNN O

qsubj B-DAT
weighted O
average O

three B-DAT
attention O
weights O
over O
each O

word B-DAT
{at,subj}, O
{at,rel}, O
{at,obj O

} B-DAT
subject O

at,subj}: B-DAT
the O
man O
riding O
a O

black B-DAT
bicycle O

{at,rel B-DAT

the B-DAT
man O
riding O
a O
black O

bicycle B-DAT

at,obj B-DAT
}: O
the O
man O
riding O
a O

black B-DAT
bicycle O

a) B-DAT
language O
representation O
(b) O
localization O

module B-DAT
(c) O
relationship O
module O
2-layer O
bidirectional O
LSTM O

fully B-DAT
connected O

layer B-DAT
2 O
softmax O
fully O

layer B-DAT
3 O
softmax O
element-wise O

multiplication B-DAT
l2-normalization O

fully B-DAT
connected O
layer O
qrel O
qobj O

Figure B-DAT
2. O
Detailed O
illustration O
of O

our B-DAT
model. O
(a) O
Our O
model O

learns B-DAT
to O
parse O
an O
expression O

into B-DAT
subject, O
relationship O
and O
object O

with B-DAT
attention O
for O
language O
representation O
( O

Sec. B-DAT
3.1). O
(b) O
The O
localization O

module B-DAT
matches O
subject O
or O
object O

with B-DAT
each O
image O
region O
and O

returns B-DAT
a O
unary O
score O
(Sec. O
3 O

.2). B-DAT
(c) O
The O
relationship O
module O

matches B-DAT
a O
relationship O
with O
a O

pair B-DAT
of O
regions O
and O
returns O

a B-DAT
pairwise O
score O
(Sec. O
3.3). O
but O
also O
based O
on O
other O

regions B-DAT
in O
the O
image. O
In O

our B-DAT
model, O
we O
focus O
on O

the B-DAT
relationships O
in O
referential O
expressions O

that B-DAT
can O
be O
represented O
as O

a B-DAT
3-component O
triplet O
(subject, O
relationship O

, B-DAT
object), O
and O
learn O
to O

parse B-DAT
the O
ex- O
pressions O
into O

these B-DAT
components O
with O
attention. O
For O

exam- B-DAT
ple, O
a O
young O
man O

wearing B-DAT
a O
blue O
shirt O
can O

be B-DAT
parsed O
as O
the O
triplet O
( O

a B-DAT
young O
man, O
wearing, O
a O

blue B-DAT
shirt). O
The O
score O
of O

a B-DAT
region O
is O
determined O
by O

simultaneously B-DAT
looking O
at O
whether O
it O

matches B-DAT
the O
description O
of O
the O

subject B-DAT
entity O
and O
whether O
it O

matches B-DAT
the O
relationship O
with O
another O

interacting B-DAT
object O
entity O
mentioned O
in O

the B-DAT
expression. O
Our O
model O
handles O
such O
inter-object O

relationships B-DAT
by O
looking O
at O
pairs O

of B-DAT
regions O
(bi, O
bj). O
For O

referential B-DAT
expres- O
sions O
like O
“the O

red B-DAT
apple O
on O
top O
of O

the B-DAT
bookshelf”, O
we O
want O
to O

find B-DAT
a O
region O
pair O
(bi O

, B-DAT
bj) O
such O
that O
bi O

matches B-DAT
the O
subject O
entity O
“red O

apple” B-DAT
and O
bj O
matches O
the O

object B-DAT
entity O
“book- O
shelf” O
and O

the B-DAT
configuration O
of O
(bi, O
bj) O

matches B-DAT
the O
relation- O
ship O
“on O

top B-DAT
of”. O
To O
achieve O
this O

goal, B-DAT
our O
model O
is O
based O

on B-DAT
a O
compositional O
modular O
structure, O

composed B-DAT
of O
two O
mod- O
ules O

assembled B-DAT
in O
a O
pipeline O
for O

different B-DAT
sub-tasks: O
one O
localization O
module O

floc(·, B-DAT
qloc; O
Θloc) O
for O
deciding O

whether B-DAT
a O
region O
matches O
the O

subject B-DAT
or O
object O
in O
the O

expression, B-DAT
where O
qloc O
is O
the O

textual B-DAT
vector O
representation O
of O
the O

sub- B-DAT
ject O
component O
“red O
apple” O

or B-DAT
the O
object O
component O
“book- O

shelf”, B-DAT
and O
one O
relationship O
module O

frel(·, B-DAT
·, O
qrel; O
Θrel) O
for O

deciding B-DAT
whether O
a O
pair O
of O

regions B-DAT
matches O
the O
relationship O
described O

in B-DAT
the O
expression O
represented O
by O

qrel, B-DAT
the O
textual O
vector O
representation O

of B-DAT
the O
relationship O
“on O
top O

of”. B-DAT
The O
representations O
qsubj O
, O

qrel B-DAT
and O
qobj O
are O
learned O

jointly B-DAT
in O
our O
model O
in O

Sec. B-DAT
3.1. O
We O
define O
the O
pairwise O
score O

spair(bi, B-DAT
bj) O
over O
a O
pair O

of B-DAT

image B-DAT
regions O
(bi, O
bj) O
matching O

an B-DAT
input O
referential O
expres- O
sion O

Q B-DAT
as O
the O
sum O
of O

three B-DAT
components: O
spair(bi, O
bj) O
= O
floc(bi, O
qsubj O

; B-DAT
Θloc O

) B-DAT
+ O
floc(bj O
, O
qobj O

; B-DAT
Θloc O

) B-DAT
+ O
frel(bi, O
bj O
, O
qrel O

; B-DAT
Θrel), O
(1 O

) B-DAT
where O
qsubj O
, O
qobj O
and O

qrel B-DAT
are O
vector O
representations O
of O

sub- B-DAT
ject, O
relationship O
and O
object O

, B-DAT
respectively. O
For O
inference, O
we O
define O
the O

final B-DAT
subject O
unary O
score O
ssubj(bi O

) B-DAT
of O
a O
bounding O
of O

bi B-DAT
corresponding O
to O
the O
subject O
( O

e.g. B-DAT
“the O
red O
apple” O

in B-DAT
“the O
red O
apple O
on O

top B-DAT
of O
the O
book- O
shelf”) O

as B-DAT
the O
score O
of O
the O

best B-DAT
possible O
pair O
(bi, O
bj) O

that B-DAT
matches O
the O
entire O
expression: O
ssubj(bi) O
, O
max O
bj∈B O

spair(bi, B-DAT
bj). O
(2) O
The O
subject O
is O
ultimately O
grounded O

(localized) B-DAT
to O
the O
highest O
scoring O

region B-DAT
as O

b∗subj B-DAT
= O
arg O
max O
bi∈B O
(ssubj(bi)). O
(3 O

) B-DAT
3.1. O
Expression O
parsing O
with O
attention O

Given B-DAT
a O
referential O
expressionQ O
like O

the B-DAT
tall O
woman O
car- O
rying O

a B-DAT
red O
bag, O
how O
can O

we B-DAT
decide O
which O
substrings O
cor- O

responds B-DAT
to O
the O
subject, O
the O

relationship, B-DAT
and O
the O
object, O
and O

extract B-DAT
three O
vector O
representations O
qsubj O
, O
qrel O
and O
qobj O
cor- O
responding O

to B-DAT
these O
three O
components? O
One O

possible B-DAT
ap- O
proach O
is O
to O

use B-DAT
an O
external O
language O
parser O

to B-DAT
parse O
the O
referential O
expression O

into B-DAT
the O
triplet O
format O
(subject O

relationship, B-DAT
object) O
and O
then O
process O

each B-DAT
com- O
ponent O
with O
an O

encoder B-DAT
(e.g. O
a O
recurrent O
neural O

network) B-DAT
to O
extract O
qsubj O
, O

qrel B-DAT
and O
qobj O
. O
However, O

the B-DAT
formal O
represen- O
tations O
of O

language B-DAT
produced O
by O
syntactic O
parsers O

do B-DAT
not O
al- O
ways O
correspond O

to B-DAT
intuitive O
visual O
representations. O
As O

a B-DAT
simple O
example, O
the O
apple O

on B-DAT
top O
of O
the O
bookshelf O

is B-DAT
ana- O
lyzed O
[31] O
as O

having B-DAT
a O
subject O
phrase O
the O

apple, B-DAT
a O
relation- O
ship O
on, O

and B-DAT
an O
object O
phrase O
top O

of B-DAT
the O
bookshelf, O
when O
in O

fact B-DAT
the O
visually O
salient O
objects O

are B-DAT
simply O
the O
apple O
and O

the B-DAT
bookshelf, O
while O
the O
complete O

expression B-DAT
on O
top O
of O
de- O

scribes B-DAT
the O
relationship O
between O
them. O
Therefore, O
in O
this O
work O
we O

learn B-DAT
to O
decompose O
the O
input O

expression B-DAT
Q O
into O
the O
above O

3 B-DAT
components, O
and O
generate O
vector O

representations B-DAT
qsubj O
, O
qrel O
and O

qobj B-DAT
fromQ O
through O
a O
soft O

attention B-DAT
mechanism O
over O
the O
word O

sequence, B-DAT
as O
shown O
in O
Figure O

2 B-DAT
(a). O
For O
a O
referential O

expression B-DAT
Q O
that O
is O
a O

sequence B-DAT
of O
T O
words O
{wt}Tt=1 O

, B-DAT
we O
first O
embed O
each O

word B-DAT
wt O
to O
a O
vector O

et B-DAT
using O
GloVe O
[22], O
and O

then B-DAT
scan O
through O
the O
word O

embedding B-DAT
sequence O
{et}Tt=1 O
with O
a O
2 O

-layer B-DAT
bi- O
directional O
LSTM O

network B-DAT
[26]. O
The O
first O
layer O

takes B-DAT
as O
in- O
put O
the O

sequence B-DAT
{et} O
and O
outputs O
a O

forward B-DAT
hidden O
state O
h O
(1,fw) O

t B-DAT
and O
a O
backward O
hidden O

state B-DAT
h O
(1,bw) O
t O
at O
each O
time O

step, B-DAT
which O
are O
concatenated O
into O

h(1)t B-DAT
. O
The O
second O
layer O

then B-DAT
takes O
the O
first O
layer’s O

output B-DAT
sequence O
{h(1)t O
} O
as O

input B-DAT
and O
outputs O
forward O
and O

backward B-DAT
hidden O
states O
h(2,fw)t O
and O

h(2,bw)t B-DAT
at O
each O
time O
step. O

All B-DAT
the O
hidden O
states O
in O

the B-DAT
first O
layer O
and O
second O

layer B-DAT
are O
concatenated O
into O
a O

single B-DAT
vector O
ht. O
ht O
= O
[ O
h O
(1,fw O

) B-DAT
t O
h O
(1,bw) O
t O
h O

2,fw) B-DAT
t O
h O
(2,bw) O
t O

4) B-DAT
The O
concatenated O
state O
ht O
contains O

information B-DAT
from O
word O
wt O
itself O

and B-DAT
also O
context O
from O
words O

before B-DAT
and O
after O
wt. O
Then O

the B-DAT
attention O
weights O
at,subj O

, B-DAT
at,rel O
and O
at,obj O
for O

subject, B-DAT
relationship, O
object O
over O
each O

word B-DAT
wt O
are O
obtained O
by O

three B-DAT
linear O
predictions O
over O
ht O

followed B-DAT
by O
a O
softmax O
as O

at,subj B-DAT
= O
exp O
( O
βTsubjht O

T B-DAT
τ=1 O
exp O
( O
βTsubjhτ O

5) B-DAT
at,rel O
= O
exp O
( O
βTrelht O

T B-DAT
τ=1 O
exp O
( O
βTrelhτ O

6) B-DAT
at,obj O
= O
exp O
( O
βTobjht O

T B-DAT
τ=1 O
exp O
( O
βTobjhτ O

7) B-DAT
and O
the O
language O
representations O

of B-DAT
the O
subject O
qsubj O
, O

rela- B-DAT
tionship O
qrel O
and O
object O

qobj B-DAT
are O
extracted O
as O
weighed O

aver- B-DAT
age O
of O
word O
embedding O

vectors B-DAT
{et}with O
attention O
weights, O
as O
follows O

: B-DAT
qsubj O

T∑ B-DAT
t=1 O
at,subjet O
(8 O

) B-DAT
qrel O

T∑ B-DAT
t=1 O
at,relet O
(9 O

) B-DAT
qobj O

T∑ B-DAT
t=1 O
at,objet. O
(10 O

) B-DAT
In O
our O
implementation, O
both O
the O

forward B-DAT
and O
the O
back- O
ward O

LSTM B-DAT
in O
each O
layer O
of O

the B-DAT
bi-directional O
LSTM O
net- O
work O

have B-DAT
1000-dimensional O
hidden O
states, O
so O

the B-DAT
final O
ht O
is O
4000-dimensional O

. B-DAT
During O
training, O
dropout O
is O

added B-DAT
on O
top O
of O
ht O

as B-DAT
regularization. O
3.2. O
Localization O
module O

As B-DAT
shown O
in O
Figure O
2 O
( O

b), B-DAT
the O
localization O
module O
floc O

outputs B-DAT
a O
score O
sloc O
= O

floc(b, B-DAT
qloc; O
Θloc) O
representing O
how O

likely B-DAT
a O
region O
bounding O
box O

b B-DAT
matches O
qloc, O
which O
is O

either B-DAT
the O
subject O
textual O
vector O

qsubj B-DAT
in O
Eqn. O
8 O
or O

object B-DAT
textual O
vector O
qobj O
in O

Eqn. B-DAT
10. O
This O
module O
takes O
the O
local O

visual B-DAT
feature O
xvis O
and O
spa O

- B-DAT
tial O
feature O
xspatial O
of O

image B-DAT
region O
b. O
We O
extract O

visual B-DAT
feature O
xv O
from O
image O

region B-DAT
b O
using O
a O
convolutional O

neu- B-DAT
ral O
network O
[27], O
and O

extract B-DAT
a O
5-dimensional O
spatial O
feature O

xs B-DAT
= O
[ O
xmin O
WI O

yminHI B-DAT
, O
xmax O
WI O
, O
ymaxHI O
, O
Sb O
SI O

from B-DAT
b O
using O
the O
same O

representation B-DAT
as O
in O
[20], O

where B-DAT
[xmin, O
ymin, O
xmax, O
ymax] O

and B-DAT
Sb O
are O
bounding O
box O

coordinates B-DAT
and O
area O
of O
b, O

andWI B-DAT
, O
HI O
and O
SI O

are B-DAT
width, O
height O
and O
area O

of B-DAT
the O
image O
I O
. O

Then, B-DAT
xv O
and O
xs O
are O

concatenated B-DAT
into O
a O
vector O

xv,s B-DAT
= O
[xv O
xs] O
as O

representation B-DAT
of O
region O
b. O
Since O
element-wise O
multiplication O
is O
shown O

to B-DAT
be O
a O
pow- O
erful O

way B-DAT
to O
combine O
representations O
from O

different B-DAT
modal- O
ities O
[5], O
we O

adopt B-DAT
it O
here O
to O
obtain O

a B-DAT
joint O
vision O
and O
lan O

- B-DAT
guage O
representation. O
In O
our O

implementation, B-DAT
xv,s O
is O
first O
embedded O

to B-DAT
a O
new O
vector O
x̃v,s O

that B-DAT
has O
the O
same O
dimen- O

sion B-DAT
as O
qloc O
(which O
is O

either B-DAT
qsubj O
in O
Eqn. O
8 O

or B-DAT
qobj O
in O
Eqn. O
10) O

through B-DAT
a O
linear O
transform, O
and O

then B-DAT
element-wise O
multiplied O
with O
qloc O

to B-DAT
obtain O
a O
vector O
zloc, O

which B-DAT
is O
L2- O
normalized O
into O

ẑloc B-DAT
to O
obtain O
a O
more O

robust B-DAT
representation, O
as O
follows: O
x̃v,s O
= O
Wv,sxv,s O
+ O
bv,s O

(11) B-DAT
zloc O
= O
x̃v,s O

� B-DAT
qloc O
(12) O
ẑloc O

= B-DAT
zloc/‖zloc‖2 O
(13 O

) B-DAT
where O
� O
is O
element-wise O
multiplication O

between B-DAT
two O
vec- O
tors. O
Then O

the B-DAT
score O
sloc O
is O
predicted O

linearly B-DAT
from O
ẑloc O
as O

sloc B-DAT
= O
w O
T O

locẑloc B-DAT
+ O
bloc. O
(14) O
The O
parameters O
in O
Θloc O
are O

(Wv,s, B-DAT
bv,s, O
wloc, O
bloc O

3.3. B-DAT
Relationship O
module O
As O
shown O
in O
Figure O
2 O

(c), B-DAT
the O
relationship O
module O
frel O

outputs B-DAT
a O
score O
srel O

= B-DAT
frel(b1, O
b2, O
qrel; O
Θrel O

) B-DAT
representing O
how O
likely O
a O

pair B-DAT
of O
region O
bounding O
boxes O
( O

b1, B-DAT
b2) O
matches O
qrel, O
the O

representation B-DAT
of O
relationship O
in O
the O

expression. B-DAT
In O
our O
implementation, O
we O
use O

the B-DAT
spatial O
features O
xs1 O
and O

xs2 B-DAT
of O
the O
two O
regions O

b1 B-DAT
and O
b2 O
extracted O
in O

the B-DAT
same O
way O
as O
in O

localization B-DAT
module O
(we O
empirically O
find O

that B-DAT
adding O
visual O
features O
of O

b1 B-DAT
and O
b2 O
leads O
to O

no B-DAT
noticeable O
performance O
boost O
while O

slowing B-DAT
training O
significantly). O
Then O
xs1 O

and B-DAT
xs2 O
are O
concatenated O
as O

xs1,s2 B-DAT
= O
[xs1 O
xs2], O
and O

then B-DAT
processed O
in O
a O
similar O

way B-DAT
as O
in O
localization O
mod O

- B-DAT
ule O
to O
obtain O
srel, O

as B-DAT
shown O
below: O
x̃s1,s2 O
= O
Ws1,s2xs1,s2 O
+ O
bs1,s2 O

(15) B-DAT
zrel O
= O
x̃s1,s2 O

� B-DAT
qrel O
(16) O
ẑrel O

= B-DAT
zrel/‖zrel‖2 O
(17) O
srel O

= B-DAT
w O

T B-DAT
relẑrel O
+ O
brel. O
(18) O
The O
parameters O
in O
Θrel O
are O

(Ws1,s2, B-DAT
bs1,s2, O
wrel, O
brel O

). B-DAT
3.4. O
End-to-end O
learning O

During B-DAT
training, O
for O
an O
image O

I B-DAT
, O
a O
referential O
expression O

Q B-DAT
and O
a O
set O
of O

candidate B-DAT
regions O
B O
extracted O
from O

I B-DAT
, O
if O
the O
ground-truth O

regions B-DAT
bsubj O
gt O
of O
the O

subject B-DAT
entity O
and O
bobj O
gt O

of B-DAT
the O
object O
entity O
are O

both B-DAT
available, O
then O
we O
can O

optimize B-DAT
the O
pairwise O
score O
spair O

in B-DAT
Eqn. O
1 O
with O
strong O

supervision B-DAT
using O
softmax O
loss O
Lossstrong. O
Lossstrong O
= O
− O
log O

exp B-DAT
(spair(bsubj O
gt, O
bobj O
gt))∑ O
( O

bi,bj)∈B×B B-DAT
exp O
(spair(bi, O
bj)) O
) O
(19 O

) B-DAT
However, O
it O
is O
often O
hard O

to B-DAT
obtain O
ground-truth O
regions O
for O

both B-DAT
subject O
entity O
and O
object O

entity. B-DAT
For O
referential O
expres- O
sions O

like B-DAT
“a O
red O
vase O
on O

top B-DAT
of O
the O
table”, O
often O

there B-DAT
is O
only O
a O
ground-truth O

bounding B-DAT
box O
annotation O
b1 O
for O

the B-DAT
subject O
(vase) O
in O
the O

expression, B-DAT
but O
no O
bounding O
box O

annotation B-DAT
b2 O
for O
the O
object O

(table), B-DAT
so O
one O
cannot O
directly O

optimize B-DAT
the O
pairwise O
score O
spair(b1 O

, B-DAT
b2). O
To O
address O
this O

issue, B-DAT
we O
treat O
the O
object O

region B-DAT
b2 O
as O
a O
latent O

variable, B-DAT
and O
optimize O
the O
unary O

score B-DAT
ssubj(b1) O
in O
Eqn. O
2. O

Since B-DAT
ssubj(b1) O
is O
ob- O
tained O

by B-DAT
maximizing O
over O
all O
possible O

region B-DAT
b2 O
∈ O
B O
in O

spair(b1, B-DAT
b2), O
this O
can O
be O

regarded B-DAT
as O
a O
weakly O
supervised O

Multiple B-DAT
Instance O
Learning O
(MIL) O
approach O

similar B-DAT
to O
[21]. O
The O
unary O

score B-DAT
ssubj O
can O
be O
optimized O

with B-DAT
weak O
supervi- O
sion O
using O

softmax B-DAT
loss O
Lossweak. O
Lossweak O
= O
− O
log O

exp B-DAT
(ssubj(bsubj O
gt))∑ O
bi∈B O
exp O
( O

ssubj(bi)) B-DAT
) O
(20 O

) B-DAT
The O
whole O
system O
is O
trained O

end-to-end B-DAT
with O
backpropa- O
gation. O
In O

our B-DAT
experiments, O
we O
train O
for O

300000 B-DAT
iterations O

, B-DAT
Method O
Accuracy O
baseline O
(loc O
module O

) B-DAT
46.27% O
our O
full O
model O
99 O

.99% B-DAT
Table O
1. O
Accuracy O
of O
our O

model B-DAT
and O
the O
baseline O
on O

the B-DAT
synthetic O
shape O
dataset. O
See O

Sec. B-DAT
4.1 O
for O
details O

. B-DAT
expression=“the O
green O
square O
right O
of O

a B-DAT
red O
circle” O
baseline O

- B-DAT
sloc O
ssubj O
sobj O

a) B-DAT
(b) O
(c) O
(d) O
Figure O
3. O
For O
the O
image O

in B-DAT
(a) O
and O
the O
expression O

“the B-DAT
green O
square O
right O
of O

a B-DAT
red O
circle”, O
(b) O
baseline O

scores B-DAT
on O
each O
location O
on O

the B-DAT
5 O
by O
5 O
grid O

using B-DAT
localization O
module O
only O
(darker O

is B-DAT
higher), O
and O
(c, O
d O

) B-DAT
scores O
ssubj O
and O
sobj O

using B-DAT
our O
full O
model. O
ssubj O

is B-DAT
highest O
on O
the O
exact O

green B-DAT
square O
that O
is O
on O

the B-DAT
right O
of O
a O
red O

circle, B-DAT
and O
sobj O
is O
highest O

on B-DAT
this O
red O
circle. O
with O
0.95 O
momentum O
and O
an O

initial B-DAT
learning O
rate O
of O
0.005 O

, B-DAT
multiplied O
by O
0.1 O
after O

every B-DAT
120000 O
iterations. O
Each O
batch O

contains B-DAT
one O
image O
with O
all O

referential B-DAT
expressions O
anno- O
tated O
over O

that B-DAT
image. O
Parameters O
in O
the O

localization B-DAT
mod- O
ule, O
the O
relationship O

module B-DAT
and O
the O
language O
representation O

in B-DAT
our O
model O
are O
initialized O

randomly B-DAT
with O
Xavier O
initializer O
[9]. O

Our B-DAT
model O
is O
implemented O
using O

TensorFlow B-DAT
[1] O
and O
we O
plan O

to B-DAT
release O
our O
code O
and O

data B-DAT
to O
facilitate O
reproduc- O
tion O

of B-DAT
our O
results. O
4. O
Experiments O
We O
first O
evaluate O

our B-DAT
model O
on O
a O
synthetic O

dataset B-DAT
to O
ver O

- B-DAT
ify O
its O
ability O
to O
handle O

inter-object B-DAT
relationships O
in O
refer- O
ential O

expressions. B-DAT
Next O
we O
apply O
our O

method B-DAT
to O
real O
im- O
ages O

and B-DAT
expressions O
in O
the O
Visual O

Genome B-DAT
dataset O
[14] O
and O
Google-Ref O

dataset B-DAT
[20]. O
Since O
the O
task O

of B-DAT
answering O
pointing O
questions O
in O

visual B-DAT
question O
answering O
is O
similar O

to B-DAT
grounding O
referential O
expressions, O
we O

also B-DAT
evaluate O
our O
model O
on O

the B-DAT
pointing O
questions O
in O
the O

Visual-7W B-DAT
dataset O
[32 O

]. B-DAT
4.1. O
Analysis O
on O
a O
synthetic O

dataset B-DAT

Inspired B-DAT
by O
[3], O
we O
first O

perform B-DAT
a O
simulation O
exper- O
iment O

on B-DAT
a O
synthetic O
shape O
dataset. O

The B-DAT
dataset O
con- O
sists O
of O
30000 O
images O
with O
simple O
circles, O
squares O

and B-DAT
triangles O
of O
different O
sizes O

and B-DAT
colors O
on O
a O
5 O

by B-DAT
5 O
grid, O
and O
referential O

expressions B-DAT
constructed O
using O
a O
template O

of B-DAT
the O
form O
[subj] O
[relationship O

] B-DAT
[obj], O
where O
[subj] O

and B-DAT
[obj] O
involve O
both O
shape O

classes B-DAT
and O
at- O
tributes O

and B-DAT
[relationship] O
is O
some O
spatial O

relation- B-DAT
ships O
such O
as O
“above”. O

The B-DAT
task O
is O
to O
localize O

the B-DAT
corre- O
sponding O
shape O
region O

described B-DAT
by O
the O
expression O
on O

5 B-DAT
by O
5 O
grid. O
Figure O
3 O
(a) O
shows O
an O
example O
in O

this B-DAT
dataset O
with O
the O
synthetic O

expression B-DAT
“the O
green O
square O
right O

of B-DAT
a O
red O
circle”. O
In O

the B-DAT
synthesizing O
procedure, O
we O
make O

sure B-DAT
that O
the O
shape O
region O

being B-DAT
referred O
to O
cannot O
be O

inferred B-DAT
simply O
from O
[subj] O
as O

there B-DAT
will O
be O
multiple O
matching O

regions, B-DAT
and O
the O
relationship O
with O

another B-DAT
region O
described O
by O
[obj O

] B-DAT
has O
to O
be O
taken O

into B-DAT
consideration. O
On O
this O
dataset, O
we O
train O

our B-DAT
model O
with O
weak O
super O

- B-DAT
vision O
by O
Eqn. O
20 O

using B-DAT
the O
ground-truth O
subject O
region O

bsubj B-DAT
gt O
of O
the O
subject O

shape B-DAT
described O
in O
the O
expression. O

Here B-DAT
the O
candidate O
region O
set O

B B-DAT
are O
the O
25 O
possible O

loca- B-DAT
tions O
on O
the O
5 O

by B-DAT
5 O
grid, O
and O
visual O

features B-DAT
are O
extracted O
from O
the O

corresponding B-DAT
cropped O
image O
region O
with O

a B-DAT
VGG- O
16 O
network O
[27] O

pretrained B-DAT
on O
ImageNET O
classification. O
As O

a B-DAT
comparison, O
we O
also O
train O

a B-DAT
baseline O
model O
using O
only O

the B-DAT
localization O
module, O
with O
a O

softmax B-DAT
loss O
on O
its O
output O

sloc B-DAT
in O
Eqn. O
14 O
over O

all B-DAT
25 O
locations O
on O
the O

grid, B-DAT
and O
language O
representation O
qloc O

obtained B-DAT
by O
scanning O
through O
the O

word B-DAT
embedding O
sequence O
with O
a O

single B-DAT
LSTM O
network O
and O
tak- O

ing B-DAT
the O
hidden O
state O
at O

the B-DAT
last O
time O
step O
same O

as B-DAT
in O
[25, O
10]. O
This O

baseline B-DAT
method O
resembles O
the O
supervised O

version B-DAT
of O
GroundeR O
[25], O
and O

the B-DAT
main O
difference O
between O
this O

base- B-DAT
line O
and O
our O
model O

is B-DAT
that O
the O
baseline O
only O

looks B-DAT
at O
a O
re- O
gion’s O

appearance B-DAT
and O
spatial O
property O
but O

ignores B-DAT
pairwise O
relationship O
with O
other O

regions. B-DAT
We O
evaluate O
with O
the O
accuracy O

on B-DAT
whether O
the O
pre- O
dicted O

subject B-DAT
region O
b∗subj O
matches O
the O

ground-truth B-DAT
region O
bsubj O
gt. O
Table O

1 B-DAT
shows O
the O
results O
on O

this B-DAT
dataset, O
where O
our O
model O

trained B-DAT
with O
weak O
supervision O
(the O

same B-DAT
as O
the O
super- O
vision O

given B-DAT
to O
baseline) O
achieves O
nearly O

perfect B-DAT
accuracy— O
significantly O
outperforming O
the O

baseline B-DAT
using O
a O
localization O
module O

only. B-DAT
Figure O
3 O
shows O
an O

example, B-DAT
where O
the O
base- O
line O

can B-DAT
localize O
green O
squares O
but O

fails B-DAT
to O
distinguish O
the O
exact O

green B-DAT
square O
right O
of O
a O

red B-DAT
circle, O
while O
our O
model O

suc- B-DAT
cessfully O
finds O
the O
subject-object O

pair, B-DAT
although O
it O
has O
never O

seen B-DAT
the O
ground-truth O
location O
for O

the B-DAT
object O
entity O
during O
training O

. B-DAT
4.2. O
Localizing O
relationships O
in O
Visual O

Genome B-DAT

We B-DAT
also O
evaluate O
our O
method O

on B-DAT
the O
Visual O
Genome O

dataset B-DAT
[14], O
which O
contains O
relationship O

expressions B-DAT
anno- O
tated O
over O
pairs O

of B-DAT
objects, O
such O
as O
“computer O

on B-DAT
top O
of O
ta- O
ble” O

and B-DAT
“person O
wearing O
shirt”. O
On O
the O
relationship O
annotations O
in O

Visual B-DAT
Genome, O
given O
an O
image O

and B-DAT
an O
expression O
like O
“man O

wearing B-DAT
hat”, O
we O
evaluate O
our O

method B-DAT
in O
two O
test O
scenarios O

: B-DAT
retrieving O
the O
subject O

region B-DAT
(“man”) O
and O
retrieving O
the O

subject-object B-DAT
pair O
(both O
“man” O

and B-DAT
“hat”). O
In O
our O
experiment, O

we B-DAT
take O
the O
bounding O
boxes O

of B-DAT
all O
the O
annotated O
entities O

in B-DAT
each O
im- O
age O
(around O
35 O
per O
image) O
as O
candidate O
region O

set B-DAT
B O
at O
both O
training O

and B-DAT
test O
time, O
and O
extract O

visual B-DAT
features O
for O
each O
region O

from B-DAT
fc7 O
output O
of O
a O

Faster-RCNN B-DAT
VGG-16 O
network O
[24] O
pretrained O

on B-DAT
MSCOCO O
detection O
dataset O
[16 O

]. B-DAT
The O
Method O
training O
supervision O
P@1-subj O
P@1-pair O

baseline B-DAT
subject-GT O
41.20% O
- O
baseline O

subject-object-GT B-DAT
- O
23.37% O
our O
full O

model B-DAT
subject-GT O
43.81% O
26.56% O
our O

full B-DAT
model O
subject-object-GT O
44.24% O
28.52 O

% B-DAT
Table O
2. O
Performance O
of O
our O

model B-DAT
on O
relationship O
expressions O
in O

Visual B-DAT
Genome O
dataset. O
See O
Sec O

. B-DAT
4.2 O
for O
details. O
input O
images O
are O
first O
forwarded O

through B-DAT
the O
convolutional O
layers O
of O

the B-DAT
network, O
and O
the O
features O

of B-DAT
each O
image O
region O
are O

extracted B-DAT
by O
ROI-pooling O
over O
the O

convolutional B-DAT
feature O
map, O
followed O
by O

subsequent B-DAT
fully O
connected O
layers. O
We O

use B-DAT
the O
same O
training, O
validation O

and B-DAT
test O
split O
as O
in O

[12 B-DAT

]. B-DAT
Since O
there O
are O
ground-truth O
annotations O

for B-DAT
both O
subject O
region O
and O

object B-DAT
region O
in O
this O
dataset O

, B-DAT
we O
experiment O
with O
two O

training B-DAT
supervision O
settings: O
(1) O
weak O

supervision B-DAT
by O
only O
providing O
the O

ground-truth B-DAT
region O
of O
the O
subject O

en- B-DAT
tity O
at O
training O
time O
( O

subject-GT B-DAT
in O
Table O
2) O
and O

optimizing B-DAT
unary O
subject O
score O
ssubj O

with B-DAT
Eqn. O
20 O
and O
(2) O

strong B-DAT
su- O
pervision O
by O
providing O

the B-DAT
ground-truth O
region O
pair O
of O

both B-DAT
subject O
and O
object O
entities O

at B-DAT
training O
time O
(subject-object- O
GT O

in B-DAT
Table O
2) O
and O
optimizing O

pairwise B-DAT
score O
spair O
with O
Eqn. O
19 O

. B-DAT
Similar O
to O
the O
experiment O
on O

the B-DAT
synthetic O
dataset O
in O
Sec O

. B-DAT
4.1, O
we O
also O
train O

a B-DAT
baseline O
model O
that O
only O

looks B-DAT
at O
local O
appearance O
and O

spatial B-DAT
properties O
but O
ignores O
pairwise O

rela- B-DAT
tionships. O
For O
the O
first O

evaluation B-DAT
scenario O
of O
retrieving O
the O

subject B-DAT
region, O
we O
train O
a O

baseline B-DAT
model O
using O
a O
localiza- O

tion B-DAT
module O
only O
by O
optimizing O

its B-DAT
output O
sloc O
for O
ground- O

truth B-DAT
subject O
region O
with O
softmax O

loss B-DAT
(the O
same O
training O
su- O

pervision B-DAT
as O
subject-GT). O
For O
the O

second B-DAT
scenario O
of O
retriev- O
ing O

the B-DAT
subject-object O
pair, O
we O
train O

two B-DAT
such O
baseline O
mod- O
els O

optimized B-DAT
with O
subject O
ground-truth O
and O

object B-DAT
ground- O
truth O
respectively, O
to O

localize B-DAT
of O
the O
subject O
region O

and B-DAT
ob- O
ject O
region O
separately O

with B-DAT
each O
model O
and O
at O

test B-DAT
time O
com- O
bine O
the O

predicted B-DAT
subject O
region O
and O
predicted O

object B-DAT
region O
from O
each O
model O

be B-DAT
the O
subject-object O
pair O
(same O

training B-DAT
supervision O
as O
subject-object-GT). O
We O
evaluate O
with O
top-1 O
precision O

(P@1), B-DAT
which O
is O
the O
percentage O

of B-DAT
test O
instances O
where O
the O

top B-DAT
scoring O
predic- O
tion O
matches O

the B-DAT
ground-truth O
in O
each O
image O

(P@1-subj B-DAT
for O
predicted O
subject O
regions O

matching B-DAT
subject O
ground-truth O
in O
the O

first B-DAT
scenario, O
and O
P@1-pair O
for O

predicted B-DAT
subject O
and O
object O
regions O

both B-DAT
matching O
the O
ground-truth O
in O

the B-DAT
second O
scenario). O
The O
results O

are B-DAT
summarized O
in O
Table O
2 O

, B-DAT
where O
it O
can O
be O

seen B-DAT
that O
our O
full O
model O

outperforms B-DAT
the O
baseline O
using O
only O

localization B-DAT
modules O
in O
both O
evaluation O

scenar- B-DAT
ios. O
Note O
that O
in O

the B-DAT
second O
evaluation O
scenario O
of O

retrieving B-DAT
subject-object O
pairs, O
our O
weakly O

supervised B-DAT
model O
still O
out- O
performs O

the B-DAT
baseline O
trained O
with O
strong O

supervision. B-DAT
Figure O
4 O
shows O
some O
examples O

of B-DAT
our O
model O
trained O
with O

weak B-DAT
supervision O
(subject-GT) O
and O
attention O

weights B-DAT

ground-truth B-DAT
our O
prediction O
attention O
weights O

ground-truth B-DAT
our O
prediction O
attention O
weights O

expression=“tennis B-DAT
player O
wears O
shorts” O
expression=“building O

behind B-DAT
bus” O
expression=“car O
has O
tail O
light” O
expression=“window O

on B-DAT
front O
of O
building O

” B-DAT
expression=“business O
name O
on O
sign” O
expression=“board O

on B-DAT
top O
of O
store O

” B-DAT
expression=“wine O
bottle O
next O
to O
glasses O

” B-DAT
expression=“chairs O
around O
table” O
expression=“marker O
on O
top O
of O
ledge O

” B-DAT
expression=“chair O
next O
to O
table” O
Figure O
4. O
Visualization O
of O
grounded O

relationship B-DAT
expressions O
in O
the O
Visual O

Genome B-DAT
dataset, O
trained O
with O
weak O

supervision B-DAT
(subject-GT). O
In O
each O
example O

, B-DAT
the O
first O
and O
the O

second B-DAT
column O
show O
ground-truth O
region O

pairs B-DAT
and O
our O
predicted O
region O

pairs B-DAT
respectively O
(subject O
in O
red O

solid B-DAT
box O
and O
object O
in O

green B-DAT
dashed O
box). O
The O
third O

column B-DAT
visualizes O
attention O
weights O
in O

Eqn. B-DAT
5–7 O
for O
subject, O
relationship O

and B-DAT
object O
(darker O
is O
higher). O
in O
Eqn. O
5–7. O
It O
can O

be B-DAT
seen O
that O
even O
with O

weak B-DAT
supervi- O
sion, O
our O
model O

still B-DAT
generates O
reasonable O
attention O
weights O

over B-DAT
words O
for O
subject, O
relationship O

and B-DAT
object O

. B-DAT
4.3. O
Grounding O
referential O
expressions O
in O

images B-DAT

We B-DAT
apply O
our O
model O
to O

the B-DAT
Google-Ref O
dataset O
[20], O
a O

benchmark B-DAT
dataset O
for O
grounding O
referential O

expressions. B-DAT
As O
this O
dataset O
does O

not B-DAT
explicitly O
contain O
subject-object O
pair O

annotation B-DAT
for O
the O
referential O
expressions, O

we B-DAT
train O
our O
model O
with O

weak B-DAT
supervision O
(Eqn. O
20) O
by O

optimizing B-DAT
the O
subject O
score O
ssubj O

using B-DAT
the O
expression-level O
region O
ground-truth. O

The B-DAT
candidate O
bounding O
box O
set O

B B-DAT
at O
both O
training O
and O

test B-DAT
time O
are O
all O
the O

annotated B-DAT
entities O
in O
the O
image O
( O

which B-DAT
is O
the O
“Ground-Truth” O
evaluation O

setting B-DAT
in O
[20]). O
As O
in O

Sec. B-DAT
4.2, O
fc7 O
output O
of O

a B-DAT
MSCOCO-pretrained O
Method O
P@1 O
Mao O
et O
al O

. B-DAT
[20] O
60.7% O
Yu O
et O

al. B-DAT
[30] O
64.0% O
Nagaraja O
et O

al. B-DAT
[21] O
68.4% O
baseline O
(loc O

module) B-DAT
66.5% O
our O
model O
(w/ O

external B-DAT
parser) O
53.5% O
our O
full O

model B-DAT
69.3% O
Table O
3. O
Top-1 O
precision O
of O

our B-DAT
model O
and O
previous O
methods O

on B-DAT
Google-Ref O
dataset. O
See O
Sec O

. B-DAT
4.3 O
for O
details. O
Faster-RCNN O
VGG-16 O
network O
is O
used O

for B-DAT
visual O
feature O
extraction. O
Similar O

to B-DAT
Sec. O
4.1, O
we O
also O

train B-DAT
a O
GroundeR- O
like O
[25 O

] B-DAT
baseline O
model O
with O
localization O

module B-DAT
which O
looks O
only O
at O

a B-DAT
region’s O
local O

Method B-DAT
Accuracy O
Zhu O
et O

al. B-DAT
[32] O
56.10% O
baseline O
(loc O

module) B-DAT
71.61% O
our O
model O
(w/ O

external B-DAT
parser) O
61.66% O
our O
full O

model B-DAT
72.53% O
Table O
4. O
Accuracy O
of O
our O

model B-DAT
and O
previous O
methods O
on O

the B-DAT
pointing O
questions O
in O
Visual-7W O

dataset. B-DAT
See O
Sec. O
4.4 O
for O

details B-DAT

. B-DAT
In O
addition, O
instead O
of O
learning O

a B-DAT
linguistic O
analysis O
end- O
to-end O

as B-DAT
in O
Sec. O
3.1, O
we O

also B-DAT
experiment O
with O
parsing O
the O

expression B-DAT
using O
the O
Stanford O
Parser O

[31, B-DAT
19]. O
An O
expres- O
sion O

is B-DAT
parsed O
into O
subject, O
relationship O

and B-DAT
object O
compo- O
nent O
according O

to B-DAT
the O
constituency O
tree, O
and O

the B-DAT
components O
are O
encoded O
into O

vectors B-DAT
qsubj O
, O
qrel O
and O

qobj B-DAT
using O
three O
sep- O
arate O

LSTM B-DAT
encoders, O
similar O
to O
the O

baseline B-DAT
and O
[25 O

]. B-DAT
Following O
[20], O
we O
evaluate O
on O

this B-DAT
dataset O
using O
the O
top-1 O

precision B-DAT
(P@1) O
metric, O
which O
is O

the B-DAT
fraction O
of O
the O
highest O

scoring B-DAT
subject O
region O
matching O
the O

ground-truth B-DAT
for O
the O
expression. O
Table O

3 B-DAT
shows O
the O
performance O
of O

our B-DAT
model, O
baseline O
model O
and O

previous B-DAT
work. O
Note O
that O
all O

the B-DAT
methods O
are O
trained O
with O

the B-DAT
same O
weak O
supervision O
(only O

a B-DAT
ground-truth O
subject O
region). O
It O

can B-DAT
be O
seen O
that O
by O

in- B-DAT
corporating O
inter-object O
relationships, O
our O

full B-DAT
model O
out- O
performs O
the O

baseline B-DAT
using O
only O
localization O
modules O

, B-DAT
and O
works O
better O
than O

previous B-DAT
state-of-the-art O
methods. O
Additionally, O
replacing O
the O
learned O
expression O

parsing B-DAT
and O
language O
representation O
in O

Sec. B-DAT
3.1 O
with O
an O
external O

parser B-DAT
(“our O
model O
w/ O
external O

parser” B-DAT
in O
Table O
3) O
leads O

to B-DAT
a O
significant O
performance O
drop O

. B-DAT
We O
find O
that O
this O

is B-DAT
mainly O
because O
existing O
parsers O

are B-DAT
not O
specifically O
tuned O
for O

the B-DAT
referring O
expression O
task—as O
noted O

in B-DAT
Sec. O
3.1, O
expressions O
like O

chair B-DAT
on O
the O
left O
of O

the B-DAT
table O
are O
parsed O
as O
( O

chair, B-DAT
on, O
the O
left O
of O

the B-DAT
table) O
rather O
than O
the O

desired B-DAT
triplet O
(chair, O
on O
the O

left B-DAT
of, O
the O
table). O
In O

our B-DAT
full O
model, O
the O
language O

repre- B-DAT
sentation O
is O
end-to-end O
optimized O

with B-DAT
other O
parts, O
while O
it O

is B-DAT
hard O
to O
jointly O
optimize O

an B-DAT
external O
language O
parser O

like B-DAT
[31] O
for O
this O
task. O
Figure O
5 O
shows O
some O
example O

results B-DAT
on O
this O
dataset. O
It O

can B-DAT
be O
seen O
that O
although O

weakly B-DAT
supervised, O
our O
model O
not O

only B-DAT
grounds O
the O
subject O
region O

correctly B-DAT
(solid O
box), O
but O
also O

finds B-DAT
reasonable O
regions O
(dashed O
box O

) B-DAT
for O
the O
object O
entity. O
4.4. O
Answering O
pointing O
questions O
in O

Visual-7W B-DAT

Finally, B-DAT
we O
evaluate O
our O
method O

on B-DAT
the O
multiple O
choice O
pointing O

questions B-DAT
(i.e. O
“which” O
questions) O
in O

visual B-DAT
ques- O
tion O
answering O
on O

the B-DAT
Visual-7W O
dataset O
[32]. O
Given O

an B-DAT
image O
and O
a O
question O

like B-DAT
“which O
tomato O
slice O
is O

under B-DAT
the O
knife”, O
the O
task O

is B-DAT
to O
select O
the O
corresponding O

region B-DAT
from O
a O
few O
choice O

regions B-DAT
(4 O
choices O
in O
this O

dataset) B-DAT
as O
answer. O
Since O
this O

task B-DAT
is O
closely O
related O
to O

grounding B-DAT
referential O
ex- O
pressions, O
our O

model B-DAT
can O
be O
trained O
in O

the B-DAT
same O
way O
as O
in O
Sec. O
4.3 O
to O
score O
each O

choice B-DAT
region O
using O
subject O
score O

ssubj B-DAT
and O
pick O
the O
highest O

scoring B-DAT
choice O
as O
answer O

. B-DAT
As O
before, O
we O
train O
our O

model B-DAT
with O
weak O
supervision O
through O

Eqn. B-DAT
20 O
and O
use O
a O

MSCOCO-pretrained B-DAT
Faster- O
RCNN O
VGG-16 O
network O

for B-DAT
visual O
feature O
extraction. O
Here O

we B-DAT
use O
two O
different O
candidate O

bounding B-DAT
box O
setsBsubj O
and O
Bobj O

of B-DAT
the O
subject O
regions O
(the O

choices) B-DAT
and O
the O
object O
re O

- B-DAT
gions, O
where O
Bsubj O
is O

the B-DAT
4 O
choice O
bounding O
boxes, O

and B-DAT
Bobj O
is O
the O
set O

of B-DAT
300 O
proposal O
bounding O
boxes O

extracted B-DAT
using O
RPN O
in O

Faster-RCNN B-DAT
[24]. O
Similar O
to O
Sec. O
4 O

.3, B-DAT
we O
also O
train O
a O

baseline B-DAT
model O
using O
only O
a O

localization B-DAT
module O
to O
score O
each O

choice B-DAT
based O
only O
on O
its O

local B-DAT
appearance O
and O
spatial O
properties, O

and B-DAT
a O
truncated O
model O
that O

uses B-DAT
the O
Stan- O
ford O

parser B-DAT
[31, O
19] O
for O
expression O

parsing B-DAT
and O
language O
representation. O
The O
results O
are O
shown O
in O

Table B-DAT
4. O
It O
can O
be O

seen B-DAT
that O
our O
full O
model O

outperforms B-DAT
the O
baseline O
and O
the O

truncated B-DAT
model O
with O
an O
external O

parser, B-DAT
and O
achieves O
much O
higher O

accuracy B-DAT
than O
previous O
work O
[32 O

]. B-DAT
Figure O
6 O
shows O
some O

question B-DAT
answering O
examples O
on O
this O

dataset. B-DAT
5. O
Conclusion O
We O
have O
proposed O

Compositional B-DAT
Modular O
Networks, O
a O

novel B-DAT
end-to-end O
trainable O
model O
for O

handling B-DAT
relationships O
in O
referential O
expressions. O

Our B-DAT
model O
learns O
to O
parse O

input B-DAT
expressions O
with O
soft O
attention, O

and B-DAT
incorporates O
two O
types O
of O

modules B-DAT
that O
consider O
a O
region’s O

local B-DAT
features O
and O
pair- O
wise O

interaction B-DAT
between O
regions O
respectively. O
The O

model B-DAT
induces O
intuitive O
linguistic O
and O

visual B-DAT
analyses O
of O
referential O
expressions O

from B-DAT
only O
weak O
supervision, O
and O

experimen- B-DAT
tal O
results O
demonstrate O
that O

our B-DAT
approach O
outperforms O
both O
natural O

baselines B-DAT
and O
state-of-the-art O
methods O
on O

multiple B-DAT
datasets. O
Acknowledgements O
This O
work O
was O
supported O

by B-DAT
DARPA, O
AFRL, O
DoD O

MURI B-DAT
award O
N000141110688, O
NSF O
awards O

IIS-1427425, B-DAT
IIS-1212798 O
and O
IIS-1212928, O
NGA O

and B-DAT
the O
Berkeley O
Ar- O
tificial O

Intelligence B-DAT
Research O
(BAIR) O
Lab. O
Jacob O

Andreas B-DAT
is O
supported O
by O
a O

Facebook B-DAT
graduate O
fellowship O
and O
a O

Huawei B-DAT
/ O
Berkeley O
AI O
fellowship. O
References O
[1] O
M. O
Abadi, O
A O

. B-DAT
Agarwal, O
P. O
Barham, O
E. O

Brevdo, B-DAT
Z. O
Chen, O
C. O
Citro, O
G. O
S. O
Corrado O

, B-DAT
A. O
Davis, O
J. O
Dean, O

M. B-DAT
Devin, O
S. O
Ghe- O
mawat, O

I. B-DAT
Goodfellow, O
A. O
Harp, O
G. O

Irving, B-DAT
M. O
Isard, O
Y. O
Jia, O

R. B-DAT
Jozefowicz, O
L. O
Kaiser, O
M. O

Kudlur, B-DAT
J. O
Levenberg, O
D. O
Mané, O

R. B-DAT
Monga, O
S. O
Moore, O
D. O

Murray, B-DAT
C. O
Olah, O
M. O
Schuster, O

J. B-DAT
Shlens, O
B. O
Steiner, O
I. O

Sutskever, B-DAT
K. O
Talwar, O
P. O
Tucker, O

V. B-DAT
Vanhoucke, O
V. O
Vasudevan, O
F. O

Viégas, B-DAT
O. O
Vinyals, O
P. O
War- O

den, B-DAT
M. O
Wattenberg, O
M. O
Wicke, O

Y. B-DAT
Yu, O
and O
X. O
Zheng. O

Tensor- B-DAT
Flow: O
Large-scale O
machine O
learning O

on B-DAT
heterogeneous O
sys- O
tems. O
arXiv:1603.04467, O
2016 O

. B-DAT
5 O

ground-truth B-DAT
our O
prediction O
ground-truth O
our O

prediction B-DAT
ground-truth O
our O
prediction O
expression=“a O

bear B-DAT
lying O
to O
the O
right O

of B-DAT
another O
bear” O
expression=“man O
in O
sunglasses O

walking B-DAT

towards B-DAT
two O
talking O
men” O
expression=“a O

picnic B-DAT
table O
that O
has O
a O

bottle B-DAT
of O
water O
sitting O
on O
it O

” B-DAT
correct O
correct O
correct O

expression=“woman B-DAT
in O
a O
cream O
colored O

wedding B-DAT
dress O
cutting O
cake” O
expression=“a O
man O
going O
before O
a O

lady B-DAT
carrying O
a O
cellphone O

” B-DAT
expression=“pizza O
slice O
not O
eaten O

” B-DAT
correct O
correct O
incorrect O

expression=“a B-DAT
full O
grown O
brown O
bear O

near B-DAT
a O
young O
bear” O
expression=“black O
dog O
standing O
on O
all O

four B-DAT
legs O

” B-DAT
expression=“chair O
being O
sat O
in O
by O

a B-DAT
man O

” B-DAT
correct O
incorrect O
correct O
Figure O
5 O

. B-DAT
Examples O
of O
referential O
expressions O

in B-DAT
the O
Google-Ref O
dataset. O
The O

left B-DAT
column O
shows O
the O
ground-truth O

region B-DAT
and O
the O
right O
column O

shows B-DAT
the O
grounded O
subject O
region O
( O

our B-DAT
prediction) O
in O
solid O
box O

and B-DAT
the O
grounded O
object O
region O

in B-DAT
dashed O
box. O
A O
prediction O

is B-DAT
labeled O
as O
correct O
if O

the B-DAT
predicted O
subject O
region O
matches O

the B-DAT
ground-truth O
region. O
ground-truth O
our O
prediction O
ground-truth O
our O

prediction B-DAT
ground-truth O
our O
prediction O
question=“Which O

wine B-DAT
glass O
is O
in O
the O

man’s B-DAT

hand?” B-DAT
question=“Which O
person O
is O
wearing O

a B-DAT
helmet?” O
question=“Which O
mouse O
is O

on B-DAT
a O
pad O
by O
computer O

?” B-DAT
correct O
correct O
correct O

question=“Which B-DAT
head O
is O
that O
of O

an B-DAT
adult O
giraffe?” O
question=“Which O
pants O
belong O
to O
the O

man B-DAT
closest O
to O
the O
train O

?” B-DAT
question=“Which O
white O
pillow O
is O
leftmost O

on B-DAT
the O
bed O

?” B-DAT
correct O
correct O
correct O

question=“Which B-DAT
red O
shape O
is O
on O

a B-DAT
large O
white O
sign?” O
question=“Which O
is O
not O
a O
pair O

of B-DAT
a O
living O
canine O

?” B-DAT
question=“Which O
hand O
can O
be O
seen O

from B-DAT
under O
the O
umbrella O

?” B-DAT
correct O
incorrect O
correct O
Figure O
6 O

. B-DAT
Example O
pointing O
questions O
in O

the B-DAT
Visual-7W O
dataset. O
The O
left O

column B-DAT
shows O
the O
4 O
multiple O

choices B-DAT
(ground-truth O
answer O
in O
yellow) O

and B-DAT
the O
right O
column O
shows O

the B-DAT
grounded O
subject O
region O
(predicted O

answer) B-DAT
in O
solid O
box O
and O

the B-DAT
grounded O
object O
region O
in O

dashed B-DAT
box. O
A O
prediction O
is O

labeled B-DAT
as O
correct O
if O
the O

predicted B-DAT
subject O
region O
matches O
the O

ground-truth B-DAT

2] B-DAT
J. O
Andreas, O
M. O
Rohrbach, O

T. B-DAT
Darrell, O
and O
D. O
Klein. O

Learning B-DAT
to O
compose O
neural O
networks O

for B-DAT
question O
answering. O
In O
Pro- O

ceedings B-DAT
of O
the O
Conference O
of O

the B-DAT
North O
American O
Chapter O
of O

the B-DAT
Association O
for O
Computational O
Linguistics O
( O

NAACL), B-DAT
2016. O
2 O
[3] O
J. O
Andreas, O
M. O
Rohrbach O

, B-DAT
T. O
Darrell, O
and O
D. O

Klein. B-DAT
Neural O
module O
networks. O
In O

Proceedings B-DAT
of O
the O
IEEE O
Conference O

on B-DAT
Computer O
Vision O
and O
Pattern O

Recognition B-DAT
(CVPR), O
2016. O
2, O
5 O
[4] O
P. O
Arbeláez, O
J. O
Pont-Tuset O

, B-DAT
J. O
Barron, O
F. O
Marques, O

and B-DAT
J. O
Ma- O
lik. O
Multiscale O

combinatorial B-DAT
grouping. O
In O
Proceedings O
of O

the B-DAT
IEEE O
Conference O
on O
Computer O

Vision B-DAT
and O
Pattern O
Recognition O
(CVPR), O
2014 O

. B-DAT
2 O
[5] O
J. O
Ba, O
V. O
Mnih O

, B-DAT
and O
K. O
Kavukcuoglu. O
Multiple O

object B-DAT
recog- O
nition O
with O
visual O

attention. B-DAT
In O
Proceedings O
of O
the O

Inter- B-DAT
national O
Conference O
on O
Learning O

Representations B-DAT
(ICLR), O
2015. O
4 O
[6] O
J. O
Dai, O
Y. O
Li O

, B-DAT
K. O
He, O
and O
J. O

Sun. B-DAT
R-fcn: O
Object O
detection O
via O

region-based B-DAT
fully O
convolutional O
networks. O
In O

Advances B-DAT
in O
Neural O
Information O
Processing O

Systems B-DAT
(NIPS), O
2016. O
1 O
[7] O
A. O
Fukui, O
D. O
H O

. B-DAT
Park, O
D. O
Yang, O
A. O

Rohrbach, B-DAT
T. O
Darrell, O
and O
M. O

Rohrbach. B-DAT
Multimodal O
compact O
bilinear O
pooling O

for B-DAT
visual O
question O
answering O
and O

visual B-DAT
grounding. O
In O
Pro- O
ceedings O

of B-DAT
the O
Conference O
on O
Empirical O

Methods B-DAT
in O
Natural O
Language O
Processing O
( O

EMNLP), B-DAT
2016. O
2 O
[8] O
R. O
Girshick, O
J. O
Donahue O

, B-DAT
T. O
Darrell, O
and O
J. O

Malik. B-DAT
Rich O
fea- O
ture O
hierarchies O

for B-DAT
accurate O
object O
detection O
and O

semantic B-DAT
segmentation. O
In O
Proceedings O
of O

the B-DAT
IEEE O
Conference O
on O
Computer O

Vision B-DAT
and O
Pattern O
Recognition O
(CVPR), O
2014 O

. B-DAT
1, O
2 O
[9] O
X. O
Glorot O
and O
Y O

. B-DAT
Bengio. O
Understanding O
the O
difficulty O

of B-DAT
training O
deep O
feedforward O
neural O

networks. B-DAT
In O
Aistats, O
vol- O
ume O
9, O
pages O
249–256, O
2010. O
5 O

10] B-DAT
R. O
Hu, O
M. O
Rohrbach, O

and B-DAT
T. O
Darrell. O
Segmentation O
from O

nat- B-DAT
ural O
language O
expressions. O
In O

Proceedings B-DAT
of O
the O
European O
Conference O

on B-DAT
Computer O
Vision O
(ECCV), O
2016. O
6 O

11] B-DAT
R. O
Hu, O
H. O
Xu, O

M. B-DAT
Rohrbach, O
J. O
Feng, O
K. O

Saenko, B-DAT
and O
T. O
Dar- O
rell. O

Natural B-DAT
language O
object O
retrieval. O
In O

Proceedings B-DAT
of O
the O
IEEE O
Conference O

on B-DAT
Computer O
Vision O
and O
Pattern O

Recogni- B-DAT
tion O
(CVPR), O
2016. O
1, O
2 O

12] B-DAT
J. O
Johnson, O
A. O
Karpathy, O

and B-DAT
L. O
Fei-Fei. O
Densecap: O
Fully O

convolutional B-DAT
localization O
networks O
for O
dense O

captioning. B-DAT
In O
Proceedings O
of O
the O

IEEE B-DAT
Conference O
on O
Computer O
Vision O

and B-DAT
Pattern O
Recognition O
(CVPR), O
2016. O
6 O

13] B-DAT
P. O
Krähenbühl O
and O
V. O

Koltun. B-DAT
Geodesic O
object O
proposals. O
In O

Proceedings B-DAT
of O
the O
European O
Conference O

on B-DAT
Computer O
Vi- O
sion O
(ECCV), O
2014 O

. B-DAT
2 O
[14] O
R. O
Krishna, O
Y. O
Zhu O

, B-DAT
O. O
Groth, O
J. O
Johnson, O

K. B-DAT
Hata, O
J. O
Kravitz, O
S. O

Chen, B-DAT
Y. O
Kalantidis, O
L.-J. O
Li, O

D. B-DAT
A. O
Shamma, O
et O
al. O

Visual B-DAT
genome: O
Connecting O
language O
and O

vision B-DAT
using O
crowdsourced O
dense O
image O

annotations. B-DAT
arXiv O
preprint O
arXiv:1602.07332, O
2016. O
5, O
6 O

15] B-DAT
J. O
Krishnamurthy O
and O
T. O

Kollar. B-DAT
Jointly O
learning O
to O
parse O

and B-DAT
perceive: O
Connecting O
natural O
language O

to B-DAT
the O
physical O
world. O
Transactions O

of B-DAT
the O
Association O
for O
Computational O

Linguistics, B-DAT
1:193–206, O
2013. O
2 O
[16] O
T.-Y. O
Lin, O
M. O
Maire O

, B-DAT
S. O
Belongie, O
J. O
Hays, O

P. B-DAT
Perona, O
D. O
Ra- O
manan, O

P. B-DAT
Dollár, O
and O
C. O
L. O

Zitnick. B-DAT
Microsoft O
coco: O
Com- O
mon O

objects B-DAT
in O
context. O
In O
Proceedings O

of B-DAT
the O
European O
Con- O
ference O

on B-DAT
Computer O
Vision O
(ECCV), O
2014. O
6 O

17] B-DAT
W. O
Liu, O
D. O
Anguelov, O

D. B-DAT
Erhan, O
C. O
Szegedy, O
and O

S. B-DAT
Reed. O
Ssd: O
Single O
shot O

multibox B-DAT
detector. O
In O
Proceedings O
of O

the B-DAT
European O
Conference O
on O
Computer O

Vision B-DAT
(ECCV), O
2016. O
1 O
[18] O
C. O
Lu, O
R. O
Krishna O

, B-DAT
M. O
Bernstein, O
and O
L. O

Fei-Fei. B-DAT
Visual O
re- O
lationship O
detection O

with B-DAT
language O
priors. O
In O
Proceedings O

of B-DAT
the O
European O
Conference O
on O

Computer B-DAT
Vision O
(ECCV), O
2016. O
2 O
[19] O
C. O
D. O
Manning, O
M O

. B-DAT
Surdeanu, O
J. O
Bauer, O
J. O

Finkel, B-DAT
S. O
J. O
Bethard, O
and O

D. B-DAT
McClosky. O
The O
Stanford O
CoreNLP O

natural B-DAT
language O
processing O
toolkit. O
In O

Association B-DAT
for O
Computa- O
tional O
Linguistics O
( O

ACL) B-DAT
System O
Demonstrations, O
pages O
55– O
60, O
2014. O
8 O

20] B-DAT
J. O
Mao, O
J. O
Huang, O

A. B-DAT
Toshev, O
O. O
Camburu, O
A. O

Yuille, B-DAT
and O
K. O
Murphy. O
Generation O

and B-DAT
comprehension O
of O
unambiguous O
object O

descriptions. B-DAT
In O
Proceedings O
of O
the O

IEEE B-DAT
Conference O
on O
Computer O
Vision O

and B-DAT
Pattern O
Recognition O
(CVPR), O
2016. O
1, O
2, O
4, O
5, O
7, O
8 O

21] B-DAT
V. O
K. O
Nagaraja, O
V. O

I. B-DAT
Morariu, O
and O
L. O
S. O

Davis. B-DAT
Modeling O
con- O
text O
between O

objects B-DAT
for O
referring O
expression O
understanding. O

In B-DAT
Proceedings O
of O
the O
European O

Conference B-DAT
on O
Computer O
Vi- O
sion O
( O

ECCV), B-DAT
2016. O
2, O
5, O
7 O
[22] O
J. O
Pennington, O
R. O
Socher O

, B-DAT
and O
C. O
D. O
Manning. O

Glove: B-DAT
Global O
vectors O
for O
word O

representation. B-DAT
In O
Proceedings O
of O
the O

Con- B-DAT
ference O
on O
Empirical O
Methods O

in B-DAT
Natural O
Language O
Process- O
ing O
( O

EMNLP), B-DAT
2014. O
4 O
[23] O
J. O
Redmon, O
S. O
Divvala O

, B-DAT
R. O
Girshick, O
and O
A. O

Farhadi. B-DAT
You O
only O
look O
once: O

Unified, B-DAT
real-time O
object O
detection. O
In O

Pro- B-DAT
ceedings O
of O
the O
IEEE O

Conference B-DAT
on O
Computer O
Vision O
and O

Pattern B-DAT
Recognition O
(CVPR), O
2015. O
1 O
[24] O
S. O
Ren, O
K. O
He O

, B-DAT
R. O
Girshick, O
and O
J. O

Sun. B-DAT
Faster O
r-cnn: O
Towards O
real-time O

object B-DAT
detection O
with O
region O
proposal O

networks. B-DAT
In O
Advances O
in O
Neural O

Information B-DAT
Processing O
Systems O
(NIPS), O
2015. O
1, O
6, O
8 O

25] B-DAT
A. O
Rohrbach, O
M. O
Rohrbach, O

R. B-DAT
Hu, O
T. O
Darrell, O
and O

B. B-DAT
Schiele. O
Grounding O
of O
textual O

phrases B-DAT
in O
images O
by O
re- O

construction. B-DAT
In O
Proceedings O
of O
the O

European B-DAT
Conference O
on O
Computer O
Vision O
( O

ECCV), B-DAT
2016. O
1, O
2, O
6, O
7, O
8 O

26] B-DAT
M. O
Schuster O
and O
K. O

K. B-DAT
Paliwal. O
Bidirectional O
recurrent O
neural O

networks. B-DAT
IEEE O
Transactions O
on O
Signal O

Processing, B-DAT
45(11):2673–2681, O
1997. O
4 O
[27] O
K. O
Simonyan O
and O
A O

. B-DAT
Zisserman. O
Very O
deep O
convolutional O

networks B-DAT
for O
large-scale O
image O
recognition. O

In B-DAT
Proceedings O
of O
the O
International O

Conference B-DAT
on O
Learning O
Representations O
(ICLR), O
2015 O

. B-DAT
4, O
6 O
[28] O
J. O
R. O
Uijlings, O
K O

. B-DAT
E. O
van O
de O
Sande, O

T. B-DAT
Gevers, O
and O
A. O
W. O

Smeulders. B-DAT
Selective O
search O
for O
object O

recognition. B-DAT
Inter- O
national O
journal O
of O

computer B-DAT
vision, O
104(2):154–171, O
2013. O
2 O
[29] O
M. O
Werning, O
W. O
Hinzen O

, B-DAT
and O
E. O
Machery. O
The O

Oxford B-DAT
hand- O
book O
of O
compositionality. O

Oxford B-DAT
University O
Press, O
2012. O
2 O
[30] O
L. O
Yu, O
P. O
Poirson O

, B-DAT
S. O
Yang, O
A. O
C. O

Berg, B-DAT
and O
T. O
L. O
Berg. O

Mod- B-DAT
eling O
context O
in O
referring O

expressions. B-DAT
In O
Proceedings O
of O
the O

European B-DAT
Conference O
on O
Computer O
Vision O
( O

ECCV), B-DAT
2016. O
2, O
7 O

31] B-DAT
M. O
Zhu, O
Y. O
Zhang, O

W. B-DAT
Chen, O
M. O
Zhang, O
and O

J. B-DAT
Zhu. O
Fast O
and O
accurate O

shift-reduce B-DAT
constituent O
parsing. O
In O
ACL O
(1 O

), B-DAT
pages O
434–443, O
2013. O
4, O
8 O

32] B-DAT
Y. O
Zhu, O
O. O
Groth, O

M. B-DAT
Bernstein, O
and O
L. O
Fei-Fei. O

Visual7w: B-DAT
Grounded O
question O
answering O
in O

images. B-DAT
arXiv O
preprint O
arXiv:1511.03416, O
2015. O
5, O
8 O

33] B-DAT
C. O
L. O
Zitnick O
and O

P. B-DAT
Dollár. O
Edge O
boxes: O
Locating O

object B-DAT
proposals O
from O
edges. O
In O

Proceedings B-DAT
of O
the O
European O
Con- O

ference B-DAT
on O
Computer O
Vision O
(ECCV), O
2014 O

. B-DAT
2 O

ages O
and O
expressions O
in O
the O
Visual B-DAT
Genome I-DAT
dataset O
[14] O
and O
Google-Ref O
dataset O

4.2. O
Localizing O
relationships O
in O
Visual B-DAT
Genome I-DAT

evaluate O
our O
method O
on O
the O
Visual B-DAT
Genome I-DAT
dataset O
[14], O
which O
contains O
relationship O

On O
the O
relationship O
annotations O
in O
Visual B-DAT
Genome, I-DAT
given O
an O
image O
and O
an O

model O
on O
relationship O
expressions O
in O
Visual B-DAT
Genome I-DAT
dataset. O
See O
Sec. O
4.2 O
for O

grounded O
relationship O
expressions O
in O
the O
Visual B-DAT
Genome I-DAT
dataset, O
trained O
with O
weak O
supervision O

and O
expressions O
in O
the O
Visual O
Genome B-DAT
dataset O
[14] O
and O
Google-Ref O
dataset O

4.2. O
Localizing O
relationships O
in O
Visual O
Genome B-DAT

our O
method O
on O
the O
Visual O
Genome B-DAT
dataset O
[14], O
which O
contains O
relationship O

the O
relationship O
annotations O
in O
Visual O
Genome, B-DAT
given O
an O
image O
and O
an O

on O
relationship O
expressions O
in O
Visual O
Genome B-DAT
dataset. O
See O
Sec. O
4.2 O
for O

relationship O
expressions O
in O
the O
Visual O
Genome B-DAT
dataset, O
trained O
with O
weak O
supervision O

final O
scores O
on O
region O
pairs B-DAT

region O
pairs B-DAT

module O
produces O
scores O
over O
region O
pairs B-DAT

into O
final O
scores O
over O
region O
pairs, B-DAT
producing O
the O
top O
region O
pair O

determining O
the O
relationship O
between O
two O
pairs B-DAT
of O
bounding O
boxes O
by O
outputting O

pairwise O
scores O
over O
region-region O
pairs B-DAT

inter-object O
relationships O
by O
looking O
at O
pairs B-DAT
of O
regions O
(bi, O
bj). O
For O

relationship O
expressions O
anno- O
tated O
over O
pairs B-DAT
of O
objects, O
such O
as O
“computer O

evaluation O
scenario O
of O
retrieving O
subject-object O
pairs, B-DAT
our O
weakly O
supervised O
model O
still O

second O
column O
show O
ground-truth O
region O
pairs B-DAT
and O
our O
predicted O
region O
pairs O

ages O
and O
expressions O
in O
the O
Visual B-DAT
Genome O
dataset O
[14] O
and O
Google-Ref O

the O
pointing O
questions O
in O
the O
Visual B-DAT

4.2. O
Localizing O
relationships O
in O
Visual B-DAT
Genome O

evaluate O
our O
method O
on O
the O
Visual B-DAT
Genome O
dataset O
[14], O
which O
contains O

On O
the O
relationship O
annotations O
in O
Visual B-DAT
Genome, O
given O
an O
image O
and O

model O
on O
relationship O
expressions O
in O
Visual B-DAT
Genome O
dataset. O
See O
Sec. O
4.2 O

grounded O
relationship O
expressions O
in O
the O
Visual B-DAT
Genome O
dataset, O
trained O
with O
weak O

on O
the O
pointing O
questions O
in O
Visual B-DAT

4.4. O
Answering O
pointing O
questions O
in O
Visual B-DAT

ques- O
tion O
answering O
on O
the O
Visual B-DAT

Example O
pointing O
questions O
in O
the O
Visual B-DAT

D. O
A. O
Shamma, O
et O
al. O
Visual B-DAT
genome: O
Connecting O
language O
and O
vision O

M. O
Bernstein, O
and O
L. O
Fei-Fei. O
Visual B-DAT
re- O
lationship O
detection O
with O
language O

final O
scores O
on O
region O
pairs B-DAT

region O
pairs B-DAT

module O
produces O
scores O
over O
region O
pairs B-DAT

into O
final O
scores O
over O
region O
pairs, B-DAT
producing O
the O
top O
region O
pair O

determining O
the O
relationship O
between O
two O
pairs B-DAT
of O
bounding O
boxes O
by O
outputting O

pairwise O
scores O
over O
region-region O
pairs B-DAT

inter-object O
relationships O
by O
looking O
at O
pairs B-DAT
of O
regions O
(bi, O
bj). O
For O

relationship O
expressions O
anno- O
tated O
over O
pairs B-DAT
of O
objects, O
such O
as O
“computer O

evaluation O
scenario O
of O
retrieving O
subject-object O
pairs, B-DAT
our O
weakly O
supervised O
model O
still O

second O
column O
show O
ground-truth O
region O
pairs B-DAT
and O
our O
predicted O
region O
pairs O

on O
two O
benchmark O
biomedical O
corpora, O
GENIA B-DAT
and O
CRAFT. O
To O
the O
best O

on O
two O
benchmark O
biomedical O
corpora O
GENIA B-DAT
and O
CRAFT. O
We O
also O
perform O

linguistically-annotated O
resources, O
notably O
including O
the O
GENIA B-DAT
[2] O
and O
CRAFT O
[3] O
corpora O

We O
use O
two O
biomedical O
corpora: O
GENIA B-DAT
[2] O
and O
CRAFT O
[3]. O
GENIA O

The O
GENIA B-DAT
corpus O
contains O
18K O
sentences O
(∼486K O

for O
POS O
tagging O
over O
both O
GENIA B-DAT
and O
CRAFT. O
We O
consider O
the O

denote O
the O
occurrence O
proportions O
in O
GENIA B-DAT
and O
CRAFT, O
respectively O

GENIA B-DAT
CRAFT O
Type O
% O
Type O

ADV O
4.0 O
CC O
3.6 O
3.2 O
GENIA B-DAT
< O
−5 O
4.1 O
3.9 O
amod O

of O
at O
least O
0.17% O
in O
GENIA B-DAT
and O
0.26% O
in O
CRAFT O
between O

Model O
GENIA B-DAT
CRAFT O
MarMoT O
98.61 O
97.07 O
jPTDP-v1 O

Stanford O
tagger O
[?] O
98.37 O
GENIA B-DAT
tagger O
[?] O
98.49 O

published O
re- O
sults O
of O
the O
GENIA B-DAT
POS O
tagger O
[36], O
when O
trained O

on O
90% O
of O
the O
GENIA B-DAT
corpus O
(cf. O
our O
85% O
training O

obtain O
the O
lowest O
scores O
on O
GENIA B-DAT
and O
CRAFT, O
respectively. O
jPTDP O
obtains O

score O
to O
Mar- O
MoT O
on O
GENIA B-DAT
and O
similar O
score O
to O
BiLSTM-CRF O

at O
98.61% O
and O
97.07% O
on O
GENIA B-DAT
and O
CRAFT, O
which O
are O
about O

0–18, O
the O
accuracies O
for O
the O
GENIA B-DAT
tagger, O
Stanford O
tagger, O
MarMoT, O
NLP4J O

and O
predicted O
POS O
tags O
on O
GENIA, B-DAT
i.e. O
92.51 O
vs. O
92.31 O
and O

16]. O
The O
larger O
improvements O
on O
GENIA B-DAT
and O
CRAFT O
show O
that O
character-level O

On O
both O
GENIA B-DAT
and O
CRAFT, O
BiLSTM-CRF O
with O
character-level O

results O
on O
the O
GENIA B-DAT
test O
set O
of O
“pre-trained” O
parsers O

use O
the O
data O
split O
on O
GENIA B-DAT
as O
used O
in O
[10], O
therefore O

On O
GENIA, B-DAT
among O
pre-trained O
models, O
BLLIP O
ob O

trained O
models, O
was O
trained O
using O
GENIA, B-DAT
so O
this O
re- O
sult O
is O

the O
pre-trained O
Stanford-NNdep O
model O
on O
GENIA B-DAT

retrained O
parsing O
models, O
on O
both O
GENIA B-DAT
and O
CRAFT, O
Stanford-Biaffine O
achieves O
the O

GENIA B-DAT
– O
sentence O
length) O
(CRAFT O

GENIA B-DAT
– O
dependency O
distance) O
(CRAFT O

and O
UAS O
at O
92.64% O
on O
GENIA, B-DAT
and O
LAS O
at O
90.77% O
and O

absolute O
lower O
than O
Stanford-Biaffine O
on O
GENIA B-DAT
and O
CRAFT, O
respectively. O
jPTDP O
is O

without O
punctuation) O
than O
NLP4J-dep O
on O
GENIA B-DAT
and O
CRAFT, O
respectively. O
Table O
4 O

sentences O
<=10 O
words. O
Exceptionally, O
on O
GENIA B-DAT
we O
find O
lower O
scores O
for O

in O
the O
first O
bin O
on O
GENIA B-DAT
are O
relatively O
long, O
with O
an O

F1) O
scores O
of O
Stanford-Biaffine O
on O
GENIA, B-DAT
by O
frequent O
dependency O
labels O
in O

one O
surprising O
excep- O
tion: O
on O
GENIA, B-DAT
in O
distance O
bins O
of O
−4 O

characteristics O
of O
sentences O
in O
the O
GENIA B-DAT
corpus. O
Ta- O
ble O
5 O
details O

frequent O
dependency O
relation O
types O
on O
GENIA B-DAT
and O
CRAFT, O
respectively. O
In O
most O

with O
the O
following O
exceptions: O
on O
GENIA, B-DAT
jPTDP O
gets O
the O
highest O
results O

and O
num, O
respec- O
tively. O
On O
GENIA B-DAT
the O
labels O
associated O
with O
the O

basic O
Stanford O
dependency O
labels O
on O
GENIA B-DAT

generally O
<70%) O
are O
dep O
on O
GENIA B-DAT
and O
DEP, O
LOC, O
PRN O
and O

Type O
GENIA B-DAT
CRAFT O

is O
relatively O
rare O
tag O
in O
GENIA B-DAT
and O
is O
the O
least O
fre O

second O
least O
frequent O
one O
in O
GENIA B-DAT
and O
CRAFT, O
respectively, O
and O
generally O

ranges O
across O
parsers O
on O
both O
GENIA B-DAT
and O
CRAFT. O
The O
re- O
sults O

higher O
scores O
for O
IN O
on O
GENIA B-DAT
than O
on O
CRAFT, O
and O
vice O

lower O
scores O
for O
VB O
on O
GENIA B-DAT

. O
This O
is O
because O
on O
GENIA, B-DAT
IN O
is O
mostly O
coupled O
with O

words O
at O
15%, O
while O
on O
GENIA B-DAT
it O
associates O
with O
longer O
dependency O

VB O
on O
CRAFT O
than O
on O
GENIA B-DAT

the O
intersected O
parsing O
errors O
on O
GENIA B-DAT
and O
12% O
on O
CRAFT O
are O

intersected O
across O
all O
parsers O
on O
GENIA B-DAT
and O
0.5% O
on O
CRAFT). O
Based O

which O
was O
derived O
from O
the O
GENIA B-DAT
treebank O
cor- O
pus O
(800, O
150 O

files O
are O
included O
in O
the O
GENIA B-DAT
treebank O
training O
set O

dataset O
is O
extracted O
from O
the O
GENIA B-DAT
treebank O
training O
set. O
Although O
gold O

for O
parsers O
trained O
with O
the O
GENIA B-DAT
tree- O
bank O
(Rows O
1-6, O
Table O

the O
Stanford&Paris O
team O
[50] O
employ O
GENIA B-DAT
data, O
ob- O
taining O
the O
highest O

was O
a O
subset O
of O
the O
GENIA B-DAT
corpus. O
However, O
we O
find O
that O

four O
dependency O
parsers O
trained O
on O
GENIA, B-DAT
Stanford-Biaffine, O
jPTDP O
and O
NLP4J-dep O
produce O

set O
(except O
NLP4J-dep O
trained O
on O
GENIA B-DAT
and O
Stanford- O
NNdep O
trained O
on O

on O
two O
benchmark O
biomedical O
corpora O
GENIA B-DAT
and O
CRAFT. O
In O
particular, O
BiLSTM-CRF-based O

the O
GENIA B-DAT
Corpus. O
In: O
Proceedings O
of O
the O

LAS O
UAS B-DAT
LAS I-DAT
UAS O
LAS O
UAS O

are O
the O
labeled O
attachment O
score O
(LAS) B-DAT
and O
unlabeled O
attach- O
ment O
score O

UAS): O
LAS B-DAT
is O
the O
proportion O
of O
words O

Exact O
match O
Overall O
Exact O
match O
LAS B-DAT
UAS O
LAS O
UAS O
LAS O
UAS O

LAS B-DAT
UAS O

parsing O
results O
We O
present O
the O
LAS B-DAT
and O
UAS O
scores O
of O
different O

Figure O
2 O
LAS B-DAT
scores O
by O
sentence O
length O

Figure O
3 O
LAS B-DAT
(F1) O
scores O
by O
dependency O
distance O

highest O
parsing O
results O
with O
LAS B-DAT
at O
91.23% O
and O
UAS O
at O

92.64% O
on O
GENIA, O
and O
LAS B-DAT
at O
90.77% O
and O
UAS O
at O

following O
more O
detailed O
analyses O
report O
LAS B-DAT
scores, O
computed O
without O
punc- O
tuation O

Figure O
2 O
presents O
LAS B-DAT
scores O
by O
sentence O
length O
in O

Figure O
3 O
shows O
LAS B-DAT
(F1) O
scores O
corresponding O
to O
the O

Table O
5 O
LAS B-DAT
(F1) O
scores O
of O
Stanford-Biaffine O
on O

Prop. O
LAS B-DAT
Prop. O
LAS O
Prop. O
LAS O
advmod O
7.2 O
94.62 O
4.2 O
90.91 O

Tables O
6 O
and O
7 O
present O
LAS B-DAT
scores O
for O
the O
most O
frequent O

with O
the O
high- O
est O
average O
LAS B-DAT
scores O
(generally O
>90%) O
are O
amod O

Table O
6 O
LAS B-DAT
by O
the O
basic O
Stanford O
dependency O

Table O
7 O
LAS B-DAT
by O
the O
CoNLL O
2008 O
dependency O

relation O
types O
with O
the O
lowest O
LAS B-DAT
scores O
(generally O
<70%) O
are O
dep O

Table O
8 O
LAS B-DAT
by O
POS O
tag O
of O
the O

dependent O
Table O
8 O
analyzes O
the O
LAS B-DAT
scores O
by O
the O
most O
fre O

of O
90% O
(thus O
their O
corresponding O
LAS B-DAT
scores O
in O
tables O
8 O
and O

Table O
10 O
UAS O
and O
LAS B-DAT
(F1) O
scores O
of O
re-trained O
models O

UAS O
95.51 O
93.14 O
92.50 O
91.02 O
LAS B-DAT
94.82 O
92.18 O
91.96 O
90.30 O

presents O
the O
intrinsic O
UAS O
and O
LAS B-DAT
(F1) O
scores O
on O
the O
pre-processed O

the O
similar O
intrinsic O
UAS O
and O
LAS B-DAT
scores O
on O
the O
pre-processed O
segmented O

State-of-the-art; O
LAS B-DAT

on O
two O
benchmark O
biomedical O
corpora, O
GENIA B-DAT

on O
two O
benchmark O
biomedical O
corpora O
GENIA B-DAT

GENIA B-DAT
[2] I-DAT
and O
CRAFT O
[3]. O
GENIA O

The O
GENIA B-DAT

for O
POS O
tagging O
over O
both O
GENIA B-DAT

denote O
the O
occurrence O
proportions O
in O
GENIA B-DAT

GENIA B-DAT

of O
at O
least O
0.17% O
in O
GENIA B-DAT

Model O
GENIA B-DAT

Stanford O
tagger O
[?] O
98.37 O
GENIA B-DAT

published O
re- O
sults O
of O
the O
GENIA B-DAT

trained O
on O
90% O
of O
the O
GENIA B-DAT

obtain O
the O
lowest O
scores O
on O
GENIA B-DAT

score O
to O
Mar- O
MoT O
on O
GENIA B-DAT

at O
98.61% O
and O
97.07% O
on O
GENIA B-DAT

0–18, O
the O
accuracies O
for O
the O
GENIA B-DAT

16]. O
The O
larger O
improvements O
on O
GENIA B-DAT

On O
both O
GENIA B-DAT

results O
on O
the O
GENIA B-DAT

use O
the O
data O
split O
on O
GENIA B-DAT

retrained O
parsing O
models, O
on O
both O
GENIA B-DAT

absolute O
lower O
than O
Stanford-Biaffine O
on O
GENIA B-DAT

without O
punctuation) O
than O
NLP4J-dep O
on O
GENIA B-DAT

sentences O
<=10 O
words. O
Exceptionally, O
on O
GENIA B-DAT

in O
the O
first O
bin O
on O
GENIA B-DAT

characteristics O
of O
sentences O
in O
the O
GENIA B-DAT

frequent O
dependency O
relation O
types O
on O
GENIA B-DAT

and O
num, O
respec- O
tively. O
On O
GENIA B-DAT

generally O
<70%) O
are O
dep O
on O
GENIA B-DAT

Type O
GENIA B-DAT

is O
relatively O
rare O
tag O
in O
GENIA B-DAT

second O
least O
frequent O
one O
in O
GENIA B-DAT

ranges O
across O
parsers O
on O
both O
GENIA B-DAT

higher O
scores O
for O
IN O
on O
GENIA B-DAT

words O
at O
15%, O
while O
on O
GENIA B-DAT

the O
intersected O
parsing O
errors O
on O
GENIA B-DAT

intersected O
across O
all O
parsers O
on O
GENIA B-DAT

which O
was O
derived O
from O
the O
GENIA B-DAT

files O
are O
included O
in O
the O
GENIA B-DAT

dataset O
is O
extracted O
from O
the O
GENIA B-DAT

for O
parsers O
trained O
with O
the O
GENIA B-DAT

the O
Stanford&Paris O
team O
[50] O
employ O
GENIA B-DAT

was O
a O
subset O
of O
the O
GENIA B-DAT

set O
(except O
NLP4J-dep O
trained O
on O
GENIA B-DAT

on O
two O
benchmark O
biomedical O
corpora O
GENIA B-DAT

the O
GENIA B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lel O
repositories O
of O
PubMed[1] O
(abstracts O

- B-DAT
able O
rate O
of O
over O
one O

- B-DAT
fort O
to O
catalog O
the O
key O

- B-DAT
cations O
demands O
automation O
[1]. O
Hence O

- B-DAT
sis O
on O
the O
value O
of O

- B-DAT

- B-DAT
ical O
publications, O
based O
on O
the O

- B-DAT
stantive O
differences O
between O
these O
domain O

- B-DAT
tic O
information O
[5]. O
Despite O
this O

- B-DAT
alyze O
the O
syntactic O
characteristics O
of O

- B-DAT

- B-DAT
cant O
improvements O
in O
parsing O
performance O

- B-DAT

- B-DAT

- B-DAT
of-the-art O
(SOTA) O
approaches O
to O
dependency O

- B-DAT
tailed O
results O
on O
the O
precursor O

- B-DAT
trinsic O
parser O
evaluation O
shared O
task O

- B-DAT
ences O
in O
overall O
intrinsic O
parser O

- B-DAT
tion O
performance O

- B-DAT
ing O
models O
on O
benchmark O
biomedical O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferences O
between O
the O
abstracts O
and O

- B-DAT
ing O
full O
text O
publications O
[9 O

- B-DAT
ually O
annotated O
following O
the O
Penn O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
version O
toolkit O
(v3.5.1) O
to O
generate O

- B-DAT

- B-DAT
ing O
dependency O
trees. O
Hence, O
a O

- B-DAT
ments. O
We O
use O
sentences O
from O

- B-DAT
tences O
from O
the O
next O
6 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
based O
models O
for O
POS O
tagging O

- B-DAT

- B-DAT
phological O
tagger.[5 O

- B-DAT

- B-DAT
tomatically O
optimizes O
feature O
combinations.[6 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
3.5 O
−5 O
1.2 O
1.2 O
appos O

-20 B-DAT
31.0 O
−4 O
2.1 O
2.1 O
aux O

-30 B-DAT
35.7 O
−3 O
4.4 O
3.2 O
auxpass O

-40 B-DAT
19.4 O
−2 O
10.6 O
8.5 O
cc O

-50 B-DAT
7.1 O
−1 O
24.1 O
21.7 O
conj O

-10 B-DAT
17.8 O
4 O
4.0 O
3.4 O
mark O

-20 B-DAT
23.1 O
5 O
2.4 O
2.3 O
nn O

-30 B-DAT
25.2 O
> O
5 O
12.3 O
11.6 O

VBN O
3.1 O
3.8 O
31-40 B-DAT
17.5 O
- O
- O
- O
nsubjpass O
1.4 O
SBJ O

VBP O
1.4 O
1.1 O
41-50 B-DAT
9.3 O
- O
- O
- O
num O
1.2 O
SUB O

1.9 O
1.4 O
> O
50 O
7.1 O
- B-DAT
- O
- O
pobj O
12.2 O
TMP O

0.9 O
- B-DAT
- O
- O
- O
- O
- O
- O
- O
prep O
12.3 O

VC O
2.4 O
- B-DAT
- O
- O
- O
- O
- O
- O
- O
punct O

- B-DAT

- B-DAT
work O
[17, O
18] O
with O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dings. O
For O
each O
word O
token O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
STM O
to O
each O
word’s O
character O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
from O
[22].[7] O
As O
detailed O

- B-DAT

- B-DAT

- B-DAT
bank O
identified O
the O
domain-retrained O
ClearParser O

- B-DAT

- B-DAT
mains O
the O
best O
performing O
non-neural O

- B-DAT
pendency O
parsing. O
In O
particular, O
we O

- B-DAT
lowing O
parsers O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
based O
parsing O
model O
which O
concatenates O

- B-DAT
layer O
perceptron O
with O
one O
hidden O

- B-DAT
tion O
classification.[8 O

- B-DAT
dep) O
is O
a O
transition-based O
parser O

- B-DAT
tional O
branching O
method O
that O
uses O

- B-DAT
timates O
to O
decide O
when O
employing O

- B-DAT

- B-DAT
pendency O
parser O
bmstparser O
[28], O
replacing O

- B-DAT

- B-DAT

- B-DAT
pendency O
arcs O
and O
labels, O
obtaining O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
velopment O
set. O
Then O
we O
report O

- B-DAT
curacy. O
The O
metrics O
for O
dependency O

- B-DAT
ment O
score O
(UAS): O
LAS O
is O

- B-DAT

- B-DAT

- B-DAT
NNdep, O
jPTDP O
and O
Stanford-Biaffine O
which O

- B-DAT
lizes O
pre-trained O
word O
embeddings, O
we O

- B-DAT
dimensional O
pre-trained O
word O
vectors O
from O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nal O
pure O
Java O
implementations O
with O

- B-DAT
parameter O
settings O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
parameters O
to O
select O
the O
number O

- B-DAT
velopment O
set O
is O
obtained O
after O

- B-DAT

- B-DAT
parameters O
with O
their O
default O
values O

- B-DAT

- B-DAT
dings O
and O
fix O
the O
initial O

- B-DAT
lect O
the O
number O
of O
LSTM O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults O
of O
the O
GENIA O
POS O

- B-DAT

- B-DAT
MoT O
obtain O
the O
lowest O
scores O

- B-DAT
MoT O
on O
GENIA O
and O
similar O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
viding O
useful O
extra O
information O

- B-DAT

- B-DAT
NIA O
and O
97.25% O
on O
CRAFT O

- B-DAT

- B-DAT
POS, O
BiLSTM-CRF O
and O
BiLSTM-CRF+CNN-char O
on O

-24 B-DAT
were O
reported O
at O
97.05%, O
97.23 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
NIA O
and O
CRAFT, O
respectively, O
resulting O

- B-DAT
est O
accuracies O
on O
both O
experimental O

- B-DAT

- B-DAT

- B-DAT
CRF O
[16]. O
The O
larger O
improvements O

- B-DAT

- B-DAT

- B-DAT
dings O
are O
useful O
for O
morphologically O

- B-DAT

- B-DAT
tions O
which O
are O
also O
out-of-vocabulary O

- B-DAT

- B-DAT
ture O
those O
unseen O
abbreviated O
words O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cal O
factor O
in O
parsing O
performance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
[23] O
to O
generate O
POS O

- B-DAT
ing O
models O
in O
Table O
4 O

- B-DAT

- B-DAT

- B-DAT

-4 B-DAT
present O
scores O
of O
these O
pre O

- B-DAT
trained O
models O
with O
POS O
tags O

- B-DAT
POS. O
Both O
pre-trained O
NNdep O
and O

- B-DAT
tences, O
which O
was O
converted O
from O

- B-DAT
tions O
2–21. O
The O
fifth O
row O

- B-DAT

- B-DAT
erate O
dependency O
trees O
with O
the O

- B-DAT
dencies O
and O
use O
the O
data O

- B-DAT
maining O
rows O
show O
results O
of O

- B-DAT

- B-DAT
tains O
highest O
results. O
This O
model O

- B-DAT
trained O
models, O
was O
trained O
using O

- B-DAT
sult O
is O
unsurprising. O
The O
pre-trained O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
prising O
because O
the O
pre-trained O
Stanford-Biaffine O

- B-DAT
lizes O
pre-trained O
word O
vectors O
which O

- B-DAT

- B-DAT
mance O
differences O
irrespective O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

5 O
-5 B-DAT
-4 O
-3 O
-2 O
-1 O
1 O
2 O
3 O
4 O
5 O

- B-DAT

- B-DAT

- B-DAT

5 O
-5 B-DAT
-4 O
-3 O
-2 O
-1 O
1 O
2 O
3 O
4 O
5 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions. O
Stanford-NNdep O
obtains O
the O
lowest O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stead O
of O
predicted O
POS O
tags O

- B-DAT
sults O
obtained O
by O
the O
retrained O

- B-DAT
tuation. O
Using O
UAS O
scores O
or O

- B-DAT
ation O
does O
not O
reveal O
any O

- B-DAT
ative O
to O
short O
sentences O
<=10 O

- B-DAT
NIA O
(cf. O
17.8% O
on O
CRAFT O

- B-DAT

- B-DAT
dices O
of O
words O
in O
a O

- B-DAT
jectives O
or O
pronouns O
modifying O
their O

- B-DAT
pendencies. O
This O
is O
not O
completely O

- B-DAT
glish O
is O
strongly O
head-initial. O
In O

- B-DAT
tween O
LSTM-based O
models O
(i.e. O
Stanford-Biaffine O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion: O
on O
GENIA, O
in O
distance O

- B-DAT

- B-DAT
ble O
5 O
details O
the O
scores O

- B-DAT

- B-DAT
most O
dependency O
bins. O
We O
find O

- B-DAT

- B-DAT

- B-DAT
tively. O
On O
GENIA O
the O
labels O

- B-DAT
est O
average O
LAS O
scores O
(generally O

- B-DAT
bels O
either O
correspond O
to O
short O

- B-DAT
eral O
labels O
while O
LOC, O
PRN O

- B-DAT

- B-DAT
ation O
across O
parsers. O
These O
9 O

- B-DAT
bels O
generally O
correspond O
to O
long O

- B-DAT
fore, O
it O
is O
not O
surprising O

- B-DAT

- B-DAT
els O
Stanford-Biaffine O
and O
jPTDP O
can O

- B-DAT

- B-DAT
els O
NLP4J-dep O
and O
NNdep O

- B-DAT
quent O
POS O
tags O
(across O
two O

- B-DAT
dent. O
Stanford-Biaffine O
achieves O
the O
highest O

- B-DAT
based O
model O
NLP4J-dep O
obtains O
the O

- B-DAT
quent O
tag O
in O
CRAFT O
among O

- B-DAT
ordinating O
conjunction O
tag O
CC O
also O

- B-DAT
sults O
for O
CC O
are O
consistent O

- B-DAT
tion O
of O
dependency O
labels O
such O

-1 B-DAT

- B-DAT

- B-DAT

- B-DAT
sistently O
across O
all O
parsers O
(i.e O

- B-DAT
terns. O
The O
first O
one O
is O

- B-DAT
diction O
(8% O
of O
the O
intersected O

- B-DAT
dicted O
POS O
tags). O
For O
example O

- B-DAT
mains” O
is O
the O
head O
of O

- B-DAT

- B-DAT

- B-DAT
gers) O
produced O
an O
incorrect O
tag O

- B-DAT
jective O
(JJ) O
for O
“POU(S)”. O
As O

-1 B-DAT

- B-DAT

- B-DAT
GCAAAT” O
(in O
Table O
9) O
and O

-1 B-DAT
coregulator O
VP16”, O
commonly O
referred O
to O

- B-DAT
positive O
structures, O
where O
the O
second O

- B-DAT

- B-DAT
dency O
arc O
with O
the O
same O

- B-DAT
dency O
parsers O
for O
the O
downstream O

- B-DAT

- B-DAT
trained” O
parsers O
with O
3 O
different O

- B-DAT
quires O
use O
of O
a O
currently O

- B-DAT
mance O
on O
downstream O
tasks O
[47 O

- B-DAT
cal O
event O
extraction O
task O
[8 O

- B-DAT
imental O
setup O
used O
there; O
employing O

- B-DAT
pus O
(800, O
150 O
and O
260 O

- B-DAT

- B-DAT
segmented O
data O
provided O
by O
the O

- B-DAT

- B-DAT

- B-DAT
NNdep O
parsers O
that O
require O
predicted O

- B-DAT

-2017 B-DAT
[14]678 O
of O
800 O
training, O
132 O

- B-DAT

- B-DAT

-2009 B-DAT
development O
sentences O
which O
contain O
event O

- B-DAT
dency O
parsing O
models O

- B-DAT
opment O
data O
(gold O
event O
annotations O

- B-DAT
able O
to O
public O
for O
training O

- B-DAT
uation O
system. O
The O
online O
evaluation O

- B-DAT
proximate O
span O
& O
recursive O
evaluation O

- B-DAT

- B-DAT
dicted O
segmentation), O
for O
which O
these O

- B-DAT
tain O
event O
interactions. O
These O
scores O

- B-DAT

- B-DAT

- B-DAT
NIA O
treebank O
data.[16] O
The O
second O

- B-DAT

- B-DAT
ing O
rows O
show O
scores O
using O

- B-DAT

- B-DAT
bank O
(Rows O
1-6, O
Table O
11 O

- B-DAT

- B-DAT

- B-DAT
ating O
different O
dependency O
representations O
in O

- B-DAT
stream O
tasks, O
not O
on O
comparing O

- B-DAT
taining O
the O
highest O
biomedical O
event O

-2017 B-DAT
http://bionlp-st.dbcls.jp/GE/2011/eval-test/eval.cgi O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ences O
in O
intrinsic O
parsing O
results O

- B-DAT
tending O
preliminary O
related O
observations O
in O

- B-DAT

- B-DAT

- B-DAT
ment O
set, O
while O
on O
the O

- B-DAT
dep O
obtain O
the O
lowest O
and O

- B-DAT
dency O
structures O
only O
(i.e. O
results O

- B-DAT
bels O
by O
“UNK” O
before O
training O

- B-DAT

- B-DAT

- B-DAT
NNdep O
trained O
on O
CRAFT). O
Without O

- B-DAT
bels, O
better O
event O
extraction O
scores O

- B-DAT
dition, O
the O
differences O
in O
these O

- B-DAT
mance. O
Some O
(predicted) O
dependency O
labels O

- B-DAT
quent O
dependency O
labels O
in O
each O

- B-DAT
paring O
SOTA O
traditional O
feature-based O
and O

- B-DAT

- B-DAT
dency O
parsing O
in O
the O
biomedical O

- B-DAT

- B-DAT
els O
on O
two O
benchmark O
biomedical O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
est O
POS O
tagging O
accuracies O
which O

- B-DAT

- B-DAT

- B-DAT
ing O
model O
obtains O
significantly O
better O

- B-DAT
tion O
performance. O
Whether O
this O
pattern O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Technology O
- B-DAT
Volume O
1, O
pp. O
173–180 O
(2003 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

papers. O
The O
SOTA O
result O
for O
GENIA B-DAT
is O

DEP O
GENIA B-DAT
(Kim O
et O
al., O
2003 O

GENIA B-DAT
(Kim O
et O
al., O
2003 O

Tateisi, O
and O
Jun’ichi O
Tsujii. O
2003. O
GENIA B-DAT
corpus O
- O
a O
semanti- O
cally O

DEP O
since O
we O
already O
include O
LAS B-DAT

Kim O
et O
al., O
2003) O
- O
LAS B-DAT
91.92 O
90.22 O
90.33 O
90.36 O
90.43 O

For O
DEP, O
we O
report O
labeled O
(LAS) B-DAT
and O
unlabeled O
(UAS O

model O
with O
hyperparameters O
tuned O
for O
LAS B-DAT

papers. O
The O
SOTA O
result O
for O
GENIA B-DAT

Tateisi, O
and O
Jun’ichi O
Tsujii. O
2003. O
GENIA B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-70 B-DAT
days O
(Dettmers, O
2019) O
on O
an O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-6, B-DAT
1e-5, O
2e-5, O
or O
5e-5 O
with O

-5 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

GENIA O
(Kim O
et O
al., O
2003) O
- B-DAT
LAS O
91.92 O
90.22 O
90.33 O
90.36 O

GENIA O
(Kim O
et O
al., O
2003) O
- B-DAT
UAS O
92.84 O
91.84 O
91.89 O
92.00 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Hung O
Weng, O
Di O
Jin, O
Tristan O

- B-DAT
cal O
bert O
embeddings. O
In O
ClinicalNLP O

- B-DAT
ula, O
Iz O
Beltagy, O
Miles O
Crawford O

- B-DAT
son O
Dunkelberger, O
Ahmed O
Elgohary, O
Sergey O

- B-DAT
man, O
Vu O
Ha, O
Rodney O
Kinney O

- B-DAT

- B-DAT
ters, O
Joanna O
Power, O
Sam O
Skjonsberg O

- B-DAT
ture O
graph O
in O
semantic O
scholar O

- B-DAT

- B-DAT
tics O

- B-DAT

- B-DAT

- B-DAT
BA/BioNLP O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-02 B-DAT

-22 B-DAT

- B-DAT

- B-DAT

- B-DAT
standing. O
In O
NAACL-HLT O

- B-DAT
ease O
name O
recognition O
and O
concept O

- B-DAT
ing. O
ICLR O

- B-DAT

- B-DAT
dicting O
hospital O
readmission. O
arXiv:1904.05342 O

- B-DAT
mad O
Ghassemi, O
Benjamin O
Moody, O
Peter O

-1361 B-DAT
http://timdettmers.com/2018/10/17/tpus-vs-gpus-for-transformers-bert O

- B-DAT

- B-DAT

Jun’ichi O
Tsujii. O
2003. O
GENIA O
corpus O
- B-DAT
a O
semanti- O
cally O
annotated O
corpus O

- B-DAT

- B-DAT
matics, O
19:i180i182 O

- B-DAT
tences O
to O
support O
evidence O
based O

-3 B-DAT

- B-DAT
eases O
mapping. O
In O
Database O

- B-DAT

- B-DAT
aky, O
Chih-Hsuan O
Wei, O
Robert O
Leaman O

- B-DAT

- B-DAT
ties, O
relations, O
and O
coreference O
for O

- B-DAT
edge O
graph O
construction. O
In O
EMNLP O

- B-DAT
els O
for O
biomedical O
natural O
language O

- B-DAT
ron O
C. O
Wallace. O
2018. O
A O

- B-DAT

- B-DAT
notations O
of O
patients, O
interventions O
and O

- B-DAT

- B-DAT
standing O
by O
generative O
pre-training O

- B-DAT
perparameters O
for O
deep O
lstm-networks O
for O

- B-DAT
rin O
Eide, O
Bo-June O
Paul O
Hsu O

- B-DAT
chine O
translation. O
abs/1609.08144 O

- B-DAT
ral O
networks O
for O
biomedical O
named O

- B-DAT
tion. O
In O
DTMBio O
workshop O
at O

papers. O
The O
SOTA O
result O
for O
GENIA B-DAT
is O

DEP O
GENIA B-DAT
(Kim O
et O
al., O
2003 O

GENIA B-DAT
(Kim O
et O
al., O
2003 O

Tateisi, O
and O
Jun’ichi O
Tsujii. O
2003. O
GENIA B-DAT
corpus O
- O
a O
semanti- O
cally O

DEP O
since O
we O
already O
include O
LAS B-DAT

Kim O
et O
al., O
2003) O
- O
LAS B-DAT
91.92 O
90.22 O
90.33 O
90.36 O
90.43 O

For O
DEP, O
we O
report O
labeled O
(LAS) B-DAT
and O
unlabeled O
(UAS O

model O
with O
hyperparameters O
tuned O
for O
LAS B-DAT

papers. O
The O
SOTA O
result O
for O
GENIA B-DAT

Tateisi, O
and O
Jun’ichi O
Tsujii. O
2003. O
GENIA B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-70 B-DAT
days O
(Dettmers, O
2019) O
on O
an O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-6, B-DAT
1e-5, O
2e-5, O
or O
5e-5 O
with O

-5 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

GENIA O
(Kim O
et O
al., O
2003) O
- B-DAT
LAS O
91.92 O
90.22 O
90.33 O
90.36 O

GENIA O
(Kim O
et O
al., O
2003) O
- B-DAT
UAS O
92.84 O
91.84 O
91.89 O
92.00 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Hung O
Weng, O
Di O
Jin, O
Tristan O

- B-DAT
cal O
bert O
embeddings. O
In O
ClinicalNLP O

- B-DAT
ula, O
Iz O
Beltagy, O
Miles O
Crawford O

- B-DAT
son O
Dunkelberger, O
Ahmed O
Elgohary, O
Sergey O

- B-DAT
man, O
Vu O
Ha, O
Rodney O
Kinney O

- B-DAT

- B-DAT
ters, O
Joanna O
Power, O
Sam O
Skjonsberg O

- B-DAT
ture O
graph O
in O
semantic O
scholar O

- B-DAT

- B-DAT
tics O

- B-DAT

- B-DAT

- B-DAT
BA/BioNLP O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-02 B-DAT

-22 B-DAT

- B-DAT

- B-DAT

- B-DAT
standing. O
In O
NAACL-HLT O

- B-DAT
ease O
name O
recognition O
and O
concept O

- B-DAT
ing. O
ICLR O

- B-DAT

- B-DAT
dicting O
hospital O
readmission. O
arXiv:1904.05342 O

- B-DAT
mad O
Ghassemi, O
Benjamin O
Moody, O
Peter O

-1361 B-DAT
http://timdettmers.com/2018/10/17/tpus-vs-gpus-for-transformers-bert O

- B-DAT

- B-DAT

Jun’ichi O
Tsujii. O
2003. O
GENIA O
corpus O
- B-DAT
a O
semanti- O
cally O
annotated O
corpus O

- B-DAT

- B-DAT
matics, O
19:i180i182 O

- B-DAT
tences O
to O
support O
evidence O
based O

-3 B-DAT

- B-DAT
eases O
mapping. O
In O
Database O

- B-DAT

- B-DAT
aky, O
Chih-Hsuan O
Wei, O
Robert O
Leaman O

- B-DAT

- B-DAT
ties, O
relations, O
and O
coreference O
for O

- B-DAT
edge O
graph O
construction. O
In O
EMNLP O

- B-DAT
els O
for O
biomedical O
natural O
language O

- B-DAT
ron O
C. O
Wallace. O
2018. O
A O

- B-DAT

- B-DAT
notations O
of O
patients, O
interventions O
and O

- B-DAT

- B-DAT
standing O
by O
generative O
pre-training O

- B-DAT
perparameters O
for O
deep O
lstm-networks O
for O

- B-DAT
rin O
Eide, O
Bo-June O
Paul O
Hsu O

- B-DAT
chine O
translation. O
abs/1609.08144 O

- B-DAT
ral O
networks O
for O
biomedical O
named O

- B-DAT
tion. O
In O
DTMBio O
workshop O
at O

LINNAEUS B-DAT
(13) O
Species O
23,152 O

LINNAEUS B-DAT
89.81 O
88.13 O
NA O
NA O
87.02 O

LINNAEUS B-DAT

stracts. O
SciERC B-DAT
(Luan O
et O
al., O
2018) O
annotates O

NER O
SciERC B-DAT
(Luan O
et O
al., O
2018) O
64.20 O

REL O
SciERC B-DAT
(Luan O
et O
al., O
2018) O
n/a O

SciERC B-DAT
(Luan O
et O
al., O
2018). O
For O

stracts. O
SciERC B-DAT
(Luan O
et O
al., O
2018) O
annotates O

NER O
SciERC B-DAT
(Luan O
et O
al., O
2018) O
64.20 O

REL O
SciERC B-DAT
(Luan O
et O
al., O
2018) O
n/a O

SciERC B-DAT
(Luan O
et O
al., O
2018). O
For O

scientific O
articles. O
We O
cre- O
ate O
SCIERC, B-DAT
a O
dataset O
that O
includes O
annota O

Our O
dataset O
(called O
SCIERC) B-DAT
includes O
annotations O
for O
scientific O
entities O

the O
Se- O
mantic O
Scholar O
Corpus2. O
SCIERC B-DAT
extends O
pre- O
vious O
datasets O
in O

Statistics O
SCIERC B-DAT
SemEval O
17 O
SemEval O
18 O

Dataset O
statistics O
for O
our O
dataset O
SCIERC B-DAT
and O
two O
previous O
datasets O
on O

Comparison O
with O
previous O
datasets O
SCIERC B-DAT
is O
focused O
on O
annotating O
cross-sentence O

9.4 O
relations O
per O
ab- O
stract). O
SCIERC B-DAT
extends O
these O
datasets O
by O
adding O

the O
three O
datasets. O
In O
addition, O
SCIERC B-DAT
aims O
at O
including O
broader O
coverage O

We O
evaluate O
SCIIE O
on O
SCIERC B-DAT
and O
SemEval O
17 O
datasets. O
We O

study O
for O
multitask O
learning O
on O
SCIERC B-DAT
development O
set. O
Each O
column O
shows O

entity O
and O
relation O
extraction O
in O
SCIERC B-DAT

less O
gain O
compared O
to O
the O
SCIERC B-DAT
dataset O
mainly O
because O
there O
are O

7.1 O
IE O
Results O
Results O
on O
SciERC B-DAT
Table O
2 O
compares O
the O
result O

disease B-DAT
(9 O

disease B-DAT
(10) O
Disease O
15,030 O

disease B-DAT
corpus O
is O
the O
smallest O
of O

disease B-DAT
NER O
dataset O

disease B-DAT

disease) B-DAT
BioFLAIR O
(V1) O
+ O
BioELMo O
88.85 O

disease B-DAT
BioFLAIR O
(V1) O
+ O
BioELMo O
85.31 O

disease B-DAT
(+ O
NCBI) O
BioFLAIR O
(V1 O

ones O
as O
similar O
as O
the O
disease B-DAT
identification O
benchmarks, O
did O
not O
automatically O

disease) B-DAT
dataset O

Leaman, O
and O
Zhiyong O
Lu. O
"NCBI O
disease B-DAT
corpus: O
a O
resource O

for O
disease B-DAT
name O
recognition O
and O
concept O
normalization O

corpus: O
a O
resource O
for O
chemical O
disease B-DAT
relation O

effect O
of O
combining O
similar O
corpora O
(NCBI B-DAT

NCBI B-DAT

The O
NCBI B-DAT

Models O
tested O
on O
NCBI B-DAT

NCBI B-DAT
BioFLAIR O
(V1) O
87.33 O

NCBI B-DAT
BioFLAIR O
(V2) O
86.99 O

NCBI B-DAT
BioFLAIR O
(V3) O
87 O

NCBI B-DAT
BioFLAIR O
(V1) O
+ O
CRAWL O
88.18 O

NCBI B-DAT
BioFLAIR O
(V3) O
+ O
BioELMo O
88.3 O

NCBI B-DAT
BioFLAIR O
(V1) O
+ O
BioELMo O

data O
and O
different O
embeddings O
on O
NCBI B-DAT

the O
best O
performance O
on O
the O
NCBI B-DAT

NCBI B-DAT
89.36 O
87.38 O
86.91 O
84.72 O
88.855 O

BC5DR-Disease O
training O
material O
to O
the O
NCBI B-DAT

on O
the O
NCBI B-DAT

than O
the O
improvement O
in O
the O
NCBI B-DAT

NCBI B-DAT

NCBI B-DAT
(+BC5DR-disease) O
BioFLAIR O
(V1) O
+ O
BioELMo O

BC5DR-disease O
(+ O
NCBI) B-DAT
BioFLAIR O
(V1) O
+ O
BioELMo O
84.49 O

5 O
Trained O
on O
NCBI B-DAT
(+BC5DR-disease) O
dataset O

NCBI B-DAT
disease O
corpus: O
a O
resource O

NCBI-disease B-DAT
(9 O

The O
NCBI-disease B-DAT
corpus O
is O
the O
smallest O
of O

disease B-DAT

disease B-DAT
(Dogan O
et O
al., O
2014) O
89.36 O

disease B-DAT
89.36 O
88.57 O

and O
Zhiyong O
Lu. O
2014. O
NCBI O
disease B-DAT
corpus: O
A O
resource O
for O
dis O

corpus: O
a O
resource O
for O
chemical O
disease B-DAT
relation O
extraction. O
Database O
: O
the O

2018). O
The O
SOTA O
model O
for O
NCBI B-DAT

NCBI B-DAT

NCBI B-DAT

Leaman, O
and O
Zhiyong O
Lu. O
2014. O
NCBI B-DAT
disease O
corpus: O
A O
resource O
for O

2018). O
The O
SOTA O
model O
for O
NCBI-disease B-DAT

NCBI-disease B-DAT
(Dogan O
et O
al., O
2014) O
89.36 O

NCBI-disease B-DAT
89.36 O
88.57 O

disease B-DAT

disease B-DAT
(Dogan O
et O
al., O
2014) O
89.36 O

disease B-DAT
89.36 O
88.57 O

and O
Zhiyong O
Lu. O
2014. O
NCBI O
disease B-DAT
corpus: O
A O
resource O
for O
dis O

corpus: O
a O
resource O
for O
chemical O
disease B-DAT
relation O
extraction. O
Database O
: O
the O

2018). O
The O
SOTA O
model O
for O
NCBI B-DAT

NCBI B-DAT

NCBI B-DAT

Leaman, O
and O
Zhiyong O
Lu. O
2014. O
NCBI B-DAT
disease O
corpus: O
A O
resource O
for O

2018). O
The O
SOTA O
model O
for O
NCBI-disease B-DAT

NCBI-disease B-DAT
(Dogan O
et O
al., O
2014) O
89.36 O

NCBI-disease B-DAT
89.36 O
88.57 O

Species-800 B-DAT
(12) O
Species O
8,193 O

LINNAEUS O
(13) O
Species B-DAT
23,152 O

variants O
on O
the O
BC5DR O
and O
Species B-DAT

score O
for O
the O
Species B-DAT

Species B-DAT

Species-800 B-DAT
(12) O
Species O
8,193 O

variants O
on O
the O
BC5DR O
and O
Species-800 B-DAT
benchmarks. O
We O
even O
set O
a O

score O
for O
the O
Species-800 B-DAT
benchmark O

Species-800 B-DAT
75.31 O
73.08 O
NA O
75.7 O
82.44 O

Species-800 B-DAT
(12) O
Species O
8,193 O

variants O
on O
the O
BC5DR O
and O
Species-800 B-DAT
benchmarks. O
We O
even O
set O
a O

score O
for O
the O
Species-800 B-DAT
benchmark O

Species-800 B-DAT
75.31 O
73.08 O
NA O
75.7 O
82.44 O

800 B-DAT
(12) O
Species O
8,193 O

800 B-DAT
benchmarks. O
We O
even O
set O
a O

800 B-DAT
benchmark O

800 B-DAT
75.31 O
73.08 O
NA O
75.7 O
82.44 O

and O
ChemProt B-DAT
(Lee O
et O
al., O
2019), O
and O

REL O
ChemProt B-DAT
(Kringelum O
et O
al., O
2016) O
76.68 O

token-level), O
and O
micro O
F1 O
for O
ChemProt B-DAT
specifically. O
For O
DEP, O
we O
report O

REL O
ChemProt B-DAT
76.68 O
83.64 O

BC5CDR O
and O
ChemProt, B-DAT
and O
performs O
similarly O

Oprea, O
and O
Olivier O
Taboureau. O
2016. O
ChemProt B-DAT

and O
ChemProt B-DAT
(Lee O
et O
al., O
2019), O
and O

REL O
ChemProt B-DAT
(Kringelum O
et O
al., O
2016) O
76.68 O

token-level), O
and O
micro O
F1 O
for O
ChemProt B-DAT
specifically. O
For O
DEP, O
we O
report O

REL O
ChemProt B-DAT
76.68 O
83.64 O

BC5CDR O
and O
ChemProt, B-DAT
and O
performs O
similarly O

Oprea, O
and O
Olivier O
Taboureau. O
2016. O
ChemProt B-DAT

stracts. O
SciERC B-DAT
(Luan O
et O
al., O
2018) O
annotates O

NER O
SciERC B-DAT
(Luan O
et O
al., O
2018) O
64.20 O

REL O
SciERC B-DAT
(Luan O
et O
al., O
2018) O
n/a O

SciERC B-DAT
(Luan O
et O
al., O
2018). O
For O

stracts. O
SciERC B-DAT
(Luan O
et O
al., O
2018) O
annotates O

NER O
SciERC B-DAT
(Luan O
et O
al., O
2018) O
64.20 O

REL O
SciERC B-DAT
(Luan O
et O
al., O
2018) O
n/a O

SciERC B-DAT
(Luan O
et O
al., O
2018). O
For O

on O
two O
benchmark O
biomedical O
corpora, O
GENIA B-DAT
and O
CRAFT. O
To O
the O
best O

on O
two O
benchmark O
biomedical O
corpora O
GENIA B-DAT
and O
CRAFT. O
We O
also O
perform O

linguistically-annotated O
resources, O
notably O
including O
the O
GENIA B-DAT
[2] O
and O
CRAFT O
[3] O
corpora O

We O
use O
two O
biomedical O
corpora: O
GENIA B-DAT
[2] O
and O
CRAFT O
[3]. O
GENIA O

The O
GENIA B-DAT
corpus O
contains O
18K O
sentences O
(∼486K O

for O
POS O
tagging O
over O
both O
GENIA B-DAT
and O
CRAFT. O
We O
consider O
the O

denote O
the O
occurrence O
proportions O
in O
GENIA B-DAT
and O
CRAFT, O
respectively O

GENIA B-DAT
CRAFT O
Type O
% O
Type O

ADV O
4.0 O
CC O
3.6 O
3.2 O
GENIA B-DAT
< O
−5 O
4.1 O
3.9 O
amod O

of O
at O
least O
0.17% O
in O
GENIA B-DAT
and O
0.26% O
in O
CRAFT O
between O

Model O
GENIA B-DAT
CRAFT O
MarMoT O
98.61 O
97.07 O
jPTDP-v1 O

Stanford O
tagger O
[?] O
98.37 O
GENIA B-DAT
tagger O
[?] O
98.49 O

published O
re- O
sults O
of O
the O
GENIA B-DAT
POS O
tagger O
[36], O
when O
trained O

on O
90% O
of O
the O
GENIA B-DAT
corpus O
(cf. O
our O
85% O
training O

obtain O
the O
lowest O
scores O
on O
GENIA B-DAT
and O
CRAFT, O
respectively. O
jPTDP O
obtains O

score O
to O
Mar- O
MoT O
on O
GENIA B-DAT
and O
similar O
score O
to O
BiLSTM-CRF O

at O
98.61% O
and O
97.07% O
on O
GENIA B-DAT
and O
CRAFT, O
which O
are O
about O

0–18, O
the O
accuracies O
for O
the O
GENIA B-DAT
tagger, O
Stanford O
tagger, O
MarMoT, O
NLP4J O

and O
predicted O
POS O
tags O
on O
GENIA, B-DAT
i.e. O
92.51 O
vs. O
92.31 O
and O

16]. O
The O
larger O
improvements O
on O
GENIA B-DAT
and O
CRAFT O
show O
that O
character-level O

On O
both O
GENIA B-DAT
and O
CRAFT, O
BiLSTM-CRF O
with O
character-level O

results O
on O
the O
GENIA B-DAT
test O
set O
of O
“pre-trained” O
parsers O

use O
the O
data O
split O
on O
GENIA B-DAT
as O
used O
in O
[10], O
therefore O

On O
GENIA, B-DAT
among O
pre-trained O
models, O
BLLIP O
ob O

trained O
models, O
was O
trained O
using O
GENIA, B-DAT
so O
this O
re- O
sult O
is O

the O
pre-trained O
Stanford-NNdep O
model O
on O
GENIA B-DAT

retrained O
parsing O
models, O
on O
both O
GENIA B-DAT
and O
CRAFT, O
Stanford-Biaffine O
achieves O
the O

GENIA B-DAT
– O
sentence O
length) O
(CRAFT O

GENIA B-DAT
– O
dependency O
distance) O
(CRAFT O

and O
UAS O
at O
92.64% O
on O
GENIA, B-DAT
and O
LAS O
at O
90.77% O
and O

absolute O
lower O
than O
Stanford-Biaffine O
on O
GENIA B-DAT
and O
CRAFT, O
respectively. O
jPTDP O
is O

without O
punctuation) O
than O
NLP4J-dep O
on O
GENIA B-DAT
and O
CRAFT, O
respectively. O
Table O
4 O

sentences O
<=10 O
words. O
Exceptionally, O
on O
GENIA B-DAT
we O
find O
lower O
scores O
for O

in O
the O
first O
bin O
on O
GENIA B-DAT
are O
relatively O
long, O
with O
an O

F1) O
scores O
of O
Stanford-Biaffine O
on O
GENIA, B-DAT
by O
frequent O
dependency O
labels O
in O

one O
surprising O
excep- O
tion: O
on O
GENIA, B-DAT
in O
distance O
bins O
of O
−4 O

characteristics O
of O
sentences O
in O
the O
GENIA B-DAT
corpus. O
Ta- O
ble O
5 O
details O

frequent O
dependency O
relation O
types O
on O
GENIA B-DAT
and O
CRAFT, O
respectively. O
In O
most O

with O
the O
following O
exceptions: O
on O
GENIA, B-DAT
jPTDP O
gets O
the O
highest O
results O

and O
num, O
respec- O
tively. O
On O
GENIA B-DAT
the O
labels O
associated O
with O
the O

basic O
Stanford O
dependency O
labels O
on O
GENIA B-DAT

generally O
<70%) O
are O
dep O
on O
GENIA B-DAT
and O
DEP, O
LOC, O
PRN O
and O

Type O
GENIA B-DAT
CRAFT O

is O
relatively O
rare O
tag O
in O
GENIA B-DAT
and O
is O
the O
least O
fre O

second O
least O
frequent O
one O
in O
GENIA B-DAT
and O
CRAFT, O
respectively, O
and O
generally O

ranges O
across O
parsers O
on O
both O
GENIA B-DAT
and O
CRAFT. O
The O
re- O
sults O

higher O
scores O
for O
IN O
on O
GENIA B-DAT
than O
on O
CRAFT, O
and O
vice O

lower O
scores O
for O
VB O
on O
GENIA B-DAT

. O
This O
is O
because O
on O
GENIA, B-DAT
IN O
is O
mostly O
coupled O
with O

words O
at O
15%, O
while O
on O
GENIA B-DAT
it O
associates O
with O
longer O
dependency O

VB O
on O
CRAFT O
than O
on O
GENIA B-DAT

the O
intersected O
parsing O
errors O
on O
GENIA B-DAT
and O
12% O
on O
CRAFT O
are O

intersected O
across O
all O
parsers O
on O
GENIA B-DAT
and O
0.5% O
on O
CRAFT). O
Based O

which O
was O
derived O
from O
the O
GENIA B-DAT
treebank O
cor- O
pus O
(800, O
150 O

files O
are O
included O
in O
the O
GENIA B-DAT
treebank O
training O
set O

dataset O
is O
extracted O
from O
the O
GENIA B-DAT
treebank O
training O
set. O
Although O
gold O

for O
parsers O
trained O
with O
the O
GENIA B-DAT
tree- O
bank O
(Rows O
1-6, O
Table O

the O
Stanford&Paris O
team O
[50] O
employ O
GENIA B-DAT
data, O
ob- O
taining O
the O
highest O

was O
a O
subset O
of O
the O
GENIA B-DAT
corpus. O
However, O
we O
find O
that O

four O
dependency O
parsers O
trained O
on O
GENIA, B-DAT
Stanford-Biaffine, O
jPTDP O
and O
NLP4J-dep O
produce O

set O
(except O
NLP4J-dep O
trained O
on O
GENIA B-DAT
and O
Stanford- O
NNdep O
trained O
on O

on O
two O
benchmark O
biomedical O
corpora O
GENIA B-DAT
and O
CRAFT. O
In O
particular, O
BiLSTM-CRF-based O

the O
GENIA B-DAT
Corpus. O
In: O
Proceedings O
of O
the O

on O
two O
benchmark O
biomedical O
corpora, O
GENIA B-DAT

on O
two O
benchmark O
biomedical O
corpora O
GENIA B-DAT

GENIA B-DAT
[2] I-DAT
and O
CRAFT O
[3]. O
GENIA O

The O
GENIA B-DAT

for O
POS O
tagging O
over O
both O
GENIA B-DAT

denote O
the O
occurrence O
proportions O
in O
GENIA B-DAT

GENIA B-DAT

of O
at O
least O
0.17% O
in O
GENIA B-DAT

Model O
GENIA B-DAT

Stanford O
tagger O
[?] O
98.37 O
GENIA B-DAT

published O
re- O
sults O
of O
the O
GENIA B-DAT

trained O
on O
90% O
of O
the O
GENIA B-DAT

obtain O
the O
lowest O
scores O
on O
GENIA B-DAT

score O
to O
Mar- O
MoT O
on O
GENIA B-DAT

at O
98.61% O
and O
97.07% O
on O
GENIA B-DAT

0–18, O
the O
accuracies O
for O
the O
GENIA B-DAT

16]. O
The O
larger O
improvements O
on O
GENIA B-DAT

On O
both O
GENIA B-DAT

results O
on O
the O
GENIA B-DAT

use O
the O
data O
split O
on O
GENIA B-DAT

retrained O
parsing O
models, O
on O
both O
GENIA B-DAT

absolute O
lower O
than O
Stanford-Biaffine O
on O
GENIA B-DAT

without O
punctuation) O
than O
NLP4J-dep O
on O
GENIA B-DAT

sentences O
<=10 O
words. O
Exceptionally, O
on O
GENIA B-DAT

in O
the O
first O
bin O
on O
GENIA B-DAT

characteristics O
of O
sentences O
in O
the O
GENIA B-DAT

frequent O
dependency O
relation O
types O
on O
GENIA B-DAT

and O
num, O
respec- O
tively. O
On O
GENIA B-DAT

generally O
<70%) O
are O
dep O
on O
GENIA B-DAT

Type O
GENIA B-DAT

is O
relatively O
rare O
tag O
in O
GENIA B-DAT

second O
least O
frequent O
one O
in O
GENIA B-DAT

ranges O
across O
parsers O
on O
both O
GENIA B-DAT

higher O
scores O
for O
IN O
on O
GENIA B-DAT

words O
at O
15%, O
while O
on O
GENIA B-DAT

the O
intersected O
parsing O
errors O
on O
GENIA B-DAT

intersected O
across O
all O
parsers O
on O
GENIA B-DAT

which O
was O
derived O
from O
the O
GENIA B-DAT

files O
are O
included O
in O
the O
GENIA B-DAT

dataset O
is O
extracted O
from O
the O
GENIA B-DAT

for O
parsers O
trained O
with O
the O
GENIA B-DAT

the O
Stanford&Paris O
team O
[50] O
employ O
GENIA B-DAT

was O
a O
subset O
of O
the O
GENIA B-DAT

set O
(except O
NLP4J-dep O
trained O
on O
GENIA B-DAT

on O
two O
benchmark O
biomedical O
corpora O
GENIA B-DAT

the O
GENIA B-DAT

UAS O
LAS B-DAT
UAS I-DAT
LAS O
UAS O
LAS O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lel O
repositories O
of O
PubMed[1] O
(abstracts O

- B-DAT
able O
rate O
of O
over O
one O

- B-DAT
fort O
to O
catalog O
the O
key O

- B-DAT
cations O
demands O
automation O
[1]. O
Hence O

- B-DAT
sis O
on O
the O
value O
of O

- B-DAT

- B-DAT
ical O
publications, O
based O
on O
the O

- B-DAT
stantive O
differences O
between O
these O
domain O

- B-DAT
tic O
information O
[5]. O
Despite O
this O

- B-DAT
alyze O
the O
syntactic O
characteristics O
of O

- B-DAT

- B-DAT
cant O
improvements O
in O
parsing O
performance O

- B-DAT

- B-DAT

- B-DAT
of-the-art O
(SOTA) O
approaches O
to O
dependency O

- B-DAT
tailed O
results O
on O
the O
precursor O

- B-DAT
trinsic O
parser O
evaluation O
shared O
task O

- B-DAT
ences O
in O
overall O
intrinsic O
parser O

- B-DAT
tion O
performance O

- B-DAT
ing O
models O
on O
benchmark O
biomedical O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferences O
between O
the O
abstracts O
and O

- B-DAT
ing O
full O
text O
publications O
[9 O

- B-DAT
ually O
annotated O
following O
the O
Penn O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
version O
toolkit O
(v3.5.1) O
to O
generate O

- B-DAT

- B-DAT
ing O
dependency O
trees. O
Hence, O
a O

- B-DAT
ments. O
We O
use O
sentences O
from O

- B-DAT
tences O
from O
the O
next O
6 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
based O
models O
for O
POS O
tagging O

- B-DAT

- B-DAT
phological O
tagger.[5 O

- B-DAT

- B-DAT
tomatically O
optimizes O
feature O
combinations.[6 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
3.5 O
−5 O
1.2 O
1.2 O
appos O

-20 B-DAT
31.0 O
−4 O
2.1 O
2.1 O
aux O

-30 B-DAT
35.7 O
−3 O
4.4 O
3.2 O
auxpass O

-40 B-DAT
19.4 O
−2 O
10.6 O
8.5 O
cc O

-50 B-DAT
7.1 O
−1 O
24.1 O
21.7 O
conj O

-10 B-DAT
17.8 O
4 O
4.0 O
3.4 O
mark O

-20 B-DAT
23.1 O
5 O
2.4 O
2.3 O
nn O

-30 B-DAT
25.2 O
> O
5 O
12.3 O
11.6 O

VBN O
3.1 O
3.8 O
31-40 B-DAT
17.5 O
- O
- O
- O
nsubjpass O
1.4 O
SBJ O

VBP O
1.4 O
1.1 O
41-50 B-DAT
9.3 O
- O
- O
- O
num O
1.2 O
SUB O

1.9 O
1.4 O
> O
50 O
7.1 O
- B-DAT
- O
- O
pobj O
12.2 O
TMP O

0.9 O
- B-DAT
- O
- O
- O
- O
- O
- O
- O
prep O
12.3 O

VC O
2.4 O
- B-DAT
- O
- O
- O
- O
- O
- O
- O
punct O

- B-DAT

- B-DAT
work O
[17, O
18] O
with O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dings. O
For O
each O
word O
token O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
STM O
to O
each O
word’s O
character O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
from O
[22].[7] O
As O
detailed O

- B-DAT

- B-DAT

- B-DAT
bank O
identified O
the O
domain-retrained O
ClearParser O

- B-DAT

- B-DAT
mains O
the O
best O
performing O
non-neural O

- B-DAT
pendency O
parsing. O
In O
particular, O
we O

- B-DAT
lowing O
parsers O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
based O
parsing O
model O
which O
concatenates O

- B-DAT
layer O
perceptron O
with O
one O
hidden O

- B-DAT
tion O
classification.[8 O

- B-DAT
dep) O
is O
a O
transition-based O
parser O

- B-DAT
tional O
branching O
method O
that O
uses O

- B-DAT
timates O
to O
decide O
when O
employing O

- B-DAT

- B-DAT
pendency O
parser O
bmstparser O
[28], O
replacing O

- B-DAT

- B-DAT

- B-DAT
pendency O
arcs O
and O
labels, O
obtaining O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
velopment O
set. O
Then O
we O
report O

- B-DAT
curacy. O
The O
metrics O
for O
dependency O

- B-DAT
ment O
score O
(UAS): O
LAS O
is O

- B-DAT

- B-DAT

- B-DAT
NNdep, O
jPTDP O
and O
Stanford-Biaffine O
which O

- B-DAT
lizes O
pre-trained O
word O
embeddings, O
we O

- B-DAT
dimensional O
pre-trained O
word O
vectors O
from O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nal O
pure O
Java O
implementations O
with O

- B-DAT
parameter O
settings O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
parameters O
to O
select O
the O
number O

- B-DAT
velopment O
set O
is O
obtained O
after O

- B-DAT

- B-DAT
parameters O
with O
their O
default O
values O

- B-DAT

- B-DAT
dings O
and O
fix O
the O
initial O

- B-DAT
lect O
the O
number O
of O
LSTM O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults O
of O
the O
GENIA O
POS O

- B-DAT

- B-DAT
MoT O
obtain O
the O
lowest O
scores O

- B-DAT
MoT O
on O
GENIA O
and O
similar O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
viding O
useful O
extra O
information O

- B-DAT

- B-DAT
NIA O
and O
97.25% O
on O
CRAFT O

- B-DAT

- B-DAT
POS, O
BiLSTM-CRF O
and O
BiLSTM-CRF+CNN-char O
on O

-24 B-DAT
were O
reported O
at O
97.05%, O
97.23 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
NIA O
and O
CRAFT, O
respectively, O
resulting O

- B-DAT
est O
accuracies O
on O
both O
experimental O

- B-DAT

- B-DAT

- B-DAT
CRF O
[16]. O
The O
larger O
improvements O

- B-DAT

- B-DAT

- B-DAT
dings O
are O
useful O
for O
morphologically O

- B-DAT

- B-DAT
tions O
which O
are O
also O
out-of-vocabulary O

- B-DAT

- B-DAT
ture O
those O
unseen O
abbreviated O
words O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cal O
factor O
in O
parsing O
performance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
[23] O
to O
generate O
POS O

- B-DAT
ing O
models O
in O
Table O
4 O

- B-DAT

- B-DAT

- B-DAT

-4 B-DAT
present O
scores O
of O
these O
pre O

- B-DAT
trained O
models O
with O
POS O
tags O

- B-DAT
POS. O
Both O
pre-trained O
NNdep O
and O

- B-DAT
tences, O
which O
was O
converted O
from O

- B-DAT
tions O
2–21. O
The O
fifth O
row O

- B-DAT

- B-DAT
erate O
dependency O
trees O
with O
the O

- B-DAT
dencies O
and O
use O
the O
data O

- B-DAT
maining O
rows O
show O
results O
of O

- B-DAT

- B-DAT
tains O
highest O
results. O
This O
model O

- B-DAT
trained O
models, O
was O
trained O
using O

- B-DAT
sult O
is O
unsurprising. O
The O
pre-trained O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
prising O
because O
the O
pre-trained O
Stanford-Biaffine O

- B-DAT
lizes O
pre-trained O
word O
vectors O
which O

- B-DAT

- B-DAT
mance O
differences O
irrespective O
of O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

5 O
-5 B-DAT
-4 O
-3 O
-2 O
-1 O
1 O
2 O
3 O
4 O
5 O

- B-DAT

- B-DAT

- B-DAT

5 O
-5 B-DAT
-4 O
-3 O
-2 O
-1 O
1 O
2 O
3 O
4 O
5 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tions. O
Stanford-NNdep O
obtains O
the O
lowest O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
stead O
of O
predicted O
POS O
tags O

- B-DAT
sults O
obtained O
by O
the O
retrained O

- B-DAT
tuation. O
Using O
UAS O
scores O
or O

- B-DAT
ation O
does O
not O
reveal O
any O

- B-DAT
ative O
to O
short O
sentences O
<=10 O

- B-DAT
NIA O
(cf. O
17.8% O
on O
CRAFT O

- B-DAT

- B-DAT
dices O
of O
words O
in O
a O

- B-DAT
jectives O
or O
pronouns O
modifying O
their O

- B-DAT
pendencies. O
This O
is O
not O
completely O

- B-DAT
glish O
is O
strongly O
head-initial. O
In O

- B-DAT
tween O
LSTM-based O
models O
(i.e. O
Stanford-Biaffine O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion: O
on O
GENIA, O
in O
distance O

- B-DAT

- B-DAT
ble O
5 O
details O
the O
scores O

- B-DAT

- B-DAT
most O
dependency O
bins. O
We O
find O

- B-DAT

- B-DAT

- B-DAT
tively. O
On O
GENIA O
the O
labels O

- B-DAT
est O
average O
LAS O
scores O
(generally O

- B-DAT
bels O
either O
correspond O
to O
short O

- B-DAT
eral O
labels O
while O
LOC, O
PRN O

- B-DAT

- B-DAT
ation O
across O
parsers. O
These O
9 O

- B-DAT
bels O
generally O
correspond O
to O
long O

- B-DAT
fore, O
it O
is O
not O
surprising O

- B-DAT

- B-DAT
els O
Stanford-Biaffine O
and O
jPTDP O
can O

- B-DAT

- B-DAT
els O
NLP4J-dep O
and O
NNdep O

- B-DAT
quent O
POS O
tags O
(across O
two O

- B-DAT
dent. O
Stanford-Biaffine O
achieves O
the O
highest O

- B-DAT
based O
model O
NLP4J-dep O
obtains O
the O

- B-DAT
quent O
tag O
in O
CRAFT O
among O

- B-DAT
ordinating O
conjunction O
tag O
CC O
also O

- B-DAT
sults O
for O
CC O
are O
consistent O

- B-DAT
tion O
of O
dependency O
labels O
such O

-1 B-DAT

- B-DAT

- B-DAT

- B-DAT
sistently O
across O
all O
parsers O
(i.e O

- B-DAT
terns. O
The O
first O
one O
is O

- B-DAT
diction O
(8% O
of O
the O
intersected O

- B-DAT
dicted O
POS O
tags). O
For O
example O

- B-DAT
mains” O
is O
the O
head O
of O

- B-DAT

- B-DAT

- B-DAT
gers) O
produced O
an O
incorrect O
tag O

- B-DAT
jective O
(JJ) O
for O
“POU(S)”. O
As O

-1 B-DAT

- B-DAT

- B-DAT
GCAAAT” O
(in O
Table O
9) O
and O

-1 B-DAT
coregulator O
VP16”, O
commonly O
referred O
to O

- B-DAT
positive O
structures, O
where O
the O
second O

- B-DAT

- B-DAT
dency O
arc O
with O
the O
same O

- B-DAT
dency O
parsers O
for O
the O
downstream O

- B-DAT

- B-DAT
trained” O
parsers O
with O
3 O
different O

- B-DAT
quires O
use O
of O
a O
currently O

- B-DAT
mance O
on O
downstream O
tasks O
[47 O

- B-DAT
cal O
event O
extraction O
task O
[8 O

- B-DAT
imental O
setup O
used O
there; O
employing O

- B-DAT
pus O
(800, O
150 O
and O
260 O

- B-DAT

- B-DAT
segmented O
data O
provided O
by O
the O

- B-DAT

- B-DAT

- B-DAT
NNdep O
parsers O
that O
require O
predicted O

- B-DAT

-2017 B-DAT
[14]678 O
of O
800 O
training, O
132 O

- B-DAT

- B-DAT

-2009 B-DAT
development O
sentences O
which O
contain O
event O

- B-DAT
dency O
parsing O
models O

- B-DAT
opment O
data O
(gold O
event O
annotations O

- B-DAT
able O
to O
public O
for O
training O

- B-DAT
uation O
system. O
The O
online O
evaluation O

- B-DAT
proximate O
span O
& O
recursive O
evaluation O

- B-DAT

- B-DAT
dicted O
segmentation), O
for O
which O
these O

- B-DAT
tain O
event O
interactions. O
These O
scores O

- B-DAT

- B-DAT

- B-DAT
NIA O
treebank O
data.[16] O
The O
second O

- B-DAT

- B-DAT
ing O
rows O
show O
scores O
using O

- B-DAT

- B-DAT
bank O
(Rows O
1-6, O
Table O
11 O

- B-DAT

- B-DAT

- B-DAT
ating O
different O
dependency O
representations O
in O

- B-DAT
stream O
tasks, O
not O
on O
comparing O

- B-DAT
taining O
the O
highest O
biomedical O
event O

-2017 B-DAT
http://bionlp-st.dbcls.jp/GE/2011/eval-test/eval.cgi O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ences O
in O
intrinsic O
parsing O
results O

- B-DAT
tending O
preliminary O
related O
observations O
in O

- B-DAT

- B-DAT

- B-DAT
ment O
set, O
while O
on O
the O

- B-DAT
dep O
obtain O
the O
lowest O
and O

- B-DAT
dency O
structures O
only O
(i.e. O
results O

- B-DAT
bels O
by O
“UNK” O
before O
training O

- B-DAT

- B-DAT

- B-DAT
NNdep O
trained O
on O
CRAFT). O
Without O

- B-DAT
bels, O
better O
event O
extraction O
scores O

- B-DAT
dition, O
the O
differences O
in O
these O

- B-DAT
mance. O
Some O
(predicted) O
dependency O
labels O

- B-DAT
quent O
dependency O
labels O
in O
each O

- B-DAT
paring O
SOTA O
traditional O
feature-based O
and O

- B-DAT

- B-DAT
dency O
parsing O
in O
the O
biomedical O

- B-DAT

- B-DAT
els O
on O
two O
benchmark O
biomedical O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
est O
POS O
tagging O
accuracies O
which O

- B-DAT

- B-DAT

- B-DAT
ing O
model O
obtains O
significantly O
better O

- B-DAT
tion O
performance. O
Whether O
this O
pattern O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

Technology O
- B-DAT
Volume O
1, O
pp. O
173–180 O
(2003 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

and O
unlabeled O
attach- O
ment O
score O
(UAS B-DAT

dependency O
arc O
and O
label O
while O
UAS B-DAT
is O
the O
proportion O
of O
words O

0.001 O
using O
McNemar’s O
test O
(except O
UAS B-DAT
scores O
obtained O
by O
Stanford-Biaffine-v2 O
for O

match O
Overall O
Exact O
match O
LAS O
UAS B-DAT
LAS O
UAS O
LAS O
UAS O
LAS O

UAS B-DAT

We O
present O
the O
LAS O
and O
UAS B-DAT
scores O
of O
different O
pars- O
ing O

with O
LAS O
at O
91.23% O
and O
UAS B-DAT
at O
92.64% O
on O
GENIA, O
and O

LAS O
at O
90.77% O
and O
UAS B-DAT
at O
92.67% O
on O
CRAFT, O
computed O

computed O
without O
punc- O
tuation. O
Using O
UAS B-DAT
scores O
or O
computing O
with O
punctu O

Table O
10 O
UAS B-DAT
and O
LAS O
(F1) O
scores O
of O

Metric O
Biaffine O
jPTDP O
NLP4J O
NNdep O
UAS B-DAT
95.51 O
93.14 O
92.50 O
91.02 O
LAS O

Table O
10 O
presents O
the O
intrinsic O
UAS B-DAT
and O
LAS O
(F1) O
scores O
on O

would O
obtain O
the O
similar O
intrinsic O
UAS B-DAT
and O
LAS O
scores O
on O
the O

State-of-the-art; O
LAS: O
Labeled O
attachment O
score; O
UAS B-DAT

papers. O
The O
SOTA O
result O
for O
GENIA B-DAT
is O

DEP O
GENIA B-DAT
(Kim O
et O
al., O
2003 O

GENIA B-DAT
(Kim O
et O
al., O
2003 O

Tateisi, O
and O
Jun’ichi O
Tsujii. O
2003. O
GENIA B-DAT
corpus O
- O
a O
semanti- O
cally O

papers. O
The O
SOTA O
result O
for O
GENIA B-DAT

Tateisi, O
and O
Jun’ichi O
Tsujii. O
2003. O
GENIA B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-70 B-DAT
days O
(Dettmers, O
2019) O
on O
an O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-6, B-DAT
1e-5, O
2e-5, O
or O
5e-5 O
with O

-5 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

GENIA O
(Kim O
et O
al., O
2003) O
- B-DAT
LAS O
91.92 O
90.22 O
90.33 O
90.36 O

GENIA O
(Kim O
et O
al., O
2003) O
- B-DAT
UAS O
92.84 O
91.84 O
91.89 O
92.00 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Hung O
Weng, O
Di O
Jin, O
Tristan O

- B-DAT
cal O
bert O
embeddings. O
In O
ClinicalNLP O

- B-DAT
ula, O
Iz O
Beltagy, O
Miles O
Crawford O

- B-DAT
son O
Dunkelberger, O
Ahmed O
Elgohary, O
Sergey O

- B-DAT
man, O
Vu O
Ha, O
Rodney O
Kinney O

- B-DAT

- B-DAT
ters, O
Joanna O
Power, O
Sam O
Skjonsberg O

- B-DAT
ture O
graph O
in O
semantic O
scholar O

- B-DAT

- B-DAT
tics O

- B-DAT

- B-DAT

- B-DAT
BA/BioNLP O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-02 B-DAT

-22 B-DAT

- B-DAT

- B-DAT

- B-DAT
standing. O
In O
NAACL-HLT O

- B-DAT
ease O
name O
recognition O
and O
concept O

- B-DAT
ing. O
ICLR O

- B-DAT

- B-DAT
dicting O
hospital O
readmission. O
arXiv:1904.05342 O

- B-DAT
mad O
Ghassemi, O
Benjamin O
Moody, O
Peter O

-1361 B-DAT
http://timdettmers.com/2018/10/17/tpus-vs-gpus-for-transformers-bert O

- B-DAT

- B-DAT

Jun’ichi O
Tsujii. O
2003. O
GENIA O
corpus O
- B-DAT
a O
semanti- O
cally O
annotated O
corpus O

- B-DAT

- B-DAT
matics, O
19:i180i182 O

- B-DAT
tences O
to O
support O
evidence O
based O

-3 B-DAT

- B-DAT
eases O
mapping. O
In O
Database O

- B-DAT

- B-DAT
aky, O
Chih-Hsuan O
Wei, O
Robert O
Leaman O

- B-DAT

- B-DAT
ties, O
relations, O
and O
coreference O
for O

- B-DAT
edge O
graph O
construction. O
In O
EMNLP O

- B-DAT
els O
for O
biomedical O
natural O
language O

- B-DAT
ron O
C. O
Wallace. O
2018. O
A O

- B-DAT

- B-DAT
notations O
of O
patients, O
interventions O
and O

- B-DAT

- B-DAT
standing O
by O
generative O
pre-training O

- B-DAT
perparameters O
for O
deep O
lstm-networks O
for O

- B-DAT
rin O
Eide, O
Bo-June O
Paul O
Hsu O

- B-DAT
chine O
translation. O
abs/1609.08144 O

- B-DAT
ral O
networks O
for O
biomedical O
named O

- B-DAT
tion. O
In O
DTMBio O
workshop O
at O

are O
averaged O
over O
datasets O
excluding O
UAS B-DAT
for O
DEP O
since O
we O
already O

Kim O
et O
al., O
2003) O
- O
UAS B-DAT
92.84 O
91.84 O
91.89 O
92.00 O
91.99 O

report O
labeled O
(LAS) O
and O
unlabeled O
(UAS B-DAT

papers. O
The O
SOTA O
result O
for O
GENIA B-DAT
is O

DEP O
GENIA B-DAT
(Kim O
et O
al., O
2003 O

GENIA B-DAT
(Kim O
et O
al., O
2003 O

Tateisi, O
and O
Jun’ichi O
Tsujii. O
2003. O
GENIA B-DAT
corpus O
- O
a O
semanti- O
cally O

papers. O
The O
SOTA O
result O
for O
GENIA B-DAT

Tateisi, O
and O
Jun’ichi O
Tsujii. O
2003. O
GENIA B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-70 B-DAT
days O
(Dettmers, O
2019) O
on O
an O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-6, B-DAT
1e-5, O
2e-5, O
or O
5e-5 O
with O

-5 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

GENIA O
(Kim O
et O
al., O
2003) O
- B-DAT
LAS O
91.92 O
90.22 O
90.33 O
90.36 O

GENIA O
(Kim O
et O
al., O
2003) O
- B-DAT
UAS O
92.84 O
91.84 O
91.89 O
92.00 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Hung O
Weng, O
Di O
Jin, O
Tristan O

- B-DAT
cal O
bert O
embeddings. O
In O
ClinicalNLP O

- B-DAT
ula, O
Iz O
Beltagy, O
Miles O
Crawford O

- B-DAT
son O
Dunkelberger, O
Ahmed O
Elgohary, O
Sergey O

- B-DAT
man, O
Vu O
Ha, O
Rodney O
Kinney O

- B-DAT

- B-DAT
ters, O
Joanna O
Power, O
Sam O
Skjonsberg O

- B-DAT
ture O
graph O
in O
semantic O
scholar O

- B-DAT

- B-DAT
tics O

- B-DAT

- B-DAT

- B-DAT
BA/BioNLP O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-02 B-DAT

-22 B-DAT

- B-DAT

- B-DAT

- B-DAT
standing. O
In O
NAACL-HLT O

- B-DAT
ease O
name O
recognition O
and O
concept O

- B-DAT
ing. O
ICLR O

- B-DAT

- B-DAT
dicting O
hospital O
readmission. O
arXiv:1904.05342 O

- B-DAT
mad O
Ghassemi, O
Benjamin O
Moody, O
Peter O

-1361 B-DAT
http://timdettmers.com/2018/10/17/tpus-vs-gpus-for-transformers-bert O

- B-DAT

- B-DAT

Jun’ichi O
Tsujii. O
2003. O
GENIA O
corpus O
- B-DAT
a O
semanti- O
cally O
annotated O
corpus O

- B-DAT

- B-DAT
matics, O
19:i180i182 O

- B-DAT
tences O
to O
support O
evidence O
based O

-3 B-DAT

- B-DAT
eases O
mapping. O
In O
Database O

- B-DAT

- B-DAT
aky, O
Chih-Hsuan O
Wei, O
Robert O
Leaman O

- B-DAT

- B-DAT
ties, O
relations, O
and O
coreference O
for O

- B-DAT
edge O
graph O
construction. O
In O
EMNLP O

- B-DAT
els O
for O
biomedical O
natural O
language O

- B-DAT
ron O
C. O
Wallace. O
2018. O
A O

- B-DAT

- B-DAT
notations O
of O
patients, O
interventions O
and O

- B-DAT

- B-DAT
standing O
by O
generative O
pre-training O

- B-DAT
perparameters O
for O
deep O
lstm-networks O
for O

- B-DAT
rin O
Eide, O
Bo-June O
Paul O
Hsu O

- B-DAT
chine O
translation. O
abs/1609.08144 O

- B-DAT
ral O
networks O
for O
biomedical O
named O

- B-DAT
tion. O
In O
DTMBio O
workshop O
at O

are O
averaged O
over O
datasets O
excluding O
UAS B-DAT
for O
DEP O
since O
we O
already O

Kim O
et O
al., O
2003) O
- O
UAS B-DAT
92.84 O
91.84 O
91.89 O
92.00 O
91.99 O

report O
labeled O
(LAS) O
and O
unlabeled O
(UAS B-DAT

BC5CDR B-DAT

BC5CDR B-DAT
(10) O
Disease O
and O

BC5CDR B-DAT

BC5CDR B-DAT
NA O
NA O
88.94 O
82.93 O
89.42 O

achieves O
new O
SOTA O
results O
on O
BC5CDR B-DAT

BC5CDR B-DAT
(Li O
et O
al., O
2016) O
88.857 O

BC5CDR B-DAT
88.85 O
90.01 O

BC5CDR B-DAT
and O
ChemProt, O
and O
performs O
similarly O

BC5CDR B-DAT
and O
SciCite, O
BERT-Base O
with O
finetuning O

achieves O
new O
SOTA O
results O
on O
BC5CDR B-DAT

BC5CDR B-DAT
(Li O
et O
al., O
2016) O
88.857 O

BC5CDR B-DAT
88.85 O
90.01 O

BC5CDR B-DAT
and O
ChemProt, O
and O
performs O
similarly O

BC5CDR B-DAT
and O
SciCite, O
BERT-Base O
with O
finetuning O

datasets O
(BC2GM O
[33], O
BC4CHEMD O
[34], O
BC5CDR B-DAT
[35–38], O
JNLPBA O
[22], O
NCBI O
[21 O

The O
BC5CDR B-DAT
dataset O
has O
the O
sub-datasets O
BC5CDR O

- O
chem, O
BC5CDR-disease B-DAT
and O
BC5CDR O

BC5CDR B-DAT
(Li O
et O
al., O
2016) O
Chemicals O

BC5CDR B-DAT
(Li O
et O
al., O
2016) O
Diseases O

reported O
the O
perfor- O
mance O
on O
BC5CDR B-DAT

-chem O
and O
BC5CDR B-DAT

of O
six O
datasets: O
BC2GM, O
BC4CHEMD, O
BC5CDR B-DAT

- O
chem, O
BC5CDR B-DAT

Wang O
et O
al. O
[25] O
used O
BC5CDR B-DAT

we O
reran O
their O
models O
on O
BC5CDR B-DAT

-chem O
and O
BC5CDR B-DAT

72.17 O
72.51 O
82.98 O
77.39 O
(±0.24) O
BC5CDR B-DAT

89.85 O
94.02 O
91.50 O
92.74 O
(±0.47) O
BC5CDR B-DAT

CollaboNet O
on O
5 O
datasets O
(BC2GM, O
BC5CDR B-DAT

-chem, O
BC5CDR B-DAT

BC5CDR B-DAT

BC5CDR B-DAT

BC5CDR B-DAT

BC5CDR B-DAT

1 O
for O
the O
older O
datasets. O
EBM B-DAT

Lee O
et O
al., O
2019), O
and O
EBM B-DAT

PICO O
EBM B-DAT

1 O
for O
the O
older O
datasets. O
EBM-NLP B-DAT
(Nye O
et O
al O

PICO O
EBM-NLP B-DAT
(Nye O
et O
al., O
2018) O
66.30 O

Obtaining O
large-scale O
annotated O
data O
for O
NLP B-DAT

mance O
on O
downstream O
scientific O
NLP B-DAT
tasks O

NLP B-DAT
an O
essential O
tool O
for O
large-scale O

ments. O
Recent O
progress O
in O
NLP B-DAT
has O
been O
driven O

NLP B-DAT
tasks. O
These O
models O
return O
contextualized O

obtain, O
like O
in O
scientific O
NLP B-DAT

performance O
on O
a O
range O
of O
NLP B-DAT

experiment O
on O
the O
following O
core O
NLP B-DAT
tasks O

NLP B-DAT
(Nye O
et O
al O

NLP B-DAT
(Nye O
et O
al., O
2018 O

NLP B-DAT
(Nye O
et O
al., O
2018) O
66.30 O

recognition O
task O
at O
jnlpba. O
In O
NLP B-DAT

1 O
for O
the O
older O
datasets. O
EBM B-DAT

Lee O
et O
al., O
2019), O
and O
EBM B-DAT

PICO O
EBM B-DAT

1 O
for O
the O
older O
datasets. O
EBM-NLP B-DAT
(Nye O
et O
al O

PICO O
EBM-NLP B-DAT
(Nye O
et O
al., O
2018) O
66.30 O

Obtaining O
large-scale O
annotated O
data O
for O
NLP B-DAT

mance O
on O
downstream O
scientific O
NLP B-DAT
tasks O

NLP B-DAT
an O
essential O
tool O
for O
large-scale O

ments. O
Recent O
progress O
in O
NLP B-DAT
has O
been O
driven O

NLP B-DAT
tasks. O
These O
models O
return O
contextualized O

obtain, O
like O
in O
scientific O
NLP B-DAT

performance O
on O
a O
range O
of O
NLP B-DAT

experiment O
on O
the O
following O
core O
NLP B-DAT
tasks O

NLP B-DAT
(Nye O
et O
al O

NLP B-DAT
(Nye O
et O
al., O
2018 O

NLP B-DAT
(Nye O
et O
al., O
2018) O
66.30 O

recognition O
task O
at O
jnlpba. O
In O
NLP B-DAT

aims O
of O
evidence O
based O
medicine O
(EBM), B-DAT
which O
aspires O
to O
inform O
patient O

address O
this O
gap O
by O
introducing O
EBM B-DAT

models O
in O
sup- O
port O
of O
EBM B-DAT

code O
are O
publicly O
avail- O
able.2 O
EBM B-DAT

2http://www.ccs.neu.edu/home/bennye/ O
EBM B-DAT

EBM B-DAT

EBM B-DAT

directly O
support O
the O
practice O
of O
EBM B-DAT
and O
that O
may O
be O
explored O

work O
on O
NLP O
to O
facilitate O
EBM, B-DAT
and O
research O
in O
crowdsourcing O
for O

2.1 O
NLP O
for O
EBM B-DAT

Prior O
work O
on O
NLP O
for O
EBM B-DAT
has O
been O
limited O
by O
the O

Larger O
corpora O
for O
EBM B-DAT
tasks O
have O
been O
de- O
rived O

multi-document) O
summarization O
problems O
to O
support O
EBM B-DAT
(Demner-Fushman O
and O
Lin, O
2007; O
Molla O

annotations O
in O
the O
domain O
of O
EBM B-DAT
specifically O
(Mortensen O
et O
al., O
2017 O

to O
aiding O
practitioners O
of O
EBM B-DAT
specifically. O
First, O
we O
consider O
the O

We O
have O
presented O
EBM B-DAT

to O
aid O
the O
conduct O
of O
EBM B-DAT

The O
EBM B-DAT

EBM B-DAT

EBM B-DAT

EBM B-DAT

code O
are O
publicly O
avail- O
able.2 O
EBM-NLP B-DAT
comprises O
∼5,000 O
medical O
ab- O
stracts O

2http://www.ccs.neu.edu/home/bennye/ O
EBM-NLP B-DAT

EBM-NLP B-DAT
http://www.ccs.neu.edu/home/bennye/EBM-NLP O

We O
have O
presented O
EBM-NLP B-DAT

The O
EBM-NLP B-DAT
corpus, O
accompanying O
docu- O
mentation, O
code O

EBM-NLP B-DAT

EBM-NLP B-DAT
http://www.ccs.neu.edu/home/bennye/EBM-NLP O

a O
set O
of O
challeng- O
ing O
NLP B-DAT
tasks O
that O
would O
aid O
searching O

2013) O
and O
natural O
language O
processing O
(NLP) B-DAT
in O
particular O
can O
play O
a O

has O
explored O
the O
use O
of O
NLP B-DAT
meth- O
ods O
to O
automate O
biomedical O

than O
it O
might O
from O
the O
NLP B-DAT
community, O
due O
pri- O
marily O
to O

this O
gap O
by O
introducing O
EBM- O
NLP, B-DAT
a O
new O
corpus O
to O
power O

NLP B-DAT
models O
in O
sup- O
port O
of O

NLP B-DAT
comprises O
∼5,000 O
medical O
ab- O
stracts O

NLP B-DAT

NLP B-DAT
http://www.ccs.neu.edu/home/bennye/EBM-NLP O

In O
addition, O
we O
outline O
several O
NLP B-DAT
tasks O
that O
would O
directly O
support O

the O
current O
effort: O
work O
on O
NLP B-DAT
to O
facilitate O
EBM, O
and O
research O

in O
crowdsourcing O
for O
NLP B-DAT

2.1 O
NLP B-DAT
for O
EBM O

Prior O
work O
on O
NLP B-DAT
for O
EBM O
has O
been O
limited O

has O
shown O
encouraging O
results O
in O
NLP B-DAT
(Novot- O
ney O
and O
Callison-Burch, O
2010 O

with O
which O
to O
evaluate O
developed O
NLP B-DAT
systems. O
We O
plan O
to O
enlarge O

We O
outline O
a O
few O
NLP B-DAT
tasks O
that O
are O
central O
to O

NLP B-DAT

corpora O
to O
facilitate O
research O
on O
NLP B-DAT
methods O
for O
processing O
the O
biomedical O

NLP B-DAT
corpus, O
accompanying O
docu- O
mentation, O
code O

NLP B-DAT

NLP B-DAT
http://www.ccs.neu.edu/home/bennye/EBM-NLP O

Augenstein O
et O
al., O
2017) O
and O
WetLab B-DAT
(Kulkarni O
et O
al., O
2018). O
Details O

All O
sources O
are O
measured O
against O
WetLab B-DAT
with O
relatively O
high O
PPL O
and O

fact O
that O
the O
tenor O
of O
WetLab B-DAT
(experimental O
protocols) O
is O
different O
from O

tenor O
of O
all O
sources, O
although O
WetLab B-DAT
has O
a O
similar O
field O
(biology O

comparisons. O
For O
exam- O
ple, O
given O
WetLab, B-DAT
is O
1BWB O
a O
more O
similar O

WetLab B-DAT

ScienceIE O
37.10 O
37.91 O
41.15 O
42.07 O
WetLab B-DAT
79.15 O
78.93 O
79.57 O
79.62 O

ScienceIE O
WetLab B-DAT
Def O
Opt O
Def O
Opt O

vectors O
on O
the O
ScienceIE O
and O
WetLab B-DAT
data O
sets. O
The O
reason O
for O

WetLab B-DAT
pairs) O
produce O
better O
performance O
compare O

the O
bio-entity O
r O
ecognitiontask O
vat O
jnlpba B-DAT

On O
the O
other O
hand, O
the O
JNLPBA B-DAT
cor- O
pus O
[22] O
contains O
annotations O

33], O
BC4CHEMD O
[34], O
BC5CDR O
[35–38], O
JNLPBA B-DAT
[22], O
NCBI O
[21]), O
all O
of O

use O
cell-type O
entity O
tags O
from O
JNLPBA B-DAT
for O
the O
entity O
types O

for O
the O
sentences. O
While O
the O
JNLPBA B-DAT
dataset O
has O
only O
training O
and O

development O
and O
test O
sets. O
For O
JNLPBA, B-DAT
we O
used O
part O
of O
its O

Also, O
we O
found O
that O
the O
JNLPBA B-DAT
dataset O
from O
Crichton O
et O
al O

JNLPBA B-DAT
(Kim O
et O
al., O
2004) O
Gene/Proteins O

BC2GM, O
BC4CHEMD, O
BC5CDR- O
chem, O
BC5CDR-disease, O
JNLPBA, B-DAT
and O
NCBI O

83.92 O
83.95 O
85.45 O
84.69 O
(±0.54) O
JNLPBA B-DAT
74.83 O
79.82 O
77.25 O
69.60 O
74.95 O

5 O
datasets O
(BC2GM, O
BC5CDR-chem, O
BC5CDR-disease, O
JNLPBA, B-DAT
NCBI). O
Error O
analysis O
was O
conducted O

86.14 O
85.48 O
87.27 O
86.36 O
(±0.54) O
JNLPBA B-DAT
70.91 O
76.34 O
73.52 O
74.43 O
83.22 O

JNLPBA B-DAT
749 O
1520 O
49.3% O
227 O
1437 O

of O
the O
total O
errors O
in O
JNLPBA B-DAT

the O
inconsistent O
annotations O
in O
the O
JNLPBA B-DAT
corpus O
limit O
the O
NER O
system O

JNLPBA B-DAT
(11) O
Gene/Protein O
24,806 O

JNLPBA B-DAT
77.59 O
76.65 O
75.95 O
74.14 O
77.03 O

JNLPBA B-DAT

the O
bio-entity O
recognition O
task O
at O
jnlpba B-DAT

datasets. O
The O
SOTA O
model O
for O
JNLPBA B-DAT

ple O
NER O
datasets O
not O
just O
JNLPBA B-DAT
(Yoon O
et O
al O

JNLPBA B-DAT
(Collier O
and O
Kim, O
2004) O
78.58 O

JNLPBA B-DAT
77.59 O
77.28 O

on O
JNLPBA B-DAT
despite O
being O
trained O
on O
a O

the O
bio-entity O
recognition O
task O
at O
jnlpba B-DAT

datasets. O
The O
SOTA O
model O
for O
JNLPBA B-DAT

ple O
NER O
datasets O
not O
just O
JNLPBA B-DAT
(Yoon O
et O
al O

JNLPBA B-DAT
(Collier O
and O
Kim, O
2004) O
78.58 O

JNLPBA B-DAT
77.59 O
77.28 O

on O
JNLPBA B-DAT
despite O
being O
trained O
on O
a O

CRAFT O
(Bada O
et O
al., O
2012), O
JNLPBA B-DAT
(Collier O
and O
Kim, O
2004), O
ScienceIE O

JNLPBA B-DAT
593,590 O
Protein, O
DNA, O
RNA, O
Cell O

when O
eval- O
uated O
on O
CRAFT, O
JNLPBA B-DAT
and O
ScienceIE O
com- O
pared O
to O

is O
similar O
to O
CRAFT O
and O
JNLPBA, B-DAT
since O
they O
are O
all O
sampled O

JNLPBA B-DAT

except O
pretrained O
word O
vectors O
for O
JNLPBA B-DAT
data O
set O

JNLPBA B-DAT
data O
pairs). O
However, O
if O
the O

CRAFT O
74.22 O
75.45 O
75.77 O
75.45 O
JNLPBA B-DAT
73.19 O
73.24 O
73.65 O
74.29 O

larger O
sources O
(Table O
5). O
On O
JNLPBA, B-DAT
Scien- O
ceIE O
and O
Wetlab, O
LMs O

source O
perform O
better O
on O
CRAFT, O
JNLPBA, B-DAT
and O
ScienceIE O

the O
most O
dissimilar O
source, O
and O
JNLPBA, B-DAT
to O
which O
PubMed O
is O
the O

JNLPBA B-DAT
pair O

Word O
Vectors) O
CoNLL2003 O
(Pretrained O
LMs) O
JNLPBA B-DAT
(Pretrained O
Word O
Vectors) O
JNLPBA O
(Pretrained O

the O
bio-entity O
recognition O
task O
at O
JNLPBA B-DAT

ICVL B-DAT
Hand O
Posture O
Dataset. O
The O
ICVL O
dataset O
[41] O
consists O
of O
330K O

errors O
per O
hand O
keypoints. O
Left: O
ICVL B-DAT
dataset, O
middle: O
NYU O
dataset, O
right O

a) O
ICVL B-DAT

3D O
hand O
pose O
estimation O
datasets O
(ICVL B-DAT
[41 O

of O
the O
V2V-PoseNet O
on O
the O
ICVL, B-DAT
NYU, O
MSRA, O
HANDS O
2017, O
ITOP O

V2V-PoseNet O
is O
two O
days O
for O
ICVL B-DAT
dataset, O
12 O
hours O
for O
NYU O

without O
epoch O
ensemble O
on O
the O
ICVL, B-DAT
NYU, O
MSRA, O
and O
ITOP O
datasets O

of O
our O
V2V-PoseNet O
on O
the O
ICVL B-DAT
dataset. O
Backgrounds O
are O
removed O
to O

80.1 O
40.2 O
86.2 O
74.4 O
79.16 O
Hands B-DAT
51.3 O
70.5 O
30.9 O
68.7 O
55.2 O

P. O
Wohlhart, O
and O
V. O
Lepetit. O
Hands B-DAT
deep O
in O
deep O
learning O
for O

2017 O
[48], O
NYU O
[37], O
and O
ICVL B-DAT
[36]) O
and O
2 O
body O
pose O

from O
frontal O
view O
for O
evaluation. O
ICVL B-DAT
Hand O
Pose O
Dataset O
[36]. O
It O

and O
for O
17 O
epochs O
on O
ICVL B-DAT
and O
HANDS O
2017 O
with O
a O

Table O
4. O
Performance O
comparison O
on O
ICVL B-DAT
dataset O
[36]. O
“Mean O
error" O
indicates O

NYU O
and O
ICVL B-DAT
datasets: O
We O
compare O
A2J O
with O

to O
V2V∗ O
and O
P2P O
on O
ICVL B-DAT
dataset O
on O
accuracy O
but O
with O

keypoints. O
Top: O
NYU O
dataset. O
Bottom: O
ICVL B-DAT
dataset O

40.2 O
86.2 O
74.4 O
79.16 O
75.88 O
Hands B-DAT
51.3 O
70.5 O
30.9 O
68.7 O
84.4 O

Paul O
Wohlhart, O
and O
Vincent O
Lepetit. O
Hands B-DAT
deep O
in O
deep O
learning O
for O

a) O
MSRA O
(b) O
NYU O
(c) O
ICVL B-DAT

dataset O
and O
16 O
joints O
in O
ICVL B-DAT
dataset. O
To O
balance O
the O
complexity O

three O
challenging O
pub- O
lic O
datasets: O
ICVL B-DAT
Hand O
Posture O
Dataset O
[22], O
NYU O

ICVL B-DAT
Hand O
Posture O
Dataset O
[22]. O
This O

approach O
with O
state-of-the-art O
methods O
on O
ICVL B-DAT
dataset. O
Left: O
the O
proportion O
of O

On O
ICVL B-DAT
dataset, O
we O
compare O
our O
proposed O

NYU O
[5] O
11.869 O
11.811(−0.5%) O
ICVL B-DAT
[22] O
6.932 O
6.793(−2.0%) O
MSRA O
[23 O

joint O
er- O
rors O
on O
NYU, O
ICVL B-DAT
and O
MSRA O
dataset O
are O
shown O

ICVL B-DAT

Oberweger, O
P. O
Wohlhart, O
V. O
Lepetit, O
Hands B-DAT
deep O
in O
deep O
learning O
for O

a O
large O
margin. O
On O
the O
ICVL B-DAT
hand O
dataset, O
our O
method O
achieves O

datasets, O
i.e., O
NYU[43], O
ICVL B-DAT

i.e. O
NYU[43], O
MSRA[37] O
and O
ICVL B-DAT

ICVL B-DAT
Dataset O
The O
ICVL O
hand O
dataset[39] O
has O
22K O
frames O

7. O
Comparison O
with O
state-of-the-art O
on O
ICVL B-DAT
[39]. O
We O
plot O
the O
percentage O

pose O
is O
much O
smaller O
in O
ICVL B-DAT
compared O
to O
the O
NYU O
and O

4. O
Comparison O
with O
state-of-art O
on O
ICVL B-DAT

pose O
estimation O
results O
from O
(a) O
ICVL B-DAT

P. O
Wohlhart, O
and O
V. O
Lepetit. O
Hands B-DAT
deep O
in O
deep O
learning O
for O

method O
on O
two O
publicly O
datasets: O
ICVL B-DAT
[11 O

Fig. O
3. O
Example O
results O
on O
ICVL B-DAT
[11] O
and O
NYU O
[4]: O
basic O

settings O
on O
ICVL B-DAT
dataset. O
Then O
we O
compare O
it O

against O
several O
state-of-the-art O
approaches O
on O
ICVL B-DAT

ICVL B-DAT
dataset O
[11 O

In O
details, O
on O
ICVL B-DAT
our O
method O
outperforms O
LSN O
[10 O

5. O
Comparison O
with O
state-of-the-arts O
on O
ICVL B-DAT
[11] O
dataset: O
distance O
error O
(left O

ms) O
of O
different O
methods O
on O
ICVL B-DAT
dataset O
[11 O

Hands B-DAT
deep O
in O
deep O
learning O
for O

the O
three O
main O
benchmarks O
(NYU, O
ICVL, B-DAT
MSRA) O
while O
keeping O
the O
simplicity O

NYU O
[40], O
MSRA O
[28], O
and O
ICVL B-DAT
[34]. O
These O
works O
are O
marked O

the O
NYU O
dataset O
[40], O
the O
ICVL B-DAT
dataset O
[34], O
and O
the O
MSRA O

5.3. O
ICVL B-DAT
Dataset O

The O
ICVL B-DAT
dataset O
[34] O
comprises O
a O
training O

Comparison O
with O
state-of-the-art O
on O
the O
ICVL B-DAT
dataset O
[34]. O
We O
report O
the O

to O
other O
methods O
on O
the O
ICVL B-DAT
dataset O
[34]. O
Our O
approach O
performs O

Comparison O
with O
state-of-the-art O
on O
the O
ICVL B-DAT
dataset O
[34]. O
We O
plot O
the O

P. O
Wohlhart, O
and O
V. O
Lepetit. O
Hands B-DAT
Deep O
in O
Deep O
Learning O
for O

Continuous O
Pose O
Recovery O
of O
Human O
Hands B-DAT
Using O
Convolu- O
tional O
Networks. O
ACM O

on O
3 O
hand O
datasets O
(i.e., O
HANDS B-DAT
2017 O
[48], O
NYU O
[37], O
and O

HANDS B-DAT
2017 O
dataset O
[48]. O
It O
contains O

Table O
2. O
Performance O
comparison O
on O
HANDS B-DAT
2017 O
dataset O
[48]. O
“SEEN" O
and O

17 O
epochs O
on O
ICVL O
and O
HANDS B-DAT
2017 O
with O
a O
learning O
rate O

HANDS B-DAT
2017 O
dataset: O
A2J O
is O
compared O

on O
3 O
hand O
datasets O
(i.e., O
HANDS B-DAT
2017 I-DAT
[48], O
NYU O
[37], O
and O
ICVL O

HANDS B-DAT
2017 I-DAT
dataset O
[48]. O
It O
contains O
957K O

Table O
2. O
Performance O
comparison O
on O
HANDS B-DAT
2017 I-DAT
dataset O
[48]. O
“SEEN" O
and O
“UNSEEN O

17 O
epochs O
on O
ICVL O
and O
HANDS B-DAT
2017 I-DAT
with O
a O
learning O
rate O
decay O

HANDS B-DAT
2017 I-DAT
dataset: O
A2J O
is O
compared O
with O

on O
3 O
hand O
datasets O
(i.e., O
HANDS B-DAT
2017 I-DAT
[48], O
NYU O
[37], O
and O
ICVL O

HANDS B-DAT
2017 I-DAT
dataset O
[48]. O
It O
contains O
957K O

Table O
2. O
Performance O
comparison O
on O
HANDS B-DAT
2017 I-DAT
dataset O
[48]. O
“SEEN" O
and O
“UNSEEN O

17 O
epochs O
on O
ICVL O
and O
HANDS B-DAT
2017 I-DAT
with O
a O
learning O
rate O
decay O

HANDS B-DAT
2017 I-DAT
dataset: O
A2J O
is O
compared O
with O

3 O
hand O
datasets O
(i.e., O
HANDS O
2017 B-DAT
[48], O
NYU O
[37], O
and O
ICVL O

HANDS O
2017 B-DAT
dataset O
[48]. O
It O
contains O
957K O

2. O
Performance O
comparison O
on O
HANDS O
2017 B-DAT
dataset O
[48]. O
“SEEN" O
and O
“UNSEEN O

epochs O
on O
ICVL O
and O
HANDS O
2017 B-DAT
with O
a O
learning O
rate O
decay O

HANDS O
2017 B-DAT
dataset: O
A2J O
is O
compared O
with O

Pattern O
Recognition O
(CVPR), O
pages O
7291–7299, O
2017 B-DAT

Pattern O
Recognition O
(CVPR), O
pages O
77–85, O
2017 B-DAT

pose O
estimation. O
arXiv O
preprint O
arXiv:1708.03416, O
2017 B-DAT

pose O
estimation. O
arXiv O
preprint O
arXiv:1708.03416, O
2017 B-DAT

neural O
network. O
arXiv O
preprint O
arXiv:1704.02224, O
2017 B-DAT

Computer O
Vision O
(ICCV), O
pages O
2334–2343, O
2017 B-DAT

and O
pose O
regression. O
Neurocomputing, O
251:68–80, O
2017 B-DAT

CVPR), O
volume O
1, O
page O
5, O
2017 B-DAT

es- O
timation. O
arXiv O
preprint O
arXiv:1707.07248, O
2017 B-DAT

Process- O
ing O
(ICIP), O
pages O
4512–4516, O
2017 B-DAT

depth O
data. O
arXiv O
preprint O
arXiv:1705.09606, O
2017 B-DAT

pose O
estimation. O
arXiv O
preprint O
arXiv:1706.04758, O
2017 B-DAT

ICCVW), O
volume O
840, O
page O
2, O
2017 B-DAT

Processing O
Systems O
(NIPS), O
pages O
5099–5108, O
2017 B-DAT

Vision O
and O
Pattern O
Recognition O
(CVPR), O
2017 B-DAT

Journal O
of O
Computer O
Vision, O
123(3):454–478, O
2017 B-DAT

and O
Tae- O
Kyun O
Kim. O
The O
2017 B-DAT
hands O
in O
the O
million O
challenge O

pose O
estimation. O
arXiv O
preprint O
arXiv:1707.02237, O
2017 B-DAT

Pattern O
Recognition O
(CVPR), O
pages O
4866–4874, O
2017 B-DAT

and O
placed O
first O
in O
the O
HANDS B-DAT
2017 O
frame-based O
3D O
hand O
pose O

also O
placed O
first O
in O
the O
HANDS B-DAT
2017 O
frame-based O
3D O
hand O
pose O

also O
placed O
first O
in O
the O
HANDS B-DAT
2017 O
frame-based O
3D O
hand O
pose O

HANDS B-DAT
2017 O
Frame-based O
3D O
Hand O
Pose O

Estima- O
tion O
Challenge O
Dataset. O
The O
HANDS B-DAT
2017 O
frame-based O
3D O
hand O
pose O

The O
top-5 O
results O
of O
the O
HANDS B-DAT
2017 O
frame-based O
3D O
hand O
pose O

also O
placed O
first O
in O
the O
HANDS B-DAT
2017 O
frame- O
based O
3D O
hand O

on O
the O
ICVL, O
NYU, O
MSRA, O
HANDS B-DAT
2017, O
ITOP O
front-view, O
and O
ITOP O

MSRA O
datasets, O
six O
days O
for O
HANDS B-DAT
2017 O
challenge O
dataset, O
and O
three O

of O
our O
V2V-PoseNet O
on O
the O
HANDS B-DAT
2017 O
frame-based O
3D O
hand O
pose O

and O
placed O
first O
in O
the O
HANDS B-DAT
2017 I-DAT
frame-based O
3D O
hand O
pose O
estimation O

also O
placed O
first O
in O
the O
HANDS B-DAT
2017 I-DAT
frame-based O
3D O
hand O
pose O
estimation O

also O
placed O
first O
in O
the O
HANDS B-DAT
2017 I-DAT
frame-based O
3D O
hand O
pose O
esti O

HANDS B-DAT
2017 I-DAT
Frame-based O
3D O
Hand O
Pose O
Estima O

- O
tion O
Challenge O
Dataset. O
The O
HANDS B-DAT
2017 I-DAT
frame-based O
3D O
hand O
pose O
estimation O

The O
top-5 O
results O
of O
the O
HANDS B-DAT
2017 I-DAT
frame-based O
3D O
hand O
pose O
estimation O

also O
placed O
first O
in O
the O
HANDS B-DAT
2017 I-DAT
frame- O
based O
3D O
hand O
pose O

on O
the O
ICVL, O
NYU, O
MSRA, O
HANDS B-DAT
2017, I-DAT
ITOP O
front-view, O
and O
ITOP O
top-view O

MSRA O
datasets, O
six O
days O
for O
HANDS B-DAT
2017 I-DAT
challenge O
dataset, O
and O
three O
hours O

of O
our O
V2V-PoseNet O
on O
the O
HANDS B-DAT
2017 I-DAT
frame-based O
3D O
hand O
pose O
estimation O

and O
placed O
first O
in O
the O
HANDS B-DAT
2017 I-DAT
frame-based O
3D O
hand O
pose O
estimation O

also O
placed O
first O
in O
the O
HANDS B-DAT
2017 I-DAT
frame-based O
3D O
hand O
pose O
estimation O

also O
placed O
first O
in O
the O
HANDS B-DAT
2017 I-DAT
frame-based O
3D O
hand O
pose O
esti O

HANDS B-DAT
2017 I-DAT
Frame-based O
3D O
Hand O
Pose O
Estima O

- O
tion O
Challenge O
Dataset. O
The O
HANDS B-DAT
2017 I-DAT
frame-based O
3D O
hand O
pose O
estimation O

The O
top-5 O
results O
of O
the O
HANDS B-DAT
2017 I-DAT
frame-based O
3D O
hand O
pose O
estimation O

also O
placed O
first O
in O
the O
HANDS B-DAT
2017 I-DAT
frame- O
based O
3D O
hand O
pose O

on O
the O
ICVL, O
NYU, O
MSRA, O
HANDS B-DAT
2017, I-DAT
ITOP O
front-view, O
and O
ITOP O
top-view O

MSRA O
datasets, O
six O
days O
for O
HANDS B-DAT
2017 I-DAT
challenge O
dataset, O
and O
three O
hours O

of O
our O
V2V-PoseNet O
on O
the O
HANDS B-DAT
2017 I-DAT
frame-based O
3D O
hand O
pose O
estimation O

placed O
first O
in O
the O
HANDS O
2017 B-DAT
frame-based O
3D O
hand O
pose O
estimation O

placed O
first O
in O
the O
HANDS O
2017 B-DAT
frame-based O
3D O
hand O
pose O
estimation O

placed O
first O
in O
the O
HANDS O
2017 B-DAT
frame-based O
3D O
hand O
pose O
esti O

HANDS O
2017 B-DAT
Frame-based O
3D O
Hand O
Pose O
Estima O

tion O
Challenge O
Dataset. O
The O
HANDS O
2017 B-DAT
frame-based O
3D O
hand O
pose O
estimation O

top-5 O
results O
of O
the O
HANDS O
2017 B-DAT
frame-based O
3D O
hand O
pose O
estimation O

placed O
first O
in O
the O
HANDS O
2017 B-DAT
frame- O
based O
3D O
hand O
pose O

the O
ICVL, O
NYU, O
MSRA, O
HANDS O
2017, B-DAT
ITOP O
front-view, O
and O
ITOP O
top-view O

datasets, O
six O
days O
for O
HANDS O
2017 B-DAT
challenge O
dataset, O
and O
three O
hours O

our O
V2V-PoseNet O
on O
the O
HANDS O
2017 B-DAT
frame-based O
3D O
hand O
pose O
estimation O

pose O
estimation. O
arXiv O
preprint O
arXiv:1708.03416, O
2017 B-DAT

Conference O
on O
Computer O
Vision, O
Oct O
2017 B-DAT

neural O
network. O
arXiv O
preprint O
arXiv:1704.02224, O
2017 B-DAT

and O
pose O
regression. O
Neurocomputing, O
251:68–80, O
2017 B-DAT

pose O
annotations. O
arXiv O
preprint O
arXiv:1704.02463, O
2017 B-DAT

Computer O
Vision O
and O
Pattern O
Recognition, O
2017 B-DAT

pose O
estimation. O
arXiv O
preprint O
arXiv:1707.07248, O
2017 B-DAT

Confer- O
ence O
on O
Image O
Processing, O
2017 B-DAT

depth O
data. O
arXiv O
preprint O
arXiv:1705.09606, O
2017 B-DAT

Recognition, O
pages O
230– O
237. O
IEEE, O
2017 B-DAT

on O
Computer O
Vision O
Workshop, O
Oct O
2017 B-DAT

and O
Pattern O
Recognition, O
pages O
7025–7034, O
2017 B-DAT

Vision O
and O
Pattern O
Recognition, O
July O
2017 B-DAT

of O
Computer O
Vision, O
pages O
1–25, O
2017 B-DAT

Garcia-Hernando, O
and O
T.-K. O
Kim. O
The O
2017 B-DAT
hands O
in O
the O
million O
challenge O

estima- O
tion. O
arXiv O
preprint O
arXiv:1707.02237, O
2017 B-DAT

and O
Pat- O
tern O
Recognition, O
July O
2017 B-DAT

images O
for O
each O
of O
the O
front-view B-DAT
and O
top-view O
tracks. O
This O
dataset O

mAP O
(front-view) B-DAT
mAP O
(top-view O

NYU, O
MSRA, O
HANDS O
2017, O
ITOP O
front-view, B-DAT
and O
ITOP O
top-view O
datasets O
are O

V2V-PoseNet O
on O
the O
ITOP O
dataset O
(front-view B-DAT

view B-DAT
CNN O
to O
estimate O
2D O
heatmaps O

for O
each O
view B-DAT

used O
frames O
from O
the O
frontal O
view B-DAT
and O
14 O
out O
of O
36 O

view B-DAT
and O
top-view O
tracks. O
This O
dataset O

surface O
normals O
(LSN) O
[48], O
multi- O
view B-DAT
CNN O
(MultiView) O
[10], O
DISCO O
[1 O

view) B-DAT
mAP O
(top-view O

view, B-DAT
and O
ITOP O
top-view O
datasets O
are O

view B-DAT

view B-DAT

view B-DAT
cnn O
to O
multi-view O
cnns. O
In O

ITOP B-DAT
Human O
Pose O
Dataset. O
The O
ITOP O
dataset O
[16] O
consists O
of O
40K O

proposed O
sys- O
tem O
on O
the O
ITOP B-DAT
3D O
human O
pose O
estimation O
dataset O

and O
top O
views O
of O
the O
ITOP B-DAT
dataset O

ICVL, O
NYU, O
MSRA, O
HANDS O
2017, O
ITOP B-DAT
front-view, O
and O
ITOP O
top-view O
datasets O

dataset, O
and O
three O
hours O
for O
ITOP B-DAT
dataset. O
The O
testing O
time O
is O

the O
ICVL, O
NYU, O
MSRA, O
and O
ITOP B-DAT
datasets O
while O
running O
in O
real-time O

of O
our O
V2V-PoseNet O
on O
the O
ITOP B-DAT
dataset O
(front-view). O
Backgrounds O
are O
removed O

of O
our O
V2V-PoseNet O
on O
the O
ITOP B-DAT
dataset O
(top-view). O
Backgrounds O
are O
removed O

ICVL, O
NYU, O
MSRA, O
HANDS O
2017, O
ITOP B-DAT
front I-DAT

ICVL, O
NYU, O
MSRA, O
HANDS O
2017, O
ITOP B-DAT
front-view, I-DAT
and O
ITOP O
top-view O
datasets O
are O

b) O
Body O
cases O
from O
ITOP O
front-view B-DAT
dataset O

depth O
images O
both O
for O
the O
front-view B-DAT
and O
top- O
view O
tracks. O
Each O

front-view B-DAT
and O
top-view O
tracks, O
except O
V2V O

is O
3.1 O
at O
least O
for O
front-view B-DAT
case, O
and O
5 O
at O
least O

mAP O
(front-view) B-DAT
mAP O
(top-view O

ITOP O
front-view B-DAT

ITOP O
front-view B-DAT
(mAP) O
87.3 O
88.0 O

ITOP O
front-view B-DAT

on O
NYU O
[37] O
and O
ITOP O
(front-view) B-DAT
[20] O
datasets O
are O
shown O
in O

b) O
Qualitative O
results O
on O
ITOP O
front-view B-DAT
dataset O

view B-DAT
dataset O

the O
36 O
joints O
from O
frontal O
view B-DAT
for O
evaluation. O
ICVL O
Hand O
Pose O

for O
the O
front-view B-DAT
and O
top- O
view O
tracks. O
Each O
depth O
image O
is O

view B-DAT
and O
top-view O
tracks, O
except O
V2V O

view B-DAT
case, O
and O
5 O
at O
least O

view B-DAT
case. O
This O
reveals O
that O
A2J O

view) B-DAT
mAP O
(top-view O

view B-DAT

view B-DAT
(mAP) O
87.3 O
88.0 O

view B-DAT

view) B-DAT
[20] O
datasets O
are O
shown O
in O

view B-DAT
dataset O

view B-DAT
cnn O
to O
multi-view O
cnns. O
In O

2 O
body O
pose O
datasets O
(i.e., O
ITOP B-DAT
[20] O
and O
K2HPD O
[42]) O
to O

b) O
Body O
cases O
from O
ITOP B-DAT
front-view O
dataset O

16 O
hand O
joints O
are O
annotated. O
ITOP B-DAT
Body O
Pose O
Dataset O
[20]. O
It O

ITOP B-DAT
dataset: O
We O
also O
compare O
A2J O

Table O
5. O
Performance O
comparison O
on O
ITOP B-DAT
3D O
body O
pose O
estimation O
dataset O

on O
NYU O
[37] O
(hand), O
and O
ITOP B-DAT
[20] O
dataset O
(body). O
We O
will O

ITOP B-DAT
front-view O

ImageNet O
on O
NYU O
(hand) O
and O
ITOP B-DAT
(body) O
datasets. O
The O
performance O
comparison O

ITOP B-DAT
front-view O
(mAP) O
87.3 O
88.0 O

ITOP B-DAT
front-view O

A2J O
on O
NYU O
[37] O
and O
ITOP B-DAT
(front-view) O
[20] O
datasets O
are O
shown O

b) O
Qualitative O
results O
on O
ITOP B-DAT
front-view O
dataset O

b) O
Body O
cases O
from O
ITOP B-DAT
front I-DAT

ITOP B-DAT
front I-DAT

ITOP B-DAT
front I-DAT

ITOP B-DAT
front I-DAT

b) O
Qualitative O
results O
on O
ITOP B-DAT
front I-DAT

b) O
Body O
cases O
from O
ITOP B-DAT
front-view I-DAT
dataset O

ITOP B-DAT
front-view I-DAT

ITOP B-DAT
front-view I-DAT
(mAP) O
87.3 O
88.0 O

ITOP B-DAT
front-view I-DAT

b) O
Qualitative O
results O
on O
ITOP B-DAT
front-view I-DAT
dataset O

a) O
ITOP O
(front-view B-DAT

ITOP O
(front-view) B-DAT
ITOP O
(top-view) O
EVAL O

results O
in O
Figure O
6. O
The O
front-view B-DAT
ITOP O
dataset O
is O
shown O
in O

annotated O
depth O
images O
from O
extreme O
view B-DAT

to O
the O
reader, O
a O
front O
view B-DAT
is O
shown O
in O
the O
lower O

results O
on O
front O
and O
side O
view B-DAT
RGB O
images. O
However, O
a O
fundamental O

contains O
challenging O
front O
and O
top O
view B-DAT
images O

locations O
from O
the O
point O
of O
view B-DAT
of O
the O
respective O
camera. O
The O

two O
“views,” B-DAT
namely O
the O
front/side O
view O
and O
the O
top O
view. O
The O

frontal O
view B-DAT
contains O
360 O

necessarily O
uniformly O
distributed. O
The O
top O
view B-DAT
contains O
images O
captured O
solely O
from O

view B-DAT

view B-DAT

test O
all O
models O
on O
front O
view B-DAT
images. O
This O
is O
the O
classical O

test O
all O
models O
on O
top O
view B-DAT
images. O
This O
is O
similar O
to O

Third, O
we O
train O
on O
front O
view B-DAT
images O
and O
test O
on O
top O

view B-DAT
images. O
This O
is O
the O
most O

view) B-DAT
ITOP O
(top-view) O
EVAL O

view B-DAT
ITOP O
dataset O
is O
shown O
in O

Train O
on O
top O
view, B-DAT
test O
on O
top O
view. O
Figure O

RTW) O
[67]. O
For O
the O
top-down O
view, B-DAT
we O
show O
only O
8 O
joints O

view B-DAT
case O
where O
recurrent O
feedback O
achieves O

annotated O
depth O
images O
from O
several O
view B-DAT
points O

Combining O
local O
appearance O
and O
holistic O
view B-DAT

ground O
truth. O
From O
the O
top O
view, B-DAT
the O
lower O
body O
parts O
are O

walk O
(step O
300) O
for O
the O
view B-DAT

a) O
EVAL O
[21] O
(b) O
ITOP B-DAT
(Front) O
(c) O
ITOP O
(Top O

the O
datasets. O
Our O
newly O
collected O
ITOP B-DAT
dataset O
contains O
challenging O
front O
and O

Invariant-Top O
View O
Dataset O
(ITOP B-DAT

from O
multiple O
camera O
viewpoints. O
Named O
ITOP, B-DAT
the O
dataset O
consists O
of O
20 O

a) O
ITOP B-DAT
(front-view O

b) O
ITOP B-DAT
(top-view O

EVAL O
dataset. O
We O
discuss O
the O
ITOP B-DAT
results O
below. O
For O
frontal O
views O

ITOP B-DAT
(front-view) O
ITOP O
(top-view) O
EVAL O

in O
Figure O
6. O
The O
front-view O
ITOP B-DAT
dataset O
is O
shown O
in O
columns O

different O
feedback O
mechanisms O
on O
the O
ITOP B-DAT
front O
dataset. O
Rows O
denote O
a O

different O
feedback O
mechanisms O
on O
the O
ITOP B-DAT
front I-DAT
dataset. O
Rows O
denote O
a O
different O

on O
3 O
hand O
datasets O
(i.e., O
HANDS B-DAT
2017 O
[48], O
NYU O
[37], O
and O

HANDS B-DAT
2017 O
dataset O
[48]. O
It O
contains O

Table O
2. O
Performance O
comparison O
on O
HANDS B-DAT
2017 O
dataset O
[48]. O
“SEEN" O
and O

17 O
epochs O
on O
ICVL O
and O
HANDS B-DAT
2017 O
with O
a O
learning O
rate O

HANDS B-DAT
2017 O
dataset: O
A2J O
is O
compared O

on O
3 O
hand O
datasets O
(i.e., O
HANDS B-DAT
2017 I-DAT
[48], O
NYU O
[37], O
and O
ICVL O

HANDS B-DAT
2017 I-DAT
dataset O
[48]. O
It O
contains O
957K O

Table O
2. O
Performance O
comparison O
on O
HANDS B-DAT
2017 I-DAT
dataset O
[48]. O
“SEEN" O
and O
“UNSEEN O

17 O
epochs O
on O
ICVL O
and O
HANDS B-DAT
2017 I-DAT
with O
a O
learning O
rate O
decay O

HANDS B-DAT
2017 I-DAT
dataset: O
A2J O
is O
compared O
with O

on O
3 O
hand O
datasets O
(i.e., O
HANDS B-DAT
2017 I-DAT
[48], O
NYU O
[37], O
and O
ICVL O

HANDS B-DAT
2017 I-DAT
dataset O
[48]. O
It O
contains O
957K O

Table O
2. O
Performance O
comparison O
on O
HANDS B-DAT
2017 I-DAT
dataset O
[48]. O
“SEEN" O
and O
“UNSEEN O

17 O
epochs O
on O
ICVL O
and O
HANDS B-DAT
2017 I-DAT
with O
a O
learning O
rate O
decay O

HANDS B-DAT
2017 I-DAT
dataset: O
A2J O
is O
compared O
with O

3 O
hand O
datasets O
(i.e., O
HANDS O
2017 B-DAT
[48], O
NYU O
[37], O
and O
ICVL O

HANDS O
2017 B-DAT
dataset O
[48]. O
It O
contains O
957K O

2. O
Performance O
comparison O
on O
HANDS O
2017 B-DAT
dataset O
[48]. O
“SEEN" O
and O
“UNSEEN O

epochs O
on O
ICVL O
and O
HANDS O
2017 B-DAT
with O
a O
learning O
rate O
decay O

HANDS O
2017 B-DAT
dataset: O
A2J O
is O
compared O
with O

Pattern O
Recognition O
(CVPR), O
pages O
7291–7299, O
2017 B-DAT

Pattern O
Recognition O
(CVPR), O
pages O
77–85, O
2017 B-DAT

pose O
estimation. O
arXiv O
preprint O
arXiv:1708.03416, O
2017 B-DAT

pose O
estimation. O
arXiv O
preprint O
arXiv:1708.03416, O
2017 B-DAT

neural O
network. O
arXiv O
preprint O
arXiv:1704.02224, O
2017 B-DAT

Computer O
Vision O
(ICCV), O
pages O
2334–2343, O
2017 B-DAT

and O
pose O
regression. O
Neurocomputing, O
251:68–80, O
2017 B-DAT

CVPR), O
volume O
1, O
page O
5, O
2017 B-DAT

es- O
timation. O
arXiv O
preprint O
arXiv:1707.07248, O
2017 B-DAT

Process- O
ing O
(ICIP), O
pages O
4512–4516, O
2017 B-DAT

depth O
data. O
arXiv O
preprint O
arXiv:1705.09606, O
2017 B-DAT

pose O
estimation. O
arXiv O
preprint O
arXiv:1706.04758, O
2017 B-DAT

ICCVW), O
volume O
840, O
page O
2, O
2017 B-DAT

Processing O
Systems O
(NIPS), O
pages O
5099–5108, O
2017 B-DAT

Vision O
and O
Pattern O
Recognition O
(CVPR), O
2017 B-DAT

Journal O
of O
Computer O
Vision, O
123(3):454–478, O
2017 B-DAT

and O
Tae- O
Kyun O
Kim. O
The O
2017 B-DAT
hands O
in O
the O
million O
challenge O

pose O
estimation. O
arXiv O
preprint O
arXiv:1707.02237, O
2017 B-DAT

Pattern O
Recognition O
(CVPR), O
pages O
4866–4874, O
2017 B-DAT

NYU B-DAT
Hand O
Pose O
Dataset. O
The O
NYU O
dataset O
[45] O
con- O
sists O
of O

and O
output O
types O
in O
the O
NYU B-DAT
dataset. O
The O
number O
in O
the O

error O
is O
calculated O
in O
the O
NYU B-DAT
dataset O

We O
used O
NYU B-DAT
hand O
pose O
dataset O
[45] O
to O

keypoints. O
Left: O
ICVL O
dataset, O
middle: O
NYU B-DAT
dataset, O
right: O
MSRA O
dataset O

b) O
NYU B-DAT

NYU B-DAT
[45], O
and O
MSRA O
[39]) O
with O

works O
is O
largest O
on O
the O
NYU B-DAT
dataset O
that O
is O
very O
challenging O

the O
V2V-PoseNet O
on O
the O
ICVL, O
NYU, B-DAT
MSRA, O
HANDS O
2017, O
ITOP O
front-view O

ICVL O
dataset, O
12 O
hours O
for O
NYU B-DAT
and O
MSRA O
datasets, O
six O
days O

epoch O
ensemble O
on O
the O
ICVL, O
NYU, B-DAT
MSRA, O
and O
ITOP O
datasets O
while O

of O
our O
V2V-PoseNet O
on O
the O
NYU B-DAT
dataset. O
Backgrounds O
are O
removed O
to O

80.1 O
40.2 O
86.2 O
74.4 O
79.16 O
Hands B-DAT
51.3 O
70.5 O
30.9 O
68.7 O
55.2 O

P. O
Wohlhart, O
and O
V. O
Lepetit. O
Hands B-DAT
deep O
in O
deep O
learning O
for O

datasets O
(i.e., O
HANDS O
2017 O
[48], O
NYU B-DAT
[37], O
and O
ICVL O
[36]) O
and O

a) O
Hand O
cases O
from O
NYU B-DAT
dataset O

coordinates O
of O
21 O
hand O
joints. O
NYU B-DAT
Hand O
Pose O
Dataset O
[37]. O
It O

Table O
3. O
Performance O
comparison O
on O
NYU B-DAT
dataset O
[37]. O
“Mean O
error" O
indicates O

cases. O
A2J O
is O
trained O
on O
NYU B-DAT
for O
34 O
epochs O
with O
a O

NYU B-DAT
and O
ICVL O
datasets: O
We O
compare O

errors O
per O
hand O
keypoints. O
Top: O
NYU B-DAT
dataset. O
Bottom: O
ICVL O
dataset O

A2J O
is O
exe- O
cuted O
on O
NYU B-DAT
[37] O
(hand), O
and O
ITOP O
[20 O

NYU B-DAT
(hand O

without O
pre-training O
on O
ImageNet O
on O
NYU B-DAT
(hand) O
and O
ITOP O
(body) O
datasets O

Pre-train O
From O
scratch O
ImageNet O
pre-training O
NYU B-DAT
(error) O
10.08 O
8.61 O

NYU B-DAT
error O
9.32 O
9.01 O
8.61 O

qualitative O
results O
of O
A2J O
on O
NYU B-DAT
[37] O
and O
ITOP O
(front-view) O
[20 O

a) O
Qualitative O
results O
on O
NYU B-DAT
dataset O

40.2 O
86.2 O
74.4 O
79.16 O
75.88 O
Hands B-DAT
51.3 O
70.5 O
30.9 O
68.7 O
84.4 O

Paul O
Wohlhart, O
and O
Vincent O
Lepetit. O
Hands B-DAT
deep O
in O
deep O
learning O
for O

highly O
accurate. O
On O
MSRA O
and O
NYU B-DAT
hand O
dataset, O
our O
method O
outperforms O

datasets, O
i.e., O
NYU B-DAT

i.e. O
NYU B-DAT

and O
ICVL[39]. O
We O
choose O
the O
NYU B-DAT
dataset O
to O
conduct O
ablation O
experiments O

methods O
(Sec. O
4.1) O
on O
the O
NYU B-DAT
dataset[43]. O
Num- O
ber O
in O
the O

NYU B-DAT
Dataset O
The O
NYU O
hand O
dataset O
[43] O
contains O
over O

joints O
and O
all O
frames O
on O
NYU B-DAT
dataset[43]. O
We O
choose O
2 O
stacks O

4. O
Comparison O
with O
state-of-the-art O
on O
NYU B-DAT
[43]. O
We O
plot O
the O
percentage O

2. O
Comparison O
with O
state-of-the-art O
on O
NYU B-DAT

average O
3D O
error O
on O
the O
NYU B-DAT

Hand O
pose O
estimation O
results O
on O
NYU B-DAT
dataset[43]. O
(a) O
Successful O
samples O
with O

in O
ICVL O
compared O
to O
the O
NYU B-DAT
and O
MSRA O
datasets. O
We O
compare O

P. O
Wohlhart, O
and O
V. O
Lepetit. O
Hands B-DAT
deep O
in O
deep O
learning O
for O

a) O
MSRA O
(b) O
NYU B-DAT
(c) O
ICVL O

ICVL O
Hand O
Posture O
Dataset O
[22], O
NYU B-DAT
Hand O
Pose O
Dataset O
[5] O
and O

NYU B-DAT
Hand O
Pose O
Dataset O
[5]. O
The O

NYU B-DAT
hand O
pose O
dataset O
was O
collected O

approach O
with O
state-of-the-art O
methods O
on O
NYU B-DAT
dataset. O
Left: O
the O
proportion O
of O

methods O
on O
the O
bench- O
mark O
NYU B-DAT
dataset O
for O
hand O
pose O
estimation O

On O
NYU B-DAT
dataset, O
we O
compare O
our O
proposed O

The O
average O
joint O
errors O
on O
NYU B-DAT
dataset O
with O
using O
the O
different O

NYU B-DAT
[5] O
11.869 O
11.811(−0.5%) O
ICVL O
[22 O

curve O
and O
per-joint O
errors O
on O
NYU B-DAT
dataset O
are O
shown O
in O
Figure O

mean O
joint O
er- O
rors O
on O
NYU, B-DAT
ICVL O
and O
MSRA O
dataset O
are O

we O
conduct O
several O
experiments O
on O
NYU B-DAT
dataset O
with O
different O
initializations O

the O
number O
of O
iteration O
on O
NYU B-DAT
dataset O

of O
the O
iterative O
process O
on O
NYU B-DAT
dataset. O
The O
first O
column O
shows O

used O
in O
inference O
phase O
on O
NYU B-DAT
dataset. O
Left: O
the O
proportion O
of O

pose O
as O
the O
initialization O
on O
NYU B-DAT
dataset. O
Left: O
the O
proportion O
of O

Figure O
13: O
Qualitative O
results O
on O
NYU B-DAT
dataset O
of O
different O
stages O

NYU B-DAT

Oberweger, O
P. O
Wohlhart, O
V. O
Lepetit, O
Hands B-DAT
deep O
in O
deep O
learning O
for O

on O
the O
three O
main O
benchmarks O
(NYU, B-DAT
ICVL, O
MSRA) O
while O
keeping O
the O

three O
major O
benchmark O
datasets, O
i.e. O
NYU B-DAT
[40], O
MSRA O
[28], O
and O
ICVL O

Scaling: O
The O
MSRA O
[28] O
and O
NYU B-DAT
[40] O
datasets O
contain O
different O
persons O

for O
hand O
pose O
estimation: O
the O
NYU B-DAT
dataset O
[40], O
the O
ICVL O
dataset O

5.2. O
NYU B-DAT
Dataset O

The O
NYU B-DAT
dataset O
[40] O
contains O
over O
72k O

Comparison O
with O
state-of-the-art O
on O
the O
NYU B-DAT
dataset O
[40]. O
We O
report O
the O

state-of-the-art O
discriminative O
methods O
on O
the O
NYU B-DAT
dataset O
[40]. O
We O
plot O
the O

state-of-the-art O
model-based O
methods O
on O
the O
NYU B-DAT
dataset O
[40]. O
We O
plot O
the O

performs O
significantly O
better O
on O
the O
NYU B-DAT
dataset, O
as O
shown O
in O
Figure O

evaluate O
the O
modifications O
on O
the O
NYU B-DAT
dataset O
[40], O
since O
it O
has O

translation O
or O
rotation O
on O
the O
NYU B-DAT
dataset, O
it O
can O
help O
in O

new O
training O
procedure O
on O
the O
NYU B-DAT
dataset O
[40]. O
By O
using O
different O

of O
hand O
localization O
accuracy O
on O
NYU B-DAT
dataset O
[40]. O
The O
ground O
truth O

DeepPrior O
and O
DeepPrior++ O
on O
the O
NYU B-DAT
dataset O
[40]. O
We O
show O
the O

of O
network O
architecture O
on O
the O
NYU B-DAT
dataset O
[40]. O
The O
more O
recent O

P. O
Wohlhart, O
and O
V. O
Lepetit. O
Hands B-DAT
Deep O
in O
Deep O
Learning O
for O

Continuous O
Pose O
Recovery O
of O
Human O
Hands B-DAT
Using O
Convolu- O
tional O
Networks. O
ACM O

and O
NYU B-DAT
[4]. O
The O
former O
dataset O
has O

results O
on O
ICVL O
[11] O
and O
NYU B-DAT
[4]: O
basic O
network O
(first O
row O

5] O
[9] O
[14] O
[10] O
and O
NYU B-DAT
dataset O
[4] O
[5] O
[6 O

7.77% O
relative O
improvement. O
Similarly O
on O
NYU, B-DAT
our O
re- O
sults O
are O
better O

6. O
Comparison O
with O
state-of-the-arts O
on O
NYU B-DAT
[4] O
datasets: O
distance O
error O
(left O

Hands B-DAT
deep O
in O
deep O
learning O
for O

HANDS O
2017, O
ITOP O
front-view, O
and O
ITOP B-DAT
top-view I-DAT
datasets O
are O
shown O
in O
Figure O

view B-DAT
CNN O
to O
estimate O
2D O
heatmaps O

for O
each O
view B-DAT

used O
frames O
from O
the O
frontal O
view B-DAT
and O
14 O
out O
of O
36 O

view B-DAT
and O
top-view O
tracks. O
This O
dataset O

surface O
normals O
(LSN) O
[48], O
multi- O
view B-DAT
CNN O
(MultiView) O
[10], O
DISCO O
[1 O

view) B-DAT
mAP O
(top-view O

view, B-DAT
and O
ITOP O
top-view O
datasets O
are O

view B-DAT

view B-DAT

view B-DAT
cnn O
to O
multi-view O
cnns. O
In O

ITOP B-DAT
Human O
Pose O
Dataset. O
The O
ITOP O
dataset O
[16] O
consists O
of O
40K O

proposed O
sys- O
tem O
on O
the O
ITOP B-DAT
3D O
human O
pose O
estimation O
dataset O

and O
top O
views O
of O
the O
ITOP B-DAT
dataset O

ICVL, O
NYU, O
MSRA, O
HANDS O
2017, O
ITOP B-DAT
front-view, O
and O
ITOP O
top-view O
datasets O

dataset, O
and O
three O
hours O
for O
ITOP B-DAT
dataset. O
The O
testing O
time O
is O

the O
ICVL, O
NYU, O
MSRA, O
and O
ITOP B-DAT
datasets O
while O
running O
in O
real-time O

of O
our O
V2V-PoseNet O
on O
the O
ITOP B-DAT
dataset O
(front-view). O
Backgrounds O
are O
removed O

of O
our O
V2V-PoseNet O
on O
the O
ITOP B-DAT
dataset O
(top-view). O
Backgrounds O
are O
removed O

each O
of O
the O
front-view O
and O
top-view B-DAT
tracks. O
This O
dataset O

mAP O
(front-view) O
mAP O
(top-view B-DAT

2017, O
ITOP O
front-view, O
and O
ITOP O
top-view B-DAT
datasets O
are O
shown O
in O
Figure O

V2V-PoseNet O
on O
the O
ITOP O
dataset O
(top-view B-DAT

HANDS O
2017, O
ITOP O
front-view, O
and O
ITOP B-DAT
top I-DAT

view B-DAT
dataset O

the O
36 O
joints O
from O
frontal O
view B-DAT
for O
evaluation. O
ICVL O
Hand O
Pose O

for O
the O
front-view B-DAT
and O
top- O
view O
tracks. O
Each O
depth O
image O
is O

view B-DAT
and O
top-view O
tracks, O
except O
V2V O

view B-DAT
case, O
and O
5 O
at O
least O

view B-DAT
case. O
This O
reveals O
that O
A2J O

view) B-DAT
mAP O
(top-view O

view B-DAT

view B-DAT
(mAP) O
87.3 O
88.0 O

view B-DAT

view) B-DAT
[20] O
datasets O
are O
shown O
in O

view B-DAT
dataset O

view B-DAT
cnn O
to O
multi-view O
cnns. O
In O

2 O
body O
pose O
datasets O
(i.e., O
ITOP B-DAT
[20] O
and O
K2HPD O
[42]) O
to O

b) O
Body O
cases O
from O
ITOP B-DAT
front-view O
dataset O

16 O
hand O
joints O
are O
annotated. O
ITOP B-DAT
Body O
Pose O
Dataset O
[20]. O
It O

ITOP B-DAT
dataset: O
We O
also O
compare O
A2J O

Table O
5. O
Performance O
comparison O
on O
ITOP B-DAT
3D O
body O
pose O
estimation O
dataset O

on O
NYU O
[37] O
(hand), O
and O
ITOP B-DAT
[20] O
dataset O
(body). O
We O
will O

ITOP B-DAT
front-view O

ImageNet O
on O
NYU O
(hand) O
and O
ITOP B-DAT
(body) O
datasets. O
The O
performance O
comparison O

ITOP B-DAT
front-view O
(mAP) O
87.3 O
88.0 O

ITOP B-DAT
front-view O

A2J O
on O
NYU O
[37] O
and O
ITOP B-DAT
(front-view) O
[20] O
datasets O
are O
shown O

b) O
Qualitative O
results O
on O
ITOP B-DAT
front-view O
dataset O

front-view O
and O
top-view B-DAT
tracks, O
except O
V2V∗. O
The O
perfor O

and O
5 O
at O
least O
for O
top-view B-DAT
case. O
This O
reveals O
that O
A2J O

mAP O
(front-view) O
mAP O
(top-view B-DAT

annotated O
depth O
images O
from O
extreme O
view B-DAT

to O
the O
reader, O
a O
front O
view B-DAT
is O
shown O
in O
the O
lower O

results O
on O
front O
and O
side O
view B-DAT
RGB O
images. O
However, O
a O
fundamental O

contains O
challenging O
front O
and O
top O
view B-DAT
images O

locations O
from O
the O
point O
of O
view B-DAT
of O
the O
respective O
camera. O
The O

two O
“views,” B-DAT
namely O
the O
front/side O
view O
and O
the O
top O
view. O
The O

frontal O
view B-DAT
contains O
360 O

necessarily O
uniformly O
distributed. O
The O
top O
view B-DAT
contains O
images O
captured O
solely O
from O

view B-DAT

view B-DAT

test O
all O
models O
on O
front O
view B-DAT
images. O
This O
is O
the O
classical O

test O
all O
models O
on O
top O
view B-DAT
images. O
This O
is O
similar O
to O

Third, O
we O
train O
on O
front O
view B-DAT
images O
and O
test O
on O
top O

view B-DAT
images. O
This O
is O
the O
most O

view) B-DAT
ITOP O
(top-view) O
EVAL O

view B-DAT
ITOP O
dataset O
is O
shown O
in O

Train O
on O
top O
view, B-DAT
test O
on O
top O
view. O
Figure O

RTW) O
[67]. O
For O
the O
top-down O
view, B-DAT
we O
show O
only O
8 O
joints O

view B-DAT
case O
where O
recurrent O
feedback O
achieves O

annotated O
depth O
images O
from O
several O
view B-DAT
points O

Combining O
local O
appearance O
and O
holistic O
view B-DAT

ground O
truth. O
From O
the O
top O
view, B-DAT
the O
lower O
body O
parts O
are O

walk O
(step O
300) O
for O
the O
view B-DAT

a) O
EVAL O
[21] O
(b) O
ITOP B-DAT
(Front) O
(c) O
ITOP O
(Top O

the O
datasets. O
Our O
newly O
collected O
ITOP B-DAT
dataset O
contains O
challenging O
front O
and O

Invariant-Top O
View O
Dataset O
(ITOP B-DAT

from O
multiple O
camera O
viewpoints. O
Named O
ITOP, B-DAT
the O
dataset O
consists O
of O
20 O

a) O
ITOP B-DAT
(front-view O

b) O
ITOP B-DAT
(top-view O

EVAL O
dataset. O
We O
discuss O
the O
ITOP B-DAT
results O
below. O
For O
frontal O
views O

ITOP B-DAT
(front-view) O
ITOP O
(top-view) O
EVAL O

in O
Figure O
6. O
The O
front-view O
ITOP B-DAT
dataset O
is O
shown O
in O
columns O

different O
feedback O
mechanisms O
on O
the O
ITOP B-DAT
front O
dataset. O
Rows O
denote O
a O

b) O
ITOP O
(top-view B-DAT

ITOP O
(front-view) O
ITOP O
(top-view) B-DAT
EVAL O

improve O
performance, O
especially O
in O
the O
top-view B-DAT
case O
where O
recurrent O
feedback O
achieves O

efficient O
and O
highly O
accurate. O
On O
MSRA B-DAT
and O
NYU O
hand O
dataset, O
our O

datasets, O
i.e., O
NYU[43], O
ICVL[39] O
and O
MSRA B-DAT

i.e. O
NYU[43], O
MSRA B-DAT

MSRA B-DAT
Dataset O
The O
MSRA O
hand O
dataset[37] O
contains O
76.5K O
images O

6. O
Comparison O
with O
state-of-the-art O
on O
MSRA B-DAT

3. O
Comparison O
with O
state-of-art O
on O
MSRA B-DAT
[37]. O
We O
plot O
the O
percentage O

compared O
to O
the O
NYU O
and O
MSRA B-DAT
datasets. O
We O
compare O
our O
method O

results O
from O
(a) O
ICVL[39], O
(b) O
MSRA B-DAT

P. O
Wohlhart, O
and O
V. O
Lepetit. O
Hands B-DAT
deep O
in O
deep O
learning O
for O

camera O
projection O
parameters O
in O
the O
MSRA B-DAT
dataset O
were O
used O
for O
our O

MSRA B-DAT
Hand O
Pose O
Dataset. O
The O
MSRA O
dataset O
[39] O
contains O
9 O
subjects O

dataset, O
middle: O
NYU O
dataset, O
right: O
MSRA B-DAT
dataset O

c) O
MSRA B-DAT

pitch O
(right) O
angles O
on O
the O
MSRA B-DAT
dataset O

NYU O
[45], O
and O
MSRA B-DAT
[39]) O
with O
most O
of O
the O

and O
pitch O
angles O
on O
the O
MSRA B-DAT
dataset O
following O
the O
protocol O
of O

V2V-PoseNet O
on O
the O
ICVL, O
NYU, O
MSRA, B-DAT
HANDS O
2017, O
ITOP O
front-view, O
and O

12 O
hours O
for O
NYU O
and O
MSRA B-DAT
datasets, O
six O
days O
for O
HANDS O

ensemble O
on O
the O
ICVL, O
NYU, O
MSRA, B-DAT
and O
ITOP O
datasets O
while O
running O

of O
our O
V2V-PoseNet O
on O
the O
MSRA B-DAT
dataset. O
Backgrounds O
are O
removed O
to O

80.1 O
40.2 O
86.2 O
74.4 O
79.16 O
Hands B-DAT
51.3 O
70.5 O
30.9 O
68.7 O
55.2 O

P. O
Wohlhart, O
and O
V. O
Lepetit. O
Hands B-DAT
deep O
in O
deep O
learning O
for O

a) O
MSRA B-DAT
(b) O
NYU O
(c) O
ICVL O

joints, O
e.g. O
21 O
joints O
in O
MSRA B-DAT
dataset O
and O
16 O
joints O
in O

Hand O
Pose O
Dataset O
[5] O
and O
MSRA B-DAT
Hand O
Pose O
Dataset O
[23]. O
Fi O

MSRA B-DAT
Hand O
Pose O
Dataset O
[23]. O
The O

MSRA B-DAT
hand O
pose O
dataset O
contains O
76500 O

approach O
with O
state-of-the-art O
methods O
on O
MSRA B-DAT
dataset. O
Left: O
the O
proportion O
of O

pitch O
(right) O
viewpoint O
angles O
on O
MSRA B-DAT
dataset O

On O
MSRA B-DAT
dataset, O
we O
compare O
with O
several O

11.811(−0.5%) O
ICVL O
[22] O
6.932 O
6.793(−2.0%) O
MSRA B-DAT
[23] O
8.728 O
8.649(−0.9 O

rors O
on O
NYU, O
ICVL O
and O
MSRA B-DAT
dataset O
are O
shown O
in O
Ta O

MSRA B-DAT

Oberweger, O
P. O
Wohlhart, O
V. O
Lepetit, O
Hands B-DAT
deep O
in O
deep O
learning O
for O

three O
main O
benchmarks O
(NYU, O
ICVL, O
MSRA) B-DAT
while O
keeping O
the O
simplicity O
of O

benchmark O
datasets, O
i.e. O
NYU O
[40], O
MSRA B-DAT
[28], O
and O
ICVL O
[34]. O
These O

Scaling: O
The O
MSRA B-DAT
[28] O
and O
NYU O
[40] O
datasets O

ICVL O
dataset O
[34], O
and O
the O
MSRA B-DAT
dataset O
[28]. O
For O
the O
comparison O

5.4. O
MSRA B-DAT
Dataset O

The O
MSRA B-DAT
dataset O
[28] O
contains O
about O
76k O

Comparison O
with O
state-of-the-art O
on O
the O
MSRA B-DAT
dataset O
[28]. O
We O
report O
the O

Comparison O
with O
state-of-the-art O
on O
the O
MSRA B-DAT
dataset O
[28]. O
We O
plot O
the O

P. O
Wohlhart, O
and O
V. O
Lepetit. O
Hands B-DAT
Deep O
in O
Deep O
Learning O
for O

Continuous O
Pose O
Recovery O
of O
Human O
Hands B-DAT
Using O
Convolu- O
tional O
Networks. O
ACM O

Hands B-DAT
deep O
in O
deep O
learning O
for O

from O
Facebook B-DAT
and O
Deezer O
data O

Facebook B-DAT

Facebook B-DAT
page O
networks: O
These O
graphs O
represent O

like O
networks O
among O
verified O
Facebook B-DAT
pages O
– O
the O
types O

Using O
Facebook B-DAT
page O
networks O
we O
evaluate O
the O

performance. O
The O
Politicians O
Facebook B-DAT
graph O
is O
embedded O

Athletes B-DAT
13,866 O
0.0009 O
0.1292 O

Politicians O
Companies O
Athletes B-DAT
Media O
Celebrities O
Artists O
Government O
TV O

marks O
is O
highest O
on O
the O
Athletes B-DAT
dataset O
as O
the O
clustering’s O

from O
Facebook B-DAT
and O
Deezer O
data O

Facebook B-DAT

Facebook B-DAT
page O
networks: O
These O
graphs O
represent O

like O
networks O
among O
verified O
Facebook B-DAT
pages O
– O
the O
types O

Using O
Facebook B-DAT
page O
networks O
we O
evaluate O
the O

performance. O
The O
Politicians O
Facebook B-DAT
graph O
is O
embedded O

Artists B-DAT
50,515 O
0.0006 O
0.1140 O

Politicians O
Companies O
Athletes O
Media O
Celebrities O
Artists B-DAT
Government O
TV O
Shows O

Companies B-DAT
14,113 O
0.0005 O
0.1532 O

Politicians O
Companies B-DAT
Athletes O
Media O
Celebrities O
Artists O
Government O

from O
Facebook B-DAT
and O
Deezer O
data O

Facebook B-DAT

Facebook B-DAT
page O
networks: O
These O
graphs O
represent O

like O
networks O
among O
verified O
Facebook B-DAT
pages O
– O
the O
types O

Using O
Facebook B-DAT
page O
networks O
we O
evaluate O
the O

performance. O
The O
Politicians O
Facebook B-DAT
graph O
is O
embedded O

from O
Facebook B-DAT
and O
Deezer O
data O

Facebook B-DAT

Facebook B-DAT
page O
networks: O
These O
graphs O
represent O

like O
networks O
among O
verified O
Facebook B-DAT
pages O
– O
the O
types O

Using O
Facebook B-DAT
page O
networks O
we O
evaluate O
the O

performance. O
The O
Politicians O
Facebook B-DAT
graph O
is O
embedded O

TV B-DAT
Shows O
3,892 O
0.0023 O
0.5906 O

of O
sites O
included O
TV B-DAT
shows, O
politicians, O
athletes, O
and O

Athletes O
Media O
Celebrities O
Artists O
Government O
TV B-DAT
Shows O

datasets O
from O
the O
streaming O
service O
Deezer B-DAT
and O
show O

from O
Facebook O
and O
Deezer B-DAT
data O

Deezer B-DAT
Hungary O
47,538 O
0.0002 O
0.0929 O

Deezer B-DAT
user-user O
friendship O
networks: O
We O
collected O

from O
the O
music O
streaming O
site O
Deezer B-DAT

we O
use O
social O
networks O
of O
Deezer B-DAT
users O
collected O
from O

Deezer B-DAT
Hungary I-DAT
47,538 O
0.0002 O
0.0929 O

Deezer O
Hungary B-DAT
47,538 O
0.0002 O
0.0929 O

included O
3 O
European O
countries O
(Croatia, O
Hungary, B-DAT
and O
Romania). O
For O
each O
user O

Croatia B-DAT
54,573 O
0.0004 O
0.1146 O

and O
included O
3 O
European O
countries O
(Croatia, B-DAT
Hungary, O
and O
Romania). O
For O
each O

datasets O
from O
the O
streaming O
service O
Deezer B-DAT
and O
show O

from O
Facebook O
and O
Deezer B-DAT
data O

Deezer B-DAT
Hungary O
47,538 O
0.0002 O
0.0929 O

Deezer B-DAT
user-user O
friendship O
networks: O
We O
collected O

from O
the O
music O
streaming O
site O
Deezer B-DAT

we O
use O
social O
networks O
of O
Deezer B-DAT
users O
collected O
from O

Politicians B-DAT
5,908 O
0.0024 O
0.3011 O

Politicians B-DAT
Companies O
Athletes O
Media O
Celebrities O
Artists O

performance. O
The O
Politicians B-DAT
Facebook O
graph O
is O
embedded O

from O
Facebook B-DAT
and O
Deezer O
data O

Facebook B-DAT

Facebook B-DAT
page O
networks: O
These O
graphs O
represent O

like O
networks O
among O
verified O
Facebook B-DAT
pages O
– O
the O
types O

Using O
Facebook B-DAT
page O
networks O
we O
evaluate O
the O

performance. O
The O
Politicians O
Facebook B-DAT
graph O
is O
embedded O

Celebrities B-DAT
11,565 O
0.0010 O
0.1666 O

Politicians O
Companies O
Athletes O
Media O
Celebrities B-DAT
Artists O
Government O
TV O
Shows O

from O
Facebook B-DAT
and O
Deezer O
data O

Facebook B-DAT

Facebook B-DAT
page O
networks: O
These O
graphs O
represent O

like O
networks O
among O
verified O
Facebook B-DAT
pages O
– O
the O
types O

Using O
Facebook B-DAT
page O
networks O
we O
evaluate O
the O

performance. O
The O
Politicians O
Facebook B-DAT
graph O
is O
embedded O

Government B-DAT
7,057 O
0.0036 O
0.2238 O

Companies O
Athletes O
Media O
Celebrities O
Artists O
Government B-DAT
TV O
Shows O

from O
Facebook B-DAT
and O
Deezer O
data O

Facebook B-DAT

Facebook B-DAT
page O
networks: O
These O
graphs O
represent O

like O
networks O
among O
verified O
Facebook B-DAT
pages O
– O
the O
types O

Using O
Facebook B-DAT
page O
networks O
we O
evaluate O
the O

performance. O
The O
Politicians O
Facebook B-DAT
graph O
is O
embedded O

datasets O
from O
the O
streaming O
service O
Deezer B-DAT
and O
show O

from O
Facebook O
and O
Deezer B-DAT
data O

Deezer B-DAT
Hungary O
47,538 O
0.0002 O
0.0929 O

Deezer B-DAT
user-user O
friendship O
networks: O
We O
collected O

from O
the O
music O
streaming O
site O
Deezer B-DAT

we O
use O
social O
networks O
of O
Deezer B-DAT
users O
collected O
from O

Romania B-DAT
41,773 O
0.0001 O
0.0752 O

European O
countries O
(Croatia, O
Hungary, O
and O
Romania B-DAT

Media B-DAT
27,917 O
0.0005 O
0.1140 O

Politicians O
Companies O
Athletes O
Media B-DAT
Celebrities O
Artists O
Government O
TV O
Shows O

is O
the O
worst O
on O
the O
Media B-DAT
dataset O
with O
a O
disadvantage O
of O

from O
Facebook B-DAT
and O
Deezer O
data O

Facebook B-DAT

Facebook B-DAT
page O
networks: O
These O
graphs O
represent O

like O
networks O
among O
verified O
Facebook B-DAT
pages O
– O
the O
types O

Using O
Facebook B-DAT
page O
networks O
we O
evaluate O
the O

performance. O
The O
Politicians O
Facebook B-DAT
graph O
is O
embedded O

section. O
The O
total O
amount O
of O
English-Finnish B-DAT
parallel O
sentences O
was O
2 O
719 O

2. O
Detailed O
results O
on O
filtering O
English-Finnish B-DAT

larger O
common O
parallel O
corpora O
from O
WMT B-DAT
shared O
tasks O

parallel O
corpora O
provided O
in O
the O
WMT174 B-DAT
and O
WMT185 O

data O
sets O
provided O
by O
the O
WMT B-DAT
shared O
tasks O

English-Finnish/Latvian O
smaller O
parallel O
corpora O
from O
WMT B-DAT
shared O
tasks O

system O
submission O
[8] O
in O
the O
WMT18 B-DAT
news O
translation O
task O
for O
cleaning O

Tilde’s O
Machine O
Translation O
Systems O
for O
WMT B-DAT
2018, O
in: O
Pro- O
ceedings O
of O

Third O
Conference O
on O
Machine O
Translation O
(WMT B-DAT
2018), O
Volume O
2: O
Shared O
Task O

Machine O
Translation O
Systems O
for O
WMT O
2018, B-DAT
in: O
Pro- O
ceedings O
of O
the O

Conference O
on O
Machine O
Translation O
(WMT O
2018), B-DAT
Volume O
2: O
Shared O
Task O
Papers O

for O
Computational O
Linguistics, O
Brussels, O
Belgium, O
2018 B-DAT

Finnish B-DAT

Finnish B-DAT

Finnish B-DAT
parallel O
sentences O
was O
2 O
719 O

and O
Latvian O
systems, O
but O
not O
Finnish B-DAT

The O
lack O
of O
improvement O
for O
Finnish B-DAT
is O
mainly O
due O
to O
the O

are O
in O
light/dark O
blue O
colours, O
Finnish B-DAT
- O
orange/yellow, O
and O
Latvian O
are O

unfiltered O
systems. O
As O
for O
the O
Finnish B-DAT
systems O
- O
there O
is O
no O

Finnish B-DAT

Finnish B-DAT

English B-DAT
direction O
and O
got O
the O
best O

BERT O
model O
using O
all O
available O
English B-DAT
cor- O
pora O
show O
that O
the O

English B-DAT
(FI→EN) O
trans- O
lation O
task O

English B-DAT
lan- O
guage O
pair, O
i.e., O
it O

English B-DAT
except O
the O
“Wiki O
Headlines” O
due O

and O
for O
monolingual O
target O
side O
English B-DAT
data, O
we O
selected O
all O
besides O

Remove O
pairs O
containing O
influent O
English B-DAT
sentences O
according O
to O
a O
series O

English B-DAT
system, O
back O
translation O
was O
performed O

for O
mono- O
lingual O
English B-DAT
data. O
Before O
back-translation, O
we O
filter O

consis- O
tently. O
Both O
Finnish O
and O
English B-DAT
sentences O
are O
performed O
tokenization O
and O

English B-DAT
news O
translation O
task. O
We O
leveraged O

Sydney’s O
Machine O
Translation O
System O
for O
WMT19 B-DAT

Syd- O
ney’s O
submission O
of O
the O
WMT B-DAT
2019 O
shared O
news O
translation O
task O

USYD O
NMT O
systems O
for O
the O
WMT B-DAT
2019 O
Finnish→English O
(FI→EN) O
trans- O
lation O

place O
in O
this O
direction O
at O
WMT18 B-DAT

of O
Sydney’s O
NMT O
systems O
for O
WMT2019 B-DAT
Finnish→English O
news O
translation O
task. O
We O

for O
the O
shared O
task O
of O
WMT18 B-DAT

confer- O
ence O
on O
machine O
translation O
(WMT18 B-DAT

neural O
machine O
translation O
systems O
for O
WMT18 B-DAT

machine O
translation O
systems O
for O
the O
WMT18 B-DAT
news O
translation O
task. O
In O
Proceedings O

machine O
translation O
sys- O
tems O
for O
WMT B-DAT
16. O
In O
Proceedings O
of O
the O

machine O
trans- O
lation O
system O
for O
WMT18 B-DAT

translation O
performance O
(Bojar O
et O
al., O
2018 B-DAT

al., O
2017; O
Fu O
et O
al., O
2018 B-DAT

; O
Prabhumoye O
et O
al., O
2018) B-DAT
which O
transfers O
text O
to O
specific O

al., O
2008; O
Deng O
et O
al., O
2018 B-DAT

Inspired O
by O
(Bei O
et O
al., O
2018), B-DAT
where O
their O
system O
shows O
data O

BERT O
LM O
(Devlin O
et O
al., O
2018 B-DAT

Transformer O
LM O
(Bei O
et O
al., O
2018 B-DAT

Illegal O
characters O
(Bei O
et O
al., O
2018 B-DAT

al., O
2015; O
Bojar O
et O
al., O
2018), B-DAT
translating O
the O
large O
scale O
mono O

instruction O
of O
(Deng O
et O
al., O
2018 B-DAT

post-processing O
algorithm O
(Wang O
et O
al., O
2018) B-DAT
for O
inconsistent O
number, O
date O
translation O

inspired O
by O
(Marie O
et O
al., O
2018), B-DAT
who O
won O
the O
first O
place O

beam+noise O
method O
(Edunov O
et O
al., O
2018) B-DAT
to O
generate O
robust O
synthetic O
data O

Shiqi O
Li, O
and O
Conghu O
Yuan. O
2018 B-DAT

Philipp O
Koehn, O
and O
Christof O
Monz. O
2018 B-DAT

. O
Findings O
of O
the O
2018 B-DAT
confer- O
ence O
on O
machine O
translation O

Changfeng O
Zhu, O
and O
Boxing O
Chen. O
2018 B-DAT

Kenton O
Lee, O
and O
Kristina O
Toutanova. O
2018 B-DAT

Michael O
Auli, O
and O
David O
Grangier. O
2018 B-DAT

Dongyan O
Zhao, O
and O
Rui O
Yan. O
2018 B-DAT

evaluation. O
In O
Proceedings O
of O
AAAI O
2018 B-DAT

Masao O
Utiyama, O
and O
Eiichiro O
Sumita. O
2018 B-DAT

Salakhutdinov, O
and O
Alan O
W O
Black. O
2018 B-DAT

Tong O
Xiao, O
and O
Jingbo O
Zhu. O
2018 B-DAT

examples O
were O
obtained O
from O
the O
English B-DAT

3. O
An O
example O
of O
an O
English B-DAT
sentence O
aligned O
to O
multiple O
different O

Figure O
4. O
Multiple O
English B-DAT
paraphrased O
sentences O
aligned O
to O
one O

1. O
Detailed O
results O
on O
filtering O
English B-DAT

news O
MT O
shared O
tasks O
for O
English B-DAT
↔ O
Estonian/Finnish/Latvian. O
Detailed O
results O
of O

the O
removed O
parts O
of O
the O
English B-DAT

combined O
and O
shuffled O
all O
three O
English B-DAT

section. O
The O
total O
amount O
of O
English B-DAT

the O
Wiki O
Headlines O
corpus, O
and O
English B-DAT

2. O
Detailed O
results O
on O
filtering O
English B-DAT

higher O
quality O
quicker. O
For O
both O
English B-DAT

Figure O
7. O
Training O
progress O
of O
English B-DAT
↔ O
Estonian/Finnish/Latvian O
NMT O
systems O

integral O
part O
of O
the O
runner-up O
English B-DAT

larger O
common O
parallel O
corpora O
from O
WMT B-DAT
shared O
tasks O

parallel O
corpora O
provided O
in O
the O
WMT174 B-DAT
and O
WMT185 O

data O
sets O
provided O
by O
the O
WMT B-DAT
shared O
tasks O

English-Finnish/Latvian O
smaller O
parallel O
corpora O
from O
WMT B-DAT
shared O
tasks O

system O
submission O
[8] O
in O
the O
WMT18 B-DAT
news O
translation O
task O
for O
cleaning O

Tilde’s O
Machine O
Translation O
Systems O
for O
WMT B-DAT
2018, O
in: O
Pro- O
ceedings O
of O

Third O
Conference O
on O
Machine O
Translation O
(WMT B-DAT
2018), O
Volume O
2: O
Shared O
Task O

Machine O
Translation O
Systems O
for O
WMT O
2018, B-DAT
in: O
Pro- O
ceedings O
of O
the O

Conference O
on O
Machine O
Translation O
(WMT O
2018), B-DAT
Volume O
2: O
Shared O
Task O
Papers O

for O
Computational O
Linguistics, O
Brussels, O
Belgium, O
2018 B-DAT

English B-DAT
direction O
and O
got O
the O
best O

BERT O
model O
using O
all O
available O
English B-DAT
cor- O
pora O
show O
that O
the O

English B-DAT
(FI→EN) O
trans- O
lation O
task O

English B-DAT
lan- O
guage O
pair, O
i.e., O
it O

English B-DAT
except O
the O
“Wiki O
Headlines” O
due O

and O
for O
monolingual O
target O
side O
English B-DAT
data, O
we O
selected O
all O
besides O

Remove O
pairs O
containing O
influent O
English B-DAT
sentences O
according O
to O
a O
series O

English B-DAT
system, O
back O
translation O
was O
performed O

for O
mono- O
lingual O
English B-DAT
data. O
Before O
back-translation, O
we O
filter O

consis- O
tently. O
Both O
Finnish O
and O
English B-DAT
sentences O
are O
performed O
tokenization O
and O

English B-DAT
news O
translation O
task. O
We O
leveraged O

of O
Sydney’s O
NMT O
systems O
for O
WMT2019 B-DAT
Finnish I-DAT

English B-DAT
direction O
and O
got O
the O
best O

BERT O
model O
using O
all O
available O
English B-DAT
cor- O
pora O
show O
that O
the O

English B-DAT
(FI→EN) O
trans- O
lation O
task O

English B-DAT
lan- O
guage O
pair, O
i.e., O
it O

English B-DAT
except O
the O
“Wiki O
Headlines” O
due O

and O
for O
monolingual O
target O
side O
English B-DAT
data, O
we O
selected O
all O
besides O

Remove O
pairs O
containing O
influent O
English B-DAT
sentences O
according O
to O
a O
series O

English B-DAT
system, O
back O
translation O
was O
performed O

for O
mono- O
lingual O
English B-DAT
data. O
Before O
back-translation, O
we O
filter O

consis- O
tently. O
Both O
Finnish O
and O
English B-DAT
sentences O
are O
performed O
tokenization O
and O

English B-DAT
news O
translation O
task. O
We O
leveraged O

of O
Sydney’s O
NMT O
systems O
for O
WMT2019 B-DAT
Finnish→English O
news O
translation O
task. O
We O

English B-DAT
direction O
and O
got O
the O
best O

BERT O
model O
using O
all O
available O
English B-DAT
cor- O
pora O
show O
that O
the O

English B-DAT
(FI→EN) O
trans- O
lation O
task O

English B-DAT
lan- O
guage O
pair, O
i.e., O
it O

English B-DAT
except O
the O
“Wiki O
Headlines” O
due O

and O
for O
monolingual O
target O
side O
English B-DAT
data, O
we O
selected O
all O
besides O

Remove O
pairs O
containing O
influent O
English B-DAT
sentences O
according O
to O
a O
series O

English B-DAT
system, O
back O
translation O
was O
performed O

for O
mono- O
lingual O
English B-DAT
data. O
Before O
back-translation, O
we O
filter O

consis- O
tently. O
Both O
Finnish O
and O
English B-DAT
sentences O
are O
performed O
tokenization O
and O

English B-DAT
news O
translation O
task. O
We O
leveraged O

examples O
were O
obtained O
from O
the O
English B-DAT

3. O
An O
example O
of O
an O
English B-DAT
sentence O
aligned O
to O
multiple O
different O

Figure O
4. O
Multiple O
English B-DAT
paraphrased O
sentences O
aligned O
to O
one O

1. O
Detailed O
results O
on O
filtering O
English B-DAT

news O
MT O
shared O
tasks O
for O
English B-DAT
↔ O
Estonian/Finnish/Latvian. O
Detailed O
results O
of O

the O
removed O
parts O
of O
the O
English B-DAT

combined O
and O
shuffled O
all O
three O
English B-DAT

section. O
The O
total O
amount O
of O
English B-DAT

the O
Wiki O
Headlines O
corpus, O
and O
English B-DAT

2. O
Detailed O
results O
on O
filtering O
English B-DAT

higher O
quality O
quicker. O
For O
both O
English B-DAT

Figure O
7. O
Training O
progress O
of O
English B-DAT
↔ O
Estonian/Finnish/Latvian O
NMT O
systems O

integral O
part O
of O
the O
runner-up O
English B-DAT

larger O
common O
parallel O
corpora O
from O
WMT B-DAT
shared O
tasks O

parallel O
corpora O
provided O
in O
the O
WMT174 B-DAT
and O
WMT185 O

data O
sets O
provided O
by O
the O
WMT B-DAT
shared O
tasks O

English-Finnish/Latvian O
smaller O
parallel O
corpora O
from O
WMT B-DAT
shared O
tasks O

system O
submission O
[8] O
in O
the O
WMT18 B-DAT
news O
translation O
task O
for O
cleaning O

Tilde’s O
Machine O
Translation O
Systems O
for O
WMT B-DAT
2018, O
in: O
Pro- O
ceedings O
of O

Third O
Conference O
on O
Machine O
Translation O
(WMT B-DAT
2018), O
Volume O
2: O
Shared O
Task O

Web-Crawled O
Par- O
allel O
Corpora, O
Emnlp O
(2017), B-DAT
2935–2940. O
http://www.aclweb.org/anthology/D17-1319% O
0Ahttp://aclweb.org/anthology/D17-1318 O

Neural O
Machine O
Translation, O
ArXiv O
e-prints O
(2017 B-DAT

The O
paper O
describes O
Tilde’s O
English B-DAT

English B-DAT
machine O
translation O
systems O
for O
the O

best O
scoring O
systems O
for O
the O
English B-DAT

English B-DAT
translation O
directions O

Constrained O
English-Latvian B-DAT
and O
Latvian- O
English O
NMT-SMT O
hybrid O
systems O

Unconstrained O
English-Latvian B-DAT
and O
Latvian- O
English O
NMT-SMT O
hybrid O
systems O
trained O
on O

An O
unconstrained O
English B-DAT

Pinnis O
and O
Goba O
(2011). O
For O
English, B-DAT
we O
used O
the O
lexicalized O
probabilistic O

Factor O
English B-DAT
Latvian O
Word O
part O
350 O
360 O

of O
the O
translations O
of O
the O
English B-DAT

English B-DAT
unconstrained O
system, O
we O
use O
a O

data O
set O
show O
that O
for O
English B-DAT

NMT O
system O
and O
for O
Latvian- O
English B-DAT
both O
the O
constrained O
and O
unconstrained O

the O
quality O
of O
the O
unconstrained O
English B-DAT

English B-DAT
unconstrained O
sce- O
nario O
for O
which O

the O
paper, O
we O
have O
described O
English B-DAT

English B-DAT
MT O
systems O
that O
were O
devel O

unconstrained O
systems) O
and O
one O
unconstrained O
English B-DAT

results O
of O
the O
top O
three O
English B-DAT

English B-DAT
constrained O
systems O
submitted O
for O
the O

Hybrid O
Neural O
Machine O
Translation O
for O
English B-DAT

Tilde's O
Machine O
Translation O
Systems O
for O
WMT B-DAT
2017 O

the O
Conference O
on O
Machine O
Translation O
(WMT), B-DAT
Volume O
2: O
Shared O
Task O
Papers O

Tilde’s O
Machine O
Translation O
Systems O
for O
WMT B-DAT
2017 O

machine O
translation O
systems O
for O
the O
WMT B-DAT
2017 O
shared O
task O
in O
news O

translation O
shared O
task O
of O
the O
WMT B-DAT
conference. O
This O
was O
achieved O
due O

For O
the O
WMT B-DAT
2017 O
shared O
task O
in O
news O

best O
re- O
sults O
in O
the O
WMT B-DAT
2016 O
shared O
task. O
We O
also O

provides O
an O
overview O
of O
our O
WMT B-DAT
2017 O
systems, O
Section O
3 O
describes O

For O
the O
WMT B-DAT
2017 O
shared O
task, O
we O
developed O

MT O
systems, O
we O
used O
the O
WMT B-DAT
2017 O
training O
data, O
however, O
for O

that O
was O
provided O
by O
the O
WMT B-DAT
2017 O
organisers O

multiple O
language O
pairs O
in O
the O
WMT B-DAT
2016 O
shared O
task O
in O
news O

oped O
by O
Tilde O
for O
the O
WMT B-DAT
2017 O
shared O
task O
in O
news O

constrained O
systems O
submitted O
for O
the O
WMT B-DAT
2017 O
shared O
task O
on O
news O

First O
Conference O
on O
Machine O
Translation O
(WMT B-DAT
2016) O
- O
Volume O
1: O
Research O

Machine O
Translation O
Sys- O
tems O
for O
WMT B-DAT
16. O
In O
Proceedings O
of O
the O

Con- O
ference O
on O
Machine O
Translation O
(WMT B-DAT
2016), O
Vol- O
ume O
2: O
Shared O

First O
Conference O
on O
Machine O
Translation O
(WMT B-DAT
2016), O
Volume O
2: O
Shared O
Task O

describes O
Tilde’s O
English- O
Latvian O
and O
Latvian-English B-DAT
machine O
translation O
systems O
for O
the O

systems O
for O
the O
English-Latvian O
and O
Latvian-English B-DAT
translation O
directions O

5. O
Then, O
for O
the O
Latvian-English B-DAT
unconstrained O
system, O
we O
use O
a O

all O
scenar- O
ios O
(except O
the O
Latvian-English B-DAT
unconstrained O
sce- O
nario O
for O
which O

we O
have O
described O
English-Latvian O
and O
Latvian-English B-DAT
MT O
systems O
that O
were O
devel O

the O
top O
three O
English-Latvian O
and O
Latvian-English B-DAT
constrained O
systems O
submitted O
for O
the O

Machine O
Translation O
Systems O
for O
WMT O
2017 B-DAT

374–381 O
Copenhagen, O
Denmark, O
September O
711, O
2017 B-DAT

2017 B-DAT
Association O
for O
Computational O
Linguistics O

Machine O
Translation O
Systems O
for O
WMT O
2017 B-DAT

translation O
systems O
for O
the O
WMT O
2017 B-DAT
shared O
task O
in O
news O
translation O

tem O
training O
(Britz O
et O
al., O
2017 B-DAT

For O
the O
WMT O
2017 B-DAT
shared O
task O
in O
news O
trans O

an O
overview O
of O
our O
WMT O
2017 B-DAT
systems, O
Section O
3 O
describes O
the O

For O
the O
WMT O
2017 B-DAT
shared O
task, O
we O
developed O
both O

systems, O
we O
used O
the O
WMT O
2017 B-DAT
training O
data, O
however, O
for O
the O

was O
provided O
by O
the O
WMT O
2017 B-DAT
organisers O

Nematus O
toolkit O
(Sennrich O
et O
al., O
2017) B-DAT
which O
improves O
on O
the O
original O

by O
Tilde O
for O
the O
WMT O
2017 B-DAT
shared O
task O
in O
news O
translation O

systems O
submitted O
for O
the O
WMT O
2017 B-DAT
shared O
task O
on O
news O
translation O

Thang O
Luong, O
and O
Quoc O
Le. O
2017 B-DAT

and O
Com- O
putational O
Linguistics O
(CICLING O
2017 B-DAT

Barone, O
Jozef O
Mokry, O
et O
al. O
2017 B-DAT

for O
hybrid O
translation, O
tested O
on O
English B-DAT

↔German O
and O
English B-DAT

al., O
2002) O
for O
translating O
between O
English B-DAT
and O
Czech, O
German, O
Romanian O
and O

English B-DAT
and O
report O
BLEU O
score O
improvement O

best O
single O
system, O
translating O
from O
English B-DAT
into O
Romanian O

and O
measurements O
were O
done O
for O
English B-DAT

English, B-DAT
using O
corpora O
from O
the O
news O

6 O
million O
sentences O
from O
the O
English B-DAT
News O
Crawl O
2016. O
Much O
more O

in O
BLEU O
for O
translating O
between O
English B-DAT

both O
transla- O
tion O
directions O
from O
English B-DAT
human O
evaluators O
prefer O
the O
lower-scoring O

in O
their O
submission O
for O
the O
WMT16 B-DAT
news O
translation O
shared O
task. O
They O

news O
translation O
shared O
task O
of O
WMT B-DAT

directions O
was O
used O
from O
the O
WMT17 B-DAT
news O
translation O
task O
3. O
For O

First O
Conference O
on O
Machine O
Translation O
(WMT B-DAT
2016), O
Volume O
2: O
Shared O
Task O

were O
done O
for O
English-Latvian O
and O
Latvian-English, B-DAT
using O
corpora O
from O
the O
news O

soft O
alignments O
(Rikters O
et O
al., O
2017 B-DAT

Pinnis O
et O
al. O
(2017) B-DAT
experimented O
with O
using O
large O
and O

System O
Combination O
Zhou O
et O
al. O
(2017) B-DAT
used O
attention O
to O
combine O
outputs O

beam O
search O
(Sennrich O
et O
al., O
2017 B-DAT

with O
Nematus O
(Sennrich O
et O
al., O
2017) B-DAT
and O
the O
other O
– O
with O

Neural O
Monkey O
(Helcl O
and O
Libovickỳ, O
2017 B-DAT

2017 B-DAT

Nematus O
(NT) O
(Sennrich O
et O
al., O
2017) B-DAT
and O
Neural O
Monkey O
(NM) O
(Helcl O

and O
Libovickỳ, O
2017 B-DAT

3EMNLP O
2017 B-DAT
Second O
Conference O
on O
Machine O
Translation O

Helcl, O
J. O
and O
Libovickỳ, O
J. O
(2017 B-DAT

Deksne, O
D., O
and O
Miks, O
T. O
(2017 B-DAT

Fishel, O
M., O
and O
Bojar, O
O. O
(2017 B-DAT

Mokry, O
J., O
and O
Nadejde, O
M. O
(2017 B-DAT

Zhang, O
J., O
and O
Zong, O
C. O
(2017 B-DAT

more O
tolerance O
(e.g., O
the O
three-word O
English B-DAT
sentence O
“Thanks O
Barack O
Obama." O
can O

translation O
by O
the O
translator O
(assuming O
English B-DAT
was O
the O
original) O
and O
does O

sequence O
learning. O
arXiv O
preprint O
arXiv:1705.03122 O
(2017 B-DAT

of O
Mathematical O
Linguistics O
pp. O
5–17 O
(2017 B-DAT

2017 B-DAT

Machine O
Translation. O
ArXiv O
e-prints O
(Dec O
2017), B-DAT
https://arxiv.org/abs/1712.05690 O

Neural O
Machine O
Translation. O
ArXiv O
e-prints O
(2017 B-DAT

transla- O
tion O
systems O
for O
wmt O
2017 B-DAT

tional O
Linguistics, O
Copenhagen, O
Denmark O
(September O
2017), B-DAT
http://www.aclweb. O
org/anthology/W17-4737 O

of O
Mathematical O
Linguistics O
109(1), O
39–50 O
(2017 B-DAT

The O
16th O
Machine O
Translation O
Summit O
(2017 B-DAT

for O
neural O
machine O
translation. O
EACL O
2017 B-DAT
p. O
65 O
(2017 O

all O
you O
need. O
CoRR O
abs/1706.03762 O
(2017), B-DAT
http: O
//arxiv.org/abs/1706.03762 O

Latvian B-DAT
larger O
common O
parallel O
corpora O
from O

Latvian B-DAT

Latvian B-DAT
- O
1 O
617 O
793 O
(35.85 O

NMT O
quality O
for O
Estonian O
and O
Latvian B-DAT
systems, O
but O
not O
Finnish. O
The O

biggest O
corpora O
for O
Estonian O
and O
Latvian B-DAT
- O
ParaCrawl O
(about O
35 O
of O

colours, O
Finnish O
- O
orange/yellow, O
and O
Latvian B-DAT
are O
in O
light/dark O
red O
colours O

that O
the O
filtered O
Estonian O
and O
Latvian B-DAT
systems O
are O
much O
quicker O
to O

Latvian B-DAT
smaller O
parallel O
corpora O
from O
WMT O

Latvian B-DAT
NMT O
systems O

rich O
languages O
like O
Estonian O
and O
Latvian B-DAT

larger O
common O
parallel O
corpora O
from O
WMT B-DAT
shared O
tasks O

parallel O
corpora O
provided O
in O
the O
WMT174 B-DAT
and O
WMT185 O

data O
sets O
provided O
by O
the O
WMT B-DAT
shared O
tasks O

English-Finnish/Latvian O
smaller O
parallel O
corpora O
from O
WMT B-DAT
shared O
tasks O

system O
submission O
[8] O
in O
the O
WMT18 B-DAT
news O
translation O
task O
for O
cleaning O

Tilde’s O
Machine O
Translation O
Systems O
for O
WMT B-DAT
2018, O
in: O
Pro- O
ceedings O
of O

Third O
Conference O
on O
Machine O
Translation O
(WMT B-DAT
2018), O
Volume O
2: O
Shared O
Task O

the O
Wiki O
Headlines O
corpus, O
and O
English-Latvian B-DAT
- O
1 O
617 O
793 O
(35.85 O

Web-Crawled O
Par- O
allel O
Corpora, O
Emnlp O
(2017), B-DAT
2935–2940. O
http://www.aclweb.org/anthology/D17-1319% O
0Ahttp://aclweb.org/anthology/D17-1318 O

Neural O
Machine O
Translation, O
ArXiv O
e-prints O
(2017 B-DAT

by O
Guo O
and O
Barbosa O
(2018), O
WNED B-DAT

-CWEB O
(CWEB), O
and O
WNED B-DAT

namely O
MSNBC, O
AQUAINT, O
ACE2004, O
and O
WNED B-DAT

and O
performed O
competitive O
on O
the O
WNED B-DAT

Barbosa O
(2018), O
WNED-CWEB O
(CWEB), O
and O
WNED-WIKI B-DAT
(WW), O
which O
were O
obtained O
from O

namely O
MSNBC, O
AQUAINT, O
ACE2004, O
and O
WNED-WIKI, B-DAT
and O
performed O
competitive O
on O
the O

WIKI B-DAT
(WW), O
which O
were O
obtained O
from O

WIKI, B-DAT
and O
performed O
competitive O
on O
the O

WNED-CWEB B-DAT
11154 O
320 O
34.8 O
WNED O

WNED-WIKI B-DAT
(WW) O
and O
WNED O

WNED-CWEB O
11154 O
320 O
34.8 O
WNED-WIKI B-DAT
6821 O
320 O
21.3 O

WNED-WIKI B-DAT
(WW) O
and O
WNED-CWEB O
(CWEB): O
are O

WIKI B-DAT
6821 O
320 O
21.3 O

WIKI B-DAT
(WW) O
and O
WNED-CWEB O
(CWEB): O
are O

datasets O
(i.e., O
CoNLL O
2003 O
and O
TAC B-DAT
2010) O
for O
the O
EL O
task O

the O
CoNLL O
dataset O
and O
the O
TAC B-DAT
2010 O
dataset. O
The O
CoNLL O
dataset O

The O
TAC B-DAT
2010 O
dataset O
is O
another O
dataset O

for O
the O
Text O
Analysis O
Conference O
(TAC)8 B-DAT

the O
CoNLL O
dataset. O
For O
the O
TAC B-DAT
2010 O
dataset, O
we O
use O
a O

sets O
of O
the O
CoNLL O
and O
TAC B-DAT
2010 O
datasets, O
respec- O
tively O

on O
both O
the O
CoNLL O
and O
TAC B-DAT
2010 O
datasets. O
Further, O
we O
did O

0.9 O
CoNLL O
(SG-proj-dbp) O
5,000 O
0.6 O
TAC10 B-DAT
(NTEE) O
5,000 O
0.0 O
TAC10 O
(NTEE O

w/o O
strsim) O
5,000 O
0.0 O
TAC10 B-DAT
(Fixed O
NTEE) O
5,000 O
0.0 O
TAC10 O

SG-proj) O
2,000 O
0.4 O
TAC10 B-DAT
(SG-proj-dbp) O
5,000 O
0.0 O

both O
the O
CoNLL O
and O
the O
TAC B-DAT
2010 O
datasets. O
In O
particular, O
the O

TAC10 B-DAT

Ellis. O
2010. O
Overview O
of O
the O
TAC B-DAT
2010 O
Knowledge O
Base O
Population O
Track O

dataset O
and O
85.2% O
on O
the O
TAC B-DAT
2010 O
dataset O

the O
CoNLL O
dataset O
and O
the O
TAC B-DAT
2010 O
dataset. O
Experimental O
results O
revealed O

the O
CoNLL O
dataset O
and O
the O
TAC B-DAT
2010 O
dataset. O
The O
de- O
tails O

TAC B-DAT
2010 O
The O
TAC O
2010 O
dataset O
is O
another O
pop O

for O
the O
Text O
Analysis O
Conference O
(TAC)5 B-DAT
(Ji O
et O
al., O
2010). O
The O

92.6 O
CoNLL O
(YAGO) O
91.5 O
90.9 O
TAC B-DAT
2010 O
85.2 O

TAC10 B-DAT

both O
the O
CoNLL O
and O
the O
TAC B-DAT
2010 O
datasets. O
Moreover, O
we O
found O

90.3 O
90.8 O
Two-step O
91.5 O
90.9 O
TAC B-DAT
2010: O
Base O
80.1 O
- O
+String O

to O
perfor- O
mance O
on O
the O
TAC B-DAT
2010 O
dataset. O
This O
was O
because O

mentions O
per O
document, O
but O
the O
TAC B-DAT
2010 O
only O
contains O
ap- O
proximately O

Ellis. O
2010. O
Overview O
of O
the O
TAC B-DAT
2010 O
Knowledge O
Base O
Population O
Track O

Proceeding O
of O
Text O
Analytics O
Conference O
(TAC B-DAT

Dang. O
2009. O
Overview O
of O
the O
TAC B-DAT
2009 O
Knowledge O
Base O
Population O
Track O

Proceeding O
of O
Text O
Analysis O
Conference O
(TAC B-DAT

WNED-CWEB B-DAT
11154 O
320 O
34.8 O
WNED O

WNED-WIKI B-DAT
(WW) O
and O
WNED O

CWEB B-DAT
11154 O
320 O
34.8 O
WNED-WIKI O
6821 O

WNED-WIKI O
(WW) O
and O
WNED-CWEB B-DAT
(CWEB O

Global O
methods O
MSB O
AQ O
ACE O
CWEB B-DAT
WW O
prior O
p̂(e|m) O
89.3 O
83.2 O

WNED-CWEB B-DAT
11154 O
320 O
34.8 O
WNED-WIKI O
6821 O

WNED-WIKI O
(WW) O
and O
WNED-CWEB B-DAT
(CWEB): O
are O
larger, O
but O
automatically O

by O
Guo O
and O
Barbosa O
(2018), O
WNED B-DAT

-CWEB O
(CWEB), O
and O
WNED B-DAT

namely O
MSNBC, O
AQUAINT, O
ACE2004, O
and O
WNED B-DAT

and O
performed O
competitive O
on O
the O
WNED B-DAT

Guo O
and O
Barbosa O
(2018), O
WNED-CWEB B-DAT
(CWEB), O
and O
WNED-WIKI O
(WW), O
which O
were O

Methods O
MSB O
AQ O
ACE O
CWEB B-DAT
WW O
Milne O
and O
Witten O
(2008 O

by O
Guo O
and O
Barbosa O
(2018), O
WNED-CWEB B-DAT
(CWEB), O
and O
WNED-WIKI O
(WW), O
which O

sets: O
MSNBC O
(MSB), O
AQUAINT O
(AQ), O
ACE2004 B-DAT
(ACE), O
which O
were O
cleaned O
by O

Methods O
MSB O
AQ O
ACE B-DAT
CWEB O
WW O
Milne O
and O
Witten O

five O
datasets, O
namely O
MSNBC, O
AQUAINT, O
ACE2004, B-DAT
and O
WNED-WIKI, O
and O
performed O
competitive O

mance O
on O
the O
AQUAINT O
and O
ACE2004 B-DAT
datasets O

sets: O
MSNBC O
(MSB), O
AQUAINT O
(AQ), O
ACE2004 B-DAT
(ACE), O
which O
were O
cleaned O
by O

five O
datasets, O
namely O
MSNBC, O
AQUAINT, O
ACE2004, B-DAT
and O
WNED-WIKI, O
and O
performed O
competitive O

mance O
on O
the O
AQUAINT O
and O
ACE2004 B-DAT
datasets O

32.8 O
AQUAINT O
727 O
50 O
14.5 O
ACE2004 B-DAT
257 O
36 O
7.1 O

MSNBC O
(MSB), O
AQUAINT O
(AQ) O
and O
ACE2004 B-DAT
(ACE) O
datasets O
cleaned O
and O
up O

Global O
methods O
MSB O
AQ O
ACE B-DAT
CWEB O
WW O
prior O
p̂(e|m) O
89.3 O

32.8 O
AQUAINT O
727 O
50 O
14.5 O
ACE2004 B-DAT
257 O
36 O
7.1 O

MSNBC O
(MSB), O
AQUAINT O
(AQ) O
and O
ACE2004 B-DAT
(ACE) O
datasets O
cleaned O
and O
up O

using O
two O
standard O
datasets O
(i.e., O
CoNLL B-DAT
2003 O
and O
TAC O
2010) O
for O

use O
two O
standard O
datasets: O
the O
CoNLL B-DAT
dataset O
and O
the O
TAC O
2010 O

dataset. O
The O
CoNLL B-DAT
dataset, O
which O
was O
pro- O
posed O

et O
al. O
(2015) O
for O
the O
CoNLL B-DAT
dataset. O
For O
the O
TAC O
2010 O

the O
test O
sets O
of O
the O
CoNLL B-DAT
and O
TAC O
2010 O
datasets, O
respec O

this O
approach O
on O
both O
the O
CoNLL B-DAT
and O
TAC O
2010 O
datasets. O
Further O

CoNLL B-DAT
(NTEE) O
2,000 O
0.3 O
CoNLL O
(NTEE O
w/o O
strsim) O
3,000 O
0.8 O

CoNLL B-DAT
(Fixed O
NTEE) O
5,000 O
0.0 O
CoNLL O
(SG-proj) O
2,000 O
0.9 O
CoNLL O
(SG-proj-dbp O

strong O
results O
on O
both O
the O
CoNLL B-DAT
and O
the O
TAC O
2010 O
datasets O

the O
test O
set O
of O
the O
CoNLL B-DAT
dataset O
by O
randomly O
inspecting O
200 O

CoNLL B-DAT

CoNLL B-DAT

of O
93.1% O
on O
the O
standard O
CoNLL B-DAT
dataset O
and O
85.2% O
on O
the O

stan- O
dard O
NED O
datasets: O
the O
CoNLL B-DAT
dataset O
and O
the O
TAC O
2010 O

two O
standard O
NED O
datasets: O
the O
CoNLL B-DAT
dataset O
and O
the O
TAC O
2010 O

a O
parameter O
search O
on O
the O
CoNLL B-DAT
development O
set. O
We O
used O
η O

CoNLL B-DAT
The O
CoNLL O
dataset O
is O
a O
popular O
NED O

on O
NER O
data O
from O
the O
CoNLL B-DAT
2003 O
shared O
task, O
and O
consists O

CoNLL B-DAT
(PPRforNED) O
93.1 O
92.6 O
CoNLL O
(YAGO) O
91.5 O
90.9 O
TAC O
2010 O

CoNLL B-DAT

CoNLL B-DAT

the O
best-known O
accuracy O
on O
the O
CoNLL B-DAT
dataset O

enhanced O
performance O
on O
both O
the O
CoNLL B-DAT
and O
the O
TAC O
2010 O
datasets O

ably O
affected O
performance O
on O
the O
CoNLL B-DAT
dataset O

CoNLL B-DAT
(PPRforNED): O
Base O
85.4 O
87.4 O
+String O

91.4 O
92.1 O
Two-step O
93.1 O
92.6 O
CoNLL B-DAT
(YAGO): O
Base O
81.1 O
83.6 O
+String O

nificantly O
improved O
performance O
on O
the O
CoNLL B-DAT
dataset. O
However, O
it O
did O
not O

mentions O
between O
the O
datasets. O
The O
CoNLL B-DAT
dataset O
contains O
approximately O
20 O
entity O

an O
error O
analysis O
on O
the O
CoNLL B-DAT
test O
set O
with O
candidate O
generation O

CoNLL), B-DAT
pages O
708–716 O

models O
on O
five O
test O
sets: O
MSNBC B-DAT
(MSB), O
AQUAINT O
(AQ), O
ACE2004 O
(ACE O

of O
the O
five O
datasets, O
namely O
MSNBC, B-DAT
AQUAINT, O
ACE2004, O
and O
WNED-WIKI, O
and O

MSNBC B-DAT
656 O
20 O
32.8 O
AQUAINT O
727 O

MSNBC B-DAT
(MSB), O
AQUAINT O
(AQ) O
and O
ACE2004 O

using O
two O
standard O
datasets O
(i.e., O
CoNLL B-DAT
2003 O
and O
TAC O
2010) O
for O

use O
two O
standard O
datasets: O
the O
CoNLL B-DAT
dataset O
and O
the O
TAC O
2010 O

dataset. O
The O
CoNLL B-DAT
dataset, O
which O
was O
pro- O
posed O

et O
al. O
(2015) O
for O
the O
CoNLL B-DAT
dataset. O
For O
the O
TAC O
2010 O

the O
test O
sets O
of O
the O
CoNLL B-DAT
and O
TAC O
2010 O
datasets, O
respec O

this O
approach O
on O
both O
the O
CoNLL B-DAT
and O
TAC O
2010 O
datasets. O
Further O

CoNLL B-DAT
(NTEE) O
2,000 O
0.3 O
CoNLL O
(NTEE O
w/o O
strsim) O
3,000 O
0.8 O

CoNLL B-DAT
(Fixed O
NTEE) O
5,000 O
0.0 O
CoNLL O
(SG-proj) O
2,000 O
0.9 O
CoNLL O
(SG-proj-dbp O

strong O
results O
on O
both O
the O
CoNLL B-DAT
and O
the O
TAC O
2010 O
datasets O

the O
test O
set O
of O
the O
CoNLL B-DAT
dataset O
by O
randomly O
inspecting O
200 O

CoNLL B-DAT

CoNLL B-DAT

CoNLL B-DAT
dataset O
(Hoffart O
et O
al., O
2011 O

training O
set O
of O
the O
AIDA- O
CoNLL B-DAT
dataset. O
During O
the O
training, O
we O

CoNLL B-DAT
dataset O

CoNLL B-DAT
dataset O

test O
b O
sets O
of O
the O
AIDA-CoNLL B-DAT
dataset O
(Hoffart O
et O
al., O
2011 O

the O
development O
set O
of O
the O
AIDA-CoNLL B-DAT
dataset O

Table O
1: O
Accuracies O
on O
the O
AIDA-CoNLL B-DAT
dataset O

test O
b O
sets O
of O
the O
AIDA B-DAT

the O
training O
set O
of O
the O
AIDA B-DAT

the O
development O
set O
of O
the O
AIDA B-DAT

Table O
1: O
Accuracies O
on O
the O
AIDA B-DAT

CoNLL B-DAT
dataset O
(Hoffart O
et O
al., O
2011 O

knowledge O
and O
text O
jointly O
embedding. O
CoNLL B-DAT
2016, O
page O
260 O

for O
named O
entity O
dis- O
ambiguation. O
CoNLL B-DAT
2016, O
page O
250 O

AIDA-CoNLL B-DAT
dataset O
(Hoffart O
et O
al., O
2011 O

Achieves O
a O
1.7% O
improvement O
on O
AIDA B-DAT

AIDA-train B-DAT
18448 O
946 O
19.5 O
AIDA O

-A O
(valid) O
4791 O
216 O
22.1 O
AIDA B-DAT

Methods O
AIDA B-DAT

Table O
3: O
In-KB O
accuracy O
for O
AIDA B-DAT

AIDA B-DAT

ED O
datasets. O
It O
contains O
training O
(AIDA B-DAT

-train), O
validation O
(AIDA-A) B-DAT
and O
test O
(AIDA O

ED O
models O
are O
trained O
on O
AIDA B-DAT

mul- O
tiple O
epochs), O
validated O
on O
AIDA B-DAT

-A O
and O
tested O
on O
AIDA B-DAT

hours O
for O
1250 O
epochs O
over O
AIDA B-DAT

Table O
6: O
ED O
accuracy O
on O
AIDA B-DAT

The O
best O
results O
for O
the O
AIDA B-DAT
datasets O
are O
reported O
by O
(Yamada O

of O
the O
art O
accuracy O
on O
AIDA B-DAT
which O
is O
the O
largest O
and O

the O
accu- O
racy O
on O
the O
AIDA B-DAT

by O
our O
model O
on O
the O
AIDA B-DAT

of O
93.1% O
on O
the O
standard O
CoNLL B-DAT
dataset O
and O
85.2% O
on O
the O

stan- O
dard O
NED O
datasets: O
the O
CoNLL B-DAT
dataset O
and O
the O
TAC O
2010 O

two O
standard O
NED O
datasets: O
the O
CoNLL B-DAT
dataset O
and O
the O
TAC O
2010 O

a O
parameter O
search O
on O
the O
CoNLL B-DAT
development O
set. O
We O
used O
η O

CoNLL B-DAT
The O
CoNLL O
dataset O
is O
a O
popular O
NED O

on O
NER O
data O
from O
the O
CoNLL B-DAT
2003 O
shared O
task, O
and O
consists O

CoNLL B-DAT
(PPRforNED) O
93.1 O
92.6 O
CoNLL O
(YAGO) O
91.5 O
90.9 O
TAC O
2010 O

CoNLL B-DAT

CoNLL B-DAT

the O
best-known O
accuracy O
on O
the O
CoNLL B-DAT
dataset O

enhanced O
performance O
on O
both O
the O
CoNLL B-DAT
and O
the O
TAC O
2010 O
datasets O

ably O
affected O
performance O
on O
the O
CoNLL B-DAT
dataset O

CoNLL B-DAT
(PPRforNED): O
Base O
85.4 O
87.4 O
+String O

91.4 O
92.1 O
Two-step O
93.1 O
92.6 O
CoNLL B-DAT
(YAGO): O
Base O
81.1 O
83.6 O
+String O

nificantly O
improved O
performance O
on O
the O
CoNLL B-DAT
dataset. O
However, O
it O
did O
not O

mentions O
between O
the O
datasets. O
The O
CoNLL B-DAT
dataset O
contains O
approximately O
20 O
entity O

an O
error O
analysis O
on O
the O
CoNLL B-DAT
test O
set O
with O
candidate O
generation O

CoNLL), B-DAT
pages O
708–716 O

five O
test O
sets: O
MSNBC O
(MSB), O
AQUAINT B-DAT
(AQ), O
ACE2004 O
(ACE), O
which O
were O

the O
five O
datasets, O
namely O
MSNBC, O
AQUAINT, B-DAT
ACE2004, O
and O
WNED-WIKI, O
and O
performed O

the O
perfor- O
mance O
on O
the O
AQUAINT B-DAT
and O
ACE2004 O
datasets O

MSNBC O
656 O
20 O
32.8 O
AQUAINT B-DAT
727 O
50 O
14.5 O
ACE2004 O
257 O

MSNBC O
(MSB), O
AQUAINT B-DAT
(AQ) O
and O
ACE2004 O
(ACE) O
datasets O

experiments O
on O
three O
public O
datasets: O
NYT B-DAT

- O
single, O
NYT B-DAT

on O
three O
benchmark O
datasets: O
(1) O
NYT B-DAT

triplet O
in O
each O
sentence. O
(2) O
NYT B-DAT

they O
selected O
5000 O
sentences O
from O
NYT B-DAT

entities O
are O
all O
correct. O
For O
NYT B-DAT

Dataset O
NYT-single B-DAT
NYT O

the O
recent O
state-of-the-art O
result O
on O
NYT B-DAT

the O
latest O
state-of-the-art O
method O
on O
NYT B-DAT

Model O
NYT-single B-DAT
NYT O

17.2% O
in O
F1 O
on O
the O
NYT B-DAT

in O
some O
relations. O
For O
the O
NYT B-DAT

Model O
NYT-single B-DAT
NYT O

NYT B-DAT

4: O
An O
ablation O
study O
on O
NYT B-DAT

4 O
summarizes O
the O
results O
on O
NYT B-DAT

score O
w.r.t O
different O
λ O
on O
NYT B-DAT

divide O
the O
test O
set O
of O
NYT B-DAT

on O
three O
benchmark O
datasets: O
(1) O
NYT-single B-DAT
is O
sampled O
from O
the O
New O

they O
selected O
5000 O
sentences O
from O
NYT-single B-DAT
as O
the O
test O
set, O
5000 O

Dataset O
NYT-single B-DAT
NYT-multi O
WebNLG O

the O
recent O
state-of-the-art O
result O
on O
NYT-single B-DAT
and O
Wiki-KBP. O
(5) O
GraphRel O
(Fu O

Model O
NYT-single B-DAT
NYT-multi O
Wiki-KBPPrec. O
Rec. O
F1 O
Prec O

in O
some O
relations. O
For O
the O
NYT-single B-DAT
dataset, O
ETL-Span O
outperforms O
PA-LSTM O
by O

Model O
NYT-single B-DAT
NYT-multi O
Wiki-KBP O

NYT-single B-DAT
F1 O
ETL-Span O
59.4 O

4: O
An O
ablation O
study O
on O
NYT-single B-DAT

4 O
summarizes O
the O
results O
on O
NYT-single B-DAT

score O
w.r.t O
different O
λ O
on O
NYT-single B-DAT

with O
their O
relations O
using O
a O
single B-DAT
model. O
Prior O
works O
typically O
solve O

on O
three O
public O
datasets: O
NYT- O
single, B-DAT
NYT-multi O
and O
WebNLG. O
The O
results O

hi O
with O
ai O
into O
a O
single B-DAT
vector O
hstai O
. O
Analogously, O
xi’s O

single B-DAT
is O
sampled O
from O
the O
New O

single B-DAT
as O
the O
test O
set, O
5000 O

are O
all O
correct. O
For O
NYT- O
single B-DAT
and O
Wiki-KBP, O
we O
create O
a O

single B-DAT
NYT-multi O
WebNLG O

single B-DAT
and O
Wiki-KBP. O
(5) O
GraphRel O
(Fu O

single B-DAT
NYT-multi O
Wiki-KBPPrec. O
Rec. O
F1 O
Prec O

single B-DAT
dataset, O
ETL-Span O
outperforms O
PA-LSTM O
by O

relations O
re- O
lying O
on O
a O
single B-DAT
query O
word O

single B-DAT
NYT-multi O
Wiki-KBP O

single B-DAT
F1 O
ETL-Span O
59.4 O

single B-DAT

single B-DAT

single B-DAT

Datasets O
NYT B-DAT
Wiki-KBP O
#Relation O
types O
24 O
14 O

fectiveness O
of O
our O
method: O
(1) O
NYT B-DAT
(Riedel, O
Yao, O
and O
Mc- O
Callum O

Method O
NYT B-DAT
Wiki-KBPPrec. O
Rec. O
F1 O
Prec. O
Rec O

model O
and O
baseline O
methods O
on O
NYT B-DAT
and O
Wiki-KBP O
datasets. O
PA-LSTM-CRF O
denotes O

We O
compare O
our O
method O
on O
NYT B-DAT
and O
Wiki-KBP O
datasets O
with O
the O

the O
result O
on O
manually O
labeled O
NYT B-DAT
test O
set. O
Instead, O
they O
use O

report O
our O
experimental O
results O
on O
NYT B-DAT
and O
Wiki-KBP O
datasets O
in O
Table O

can O
extract O
overlapping O
relations. O
For O
NYT B-DAT
dataset, O
although O
no O
overlapping O
rela O

and O
“/loca- O
tion/administrative O
division/country” O
in O
NYT B-DAT

and O
re- O
lation O
using O
a O
single B-DAT
model. O
In O
this O
paper, O
we O

and O
overlapping O
relations O
using O
a O
single B-DAT
unified O
model O

n O
tag O
sequences O
in O
a O
single B-DAT
unified O
model, O
a O
novel O
position-attention O

we O
use O
the O
public O
dataset O
NYT B-DAT
2 O
which O
is O
pro- O
duced O

Mikolov O
et O
al., O
2013) O
on O
NYT B-DAT
training O
corpus. O
The O
dimension O
of O

and O
we O
only O
use O
the O
NYT B-DAT
dataset. O
Because O
more O
than O
50 O

together O
with O
relations O
using O
a O
single B-DAT
model. O
It O
can O
effec- O
tively O

with O
shared O
parameters O
in O
a O
single B-DAT
model, O
they O
also O
extract O
the O

entities O
and O
relations O
using O
a O
single B-DAT
model. O
Most O
of O
the O
joint O

to O
the O
prediction O
of O
more O
single B-DAT
E O
and O
less O
(E1, O
E2 O

precision O
and O
lower O
recall O
than O
single B-DAT
E. O
Besides, O
the O
predicted O
results O

visualize O
the O
ratio O
of O
predicted O
single B-DAT
entities O
for O
each O
end-to-end O
method O

low O
ra- O
tio O
on O
the O
single B-DAT
entities. O
It O
means O
that O
our O

4: O
The O
ratio O
of O
predicted O
single B-DAT
entities O
for O
each O
method. O
The O

pub- O
lic O
datasets O
NYT O
and O
WebNLG, B-DAT
respectively. O
In-depth O
anal- O
ysis O
on O

Yao, O
and O
McCallum O
2010) O
and O
WebNLG B-DAT
(Gardent O
et O
al. O
2017). O
NYT O

with O
24 O
predefined O
relation O
types. O
WebNLG B-DAT
dataset O
was O
originally O

Category O
NYT O
WebNLG B-DAT

relational O
triples, O
thus O
NYT O
and O
WebNLG B-DAT
datasets O
are O
widely O
used O
to O

5000 O
sentences O
for O
test, O
and O
WebNLG B-DAT
contains O
5019 O
sentences O
for O
training O

Method O
NYT O
WebNLG B-DAT

different O
methods O
on O
NYT O
and O
WebNLG B-DAT
datasets O

Ma O
2019) O
on O
NYT O
and O
WebNLG B-DAT
datasets O
respectively. O
Even O
without O
taking O

the O
performances O
on O
NYT O
and O
WebNLG B-DAT
datasets O
for O
existing O
models O
and O

the O
majority O
of O
sentences O
in O
WebNLG B-DAT
dataset O
belong O
to O
EPO O
and O

and O
a O
worse O
performance O
on O
WebNLG B-DAT
for O
all O
the O
base- O
lines O

performance O
on O
both O
NYT O
and O
WebNLG B-DAT
datasets, O
demonstrating O
the O
effectiveness O
of O

b) O
F1 O
of O
Different O
#Triples O
(WebNLG B-DAT

Element O
NYT O
WebNLG B-DAT

elements. O
For O
both O
NYT O
and O
WebNLG, B-DAT
the O
performance O
on O
extracting O
(E1 O

pub- O
lic O
datasets: O
NYT O
and O
WebNLG B-DAT

ex- O
traction O
datasets: O
NYT O
and O
WebNLG B-DAT

Method O
NYT O
WebNLG B-DAT

Results O
for O
both O
NYT O
and O
WebNLG B-DAT
datasets O

Category O
NYT O
WebNLG B-DAT

Riedel O
et O
al., O
2010) O
and O
WebNLG B-DAT
(Gardent O
et O
al., O
2017) O
datasets O

than O
100 O
words O
and O
for O
WebNLG, B-DAT
we O
use O
only O
the O
first O

The O
statistics O
of O
NYT O
and O
WebNLG B-DAT
is O
described O
in O
Table. O
2 O

for O
both O
the O
NYT O
and O
WebNLG B-DAT
datasets. O
OneDe- O
coder, O
proposed O
in O

can O
be O
found O
on O
the O
WebNLG B-DAT
dataset: O
GraphRel1p O
outperforms O
baseline O
F1 O

GraphRel1p. O
From O
the O
NYT O
and O
WebNLG B-DAT
results, O
we O
show O
that O
GCN’s O

results O
for O
both O
NYT O
and O
WebNLG B-DAT
datasets O

categories. O
For O
instance, O
on O
the O
WebNLG B-DAT
dataset, O
GraphRel1p O
outperforms O
Mul- O
tiDecoder O

Method O
NYT O
WebNLG B-DAT
GraphRel1p O
88.8% O
89.1% O
GraphRel2p O
89.2 O

to O
GraphRel2p O
on O
NYT O
and O
WebNLG B-DAT

1.4% O
respectively O
on O
NYT O
and O
WebNLG, B-DAT
with O
only O
a O
marginal O
0.6 O

recognition O
on O
both O
NYT O
and O
WebNLG B-DAT

Phase O
#GCN O
layer O
NYT O
WebNLG B-DAT

method O
on O
the O
NYT O
and O
WebNLG B-DAT
datasets. O
The O
results O
show O
that O

Class O
NYT O
WebNLG B-DAT

The O
second O
is O
WebNLG B-DAT
dataset O
(Gardent O
et O
al., O
2017 O

standard O
sen- O
tence. O
The O
origin O
WebNLG B-DAT
dataset O
contains O
train O
set O
and O

every O
class O
in O
NYT O
and O
WebNLG B-DAT
dataset O
are O
shown O
in O
Table O

Model O
NYT O
WebNLG B-DAT

models O
in O
NYT O
dataset O
and O
WebNLG B-DAT
dataset O

the O
NovelTagging O
model. O
In O
the O
WebNLG B-DAT
dataset, O
MultiDecoder O
model O
achieves O
the O

that, O
in O
both O
NYT O
and O
WebNLG B-DAT
dataset, O
the O
NovelTagging O
model O
achieves O

Model O
NYT O
WebNLG B-DAT
OneDecoder O
0.858 O
0.745 O

Model O
NYT O
WebNLG B-DAT
OneDecoder O
0.874 O
0.759 O

that O
on O
both O
NYT O
and O
WebNLG B-DAT
datasets, O
these O
t- O
wo O
models O

on O
two O
pub- O
lic O
datasets O
NYT B-DAT
and O
WebNLG, O
respectively. O
In-depth O
anal O

framework O
on O
two O
public O
datasets O
NYT B-DAT
(Riedel, O
Yao, O
and O
McCallum O
2010 O

WebNLG O
(Gardent O
et O
al. O
2017). O
NYT B-DAT
dataset O
was O
originally O
produced O
by O

Category O
NYT B-DAT
WebNLG O

contain O
multiple O
relational O
triples, O
thus O
NYT B-DAT
and O
WebNLG O
datasets O
are O
widely O

et O
al. O
2018), O
in O
which O
NYT B-DAT
contains O
56195 O
sentences O
for O
training O

Method O
NYT B-DAT
WebNLG O

Results O
of O
different O
methods O
on O
NYT B-DAT
and O
WebNLG O
datasets O

Li, O
and O
Ma O
2019) O
on O
NYT B-DAT
and O
WebNLG O
datasets O
respectively. O
Even O

gap O
between O
the O
performances O
on O
NYT B-DAT
and O
WebNLG O
datasets O
for O
existing O

1, O
we O
can O
see O
that O
NYT B-DAT
dataset O
is O
mainly O
comprised O
of O

a O
comparatively O
better O
performance O
on O
NYT B-DAT
and O
a O
worse O
performance O
on O

and O
competitive O
performance O
on O
both O
NYT B-DAT
and O
WebNLG O
datasets, O
demonstrating O
the O

a) O
F1 O
of O
Different O
#Triples O
(NYT) B-DAT
(b) O
F1 O
of O
Different O
#Triples O

Element O
NYT B-DAT
WebNLG O

relational O
triple O
elements. O
For O
both O
NYT B-DAT
and O
WebNLG, O
the O
performance O
on O

experiments O
on O
three O
public O
datasets: O
NYT B-DAT

- O
single, O
NYT B-DAT

on O
three O
benchmark O
datasets: O
(1) O
NYT B-DAT

triplet O
in O
each O
sentence. O
(2) O
NYT B-DAT

they O
selected O
5000 O
sentences O
from O
NYT B-DAT

entities O
are O
all O
correct. O
For O
NYT B-DAT

Dataset O
NYT-single B-DAT
NYT O

the O
recent O
state-of-the-art O
result O
on O
NYT B-DAT

the O
latest O
state-of-the-art O
method O
on O
NYT B-DAT

Model O
NYT-single B-DAT
NYT O

17.2% O
in O
F1 O
on O
the O
NYT B-DAT

in O
some O
relations. O
For O
the O
NYT B-DAT

Model O
NYT-single B-DAT
NYT O

NYT B-DAT

4: O
An O
ablation O
study O
on O
NYT B-DAT

4 O
summarizes O
the O
results O
on O
NYT B-DAT

score O
w.r.t O
different O
λ O
on O
NYT B-DAT

divide O
the O
test O
set O
of O
NYT B-DAT

NYT-single O
is O
sampled O
from O
the O
New B-DAT
York I-DAT
Times I-DAT
corpus O
(Riedel, O
Yao, O
and O
McCallum O

on O
two O
pub- O
lic O
datasets: O
NYT B-DAT
and O
WebNLG. O
Results O
show O
that O

public O
relation O
ex- O
traction O
datasets: O
NYT B-DAT
and O
WebNLG. O
The O
exper- O
imental O

showing O
promising O
results O
on O
the O
NYT B-DAT
dataset, O
their O
strength O
comes O
from O

Method O
NYT B-DAT
WebNLG O

Table O
1: O
Results O
for O
both O
NYT B-DAT
and O
WebNLG O
datasets O

Category O
NYT B-DAT
WebNLG O

5.1.1 O
Dataset O
We O
use O
the O
NYT B-DAT
(Riedel O
et O
al., O
2010) O
and O

As O
NovelTagging O
and O
MultiDecoder, O
for O
NYT, B-DAT
we O
filter O
sentences O
with O
more O

ex- O
periments. O
The O
statistics O
of O
NYT B-DAT
and O
WebNLG O
is O
described O
in O

and O
GraphRel O
for O
both O
the O
NYT B-DAT
and O
WebNLG O
datasets. O
OneDe- O
coder O

For O
the O
NYT B-DAT
dataset, O
we O
see O
that O
GraphRel1-hop O

2.2% O
upon O
GraphRel1p. O
From O
the O
NYT B-DAT
and O
WebNLG O
results, O
we O
show O

presents O
the O
results O
for O
both O
NYT B-DAT
and O
WebNLG O
datasets O

passes O
MultiDecoder O
by O
11.1% O
on O
NYT B-DAT

Method O
NYT B-DAT
WebNLG O
GraphRel1p O
88.8% O
89.1% O
GraphRel2p O

when O
applied O
to O
GraphRel2p O
on O
NYT B-DAT
and O
WebNLG O

1.8% O
and O
1.4% O
respectively O
on O
NYT B-DAT
and O
WebNLG, O
with O
only O
a O

for O
entity O
recognition O
on O
both O
NYT B-DAT
and O
WebNLG. O
It O
also O
shows O

Phase O
#GCN O
layer O
NYT B-DAT
WebNLG O

the O
proposed O
method O
on O
the O
NYT B-DAT
and O
WebNLG O
datasets. O
The O
results O

Class O
NYT B-DAT
WebNLG O

first O
is O
New O
York O
Times O
(NYT) B-DAT
dataset, O
which O
is O
produced O
by O

sentences O
of O
every O
class O
in O
NYT B-DAT
and O
WebNLG O
dataset O
are O
shown O

Model O
NYT B-DAT
WebNLG O

Results O
of O
different O
models O
in O
NYT B-DAT
dataset O
and O
WebNLG O
dataset O

EntityPairOverlap O
and O
SingleEntityOverlap O
classes O
in O
NYT B-DAT
dataset O

As O
we O
can O
see, O
in O
NYT B-DAT
dataset, O
our O
MultiDe- O
coder O
model O

also O
observe O
that, O
in O
both O
NYT B-DAT
and O
WebNLG O
dataset, O
the O
NovelTagging O

conduct O
further O
experi- O
ments O
on O
NYT B-DAT
dataset O

We O
divide O
the O
sentences O
of O
NYT B-DAT
test O
set O
into O
5 O
subclasses O

Model O
NYT B-DAT
WebNLG O
OneDecoder O
0.858 O
0.745 O

We O
divide O
the O
sentences O
in O
NYT B-DAT
test O
set O
into O
5 O
subclasses O

Model O
NYT B-DAT
WebNLG O
OneDecoder O
0.874 O
0.759 O

can O
observe O
that O
on O
both O
NYT B-DAT
and O
WebNLG O
datasets, O
these O
t O

The O
first O
is O
New B-DAT
York I-DAT
Times I-DAT
(NYT) O
dataset, O
which O
is O
produced O

sentences O
sampled O
from O
294k O
1987-2007 O
New B-DAT
York I-DAT
Times I-DAT
news O
articles. O
There O
are O
24 O

we O
use O
the O
public O
dataset O
NYT B-DAT
2 O
which O
is O
pro- O
duced O

Mikolov O
et O
al., O
2013) O
on O
NYT B-DAT
training O
corpus. O
The O
dimension O
of O

and O
we O
only O
use O
the O
NYT B-DAT
dataset. O
Because O
more O
than O
50 O

York O
Urban O
(YU) O
[2] O
and O
Eurasian B-DAT
Cities O
(EC) O
[14] O
(see O
Section O

Manhattan O
world O
assumption, O
and O
(ii) O
Eurasian B-DAT
City O
(EC) O
[14], O
consisting O
of O

Urban O
(YU) O
[2] O
and O
Eurasian O
Cities B-DAT
(EC) O
[14] O
(see O
Section O
5 O

standard O
bench- O
mark O
datasets, O
the O
Eurasian B-DAT
Cities O
Dataset O
[5] O
and O
the O

The O
Eurasian B-DAT
Cities O
Dataset O
(ECD) O
[5] O
is O

mark O
datasets, O
the O
Eurasian O
Cities O
Dataset B-DAT
[5] O
and O
the O
York O
Urban O

Dataset B-DAT
[11]. O
To O
our O
knowledge, O
our O

The O
York O
Urban O
Dataset B-DAT
(YUD) O
[11] O
is O
a O
commonly O

The O
Eurasian O
Cities O
Dataset B-DAT
(ECD) O
[5] O
is O
another O
com O

standard O
bench- O
mark O
datasets, O
the O
Eurasian B-DAT
Cities I-DAT
Dataset I-DAT
[5] O
and O
the O
York O
Urban O

The O
Eurasian B-DAT
Cities I-DAT
Dataset I-DAT
(ECD) O
[5] O
is O
another O
com O

bench- O
mark O
datasets, O
the O
Eurasian B-DAT
Cities I-DAT
Dataset I-DAT
[5] O
and O
the O
York O

The O
Eurasian B-DAT
Cities I-DAT
Dataset I-DAT
(ECD) O
[5] O
is O
another O

The O
Eurasian B-DAT
Cities O
Dataset O
(ECD) O
[3] O
contains O

The O
York O
Urban O
Dataset B-DAT
(YUD) O
[9] O
contains O
102 O
images O

The O
Eurasian O
Cities O
Dataset B-DAT
(ECD) O
[3] O
contains O
103 O
urban O

The O
Eurasian B-DAT
Cities I-DAT
Dataset I-DAT
(ECD) O
[3] O
contains O
103 O
urban O

The O
Eurasian B-DAT
Cities I-DAT
Dataset I-DAT
(ECD) O
[3] O
contains O
103 O

the O
same O
datasets O
(DSs), O
York O
Urban B-DAT
(YU) O
[2] O
and O
Eurasian O
Cities O

two O
usual O
DSs: O
(i) O
York O
Urban B-DAT
(YU) O
[2], O
consisting O
of O
102 O

Simon, O
G.: O
Facade O
Proposals O
for O
Urban B-DAT
Augmented O
Real- O
ity. O
In: O
IEEE O

on O
the O
same O
datasets O
(DSs), O
York B-DAT
Urban O
(YU) O
[2] O
and O
Eurasian O

the O
two O
usual O
DSs: O
(i) O
York B-DAT
Urban O
(YU) O
[2], O
consisting O
of O

Cities O
Dataset O
[5] O
and O
the O
York B-DAT
Urban I-DAT
Dataset I-DAT
[11]. O
To O
our O
knowledge, O
our O

The O
York B-DAT
Urban I-DAT
Dataset I-DAT
(YUD) O
[11] O
is O
a O
commonly O

mark O
datasets, O
the O
Eurasian O
Cities O
Dataset B-DAT
[5] O
and O
the O
York O
Urban O

Dataset B-DAT
[11]. O
To O
our O
knowledge, O
our O

The O
York O
Urban O
Dataset B-DAT
(YUD) O
[11] O
is O
a O
commonly O

The O
Eurasian O
Cities O
Dataset B-DAT
(ECD) O
[5] O
is O
another O
com O

Dataset O
[5] O
and O
the O
York B-DAT
Urban I-DAT
Dataset I-DAT
[11]. O
To O
our O
knowledge O

The O
York B-DAT
Urban I-DAT
Dataset I-DAT
(YUD) O
[11] O
is O
a O

Cities O
Dataset O
[5] O
and O
the O
York B-DAT
Urban O
Dataset O
[11]. O
To O
our O

cities. O
Example O
cities O
include O
New O
York, B-DAT
Rio O
de O
Janeiro, O
London, O
and O

The O
York B-DAT
Urban O
Dataset O
(YUD) O
[11] O
is O

The O
York B-DAT
Urban I-DAT
Dataset I-DAT
(YUD) O
[9] O
contains O
102 O
images O

The O
York O
Urban O
Dataset B-DAT
(YUD) O
[9] O
contains O
102 O
images O

The O
Eurasian O
Cities O
Dataset B-DAT
(ECD) O
[3] O
contains O
103 O
urban O

The O
York B-DAT
Urban I-DAT
Dataset I-DAT
(YUD) O
[9] O
contains O
102 O

The O
York B-DAT
Urban O
Dataset O
(YUD) O
[9] O
contains O

extension O
to O
the B-DAT
classic O
RANSAC O
algorithm O
from O
robust O

improve O
model O
hypothesis B-DAT
search, O
increasing O
the O
chance O
of O
finding O
outlier-free O
minimal O

tensions O
to O
NG-RANSAC. O
Firstly, O
using O
the B-DAT
inlier O
count O
it- O
self O
as O

focus O
on O
certain O
parts O
of O
the B-DAT
input O
data O
and O
make O
the O

the B-DAT

the B-DAT

sus O
with O
all O
observations, O
and O
the B-DAT
top-ranked O
hypothesis O
is O
returned O
as O

the B-DAT
final O
estimate O

domains O
with O
many O
outliers. O
As O
the B-DAT
ratio O
of O
outliers O
in- O
creases O

of O
RANSAC O
therefore B-DAT
often O
restrict O
the O
maximum O
number O
of O
iterations, O
and O

return O
the B-DAT
best O
model O
found O
so O
far O

88%, O
RANSAC O
fails O
to O
find O
the B-DAT
correct O
relative O
transformation O
(green O
correct O

over O
correspondences. O
Over O
90% O
of O
the B-DAT
probability O
mass O
falls O
onto O
239 O

to O
this O
distribution, O
and O
finds O
the B-DAT
correct O
transformation O
up O
to O
an O

observation. O
The O
weights O
ultimately O
guide O
the B-DAT
sampling O
of O
minimal O
sets. O
We O

call O
the B-DAT
resulting O
algorithm O
Neural-Guided O
RANSAC O
(NG-RANSAC O

final O
model O
parameters O
only O
to O
the B-DAT
latter. O
Although O
designed O
to O
re O

would O
remove O
any O
outliers O
that O
the B-DAT
neural O
network O
might O
have O
missed O

This O
motivates O
us O
to O
train O
the B-DAT
neural O
network O
in O
conjunction O
with O

Instead O
of O
interpreting O
the B-DAT
neural O
network O
output O
as O
soft O

robust O
model O
fit, O
we O
let O
the B-DAT
output O
weights O
guide O
RANSAC O
hypothesis O

sampling. O
Intuitively, O
the B-DAT
neural O
network O
should O
learn O
to O

paradigm O
yields O
substantial O
flexibility O
for O
the B-DAT
neural O
network O
in O
allowing O
a O

rate O
without O
negative O
effects O
on O
the B-DAT
final O
fitting O
accuracy O
due O
to O

the B-DAT
robustness O
of O
RANSAC. O
The O
distinc O

tolerable, O
is O
solely O
guided O
by O
the B-DAT
minimiza- O
tion O
of O
the O
task O

essential O
matrices, O
we O
may O
use O
the B-DAT
5-point O
algorithm O
rather O
than O
the O

36]. O
The O
flexibility O
in O
choosing O
the B-DAT
task O
loss O
also O
allows O
us O

self-supervised O
by O
using O
maximization O
of O
the B-DAT
inlier O
count O
as O
training O
objective O

Murray O
first O
proposed O
to O
guide O
the B-DAT
hy- O
pothesis O
search O
of O
MLESAC O

measures O
can O
be O
difficult O
as O
the B-DAT
reliance O
on O
over-confident O
but O
wrong O

probabilities O
can O
yield O
situations O
where O
the B-DAT
same O
few O
observations O
are O
sampled O

side O
information O
only O
to O
change O
the B-DAT
order O
in O
which O
RANSAC O
draws O

minimal O
sets. O
In O
the B-DAT
worst O
case, O
if O
the O
side O

different O
approach O
in O
(i) O
learning O
the B-DAT
weights O
to O
guide O
hypothesis O
search O

ii) O
integrating O
RANSAC O
itself O
in O
the B-DAT
training O
process O
which O
leads O
to O

self-calibration O
of O
the B-DAT
pre- O
dicted O
weights O

to O
make O
DSAC O
differentiable, O
namely O
the B-DAT
optimization O
of O
the O
ex- O
pected O

servations O
and O
observation O
confidences O
at O
the B-DAT
same O
time. O
We O
summarize O
our O

solver O
for O
training. O
• O
Choosing O
the B-DAT
inlier O
count O
itself O
as O
training O

accurate O
predictions O
for O
parts O
of O
the B-DAT
input, O
while O
neglecting O
other O
parts O

the B-DAT

in O
various O
ways, O
see O
e.g. O
the B-DAT
survey O
by O
Raguram O
et O
al O

. O
[35]. O
Combining O
some O
of O
the B-DAT
most O
promising O
improvements, O
Raguram O
et O

al. O
created O
the B-DAT
Uni- O
versal O
RANSAC O
(USAC) O
framework O

34] O
which O
represents O
the B-DAT
state-of-the-art O
of O
classic O
RANSAC O
variants O

Randomized O
RANSAC O
[10]. O
Many O
of O
the B-DAT
improvements O
proposed O
for O
RANSAC O
could O

RANSAC O
is O
not O
part O
of O
the B-DAT
training O
process O
because O
of O
its O

overcomes O
this O
limitation O
by O
making O
the B-DAT
hypothesis O

which O
facilitates O
optimiza- O
tion O
of O
the B-DAT
expected O
task O
loss O
during O
training O

can O
use O
it O
to O
calculate O
the B-DAT
gradient O
of O
image O
coor- O
dinates O

The O
combination O
with O
DSAC O
enables O
the B-DAT
full O
flexibility O
of O
learning O
both O

robust O
estima- O
tors. O
We O
discussed O
the B-DAT
work O
of O
Yi O
et O
al O

. O
[56] O
in O
the B-DAT
intro- O
duction. O
Ranftl O
and O
Koltun O

model O
fit, O
taking O
into O
account O
the B-DAT
residuals O
of O
the O
last O
iteration O

when O
it O
comes O
to O
defining O
the B-DAT
training O
objective. O
This O
flexibility O
also O

3. O
Method O
Preliminaries. O
We O
address O
the B-DAT
problem O
of O
fitting O
model O
parameters O

a O
fundamental O
matrix O
that O
describes O
the B-DAT
epipolar O
geometry O
of O
an O
image O

16], O
and O
Y O
could O
be O
the B-DAT
set O
of O
SIFT O
cor- O
respondences O

27] O
we O
extract O
for O
the B-DAT
image O
pair. O
To O
calcu- O
late O

model O
parameters O
from O
the B-DAT
observations, O
we O
utilize O
a O
solver O

f O
, O
for O
example O
the B-DAT
8-point O
algorithm O
[15]. O
However, O
calculating O

N O
= O
8 O
when O
using O
the B-DAT
8-point O
algorithm. O
RANSAC O
[12] O
is O

set O
from O
Y O
such O
that O
the B-DAT
resulting O
estimate O
h O
is O
accurate O

on O
an O
online O
estimate O
of O
the B-DAT
outlier O
ratio O
[12]. O
The O
strategy O

For O
notational O
simplicity, O
we O
take O
the B-DAT
perspective O
of O
a O
fixed O
M O

but O
do O
not O
restrict O
the B-DAT
use O
of O
an O
early-stopping O
strategy O

chooses O
a O
model O
hypothesis B-DAT
as O
the O
final O
esti- O
mate O
ĥ O
according O

The O
scoring O
function O
measures O
the B-DAT
consensus O
of O
an O
hypoth- O
esis O

uni- O
formly O
random O
to O
create O
the B-DAT
hypothesis O
pool O
H. O
We O
aim O

is O
a O
categorical O
distribution O
over O
the B-DAT
discrete O
set O
of O
observations O
Y O

in O
a O
way O
that O
increases O
the B-DAT
chance O
of O
selecting O
outlier- O
free O

assume O
that O
we O
can O
measure O
the B-DAT
quality O
of O
the O
estimate O
with O

or O
self-supervised, O
e.g. O
by O
using O
the B-DAT
inlier O
count O
of O
the O
final O

s(ĥ,Y). O
We O
wish O
to O
learn O
the B-DAT
distribution O
p(H;w) O
in O
a O
way O

define O
our O
training O
objective O
as O
the B-DAT
minimiza- O
tion O
of O
the O
expected O

We O
compute O
the B-DAT
gradients O
of O
the O
expected O
task O
loss O
w.r.t. O
the O

possible O
hypothesis B-DAT
pools O
to O
calculate O
the O
expectation O
is O
infeasible. O
Therefore, O
we O

approximate O
the B-DAT
gradients O
by O
drawing O
K O
samplesHk O

Note O
that O
gradients O
of O
the B-DAT
task O
loss O
function O
` O
do O

not O
appear O
in O
the B-DAT
expression O
above. O
Therefore, O
differentiability O
of O

the B-DAT

task O
loss O
`, O
the B-DAT
robust O
solver O
ĥ O
(i.e. O
RANSAC O

) O
or O
the B-DAT
min- O
imal O
solver O
f O
is O

a O
training O
signal O
for O
steering O
the B-DAT
sampling O
proba- O
bility O
p(H;w) O
in O

a O
good O
direction. O
Due O
to O
the B-DAT
approxima- O
tion O
by O
sampling, O
the O

found O
a O
simple O
baseline O
in O
the B-DAT
form O
of O
the O
average O
loss O

i.e. O
b O
= O
¯̀. O
Subtracting O
the B-DAT
baseline O
will O
move O
the O
probability O

Y(w). O
End-to- O
end O
training O
of O
the B-DAT
pipeline, O
and O
therefore O
learning O
the O

y(w), O
is O
possible O
by O
turning O
the B-DAT
argmax O
hypoth- O
esis O
selection O
of O

In O
the B-DAT
following, O
we O
extend O
the O
formulation O
of O
DSAC O
with O
neural O

guidance O
(NG-DSAC). O
We O
let O
the B-DAT
neural O
network O
predict O
observations O
y(w O

with O
each O
observation O
p(y;w). O
Intuitively, O
the B-DAT
neural O
network O
can O
express O
a O

if O
a O
certain O
input O
for O
the B-DAT
neural O
network O
contains O
no O
information O

about O
the B-DAT
desired O
model O
h. O
In O
this O

case, O
the B-DAT
observation O
prediction O
y(w) O
is O
necessarily O

an O
outlier, O
and O
the B-DAT
best O
the O
neural O
net- O
work O
can O
do O

low O
proba- O
bility. O
We O
combine O
the B-DAT
training O
objectives O
of O
NG-RANSAC O
(Eq O

consists O
of O
two O
expectations. O
Firstly, O
the B-DAT
expectation O
w.r.t. O
sampling O
a O
hypothesis O

pool O
according O
to O
the B-DAT
probabili- O
ties O
predicted O
by O
the O

neural O
network. O
Secondly, O
the B-DAT
expecta- O
tion O
w.r.t. O
sampling O
a O

final O
estimate O
from O
the B-DAT
pool O
according O

to O
the B-DAT
scoring O
function. O
As O
in O
NG-RANSAC O

, O
we O
approxi- O
mate O
the B-DAT
first O
expectation O
via O
sampling, O
as O

hypothesis B-DAT
pools O
is O
infeasible. O
For O
the O
second O
expectation, O
we O
can O
calculate O

DSAC, O
since O
it O
integrates O
over O
the B-DAT
discrete O
set O
of O
hypotheses O
hj O

to O
Eq. O
6, O
we O
give O
the B-DAT
approximate O
gradients O
∂∂wL(w) O
of O
NG-DSAC O

of O
gradients O
for O
NG-DSAC O
requires O
the B-DAT
derivative O
of O
the O
task O
loss O

note O
the B-DAT
last O
part O
of O
Eq. O
10 O

observations O
and O
observation O
confidences, O
at O
the B-DAT
same O
time O

geometry O
of O
image O
pairs O
in O
the B-DAT
form O
of O
essential O
matrices O
and O

camera O
re- O
localization. O
We O
present O
the B-DAT
main O
experimental O
results O
here, O
and O

refer O
to O
the B-DAT
appendix O
for O
details O
about O
network O

32], O
and O
we O
will O
make O
the B-DAT
code O
publicly O
available O
for O
all O

Epipolar O
geometry O
describes O
the B-DAT
geometry O
of O
two O
images O
that O

observe O
the B-DAT
same O
scene O
[16]. O
In O
particular O

points O
x O
and O
x′ O
in O
the B-DAT
left O
and O
right O
image O
corresponding O

to O
the B-DAT
same O
3D O
point O
satisfy O
x′>Fx O

0, O
where O
the B-DAT
3× O
3 O
ma- O
trix O
F O

denotes O
the B-DAT
fundamental O
matrix. O
We O
can O
estimate O

is O
a O
special O
case O
of O
the B-DAT
fundamental O
ma- O
trix O
when O
the O

from O
5 O
correspondences O
[31]. O
Decomposing O
the B-DAT
essential O
matrix O
allows O
to O
recover O

the B-DAT
relative O
pose O
between O
the O
observing O
cameras, O
and O
is O
a O

reconstruction O
[40]. O
As O
such, O
estimating O
the B-DAT
fundamen- O
tal O
or O
essential O
matrices O

Essential O
Matrix O
Estimation. O
We O
calculate O
the B-DAT
relative O
pose O
between O
outdoor O
and O

indoor O
image O
pairs O
via O
the B-DAT
essential O
matrix. O
We O
measure O
the O

AUC O
of O
the B-DAT
cumulative O
angular O
error O
up O
to O

use O
no O
side O
information O
about O
the B-DAT
sparse O
correspondences. O
b) O
We O
use O

side O
information O
in O
the B-DAT
form O
of O
descriptor O
distance O
ratios O

between O
the B-DAT
best O
and O
second O
best O
match O

a O
self-supervised O
fashion O
by O
using O
the B-DAT
inlier O
count O
as O
training O
objective O

In O
the B-DAT
following, O
we O
firstly O
evaluate O
NG-RANSAC O

for O
the B-DAT
calibrated O
case O
and O
estimate O
essential O

from O
SIFT O
correspondences O
[27]. O
For O
the B-DAT
sake O
of O
comparabil- O
ity O
with O

the B-DAT
recent, O
learned O
robust O
estimator O
of O

well O
as O
indoor O
settings. O
For O
the B-DAT
outdoor O
datasets, O
they O
select O
five O

scenes O
from O
the B-DAT
structure-from-motion O
(SfM) O
dataset O
of O
[19 O

al. O
choose O
16 O
sequences O
from O
the B-DAT
SUN3D O
dataset O
[54] O
which O
readily O

on O
completely O
separate O
scenes, O
i.e. O
the B-DAT
neural O
net- O
work O
has O
to O

unknown O
environments. O
Evaluation O
Metric. O
Via O
the B-DAT
essential O
matrix, O
we O
recover O
the O

to O
scale, O
and O
compare O
to O
the B-DAT
ground O
truth O
pose O
as O
follows O

. O
We O
measure O
the B-DAT
angular O
error O
between O
the O
pose O

rotations, O
as O
well O
as O
the B-DAT
angular O
error O
be- O
tween O
the O

vectors O
in O
degrees. O
We O
take O
the B-DAT
maximum O
of O
the O
two O
values O

as O
the B-DAT
final O
angular O
error. O
We O
calculate O

the B-DAT
cumulative O
error O
curve O
for O
each O

test O
sequence, O
and O
compute O
the B-DAT
area O
under O
the O
curve O
(AUC O

or O
20◦. O
Finally, O
we O
report O
the B-DAT
average O
AUC O
over O
all O
test O

sequences O
(but O
separately O
for O
the B-DAT
indoor O
and O
outdoor O
setting O

a O
4D O
vector O
com- O
bining O
the B-DAT
2D O
coordinate O
in O
the O
left O

RANSAC O
(Eq. O
3). O
That O
is, O
the B-DAT
network O
predicts O
weights O
to O
guide O

inlier O
class O
labels. O
We O
use O
the B-DAT
angular O
error O
between O
the O
estimated O

relative O
pose, O
and O
the B-DAT
ground O
truth O
pose O
as O
task O

solver O
f O
, O
we O
use O
the B-DAT
5-point O
algorithm O
[31]. O
To O
speed O

up O
training, O
we O
initialize O
the B-DAT
network O
by O
learning O
to O
predict O

the B-DAT
distance O
of O
each O
correspondence O
to O

the B-DAT
ground O
truth O
epipolar O
line, O
see O

For O
testing, O
we O
in- O
crease O
the B-DAT
number O
of O
hypotheses O
to O
M O

Results. O
We O
compare O
NG-RANSAC O
to O
the B-DAT
inlier O
classifi- O
cation O
(InClass) O
of O

all O
thresh- O
olds, O
scoring O
as O
the B-DAT
weakest O
method. O
In O
this O
experiment O

side O
information O
is O
available O
about O
the B-DAT
quality O
of O
correspondences. O
Therefore, O
USAC O

for O
RANSAC O
and O
NG-RANSAC. O
For O
the B-DAT
indoor O
and O
outdoor O
image O
pairs O

fit O
essential O
matrices, O
and O
for O
the B-DAT
Kitti O
image O
pair O
we O
fit O

the B-DAT
fundamental O
matrix. O
We O
draw O
final O

green O
if O
they B-DAT
adhere O
to O
the O
ground O
truth O
model, O
and O
red O

otherwise. B-DAT
We O
also O
measure O
the O
quality O
of O
each O
estimate, O
see O

the B-DAT
main O
text O
for O
details O
on O

the B-DAT
metrics O

guidance O
(NG-RANSAC) O
elevates O
it O
to O
the B-DAT
leading O
position O
with O
a O
comfortable O

useful O
guiding O
weights O
solely O
from O
the B-DAT
spatial O
distribution O
of O
correspondences. O
See O

spite O
some O
similarities. O
Both O
use O
the B-DAT
same O
network O
archi- O
tecture, O
are O

brid O
classification-regression O
loss O
based O
on O
the B-DAT
8-point O
al- O
gorithm, O
and O
ultimately O

is O
very O
dif- O
ferent O
from O
the B-DAT
evaluation O
procedure. O
During O
evaluation, O
they O

use O
RANSAC O
with O
the B-DAT
5-point O
algorithm O
on O
top O
of O

their B-DAT
inlier O
predictions, O
and O
measure O
the O
angular O
error. O
NG- O
RANSAC O
incorporates O

training O
procedure, O
and O
therefore B-DAT
optimizes O
the O
correct O
objective O

datasets. O
The O
distance O
ratio O
of O
the B-DAT
best O
and O
second-best O
SIFT O
match O

guide O
hypothesis B-DAT
sampling O
according O
to O
the O
PROSAC O
strategy O
[9]. O
Furthermore, O
Lowe’s O

before O
running O
RANSAC. O
We O
denote O
the B-DAT
ratio O
filter O
as O
+Ratio O
in O

vs. O
Hypothesis B-DAT
Budget. O
We O
compare O
the O
AUC O
of O
NG-RANSAC O
and O
USAC O

34] O
and O
NG-RANSAC O
depend O
on O
the B-DAT
hypothesis O
budgetM O
, O
see O
Fig O

USAC O
is O
designed O
to O
draw O
the B-DAT
same O
hypotheses O
as O
RANSAC O
but O

to O
USAC. O
For O
example, O
for O
the B-DAT
outdoor O
set- O
ting, O
w/o O
ratio O

USAC/PROSAC O
sampling O
scheme O
assumes O
that O
the B-DAT
probability O
of O
correspondences O
being O
inliers O

in- O
creases O
monotonically O
with O
the B-DAT
sampling O
weight O
[9]. O
In O
contrast O

might O
be O
ranked O
high O
by O
the B-DAT
neural O
network, O
without O
affecting O
accuracy O

neg- O
atively, O
thus O
violating O
the B-DAT
assumption O
of O
PROSAC O

5. O
Hypothesis B-DAT
Search. O
We O
visualize O
the O
best O
hypothesis O
found O
after O
M O

For O
each O
result, O
we O
give O
the B-DAT
number O
of O
correspondences O
which O
are O

also O
inliers O
for O
the B-DAT
ground O
truth O
model O
(GT O
Inliers O

We O
perform O
this O
experiment O
in O
the B-DAT
Indoor O
scenario, O
using O
side O
information O

task O
loss O
` O
to O
assess O
the B-DAT
quality O
of O
an O
estimate O
independent O

h∗. O
A O
natural O
choice O
is O
the B-DAT
inlier O
count O
of O
the O
final O

estimate. O
We O
found O
the B-DAT
inlier O
count O
to O
be O
a O

stable O
training O
signal, O
even O
in O
the B-DAT
beginning O
of O
training O
such O
that O

require O
no O
special O
initialization O
of O
the B-DAT
network. O
We O
report O
results O
of O

to O
adapt O
more O
precisely O
to O
the B-DAT
evaluation O
measure O
used O
at O
test O

time. O
For O
the B-DAT
datasets O
used O
so O
far, O
the O

a O
form O
of O
supervision. O
In O
the B-DAT
next O
sec- O
tion, O
we O
learn O

NG-RANSAC O
fully O
self-supervised O
by O
using O
the B-DAT
ordering O
of O
sequential O
data O
to O

Runtime. O
A O
forward O
pass O
of O
the B-DAT
network O
takes O
3ms O
on O
CPU O

runtime O
(and O
accuracy) O
depends O
on O
the B-DAT
hypothesis O
count O
M O
. O
For O

estima- O
tion, O
comparing O
it O
to O
the B-DAT
learned O
robust O
estimator O
of O
Ranftl O

model O
fit. O
The O
residuals O
of O
the B-DAT
last O
iteration O
are O
an O
additional O

input O
to O
the B-DAT
network O
in O
the O
next O
iteration. O
The O
network O
architecture O

is O
similar O
to O
the B-DAT
one O
used O
in O
[56]. O
Correspondences O

4D O
vectors, O
and O
they B-DAT
use O
the O
descriptor O
matching O
ratio O
as O
an O

Fundamental O
Matrix O
Estimation. O
We O
measure O
the B-DAT
average O
percentage O
of O
inliers O
of O

the B-DAT
estimated O
model, O
the O
align- O
ment O
of O
estimated O
inliers O

ground O
truth O
inliers O
(F-score), O
and O
the B-DAT
mean O
and O
median O
distance O
of O

epilines. O
For O
NG-RANSAC, O
we O
compare O
the B-DAT
performance O
after O
training O
with O
different O

was O
published O
very O
recently, O
and O
the B-DAT
code O
is O
not O
yet O
available O

. O
We O
therefore B-DAT
follow O
the O
evaluation O
procedure O
de- O
scribed O
in O

compare O
to O
their B-DAT
method O
on O
the O
Kitti O
dataset O
[14], O
which O
is O

method O
on O
sequences O
00-05 O
of O
the B-DAT
Kitti O
odometry O
benchmark, O
and O
test O

ing O
multiple O
metrics. O
They O
measure O
the B-DAT
percentage O
of O
in- O
lier O
correspondences O

of O
the B-DAT
final O
model O
relative O
to O
all O

cor- O
respondences. O
They O
calculate O
the B-DAT
F-score O
over O
correspon- O
dences O
where O

positives O
are O
inliers O
of O
both O
the B-DAT
ground O
truth O
model O
and O
the O

estimated O
model. O
The O
F-score O
measures O
the B-DAT
alignment O
of O
estimated O
and O
true O

of O
0.1px. O
Finally, O
they B-DAT
calculate O
the O
mean O
and O
median O
epipolar O
error O

of O
inlier O
correspondences O
w.r.t. O
the B-DAT
ground O
truth O
model, O
us- O
ing O

1px. O
Implementation. O
We O
cannot O
use O
the B-DAT
architecture O
of O
Deep O
F-Mat O
which O

application. O
There- O
fore, O
we O
re-use O
the B-DAT
architecture O
of O
Yi O
et O
al O

. O
[56] O
from O
the B-DAT
previous O
section O
for O
NG-RANSAC O
(also O

for O
details). O
We O
adhere O
to O
the B-DAT
training O
setup O
described O
in O
Sec O

. O
4.1 O
with O
the B-DAT
following O
changes. O
We O
observed O
faster O

on O
Kitti, O
so O
we O
omit O
the B-DAT
initialization O
stage, O
and O
directly O
optimize O

the B-DAT
expected O
task O
loss O
(Eq. O
3 O

36] O
evaluate O
using O
multiple O
metrics, O
the B-DAT
choice O
of O
the O
task O
loss O

and O
Mean O
error) O
and O
report O
the B-DAT
corresponding O
results. O
As O
minimal O
solver O

f O
, O
we O
use O
the B-DAT
7-point O
algorithm, O
a O
RANSAC O
threshold O

Line O
Estimation. O
Left: O
AUC O
on O
the B-DAT
HLW O
dataset. O
Right: O
Qualitative O
results O

. O
We O
draw O
the B-DAT
ground O
truth O
hori- O
zon O
in O

green O
and O
the B-DAT
estimate O
in O
blue. O
Dots O
mark O

the B-DAT
observations O
predicted O
by O
NG-DSAC, O
and O

the B-DAT
dot O
colors O
mark O
their O
confidence O

dark O
= O
low). O
Note O
that O
the B-DAT
horizon O
can O
be O
outside O
the O

Deep O
F- O
Mat. O
NG-RANSAC O
outperforms O
the B-DAT
classical O
approaches O
RANSAC O
and O
USAC O

Deep O
F-Mat. O
We O
observe O
that O
the B-DAT
choice O
of O
the O
training O
objective O

small O
but O
significant O
influence O
on O
the B-DAT
evaluation. O
All O
metrics O
are O
highly O

test O
time. O
In- O
terestingly, O
optimizing O
the B-DAT
inlier O
count O
during O
training O
per O

We O
fit O
a O
parametric O
model, O
the B-DAT
horizon O
line, O
to O
a O
single O

64 O
2D O
points O
based O
on O
the B-DAT
image O
to O
which O
we O
fit O

two O
output O
branches O
predicting O
(i) O
the B-DAT
2D O
points O
y(w) O
∈ O
Y(w O

details). O
Dataset. O
We O
evaluate O
on O
the B-DAT
HLW O
dataset O
[52] O
which O
is O

and O
training O
images O
partly O
show O
the B-DAT
same O
scenes, O
and O
the O
horizon O

line O
can O
be O
outside O
the B-DAT
image O
area. O
Evaluation O
Metric. O
As O

practice O
on O
HLW, O
we O
measure O
the B-DAT
maximum O
distance O
between O
the O
estimated O

zon O
and O
ground O
truth O
within O
the B-DAT
image, O
normalized O
by O
im- O
age O

height. O
We O
calculate O
the B-DAT
AUC O
of O
the O
cumulative O
error O

0.25. O
Implementation. O
We O
train O
using O
the B-DAT
NG-DSAC O
objective O
(Eq. O
9) O
from O

task O
loss O
`, O
we O
use O
the B-DAT
normalized O
maximum O
distance O
between O
estimated O

training. O
b) O
Internal O
representation O
of O
the B-DAT
neural O
network. O
We O
predict O
scene O

we O
choose O
randomly O
according O
to O
the B-DAT
predicted O
distribution O

we O
train O
similarly O
but O
disable O
the B-DAT
probability O
branch. O
Results. O
We O
report O

this O
dataset, O
rank- O
ing O
among O
the B-DAT
top O
methods. O
NG-DSAC O
has O
a O

informed O
candidate O
sam- O
pling. O
Unfortunately, O
the B-DAT
code O
of O
SLNet O
is O
not O

online O
and O
the B-DAT
authors O
did O
not O
respond O
to O

We O
estimate O
the B-DAT
absolute O
6D O
camera O
pose O
(position O

image. O
Dataset. O
We O
evaluate O
on O
the B-DAT
Cambridge O
Landmarks O
[22] O
dataset. O
It O

pipeline. O
Evaluation O
Metric. O
We O
measure O
the B-DAT
median O
translational O
error O
of O
estimated O

2We O
omitted O
the B-DAT
Street O
scene. O
Like O
DSAC++ O
[6 O

results, O
here. O
By O
visual O
inspection, O
the B-DAT
corresponding O
SfM O
recon- O
struction O
seems O

Implementation. O
We O
build O
on O
the B-DAT
publicly O
available O
DSAC++ O
pipeline O
[6 O

in O
scene O
space. O
We O
recover O
the B-DAT
pose O
from O
the O
2D-3D O
correspondences O

facilitates O
end-to-end O
training. O
We O
re-implement O
the B-DAT
neural O
network O
integration O
of O
DSAC O

++ O
with O
PyTorch O
(the B-DAT
original O
uses O
LUA/Torch). O
We O
also O

update O
the B-DAT
network O
architecture O
of O
DSAC++ O
by O

a O
second O
output O
branch O
to O
the B-DAT
net- O
work O
for O
estimating O
a O

architecture O
NG-DSAC++. O
We O
adhere O
to O
the B-DAT
training O
procedure O
and O
hyperparamters O
of O

see O
Appendix O
D) O
but O
optimize O
the B-DAT
NG-DSAC O
objective O
(Eq. O
9) O
during O

task O
loss O
`, O
we O
use O
the B-DAT
average O
of O
the O
rotational O
and O

translational O
error O
w.r.t. O
the B-DAT
ground O
truth O
pose. O
We O
sample O

per O
training O
image, O
and O
increase O
the B-DAT
number O
of O
hypotheses O
to O
M O

neural O
guidance O
is O
largest O
for O
the B-DAT
Great O
Court O
scene, O
which O
features O

to O
ignore O
such O
areas, O
see O
the B-DAT
visualization O
in O
Fig. O
8 O
a O

these B-DAT
areas O
solely O
guided O
by O
the O
task O
loss O
during O
training, O
as O

the B-DAT
network O
fails O
to O
predict O
accurate O

Fig. O
8 O
b), O
we O
visualize O
the B-DAT
internal O
representation O
learned O
by O
DSAC O

and O
non-differentiable O
minimal O
solvers. O
Using O
the B-DAT
inlier O
count O
as O
training O
objective O

project O
has O
received O
funding O
from O
the B-DAT
European O
Research O
Council O
(ERC) O
under O

the B-DAT
Eu- O
ropean O
Unions O
Horizon O
2020 O

on O
an O
HPC O
Cluster O
at O
the B-DAT
Center O
for O
Infor- O
mation O
Services O

Network O
Architecture. O
As O
mentioned O
in O
the B-DAT
main O
paper, O
we O
replicated O
the O

correspondence O
indepen- O
dently. O
We O
implement O
the B-DAT
MLPs O
with O
1 O
× O
1 O

20]. O
The O
main O
body O
of O
the B-DAT
network O
is O
comprised O
of O
12 O

apply O
a O
Sigmoid O
activation O
to O
the B-DAT
last O
layer, O
and O
normal- O
ize O

by O
dividing O
by O
the B-DAT
sum O
of O
outputs.4 O

We O
initialize O
our O
network O
in O
the B-DAT
following O
way. O
We O
define O
a O

target O
sampling O
distribution O
g(y;E∗) O
using O
the B-DAT
ground O
truth O
essential O
matrix O
E O

for O
each O
training O
pair. O
Intuitively, O
the B-DAT
target O
distribution O
should O
return O
a O

correspondence O
y O
is O
aligned O
with O
the B-DAT
ground O
truth O
essential O
matrixE∗, O
and O

in O
homogeneous O
coordinates). O
We O
define O
the B-DAT
epipolar O
error O
of O
a O
correspondence O

where O
[·]i O
returns O
the B-DAT
ith O
entry O
of O
a O
vector O

. O
Using O
the B-DAT
epipo- O
lar O
error, O
we O
define O

the B-DAT
target O
sampling O
distribution O

Parameter O
σ O
controls O
the B-DAT
softness O
of O
the O
target O
distribution O

10−3 O
which O
corresponds O
to O
the B-DAT
inlier O
threshold O
we O
use O
for O

initialize O
our O
network, O
we O
minimize O
the B-DAT
KL O
divergence O
between O
the O
network O

pre- O
diction O
p(y;w) O
and O
the B-DAT
target O
distribution O
g(y;E∗). O
We O
initialize O

of O
32. O
Implementation O
Details. O
For O
the B-DAT
following O
components O
we O
rely O
on O

the B-DAT
implementations O
provided O
by O
OpenCV O
[7 O

]: O
the B-DAT
5-point O
algorithm O
[31], O
epipolar O
error O

filtering O
and O
hence O
re- O
ducing O
the B-DAT
number O
of O
correspondences, O
we O
randomly O

du- O
plicate O
correspondences O
to O
restore O
the B-DAT
number O
of O
2000. O
We O

put O
processing O
due O
to O
using O
the B-DAT
output O
as O
weights O
for O
a O

skip O
connection O
for O
each O
of O
the B-DAT
twelve O
blocks O
[18]. O
This O
architecture O

minimize O
the B-DAT
expected O
task O
loss O
using O
Adam O

based O
on O
validation O
error O
of O
the B-DAT
Reichstag O
scene. O
We O
observe O
that O

the B-DAT
magnitude O
of O
the O
validation O
er- O
ror O
corresponds O
well O

to O
the B-DAT
magnitude O
of O
the O
training O
error, O
i.e. O
a O
validation O

When O
calculating O
the B-DAT
AUC O
for O
evaluation, O
we O
adhere O

to O
the B-DAT
protocol O
of O
Yi O
et O
al O

comparability. O
Yi O
et O
al. O
approximate O
the B-DAT
AUC O
via O
the O
area O
under O

the B-DAT
cumulative O
histogram O
with O
a O
bin O

ob- O
tain O
these B-DAT
results O
in O
the O
high-outlier O
setup, O
i.e. O
without O
using O

Estimation O
Implementation O
Details. O
We O
reuse O
the B-DAT
architecture O
of O
Fig. O
10. O
To O

of O
feature O
matches, O
we O
subtract O
the B-DAT
mean O
coordinate O
and O
divide O
by O

the B-DAT
coordi- O
nate O
standard O
deviation, O
where O

and O
stan- O
dard O
deviation O
over O
the B-DAT
training O
set. O
Ranftl O
and O
Koltun O

36] O
fit O
the B-DAT
final O
fundamental O
matrix O
to O
the O

their B-DAT
network. O
Similarly, O
we O
re-fit O
the O
final O
fundamental O
matrix O
to O
the O

noticeable O
increase O
in O
accuracy. O
For O
the B-DAT
follow- O
ing O
components O
we O
rely O

on O
the B-DAT
implementations O
provided O
by O
OpenCV O
[7 O

]: O
the B-DAT
7-point O
algorithm, O
epipolar O
error, O
SIFT O

additional O
qualitative O
re- O
sults O
for O
the B-DAT
Kitti O
dataset O
[14] O
in O
Fig O

arbitrary O
aspect O
ratio O
such O
that O
the B-DAT
long O
side O
is O
256px. O
We O

symmetrically O
zero-pad O
the B-DAT
short O
side O
to O
256px. O
The O

y(w), O
to O
which O
we O
fit O
the B-DAT
horizon O
line. O
We O
apply O
a O

to O
support O
horizon O
lines O
outside O
the B-DAT
image O
area. O
We O
implement O
the O

or O
restricted O
receptive O
field, O
of O
the B-DAT
input O
image. O
Therefore, O
we O
shift O

the B-DAT
coordinate O
of O
each O
output O
point O

to O
the B-DAT
center O
of O
its O
associ- O
ated O

We O
apply O
a O
Sigmoid O
to O
the B-DAT
output O
of O
the O
second O
branch O

and O
normalize O
by O
dividing O
by O
the B-DAT
sum O
of O
outputs. O
During O
training O

, O
we O
block O
the B-DAT
gradients O
of O
the O
second O
output O

branch O
when O
back O
propagating O
to O
the B-DAT
base O
network. O
The O
sampling O
gradients O

larger O
variance O
and O
magnitude O
than O
the B-DAT
observation O
gradients O
of O
the O
first O

branch, O
especially O
in O
the B-DAT
beginning O
of O
training O
with O
a O

tive O
effect O
on O
convergence O
of O
the B-DAT
network O
as O
a O
whole. O
Intu O

want O
to O
give O
priority O
to O
the B-DAT
observation O
prediction O
because O
they O
determine O

the B-DAT
accuracy O
of O
the O
final O
model O
pa O

Below O
each O
result, O
we O
give O
the B-DAT
angular O
error O
between O
estimated O
and O

green O
if O
they B-DAT
adhere O
to O
the O
ground O
truth O
essential O
matrix O
with O

should O
address O
deficien- O
cies O
in O
the B-DAT
observation O
predictions O
without O
influencing O
them O

where O
d(y,h) O
denotes O
the B-DAT
point-line O
distance O
between O
ob- O
servation O

h. O
Hyperparameter O
α O
deter- O
mines O
the B-DAT
softness O
of O
the O
scoring O
distribution O

in O
DSAC, O
β O
determines O
the B-DAT
softness O
of O
the O
Sigmoid, O
and O

τ O
is O
the B-DAT
inlier O
threshold. O
We O
use O
α O

images O
to O
grayscale, O
and O
apply O
the B-DAT
fol- O
lowing O
data O
augmentation O
strategy O

adjust O
brightness O
and O
contrast O
in O
the B-DAT
range O
of O
±10 O

Below O
each O
result, O
we O
give O
the B-DAT
percentage O
of O
inliers O
of O
the O

final O
model, O
the B-DAT
F-score O
which O
measures O
the O
alignment O

true O
fundamental O
ma- O
trix, O
and O
the B-DAT
mean O
epipolar O
error O
of O
estimated O

inlier O
correspondences O
w.r.t. O
the B-DAT
ground O
truth O
fundamental O
matrix. O
We O

green O
if O
they B-DAT
adhere O
to O
the O
ground O
truth O
fundamental O
matrix O
with O

skip O
connection O
[18]. O
We O
use O
the B-DAT
gradient O
blockage O
during O
training O
to O

prevent O
direct O
influence O
of O
the B-DAT
sampling O
prediction O
(second O
branch) O
to O

learning O
the B-DAT
observations O
(first O
branch O

ground O
truth O
horizon O
lines) O
in O
the B-DAT
range O
of O
±5◦/20%/8px O

As O
discussed O
in O
the B-DAT
main O
paper, O
we O
use O
the O

between O
a O
line O
hypothesis B-DAT
and O
the O
ground O
truth O
horizon O
in O
the O

steep O
slope. O
Therefore, O
we O
clamp O
the B-DAT
task O
loss O
to O
a O
maximum O

of O
1, O
i.e. O
the B-DAT
normalized O
image O
height O

mentioned O
before, O
some O
images O
in O
the B-DAT
HLW O
dataset O
[52] O
have O
their O

horizon O
outside O
the B-DAT
image. O
Some O
of O
these O
images O

virtually O
no O
visual O
cue O
where O
the B-DAT
horizon O
exactly O
lies. O
Therefore, O
we O

use O
a O
robust O
variant O
of O
the B-DAT
task O
loss O
`′ O
that O
limits O

the B-DAT
influence O
of O
such O
outliers. O
We O

i.e. O
we O
use O
the B-DAT
square O
root O
of O
the O
task O

magnitude O
of O
0.25, O
which O
is O
the B-DAT
magnitude O
up O
to O
which O
the O

additional O
qualitative O
re- O
sults O
for O
the B-DAT
HLW O
dataset O
[52] O
in O
Fig O

y(w), O
to O
which O
we O
fit O
the B-DAT
6D O
camera O
pose. O
The O
second O

predicts O
sampling O
probabilities O
p(y;w) O
for O
the B-DAT
scene O
coor- O
dinates. O
We O
apply O

a O
Sigmoid O
to O
the B-DAT
output O
of O
the O
second O
branch O

and O
normalize O
by O
dividing O
by O
the B-DAT
sum O
of O
outputs. O
During O
training O

, O
we O
block O
the B-DAT
gradients O
of O
the O
second O
out O

branch O
when O
back O
propagating O
to O
the B-DAT
base O
network. O
The O
sampling O
gradients O

larger O
variance O
and O
magnitude O
than O
the B-DAT
observation O
gradients O
of O
the O
first O

branch, O
especially O
in O
the B-DAT
beginning O
of O
training. O
This O
has O

effect O
on O
con- O
vergence O
of O
the B-DAT
network O
as O
a O
whole. O
Intuitively O

want O
to O
give O
priority O
to O
the B-DAT
scene O
coordinate O
prediction O
because O
they O

determine O
the B-DAT
accuracy O
of O
the O
pose O
estimate. O
The O
sampling O
prediction O

should O
address O
deficiencies O
in O
the B-DAT
scene O
coordi- O
nate O
predictions O
without O

properties. O
Implementation O
details. O
We O
follow O
the B-DAT
three-stage O
train- O
ing O
procedure O
proposed O

Firstly, O
we O
optimize O
the B-DAT
distance O
between O
predicted O
and O
ground O

truth O
scene O
coordinates O
by O
rendering O
the B-DAT
sparse O
reconstructions O
given O
in O
the O

no O
corresponding O
3D O
point O
in O
the B-DAT
reconstruction. O
Since O
the O
reconstructions O
contain O

outlier O
3D O
points, O
we O
use O
the B-DAT
following O
robust O
distance O

each O
input O
image, O
we O
show O
the B-DAT
estimated O
horizon O
line O
in O
blue O

and O
the B-DAT
true O
horizon O
line O
in O
green O

. O
We O
also O
show O
the B-DAT
observation O
points O
predicted O
by O
our O

skip O
connection O
[18]. O
We O
use O
the B-DAT
gradient O
blockage O
during O
training O
to O

prevent O
direct O
influence O
of O
the B-DAT
sampling O
prediction O
(second O
branch) O
to O

learning O
the B-DAT
scene O
coordinates O
(first O
branch O

i.e. O
we O
use O
the B-DAT
Euclidean O
distance O
up O
to O
a O

10m O
after O
which O
we O
use O
the B-DAT
square O
root O
of O
the O
Euclidean O

distance. O
We O
train O
the B-DAT
first O
stage O
for O
500k O
iterations O

Secondly, O
we O
optimize O
the B-DAT
reprojection O
error O
of O
the O
scene O

coordinate O
predictions O
w.r.t. O
to O
the B-DAT
ground O
truth O
camera O
pose. O
Similar O

to O
the B-DAT
first O
stage, O
we O
use O
a O

10px O
after O
which O
we O
use O
the B-DAT
square O
root O
of O
the O
reprojection O

error. O
We O
train O
the B-DAT
second O
stage O
for O
300k O
iterations O

Thirdly, O
we O
optimize O
the B-DAT
expected O
task O
loss O
according O
to O

the B-DAT
NG-DSAC O
objective O
as O
explained O
in O

the B-DAT
main O
paper. O
As O

θ,θ∗)+ O
||t−t∗||2. O
We O
measure O
the B-DAT
angle O
between O
estimated O
camera O
rotation O

θ∗ O
in O
degree. O
We O
measure O
the B-DAT
distance O
between O
the O
estimated O
camera O

τ O
= O
10. O
We O
train O
the B-DAT
third O
stage O
for O
200k O
iterations O

Learned O
3D O
Representations. O
We O
visualize O
the B-DAT
internal O
3D O
scene O
representations O
learned O

Learned O
3D O
Representations. O
We O
visualize O
the B-DAT
internal O
representation O
of O
the O
neural O

NG-DSAC++ O
we O
choose O
randomly O
among O
the B-DAT
top O
1000 O
pixels O
per O
training O

image O
according O
to O
the B-DAT
predicted O
distribution O

the B-DAT

Cheng. O
Complete O
solution O
classification O
for O
the B-DAT
perspective-three-point O
prob- O
lem. O
TPAMI, O
2003 O

I. O
Hartley. O
In O
defense O
of O
the B-DAT
eight-point O
algorithm. O
TPAMI, O
1997. O
3 O

and O
J.-M. O
Frahm. O
Re- O
constructing O
the B-DAT
World* O
in O
Six O
Days O
*(As O

Captured O
by O
the B-DAT
Yahoo O
100 O
Million O
Image O
Dataset O

Nistér. O
An O
efficient O
solution O
to O
the B-DAT
five-point O
relative O
pose O
problem. O
TPAMI O

N. O
Jacobs. O
Horizon O
lines O
in O
the B-DAT
wild. O
In O
BMVC, O
2016. O
8 O

4.3. O
Horizon O
Lines B-DAT

C. O
Horizon O
Lines B-DAT
Network O
Architecture. O
We O
provide O
a O

Figure O
7. O
Horizon B-DAT
Line O
Estimation. O
Left: O
AUC O
on O

4.3. O
Horizon B-DAT
Lines O

under O
the O
Eu- O
ropean O
Unions O
Horizon B-DAT
2020 O
research O
and O
innovation O
pro O

C. O
Horizon B-DAT
Lines O
Network O
Architecture. O
We O
provide O

13. O
NG-DSAC O
Network O
Architecture O
for O
Horizon B-DAT
Line O
Estimation. O
The O
network O
takes O

Figure O
14. O
Qualitative O
Results O
for O
Horizon B-DAT
Line O
Estimation. O
Next O
to O
each O

M. O
Zhai, O
and O
N. O
Jacobs. O
Horizon B-DAT
lines O
in O
the O
wild. O
In O

contrast, O
we O
learn O
hypothesis O
search O
in B-DAT
a O
principled O
fashion O
that O
lets O

us O
to O
train B-DAT
neural O
guidance O
in O
a O
self-supervised O
fashion. O
Secondly, O
we O

RANSAC O
is O
its O
poor O
performance O
in B-DAT
domains O
with O
many O
outliers. O
As O

the O
ratio O
of O
outliers O
in B-DAT

vanilla O
RANSAC O
can O
be O
seen O
in B-DAT
Fig. O
1 O

to O
train B-DAT
the O
neural O
network O
in O
conjunction O
with O
RANSAC O
in O
a O

flexibility O
for O
the O
neural O
network O
in B-DAT
allowing O
a O
certain O
mis- O
classification O

on O
[56, O
36]. O
The O
flexibility O
in B-DAT
choosing O
the O
task O
loss O
also O

idea O
of O
using B-DAT
guided O
sampling O
in O
RANSAC O
is O
not O
new. O
Tordoff O

positive O
affect O
on O
RANSAC O
performance O
in B-DAT
some O
ap- O
plications, O
feature O
matching O

only O
to O
change O
the O
order O
in B-DAT
which O
RANSAC O
draws O
minimal O
sets O

NG-RANSAC O
takes O
a O
different O
approach O
in B-DAT
(i) O
learning O
the O
weights O
to O

and O
(ii) O
integrating B-DAT
RANSAC O
itself O
in O
the O
training O
process O
which O
leads O

neural O
guidance O
can O
be O
used O
in B-DAT
conjunction O
with O
DSAC O
(NG-DSAC) O
to O

Related O
Work O
RANSAC O
was O
introduced B-DAT
in O
1981 O
by O
Fischler O
and O
Bolles O

Since B-DAT
then O
it O
was O
extended O
in O
various O
ways, O
see O
e.g. O
the O

of O
classic O
RANSAC O
variants. O
USAC O
in B-DAT

RANSAC O
is O
not O
often O
used O
in B-DAT
recent O
machine O
learning- O
heavy O
vision O

predict O
image-to-object O
correspondences. O
How- O
ever, O
in B-DAT
most O
of O
these O
works, O
RANSAC O

training. B-DAT
However, O
DSAC O
is O
limited O
in O
which O
derivatives O
can O
be O
calculated O

of O
Yi O
et O
al. O
[56] O
in B-DAT
the O
intro- O
duction. O
Ranftl O
and O

enables O
us O
to O
train B-DAT
NG-RANSAC O
in O
a O
self- O
supervised O
fashion, O
possible O

from O
all O
observations O
will O
result O
in B-DAT
a O
poor O
es- O
timate O
due O

use O
of O
an O
early-stopping B-DAT
strategy O
in O
practice O

not O
a O
continuous B-DAT
dis- O
tribution O
in O
observation O
space. O
We O
wish O
to O

learn O
parameters O
w O
in B-DAT
a O
way O
that O
increases O
the O

minimal B-DAT
sets, O
which O
will O
result O
in O
accurate O
estimates O
ĥ. O
We O
sample O

to O
learn O
the O
distribution O
p(H;w) O
in B-DAT
a O
way O
that O
we O
receive O

function O
` O
do O
not O
appear O
in B-DAT
the O
expression O
above. O
Therefore, O
differentiability O

the O
sampling B-DAT
proba- O
bility O
p(H;w) O
in O
a O
good O
direction. O
Due O
to O

We O
found O
a O
simple O
baseline B-DAT
in O
the O
form O
of O
the O
average O

network O
can O
express O
a O
confidence O
in B-DAT
its O
own O
predic- O
tions O
through O

to O
the O
scoring B-DAT
function. O
As O
in O
NG-RANSAC, O
we O
approxi- O
mate O
the O

can O
calculate O
it O
analytically, O
as O
in B-DAT
DSAC, O
since O
it O
integrates O
over O

discrete O
set O
of O
hypotheses O
hj O
in B-DAT
a O
given O
pool O
H. O
Similar O

in B-DAT
for O
Ej∼p(j|Hk) O
[`(hj)]. O
The O
calculation O

f O
. O
Note O
that O
we O
in B-DAT

epipolar O
geometry O
of O
image O
pairs O
in B-DAT
the O
form O
of O
essential O
matrices O

image O
points B-DAT
x O
and O
x′ O
in O
the O
left O
and O
right O
image O

and O
is O
a O
central O
step O
in B-DAT
image-based O
3D O
reconstruction O
[40]. O
As O

classic O
and O
well- O
researched O
problem O
in B-DAT
computer O
vision O

b) O
We O
use O
side O
information B-DAT
in O
the O
form O
of O
descriptor O
distance O

34]. O
c) O
We O
train B-DAT
NG-RANSAC O
in O
a O
self-supervised O
fashion O
by O
using O

al. O
[56] O
evaluate O
their O
approach O
in B-DAT
outdoor O
as O
well O
as O
indoor O

tween O
the O
pose O
translation O
vectors O
in B-DAT
degrees. O
We O
take O
the O
maximum O

a O
set O
of O
sparse O
correspondences O
in B-DAT
inliers O
and O
outliers. O
They O
represent O

com- O
bining B-DAT
the O
2D O
coordinate O
in O
the O
left O
and O
right O
image O

context O
is O
infused B-DAT
by O
using O
in O

- O
stance O
normalization O
[49] O
in B-DAT

layers. O
We O
re-build O
this O
architecture O
in B-DAT
PyTorch, O
and O
train O
it O
according O

sampling B-DAT
pools. O
For O
testing, O
we O
in O

We O
draw O
final B-DAT
model O
inliers O
in O
green O
if O
they O
adhere O
to O

RANSAC O
incorporates B-DAT
all O
these O
components O
in O
its O
training O
procedure, O
and O
therefore O

to O
test O
a O
robust O
estimator O
in B-DAT
high- O
outlier O
domains. O
However, O
it O

the O
ratio O
filter O
as O
+Ratio O
in B-DAT
Fig. O
2 O
b), O
and O
ob O

same O
hypotheses O
as O
RANSAC O
but O
in B-DAT
a O
different O
order. O
Therefore, O
USAC O

probability O
of O
correspondences O
being B-DAT
inliers O
in O

truth O
model O
(GT O
Inliers, O
drawn O
in B-DAT
green). O
We O
perform O
this O
experiment O

in B-DAT
the O
Indoor O
scenario, O
using O
side O

very O
stable O
training B-DAT
signal, O
even O
in O
the O
beginning O
of O
training O
such O

report O
results O
of O
self-supervised O
NG-RANSAC O
in B-DAT
Fig. O
2 O
c). O
It O
outperforms O

USAC O
[34] O
which O
it O
matches O
in B-DAT
accu- O
racy. O
Unsupervised O
NG-RANSAC O
achieves O

additional O
input B-DAT
to O
the O
network O
in O
the O
next O
iteration. O
The O
network O

similar O
to O
the O
one O
used O
in B-DAT
[56]. O
Correspondences O
are O
represented O
as O

the O
evaluation O
procedure O
de- O
scribed O
in B-DAT
[36] O
and O
compare O
to O
their O

They O
measure O
the O
percentage O
of O
in B-DAT

estimated O
and O
true O
fundamental O
matrix O
in B-DAT
image O
space. O
Both O
metrics O
use O

to O
the O
training B-DAT
setup O
described O
in O
Sec. O
4.1 O
with O
the O
following O

the O
ground O
truth O
hori- O
zon O
in B-DAT
green O
and O
the O
estimate O
in O

Results. O
We O
report O
results O
in B-DAT
Fig. O
6 O
where O
we O
compare O

and O
optimiz- O
ing B-DAT
a O
metric O
in O
training O
generally O
also O
achieves O
good O

can O
serve O
as O
a O
cue O
in B-DAT
image O
understand- O
ing O
[52] O
or O

branch. O
Results. O
We O
report O
results O
in B-DAT
Fig. O
7. O
DSAC O
and O
NG O

designed O
to O
find B-DAT
semantic O
lines O
in O
images. O
SLNet O
generates O
a O
large O

depicting B-DAT
five O
land- O
mark O
buildings2 O
in O
Cambridge, O
UK. O
Ground O
truth O
poses O

age O
pixel O
a O
3D O
coordinate B-DAT
in O
scene O
space. O
We O
recover O
the O

pipeline B-DAT
implements O
geometric O
pose O
optimization O
in O
a O
fully O
differentiable O
way O
which O

We O
report O
our O
quantitative O
results O
in B-DAT
Fig. O
9. O
Firstly, O
we O
observe O

to O
moderate, O
but O
consistent, O
improvement O
in B-DAT
accuracy. O
The O
ad- O
vantage O
of O

large O
areas O
of O
sky O
visible O
in B-DAT
many O
images. O
NG- O
DSAC++ O
learns O

such O
areas, O
see O
the O
visualization O
in B-DAT
Fig. O
8 O
a). O
The O
network O

Network O
Architecture. O
As O
mentioned O
in B-DAT
the O
main O
paper, O
we O
replicated O

set O
of O
feature O
correspondences O
as O
in B-DAT

Procedure. O
We O
initialize B-DAT
our O
network O
in O
the O
following O
way. O
We O
define O

3D O
in B-DAT
homogeneous O
coordinates). O
We O
define O
the O

for O
indoor B-DAT
and O
outdoor O
scenarios O
in O
Fig. O
11. O
We O
com- O
pare O

We O
ob- O
tain B-DAT
these O
results O
in O
the O
high-outlier O
setup, O
i.e. O
without O

NG-RANSAC. O
This O
refinement B-DAT
step O
results O
in O
a O
small O
but O
noticeable O
increase O

in B-DAT
accuracy. O
For O
the O
follow- O
ing O

for O
the O
Kitti O
dataset O
[14] O
in B-DAT
Fig. O
12. O
We O
compare O
re O

architecture O
for O
horizon O
line B-DAT
estimation O
in O
Fig. O
13. O
The O
network O
takes O

re-scale O
output O
points B-DAT
to O
[-1.5,1.5] O
in O
relative O
image O
coordi- O
nates O
to O

area. O
We O
implement O
the O
network O
in B-DAT
a O
fully O
convolutional O
way O
[26 O

of O
the O
first O
branch, O
especially O
in B-DAT
the O
beginning O
of O
training O
with O

rotation O
matrices. O
We O
draw O
correspondences O
in B-DAT
green O
if O
they O
adhere O
to O

prediction O
should O
address O
deficien- O
cies O
in B-DAT
the O
observation O
predictions O
without O
influencing O

softness O
of O
the O
scoring B-DAT
distribution O
in O
DSAC, O
β O
determines O
the O
softness O

domly O
adjust O
brightness O
and O
contrast O
in B-DAT
the O
range O
of O
±10 O

fundamental O
matrix. O
We O
draw O
correspondences O
in B-DAT
green O
if O
they O
adhere O
to O

and O
ground O
truth O
horizon O
lines) B-DAT
in O
the O
range O
of O
±5◦/20%/8px O

As O
discussed O
in B-DAT
the O
main O
paper, O
we O
use O

and O
the O
ground O
truth O
horizon O
in B-DAT
the O
image O
as O
task O
loss O

As O
mentioned O
before, O
some O
images O
in B-DAT
the O
HLW O
dataset O
[52] O
have O

for O
the O
HLW O
dataset O
[52] O
in B-DAT
Fig. O
14 O

work O
architecture O
for O
camera O
re-localization O
in B-DAT
Fig. O
15. O
The O
network O
is O

of O
the O
first O
branch, O
especially O
in B-DAT
the O
beginning O
of O
training. O
This O

sampling B-DAT
prediction O
should O
address O
deficiencies O
in O
the O
scene O
coordi- O
nate O
predictions O

rendering B-DAT
the O
sparse O
reconstructions O
given O
in O
the O
Cambridge O
Landmarks O
dataset O
[22 O

with O
no O
corresponding B-DAT
3D O
point O
in O
the O
reconstruction. O
Since O
the O
reconstructions O

show O
the O
estimated O
horizon O
line B-DAT
in O
blue O
and O
the O
true O
horizon O

line B-DAT
in O
green. O
We O
also O
show O
the O

the O
NG-DSAC O
objective O
as O
explained B-DAT
in O
the O
main O
paper. O
As O

and O
ground O
truth O
rotation O
θ∗ O
in B-DAT
degree. O
We O
measure O
the O
distance O

in B-DAT
meters. O
As O
with O
horizon O
line O

by O
DSAC++ O
and O
NG- O
DSAC++ O
in B-DAT
Fig. O
16 O
for O
two O
more O

A. O
Zisserman. O
Multiple O
View O
Geometry O
in B-DAT
Computer O
Vision. O
Cambridge O
University O
Press O

Frahm. O
Re- O
constructing B-DAT
the O
World* O
in O
Six O
Days O
*(As O
Captured O
by O

A. O
Lerer. O
Auto- O
matic O
differentiation O
in B-DAT
PyTorch. O
In O
NIPS-W, O
2017. O
4 O

approach O
for O
vanishing B-DAT
point O
detection O
in O
architectural O
environments. O
In O
BMVC, O
2002 O

forests O
for O
cam- O
era O
relocalization O
in B-DAT
RGB-D O
images. O
In O
CVPR, O
2013 O

H. O
S. O
Torr. O
Exploiting B-DAT
uncertainty O
in O
regression O
forests O
for O
accurate O
camera O

and O
N. O
Jacobs. O
Horizon O
lines B-DAT
in O
the O
wild. O
In O
BMVC, O
2016 O

points B-DAT
using O
global O
image O
context O
in O
a O
non-manhattan O
world. O
In O
CVPR O

Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT

image. O
These O
methods O
typically O
require O
the B-DAT
image O
to O
contain O
specific O
cues O

evaluation O
dataset, O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW), O
containing O
natural O
images O

Using O
this O
dataset, O
we O
investigate O
the B-DAT
application O
of O
convolutional O
neural O
networks O

for O
directly O
es- O
timating O
the B-DAT
horizon O
line, O
without O
requiring O
any O

we O
achieve O
state-of-the-art B-DAT
results O
on O
the O
challenging O
HLW O
dataset O
and O
two O

line O
estimation O
is O
one O
of O
the B-DAT
most O
fundamental O
geometric O
problems O
in O

computer O
vision. O
Knowledge O
of O
the B-DAT
horizon O
line O
enables O
a O
wide O

this O
problem O
make O
assumptions O
about O
the B-DAT
presence O
of O
particular O
geometric O
objects O

in O
the B-DAT
scene, O
such O
as O
vanishing O
points O

to O
evaluate O
methods O
that O
use O
the B-DAT
orthogonal O
vanishing O
point O
cue, O
contributing O

benchmark O
dataset, O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW), O
containing O
real-world O
images O

network O
(CNN) O
to O
directly O
estimate O
the B-DAT
horizon O
line. O
The O
resulting O
network O

makes O
no O
explicit O
assumptions O
about O
the B-DAT
contents O
of O
the O
underlying O
scene O

the B-DAT

al. O
[11] O
proposed O
to O
segment O
the B-DAT
sky O
and O
then O
detect O
the O

horizon O
line O
in O
the B-DAT
resulting O
binary O
mask. O
This O
approach O

is O
limited O
to O
when O
the B-DAT
horizon O
line O
is O
visible, O
such O

as O
from O
a O
boat O
on O
the B-DAT
ocean O
on O
a O
clear O
day O

a O
segmentation O
approach O
to O
estimate O
the B-DAT
location O
of O
the O
skyline, O
a O

locations, O
but O
they B-DAT
focus O
on O
the O
vanishing O
point O
cue. O
We O
propose O

a O
CNN O
to O
directly O
estimate O
the B-DAT
horizon O
line O
location. O
However, O
we O

context O
for O
their B-DAT
method, O
replacing O
the O
one O
they O
proposed, O
significantly O
improves O

the B-DAT

showcase O
purely O
geometric O
methods, O
and O
the B-DAT
challenging O
HLW O
dataset O

CNN-based O
approach O
for O
directly O
estimating O
the B-DAT
horizon O
line O
in O
a O
single O

Definition O
The O
image O
location O
of O
the B-DAT
horizon O
line O
is O
defined O
as O

the B-DAT
projection O
of O
the O
line O
at O
infinity O
for O
any O

plane O
which O
is O
orthogonal O
to O
the B-DAT
local O
gravity O
vector. O
The O
gravity O

vector O
often O
coincides O
with O
the B-DAT
local O
ground O
plane O
surface O
normal O

always. O
This O
is O
distinct O
from O
the B-DAT
problem O
of O
detecting O
the O
skyline O

, O
which O
is O
the B-DAT
set O
of O
points O
where O
the O

sky O
and O
the B-DAT
ground O
meet O

intrinsic O
parameters. O
A O
point O
in O
the B-DAT
world, O
Xi, O
is O
related O
to O

where O
Rc O
is O
the B-DAT
camera O
orientation, O
tc O
is O
the O

camera O
translation, O
and O
Kc O
is O
the B-DAT
intrinsic O
cali- O
bration. O
For O
our O

camera O
coordinates O
we O
assume O
that O
the B-DAT
positive O
x-direction O
is O
to O
the O

right, O
the B-DAT
positive O
y-direction O
is O
up, O
and O

the B-DAT
viewing O
direction O
is O
down O
the O
negative O
z-axis. O
Using O
this O
parameterization O

, O
the B-DAT
world O
viewing O
direction O
of O
our O

is O
RTc O
[0,0,−1]T. O
Assuming O
that O
the B-DAT
world O
vector O
[0,1,0]T O
points O
in O

the B-DAT
zenith O
direction, O
the O
horizon O
line O
in O
our O
image O

is O
defined O
as O
the B-DAT
set O
of O
pixels, O
p, O
where O

If O
the B-DAT
intrinsic O
calibration, O
Kc, O
of O
the O

camera O
is O
known, O
then B-DAT
the O
horizon O
line O
provides O
a O
sufficient O

set O
of O
constraints O
to O
estimate O
the B-DAT
camera O
tilt O
and O
roll O
in O

We O
introduce O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW), O
a O
large O
dataset O

Figure O
1: O
Montages O
highlighting O
the B-DAT
diversity O
of O
perspectives O
and O
scenes O

work O
on O
estimating O
horizon O
lines: O
the B-DAT
Eurasian O
Cities O
Dataset O
[3] O
(ECD O

) O
and O
the B-DAT
older O
York O
Urban O
Dataset O
[9 O

small O
and O
do O
not O
reflect O
the B-DAT
diversity O
of O
environments O
in O
which O

ECD O
is O
the B-DAT
predominant O
benchmark O
dataset O
used O
for O

of O
which O
do O
not O
satisfy O
the B-DAT
Manhattan O
world O
assumption O
[6], O
i.e O

is O
up. O
Of O
these B-DAT
images, O
the O
first O
25 O
are O
used O
for O

model O
fitting, O
with O
the B-DAT
remainder O
used O
for O
testing. O
Of O

the B-DAT
78 O
testing O
images, O
a O
majority O

Due O
to O
a O
combination O
of O
the B-DAT
few O
number O
of O
testing O
images O

and O
the B-DAT
small O
number O
of O
challenging O
images O

, O
the B-DAT
difference O
in O
per- O
formance O
between O

seen O
as O
too O
easy O
because O
the B-DAT
images O
are O
captured O
in O
a O

relatively O
fewer O
outlier O
line O
segments, O
the B-DAT
scenes O
satisfy O
the O
Manhattan O
world O

a O
manual O
process O
akin O
to O
the B-DAT
following O
was O
used: O
identify O
families O

point O
for O
each, O
and O
compute O
the B-DAT
horizon O
line O
from O
the O
horizontal O

error O
prone, O
and O
severely O
limits O
the B-DAT
diversity O
of O
scenes. O
As O
Lezama O

It O
is O
our O
belief O
that O
the B-DAT
limitations O
of O
these O
datasets O
have O

the B-DAT

For O
example, O
we O
find O
that O
the B-DAT
approach O
of O
Lezama O
et O
al O

using O
code O
made O
available O
by O
the B-DAT
authors). O
These O
methods O
have O
also O

a O
SfM O
model O
to O
estimate O
the B-DAT
horizon O
line. O
(a) O
Each O
point O

represents O
the B-DAT
left/right O
direction O
of O
an O
image O

outlier). O
Two O
vectors O
represent O
the B-DAT
estimated O
horizon O
plane. O
(b) O
The O

projected O
into O
one O
image O
from O
the B-DAT
model O

The O
output O
of O
SfM O
is O
the B-DAT
extrinsic O
and O
intrinsic O
camera O
parameters O

for O
a O
subset O
of O
the B-DAT
input O
images. O
Typically O
these O
images O

translation. O
Since O
our O
focus O
is O
the B-DAT
horizon O
line, O
we O
just O
need O

to O
estimate O
the B-DAT
global O
up O
direction O
(the O
yaw O

of O
the B-DAT
reconstruction O
is O
irrelevant O
to O
our O

commonly O
used O
approach O
to O
estimate O
the B-DAT
global O
orientation O
is O
to O
average O

the B-DAT
image O
‘up’ O
directions O
in O
world O

of O
this O
approach O
is O
that O
the B-DAT
expected O
tilt O
and O
roll O
of O

we O
instead O
only O
assume O
that O
the B-DAT
expected O
roll O
of O
a O
camera O

of O
images, O
we O
solve O
for O
the B-DAT
world O
direction O
of O
the O
points O

at O
infinity O
in O
the B-DAT
left, O
[−1,0,0], O
and O
right, O
[1,0,0 O

to O
estimate O
a O
basis O
for O
the B-DAT
horizon O
plane O
(Figure O
2), O
ignoring O

185 O
high-quality O
SfM O
models O
in O
the B-DAT
1DSfM O
[24], O
Landmarks O
[20], O
and O

each O
model, O
and O
then B-DAT
projected O
the O
horizon O
line O
back O
into O
each O

as O
Mount O
Rushmore, O
Stonehenge, O
and O
the B-DAT
Grand O
Canyon, O
are O
included. O
However O

, O
the B-DAT
dataset O
contains O
few, O
if O
any O

The O
x-axis O
is O
slope O
and O
the B-DAT
y-axis O
is O
vertical O
offset O

We O
first O
use O
the B-DAT
SfM O
models O
to O
learn O
a O

normal O
distribution. O
We O
find O
that O
the B-DAT
camera O
roll O
is O
well O
modeled O

from O
50000 O
panoramas, O
sampled O
from O
the B-DAT
continental O
US O
and O
93 O
metropolitan O

areas O
around O
the B-DAT
world, O
we O
generate O
500000 O
training O

sampling O
square O
cutouts O
based O
on O
the B-DAT
learned O
distributions O

Figure O
4: O
Evaluating O
the B-DAT
recent O
state-of-the-art O
method O
by O
Lezama O

The O
AUC O
is O
shown O
in O
the B-DAT
legend O

ages O
with O
large O
buildings O
in O
the B-DAT
back- O
ground. O
HLW O
also O
has O

ECD O
or O
YUD. O
We O
represent O
the B-DAT
horizon O
line O
as O
ρ O

ysinθ O
, O
where O
ρ O
is O
the B-DAT
perpendicular O
distance O
from O
the O
origin O

to O
the B-DAT
horizon O
line O
and O
θ O
is O

the B-DAT
angle O
the O
horizon O
line O
makes O
with O
the O

horizontal O
axis. O
Figure O
3 O
shows O
the B-DAT
joint O
distribu- O
tion O
over O
θ O

We O
evaluated O
the B-DAT
recent O
state-of- O
the O

for O
horizon O
line O
detection O
is O
the B-DAT
maximum O
distance O
from O
the O
detection O

to O
the B-DAT
ground O
truth O
in O
image O
space O

, O
normalized O
by O
the B-DAT
height O
of O
the O
image, O
which O

a O
set O
of O
images O
as O
the B-DAT
area O
under O
the O
curve O
of O

the B-DAT
cumulative O
histogram O
of O

Barinova O
et O
al. O
[3] O
motivate O
the B-DAT
use O
of O
horizon O
detection O
error O

as O
the B-DAT
standard O
accuracy O
measure O
for O
automatic O

detection O
algorithms. O
Figure O
4 O
visualizes O
the B-DAT
result. O
The O
large O
relative O
performance O

compared O
to O
other B-DAT
benchmarks O
highlights O
the O
challenging O
nature O
of O
the O
HLW O

neural O
networks O
(CNNs) O
to O
estimate O
the B-DAT
location O
of O
the O
horizon O
line O

manual O
tuning O
of O
parameters. O
Importantly, O
the B-DAT
computational O
cost O
only O
depends O
on O

the B-DAT
size O
of O
the O
image, O
not O
the O
content O
of O

the B-DAT
scene, O
such O
as O
the O
number O
of O
line O
segments. O
Our O

have O
a O
significant O
impact O
on O
the B-DAT
accuracy O
of O
the O
resulting O
model O

For O
all O
experiments O
we O
use O
the B-DAT
GoogleNet O
architecture O
[21] O
because O
it O

fewer O
parameters. O
Our O
CNNs O
expect O
the B-DAT
input O
images O
to O
have O
a O

3.3). O
We O
experimented O
with O
reshaping O
the B-DAT
image O
to O
be O
square, O
but O

the B-DAT
resulting O
networks O
were O
far O
less O

We O
consider O
two O
parameterizations O
of O
the B-DAT
horizon O
line: O
1) O
slope/offset, O
(θ O

ρ), O
where O
ρ O
is O
the B-DAT
perpendicular O
distance O
from O
the O
origin O

to O
the B-DAT
horizon O
line O
and O
θ O
is O

the B-DAT
angle O
the O
horizon O
line O
makes O
with O
the O

x-axis O
of O
the B-DAT
image O
and O
2) O
left/right, O
(l,r O

), O
where O
l O
is O
the B-DAT
vertical O
offset O
at O
which O
the O

horizon O
line O
intersects O
the B-DAT
left O
side O
of O
the O
image O

two O
CNN O
variants O
for O
predicting O
the B-DAT
horizon O
line O
location O

such O
a O
formulation O
is O
that O
the B-DAT
output O
of O
a O
CNN O
trained O

is O
a O
probability O
distribution O
over O
the B-DAT
categories; O
in O
our O
case O
a O

over O
possible O
horizon O
lines O
in O
the B-DAT
image. O
For O
each O
parameter O
we O

100 O
bins O
by O
linearly O
interpolating O
the B-DAT
cumulative O
distribu- O
tion O
function O
of O

that O
parameter O
over O
the B-DAT
training O
data. O
Additionally O
for O
slope O

, O
θ O
, O
we O
force O
the B-DAT
bin O
edges O
to O
be O
symmetric O

Our O
process O
for O
adapting O
the B-DAT
GoogleNet O
architecture O
is O
as O
follows O

parameter O
and O
then B-DAT
2) O
modify O
the O
fully O
connected O
layer O
for O
each O

a O
N-dimensional O
vector O
corresponding O
to O
the B-DAT
N O
bins O

due O
to O
difficulties O
in O
controlling O
the B-DAT
optimization O
process O
and O
handling O
outliers O

optimization O
is O
typically O
performed O
using O
the B-DAT
L2 O
loss, O
but O
outliers O
reduce O

the B-DAT
generalization O
ability O
of O
the O
network O
and O
increase O
the O
convergence O

Girshick O
[12] O
note O
that O
if O
the B-DAT
regression O
targets O
are O
unbounded, O
training O

with O
the B-DAT
L2 O
loss O
can O
require O
careful O

our O
regression O
networks O
we O
minimize O
the B-DAT
Huber O
loss O
[15], O
a O
robust O

δ O
= O
1. O
To O
adapt O
the B-DAT
GoogleNet O
architecture O
for O
regressing O
the O

for O
each O
parameter) O
and O
modify O
the B-DAT
corresponding O
fully O
connected O
layer O
to O

results O
show O
that O
optimization O
using O
the B-DAT
Huber O
loss O
results O
in O
more O

accurate O
predic- O
tions O
than O
using O
the B-DAT
L2 O
loss. O
However, O
using O
only O

strate- O
gies: O
1) O
initializing O
from O
the B-DAT
weights O
of O
a O
previously O
trained O

network, O
with O
shared O
weights, O
where O
the B-DAT
softmax O
classifiers O
act O
as O
a O

convergence O
time, O
even O
when O
using O
the B-DAT
L2 O
loss O

When O
applied O
to O
classification O
problems, O
the B-DAT
standard O
procedure O
for O
processing O
an O

multiple O
subwindows, O
feed O
each O
through O
the B-DAT
network O
separately, O
and O
average O
the O

This O
strategy O
is O
applicable O
to O
the B-DAT
problem O
of O
object O
recogni- O
tion O

, O
where O
the B-DAT
target O
label O
is O
shared O
across O

a O
unique O
target O
label O
(as O
the B-DAT
horizon O
line O
position O
changes). O
Therefore O

for O
aggregating O
estimates: O
1) O
projecting O
the B-DAT
horizon O
line O
from O
the O
subwindow O

to O
the B-DAT
full-size O
image O
and O
averaging O
in O

image O
space O
(weighted O
by O
the B-DAT
con- O
fidence O
in O
each O
estimate O

), O
and O
2) O
optimizing O
for O
the B-DAT
horizon O
line O
in O
the O
full O

likely O
in O
all O
subwindows. O
For O
the B-DAT
latter, O
we O
assume O
that O
each O

is O
inde- O
pendent O
and O
minimize O
the B-DAT
negative O
log-likelihood O

is O
a O
function O
that O
maps O
the B-DAT
global O
horizon O
line, O
Θ, O
into O

the B-DAT
coordinate O
frame O
for O
subwindow O
Ii O

, O
and O
extracts O
the B-DAT
probability. O
Our O
results O
show O
that O

only O
a O
center O
crop, O
but O
the B-DAT
averaging O
strategy O
is O
faster O

for O
horizon O
line O
estimation, O
on O
the B-DAT
HLW, O
YUD, O
and O
ECD O
datasets O

the B-DAT

4.1 O
Implementation O
Details O
We O
implemented O
the B-DAT
proposed O
networks O
using O
the O
Caffe O

solver O
settings, O
is O
available O
on O
the B-DAT
project O
website O
[1]. O
We O
trained O

approximately O
35 O
epochs). O
We O
set O
the B-DAT
base O
learning O
rates O
to O
10−3 O

training O
from O
scratch, O
we O
use O
the B-DAT
GoogleNet O
quick O
solver O
[16]). O
We O

snapshot O
every O
1000 O
iterations, O
selecting O
the B-DAT
snapshot O
that O
minimizes O
horizon O
error O

on O
the B-DAT
HLW O
validation O
set. O
The O
input O

We O
combined O
the B-DAT
HLW O
and O
street-side O
imagery O
to O

form O
a O
training O
set. O
For O
the B-DAT
HLW O
imagery, O
we O
performed O
data O

augmentation O
by O
randomly O
mirroring O
the B-DAT
image O
horizontally O
with O
50% O
probability O

minimum O
side O
length O
85% O
of O
the B-DAT
small- O
est O
image O
dimension). O
We O

crops O
from O
each O
image, O
adjusting O
the B-DAT
horizon O
line O
for O
each. O
Since O

the B-DAT
street-side O
imagery O
was O
already O
square O

orientations, O
we O
just O
scaled O
to O
the B-DAT
input O
size O
of O
the O
network O

practice O
to O
start O
optimization O
from O
the B-DAT
weights O
of O
a O
previously O
trained O

network O
[27] O
and O
“fine-tune” O
(updating O
the B-DAT
weights O
of O
randomly O
ini- O
tialized O

models O
made O
publicly O
available O
by O
the B-DAT
authors. O
The O
accuracy O
of O
each O

in O
in O
Table O
1, O
where O
the B-DAT
leftmost O
column O
represents O
which O
network O

horizon O
detection O
error O
and O
report O
the B-DAT
area O
under O
the O
curve O
of O

the B-DAT
cumulative O
histogram O
of O
errors. O
For O

competitive O
performance O
on O
HLW, O
but O
the B-DAT
choice O
of O
initialization O
is O
significant O

and O
we O
found O
the B-DAT
(θ O
,ρ) O
parameterization O
to O
be O

this O
network O
as O
‘Best’ O
in O
the B-DAT
remainder O
of O
the O
table). O
Overall O

performance O
is O
lower O
on O
the B-DAT
test O
imagery O
from O
the O
held O

out O
models O
(held), O
compared O
to O
the B-DAT
full O
set O
(all). O
This O
result O

scene-specific O
camera O
relocalization O
[17] O
demonstrating O
the B-DAT
capability O
of O
a O
CNN O
to O

to O
obtain O
good O
results O
for O
the B-DAT
regression O
task. O
Fine-tuning O
performed O
much O

tweaking. O
Despite O
this, O
we O
found O
the B-DAT
(l,r) O
parameterization O
to O
be O
superior O

, O
and O
the B-DAT
Huber O
loss O
to O
be O
significantly O

better O
than O
the B-DAT
L2 O
loss. O
Applying O
the O
strategies O

Section O
3.2, O
namely O
initializing O
from O
the B-DAT
weights O
of O
the O
best O
classification O

with O
softmax O
classifiers, O
significantly O
improves O
the B-DAT
performance O
of O
our O
networks, O
making O

best O
classification O
network O
we O
evaluate O
the B-DAT
subwindow O
aggregation O
methods O
from O
Section O

of O
crops O
(each O
99% O
of O
the B-DAT
minimum O
dimension), O
chosen O
empirically O
using O

the B-DAT
HLW O
validation O
set. O
We O
saw O

Figure O
5: O
Example O
results O
showing O
the B-DAT
estimated O
distribution O
over O
horizon O
lines O

. O
For O
each O
image, O
the B-DAT
ground O
truth O
horizon O
line O
(dash O

green) O
and O
the B-DAT
predicted O
horizon O
line O
(magenta) O
are O

transparent O
= O
less O
likely) O
shows O
the B-DAT
estimated O
distribution O
over O
the O
point O

on O
the B-DAT
horizon O
line O
closest O
to O
the O

over O
a O
network O
evaluated O
on O
the B-DAT
center O
crop O
alone O

To O
highlight O
the B-DAT
ability O
of O
our O
networks, O
we O

update O
the B-DAT
recent O
state-of-the-art O
method O
by O
Zhai O

network O
(using O
code O
provided O
by O
the B-DAT
authors). O
With O
this O
change, O
we O

mance O
on O
HLW O
and O
advance O
the B-DAT
state-of-the-art O
results O
on O
both O
the O

relative O
improvement O
of O
5.0%. O
Despite O
the B-DAT
limitations O
of O
these O
two O
benchmark O

We O
introduced O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW), O
a O
new O
dataset O

horizon O
line O
estimation, O
to O
address O
the B-DAT
limitations O
of O
existing O
horizon O
line O

and O
wasn’t O
constructed O
to O
highlight O
the B-DAT
value O
of O
any O
particular O
geometric O

on O
this O
important O
problem O
in O
the B-DAT
future O

investigated O
methods O
for O
directly O
estimating O
the B-DAT
horizon O
line O
using O
con- O
volutional O

make O
explicit O
geometric O
assumptions O
on O
the B-DAT
contents O
of O
the O
underlying O
scene O

cues O
that O
are O
present O
in O
the B-DAT
image. O
Despite O
this O
generality, O
the O

the B-DAT

outperforming O
all O
existing O
methods O
on O
the B-DAT
challenging O
real-world O
imagery O
contained O
in O

research O
was O
sup- O
ported O
by O
the B-DAT
Intelligence O
Advanced O
Research O
Projects O
Activity O

contained O
herein O
are O
those O
of O
the B-DAT
authors O
and O
should O
not O
be O

interpreted O
as O
necessarily O
representing O
the B-DAT
official O
policies O
or O
endorsements, O
either O

implied, O
of O
IARPA, O
AFRL, O
or O
the B-DAT
U.S. O
Government O

and O
Jan-Michael O
Frahm. O
Recon- O
structing O
the B-DAT
world* O
in O
six O
days O
*(as O

captured O
by O
the B-DAT
yahoo O
100 O
million O
image O
dataset O

Horizon O
Lines O
in O
the O
Wild B-DAT

dataset, O
Horizon O
Lines O
in O
the O
Wild B-DAT
(HLW), O
containing O
natural O
images O
with O

dataset, O
Horizon O
Lines O
in O
the O
Wild B-DAT
(HLW), O
containing O
real-world O
images O
with O

introduce O
Horizon O
Lines O
in O
the O
Wild B-DAT
(HLW), O
a O
large O
dataset O
of O

introduced O
Horizon O
Lines O
in O
the O
Wild B-DAT
(HLW), O
a O
new O
dataset O
for O

1] O
Horizon O
Lines O
in O
The O
Wild B-DAT
project O
website. O
http://hlw.csr.uky.edu O

Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT

large, O
realistic O
evaluation O
dataset, O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW), O
containing O

a O
new O
benchmark O
dataset, O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW), O
containing O

Line O
Detection O
We O
introduce O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW), O
a O

We O
introduced O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW), O
a O

References O
[1] O
Horizon O
Lines B-DAT
in O
The O
Wild O
project O
website O

Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT

a O
large, O
realistic O
evaluation O
dataset, O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW), O
containing O
natural O
images O
with O

introduce O
a O
new O
benchmark O
dataset, O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW), O
containing O
real-world O
images O
with O

Horizon O
Line O
Detection O
We O
introduce O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW), O
a O
large O
dataset O
of O

We O
introduced O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW), O
a O
new O
dataset O
for O

Horizon B-DAT
Lines O
in O
the O
Wild O

a O
large, O
realistic O
evaluation O
dataset, O
Horizon B-DAT
Lines O
in O
the O
Wild O
(HLW O

introduce O
a O
new O
benchmark O
dataset, O
Horizon B-DAT
Lines O
in O
the O
Wild O
(HLW O

1.1 O
Horizon B-DAT
Line: O
Geometric O
Definition O
The O
image O

2 O
A O
New O
Dataset O
for O
Horizon B-DAT
Line O
Detection O
We O
introduce O
Horizon O

3 O
Direct O
Horizon B-DAT
Line O
Estimation O

We O
introduced O
Horizon B-DAT
Lines O
in O
the O
Wild O
(HLW O

References O
[1] O
Horizon B-DAT
Lines O
in O
The O
Wild O
project O

WORKMAN, O
ZHAI, O
JACOBS: O
HORIZON B-DAT
LINES I-DAT
IN I-DAT
THE I-DAT
WILD I-DAT
1 O

2 O
WORKMAN, O
ZHAI, O
JACOBS: O
HORIZON B-DAT
LINES I-DAT
IN I-DAT
THE I-DAT
WILD I-DAT

WORKMAN, O
ZHAI, O
JACOBS: O
HORIZON B-DAT
LINES I-DAT
IN I-DAT
THE I-DAT
WILD I-DAT
3 O

4 O
WORKMAN, O
ZHAI, O
JACOBS: O
HORIZON B-DAT
LINES I-DAT
IN I-DAT
THE I-DAT
WILD I-DAT

WORKMAN, O
ZHAI, O
JACOBS: O
HORIZON B-DAT
LINES I-DAT
IN I-DAT
THE I-DAT
WILD I-DAT
5 O

6 O
WORKMAN, O
ZHAI, O
JACOBS: O
HORIZON B-DAT
LINES I-DAT
IN I-DAT
THE I-DAT
WILD I-DAT

WORKMAN, O
ZHAI, O
JACOBS: O
HORIZON B-DAT
LINES I-DAT
IN I-DAT
THE I-DAT
WILD I-DAT
7 O

8 O
WORKMAN, O
ZHAI, O
JACOBS: O
HORIZON B-DAT
LINES I-DAT
IN I-DAT
THE I-DAT
WILD I-DAT

WORKMAN, O
ZHAI, O
JACOBS: O
HORIZON B-DAT
LINES I-DAT
IN I-DAT
THE I-DAT
WILD I-DAT
9 O

10 O
WORKMAN, O
ZHAI, O
JACOBS: O
HORIZON B-DAT
LINES I-DAT
IN I-DAT
THE I-DAT
WILD I-DAT

WORKMAN, O
ZHAI, O
JACOBS: O
HORIZON B-DAT
LINES I-DAT
IN I-DAT
THE I-DAT
WILD I-DAT
11 O

12 O
WORKMAN, O
ZHAI, O
JACOBS: O
HORIZON B-DAT
LINES I-DAT
IN I-DAT
THE I-DAT
WILD I-DAT

Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT

realistic O
evaluation O
dataset, O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW), O
containing O
natural O

that O
using B-DAT
our O
CNNs, O
either O
in O
isolation O
or O
in O
conjunction O
with O

the O
most O
fundamental O
geometric O
problems O
in B-DAT
computer O
vision. O
Knowledge O
of O
the O

detection O
[14], O
and O
perspective O
correction O
in B-DAT
consumer O
photographs O
[18]. O
Despite O
this O

presence O
of O
particular O
geometric O
objects O
in B-DAT
the O
scene, O
such O
as O
vanishing O

new O
benchmark O
dataset, O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW), O
containing O
real-world O

may O
be O
distributed O
unchanged O
freely O
in B-DAT
print O
or O
electronic O
forms O

then O
detect O
the O
horizon O
line B-DAT
in O
the O
resulting O
binary O
mask. O
This O

directly O
estimating B-DAT
the O
horizon O
line O
in O
a O
single O
image, O
and O
4 O

and O
intrinsic B-DAT
parameters. O
A O
point O
in O
the O
world, O
Xi, O
is O
related O

to O
a O
pixel, O
pci, O
in B-DAT
a O
camera, O
c, O
as O
follows O

the O
world O
vector O
[0,1,0]T O
points B-DAT
in O
the O
zenith O
direction, O
the O
horizon O

line B-DAT
in O
our O
image O
is O
defined O
as O

the O
camera O
tilt O
and O
roll O
in B-DAT
world O
coordinates O

Detection O
We O
introduce B-DAT
Horizon O
Lines O
in O
the O
Wild O
(HLW), O
a O
large O

with O
labeled O
horizon O
lines, B-DAT
captured O
in O
a O
diverse O
set O
of O
environments O

We O
begin B-DAT
by O
characterizing O
limitations O
in O
existing O
datasets O
for O
evaluating O
horizon O

diversity O
of O
perspectives O
and O
scenes O
in B-DAT
HLW O

datasets O
that O
have O
been O
used O
in B-DAT
recent O
work O
on O
estimating O
horizon O

reflect O
the O
diversity O
of O
environments O
in B-DAT
which O
real-world O
horizon O
line O
detection O

of O
103 O
outdoor O
images O
captured O
in B-DAT
large O
urban O
ar- O
eas, O
many O

of O
challenging B-DAT
images, O
the O
difference O
in O
per- O
formance O
between O
various O
methods O

because O
the O
images O
are O
captured O
in B-DAT
a O
confined O
area O
with O
a O

even O
a O
duplicated O
testing B-DAT
image O
in O
ECD, O
with O
each O
instance O
having O

datasets O
have O
caused O
useful O
progress O
in B-DAT
this O
research O
area O
to O
stagnate O

left/right O
direction O
of O
an O
image O
in B-DAT
world O
coordinates O
(blue O
= O
outlier O

average O
the O
image O
‘up’ O
directions O
in B-DAT
world O
coordinates. O
The O
implicit O
assumption O

zero. O
While O
this O
works O
well O
in B-DAT
many O
cases, O
it O
fails O
in O

one O
direction O
(e.g., O
Notre O
Dame O
in B-DAT
Paris). O
In O
practice, O
we O
found O

of O
the O
points B-DAT
at O
infinity O
in O
the O
left, O
[−1,0,0], O
and O
right O

from O
185 O
high-quality O
SfM O
models O
in B-DAT
the O
1DSfM O
[24], O
Landmarks O
[20 O

out O
two O
models O
completely, O
resulting B-DAT
in O
2018 O
images O
to O
be O
used O

tourist O
landmarks, O
which O
are O
usually O
in B-DAT
urban O
areas. O
Images O
of O
more O

of O
horizon O
lines B-DAT
for O
images O
in O
HLW O
versus O
other O
benchmark O
datasets O

x-axis). O
The O
AUC O
is O
shown O
in B-DAT
the O
legend O

HLW O
and O
ECD O
are O
shown O
in B-DAT
Fig- O
ure O
1. O
Even O
when O

greater O
diversity O
of O
scene O
types O
in B-DAT
HLW O
(e.g., O
zoomed O
in O
view O

of O
a O
city). O
The O
scenes O
in B-DAT
ECD O
consist O
primarily O
of O
urban O

im- O
ages O
with O
large O
buildings B-DAT
in O
the O
back- O
ground. O
HLW O
also O

detection O
to O
the O
ground O
truth O
in B-DAT
image O
space, O
normalized O
by O
the O

less O
accurate. O
This O
result O
is O
in B-DAT
line O
with O
previous O
work O
[25 O

ρ O
, O
l, O
and O
r O
in B-DAT
units O
of O
image O
heights. O
The O

probability O
distribution O
over O
the O
categories; O
in B-DAT
our O
case O
a O
distribution O
over O

possible O
horizon O
lines B-DAT
in O
the O
image. O
For O
each O
parameter O

than O
classification O
due O
to O
difficulties O
in B-DAT
controlling O
the O
optimization O
process O
and O

using B-DAT
the O
Huber O
loss O
results O
in O
more O
accurate O
predic- O
tions O
than O

the O
full-size O
image O
and O
averaging B-DAT
in O
image O
space O
(weighted O
by O
the O

con- O
fidence O
in B-DAT
each O
estimate), O
and O
2) O
optimizing O

for O
the O
horizon O
line B-DAT
in O
the O
full O
image O
that O
is O

maximally O
likely O
in B-DAT
all O
subwindows. O
For O
the O
latter O

By O
using B-DAT
our O
networks, O
either O
in O
isolation O
or O
in O
conjunction O
with O

more). O
We O
apply O
this O
strategy, O
in B-DAT
conjunction O
with O
our O
methods O
outlined O

in B-DAT
Section O
3, O
starting O
from O
a O

several O
datasets O
can O
be O
seen O
in B-DAT
in O
Table O
1, O
where O
the O

As O
in B-DAT
Section O
2.4, O
we O
compute O
horizon O

to O
this O
network O
as O
‘Best’ O
in B-DAT
the O
remainder O
of O
the O
table O

Applying B-DAT
the O
strategies O
out- O
lined O
in O
Section O
3.2, O
namely O
initializing O
from O

from O
our O
approach O
are O
shown O
in B-DAT
Figure O
5 O
for O
four O
ECD O

3.3. O
The O
results O
are O
shown O
in B-DAT
Table O
2. O
In O
addition O
to O

semantic O
image O
classification. O
Both O
averaging B-DAT
in O
image O
space O
(average) O
and O
optimizing O

For O
ECD, O
our O
relative O
improvement O
in B-DAT
AUC O
is O
5.3%. O
For O
YUD O

We O
introduced B-DAT
Horizon O
Lines O
in O
the O
Wild O
(HLW), O
a O
new O

advances O
on O
this O
important O
problem O
in B-DAT
the O
future O

semantic O
cues O
that O
are O
present O
in B-DAT
the O
image. O
Despite O
this O
generality O

the O
challenging B-DAT
real-world O
imagery O
contained O
in O
HLW. O
Our O
method O
is O
fast O

, O
works O
in B-DAT
natural O
environments, O
and O
can O
provide O

References O
[1] O
Horizon O
Lines B-DAT
in O
The O
Wild O
project O
website. O
http://hlw.csr.uky.edu O

Kohli. O
Geometric O
im- O
age O
parsing B-DAT
in O
man-made O
environments. O
In O
ECCV, O
2010 O

methods O
for O
estimating B-DAT
manhattan O
frames O
in O
urban O
imagery. O
In O
ECCV, O
2008 O

Frahm. O
Recon- O
structing B-DAT
the O
world* O
in O
six O
days O
*(as O
captured O
by O

and O
Martial O
Hebert. O
Putting B-DAT
objects O
in O
perspective. O
IJCV, O
80(1):3–15, O
2008 O

vanishing B-DAT
points O
via O
point O
alignments O
in O
image O
primal O
and O
dual O
domains O

Lipson. O
How O
transferable O
are O
features O
in B-DAT
deep O
neural O
networks? O
In O
NIPS O

points B-DAT
using O
global O
image O
context O
in O
a O
non-manhattan O
world. O
In O
CVPR O

horizontal O
van- O
ishing O
points O
and O
the B-DAT
zenith O
vanishing O
point O
in O
man-made O

and O
score O
each O
based O
on O
the B-DAT
vanishing O
points O
it O
contains. O
A O

element O
of O
our O
approach O
is O
the B-DAT
use O
of O
global O
image O
context O

deep O
convolutional O
network, O
to O
constrain O
the B-DAT
set O
of O
candidates O
under O
consider O

benchmark O
datasets O
and O
achieve O
state-of- O
the B-DAT

approach O
is O
significantly O
faster O
than O
the B-DAT
previous O
best O
method O

detec- O
tion O
are O
two O
of O
the B-DAT
most O
fundamental O
problems O
in O
geometric O

Knowledge O
of O
these B-DAT
quantities O
is O
the O
foundation O
for O
many O
higher O
level O

A O
vanishing O
point O
results O
from O
the B-DAT
intersection O
of O
projec- O
tions O
of O

set O
of O
parallel O
lines O
in O
the B-DAT
world. O
In O
man-made O
en- O
vironments O

lines O
are O
often O
caused O
by O
the B-DAT
edges O
of O
buildings, O
roads, O
and O

are O
numerous O
methods O
to O
estimate O
the B-DAT
horizon O
line. O
Therefore, O
previous O
approaches O

problem O
focus O
on O
first O
detecting O
the B-DAT
vanishing O
points, O
which O
is O
a O

red O
means O
high O
score), O
and O
the B-DAT
true O
horizon O
line O
(green O
dash O

algorithm O
is O
very O
close O
to O
the B-DAT
true O
horizon O
line O
(green O
dash O

are O
color O
coded O
based O
on O
the B-DAT
most O
consistent O
detected O
van- O
ishing O

them, B-DAT
and O
keep O
the O
best O
(Fig. O
1). O
We O
use O

global O
image O
context O
and O
guide O
the B-DAT
generation O
of O
a O
set O
of O

candidate O
line O
is O
based O
on O
the B-DAT
consistency O
of O
the O
lines O
in O

the B-DAT
image O
with O
the O
selected O
vanishing O
points O

shift O
in O
approach O
leads O
to O
the B-DAT
need O
for O
novel O
algorithms O
and O

has O
excellent O
performance. O
We O
evaluated O
the B-DAT
proposed O
approach O
on O
two O
standard O

bench- O
mark O
datasets, O
the B-DAT
Eurasian O
Cities O
Dataset O
[5] O
and O

the B-DAT
York O
Urban O
Dataset O
[11]. O
To O

our O
knowledge, O
our O
approach O
has O
the B-DAT
current O
best O
performance O
on O
both O

further, B-DAT
we O
also O
compare O
with O
the O
previous O
state-of-the-art O
method O
(Lezama O
et O

a O
recently O
introduced O
dataset O
[32]; O
the B-DAT
results O
shows O
that O
our O
method O

extract O
line O
segments; O
3) O
identify O
the B-DAT
zenith O
VP O
(Sec. O
4.1); O
4 O

horizon O
line O
candidates O
consistent O
with O
the B-DAT
zenith O
VP O
(Sec. O
4.2); O
5 O

Sec. O
4.2); O
and O
6) O
select O
the B-DAT
best O
horizon O
line O
based O
on O

the B-DAT
VPs O
it O
contains O
(Sec. O
4.3 O

the B-DAT
form O
of O
constraints O
on O
possible O

and O
4) O
an O
evaluation O
of O
the B-DAT
proposed O
approach O
on O
three O
benchmark O

Vanishing O
points O
and O
the B-DAT
horizon O
line O
provide O
a O
strong O

et O
al. O
[13] O
show O
how O
the B-DAT
horizon O
line O
improves O
the O
accuracy O

vide O
a O
brief O
overview O
of O
the B-DAT
main O
approaches, O
refer O
to O
[26 O

of O
methods O
exist, O
distinguished O
by O
the B-DAT
features O
they O
use. O
The O
first O

33] O
build O
on O
top O
of O
the B-DAT
closely O
re- O
lated O
problem O
of O

is O
most O
closely O
related O
to O
the B-DAT
latter O
category, O
so O
we O
focus O

line O
segments O
is O
to O
cluster O
the B-DAT
line O
segments O
that O
pass O
through O

the B-DAT
same O
location. O
Various O
methods O
of O

RANSAC O
[7], O
J-linkage O
[27], O
and O
the B-DAT
Hough O
transform O
[14]. O
Once O
the O

between O
such O
meth- O
ods O
is O
the B-DAT
choice O
of O
point O
and O
line O

a O
statistical O
estimation O
problem O
on O
the B-DAT
Gaussian O
Sphere, O
which O
is O
similar O

to O
the B-DAT
geometry O
we O
use. O
More O
re O

- O
cent O
work O
has O
explored O
the B-DAT
use O
of O
dual O
space O
[19 O

, O
35] O
repre- O
sentations. O
Among O
the B-DAT
clustering-based O
approaches, O
Xu O
et O
al O

function O
that O
models O
errors O
in O
the B-DAT
line O
seg- O
ment O
extraction O
step O

Barinova O
et O
al. O
[5] O
solve O
the B-DAT
problem O
in O
a O
unified O
framework O

Our O
approach O
is O
motivated O
by O
the B-DAT
fact O
that O
properties O
of O
the O

nonsensical O
ways O
and O
2) O
identifying O
the B-DAT
true O
vanishing O
points O
for O
many O

and O
computationally O
expensive O
due O
to O
the B-DAT
large O
number O
of O
outlier O
line O

context O
to O
estimate O
priors O
over O
the B-DAT
horizon O
line O
and O

the B-DAT
zenith O
vanishing O
point O
(Sec. O
3 O

that O
samples O
horizon O
lines O
from O
the B-DAT
prior O
and O
performs O
a O
fast O

are O
essential O
for O
accurate O
results: O
the B-DAT
prior O
helps O
ensure O
a O
good O

this O
work O
is O
to O
detect O
the B-DAT
horizon O
line, O
the O

of O
this O
section O
de- O
fines O
the B-DAT
notation O
and O
basic O
geometric O
facts O

points O
in O
world O
coordinates O
or O
the B-DAT
image O
plane O
and O
bolded O
letters O

coordinates. O
We O
primar- O
ily O
follow O
the B-DAT
notation O
convention O
of O
Vedaldi O
and O

a O
point O
(u, O
v) O
in O
the B-DAT
image O
plane, O
its O
homogeneous O
coordinate O

with O
respect O
to O
the B-DAT
calibrated O
image O
plane O
is O
de O

scale O
constant, O
(cu, O
cv) O
is O
the B-DAT
camera O
principal O
point O
in O
the O

which O
we O
assume O
to O
be O
the B-DAT
cen- O
ter O
of O
the O
image O

, O
and O
Σ O
is O
the B-DAT
constant O
that O
makes O
p O
a O

vectors O
(Fig. O
3). O
Comput- O
ing O
the B-DAT
line, O
l, O
that O
passes O
through O

two O
points, O
(p1,p2), O
and O
the B-DAT
point, O
p, O
at O
the O
intersection O

We O
denote O
the B-DAT
smallest O
angle O
between O
two O
vectors O

We O
use O
this O
to O
define O
the B-DAT
con- O
sistency O
between O
a O
line O

it O
is O
possible O
to O
extend O
the B-DAT
line O
segment O
to O
con- O
tain O

the B-DAT
vanishing O
point O

We O
parameterize O
the B-DAT
horizon O
line O
by O
its O
slope O

0, O
inf), O
which O
is O
the B-DAT
shortest O

red O
lines) O
are O
defined O
by O
the B-DAT
normal O
(red O
arrow) O
of O
the O

red O
triangle) O
they B-DAT
form O
with O
the O
origin O
(green O
dot). O
Two O
lines O

distance O
between O
the B-DAT
horizon O
line O
and O
the O
principal O

point. O
In O
order O
to O
span O
the B-DAT
entire O
horizon O
line O
parameter O
space O

o O
from O
pixel O
coordinates O
to O
the B-DAT
interval O
[0, O
π/2), O
through O
a O

factor O
that O
affects O
how O
dense O
the B-DAT
sampling O
is O
near O
the O
center O

of O
the B-DAT
image O

For O
our O
task, O
we O
adapt O
the B-DAT
popular O
AlexNet O
[18] O
archi- O
tecture O

object O
recognition O
as O
part O
of O
the B-DAT
ImageNet O
ILSVRC-2012 O
challenge O
[24]. O
It O

A O
softmax O
is O
applied O
to O
the B-DAT
final O
output O
layer O
to O
produce O

We O
modify O
the B-DAT
original O
AlexNet O
architecture O
in O
the O

classifi- O
cation O
[36]. O
We O
remove O
the B-DAT
original O
fully O
connected O
lay- O
ers O

α O
and O
w. O
We O
convert O
the B-DAT
slope, O
α, O
and O
the O
squashed O

500 O
bins. O
We O
randomly O
initialize O
the B-DAT
weights O
for O
these O
new O
layers O

function. O
The O
learning O
rates O
for O
the B-DAT
convolutional O
layers O
are O
progressively O
increased O

such O
that O
the B-DAT
latter O
layers O
change O
more. O
The O

Sec. O
3.2), O
each O
overlaid O
with O
the B-DAT
ground-truth O
horizon O
line O

in O
large O
metropolitan O
cities O
around O
the B-DAT
world. O
We O
identified O
a O
set O

5km O
re- O
gion O
around O
the B-DAT
city O
center. O
This O
resulted O
in O

Here O
yaw O
is O
relative O
to O
the B-DAT
Google O
Street O
View O
capture O
vehicle O

selected O
empir- O
ically O
to O
match O
the B-DAT
distribution O
of O
images O
captured O
by O

casual O
photographers O
in O
the B-DAT
wild O

Given O
the B-DAT
FOV, O
pitch, O
and O
roll O
of O

it O
is O
straightforward O
to O
compute O
the B-DAT
horizon O
line O
position O
in O
image O

from O
our O
dataset O
annotated O
with O
the B-DAT
ground-truth O
horizon O
line O

3.3. O
Making O
the B-DAT
Output O
Continuous O

Given O
an O
image, O
I O
, O
the B-DAT
network O
outputs O
a O
categorical O
probability O

distribution O
for O
the B-DAT
slope, O
α, O
and O
squashed O
off O

For O
each, O
we O
es- O
timate O
the B-DAT
mean O
and O
variance O
from O
5 O

000 O
samples O
generated O
from O
the B-DAT
categorical O
probability O
distribution. O
Since O
the O

and O
p(o|I), O
are O
used O
in O
the B-DAT
next O
step O
of O
our O
approach O

to O
aid O
in O
detecting O
the B-DAT
zenith O
VP O
and O
as O
a O

lo- O
cations. O
For O
each O
image, O
the B-DAT
ground-truth O
horizon O
line O
(dash O
green O

) O
and O
the B-DAT
line O
that O
maximizes O
the O
prior O

observe O
that O
the B-DAT
horizon O
line O
can O
be O
uniquely O

defined O
by O
the B-DAT
point O
on O
the O
line O
closest O
to O
the O
principal O

a O
distribution O
over O
points O
in O
the B-DAT
image. O
Fig. O
5 O
shows O
this O

the B-DAT
horizon O
line, O
the O
zenith O
vanishing O
point, O
and O
one O

our O
approach O
makes O
use O
of O
the B-DAT
distributions O
estimated O
from O
global O
image O

29]. O
The O
algorithm O
consists O
of O
the B-DAT
following O
major O
steps O

1. O
detect O
the B-DAT
zenith O
vanishing O
point O
(Sec. O
4.1 O

4.1. O
Detecting O
the B-DAT
Zenith O
Vanishing O
Point O

To O
detect O
the B-DAT
zenith O
vanishing O
point, O
we O
first O

set O
of O
line O
segments O
using O
the B-DAT
zenith O
direction, O
lz, O
from O
the O

global O
image O
context, O
then B-DAT
use O
the O
RANSAC O
[7] O
algorithm O
to O
refine O

it. O
The O
zenith O
direction O
is O
the B-DAT
line O
con O

necting O
the B-DAT
principal O
point O
and O
the O
zenith O
vanishing O
point, O
which O
is O

uniquely O
determined O
by O
the B-DAT
horizon O
line O
slope O
(see O
supplemental O

initial O
estimate O
of O
lz O
using O
the B-DAT
global O
image O
context O
by O
choosing O

the B-DAT
value O
that O
maximizes O
the O
posterior: O
α̂ O
= O
arg O
maxα O

p(α|I). O
To O
handle O
the B-DAT
presence O
of O
outlier O
line O
segments O

candidate O
vertical O
line O
segments O
as O
the B-DAT
RANSAC O
inputs O
by O
threshold- O
ing O

the B-DAT
angle O
between O
each O
line O
segment O

and O
the B-DAT
estimated O
zenith O
direction, O
Θl,lz O

with O
intersection, O
p, O
we O
compute O
the B-DAT
set O
of O
inlier O
line O
segments O

fc(p, O
l) O
> O
0}. O
If O
the B-DAT
largest O
set O
of O
inliers O
has O

candidate O
line O
segments), O
we O
obtain O
the B-DAT
final O
estimate O
of O
the O
zenith O

vanishing O
point, O
z, O
by O
minimizing O
the B-DAT
algebraic O
distance, O
‖lTp‖ O
using O
singular O

decomposition O
(SVD), O
and O
up- O
date O
the B-DAT
zenith O
direction, O
lz. O
Otherwise, O
we O

keep O
the B-DAT
zenith O
direction O
estimated O
from O
the O

are O
perpendicular O
to O
lz O
in O
the B-DAT
image O
space, O
under O
the O
distribution O

VPs O
by O
selecting O
points O
along O
the B-DAT
horizon O
line O
where O
many O
line O

intersect. O
We O
assume O
that O
for O
the B-DAT
true O
horizon O
line O
the O
identified O

As O
a O
preprocessing O
step, O
given O
the B-DAT
zenith O
direction, O
lz, O
and O
a O

are O
likely O
associ- O
ated O
with O
the B-DAT
zenith O
vanishing O
point, O
and O
nearly O

horizon O
line O
candidate, O
h, O
and O
the B-DAT
filtered O
line O
segments O
in O
homogeneous O

P O
= O
{pi}, O
by O
minimizing O
the B-DAT
following O
objective O
function O

too O
close O
together, B-DAT
which O
eliminates O
the O
possibility O
of O
selecting O
multiple O
vanishing O

points O
in O
the B-DAT
same O
location O

line O
candi- O
dates O
(red) O
near O
the B-DAT
ground O
truth O
(green O
dash) O
with O

context O
than O
without O
(left). O
In O
the B-DAT
case O
of O
sampling O
with O
global O

image O
context, O
the B-DAT
offset O
PDF, O
p(o|I) O
(blue O
curve O

), O
is O
fit O
from O
the B-DAT
CNN O
categorical O
probability O
dis- O
tribution O

We O
propose O
the B-DAT
following O
combinatorial O
optimization O
process O
for O

constrained O
nonlinear O
optimization O
to O
refine O
the B-DAT
vanishing O
points O

and O
compute O
their B-DAT
intersection O
with O
the O
horizon O
line. O
We O
then O
construct O

are O
many O
line O
segments O
in O
the B-DAT
image O
that O
are O
consistent O
with O

i, O
j), O
are O
connected O
if O
the B-DAT
corresponding O
vanishing O
points, O
pi,pj O

subset O
of O
VPs O
by O
maximizing O
the B-DAT
sum O
of O
weights, O
while O
ensuring O

no O
VPs O
in O
the B-DAT
final O
set O
are O
too O
close O

. O
Therefore, O
the B-DAT
problem O
of O
choosing O
the O
initial O

hard O
in O
general. O
Due O
to O
the B-DAT
nature O
of O
the O
constraints, O
the O

which O
means O
that, O
in O
practice, O
the B-DAT
problem O
can O
be O
quickly O
solved O

VPs O
that O
when O
removed O
convert O
the B-DAT
ring-like O
graph O
into O
a O
set O

vanishing O
points O
are O
found O
near O
the B-DAT
horizon O
line O
ground O
truth O

node O
with O
minimal O
degree. O
For O
the B-DAT
subgraphs, O
the O
red O
node O
is O

mandatory, O
the B-DAT
dashed O
nodes O
are O
excluded, O
and O

a O
subset O
of O
the B-DAT
solid O
nodes O
are O
selected O
using O

Since O
they B-DAT
were O
randomly O
sampled, O
the O
set O
of O
vanishing O
points O
selected O

pi}opt, O
may O
not O
be O
at O
the B-DAT
optimal O
locations. O
We O
optimize O
their O

locations O
to O
fur- O
ther B-DAT
minimize O
the O
objective O
function O
(2). O
We O
perform O

an O
EM-like O
algorithm O
to O
refine O
the B-DAT
vanishing O
point O
locations, O
subject O
to O

the B-DAT
constraint O
that O
they O
lie O
on O

the B-DAT
horizon O
line: O
• O
E-step: O
Given O

0}. O
• O
M-step: O
Given O
the B-DAT
assigned O
line O
segments O
as O
a O

ln], O
and O
the B-DAT
horizon O
line, O
h, O
both O
represented O

vanishing O
point, O
p∗, O
by O
minimizing O
the B-DAT
al- O
gebraic O
distance, O
‖LTp‖ O
such O

define O
a O
basis, O
Bh, O
for O
the B-DAT
null O
space O
of O
h, O
and O

re- O
formulate O
the B-DAT
problem O
as O
λ∗ O
= O
arg O

we O
solve O
using O
SVD. O
Given O
the B-DAT
optimal O
coeffi- O
cients, O
λ∗, O
we O

reconstruct O
the B-DAT
optimal O
vanishing O
point O
as: O
p O

three O
iter- O
ations O
for O
all O
the B-DAT
experiments. O
The O
final O
set O
of O

to O
assign O
a O
score O
to O
the B-DAT
current O
horizon O
line O
candidate O

assign O
a O
score O
based O
on O
the B-DAT
total O
consistency O
of O
lines O
in O

the B-DAT
image O
with O
the O
VPs O
selected O
in O
the O
previous O

To O
reduce O
the B-DAT
impact O
of O
false O
positive O
vanishing O

points, O
we O
select O
from O
{pi}opt O
the B-DAT
two O
highest O
weighted O
vanishing O

the B-DAT

on O
horizon-line O
detection O
er- O
ror, O
the B-DAT
standard O
criteria O
in O
recent O
work O

detection O
error O
is O
defined O
as O
the B-DAT
maximum O
distance O
from O
the O
detected O

horizon O
line O
to O
the B-DAT
ground-truth O
horizon O
line, O
normalized O
by O

the B-DAT
image O
height. O
Following O
tradition, O
we O

show O
the B-DAT
cumulative O
histogram O
of O
these O
errors O

and O
report O
the B-DAT
area O
under O
the O
curve O
(AUC O

is O
implemented O
using O
MATLAB, O
with O
the B-DAT
exception O
of O
detecting O
line O
segments O

using O
Caffe O
[16]. O
We O
use O
the B-DAT
pa- O
rameters O
defined O
in O
Tab O

vanishing O
points. O
The O
scenes O
obey O
the B-DAT
Manhattan-world O
assumption, O
how- O
ever O
we O

this O
assumption. O
Fig. O
8a O
shows O
the B-DAT
performance O
of O
our O
methods O
relative O

achieves O
state-of-the-art B-DAT
AUC, O
improving O
upon O
the O
previous O
best O
of O
Lezama O
et O

considered O
chal- O
lenging O
due O
to O
the B-DAT
large O
number O
of O
outlier O
line O

1We O
define O
the B-DAT
relative O
improvement O
as O
AUCnew−AUCold O
1−AUCold O

8: O
For O
three O
benchmark O
datasets, O
the B-DAT
fraction O
of O
images O
(y-axis) O
with O

each O
curve O
is O
shown O
in O
the B-DAT
legend. O
For O
additional O
details O
see O

in O
urban O
areas O
and, O
unlike O
the B-DAT
YUD O
dataset, O
not O
all O
images O

satisfy O
the B-DAT
Manhattan-world O
assumption. O
It O
provides O
reliable O

line O
detection. O
To O
our O
knowledge, O
the B-DAT
previous O
state-of-the-art O
performance O
in O
terms O

of O
the B-DAT
AUC O
metric O
on O
this O
dataset O

upon O
their B-DAT
performance, O
increas- O
ing O
the O
state O
of O
the O
art O
to O

considering O
their B-DAT
improvement O
relative O
to O
the O
state O
of O
the O
art O
was O

seconds O
per O
image. O
We O
present O
the B-DAT
performance O
comparison O
with O
other O
methods O

The O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW) O
dataset O
[32] O
is O

challenging O
benchmark O
dataset. O
We O
use O
the B-DAT
provided O
test O
set, O
which O
contains O

images O
not O
adher- O
ing O
to O
the B-DAT
Manhattan-world O
assumption. O
Fig. O
8c O
compares O

our O
method O
with O
the B-DAT
method O
of O
Lezama O
et O
al O

. O
[19] O
(the B-DAT
only O
publicly O
available O
implementation O
from O

section O
provides O
an O
analysis O
of O
the B-DAT
impact O
each O
component O
has O
on O

To O
evaluate O
the B-DAT
impact O
of O
global O
context O
extraction O

our O
proposed O
approach O
(CNN), O
replacing O
the B-DAT
CNN O
with O
a O
random O
forest O

us- O
ing O
the B-DAT
Python O
“sklearn” O
library O
with O
25 O

text O
entirely O
(NONE). O
When O
omitting O
the B-DAT
global O
context, O
we O
assume O
no O

horizon O
lines O
are O
horizontal O
in O
the B-DAT
image) O
and O
sample O
horizon O
lines O

between O
[−2H, O
2H] O
(H O
is O
the B-DAT
image O
height). O
To O
evaluate O
the O

proposed O
approach O
(FULL) O
and O
omitting O
the B-DAT
vanishing O
point O
detection O
step O
(EMPTY O

point O
detection, O
we O
directly O
estimate O
the B-DAT
horizon O
line, O
(α, O
o), O
by O

maximizing O
the B-DAT
posterior O
estimated O
by O
our O
global-context O

components O
play O
important O
roles O
in O
the B-DAT
algorithm O
and O
that O
CNN O
provides O

global O
image O
context O
helps O
improve O
the B-DAT
accuracy O
further. O
Fig. O
8c O
visual O

error O
on O
HLW. O
To O
illustrate O
the B-DAT
impact O
of O
global O
image O
context O

NONE+FULL). O
When O
using O
global O
context, O
the B-DAT
estimated O
horizon O
lines O
are O
very O

close O
to O
the B-DAT
ground O
truth. O
Without, O
the O
estimates O

an O
estimate O
that O
is O
off O
the B-DAT
image O

segments O
color O
coded O
based O
on O
the B-DAT
most O
consistent O
VP, O
the O
ground-truth O

lines O
(magenta). O
For O
clarity O
only O
the B-DAT
top O
two O
horizontal O
VPs O
are O

two O
representative O
failure O
cases O
in O
the B-DAT
last O
column O
of O
Fig. O
10 O

top O
image O
fails O
due O
to O
the B-DAT
propa- O
gation O
of O
measurement O
errors O

from O
the B-DAT
short O
line O
segments. O
The O
bottom O

image O
is O
challenging O
because O
the B-DAT
curved O
struc- O
tures O
lead O
to O

the B-DAT

innovation O
in O
our O
method O
is O
the B-DAT
use O
of O
global O
image O
context O

each O
horizon O
line O
by O
choosing O
the B-DAT
optimal O
vanishing O
points O
for O
the O

curate O
and O
more O
efficient O
than O
the B-DAT
previous O
state-of-the-art O
algorithm, O
requiring O
no O

We O
gratefully O
acknowledge O
the B-DAT
support O
of O
DARPA O
(con- O
tract O

contained O
herein O
are O
those O
of O
the B-DAT
authors O
and O
should O
not O
be O

interpreted O
as O
necessarily O
representing O
the B-DAT
official O
policies O
or O
endorse- O
ments O

or O
implied, O
of O
DARPA O
or O
the B-DAT
U.S. O
Government O

Barreto. O
A O
global O
approach O
for O
the B-DAT
detection O
of O
vanishing O
points O
and O

as O
a O
statistical O
inference O
on O
the B-DAT
unit O
sphere. O
In O
ICCV, O
1990 O

and O
J. O
Lopez-Krahe. O
Contribution O
to O
the B-DAT
determination O
of O
vanishing O
points O
using O

Oliva O
and O
A. O
Torralba. O
Modeling O
the B-DAT
shape O
of O
the O
scene: O
A O

holistic O
representation O
of O
the B-DAT
spatial O
envelope. O
IJCV, O
42(3):145–175, O
2001 O

N. O
Jacobs. O
Horizon O
lines O
in O
the B-DAT
wild. O
arXiv O
preprint O
arXiv:1604.02129, O
2016 O

The O
Horizon O
Lines O
in O
the O
Wild B-DAT
(HLW) O
dataset O
[32] O
is O
a O

The O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW) O
dataset O

The O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW) O
dataset O
[32] O
is O
a O

3. O
Horizon B-DAT
Priors O
from O
Global O
Image O
Context O

4. O
Horizon B-DAT

4.3. O
Optimal O
Horizon B-DAT
Line O
Selection O

tion O
[5, O
19, O
28, O
33]. O
Horizon B-DAT
detection O
error O
is O
defined O
as O

The O
Horizon B-DAT
Lines O
in O
the O
Wild O
(HLW O

M. O
Zhai, O
and O
N. O
Jacobs. O
Horizon B-DAT
lines O
in O
the O
wild. O
arXiv O

Points B-DAT
using O
Global O
Image O
Context O
in O
a O
Non-Manhattan O
World O

and O
the O
zenith O
vanishing B-DAT
point O
in O
man-made O
environments. O
The O
dominant O
trend O

in B-DAT
existing O
methods O
is O
to O
first O

of O
the O
most O
fundamental O
problems O
in B-DAT
geometric O
computer O
vision O
[6, O
22 O

12, O
15, O
17]. O
Recent O
work O
in B-DAT
this O
area O
[3, O
30, O
33 O

a O
set O
of O
parallel O
lines B-DAT
in O
the O
world. O
In O
man-made O
en O

is O
a O
challeng- O
ing B-DAT
problem O
in O
many O
images O
due O
to O
line O

the O
consistency O
of O
the O
lines B-DAT
in O
the O
image O
with O
the O
selected O

This O
seemingly B-DAT
simple O
shift O
in O
approach O
leads O
to O
the O
need O

for O
quickly O
extracting B-DAT
this O
context, O
in O

consistency O
function O
that O
models O
errors O
in B-DAT
the O
line O
seg- O
ment O
extraction O

al. O
[5] O
solve O
the O
problem O
in B-DAT
a O
unified O
framework, O
where O
edges O

point B-DAT
de- O
tection O
often O
fail O
in O
seemingly O
nonsensical O
ways O
and O
2 O

search O
for O
high-quality O
vanishing B-DAT
points O
in O
each. O
Both O
steps O
are O
essential O

use O
unbolded O
letters O
for O
points B-DAT
in O
world O
coordinates O
or O
the O
image O

letters O
for O
points B-DAT
or O
lines O
in O
homogeneous O
coordinates. O
We O
primar- O
ily O

Given O
a O
point B-DAT
(u, O
v) O
in O
the O
image O
plane, O
its O
homogeneous O

is O
the O
camera O
principal B-DAT
point O
in O
the O
image O
frame, O
which O
we O

34], O
and O
are O
quite O
fast O
in B-DAT
practice. O
We O
propose O
to O
use O

their O
common O
point B-DAT
(blue O
dot) O
in O
homogeneous O
coordinates O

one-to-one O
function, O
w O
= O
tan−1(o/κ), O
in B-DAT
which O
κ O
is O
a O
scaling O

modify O
the O
original B-DAT
AlexNet O
architecture O
in O
the O
fol- O
lowing O
way: O
The O

loaded O
from O
Google O
Street O
View O
in B-DAT
large O
metropolitan O
cities O
around O
the O

down- O
loaded O
panoramas O
randomly O
sampled O
in B-DAT
a O
5km O
× O
5km O
re O

the O
city O
center. O
This O
resulted O
in B-DAT
11 O
001 O
panora- O
mas O
from O

images O
captured O
by O
casual O
photographers O
in B-DAT
the O
wild O

compute O
the O
horizon O
line B-DAT
position O
in O
image O
space. O
In O
total, O
our O

is O
one-to-one, O
this O
also O
results O
in B-DAT
a O
continuous O
distribution O
over O
o O

p(α|I) O
and O
p(o|I), O
are O
used O
in B-DAT
the O
next O
step O
of O
our O

approach O
to O
aid O
in B-DAT
detecting O
the O
zenith O
VP O
and O

as O
a O
distribution O
over O
points B-DAT
in O
the O
image. O
Fig. O
5 O
shows O

that O
are O
perpendicular O
to O
lz O
in B-DAT
the O
image O
space, O
under O
the O

Θl,h O
< O
θhor), O
which O
result O
in B-DAT
noisy O
horizon O
line O
intersection O
points O

and O
the O
filtered O
line B-DAT
segments O
in O
homogeneous O
coordinates, O
L O
= O
{li O

of O
selecting B-DAT
multiple O
vanishing O
points O
in O
the O
same O
location O

there O
are O
many O
line B-DAT
segments O
in O
the O
image O
that O
are O
consistent O

pi,pj O
, O
are O
sufficiently O
close O
in B-DAT
homogeneous O
space O
(Θpi,pj O
≤ O
θdist O

weights, O
while O
ensuring B-DAT
no O
VPs O
in O
the O
final O
set O
are O
too O

problem, O
which O
is O
NP- O
hard O
in B-DAT
general. O
Due O
to O
the O
nature O

ring-like B-DAT
structure O
which O
means O
that, O
in O
practice, O
the O
problem O
can O
be O

horizon O
line, B-DAT
h, O
both O
represented O
in O
homogeneous O
coordinates, O
we O
solve O
for O

the O
total O
consistency O
of O
lines B-DAT
in O
the O
image O
with O
the O
VPs O

selected O
in B-DAT
the O
previous O
section. O
The O
score O

er- O
ror, O
the O
standard O
criteria O
in B-DAT
recent O
work O
on O
VP O
detec O

use O
the O
pa- O
rameters O
defined B-DAT
in O
Tab. O
1 O
for O
all O
experiments O

for O
each O
curve O
is O
shown O
in B-DAT
the O
legend. O
For O
additional O
details O

contains B-DAT
103 O
images O
cap- O
tured O
in O
urban O
areas O
and, O
unlike O
the O

knowledge, O
the O
previous O
state-of-the-art O
performance O
in B-DAT
terms O
of O
the O
AUC O
metric O

performance O
comparison O
with O
other O
methods O
in B-DAT
Fig. O
8b O

The O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW) O
dataset O
[32 O

roll O
(horizon O
lines B-DAT
are O
horizontal O
in O
the O
image) O
and O
sample O
horizon O

Quantitative O
results O
presented O
in B-DAT
Tab. O
2 O
show O
that O
both O

components O
play O
important O
roles O
in B-DAT
the O
algorithm O
and O
that O
CNN O

context, O
we O
present O
two O
examples O
in B-DAT
Fig. O
9 O
that O
compare O
horizon O

are O
implausible, O
even O
re- O
sulting B-DAT
in O
an O
estimate O
that O
is O
off O

and O
their O
VPs O
(rings) B-DAT
represented O
in O
homogeneous O
coordinates. O
(last O
column) O
Two O

highlight O
two O
representative O
failure O
cases O
in B-DAT
the O
last O
column O
of O
Fig O

benchmark O
datasets. O
The O
main B-DAT
innovation O
in O
our O
method O
is O
the O
use O

testing B-DAT
dataset, O
which O
is O
common O
in O
other O
methods O

Kohli. O
Geo- O
metric O
image O
parsing B-DAT
in O
man-made O
environments. O
In O
ECCV, O
2010 O

its O
application O
to O
finding B-DAT
cylinders O
in O
range O
data. O
In O
International O
Joint O

methods O
for O
estimating B-DAT
manhattan O
frames O
in O
urban O
imagery. O
In O
ECCV, O
2008 O

and O
M. O
Hebert. O
Putting B-DAT
objects O
in O
perspective. O
IJCV, O
80(1):3–15, O
2008. O
2 O

vanishing B-DAT
points O
via O
point O
alignments O
in O
image O
primal O
and O
dual O
domains O

edge O
grouping B-DAT
and O
camera O
calibration O
in O
complex O
man-made O
environments. O
In O
CVPR O

and O
N. O
Jacobs. O
Horizon O
lines B-DAT
in O
the O
wild. O
arXiv O
preprint O
arXiv:1604.02129 O

How O
trans- O
ferable O
are O
features O
in B-DAT
deep O
neural O
networks? O
In O
NIPS O

K. O
Tang. O
Calculating B-DAT
vanishing O
points O
in O
dual O
space. O
Intel- O
ligent O
Science O

the B-DAT

no O
a O
priori O
assumptions O
about O
the B-DAT
observed O
scene. O
Our O
method O
is O

synthetic B-DAT
data O
for O
training, O
eliminating O
the O
need O
for O
labelled O
images. O
Our O

are O
strong O
cognitive O
cues O
for O
the B-DAT
human O
visual O
percep- O
tion, O
as O

they B-DAT
provide O
characteristic O
information O
about O
the O
geometry O
of O
a O
scene, O
and O

is O
a O
fundamental O
problem O
in O
the B-DAT
field O
of O
computer O
vision, O
because O

and O
is O
uniquely O
defined O
by O
the B-DAT
lines’ O
direction. O
Under O
a O
projective O

intersections, O
which O
is O
difficult O
in O
the B-DAT
presence O
of O
noise, O
spurious O
line O

not O
been O
addressed O
often O
in O
the B-DAT
past O
years O

Since O
the B-DAT
seminal O
work O
of O
Barnard O
[4 O

20, O
22, O
25] O
rely O
on O
the B-DAT
Manhattan- O
world O
assumption O
[7], O
which O

21, O
27, O
28] O
rely O
on O
the B-DAT
less O
rigid O
Atlanta-world O
assumption O
[21 O

are O
estimated, O
usually O
by O
grouping O
the B-DAT
oriented O
elements O
into O
clusters O
[18 O

It O
is O
common O
to O
refine O
the B-DAT
thereby O
detected O
vanishing O
points O
in O

an O
iterative O
process, O
such O
as O
the B-DAT
Expectation O
Max- O
imisation O
(EM) O
algorithm O

utilising O
as O
well. O
Ever O
since O
the B-DAT
AlexNet O
by O
Krizhevsky O
et O
al O

. O
[16] O
succeeded O
in O
the B-DAT
2012 O
ImageNet O
competition, O
convolutional O
neural O

which O
extracts O
prior O
information O
about O
the B-DAT
horizon O
line O
location O
from O
an O

image, O
and O
currently O
achieves O
the B-DAT
best O
state-of-the-art O
performance O
on O
horizon O

more O
abstract O
presen- O
tation O
of O
the B-DAT
scene O
based O
on O
the O
Gaussian O

is O
a O
mapping O
that O
transforms O
the B-DAT
un- O
bounded O
image O
plane O
onto O

making O
vanishing O
points O
far O
from O
the B-DAT
image O
centre O
easier O
to O
handle O

it O
allows O
us O
to O
train O
the B-DAT
CNN O
solely O
using O
synthetic O
data O

very O
straightforward O
manner, O
thus O
eliminating O
the B-DAT
need O
for O
labelled O
real-world O
data O

. O
The O
use O
of O
the B-DAT
CNN O
is O
motivated O
by O
the O

directly O
estimate O
VP O
candidates O
on O
the B-DAT
Gaussian O
sphere, O
which O
are O
then O

improved O
line O
weighting O
scheme O
for O
the B-DAT
EM O
process, O
which O
imposes O
a O

spatial O
consistency O
prior O
over O
the B-DAT
line-to-vanishing-point O
associations O
in O
order O
to O

become O
more O
robust O
in O
the B-DAT
presence O
of O
noise O
and O
clutter O

. O
This O
is O
motivated O
by O
the B-DAT
fact O
that O
spurious O
lines, O
for O

Our O
approach O
consists O
of O
the B-DAT
following O
stages: O
First, O
line O
segments O

are O
extracted O
from O
the B-DAT
input O
image O
using O
the O
LSD O

lines O
are O
then B-DAT
mapped O
onto O
the O
Gaussian O
sphere, O
and O
its O
image O

This O
image O
is O
used O
as O
the B-DAT
input O
for O
a O
CNN O
(Sec O

it O
is O
reasonable O
to O
transform O
the B-DAT
unbounded O
image O
plane O
onto O
a O

bounded O
space, O
such O
as O
the B-DAT
Gaussian O
sphere O
representation, O
which O
based O

The O
lines O
are O
projected O
from O
the B-DAT
image O
plane O
at O
a O
fixed O

distance O
onto O
the B-DAT
unit O
sphere O
at O
origin. O
A O

square O
image O
of O
the B-DAT
sphere’s O
front O
half O
surface O
is O

rendered, O
so O
that O
the B-DAT
lines O
appear O
as O
opaque O
curves O

, O
and O
the B-DAT
image’s O
x,y-coordinates O
cor- O
respond O
to O

and O
elevation O
(α, O
β) O
on O
the B-DAT
sphere. O
This O
sphere O
image O
(cf O

used O
as O
an O
input O
for O
the B-DAT
CNN. O
The O
vanishing O
points O
are O

likewise O
parametrised O
in O
the B-DAT
α, O
β-space, O
and O
are O
then O

N O
grid, O
so O
that O
the B-DAT
occurrence O
of O
a O
vanishing O
point O

1, O
1) O
border O
by O
applying O
the B-DAT
following, O
aspect O
ratio O
preserving, O
transformation O

with O
h O
and O
w O
being O
the B-DAT
image’s O
height O
and O
width, O
respectively O

We O
used O
the B-DAT
popular O
AlexNet O
[16] O
as O
a O

clas- O
sification O
task O
by O
partitioning O
the B-DAT
surface O
of O
the O
Gaussian O
sphere O

suit O
our O
task, O
we O
modified O
the B-DAT
last O
FC O
layer O
of O
the O

N2 O
output O
nodes, O
and O
replaced O
the B-DAT
softmax O
with O
a O
sigmoid O
function O

like O
this. O
The O
output O
of O
the B-DAT
network O
is O
a O
likelihood O
image O

of O
possible O
vanishing O
points O
in O
the B-DAT
given O
scene O

we O
proceed O
as O
follows: O
First, O
the B-DAT
number O
of O
vanishing O
directions O
Kd O

then B-DAT
chosen O
randomly, O
but O
with O
the O
condition O
that O
they O
must O
be O

Fig. O
2: O
Example O
from O
the B-DAT
synthetic O
training O
dataset O
(Sec. O
2.3 O

not O
aligned O
with O
any O
of O
the B-DAT
vanishing O
directions O
are O
interspersed. O
This O

Gaussian O
noise O
is O
added O
to O
the B-DAT
resulting O
2D O
line O
segments, O
with O

line O
in O
normal O
form, O
and O
the B-DAT
true O
vanishing O
point O
for O
each O

As O
the B-DAT
response O
of O
the O
CNN O
is O
rather O
coarse, O
a O

step O
is O
needed O
to O
determine O
the B-DAT
exact O
vanishing O
point O
locations. O
We O

to O
utilise O
a O
variant O
of O
the B-DAT
Expectation O
Maximisation O
(EM) O
algorithm, O
based O

on O
the B-DAT
method O
described O
by O
Košecká O
and O

vk O
is O
calculated O
based O
on O
the B-DAT
posterior O
distribution O

estimates O
are O
obtained O
by O
solving O
the B-DAT
following O
least-squares O
problem O

As O
in O
[15], O
we O
measure O
the B-DAT
distance O
d O
(1) O
ik O

T O
i O
vk O
on O
the B-DAT
Gaussian O

angle-based O
consistency O
measure, O
similar O
to O
the B-DAT
suggestions O
of O
[9, O
20], O
to O

better O
accuracy. O
With O
mi O
being O
the B-DAT
midpoint O
of O
li O
and O
mi O

Departing O
from O
[15], O
we O
utilise O
the B-DAT
output O
of O
our O
CNN O
to O

estimate O
the B-DAT
prior O
p(vk) O
and O
to O
initialise O

the B-DAT
VP O
candidates, O
and O
furthermore O
propose O

affinity O
measure O
wik O
to O
consider O
the B-DAT
spatial O
structure O
of O
line O
segments O

Vanishing O
point O
prior: O
We O
treat O
the B-DAT
output O
of O
the O
CNN O
as O

an O
approximation O
of O
the B-DAT
true O
probability O
density O
distribution O
for O

p(vk) O
in O
the B-DAT
(α, O
β)-space, O
which O
we O
model O

Each O
component O
is O
located O
at O
the B-DAT
centre O
of O
the O
corresponding O
patch O

on O
the B-DAT
Gaussian O
sphere O
and O
weighted O
proportionally O

in O
Fig. O
1. O
Initialisation: O
First, O
the B-DAT
Kinit O
strongest O
local O
maxima O
of O

the B-DAT
CNN O
response O
are O
detected. O
Each O

corresponds O
to O
a O
patch O
on O
the B-DAT
Gaussian O
sphere O
image O
(cf. O
Fig O

. O
1). O
Then, O
the B-DAT
global O
maximum O
within O
such O
a O

detected O
and O
its O
position O
on O
the B-DAT
sphere O
converted O
back O
to O
euclidean O

As O
it O
does O
not O
take O
the B-DAT
spatial O
structure O
of O
line O
segments O

modified O
affinity O
measure O
based O
on O
the B-DAT
following O
assumptions: O
1. O
Line O
segments O

close O
proximity O
likely O
belong O
to O
the B-DAT
same O
vanishing O
point. O
2. O
Line O

and O
dl(li, O
lj) O
being O
the B-DAT
shortest O

distance O
between O
the B-DAT
two O
line O
segments. O
Using O
this O

line O
segments O
which, O
according O
to O
the B-DAT
similarity O
mea- O
sure, O
appear O
to O

occur O
at O
a O
spot O
within O
the B-DAT
image’s O
borders O
which O
is O
not O

once O
every O
fs O
iterations O
of O
the B-DAT
EM O
process: O
First, O
a O
vanishing O

point O
within O
the B-DAT
image O
whose O
associated O
line O
segments O

have O
the B-DAT
highest O
standard O
deviation O
w.r.t. O
their O

vanishing O
points O
are O
calculated, O
replacing O
the B-DAT
old O
one. O
If O
one O
resulting O

As O
is O
customary, O
we O
used O
the B-DAT
horizon O
detection O
error O
metric O
to O

vanishing O
points. O
First, O
we O
select O
the B-DAT
Nvp O
most O
significant O
vanishing O
points O

where O
significance O
is O
measured O
by O
the B-DAT
number O
of O
lines O
nk O
associated O

elevation O
|βk| O
> O
θz O
on O
the B-DAT
Gaussian O
sphere O
is O
considered O
as O

θhor. O
We O
assume O
that O
the B-DAT
projection O
of O
the O
camera O
centre O

c O
coincides O
with O
the B-DAT
center O
of O
the O
image O
and O

calculate O
the B-DAT
angle O
φhz,T O
between O
the O
tentative O
horizon O
line O
and O
the O

and O
select O
the B-DAT
triplet O
with O
the O
highest O
score. O
Finally, O
a O
horizon O

h O
is O
calculated O
– O
under O
the B-DAT
condition O
that O
h O
and O
lzc O

Fig. O
3: O
Cumulative O
histograms O
of O
the B-DAT
horizon O
detection O
error. O
The O
horizon O

error O
is O
represented O
on O
the B-DAT
x-axis, O
while O
the O
y-axis O
represents O

the B-DAT
fraction O
of O
images O
with O
less O

than O
the B-DAT
corresponding O
error O

and O
trained O
our O
CNN O
with O
the B-DAT
Caffe O
[14] O
framework O
and O
used O

in O
Python, O
making O
use O
of O
the B-DAT
Numpy O
and O
Scikit-learn O
[19] O
packages O

seconds O
on O
average O
to O
compute O
the B-DAT
result O
for O
a O
640x480 O
pixel O

95% O
– O
is O
needed O
for O
the B-DAT
EM O
based O
refinement O
step O

of O
our O
method, O
we O
computed O
the B-DAT
horizon O
detec- O
tion O
error O
on O

were O
commonly O
used O
to O
assess O
the B-DAT
performance O
of O
vanishing O
point O
detection O

detection O
error O
is O
defined O
as O
the B-DAT
maximum O
distance O
between O
the O
detected O

and O
the B-DAT
true O
horizon O
line, O
relative O
to O

the B-DAT
image’s O
height O

scene. O
Generally, O
these B-DAT
scenes O
fulfil O
the O
Manhattan-world O
assumption, O
though O
our O
method O

of O
that. O
Fig. O
3a O
shows O
the B-DAT
cumulative O
horizon O
error O
histogram O
and O

the B-DAT
area O
under O
the O
curve O
(AUC) O
as O
a O
performance O

94.27%, O
compared O
to O
94.78% O
of O
the B-DAT
current O
best O
state-of-the-art O
method O
[28 O

to O
[28], O
in O
which O
only O
the B-DAT
horizon O
line O
estimation O
is O
evaluated O

we O
are O
able O
to O
identify O
the B-DAT
three O
orthogonal O
vanishing O
directions O
with O

which O
generally O
do O
not O
satisfy O
the B-DAT
Manhattan-world O
assumption, O
but O
often O
con O

show O
line O
segments O
associated O
with O
the B-DAT
three O
VPs O
used O
for O
horizon O

horizon O
(cyan). O
Images O
4-6 O
show O
the B-DAT
corresponding O
sphere O
images O
with O
most O

1st O
and O
4th O
images O
show O
the B-DAT
best O
case O
example, O
the O
2nd O

5th O
an O
average O
case O
example, O
the B-DAT
3rd O
and O
6th O
a O
failure O

compared O
to O
the B-DAT
YUD. O
The O
horizon O
line O
and O

the B-DAT

The O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW) O
dataset O
[26] O
is O

set O
images O
do O
not O
fulfil O
the B-DAT
Atlanta-world O
assumption. O
Here, O
our O
method O

number O
of O
line O
segments O
near O
the B-DAT
horizon, O
large O
curved O
structures, O
or O

failure O
case O
is O
shown O
by O
the B-DAT
3rd O
and O
6th O
images O
in O

useful O
information O
as O
possible O
from O
the B-DAT
images O
they O
capture. O
We O
want O

from O
camera O
images, O
knowledge O
of O
the B-DAT
cameras O
intrinsic O
parameters O
K O
is O

acquired O
by O
calibration O
before O
deploying O
the B-DAT
camera O
in O
a O
vehicle, O
shock O

and O
vibration O
may O
alter O
the B-DAT
camera’s O
internal O
alignment O
over O
time O

possible O
by O
way O
of O
determining O
the B-DAT
image O
of O
the O
absolute O
conic O

a O
simplified O
version O
that O
assumes O
the B-DAT
camera’s O
principle O
point O
to O
be O

is O
described O
in O
[25]. O
If O
the B-DAT
camera’s O
intrinsic O
parameters O
are O
known O

akin O
to O
a O
rotation O
of O
the B-DAT
camera O
with O
a O
3D O
rotational O

Fig. O
5: O
(a) O
Image O
from O
the B-DAT
KITTI O
[10] O
dataset, O
with O
a O

point O
(red O
dot) O
arising O
from O
the B-DAT
central O
perspective. O
(b) O
Rectified O
version O

of O
(a) O
after O
aligning O
the B-DAT
vanishing O
direction O
with O
the O
y-axis O

amount O
of O
space O
next O
to O
the B-DAT
cyclist O
can O
be O
measured O
easily O

. O
(c) O
Image3 O
of O
the B-DAT
Spalentor O
in O
Basel O
with O
a O

van- O
ishing O
points O
and O
aligning O
the B-DAT
central O
vanishing O
point O
with O
the O

the B-DAT
canonical O
x, O
y O
or O
z-axes O

5c) O
and O
display O
it O
to O
the B-DAT
driver, O
thus O
facilitating O
a O
form O

visually O
appealing O
augmented O
reality O
without O
the B-DAT
need O
for O
explicit O
3D O
reconstruction O

and O
outliers O
exclusively, O
elimi- O
nating O
the B-DAT
need O
for O
labelled O
data. O
Despite O

not O
relying O
on O
either B-DAT
the O
Manhattan- O
world O
or O
Atlanta-world O
assumptions O

in O
two O
further B-DAT
applications. O
Obviously, O
the O
capability O
of O
the O
trained O
CNN O

handle O
different O
scenes O
depends O
on O
the B-DAT
training O
data. O
Since O
the O
proposed O

results O
on O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW) O
demonstrates O
that O
the O

training O
data O
and O
simply O
re-training O
the B-DAT
CNN O

J.P.: O
A O
global O
approach O
for O
the B-DAT
detection O
of O
vanishing O
points O
and O

vanishing O
directions. O
In: O
Proceedings O
of O
the B-DAT
IEEE O
Con- O
ference O
on O
Computer O

Vision, O
1999. O
The O
Proceedings O
of O
the B-DAT
Seventh O
IEEE O
International O
Conference O
on O

Hoiem, O
D., O
Forsyth, O
D.: O
Recovering O
the B-DAT
spatial O
layout O
of O
cluttered O
rooms O

to O
document O
recognition. O
Proceedings O
of O
the B-DAT
IEEE O
86(11), O
2278–2324 O
(1998 O

dual O
domains. O
In: O
Proceedings O
of O
the B-DAT
IEEE O
Conference O
on O
Computer O
Vision O

2004. O
CVPR O
2004. O
Proceedings O
of O
the B-DAT
2004 O
IEEE O
Computer O
Society O
Conference O

Jacobs, O
N.: O
Horizon O
lines O
in O
the B-DAT
wild. O
arXiv O
preprint O
arXiv:1604.02129 O
(2016 O

man-made O
environments. O
In: O
Proceedings O
of O
the B-DAT
IEEE O
Conference O
on O
Computer O
Vision O

non-manhattan O
world. O
In: O
Proceedings O
of O
the B-DAT
IEEE O
Conference O
on O
Computer O
Vision O

The O
Horizon O
Lines O
in O
the O
Wild B-DAT
(HLW) O
dataset O
[26] O
is O
a O

on O
Horizon O
Lines O
in O
the O
Wild B-DAT
(HLW) O
demonstrates O
that O
the O
used O

The O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW) O
dataset O

cases. O
The O
results O
on O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW) O
demonstrates O

The O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW) O
dataset O
[26] O
is O
a O

different O
cases. O
The O
results O
on O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW) O
demonstrates O
that O
the O
used O

2.5 O
Horizon B-DAT
Line O
and O
Orthogonal O
Vanishing O
Point O

3.1 O
Horizon B-DAT
Estimation O

The O
Horizon B-DAT
Lines O
in O
the O
Wild O
(HLW O

different O
cases. O
The O
results O
on O
Horizon B-DAT
Lines O
in O
the O
Wild O
(HLW O

S., O
Zhai, O
M., O
Jacobs, O
N.: O
Horizon B-DAT
lines O
in O
the O
wild. O
arXiv O

gnomonic O
projection O
of O
lines B-DAT
detected O
in O
an O
image. O
This O
allows O
us O

detection O
is O
a O
fundamental O
problem O
in B-DAT
the O
field O
of O
computer O
vision O

a O
projective O
transformation, O
parallel O
lines B-DAT
in O
space O
may O
be O
transformed O
to O

point. B-DAT
The O
detection O
of O
VPs O
in O
perspective O
images O
is O
therefore O
a O

their O
intersections, B-DAT
which O
is O
difficult O
in O
the O
presence O
of O
noise, O
spurious O

has O
not O
been O
addressed O
often O
in B-DAT
the O
past O
years O

orthogonal O
vanish- O
ing B-DAT
directions O
exist O
in O
a O
scene, O
as O
is O
reasonably O

common O
in B-DAT
urban O
scenes O
where O

the O
thereby O
detected O
vanishing B-DAT
points O
in O
an O
iterative O
process, O
such O
as O

Krizhevsky O
et O
al. O
[16] O
succeeded O
in B-DAT
the O
2012 O
ImageNet O
competition, O
convolutional O

as O
they O
perform O
exceedingly B-DAT
well O
in O
a O
variety O
of O
settings. O
Borji O

candidates, O
which O
are O
eventually O
refined B-DAT
in O
an O
EM-like O
process. O
As O
it O

synthetic O
data, O
which O
we O
generate O
in B-DAT
a O
very O
straightforward O
manner, O
thus O

using B-DAT
a O
CNN O
over, O
for O
in O

prior O
over O
the O
line-to-vanishing-point B-DAT
associations O
in O
order O
to O
become O
more O
robust O

in B-DAT
the O
presence O
of O
noise O
and O

locations, O
which O
are O
ultimately O
refined B-DAT
in O
an O
Expectation O
Maximisation O
based O
process O

T O
and O
lines B-DAT
in O
normal O
form O
l O
= O
(l1 O

vanishing B-DAT
points O
are O
likewise O
parametrised O
in O
the O
α, O
β-space, O
and O
are O

N O
patches, O
as O
described O
in B-DAT
Sec. O
2.1, O
and O
assigning O
distinct O

image O
of O
possible O
vanishing B-DAT
points O
in O
the O
given O
scene O

of O
line B-DAT
clusters O
is O
placed O
in O
3D O
space. O
Each O
cluster O
consists O

parallel O
or O
collinear B-DAT
line O
segments O
in O
close O
proximity. O
Additionally, O
some O

directions, O
with O
outlier O
lines B-DAT
shown O
in O
black. O
Left: O
3D O
line O
segment O

border. O
One O
example O
is O
shown O
in B-DAT
Fig. O
2. O
Using O
this O
procedure O

number O
of O
vanishing B-DAT
directions, O
resulting O
in O
a O
dataset O
of O
576,000 O
training O

then O
converted O
into B-DAT
a O
line O
in O
normal O
form, O
and O
the O
true O

for O
CNN O
training B-DAT
as O
described O
in O
Sec. O
2.1 O

Modifications O
As O
in B-DAT
[15], O
we O
measure O
the O
distance O

probability O
density O
distribution O
for O
p(vk) O
in B-DAT
the O
(α, O
β)-space, O
which O
we O

CNN O
response. O
This O
is O
illustrated O
in B-DAT
Fig. O
1. O
Initialisation: O
First, O
the O

wik O
= O
p(vk|li) O
was O
used O
in B-DAT
[15] O
as O
an O
affinity O
mea O

Line B-DAT
segments O
with O
similar O
orientation O
in O
close O
proximity O
likely O
belong O
to O

force O
a O
prior O
on O
wik O
in B-DAT
order O
to O
achieve O
higher O
spatial O

mea- O
sure, O
appear O
to O
lie O
in B-DAT
a O
neighbourhood O
of O
other, O
similar O

e.g. O
those O
which O
would O
result O
in B-DAT
a O
horizon O
line O
slope O
φhor O

and O
post-processing B-DAT
steps O
were O
implemented O
in O
Python, O
making O
use O
of O
the O

Scikit-learn O
[19] O
packages. O
The O
parameters O
in B-DAT
Tab. O
1 O
were O
used O
for O

performance O
of O
vanishing B-DAT
point O
detection O
in O
previous O
works O
[3, O
18, O
24 O

dataset O
additionally O
used O
for O
evaluation O
in B-DAT
[28]. O
The O
horizon O
detection O
error O

28]. O
In O
contrast O
to O
[28], O
in B-DAT
which O
only O
the O
horizon O
line O

The O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW) O
dataset O
[26 O

the O
3rd O
and O
6th O
images O
in B-DAT
Fig. O
4 O

calibration O
before O
deploying B-DAT
the O
camera O
in O
a O
vehicle, O
shock O
and O
vibration O

internal B-DAT
alignment O
over O
time, O
resulting O
in O
a O
need O
for O
recalibration. O
Such O

that O
facilitates O
this O
is O
outlined B-DAT
in O
[12], O
while O
a O
simplified O
version O

vanishing B-DAT
points O
– O
is O
described O
in O
[25]. O
If O
the O
camera’s O
intrinsic O

c) O
Image3 O
of O
the O
Spalentor O
in B-DAT
Basel O
with O
a O
virtual O
sign O

canonical O
x, O
y O
or O
z-axes O
in B-DAT
a O
way O
that O
results O
in O

benchmark O
datasets O
and O
good O
results O
in B-DAT
two O
further O
applications. O
Obviously, O
the O

The O
results O
on O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW) O
demonstrates O
that O

Kohli, O
P.: O
Geometric O
image O
parsing B-DAT
in O
man-made O
environments. O
In: O
European O
conference O

methods O
for O
estimating B-DAT
manhattan O
frames O
in O
urban O
imagery. O
In: O
European O
conference O

Zisserman, O
A.: O
Multiple O
view O
geometry O
in B-DAT
computer O
vision. O
Cambridge O
university O
press O

volutional O
neural O
networks. O
In: O
Advances O
in B-DAT
neural O
information O
processing O
systems. O
pp O

vanishing B-DAT
points O
via O
point O
alignments O
in O
image O
primal O
and O
dual O
domains O

Duchesnay, O
E.: O
Scikit-learn: O
Machine B-DAT
learning O
in O
Python. O
Journal O
of O
Machine O
Learning O

approach O
to O
vanishing B-DAT
point O
detection O
in O
architectural O
environ- O
ments. O
Image O
and O

edge O
grouping B-DAT
and O
camera O
calibration O
in O
complex O
man- O
made O
environments. O
In O

M., O
Jacobs, O
N.: O
Horizon O
lines B-DAT
in O
the O
wild. O
arXiv O
preprint O
arXiv:1604.02129 O

points B-DAT
using O
global O
image O
context O
in O
a O
non-manhattan O
world. O
In: O
Proceedings O

in O
images O
of O
man-made O
environments, O
the B-DAT
horizon O
line O
can O
usually O
be O

grouping O
events. O
This O
allows O
constraining O
the B-DAT
extraction O
of O
the O
horizontal O
vanishing O

the B-DAT

less O
spurious O
vanishing O
points O
than O
the B-DAT
previous O
top-ranked O
methods O

4], O
among O
many O
others. B-DAT
Under O
the O
pinhole O
camera O
model, O
a O
VP O

is O
an O
abstract O
point O
on O
the B-DAT
image O
plane O
where O
2-D O
projections O

space O
appear O
to O
converge. O
In O
the B-DAT
Gestalt O
theory O
of O
perception O
[3 O

tasks O
in O
computer O
vision, O
including O
the B-DAT
examples O
mentioned O
above, O
only O
require O

that O
the B-DAT
vertical O
(so-called O
zenith) O
VP O
and O

be O
avoided O
by O
first O
detecting O
the B-DAT
zenith O
and O
the O
horizon O
line O

HL), O
and O
then B-DAT
constraining O
the O
hVPs O
on O
the O
HL. O
The O

man-made O
environments. O
However, O
until O
recently, O
the B-DAT
HL O
was O
detected O
as O
an O

and-egg” O
situation, O
that O
motivated O
e.g. O
the B-DAT
authors O
of O
[14], O
to O
minimize O

an O
overall O
energy O
across O
the B-DAT
VPs O
and O
the O
HL, O
at O

the B-DAT
expense O
of O
a O
high O
computational O

image O
line O
segments O
orthogonal O
to O
the B-DAT
zenith O
line O

show O
that, O
as O
soon O
as O
the B-DAT
HL O
is O
inside O
the O
image O

is, O
a O
second-order O
gestalt O
(at O
the B-DAT
same O
perceptive O
level O
as O
the O

that O
any O
horizontal O
LS O
at O
the B-DAT
height O
of O
the O
camera’s O
optical O

center O
projects O
to O
the B-DAT
HL O
regardless O
of O
its O
3-D O

alignments O
of O
oriented O
LSs O
around O
the B-DAT
HLs O
are O
indeed O
observed O
in O

than O
[12], O
we O
effectively O
put O
the B-DAT
HL O
detection O
into O
an O
a-contrario O

of O
computation O
and O
accuracy O
of O
the B-DAT
HL, O
along O
with O
more O
relevant O

VPs O
than O
with O
the B-DAT
previous O
top-ranked O
methods O

vast O
body O
of O
literature O
on O
the B-DAT
problem O
of O
VP O
detection O
in O

EM) O
algorithm, O
which O
iteratively O
estimates O
the B-DAT
coordinates O
of O
VPs O
as O
well O

as O
the B-DAT
probabilities O
of O
individual O
LSs O
belonging O

13] O
estimate O
VP O
hypotheses B-DAT
in O
the O
image O
plane O
using O
pairs O
of O

and O
compute O
consensus O
sets O
using O
the B-DAT
J-linkage O
algorithm. O
The O
same O
framework O

methods O
have O
been O
compared O
on O
the B-DAT
same O
datasets O
(DSs), O
York O
Urban O

see O
Section O
5) O
and O
with O
the B-DAT
same O
protocol O
of O
[14]. O
It O

a O
ground O
truth O
(GT) O
for O
the B-DAT
VPs, O
as O
selecting O
relevant O
VPs O

task. O
For O
that O
reason, O
the B-DAT
evaluation O
in O
[14] O
is O
focused O

on O
the B-DAT
accuracy O
of O
the O
HL. O
It O
is O
easy O
to O

show O
that O
the B-DAT
HL O
is O
orthogonal O
to O
the O

zenith O
line O
(ZL), O
which O
is O
the B-DAT
line O
connecting O
the O
principal O
point O

PP) O
and O
the B-DAT
zenith O
(Fig. O
1). O
The O
HL O

performing O
a O
1-D O
search O
along O
the B-DAT
ZL, O
and O
a O
weighted O
least O

squares O
fit, O
where O
the B-DAT
weight O
of O
each O
detected O
VP O

equals O
the B-DAT
number O
of O
corresponding O
lines. O
The O

error O
is O
then B-DAT
defined O
as O
the O
maximum O
Euclidean O
distance O
between O
the O

estimated O
HL O
and O
the B-DAT
GT O
HL O
within O
the O
image O

boundaries, O
divided O
by O
the B-DAT
image O
height. O
To O
represent O
this O

report O
a O
numerical O
value O
as O
the B-DAT
percentage O
of O
area O
under O
the O

curve O
(AUC) O
in O
the B-DAT
subset O
[0, O
0.25]× O
[0, O
1 O

that O
each O
new O
method O
improves O
the B-DAT
accuracy, O
from O
74.34% O
AUC O
for O

and O
68.62% O
for O
EC O
with O
the B-DAT
earliest O
method O
of O
[6] O
to O

and O
89.15% O
for O
EC O
with O
the B-DAT
state O
of O
the O
art O
method O

to O
detect O
meaningful O
VPs O
in O
the B-DAT
sense O
of O
the O
Gestalt O
Theory O

vision O
program O
[3]. O
According O
to O
the B-DAT
Helmholtz O
principle, O
which O
states O
that O

to O
many O
detection O
prob- O
lems, O
the B-DAT
Number O
of O
False O
Alarms O
(NFA O

NFA O
of O
an O
event O
is O
the B-DAT
expectation O
of O
the O
number O
of O

variable, O
a O
meaningful O
event O
in O
the B-DAT
phenomenological O
sense O
can O
be O
detected O

1. O
When O
ǫ O
≤ O
1, O
the B-DAT
event O
is O
said O
meaningful. O
The O

principle O
has O
been O
applied O
to O
the B-DAT
VP O
detection O
problem O
in O
[1 O

1], O
a O
practical O
application O
of O
the B-DAT
Santaló’s O
theory O
[11] O
is O
used O

to O
partition O
the B-DAT
infinite O
image O
plane O
into O
a O

it O
presents O
interesting O
matter O
for O
the B-DAT
building O
of O
our O
own O
method O

, O
and O
especially O
the B-DAT
use O
of O
the O
Santaló’s O
theory O

point O
alignment O
detector O
based O
on O
the B-DAT
Helmholtz O
principle O
[9] O
is O
used O

twice: O
in O
the B-DAT
image O
domain, O
to O
group O
LSs O

the B-DAT

works O
[12,18], O
both O
based O
on O
the B-DAT
same O
principle: O
propose O
candidate O
HLs O

, O
score O
them, B-DAT
and O
keep O
the O
best. O
In O
[18], O
a O
deep O

global O
image O
context O
and O
guide O
the B-DAT
generation O
of O
a O
set O
of O

candidate O
HL O
is O
based O
on O
the B-DAT
consistency O
of O
the O
lines O
in O

the B-DAT
image O
w.r.t. O
the O
selected O
VPs. O
This O
method O
achieved O

the B-DAT

results O
with O
ours O
and O
discuss O
the B-DAT
strengths O
and O
weaknesses O
of O
each O

In O
[12] O
(our O
previous O
work) O
the B-DAT
ZL O
is O
first O
obtained O
using O

brute O
force O
algorithm. O
Centroids O
of O
the B-DAT
LSs O
orthogonal O
to O
the O
ZL O

are O
then B-DAT
projected O
to O
the O
ZL O
and O
candidate O
HLs O
are O

taken O
at O
the B-DAT
peaks O
of O
a O
histogram O
of O

the B-DAT
obtained O
coordinates. O
A O
decreasing O
density O

sample O
point O
is O
scored O
by O
the B-DAT
number O
of O
LSs O
consistent O
with O

angular O
consistency O
mea- O
sure, O
and O
the B-DAT
VPs O
are O
found O
as O
peaks O

in O
the B-DAT
score O
curve. O
The O
final O
score O

each O
candidate O
HL O
is O
finally O
the B-DAT
sum O
of O
the O
best O
two O

scores O
at O
the B-DAT
peaks O
(or O
the O
best O
score O
in O
case O
there O

this O
paper, O
we O
build O
on O
the B-DAT
advances O
of O
several O
of O
these O

as O
in O
[1,8] O
we O
put O
the B-DAT
method O
into O
a O
mathematically, O
well O

a-contrario O
framework. O
However, O
by O
fractioning O
the B-DAT
2-D O
search O
of O
meaningful O
VPs O

computationally O
expensive O
processes O
encountered O
using O
the B-DAT
previous O
a-contrario O
approaches; O
(ii) O
the O

itself O
is O
obtained O
based O
on O
the B-DAT
Helmholtz O
principle. O
One O
benefit O
of O

allow O
considering O
several O
orientations O
for O
the B-DAT
candidate O
HLs, O
therefore O
succeeding O
where O

methods O
fail O
in O
cases O
where O
the B-DAT
vertical O
of O
the O
scene O
is O

candidate O
HLs O
is O
sampled O
around O
the B-DAT
meaning- O
ful O
HLs. O
We O
use O

that O
step, O
whose O
modes O
are O
the B-DAT
offsets O
of O
the O
meaningful O
HLs O

. O
This O
step O
significantly O
improves O
the B-DAT
accu- O
racy O
w.r.t. O
[12], O
where O

no O
other B-DAT
candidates O
than O
the O
peaks O
of O
the O
histogram O
are O

18], O
VPs O
are O
hypothesized B-DAT
along O
the O
candidate O
HLs. O
However, O
thanks O
to O

the B-DAT
Helmholtz O
principle, O
we O
get O
more O

in O
terms O
of O
accuracy O
on O
the B-DAT
usual O
DSs O
(95.35% O
on O
YU O

on O
EC), O
without O
compromising O
neither B-DAT
the O
easiness O
of O
implementation, O
nor O
the O

and O
slightly O
faster O
than O
[18], O
the B-DAT
two O
previous O
state-of-the-art O
methods. O
In O

the B-DAT
next O
section, O
we O
describe O
how O

HLs O
are O
hypothesized B-DAT
based O
on O
the O
Helmholtz O
principle. O
VP O
detec- O
tion O

along O
the B-DAT
candidate O
HLs O
and O
candidate O
scoring O

As O
mentioned O
in O
introduction, O
the B-DAT
ZL O
Lz O
is O
the O
line O

connecting O
the B-DAT
PP O
and O
the O
zenith O
VP. O
An O
initial O
guess O

line O
is O
obtained O
based O
on O
the B-DAT
fact O
that O
the O
vertical O
LSs O

in O
the B-DAT
scene O
are O
aligned O
with O
Lz O

when O
passing O
through O
the B-DAT
PP O

in O
a O
narrow O
strip O
around O
the B-DAT
PP O
(Fig. O
1). O
This O
yields O

can O
be O
detected O
by O
finding O
the B-DAT
maximal O
meaningful O
modes O
(MMMs) O
[3 O

detecting O
hypothesized B-DAT
ZLs O
consists O
of O
the O
following O
steps O
(Fig. O
2): O
(i O

0, O
π[ O
are O
detected O
using O
the B-DAT
LSD O
algorithm O
[5], O
(ii) O
LSs O

far O
from O
the B-DAT
PP O
(|lTi O
c| O
> O
dPP O

with O
li O
the B-DAT
homogeneous O
coordinates O
of O
the O
LSs O

2 O
= O
1 O
and O
c O
the B-DAT
homogeneous O
coordinates O
of O
the O
PP O

far O
from O
being O
vertical O
in O
the B-DAT
image O
(|θi O
− O
π/2 O

are O
discarded O
(Fig. O
2-A1 O
shows O
the B-DAT
LSs O
remaining O
at O
the O
end O

a O
Lz-bin O
orientation O
histogram O
of O
the B-DAT
remaining O
LSs O
is O
built O
(Fig O

. O
2-A2) O
and O
the B-DAT
MMMs O
of O
this O
histogram O
are O

blue O
bins O
in O
Fig. O
2-A3); O
the B-DAT
middle O
orientations O
of O
the O
highest O

bins O
of O
the B-DAT
MMMs O
are O
chosen O
as O
rough O

estimates O
of O
the B-DAT
hypothesized O
ZLs O
(colored O
circles O
in O

LSs O
is O
selected O
by O
thresholding O
the B-DAT
angles O
between O
all O
image O
LSs O

and O
the B-DAT
estimate O
(|θi O
− O
θLz O

with O
θLz O
∈ O
[0, O
π[ O
the B-DAT
orientation O
of O
Lz O
(Fig. O
2-B1 O

, O
the B-DAT
LSs O
are O
drawn O
using O
the O

same O
color O
as O
the B-DAT
corresponding O
circles O
in O
Fig. O
2-A3 O

); O
the B-DAT
intersection O
point O
of O
these O
LSs O

in O
direction O
of O
the B-DAT
colored O
dashed O
lines O
in O
Fig O

using O
a O
RANSAC O
algorithm; O
finally, O
the B-DAT
intersection O
point O
(the O
hypothesized O
zenith O

VP) O
is O
refined O
from O
the B-DAT
set O
of O
inliers, O
based O
on O

Step O
(iv) O
is O
the B-DAT
same O
as O
in O
[18]. O
MMMs O

are O
computed O
using O
the B-DAT
large O
deviation O
estimate O
of O
the O

L O
= O
Lz) O
the B-DAT
prior O
probability O
for O
a O
LS O

YU, O
1.66 O
on O
EC) O
while O
the B-DAT
mode O
with O
highest O
NFA O
does O

not O
correspond O
to O
the B-DAT
expected O
direction. O
A O
benefit O
of O

generate O
candidate O
HLs, O
so O
that O
the B-DAT
correct O
solution O
can O
still O
be O

such O
difficult O
cases O
(Fig. O
2-B2, O
the B-DAT
GT O
HL O
is O
drawn O
in O

dashed O
yellow, O
the B-DAT
estimated O
HL O
in O
cyan). O
This O

no O
MMM. O
In O
that O
case, O
the B-DAT
vertical O
direction O
of O
the O
image O

as O
an O
initial O
guess O
for O
the B-DAT
ZL, O
and O
refined O
according O
to O

1 O
Let O
L O
be O
the B-DAT
number O
of O
bins O
of O
the O

histogram, O
M O
the B-DAT
number O
of O
data, O
r(a, O
b O

) O
the B-DAT
density O
of O
data O
with O
values O

a, O
b], O
and O
p(a, O
b) O
the B-DAT
prior O
probability O
for O
a O
data O

a O
Meaningful O
Gap O
MG) O
in O
the B-DAT
large O
deviation O
sense O
if O
r(a O

Fig. O
2. O
A-contrario O
detection O
of O
the B-DAT
zenith O
line O
(see O
the O
text O

The O
detection O
of O
the B-DAT
HL O
is O
based O
on O
following O

geometric O
properties O
(Fig. O
1): O
(i) O
the B-DAT
HL O
is O
perpendicular O
to O
the O

ii) O
any O
horizontal O
LS O
at O
the B-DAT
height O
of O
the O
camera’s O
optical O

center O
projects O
to O
the B-DAT
HL O
regardless O
of O
its O
3-D O

horizontal O
LSs O
at O
height O
of O
the B-DAT
optical O
center O
in O
the O
scene O

accumulate O
on O
a O
line O
in O
the B-DAT
image O
plane, O
perpendicular O
to O
the O

which O
is O
detected O
by O
finding O
the B-DAT
MMMs O
of O
an O
offset O
histogram O

specifically, O
our O
method O
for O
detecting O
the B-DAT
HL O
is O
as O
follows O
(Fig O

far O
from O
being O
perpendicular O
to O
the B-DAT
ZL O
(||θi−θLz O
|−π/2| O
< O
θh O

) O
are O
discarded, O
(ii) O
the B-DAT
centroids O
of O
the O
remaining O
LSs O

are O
orthogonally O
projected O
on O
the B-DAT
ZL O
and O
their O
offsets O
are O

computed O
relative O
to O
the B-DAT
projection O
of O
the O
PP, O
(iii O

offset O
histogram O
is O
generated O
and O
the B-DAT
MMMs O
of O
this O
histogram O
are O

though O
more O
rarely O
than O
for O
the B-DAT
ZL, O
this O
procedure O
can O
yield O

on O
EC). O
The O
centers O
of O
the B-DAT
highest O
peaks O
of O
the O
Ninit O

This O
estimate O
of O
the B-DAT
HL O
can O
be O
inaccurate O
in O

some O
cases, O
due O
to O
the B-DAT
histogram O
binning O
and, O
sometimes, O
to O

some O
offsets O
between O
the B-DAT
position O
of O
the O
accumu- O
lated O

LSs O
and O
the B-DAT
HL. O
Following O
the O
approach O
used O
in O
[18], O
we O

additional O
candidate O
HLs O
perpendicularly O
to O
the B-DAT
ZL, O
around O
the O
initial O
candidates O

. O
In O
[18], O
the B-DAT
offset O
probability O
density O
function O
(PDF O

a O
Gaussian O
model, O
fit O
from O
the B-DAT
CNN O
categorical O
probability O
dis- O
tribution O

mixture O
model O
(GMM) O
where O
the B-DAT
modes O
are O
the O
offsets O
of O

the B-DAT
initial O
candidates O
and O
the O
standard O
deviations O
are O
identically O
equal O

to O
σH, O
withH O
the B-DAT
image O
height O
and O
σ O
provided O

candidates, O
equally O
di- O
vided O
between O
the B-DAT
Ninit O
initial O
candidates. O
In O
the O

no O
a O
priori O
knowledge O
on O
the B-DAT
position O
of O
the O
HL O
along O

the B-DAT
ZL. O
The O
offsets O
of O
the O
S O
candidate O
HLs O
are O
then O

candidate O
HLs O
are O
assessed O
against O
the B-DAT
success O
of O
detecting O
VPs O
along O

the B-DAT
line. O
Let O
us O
assume O
a O

coordinates O
(θ, O
ρ) O
is O
indeed O
the B-DAT
HL. O
Then, O
intersecting O
all O
image O

accumulation O
of O
intersection O
points O
around O
the B-DAT
VPs O
(Fig. O
3-A,B). O
In O
the O

can O
be O
detected O
by O
finding O
the B-DAT
MMMs O
of O
a O
coordinate O
histogram O

of O
the B-DAT
intersection O
points. O
However, O
the O
prior O
probability O
for O
the O
coordinates O

along O
the B-DAT
HL O
is O
not O
uniform, O
leading O

Eqn. O
(1) O
(e.g. O
Fig. O
3-B, O
the B-DAT
MMM, O
shown O
in O
red, O
is O

In O
this O
section, O
we O
provide O
the B-DAT
prior O
(null O
hypothesis) O
suited O
to O

this O
problem O
and O
describe O
how O
the B-DAT
VPs O
and O
the O
HL O
are O

For O
simplicity, O
we O
shall O
consider O
the B-DAT
image O
domain O
as O
a O
circle O

3-A). O
The O
polar O
coordinates O
of O
the B-DAT
detected O
LSs O
are O
assumed O
uniformly O

by O
Luis O
A. O
Santaló O
in O
the B-DAT
late O
1970s O
[11 O

two O
bounded O
convex O
sets O
in O
the B-DAT
plane O
(which O
may O
or O
may O

not O
overlap) O
and O
L1, O
L2 O
the B-DAT
lengths O
of O
the O
boundaries O
∂K1 O

, O
∂K2, O
the B-DAT
probability O
that O
a O
random O
chord O

where O
Le O
is O
the B-DAT
length O
of O
the O
external O
cover O

and O
K2, O
and O
Li O
is O
the B-DAT
length O
of O
the O
internal O
cover O

as O
follows. O
Let O
O′ O
be O
the B-DAT
orthogonal O
projection O
of O
O O
onto O

the B-DAT
candidate O
HL O
L O
and O
let O

Let O
A,B O
(resp. O
C,D) O
be O
the B-DAT
points O
of O
contact O
of O
the O

tangents O
to O
the B-DAT
circle O
C O
from O
point O
O O

The O
external O
cover O
Ce O
is O
the B-DAT
boundary O
of O
the O
convex O
hull O

to O
an O
intersection O
point O
with O
the B-DAT
horizon O
line O
(A). O
The O
modes O

and O
yellow) O
should O
appear O
at O
the B-DAT
positions O
of O
the O
vanishing O
points O

are O
shown O
(B,C,D) O
depending O
on O
the B-DAT
choice O
of O
the O
null O
hypothesis O

and O
the B-DAT
way O
the O
histogram O
is O
built. O
Right: O
computation O

of O
p O
depending O
on O
whether B-DAT
the O
line O
meets O
the O
circle O
or O

of O
C, O
and O
E,F O
are O
the B-DAT
intersection O
points O
of O
the O
circle O

this O
expression O
is O
similar O
to O
the B-DAT
inverse O
of O
the O
sampling O
function O

tan(k∆θ) O
used O
in O
[12], O
though O
the B-DAT
term O
ρ O
is O
also O
involved O

X O
is O
inside O
or O
outside O
the B-DAT
circle O
C. O
In O
the O
sub-case O

where O
X O
is O
inside O
the B-DAT
circle, O
Le O
= O
L1 O
and O

is O
independent O
from O
ρ. O
In O
the B-DAT
sub-case O
where O
X O
is O
outside O

the B-DAT
circle O
(Fig O

2AX)/2π, O
where O
A,B O
denote O
the B-DAT
points O
of O
contact O
of O
the O

tangents O
to O
the B-DAT
circle O
C O
from O
point O
X O

given O
a O
coordinate O
histogram O
of O
the B-DAT
intersection O
points O
and O
given O
a O

bin O
range O
[a, O
b], O
the B-DAT
prior O
probability O
p(a, O
b) O
is O

where O
l(a), O
r(a) O
denote O
the B-DAT
min O
and O
(resp.) O
max O
values O

of O
the B-DAT
histogram O
bin O
a O

3-C O
shows O
an O
example O
of O
the B-DAT
PDF O
r(x) O
= O
∂p∂x O
(x O

purple O
curve). O
In O
this O
figure, O
the B-DAT
red O
and O
yellow O
MMMs O
are O

are O
correctly O
detected. O
How- O
ever, O
the B-DAT
coordinates O
of O
the O
intersection O
points O

can O
be O
large, O
depending O
on O
the B-DAT
orientations O
of O
the O
detected O
LSs O

w.r.t. O
the B-DAT
HL. O
For O
a O
given O
bin O

poor O
time O
per- O
formance O
for O
the B-DAT
MMM O
detection. O
For O
that O
reason O

, O
we O
rather B-DAT
use O
the O
following O
approach: O
(i) O
the O
coordinates O

of O
the B-DAT
intersection O
points O
are O
transformed O
using O

the B-DAT
function O
p(x), O
yielding O
new O
coordinates O

theoretically B-DAT
uniformly O
distributed O
(except O
at O
the O
VPs) O
between O
−1/2 O
and O
1/2 O

of O
bins O
is O
computed O
from O
the B-DAT
new O
coordinates O
and O
the O
MMMs O

his- O
togram O
are O
detected O
using O
the B-DAT
prior O
probability O
p(a, O
b) O
provided O

VPs O
are O
still O
detected, O
while O
the B-DAT
histogram O
is O
much O
more O
compact O

46 O
bins O
against O
3630) O
for O
the B-DAT
same O
accuracy O
(30 O
bins) O
inside O

the B-DAT
image O
domain. O
The O
accuracy O
may O

be O
worse O
outside O
the B-DAT
image O
domain O
but, O
as O
a O

counterpart, O
the B-DAT
propagated O
error O
e.g. O
on O
the O

3-D O
vanishing O
directions, O
decreases O
as O
the B-DAT
distance O
between O
the O
PP O
and O

the B-DAT
VP O
increases4. O
Finally, O
an O
initial O

candidate O
VPs O
are O
extracted O
at O
the B-DAT
centers O
of O
the O
highest O
bins O

of O
the B-DAT
MMMs. O
These O
candidate O
VPs O
are O

EM-like O
algorithm O
sim- O
ilar O
to O
the B-DAT
one O
used O
in O
[18]. O
This O

algorithm O
relies O
on O
the B-DAT
consistency O
measure O

4 O
As O
the B-DAT
angle O
θ O
between O
the O
optical O

vanishing O
direction O
is O
arc-tangential O
in O
the B-DAT
distance O
d O
between O
the O
VP O

and O
the B-DAT
PP, O
the O
propagated O
error O
∂θ/∂d O
is O
inversely O

4. O
Horizon O
lines O
obtained O
at O
the B-DAT
1st, O
25th, O
50th, O
75th O
and O

100th O
percentiles O
of O
the B-DAT
horizon O
error O
(Col. O
1-5, O
resp O

shown O
in O
yellow O
dashed O
line, O
the B-DAT
MMMs O
in O
blue O
dashed O
lines O

and O
the B-DAT
estimated O
HL O
in O
cyan O
solid O

horizon O
error O
is O
displayed O
on O
the B-DAT
top-left O
corner O
of O
each O
image O

VP O
vi O
is O
measured. O
At O
the B-DAT
end O
of O
this O
procedure, O
we O

select O
the B-DAT
two O
high- O
est O
weighted O
VPs O

only O
one O
candidate) O
and O
compute O
the B-DAT
score O
of O
the O
candidate O
HL O

is O
important O
to O
notice O
that O
the B-DAT
consistency O
measure O
is O
used O
to O

refine O
the B-DAT
VPs, O
but O
not O
to O
detect O

in O
comparison O
with O
[18], O
where O
the B-DAT
consistency O
measure O
is O
used O
both O

to O
detect O
and O
refine O
the B-DAT
VPs, O
yielding O
more O
spurious O
VPs O

Moreover, O
our O
1-D O
search O
of O
the B-DAT
VPs O
has O
several O
advantages O
over O

the B-DAT
previous O
a-contrario O
approaches O
[1,8] O
that O

combinatorial O
point O
alignment O
detection O
in O
the B-DAT
dual O
space, O
along O
with O
tricky O

using O
a O
few O
images O
from O
the B-DAT
DSs. O
We O
used O
the O
same O

image O
center. O
In O
order O
quantify O
the B-DAT
parameters’ O
sensitivity, O
we O
did O
the O

row: O
parameters’ O
values O
(W O
is O
the B-DAT
image O
width). O
Second O
row: O
parameters O

2, O
respectively, O
and O
leaving O
the B-DAT
other O
parameters O
unchanged O

the B-DAT
first O
20 O
images O
of O
YU O

For O
each O
parameter, O
we O
report O
the B-DAT
relative O
decrease O
from O
the O
maximum O

to O
the B-DAT
minimum O
AUC O
obtained O
over O
the O

also O
used O
in O
[17]) O
are O
the B-DAT
most O
sensitive O
parameters. O
The O
number O

of O
bins O
in O
the B-DAT
histograms O
(Lz, O
Lh, O
Lvp) O
are O

Lh O
is O
more O
sensitive O
than O
the B-DAT
other O
two. O
dpp, O
θv O
and O

150 O
to O
S O
= O
600, O
the B-DAT
AUC O
increases O
from O
93.7% O
to O

5.2 O
Accuracy O
of O
the B-DAT
Horizon O
Line O

Computation O
of O
the B-DAT
HL O
was O
first O
evaluated O
on O

the B-DAT
two O
usual O
DSs: O
(i) O
York O

and O
outdoor O
and O
mostly O
following O
the B-DAT
Manhattan O
world O
assumption, O
and O
(ii O

scenes O
from O
different O
parts O
of O
the B-DAT
world, O
more O
varied O
viewpoints, O
and O

poorer O
fit O
to O
the B-DAT
Manhattan O
assumption. O
Example O
results O
are O

YU O
and O
EC). O
We O
show O
the B-DAT
images O
where O
the O
horizon O
error O

is O
the B-DAT
lowest O
(column O
1), O
the O
highest O
(column O
5), O
and O
at O

the B-DAT
25th, O
50th O
and O
75th O
percentiles O

table O
in O
Fig. O
5 O
shows O
the B-DAT
performance O
of O
our O
method, O
based O

on O
the B-DAT
cumulative O
histogram O
of O
the O
horizon O
error O
and O
the O
AUC O

the B-DAT

On O
YU, O
we O
improve O
upon O
the B-DAT
previous O
best O
of O
Zhai O
et O

considering O
their B-DAT
improvement O
relative O
to O
the O
previous O
state O
of O
the O
art O

8] O
was O
5%. O
On O
EC, O
the B-DAT
relative O
improvement O
upon O
the O
previous O

new O
AUC O
are O
shown O
in O
the B-DAT
table O
of O
Fig O
5 O
(“Linear O

and O
higher O
to O
that O
with O
the B-DAT
PDF O
of O
[18]. O
This O
signifies O

as O
long O
as O
it O
covers O
the B-DAT
range O
[−2H, O
2H] O
with O
sufficient O

density. O
This O
tends O
to O
attribute O
the B-DAT
improvement O
of O
accuracy O
w.r.t. O
to O

procedure. O
It O
indeed O
appears O
that O
the B-DAT
method O
of O
[18] O
gets O
much O

Sec. O
5.3 O
below). O
By O
contrast, O
the B-DAT
best O
result O
obtained O
by O
our O

both O
our O
sampling O
PDF O
and O
the B-DAT
one O
of O
[18] O
improve O
the O

both O
sampling O
and O
scoring O
of O
the B-DAT
candidate O
HLs O
contribute O
to O
our O

evaluated O
on O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW), O
a O
DS O
intro O

also O
much O
more O
challenging O
than O
the B-DAT
previous O
ones. O
Most O
of O
the O

occupying O
a O
large O
part O
of O
the B-DAT
image, O
and O
so O
on. O
Fur O

- O
thermore, B-DAT
the O
roll O
and O
tilt O
angles O
of O

the B-DAT
camera O
have O
very O
large O
range O

leading O
to O
HLs O
far O
from O
the B-DAT
image O
boundaries, O
and O
ZL O
angles O

out O
of O
the B-DAT
assumed O
range O
(e.g. O
Fig. O
4-C5 O

4-Row O
C, O
and O
(resp.) O
in O
the B-DAT
third O
column O
of O
the O
table O

closely O
compare O
our O
PDF O
with O
the B-DAT
one O
of O
[18] O
and O
establish O

which O
parameters O
of O
the B-DAT
PDFs, O
among O
the O
modes O
and O

the B-DAT
spreads, O
are O
the O
most O
critical, O
we O
tested O
both O

sample O
(S O
= O
1), O
namely O
the B-DAT
mode O
of O
the O
GMM O
with O

NFA O
with O
our O
method, O
and O
the B-DAT
center O
of O
the O
PDF O
with O

the B-DAT
method O
of O
Zhai O
et O
al O

The O
results O
are O
shown O
in O
the B-DAT
last O
two O
rows O
of O
the O

our O
method O
is O
now O
quite O
the B-DAT
same O
as O
with O
[18]. O
This O

indicates O
that, O
in O
HLW, O
the B-DAT
spread O
of O
the O
sampling O
is O

the B-DAT
key O
element O
of O
the O
difference O
in O
performance O
between O
[18 O

is O
re-estimated O
each O
frame O
from O
the B-DAT
CNN O
output, O
while O
we O
take O

results O
may O
be O
to O
consider O
the B-DAT
NFAs O
of O
the O
candidate O
HLs O

The O
predictive O
power O
of O
the B-DAT
CNN O
is O
interesting O
in O
bad O

along O
with O
a O
GT, O
to O
the B-DAT
learning O
process. O
By O
contrast, O
our O

results O
in O
some O
images O
where O
the B-DAT
CNN O
fails O
due O
to O
insufficient O

representation O
in O
the B-DAT
learning O
DS. O
For O
instance, O
Fig O

Our O
method O
succeeds O
in O
predicting O
the B-DAT
HL, O
refining O
it O
and O
getting O

meaningful O
VPs O
(Fig. O
6-A1), O
while O
the B-DAT
method O
of O
Zhai O
et O
al O

. O
poorly O
estimates O
the B-DAT
sampling O
PDF O
and O
finally O
the O

5.3 O
Relevance O
of O
the B-DAT
Vanishing O
Points O

some O
example O
VPs O
(represented O
by O
the B-DAT
LSs O
consistent O
with O
them) O
obtained O

using O
our O
method. O
Performance O
w.r.t. O
the B-DAT
previous O
two O
best O
of O
[8,18 O

] O
was O
measured O
by O
counting O
the B-DAT
number O
of O
good O
and O
spurious O

VPs O
obtained O
on O
the B-DAT
YU O
and O
EC O
DSs. O
We O

our O
method O
(Col. O
1) O
and O
the B-DAT
method O
of O
Zhai O
et O
al O

. O
[18] O
(Col. O
2) O
on O
the B-DAT
one O
hand, O
and O
between O
our O

method O
(Col. O
3) O
and O
the B-DAT
method O
of O
Lezama O
et O
al O

. O
[8] O
(Col. O
4) O
on O
the B-DAT
other O
hand. O
Plotting O
conventions O
are O

horizontal O
lines O
normally O
corresponding O
to O
the B-DAT
same O
VP. O
In O
the O
latter O

are O
counted. O
Fig. O
7-Left O
shows O
the B-DAT
total O
number O
of O
good O
VPs O

and O
split O
VPs O
obtained O
on O
the B-DAT
two O
DSs O
for O
each O
method O

. O
Our O
method O
is O
the B-DAT
most O
relevant O
regarding O
the O
three O

criteria. O
We O
obtain O
the B-DAT
highest O
number O
of O
good O
VPs O

split O
VP O
at O
all, O
whatever O
the B-DAT
DS O
is O

of O
2.11 O
per O
image–p.i. O
on O
the B-DAT
two O
DSs, O
against O
2.14 O
with O

results O
are O
mainly O
due O
to O
the B-DAT
approach O
used O
by O
[18] O
to O

initialize O
VPs O
along O
the B-DAT
candidate O
HLs. O
This O
approach O
consists O

and O
computing O
their B-DAT
intersection O
with O
the O
HL. O
An O
optimal O
subset O
of O

VPs O
vi O
is O
extracted O
from O
the B-DAT
intersections, O
so O
that O
the O
sum O

while O
ensuring O
no O
VPs O
in O
the B-DAT
final O
set O
are O
too O
close O

groups O
while O
they B-DAT
correspond O
to O
the O
same O
VP O
(e.g. O
the O
blue O

and O
yellow O
LSs O
on O
the B-DAT
building’s O
facade O
in O
Fig. O
6-B2 O

represented O
by O
few O
LSs O
(e.g. O
the B-DAT
VP O
consistent O
with O
the O
yellow O

has O
to O
be O
fixed O
for O
the B-DAT
consistency O
measure, O
any O
set O
of O

LSs O
that O
meet O
accidentally O
“near” O
the B-DAT
same O
point O
on O
the O
HL O

generate O
a O
spurious O
VP O
(e.g. O
the B-DAT
yellow O
LSs O
in O
Fig. O
6-C2 O

relying O
on O
an O
a-contrario O
framework, O
the B-DAT
method O
in O
[8] O
gets O
poor O

results O
regarding O
the B-DAT
detected O
VPs: O
the O
lowest O
number O
of O
found O
VPs O

1.80 O
p.i.), O
the B-DAT
second O
highest O
number O
of O
spurious O

one O
for O
3 O
good) O
and O
the B-DAT
highest O
number O
of O
split O
VPs O

of O
good O
VPs O
(see O
e.g. O
the B-DAT
VPs O
consistent O
with O
the O
orange O

resp.) O
may O
be O
explained O
by O
the B-DAT
fact O
that O
a O
VP O
can O

appear O
as O
meaningful O
along O
the B-DAT
HL, O
but O
not O
in O
the O

number O
of O
spurious O
VPs O
(e.g. O
the B-DAT
VPs O
consistent O
with O
the O
cyan O

that O
appear O
more O
frequently O
in O
the B-DAT
whole O
image O
dual O
domain O
than O

on O
the B-DAT
HL. O
Finally, O
the O
high O
number O
of O
split O
VPs O

is O
mainly O
due O
to O
the B-DAT
fact O
that O
aligned O
points O
in O

the B-DAT
dual O
domain O
(meeting O
LSs O
in O

the B-DAT
primal O
domain) O
can O
be O
scattered O

in O
the B-DAT
direction O
orthogonal O
to O
the O
alignment, O
producing O
several O
meaningful O
alignments O

our O
method, O
LSs O
corresponding O
to O
the B-DAT
same O
VP O
can O
meet O
the O

HL O
at O
coordinates O
scattered O
along O
the B-DAT
HL, O
but O
generally O
in O
contiguous O

bins O
of O
the B-DAT
coordinate O
histogram, O
so O
that O
those O

Our O
method O
is O
faster O
than O
the B-DAT
previous O
methods O
whose O
code O
is O

slightly O
affected O
by O
increases O
in O
the B-DAT
image O
size, O
which O
generally O
yield O

therefore B-DAT
only O
linearly O
affected O
by O
the O
number O
of O
LSs O

detect O
Manhattan O
directions, O
hVPs O
and/or O
the B-DAT
HL O
in O
an O
image, O
which O

the B-DAT

provides O
more O
relevant O
VPs O
than O
the B-DAT
previous O
two O
state-of-the-art O
approaches, O
which O

for O
any O
practical O
use O
of O
the B-DAT
VPs O
(e.g. O
finding O
the O
Manhattan O

large O
GT O
DSs, O
especially O
when O
the B-DAT
later O
condition O
is O
not O
met O

urban O
imagery. O
In: O
Proceedings O
of O
the B-DAT
European O
Conference O
on O
Computer O
Vision O

Video O
compass. O
In: O
Proceedings O
of O
the B-DAT
European O
Confer- O
ence O
on O
Computer O

Self-similar O
sketch. O
In: O
Proceedings O
of O
the B-DAT
European O
Conference O
on O
Computer O
Vision O

on O
Horizon O
Lines O
in O
the O
Wild B-DAT
(HLW), O
a O
DS O
intro- O
duced O

3 O
Candidate O
Horizon O
Lines B-DAT

was O
then O
evaluated O
on O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW), O
a O

method O
was O
then O
evaluated O
on O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW), O
a O
DS O
intro- O
duced O

A O
Contrario O
Horizon B-DAT

A-Contrario O
Horizon B-DAT

Keywords: O
Horizon B-DAT
line O
· O
Vanishing O
point O
detection O

A-Contrario O
Horizon B-DAT

Horizon-First B-DAT
VP O
Detection. O
Horizon O

3 O
Candidate O
Horizon B-DAT
Lines O

A-Contrario O
Horizon B-DAT

3.2 O
A-Contrario O
Horizon B-DAT
Line O
Detection O

A-Contrario O
Horizon B-DAT

A-Contrario O
Horizon B-DAT

Fig. O
4. O
Horizon B-DAT
lines O
obtained O
at O
the O
1st O

A-Contrario O
Horizon B-DAT

5.2 O
Accuracy O
of O
the O
Horizon B-DAT
Line O

method O
was O
then O
evaluated O
on O
Horizon B-DAT
Lines O
in O
the O
Wild O
(HLW O

A-Contrario O
Horizon B-DAT

A-Contrario O
Horizon B-DAT

Abstract. O
We O
show O
that, O
in B-DAT
images O
of O
man-made O
environments, O
the O

set O
of O
parallel O
line B-DAT
segments O
in O
3-D O
space O
appear O
to O
converge O

2-D O
line B-DAT
segment O
(LS) O
is O
in O
itself O
a O
gestalt O
(grouping O
of O

this O
paper, O
we O
are O
interested B-DAT
in O
VP O
detection O
from O
uncalibrated O
monocular O

any O
two O
parallel O
lines B-DAT
intersect O
in O
a O
VP, O
LSs O
grouping O
is O

spurious O
VPs. O
However, O
many O
tasks O
in B-DAT
computer O
vision, O
including O
the O
examples O

lines B-DAT
converge O
towards O
that O
point O
in O
man-made O
environments. O
However, O
until O
recently O

as O
an O
alignment O
of O
VPs, O
in B-DAT
other O
words, O
a O
third-order O
gestalt O

its O
3-D O
direction O
(red O
LSs O
in B-DAT
Fig. O
1). O
In O
practice, O
doors O

the O
HLs O
are O
indeed B-DAT
observed O
in O
most O
images O
from O
urban O
and O

us O
to O
obtain B-DAT
top-ranked O
results O
in O
terms O
of O
both O
rapidity O
of O

the O
problem O
of O
VP O
detection O
in B-DAT
uncalibrated O
images. O
[6] O
use O
an O

initialization. B-DAT
[13] O
estimate O
VP O
hypotheses O
in O
the O
image O
plane O
using O
pairs O

The O
same O
framework O
is O
used O
in B-DAT
[17], O
though O
a O
probabilistic O
consistency O

relevant O
VPs O
among O
hundreds O
e.g. O
in B-DAT
an O
urban O
scene O
is O
a O

For O
that O
reason, O
the O
evaluation O
in B-DAT
[14] O
is O
focused O
on O
the O

plotted. O
The O
plots O
are O
reported O
in B-DAT
Fig. O
5. O
[15] O
also O
proposed O

area O
under O
the O
curve O
(AUC) O
in B-DAT
the O
subset O
[0, O
0.25]× O
[0 O

These O
values O
are O
also O
reported O
in B-DAT
Fig. O
5. O
It O
can O
be O

state O
of O
the O
art O
method O
in B-DAT
2013 O
[17 O

proposed O
to O
detect O
meaningful B-DAT
VPs O
in O
the O
sense O
of O
the O
Gestalt O

this O
variable, O
a O
meaningful B-DAT
event O
in O
the O
phenomenological O
sense O
can O
be O

to O
the O
VP O
detection O
problem O
in B-DAT
[1] O
and O
[8]. O
In O
[1 O

large O
votes O
of O
lines B-DAT
meeting O
in O
a O
vanishing O
region, O
thus O
producing O

principle B-DAT
[9] O
is O
used O
twice: O
in O
the O
image O
domain, O
to O
group O

into B-DAT
more O
precise O
lines, O
and O
in O
dual O
domains O
where O
converging O
lines O

This O
method O
achieved O
state-of-the-art O
accuracy O
in B-DAT
2014 O
(94.51% O
for O
YU, O
89.20 O

detection O
was O
simultane- O
ously O
introduced B-DAT
in O
two O
recent O
works O
[12,18], O
both O

the O
consistency O
of O
the O
lines B-DAT
in O
the O
image O
w.r.t. O
the O
selected O

This O
method O
achieved O
state-of-the-art O
accuracy O
in B-DAT
2016: O
94.78% O
for O
YU, O
90.80 O

VPs O
are O
found O
as O
peaks O
in B-DAT
the O
score O
curve. O
The O
final O

peaks O
(or O
the O
best O
score O
in B-DAT
case O
there O
is O
only O
one O

peak). O
This O
method O
is O
fast O
in B-DAT
execution O
and O
easy O
to O
implement O

, O
but O
middle O
rank O
in B-DAT
terms O
of O
accuracy O
(90.40% O
for O

algorithm. O
In O
particular: O
(i) O
as O
in B-DAT
[1,8] O
we O
put O
the O
method O

succeeding B-DAT
where O
other O
methods O
fail O
in O
cases O
where O
the O
vertical O
of O

another O
near-vertical O
direction; O
(iii) O
As O
in B-DAT
[18], O
a O
set O
of O
candidate O

histogram O
are O
considered; O
(iv) O
as O
in B-DAT
[12] O
and O
[18], O
VPs O
are O

improvements, O
our O
approach O
is O
top-ranked O
in B-DAT
terms O
of O
accuracy O
on O
the O

and O
candidate O
scoring B-DAT
are O
presented O
in O
section O
4. O
Experimental O
results O
are O

finally B-DAT
provided O
and O
discussed O
in O
section O
5 O

As O
mentioned O
in B-DAT
introduction, O
the O
ZL O
Lz O
is O

fact O
that O
the O
vertical O
LSs O
in B-DAT
the O
scene O
are O
aligned O
with O

and O
near-parallel O
to O
Lz O
in B-DAT
a O
narrow O
strip O
around O
the O

or O
far O
from O
being B-DAT
vertical O
in O
the O
image O
(|θi O
− O
π/2 O

histogram O
are O
computed O
(blue O
bins B-DAT
in O
Fig. O
2-A3); O
the O
middle O
orientations O

the O
hypothesized O
ZLs O
(colored O
circles O
in B-DAT
Fig. O
2-A3), O
(iv) O
for O
each O

color O
as O
the O
corresponding B-DAT
circles O
in O
Fig. O
2-A3); O
the O
intersection O
point O

of O
these O
LSs O
(in B-DAT
direction O
of O
the O
colored O
dashed O

lines B-DAT
in O
Fig. O
2-B2) O
and O
a O
set O

iv) O
is O
the O
same O
as O
in B-DAT
[18]. O
MMMs O
are O
computed O
using O

LS O
to O
have O
its O
orientation O
in B-DAT
a O
bin O
between O
[a, O
b O

However, O
it O
can O
happen, O
as O
in B-DAT
Fig. O
2, O
that O
several O
modes O

of O
1.71 O
MMMs O
is O
obtained B-DAT
in O
our O
experiments O
on O
YU, O
1.66 O

solution O
can O
still O
be O
found O
in B-DAT
such O
difficult O
cases O
(Fig. O
2-B2 O

the O
GT O
HL O
is O
drawn O
in B-DAT
dashed O
yellow, O
the O
estimated O
HL O

in B-DAT
cyan). O
This O
is O
a O
key O

improvement O
in B-DAT
comparison O
with O
[12] O
and O
[18 O

stage, O
leading B-DAT
to O
incorrect O
results O
in O
such O
cases O
(e.g. O
with O
[18 O

] O
in B-DAT
Fig. O
2-B3). O
Rarely, O
a O
histogram O

density O
of O
data O
with O
values O
in B-DAT
a O
bin O
between O
[a, O
b O

data O
to O
have O
its O
value O
in B-DAT
a O
bin O
between O
[a, O
b O

resp. O
a O
Meaningful B-DAT
Gap O
MG) O
in O
the O
large O
deviation O
sense O
if O

height O
of O
the O
optical O
center O
in B-DAT
the O
scene O
accumulate O
on O
a O

line B-DAT
in O
the O
image O
plane, O
perpendicular O
to O

histogram O
are O
computed O
(red O
bins B-DAT
in O
Fig. O
1). O
Again, O
though O
more O

candidate O
HLs O
(blue O
dashed O
line B-DAT
in O
Fig. O
1 O

the O
HL O
can O
be O
inaccurate B-DAT
in O
some O
cases, O
due O
to O
the O

HL. O
Following B-DAT
the O
approach O
used O
in O
[18], O
we O
tackle O
this O
issue O

image O
height O
and O
σ O
provided O
in B-DAT
Tab. O
1. O
We O
draw O
S O

p(a, O
b) O
is O
taken O
as O
in B-DAT
Eqn. O
(1) O
(e.g. O
Fig. O
3-B O

, O
the O
MMM, O
shown O
in B-DAT
red, O
is O
very O
large O
and O

obtained B-DAT
by O
Luis O
A. O
Santaló O
in O
the O
late O
1970s O
[11 O

are O
two O
bounded O
convex O
sets O
in B-DAT
the O
plane O
(which O
may O
or O

coordinate B-DAT
histogram O
of O
these O
intersections O
(in O
red O
and O
yellow) O
should O
appear O

s(k) O
= O
L O
tan(k∆θ) O
used O
in B-DAT
[12], O
though O
the O
term O
ρ O

obtained B-DAT
for O
a O
line O
L O
in O
case O
2 O
(purple O
curve). O
In O

given O
bin B-DAT
width, O
this O
results O
in O
an O
arbitrary O
and O
potentially O
very O

following B-DAT
this O
procedure O
are O
shown O
in O
Fig. O
3-D. O
Both O
VPs O
are O

ilar O
to O
the O
one O
used O
in B-DAT
[18]. O
This O
algorithm O
relies O
on O

a O
vanishing B-DAT
direction O
is O
arc-tangential O
in O
the O
distance O
d O
between O
the O

The O
GT O
HL O
is O
shown O
in B-DAT
yellow O
dashed O
line, O
the O
MMMs O

in B-DAT
blue O
dashed O
lines O
and O
the O

estimated O
HL O
in B-DAT
cyan O
solid O
line. O
The O
horizon O

This O
is O
a O
great O
difference O
in B-DAT
comparison O
with O
[18], O
where O
the O

a-contrario O
approaches O
[1,8] O
that O
operated O
in B-DAT
2-D O
space. O
With O
regard O
to O

highly O
combinatorial B-DAT
point O
alignment O
detection O
in O
the O
dual O
space, O
along O
with O

https://members.loria.fr/GSimon/v/. O
Algorithm O
parameters O
are O
provided O
in B-DAT
Tab. O
1. O
Those O
were O
tuned O

samples, O
S O
= O
300, O
as O
in B-DAT
[18]. O
The O
PP O
is O
assumed O

and O
particularly O
θcon O
(also O
used O
in B-DAT
[17]) O
are O
the O
most O
sensitive O

parameters. O
The O
number O
of O
bins B-DAT
in O
the O
histograms O
(Lz, O
Lh, O
Lvp O

assumption. O
Example O
results O
are O
provided O
in B-DAT
Fig. O
4, O
first O
and O
second O

3, O
4, O
resp.). O
The O
table O
in B-DAT
Fig. O
5 O
shows O
the O
performance O

The O
new O
AUC O
are O
shown O
in B-DAT
the O
table O
of O
Fig O
5 O

of O
parallel O
lines B-DAT
are O
detected O
in O
most O
images), O
that O
does O
not O

then O
evaluated O
on O
Horizon B-DAT
Lines I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(HLW), O
a O
DS O

with O
our O
method O
are O
shown O
in B-DAT
Fig. O
4-Row O
C, O
and O
(resp O

.) O
in B-DAT
the O
third O
column O
of O
the O

table O
in B-DAT
Fig. O
5. O
The O
approach O
of O

al. O
The O
results O
are O
shown O
in B-DAT
the O
last O
two O
rows O
of O

the O
table O
in B-DAT
Fig. O
5. O
The O
AUC O
with O

with O
[18]. O
This O
indicates B-DAT
that, O
in O
HLW, O
the O
spread O
of O
the O

key O
element O
of O
the O
difference O
in B-DAT
performance O
between O
[18] O
and O
our O

take O
a O
constant, O
empirical O
value O
in B-DAT
our O
method. O
A O
way O
to O

of O
the O
CNN O
is O
interesting B-DAT
in O
bad O
images O
where O
analytical O
vision O

method O
may O
provide O
accurate O
results O
in B-DAT
some O
images O
where O
the O
CNN O

fails O
due O
to O
insufficient B-DAT
representation O
in O
the O
learning O
DS. O
For O
instance O

example O
of O
an O
image O
acquired O
in B-DAT
an O
industrial O
environment. O
Our O
method O

succeeds O
in B-DAT
predicting O
the O
HL, O
refining O
it O

hand. O
Plotting B-DAT
conventions O
are O
as O
in O
Fig. O
4 O

The O
method O
in B-DAT
[18] O
detects O
slightly O
less O
good O

candidate O
HLs. O
This O
approach O
consists O
in B-DAT
randomly O
selecting O
a O
subset O
of O

maximal, O
while O
ensuring B-DAT
no O
VPs O
in O
the O
final O
set O
are O
too O

LSs O
on O
the O
building’s B-DAT
facade O
in O
Fig. O
6-B2). O
Moreover, O
random O
selection O

consistent O
with O
the O
yellow O
LSs O
in B-DAT
Fig. O
6-C1, O
not O
found O
in O

VP O
(e.g. O
the O
yellow O
LSs O
in B-DAT
Fig. O
6-C2). O
All O
these O
threshold O

VP O
detection. O
Right: O
computation O
times O
in B-DAT
sec O

an O
a-contrario O
framework, O
the O
method O
in B-DAT
[8] O
gets O
poor O
results O
regarding O

consistent O
with O
the O
orange O
LSs O
in B-DAT
Fig. O
6-B3 O
and O
C3, O
not O

found O
in B-DAT
Fig. O
6-B4 O
and O
C4, O
resp O

along O
the O
HL, O
but O
not O
in B-DAT
the O
whole O
image O
dual O
domain O

LSs, O
that O
appear O
more O
frequently O
in B-DAT
the O
whole O
image O
dual O
domain O

the O
fact O
that O
aligned O
points B-DAT
in O
the O
dual O
domain O
(meeting O
LSs O

in B-DAT
the O
primal O
domain) O
can O
be O

scattered O
in B-DAT
the O
direction O
orthogonal O
to O
the O

along O
the O
HL, O
but O
generally O
in B-DAT
contiguous O
bins O
of O
the O
coordinate O

so O
that O
those O
are O
fused O
in B-DAT
a O
single O
MMM O
(Fig. O
6-A3&B3 O

The O
method O
was O
implemented O
in B-DAT
Matlab O
and O
run O
on O
a O

CPU. O
Computation O
times O
are O
given O
in B-DAT
Fig. O
7-Right. O
Our O
method O
is O

only O
slightly O
affected O
by O
increases B-DAT
in O
the O
image O
size, O
which O
generally O

LSs. O
Indeed, O
our O
method O
is O
in B-DAT
O(L2z O
+ O
L O

directions, O
hVPs O
and/or O
the O
HL O
in B-DAT
an O
image, O
which O
are O
common O

tasks O
in B-DAT
computer O
vision, O
our O
experimental O
results O

directions). O
Finally, B-DAT
it O
performs O
well O
in O
any O
kind O
of O
environment, O
as O

methods O
for O
estimating B-DAT
manhattan O
frames O
in O
urban O
imagery. O
In: O
Proceedings O
of O

vanishing B-DAT
points O
via O
point O
alignments O
in O
image O
primal O
and O
dual O
domains O

to O
Detect O
Orthogonal O
Vanishing B-DAT
Points O
in O
Uncalibrated O
Images O
of O
Man-Made O
Environments O

Lempitsky, O
V.: O
Geometric O
image O
parsing B-DAT
in O
man-made O
environments. O
International O
Journal O
of O

points B-DAT
using O
global O
image O
context O
in O
a O
non-manhattan O
world. O
In: O
IEEE O

Image O
Caption O
and O
Chi- O
nese O
Poems B-DAT
as O
the O
long, O
mid-length O
and O

The O
BLEU O
performance O
on O
Chinese O
Poems B-DAT

Short O
Text O
Generation: O
Chinese O
Poems B-DAT
To O
evaluate O
the O
performance O
of O

The O
re- O
sults O
on O
Chinese O
Poems B-DAT
indicate O
that O
LeakGAN O
successfully O
handles O

4: O
The O
BLEU O
performance O
on O
Chinese B-DAT
Poems I-DAT

Short O
Text O
Generation: O
Chinese B-DAT
Poems I-DAT
To O
evaluate O
the O
performance O
of O

4. O
The O
re- O
sults O
on O
Chinese B-DAT
Poems I-DAT
indicate O
that O
LeakGAN O
successfully O
handles O

4: O
The O
BLEU O
performance O
on O
Chinese B-DAT
Poems. O
Method O
SeqGAN O
RankGAN O
LeakGAN O

Short O
Text O
Generation: O
Chinese B-DAT
Poems O
To O
evaluate O
the O
performance O

we O
pick O
the O
dataset O
of O
Chinese B-DAT
poems O
which O
is O
proposed O
by O

4. O
The O
re- O
sults O
on O
Chinese B-DAT
Poems O
indicate O
that O
LeakGAN O
successfully O

X., O
and O
Lapata, O
M. O
2014. O
Chinese B-DAT
poetry O
generation O
with O
recurrent O
neural O

multiple O
public O
language O
datasets O
including O
Chinese B-DAT
poems O
[37], O
COCO O
captions O
[19 O

of O
different O
methods O
on O
the O
Chinese B-DAT
poem O
generation O
in O
terms O
of O

4.2 O
Results O
on O
Chinese B-DAT
poems O
composition O

conduct O
experi- O
ments O
on O
the O
Chinese B-DAT
poem O
dataset O
[37], O
which O
contains O

participants O
who O
are O
native O
mandarin O
Chinese B-DAT
speakers O
to O
score O
the O
poems O

to O
the O
finding O
in O
the O
Chinese B-DAT
poem O
composition. O
The O
results O
demonstrate O

Xingxing O
Zhang O
and O
Mirella O
Lapata. O
Chinese B-DAT
poetry O
generation O
with O
recurrent O
neural O

4.2 O
Results O
on O
Chinese B-DAT
poems O
composition O

proposed O
Seq- O
GAN O
to O
generate O
Chinese B-DAT
poems O
and O
Barack O
Obama O
polit O

a O
cor- O
pus4 O
of O
16,394 O
Chinese B-DAT
quatrains, O
each O
containing O
four O
lines O

Table O
2: O
Chinese B-DAT
poem O
generation O
performance O
comparison. O
Algorithm O

of O
special O
structure O
rules O
in O
Chinese B-DAT
poems O
such O
as O
specific O
phonological O

words O
(dependency) O
in O
classi- O
cal O
Chinese B-DAT
poems O
consist O
of O
one O
or O

MLE. O
Then O
70 O
experts O
on O
Chinese B-DAT
poems O
are O
invited O
to O
judge O

X., O
and O
Lapata, O
M. O
2014. O
Chinese B-DAT
poetry O
generation O
with O
recurrent O
neural O

we O
use O
the O
text O
in O
EMNLP2017 B-DAT
WMT O
News, O
COCO O
Image O
Caption O

2: O
BLEU O
scores O
performance O
on O
EMNLP2017 B-DAT
WMT. O
Method O
SeqGAN O
RankGAN O
LeakGAN O

Long O
Text O
Generation: O
EMNLP2017 B-DAT
WMT O
News O
We O
choose O
the O

EMNLP2017 B-DAT
WMT4 O
Dataset O
as O
the O
long O

The O
results O
over O
EMNLP2017 B-DAT
WMT O
News O
data O
are O
shown O

on O
COCO O
Image O
Captions O
and O
EMNLP2017 B-DAT
WMT O
News. O
Datasets O
LeakGAN O
SeqGAN O

EMNLP2017 B-DAT
WMT O
(1) O
The O
American O
Medical O

we O
use O
the O
text O
in O
EMNLP2017 B-DAT
WMT I-DAT
News, O
COCO O
Image O
Caption O
and O

2: O
BLEU O
scores O
performance O
on O
EMNLP2017 B-DAT
WMT I-DAT

Long O
Text O
Generation: O
EMNLP2017 B-DAT
WMT I-DAT
News O
We O
choose O
the O
EMNLP2017 O

The O
results O
over O
EMNLP2017 B-DAT
WMT I-DAT
News O
data O
are O
shown O
in O

on O
COCO O
Image O
Captions O
and O
EMNLP2017 B-DAT
WMT I-DAT
News. O
Datasets O
LeakGAN O
SeqGAN O

EMNLP2017 B-DAT
WMT I-DAT
(1) O
The O
American O
Medical O
Association O

use O
the O
text O
in O
EMNLP2017 O
WMT B-DAT
News, O
COCO O
Image O
Caption O
and O

BLEU O
scores O
performance O
on O
EMNLP2017 O
WMT B-DAT

Long O
Text O
Generation: O
EMNLP2017 O
WMT B-DAT
News O
We O
choose O
the O
EMNLP2017 O

WMT4 B-DAT
Dataset O
as O
the O
long O
text O

the O
generated O
text O
length O
on O
WMT B-DAT
News O

WMT B-DAT
News O
0.236 O
0.554 O
0.651 O

The O
results O
over O
EMNLP2017 O
WMT B-DAT
News O
data O
are O
shown O
in O

by O
the O
models O
trained O
on O
WMT B-DAT
News O
and O
COCO O
Image O
Captions O

COCO O
Image O
Captions O
and O
EMNLP2017 O
WMT B-DAT
News. O
Datasets O
LeakGAN O
SeqGAN O

EMNLP2017 O
WMT B-DAT
(1) O
The O
American O
Medical O
Association O

to O
2-dim O
by O
PCA) O
on O
WMT B-DAT
News O

to O
2-dim O
by O
PCA) O
on O
WMT B-DAT
News O

BLEU O
scores O
on O
COCO O
Image O
Captions B-DAT

Middle O
Text O
Generation: O
COCO O
Image O
Captions B-DAT
Another O
real O
dataset O
we O
use O

is O
the O
COCO O
Image O
Captions B-DAT
Dataset O
(Chen O
et O
al. O
2015 O

the O
dataset. O
The O
COCO O
Image O
Captions B-DAT
training O
dataset O
consists O
of O
20,734 O

WMT O
News O
and O
COCO O
Image O
Captions B-DAT

different O
methods O
on O
COCO O
Image O
Captions B-DAT
and O
EMNLP2017 O
WMT O
News. O
Datasets O

COCO O
Image O
Captions B-DAT
(1) O
A O
man O
sitting O
in O

text O
in O
EMNLP2017 O
WMT O
News, O
COCO B-DAT
Image O
Caption O
and O
Chi- O
nese O

Table O
3: O
BLEU O
scores O
on O
COCO B-DAT
Image O
Captions. O
Method O
SeqGAN O
RankGAN O

Middle O
Text O
Generation: O
COCO B-DAT
Image O
Captions O
Another O
real O
dataset O

we O
use O
is O
the O
COCO B-DAT
Image O
Captions O
Dataset O
(Chen O
et O

to O
generate. O
Note O
that O
the O
COCO B-DAT
Dataset O
is O
not O
a O
long O

preprocessing O
on O
the O
dataset. O
The O
COCO B-DAT
Image O
Captions O
training O
dataset O
consists O

the O
BLEU O
scores O
on O
the O
COCO B-DAT
dataset O
indicate O
that O
LeakGAN O
performs O

0.236 O
0.554 O
0.651 O
< O
10−6 O
COCO B-DAT
0.405 O
0.574 O
0.675 O
< O
10−6 O

trained O
on O
WMT O
News O
and O
COCO B-DAT
Image O
Captions. O
The O
average O
score O

Samples O
from O
different O
methods O
on O
COCO B-DAT
Image O
Captions O
and O
EMNLP2017 O
WMT O

COCO B-DAT
Image O
Captions O
(1) O
A O
man O

Table O
2: O
Appendix O
1 O
- O
COCO B-DAT
Examples O
in O
the O
Questionaire O
Sources O

datasets O
including O
Chinese O
poems O
[37], O
COCO B-DAT
captions O
[19], O
and O
Shakespear’s O
plays O

of O
different O
methods O
on O
the O
COCO B-DAT
captions O
in O
terms O
of O
the O

language O
models O
are O
trained O
on O
COCO B-DAT
caption O
dataset O
without O
the O
images O

4.3 O
Results O
on O
COCO B-DAT
image O
captions O

image O
captions O
provided O
by O
the O
COCO B-DAT
dataset O
[19]. O
The O
captions O
are O

4.3 O
Results O
on O
COCO B-DAT
image O
captions O

and O
C O
Lawrence O
Zitnick. O
Microsoft O
coco B-DAT
captions I-DAT

experiments O
on O
the O
COCO O
Image O
Captions B-DAT
dataset. O
By O
following O
the O
same O

high-quality O
sentences O
of O
COCO O
Image O
Captions B-DAT

NLLgen O
scores O
on O
COCO O
Image O
Captions B-DAT
where O
βmax O
= O
100 O
and O

results O
on O
the O
COCO O
Image O
Captions B-DAT
dataset O
are O
shown O
in O
Figure O

BLEU-4 O
score O
on O
COCO O
Image O
Captions B-DAT
with O
different O
generator O
architectures O

BLEU-2 O
score O
on O
COCO O
Image O
Captions B-DAT
with O
Gumbel-Softmax O
relaxation O
and O
the O

Rel- O
GAN O
on O
COCO O
Image O
Captions B-DAT
with O
S O
∈ O
{1, O
64 O

BLEU-3 O
score O
on O
COCO O
Image O
Captions B-DAT
with O
the O
number O
of O
embedded O

BLEU O
scores O
on O
COCO O
Image O
Captions B-DAT
with O
different O
loss O
functions: O
RSGAN O

BLEU O
scores O
on O
COCO O
Image O
Captions B-DAT
with O
different O
generator O
architectures O

BLEU O
scores O
on O
COCO O
Image O
Captions B-DAT
with O
different O
gradient O
relaxations O
for O

BLEU O
scores O
on O
COCO O
Image O
Captions B-DAT
with O
different O
number O
of O
embedded O

in O
RelGAN O
on O
COCO O
Image O
Captions, B-DAT
where O
βmax O
= O
1000. O
We O

scores O
evaluated O
on O
COCO O
Image O
Captions B-DAT
are O
shown O
in O
Table O
8 O

without O
pre-training O
on O
COCO O
Image O
Captions B-DAT
where O
with O
a O
little O
hyperparameter O

RelGAN O
on O
the O
COCO O
Image O
Captions B-DAT
dataset. O
The O
BLEU O
and O
NLLgen O

of O
βmax O
on O
COCO O
Image O
Captions B-DAT

in O
RelGAN O
on O
COCO O
Image O
Captions B-DAT
with O
varying O
maximum O
inverse O
temperature O

COCO O
Image O
Captions B-DAT
Dataset O

Generated O
Samples O
on O
COCO O
Image O
Captions B-DAT
Dataset O

the O
real O
data O
include O
the O
COCO B-DAT
image O
captions O
(Chen O
et O
al O

3.2 O
COCO B-DAT
IMAGE O
CAPTIONS O
DATASET O

first O
run O
experiments O
on O
the O
COCO B-DAT
Image O
Captions O
dataset. O
By O
following O

of O
generating O
high-quality O
sentences O
of O
COCO B-DAT
Image O
Captions. O
Furthermore, O
the O
NLLgen O

BLEU O
and O
NLLgen O
scores O
on O
COCO B-DAT
Image O
Captions O
where O
βmax O

The O
results O
on O
the O
COCO B-DAT
Image O
Captions O
dataset O
are O
shown O

of O
the O
BLEU-4 O
score O
on O
COCO B-DAT
Image O
Captions O
with O
different O
generator O

of O
the O
BLEU-2 O
score O
on O
COCO B-DAT
Image O
Captions O
with O
Gumbel-Softmax O
relaxation O

we O
test O
Rel- O
GAN O
on O
COCO B-DAT
Image O
Captions O
with O
S O

curves O
of O
BLEU-3 O
score O
on O
COCO B-DAT
Image O
Captions O
with O
the O
number O

curves O
of O
BLEU O
scores O
on O
COCO B-DAT
Image O
Captions O
with O
different O
loss O

C.1 O
GENERATED O
SAMPLES O
ON O
COCO B-DAT
IMAGE O
CAPTIONS O
DATASET O

15 O
generated O
samples O
trained O
on O
COCO B-DAT
image O
caption O
data O
with O
βmax O

curves O
of O
BLEU O
scores O
on O
COCO B-DAT
Image O
Captions O
with O
different O
generator O

curves O
of O
BLEU O
scores O
on O
COCO B-DAT
Image O
Captions O
with O
different O
gradient O

curves O
of O
BLEU O
scores O
on O
COCO B-DAT
Image O
Captions O
with O
different O
number O

of O
NLLgen O
in O
RelGAN O
on O
COCO B-DAT
Image O
Captions, O
where O
βmax O

and O
NLLgen O
scores O
evaluated O
on O
COCO B-DAT
Image O
Captions O
are O
shown O
in O

of O
RelGAN O
without O
pre-training O
on O
COCO B-DAT
Image O
Captions O
where O
with O
a O

impact O
in O
RelGAN O
on O
the O
COCO B-DAT
Image O
Captions O
dataset. O
The O
BLEU O

various O
values O
of O
βmax O
on O
COCO B-DAT
Image O
Captions. O
All O
the O
results O

error O
bars O
in O
RelGAN O
on O
COCO B-DAT
Image O
Captions O
with O
varying O
maximum O

COCO B-DAT
Image O
Captions O
Dataset O

Generated O
Samples O
on O
COCO B-DAT
Image O
Captions O
Dataset O

Glass B-DAT
identification I-DAT
Class O
1 O
67.58 O
80.44 O
84.25 O

Glass O
identification B-DAT
Class O
1 O
67.58 O
80.44 O
84.25 O

Glass B-DAT
identifi- O
cation O

Glass B-DAT
identification O
Class O
1 O
67.58 O
80.44 O

Shamshirband B-DAT
et O
al. O
2014, O
2015a, O
b O

2015; O
Petković O
et O
al. O
2014a, O
b B-DAT

Moham- O
madi O
et O
al. O
2015a, O
b B-DAT

expression. O
a O
Upper O
approximation O
set, O
b B-DAT
lower O
approximation O
set, O
c O
validation O

and O
Tax O
and O
Duin O
(1999a, O
b), B-DAT
both O
the O
grid O
search O
of O

Class O
b B-DAT
126 O
225 O

Class O
b B-DAT
81.19 O
83.17 O
67.46 O
86.33 O

147 B-DAT

158 B-DAT
https://doi.org/10.1007/s00500-016-2317-5 O

Published O
online: O
18 B-DAT
August O
2016 O
© O
Springer-Verlag O
Berlin O

1 B-DAT
Department O
of O
Computer O
Engineering, O
Imam O

1 B-DAT
Introduction O

123 B-DAT

10 B-DAT

1007 B-DAT

148 B-DAT
R. O
Sadeghi, O
J. O
Hamidzadeh O

rough O
set O
(Dubois O
and O
Prade O
1990 B-DAT

1, B-DAT
concepts O
of O
data O
segregation O
by O

1 B-DAT
Support O
vector O
data O
description O

1 B-DAT
ξi O

ξi O
≥ O
0 O
i O
= O
1, B-DAT
. O
. O
. O
, O
L O

1 B-DAT

1 B-DAT
ξi O

1 B-DAT
αi O
(R O

1 B-DAT
γiξi O
(2 O

1, B-DAT
j=1 O
αiα O
j O
(si O
.s O

1 B-DAT
αi O
(si O
.si O

i=1 B-DAT
αi O
= O
1 O
(3 O

Positive O
αi O
(i O
= O
1, B-DAT
. O
. O
. O
, O
L O

123 B-DAT

Automatic O
support O
vector O
data O
description O
149 B-DAT

R2 O
= O
1 B-DAT

123 B-DAT

150 B-DAT
R. O
Sadeghi, O
J. O
Hamidzadeh O

rough O
set O
(Dubois O
and O
Prade O
1990 B-DAT

combination O
of O
fuzzy O
set O
(Zadeh O
1965 B-DAT

) O
and O
rough O
set O
(Pawlak O
1982 B-DAT

rough O
set, O
Dubois O
and O
Prade O
(1990 B-DAT

samples O
are O
identical, O
IR O
becomes O
1, B-DAT
andwhen O
the O
samples O
are O
completely O

(1 B-DAT
− O
|si O
(a) O
− O
s O

popular O
fuzzy O
belief O
function O
(Shafer O
1976 B-DAT

functions O
in O
the O
form O
of O
(10 B-DAT

) O
and O
(11 B-DAT

B:B∩A O
�=φ O
Pα(B) O
(10 B-DAT

B:B∩A O
�=φ O
Pα(B) O
(11 B-DAT

explained O
in O
Dubois O
and O
Prade O
(1990 B-DAT

2008), O
Yao O
and O
Lin- O
gras O
(1998 B-DAT

Bel(A) O
= O
μFIR,μF O
(A) O
(12 B-DAT

PI(A) O
= O
μFIR,μF O
(A) O
(13 B-DAT

123 B-DAT

Automatic O
support O
vector O
data O
description O
151 B-DAT

Fitness O
function O
(xi), O
i O
= O
1 B-DAT

1 B-DAT
While O
(MNI O
< O
I O

1 B-DAT

1 B-DAT

1 B-DAT
1I O
I O
I O
I O
i O

1 B-DAT
1I O
I O
I O

1 B-DAT
i O
= O
ω O
LI O
i O

1 B-DAT
i O
= O
Sinusoidal O
map O
(PI O

1 B-DAT
Best O
= O
Max(Solution) O
x O
Best O

Algorithm O
1 B-DAT

function O
of O
VD O
shown O
in O
(14 B-DAT

VD O
in O
the O
form O
of O
(14 B-DAT

maps O
to O
range O
of O
[0, O
1 B-DAT

constructing O
VD O
use O
(5) O
and O
(15 B-DAT

123 B-DAT

152 B-DAT
R. O
Sadeghi, O
J. O
Hamidzadeh O

Fig. O
1 B-DAT
An O
example O
for O
validation O
degree O

14 B-DAT

μF O
(s O
j O
) O
= O
1 B-DAT

c| O
(15 B-DAT

1 B-DAT
VD(si O
)ξi O

ξi O
≥ O
0 O
i O
= O
1, B-DAT
. O
. O
. O
, O
L O

16 B-DAT

1 B-DAT
VD(si O
)ξi O

1 B-DAT
αi O
(R O

i=1 B-DAT
γiξi O
(17 O

Since O
minimization O
of O
(17 B-DAT

1 B-DAT
αi2R O
= O
0 O
→ O
2R O

1 B-DAT
− O
L O

1 B-DAT
αi O

i=1 B-DAT
αi O
= O
1 O
(18 O

1 B-DAT
αi O
(−2φ(si O
) O
+ O
2 O

1 B-DAT
2αiφ(si O

1 B-DAT
2αi O
‖C‖ O
= O
0 O

1 B-DAT
αi O
= O
2 O

1 B-DAT
αiφ(si O
) O
→ O
‖C O

1 B-DAT
αiφ(si O
) O
∑L O

1 B-DAT
αi O

1 B-DAT
αi=1→ O
‖C O

i=1 B-DAT
αiφ(si O
) O
(19 O

1 B-DAT
P O
· O
VD(si O

1 B-DAT
αi O

1 B-DAT
γi O

1 B-DAT
(P−αi O
− O
γi O

By O
replacing O
of O
(18) B-DAT
and O
(19 O

) O
with O
(17 B-DAT

1, B-DAT
j=1 O
αiα O
j O
(si O

1 B-DAT
αi O
(si O
· O
si O

i=1 B-DAT
αi O
= O
1 O
(21 O

of O
the O
hyper-sphere, O
indicated O
in O
(19 B-DAT

123 B-DAT

Automatic O
support O
vector O
data O
description O
153 B-DAT

123 B-DAT

154 B-DAT
R. O
Sadeghi, O
J. O
Hamidzadeh O

1 B-DAT
and O
then O
the O
results O
of O

1 B-DAT
Comparison O
methodology O

of O
themethods O
are O
compared O
through O
19 B-DAT
experiments O
to O
find O
pros O
and O

results O
are O
indicated O
in O
Table O
1 B-DAT

are O
set O
to O
0, O
27, O
100, B-DAT
and O
100, O
respec- O
tively. O
Finally O

12 B-DAT

Table O
1 B-DAT
Detail O
of O
data O
sets O
used O

10 B-DAT
Class O
2 O
444 O
239 O

Balance O
scale O
4 O
Class O
1 B-DAT
49 O
576 O

10 B-DAT
Class O
1 O
70 O
144 O

Class O
2 O
76 O
138 B-DAT

Class O
3 O
17 B-DAT
197 O

Class O
5 O
13 B-DAT
201 O

Class O
7 O
29 O
185 B-DAT

Ionosphere O
33 O
Class O
g O
225 O
126 B-DAT

Class O
b O
126 B-DAT
225 O

Iris O
4 O
Class O
Se O
50 O
100 B-DAT

Class O
Ve O
50 O
100 B-DAT

Class O
Vi O
50 O
100 B-DAT

Seeds O
7 O
Class O
1 B-DAT
70 O
140 O

Class O
2 O
70 O
140 B-DAT

Class O
3 O
70 O
140 B-DAT

123 B-DAT

Automatic O
support O
vector O
data O
description O
155 B-DAT

Balance O
scale O
Class O
1 B-DAT
92.13 O
95.33 O
94.37 O
99.03 O

10 B-DAT
73.39 O
91.14 O

Glass O
identification O
Class O
1 B-DAT
67.58 O
80.44 O
84.25 O
99.05 O

19 B-DAT
99.05 O

14 B-DAT
97.62 O
95.71 O

19 B-DAT
83.17 O
67.46 O
86.33 O

Seeds O
Class O
1 B-DAT
66.67 O
86.62 O
88.57 O
98.10 O

14 B-DAT
97.14 O

19 B-DAT
88.10 O
98.57 O

123 B-DAT

156 B-DAT
R. O
Sadeghi, O
J. O
Hamidzadeh O

0.0112 B-DAT
0.0029 O
Wilcoxon O
test O
result O
1 O
1 O
1 O

1 B-DAT

1 B-DAT

1 B-DAT

1 B-DAT

AUC O
= O
(1 B-DAT
+ O
TPR O
− O
FPR)/2 O
(24 O

123 B-DAT

Automatic O
support O
vector O
data O
description O
157 B-DAT

Engineering O
andTech- O
nology O
(WASET), O
p O
1020 B-DAT

fuzzy O
rough O
environment. O
Soft O
Comput O
1 B-DAT

–15. B-DAT
doi:10. O
1007 O

a O
probabilistic O
space. O
Inf O
Sci O
178 B-DAT

based O
prototype O
selection. O
Soft O
Comput O
17 B-DAT

Dubois O
D, O
Prade O
H O
(1990 B-DAT

sets*. O
Int O
J O
Gen O
Syst O
17 B-DAT

191 B-DAT

and O
applications O
(SITA-14). B-DAT
IEEE, O
pp O
1 O

description O
(FSVDD*). O
Pattern O
Anal O
Appl O
15 B-DAT

and O
simulation O
(ISMS). O
IEEE, O
pp O
10 B-DAT

14 B-DAT

1878 B-DAT

1889 B-DAT

121 B-DAT

130 B-DAT

and O
cybernetics O
(ICMLC). O
IEEE, O
pp O
1459 B-DAT

1463 B-DAT

184 B-DAT

191 B-DAT

density O
degree. O
Pattern O
Recogn O
38:1768– B-DAT
1771 O

description. O
IEEE O
Trans O
Neural O
Netw O
18 B-DAT

for O
visual O
analysis. O
Springer, O
pp O
181 B-DAT

1296 B-DAT

1313 B-DAT

133 B-DAT

141 B-DAT

119 B-DAT

132 B-DAT

162 B-DAT

171 B-DAT

solu- O
tions O
(CIES). O
IEEE, O
pp O
183 B-DAT

187 B-DAT

Pawlak O
Z O
(1982 B-DAT

Int O
J O
Comput O
Inf O
Sci O
11 B-DAT

1693 B-DAT

1698 B-DAT
Peng O
X, O
Xu O
D O
(2012 O

102 B-DAT

179 B-DAT

185 B-DAT

rough O
sets. O
Fuzzy O
Sets O
Syst O
126 B-DAT

137 B-DAT

155 B-DAT

Shafer O
G O
(1976 B-DAT

123 B-DAT

10 B-DAT

1007 B-DAT

10 B-DAT

1007 B-DAT

158 B-DAT
R. O
Sadeghi, O
J. O
Hamidzadeh O

comparative O
study. O
IEEE O
Sens O
J O
15 B-DAT

Tax O
DM, O
Duin O
RP O
(1999 B-DAT

Tax O
DM, O
Duin O
RP O
(1999 B-DAT

1191 B-DAT

1199 B-DAT

J O
Dyn O
Syst O
Meas O
Control O
137 B-DAT

outlier O
examples O
by O
SVM. O
Neurocomputing O
149 B-DAT

100 B-DAT

105 B-DAT

196 B-DAT

and O
knowledge O
management. O
ACM, O
pp O
1701 B-DAT

1704 B-DAT

Yao O
YY, O
Lingras O
PJ O
(1998 B-DAT

of O
rough O
sets. O
Inf O
Sci O
104 B-DAT

106 B-DAT

Zadeh O
LA O
(1965 B-DAT

Comput O
15 B-DAT

1087 B-DAT

1096 B-DAT
Zhang O
Y, O
Chi O
Z-X, O
Li O

123 B-DAT

1 B-DAT
Introduction O

1 B-DAT
Support O
vector O
data O
description O

1 B-DAT
Comparison O
methodology O

Balance B-DAT
scale O
4 O
Class O
1 O
49 O

Balance B-DAT
scale O
Class O
1 O
92.13 O
95.33 O

Breast O
cancer B-DAT
Wisconsin O

Breast O
cancer B-DAT
Wisconsin O
Class O
2 O
45.96 O
45.82 O

Breast B-DAT
cancer O
Wisconsin O

Breast B-DAT
cancer O
Wisconsin O
Class O
2 O
45.96 O

results O
of O
experiments O
in O
Sect. O
4 B-DAT

radius O
in O
the O
form O
of O
(4 B-DAT

4 B-DAT

4 B-DAT
Chaotic O
bat O
algorithm O

and O
radius, O
calculated O
in O
(4), B-DAT
new O
samples O
are O
considered O
as O

4 B-DAT
Experimental O
results O

is O
first O
detailed O
in O
Sect. O
4 B-DAT

experiments O
are O
interpreted O
in O
Sect. O
4 B-DAT

4 B-DAT

which O
range O
from O
9 O
to O
444 B-DAT

Also, O
they O
are O
described O
by O
4 B-DAT

10 O
Class O
2 O
444 B-DAT
239 O

Class O
4 B-DAT
239 O
444 O

Balance O
scale O
4 B-DAT
Class O
1 O
49 O
576 O

Iris O
4 B-DAT
Class O
Se O
50 O
100 O

4 B-DAT

test O
is O
reported O
in O
Table O
4 B-DAT

the O
first O
row O
of O
Table O
4 B-DAT

the O
second O
row O
of O
Table O
4 B-DAT
because O
thepvalues O
are O
lower O
than O

Breast O
cancer O
Wisconsin O
Class O
2 O
45 B-DAT

.96 O
45 B-DAT

Class O
4 B-DAT
66.20 O
65.90 O
65.02 O
65.60 O

44 B-DAT
84.25 O
99.05 O

48 B-DAT
84.51 O
99.05 O

49 B-DAT
99.05 O

Class O
g O
35.94 B-DAT
71.79 O
77.21 O
40 O

46 B-DAT
86.33 O

Table O
4 B-DAT
Result O
of O
Wilcoxon O
signed-rank O
test O

r- O
4 B-DAT

non-stationary O
classes. O
Pat- O
tern O
Recogn O
41 B-DAT

data O
description. O
Expert O
Syst O
Appl O
41 B-DAT

4 B-DAT

using O
hyperrectangle O
clustering. O
Pattern O
Recogn O
48 B-DAT

vector O
data O
description. O
Pattern O
Recogn O
44 B-DAT

intelligence. O
Springer, O
Berlin, O
Heidelberg, O
pp O
425 B-DAT

451 B-DAT

404 B-DAT

483 B-DAT

497 B-DAT

surface. O
Energy O
Convers O
Manag O
91:433– B-DAT
441 O

compliant O
robotic O
gripper. O
Appl O
Intell O
41 B-DAT

4 B-DAT
http://dx.doi.org/10.1007/s00500-016-2075-4 O
http://archive.ics.uci.edu/ml O
http://archive.ics.uci.edu/ml O

45 B-DAT

processing, O
2003. O
NNSP’03. O
IEEE, O
pp O
499 B-DAT

45 B-DAT

prototype O
selection O
method. O
Pattern O
Recogn O
46 B-DAT

405 B-DAT

430 B-DAT

vector O
data O
description. O
Appl O
Intell O
41 B-DAT

domain O
descrip- O
tion. O
Pattern O
Recogn O
49 B-DAT

4 B-DAT
Chaotic O
bat O
algorithm O

4 B-DAT
Experimental O
results O

4 B-DAT

4 B-DAT

Soft O
Comput O
(2018) B-DAT
22 O

2317 B-DAT

Published O
online: O
18 O
August O
2016 B-DAT
© O
Springer-Verlag O
Berlin O
Heidelberg O
2016 O

2 B-DAT
Faculty O
of O
Computer O
Engineering O
and O

mous O
cases O
(Shamshirband O
et O
al. O
2014, B-DAT
2015a, O
b; O
Altameem O
et O
al O

. O
2015 B-DAT

2014a, B-DAT
b; O
Piri O
et O
al. O
2015 O

2015a, B-DAT
b; O
Hamidzadeh O
et O
al. O
2015 O

; O
Hamidzadeh O
2015 B-DAT

description O
(SVDD) O
(Tax O
and O
Duin O
2004 B-DAT

estimation O
(Mar- O
tinez O
and O
Gray O
2016 B-DAT

video O
summarization O
(Mygdalis O
et O
al. O
2014, B-DAT
2015), O
and O
chemical O
process O
monitoring O

Jiang O
and O
Yan O
2014 B-DAT

data O
structure O
(Lee O
et O
al. O
2005, B-DAT
2007; O
El O
Boujnouni O
et O
al O

. O
2014; B-DAT
Cha O
et O
al. O
2012, O
2014; O
Peng O
and O
Tan O
2015 O

; O
GhasemiGol O
et O
al. O
2010 B-DAT

; O
Allahyari O
and O
Sadoghi-Yazdi O
2012 B-DAT

; O
Liu O
et O
al. O
2013 B-DAT

; O
Phaladiganon O
et O
al. O
2014 B-DAT

; O
Forghani O
et O
al. O
2012 B-DAT

; O
Hu O
et O
al. O
2012 B-DAT

; O
Xiao O
et O
al. O
2010, B-DAT
2014 O

weak O
divisibility O
(Liu O
et O
al. O
2010 B-DAT

; O
Peng O
and O
Xu O
2012 B-DAT

2317 B-DAT

Jeong O
and O
Jayaraman O
2015; B-DAT
Zheng O
2016 O

; O
Jiang O
et O
al. O
2015 B-DAT

hyper-sphere O
margin O
(Tax O
and O
Duin O
2004 B-DAT

; O
Wu O
and O
Ye O
2009 B-DAT

; O
Huang O
et O
al. O
2010, B-DAT
2011 O

; O
Le O
et O
al. O
2013 B-DAT

; O
Zhang O
et O
al. O
2009 B-DAT

; O
Li O
et O
al. O
2014 B-DAT

; O
Forghani O
et O
al. O
2011 B-DAT

non-stationary O
data O
(Tax O
and O
Laskov O
2003 B-DAT

; O
Tavakkoli O
et O
al. O
2008 B-DAT

; O
Camci O
and O
Chinnam O
2008 B-DAT

; O
Theljani O
et O
al. O
2015 B-DAT

concepts. O
In O
Liu O
et O
al. O
(2013 B-DAT

k-NN) O
distance O
(Cha O
et O
al. O
2014 B-DAT

data O
sets O
(Radzikowska O
and O
Kerre O
2002 B-DAT

; O
Chang- O
dar O
et O
al. O
2016 B-DAT

CBA) O
(Gan- O
domi O
and O
Yang O
2014 B-DAT

organized O
as O
follows: O
In O
Sect. O
2 B-DAT
pri- O
mary O
concepts O
of O
SVDD O

2 B-DAT
Background O

con- O
structing O
ASVDD. O
In O
Sect. O
2 B-DAT

are O
investigated. O
Following O
this, O
Sect. O
2 B-DAT

2 B-DAT
reviews O
related O
works O
of O
SVDD O

and O
CBA O
are O
explained O
in O
2 B-DAT

.2 B-DAT
and O
2 O

2 B-DAT

2 B-DAT
≤ O
R2 O
+ O
ξi O
si O

problem O
in O
the O
form O
of O
(2) B-DAT
by O
introducing O
Lagrange O
parameters O
(αi O

2 B-DAT

2 B-DAT

i=1 O
γiξi O
(2 B-DAT

The O
result O
of O
(2) B-DAT
shapes O
a O
saddle O
point; O
hence O

partial O
derivatives O
are O
replaced O
in O
(2 B-DAT

sk∈SV O
sk O
· O
sk O
− O
2 B-DAT

2 B-DAT

2 B-DAT
Related O
works O

an O
example, O
Lee O
et O
al. O
(2005 B-DAT

data O
set O
(Lee O
et O
al. O
2007 B-DAT

in O
El O
Boujnouni O
et O
al. O
(2014 B-DAT

weighted-SVDD O
(DW-SVDD) O
(Cha O
et O
al. O
2012, B-DAT
2014) O
devises O
a O
density O
weight O

control O
parameters O
(Peng O
and O
Tan O
2015 B-DAT

of O
density, O
GhasemiGol O
et O
al. O
(2010 B-DAT

) O
and O
Allahyari O
and O
Sadoghi-Yazdi O
(2012 B-DAT

onuncertain O
data O
(Liu O
et O
al. O
2013 B-DAT

more O
advantages, O
Phaladiganon O
et O
al. O
(2014 B-DAT

concepts O
in O
Forghani O
et O
al. O
(2012 B-DAT

) O
and O
Hu O
et O
al. O
(2012 B-DAT

ity. O
In O
Liu O
et O
al. O
(2010 B-DAT

preimaging O
process. O
Peng O
and O
Xu O
(2012 B-DAT

some O
works, O
such O
as O
Zheng O
(2016 B-DAT

another O
way, O
Jeong O
and O
Jayaraman O
(2015 B-DAT

) O
and O
Jiang O
et O
al. O
(2015 B-DAT

proposed O
in O
Tax O
and O
Duin O
(2004 B-DAT

ously. O
In O
Wu O
and O
Ye O
(2009 B-DAT

negative O
SVDD, O
Wang O
et O
al. O
(2015 B-DAT

same O
time O
(Huang O
et O
al. O
2010, B-DAT
2011), O
but O
also O
multiple O
hyper-spheres O

on O
fuzzy O
(Le O
et O
al. O
2013 B-DAT

fuzzy O
PCM O
(Zhang O
et O
al. O
2009 B-DAT

con- O
cepts O
(Li O
et O
al. O
2014 B-DAT

SVDD O
(ESVDD) O
(Forghani O
et O
al. O
2011 B-DAT

online O
SVDD O
(Tax O
and O
Laskov O
2003 B-DAT

samples. O
In O
Tavakkoli O
et O
al. O
(2008 B-DAT

machine O
(GSVRM) O
(Camci O
and O
Chinnam O
2008 B-DAT

dis- O
advantage, O
Theljani O
et O
al. O
(2015 B-DAT

2 B-DAT

2) B-DAT
(5 O

range. O
Therefore, O
Verbiest O
et O
al. O
(2013 B-DAT

uncertain O
information O
(Liu O
et O
al. O
2016, B-DAT
2015, O
2011). O
In O
fact, O
it O

fuzzy O
concept O
(Chen O
et O
al. O
2008 B-DAT

Prade O
(1990), O
Chen O
et O
al. O
(2008 B-DAT

gras O
(1998), O
Wu O
et O
al. O
(2002 B-DAT

) O
and O
Inuiguchi O
et O
al. O
(2015 B-DAT

NN O
improvement O
(Verbiest O
et O
al. O
2013 B-DAT

; O
Bian O
and O
Mazlack O
2003 B-DAT

; O
Derrac O
et O
al. O
2013 B-DAT

fuzzy O
decision O
tree O
expansion O
(Zhai O
2011 B-DAT

salesman O
problem O
(Changdar O
et O
al. O
2016 B-DAT

2 B-DAT

Bat O
algorithm O
(BA) O
(Yang O
2010 B-DAT

bats. O
In O
Gandomi O
and O
Yang O
(2014 B-DAT

search O
fromBA,while O
asmentioned O
inGandomi O
andYang O
(2014 B-DAT

μFIR,μF O
(si O
) O
2 B-DAT

2 B-DAT

2 B-DAT
≤ O
R2 O
+ O
ξi O
si O

2 B-DAT

2) B-DAT
− O
L O

i=1 O
αi O
(−2φ(si B-DAT
) O
+ O
2 O
‖C O

2 B-DAT
‖C‖ O
L O

i=1 O
αi O
= O
2 B-DAT

0 O
≤ O
αi O
≤ O
P O
(20 B-DAT

constructed O
in O
the O
form O
of O
(21 B-DAT

i=1 O
αi O
= O
1 O
(21 B-DAT

quadratic O
optimization. O
Positive O
results O
of O
(21 B-DAT

φ(snew) O
· O
φ(snew)) O
− O
2 B-DAT

ϕ(s O
j O
))] O
≤ O
R O
(22 B-DAT

Fig. O
2 B-DAT
ASVDD O
process O
flowchart O

The O
following O
flowchart O
in O
Fig. O
2 B-DAT
summarizes O
the O
proposed O
method O

C- O
SVDD) O
(Tax O
and O
Duin O
2004 B-DAT

positive O
data. O
Explained O
in O
Sect. O
2 B-DAT

2, B-DAT
cate- O
gories O
of O
enhancements O
based O

weighted-SVDD O
(DW-SVDD) O
(Cha O
et O
al. O
2014 B-DAT

map-SVDD O
(SM-SVDD) O
(Jiang O
et O
al. O
2015 B-DAT

parameter O
using O
k-NN O
with O
O(N O
2 B-DAT

(2 B-DAT
N O
3) O
= O
O(N O
3 O

2 B-DAT

from O
the O
UCI O
repository O
(Lichman O
2013 B-DAT

experiments O
in O
Tax O
and O
Duin O
(2004 B-DAT

), O
Cha O
et O
al. O
(2014 B-DAT

), O
Liu O
et O
al. O
(2013 B-DAT

set O
to O
the O
range O
of O
2 B-DAT

−8 O
to O
28 B-DAT
and O
the O
number O
of O
neighborhood O

suggestions O
in O
Gandomi O
and O
Yang O
(2014 B-DAT

) O
and O
Yang O
(2010 B-DAT

iterations O
are O
set O
to O
0, O
27, B-DAT
100, O
and O
100, O
respec- O
tively O

10 O
Class O
2 B-DAT
444 O
239 O

Class O
4 O
239 B-DAT
444 O

Class O
2 B-DAT
288 O
337 O

Class O
3 O
288 B-DAT
337 O

Class O
2 B-DAT
76 O
138 O

Class O
5 O
13 O
201 B-DAT

Class O
6 O
9 O
205 B-DAT

Class O
7 O
29 B-DAT
185 O

Ionosphere O
33 O
Class O
g O
225 B-DAT
126 O

Class O
b O
126 B-DAT
225 O

Class O
2 B-DAT
70 O
140 O

Table O
2 B-DAT
Confusion O
matrix O

2 B-DAT
Result O
interpretation O

the O
following O
confusion O
matrix O
(Table O
2 B-DAT

can O
be O
computed O
using O
equation O
(23 B-DAT

FN) O
+ O
(FP O
+ O
TN) O
(23 B-DAT

test O
called O
Wilcoxon O
signed-rank O
(Sheskin O
2003 B-DAT

C-SVDD O
(Tax O
and O
Duin O
2004 B-DAT

DW-SVDD O
(Cha O
et O
al. O
2014 B-DAT

SM-SVDD O
(Jiang O
et O
al. O
2015 B-DAT

Breast O
cancer O
Wisconsin O
Class O
2 B-DAT
45.96 O
45.82 O
34.98 O
37.62 O

20 B-DAT
65.90 O
65.02 O
65.60 O

Class O
2 B-DAT
53.90 O
77.10 O
73.39 O
91.14 O

29 B-DAT
82.30 O

25 B-DAT
99.05 O

Class O
2 B-DAT
64.34 O
90.48 O
84.51 O
99.05 O

21 B-DAT
40.80 O

Class O
2 B-DAT
66.67 O
84.76 O
87.14 O
97.14 O

vs. O
C-SVDD O
(Tax O
and O
Duin O
2004 B-DAT

) O
DW-SVDD O
(Cha O
et O
al. O
2014 B-DAT

) O
SM-SVDD O
(Jiang O
et O
al. O
2015 B-DAT

2 B-DAT

2 B-DAT

r- O
2 B-DAT

2 B-DAT

AUC O
in O
the O
form O
of O
(24 B-DAT

1 O
+ O
TPR O
− O
FPR)/2 B-DAT
(24 O

Allahyari O
Y, O
Sadoghi-Yazdi O
H O
(2012 B-DAT

H, O
Kiah O
MLM, O
Gani O
A O
(2015 B-DAT

Bian O
H, O
Mazlack O
L O
(2003 B-DAT

conference O
of O
theNorthAmerican O
Fuzzy O
InformationProcessingSociety, O
2003 B-DAT

Camci O
F, O
Chinnam O
RB O
(2008 B-DAT

M, O
Kim O
JS, O
Baek O
J-G O
(2014 B-DAT

JS, O
Park O
SH, O
Baek O
J-G O
(2012 B-DAT

C, O
Pal O
RK, O
Mahapatra O
GS O
(2016 B-DAT

2075 B-DAT

D, O
Yang O
W, O
Li O
F O
(2008 B-DAT

S, O
Cornelis O
C, O
Herrera O
F O
(2013 B-DAT

223 B-DAT

238 B-DAT

209 B-DAT

M, O
Jedra O
M, O
Zahid O
N O
(2014 B-DAT

a O
new O
confidence O
coefficient. O
In: O
2014 B-DAT
9th O
interna- O
tional O
conference O
on O

Y, O
Yazdi O
HS, O
Effati O
S O
(2012 B-DAT

237 B-DAT

247 B-DAT

HS, O
Effati O
S, O
Tabrizi O
RS O
(2011 B-DAT

hyper-ellipse O
instead O
of O
hyper-sphere. O
In: O
2011 B-DAT
1st O
international O
econference O
on O
computer O

knowledge O
engineering O
(ICCKE). O
IEEE, O
pp O
22 B-DAT

27 B-DAT

Gandomi O
AH, O
Yang O
X-S O
(2014 B-DAT

224 B-DAT

232 B-DAT

R, O
Naghibzadeh O
M, O
Yazdi O
HS O
(2010 B-DAT

with O
fuzzy O
con- O
straints. O
In: O
2010 B-DAT
international O
conference O
on O
intelligent O
systems O

J, O
Monsefi O
R, O
Yazdi O
HS O
(2015 B-DAT

Hamidzadeh O
J O
(2015 B-DAT

G-X, O
Chen O
H-F, O
Yin O
F O
(2010 B-DAT

support O
vector O
data O
description. O
In: O
2010 B-DAT
international O
conference O
onmachine O
learning O
and O

Z, O
Yin O
F, O
Guo O
K O
(2011 B-DAT

JN, O
Wang O
Y, O
Lai O
L O
(2012 B-DAT

on O
rough O
neighborhood O
approximation. O
In: O
2012 B-DAT
IEEE O
12th O
international O
conference O
on O

W-Z, O
Cornelis O
C, O
Verbiest O
N O
(2015 B-DAT

Jeong O
Y-S, O
Jayaraman O
R O
(2015 B-DAT

Y, O
Wang O
Y, O
Luo O
H O
(2015 B-DAT

Jiang O
Q, O
YanX O
(2014 B-DAT

process O
monitoring. O
Control O
Eng O
Pract O
28 B-DAT

KimD-W, O
Lee O
D, O
Lee O
KH O
(2005 B-DAT

D-W, O
Lee O
KH, O
Lee O
D O
(2007 B-DAT

284 B-DAT

289 B-DAT

T, O
Tran O
D, O
Ma O
W O
(2013 B-DAT

LichmanM O
(2013 B-DAT

S, O
Shao O
M, O
Fu O
Y O
(2014 B-DAT

202 B-DAT

LiuY-H, O
LiuY-C, O
ChenY-J O
(2010 B-DAT

detection. O
IEEE O
Trans O
Neural O
Netw O
21 B-DAT

J, O
Pan O
Q, O
Mercier O
G O
(2011 B-DAT

L, O
Hao O
Z, O
Deng O
F O
(2013 B-DAT

Q, O
Dezert O
J, O
Mercier O
G O
(2015 B-DAT

Z, O
PanQ, O
Dezert O
J,Martin O
A O
(2016 B-DAT

MartinezW,Gray O
JB O
(2016 B-DAT

MH, O
Alam O
KA, O
Petković O
D O
(2015 B-DAT

M, O
Petković O
D, O
Ch O
S O
(2015 B-DAT

A, O
Tefas O
A, O
Pitas O
I O
(2014 B-DAT

support O
vector O
data O
description. O
In: O
2014 B-DAT
IEEE O
symposium O
on O
computational O
intelligence O

A, O
Tefas O
A, O
Pitas O
I O
(2015 B-DAT

for O
video O
sum- O
marization, O
In: O
2015 B-DAT
IEEE O
international O
conference O
on O
acoustics O

signal O
processing O
(ICASSP). O
IEEE, O
pp O
2259 B-DAT

2263 B-DAT

11:341–356 O
Peng O
JX, O
Tan O
JX O
(2015 B-DAT

713:1693–1698 O
Peng O
X, O
Xu O
D O
(2012 B-DAT

novelty O
detection. O
Neural O
Comput O
Appl O
21 B-DAT

2023 B-DAT

2032 B-DAT
Petković O
D, O
Shamshirband O
S, O
Saboohi O

man O
ZA, O
Pavlović O
NT O
(2014 B-DAT

TF, O
Anuar O
NB, O
Pavlović O
ND O
(2014 B-DAT

P, O
Kim O
SB, O
Chen O
VC O
(2014 B-DAT

Tong O
CW, O
ur O
Rehman O
MH O
(2015 B-DAT

RadzikowskaAM,Kerre O
EE O
(2002 B-DAT

Ž, O
Kiah O
MLM, O
Gani O
A O
(2014 B-DAT

2075 B-DAT

2075 B-DAT

D, O
Javidnia O
H, O
Gani O
A O
(2015 B-DAT

NT, O
Ch O
S, O
AltameemTA,Gani O
A O
(2015 B-DAT

Sheskin O
DJ O
(2003 B-DAT

M, O
Nicolescu O
M, O
Bebis O
G O
(2008 B-DAT

10th O
IASTED O
international O
confer- O
ence, O
2008, B-DAT
p O
92 O

vec- O
tors. O
In: O
ESANN, O
pp O
251 B-DAT

256 B-DAT

Tax O
DM, O
Laskov O
P O
(2003 B-DAT

data O
description O
and O
back. O
In: O
2003 B-DAT
IEEE O
13th O
workshop O
on O
neural O

networks O
for O
signal O
processing, O
2003 B-DAT

domain O
description. O
Pattern O
Recogn O
Lett O
20 B-DAT

Tax O
DM, O
Duin O
RP O
(2004 B-DAT

K, O
Zidi O
S, O
Ksouri O
M O
(2015 B-DAT

N, O
Cornelis O
C, O
Herrera O
F O
(2013 B-DAT

2770 B-DAT

2782 B-DAT

Z, O
Weng O
S, O
Zhang O
C O
(2015 B-DAT

WuW-Z, O
Leung O
Y, O
Zhang O
W-X O
(2002 B-DAT

Wu O
M, O
Ye O
J O
(2009 B-DAT

2088 B-DAT

2092 B-DAT

B, O
Hao O
Z, O
Cao O
L O
(2014 B-DAT

data O
description. O
Appl O
Intell O
41:196– O
211 B-DAT

Y, O
Liu O
B, O
Cao O
L O
(2010 B-DAT

Yang O
X-S O
(2010 B-DAT

cooperative O
strategies O
for O
optimization O
(NICSO O
2010 B-DAT

Inf O
Control O
8:338–353 O
Zhai O
J O
(2011 B-DAT

Y, O
Chi O
Z-X, O
Li O
K-Q O
(2009 B-DAT

ZhengS O
(2016 B-DAT

2 B-DAT
Background O

2 B-DAT

2 B-DAT

2 B-DAT
Related O
works O

2 B-DAT

2 B-DAT

2 B-DAT
Result O
interpretation O

Breast O
cancer B-DAT
Wisconsin O

Breast O
cancer B-DAT
Wisconsin O
Class O
2 O
45.96 O
45.82 O

Breast B-DAT
cancer O
Wisconsin O

Breast B-DAT
cancer O
Wisconsin O
Class O
2 O
45.96 O

in O
their O
ground O
truth O
segmentation. O
HKU-IS B-DAT
[19] O
contains O
4447 O
challenging O
images O

Methods O
DUTS-test O
ECSSD O
HKU-IS B-DAT
PASCAL-S O
DUT-OMRON O
wFβ O
MAE O
wFβ O

standard O
benchmark O
datasets: O
DUTS-test[30], O
ECSSD[37], O
HKU B-DAT

in O
their O
ground O
truth O
segmentation. O
HKU B-DAT

Methods O
DUTS-test O
ECSSD O
HKU B-DAT

benchmark O
datasets: O
DUTS-test[30], O
ECSSD[37], O
HKU- O
IS B-DAT

IS B-DAT
[19] O
contains O
4447 O
challenging O
images O

IS B-DAT
PASCAL-S O
DUT-OMRON O
wFβ O
MAE O
wFβ O

five O
standard O
benchmark O
datasets: O
DUTS-test[30], O
ECSSD B-DAT

and O
5019 O
images O
for O
testing. O
ECSSD B-DAT
[37] O
contains O
1,000 O
images O
with O

Methods O
DUTS-test O
ECSSD B-DAT
HKU-IS O
PASCAL-S O
DUT-OMRON O
wFβ O
MAE O

on O
five O
standard O
benchmark O
datasets: O
DUTS B-DAT

HKU- O
IS[19], O
PASCAL-S[21] O
and O
DUT-OMRON[40]. O
DUTS B-DAT

Imagenet[7] O
as O
basic O
model. O
The O
DUTS B-DAT

Methods O
DUTS B-DAT

red. O
The O
test O
dataset O
is O
DUTS B-DAT

on O
five O
standard O
benchmark O
datasets: O
DUTS-test B-DAT

Methods O
DUTS-test B-DAT
ECSSD O
HKU-IS O
PASCAL-S O
DUT-OMRON O
wFβ O

red. O
The O
test O
dataset O
is O
DUTS-test B-DAT

test B-DAT

detection O
ap- O
proaches O
on O
five O
test B-DAT
datasets, O
including O
BDMPM O
[42], O
GRL O

object O
detection O
approaches O
on O
all O
test B-DAT
datasets. O
The O
best O
three O
results O

test B-DAT
ECSSD O
HKU-IS O
PASCAL-S O
DUT-OMRON O
wFβ O

object O
detection O
approaches O
on O
five O
test B-DAT
datasets O
in O
terms O
of O
PR O

the O
best O
result O
on O
five O
test B-DAT
datasets O
in O
terms O
of O
wFβ O

is O
shown O
in O
red. O
The O
test B-DAT
dataset O
is O
DUTS-test O

S B-DAT

S B-DAT
[21] O
contains O
850 O
images, O
different O

S B-DAT
DUT-OMRON O
wFβ O
MAE O
wFβ O
MAE O

References O
[1] O
A. O
Borji, O
S B-DAT

S B-DAT

Huang, O
P. O
H. O
Torr, O
and O
S B-DAT

12] O
D. O
Gilbarg O
and O
N. O
S B-DAT

T.-H. O
Lin, O
H. O
Chung, O
and O
S B-DAT

14] O
S. B-DAT
Hong, O
T. O
You, O
S O

17] O
Z. O
Jiang O
and O
L. O
S B-DAT

A. O
Achkar, O
J. O
A. O
Eichel, O
S B-DAT

33] O
T. O
Wang, O
L. O
Zhang, O
S B-DAT

datasets: O
DUTS-test[30], O
ECSSD[37], O
HKU- O
IS[19], O
PASCAL-S B-DAT

boundary O
or O
low O
color O
contrast. O
PASCAL-S B-DAT
[21] O
contains O
850 O
images, O
different O

Methods O
DUTS-test O
ECSSD O
HKU-IS O
PASCAL-S B-DAT
DUT-OMRON O
wFβ O
MAE O
wFβ O
MAE O

datasets: O
DUTS-test[30], O
ECSSD[37], O
HKU- O
IS[19], O
PASCAL B-DAT

boundary O
or O
low O
color O
contrast. O
PASCAL B-DAT

Methods O
DUTS-test O
ECSSD O
HKU-IS O
PASCAL B-DAT

ECSSD[37], O
HKU- O
IS[19], O
PASCAL-S[21] O
and O
DUT B-DAT

are O
labeled O
with O
different O
saliencies. O
DUT B-DAT

Methods O
DUTS-test B-DAT
ECSSD O
HKU-IS O
PASCAL-S O
DUT O

the O
best O
existing O
approach O
on O
DUT B-DAT

-OMRON O
dataset. O
DUT B-DAT

ECSSD[37], O
HKU- O
IS[19], O
PASCAL-S[21] O
and O
DUT-OMRON B-DAT

are O
labeled O
with O
different O
saliencies. O
DUT-OMRON B-DAT
[40] O
has O
5,168 O
high O
quality O

Methods O
DUTS-test O
ECSSD O
HKU-IS O
PASCAL-S O
DUT-OMRON B-DAT
wFβ O
MAE O
wFβ O
MAE O
wFβ O

the O
best O
existing O
approach O
on O
DUT-OMRON B-DAT
dataset. O
DUT-OMRON O
dataset O
is O
a O

OMRON B-DAT

OMRON B-DAT
[40] O
has O
5,168 O
high O
quality O

OMRON B-DAT
wFβ O
MAE O
wFβ O
MAE O
wFβ O

OMRON B-DAT
dataset. O
DUT-OMRON O
dataset O
is O
a O

Adaptation O
· O
Text O
Classification O
· O
Sentiment B-DAT
Classification O
· O
Cross-Domain O
Classification O

Figure O
1: O
Multi B-DAT

2.3 O
Multi B-DAT

plexity O
of O
tri-training, O
we O
propose O
Multi B-DAT

labels O
y1, O
.., O
yn, O
our O
Multi B-DAT

Algorithm O
3 O
Multi B-DAT

reaching O
FLORS O
performance O
on O
weblogs. O
Multi B-DAT

Multi B-DAT

and O
Anders O
Sø- O
gaard. O
2018. O
Multi B-DAT

Vinyals, O
and O
Lukasz O
Kaiser. O
2015. O
Multi B-DAT

3.2 O
Sentiment B-DAT
analysis O

Sentiment B-DAT
analysis O
We O
show O
results O
for O

Wu O
and O
Yongfeng O
Huang. O
2016. O
Sentiment B-DAT
Domain O
Adaptation O
with O
Multiple O
Sources O

Second O
Workshop O
on O
Transfer O
and O
Multi B-DAT

2014 O
Workshop O
on O
Transfer O
and O
Multi B-DAT

Jonathan O
Masci, O
and O
Jürgen O
Schmidhuber. O
Multi B-DAT

Joerg O
Liebelt O
and O
Cordelia O
Schmid. O
Multi B-DAT

5.1.3 O
Experiments O
on O
Sentiment B-DAT
Analysis O
Data O
Sets O

5.1.3 O
Experiments O
on O
Sentiment B-DAT
Analysis O
Data O
Sets O

EEG O
emotion O
recognition. O
In O
sections O
IV B-DAT
and O
V, O
we O
discuss O
the O

IV2 B-DAT
[5]. O
SEED-IV O
dataset O
also O
contains O

DE) O
in O
SEED O
and O
SEED- O
IV, B-DAT
and O
the O
Short-Time O
Fourier O
Transform O

IV B-DAT
and O
MPED O
are O
multi-modal O
datasets O

IV, B-DAT
we O
use O
the O
first O
sixteen O

IV B-DAT
and O
MPED O
datasets O

IV B-DAT
MPED O

IV, B-DAT
the O
proposed O
method O
improves O
over O

IV B-DAT
MPED O

IV B-DAT
and O
MPED O
datasets, O
which O
still O

IV B-DAT
and O
MPED O
datasets O

IV B-DAT
MPED O

IV B-DAT
dataset O
from O
Fig. O
2(b) O
and O

IV B-DAT
and O
MPED O
datasets O
by O
replacing O

IV B-DAT

IV B-DAT

IV B-DAT

IV, B-DAT
joy O
and O
funny O
in O
MPED O

IV B-DAT

IV B-DAT
(c) O
MPED O

IV B-DAT
and O
MPED O
which O
contains O
more O

results O
are O
shown O
in O
Table O
IV B-DAT

TABLE O
IV B-DAT

IV B-DAT
and O
MPED O
datasets O

IV B-DAT
MPED O

IV B-DAT
MPED O

IV B-DAT

IV, B-DAT
and O
MPED O
datasets O
respectively. O
Note O

IV B-DAT
and O
MPED O
datasets. O
Besides, O
comparing O

with O
the O
results O
in O
Table O
IV B-DAT

IV B-DAT
and O
MPED O
datasets O

IV B-DAT
MPED O

IV B-DAT
and O
MPED O
datasets O

IV B-DAT
MPED O

IV B-DAT
MPED O

IV B-DAT
Discussion O

IV B-DAT

IV B-DAT

IV B-DAT

IV B-DAT

1) O
SEED B-DAT
[21]. O
SEED O
dataset O
contains O
15 O
subjects, O
and O

2) O
SEED-IV2 B-DAT
[5]. O
SEED O

extra O
emotion O
fear O
compared O
with O
SEED, B-DAT
and O
each O
emotion O
has O
6 O

the O
differential O
entropy O
(DE) O
in O
SEED B-DAT
and O
SEED- O
IV, O
and O
the O

5] O
and O
[22]. O
Namely, O
for O
SEED, B-DAT
we O
use O
the O
former O
nine O

2Note O
that O
both O
SEED B-DAT

target O
(testing) O
domain O
data; O
for O
SEED B-DAT

dependent O
EEG O
emotion O
recognition O
on O
SEED, B-DAT
SEED-IV O
and O
MPED O
datasets O

SEED B-DAT
SEED O

Especially O
for O
the O
result O
on O
SEED B-DAT

SEED B-DAT
SEED O

91.07%, O
72.22% O
and O
38.55% O
on O
SEED, B-DAT
SEED-IV O
and O
MPED O
datasets, O
which O

independent O
EEG O
emotion O
recognition O
on O
SEED, B-DAT
SEED-IV O
and O
MPED O
datasets O

SEED B-DAT
SEED O

Fig. O
2(d) O
corresponding O
to O
the O
SEED B-DAT

two O
confusion O
matrices O
of O
the O
SEED B-DAT

subject-dependent O
EEG O
emotion O
recognition O
on O
SEED, B-DAT
SEED-IV O
and O
MPED O
datasets O
by O

a) O
SEED B-DAT
(b) O
SEED O

d) O
SEED B-DAT
(e) O
SEED O

the O
positive O
emotions O
(happy O
in O
SEED B-DAT
and O
SEED-IV, O
joy O
and O
funny O

a) O
SEED B-DAT
(b) O
SEED O

a) O
SEED B-DAT
(b) O
SEED O

the O
frontal-central O
lobe, O
especially O
in O
SEED B-DAT

more O
types O
of O
emotions O
than O
SEED B-DAT

31 O
paired O
electrodes O
especially O
on O
SEED B-DAT
dataset, O
which O
agrees O
with O
the O

subject-independent O
EEG O
emotion O
recognition O
on O
SEED, B-DAT
SEED-IV O
and O
MPED O
datasets O

SEED B-DAT
SEED O

SEED B-DAT
SEED O

1) O
SEED B-DAT

2) O
SEED B-DAT

h)-(n) O
are O
the O
results O
on O
SEED, B-DAT
SEED-IV, O
and O
MPED O
datasets O
respectively O

especially O
in O
the O
experiments O
on O
SEED B-DAT

subject-dependent O
EEG O
emotion O
recognition O
on O
SEED, B-DAT
SEED-IV O
and O
MPED O
datasets O

SEED B-DAT
SEED O

for O
EEG O
emotion O
recognition O
on O
SEED, B-DAT
SEED-IV O
and O
MPED O
datasets O

SEED B-DAT
SEED O

SEED B-DAT
SEED O

2) O
SEED-IV2 B-DAT
[5]. O
SEED-IV O
dataset O
also O
contains O
15 O
sub O

2Note O
that O
both O
SEED-IV B-DAT
and O
MPED O
are O
multi-modal O
datasets O

target O
(testing) O
domain O
data; O
for O
SEED-IV, B-DAT
we O
use O
the O
first O
sixteen O

EEG O
emotion O
recognition O
on O
SEED, O
SEED-IV B-DAT
and O
MPED O
datasets O

SEED O
SEED-IV B-DAT
MPED O

Especially O
for O
the O
result O
on O
SEED-IV, B-DAT
the O
proposed O
method O
improves O
over O

SEED O
SEED-IV B-DAT
MPED O

72.22% O
and O
38.55% O
on O
SEED, O
SEED-IV B-DAT
and O
MPED O
datasets, O
which O
still O

EEG O
emotion O
recognition O
on O
SEED, O
SEED-IV B-DAT
and O
MPED O
datasets O

SEED O
SEED-IV B-DAT
MPED O

two O
confusion O
matrices O
of O
the O
SEED-IV B-DAT
dataset O
from O
Fig. O
2(b) O
and O

EEG O
emotion O
recognition O
on O
SEED, O
SEED-IV B-DAT
and O
MPED O
datasets O
by O
replacing O

a) O
SEED O
(b) O
SEED-IV B-DAT

d) O
SEED O
(e) O
SEED-IV B-DAT

emotions O
(happy O
in O
SEED O
and O
SEED-IV, B-DAT
joy O
and O
funny O
in O
MPED O

a) O
SEED O
(b) O
SEED-IV B-DAT

a) O
SEED O
(b) O
SEED-IV B-DAT
(c) O
MPED O

the O
frontal-central O
lobe, O
especially O
in O
SEED-IV B-DAT
and O
MPED O
which O
contains O
more O

EEG O
emotion O
recognition O
on O
SEED, O
SEED-IV B-DAT
and O
MPED O
datasets O

SEED O
SEED-IV B-DAT
MPED O

SEED O
SEED-IV B-DAT
MPED O

2) O
SEED-IV B-DAT

are O
the O
results O
on O
SEED, O
SEED-IV, B-DAT
and O
MPED O
datasets O
respectively. O
Note O

especially O
in O
the O
experiments O
on O
SEED-IV B-DAT
and O
MPED O
datasets. O
Besides, O
comparing O

EEG O
emotion O
recognition O
on O
SEED, O
SEED-IV B-DAT
and O
MPED O
datasets O

SEED O
SEED-IV B-DAT
MPED O

EEG O
emotion O
recognition O
on O
SEED, O
SEED-IV B-DAT
and O
MPED O
datasets O

SEED O
SEED-IV B-DAT
MPED O

SEED O
SEED-IV B-DAT
MPED O

experiments O
are O
conducted O
in O
section O
IV B-DAT

IV B-DAT

of O
the O
DGCNN O
method. O
Table O
IV B-DAT
shows O
the O
information O
about O
the O

TABLE O
IV B-DAT
NUMBER O
OF O
THE O
PSD O
EEG O

the O
SJTU O
emotion O
EEG O
dataset O
(SEED) B-DAT
and O
DREAMER O
dataset. O
The O
experimental O

independent O
cross-validation O
one O
on O
the O
SEED B-DAT
database, O
and O
the O
average O
accuracies O

the O
SJTU O
Emotion O
EEG O
Database O
(SEED) B-DAT
[18] O
while O
the O
second O
one O

The O
SEED B-DAT
database O
contains O
EEG O
data O
of O

EEG O
Emotion O
Recognition O
Experiments O
on O
SEED B-DAT
Database O

1) O
Subject-dependent O
Experiments O
on O
SEED B-DAT
Database: O
In O
subject-dependent O
experiments, O
we O

FREQUENCY O
BAND O
ON O
SEED B-DAT
DATABASE O

EXPERIMENTS O
ON O
SEED B-DAT
DATABASE O
AMONG O
THE O
VARIOUS O
METHODS O

RECOGNITION O
EXPERIMENTS O
ON O
SEED B-DAT
DATABASE O
USING O
DGCNN O

2) O
Subject-independent O
Experiments O
on O
SEED B-DAT

independent O
cross O
validation O
experiments O
on O
SEED B-DAT
EEG O
emotion O
database O
had O
been O

given O
in O
Section O
III. O
Section O
IV B-DAT
gives O
the O
motivation O
and O
rationale O

IV B-DAT

plicit O
feature O
selection O
properties. O
Table O
IV B-DAT
shows O
the O
mean O
ac- O
curacies O

TABLE O
IV B-DAT
THE O
MEAN O
ACCURACIES O
AND O
STANDARD O

IV B-DAT

IV B-DAT
[25]. O
Experimental O
results O
demon- O
strate O

IV B-DAT

IV B-DAT
[25]. O
The O
selection O
of O
global O

IV, B-DAT
there O
are O
four O
classes: O
neutral O

IV B-DAT
datasets O
in O
our O
exper- O
iments O

IV B-DAT
dataset O
[25] O
comprises O
EEG O
data O

IV B-DAT

IV B-DAT
to O
evaluate O
our O
model O

IV, B-DAT
we O
follow O
the O
experimental O
settings O

IV, B-DAT
we O
follow O
the O
experimental O
settings O

IV B-DAT
also O
contains O
eye O
movement O
data O

IV B-DAT

IV B-DAT
Model O
delta O
band O
theta O
band O

IV B-DAT

IV B-DAT
Model O
delta O
band O
theta O
band O

IV B-DAT
using O
the O
pre- O
computed O
DE O

IV B-DAT
by O
around O
5%. O
In O
particular O

IV B-DAT
(nearly O
5% O
improvement). O
In O
addition O

SEED O
and O
19% O
on O
SEED- O
IV B-DAT
using O
SVM O
to O
around O
9 O

SEED O
and O
6% O
on O
SEED- O
IV B-DAT
using O
our O
model. O
One O
possible O

IV B-DAT
for O
sad O
emotion O
as O
well O

IV, B-DAT
our O
model O
performs O
significantly O
better O

IV B-DAT

IV B-DAT
RGNN O
85.30/06.72 O
73.84/08.02 O

IV B-DAT

IV B-DAT

IV B-DAT
subject-dependent O
SEED-IV O
subject-independent O

IV B-DAT
subject-dependent O
SEED-IV O
subject-independent O

IV B-DAT

IV B-DAT

IV B-DAT

IV, B-DAT
respectively, O
outperforming O
the O
current O
state O

settings O
on O
two O
public O
datasets: O
SEED B-DAT
and O
SEED-IV. O
Our O
model O
obtains O

Computing, O
EEG, O
Graph O
Neural O
Network, O
SEED B-DAT

two O
public O
EEG O
datasets, O
namely O
SEED B-DAT
[7] O
and O
SEED-IV O
[25]. O
Experimental O

used O
to O
collect O
data O
in O
SEED B-DAT
and O
SEED-IV. O
Gray O
symmetric O
channels O

the O
global O
connections O
in O
both O
SEED B-DAT
[7] O
and O
SEED-IV O
[25]. O
The O

The O
conversion O
is O
dataset-dependent. O
In O
SEED, B-DAT
there O
are O
three O
classes: O
nega O

In O
SEED B-DAT

5.1 O
Datasets O
We O
use O
both O
SEED B-DAT
and O
SEED-IV O
datasets O
in O
our O

exper- O
iments. O
The O
SEED B-DAT
dataset O
[7] O
comprises O
EEG O
data O

systems O
(LDS) O
[7], O
[65] O
in O
SEED B-DAT

certain O
frequency O
band O
[8]. O
In O
SEED, B-DAT
DE O
features O
are O
pre-computed O
over O

The O
SEED B-DAT

as O
the O
one O
used O
in O
SEED B-DAT

for O
each O
subject. O
Similar O
to O
SEED, B-DAT
we O
adopt O
the O
pre-computed O
DE O

features O
from O
SEED B-DAT

subject- O
independent O
classifications O
on O
both O
SEED B-DAT
and O
SEED-IV O
to O
evaluate O
our O

5.2.1 O
Subject-Dependent O
Classification O
For O
SEED, B-DAT
we O
follow O
the O
experimental O
settings O

sessions O
of O
EEG O
data O
in O
SEED B-DAT
[7]. O
For O
SEED-IV, O
we O
follow O

5.2.2 O
Subject-Independent O
Classification O
For O
SEED, B-DAT
we O
follow O
the O
experimental O
settings O

session O
of O
EEG O
data O
in O
SEED B-DAT
[14]. O
For O
SEED-IV, O
we O
follow O

2. O
SEED B-DAT

classification O
accuracy O
(mean/standard O
deviation) O
on O
SEED B-DAT
and O
SEED-IV O

SEED B-DAT
SEED O

classification O
accuracy O
(mean/standard O
deviation) O
on O
SEED B-DAT
and O
SEED-IV O

SEED B-DAT
SEED O

and O
all O
baselines O
on O
both O
SEED B-DAT
and O
SEED-IV O
using O
the O
pre O

DE O
features. O
The O
performance O
on O
SEED B-DAT
using O
DE O
feature O
in O
the O

of O
the O
state-of-the-art O
model O
on O
SEED B-DAT

marginally O
worse O
than O
BiHDM O
on O
SEED B-DAT
but O
much O
better O
than O
BiHDM O

on O
SEED B-DAT

decreasing O
from O
around O
27% O
on O
SEED B-DAT
and O
19% O
on O
SEED- O
IV O

SVM O
to O
around O
9% O
on O
SEED B-DAT
and O
6% O
on O
SEED- O
IV O

In O
subject-dependent O
experi- O
ments O
on O
SEED, B-DAT
STRNN O
achieves O
the O
highest O
accuracy O

band. O
In O
subject-independent O
experiments O
on O
SEED, B-DAT
BiDANN-S O
achieves O
the O
highest O
accuracy O

and O
subject- O
independent O
settings O
on O
SEED, B-DAT
we O
compare O
the O
perfor- O
mance O

subject-dependent O
and O
subject-independent O
settings O
on O
SEED, B-DAT
our O
model O
can O
recognize O
better O

phe- O
nomenon O
is O
observed O
in O
SEED B-DAT

3 O
(c) O
and O
(d)). O
For O
SEED B-DAT

cation O
accuracy O
(mean/standard O
deviation) O
on O
SEED B-DAT
and O
SEED-IV. O
Symbols O
- O
and O

Model O
SEED B-DAT
SEED O

RGNN. O
(a) O
subject-dependent O
classification O
on O
SEED B-DAT

. O
(b) O
subject-independent O
classification O
on O
SEED B-DAT

. O
(c) O
subject-dependent O
classification O
on O
SEED B-DAT

-IV. O
(d) O
subject-independent O
classification O
on O
SEED B-DAT

SEED B-DAT
subject-dependent O
SEED O
subject-independent O
SEED O

-IV O
subject-dependent O
SEED B-DAT

SEED B-DAT
subject-dependent O
SEED O
subject-independent O
SEED O

-IV O
subject-dependent O
SEED B-DAT

in O
subject-dependent O
classification O
on O
both O
SEED B-DAT
and O
SEED-IV. O
The O
values O
for O

subject- O
dependent O
classification O
on O
both O
SEED B-DAT
and O
SEED-IV O

settings O
on O
two O
public O
datasets O
SEED B-DAT
and O
SEED-IV. O
Our O
model O
obtains O

subject-dependent O
and O
subject-independent O
classifications O
on O
SEED B-DAT

two O
public O
datasets: O
SEED O
and O
SEED-IV B-DAT

datasets, O
namely O
SEED O
[7] O
and O
SEED-IV B-DAT
[25]. O
Experimental O
results O
demon- O
strate O

collect O
data O
in O
SEED O
and O
SEED-IV B-DAT

in O
both O
SEED O
[7] O
and O
SEED-IV B-DAT
[25]. O
The O
selection O
of O
global O

In O
SEED-IV, B-DAT
there O
are O
four O
classes: O
neutral O

We O
use O
both O
SEED O
and O
SEED-IV B-DAT
datasets O
in O
our O
exper- O
iments O

The O
SEED-IV B-DAT
dataset O
[25] O
comprises O
EEG O
data O

the O
pre-computed O
DE O
features O
from O
SEED-IV B-DAT

classifications O
on O
both O
SEED O
and O
SEED-IV B-DAT
to O
evaluate O
our O
model O

data O
in O
SEED O
[7]. O
For O
SEED-IV, B-DAT
we O
follow O
the O
experimental O
settings O

data O
in O
SEED O
[14]. O
For O
SEED-IV, B-DAT
we O
follow O
the O
experimental O
settings O

2. O
SEED-IV B-DAT
also O
contains O
eye O
movement O
data O

mean/standard O
deviation) O
on O
SEED O
and O
SEED-IV B-DAT

SEED O
SEED-IV B-DAT
Model O
delta O
band O
theta O
band O

mean/standard O
deviation) O
on O
SEED O
and O
SEED-IV B-DAT

SEED O
SEED-IV B-DAT
Model O
delta O
band O
theta O
band O

baselines O
on O
both O
SEED O
and O
SEED-IV B-DAT
using O
the O
pre- O
computed O
DE O

of O
the O
state-of-the-art O
model O
on O
SEED-IV B-DAT
by O
around O
5%. O
In O
particular O

much O
better O
than O
BiHDM O
on O
SEED-IV B-DAT
(nearly O
5% O
improvement). O
In O
addition O

phe- O
nomenon O
is O
observed O
in O
SEED-IV B-DAT
for O
sad O
emotion O
as O
well O

3 O
(c) O
and O
(d)). O
For O
SEED-IV, B-DAT
our O
model O
performs O
significantly O
better O

mean/standard O
deviation) O
on O
SEED O
and O
SEED-IV B-DAT

Model O
SEED O
SEED-IV B-DAT
RGNN O
85.30/06.72 O
73.84/08.02 O

SEED. O
(c) O
subject-dependent O
classification O
on O
SEED-IV B-DAT

. O
(d) O
subject-independent O
classification O
on O
SEED-IV B-DAT

SEED O
subject-dependent O
SEED O
subject-independent O
SEED-IV B-DAT
subject-dependent O
SEED-IV O
subject-independent O

SEED O
subject-dependent O
SEED O
subject-independent O
SEED-IV B-DAT
subject-dependent O
SEED-IV O
subject-independent O

classification O
on O
both O
SEED O
and O
SEED-IV B-DAT

classification O
on O
both O
SEED O
and O
SEED-IV B-DAT

two O
public O
datasets O
SEED O
and O
SEED-IV B-DAT

subject-dependent O
and O
subject-independent O
classifications O
on O
SEED-IV, B-DAT
respectively, O
outperforming O
the O
current O
state O

seed B-DAT

Crowd O
Instance-level O
Human O
Parsing), O
MHP O
v2.0 B-DAT
(Multi- O
Human O
Parsing) O
and O
DensePose-COCO O

Instance-level O
Human O
Parsing) O
and O
MHP O
v2.0 B-DAT
[43] O
(Multi- O
Human O
Parsing) O
datasets O

43] O
put O
forward O
the O
MHP O
v2.0 B-DAT
(Multi-Human O
Parsing) O
dataset, O
which O
contains O

86% O
in O
CIHP O
and O
MHP O
v2.0 B-DAT
datasets O
respectively. O
According O
to O
the O

and O
40k O
iteration. O
For O
MHP O
v2.0 B-DAT
dataset, O
the O
max O
iteration O
is O

part O
segmentation O
results O
on O
MHP O
v2.0 B-DAT
val, O
we O
adopt O
ResNet50-FPN O
as O

v2.0 B-DAT
NAN O
[43] O
– O
25.1 O
41.7 O

methods O
on O
CIHP O
and O
MHP O
v2.0 B-DAT
val. O
† O
denotes O
using O
test-time O

Component O
Ablation O
Studies O
on O
MHP O
v2.0 B-DAT

for O
ablation O
studies O
on O
MHP O
v2.0 B-DAT
[43] O
val, O
the O
results O
are O

semantic O
categories O
in O
the O
MHP O
v2.0 B-DAT
dataset, O
and O
some O
of O
them O

signifi- O
cantly O
improving O
for O
MHP O
v2.0 B-DAT
dataset, O
which O
yields O
10.3 O
improvement O

methods O
on O
CIHP O
and O
MHP O
v2.0 B-DAT
datasets, O
respectively O

For O
MHP O
v2.0 B-DAT
dataset, O
we O
also O
report O
the O

results O
evalu- O
ated O
on O
MHP O
v2.0 B-DAT
test O

ResNet50-FPN O
on O
CIHP O
val, O
MHP O
v2.0 B-DAT
val O
and O
DensePose-COCO O
val, O
respectively O

ResNet50-FPN O
on O
CIHP O
val, O
MHP O
v2.0 B-DAT
val O
and O
DensePose-COCO O
val, O
respectively O

CIHP O
(Crowd O
Instance-level O
Human O
Parsing), O
MHP B-DAT
v2.0 O
(Multi- O
Human O
Parsing) O
and O

Crowd O
Instance-level O
Human O
Parsing) O
and O
MHP B-DAT
v2.0 O
[43] O
(Multi- O
Human O
Parsing O

al. O
[43] O
put O
forward O
the O
MHP B-DAT
v2.0 O
(Multi-Human O
Parsing) O
dataset, O
which O

and O
86% O
in O
CIHP O
and O
MHP B-DAT
v2.0 O
datasets O
respectively. O
According O
to O

30k O
and O
40k O
iteration. O
For O
MHP B-DAT
v2.0 O
dataset, O
the O
max O
iteration O

Human O
part O
segmentation O
results O
on O
MHP B-DAT
v2.0 O
val, O
we O
adopt O
ResNet50-FPN O

MHP B-DAT

state-of-the-art O
methods O
on O
CIHP O
and O
MHP B-DAT
v2.0 O
val. O
† O
denotes O
using O

Component O
Ablation O
Studies O
on O
MHP B-DAT
v2.0. O
We O
also O
grad- O
ually O

PBD) O
for O
ablation O
studies O
on O
MHP B-DAT
v2.0 O
[43] O
val, O
the O
results O

59 O
semantic O
categories O
in O
the O
MHP B-DAT
v2.0 O
dataset, O
and O
some O
of O

also O
signifi- O
cantly O
improving O
for O
MHP B-DAT
v2.0 O
dataset, O
which O
yields O
10.3 O

state-of-the-art O
methods O
on O
CIHP O
and O
MHP B-DAT
v2.0 O
datasets, O
respectively O

For O
MHP B-DAT
v2.0 O
dataset, O
we O
also O
report O

the O
results O
evalu- O
ated O
on O
MHP B-DAT
v2.0 O
test O

using O
ResNet50-FPN O
on O
CIHP O
val, O
MHP B-DAT
v2.0 O
val O
and O
DensePose-COCO O
val O

ing O
ResNet50-FPN O
on O
CIHP O
val, O
MHP B-DAT
v2.0 O
val O
and O
DensePose-COCO O
val O

CIHP O
(Crowd O
Instance-level O
Human O
Parsing), O
MHP B-DAT
v2 I-DAT

Crowd O
Instance-level O
Human O
Parsing) O
and O
MHP B-DAT
v2 I-DAT

al. O
[43] O
put O
forward O
the O
MHP B-DAT
v2 I-DAT

and O
86% O
in O
CIHP O
and O
MHP B-DAT
v2 I-DAT

30k O
and O
40k O
iteration. O
For O
MHP B-DAT
v2 I-DAT

Human O
part O
segmentation O
results O
on O
MHP B-DAT
v2 I-DAT

state-of-the-art O
methods O
on O
CIHP O
and O
MHP B-DAT
v2 I-DAT

Component O
Ablation O
Studies O
on O
MHP B-DAT
v2 I-DAT

PBD) O
for O
ablation O
studies O
on O
MHP B-DAT
v2 I-DAT

59 O
semantic O
categories O
in O
the O
MHP B-DAT
v2 I-DAT

also O
signifi- O
cantly O
improving O
for O
MHP B-DAT
v2 I-DAT

state-of-the-art O
methods O
on O
CIHP O
and O
MHP B-DAT
v2 I-DAT

For O
MHP B-DAT
v2 I-DAT

the O
results O
evalu- O
ated O
on O
MHP B-DAT
v2 I-DAT

using O
ResNet50-FPN O
on O
CIHP O
val, O
MHP B-DAT
v2 I-DAT

ing O
ResNet50-FPN O
on O
CIHP O
val, O
MHP B-DAT
v2 I-DAT

CIHP O
(Crowd O
Instance-level O
Human O
Parsing), O
MHP B-DAT
v2.0 I-DAT
(Multi- O
Human O
Parsing) O
and O
DensePose-COCO O

Crowd O
Instance-level O
Human O
Parsing) O
and O
MHP B-DAT
v2.0 I-DAT
[43] O
(Multi- O
Human O
Parsing) O
datasets O

al. O
[43] O
put O
forward O
the O
MHP B-DAT
v2.0 I-DAT
(Multi-Human O
Parsing) O
dataset, O
which O
contains O

and O
86% O
in O
CIHP O
and O
MHP B-DAT
v2.0 I-DAT
datasets O
respectively. O
According O
to O
the O

30k O
and O
40k O
iteration. O
For O
MHP B-DAT
v2.0 I-DAT
dataset, O
the O
max O
iteration O
is O

Human O
part O
segmentation O
results O
on O
MHP B-DAT
v2.0 I-DAT
val, O
we O
adopt O
ResNet50-FPN O
as O

state-of-the-art O
methods O
on O
CIHP O
and O
MHP B-DAT
v2.0 I-DAT
val. O
† O
denotes O
using O
test-time O

Component O
Ablation O
Studies O
on O
MHP B-DAT
v2.0 I-DAT

PBD) O
for O
ablation O
studies O
on O
MHP B-DAT
v2.0 I-DAT
[43] O
val, O
the O
results O
are O

59 O
semantic O
categories O
in O
the O
MHP B-DAT
v2.0 I-DAT
dataset, O
and O
some O
of O
them O

also O
signifi- O
cantly O
improving O
for O
MHP B-DAT
v2.0 I-DAT
dataset, O
which O
yields O
10.3 O
improvement O

state-of-the-art O
methods O
on O
CIHP O
and O
MHP B-DAT
v2.0 I-DAT
datasets, O
respectively O

For O
MHP B-DAT
v2.0 I-DAT
dataset, O
we O
also O
report O
the O

the O
results O
evalu- O
ated O
on O
MHP B-DAT
v2.0 I-DAT
test O

using O
ResNet50-FPN O
on O
CIHP O
val, O
MHP B-DAT
v2.0 I-DAT
val O
and O
DensePose-COCO O
val, O
respectively O

ing O
ResNet50-FPN O
on O
CIHP O
val, O
MHP B-DAT
v2.0 I-DAT
val O
and O
DensePose-COCO O
val, O
respectively O

v2.0 O
(Multi- O
Human O
Parsing) O
and O
DensePose B-DAT

in O
the O
COCO O
2018 O
Challenge O
DensePose B-DAT
Estimation O
task. O
Code O
and O
models O

achieves O
64.1% O
mAP O
on O
COCO O
DensePose B-DAT
[14] O
test O
dataset, O
winning O
the O

DensePose B-DAT
task O
by O
a O
very O
large O

dataset O
for O
instance-level O
human O
analysis, O
DensePose B-DAT

body. O
They O
also O
present O
the O
DensePose B-DAT

system O
that O
further O
improves O
accuracy. O
DensePose B-DAT

segmen- O
tation O
masks, O
and O
the O
DensePose B-DAT

change O
points O
scaled O
proportionally. O
For O
DensePose B-DAT

Dense O
pose O
estimation O
results O
on O
DensePose B-DAT

backbone O
respectively. O
The O
baseline O
is O
DensePose B-DAT

val, O
MHP O
v2.0 O
val O
and O
DensePose B-DAT

DensePose B-DAT

Component O
Ablation O
Studies O
on O
DensePose B-DAT

Parsing O
R-CNN O
outperforms O
the O
baseline O
(DensePose B-DAT

which O
yields O
6.1 O
improvement O
than O
DensePose B-DAT

pated O
in O
the O
COCO O
2018 O
DensePose B-DAT
Estimation O
Challenge, O
and O
reach O
the O

and O
attains O
64.1% O
AP O
on O
DensePose B-DAT

val, O
MHP O
v2.0 O
val O
and O
DensePose B-DAT

in O
the O
COCO O
2018 O
Challenge O
DensePose B-DAT
Estimation O
task. O
In O
the O
future O

COCO B-DAT
datasets. O
Based O
on O
the O
proposed O

the O
1st O
place O
in O
the O
COCO B-DAT
2018 O
Challenge O
DensePose O
Estimation O
task O

R-CNN O
achieves O
64.1% O
mAP O
on O
COCO B-DAT
DensePose O
[14] O
test O
dataset, O
winning O

the O
1st O
place O
in O
COCO B-DAT
2018 O
Challenge O

for O
instance-level O
human O
analysis, O
DensePose- O
COCO, B-DAT
a O
large-scale O
ground-truth O
dataset O
with O

correspondences O
manually O
annotated O
on O
50k O
COCO B-DAT
images. O
Dense O
pose O
estimation O
can O

20% O
of O
object O
instances O
in O
COCO B-DAT
dataset O
occupy O
more O
than O
10 O

COCO B-DAT
[29] O
1x O
55.9 O
63.1 O
53.5 O

COCO, B-DAT
we O
train O
for O
130k O
iterations O

ERR O
GCE O
PBD O
3x O
LR O
COCO B-DAT
mIoU O
APp50 O
AP O
p O
vol O

pretraining O
the O
whole O
model O
on O
COCO B-DAT
keypoint O
annotations O
(COCO O

ERR O
GCE O
PBD O
3x O
LR O
COCO B-DAT
mIoU O
APp50 O
AP O
p O
vol O

the O
number O
of O
iterations O
and O
COCO B-DAT
pretraining. O
Our O
ablation O
study O
on O

Baseline O
PSS O
ERR O
GCE O
PBD O
COCO B-DAT
AP O
AP50 O
AP75 O
APM O
APL O

COCO B-DAT
val. O
We O
adopt O
ResNet50-FPN O
and O

5) O
Increasing O
iterations O
and O
COCO B-DAT
pretraining. O
Increas- O
ing O
iterations O
is O

Parsing O
R-CNN O
models O
on O
the O
COCO B-DAT
keypoints O
annotations2, O
and O
initialize O
the O

ERR) O
achieves O
66.2% O
AP O
on O
COCO B-DAT
val, O
which O
yields O
0.8 O
improvement O

COCO B-DAT
val, O
respectively O

Table O
10. O
2018 O
COCO B-DAT
Challenge O
results O
of O
Dense O
Pose O

COCO B-DAT

6.1 O
improvement O
than O
DensePose-RCNN. O
With O
COCO B-DAT
pretraining, O
Parsing O
R-CNN O
further O
improves O

COCO B-DAT
2018 O
Challenge. O
With O
Parsing O
R-CNN O

we O
partici- O
pated O
in O
the O
COCO B-DAT
2018 O
DensePose O
Estimation O
Challenge, O
and O

entries O
from O
the O
leaderboard O
of O
COCO B-DAT
2018 O
Chal- O
lenge. O
Our O
entry O

COCO B-DAT
test O
which O
surpasses O
the O
2nd O

COCO B-DAT
val, O
respectively O

the O
1st O
place O
in O
the O
COCO B-DAT
2018 O
Challenge O
DensePose O
Estimation O
task O

v2.0 O
(Multi- O
Human O
Parsing) O
and O
DensePose-COCO B-DAT
datasets. O
Based O
on O
the O
proposed O

change O
points O
scaled O
proportionally. O
For O
DensePose-COCO, B-DAT
we O
train O
for O
130k O
iterations O

Dense O
pose O
estimation O
results O
on O
DensePose-COCO B-DAT
val. O
We O
adopt O
ResNet50-FPN O
and O

val, O
MHP O
v2.0 O
val O
and O
DensePose-COCO B-DAT
val, O
respectively O

Component O
Ablation O
Studies O
on O
DensePose-COCO B-DAT

and O
attains O
64.1% O
AP O
on O
DensePose-COCO B-DAT
test O
which O
surpasses O
the O
2nd O

val, O
MHP O
v2.0 O
val O
and O
DensePose-COCO B-DAT
val, O
respectively O

DensePose B-DAT

DensePose B-DAT
COCO O
DatasetDensePose-RCNN O
Results O

the O
human O
body. O
We O
introduce O
DensePose B-DAT

50K O
COCO O
images O
and O
train O
DensePose B-DAT

and O
the O
regressed O
correspondence O
by O
DensePose B-DAT

-RCNN, O
Middle: O
DensePose B-DAT
COCO O
Dataset O
annotations, O
Right: O
Partitioning O

COCO O
dataset, O
yielding O
our O
new O
DensePose B-DAT

DensePose B-DAT
Dataset O

DensePose B-DAT
dataset, O
alongside O
with O
evaluation O
measures O

DensePose B-DAT

We O
develop O
cascaded O
extensions O
of O
DensePose B-DAT

Figure O
7: O
DensePose B-DAT

DensePose B-DAT
head O

refined O
output O
(DensePose B-DAT

DensePose B-DAT
losses O

output O
(masks O
/ O
keypoints) O
DensePose B-DAT

output O
(DensePose B-DAT

Fig. O
7 O
feeds O
into O
the O
DensePose B-DAT
net- O
work O
as O
well O
as O

DensePose B-DAT

DensePose* B-DAT
DensePose O

DensePose B-DAT

0.155 O
0.306 O
SMPLify-14 O
0.226 O
0.416 O
DensePose B-DAT
0.429 O
0.630 O

All O
images O
SMPLify-14 O
0.099 O
0.19 O
DensePose B-DAT
0.378 O
0.614 O

SR O
+ O
UP O
0.201 O
0.424 O
DensePose B-DAT
+ O
SR O
0.357 O
0.592 O
DensePose O

0.378 O
0.614 O
DensePose B-DAT

curate O
results O
than O
surrogate O
datasets. O
DensePose B-DAT

performance O
of O
our O
pro- O
posed O
DensePose B-DAT

the O
usefulness O
of O
the O
COCO- O
DensePose B-DAT
dataset O

4.1.1 O
by O
comparing O
the O
COCO- O
DensePose B-DAT
dataset O
to O
other O
sources O
of O

DensePose B-DAT
improves O
the O
accuracy O
of O
dense O

Pose O
dataset, O
while O
combining O
the O
DensePose B-DAT
with O
SUR- O
REAL O
results O
in O

DensePose B-DAT

Figure O
13: O
Qualitative O
evaluation O
of O
DensePose B-DAT

-RCNN. O
Left: O
input, O
Right: O
DensePose B-DAT

of O
having O
at O
our O
disposal O
DensePose B-DAT

established O
the O
merit O
of O
the O
DensePose B-DAT

DensePose B-DAT

lower O
than O
that O
of O
the O
DensePose B-DAT
curve O
in O
Fig. O
12. O
Even O

DensePose B-DAT

DensePose B-DAT

supervision O
signal O
delivered O
by O
our O
DensePose B-DAT

DensePose B-DAT

DensePose B-DAT

DensePose B-DAT

on O
the O
ResNet-50-FPN O
version O
of O
DensePose B-DAT

DensePose, B-DAT
a O
large-scale O
dataset O
of O
ground-truth O

AR O
AR50 O
AP75 O
ARM O
ARL O
DensePose B-DAT
(ResNet-50) O
51.0 O
83.5 O
54.2 O
39.4 O

60.1 O
88.5 O
64.5 O
42.0 O
61.3 O
DensePose B-DAT
(ResNet-101) O
51.8 O
83.7 O
56.3 O
42.2 O

Multi-task O
learning O
DensePose B-DAT
+ O
masks O
51.9 O
85.5 O
54.7 O

DensePose B-DAT
+ O
keypoints O
52.8 O
85.6 O
56.2 O

Multi-task O
learning O
with O
cascading O
DensePose B-DAT

60.4 O
88.9 O
65.3 O
43.3 O
61.6 O
DensePose B-DAT
+ O
masks O
52.8 O
85.5 O
56.1 O

62.0 O
89.7 O
67.0 O
42.4 O
63.3 O
DensePose B-DAT
+ O
keypoints O
55.8 O
87.5 O
61.2 O

Table O
1: O
Per-instance O
evaluation O
of O
DensePose B-DAT

are O
based O
on O
ResNet-50 O
architecture. O
DensePose B-DAT

DensePose O
COCO B-DAT
DatasetDensePose-RCNN O
Results O

COCO, B-DAT
a O
large-scale O
ground-truth O
dataset O
with O

correspondences O
manually O
annotated O
on O
50K O
COCO B-DAT
images O
and O
train O
DensePose-RCNN, O
to O

correspondence O
by O
DensePose-RCNN, O
Middle: O
DensePose O
COCO B-DAT
Dataset O
annotations, O
Right: O
Partitioning O
and O

50K O
persons O
appearing O
in O
the O
COCO B-DAT
dataset O
by O
intro- O
ducing O
an O

for O
50K O
images O
of O
the O
COCO B-DAT
dataset, O
yielding O
our O
new O
DensePose-COCO O

and O
persons O
appearing O
in O
the O
COCO B-DAT
dataset. O
This O
is O
accomplished O
through O

2. O
COCO B-DAT

this O
Section O
we O
introduce O
our O
COCO B-DAT

for O
pose O
evaluation O
on O
the O
COCO B-DAT
dataset O
[26, O
36], O
we O
introduce O

is O
performed, O
we O
follow O
the O
COCO B-DAT
challenge O
protocol O
[26, O
37] O
and O

comes O
challenging O
for O
humans O
in O
COCO B-DAT

Our O
test-set O
coincides O
with O
the O
COCO B-DAT
keypoints-minival O
partition O
used O
by O
[15 O

the O
training O
set O
with O
the O
COCO B-DAT

for O
the O
remainder O
of O
the O
COCO B-DAT
dataset, O
which O
will O
soon O
allow O

real- O
istic O
images O
from O
the O
COCO B-DAT
dataset O
in- O
cluding O
multiple O
persons O

assess O
the O
usefulness O
of O
the O
COCO B-DAT

Sec. O
4.1.1 O
by O
comparing O
the O
COCO B-DAT

We O
start O
by O
assessing O
whether O
COCO B-DAT

get O
by O
us- O
ing O
the O
COCO B-DAT
human O
segmentation O
masks O
in O
order O

COCO B-DAT
for O
discriminative O
training O

COCO B-DAT
dataset, O
we O
now O
turn O
to O

trained O
models. O
We O
have O
introduced O
COCO B-DAT

evaluation O
of O
DensePose-RCNN O
performance O
on O
COCO B-DAT
minival O
subset. O
All O
multi-task O
experiments O

Lin O
for O
his O
help O
with O
COCO B-DAT

and O
C. O
L. O
Zitnick. O
Microsoft O
COCO B-DAT

the O
human O
body. O
We O
introduce O
DensePose-COCO, B-DAT
a O
large-scale O
ground-truth O
dataset O
with O

COCO O
dataset, O
yielding O
our O
new O
DensePose-COCO B-DAT
dataset O

of O
having O
at O
our O
disposal O
DensePose-COCO B-DAT
for O
discriminative O
training O

established O
the O
merit O
of O
the O
DensePose-COCO B-DAT
dataset, O
we O
now O
turn O
to O

outperforms O
all O
state-of-the-art O
methods O
on O
CIHP B-DAT
(Crowd O
Instance-level O
Human O
Parsing), O
MHP O

or O
bottom-up O
methods O
both O
on O
CIHP B-DAT
[12] O
(Crowd O
Instance-level O
Human O
Parsing O

Crowd O
Instance-level O
Human O
Pars- O
ing O
(CIHP) B-DAT
dataset, O
which O
has O
38,280 O
diverse O

human O
images. O
Each O
image O
in O
CIHP B-DAT
is O
labeled O
with O
pixel-wise O
annotations O

about O
74% O
and O
86% O
in O
CIHP B-DAT
and O
MHP O
v2.0 O
datasets O
respectively O

and O
maximum O
itera- O
tions O
on O
CIHP B-DAT
val O

scale O
of O
800 O
pixels. O
For O
CIHP B-DAT
dataset, O
we O
train O
on O
train O

half O
as O
long O
as O
the O
CIHP B-DAT
dataset O
with O
the O
learning O
rate O

Human O
part O
segmentation O
results O
on O
CIHP B-DAT
val. O
We O
adopt O
ResNet50-FPN O
as O

CIHP B-DAT

Results O
of O
state-of-the-art O
methods O
on O
CIHP B-DAT
and O
MHP O
v2.0 O
val O

Component O
Ablation O
Studies O
on O
CIHP B-DAT

pretraining. O
Our O
ablation O
study O
on O
CIHP B-DAT
[12] O
val O
from O
the O
baseline O

as O
the O
standard O
schedule O
on O
CIHP B-DAT
val O
and O
find O
the O
improvements O

the O
baseline O
is O
worse O
than O
CIHP B-DAT
dataset. O
Parsing O
R-CNN O
is O
also O

to O
the O
state-of-the-art O
methods O
on O
CIHP B-DAT
and O
MHP O
v2.0 O
datasets, O
respectively O

For O
CIHP B-DAT
dataset, O
Parsing O
R-CNN O
using O
ResNet-50-FPN O

Parsing O
R-CNN O
using O
ResNet50-FPN O
on O
CIHP B-DAT
val, O
MHP O
v2.0 O
val O
and O

R-CNN O
us- O
ing O
ResNet50-FPN O
on O
CIHP B-DAT
val, O
MHP O
v2.0 O
val O
and O

all O
state-of-the-art O
methods O
on O
CIHP O
(Crowd B-DAT
Instance-level I-DAT
Human I-DAT
Parsing), I-DAT
MHP O
v2.0 O
(Multi- O
Human O
Parsing O

methods O
both O
on O
CIHP O
[12] O
(Crowd B-DAT
Instance-level I-DAT
Human I-DAT
Parsing) I-DAT
and O
MHP O
v2.0 O
[43] O
(Multi O

present O
another O
large-scale O
dataset O
called O
Crowd B-DAT
Instance-level I-DAT
Human I-DAT
Pars- I-DAT
ing O
(CIHP) O
dataset, O
which O
has O

newly O
collected O
multi-person O
parsing O
dataset O
(CIHP) B-DAT
including O
38,280 O
diverse O
images, O
which O

more O
advanced O
human O
analysis. O
The O
CIHP B-DAT
benchmark O
and O
our O
source O
code O

large-scale O
“Crowd O
Instance-level O
Human O
Parsing O
(CIHP B-DAT

as O
Crowd O
Instance-level O
Human O
Parsing O
(CIHP), B-DAT
including O
38,280 O
multi-person O
images O
with O

PASCAL-Person-Part O
[6] O
and O
our O
new O
CIHP B-DAT
dataset O

containing O
38,280 O
images, O
the O
proposed O
CIHP B-DAT
dataset O
is O
the O
first O
and O

and O
fashion O
modeling O
[36], O
our O
CIHP B-DAT
that O
mainly O
focuses O
on O
instance-level O

standard O
server O
benchmark O
for O
our O
CIHP B-DAT
can O
facilitate O
the O
human O
analysis O

50,462 O
30,462 O
10,000 O
10,000 O
20 O
CIHP B-DAT
3.4 O
38,280 O
28,280 O
5,000 O
5,000 O

called O
Crowd O
Instance-level O
Human O
Parsing O
(CIHP) B-DAT
Dataset, O
which O
has O
several O
appealing O

parsing O
dataset O
to O
date. O
Second, O
CIHP B-DAT
is O
annotated O
with O
rich O
information O

semantic O
part O
labels O
in O
the O
CIHP B-DAT
dataset O

in O
Fig. O
1. O
With O
the O
CIHP B-DAT
dataset, O
we O
propose O
a O
new O

The O
images O
in O
the O
CIHP B-DAT
are O
collected O
from O
unconstrained O
resources O

are O
kept O
to O
construct O
the O
CIHP B-DAT
dataset. O
Following O
random O
selection, O
we O

images O
and O
categories O
in O
the O
CIHP B-DAT
dataset O
with O
more O
statistical O
details O

image, O
all O
images O
of O
the O
CIHP B-DAT
dataset O
contain O
two O
or O
more O

semantic O
part O
labels O
in O
the O
CIHP B-DAT
are O
Hat, O
Hair O

6] O
and O
20 O
for O
our O
CIHP B-DAT
dataset O

components O
of O
PGN O
on O
the O
CIHP B-DAT

5.3 O
CIHP B-DAT
Dataset O

our O
PGN O
architecture O
on O
the O
CIHP B-DAT
test O
set, O
as O
shown O
in O

human O
parsing, O
the O
performance O
on O
CIHP B-DAT
is O
worse O
than O
those O
on O

PASCAL-Person-Part O
[6], O
because O
the O
CIHP B-DAT
dataset O
contains O
more O
instances O
with O

6. O
However, O
the O
images O
in O
CIHP B-DAT
are O
high-quality O
with O
higher O
resolutions O

PASCAL-Person-Part O
CIHP B-DAT

by O
our O
PGN O
on O
the O
CIHP B-DAT
dataset O
are O
presented O
vertically O

PASCAL-Person-Part O
dataset O
[6] O
and O
the O
CIHP B-DAT
dataset O
are O
visualized O
in O
Fig O

on O
PASCAL-Person-Part O
[6] O
and O
our O
CIHP B-DAT
dataset O
demonstrate O
the O
superiority O
of O

Crowd B-DAT
Instance-level I-DAT
Human I-DAT
Parsing I-DAT
(CIHP)” O
dataset, O
which O
contains O
38,280 O

new O
large-scale O
dataset, O
named O
as O
Crowd B-DAT
Instance-level I-DAT
Human I-DAT
Parsing I-DAT
(CIHP), O
including O
38,280 O
multi-person O
images O

3 O
Crowd B-DAT
Instance-level I-DAT
Human I-DAT
Parsing I-DAT
Dataset O

build O
a O
large-scale O
dataset O
called O
Crowd B-DAT
Instance-level I-DAT
Human I-DAT
Parsing I-DAT
(CIHP) O
Dataset, O
which O
has O
several O

observing O
faster O
convergence O
on O
the O
ResNet-50 B-DAT
and O
ResNet-152 O
architectures O
[10]. O
We O

second-order O
method O
to O
train O
a O
ResNet-50 B-DAT
to O
75% O
top-1 O
accuracy O
in O

their O
default O
hyperparameter O
settings O
on O
ImageNet, B-DAT
CIFAR- O
10/100, O
neural O
machine O
translation O

on O
the O
CIFAR O
[18] O
and O
ImageNet B-DAT
datasets O
[5], O
observing O
faster O
convergence O

classification O
on O
CIFAR-10/CIFAR-100 O
[18] O
and O
ImageNet B-DAT
[5]. O
We O
also O
trained O
LSTM O

single O
crop O
validation O
accuracies O
on O
ImageNet B-DAT

Figure O
6: O
ImageNet B-DAT
training O
loss. O
The O
asterisk O
denotes O

5.2 O
ImageNet B-DAT

The O
1000-way O
ImageNet B-DAT
task O
[5] O
is O
a O
classification O

single O
crop O
top-1 O
accuracy O
on O
ImageNet B-DAT
in O
just O
50 O
epochs O
and O

approaches O
for O
improving O
convergence O
on O
ImageNet B-DAT
can O
require O
hundreds O
of O
GPUs O

C.2 O
ImageNet B-DAT

5.2 O
ImageNet B-DAT

C.2 O
ImageNet B-DAT

50 B-DAT
and O
ResNet-152 O
architectures O
[10]. O
We O

0 O
25 O
50 B-DAT
75 O
100 O
125 O
150 O
175 O

EPOCH O
50 B-DAT
- O
TOP O
1 O
75.13 O
74.43 O

EPOCH O
50 B-DAT
- O
TOP O
5 O
92.22 O
92.15 O

into O
a O
training O
set O
with O
50, B-DAT

1.28 O
million O
training O
images O
and O
50, B-DAT

PyTorch O
implementation3 O
and O
the O
ResNet- O
50 B-DAT
and O
ResNet-152 O
[10] O
architectures. O
Our O

accuracy O
on O
ImageNet O
in O
just O
50 B-DAT
epochs O
and O
reach O
75.5% O
top-1 O

0 O
10000 O
20000 O
30000 O
40000 O
50000 B-DAT
Inner O
Loop O
(Fast O
Weights) O
Steps O

50 B-DAT

50 B-DAT

50 B-DAT

50 B-DAT
to O
75% O
top-1 O
accuracy O
in O

0 O
25 O
50 B-DAT
75 O
100 O
125 O
150 O
175 O

0 O
25 O
50 B-DAT
75 O
100 O
125 O
150 O
175 O

0 O
25 O
50 B-DAT
75 O
100 O
125 O
150 O
175 O

50 B-DAT
on O
imagenet O
in O
35 O
epochs O

is O
extracted O
and O
mirrored O
horizontally O
50 B-DAT

0 O
25 O
50 B-DAT
75 O
100 O
125 O
150 O
175 O

50, B-DAT
30, O
10, O
5, O
2.5, O
1 O

50 B-DAT

0 O
25 O
50 B-DAT
75 I-DAT
100 O
125 O
150 O
175 O
200 O

PyTorch O
implementation3 O
and O
the O
ResNet- O
50 B-DAT

accuracy O
on O
ImageNet O
in O
just O
50 B-DAT

50 B-DAT
1.75 I-DAT

50 B-DAT
2.75 I-DAT

50 B-DAT
Tr I-DAT
ai O

50 B-DAT

0 O
25 O
50 B-DAT
75 I-DAT
100 O
125 O
150 O
175 O
200 O

0 O
25 O
50 B-DAT
75 I-DAT
100 O
125 O
150 O
175 O
200 O

0 O
25 O
50 B-DAT
75 I-DAT
100 O
125 O
150 O
175 O
200 O

50 B-DAT

0 O
25 O
50 B-DAT
75 I-DAT
100 O
125 O
150 O
175 O
200 O

- B-DAT

- B-DAT
10/100, O
neural O
machine O
translation, O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ball O
[32] O
and O
Nesterov O
momentum O

-50 B-DAT
and O
ResNet-152 O
architectures O
[10]. O
We O

- B-DAT

- B-DAT

- B-DAT

-32 B-DAT
test O
accuracy O
surface O
at O
epoch O

-100 B-DAT

- B-DAT

-32 B-DAT
model O
on O
CIFAR-100. O
While O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
training O
loss O
with O
fixed O
and O

- B-DAT

- B-DAT

-18 B-DAT
trained O
on O
CIFAR-10 O
with O
two O

- B-DAT

- B-DAT
gence O
of O
optimizers O
with O
the O

- B-DAT
ters O
such O
that O
the O
variance O

- B-DAT
head O
and O
SGD. O
At O
this O

- B-DAT

- B-DAT
tum O
versus O
Lookahead O
wrapping O
classical O

- B-DAT

- B-DAT
derstand O
the O
sensitivity O
of O
Lookahead O

- B-DAT
cal O
momentum O
and O
explore O
the O

- B-DAT
vergence O
rate O
over O
varying O
condition O

- B-DAT

- B-DAT
tum O
is O
set O
too O
low O

- B-DAT
timum. O
However, O
when O
the O
system O

- B-DAT

- B-DAT
cur) O
Lookahead O
is O
able O
to O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-100 B-DAT
[18] O
and O
ImageNet O
[5]. O
We O

-10 B-DAT
Train O
Loss O

-10 B-DAT
CIFAR-100 O

-100 B-DAT

-18 B-DAT
validation O
accuracies O
with O
various O
optimizers O

EPOCH O
50 O
- B-DAT
TOP O
1 O
75.13 O
74.43 O
EPOCH O

50 O
- B-DAT
TOP O
5 O
92.22 O
92.15 O
EPOCH O

60 O
- B-DAT
TOP O
1 O
75.49 O
75.15 O
EPOCH O

60 O
- B-DAT
TOP O
5 O
92.53 O
92.56 O

-1 B-DAT
and O
Top-5 O
single O
crop O
validation O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

-10 B-DAT
and O
CIFAR-100 O
datasets O
for O
classification O

-18 B-DAT
[10] O
with O
batches O
of O
128 O

- B-DAT

- B-DAT
50 O
and O
ResNet-152 O
[10] O
architectures O

-1 B-DAT
accuracy O
on O
ImageNet O
in O
just O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
The O

-152 B-DAT

-1 B-DAT
accuracy O
in O
49 O
epochs O
(matching O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
Other O

- B-DAT
scribed O
in O
Vaswani O
et O
al O

-14 B-DAT
machine O
translation O
task O

- B-DAT
plexity O
on O
the O
Penn O
Treebank O

LA(ADAM) O
31.92 O
60.28 O
57.72 O
POLYAK O
- B-DAT
61.18 O
58.79 O

- B-DAT

- B-DAT

- B-DAT

-) B-DAT
24.3 O
24.4 O
ADAFACTOR O
24.17 O
24.51 O

- B-DAT

- B-DAT

-50 B-DAT
to O
75% O
top-1 O
accuracy O
in O

-152 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
LR O

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
momentum O

-100 B-DAT

- B-DAT
racy O
than O
SGD O
(77.72 O

-100 B-DAT
train O
loss O
and O
final O
test O

- B-DAT
tion O
method. O
Our O
algorithm O
computes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
proximate O
curvature. O
In O
International O
conference O

- B-DAT

- B-DAT

- B-DAT

-50 B-DAT
on O
imagenet O
in O
35 O
epochs O

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT

- B-DAT
assets/research-covers/languageunsupervised/language O
understanding O
paper. O
pdf, O
2018 O

- B-DAT

- B-DAT
cal O
report, O
Cornell O
University O
Operations O

- B-DAT
tion O
Processing O
Systems, O
pages O
5998–6008 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Sheng O
Foo. O
The O
unusual O
effectiveness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-4, B-DAT
1e-3, O
3e-3} O
and O
weight O
decay O

-4 B-DAT

-10 B-DAT

-10 B-DAT

-6 B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

- B-DAT

observing O
faster O
convergence O
on O
the O
ResNet-50 B-DAT
and O
ResNet-152 O
architectures O
[10]. O
We O

second-order O
method O
to O
train O
a O
ResNet-50 B-DAT
to O
75% O
top-1 O
accuracy O
in O

their O
default O
hyperparameter O
settings O
on O
ImageNet, B-DAT
CIFAR- O
10/100, O
neural O
machine O
translation O

on O
the O
CIFAR O
[18] O
and O
ImageNet B-DAT
datasets O
[5], O
observing O
faster O
convergence O

classification O
on O
CIFAR-10/CIFAR-100 O
[18] O
and O
ImageNet B-DAT
[5]. O
We O
also O
trained O
LSTM O

single O
crop O
validation O
accuracies O
on O
ImageNet B-DAT

Figure O
6: O
ImageNet B-DAT
training O
loss. O
The O
asterisk O
denotes O

5.2 O
ImageNet B-DAT

The O
1000-way O
ImageNet B-DAT
task O
[5] O
is O
a O
classification O

single O
crop O
top-1 O
accuracy O
on O
ImageNet B-DAT
in O
just O
50 O
epochs O
and O

approaches O
for O
improving O
convergence O
on O
ImageNet B-DAT
can O
require O
hundreds O
of O
GPUs O

C.2 O
ImageNet B-DAT

5.2 O
ImageNet B-DAT

C.2 O
ImageNet B-DAT

50 B-DAT
and O
ResNet-152 O
architectures O
[10]. O
We O

0 O
25 O
50 B-DAT
75 O
100 O
125 O
150 O
175 O

EPOCH O
50 B-DAT
- O
TOP O
1 O
75.13 O
74.43 O

EPOCH O
50 B-DAT
- O
TOP O
5 O
92.22 O
92.15 O

into O
a O
training O
set O
with O
50, B-DAT

1.28 O
million O
training O
images O
and O
50, B-DAT

PyTorch O
implementation3 O
and O
the O
ResNet- O
50 B-DAT
and O
ResNet-152 O
[10] O
architectures. O
Our O

accuracy O
on O
ImageNet O
in O
just O
50 B-DAT
epochs O
and O
reach O
75.5% O
top-1 O

0 O
10000 O
20000 O
30000 O
40000 O
50000 B-DAT
Inner O
Loop O
(Fast O
Weights) O
Steps O

50 B-DAT

50 B-DAT

50 B-DAT

50 B-DAT
to O
75% O
top-1 O
accuracy O
in O

0 O
25 O
50 B-DAT
75 O
100 O
125 O
150 O
175 O

0 O
25 O
50 B-DAT
75 O
100 O
125 O
150 O
175 O

0 O
25 O
50 B-DAT
75 O
100 O
125 O
150 O
175 O

50 B-DAT
on O
imagenet O
in O
35 O
epochs O

is O
extracted O
and O
mirrored O
horizontally O
50 B-DAT

0 O
25 O
50 B-DAT
75 O
100 O
125 O
150 O
175 O

50, B-DAT
30, O
10, O
5, O
2.5, O
1 O

50 B-DAT

0 O
25 O
50 B-DAT
75 I-DAT
100 O
125 O
150 O
175 O
200 O

PyTorch O
implementation3 O
and O
the O
ResNet- O
50 B-DAT

accuracy O
on O
ImageNet O
in O
just O
50 B-DAT

50 B-DAT
1.75 I-DAT

50 B-DAT
2.75 I-DAT

50 B-DAT
Tr I-DAT
ai O

50 B-DAT

0 O
25 O
50 B-DAT
75 I-DAT
100 O
125 O
150 O
175 O
200 O

0 O
25 O
50 B-DAT
75 I-DAT
100 O
125 O
150 O
175 O
200 O

0 O
25 O
50 B-DAT
75 I-DAT
100 O
125 O
150 O
175 O
200 O

50 B-DAT

0 O
25 O
50 B-DAT
75 I-DAT
100 O
125 O
150 O
175 O
200 O

- B-DAT

- B-DAT
10/100, O
neural O
machine O
translation, O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ball O
[32] O
and O
Nesterov O
momentum O

-50 B-DAT
and O
ResNet-152 O
architectures O
[10]. O
We O

- B-DAT

- B-DAT

- B-DAT

-32 B-DAT
test O
accuracy O
surface O
at O
epoch O

-100 B-DAT

- B-DAT

-32 B-DAT
model O
on O
CIFAR-100. O
While O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
training O
loss O
with O
fixed O
and O

- B-DAT

- B-DAT

-18 B-DAT
trained O
on O
CIFAR-10 O
with O
two O

- B-DAT

- B-DAT
gence O
of O
optimizers O
with O
the O

- B-DAT
ters O
such O
that O
the O
variance O

- B-DAT
head O
and O
SGD. O
At O
this O

- B-DAT

- B-DAT
tum O
versus O
Lookahead O
wrapping O
classical O

- B-DAT

- B-DAT
derstand O
the O
sensitivity O
of O
Lookahead O

- B-DAT
cal O
momentum O
and O
explore O
the O

- B-DAT
vergence O
rate O
over O
varying O
condition O

- B-DAT

- B-DAT
tum O
is O
set O
too O
low O

- B-DAT
timum. O
However, O
when O
the O
system O

- B-DAT

- B-DAT
cur) O
Lookahead O
is O
able O
to O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-100 B-DAT
[18] O
and O
ImageNet O
[5]. O
We O

-10 B-DAT
Train O
Loss O

-10 B-DAT
CIFAR-100 O

-100 B-DAT

-18 B-DAT
validation O
accuracies O
with O
various O
optimizers O

EPOCH O
50 O
- B-DAT
TOP O
1 O
75.13 O
74.43 O
EPOCH O

50 O
- B-DAT
TOP O
5 O
92.22 O
92.15 O
EPOCH O

60 O
- B-DAT
TOP O
1 O
75.49 O
75.15 O
EPOCH O

60 O
- B-DAT
TOP O
5 O
92.53 O
92.56 O

-1 B-DAT
and O
Top-5 O
single O
crop O
validation O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

-10 B-DAT
and O
CIFAR-100 O
datasets O
for O
classification O

-18 B-DAT
[10] O
with O
batches O
of O
128 O

- B-DAT

- B-DAT
50 O
and O
ResNet-152 O
[10] O
architectures O

-1 B-DAT
accuracy O
on O
ImageNet O
in O
just O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
The O

-152 B-DAT

-1 B-DAT
accuracy O
in O
49 O
epochs O
(matching O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
Other O

- B-DAT
scribed O
in O
Vaswani O
et O
al O

-14 B-DAT
machine O
translation O
task O

- B-DAT
plexity O
on O
the O
Penn O
Treebank O

LA(ADAM) O
31.92 O
60.28 O
57.72 O
POLYAK O
- B-DAT
61.18 O
58.79 O

- B-DAT

- B-DAT

- B-DAT

-) B-DAT
24.3 O
24.4 O
ADAFACTOR O
24.17 O
24.51 O

- B-DAT

- B-DAT

-50 B-DAT
to O
75% O
top-1 O
accuracy O
in O

-152 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
LR O

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
momentum O

-100 B-DAT

- B-DAT
racy O
than O
SGD O
(77.72 O

-100 B-DAT
train O
loss O
and O
final O
test O

- B-DAT
tion O
method. O
Our O
algorithm O
computes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
proximate O
curvature. O
In O
International O
conference O

- B-DAT

- B-DAT

- B-DAT

-50 B-DAT
on O
imagenet O
in O
35 O
epochs O

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT

- B-DAT
assets/research-covers/languageunsupervised/language O
understanding O
paper. O
pdf, O
2018 O

- B-DAT

- B-DAT
cal O
report, O
Cornell O
University O
Operations O

- B-DAT
tion O
Processing O
Systems, O
pages O
5998–6008 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Sheng O
Foo. O
The O
unusual O
effectiveness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-4, B-DAT
1e-3, O
3e-3} O
and O
weight O
decay O

-4 B-DAT

-10 B-DAT

-10 B-DAT

-6 B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

- B-DAT

observing O
faster O
convergence O
on O
the O
ResNet-50 B-DAT
and O
ResNet-152 O
architectures O
[10]. O
We O

second-order O
method O
to O
train O
a O
ResNet-50 B-DAT
to O
75% O
top-1 O
accuracy O
in O

their O
default O
hyperparameter O
settings O
on O
ImageNet, B-DAT
CIFAR- O
10/100, O
neural O
machine O
translation O

on O
the O
CIFAR O
[18] O
and O
ImageNet B-DAT
datasets O
[5], O
observing O
faster O
convergence O

classification O
on O
CIFAR-10/CIFAR-100 O
[18] O
and O
ImageNet B-DAT
[5]. O
We O
also O
trained O
LSTM O

single O
crop O
validation O
accuracies O
on O
ImageNet B-DAT

Figure O
6: O
ImageNet B-DAT
training O
loss. O
The O
asterisk O
denotes O

5.2 O
ImageNet B-DAT

The O
1000-way O
ImageNet B-DAT
task O
[5] O
is O
a O
classification O

single O
crop O
top-1 O
accuracy O
on O
ImageNet B-DAT
in O
just O
50 O
epochs O
and O

approaches O
for O
improving O
convergence O
on O
ImageNet B-DAT
can O
require O
hundreds O
of O
GPUs O

C.2 O
ImageNet B-DAT

5.2 O
ImageNet B-DAT

C.2 O
ImageNet B-DAT

TOP O
5 O
92.22 O
92.15 O
EPOCH O
60 B-DAT
- O
TOP O
1 O
75.49 O
75.15 O

EPOCH O
60 B-DAT
- O
TOP O
5 O
92.53 O
92.56 O

reach O
75.5% O
top-1 O
accuracy O
in O
60 B-DAT
epochs. O
The O
results O
are O
shown O

and O
77.96% O
top-1 O
accuracy O
in O
60 B-DAT
epochs. O
Other O
approaches O
for O
improving O

33.54 O
61.64 O
59.33 O
LA(ADAM) O
31.92 O
60 B-DAT

10: O
Visualizing O
Lookahead O
accuracy O
for O
60 B-DAT
fast O
weight O
updates. O
We O
plot O

6008, B-DAT
2017 O

50 B-DAT

0 O
25 O
50 B-DAT
75 I-DAT
100 O
125 O
150 O
175 O
200 O

PyTorch O
implementation3 O
and O
the O
ResNet- O
50 B-DAT

accuracy O
on O
ImageNet O
in O
just O
50 B-DAT

50 B-DAT
1.75 I-DAT

50 B-DAT
2.75 I-DAT

50 B-DAT
Tr I-DAT
ai O

50 B-DAT

0 O
25 O
50 B-DAT
75 I-DAT
100 O
125 O
150 O
175 O
200 O

0 O
25 O
50 B-DAT
75 I-DAT
100 O
125 O
150 O
175 O
200 O

0 O
25 O
50 B-DAT
75 I-DAT
100 O
125 O
150 O
175 O
200 O

50 B-DAT

0 O
25 O
50 B-DAT
75 I-DAT
100 O
125 O
150 O
175 O
200 O

- B-DAT

- B-DAT
10/100, O
neural O
machine O
translation, O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ball O
[32] O
and O
Nesterov O
momentum O

-50 B-DAT
and O
ResNet-152 O
architectures O
[10]. O
We O

- B-DAT

- B-DAT

- B-DAT

-32 B-DAT
test O
accuracy O
surface O
at O
epoch O

-100 B-DAT

- B-DAT

-32 B-DAT
model O
on O
CIFAR-100. O
While O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
training O
loss O
with O
fixed O
and O

- B-DAT

- B-DAT

-18 B-DAT
trained O
on O
CIFAR-10 O
with O
two O

- B-DAT

- B-DAT
gence O
of O
optimizers O
with O
the O

- B-DAT
ters O
such O
that O
the O
variance O

- B-DAT
head O
and O
SGD. O
At O
this O

- B-DAT

- B-DAT
tum O
versus O
Lookahead O
wrapping O
classical O

- B-DAT

- B-DAT
derstand O
the O
sensitivity O
of O
Lookahead O

- B-DAT
cal O
momentum O
and O
explore O
the O

- B-DAT
vergence O
rate O
over O
varying O
condition O

- B-DAT

- B-DAT
tum O
is O
set O
too O
low O

- B-DAT
timum. O
However, O
when O
the O
system O

- B-DAT

- B-DAT
cur) O
Lookahead O
is O
able O
to O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-100 B-DAT
[18] O
and O
ImageNet O
[5]. O
We O

-10 B-DAT
Train O
Loss O

-10 B-DAT
CIFAR-100 O

-100 B-DAT

-18 B-DAT
validation O
accuracies O
with O
various O
optimizers O

EPOCH O
50 O
- B-DAT
TOP O
1 O
75.13 O
74.43 O
EPOCH O

50 O
- B-DAT
TOP O
5 O
92.22 O
92.15 O
EPOCH O

60 O
- B-DAT
TOP O
1 O
75.49 O
75.15 O
EPOCH O

60 O
- B-DAT
TOP O
5 O
92.53 O
92.56 O

-1 B-DAT
and O
Top-5 O
single O
crop O
validation O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

-10 B-DAT
and O
CIFAR-100 O
datasets O
for O
classification O

-18 B-DAT
[10] O
with O
batches O
of O
128 O

- B-DAT

- B-DAT
50 O
and O
ResNet-152 O
[10] O
architectures O

-1 B-DAT
accuracy O
on O
ImageNet O
in O
just O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
The O

-152 B-DAT

-1 B-DAT
accuracy O
in O
49 O
epochs O
(matching O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
Other O

- B-DAT
scribed O
in O
Vaswani O
et O
al O

-14 B-DAT
machine O
translation O
task O

- B-DAT
plexity O
on O
the O
Penn O
Treebank O

LA(ADAM) O
31.92 O
60.28 O
57.72 O
POLYAK O
- B-DAT
61.18 O
58.79 O

- B-DAT

- B-DAT

- B-DAT

-) B-DAT
24.3 O
24.4 O
ADAFACTOR O
24.17 O
24.51 O

- B-DAT

- B-DAT

-50 B-DAT
to O
75% O
top-1 O
accuracy O
in O

-152 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
LR O

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
momentum O

-100 B-DAT

- B-DAT
racy O
than O
SGD O
(77.72 O

-100 B-DAT
train O
loss O
and O
final O
test O

- B-DAT
tion O
method. O
Our O
algorithm O
computes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
proximate O
curvature. O
In O
International O
conference O

- B-DAT

- B-DAT

- B-DAT

-50 B-DAT
on O
imagenet O
in O
35 O
epochs O

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT

- B-DAT
assets/research-covers/languageunsupervised/language O
understanding O
paper. O
pdf, O
2018 O

- B-DAT

- B-DAT
cal O
report, O
Cornell O
University O
Operations O

- B-DAT
tion O
Processing O
Systems, O
pages O
5998–6008 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Sheng O
Foo. O
The O
unusual O
effectiveness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-4, B-DAT
1e-3, O
3e-3} O
and O
weight O
decay O

-4 B-DAT

-10 B-DAT

-10 B-DAT

-6 B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

- B-DAT

observing O
faster O
convergence O
on O
the O
ResNet-50 B-DAT
and O
ResNet-152 O
architectures O
[10]. O
We O

second-order O
method O
to O
train O
a O
ResNet-50 B-DAT
to O
75% O
top-1 O
accuracy O
in O

their O
default O
hyperparameter O
settings O
on O
ImageNet, B-DAT
CIFAR- O
10/100, O
neural O
machine O
translation O

on O
the O
CIFAR O
[18] O
and O
ImageNet B-DAT
datasets O
[5], O
observing O
faster O
convergence O

classification O
on O
CIFAR-10/CIFAR-100 O
[18] O
and O
ImageNet B-DAT
[5]. O
We O
also O
trained O
LSTM O

single O
crop O
validation O
accuracies O
on O
ImageNet B-DAT

Figure O
6: O
ImageNet B-DAT
training O
loss. O
The O
asterisk O
denotes O

5.2 O
ImageNet B-DAT

The O
1000-way O
ImageNet B-DAT
task O
[5] O
is O
a O
classification O

single O
crop O
top-1 O
accuracy O
on O
ImageNet B-DAT
in O
just O
50 O
epochs O
and O

approaches O
for O
improving O
convergence O
on O
ImageNet B-DAT
can O
require O
hundreds O
of O
GPUs O

C.2 O
ImageNet B-DAT

5.2 O
ImageNet B-DAT

C.2 O
ImageNet B-DAT

TOP O
5 O
92.22 O
92.15 O
EPOCH O
60 B-DAT
- O
TOP O
1 O
75.49 O
75.15 O

EPOCH O
60 B-DAT
- O
TOP O
5 O
92.53 O
92.56 O

reach O
75.5% O
top-1 O
accuracy O
in O
60 B-DAT
epochs. O
The O
results O
are O
shown O

and O
77.96% O
top-1 O
accuracy O
in O
60 B-DAT
epochs. O
Other O
approaches O
for O
improving O

33.54 O
61.64 O
59.33 O
LA(ADAM) O
31.92 O
60 B-DAT

10: O
Visualizing O
Lookahead O
accuracy O
for O
60 B-DAT
fast O
weight O
updates. O
We O
plot O

6008, B-DAT
2017 O

50 B-DAT

0 O
25 O
50 B-DAT
75 I-DAT
100 O
125 O
150 O
175 O
200 O

PyTorch O
implementation3 O
and O
the O
ResNet- O
50 B-DAT

accuracy O
on O
ImageNet O
in O
just O
50 B-DAT

50 B-DAT
1.75 I-DAT

50 B-DAT
2.75 I-DAT

50 B-DAT
Tr I-DAT
ai O

50 B-DAT

0 O
25 O
50 B-DAT
75 I-DAT
100 O
125 O
150 O
175 O
200 O

0 O
25 O
50 B-DAT
75 I-DAT
100 O
125 O
150 O
175 O
200 O

0 O
25 O
50 B-DAT
75 I-DAT
100 O
125 O
150 O
175 O
200 O

50 B-DAT

0 O
25 O
50 B-DAT
75 I-DAT
100 O
125 O
150 O
175 O
200 O

- B-DAT

- B-DAT
10/100, O
neural O
machine O
translation, O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ball O
[32] O
and O
Nesterov O
momentum O

-50 B-DAT
and O
ResNet-152 O
architectures O
[10]. O
We O

- B-DAT

- B-DAT

- B-DAT

-32 B-DAT
test O
accuracy O
surface O
at O
epoch O

-100 B-DAT

- B-DAT

-32 B-DAT
model O
on O
CIFAR-100. O
While O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
training O
loss O
with O
fixed O
and O

- B-DAT

- B-DAT

-18 B-DAT
trained O
on O
CIFAR-10 O
with O
two O

- B-DAT

- B-DAT
gence O
of O
optimizers O
with O
the O

- B-DAT
ters O
such O
that O
the O
variance O

- B-DAT
head O
and O
SGD. O
At O
this O

- B-DAT

- B-DAT
tum O
versus O
Lookahead O
wrapping O
classical O

- B-DAT

- B-DAT
derstand O
the O
sensitivity O
of O
Lookahead O

- B-DAT
cal O
momentum O
and O
explore O
the O

- B-DAT
vergence O
rate O
over O
varying O
condition O

- B-DAT

- B-DAT
tum O
is O
set O
too O
low O

- B-DAT
timum. O
However, O
when O
the O
system O

- B-DAT

- B-DAT
cur) O
Lookahead O
is O
able O
to O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-100 B-DAT
[18] O
and O
ImageNet O
[5]. O
We O

-10 B-DAT
Train O
Loss O

-10 B-DAT
CIFAR-100 O

-100 B-DAT

-18 B-DAT
validation O
accuracies O
with O
various O
optimizers O

EPOCH O
50 O
- B-DAT
TOP O
1 O
75.13 O
74.43 O
EPOCH O

50 O
- B-DAT
TOP O
5 O
92.22 O
92.15 O
EPOCH O

60 O
- B-DAT
TOP O
1 O
75.49 O
75.15 O
EPOCH O

60 O
- B-DAT
TOP O
5 O
92.53 O
92.56 O

-1 B-DAT
and O
Top-5 O
single O
crop O
validation O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

-10 B-DAT
and O
CIFAR-100 O
datasets O
for O
classification O

-18 B-DAT
[10] O
with O
batches O
of O
128 O

- B-DAT

- B-DAT
50 O
and O
ResNet-152 O
[10] O
architectures O

-1 B-DAT
accuracy O
on O
ImageNet O
in O
just O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
The O

-152 B-DAT

-1 B-DAT
accuracy O
in O
49 O
epochs O
(matching O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
Other O

- B-DAT
scribed O
in O
Vaswani O
et O
al O

-14 B-DAT
machine O
translation O
task O

- B-DAT
plexity O
on O
the O
Penn O
Treebank O

LA(ADAM) O
31.92 O
60.28 O
57.72 O
POLYAK O
- B-DAT
61.18 O
58.79 O

- B-DAT

- B-DAT

- B-DAT

-) B-DAT
24.3 O
24.4 O
ADAFACTOR O
24.17 O
24.51 O

- B-DAT

- B-DAT

-50 B-DAT
to O
75% O
top-1 O
accuracy O
in O

-152 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
LR O

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
momentum O

-100 B-DAT

- B-DAT
racy O
than O
SGD O
(77.72 O

-100 B-DAT
train O
loss O
and O
final O
test O

- B-DAT
tion O
method. O
Our O
algorithm O
computes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
proximate O
curvature. O
In O
International O
conference O

- B-DAT

- B-DAT

- B-DAT

-50 B-DAT
on O
imagenet O
in O
35 O
epochs O

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT

- B-DAT
assets/research-covers/languageunsupervised/language O
understanding O
paper. O
pdf, O
2018 O

- B-DAT

- B-DAT
cal O
report, O
Cornell O
University O
Operations O

- B-DAT
tion O
Processing O
Systems, O
pages O
5998–6008 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Sheng O
Foo. O
The O
unusual O
effectiveness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-4, B-DAT
1e-3, O
3e-3} O
and O
weight O
decay O

-4 B-DAT

-10 B-DAT

-10 B-DAT

-6 B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

- B-DAT

surface O
at O
epoch O
100 O
on O
CIFAR-100 B-DAT

of O
a O
ResNet-32 O
model O
on O
CIFAR-100 B-DAT

CIFAR-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

CIFAR-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

Figure O
2: O
CIFAR-10 B-DAT
training O
loss O
with O
fixed O
and O

on O
a O
ResNet-18 O
trained O
on O
CIFAR-10 B-DAT
with O
two O
different O
learning O
rates O

We O
explored O
image O
classification O
on O
CIFAR-10 B-DAT

CIFAR-100 B-DAT
[18] O
and O
ImageNet O
[5]. O
We O

CIFAR-10 B-DAT
Train O
Loss O

OPTIMIZER O
CIFAR-10 B-DAT
CIFAR-100 O

algorithms. O
(Left) O
Train O
Loss O
on O
CIFAR-100 B-DAT

5.1 O
CIFAR-10 B-DAT
and O
CIFAR-100 O

The O
CIFAR-10 B-DAT
and O
CIFAR-100 O
datasets O
for O
classification O
consist O
of O

of O
Inner O
Optimizer O
Learning O
Rates O
(CIFAR-10 B-DAT

a) O
CIFAR-10 B-DAT
Train O
Loss: O
Different O
LR O

Evaluation O
of O
Inner O
Optimizer O
Momentum O
(CIFAR-10 B-DAT

b) O
CIFAR-10 B-DAT
Train O
Loss: O
Different O
momentum O

Train O
Loss O
on O
CIFAR-100 B-DAT

Figure O
9: O
CIFAR-100 B-DAT
train O
loss O
and O
final O
test O

OPTIMIZER O
CIFAR-10 B-DAT

interpolating, O
and O
resetting O
momentum O
on O
CIFAR-10 B-DAT

5.1 O
CIFAR-10 B-DAT
and O
CIFAR-100 O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

3 O
seeds O
and O
trained O
for O
200 B-DAT
epochs O
on O
a O
ResNet-18 O
[10 O

0 O
10000 O
20000 B-DAT
30000 O
40000 O
50000 O
Inner O
Loop O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

Computer O
Vision O
and O
Pattern O
Recognition, O
2009 B-DAT

. O
CVPR O
2009 B-DAT

Conference O
on, O
pages O
248–255. O
Ieee, O
2009 B-DAT

2007, B-DAT
2014 O

tiny O
images. O
Technical O
report, O
Citeseer, O
2009 B-DAT

symmetric O
lanczos O
methods. O
Technical O
report, O
2005 B-DAT

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

18 B-DAT

18 B-DAT

18 B-DAT

18 B-DAT
58 I-DAT

default O
hyperparameter O
settings O
on O
ImageNet, O
CIFAR B-DAT

by O
training O
classifiers O
on O
the O
CIFAR B-DAT
[18] O
and O
ImageNet O
datasets O
[5 O

surface O
at O
epoch O
100 O
on O
CIFAR B-DAT

of O
a O
ResNet-32 O
model O
on O
CIFAR B-DAT

CIFAR B-DAT

CIFAR B-DAT

Figure O
2: O
CIFAR B-DAT

describe O
this O
tradeoff O
on O
the O
CIFAR B-DAT
dataset O
in O
Appendix O
C.5 O
and O

on O
a O
ResNet-18 O
trained O
on O
CIFAR B-DAT

We O
explored O
image O
classification O
on O
CIFAR B-DAT

CIFAR B-DAT

CIFAR B-DAT

OPTIMIZER O
CIFAR-10 B-DAT
CIFAR O

Table O
1: O
CIFAR B-DAT
Final O
Validation O
Accuracy O

algorithms. O
(Left) O
Train O
Loss O
on O
CIFAR B-DAT

-100. O
(Right) O
CIFAR B-DAT
ResNet-18 O
validation O
accuracies O
with O
various O

5.1 O
CIFAR-10 B-DAT
and O
CIFAR O

The O
CIFAR-10 B-DAT
and O
CIFAR O

images. O
We O
ran O
all O
our O
CIFAR B-DAT
experiments O
with O
3 O
seeds O
and O

of O
Inner O
Optimizer O
Learning O
Rates O
(CIFAR B-DAT

a) O
CIFAR B-DAT

Evaluation O
of O
Inner O
Optimizer O
Momentum O
(CIFAR B-DAT

b) O
CIFAR B-DAT

Train O
Loss O
on O
CIFAR B-DAT

Figure O
9: O
CIFAR B-DAT

We O
demonstrate O
empirically O
on O
the O
CIFAR B-DAT
dataset O
that O
Lookahead O
consistently O
delivers O

C.1 O
CIFAR B-DAT

to O
as O
Adam O
throughout O
our O
CIFAR B-DAT
experiment O
section. O
For O
Adam, O
we O

that O
resets O
momentum O
in O
our O
CIFAR B-DAT
experiments O

OPTIMIZER O
CIFAR B-DAT

Table O
6: O
CIFAR B-DAT
Final O
Validation O
Accuracy O

interpolating, O
and O
resetting O
momentum O
on O
CIFAR B-DAT

5.1 O
CIFAR-10 B-DAT
and O
CIFAR O

C.1 O
CIFAR B-DAT

and O
standard O
Adam O
on O
a O
ResNet-18 B-DAT
trained O
on O
CIFAR-10 O
with O
two O

Loss O
on O
CIFAR-100. O
(Right) O
CIFAR O
ResNet-18 B-DAT
validation O
accuracies O
with O
various O
optimizers O

for O
200 O
epochs O
on O
a O
ResNet-18 B-DAT
[10] O
with O
batches O
of O
128 O

- B-DAT

- B-DAT
10/100, O
neural O
machine O
translation, O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ball O
[32] O
and O
Nesterov O
momentum O

-50 B-DAT
and O
ResNet-152 O
architectures O
[10]. O
We O

- B-DAT

- B-DAT

- B-DAT

-32 B-DAT
test O
accuracy O
surface O
at O
epoch O

-100 B-DAT

- B-DAT

-32 B-DAT
model O
on O
CIFAR-100. O
While O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
training O
loss O
with O
fixed O
and O

- B-DAT

- B-DAT

-18 B-DAT
trained O
on O
CIFAR-10 O
with O
two O

- B-DAT

- B-DAT
gence O
of O
optimizers O
with O
the O

- B-DAT
ters O
such O
that O
the O
variance O

- B-DAT
head O
and O
SGD. O
At O
this O

- B-DAT

- B-DAT
tum O
versus O
Lookahead O
wrapping O
classical O

- B-DAT

- B-DAT
derstand O
the O
sensitivity O
of O
Lookahead O

- B-DAT
cal O
momentum O
and O
explore O
the O

- B-DAT
vergence O
rate O
over O
varying O
condition O

- B-DAT

- B-DAT
tum O
is O
set O
too O
low O

- B-DAT
timum. O
However, O
when O
the O
system O

- B-DAT

- B-DAT
cur) O
Lookahead O
is O
able O
to O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-100 B-DAT
[18] O
and O
ImageNet O
[5]. O
We O

-10 B-DAT
Train O
Loss O

-10 B-DAT
CIFAR-100 O

-100 B-DAT

-18 B-DAT
validation O
accuracies O
with O
various O
optimizers O

EPOCH O
50 O
- B-DAT
TOP O
1 O
75.13 O
74.43 O
EPOCH O

50 O
- B-DAT
TOP O
5 O
92.22 O
92.15 O
EPOCH O

60 O
- B-DAT
TOP O
1 O
75.49 O
75.15 O
EPOCH O

60 O
- B-DAT
TOP O
5 O
92.53 O
92.56 O

-1 B-DAT
and O
Top-5 O
single O
crop O
validation O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

-10 B-DAT
and O
CIFAR-100 O
datasets O
for O
classification O

-18 B-DAT
[10] O
with O
batches O
of O
128 O

- B-DAT

- B-DAT
50 O
and O
ResNet-152 O
[10] O
architectures O

-1 B-DAT
accuracy O
on O
ImageNet O
in O
just O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
The O

-152 B-DAT

-1 B-DAT
accuracy O
in O
49 O
epochs O
(matching O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
Other O

- B-DAT
scribed O
in O
Vaswani O
et O
al O

-14 B-DAT
machine O
translation O
task O

- B-DAT
plexity O
on O
the O
Penn O
Treebank O

LA(ADAM) O
31.92 O
60.28 O
57.72 O
POLYAK O
- B-DAT
61.18 O
58.79 O

- B-DAT

- B-DAT

- B-DAT

-) B-DAT
24.3 O
24.4 O
ADAFACTOR O
24.17 O
24.51 O

- B-DAT

- B-DAT

-50 B-DAT
to O
75% O
top-1 O
accuracy O
in O

-152 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
LR O

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
momentum O

-100 B-DAT

- B-DAT
racy O
than O
SGD O
(77.72 O

-100 B-DAT
train O
loss O
and O
final O
test O

- B-DAT
tion O
method. O
Our O
algorithm O
computes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
proximate O
curvature. O
In O
International O
conference O

- B-DAT

- B-DAT

- B-DAT

-50 B-DAT
on O
imagenet O
in O
35 O
epochs O

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT

- B-DAT
assets/research-covers/languageunsupervised/language O
understanding O
paper. O
pdf, O
2018 O

- B-DAT

- B-DAT
cal O
report, O
Cornell O
University O
Operations O

- B-DAT
tion O
Processing O
Systems, O
pages O
5998–6008 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Sheng O
Foo. O
The O
unusual O
effectiveness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-4, B-DAT
1e-3, O
3e-3} O
and O
weight O
decay O

-4 B-DAT

-10 B-DAT

-10 B-DAT

-6 B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

- B-DAT

surface O
at O
epoch O
100 O
on O
CIFAR-100 B-DAT

of O
a O
ResNet-32 O
model O
on O
CIFAR-100 B-DAT

CIFAR-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

CIFAR-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

Figure O
2: O
CIFAR-10 B-DAT
training O
loss O
with O
fixed O
and O

on O
a O
ResNet-18 O
trained O
on O
CIFAR-10 B-DAT
with O
two O
different O
learning O
rates O

We O
explored O
image O
classification O
on O
CIFAR-10 B-DAT

CIFAR-100 B-DAT
[18] O
and O
ImageNet O
[5]. O
We O

CIFAR-10 B-DAT
Train O
Loss O

OPTIMIZER O
CIFAR-10 B-DAT
CIFAR-100 O

algorithms. O
(Left) O
Train O
Loss O
on O
CIFAR-100 B-DAT

5.1 O
CIFAR-10 B-DAT
and O
CIFAR-100 O

The O
CIFAR-10 B-DAT
and O
CIFAR-100 O
datasets O
for O
classification O
consist O
of O

of O
Inner O
Optimizer O
Learning O
Rates O
(CIFAR-10 B-DAT

a) O
CIFAR-10 B-DAT
Train O
Loss: O
Different O
LR O

Evaluation O
of O
Inner O
Optimizer O
Momentum O
(CIFAR-10 B-DAT

b) O
CIFAR-10 B-DAT
Train O
Loss: O
Different O
momentum O

Train O
Loss O
on O
CIFAR-100 B-DAT

Figure O
9: O
CIFAR-100 B-DAT
train O
loss O
and O
final O
test O

OPTIMIZER O
CIFAR-10 B-DAT

interpolating, O
and O
resetting O
momentum O
on O
CIFAR-10 B-DAT

5.1 O
CIFAR-10 B-DAT
and O
CIFAR-100 O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

3 O
seeds O
and O
trained O
for O
200 B-DAT
epochs O
on O
a O
ResNet-18 O
[10 O

0 O
10000 O
20000 B-DAT
30000 O
40000 O
50000 O
Inner O
Loop O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

Computer O
Vision O
and O
Pattern O
Recognition, O
2009 B-DAT

. O
CVPR O
2009 B-DAT

Conference O
on, O
pages O
248–255. O
Ieee, O
2009 B-DAT

2007, B-DAT
2014 O

tiny O
images. O
Technical O
report, O
Citeseer, O
2009 B-DAT

symmetric O
lanczos O
methods. O
Technical O
report, O
2005 B-DAT

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

18 B-DAT

18 B-DAT

18 B-DAT

18 B-DAT
58 I-DAT

default O
hyperparameter O
settings O
on O
ImageNet, O
CIFAR B-DAT

by O
training O
classifiers O
on O
the O
CIFAR B-DAT
[18] O
and O
ImageNet O
datasets O
[5 O

surface O
at O
epoch O
100 O
on O
CIFAR B-DAT

of O
a O
ResNet-32 O
model O
on O
CIFAR B-DAT

CIFAR B-DAT

CIFAR B-DAT

Figure O
2: O
CIFAR B-DAT

describe O
this O
tradeoff O
on O
the O
CIFAR B-DAT
dataset O
in O
Appendix O
C.5 O
and O

on O
a O
ResNet-18 O
trained O
on O
CIFAR B-DAT

We O
explored O
image O
classification O
on O
CIFAR B-DAT

CIFAR B-DAT

CIFAR B-DAT

OPTIMIZER O
CIFAR-10 B-DAT
CIFAR O

Table O
1: O
CIFAR B-DAT
Final O
Validation O
Accuracy O

algorithms. O
(Left) O
Train O
Loss O
on O
CIFAR B-DAT

-100. O
(Right) O
CIFAR B-DAT
ResNet-18 O
validation O
accuracies O
with O
various O

5.1 O
CIFAR-10 B-DAT
and O
CIFAR O

The O
CIFAR-10 B-DAT
and O
CIFAR O

images. O
We O
ran O
all O
our O
CIFAR B-DAT
experiments O
with O
3 O
seeds O
and O

of O
Inner O
Optimizer O
Learning O
Rates O
(CIFAR B-DAT

a) O
CIFAR B-DAT

Evaluation O
of O
Inner O
Optimizer O
Momentum O
(CIFAR B-DAT

b) O
CIFAR B-DAT

Train O
Loss O
on O
CIFAR B-DAT

Figure O
9: O
CIFAR B-DAT

We O
demonstrate O
empirically O
on O
the O
CIFAR B-DAT
dataset O
that O
Lookahead O
consistently O
delivers O

C.1 O
CIFAR B-DAT

to O
as O
Adam O
throughout O
our O
CIFAR B-DAT
experiment O
section. O
For O
Adam, O
we O

that O
resets O
momentum O
in O
our O
CIFAR B-DAT
experiments O

OPTIMIZER O
CIFAR B-DAT

Table O
6: O
CIFAR B-DAT
Final O
Validation O
Accuracy O

interpolating, O
and O
resetting O
momentum O
on O
CIFAR B-DAT

5.1 O
CIFAR-10 B-DAT
and O
CIFAR O

C.1 O
CIFAR B-DAT

and O
standard O
Adam O
on O
a O
ResNet-18 B-DAT
trained O
on O
CIFAR-10 O
with O
two O

Loss O
on O
CIFAR-100. O
(Right) O
CIFAR O
ResNet-18 B-DAT
validation O
accuracies O
with O
various O
optimizers O

for O
200 O
epochs O
on O
a O
ResNet-18 B-DAT
[10] O
with O
batches O
of O
128 O

- B-DAT

- B-DAT
10/100, O
neural O
machine O
translation, O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ball O
[32] O
and O
Nesterov O
momentum O

-50 B-DAT
and O
ResNet-152 O
architectures O
[10]. O
We O

- B-DAT

- B-DAT

- B-DAT

-32 B-DAT
test O
accuracy O
surface O
at O
epoch O

-100 B-DAT

- B-DAT

-32 B-DAT
model O
on O
CIFAR-100. O
While O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
training O
loss O
with O
fixed O
and O

- B-DAT

- B-DAT

-18 B-DAT
trained O
on O
CIFAR-10 O
with O
two O

- B-DAT

- B-DAT
gence O
of O
optimizers O
with O
the O

- B-DAT
ters O
such O
that O
the O
variance O

- B-DAT
head O
and O
SGD. O
At O
this O

- B-DAT

- B-DAT
tum O
versus O
Lookahead O
wrapping O
classical O

- B-DAT

- B-DAT
derstand O
the O
sensitivity O
of O
Lookahead O

- B-DAT
cal O
momentum O
and O
explore O
the O

- B-DAT
vergence O
rate O
over O
varying O
condition O

- B-DAT

- B-DAT
tum O
is O
set O
too O
low O

- B-DAT
timum. O
However, O
when O
the O
system O

- B-DAT

- B-DAT
cur) O
Lookahead O
is O
able O
to O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-100 B-DAT
[18] O
and O
ImageNet O
[5]. O
We O

-10 B-DAT
Train O
Loss O

-10 B-DAT
CIFAR-100 O

-100 B-DAT

-18 B-DAT
validation O
accuracies O
with O
various O
optimizers O

EPOCH O
50 O
- B-DAT
TOP O
1 O
75.13 O
74.43 O
EPOCH O

50 O
- B-DAT
TOP O
5 O
92.22 O
92.15 O
EPOCH O

60 O
- B-DAT
TOP O
1 O
75.49 O
75.15 O
EPOCH O

60 O
- B-DAT
TOP O
5 O
92.53 O
92.56 O

-1 B-DAT
and O
Top-5 O
single O
crop O
validation O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

-10 B-DAT
and O
CIFAR-100 O
datasets O
for O
classification O

-18 B-DAT
[10] O
with O
batches O
of O
128 O

- B-DAT

- B-DAT
50 O
and O
ResNet-152 O
[10] O
architectures O

-1 B-DAT
accuracy O
on O
ImageNet O
in O
just O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
The O

-152 B-DAT

-1 B-DAT
accuracy O
in O
49 O
epochs O
(matching O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
Other O

- B-DAT
scribed O
in O
Vaswani O
et O
al O

-14 B-DAT
machine O
translation O
task O

- B-DAT
plexity O
on O
the O
Penn O
Treebank O

LA(ADAM) O
31.92 O
60.28 O
57.72 O
POLYAK O
- B-DAT
61.18 O
58.79 O

- B-DAT

- B-DAT

- B-DAT

-) B-DAT
24.3 O
24.4 O
ADAFACTOR O
24.17 O
24.51 O

- B-DAT

- B-DAT

-50 B-DAT
to O
75% O
top-1 O
accuracy O
in O

-152 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
LR O

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
momentum O

-100 B-DAT

- B-DAT
racy O
than O
SGD O
(77.72 O

-100 B-DAT
train O
loss O
and O
final O
test O

- B-DAT
tion O
method. O
Our O
algorithm O
computes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
proximate O
curvature. O
In O
International O
conference O

- B-DAT

- B-DAT

- B-DAT

-50 B-DAT
on O
imagenet O
in O
35 O
epochs O

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT

- B-DAT
assets/research-covers/languageunsupervised/language O
understanding O
paper. O
pdf, O
2018 O

- B-DAT

- B-DAT
cal O
report, O
Cornell O
University O
Operations O

- B-DAT
tion O
Processing O
Systems, O
pages O
5998–6008 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Sheng O
Foo. O
The O
unusual O
effectiveness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-4, B-DAT
1e-3, O
3e-3} O
and O
weight O
decay O

-4 B-DAT

-10 B-DAT

-10 B-DAT

-6 B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

- B-DAT

surface O
at O
epoch O
100 O
on O
CIFAR-100 B-DAT

of O
a O
ResNet-32 O
model O
on O
CIFAR-100 B-DAT

CIFAR-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

CIFAR-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

Figure O
2: O
CIFAR-10 B-DAT
training O
loss O
with O
fixed O
and O

on O
a O
ResNet-18 O
trained O
on O
CIFAR-10 B-DAT
with O
two O
different O
learning O
rates O

We O
explored O
image O
classification O
on O
CIFAR-10 B-DAT

CIFAR-100 B-DAT
[18] O
and O
ImageNet O
[5]. O
We O

CIFAR-10 B-DAT
Train O
Loss O

OPTIMIZER O
CIFAR-10 B-DAT
CIFAR-100 O

algorithms. O
(Left) O
Train O
Loss O
on O
CIFAR-100 B-DAT

5.1 O
CIFAR-10 B-DAT
and O
CIFAR-100 O

The O
CIFAR-10 B-DAT
and O
CIFAR-100 O
datasets O
for O
classification O
consist O
of O

of O
Inner O
Optimizer O
Learning O
Rates O
(CIFAR-10 B-DAT

a) O
CIFAR-10 B-DAT
Train O
Loss: O
Different O
LR O

Evaluation O
of O
Inner O
Optimizer O
Momentum O
(CIFAR-10 B-DAT

b) O
CIFAR-10 B-DAT
Train O
Loss: O
Different O
momentum O

Train O
Loss O
on O
CIFAR-100 B-DAT

Figure O
9: O
CIFAR-100 B-DAT
train O
loss O
and O
final O
test O

OPTIMIZER O
CIFAR-10 B-DAT

interpolating, O
and O
resetting O
momentum O
on O
CIFAR-10 B-DAT

5.1 O
CIFAR-10 B-DAT
and O
CIFAR-100 O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

3 O
seeds O
and O
trained O
for O
200 B-DAT
epochs O
on O
a O
ResNet-18 O
[10 O

0 O
10000 O
20000 B-DAT
30000 O
40000 O
50000 O
Inner O
Loop O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

Computer O
Vision O
and O
Pattern O
Recognition, O
2009 B-DAT

. O
CVPR O
2009 B-DAT

Conference O
on, O
pages O
248–255. O
Ieee, O
2009 B-DAT

2007, B-DAT
2014 O

tiny O
images. O
Technical O
report, O
Citeseer, O
2009 B-DAT

symmetric O
lanczos O
methods. O
Technical O
report, O
2005 B-DAT

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

18 B-DAT

18 B-DAT

18 B-DAT

18 B-DAT
58 I-DAT

default O
hyperparameter O
settings O
on O
ImageNet, O
CIFAR B-DAT

by O
training O
classifiers O
on O
the O
CIFAR B-DAT
[18] O
and O
ImageNet O
datasets O
[5 O

surface O
at O
epoch O
100 O
on O
CIFAR B-DAT

of O
a O
ResNet-32 O
model O
on O
CIFAR B-DAT

CIFAR B-DAT

CIFAR B-DAT

Figure O
2: O
CIFAR B-DAT

describe O
this O
tradeoff O
on O
the O
CIFAR B-DAT
dataset O
in O
Appendix O
C.5 O
and O

on O
a O
ResNet-18 O
trained O
on O
CIFAR B-DAT

We O
explored O
image O
classification O
on O
CIFAR B-DAT

CIFAR B-DAT

CIFAR B-DAT

OPTIMIZER O
CIFAR-10 B-DAT
CIFAR O

Table O
1: O
CIFAR B-DAT
Final O
Validation O
Accuracy O

algorithms. O
(Left) O
Train O
Loss O
on O
CIFAR B-DAT

-100. O
(Right) O
CIFAR B-DAT
ResNet-18 O
validation O
accuracies O
with O
various O

5.1 O
CIFAR-10 B-DAT
and O
CIFAR O

The O
CIFAR-10 B-DAT
and O
CIFAR O

images. O
We O
ran O
all O
our O
CIFAR B-DAT
experiments O
with O
3 O
seeds O
and O

of O
Inner O
Optimizer O
Learning O
Rates O
(CIFAR B-DAT

a) O
CIFAR B-DAT

Evaluation O
of O
Inner O
Optimizer O
Momentum O
(CIFAR B-DAT

b) O
CIFAR B-DAT

Train O
Loss O
on O
CIFAR B-DAT

Figure O
9: O
CIFAR B-DAT

We O
demonstrate O
empirically O
on O
the O
CIFAR B-DAT
dataset O
that O
Lookahead O
consistently O
delivers O

C.1 O
CIFAR B-DAT

to O
as O
Adam O
throughout O
our O
CIFAR B-DAT
experiment O
section. O
For O
Adam, O
we O

that O
resets O
momentum O
in O
our O
CIFAR B-DAT
experiments O

OPTIMIZER O
CIFAR B-DAT

Table O
6: O
CIFAR B-DAT
Final O
Validation O
Accuracy O

interpolating, O
and O
resetting O
momentum O
on O
CIFAR B-DAT

5.1 O
CIFAR-10 B-DAT
and O
CIFAR O

C.1 O
CIFAR B-DAT

and O
standard O
Adam O
on O
a O
ResNet-18 B-DAT
trained O
on O
CIFAR-10 O
with O
two O

Loss O
on O
CIFAR-100. O
(Right) O
CIFAR O
ResNet-18 B-DAT
validation O
accuracies O
with O
various O
optimizers O

for O
200 O
epochs O
on O
a O
ResNet-18 B-DAT
[10] O
with O
batches O
of O
128 O

- B-DAT

- B-DAT
10/100, O
neural O
machine O
translation, O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ball O
[32] O
and O
Nesterov O
momentum O

-50 B-DAT
and O
ResNet-152 O
architectures O
[10]. O
We O

- B-DAT

- B-DAT

- B-DAT

-32 B-DAT
test O
accuracy O
surface O
at O
epoch O

-100 B-DAT

- B-DAT

-32 B-DAT
model O
on O
CIFAR-100. O
While O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
training O
loss O
with O
fixed O
and O

- B-DAT

- B-DAT

-18 B-DAT
trained O
on O
CIFAR-10 O
with O
two O

- B-DAT

- B-DAT
gence O
of O
optimizers O
with O
the O

- B-DAT
ters O
such O
that O
the O
variance O

- B-DAT
head O
and O
SGD. O
At O
this O

- B-DAT

- B-DAT
tum O
versus O
Lookahead O
wrapping O
classical O

- B-DAT

- B-DAT
derstand O
the O
sensitivity O
of O
Lookahead O

- B-DAT
cal O
momentum O
and O
explore O
the O

- B-DAT
vergence O
rate O
over O
varying O
condition O

- B-DAT

- B-DAT
tum O
is O
set O
too O
low O

- B-DAT
timum. O
However, O
when O
the O
system O

- B-DAT

- B-DAT
cur) O
Lookahead O
is O
able O
to O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-100 B-DAT
[18] O
and O
ImageNet O
[5]. O
We O

-10 B-DAT
Train O
Loss O

-10 B-DAT
CIFAR-100 O

-100 B-DAT

-18 B-DAT
validation O
accuracies O
with O
various O
optimizers O

EPOCH O
50 O
- B-DAT
TOP O
1 O
75.13 O
74.43 O
EPOCH O

50 O
- B-DAT
TOP O
5 O
92.22 O
92.15 O
EPOCH O

60 O
- B-DAT
TOP O
1 O
75.49 O
75.15 O
EPOCH O

60 O
- B-DAT
TOP O
5 O
92.53 O
92.56 O

-1 B-DAT
and O
Top-5 O
single O
crop O
validation O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

-10 B-DAT
and O
CIFAR-100 O
datasets O
for O
classification O

-18 B-DAT
[10] O
with O
batches O
of O
128 O

- B-DAT

- B-DAT
50 O
and O
ResNet-152 O
[10] O
architectures O

-1 B-DAT
accuracy O
on O
ImageNet O
in O
just O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
The O

-152 B-DAT

-1 B-DAT
accuracy O
in O
49 O
epochs O
(matching O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
Other O

- B-DAT
scribed O
in O
Vaswani O
et O
al O

-14 B-DAT
machine O
translation O
task O

- B-DAT
plexity O
on O
the O
Penn O
Treebank O

LA(ADAM) O
31.92 O
60.28 O
57.72 O
POLYAK O
- B-DAT
61.18 O
58.79 O

- B-DAT

- B-DAT

- B-DAT

-) B-DAT
24.3 O
24.4 O
ADAFACTOR O
24.17 O
24.51 O

- B-DAT

- B-DAT

-50 B-DAT
to O
75% O
top-1 O
accuracy O
in O

-152 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
LR O

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
momentum O

-100 B-DAT

- B-DAT
racy O
than O
SGD O
(77.72 O

-100 B-DAT
train O
loss O
and O
final O
test O

- B-DAT
tion O
method. O
Our O
algorithm O
computes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
proximate O
curvature. O
In O
International O
conference O

- B-DAT

- B-DAT

- B-DAT

-50 B-DAT
on O
imagenet O
in O
35 O
epochs O

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT

- B-DAT
assets/research-covers/languageunsupervised/language O
understanding O
paper. O
pdf, O
2018 O

- B-DAT

- B-DAT
cal O
report, O
Cornell O
University O
Operations O

- B-DAT
tion O
Processing O
Systems, O
pages O
5998–6008 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Sheng O
Foo. O
The O
unusual O
effectiveness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-4, B-DAT
1e-3, O
3e-3} O
and O
weight O
decay O

-4 B-DAT

-10 B-DAT

-10 B-DAT

-6 B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

- B-DAT

surface O
at O
epoch O
100 O
on O
CIFAR-100 B-DAT

of O
a O
ResNet-32 O
model O
on O
CIFAR-100 B-DAT

CIFAR-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

CIFAR-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

Figure O
2: O
CIFAR-10 B-DAT
training O
loss O
with O
fixed O
and O

on O
a O
ResNet-18 O
trained O
on O
CIFAR-10 B-DAT
with O
two O
different O
learning O
rates O

We O
explored O
image O
classification O
on O
CIFAR-10 B-DAT

CIFAR-100 B-DAT
[18] O
and O
ImageNet O
[5]. O
We O

CIFAR-10 B-DAT
Train O
Loss O

OPTIMIZER O
CIFAR-10 B-DAT
CIFAR-100 O

algorithms. O
(Left) O
Train O
Loss O
on O
CIFAR-100 B-DAT

5.1 O
CIFAR-10 B-DAT
and O
CIFAR-100 O

The O
CIFAR-10 B-DAT
and O
CIFAR-100 O
datasets O
for O
classification O
consist O
of O

of O
Inner O
Optimizer O
Learning O
Rates O
(CIFAR-10 B-DAT

a) O
CIFAR-10 B-DAT
Train O
Loss: O
Different O
LR O

Evaluation O
of O
Inner O
Optimizer O
Momentum O
(CIFAR-10 B-DAT

b) O
CIFAR-10 B-DAT
Train O
Loss: O
Different O
momentum O

Train O
Loss O
on O
CIFAR-100 B-DAT

Figure O
9: O
CIFAR-100 B-DAT
train O
loss O
and O
final O
test O

OPTIMIZER O
CIFAR-10 B-DAT

interpolating, O
and O
resetting O
momentum O
on O
CIFAR-10 B-DAT

5.1 O
CIFAR-10 B-DAT
and O
CIFAR-100 O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

3 O
seeds O
and O
trained O
for O
200 B-DAT
epochs O
on O
a O
ResNet-18 O
[10 O

0 O
10000 O
20000 B-DAT
30000 O
40000 O
50000 O
Inner O
Loop O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

Computer O
Vision O
and O
Pattern O
Recognition, O
2009 B-DAT

. O
CVPR O
2009 B-DAT

Conference O
on, O
pages O
248–255. O
Ieee, O
2009 B-DAT

2007, B-DAT
2014 O

tiny O
images. O
Technical O
report, O
Citeseer, O
2009 B-DAT

symmetric O
lanczos O
methods. O
Technical O
report, O
2005 B-DAT

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

18 B-DAT

18 B-DAT

18 B-DAT

18 B-DAT
58 I-DAT

default O
hyperparameter O
settings O
on O
ImageNet, O
CIFAR B-DAT

by O
training O
classifiers O
on O
the O
CIFAR B-DAT
[18] O
and O
ImageNet O
datasets O
[5 O

surface O
at O
epoch O
100 O
on O
CIFAR B-DAT

of O
a O
ResNet-32 O
model O
on O
CIFAR B-DAT

CIFAR B-DAT

CIFAR B-DAT

Figure O
2: O
CIFAR B-DAT

describe O
this O
tradeoff O
on O
the O
CIFAR B-DAT
dataset O
in O
Appendix O
C.5 O
and O

on O
a O
ResNet-18 O
trained O
on O
CIFAR B-DAT

We O
explored O
image O
classification O
on O
CIFAR B-DAT

CIFAR B-DAT

CIFAR B-DAT

OPTIMIZER O
CIFAR-10 B-DAT
CIFAR O

Table O
1: O
CIFAR B-DAT
Final O
Validation O
Accuracy O

algorithms. O
(Left) O
Train O
Loss O
on O
CIFAR B-DAT

-100. O
(Right) O
CIFAR B-DAT
ResNet-18 O
validation O
accuracies O
with O
various O

5.1 O
CIFAR-10 B-DAT
and O
CIFAR O

The O
CIFAR-10 B-DAT
and O
CIFAR O

images. O
We O
ran O
all O
our O
CIFAR B-DAT
experiments O
with O
3 O
seeds O
and O

of O
Inner O
Optimizer O
Learning O
Rates O
(CIFAR B-DAT

a) O
CIFAR B-DAT

Evaluation O
of O
Inner O
Optimizer O
Momentum O
(CIFAR B-DAT

b) O
CIFAR B-DAT

Train O
Loss O
on O
CIFAR B-DAT

Figure O
9: O
CIFAR B-DAT

We O
demonstrate O
empirically O
on O
the O
CIFAR B-DAT
dataset O
that O
Lookahead O
consistently O
delivers O

C.1 O
CIFAR B-DAT

to O
as O
Adam O
throughout O
our O
CIFAR B-DAT
experiment O
section. O
For O
Adam, O
we O

that O
resets O
momentum O
in O
our O
CIFAR B-DAT
experiments O

OPTIMIZER O
CIFAR B-DAT

Table O
6: O
CIFAR B-DAT
Final O
Validation O
Accuracy O

interpolating, O
and O
resetting O
momentum O
on O
CIFAR B-DAT

5.1 O
CIFAR-10 B-DAT
and O
CIFAR O

C.1 O
CIFAR B-DAT

and O
standard O
Adam O
on O
a O
ResNet-18 B-DAT
trained O
on O
CIFAR-10 O
with O
two O

Loss O
on O
CIFAR-100. O
(Right) O
CIFAR O
ResNet-18 B-DAT
validation O
accuracies O
with O
various O
optimizers O

for O
200 O
epochs O
on O
a O
ResNet-18 B-DAT
[10] O
with O
batches O
of O
128 O

- B-DAT

- B-DAT
10/100, O
neural O
machine O
translation, O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ball O
[32] O
and O
Nesterov O
momentum O

-50 B-DAT
and O
ResNet-152 O
architectures O
[10]. O
We O

- B-DAT

- B-DAT

- B-DAT

-32 B-DAT
test O
accuracy O
surface O
at O
epoch O

-100 B-DAT

- B-DAT

-32 B-DAT
model O
on O
CIFAR-100. O
While O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
training O
loss O
with O
fixed O
and O

- B-DAT

- B-DAT

-18 B-DAT
trained O
on O
CIFAR-10 O
with O
two O

- B-DAT

- B-DAT
gence O
of O
optimizers O
with O
the O

- B-DAT
ters O
such O
that O
the O
variance O

- B-DAT
head O
and O
SGD. O
At O
this O

- B-DAT

- B-DAT
tum O
versus O
Lookahead O
wrapping O
classical O

- B-DAT

- B-DAT
derstand O
the O
sensitivity O
of O
Lookahead O

- B-DAT
cal O
momentum O
and O
explore O
the O

- B-DAT
vergence O
rate O
over O
varying O
condition O

- B-DAT

- B-DAT
tum O
is O
set O
too O
low O

- B-DAT
timum. O
However, O
when O
the O
system O

- B-DAT

- B-DAT
cur) O
Lookahead O
is O
able O
to O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-100 B-DAT
[18] O
and O
ImageNet O
[5]. O
We O

-10 B-DAT
Train O
Loss O

-10 B-DAT
CIFAR-100 O

-100 B-DAT

-18 B-DAT
validation O
accuracies O
with O
various O
optimizers O

EPOCH O
50 O
- B-DAT
TOP O
1 O
75.13 O
74.43 O
EPOCH O

50 O
- B-DAT
TOP O
5 O
92.22 O
92.15 O
EPOCH O

60 O
- B-DAT
TOP O
1 O
75.49 O
75.15 O
EPOCH O

60 O
- B-DAT
TOP O
5 O
92.53 O
92.56 O

-1 B-DAT
and O
Top-5 O
single O
crop O
validation O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

-10 B-DAT
and O
CIFAR-100 O
datasets O
for O
classification O

-18 B-DAT
[10] O
with O
batches O
of O
128 O

- B-DAT

- B-DAT
50 O
and O
ResNet-152 O
[10] O
architectures O

-1 B-DAT
accuracy O
on O
ImageNet O
in O
just O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
The O

-152 B-DAT

-1 B-DAT
accuracy O
in O
49 O
epochs O
(matching O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
Other O

- B-DAT
scribed O
in O
Vaswani O
et O
al O

-14 B-DAT
machine O
translation O
task O

- B-DAT
plexity O
on O
the O
Penn O
Treebank O

LA(ADAM) O
31.92 O
60.28 O
57.72 O
POLYAK O
- B-DAT
61.18 O
58.79 O

- B-DAT

- B-DAT

- B-DAT

-) B-DAT
24.3 O
24.4 O
ADAFACTOR O
24.17 O
24.51 O

- B-DAT

- B-DAT

-50 B-DAT
to O
75% O
top-1 O
accuracy O
in O

-152 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
LR O

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
momentum O

-100 B-DAT

- B-DAT
racy O
than O
SGD O
(77.72 O

-100 B-DAT
train O
loss O
and O
final O
test O

- B-DAT
tion O
method. O
Our O
algorithm O
computes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
proximate O
curvature. O
In O
International O
conference O

- B-DAT

- B-DAT

- B-DAT

-50 B-DAT
on O
imagenet O
in O
35 O
epochs O

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT

- B-DAT
assets/research-covers/languageunsupervised/language O
understanding O
paper. O
pdf, O
2018 O

- B-DAT

- B-DAT
cal O
report, O
Cornell O
University O
Operations O

- B-DAT
tion O
Processing O
Systems, O
pages O
5998–6008 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Sheng O
Foo. O
The O
unusual O
effectiveness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-4, B-DAT
1e-3, O
3e-3} O
and O
weight O
decay O

-4 B-DAT

-10 B-DAT

-10 B-DAT

-6 B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

- B-DAT

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

3 O
seeds O
and O
trained O
for O
200 B-DAT
epochs O
on O
a O
ResNet-18 O
[10 O

0 O
10000 O
20000 B-DAT
30000 O
40000 O
50000 O
Inner O
Loop O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

Computer O
Vision O
and O
Pattern O
Recognition, O
2009 B-DAT

. O
CVPR O
2009 B-DAT

Conference O
on, O
pages O
248–255. O
Ieee, O
2009 B-DAT

2007, B-DAT
2014 O

tiny O
images. O
Technical O
report, O
Citeseer, O
2009 B-DAT

symmetric O
lanczos O
methods. O
Technical O
report, O
2005 B-DAT

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

18 B-DAT

18 B-DAT

18 B-DAT

18 B-DAT
58 I-DAT

default O
hyperparameter O
settings O
on O
ImageNet, O
CIFAR B-DAT

by O
training O
classifiers O
on O
the O
CIFAR B-DAT
[18] O
and O
ImageNet O
datasets O
[5 O

surface O
at O
epoch O
100 O
on O
CIFAR B-DAT

of O
a O
ResNet-32 O
model O
on O
CIFAR B-DAT

CIFAR B-DAT

CIFAR B-DAT

Figure O
2: O
CIFAR B-DAT

describe O
this O
tradeoff O
on O
the O
CIFAR B-DAT
dataset O
in O
Appendix O
C.5 O
and O

on O
a O
ResNet-18 O
trained O
on O
CIFAR B-DAT

We O
explored O
image O
classification O
on O
CIFAR B-DAT

CIFAR B-DAT

CIFAR B-DAT

OPTIMIZER O
CIFAR-10 B-DAT
CIFAR O

Table O
1: O
CIFAR B-DAT
Final O
Validation O
Accuracy O

algorithms. O
(Left) O
Train O
Loss O
on O
CIFAR B-DAT

-100. O
(Right) O
CIFAR B-DAT
ResNet-18 O
validation O
accuracies O
with O
various O

5.1 O
CIFAR-10 B-DAT
and O
CIFAR O

The O
CIFAR-10 B-DAT
and O
CIFAR O

images. O
We O
ran O
all O
our O
CIFAR B-DAT
experiments O
with O
3 O
seeds O
and O

of O
Inner O
Optimizer O
Learning O
Rates O
(CIFAR B-DAT

a) O
CIFAR B-DAT

Evaluation O
of O
Inner O
Optimizer O
Momentum O
(CIFAR B-DAT

b) O
CIFAR B-DAT

Train O
Loss O
on O
CIFAR B-DAT

Figure O
9: O
CIFAR B-DAT

We O
demonstrate O
empirically O
on O
the O
CIFAR B-DAT
dataset O
that O
Lookahead O
consistently O
delivers O

C.1 O
CIFAR B-DAT

to O
as O
Adam O
throughout O
our O
CIFAR B-DAT
experiment O
section. O
For O
Adam, O
we O

that O
resets O
momentum O
in O
our O
CIFAR B-DAT
experiments O

OPTIMIZER O
CIFAR B-DAT

Table O
6: O
CIFAR B-DAT
Final O
Validation O
Accuracy O

interpolating, O
and O
resetting O
momentum O
on O
CIFAR B-DAT

5.1 O
CIFAR-10 B-DAT
and O
CIFAR O

C.1 O
CIFAR B-DAT

and O
standard O
Adam O
on O
a O
ResNet-18 B-DAT
trained O
on O
CIFAR-10 O
with O
two O

Loss O
on O
CIFAR-100. O
(Right) O
CIFAR O
ResNet-18 B-DAT
validation O
accuracies O
with O
various O
optimizers O

for O
200 O
epochs O
on O
a O
ResNet-18 B-DAT
[10] O
with O
batches O
of O
128 O

surface O
at O
epoch O
100 O
on O
CIFAR-100 B-DAT

of O
a O
ResNet-32 O
model O
on O
CIFAR-100 B-DAT

CIFAR-100 B-DAT
[18] O
and O
ImageNet O
[5]. O
We O

OPTIMIZER O
CIFAR-10 O
CIFAR-100 B-DAT

algorithms. O
(Left) O
Train O
Loss O
on O
CIFAR-100 B-DAT

5.1 O
CIFAR-10 O
and O
CIFAR-100 B-DAT

The O
CIFAR-10 O
and O
CIFAR-100 B-DAT
datasets O
for O
classification O
consist O
of O

Train O
Loss O
on O
CIFAR-100 B-DAT

Figure O
9: O
CIFAR-100 B-DAT
train O
loss O
and O
final O
test O

5.1 O
CIFAR-10 O
and O
CIFAR-100 B-DAT

- B-DAT

- B-DAT
10/100, O
neural O
machine O
translation, O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ball O
[32] O
and O
Nesterov O
momentum O

-50 B-DAT
and O
ResNet-152 O
architectures O
[10]. O
We O

- B-DAT

- B-DAT

- B-DAT

-32 B-DAT
test O
accuracy O
surface O
at O
epoch O

-100 B-DAT

- B-DAT

-32 B-DAT
model O
on O
CIFAR-100. O
While O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
training O
loss O
with O
fixed O
and O

- B-DAT

- B-DAT

-18 B-DAT
trained O
on O
CIFAR-10 O
with O
two O

- B-DAT

- B-DAT
gence O
of O
optimizers O
with O
the O

- B-DAT
ters O
such O
that O
the O
variance O

- B-DAT
head O
and O
SGD. O
At O
this O

- B-DAT

- B-DAT
tum O
versus O
Lookahead O
wrapping O
classical O

- B-DAT

- B-DAT
derstand O
the O
sensitivity O
of O
Lookahead O

- B-DAT
cal O
momentum O
and O
explore O
the O

- B-DAT
vergence O
rate O
over O
varying O
condition O

- B-DAT

- B-DAT
tum O
is O
set O
too O
low O

- B-DAT
timum. O
However, O
when O
the O
system O

- B-DAT

- B-DAT
cur) O
Lookahead O
is O
able O
to O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-100 B-DAT
[18] O
and O
ImageNet O
[5]. O
We O

-10 B-DAT
Train O
Loss O

-10 B-DAT
CIFAR-100 O

-100 B-DAT

-18 B-DAT
validation O
accuracies O
with O
various O
optimizers O

EPOCH O
50 O
- B-DAT
TOP O
1 O
75.13 O
74.43 O
EPOCH O

50 O
- B-DAT
TOP O
5 O
92.22 O
92.15 O
EPOCH O

60 O
- B-DAT
TOP O
1 O
75.49 O
75.15 O
EPOCH O

60 O
- B-DAT
TOP O
5 O
92.53 O
92.56 O

-1 B-DAT
and O
Top-5 O
single O
crop O
validation O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

-10 B-DAT
and O
CIFAR-100 O
datasets O
for O
classification O

-18 B-DAT
[10] O
with O
batches O
of O
128 O

- B-DAT

- B-DAT
50 O
and O
ResNet-152 O
[10] O
architectures O

-1 B-DAT
accuracy O
on O
ImageNet O
in O
just O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
The O

-152 B-DAT

-1 B-DAT
accuracy O
in O
49 O
epochs O
(matching O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
Other O

- B-DAT
scribed O
in O
Vaswani O
et O
al O

-14 B-DAT
machine O
translation O
task O

- B-DAT
plexity O
on O
the O
Penn O
Treebank O

LA(ADAM) O
31.92 O
60.28 O
57.72 O
POLYAK O
- B-DAT
61.18 O
58.79 O

- B-DAT

- B-DAT

- B-DAT

-) B-DAT
24.3 O
24.4 O
ADAFACTOR O
24.17 O
24.51 O

- B-DAT

- B-DAT

-50 B-DAT
to O
75% O
top-1 O
accuracy O
in O

-152 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
LR O

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
momentum O

-100 B-DAT

- B-DAT
racy O
than O
SGD O
(77.72 O

-100 B-DAT
train O
loss O
and O
final O
test O

- B-DAT
tion O
method. O
Our O
algorithm O
computes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
proximate O
curvature. O
In O
International O
conference O

- B-DAT

- B-DAT

- B-DAT

-50 B-DAT
on O
imagenet O
in O
35 O
epochs O

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT

- B-DAT
assets/research-covers/languageunsupervised/language O
understanding O
paper. O
pdf, O
2018 O

- B-DAT

- B-DAT
cal O
report, O
Cornell O
University O
Operations O

- B-DAT
tion O
Processing O
Systems, O
pages O
5998–6008 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Sheng O
Foo. O
The O
unusual O
effectiveness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-4, B-DAT
1e-3, O
3e-3} O
and O
weight O
decay O

-4 B-DAT

-10 B-DAT

-10 B-DAT

-6 B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

- B-DAT

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

3 O
seeds O
and O
trained O
for O
200 B-DAT
epochs O
on O
a O
ResNet-18 O
[10 O

0 O
10000 O
20000 B-DAT
30000 O
40000 O
50000 O
Inner O
Loop O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

Computer O
Vision O
and O
Pattern O
Recognition, O
2009 B-DAT

. O
CVPR O
2009 B-DAT

Conference O
on, O
pages O
248–255. O
Ieee, O
2009 B-DAT

2007, B-DAT
2014 O

tiny O
images. O
Technical O
report, O
Citeseer, O
2009 B-DAT

symmetric O
lanczos O
methods. O
Technical O
report, O
2005 B-DAT

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

18 B-DAT

18 B-DAT

18 B-DAT

18 B-DAT
58 I-DAT

default O
hyperparameter O
settings O
on O
ImageNet, O
CIFAR B-DAT

by O
training O
classifiers O
on O
the O
CIFAR B-DAT
[18] O
and O
ImageNet O
datasets O
[5 O

surface O
at O
epoch O
100 O
on O
CIFAR B-DAT

of O
a O
ResNet-32 O
model O
on O
CIFAR B-DAT

CIFAR B-DAT

CIFAR B-DAT

Figure O
2: O
CIFAR B-DAT

describe O
this O
tradeoff O
on O
the O
CIFAR B-DAT
dataset O
in O
Appendix O
C.5 O
and O

on O
a O
ResNet-18 O
trained O
on O
CIFAR B-DAT

We O
explored O
image O
classification O
on O
CIFAR B-DAT

CIFAR B-DAT

CIFAR B-DAT

OPTIMIZER O
CIFAR-10 B-DAT
CIFAR O

Table O
1: O
CIFAR B-DAT
Final O
Validation O
Accuracy O

algorithms. O
(Left) O
Train O
Loss O
on O
CIFAR B-DAT

-100. O
(Right) O
CIFAR B-DAT
ResNet-18 O
validation O
accuracies O
with O
various O

5.1 O
CIFAR-10 B-DAT
and O
CIFAR O

The O
CIFAR-10 B-DAT
and O
CIFAR O

images. O
We O
ran O
all O
our O
CIFAR B-DAT
experiments O
with O
3 O
seeds O
and O

of O
Inner O
Optimizer O
Learning O
Rates O
(CIFAR B-DAT

a) O
CIFAR B-DAT

Evaluation O
of O
Inner O
Optimizer O
Momentum O
(CIFAR B-DAT

b) O
CIFAR B-DAT

Train O
Loss O
on O
CIFAR B-DAT

Figure O
9: O
CIFAR B-DAT

We O
demonstrate O
empirically O
on O
the O
CIFAR B-DAT
dataset O
that O
Lookahead O
consistently O
delivers O

C.1 O
CIFAR B-DAT

to O
as O
Adam O
throughout O
our O
CIFAR B-DAT
experiment O
section. O
For O
Adam, O
we O

that O
resets O
momentum O
in O
our O
CIFAR B-DAT
experiments O

OPTIMIZER O
CIFAR B-DAT

Table O
6: O
CIFAR B-DAT
Final O
Validation O
Accuracy O

interpolating, O
and O
resetting O
momentum O
on O
CIFAR B-DAT

5.1 O
CIFAR-10 B-DAT
and O
CIFAR O

C.1 O
CIFAR B-DAT

and O
standard O
Adam O
on O
a O
ResNet-18 B-DAT
trained O
on O
CIFAR-10 O
with O
two O

Loss O
on O
CIFAR-100. O
(Right) O
CIFAR O
ResNet-18 B-DAT
validation O
accuracies O
with O
various O
optimizers O

for O
200 O
epochs O
on O
a O
ResNet-18 B-DAT
[10] O
with O
batches O
of O
128 O

surface O
at O
epoch O
100 O
on O
CIFAR-100 B-DAT

of O
a O
ResNet-32 O
model O
on O
CIFAR-100 B-DAT

CIFAR-100 B-DAT
[18] O
and O
ImageNet O
[5]. O
We O

OPTIMIZER O
CIFAR-10 O
CIFAR-100 B-DAT

algorithms. O
(Left) O
Train O
Loss O
on O
CIFAR-100 B-DAT

5.1 O
CIFAR-10 O
and O
CIFAR-100 B-DAT

The O
CIFAR-10 O
and O
CIFAR-100 B-DAT
datasets O
for O
classification O
consist O
of O

Train O
Loss O
on O
CIFAR-100 B-DAT

Figure O
9: O
CIFAR-100 B-DAT
train O
loss O
and O
final O
test O

5.1 O
CIFAR-10 O
and O
CIFAR-100 B-DAT

- B-DAT

- B-DAT
10/100, O
neural O
machine O
translation, O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ball O
[32] O
and O
Nesterov O
momentum O

-50 B-DAT
and O
ResNet-152 O
architectures O
[10]. O
We O

- B-DAT

- B-DAT

- B-DAT

-32 B-DAT
test O
accuracy O
surface O
at O
epoch O

-100 B-DAT

- B-DAT

-32 B-DAT
model O
on O
CIFAR-100. O
While O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
training O
loss O
with O
fixed O
and O

- B-DAT

- B-DAT

-18 B-DAT
trained O
on O
CIFAR-10 O
with O
two O

- B-DAT

- B-DAT
gence O
of O
optimizers O
with O
the O

- B-DAT
ters O
such O
that O
the O
variance O

- B-DAT
head O
and O
SGD. O
At O
this O

- B-DAT

- B-DAT
tum O
versus O
Lookahead O
wrapping O
classical O

- B-DAT

- B-DAT
derstand O
the O
sensitivity O
of O
Lookahead O

- B-DAT
cal O
momentum O
and O
explore O
the O

- B-DAT
vergence O
rate O
over O
varying O
condition O

- B-DAT

- B-DAT
tum O
is O
set O
too O
low O

- B-DAT
timum. O
However, O
when O
the O
system O

- B-DAT

- B-DAT
cur) O
Lookahead O
is O
able O
to O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-100 B-DAT
[18] O
and O
ImageNet O
[5]. O
We O

-10 B-DAT
Train O
Loss O

-10 B-DAT
CIFAR-100 O

-100 B-DAT

-18 B-DAT
validation O
accuracies O
with O
various O
optimizers O

EPOCH O
50 O
- B-DAT
TOP O
1 O
75.13 O
74.43 O
EPOCH O

50 O
- B-DAT
TOP O
5 O
92.22 O
92.15 O
EPOCH O

60 O
- B-DAT
TOP O
1 O
75.49 O
75.15 O
EPOCH O

60 O
- B-DAT
TOP O
5 O
92.53 O
92.56 O

-1 B-DAT
and O
Top-5 O
single O
crop O
validation O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

-10 B-DAT
and O
CIFAR-100 O
datasets O
for O
classification O

-18 B-DAT
[10] O
with O
batches O
of O
128 O

- B-DAT

- B-DAT
50 O
and O
ResNet-152 O
[10] O
architectures O

-1 B-DAT
accuracy O
on O
ImageNet O
in O
just O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
The O

-152 B-DAT

-1 B-DAT
accuracy O
in O
49 O
epochs O
(matching O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
Other O

- B-DAT
scribed O
in O
Vaswani O
et O
al O

-14 B-DAT
machine O
translation O
task O

- B-DAT
plexity O
on O
the O
Penn O
Treebank O

LA(ADAM) O
31.92 O
60.28 O
57.72 O
POLYAK O
- B-DAT
61.18 O
58.79 O

- B-DAT

- B-DAT

- B-DAT

-) B-DAT
24.3 O
24.4 O
ADAFACTOR O
24.17 O
24.51 O

- B-DAT

- B-DAT

-50 B-DAT
to O
75% O
top-1 O
accuracy O
in O

-152 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
LR O

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
momentum O

-100 B-DAT

- B-DAT
racy O
than O
SGD O
(77.72 O

-100 B-DAT
train O
loss O
and O
final O
test O

- B-DAT
tion O
method. O
Our O
algorithm O
computes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
proximate O
curvature. O
In O
International O
conference O

- B-DAT

- B-DAT

- B-DAT

-50 B-DAT
on O
imagenet O
in O
35 O
epochs O

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT

- B-DAT
assets/research-covers/languageunsupervised/language O
understanding O
paper. O
pdf, O
2018 O

- B-DAT

- B-DAT
cal O
report, O
Cornell O
University O
Operations O

- B-DAT
tion O
Processing O
Systems, O
pages O
5998–6008 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Sheng O
Foo. O
The O
unusual O
effectiveness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-4, B-DAT
1e-3, O
3e-3} O
and O
weight O
decay O

-4 B-DAT

-10 B-DAT

-10 B-DAT

-6 B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

- B-DAT

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

3 O
seeds O
and O
trained O
for O
200 B-DAT
epochs O
on O
a O
ResNet-18 O
[10 O

0 O
10000 O
20000 B-DAT
30000 O
40000 O
50000 O
Inner O
Loop O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

Computer O
Vision O
and O
Pattern O
Recognition, O
2009 B-DAT

. O
CVPR O
2009 B-DAT

Conference O
on, O
pages O
248–255. O
Ieee, O
2009 B-DAT

2007, B-DAT
2014 O

tiny O
images. O
Technical O
report, O
Citeseer, O
2009 B-DAT

symmetric O
lanczos O
methods. O
Technical O
report, O
2005 B-DAT

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

18 B-DAT

18 B-DAT

18 B-DAT

18 B-DAT
58 I-DAT

default O
hyperparameter O
settings O
on O
ImageNet, O
CIFAR B-DAT

by O
training O
classifiers O
on O
the O
CIFAR B-DAT
[18] O
and O
ImageNet O
datasets O
[5 O

surface O
at O
epoch O
100 O
on O
CIFAR B-DAT

of O
a O
ResNet-32 O
model O
on O
CIFAR B-DAT

CIFAR B-DAT

CIFAR B-DAT

Figure O
2: O
CIFAR B-DAT

describe O
this O
tradeoff O
on O
the O
CIFAR B-DAT
dataset O
in O
Appendix O
C.5 O
and O

on O
a O
ResNet-18 O
trained O
on O
CIFAR B-DAT

We O
explored O
image O
classification O
on O
CIFAR B-DAT

CIFAR B-DAT

CIFAR B-DAT

OPTIMIZER O
CIFAR-10 B-DAT
CIFAR O

Table O
1: O
CIFAR B-DAT
Final O
Validation O
Accuracy O

algorithms. O
(Left) O
Train O
Loss O
on O
CIFAR B-DAT

-100. O
(Right) O
CIFAR B-DAT
ResNet-18 O
validation O
accuracies O
with O
various O

5.1 O
CIFAR-10 B-DAT
and O
CIFAR O

The O
CIFAR-10 B-DAT
and O
CIFAR O

images. O
We O
ran O
all O
our O
CIFAR B-DAT
experiments O
with O
3 O
seeds O
and O

of O
Inner O
Optimizer O
Learning O
Rates O
(CIFAR B-DAT

a) O
CIFAR B-DAT

Evaluation O
of O
Inner O
Optimizer O
Momentum O
(CIFAR B-DAT

b) O
CIFAR B-DAT

Train O
Loss O
on O
CIFAR B-DAT

Figure O
9: O
CIFAR B-DAT

We O
demonstrate O
empirically O
on O
the O
CIFAR B-DAT
dataset O
that O
Lookahead O
consistently O
delivers O

C.1 O
CIFAR B-DAT

to O
as O
Adam O
throughout O
our O
CIFAR B-DAT
experiment O
section. O
For O
Adam, O
we O

that O
resets O
momentum O
in O
our O
CIFAR B-DAT
experiments O

OPTIMIZER O
CIFAR B-DAT

Table O
6: O
CIFAR B-DAT
Final O
Validation O
Accuracy O

interpolating, O
and O
resetting O
momentum O
on O
CIFAR B-DAT

5.1 O
CIFAR-10 B-DAT
and O
CIFAR O

C.1 O
CIFAR B-DAT

and O
standard O
Adam O
on O
a O
ResNet-18 B-DAT
trained O
on O
CIFAR-10 O
with O
two O

Loss O
on O
CIFAR-100. O
(Right) O
CIFAR O
ResNet-18 B-DAT
validation O
accuracies O
with O
various O
optimizers O

for O
200 O
epochs O
on O
a O
ResNet-18 B-DAT
[10] O
with O
batches O
of O
128 O

surface O
at O
epoch O
100 O
on O
CIFAR-100 B-DAT

of O
a O
ResNet-32 O
model O
on O
CIFAR-100 B-DAT

CIFAR-100 B-DAT
[18] O
and O
ImageNet O
[5]. O
We O

OPTIMIZER O
CIFAR-10 O
CIFAR-100 B-DAT

algorithms. O
(Left) O
Train O
Loss O
on O
CIFAR-100 B-DAT

5.1 O
CIFAR-10 O
and O
CIFAR-100 B-DAT

The O
CIFAR-10 O
and O
CIFAR-100 B-DAT
datasets O
for O
classification O
consist O
of O

Train O
Loss O
on O
CIFAR-100 B-DAT

Figure O
9: O
CIFAR-100 B-DAT
train O
loss O
and O
final O
test O

5.1 O
CIFAR-10 O
and O
CIFAR-100 B-DAT

- B-DAT

- B-DAT
10/100, O
neural O
machine O
translation, O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ball O
[32] O
and O
Nesterov O
momentum O

-50 B-DAT
and O
ResNet-152 O
architectures O
[10]. O
We O

- B-DAT

- B-DAT

- B-DAT

-32 B-DAT
test O
accuracy O
surface O
at O
epoch O

-100 B-DAT

- B-DAT

-32 B-DAT
model O
on O
CIFAR-100. O
While O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
training O
loss O
with O
fixed O
and O

- B-DAT

- B-DAT

-18 B-DAT
trained O
on O
CIFAR-10 O
with O
two O

- B-DAT

- B-DAT
gence O
of O
optimizers O
with O
the O

- B-DAT
ters O
such O
that O
the O
variance O

- B-DAT
head O
and O
SGD. O
At O
this O

- B-DAT

- B-DAT
tum O
versus O
Lookahead O
wrapping O
classical O

- B-DAT

- B-DAT
derstand O
the O
sensitivity O
of O
Lookahead O

- B-DAT
cal O
momentum O
and O
explore O
the O

- B-DAT
vergence O
rate O
over O
varying O
condition O

- B-DAT

- B-DAT
tum O
is O
set O
too O
low O

- B-DAT
timum. O
However, O
when O
the O
system O

- B-DAT

- B-DAT
cur) O
Lookahead O
is O
able O
to O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-100 B-DAT
[18] O
and O
ImageNet O
[5]. O
We O

-10 B-DAT
Train O
Loss O

-10 B-DAT
CIFAR-100 O

-100 B-DAT

-18 B-DAT
validation O
accuracies O
with O
various O
optimizers O

EPOCH O
50 O
- B-DAT
TOP O
1 O
75.13 O
74.43 O
EPOCH O

50 O
- B-DAT
TOP O
5 O
92.22 O
92.15 O
EPOCH O

60 O
- B-DAT
TOP O
1 O
75.49 O
75.15 O
EPOCH O

60 O
- B-DAT
TOP O
5 O
92.53 O
92.56 O

-1 B-DAT
and O
Top-5 O
single O
crop O
validation O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

-10 B-DAT
and O
CIFAR-100 O
datasets O
for O
classification O

-18 B-DAT
[10] O
with O
batches O
of O
128 O

- B-DAT

- B-DAT
50 O
and O
ResNet-152 O
[10] O
architectures O

-1 B-DAT
accuracy O
on O
ImageNet O
in O
just O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
The O

-152 B-DAT

-1 B-DAT
accuracy O
in O
49 O
epochs O
(matching O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
Other O

- B-DAT
scribed O
in O
Vaswani O
et O
al O

-14 B-DAT
machine O
translation O
task O

- B-DAT
plexity O
on O
the O
Penn O
Treebank O

LA(ADAM) O
31.92 O
60.28 O
57.72 O
POLYAK O
- B-DAT
61.18 O
58.79 O

- B-DAT

- B-DAT

- B-DAT

-) B-DAT
24.3 O
24.4 O
ADAFACTOR O
24.17 O
24.51 O

- B-DAT

- B-DAT

-50 B-DAT
to O
75% O
top-1 O
accuracy O
in O

-152 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
LR O

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
momentum O

-100 B-DAT

- B-DAT
racy O
than O
SGD O
(77.72 O

-100 B-DAT
train O
loss O
and O
final O
test O

- B-DAT
tion O
method. O
Our O
algorithm O
computes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
proximate O
curvature. O
In O
International O
conference O

- B-DAT

- B-DAT

- B-DAT

-50 B-DAT
on O
imagenet O
in O
35 O
epochs O

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT

- B-DAT
assets/research-covers/languageunsupervised/language O
understanding O
paper. O
pdf, O
2018 O

- B-DAT

- B-DAT
cal O
report, O
Cornell O
University O
Operations O

- B-DAT
tion O
Processing O
Systems, O
pages O
5998–6008 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Sheng O
Foo. O
The O
unusual O
effectiveness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-4, B-DAT
1e-3, O
3e-3} O
and O
weight O
decay O

-4 B-DAT

-10 B-DAT

-10 B-DAT

-6 B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

- B-DAT

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

3 O
seeds O
and O
trained O
for O
200 B-DAT
epochs O
on O
a O
ResNet-18 O
[10 O

0 O
10000 O
20000 B-DAT
30000 O
40000 O
50000 O
Inner O
Loop O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

Computer O
Vision O
and O
Pattern O
Recognition, O
2009 B-DAT

. O
CVPR O
2009 B-DAT

Conference O
on, O
pages O
248–255. O
Ieee, O
2009 B-DAT

2007, B-DAT
2014 O

tiny O
images. O
Technical O
report, O
Citeseer, O
2009 B-DAT

symmetric O
lanczos O
methods. O
Technical O
report, O
2005 B-DAT

75 O
100 O
125 O
150 O
175 O
200 B-DAT
Epoch O

18 B-DAT

18 B-DAT

18 B-DAT

18 B-DAT
58 I-DAT

default O
hyperparameter O
settings O
on O
ImageNet, O
CIFAR B-DAT

by O
training O
classifiers O
on O
the O
CIFAR B-DAT
[18] O
and O
ImageNet O
datasets O
[5 O

surface O
at O
epoch O
100 O
on O
CIFAR B-DAT

of O
a O
ResNet-32 O
model O
on O
CIFAR B-DAT

CIFAR B-DAT

CIFAR B-DAT

Figure O
2: O
CIFAR B-DAT

describe O
this O
tradeoff O
on O
the O
CIFAR B-DAT
dataset O
in O
Appendix O
C.5 O
and O

on O
a O
ResNet-18 O
trained O
on O
CIFAR B-DAT

We O
explored O
image O
classification O
on O
CIFAR B-DAT

CIFAR B-DAT

CIFAR B-DAT

OPTIMIZER O
CIFAR-10 B-DAT
CIFAR O

Table O
1: O
CIFAR B-DAT
Final O
Validation O
Accuracy O

algorithms. O
(Left) O
Train O
Loss O
on O
CIFAR B-DAT

-100. O
(Right) O
CIFAR B-DAT
ResNet-18 O
validation O
accuracies O
with O
various O

5.1 O
CIFAR-10 B-DAT
and O
CIFAR O

The O
CIFAR-10 B-DAT
and O
CIFAR O

images. O
We O
ran O
all O
our O
CIFAR B-DAT
experiments O
with O
3 O
seeds O
and O

of O
Inner O
Optimizer O
Learning O
Rates O
(CIFAR B-DAT

a) O
CIFAR B-DAT

Evaluation O
of O
Inner O
Optimizer O
Momentum O
(CIFAR B-DAT

b) O
CIFAR B-DAT

Train O
Loss O
on O
CIFAR B-DAT

Figure O
9: O
CIFAR B-DAT

We O
demonstrate O
empirically O
on O
the O
CIFAR B-DAT
dataset O
that O
Lookahead O
consistently O
delivers O

C.1 O
CIFAR B-DAT

to O
as O
Adam O
throughout O
our O
CIFAR B-DAT
experiment O
section. O
For O
Adam, O
we O

that O
resets O
momentum O
in O
our O
CIFAR B-DAT
experiments O

OPTIMIZER O
CIFAR B-DAT

Table O
6: O
CIFAR B-DAT
Final O
Validation O
Accuracy O

interpolating, O
and O
resetting O
momentum O
on O
CIFAR B-DAT

5.1 O
CIFAR-10 B-DAT
and O
CIFAR O

C.1 O
CIFAR B-DAT

and O
standard O
Adam O
on O
a O
ResNet-18 B-DAT
trained O
on O
CIFAR-10 O
with O
two O

Loss O
on O
CIFAR-100. O
(Right) O
CIFAR O
ResNet-18 B-DAT
validation O
accuracies O
with O
various O
optimizers O

for O
200 O
epochs O
on O
a O
ResNet-18 B-DAT
[10] O
with O
batches O
of O
128 O

surface O
at O
epoch O
100 O
on O
CIFAR-100 B-DAT

of O
a O
ResNet-32 O
model O
on O
CIFAR-100 B-DAT

CIFAR-100 B-DAT
[18] O
and O
ImageNet O
[5]. O
We O

OPTIMIZER O
CIFAR-10 O
CIFAR-100 B-DAT

algorithms. O
(Left) O
Train O
Loss O
on O
CIFAR-100 B-DAT

5.1 O
CIFAR-10 O
and O
CIFAR-100 B-DAT

The O
CIFAR-10 O
and O
CIFAR-100 B-DAT
datasets O
for O
classification O
consist O
of O

Train O
Loss O
on O
CIFAR-100 B-DAT

Figure O
9: O
CIFAR-100 B-DAT
train O
loss O
and O
final O
test O

5.1 O
CIFAR-10 O
and O
CIFAR-100 B-DAT

- B-DAT

- B-DAT
10/100, O
neural O
machine O
translation, O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ball O
[32] O
and O
Nesterov O
momentum O

-50 B-DAT
and O
ResNet-152 O
architectures O
[10]. O
We O

- B-DAT

- B-DAT

- B-DAT

-32 B-DAT
test O
accuracy O
surface O
at O
epoch O

-100 B-DAT

- B-DAT

-32 B-DAT
model O
on O
CIFAR-100. O
While O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
Training O
Loss O
with O
Different O
Alpha O

-10 B-DAT
training O
loss O
with O
fixed O
and O

- B-DAT

- B-DAT

-18 B-DAT
trained O
on O
CIFAR-10 O
with O
two O

- B-DAT

- B-DAT
gence O
of O
optimizers O
with O
the O

- B-DAT
ters O
such O
that O
the O
variance O

- B-DAT
head O
and O
SGD. O
At O
this O

- B-DAT

- B-DAT
tum O
versus O
Lookahead O
wrapping O
classical O

- B-DAT

- B-DAT
derstand O
the O
sensitivity O
of O
Lookahead O

- B-DAT
cal O
momentum O
and O
explore O
the O

- B-DAT
vergence O
rate O
over O
varying O
condition O

- B-DAT

- B-DAT
tum O
is O
set O
too O
low O

- B-DAT
timum. O
However, O
when O
the O
system O

- B-DAT

- B-DAT
cur) O
Lookahead O
is O
able O
to O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-100 B-DAT
[18] O
and O
ImageNet O
[5]. O
We O

-10 B-DAT
Train O
Loss O

-10 B-DAT
CIFAR-100 O

-100 B-DAT

-18 B-DAT
validation O
accuracies O
with O
various O
optimizers O

EPOCH O
50 O
- B-DAT
TOP O
1 O
75.13 O
74.43 O
EPOCH O

50 O
- B-DAT
TOP O
5 O
92.22 O
92.15 O
EPOCH O

60 O
- B-DAT
TOP O
1 O
75.49 O
75.15 O
EPOCH O

60 O
- B-DAT
TOP O
5 O
92.53 O
92.56 O

-1 B-DAT
and O
Top-5 O
single O
crop O
validation O

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

-10 B-DAT
and O
CIFAR-100 O
datasets O
for O
classification O

-18 B-DAT
[10] O
with O
batches O
of O
128 O

- B-DAT

- B-DAT
50 O
and O
ResNet-152 O
[10] O
architectures O

-1 B-DAT
accuracy O
on O
ImageNet O
in O
just O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
The O

-152 B-DAT

-1 B-DAT
accuracy O
in O
49 O
epochs O
(matching O

-1 B-DAT
accuracy O
in O
60 O
epochs. O
Other O

- B-DAT
scribed O
in O
Vaswani O
et O
al O

-14 B-DAT
machine O
translation O
task O

- B-DAT
plexity O
on O
the O
Penn O
Treebank O

LA(ADAM) O
31.92 O
60.28 O
57.72 O
POLYAK O
- B-DAT
61.18 O
58.79 O

- B-DAT

- B-DAT

- B-DAT

-) B-DAT
24.3 O
24.4 O
ADAFACTOR O
24.17 O
24.51 O

- B-DAT

- B-DAT

-50 B-DAT
to O
75% O
top-1 O
accuracy O
in O

-152 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
LR O

-10 B-DAT

-10 B-DAT
Train O
Loss: O
Different O
momentum O

-100 B-DAT

- B-DAT
racy O
than O
SGD O
(77.72 O

-100 B-DAT
train O
loss O
and O
final O
test O

- B-DAT
tion O
method. O
Our O
algorithm O
computes O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
proximate O
curvature. O
In O
International O
conference O

- B-DAT

- B-DAT

- B-DAT

-50 B-DAT
on O
imagenet O
in O
35 O
epochs O

- B-DAT

- B-DAT

- B-DAT

-2 B-DAT

- B-DAT
assets/research-covers/languageunsupervised/language O
understanding O
paper. O
pdf, O
2018 O

- B-DAT

- B-DAT
cal O
report, O
Cornell O
University O
Operations O

- B-DAT
tion O
Processing O
Systems, O
pages O
5998–6008 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Sheng O
Foo. O
The O
unusual O
effectiveness O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

-4, B-DAT
1e-3, O
3e-3} O
and O
weight O
decay O

-4 B-DAT

-10 B-DAT

-10 B-DAT

-6 B-DAT

- B-DAT

- B-DAT

- B-DAT

-10 B-DAT
and O
CIFAR-100 O

- B-DAT

C.: O
The O
lear O
submission O
at O
thumos B-DAT
2014 I-DAT
(2014) O
37. O
Panda, O
R., O
Das O

appearance O
features. O
THUMOS14 B-DAT
Action O
Recognition O
Challenge O
1(2), O
2 O

In: O
ECCV. O
pp. O
628–643. O
Springer O
(2014 B-DAT

of O
amsterdam O
at O
thumos O
challenge O
2014 B-DAT

. O
ECCVW O
2014 B-DAT
(2014 O

ECCVW. O
vol. O
1, O
p. O
5 O
(2014 B-DAT

A O
method O
for O
stochastic O
optimization O
(2014) B-DAT
31. O
Kuehne, O
H., O
Richard, O
A O

The O
lear O
submission O
at O
thumos O
2014 B-DAT
(2014) O
37. O
Panda, O
R., O
Das O

videos. O
In: O
NIPS. O
pp. O
568–576 O
(2014 B-DAT

from O
overfitting. O
JMLR O
15(1), O
1929–1958 O
(2014) B-DAT
50. O
Sultani, O
W., O
Shah, O
M O

Action O
Recognition O
Challenge O
1(2), O
2 O
(2014) B-DAT
57. O
Wang, O
L., O
Xiong, O
Y O

oracles. O
Pattern O
Recognition O
47(3), O
1395–1410 O
(2014 B-DAT

C.: O
The O
lear O
submission O
at O
thumos B-DAT
2014 I-DAT

Shah, O
M., O
Suk- O
thankar, O
R.: O
THUMOS B-DAT

I., O
Shah, O
M., O
Sukthankar, O
R.: O
THUMOS B-DAT

encoded O
dense O
trajectories. O
In: O
ECCV O
THUMOS B-DAT

at O
thumos O
2014. O
In: O
ECCV O
THUMOS B-DAT

and O
appearance O
features. O
In: O
ECCV O
THUMOS B-DAT

our O
method O
improves O
mAP O
on O
THUMOS B-DAT

our O
method O
improves O
mAP O
on O
THUMOS B-DAT

Charades O
[63, O
62], O
ActivityNet O
[28], O
THUMOS B-DAT
[37, O
24]. O
In O
order O
to O

via O
grid O
search O
on O
the O
THUMOS B-DAT

by O
grid O
search O
on O
the O
THUMOS B-DAT

THUMOS B-DAT

temporal O
action O
localization O
task O
in O
THUMOS B-DAT

can O
converge O
quickly O
on O
both O
THUMOS B-DAT

4, O
8, O
16, O
32 O
for O
THUMOS B-DAT

The O
results O
on O
THUMOS B-DAT

under O
different O
IoU O
thresholds O
on O
THUMOS B-DAT

Shah, O
M., O
Suk- O
thankar, O
R.: O
THUMOS B-DAT
challenge: O
Action O
recognition O
with O
a O

I., O
Shah, O
M., O
Sukthankar, O
R.: O
THUMOS B-DAT
challenge: O
Action O
recognition O
with O
a O

THUMOS14 B-DAT

encoded O
dense O
trajectories. O
In: O
ECCV O
THUMOS B-DAT
Workshop O
(2014 O

at O
thumos O
2014. O
In: O
ECCV O
THUMOS B-DAT
Workshop O
(2014 O

and O
appearance O
features. O
In: O
ECCV O
THUMOS B-DAT
Workshop O
(2014 O

feature O
embedding. O
In: O
ACM O
MM O
(2014 B-DAT

large O
number O
of O
classes. O
http://crcv.ucf.edu/THUMOS14/ O
(2014 B-DAT

trajectories. O
In: O
ECCV O
THUMOS O
Workshop O
(2014 B-DAT

The O
lear O
submission O
at O
thumos O
2014 B-DAT

. O
In: O
ECCV O
THUMOS O
Workshop O
(2014 B-DAT

nition O
in O
videos. O
In: O
NIPS O
(2014 B-DAT

features. O
In: O
ECCV O
THUMOS O
Workshop O
(2014 B-DAT

M. O
Shah, O
and O
R. O
Sukthankar. O
THUMOS B-DAT

attains O
state-of-the-art O
results O
on O
the O
THUMOS14 B-DAT
dataset O
and O
outstanding O
perfor- O
mance O

throughout O
each O
clip. O
In O
contrast, O
THUMOS14 B-DAT
dataset O
[17] O
and O
ActivityNet O
[14 O

Note O
that O
each O
video O
in O
THUMOS14 B-DAT
and O
ActivityNet O
may O
have O
multiple O

an O
example O
video O
in O
the O
THUMOS14 B-DAT
dataset O
[17]. O
The O
horizontal O
axis O

action O
localiza- O
tion O
benchmark O
datasets, O
THUMOS14 B-DAT
[17] O
and O
Activi- O
tyNet1.3 O
[14 O

The O
THUMOS14 B-DAT
dataset O
has O
video-level O
annotations O
of O

other O
recent O
techniques O
on O
the O
THUMOS14 B-DAT
testing O
set. O
We O
divide O
the O

summarizes O
the O
test O
results O
on O
THUMOS14 B-DAT
for O
action O
localization O
methods O
in O

Figure O
4: O
Qualitative O
results O
on O
THUMOS14 B-DAT

study O
are O
performed O
on O
the O
THUMOS14 B-DAT
dataset O

achieved O
state-of-the-art O
performance O
on O
the O
THUMOS14 B-DAT
dataset, O
and O
we O
reported O
weakly O

M. O
Shah, O
and O
R. O
Sukthankar. O
THUMOS B-DAT
challenge: O
Action O
recognition O
with O
a O

under O
ordering O
constraints. O
In O
ECCV, O
2014 B-DAT

a O
large O
number O
of O
classes, O
2014 B-DAT

lutional O
neural O
networks. O
In O
CVPR, O
2014 B-DAT

recognition O
in O
videos. O
In O
NIPS, O
2014 B-DAT

Schmid. O
The O
lear O
submission O
at O
thumos B-DAT
2014 I-DAT

M. O
Shah, O
and O
R. O
Sukthankar. O
THUMOS B-DAT

submission O
at O
thumos O
2014. O
In O
THUMOS B-DAT

the O
untrimmed O
video O
datasets O
of O
THUMOS14 B-DAT
and O
ActivityNet. O
Although O
our O
UntrimmedNet O

41]) O
or O
untrimmed O
videos O
(e.g., O
THUMOS14 B-DAT
[22] O
and O
ActivityNet O
[16]). O
Al O

challenging O
untrimmed O
video O
datasets, O
namely O
THUMOS14 B-DAT
[22] O
and O
Acitivi- O
tyNet O
[16 O

on O
two O
large O
datasets, O
namely O
THUMOS14 B-DAT
[22] O
and O
ActivityNet O
[16]. O
These O

The O
THUMOS14 B-DAT
dataset O
has O
101 O
classes O
for O

values O
on O
the O
dataset O
of O
THUMOS14 B-DAT

proposal O
sampling O
methods O
on O
the O
THUMOS14 B-DAT
dataset O

the O
training O
set O
size O
of O
THUMOS14 B-DAT
and O
ActivityNet O
is O
relatively O
small O

we O
perform O
investigation O
on O
the O
THUMOS14 B-DAT
dataset, O
where O
we O
train O
the O

feature O
extrac- O
tion O
on O
the O
THUMOS14 B-DAT
dataset O

proposal O
per O
video O
on O
the O
THUMOS14 B-DAT
dataset O

for O
each O
video O
on O
the O
THUMOS14 B-DAT
dataset O
and O
20 O
clip O
proposals O

Method O
THUMOS14 B-DAT
ActivityNet O
(a) O
ActivityNet O
(b) O
TSN O

action O
recognition O
(WSR). O
On O
the O
THUMOS14 B-DAT
dataset, O
we O
train O
UntrimmedNet O
on O

THUMOS14 B-DAT
ActivityNet O
iDT+FV O
[45] O
63.1% O
iDT+FV O

methods O
on O
the O
datasets O
of O
THUMOS14 B-DAT
and O
AcitivtyNet O
(v1.2) O
for O
action O

WSR) O
on O
the O
datasets O
of O
THUMOS14 B-DAT
and O
ActivityNet O
in O
this O
subsection O

data O
for O
training O
on O
the O
THUMOS14 B-DAT
dataset, O
and O
use O
the O
untrimmed O

methods O
on O
the O
datasets O
of O
THUMOS14 B-DAT
for O
action O
detection O

to O
learn O
UntrimmedNets O
on O
the O
THUMOS14 B-DAT
dataset. O
As O
its O
training O
data O

of O
other O
methods O
on O
the O
THUMOS14 B-DAT
dataset O
and O
2.5% O
on O
the O

on O
the O
test O
data O
of O
THUMOS14 B-DAT
and O
AcitivtyNet. O
The O
left O
four O

above O
three O
videos O
are O
from O
THUMOS14 B-DAT
test O
data O
with O
action O
categories O

of O
action O
detection O
on O
the O
THUMOS14 B-DAT
dataset, O
based O
on O
the O
standard O

M. O
Shah, O
and O
R. O
Sukthankar. O
THUMOS B-DAT
challenge: O
Action O
recognition O
with O
a O

submission O
at O
thumos O
2014. O
In O
THUMOS B-DAT
Action O
Recognition O
challenge, O
pages O
1–7 O

In O
ECCV, O
pages O
628– O
643, O
2014 B-DAT

a O
large O
number O
of O
classes, O
2014 B-DAT

networks. O
In O
CVPR, O
pages O
1725–1732, O
2014 B-DAT

attention. O
In O
NIPS, O
pages O
2204–2212, O
2014 B-DAT

The O
lear O
submission O
at O
thumos O
2014 B-DAT

Action O
Recognition O
challenge, O
pages O
1–7, O
2014 B-DAT

videos. O
In O
NIPS, O
pages O
568–576, O
2014 B-DAT

single-label O
to O
multi-label. O
CoRR, O
abs/1406.5726, O
2014 B-DAT

Table O
3: O
Test B-DAT
scores O
and O
ranks O
for O
Task O

Cloze B-DAT
Task, O
and O
were O
used O
for O

dsem O
2017 O
shared O
task: O
The O
story B-DAT
cloze I-DAT
test I-DAT

Table O
1: O
Test B-DAT

Story B-DAT
Comprehension O
for O
Predicting O
What O
Happens O

Story B-DAT
Comprehension O
for O
Predicting O
What O
Happens O

Compu- O
tational O
Linguists O
(Mani, O
2012). O
Story B-DAT
compre- O
hension O
involves O
not O
only O

2 O
Predicting O
Story B-DAT
Ending O

1871 O
instances O
each O
for O
the O
Story B-DAT

4.1 O
Story B-DAT
understanding O

Story B-DAT
comprehension O
is O
a O
complex O
Natural O

Seymour O
Chatman. O
1980. O
Story B-DAT
and O
Discourse. O
Cor- O
nell O
University O

tion O
and O
Evolutionary O
Search O
for O
Story B-DAT
Generation. O
In O
Proceedings O
of O
the O

and O
Noah O
A. O
Smith. O
2017. O
Story B-DAT
cloze O
task: O
UW O
NLP O
system O

Effective O
Approach O
to O
the O
Story B-DAT
Cloze I-DAT
Test I-DAT

Effective O
Approach O
to O
the O
Story B-DAT
Cloze I-DAT
Test I-DAT

Abstract O
In O
the O
Story B-DAT
Cloze I-DAT
Test, I-DAT
a O
system O
is O
presented O

fully-neural O
approach O
to O
the O
Story B-DAT
Cloze I-DAT
Test I-DAT
using O
skip-thought O
embeddings O
of O

al. O
(2016) O
introduced O
the O
Story B-DAT
Cloze I-DAT
Test: I-DAT
given O
a O
four-sentence O
story O

ending O
from O
two O
options. O
The O
Cloze B-DAT
Test O
is O
intended O
to O
be O

Many O
previous O
approaches O
to O
the O
Cloze B-DAT
Test O
have O
ignored O
the O
training O

2016) O
presented O
the O
original O
Story B-DAT
Cloze I-DAT
Test, I-DAT
and O
showed O
that O
while O

The O
Story B-DAT
Cloze I-DAT
Test I-DAT
was O
the O
shared O
task O

various O
models O
on O
the O
Story B-DAT
Cloze I-DAT
Test I-DAT

we O
observe O
that O
the O
Story B-DAT
Cloze I-DAT
Test I-DAT
is O
an O
easier O
task O

achieves O
high O
accuracy O
on O
the O
Cloze B-DAT
Test, O
which O
is O
within O
1.1 O

dsem O
2017 O
shared O
task: O
The O
story B-DAT
cloze I-DAT
test I-DAT

rnn-based O
binary O
classifier O
for O
the O
story B-DAT
cloze I-DAT
test I-DAT

Approach O
to O
the O
Story O
Cloze O
Test B-DAT

Approach O
to O
the O
Story O
Cloze O
Test B-DAT

Abstract O
In O
the O
Story O
Cloze O
Test, B-DAT
a O
system O
is O
presented O
with O

approach O
to O
the O
Story O
Cloze O
Test B-DAT
using O
skip-thought O
embeddings O
of O
the O

2016) O
introduced O
the O
Story O
Cloze O
Test B-DAT

from O
two O
options. O
The O
Cloze O
Test B-DAT
is O
intended O
to O
be O
a O

previous O
approaches O
to O
the O
Cloze O
Test B-DAT
have O
ignored O
the O
training O
set O

presented O
the O
original O
Story O
Cloze O
Test, B-DAT
and O
showed O
that O
while O
humans O

The O
Story O
Cloze O
Test B-DAT
was O
the O
shared O
task O
at O

models O
on O
the O
Story O
Cloze O
Test B-DAT

observe O
that O
the O
Story O
Cloze O
Test B-DAT
is O
an O
easier O
task O
than O

high O
accuracy O
on O
the O
Cloze O
Test, B-DAT
which O
is O
within O
1.1% O
of O

and O
Effective O
Approach O
to O
the O
Story B-DAT
Cloze O
Test O

and O
Effective O
Approach O
to O
the O
Story B-DAT
Cloze O
Test O

Abstract O
In O
the O
Story B-DAT
Cloze O
Test, O
a O
system O
is O

pler O
fully-neural O
approach O
to O
the O
Story B-DAT
Cloze O
Test O
using O
skip-thought O
embeddings O

et O
al. O
(2016) O
introduced O
the O
Story B-DAT
Cloze O
Test: O
given O
a O
four-sentence O

Story B-DAT
Context O
Bob O
loved O
to O
watch O

al. O
(2016) O
presented O
the O
original O
Story B-DAT
Cloze O
Test, O
and O
showed O
that O

The O
Story B-DAT
Cloze O
Test O
was O
the O
shared O

for O
various O
models O
on O
the O
Story B-DAT
Cloze O
Test O

Finally, O
we O
observe O
that O
the O
Story B-DAT
Cloze O
Test O
is O
an O
easier O

Peng, O
and O
Dan O
Roth. O
2017. O
Story B-DAT
comprehension O
for O
predicting O
what O
hap O

and O
Noah O
A O
Smith. O
2017b. O
Story B-DAT
cloze O
task: O
Uw O
nlp O
system O

and O
Effective O
Approach O
to O
the O
Story B-DAT
Cloze I-DAT
Test I-DAT

and O
Effective O
Approach O
to O
the O
Story B-DAT
Cloze I-DAT
Test I-DAT

Abstract O
In O
the O
Story B-DAT
Cloze I-DAT
Test, I-DAT
a O
system O
is O
presented O
with O

pler O
fully-neural O
approach O
to O
the O
Story B-DAT
Cloze I-DAT
Test I-DAT
using O
skip-thought O
embeddings O
of O
the O

et O
al. O
(2016) O
introduced O
the O
Story B-DAT
Cloze I-DAT
Test I-DAT

al. O
(2016) O
presented O
the O
original O
Story B-DAT
Cloze I-DAT
Test, I-DAT
and O
showed O
that O
while O
humans O

The O
Story B-DAT
Cloze I-DAT
Test I-DAT
was O
the O
shared O
task O
at O

for O
various O
models O
on O
the O
Story B-DAT
Cloze I-DAT
Test I-DAT

Finally, O
we O
observe O
that O
the O
Story B-DAT
Cloze I-DAT
Test I-DAT
is O
an O
easier O
task O
than O

and O
evaluate O
them O
against O
the O
HOList B-DAT
benchmark I-DAT
for O
higher-order O
theorem O
proving O

We O
elected O
to O
use O
the O
HOList B-DAT
benchmark I-DAT
as O
opposed O
to O
the O
GamePad O

higher-order O
logic O
(HOL) O
terms. O
The O
HOList B-DAT
benchmark I-DAT
provides O
its O
data O
in O
the O

our O
experimental O
results O
on O
the O
HOList B-DAT
benchmark I-DAT
(Bansal O
et O
al. O
2019), O
which O

the O
state-of-the-art O
result O
on O
the O
HOList B-DAT
benchmark, I-DAT
clos- O
ing O
49.95% O
of O
the O

and O
evaluate O
them O
against O
the O
HOList B-DAT
benchmark O
for O
higher-order O
theorem O
proving O

test-bed O
for O
general-purpose O
rea- O
soning. O
HOList B-DAT
(Bansal O
et O
al. O
2019) O
is O

Hales O
et O
al. O
2017). O
The O
HOList B-DAT
environment O
and O
benchmark O
allows O
us O

we O
learn O
from O
in O
the O
HOList B-DAT
dataset O
were O
written O
by O
human O

We O
elected O
to O
use O
the O
HOList B-DAT
benchmark O
as O
opposed O
to O
the O

which O
is O
made O
simple O
in O
HOList B-DAT

higher-order O
logic O
(HOL) O
terms. O
The O
HOList B-DAT
benchmark O
provides O
its O
data O
in O

these O
fundamental O
building O
blocks O
of O
HOList B-DAT

mans O
and O
released O
in O
the O
HOList B-DAT
dataset. O
For O
evaluation, O
we O
plug O

proof O
search O
provided O
by O
the O
HOList B-DAT
environment. O
We O
first O
start O
with O

closed. O
During O
proof O
search, O
the O
HOList B-DAT
proof- O
search O
graph O
maintains O
several O

our O
experimental O
results O
on O
the O
HOList B-DAT
benchmark O
(Bansal O
et O
al. O
2019 O

the O
standard O
validation O
split O
of O
HOList B-DAT
which O
consists O
of O
3,225 O
held O

theorems O
and O
definitions O
in O
the O
HOList B-DAT
corpus. O
These O
theorems O
are O
proved O

the O
state-of-the-art O
result O
on O
the O
HOList B-DAT
benchmark, O
clos- O
ing O
49.95% O
of O

the O
baseline O
models O
in O
the O
HOList B-DAT
paper O
(32.65%) O
and O
even O
outperforms O

For O
each O
theorem O
in O
the O
HOList B-DAT
training O
set, O
there O
is O
only O

for O
imitation O
learning O
on O
the O
HOList B-DAT
theorem O
set O
and O
proof O
search O

all O
the O
theorems O
in O
the O
HOList B-DAT
theorem O
database. O
We O
can O
see O

evaluate O
them O
against O
the O
HOList O
benchmark B-DAT
for O
higher-order O
theorem O
proving O

less O
theorem O
proving O
API, O
a O
benchmark B-DAT
consisting O
of O
over O
twenty O
thousand O

2017). O
The O
HOList O
environment O
and O
benchmark B-DAT
allows O
us O
to O
measure O
progress O

models O
on O
a O
theorem O
proving O
benchmark B-DAT
where O
we O
demonstrate O
that O
our O

elected O
to O
use O
the O
HOList O
benchmark B-DAT
as O
opposed O
to O
the O
GamePad O

logic O
(HOL) O
terms. O
The O
HOList O
benchmark B-DAT
provides O
its O
data O
in O
the O

experimental O
results O
on O
the O
HOList O
benchmark B-DAT
(Bansal O
et O
al. O
2019), O
which O

state-of-the-art O
result O
on O
the O
HOList O
benchmark, B-DAT
clos- O
ing O
49.95% O
of O
the O

HOList B-DAT

Preprint O
– O
HOList B-DAT

Preprint O
– O
HOList B-DAT

Preprint O
– O
HOList B-DAT

Preprint O
– O
HOList B-DAT

Preprint O
– O
HOList B-DAT

Preprint O
– O
HOList B-DAT

Preprint O
– O
HOList B-DAT

Preprint O
– O
HOList B-DAT

Abstract O
We O
present O
an O
environment, O
benchmark, B-DAT
and O
deep O
learning O
driven O
automated O

which O
we O
derive O
a O
challenging O
benchmark B-DAT
for O
automated O
reasoning. O
We O
also O

strong O
initial O
results O
on O
this O
benchmark B-DAT

This O
paper O
provides O
a O
benchmark B-DAT
and O
reinforcement O
learning O
en- O
vironment O

and O
multivariate O
analysis. O
The O
resulting O
benchmark B-DAT
consists O
of O
2199 O
definitions O
and O

benchmark B-DAT

propose O
an O
easy O
to O
use O
benchmark B-DAT
or O
environment O
for O
machine O
learning O

of O
the O
Feit-Thompson O
theorem, O
their O
benchmark B-DAT
comprises O
only O
1602 O
theorems O
and O

24] O
proposed O
a O
machine O
learning O
benchmark B-DAT
for O
higher-order O
logic O
reasoning O
based O

The O
three O
corpora O
of O
the O
benchmark B-DAT

system. O
We O
also O
suggest O
a O
benchmark B-DAT
for O
machine O
reasoning O
in O
higher-order O

theorems O
with O
varying O
complexity. O
Our O
benchmark B-DAT
includes O
purely O
neural O
network O
based O

We O
evaluate O
these O
on O
the O
HOList B-DAT
benchmark I-DAT
for O
tactics O
based O
higher O
order O

can O
exceed O
20000 O
in O
the O
HOList B-DAT
benchmark I-DAT

3225 O
theorems O
– O
of O
the O
HOList B-DAT
benchmark I-DAT
as O
the O
held-out O
set. O
We O

of O
this O
approach O
on O
the O
HOList B-DAT
benchmark I-DAT

We O
evaluate O
these O
on O
the O
HOList B-DAT
benchmark O
for O
tactics O
based O
higher O

experiments O
we O
have O
used O
the O
HOList B-DAT
environment O
and O
benchmark O
for O
higher O

The O
HOList B-DAT
environment O
enables O
testing O
AI O
methods O

of O
the O
theorem O
prover. O
The O
HOList B-DAT
baseline O
solution O
by O
Bansal O
et O

complex O
analysis O
benchmark O
dataset O
of O
HOList B-DAT

can O
exceed O
20000 O
in O
the O
HOList B-DAT
benchmark. O
However, O
tactics O
are O
engineered O

3225 O
theorems O
– O
of O
the O
HOList B-DAT
benchmark O
as O
the O
held-out O
set O

of O
this O
approach O
on O
the O
HOList B-DAT
benchmark. O
This O
approach O
lowers O
the O

evaluate O
these O
on O
the O
HOList O
benchmark B-DAT
for O
tactics O
based O
higher O
order O

used O
the O
HOList O
environment O
and O
benchmark B-DAT
for O
higher O
order O
theorem O
proving O

Bansal O
et O
al. O
[2019]. O
This O
benchmark B-DAT
is O
based O
on O
the O
core O

tested O
on O
the O
complex O
analysis O
benchmark B-DAT
dataset O
of O
HOList. O
This O
benchmark O

exceed O
20000 O
in O
the O
HOList O
benchmark B-DAT

theorems O
– O
of O
the O
HOList O
benchmark B-DAT
as O
the O
held-out O
set. O
We O

this O
approach O
on O
the O
HOList O
benchmark B-DAT

HOList B-DAT

Preprint O
– O
HOList B-DAT

Preprint O
– O
HOList B-DAT

Preprint O
– O
HOList B-DAT

Preprint O
– O
HOList B-DAT

Preprint O
– O
HOList B-DAT

Preprint O
– O
HOList B-DAT

Preprint O
– O
HOList B-DAT

Preprint O
– O
HOList B-DAT

Abstract O
We O
present O
an O
environment, O
benchmark, B-DAT
and O
deep O
learning O
driven O
automated O

which O
we O
derive O
a O
challenging O
benchmark B-DAT
for O
automated O
reasoning. O
We O
also O

strong O
initial O
results O
on O
this O
benchmark B-DAT

This O
paper O
provides O
a O
benchmark B-DAT
and O
reinforcement O
learning O
en- O
vironment O

and O
multivariate O
analysis. O
The O
resulting O
benchmark B-DAT
consists O
of O
2199 O
definitions O
and O

benchmark B-DAT

propose O
an O
easy O
to O
use O
benchmark B-DAT
or O
environment O
for O
machine O
learning O

of O
the O
Feit-Thompson O
theorem, O
their O
benchmark B-DAT
comprises O
only O
1602 O
theorems O
and O

24] O
proposed O
a O
machine O
learning O
benchmark B-DAT
for O
higher-order O
logic O
reasoning O
based O

The O
three O
corpora O
of O
the O
benchmark B-DAT

system. O
We O
also O
suggest O
a O
benchmark B-DAT
for O
machine O
reasoning O
in O
higher-order O

theorems O
with O
varying O
complexity. O
Our O
benchmark B-DAT
includes O
purely O
neural O
network O
based O

variety O
of O
tasks O
from O
the O
DeepMind B-DAT
control O
suite, O
shown O
in O
Figure O

continuous O
control O
tasks O
of O
the O
DeepMind B-DAT
control O
suite O
(Tassa O
et O
al O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
Danijar O
Hafner O
1 O
2 O
Timothy O

Lillicrap B-DAT
3 O
Ian O
Fischer O
4 O

Ruben B-DAT
Villegas O
1 O
5 O
David O

Ha B-DAT
1 O
Honglak O
Lee O
1 O

James B-DAT
Davidson O
1 O

Abstract B-DAT
Planning O
has O
been O
very O

successful B-DAT
for O
control O
tasks O
with O

known B-DAT
environment O
dynamics. O
To O
leverage O

planning B-DAT
in O
unknown O
environments, O
the O

agent B-DAT
needs O
to O
learn O
the O

dynamics B-DAT
from O
interactions O
with O
the O

world. B-DAT
However, O
learning O
dynamics O
models O

that B-DAT
are O
accurate O
enough O
for O

planning B-DAT
has O
been O
a O
long-standing O

challenge, B-DAT
especially O
in O
image-based O
domains. O

We B-DAT
propose O
the O
Deep O
Planning O

Network B-DAT
(PlaNet), O
a O
purely O
model-based O

agent B-DAT
that O
learns O
the O
environment O

dynamics B-DAT
from O
images O
and O
chooses O

actions B-DAT
through O
fast O
online O
planning O

in B-DAT
latent O
space. O
To O
achieve O

high B-DAT
performance, O
the O
dynamics O
model O

must B-DAT
accurately O
predict O
the O
rewards O

ahead B-DAT
for O
multiple O
time O
steps. O

We B-DAT
approach O
this O
using O
a O

latent B-DAT
dynamics O
model O
with O
both O

deterministic B-DAT
and O
stochastic O
transition O
components. O

Moreover, B-DAT
we O
propose O
a O
multi-step O

variational B-DAT
inference O
objective O
that O
we O

name B-DAT
latent O
overshooting. O
Using O
only O

pixel B-DAT
observations, O
our O
agent O
solves O

continuous B-DAT
control O
tasks O
with O
contact O

dynamics, B-DAT
partial O
observability, O
and O
sparse O

rewards, B-DAT
which O
exceed O
the O
difficulty O

of B-DAT
tasks O
that O
were O
previously O

solved B-DAT
by O
planning O
with O
learned O

models. B-DAT
PlaNet O
uses O
substantially O
fewer O

episodes B-DAT
and O
reaches O
final O
performance O

close B-DAT
to O
and O
sometimes O
higher O

than B-DAT
strong O
model-free O
algorithms. O
1. O
Introduction O
Planning O
is O
a O

natural B-DAT
and O
powerful O
approach O
to O

decision B-DAT
making O
problems O
with O
known O

dynamics, B-DAT
such O
as O
game O
play O

- B-DAT
ing O
and O
simulated O
robot O

control B-DAT
(Tassa O
et O
al., O
2012; O

Silver B-DAT
et O
al., O
2017; O
Moravčík O

et B-DAT
al., O
2017). O
To O
plan O

in B-DAT
unknown O
environments, O
the O
agent O

needs B-DAT
to O
learn O
the O
dynamics O

from B-DAT
experience. O
Learning O
dynamics O
models O

that B-DAT
are O
accurate O
1Google O
Brain O
2University O
of O
Toronto O

3DeepMind B-DAT
4Google O
Research O
5University O
of O

Michigan. B-DAT
Correspondence O
to: O
Danijar O
Hafner O

<mail@danijar.com B-DAT

>. B-DAT
Proceedings O
of O
the O
36 O
th O

International B-DAT
Conference O
on O
Machine O
Learning O

, B-DAT
Long O
Beach, O
California, O
PMLR O
97, O
2019. O
Copyright O
2019 O
by O
the O

author(s B-DAT

). B-DAT
enough O
for O
planning O
has O
been O

a B-DAT
long-standing O
challenge. O
Key O
difficulties O

include B-DAT
model O
inaccuracies, O
accumulating O
errors O

of B-DAT
multi-step O
predictions, O
failure O
to O

capture B-DAT
multiple O
possible O
futures, O
and O

overconfident B-DAT
predictions O
outside O
of O
the O

training B-DAT
distribution O

. B-DAT
Planning O
using O
learned O
models O
offers O

several B-DAT
benefits O
over O
model-free O
reinforcement O

learning. B-DAT
First, O
model-based O
plan- O
ning O

can B-DAT
be O
more O
data O
efficient O

because B-DAT
it O
leverages O
a O
richer O

training B-DAT
signal O
and O
does O
not O

require B-DAT
propagating O
rewards O
through O
Bellman O

backups. B-DAT
Moreover, O
planning O
carries O
the O

promise B-DAT
of O
increasing O
performance O
just O

by B-DAT
increasing O
the O
computational O
budget O

for B-DAT
searching O
for O
actions, O
as O

shown B-DAT
by O
Silver O
et O
al O

. B-DAT
(2017). O
Finally, O
learned O
dynamics O

can B-DAT
be O
independent O
of O
any O

specific B-DAT
task O
and O
thus O
have O

the B-DAT
potential O
to O
transfer O
well O

to B-DAT
other O
tasks O
in O
the O

environment. B-DAT
Recent O
work O
has O
shown O
promise O

in B-DAT
learning O
the O
dynamics O
of O

simple B-DAT
low-dimensional O
environments O
(Deisenroth O

& B-DAT
Ras- O
mussen, O
2011; O
Gal O

et B-DAT
al., O
2016; O
Amos O
et O

al., B-DAT
2018; O
Chua O
et O
al O

., B-DAT
2018; O
Henaff O
et O
al., O
2018 O

). B-DAT
However, O
these O
approaches O
typically O

assume B-DAT
access O
to O
the O
underlying O

state B-DAT
of O
the O
world O
and O

the B-DAT
reward O
function, O
which O
may O

not B-DAT
be O
available O
in O
prac- O

tice. B-DAT
In O
high-dimensional O
environments, O
we O

would B-DAT
like O
to O
learn O
the O

dynamics B-DAT
in O
a O
compact O
latent O

space B-DAT
to O
enable O
fast O
planning. O

The B-DAT
success O
of O
such O
latent O

models B-DAT
has O
previously O
been O
limited O

to B-DAT
simple O
tasks O
such O
as O

balancing B-DAT
cartpoles O
and O
controlling O
2-link O

arms B-DAT
from O
dense O
rewards O
(Watter O

et B-DAT
al., O
2015; O
Banijamali O
et O

al., B-DAT
2017). O
In O
this O
paper, O
we O
propose O

the B-DAT
Deep O
Planning O
Network O
(PlaNet O

), B-DAT
a O
model-based O
agent O
that O

learns B-DAT
the O
environment O
dynamics O
from O

pixels B-DAT
and O
chooses O
actions O
through O

online B-DAT
planning O
in O
a O
compact O

latent B-DAT
space. O
To O
learn O
the O

dynamics, B-DAT
we O
use O
a O
transition O

model B-DAT
with O
both O
stochastic O
and O

determin- B-DAT
istic O
components. O
Moreover, O
we O

experiment B-DAT
with O
a O
novel O
generalized O

variational B-DAT
objective O
that O
encourages O
multi-step O

predictions. B-DAT
PlaNet O
solves O
continuous O
control O

tasks B-DAT
from O
pixels O
that O
are O

more B-DAT
difficult O
than O
those O
previously O

solved B-DAT
by O
planning O
with O
learned O

models. B-DAT
Key O
contributions O
of O
this O
work O

are B-DAT
summarized O
as O
follows O

: B-DAT
• O
Planning O
in O
latent O
spaces O

We B-DAT
solve O
a O
variety O
of O

tasks B-DAT
from O
the O
DeepMind O
control O

suite, B-DAT
shown O
in O
Figure O
1 O

, B-DAT
by O
learning O
a O
dynamics O

model B-DAT
and O
efficiently O
planning O
in O
ar O
X O

iv B-DAT
:1 O
81 O
1 O

. B-DAT
04 O
55 O

1v B-DAT
5 O

cs B-DAT
.L O
G O

4 B-DAT

J B-DAT

2 B-DAT
01 O
9 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
(a) O
Cartpole O
(b) O
Reacher O
(c O

) B-DAT
Cheetah O
(d) O
Finger O
(e) O

Cup B-DAT
(f) O
Walker O
Figure O
1: O
Image-based O
control O
domains O

used B-DAT
in O
our O
experiments. O
The O

images B-DAT
show O
agent O
observations O
before O

downscaling B-DAT
to O
64× O
64× O
3 O

pixels. B-DAT
(a) O
The O
cartpole O
swingup O

task B-DAT
has O
a O
fixed O
camera O

so B-DAT
the O
cart O
can O
move O

out B-DAT
of O
sight. O
(b) O
The O

reacher B-DAT
task O
has O
only O
a O

sparse B-DAT
reward. O
(c) O
The O
cheetah O

running B-DAT
task O
includes O
both O
contacts O

and B-DAT
a O
larger O
number O
of O

joints. B-DAT
(d) O
The O
finger O
spinning O

task B-DAT
includes O
contacts O
between O
the O

finger B-DAT
and O
the O
object. O
(e O

) B-DAT
The O
cup O
task O
has O

a B-DAT
sparse O
reward O
that O
is O

only B-DAT
given O
once O
the O
ball O

is B-DAT
caught. O
(f) O
The O
walker O

task B-DAT
requires O
balance O
and O
predicting O

difficult B-DAT
interactions O
with O
the O
ground O

when B-DAT
the O
robot O
is O
lying O

down. B-DAT
its O
latent O
space. O
Our O
agent O

substantially B-DAT
outperforms O
the O
model-free O
A3C O

and B-DAT
in O
some O
cases O
D4PG O

algorithm B-DAT
in O
final O
performance, O
with O

on B-DAT
average O
200× O
less O
environ O

- B-DAT
ment O
interaction O
and O
similar O

computation B-DAT
time. O
• O
Recurrent O
state O
space O
model O

We B-DAT
design O
a O
latent O
dy O

- B-DAT
namics O
model O
with O
both O

deterministic B-DAT
and O
stochastic O
components O
(Buesing O

et B-DAT
al., O
2018; O
Chung O
et O

al., B-DAT
2015). O
Our O
experiments O
indicate O

having B-DAT
both O
components O
to O
be O

crucial B-DAT
for O
high O
planning O
performance. O
• O
Latent O
overshooting O
We O
generalize O

the B-DAT
standard O
vari- O
ational O
bound O

to B-DAT
include O
multi-step O
predictions. O
Using O

only B-DAT
terms O
in O
latent O
space O

results B-DAT
in O
a O
fast O
regularizer O

that B-DAT
can O
improve O
long-term O
predictions O

and B-DAT
is O
compati- O
ble O
with O

any B-DAT
latent O
sequence O
model O

. B-DAT
2. O
Latent O
Space O
Planning O
To O

solve B-DAT
unknown O
environments O
via O
planning O

, B-DAT
we O
need O
to O
model O

the B-DAT
environment O
dynamics O
from O
experience. O

PlaNet B-DAT
does O
so O
by O
iteratively O

collecting B-DAT
data O
using O
planning O
and O

training B-DAT
the O
dynamics O
model O
on O

the B-DAT
gathered O
data. O
In O
this O

section, B-DAT
we O
introduce O
notation O
for O

the B-DAT
environment O
and O
de- O
scribe O

the B-DAT
general O
implementation O
of O
our O

model-based B-DAT
agent. O
In O
this O
section, O

we B-DAT
assume O
access O
to O
a O

learned B-DAT
dynamics O
model. O
Our O
design O

and B-DAT
training O
objective O
for O
this O

model B-DAT
are O
detailed O
in O
Section O
3 O

. B-DAT
Problem O
setup O
Since O
individual O
image O

observations B-DAT
gen- O
erally O
do O
not O

reveal B-DAT
the O
full O
state O
of O

the B-DAT
environment, O
we O
consider O
a O

partially B-DAT
observable O
Markov O
decision O
process O

(POMDP). B-DAT
We O
define O
a O
discrete O

time B-DAT
step O
t, O
hidden O
states O

st, B-DAT
image O
observations O
ot, O
continuous O

action B-DAT
vectors O
at, O
and O
scalar O

rewards B-DAT
rt, O
that O
follow O
the O

stochastic B-DAT
dynamics O

Transition B-DAT
function: O
st O
∼ O

p(st B-DAT
| O
st−1, O
at−1) O
Observation O

function: B-DAT
ot O
∼ O
p(ot O
| O

st) B-DAT
Reward O
function: O
rt O
∼ O

p(rt B-DAT
| O
st) O
Policy: O

at B-DAT
∼ O
p(at O
| O
o≤t, O

a<t), B-DAT
(1 O

) B-DAT
Algorithm O
1: O
Deep O
Planning O
Network O

(PlaNet) B-DAT
Input O
: O
R O
Action O

repeat B-DAT
S O
Seed O
episodes O
C O

Collect B-DAT
interval O
B O
Batch O
size O

L B-DAT
Chunk O
length O
α O
Learning O

rate B-DAT

p(st B-DAT
| O
st−1, O
at−1) O
Transition O

model B-DAT
p(ot O
| O
st) O
Observation O

model B-DAT
p(rt O
| O
st) O
Reward O

model B-DAT
q(st O
| O
o≤t, O
a<t) O

Encoder B-DAT
p(�) O
Exploration O
noise O
1 O
Initialize O
dataset O
D O
with O

S B-DAT
random O
seed O
episodes. O
2 O

Initialize B-DAT
model O
parameters O
θ O
randomly O

. B-DAT
3 O
while O
not O
converged O

do B-DAT
// O
Model O
fitting O
4 O
for O

update B-DAT
step O
s O
= O
1..C O

do B-DAT
5 O
Draw O
sequence O
chunks O

{(ot, B-DAT
at, O
rt)L+kt=k O
}Bi=1 O

∼ B-DAT
D O

uniformly B-DAT
at O
random O
from O
the O

dataset. B-DAT
6 O
Compute O
loss O
L(θ) O

from B-DAT
Equation O
3. O
7 O
Update O

model B-DAT
parameters O
θ O
← O

θ B-DAT
− O
α∇θL(θ). O
// O
Data O
collection O
8 O
o1 O

← B-DAT
env.reset() O
9 O
for O
time O

step B-DAT
t O
= O
1 O

.. B-DAT
⌈ O
T O
R O

do B-DAT
10 O
Infer O
belief O
over O
current O

state B-DAT
q(st O
| O
o≤t, O
a<t O

) B-DAT
from O
the O
history. O
11 O
at O
← O
planner(q(st O

| B-DAT
o≤t, O
a<t), O
p), O
see O

Algorithm B-DAT
2 O
in O
the O
appendix O

for B-DAT
details O

. B-DAT
12 O
Add O
exploration O
noise O

� B-DAT
∼ O
p(�) O
to O
the O

action. B-DAT
13 O
for O
action O
repeat O

k B-DAT
= O
1..R O
do O
14 O

rkt B-DAT
, O
o O

k B-DAT
t+1 O
← O
env.step(at) O
15 O
rt, O
ot+1 O
← O
∑R O

k=1 B-DAT
r O

k B-DAT
t O
, O
o O
R O
t+1 O

16 B-DAT
D O
← O

D B-DAT
∪ O
{(ot, O
at, O
rt)Tt=1} O
where O
we O
assume O
a O
fixed O

initial B-DAT
state O
s0 O
without O
loss O

of B-DAT
gen- O
erality. O
The O
goal O

is B-DAT
to O
implement O
a O
policy O

p(at B-DAT
| O
o≤t, O
a<t) O
that O

maximizes B-DAT
the O
expected O
sum O
of O

rewards B-DAT
Ep O

T B-DAT
t=1 O

where B-DAT
the O
expectation O
is O
over O

the B-DAT
distributions O
of O
the O
envi- O

ronment B-DAT
and O
the O
policy. O
2 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
Model-based O
planning O
PlaNet O
learns O
a O

transition B-DAT
model O
p(st O
| O
st−1 O

, B-DAT
at−1), O
observation O
model O

p(ot B-DAT
| O
st), O
and O
reward O

model B-DAT
p(rt O
| O
st) O
from O

previously B-DAT
experienced O
episodes O
(note O
italic O

letters B-DAT
for O
the O
model O
compared O

to B-DAT
upright O
letters O
for O
the O

true B-DAT
dynamics). O
The O
observation O
model O

provides B-DAT
a O
rich O
training O
signal O

but B-DAT
is O
not O
used O
for O

planning. B-DAT
We O
also O
learn O
an O

encoder B-DAT
q(st O
| O
o≤t, O
a<t) O

to B-DAT
infer O
an O
approximate O
belief O

over B-DAT
the O
current O
hidden O
state O

from B-DAT
the O
history O
using O
filtering. O

Given B-DAT
these O
components, O
we O
implement O

the B-DAT
policy O
as O
a O
planning O

algorithm B-DAT
that O
searches O
for O
the O

best B-DAT
sequence O
of O
future O
actions. O

We B-DAT
use O
model-predictive O
control O
(MPC; O

Richards, B-DAT
2005) O
to O
allow O
the O

agent B-DAT
to O
adapt O
its O
plan O

based B-DAT
on O
new O
observations, O
meaning O

we B-DAT
replan O
at O
each O
step. O

In B-DAT
contrast O
to O
model-free O
and O

hybrid B-DAT
reinforcement O
learning O
algorithms, O
we O

do B-DAT
not O
use O
a O
policy O

or B-DAT
value O
network. O
Experience O
collection O
Since O
the O
agent O

may B-DAT
not O
initially O
visit O
all O

parts B-DAT
of O
the O
environment, O
we O

need B-DAT
to O
iteratively O
collect O
new O

experience B-DAT
and O
refine O
the O
dynamics O

model. B-DAT
We O
do O
so O
by O

planning B-DAT
with O
the O
partially O
trained O

model, B-DAT
as O
shown O
in O
Algorithm O

1. B-DAT
Starting O
from O
a O
small O

amount B-DAT
of O
S O
seed O
episodes O

collected B-DAT
under O
random O
actions, O
we O

train B-DAT
the O
model O
and O
add O

one B-DAT
additional O
episode O
to O
the O

data B-DAT
set O
everyC O
update O
steps O

. B-DAT
When O
collecting O
episodes O
for O

the B-DAT
data O
set, O
we O
add O

small B-DAT
Gaussian O
exploration O
noise O
to O

the B-DAT
action. O
To O
reduce O
the O

planning B-DAT
horizon O
and O
provide O
a O

clearer B-DAT
learning O
signal O
to O
the O

model, B-DAT
we O
repeat O
each O
action O

R B-DAT
times, O
as O
common O
in O

reinforcement B-DAT
learning O
(Mnih O
et O
al., O
2015 O

; B-DAT
2016). O
Planning O
algorithm O
We O
use O
the O

cross B-DAT
entropy O
method O
(CEM; O
Rubinstein O

, B-DAT
1997; O
Chua O
et O
al., O
2018 O

) B-DAT
to O
search O
for O
the O

best B-DAT
action O
sequence O
under O
the O

model, B-DAT
as O
outlined O
in O
Algorithm O
2 O

. B-DAT
We O
decided O
on O
this O

algorithm B-DAT
because O
of O
its O
robustness O

and B-DAT
because O
it O
solved O
all O

considered B-DAT
tasks O
when O
given O
the O

true B-DAT
dynamics O
for O
planning. O
CEM O

is B-DAT
a O
population- O
based O
optimization O

algorithm B-DAT
that O
infers O
a O
distribution O

over B-DAT
action O
sequences O
that O
maximize O

the B-DAT
objective. O
As O
detailed O
in O

Algorithm B-DAT
2 O
in O
the O
appendix, O

we B-DAT
initialize O
a O
time-dependent O
diagonal O

Gaussian B-DAT
belief O
over O
optimal O
action O

sequences B-DAT
at:t+H O
∼ O
Normal(µt:t+H O
, O

σ2t:t+HI), B-DAT
where O
t O
is O
the O

current B-DAT
time O
step O
of O
the O

agent B-DAT
and O
H O
is O
the O

length B-DAT
of O
the O
planning O
horizon. O

Starting B-DAT
from O
zero O
mean O
and O

unit B-DAT
variance, O
we O
repeatedly O
sample O

J B-DAT
candidate O
action O
sequences, O
evaluate O

them B-DAT
under O
the O
model, O
and O

re-fit B-DAT
the O
belief O
to O
the O

top B-DAT
K O
action O
sequences. O
After O

I B-DAT
iterations, O
the O
planner O
returns O

the B-DAT
mean O
of O
the O
belief O

for B-DAT
the O
current O
time O
step, O

µt. B-DAT
Importantly, O
after O
receiving O
the O

next B-DAT
observation, O
the O
belief O
over O

action B-DAT
sequences O
starts O
from O
zero O

mean B-DAT
and O
unit O
variance O
again O

to B-DAT
avoid O
local O
optima. O
To O
evaluate O
a O
candidate O
action O

sequence B-DAT
under O
the O
learned O
model O

, B-DAT
we O
sample O
a O
state O

trajectory B-DAT
starting O
from O
the O
current O

state B-DAT
belief, O
and O
sum O
the O

mean B-DAT
rewards O
predicted O
along O
the O

sequence. B-DAT
Since O
we O
use O
a O

population-based B-DAT
optimizer, O
we O
found O
it O
sufficient O
to O

consider B-DAT
a O
single O
trajectory O
per O

action B-DAT
sequence O
and O
thus O
focus O

the B-DAT
computational O
budget O
on O
evaluating O

a B-DAT
larger O
number O
of O
different O

sequences. B-DAT
Because O
the O
reward O
is O

modeled B-DAT
as O
a O
function O
of O

the B-DAT
latent O
state, O
the O
planner O

can B-DAT
operate O
purely O
in O
latent O

space B-DAT
without O
generating O
images, O
which O

allows B-DAT
for O
fast O
evaluation O
of O

large B-DAT
batches O
of O
action O
sequences O

. B-DAT
The O
next O
section O
introduces O

the B-DAT
latent O
dynamics O
model O
that O

the B-DAT
planner O
uses. O
3. O
Recurrent O
State O
Space O
Model O

For B-DAT
planning, O
we O
need O
to O

evaluate B-DAT
thousands O
of O
action O
se O

- B-DAT
quences O
at O
every O
time O

step B-DAT
of O
the O
agent. O
Therefore, O

we B-DAT
use O
a O
recurrent O
state-space O

model B-DAT
(RSSM) O
that O
can O
predict O

for- B-DAT
ward O
purely O
in O
latent O

space, B-DAT
similar O
to O
recently O
proposed O

models B-DAT
(Karl O
et O
al., O
2016; O

Buesing B-DAT
et O
al., O
2018; O
Doerr O

et B-DAT
al., O
2018). O
This O
model O

can B-DAT
be O
thought O
of O
as O

a B-DAT
non-linear O
Kalman O
filter O
or O

sequential B-DAT
VAE. O
Instead O
of O
an O

extensive B-DAT
comparison O
to O
prior O
architectures, O

we B-DAT
highlight O
two O
findings O
that O

can B-DAT
guide O
future O
designs O
of O

dynamics B-DAT
models: O
our O
experiments O
show O

that B-DAT
both O
stochastic O
and O
deterministic O

paths B-DAT
in O
the O
transition O
model O

are B-DAT
crucial O
for O
successful O
planning. O

In B-DAT
this O
section, O
we O
remind O

the B-DAT
reader O
of O
latent O
state-space O

models B-DAT
and O
then O
describe O
our O

dynamics B-DAT
model. O
Latent O
dynamics O
We O
consider O
sequences O

{ot, B-DAT
at, O
rt}Tt=1 O
with O
discrete O

time B-DAT
step O
t, O
image O
observations O

ot, B-DAT
continuous O
action O
vectors O
at O

, B-DAT
and O
scalar O
rewards O
rt. O

A B-DAT
typical O
latent O
state-space O
model O

is B-DAT
shown O
in O
Figure O
2b O

and B-DAT
resembles O
the O
structure O
of O

a B-DAT
partially O
observable O
Markov O
decision O

process. B-DAT
It O
defines O
the O
generative O

process B-DAT
of O
the O
images O
and O

rewards B-DAT
using O
a O
hidden O
state O

sequence B-DAT
{st}Tt=1, O
Transition O
model: O
st O
∼ O
p(st O

| B-DAT
st−1, O
at−1) O
Observation O
model O

: B-DAT
ot O
∼ O
p(ot O
| O

st) B-DAT
Reward O
model: O
rt O
∼ O

p(rt B-DAT
| O
st), O
(2 O

) B-DAT
where O
we O
assume O
a O
fixed O

initial B-DAT
state O
s0 O
without O
loss O

of B-DAT
generality. O
The O
transition O
model O

is B-DAT
Gaussian O
with O
mean O
and O

variance B-DAT
parameterized O
by O
a O
feed-forward O

neural B-DAT
network, O
the O
observation O
model O

is B-DAT
Gaussian O
with O
mean O
parameterized O

by B-DAT
a O
deconvolutional O
neural O
network O

and B-DAT
identity O
covariance, O
and O
the O

reward B-DAT
model O
is O
a O
scalar O

Gaussian B-DAT
with O
mean O
param- O
eterized O

by B-DAT
a O
feed-forward O
neural O
network O

and B-DAT
unit O
variance. O
Note O
that O

the B-DAT
log-likelihood O
under O
a O
Gaussian O

distribution B-DAT
with O
unit O
variance O
equals O

the B-DAT
mean O
squared O
error O
up O

to B-DAT
a O
constant O

. B-DAT
Variational O
encoder O
Since O
the O
model O

is B-DAT
non-linear, O
we O
cannot O
directly O

compute B-DAT
the O
state O
posteriors O
that O

are B-DAT
needed O
for O
parameter O
learning O

. B-DAT
Instead, O
we O
use O
an O

encoder B-DAT
q(s1:T O
| O
o1:T O
, O

a1:T B-DAT
) O
= O
∏T O
t=1 O
q(st O
| O
st−1 O

, B-DAT
at−1, O
ot) O
to O
infer O

approx- B-DAT
imate O
state O
posteriors O
from O
past O

observations B-DAT
and O
actions, O
where O
q(st O

| B-DAT
st−1, O
at−1, O
ot) O
is O

a B-DAT
diagonal O
Gaussian O
with O
mean O

and B-DAT
variance O
parameterized O
by O
a O

convolutional B-DAT
neural O

3 B-DAT

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
o1, O
r1 O
o2, O
r2 O
o3 O

, B-DAT
r3 O
h1 O
h2 O
h3 O

a1 B-DAT
a2 O
(a) O
Deterministic O
model O
(RNN O

) B-DAT
o1, O
r1 O
o2, O
r2 O
o3 O

, B-DAT
r3 O
s1 O
s2 O
s3 O

a1 B-DAT
a2 O
(b) O
Stochastic O
model O
(SSM O

) B-DAT
o1, O
r1 O
o2, O
r2 O
o3 O

, B-DAT
r3 O
s1 O
s2 O
s3 O

h1 B-DAT
h2 O
h3 O
a1 O
a2 O

c) B-DAT
Recurrent O
state-space O
model O
(RSSM) O
Figure O
2: O
Latent O
dynamics O
model O

designs. B-DAT
In O
this O
example, O
the O

model B-DAT
observes O
the O
first O
two O

time B-DAT
steps O
and O
predicts O
the O

third. B-DAT
Circles O
represent O
stochastic O
variables O

and B-DAT
squares O
deterministic O
variables. O
Solid O

lines B-DAT
denote O
the O
generative O
process O

and B-DAT
dashed O
lines O
the O
inference O

model. B-DAT
(a) O
Transitions O
in O
a O

recurrent B-DAT
neural O
network O
are O
purely O

deterministic. B-DAT
This O
prevents O
the O
model O

from B-DAT
capturing O
multiple O
futures O
and O

makes B-DAT
it O
easy O
for O
the O

planner B-DAT
to O
exploit O
inaccuracies. O
(b O

) B-DAT
Transitions O
in O
a O
state-space O

model B-DAT
are O
purely O
stochastic. O
This O

makes B-DAT
it O
difficult O
to O
remember O

information B-DAT
over O
multiple O
time O
steps. O
( O

c) B-DAT
We O
split O
the O
state O

into B-DAT
stochastic O
and O
deterministic O
parts, O

allowing B-DAT
the O
model O
to O
robustly O

learn B-DAT
to O
predict O
multiple O
futures. O
network O
followed O
by O
a O
feed-forward O

neural B-DAT
network. O
We O
use O
the O

filtering B-DAT
posterior O
that O
conditions O
on O

past B-DAT
observations O
since O
we O
are O

ultimately B-DAT
interested O
in O
using O
the O

model B-DAT
for O
planning, O
but O
one O

may B-DAT
also O
use O
the O
full O

smoothing B-DAT
posterior O
during O
training O
(Babaeizadeh O

et B-DAT
al., O
2017; O
Gregor O

& B-DAT
Besse, O
2018 O

). B-DAT
Training O
objective O
Using O
the O
encoder O

, B-DAT
we O
construct O
a O
variational O

bound B-DAT
on O
the O
data O
log-likelihood. O

For B-DAT
simplicity, O
we O
write O
losses O

for B-DAT
predicting O
only O
the O

observations B-DAT
— O
the O
reward O
losses O

follow B-DAT
by O
analogy. O
The O
variational O

bound B-DAT
obtained O
using O
Jensen’s O
inequality O

is B-DAT
ln O
p(o1:T O
| O
a1:T O

) B-DAT
, O
ln O

t B-DAT
p(st O
| O
st−1, O
at−1)p(ot O

| B-DAT
st) O
ds1:T O

T∑ B-DAT
t=1 O

Eq(st|o≤t,a<t)[ln B-DAT
p(ot O
| O
st)] O
reconstruction O

E B-DAT
q(st−1|o≤t−1,a<t−1) O
[ O
KL[q(st O
| O
o≤t, O
a<t O

) B-DAT
‖ O
p(st O
| O
st−1, O

at−1)] B-DAT
] O
complexity O

3) B-DAT
For O
the O
derivation, O
please O
see O

Equation B-DAT
8 O
in O
the O
appendix O

. B-DAT
Estimating O
the O
outer O
expectations O

using B-DAT
a O
single O
reparam- O
eterized O

sample B-DAT
yields O
an O
efficient O
objective O

for B-DAT
inference O
and O
learning O
in O

non-linear B-DAT
latent O
variable O
models O
that O

can B-DAT
be O
optimized O
using O
gradient O

ascent B-DAT
(Kingma O
& O
Welling, O
2013; O

Rezende B-DAT
et O
al., O
2014; O
Krishnan O

et B-DAT
al., O
2017). O
Deterministic O
path O
Despite O
its O
generality O

, B-DAT
the O
purely O
stochastic O
transitions O

make B-DAT
it O
difficult O
for O
the O

transition B-DAT
model O
to O
reliably O
remember O

information B-DAT
for O
multiple O
time O
steps. O

In B-DAT
theory, O
this O
model O
could O

learn B-DAT
to O
set O
the O
variance O

to B-DAT
zero O
for O
some O
state O

components, B-DAT
but O
the O
optimization O
pro- O

cedure B-DAT
may O
not O
find O
this O

solution. B-DAT
This O
motivates O
including O
a O

deterministic B-DAT
sequence O
of O
activation O

vectors B-DAT
{ht}Tt=1 O
that O
allow O
the O

model B-DAT
to O
access O
not O
just O

the B-DAT
last O
state O
but O
all O

pre- B-DAT
vious O
states O
deterministically O
(Chung O

et B-DAT
al., O
2015; O
Buesing O
et O

al., B-DAT
2018). O
We O
use O
such O

a B-DAT
model, O
shown O
in O
Figure O

2c, B-DAT
that O
we O
name O
recurrent O
state-space O
model O

(RSSM B-DAT

), B-DAT
Deterministic O
state O
model: O
ht O

= B-DAT
f(ht−1, O
st−1, O
at−1) O
Stochastic O

state B-DAT
model: O
st O
∼ O
p(st O

| B-DAT
ht) O
Observation O
model: O
ot O

∼ B-DAT
p(ot O
| O
ht, O
st O

) B-DAT
Reward O
model: O
rt O
∼ O

p(rt B-DAT
| O
ht, O
st), O
(4 O

) B-DAT
where O
f(ht−1, O
st−1, O
at−1) O
is O

implemented B-DAT
as O
a O
recurrent O
neural O

network B-DAT
(RNN). O
Intuitively, O
we O
can O

understand B-DAT
this O
model O
as O
splitting O

the B-DAT
state O
into O
a O
stochastic O

part B-DAT
st O
and O
a O
de O

- B-DAT
terministic O
part O
ht, O
which O

depend B-DAT
on O
the O
stochastic O
and O

deter- B-DAT
ministic O
parts O
at O
the O

previous B-DAT
time O
step O
through O
the O

RNN. B-DAT
We O
use O
the O
encoder O

q(s1:T B-DAT
| O
o1:T O
, O

a1:T B-DAT
) O
= O
∏T O
t=1 O
q(st O
| O
ht O

, B-DAT
ot) O
to O
parameterize O
the O
approximate O
state O

posteriors. B-DAT
Impor- O
tantly, O
all O
information O

about B-DAT
the O
observations O
must O
pass O

through B-DAT
the O
sampling O
step O
of O

the B-DAT
encoder O
to O
avoid O
a O

deter- B-DAT
ministic O
shortcut O
from O
inputs O

to B-DAT
reconstructions O

. B-DAT
In O
the O
next O
section, O
we O

identify B-DAT
a O
limitation O
of O
the O

standard B-DAT
objective O
for O
latent O
sequence O

models B-DAT
and O
propose O
a O
general O

- B-DAT
ization O
of O
it O
that O

improves B-DAT
long-term O
predictions. O
4. O
Latent O
Overshooting O
In O
the O

previous B-DAT
section, O
we O
derived O
the O

typical B-DAT
variational O
bound O
for O
learning O

and B-DAT
inference O
in O
latent O
sequence O

models B-DAT
(Equation O
3). O
As O
show O

in B-DAT
Figure O
3a, O
this O
objective O

function B-DAT
contains O
reconstruction O
terms O
for O

the B-DAT
observations O
and O
KL- O
divergence O

regularizers B-DAT
for O
the O
approximate O
posteriors O

. B-DAT
A O
limitation O
of O
this O

objective B-DAT
is O
that O
the O
stochastic O

path B-DAT
of O
the O
transition O
function O

p(st B-DAT
| O
st−1, O
at−1) O
is O

only B-DAT
trained O
via O
the O
KL-divergence O

regularizers B-DAT
for O
one-step O
predictions: O
the O

gra- B-DAT
dient O
flows O
through O

p(st B-DAT
| O
st−1, O
at−1) O
directly O

into B-DAT
q(st−1) O
but O
never O
traverses O

a B-DAT
chain O
of O
multiple O

p(st B-DAT
| O
st−1, O
at−1). O
In O

this B-DAT
section, O
we O
generalize O
this O

variational B-DAT
bound O
to O
latent O
overshooting, O

which B-DAT
trains O
all O
multi-step O
predictions O

in B-DAT
la- O
tent O
space. O
We O

found B-DAT
that O
several O
dynamics O
models O

benefit B-DAT
4 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
o1, O
r1 O
o2, O
r2 O
o3 O

, B-DAT
r3 O
s1|1 O
s2|2 O
s3|3 O

s2|1 B-DAT
s3|2 O
(a) O
Standard O
variational O
bound O

o1, B-DAT
r1 O
o2, O
r2 O
o3, O

r3 B-DAT
s1|1 O
s2|2 O
s3|3 O

s2|1 B-DAT
s3|2 O
s3|1 O

b) B-DAT
Observation O
overshooting O
o1, O
r1 O
o2, O
r2 O
o3 O

, B-DAT
r3 O
s1|1 O
s2|2 O
s3|3 O

s2|1 B-DAT
s3|2 O
s3|1 O

c) B-DAT
Latent O
overshooting O
Figure O
3: O
Unrolling O
schemes. O
The O

labels B-DAT
si|j O
are O
short O
for O

the B-DAT
state O
at O
time O
i O

conditioned B-DAT
on O
observations O
up O
to O

time B-DAT
j. O
Arrows O
pointing O
at O

shaded B-DAT
circles O
indicate O
log-likelihood O
loss O

terms. B-DAT
Wavy O
arrows O
indicate O
KL-divergence O

loss B-DAT
terms. O
(a) O
The O
standard O

variational B-DAT
objectives O
decodes O
the O
posterior O

at B-DAT
every O
step O
to O
compute O

the B-DAT
reconstruction O
loss. O
It O
also O

places B-DAT
a O
KL O
on O
the O

prior B-DAT
and O
posterior O
at O
every O

step, B-DAT
which O
trains O
the O
transition O

function B-DAT
for O
one-step O
predictions. O
(b O

) B-DAT
Observation O
overshooting O
(Amos O
et O

al., B-DAT
2018) O
decodes O
all O
multi-step O

predictions B-DAT
to O
apply O
additional O
reconstruction O

losses. B-DAT
This O
is O
typically O
too O

expensive B-DAT
in O
image O
domains. O
(c) O

Latent B-DAT
overshooting O
predicts O
all O
multi-step O

priors. B-DAT
These O
state O
beliefs O
are O

trained B-DAT
towards O
their O
corresponding O
posteriors O

in B-DAT
latent O
space O
to O
encourage O

accurate B-DAT
multi-step O
predictions. O
from O
latent O
overshooting, O
although O
our O

final B-DAT
agent O
using O
the O
RSSM O

model B-DAT
does O
not O
require O
it O

(see B-DAT
Appendix O
D O

). B-DAT
Limited O
capacity O
If O
we O
could O

train B-DAT
our O
model O
to O
make O

per- B-DAT
fect O
one-step O
predictions, O
it O

would B-DAT
also O
make O
perfect O
multi O

- B-DAT
step O
predictions, O
so O
this O

would B-DAT
not O
be O
a O
problem. O

However, B-DAT
when O
using O
a O
model O

with B-DAT
limited O
capacity O
and O
restricted O

distributional B-DAT
family, O
training O
the O
model O

only B-DAT
on O
one-step O
predictions O
until O

convergence B-DAT
does O
in O
general O
not O

coincide B-DAT
with O
the O
model O
that O

is B-DAT
best O
at O
multi-step O
predictions. O

For B-DAT
suc- O
cessful O
planning, O
we O

need B-DAT
accurate O
multi-step O
predictions. O
Therefore, O

we B-DAT
take O
inspiration O
from O
Amos O

et B-DAT
al. O
(2018) O
and O
earlier O

related B-DAT
ideas O
(Krishnan O
et O
al., O
2015 O

; B-DAT
Lamb O
et O
al., O
2016; O

Chiappa B-DAT
et O
al., O
2017), O
and O

train B-DAT
the O
model O
on O
multi- O

step B-DAT
predictions O
of O
all O
distances. O

We B-DAT
develop O
this O
idea O
for O

latent B-DAT
sequence O
models, O
showing O
that O

multi-step B-DAT
predictions O
can O
be O
improved O

by B-DAT
a O
loss O
in O
latent O

space, B-DAT
without O
having O
to O
generate O

additional B-DAT
images. O
Multi-step O
prediction O
We O
start O
by O

generalizing B-DAT
the O
stan- O
dard O
variational O

bound B-DAT
(Equation O
3) O
from O
training O

one-step B-DAT
predictions O
to O
training O
multi-step O

predictions B-DAT
of O
a O
fixed O
dis O

- B-DAT
tance O
d. O
For O
ease O

of B-DAT
notation, O
we O
omit O
actions O

in B-DAT
the O
con- O
ditioning O
set O

here; B-DAT
every O
distribution O
over O
st O

is B-DAT
conditioned O
upon O
a<t. O
We O

first B-DAT
define O
multi-step O
predictions, O
which O

are B-DAT
computed O
by O
repeatedly O
applying O

the B-DAT
transition O
model O
and O
integrating O

out B-DAT
the O
intermediate O
states, O
p(st O
| O
st−d O

) B-DAT
, O
∫ O
t∏ O
τ=t−d+1 O

p(sτ B-DAT
| O
sτ−1) O
dst−d+1:t−1 O
= O
Ep(st−1|st−d)[p(st O
| O
st−1 O

)]. B-DAT
(5 O

) B-DAT
The O
case O
d O
= O
1 O

recovers B-DAT
the O
one-step O
transitions O
used O

in B-DAT
the O
original O
model. O
Given O

this B-DAT
definition O
of O
a O
multi-step O

predic B-DAT

- B-DAT
tion, O
we O
generalize O
Equation O
3 O

to B-DAT
the O
variational O
bound O
on O

the B-DAT
multi-step O
predictive O
distribution O
pd O

, B-DAT
ln O
pd(o1:T O
) O
, O
ln O

∫ B-DAT
T O

∏ B-DAT
t=1 O

p(st B-DAT
| O
st−d)p(ot O
| O
st) O

ds1:T B-DAT
≥ O
T∑ O
t=1 O

Eq(st|o≤t)[ln B-DAT
p(ot O
| O
st)] O
reconstruction O

E B-DAT
p(st−1|st−d)q(st−d|o≤t−d) O
[ O
KL[q(st O
| O
o≤t O

) B-DAT
‖ O
p(st O
| O
st−1)] O
] O
multi-step O
prediction O

6) B-DAT
For O
the O
derivation, O
please O
see O

Equation B-DAT
9 O
in O
the O
appendix O

. B-DAT
Maximizing O
this O
objective O
trains O

the B-DAT
multi-step O
predictive O
distribution. O
This O

reflects B-DAT
the O
fact O
that O
during O

planning, B-DAT
the O
model O
makes O
predictions O

without B-DAT
having O
access O
to O
all O

the B-DAT
preceding O
observations. O
We O
conjecture O
that O
Equation O
6 O

is B-DAT
also O
a O
lower O
bound O

on B-DAT
ln O
p(o1:T O
) O
based O

on B-DAT
the O
data O
processing O
inequality O

. B-DAT
Since O
the O
latent O
state O

sequence B-DAT
is O
Markovian, O
for O

d B-DAT
≥ O
1 O
we O
have O

I(st; B-DAT
st−d) O
≤ O
I(st; O
st−1) O

and B-DAT
thus O
E[ln O

pd(o1:T B-DAT
)] O
≤ O
E[ln O

p(o1:T B-DAT
)]. O
Hence, O
every O
bound O

on B-DAT
the O
multi-step O
predic- O
tive O

distribution B-DAT
is O
also O
a O
bound O

on B-DAT
the O
one-step O
predictive O
distribution O

in B-DAT
expectation O
over O
the O
data O

set. B-DAT
For O
details, O
please O
see O

Equation B-DAT
10 O
in O
the O
appendix. O

In B-DAT
the O
next O
para- O
graph, O

we B-DAT
alleviate O
the O
limitation O
that O

a B-DAT
particular O
pd O
only O
trains O

predictions B-DAT
of O
one O
distance O
and O

arrive B-DAT
at O
our O
final O
objective. O
Latent O
overshooting O
We O
introduced O
a O

bound B-DAT
on O
predic- O
tions O
of O

a B-DAT
given O
distance O
d. O
However O

, B-DAT
for O
planning O
we O
need O

accurate B-DAT
predictions O
not O
just O
for O

a B-DAT
fixed O
distance O
but O
for O

all B-DAT
distances O
up O
to O
the O

planning B-DAT
horizon. O
We O
introduce O
la- O

tent B-DAT
overshooting O
for O
this, O
an O

objective B-DAT
function O
for O
latent O
sequence O

models B-DAT
that O
generalizes O
the O
standard O

variational B-DAT
bound O
(Equation O
3) O
to O

train B-DAT
the O
model O
on O
multi-step O

predic- B-DAT
5 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
tions O
of O
all O
distances O
1 O

≤ B-DAT
d O
≤ O
D O

, B-DAT
1 O

D B-DAT
D∑ O
d=1 O

ln B-DAT
pd(o1:T O
) O
≥ O
T∑ O
t=1 O

Eq(st|o≤t)[ln B-DAT
p(ot O
| O
st)] O
reconstruction O

1 B-DAT
D O
D∑ O
d=1 O

βd B-DAT
E O
p(st−1|st−d)q(st−d|o≤t−d) O
[ O
KL[q(st O
| O
o≤t O

) B-DAT
‖ O
p(st O
| O
st−1)] O
] O
latent O
overshooting O

7) B-DAT
Latent O
overshooting O
can O
be O
interpreted O

as B-DAT
a O
regularizer O
in O
latent O

space B-DAT
that O
encourages O
consistency O
between O

one-step B-DAT
and O
multi-step O
predictions, O
which O

we B-DAT
know O
should O
be O
equiv O

- B-DAT
alent O
in O
expectation O
over O

the B-DAT
data O
set. O
We O
include O

weighting B-DAT
factors O
{βd}Dd=1 O
analogously O
to O

the B-DAT
β-VAE O
(Higgins O
et O
al., O
2016 O

). B-DAT
While O
we O
set O
all O

β>1 B-DAT
to O
the O
same O
value O

for B-DAT
sim- O
plicity, O
they O
could O

be B-DAT
chosen O
to O
let O
the O

model B-DAT
focus O
more O
on O
long-term O

or B-DAT
short-term O
predictions. O
In O
practice, O

we B-DAT
stop O
gradients O
of O
the O

posterior B-DAT
distributions O
for O
overshoot- O
ing O

distances B-DAT
d O
> O
1, O
so O

that B-DAT
the O
multi-step O
predictions O
are O

trained B-DAT
towards O
the O
informed O
posteriors, O

but B-DAT
not O
the O
other O
way O

around. B-DAT
5. O
Experiments O
We O
evaluate O
PlaNet O

on B-DAT
six O
continuous O
control O
tasks O

from B-DAT
pixels. O
We O
explore O
multiple O

design B-DAT
axes O
of O
the O
agent O

: B-DAT
the O
stochastic O
and O
deterministic O

paths B-DAT
in O
the O
dynamics O
model, O

iterative B-DAT
planning, O
and O
online O
experience O

collection. B-DAT
We O
refer O
to O
the O

appendix B-DAT
for O
hyper O
parameters O
(Appendix O

A) B-DAT
and O
additional O
experiments O
(Appendices O

C B-DAT
to O
E). O
Besides O
the O

action B-DAT
repeat, O
we O
use O
the O

same B-DAT
hyper O
parameters O
for O
all O

tasks. B-DAT
Within O
less O
than O
one O

hundredth B-DAT
the O
episodes, O
PlaNet O
outperforms O

A3C B-DAT
(Mnih O
et O
al., O
2016) O

and B-DAT
achieves O
sim- O
ilar O
performance O

to B-DAT
the O
top O
model-free O
algorithm O

D4PG B-DAT
(Barth-Maron O
et O
al., O
2018). O

The B-DAT
training O
time O
of O
10 O

to B-DAT
20 O
hours O
(depending O
on O

the B-DAT
task) O
on O
a O
single O

Nvidia B-DAT
V100 O
GPU O
compares O
favorably O

to B-DAT
that O
of O
A3C O
and O

D4PG. B-DAT
Our O
implementation O
uses O
TensorFlow O

Probability B-DAT
(Dillon O
et O
al., O
2017). O

Please B-DAT
visit O
https://danijar.com/planet O
for O
access O

to B-DAT
the O
code O
and O
videos O

of B-DAT
the O
trained O
agent. O
For O
our O
evaluation, O
we O
consider O

six B-DAT
image-based O
continuous O
control O
tasks O

of B-DAT
the O
DeepMind O
control O
suite O

(Tassa B-DAT
et O
al., O
2018), O
shown O

in B-DAT
Figure O
1. O
These O
environments O

provide B-DAT
qualitatively O
different O
challenges. O
The O

cartpole B-DAT
swingup O
task O
requires O
a O

long B-DAT
planning O
horizon O
and O
to O

memorize B-DAT
the O
cart O
when O
it O

is B-DAT
out O
of O
view, O
reacher O

has B-DAT
a O
sparse O
reward O
given O

when B-DAT
the O
hand O
and O
goal O

area B-DAT
overlap, O
finger O
spinning O
includes O

contact B-DAT
dynamics O
between O
the O
finger O

and B-DAT
the O
object, O
cheetah O
exhibits O

larger B-DAT
state O
and O
action O
spaces O

, B-DAT
the O
cup O
task O
only O

has B-DAT
a O
sparse O
reward O
for O

when B-DAT
the O
ball O
is O
caught, O

and B-DAT
the O
walker O
is O
challenging O

because B-DAT
the O
robot O
first O
has O

to B-DAT
stand O
up O
and O
then O

walk, B-DAT
resulting O
in O
collisions O
with O

the B-DAT
ground O
that O
are O
difficult O

to B-DAT
predict. O
In O
all O
tasks, O

the B-DAT
only O
observations O
are O
third-person O

camera B-DAT
images O
of O
size O
64× O
64 O

× B-DAT
3 O
pixels. O
Comparison O
to O
model-free O
methods O
Figure O

4 B-DAT
compares O
the O
performance O
of O

PlaNet B-DAT
to O
the O
model-free O
algorithms O

re- B-DAT
ported O
by O
Tassa O
et O

al. B-DAT
(2018). O
Within O
100 O
episodes O

, B-DAT
PlaNet O
outperforms O
the O
policy-gradient O

method B-DAT
A3C O
trained O
from O
proprioceptive O

states B-DAT
for O
100,000 O
episodes, O
on O

all B-DAT
tasks. O
Af- O
ter O
500 O

episodes, B-DAT
it O
achieves O
performance O
similar O

to B-DAT
D4PG, O
trained O
from O
images O

for B-DAT
100,000 O
episodes, O
except O
for O

the B-DAT
finger O
task. O
PlaNet O
surpasses O

the B-DAT
final O
performance O
of O
D4PG O

with B-DAT
a O
relative O
improvement O
of O
26 O

% B-DAT
on O
the O
cheetah O
running O

task. B-DAT
We O
refer O
to O
Table O
1 O
for O
numerical O
results, O
which O
also O

includes B-DAT
the O
performance O
of O
CEM O

planning B-DAT
with O
the O
true O
dynamics O

of B-DAT
the O
simulator O

. B-DAT
Model O
designs O
Figure O
4 O
additionally O

compares B-DAT
design O
choices O
of O
the O

dynamics B-DAT
model. O
We O
train O
PlaNet O

using B-DAT
our O
recurrent O
state-space O
model O

(RSSM), B-DAT
as O
well O
as O
ver O

- B-DAT
sions O
with O
purely O
deterministic O

GRU B-DAT
(Cho O
et O
al., O
2014), O

and B-DAT
purely O
stochastic O
state-space O
model O
( O

SSM). B-DAT
We O
observe O
the O
importance O

of B-DAT
both O
stochastic O
and O
deterministic O

elements B-DAT
in O
the O
transition O
function O

on B-DAT
all O
tasks. O
The O
deterministic O

part B-DAT
allows O
the O
model O
to O

remember B-DAT
information O
over O
many O
time O

steps. B-DAT
The O
stochastic O
component O
is O

even B-DAT
more O
impor- O
tant O
– O

the B-DAT
agent O
does O
not O
learn O

without B-DAT
it. O
This O
could O
be O

because B-DAT
the O
tasks O
are O
stochastic O

from B-DAT
the O
agent’s O
perspective O
due O

to B-DAT
partial O
observability O
of O
the O

initial B-DAT
states. O
The O
noise O
might O

also B-DAT
add O
a O
safety O
margin O

to B-DAT
the O
planning O
objective O
that O

results B-DAT
in O
more O
robust O
action O

sequences. B-DAT
Agent O
designs O
Figure O
5 O
compares O

PlaNet, B-DAT
a O
version O
col- O
lecting O

episodes B-DAT
under O
random O
actions O
rather O

than B-DAT
by O
plan- O
ning, O
and O

a B-DAT
version O
that O
at O
each O

environment B-DAT
step O
selects O
the O
best O

action B-DAT
out O
of O
1000 O
sequences O

rather B-DAT
than O
iteratively O
re- O
fining O

plans B-DAT
via O
CEM. O
We O
observe O

that B-DAT
online O
data O
collection O
helps O

for B-DAT
all O
tasks O
and O
is O

necessary B-DAT
for O
the O
cartpole, O
finger O

, B-DAT
and O
walker O
tasks. O
Iterative O

search B-DAT
for O
action O
sequences O
using O

CEM B-DAT
improves O
performance O
on O
all O

tasks. B-DAT
One O
agent O
all O
tasks O
Figure O

7 B-DAT
in O
the O
appendix O
shows O

the B-DAT
performance O
of O
a O
single O

agent B-DAT
trained O
on O
all O
six O

tasks. B-DAT
The O
agent O
is O
not O

told B-DAT
which O
task O
it O
is O

facing; B-DAT
it O
needs O
to O
infer O

this B-DAT
from O
the O
image O
observations O

. B-DAT
We O
pad O
the O
action O

spaces B-DAT
with O
unused O
elements O
to O

make B-DAT
them O
compatible O
and O
adapt O

Algorithm B-DAT
1 O
to O
collect O
one O

episode B-DAT
of O
each O
task O
every O

C B-DAT
update O
steps. O
We O
use O

the B-DAT
same O
hyper O
parameters O
as O

for B-DAT
the O
main O
experiments O
above. O

The B-DAT
agent O
solves O
all O
tasks O

while B-DAT
learning O
slower O
compared O
to O

individually B-DAT
trained O
agents. O
This O
indicates O

that B-DAT
the O
model O
can O
learn O

to B-DAT
predict O
multiple O
domains, O
regardless O

of B-DAT
the O
conceptually O
different O
visuals. O
6. O
Related O
Work O
Previous O
work O

in B-DAT
model-based O
reinforcement O
learning O
has O

focused B-DAT
on O
planning O
in O
low-dimensional O

state B-DAT
spaces O
(Gal O
et O
al O

., B-DAT
2016; O
Higuera O
et O
al., O
2018 O

; B-DAT
Henaff O
et O
al., O
2018; O
6 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
Cartpole O
Swing O
Up O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Reacher O
Easy O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cheetah O
Run O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Finger O
Spin O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cup O
Catch O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Walker O
Walk O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
PlaNet O
(RSSM) O
Stochastic O
(SSM) O
Deterministic O

(GRU B-DAT

) B-DAT
D4PG O
(100k O
episodes) O
A3C O
(100k O

episodes, B-DAT
proprio O

) B-DAT
Figure O
4: O
Comparison O
of O
PlaNet O

to B-DAT
model-free O
algorithms O
and O
other O

model B-DAT
designs. O
Plots O
show O
test O

performance B-DAT
over O
the O
number O
of O

collected B-DAT
episodes. O
We O
compare O
PlaNet O

using B-DAT
our O
RSSM O
(Section O
3 O

) B-DAT
to O
purely O
deterministic O
(GRU) O

and B-DAT
purely O
stochastic O
models O
(SSM). O

The B-DAT
RNN O
does O
not O
use O

latent B-DAT
overshooting, O
as O
it O
does O

not B-DAT
have O
stochastic O
latents. O
The O

lines B-DAT
show O
medians O
and O
the O

areas B-DAT
show O
percentiles O
5 O
to O
95 O
over O
5 O
seeds O
and O
10 O

trajectories. B-DAT
The O
shaded O
areas O
are O

large B-DAT
on O
two O
of O
the O

tasks B-DAT
due O
to O
the O
sparse O

rewards B-DAT

. B-DAT
Table O
1: O
Comparison O
of O
PlaNet O

to B-DAT
the O
model-free O
algorithms O
A3C O

and B-DAT
D4PG O
reported O
by O
Tassa O

et B-DAT
al. O
(2018). O
The O
training O

curves B-DAT
for O
these O
are O
shown O

as B-DAT
orange O
lines O
in O
Figure O

4 B-DAT
and O
as O
solid O
green O

lines B-DAT
in O
Figure O
6 O
in O

their B-DAT
paper. O
From O
these, O
we O

estimate B-DAT
the O
number O
of O
episodes O

that B-DAT
D4PG O
takes O
to O
achieve O

the B-DAT
final O
performance O
of O
PlaNet O

to B-DAT
estimate O
the O
data O
efficiency O

gain. B-DAT
We O
further O
include O
CEM O

planning B-DAT
(H O
= O
12, O
I O

= B-DAT
10, O
J O
= O
1000,K O

= B-DAT
100) O
with O
the O
true O

simulator B-DAT
instead O
of O
learned O
dynamics O

as B-DAT
an O
estimated O
upper O
bound O

on B-DAT
performance. O
Numbers O
indicate O
mean O

final B-DAT
performance O
over O
5 O
seeds O

and B-DAT
10 O
trajectories O

. B-DAT
Method O
Modality O
Episodes O
C O
ar O

tp B-DAT
ol O
e O
Sw O

in B-DAT
g O
U O
p O

R B-DAT
ea O
ch O
er O

E B-DAT
as O
y O

C B-DAT
he O
et O
ah O

R B-DAT
un O
Fi O
ng O

er B-DAT
Sp O
in O

C B-DAT
up O
C O
at O

ch B-DAT
W O
al O

ke B-DAT
r O
W O
al O

k B-DAT
A3C O
proprioceptive O
100,000 O
558 O
285 O

214 B-DAT
129 O
105 O
311 O
D4PG O

pixels B-DAT
100,000 O
862 O
967 O
524 O

985 B-DAT
980 O
968 O
PlaNet O
(ours O

) B-DAT
pixels O
1,000 O
821 O
832 O
662 O
700 O
930 O
951 O
CEM O

+ B-DAT
true O
simulator O
simulator O
state O

0 B-DAT
850 O
964 O
656 O
825 O

993 B-DAT
994 O

Data B-DAT
efficiency O
gain O
PlaNet O
over O

D4PG B-DAT
(factor) O
250 O
40 O
500+ O
300 O
100 O
90 O

Chua B-DAT
et O
al., O
2018), O
combining O

the B-DAT
benefits O
of O
model-based O
and O

model-free B-DAT
approaches O
(Kalweit O
& O
Boedecker, O
2017 O

; B-DAT
Nagabandi O
et O
al., O
2017; O

Weber B-DAT
et O
al., O
2017; O
Kurutach O

et B-DAT
al., O
2018; O
Buckman O
et O

al., B-DAT
2018; O
Ha O
& O
Schmidhuber, O
2018 O

; B-DAT
Wayne O
et O
al., O
2018; O

Igl B-DAT
et O
al., O
2018; O
Srinivas O

et B-DAT
al., O
2018), O
and O
pure O

video B-DAT
prediction O
without O
planning O
(Oh O

et B-DAT
al., O
2015; O
Krishnan O
et O

al., B-DAT
2015; O
Karl O
et O
al., O
2016 O

; B-DAT
Chiappa O
et O
al., O
2017; O

Babaeizadeh B-DAT
et O
al., O
2017; O
Gemici O

et B-DAT
al., O
2017; O
Denton O
& O

Fergus, B-DAT
2018; O
Buesing O
et O
al., O
2018 O

; B-DAT
Doerr O
et O
al., O
2018; O

Gre- B-DAT
gor O
& O
Besse, O
2018). O
Appendix O

G B-DAT
reviews O
these O
orthogonal O
research O

directions B-DAT
in O
more O
detail O

. B-DAT
Relatively O
few O
works O
have O
demonstrated O

successful B-DAT
plan- O
ning O
from O
pixels O

using B-DAT
learned O
dynamics O
models. O
The O

robotics B-DAT
community O
focuses O
on O
video O

prediction B-DAT
models O
for O
planning O
(Agrawal O

et B-DAT
al., O
2016; O
Finn O

& B-DAT
Levine, O
2017; O
Ebert O
et O

al., B-DAT
2018; O
Zhang O
et O
al O

., B-DAT
2018) O
that O
deal O
with O

the B-DAT
visual O
complexity O
of O
the O

real B-DAT
world O
and O
solve O
tasks O

with B-DAT
7 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
Cartpole O
Swing O
Up O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Reacher O
Easy O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cheetah O
Run O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Finger O
Spin O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cup O
Catch O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Walker O
Walk O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
PlaNet O
Random O
collection O
Random O
shooting O

D4PG B-DAT
(100k O
episodes) O
A3C O
(100k O

episodes, B-DAT
proprio) O
Figure O
5: O
Comparison O
of O
agent O

designs. B-DAT
Plots O
show O
test O
performance O

over B-DAT
the O
number O
of O
collected O

episodes. B-DAT
We O
compare O
PlaNet, O
a O

version B-DAT
that O
collects O
data O
under O

random B-DAT
actions O
(random O
collection), O
and O

a B-DAT
version O
that O
chooses O
the O

best B-DAT
action O
out O
of O
1000 O

sequences B-DAT
at O
each O
environment O
step O

(random B-DAT
shooting) O
without O
iteratively O
refining O

plans B-DAT
via O
CEM. O
The O
lines O

show B-DAT
medians O
and O
the O
areas O

show B-DAT
percentiles O
5 O
to O
95 O

over B-DAT
5 O
seeds O
and O
10 O

trajectories B-DAT

. B-DAT
a O
simple O
gripper, O
such O
as O

grasping B-DAT
or O
pushing O
objects. O
In O

comparison, B-DAT
we O
focus O
on O
simulated O

environments, B-DAT
where O
we O
leverage O
latent O

planning B-DAT
to O
scale O
to O
larger O

state B-DAT
and O
ac- O
tion O
spaces O

, B-DAT
longer O
planning O
horizons, O
as O

well B-DAT
as O
sparse O
reward O
tasks. O

E2C B-DAT
(Watter O
et O
al., O
2015) O

and B-DAT
RCE O
(Banija- O
mali O
et O

al., B-DAT
2017) O
embed O
images O
into O

a B-DAT
latent O
space, O
where O
they O

learn B-DAT
local-linear O
latent O
transitions O
and O

plan B-DAT
for O
actions O
using O
LQR. O

These B-DAT
methods O
balance O
simulated O
cartpoles O

and B-DAT
control O
2-link O
arms O
from O

images, B-DAT
but O
have O
been O
difficult O

to B-DAT
scale O
up. O
We O
lift O

the B-DAT
Markov O
assumption O
of O
these O

models, B-DAT
making O
our O
method O
applicable O

under B-DAT
partial O
observability, O
and O
present O

results B-DAT
on O
more O
challenging O
environments O

that B-DAT
include O
longer O
planning O
horizons, O

contact B-DAT
dynamics, O
and O
sparse O
rewards. O
7. O
Discussion O
We O
present O
PlaNet O

, B-DAT
a O
model-based O
agent O
that O

learns B-DAT
a O
la- O
tent O
dynamics O

model B-DAT
from O
image O
observations O
and O

chooses B-DAT
actions O
by O
fast O
planning O

in B-DAT
latent O
space. O
To O
enable O

accu- B-DAT
rate O
long-term O
predictions, O
we O

design B-DAT
a O
model O
with O
both O

stochastic B-DAT
and O
deterministic O
paths. O
We O

show B-DAT
that O
our O
agent O
succeeds O

at B-DAT
several O
continuous O
control O
tasks O

from B-DAT
image O
observations, O
reaching O
performance O

that B-DAT
is O
comparable O
to O
the O

best B-DAT
model-free O
algorithms O
while O
using O
200 O

× B-DAT
fewer O
episodes O
and O
similar O

or B-DAT
less O
computation O
time. O
The O

results B-DAT
show O
that O
learning O
latent O
dynamics O

models B-DAT
for O
planning O
in O
image O

domains B-DAT
is O
a O
promising O
approach O

. B-DAT
Directions O
for O
future O
work O
include O

learning B-DAT
temporal O
ab- O
straction O
instead O

of B-DAT
using O
a O
fixed O
action O

repeat, B-DAT
possibly O
through O
hierarchical O
models O

. B-DAT
To O
further O
improve O
final O

per- B-DAT
formance, O
one O
could O
learn O

a B-DAT
value O
function O
to O
approximate O

the B-DAT
sum O
of O
rewards O
beyond O

the B-DAT
planning O
horizon. O
Moreover, O
gradient-based O

planning B-DAT
could O
increase O
the O
computational O

efficiency B-DAT
of O
the O
agent O
and O

learning B-DAT
representations O
without O
reconstruction O
could O

help B-DAT
to O
solve O
tasks O
with O

higher B-DAT
visual O
diversity. O
Our O
work O

provides B-DAT
a O
starting O
point O
for O

multi-task B-DAT
control O
by O
sharing O
the O

dynamics B-DAT
model. O
Acknowledgements O
We O
thank O
Jacob O
Buckman O

, B-DAT
Nicolas O
Heess, O
John O
Schulman, O

Rishabh B-DAT
Agarwal, O
Silviu O
Pitis, O
Mohammad O

Norouzi, B-DAT
George O
Tucker, O
David O
Duvenaud, O

Shane B-DAT
Gu, O
Chelsea O
Finn, O
Steven O

Bohez, B-DAT
Jimmy O
Ba, O
Stephanie O
Chan, O

and B-DAT
Jenny O
Liu O
for O
helpful O

discussions. B-DAT
8 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
References O

Agrawal, B-DAT
P., O
Nair, O
A. O
V., O

Abbeel, B-DAT
P., O
Malik, O
J., O
and O

Levine, B-DAT
S. O
Learning O
to O
poke O

by B-DAT
poking: O
Experiential O
learning O
of O

intuitive B-DAT
physics. O
In O
Advances O
in O

Neural B-DAT
Information O
Processing O
Systems, O
pp. O
5074 O

–5082, B-DAT
2016. O
Amos, O
B., O
Dinh, O
L., O
Cabi O

, B-DAT
S., O
Rothörl, O
T., O
Muldal, O

A., B-DAT
Erez, O
T., O
Tassa, O
Y., O

de B-DAT
Freitas, O
N., O
and O
Denil, O

M. B-DAT
Learning O
awareness O
models. O
In O

International B-DAT
Conference O
on O
Learn- O
ing O

Representations, B-DAT
2018. O
Babaeizadeh, O
M., O
Finn, O
C., O
Erhan O

, B-DAT
D., O
Campbell, O
R. O
H., O

and B-DAT
Levine, O
S. O
Stochastic O
variational O

video B-DAT
prediction. O
arXiv O
preprint O
arXiv:1710.11252, O
2017 O

. B-DAT
Banijamali, O
E., O
Shu, O
R., O
Ghavamzadeh O

, B-DAT
M., O
Bui, O
H., O
and O

Ghodsi, B-DAT
A. O
Robust O
locally-linear O
controllable O

embedding. B-DAT
arXiv O
preprint O
arXiv:1710.05373, O
2017. O
Barth-Maron, O
G., O
Hoffman, O
M. O
W O

., B-DAT
Budden, O
D., O
Dabney, O
W., O

Horgan, B-DAT
D., O
Muldal, O
A., O
Heess, O

N., B-DAT
and O
Lillicrap, O
T. O
Distributed O

distributional B-DAT
deterministic O
policy O
gradients. O
arXiv O

preprint B-DAT
arXiv:1804.08617, O
2018. O
Bengio, O
S., O
Vinyals, O
O., O
Jaitly O

, B-DAT
N., O
and O
Shazeer, O
N. O

Sched- B-DAT
uled O
sampling O
for O
sequence O

prediction B-DAT
with O
recurrent O
neu- O
ral O

networks. B-DAT
In O
Advances O
in O
Neural O

Information B-DAT
Process- O
ing O
Systems, O
pp. O
1171 O

–1179, B-DAT
2015. O
Buckman, O
J., O
Hafner, O
D., O
Tucker O

, B-DAT
G., O
Brevdo, O
E., O
and O

Lee, B-DAT
H. O
Sample-efficient O
reinforcement O
learning O

with B-DAT
stochastic O
ensemble O
value O
expansion. O

arXiv B-DAT
preprint O
arXiv:1807.01675, O
2018. O
Buesing, O
L., O
Weber, O
T., O
Racaniere O

, B-DAT
S., O
Eslami, O
S., O
Rezende, O

D., B-DAT
Reichert, O
D. O
P., O
Viola, O

F., B-DAT
Besse, O
F., O
Gregor, O
K., O

Hassabis, B-DAT
D., O
et O
al. O
Learning O

and B-DAT
querying O
fast O
gener- O
ative O

models B-DAT
for O
reinforcement O
learning. O
arXiv O

preprint B-DAT
arXiv:1802.03006, O
2018. O
Chiappa, O
S., O
Racaniere, O
S., O
Wierstra O

, B-DAT
D., O
and O
Mohamed, O
S. O

Recurrent B-DAT
environment O
simulators. O
arXiv O
preprint O

arXiv:1704.02254, B-DAT
2017. O
Cho, O
K., O
Van O
Merriënboer, O
B O

., B-DAT
Gulcehre, O
C., O
Bahdanau, O
D., O

Bougares, B-DAT
F., O
Schwenk, O
H., O
and O

Bengio, B-DAT
Y. O
Learn- O
ing O
phrase O

representations B-DAT
using O
rnn O
encoder-decoder O
for O

statistical B-DAT
machine O
translation. O
arXiv O
preprint O

arXiv:1406.1078, B-DAT
2014. O
Chua, O
K., O
Calandra, O
R., O
McAllister O

, B-DAT
R., O
and O
Levine, O
S. O

Deep B-DAT
reinforcement O
learning O
in O
a O

handful B-DAT
of O
trials O
us- O
ing O

probabilistic B-DAT
dynamics O
models. O
arXiv O
preprint O

arXiv:1805.12114, B-DAT
2018. O
Chung, O
J., O
Kastner, O
K., O
Dinh O

, B-DAT
L., O
Goel, O
K., O
Courville, O

A. B-DAT
C., O
and O
Bengio, O
Y. O

A B-DAT
recurrent O
latent O
variable O
model O

for B-DAT
sequential O
data. O
In O
Advances O

in B-DAT
neural O
information O
pro- O
cessing O

systems, B-DAT
pp. O
2980–2988, O
2015. O
Clevert, O
D.-A., O
Unterthiner, O
T., O
and O

Hochreiter, B-DAT
S. O
Fast O
and O
accurate O

deep B-DAT
network O
learning O
by O
exponential O

linear B-DAT
units O
(elus). O
arXiv O
preprint O

arXiv:1511.07289, B-DAT
2015 O

. B-DAT
Deisenroth, O
M. O
and O
Rasmussen, O
C O

. B-DAT
E. O
Pilco: O
A O
model-based O

and B-DAT
data-efficient O
approach O
to O
policy O

search. B-DAT
In O
Proceed- O
ings O
of O

the B-DAT
28th O
International O
Conference O
on O

machine B-DAT
learning O
(ICML-11), O
pp. O
465–472, O
2011 O

. B-DAT
Denton, O
E. O
and O
Fergus, O
R O

. B-DAT
Stochastic O
video O
generation O
with O

a B-DAT
learned O
prior. O
arXiv O
preprint O

arXiv:1802.07687, B-DAT
2018. O
Dillon, O
J. O
V., O
Langmore, O
I O

., B-DAT
Tran, O
D., O
Brevdo, O
E., O

Vasudevan, B-DAT
S., O
Moore, O
D., O
Patton, O

B., B-DAT
Alemi, O
A., O
Hoffman, O
M., O

and B-DAT
Saurous, O
R. O
A. O
Tensorflow O

distributions. B-DAT
arXiv O
preprint O
arXiv:1711.10604, O
2017. O
Doerr, O
A., O
Daniel, O
C., O
Schiegg O

, B-DAT
M., O
Nguyen-Tuong, O
D., O
Schaal, O

S., B-DAT
Toussaint, O
M., O
and O
Trimpe, O

S. B-DAT
Proba- O
bilistic O
recurrent O
state-space O

models. B-DAT
arXiv O
preprint O
arXiv:1801.10395, O
2018. O
Ebert, O
F., O
Finn, O
C., O
Dasari O

, B-DAT
S., O
Xie, O
A., O
Lee, O

A., B-DAT
and O
Levine, O
S. O
Visual O

foresight: B-DAT
Model-based O
deep O
reinforcement O
learning O

for B-DAT
vision-based O
robotic O
control. O
arXiv O

preprint B-DAT
arXiv:1812.00568, O
2018. O
Finn, O
C. O
and O
Levine, O
S O

. B-DAT
Deep O
visual O
foresight O
for O

planning B-DAT
robot O
motion. O
In O
Robotics O

and B-DAT
Automation O
(ICRA), O
2017 O
IEEE O

International B-DAT
Conference O
on, O
pp. O
2786–2793. O

IEEE, B-DAT
2017. O
Gal, O
Y., O
McAllister, O
R., O
and O

Rasmussen, B-DAT
C. O
E. O
Improving O
pilco O

with B-DAT
bayesian O
neural O
network O
dynamics O

models. B-DAT
In O
Data-Efficient O
Machine O
Learning O

workshop, B-DAT
ICML, O
2016 O

. B-DAT
Gemici, O
M., O
Hung, O
C.-C., O
Santoro O

, B-DAT
A., O
Wayne, O
G., O
Mo- O

hamed, B-DAT
S., O
Rezende, O
D. O
J., O

Amos, B-DAT
D., O
and O
Lillicrap, O
T. O

Generative B-DAT
temporal O
models O
with O
memory. O

arXiv B-DAT
preprint O
arXiv:1702.04649, O
2017. O
Gregor, O
K. O
and O
Besse, O
F O

. B-DAT
Temporal O
difference O
variational O
auto-encoder. O

arXiv B-DAT
preprint O
arXiv:1806.03107, O
2018. O
Ha, O
D. O
and O
Schmidhuber, O
J O

. B-DAT
World O
models. O
arXiv O
preprint O

arXiv:1803.10122, B-DAT
2018. O
Henaff, O
M., O
Whitney, O
W. O
F O

., B-DAT
and O
LeCun, O
Y. O
Model-based O

planning B-DAT
with O
discrete O
and O
continuous O

actions. B-DAT
arXiv O
preprint O
arXiv:1705.07177, O
2018. O
9 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
Higgins, O
I., O
Matthey, O
L., O
Pal O

, B-DAT
A., O
Burgess, O
C., O
Glorot, O

X., B-DAT
Botvinick, O
M., O
Mohamed, O
S., O

and B-DAT
Lerchner, O
A. O
beta- O
vae: O

Learning B-DAT
basic O
visual O
concepts O
with O

a B-DAT
constrained O
variational O
framework. O
In O

International B-DAT
Conference O
on O
Learning O
Representations, O
2016 O

. B-DAT
Higuera, O
J. O
C. O
G., O
Meger O

, B-DAT
D., O
and O
Dudek, O
G. O

Synthesizing B-DAT
neural O
network O
controllers O
with O

probabilistic B-DAT
model O
based O
reinforcement O
learning. O

arXiv B-DAT
preprint O
arXiv:1803.02291, O
2018. O
Igl, O
M., O
Zintgraf, O
L., O
Le O

, B-DAT
T. O
A., O
Wood, O
F., O

and B-DAT
Whiteson, O
S. O
Deep O
variational O

reinforcement B-DAT
learning O
for O
pomdps. O
arXiv O

preprint B-DAT
arXiv:1806.02426, O
2018. O
Kalchbrenner, O
N., O
Oord, O
A. O
v O

. B-DAT
d., O
Simonyan, O
K., O
Danihelka, O

I., B-DAT
Vinyals, O
O., O
Graves, O
A., O

and B-DAT
Kavukcuoglu, O
K. O
Video O
pixel O

networks. B-DAT
arXiv O
preprint O
arXiv:1610.00527, O
2016. O
Kalweit, O
G. O
and O
Boedecker, O
J O

. B-DAT
Uncertainty-driven O
imagi- O
nation O
for O

continuous B-DAT
deep O
reinforcement O
learning. O
In O

Conference B-DAT
on O
Robot O
Learning, O
pp. O
195 O

–206, B-DAT
2017. O
Karl, O
M., O
Soelch, O
M., O
Bayer O

, B-DAT
J., O
and O
van O
der O

Smagt, B-DAT
P. O
Deep O
variational O
bayes O

filters: B-DAT
Unsupervised O
learning O
of O
state O

space B-DAT
models O
from O
raw O
data. O

arXiv B-DAT
preprint O
arXiv:1605.06432, O
2016. O
Kingma, O
D. O
P. O
and O
Ba O

, B-DAT
J. O
Adam: O
A O
method O

for B-DAT
stochastic O
optimization. O
arXiv O
preprint O

arXiv:1412.6980, B-DAT
2014. O
Kingma, O
D. O
P. O
and O
Dhariwal O

, B-DAT
P. O
Glow: O
Generative O
flow O

with B-DAT
invertible O
1x1 O
convolutions. O
arXiv O

preprint B-DAT
arXiv:1807.03039, O
2018. O
Kingma, O
D. O
P. O
and O
Welling O

, B-DAT
M. O
Auto-encoding O
variational O
bayes. O

arXiv B-DAT
preprint O
arXiv:1312.6114, O
2013. O
Krishnan, O
R. O
G., O
Shalit, O
U O

., B-DAT
and O
Sontag, O
D. O
Deep O

kalman B-DAT
filters. O
arXiv O
preprint O
arXiv:1511.05121, O
2015 O

. B-DAT
Krishnan, O
R. O
G., O
Shalit, O
U O

., B-DAT
and O
Sontag, O
D. O
Structured O

inference B-DAT
networks O
for O
nonlinear O
state O

space B-DAT
models. O
In O
AAAI, O
pp. O
2101 O

–2109, B-DAT
2017. O
Kurutach, O
T., O
Clavera, O
I., O
Duan O

, B-DAT
Y., O
Tamar, O
A., O
and O

Abbeel, B-DAT
P. O
Model-ensemble O
trust-region O
policy O

optimization. B-DAT
arXiv O
preprint O
arXiv:1802.10592, O
2018. O
Lamb, O
A. O
M., O
GOYAL, O
A O

. B-DAT
G. O
A. O
P., O
Zhang, O

Y., B-DAT
Zhang, O
S., O
Courville, O
A. O

C., B-DAT
and O
Bengio, O
Y. O
Professor O

forcing: B-DAT
A O
new O
algorithm O
for O

training B-DAT
recurrent O
networks. O
In O
Advances O

In B-DAT
Neural O
Information O
Processing O
Systems, O

pp. B-DAT
4601–4609, O
2016. O
Mathieu, O
M., O
Couprie, O
C., O
and O

LeCun, B-DAT
Y. O
Deep O
multi- O
scale O

video B-DAT
prediction O
beyond O
mean O
square O

error. B-DAT
arXiv O
preprint O
arXiv:1511.05440, O
2015 O

. B-DAT
Mnih, O
V., O
Kavukcuoglu, O
K., O
Silver O

, B-DAT
D., O
Rusu, O
A. O
A., O

Veness, B-DAT
J., O
Bellemare, O
M. O
G., O

Graves, B-DAT
A., O
Riedmiller, O
M., O
Fidje- O

land, B-DAT
A. O
K., O
Ostrovski, O
G., O

et B-DAT
al. O
Human-level O
control O
through O

deep B-DAT
reinforcement O
learning. O
Nature, O
518(7540): O
529, O
2015 O

. B-DAT
Mnih, O
V., O
Badia, O
A. O
P O

., B-DAT
Mirza, O
M., O
Graves, O
A., O

Lillicrap, B-DAT
T., O
Harley, O
T., O
Silver, O

D., B-DAT
and O
Kavukcuoglu, O
K. O
Asyn- O

chronous B-DAT
methods O
for O
deep O
reinforcement O

learning. B-DAT
In O
International O
Conference O
on O

Machine B-DAT
Learning, O
pp. O
1928– O
1937, O
2016 O

. B-DAT
Moerland, O
T. O
M., O
Broekens, O
J O

., B-DAT
and O
Jonker, O
C. O
M. O

Learning B-DAT
multimodal O
transition O
dynamics O
for O

model-based B-DAT
rein- O
forcement O
learning. O
arXiv O

preprint B-DAT
arXiv:1705.00470, O
2017. O
Moravčík, O
M., O
Schmid, O
M., O
Burch O

, B-DAT
N., O
Lisỳ, O
V., O
Morrill, O

D., B-DAT
Bard, O
N., O
Davis, O
T., O

Waugh, B-DAT
K., O
Johanson, O
M., O
and O

Bowl- B-DAT
ing, O
M. O
Deepstack: O
Expert-level O

artificial B-DAT
intelligence O
in O
heads-up O
no-limit O

poker. B-DAT
Science, O
356(6337):508–513, O
2017. O
Nagabandi, O
A., O
Kahn, O
G., O
Fearing O

, B-DAT
R. O
S., O
and O
Levine, O

S. B-DAT
Neural O
network O
dynamics O
for O

model-based B-DAT
deep O
rein- O
forcement O
learning O

with B-DAT
model-free O
fine-tuning. O
arXiv O
preprint O

arXiv:1708.02596, B-DAT
2017. O
Nair, O
V. O
and O
Hinton, O
G O

. B-DAT
E. O
Rectified O
linear O
units O

improve B-DAT
restricted O
boltzmann O
machines. O
In O

Proceedings B-DAT
of O
the O
27th O
international O

conference B-DAT
on O
machine O
learning O
(ICML-10), O

pp. B-DAT
807–814, O
2010. O
Oh, O
J., O
Guo, O
X., O
Lee O

, B-DAT
H., O
Lewis, O
R. O
L., O

and B-DAT
Singh, O
S. O
Action- O
conditional O

video B-DAT
prediction O
using O
deep O
networks O

in B-DAT
atari O
games. O
In O
Advances O

in B-DAT
Neural O
Information O
Processing O
Systems, O

pp. B-DAT
2863–2871, O
2015. O
Rezende, O
D. O
J., O
Mohamed, O
S O

., B-DAT
and O
Wierstra, O
D. O
Stochastic O

backpropagation B-DAT
and O
approximate O
inference O
in O

deep B-DAT
gen- O
erative O
models. O
arXiv O

preprint B-DAT
arXiv:1401.4082, O
2014. O
Richards, O
A. O
G. O
Robust O
constrained O

model B-DAT
predictive O
control. O
PhD O
thesis O

, B-DAT
Massachusetts O
Institute O
of O
Technology, O
2005 O

. B-DAT
Rubinstein, O
R. O
Y. O
Optimization O
of O

computer B-DAT
simulation O
mod- O
els O
with O

rare B-DAT
events. O
European O
Journal O
of O

Operational B-DAT
Research, O
99(1):89–112, O
1997 O

. B-DAT
Silver, O
D., O
Schrittwieser, O
J., O
Simonyan O

, B-DAT
K., O
Antonoglou, O
I., O
Huang, O

A., B-DAT
Guez, O
A., O
Hubert, O
T., O

Baker, B-DAT
L., O
Lai, O
M., O
Bolton, O

A., B-DAT
et O
al. O
Mastering O
the O

game B-DAT
of O
go O
without O
human O

knowledge. B-DAT
Nature, O
550(7676):354, O
2017. O
Srinivas, O
A., O
Jabri, O
A., O
Abbeel O

, B-DAT
P., O
Levine, O
S., O
and O

Finn, B-DAT
C. O
Universal O
planning O
networks. O

arXiv B-DAT
preprint O
arXiv:1804.00645, O
2018. O
10 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
Talvitie, O
E. O
Model O
regularization O
for O

stable B-DAT
sample O
rollouts. O
In O
UAI O

, B-DAT
pp. O
780–789, O
2014. O
Tassa, O
Y., O
Erez, O
T., O
and O

Todorov, B-DAT
E. O
Synthesis O
and O
stabi O

- B-DAT
lization O
of O
complex O
behaviors O

through B-DAT
online O
trajectory O
optimization. O
In O

Intelligent B-DAT
Robots O
and O
Systems O
(IROS), O
2012 O
IEEE/RSJ O
International O
Conference O
on, O
pp O

. B-DAT
4906– O
4913. O
IEEE, O
2012. O
Tassa, O
Y., O
Doron, O
Y., O
Muldal O

, B-DAT
A., O
Erez, O
T., O
Li, O

Y., B-DAT
Casas, O
D. O
d. O
L., O

Budden, B-DAT
D., O
Abdolmaleki, O
A., O
Merel, O

J., B-DAT
Lefrancq, O
A., O
et O
al. O

Deepmind B-DAT
control O
suite. O
arXiv O
preprint O

arXiv:1801.00690, B-DAT
2018. O
van O
den O
Oord, O
A., O
Vinyals O

, B-DAT
O., O
et O
al. O
Neural O

discrete B-DAT
repre- O
sentation O
learning. O
In O

Advances B-DAT
in O
Neural O
Information O
Processing O

Systems, B-DAT
pp. O
6309–6318, O
2017. O
Venkatraman, O
A., O
Hebert, O
M., O
and O

Bagnell, B-DAT
J. O
A. O
Improving O
multi-step O

prediction B-DAT
of O
learned O
time O
series O

models. B-DAT
In O
AAAI, O
pp. O
3024–3030 O

, B-DAT
2015. O
Vondrick, O
C., O
Pirsiavash, O
H., O
and O

Torralba, B-DAT
A. O
Generating O
videos O
with O

scene B-DAT
dynamics. O
In O
Advances O
In O

Neural B-DAT
Information O
Processing O
Systems, O
2016 O

. B-DAT
Watter, O
M., O
Springenberg, O
J., O
Boedecker O

, B-DAT
J., O
and O
Riedmiller, O
M. O

Embed B-DAT
to O
control: O
A O
locally O

linear B-DAT
latent O
dynamics O
model O
for O

control B-DAT
from O
raw O
images. O
In O

Advances B-DAT
in O
neural O
information O
processing O

systems, B-DAT
pp. O
2746–2754, O
2015. O
Wayne, O
G., O
Hung, O
C.-C., O
Amos O

, B-DAT
D., O
Mirza, O
M., O
Ahuja, O

A., B-DAT
Grabska-Barwinska, O
A., O
Rae, O
J., O

Mirowski, B-DAT
P., O
Leibo, O
J. O
Z., O

Santoro, B-DAT
A., O
et O
al. O
Unsupervised O

predic- B-DAT
tive O
memory O
in O
a O

goal-directed B-DAT
agent. O
arXiv O
preprint O
arXiv:1803.10760, O
2018 O

. B-DAT
Weber, O
T., O
Racanière, O
S., O
Reichert O

, B-DAT
D. O
P., O
Buesing, O
L., O

Guez, B-DAT
A., O
Rezende, O
D. O
J., O

Badia, B-DAT
A. O
P., O
Vinyals, O
O., O

Heess, B-DAT
N., O
Li, O
Y., O
et O

al. B-DAT
Imagination-augmented O
agents O
for O
deep O

reinforcement B-DAT
learning. O
arXiv O
preprint O
arXiv:1707.06203, O
2017 O

. B-DAT
Zhang, O
M., O
Vikram, O
S., O
Smith O

, B-DAT
L., O
Abbeel, O
P., O
Johnson, O

M., B-DAT
and O
Levine, O
S. O
SOLAR: O

deep B-DAT
structured O
representations O
for O
model-based O

reinforcement B-DAT
learning. O
arXiv O
preprint O
arXiv:1808.09105, O
2018 O

. B-DAT
11 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
A. O
Hyper O
Parameters O
We O
use O

the B-DAT
convolutional O
and O
deconvolutional O
networks O

from B-DAT
Ha O
& O
Schmidhuber O
(2018 O

), B-DAT
a O
GRU O
(Cho O
et O

al., B-DAT
2014) O
with O
200 O
units O

as B-DAT
deterministic O
path O
in O
the O

dynamics B-DAT
model, O
and O
implement O
all O

other B-DAT
functions O
as O
two O
fully O

connected B-DAT
layers O
of O
size O
200 O

with B-DAT
ReLU O
activations O
(Nair O
& O

Hinton, B-DAT
2010). O
Distributions O
in O
latent O

space B-DAT
are O
30-dimensional O
diagonal O
Gaussians O

with B-DAT
predicted O
mean O
and O
standard O

deviation. B-DAT
We O
pre-process O
images O
by O
reducing O

the B-DAT
bit O
depth O
to O
5 O

bits B-DAT
as O
in O
Kingma O

& B-DAT
Dhariwal O
(2018). O
The O
model O

is B-DAT
trained O
using O
the O
Adam O

optimizer B-DAT
(Kingma O
& O
Ba, O
2014 O

) B-DAT
with O
a O
learning O
rate O

of B-DAT
10−3, O
� O
= O
10−4, O

and B-DAT
gradient O
clipping O
norm O
of O
1000 O
on O
batches O
of O
B O

= B-DAT
50 O
sequence O
chunks O
of O

length B-DAT
L O
= O
50. O
We O

do B-DAT
not O
scale O
the O
KL O

divergence B-DAT
terms O
relatively O
to O
the O

reconstruction B-DAT
terms O
but O
grant O
the O

model B-DAT
3 O
free O
nats O
by O

clipping B-DAT
the O
divergence O
loss O
below O

this B-DAT
value. O
In O
a O
previous O

version B-DAT
of O
the O
agent, O
we O

used B-DAT
latent O
overshooting O
and O
an O

additional B-DAT
fixed O
global O
prior, O
but O

we B-DAT
found O
this O
to O
not O

be B-DAT
necessary O

. B-DAT
For O
planning, O
we O
use O
CEM O

with B-DAT
horizon O
length O
H O

= B-DAT
12, O
optimization O
iterations O
I O

= B-DAT
10, O
candidate O
samples O
J O

= B-DAT
1000, O
and O
refitting O
to O

the B-DAT
best O
K O
= O
100 O

. B-DAT
We O
start O
from O

S B-DAT
= O
5 O
seed O
episodes O

with B-DAT
random O
actions O
and O
collect O

another B-DAT
episode O
every O
C O
= O
100 O
update O
steps O
under O

� B-DAT
∼ O
Normal(0, O
0.3) O
action O

noise. B-DAT
The O
action O
repeat O
differs O

between B-DAT
domains: O
cartpole O
(R O

= B-DAT
8), O
reacher O
(R O

= B-DAT
4), O
cheetah O
(R O

= B-DAT
4), O
finger O
(R O

= B-DAT
2), O
cup O
(R O

= B-DAT
4), O
walker O
(R O

= B-DAT
2). O
We O
found O
important O

hyper B-DAT
parameters O
to O
be O
the O

action B-DAT
repeat, O
the O
KL-divergence O
scales O

β, B-DAT
and O
the O
learning O
rate O

. B-DAT
B. O
Planning O
Algorithm O

Algorithm B-DAT
2: O
Latent O
planning O
with O

CEM B-DAT
Input O
: O
H O
Planning O
horizon O

distance B-DAT
I O
Optimization O
iterations O
J O

Candidates B-DAT
per O
iteration O
K O
Number O

of B-DAT
top O
candidates O
to O
fit O

q(st B-DAT
| O
o≤t, O
a<t) O
Current O

state B-DAT
belief O
p(st O
| O
st−1, O

at−1) B-DAT
Transition O
model O
p(rt O
| O

st) B-DAT
Reward O
model O
1 O
Initialize O
factorized O
belief O
over O

action B-DAT
sequences O
q(at:t+H)← O
Normal(0, O
I O

). B-DAT
2 O
for O
optimization O
iteration O

i B-DAT
= O
1..I O
do O
// O
Evaluate O
J O
action O
sequences O

from B-DAT
the O
current O
belief. O
3 O

for B-DAT
candidate O
action O
sequence O
j O

= B-DAT
1..J O
do O
4 O
a O

j) B-DAT
t:t+H O
∼ O
q(at:t+H) O
5 O
s O
(j) O
t:t+H+1 O

∼ B-DAT
q(st O
| O
o1:t, O
a1:t−1 O

) B-DAT
∏t+H+1 O
τ=t+1 O
p(sτ O
| O
sτ−1 O

, B-DAT
a O
(j) O
τ−1 O

) B-DAT
6 O
R(j) O
= O
∑t+H+1 O
τ=t+1 O

E[p(rτ B-DAT
| O
s O

j) B-DAT
τ O
)] O
// O
Re-fit O
belief O
to O
the O

K B-DAT
best O
action O
sequences O

. B-DAT
7 O
K O
← O
argsort({R(j)}Jj=1)1:K O
8 O

µt:t+H B-DAT

1 B-DAT
K O
∑ O
k∈K O
a O

k) B-DAT
t:t+H O
, O
σt:t+H O
= O
1 O
K−1 O

k∈K B-DAT
|a O
(k) O
t:t+H O
− O
µt:t+H O

9 B-DAT
q(at:t+H)← O
Normal(µt:t+H O
, O
σ2t:t+H O

I) B-DAT
10 O
return O
first O
action O

mean B-DAT
µt. O
12 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
C. O
Multi-Task O
Learning O
Average O
over O

tasks B-DAT

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Figure O
6: O
We O
compare O
a O

single B-DAT
PlaNet O
agent O
trained O
on O

all B-DAT
tasks O
to O
individual O
PlaNet O

agents. B-DAT
The O
plot O
shows O
test O

performance B-DAT
over O
the O
number O
of O

episodes B-DAT
collected O
for O
each O
task O

. B-DAT
The O
single O
agent O
learns O

to B-DAT
solve O
all O
the O
tasks O

while B-DAT
learning O
more O
slowly O
compared O

to B-DAT
the O
individual O
agents. O
The O

lines B-DAT
show O
mean O
and O
one O

standard B-DAT
deviation O
over O
6 O
tasks, O
5 O
seeds, O
and O
10 O
trajectories O

. B-DAT
Cartpole O
Swing O
Up O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Reacher O
Easy O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cheetah O
Run O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Finger O
Spin O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cup O
Catch O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Walker O
Walk O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Separate O
agents O
Single O
agent O

D4PG B-DAT
(100k O
episodes) O
A3C O
(100k O

episodes, B-DAT
proprio) O
Figure O
7: O
Per-task O
performance O
of O

a B-DAT
single O
PlaNet O
agent O
trained O

on B-DAT
the O
six O
tasks. O
Plots O

show B-DAT
test O
performance O
over O
the O

number B-DAT
of O
episodes O
collected O
per O

task. B-DAT
The O
agent O
is O
not O

told B-DAT
which O
task O
it O
is O

solving B-DAT
and O
it O
needs O
to O

infer B-DAT
this O
from O
the O
image O

observations. B-DAT
The O
agent O
learns O
to O

distinguish B-DAT
the O
tasks O
and O
solve O

them B-DAT
with O
just O
a O
moderate O

slowdown B-DAT
in O
learning. O
The O
lines O

show B-DAT
medians O
and O
the O
areas O

show B-DAT
percentiles O
5 O
to O
95 O

over B-DAT
4 O
seeds O
and O
10 O

trajectories B-DAT

. B-DAT
13 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
D. O
Latent O
Overshooting O
Cartpole O
Swing O

Up B-DAT

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Reacher O
Easy O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cheetah O
Run O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Finger O
Spin O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cup O
Catch O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Walker O
Walk O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
RSSM O
RSSM O
+ O
Overshooting O

DRNN B-DAT
DRNN O
+ O
Overshooting O
D4PG O
(100k O
episodes) O
A3C O
(100k O

episodes, B-DAT
proprio O

) B-DAT
Figure O
8: O
We O
compare O
the O

standard B-DAT
variational O
objective O
with O
latent O

overshooting B-DAT
on O
our O
proposed O
RSSM O

and B-DAT
another O
model O
called O
DRNN O

that B-DAT
uses O
two O
RNNs O
as O

encoder B-DAT
and O
decoder O
with O
a O

stochastic B-DAT
state O
sequence O
in O
between O

. B-DAT
Latent O
overshooting O
can O
substantially O

improve B-DAT
the O
performance O
of O
the O

DRNN B-DAT
and O
other O
models O
we O

have B-DAT
experimented O
with O
(not O
shown), O

but B-DAT
slightly O
reduces O
performance O
of O

our B-DAT
RSSM. O
The O
lines O
show O

medians B-DAT
and O
the O
areas O
show O

percentiles B-DAT
5 O
to O
95 O
over O
5 O
seeds O
and O
10 O
trajectories O

. B-DAT
14 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
E. O
Activation O
Function O
Cartpole O
Swing O

Up B-DAT

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Reacher O
Easy O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cheetah O
Run O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Finger O
Spin O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cup O
Catch O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Walker O
Walk O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
PlaNet O
(ReLU) O
PlaNet O
(ELU O

) B-DAT
SSM O
(ReLU) O
SSM O
(ELU O

) B-DAT
D4PG O
(100k O
episodes) O
A3C O
(100k O

episodes, B-DAT
proprio O

) B-DAT
Figure O
9: O
Comparison O
of O
hard O

ReLU B-DAT
(Nair O
& O
Hinton, O
2010 O

) B-DAT
and O
smooth O
ELU O
(Clevert O

et B-DAT
al., O
2015) O
activation O
functions. O

We B-DAT
find O
that O
smooth O
activations O

help B-DAT
improve O
performance O
of O
the O

purely B-DAT
stochastic O
model O
(and O
the O

purely B-DAT
deterministic O
model; O
not O
shown) O

while B-DAT
our O
proposed O
RSSM O
is O

robust B-DAT
to O
the O
choice O
of O

activation B-DAT
function. O
The O
lines O
show O

medians B-DAT
and O
the O
areas O
show O

percentiles B-DAT
5 O
to O
95 O
over O
5 O
seeds O
and O
10 O
trajectories O

. B-DAT
15 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
F. O
Bound O
Derivations O
One-step O
predictive O

distribution B-DAT
The O
variational O
bound O
for O

latent B-DAT
dynamics O
models O
p(o1:T O

, B-DAT
s1:T O
| O
a1:T O

t B-DAT
p(st O
| O
st−1, O
at−1)p(ot O
| O
st) O
and O

a B-DAT
variational O
posterior O
q(s1:T O

| B-DAT
o1:T O
, O
a1:T O

) B-DAT
= O
∏ O
t O
q(st O

| B-DAT
o≤t, O
a<t) O
follows O
from O

importance B-DAT

weighting B-DAT
and O
Jensen’s O
inequality O
as O

shown, B-DAT
ln O
p(o1:T O
| O
a1:T O

) B-DAT
, O
ln O
Ep(s1:T O
|a1:T O

) B-DAT
[ O
T∏ O
t=1 O

p(ot B-DAT
| O
st) O
] O
= O
lnEq(s1:T O
|o1:T O
,a1:T O

T∏ B-DAT
t=1 O
p(ot O
| O
st)p(st O
| O
st−1 O

, B-DAT
at−1)/q(st O
| O
o≤t, O

a<t) B-DAT
] O
≥ O
Eq(s1:T O
|o1:T O
,a1:T O

) B-DAT
[ O
T∑ O
t=1 O

ln B-DAT
p(ot O
| O
st) O
+ O

ln B-DAT
p(st O
| O
st−1, O
at−1)− O

ln B-DAT
q(st O
| O
o≤t, O

T∑ B-DAT
t=1 O
( O
E O
q(st|o≤t,a<t) O
[ln O
p(ot O

| B-DAT
st O

)] B-DAT
reconstruction O

E B-DAT
q(st−1|o≤t−1,a<a−1 O
) O
[ O
KL[q(st O
| O
o≤t, O
a<t O

) B-DAT
‖ O
p(st O
| O
st−1, O

at−1)] B-DAT
] O
complexity O

8) B-DAT
Multi-step O
predictive O
distribution O
The O
variational O

bound B-DAT
on O
the O
d-step O
predictive O

distribution B-DAT
pd(o1:T O
, O
s1:T O

| B-DAT
a1:T O
) O
=∏ O
t O

p(st B-DAT
| O
st−d, O
at−1)p(ot O

| B-DAT
st) O
and O
a O
variational O

posterior B-DAT
q(s1:T O
| O
o1:T O

, B-DAT
a1:T O

t B-DAT
q(st O
| O
o≤t, O
a<t) O

follows B-DAT
anal- O
ogously. O
The O
second O
bound O
comes O

from B-DAT
moving O
the O
log O
inside O

the B-DAT
multi-step O
priors, O
which O
satisfy O

the B-DAT
recursion O
p(st O
| O
st−d O

, B-DAT
at−d−1:t−1) O
= O
Ep(st−1|st−d,at−d−1:t−2)[p(st O
| O

st−1, B-DAT
at−1)]. O
ln O
pd(o1:T O
| O
a1:T O

) B-DAT
, O
ln O
Epd(s1:T O
|a1:T O

) B-DAT
[ O
T∏ O
t=1 O

p(ot B-DAT
| O
st) O
] O
= O
lnEq(s1:T O
|o1:T O
,a1:T O

T∏ B-DAT
t=1 O
p(ot O
| O
st)p(st O
| O
st−d O

, B-DAT
at−d−1:t−1)/q(st O
| O
o≤t, O

a<t) B-DAT
] O
≥ O
Eq(s1:T O
|o1:T O
,a1:T O

) B-DAT
[ O
T∑ O
t=1 O

ln B-DAT
p(ot O
| O
st) O
+ O

ln B-DAT
p(st O
| O
st−d, O
at−d−1:t−1)− O

ln B-DAT
q(st O
| O
o≤t, O

a<t) B-DAT
] O
≥ O
Eq(s1:T O
|o1:T O
,a1:T O

) B-DAT
[ O
T∑ O
t=1 O

ln B-DAT
p(ot O
| O
st) O
+ O

E B-DAT
p(st−1|st−d,at−d−1:t−2) O
[ln O
p(st O
| O

st−1, B-DAT
at−1)]− O
ln O
q(st O
| O

o≤t, B-DAT

T∑ B-DAT
t=1 O
( O
Eq(st|o≤t,a<t)[ln O
p(ot O
| O
st O

)] B-DAT
reconstruction O

E B-DAT
p(st−1|st−d,at−d−1:t−2)q(st−d|o≤t−d,a<t−d) O
[ O
KL[q(st O
| O
o≤t, O
a<t O

) B-DAT
‖ O
p(st O
| O
st−1, O

at−1)] B-DAT
] O
multi-step O
prediction O

9) B-DAT
Since O
all O
expectations O
are O
on O

the B-DAT
outside O
of O
the O
objective O

, B-DAT
we O
can O
easily O
obtain O

an B-DAT
unbiased O
estimator O
of O
this O

bound B-DAT
by O
changing O
expectations O
to O

sample B-DAT
averages. O
Relation O
between O
one-step O
and O
multi-step O

predictive B-DAT
distributions O
We O
conjecture O
that O

the B-DAT
multi-step O
predictive O
dis- O
tribution O

pd(o1:T B-DAT
) O
lower O
bounds O
the O

one-step B-DAT
predictive O
distribution O
p(o1:T O

) B-DAT
of O
the O
same O
latent O

sequence B-DAT
model O
model O
in O
expectation O

over B-DAT
the O
data O
set. O
Since O

the B-DAT
latent O
state O
sequence O
is O

Markovian, B-DAT
for O
d O
≥ O
1 O

we B-DAT
have O
the O
data O
processing O

inequality B-DAT

I(st; B-DAT
st−d) O
≤ O
I(st; O
st−1) O

H(st)−H(st|st−d) B-DAT
≤ O
H(st)−H(st|st−1) O
E[ln O

p(st B-DAT
| O
st−d)] O
≤ O
E[ln O

p(st B-DAT
| O
st−1)] O
E[ln O
pd(o1:T O
)] O
≤ O
E[ln O

p(o1:T B-DAT

10) B-DAT
Therefore, O
any O
bound O
on O
the O

multi-step B-DAT
predictive O
distribution, O
including O
Equation O

9 B-DAT
and O
Equation O
7, O
is O

also B-DAT
a O
bound O
on O
the O

one-step B-DAT
predictive O
distribution O

. B-DAT
16 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
G. O
Additional O
Related O
Work O
Planning O

in B-DAT
state O
space O
When O
low-dimensional O

states B-DAT
of O
the O
environment O
are O

available B-DAT
to O
the O
agent, O
it O

is B-DAT
possible O
to O
learn O
the O

dynamics B-DAT
directly O
in O
state O
space O

. B-DAT
In O
the O
regime O
of O

control B-DAT
tasks O
with O
only O
a O

few B-DAT
state O
variables, O
such O
as O

the B-DAT
cart O
pole O
and O
mountain O

car B-DAT
tasks, O
PILCO O
(Deisenroth O
& O

Rasmussen, B-DAT
2011) O
achieves O
remarkable O
sample O

efficiency B-DAT
using O
Gaussian O
processes O
to O

model B-DAT
the O
dynamics. O
Similar O
approaches O

using B-DAT
neural O
networks O
dynamics O
models O

can B-DAT
solve O
two-link O
balancing O
problems O
( O

Gal B-DAT
et O
al., O
2016; O
Higuera O

et B-DAT
al., O
2018) O
and O
implement O

planning B-DAT
via O
gradients O
(Henaff O
et O

al., B-DAT
2018). O
Chua O
et O
al. O
(2018 O

) B-DAT
use O
ensembles O
of O
neural O

networks, B-DAT
scaling O
up O
to O
the O

cheetah B-DAT
running O
task. O
The O
limitation O

of B-DAT
these O
methods O
is O
that O

they B-DAT
access O
the O
low-dimensional O
Markovian O

state B-DAT
of O
the O
underlying O
system O

and B-DAT
sometimes O
the O
reward O
function. O

Amos B-DAT
et O
al. O
(2018) O
train O

a B-DAT
deterministic O
model O
using O
overshooting O

in B-DAT
observation O
space O
for O
active O

exploration B-DAT
with O
a O
robotics O
hand. O

We B-DAT
move O
beyond O
low-dimensional O
state O

representations B-DAT
and O
use O
a O
latent O

dynamics B-DAT
model O
to O
solve O
control O

tasks B-DAT
from O
images. O
Hybrid O
agents O
The O
challenges O
of O

model-based B-DAT
RL O
have O
motivated O
the O

research B-DAT
community O
to O
develop O
hybrid O

agents B-DAT
that O
accelerate O
policy O
learning O

by B-DAT
training O
on O
imagined O
experience O

(Kalweit B-DAT
& O
Boedecker, O
2017; O
Nagabandi O

et B-DAT
al., O
2017; O
Kurutach O
et O

al., B-DAT
2018; O
Buckman O
et O
al O

., B-DAT
2018; O
Ha O
& O
Schmidhuber, O
2018 O

), B-DAT
improving O
feature O
representations O
(Wayne O

et B-DAT
al., O
2018; O
Igl O
et O

al., B-DAT
2018), O
or O
leveraging O
the O

information B-DAT
content O
of O
the O
model O

directly B-DAT
(Weber O
et O
al., O
2017). O

Srinivas B-DAT
et O
al. O
(2018) O
learn O

a B-DAT
policy O
network O
with O
integrated O

planning B-DAT
computation O
using O
reinforcement O
learning O

and B-DAT
without O
prediction O
loss, O
yet O

require B-DAT
expert O
demonstrations O
for O
training. O
Multi-step O
predictions O
Training O
sequence O
models O

on B-DAT
multi-step O
predictions O
has O
been O

explored B-DAT
for O
several O
years. O
Scheduled O

sampling B-DAT
(Bengio O
et O
al., O
2015 O

) B-DAT
changes O
the O
rollout O
distance O

of B-DAT
the O
sequence O
model O
over O

the B-DAT
course O
of O
training. O
Hallucinated O

replay B-DAT
(Talvitie, O
2014) O
mixes O
predictions O

into B-DAT
the O
data O
set O
to O

indirectly B-DAT
train O
multi-step O
predictions. O
Venkatraman O

et B-DAT
al. O
(2015) O
take O
an O

imitation B-DAT
learning O
approach. O
Recently, O
Amos O

et B-DAT
al. O
(2018) O
train O
a O

dynamics B-DAT
model O
on O
all O
multi-step O

predictions B-DAT
at O
once. O
We O
generalize O

this B-DAT
idea O
to O
latent O
sequence O

models B-DAT
trained O
via O
variational O
inference. O
Latent O
sequence O
models O
Classic O
work O

has B-DAT
explored O
models O
for O
non-Markovian O

observation B-DAT
sequences, O
including O
recurrent O
neural O

networks B-DAT
(RNNs) O
with O
deterministic O
hidden O

state B-DAT
and O
probabilistic O
state-space O
models O

(SSMs). B-DAT
The O
ideas O
behind O
variational O

autoencoders B-DAT
(Kingma O
& O
Welling, O
2013 O

; B-DAT
Rezende O
et O
al., O
2014) O

have B-DAT
enabled O
non-linear O
SSMs O
that O

are B-DAT
trained O
via O
variational O
inference O
( O

Krishnan B-DAT
et O
al., O
2015). O
The O

VRNN B-DAT
(Chung O
et O
al., O
2015) O

combines B-DAT
RNNs O
and O
SSMs O
and O

is B-DAT
trained O
via O
variational O
inference. O

In B-DAT
contrast O
to O
our O
RSSM, O

it B-DAT
feeds O
generated O
observations O
back O

into B-DAT
the O
model O
which O
makes O

forward B-DAT
predictions O
expensive. O
Karl O
et O

al. B-DAT
(2016) O
address O
mode O
collapse O

to B-DAT
a O
single O
future O
by O

restricting B-DAT
the O
transition O
function, O
(Moerland O

et B-DAT
al., O
2017) O
focus O
on O

multi-modal B-DAT
transitions, O
and O
Doerr O
et O

al. B-DAT
(2018) O
stabilize O
training O
of O

purely B-DAT
stochastic O
models. O
Buesing O
et O

al. B-DAT
(2018) O
propose O
a O
model O

similar B-DAT
to O
ours O
but O
use O

in B-DAT
a O
hybrid O
agent O
instead O

for B-DAT
explicit O
planning. O
Video O
prediction O
Video O
prediction O
is O

an B-DAT
active O
area O
of O
research O

in B-DAT
deep O
learning. O
Oh O
et O

al. B-DAT
(2015) O
and O
Chiappa O
et O

al. B-DAT
(2017) O
achieve O
visually O
plausible O

predictions B-DAT
on O
Atari O
games O
using O

deterministic B-DAT
models. O
Kalchbrenner O
et O
al O

. B-DAT
(2016) O
introduce O
an O
autoregressive O

video B-DAT
prediction O
model O
using O
gated O

CNNs B-DAT
and O
LSTMs. O
Recent O
approaches O

introduce B-DAT
stochasticity O
to O
the O
model O

to B-DAT
capture O
multiple O
futures O
(Babaeizadeh O

et B-DAT
al., O
2017; O
Denton O
& O

Fergus, B-DAT
2018). O
To O
obtain O
realistic O

predictions, B-DAT
Mathieu O
et O
al. O
(2015) O

and B-DAT
Vondrick O
et O
al. O
(2016) O

use B-DAT
adversarial O
losses. O
In O
simulated O

environments, B-DAT
Gemici O
et O
al. O
(2017) O

augment B-DAT
dynamics O
models O
with O
an O

external B-DAT
memory O
to O
remember O
long-time O

contexts. B-DAT
van O
den O
Oord O
et O

al. B-DAT
(2017) O
propose O
a O
variational O

model B-DAT
that O
avoids O
sampling O
using O

a B-DAT
nearest O
neighbor O
look-up, O
yielding O

high B-DAT
fidelity O
image O
predictions. O
These O

models B-DAT
are O
complimentary O
to O
our O

approach. B-DAT
17 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
H. O
Video O
Predictions O

Pl B-DAT
aN O
et O
Tr O

ue B-DAT
Context O
6 O
10 O
15 O
20 O

25 B-DAT
30 O
35 O
40 O
45 O

50 B-DAT

M B-DAT
od O
el O
Tr O

ue B-DAT
M O
od O
el O

Tr B-DAT
ue O
M O
od O

el B-DAT
Pl O
aN O

et B-DAT
+ O
O O
ve O

rs B-DAT
ho O
ot O
in O

g B-DAT
Tr O
ue O

Context B-DAT
6 O
10 O
15 O
20 O
25 O
30 O
35 O
40 O
45 O
50 O

M B-DAT
od O
el O
Tr O

ue B-DAT
M O
od O
el O

Tr B-DAT
ue O
M O
od O

el B-DAT
D O
et O

er B-DAT
m O
in O
is O

tic B-DAT
(G O
R O
U O

Tr B-DAT
ue O
Context O
6 O
10 O
15 O
20 O

25 B-DAT
30 O
35 O
40 O
45 O

50 B-DAT

M B-DAT
od O
el O
Tr O

ue B-DAT
M O
od O
el O

Tr B-DAT
ue O
M O
od O

el B-DAT
St O
oc O

ha B-DAT
st O
ic O
(S O

SM B-DAT
) O
Tr O
ue O

Context B-DAT
6 O
10 O
15 O
20 O
25 O
30 O
35 O
40 O
45 O
50 O

M B-DAT
od O
el O
Tr O

ue B-DAT
M O
od O
el O

Tr B-DAT
ue O
M O
od O

el B-DAT
Figure O
10: O
Open-loop O
video O
predictions O

for B-DAT
test O
episodes. O
The O
columns O

1–5 B-DAT
show O
reconstructed O
context O
frames O

and B-DAT
the O
remaining O
images O
are O

generated B-DAT
open-loop. O
Our O
RSSM O
achieves O

pixel-accurate B-DAT
predictions O
for O
50 O
steps O

into B-DAT
the O
future O
in O
the O

cheetah B-DAT
environment. O
We O
randomly O
selected O

action B-DAT
sequences O
from O
test O
episodes O

collected B-DAT
with O
action O
noise O
alongside O

the B-DAT
training O
episodes O

. B-DAT
18 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
I. O
State O
Diagnostics O

Figure B-DAT
11: O
Open-loop O
state O
diagnostics. O

We B-DAT
freeze O
the O
dynamics O
model O

of B-DAT
a O
PlaNet O
agent O
and O

learn B-DAT
small O
neural O
networks O
to O

predict B-DAT
the O
true O
positions, O
velocities, O

and B-DAT
reward O
of O
the O
simulator. O

The B-DAT
open-loop O
predictions O
of O
these O

quantities B-DAT
show O
that O
most O
information O

about B-DAT
the O
underlying O
system O
is O

present B-DAT
in O
the O
learned O
latent O

space B-DAT
and O
can O
be O
accurately O

predicted B-DAT
forward O
further O
than O
the O

planning B-DAT
horizons O
used O
in O
this O

work. B-DAT
19 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
J. O
Planning O
Parameters O

3.0 B-DAT
5.0 O

10.0 B-DAT
15.0 O

ite B-DAT
ra O
tio O
ns O

fraction=0.05 B-DAT
horizon=6.0 O
fraction=0.05 O
horizon=8.0 O
fraction=0.05 O

horizon=10.0 B-DAT
fraction=0.05 O
horizon=12.0 O
fraction=0.05 O
horizon=14.0 O
3.0 O

5.0 B-DAT
10.0 O

15.0 B-DAT
ite O
ra O

tio B-DAT
ns O
fraction=0.1 O
horizon=6.0 O
fraction=0.1 O
horizon=8.0 O
fraction=0.1 O

horizon=10.0 B-DAT
fraction=0.1 O
horizon=12.0 O
fraction=0.1 O
horizon=14.0 O

3.0 B-DAT
5.0 O

10.0 B-DAT
15.0 O

ite B-DAT
ra O
tio O
ns O

fraction=0.3 B-DAT
horizon=6.0 O
fraction=0.3 O
horizon=8.0 O
fraction=0.3 O

horizon=10.0 B-DAT
fraction=0.3 O
horizon=12.0 O
fraction=0.3 O
horizon=14.0 O
100.0 O
300.0 O
500.0 O
1000.0 O
proposals O

3.0 B-DAT
5.0 O

10.0 B-DAT
15.0 O

ite B-DAT
ra O
tio O
ns O

fraction=0.5 B-DAT
horizon=6.0 O
100.0 O
300.0 O
500.0 O
1000.0 O
proposals O

fraction=0.5 B-DAT
horizon=8.0 O
100.0 O
300.0 O
500.0 O
1000.0 O
proposals O

fraction=0.5 B-DAT
horizon=10.0 O
100.0 O
300.0 O
500.0 O
1000.0 O
proposals O

fraction=0.5 B-DAT
horizon=12.0 O
100.0 O
300.0 O
500.0 O
1000.0 O
proposals O

fraction=0.5 B-DAT
horizon=14.0 O
Figure O
12: O
Planning O
performance O
on O

the B-DAT
cheetah O
running O
task O
with O

the B-DAT
true O
simulator O
using O
different O

planner B-DAT
settings. O
Performance O
ranges O
from O

132 B-DAT
(blue) O
to O
837 O
(yellow O

). B-DAT
Evaluating O
more O
action O
sequences, O

optimizing B-DAT
for O
more O
iterations, O
and O

re-fitting B-DAT
to O
fewer O
of O
the O

best B-DAT
proposals O
tend O
to O
improve O

performance. B-DAT
A O
planning O
horizon O
length O

of B-DAT
6 O
is O
not O
sufficient O

and B-DAT
results O
in O
poor O
performance. O

Much B-DAT
longer O
planning O
horizons O
hurt O

performance B-DAT
because O
of O
the O
increased O

search B-DAT
space. O
For O
this O
environment, O

best B-DAT
planning O
horizon O
length O
is O

near B-DAT
8 O
steps. O
20 O

1000 O
Cheetah O
Run B-DAT

1000 O
Cheetah O
Run B-DAT

1000 O
Cheetah O
Run B-DAT

1000 O
Cheetah O
Run B-DAT

1000 O
Cheetah O
Run B-DAT

a) O
Cartpole O
(b) O
Reacher O
(c) O
Cheetah B-DAT
(d) O
Finger O
(e) O
Cup O
(f O

1000 O
Cheetah B-DAT
Run O

1000 O
Cheetah B-DAT
Run O

1000 O
Cheetah B-DAT
Run O

1000 O
Cheetah B-DAT
Run O

1000 O
Cheetah B-DAT
Run O

variety O
of O
tasks O
from O
the O
DeepMind B-DAT
control O
suite, O
shown O
in O
Figure O

continuous O
control O
tasks O
of O
the O
DeepMind B-DAT
control O
suite O
(Tassa O
et O
al O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
Danijar O
Hafner O
1 O
2 O
Timothy O

Lillicrap B-DAT
3 O
Ian O
Fischer O
4 O

Ruben B-DAT
Villegas O
1 O
5 O
David O

Ha B-DAT
1 O
Honglak O
Lee O
1 O

James B-DAT
Davidson O
1 O

Abstract B-DAT
Planning O
has O
been O
very O

successful B-DAT
for O
control O
tasks O
with O

known B-DAT
environment O
dynamics. O
To O
leverage O

planning B-DAT
in O
unknown O
environments, O
the O

agent B-DAT
needs O
to O
learn O
the O

dynamics B-DAT
from O
interactions O
with O
the O

world. B-DAT
However, O
learning O
dynamics O
models O

that B-DAT
are O
accurate O
enough O
for O

planning B-DAT
has O
been O
a O
long-standing O

challenge, B-DAT
especially O
in O
image-based O
domains. O

We B-DAT
propose O
the O
Deep O
Planning O

Network B-DAT
(PlaNet), O
a O
purely O
model-based O

agent B-DAT
that O
learns O
the O
environment O

dynamics B-DAT
from O
images O
and O
chooses O

actions B-DAT
through O
fast O
online O
planning O

in B-DAT
latent O
space. O
To O
achieve O

high B-DAT
performance, O
the O
dynamics O
model O

must B-DAT
accurately O
predict O
the O
rewards O

ahead B-DAT
for O
multiple O
time O
steps. O

We B-DAT
approach O
this O
using O
a O

latent B-DAT
dynamics O
model O
with O
both O

deterministic B-DAT
and O
stochastic O
transition O
components. O

Moreover, B-DAT
we O
propose O
a O
multi-step O

variational B-DAT
inference O
objective O
that O
we O

name B-DAT
latent O
overshooting. O
Using O
only O

pixel B-DAT
observations, O
our O
agent O
solves O

continuous B-DAT
control O
tasks O
with O
contact O

dynamics, B-DAT
partial O
observability, O
and O
sparse O

rewards, B-DAT
which O
exceed O
the O
difficulty O

of B-DAT
tasks O
that O
were O
previously O

solved B-DAT
by O
planning O
with O
learned O

models. B-DAT
PlaNet O
uses O
substantially O
fewer O

episodes B-DAT
and O
reaches O
final O
performance O

close B-DAT
to O
and O
sometimes O
higher O

than B-DAT
strong O
model-free O
algorithms. O
1. O
Introduction O
Planning O
is O
a O

natural B-DAT
and O
powerful O
approach O
to O

decision B-DAT
making O
problems O
with O
known O

dynamics, B-DAT
such O
as O
game O
play O

- B-DAT
ing O
and O
simulated O
robot O

control B-DAT
(Tassa O
et O
al., O
2012; O

Silver B-DAT
et O
al., O
2017; O
Moravčík O

et B-DAT
al., O
2017). O
To O
plan O

in B-DAT
unknown O
environments, O
the O
agent O

needs B-DAT
to O
learn O
the O
dynamics O

from B-DAT
experience. O
Learning O
dynamics O
models O

that B-DAT
are O
accurate O
1Google O
Brain O
2University O
of O
Toronto O

3DeepMind B-DAT
4Google O
Research O
5University O
of O

Michigan. B-DAT
Correspondence O
to: O
Danijar O
Hafner O

<mail@danijar.com B-DAT

>. B-DAT
Proceedings O
of O
the O
36 O
th O

International B-DAT
Conference O
on O
Machine O
Learning O

, B-DAT
Long O
Beach, O
California, O
PMLR O
97, O
2019. O
Copyright O
2019 O
by O
the O

author(s B-DAT

). B-DAT
enough O
for O
planning O
has O
been O

a B-DAT
long-standing O
challenge. O
Key O
difficulties O

include B-DAT
model O
inaccuracies, O
accumulating O
errors O

of B-DAT
multi-step O
predictions, O
failure O
to O

capture B-DAT
multiple O
possible O
futures, O
and O

overconfident B-DAT
predictions O
outside O
of O
the O

training B-DAT
distribution O

. B-DAT
Planning O
using O
learned O
models O
offers O

several B-DAT
benefits O
over O
model-free O
reinforcement O

learning. B-DAT
First, O
model-based O
plan- O
ning O

can B-DAT
be O
more O
data O
efficient O

because B-DAT
it O
leverages O
a O
richer O

training B-DAT
signal O
and O
does O
not O

require B-DAT
propagating O
rewards O
through O
Bellman O

backups. B-DAT
Moreover, O
planning O
carries O
the O

promise B-DAT
of O
increasing O
performance O
just O

by B-DAT
increasing O
the O
computational O
budget O

for B-DAT
searching O
for O
actions, O
as O

shown B-DAT
by O
Silver O
et O
al O

. B-DAT
(2017). O
Finally, O
learned O
dynamics O

can B-DAT
be O
independent O
of O
any O

specific B-DAT
task O
and O
thus O
have O

the B-DAT
potential O
to O
transfer O
well O

to B-DAT
other O
tasks O
in O
the O

environment. B-DAT
Recent O
work O
has O
shown O
promise O

in B-DAT
learning O
the O
dynamics O
of O

simple B-DAT
low-dimensional O
environments O
(Deisenroth O

& B-DAT
Ras- O
mussen, O
2011; O
Gal O

et B-DAT
al., O
2016; O
Amos O
et O

al., B-DAT
2018; O
Chua O
et O
al O

., B-DAT
2018; O
Henaff O
et O
al., O
2018 O

). B-DAT
However, O
these O
approaches O
typically O

assume B-DAT
access O
to O
the O
underlying O

state B-DAT
of O
the O
world O
and O

the B-DAT
reward O
function, O
which O
may O

not B-DAT
be O
available O
in O
prac- O

tice. B-DAT
In O
high-dimensional O
environments, O
we O

would B-DAT
like O
to O
learn O
the O

dynamics B-DAT
in O
a O
compact O
latent O

space B-DAT
to O
enable O
fast O
planning. O

The B-DAT
success O
of O
such O
latent O

models B-DAT
has O
previously O
been O
limited O

to B-DAT
simple O
tasks O
such O
as O

balancing B-DAT
cartpoles O
and O
controlling O
2-link O

arms B-DAT
from O
dense O
rewards O
(Watter O

et B-DAT
al., O
2015; O
Banijamali O
et O

al., B-DAT
2017). O
In O
this O
paper, O
we O
propose O

the B-DAT
Deep O
Planning O
Network O
(PlaNet O

), B-DAT
a O
model-based O
agent O
that O

learns B-DAT
the O
environment O
dynamics O
from O

pixels B-DAT
and O
chooses O
actions O
through O

online B-DAT
planning O
in O
a O
compact O

latent B-DAT
space. O
To O
learn O
the O

dynamics, B-DAT
we O
use O
a O
transition O

model B-DAT
with O
both O
stochastic O
and O

determin- B-DAT
istic O
components. O
Moreover, O
we O

experiment B-DAT
with O
a O
novel O
generalized O

variational B-DAT
objective O
that O
encourages O
multi-step O

predictions. B-DAT
PlaNet O
solves O
continuous O
control O

tasks B-DAT
from O
pixels O
that O
are O

more B-DAT
difficult O
than O
those O
previously O

solved B-DAT
by O
planning O
with O
learned O

models. B-DAT
Key O
contributions O
of O
this O
work O

are B-DAT
summarized O
as O
follows O

: B-DAT
• O
Planning O
in O
latent O
spaces O

We B-DAT
solve O
a O
variety O
of O

tasks B-DAT
from O
the O
DeepMind O
control O

suite, B-DAT
shown O
in O
Figure O
1 O

, B-DAT
by O
learning O
a O
dynamics O

model B-DAT
and O
efficiently O
planning O
in O
ar O
X O

iv B-DAT
:1 O
81 O
1 O

. B-DAT
04 O
55 O

1v B-DAT
5 O

cs B-DAT
.L O
G O

4 B-DAT

J B-DAT

2 B-DAT
01 O
9 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
(a) O
Cartpole O
(b) O
Reacher O
(c O

) B-DAT
Cheetah O
(d) O
Finger O
(e) O

Cup B-DAT
(f) O
Walker O
Figure O
1: O
Image-based O
control O
domains O

used B-DAT
in O
our O
experiments. O
The O

images B-DAT
show O
agent O
observations O
before O

downscaling B-DAT
to O
64× O
64× O
3 O

pixels. B-DAT
(a) O
The O
cartpole O
swingup O

task B-DAT
has O
a O
fixed O
camera O

so B-DAT
the O
cart O
can O
move O

out B-DAT
of O
sight. O
(b) O
The O

reacher B-DAT
task O
has O
only O
a O

sparse B-DAT
reward. O
(c) O
The O
cheetah O

running B-DAT
task O
includes O
both O
contacts O

and B-DAT
a O
larger O
number O
of O

joints. B-DAT
(d) O
The O
finger O
spinning O

task B-DAT
includes O
contacts O
between O
the O

finger B-DAT
and O
the O
object. O
(e O

) B-DAT
The O
cup O
task O
has O

a B-DAT
sparse O
reward O
that O
is O

only B-DAT
given O
once O
the O
ball O

is B-DAT
caught. O
(f) O
The O
walker O

task B-DAT
requires O
balance O
and O
predicting O

difficult B-DAT
interactions O
with O
the O
ground O

when B-DAT
the O
robot O
is O
lying O

down. B-DAT
its O
latent O
space. O
Our O
agent O

substantially B-DAT
outperforms O
the O
model-free O
A3C O

and B-DAT
in O
some O
cases O
D4PG O

algorithm B-DAT
in O
final O
performance, O
with O

on B-DAT
average O
200× O
less O
environ O

- B-DAT
ment O
interaction O
and O
similar O

computation B-DAT
time. O
• O
Recurrent O
state O
space O
model O

We B-DAT
design O
a O
latent O
dy O

- B-DAT
namics O
model O
with O
both O

deterministic B-DAT
and O
stochastic O
components O
(Buesing O

et B-DAT
al., O
2018; O
Chung O
et O

al., B-DAT
2015). O
Our O
experiments O
indicate O

having B-DAT
both O
components O
to O
be O

crucial B-DAT
for O
high O
planning O
performance. O
• O
Latent O
overshooting O
We O
generalize O

the B-DAT
standard O
vari- O
ational O
bound O

to B-DAT
include O
multi-step O
predictions. O
Using O

only B-DAT
terms O
in O
latent O
space O

results B-DAT
in O
a O
fast O
regularizer O

that B-DAT
can O
improve O
long-term O
predictions O

and B-DAT
is O
compati- O
ble O
with O

any B-DAT
latent O
sequence O
model O

. B-DAT
2. O
Latent O
Space O
Planning O
To O

solve B-DAT
unknown O
environments O
via O
planning O

, B-DAT
we O
need O
to O
model O

the B-DAT
environment O
dynamics O
from O
experience. O

PlaNet B-DAT
does O
so O
by O
iteratively O

collecting B-DAT
data O
using O
planning O
and O

training B-DAT
the O
dynamics O
model O
on O

the B-DAT
gathered O
data. O
In O
this O

section, B-DAT
we O
introduce O
notation O
for O

the B-DAT
environment O
and O
de- O
scribe O

the B-DAT
general O
implementation O
of O
our O

model-based B-DAT
agent. O
In O
this O
section, O

we B-DAT
assume O
access O
to O
a O

learned B-DAT
dynamics O
model. O
Our O
design O

and B-DAT
training O
objective O
for O
this O

model B-DAT
are O
detailed O
in O
Section O
3 O

. B-DAT
Problem O
setup O
Since O
individual O
image O

observations B-DAT
gen- O
erally O
do O
not O

reveal B-DAT
the O
full O
state O
of O

the B-DAT
environment, O
we O
consider O
a O

partially B-DAT
observable O
Markov O
decision O
process O

(POMDP). B-DAT
We O
define O
a O
discrete O

time B-DAT
step O
t, O
hidden O
states O

st, B-DAT
image O
observations O
ot, O
continuous O

action B-DAT
vectors O
at, O
and O
scalar O

rewards B-DAT
rt, O
that O
follow O
the O

stochastic B-DAT
dynamics O

Transition B-DAT
function: O
st O
∼ O

p(st B-DAT
| O
st−1, O
at−1) O
Observation O

function: B-DAT
ot O
∼ O
p(ot O
| O

st) B-DAT
Reward O
function: O
rt O
∼ O

p(rt B-DAT
| O
st) O
Policy: O

at B-DAT
∼ O
p(at O
| O
o≤t, O

a<t), B-DAT
(1 O

) B-DAT
Algorithm O
1: O
Deep O
Planning O
Network O

(PlaNet) B-DAT
Input O
: O
R O
Action O

repeat B-DAT
S O
Seed O
episodes O
C O

Collect B-DAT
interval O
B O
Batch O
size O

L B-DAT
Chunk O
length O
α O
Learning O

rate B-DAT

p(st B-DAT
| O
st−1, O
at−1) O
Transition O

model B-DAT
p(ot O
| O
st) O
Observation O

model B-DAT
p(rt O
| O
st) O
Reward O

model B-DAT
q(st O
| O
o≤t, O
a<t) O

Encoder B-DAT
p(�) O
Exploration O
noise O
1 O
Initialize O
dataset O
D O
with O

S B-DAT
random O
seed O
episodes. O
2 O

Initialize B-DAT
model O
parameters O
θ O
randomly O

. B-DAT
3 O
while O
not O
converged O

do B-DAT
// O
Model O
fitting O
4 O
for O

update B-DAT
step O
s O
= O
1..C O

do B-DAT
5 O
Draw O
sequence O
chunks O

{(ot, B-DAT
at, O
rt)L+kt=k O
}Bi=1 O

∼ B-DAT
D O

uniformly B-DAT
at O
random O
from O
the O

dataset. B-DAT
6 O
Compute O
loss O
L(θ) O

from B-DAT
Equation O
3. O
7 O
Update O

model B-DAT
parameters O
θ O
← O

θ B-DAT
− O
α∇θL(θ). O
// O
Data O
collection O
8 O
o1 O

← B-DAT
env.reset() O
9 O
for O
time O

step B-DAT
t O
= O
1 O

.. B-DAT
⌈ O
T O
R O

do B-DAT
10 O
Infer O
belief O
over O
current O

state B-DAT
q(st O
| O
o≤t, O
a<t O

) B-DAT
from O
the O
history. O
11 O
at O
← O
planner(q(st O

| B-DAT
o≤t, O
a<t), O
p), O
see O

Algorithm B-DAT
2 O
in O
the O
appendix O

for B-DAT
details O

. B-DAT
12 O
Add O
exploration O
noise O

� B-DAT
∼ O
p(�) O
to O
the O

action. B-DAT
13 O
for O
action O
repeat O

k B-DAT
= O
1..R O
do O
14 O

rkt B-DAT
, O
o O

k B-DAT
t+1 O
← O
env.step(at) O
15 O
rt, O
ot+1 O
← O
∑R O

k=1 B-DAT
r O

k B-DAT
t O
, O
o O
R O
t+1 O

16 B-DAT
D O
← O

D B-DAT
∪ O
{(ot, O
at, O
rt)Tt=1} O
where O
we O
assume O
a O
fixed O

initial B-DAT
state O
s0 O
without O
loss O

of B-DAT
gen- O
erality. O
The O
goal O

is B-DAT
to O
implement O
a O
policy O

p(at B-DAT
| O
o≤t, O
a<t) O
that O

maximizes B-DAT
the O
expected O
sum O
of O

rewards B-DAT
Ep O

T B-DAT
t=1 O

where B-DAT
the O
expectation O
is O
over O

the B-DAT
distributions O
of O
the O
envi- O

ronment B-DAT
and O
the O
policy. O
2 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
Model-based O
planning O
PlaNet O
learns O
a O

transition B-DAT
model O
p(st O
| O
st−1 O

, B-DAT
at−1), O
observation O
model O

p(ot B-DAT
| O
st), O
and O
reward O

model B-DAT
p(rt O
| O
st) O
from O

previously B-DAT
experienced O
episodes O
(note O
italic O

letters B-DAT
for O
the O
model O
compared O

to B-DAT
upright O
letters O
for O
the O

true B-DAT
dynamics). O
The O
observation O
model O

provides B-DAT
a O
rich O
training O
signal O

but B-DAT
is O
not O
used O
for O

planning. B-DAT
We O
also O
learn O
an O

encoder B-DAT
q(st O
| O
o≤t, O
a<t) O

to B-DAT
infer O
an O
approximate O
belief O

over B-DAT
the O
current O
hidden O
state O

from B-DAT
the O
history O
using O
filtering. O

Given B-DAT
these O
components, O
we O
implement O

the B-DAT
policy O
as O
a O
planning O

algorithm B-DAT
that O
searches O
for O
the O

best B-DAT
sequence O
of O
future O
actions. O

We B-DAT
use O
model-predictive O
control O
(MPC; O

Richards, B-DAT
2005) O
to O
allow O
the O

agent B-DAT
to O
adapt O
its O
plan O

based B-DAT
on O
new O
observations, O
meaning O

we B-DAT
replan O
at O
each O
step. O

In B-DAT
contrast O
to O
model-free O
and O

hybrid B-DAT
reinforcement O
learning O
algorithms, O
we O

do B-DAT
not O
use O
a O
policy O

or B-DAT
value O
network. O
Experience O
collection O
Since O
the O
agent O

may B-DAT
not O
initially O
visit O
all O

parts B-DAT
of O
the O
environment, O
we O

need B-DAT
to O
iteratively O
collect O
new O

experience B-DAT
and O
refine O
the O
dynamics O

model. B-DAT
We O
do O
so O
by O

planning B-DAT
with O
the O
partially O
trained O

model, B-DAT
as O
shown O
in O
Algorithm O

1. B-DAT
Starting O
from O
a O
small O

amount B-DAT
of O
S O
seed O
episodes O

collected B-DAT
under O
random O
actions, O
we O

train B-DAT
the O
model O
and O
add O

one B-DAT
additional O
episode O
to O
the O

data B-DAT
set O
everyC O
update O
steps O

. B-DAT
When O
collecting O
episodes O
for O

the B-DAT
data O
set, O
we O
add O

small B-DAT
Gaussian O
exploration O
noise O
to O

the B-DAT
action. O
To O
reduce O
the O

planning B-DAT
horizon O
and O
provide O
a O

clearer B-DAT
learning O
signal O
to O
the O

model, B-DAT
we O
repeat O
each O
action O

R B-DAT
times, O
as O
common O
in O

reinforcement B-DAT
learning O
(Mnih O
et O
al., O
2015 O

; B-DAT
2016). O
Planning O
algorithm O
We O
use O
the O

cross B-DAT
entropy O
method O
(CEM; O
Rubinstein O

, B-DAT
1997; O
Chua O
et O
al., O
2018 O

) B-DAT
to O
search O
for O
the O

best B-DAT
action O
sequence O
under O
the O

model, B-DAT
as O
outlined O
in O
Algorithm O
2 O

. B-DAT
We O
decided O
on O
this O

algorithm B-DAT
because O
of O
its O
robustness O

and B-DAT
because O
it O
solved O
all O

considered B-DAT
tasks O
when O
given O
the O

true B-DAT
dynamics O
for O
planning. O
CEM O

is B-DAT
a O
population- O
based O
optimization O

algorithm B-DAT
that O
infers O
a O
distribution O

over B-DAT
action O
sequences O
that O
maximize O

the B-DAT
objective. O
As O
detailed O
in O

Algorithm B-DAT
2 O
in O
the O
appendix, O

we B-DAT
initialize O
a O
time-dependent O
diagonal O

Gaussian B-DAT
belief O
over O
optimal O
action O

sequences B-DAT
at:t+H O
∼ O
Normal(µt:t+H O
, O

σ2t:t+HI), B-DAT
where O
t O
is O
the O

current B-DAT
time O
step O
of O
the O

agent B-DAT
and O
H O
is O
the O

length B-DAT
of O
the O
planning O
horizon. O

Starting B-DAT
from O
zero O
mean O
and O

unit B-DAT
variance, O
we O
repeatedly O
sample O

J B-DAT
candidate O
action O
sequences, O
evaluate O

them B-DAT
under O
the O
model, O
and O

re-fit B-DAT
the O
belief O
to O
the O

top B-DAT
K O
action O
sequences. O
After O

I B-DAT
iterations, O
the O
planner O
returns O

the B-DAT
mean O
of O
the O
belief O

for B-DAT
the O
current O
time O
step, O

µt. B-DAT
Importantly, O
after O
receiving O
the O

next B-DAT
observation, O
the O
belief O
over O

action B-DAT
sequences O
starts O
from O
zero O

mean B-DAT
and O
unit O
variance O
again O

to B-DAT
avoid O
local O
optima. O
To O
evaluate O
a O
candidate O
action O

sequence B-DAT
under O
the O
learned O
model O

, B-DAT
we O
sample O
a O
state O

trajectory B-DAT
starting O
from O
the O
current O

state B-DAT
belief, O
and O
sum O
the O

mean B-DAT
rewards O
predicted O
along O
the O

sequence. B-DAT
Since O
we O
use O
a O

population-based B-DAT
optimizer, O
we O
found O
it O
sufficient O
to O

consider B-DAT
a O
single O
trajectory O
per O

action B-DAT
sequence O
and O
thus O
focus O

the B-DAT
computational O
budget O
on O
evaluating O

a B-DAT
larger O
number O
of O
different O

sequences. B-DAT
Because O
the O
reward O
is O

modeled B-DAT
as O
a O
function O
of O

the B-DAT
latent O
state, O
the O
planner O

can B-DAT
operate O
purely O
in O
latent O

space B-DAT
without O
generating O
images, O
which O

allows B-DAT
for O
fast O
evaluation O
of O

large B-DAT
batches O
of O
action O
sequences O

. B-DAT
The O
next O
section O
introduces O

the B-DAT
latent O
dynamics O
model O
that O

the B-DAT
planner O
uses. O
3. O
Recurrent O
State O
Space O
Model O

For B-DAT
planning, O
we O
need O
to O

evaluate B-DAT
thousands O
of O
action O
se O

- B-DAT
quences O
at O
every O
time O

step B-DAT
of O
the O
agent. O
Therefore, O

we B-DAT
use O
a O
recurrent O
state-space O

model B-DAT
(RSSM) O
that O
can O
predict O

for- B-DAT
ward O
purely O
in O
latent O

space, B-DAT
similar O
to O
recently O
proposed O

models B-DAT
(Karl O
et O
al., O
2016; O

Buesing B-DAT
et O
al., O
2018; O
Doerr O

et B-DAT
al., O
2018). O
This O
model O

can B-DAT
be O
thought O
of O
as O

a B-DAT
non-linear O
Kalman O
filter O
or O

sequential B-DAT
VAE. O
Instead O
of O
an O

extensive B-DAT
comparison O
to O
prior O
architectures, O

we B-DAT
highlight O
two O
findings O
that O

can B-DAT
guide O
future O
designs O
of O

dynamics B-DAT
models: O
our O
experiments O
show O

that B-DAT
both O
stochastic O
and O
deterministic O

paths B-DAT
in O
the O
transition O
model O

are B-DAT
crucial O
for O
successful O
planning. O

In B-DAT
this O
section, O
we O
remind O

the B-DAT
reader O
of O
latent O
state-space O

models B-DAT
and O
then O
describe O
our O

dynamics B-DAT
model. O
Latent O
dynamics O
We O
consider O
sequences O

{ot, B-DAT
at, O
rt}Tt=1 O
with O
discrete O

time B-DAT
step O
t, O
image O
observations O

ot, B-DAT
continuous O
action O
vectors O
at O

, B-DAT
and O
scalar O
rewards O
rt. O

A B-DAT
typical O
latent O
state-space O
model O

is B-DAT
shown O
in O
Figure O
2b O

and B-DAT
resembles O
the O
structure O
of O

a B-DAT
partially O
observable O
Markov O
decision O

process. B-DAT
It O
defines O
the O
generative O

process B-DAT
of O
the O
images O
and O

rewards B-DAT
using O
a O
hidden O
state O

sequence B-DAT
{st}Tt=1, O
Transition O
model: O
st O
∼ O
p(st O

| B-DAT
st−1, O
at−1) O
Observation O
model O

: B-DAT
ot O
∼ O
p(ot O
| O

st) B-DAT
Reward O
model: O
rt O
∼ O

p(rt B-DAT
| O
st), O
(2 O

) B-DAT
where O
we O
assume O
a O
fixed O

initial B-DAT
state O
s0 O
without O
loss O

of B-DAT
generality. O
The O
transition O
model O

is B-DAT
Gaussian O
with O
mean O
and O

variance B-DAT
parameterized O
by O
a O
feed-forward O

neural B-DAT
network, O
the O
observation O
model O

is B-DAT
Gaussian O
with O
mean O
parameterized O

by B-DAT
a O
deconvolutional O
neural O
network O

and B-DAT
identity O
covariance, O
and O
the O

reward B-DAT
model O
is O
a O
scalar O

Gaussian B-DAT
with O
mean O
param- O
eterized O

by B-DAT
a O
feed-forward O
neural O
network O

and B-DAT
unit O
variance. O
Note O
that O

the B-DAT
log-likelihood O
under O
a O
Gaussian O

distribution B-DAT
with O
unit O
variance O
equals O

the B-DAT
mean O
squared O
error O
up O

to B-DAT
a O
constant O

. B-DAT
Variational O
encoder O
Since O
the O
model O

is B-DAT
non-linear, O
we O
cannot O
directly O

compute B-DAT
the O
state O
posteriors O
that O

are B-DAT
needed O
for O
parameter O
learning O

. B-DAT
Instead, O
we O
use O
an O

encoder B-DAT
q(s1:T O
| O
o1:T O
, O

a1:T B-DAT
) O
= O
∏T O
t=1 O
q(st O
| O
st−1 O

, B-DAT
at−1, O
ot) O
to O
infer O

approx- B-DAT
imate O
state O
posteriors O
from O
past O

observations B-DAT
and O
actions, O
where O
q(st O

| B-DAT
st−1, O
at−1, O
ot) O
is O

a B-DAT
diagonal O
Gaussian O
with O
mean O

and B-DAT
variance O
parameterized O
by O
a O

convolutional B-DAT
neural O

3 B-DAT

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
o1, O
r1 O
o2, O
r2 O
o3 O

, B-DAT
r3 O
h1 O
h2 O
h3 O

a1 B-DAT
a2 O
(a) O
Deterministic O
model O
(RNN O

) B-DAT
o1, O
r1 O
o2, O
r2 O
o3 O

, B-DAT
r3 O
s1 O
s2 O
s3 O

a1 B-DAT
a2 O
(b) O
Stochastic O
model O
(SSM O

) B-DAT
o1, O
r1 O
o2, O
r2 O
o3 O

, B-DAT
r3 O
s1 O
s2 O
s3 O

h1 B-DAT
h2 O
h3 O
a1 O
a2 O

c) B-DAT
Recurrent O
state-space O
model O
(RSSM) O
Figure O
2: O
Latent O
dynamics O
model O

designs. B-DAT
In O
this O
example, O
the O

model B-DAT
observes O
the O
first O
two O

time B-DAT
steps O
and O
predicts O
the O

third. B-DAT
Circles O
represent O
stochastic O
variables O

and B-DAT
squares O
deterministic O
variables. O
Solid O

lines B-DAT
denote O
the O
generative O
process O

and B-DAT
dashed O
lines O
the O
inference O

model. B-DAT
(a) O
Transitions O
in O
a O

recurrent B-DAT
neural O
network O
are O
purely O

deterministic. B-DAT
This O
prevents O
the O
model O

from B-DAT
capturing O
multiple O
futures O
and O

makes B-DAT
it O
easy O
for O
the O

planner B-DAT
to O
exploit O
inaccuracies. O
(b O

) B-DAT
Transitions O
in O
a O
state-space O

model B-DAT
are O
purely O
stochastic. O
This O

makes B-DAT
it O
difficult O
to O
remember O

information B-DAT
over O
multiple O
time O
steps. O
( O

c) B-DAT
We O
split O
the O
state O

into B-DAT
stochastic O
and O
deterministic O
parts, O

allowing B-DAT
the O
model O
to O
robustly O

learn B-DAT
to O
predict O
multiple O
futures. O
network O
followed O
by O
a O
feed-forward O

neural B-DAT
network. O
We O
use O
the O

filtering B-DAT
posterior O
that O
conditions O
on O

past B-DAT
observations O
since O
we O
are O

ultimately B-DAT
interested O
in O
using O
the O

model B-DAT
for O
planning, O
but O
one O

may B-DAT
also O
use O
the O
full O

smoothing B-DAT
posterior O
during O
training O
(Babaeizadeh O

et B-DAT
al., O
2017; O
Gregor O

& B-DAT
Besse, O
2018 O

). B-DAT
Training O
objective O
Using O
the O
encoder O

, B-DAT
we O
construct O
a O
variational O

bound B-DAT
on O
the O
data O
log-likelihood. O

For B-DAT
simplicity, O
we O
write O
losses O

for B-DAT
predicting O
only O
the O

observations B-DAT
— O
the O
reward O
losses O

follow B-DAT
by O
analogy. O
The O
variational O

bound B-DAT
obtained O
using O
Jensen’s O
inequality O

is B-DAT
ln O
p(o1:T O
| O
a1:T O

) B-DAT
, O
ln O

t B-DAT
p(st O
| O
st−1, O
at−1)p(ot O

| B-DAT
st) O
ds1:T O

T∑ B-DAT
t=1 O

Eq(st|o≤t,a<t)[ln B-DAT
p(ot O
| O
st)] O
reconstruction O

E B-DAT
q(st−1|o≤t−1,a<t−1) O
[ O
KL[q(st O
| O
o≤t, O
a<t O

) B-DAT
‖ O
p(st O
| O
st−1, O

at−1)] B-DAT
] O
complexity O

3) B-DAT
For O
the O
derivation, O
please O
see O

Equation B-DAT
8 O
in O
the O
appendix O

. B-DAT
Estimating O
the O
outer O
expectations O

using B-DAT
a O
single O
reparam- O
eterized O

sample B-DAT
yields O
an O
efficient O
objective O

for B-DAT
inference O
and O
learning O
in O

non-linear B-DAT
latent O
variable O
models O
that O

can B-DAT
be O
optimized O
using O
gradient O

ascent B-DAT
(Kingma O
& O
Welling, O
2013; O

Rezende B-DAT
et O
al., O
2014; O
Krishnan O

et B-DAT
al., O
2017). O
Deterministic O
path O
Despite O
its O
generality O

, B-DAT
the O
purely O
stochastic O
transitions O

make B-DAT
it O
difficult O
for O
the O

transition B-DAT
model O
to O
reliably O
remember O

information B-DAT
for O
multiple O
time O
steps. O

In B-DAT
theory, O
this O
model O
could O

learn B-DAT
to O
set O
the O
variance O

to B-DAT
zero O
for O
some O
state O

components, B-DAT
but O
the O
optimization O
pro- O

cedure B-DAT
may O
not O
find O
this O

solution. B-DAT
This O
motivates O
including O
a O

deterministic B-DAT
sequence O
of O
activation O

vectors B-DAT
{ht}Tt=1 O
that O
allow O
the O

model B-DAT
to O
access O
not O
just O

the B-DAT
last O
state O
but O
all O

pre- B-DAT
vious O
states O
deterministically O
(Chung O

et B-DAT
al., O
2015; O
Buesing O
et O

al., B-DAT
2018). O
We O
use O
such O

a B-DAT
model, O
shown O
in O
Figure O

2c, B-DAT
that O
we O
name O
recurrent O
state-space O
model O

(RSSM B-DAT

), B-DAT
Deterministic O
state O
model: O
ht O

= B-DAT
f(ht−1, O
st−1, O
at−1) O
Stochastic O

state B-DAT
model: O
st O
∼ O
p(st O

| B-DAT
ht) O
Observation O
model: O
ot O

∼ B-DAT
p(ot O
| O
ht, O
st O

) B-DAT
Reward O
model: O
rt O
∼ O

p(rt B-DAT
| O
ht, O
st), O
(4 O

) B-DAT
where O
f(ht−1, O
st−1, O
at−1) O
is O

implemented B-DAT
as O
a O
recurrent O
neural O

network B-DAT
(RNN). O
Intuitively, O
we O
can O

understand B-DAT
this O
model O
as O
splitting O

the B-DAT
state O
into O
a O
stochastic O

part B-DAT
st O
and O
a O
de O

- B-DAT
terministic O
part O
ht, O
which O

depend B-DAT
on O
the O
stochastic O
and O

deter- B-DAT
ministic O
parts O
at O
the O

previous B-DAT
time O
step O
through O
the O

RNN. B-DAT
We O
use O
the O
encoder O

q(s1:T B-DAT
| O
o1:T O
, O

a1:T B-DAT
) O
= O
∏T O
t=1 O
q(st O
| O
ht O

, B-DAT
ot) O
to O
parameterize O
the O
approximate O
state O

posteriors. B-DAT
Impor- O
tantly, O
all O
information O

about B-DAT
the O
observations O
must O
pass O

through B-DAT
the O
sampling O
step O
of O

the B-DAT
encoder O
to O
avoid O
a O

deter- B-DAT
ministic O
shortcut O
from O
inputs O

to B-DAT
reconstructions O

. B-DAT
In O
the O
next O
section, O
we O

identify B-DAT
a O
limitation O
of O
the O

standard B-DAT
objective O
for O
latent O
sequence O

models B-DAT
and O
propose O
a O
general O

- B-DAT
ization O
of O
it O
that O

improves B-DAT
long-term O
predictions. O
4. O
Latent O
Overshooting O
In O
the O

previous B-DAT
section, O
we O
derived O
the O

typical B-DAT
variational O
bound O
for O
learning O

and B-DAT
inference O
in O
latent O
sequence O

models B-DAT
(Equation O
3). O
As O
show O

in B-DAT
Figure O
3a, O
this O
objective O

function B-DAT
contains O
reconstruction O
terms O
for O

the B-DAT
observations O
and O
KL- O
divergence O

regularizers B-DAT
for O
the O
approximate O
posteriors O

. B-DAT
A O
limitation O
of O
this O

objective B-DAT
is O
that O
the O
stochastic O

path B-DAT
of O
the O
transition O
function O

p(st B-DAT
| O
st−1, O
at−1) O
is O

only B-DAT
trained O
via O
the O
KL-divergence O

regularizers B-DAT
for O
one-step O
predictions: O
the O

gra- B-DAT
dient O
flows O
through O

p(st B-DAT
| O
st−1, O
at−1) O
directly O

into B-DAT
q(st−1) O
but O
never O
traverses O

a B-DAT
chain O
of O
multiple O

p(st B-DAT
| O
st−1, O
at−1). O
In O

this B-DAT
section, O
we O
generalize O
this O

variational B-DAT
bound O
to O
latent O
overshooting, O

which B-DAT
trains O
all O
multi-step O
predictions O

in B-DAT
la- O
tent O
space. O
We O

found B-DAT
that O
several O
dynamics O
models O

benefit B-DAT
4 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
o1, O
r1 O
o2, O
r2 O
o3 O

, B-DAT
r3 O
s1|1 O
s2|2 O
s3|3 O

s2|1 B-DAT
s3|2 O
(a) O
Standard O
variational O
bound O

o1, B-DAT
r1 O
o2, O
r2 O
o3, O

r3 B-DAT
s1|1 O
s2|2 O
s3|3 O

s2|1 B-DAT
s3|2 O
s3|1 O

b) B-DAT
Observation O
overshooting O
o1, O
r1 O
o2, O
r2 O
o3 O

, B-DAT
r3 O
s1|1 O
s2|2 O
s3|3 O

s2|1 B-DAT
s3|2 O
s3|1 O

c) B-DAT
Latent O
overshooting O
Figure O
3: O
Unrolling O
schemes. O
The O

labels B-DAT
si|j O
are O
short O
for O

the B-DAT
state O
at O
time O
i O

conditioned B-DAT
on O
observations O
up O
to O

time B-DAT
j. O
Arrows O
pointing O
at O

shaded B-DAT
circles O
indicate O
log-likelihood O
loss O

terms. B-DAT
Wavy O
arrows O
indicate O
KL-divergence O

loss B-DAT
terms. O
(a) O
The O
standard O

variational B-DAT
objectives O
decodes O
the O
posterior O

at B-DAT
every O
step O
to O
compute O

the B-DAT
reconstruction O
loss. O
It O
also O

places B-DAT
a O
KL O
on O
the O

prior B-DAT
and O
posterior O
at O
every O

step, B-DAT
which O
trains O
the O
transition O

function B-DAT
for O
one-step O
predictions. O
(b O

) B-DAT
Observation O
overshooting O
(Amos O
et O

al., B-DAT
2018) O
decodes O
all O
multi-step O

predictions B-DAT
to O
apply O
additional O
reconstruction O

losses. B-DAT
This O
is O
typically O
too O

expensive B-DAT
in O
image O
domains. O
(c) O

Latent B-DAT
overshooting O
predicts O
all O
multi-step O

priors. B-DAT
These O
state O
beliefs O
are O

trained B-DAT
towards O
their O
corresponding O
posteriors O

in B-DAT
latent O
space O
to O
encourage O

accurate B-DAT
multi-step O
predictions. O
from O
latent O
overshooting, O
although O
our O

final B-DAT
agent O
using O
the O
RSSM O

model B-DAT
does O
not O
require O
it O

(see B-DAT
Appendix O
D O

). B-DAT
Limited O
capacity O
If O
we O
could O

train B-DAT
our O
model O
to O
make O

per- B-DAT
fect O
one-step O
predictions, O
it O

would B-DAT
also O
make O
perfect O
multi O

- B-DAT
step O
predictions, O
so O
this O

would B-DAT
not O
be O
a O
problem. O

However, B-DAT
when O
using O
a O
model O

with B-DAT
limited O
capacity O
and O
restricted O

distributional B-DAT
family, O
training O
the O
model O

only B-DAT
on O
one-step O
predictions O
until O

convergence B-DAT
does O
in O
general O
not O

coincide B-DAT
with O
the O
model O
that O

is B-DAT
best O
at O
multi-step O
predictions. O

For B-DAT
suc- O
cessful O
planning, O
we O

need B-DAT
accurate O
multi-step O
predictions. O
Therefore, O

we B-DAT
take O
inspiration O
from O
Amos O

et B-DAT
al. O
(2018) O
and O
earlier O

related B-DAT
ideas O
(Krishnan O
et O
al., O
2015 O

; B-DAT
Lamb O
et O
al., O
2016; O

Chiappa B-DAT
et O
al., O
2017), O
and O

train B-DAT
the O
model O
on O
multi- O

step B-DAT
predictions O
of O
all O
distances. O

We B-DAT
develop O
this O
idea O
for O

latent B-DAT
sequence O
models, O
showing O
that O

multi-step B-DAT
predictions O
can O
be O
improved O

by B-DAT
a O
loss O
in O
latent O

space, B-DAT
without O
having O
to O
generate O

additional B-DAT
images. O
Multi-step O
prediction O
We O
start O
by O

generalizing B-DAT
the O
stan- O
dard O
variational O

bound B-DAT
(Equation O
3) O
from O
training O

one-step B-DAT
predictions O
to O
training O
multi-step O

predictions B-DAT
of O
a O
fixed O
dis O

- B-DAT
tance O
d. O
For O
ease O

of B-DAT
notation, O
we O
omit O
actions O

in B-DAT
the O
con- O
ditioning O
set O

here; B-DAT
every O
distribution O
over O
st O

is B-DAT
conditioned O
upon O
a<t. O
We O

first B-DAT
define O
multi-step O
predictions, O
which O

are B-DAT
computed O
by O
repeatedly O
applying O

the B-DAT
transition O
model O
and O
integrating O

out B-DAT
the O
intermediate O
states, O
p(st O
| O
st−d O

) B-DAT
, O
∫ O
t∏ O
τ=t−d+1 O

p(sτ B-DAT
| O
sτ−1) O
dst−d+1:t−1 O
= O
Ep(st−1|st−d)[p(st O
| O
st−1 O

)]. B-DAT
(5 O

) B-DAT
The O
case O
d O
= O
1 O

recovers B-DAT
the O
one-step O
transitions O
used O

in B-DAT
the O
original O
model. O
Given O

this B-DAT
definition O
of O
a O
multi-step O

predic B-DAT

- B-DAT
tion, O
we O
generalize O
Equation O
3 O

to B-DAT
the O
variational O
bound O
on O

the B-DAT
multi-step O
predictive O
distribution O
pd O

, B-DAT
ln O
pd(o1:T O
) O
, O
ln O

∫ B-DAT
T O

∏ B-DAT
t=1 O

p(st B-DAT
| O
st−d)p(ot O
| O
st) O

ds1:T B-DAT
≥ O
T∑ O
t=1 O

Eq(st|o≤t)[ln B-DAT
p(ot O
| O
st)] O
reconstruction O

E B-DAT
p(st−1|st−d)q(st−d|o≤t−d) O
[ O
KL[q(st O
| O
o≤t O

) B-DAT
‖ O
p(st O
| O
st−1)] O
] O
multi-step O
prediction O

6) B-DAT
For O
the O
derivation, O
please O
see O

Equation B-DAT
9 O
in O
the O
appendix O

. B-DAT
Maximizing O
this O
objective O
trains O

the B-DAT
multi-step O
predictive O
distribution. O
This O

reflects B-DAT
the O
fact O
that O
during O

planning, B-DAT
the O
model O
makes O
predictions O

without B-DAT
having O
access O
to O
all O

the B-DAT
preceding O
observations. O
We O
conjecture O
that O
Equation O
6 O

is B-DAT
also O
a O
lower O
bound O

on B-DAT
ln O
p(o1:T O
) O
based O

on B-DAT
the O
data O
processing O
inequality O

. B-DAT
Since O
the O
latent O
state O

sequence B-DAT
is O
Markovian, O
for O

d B-DAT
≥ O
1 O
we O
have O

I(st; B-DAT
st−d) O
≤ O
I(st; O
st−1) O

and B-DAT
thus O
E[ln O

pd(o1:T B-DAT
)] O
≤ O
E[ln O

p(o1:T B-DAT
)]. O
Hence, O
every O
bound O

on B-DAT
the O
multi-step O
predic- O
tive O

distribution B-DAT
is O
also O
a O
bound O

on B-DAT
the O
one-step O
predictive O
distribution O

in B-DAT
expectation O
over O
the O
data O

set. B-DAT
For O
details, O
please O
see O

Equation B-DAT
10 O
in O
the O
appendix. O

In B-DAT
the O
next O
para- O
graph, O

we B-DAT
alleviate O
the O
limitation O
that O

a B-DAT
particular O
pd O
only O
trains O

predictions B-DAT
of O
one O
distance O
and O

arrive B-DAT
at O
our O
final O
objective. O
Latent O
overshooting O
We O
introduced O
a O

bound B-DAT
on O
predic- O
tions O
of O

a B-DAT
given O
distance O
d. O
However O

, B-DAT
for O
planning O
we O
need O

accurate B-DAT
predictions O
not O
just O
for O

a B-DAT
fixed O
distance O
but O
for O

all B-DAT
distances O
up O
to O
the O

planning B-DAT
horizon. O
We O
introduce O
la- O

tent B-DAT
overshooting O
for O
this, O
an O

objective B-DAT
function O
for O
latent O
sequence O

models B-DAT
that O
generalizes O
the O
standard O

variational B-DAT
bound O
(Equation O
3) O
to O

train B-DAT
the O
model O
on O
multi-step O

predic- B-DAT
5 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
tions O
of O
all O
distances O
1 O

≤ B-DAT
d O
≤ O
D O

, B-DAT
1 O

D B-DAT
D∑ O
d=1 O

ln B-DAT
pd(o1:T O
) O
≥ O
T∑ O
t=1 O

Eq(st|o≤t)[ln B-DAT
p(ot O
| O
st)] O
reconstruction O

1 B-DAT
D O
D∑ O
d=1 O

βd B-DAT
E O
p(st−1|st−d)q(st−d|o≤t−d) O
[ O
KL[q(st O
| O
o≤t O

) B-DAT
‖ O
p(st O
| O
st−1)] O
] O
latent O
overshooting O

7) B-DAT
Latent O
overshooting O
can O
be O
interpreted O

as B-DAT
a O
regularizer O
in O
latent O

space B-DAT
that O
encourages O
consistency O
between O

one-step B-DAT
and O
multi-step O
predictions, O
which O

we B-DAT
know O
should O
be O
equiv O

- B-DAT
alent O
in O
expectation O
over O

the B-DAT
data O
set. O
We O
include O

weighting B-DAT
factors O
{βd}Dd=1 O
analogously O
to O

the B-DAT
β-VAE O
(Higgins O
et O
al., O
2016 O

). B-DAT
While O
we O
set O
all O

β>1 B-DAT
to O
the O
same O
value O

for B-DAT
sim- O
plicity, O
they O
could O

be B-DAT
chosen O
to O
let O
the O

model B-DAT
focus O
more O
on O
long-term O

or B-DAT
short-term O
predictions. O
In O
practice, O

we B-DAT
stop O
gradients O
of O
the O

posterior B-DAT
distributions O
for O
overshoot- O
ing O

distances B-DAT
d O
> O
1, O
so O

that B-DAT
the O
multi-step O
predictions O
are O

trained B-DAT
towards O
the O
informed O
posteriors, O

but B-DAT
not O
the O
other O
way O

around. B-DAT
5. O
Experiments O
We O
evaluate O
PlaNet O

on B-DAT
six O
continuous O
control O
tasks O

from B-DAT
pixels. O
We O
explore O
multiple O

design B-DAT
axes O
of O
the O
agent O

: B-DAT
the O
stochastic O
and O
deterministic O

paths B-DAT
in O
the O
dynamics O
model, O

iterative B-DAT
planning, O
and O
online O
experience O

collection. B-DAT
We O
refer O
to O
the O

appendix B-DAT
for O
hyper O
parameters O
(Appendix O

A) B-DAT
and O
additional O
experiments O
(Appendices O

C B-DAT
to O
E). O
Besides O
the O

action B-DAT
repeat, O
we O
use O
the O

same B-DAT
hyper O
parameters O
for O
all O

tasks. B-DAT
Within O
less O
than O
one O

hundredth B-DAT
the O
episodes, O
PlaNet O
outperforms O

A3C B-DAT
(Mnih O
et O
al., O
2016) O

and B-DAT
achieves O
sim- O
ilar O
performance O

to B-DAT
the O
top O
model-free O
algorithm O

D4PG B-DAT
(Barth-Maron O
et O
al., O
2018). O

The B-DAT
training O
time O
of O
10 O

to B-DAT
20 O
hours O
(depending O
on O

the B-DAT
task) O
on O
a O
single O

Nvidia B-DAT
V100 O
GPU O
compares O
favorably O

to B-DAT
that O
of O
A3C O
and O

D4PG. B-DAT
Our O
implementation O
uses O
TensorFlow O

Probability B-DAT
(Dillon O
et O
al., O
2017). O

Please B-DAT
visit O
https://danijar.com/planet O
for O
access O

to B-DAT
the O
code O
and O
videos O

of B-DAT
the O
trained O
agent. O
For O
our O
evaluation, O
we O
consider O

six B-DAT
image-based O
continuous O
control O
tasks O

of B-DAT
the O
DeepMind O
control O
suite O

(Tassa B-DAT
et O
al., O
2018), O
shown O

in B-DAT
Figure O
1. O
These O
environments O

provide B-DAT
qualitatively O
different O
challenges. O
The O

cartpole B-DAT
swingup O
task O
requires O
a O

long B-DAT
planning O
horizon O
and O
to O

memorize B-DAT
the O
cart O
when O
it O

is B-DAT
out O
of O
view, O
reacher O

has B-DAT
a O
sparse O
reward O
given O

when B-DAT
the O
hand O
and O
goal O

area B-DAT
overlap, O
finger O
spinning O
includes O

contact B-DAT
dynamics O
between O
the O
finger O

and B-DAT
the O
object, O
cheetah O
exhibits O

larger B-DAT
state O
and O
action O
spaces O

, B-DAT
the O
cup O
task O
only O

has B-DAT
a O
sparse O
reward O
for O

when B-DAT
the O
ball O
is O
caught, O

and B-DAT
the O
walker O
is O
challenging O

because B-DAT
the O
robot O
first O
has O

to B-DAT
stand O
up O
and O
then O

walk, B-DAT
resulting O
in O
collisions O
with O

the B-DAT
ground O
that O
are O
difficult O

to B-DAT
predict. O
In O
all O
tasks, O

the B-DAT
only O
observations O
are O
third-person O

camera B-DAT
images O
of O
size O
64× O
64 O

× B-DAT
3 O
pixels. O
Comparison O
to O
model-free O
methods O
Figure O

4 B-DAT
compares O
the O
performance O
of O

PlaNet B-DAT
to O
the O
model-free O
algorithms O

re- B-DAT
ported O
by O
Tassa O
et O

al. B-DAT
(2018). O
Within O
100 O
episodes O

, B-DAT
PlaNet O
outperforms O
the O
policy-gradient O

method B-DAT
A3C O
trained O
from O
proprioceptive O

states B-DAT
for O
100,000 O
episodes, O
on O

all B-DAT
tasks. O
Af- O
ter O
500 O

episodes, B-DAT
it O
achieves O
performance O
similar O

to B-DAT
D4PG, O
trained O
from O
images O

for B-DAT
100,000 O
episodes, O
except O
for O

the B-DAT
finger O
task. O
PlaNet O
surpasses O

the B-DAT
final O
performance O
of O
D4PG O

with B-DAT
a O
relative O
improvement O
of O
26 O

% B-DAT
on O
the O
cheetah O
running O

task. B-DAT
We O
refer O
to O
Table O
1 O
for O
numerical O
results, O
which O
also O

includes B-DAT
the O
performance O
of O
CEM O

planning B-DAT
with O
the O
true O
dynamics O

of B-DAT
the O
simulator O

. B-DAT
Model O
designs O
Figure O
4 O
additionally O

compares B-DAT
design O
choices O
of O
the O

dynamics B-DAT
model. O
We O
train O
PlaNet O

using B-DAT
our O
recurrent O
state-space O
model O

(RSSM), B-DAT
as O
well O
as O
ver O

- B-DAT
sions O
with O
purely O
deterministic O

GRU B-DAT
(Cho O
et O
al., O
2014), O

and B-DAT
purely O
stochastic O
state-space O
model O
( O

SSM). B-DAT
We O
observe O
the O
importance O

of B-DAT
both O
stochastic O
and O
deterministic O

elements B-DAT
in O
the O
transition O
function O

on B-DAT
all O
tasks. O
The O
deterministic O

part B-DAT
allows O
the O
model O
to O

remember B-DAT
information O
over O
many O
time O

steps. B-DAT
The O
stochastic O
component O
is O

even B-DAT
more O
impor- O
tant O
– O

the B-DAT
agent O
does O
not O
learn O

without B-DAT
it. O
This O
could O
be O

because B-DAT
the O
tasks O
are O
stochastic O

from B-DAT
the O
agent’s O
perspective O
due O

to B-DAT
partial O
observability O
of O
the O

initial B-DAT
states. O
The O
noise O
might O

also B-DAT
add O
a O
safety O
margin O

to B-DAT
the O
planning O
objective O
that O

results B-DAT
in O
more O
robust O
action O

sequences. B-DAT
Agent O
designs O
Figure O
5 O
compares O

PlaNet, B-DAT
a O
version O
col- O
lecting O

episodes B-DAT
under O
random O
actions O
rather O

than B-DAT
by O
plan- O
ning, O
and O

a B-DAT
version O
that O
at O
each O

environment B-DAT
step O
selects O
the O
best O

action B-DAT
out O
of O
1000 O
sequences O

rather B-DAT
than O
iteratively O
re- O
fining O

plans B-DAT
via O
CEM. O
We O
observe O

that B-DAT
online O
data O
collection O
helps O

for B-DAT
all O
tasks O
and O
is O

necessary B-DAT
for O
the O
cartpole, O
finger O

, B-DAT
and O
walker O
tasks. O
Iterative O

search B-DAT
for O
action O
sequences O
using O

CEM B-DAT
improves O
performance O
on O
all O

tasks. B-DAT
One O
agent O
all O
tasks O
Figure O

7 B-DAT
in O
the O
appendix O
shows O

the B-DAT
performance O
of O
a O
single O

agent B-DAT
trained O
on O
all O
six O

tasks. B-DAT
The O
agent O
is O
not O

told B-DAT
which O
task O
it O
is O

facing; B-DAT
it O
needs O
to O
infer O

this B-DAT
from O
the O
image O
observations O

. B-DAT
We O
pad O
the O
action O

spaces B-DAT
with O
unused O
elements O
to O

make B-DAT
them O
compatible O
and O
adapt O

Algorithm B-DAT
1 O
to O
collect O
one O

episode B-DAT
of O
each O
task O
every O

C B-DAT
update O
steps. O
We O
use O

the B-DAT
same O
hyper O
parameters O
as O

for B-DAT
the O
main O
experiments O
above. O

The B-DAT
agent O
solves O
all O
tasks O

while B-DAT
learning O
slower O
compared O
to O

individually B-DAT
trained O
agents. O
This O
indicates O

that B-DAT
the O
model O
can O
learn O

to B-DAT
predict O
multiple O
domains, O
regardless O

of B-DAT
the O
conceptually O
different O
visuals. O
6. O
Related O
Work O
Previous O
work O

in B-DAT
model-based O
reinforcement O
learning O
has O

focused B-DAT
on O
planning O
in O
low-dimensional O

state B-DAT
spaces O
(Gal O
et O
al O

., B-DAT
2016; O
Higuera O
et O
al., O
2018 O

; B-DAT
Henaff O
et O
al., O
2018; O
6 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
Cartpole O
Swing O
Up O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Reacher O
Easy O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cheetah O
Run O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Finger O
Spin O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cup O
Catch O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Walker O
Walk O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
PlaNet O
(RSSM) O
Stochastic O
(SSM) O
Deterministic O

(GRU B-DAT

) B-DAT
D4PG O
(100k O
episodes) O
A3C O
(100k O

episodes, B-DAT
proprio O

) B-DAT
Figure O
4: O
Comparison O
of O
PlaNet O

to B-DAT
model-free O
algorithms O
and O
other O

model B-DAT
designs. O
Plots O
show O
test O

performance B-DAT
over O
the O
number O
of O

collected B-DAT
episodes. O
We O
compare O
PlaNet O

using B-DAT
our O
RSSM O
(Section O
3 O

) B-DAT
to O
purely O
deterministic O
(GRU) O

and B-DAT
purely O
stochastic O
models O
(SSM). O

The B-DAT
RNN O
does O
not O
use O

latent B-DAT
overshooting, O
as O
it O
does O

not B-DAT
have O
stochastic O
latents. O
The O

lines B-DAT
show O
medians O
and O
the O

areas B-DAT
show O
percentiles O
5 O
to O
95 O
over O
5 O
seeds O
and O
10 O

trajectories. B-DAT
The O
shaded O
areas O
are O

large B-DAT
on O
two O
of O
the O

tasks B-DAT
due O
to O
the O
sparse O

rewards B-DAT

. B-DAT
Table O
1: O
Comparison O
of O
PlaNet O

to B-DAT
the O
model-free O
algorithms O
A3C O

and B-DAT
D4PG O
reported O
by O
Tassa O

et B-DAT
al. O
(2018). O
The O
training O

curves B-DAT
for O
these O
are O
shown O

as B-DAT
orange O
lines O
in O
Figure O

4 B-DAT
and O
as O
solid O
green O

lines B-DAT
in O
Figure O
6 O
in O

their B-DAT
paper. O
From O
these, O
we O

estimate B-DAT
the O
number O
of O
episodes O

that B-DAT
D4PG O
takes O
to O
achieve O

the B-DAT
final O
performance O
of O
PlaNet O

to B-DAT
estimate O
the O
data O
efficiency O

gain. B-DAT
We O
further O
include O
CEM O

planning B-DAT
(H O
= O
12, O
I O

= B-DAT
10, O
J O
= O
1000,K O

= B-DAT
100) O
with O
the O
true O

simulator B-DAT
instead O
of O
learned O
dynamics O

as B-DAT
an O
estimated O
upper O
bound O

on B-DAT
performance. O
Numbers O
indicate O
mean O

final B-DAT
performance O
over O
5 O
seeds O

and B-DAT
10 O
trajectories O

. B-DAT
Method O
Modality O
Episodes O
C O
ar O

tp B-DAT
ol O
e O
Sw O

in B-DAT
g O
U O
p O

R B-DAT
ea O
ch O
er O

E B-DAT
as O
y O

C B-DAT
he O
et O
ah O

R B-DAT
un O
Fi O
ng O

er B-DAT
Sp O
in O

C B-DAT
up O
C O
at O

ch B-DAT
W O
al O

ke B-DAT
r O
W O
al O

k B-DAT
A3C O
proprioceptive O
100,000 O
558 O
285 O

214 B-DAT
129 O
105 O
311 O
D4PG O

pixels B-DAT
100,000 O
862 O
967 O
524 O

985 B-DAT
980 O
968 O
PlaNet O
(ours O

) B-DAT
pixels O
1,000 O
821 O
832 O
662 O
700 O
930 O
951 O
CEM O

+ B-DAT
true O
simulator O
simulator O
state O

0 B-DAT
850 O
964 O
656 O
825 O

993 B-DAT
994 O

Data B-DAT
efficiency O
gain O
PlaNet O
over O

D4PG B-DAT
(factor) O
250 O
40 O
500+ O
300 O
100 O
90 O

Chua B-DAT
et O
al., O
2018), O
combining O

the B-DAT
benefits O
of O
model-based O
and O

model-free B-DAT
approaches O
(Kalweit O
& O
Boedecker, O
2017 O

; B-DAT
Nagabandi O
et O
al., O
2017; O

Weber B-DAT
et O
al., O
2017; O
Kurutach O

et B-DAT
al., O
2018; O
Buckman O
et O

al., B-DAT
2018; O
Ha O
& O
Schmidhuber, O
2018 O

; B-DAT
Wayne O
et O
al., O
2018; O

Igl B-DAT
et O
al., O
2018; O
Srinivas O

et B-DAT
al., O
2018), O
and O
pure O

video B-DAT
prediction O
without O
planning O
(Oh O

et B-DAT
al., O
2015; O
Krishnan O
et O

al., B-DAT
2015; O
Karl O
et O
al., O
2016 O

; B-DAT
Chiappa O
et O
al., O
2017; O

Babaeizadeh B-DAT
et O
al., O
2017; O
Gemici O

et B-DAT
al., O
2017; O
Denton O
& O

Fergus, B-DAT
2018; O
Buesing O
et O
al., O
2018 O

; B-DAT
Doerr O
et O
al., O
2018; O

Gre- B-DAT
gor O
& O
Besse, O
2018). O
Appendix O

G B-DAT
reviews O
these O
orthogonal O
research O

directions B-DAT
in O
more O
detail O

. B-DAT
Relatively O
few O
works O
have O
demonstrated O

successful B-DAT
plan- O
ning O
from O
pixels O

using B-DAT
learned O
dynamics O
models. O
The O

robotics B-DAT
community O
focuses O
on O
video O

prediction B-DAT
models O
for O
planning O
(Agrawal O

et B-DAT
al., O
2016; O
Finn O

& B-DAT
Levine, O
2017; O
Ebert O
et O

al., B-DAT
2018; O
Zhang O
et O
al O

., B-DAT
2018) O
that O
deal O
with O

the B-DAT
visual O
complexity O
of O
the O

real B-DAT
world O
and O
solve O
tasks O

with B-DAT
7 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
Cartpole O
Swing O
Up O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Reacher O
Easy O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cheetah O
Run O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Finger O
Spin O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cup O
Catch O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Walker O
Walk O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
PlaNet O
Random O
collection O
Random O
shooting O

D4PG B-DAT
(100k O
episodes) O
A3C O
(100k O

episodes, B-DAT
proprio) O
Figure O
5: O
Comparison O
of O
agent O

designs. B-DAT
Plots O
show O
test O
performance O

over B-DAT
the O
number O
of O
collected O

episodes. B-DAT
We O
compare O
PlaNet, O
a O

version B-DAT
that O
collects O
data O
under O

random B-DAT
actions O
(random O
collection), O
and O

a B-DAT
version O
that O
chooses O
the O

best B-DAT
action O
out O
of O
1000 O

sequences B-DAT
at O
each O
environment O
step O

(random B-DAT
shooting) O
without O
iteratively O
refining O

plans B-DAT
via O
CEM. O
The O
lines O

show B-DAT
medians O
and O
the O
areas O

show B-DAT
percentiles O
5 O
to O
95 O

over B-DAT
5 O
seeds O
and O
10 O

trajectories B-DAT

. B-DAT
a O
simple O
gripper, O
such O
as O

grasping B-DAT
or O
pushing O
objects. O
In O

comparison, B-DAT
we O
focus O
on O
simulated O

environments, B-DAT
where O
we O
leverage O
latent O

planning B-DAT
to O
scale O
to O
larger O

state B-DAT
and O
ac- O
tion O
spaces O

, B-DAT
longer O
planning O
horizons, O
as O

well B-DAT
as O
sparse O
reward O
tasks. O

E2C B-DAT
(Watter O
et O
al., O
2015) O

and B-DAT
RCE O
(Banija- O
mali O
et O

al., B-DAT
2017) O
embed O
images O
into O

a B-DAT
latent O
space, O
where O
they O

learn B-DAT
local-linear O
latent O
transitions O
and O

plan B-DAT
for O
actions O
using O
LQR. O

These B-DAT
methods O
balance O
simulated O
cartpoles O

and B-DAT
control O
2-link O
arms O
from O

images, B-DAT
but O
have O
been O
difficult O

to B-DAT
scale O
up. O
We O
lift O

the B-DAT
Markov O
assumption O
of O
these O

models, B-DAT
making O
our O
method O
applicable O

under B-DAT
partial O
observability, O
and O
present O

results B-DAT
on O
more O
challenging O
environments O

that B-DAT
include O
longer O
planning O
horizons, O

contact B-DAT
dynamics, O
and O
sparse O
rewards. O
7. O
Discussion O
We O
present O
PlaNet O

, B-DAT
a O
model-based O
agent O
that O

learns B-DAT
a O
la- O
tent O
dynamics O

model B-DAT
from O
image O
observations O
and O

chooses B-DAT
actions O
by O
fast O
planning O

in B-DAT
latent O
space. O
To O
enable O

accu- B-DAT
rate O
long-term O
predictions, O
we O

design B-DAT
a O
model O
with O
both O

stochastic B-DAT
and O
deterministic O
paths. O
We O

show B-DAT
that O
our O
agent O
succeeds O

at B-DAT
several O
continuous O
control O
tasks O

from B-DAT
image O
observations, O
reaching O
performance O

that B-DAT
is O
comparable O
to O
the O

best B-DAT
model-free O
algorithms O
while O
using O
200 O

× B-DAT
fewer O
episodes O
and O
similar O

or B-DAT
less O
computation O
time. O
The O

results B-DAT
show O
that O
learning O
latent O
dynamics O

models B-DAT
for O
planning O
in O
image O

domains B-DAT
is O
a O
promising O
approach O

. B-DAT
Directions O
for O
future O
work O
include O

learning B-DAT
temporal O
ab- O
straction O
instead O

of B-DAT
using O
a O
fixed O
action O

repeat, B-DAT
possibly O
through O
hierarchical O
models O

. B-DAT
To O
further O
improve O
final O

per- B-DAT
formance, O
one O
could O
learn O

a B-DAT
value O
function O
to O
approximate O

the B-DAT
sum O
of O
rewards O
beyond O

the B-DAT
planning O
horizon. O
Moreover, O
gradient-based O

planning B-DAT
could O
increase O
the O
computational O

efficiency B-DAT
of O
the O
agent O
and O

learning B-DAT
representations O
without O
reconstruction O
could O

help B-DAT
to O
solve O
tasks O
with O

higher B-DAT
visual O
diversity. O
Our O
work O

provides B-DAT
a O
starting O
point O
for O

multi-task B-DAT
control O
by O
sharing O
the O

dynamics B-DAT
model. O
Acknowledgements O
We O
thank O
Jacob O
Buckman O

, B-DAT
Nicolas O
Heess, O
John O
Schulman, O

Rishabh B-DAT
Agarwal, O
Silviu O
Pitis, O
Mohammad O

Norouzi, B-DAT
George O
Tucker, O
David O
Duvenaud, O

Shane B-DAT
Gu, O
Chelsea O
Finn, O
Steven O

Bohez, B-DAT
Jimmy O
Ba, O
Stephanie O
Chan, O

and B-DAT
Jenny O
Liu O
for O
helpful O

discussions. B-DAT
8 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
References O

Agrawal, B-DAT
P., O
Nair, O
A. O
V., O

Abbeel, B-DAT
P., O
Malik, O
J., O
and O

Levine, B-DAT
S. O
Learning O
to O
poke O

by B-DAT
poking: O
Experiential O
learning O
of O

intuitive B-DAT
physics. O
In O
Advances O
in O

Neural B-DAT
Information O
Processing O
Systems, O
pp. O
5074 O

–5082, B-DAT
2016. O
Amos, O
B., O
Dinh, O
L., O
Cabi O

, B-DAT
S., O
Rothörl, O
T., O
Muldal, O

A., B-DAT
Erez, O
T., O
Tassa, O
Y., O

de B-DAT
Freitas, O
N., O
and O
Denil, O

M. B-DAT
Learning O
awareness O
models. O
In O

International B-DAT
Conference O
on O
Learn- O
ing O

Representations, B-DAT
2018. O
Babaeizadeh, O
M., O
Finn, O
C., O
Erhan O

, B-DAT
D., O
Campbell, O
R. O
H., O

and B-DAT
Levine, O
S. O
Stochastic O
variational O

video B-DAT
prediction. O
arXiv O
preprint O
arXiv:1710.11252, O
2017 O

. B-DAT
Banijamali, O
E., O
Shu, O
R., O
Ghavamzadeh O

, B-DAT
M., O
Bui, O
H., O
and O

Ghodsi, B-DAT
A. O
Robust O
locally-linear O
controllable O

embedding. B-DAT
arXiv O
preprint O
arXiv:1710.05373, O
2017. O
Barth-Maron, O
G., O
Hoffman, O
M. O
W O

., B-DAT
Budden, O
D., O
Dabney, O
W., O

Horgan, B-DAT
D., O
Muldal, O
A., O
Heess, O

N., B-DAT
and O
Lillicrap, O
T. O
Distributed O

distributional B-DAT
deterministic O
policy O
gradients. O
arXiv O

preprint B-DAT
arXiv:1804.08617, O
2018. O
Bengio, O
S., O
Vinyals, O
O., O
Jaitly O

, B-DAT
N., O
and O
Shazeer, O
N. O

Sched- B-DAT
uled O
sampling O
for O
sequence O

prediction B-DAT
with O
recurrent O
neu- O
ral O

networks. B-DAT
In O
Advances O
in O
Neural O

Information B-DAT
Process- O
ing O
Systems, O
pp. O
1171 O

–1179, B-DAT
2015. O
Buckman, O
J., O
Hafner, O
D., O
Tucker O

, B-DAT
G., O
Brevdo, O
E., O
and O

Lee, B-DAT
H. O
Sample-efficient O
reinforcement O
learning O

with B-DAT
stochastic O
ensemble O
value O
expansion. O

arXiv B-DAT
preprint O
arXiv:1807.01675, O
2018. O
Buesing, O
L., O
Weber, O
T., O
Racaniere O

, B-DAT
S., O
Eslami, O
S., O
Rezende, O

D., B-DAT
Reichert, O
D. O
P., O
Viola, O

F., B-DAT
Besse, O
F., O
Gregor, O
K., O

Hassabis, B-DAT
D., O
et O
al. O
Learning O

and B-DAT
querying O
fast O
gener- O
ative O

models B-DAT
for O
reinforcement O
learning. O
arXiv O

preprint B-DAT
arXiv:1802.03006, O
2018. O
Chiappa, O
S., O
Racaniere, O
S., O
Wierstra O

, B-DAT
D., O
and O
Mohamed, O
S. O

Recurrent B-DAT
environment O
simulators. O
arXiv O
preprint O

arXiv:1704.02254, B-DAT
2017. O
Cho, O
K., O
Van O
Merriënboer, O
B O

., B-DAT
Gulcehre, O
C., O
Bahdanau, O
D., O

Bougares, B-DAT
F., O
Schwenk, O
H., O
and O

Bengio, B-DAT
Y. O
Learn- O
ing O
phrase O

representations B-DAT
using O
rnn O
encoder-decoder O
for O

statistical B-DAT
machine O
translation. O
arXiv O
preprint O

arXiv:1406.1078, B-DAT
2014. O
Chua, O
K., O
Calandra, O
R., O
McAllister O

, B-DAT
R., O
and O
Levine, O
S. O

Deep B-DAT
reinforcement O
learning O
in O
a O

handful B-DAT
of O
trials O
us- O
ing O

probabilistic B-DAT
dynamics O
models. O
arXiv O
preprint O

arXiv:1805.12114, B-DAT
2018. O
Chung, O
J., O
Kastner, O
K., O
Dinh O

, B-DAT
L., O
Goel, O
K., O
Courville, O

A. B-DAT
C., O
and O
Bengio, O
Y. O

A B-DAT
recurrent O
latent O
variable O
model O

for B-DAT
sequential O
data. O
In O
Advances O

in B-DAT
neural O
information O
pro- O
cessing O

systems, B-DAT
pp. O
2980–2988, O
2015. O
Clevert, O
D.-A., O
Unterthiner, O
T., O
and O

Hochreiter, B-DAT
S. O
Fast O
and O
accurate O

deep B-DAT
network O
learning O
by O
exponential O

linear B-DAT
units O
(elus). O
arXiv O
preprint O

arXiv:1511.07289, B-DAT
2015 O

. B-DAT
Deisenroth, O
M. O
and O
Rasmussen, O
C O

. B-DAT
E. O
Pilco: O
A O
model-based O

and B-DAT
data-efficient O
approach O
to O
policy O

search. B-DAT
In O
Proceed- O
ings O
of O

the B-DAT
28th O
International O
Conference O
on O

machine B-DAT
learning O
(ICML-11), O
pp. O
465–472, O
2011 O

. B-DAT
Denton, O
E. O
and O
Fergus, O
R O

. B-DAT
Stochastic O
video O
generation O
with O

a B-DAT
learned O
prior. O
arXiv O
preprint O

arXiv:1802.07687, B-DAT
2018. O
Dillon, O
J. O
V., O
Langmore, O
I O

., B-DAT
Tran, O
D., O
Brevdo, O
E., O

Vasudevan, B-DAT
S., O
Moore, O
D., O
Patton, O

B., B-DAT
Alemi, O
A., O
Hoffman, O
M., O

and B-DAT
Saurous, O
R. O
A. O
Tensorflow O

distributions. B-DAT
arXiv O
preprint O
arXiv:1711.10604, O
2017. O
Doerr, O
A., O
Daniel, O
C., O
Schiegg O

, B-DAT
M., O
Nguyen-Tuong, O
D., O
Schaal, O

S., B-DAT
Toussaint, O
M., O
and O
Trimpe, O

S. B-DAT
Proba- O
bilistic O
recurrent O
state-space O

models. B-DAT
arXiv O
preprint O
arXiv:1801.10395, O
2018. O
Ebert, O
F., O
Finn, O
C., O
Dasari O

, B-DAT
S., O
Xie, O
A., O
Lee, O

A., B-DAT
and O
Levine, O
S. O
Visual O

foresight: B-DAT
Model-based O
deep O
reinforcement O
learning O

for B-DAT
vision-based O
robotic O
control. O
arXiv O

preprint B-DAT
arXiv:1812.00568, O
2018. O
Finn, O
C. O
and O
Levine, O
S O

. B-DAT
Deep O
visual O
foresight O
for O

planning B-DAT
robot O
motion. O
In O
Robotics O

and B-DAT
Automation O
(ICRA), O
2017 O
IEEE O

International B-DAT
Conference O
on, O
pp. O
2786–2793. O

IEEE, B-DAT
2017. O
Gal, O
Y., O
McAllister, O
R., O
and O

Rasmussen, B-DAT
C. O
E. O
Improving O
pilco O

with B-DAT
bayesian O
neural O
network O
dynamics O

models. B-DAT
In O
Data-Efficient O
Machine O
Learning O

workshop, B-DAT
ICML, O
2016 O

. B-DAT
Gemici, O
M., O
Hung, O
C.-C., O
Santoro O

, B-DAT
A., O
Wayne, O
G., O
Mo- O

hamed, B-DAT
S., O
Rezende, O
D. O
J., O

Amos, B-DAT
D., O
and O
Lillicrap, O
T. O

Generative B-DAT
temporal O
models O
with O
memory. O

arXiv B-DAT
preprint O
arXiv:1702.04649, O
2017. O
Gregor, O
K. O
and O
Besse, O
F O

. B-DAT
Temporal O
difference O
variational O
auto-encoder. O

arXiv B-DAT
preprint O
arXiv:1806.03107, O
2018. O
Ha, O
D. O
and O
Schmidhuber, O
J O

. B-DAT
World O
models. O
arXiv O
preprint O

arXiv:1803.10122, B-DAT
2018. O
Henaff, O
M., O
Whitney, O
W. O
F O

., B-DAT
and O
LeCun, O
Y. O
Model-based O

planning B-DAT
with O
discrete O
and O
continuous O

actions. B-DAT
arXiv O
preprint O
arXiv:1705.07177, O
2018. O
9 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
Higgins, O
I., O
Matthey, O
L., O
Pal O

, B-DAT
A., O
Burgess, O
C., O
Glorot, O

X., B-DAT
Botvinick, O
M., O
Mohamed, O
S., O

and B-DAT
Lerchner, O
A. O
beta- O
vae: O

Learning B-DAT
basic O
visual O
concepts O
with O

a B-DAT
constrained O
variational O
framework. O
In O

International B-DAT
Conference O
on O
Learning O
Representations, O
2016 O

. B-DAT
Higuera, O
J. O
C. O
G., O
Meger O

, B-DAT
D., O
and O
Dudek, O
G. O

Synthesizing B-DAT
neural O
network O
controllers O
with O

probabilistic B-DAT
model O
based O
reinforcement O
learning. O

arXiv B-DAT
preprint O
arXiv:1803.02291, O
2018. O
Igl, O
M., O
Zintgraf, O
L., O
Le O

, B-DAT
T. O
A., O
Wood, O
F., O

and B-DAT
Whiteson, O
S. O
Deep O
variational O

reinforcement B-DAT
learning O
for O
pomdps. O
arXiv O

preprint B-DAT
arXiv:1806.02426, O
2018. O
Kalchbrenner, O
N., O
Oord, O
A. O
v O

. B-DAT
d., O
Simonyan, O
K., O
Danihelka, O

I., B-DAT
Vinyals, O
O., O
Graves, O
A., O

and B-DAT
Kavukcuoglu, O
K. O
Video O
pixel O

networks. B-DAT
arXiv O
preprint O
arXiv:1610.00527, O
2016. O
Kalweit, O
G. O
and O
Boedecker, O
J O

. B-DAT
Uncertainty-driven O
imagi- O
nation O
for O

continuous B-DAT
deep O
reinforcement O
learning. O
In O

Conference B-DAT
on O
Robot O
Learning, O
pp. O
195 O

–206, B-DAT
2017. O
Karl, O
M., O
Soelch, O
M., O
Bayer O

, B-DAT
J., O
and O
van O
der O

Smagt, B-DAT
P. O
Deep O
variational O
bayes O

filters: B-DAT
Unsupervised O
learning O
of O
state O

space B-DAT
models O
from O
raw O
data. O

arXiv B-DAT
preprint O
arXiv:1605.06432, O
2016. O
Kingma, O
D. O
P. O
and O
Ba O

, B-DAT
J. O
Adam: O
A O
method O

for B-DAT
stochastic O
optimization. O
arXiv O
preprint O

arXiv:1412.6980, B-DAT
2014. O
Kingma, O
D. O
P. O
and O
Dhariwal O

, B-DAT
P. O
Glow: O
Generative O
flow O

with B-DAT
invertible O
1x1 O
convolutions. O
arXiv O

preprint B-DAT
arXiv:1807.03039, O
2018. O
Kingma, O
D. O
P. O
and O
Welling O

, B-DAT
M. O
Auto-encoding O
variational O
bayes. O

arXiv B-DAT
preprint O
arXiv:1312.6114, O
2013. O
Krishnan, O
R. O
G., O
Shalit, O
U O

., B-DAT
and O
Sontag, O
D. O
Deep O

kalman B-DAT
filters. O
arXiv O
preprint O
arXiv:1511.05121, O
2015 O

. B-DAT
Krishnan, O
R. O
G., O
Shalit, O
U O

., B-DAT
and O
Sontag, O
D. O
Structured O

inference B-DAT
networks O
for O
nonlinear O
state O

space B-DAT
models. O
In O
AAAI, O
pp. O
2101 O

–2109, B-DAT
2017. O
Kurutach, O
T., O
Clavera, O
I., O
Duan O

, B-DAT
Y., O
Tamar, O
A., O
and O

Abbeel, B-DAT
P. O
Model-ensemble O
trust-region O
policy O

optimization. B-DAT
arXiv O
preprint O
arXiv:1802.10592, O
2018. O
Lamb, O
A. O
M., O
GOYAL, O
A O

. B-DAT
G. O
A. O
P., O
Zhang, O

Y., B-DAT
Zhang, O
S., O
Courville, O
A. O

C., B-DAT
and O
Bengio, O
Y. O
Professor O

forcing: B-DAT
A O
new O
algorithm O
for O

training B-DAT
recurrent O
networks. O
In O
Advances O

In B-DAT
Neural O
Information O
Processing O
Systems, O

pp. B-DAT
4601–4609, O
2016. O
Mathieu, O
M., O
Couprie, O
C., O
and O

LeCun, B-DAT
Y. O
Deep O
multi- O
scale O

video B-DAT
prediction O
beyond O
mean O
square O

error. B-DAT
arXiv O
preprint O
arXiv:1511.05440, O
2015 O

. B-DAT
Mnih, O
V., O
Kavukcuoglu, O
K., O
Silver O

, B-DAT
D., O
Rusu, O
A. O
A., O

Veness, B-DAT
J., O
Bellemare, O
M. O
G., O

Graves, B-DAT
A., O
Riedmiller, O
M., O
Fidje- O

land, B-DAT
A. O
K., O
Ostrovski, O
G., O

et B-DAT
al. O
Human-level O
control O
through O

deep B-DAT
reinforcement O
learning. O
Nature, O
518(7540): O
529, O
2015 O

. B-DAT
Mnih, O
V., O
Badia, O
A. O
P O

., B-DAT
Mirza, O
M., O
Graves, O
A., O

Lillicrap, B-DAT
T., O
Harley, O
T., O
Silver, O

D., B-DAT
and O
Kavukcuoglu, O
K. O
Asyn- O

chronous B-DAT
methods O
for O
deep O
reinforcement O

learning. B-DAT
In O
International O
Conference O
on O

Machine B-DAT
Learning, O
pp. O
1928– O
1937, O
2016 O

. B-DAT
Moerland, O
T. O
M., O
Broekens, O
J O

., B-DAT
and O
Jonker, O
C. O
M. O

Learning B-DAT
multimodal O
transition O
dynamics O
for O

model-based B-DAT
rein- O
forcement O
learning. O
arXiv O

preprint B-DAT
arXiv:1705.00470, O
2017. O
Moravčík, O
M., O
Schmid, O
M., O
Burch O

, B-DAT
N., O
Lisỳ, O
V., O
Morrill, O

D., B-DAT
Bard, O
N., O
Davis, O
T., O

Waugh, B-DAT
K., O
Johanson, O
M., O
and O

Bowl- B-DAT
ing, O
M. O
Deepstack: O
Expert-level O

artificial B-DAT
intelligence O
in O
heads-up O
no-limit O

poker. B-DAT
Science, O
356(6337):508–513, O
2017. O
Nagabandi, O
A., O
Kahn, O
G., O
Fearing O

, B-DAT
R. O
S., O
and O
Levine, O

S. B-DAT
Neural O
network O
dynamics O
for O

model-based B-DAT
deep O
rein- O
forcement O
learning O

with B-DAT
model-free O
fine-tuning. O
arXiv O
preprint O

arXiv:1708.02596, B-DAT
2017. O
Nair, O
V. O
and O
Hinton, O
G O

. B-DAT
E. O
Rectified O
linear O
units O

improve B-DAT
restricted O
boltzmann O
machines. O
In O

Proceedings B-DAT
of O
the O
27th O
international O

conference B-DAT
on O
machine O
learning O
(ICML-10), O

pp. B-DAT
807–814, O
2010. O
Oh, O
J., O
Guo, O
X., O
Lee O

, B-DAT
H., O
Lewis, O
R. O
L., O

and B-DAT
Singh, O
S. O
Action- O
conditional O

video B-DAT
prediction O
using O
deep O
networks O

in B-DAT
atari O
games. O
In O
Advances O

in B-DAT
Neural O
Information O
Processing O
Systems, O

pp. B-DAT
2863–2871, O
2015. O
Rezende, O
D. O
J., O
Mohamed, O
S O

., B-DAT
and O
Wierstra, O
D. O
Stochastic O

backpropagation B-DAT
and O
approximate O
inference O
in O

deep B-DAT
gen- O
erative O
models. O
arXiv O

preprint B-DAT
arXiv:1401.4082, O
2014. O
Richards, O
A. O
G. O
Robust O
constrained O

model B-DAT
predictive O
control. O
PhD O
thesis O

, B-DAT
Massachusetts O
Institute O
of O
Technology, O
2005 O

. B-DAT
Rubinstein, O
R. O
Y. O
Optimization O
of O

computer B-DAT
simulation O
mod- O
els O
with O

rare B-DAT
events. O
European O
Journal O
of O

Operational B-DAT
Research, O
99(1):89–112, O
1997 O

. B-DAT
Silver, O
D., O
Schrittwieser, O
J., O
Simonyan O

, B-DAT
K., O
Antonoglou, O
I., O
Huang, O

A., B-DAT
Guez, O
A., O
Hubert, O
T., O

Baker, B-DAT
L., O
Lai, O
M., O
Bolton, O

A., B-DAT
et O
al. O
Mastering O
the O

game B-DAT
of O
go O
without O
human O

knowledge. B-DAT
Nature, O
550(7676):354, O
2017. O
Srinivas, O
A., O
Jabri, O
A., O
Abbeel O

, B-DAT
P., O
Levine, O
S., O
and O

Finn, B-DAT
C. O
Universal O
planning O
networks. O

arXiv B-DAT
preprint O
arXiv:1804.00645, O
2018. O
10 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
Talvitie, O
E. O
Model O
regularization O
for O

stable B-DAT
sample O
rollouts. O
In O
UAI O

, B-DAT
pp. O
780–789, O
2014. O
Tassa, O
Y., O
Erez, O
T., O
and O

Todorov, B-DAT
E. O
Synthesis O
and O
stabi O

- B-DAT
lization O
of O
complex O
behaviors O

through B-DAT
online O
trajectory O
optimization. O
In O

Intelligent B-DAT
Robots O
and O
Systems O
(IROS), O
2012 O
IEEE/RSJ O
International O
Conference O
on, O
pp O

. B-DAT
4906– O
4913. O
IEEE, O
2012. O
Tassa, O
Y., O
Doron, O
Y., O
Muldal O

, B-DAT
A., O
Erez, O
T., O
Li, O

Y., B-DAT
Casas, O
D. O
d. O
L., O

Budden, B-DAT
D., O
Abdolmaleki, O
A., O
Merel, O

J., B-DAT
Lefrancq, O
A., O
et O
al. O

Deepmind B-DAT
control O
suite. O
arXiv O
preprint O

arXiv:1801.00690, B-DAT
2018. O
van O
den O
Oord, O
A., O
Vinyals O

, B-DAT
O., O
et O
al. O
Neural O

discrete B-DAT
repre- O
sentation O
learning. O
In O

Advances B-DAT
in O
Neural O
Information O
Processing O

Systems, B-DAT
pp. O
6309–6318, O
2017. O
Venkatraman, O
A., O
Hebert, O
M., O
and O

Bagnell, B-DAT
J. O
A. O
Improving O
multi-step O

prediction B-DAT
of O
learned O
time O
series O

models. B-DAT
In O
AAAI, O
pp. O
3024–3030 O

, B-DAT
2015. O
Vondrick, O
C., O
Pirsiavash, O
H., O
and O

Torralba, B-DAT
A. O
Generating O
videos O
with O

scene B-DAT
dynamics. O
In O
Advances O
In O

Neural B-DAT
Information O
Processing O
Systems, O
2016 O

. B-DAT
Watter, O
M., O
Springenberg, O
J., O
Boedecker O

, B-DAT
J., O
and O
Riedmiller, O
M. O

Embed B-DAT
to O
control: O
A O
locally O

linear B-DAT
latent O
dynamics O
model O
for O

control B-DAT
from O
raw O
images. O
In O

Advances B-DAT
in O
neural O
information O
processing O

systems, B-DAT
pp. O
2746–2754, O
2015. O
Wayne, O
G., O
Hung, O
C.-C., O
Amos O

, B-DAT
D., O
Mirza, O
M., O
Ahuja, O

A., B-DAT
Grabska-Barwinska, O
A., O
Rae, O
J., O

Mirowski, B-DAT
P., O
Leibo, O
J. O
Z., O

Santoro, B-DAT
A., O
et O
al. O
Unsupervised O

predic- B-DAT
tive O
memory O
in O
a O

goal-directed B-DAT
agent. O
arXiv O
preprint O
arXiv:1803.10760, O
2018 O

. B-DAT
Weber, O
T., O
Racanière, O
S., O
Reichert O

, B-DAT
D. O
P., O
Buesing, O
L., O

Guez, B-DAT
A., O
Rezende, O
D. O
J., O

Badia, B-DAT
A. O
P., O
Vinyals, O
O., O

Heess, B-DAT
N., O
Li, O
Y., O
et O

al. B-DAT
Imagination-augmented O
agents O
for O
deep O

reinforcement B-DAT
learning. O
arXiv O
preprint O
arXiv:1707.06203, O
2017 O

. B-DAT
Zhang, O
M., O
Vikram, O
S., O
Smith O

, B-DAT
L., O
Abbeel, O
P., O
Johnson, O

M., B-DAT
and O
Levine, O
S. O
SOLAR: O

deep B-DAT
structured O
representations O
for O
model-based O

reinforcement B-DAT
learning. O
arXiv O
preprint O
arXiv:1808.09105, O
2018 O

. B-DAT
11 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
A. O
Hyper O
Parameters O
We O
use O

the B-DAT
convolutional O
and O
deconvolutional O
networks O

from B-DAT
Ha O
& O
Schmidhuber O
(2018 O

), B-DAT
a O
GRU O
(Cho O
et O

al., B-DAT
2014) O
with O
200 O
units O

as B-DAT
deterministic O
path O
in O
the O

dynamics B-DAT
model, O
and O
implement O
all O

other B-DAT
functions O
as O
two O
fully O

connected B-DAT
layers O
of O
size O
200 O

with B-DAT
ReLU O
activations O
(Nair O
& O

Hinton, B-DAT
2010). O
Distributions O
in O
latent O

space B-DAT
are O
30-dimensional O
diagonal O
Gaussians O

with B-DAT
predicted O
mean O
and O
standard O

deviation. B-DAT
We O
pre-process O
images O
by O
reducing O

the B-DAT
bit O
depth O
to O
5 O

bits B-DAT
as O
in O
Kingma O

& B-DAT
Dhariwal O
(2018). O
The O
model O

is B-DAT
trained O
using O
the O
Adam O

optimizer B-DAT
(Kingma O
& O
Ba, O
2014 O

) B-DAT
with O
a O
learning O
rate O

of B-DAT
10−3, O
� O
= O
10−4, O

and B-DAT
gradient O
clipping O
norm O
of O
1000 O
on O
batches O
of O
B O

= B-DAT
50 O
sequence O
chunks O
of O

length B-DAT
L O
= O
50. O
We O

do B-DAT
not O
scale O
the O
KL O

divergence B-DAT
terms O
relatively O
to O
the O

reconstruction B-DAT
terms O
but O
grant O
the O

model B-DAT
3 O
free O
nats O
by O

clipping B-DAT
the O
divergence O
loss O
below O

this B-DAT
value. O
In O
a O
previous O

version B-DAT
of O
the O
agent, O
we O

used B-DAT
latent O
overshooting O
and O
an O

additional B-DAT
fixed O
global O
prior, O
but O

we B-DAT
found O
this O
to O
not O

be B-DAT
necessary O

. B-DAT
For O
planning, O
we O
use O
CEM O

with B-DAT
horizon O
length O
H O

= B-DAT
12, O
optimization O
iterations O
I O

= B-DAT
10, O
candidate O
samples O
J O

= B-DAT
1000, O
and O
refitting O
to O

the B-DAT
best O
K O
= O
100 O

. B-DAT
We O
start O
from O

S B-DAT
= O
5 O
seed O
episodes O

with B-DAT
random O
actions O
and O
collect O

another B-DAT
episode O
every O
C O
= O
100 O
update O
steps O
under O

� B-DAT
∼ O
Normal(0, O
0.3) O
action O

noise. B-DAT
The O
action O
repeat O
differs O

between B-DAT
domains: O
cartpole O
(R O

= B-DAT
8), O
reacher O
(R O

= B-DAT
4), O
cheetah O
(R O

= B-DAT
4), O
finger O
(R O

= B-DAT
2), O
cup O
(R O

= B-DAT
4), O
walker O
(R O

= B-DAT
2). O
We O
found O
important O

hyper B-DAT
parameters O
to O
be O
the O

action B-DAT
repeat, O
the O
KL-divergence O
scales O

β, B-DAT
and O
the O
learning O
rate O

. B-DAT
B. O
Planning O
Algorithm O

Algorithm B-DAT
2: O
Latent O
planning O
with O

CEM B-DAT
Input O
: O
H O
Planning O
horizon O

distance B-DAT
I O
Optimization O
iterations O
J O

Candidates B-DAT
per O
iteration O
K O
Number O

of B-DAT
top O
candidates O
to O
fit O

q(st B-DAT
| O
o≤t, O
a<t) O
Current O

state B-DAT
belief O
p(st O
| O
st−1, O

at−1) B-DAT
Transition O
model O
p(rt O
| O

st) B-DAT
Reward O
model O
1 O
Initialize O
factorized O
belief O
over O

action B-DAT
sequences O
q(at:t+H)← O
Normal(0, O
I O

). B-DAT
2 O
for O
optimization O
iteration O

i B-DAT
= O
1..I O
do O
// O
Evaluate O
J O
action O
sequences O

from B-DAT
the O
current O
belief. O
3 O

for B-DAT
candidate O
action O
sequence O
j O

= B-DAT
1..J O
do O
4 O
a O

j) B-DAT
t:t+H O
∼ O
q(at:t+H) O
5 O
s O
(j) O
t:t+H+1 O

∼ B-DAT
q(st O
| O
o1:t, O
a1:t−1 O

) B-DAT
∏t+H+1 O
τ=t+1 O
p(sτ O
| O
sτ−1 O

, B-DAT
a O
(j) O
τ−1 O

) B-DAT
6 O
R(j) O
= O
∑t+H+1 O
τ=t+1 O

E[p(rτ B-DAT
| O
s O

j) B-DAT
τ O
)] O
// O
Re-fit O
belief O
to O
the O

K B-DAT
best O
action O
sequences O

. B-DAT
7 O
K O
← O
argsort({R(j)}Jj=1)1:K O
8 O

µt:t+H B-DAT

1 B-DAT
K O
∑ O
k∈K O
a O

k) B-DAT
t:t+H O
, O
σt:t+H O
= O
1 O
K−1 O

k∈K B-DAT
|a O
(k) O
t:t+H O
− O
µt:t+H O

9 B-DAT
q(at:t+H)← O
Normal(µt:t+H O
, O
σ2t:t+H O

I) B-DAT
10 O
return O
first O
action O

mean B-DAT
µt. O
12 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
C. O
Multi-Task O
Learning O
Average O
over O

tasks B-DAT

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Figure O
6: O
We O
compare O
a O

single B-DAT
PlaNet O
agent O
trained O
on O

all B-DAT
tasks O
to O
individual O
PlaNet O

agents. B-DAT
The O
plot O
shows O
test O

performance B-DAT
over O
the O
number O
of O

episodes B-DAT
collected O
for O
each O
task O

. B-DAT
The O
single O
agent O
learns O

to B-DAT
solve O
all O
the O
tasks O

while B-DAT
learning O
more O
slowly O
compared O

to B-DAT
the O
individual O
agents. O
The O

lines B-DAT
show O
mean O
and O
one O

standard B-DAT
deviation O
over O
6 O
tasks, O
5 O
seeds, O
and O
10 O
trajectories O

. B-DAT
Cartpole O
Swing O
Up O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Reacher O
Easy O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cheetah O
Run O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Finger O
Spin O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cup O
Catch O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Walker O
Walk O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Separate O
agents O
Single O
agent O

D4PG B-DAT
(100k O
episodes) O
A3C O
(100k O

episodes, B-DAT
proprio) O
Figure O
7: O
Per-task O
performance O
of O

a B-DAT
single O
PlaNet O
agent O
trained O

on B-DAT
the O
six O
tasks. O
Plots O

show B-DAT
test O
performance O
over O
the O

number B-DAT
of O
episodes O
collected O
per O

task. B-DAT
The O
agent O
is O
not O

told B-DAT
which O
task O
it O
is O

solving B-DAT
and O
it O
needs O
to O

infer B-DAT
this O
from O
the O
image O

observations. B-DAT
The O
agent O
learns O
to O

distinguish B-DAT
the O
tasks O
and O
solve O

them B-DAT
with O
just O
a O
moderate O

slowdown B-DAT
in O
learning. O
The O
lines O

show B-DAT
medians O
and O
the O
areas O

show B-DAT
percentiles O
5 O
to O
95 O

over B-DAT
4 O
seeds O
and O
10 O

trajectories B-DAT

. B-DAT
13 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
D. O
Latent O
Overshooting O
Cartpole O
Swing O

Up B-DAT

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Reacher O
Easy O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cheetah O
Run O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Finger O
Spin O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cup O
Catch O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Walker O
Walk O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
RSSM O
RSSM O
+ O
Overshooting O

DRNN B-DAT
DRNN O
+ O
Overshooting O
D4PG O
(100k O
episodes) O
A3C O
(100k O

episodes, B-DAT
proprio O

) B-DAT
Figure O
8: O
We O
compare O
the O

standard B-DAT
variational O
objective O
with O
latent O

overshooting B-DAT
on O
our O
proposed O
RSSM O

and B-DAT
another O
model O
called O
DRNN O

that B-DAT
uses O
two O
RNNs O
as O

encoder B-DAT
and O
decoder O
with O
a O

stochastic B-DAT
state O
sequence O
in O
between O

. B-DAT
Latent O
overshooting O
can O
substantially O

improve B-DAT
the O
performance O
of O
the O

DRNN B-DAT
and O
other O
models O
we O

have B-DAT
experimented O
with O
(not O
shown), O

but B-DAT
slightly O
reduces O
performance O
of O

our B-DAT
RSSM. O
The O
lines O
show O

medians B-DAT
and O
the O
areas O
show O

percentiles B-DAT
5 O
to O
95 O
over O
5 O
seeds O
and O
10 O
trajectories O

. B-DAT
14 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
E. O
Activation O
Function O
Cartpole O
Swing O

Up B-DAT

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Reacher O
Easy O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cheetah O
Run O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Finger O
Spin O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cup O
Catch O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Walker O
Walk O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
PlaNet O
(ReLU) O
PlaNet O
(ELU O

) B-DAT
SSM O
(ReLU) O
SSM O
(ELU O

) B-DAT
D4PG O
(100k O
episodes) O
A3C O
(100k O

episodes, B-DAT
proprio O

) B-DAT
Figure O
9: O
Comparison O
of O
hard O

ReLU B-DAT
(Nair O
& O
Hinton, O
2010 O

) B-DAT
and O
smooth O
ELU O
(Clevert O

et B-DAT
al., O
2015) O
activation O
functions. O

We B-DAT
find O
that O
smooth O
activations O

help B-DAT
improve O
performance O
of O
the O

purely B-DAT
stochastic O
model O
(and O
the O

purely B-DAT
deterministic O
model; O
not O
shown) O

while B-DAT
our O
proposed O
RSSM O
is O

robust B-DAT
to O
the O
choice O
of O

activation B-DAT
function. O
The O
lines O
show O

medians B-DAT
and O
the O
areas O
show O

percentiles B-DAT
5 O
to O
95 O
over O
5 O
seeds O
and O
10 O
trajectories O

. B-DAT
15 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
F. O
Bound O
Derivations O
One-step O
predictive O

distribution B-DAT
The O
variational O
bound O
for O

latent B-DAT
dynamics O
models O
p(o1:T O

, B-DAT
s1:T O
| O
a1:T O

t B-DAT
p(st O
| O
st−1, O
at−1)p(ot O
| O
st) O
and O

a B-DAT
variational O
posterior O
q(s1:T O

| B-DAT
o1:T O
, O
a1:T O

) B-DAT
= O
∏ O
t O
q(st O

| B-DAT
o≤t, O
a<t) O
follows O
from O

importance B-DAT

weighting B-DAT
and O
Jensen’s O
inequality O
as O

shown, B-DAT
ln O
p(o1:T O
| O
a1:T O

) B-DAT
, O
ln O
Ep(s1:T O
|a1:T O

) B-DAT
[ O
T∏ O
t=1 O

p(ot B-DAT
| O
st) O
] O
= O
lnEq(s1:T O
|o1:T O
,a1:T O

T∏ B-DAT
t=1 O
p(ot O
| O
st)p(st O
| O
st−1 O

, B-DAT
at−1)/q(st O
| O
o≤t, O

a<t) B-DAT
] O
≥ O
Eq(s1:T O
|o1:T O
,a1:T O

) B-DAT
[ O
T∑ O
t=1 O

ln B-DAT
p(ot O
| O
st) O
+ O

ln B-DAT
p(st O
| O
st−1, O
at−1)− O

ln B-DAT
q(st O
| O
o≤t, O

T∑ B-DAT
t=1 O
( O
E O
q(st|o≤t,a<t) O
[ln O
p(ot O

| B-DAT
st O

)] B-DAT
reconstruction O

E B-DAT
q(st−1|o≤t−1,a<a−1 O
) O
[ O
KL[q(st O
| O
o≤t, O
a<t O

) B-DAT
‖ O
p(st O
| O
st−1, O

at−1)] B-DAT
] O
complexity O

8) B-DAT
Multi-step O
predictive O
distribution O
The O
variational O

bound B-DAT
on O
the O
d-step O
predictive O

distribution B-DAT
pd(o1:T O
, O
s1:T O

| B-DAT
a1:T O
) O
=∏ O
t O

p(st B-DAT
| O
st−d, O
at−1)p(ot O

| B-DAT
st) O
and O
a O
variational O

posterior B-DAT
q(s1:T O
| O
o1:T O

, B-DAT
a1:T O

t B-DAT
q(st O
| O
o≤t, O
a<t) O

follows B-DAT
anal- O
ogously. O
The O
second O
bound O
comes O

from B-DAT
moving O
the O
log O
inside O

the B-DAT
multi-step O
priors, O
which O
satisfy O

the B-DAT
recursion O
p(st O
| O
st−d O

, B-DAT
at−d−1:t−1) O
= O
Ep(st−1|st−d,at−d−1:t−2)[p(st O
| O

st−1, B-DAT
at−1)]. O
ln O
pd(o1:T O
| O
a1:T O

) B-DAT
, O
ln O
Epd(s1:T O
|a1:T O

) B-DAT
[ O
T∏ O
t=1 O

p(ot B-DAT
| O
st) O
] O
= O
lnEq(s1:T O
|o1:T O
,a1:T O

T∏ B-DAT
t=1 O
p(ot O
| O
st)p(st O
| O
st−d O

, B-DAT
at−d−1:t−1)/q(st O
| O
o≤t, O

a<t) B-DAT
] O
≥ O
Eq(s1:T O
|o1:T O
,a1:T O

) B-DAT
[ O
T∑ O
t=1 O

ln B-DAT
p(ot O
| O
st) O
+ O

ln B-DAT
p(st O
| O
st−d, O
at−d−1:t−1)− O

ln B-DAT
q(st O
| O
o≤t, O

a<t) B-DAT
] O
≥ O
Eq(s1:T O
|o1:T O
,a1:T O

) B-DAT
[ O
T∑ O
t=1 O

ln B-DAT
p(ot O
| O
st) O
+ O

E B-DAT
p(st−1|st−d,at−d−1:t−2) O
[ln O
p(st O
| O

st−1, B-DAT
at−1)]− O
ln O
q(st O
| O

o≤t, B-DAT

T∑ B-DAT
t=1 O
( O
Eq(st|o≤t,a<t)[ln O
p(ot O
| O
st O

)] B-DAT
reconstruction O

E B-DAT
p(st−1|st−d,at−d−1:t−2)q(st−d|o≤t−d,a<t−d) O
[ O
KL[q(st O
| O
o≤t, O
a<t O

) B-DAT
‖ O
p(st O
| O
st−1, O

at−1)] B-DAT
] O
multi-step O
prediction O

9) B-DAT
Since O
all O
expectations O
are O
on O

the B-DAT
outside O
of O
the O
objective O

, B-DAT
we O
can O
easily O
obtain O

an B-DAT
unbiased O
estimator O
of O
this O

bound B-DAT
by O
changing O
expectations O
to O

sample B-DAT
averages. O
Relation O
between O
one-step O
and O
multi-step O

predictive B-DAT
distributions O
We O
conjecture O
that O

the B-DAT
multi-step O
predictive O
dis- O
tribution O

pd(o1:T B-DAT
) O
lower O
bounds O
the O

one-step B-DAT
predictive O
distribution O
p(o1:T O

) B-DAT
of O
the O
same O
latent O

sequence B-DAT
model O
model O
in O
expectation O

over B-DAT
the O
data O
set. O
Since O

the B-DAT
latent O
state O
sequence O
is O

Markovian, B-DAT
for O
d O
≥ O
1 O

we B-DAT
have O
the O
data O
processing O

inequality B-DAT

I(st; B-DAT
st−d) O
≤ O
I(st; O
st−1) O

H(st)−H(st|st−d) B-DAT
≤ O
H(st)−H(st|st−1) O
E[ln O

p(st B-DAT
| O
st−d)] O
≤ O
E[ln O

p(st B-DAT
| O
st−1)] O
E[ln O
pd(o1:T O
)] O
≤ O
E[ln O

p(o1:T B-DAT

10) B-DAT
Therefore, O
any O
bound O
on O
the O

multi-step B-DAT
predictive O
distribution, O
including O
Equation O

9 B-DAT
and O
Equation O
7, O
is O

also B-DAT
a O
bound O
on O
the O

one-step B-DAT
predictive O
distribution O

. B-DAT
16 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
G. O
Additional O
Related O
Work O
Planning O

in B-DAT
state O
space O
When O
low-dimensional O

states B-DAT
of O
the O
environment O
are O

available B-DAT
to O
the O
agent, O
it O

is B-DAT
possible O
to O
learn O
the O

dynamics B-DAT
directly O
in O
state O
space O

. B-DAT
In O
the O
regime O
of O

control B-DAT
tasks O
with O
only O
a O

few B-DAT
state O
variables, O
such O
as O

the B-DAT
cart O
pole O
and O
mountain O

car B-DAT
tasks, O
PILCO O
(Deisenroth O
& O

Rasmussen, B-DAT
2011) O
achieves O
remarkable O
sample O

efficiency B-DAT
using O
Gaussian O
processes O
to O

model B-DAT
the O
dynamics. O
Similar O
approaches O

using B-DAT
neural O
networks O
dynamics O
models O

can B-DAT
solve O
two-link O
balancing O
problems O
( O

Gal B-DAT
et O
al., O
2016; O
Higuera O

et B-DAT
al., O
2018) O
and O
implement O

planning B-DAT
via O
gradients O
(Henaff O
et O

al., B-DAT
2018). O
Chua O
et O
al. O
(2018 O

) B-DAT
use O
ensembles O
of O
neural O

networks, B-DAT
scaling O
up O
to O
the O

cheetah B-DAT
running O
task. O
The O
limitation O

of B-DAT
these O
methods O
is O
that O

they B-DAT
access O
the O
low-dimensional O
Markovian O

state B-DAT
of O
the O
underlying O
system O

and B-DAT
sometimes O
the O
reward O
function. O

Amos B-DAT
et O
al. O
(2018) O
train O

a B-DAT
deterministic O
model O
using O
overshooting O

in B-DAT
observation O
space O
for O
active O

exploration B-DAT
with O
a O
robotics O
hand. O

We B-DAT
move O
beyond O
low-dimensional O
state O

representations B-DAT
and O
use O
a O
latent O

dynamics B-DAT
model O
to O
solve O
control O

tasks B-DAT
from O
images. O
Hybrid O
agents O
The O
challenges O
of O

model-based B-DAT
RL O
have O
motivated O
the O

research B-DAT
community O
to O
develop O
hybrid O

agents B-DAT
that O
accelerate O
policy O
learning O

by B-DAT
training O
on O
imagined O
experience O

(Kalweit B-DAT
& O
Boedecker, O
2017; O
Nagabandi O

et B-DAT
al., O
2017; O
Kurutach O
et O

al., B-DAT
2018; O
Buckman O
et O
al O

., B-DAT
2018; O
Ha O
& O
Schmidhuber, O
2018 O

), B-DAT
improving O
feature O
representations O
(Wayne O

et B-DAT
al., O
2018; O
Igl O
et O

al., B-DAT
2018), O
or O
leveraging O
the O

information B-DAT
content O
of O
the O
model O

directly B-DAT
(Weber O
et O
al., O
2017). O

Srinivas B-DAT
et O
al. O
(2018) O
learn O

a B-DAT
policy O
network O
with O
integrated O

planning B-DAT
computation O
using O
reinforcement O
learning O

and B-DAT
without O
prediction O
loss, O
yet O

require B-DAT
expert O
demonstrations O
for O
training. O
Multi-step O
predictions O
Training O
sequence O
models O

on B-DAT
multi-step O
predictions O
has O
been O

explored B-DAT
for O
several O
years. O
Scheduled O

sampling B-DAT
(Bengio O
et O
al., O
2015 O

) B-DAT
changes O
the O
rollout O
distance O

of B-DAT
the O
sequence O
model O
over O

the B-DAT
course O
of O
training. O
Hallucinated O

replay B-DAT
(Talvitie, O
2014) O
mixes O
predictions O

into B-DAT
the O
data O
set O
to O

indirectly B-DAT
train O
multi-step O
predictions. O
Venkatraman O

et B-DAT
al. O
(2015) O
take O
an O

imitation B-DAT
learning O
approach. O
Recently, O
Amos O

et B-DAT
al. O
(2018) O
train O
a O

dynamics B-DAT
model O
on O
all O
multi-step O

predictions B-DAT
at O
once. O
We O
generalize O

this B-DAT
idea O
to O
latent O
sequence O

models B-DAT
trained O
via O
variational O
inference. O
Latent O
sequence O
models O
Classic O
work O

has B-DAT
explored O
models O
for O
non-Markovian O

observation B-DAT
sequences, O
including O
recurrent O
neural O

networks B-DAT
(RNNs) O
with O
deterministic O
hidden O

state B-DAT
and O
probabilistic O
state-space O
models O

(SSMs). B-DAT
The O
ideas O
behind O
variational O

autoencoders B-DAT
(Kingma O
& O
Welling, O
2013 O

; B-DAT
Rezende O
et O
al., O
2014) O

have B-DAT
enabled O
non-linear O
SSMs O
that O

are B-DAT
trained O
via O
variational O
inference O
( O

Krishnan B-DAT
et O
al., O
2015). O
The O

VRNN B-DAT
(Chung O
et O
al., O
2015) O

combines B-DAT
RNNs O
and O
SSMs O
and O

is B-DAT
trained O
via O
variational O
inference. O

In B-DAT
contrast O
to O
our O
RSSM, O

it B-DAT
feeds O
generated O
observations O
back O

into B-DAT
the O
model O
which O
makes O

forward B-DAT
predictions O
expensive. O
Karl O
et O

al. B-DAT
(2016) O
address O
mode O
collapse O

to B-DAT
a O
single O
future O
by O

restricting B-DAT
the O
transition O
function, O
(Moerland O

et B-DAT
al., O
2017) O
focus O
on O

multi-modal B-DAT
transitions, O
and O
Doerr O
et O

al. B-DAT
(2018) O
stabilize O
training O
of O

purely B-DAT
stochastic O
models. O
Buesing O
et O

al. B-DAT
(2018) O
propose O
a O
model O

similar B-DAT
to O
ours O
but O
use O

in B-DAT
a O
hybrid O
agent O
instead O

for B-DAT
explicit O
planning. O
Video O
prediction O
Video O
prediction O
is O

an B-DAT
active O
area O
of O
research O

in B-DAT
deep O
learning. O
Oh O
et O

al. B-DAT
(2015) O
and O
Chiappa O
et O

al. B-DAT
(2017) O
achieve O
visually O
plausible O

predictions B-DAT
on O
Atari O
games O
using O

deterministic B-DAT
models. O
Kalchbrenner O
et O
al O

. B-DAT
(2016) O
introduce O
an O
autoregressive O

video B-DAT
prediction O
model O
using O
gated O

CNNs B-DAT
and O
LSTMs. O
Recent O
approaches O

introduce B-DAT
stochasticity O
to O
the O
model O

to B-DAT
capture O
multiple O
futures O
(Babaeizadeh O

et B-DAT
al., O
2017; O
Denton O
& O

Fergus, B-DAT
2018). O
To O
obtain O
realistic O

predictions, B-DAT
Mathieu O
et O
al. O
(2015) O

and B-DAT
Vondrick O
et O
al. O
(2016) O

use B-DAT
adversarial O
losses. O
In O
simulated O

environments, B-DAT
Gemici O
et O
al. O
(2017) O

augment B-DAT
dynamics O
models O
with O
an O

external B-DAT
memory O
to O
remember O
long-time O

contexts. B-DAT
van O
den O
Oord O
et O

al. B-DAT
(2017) O
propose O
a O
variational O

model B-DAT
that O
avoids O
sampling O
using O

a B-DAT
nearest O
neighbor O
look-up, O
yielding O

high B-DAT
fidelity O
image O
predictions. O
These O

models B-DAT
are O
complimentary O
to O
our O

approach. B-DAT
17 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
H. O
Video O
Predictions O

Pl B-DAT
aN O
et O
Tr O

ue B-DAT
Context O
6 O
10 O
15 O
20 O

25 B-DAT
30 O
35 O
40 O
45 O

50 B-DAT

M B-DAT
od O
el O
Tr O

ue B-DAT
M O
od O
el O

Tr B-DAT
ue O
M O
od O

el B-DAT
Pl O
aN O

et B-DAT
+ O
O O
ve O

rs B-DAT
ho O
ot O
in O

g B-DAT
Tr O
ue O

Context B-DAT
6 O
10 O
15 O
20 O
25 O
30 O
35 O
40 O
45 O
50 O

M B-DAT
od O
el O
Tr O

ue B-DAT
M O
od O
el O

Tr B-DAT
ue O
M O
od O

el B-DAT
D O
et O

er B-DAT
m O
in O
is O

tic B-DAT
(G O
R O
U O

Tr B-DAT
ue O
Context O
6 O
10 O
15 O
20 O

25 B-DAT
30 O
35 O
40 O
45 O

50 B-DAT

M B-DAT
od O
el O
Tr O

ue B-DAT
M O
od O
el O

Tr B-DAT
ue O
M O
od O

el B-DAT
St O
oc O

ha B-DAT
st O
ic O
(S O

SM B-DAT
) O
Tr O
ue O

Context B-DAT
6 O
10 O
15 O
20 O
25 O
30 O
35 O
40 O
45 O
50 O

M B-DAT
od O
el O
Tr O

ue B-DAT
M O
od O
el O

Tr B-DAT
ue O
M O
od O

el B-DAT
Figure O
10: O
Open-loop O
video O
predictions O

for B-DAT
test O
episodes. O
The O
columns O

1–5 B-DAT
show O
reconstructed O
context O
frames O

and B-DAT
the O
remaining O
images O
are O

generated B-DAT
open-loop. O
Our O
RSSM O
achieves O

pixel-accurate B-DAT
predictions O
for O
50 O
steps O

into B-DAT
the O
future O
in O
the O

cheetah B-DAT
environment. O
We O
randomly O
selected O

action B-DAT
sequences O
from O
test O
episodes O

collected B-DAT
with O
action O
noise O
alongside O

the B-DAT
training O
episodes O

. B-DAT
18 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
I. O
State O
Diagnostics O

Figure B-DAT
11: O
Open-loop O
state O
diagnostics. O

We B-DAT
freeze O
the O
dynamics O
model O

of B-DAT
a O
PlaNet O
agent O
and O

learn B-DAT
small O
neural O
networks O
to O

predict B-DAT
the O
true O
positions, O
velocities, O

and B-DAT
reward O
of O
the O
simulator. O

The B-DAT
open-loop O
predictions O
of O
these O

quantities B-DAT
show O
that O
most O
information O

about B-DAT
the O
underlying O
system O
is O

present B-DAT
in O
the O
learned O
latent O

space B-DAT
and O
can O
be O
accurately O

predicted B-DAT
forward O
further O
than O
the O

planning B-DAT
horizons O
used O
in O
this O

work. B-DAT
19 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
J. O
Planning O
Parameters O

3.0 B-DAT
5.0 O

10.0 B-DAT
15.0 O

ite B-DAT
ra O
tio O
ns O

fraction=0.05 B-DAT
horizon=6.0 O
fraction=0.05 O
horizon=8.0 O
fraction=0.05 O

horizon=10.0 B-DAT
fraction=0.05 O
horizon=12.0 O
fraction=0.05 O
horizon=14.0 O
3.0 O

5.0 B-DAT
10.0 O

15.0 B-DAT
ite O
ra O

tio B-DAT
ns O
fraction=0.1 O
horizon=6.0 O
fraction=0.1 O
horizon=8.0 O
fraction=0.1 O

horizon=10.0 B-DAT
fraction=0.1 O
horizon=12.0 O
fraction=0.1 O
horizon=14.0 O

3.0 B-DAT
5.0 O

10.0 B-DAT
15.0 O

ite B-DAT
ra O
tio O
ns O

fraction=0.3 B-DAT
horizon=6.0 O
fraction=0.3 O
horizon=8.0 O
fraction=0.3 O

horizon=10.0 B-DAT
fraction=0.3 O
horizon=12.0 O
fraction=0.3 O
horizon=14.0 O
100.0 O
300.0 O
500.0 O
1000.0 O
proposals O

3.0 B-DAT
5.0 O

10.0 B-DAT
15.0 O

ite B-DAT
ra O
tio O
ns O

fraction=0.5 B-DAT
horizon=6.0 O
100.0 O
300.0 O
500.0 O
1000.0 O
proposals O

fraction=0.5 B-DAT
horizon=8.0 O
100.0 O
300.0 O
500.0 O
1000.0 O
proposals O

fraction=0.5 B-DAT
horizon=10.0 O
100.0 O
300.0 O
500.0 O
1000.0 O
proposals O

fraction=0.5 B-DAT
horizon=12.0 O
100.0 O
300.0 O
500.0 O
1000.0 O
proposals O

fraction=0.5 B-DAT
horizon=14.0 O
Figure O
12: O
Planning O
performance O
on O

the B-DAT
cheetah O
running O
task O
with O

the B-DAT
true O
simulator O
using O
different O

planner B-DAT
settings. O
Performance O
ranges O
from O

132 B-DAT
(blue) O
to O
837 O
(yellow O

). B-DAT
Evaluating O
more O
action O
sequences, O

optimizing B-DAT
for O
more O
iterations, O
and O

re-fitting B-DAT
to O
fewer O
of O
the O

best B-DAT
proposals O
tend O
to O
improve O

performance. B-DAT
A O
planning O
horizon O
length O

of B-DAT
6 O
is O
not O
sufficient O

and B-DAT
results O
in O
poor O
performance. O

Much B-DAT
longer O
planning O
horizons O
hurt O

performance B-DAT
because O
of O
the O
increased O

search B-DAT
space. O
For O
this O
environment, O

best B-DAT
planning O
horizon O
length O
is O

near B-DAT
8 O
steps. O
20 O

c) O
Cheetah O
(d) O
Finger O
(e) O
Cup B-DAT
(f) O
Walker O

1000 O
Cup B-DAT
Catch O

1000 O
Cup B-DAT
Catch O

1000 O
Cup B-DAT
Catch O

1000 O
Cup B-DAT
Catch O

1000 O
Cup B-DAT
Catch O

1000 O
Cup O
Catch B-DAT

1000 O
Cup O
Catch B-DAT

1000 O
Cup O
Catch B-DAT

1000 O
Cup O
Catch B-DAT

1000 O
Cup O
Catch B-DAT

variety O
of O
tasks O
from O
the O
DeepMind B-DAT
control O
suite, O
shown O
in O
Figure O

continuous O
control O
tasks O
of O
the O
DeepMind B-DAT
control O
suite O
(Tassa O
et O
al O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
Danijar O
Hafner O
1 O
2 O
Timothy O

Lillicrap B-DAT
3 O
Ian O
Fischer O
4 O

Ruben B-DAT
Villegas O
1 O
5 O
David O

Ha B-DAT
1 O
Honglak O
Lee O
1 O

James B-DAT
Davidson O
1 O

Abstract B-DAT
Planning O
has O
been O
very O

successful B-DAT
for O
control O
tasks O
with O

known B-DAT
environment O
dynamics. O
To O
leverage O

planning B-DAT
in O
unknown O
environments, O
the O

agent B-DAT
needs O
to O
learn O
the O

dynamics B-DAT
from O
interactions O
with O
the O

world. B-DAT
However, O
learning O
dynamics O
models O

that B-DAT
are O
accurate O
enough O
for O

planning B-DAT
has O
been O
a O
long-standing O

challenge, B-DAT
especially O
in O
image-based O
domains. O

We B-DAT
propose O
the O
Deep O
Planning O

Network B-DAT
(PlaNet), O
a O
purely O
model-based O

agent B-DAT
that O
learns O
the O
environment O

dynamics B-DAT
from O
images O
and O
chooses O

actions B-DAT
through O
fast O
online O
planning O

in B-DAT
latent O
space. O
To O
achieve O

high B-DAT
performance, O
the O
dynamics O
model O

must B-DAT
accurately O
predict O
the O
rewards O

ahead B-DAT
for O
multiple O
time O
steps. O

We B-DAT
approach O
this O
using O
a O

latent B-DAT
dynamics O
model O
with O
both O

deterministic B-DAT
and O
stochastic O
transition O
components. O

Moreover, B-DAT
we O
propose O
a O
multi-step O

variational B-DAT
inference O
objective O
that O
we O

name B-DAT
latent O
overshooting. O
Using O
only O

pixel B-DAT
observations, O
our O
agent O
solves O

continuous B-DAT
control O
tasks O
with O
contact O

dynamics, B-DAT
partial O
observability, O
and O
sparse O

rewards, B-DAT
which O
exceed O
the O
difficulty O

of B-DAT
tasks O
that O
were O
previously O

solved B-DAT
by O
planning O
with O
learned O

models. B-DAT
PlaNet O
uses O
substantially O
fewer O

episodes B-DAT
and O
reaches O
final O
performance O

close B-DAT
to O
and O
sometimes O
higher O

than B-DAT
strong O
model-free O
algorithms. O
1. O
Introduction O
Planning O
is O
a O

natural B-DAT
and O
powerful O
approach O
to O

decision B-DAT
making O
problems O
with O
known O

dynamics, B-DAT
such O
as O
game O
play O

- B-DAT
ing O
and O
simulated O
robot O

control B-DAT
(Tassa O
et O
al., O
2012; O

Silver B-DAT
et O
al., O
2017; O
Moravčík O

et B-DAT
al., O
2017). O
To O
plan O

in B-DAT
unknown O
environments, O
the O
agent O

needs B-DAT
to O
learn O
the O
dynamics O

from B-DAT
experience. O
Learning O
dynamics O
models O

that B-DAT
are O
accurate O
1Google O
Brain O
2University O
of O
Toronto O

3DeepMind B-DAT
4Google O
Research O
5University O
of O

Michigan. B-DAT
Correspondence O
to: O
Danijar O
Hafner O

<mail@danijar.com B-DAT

>. B-DAT
Proceedings O
of O
the O
36 O
th O

International B-DAT
Conference O
on O
Machine O
Learning O

, B-DAT
Long O
Beach, O
California, O
PMLR O
97, O
2019. O
Copyright O
2019 O
by O
the O

author(s B-DAT

). B-DAT
enough O
for O
planning O
has O
been O

a B-DAT
long-standing O
challenge. O
Key O
difficulties O

include B-DAT
model O
inaccuracies, O
accumulating O
errors O

of B-DAT
multi-step O
predictions, O
failure O
to O

capture B-DAT
multiple O
possible O
futures, O
and O

overconfident B-DAT
predictions O
outside O
of O
the O

training B-DAT
distribution O

. B-DAT
Planning O
using O
learned O
models O
offers O

several B-DAT
benefits O
over O
model-free O
reinforcement O

learning. B-DAT
First, O
model-based O
plan- O
ning O

can B-DAT
be O
more O
data O
efficient O

because B-DAT
it O
leverages O
a O
richer O

training B-DAT
signal O
and O
does O
not O

require B-DAT
propagating O
rewards O
through O
Bellman O

backups. B-DAT
Moreover, O
planning O
carries O
the O

promise B-DAT
of O
increasing O
performance O
just O

by B-DAT
increasing O
the O
computational O
budget O

for B-DAT
searching O
for O
actions, O
as O

shown B-DAT
by O
Silver O
et O
al O

. B-DAT
(2017). O
Finally, O
learned O
dynamics O

can B-DAT
be O
independent O
of O
any O

specific B-DAT
task O
and O
thus O
have O

the B-DAT
potential O
to O
transfer O
well O

to B-DAT
other O
tasks O
in O
the O

environment. B-DAT
Recent O
work O
has O
shown O
promise O

in B-DAT
learning O
the O
dynamics O
of O

simple B-DAT
low-dimensional O
environments O
(Deisenroth O

& B-DAT
Ras- O
mussen, O
2011; O
Gal O

et B-DAT
al., O
2016; O
Amos O
et O

al., B-DAT
2018; O
Chua O
et O
al O

., B-DAT
2018; O
Henaff O
et O
al., O
2018 O

). B-DAT
However, O
these O
approaches O
typically O

assume B-DAT
access O
to O
the O
underlying O

state B-DAT
of O
the O
world O
and O

the B-DAT
reward O
function, O
which O
may O

not B-DAT
be O
available O
in O
prac- O

tice. B-DAT
In O
high-dimensional O
environments, O
we O

would B-DAT
like O
to O
learn O
the O

dynamics B-DAT
in O
a O
compact O
latent O

space B-DAT
to O
enable O
fast O
planning. O

The B-DAT
success O
of O
such O
latent O

models B-DAT
has O
previously O
been O
limited O

to B-DAT
simple O
tasks O
such O
as O

balancing B-DAT
cartpoles O
and O
controlling O
2-link O

arms B-DAT
from O
dense O
rewards O
(Watter O

et B-DAT
al., O
2015; O
Banijamali O
et O

al., B-DAT
2017). O
In O
this O
paper, O
we O
propose O

the B-DAT
Deep O
Planning O
Network O
(PlaNet O

), B-DAT
a O
model-based O
agent O
that O

learns B-DAT
the O
environment O
dynamics O
from O

pixels B-DAT
and O
chooses O
actions O
through O

online B-DAT
planning O
in O
a O
compact O

latent B-DAT
space. O
To O
learn O
the O

dynamics, B-DAT
we O
use O
a O
transition O

model B-DAT
with O
both O
stochastic O
and O

determin- B-DAT
istic O
components. O
Moreover, O
we O

experiment B-DAT
with O
a O
novel O
generalized O

variational B-DAT
objective O
that O
encourages O
multi-step O

predictions. B-DAT
PlaNet O
solves O
continuous O
control O

tasks B-DAT
from O
pixels O
that O
are O

more B-DAT
difficult O
than O
those O
previously O

solved B-DAT
by O
planning O
with O
learned O

models. B-DAT
Key O
contributions O
of O
this O
work O

are B-DAT
summarized O
as O
follows O

: B-DAT
• O
Planning O
in O
latent O
spaces O

We B-DAT
solve O
a O
variety O
of O

tasks B-DAT
from O
the O
DeepMind O
control O

suite, B-DAT
shown O
in O
Figure O
1 O

, B-DAT
by O
learning O
a O
dynamics O

model B-DAT
and O
efficiently O
planning O
in O
ar O
X O

iv B-DAT
:1 O
81 O
1 O

. B-DAT
04 O
55 O

1v B-DAT
5 O

cs B-DAT
.L O
G O

4 B-DAT

J B-DAT

2 B-DAT
01 O
9 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
(a) O
Cartpole O
(b) O
Reacher O
(c O

) B-DAT
Cheetah O
(d) O
Finger O
(e) O

Cup B-DAT
(f) O
Walker O
Figure O
1: O
Image-based O
control O
domains O

used B-DAT
in O
our O
experiments. O
The O

images B-DAT
show O
agent O
observations O
before O

downscaling B-DAT
to O
64× O
64× O
3 O

pixels. B-DAT
(a) O
The O
cartpole O
swingup O

task B-DAT
has O
a O
fixed O
camera O

so B-DAT
the O
cart O
can O
move O

out B-DAT
of O
sight. O
(b) O
The O

reacher B-DAT
task O
has O
only O
a O

sparse B-DAT
reward. O
(c) O
The O
cheetah O

running B-DAT
task O
includes O
both O
contacts O

and B-DAT
a O
larger O
number O
of O

joints. B-DAT
(d) O
The O
finger O
spinning O

task B-DAT
includes O
contacts O
between O
the O

finger B-DAT
and O
the O
object. O
(e O

) B-DAT
The O
cup O
task O
has O

a B-DAT
sparse O
reward O
that O
is O

only B-DAT
given O
once O
the O
ball O

is B-DAT
caught. O
(f) O
The O
walker O

task B-DAT
requires O
balance O
and O
predicting O

difficult B-DAT
interactions O
with O
the O
ground O

when B-DAT
the O
robot O
is O
lying O

down. B-DAT
its O
latent O
space. O
Our O
agent O

substantially B-DAT
outperforms O
the O
model-free O
A3C O

and B-DAT
in O
some O
cases O
D4PG O

algorithm B-DAT
in O
final O
performance, O
with O

on B-DAT
average O
200× O
less O
environ O

- B-DAT
ment O
interaction O
and O
similar O

computation B-DAT
time. O
• O
Recurrent O
state O
space O
model O

We B-DAT
design O
a O
latent O
dy O

- B-DAT
namics O
model O
with O
both O

deterministic B-DAT
and O
stochastic O
components O
(Buesing O

et B-DAT
al., O
2018; O
Chung O
et O

al., B-DAT
2015). O
Our O
experiments O
indicate O

having B-DAT
both O
components O
to O
be O

crucial B-DAT
for O
high O
planning O
performance. O
• O
Latent O
overshooting O
We O
generalize O

the B-DAT
standard O
vari- O
ational O
bound O

to B-DAT
include O
multi-step O
predictions. O
Using O

only B-DAT
terms O
in O
latent O
space O

results B-DAT
in O
a O
fast O
regularizer O

that B-DAT
can O
improve O
long-term O
predictions O

and B-DAT
is O
compati- O
ble O
with O

any B-DAT
latent O
sequence O
model O

. B-DAT
2. O
Latent O
Space O
Planning O
To O

solve B-DAT
unknown O
environments O
via O
planning O

, B-DAT
we O
need O
to O
model O

the B-DAT
environment O
dynamics O
from O
experience. O

PlaNet B-DAT
does O
so O
by O
iteratively O

collecting B-DAT
data O
using O
planning O
and O

training B-DAT
the O
dynamics O
model O
on O

the B-DAT
gathered O
data. O
In O
this O

section, B-DAT
we O
introduce O
notation O
for O

the B-DAT
environment O
and O
de- O
scribe O

the B-DAT
general O
implementation O
of O
our O

model-based B-DAT
agent. O
In O
this O
section, O

we B-DAT
assume O
access O
to O
a O

learned B-DAT
dynamics O
model. O
Our O
design O

and B-DAT
training O
objective O
for O
this O

model B-DAT
are O
detailed O
in O
Section O
3 O

. B-DAT
Problem O
setup O
Since O
individual O
image O

observations B-DAT
gen- O
erally O
do O
not O

reveal B-DAT
the O
full O
state O
of O

the B-DAT
environment, O
we O
consider O
a O

partially B-DAT
observable O
Markov O
decision O
process O

(POMDP). B-DAT
We O
define O
a O
discrete O

time B-DAT
step O
t, O
hidden O
states O

st, B-DAT
image O
observations O
ot, O
continuous O

action B-DAT
vectors O
at, O
and O
scalar O

rewards B-DAT
rt, O
that O
follow O
the O

stochastic B-DAT
dynamics O

Transition B-DAT
function: O
st O
∼ O

p(st B-DAT
| O
st−1, O
at−1) O
Observation O

function: B-DAT
ot O
∼ O
p(ot O
| O

st) B-DAT
Reward O
function: O
rt O
∼ O

p(rt B-DAT
| O
st) O
Policy: O

at B-DAT
∼ O
p(at O
| O
o≤t, O

a<t), B-DAT
(1 O

) B-DAT
Algorithm O
1: O
Deep O
Planning O
Network O

(PlaNet) B-DAT
Input O
: O
R O
Action O

repeat B-DAT
S O
Seed O
episodes O
C O

Collect B-DAT
interval O
B O
Batch O
size O

L B-DAT
Chunk O
length O
α O
Learning O

rate B-DAT

p(st B-DAT
| O
st−1, O
at−1) O
Transition O

model B-DAT
p(ot O
| O
st) O
Observation O

model B-DAT
p(rt O
| O
st) O
Reward O

model B-DAT
q(st O
| O
o≤t, O
a<t) O

Encoder B-DAT
p(�) O
Exploration O
noise O
1 O
Initialize O
dataset O
D O
with O

S B-DAT
random O
seed O
episodes. O
2 O

Initialize B-DAT
model O
parameters O
θ O
randomly O

. B-DAT
3 O
while O
not O
converged O

do B-DAT
// O
Model O
fitting O
4 O
for O

update B-DAT
step O
s O
= O
1..C O

do B-DAT
5 O
Draw O
sequence O
chunks O

{(ot, B-DAT
at, O
rt)L+kt=k O
}Bi=1 O

∼ B-DAT
D O

uniformly B-DAT
at O
random O
from O
the O

dataset. B-DAT
6 O
Compute O
loss O
L(θ) O

from B-DAT
Equation O
3. O
7 O
Update O

model B-DAT
parameters O
θ O
← O

θ B-DAT
− O
α∇θL(θ). O
// O
Data O
collection O
8 O
o1 O

← B-DAT
env.reset() O
9 O
for O
time O

step B-DAT
t O
= O
1 O

.. B-DAT
⌈ O
T O
R O

do B-DAT
10 O
Infer O
belief O
over O
current O

state B-DAT
q(st O
| O
o≤t, O
a<t O

) B-DAT
from O
the O
history. O
11 O
at O
← O
planner(q(st O

| B-DAT
o≤t, O
a<t), O
p), O
see O

Algorithm B-DAT
2 O
in O
the O
appendix O

for B-DAT
details O

. B-DAT
12 O
Add O
exploration O
noise O

� B-DAT
∼ O
p(�) O
to O
the O

action. B-DAT
13 O
for O
action O
repeat O

k B-DAT
= O
1..R O
do O
14 O

rkt B-DAT
, O
o O

k B-DAT
t+1 O
← O
env.step(at) O
15 O
rt, O
ot+1 O
← O
∑R O

k=1 B-DAT
r O

k B-DAT
t O
, O
o O
R O
t+1 O

16 B-DAT
D O
← O

D B-DAT
∪ O
{(ot, O
at, O
rt)Tt=1} O
where O
we O
assume O
a O
fixed O

initial B-DAT
state O
s0 O
without O
loss O

of B-DAT
gen- O
erality. O
The O
goal O

is B-DAT
to O
implement O
a O
policy O

p(at B-DAT
| O
o≤t, O
a<t) O
that O

maximizes B-DAT
the O
expected O
sum O
of O

rewards B-DAT
Ep O

T B-DAT
t=1 O

where B-DAT
the O
expectation O
is O
over O

the B-DAT
distributions O
of O
the O
envi- O

ronment B-DAT
and O
the O
policy. O
2 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
Model-based O
planning O
PlaNet O
learns O
a O

transition B-DAT
model O
p(st O
| O
st−1 O

, B-DAT
at−1), O
observation O
model O

p(ot B-DAT
| O
st), O
and O
reward O

model B-DAT
p(rt O
| O
st) O
from O

previously B-DAT
experienced O
episodes O
(note O
italic O

letters B-DAT
for O
the O
model O
compared O

to B-DAT
upright O
letters O
for O
the O

true B-DAT
dynamics). O
The O
observation O
model O

provides B-DAT
a O
rich O
training O
signal O

but B-DAT
is O
not O
used O
for O

planning. B-DAT
We O
also O
learn O
an O

encoder B-DAT
q(st O
| O
o≤t, O
a<t) O

to B-DAT
infer O
an O
approximate O
belief O

over B-DAT
the O
current O
hidden O
state O

from B-DAT
the O
history O
using O
filtering. O

Given B-DAT
these O
components, O
we O
implement O

the B-DAT
policy O
as O
a O
planning O

algorithm B-DAT
that O
searches O
for O
the O

best B-DAT
sequence O
of O
future O
actions. O

We B-DAT
use O
model-predictive O
control O
(MPC; O

Richards, B-DAT
2005) O
to O
allow O
the O

agent B-DAT
to O
adapt O
its O
plan O

based B-DAT
on O
new O
observations, O
meaning O

we B-DAT
replan O
at O
each O
step. O

In B-DAT
contrast O
to O
model-free O
and O

hybrid B-DAT
reinforcement O
learning O
algorithms, O
we O

do B-DAT
not O
use O
a O
policy O

or B-DAT
value O
network. O
Experience O
collection O
Since O
the O
agent O

may B-DAT
not O
initially O
visit O
all O

parts B-DAT
of O
the O
environment, O
we O

need B-DAT
to O
iteratively O
collect O
new O

experience B-DAT
and O
refine O
the O
dynamics O

model. B-DAT
We O
do O
so O
by O

planning B-DAT
with O
the O
partially O
trained O

model, B-DAT
as O
shown O
in O
Algorithm O

1. B-DAT
Starting O
from O
a O
small O

amount B-DAT
of O
S O
seed O
episodes O

collected B-DAT
under O
random O
actions, O
we O

train B-DAT
the O
model O
and O
add O

one B-DAT
additional O
episode O
to O
the O

data B-DAT
set O
everyC O
update O
steps O

. B-DAT
When O
collecting O
episodes O
for O

the B-DAT
data O
set, O
we O
add O

small B-DAT
Gaussian O
exploration O
noise O
to O

the B-DAT
action. O
To O
reduce O
the O

planning B-DAT
horizon O
and O
provide O
a O

clearer B-DAT
learning O
signal O
to O
the O

model, B-DAT
we O
repeat O
each O
action O

R B-DAT
times, O
as O
common O
in O

reinforcement B-DAT
learning O
(Mnih O
et O
al., O
2015 O

; B-DAT
2016). O
Planning O
algorithm O
We O
use O
the O

cross B-DAT
entropy O
method O
(CEM; O
Rubinstein O

, B-DAT
1997; O
Chua O
et O
al., O
2018 O

) B-DAT
to O
search O
for O
the O

best B-DAT
action O
sequence O
under O
the O

model, B-DAT
as O
outlined O
in O
Algorithm O
2 O

. B-DAT
We O
decided O
on O
this O

algorithm B-DAT
because O
of O
its O
robustness O

and B-DAT
because O
it O
solved O
all O

considered B-DAT
tasks O
when O
given O
the O

true B-DAT
dynamics O
for O
planning. O
CEM O

is B-DAT
a O
population- O
based O
optimization O

algorithm B-DAT
that O
infers O
a O
distribution O

over B-DAT
action O
sequences O
that O
maximize O

the B-DAT
objective. O
As O
detailed O
in O

Algorithm B-DAT
2 O
in O
the O
appendix, O

we B-DAT
initialize O
a O
time-dependent O
diagonal O

Gaussian B-DAT
belief O
over O
optimal O
action O

sequences B-DAT
at:t+H O
∼ O
Normal(µt:t+H O
, O

σ2t:t+HI), B-DAT
where O
t O
is O
the O

current B-DAT
time O
step O
of O
the O

agent B-DAT
and O
H O
is O
the O

length B-DAT
of O
the O
planning O
horizon. O

Starting B-DAT
from O
zero O
mean O
and O

unit B-DAT
variance, O
we O
repeatedly O
sample O

J B-DAT
candidate O
action O
sequences, O
evaluate O

them B-DAT
under O
the O
model, O
and O

re-fit B-DAT
the O
belief O
to O
the O

top B-DAT
K O
action O
sequences. O
After O

I B-DAT
iterations, O
the O
planner O
returns O

the B-DAT
mean O
of O
the O
belief O

for B-DAT
the O
current O
time O
step, O

µt. B-DAT
Importantly, O
after O
receiving O
the O

next B-DAT
observation, O
the O
belief O
over O

action B-DAT
sequences O
starts O
from O
zero O

mean B-DAT
and O
unit O
variance O
again O

to B-DAT
avoid O
local O
optima. O
To O
evaluate O
a O
candidate O
action O

sequence B-DAT
under O
the O
learned O
model O

, B-DAT
we O
sample O
a O
state O

trajectory B-DAT
starting O
from O
the O
current O

state B-DAT
belief, O
and O
sum O
the O

mean B-DAT
rewards O
predicted O
along O
the O

sequence. B-DAT
Since O
we O
use O
a O

population-based B-DAT
optimizer, O
we O
found O
it O
sufficient O
to O

consider B-DAT
a O
single O
trajectory O
per O

action B-DAT
sequence O
and O
thus O
focus O

the B-DAT
computational O
budget O
on O
evaluating O

a B-DAT
larger O
number O
of O
different O

sequences. B-DAT
Because O
the O
reward O
is O

modeled B-DAT
as O
a O
function O
of O

the B-DAT
latent O
state, O
the O
planner O

can B-DAT
operate O
purely O
in O
latent O

space B-DAT
without O
generating O
images, O
which O

allows B-DAT
for O
fast O
evaluation O
of O

large B-DAT
batches O
of O
action O
sequences O

. B-DAT
The O
next O
section O
introduces O

the B-DAT
latent O
dynamics O
model O
that O

the B-DAT
planner O
uses. O
3. O
Recurrent O
State O
Space O
Model O

For B-DAT
planning, O
we O
need O
to O

evaluate B-DAT
thousands O
of O
action O
se O

- B-DAT
quences O
at O
every O
time O

step B-DAT
of O
the O
agent. O
Therefore, O

we B-DAT
use O
a O
recurrent O
state-space O

model B-DAT
(RSSM) O
that O
can O
predict O

for- B-DAT
ward O
purely O
in O
latent O

space, B-DAT
similar O
to O
recently O
proposed O

models B-DAT
(Karl O
et O
al., O
2016; O

Buesing B-DAT
et O
al., O
2018; O
Doerr O

et B-DAT
al., O
2018). O
This O
model O

can B-DAT
be O
thought O
of O
as O

a B-DAT
non-linear O
Kalman O
filter O
or O

sequential B-DAT
VAE. O
Instead O
of O
an O

extensive B-DAT
comparison O
to O
prior O
architectures, O

we B-DAT
highlight O
two O
findings O
that O

can B-DAT
guide O
future O
designs O
of O

dynamics B-DAT
models: O
our O
experiments O
show O

that B-DAT
both O
stochastic O
and O
deterministic O

paths B-DAT
in O
the O
transition O
model O

are B-DAT
crucial O
for O
successful O
planning. O

In B-DAT
this O
section, O
we O
remind O

the B-DAT
reader O
of O
latent O
state-space O

models B-DAT
and O
then O
describe O
our O

dynamics B-DAT
model. O
Latent O
dynamics O
We O
consider O
sequences O

{ot, B-DAT
at, O
rt}Tt=1 O
with O
discrete O

time B-DAT
step O
t, O
image O
observations O

ot, B-DAT
continuous O
action O
vectors O
at O

, B-DAT
and O
scalar O
rewards O
rt. O

A B-DAT
typical O
latent O
state-space O
model O

is B-DAT
shown O
in O
Figure O
2b O

and B-DAT
resembles O
the O
structure O
of O

a B-DAT
partially O
observable O
Markov O
decision O

process. B-DAT
It O
defines O
the O
generative O

process B-DAT
of O
the O
images O
and O

rewards B-DAT
using O
a O
hidden O
state O

sequence B-DAT
{st}Tt=1, O
Transition O
model: O
st O
∼ O
p(st O

| B-DAT
st−1, O
at−1) O
Observation O
model O

: B-DAT
ot O
∼ O
p(ot O
| O

st) B-DAT
Reward O
model: O
rt O
∼ O

p(rt B-DAT
| O
st), O
(2 O

) B-DAT
where O
we O
assume O
a O
fixed O

initial B-DAT
state O
s0 O
without O
loss O

of B-DAT
generality. O
The O
transition O
model O

is B-DAT
Gaussian O
with O
mean O
and O

variance B-DAT
parameterized O
by O
a O
feed-forward O

neural B-DAT
network, O
the O
observation O
model O

is B-DAT
Gaussian O
with O
mean O
parameterized O

by B-DAT
a O
deconvolutional O
neural O
network O

and B-DAT
identity O
covariance, O
and O
the O

reward B-DAT
model O
is O
a O
scalar O

Gaussian B-DAT
with O
mean O
param- O
eterized O

by B-DAT
a O
feed-forward O
neural O
network O

and B-DAT
unit O
variance. O
Note O
that O

the B-DAT
log-likelihood O
under O
a O
Gaussian O

distribution B-DAT
with O
unit O
variance O
equals O

the B-DAT
mean O
squared O
error O
up O

to B-DAT
a O
constant O

. B-DAT
Variational O
encoder O
Since O
the O
model O

is B-DAT
non-linear, O
we O
cannot O
directly O

compute B-DAT
the O
state O
posteriors O
that O

are B-DAT
needed O
for O
parameter O
learning O

. B-DAT
Instead, O
we O
use O
an O

encoder B-DAT
q(s1:T O
| O
o1:T O
, O

a1:T B-DAT
) O
= O
∏T O
t=1 O
q(st O
| O
st−1 O

, B-DAT
at−1, O
ot) O
to O
infer O

approx- B-DAT
imate O
state O
posteriors O
from O
past O

observations B-DAT
and O
actions, O
where O
q(st O

| B-DAT
st−1, O
at−1, O
ot) O
is O

a B-DAT
diagonal O
Gaussian O
with O
mean O

and B-DAT
variance O
parameterized O
by O
a O

convolutional B-DAT
neural O

3 B-DAT

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
o1, O
r1 O
o2, O
r2 O
o3 O

, B-DAT
r3 O
h1 O
h2 O
h3 O

a1 B-DAT
a2 O
(a) O
Deterministic O
model O
(RNN O

) B-DAT
o1, O
r1 O
o2, O
r2 O
o3 O

, B-DAT
r3 O
s1 O
s2 O
s3 O

a1 B-DAT
a2 O
(b) O
Stochastic O
model O
(SSM O

) B-DAT
o1, O
r1 O
o2, O
r2 O
o3 O

, B-DAT
r3 O
s1 O
s2 O
s3 O

h1 B-DAT
h2 O
h3 O
a1 O
a2 O

c) B-DAT
Recurrent O
state-space O
model O
(RSSM) O
Figure O
2: O
Latent O
dynamics O
model O

designs. B-DAT
In O
this O
example, O
the O

model B-DAT
observes O
the O
first O
two O

time B-DAT
steps O
and O
predicts O
the O

third. B-DAT
Circles O
represent O
stochastic O
variables O

and B-DAT
squares O
deterministic O
variables. O
Solid O

lines B-DAT
denote O
the O
generative O
process O

and B-DAT
dashed O
lines O
the O
inference O

model. B-DAT
(a) O
Transitions O
in O
a O

recurrent B-DAT
neural O
network O
are O
purely O

deterministic. B-DAT
This O
prevents O
the O
model O

from B-DAT
capturing O
multiple O
futures O
and O

makes B-DAT
it O
easy O
for O
the O

planner B-DAT
to O
exploit O
inaccuracies. O
(b O

) B-DAT
Transitions O
in O
a O
state-space O

model B-DAT
are O
purely O
stochastic. O
This O

makes B-DAT
it O
difficult O
to O
remember O

information B-DAT
over O
multiple O
time O
steps. O
( O

c) B-DAT
We O
split O
the O
state O

into B-DAT
stochastic O
and O
deterministic O
parts, O

allowing B-DAT
the O
model O
to O
robustly O

learn B-DAT
to O
predict O
multiple O
futures. O
network O
followed O
by O
a O
feed-forward O

neural B-DAT
network. O
We O
use O
the O

filtering B-DAT
posterior O
that O
conditions O
on O

past B-DAT
observations O
since O
we O
are O

ultimately B-DAT
interested O
in O
using O
the O

model B-DAT
for O
planning, O
but O
one O

may B-DAT
also O
use O
the O
full O

smoothing B-DAT
posterior O
during O
training O
(Babaeizadeh O

et B-DAT
al., O
2017; O
Gregor O

& B-DAT
Besse, O
2018 O

). B-DAT
Training O
objective O
Using O
the O
encoder O

, B-DAT
we O
construct O
a O
variational O

bound B-DAT
on O
the O
data O
log-likelihood. O

For B-DAT
simplicity, O
we O
write O
losses O

for B-DAT
predicting O
only O
the O

observations B-DAT
— O
the O
reward O
losses O

follow B-DAT
by O
analogy. O
The O
variational O

bound B-DAT
obtained O
using O
Jensen’s O
inequality O

is B-DAT
ln O
p(o1:T O
| O
a1:T O

) B-DAT
, O
ln O

t B-DAT
p(st O
| O
st−1, O
at−1)p(ot O

| B-DAT
st) O
ds1:T O

T∑ B-DAT
t=1 O

Eq(st|o≤t,a<t)[ln B-DAT
p(ot O
| O
st)] O
reconstruction O

E B-DAT
q(st−1|o≤t−1,a<t−1) O
[ O
KL[q(st O
| O
o≤t, O
a<t O

) B-DAT
‖ O
p(st O
| O
st−1, O

at−1)] B-DAT
] O
complexity O

3) B-DAT
For O
the O
derivation, O
please O
see O

Equation B-DAT
8 O
in O
the O
appendix O

. B-DAT
Estimating O
the O
outer O
expectations O

using B-DAT
a O
single O
reparam- O
eterized O

sample B-DAT
yields O
an O
efficient O
objective O

for B-DAT
inference O
and O
learning O
in O

non-linear B-DAT
latent O
variable O
models O
that O

can B-DAT
be O
optimized O
using O
gradient O

ascent B-DAT
(Kingma O
& O
Welling, O
2013; O

Rezende B-DAT
et O
al., O
2014; O
Krishnan O

et B-DAT
al., O
2017). O
Deterministic O
path O
Despite O
its O
generality O

, B-DAT
the O
purely O
stochastic O
transitions O

make B-DAT
it O
difficult O
for O
the O

transition B-DAT
model O
to O
reliably O
remember O

information B-DAT
for O
multiple O
time O
steps. O

In B-DAT
theory, O
this O
model O
could O

learn B-DAT
to O
set O
the O
variance O

to B-DAT
zero O
for O
some O
state O

components, B-DAT
but O
the O
optimization O
pro- O

cedure B-DAT
may O
not O
find O
this O

solution. B-DAT
This O
motivates O
including O
a O

deterministic B-DAT
sequence O
of O
activation O

vectors B-DAT
{ht}Tt=1 O
that O
allow O
the O

model B-DAT
to O
access O
not O
just O

the B-DAT
last O
state O
but O
all O

pre- B-DAT
vious O
states O
deterministically O
(Chung O

et B-DAT
al., O
2015; O
Buesing O
et O

al., B-DAT
2018). O
We O
use O
such O

a B-DAT
model, O
shown O
in O
Figure O

2c, B-DAT
that O
we O
name O
recurrent O
state-space O
model O

(RSSM B-DAT

), B-DAT
Deterministic O
state O
model: O
ht O

= B-DAT
f(ht−1, O
st−1, O
at−1) O
Stochastic O

state B-DAT
model: O
st O
∼ O
p(st O

| B-DAT
ht) O
Observation O
model: O
ot O

∼ B-DAT
p(ot O
| O
ht, O
st O

) B-DAT
Reward O
model: O
rt O
∼ O

p(rt B-DAT
| O
ht, O
st), O
(4 O

) B-DAT
where O
f(ht−1, O
st−1, O
at−1) O
is O

implemented B-DAT
as O
a O
recurrent O
neural O

network B-DAT
(RNN). O
Intuitively, O
we O
can O

understand B-DAT
this O
model O
as O
splitting O

the B-DAT
state O
into O
a O
stochastic O

part B-DAT
st O
and O
a O
de O

- B-DAT
terministic O
part O
ht, O
which O

depend B-DAT
on O
the O
stochastic O
and O

deter- B-DAT
ministic O
parts O
at O
the O

previous B-DAT
time O
step O
through O
the O

RNN. B-DAT
We O
use O
the O
encoder O

q(s1:T B-DAT
| O
o1:T O
, O

a1:T B-DAT
) O
= O
∏T O
t=1 O
q(st O
| O
ht O

, B-DAT
ot) O
to O
parameterize O
the O
approximate O
state O

posteriors. B-DAT
Impor- O
tantly, O
all O
information O

about B-DAT
the O
observations O
must O
pass O

through B-DAT
the O
sampling O
step O
of O

the B-DAT
encoder O
to O
avoid O
a O

deter- B-DAT
ministic O
shortcut O
from O
inputs O

to B-DAT
reconstructions O

. B-DAT
In O
the O
next O
section, O
we O

identify B-DAT
a O
limitation O
of O
the O

standard B-DAT
objective O
for O
latent O
sequence O

models B-DAT
and O
propose O
a O
general O

- B-DAT
ization O
of O
it O
that O

improves B-DAT
long-term O
predictions. O
4. O
Latent O
Overshooting O
In O
the O

previous B-DAT
section, O
we O
derived O
the O

typical B-DAT
variational O
bound O
for O
learning O

and B-DAT
inference O
in O
latent O
sequence O

models B-DAT
(Equation O
3). O
As O
show O

in B-DAT
Figure O
3a, O
this O
objective O

function B-DAT
contains O
reconstruction O
terms O
for O

the B-DAT
observations O
and O
KL- O
divergence O

regularizers B-DAT
for O
the O
approximate O
posteriors O

. B-DAT
A O
limitation O
of O
this O

objective B-DAT
is O
that O
the O
stochastic O

path B-DAT
of O
the O
transition O
function O

p(st B-DAT
| O
st−1, O
at−1) O
is O

only B-DAT
trained O
via O
the O
KL-divergence O

regularizers B-DAT
for O
one-step O
predictions: O
the O

gra- B-DAT
dient O
flows O
through O

p(st B-DAT
| O
st−1, O
at−1) O
directly O

into B-DAT
q(st−1) O
but O
never O
traverses O

a B-DAT
chain O
of O
multiple O

p(st B-DAT
| O
st−1, O
at−1). O
In O

this B-DAT
section, O
we O
generalize O
this O

variational B-DAT
bound O
to O
latent O
overshooting, O

which B-DAT
trains O
all O
multi-step O
predictions O

in B-DAT
la- O
tent O
space. O
We O

found B-DAT
that O
several O
dynamics O
models O

benefit B-DAT
4 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
o1, O
r1 O
o2, O
r2 O
o3 O

, B-DAT
r3 O
s1|1 O
s2|2 O
s3|3 O

s2|1 B-DAT
s3|2 O
(a) O
Standard O
variational O
bound O

o1, B-DAT
r1 O
o2, O
r2 O
o3, O

r3 B-DAT
s1|1 O
s2|2 O
s3|3 O

s2|1 B-DAT
s3|2 O
s3|1 O

b) B-DAT
Observation O
overshooting O
o1, O
r1 O
o2, O
r2 O
o3 O

, B-DAT
r3 O
s1|1 O
s2|2 O
s3|3 O

s2|1 B-DAT
s3|2 O
s3|1 O

c) B-DAT
Latent O
overshooting O
Figure O
3: O
Unrolling O
schemes. O
The O

labels B-DAT
si|j O
are O
short O
for O

the B-DAT
state O
at O
time O
i O

conditioned B-DAT
on O
observations O
up O
to O

time B-DAT
j. O
Arrows O
pointing O
at O

shaded B-DAT
circles O
indicate O
log-likelihood O
loss O

terms. B-DAT
Wavy O
arrows O
indicate O
KL-divergence O

loss B-DAT
terms. O
(a) O
The O
standard O

variational B-DAT
objectives O
decodes O
the O
posterior O

at B-DAT
every O
step O
to O
compute O

the B-DAT
reconstruction O
loss. O
It O
also O

places B-DAT
a O
KL O
on O
the O

prior B-DAT
and O
posterior O
at O
every O

step, B-DAT
which O
trains O
the O
transition O

function B-DAT
for O
one-step O
predictions. O
(b O

) B-DAT
Observation O
overshooting O
(Amos O
et O

al., B-DAT
2018) O
decodes O
all O
multi-step O

predictions B-DAT
to O
apply O
additional O
reconstruction O

losses. B-DAT
This O
is O
typically O
too O

expensive B-DAT
in O
image O
domains. O
(c) O

Latent B-DAT
overshooting O
predicts O
all O
multi-step O

priors. B-DAT
These O
state O
beliefs O
are O

trained B-DAT
towards O
their O
corresponding O
posteriors O

in B-DAT
latent O
space O
to O
encourage O

accurate B-DAT
multi-step O
predictions. O
from O
latent O
overshooting, O
although O
our O

final B-DAT
agent O
using O
the O
RSSM O

model B-DAT
does O
not O
require O
it O

(see B-DAT
Appendix O
D O

). B-DAT
Limited O
capacity O
If O
we O
could O

train B-DAT
our O
model O
to O
make O

per- B-DAT
fect O
one-step O
predictions, O
it O

would B-DAT
also O
make O
perfect O
multi O

- B-DAT
step O
predictions, O
so O
this O

would B-DAT
not O
be O
a O
problem. O

However, B-DAT
when O
using O
a O
model O

with B-DAT
limited O
capacity O
and O
restricted O

distributional B-DAT
family, O
training O
the O
model O

only B-DAT
on O
one-step O
predictions O
until O

convergence B-DAT
does O
in O
general O
not O

coincide B-DAT
with O
the O
model O
that O

is B-DAT
best O
at O
multi-step O
predictions. O

For B-DAT
suc- O
cessful O
planning, O
we O

need B-DAT
accurate O
multi-step O
predictions. O
Therefore, O

we B-DAT
take O
inspiration O
from O
Amos O

et B-DAT
al. O
(2018) O
and O
earlier O

related B-DAT
ideas O
(Krishnan O
et O
al., O
2015 O

; B-DAT
Lamb O
et O
al., O
2016; O

Chiappa B-DAT
et O
al., O
2017), O
and O

train B-DAT
the O
model O
on O
multi- O

step B-DAT
predictions O
of O
all O
distances. O

We B-DAT
develop O
this O
idea O
for O

latent B-DAT
sequence O
models, O
showing O
that O

multi-step B-DAT
predictions O
can O
be O
improved O

by B-DAT
a O
loss O
in O
latent O

space, B-DAT
without O
having O
to O
generate O

additional B-DAT
images. O
Multi-step O
prediction O
We O
start O
by O

generalizing B-DAT
the O
stan- O
dard O
variational O

bound B-DAT
(Equation O
3) O
from O
training O

one-step B-DAT
predictions O
to O
training O
multi-step O

predictions B-DAT
of O
a O
fixed O
dis O

- B-DAT
tance O
d. O
For O
ease O

of B-DAT
notation, O
we O
omit O
actions O

in B-DAT
the O
con- O
ditioning O
set O

here; B-DAT
every O
distribution O
over O
st O

is B-DAT
conditioned O
upon O
a<t. O
We O

first B-DAT
define O
multi-step O
predictions, O
which O

are B-DAT
computed O
by O
repeatedly O
applying O

the B-DAT
transition O
model O
and O
integrating O

out B-DAT
the O
intermediate O
states, O
p(st O
| O
st−d O

) B-DAT
, O
∫ O
t∏ O
τ=t−d+1 O

p(sτ B-DAT
| O
sτ−1) O
dst−d+1:t−1 O
= O
Ep(st−1|st−d)[p(st O
| O
st−1 O

)]. B-DAT
(5 O

) B-DAT
The O
case O
d O
= O
1 O

recovers B-DAT
the O
one-step O
transitions O
used O

in B-DAT
the O
original O
model. O
Given O

this B-DAT
definition O
of O
a O
multi-step O

predic B-DAT

- B-DAT
tion, O
we O
generalize O
Equation O
3 O

to B-DAT
the O
variational O
bound O
on O

the B-DAT
multi-step O
predictive O
distribution O
pd O

, B-DAT
ln O
pd(o1:T O
) O
, O
ln O

∫ B-DAT
T O

∏ B-DAT
t=1 O

p(st B-DAT
| O
st−d)p(ot O
| O
st) O

ds1:T B-DAT
≥ O
T∑ O
t=1 O

Eq(st|o≤t)[ln B-DAT
p(ot O
| O
st)] O
reconstruction O

E B-DAT
p(st−1|st−d)q(st−d|o≤t−d) O
[ O
KL[q(st O
| O
o≤t O

) B-DAT
‖ O
p(st O
| O
st−1)] O
] O
multi-step O
prediction O

6) B-DAT
For O
the O
derivation, O
please O
see O

Equation B-DAT
9 O
in O
the O
appendix O

. B-DAT
Maximizing O
this O
objective O
trains O

the B-DAT
multi-step O
predictive O
distribution. O
This O

reflects B-DAT
the O
fact O
that O
during O

planning, B-DAT
the O
model O
makes O
predictions O

without B-DAT
having O
access O
to O
all O

the B-DAT
preceding O
observations. O
We O
conjecture O
that O
Equation O
6 O

is B-DAT
also O
a O
lower O
bound O

on B-DAT
ln O
p(o1:T O
) O
based O

on B-DAT
the O
data O
processing O
inequality O

. B-DAT
Since O
the O
latent O
state O

sequence B-DAT
is O
Markovian, O
for O

d B-DAT
≥ O
1 O
we O
have O

I(st; B-DAT
st−d) O
≤ O
I(st; O
st−1) O

and B-DAT
thus O
E[ln O

pd(o1:T B-DAT
)] O
≤ O
E[ln O

p(o1:T B-DAT
)]. O
Hence, O
every O
bound O

on B-DAT
the O
multi-step O
predic- O
tive O

distribution B-DAT
is O
also O
a O
bound O

on B-DAT
the O
one-step O
predictive O
distribution O

in B-DAT
expectation O
over O
the O
data O

set. B-DAT
For O
details, O
please O
see O

Equation B-DAT
10 O
in O
the O
appendix. O

In B-DAT
the O
next O
para- O
graph, O

we B-DAT
alleviate O
the O
limitation O
that O

a B-DAT
particular O
pd O
only O
trains O

predictions B-DAT
of O
one O
distance O
and O

arrive B-DAT
at O
our O
final O
objective. O
Latent O
overshooting O
We O
introduced O
a O

bound B-DAT
on O
predic- O
tions O
of O

a B-DAT
given O
distance O
d. O
However O

, B-DAT
for O
planning O
we O
need O

accurate B-DAT
predictions O
not O
just O
for O

a B-DAT
fixed O
distance O
but O
for O

all B-DAT
distances O
up O
to O
the O

planning B-DAT
horizon. O
We O
introduce O
la- O

tent B-DAT
overshooting O
for O
this, O
an O

objective B-DAT
function O
for O
latent O
sequence O

models B-DAT
that O
generalizes O
the O
standard O

variational B-DAT
bound O
(Equation O
3) O
to O

train B-DAT
the O
model O
on O
multi-step O

predic- B-DAT
5 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
tions O
of O
all O
distances O
1 O

≤ B-DAT
d O
≤ O
D O

, B-DAT
1 O

D B-DAT
D∑ O
d=1 O

ln B-DAT
pd(o1:T O
) O
≥ O
T∑ O
t=1 O

Eq(st|o≤t)[ln B-DAT
p(ot O
| O
st)] O
reconstruction O

1 B-DAT
D O
D∑ O
d=1 O

βd B-DAT
E O
p(st−1|st−d)q(st−d|o≤t−d) O
[ O
KL[q(st O
| O
o≤t O

) B-DAT
‖ O
p(st O
| O
st−1)] O
] O
latent O
overshooting O

7) B-DAT
Latent O
overshooting O
can O
be O
interpreted O

as B-DAT
a O
regularizer O
in O
latent O

space B-DAT
that O
encourages O
consistency O
between O

one-step B-DAT
and O
multi-step O
predictions, O
which O

we B-DAT
know O
should O
be O
equiv O

- B-DAT
alent O
in O
expectation O
over O

the B-DAT
data O
set. O
We O
include O

weighting B-DAT
factors O
{βd}Dd=1 O
analogously O
to O

the B-DAT
β-VAE O
(Higgins O
et O
al., O
2016 O

). B-DAT
While O
we O
set O
all O

β>1 B-DAT
to O
the O
same O
value O

for B-DAT
sim- O
plicity, O
they O
could O

be B-DAT
chosen O
to O
let O
the O

model B-DAT
focus O
more O
on O
long-term O

or B-DAT
short-term O
predictions. O
In O
practice, O

we B-DAT
stop O
gradients O
of O
the O

posterior B-DAT
distributions O
for O
overshoot- O
ing O

distances B-DAT
d O
> O
1, O
so O

that B-DAT
the O
multi-step O
predictions O
are O

trained B-DAT
towards O
the O
informed O
posteriors, O

but B-DAT
not O
the O
other O
way O

around. B-DAT
5. O
Experiments O
We O
evaluate O
PlaNet O

on B-DAT
six O
continuous O
control O
tasks O

from B-DAT
pixels. O
We O
explore O
multiple O

design B-DAT
axes O
of O
the O
agent O

: B-DAT
the O
stochastic O
and O
deterministic O

paths B-DAT
in O
the O
dynamics O
model, O

iterative B-DAT
planning, O
and O
online O
experience O

collection. B-DAT
We O
refer O
to O
the O

appendix B-DAT
for O
hyper O
parameters O
(Appendix O

A) B-DAT
and O
additional O
experiments O
(Appendices O

C B-DAT
to O
E). O
Besides O
the O

action B-DAT
repeat, O
we O
use O
the O

same B-DAT
hyper O
parameters O
for O
all O

tasks. B-DAT
Within O
less O
than O
one O

hundredth B-DAT
the O
episodes, O
PlaNet O
outperforms O

A3C B-DAT
(Mnih O
et O
al., O
2016) O

and B-DAT
achieves O
sim- O
ilar O
performance O

to B-DAT
the O
top O
model-free O
algorithm O

D4PG B-DAT
(Barth-Maron O
et O
al., O
2018). O

The B-DAT
training O
time O
of O
10 O

to B-DAT
20 O
hours O
(depending O
on O

the B-DAT
task) O
on O
a O
single O

Nvidia B-DAT
V100 O
GPU O
compares O
favorably O

to B-DAT
that O
of O
A3C O
and O

D4PG. B-DAT
Our O
implementation O
uses O
TensorFlow O

Probability B-DAT
(Dillon O
et O
al., O
2017). O

Please B-DAT
visit O
https://danijar.com/planet O
for O
access O

to B-DAT
the O
code O
and O
videos O

of B-DAT
the O
trained O
agent. O
For O
our O
evaluation, O
we O
consider O

six B-DAT
image-based O
continuous O
control O
tasks O

of B-DAT
the O
DeepMind O
control O
suite O

(Tassa B-DAT
et O
al., O
2018), O
shown O

in B-DAT
Figure O
1. O
These O
environments O

provide B-DAT
qualitatively O
different O
challenges. O
The O

cartpole B-DAT
swingup O
task O
requires O
a O

long B-DAT
planning O
horizon O
and O
to O

memorize B-DAT
the O
cart O
when O
it O

is B-DAT
out O
of O
view, O
reacher O

has B-DAT
a O
sparse O
reward O
given O

when B-DAT
the O
hand O
and O
goal O

area B-DAT
overlap, O
finger O
spinning O
includes O

contact B-DAT
dynamics O
between O
the O
finger O

and B-DAT
the O
object, O
cheetah O
exhibits O

larger B-DAT
state O
and O
action O
spaces O

, B-DAT
the O
cup O
task O
only O

has B-DAT
a O
sparse O
reward O
for O

when B-DAT
the O
ball O
is O
caught, O

and B-DAT
the O
walker O
is O
challenging O

because B-DAT
the O
robot O
first O
has O

to B-DAT
stand O
up O
and O
then O

walk, B-DAT
resulting O
in O
collisions O
with O

the B-DAT
ground O
that O
are O
difficult O

to B-DAT
predict. O
In O
all O
tasks, O

the B-DAT
only O
observations O
are O
third-person O

camera B-DAT
images O
of O
size O
64× O
64 O

× B-DAT
3 O
pixels. O
Comparison O
to O
model-free O
methods O
Figure O

4 B-DAT
compares O
the O
performance O
of O

PlaNet B-DAT
to O
the O
model-free O
algorithms O

re- B-DAT
ported O
by O
Tassa O
et O

al. B-DAT
(2018). O
Within O
100 O
episodes O

, B-DAT
PlaNet O
outperforms O
the O
policy-gradient O

method B-DAT
A3C O
trained O
from O
proprioceptive O

states B-DAT
for O
100,000 O
episodes, O
on O

all B-DAT
tasks. O
Af- O
ter O
500 O

episodes, B-DAT
it O
achieves O
performance O
similar O

to B-DAT
D4PG, O
trained O
from O
images O

for B-DAT
100,000 O
episodes, O
except O
for O

the B-DAT
finger O
task. O
PlaNet O
surpasses O

the B-DAT
final O
performance O
of O
D4PG O

with B-DAT
a O
relative O
improvement O
of O
26 O

% B-DAT
on O
the O
cheetah O
running O

task. B-DAT
We O
refer O
to O
Table O
1 O
for O
numerical O
results, O
which O
also O

includes B-DAT
the O
performance O
of O
CEM O

planning B-DAT
with O
the O
true O
dynamics O

of B-DAT
the O
simulator O

. B-DAT
Model O
designs O
Figure O
4 O
additionally O

compares B-DAT
design O
choices O
of O
the O

dynamics B-DAT
model. O
We O
train O
PlaNet O

using B-DAT
our O
recurrent O
state-space O
model O

(RSSM), B-DAT
as O
well O
as O
ver O

- B-DAT
sions O
with O
purely O
deterministic O

GRU B-DAT
(Cho O
et O
al., O
2014), O

and B-DAT
purely O
stochastic O
state-space O
model O
( O

SSM). B-DAT
We O
observe O
the O
importance O

of B-DAT
both O
stochastic O
and O
deterministic O

elements B-DAT
in O
the O
transition O
function O

on B-DAT
all O
tasks. O
The O
deterministic O

part B-DAT
allows O
the O
model O
to O

remember B-DAT
information O
over O
many O
time O

steps. B-DAT
The O
stochastic O
component O
is O

even B-DAT
more O
impor- O
tant O
– O

the B-DAT
agent O
does O
not O
learn O

without B-DAT
it. O
This O
could O
be O

because B-DAT
the O
tasks O
are O
stochastic O

from B-DAT
the O
agent’s O
perspective O
due O

to B-DAT
partial O
observability O
of O
the O

initial B-DAT
states. O
The O
noise O
might O

also B-DAT
add O
a O
safety O
margin O

to B-DAT
the O
planning O
objective O
that O

results B-DAT
in O
more O
robust O
action O

sequences. B-DAT
Agent O
designs O
Figure O
5 O
compares O

PlaNet, B-DAT
a O
version O
col- O
lecting O

episodes B-DAT
under O
random O
actions O
rather O

than B-DAT
by O
plan- O
ning, O
and O

a B-DAT
version O
that O
at O
each O

environment B-DAT
step O
selects O
the O
best O

action B-DAT
out O
of O
1000 O
sequences O

rather B-DAT
than O
iteratively O
re- O
fining O

plans B-DAT
via O
CEM. O
We O
observe O

that B-DAT
online O
data O
collection O
helps O

for B-DAT
all O
tasks O
and O
is O

necessary B-DAT
for O
the O
cartpole, O
finger O

, B-DAT
and O
walker O
tasks. O
Iterative O

search B-DAT
for O
action O
sequences O
using O

CEM B-DAT
improves O
performance O
on O
all O

tasks. B-DAT
One O
agent O
all O
tasks O
Figure O

7 B-DAT
in O
the O
appendix O
shows O

the B-DAT
performance O
of O
a O
single O

agent B-DAT
trained O
on O
all O
six O

tasks. B-DAT
The O
agent O
is O
not O

told B-DAT
which O
task O
it O
is O

facing; B-DAT
it O
needs O
to O
infer O

this B-DAT
from O
the O
image O
observations O

. B-DAT
We O
pad O
the O
action O

spaces B-DAT
with O
unused O
elements O
to O

make B-DAT
them O
compatible O
and O
adapt O

Algorithm B-DAT
1 O
to O
collect O
one O

episode B-DAT
of O
each O
task O
every O

C B-DAT
update O
steps. O
We O
use O

the B-DAT
same O
hyper O
parameters O
as O

for B-DAT
the O
main O
experiments O
above. O

The B-DAT
agent O
solves O
all O
tasks O

while B-DAT
learning O
slower O
compared O
to O

individually B-DAT
trained O
agents. O
This O
indicates O

that B-DAT
the O
model O
can O
learn O

to B-DAT
predict O
multiple O
domains, O
regardless O

of B-DAT
the O
conceptually O
different O
visuals. O
6. O
Related O
Work O
Previous O
work O

in B-DAT
model-based O
reinforcement O
learning O
has O

focused B-DAT
on O
planning O
in O
low-dimensional O

state B-DAT
spaces O
(Gal O
et O
al O

., B-DAT
2016; O
Higuera O
et O
al., O
2018 O

; B-DAT
Henaff O
et O
al., O
2018; O
6 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
Cartpole O
Swing O
Up O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Reacher O
Easy O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cheetah O
Run O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Finger O
Spin O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cup O
Catch O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Walker O
Walk O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
PlaNet O
(RSSM) O
Stochastic O
(SSM) O
Deterministic O

(GRU B-DAT

) B-DAT
D4PG O
(100k O
episodes) O
A3C O
(100k O

episodes, B-DAT
proprio O

) B-DAT
Figure O
4: O
Comparison O
of O
PlaNet O

to B-DAT
model-free O
algorithms O
and O
other O

model B-DAT
designs. O
Plots O
show O
test O

performance B-DAT
over O
the O
number O
of O

collected B-DAT
episodes. O
We O
compare O
PlaNet O

using B-DAT
our O
RSSM O
(Section O
3 O

) B-DAT
to O
purely O
deterministic O
(GRU) O

and B-DAT
purely O
stochastic O
models O
(SSM). O

The B-DAT
RNN O
does O
not O
use O

latent B-DAT
overshooting, O
as O
it O
does O

not B-DAT
have O
stochastic O
latents. O
The O

lines B-DAT
show O
medians O
and O
the O

areas B-DAT
show O
percentiles O
5 O
to O
95 O
over O
5 O
seeds O
and O
10 O

trajectories. B-DAT
The O
shaded O
areas O
are O

large B-DAT
on O
two O
of O
the O

tasks B-DAT
due O
to O
the O
sparse O

rewards B-DAT

. B-DAT
Table O
1: O
Comparison O
of O
PlaNet O

to B-DAT
the O
model-free O
algorithms O
A3C O

and B-DAT
D4PG O
reported O
by O
Tassa O

et B-DAT
al. O
(2018). O
The O
training O

curves B-DAT
for O
these O
are O
shown O

as B-DAT
orange O
lines O
in O
Figure O

4 B-DAT
and O
as O
solid O
green O

lines B-DAT
in O
Figure O
6 O
in O

their B-DAT
paper. O
From O
these, O
we O

estimate B-DAT
the O
number O
of O
episodes O

that B-DAT
D4PG O
takes O
to O
achieve O

the B-DAT
final O
performance O
of O
PlaNet O

to B-DAT
estimate O
the O
data O
efficiency O

gain. B-DAT
We O
further O
include O
CEM O

planning B-DAT
(H O
= O
12, O
I O

= B-DAT
10, O
J O
= O
1000,K O

= B-DAT
100) O
with O
the O
true O

simulator B-DAT
instead O
of O
learned O
dynamics O

as B-DAT
an O
estimated O
upper O
bound O

on B-DAT
performance. O
Numbers O
indicate O
mean O

final B-DAT
performance O
over O
5 O
seeds O

and B-DAT
10 O
trajectories O

. B-DAT
Method O
Modality O
Episodes O
C O
ar O

tp B-DAT
ol O
e O
Sw O

in B-DAT
g O
U O
p O

R B-DAT
ea O
ch O
er O

E B-DAT
as O
y O

C B-DAT
he O
et O
ah O

R B-DAT
un O
Fi O
ng O

er B-DAT
Sp O
in O

C B-DAT
up O
C O
at O

ch B-DAT
W O
al O

ke B-DAT
r O
W O
al O

k B-DAT
A3C O
proprioceptive O
100,000 O
558 O
285 O

214 B-DAT
129 O
105 O
311 O
D4PG O

pixels B-DAT
100,000 O
862 O
967 O
524 O

985 B-DAT
980 O
968 O
PlaNet O
(ours O

) B-DAT
pixels O
1,000 O
821 O
832 O
662 O
700 O
930 O
951 O
CEM O

+ B-DAT
true O
simulator O
simulator O
state O

0 B-DAT
850 O
964 O
656 O
825 O

993 B-DAT
994 O

Data B-DAT
efficiency O
gain O
PlaNet O
over O

D4PG B-DAT
(factor) O
250 O
40 O
500+ O
300 O
100 O
90 O

Chua B-DAT
et O
al., O
2018), O
combining O

the B-DAT
benefits O
of O
model-based O
and O

model-free B-DAT
approaches O
(Kalweit O
& O
Boedecker, O
2017 O

; B-DAT
Nagabandi O
et O
al., O
2017; O

Weber B-DAT
et O
al., O
2017; O
Kurutach O

et B-DAT
al., O
2018; O
Buckman O
et O

al., B-DAT
2018; O
Ha O
& O
Schmidhuber, O
2018 O

; B-DAT
Wayne O
et O
al., O
2018; O

Igl B-DAT
et O
al., O
2018; O
Srinivas O

et B-DAT
al., O
2018), O
and O
pure O

video B-DAT
prediction O
without O
planning O
(Oh O

et B-DAT
al., O
2015; O
Krishnan O
et O

al., B-DAT
2015; O
Karl O
et O
al., O
2016 O

; B-DAT
Chiappa O
et O
al., O
2017; O

Babaeizadeh B-DAT
et O
al., O
2017; O
Gemici O

et B-DAT
al., O
2017; O
Denton O
& O

Fergus, B-DAT
2018; O
Buesing O
et O
al., O
2018 O

; B-DAT
Doerr O
et O
al., O
2018; O

Gre- B-DAT
gor O
& O
Besse, O
2018). O
Appendix O

G B-DAT
reviews O
these O
orthogonal O
research O

directions B-DAT
in O
more O
detail O

. B-DAT
Relatively O
few O
works O
have O
demonstrated O

successful B-DAT
plan- O
ning O
from O
pixels O

using B-DAT
learned O
dynamics O
models. O
The O

robotics B-DAT
community O
focuses O
on O
video O

prediction B-DAT
models O
for O
planning O
(Agrawal O

et B-DAT
al., O
2016; O
Finn O

& B-DAT
Levine, O
2017; O
Ebert O
et O

al., B-DAT
2018; O
Zhang O
et O
al O

., B-DAT
2018) O
that O
deal O
with O

the B-DAT
visual O
complexity O
of O
the O

real B-DAT
world O
and O
solve O
tasks O

with B-DAT
7 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
Cartpole O
Swing O
Up O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Reacher O
Easy O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cheetah O
Run O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Finger O
Spin O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cup O
Catch O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Walker O
Walk O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
PlaNet O
Random O
collection O
Random O
shooting O

D4PG B-DAT
(100k O
episodes) O
A3C O
(100k O

episodes, B-DAT
proprio) O
Figure O
5: O
Comparison O
of O
agent O

designs. B-DAT
Plots O
show O
test O
performance O

over B-DAT
the O
number O
of O
collected O

episodes. B-DAT
We O
compare O
PlaNet, O
a O

version B-DAT
that O
collects O
data O
under O

random B-DAT
actions O
(random O
collection), O
and O

a B-DAT
version O
that O
chooses O
the O

best B-DAT
action O
out O
of O
1000 O

sequences B-DAT
at O
each O
environment O
step O

(random B-DAT
shooting) O
without O
iteratively O
refining O

plans B-DAT
via O
CEM. O
The O
lines O

show B-DAT
medians O
and O
the O
areas O

show B-DAT
percentiles O
5 O
to O
95 O

over B-DAT
5 O
seeds O
and O
10 O

trajectories B-DAT

. B-DAT
a O
simple O
gripper, O
such O
as O

grasping B-DAT
or O
pushing O
objects. O
In O

comparison, B-DAT
we O
focus O
on O
simulated O

environments, B-DAT
where O
we O
leverage O
latent O

planning B-DAT
to O
scale O
to O
larger O

state B-DAT
and O
ac- O
tion O
spaces O

, B-DAT
longer O
planning O
horizons, O
as O

well B-DAT
as O
sparse O
reward O
tasks. O

E2C B-DAT
(Watter O
et O
al., O
2015) O

and B-DAT
RCE O
(Banija- O
mali O
et O

al., B-DAT
2017) O
embed O
images O
into O

a B-DAT
latent O
space, O
where O
they O

learn B-DAT
local-linear O
latent O
transitions O
and O

plan B-DAT
for O
actions O
using O
LQR. O

These B-DAT
methods O
balance O
simulated O
cartpoles O

and B-DAT
control O
2-link O
arms O
from O

images, B-DAT
but O
have O
been O
difficult O

to B-DAT
scale O
up. O
We O
lift O

the B-DAT
Markov O
assumption O
of O
these O

models, B-DAT
making O
our O
method O
applicable O

under B-DAT
partial O
observability, O
and O
present O

results B-DAT
on O
more O
challenging O
environments O

that B-DAT
include O
longer O
planning O
horizons, O

contact B-DAT
dynamics, O
and O
sparse O
rewards. O
7. O
Discussion O
We O
present O
PlaNet O

, B-DAT
a O
model-based O
agent O
that O

learns B-DAT
a O
la- O
tent O
dynamics O

model B-DAT
from O
image O
observations O
and O

chooses B-DAT
actions O
by O
fast O
planning O

in B-DAT
latent O
space. O
To O
enable O

accu- B-DAT
rate O
long-term O
predictions, O
we O

design B-DAT
a O
model O
with O
both O

stochastic B-DAT
and O
deterministic O
paths. O
We O

show B-DAT
that O
our O
agent O
succeeds O

at B-DAT
several O
continuous O
control O
tasks O

from B-DAT
image O
observations, O
reaching O
performance O

that B-DAT
is O
comparable O
to O
the O

best B-DAT
model-free O
algorithms O
while O
using O
200 O

× B-DAT
fewer O
episodes O
and O
similar O

or B-DAT
less O
computation O
time. O
The O

results B-DAT
show O
that O
learning O
latent O
dynamics O

models B-DAT
for O
planning O
in O
image O

domains B-DAT
is O
a O
promising O
approach O

. B-DAT
Directions O
for O
future O
work O
include O

learning B-DAT
temporal O
ab- O
straction O
instead O

of B-DAT
using O
a O
fixed O
action O

repeat, B-DAT
possibly O
through O
hierarchical O
models O

. B-DAT
To O
further O
improve O
final O

per- B-DAT
formance, O
one O
could O
learn O

a B-DAT
value O
function O
to O
approximate O

the B-DAT
sum O
of O
rewards O
beyond O

the B-DAT
planning O
horizon. O
Moreover, O
gradient-based O

planning B-DAT
could O
increase O
the O
computational O

efficiency B-DAT
of O
the O
agent O
and O

learning B-DAT
representations O
without O
reconstruction O
could O

help B-DAT
to O
solve O
tasks O
with O

higher B-DAT
visual O
diversity. O
Our O
work O

provides B-DAT
a O
starting O
point O
for O

multi-task B-DAT
control O
by O
sharing O
the O

dynamics B-DAT
model. O
Acknowledgements O
We O
thank O
Jacob O
Buckman O

, B-DAT
Nicolas O
Heess, O
John O
Schulman, O

Rishabh B-DAT
Agarwal, O
Silviu O
Pitis, O
Mohammad O

Norouzi, B-DAT
George O
Tucker, O
David O
Duvenaud, O

Shane B-DAT
Gu, O
Chelsea O
Finn, O
Steven O

Bohez, B-DAT
Jimmy O
Ba, O
Stephanie O
Chan, O

and B-DAT
Jenny O
Liu O
for O
helpful O

discussions. B-DAT
8 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
References O

Agrawal, B-DAT
P., O
Nair, O
A. O
V., O

Abbeel, B-DAT
P., O
Malik, O
J., O
and O

Levine, B-DAT
S. O
Learning O
to O
poke O

by B-DAT
poking: O
Experiential O
learning O
of O

intuitive B-DAT
physics. O
In O
Advances O
in O

Neural B-DAT
Information O
Processing O
Systems, O
pp. O
5074 O

–5082, B-DAT
2016. O
Amos, O
B., O
Dinh, O
L., O
Cabi O

, B-DAT
S., O
Rothörl, O
T., O
Muldal, O

A., B-DAT
Erez, O
T., O
Tassa, O
Y., O

de B-DAT
Freitas, O
N., O
and O
Denil, O

M. B-DAT
Learning O
awareness O
models. O
In O

International B-DAT
Conference O
on O
Learn- O
ing O

Representations, B-DAT
2018. O
Babaeizadeh, O
M., O
Finn, O
C., O
Erhan O

, B-DAT
D., O
Campbell, O
R. O
H., O

and B-DAT
Levine, O
S. O
Stochastic O
variational O

video B-DAT
prediction. O
arXiv O
preprint O
arXiv:1710.11252, O
2017 O

. B-DAT
Banijamali, O
E., O
Shu, O
R., O
Ghavamzadeh O

, B-DAT
M., O
Bui, O
H., O
and O

Ghodsi, B-DAT
A. O
Robust O
locally-linear O
controllable O

embedding. B-DAT
arXiv O
preprint O
arXiv:1710.05373, O
2017. O
Barth-Maron, O
G., O
Hoffman, O
M. O
W O

., B-DAT
Budden, O
D., O
Dabney, O
W., O

Horgan, B-DAT
D., O
Muldal, O
A., O
Heess, O

N., B-DAT
and O
Lillicrap, O
T. O
Distributed O

distributional B-DAT
deterministic O
policy O
gradients. O
arXiv O

preprint B-DAT
arXiv:1804.08617, O
2018. O
Bengio, O
S., O
Vinyals, O
O., O
Jaitly O

, B-DAT
N., O
and O
Shazeer, O
N. O

Sched- B-DAT
uled O
sampling O
for O
sequence O

prediction B-DAT
with O
recurrent O
neu- O
ral O

networks. B-DAT
In O
Advances O
in O
Neural O

Information B-DAT
Process- O
ing O
Systems, O
pp. O
1171 O

–1179, B-DAT
2015. O
Buckman, O
J., O
Hafner, O
D., O
Tucker O

, B-DAT
G., O
Brevdo, O
E., O
and O

Lee, B-DAT
H. O
Sample-efficient O
reinforcement O
learning O

with B-DAT
stochastic O
ensemble O
value O
expansion. O

arXiv B-DAT
preprint O
arXiv:1807.01675, O
2018. O
Buesing, O
L., O
Weber, O
T., O
Racaniere O

, B-DAT
S., O
Eslami, O
S., O
Rezende, O

D., B-DAT
Reichert, O
D. O
P., O
Viola, O

F., B-DAT
Besse, O
F., O
Gregor, O
K., O

Hassabis, B-DAT
D., O
et O
al. O
Learning O

and B-DAT
querying O
fast O
gener- O
ative O

models B-DAT
for O
reinforcement O
learning. O
arXiv O

preprint B-DAT
arXiv:1802.03006, O
2018. O
Chiappa, O
S., O
Racaniere, O
S., O
Wierstra O

, B-DAT
D., O
and O
Mohamed, O
S. O

Recurrent B-DAT
environment O
simulators. O
arXiv O
preprint O

arXiv:1704.02254, B-DAT
2017. O
Cho, O
K., O
Van O
Merriënboer, O
B O

., B-DAT
Gulcehre, O
C., O
Bahdanau, O
D., O

Bougares, B-DAT
F., O
Schwenk, O
H., O
and O

Bengio, B-DAT
Y. O
Learn- O
ing O
phrase O

representations B-DAT
using O
rnn O
encoder-decoder O
for O

statistical B-DAT
machine O
translation. O
arXiv O
preprint O

arXiv:1406.1078, B-DAT
2014. O
Chua, O
K., O
Calandra, O
R., O
McAllister O

, B-DAT
R., O
and O
Levine, O
S. O

Deep B-DAT
reinforcement O
learning O
in O
a O

handful B-DAT
of O
trials O
us- O
ing O

probabilistic B-DAT
dynamics O
models. O
arXiv O
preprint O

arXiv:1805.12114, B-DAT
2018. O
Chung, O
J., O
Kastner, O
K., O
Dinh O

, B-DAT
L., O
Goel, O
K., O
Courville, O

A. B-DAT
C., O
and O
Bengio, O
Y. O

A B-DAT
recurrent O
latent O
variable O
model O

for B-DAT
sequential O
data. O
In O
Advances O

in B-DAT
neural O
information O
pro- O
cessing O

systems, B-DAT
pp. O
2980–2988, O
2015. O
Clevert, O
D.-A., O
Unterthiner, O
T., O
and O

Hochreiter, B-DAT
S. O
Fast O
and O
accurate O

deep B-DAT
network O
learning O
by O
exponential O

linear B-DAT
units O
(elus). O
arXiv O
preprint O

arXiv:1511.07289, B-DAT
2015 O

. B-DAT
Deisenroth, O
M. O
and O
Rasmussen, O
C O

. B-DAT
E. O
Pilco: O
A O
model-based O

and B-DAT
data-efficient O
approach O
to O
policy O

search. B-DAT
In O
Proceed- O
ings O
of O

the B-DAT
28th O
International O
Conference O
on O

machine B-DAT
learning O
(ICML-11), O
pp. O
465–472, O
2011 O

. B-DAT
Denton, O
E. O
and O
Fergus, O
R O

. B-DAT
Stochastic O
video O
generation O
with O

a B-DAT
learned O
prior. O
arXiv O
preprint O

arXiv:1802.07687, B-DAT
2018. O
Dillon, O
J. O
V., O
Langmore, O
I O

., B-DAT
Tran, O
D., O
Brevdo, O
E., O

Vasudevan, B-DAT
S., O
Moore, O
D., O
Patton, O

B., B-DAT
Alemi, O
A., O
Hoffman, O
M., O

and B-DAT
Saurous, O
R. O
A. O
Tensorflow O

distributions. B-DAT
arXiv O
preprint O
arXiv:1711.10604, O
2017. O
Doerr, O
A., O
Daniel, O
C., O
Schiegg O

, B-DAT
M., O
Nguyen-Tuong, O
D., O
Schaal, O

S., B-DAT
Toussaint, O
M., O
and O
Trimpe, O

S. B-DAT
Proba- O
bilistic O
recurrent O
state-space O

models. B-DAT
arXiv O
preprint O
arXiv:1801.10395, O
2018. O
Ebert, O
F., O
Finn, O
C., O
Dasari O

, B-DAT
S., O
Xie, O
A., O
Lee, O

A., B-DAT
and O
Levine, O
S. O
Visual O

foresight: B-DAT
Model-based O
deep O
reinforcement O
learning O

for B-DAT
vision-based O
robotic O
control. O
arXiv O

preprint B-DAT
arXiv:1812.00568, O
2018. O
Finn, O
C. O
and O
Levine, O
S O

. B-DAT
Deep O
visual O
foresight O
for O

planning B-DAT
robot O
motion. O
In O
Robotics O

and B-DAT
Automation O
(ICRA), O
2017 O
IEEE O

International B-DAT
Conference O
on, O
pp. O
2786–2793. O

IEEE, B-DAT
2017. O
Gal, O
Y., O
McAllister, O
R., O
and O

Rasmussen, B-DAT
C. O
E. O
Improving O
pilco O

with B-DAT
bayesian O
neural O
network O
dynamics O

models. B-DAT
In O
Data-Efficient O
Machine O
Learning O

workshop, B-DAT
ICML, O
2016 O

. B-DAT
Gemici, O
M., O
Hung, O
C.-C., O
Santoro O

, B-DAT
A., O
Wayne, O
G., O
Mo- O

hamed, B-DAT
S., O
Rezende, O
D. O
J., O

Amos, B-DAT
D., O
and O
Lillicrap, O
T. O

Generative B-DAT
temporal O
models O
with O
memory. O

arXiv B-DAT
preprint O
arXiv:1702.04649, O
2017. O
Gregor, O
K. O
and O
Besse, O
F O

. B-DAT
Temporal O
difference O
variational O
auto-encoder. O

arXiv B-DAT
preprint O
arXiv:1806.03107, O
2018. O
Ha, O
D. O
and O
Schmidhuber, O
J O

. B-DAT
World O
models. O
arXiv O
preprint O

arXiv:1803.10122, B-DAT
2018. O
Henaff, O
M., O
Whitney, O
W. O
F O

., B-DAT
and O
LeCun, O
Y. O
Model-based O

planning B-DAT
with O
discrete O
and O
continuous O

actions. B-DAT
arXiv O
preprint O
arXiv:1705.07177, O
2018. O
9 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
Higgins, O
I., O
Matthey, O
L., O
Pal O

, B-DAT
A., O
Burgess, O
C., O
Glorot, O

X., B-DAT
Botvinick, O
M., O
Mohamed, O
S., O

and B-DAT
Lerchner, O
A. O
beta- O
vae: O

Learning B-DAT
basic O
visual O
concepts O
with O

a B-DAT
constrained O
variational O
framework. O
In O

International B-DAT
Conference O
on O
Learning O
Representations, O
2016 O

. B-DAT
Higuera, O
J. O
C. O
G., O
Meger O

, B-DAT
D., O
and O
Dudek, O
G. O

Synthesizing B-DAT
neural O
network O
controllers O
with O

probabilistic B-DAT
model O
based O
reinforcement O
learning. O

arXiv B-DAT
preprint O
arXiv:1803.02291, O
2018. O
Igl, O
M., O
Zintgraf, O
L., O
Le O

, B-DAT
T. O
A., O
Wood, O
F., O

and B-DAT
Whiteson, O
S. O
Deep O
variational O

reinforcement B-DAT
learning O
for O
pomdps. O
arXiv O

preprint B-DAT
arXiv:1806.02426, O
2018. O
Kalchbrenner, O
N., O
Oord, O
A. O
v O

. B-DAT
d., O
Simonyan, O
K., O
Danihelka, O

I., B-DAT
Vinyals, O
O., O
Graves, O
A., O

and B-DAT
Kavukcuoglu, O
K. O
Video O
pixel O

networks. B-DAT
arXiv O
preprint O
arXiv:1610.00527, O
2016. O
Kalweit, O
G. O
and O
Boedecker, O
J O

. B-DAT
Uncertainty-driven O
imagi- O
nation O
for O

continuous B-DAT
deep O
reinforcement O
learning. O
In O

Conference B-DAT
on O
Robot O
Learning, O
pp. O
195 O

–206, B-DAT
2017. O
Karl, O
M., O
Soelch, O
M., O
Bayer O

, B-DAT
J., O
and O
van O
der O

Smagt, B-DAT
P. O
Deep O
variational O
bayes O

filters: B-DAT
Unsupervised O
learning O
of O
state O

space B-DAT
models O
from O
raw O
data. O

arXiv B-DAT
preprint O
arXiv:1605.06432, O
2016. O
Kingma, O
D. O
P. O
and O
Ba O

, B-DAT
J. O
Adam: O
A O
method O

for B-DAT
stochastic O
optimization. O
arXiv O
preprint O

arXiv:1412.6980, B-DAT
2014. O
Kingma, O
D. O
P. O
and O
Dhariwal O

, B-DAT
P. O
Glow: O
Generative O
flow O

with B-DAT
invertible O
1x1 O
convolutions. O
arXiv O

preprint B-DAT
arXiv:1807.03039, O
2018. O
Kingma, O
D. O
P. O
and O
Welling O

, B-DAT
M. O
Auto-encoding O
variational O
bayes. O

arXiv B-DAT
preprint O
arXiv:1312.6114, O
2013. O
Krishnan, O
R. O
G., O
Shalit, O
U O

., B-DAT
and O
Sontag, O
D. O
Deep O

kalman B-DAT
filters. O
arXiv O
preprint O
arXiv:1511.05121, O
2015 O

. B-DAT
Krishnan, O
R. O
G., O
Shalit, O
U O

., B-DAT
and O
Sontag, O
D. O
Structured O

inference B-DAT
networks O
for O
nonlinear O
state O

space B-DAT
models. O
In O
AAAI, O
pp. O
2101 O

–2109, B-DAT
2017. O
Kurutach, O
T., O
Clavera, O
I., O
Duan O

, B-DAT
Y., O
Tamar, O
A., O
and O

Abbeel, B-DAT
P. O
Model-ensemble O
trust-region O
policy O

optimization. B-DAT
arXiv O
preprint O
arXiv:1802.10592, O
2018. O
Lamb, O
A. O
M., O
GOYAL, O
A O

. B-DAT
G. O
A. O
P., O
Zhang, O

Y., B-DAT
Zhang, O
S., O
Courville, O
A. O

C., B-DAT
and O
Bengio, O
Y. O
Professor O

forcing: B-DAT
A O
new O
algorithm O
for O

training B-DAT
recurrent O
networks. O
In O
Advances O

In B-DAT
Neural O
Information O
Processing O
Systems, O

pp. B-DAT
4601–4609, O
2016. O
Mathieu, O
M., O
Couprie, O
C., O
and O

LeCun, B-DAT
Y. O
Deep O
multi- O
scale O

video B-DAT
prediction O
beyond O
mean O
square O

error. B-DAT
arXiv O
preprint O
arXiv:1511.05440, O
2015 O

. B-DAT
Mnih, O
V., O
Kavukcuoglu, O
K., O
Silver O

, B-DAT
D., O
Rusu, O
A. O
A., O

Veness, B-DAT
J., O
Bellemare, O
M. O
G., O

Graves, B-DAT
A., O
Riedmiller, O
M., O
Fidje- O

land, B-DAT
A. O
K., O
Ostrovski, O
G., O

et B-DAT
al. O
Human-level O
control O
through O

deep B-DAT
reinforcement O
learning. O
Nature, O
518(7540): O
529, O
2015 O

. B-DAT
Mnih, O
V., O
Badia, O
A. O
P O

., B-DAT
Mirza, O
M., O
Graves, O
A., O

Lillicrap, B-DAT
T., O
Harley, O
T., O
Silver, O

D., B-DAT
and O
Kavukcuoglu, O
K. O
Asyn- O

chronous B-DAT
methods O
for O
deep O
reinforcement O

learning. B-DAT
In O
International O
Conference O
on O

Machine B-DAT
Learning, O
pp. O
1928– O
1937, O
2016 O

. B-DAT
Moerland, O
T. O
M., O
Broekens, O
J O

., B-DAT
and O
Jonker, O
C. O
M. O

Learning B-DAT
multimodal O
transition O
dynamics O
for O

model-based B-DAT
rein- O
forcement O
learning. O
arXiv O

preprint B-DAT
arXiv:1705.00470, O
2017. O
Moravčík, O
M., O
Schmid, O
M., O
Burch O

, B-DAT
N., O
Lisỳ, O
V., O
Morrill, O

D., B-DAT
Bard, O
N., O
Davis, O
T., O

Waugh, B-DAT
K., O
Johanson, O
M., O
and O

Bowl- B-DAT
ing, O
M. O
Deepstack: O
Expert-level O

artificial B-DAT
intelligence O
in O
heads-up O
no-limit O

poker. B-DAT
Science, O
356(6337):508–513, O
2017. O
Nagabandi, O
A., O
Kahn, O
G., O
Fearing O

, B-DAT
R. O
S., O
and O
Levine, O

S. B-DAT
Neural O
network O
dynamics O
for O

model-based B-DAT
deep O
rein- O
forcement O
learning O

with B-DAT
model-free O
fine-tuning. O
arXiv O
preprint O

arXiv:1708.02596, B-DAT
2017. O
Nair, O
V. O
and O
Hinton, O
G O

. B-DAT
E. O
Rectified O
linear O
units O

improve B-DAT
restricted O
boltzmann O
machines. O
In O

Proceedings B-DAT
of O
the O
27th O
international O

conference B-DAT
on O
machine O
learning O
(ICML-10), O

pp. B-DAT
807–814, O
2010. O
Oh, O
J., O
Guo, O
X., O
Lee O

, B-DAT
H., O
Lewis, O
R. O
L., O

and B-DAT
Singh, O
S. O
Action- O
conditional O

video B-DAT
prediction O
using O
deep O
networks O

in B-DAT
atari O
games. O
In O
Advances O

in B-DAT
Neural O
Information O
Processing O
Systems, O

pp. B-DAT
2863–2871, O
2015. O
Rezende, O
D. O
J., O
Mohamed, O
S O

., B-DAT
and O
Wierstra, O
D. O
Stochastic O

backpropagation B-DAT
and O
approximate O
inference O
in O

deep B-DAT
gen- O
erative O
models. O
arXiv O

preprint B-DAT
arXiv:1401.4082, O
2014. O
Richards, O
A. O
G. O
Robust O
constrained O

model B-DAT
predictive O
control. O
PhD O
thesis O

, B-DAT
Massachusetts O
Institute O
of O
Technology, O
2005 O

. B-DAT
Rubinstein, O
R. O
Y. O
Optimization O
of O

computer B-DAT
simulation O
mod- O
els O
with O

rare B-DAT
events. O
European O
Journal O
of O

Operational B-DAT
Research, O
99(1):89–112, O
1997 O

. B-DAT
Silver, O
D., O
Schrittwieser, O
J., O
Simonyan O

, B-DAT
K., O
Antonoglou, O
I., O
Huang, O

A., B-DAT
Guez, O
A., O
Hubert, O
T., O

Baker, B-DAT
L., O
Lai, O
M., O
Bolton, O

A., B-DAT
et O
al. O
Mastering O
the O

game B-DAT
of O
go O
without O
human O

knowledge. B-DAT
Nature, O
550(7676):354, O
2017. O
Srinivas, O
A., O
Jabri, O
A., O
Abbeel O

, B-DAT
P., O
Levine, O
S., O
and O

Finn, B-DAT
C. O
Universal O
planning O
networks. O

arXiv B-DAT
preprint O
arXiv:1804.00645, O
2018. O
10 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
Talvitie, O
E. O
Model O
regularization O
for O

stable B-DAT
sample O
rollouts. O
In O
UAI O

, B-DAT
pp. O
780–789, O
2014. O
Tassa, O
Y., O
Erez, O
T., O
and O

Todorov, B-DAT
E. O
Synthesis O
and O
stabi O

- B-DAT
lization O
of O
complex O
behaviors O

through B-DAT
online O
trajectory O
optimization. O
In O

Intelligent B-DAT
Robots O
and O
Systems O
(IROS), O
2012 O
IEEE/RSJ O
International O
Conference O
on, O
pp O

. B-DAT
4906– O
4913. O
IEEE, O
2012. O
Tassa, O
Y., O
Doron, O
Y., O
Muldal O

, B-DAT
A., O
Erez, O
T., O
Li, O

Y., B-DAT
Casas, O
D. O
d. O
L., O

Budden, B-DAT
D., O
Abdolmaleki, O
A., O
Merel, O

J., B-DAT
Lefrancq, O
A., O
et O
al. O

Deepmind B-DAT
control O
suite. O
arXiv O
preprint O

arXiv:1801.00690, B-DAT
2018. O
van O
den O
Oord, O
A., O
Vinyals O

, B-DAT
O., O
et O
al. O
Neural O

discrete B-DAT
repre- O
sentation O
learning. O
In O

Advances B-DAT
in O
Neural O
Information O
Processing O

Systems, B-DAT
pp. O
6309–6318, O
2017. O
Venkatraman, O
A., O
Hebert, O
M., O
and O

Bagnell, B-DAT
J. O
A. O
Improving O
multi-step O

prediction B-DAT
of O
learned O
time O
series O

models. B-DAT
In O
AAAI, O
pp. O
3024–3030 O

, B-DAT
2015. O
Vondrick, O
C., O
Pirsiavash, O
H., O
and O

Torralba, B-DAT
A. O
Generating O
videos O
with O

scene B-DAT
dynamics. O
In O
Advances O
In O

Neural B-DAT
Information O
Processing O
Systems, O
2016 O

. B-DAT
Watter, O
M., O
Springenberg, O
J., O
Boedecker O

, B-DAT
J., O
and O
Riedmiller, O
M. O

Embed B-DAT
to O
control: O
A O
locally O

linear B-DAT
latent O
dynamics O
model O
for O

control B-DAT
from O
raw O
images. O
In O

Advances B-DAT
in O
neural O
information O
processing O

systems, B-DAT
pp. O
2746–2754, O
2015. O
Wayne, O
G., O
Hung, O
C.-C., O
Amos O

, B-DAT
D., O
Mirza, O
M., O
Ahuja, O

A., B-DAT
Grabska-Barwinska, O
A., O
Rae, O
J., O

Mirowski, B-DAT
P., O
Leibo, O
J. O
Z., O

Santoro, B-DAT
A., O
et O
al. O
Unsupervised O

predic- B-DAT
tive O
memory O
in O
a O

goal-directed B-DAT
agent. O
arXiv O
preprint O
arXiv:1803.10760, O
2018 O

. B-DAT
Weber, O
T., O
Racanière, O
S., O
Reichert O

, B-DAT
D. O
P., O
Buesing, O
L., O

Guez, B-DAT
A., O
Rezende, O
D. O
J., O

Badia, B-DAT
A. O
P., O
Vinyals, O
O., O

Heess, B-DAT
N., O
Li, O
Y., O
et O

al. B-DAT
Imagination-augmented O
agents O
for O
deep O

reinforcement B-DAT
learning. O
arXiv O
preprint O
arXiv:1707.06203, O
2017 O

. B-DAT
Zhang, O
M., O
Vikram, O
S., O
Smith O

, B-DAT
L., O
Abbeel, O
P., O
Johnson, O

M., B-DAT
and O
Levine, O
S. O
SOLAR: O

deep B-DAT
structured O
representations O
for O
model-based O

reinforcement B-DAT
learning. O
arXiv O
preprint O
arXiv:1808.09105, O
2018 O

. B-DAT
11 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
A. O
Hyper O
Parameters O
We O
use O

the B-DAT
convolutional O
and O
deconvolutional O
networks O

from B-DAT
Ha O
& O
Schmidhuber O
(2018 O

), B-DAT
a O
GRU O
(Cho O
et O

al., B-DAT
2014) O
with O
200 O
units O

as B-DAT
deterministic O
path O
in O
the O

dynamics B-DAT
model, O
and O
implement O
all O

other B-DAT
functions O
as O
two O
fully O

connected B-DAT
layers O
of O
size O
200 O

with B-DAT
ReLU O
activations O
(Nair O
& O

Hinton, B-DAT
2010). O
Distributions O
in O
latent O

space B-DAT
are O
30-dimensional O
diagonal O
Gaussians O

with B-DAT
predicted O
mean O
and O
standard O

deviation. B-DAT
We O
pre-process O
images O
by O
reducing O

the B-DAT
bit O
depth O
to O
5 O

bits B-DAT
as O
in O
Kingma O

& B-DAT
Dhariwal O
(2018). O
The O
model O

is B-DAT
trained O
using O
the O
Adam O

optimizer B-DAT
(Kingma O
& O
Ba, O
2014 O

) B-DAT
with O
a O
learning O
rate O

of B-DAT
10−3, O
� O
= O
10−4, O

and B-DAT
gradient O
clipping O
norm O
of O
1000 O
on O
batches O
of O
B O

= B-DAT
50 O
sequence O
chunks O
of O

length B-DAT
L O
= O
50. O
We O

do B-DAT
not O
scale O
the O
KL O

divergence B-DAT
terms O
relatively O
to O
the O

reconstruction B-DAT
terms O
but O
grant O
the O

model B-DAT
3 O
free O
nats O
by O

clipping B-DAT
the O
divergence O
loss O
below O

this B-DAT
value. O
In O
a O
previous O

version B-DAT
of O
the O
agent, O
we O

used B-DAT
latent O
overshooting O
and O
an O

additional B-DAT
fixed O
global O
prior, O
but O

we B-DAT
found O
this O
to O
not O

be B-DAT
necessary O

. B-DAT
For O
planning, O
we O
use O
CEM O

with B-DAT
horizon O
length O
H O

= B-DAT
12, O
optimization O
iterations O
I O

= B-DAT
10, O
candidate O
samples O
J O

= B-DAT
1000, O
and O
refitting O
to O

the B-DAT
best O
K O
= O
100 O

. B-DAT
We O
start O
from O

S B-DAT
= O
5 O
seed O
episodes O

with B-DAT
random O
actions O
and O
collect O

another B-DAT
episode O
every O
C O
= O
100 O
update O
steps O
under O

� B-DAT
∼ O
Normal(0, O
0.3) O
action O

noise. B-DAT
The O
action O
repeat O
differs O

between B-DAT
domains: O
cartpole O
(R O

= B-DAT
8), O
reacher O
(R O

= B-DAT
4), O
cheetah O
(R O

= B-DAT
4), O
finger O
(R O

= B-DAT
2), O
cup O
(R O

= B-DAT
4), O
walker O
(R O

= B-DAT
2). O
We O
found O
important O

hyper B-DAT
parameters O
to O
be O
the O

action B-DAT
repeat, O
the O
KL-divergence O
scales O

β, B-DAT
and O
the O
learning O
rate O

. B-DAT
B. O
Planning O
Algorithm O

Algorithm B-DAT
2: O
Latent O
planning O
with O

CEM B-DAT
Input O
: O
H O
Planning O
horizon O

distance B-DAT
I O
Optimization O
iterations O
J O

Candidates B-DAT
per O
iteration O
K O
Number O

of B-DAT
top O
candidates O
to O
fit O

q(st B-DAT
| O
o≤t, O
a<t) O
Current O

state B-DAT
belief O
p(st O
| O
st−1, O

at−1) B-DAT
Transition O
model O
p(rt O
| O

st) B-DAT
Reward O
model O
1 O
Initialize O
factorized O
belief O
over O

action B-DAT
sequences O
q(at:t+H)← O
Normal(0, O
I O

). B-DAT
2 O
for O
optimization O
iteration O

i B-DAT
= O
1..I O
do O
// O
Evaluate O
J O
action O
sequences O

from B-DAT
the O
current O
belief. O
3 O

for B-DAT
candidate O
action O
sequence O
j O

= B-DAT
1..J O
do O
4 O
a O

j) B-DAT
t:t+H O
∼ O
q(at:t+H) O
5 O
s O
(j) O
t:t+H+1 O

∼ B-DAT
q(st O
| O
o1:t, O
a1:t−1 O

) B-DAT
∏t+H+1 O
τ=t+1 O
p(sτ O
| O
sτ−1 O

, B-DAT
a O
(j) O
τ−1 O

) B-DAT
6 O
R(j) O
= O
∑t+H+1 O
τ=t+1 O

E[p(rτ B-DAT
| O
s O

j) B-DAT
τ O
)] O
// O
Re-fit O
belief O
to O
the O

K B-DAT
best O
action O
sequences O

. B-DAT
7 O
K O
← O
argsort({R(j)}Jj=1)1:K O
8 O

µt:t+H B-DAT

1 B-DAT
K O
∑ O
k∈K O
a O

k) B-DAT
t:t+H O
, O
σt:t+H O
= O
1 O
K−1 O

k∈K B-DAT
|a O
(k) O
t:t+H O
− O
µt:t+H O

9 B-DAT
q(at:t+H)← O
Normal(µt:t+H O
, O
σ2t:t+H O

I) B-DAT
10 O
return O
first O
action O

mean B-DAT
µt. O
12 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
C. O
Multi-Task O
Learning O
Average O
over O

tasks B-DAT

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Figure O
6: O
We O
compare O
a O

single B-DAT
PlaNet O
agent O
trained O
on O

all B-DAT
tasks O
to O
individual O
PlaNet O

agents. B-DAT
The O
plot O
shows O
test O

performance B-DAT
over O
the O
number O
of O

episodes B-DAT
collected O
for O
each O
task O

. B-DAT
The O
single O
agent O
learns O

to B-DAT
solve O
all O
the O
tasks O

while B-DAT
learning O
more O
slowly O
compared O

to B-DAT
the O
individual O
agents. O
The O

lines B-DAT
show O
mean O
and O
one O

standard B-DAT
deviation O
over O
6 O
tasks, O
5 O
seeds, O
and O
10 O
trajectories O

. B-DAT
Cartpole O
Swing O
Up O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Reacher O
Easy O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cheetah O
Run O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Finger O
Spin O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cup O
Catch O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Walker O
Walk O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Separate O
agents O
Single O
agent O

D4PG B-DAT
(100k O
episodes) O
A3C O
(100k O

episodes, B-DAT
proprio) O
Figure O
7: O
Per-task O
performance O
of O

a B-DAT
single O
PlaNet O
agent O
trained O

on B-DAT
the O
six O
tasks. O
Plots O

show B-DAT
test O
performance O
over O
the O

number B-DAT
of O
episodes O
collected O
per O

task. B-DAT
The O
agent O
is O
not O

told B-DAT
which O
task O
it O
is O

solving B-DAT
and O
it O
needs O
to O

infer B-DAT
this O
from O
the O
image O

observations. B-DAT
The O
agent O
learns O
to O

distinguish B-DAT
the O
tasks O
and O
solve O

them B-DAT
with O
just O
a O
moderate O

slowdown B-DAT
in O
learning. O
The O
lines O

show B-DAT
medians O
and O
the O
areas O

show B-DAT
percentiles O
5 O
to O
95 O

over B-DAT
4 O
seeds O
and O
10 O

trajectories B-DAT

. B-DAT
13 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
D. O
Latent O
Overshooting O
Cartpole O
Swing O

Up B-DAT

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Reacher O
Easy O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cheetah O
Run O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Finger O
Spin O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cup O
Catch O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Walker O
Walk O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
RSSM O
RSSM O
+ O
Overshooting O

DRNN B-DAT
DRNN O
+ O
Overshooting O
D4PG O
(100k O
episodes) O
A3C O
(100k O

episodes, B-DAT
proprio O

) B-DAT
Figure O
8: O
We O
compare O
the O

standard B-DAT
variational O
objective O
with O
latent O

overshooting B-DAT
on O
our O
proposed O
RSSM O

and B-DAT
another O
model O
called O
DRNN O

that B-DAT
uses O
two O
RNNs O
as O

encoder B-DAT
and O
decoder O
with O
a O

stochastic B-DAT
state O
sequence O
in O
between O

. B-DAT
Latent O
overshooting O
can O
substantially O

improve B-DAT
the O
performance O
of O
the O

DRNN B-DAT
and O
other O
models O
we O

have B-DAT
experimented O
with O
(not O
shown), O

but B-DAT
slightly O
reduces O
performance O
of O

our B-DAT
RSSM. O
The O
lines O
show O

medians B-DAT
and O
the O
areas O
show O

percentiles B-DAT
5 O
to O
95 O
over O
5 O
seeds O
and O
10 O
trajectories O

. B-DAT
14 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
E. O
Activation O
Function O
Cartpole O
Swing O

Up B-DAT

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Reacher O
Easy O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cheetah O
Run O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Finger O
Spin O

5 B-DAT
250 O
500 O
750 O
1000 O
0 O

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Cup O
Catch O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
Walker O
Walk O
5 O
250 O
500 O
750 O
1000 O

0 B-DAT

200 B-DAT
400 O

600 B-DAT
800 O

1000 B-DAT
PlaNet O
(ReLU) O
PlaNet O
(ELU O

) B-DAT
SSM O
(ReLU) O
SSM O
(ELU O

) B-DAT
D4PG O
(100k O
episodes) O
A3C O
(100k O

episodes, B-DAT
proprio O

) B-DAT
Figure O
9: O
Comparison O
of O
hard O

ReLU B-DAT
(Nair O
& O
Hinton, O
2010 O

) B-DAT
and O
smooth O
ELU O
(Clevert O

et B-DAT
al., O
2015) O
activation O
functions. O

We B-DAT
find O
that O
smooth O
activations O

help B-DAT
improve O
performance O
of O
the O

purely B-DAT
stochastic O
model O
(and O
the O

purely B-DAT
deterministic O
model; O
not O
shown) O

while B-DAT
our O
proposed O
RSSM O
is O

robust B-DAT
to O
the O
choice O
of O

activation B-DAT
function. O
The O
lines O
show O

medians B-DAT
and O
the O
areas O
show O

percentiles B-DAT
5 O
to O
95 O
over O
5 O
seeds O
and O
10 O
trajectories O

. B-DAT
15 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
F. O
Bound O
Derivations O
One-step O
predictive O

distribution B-DAT
The O
variational O
bound O
for O

latent B-DAT
dynamics O
models O
p(o1:T O

, B-DAT
s1:T O
| O
a1:T O

t B-DAT
p(st O
| O
st−1, O
at−1)p(ot O
| O
st) O
and O

a B-DAT
variational O
posterior O
q(s1:T O

| B-DAT
o1:T O
, O
a1:T O

) B-DAT
= O
∏ O
t O
q(st O

| B-DAT
o≤t, O
a<t) O
follows O
from O

importance B-DAT

weighting B-DAT
and O
Jensen’s O
inequality O
as O

shown, B-DAT
ln O
p(o1:T O
| O
a1:T O

) B-DAT
, O
ln O
Ep(s1:T O
|a1:T O

) B-DAT
[ O
T∏ O
t=1 O

p(ot B-DAT
| O
st) O
] O
= O
lnEq(s1:T O
|o1:T O
,a1:T O

T∏ B-DAT
t=1 O
p(ot O
| O
st)p(st O
| O
st−1 O

, B-DAT
at−1)/q(st O
| O
o≤t, O

a<t) B-DAT
] O
≥ O
Eq(s1:T O
|o1:T O
,a1:T O

) B-DAT
[ O
T∑ O
t=1 O

ln B-DAT
p(ot O
| O
st) O
+ O

ln B-DAT
p(st O
| O
st−1, O
at−1)− O

ln B-DAT
q(st O
| O
o≤t, O

T∑ B-DAT
t=1 O
( O
E O
q(st|o≤t,a<t) O
[ln O
p(ot O

| B-DAT
st O

)] B-DAT
reconstruction O

E B-DAT
q(st−1|o≤t−1,a<a−1 O
) O
[ O
KL[q(st O
| O
o≤t, O
a<t O

) B-DAT
‖ O
p(st O
| O
st−1, O

at−1)] B-DAT
] O
complexity O

8) B-DAT
Multi-step O
predictive O
distribution O
The O
variational O

bound B-DAT
on O
the O
d-step O
predictive O

distribution B-DAT
pd(o1:T O
, O
s1:T O

| B-DAT
a1:T O
) O
=∏ O
t O

p(st B-DAT
| O
st−d, O
at−1)p(ot O

| B-DAT
st) O
and O
a O
variational O

posterior B-DAT
q(s1:T O
| O
o1:T O

, B-DAT
a1:T O

t B-DAT
q(st O
| O
o≤t, O
a<t) O

follows B-DAT
anal- O
ogously. O
The O
second O
bound O
comes O

from B-DAT
moving O
the O
log O
inside O

the B-DAT
multi-step O
priors, O
which O
satisfy O

the B-DAT
recursion O
p(st O
| O
st−d O

, B-DAT
at−d−1:t−1) O
= O
Ep(st−1|st−d,at−d−1:t−2)[p(st O
| O

st−1, B-DAT
at−1)]. O
ln O
pd(o1:T O
| O
a1:T O

) B-DAT
, O
ln O
Epd(s1:T O
|a1:T O

) B-DAT
[ O
T∏ O
t=1 O

p(ot B-DAT
| O
st) O
] O
= O
lnEq(s1:T O
|o1:T O
,a1:T O

T∏ B-DAT
t=1 O
p(ot O
| O
st)p(st O
| O
st−d O

, B-DAT
at−d−1:t−1)/q(st O
| O
o≤t, O

a<t) B-DAT
] O
≥ O
Eq(s1:T O
|o1:T O
,a1:T O

) B-DAT
[ O
T∑ O
t=1 O

ln B-DAT
p(ot O
| O
st) O
+ O

ln B-DAT
p(st O
| O
st−d, O
at−d−1:t−1)− O

ln B-DAT
q(st O
| O
o≤t, O

a<t) B-DAT
] O
≥ O
Eq(s1:T O
|o1:T O
,a1:T O

) B-DAT
[ O
T∑ O
t=1 O

ln B-DAT
p(ot O
| O
st) O
+ O

E B-DAT
p(st−1|st−d,at−d−1:t−2) O
[ln O
p(st O
| O

st−1, B-DAT
at−1)]− O
ln O
q(st O
| O

o≤t, B-DAT

T∑ B-DAT
t=1 O
( O
Eq(st|o≤t,a<t)[ln O
p(ot O
| O
st O

)] B-DAT
reconstruction O

E B-DAT
p(st−1|st−d,at−d−1:t−2)q(st−d|o≤t−d,a<t−d) O
[ O
KL[q(st O
| O
o≤t, O
a<t O

) B-DAT
‖ O
p(st O
| O
st−1, O

at−1)] B-DAT
] O
multi-step O
prediction O

9) B-DAT
Since O
all O
expectations O
are O
on O

the B-DAT
outside O
of O
the O
objective O

, B-DAT
we O
can O
easily O
obtain O

an B-DAT
unbiased O
estimator O
of O
this O

bound B-DAT
by O
changing O
expectations O
to O

sample B-DAT
averages. O
Relation O
between O
one-step O
and O
multi-step O

predictive B-DAT
distributions O
We O
conjecture O
that O

the B-DAT
multi-step O
predictive O
dis- O
tribution O

pd(o1:T B-DAT
) O
lower O
bounds O
the O

one-step B-DAT
predictive O
distribution O
p(o1:T O

) B-DAT
of O
the O
same O
latent O

sequence B-DAT
model O
model O
in O
expectation O

over B-DAT
the O
data O
set. O
Since O

the B-DAT
latent O
state O
sequence O
is O

Markovian, B-DAT
for O
d O
≥ O
1 O

we B-DAT
have O
the O
data O
processing O

inequality B-DAT

I(st; B-DAT
st−d) O
≤ O
I(st; O
st−1) O

H(st)−H(st|st−d) B-DAT
≤ O
H(st)−H(st|st−1) O
E[ln O

p(st B-DAT
| O
st−d)] O
≤ O
E[ln O

p(st B-DAT
| O
st−1)] O
E[ln O
pd(o1:T O
)] O
≤ O
E[ln O

p(o1:T B-DAT

10) B-DAT
Therefore, O
any O
bound O
on O
the O

multi-step B-DAT
predictive O
distribution, O
including O
Equation O

9 B-DAT
and O
Equation O
7, O
is O

also B-DAT
a O
bound O
on O
the O

one-step B-DAT
predictive O
distribution O

. B-DAT
16 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
G. O
Additional O
Related O
Work O
Planning O

in B-DAT
state O
space O
When O
low-dimensional O

states B-DAT
of O
the O
environment O
are O

available B-DAT
to O
the O
agent, O
it O

is B-DAT
possible O
to O
learn O
the O

dynamics B-DAT
directly O
in O
state O
space O

. B-DAT
In O
the O
regime O
of O

control B-DAT
tasks O
with O
only O
a O

few B-DAT
state O
variables, O
such O
as O

the B-DAT
cart O
pole O
and O
mountain O

car B-DAT
tasks, O
PILCO O
(Deisenroth O
& O

Rasmussen, B-DAT
2011) O
achieves O
remarkable O
sample O

efficiency B-DAT
using O
Gaussian O
processes O
to O

model B-DAT
the O
dynamics. O
Similar O
approaches O

using B-DAT
neural O
networks O
dynamics O
models O

can B-DAT
solve O
two-link O
balancing O
problems O
( O

Gal B-DAT
et O
al., O
2016; O
Higuera O

et B-DAT
al., O
2018) O
and O
implement O

planning B-DAT
via O
gradients O
(Henaff O
et O

al., B-DAT
2018). O
Chua O
et O
al. O
(2018 O

) B-DAT
use O
ensembles O
of O
neural O

networks, B-DAT
scaling O
up O
to O
the O

cheetah B-DAT
running O
task. O
The O
limitation O

of B-DAT
these O
methods O
is O
that O

they B-DAT
access O
the O
low-dimensional O
Markovian O

state B-DAT
of O
the O
underlying O
system O

and B-DAT
sometimes O
the O
reward O
function. O

Amos B-DAT
et O
al. O
(2018) O
train O

a B-DAT
deterministic O
model O
using O
overshooting O

in B-DAT
observation O
space O
for O
active O

exploration B-DAT
with O
a O
robotics O
hand. O

We B-DAT
move O
beyond O
low-dimensional O
state O

representations B-DAT
and O
use O
a O
latent O

dynamics B-DAT
model O
to O
solve O
control O

tasks B-DAT
from O
images. O
Hybrid O
agents O
The O
challenges O
of O

model-based B-DAT
RL O
have O
motivated O
the O

research B-DAT
community O
to O
develop O
hybrid O

agents B-DAT
that O
accelerate O
policy O
learning O

by B-DAT
training O
on O
imagined O
experience O

(Kalweit B-DAT
& O
Boedecker, O
2017; O
Nagabandi O

et B-DAT
al., O
2017; O
Kurutach O
et O

al., B-DAT
2018; O
Buckman O
et O
al O

., B-DAT
2018; O
Ha O
& O
Schmidhuber, O
2018 O

), B-DAT
improving O
feature O
representations O
(Wayne O

et B-DAT
al., O
2018; O
Igl O
et O

al., B-DAT
2018), O
or O
leveraging O
the O

information B-DAT
content O
of O
the O
model O

directly B-DAT
(Weber O
et O
al., O
2017). O

Srinivas B-DAT
et O
al. O
(2018) O
learn O

a B-DAT
policy O
network O
with O
integrated O

planning B-DAT
computation O
using O
reinforcement O
learning O

and B-DAT
without O
prediction O
loss, O
yet O

require B-DAT
expert O
demonstrations O
for O
training. O
Multi-step O
predictions O
Training O
sequence O
models O

on B-DAT
multi-step O
predictions O
has O
been O

explored B-DAT
for O
several O
years. O
Scheduled O

sampling B-DAT
(Bengio O
et O
al., O
2015 O

) B-DAT
changes O
the O
rollout O
distance O

of B-DAT
the O
sequence O
model O
over O

the B-DAT
course O
of O
training. O
Hallucinated O

replay B-DAT
(Talvitie, O
2014) O
mixes O
predictions O

into B-DAT
the O
data O
set O
to O

indirectly B-DAT
train O
multi-step O
predictions. O
Venkatraman O

et B-DAT
al. O
(2015) O
take O
an O

imitation B-DAT
learning O
approach. O
Recently, O
Amos O

et B-DAT
al. O
(2018) O
train O
a O

dynamics B-DAT
model O
on O
all O
multi-step O

predictions B-DAT
at O
once. O
We O
generalize O

this B-DAT
idea O
to O
latent O
sequence O

models B-DAT
trained O
via O
variational O
inference. O
Latent O
sequence O
models O
Classic O
work O

has B-DAT
explored O
models O
for O
non-Markovian O

observation B-DAT
sequences, O
including O
recurrent O
neural O

networks B-DAT
(RNNs) O
with O
deterministic O
hidden O

state B-DAT
and O
probabilistic O
state-space O
models O

(SSMs). B-DAT
The O
ideas O
behind O
variational O

autoencoders B-DAT
(Kingma O
& O
Welling, O
2013 O

; B-DAT
Rezende O
et O
al., O
2014) O

have B-DAT
enabled O
non-linear O
SSMs O
that O

are B-DAT
trained O
via O
variational O
inference O
( O

Krishnan B-DAT
et O
al., O
2015). O
The O

VRNN B-DAT
(Chung O
et O
al., O
2015) O

combines B-DAT
RNNs O
and O
SSMs O
and O

is B-DAT
trained O
via O
variational O
inference. O

In B-DAT
contrast O
to O
our O
RSSM, O

it B-DAT
feeds O
generated O
observations O
back O

into B-DAT
the O
model O
which O
makes O

forward B-DAT
predictions O
expensive. O
Karl O
et O

al. B-DAT
(2016) O
address O
mode O
collapse O

to B-DAT
a O
single O
future O
by O

restricting B-DAT
the O
transition O
function, O
(Moerland O

et B-DAT
al., O
2017) O
focus O
on O

multi-modal B-DAT
transitions, O
and O
Doerr O
et O

al. B-DAT
(2018) O
stabilize O
training O
of O

purely B-DAT
stochastic O
models. O
Buesing O
et O

al. B-DAT
(2018) O
propose O
a O
model O

similar B-DAT
to O
ours O
but O
use O

in B-DAT
a O
hybrid O
agent O
instead O

for B-DAT
explicit O
planning. O
Video O
prediction O
Video O
prediction O
is O

an B-DAT
active O
area O
of O
research O

in B-DAT
deep O
learning. O
Oh O
et O

al. B-DAT
(2015) O
and O
Chiappa O
et O

al. B-DAT
(2017) O
achieve O
visually O
plausible O

predictions B-DAT
on O
Atari O
games O
using O

deterministic B-DAT
models. O
Kalchbrenner O
et O
al O

. B-DAT
(2016) O
introduce O
an O
autoregressive O

video B-DAT
prediction O
model O
using O
gated O

CNNs B-DAT
and O
LSTMs. O
Recent O
approaches O

introduce B-DAT
stochasticity O
to O
the O
model O

to B-DAT
capture O
multiple O
futures O
(Babaeizadeh O

et B-DAT
al., O
2017; O
Denton O
& O

Fergus, B-DAT
2018). O
To O
obtain O
realistic O

predictions, B-DAT
Mathieu O
et O
al. O
(2015) O

and B-DAT
Vondrick O
et O
al. O
(2016) O

use B-DAT
adversarial O
losses. O
In O
simulated O

environments, B-DAT
Gemici O
et O
al. O
(2017) O

augment B-DAT
dynamics O
models O
with O
an O

external B-DAT
memory O
to O
remember O
long-time O

contexts. B-DAT
van O
den O
Oord O
et O

al. B-DAT
(2017) O
propose O
a O
variational O

model B-DAT
that O
avoids O
sampling O
using O

a B-DAT
nearest O
neighbor O
look-up, O
yielding O

high B-DAT
fidelity O
image O
predictions. O
These O

models B-DAT
are O
complimentary O
to O
our O

approach. B-DAT
17 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
H. O
Video O
Predictions O

Pl B-DAT
aN O
et O
Tr O

ue B-DAT
Context O
6 O
10 O
15 O
20 O

25 B-DAT
30 O
35 O
40 O
45 O

50 B-DAT

M B-DAT
od O
el O
Tr O

ue B-DAT
M O
od O
el O

Tr B-DAT
ue O
M O
od O

el B-DAT
Pl O
aN O

et B-DAT
+ O
O O
ve O

rs B-DAT
ho O
ot O
in O

g B-DAT
Tr O
ue O

Context B-DAT
6 O
10 O
15 O
20 O
25 O
30 O
35 O
40 O
45 O
50 O

M B-DAT
od O
el O
Tr O

ue B-DAT
M O
od O
el O

Tr B-DAT
ue O
M O
od O

el B-DAT
D O
et O

er B-DAT
m O
in O
is O

tic B-DAT
(G O
R O
U O

Tr B-DAT
ue O
Context O
6 O
10 O
15 O
20 O

25 B-DAT
30 O
35 O
40 O
45 O

50 B-DAT

M B-DAT
od O
el O
Tr O

ue B-DAT
M O
od O
el O

Tr B-DAT
ue O
M O
od O

el B-DAT
St O
oc O

ha B-DAT
st O
ic O
(S O

SM B-DAT
) O
Tr O
ue O

Context B-DAT
6 O
10 O
15 O
20 O
25 O
30 O
35 O
40 O
45 O
50 O

M B-DAT
od O
el O
Tr O

ue B-DAT
M O
od O
el O

Tr B-DAT
ue O
M O
od O

el B-DAT
Figure O
10: O
Open-loop O
video O
predictions O

for B-DAT
test O
episodes. O
The O
columns O

1–5 B-DAT
show O
reconstructed O
context O
frames O

and B-DAT
the O
remaining O
images O
are O

generated B-DAT
open-loop. O
Our O
RSSM O
achieves O

pixel-accurate B-DAT
predictions O
for O
50 O
steps O

into B-DAT
the O
future O
in O
the O

cheetah B-DAT
environment. O
We O
randomly O
selected O

action B-DAT
sequences O
from O
test O
episodes O

collected B-DAT
with O
action O
noise O
alongside O

the B-DAT
training O
episodes O

. B-DAT
18 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
I. O
State O
Diagnostics O

Figure B-DAT
11: O
Open-loop O
state O
diagnostics. O

We B-DAT
freeze O
the O
dynamics O
model O

of B-DAT
a O
PlaNet O
agent O
and O

learn B-DAT
small O
neural O
networks O
to O

predict B-DAT
the O
true O
positions, O
velocities, O

and B-DAT
reward O
of O
the O
simulator. O

The B-DAT
open-loop O
predictions O
of O
these O

quantities B-DAT
show O
that O
most O
information O

about B-DAT
the O
underlying O
system O
is O

present B-DAT
in O
the O
learned O
latent O

space B-DAT
and O
can O
be O
accurately O

predicted B-DAT
forward O
further O
than O
the O

planning B-DAT
horizons O
used O
in O
this O

work. B-DAT
19 O

Learning B-DAT
Latent O
Dynamics O
for O
Planning O

from B-DAT
Pixels O
J. O
Planning O
Parameters O

3.0 B-DAT
5.0 O

10.0 B-DAT
15.0 O

ite B-DAT
ra O
tio O
ns O

fraction=0.05 B-DAT
horizon=6.0 O
fraction=0.05 O
horizon=8.0 O
fraction=0.05 O

horizon=10.0 B-DAT
fraction=0.05 O
horizon=12.0 O
fraction=0.05 O
horizon=14.0 O
3.0 O

5.0 B-DAT
10.0 O

15.0 B-DAT
ite O
ra O

tio B-DAT
ns O
fraction=0.1 O
horizon=6.0 O
fraction=0.1 O
horizon=8.0 O
fraction=0.1 O

horizon=10.0 B-DAT
fraction=0.1 O
horizon=12.0 O
fraction=0.1 O
horizon=14.0 O

3.0 B-DAT
5.0 O

10.0 B-DAT
15.0 O

ite B-DAT
ra O
tio O
ns O

fraction=0.3 B-DAT
horizon=6.0 O
fraction=0.3 O
horizon=8.0 O
fraction=0.3 O

horizon=10.0 B-DAT
fraction=0.3 O
horizon=12.0 O
fraction=0.3 O
horizon=14.0 O
100.0 O
300.0 O
500.0 O
1000.0 O
proposals O

3.0 B-DAT
5.0 O

10.0 B-DAT
15.0 O

ite B-DAT
ra O
tio O
ns O

fraction=0.5 B-DAT
horizon=6.0 O
100.0 O
300.0 O
500.0 O
1000.0 O
proposals O

fraction=0.5 B-DAT
horizon=8.0 O
100.0 O
300.0 O
500.0 O
1000.0 O
proposals O

fraction=0.5 B-DAT
horizon=10.0 O
100.0 O
300.0 O
500.0 O
1000.0 O
proposals O

fraction=0.5 B-DAT
horizon=12.0 O
100.0 O
300.0 O
500.0 O
1000.0 O
proposals O

fraction=0.5 B-DAT
horizon=14.0 O
Figure O
12: O
Planning O
performance O
on O

the B-DAT
cheetah O
running O
task O
with O

the B-DAT
true O
simulator O
using O
different O

planner B-DAT
settings. O
Performance O
ranges O
from O

132 B-DAT
(blue) O
to O
837 O
(yellow O

). B-DAT
Evaluating O
more O
action O
sequences, O

optimizing B-DAT
for O
more O
iterations, O
and O

re-fitting B-DAT
to O
fewer O
of O
the O

best B-DAT
proposals O
tend O
to O
improve O

performance. B-DAT
A O
planning O
horizon O
length O

of B-DAT
6 O
is O
not O
sufficient O

and B-DAT
results O
in O
poor O
performance. O

Much B-DAT
longer O
planning O
horizons O
hurt O

performance B-DAT
because O
of O
the O
increased O

search B-DAT
space. O
For O
this O
environment, O

best B-DAT
planning O
horizon O
length O
is O

near B-DAT
8 O
steps. O
20 O

d) O
Finger O
(e) O
Cup O
(f) O
Walker B-DAT

1000 O
Walker B-DAT
Walk O

1000 O
Walker B-DAT
Walk O

1000 O
Walker B-DAT
Walk O

1000 O
Walker B-DAT
Walk O

1000 O
Walker B-DAT
Walk O

1000 O
Walker B-DAT
Walk O

1000 O
Walker B-DAT
Walk O

1000 O
Walker B-DAT
Walk O

1000 O
Walker B-DAT
Walk O

1000 O
Walker B-DAT
Walk O

57 B-DAT
G O

Left: O
Atari O
results O
aggregated O
across O
57 B-DAT
games, O
evaluated O
from O
random O
no-op O

1: O
Median O
normalized O
scores O
across O
57 B-DAT
Atari O
games. O
a O
Tesla O
P100 O

human O
normalized O
score O
across O
all O
57 B-DAT
games O
to O
several O
baselines: O
DQN O

chains O
and O
their O
applications. O
Biometrika, O
57 B-DAT

Figure O
9: O
Training O
curves O
for O
57 B-DAT
Atari O
games O
(performance O
against O
wall O

Figure O
10: O
Training O
curves O
for O
57 B-DAT
Atari O
games O
(performance O
against O
environment O

beam O
rider O
63,305.2 O
72,233.7 O
berzerk O
57, B-DAT

12,974.0 O
5,711.6 O
chopper O
command O
721,851.0 O
576, B-DAT

jamesbond O
21,322.5 O
18,992.3 O
kangaroo O
1,416.0 O
577 B-DAT

of O
the O
art O
performance O
on O
Atari B-DAT
games, O
using O
a O
fraction O
of O

Figure O
2: O
Left: O
Atari B-DAT
results O
aggregated O
across O
57 O
games O

from O
random O
no-op O
starts. O
Right: O
Atari B-DAT
training O
curves O
for O
selected O
games O

we O
evaluate O
Ape-X O
DQN O
on O
Atari, B-DAT
and O
show O
state O
of O
the O

Median O
normalized O
scores O
across O
57 O
Atari B-DAT
games. O
a O
Tesla O
P100. O
b O

of O
Ape-X O
DQN O
on O
the O
Atari B-DAT
benchmark O
to O
corresponding O
metrics O
as O

than O
it O
is O
in O
the O
Atari B-DAT
domain. O
We O
therefore O
use O
small O

additional O
Ape-X O
DQN O
experiments O
on O
Atari B-DAT
that O
helped O
improve O
our O
understanding O

on O
a O
subset O
of O
6 O
Atari B-DAT
games. O
In O
all O
experiments O
we O

removing O
data O
than O
in O
the O
Atari B-DAT
experiments, O
which O
simply O
removed O
the O

On O
Atari, B-DAT
we O
performed O
some O
limited O
tuning O

9: O
Training O
curves O
for O
57 O
Atari B-DAT
games O
(performance O
against O
wall O
clock O

10: O
Training O
curves O
for O
57 O
Atari B-DAT
games O
(performance O
against O
environment O
frames O

actors O
on O
a O
selection O
of O
Atari B-DAT
games. O
Blue: O
prioritized O
replay, O
with O

4.1 O
Atari B-DAT

C O
Atari B-DAT

57 B-DAT
(all O
available O
Atari O
games O
in O

57 B-DAT
set O
of O
tasks O

Lab O
tasks O
and O
on O
all O
57 B-DAT
games O
of O
the O
Atari O
Learning O

recent O
deep O
reinforcement O
agents. O
Its O
57 B-DAT
tasks O
pose O
challenging O
rein- O
forcement O

57 B-DAT
agent—one O
agent, O
one O
set O
of O

weights— O
on O
all O
57 B-DAT
Atari O
games O
at O
once O
for O

57 B-DAT
agent, O
we O
use O
population O
based O

nor- O
malised O
score O
across O
all O
57 B-DAT
Atari O
games. O
Evaluation O
follows O
a O

single O
agent O
trained O
on O
all O
57 B-DAT
ALE O
games O
at O
once, O
reaches O

57 B-DAT

a O
multi-task O
setting O
on O
all O
57 B-DAT
games O
of O
ALE O
that O
is O

57 B-DAT
set. O
To O
the O
best O
of O

object O
locations O
small O
74.5 O
3.6 O
57 B-DAT

name O
this O
game O
N/A O
9907.20 O
5719 B-DAT

runner O
53446.00 O
71168 O
24435.50 O
37505.00 O
57121 B-DAT

pinball O
100496.60 O
469366 O
20125.14 O
228642.52 O
572898 B-DAT

Beattie O
et O
al., O
2016)) O
and O
Atari B-DAT

-57 O
(all O
available O
Atari B-DAT
games O
in O
Arcade O
Learning O
Environment O

ver O
et O
al., O
2017) O
and O
Atari B-DAT
games O
(Horgan O
et O
al., O
2018 O

on O
all O
games O
in O
the O
Atari B-DAT

works O
partic- O
ularly O
well O
on O
Atari B-DAT
environments, O
because O
rendering O
and O
game O

all O
57 O
games O
of O
the O
Atari B-DAT
Learning O
Environment O
(Bellemare O
et O
al O

The O
Atari B-DAT
Learning O
Environment O
(ALE) O
(Bellemare O
et O

hyperparameters, O
we O
train O
an O
IMPALA O
Atari B-DAT

of O
weights— O
on O
all O
57 O
Atari B-DAT
games O
at O
once O
for O
200 O

11.4 O
billion O
frames. O
For O
the O
Atari B-DAT

malised O
score O
across O
all O
57 O
Atari B-DAT
games. O
Evaluation O
follows O
a O
standard O

4. O
Human O
normalised O
scores O
on O
Atari B-DAT

new O
DMLab-30 O
set O
and O
the O
Atari B-DAT

C. O
Atari B-DAT
Scores O
ACKTR O
The O
Reactor O
IMPALA O

Table O
C.1. O
Atari B-DAT
scores O
after O
200M O
steps O
environment O

G. O
Atari B-DAT
Experiments O
All O
agents O
trained O
on O

Atari B-DAT
are O
equipped O
only O
with O
a O

same O
policy O
layer O
on O
all O
Atari B-DAT
games O
in O
the O
multi-task O
setting O

multi-task O
agent O
on O
the O
full O
Atari B-DAT
action O
set O
consisting O
of O
18 O

Table O
G.1. O
Hyperparameters O
for O
Atari B-DAT
experiments O

Beattie O
et O
al., O
2016)) O
and O
Atari-57 B-DAT
(all O
available O
Atari O
games O
in O

on O
all O
games O
in O
the O
Atari-57 B-DAT
set O
of O
tasks O

hyperparameters, O
we O
train O
an O
IMPALA O
Atari-57 B-DAT
agent—one O
agent, O
one O
set O
of O

11.4 O
billion O
frames. O
For O
the O
Atari-57 B-DAT
agent, O
we O
use O
population O
based O

4. O
Human O
normalised O
scores O
on O
Atari-57 B-DAT

new O
DMLab-30 O
set O
and O
the O
Atari-57 B-DAT
set. O
To O
the O
best O
of O

Beattie O
et O
al., O
2016)) O
and O
Atari-57 B-DAT
(all O
available O
Atari O
games O
in O

on O
all O
games O
in O
the O
Atari-57 B-DAT
set O
of O
tasks O

hyperparameters, O
we O
train O
an O
IMPALA O
Atari-57 B-DAT
agent—one O
agent, O
one O
set O
of O

11.4 O
billion O
frames. O
For O
the O
Atari-57 B-DAT
agent, O
we O
use O
population O
based O

4. O
Human O
normalised O
scores O
on O
Atari-57 B-DAT

new O
DMLab-30 O
set O
and O
the O
Atari-57 B-DAT
set. O
To O
the O
best O
of O

and O
reaches O
state-of-the-art O
performance O
over O
57 B-DAT
Atari O
2600 O
games O

present O
experimental O
results O
on O
the O
57 B-DAT
Atari O
2600 O
games O
from O
the O

human-level, O
and O
beyond, O
performance O
across O
57 B-DAT
Atari O
2600 O
games O
in O
the O

trained O
and O
evaluated O
Reactor O
on O
57 B-DAT
Atari O
games O
(Bellemare O
et O
al O

with O
β O
= O
1 O
on O
57 B-DAT
Atari O
games O
trained O
on O
10 O

of O
each O
algorithm O
across O
all O
57 B-DAT
Atari O
games. O
We O
also O
evaluated O

several O
other O
state-of-art O
algorithms O
across O
57 B-DAT
Atari O
games O
for O
a O
fixed O

mean O
human O
normalized O
score O
across O
57 B-DAT
Atari O
games O
is O
almost O
equivalent O

mean O
rank O
was O
evaluated O
across O
57 B-DAT
Atari O
games. O
For O
Elo O
score O

11.5 O
7845.0 O
39544.0 O
44127.0 O
69524.0 O
57608 B-DAT

replay O
prioritiza- O
tion. O
Using O
the O
Atari B-DAT
2600 O
benchmarks, O
we O
show O
that O

a O
week O
to O
train O
on O
Atari B-DAT

reaches O
state-of-the-art O
performance O
over O
57 O
Atari B-DAT
2600 O
games O

experimental O
results O
on O
the O
57 O
Atari B-DAT
2600 O
games O
from O
the O
Arcade O

and O
beyond, O
performance O
across O
57 O
Atari B-DAT
2600 O
games O
in O
the O
ALE O

performance O
over O
DQN O
on O
the O
Atari B-DAT
2600 O
benchmarks O
(Schaul O
et O
al O

showed O
state-of-the-art O
performance O
in O
the O
Atari B-DAT
2600 O
benchmarks. O
C51 O
parameterizes O
the O

In O
some O
domains, O
such O
as O
Atari, B-DAT
it O
is O
useful O
to O
base O

and O
evaluated O
Reactor O
on O
57 O
Atari B-DAT
games O
(Bellemare O
et O
al., O
2013 O

β O
= O
1 O
on O
57 O
Atari B-DAT
games O
trained O
on O
10 O
machines O

each O
algorithm O
across O
all O
57 O
Atari B-DAT
games. O
We O
also O
evaluated O
mean O

other O
state-of-art O
algorithms O
across O
57 O
Atari B-DAT
games O
for O
a O
fixed O
random O

on O
a O
subset O
of O
7 O
Atari B-DAT
games O
without O
prioritization O
using O
non-distributional O

human O
normalized O
score O
across O
57 O
Atari B-DAT
games O
is O
almost O
equivalent O
to O

rank O
was O
evaluated O
across O
57 O
Atari B-DAT
games. O
For O
Elo O
score O
evaluation O

more O
scores O
on O
a O
given O
Atari B-DAT

6.6 O
Atari B-DAT
results O

In O
this O
work, O
we O
explore O
the B-DAT
benefits O
of O
using O
a O
man O

recognition O
accuracy O
of O
58.14% O
on O
the B-DAT
validation O
set O
of O
Static O
Facial O

Expressions O
in O
the B-DAT
Wild O
(SFEW O
2.0) O
and O
87.0 O

% O
on O
the B-DAT
vali- O
dation O
set O
of O
Real-World O

Both O
of O
these B-DAT
results O
are O
the O
best O
results O
we O
are O
aware O

leverage O
covariance O
pooling O
to O
capture O
the B-DAT
tem- O
poral O
evolution O
of O
per-frame O

recognition. O
Our O
reported O
results O
demonstrate O
the B-DAT
advantage O
of O
pooling O
image-set O
features O

temporally O
by O
stacking O
the B-DAT
designed O
manifold O
network O
of O
covariance O

cating O
the B-DAT
state O
of O
our O
mind. O
Both O

different O
facial O
expression O
classes O
from O
the B-DAT
SFEW O
dataset. O
Bottom: O
distortion O
of O

region O
between O
two O
eyebrows O
in O
the B-DAT
corresponding O
facial O
images O

further B-DAT
dimensionality O
reduction O
we O
borrow O
the O
concepts O
from O
the O
manifold O
network O

pooling O
co- O
variance O
matrix O
from O
the B-DAT
outputs O
of O
CNNs. O
[25] O
proposed O

to O
compute O
second-order O
statistics O
in O
the B-DAT
set- O
ting O
of O
CNNs. O
However O

motivation O
for O
exploring O
them B-DAT
in O
the O
context O
of O
fa- O
cial O
expression O

In O
summary, O
the B-DAT
contribution O
of O
this O
paper O
is O

both O
videos O
and O
images O
in O
the B-DAT
context O
of O
facial O
expression O
recognition O

Most O
of O
the B-DAT
recent O
approaches O
in O
facial O
expression O

to O
train O
classifiers O
or O
fine-tune O
the B-DAT
whole O
network. O
Use O
of O
ensemble O

multiple O
CNNs O
and O
fusion O
of O
the B-DAT
predicted O
scores O
is O
also O
widely O

ensemble O
of O
CNNs O
to O
achieve O
the B-DAT
best O
reported O
score. O
There, O
pre O

statistics. O
Covariance O
pool- O
ing, O
on O
the B-DAT
other O
hand O
captures O
second-order O
statistics O

. O
One O
of O
the B-DAT
earliest O
works O
employing O
covariance O
pooling O

use O
various O
techniques O
to O
capture O
the B-DAT
temporal O
evolution O
of O
the O
per-features O

was O
used O
as O
one O
of O
the B-DAT
summary O
statistics O
of O
per-frame O
features O

recog- O
nition. O
Here, O
we O
use O
the B-DAT
methods O
in O
[17] O
as O
baseline O

and O
use O
the B-DAT
SPD O
Riemannian O
networks O
instead O
of O

Facial O
expression O
is O
localized O
in O
the B-DAT
facial O
region O
whereas O
images O
in O

the B-DAT
wild O
contain O
large O
irrelevant O
infor O

landmark O
locations. O
Next, O
we O
feed O
the B-DAT
normalized O
faces O
into O
a O
deep O

CNN. O
To O
pool O
the B-DAT
feature O
maps O
spatially O
from O
the O

covariance O
pooling, O
and O
then B-DAT
employ O
the O
manifold O
network O
[11] O
to O
deeply O

learn O
the B-DAT
second-order O
statistics. O
The O
pipeline O
of O

As O
the B-DAT
case O
of O
image-based O
facial O
expression O

recogni- O
tion, O
videos O
in O
the B-DAT
wild O
contain O
large O
irrelevant O
information O

. O
First, O
all O
the B-DAT
frames O
are O
extracted O
from O
a O

each O
individual O
frame. O
Depending O
on O
the B-DAT
feature O
extraction O
algorithm, O
ei- O
ther O

image O
features O
are O
extracted O
from O
the B-DAT
normalized O
faces O
or O
the O
normalized O

convolu- O
tions O
are O
applied O
to O
the B-DAT
concatenated O
frames. O
Intuitively, O
as O
the O

temporal O
convariance O
can O
capture O
the B-DAT
useful O
facial O
motion O
pattern, O
we O

propose O
to O
pool O
the B-DAT
frames O
over O
time. O
To O
deeply O

learn O
the B-DAT
temporal O
second-order O
information, O
we O
also O

employ O
the B-DAT
manifold O
network O
[11] O
for O
dimensionality O

Accordingly, O
the B-DAT
core O
techniques O
of O
the O
two O
proposed O
models O
are O
spatial/temporal O

covariance O
pooling O
and O
the B-DAT
manifold O
network O
for O
learning O
the O

second-order O
features O
deeply. O
In O
the B-DAT
following O
we O
will O
introduce O
the O

be O
used O
to O
compactly O
summarize O
the B-DAT
second-order O
information O
in O
the O
set O

fn O
∈ O
Rd O
be O
the B-DAT
set O
of O
features, O
the O
covari O

In O
order O
to O
em- O
ploy O
the B-DAT
geometric O
structure O
preserving O
layers O
of O

the B-DAT
SPD O
manifold O
network O
[11], O
the O
covariance O
matrices O
are O
required O
to O

be O
SPD. O
However, O
even O
if O
the B-DAT
matrices O
are O
only O
positive O
semi O

trace O
to O
diagonal O
entries O
of O
the B-DAT
covariance O
matrix O

Let O
X O
∈ O
Rw×h×d O
be O
the B-DAT
output O
obtained O
after O
several O
convolutional O

and O
number O
of O
channels O
in O
the B-DAT
output O
respectively. O
X O
can O
be O

of O
X′, O
we O
can O
capture O
the B-DAT
variation O
across O
channels O
by O
computing O

can O
compute O
covariance O
matrix O
using O
the B-DAT
Eqn. O
1 O
and O
regularize O
it O

thus O
obtained O
typically O
reside O
on O
the B-DAT
Riemannian O
manifold O
of O
SPD O
matrices O

log- O
arithm O
operation O
to O
flatten O
the B-DAT
Riemannian O
manifold O
struc- O
ture O
to O

SPD O
matrices O
and O
to O
flatten O
the B-DAT
Riemannian O
manifold O
to O
be O
able O

this O
subsection, O
we O
briefly O
discuss O
the B-DAT
layers O
intro- O
duced O
in O
[11 O

of O
these B-DAT
conditions O
and O
plays O
the O
same O
role O
as O
traditional O
fully O

be O
weight O
matrix O
in O
the B-DAT
space O

be O
output O
matrix, O
then B-DAT
k-th O
the O
bilinear O
mapping O
fkb O
is O
defined O

used O
to O
introduce O
non-linearity O
in O
the B-DAT
similar O
way O
as O
Rec- O
tified O

matrix, O
Xk O
be O
output O
matrix, O
the B-DAT
LogEig O
layer O
applied O
in O
k-th O

or O
acted O
facial O
expressions O
in O
the B-DAT
wild O
were O
chosen. O
Such O
datasets O

are O
better O
approximation O
to O
the B-DAT
real O
world O
scenarios O
than O
posed O

use O
Static B-DAT
Facial I-DAT
Expressions I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(SFEW) O
2.0 O
[2] O
[1 O

Facial O
landmark O
points O
provided O
by O
the B-DAT
authors O
were O
de- O
tected O
using O

from O
various O
search O
engines O
and O
the B-DAT
facial O
landmarks O
were O
an- O
notated O

own O
downsides. O
Though O
EmotioNet O
is O
the B-DAT
largest O
existing O
dataset O
for O
facial O

expression O
recognition, O
the B-DAT
images O
were O
automatically O
an- O
notated O

and O
the B-DAT
labels O
are O
incomplete. O
FER-2013, O
contains O

use O
Acted O
Facial O
Expressions O
in O
the B-DAT
Wild O
(AFEW) O
dataset O
to O
compare O

case O
of O
SFEW O
2.0 O
dataset, O
the B-DAT
landmarks O
and O
aligned O
images O
provided O

Images O
and O
videos O
captured O
in O
the B-DAT
wild O
contain O
large O
amount O
of O

helps O
remove O
non-essential O
information O
from O
the B-DAT
data O
samples. O
Furthermore, O
to O
be O

alignment O
additionally O
helps O
to O
capture O
the B-DAT
dynamic O
evolu O

models O
on O
validation O
set O
of O
the B-DAT
RAF O
and O
SFEW O
2.0 O
datasets O

. O
Here O
the B-DAT
models O
labelled O
‡ O
were O
trained O

facial O
features O
across O
images O
of O
the B-DAT
same O
videos O
in O
an O
effective O

and O
midpoint O
of O
corners O
of O
the B-DAT
lips O
were O
used O
for O
alignment O

In O
Table O
1 O
we O
present O
the B-DAT
comparison O
of O
accuracies O
of O
training O

a O
baseline O
model, O
we O
take O
the B-DAT
network O
architecture O
presented O
in O
[16 O

their B-DAT
base O
line O
model. O
So O
the O
networks O
are O
not O
trained O
again O

use O
center O
loss[22] O
to O
train O
the B-DAT
network O
in O
all O
cases O
rather O

were O
trained O
separately. O
Note O
that O
the B-DAT
models O
labelled O
‡ O
were O
trained O

dataset. O
It O
is O
evident O
from O
the B-DAT
table O
that O
fine-tuning O
the O
Inception-ResnetV1 O

introduce O
covariance O
pooling, O
we O
use O
the B-DAT
baseline O
model O
from O
[16 O

introduce O
covariance O
pooling O
and O
subsequently O
the B-DAT
layers O
from O
the O
SPD O
manifold O

network O
(SPDNet) O
after O
the B-DAT
final O
convolutional O
layer. O
While O
introducing O

experimented O
with O
various O
models O
for O
the B-DAT
ar- O
chitecture. O
The O
details O
of O

the B-DAT
various O
models O
considered O
are O
summarized O

below O
in O
Table O
3. O
For O
the B-DAT
RAF O
database, O
as O
stated O
earlier O

, O
the B-DAT

2000 O
dimensional O
feature O
depending O
on O
the B-DAT
model O
considered). O
It O
is O
worth O

It O
could O
be O
argued O
that O
the B-DAT
datasets O
used O
for O
pre-training O
were O

almost O
3.7% O
over O
baseline O
in O
the B-DAT

SFEW O
2.0 O
dataset O
justifies O
the B-DAT
use O
of O
SPDNet O
for O
facial O

to O
point O
out O
that O
on O
the B-DAT
SFEW O
2.0 O
and O
AFEW O
datasets O

for O
correct O
recognition O
to O
the B-DAT
samples O
on O
which O
face O
detection O

Confusion O
matrix O
for O
Model-2 O
on O
the B-DAT
RAF O
dataset O

For O
comparing O
the B-DAT
benefits O
of O
using O
SPDNet O
over O

frame O
of O
a O
video O
and O
the B-DAT
video O
was O
modeled O
with O
a O

Confusion O
matrix O
for O
Model-4 O
on O
the B-DAT
SFEW O
2.0 O
dataset O

for O
our O
method O
(4-Bire) O
on O
the B-DAT
AFEW O
dataset O

proposed O
methods, O
baseline O
method O
and O
the B-DAT
accuracies O
of O
other O
C3D O
and O

existing O
methods O
is O
not O
within O
the B-DAT
scope O
of O
this O
work. O
As O

was O
able O
to O
slightly O
surpass O
the B-DAT
results O
of O
the O
base O
line O

In O
this O
work, O
we O
exploit O
the B-DAT
use O
of O
SPDNet O
on O
facial O

the B-DAT

facial O
expression O
recognition O
problems O
on O
the B-DAT
SFEW O
2.0 O
and O
RAF O
datasets O

the B-DAT

In O
the B-DAT
context O
of O
video-based O
facial O
expression O

Samples O
from O
each O
class O
of O
the B-DAT
SFEW O
dataset O
that O
were O
most O

fusion O
of O
multiple O
models. O
Here O
the B-DAT
results O
of O
the O
methods O
marked O

which O
is O
worse O
than O
the B-DAT
score O
reported O
[11]. O
It O
is O

dataset O
compared O
to O
parameters O
in O
the B-DAT
network. O
Further O
work O
is O
necessary O

is O
able O
to O
further B-DAT
improve O
the O
effectiveness O
of O
second-order O
statistics. O
Formally O

, O
the B-DAT
SPD O
form O
of O
Gaussian O
matrix O

where O
Σ O
is O
the B-DAT
covariance O
matrix O
defined O
in O
Eqn O

is O
the B-DAT
mean O
of O
the O
samples O
f1, O
f2 O

recognition O
in O
the B-DAT
wild O
challenge O
2014: O
Baseline, O
data O

watching O
tv. O
In O
Proceedings O
of O
the B-DAT
British O
Machine O
Vision O
Conference O
(BMVC O

Zhang. O
Emo- O
tion O
recognition O
in O
the B-DAT
wild O
from O
videos O
using O
images O

. O
In O
Proceedings O
of O
the B-DAT
18th O
ACM O
International O
Conference O
on O

An O
accurate, O
real-time O
algorithm O
for O
the B-DAT
auto- O
matic O
annotation O
of O
a O

million O
facial O
expressions O
in O
the B-DAT
wild. O
In O
2016 O
IEEE O
Conference O

pooling. O
In O
Pro- O
ceedings O
of O
the B-DAT
12th O
European O
Conference O
on O
Computer O

in O
video. O
In O
Proceedings O
of O
the B-DAT
2015 O
ACM O
on O
Inter- O
national O

networks. O
In O
Proceed- O
ings O
of O
the B-DAT
18th O
ACM O
International O
Conference O
on O

expression O
recognition. O
In O
Proceedings O
of O
the B-DAT
2015 O
ACM O
on O
International O
Conference O

learning O
for O
expression O
recognition O
in O
the B-DAT
wild. O
In O
The O
IEEE O
Conference O

fold O
for O
emotion O
recognition O
in O
the B-DAT
wild. O
In O
Proceedings O
of O
the O

A. O
Alemi. O
Inception-v4, O
inception-resnet O
and O
the B-DAT
impact O
of O
residual O
connections O
on O

and O
classification. O
In O
Proceedings O
of O
the B-DAT
9th O
European O
Conference O
on O
Computer O

fusion O
for O
emotion O
recognition O
in O
the B-DAT
wild. O
In O
Proceedings O
of O
the O

for O
emotion O
recogni- O
tion O
in O
the B-DAT
wild. O
In O
Proceedings O
of O
the O

learning. O
In O
Pro- O
ceedings O
of O
the B-DAT
2015 O
ACM O
on O
International O
Conference O

estimation O
and O
landmark O
estimation O
in O
the B-DAT
wild. O
In O
IEEE O
Conference O
on O

on O
the O
validation O
set O
of O
Static B-DAT
Facial I-DAT
Expressions I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(SFEW O
2.0) O
and O
87.0% O
on O

against O
standard O
results, O
we O
use O
Static B-DAT
Facial I-DAT
Expressions I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(SFEW) O
2.0 O
[2] O
[1] O
dataset O

Covariance O
Pooling O
for O
Facial B-DAT
Expression O
Recognition O

the O
validation O
set O
of O
Static B-DAT
Facial I-DAT
Expressions I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(SFEW O

1. O
Introduction O
Facial B-DAT
expressions O
play O
an O
important O
role O

2.1. O
Facial B-DAT
Expression O
Recognition O
from O
Images O

2.2. O
Facial B-DAT
Expression O
Recognition O
from O
Videos O

3. O
Facial B-DAT
Expression O
Recognition O
and O
Covari- O
ance O

Facial B-DAT
expression O
is O
localized O
in O
the O

Image-based O
Facial B-DAT
Expression O
Recognition O
For O
com- O
paring O

standard O
results, O
we O
use O
Static B-DAT
Facial I-DAT
Expressions I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(SFEW O

from O
videos O
of O
AFEW O
dataset. O
Facial B-DAT
landmark O
points O
provided O
by O
the O

Video-based O
Facial B-DAT
Expression O
Recognition O
For O
video- O
based O

expression O
recognition, O
we O
use O
Acted O
Facial B-DAT
Expressions O
in O
the O
Wild O
(AFEW O

Static O
Facial O
Expressions O
in O
the O
Wild B-DAT
(SFEW O
2.0) O
and O
87.0% O
on O

Static O
Facial O
Expressions O
in O
the O
Wild B-DAT
(SFEW) O
2.0 O
[2] O
[1] O
dataset O

Acted O
Facial O
Expressions O
in O
the O
Wild B-DAT
(AFEW) O
dataset O
to O
compare O
our O

on O
the O
validation O
set O
of O
Static B-DAT
Facial O
Expressions O
in O
the O
Wild O

against O
standard O
results, O
we O
use O
Static B-DAT
Facial O
Expressions O
in O
the O
Wild O

able O
to O
capture O
such O
distortions O
in B-DAT
regional O
facial O
fea- O
tures. O
In O

such O
kind B-DAT
of O
manifold O
networks O
in O
conjunction O
with O
tradi- O
tional O
convolutional O

networks O
for O
spatial O
pooling B-DAT
within O
in O

- O
dividual O
image O
feature O
maps O
in B-DAT
an O
end-to-end O
deep O
learning O
manner O

set O
of O
Static B-DAT
Facial I-DAT
Expressions I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(SFEW O
2.0) O
and O

expressions O
play O
an O
important O
role O
in B-DAT
communi O

recommendations, O
detec- O
tion O
of O
pain B-DAT
in O
telemedicine O
etc O

of O
region O
between O
two O
eyebrows O
in B-DAT
the O
corresponding O
facial O
images O

in B-DAT
Figure O
1, O
facial O
expression O
recognition O

it O
together O
with O
conventional O
CNNs O
in B-DAT
an O
end-to-end O
fashion. O
It O
is O

variance O
pooling B-DAT
was O
initially O
used O
in O
[13] O
for O
pooling O
co- O
variance O

alternative O
to O
compute O
second-order O
statistics O
in B-DAT
the O
set- O
ting O
of O
CNNs O

strong O
motivation O
for O
exploring B-DAT
them O
in O
the O
context O
of O
fa- O
cial O

better O
able O
to O
capture O
distortions O
in B-DAT
regional O
facial O
features, O
covariance O
pooling O

for O
both O
videos O
and O
images O
in B-DAT
the O
context O
of O
facial O
expression O

Most O
of O
the O
recent O
approaches O
in B-DAT
facial O
expression O
recog- O
nition O
from O

to O
be O
successful. O
For O
example, O
in B-DAT
Emotiw2015 O
sub O
chal- O
lenge O
on O

done O
on O
FER-2013 O
dataset. O
Recently, O
in B-DAT
[3], O
authors O
reported O
validation O
accuracy O

9]. O
State-of- O
art O
result O
reported O
in B-DAT
[9] O
was O
obtained O
by O
score O

summary O
statistics O
of O
per-frame O
features O
in B-DAT
[17]. O
Kernel- O
based O
partial O
least O

Here, O
we O
use O
the O
methods O
in B-DAT
[17] O
as O
baseline O
and O
use O

Facial O
expression O
is O
localized O
in B-DAT
the O
facial O
region O
whereas O
images O

in B-DAT
the O
wild O
contain O
large O
irrelevant O

facial O
expression O
recognition O
is O
shown O
in B-DAT
Figure O
2 O

facial O
expression O
recogni- O
tion, O
videos O
in B-DAT
the O
wild O
contain O
large O
irrelevant O

ex- O
pression O
recognition O
is O
illustrated O
in B-DAT
Figure O
3 O

compactly O
summarize O
the O
second-order O
information B-DAT
in O
the O
set. O
If O
f1, O
f2 O

of O
linearly B-DAT
independent O
compo- O
nents O
in O
{f1, O
f2 O

expression O
recognition O
problem, O
as O
shown O
in B-DAT
Figure O
2, O
outputs O
from O
final O

height O
and O
number O
of O
channels O
in B-DAT
the O
output O
respectively. O
X O
can O

covariance O
as O
in B-DAT
Eqn O
1 O
and O
regularizing O
thus O

Temporal O
Pooling: B-DAT
As O
illus- O
trated O
in O
Figure O
3, O
covariance O
pooling O
can O

be O
employed O
in B-DAT
[17] O
to O
pool O
temporal O
features O

discuss O
the O
layers O
intro- B-DAT
duced O
in O
[11] O
for O
learning O
on O
Riemannian O

dk×dk−1 O
∗ O
be O
weight O
matrix O
in B-DAT
the O
space O

be O
used O
to O
introduce B-DAT
non-linearity O
in O
the O
similar O
way O
as O
Rec O

tified O
Linear B-DAT
Unit O
(ReLU) O
layers O
in O
traditional O
neural O
net- O
works. O
If O

Lo- O
gEig O
layer O
endows O
elements O
in B-DAT
Riemannian O
manifold O
with O
a O
Lie O

matrix, O
the O
LogEig O
layer O
applied O
in B-DAT
k-th O
layer O
fkl O
is O
defined O

with O
2-BiRe O
layers O
is O
shown O
in B-DAT
Figure O
4 O

real O
or O
acted O
facial O
expressions O
in B-DAT
the O
wild O
were O
chosen. O
Such O

we O
use O
Static B-DAT
Facial I-DAT
Expressions I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(SFEW) O
2.0 O
[2 O

sam- O
ples O
or O
are O
taken O
in B-DAT
well O
posed O
laboratory O
setting O

we O
use O
Acted O
Facial O
Expressions O
in B-DAT
the O
Wild O
(AFEW) O
dataset O
to O

383 O
for O
validation. O
Just O
as O
in B-DAT
case O
of O
SFEW O
2.0 O
dataset O

them O
are O
either O
sam- O
pled O
in B-DAT
well O
controlled O
laboratory O
environment O
or O

28]. O
Images O
and O
videos O
captured O
in B-DAT
the O
wild O
contain O
large O
amount O

able O
to O
compare O
vari- O
ations O
in B-DAT
local O
facial O
features O
across O
images O

images O
of O
the O
same O
videos O
in B-DAT
an O
effective O
manner O

take O
the O
network O
architecture O
presented O
in B-DAT
[16]. O
The O
scores O
reported O
on O

for O
VGG O
network O
and O
AlexNet O
in B-DAT
[16] O
is O
less O
compared O
to O

loss[22] O
to O
train B-DAT
the O
network O
in O
all O
cases O
rather O
than O
local O

random O
flip. O
For O
SFEW O
2.0, O
in B-DAT
all O
cases, O
output O
of O
sec O

pa- O
rameters O
to O
be O
learned O
in B-DAT
deeper O
models. O
For O
further O
ex O

various O
models O
considered O
are O
summarized O
in B-DAT
Table O
2 O

connected O
layers. O
Various O
models O
described O
in B-DAT
Table O
2 O
and O
their O
accuracies O

are O
listed O
below O
in B-DAT
Table O
3. O
For O
the O
RAF O

network O
was O
trained B-DAT
in O
end-to-end O
fashion. O
However, O
for O
SFEW O

ensemble O
of O
convolutional O
neural O
networks O
in B-DAT
[26] O
and O
[15]. O
It O
could O

used O
for O
pre-training B-DAT
were O
different O
in O
our O
case O
and O
in O
[26][15 O

of O
almost O
3.7% O
over O
baseline B-DAT
in O
the O

AFEW O
datasets, O
face O
detection O
failed O
in B-DAT
several O
images O
and O
videos. O
To O

covariance O
matrices O
as O
features O
[17] O
in B-DAT
baseline O
method. O
128 O
dimen- O
sional O

covariance O
pooling B-DAT
to O
C3D O
model O
in O
[9] O
and O
did O
not O
obtain O

recog- O
nition O
problem, O
architecture O
presented O
in B-DAT
Figure O
8 O
can O
be O
trained O

in B-DAT
end-to-end O
training. O
Though O
with O
brief O

AFEW O
dataset O
compared O
to O
parameters O
in B-DAT
the O
network. O
Further O
work O
is O

second-order O
statistics. O
As O
studied O
in B-DAT
[12], O
Gaussian O
matrix O
is O
able O

is O
the O
covariance O
matrix O
defined B-DAT
in O
Eqn. O
1, O
and O

statistics. O
It O
was O
also O
employed O
in B-DAT
[25] O
to O
develop O
second-order O
convolutional O

recognition O
in B-DAT
the O
wild O
challenge O
2014: O
Baseline O

C. O
Zhang. O
Emo- O
tion O
recognition O
in B-DAT
the O
wild O
from O
videos O
using O

of O
a O
million O
facial O
expressions O
in B-DAT
the O
wild. O
In O
2016 O
IEEE O

networks O
for O
emotion O
recog- O
nition O
in B-DAT
video. O
In O
Proceedings O
of O
the O

Chuang, O
and O
Y. O
Bengio. O
Challenges O
in B-DAT
representation O
learning. O
Neural O
Netw., O
64(C):59–63 O

locality-preserving B-DAT
learning O
for O
expression O
recognition O
in O
the O
wild. O
In O
The O
IEEE O

mani- O
fold O
for O
emotion O
recognition O
in B-DAT
the O
wild. O
In O
Proceedings O
of O

Multi-clue O
fusion O
for O
emotion O
recognition O
in B-DAT
the O
wild. O
In O
Proceedings O
of O

relations O
for O
emotion O
recogni- O
tion O
in B-DAT
the O
wild. O
In O
Proceedings O
of O

pose O
estimation O
and O
landmark O
estimation O
in B-DAT
the O
wild. O
In O
IEEE O
Conference O

validation O
set O
of O
Static B-DAT
Facial I-DAT
Expressions I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(SFEW O
2.0 O

results, O
we O
use O
Static B-DAT
Facial I-DAT
Expressions I-DAT
in I-DAT
the I-DAT
Wild I-DAT
(SFEW) O
2.0 O

recognition, O
we O
use O
Acted O
Facial O
Expressions B-DAT
in O
the O
Wild O
(AFEW) O
dataset O

dation O
set O
of O
Real-World O
Affective O
Faces B-DAT
(RAF) O
Database1. O
Both O
of O
these O

1] O
dataset O
and O
Real-world O
Affective O
Faces B-DAT
(RAF) O
dataset O
[16]. O
SFEW O
2.0 O

the O
vali- O
dation O
set O
of O
Real B-DAT

2.0 O
[2] O
[1] O
dataset O
and O
Real B-DAT

the O
vali- O
dation O
set O
of O
Real-World B-DAT
Affective I-DAT
Faces I-DAT
(RAF) O
Database1. O
Both O
of O
these O

World B-DAT
Affective I-DAT
Faces I-DAT
(RAF) O
Database1. O
Both O
of O
these O

vali- O
dation O
set O
of O
Real-World B-DAT
Affective I-DAT
Faces I-DAT
(RAF) O
Database1. O
Both O
of O

2] O
[1] O
dataset O
and O
Real-world O
Affective B-DAT
Faces O
(RAF) O
dataset O
[16]. O
SFEW O

the O
vali- O
dation O
set O
of O
Real-World B-DAT
Affective O
Faces O
(RAF) O
Database1. O
Both O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

patients O
contained O
in O
the O
publicly-available O
MIMIC-III B-DAT
database O
into O
dataframes O
that O
are O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Information O
Mart O
for O
Intensive O
Care O
(MIMIC-III) B-DAT
dataset O
(Johnson O
et O
al., O
2016 O

). O
While O
MIMIC-III B-DAT
is O
publicly O
available, O
work- O
ing O

with O
MIMIC-III B-DAT
data O
remains O
challenging O
due O
to O

in O
machine O
learning O
studies O
on O
MIMIC-III B-DAT
data O
because O
researchers O
develop O
independent O

preprocess, O
and O
represent O
data O
from O
MIMIC-III B-DAT
v1.4, O
including O
static O
features O
and O

of O
the O
raw O
features O
of O
MIMIC-III B-DAT
together O
into O
clinically O
meaningful O
buckets O

and O
extensible O
pipeline O
to O
ingest O
MIMIC-III B-DAT
database O
and O
yield O
Pandas O
DataFrames O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

into O
many O
prediction O
tasks O
using O
MIMIC-III B-DAT
data O
and O
will O
be O
of O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

prescrip- O
tion O
signals O
in O
the O
MIMIC-III B-DAT
database. O
Without O
additional O
insight O
into O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

MIMIC-III B-DAT
provides O
a O
high O
time-resolution O
of O

vasopressor O
usage, O
consistent O
with O
the O
MIMIC-III B-DAT
codebase O
(Johnson O
et O
al., O
2017b O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

lab O
measurements, O
and O
treatment O
interventions, O
MIMIC-III B-DAT
contains O
more O
clinical O
information O
such O

in O
addressing O
this O
challenge O
with O
MIMIC-III B-DAT

released O
public O
code O
that O
transforms O
MIMIC-III B-DAT
into O
processed O
dataset O
views O
suitable O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

consistent O
with O
many O
papers O
using O
MIMIC-III B-DAT
(Ghassemi O
et O
al., O
2014, O
2015 O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

selection O
and O
pre-processing O
pipeline O
for O
MIMIC-III B-DAT
labs O
and O
vitals. O
The O
system O

machine O
learning O
models O
over O
the O
MIMIC-III B-DAT
data O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

than O
89 O
years O
old O
in O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

data O
processing O
frameworks. O
We O
present O
MIMIC B-DAT

patients O
contained O
in O
the O
publicly-available O
MIMIC B-DAT

in O
common O
machine O
learning O
pipelines. O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

Information O
Mart O
for O
Intensive O
Care O
(MIMIC B-DAT

Johnson O
et O
al., O
2016). O
While O
MIMIC B-DAT

publicly O
available, O
work- O
ing O
with O
MIMIC B-DAT

in O
machine O
learning O
studies O
on O
MIMIC B-DAT

In O
this O
paper, O
we O
introduce O
MIMIC B-DAT

preprocess, O
and O
represent O
data O
from O
MIMIC B-DAT

2007) O
that O
demonstrate O
the O
use O
MIMIC B-DAT

and O
baseline O
model O
building. O
Lastly, O
MIMIC B-DAT

of O
the O
raw O
features O
of O
MIMIC B-DAT

a O
detailed O
description O
of O
the O
MIMIC B-DAT

Technical O
Significance O
MIMIC B-DAT

and O
extensible O
pipeline O
to O
ingest O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

into O
many O
prediction O
tasks O
using O
MIMIC B-DAT

features O
and O
temporal O
dynam- O
ics, O
MIMIC B-DAT

1 O
summarizes O
the O
workflow O
in O
MIMIC B-DAT

-Extract. O
From O
the O
MIMIC B-DAT
relational O
database, O
SQL O
query O
results O

Figure O
1: O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

of O
Output O
Tables O
Generated O
by O
MIMIC B-DAT

prescrip- O
tion O
signals O
in O
the O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

unit O
conversion O
and O
outlier O
detection, O
MIMIC B-DAT

into O
hourly O
buckets. O
By O
default, O
MIMIC B-DAT

and O
includes O
statistics O
about O
the O
MIMIC B-DAT

vasopressor O
usage, O
consistent O
with O
the O
MIMIC B-DAT

While O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

Keywords O
Functions O
in O
MIMIC B-DAT

querying O
respective O
tables O
in O
the O
MIMIC B-DAT
relational O
database. O
We O
plan O
to O

cohort O
for O
all O
output O
dataframes, O
MIMIC B-DAT

lab O
measurements, O
and O
treatment O
interventions, O
MIMIC B-DAT

in O
addressing O
this O
challenge O
with O
MIMIC B-DAT

released O
public O
code O
that O
transforms O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

the O
input O
representation O
schema O
of O
MIMIC B-DAT
(at O
the O
ItemID O
level), O
and O

In O
this O
work, O
we O
profile O
MIMIC B-DAT

MIMIC B-DAT

consistent O
with O
many O
papers O
using O
MIMIC B-DAT

addition O
to O
this O
default O
cohort, O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

models O
using O
data O
extracted O
with O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

We O
also O
use O
MIMIC B-DAT

meaningful O
predictions, O
we O
extract O
from O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

Though O
highly O
useful, O
MIMIC B-DAT

MIMIC B-DAT

selection O
and O
pre-processing O
pipeline O
for O
MIMIC B-DAT

practice O
drift. O
We O
demonstrate O
how O
MIMIC B-DAT

of O
models. O
Ultimately, O
we O
hope O
MIMIC B-DAT

machine O
learning O
models O
over O
the O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

than O
89 O
years O
old O
in O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

III B-DAT

III B-DAT
database O
into O
dataframes O
that O
are O

III B-DAT

III) B-DAT
dataset O
(Johnson O
et O
al., O
2016 O

III B-DAT
is O
publicly O
available, O
work- O
ing O

III B-DAT
data O
remains O
challenging O
due O
to O

III B-DAT
data O
because O
researchers O
develop O
independent O

III B-DAT
v1.4, O
including O
static O
features O
and O

III B-DAT
together O
into O
clinically O
meaningful O
buckets O

III B-DAT
database O
and O
yield O
Pandas O
DataFrames O

III B-DAT

III B-DAT
data O
and O
will O
be O
of O

III B-DAT

III B-DAT
database. O
Without O
additional O
insight O
into O

III B-DAT

III B-DAT
provides O
a O
high O
time-resolution O
of O

III B-DAT
codebase O
(Johnson O
et O
al., O
2017b O

III B-DAT

III B-DAT
contains O
more O
clinical O
information O
such O

III B-DAT

III B-DAT
into O
processed O
dataset O
views O
suitable O

III B-DAT

III B-DAT
(Ghassemi O
et O
al., O
2014, O
2015 O

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT
labs O
and O
vitals. O
The O
system O

III B-DAT
data O

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

patients O
contained O
in O
the O
publicly-available O
MIMIC-III B-DAT
database O
into O
dataframes O
that O
are O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Information O
Mart O
for O
Intensive O
Care O
(MIMIC-III) B-DAT
dataset O
(Johnson O
et O
al., O
2016 O

). O
While O
MIMIC-III B-DAT
is O
publicly O
available, O
work- O
ing O

with O
MIMIC-III B-DAT
data O
remains O
challenging O
due O
to O

in O
machine O
learning O
studies O
on O
MIMIC-III B-DAT
data O
because O
researchers O
develop O
independent O

preprocess, O
and O
represent O
data O
from O
MIMIC-III B-DAT
v1.4, O
including O
static O
features O
and O

of O
the O
raw O
features O
of O
MIMIC-III B-DAT
together O
into O
clinically O
meaningful O
buckets O

and O
extensible O
pipeline O
to O
ingest O
MIMIC-III B-DAT
database O
and O
yield O
Pandas O
DataFrames O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

into O
many O
prediction O
tasks O
using O
MIMIC-III B-DAT
data O
and O
will O
be O
of O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

prescrip- O
tion O
signals O
in O
the O
MIMIC-III B-DAT
database. O
Without O
additional O
insight O
into O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

MIMIC-III B-DAT
provides O
a O
high O
time-resolution O
of O

vasopressor O
usage, O
consistent O
with O
the O
MIMIC-III B-DAT
codebase O
(Johnson O
et O
al., O
2017b O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

lab O
measurements, O
and O
treatment O
interventions, O
MIMIC-III B-DAT
contains O
more O
clinical O
information O
such O

in O
addressing O
this O
challenge O
with O
MIMIC-III B-DAT

released O
public O
code O
that O
transforms O
MIMIC-III B-DAT
into O
processed O
dataset O
views O
suitable O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

consistent O
with O
many O
papers O
using O
MIMIC-III B-DAT
(Ghassemi O
et O
al., O
2014, O
2015 O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

selection O
and O
pre-processing O
pipeline O
for O
MIMIC-III B-DAT
labs O
and O
vitals. O
The O
system O

machine O
learning O
models O
over O
the O
MIMIC-III B-DAT
data O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

than O
89 O
years O
old O
in O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

data O
processing O
frameworks. O
We O
present O
MIMIC B-DAT

patients O
contained O
in O
the O
publicly-available O
MIMIC B-DAT

in O
common O
machine O
learning O
pipelines. O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

Information O
Mart O
for O
Intensive O
Care O
(MIMIC B-DAT

Johnson O
et O
al., O
2016). O
While O
MIMIC B-DAT

publicly O
available, O
work- O
ing O
with O
MIMIC B-DAT

in O
machine O
learning O
studies O
on O
MIMIC B-DAT

In O
this O
paper, O
we O
introduce O
MIMIC B-DAT

preprocess, O
and O
represent O
data O
from O
MIMIC B-DAT

2007) O
that O
demonstrate O
the O
use O
MIMIC B-DAT

and O
baseline O
model O
building. O
Lastly, O
MIMIC B-DAT

of O
the O
raw O
features O
of O
MIMIC B-DAT

a O
detailed O
description O
of O
the O
MIMIC B-DAT

Technical O
Significance O
MIMIC B-DAT

and O
extensible O
pipeline O
to O
ingest O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

into O
many O
prediction O
tasks O
using O
MIMIC B-DAT

features O
and O
temporal O
dynam- O
ics, O
MIMIC B-DAT

1 O
summarizes O
the O
workflow O
in O
MIMIC B-DAT

-Extract. O
From O
the O
MIMIC B-DAT
relational O
database, O
SQL O
query O
results O

Figure O
1: O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

of O
Output O
Tables O
Generated O
by O
MIMIC B-DAT

prescrip- O
tion O
signals O
in O
the O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

unit O
conversion O
and O
outlier O
detection, O
MIMIC B-DAT

into O
hourly O
buckets. O
By O
default, O
MIMIC B-DAT

and O
includes O
statistics O
about O
the O
MIMIC B-DAT

vasopressor O
usage, O
consistent O
with O
the O
MIMIC B-DAT

While O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

Keywords O
Functions O
in O
MIMIC B-DAT

querying O
respective O
tables O
in O
the O
MIMIC B-DAT
relational O
database. O
We O
plan O
to O

cohort O
for O
all O
output O
dataframes, O
MIMIC B-DAT

lab O
measurements, O
and O
treatment O
interventions, O
MIMIC B-DAT

in O
addressing O
this O
challenge O
with O
MIMIC B-DAT

released O
public O
code O
that O
transforms O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

the O
input O
representation O
schema O
of O
MIMIC B-DAT
(at O
the O
ItemID O
level), O
and O

In O
this O
work, O
we O
profile O
MIMIC B-DAT

MIMIC B-DAT

consistent O
with O
many O
papers O
using O
MIMIC B-DAT

addition O
to O
this O
default O
cohort, O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

models O
using O
data O
extracted O
with O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

We O
also O
use O
MIMIC B-DAT

meaningful O
predictions, O
we O
extract O
from O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

Though O
highly O
useful, O
MIMIC B-DAT

MIMIC B-DAT

selection O
and O
pre-processing O
pipeline O
for O
MIMIC B-DAT

practice O
drift. O
We O
demonstrate O
how O
MIMIC B-DAT

of O
models. O
Ultimately, O
we O
hope O
MIMIC B-DAT

machine O
learning O
models O
over O
the O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

than O
89 O
years O
old O
in O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

III B-DAT

III B-DAT
database O
into O
dataframes O
that O
are O

III B-DAT

III) B-DAT
dataset O
(Johnson O
et O
al., O
2016 O

III B-DAT
is O
publicly O
available, O
work- O
ing O

III B-DAT
data O
remains O
challenging O
due O
to O

III B-DAT
data O
because O
researchers O
develop O
independent O

III B-DAT
v1.4, O
including O
static O
features O
and O

III B-DAT
together O
into O
clinically O
meaningful O
buckets O

III B-DAT
database O
and O
yield O
Pandas O
DataFrames O

III B-DAT

III B-DAT
data O
and O
will O
be O
of O

III B-DAT

III B-DAT
database. O
Without O
additional O
insight O
into O

III B-DAT

III B-DAT
provides O
a O
high O
time-resolution O
of O

III B-DAT
codebase O
(Johnson O
et O
al., O
2017b O

III B-DAT

III B-DAT
contains O
more O
clinical O
information O
such O

III B-DAT

III B-DAT
into O
processed O
dataset O
views O
suitable O

III B-DAT

III B-DAT
(Ghassemi O
et O
al., O
2014, O
2015 O

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT
labs O
and O
vitals. O
The O
system O

III B-DAT
data O

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

patients O
contained O
in O
the O
publicly-available O
MIMIC-III B-DAT
database O
into O
dataframes O
that O
are O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Information O
Mart O
for O
Intensive O
Care O
(MIMIC-III) B-DAT
dataset O
(Johnson O
et O
al., O
2016 O

). O
While O
MIMIC-III B-DAT
is O
publicly O
available, O
work- O
ing O

with O
MIMIC-III B-DAT
data O
remains O
challenging O
due O
to O

in O
machine O
learning O
studies O
on O
MIMIC-III B-DAT
data O
because O
researchers O
develop O
independent O

preprocess, O
and O
represent O
data O
from O
MIMIC-III B-DAT
v1.4, O
including O
static O
features O
and O

of O
the O
raw O
features O
of O
MIMIC-III B-DAT
together O
into O
clinically O
meaningful O
buckets O

and O
extensible O
pipeline O
to O
ingest O
MIMIC-III B-DAT
database O
and O
yield O
Pandas O
DataFrames O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

into O
many O
prediction O
tasks O
using O
MIMIC-III B-DAT
data O
and O
will O
be O
of O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

prescrip- O
tion O
signals O
in O
the O
MIMIC-III B-DAT
database. O
Without O
additional O
insight O
into O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

MIMIC-III B-DAT
provides O
a O
high O
time-resolution O
of O

vasopressor O
usage, O
consistent O
with O
the O
MIMIC-III B-DAT
codebase O
(Johnson O
et O
al., O
2017b O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

lab O
measurements, O
and O
treatment O
interventions, O
MIMIC-III B-DAT
contains O
more O
clinical O
information O
such O

in O
addressing O
this O
challenge O
with O
MIMIC-III B-DAT

released O
public O
code O
that O
transforms O
MIMIC-III B-DAT
into O
processed O
dataset O
views O
suitable O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

consistent O
with O
many O
papers O
using O
MIMIC-III B-DAT
(Ghassemi O
et O
al., O
2014, O
2015 O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

selection O
and O
pre-processing O
pipeline O
for O
MIMIC-III B-DAT
labs O
and O
vitals. O
The O
system O

machine O
learning O
models O
over O
the O
MIMIC-III B-DAT
data O

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

than O
89 O
years O
old O
in O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC-III B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

data O
processing O
frameworks. O
We O
present O
MIMIC B-DAT

patients O
contained O
in O
the O
publicly-available O
MIMIC B-DAT

in O
common O
machine O
learning O
pipelines. O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

Information O
Mart O
for O
Intensive O
Care O
(MIMIC B-DAT

Johnson O
et O
al., O
2016). O
While O
MIMIC B-DAT

publicly O
available, O
work- O
ing O
with O
MIMIC B-DAT

in O
machine O
learning O
studies O
on O
MIMIC B-DAT

In O
this O
paper, O
we O
introduce O
MIMIC B-DAT

preprocess, O
and O
represent O
data O
from O
MIMIC B-DAT

2007) O
that O
demonstrate O
the O
use O
MIMIC B-DAT

and O
baseline O
model O
building. O
Lastly, O
MIMIC B-DAT

of O
the O
raw O
features O
of O
MIMIC B-DAT

a O
detailed O
description O
of O
the O
MIMIC B-DAT

Technical O
Significance O
MIMIC B-DAT

and O
extensible O
pipeline O
to O
ingest O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

into O
many O
prediction O
tasks O
using O
MIMIC B-DAT

features O
and O
temporal O
dynam- O
ics, O
MIMIC B-DAT

1 O
summarizes O
the O
workflow O
in O
MIMIC B-DAT

-Extract. O
From O
the O
MIMIC B-DAT
relational O
database, O
SQL O
query O
results O

Figure O
1: O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

of O
Output O
Tables O
Generated O
by O
MIMIC B-DAT

prescrip- O
tion O
signals O
in O
the O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

unit O
conversion O
and O
outlier O
detection, O
MIMIC B-DAT

into O
hourly O
buckets. O
By O
default, O
MIMIC B-DAT

and O
includes O
statistics O
about O
the O
MIMIC B-DAT

vasopressor O
usage, O
consistent O
with O
the O
MIMIC B-DAT

While O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

Keywords O
Functions O
in O
MIMIC B-DAT

querying O
respective O
tables O
in O
the O
MIMIC B-DAT
relational O
database. O
We O
plan O
to O

cohort O
for O
all O
output O
dataframes, O
MIMIC B-DAT

lab O
measurements, O
and O
treatment O
interventions, O
MIMIC B-DAT

in O
addressing O
this O
challenge O
with O
MIMIC B-DAT

released O
public O
code O
that O
transforms O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

the O
input O
representation O
schema O
of O
MIMIC B-DAT
(at O
the O
ItemID O
level), O
and O

In O
this O
work, O
we O
profile O
MIMIC B-DAT

MIMIC B-DAT

consistent O
with O
many O
papers O
using O
MIMIC B-DAT

addition O
to O
this O
default O
cohort, O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

models O
using O
data O
extracted O
with O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

We O
also O
use O
MIMIC B-DAT

meaningful O
predictions, O
we O
extract O
from O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

Though O
highly O
useful, O
MIMIC B-DAT

MIMIC B-DAT

selection O
and O
pre-processing O
pipeline O
for O
MIMIC B-DAT

practice O
drift. O
We O
demonstrate O
how O
MIMIC B-DAT

of O
models. O
Ultimately, O
we O
hope O
MIMIC B-DAT

machine O
learning O
models O
over O
the O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

than O
89 O
years O
old O
in O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

MIMIC B-DAT

Preprocessing, O
and O
Representation O
Pipeline O
for O
MIMIC B-DAT

III B-DAT

III B-DAT
database O
into O
dataframes O
that O
are O

III B-DAT

III) B-DAT
dataset O
(Johnson O
et O
al., O
2016 O

III B-DAT
is O
publicly O
available, O
work- O
ing O

III B-DAT
data O
remains O
challenging O
due O
to O

III B-DAT
data O
because O
researchers O
develop O
independent O

III B-DAT
v1.4, O
including O
static O
features O
and O

III B-DAT
together O
into O
clinically O
meaningful O
buckets O

III B-DAT
database O
and O
yield O
Pandas O
DataFrames O

III B-DAT

III B-DAT
data O
and O
will O
be O
of O

III B-DAT

III B-DAT
database. O
Without O
additional O
insight O
into O

III B-DAT

III B-DAT
provides O
a O
high O
time-resolution O
of O

III B-DAT
codebase O
(Johnson O
et O
al., O
2017b O

III B-DAT

III B-DAT
contains O
more O
clinical O
information O
such O

III B-DAT

III B-DAT
into O
processed O
dataset O
views O
suitable O

III B-DAT

III B-DAT
(Ghassemi O
et O
al., O
2014, O
2015 O

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT
labs O
and O
vitals. O
The O
system O

III B-DAT
data O

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT

III B-DAT

Mart O
for O
Intensive O
Care O
III O
(MIMIC-III B-DAT

Mart O
for O
Intensive O
Care O
III, O
MIMIC-III B-DAT
Waveform O
Database O
records O
[17]. O
We O

early O
mortality O
prediction O
using O
the O
MIMIC-III B-DAT
dataset. O
We O
describe O
each O
signal O

provides O
a O
review O
on O
the O
MIMIC-III B-DAT
clinical O
dataset O
while O
subsections O
3.2 O

age O
distribution O
over O
the O
Whole O
MIMIC-III B-DAT
(without O
infants) O
and O
the O
Matched O

the O
clinical O
data O
in O
the O
MIMIC-III B-DAT
are O
associated O
with O
the O
related O

age O
distributions O
of O
the O
whole O
MIMIC-III B-DAT
(without O
in- O
fants) O
and O
the O

Table O
1: O
Care O
Units O
in O
MIMIC-III B-DAT

MIMIC-III B-DAT
database O
is O
extracted O
from O
the O

1 O
to O
0.17 O
Hz O
in O
MIMIC-III B-DAT
database. O
To O
avoid O
bi- O
ased O

information O
recorded O
in O
from O
the O
MIMIC-III B-DAT
Waveform O
Database O
Matched O
Subset. O
This O

and O
deep O
learning O
in O
the O
MIMIC-III B-DAT
critical O
care O
database, O
bioRxiv O
(2017 O

A. O
Celi, O
R. O
G. O
Mark, O
MIMIC-III, B-DAT
a O
freely O
accessible O
critical O
care O

Mart O
for O
Intensive O
Care O
III O
(MIMIC B-DAT

Mart O
for O
Intensive O
Care O
III, O
MIMIC B-DAT

early O
mortality O
prediction O
using O
the O
MIMIC B-DAT

provides O
a O
review O
on O
the O
MIMIC B-DAT

age O
distribution O
over O
the O
Whole O
MIMIC B-DAT

is O
conducted O
over O
the O
well-known O
MIMIC B-DAT

the O
clinical O
data O
in O
the O
MIMIC B-DAT

age O
distributions O
of O
the O
whole O
MIMIC B-DAT

Table O
1: O
Care O
Units O
in O
MIMIC B-DAT

MIMIC B-DAT

1 O
to O
0.17 O
Hz O
in O
MIMIC B-DAT

In O
the O
MIMIC B-DAT
III O
dataset, O
the O
number O
of O

information O
recorded O
in O
from O
the O
MIMIC B-DAT

Learning O
Models O
on O
Large O
Healthcare O
MIMIC B-DAT
Datasets, O
arXiv O
preprint O
arXiv:1710.08531 O
(2017 O

and O
deep O
learning O
in O
the O
MIMIC B-DAT

A. O
Celi, O
R. O
G. O
Mark, O
MIMIC B-DAT

Information O
Mart O
for O
Intensive O
Care O
III B-DAT
(MIMIC-III). O
The O
experimental O
results O
demonstrate O

Information O
Mart O
for O
Intensive O
Care O
III, B-DAT
MIMIC-III O
Waveform O
Database O
records O
[17 O

patients. O
Extending O
that, O
the O
APACHE O
III B-DAT
improved O
the O
effectiveness O
of O
mortality O

prediction O
problem O
of O
the O
APACHE O
III B-DAT
by O
adding O
new O
variables O
and O

the O
weights O
utilized O
in O
APACHE O
III B-DAT
[20]. O
The O
traditional O
severity O
of O

III B-DAT
dataset. O
We O
describe O
each O
signal O

III B-DAT
clinical O
dataset O
while O
subsections O
3.2 O

III B-DAT
(without O
infants) O
and O
the O
Matched O

conducted O
over O
the O
well-known O
MIMIC- O
III B-DAT
database O
comprising O
the O
records O
of O

III B-DAT
are O
associated O
with O
the O
related O

III B-DAT
(without O
in- O
fants) O
and O
the O

III B-DAT

III B-DAT
database O
is O
extracted O
from O
the O

III B-DAT
database. O
To O
avoid O
bi- O
ased O

In O
the O
MIMIC O
III B-DAT
dataset, O
the O
number O
of O
patients O

III B-DAT
Waveform O
Database O
Matched O
Subset. O
This O

III B-DAT
critical O
care O
database, O
bioRxiv O
(2017 O

III, B-DAT
a O
freely O
accessible O
critical O
care O

Mart O
for O
Intensive O
Care O
III O
(MIMIC-III B-DAT

Mart O
for O
Intensive O
Care O
III, O
MIMIC-III B-DAT
Waveform O
Database O
records O
[17]. O
We O

early O
mortality O
prediction O
using O
the O
MIMIC-III B-DAT
dataset. O
We O
describe O
each O
signal O

provides O
a O
review O
on O
the O
MIMIC-III B-DAT
clinical O
dataset O
while O
subsections O
3.2 O

age O
distribution O
over O
the O
Whole O
MIMIC-III B-DAT
(without O
infants) O
and O
the O
Matched O

the O
clinical O
data O
in O
the O
MIMIC-III B-DAT
are O
associated O
with O
the O
related O

age O
distributions O
of O
the O
whole O
MIMIC-III B-DAT
(without O
in- O
fants) O
and O
the O

Table O
1: O
Care O
Units O
in O
MIMIC-III B-DAT

MIMIC-III B-DAT
database O
is O
extracted O
from O
the O

1 O
to O
0.17 O
Hz O
in O
MIMIC-III B-DAT
database. O
To O
avoid O
bi- O
ased O

information O
recorded O
in O
from O
the O
MIMIC-III B-DAT
Waveform O
Database O
Matched O
Subset. O
This O

and O
deep O
learning O
in O
the O
MIMIC-III B-DAT
critical O
care O
database, O
bioRxiv O
(2017 O

A. O
Celi, O
R. O
G. O
Mark, O
MIMIC-III, B-DAT
a O
freely O
accessible O
critical O
care O

Mart O
for O
Intensive O
Care O
III O
(MIMIC B-DAT

Mart O
for O
Intensive O
Care O
III, O
MIMIC B-DAT

early O
mortality O
prediction O
using O
the O
MIMIC B-DAT

provides O
a O
review O
on O
the O
MIMIC B-DAT

age O
distribution O
over O
the O
Whole O
MIMIC B-DAT

is O
conducted O
over O
the O
well-known O
MIMIC B-DAT

the O
clinical O
data O
in O
the O
MIMIC B-DAT

age O
distributions O
of O
the O
whole O
MIMIC B-DAT

Table O
1: O
Care O
Units O
in O
MIMIC B-DAT

MIMIC B-DAT

1 O
to O
0.17 O
Hz O
in O
MIMIC B-DAT

In O
the O
MIMIC B-DAT
III O
dataset, O
the O
number O
of O

information O
recorded O
in O
from O
the O
MIMIC B-DAT

Learning O
Models O
on O
Large O
Healthcare O
MIMIC B-DAT
Datasets, O
arXiv O
preprint O
arXiv:1710.08531 O
(2017 O

and O
deep O
learning O
in O
the O
MIMIC B-DAT

A. O
Celi, O
R. O
G. O
Mark, O
MIMIC B-DAT

Information O
Mart O
for O
Intensive O
Care O
III B-DAT
(MIMIC-III). O
The O
experimental O
results O
demonstrate O

Information O
Mart O
for O
Intensive O
Care O
III, B-DAT
MIMIC-III O
Waveform O
Database O
records O
[17 O

patients. O
Extending O
that, O
the O
APACHE O
III B-DAT
improved O
the O
effectiveness O
of O
mortality O

prediction O
problem O
of O
the O
APACHE O
III B-DAT
by O
adding O
new O
variables O
and O

the O
weights O
utilized O
in O
APACHE O
III B-DAT
[20]. O
The O
traditional O
severity O
of O

III B-DAT
dataset. O
We O
describe O
each O
signal O

III B-DAT
clinical O
dataset O
while O
subsections O
3.2 O

III B-DAT
(without O
infants) O
and O
the O
Matched O

conducted O
over O
the O
well-known O
MIMIC- O
III B-DAT
database O
comprising O
the O
records O
of O

III B-DAT
are O
associated O
with O
the O
related O

III B-DAT
(without O
in- O
fants) O
and O
the O

III B-DAT

III B-DAT
database O
is O
extracted O
from O
the O

III B-DAT
database. O
To O
avoid O
bi- O
ased O

In O
the O
MIMIC O
III B-DAT
dataset, O
the O
number O
of O
patients O

III B-DAT
Waveform O
Database O
Matched O
Subset. O
This O

III B-DAT
critical O
care O
database, O
bioRxiv O
(2017 O

III, B-DAT
a O
freely O
accessible O
critical O
care O

Mart O
for O
Intensive O
Care O
III O
(MIMIC-III B-DAT

Mart O
for O
Intensive O
Care O
III, O
MIMIC-III B-DAT
Waveform O
Database O
records O
[17]. O
We O

early O
mortality O
prediction O
using O
the O
MIMIC-III B-DAT
dataset. O
We O
describe O
each O
signal O

provides O
a O
review O
on O
the O
MIMIC-III B-DAT
clinical O
dataset O
while O
subsections O
3.2 O

age O
distribution O
over O
the O
Whole O
MIMIC-III B-DAT
(without O
infants) O
and O
the O
Matched O

the O
clinical O
data O
in O
the O
MIMIC-III B-DAT
are O
associated O
with O
the O
related O

age O
distributions O
of O
the O
whole O
MIMIC-III B-DAT
(without O
in- O
fants) O
and O
the O

Table O
1: O
Care O
Units O
in O
MIMIC-III B-DAT

MIMIC-III B-DAT
database O
is O
extracted O
from O
the O

1 O
to O
0.17 O
Hz O
in O
MIMIC-III B-DAT
database. O
To O
avoid O
bi- O
ased O

information O
recorded O
in O
from O
the O
MIMIC-III B-DAT
Waveform O
Database O
Matched O
Subset. O
This O

and O
deep O
learning O
in O
the O
MIMIC-III B-DAT
critical O
care O
database, O
bioRxiv O
(2017 O

A. O
Celi, O
R. O
G. O
Mark, O
MIMIC-III, B-DAT
a O
freely O
accessible O
critical O
care O

Mart O
for O
Intensive O
Care O
III O
(MIMIC B-DAT

Mart O
for O
Intensive O
Care O
III, O
MIMIC B-DAT

early O
mortality O
prediction O
using O
the O
MIMIC B-DAT

provides O
a O
review O
on O
the O
MIMIC B-DAT

age O
distribution O
over O
the O
Whole O
MIMIC B-DAT

is O
conducted O
over O
the O
well-known O
MIMIC B-DAT

the O
clinical O
data O
in O
the O
MIMIC B-DAT

age O
distributions O
of O
the O
whole O
MIMIC B-DAT

Table O
1: O
Care O
Units O
in O
MIMIC B-DAT

MIMIC B-DAT

1 O
to O
0.17 O
Hz O
in O
MIMIC B-DAT

In O
the O
MIMIC B-DAT
III O
dataset, O
the O
number O
of O

information O
recorded O
in O
from O
the O
MIMIC B-DAT

Learning O
Models O
on O
Large O
Healthcare O
MIMIC B-DAT
Datasets, O
arXiv O
preprint O
arXiv:1710.08531 O
(2017 O

and O
deep O
learning O
in O
the O
MIMIC B-DAT

A. O
Celi, O
R. O
G. O
Mark, O
MIMIC B-DAT

Information O
Mart O
for O
Intensive O
Care O
III B-DAT
(MIMIC-III). O
The O
experimental O
results O
demonstrate O

Information O
Mart O
for O
Intensive O
Care O
III, B-DAT
MIMIC-III O
Waveform O
Database O
records O
[17 O

patients. O
Extending O
that, O
the O
APACHE O
III B-DAT
improved O
the O
effectiveness O
of O
mortality O

prediction O
problem O
of O
the O
APACHE O
III B-DAT
by O
adding O
new O
variables O
and O

the O
weights O
utilized O
in O
APACHE O
III B-DAT
[20]. O
The O
traditional O
severity O
of O

III B-DAT
dataset. O
We O
describe O
each O
signal O

III B-DAT
clinical O
dataset O
while O
subsections O
3.2 O

III B-DAT
(without O
infants) O
and O
the O
Matched O

conducted O
over O
the O
well-known O
MIMIC- O
III B-DAT
database O
comprising O
the O
records O
of O

III B-DAT
are O
associated O
with O
the O
related O

III B-DAT
(without O
in- O
fants) O
and O
the O

III B-DAT

III B-DAT
database O
is O
extracted O
from O
the O

III B-DAT
database. O
To O
avoid O
bi- O
ased O

In O
the O
MIMIC O
III B-DAT
dataset, O
the O
number O
of O
patients O

III B-DAT
Waveform O
Database O
Matched O
Subset. O
This O

III B-DAT
critical O
care O
database, O
bioRxiv O
(2017 O

III, B-DAT
a O
freely O
accessible O
critical O
care O

Mart O
for O
Intensive O
Care O
III O
(MIMIC-III B-DAT

Mart O
for O
Intensive O
Care O
III, O
MIMIC-III B-DAT
Waveform O
Database O
records O
[17]. O
We O

early O
mortality O
prediction O
using O
the O
MIMIC-III B-DAT
dataset. O
We O
describe O
each O
signal O

provides O
a O
review O
on O
the O
MIMIC-III B-DAT
clinical O
dataset O
while O
subsections O
3.2 O

age O
distribution O
over O
the O
Whole O
MIMIC-III B-DAT
(without O
infants) O
and O
the O
Matched O

the O
clinical O
data O
in O
the O
MIMIC-III B-DAT
are O
associated O
with O
the O
related O

age O
distributions O
of O
the O
whole O
MIMIC-III B-DAT
(without O
in- O
fants) O
and O
the O

Table O
1: O
Care O
Units O
in O
MIMIC-III B-DAT

MIMIC-III B-DAT
database O
is O
extracted O
from O
the O

1 O
to O
0.17 O
Hz O
in O
MIMIC-III B-DAT
database. O
To O
avoid O
bi- O
ased O

information O
recorded O
in O
from O
the O
MIMIC-III B-DAT
Waveform O
Database O
Matched O
Subset. O
This O

and O
deep O
learning O
in O
the O
MIMIC-III B-DAT
critical O
care O
database, O
bioRxiv O
(2017 O

A. O
Celi, O
R. O
G. O
Mark, O
MIMIC-III, B-DAT
a O
freely O
accessible O
critical O
care O

Mart O
for O
Intensive O
Care O
III O
(MIMIC B-DAT

Mart O
for O
Intensive O
Care O
III, O
MIMIC B-DAT

early O
mortality O
prediction O
using O
the O
MIMIC B-DAT

provides O
a O
review O
on O
the O
MIMIC B-DAT

age O
distribution O
over O
the O
Whole O
MIMIC B-DAT

is O
conducted O
over O
the O
well-known O
MIMIC B-DAT

the O
clinical O
data O
in O
the O
MIMIC B-DAT

age O
distributions O
of O
the O
whole O
MIMIC B-DAT

Table O
1: O
Care O
Units O
in O
MIMIC B-DAT

MIMIC B-DAT

1 O
to O
0.17 O
Hz O
in O
MIMIC B-DAT

In O
the O
MIMIC B-DAT
III O
dataset, O
the O
number O
of O

information O
recorded O
in O
from O
the O
MIMIC B-DAT

Learning O
Models O
on O
Large O
Healthcare O
MIMIC B-DAT
Datasets, O
arXiv O
preprint O
arXiv:1710.08531 O
(2017 O

and O
deep O
learning O
in O
the O
MIMIC B-DAT

A. O
Celi, O
R. O
G. O
Mark, O
MIMIC B-DAT

Information O
Mart O
for O
Intensive O
Care O
III B-DAT
(MIMIC-III). O
The O
experimental O
results O
demonstrate O

Information O
Mart O
for O
Intensive O
Care O
III, B-DAT
MIMIC-III O
Waveform O
Database O
records O
[17 O

patients. O
Extending O
that, O
the O
APACHE O
III B-DAT
improved O
the O
effectiveness O
of O
mortality O

prediction O
problem O
of O
the O
APACHE O
III B-DAT
by O
adding O
new O
variables O
and O

the O
weights O
utilized O
in O
APACHE O
III B-DAT
[20]. O
The O
traditional O
severity O
of O

III B-DAT
dataset. O
We O
describe O
each O
signal O

III B-DAT
clinical O
dataset O
while O
subsections O
3.2 O

III B-DAT
(without O
infants) O
and O
the O
Matched O

conducted O
over O
the O
well-known O
MIMIC- O
III B-DAT
database O
comprising O
the O
records O
of O

III B-DAT
are O
associated O
with O
the O
related O

III B-DAT
(without O
in- O
fants) O
and O
the O

III B-DAT

III B-DAT
database O
is O
extracted O
from O
the O

III B-DAT
database. O
To O
avoid O
bi- O
ased O

In O
the O
MIMIC O
III B-DAT
dataset, O
the O
number O
of O
patients O

III B-DAT
Waveform O
Database O
Matched O
Subset. O
This O

III B-DAT
critical O
care O
database, O
bioRxiv O
(2017 O

III, B-DAT
a O
freely O
accessible O
critical O
care O

Mart O
for O
Intensive O
Care O
III O
(MIMIC-III B-DAT

Mart O
for O
Intensive O
Care O
III, O
MIMIC-III B-DAT
Waveform O
Database O
records O
[17]. O
We O

early O
mortality O
prediction O
using O
the O
MIMIC-III B-DAT
dataset. O
We O
describe O
each O
signal O

provides O
a O
review O
on O
the O
MIMIC-III B-DAT
clinical O
dataset O
while O
subsections O
3.2 O

age O
distribution O
over O
the O
Whole O
MIMIC-III B-DAT
(without O
infants) O
and O
the O
Matched O

the O
clinical O
data O
in O
the O
MIMIC-III B-DAT
are O
associated O
with O
the O
related O

age O
distributions O
of O
the O
whole O
MIMIC-III B-DAT
(without O
in- O
fants) O
and O
the O

Table O
1: O
Care O
Units O
in O
MIMIC-III B-DAT

MIMIC-III B-DAT
database O
is O
extracted O
from O
the O

1 O
to O
0.17 O
Hz O
in O
MIMIC-III B-DAT
database. O
To O
avoid O
bi- O
ased O

information O
recorded O
in O
from O
the O
MIMIC-III B-DAT
Waveform O
Database O
Matched O
Subset. O
This O

and O
deep O
learning O
in O
the O
MIMIC-III B-DAT
critical O
care O
database, O
bioRxiv O
(2017 O

A. O
Celi, O
R. O
G. O
Mark, O
MIMIC-III, B-DAT
a O
freely O
accessible O
critical O
care O

Mart O
for O
Intensive O
Care O
III O
(MIMIC B-DAT

Mart O
for O
Intensive O
Care O
III, O
MIMIC B-DAT

early O
mortality O
prediction O
using O
the O
MIMIC B-DAT

provides O
a O
review O
on O
the O
MIMIC B-DAT

age O
distribution O
over O
the O
Whole O
MIMIC B-DAT

is O
conducted O
over O
the O
well-known O
MIMIC B-DAT

the O
clinical O
data O
in O
the O
MIMIC B-DAT

age O
distributions O
of O
the O
whole O
MIMIC B-DAT

Table O
1: O
Care O
Units O
in O
MIMIC B-DAT

MIMIC B-DAT

1 O
to O
0.17 O
Hz O
in O
MIMIC B-DAT

In O
the O
MIMIC B-DAT
III O
dataset, O
the O
number O
of O

information O
recorded O
in O
from O
the O
MIMIC B-DAT

Learning O
Models O
on O
Large O
Healthcare O
MIMIC B-DAT
Datasets, O
arXiv O
preprint O
arXiv:1710.08531 O
(2017 O

and O
deep O
learning O
in O
the O
MIMIC B-DAT

A. O
Celi, O
R. O
G. O
Mark, O
MIMIC B-DAT

Information O
Mart O
for O
Intensive O
Care O
III B-DAT
(MIMIC-III). O
The O
experimental O
results O
demonstrate O

Information O
Mart O
for O
Intensive O
Care O
III, B-DAT
MIMIC-III O
Waveform O
Database O
records O
[17 O

patients. O
Extending O
that, O
the O
APACHE O
III B-DAT
improved O
the O
effectiveness O
of O
mortality O

prediction O
problem O
of O
the O
APACHE O
III B-DAT
by O
adding O
new O
variables O
and O

the O
weights O
utilized O
in O
APACHE O
III B-DAT
[20]. O
The O
traditional O
severity O
of O

III B-DAT
dataset. O
We O
describe O
each O
signal O

III B-DAT
clinical O
dataset O
while O
subsections O
3.2 O

III B-DAT
(without O
infants) O
and O
the O
Matched O

conducted O
over O
the O
well-known O
MIMIC- O
III B-DAT
database O
comprising O
the O
records O
of O

III B-DAT
are O
associated O
with O
the O
related O

III B-DAT
(without O
in- O
fants) O
and O
the O

III B-DAT

III B-DAT
database O
is O
extracted O
from O
the O

III B-DAT
database. O
To O
avoid O
bi- O
ased O

In O
the O
MIMIC O
III B-DAT
dataset, O
the O
number O
of O
patients O

III B-DAT
Waveform O
Database O
Matched O
Subset. O
This O

III B-DAT
critical O
care O
database, O
bioRxiv O
(2017 O

III, B-DAT
a O
freely O
accessible O
critical O
care O

Mart O
for O
Intensive O
Care O
III O
(MIMIC-III B-DAT

Mart O
for O
Intensive O
Care O
III, O
MIMIC-III B-DAT
Waveform O
Database O
records O
[17]. O
We O

early O
mortality O
prediction O
using O
the O
MIMIC-III B-DAT
dataset. O
We O
describe O
each O
signal O

provides O
a O
review O
on O
the O
MIMIC-III B-DAT
clinical O
dataset O
while O
subsections O
3.2 O

age O
distribution O
over O
the O
Whole O
MIMIC-III B-DAT
(without O
infants) O
and O
the O
Matched O

the O
clinical O
data O
in O
the O
MIMIC-III B-DAT
are O
associated O
with O
the O
related O

age O
distributions O
of O
the O
whole O
MIMIC-III B-DAT
(without O
in- O
fants) O
and O
the O

Table O
1: O
Care O
Units O
in O
MIMIC-III B-DAT

MIMIC-III B-DAT
database O
is O
extracted O
from O
the O

1 O
to O
0.17 O
Hz O
in O
MIMIC-III B-DAT
database. O
To O
avoid O
bi- O
ased O

information O
recorded O
in O
from O
the O
MIMIC-III B-DAT
Waveform O
Database O
Matched O
Subset. O
This O

and O
deep O
learning O
in O
the O
MIMIC-III B-DAT
critical O
care O
database, O
bioRxiv O
(2017 O

A. O
Celi, O
R. O
G. O
Mark, O
MIMIC-III, B-DAT
a O
freely O
accessible O
critical O
care O

Mart O
for O
Intensive O
Care O
III O
(MIMIC B-DAT

Mart O
for O
Intensive O
Care O
III, O
MIMIC B-DAT

early O
mortality O
prediction O
using O
the O
MIMIC B-DAT

provides O
a O
review O
on O
the O
MIMIC B-DAT

age O
distribution O
over O
the O
Whole O
MIMIC B-DAT

is O
conducted O
over O
the O
well-known O
MIMIC B-DAT

the O
clinical O
data O
in O
the O
MIMIC B-DAT

age O
distributions O
of O
the O
whole O
MIMIC B-DAT

Table O
1: O
Care O
Units O
in O
MIMIC B-DAT

MIMIC B-DAT

1 O
to O
0.17 O
Hz O
in O
MIMIC B-DAT

In O
the O
MIMIC B-DAT
III O
dataset, O
the O
number O
of O

information O
recorded O
in O
from O
the O
MIMIC B-DAT

Learning O
Models O
on O
Large O
Healthcare O
MIMIC B-DAT
Datasets, O
arXiv O
preprint O
arXiv:1710.08531 O
(2017 O

and O
deep O
learning O
in O
the O
MIMIC B-DAT

A. O
Celi, O
R. O
G. O
Mark, O
MIMIC B-DAT

Information O
Mart O
for O
Intensive O
Care O
III B-DAT
(MIMIC-III). O
The O
experimental O
results O
demonstrate O

Information O
Mart O
for O
Intensive O
Care O
III, B-DAT
MIMIC-III O
Waveform O
Database O
records O
[17 O

patients. O
Extending O
that, O
the O
APACHE O
III B-DAT
improved O
the O
effectiveness O
of O
mortality O

prediction O
problem O
of O
the O
APACHE O
III B-DAT
by O
adding O
new O
variables O
and O

the O
weights O
utilized O
in O
APACHE O
III B-DAT
[20]. O
The O
traditional O
severity O
of O

III B-DAT
dataset. O
We O
describe O
each O
signal O

III B-DAT
clinical O
dataset O
while O
subsections O
3.2 O

III B-DAT
(without O
infants) O
and O
the O
Matched O

conducted O
over O
the O
well-known O
MIMIC- O
III B-DAT
database O
comprising O
the O
records O
of O

III B-DAT
are O
associated O
with O
the O
related O

III B-DAT
(without O
in- O
fants) O
and O
the O

III B-DAT

III B-DAT
database O
is O
extracted O
from O
the O

III B-DAT
database. O
To O
avoid O
bi- O
ased O

In O
the O
MIMIC O
III B-DAT
dataset, O
the O
number O
of O
patients O

III B-DAT
Waveform O
Database O
Matched O
Subset. O
This O

III B-DAT
critical O
care O
database, O
bioRxiv O
(2017 O

III, B-DAT
a O
freely O
accessible O
critical O
care O

Mart O
for O
Intensive O
Care O
III O
(MIMIC-III B-DAT

Mart O
for O
Intensive O
Care O
III, O
MIMIC-III B-DAT
Waveform O
Database O
records O
[17]. O
We O

early O
mortality O
prediction O
using O
the O
MIMIC-III B-DAT
dataset. O
We O
describe O
each O
signal O

provides O
a O
review O
on O
the O
MIMIC-III B-DAT
clinical O
dataset O
while O
subsections O
3.2 O

age O
distribution O
over O
the O
Whole O
MIMIC-III B-DAT
(without O
infants) O
and O
the O
Matched O

the O
clinical O
data O
in O
the O
MIMIC-III B-DAT
are O
associated O
with O
the O
related O

age O
distributions O
of O
the O
whole O
MIMIC-III B-DAT
(without O
in- O
fants) O
and O
the O

Table O
1: O
Care O
Units O
in O
MIMIC-III B-DAT

MIMIC-III B-DAT
database O
is O
extracted O
from O
the O

1 O
to O
0.17 O
Hz O
in O
MIMIC-III B-DAT
database. O
To O
avoid O
bi- O
ased O

information O
recorded O
in O
from O
the O
MIMIC-III B-DAT
Waveform O
Database O
Matched O
Subset. O
This O

and O
deep O
learning O
in O
the O
MIMIC-III B-DAT
critical O
care O
database, O
bioRxiv O
(2017 O

A. O
Celi, O
R. O
G. O
Mark, O
MIMIC-III, B-DAT
a O
freely O
accessible O
critical O
care O

Mart O
for O
Intensive O
Care O
III O
(MIMIC B-DAT

Mart O
for O
Intensive O
Care O
III, O
MIMIC B-DAT

early O
mortality O
prediction O
using O
the O
MIMIC B-DAT

provides O
a O
review O
on O
the O
MIMIC B-DAT

age O
distribution O
over O
the O
Whole O
MIMIC B-DAT

is O
conducted O
over O
the O
well-known O
MIMIC B-DAT

the O
clinical O
data O
in O
the O
MIMIC B-DAT

age O
distributions O
of O
the O
whole O
MIMIC B-DAT

Table O
1: O
Care O
Units O
in O
MIMIC B-DAT

MIMIC B-DAT

1 O
to O
0.17 O
Hz O
in O
MIMIC B-DAT

In O
the O
MIMIC B-DAT
III O
dataset, O
the O
number O
of O

information O
recorded O
in O
from O
the O
MIMIC B-DAT

Learning O
Models O
on O
Large O
Healthcare O
MIMIC B-DAT
Datasets, O
arXiv O
preprint O
arXiv:1710.08531 O
(2017 O

and O
deep O
learning O
in O
the O
MIMIC B-DAT

A. O
Celi, O
R. O
G. O
Mark, O
MIMIC B-DAT

Information O
Mart O
for O
Intensive O
Care O
III B-DAT
(MIMIC-III). O
The O
experimental O
results O
demonstrate O

Information O
Mart O
for O
Intensive O
Care O
III, B-DAT
MIMIC-III O
Waveform O
Database O
records O
[17 O

patients. O
Extending O
that, O
the O
APACHE O
III B-DAT
improved O
the O
effectiveness O
of O
mortality O

prediction O
problem O
of O
the O
APACHE O
III B-DAT
by O
adding O
new O
variables O
and O

the O
weights O
utilized O
in O
APACHE O
III B-DAT
[20]. O
The O
traditional O
severity O
of O

III B-DAT
dataset. O
We O
describe O
each O
signal O

III B-DAT
clinical O
dataset O
while O
subsections O
3.2 O

III B-DAT
(without O
infants) O
and O
the O
Matched O

conducted O
over O
the O
well-known O
MIMIC- O
III B-DAT
database O
comprising O
the O
records O
of O

III B-DAT
are O
associated O
with O
the O
related O

III B-DAT
(without O
in- O
fants) O
and O
the O

III B-DAT

III B-DAT
database O
is O
extracted O
from O
the O

III B-DAT
database. O
To O
avoid O
bi- O
ased O

In O
the O
MIMIC O
III B-DAT
dataset, O
the O
number O
of O
patients O

III B-DAT
Waveform O
Database O
Matched O
Subset. O
This O

III B-DAT
critical O
care O
database, O
bioRxiv O
(2017 O

III, B-DAT
a O
freely O
accessible O
critical O
care O

Mart O
for O
Intensive O
Care O
III O
(MIMIC-III B-DAT

Mart O
for O
Intensive O
Care O
III, O
MIMIC-III B-DAT
Waveform O
Database O
records O
[17]. O
We O

early O
mortality O
prediction O
using O
the O
MIMIC-III B-DAT
dataset. O
We O
describe O
each O
signal O

provides O
a O
review O
on O
the O
MIMIC-III B-DAT
clinical O
dataset O
while O
subsections O
3.2 O

age O
distribution O
over O
the O
Whole O
MIMIC-III B-DAT
(without O
infants) O
and O
the O
Matched O

the O
clinical O
data O
in O
the O
MIMIC-III B-DAT
are O
associated O
with O
the O
related O

age O
distributions O
of O
the O
whole O
MIMIC-III B-DAT
(without O
in- O
fants) O
and O
the O

Table O
1: O
Care O
Units O
in O
MIMIC-III B-DAT

MIMIC-III B-DAT
database O
is O
extracted O
from O
the O

1 O
to O
0.17 O
Hz O
in O
MIMIC-III B-DAT
database. O
To O
avoid O
bi- O
ased O

information O
recorded O
in O
from O
the O
MIMIC-III B-DAT
Waveform O
Database O
Matched O
Subset. O
This O

and O
deep O
learning O
in O
the O
MIMIC-III B-DAT
critical O
care O
database, O
bioRxiv O
(2017 O

A. O
Celi, O
R. O
G. O
Mark, O
MIMIC-III, B-DAT
a O
freely O
accessible O
critical O
care O

Mart O
for O
Intensive O
Care O
III O
(MIMIC B-DAT

Mart O
for O
Intensive O
Care O
III, O
MIMIC B-DAT

early O
mortality O
prediction O
using O
the O
MIMIC B-DAT

provides O
a O
review O
on O
the O
MIMIC B-DAT

age O
distribution O
over O
the O
Whole O
MIMIC B-DAT

is O
conducted O
over O
the O
well-known O
MIMIC B-DAT

the O
clinical O
data O
in O
the O
MIMIC B-DAT

age O
distributions O
of O
the O
whole O
MIMIC B-DAT

Table O
1: O
Care O
Units O
in O
MIMIC B-DAT

MIMIC B-DAT

1 O
to O
0.17 O
Hz O
in O
MIMIC B-DAT

In O
the O
MIMIC B-DAT
III O
dataset, O
the O
number O
of O

information O
recorded O
in O
from O
the O
MIMIC B-DAT

Learning O
Models O
on O
Large O
Healthcare O
MIMIC B-DAT
Datasets, O
arXiv O
preprint O
arXiv:1710.08531 O
(2017 O

and O
deep O
learning O
in O
the O
MIMIC B-DAT

A. O
Celi, O
R. O
G. O
Mark, O
MIMIC B-DAT

Information O
Mart O
for O
Intensive O
Care O
III B-DAT
(MIMIC-III). O
The O
experimental O
results O
demonstrate O

Information O
Mart O
for O
Intensive O
Care O
III, B-DAT
MIMIC-III O
Waveform O
Database O
records O
[17 O

patients. O
Extending O
that, O
the O
APACHE O
III B-DAT
improved O
the O
effectiveness O
of O
mortality O

prediction O
problem O
of O
the O
APACHE O
III B-DAT
by O
adding O
new O
variables O
and O

the O
weights O
utilized O
in O
APACHE O
III B-DAT
[20]. O
The O
traditional O
severity O
of O

III B-DAT
dataset. O
We O
describe O
each O
signal O

III B-DAT
clinical O
dataset O
while O
subsections O
3.2 O

III B-DAT
(without O
infants) O
and O
the O
Matched O

conducted O
over O
the O
well-known O
MIMIC- O
III B-DAT
database O
comprising O
the O
records O
of O

III B-DAT
are O
associated O
with O
the O
related O

III B-DAT
(without O
in- O
fants) O
and O
the O

III B-DAT

III B-DAT
database O
is O
extracted O
from O
the O

III B-DAT
database. O
To O
avoid O
bi- O
ased O

In O
the O
MIMIC O
III B-DAT
dataset, O
the O
number O
of O
patients O

III B-DAT
Waveform O
Database O
Matched O
Subset. O
This O

III B-DAT
critical O
care O
database, O
bioRxiv O
(2017 O

III, B-DAT
a O
freely O
accessible O
critical O
care O

Probabilistic O
framework O
for O
solving O
Visual O
Dialog B-DAT

Vision O
and O
Language, O
Visual O
Dialog, B-DAT
VQA, O
Answer O
Generation, O
Question O
Generation O

Dialog B-DAT

tainty O
while O
solving O
the O
‘Visual O
Dialog B-DAT

by O
conducting O
experiments O
on O
Visual O
Dialog B-DAT

Grad-CAM O
for O
a O
particular O
Dialog B-DAT

v1.0 B-DAT
in O
test-std O
dataset O

version O
of O
the O
dataset. O
“VisDial O
v1.0 B-DAT

v1.0 B-DAT

dialog O
dataset O
v0.9 O
and O
v1.0 B-DAT
are O
provided O
in O
table O
4 O

Table O
6: O
Results O
on O
dataset O
v1.0 B-DAT
for O
Visual O
dialog O

Probabilistic O
framework O
for O
solving O
Visual B-DAT
Dialog O

Visual B-DAT
Di O

Vision O
and O
Language, O
Visual B-DAT
Dialog, O
VQA, O
Answer O
Generation, O
Question O

Visual B-DAT

Visual B-DAT
Dialog’ O
task, O
as O
shown O
in O

Visual B-DAT
dialog’ O
has O
been O
introduced O
recently O

Visual B-DAT
dialog O
task O
requires O
the O
agents O

Visual B-DAT
dialog O
task O
is O
introduced O
by O

aleatoric O
and O
predictive O
uncertainty O
for O
Visual B-DAT
Question O
An O

approach O
by O
conducting O
experiments O
on O
Visual B-DAT
Dialog O

4: O
Results O
on O
dataset-v0.9 O
for O
Visual B-DAT
dialog O

5: O
Answer O
Diversity O
results O
for O
Visual B-DAT
dialog O

Results O
on O
dataset O
v1.0 O
for O
Visual B-DAT
dialog O

VQA: O
Visual B-DAT
Question O
Answering, O
in: O
International O
Conference O

Visual B-DAT
dialog, O
in: O
Proceedings O
of O
the O

Patel, O
V. O
P. O
Namboodiri, O
U-cam: O
Visual B-DAT
ex O

Two O
can O
play O
this O
game: O
Visual B-DAT
dialog O
with O

cam: O
Visual B-DAT
explanations O
from O
deep O
networks O
via O

Lehrmann, O
B. O
Han, O
L. O
Sigal, O
Visual B-DAT
reference O
resolution O
using O
at O

Parikh, O
D. O
Batra, O
M. O
Rohrbach, O
Visual B-DAT
coreference O

Probabilistic O
framework O
for O
solving O
Visual O
Dialog B-DAT

Vision O
and O
Language, O
Visual O
Dialog, B-DAT
VQA, O
Answer O
Generation, O
Question O
Generation O

Dialog B-DAT

tainty O
while O
solving O
the O
‘Visual O
Dialog B-DAT

by O
conducting O
experiments O
on O
Visual O
Dialog B-DAT

Grad-CAM O
for O
a O
particular O
Dialog B-DAT

v0.9 B-DAT
for O
Visual O
dialog O

dialog O
dataset O
v0.9 B-DAT
and O
v1.0 O
are O
provided O
in O

Probabilistic O
framework O
for O
solving O
Visual B-DAT
Dialog O

Visual B-DAT
Di O

Vision O
and O
Language, O
Visual B-DAT
Dialog, O
VQA, O
Answer O
Generation, O
Question O

Visual B-DAT

Visual B-DAT
Dialog’ O
task, O
as O
shown O
in O

Visual B-DAT
dialog’ O
has O
been O
introduced O
recently O

Visual B-DAT
dialog O
task O
requires O
the O
agents O

Visual B-DAT
dialog O
task O
is O
introduced O
by O

aleatoric O
and O
predictive O
uncertainty O
for O
Visual B-DAT
Question O
An O

approach O
by O
conducting O
experiments O
on O
Visual B-DAT
Dialog O

4: O
Results O
on O
dataset-v0.9 O
for O
Visual B-DAT
dialog O

5: O
Answer O
Diversity O
results O
for O
Visual B-DAT
dialog O

Results O
on O
dataset O
v1.0 O
for O
Visual B-DAT
dialog O

VQA: O
Visual B-DAT
Question O
Answering, O
in: O
International O
Conference O

Visual B-DAT
dialog, O
in: O
Proceedings O
of O
the O

Patel, O
V. O
P. O
Namboodiri, O
U-cam: O
Visual B-DAT
ex O

Two O
can O
play O
this O
game: O
Visual B-DAT
dialog O
with O

cam: O
Visual B-DAT
explanations O
from O
deep O
networks O
via O

Lehrmann, O
B. O
Han, O
L. O
Sigal, O
Visual B-DAT
reference O
resolution O
using O
at O

Parikh, O
D. O
Batra, O
M. O
Rohrbach, O
Visual B-DAT
coreference O

both O
constrained O
and O
unconstrained) O
for O
English B-DAT

English B-DAT
translation O
directions. O
The O
submitted O
systems O

models O
with O
Transformer O
models O
for O
English B-DAT

English B-DAT
and O
we O
show O
that O
the O

Constrained O
English-Estonian B-DAT
and O
Estonian- O
English O
NMT O
systems O
(tilde-c-nmt) O
that O
were O

Unconstrained O
English B-DAT

English B-DAT
NMT O
systems O
(tilde O

English B-DAT
NMT O
system O
(tilde-c-nmt-comb) O
that O
is O

Constrained O
English-Estonian B-DAT
and O
Estonian- O
English O
NMT O
systems O
(tilde-c-nmt-2bt) O
av- O
eraged O

English B-DAT
ParaCrawl O
corpus3. O
The O
corpus O
consisted O

Manning O
et O
al., O
2014) O
for O
English B-DAT

lemma, O
and O
morpho-syntactic O
tag. O
The O
English B-DAT
data O
consist O
of O
the O
following O

English B-DAT

English B-DAT

English B-DAT
system, O
we O
performed O
majority O
voting O

on O
the O
validation O
set) O
for O
English B-DAT

English B-DAT
(right). O
Note O
that O
batch O
size O

entries O
for O
the O
constrained O
(unconstrained) O
English B-DAT

English B-DAT
NMT O
systems O
respectively O

Figure O
2 O
shows O
that O
the O
English B-DAT

on O
the O
validation O
set) O
for O
English B-DAT

English B-DAT
(right O

English B-DAT
translation O
direction O
in O
the O
constrained O

not O
perform O
the O
combination O
for O
English B-DAT

System O
Configuration O
BLEU O
English B-DAT

English B-DAT
MLSTM O
(final O

System O
BLEU O
DA O
Cluster O
English B-DAT

English B-DAT

submitted O
by O
Tilde O
for O
the O
English B-DAT

Tilde's O
Machine O
Translation O
Systems O
for O
WMT B-DAT
2018 O

Third O
Conference O
on O
Machine O
Translation O
(WMT), B-DAT
Volume O
2: O
Shared O
Task O
Papers O

Tilde’s O
Machine O
Translation O
Systems O
for O
WMT B-DAT
2018 O

were O
submit- O
ted O
for O
the O
WMT B-DAT
2018 O
shared O
task O
on O
news O

evalua- O
tion O
results. O
For O
the O
WMT B-DAT
2018 O
shared O
task, O
we O
submitted O

Third O
Conference O
on O
Machine O
Translation O
(WMT) B-DAT
were O
trained O
us- O
ing O
Transformer O

even O
further O
in O
2018 O
(beyond O
WMT B-DAT
2018). O
For O
instance, O
Chen O
et O

In O
WMT B-DAT
2017, O
Tilde O
participated O
with O
MLSTM-based O

show O
that O
the O
state-of-the-art O
of O
WMT B-DAT
2017 O
is O
well O
behind O
the O

new O
models. O
Therefore, O
for O
WMT B-DAT
2018, O
Tilde O
submitted O
NMT O
systems O

systems O
submit- O
ted O
for O
the O
WMT B-DAT
2018 O
shared O
task O
on O
news O

For O
the O
WMT B-DAT
2018 O
shared O
task O
on O
news O

only O
data O
provided O
by O
the O
WMT B-DAT
2018 O
organisers O
were O
used, O
however O

Contrary O
to O
Tilde’s O
submissions O
for O
WMT B-DAT
2017, O
isolated O
sentence O
pair O
filtering O

for O
the O
WMT B-DAT
2018 O
submissions O
was O
supplemented O
with O

Ta- O
ble O
5) O
from O
the O
WMT B-DAT
2018 O
shared O
task O
on O
news O

that O
were O
submitted O
for O
the O
WMT B-DAT
2018 O
shared O
task O
on O
news O

the O
official O
results O
of O
the O
WMT B-DAT
2018 O
shared O
task O
on O
news O

2018 O
conference O
on O
machine O
translation O
(WMT18 B-DAT

Corpus O
Filter- O
ing O
Methods O
for O
WMT B-DAT
2018. O
In O
Proceedings O
of O
the O

Third O
Conference O
on O
Machine O
Translation O
(WMT B-DAT
2018), O
Volume O
2: O
Shared O
Task O

Tilde’s O
Machine O
Translation O
Systems O
for O
WMT B-DAT
2017. O
In O
Proceed- O
ings O
of O

Conference O
on O
Machine O
Trans- O
lation O
(WMT B-DAT
2017), O
Volume O
2: O
Shared O
Task O

C-3MA: O
Tartu-Riga-Zurich O
Translation O
Systems O
for O
WMT17 B-DAT

First O
Conference O
on O
Machine O
Translation O
(WMT B-DAT
2016) O
- O
Volume O
1: O
Research O

Machine O
Translation O
Systems O
for O
WMT O
2018 B-DAT

October O
31 O
- O
Novermber O
1, O
2018 B-DAT

2018 B-DAT
Association O
for O
Computational O
Linguistics O

Machine O
Translation O
Systems O
for O
WMT O
2018 B-DAT

submit- O
ted O
for O
the O
WMT O
2018 B-DAT
shared O
task O
on O
news O
translation O

tion O
results. O
For O
the O
WMT O
2018 B-DAT
shared O
task, O
we O
submitted O
seven O

et O
al., O
2017). O
Consequently, O
in O
2018, B-DAT
most O
of O
the O
top O
scoring O

be O
pushed O
even O
further O
in O
2018 B-DAT
(beyond O
WMT O
2018). O
For O
instance O

, O
Chen O
et O
al. O
(2018) B-DAT
have O
recently O
proposed O
RNMT+ O
models O

new O
models. O
Therefore, O
for O
WMT O
2018, B-DAT
Tilde O
submitted O
NMT O
systems O
that O

submit- O
ted O
for O
the O
WMT O
2018 B-DAT
shared O
task O
on O
news O
trans O

For O
the O
WMT O
2018 B-DAT
shared O
task O
on O
news O
transla O

data O
provided O
by O
the O
WMT O
2018 B-DAT
organisers O
were O
used, O
however, O
for O

filtering O
methods O
described O
by O
Pinnis O
(2018 B-DAT

pair O
filtering O
for O
the O
WMT O
2018 B-DAT
submissions O
was O
supplemented O
with O
a O

MT O
platform O
(Pinnis O
et O
al., O
2018) B-DAT
that O
performs O
the O
fol- O
lowing O

in O
2017 O
(Pinnis O
and O
Kalniņš, O
2018 B-DAT

pre-processing O
methods O
described O
by O
Rikters O
(2018 B-DAT

the O
SacreBLEU O
evaluation O
tool O
(Post, O
2018 B-DAT

ble O
5) O
from O
the O
WMT O
2018 B-DAT
shared O
task O
on O
news O
translation O

Bojar O
et O
al., O
2018) B-DAT
show O
that O

were O
submitted O
for O
the O
WMT O
2018 B-DAT
shared O
task O
on O
news O
translation O

official O
results O
of O
the O
WMT O
2018 B-DAT
shared O
task O
on O
news O
trans O

Philipp O
Koehn, O
and O
Christof O
Monz. O
2018 B-DAT

. O
Find- O
ings O
of O
the O
2018 B-DAT
conference O
on O
machine O
translation O
(WMT18 O

Yonghui O
Wu, O
and O
Macduff O
Hughes. O
2018 B-DAT

Mārcis O
Pinnis. O
2018 B-DAT

Filter- O
ing O
Methods O
for O
WMT O
2018 B-DAT

Conference O
on O
Machine O
Translation O
(WMT O
2018), B-DAT
Volume O
2: O
Shared O
Task O
Papers O

Mārcis O
Pinnis O
and O
Rihards O
Kalniņš. O
2018 B-DAT

Translation O
Service O
for O
the O
2017- O
2018 B-DAT
European O
Union O
Presidency. O
In O
Proceedings O

Translation O
in O
the O
Americas O
(AMTA O
2018), B-DAT
vol. O
2: O
MT O
Users, O
pages O

Raivis O
Skadiņš, O
and O
Valters O
Šics. O
2018 B-DAT

Re- O
sources O
and O
Evaluation O
(LREC O
2018), B-DAT
Miyazaki, O
Japan. O
European O
Language O
Resources O

Matt O
Post. O
2018 B-DAT

Matı̄ss O
Rikters. O
2018 B-DAT

The O
Baltic O
Perspective O
(Baltic O
HLT O
2018), B-DAT
Tartu, O
Estonia O

and O
unconstrained) O
for O
English-Estonian O
and O
Estonian-English B-DAT
translation O
directions. O
The O
submitted O
systems O

Transformer O
models O
for O
English-Estonian O
and O
Estonian-English B-DAT
and O
we O
show O
that O
the O

Unconstrained O
English-Estonian O
and O
Estonian-English B-DAT
NMT O
systems O
(tilde O

A O
constrained O
Estonian-English B-DAT
NMT O
system O
(tilde-c-nmt-comb) O
that O
is O

720 O
(U) O
transf-u O
50k O
512 O
Estonian-English B-DAT

For O
the O
tilde-c-nmt-comb O
Estonian-English B-DAT
system, O
we O
performed O
majority O
voting O

set) O
for O
English-Estonian O
(left) O
and O
Estonian-English B-DAT
(right). O
Note O
that O
batch O
size O

the O
constrained O
(unconstrained) O
English-Estonian O
and O
Estonian-English B-DAT
NMT O
systems O
respectively O

set) O
for O
English-Estonian O
(left) O
and O
Estonian-English B-DAT
(right O

our O
best-scoring O
outputs O
in O
the O
Estonian-English B-DAT
translation O
direction O
in O
the O
constrained O

Estonian-English B-DAT
MLSTM O
(final O

Estonian-English B-DAT

Tilde's O
Machine O
Translation O
Systems O
for O
WMT B-DAT
2018 O

Third O
Conference O
on O
Machine O
Translation O
(WMT), B-DAT
Volume O
2: O
Shared O
Task O
Papers O

Tilde’s O
Machine O
Translation O
Systems O
for O
WMT B-DAT
2018 O

were O
submit- O
ted O
for O
the O
WMT B-DAT
2018 O
shared O
task O
on O
news O

evalua- O
tion O
results. O
For O
the O
WMT B-DAT
2018 O
shared O
task, O
we O
submitted O

Third O
Conference O
on O
Machine O
Translation O
(WMT) B-DAT
were O
trained O
us- O
ing O
Transformer O

even O
further O
in O
2018 O
(beyond O
WMT B-DAT
2018). O
For O
instance, O
Chen O
et O

In O
WMT B-DAT
2017, O
Tilde O
participated O
with O
MLSTM-based O

show O
that O
the O
state-of-the-art O
of O
WMT B-DAT
2017 O
is O
well O
behind O
the O

new O
models. O
Therefore, O
for O
WMT B-DAT
2018, O
Tilde O
submitted O
NMT O
systems O

systems O
submit- O
ted O
for O
the O
WMT B-DAT
2018 O
shared O
task O
on O
news O

For O
the O
WMT B-DAT
2018 O
shared O
task O
on O
news O

only O
data O
provided O
by O
the O
WMT B-DAT
2018 O
organisers O
were O
used, O
however O

Contrary O
to O
Tilde’s O
submissions O
for O
WMT B-DAT
2017, O
isolated O
sentence O
pair O
filtering O

for O
the O
WMT B-DAT
2018 O
submissions O
was O
supplemented O
with O

Ta- O
ble O
5) O
from O
the O
WMT B-DAT
2018 O
shared O
task O
on O
news O

that O
were O
submitted O
for O
the O
WMT B-DAT
2018 O
shared O
task O
on O
news O

the O
official O
results O
of O
the O
WMT B-DAT
2018 O
shared O
task O
on O
news O

2018 O
conference O
on O
machine O
translation O
(WMT18 B-DAT

Corpus O
Filter- O
ing O
Methods O
for O
WMT B-DAT
2018. O
In O
Proceedings O
of O
the O

Third O
Conference O
on O
Machine O
Translation O
(WMT B-DAT
2018), O
Volume O
2: O
Shared O
Task O

Tilde’s O
Machine O
Translation O
Systems O
for O
WMT B-DAT
2017. O
In O
Proceed- O
ings O
of O

Conference O
on O
Machine O
Trans- O
lation O
(WMT B-DAT
2017), O
Volume O
2: O
Shared O
Task O

C-3MA: O
Tartu-Riga-Zurich O
Translation O
Systems O
for O
WMT17 B-DAT

First O
Conference O
on O
Machine O
Translation O
(WMT B-DAT
2016) O
- O
Volume O
1: O
Research O

both O
constrained O
and O
unconstrained) O
for O
English-Estonian B-DAT
and O
Estonian-English O
translation O
directions. O
The O

models O
with O
Transformer O
models O
for O
English-Estonian B-DAT
and O
Estonian-English O
and O
we O
show O

Constrained O
English-Estonian B-DAT
and O
Estonian- O
English O
NMT O
systems O

Unconstrained O
English-Estonian B-DAT
and O
Estonian-English O
NMT O
systems O
(tilde O

Constrained O
English-Estonian B-DAT
and O
Estonian- O
English O
NMT O
systems O

English-Estonian B-DAT

on O
the O
validation O
set) O
for O
English-Estonian B-DAT
(left) O
and O
Estonian-English O
(right). O
Note O

entries O
for O
the O
constrained O
(unconstrained) O
English-Estonian B-DAT
and O
Estonian-English O
NMT O
systems O
respectively O

Figure O
2 O
shows O
that O
the O
English-Estonian B-DAT
system O
benefits O
from O
the O
additional O

on O
the O
validation O
set) O
for O
English-Estonian B-DAT
(left) O
and O
Estonian-English O
(right O

System O
Configuration O
BLEU O
English-Estonian B-DAT
MLSTM O
(final O

System O
BLEU O
DA O
Cluster O
English-Estonian B-DAT

Machine O
Translation O
Systems O
for O
WMT O
2018 B-DAT

October O
31 O
- O
Novermber O
1, O
2018 B-DAT

2018 B-DAT
Association O
for O
Computational O
Linguistics O

Machine O
Translation O
Systems O
for O
WMT O
2018 B-DAT

submit- O
ted O
for O
the O
WMT O
2018 B-DAT
shared O
task O
on O
news O
translation O

tion O
results. O
For O
the O
WMT O
2018 B-DAT
shared O
task, O
we O
submitted O
seven O

et O
al., O
2017). O
Consequently, O
in O
2018, B-DAT
most O
of O
the O
top O
scoring O

be O
pushed O
even O
further O
in O
2018 B-DAT
(beyond O
WMT O
2018). O
For O
instance O

, O
Chen O
et O
al. O
(2018) B-DAT
have O
recently O
proposed O
RNMT+ O
models O

new O
models. O
Therefore, O
for O
WMT O
2018, B-DAT
Tilde O
submitted O
NMT O
systems O
that O

submit- O
ted O
for O
the O
WMT O
2018 B-DAT
shared O
task O
on O
news O
trans O

For O
the O
WMT O
2018 B-DAT
shared O
task O
on O
news O
transla O

data O
provided O
by O
the O
WMT O
2018 B-DAT
organisers O
were O
used, O
however, O
for O

filtering O
methods O
described O
by O
Pinnis O
(2018 B-DAT

pair O
filtering O
for O
the O
WMT O
2018 B-DAT
submissions O
was O
supplemented O
with O
a O

MT O
platform O
(Pinnis O
et O
al., O
2018) B-DAT
that O
performs O
the O
fol- O
lowing O

in O
2017 O
(Pinnis O
and O
Kalniņš, O
2018 B-DAT

pre-processing O
methods O
described O
by O
Rikters O
(2018 B-DAT

the O
SacreBLEU O
evaluation O
tool O
(Post, O
2018 B-DAT

ble O
5) O
from O
the O
WMT O
2018 B-DAT
shared O
task O
on O
news O
translation O

Bojar O
et O
al., O
2018) B-DAT
show O
that O

were O
submitted O
for O
the O
WMT O
2018 B-DAT
shared O
task O
on O
news O
translation O

official O
results O
of O
the O
WMT O
2018 B-DAT
shared O
task O
on O
news O
trans O

Philipp O
Koehn, O
and O
Christof O
Monz. O
2018 B-DAT

. O
Find- O
ings O
of O
the O
2018 B-DAT
conference O
on O
machine O
translation O
(WMT18 O

Yonghui O
Wu, O
and O
Macduff O
Hughes. O
2018 B-DAT

Mārcis O
Pinnis. O
2018 B-DAT

Filter- O
ing O
Methods O
for O
WMT O
2018 B-DAT

Conference O
on O
Machine O
Translation O
(WMT O
2018), B-DAT
Volume O
2: O
Shared O
Task O
Papers O

Mārcis O
Pinnis O
and O
Rihards O
Kalniņš. O
2018 B-DAT

Translation O
Service O
for O
the O
2017- O
2018 B-DAT
European O
Union O
Presidency. O
In O
Proceedings O

Translation O
in O
the O
Americas O
(AMTA O
2018), B-DAT
vol. O
2: O
MT O
Users, O
pages O

Raivis O
Skadiņš, O
and O
Valters O
Šics. O
2018 B-DAT

Re- O
sources O
and O
Evaluation O
(LREC O
2018), B-DAT
Miyazaki, O
Japan. O
European O
Language O
Resources O

Matt O
Post. O
2018 B-DAT

Matı̄ss O
Rikters. O
2018 B-DAT

The O
Baltic O
Perspective O
(Baltic O
HLT O
2018), B-DAT
Tartu, O
Estonia O

and O
unconstrained) O
for O
English-Estonian B-DAT
and O
Estonian O

Transformer O
models O
for O
English-Estonian B-DAT
and O
Estonian O

Constrained O
English-Estonian B-DAT
and O
Estonian O

Unconstrained O
English-Estonian B-DAT
and O
Estonian O

A O
constrained O
Estonian B-DAT

Constrained O
English-Estonian B-DAT
and O
Estonian O

data O
corruption. O
It O
was O
the O
Estonian B-DAT

morpho-syntactic O
tagger O
(Nikiforovs, O
2014) O
for O
Estonian B-DAT
and O
the O
lexicalized O
probabilis- O
tic O

O). O
As O
a O
result, O
the O
Estonian B-DAT
data O
consist O
of O
the O
the O

developed O
by O
Tilde O
for O
the O
Estonian B-DAT
EU O
Council O
Presidency O
in O
2017 O

Estonian B-DAT

720 O
(U) O
transf-u O
50k O
512 O
Estonian B-DAT

For O
the O
tilde-c-nmt-comb O
Estonian B-DAT

set) O
for O
English-Estonian B-DAT
(left) O
and O
Estonian O

the O
constrained O
(unconstrained) O
English-Estonian B-DAT
and O
Estonian O

Estonian B-DAT
system O
benefits O
from O
the O
additional O

set) O
for O
English-Estonian B-DAT
(left) O
and O
Estonian O

our O
best-scoring O
outputs O
in O
the O
Estonian B-DAT

perform O
the O
combination O
for O
English- O
Estonian B-DAT
due O
to O
lack O
of O
support O

for O
alignment O
ex- O
traction O
for O
Estonian B-DAT
in O
Meteor O
(Denkowski O
and O
Lavie O

Estonian B-DAT
MLSTM O
(final O

Estonian B-DAT

Estonian B-DAT

Estonian B-DAT

Estonian B-DAT
language O
pair O

256 O
x O
256. O
For O
the O
anime B-DAT
dataset, O
we O
have O
firstly O
retrieved O

are O
extracted O
by O
using O
an O
anime B-DAT

lected O
two O
datasets O
of O
female O
anime B-DAT
face O
images, O
with O
the O
sizes O

anime B-DAT

anime B-DAT
face O
images O
are O
resized O
to O

selfie2anime B-DAT
The O
selfie O
dataset O
contains O
46,836 O
selfie O
images O

the O
same O
numbers O
as O
the O
selfie B-DAT
dataset. O
Finally, O
all O

to B-DAT

novel O
method O
for O
unsupervised O
image- O
to B-DAT

to B-DAT

attention O
module O
guides O
our O
model O
to B-DAT
focus O
on O
more O
important O
regions O

function O
helps O
our O
attention-guided O
model O
to B-DAT
flexibly O
control O
the O
amount O
of O

of O
the O
proposed O
method O
compared O
to B-DAT
the O
existing O
state-of-the-art O
models O
with O

1. O
Introduction O
Image-to-image B-DAT
translation O
aims O
to O
learn O
a O
function O
that O

works O
have O
been O
further O
developed O
to B-DAT
handle O
the O
multi-modality O
of O
the O

and O
alignment O
are O
often O
required O
to B-DAT
avoid O
these O
problems O
by O
limiting O

structure O
or O
hyper-parameter O
setting O
needs O
to B-DAT
be O
ad- O
justed O
for O
the O

to B-DAT

to B-DAT

29, O
24] O
do O
not O
allow O
to B-DAT
transform O
the O
shape O
of O
the O

because O
of O
attaching O
the O
background O
to B-DAT
the O
(translated) O
cropped O
objects. O
Unlike O

Our O
model O
guides O
the O
translation O
to B-DAT
focus O
on O
more O
important O
regions O

into B-DAT
the O
generator O
and O
discriminator O
to O
focus O
on O
semantically O
impor- O
tant O

In O
addition O
to B-DAT
the O
attentional O
mechanism, O
we O
have O

function O
helps O
our O
attention-guided O
model O
to B-DAT
flexibly O
control O
the O
amount O
of O

of O
the O
proposed O
method O
compared O
to B-DAT
the O
existing O
state-of-the-art O
models O
on O

novel O
method O
for O
unsupervised O
image- O
to B-DAT

attention O
module O
helps O
the O
model O
to B-DAT
know O
where O
to O
transform O
intensively O

function O
helps O
our O
attention-guided O
model O
to B-DAT
flexibly O
control O
the O
amount O
of O

training, O
a O
genera- O
tor B-DAT
aims O
to O
generate O
realistic O
images O
to O
fool O

discriminator B-DAT
while O
the O
discriminator O
tries O
to O
distinguish O
the O
generated O
im- O
ages O

28, O
42] O
have O
been O
proposed O
to B-DAT
generate O
more O
realistic O
images. O
In O

per, O
our O
model O
uses O
GAN O
to B-DAT
learn O
the O
transformation O
from O
a O

source O
domain O
to B-DAT
a O
significantly O
different O
target O
domain O

to B-DAT

to B-DAT

13, O
18, O
25, O
35, O
44] O
to B-DAT
learn O
image O
translation O
from O
an O

loss O
for O
the O
first O
time O
to B-DAT
enforce O
one-to- O
one O
mapping. O
UNIT O

25] O
assumed O
a O
shared-latent O
space O
to B-DAT
tackle O
unsupervised O
image O
translation. O
However O

MUNIT O
[13] O
makes O
it O
possible O
to B-DAT
extend O
to O
many-to-many O
mapping O
by O

the O
separated O
content O
and O
style O
to B-DAT
generate O
the O
final O
image, O
where O

to B-DAT

13, O
25, O
21] O
are O
limited O
to B-DAT
the O
dataset O
that O
contains O
well-aligned O

re- O
gions O
by O
the O
CNN O
to B-DAT
determine O
that O
class. O
In O
this O

work, O
our O
model O
leads O
to B-DAT
intensively O
change O
discriminative O
image O
regions O

max O
pooling O
is O
also O
used O
to B-DAT
make O
the O
results O
better O

Adaptive O
Layer-Instance O
Normalization O
(AdaLIN) O
function O
to B-DAT
adaptively O
select O
a O
proper O
ratio O

3. O
U-GAT-IT O
Our O
goal O
is O
to B-DAT
train O
a O
function O
Gs→t O
that O

from O
a O
source O
domain O
Xs O
to B-DAT
a O
target O
domain O
Xt O
using O

the O
discriminator B-DAT
guides O
the O
generator O
to O
focus O
on O
regions O
that O
are O

critical O
to B-DAT
generate O
a O
realistic O
image. O
The O

in O
the O
generator B-DAT
gives O
attention O
to O
the O
region O
distinguished O
from O
the O

the O
auxiliary O
classifier O
is O
trained O
to B-DAT
learn O
the O
importance O
weights O
of O

of O
ρ O
are O
con- O
strained O
to B-DAT
the O
range O
of O
[0, O
1 O

value O
of O
ρ O
is O
close O
to B-DAT
1 O
in O
the O
task O
where O

value O
of O
ρ O
is O
close O
to B-DAT
0 O
in O
the O
task O
where O

value O
of O
ρ O
is O
initialized O
to B-DAT
1 O
in O
the O
residual O
blocks O

An O
optimal O
method O
to B-DAT
transfer O
the O
content O
features O
onto O

the O
style O
features O
is O
to B-DAT
apply O
Whitening O
and O
Coloring O
Trans O

computational O
cost O
is O
high O
due O
to B-DAT
the O
calculation O
of O
the O
covariance O

the O
WCT, O
it O
is O
sub-optimal O
to B-DAT
WCT O
as O
it O
assumes O
uncorrelation O

the O
content O
information, O
which O
helps O
to B-DAT
solve O
a O
wide O
range O
of O

to B-DAT

the O
translated O
source O
domain. O
Similar O
to B-DAT
other O
translation O
models, O
the O
discriminator O

ηDt(x) O
andDt(x) O
are O
now O
trained O
to B-DAT
discriminate O
whether O
x O
comes O
from O

our O
discriminator B-DAT
Dt(x) O
becomes O
equal O
to O
CDt(aDt(x O

An O
adversarial O
loss O
is O
employed O
to B-DAT
match O
the O
distribution O
of O
the O

translated O
images O
to B-DAT
the O
target O
image O
distribution O

apply O
a O
cycle O
consistency O
constraint O
to B-DAT
the O
generator. O
Given O
an O
image O

translations O
of O
x O
from O
Xs O
to B-DAT
Xt O
and O
from O
Xt O
to O

be O
success- O
fully O
translated O
back O
to B-DAT
the O
original O
domain O

an O
iden- O
tity O
consistency O
constraint O
to B-DAT
the O
generator. O
Given O
an O
image O

Xt}. O
Gs→t O
and O
Dt O
get O
to B-DAT
know O
where O
they O
need O
to O

coders, O
discriminators, B-DAT
and O
auxiliary O
classifiers O
to O
optimize O
the O
final O
objective O

the O
encoder O
in O
the O
generator, B-DAT
to O
increase O
the O
accuracy O
of O
the O

probability O
of O
0.5, O
resized O
them O
to B-DAT
286 O
x O
286, O
and O
random O

cropped O
them O
to B-DAT
256 O
x O
256. O
The O
batch O

size O
is O
set O
to B-DAT
one O
for O
all O
experiments. O
We O

iterations O
and O
linearly O
decayed O
up O
to B-DAT
1,000,000 O
iterations. O
We O
also O
use O

CycleGAN O
uses O
an O
adversarial O
loss O
to B-DAT
learn O
the O
mapping O
between O
two O

structure O
of O
UNIT O
is O
similar O
to B-DAT
CycleGAN, O
but O
UNIT O
is O
different O

to B-DAT

image O
like O
MUNIT. O
Simi- O
lar O
to B-DAT
MUNIT, O
it O
decomposes O
the O
image O

selfie2anime. O
All O
images O
are O
resized O
to B-DAT
256 O
x O
256 O
for O
training O

anime O
face O
images O
are O
resized O
to B-DAT
256 O
x O
256 O
by O
applying O

user O
study. O
Users O
are O
asked O
to B-DAT
select O
the O
best O
image O
among O

we O
conduct O
an O
ablation O
study O
to B-DAT
confirm O
the O
bene- O
fit O
from O

feature O
map O
helps O
the O
generator B-DAT
to O
focus O
on O
the O
source O
image O

the O
discriminator B-DAT
concentrates O
its O
attention O
to O
deter- O
mine O
whether O
the O
target O

maps O
can O
help O
the O
generator B-DAT
to O
capture O
the O
global O
structure O
(e.g O

have O
applied O
the O
AdaLIN O
only O
to B-DAT
the O
decoder O
of O
the O
generator O

blocks O
in O
the O
decoder O
is O
to B-DAT
embed O
features, O
and O
the O
role O

blocks O
in O
the O
decoder O
is O
to B-DAT
generate O
target O
domain O
images O
from O

gate O
parameter O
ρ O
is O
closer O
to B-DAT
1, O
it O
means O
that O
the O

value O
of O
ρ O
is O
closer O
to B-DAT
0, O
it O
means O
that O
the O

cheekbones) O
are O
well O
preserved O
due O
to B-DAT
channel-wise O
normalized O
feature O
statistics O
used O

However, O
the O
amount O
of O
translation O
to B-DAT
tar- O
get O
domain O
style O
is O

us O
that O
it O
is O
beneficial O
to B-DAT
rely O
more O
on O
IN O
than O

in O
the O
feature O
representation O
layers O
to B-DAT
preserve O
semantic O
characteristics O
of O
source O

in O
the O
de- O
coder O
according O
to B-DAT
source O
and O
target O
domain O
distributions O

to B-DAT

are O
showing O
better O
results O
compared O
to B-DAT
these O

photo2portrait, B-DAT
the O
IN O
is O
known O
to O
perform O
well. O
The O
AdaLIN O
can O

the O
ρ O
value O
is O
close O
to B-DAT
1 O

with O
source O
image, O
and O
asked O
to B-DAT
select O
the O
best O
translated O
im O

- O
age O
to B-DAT
target O
domain. O
We O
inform O
only O

i.e., O
animation, O
dog, O
and O
zebra O
to B-DAT
the O
participants. O
But, O
some O
example O

datasets O
as O
minimum O
informa- O
tion O
to B-DAT
ensure O
proper O
judgments. O
Table O
2 O

in O
human O
perceptual O
study O
compared O
to B-DAT
other O
methods. O
In O
Fig O
6 O

using O
U-GAT-IT O
are O
visually O
superior O
to B-DAT
other O
methods O
while O
preserving O
semantic O

and O
DRIT O
are O
much O
dissimilar O
to B-DAT
the O
source O
images O
since O
they O

Inception O
network O
[34]. O
In O
contrast O
to B-DAT
the O
Frchet O
Inception O
Distance O
[11 O

the O
five O
datasets. O
From O
top B-DAT
to O
bottom: O
selfie2anime, O
horse2zebra, O
cat2dog, O
photo2portrait O

the O
five O
datasets. O
From O
top B-DAT
to O
bottom: O
anime2selfie, O
zebra2horse, O
dog2cat, O
protrait2photo O

to B-DAT

classifier O
can O
guide O
gen- O
erator B-DAT
to O
focus O
more O
on O
distinct O
regions O

of O
the O
proposed O
method O
compared O
to B-DAT
the O
existing O
state-of-the-art O
GAN- O
based O

to B-DAT

to B-DAT

two O
time-scale O
update O
rule O
converge O
to B-DAT
a O
local O
nash O
equilibrium. O
In O

to B-DAT

and O
A. O
A. O
Efros. O
Image- O
to B-DAT

Lee, O
and O
J. O
Kim. O
Learning O
to B-DAT
discover O
cross-domain O
relations O
with O
generative O

to B-DAT

to B-DAT

to B-DAT

K. O
Murphy. O
Xgan: O
Unsupervised O
image- O
to B-DAT

to B-DAT

to B-DAT

A. O
A. O
Efros. O
Unpaired O
image- O
to B-DAT

Additional O
experimental O
results O
In O
addition O
to B-DAT
the O
results O
presented O
in O
the O

256 O
x O
256. O
For O
the O
anime B-DAT
dataset, O
we O
have O
firstly O
retrieved O

are O
extracted O
by O
using O
an O
anime B-DAT

lected O
two O
datasets O
of O
female O
anime B-DAT
face O
images, O
with O
the O
sizes O

anime B-DAT

anime B-DAT
face O
images O
are O
resized O
to O

selfie2anime B-DAT
The O
selfie O
dataset O
contains O
46,836 O
selfie O
images O

the O
same O
numbers O
as O
the O
selfie B-DAT
dataset. O
Finally, O
all O

to B-DAT

novel O
method O
for O
unsupervised O
image- O
to B-DAT

to B-DAT

attention O
module O
guides O
our O
model O
to B-DAT
focus O
on O
more O
important O
regions O

function O
helps O
our O
attention-guided O
model O
to B-DAT
flexibly O
control O
the O
amount O
of O

of O
the O
proposed O
method O
compared O
to B-DAT
the O
existing O
state-of-the-art O
models O
with O

1. O
Introduction O
Image-to-image B-DAT
translation O
aims O
to O
learn O
a O
function O
that O

works O
have O
been O
further O
developed O
to B-DAT
handle O
the O
multi-modality O
of O
the O

and O
alignment O
are O
often O
required O
to B-DAT
avoid O
these O
problems O
by O
limiting O

structure O
or O
hyper-parameter O
setting O
needs O
to B-DAT
be O
ad- O
justed O
for O
the O

to B-DAT

to B-DAT

29, O
24] O
do O
not O
allow O
to B-DAT
transform O
the O
shape O
of O
the O

because O
of O
attaching O
the O
background O
to B-DAT
the O
(translated) O
cropped O
objects. O
Unlike O

Our O
model O
guides O
the O
translation O
to B-DAT
focus O
on O
more O
important O
regions O

into B-DAT
the O
generator O
and O
discriminator O
to O
focus O
on O
semantically O
impor- O
tant O

In O
addition O
to B-DAT
the O
attentional O
mechanism, O
we O
have O

function O
helps O
our O
attention-guided O
model O
to B-DAT
flexibly O
control O
the O
amount O
of O

of O
the O
proposed O
method O
compared O
to B-DAT
the O
existing O
state-of-the-art O
models O
on O

novel O
method O
for O
unsupervised O
image- O
to B-DAT

attention O
module O
helps O
the O
model O
to B-DAT
know O
where O
to O
transform O
intensively O

function O
helps O
our O
attention-guided O
model O
to B-DAT
flexibly O
control O
the O
amount O
of O

training, O
a O
genera- O
tor B-DAT
aims O
to O
generate O
realistic O
images O
to O
fool O

discriminator B-DAT
while O
the O
discriminator O
tries O
to O
distinguish O
the O
generated O
im- O
ages O

28, O
42] O
have O
been O
proposed O
to B-DAT
generate O
more O
realistic O
images. O
In O

per, O
our O
model O
uses O
GAN O
to B-DAT
learn O
the O
transformation O
from O
a O

source O
domain O
to B-DAT
a O
significantly O
different O
target O
domain O

to B-DAT

to B-DAT

13, O
18, O
25, O
35, O
44] O
to B-DAT
learn O
image O
translation O
from O
an O

loss O
for O
the O
first O
time O
to B-DAT
enforce O
one-to- O
one O
mapping. O
UNIT O

25] O
assumed O
a O
shared-latent O
space O
to B-DAT
tackle O
unsupervised O
image O
translation. O
However O

MUNIT O
[13] O
makes O
it O
possible O
to B-DAT
extend O
to O
many-to-many O
mapping O
by O

the O
separated O
content O
and O
style O
to B-DAT
generate O
the O
final O
image, O
where O

to B-DAT

13, O
25, O
21] O
are O
limited O
to B-DAT
the O
dataset O
that O
contains O
well-aligned O

re- O
gions O
by O
the O
CNN O
to B-DAT
determine O
that O
class. O
In O
this O

work, O
our O
model O
leads O
to B-DAT
intensively O
change O
discriminative O
image O
regions O

max O
pooling O
is O
also O
used O
to B-DAT
make O
the O
results O
better O

Adaptive O
Layer-Instance O
Normalization O
(AdaLIN) O
function O
to B-DAT
adaptively O
select O
a O
proper O
ratio O

3. O
U-GAT-IT O
Our O
goal O
is O
to B-DAT
train O
a O
function O
Gs→t O
that O

from O
a O
source O
domain O
Xs O
to B-DAT
a O
target O
domain O
Xt O
using O

the O
discriminator B-DAT
guides O
the O
generator O
to O
focus O
on O
regions O
that O
are O

critical O
to B-DAT
generate O
a O
realistic O
image. O
The O

in O
the O
generator B-DAT
gives O
attention O
to O
the O
region O
distinguished O
from O
the O

the O
auxiliary O
classifier O
is O
trained O
to B-DAT
learn O
the O
importance O
weights O
of O

of O
ρ O
are O
con- O
strained O
to B-DAT
the O
range O
of O
[0, O
1 O

value O
of O
ρ O
is O
close O
to B-DAT
1 O
in O
the O
task O
where O

value O
of O
ρ O
is O
close O
to B-DAT
0 O
in O
the O
task O
where O

value O
of O
ρ O
is O
initialized O
to B-DAT
1 O
in O
the O
residual O
blocks O

An O
optimal O
method O
to B-DAT
transfer O
the O
content O
features O
onto O

the O
style O
features O
is O
to B-DAT
apply O
Whitening O
and O
Coloring O
Trans O

computational O
cost O
is O
high O
due O
to B-DAT
the O
calculation O
of O
the O
covariance O

the O
WCT, O
it O
is O
sub-optimal O
to B-DAT
WCT O
as O
it O
assumes O
uncorrelation O

the O
content O
information, O
which O
helps O
to B-DAT
solve O
a O
wide O
range O
of O

to B-DAT

the O
translated O
source O
domain. O
Similar O
to B-DAT
other O
translation O
models, O
the O
discriminator O

ηDt(x) O
andDt(x) O
are O
now O
trained O
to B-DAT
discriminate O
whether O
x O
comes O
from O

our O
discriminator B-DAT
Dt(x) O
becomes O
equal O
to O
CDt(aDt(x O

An O
adversarial O
loss O
is O
employed O
to B-DAT
match O
the O
distribution O
of O
the O

translated O
images O
to B-DAT
the O
target O
image O
distribution O

apply O
a O
cycle O
consistency O
constraint O
to B-DAT
the O
generator. O
Given O
an O
image O

translations O
of O
x O
from O
Xs O
to B-DAT
Xt O
and O
from O
Xt O
to O

be O
success- O
fully O
translated O
back O
to B-DAT
the O
original O
domain O

an O
iden- O
tity O
consistency O
constraint O
to B-DAT
the O
generator. O
Given O
an O
image O

Xt}. O
Gs→t O
and O
Dt O
get O
to B-DAT
know O
where O
they O
need O
to O

coders, O
discriminators, B-DAT
and O
auxiliary O
classifiers O
to O
optimize O
the O
final O
objective O

the O
encoder O
in O
the O
generator, B-DAT
to O
increase O
the O
accuracy O
of O
the O

probability O
of O
0.5, O
resized O
them O
to B-DAT
286 O
x O
286, O
and O
random O

cropped O
them O
to B-DAT
256 O
x O
256. O
The O
batch O

size O
is O
set O
to B-DAT
one O
for O
all O
experiments. O
We O

iterations O
and O
linearly O
decayed O
up O
to B-DAT
1,000,000 O
iterations. O
We O
also O
use O

CycleGAN O
uses O
an O
adversarial O
loss O
to B-DAT
learn O
the O
mapping O
between O
two O

structure O
of O
UNIT O
is O
similar O
to B-DAT
CycleGAN, O
but O
UNIT O
is O
different O

to B-DAT

image O
like O
MUNIT. O
Simi- O
lar O
to B-DAT
MUNIT, O
it O
decomposes O
the O
image O

selfie2anime. O
All O
images O
are O
resized O
to B-DAT
256 O
x O
256 O
for O
training O

anime O
face O
images O
are O
resized O
to B-DAT
256 O
x O
256 O
by O
applying O

user O
study. O
Users O
are O
asked O
to B-DAT
select O
the O
best O
image O
among O

we O
conduct O
an O
ablation O
study O
to B-DAT
confirm O
the O
bene- O
fit O
from O

feature O
map O
helps O
the O
generator B-DAT
to O
focus O
on O
the O
source O
image O

the O
discriminator B-DAT
concentrates O
its O
attention O
to O
deter- O
mine O
whether O
the O
target O

maps O
can O
help O
the O
generator B-DAT
to O
capture O
the O
global O
structure O
(e.g O

have O
applied O
the O
AdaLIN O
only O
to B-DAT
the O
decoder O
of O
the O
generator O

blocks O
in O
the O
decoder O
is O
to B-DAT
embed O
features, O
and O
the O
role O

blocks O
in O
the O
decoder O
is O
to B-DAT
generate O
target O
domain O
images O
from O

gate O
parameter O
ρ O
is O
closer O
to B-DAT
1, O
it O
means O
that O
the O

value O
of O
ρ O
is O
closer O
to B-DAT
0, O
it O
means O
that O
the O

cheekbones) O
are O
well O
preserved O
due O
to B-DAT
channel-wise O
normalized O
feature O
statistics O
used O

However, O
the O
amount O
of O
translation O
to B-DAT
tar- O
get O
domain O
style O
is O

us O
that O
it O
is O
beneficial O
to B-DAT
rely O
more O
on O
IN O
than O

in O
the O
feature O
representation O
layers O
to B-DAT
preserve O
semantic O
characteristics O
of O
source O

in O
the O
de- O
coder O
according O
to B-DAT
source O
and O
target O
domain O
distributions O

to B-DAT

are O
showing O
better O
results O
compared O
to B-DAT
these O

photo2portrait, B-DAT
the O
IN O
is O
known O
to O
perform O
well. O
The O
AdaLIN O
can O

the O
ρ O
value O
is O
close O
to B-DAT
1 O

with O
source O
image, O
and O
asked O
to B-DAT
select O
the O
best O
translated O
im O

- O
age O
to B-DAT
target O
domain. O
We O
inform O
only O

i.e., O
animation, O
dog, O
and O
zebra O
to B-DAT
the O
participants. O
But, O
some O
example O

datasets O
as O
minimum O
informa- O
tion O
to B-DAT
ensure O
proper O
judgments. O
Table O
2 O

in O
human O
perceptual O
study O
compared O
to B-DAT
other O
methods. O
In O
Fig O
6 O

using O
U-GAT-IT O
are O
visually O
superior O
to B-DAT
other O
methods O
while O
preserving O
semantic O

and O
DRIT O
are O
much O
dissimilar O
to B-DAT
the O
source O
images O
since O
they O

Inception O
network O
[34]. O
In O
contrast O
to B-DAT
the O
Frchet O
Inception O
Distance O
[11 O

the O
five O
datasets. O
From O
top B-DAT
to O
bottom: O
selfie2anime, O
horse2zebra, O
cat2dog, O
photo2portrait O

the O
five O
datasets. O
From O
top B-DAT
to O
bottom: O
anime2selfie, O
zebra2horse, O
dog2cat, O
protrait2photo O

to B-DAT

classifier O
can O
guide O
gen- O
erator B-DAT
to O
focus O
more O
on O
distinct O
regions O

of O
the O
proposed O
method O
compared O
to B-DAT
the O
existing O
state-of-the-art O
GAN- O
based O

to B-DAT

to B-DAT

two O
time-scale O
update O
rule O
converge O
to B-DAT
a O
local O
nash O
equilibrium. O
In O

to B-DAT

and O
A. O
A. O
Efros. O
Image- O
to B-DAT

Lee, O
and O
J. O
Kim. O
Learning O
to B-DAT
discover O
cross-domain O
relations O
with O
generative O

to B-DAT

to B-DAT

to B-DAT

K. O
Murphy. O
Xgan: O
Unsupervised O
image- O
to B-DAT

to B-DAT

to B-DAT

A. O
A. O
Efros. O
Unpaired O
image- O
to B-DAT

Additional O
experimental O
results O
In O
addition O
to B-DAT
the O
results O
presented O
in O
the O

UCY O
[23, O
16], O
and O
ActEV B-DAT

23] O
and O
UCY O
[16], O
and O
ActEV B-DAT

4.1. O
ActEV B-DAT

Dataset O
& O
Setups. O
ActEV B-DAT

in O
the O
pixel O
space O
on O
ActEV B-DAT

to O
baseline O
methods O
on O
the O
ActEV B-DAT

iment O
on O
the O
public O
benchmark O
ActEV, B-DAT
the O
primary O
driver O
of O
which O

for O
our O
experiments O
on O
the O
ActEV B-DAT

5.1. O
ActEV B-DAT

In O
ActEV B-DAT

Since O
the O
ActEV B-DAT

for O
different O
trajectory O
class O
in O
ActEV B-DAT
dataset O
(on O
the O
training O
set O

single O
model O
experiment O
on O
the O
ActEV B-DAT

activity O
detection O
experiments O
on O
the O
ActEV B-DAT

5.2.2 O
Comparing O
ActEV B-DAT

We O
compare O
the O
ActEV B-DAT

5.2.2. O
As O
we O
see, O
the O
ActEV B-DAT

the O
other O
benchmark. O
Also, O
the O
ActEV B-DAT

for O
multi- O
task O
learning. O
The O
ActEV B-DAT

ActEV B-DAT
ETH, O
UCY O
#Scene O
5 O
4 O

actev B-DAT

actev B-DAT

actev B-DAT

actev B-DAT

actev B-DAT

actev B-DAT

model O
on O
two O
bench- O
marks: O
ETH B-DAT
& O
UCY O
[23, O
16], O
and O

benchmarks O
for O
future O
path O
prediction: O
ETH B-DAT
[23] O
and O
UCY O
[16], O
and O

ActEV/VIRAT O
whereas O
in O
meters O
on O
ETH B-DAT
and O
UCY. O
For O
future O
activity O

Fig. O
6 O
for O
legends. O
Method O
ETH B-DAT
HOTEL O
UNIV O
* O
ZARA1 O
ZARA2 O

Comparison O
of O
different O
methods O
on O
ETH B-DAT
(Column O
3 O
and O
4) O
and O

4.3. O
ETH B-DAT
& O
UCY O

Dataset. O
ETH B-DAT
[23] O
and O
UCY O
[16] O
are O

includes O
videos O
from O
five O
scenes: O
ETH, B-DAT
HO- O
TEL, O
UNIV, O
ZARA1 O
and O

the O
best-published O
single O
model O
on O
ETH B-DAT
and O
best O
average O
performance O
on O

the O
ETH B-DAT
& O
UCY O
bench- O
mark. O
As O

than O
others: O
0.389 O
(ZARA1), O
0.460 O
(ETH), B-DAT
0.258 O
(UNIV). O
Recall O
that O
the O

experiments O
on O
the O
ActEV/VIRAT O
and O
ETH B-DAT
& O
UCY O
Benchmarks. O
We O
also O

5.2.2 O
Comparing O
ActEV/VIRAT O
to O
ETH B-DAT
& O
UCY O
Benchmark O

the O
ActEV/VIRAT O
dataset O
and O
the O
ETH B-DAT
& O
UCY O
trajectory O
benchmark O
in O

5.3. O
ETH B-DAT
& O
UCY O
Details O

ActEV O
ETH, B-DAT
UCY O
#Scene O
5 O
4 O

two O
bench- O
marks: O
ETH O
& O
UCY B-DAT
[23, O
16], O
and O
ActEV/VIRAT O
[22 O

path O
prediction: O
ETH O
[23] O
and O
UCY B-DAT
[16], O
and O
ActEV/VIRAT O
[3, O
22 O

in O
meters O
on O
ETH O
and O
UCY B-DAT

Column O
3 O
and O
4) O
and O
UCY B-DAT
datasets O
(Column O
5-7). O
* O
We O

4.3. O
ETH O
& O
UCY B-DAT

Dataset. O
ETH O
[23] O
and O
UCY B-DAT
[16] O
are O
common O
datasets O
for O

performance O
on O
the O
ETH O
& O
UCY B-DAT
bench- O
mark. O
As O
shown O
in O

the O
ActEV/VIRAT O
and O
ETH O
& O
UCY B-DAT
Benchmarks. O
We O
also O
provide O
statistical O

Comparing O
ActEV/VIRAT O
to O
ETH O
& O
UCY B-DAT
Benchmark O

dataset O
and O
the O
ETH O
& O
UCY B-DAT
trajectory O
benchmark O
in O
Table O
5.2.2 O

5.3. O
ETH O
& O
UCY B-DAT
Details O

ActEV O
ETH, O
UCY B-DAT
#Scene O
5 O
4 O

have O
designed O
a O
toy O
example O
dataset B-DAT
of O
trajectories O
that O
can O
be O

The O
design O
of O
a O
synthetic O
dataset B-DAT
specifically O
oriented O
to O
the O
evaluation O

In O
each O
dataset, B-DAT
we O
train O
the O
GAN O
network O

of O
2.5 O
fps. O
The O
ETH O
dataset B-DAT
contains O
2 O
ex- O
periments O
(coined O

and O
Hotel) O
and O
the O
UCY O
dataset B-DAT
contains O
3 O
experiments O
(ZARA01, O
ZARA02 O

evaluate O
the O
prediction O
algorithm, O
each O
dataset B-DAT
is O
split O
into O
5 O
subsets O

it, O
it O
needs O
a O
synthetic O
dataset B-DAT
as O
a O
second O
source O
of O

have O
created O
a O
toy O
example O
dataset B-DAT
to O
study O
the O
mode O
collapsing O

EMD O
metrics O
on O
our O
toy O
dataset B-DAT
with O
|Sr| O
= O
|Sg O

Figure O
4. O
Toy O
trajectory O
dataset B-DAT

predictive O
distributions O
on O
the O
SDD O
dataset B-DAT

proposed O
a O
specifi- O
cally O
designed O
dataset B-DAT
and O
an O
evaluation O
benchmark O
to O

periments O
(coined O
as O
ETH O
and O
Hotel) B-DAT
and O
the O
UCY O
dataset O
contains O

rors O
for O
the O
ETH O
and O
Hotel B-DAT
experiments, O
but O
not O
on O
the O

significantly O
smaller O
than O
in O
the O
Hotel B-DAT
and O
ETH O
scenes. O
Hence, O
there O

1.43 O
0.39 O
/ O
0.64 O
Hotel B-DAT
0.36 O
/ O
0.64 O
0.52 O

use O
two O
publicly O
available O
datasets: O
ETH B-DAT
[13] O
and O
UCY O
[11]. O
These O

rate O
of O
2.5 O
fps. O
The O
ETH B-DAT
dataset O
contains O
2 O
ex- O
periments O

coined O
as O
ETH B-DAT
and O
Hotel) O
and O
the O
UCY O

prediction O
er- O
rors O
for O
the O
ETH B-DAT
and O
Hotel O
experiments, O
but O
not O

than O
in O
the O
Hotel O
and O
ETH B-DAT
scenes. O
Hence, O
there O
is O
less O

S-LSTM O
S-GAN O
S-GAN-P O
SoPhie O
S-Ways O
ETH B-DAT
0.59 O
/ O
1.22 O
0.67 O

have O
designed O
a O
toy O
example O
dataset B-DAT
of O
trajectories O
that O
can O
be O

The O
design O
of O
a O
synthetic O
dataset B-DAT
specifically O
oriented O
to O
the O
evaluation O

In O
each O
dataset, B-DAT
we O
train O
the O
GAN O
network O

of O
2.5 O
fps. O
The O
ETH O
dataset B-DAT
contains O
2 O
ex- O
periments O
(coined O

and O
Hotel) O
and O
the O
UCY O
dataset B-DAT
contains O
3 O
experiments O
(ZARA01, O
ZARA02 O

evaluate O
the O
prediction O
algorithm, O
each O
dataset B-DAT
is O
split O
into O
5 O
subsets O

it, O
it O
needs O
a O
synthetic O
dataset B-DAT
as O
a O
second O
source O
of O

have O
created O
a O
toy O
example O
dataset B-DAT
to O
study O
the O
mode O
collapsing O

EMD O
metrics O
on O
our O
toy O
dataset B-DAT
with O
|Sr| O
= O
|Sg O

Figure O
4. O
Toy O
trajectory O
dataset B-DAT

predictive O
distributions O
on O
the O
SDD O
dataset B-DAT

proposed O
a O
specifi- O
cally O
designed O
dataset B-DAT
and O
an O
evaluation O
benchmark O
to O

ray14 B-DAT
dataset O
[31], O
which O
is O
the O

ray14 B-DAT
dataset O
following O
the O
official O
train-test O

ray14, B-DAT
a O
relatively O
smaller O
dataset, O
chestx-ray8 O

ray14 B-DAT

ray14 B-DAT
dataset O
contains O
total O
112, O
120 O

ray14 B-DAT
dataset. O
Images O
are O
prepared O
from O

then O
fine-tuned O
on O
the O
chestx- O
ray14 B-DAT
dataset O
with O
various O
image O
augmentation O

ray14 B-DAT
dataset. O
In O
the O
second O
stage O

ray14 B-DAT
dataset. O
It O
ensures O
faster O
convergence O

ray14 B-DAT
dataset O
on O
its O
official O
test O

health O
(NIH) O
has O
released O
the O
chestx-ray14 B-DAT
dataset O
[31], O
which O
is O
the O

gives O
the O
state-of-the-art O
result O
on O
chestx-ray14 B-DAT
dataset O
following O
the O
official O
train-test O

WORK O
Before O
the O
release O
of O
chestx-ray14, B-DAT
a O
relatively O
smaller O
dataset, O
chestx-ray8 O

compared O
to O
0.1 O
million O
in O
chestx-ray14 B-DAT

4.1 O
Dataset O
The O
chestx-ray14 B-DAT
dataset O
contains O
total O
112, O
120 O

distribu- O
tion O
of O
samples O
in O
chestx-ray14 B-DAT
dataset. O
Images O
are O
prepared O
from O

patches O
of O
128× O
128 O
of O
chestx-ray14 B-DAT
dataset. O
In O
the O
second O
stage O

weights O
of O
BL5 O
finetuned O
on O
chestx-ray14 B-DAT
dataset. O
It O
ensures O
faster O
convergence O

all O
14 O
diseases O
in O
the O
chestx-ray14 B-DAT
dataset O
on O
its O
official O
test O

convolutional O
neural O
network O
trained O
on O
ChestX-ray14, B-DAT
cur- O
rently O
the O
largest O
publicly O

detect O
all O
14 O
diseases O
in O
ChestX-ray14 B-DAT
and O
achieve O
state O
of O
the O

CheXNet O
on O
the O
recently O
released O
ChestX-ray14 B-DAT
dataset O
(Wang O
et O
al., O
2017 O

subset O
of O
420 O
images O
from O
ChestX-ray14 B-DAT

against O
previous O
work O
us- O
ing O
ChestX-ray14, B-DAT
we O
make O
simple O
modifications O
to O

detect O
all O
14 O
diseases O
in O
ChestX-ray14, B-DAT
and O
find O
that O
we O
outperform O

al., O
2016) O
trained O
on O
the O
ChestX-ray B-DAT
14 O
dataset. O
DenseNets O
improve O
flow O

We O
use O
the O
ChestX-ray14 B-DAT
dataset O
released O
by O
Wang O
et O

all O
14 O
pathologies O
in O
the O
ChestX-ray14 B-DAT
dataset. O
In O
detecting O
Mass, O
Nodule O

of O
the O
Art O
on O
the O
ChestX-ray14 B-DAT
Dataset O

sets, O
following O
previous O
work O
on O
ChestX-ray14 B-DAT
(Wang O
et O
al., O
2017; O
Yao O

Wang O
et O
al. O
(2017) O
released O
ChestX-ray B-DAT

state O
of O
the O
art O
on O
ChestX-ray14, B-DAT
the O
largest O
publicly O
available O
chest O

convolutional O
neural O
network O
trained O
on O
ChestX B-DAT

detect O
all O
14 O
diseases O
in O
ChestX B-DAT

CheXNet O
on O
the O
recently O
released O
ChestX B-DAT

subset O
of O
420 O
images O
from O
ChestX B-DAT

against O
previous O
work O
us- O
ing O
ChestX B-DAT

detect O
all O
14 O
diseases O
in O
ChestX B-DAT

al., O
2016) O
trained O
on O
the O
ChestX B-DAT

We O
use O
the O
ChestX B-DAT

all O
14 O
pathologies O
in O
the O
ChestX B-DAT

of O
the O
Art O
on O
the O
ChestX B-DAT

sets, O
following O
previous O
work O
on O
ChestX B-DAT

Wang O
et O
al. O
(2017) O
released O
ChestX B-DAT

state O
of O
the O
art O
on O
ChestX B-DAT

ray14, B-DAT
cur- O
rently O
the O
largest O
publicly O

ray14 B-DAT
and O
achieve O
state O
of O
the O

ray14 B-DAT
dataset O
(Wang O
et O
al., O
2017 O

ray14 B-DAT

ray14, B-DAT
we O
make O
simple O
modifications O
to O

ray14, B-DAT
and O
find O
that O
we O
outperform O

ray14 B-DAT
dataset O
released O
by O
Wang O
et O

ray14 B-DAT
dataset. O
In O
detecting O
Mass, O
Nodule O

ray14 B-DAT
Dataset O

ray14 B-DAT
(Wang O
et O
al., O
2017; O
Yao O

ray14, B-DAT
the O
largest O
publicly O
available O
chest O

convolutional O
neural O
network O
trained O
on O
ChestX-ray14, B-DAT
cur- O
rently O
the O
largest O
publicly O

detect O
all O
14 O
diseases O
in O
ChestX-ray14 B-DAT
and O
achieve O
state O
of O
the O

CheXNet O
on O
the O
recently O
released O
ChestX-ray14 B-DAT
dataset O
(Wang O
et O
al., O
2017 O

subset O
of O
420 O
images O
from O
ChestX-ray14 B-DAT

against O
previous O
work O
us- O
ing O
ChestX-ray14, B-DAT
we O
make O
simple O
modifications O
to O

detect O
all O
14 O
diseases O
in O
ChestX-ray14, B-DAT
and O
find O
that O
we O
outperform O

We O
use O
the O
ChestX-ray14 B-DAT
dataset O
released O
by O
Wang O
et O

all O
14 O
pathologies O
in O
the O
ChestX-ray14 B-DAT
dataset. O
In O
detecting O
Mass, O
Nodule O

of O
the O
Art O
on O
the O
ChestX-ray14 B-DAT
Dataset O

sets, O
following O
previous O
work O
on O
ChestX-ray14 B-DAT
(Wang O
et O
al., O
2017; O
Yao O

state O
of O
the O
art O
on O
ChestX-ray14, B-DAT
the O
largest O
publicly O
available O
chest O

memexqa B-DAT

memexqa B-DAT

memexqa B-DAT

achieves O
state-of-the-art O
performance O
on O
the O
MemexQA B-DAT
dataset O
and O
competitive O
results O
on O

end, O
we O
experiment O
on O
the O
MemexQA B-DAT
dataset O
[9]. O
The O
sequential O
data O

in O
MemexQA B-DAT
involves O
multiple O
modalities, O
including O
titles O

5.1. O
MemexQA B-DAT

Dataset O
MemexQA B-DAT
[9] O
is O
a O
recently O
proposed O

MemexQA B-DAT
provides O
4 O
answer O
choices O
and O

Comparison O
of O
different O
methods O
on O
MemexQA B-DAT
by O
question O
type. O
The O
first O

Implementation O
Details O
In O
MemexQA B-DAT
dataset, O
each O
ques- O
tion O
is O

compares O
the O
accuracy O
on O
the O
MemexQA B-DAT

The O
MemexQA B-DAT
dataset O
provides O
ground O
truth O
evidence O

and O
its O
ablations O
on O
the O
MemexQA B-DAT
dataset. O
To O
evaluate O
the O
FVTA O

proposed O
FVTA O
method O
on O
the O
MemexQA B-DAT
dataset. O
The O
last O
column O
shows O

same O
way O
as O
in O
the O
MemexQA B-DAT
[9] O
experiment. O
We O
use O
the O

other O
attention O
models O
on O
the O
MemexQA B-DAT
dataset. O
For O
each O
question, O
we O

3 O
MEMEXQA B-DAT
DATASET O

memexqa B-DAT

memexqa B-DAT

memexqa B-DAT

memexqa B-DAT

paper O
proposes O
a O
new O
multimodal O
MemexQA B-DAT
task: O
given O
a O

task, O
we O
1) O
present O
the O
MemexQA B-DAT
dataset, O
the O
first O

question. O
Experimental O
results O
on O
the O
MemexQA B-DAT
dataset O
demonstrate O
that O
our O
model O

a O
new O
VQA O
task O
named O
MemexQA, B-DAT

input O
to O
a O
MemexQA B-DAT
system O
is O
a O
question O
and O

them O
is O
suitable O
for O
our O
MemexQA B-DAT
research O
due O
to O
the O
following O

single O
video O
or O
topic. O
Answering O
MemexQA B-DAT
questions, O
however, O
requires O
reasoning O
over O

provide O
such O
grounding O
photos. O
Third, O
MemexQA B-DAT
is O
a O
multimodal O
QA O
task O

end, O
this O
paper O
introduces O
the O
MemexQA B-DAT
dataset, O
the O
first O
publicly O
available O

photos O
that O
justify O
the O
answer. O
MemexQA B-DAT
is O
an O
interesting O
yet O
challenging O

task. O
Answering O
a O
MemexQA B-DAT
question O
is O
non-trivial O
even O
for O

a O
minute O
to O
answer O
a O
MemexQA B-DAT
question, O
which O
is O
10x O
longer O

VQA O
ques- O
tion. O
We O
consider O
MemexQA B-DAT
to O
be O
a O
multimodal O
AI O

Wide O
Web O
[29]. O
The O
proposed O
MemexQA B-DAT
focuses O
on O
the O
deep O
understanding O

Fig. O
1. O
MemexQA B-DAT
examples. O
The O
inputs O
are O
a O

VQA O
Datasets. O
Our O
MemexQA B-DAT
work O
is O
partly O
motivated O
by O

not O
directly O
applicable O
to O
our O
MemexQA B-DAT
research. O
Compared O
to O
existing O
VQA O

to O
answer O
the O
question. O
Third, O
MemexQA B-DAT
is O
a O
multimodal O
dataset. O
A O

to O
similar O
questions O
in O
the O
MemexQA B-DAT
dataset O
as O
well O
as O
external O

MemexQA B-DAT
is O
the O
first O
publicly O
available O

datasets O
and O
their O
differences O
with O
MemexQA) B-DAT
and O
that O
it O
would O
be O

14], O
[16], O
[17], O
[18], O
the O
MemexQA B-DAT
dataset O
contains O
a O
few O
distinguishing O

characteristics. O
First, O
MemexQA B-DAT
defines O
a O
goal-driven O
VQA O
task O

birthday O
party, O
etc. O
Second, O
the O
MemexQA B-DAT
dataset O
contains O
a O
number O
of O

get O
into O
a O
limo?”. O
Finally, O
MemexQA B-DAT
is O
a O
multimodal O
dataset. O
It O

Questions O
and O
four-choice O
answer O
in O
MemexQA B-DAT

and O
Our O
New O
Dataset O
Called O
MemexQA B-DAT

movie O
plots O
- O
- O
Ours O
MemexQA B-DAT
Personal O
photos O
@ O
@ O
evidential O

due O
to O
the O
reason O
that O
MemexQA B-DAT
is O
very O
expensive O
to O
collect O

those O
restrictions, O
we O
consider O
the O
MemexQA B-DAT
dataset O
to O
be O
reasonably O
large O

examine O
the O
human O
performance O
on O
MemexQA B-DAT

the O
MemexQA B-DAT
task, O
2) O
what O
is O
the O

significantly, O
which O
indicates O
that O
the O
MemexQA B-DAT
task O
requires O
multimodal O
reasoning O
based O

TABLE O
3 O
Human O
Performance O
on O
MemexQA B-DAT

This O
section O
evaluates O
the O
MemexQA B-DAT
task: O
given O
a O
question O
and O

all O
possible O
answers. O
Since O
in O
MemexQA B-DAT
dataset O
there O
is O
only O
one O

Implementation O
Details. O
In O
MemexQA B-DAT
dataset, O
each O
ques- O
tion O
is O

the O
accu- O
racy O
on O
the O
MemexQA B-DAT
with O
multiple-choice O
input. O
We O
eval O

Comparison O
of O
Different O
Methods O
on O
MemexQA B-DAT
Multiple-Choice O
Setting O
by O
Question O
Type O

Comparison O
of O
Different O
Methods O
on O
MemexQA B-DAT
Open-Ended O
Setting O
by O
Question O
Type O

and O
its O
ablations O
on O
the O
MemexQA B-DAT
dataset. O
We O
evaluate O
them O
under O

MemexQA B-DAT
Dataset O

cases O
of O
FVTA O
model O
on O
MemexQA B-DAT
dataset. O
Our O
model O
finds O
the O

same O
way O
as O
in O
the O
MemexQA B-DAT
experi- O
ment.We O
use O
the O
AdaDelta O

other O
attention O
models O
on O
the O
MemexQA B-DAT
dataset. O
For O
each O
question, O
we O

MemexQA B-DAT

our O
model O
on O
the O
PhysioNet B-DAT
Challenge I-DAT
2012 I-DAT
dataset O
[Silva O
et O
al O

Patients: O
The O
PhysioNet/Computing O
in O
Cardiology O
Challenge B-DAT
2012. O
Computing O
in O
cardiology, O
39:245–248 O

model O
on O
the O
PhysioNet O
Challenge O
2012 B-DAT
dataset O
[Silva O
et O
al., O
2012 O

The O
PhysioNet/Computing O
in O
Cardiology O
Challenge O
2012 B-DAT

. O
Computing O
in O
cardiology, O
39:245–248, O
2012 B-DAT

2012 B-DAT

evaluated O
our O
model O
on O
the O
PhysioNet B-DAT
Challenge I-DAT
2012 I-DAT
dataset O
[Silva O
et O
al., O
2012 O

evaluated O
our O
model O
on O
the O
PhysioNet B-DAT
Challenge I-DAT
2012 I-DAT
dataset O
[Silva O
et O
al., O
2012 O

evaluated O
our O
model O
on O
the O
PhysioNet B-DAT
Challenge O
2012 O
dataset O
[Silva O
et O

MSE O
(mean O
± O
std) O
on O
PhysioNet B-DAT

MSE O
(mean O
± O
std) O
on O
PhysioNet B-DAT

Mortality O
of O
ICU O
Patients: O
The O
PhysioNet B-DAT

Physionet O
PhysioNet B-DAT
contains O
the O
data O
from O
the O

our O
model O
on O
the O
PhysioNet B-DAT
Challenge I-DAT
2012 I-DAT
dataset O
[Silva O
et O
al O

Patients: O
The O
PhysioNet/Computing O
in O
Cardiology O
Challenge B-DAT
2012. O
Computing O
in O
cardiology, O
39:245–248 O

model O
on O
the O
PhysioNet O
Challenge O
2012 B-DAT
dataset O
[Silva O
et O
al., O
2012 O

The O
PhysioNet/Computing O
in O
Cardiology O
Challenge O
2012 B-DAT

. O
Computing O
in O
cardiology, O
39:245–248, O
2012 B-DAT

2012 B-DAT

evaluated O
our O
model O
on O
the O
PhysioNet B-DAT
Challenge I-DAT
2012 I-DAT
dataset O
[Silva O
et O
al., O
2012 O

evaluated O
our O
model O
on O
the O
PhysioNet B-DAT
Challenge I-DAT
2012 I-DAT
dataset O
[Silva O
et O
al., O
2012 O

evaluated O
our O
model O
on O
the O
PhysioNet B-DAT
Challenge O
2012 O
dataset O
[Silva O
et O

MSE O
(mean O
± O
std) O
on O
PhysioNet B-DAT

MSE O
(mean O
± O
std) O
on O
PhysioNet B-DAT

Mortality O
of O
ICU O
Patients: O
The O
PhysioNet B-DAT

Physionet O
PhysioNet B-DAT
contains O
the O
data O
from O
the O

our O
model O
on O
the O
PhysioNet B-DAT
Challenge I-DAT
2012 I-DAT
dataset O
[Silva O
et O
al O

Patients: O
The O
PhysioNet/Computing O
in O
Cardiology O
Challenge B-DAT
2012. O
Computing O
in O
cardiology, O
39:245–248 O

model O
on O
the O
PhysioNet O
Challenge O
2012 B-DAT
dataset O
[Silva O
et O
al., O
2012 O

The O
PhysioNet/Computing O
in O
Cardiology O
Challenge O
2012 B-DAT

. O
Computing O
in O
cardiology, O
39:245–248, O
2012 B-DAT

2012 B-DAT

evaluated O
our O
model O
on O
the O
PhysioNet B-DAT
Challenge I-DAT
2012 I-DAT
dataset O
[Silva O
et O
al., O
2012 O

evaluated O
our O
model O
on O
the O
PhysioNet B-DAT
Challenge I-DAT
2012 I-DAT
dataset O
[Silva O
et O
al., O
2012 O

evaluated O
our O
model O
on O
the O
PhysioNet B-DAT
Challenge O
2012 O
dataset O
[Silva O
et O

MSE O
(mean O
± O
std) O
on O
PhysioNet B-DAT

MSE O
(mean O
± O
std) O
on O
PhysioNet B-DAT

Mortality O
of O
ICU O
Patients: O
The O
PhysioNet B-DAT

Physionet O
PhysioNet B-DAT
contains O
the O
data O
from O
the O

Physionet O
Challenge B-DAT
2012 O
dataset O
(PhysioNet) O
This O
dataset O

, O
from O
PhysioNet B-DAT
Challenge I-DAT
2012 I-DAT
(Silva O
et O
al., O
2012 O

or O
interpolation O
(Kreindler O
& O
Lumsden, O
2012), B-DAT
spectral O
analysis O
(Mondal O
& O
Percival O

speech O
recognition O
(Hinton O
et O
al., O
2012 B-DAT

Physionet O
Challenge O
2012 B-DAT
dataset O
(PhysioNet) O
This O
dataset, O
from O

PhysioNet O
Challenge O
2012 B-DAT
(Silva O
et O
al., O
2012), O
is O

Medical O
Center O
from O
2001 O
to O
2012 B-DAT

Signal O
Processing O
Magazine, O
IEEE, O
29(6):82–97, O
2012 B-DAT

Behavioral O
Sciences O
Using O
Real O
Data, O
2012 B-DAT

The O
physionet/computing O
in O
cardiology O
challenge O
2012 B-DAT

. O
In O
CinC, O
2012 B-DAT

dataset O
(PhysioNet) O
This O
dataset, O
from O
PhysioNet B-DAT
Challenge I-DAT
2012 I-DAT
(Silva O
et O
al., O
2012), O
is O

dataset O
(PhysioNet) O
This O
dataset, O
from O
PhysioNet B-DAT
Challenge I-DAT
2012 I-DAT
(Silva O
et O
al., O
2012), O
is O

on O
real-world O
clinical O
datasets O
(MIMIC-III, O
PhysioNet) B-DAT
and O
synthetic O
datasets O
demonstrate O
that O

model O
for O
predicting O
mortality O
on O
PhysioNet B-DAT
dataset. O
Variables O
in O
green O
are O

variables O
for O
predicting O
mortality O
on O
PhysioNet B-DAT
dataset. O
For O
input O
decay, O
we O

Physionet O
Challenge O
2012 O
dataset O
(PhysioNet) B-DAT
This O
dataset, O
from O
PhysioNet O
Challenge O

it O
to O
non-RNN O
models. O
For O
PhysioNet B-DAT
dataset, O
we O
sample O
the O
time O

in O
GRU-mean O
for O
MIMIC-III O
and O
PhysioNet B-DAT
datasets, O
respectively. O
All O
the O
other O

Section O
3.2 O
on O
MIMIC-III O
and O
PhysioNet B-DAT
datasets. O
We O
noticed O
that O
dropout O

for O
all O
4 O
tasks O
on O
PhysioNet B-DAT
and O
20 O
ICD-9 O
code O
tasks O

Models O
MIMIC-III O
PhysioNet B-DAT

MIMIC-III O
PhysioNet2012 B-DAT
Gesture O

Models O
Gesture O
MIMIC-III O
PhysioNet B-DAT

predicting O
all O
4 O
tasks O
on O
PhysioNet B-DAT
dataset. O
mortality, O
in-hospital O
mortality; O
los O

and O
all O
4 O
tasks O
on O
PhysioNet B-DAT
dataset O
in O
Figure O
10. O
The O

Models O
MIMIC-III O
PhysioNet B-DAT

CODAH B-DAT

We O
introduce O
the O
CODAH B-DAT
dataset, O
an O

for O
testing O
common O
sense. O
CODAH B-DAT
forms O
a O

CODAH) B-DAT
for O
commonsense O
question O
answering O

We O
experimentally O
demonstrate O
that O
CODAH B-DAT

accuracy, O
respectively, O
on O
the O
CODAH B-DAT
dataset O
in O

suggests O
that O
answers O
to O
the O
CODAH B-DAT
ques O

NLP O
systems, O
the O
CODAH B-DAT
construction O
procedure O

3 O
The O
CODAH B-DAT
Dataset O

CODAH B-DAT

CODAH B-DAT

CODAH B-DAT

CODAH B-DAT
that O
employ O
different O
types O
of O

on O
the O
CODAH B-DAT
dataset. O
This O
model O
consists O
of O

CODAH B-DAT

CODAH B-DAT
dataset. O
The O
CODAH O
80% O
experi O

tion O
of O
the O
CODAH B-DAT
dataset O
for O
each O
fold, O
but O

CODAH B-DAT

on O
CODAH B-DAT

same O
way O
as O
in O
the O
CODAH B-DAT

uated O
on O
CODAH B-DAT

on O
the O
full O
CODAH B-DAT
dataset O
with O
the O
ques O

for O
each O
fold O
of O
the O
CODAH B-DAT
and O

CODAH B-DAT
settings. O
For O
the O
answer-only O

for O
the O
CODAH B-DAT

higher O
accuracy O
on O
CODAH, B-DAT
so O
we O
replaced O
the O

training O
set O
in O
each O
of O
CODAH B-DAT

CODAH B-DAT
80% O
67.5 O
(1.24) O
62.3 O
(1.11 O

CODAH B-DAT
60% O
64.8 O
(2.24) O
58.6 O
(0.65 O

CODAH B-DAT
40% O
62.3 O
(1.34) O
54.4 O
(1.27 O

CODAH B-DAT
20% O
51.6 O
(1.56) O
45.3 O
(0.76 O

CODAH B-DAT
80% O
69.5 O
(0.34) O
65.5 O
(0.24 O

CODAH B-DAT
60% O
68.6 O
(0.49) O
63.7 O
(0.26 O

CODAH B-DAT
40% O
65.8 O
(0.73) O
61.2 O
(0.30 O

CODAH B-DAT
20% O
63.2 O
(0.70) O
56.9 O
(0.07 O

CODAH B-DAT
(Answer O
only) O
52.2 O
(1.34) O
53.4 O

training O
settings O
when O
tested O
on O
CODAH B-DAT

CODAH B-DAT

performance O
on O
CODAH B-DAT
is O
substantially O
lower O

CODAH B-DAT
setting. O
This O
is O
especially O
signif O

icant O
since O
human O
error O
on O
CODAH B-DAT
is O
4.7%—less O

dataset. O
This O
suggests O
that O
CODAH B-DAT
is O
challenging O

Gururangan O
et O
al., O
2018). O
In O
CODAH, B-DAT
we O
did O
not O

the O
CODAH B-DAT
experiment O
setting, O
we O
can O
interpret O

Our O
experiments O
show O
that O
CODAH B-DAT
forms O
a O
chal O

DAH O
questions O
correctly. O
However, O
CODAH B-DAT
is O

of O
whether O
CODAH B-DAT
remains O
more O
difficult O
than O

ber O
of O
questions O
as O
CODAH, B-DAT
we O
find O
that O
SWAG O

and O
CODAH B-DAT
have O
comparable O
accuracy O
for O
GPT O

curacy O
on O
CODAH B-DAT

). O
This O
suggests O
that O
CODAH B-DAT

more O
data O
on O
both O
the O
CODAH B-DAT

CODAH B-DAT
settings, O
but O
the O
rate O
of O

and O
evaluating O
on O
CODAH B-DAT
forms O
a O
more O
challeng O

ial O
collection O
approach O
to O
grow O
CODAH B-DAT
to O
tens O
of O

equal O
in O
CODAH), B-DAT
in O
order O
to O
disincentivize O
ad O

We O
present O
CODAH, B-DAT
a O
commonsense O
question O
an O

experimental O
results O
show O
that O
CODAH B-DAT
ques O

CODAH B-DAT

We O
introduce O
the O
CODAH B-DAT
dataset, O
an O

for O
testing O
common O
sense. O
CODAH B-DAT
forms O
a O

CODAH) B-DAT
for O
commonsense O
question O
answering O

We O
experimentally O
demonstrate O
that O
CODAH B-DAT

accuracy, O
respectively, O
on O
the O
CODAH B-DAT
dataset O
in O

suggests O
that O
answers O
to O
the O
CODAH B-DAT
ques O

NLP O
systems, O
the O
CODAH B-DAT
construction O
procedure O

3 O
The O
CODAH B-DAT
Dataset O

CODAH B-DAT

CODAH B-DAT

CODAH B-DAT

CODAH B-DAT
that O
employ O
different O
types O
of O

on O
the O
CODAH B-DAT
dataset. O
This O
model O
consists O
of O

CODAH B-DAT

CODAH B-DAT
dataset. O
The O
CODAH O
80% O
experi O

tion O
of O
the O
CODAH B-DAT
dataset O
for O
each O
fold, O
but O

CODAH B-DAT

on O
CODAH B-DAT

same O
way O
as O
in O
the O
CODAH B-DAT

uated O
on O
CODAH B-DAT

on O
the O
full O
CODAH B-DAT
dataset O
with O
the O
ques O

for O
each O
fold O
of O
the O
CODAH B-DAT
and O

CODAH B-DAT
settings. O
For O
the O
answer-only O

for O
the O
CODAH B-DAT

higher O
accuracy O
on O
CODAH, B-DAT
so O
we O
replaced O
the O

training O
set O
in O
each O
of O
CODAH B-DAT

CODAH B-DAT
80% O
67.5 O
(1.24) O
62.3 O
(1.11 O

CODAH B-DAT
60% O
64.8 O
(2.24) O
58.6 O
(0.65 O

CODAH B-DAT
40% O
62.3 O
(1.34) O
54.4 O
(1.27 O

CODAH B-DAT
20% O
51.6 O
(1.56) O
45.3 O
(0.76 O

CODAH B-DAT
80% O
69.5 O
(0.34) O
65.5 O
(0.24 O

CODAH B-DAT
60% O
68.6 O
(0.49) O
63.7 O
(0.26 O

CODAH B-DAT
40% O
65.8 O
(0.73) O
61.2 O
(0.30 O

CODAH B-DAT
20% O
63.2 O
(0.70) O
56.9 O
(0.07 O

CODAH B-DAT
(Answer O
only) O
52.2 O
(1.34) O
53.4 O

training O
settings O
when O
tested O
on O
CODAH B-DAT

CODAH B-DAT

performance O
on O
CODAH B-DAT
is O
substantially O
lower O

CODAH B-DAT
setting. O
This O
is O
especially O
signif O

icant O
since O
human O
error O
on O
CODAH B-DAT
is O
4.7%—less O

dataset. O
This O
suggests O
that O
CODAH B-DAT
is O
challenging O

Gururangan O
et O
al., O
2018). O
In O
CODAH, B-DAT
we O
did O
not O

the O
CODAH B-DAT
experiment O
setting, O
we O
can O
interpret O

Our O
experiments O
show O
that O
CODAH B-DAT
forms O
a O
chal O

DAH O
questions O
correctly. O
However, O
CODAH B-DAT
is O

of O
whether O
CODAH B-DAT
remains O
more O
difficult O
than O

ber O
of O
questions O
as O
CODAH, B-DAT
we O
find O
that O
SWAG O

and O
CODAH B-DAT
have O
comparable O
accuracy O
for O
GPT O

curacy O
on O
CODAH B-DAT

). O
This O
suggests O
that O
CODAH B-DAT

more O
data O
on O
both O
the O
CODAH B-DAT

CODAH B-DAT
settings, O
but O
the O
rate O
of O

and O
evaluating O
on O
CODAH B-DAT
forms O
a O
more O
challeng O

ial O
collection O
approach O
to O
grow O
CODAH B-DAT
to O
tens O
of O

equal O
in O
CODAH), B-DAT
in O
order O
to O
disincentivize O
ad O

We O
present O
CODAH, B-DAT
a O
commonsense O
question O
an O

experimental O
results O
show O
that O
CODAH B-DAT
ques O

achieved O
state-of-the-art O
performance O
on O
the O
CommonsenseQA, B-DAT
a O
large-scale O
dataset O
for O
commonsense O

2019). O
In O
a O
typical O
dataset, O
CommonsenseQA B-DAT
(Talmor O
et O
al., O
2019), O
given O

new O
state-of-the-art O
performance2 O
on O
the O
CommonsenseQA B-DAT
dataset. O
Our O
model O
also O
works O

introduce O
our O
setups O
of O
the O
CommonsenseQA B-DAT
dataset O
(Talmor O
et O
al., O
2019 O

The O
CommonsenseQA B-DAT
dataset O
consists O
of O
12,102 O
(v1.11 O

random O
terms/phrases O
for O
sanity O
check. O
CommonsenseQA B-DAT
is O
directly O
gathered O
from O
real O

the O
best O
of O
our O
knowledge, O
CommonsenseQA B-DAT
may O
be O
the O
most O
suitable O

the O
reported O
results O
in O
the O
CommonsenseQA B-DAT

model O
that O
is O
trained O
on O
CommonsenseQA B-DAT
(CSQA) O
by O
directly O
testing O
it O

in O
the O
cur- O
rent O
English O
Wikipedia B-DAT
Infoboxes O
are O
not O
de- O
scribed O

in O
English O
articles O
in O
the O
Wikipedia B-DAT
dump O
of O
April O
1, O
2018 O

Linked O
Open O
Data O
(LOD) O
and O
Wikipedia B-DAT
de- O
rived O
resources O
such O
as O

pairs O
of O
structured O
slots O
from O
Wikipedia B-DAT
infoboxes O
and O
Wikidata O
(Vrandečić O
and O

sentences O
describing O
these O
slots O
in O
Wikipedia B-DAT
articles O
as O
our O
training O
data O

a O
new O
dataset O
based O
on O
Wikipedia B-DAT
dump O
(2018/04/01) O
and O
Wikidata O
(2018/04/12 O

as O
fol- O
lows: O
(1). O
Extract O
Wikipedia B-DAT
pages O
and O
Wiki- O
data O
tables O

less O
than O
3. O
For O
each O
Wikipedia B-DAT
article, O
use O
its O
anchor O
links O

table O
in O
the O
cor- O
responding O
Wikipedia B-DAT
article O
according O
to O
its O
KB O

be O
found O
in O
the O
corresponding O
Wikipedia B-DAT
arti- O
cle. O
(4). O
For O
each O

Wikipedia B-DAT
article, O
remove O
the O
sentences O
which O

2 O
DiDi O
Labs O
and B-DAT
University O
of O
Southern O
California O
knight@isi.edu O

facts O
from O
the O
input O
KB, O
and B-DAT
add O
two O
attention O
mechanisms: O
(i O

association O
between O
a O
slot O
type O
and B-DAT
its O
corresponding O
slot O
value; O
and O

standard B-DAT
metrics O
including O
BLEU, O
METEOR, O
and O
ROUGE, O
we O
propose O
a O
KB O

KB O
from O
the O
generation O
output O
and B-DAT
comparing O
it O
with O
the O
input O

pairs O
of O
struc- O
tured O
KBs O
and B-DAT
their O
corresponding O
natu- O
ral O
language O

Show O
and B-DAT
tell, O
showing O
an O
audience O
something O

and B-DAT
telling O
them O
about O
it, O
is O

of- O
ten O
need O
to O
describe O
and B-DAT

make O
the O
knowledge O
ele- O
ments O
and B-DAT
their O
connections O
easier O
to O
comprehend O

1We O
make O
all O
data O
sets O
and B-DAT
programs O
of O
vari- O
ous O
models O

nication O
between O
health O
care O
providers O
and B-DAT
their O
patients O
and O
among O
health O

of O
Linked O
Open O
Data O
(LOD) O
and B-DAT
Wikipedia O
de- O
rived O
resources O
such O

as O
DBPedia, O
WikiData O
and B-DAT
YAGO O
encourages O
pursuing O
a O
new O

a O
set O
of O
slot O
types O
and B-DAT
their O
values) O
about O
an O
entity O

see O
example O
in O
Table O
1), O
and B-DAT
auto- O
matically O
generate O
a O
natural O

ing O
in O
22 O
matches O
and B-DAT
scoring O
29 O
goals. O
After O
Hapoel O

Jan O
returned O
to O
league O
action O
and B-DAT
joined O
ASA O
Tel O
Aviv O
University O

League, O
Jan O
returned O
to O
Israel O
and B-DAT
signed O
with O
Hapoel O
Tel O
Aviv O

Thailand B-DAT
’s O
(scoring O
one O
goal) O
and O
was O
a O
member O
of O
the O

team O
from O
1999 O
to O
2001 O
and B-DAT
played O
for O
the O
team O
in O

the O
1997 O
and B-DAT
2003 O
seasons O

was O
a O
Forward O
(association O
football) O
and B-DAT
currently O
plays O
for O

F.C.(women) O
and B-DAT
ASA O
Tel O
Aviv O
University O

ASA O
Tel O
Aviv O
University O
and B-DAT
Maccabi O
Holon O

Table O
2: O
Human O
and B-DAT
System O
Generated O
Descriptions O
about O
the O

plates O
and B-DAT
styles O
which O
human O
use O
to O

for, O
face O
of, O
loaned O
to O
and B-DAT
signed O
for. O
Instead O
of O
manually O

structured O
slots O
from O
Wikipedia O
infoboxes O
and B-DAT
Wikidata O
(Vrandečić O
and O
Krötzsch, O
2014 O

) O
and B-DAT
the O
corresponding O
sentences O
describing O
these O

2018) O
considers O
the O
slot O
type O
and B-DAT
slot O
value O
as O
two O
sequences O

and B-DAT
applies O
a O
sequence O
to O
sequence O

contained O
in O
the O
input O
KB, O
and B-DAT
the O
goal O
of O
generation O
is O

knowledge O
elements O
in O
an O
accurate O
and B-DAT
co- O
herent O
way. O
The O
seq2seq O

fails O
to O
capture O
such O
connections O
and B-DAT
tends O
to O
generate O
wrong O
in O

alignment O
between O
a O
slot O
type O
and B-DAT
its O
slot O
value, O
and O
thus O

to O
compute O
slot O
type O
attention O
and B-DAT
slot O
value O
attention O
simultaneously O
and O

state O
of O
the O
en- O
coder, O
and B-DAT
thus O
learn O
what O
to O
attend O

number O
of O
points, O
goals, O
scores O
and B-DAT
games O
participated. O
We O
design O
a O

capture O
correlations O
among O
interdependent O
slots O
and B-DAT
put O
them O
in O
the O
same O

corresponding O
slot O
value O
(e.g., O
Israel), O
and B-DAT
(ri, O
r̂i) O
denotes O
the O
position O

triple O
in O
the O
input O
list O
and B-DAT
consists O
of O
the O
forward O
position O

ri O
and B-DAT
the O
backward O
position O
r̂i O

a O
sequence-to- O
sequence O
based O
approach, O
and B-DAT
incorporate O
a O
slot- O
aware O
attention O

si, O
vi, O
r1, O
r̂1], O
and B-DAT
obtain O
L O
= O
[l1, O
l2 O

ation O
between O
a O
slot O
type O
and B-DAT
its O
slot O
value, O
we O
design O

state O
at O
step O
t. O
si O
and B-DAT
vi O
denote O
the O
embedding O
representations O

of O
slot O
type O
si O
and B-DAT
slot O
value O
vi O
respectively. O
cti O

previous O
de- O
coder O
time O
steps O
and B-DAT
can O
be O
used O
to O
reduce O

compute O
two O
context O
vectors O
L∗s O
and B-DAT
L∗v O
as O
the O
representation O
of O

the O
slot O
types O
and B-DAT
values O
respec- O
tively O

the O
context O
vectors O
L∗s, O
L∗v O
and B-DAT
the O
decoder O
hidden O
state O
h̃t O

described O
together O
with O
22 O
matches O
and B-DAT
29 O
goals. O
Previous O
studies O
(Lin O

new O
table O
position O
based O
self-attention O
and B-DAT
incorporate O
it O
into O
the O
slot-aware O

use O
the O
row O
index O
r O
and B-DAT
the O
reverse O
row O
index O
r O

it O
has O
a O
latent O
in-link O
and B-DAT
an O
out-link O
to O
denote O
where O

atten- O
tion O
applied O
in O
Liu O
and B-DAT
Lapata O
(2018), O
which O
as- O
sumes O

each O
pair O
of O
slots O
i O
and B-DAT
j, O
we O
compute O
the O
attention O

where O
Win,Wout, O
and B-DAT
Wg O
are O
learnable O
parame- O
ters O

i. O
For O
each O
slot O
si O
and B-DAT
value O
vi, O
we O
obtain O
a O

information O
from O
other O
slot O
types O
and B-DAT
their O
values O

tion O
of O
each O
slot O
type O
and B-DAT
value, O
and O
update O
their O
context O

vectors O
L∗t O
and B-DAT
L∗v O
in O
Equation O
1 O
as O

ically O
locate O
particular O
source O
words O
and B-DAT
directly O
copy O
them O
into O
the O

unique O
slot O
value O
vj O
fromαt O
and B-DAT
obtain O
its O
ag- O
gregated O
source O

the O
sig- O
nals O
they O
receive, O
and B-DAT
block O
or O
pass O
on O
informa O

types O
of O
attention O
distribution O
Psource O
and B-DAT
Pvocab, O
we O
compute O
a O
structure-aware O

word O
from O
the O
fixed O
vocabulary O
and B-DAT
copying O
a O
slot O
value O
from O

erated O
token O
at O
time O
t−1, O
and B-DAT
σ O
is O
a O
Sigmoid O
func O

be O
computed O
by O
pgen, O
Pvocab O
and B-DAT
Psource O

3.1 O
Data O
Using O
person O
and B-DAT
animal O
entities O
as O
case O
studies O

based O
on O
Wikipedia O
dump O
(2018/04/01) O
and B-DAT
Wikidata O
(2018/04/12) O
as O
fol- O
lows O

: O
(1). O
Extract O
Wikipedia O
pages O
and B-DAT
Wiki- O
data O
tables O
about O
person O

and B-DAT
animal O
entities, O
and O
align O
them O
according O
to O
their O

to O
locate O
all O
the O
entities O
and B-DAT
determine O
their O
KB O
IDs. O
(3 O

each O
value O
(including O
Number, O
Date) O
and B-DAT
entity O
contained O
in O
the O
table O

according O
to O
its O
KB O
ID, O
and B-DAT
remove O
the O
values O
and O
entities O

sentences O
which O
contain O
no O
values, O
and B-DAT
remove O
sentences O
which O
only O
contain O

whole O
corpus O
of O
ground-truth O
descriptions O
and B-DAT
label O
the O
words O
with O
frequency O

We O
further O
randomly B-DAT
shuffle O
and O
split O
the O
dataset O
into O
training O

80%), O
development O
(10%) O
and B-DAT
test O
(10%) O
subsets O
for O
person O

and B-DAT
animal O
enti- O
ties O
respectively. O
Table O

generation O
task O
more O
challenging, O
practical O
and B-DAT
interesting O

et O
al., O
2002), O
METEOR O
(Denkowski O
and B-DAT
Lavie, O
2014), O
and O
ROUGE O
(Lin O

content O
overlap O
between O
system O
output O
and B-DAT
ground-truth O
and O
also O
check O
whether O

from O
the O
generated O
para- O
graph, O
and B-DAT
compute O
precision, O
recall O
and O
F-score O

a O
pair O
of O
slot O
type O
and B-DAT
its O
slot O
value O
exists O
in O

of O
the O
recon- O
structed O
KB O
and B-DAT
the O
input O
KB, O
it’s O
considered O

one O
or O
multiple O
slot O
types O
and B-DAT
their O
slot O
values O
exist O
in O

both O
of O
the O
reconstructed O
KB O
and B-DAT
the O
input O
KB, O
it’s O
considered O

by O
(Wiseman O
et O
al., O
2017) O
and B-DAT
entity/event O
extraction O
based O
metric O
proposed O

from O
the O
ref- O
erence O
description O
and B-DAT
the O
system O
generation O
out- O
put O

art O
open-domain O
slot O
filling O
(Wu O
and B-DAT
Weld, O
2010; O
Fader O
et O
al O

to O
capture O
ad- O
equacy, O
grammaticality O
and B-DAT
fluency O
of O
the O
gener- O
ated O

metrics O
such O
as O
system O
purpose O
and B-DAT
user O
task O
are O
expensive, O
while O

correlate O
with O
extrinsic O
metrics O
(Gkatzia O
and B-DAT
Ma- O
hamood, O
2015). O
Moreover O
the O

2015). O
We O
concatenate O
slot O
types O
and B-DAT
val- O
ues O
as O
a O
sequence O

F.C. O
...} O
for O
Table O
1, O
and B-DAT
apply O
the O
sequence O
to O

from O
the O
fixed O
vocabu- O
lary O
and B-DAT
copying O
a O
word O
from O
the O

3.5 O
Results O
and B-DAT
Analysis O
Table O
5 O
shows O
the O

t-test O
between O
our O
proposed O
model O
and B-DAT
all O
the O
other O
baselines O
on O

As O
shown O
in O
Table O
6 O
and B-DAT
Table O
7, O
the O
KBs O
re O

Figure O
3 O
and B-DAT
Figure O
4 O
visualize O
the O
attentions O

ages, O
the O
number O
of O
matches O
and B-DAT
goals O
can O
all O
be O
presented O

scribe O
member O
of O
sports O
team, O
and B-DAT
times O
to O
de- O
scribe O
the O

of O
matches O
and B-DAT
17 O
should O
be O
the O
number O

generates O
“he O
made O
33 O
appearances O
and B-DAT
scored O
17 O
goals”. O
In O
addition O

membership O
with O
a O
sports O
team O
and B-DAT
its O
corresponding O
number O
of O
matches O

and B-DAT
games: O
“Bill O
Sampy O
... O
who O

appear O
in O
the O
same O
row O
and B-DAT
the O
same O
sentences, O
and O
thus O

Queensland B-DAT
throughout O
New O
South O
Wales O
and O
Victoria O
to O
Port O
Augusta O
and O

induce O
templates O
and B-DAT
then O
fill O
appropriate O
content O
into O

geli O
et O
al., O
2010; O
Duma O
and B-DAT
Klein, O
2013; O
Kon- O
stas O
and O

guage O
model O
(Belz, O
2008; O
Chen O
and B-DAT
Mooney, O
2008; O
Liang O
et O
al O

Angeli O
et O
al., O
2010; O
Konstas O
and B-DAT
Lapata, O
2012a,b, O
2013a,b; O
Mahapatra O
et O

as O
a O
sequence O
of O
facts O
and B-DAT
generat- O
ing O
one O
sentence O
only O

capture O
the O
dependencies O
among O
facts O
and B-DAT
generate O
a O
para- O
graph O
to O

links O
in O
a O
knowledge O
base O
and B-DAT
use O
them O
to O
generate O
multiple O

task O
requires O
the O
slot O
type O
and B-DAT
slot O
value O
to O
appear O
in O

vectors O
for O
both O
slot O
type O
and B-DAT
slot O
value O
simultaneously. O
To O
deal O

2016; O
See O
et O
al., O
2017) O
and B-DAT
copy O
mechanism O
(Gu O
et O
al O

5 O
Conclusions O
and B-DAT
Future O
Work O

mechanisms O
focusing O
on O
slot O
type O
and B-DAT
ta- O
ble O
position O
advance O
state-of-the-art O

on O
this O
task, O
and B-DAT
provide O
a O
KB O
reconstruction O
F-score O

as O
summarized O
in O
Section O
3.5, O
and B-DAT
tackle O
the O
setting O
where O
mul O

DARPA O
AIDA O
Program O
No. O
FA8750-18-2-0014 O
and B-DAT
U.S. O
ARL O
NS-CTA O
No. O
W911NF-09-2-0053 O

. O
The O
views O
and B-DAT
conclusions O
contained O
in O
this O
document O

are O
those O
of O
the O
authors O
and B-DAT
should O
not O
be O
inter- O
preted O

Government O
is O
authorized O
to O
reproduce O
and B-DAT
distribute O
reprints O
for O
Government O
purposes O

References O
Gabor O
Angeli, O
Percy O
Liang, O
and B-DAT
Dan O
Klein. O
2010. O
A O

Angeli, O
Melvin O
Jose O
Johnson O
Premkumar, O
and B-DAT
Christopher O
D O
Manning. O
2015. O
Leveraging O

the O
Association O
for O
Computational O
Linguistics O
and B-DAT
the O
7th O
International O
Joint O
Conference O

Dzmitry O
Bahdanau, O
Kyunghyun O
Cho, O
and B-DAT
Yoshua O
Ben- O
gio. O
2015. O
Neural O

by O
jointly O
learning O
to O
align O
and B-DAT
translate. O
In O
International O
Con- O
ference O

Knight, O
Philipp O
Koehn, O
Martha O
Palmer, O
and B-DAT
Nathan O
Schneider. O
2013. O
Abstract O
meaning O

7th O
Linguis- O
tic O
Annotation O
Workshop O
and B-DAT
Interoperability O
with O
Discourse O

Nikita O
Bhutani, O
HV O
Jagadish, O
and B-DAT
Dragomir O
Radev. O
2016. O
Nested O
propositions O

Nadjet O
Bouayad-Agha, O
Gerard O
Casamayor, O
and B-DAT
Leo O
Wanner. O
2013. O
Natural O
language O

J O
Cawsey, O
Bonnie O
L O
Webber, O
and B-DAT
Ray O
B O
Jones. O
1997. O
Natural O

David O
L O
Chen O
and B-DAT
Raymond O
J O
Mooney. O
2008. O
Learn O

Andrew O
Chisholm, O
Will O
Radford, O
and B-DAT
Ben O
Hachey. O
2017. O
Learning O
to O

Bahdanau, O
Fethi O
Bougares, O
Holger O
Schwenk, O
and B-DAT
Yoshua O
Bengio. O
2014. O
Learning O
phrase O

Denil, O
Loris O
Bazzani, O
Hugo O
Larochelle, O
and B-DAT
Nando O
de O
Freitas. O
2012. O
Learning O

Michael O
Denkowski O
and B-DAT
Alon O
Lavie. O
2014. O
Meteor O
universal O

Daniel O
Duma O
and B-DAT
Ewan O
Klein. O
2013. O
Generating O
nat O

Anthony O
Fader, O
Stephen O
Soderland, B-DAT
and O
Oren O
Etzioni. O
2011. O
Identifying O
relations O

Chris O
Dyer, O
Noah O
A O
Smith, O
and B-DAT
Jaime O
Carbonell. O
2016a. O
Generation O
from O

Chris O
Dyer, O
Noah O
A. O
Smith, O
and B-DAT
Jaime O
Carbonell. O
2016b. O
Generation O
from O

Gardent, O
Anastasia O
Shimorina, O
Shashi O
Narayan, O
and B-DAT
Laura O
Perez-Beltrachini. O
2017a. O
Creating O
train O

Gardent, O
Anastasia O
Shimorina, O
Shashi O
Narayan, O
and B-DAT
Laura O
Perez-Beltrachini. O
2017b. O
Creating O
train O

Dimitra O
Gkatzia O
and B-DAT
Saad O
Mahamood. O
2015. O
A O
snap O

Gu, O
Zhengdong O
Lu, O
Hang O
Li, O
and B-DAT
Victor O
O.K. O
Li. O
2016. O
Incorporating O

Ahn, O
Ramesh O
Nallapati, O
Bowen O
Zhou, O
and B-DAT
Yoshua O
Bengio. O
2016. O
Pointing O
the O

Chonghai O
Hu, O
Weike O
Pan, O
and B-DAT
James O
T. O
Kwok. O
2009. O
Accelerated O

methods O
for O
stochastic O
opti- O
mization O
and B-DAT
online O
learning. O
In O
Advances O
in O

Gravier, O
Frédérique O
Laforest, O
Jonathon O
Hare, O
and B-DAT
Elena O
Simperl. O
2018a. O
Learn- O
ing O

Gravier, O
Frédérique O
Laforest, O
Jonathon O
Hare, O
and B-DAT
Elena O
Simperl. O
2018b. O
Mind O
the O

Chloé O
Kiddon, O
Luke O
Zettlemoyer, O
and B-DAT
Yejin O
Choi. O
2016. O
Globally O
coherent O

Kim, O
Carl O
Denton, O
Luong O
Hoang, O
and B-DAT
Alexan- O
der O
M O
Rush. O
2017 O

Ioannis O
Konstas O
and B-DAT
Mirella O
Lapata. O
2012a. O
Concept- O
to-text O

Ioannis O
Konstas O
and B-DAT
Mirella O
Lapata. O
2012b. O
Unsuper- O
vised O

Ioannis O
Konstas O
and B-DAT
Mirella O
Lapata. O
2013a. O
A O
global O

Ioannis O
Konstas O
and B-DAT
Mirella O
Lapata. O
2013b. O
Inducing O
document O

Rémi O
Lebret, O
David O
Grangier, O
and B-DAT
Michael O
Auli. O
2016. O
Neural O
text O

Percy O
Liang, O
Michael O
I O
Jordan, O
and B-DAT
Dan O
Klein. O
2009. O
Learning O
semantic O

Annual O
Meeting O
of O
the O
ACL O
and B-DAT
the O
4th O
In- O
ternational O
Joint O

Yu, O
Bing O
Xiang, O
Bowen O
Zhou, O
and B-DAT
Yoshua O
Bengio. O
2017. O
A O
structured O

Wang, O
Lei O
Sha, O
Baobao O
Chang, O
and B-DAT
Zhifang O
Sui. O
2018. O
Table-to-text O
generation O

Yang O
Liu O
and B-DAT
Mirella O
Lapata. O
2018. O
Learning O
struc O

Whitehead, O
Lifu O
Huang, O
Heng O
Ji, O
and B-DAT
Shih-Fu O
Chang. O
2018. O
Entity-aware O
image O

Sutskever, O
Quoc O
Le, O
Oriol O
Vinyals, O
and B-DAT
Wojciech O
Zaremba. O
2015. O
Addressing O
the O

Associ- O
ation O
for O
Computational O
Linguistics O
and B-DAT
the O
7th O
In- O
ternational O
Joint O

Xu, O
Houfeng O
Wang, O
Wenjie O
Li, O
and B-DAT
Qi O
Su. O
2017. O
Improving O
semantic O

Joy O
Mahapatra, O
Sudip O
Kumar O
Naskar, O
and B-DAT
Sivaji O
Bandyopadhyay. O
2016. O
Statistical O
natural O

Hongyuan O
Mei, O
Mohit O
Bansal, O
and B-DAT
Matthew O
R. O
Walter. O
2016. O
What O

to O
talk O
about O
and B-DAT
how? O
selective O
gener- O
ation O
using O

Min, O
Shuming O
Shi, O
Ralph O
Grishman, O
and B-DAT
Chin- O
Yew O
Lin. O
2012. O
Ensemble O

Methods O
in O
Natural O
Language O
Processing O
and B-DAT
Computational O
Natural O
Language O
Learning O

Papineni, O
Salim O
Roukos, O
Todd O
Ward, O
and B-DAT
Wei- O
Jing O
Zhu. O
2002. O
Bleu O

Nima O
Pourdamghani, O
Kevin O
Knight, O
and B-DAT
Ulf O
Her- O
mjakob. O
2016. O
Generating O

Abigail O
See, O
Peter O
J. O
Liu, O
and B-DAT
Christopher O
D. O
Manning. O
2017. O
Get O

Poupart, O
Sujian O
Li, O
Baobao O
Chang, O
and B-DAT
Zhifang O
Sui. O
2018. O
Order- O
planning O

Long, O
Jing O
Jiang, O
Shirui O
Pan, O
and B-DAT
Chengqi O
Zhang. O
2018a. O
Disan: O
Di O

Zhou, O
Guodong O
Long, O
Jing O
Jiang, O
and B-DAT
Chengqi O
Zhang. O
2018b. O
Bi-directional O
block O

self- O
attention O
for O
fast O
and B-DAT
memory-efficient O
sequence O
modeling. O
In O
International O

Song, O
Yue O
Zhang, O
Zhiguo O
Wang, O
and B-DAT
Daniel O
Gildea. O
2018. O
A O
graph-to-sequence O

Ilya O
Sutskever, O
James O
Martens, O
and B-DAT
Geoffrey O
E O
Hin- O
ton. O
2011 O

Aidan O
N O
Gomez, O
Łukasz O
Kaiser, O
and B-DAT
Illia O
Polosukhin. O
2017. O
Attention O
is O

Oriol O
Vinyals, O
Meire O
Fortunato, O
and B-DAT
Navdeep O
Jaitly. O
2015. O
Pointer O
networks O

D. O
D. O
Lee, O
M. O
Sugiyama, O
and B-DAT
R. O
Garnett, O
editors, O
Advances O
in O

Denny O
Vrandečić B-DAT
and O
Markus O
Krötzsch. O
2014. O
Wiki- O
data O

Whitehead, O
Boliang O
Zhang, O
Heng O
Ji, O
and B-DAT
Kevin O
Knight. O
2018. O
Paper O
abstract O

Pei- O
Hao O
Su, O
David O
Vandyke, B-DAT
and O
Steve O
Young. O
2015. O
Semantically O
conditioned O

Ji, O
Mohit O
Bansal, O
Shih-Fu O
Chang, O
and B-DAT
Clare O
Voss. O
2018. O
Incorporating O
back O

Sam O
Wiseman, O
Stuart O
Shieber, O
and B-DAT
Alexander O
Rush. O
2017. O
Challenges O
in O

Fei O
Wu O
and B-DAT
Daniel O
S. O
Weld. O
2010. O
Open O

Kim, O
Kevin O
Quinn, O
Randy B-DAT
Goebel, O
and O
Denilson O
Barbosa. O
2013. O
Open O
information O

Dian O
Yu, O
Lifu O
Huang, O
and B-DAT
Heng O
Ji. O
2017. O
Open O
rela O

- O
tion O
extraction O
and B-DAT
grounding. O
In O
Proceedings O
of O
the O

1.9 O
16.8 O
8.0 O
70.9 O
4.2 O
Animal B-DAT
6,216 O
30 O
12 O
1.3 O
17.1 O

Model O
Person O
Animal B-DAT
BLEU O
METEOR O
ROUGE O
BLEU O
METEOR O

Model O
Person O
Animal B-DAT
P O
R O
F1 O
P O
R O

Model O
Person O
Animal B-DAT
P O
R O
F1 O
P O
R O

Person B-DAT
100,000 O
109 O
76 O
1.9 O
16.8 O

Model O
Person B-DAT
Animal O
BLEU O
METEOR O
ROUGE O
BLEU O

Model O
Person B-DAT
Animal O
P O
R O
F1 O
P O

Model O
Person B-DAT
Animal O
P O
R O
F1 O
P O

BERT O
Baseline O
for O
the O
Natural O
Questions B-DAT

new O
baseline O
for O
the O
Natural O
Questions B-DAT
(Kwiatkowski O
et O
al., O
2019). O
Our O

We O
hypothesize O
that O
the O
Natural O
Questions B-DAT
(NQ) O
(Kwiatkowski O
et O
al., O
2019 O

BERT-based O
model O
for O
the O
Natural O
Questions B-DAT

The O
Natural O
Questions B-DAT
(NQ) O
(Kwiatkowski O
et O
al., O
2019 O

for O
the O
newly O
released O
Natural O
Questions B-DAT
(Kwiatkowski O
et O
al., O
2019 O

ter O
models O
for O
the O
Natural O
Questions B-DAT
and O
for O
other O
question O
answering O

A O
BERT O
Baseline O
for O
the O
Natural B-DAT
Questions O

a O
new O
baseline O
for O
the O
Natural B-DAT
Questions O
(Kwiatkowski O
et O
al., O
2019 O

We O
hypothesize O
that O
the O
Natural B-DAT
Questions O
(NQ) O
(Kwiatkowski O
et O
al O

a O
BERT-based O
model O
for O
the O
Natural B-DAT
Questions. O
BERT O
performs O
very O
well O

The O
Natural B-DAT
Questions O
(NQ) O
(Kwiatkowski O
et O
al O

baseline O
for O
the O
newly O
released O
Natural B-DAT
Questions O
(Kwiatkowski O
et O
al., O
2019 O

bet- O
ter O
models O
for O
the O
Natural B-DAT
Questions O
and O
for O
other O
question O

Le, O
and O
Slav O
Petrov. O
2019. O
Natural B-DAT
questions: O
a O
benchmark O
for O
question O

A O
BERT O
Baseline O
for O
the O
Natural B-DAT
Questions I-DAT

a O
new O
baseline O
for O
the O
Natural B-DAT
Questions I-DAT
(Kwiatkowski O
et O
al., O
2019). O
Our O

We O
hypothesize O
that O
the O
Natural B-DAT
Questions I-DAT
(NQ) O
(Kwiatkowski O
et O
al., O
2019 O

a O
BERT-based O
model O
for O
the O
Natural B-DAT
Questions I-DAT

The O
Natural B-DAT
Questions I-DAT
(NQ) O
(Kwiatkowski O
et O
al., O
2019 O

baseline O
for O
the O
newly O
released O
Natural B-DAT
Questions I-DAT
(Kwiatkowski O
et O
al., O
2019 O

bet- O
ter O
models O
for O
the O
Natural B-DAT
Questions I-DAT
and O
for O
other O
question O
answering O

algorithm O
has O
been O
evaluated O
on O
BioEye B-DAT
2015 O
database O
and O
found O
to O

in O
RAN O
30min O
dataset O
of O
BioEye B-DAT
2015 O
database O
[22] O
containing O
153 O

in O
the O
public O
results O
of O
BioEye B-DAT
2015 O
competition) O
show O
the O
stability O

of O
the O
development O
phase O
of O
BioEye B-DAT
2015 O
[22] O
competition. O
Data O
recorded O

about O
the O
stimulus O
types O
in O
BioEye2015 B-DAT
database O
are O
given O
below O

The O
evaluation O
set O
in O
the O
BioEye2015 B-DAT
dataset O
is O
unlabeled. O
However, O
we O

was O
ranked O
first O
in O
the O
BioEye B-DAT
2015 O
[22] O
competition O

to O
thank O
the O
organizers O
of O
BioEye B-DAT
2015 O
competition O
for O
providing O
the O

bioeye B-DAT

bioeye B-DAT

bioeye B-DAT

CliCR B-DAT

CliCR B-DAT

CliCR B-DAT
(this O
work O

CliCR B-DAT
QA4MRE O
SQuAD O
Who−did−what O

a O
4-gram O
Kneser-Ney O
model O
on O
CliCR B-DAT
training O
data O
(with O
multi-word O
entities O

on O
a O
combination O
of O
the O
CliCR B-DAT
training O
corpus O
and O
PubMed O
abstracts O

clicr B-DAT

CliCR B-DAT

CliCR B-DAT

CliCR B-DAT
(this O
work O

CliCR B-DAT
QA4MRE O
SQuAD O
Who−did−what O

a O
4-gram O
Kneser-Ney O
model O
on O
CliCR B-DAT
training O
data O
(with O
multi-word O
entities O

on O
a O
combination O
of O
the O
CliCR B-DAT
training O
corpus O
and O
PubMed O
abstracts O

clicr B-DAT

achieves O
19.88% O
WER O
on O
CMUDict B-DAT
dataset, O
outperforming O
the O

best O
results O
on O
the O
public O
CMUDict B-DAT
dataset O

We O
conduct O
experiments O
on O
CMUDict B-DAT
0.7b O
and O
our O
internal O

art O
result O
on O
the O
public O
CMUDict B-DAT
0.7b O
dataset O

one O
is O
the O
publicly O
available O
CMUDict B-DAT
0.7b O
and O
the O
other O
one O

internal O
dataset. O
For O
the O
public O
CMUDict B-DAT
0.7b O
dataset O

CMUDict B-DAT

CMUDict B-DAT
0.7b O
dataset, O
as O
shown O
in O

on O
CMUDict B-DAT
0.7b O
dataset5. O
It O
can O
be O

works O
on O
CMUDict B-DAT
0.7b O
dataset O

iments O
on O
the O
publicly O
available O
CMUDict B-DAT
0.7b O
dataset O
and O
our O

We O
conduct O
experiments O
on O
CMUDict O
0.7b B-DAT
and O
our O
internal O

result O
on O
the O
public O
CMUDict O
0.7b B-DAT
dataset O

is O
the O
publicly O
available O
CMUDict O
0.7b B-DAT
and O
the O
other O
one O

dataset. O
For O
the O
public O
CMUDict O
0.7b B-DAT
dataset O

CMUDict O
0.7b B-DAT
dataset, O
as O
shown O
in O
Table O

on O
CMUDict O
0.7b B-DAT
dataset5. O
It O
can O
be O
seen O

works O
on O
CMUDict O
0.7b B-DAT
dataset O

on O
the O
publicly O
available O
CMUDict O
0.7b B-DAT
dataset O
and O
our O

We O
conduct O
experiments O
on O
CMUDict B-DAT
0.7b I-DAT
and O
our O
internal O

art O
result O
on O
the O
public O
CMUDict B-DAT
0.7b I-DAT
dataset O

one O
is O
the O
publicly O
available O
CMUDict B-DAT
0.7b I-DAT
and O
the O
other O
one O

internal O
dataset. O
For O
the O
public O
CMUDict B-DAT
0.7b I-DAT
dataset O

CMUDict B-DAT
0.7b I-DAT
dataset, O
as O
shown O
in O
Table O

on O
CMUDict B-DAT
0.7b I-DAT
dataset5. O
It O
can O
be O
seen O

works O
on O
CMUDict B-DAT
0.7b I-DAT
dataset O

iments O
on O
the O
publicly O
available O
CMUDict B-DAT
0.7b I-DAT
dataset O
and O
our O

CP B-DAT
dataset, O
achiev- O
ing O
a O
new O

CP) B-DAT
[1] O
dataset. O
The O
strong O
language O

CP B-DAT
dataset[1] O
and O
achieve O
a O
new O

CP B-DAT
where O
the O
distribution O
of O
the O

CP B-DAT

CP B-DAT
in O
order O
to O
demonstrate O
that O

CP B-DAT
training O
and O
test O
set. O
We O

CP B-DAT
dataset. O
To O
extract O
the O
potential O

CP, B-DAT
bedrooms O
are O
the O
most O
common O

CP B-DAT
v2 O
test O
VQA O
v2 O
val O

CP B-DAT
test O
and O
VQA O
v2 O
validation O

CP B-DAT
training O
set O
using O
standard O
VQA O

CP B-DAT
dataset O
achieves O
80.4%, O
and O
increasing O

CP B-DAT
(Visual O
Question O
Answering O
with O
Changing O

CP B-DAT
v2 O
test O
All O
Yes/No O
Num O

CP B-DAT
test O
data. O
The O
first O
half O

CP B-DAT
and O
VQA O
v2 O
dataset O

CP B-DAT
test O
data O
using O
VQA-HAT O
visual O

CP B-DAT
test O
set O
using O
visual O
explanations O

CP B-DAT
data. O
After O
the O
self-critical O
training O

CP B-DAT
dataset O
by O
a O
clear O
margin O

CP B-DAT
and O
VQA O
v2 O
dataset O

Visual O
Question O
Answering O
(VQA) B-DAT
deep-learning O
systems O
tend O
to O
capture O

evaluate O
our O
approach O
on O
the O
VQA B-DAT
generalization O
task O
using O
the O
VQA-CP O

Recently, O
Visual O
Question O
Answering O
(VQA) B-DAT
[4] O
has O
emerged O
as O
a O

and O
visual O
content. O
The O
state-of-the-art O
VQA B-DAT
systems O
[8, O
2, O
1, O
3 O

Question O
Answering O
under O
Changing O
Priors O
(VQA B-DAT

A O
number O
of O
recent O
VQA B-DAT
systems O
[28, O
35, O
25, O
20 O

Figure O
1, O
we O
ask O
the O
VQA B-DAT
system O
“What O
is O
the O
man O

the O
prediction O
even O
though O
the O
VQA B-DAT
system O
has O
the O
right O
reasons O

banana” O
using O
the O
baseline O
UpDn O
VQA B-DAT
system O
and O
Figure O
(f), O
(g O

dog” O
and O
“banana” O
using O
the O
VQA B-DAT
system O
after O
being O
trained O
with O

only O
requires O
that O
the O
base O
VQA B-DAT
system O
be O
differentiable O
to O
the O

also O
explored O
using O
human O
textual O
VQA B-DAT
explanations O
from O
the O
VQA-X O
[18 O

additional O
human O
annotation O
of O
the O
VQA B-DAT
training O
data O

our O
approach O
using O
the O
UpDn O
VQA B-DAT
system O
[2] O
on O
the O
VQA-CP O

i.e. O
49.5% O
overall O
score O
with O
VQA B-DAT

textual O
explanations, O
49.1 O
% O
with O
VQA B-DAT

2.1 O
Human O
Explanations O
for O
VQA B-DAT

available O
for O
the O
most O
popular O
VQA B-DAT
dataset O
[4], O
i.e. O
visual O
and O

textual O
explanations. O
The O
VQA B-DAT

correctly. O
Alternatively, O
[18] O
present O
the O
VQA B-DAT

2.2 O
Language O
Priors O
in O
VQA B-DAT

Language O
priors O
[1, O
9] O
in O
VQA B-DAT
refer O
to O
the O
fact O
that O

three. O
These O
language O
priors O
allow O
VQA B-DAT
systems O
to O
take O
a O
shortcut O

order O
to O
prevent O
this O
circumstance, O
VQA B-DAT
v2 O
[4] O
balances O
the O
answer O

a O
diagnostic O
reconfiguration O
of O
the O
VQA B-DAT
v2 O
dataset O
called O
VQA-CP O
where O

the O
test O
set. O
Most O
state-of-the-art O
VQA B-DAT
systems O
are O
found O
to O
highly O

a O
catastrophic O
performance O
drop O
on O
VQA B-DAT

We O
evaluate O
our O
approach O
on O
VQA B-DAT

2.3 O
Improving O
VQA B-DAT
using O
Human O
Explanations O

A O
desired O
property O
for O
VQA B-DAT
systems O
is O
to O
not O
only O

for O
the O
right O
reasons. O
The O
VQA B-DAT
systems O
that O
address O
this O
issue O

our O
base O
Bottom-up O
Top-down O
(UpDn) O
VQA B-DAT
system1[2]. O
Then, O
we O
describe O
the O

3.1 O
Bottom-Up O
Top-Down O
VQA B-DAT

A O
large O
number O
of O
previous O
VQA B-DAT
systems O
[8, O
5, O
21] O
utilize O

32, O
26] O
and O
significantly O
improves O
VQA B-DAT
performance O

a|V, O
Q) O
= O
f(V,q). O
The O
VQA B-DAT
task O
is O
framed O
as O
a O

1Core O
building O
blocks O
in O
the O
VQA B-DAT
challenge O
wining O
entries O
in O
the O

top O
block, O
the O
base O
UpDn O
VQA B-DAT
system O
first O
detects O
a O
set O

criticize O
the O
sensitivity O
till O
the O
VQA B-DAT
system O
answers O
the O
question O
correctly O

Following O
HINT[25], O
we O
use O
the O
VQA B-DAT

to O
approximately O
9% O
of O
the O
VQA B-DAT

to O
5% O
of O
the O
entire O
VQA B-DAT

our O
approach. O
Besides O
the O
UpDn O
VQA B-DAT
system O
(left O
top O
block), O
our O

influential O
objects. O
For O
example, O
in O
VQA B-DAT

self-critical O
objective O
to O
criticize O
the O
VQA B-DAT
systems’ O
incorrect O
but O
competitive O
decisions O

of O
our O
self-critical O
approach O
to O
VQA B-DAT

Expl. O
VQA-CP B-DAT
v2 O
test O
VQA O
v2 O
val O
All O
Yes/No O
Num O

78.9 O
41.4 O
54.3 O
UpDn+SCR O
(ours) O
VQA B-DAT

Comparison O
of O
the O
results O
on O
VQA B-DAT

-CP O
test O
and O
VQA B-DAT
v2 O
validation O
dataset O
with O
the O

systems. O
The O
first O
half O
includes O
VQA B-DAT
systems O
without O
human O
explanations O
during O

training O
and O
the O
VQA B-DAT
systems O
in O
the O
second O
half O

of O
explanations O
for O
training O
the O
VQA B-DAT
systems. O
Accuracies O
in O
percentages O

first O
pre-train O
our O
base O
UpDn O
VQA B-DAT
system O
on O
the O
VQA-CP O
training O

set O
using O
standard O
VQA B-DAT
loss O
Lvqa O
(binary O
cross O
entropy O

units O
in O
the O
base O
UpDn O
VQA B-DAT
system. O
After O
that, O
we O
finetune O

the O
VQA B-DAT
system O
with O
the O
joint O
loss O

the O
pre-trained O
system O
on O
the O
VQA B-DAT

conduct O
experiments O
mainly O
on O
the O
VQA B-DAT

present O
experimental O
results O
on O
the O
VQA B-DAT
v2 O
validation O
set O
for O
completeness O

We O
compare O
our O
self-critical O
system’s O
VQA B-DAT
performance O
with O
the O
start-of-the-art O
systems O

Expl. O
Linfl O
Lcrit O
VQA B-DAT

various O
self-critical O
loss O
weights O
on O
VQA B-DAT

The O
first O
half O
reports O
the O
VQA B-DAT
systems O
without O
human O
explanations O
during O

training O
and O
the O
VQA B-DAT
systems O
in O
the O
second O
half O

of O
explanations O
for O
training O
the O
VQA B-DAT
systems. O
The O
“Lcrit” O
column O
shows O

5.1 O
VQA B-DAT
Performance O
on O
VQA O

-CP O
and O
VQA B-DAT
v2 O
dataset O

the O
quantitative O
results O
of O
the O
VQA B-DAT
generalization O
task O
and O
compare O
our O

system’s O
performance O
on O
the O
balanced O
VQA B-DAT
v2 O
validation O
set O
for O
completeness O

amount O
of O
human O
visual O
explanation O
(VQA B-DAT

number O
of O
explanations O
compared O
to O
VQA B-DAT

these O
textual O
explanations O
help O
the O
VQA B-DAT
system O
to O
perform O
better O
by O

our O
self-critical O
objective O
helps O
the O
VQA B-DAT
system O
especially O
in O
the O
’Yes/No O

two O
types, O
and O
requires O
the O
VQA B-DAT
system O
to O
jointly O
consider O
all O

In O
VQA B-DAT
v2 O
test O
dataset, O
the O
VQA O

the O
self-critical O
loss O
on O
the O
VQA B-DAT

-CP O
test O
data O
using O
VQA B-DAT

self-critical O
approach O
helps O
the O
UpDn O
VQA B-DAT
system O
to O
improve O
6.3% O
on O

state-of-the-art O
score O
(49.2%) O
on O
the O
VQA B-DAT

loss O
Lcrit O
and O
consistently O
improves O
VQA B-DAT
performance O
for O
a O
wide O
range O

UpDn O
+ O
HAT O
UpDn O
+ O
VQA B-DAT

otherwise. O
For O
the O
original O
UpDn O
VQA B-DAT
system, O
we O
observe O
a O
false O

test O
QA O
pairs O
in O
the O
VQA B-DAT

reduces O
to O
20.4% O
using O
the O
VQA B-DAT

explanations O
and O
to O
19.6% O
using O
VQA B-DAT

is O
a O
common O
problem O
in O
VQA B-DAT
systems O
and O
indicates O
the O
utility O

have O
explored O
how O
to O
improve O
VQA B-DAT
performance O
by O
criticizing O
the O
sensitivity O

Our O
“self O
critical” O
approach O
helps O
VQA B-DAT
systems O
generalize O
to O
test O
data O

Our O
approach O
outperforms O
the O
state-of-the-art O
VQA B-DAT
systems O
on O
the O
VQA-CP O
dataset O

explanations O
together O
to O
better O
train O
VQA B-DAT
systems. O
This O
is O
difficult O
because O

Attention O
for O
Image O
Captioning O
and O
VQA B-DAT

Lawrence O
Zitnick, O
and O
D. O
Parikh. O
VQA B-DAT

2.1 O
Human O
Explanations O
for O
VQA B-DAT

2.2 O
Language O
Priors O
in O
VQA B-DAT

2.3 O
Improving O
VQA B-DAT
using O
Human O
Explanations O

3.1 O
Bottom-Up O
Top-Down O
VQA B-DAT

5.1 O
VQA B-DAT
Performance O
on O
VQA O

-CP O
and O
VQA B-DAT
v2 O
dataset O

VQA O
generalization O
task O
using O
the O
VQA-CP B-DAT
dataset, O
achiev- O
ing O
a O
new O

Question O
Answering O
under O
Changing O
Priors O
(VQA-CP) B-DAT
[1] O
dataset. O
The O
strong O
language O

VQA O
system O
[2] O
on O
the O
VQA-CP B-DAT
dataset[1] O
and O
achieve O
a O
new O

the O
VQA O
v2 O
dataset O
called O
VQA-CP B-DAT
where O
the O
distribution O
of O
the O

a O
catastrophic O
performance O
drop O
on O
VQA-CP B-DAT

We O
evaluate O
our O
approach O
on O
VQA-CP B-DAT
in O
order O
to O
demonstrate O
that O

to O
approximately O
9% O
of O
the O
VQA-CP B-DAT
training O
and O
test O
set. O
We O

to O
5% O
of O
the O
entire O
VQA-CP B-DAT
dataset. O
To O
extract O
the O
potential O

influential O
objects. O
For O
example, O
in O
VQA-CP, B-DAT
bedrooms O
are O
the O
most O
common O

Expl. O
VQA-CP B-DAT
v2 O
test O
VQA O
v2 O
val O

Comparison O
of O
the O
results O
on O
VQA-CP B-DAT
test O
and O
VQA O
v2 O
validation O

UpDn O
VQA O
system O
on O
the O
VQA-CP B-DAT
training O
set O
using O
standard O
VQA O

the O
pre-trained O
system O
on O
the O
VQA-CP B-DAT
dataset O
achieves O
80.4%, O
and O
increasing O

conduct O
experiments O
mainly O
on O
the O
VQA-CP B-DAT
(Visual O
Question O
Answering O
with O
Changing O

Expl. O
Linfl O
Lcrit O
VQA-CP B-DAT
v2 O
test O
All O
Yes/No O
Num O

various O
self-critical O
loss O
weights O
on O
VQA-CP B-DAT
test O
data. O
The O
first O
half O

5.1 O
VQA O
Performance O
on O
VQA-CP B-DAT
and O
VQA O
v2 O
dataset O

the O
self-critical O
loss O
on O
the O
VQA-CP B-DAT
test O
data O
using O
VQA-HAT O
visual O

state-of-the-art O
score O
(49.2%) O
on O
the O
VQA-CP B-DAT
test O
set O
using O
visual O
explanations O

test O
QA O
pairs O
in O
the O
VQA-CP B-DAT
data. O
After O
the O
self-critical O
training O

state-of-the-art O
VQA O
systems O
on O
the O
VQA-CP B-DAT
dataset O
by O
a O
clear O
margin O

5.1 O
VQA O
Performance O
on O
VQA-CP B-DAT
and O
VQA O
v2 O
dataset O

the O
classification O
of O
the O
500 O
TREC B-DAT
10 O
(Voorhees, O
2001) O
questions. O
Their O

and O
ensure O
that O
the O
500 O
TREC B-DAT
questions, O
which O
consist O
of O
the O

accuracy O
of O
97.2% O
on O
the O
TREC B-DAT
10 O
dataset O
which O
translates O
to O

have O
reported O
results O
on O
the O
TREC B-DAT
10 O
dataset O
in O
Table O
7 O

Voorhees. O
2001. O
Question O
answering O
in O
TREC B-DAT

500 B-DAT

to O
the O
classification O
of O
the O
500 B-DAT
TREC O
10 O
(Voorhees, O
2001) O
questions O

500 B-DAT
questions O
and O
their O
respective O
question O

discovery, O
and O
ensure O
that O
the O
500 B-DAT
TREC O
questions, O
which O
consist O
of O

tagging O
of O
14 O
of O
the O
500 B-DAT
questions O
in O
the O
dataset. O
This O

504 B-DAT

MuCo-3DHP O
and O
MuPoTS-3D B-DAT
datasets. O
These O
are O
the O
3D O

dataset O
[28]. O
The O
test O
set, O
MuPoTS-3D B-DAT
dataset, O
was O
captured O
at O
outdoors O

and O
RootNet O
settings O
on O
the O
MuPoTS-3D B-DAT
dataset O

MuCo-3DHP O
and O
MuPoTS-3D B-DAT
datasets. O
Following O
the O
previous O
protocol O

with O
state-of-the-art O
methods O
on O
the O
MuPoTS-3D B-DAT
dataset. O
∗ O
used O
extra O
synthetic O

state- O
of-the-art O
methods O
on O
the O
MuPoTS-3D B-DAT
dataset. O
All O
groundtruths O
are O
used O

methods O
[25, O
29]. O
MuCo-3DHP O
and O
MuPoTS-3D B-DAT
datasets. O
We O
com O

pose O
estimation O
methods O
on O
the O
MuPoTS-3D B-DAT
dataset O
[29] O
in O
Tables O
5 O

9: O
Sequence-wise O
3DPCKabs O
on O
the O
MuPoTS-3D B-DAT
dataset O

10: O
Joint-wise O
3DPCKabs O
on O
the O
MuPoTS-3D B-DAT
dataset. O
All O
groundtruths O
are O
used O

applying O
our O
method O
on O
the O
MuPoTS-3D B-DAT
dataset O
[29 O

Camera O
Distance-aware O
Top-down O
Approach O
for O
3D B-DAT
Multi-person O
Pose O
Estimation O
from O
a O

Qualitative O
results O
of O
applying O
our O
3D B-DAT
multi-person O
pose O
estimation O
framework O
to O

images. O
Most O
of O
the O
previous O
3D B-DAT
human O
pose O
estimation O
studies O
mainly O

focused O
on O
the O
root-relative O
3D B-DAT
single-person O
pose O
estimation. O
In O
this O

study, O
we O
propose O
a O
general O
3D B-DAT
multi-person O
pose O
estimation O
framework O
that O

factors O
including O
human O
detection O
and O
3D B-DAT
human O
root O
localization O

been O
achieved O
re- O
cently O
in O
3D B-DAT
human O
pose O
estimation, O
most O
of O

distance- O
aware O
top-down O
approach O
for O
3D B-DAT
multi-person O
pose O
esti- O
mation O
from O

consists O
of O
human O
detection, O
absolute O
3D B-DAT
hu- O
man O
root O
localization, O
and O

root-relative O
3D B-DAT
single-person O
pose O
estimation O
modules. O
Our O

comparable O
results O
with O
the O
state-of-the-art O
3D B-DAT
single-person O
pose O
es- O
timation O
models O

information O
and O
significantly O
outperforms O
previous O
3D B-DAT
multi-person O
pose O
es- O
timation O
methods O

The O
goal O
of O
3D B-DAT
human O
pose O
estimation O
is O
to O

or O
multiple O
human O
bodies O
in O
3D B-DAT
space. O
It O
is O
an O
essential O

Most O
of O
the O
previous O
3D B-DAT
human O
pose O
estimation O
meth- O
ods O

body O
is O
fed O
into O
the O
3D B-DAT
pose O
estimation O
module, O
which O
then O

estimates O
the O
3D B-DAT
location O
of O
each O
key- O
point O

49, O
52] O
estimate O
the O
relative O
3D B-DAT
pose O
to O
a O
reference O
point O

human, O
called O
root. O
The O
final O
3D B-DAT
pose O
is O
obtained O
by O
adding O

the O
3D B-DAT
coordinates O
of O
the O
root O
to O

the O
estimated O
root- O
relative O
3D B-DAT
pose. O
Prior O
information O
on O
the O

2D O
cases, O
extending O
them O
to O
3D B-DAT
cases O
is O
nontrivial. O
Note O
that O

for O
the O
estimation O
of O
3D B-DAT
multi-person O
poses, O
we O
need O
to O

propose O
a O
general O
framework O
for O
3D B-DAT
multi-person O
pose O
estimation. O
To O
the O

the O
previous O
human O
detection O
and O
3D B-DAT
human O
pose O
estimation O
methods. O
The O

input O
image. O
Second, O
the O
proposed O
3D B-DAT
human O
root O
localization O
network O
(RootNet O

humans’ O
roots. O
Third, O
a O
root-relative O
3D B-DAT
single-person O
pose O
estimation O
network O
(PoseNet O

) O
estimates O
the O
root-relative O
3D B-DAT
pose O
for O
each O
detected O
human O

that O
our O
approach O
outperforms O
previous O
3D B-DAT
multi-person O
pose O
estimation O
methods O
[29 O

40] O
on O
several O
publicly O
available O
3D B-DAT
single- O
and O
multi-person O
pose O
estima O

the O
bounding O
boxes O
and O
the O
3D B-DAT
location O
of O
the O
roots), O
our O

ble O
performance O
with O
the O
state-of-the-art O
3D B-DAT
single-person O
pose O
estimation O
methods O
that O

conventions O
of O
object O
detection O
and O
3D B-DAT
hu- O
man O
pose O
estimation O
networks O

a O
new O
general O
framework O
for O
3D B-DAT
multi- O
person O
pose O
estimation O
from O

the O
previous O
human O
detection O
and O
3D B-DAT
human O
pose O
estimation O
models O

For O
this, O
we O
propose O
a O
3D B-DAT
human O
root O
localization O
network O
(RootNet O

it O
easy O
to O
extend O
the O
3D B-DAT
single-person O
pose O
estimation O
techniques O
to O

the O
abso- O
lute O
3D B-DAT
pose O
estimation O
of O
multiple O
persons O

our O
method O
significantly O
outperforms O
previous O
3D B-DAT
multi-person O
pose O
estimation O
methods O
on O

comparable O
performance O
with O
the O
state-of-the-art O
3D B-DAT
single-person O
pose O
estimation O
methods O
without O

3D B-DAT
single-person O
pose O
estimation. O
Current O
3D O
single- O
person O
pose O
estimation O
methods O

ap- O
proach O
directly O
localizes O
the O
3D B-DAT
body O
keypoints O
from O
the O
input O

and O
lift O
them O
to O
a O
3D B-DAT
space O

3D B-DAT
root-relative O
pose O

x O
3D B-DAT
absolute O
human O
root O

3D B-DAT
multi-person O
pose O

of O
the O
proposed O
framework O
for O
3D B-DAT
multi-person O
pose O
estimation O
from O
a O

shaped O
network O
to O
estimate O
a O
3D B-DAT
heatmap O
for O
each O
joint. O
They O

soft-argmax O
operation O
to O
obtain O
the O
3D B-DAT
coor- O
dinates O
of O
body O
joints O

utilized O
it O
to O
regress O
the O
3D B-DAT
pose. O
Martinez O
et O
al. O
[26 O

network O
that O
directly O
regresses O
the O
3D B-DAT
coor- O
dinates O
of O
body O
joints O

adversarial O
loss O
to O
handle O
the O
3D B-DAT
human O
pose O
estimation O
in O
the O

3D B-DAT
multi-person O
pose O
estimation. O
Few O
studies O

have O
been O
conducted O
on O
3D B-DAT
multi-person O
pose O
estimation O
from O
a O

of O
2D O
and O
root- O
relative O
3D B-DAT
pose. O
It O
is O
generated O
by O

3D B-DAT
human O
root O
localization O
in O
3D O
multi-person O
pose O
estimation. O
Rogez O
et O

image O
coordinate O
space O
and O
the O
3D B-DAT
pose O
in O
the O
camera- O
centered O

space O
simultaneously. O
They O
obtained O
the O
3D B-DAT
location O
of O
the O
human O
root O

estimated O
2D O
pose O
and O
projected O
3D B-DAT
pose, O
similar O
to O
what O
Mehta O

cannot O
be O
generalized O
to O
other O
3D B-DAT
human O
pose O
esti- O
mation O
methods O

requires O
both O
the O
2D O
and O
3D B-DAT
esti- O
mations. O
For O
example, O
many O

PoseNet, O
which O
estimates O
the O
root-relative O
3D B-DAT
pose O
Prelj O
= O
(xj O

space. O
Then, O
the O
final O
absolute O
3D B-DAT
pose O
{Pabsj O
}Jj=1 O
is O
obtained O

of O
the O
human O
root O
in O
3D B-DAT
human O
pose O
estimation O
datasets O
[16 O

The O
Root- O
Net O
estimates O
the O
3D B-DAT
human O
root O
coordinate O

The O
PoseNet O
estimates O
the O
root-relative O
3D B-DAT
pose O
Prelj O
= O
(xj O

map O
to O
pro- O
duce O
the O
3D B-DAT
heatmaps O
for O
each O
joint. O
The O

dataset O
[16] O
is O
the O
largest O
3D B-DAT
single-person O
pose O
benchmark. O
It O
consists O

4 O
camera O
viewpoints. O
The O
groundtruth O
3D B-DAT
poses O
are O
obtained O
using O
a O

of O
the O
estimated O
and O
groundtruth O
3D B-DAT
poses. O
The O
second O
one O
is O

localiza- O
tion O
of O
the O
absolute O
3D B-DAT
human O
root, O
we O
introduce O
the O

MuPoTS-3D B-DAT
datasets. O
These O
are O
the O
3D O
multi-person O
pose O
estimation O
datasets O
proposed O

by O
compositing O
the O
existing O
MPI-INF-3DHP B-DAT
3D O
single-person O
pose O
estimation O
dataset O
[28 O

3D B-DAT
dataset, O
was O
captured O
at O
outdoors O

20 O
real-world O
scenes O
with O
groundtruth O
3D B-DAT
poses O
for O
up O
to O
three O

system. O
For O
evalua- O
tion, O
a O
3D B-DAT
percentage O
of O
correct O
keypoints O
(3DPCKrel O

the O
localization O
of O
the O
absolute O
3D B-DAT
human O
root, O
we O
use O

3D B-DAT
dataset O

the O
average O
precision O
of O
3D B-DAT
human O
root O
location O
(AP O
root25 O

3D B-DAT
datasets. O
Following O
the O
previous O
protocol O

pro- O
posed O
framework O
affects O
the O
3D B-DAT
multi-person O
pose O
estima- O
tion O
accuracy O

the O
accuracy O
of O
the O
final O
3D B-DAT
human O
root O
localization O
and O
3D O

model O
improves O
both O
of O
the O
3D B-DAT
human O
root O
localization O
and O
3D O

a O
large O
impact O
on O
the O
3D B-DAT
multi-person O
pose O
estimation O
accuracy O

how O
the O
performance O
of O
the O
3D B-DAT
human O
root O
localization O
affects O
the O

accuracy O
of O
the O
3D B-DAT
multi-person O
pose O
estimation, O
we O
compare O

3D B-DAT
dataset. O
∗ O
used O
extra O
synthetic O

3D B-DAT
dataset. O
All O
groundtruths O
are O
used O

to O
achieve O
more O
accurate O
absolute O
3D B-DAT
multi-person O
pose O
estimation O
results O

an O
evaluation O
of O
the O
root-relative O
3D B-DAT
human O
pose O
estimation O
highly O
depends O

proposed O
system O
with O
the O
state-of-the-art O
3D B-DAT
human O
pose O
estimation O
meth- O
ods O

tion O
(i.e., O
bounding O
boxes O
or O
3D B-DAT
root O
locations) O
in O
inference O
time O

the O
PoseNet O
using O
the O
groundtruth O
3D B-DAT
root O
location. O
Note O
that O
our O

vious O
3D B-DAT
multi-person O
pose O
estimation O
methods O
[25 O

3D B-DAT
datasets. O
We O
com O

proposed O
system O
with O
the O
state-of-the-art O
3D B-DAT
multi-person O
pose O
estimation O
methods O
on O

3D B-DAT
dataset O
[29] O
in O
Tables O
5 O

our O
proposed O
method O
outperforms O
previous O
3D B-DAT

Table O
2, O
using O
the O
groundtruth O
3D B-DAT
root O
location O
brings O
significant O
3DPCKabs O

a O
clue O
for O
improving O
the O
3D B-DAT
human O
root O
localization O
model O

used O
in O
applications O
other O
than O
3D B-DAT
multi-person O
pose O
estimation. O
For O
example O

, O
recent O
methods O
for O
3D B-DAT
human O
mesh O
model O
reconstruction O
[2 O

, O
18, O
19] O
reconstruct O
full O
3D B-DAT
mesh O
model O
from O
a O
single O

utilized O
2D O
multi-view O
input O
for O
3D B-DAT
multi- O
person O
mesh O
model O
reconstruction O

tion O
model O
[2, O
18, O
19], O
3D B-DAT
multi-person O
mesh O
model O
recon- O
struction O

can O
be O
applied O
to O
many O
3D B-DAT
instance- O
aware O
vision O
tasks O
which O

novel O
and O
general O
framework O
for O
3D B-DAT
multi O

framework O
consists O
of O
human O
detection, O
3D B-DAT
human O
root O
lo- O
calization, O
and O

root-relative O
3D B-DAT
single-person O
pose O
estima- O
tion O
models O

any O
existing O
human O
detection O
and O
3D B-DAT
single-person O
pose O
estimation O
models O
can O

pro- O
posed O
system O
outperforms O
previous O
3D B-DAT
multi-person O
pose O
estimation O
methods O
by O

rable O
performance O
with O
3D B-DAT
single-person O
pose O
estimation O
methods O
without O

the O
previous O
human O
detection O
and O
3D B-DAT
human O
pose O
estimation O
models. O
We O

provides O
a O
new O
basis O
for O
3D B-DAT
multi-person O
pose O
estimation, O
which O
has O

Camera O
Distance-aware O
Top-down O
Approach O
for O
3D B-DAT
Multi-person O
Pose O
Estimation O
from O
a O

2. O
Comparison O
of O
3D B-DAT
human O
root O
localization O
with O
previous O

We O
compare O
previous O
absolute O
3D B-DAT
human O
root O
localiza- O
tion O
methods O

estimate O
2D O
image O
coordinates O
and O
3D B-DAT
camera-centered O
root-relative O
co- O
ordinates O
of O

between O
2D O
predictions O
and O
projected O
3D B-DAT
predic- O
tions. O
For O
optimization, O
linear O

et O
al. O
[26] O
as O
a O
3D B-DAT
pose O
estimator, O
which O
are O
state-of-the-art O

the O
effect O
of O
outliers O
in O
3D B-DAT

models. O
In O
contrast, O
the O
previous O
3D B-DAT
root O
lo- O
calization O
methods O
[28 O

require O
both O
of O
2D O
and O
3D B-DAT
pre- O
dictions O
for O
the O
root O

3D B-DAT
dataset O

3D B-DAT
dataset. O
All O
groundtruths O
are O
used O

4. O
Absolute O
3D B-DAT
multi-person O
pose O
estimation O
errors O

the O
continual O
study O
of O
the O
3D B-DAT
multi-person O
pose O
esti- O
mation, O
we O

show O
qualitative O
results O
of O
our O
3D B-DAT

estimation O
framework O
on O
the O
MuPoTS- O
3D B-DAT
[29] O
and O
COCO O
[25] O
datasets O

are O
hardly O
included O
in O
the O
3D B-DAT
human O
pose O
estimation O
training O
sets O

3D B-DAT
dataset O
[29 O

MuCo-3DHP O
and O
MuPoTS B-DAT

dataset O
[28]. O
The O
test O
set, O
MuPoTS B-DAT

and O
RootNet O
settings O
on O
the O
MuPoTS B-DAT

MuCo-3DHP O
and O
MuPoTS B-DAT

with O
state-of-the-art O
methods O
on O
the O
MuPoTS B-DAT

state- O
of-the-art O
methods O
on O
the O
MuPoTS B-DAT

methods O
[25, O
29]. O
MuCo-3DHP O
and O
MuPoTS B-DAT

pose O
estimation O
methods O
on O
the O
MuPoTS B-DAT

9: O
Sequence-wise O
3DPCKabs O
on O
the O
MuPoTS B-DAT

10: O
Joint-wise O
3DPCKabs O
on O
the O
MuPoTS B-DAT

pose O
estimation O
framework O
on O
the O
MuPoTS B-DAT

applying O
our O
method O
on O
the O
MuPoTS B-DAT

MuCo-3DHP O
and O
MuPoTS-3D B-DAT
datasets. O
These O
are O
the O
3D O

dataset O
[28]. O
The O
test O
set, O
MuPoTS-3D B-DAT
dataset, O
was O
captured O
at O
outdoors O

and O
RootNet O
settings O
on O
the O
MuPoTS-3D B-DAT
dataset O

MuCo-3DHP O
and O
MuPoTS-3D B-DAT
datasets. O
Following O
the O
previous O
protocol O

with O
state-of-the-art O
methods O
on O
the O
MuPoTS-3D B-DAT
dataset. O
∗ O
used O
extra O
synthetic O

state- O
of-the-art O
methods O
on O
the O
MuPoTS-3D B-DAT
dataset. O
All O
groundtruths O
are O
used O

methods O
[25, O
29]. O
MuCo-3DHP O
and O
MuPoTS-3D B-DAT
datasets. O
We O
com O

pose O
estimation O
methods O
on O
the O
MuPoTS-3D B-DAT
dataset O
[29] O
in O
Tables O
5 O

9: O
Sequence-wise O
3DPCKabs O
on O
the O
MuPoTS-3D B-DAT
dataset O

10: O
Joint-wise O
3DPCKabs O
on O
the O
MuPoTS-3D B-DAT
dataset. O
All O
groundtruths O
are O
used O

applying O
our O
method O
on O
the O
MuPoTS-3D B-DAT
dataset O
[29 O

Camera O
Distance-aware O
Top-down O
Approach O
for O
3D B-DAT
Multi-person O
Pose O
Estimation O
from O
a O

Qualitative O
results O
of O
applying O
our O
3D B-DAT
multi-person O
pose O
estimation O
framework O
to O

images. O
Most O
of O
the O
previous O
3D B-DAT
human O
pose O
estimation O
studies O
mainly O

focused O
on O
the O
root-relative O
3D B-DAT
single-person O
pose O
estimation. O
In O
this O

study, O
we O
propose O
a O
general O
3D B-DAT
multi-person O
pose O
estimation O
framework O
that O

factors O
including O
human O
detection O
and O
3D B-DAT
human O
root O
localization O

been O
achieved O
re- O
cently O
in O
3D B-DAT
human O
pose O
estimation, O
most O
of O

distance- O
aware O
top-down O
approach O
for O
3D B-DAT
multi-person O
pose O
esti- O
mation O
from O

consists O
of O
human O
detection, O
absolute O
3D B-DAT
hu- O
man O
root O
localization, O
and O

root-relative O
3D B-DAT
single-person O
pose O
estimation O
modules. O
Our O

comparable O
results O
with O
the O
state-of-the-art O
3D B-DAT
single-person O
pose O
es- O
timation O
models O

information O
and O
significantly O
outperforms O
previous O
3D B-DAT
multi-person O
pose O
es- O
timation O
methods O

The O
goal O
of O
3D B-DAT
human O
pose O
estimation O
is O
to O

or O
multiple O
human O
bodies O
in O
3D B-DAT
space. O
It O
is O
an O
essential O

Most O
of O
the O
previous O
3D B-DAT
human O
pose O
estimation O
meth- O
ods O

body O
is O
fed O
into O
the O
3D B-DAT
pose O
estimation O
module, O
which O
then O

estimates O
the O
3D B-DAT
location O
of O
each O
key- O
point O

49, O
52] O
estimate O
the O
relative O
3D B-DAT
pose O
to O
a O
reference O
point O

human, O
called O
root. O
The O
final O
3D B-DAT
pose O
is O
obtained O
by O
adding O

the O
3D B-DAT
coordinates O
of O
the O
root O
to O

the O
estimated O
root- O
relative O
3D B-DAT
pose. O
Prior O
information O
on O
the O

2D O
cases, O
extending O
them O
to O
3D B-DAT
cases O
is O
nontrivial. O
Note O
that O

for O
the O
estimation O
of O
3D B-DAT
multi-person O
poses, O
we O
need O
to O

propose O
a O
general O
framework O
for O
3D B-DAT
multi-person O
pose O
estimation. O
To O
the O

the O
previous O
human O
detection O
and O
3D B-DAT
human O
pose O
estimation O
methods. O
The O

input O
image. O
Second, O
the O
proposed O
3D B-DAT
human O
root O
localization O
network O
(RootNet O

humans’ O
roots. O
Third, O
a O
root-relative O
3D B-DAT
single-person O
pose O
estimation O
network O
(PoseNet O

) O
estimates O
the O
root-relative O
3D B-DAT
pose O
for O
each O
detected O
human O

that O
our O
approach O
outperforms O
previous O
3D B-DAT
multi-person O
pose O
estimation O
methods O
[29 O

40] O
on O
several O
publicly O
available O
3D B-DAT
single- O
and O
multi-person O
pose O
estima O

the O
bounding O
boxes O
and O
the O
3D B-DAT
location O
of O
the O
roots), O
our O

ble O
performance O
with O
the O
state-of-the-art O
3D B-DAT
single-person O
pose O
estimation O
methods O
that O

conventions O
of O
object O
detection O
and O
3D B-DAT
hu- O
man O
pose O
estimation O
networks O

a O
new O
general O
framework O
for O
3D B-DAT
multi- O
person O
pose O
estimation O
from O

the O
previous O
human O
detection O
and O
3D B-DAT
human O
pose O
estimation O
models O

For O
this, O
we O
propose O
a O
3D B-DAT
human O
root O
localization O
network O
(RootNet O

it O
easy O
to O
extend O
the O
3D B-DAT
single-person O
pose O
estimation O
techniques O
to O

the O
abso- O
lute O
3D B-DAT
pose O
estimation O
of O
multiple O
persons O

our O
method O
significantly O
outperforms O
previous O
3D B-DAT
multi-person O
pose O
estimation O
methods O
on O

comparable O
performance O
with O
the O
state-of-the-art O
3D B-DAT
single-person O
pose O
estimation O
methods O
without O

3D B-DAT
single-person O
pose O
estimation. O
Current O
3D O
single- O
person O
pose O
estimation O
methods O

ap- O
proach O
directly O
localizes O
the O
3D B-DAT
body O
keypoints O
from O
the O
input O

and O
lift O
them O
to O
a O
3D B-DAT
space O

3D B-DAT
root-relative O
pose O

x O
3D B-DAT
absolute O
human O
root O

3D B-DAT
multi-person O
pose O

of O
the O
proposed O
framework O
for O
3D B-DAT
multi-person O
pose O
estimation O
from O
a O

shaped O
network O
to O
estimate O
a O
3D B-DAT
heatmap O
for O
each O
joint. O
They O

soft-argmax O
operation O
to O
obtain O
the O
3D B-DAT
coor- O
dinates O
of O
body O
joints O

utilized O
it O
to O
regress O
the O
3D B-DAT
pose. O
Martinez O
et O
al. O
[26 O

network O
that O
directly O
regresses O
the O
3D B-DAT
coor- O
dinates O
of O
body O
joints O

adversarial O
loss O
to O
handle O
the O
3D B-DAT
human O
pose O
estimation O
in O
the O

3D B-DAT
multi-person O
pose O
estimation. O
Few O
studies O

have O
been O
conducted O
on O
3D B-DAT
multi-person O
pose O
estimation O
from O
a O

of O
2D O
and O
root- O
relative O
3D B-DAT
pose. O
It O
is O
generated O
by O

3D B-DAT
human O
root O
localization O
in O
3D O
multi-person O
pose O
estimation. O
Rogez O
et O

image O
coordinate O
space O
and O
the O
3D B-DAT
pose O
in O
the O
camera- O
centered O

space O
simultaneously. O
They O
obtained O
the O
3D B-DAT
location O
of O
the O
human O
root O

estimated O
2D O
pose O
and O
projected O
3D B-DAT
pose, O
similar O
to O
what O
Mehta O

cannot O
be O
generalized O
to O
other O
3D B-DAT
human O
pose O
esti- O
mation O
methods O

requires O
both O
the O
2D O
and O
3D B-DAT
esti- O
mations. O
For O
example, O
many O

PoseNet, O
which O
estimates O
the O
root-relative O
3D B-DAT
pose O
Prelj O
= O
(xj O

space. O
Then, O
the O
final O
absolute O
3D B-DAT
pose O
{Pabsj O
}Jj=1 O
is O
obtained O

of O
the O
human O
root O
in O
3D B-DAT
human O
pose O
estimation O
datasets O
[16 O

The O
Root- O
Net O
estimates O
the O
3D B-DAT
human O
root O
coordinate O

The O
PoseNet O
estimates O
the O
root-relative O
3D B-DAT
pose O
Prelj O
= O
(xj O

map O
to O
pro- O
duce O
the O
3D B-DAT
heatmaps O
for O
each O
joint. O
The O

dataset O
[16] O
is O
the O
largest O
3D B-DAT
single-person O
pose O
benchmark. O
It O
consists O

4 O
camera O
viewpoints. O
The O
groundtruth O
3D B-DAT
poses O
are O
obtained O
using O
a O

of O
the O
estimated O
and O
groundtruth O
3D B-DAT
poses. O
The O
second O
one O
is O

localiza- O
tion O
of O
the O
absolute O
3D B-DAT
human O
root, O
we O
introduce O
the O

MuPoTS-3D B-DAT
datasets. O
These O
are O
the O
3D O
multi-person O
pose O
estimation O
datasets O
proposed O

by O
compositing O
the O
existing O
MPI-INF-3DHP B-DAT
3D O
single-person O
pose O
estimation O
dataset O
[28 O

3D B-DAT
dataset, O
was O
captured O
at O
outdoors O

20 O
real-world O
scenes O
with O
groundtruth O
3D B-DAT
poses O
for O
up O
to O
three O

system. O
For O
evalua- O
tion, O
a O
3D B-DAT
percentage O
of O
correct O
keypoints O
(3DPCKrel O

the O
localization O
of O
the O
absolute O
3D B-DAT
human O
root, O
we O
use O

3D B-DAT
dataset O

the O
average O
precision O
of O
3D B-DAT
human O
root O
location O
(AP O
root25 O

3D B-DAT
datasets. O
Following O
the O
previous O
protocol O

pro- O
posed O
framework O
affects O
the O
3D B-DAT
multi-person O
pose O
estima- O
tion O
accuracy O

the O
accuracy O
of O
the O
final O
3D B-DAT
human O
root O
localization O
and O
3D O

model O
improves O
both O
of O
the O
3D B-DAT
human O
root O
localization O
and O
3D O

a O
large O
impact O
on O
the O
3D B-DAT
multi-person O
pose O
estimation O
accuracy O

how O
the O
performance O
of O
the O
3D B-DAT
human O
root O
localization O
affects O
the O

accuracy O
of O
the O
3D B-DAT
multi-person O
pose O
estimation, O
we O
compare O

3D B-DAT
dataset. O
∗ O
used O
extra O
synthetic O

3D B-DAT
dataset. O
All O
groundtruths O
are O
used O

to O
achieve O
more O
accurate O
absolute O
3D B-DAT
multi-person O
pose O
estimation O
results O

an O
evaluation O
of O
the O
root-relative O
3D B-DAT
human O
pose O
estimation O
highly O
depends O

proposed O
system O
with O
the O
state-of-the-art O
3D B-DAT
human O
pose O
estimation O
meth- O
ods O

tion O
(i.e., O
bounding O
boxes O
or O
3D B-DAT
root O
locations) O
in O
inference O
time O

the O
PoseNet O
using O
the O
groundtruth O
3D B-DAT
root O
location. O
Note O
that O
our O

vious O
3D B-DAT
multi-person O
pose O
estimation O
methods O
[25 O

3D B-DAT
datasets. O
We O
com O

proposed O
system O
with O
the O
state-of-the-art O
3D B-DAT
multi-person O
pose O
estimation O
methods O
on O

3D B-DAT
dataset O
[29] O
in O
Tables O
5 O

our O
proposed O
method O
outperforms O
previous O
3D B-DAT

Table O
2, O
using O
the O
groundtruth O
3D B-DAT
root O
location O
brings O
significant O
3DPCKabs O

a O
clue O
for O
improving O
the O
3D B-DAT
human O
root O
localization O
model O

used O
in O
applications O
other O
than O
3D B-DAT
multi-person O
pose O
estimation. O
For O
example O

, O
recent O
methods O
for O
3D B-DAT
human O
mesh O
model O
reconstruction O
[2 O

, O
18, O
19] O
reconstruct O
full O
3D B-DAT
mesh O
model O
from O
a O
single O

utilized O
2D O
multi-view O
input O
for O
3D B-DAT
multi- O
person O
mesh O
model O
reconstruction O

tion O
model O
[2, O
18, O
19], O
3D B-DAT
multi-person O
mesh O
model O
recon- O
struction O

can O
be O
applied O
to O
many O
3D B-DAT
instance- O
aware O
vision O
tasks O
which O

novel O
and O
general O
framework O
for O
3D B-DAT
multi O

framework O
consists O
of O
human O
detection, O
3D B-DAT
human O
root O
lo- O
calization, O
and O

root-relative O
3D B-DAT
single-person O
pose O
estima- O
tion O
models O

any O
existing O
human O
detection O
and O
3D B-DAT
single-person O
pose O
estimation O
models O
can O

pro- O
posed O
system O
outperforms O
previous O
3D B-DAT
multi-person O
pose O
estimation O
methods O
by O

rable O
performance O
with O
3D B-DAT
single-person O
pose O
estimation O
methods O
without O

the O
previous O
human O
detection O
and O
3D B-DAT
human O
pose O
estimation O
models. O
We O

provides O
a O
new O
basis O
for O
3D B-DAT
multi-person O
pose O
estimation, O
which O
has O

Camera O
Distance-aware O
Top-down O
Approach O
for O
3D B-DAT
Multi-person O
Pose O
Estimation O
from O
a O

2. O
Comparison O
of O
3D B-DAT
human O
root O
localization O
with O
previous O

We O
compare O
previous O
absolute O
3D B-DAT
human O
root O
localiza- O
tion O
methods O

estimate O
2D O
image O
coordinates O
and O
3D B-DAT
camera-centered O
root-relative O
co- O
ordinates O
of O

between O
2D O
predictions O
and O
projected O
3D B-DAT
predic- O
tions. O
For O
optimization, O
linear O

et O
al. O
[26] O
as O
a O
3D B-DAT
pose O
estimator, O
which O
are O
state-of-the-art O

the O
effect O
of O
outliers O
in O
3D B-DAT

models. O
In O
contrast, O
the O
previous O
3D B-DAT
root O
lo- O
calization O
methods O
[28 O

require O
both O
of O
2D O
and O
3D B-DAT
pre- O
dictions O
for O
the O
root O

3D B-DAT
dataset O

3D B-DAT
dataset. O
All O
groundtruths O
are O
used O

4. O
Absolute O
3D B-DAT
multi-person O
pose O
estimation O
errors O

the O
continual O
study O
of O
the O
3D B-DAT
multi-person O
pose O
esti- O
mation, O
we O

show O
qualitative O
results O
of O
our O
3D B-DAT

estimation O
framework O
on O
the O
MuPoTS- O
3D B-DAT
[29] O
and O
COCO O
[25] O
datasets O

are O
hardly O
included O
in O
the O
3D B-DAT
human O
pose O
estimation O
training O
sets O

3D B-DAT
dataset O
[29 O

MuCo-3DHP O
and O
MuPoTS B-DAT

dataset O
[28]. O
The O
test O
set, O
MuPoTS B-DAT

and O
RootNet O
settings O
on O
the O
MuPoTS B-DAT

MuCo-3DHP O
and O
MuPoTS B-DAT

with O
state-of-the-art O
methods O
on O
the O
MuPoTS B-DAT

state- O
of-the-art O
methods O
on O
the O
MuPoTS B-DAT

methods O
[25, O
29]. O
MuCo-3DHP O
and O
MuPoTS B-DAT

pose O
estimation O
methods O
on O
the O
MuPoTS B-DAT

9: O
Sequence-wise O
3DPCKabs O
on O
the O
MuPoTS B-DAT

10: O
Joint-wise O
3DPCKabs O
on O
the O
MuPoTS B-DAT

pose O
estimation O
framework O
on O
the O
MuPoTS B-DAT

applying O
our O
method O
on O
the O
MuPoTS B-DAT

Method O
Routing O
Reconstruction O
MNIST O
(%) O
MultiMNIST B-DAT

6.1 O
MultiMNIST B-DAT
dataset O

We O
generate O
the O
MultiMNIST B-DAT
training O
and O
test O
dataset O
by O

MNIST O
dataset O
we O
generate O
1K O
MultiMNIST B-DAT
examples. O
So O
the O
training O
set O

with O
3 O
routing O
iterations O
on O
MultiMNIST B-DAT
test O
dataset. O
The O
two O
reconstructed O

6.2 O
MultiMNIST B-DAT
results O

model O
trained O
from O
scratch O
on O
MultiMNIST B-DAT
training O
data O
achieves O
higher O
test O

a O
10K O
subset O
of O
the O
MultiMNIST B-DAT
data. O
We O
also O
searched O
for O

6.1 O
MultiMNIST B-DAT
dataset O

6.2 O
MultiMNIST B-DAT
results O

gathered O
data O
about O
the O
music O
chords B-DAT
for O
thousands O
of O
popular O
Brazilian O

engineer O
harmonically O
related O
features O
from O
chords B-DAT
data O
and O
to O
use O
it O

MIR O
· O
random O
forests O
· O
chords B-DAT
· O
genre O
classification O

music O
fea- O
tures O
such O
as O
chords B-DAT
configure O
a O
rich O
resource O
of O

tion O
regarding O
genres. O
The O
chords B-DAT
sequence O
of O
a O
song O
fully O

genre O
classification O
task: O
music O
sheets, O
chords, B-DAT
lyrics, O
MIDI, O
audio O
files, O
and O

analysis. O
The O
choice O
of O
symbolic O
chords B-DAT
data O
allows O
us O
to O
extract O

the O
songs O
by O
representing O
their O
chords B-DAT
structures O
in O
different O
and O
meaningful O

a O
vector O
based O
representation O
for O
chords B-DAT
sequences. O
This O
work O
is O
notable O

to O
extract O
information O
about O
symbolic O
chords B-DAT
data. O
A O
similar O
problem O
to O

considered, O
while O
the O
role O
of O
chords B-DAT
in O
the O
classification O
was O
given O

a O
better O
representation O
of O
the O
chords B-DAT
structures. O
Furthermore, O
in O
all O
of O

gineer O
harmonically O
related O
features O
from O
chords B-DAT
data O
and O
use O
it O
to O

major, O
minor, O
augmented O
and O
diminished O
chords B-DAT

The O
same O
chords B-DAT
can O
have O
different O
tonal O
functions O

progressions O
are O
a O
sequence O
of O
chords B-DAT
that O
happen O
in O
a O
particular O

form. O
Some O
chords B-DAT
progressions O
have O
famil- O
iar O
patterns O

way O
how O
a O
progression O
of O
chords B-DAT
is O
organized O
defines O
how O
the O

a O
big O
collection O
of O
music O
chords B-DAT
for O
different O
instruments, O
and O
it O

sense O
that O
most O
of O
the O
chords B-DAT
infor- O
mation O
present O
there O
are O

web O
pages, O
the O
collection O
of O
chords, B-DAT
keys, O
name O
of O
the O
artist O

once O
the O
symbolic O
representation O
of O
chords B-DAT
progressions O
do O
not O
capture O
all O

If O
we O
account O
for O
the O
chords B-DAT
in O
their O
simplest O
form, O
ig O

the O
raw O
information O
about O
the O
chords B-DAT
progression O
may O
cause O
a O
lot O

various O
distinct O
features O
from O
the O
chords, B-DAT
being O
able O
to O
make O
use O

into O
the O
percentage O
of O
major O
chords B-DAT

tetrads: O
1. O
Percentage O
of O
suspended O
chords B-DAT
(e.g. O
Gsus). O
2. O
Percentage O
of O

chords B-DAT
with O
the O
seventh O
(e.g O

C7). O
3. O
Percentage O
of O
minor O
chords B-DAT
with O
the O
seventh O

4. O
Percentage O
of O
minor O
chords B-DAT
(e.g. O
Em, O
C#m). O
5. O
Percentage O

of O
diminished O
chords B-DAT
(e.g. O
Bo). O
6. O
Percentage O
of O

augmented O
chords B-DAT
(e.g O

1. O
Percentage O
of O
chords B-DAT
with O
the O
fourth O
(e.g. O
D4 O

2. O
Percentage O
of O
chords B-DAT
with O
the O
sixth O
(e.g. O
E6 O

3. O
Percentage O
of O
chords B-DAT
with O
the O
ninth O
(e.g. O
G9 O

4. O
Percentage O
of O
minor O
chords B-DAT
with O
the O
major O
seventh O
(e.g O

5. O
Percentage O
of O
chords B-DAT
with O
a O
diminished O
fifth O
(e.g O

6. O
Percentage O
of O
chords B-DAT
with O
as O
augmented O
fifth O
(e.g O

2. O
Total O
of O
non-distinct O
chords B-DAT
in O
the O
song. O
3. O
Year O

common O
chord. O
5. O
Percentage O
of O
chords B-DAT
with O
varying O
bass O
(e.g O

6. O
Mean O
distance O
of O
the O
chords B-DAT
to O
the O
’C’ O
chord O

7. O
Mean O
distance O
of O
the O
chords B-DAT
to O
the O
’C’ O
chord O

of O
slight O
alterations O
in O
the O
chords B-DAT

have O
a O
lesser O
variety O
of O
chords, B-DAT
but O
this O
does O
not O
mean O

that O
the O
same O
group O
of O
chords B-DAT
and O
their O
transitions O
will O
be O

before, O
including O
the O
total O
of O
chords, B-DAT
the O
percentage O
of O
chords O
with O

in O
music O
theory O
relationships O
between O
chords B-DAT
can O
be O
measured O
with O
the O

a O
circle O
that O
represents O
music O
chords, B-DAT
each O
one O
being O
a O
fifth O

estimate O
how O
the O
variations O
of O
chords B-DAT
happen O
in O
the O
selected O
songs O

the O
symbolic O
representation O
of O
the O
chords, B-DAT
we O
gathered O
a O
broad O
set O

the O
triad O
features O
of O
the O
chords B-DAT

Figure O
3: O
Percentage O
of O
minor O
chords B-DAT
in O
the O
song O
versus O
the O

percentage O
of O
suspended O
chords, B-DAT
identifying O
the O
music O
genre O

Figure O
4: O
Percentage O
of O
minor O
chords B-DAT
in O
the O
song O
versus O
the O

percentage O
of O
chords B-DAT
with O
the O
seventh O
note, O
identifying O

of O
the O
count O
of O
distinct O
chords, B-DAT
obtained O
per O
song O
for O
each O

Figure O
5: O
Percentage O
of O
minor O
chords B-DAT
in O
the O
song O
versus O
the O

percentage O
of O
augmented O
chords, B-DAT
identifying O
the O
music O
genre O

the O
mean O
count O
of O
distinct O
chords B-DAT

6: O
Mean O
count O
of O
distinct O
chords B-DAT
in O
the O
songs O
for O
each O

the O
information O
about O
the O
common O
chords B-DAT
transitions, O
at O
least O
some O
percentage O

correct O
classification: O
the O
percentage O
of O
chords B-DAT
with O
the O
seventh O

the O
percentage O
of O
minor O
chords B-DAT
with O
the O
seventh. O
After O
the O

and O
the O
total O
number O
of O
chords B-DAT
in O
each O
song. O
Those O
features O

model O
are O
the O
percentage O
of O
chords B-DAT
with O
the O
seventh O
note, O
percentage O

of O
minor O
chords B-DAT
with O
the O
seventh O
note, O
the O

percentage O
of O
minor O
chords, B-DAT
the O
year O
of O
release O
of O

the O
mean O
distances O
of O
the O
chords B-DAT
having O
the O
’C’ O
chord O
in O

and O
the O
total O
number O
of O
chords B-DAT
in O
each O
song. O
On O
this O

the O
symbolic O
part O
of O
the O
chords B-DAT

package O
for O
extraction O
of O
music O
chords B-DAT
data O
in O
r, O
2018 O

sentences O
generated O
from O
training O
on O
CMU B-DAT

English O
sentences O
referred O
to O
as O
CMU B-DAT

Rajeswar O
et O
al., O
2017). O
The O
CMU B-DAT

SE B-DAT
Dataset; O
mode O
collapse O
is O
overcome O

SE3 B-DAT
in O
(Rajeswar O
et O
al., O
2017 O

SE B-DAT
dataset O
consists O
of O
44,016 O
sentences O

sentences O
generated O
from O
training O
on O
CMU-SE B-DAT
Dataset; O
mode O
collapse O
is O
overcome O

English O
sentences O
referred O
to O
as O
CMU-SE3 B-DAT
in O
(Rajeswar O
et O
al., O
2017 O

). O
The O
CMU-SE B-DAT
dataset O
consists O
of O
44,016 O
sentences O

The O
resulting O
network O
won O
the O
EM B-DAT
segmentation O
challenge O
at O
ISBI O
2012 O

segmentation O
of O
neuronal O
structures O
in O
EM B-DAT
stacks). O
Prediction O
of O
the O
segmentation O

segmentation O
of O
neuronal O
structures O
in O
EM B-DAT
stacks O
(an O
ongoing O
competition O
started O

set O
is O
provided O
by O
the O
EM B-DAT
segmentation O
challenge O
[14] O
that O
was O

Table O
1. O
Ranking O
on O
the O
EM B-DAT
segmentation O
challenge O
[14] O
(march O
6th O

EM O
segmentation O
challenge O
at O
ISBI O
2012 B-DAT
by O
a O
large O
margin O

ongoing O
competition O
started O
at O
ISBI O
2012), B-DAT
where O
we O
out O

that O
was O
started O
at O
ISBI O
2012 B-DAT
and O
is O
still O
open O
for O

images. O
In: O
NIPS. O
pp. O
2852–2860 O
(2012 B-DAT

networks. O
In: O
NIPS. O
pp. O
1106–1114 O
(2012 B-DAT

Convolutional O
Networks O
for O
Biomedical O
Image O
Segmentation B-DAT

Convolutional O
Networks O
for O
Biomedical O
Image O
Segmentation B-DAT

Moreover, O
the O
network O
is O
fast. O
Segmentation B-DAT
of O
a O
512x512 O
image O
takes O

the O
“PhC-U373” O
data O
set. O
(b) O
Segmentation B-DAT
result O
(cyan O
mask) O
with O
manual O

the O
“DIC-HeLa” O
data O
set. O
(d) O
Segmentation B-DAT
result O
(random O
colored O
masks) O
with O

Table O
2. O
Segmentation B-DAT
results O
(IOU) O
on O
the O
ISBI O

Convolutional O
Networks O
for O
Biomedical O
Image O
Segmentation B-DAT

sliding-window O
convolutional O
network) O
on O
the O
ISBI B-DAT
challenge O
for O
segmentation O
of O
neu O

and O
DIC) O
we O
won O
the O
ISBI B-DAT
cell O
tracking O
challenge O
2015 O
in O

the O
EM O
segmentation O
challenge O
at O
ISBI B-DAT
2012 O
by O
a O
large O
margin O

an O
ongoing O
competition O
started O
at O
ISBI B-DAT
2012), O
where O
we O
out O

light O
microscopy O
images O
from O
the O
ISBI B-DAT
cell O
tracking O
chal- O
lenge O
2015 O

14] O
that O
was O
started O
at O
ISBI B-DAT
2012 O
and O
is O
still O
open O

Fig. O
4. O
Result O
on O
the O
ISBI B-DAT
cell O
tracking O
challenge. O
(a) O
part O

Segmentation O
results O
(IOU) O
on O
the O
ISBI B-DAT
cell O
tracking O
challenge O
2015 O

task O
is O
part O
of O
the O
ISBI B-DAT
cell O
tracking O
challenge O
2014 O
and O

obtains O
47mm O
MPJPE O
on O
the O
CHALL B-DAT
H80K O
test O
dataset, O
placing O
second O

challenge O
is O
based O
on O
the O
CHALL B-DAT
H80K O
subset O
of O
the O
popular O

Human3.6M O
[1,2] O
benchmark. O
CHALL B-DAT
H80K O
contains O
80K O
3D O
human O

have O
a O
simple O
appearance. O
In O
CHALL B-DAT
H80K, O
only O
3D O
pose O
ground-truth O

used O
in O
our O
system O
besides O
CHALL B-DAT
H80K. O
Specifically, O
2D O
human O
pose O

obtains O
47mm O
MPJPE O
on O
the O
CHALL B-DAT
H80K O
test O
dataset, O
placing O
second O

system O
(in O
pixels). O
Since O
the O
CHALL B-DAT
H80K O
dataset O
provides O
the O
3D O

For O
our O
ablation O
study, O
the O
CHALL B-DAT
H80K O
train O
and O
val O
datasets O

final O
challenge O
result O
on O
the O
CHALL B-DAT
H80K O
test O
dataset, O
both O
train O

our O
final O
result O
on O
the O
CHALL B-DAT
H80K O
test O
dataset. O
Both O
train O

All O
models O
are O
trained O
on O
CHALL B-DAT
H80K O
train O
dataset O
and O
evaluated O

on O
CHALL B-DAT
H80K O
val O
dataset O

47mm O
MPJPE O
on O
the O
CHALL O
H80K B-DAT
test O
dataset, O
placing O
second O
in O

is O
based O
on O
the O
CHALL O
H80K B-DAT
subset O
of O
the O
popular O
Human3.6M O

1,2] O
benchmark. O
CHALL O
H80K B-DAT
contains O
80K O
3D O
human O
poses O

a O
simple O
appearance. O
In O
CHALL O
H80K, B-DAT
only O
3D O
pose O
ground-truth O
is O

in O
our O
system O
besides O
CHALL O
H80K B-DAT

47mm O
MPJPE O
on O
the O
CHALL O
H80K B-DAT
test O
dataset, O
placing O
second O
in O

in O
pixels). O
Since O
the O
CHALL O
H80K B-DAT
dataset O
provides O
the O
3D O
ground O

our O
ablation O
study, O
the O
CHALL O
H80K B-DAT
train O
and O
val O
datasets O
are O

challenge O
result O
on O
the O
CHALL O
H80K B-DAT
test O
dataset, O
both O
train O
and O

final O
result O
on O
the O
CHALL O
H80K B-DAT
test O
dataset. O
Both O
train O
and O

models O
are O
trained O
on O
CHALL O
H80K B-DAT
train O
dataset O
and O
evaluated O
on O

CHALL O
H80K B-DAT
val O
dataset O

obtains O
47mm O
MPJPE O
on O
the O
CHALL B-DAT
H80K I-DAT
test O
dataset, O
placing O
second O
in O

challenge O
is O
based O
on O
the O
CHALL B-DAT
H80K I-DAT
subset O
of O
the O
popular O
Human3.6M O

1,2] O
benchmark. O
CHALL B-DAT
H80K I-DAT
contains O
80K O
3D O
human O
poses O

have O
a O
simple O
appearance. O
In O
CHALL B-DAT
H80K, I-DAT
only O
3D O
pose O
ground-truth O
is O

used O
in O
our O
system O
besides O
CHALL B-DAT
H80K I-DAT

obtains O
47mm O
MPJPE O
on O
the O
CHALL B-DAT
H80K I-DAT
test O
dataset, O
placing O
second O
in O

system O
(in O
pixels). O
Since O
the O
CHALL B-DAT
H80K I-DAT
dataset O
provides O
the O
3D O
ground O

For O
our O
ablation O
study, O
the O
CHALL B-DAT
H80K I-DAT
train O
and O
val O
datasets O
are O

final O
challenge O
result O
on O
the O
CHALL B-DAT
H80K I-DAT
test O
dataset, O
both O
train O
and O

our O
final O
result O
on O
the O
CHALL B-DAT
H80K I-DAT
test O
dataset. O
Both O
train O
and O

All O
models O
are O
trained O
on O
CHALL B-DAT
H80K I-DAT
train O
dataset O
and O
evaluated O
on O

CHALL B-DAT
H80K I-DAT
val O
dataset O

make O
caregivers O
susceptible O
to O
poor O
sleep B-DAT
quality O
with O
detrimental O
effects O
on O

overall O
health. O
Hence, O
monitoring O
caregivers’ O
sleep B-DAT
quality O
can O
provide O
important O
CPWD O

stress O
assessment. O
Most O
current O
sleep B-DAT
studies O
are O
based O
on O
polysomnography O

decision O
support O
system O
to O
predict O
sleep B-DAT
quality O
based O
on O
trends O
of O

physiological O
signals O
in O
the O
deep O
sleep B-DAT
stage. O
This O
system O
utilizes O
four O

a O
two- O
week O
period O
of O
sleep B-DAT
monitored O
on O
eight O
CPWD. O
The O

an O
accuracy O
of O
75% O
for O
sleep B-DAT
quality, O
and O
73% O
for O
restfulness O

to O
detect O
these O
measures O
are O
sleep B-DAT
efficiency O
(ratio O
of O
amount O
of O

temperature. O
The O
results O
from O
our O
sleep B-DAT
analysis O
system O
demonstrate O
the O
capability O

using O
wearable O
sensors O
to O
measure O
sleep B-DAT
quality O
and O
restfulness O
in O
CPWD O

studies O
indicate O
pivotal O
effects O
of O
sleep B-DAT
in O
job O
performance O
[1], O
memory O

prolactin, O
testosterone, O
luteinizing O
hormone) O
during O
sleep B-DAT
along O
with O
physical O
restoration O
lead O

to O
feeling O
refreshed O
after O
sleep B-DAT
[6]. O
When O
individuals O
experience O
difficulty O

of O
attention O
[7]. O
Therefore, O
assessing O
sleep B-DAT
performance, O
especially O
for O
individuals O
with O

illnesses O
are O
more O
susceptible O
to O
sleep B-DAT
problems O
than O
the O
rest O
of O

two-thirds O
of O
these O
caregivers O
reported O
sleep B-DAT
disturbance O
during O
the O
course O
of O

10], O
we O
focus O
on O
predicting O
sleep B-DAT
quality O
of O
primary O
CPWD O
in O

caregiving O
challenges O
contributes O
to O
chronic O
sleep B-DAT
defi- O
ciency. O
Poor O
sleep O
quality O

first O
work O
trying O
to O
predict O
sleep B-DAT
quality O
speci- O
fically O
among O
family O

A O
well-founded O
sleep B-DAT
quality O
assessment O
requires O
a O
comprehensive O

understanding O
of O
underlying O
sleep B-DAT
mechanisms. O
The O
American O
Academy O
of O

a O
standard O
guideline O
to O
evaluate O
sleep B-DAT
architecture O
[14]. O
PSG O
enables O
physicians O

to O
assess O
sleep B-DAT
and O
sleep O
disturbance O
by O
continuous O
monitoring O
of O

multiple O
physiological O
parameters O
during O
sleep B-DAT

expensive O
and O
intrusive O
as O
a O
sleep B-DAT
study O
which O
is O
conducted O
by O

To O
inexpensively O
evaluate O
sleep B-DAT
quality O
of O
different O
subjects O
sleeping O

and O
(iv) O
skin O
temperature. O
Our O
sleep B-DAT
quality O
prediction O
system O
tracks O
the O

applicable O
for O
different O
subjects O
who O
sleep B-DAT
in O
either O
their O
own O
homes O

assisted O
living O
as O
opposed O
to O
sleep B-DAT
laboratories O

A O
general O
outline O
about O
the O
sleep B-DAT
studies O
is O
provided O
in O
Section O

with O
pub- O
lished O
works O
in O
sleep B-DAT
quality O
analysis. O
Finally, O
Section O
6 O

effects O
on O
personal O
well-being, O
as O
sleep B-DAT
encompasses O
roughly O
one-third O
of O
human O

studies O
explore O
the O
relationship O
between O
sleep B-DAT
and O
health O
issues O
such O
as O

Ad- O
ditionally, O
the O
relationship O
between O
sleep B-DAT
and O
performance O
of O
students O
[23 O

from O
three O
different O
aspects: O
caregivers' O
sleep B-DAT
problems, O
sleep O
architecture, O
and O
its O

Studies O
specific O
to O
caregiver O
sleep B-DAT
disturbance O
have O
shown O
that O
the O

illness O
results O
in O
chronic O
stress, O
sleep B-DAT
hygiene O
difficulties, O
and O
sleep O
disturbance O

26]. O
CPWD O
can O
experience O
sleep B-DAT
difficulties O
because O
of O
both O
overwhelming O

dementia O
patient O
behaviors O
[28]. O
Inadequate O
sleep B-DAT
makes O
it O
challenging O
for O
caregivers O

To O
evaluate O
caregivers’ O
sleep, B-DAT
it O
is O
necessary O
to O
scrutinize O

the O
ar- O
chitecture O
of O
sleep B-DAT
(sleep O
stages) O
[30], O
fluctuations O
of O
physiological O

signals O
during O
sleep B-DAT
[16], O
and O
sleep O
disorders O
[31]. O
Sleep O
architecture O
comprises O

6]. O
Since O
about O
80% O
of O
sleep B-DAT
time O
of O
an O
adult O
is O

NREM O
sleep B-DAT
[7], O
most O
of O
the O
sleep O

studies O
focus O
on O
NREM O
sleep B-DAT

. O
NREM O
sleep B-DAT
can O
be O
further O
divided O
into O

one O
stage O
of O
slow O
wave O
sleep B-DAT
(SWS) O
based O
on O
the O
current O

Among O
these O
sleep B-DAT
stages, O
SWS O
or O
deep O
sleep O

special O
char- O
acteristics O
relevant O
to O
sleep B-DAT
quality. O
For O
instance, O
human O
growth O

with O
increasing O
length O
of O
deep O
sleep B-DAT
in O
patients O
suffering O
insomnia O
[35 O

]. O
Furthermore, O
patients O
suffering O
sleep B-DAT
disorders O
like O
obstructive O
sleep O
apnea O

36]. O
Moreover, O
the O
reduction O
of O
sleep B-DAT
quality O
in O
the O
elderly O
is O

with O
the O
loss O
of O
deep O
sleep B-DAT
[37]. O
Therefore, O
sleep O
quality O
can O

have O
been O
proposed O
to O
scrutinize O
sleep B-DAT
by O
classi- O
fying O
the O
sleep O

state O
of O
participants, O
i.e., O
sleep B-DAT
detection O
[38]. O
The O

to O
help O
patients O
suffering O
from O
sleep B-DAT
disorders. O
As O
a O
result, O
many O

proposed O
to O
investigate O
the O
whole O
sleep B-DAT
stages O
as O
awake, O
stage O
1 O

accurate O
medical O
procedure O
to O
investigate O
sleep B-DAT
stages. O
A O
modern O
PSG O
utilizes O

is O
an O
expensive O
and O
intrusive O
sleep B-DAT
study, O
which O
requires O
attaching O
many O

as O
ours O
seeks O
to O
address O
sleep B-DAT
detection O
via O
cheaper O
and O
easier-to O

monitor O
important O
physiological O
signals O
during O
sleep B-DAT
while O
it O
does O
not O
interrupt O

regular O
sleep B-DAT
pattern. O
Also, O
the O
E4 O
wristband O

data O
like O
step O
counts O
and O
sleep B-DAT
stages O

length O
and O
transition O
of O
different O
sleep B-DAT
stages, O
especially O
SWS O
and O
the O

quality O
of O
sleep B-DAT
[33–37]. O
This O
relation O
has O
been O

Refs. O
[49–52], O
demonstrate O
distinguishability O
of O
sleep B-DAT
stages O
based O
on O
the O
physiological O

pro- O
hibited O
and O
intrusively O
effects O
sleep B-DAT
quality O
of O
participants. O
Hence, O
there O

a O
need O
to O
explore O
the O
sleep B-DAT
quality O
using O
cost-effective O
and O
un O

our O
proposed O
method O
to O
scrutinize O
sleep B-DAT
quality O
via O
the O
trends O
of O

experiments, O
which O
involve O
mon- O
itoring O
sleep B-DAT
stages O
using O
PSG O
[33–37,45,49–66]. O
Therefore O

support O
system O
to O
predict O
the O
sleep B-DAT
quality O
based O
on O
trends O
of O

physiological O
signals O
in O
deep O
sleep B-DAT
stage. O
This O
methodology O
enables O
us O

to O
estimate O
sleep B-DAT
quality O
according O
to O
physio- O
logical O

shown O
in O
Fig. O
1, O
this O
sleep B-DAT
analysis O
strategy O
reuses O
the O
sleep O

system O
evaluates O
the O
effectiveness O
of O
sleep B-DAT
according O
to O
various O
physiological O
signals O

mean O
of O
heart O
rate O
for O
sleep B-DAT
stage O
detection O
[67] O
can O
be O

shown O
in O
Fig. O
2, O
our O
sleep B-DAT
analysis O
system O
processes O
the O
input O

the O
E4 O
wristband O
during O
caregiver's O
sleep B-DAT
period. O
This O
FDA-approved O
device O
is O

been O
recently O
used O
in O
many O
sleep B-DAT
assessment O
studies O
like O
[46–48]. O
To O

physiological O
trends O
in O
the O
deep O
sleep B-DAT
stage, O
the O
collected O
signals O
need O

3.1.1. O
Heart O
rate O
variability O
As O
sleep B-DAT
goes O
from O
stage O
1 O
to O

aim O
of O
predicting O
the O
deep O
sleep B-DAT
stages O
during O
sleep O

Fig. O
1. O
Assessing O
the O
sleep B-DAT
quality O
using O
easy-to-use O
and O
cost-effective O

physiological O
signal O
trends O
in O
deep O
sleep B-DAT
and O
sleep O
quality O

The O
outline O
of O
the O
proposed O
sleep B-DAT
analysis O
system O

detect O
the O
SWS O
during O
the O
sleep B-DAT

is O
strongly O
associated O
with O
deep O
sleep B-DAT
rather O
than O
other O
stages O
[37 O

varied O
magnitude O
of O
EDA O
during O
sleep B-DAT

further O
process O
according O
to O
the O
sleep B-DAT
time. O
Next, O
the O
low-pass O
finite O

map O
of O
EDA O
fluctuations O
during O
sleep B-DAT
stages. O
The O
higher O
the O
intensity O

this O
map, O
the O
deeper O
the O
sleep B-DAT
is. O
The O
intensity O
level O
of O

relation O
with O
sleep B-DAT
stages O
[61]. O
Short O
movements O
appear O

over O
all O
sleep B-DAT
stages, O
however O
the O
frequency O
of O

has O
a O
direct O
relationship O
with O
sleep B-DAT
quality O
since O
it O
can O
disrupt O

sleep B-DAT

a O
more O
destructive O
effect O
on O
sleep B-DAT
of O
elderly O
people O
who O
have O

difficulty O
to O
maintain O
a O
stable O
sleep B-DAT
with O
lower O
SWS O
as O
a O

of O
temperature O
during O
their O
deep O
sleep B-DAT
[64]. O
The O
speed O
and O
amount O

has O
a O
strong O
relationship O
with O
sleep B-DAT
quality O
[65]. O
Hence, O
subjects O
who O

sleep B-DAT
on O
warm O
mattresses O
lose O
more O

an O
index O
of O
quality O
of O
sleep B-DAT
[66 O

wristband O
relevant O
to O
a O
deep O
sleep B-DAT
stage O
and O
are O
important O
factors O

for O
sleep B-DAT
quality. O
Since O
these O
two O
signals O

transformation O
relating O
to O
a O
deep O
sleep B-DAT
like O
HRV O
and O
EDA O
signals O

signals O
correspond O
to O
the O
deeper O
sleep B-DAT
stages O

method O
highlights O
the O
part O
of O
sleep B-DAT
showing O
a O
declining O
pattern. O
Also O

the O
first O
half O
of O
the O
sleep B-DAT
period O
corresponding O
to O
NREM O
in O

In O
order O
to O
predict O
sleep B-DAT
quality, O
quantitative O
features O
are O
computed O

people O
experience O
different O
length O
of O
sleep B-DAT
each O
night, O
we O
con- O
sider O

the O
percentage O
of O
sleep B-DAT
time O
when O
the O
time O
series O

are O
awake O
from O
when O
they O
sleep B-DAT

suf- O
ficient O
features O
in O
deep O
sleep B-DAT

. O
In O
other O
words, O
sleep B-DAT
architecture O
is O
a O
cyclic O
procedure O

experiencing O
more O
and O
longer O
deep O
sleep B-DAT
stages. O
On O
the O
contrary, O
the O

person O
wakes O
up O
during O
the O
sleep, B-DAT
the O
less O
the O
person O
experiences O

deep O
sleep B-DAT

Therefore, O
we O
use O
amount O
of O
sleep, B-DAT
amount O
of O
awake, O
and O
times O

features. O
Furthermore, O
we O
calculate O
the O
sleep B-DAT
efficiency O
(SE) O
feature O
as O
the O

proportion O
of O
sleep B-DAT
time O
to O
the O
time O
a O

person O
tries O
to O
sleep B-DAT
[72]. O
The O
SE O
considers O
the O

time O
in O
bed O
trying O
to O
sleep B-DAT

in O
SWS O
rather O
than O
other O
sleep B-DAT
stages O
[60]. O
Then, O
we O
compute O

number O
of O
epochs O
during O
a O
sleep B-DAT
period O
as O
the O
Epoch O
peak O

of O
EDA O
in O
the O
deep O
sleep B-DAT
stage O
[59] O
results O
in O
higher O

number O
of O
epochs O
in O
a O
sleep B-DAT
period O
to O
the O
total O
possible O

EDA O
ac- O
tivity O
during O
the O
sleep B-DAT
and O
a O
long O
deep O
sleep O

in O
EDA O
study O
relevant O
to O
sleep B-DAT
quality O
is O
EDA O
storm, O
the O

storms O
appear O
most O
during O
deep O
sleep B-DAT
stage O
[73], O
we O
report O
EDA O

higher O
probability O
of O
experiencing O
deep O
sleep B-DAT
and O
it O
is O
worth O
mentioning O

to O
record O
100 O
nights O
of O
sleep B-DAT
(as O
opposed O
to O
112 O
nights O

contained O
the O
daily O
use O
caregiver O
sleep B-DAT
survey O
(DUCSS) O
[81] O
(Appendix O
B O

store O
the O
responses O
to O
the O
sleep B-DAT
survey O
questions. O
The O
app O
did O

ap- O
proximately O
15 O
min O
before O
sleep B-DAT
and O
remove O
the O
device O
immediately O

Other O
available O
sleep B-DAT
surveys O
[82–85] O
are O
not O
intended O

for O
frequent O
use O
or O
caregivers’ O
sleep B-DAT
assessment. O
To O
proactively O
recognize O
sleep O

method O
design O
utilizing O
4 O
available O
sleep B-DAT
surveys: O
the O
Pittsburgh O
sleep O
quality O

PSQI) O
[82], O
functional O
outcomes O
of O
sleep B-DAT
questionnaire O
(FOSQ) O
[83], O
Calgary O
sleep O

refined O
and O
streamlined O
multiple O
existing O
sleep B-DAT
surveys O
questions O
[82–85], O
while O
adding O

the O
following O
section, O
the O
CPWD O
sleep B-DAT
data O
set O
are O
analyzed O
from O

two O
aspects: O
(i) O
overall O
sleep B-DAT
quality O
(Question O
9 O
from O
survey O

avoid O
bias O
in O
measurement O
of O
sleep B-DAT
quality, O
the O
sleep O
survey O
evaluates O

sleep B-DAT
from O
different O
aspects. O
As O
shown O

to O
appraise O
the O
effectiveness O
of O
sleep B-DAT
through O
“overall O
quality O
of O
sleep O

represent O
different O
aspects O
of O
the O
sleep B-DAT
quality O
of O
caregivers. O
The O
“sleep O

satisfaction O
of O
caregivers O
from O
their O
sleep B-DAT
which O
has O
a O
direct O
effect O

a O
holistic O
view O
about O
the O
sleep B-DAT
quality O
of O
CPWD. O
The O
interaction O

area O
corresponding O
to O
the O
caregiver O
sleep B-DAT
with O
‘bad O
quality’ O
and O
‘feeling O

of O
caregivers’ O
mood O
versus O
their O
sleep B-DAT
quality. O
It O
also O
shows O
the O

marginal O
distribution O
of O
sleep B-DAT
quality O
(the O
width O
of O
the O

caregivers O
still O
feel O
tired O
after O
sleep B-DAT

caregivers O
who O
felt O
rested O
after O
sleep B-DAT
re- O
ported O
good O
or O
very O

good O
sleep B-DAT
quality. O
This O
denotes O
the O
strong O

relation O
among O
sleep B-DAT
quality O
and O
feeling O
refreshed. O
On O

other O
hand, O
17% O
of O
caregiver O
sleep B-DAT
records O
are O
associated O
with O
feeling O

and O
fairly O
bad O
quality O
of O
sleep B-DAT

well O
as O
have O
an O
“Ok” O
sleep B-DAT
the O
night O
before. O
In O
contrast O

the O
caregiver O
feeling O
rested O
after O
sleep B-DAT
while O
they O
report O
“Ok” O
sleep O

caregivers O
who O
feel O
tired O
after O
sleep B-DAT
experienced O
bad O
or O
fairly O
bad O

quality O
of O
sleep B-DAT

a O
coarser O
labeling O
method O
like O
sleep B-DAT
quality O
results O
in O
an O
imbalanced O

data O
set. O
The O
distribution O
of O
sleep B-DAT
quality O
displayed O
in O
Table O
2 O

the O
fairly O
bad O
and O
bad O
sleep B-DAT
quality. O
While O
over O
1 O
out O

In O
addition O
to O
general O
sleep B-DAT
characteristics, O
the O
sleep O
dataset O
also O

caregivers O
are O
usually O
deprived O
from O
sleep B-DAT
caused O
by O
interruption. O
Various O
conditions O

a O
common O
pattern O
among O
caregivers’ O
sleep B-DAT
since O
98% O
of O
the O
records O

reported O
a O
sleep B-DAT
break. O
These O
sleep O
interruptions O
can O
last O
longer O
than O

the O
chance O
of O
having O
deep O
sleep B-DAT
among O
this O
population. O
It O
may O

records O
reported O
feeling O
tired O
after O
sleep B-DAT

Ok”, O
“Good”, O
or O
“Very O
Good” O
sleep B-DAT
quality, O
possibly O
due O
to O
the O

presence O
of O
disrupted O
sleep B-DAT
is O
a O
routine O
among O
caregivers O

records O
are O
assigned O
to O
bad O
sleep B-DAT
quality O

This O
low O
number O
of O
bad O
sleep B-DAT
quality O
can O
be O
related O
to O

single O
ca- O
tegory O
of O
bad O
sleep B-DAT
quality O
for O
further O
processing. O
However O

data O
set O
in O
terms O
of O
sleep B-DAT
quality, O
we O
apply O
the O
resampling O

performance O
for O
feeling O
refreshed O
and O
sleep B-DAT
quality, O
respectively. O
The O
best O
model O

utilizes O
12 O
features O
while O
the O
sleep B-DAT
quality O
one O
employs O
10 O
features O

By O
evaluating O
sleep B-DAT
records O
referring O
to O
feeling O
“refreshed O

” O
or O
“tired” O
after O
sleep, B-DAT
we O
consider O
it O
as O
a O

On O
the O
other O
hand, O
distinguishing O
sleep B-DAT
qualities O
can O
be O
a O
multi-class O

Questions O
9 O
and O
10 O
of O
sleep B-DAT
survey O

the O
overall O
quality O
of O
my O
sleep B-DAT
as O
a.) O
Very O
good O
b O

quality O
and O
rest O
feeling O
after O
sleep B-DAT
in O
CPWD O

probability. O
As O
a O
result, O
our O
sleep B-DAT
analysis O
system O
can O
be O
a O

making O
decision O
rather O
than O
the O
sleep B-DAT
quality O
model. O
This O
fact O
is O

of O
possible O
cases O
rather O
than O
sleep B-DAT
quality. O
Also, O
sleep O
efficiency O
is O

important O
factor O
in O
both O
the O
sleep B-DAT
quality O
and O
feeling O
refreshed O
models O

previous O
results O
relevant O
to O
CPWD O
sleep B-DAT
characteristics O
showing O
they O
experienced O
fewer O

total O
sleep B-DAT
time O
while O
taking O
longer O
to O

in O
both O
models, O
especially O
in O
sleep B-DAT
quality. O
In O
fact, O
the O
nocturnal O

temperature O
dysregulation O
as O
an O
age-related O
sleep B-DAT
dis- O
turbance O
contributes O
to O
fragmentation O

of O
sleep B-DAT
which O
is O
a O
common O
predisposing O

factor O
for O
sleep B-DAT
complaints O
in O
caregivers O
[9]. O
Since O

models O
agree O
with O
well-established O
CPWD O
sleep B-DAT
studies, O
the O
proposed O
models O
can O

94] O
evaluated O
more O
than O
100 O
sleep B-DAT
quality O
analysis O
papers O
published O
in O

methods O
to O
multiple O
sensors O
for O
sleep B-DAT
quality O
mea- O
surement. O
We O
tried O

In O
some O
previous O
sleep B-DAT
studies O
[26–28], O
both O
subjective O
and O

objec- O
tive O
sleep B-DAT
quality O
parameters, O
e.g. O
total O
sleep O

time, O
sleep B-DAT
efficiency, O
and O
mean O
range O
of O

sleep B-DAT
stages, O
in O
caregivers’ O
sleep O
and O
its O
difference O
from O
the O

sleep B-DAT
of O
non-caregivers O
have O
been O
investigated O

them O
endeavor O
to O
predict O
the O
sleep B-DAT
quality O
of O
caregivers. O
To O
the O

first O
work O
that O
predicts O
the O
sleep B-DAT
quality O

of O
previous O
works O
[23–28] O
investigated O
sleep B-DAT
quality O

81] O
for O
assessing O
each O
individual's O
sleep B-DAT
quality O
every O
night O
as O
a O

Studies, O
such O
as O
[95], O
used O
sleep B-DAT
measurements, O
like O
sleep O
ef- O
ficiency O

, O
as O
a O
proxy O
for O
sleep B-DAT
quality O
without O
actually O
measuring O
sleep O

means. O
Our O
evaluation O
discovered O
that O
sleep B-DAT
efficiency O
is O
one O
of O
the O

most O
important O
factors O
in O
predicting O
sleep B-DAT
quality, O
which O
is O
consistent O
with O

Regarding O
predicting O
sleep B-DAT
quality O
using O
wearable O
devices, O
two O

of O
[23,95] O
studied O
estimation O
of O
sleep B-DAT
quality O
using O
ActiGraph O
Gt3X+ O
and O

Q-sensor, O
respectively. O
These O
studies O
predicted O
sleep B-DAT
quality O
of O
high O
school O
[23 O

features, O
and O
their O
importance O
in O
sleep B-DAT
quality O
prediction. O
The O
evaluation O
results O

decreaseTemperature O
(Temperature), O
swsLengthHR O
(HRV), O
and O
sleep B-DAT
efficiency O
(Body O
movement) O
play O
the O

most O
important O
roles O
in O
predicting O
sleep B-DAT
quality. O
On O
the O
other O
hand O

, O
sleep B-DAT
efficiency O
(Body O
movement), O
largest O
storm O

predicting O
feeling O
refreshed/ O
tired O
after O
sleep B-DAT

with O
previous O
findings O
that O
reveal O
sleep B-DAT
efficiency O
and O
skin O
temperature O
are O

common O
predis- O
posing O
factors O
for O
sleep B-DAT
complains O
in O
caregivers O
[9,27 O

Evaluation O
of O
sleep B-DAT
quality O
and O
feeling O
refreshed O
for O

ex- O
pensive, O
cumbersome O
PSG O
for O
sleep B-DAT
evaluation. O
It O
provides O
a O
reliable O

clinical O
decision O
system O
which O
predicts O
sleep B-DAT
quality O
and O
feeling O
re- O
freshed O

in O
detecting O
the O
quality O
of O
sleep B-DAT
demonstrates O
the O
capability O
of O
the O

the O
quality O
of O
daily O
short O
sleep B-DAT
can O
provide O
complementary O
information O
for O

plan O
on O
creating O
a O
personalized O
sleep B-DAT
evalua- O
tion O
systems O
to O
enhance O

the O
performance O
of O
sleep B-DAT
evaluation O
by O
em- O
bedding O
personal O

swsLengthHR O
The O
predicted O
fraction O
of O
sleep B-DAT
belonging O
to O
the O
deep O
sleep O

of O
transitions O
to O
the O
deep O
sleep B-DAT
stage O
according O
to O
the O
heart O

of O
the O
epochs O
in O
a O
sleep B-DAT
to O
the O
total O
possible O
epochs O

number O
of O
epochs O
during O
a O
sleep B-DAT
6 O
stormPeak O
The O
percentage O
of O

of O
times O
people O
awake O
from O
sleep B-DAT
Body O
movement O
12 O
sleepEfficiency O
The O

proportion O
of O
sleep B-DAT
time O
to O
the O
time O
a O

person O
tries O
to O
sleep B-DAT
13 O
amountAwake O
The O
length O
of O

swsTimeMovement O
The O
predicted O
fraction O
of O
sleep B-DAT
belonging O
to O
the O
deep O
sleep O

of O
transitions O
to O
the O
deep O
sleep B-DAT
stage O
according O
to O
the O
body O

17 O
%decreaseMovement O
The O
percentage O
of O
sleep B-DAT
time O
in O
which O
the O
body O

swsLengthTemperature O
The O
predicted O
fraction O
of O
sleep B-DAT
belonging O
to O
the O
deep O
sleep O

of O
transitions O
to O
the O
deep O
sleep B-DAT
stage O
according O
to O
the O
skin O

20 O
%decreaseTemperature O
The O
percentage O
of O
sleep B-DAT
time O
which O
participants O
experienced O
temperature O

4.) O
How O
many O
hours O
of O
sleep B-DAT
did O
you O
get O
last O
night O

one. O
Did O
you O
take O
a O
sleep B-DAT
aid O
last O
night O

Yes, O
an O
over O
the O
counter O
sleep B-DAT
aid O
c.) O
Yes, O
a O
prescription O

sleep B-DAT
aid O

last O
night O
that O
disturbed O
your O
sleep B-DAT

you O
to O
fall O
back O
to O
sleep B-DAT

the O
overall O
quality O
of O
my O
sleep B-DAT
as O
a.) O
Very O
good O
b O

select O
one. O
Did O
lack O
of O
sleep B-DAT
interfere O
with O
your O
personal O
relationships O

of O
napping O
on O
sleepiness B-DAT
and O
sleep O

E.B. O
Klerman, O
C.A. O
Czeisler, O
Irregular O
sleep B-DAT

performance O
and O
delayed O
circadian O
and O
sleep B-DAT

D.-U. O
Jeong, O
K.S. O
Park, O
Slow-wave O
sleep B-DAT
estimation O
for O
healthy O
subjects O
and O

and O
factors O
influen- O
cing O
self-reported O
sleep B-DAT
duration O
and O
quality O
in O
the O

differences O
in O
the O
associations O
between O
sleep B-DAT
beha- O
viors O
and O
all-cause O
mortality O

S. O
Chokroverty, O
Overview O
of O
normal O
sleep, B-DAT
Sleep O
Disorders O
Medicine, O
Springer O

Nazir, O
Human O
immune O
system O
during O
sleep, B-DAT
Am. O
J. O
Clin. O
Exp. O
Immunol O

of O
systematic O
light O
exposure O
on O
sleep B-DAT
and O
sleep O
quality O
in O
a O

R.W. O
Picard, O
Recognizing O
academic O
performance, O
sleep B-DAT
quality, O
stress O
level, O
and O
mental O

E.L. O
Gibbs, O
G.O. O
Matheson, O
Optimizing O
sleep B-DAT
to O
maximize O
perfor- O
mance: O
implications O

Hori, O
H. O
Nishijo, O
Association O
between O
sleep B-DAT
quality O
and O
subjective O
fatigue O
in O

nurses O
with O
good O
and O
poor O
sleep, B-DAT
Sleep O
Vigilance O
2 O
(2018) O
63–69 O

psychiatric O
symptoms: O
effect O
on O
caregiver's O
sleep, B-DAT
J. O
Clin. O
Nurs. O
22 O
(2013 O

C. O
Shelton, O
S.C. O
Mednick, O
Quantifying O
sleep B-DAT
architecture O
dynamics O
and O
individual O
differences O

31] O
K. O
Crowley, O
Sleep O
and O
sleep B-DAT
disorders O
in O
older O
adults, O
Neuropsychol O

hormone O
release: O
relation O
to O
slow-wave O
sleep B-DAT
and O
sleep-waking O
cycles, O
Science O
165 O

growth O
hormone O
rhythm O
in O
men: O
sleep B-DAT
and O
circadian O
influences O
questioned, O
J O

Walsh, O
Enhancement O
of O
slow O
wave O
sleep B-DAT

Deacon, O
Age-related O
reduction O
in O
daytime O
sleep B-DAT
propensity O
and O
nocturnal O
slow O
wave O

sleep, B-DAT
Sleep O
33 O
(2010) O
211–223 O

O. O
Cauli, O
A O
survey O
on O
sleep B-DAT
assessment O
methods, O
PeerJ O
6 O
(2018 O

D.J. O
Mullaney, O
J.C. O
Gillin, O
Automatic O
sleep B-DAT

a O
bayesian O
model O
for O
learning O
sleep B-DAT
patterns O
from O
smartphone O
events, O
PLoS O

sound O
to O
quantify O
snoring O
and O
sleep B-DAT
apnea O
severity O
using O
a O
smartphone O

with O
one O
under-pillow O
sensor O
during O
sleep, B-DAT
Med. O
Biol. O
Eng. O
Comput. O
43 O

Electrodermal O
activity O
asym- O
metry O
in O
sleep B-DAT

rate O
variability O
identifies O
differences O
in O
sleep B-DAT
between O
people O
with O
fibromyalgia O
and O

the O
E4 O
Wristband O
to O
assess O
sleep B-DAT
in O
adults O
with O
and O
without O

Banerjee, O
G. O
Goodman, O
Toward O
sensor-based O
sleep B-DAT
monitoring O
with O
electrodermal O
activity O
measures O

the O
cross-time O
frequency O
domain O
for O
sleep B-DAT
staging O
from O
a O
single-lead O
electrocardiogram O

rate O
variability O
mon- O
itoring O
during O
sleep B-DAT
based O
on O
capacitively O
coupled O
textile O

for O
heart O
rate O
variability O
in O
sleep B-DAT
and O
sleep O
apnea, O
IEEE O
(Inst O

Miglis, O
Autonomic O
dysfunction O
in O
primary O
sleep B-DAT
disorders, O
Sleep O
Med. O
19 O
(2016 O

control O
in O
health O
and O
in O
sleep B-DAT
disorders, O
Principles O
and O
Practice O
of O

T.P. O
Coleman, O
Visualization O
of O
whole-night O
sleep B-DAT
EEG O
from O
2-channel O
mobile O
recording O

device O
reveals O
distinct O
deep O
sleep B-DAT
stages O
with O
differ- O
ential O
electrodermal O

D.Y. O
Kang, O
T.P. O
Coleman, O
In-home O
sleep B-DAT
recordings O
in O
military O
veterans O
with O

of O
wrist O
electrodermal O
activity O
during O
sleep, B-DAT
Int. O
J. O
Psychophysiol. O
94 O
(2014 O

distribution O
of O
body O
movements O
during O
sleep B-DAT
in O
humans, O
Percept. O
Mot. O
Skills O

Salzarulo, O
Body O
movements O
during O
night O
sleep B-DAT
and O

their O
relationship O
with O
sleep B-DAT
stages O
are O
further O
modified O
in O

restless O
legs O
syndrome O
(RLS) O
on O
sleep, B-DAT
Neuropsychiatric O
Dis. O
Treat. O
2 O
(2006 O

J.P. O
Libert, O
Thermal O
regulation O
during O
sleep, B-DAT
Rev. O
Neurol. O
159 O
(2003) O
6S30–4 O

Van O
Someren, O
Skin O
deep: O
enhanced O
sleep B-DAT
depth O
by O

heat O
loss O
and O
slow O
wave O
sleep, B-DAT
Physiol. O
Behav. O
185 O
(2018) O
23–30 O

rate O
variability O
is O
parameter O
and O
sleep B-DAT
stage O
dependent, O
Front. O
Physiol. O
8 O

R. O
Manber, O
Chronotype O
and O
improved O
sleep B-DAT
ef- O
ficiency O
independently O
predict O
depressive O

quality O
of O
life O
questionnaire O
for O
sleep B-DAT
apnea, O
Am. O
J. O
Respir. O
Crit O

stress O
and O
the O
effects O
on O
sleep, B-DAT
fatigue O
and O
cortisol, O
Scand. O
J O

re- O
view O
of O
approaches O
for O
sleep B-DAT
quality O
analysis, O
IEEE O
Access O
7 O

with- O
drawal O
in O
ICUs, O
and O
sleep B-DAT
quality O
prediction O
using O
both O
wearable O

Larry O
W. O
Lawhornec O
a O
Department O
of B-DAT
Computer O
Science O
and O
Engineering, O
Kno.e.sis O

Dayton, O
OH, O
USA O
b O
Department O
of B-DAT
Social O
Work, O
Wright O
State O
University O

Dayton, O
OH, O
USA O
c O
Department O
of B-DAT
Geriatrics, O
Boonshoft O
School O
of O
Medicine O

Keywords: O
Caregivers O
of B-DAT
people O
with O
dementia O
(CPWD) O
Physiological O

Most O
caregivers O
of B-DAT
people O
with O
dementia O
(CPWD) O
experience O

a O
high O
degree O
of B-DAT
stress O
due O
to O
the O
demands O

of B-DAT
providing O
care, O
especially O
when O
addressing O

unpredictable O
behavioral O
and O
psychological O
symptoms O
of B-DAT
dementia. O
Such O
challenging O
responsibilities O
make O

sleep O
quality O
based O
on O
trends O
of B-DAT
physiological O
signals O
in O
the O
deep O

temperature. O
To O
evaluate O
the O
performance O
of B-DAT
the O
proposed O
method, O
analyses O
were O

on O
a O
two- O
week O
period O
of B-DAT
sleep O
monitored O
on O
eight O
CPWD O

forest O
classifier O
with O
an O
accuracy O
of B-DAT
75% O
for O
sleep O
quality, O
and O

measures O
are O
sleep O
efficiency O
(ratio O
of B-DAT
amount O
of O
time O
asleep O
to O

the O
amount O
of B-DAT
time O
in O
bed) O
and O
skin O

analysis O
system O
demonstrate O
the O
capability O
of B-DAT
using O
wearable O
sensors O
to O
measure O

Extensive O
studies O
indicate O
pivotal O
effects O
of B-DAT
sleep O
in O
job O
performance O
[1 O

sical O
health O
[5]. O
The O
discharge O
of B-DAT
anabolic O
hormones O
(e.g. O
prolactin, O
testosterone O

sleepiness, O
performance O
reduction, O
to O
lack O
of B-DAT
attention O
[7]. O
Therefore, O
assessing O
sleep O

Caregivers O
of B-DAT
people O
suffering O
dementia-related O
illnesses O
are O

sleep O
problems O
than O
the O
rest O
of B-DAT
the O
caregiver O
community O
[8]. O
At O

least, O
two-thirds O
of B-DAT
these O
caregivers O
reported O
sleep O
disturbance O

during O
the O
course O
of B-DAT
their O
care O
relationship O
[9]. O
As O

spouse O
or O
other O
family O
members O
of B-DAT
the O
patient O
[10], O
we O
focus O

on O
predicting O
sleep O
quality O
of B-DAT
primary O
CPWD O
in O
this O
paper O

high O
level O
of B-DAT
stress O
[11,12]. O
The O
potential O
for O

sleep O
quality O
increases O
the O
difficulty O
of B-DAT
providing O
quality O
care, O
and O
increases O

risk O
of B-DAT
persistent O
tiredness, O
heart O
disease, O
and O

detrimental O
level. O
To O
the O
best O
of B-DAT
our O
knowledge, O
this O
is O
the O

a O
cost O
savings O
to O
society O
of B-DAT
more O
than O
$470 O
billion O
annually O

assessment O
requires O
a O
comprehensive O
understanding O
of B-DAT
underlying O
sleep O
mechanisms. O
The O
American O

Academy O
of B-DAT
Sleep O
Medicine O
(AASM) O
suggests O
using O

sleep O
disturbance O
by O
continuous O
monitoring O
of B-DAT
multiple O
physiological O
parameters O
during O
sleep O

To O
inexpensively O
evaluate O
sleep O
quality O
of B-DAT
different O
subjects O
sleeping O
in O
their O

and O
classification O
modules. O
The O
performance O
of B-DAT
the O
proposed O
method O
is O
considered O

as O
sleep O
encompasses O
roughly O
one-third O
of B-DAT
human O
life O
[16]. O
A O
wide O

range O
of B-DAT
studies O
explore O
the O
relationship O
between O

relationship O
between O
sleep O
and O
performance O
of B-DAT
students O
[23], O
athletes O
[24], O
and O

this O
section, O
we O
review O
some O
of B-DAT
these O
massive O
studies O
from O
three O

have O
shown O
that O
the O
act O
of B-DAT
providing O
care O
to O
loved O
ones O

can O
experience O
sleep O
difficulties O
because O
of B-DAT
both O
overwhelming O
caregiving O
responsibilities O
[9,27 O

for O
caregivers O
in O
their O
role O
of B-DAT
providing O
care O
to O
their O
loved O

to O
scrutinize O
the O
ar- O
chitecture O
of B-DAT
sleep O
(sleep O
stages) O
[30], O
fluctuations O

of B-DAT
physiological O
signals O
during O
sleep O
[16 O

NREM) O
[16]. O
During O
NREM, O
most O
of B-DAT
the O
physiological O
processes O
in O
terms O

of B-DAT
brain O
activity, O
heart O
rate, O
blood O

NREM O
[6]. O
Since O
about O
80% O
of B-DAT
sleep O
time O
of O
an O
adult O

is O
NREM O
sleep O
[7], O
most O
of B-DAT
the O
sleep O
studies O
focus O
on O

the O
co- O
occurrence O
and O
similarity O
of B-DAT
NREM O
stage O
3 O
and O
stage O

con- O
sidered O
as O
one O
stage O
of B-DAT
slow O
wave O
sleep O
(SWS) O
based O

on O
the O
current O
American O
Academy O
of B-DAT
Sleep O
Medicine O
(AASM) O
scoring O
manual O

14], O
primarily O
on O
the O
basis O
of B-DAT
electroencephalogram O
(EEG) O
criteria O

is O
accompanied O
with O
increasing O
length O
of B-DAT
deep O
sleep O
in O
patients O
suffering O

subjects O
[36]. O
Moreover, O
the O
reduction O
of B-DAT
sleep O
quality O
in O
the O
elderly O

is O
accompanied O
with O
the O
loss O
of B-DAT
deep O
sleep O
[37]. O
Therefore, O
sleep O

classi- O
fying O
the O
sleep O
state O
of B-DAT
participants, O
i.e., O
sleep O
detection O
[38 O

PSG O
utilizes O
an O
exhaustive O
list O
of B-DAT
tests O
such O
as O
EEG, O
respiratory O

monitoring, O
and O
the O
recording O
of B-DAT
sounds/videos. O
PSG O
is O
an O
expensive O

enhance O
the O
performance O
and O
reliability O
of B-DAT
measurements, O
small O
contact-based O
devices O
(e.g O

access O
to O
raw O
time O
series O
of B-DAT
physiolo- O
gical O
signals O
in O
contrast O

between O
the O
length O
and O
transition O
of B-DAT
different O
sleep O
stages, O
especially O
SWS O

and O
the O
quality O
of B-DAT
sleep O
[33–37]. O
This O
relation O
has O

instance O
Refs. O
[49–52], O
demonstrate O
distinguishability O
of B-DAT
sleep O
stages O
based O
on O
the O

and O
intrusively O
effects O
sleep O
quality O
of B-DAT
participants. O
Hence, O
there O
is O
a O

sleep O
quality O
via O
the O
trends O
of B-DAT
physiological O
signals O
which O
are O
effortlessly O

sleep O
quality O
based O
on O
trends O
of B-DAT
physiological O
signals O
in O
deep O
sleep O

applicable O
for O
a O
wide O
range O
of B-DAT
users, O
especially O
for O
individuals O
experiencing O

support O
system O
evaluates O
the O
effectiveness O
of B-DAT
sleep O
according O
to O
various O
physiological O

in O
a O
time-series O
manner O
instead O
of B-DAT
focusing O
on O
certain O
statistical O
mea O

surements O
such O
as O
the O
mean O
of B-DAT
vital O
signals. O
Studies O
using O
features O

such O
as O
the O
mean O
of B-DAT
heart O
rate O
for O
sleep O
stage O

applicable O
for O
a O
wide O
variety O
of B-DAT
subjects, O
tracks O
trends O
of O
physiological O

investigate O
HRV O
with O
the O
aim O
of B-DAT
predicting O
the O
deep O
sleep O
stages O

Fig. O
2. O
The O
outline O
of B-DAT
the O
proposed O
sleep O
analysis O
system O

HRV O
as O
a O
time O
series O
of B-DAT
IBIs O
from O
the O
PPG, O
HRV O

SWS O
during O
the O
sleep. O
One O
of B-DAT
the O
best O
methods O
that O
has O

plot, O
the O
rRR O
(correlation O
coefficient O
of B-DAT
consecutive O
RR O
in- O
tervals) O
is O

computed O
in O
the O
form O
of B-DAT
(1) O
over O
a O
window O
with O

be O
calculated O
again. O
The O
rRR O
of B-DAT
HRV O
in O
time O
t O
over O

a O
series O
of B-DAT
RR O
intervals O
in O
IBI O
can O

HRV O
sampling O
rate) O
number O
of B-DAT
original O
RR O
data O
points, O
the O

series O
will O
be O
a O
transformation O
of B-DAT
HRV, O
which O
allows O
us O
to O

track O
the O
trend O
of B-DAT
HRV O
without O
the O
interference O
of O

units O
below O
the O
mean O
rRR O
of B-DAT
the O
initial O
4 O
h O
for O

3.1.2. O
Electrodermal O
activity O
The O
occurrence O
of B-DAT
intensive O
fluctuations O
in O
brain O
activities O

K-complex O
leads O
to O
a O
burst O
of B-DAT
sympathetic O
activities O
in O
NREM O
[16 O

be O
described O
by O
electrical O
changes O
of B-DAT
skin O
surface, O
called O
electrodermal O
activity O

SWS O
experience O
the O
highest O
level O
of B-DAT
both O
EDA O
values O
[59] O
and O

number O
of B-DAT
local O
EDA O
peaks O
[60]. O
Also O

60], O
exhibits O
the O
stability O
of B-DAT
EDA O
properties O
in O
SWS O
by O

different O
pattern O
and O
varied O
magnitude O
of B-DAT
EDA O
during O
sleep O

60] O
to O
isolate O
the O
tendency O
of B-DAT
EDA O
from O
different O
personal O
basic O

level O
of B-DAT
EDA. O
In O
this O
procedure, O
the O

validated O
algorithm O
of B-DAT
Cole's O
function O
[39] O
over O
the O

Following O
that, O
the O
first O
derivative O
of B-DAT
EDA O
provides O
a O
map O
of O

The O
higher O
the O
intensity O
level O
of B-DAT
fluctuations O
in O
this O
map, O
the O

sleep O
is. O
The O
intensity O
level O
of B-DAT

fluctuation O
is O
defined O
in O
terms O
of B-DAT
the O
occurrence O
of O
peaks O
which O

sleep O
stages, O
however O
the O
frequency O
of B-DAT
their O
occurrence O
in O
SWS O
is O

more O
destructive O
effect O
on O
sleep O
of B-DAT
elderly O
people O
who O
have O
severe O

lower O
SWS O
as O
a O
result O
of B-DAT
aging O
[62 O

Like O
body O
movement O
and O
length O
of B-DAT
SWS, O
body O
temperature O
is O
also O

influenced O
by O
aging. O
Indeed, O
thermoregulation O
of B-DAT
the O
newborn O
remains O
stable O
in O

while O
adults O
experience O
a O
reduction O
of B-DAT
temperature O
during O
their O
deep O
sleep O

64]. O
The O
speed O
and O
amount O
of B-DAT
this O
temperature O
reduction O
has O
a O

longer O
SWS O
as O
an O
index O
of B-DAT
quality O
of O
sleep O
[66 O

This O
method O
highlights O
the O
part O
of B-DAT
sleep O
showing O
a O
declining O
pattern O

focuses O
on O
the O
first O
half O
of B-DAT
the O
sleep O
period O
corresponding O
to O

filter O
with O
the O
windows O
length O
of B-DAT
1 O
min O
is O
conducted O
over O

signals. O
Then, O
the O
positive/negative O
sign O
of B-DAT
slope O
indicates O
the O
increasing/decreasing O
trend O

of B-DAT
the O
time O
series O

Fig. O
4. O
Prediction O
of B-DAT
SWS O
occurrence O
based O
on O
the O

in O
Fig. O
4, O
Poincare O
plot O
of B-DAT
rRR O
contains O
some O
labeled O

assigned O
as O
SWS. O
The O
number O
of B-DAT
occurrences O
of O
these O
seg- O
ments O

feature. O
Also, O
the O
accumulated O
sum O
of B-DAT
the O
length O
of O
these O
segments O

the O
filtering O
method. O
The O
result O
of B-DAT
this O
processed O
signals O
highlights O
portions O

representing O
descending O
trend O
of B-DAT
temperature O
and O
body O
movements. O
Since O

people O
experience O
different O
length O
of B-DAT
sleep O
each O
night, O
we O
con O

- O
sider O
the O
percentage O
of B-DAT
sleep O
time O
when O
the O
time O

series O
of B-DAT
temperature O
and O
body O
movements O
have O

longer O
time O
enhances O
the O
chance O
of B-DAT
experiencing O
more O
and O
longer O
deep O

sleep. O
Therefore, O
we O
use O
amount O
of B-DAT
sleep, O
amount O
of O
awake, O
and O

times O
of B-DAT
awoken O
as O
features. O
Furthermore, O
we O

SE) O
feature O
as O
the O
proportion O
of B-DAT
sleep O
time O
to O
the O
time O

SE O
considers O
the O
side O
effect O
of B-DAT
spending O
too O
much O
time O
in O

used O
to O
track O
the O
fluctuation O
of B-DAT
EDA O
signals. O
Fluctuations O
are O
reported O

in O
terms O
of B-DAT
EDA O
peak O
per O
epoch. O
An O

defined O
as O
a O
30-s O
section O
of B-DAT
EDA O
having O
at O
least O
one O

EDA O
peak. O
The O
highest O
number O
of B-DAT
EDA O
peaks O
occurs O
in O
SWS O

Then, O
we O
compute O
the O
number O
of B-DAT
epochs O
during O
a O
sleep O
period O

counter O
feature. O
Moreover, O
higher O
level O
of B-DAT
EDA O
in O
the O
deep O
sleep O

59] O
results O
in O
higher O
number O
of B-DAT
peaks O
in O
each O
epoch. O
This O

feature O
which O
provides O
the O
mean O
of B-DAT
number O
of O
peaks O
over O
all O

capacity O
feature O
as O
the O
ratio O
of B-DAT
the O
number O
of O
epochs O
in O

Epoch O
capacity O
illustrates O
high O
level O
of B-DAT
EDA O
ac- O
tivity O
during O
the O

storm, O
and O
(v) O
total O
length O
of B-DAT
storm. O
The O
storm O
peak O
expresses O

the O
percentage O
of B-DAT
peaks O
which O
occur O
in O
the O

storms O
with O
a O
large O
number O
of B-DAT
EDA O
peaks O
are O
more O
likely O

two O
features O
represent O
the O
number O
of B-DAT
epochs O
constructing O
the O
largest O
storm O

and O
the O
average O
number O
of B-DAT
epochs O
comprising O
each O
EDA O
storm O

respectively. O
Finally, O
the O
more O
occurrences O
of B-DAT
EDA O
storms O
lead O
to O
a O

higher O
probability O
of B-DAT
experiencing O
deep O
sleep O
and O
it O

is O
worth O
mentioning O
that O
most O
of B-DAT
the O
night O
sleeps O
comprise O
at O

A. O
Selecting O
the O
right O
subset O
of B-DAT
these O
features O
have O
several O
advantages O

, O
such O
as O
faster O
training O
of B-DAT
the O
classification O
algorithm, O
reducing O
the O

complexity O
of B-DAT
models, O
enhancing O
the O
interpretability O
of O

models, O
and O
improving O
the O
accuracy O
of B-DAT
final O
re- O
sults O
[74 O

like O
Pearson’ O
correlation O
or O
analysis O
of B-DAT
variance O
are O
computationally O
simple O
and O

which O
selects O
the O
optimal O
combination O
of B-DAT
features O
while O
maximizing O
the O
performance O

of B-DAT

well O
as O
reducing O
the O
risk O
of B-DAT
overfitting, O
we O
use O
a O
version O

of B-DAT
RFE O
called O
RFECV O
model O
which O

incorporates O
resampling O
of B-DAT
k-fold O
cross-validation O
strategy O
[77 O

nine O
sections O
form O
a O
group O
of B-DAT
records O
used O
to O
train O
classifiers O

features. O
And, O
again O
the O
importance O
of B-DAT
each O
feature O
is O
calculated. O
This O

further O
feature O
remains. O
The O
average O
of B-DAT
learning O
rates O
over O
the O
models O

trained O
using O
the O
same O
number O
of B-DAT
features O
illustrates O
the O
model O
performance O

corresponding O
to O
different O
number O
of B-DAT
features. O
After O
the O
optimal O
subset O

trained O
with O
optimal O
subset O
size O
of B-DAT
features O
are O
considered O
to O
select O

consensus O
ranking O
over O
the O
list O
of B-DAT
feature O
importance O
describes O
the O
final O

fitted O
using O
the O
optimal O
subset O
of B-DAT
features O
in O
the O
training O
set O

the O
best O
classifier O
in O
terms O
of B-DAT
performance, O
transparency, O
and O
computational O
complexity O

tries O
to O
predict O
the O
probability O
of B-DAT
different O
states O
of O
the O
outcome O

repeatedly O
using O
several O
bootstrapped O
subsets O
of B-DAT
the O
data O
and O
averaging O
models O

our O
experiments O
for O
a O
period O
of B-DAT
two O
weeks O
each. O
However, O
using O

able O
to O
record O
100 O
nights O
of B-DAT
sleep O
(as O
opposed O
to O
112 O

missing O
nights O
was O
that O
some O
of B-DAT
the O
caregivers O
accidently O
forgot O
to O

and O
two O
male O
caregivers. O
Two O
of B-DAT
the O
caregivers O
were O
adult O
children O

of B-DAT
dementia O
patients O
and O
six O
were O

Dementia-related O
symptoms O
caused O
the O
majority O
of B-DAT
caregivers’ O
to O
experience O
median O
and O

high O
level O
of B-DAT
caregiver O

the O
study, O
a O
team O
consisting O
of B-DAT
a O
social O
worker O
and O
a O

index O
(PSQI) O
[82], O
functional O
outcomes O
of B-DAT
sleep O
questionnaire O
(FOSQ) O
[83], O
Calgary O

sleep O
apnea O
quality O
of B-DAT
life O
index O
(SAQLI) O
[84], O
and O

85]. O
A O
qualitative O
focus O
group O
of B-DAT
geriatric O
experts O
refined O
and O
streamlined O

model O
[86] O
on O
a O
cohort O
of B-DAT
(N O
= O
24) O
participants O

in O
Subsection O
3.1. O
The O
efficiency O
of B-DAT
our O
proposed O
decision O
support O
system O

To O
avoid O
bias O
in O
measurement O
of B-DAT
sleep O
quality, O
the O
sleep O
survey O

table, O
questions O
9 O
and O
10 O
of B-DAT
the O
survey O
attempt O
to O
appraise O

the O
effectiveness O
of B-DAT
sleep O
through O
“overall O
quality O
of O

categorical O
variables O
represent O
different O
aspects O
of B-DAT
the O
sleep O
quality O
of O
caregivers O

quality” O
illustrates O
the O
overall O
satisfaction O
of B-DAT
caregivers O
from O
their O
sleep O
which O

13]. O
However, O
the O
stressful O
task O
of B-DAT
providing O
care O
for O
dementia O
patients O

destructive O
effect O
in O
the O
process O
of B-DAT
fatigue O
recovery O
[87]. O
Therefore, O
we O

view O
about O
the O
sleep O
quality O
of B-DAT
CPWD. O
The O
interaction O
can O
be O

each O
cell O
counts O
joint O
co-occurrence O
of B-DAT
two O
variables O
in O
certain O
circumstances O

Fig. O
5 O
denotes O
each O
cell O
of B-DAT
the O
contingency O
table O
in O
a O

shape O
such O
that O
the O
area O
of B-DAT
each O
rectangle O
represents O
the O
value O

tingency O
table O
measures O
the O
departure O
of B-DAT
each O
two O
random O
variables O
from O

to O
reject O
the O
null O
hypothesis O
of B-DAT
chi-square O
test O
(independence). O
In O
contrast O

Table O
1 O
Background O
information O
of B-DAT
CPWD O

8 O
6 O
7 O
3 O
Level O
of B-DAT
dementia O
in O
loved O
one O
(Mild O

Fig. O
5. O
The O
relation O
of B-DAT
Sleep O
Quality O
and O
the O
mood O

of B-DAT
caregivers O
using O
a O
mosaic O
plot O

mosaic O
plot O
shows O
the O
distribution O
of B-DAT
caregivers’ O
mood O
versus O
their O
sleep O

also O
shows O
the O
marginal O
distribution O
of B-DAT
sleep O
quality O
(the O
width O
of O

the O
bars). O
Similarly, O
the O
length O
of B-DAT
cells O
with O
respect O
to O
mood O

attributes O
indicates O
65% O
of B-DAT
caregivers O
still O
feel O
tired O
after O

sleep. O
Almost O
all O
of B-DAT
the O
caregivers O
who O
felt O
rested O

On O
the O
other O
hand, O
17% O
of B-DAT
caregiver O
sleep O
records O
are O
associated O

bad O
and O
fairly O
bad O
quality O
of B-DAT
sleep O

by O
Pearson O
residuals O
in O
terms O
of B-DAT
different O
colors O
which O
measure O
the O

departure O
of B-DAT
each O
cell O
from O
independence. O
For O

a) O
indicates O
a O
strong O
lack O
of B-DAT
probability O
of O
the O
caregiver O
feeling O

bad O
or O
fairly O
bad O
quality O
of B-DAT
sleep O

imbalanced O
data O
set. O
The O
distribution O
of B-DAT
sleep O
quality O
displayed O
in O
Table O

quality. O
While O
over O
1 O
out O
of B-DAT
4 O
records O
are O
assigned O
to O

categories O
comprise O
38% O
and O
17% O
of B-DAT
the O
data, O
respectively O

cause O
these O
interruptions. O
Special O
needs O
of B-DAT
dementia O
patients O
to O
use O
the O

among O
caregivers’ O
sleep O
since O
98% O
of B-DAT
the O
records O
reported O
a O
sleep O

h. O
This O
reduces O
the O
chance O
of B-DAT
having O
deep O
sleep O
among O
this O

be O
the O
reason O
why O
65% O
of B-DAT
records O
reported O
feeling O
tired O
after O

sleep. O
However, O
82% O
of B-DAT
them O
expressed O
the O
“Ok”, O
“Good O

presence O
of B-DAT
disrupted O
sleep O
is O
a O
routine O

This O
low O
number O
of B-DAT
bad O
sleep O
quality O
can O
be O

coming O
accustomed O
to O
the O
difficulties O
of B-DAT
dealing O
with O
dementia O
patients O
[9 O

]. O
A O
low O
number O
of B-DAT
samples O
reduces O
the O
ability O
of O

method O
to O
learn O
the O
patterns O
of B-DAT
physiological O
signals. O
There- O
fore, O
we O

combine O
two O
categories O
of B-DAT
bad O
and O
fairly O
bad O
as O

a O
single O
ca- O
tegory O
of B-DAT
bad O
sleep O
quality O
for O
further O

processing. O
However, O
the O
issue O
of B-DAT
an O
imbalanced O
data O
set O
remains O

since O
the O
number O
of B-DAT
records O
in O
the O
class O
of O

than O
two O
times O
the O
number O
of B-DAT
records O
in O
bad O
and O
very O

no O
assumptions O
about O
the O
distribution O
of B-DAT
re- O
cords O
which O
enable O
them O

the O
data O
set O
in O
terms O
of B-DAT
sleep O
quality, O
we O
apply O
the O

Thus, O
we O
evaluate O
the O
performance O
of B-DAT
the O
proposed O
method O
from O
different O

the O
decision O
making, O
and O
integration O
of B-DAT
background O
knowledge O

to O
the O
highest O
perfor- O
mance O
of B-DAT
classifiers O
since O
it O
supplies O
each O

The O
overall O
accuracy O
of B-DAT
classifiers O
is O
highly O
sensitive O
to O

the O
number O
of B-DAT
input O
features. O
This O
sensitivity O
may O

be O
the O
result O
of B-DAT
nonlinear O
rela- O
tions O
among O
predictors O

increasing O
trend O
with O
the O
growth O
of B-DAT
number O
of O
features O
selected O
by O

RFECV. O
RFECV O
utilizes O
the O
importance O
of B-DAT
features O
in O
each O
classifier O
to O

20! O
(20 O
factorial) O
possible O
subsets O
of B-DAT
features O
to O
only O
20 O
cases O

To O
evaluate O
reliability O
of B-DAT
best O
predicted O
models, O
we O
need O

to O
scruti- O
nize O
performance O
of B-DAT
classifiers O
in O
terms O
of O
sensitivity O

systems. O
Sensitivity O
computes O
the O
probability O
of B-DAT
detecting O
the O
desired O
condition O
in O

whereas O
specificity O
is O
the O
ability O
of B-DAT
the O
assistant O
system O
to O
correctly O

metric O
which O
reports O
the O
fraction O
of B-DAT
truly O
detected O
records O
among O
all O

since O
faulty O
detections O
impose O
series O
of B-DAT
medical O
observations O

91]. O
Fi- O
nally, O
the O
average O
of B-DAT
all O
values O
per O
metric O
is O

2 O
Questions O
9 O
and O
10 O
of B-DAT
sleep O
survey O

would O
rate O
the O
overall O
quality O
of B-DAT
my O
sleep O
as O
a.) O
Very O

Table O
3 O
Contingency O
table O
of B-DAT
Sleep O
quality O
and O
rest O
feeling O

to O
performance, O
respectively. O
The O
order O
of B-DAT
classifiers’ O
performance O
reveals O
there O
is O

proposed O
method O
can O
detect O
records O
of B-DAT
tiredness O
in O
caregivers O
with O
high O

the O
provided O
features. O
The O
order O
of B-DAT
these O
features O
is O
dedicated O
by O

is O
the O
observed O
fraction O
of B-DAT
samples O
in O
the O
node O
belonging O

the O
sum O
over O
the O
number O
of B-DAT
splits O
across O
all O
trees O
that O

the O
feature O
and O
the O
denominator O
of B-DAT
the O
number O
of O
samples O
it O

model O
selects O
a O
lower O
number O
of B-DAT
features O
for O
making O
decision O
rather O

fact O
is O
as O
a O
result O
of, B-DAT
the O
feeling O
refreshed O
model O
having O

lower O
varia- O
bility O
of B-DAT
possible O
cases O
rather O
than O
sleep O

dis- O
turbance O
contributes O
to O
fragmentation O
of B-DAT
sleep O
which O
is O
a O
common O

Table O
4 O
Comparison O
of B-DAT
the O
performance O
of O
classifiers O
with O

sleep O
efficiency, O
and O
mean O
range O
of B-DAT
sleep O
stages, O
in O
caregivers’ O
sleep O

its O
difference O
from O
the O
sleep O
of B-DAT
non-caregivers O
have O
been O
investigated. O
However O

, O
none O
of B-DAT
them O
endeavor O
to O
predict O
the O

sleep O
quality O
of B-DAT
caregivers. O
To O
the O
best O
of O

specifically O
in O
caregivers. O
The O
majority O
of B-DAT
previous O
works O
[23–28] O
investigated O
sleep O

using O
a O
monthly O
survey O
of B-DAT
PSQI O
[82]. O
However, O
in O
this O

we O
used O
the O
daily O
survey O
of B-DAT
DUCSS O
[81] O
for O
assessing O
each O

that O
sleep O
efficiency O
is O
one O
of B-DAT
the O
most O
important O
factors O
in O

using O
wearable O
devices, O
two O
works O
of B-DAT
[23,95] O
studied O
estimation O
of O
sleep O

These O
studies O
predicted O
sleep O
quality O
of B-DAT
high O
school O
[23] O
and O
undergraduate O

work O
burden O
(daily O
ac- O
tivity) O
of B-DAT
the O
participants O
as O
an O
input O

Both O
studies O
reported O
accuracy O
scores O
of B-DAT
89% O
without O
discussing O
the O
role O

of B-DAT
in- O
dividual O
features O

Evaluation O
of B-DAT
sleep O
quality O
and O
feeling O
refreshed O

the O
easy-to-use O
E4 O
wristband O
instead O
of B-DAT
the O
ex- O
pensive, O
cumbersome O
PSG O

body O
movement O
signals. O
The O
accuracy O
of B-DAT
75% O
in O
detecting O
the O
quality O

of B-DAT
sleep O
demonstrates O
the O
capability O
of O
the O
proposed O
method O

various O
approaches. O
Investigating O
the O
quality O
of B-DAT
daily O
short O
sleep O
can O
provide O

systems O
to O
enhance O
the O
performance O
of B-DAT
sleep O
evaluation O
by O
em- O
bedding O

Conflicts O
of B-DAT
interest O

supported O
by O
the O
National O
Institutes O
of B-DAT
Health O
under O
Grant O
no O
K01 O

in O
this O
material O
are O
those O
of B-DAT
the O
author(s) O
and O
do O
not O

necessarily O
reflect O
the O
views O
of B-DAT
the O
NIH O

Appendix O
A. O
The O
list O
of B-DAT
features O

1 O
swsLengthHR O
The O
predicted O
fraction O
of B-DAT
sleep O
belonging O
to O
the O
deep O

variability O
2 O
swsTimeHR O
The O
number O
of B-DAT
transitions O
to O
the O
deep O
sleep O

signal O
3 O
epochCapacity O
The O
ratio O
of B-DAT
the O
number O
of O
the O
epochs O

4 O
epochPeak O
The O
mean O
number O
of B-DAT
peaks O
in O
all O
epochs O
5 O

epochPeakCounter O
The O
number O
of B-DAT
epochs O
during O
a O
sleep O
6 O

stormPeak O
The O
percentage O
of B-DAT
peaks O
which O
occur O
in O
the O

storms O
7 O
largestStorm O
The O
number O
of B-DAT
epochs O
that O
construct O
the O
largest O

storm O
8 O
timesEdaStorm O
The O
number O
of B-DAT
distinct O
storms O
9 O
meanEdaStorm O
The O

average O
number O
of B-DAT
epochs O
comprising O
each O
EDA O
storm O

10 O
lengthEdaStorm O
The O
number O
of B-DAT
whole O
epochs O
shaping O
the O
storms O

11 O
timesAwoken O
The O
number O
of B-DAT
times O
people O
awake O
from O
sleep O

movement O
12 O
sleepEfficiency O
The O
proportion O
of B-DAT
sleep O
time O
to O
the O
time O

sleep O
13 O
amountAwake O
The O
length O
of B-DAT
night O
time O
subjects O
are O
awake O

14 O
amountAsleep O
The O
length O
of B-DAT
time O
subjects O
are O
asleep O
15 O

swsTimeMovement O
The O
predicted O
fraction O
of B-DAT
sleep O
belonging O
to O
the O
deep O

movement O
16 O
swsLengthMovement O
The O
number O
of B-DAT
transitions O
to O
the O
deep O
sleep O

movement O
17 O
%decreaseMovement O
The O
percentage O
of B-DAT
sleep O
time O
in O
which O
the O

18 O
swsLengthTemperature O
The O
predicted O
fraction O
of B-DAT
sleep O
belonging O
to O
the O
deep O

Temperature O
19 O
swsTimeTemperature O
The O
number O
of B-DAT
transitions O
to O
the O
deep O
sleep O

temperature O
20 O
%decreaseTemperature O
The O
percentage O
of B-DAT
sleep O
time O
which O
participants O
experienced O

9:00 O
4.) O
How O
many O
hours O
of B-DAT
sleep O
did O
you O
get O
last O

would O
rate O
the O
overall O
quality O
of B-DAT
my O
sleep O
as O
a.) O
Very O

Please O
select O
one. O
Did O
lack O
of B-DAT
sleep O
interfere O
with O
your O
personal O

Please O
rate O
your O
overall O
quality O
of B-DAT
your O
day O
yesterday. O
a.) O
Very O

at O
a O
volunteer O
position O
because O
of B-DAT
your O
sleepiness? O
a.) O
No O
b O

J.S. O
Ruggiero, O
N.S. O
Redeker, O
Effects O
of B-DAT
napping O
on O
sleepiness O
and O
sleep-related O

2012. O
[7] O
S. O
Chokroverty, O
Overview O
of B-DAT
normal O
sleep, O
Sleep O
Disorders O
Medicine O

Schulz, O
J. O
Eden, O
National O
Academies O
of B-DAT
Sciences, O
Medicine, O
Family O
Caregiving O

of B-DAT
persons O
with O
dementia: O
contributing O
factors O

health O
interventions O
for O
family O
caregivers O
of B-DAT
elderly O
individuals: O
a O
scoping O
review O

C.C. O
Lee, O
Enhancing O
the O
quality O
of B-DAT
life O
of O
de- O
mentia O
caregivers O

L. O
Teri, O
Insomnia O
in O
caregivers O
of B-DAT
persons O
with O
dementia: O
who O
is O

AASM O
Manual O
for O
the O
Scoring O
of B-DAT
Sleep O
and O
Associated O
Events: O
Rules O

and O
Technical O
Specifications, O
American O
Academy O
of B-DAT
Sleep O
Medicine, O
Westchester, O
IL, O
2007 O

obesity, O
insulin O
resistance, O
and O
risk O
of B-DAT
type O
2 O
diabetes, O
Metabolism O
84 O

Mandal, O
S. O
Ghosh, O
High-accuracy O
detection O
of B-DAT
early O
Parkinson's O
disease O
through O
multimodal O

Prashanth, O
S.D. O
Roy, O
Early O
detection O
of B-DAT
Parkinson's O
disease O
through O
patient O
questionnaire O

failure O
patients: O
an O
assess- O
ment O
of B-DAT
patient O
use O
and O
perception O
of O

Vega, O
K. O
Bovbjerg, O
The O
effect O
of B-DAT
systematic O
light O
exposure O
on O
sleep O

quality O
in O
a O
mixed O
group O
of B-DAT
fatigued O
cancer O
survivors, O
J. O
Clin O

Grant, O
Sleep O
in O
spousal O
caregivers O
of B-DAT
people O
with O
Alzheimer's O
disease, O
Sleep O

prospective O
study O
of B-DAT
the O
effects O
of O
behavioral O
symptoms O
on O
the O
institutionalization O

of B-DAT
patients O
with O
dementia, O
Int. O
Psychogeriatr O

36] O
J.K. O
Walsh, O
Enhancement O
of B-DAT
slow O
wave O
sleep: O
implications O
for O

Thomas, O
M.A. O
Jan, O
Clinical O
value O
of B-DAT
polysomnography, O
The O
Lancet O
339 O
(1992 O

severity O
using O
a O
smartphone: O
proof B-DAT
of O
concept, O
J. O
Clin. O
Sleep O
Med O

Kitamura, O
K. O
Yamakoshi, O
Unconstrained O
detection O
of B-DAT
respiration O
rhythm O
and O
pulse O
rate O

Najarian, O
D. O
Clauw, O
(317) O
Feasibility O
of B-DAT
the O
E4 O
Wristband O
to O
assess O

I. O
Fietze, O
C. O
Schöbel, O
Modulations O
of B-DAT
heart O
rate, O
ECG, O
and O
cardio O

J.-H. O
Peter, O
A. O
Bunde, O
Comparison O
of B-DAT
de- O
trended O
fluctuation O
analysis O
and O

sleep O
disorders, O
Principles O
and O
Practice O
of B-DAT
Sleep O
Medicine, O
fifth O
ed., O
Elsevier O

D.Y. O
Kang, O
T.P. O
Coleman, O
Visualization O
of B-DAT
whole-night O
sleep O
EEG O
from O
2-channel O

Picard, O
R. O
Stickgold, O
Quantitative O
analysis O
of B-DAT
wrist O
electrodermal O
activity O
during O
sleep O

H. O
Schulz, O
Rate O
and O
distribution O
of B-DAT
body O
movements O
during O
sleep O
in O

63] O
R.K. O
Bogan, O
Effects O
of B-DAT
restless O
legs O
syndrome O
(RLS) O
on O

IEEE O
38th O
Annual O
International O
Conference O
of B-DAT
the, O
IEEE, O
2016, O
pp. O
3478–3481 O

M. O
Wilhelm, O
P. O
Achermann, O
Reproducibility O
of B-DAT
heart O
rate O
variability O
is O
parameter O

R.W. O
Picard, O
Toward O
a O
Taxonomy O
of B-DAT
Autonomic O
Sleep O
Patterns O
with O
Electrodermal O

Inza, O
P. O
Larrañaga, O
A O
review O
of B-DAT
feature O
selection O
techniques O
in O
bioin O

Interdisciplinary O
Development O
of B-DAT
a O
Daily O
Use O
Caregiver O
Sleep O

functional O
status O
outcomes O
for O
disorders O
of B-DAT
excessive O
sleepiness, O
Sleep O
20 O
(1997 O

Ward O
Flemons, O
M.A. O
Reimer, O
Development O
of B-DAT
a O
disease-specific O
health-related O
quality O
of O

Kecklund, O
T. O
Akerstedt, O
Different O
levels O
of B-DAT
work-related O
stress O
and O
the O
effects O

Early O
hospital O
mortality O
pre- O
diction O
of B-DAT
intensive O
care O
unit O
patients O
using O

Rifkin, O
A. O
Klautau, O
In O
defense O
of B-DAT
one-vs-all O
classification, O
J. O
Mach. O
Learn O

Understanding O
variable O
importances O
in O
forests O
of B-DAT
randomized O
trees, O
Advances O
in O
Neural O

Wiens, O
Learning O
credible O
models, O
Proceedings O
of B-DAT
the O

T. O
Penzel, O
A O
re- O
view O
of B-DAT
approaches O
for O
sleep O
quality O
analysis O

Sadeghi O
is O
a O
PhD O
candidate O
of B-DAT
computer O
science O
and O
graduate O
research O

machine O
learning O
in O
the O
fields O
of B-DAT
healthcare, O
especially O
in O
elder O
care O

Assistant O
Professor B-DAT
in O
the O
Department O
of O
Computer O
Science O
and O
Engineering, O
with O

ap- O
pointment O
at O
the O
Department O
of B-DAT
Geriatrics, O
Boonshoft O
School O
of O
Medicine O

Features O
in O
the O
special O
issue O
of B-DAT
Women O
in O
Science, O
2018). O
Her O

areas O
of B-DAT
expertise O
include O
multimodal O
data O
fusion O

such O
as O
in O
the O
areas O
of B-DAT
gerontology O
and O
sickle O
cell O
disease O

social O
work O
from O
the O
University O
of B-DAT
Utah O
(2012), O
Her O
Master O
of O

University O
(1995) O
and O
her O
Bachelors O
of B-DAT
Science O
in O
Art O
Therapy O
from O

is O
currently O
an O
Assistant O
Professor B-DAT
of O
Social O
Work O
and O
program O
director O

of B-DAT
the O
Bachelors O
of O
social O
work O
program O
at O
Wright O

is O
Professor B-DAT
and O
former O
Chairman O
of O
the O
Department O
of O
Geriatrics O
at O

Wright O
State O
University O
Boonshoft B-DAT
School O
of O
Medicine O
in O
Dayton, O
Ohio. O
Prior O

in O
2006, O
he O
was O
Professor B-DAT
of O
Family O
Practice O
and O
Director O
of O

the O
Geriatric O
Center O
of B-DAT
Michigan O
at O
Michigan O
State O
University's O

College O
of B-DAT
Human O
Medicine O
in O
East O
Lansing O

medical O
degree O
from O
the O
University O
of B-DAT
Virginia O
in O
Charlottesville O
and O
completed O

residency O
at O
the O
University O
of B-DAT
Iowa O
in O
Iowa O
City. O
He O

He O
has O
received O
a O
number O
of B-DAT
teaching O
awards, O
including O
teacher O
of O

committee O
to O
incorporate O
the O
principles O
of B-DAT
geriatric O
medicine O
into O
the O
medical O

to O
train O
a O
new O
generation O
of B-DAT
geriatricians O

Conflicts O
of B-DAT
interest O

The O
list O
of B-DAT
features O

8 B-DAT

in O
NREM O
[6]. O
Since O
about O
80 B-DAT

80 B-DAT

81 B-DAT

82 B-DAT

85 B-DAT

82 B-DAT

83 B-DAT

84 B-DAT

85 B-DAT

82 B-DAT

85 B-DAT

86 B-DAT

87 B-DAT

88 B-DAT

3 O
4 O
5 O
6 O
7 O
8 B-DAT

1–10) O
6 O
4 O
4 O
6 O
8 B-DAT
6 O
7 O
3 O
Level O
of O

feeling O
tired O
after O
sleep. O
However, O
82 B-DAT

89 B-DAT

83 B-DAT
0.75 O

84 B-DAT

82 B-DAT

81 B-DAT

studies O
reported O
accuracy O
scores O
of O
89 B-DAT

that O
construct O
the O
largest O
storm O
8 B-DAT
timesEdaStorm O
The O
number O
of O
distinct O

you O
get O
last O
night? O
E.g. O
8 B-DAT
5.) O
How O
many O
hours O
total O

8 B-DAT

8 B-DAT

of O
type O
2 O
diabetes, O
Metabolism O
84 B-DAT
(2018) O
56–66 O

87 B-DAT

Distributed O
Sens. O
Netw. O
11 O
(2015) O
875371 B-DAT

NBC O
2017, O
Springer, O
2017, O
pp. O
835 B-DAT

838 B-DAT

81 B-DAT

sleep O
stage O
dependent, O
Front. O
Physiol. O
8 B-DAT
(2017) O
1100 O

80 B-DAT

81 B-DAT

82 B-DAT

83 B-DAT

excessive O
sleepiness, O
Sleep O
20 O
(1997) O
835 B-DAT

843 B-DAT

84 B-DAT

85 B-DAT

86 B-DAT

87 B-DAT

88 B-DAT

89 B-DAT

can O
vary O
between O
60 O
and O
100 B-DAT
beats O
per O
minute O
(BPM). O
Our O

were O
only O
able O
to O
record O
100 B-DAT
nights O
of O
sleep O
(as O
opposed O

17 O
38 O
27 O
16 O
2 O
100 B-DAT

Ref. O
[94] O
evaluated O
more O
than O
100 B-DAT
sleep O
quality O
analysis O
papers O
published O

Sleep O
quality O
prediction O
in O
caregivers B-DAT
using O
physiological O
signals O

Sleep O
quality O
prediction O
in O
caregivers B-DAT
using O
physiological O
signals O
Reza O
Sadeghia O

Most O
caregivers B-DAT
of O
people O
with O
dementia O
(CPWD O

dementia. O
Such O
challenging O
responsibilities O
make O
caregivers B-DAT
susceptible O
to O
poor O
sleep O
quality O

their O
overall O
health. O
Hence, O
monitoring O
caregivers B-DAT

caregiving, O
is O
beneficial O
to O
both O
caregivers B-DAT
and O
loved O
ones O
with O
dementia-related O

At O
least, O
two-thirds O
of O
these O
caregivers B-DAT
reported O
sleep O
disturbance O
during O
the O

relationship O
[9]. O
As O
most O
primary O
caregivers B-DAT
are O
unpaid O
spouse O
or O
other O

monitoring O
can O
be O
beneficial O
for O
caregivers B-DAT
to O
identify O
potential O
concerns O
and O

studies O
from O
three O
different O
aspects: O
caregivers B-DAT

sleep O
makes O
it O
challenging O
for O
caregivers B-DAT
in O
their O
role O
of O
providing O

To O
evaluate O
caregivers B-DAT

IRB-approved O
study, O
we O
invited O
eight O
caregivers B-DAT
to O
partici- O
pate O
in O
our O

was O
that O
some O
of O
the O
caregivers B-DAT
accidently O
forgot O
to O
wear O
the O

six O
female O
and O
two O
male O
caregivers B-DAT

. O
Two O
of O
the O
caregivers B-DAT
were O
adult O
children O
of O
dementia O

described O
in O
Table O
1, O
most O
caregivers B-DAT
reported O
that O
their O
loved O
one O

symptoms O
caused O
the O
majority O
of O
caregivers B-DAT

qualitative O
interview O
with O
the O
caregivers B-DAT

data O
with O
respect O
to O
the O
caregivers B-DAT

to O
address O
privacy O
concerns. O
The O
caregivers B-DAT
were O
requested O
to O
wear O
the O

intended O
for O
frequent O
use O
or O
caregivers B-DAT

of O
the O
sleep O
quality O
of O
caregivers B-DAT

illustrates O
the O
overall O
satisfaction O
of O
caregivers B-DAT
from O
their O
sleep O
which O
has O

Quality O
and O
the O
mood O
of O
caregivers B-DAT
using O
a O
mosaic O
plot. O
The O

plot O
shows O
the O
distribution O
of O
caregivers B-DAT

mood O
attributes O
indicates O
65% O
of O
caregivers B-DAT
still O
feel O
tired O
after O
sleep O

. O
Almost O
all O
of O
the O
caregivers B-DAT
who O
felt O
rested O
after O
sleep O

not O
en- O
ough O
evidence O
that O
caregivers B-DAT
who O
feel O
tired O
after O
sleep O

some O
special O
attributes. O
For O
instance, O
caregivers B-DAT
are O
usually O
deprived O
from O
sleep O

are O
some O
examples. O
Also, O
family O
caregivers B-DAT
are O
usually O
older O
adults O
themselves O

is O
a O
common O
pattern O
among O
caregivers B-DAT

sleep O
is O
a O
routine O
among O
caregivers B-DAT
[9]. O
Table O
3 O
indicates O
only O

quality O
can O
be O
related O
to O
caregivers B-DAT
be- O
coming O
accustomed O
to O
the O

detect O
records O
of O
tiredness O
in O
caregivers B-DAT
with O
high O
probability. O
As O
a O

reliable O
system O
to O
alarm O
the O
caregivers B-DAT
about O
the O
potential O
increase O
in O

factor O
for O
sleep O
complaints O
in O
caregivers B-DAT
[9]. O
Since O
im- O
portance O
features O

range O
of O
sleep O
stages, O
in O
caregivers B-DAT

caregivers B-DAT
have O
been O
investigated. O
However, O
none O

predict O
the O
sleep O
quality O
of O
caregivers B-DAT

specifically O
in O
caregivers B-DAT

factors O
for O
sleep O
complains O
in O
caregivers B-DAT
[9,27 O

with O
important O
tasks O
such O
as O
caregivers B-DAT
are O
necessary. O
This O
paper O
proposes O

are O
grateful O
to O
all O
the O
caregivers B-DAT
who O
participated O
in O
our O
study O

M.V. O
Vitiello, O
Sleep O
disturbances O
in O
caregivers B-DAT

Web-based O
health O
interventions O
for O
family O
caregivers B-DAT
of O
elderly O
individuals: O
a O
scoping O

of O
life O
of O
de- O
mentia O
caregivers B-DAT
from O
different O
ethnic O
or O
racial O

Vitiello, O
L. O
Teri, O
Insomnia O
in O
caregivers B-DAT
of O
persons O
with O
dementia: O
who O

I. O
Grant, O
Sleep O
in O
spousal O
caregivers B-DAT
of O
people O
with O
Alzheimer's O
disease O

ferences O
between O
older O
adult O
dementia O
caregivers B-DAT
and O
older O
adult O
noncaregivers O
using O

Sleep O
quality O
prediction O
in O
caregivers B-DAT
using O
physiological O
signals O

only O
able O
to O
record O
100 O
nights B-DAT
of O
sleep O
(as O
opposed O
to O

112 O
nights B-DAT

main O
reason O
for O
the O
missing O
nights B-DAT
was O
that O
some O
of O
the O

month O
[95] O
consecutive O
days O
and O
nights, B-DAT
respectively. O
However, O
we O
asked O
our O

broad O
community O
oversight, O
similar O
to O
Wikipedia, B-DAT
ensures O
a O
higher O
data O
quality O

We O
use O
the O
complete O
English O
Wikipedia B-DAT
corpus O
to O
generate O
training O
and O

evaluation O
data. O
Wikipedia B-DAT
and O
Wikidata O
are O
tightly O
integrated O

mapping O
between O
Wikidata O
entities O
and O
Wikipedia B-DAT
articles. O
For O
example O

are O
not O
covered O
by O
the O
Wikipedia B-DAT
annotations O
(e.g. O
Obama O
in O
the O

is O
beneficial O
to O
consider O
other O
relations B-DAT
in O
the O
sentential O
context O
while O

construct O
a O
dataset O
of O
multiple O
relations B-DAT
per O
sentence O
and O
to O
evaluate O

on O
a O
held-out O
set O
of O
relations B-DAT

get O
entities O
and O
disregard O
other O
relations B-DAT
that O
might O

novel O
architecture O
that O
considers O
other O
relations B-DAT
in O
the O
sentence O
as O
a O

We O
use O
the O
term O
context O
relations B-DAT
to O
refer O
to O
them O
throughout O

jointly O
learn O
representations O
for O
all O
relations B-DAT
in O
a O
single O
sentence. O
The O

and O
representations O
of O
the O
context O
relations B-DAT
are O
combined O
to O
make O
the O

net- O
work O
architecture O
for O
extracting O
relations B-DAT
between O
an O
entity O
pair O
that O

takes O
into O
account O
other O
relations B-DAT
in O
the O
sentence O

parses O
to O
represent O
the O
target O
relations B-DAT

entities O
and O
dis- O
regard O
other O
relations B-DAT
that O
might O
be O
present O
in O

sentence. O
Our O
method O
uses O
context O
relations B-DAT
to O
predict O
the O
target O
relation O

no O
restrictions O
as O
to O
what O
relations B-DAT
can O
appear O
together. O
Instead O
we O

encode O
all O
relations B-DAT
in O
the O
same O
context O
into O

entities O
and O
a O
P-prefix O
for O
relations B-DAT

we O
need O
to O
have O
multiple O
relations B-DAT
per O
sentence O
for O
training O
(see O

the O
training O
set: O
79.5% O
of O
relations B-DAT
in O
those O
sentences O
were O
cor O

2: O
Incorporation O
of O
the O
context O
relations B-DAT

target O
entity O
pair O
other O
context O
relations B-DAT
in O
the O
same O
sentence O
are O

construct O
a O
set O
of O
context O
relations B-DAT
by O
taking O
each O
possible O
pair O

limit O
the O
maximum O
number O
of O
relations B-DAT
in O
a O
sentence O
to O
7 O

on O
the O
target O
and O
context O
relations B-DAT
(see O
Figure O
2). O
That O
ensures O

representation O
for O
target O
and O
context O
relations B-DAT
are O
learned O
jointly. O
We O
sum O

the O
models O
that O
use O
context O
relations B-DAT
on O
the O
same O
data. O
Following O

recall O
(R) O
for O
the O
top O
relations B-DAT

give O
equal O
weights O
to O
all O
relations B-DAT
in O
the O
dataset. O
Figure O
4 O

informa- O
tion O
from O
the O
context O
relations B-DAT

be O
especially O
useful O
for O
taxonomy O
relations B-DAT
(see O
SUBCLASS O
OF, O
PART O
OF O

that O
takes O
into O
account O
other O
relations B-DAT
from O
the O
same O
context. O
We O

petitive O
baselines O
that O
these O
context O
relations B-DAT
are O
beneficial O
for O
relation O
extraction O

for O
information O
extraction O
of O
overlapping O
relations B-DAT

a O
Fashion O
Graph O
based O
on O
Polyvore B-DAT
dataset, O
where O
each O
node O
represents O

outfits. O
Experiment O
results O
on O
the O
Polyvore B-DAT
dataset O
demon- O
strate O
the O
great O

results O
on O
a O
real-world O
dataset O
(Polyvore B-DAT
dataset) O
demonstrate O
the O
great O
superiority O

3.1 O
Polyvore B-DAT
Dataset O
Polyvore O
dataset O
released O
by O
[9] O
was O

a O
popular O
fash- O
ion O
website O
Polyvore B-DAT

Examples O
of O
outfit O
composition O
on O
Polyvore B-DAT

covering O
380 O
categories O
in O
the O
Polyvore B-DAT
dataset, O
which O
was O
split O
into O

the O
most O
popular O
categories O
in O
Polyvore B-DAT
dataset. O
Each O
circle O
denotes O
a O

most O
popular O
categories O
in O
the O
Polyvore B-DAT
dataset. O
Each O
circle O
denotes O
a O

fill-in-the-blank O
dataset O
based O
on O
the O
Polyvore B-DAT
test O
dataset. O
For O
each O
outfit O

a O
dataset O
Ptest O
using O
the O
Polyvore B-DAT
test O
dataset. O
Specif- O
ically, O
for O

positive O
outfit O
s O
in O
the O
Polyvore B-DAT
test O
dataset, O
we O
create O
a O

results O
on O
a O
real-world O
dataset O
(Polyvore B-DAT
dataset) O
prove O
the O
great O
superiority O

3.1 O
Polyvore B-DAT
Dataset O

polyvore B-DAT

polyvore B-DAT

and O
compatibility O
prediction O
on O
the O
polyvore B-DAT
dataset O
described O
in O
Section O
3.1 O

in O
untrimmed O
videos O
from O
the O
ActionNet B-DAT

ActionNet B-DAT

ActionNet B-DAT

the O
distribution O
of O
actions O
in O
ActionNet B-DAT

detect O
the O
ten O
actions O
from O
ActionNet B-DAT

ten O
composite O
actions O
in O
the O
ActionNet B-DAT

ActionNet B-DAT

in O
untrimmed O
videos O
from O
the O
ActionNet-VE B-DAT
dataset, O
the O
detection O
method O
using O

ActionNet-VE B-DAT
- O
40 O
video O
clips O

ActionNet-VE B-DAT
[51], O
which O
extended O
VIRAT O
Ground O

the O
distribution O
of O
actions O
in O
ActionNet-VE B-DAT
is O
unbalanced O
because O
the O
videos O

detect O
the O
ten O
actions O
from O
ActionNet-VE B-DAT

ten O
composite O
actions O
in O
the O
ActionNet-VE B-DAT
dataset. O
In O
addition, O
it O
was O

ActionNet-VE B-DAT
Dataset: O
A O
Dataset O
for O

VE B-DAT
dataset, O
the O
detection O
method O
using O

VE B-DAT
- O
40 O
video O
clips O

VE B-DAT
[51], O
which O
extended O
VIRAT O
Ground O

VE B-DAT
is O
unbalanced O
because O
the O
videos O

VE B-DAT

VE B-DAT
dataset. O
In O
addition, O
it O
was O

VE B-DAT
Dataset: O
A O
Dataset O
for O

ditionally O
train O
L3C O
on O
the O
ImageNet32 B-DAT
and O
ImageNet64 O
datasets O
[8], O
each O

0.75 O
every O
5 O
epochs. O
On O
ImageNet32 B-DAT

bpsp] O
ImageNet32 B-DAT
Learned O

32 O
im- O
ages O
from O
ImageNet32 B-DAT
of O
our O
method O
(L3C) O
vs O

also O
evaluate O
the O
bitcost O
on O
ImageNet32, B-DAT
for O
which O
PixelCNN O
and O
MS-PixelCNN O

We O
obtain O
similar O
results O
for O
ImageNet64, B-DAT
see O
Sec. O
A.2 O

Hutter. O
A O
downsampled O
variant O
of O
ImageNet B-DAT
as O
an O
alternative O
to O
the O

A.2. O
Comparison O
on O
ImageNet64 B-DAT

show O
a O
bpsp O
comparison O
on O
ImageNet64 B-DAT
in O
Table O
A2. O
Similar O
to O

what O
we O
observed O
on O
ImageNet32 B-DAT
(see O
Sec- O
tion O
5.2), O
our O

PixelCNN O
becomes O
smaller O
compared O
to O
ImageNet32 B-DAT

bpsp] O
ImageNet64 B-DAT
Learned O

64 O
× O
64 O
images O
from O
ImageNet64 B-DAT
of O
our O
method O
(L3C) O
vs O

ditionally O
train O
L3C O
on O
the O
ImageNet32 B-DAT
and O
ImageNet64 O
datasets O
[8], O
each O

0.75 O
every O
5 O
epochs. O
On O
ImageNet32 B-DAT

bpsp] O
ImageNet32 B-DAT
Learned O

32 O
im- O
ages O
from O
ImageNet32 B-DAT
of O
our O
method O
(L3C) O
vs O

also O
evaluate O
the O
bitcost O
on O
ImageNet32, B-DAT
for O
which O
PixelCNN O
and O
MS-PixelCNN O

to O
what O
we O
observed O
on O
ImageNet32 B-DAT
(see O
Sec- O
tion O
5.2), O
our O

PixelCNN O
becomes O
smaller O
compared O
to O
ImageNet32 B-DAT

ditionally O
train O
L3C O
on O
the O
ImageNet32 B-DAT
and O
ImageNet64 O
datasets O
[8], O
each O

0.75 O
every O
5 O
epochs. O
On O
ImageNet32 B-DAT

bpsp] O
ImageNet32 B-DAT
Learned O

32 O
im- O
ages O
from O
ImageNet32 B-DAT
of O
our O
method O
(L3C) O
vs O

also O
evaluate O
the O
bitcost O
on O
ImageNet32, B-DAT
for O
which O
PixelCNN O
and O
MS-PixelCNN O

We O
obtain O
similar O
results O
for O
ImageNet64, B-DAT
see O
Sec. O
A.2 O

Hutter. O
A O
downsampled O
variant O
of O
ImageNet B-DAT
as O
an O
alternative O
to O
the O

A.2. O
Comparison O
on O
ImageNet64 B-DAT

show O
a O
bpsp O
comparison O
on O
ImageNet64 B-DAT
in O
Table O
A2. O
Similar O
to O

what O
we O
observed O
on O
ImageNet32 B-DAT
(see O
Sec- O
tion O
5.2), O
our O

PixelCNN O
becomes O
smaller O
compared O
to O
ImageNet32 B-DAT

bpsp] O
ImageNet64 B-DAT
Learned O

64 O
× O
64 O
images O
from O
ImageNet64 B-DAT
of O
our O
method O
(L3C) O
vs O

ditionally O
train O
L3C O
on O
the O
ImageNet32 B-DAT
and O
ImageNet64 O
datasets O
[8], O
each O

0.75 O
every O
5 O
epochs. O
On O
ImageNet32 B-DAT

bpsp] O
ImageNet32 B-DAT
Learned O

32 O
im- O
ages O
from O
ImageNet32 B-DAT
of O
our O
method O
(L3C) O
vs O

also O
evaluate O
the O
bitcost O
on O
ImageNet32, B-DAT
for O
which O
PixelCNN O
and O
MS-PixelCNN O

to O
what O
we O
observed O
on O
ImageNet32 B-DAT
(see O
Sec- O
tion O
5.2), O
our O

PixelCNN O
becomes O
smaller O
compared O
to O
ImageNet32 B-DAT

ditionally O
train O
L3C O
on O
the O
ImageNet32 B-DAT
and O
ImageNet64 O
datasets O
[8], O
each O

0.75 O
every O
5 O
epochs. O
On O
ImageNet32 B-DAT

bpsp] O
ImageNet32 B-DAT
Learned O

32 O
im- O
ages O
from O
ImageNet32 B-DAT
of O
our O
method O
(L3C) O
vs O

also O
evaluate O
the O
bitcost O
on O
ImageNet32, B-DAT
for O
which O
PixelCNN O
and O
MS-PixelCNN O

We O
obtain O
similar O
results O
for O
ImageNet64, B-DAT
see O
Sec. O
A.2 O

Hutter. O
A O
downsampled O
variant O
of O
ImageNet B-DAT
as O
an O
alternative O
to O
the O

A.2. O
Comparison O
on O
ImageNet64 B-DAT

show O
a O
bpsp O
comparison O
on O
ImageNet64 B-DAT
in O
Table O
A2. O
Similar O
to O

what O
we O
observed O
on O
ImageNet32 B-DAT
(see O
Sec- O
tion O
5.2), O
our O

PixelCNN O
becomes O
smaller O
compared O
to O
ImageNet32 B-DAT

bpsp] O
ImageNet64 B-DAT
Learned O

64 O
× O
64 O
images O
from O
ImageNet64 B-DAT
of O
our O
method O
(L3C) O
vs O

ditionally O
train O
L3C O
on O
the O
ImageNet32 B-DAT
and O
ImageNet64 O
datasets O
[8], O
each O

0.75 O
every O
5 O
epochs. O
On O
ImageNet32 B-DAT

bpsp] O
ImageNet32 B-DAT
Learned O

32 O
im- O
ages O
from O
ImageNet32 B-DAT
of O
our O
method O
(L3C) O
vs O

also O
evaluate O
the O
bitcost O
on O
ImageNet32, B-DAT
for O
which O
PixelCNN O
and O
MS-PixelCNN O

to O
what O
we O
observed O
on O
ImageNet32 B-DAT
(see O
Sec- O
tion O
5.2), O
our O

PixelCNN O
becomes O
smaller O
compared O
to O
ImageNet32 B-DAT

Event2Mind B-DAT

5 O
Analyzing O
Bias O
via O
Event2Mind B-DAT
Inference O

Through O
Event2Mind B-DAT
inference, O
we O
can O
attempt O
to O

Our O
Event2Mind B-DAT
inferences O
automate O
portrayal O
analyses O
that O

B O
Event2Mind B-DAT
Training O
Details O

Overlap O
criterion O
% O
of O
Event2Mind B-DAT
events O

Table O
5: O
Event2Mind B-DAT
events O
overlap O
with O
Con- O
ceptNet O

we O
show O
that O
75% O
of O
Event2Mind B-DAT
events O
are O
not O
cov- O
ered O

event2mind B-DAT

event2mind B-DAT

Event2Mind B-DAT

5 O
Analyzing O
Bias O
via O
Event2Mind B-DAT
Inference O

Through O
Event2Mind B-DAT
inference, O
we O
can O
attempt O
to O

Our O
Event2Mind B-DAT
inferences O
automate O
portrayal O
analyses O
that O

B O
Event2Mind B-DAT
Training O
Details O

Overlap O
criterion O
% O
of O
Event2Mind B-DAT
events O

Table O
5: O
Event2Mind B-DAT
events O
overlap O
with O
Con- O
ceptNet O

we O
show O
that O
75% O
of O
Event2Mind B-DAT
events O
are O
not O
cov- O
ered O

event2mind B-DAT

event2mind B-DAT

