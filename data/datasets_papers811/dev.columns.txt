state-of-the-art O
performance O
on O
Something-Something O
and O
Jester B-DAT

datasets O
Something- O
Something O
[12] O
and O
Jester B-DAT
[30] O
with O
fewer O
parameters. O
We O

datasets O
Something-something O
v2 O
[12] O
and O
Jester B-DAT
v1 O
[30] O
and O
report O
our O

5.5. O
Results O
on O
Jester B-DAT

Jester B-DAT
[30] O
is O
a O
dataset O
for O

b) O
Jester B-DAT
v1 O
Results O

with O
label O
“Thumb O
Up” O
from O
Jester B-DAT
v1 O
validation O
set O

used O
in O
Something- O
Something O
and O
Jester B-DAT
experiments O
(main O
paper O
section O
5.4 O

CPNet O
model O
on O
Something-Something O
and O
Jester B-DAT
datasets. O
Lastly O
in O
section O
G O

Architecture O
used O
in O
Something-Something O
and O
Jester B-DAT
Experiments O

in O
Something- O
Something O
[12] O
and O
Jester B-DAT
[30] O
experiments O
in O
Table O
8 O

Per-class O
accuracy O
of O
Something-Something O
and O
Jester B-DAT
models O

Architectures O
used O
in O
Something-Something O
and O
Jester B-DAT
dataset O
experiments O

the O
respective O
C2D O
baseline O
on O
Jester B-DAT
in O
Figure O
8 O
and O
Something-Something O

On O
Jester B-DAT
dataset O
[30], O
the O
largest O
accuracy O

accuracy O
gain O
in O
percentage O
on O
Jester B-DAT
v1 O
dataset O
due O
to O
CP O

the O
CPNet O
model O
used O
in O
Jester B-DAT
v1 O
experiment. O
The O
spatial O
size O

12] O
in O
Figure O
12 O
and O
Jester B-DAT
[30] O
in O
Figure O
13. O
They O

with O
label O
“Drumming O
Fingers” O
from O
Jester B-DAT
v1 O
validation O
set O

with O
label O
“Shaking O
Hand” O
from O
Jester B-DAT
v1 O
validation O
set O

with O
label O
“Stop O
Sign” O
from O
Jester B-DAT
v1 O
validation O
set O

Pushing O
Two O
Fingers O
Away” O
from O
Jester B-DAT
v1 O
validation O
set O

on O
our O
final O
models O
on O
Jester B-DAT
v1 O
dataset. O
Approach O
is O
the O

jester B-DAT
Dataset O
V1. O
https:// O
20bn.com/datasets/jester. O
2 O

jester B-DAT
https://20bn.com/datasets/jester O

Something-Something O
v1 O
& O
v2 O
and O
Jester) B-DAT
and O
scene-related O
datasets O
(i.e., O
Kinetics O

datasets O
including O
Something-Something[11], O
Kinetics O
[2], O
Jester B-DAT
[1], O
UCF101 O
[23] O
and O
HMDB-51 O

Something O
v1 O
& O
v2 O
and O
Jester) B-DAT
and O
scene-related O
datasets O
(i.e., O
Kinetics-400 O

v1 O
& O
v2 O
[11] O
and O
Jester B-DAT
[1]. O
For O
these O
datasets, O
temporal O

Something O
v1 O
& O
v2 O
and O
Jester, B-DAT
we O
start O
with O
a O
learning O

of O
the O
STM O
on O
the O
Jester B-DAT
compared O
with O
the O
state-of-the-art O
methods O

Something-Something O
v1 O
& O
v2 O
and O
Jester B-DAT

and O
greatly O
reduced O
label O
noise. O
Jester B-DAT
is O
a O
crowd-acted O
video O
dataset O

shows O
the O
results O
on O
the O
Jester B-DAT
dataset. O
Our O
STM O
also O
gains O

jester B-DAT
dataset O
v1. O
In O
https://20bn.com O

jester B-DAT

video O
datasets O
- O
Something- O
Something, O
Jester, B-DAT
and O
Charades O
- O
which O
fundamentally O

various O
human O
gestures O
on O
the O
Jester B-DAT
dataset O
with O
very O
competitive O
perfor O

recent O
video O
datasets O
(Something-Something O
[9], O
Jester B-DAT
[10], O
and O
Charades O
[11]), O
which O

something O
without O
actually O
opening O
it’. O
Jester B-DAT
dataset O
[10] O
is O
another O
recent O

recognition O
[9] O
and O
on O
the O
Jester B-DAT
dataset O
for O
hand O
gesture O
recognition O

in O
early O
July O
2018) O
[9,28], O
Jester B-DAT
dataset O
[10], O
and O
Charades O
dataset O

Something-V2 O
174 O
220,847 O
human-object O
interaction O
Jester B-DAT
27 O
148,092 O
human O
hand O
gesture O

3.3 O
Results O
on O
Jester B-DAT
and O
Charades O

the O
TRN-equipped O
networks O
on O
the O
Jester B-DAT
dataset, O
which O
is O
a O
video O

the O
validation O
set O
of O
the O
Jester B-DAT
dataset O
are O
listed O
in O
Table O

20BN O
Jester B-DAT
System O
82.34 O
VideoLSTM O
85.86 O
Guillaume O

Table O
3: O
Jester B-DAT
Dataset O
Results O
on O
(a) O
the O

examples O
on O
a) O
Something-Something, O
b) O
Jester, B-DAT
and O
c) O
Cha- O
rades. O
For O

example O
drawn O
from O
Something-Something O
and O
Jester, B-DAT
the O
top O
two O
predictions O
with O

the O
(a) O
Something-Something O
and O
(b) O
Jester B-DAT
datasets O
using O
the O
most O
representative O

Dataset O
UCF O
Kinetics O
Moments O
Something O
Jester B-DAT
Charades O

TRN O
on O
Something- O
Something O
and O
Jester B-DAT
dataset. O
Only O
the O
first O
25 O

Something O
Jester B-DAT
Frames O
baseline O
TRN O
baseline O
TRN O

10. O
: O
Twentybn O
jester B-DAT
dataset: O
a O
hand O
gesture O
dataset O

jester B-DAT
(2017 O

their O
capacity O
to O
learn, O
(2) O
Jester B-DAT
dataset O
to O
inspect O
their O
ability O

2) O
Jester B-DAT
dataset O
[1] O
to O
learn O
how O

Regularization: O
Although O
Kinetics-600 O
and O
Jester B-DAT
are O
very O
large O
benchmarks O
and O

0.2 O
for O
Kinetics- O
600 O
and O
Jester, B-DAT
it O
is O
increased O
to O
0.9 O

16-frame O
clips O
are O
used. O
For O
Jester B-DAT
benchmark, O
it O
is O
critical O
to O

frames O
from O
32 O
frames O
for O
Jester B-DAT
benchmark O
[17 O

what O
is O
being O
eaten. O
• O
Jester B-DAT
dataset O
is O
currently O
the O
largest O

Titan O
XP O
Jetson O
TX2 O
Kinetics-600 O
Jester B-DAT
UCF-101 O

Unlike O
Kinetics-600 O
benchmark, O
in O
Jester B-DAT
dataset, O
spatial O
content O
of O
the O

the O
hand. O
That O
is O
why, O
Jester B-DAT
benchmark O
is O
suitable O
to O
inspect O

data. O
Compared O
to O
Kinetics-600 O
and O
Jester B-DAT
datasets, O
UCF-101 O
contains O
very O
little O

far O
best O
perfor- O
mance O
in O
Jester B-DAT
benchmark, O
although O
it O
has O
inferior O

more O
parameters O
and O
FLOPs) O
at O
Jester B-DAT
benchmark O

modified O
flexibly. O
The O
results O
on O
Jester B-DAT
benchmark O
show O
that O
depthwise O
convolutions O

jester B-DAT

jester B-DAT

jester B-DAT

their O
capacity O
to O
learn, O
(2) O
Jester B-DAT
dataset O
to O
inspect O
their O
ability O

2) O
Jester B-DAT
dataset O
[1] O
to O
learn O
how O

Regularization: O
Although O
Kinetics-600 O
and O
Jester B-DAT
are O
very O
large O
benchmarks O
and O

0.2 O
for O
Kinetics- O
600 O
and O
Jester, B-DAT
it O
is O
increased O
to O
0.9 O

16-frame O
clips O
are O
used. O
For O
Jester B-DAT
benchmark, O
it O
is O
critical O
to O

frames O
from O
32 O
frames O
for O
Jester B-DAT
benchmark O
[17 O

what O
is O
being O
eaten. O
• O
Jester B-DAT
dataset O
is O
currently O
the O
largest O

Titan O
XP O
Jetson O
TX2 O
Kinetics-600 O
Jester B-DAT
UCF-101 O

Unlike O
Kinetics-600 O
benchmark, O
in O
Jester B-DAT
dataset, O
spatial O
content O
of O
the O

the O
hand. O
That O
is O
why, O
Jester B-DAT
benchmark O
is O
suitable O
to O
inspect O

data. O
Compared O
to O
Kinetics-600 O
and O
Jester B-DAT
datasets, O
UCF-101 O
contains O
very O
little O

far O
best O
perfor- O
mance O
in O
Jester B-DAT
benchmark, O
although O
it O
has O
inferior O

more O
parameters O
and O
FLOPs) O
at O
Jester B-DAT
benchmark O

modified O
flexibly. O
The O
results O
on O
Jester B-DAT
benchmark O
show O
that O
depthwise O
convolutions O

jester B-DAT

jester B-DAT

jester B-DAT

their O
capacity O
to O
learn, O
(2) O
Jester B-DAT
dataset O
to O
inspect O
their O
ability O

2) O
Jester B-DAT
dataset O
[1] O
to O
learn O
how O

Regularization: O
Although O
Kinetics-600 O
and O
Jester B-DAT
are O
very O
large O
benchmarks O
and O

0.2 O
for O
Kinetics- O
600 O
and O
Jester, B-DAT
it O
is O
increased O
to O
0.9 O

16-frame O
clips O
are O
used. O
For O
Jester B-DAT
benchmark, O
it O
is O
critical O
to O

frames O
from O
32 O
frames O
for O
Jester B-DAT
benchmark O
[17 O

what O
is O
being O
eaten. O
• O
Jester B-DAT
dataset O
is O
currently O
the O
largest O

Titan O
XP O
Jetson O
TX2 O
Kinetics-600 O
Jester B-DAT
UCF-101 O

Unlike O
Kinetics-600 O
benchmark, O
in O
Jester B-DAT
dataset, O
spatial O
content O
of O
the O

the O
hand. O
That O
is O
why, O
Jester B-DAT
benchmark O
is O
suitable O
to O
inspect O

data. O
Compared O
to O
Kinetics-600 O
and O
Jester B-DAT
datasets, O
UCF-101 O
contains O
very O
little O

far O
best O
perfor- O
mance O
in O
Jester B-DAT
benchmark, O
although O
it O
has O
inferior O

more O
parameters O
and O
FLOPs) O
at O
Jester B-DAT
benchmark O

modified O
flexibly. O
The O
results O
on O
Jester B-DAT
benchmark O
show O
that O
depthwise O
convolutions O

jester B-DAT

jester B-DAT

jester B-DAT

Accuracy O
vs O
Time O
on O
MiniKinetics B-DAT

1: O
Accuracy O
vs. O
time O
on O
MiniKinetics B-DAT
for O
different O

vs. O
time O
plot O
on O
the O
MiniKinetics B-DAT

of O
the O
analyses O
on O
the O
MiniKinetics B-DAT
subset O
contain O

Accuracy O
vs O
Time O
on O
MiniKinetics B-DAT

Impact O
of O
α O
on O
MiniKinetics B-DAT

a) O
Accuracy O
vs. O
time O
on O
MiniKinetics B-DAT
for O
different O
optical O
flow O
approaches O

over O
all O
videos O
of O
the O
MiniKinetics B-DAT
validation O
set. O
(b) O
Accuracy O
of O

different O
values O
of O
α O
on O
MiniKinetics B-DAT
with O

Net O
[34] O
on O
MiniKinetics B-DAT

train O
on O
Kinetics400 O
and O
MiniKinetics B-DAT
from O
scratch. O
For O

Stream O
MiniKinetics B-DAT
Kinetics400 O
UCF101-1 O
HMDB51-1 O
Something O

Top-1 O
accuracy O
using O
16f-clips. O
For O
MiniKinetics B-DAT
and O
Kinetics400, O
all O
the O
streams O

on O
MiniKinetics B-DAT

time O
per O
video O
on O
the O
MiniKinetics B-DAT
validation O
set O
(250 O
frames O
per O

Flow O
streams O
alone O
on O
MiniKinetics, B-DAT
UCF101, O
HMDB51 O

MiniKinetics B-DAT
using O
values O
of O
α O

obtain O
a O
similar O
performance O
on O
MiniKinetics, B-DAT
as O
when O

other O
datasets O
like O
Sports-1M O
and O
ActivityNet B-DAT

19] O
for O
sports O
videos O
and O
ActivityNet B-DAT
[12] O
for O
human O
activities. O
However O

frames. O
Therefore, O
unlike O
Sports-1M O
and O
ActivityNet, B-DAT
YouTube-8M O
is O
not O
restricted O
to O

benchmarks O
such O
as O
Sports-1M O
and O
ActivityNet B-DAT

annotated O
with O
239 O
categories, O
and O
ActivityNet B-DAT
[12], O
with O
∼200 O
human O
activity O

transfer O
the O
learned O
model O
to O
ActivityNet, B-DAT
we O
used O
a O
fully-connected O
model O

b) O
ActivityNet B-DAT

one O
learnt O
from O
scratch O
on O
ActivityNet B-DAT

the O
(a) O
Sports-1M O
and O
(b) O
ActivityNet B-DAT

5.4 O
Results O
on O
ActivityNet B-DAT
Our O
final O
set O
of O
experiments O

learned O
features O
for O
the O
ActivityNet B-DAT
untrimmed O
video O
classification O
task. O
Similar O

directly O
train- O
ing O
on O
the O
ActivityNet B-DAT
dataset O
against O
pre-training O
on O
YouTube-8M O

all O
metrics O
than O
training O
on O
ActivityNet B-DAT
alone. O
Notably, O
without O
the O
use O

existing O
video O
benchmarks— O
Sports-1M O
and O
ActivityNet B-DAT

setting O
a O
new O
state-of-the-art O
on O
ActivityNet B-DAT

un- O
filtered) O
for O
UCF101 O
and O
ActivityNet B-DAT

dataset O
of O
web O
images O
for O
ActivityNet B-DAT
[1]; O
a O
larger O
scale O
action O

half O
the O
training O
videos O
in O
ActivityNet B-DAT
(which O
correspond O
to O
16.2M O
frames O

comparable O
performance O
when O
replacing O
half O
ActivityNet B-DAT
videos O
(16.2M O
frames) O
with O
393K O

the O
classes O
of O
UCF101 O
and O
ActivityNet B-DAT

larger-scale O
dataset: O
Ac- O
tivityNet O
[1]. O
ActivityNet B-DAT
contains O
more O
classes O
(203) O
and O

samples O
per O
class O
than O
UCF101. O
ActivityNet B-DAT
classes O
are O
more O
diverse; O
they O

ActivityNet B-DAT
provides O
samples O
from O
203 O
activity O

of O
Activ- O
ityNet. O
Results O
on O
ActivityNet B-DAT
are O
reported O
in O
Section O
5 O

We O
also O
perform O
experiments O
on O
ActivityNet B-DAT

per- O
formance. O
Results O
reported O
on O
ActivityNet B-DAT
are O
produced O
us- O
ing O
the O

5.1.2 O
Experimental O
Setup O
for O
ActivityNet B-DAT

Table O
6: O
Although O
ActivityNet B-DAT
is O
large-scale, O
using O
unfiltered O
web O

5.2.2 O
Experimental O
Results O
for O
ActivityNet B-DAT

half O
the O
training O
videos O
of O
ActivityNet B-DAT
are O
replaced O
by O
393K O
images O

discover O
‘building-blocks’ O
from O
the O
large-scale O
ActivityNet B-DAT
dataset O
using O
distribution O
kernels. O
Essential O

ActivityNet B-DAT
Kernelised O

the O
same O
essential O
components O
in O
ActivityNet B-DAT
to O
achieve O
a O
matching O
using O

jump’ O
and O
‘triple-jump’ O
in O
the O
ActivityNet B-DAT
dataset O
using O
tSNE O

the O
URL O
on O
the O
large-scale O
ActivityNet B-DAT
[13] O
dataset. O
Cross-dataset O
UAR O
experiments O

HMDB51 O
contain O
trimmed O
videos O
while O
ActivityNet B-DAT
contains O
untrimmed O
ones. O
We O
first O

Datasets O
ActivityNet1 B-DAT
consists O
of O
10024 O
training, O
4926 O

on O
ImageNet O
and O
fine-tuned O
on O
ActivityNet B-DAT
dataset. O
Overlap- O
ping O
classes O
between O

ActivityNet B-DAT
and O
UCF101 O
are O
not O
used O

the O
latest O
release O
1.3 O
of O
ActivityNet B-DAT
for O
our O
experiments O

ActivityNet B-DAT
is O
evenly O
divided O
into O
5 O

Ghanem, O
and O
J. O
C. O
Niebles. O
ActivityNet B-DAT

un- O
filtered) O
for O
UCF101 O
and O
ActivityNet B-DAT

dataset O
of O
web O
images O
for O
ActivityNet B-DAT
[1]; O
a O
larger O
scale O
action O

half O
the O
training O
videos O
in O
ActivityNet B-DAT
(which O
correspond O
to O
16.2M O
frames O

comparable O
performance O
when O
replacing O
half O
ActivityNet B-DAT
videos O
(16.2M O
frames) O
with O
393K O

the O
classes O
of O
UCF101 O
and O
ActivityNet B-DAT

larger-scale O
dataset: O
Ac- O
tivityNet O
[1]. O
ActivityNet B-DAT
contains O
more O
classes O
(203) O
and O

samples O
per O
class O
than O
UCF101. O
ActivityNet B-DAT
classes O
are O
more O
diverse; O
they O

ActivityNet B-DAT
provides O
samples O
from O
203 O
activity O

of O
Activ- O
ityNet. O
Results O
on O
ActivityNet B-DAT
are O
reported O
in O
Section O
5 O

We O
also O
perform O
experiments O
on O
ActivityNet B-DAT

per- O
formance. O
Results O
reported O
on O
ActivityNet B-DAT
are O
produced O
us- O
ing O
the O

5.1.2 O
Experimental O
Setup O
for O
ActivityNet B-DAT

Table O
6: O
Although O
ActivityNet B-DAT
is O
large-scale, O
using O
unfiltered O
web O

5.2.2 O
Experimental O
Results O
for O
ActivityNet B-DAT

half O
the O
training O
videos O
of O
ActivityNet B-DAT
are O
replaced O
by O
393K O
images O

we O
set O
epoch O
size O
to O
1M B-DAT
clips O
due O
to O
temporal O
jitterring O

Mini-Sports O
is O
a O
subset O
of O
Sports-1M B-DAT
[18], O
a O
large-scale O
video O
classification O

as O
in O
section O
4 O
for O
Sports-1M B-DAT
and O
AudioSet. O
For O
Kinetics, O
we O

Second, O
G-Blend, O
when O
fine-tuned O
from O
Sports-1M, B-DAT
outperforms O
Shift-Attention O
Network O
[6] O
and O

Sports- O
1M O
and O
AudioSet. O
On O
Sports-1M, B-DAT
G-Blend O
significantly O
outperforms O
previously O
published O

1M B-DAT
[18], O
a O
large-scale O
video O
classification O

1M B-DAT
videos O
of O
487 O
different O
fine-grained O

1M B-DAT
and O
AudioSet. O
For O
Kinetics, O
we O

1M, B-DAT
outperforms O
Shift-Attention O
Network O
[6] O
and O

current O
best O
methods O
on O
Sports- O
1M B-DAT
and O
AudioSet. O
On O
Sports-1M, O
G-Blend O

Sports B-DAT
A O
+ O
RGB O
60.2 O
RGB O

Sports B-DAT
using O
video O
top-1 O
validation O
accuracy O

Sports, B-DAT
and O
mini- O
AudioSet. O
Kinetics O
is O

Mini-Sports B-DAT
is O
a O
subset O
of O
Sports O

Sports, B-DAT
13% O
for O
mini-AudioSet). O
For O
evaluation O

Sports B-DAT
(right). O
On O
both O
Kinetics O
and O

Sports, B-DAT
the O
audio O
model O
overfits O
the O

Sports B-DAT
Learning O
Curve O

Sports B-DAT

Sports B-DAT
(right). O
Solid O
lines O
plot O
validation O

Sports), B-DAT
and O
acoustic O
event O
detection O
(mini-AudioSet O

Sports B-DAT

Sports B-DAT
demonstrates O
that O
the O
weights O
used O

Sports B-DAT
mini-AudioSet O
Method O
Clip O
V@1 O
V@5 O

Sports, B-DAT
and O
mini-AudioSet. O
G-Blend O
consistently O
outperforms O

as O
in O
section O
4 O
for O
Sports B-DAT

Second, O
G-Blend, O
when O
fine-tuned O
from O
Sports B-DAT

with O
current O
best O
methods O
on O
Sports B-DAT

- O
1M O
and O
AudioSet. O
On O
Sports B-DAT

to O
the O
state-of-the- O
art O
on O
Sports-1M, B-DAT
Kinetics, O
UCF101, O
and O
HMDB51 O

the O
state-of-the-art O
on O
the O
challenging O
Sports-1M B-DAT
benchmark. O
This O
result O
is O
both O

performance O
on O
both O
Kinetics O
and O
Sports-1M B-DAT

recognition O
accuracy. O
For O
example, O
on O
Sports-1M B-DAT
using O
RGB O
as O
input, O
R(2+1)D O

R3D, O
R2D, O
and O
P3D O
on O
Sports-1M B-DAT
(see O
Table O
4 O

We O
use O
Kinetics O
[4] O
and O
Sports-1M B-DAT
[16] O
as O
the O
primary O
benchmarks O

by O
pretraining O
our O
models O
on O
Sports-1M B-DAT
and O
Kinetics, O
and O
finetuning O
them O

Comparison O
with O
the O
state-of-the-art O
on O
Sports-1M B-DAT

ture O
on O
four O
public O
benchmarks. O
Sports-1M B-DAT
is O
a O
large-scale O
dataset O
for O

all O
3 O
splits. O
Results O
on O
Sports-1M B-DAT

R(2+1)D-Two-Stream O
none O
73.9 O
90.9 O
R(2+1)D-RGB O
Sports-1M B-DAT
74.3 O
91.4 O
R(2+1)D-Flow O
Sports-1M O
68.5 O

R(2+1)D-Two-Stream O
Sports-1M B-DAT
75.4 O
91.9 O

on O
RGB. O
R(2+1)D O
pretrained O
on O
Sports-1M B-DAT
outperforms O
I3D O
pre- O
trained O
on O

baseline O
for O
comparison. O
Videos O
in O
Sports-1M B-DAT
are O
very O
long, O
over O
5 O

4 O
shows O
the O
results O
on O
Sports-1M B-DAT

the O
best O
published O
result O
on O
Sports-1M B-DAT

finetuning O
the O
model O
pretrained O
on O
Sports-1M B-DAT

Our O
R(2+1)D O
pre- O
trained O
on O
Sports-1M B-DAT
also O
outperforms O
I3D O
pretrained O
on O

20] O
using O
models O
pretrained O
on O
Sports-1M B-DAT
and O

92.4 O
62.0 O
Conv O
Pooling O
[42] O
Sports-1M B-DAT
88.6 O
- O
FSTCN O
[33] O
ImageNet O

that O
Kinetics O
is O
better O
than O
Sports-1M B-DAT
for O
pretraining O
our O
models O

Kinetics O
(not O
those O
finetuned O
from O
Sports-1M) B-DAT
in O
order O
to O
understand O
the O

1M, B-DAT
Kinetics, O
UCF101, O
and O
HMDB51 O

1M B-DAT
benchmark. O
This O
result O
is O
both O

recognition O
benchmarks O
such O
as O
Sports- O
1M B-DAT
[16] O
and O
Kinetics O
[17 O

1M B-DAT

1M B-DAT
using O
RGB O
as O
input, O
R(2+1)D O

1M B-DAT
(see O
Table O
4 O

1M B-DAT
[16] O
as O
the O
primary O
benchmarks O

1M B-DAT
and O
Kinetics, O
and O
finetuning O
them O

set O
epoch O
size O
to O
be O
1M B-DAT
for O
tem- O
poral O
jittering. O
The O

1M B-DAT

1M B-DAT
is O
a O
large-scale O
dataset O
for O

1M B-DAT
videos O
of O
487 O
fine-grained O
sport O

1M B-DAT

We O
train O
R(2+1)D-34 O
on O
Sports- O
1M B-DAT
[16] O
with O
both O
8-frame O
and O

1M B-DAT
74.3 O
91.4 O
R(2+1)D-Flow O
Sports-1M O
68.5 O

1M B-DAT
75.4 O
91.9 O

1M B-DAT
outperforms O
I3D O
pre- O
trained O
on O

1M B-DAT
are O
very O
long, O
over O
5 O

1M B-DAT

1M B-DAT

1M B-DAT

1M B-DAT
also O
outperforms O
I3D O
pretrained O
on O

1M B-DAT
and O

1M B-DAT
88.6 O
- O
FSTCN O
[33] O
ImageNet O

1M B-DAT
for O
pretraining O
our O
models O

1M) B-DAT
in O
order O
to O
understand O
the O

of O
the O
art O
on O
Sports- O
1M, B-DAT
Kinetics, O
UCF101, O
and O
HMDB51. O
We O

to O
the O
state-of-the- O
art O
on O
Sports B-DAT

the O
state-of-the-art O
on O
the O
challenging O
Sports B-DAT

action O
recognition O
benchmarks O
such O
as O
Sports B-DAT

performance O
on O
both O
Kinetics O
and O
Sports B-DAT

recognition O
accuracy. O
For O
example, O
on O
Sports B-DAT

R3D, O
R2D, O
and O
P3D O
on O
Sports B-DAT

We O
use O
Kinetics O
[4] O
and O
Sports B-DAT

by O
pretraining O
our O
models O
on O
Sports B-DAT

Comparison O
with O
the O
state-of-the-art O
on O
Sports B-DAT

ture O
on O
four O
public O
benchmarks. O
Sports B-DAT

all O
3 O
splits. O
Results O
on O
Sports B-DAT

-1M. O
We O
train O
R(2+1)D-34 O
on O
Sports B-DAT

R(2+1)D-Two-Stream O
none O
73.9 O
90.9 O
R(2+1)D-RGB O
Sports B-DAT

-1M O
74.3 O
91.4 O
R(2+1)D-Flow O
Sports B-DAT

R(2+1)D-Two-Stream O
Sports B-DAT

on O
RGB. O
R(2+1)D O
pretrained O
on O
Sports B-DAT

baseline O
for O
comparison. O
Videos O
in O
Sports B-DAT

4 O
shows O
the O
results O
on O
Sports B-DAT

the O
best O
published O
result O
on O
Sports B-DAT

finetuning O
the O
model O
pretrained O
on O
Sports B-DAT

Our O
R(2+1)D O
pre- O
trained O
on O
Sports B-DAT

20] O
using O
models O
pretrained O
on O
Sports B-DAT

92.4 O
62.0 O
Conv O
Pooling O
[42] O
Sports B-DAT

that O
Kinetics O
is O
better O
than O
Sports B-DAT

Kinetics O
(not O
those O
finetuned O
from O
Sports B-DAT

state O
of O
the O
art O
on O
Sports B-DAT

to O
the O
state-of-the- O
art O
on O
Sports-1M, B-DAT
Kinetics, O
UCF101, O
and O
HMDB51 O

the O
state-of-the-art O
on O
the O
challenging O
Sports-1M B-DAT
benchmark. O
This O
result O
is O
both O

performance O
on O
both O
Kinetics O
and O
Sports-1M B-DAT

recognition O
accuracy. O
For O
example, O
on O
Sports-1M B-DAT
using O
RGB O
as O
input, O
R(2+1)D O

R3D, O
R2D, O
and O
P3D O
on O
Sports-1M B-DAT
(see O
Table O
4 O

We O
use O
Kinetics O
[4] O
and O
Sports-1M B-DAT
[16] O
as O
the O
primary O
benchmarks O

by O
pretraining O
our O
models O
on O
Sports-1M B-DAT
and O
Kinetics, O
and O
finetuning O
them O

Comparison O
with O
the O
state-of-the-art O
on O
Sports-1M B-DAT

ture O
on O
four O
public O
benchmarks. O
Sports-1M B-DAT
is O
a O
large-scale O
dataset O
for O

all O
3 O
splits. O
Results O
on O
Sports-1M B-DAT

R(2+1)D-Two-Stream O
none O
73.9 O
90.9 O
R(2+1)D-RGB O
Sports-1M B-DAT
74.3 O
91.4 O
R(2+1)D-Flow O
Sports-1M O
68.5 O

R(2+1)D-Two-Stream O
Sports-1M B-DAT
75.4 O
91.9 O

on O
RGB. O
R(2+1)D O
pretrained O
on O
Sports-1M B-DAT
outperforms O
I3D O
pre- O
trained O
on O

baseline O
for O
comparison. O
Videos O
in O
Sports-1M B-DAT
are O
very O
long, O
over O
5 O

4 O
shows O
the O
results O
on O
Sports-1M B-DAT

the O
best O
published O
result O
on O
Sports-1M B-DAT

finetuning O
the O
model O
pretrained O
on O
Sports-1M B-DAT

Our O
R(2+1)D O
pre- O
trained O
on O
Sports-1M B-DAT
also O
outperforms O
I3D O
pretrained O
on O

20] O
using O
models O
pretrained O
on O
Sports-1M B-DAT
and O

92.4 O
62.0 O
Conv O
Pooling O
[42] O
Sports-1M B-DAT
88.6 O
- O
FSTCN O
[33] O
ImageNet O

that O
Kinetics O
is O
better O
than O
Sports-1M B-DAT
for O
pretraining O
our O
models O

Kinetics O
(not O
those O
finetuned O
from O
Sports-1M) B-DAT
in O
order O
to O
understand O
the O

1M, B-DAT
Kinetics, O
UCF101, O
and O
HMDB51 O

1M B-DAT
benchmark. O
This O
result O
is O
both O

recognition O
benchmarks O
such O
as O
Sports- O
1M B-DAT
[16] O
and O
Kinetics O
[17 O

1M B-DAT

1M B-DAT
using O
RGB O
as O
input, O
R(2+1)D O

1M B-DAT
(see O
Table O
4 O

1M B-DAT
[16] O
as O
the O
primary O
benchmarks O

1M B-DAT
and O
Kinetics, O
and O
finetuning O
them O

set O
epoch O
size O
to O
be O
1M B-DAT
for O
tem- O
poral O
jittering. O
The O

1M B-DAT

1M B-DAT
is O
a O
large-scale O
dataset O
for O

1M B-DAT
videos O
of O
487 O
fine-grained O
sport O

1M B-DAT

We O
train O
R(2+1)D-34 O
on O
Sports- O
1M B-DAT
[16] O
with O
both O
8-frame O
and O

1M B-DAT
74.3 O
91.4 O
R(2+1)D-Flow O
Sports-1M O
68.5 O

1M B-DAT
75.4 O
91.9 O

1M B-DAT
outperforms O
I3D O
pre- O
trained O
on O

1M B-DAT
are O
very O
long, O
over O
5 O

1M B-DAT

1M B-DAT

1M B-DAT

1M B-DAT
also O
outperforms O
I3D O
pretrained O
on O

1M B-DAT
and O

1M B-DAT
88.6 O
- O
FSTCN O
[33] O
ImageNet O

1M B-DAT
for O
pretraining O
our O
models O

1M) B-DAT
in O
order O
to O
understand O
the O

of O
the O
art O
on O
Sports- O
1M, B-DAT
Kinetics, O
UCF101, O
and O
HMDB51. O
We O

to O
the O
state-of-the- O
art O
on O
Sports B-DAT

the O
state-of-the-art O
on O
the O
challenging O
Sports B-DAT

action O
recognition O
benchmarks O
such O
as O
Sports B-DAT

performance O
on O
both O
Kinetics O
and O
Sports B-DAT

recognition O
accuracy. O
For O
example, O
on O
Sports B-DAT

R3D, O
R2D, O
and O
P3D O
on O
Sports B-DAT

We O
use O
Kinetics O
[4] O
and O
Sports B-DAT

by O
pretraining O
our O
models O
on O
Sports B-DAT

Comparison O
with O
the O
state-of-the-art O
on O
Sports B-DAT

ture O
on O
four O
public O
benchmarks. O
Sports B-DAT

all O
3 O
splits. O
Results O
on O
Sports B-DAT

-1M. O
We O
train O
R(2+1)D-34 O
on O
Sports B-DAT

R(2+1)D-Two-Stream O
none O
73.9 O
90.9 O
R(2+1)D-RGB O
Sports B-DAT

-1M O
74.3 O
91.4 O
R(2+1)D-Flow O
Sports B-DAT

R(2+1)D-Two-Stream O
Sports B-DAT

on O
RGB. O
R(2+1)D O
pretrained O
on O
Sports B-DAT

baseline O
for O
comparison. O
Videos O
in O
Sports B-DAT

4 O
shows O
the O
results O
on O
Sports B-DAT

the O
best O
published O
result O
on O
Sports B-DAT

finetuning O
the O
model O
pretrained O
on O
Sports B-DAT

Our O
R(2+1)D O
pre- O
trained O
on O
Sports B-DAT

20] O
using O
models O
pretrained O
on O
Sports B-DAT

92.4 O
62.0 O
Conv O
Pooling O
[42] O
Sports B-DAT

that O
Kinetics O
is O
better O
than O
Sports B-DAT

Kinetics O
(not O
those O
finetuned O
from O
Sports B-DAT

state O
of O
the O
art O
on O
Sports B-DAT

is O
below O
2% O
on O
the O
Sports-1M B-DAT
benchmarks O
[14]. O
As O
a O
result O

is O
the O
case O
with O
the O
Sports-1M B-DAT
dataset), O
they O
can O
still O
provide O

two O
different O
video O
classification O
tasks: O
Sports-1M B-DAT
(Section O
4.1) O
and O
UCF-101 O
(Section O

model O
and O
then O
fine-tuned O
on O
Sports-1M B-DAT
videos O

the O
Sports-1M B-DAT
and O
UCF-101 O
datasets O
with O
the O

4.1. O
Sports-1M B-DAT
dataset O
The O
Sports-1M O
dataset O
[14] O
consists O
of O
roughly O

Although O
Sports-1M B-DAT
is O
the O
largest O
publicly O
available O

pooling O
architectures O
(Figure O
2) O
on O
Sports-1M B-DAT
using O
a O
120- O
frame O
AlexNet O

improvements O
in O
Hit@1 O
on O
the O
Sports-1M B-DAT
dataset O

feature O
pooling O
architectures O
on O
the O
Sports-1M B-DAT
dataset O
when O
using O
a O
120 O

single-frames O
selected O
at O
random O
from O
Sports-1M B-DAT
videos. O
Results O
(Table O
2) O
show O

and O
LSTM. O
Experiments O
performed O
on O
Sports-1M B-DAT
using O
30-frame O
Conv-Pooling O
and O
LSTM O

Optical O
flow O
is O
noisy O
on O
Sports-1M B-DAT
and O
if O
used O
alone, O
results O

number O
of O
noisy O
images O
in O
Sports-1M B-DAT
com- O
pared O
to O
ImageNet O

the O
previous O
state-of-art O
on O
the O
Sports-1M B-DAT
dataset O
at O
the O
time O
of O

models O
that O
are O
trained O
in O
Sports-1M B-DAT
dataset O
perform O
in O
UCF-101 O

prior O
work O
on O
the O
in O
Sports-1M B-DAT
dataset. O
Hit@1, O
and O
Hit@5 O
are O

Compared O
to O
Sports-1M, B-DAT
optical O
flow O
in O
UCF-101 O
pro O

state-of-the-art O
performance O
on O
both O
the O
Sports-1M B-DAT
and O
UCF-101 O
benchmarks, O
supporting O
the O

is O
the O
case O
in O
the O
Sports-1M B-DAT
dataset. O
In O
order O
to O
take O

per- O
formance O
measure O
for O
the O
Sports-1M B-DAT
benchmark O

1M B-DAT
benchmarks O
[14]. O
As O
a O
result O

1M B-DAT
dataset), O
they O
can O
still O
provide O

1M B-DAT
(Section O
4.1) O
and O
UCF-101 O
(Section O

1M B-DAT
videos O

1M B-DAT
and O
UCF-101 O
datasets O
with O
the O

1M B-DAT
dataset O
The O
Sports-1M O
dataset O
[14 O

1M B-DAT
is O
the O
largest O
publicly O
available O

1M B-DAT
using O
a O
120- O
frame O
AlexNet O

1M B-DAT
dataset O

1M B-DAT
dataset O
when O
using O
a O
120 O

1M B-DAT
videos. O
Results O
(Table O
2) O
show O

1M B-DAT
using O
30-frame O
Conv-Pooling O
and O
LSTM O

1M B-DAT
and O
if O
used O
alone, O
results O

1M B-DAT
com- O
pared O
to O
ImageNet O

1M B-DAT
dataset O
at O
the O
time O
of O

1M B-DAT
dataset O
perform O
in O
UCF-101 O

1M B-DAT
dataset. O
Hit@1, O
and O
Hit@5 O
are O

1M, B-DAT
optical O
flow O
in O
UCF-101 O
pro O

1M B-DAT
and O
UCF-101 O
benchmarks, O
supporting O
the O

1M B-DAT
dataset. O
In O
order O
to O
take O

1M B-DAT
benchmark O

previously O
published O
results O
on O
the O
Sports B-DAT
1 O
mil- O
lion O
dataset O
(73.1 O

is O
below O
2% O
on O
the O
Sports B-DAT

is O
the O
case O
with O
the O
Sports B-DAT

two O
different O
video O
classification O
tasks: O
Sports B-DAT

model O
and O
then O
fine-tuned O
on O
Sports B-DAT

the O
Sports B-DAT

4.1. O
Sports-1M B-DAT
dataset O
The O
Sports O

Although O
Sports B-DAT

pooling O
architectures O
(Figure O
2) O
on O
Sports B-DAT

improvements O
in O
Hit@1 O
on O
the O
Sports B-DAT

feature O
pooling O
architectures O
on O
the O
Sports B-DAT

single-frames O
selected O
at O
random O
from O
Sports B-DAT

and O
LSTM. O
Experiments O
performed O
on O
Sports B-DAT

Optical O
flow O
is O
noisy O
on O
Sports B-DAT

number O
of O
noisy O
images O
in O
Sports B-DAT

be O
expected O
given O
that O
the O
Sports B-DAT
dataset O
consists O
of O
YouTube O
videos O

the O
previous O
state-of-art O
on O
the O
Sports B-DAT

models O
that O
are O
trained O
in O
Sports B-DAT

prior O
work O
on O
the O
in O
Sports B-DAT

Compared O
to O
Sports B-DAT

state-of-the-art O
performance O
on O
both O
the O
Sports B-DAT

is O
the O
case O
in O
the O
Sports B-DAT

per- O
formance O
measure O
for O
the O
Sports B-DAT

ResNet O
achieves O
clear O
improvements O
on O
Sports-1M B-DAT
video O
classifica- O
tion O
dataset O
against O

Comparisons O
of O
different O
models O
on O
Sports-1M B-DAT
dataset O
in O
terms O
of O
accuracy O

fine-tuning O
ResNet-152 O
with O
frames O
in O
Sports-1M B-DAT
dataset O
[10] O
may O
achieve O
better O

and O
Top-1&5 O
video-level O
accuracy O
on O
Sports-1M B-DAT

ResNet O
here O
was O
conducted O
on O
Sports-1M B-DAT
dataset O
[10], O
which O
is O
one O

efficient O
training O
on O
the O
large O
Sports-1M B-DAT
training O
set, O
we O
randomly O
select O

architecture O
could O
be O
trained O
on O
Sports-1M B-DAT
dataset O
from O
scratch O
or O
fine O

our O
P3D O
ResNet O
architecture O
on O
Sports-1M B-DAT
dataset, O
the O
networks O
could O
be O

from O
those O
sport-related O
data O
in O
Sports-1M B-DAT
bench- O
mark, O
resulting O
in O
not O

by O
C3D O
learnt O
purely O
on O
Sports-1M B-DAT
data. O
Instead, O
ResNet-152 O
trained O
on O

1M B-DAT
video O
classifica- O
tion O
dataset O
against O

1M B-DAT
dataset O
in O
terms O
of O
accuracy O

1M B-DAT
dataset O
[10] O
may O
achieve O
better O

1M B-DAT

1M B-DAT
dataset O
[10], O
which O
is O
one O

1M B-DAT
training O
set, O
we O
randomly O
select O

1M B-DAT
dataset O
from O
scratch O
or O
fine O

1M B-DAT
dataset, O
the O
networks O
could O
be O

1M B-DAT
bench- O
mark, O
resulting O
in O
not O

1M B-DAT
data. O
Instead, O
ResNet-152 O
trained O
on O

ResNet O
architecture O
learnt O
on O
Sports- O
1M B-DAT
dataset O
validate O
our O
proposal O
and O

ResNet O
achieves O
clear O
improvements O
on O
Sports B-DAT

Comparisons O
of O
different O
models O
on O
Sports B-DAT

fine-tuning O
ResNet-152 O
with O
frames O
in O
Sports B-DAT

and O
Top-1&5 O
video-level O
accuracy O
on O
Sports B-DAT

ResNet O
here O
was O
conducted O
on O
Sports B-DAT

million O
videos O
annotated O
with O
487 O
Sports B-DAT
labels. O
There O
are O
1K-3K O
videos O

efficient O
training O
on O
the O
large O
Sports B-DAT

architecture O
could O
be O
trained O
on O
Sports B-DAT

our O
P3D O
ResNet O
architecture O
on O
Sports B-DAT

from O
those O
sport-related O
data O
in O
Sports B-DAT

by O
C3D O
learnt O
purely O
on O
Sports B-DAT

P3D O
ResNet O
architecture O
learnt O
on O
Sports B-DAT

generalizes O
to O
other O
datasets O
like O
Sports-1M B-DAT
and O
ActivityNet. O
We O
achieve O
state-of-the-art O

with O
the O
avail- O
ability O
of O
Sports-1M B-DAT
[19] O
for O
sports O
videos O
and O

given O
its O
frames. O
Therefore, O
unlike O
Sports-1M B-DAT
and O
ActivityNet, O
YouTube-8M O
is O
not O

on O
other O
benchmarks O
such O
as O
Sports-1M B-DAT
and O
ActivityNet O

available O
video O
benchmarks O
are O
the O
Sports-1M B-DAT
[19], O
with O
487 O
sports O
related O

is O
10 O
− O
15 O
seconds, O
Sports-1M B-DAT
is O
336 O
seconds O
and O
in O

perform O
full O
fine-tuning O
experiments O
on O
Sports-1M, B-DAT
which O
is O
large O
enough O
to O

for O
other O
tasks, O
such O
as O
Sports-1M B-DAT
sports O
classification O
and O
AcitvityNet O
activity O

5.3 O
Results O
on O
Sports-1M B-DAT
Next, O
we O
investigate O
generalization O
of O

learn- O
ing O
experiments O
on O
the O
Sports-1M B-DAT
dataset. O
The O
Sports-1M O
dataset O
[19 O

a) O
Sports-1M B-DAT

YouTube-8M O
dataset O
to O
the O
(a) O
Sports-1M B-DAT
and O
(b) O
ActivityNet O

and O
fine-tune O
them O
on O
the O
Sports-1M B-DAT
dataset O
(along O
with O
a O
new O

various O
video-level O
representations O
on O
the O
Sports-1M B-DAT
dataset. O
Our O
learned O
features O
are O

of O
the O
videos O
in O
the O
Sports-1M B-DAT
dataset, O
including O
optical O
flow, O
and O

video O
classification O
task. O
Similar O
to O
Sports-1M B-DAT
experiments, O
we O
compare O
directly O
train O

experiments O
on O
existing O
video O
benchmarks— O
Sports-1M B-DAT
and O
ActivityNet. O
Our O
experiments O
show O

1M B-DAT
and O
ActivityNet. O
We O
achieve O
state-of-the-art O

1M B-DAT
[19] O
for O
sports O
videos O
and O

1M B-DAT
and O
ActivityNet, O
YouTube-8M O
is O
not O

1M B-DAT
and O
ActivityNet O

1M B-DAT
[19], O
with O
487 O
sports O
related O

activities O
and O
1M B-DAT
videos, O
the O
YFCC-100M O
[34], O
with O

1M B-DAT
is O
336 O
seconds O
and O
in O

1M, B-DAT
which O
is O
large O
enough O
to O

1M B-DAT
sports O
classification O
and O
AcitvityNet O
activity O

1M B-DAT
Next, O
we O
investigate O
generalization O
of O

1M B-DAT
dataset. O
The O
Sports-1M O
dataset O
[19 O

1M B-DAT

1M B-DAT
and O
(b) O
ActivityNet O

1M B-DAT
dataset O
(along O
with O
a O
new O

1M B-DAT
dataset. O
Our O
learned O
features O
are O

1M B-DAT
dataset, O
including O
optical O
flow, O
and O

on O
such O
a O
large O
dataset O
(1M B-DAT
videos), O
pre-training O
on O
YouTube-8M O
still O

1M B-DAT
experiments, O
we O
compare O
directly O
train O

1M B-DAT
and O
ActivityNet. O
Our O
experiments O
show O

generalizes O
to O
other O
datasets O
like O
Sports B-DAT

with O
the O
avail- O
ability O
of O
Sports B-DAT

given O
its O
frames. O
Therefore, O
unlike O
Sports B-DAT

on O
other O
benchmarks O
such O
as O
Sports B-DAT

available O
video O
benchmarks O
are O
the O
Sports B-DAT

Action-adventure O
game O
Strategy O
video O
game O
Sports B-DAT
game O
Call O
of O
Duty O
Grand O

My O
Little O
Pony O
Nike; O
Inc. O
Sports B-DAT
Motorsport O
Football O
Winter O
sport O
Cycling O

is O
10 O
− O
15 O
seconds, O
Sports B-DAT

perform O
full O
fine-tuning O
experiments O
on O
Sports B-DAT

for O
other O
tasks, O
such O
as O
Sports B-DAT

5.3 O
Results O
on O
Sports B-DAT

learn- O
ing O
experiments O
on O
the O
Sports B-DAT

-1M O
dataset. O
The O
Sports B-DAT

a) O
Sports B-DAT

YouTube-8M O
dataset O
to O
the O
(a) O
Sports B-DAT

and O
fine-tune O
them O
on O
the O
Sports B-DAT

various O
video-level O
representations O
on O
the O
Sports B-DAT

of O
the O
videos O
in O
the O
Sports B-DAT

video O
classification O
task. O
Similar O
to O
Sports B-DAT

experiments O
on O
existing O
video O
benchmarks— O
Sports B-DAT

architectures, O
we O
collected O
a O
new O
Sports-1M B-DAT
dataset, O
which O
consists O
of O
1 O

classes O
of O
sports. O
We O
make O
Sports-1M B-DAT
available O
to O
the O
re- O
search O

whether O
features O
learned O
on O
the O
Sports-1M B-DAT
dataset O
are O
generic O
enough O
to O

low-level O
features O
learned O
on O
the O
Sports-1M B-DAT
dataset O
than O
by O
training O
the O

categories O
(which O
we O
release O
as O
Sports-1M B-DAT
dataset) O
and O
report O
significant O
gains O

first O
present O
results O
on O
our O
Sports-1M B-DAT
dataset O
and O

4.1. O
Experiments O
on O
Sports-1M B-DAT

Dataset. O
The O
Sports-1M B-DAT
dataset O
consists O
of O
1 O
million O

Figure O
4: O
Predictions O
on O
Sports-1M B-DAT
test O
data. O
Blue O
(first O
row O

the O
200,000 O
videos O
of O
the O
Sports-1M B-DAT
test O
set. O
Hit@k O
values O
indicate O

results. O
The O
results O
for O
the O
Sports-1M B-DAT

of O
our O
analysis O
on O
the O
Sports-1M B-DAT
dataset O
in O

we O
cannot O
guarantee O
that O
the O
Sports-1M B-DAT
dataset O
has O
no O
overlap O
with O

provides O
the O
best O
performance O
on O
Sports-1M B-DAT

1M B-DAT
dataset, O
which O
consists O
of O
1 O

1M B-DAT
available O
to O
the O
re- O
search O

1M B-DAT
dataset O
are O
generic O
enough O
to O

1M B-DAT
dataset O
than O
by O
training O
the O

1M B-DAT
dataset) O
and O
report O
significant O
gains O

1M B-DAT
dataset O
and O

1M B-DAT

1M B-DAT
dataset O
consists O
of O
1 O
million O

1M B-DAT
test O
data. O
Blue O
(first O
row O

1M B-DAT
test O
set. O
Hit@k O
values O
indicate O

1M B-DAT

1M B-DAT
dataset O
in O

1M B-DAT
dataset O
has O
no O
overlap O
with O

1M B-DAT

architectures, O
we O
collected O
a O
new O
Sports B-DAT

classes O
of O
sports. O
We O
make O
Sports B-DAT

whether O
features O
learned O
on O
the O
Sports B-DAT

low-level O
features O
learned O
on O
the O
Sports B-DAT

categories O
(which O
we O
release O
as O
Sports B-DAT

used O
datasets O
(KTH, O
Weizmann, O
UCF O
Sports, B-DAT
IXMAS, O
Hollywood O
2, O
UCF-50) O
only O

first O
present O
results O
on O
our O
Sports B-DAT

4.1. O
Experiments O
on O
Sports B-DAT

Dataset. O
The O
Sports B-DAT

internal O
nodes O
such O
as O
Aquatic O
Sports, B-DAT
Team O
Sports, O
Winter O
Sports, O
Ball O

Sports, B-DAT
Combat O
Sports, O
Sports O
with O
Animals, O
and O
generally O
becomes O

Figure O
4: O
Predictions O
on O
Sports B-DAT

the O
200,000 O
videos O
of O
the O
Sports B-DAT

results. O
The O
results O
for O
the O
Sports B-DAT

Sports B-DAT
class O
Δ O
AP O
Δ O
AP O

Sports B-DAT
class O
Juggling O
Club O
0.12 O
-0.07 O

per-class O
average O
precision O
for O
all O
Sports B-DAT
classes O
and O
highlight O
the O
ones O

of O
our O
analysis O
on O
the O
Sports B-DAT

flute, O
guitar, O
piano, O
etc.) O
and O
Sports B-DAT

the O
performance O
improve- O
ments O
on O
Sports B-DAT
classes O
relative O
to O
classes O
from O

initialize O
with O
a O
fully O
trained O
Sports B-DAT
CNN O
and O
then O
begin O
training O

same O
evaluation O
protocol O
as O
for O
Sports B-DAT
across O
the O
3 O
suggested O
folds O

Musical O
Instruments O
0.42 O
0.65 O
0.46 O
Sports B-DAT
0.57 O
0.79 O
0.80 O
All O
groups O

we O
cannot O
guarantee O
that O
the O
Sports B-DAT

provides O
the O
best O
performance O
on O
Sports B-DAT

can O
be O
attributed O
to O
the O
Sports B-DAT
categories O
in O
UCF-101, O
but O
the O

to O
improvements O
on O
non-Sports B-DAT
categories: O
Sports O
per- O
formance O
only O
decreases O
from O

a O
dynamic O
dataset O
such O
as O
Sports B-DAT

range O
of O
benchmarks O
except O
for O
Sports-1M B-DAT
and O
UCF101. O
On O
UCF101, O
we O

we O
train O
our O
C3D O
on O
Sports-1M B-DAT
dataset O
[18] O
which O
is O
currently O

Training O
is O
done O
on O
the O
Sports-1M B-DAT
train O
split. O
As O
Sports-1M O
has O

Sports-1M B-DAT
classification O
results: O
Table O
2 O
presents O

Table O
2. O
Sports-1M B-DAT
classification O
result. O
C3D O
outperforms O
[18 O

on O
I380K, O
C3D O
trained O
on O
Sports-1M, B-DAT
and O
C3D O
trained O
on O
I380K O

and O
fine-tuned O
on O
Sports-1M B-DAT

from O
their O
model O
pre-trained O
on O
Sports-1M, B-DAT
spa- O
tial O
stream O
network O
in O

addition, O
C3D O
is O
trained O
on O
Sports-1M B-DAT
and O
used O
as O
is O
without O

1M B-DAT
and O
UCF101. O
On O
UCF101, O
we O

1M B-DAT
dataset O
[18] O
which O
is O
currently O

categories. O
Compared O
with O
UCF101, O
Sports- O
1M B-DAT
has O
5 O
times O
the O
number O

1M B-DAT
train O
split. O
As O
Sports-1M O
has O

1M B-DAT
classification O
results: O
Table O
2 O
presents O

1M B-DAT
classification O
result. O
C3D O
outperforms O
[18 O

1M, B-DAT
and O
C3D O
trained O
on O
I380K O

1M B-DAT

1M, B-DAT
spa- O
tial O
stream O
network O
in O

1M B-DAT
and O
used O
as O
is O
without O

is O
trained O
only O
on O
Sports- O
1M B-DAT
videos O
without O
any O
fine-tuning O
while O

range O
of O
benchmarks O
except O
for O
Sports B-DAT

we O
train O
our O
C3D O
on O
Sports B-DAT

sports O
categories. O
Compared O
with O
UCF101, O
Sports B-DAT

Training O
is O
done O
on O
the O
Sports B-DAT

-1M O
train O
split. O
As O
Sports B-DAT

Sports B-DAT

Table O
2. O
Sports B-DAT

on O
I380K, O
C3D O
trained O
on O
Sports B-DAT

on O
I380K O
and O
fine-tuned O
on O
Sports B-DAT

from O
their O
model O
pre-trained O
on O
Sports B-DAT

addition, O
C3D O
is O
trained O
on O
Sports B-DAT

C3D O
is O
trained O
only O
on O
Sports B-DAT

multiple O
datasets O
we O
test, O
including O
HMDB, B-DAT
Kinetics, O
and O
Moments O
in O
Time O

public O
datasets O
we O
tested, O
including O
HMDB, B-DAT
Kinetics, O
and O
Moments O
in O
time O

detail, O
we O
use O
following O
datasets: O
HMDB B-DAT
[12] O
is O
a O
dataset O
of O

Table O
1. O
HMDB B-DAT
split O
1 O
comparison O
to O
baselines O

HMDB B-DAT
HMDB O

Table O
2. O
HMDB B-DAT
performances O
averaged O
over O
the O
3 O

HMDB B-DAT

Method O
Kinetics O
Charades O
HMDB B-DAT
MiT O

38.1 O
81.8 O
31.1 O
Evolved O
on O
HMDB B-DAT
77.0 O
37.5 O
82.3 O
31.6 O
Best O

of O
them O
when O
evolved O
for O
HMDB B-DAT
or O
Kinetics. O
An O
average O
activity O

are O
around O
12 O
seconds, O
while O
HMDB B-DAT
and O
Kinetics O
videos O
are O
on O

search O
vs. O
evolutionary O
algorithm O
on O
HMDB B-DAT

T. O
Poggio, O
and O
T. O
Serre. O
HMDB B-DAT

176 O
× O
176 O
(for O
HMDB B-DAT
and O
Kinetics) O
or O
64 O

T. O
Poggio, O
and O
T. O
Serre. O
HMDB B-DAT

such O
as O
Kinetics O
[13] O
and O
HMDB B-DAT
[14]. O
We O
hypothesize O
that O
the O

infor- O
mation. O
Note O
that O
our O
HMDB B-DAT
performance O
in O
this O
table O
is O

HMDB51 B-DAT
datasets O
using O
100 O
iterations O
to O

HMDB B-DAT

HMDB B-DAT

Tiny-Kinetics O
HMDB B-DAT

in- O
puts. O
Note O
that O
the O
HMDB B-DAT
performance O
in O
[21] O
was O
reported O

HMDB B-DAT

on O
Kinetics O
before O
training/testing O
with O
HMDB B-DAT

Kinetics O
HMDB B-DAT
HMDB O

the O
state-of-the-arts O
on O
Kinetics O
and O
HMDB B-DAT

T. O
Poggio, O
and O
T. O
Serre. O
HMDB B-DAT

V100 O
GPUs. O
When O
fine-tuning O
on O
HMDB, B-DAT
the O
learning O
rate O
started O
at O

dropout O
at O
0.5 O
and O
for O
HMDB B-DAT
it O
was O
set O
to O
0.8 O

HMDB51 B-DAT
datasets O
using O
100 O
iterations O
to O

action O
classification O
datasets O
(UCF-101 O
and O
HMDB B-DAT

action O
classification, O
reaching O
80.9% O
on O
HMDB B-DAT

magnitude O
larger O
than O
previous O
datasets, O
HMDB B-DAT

and O
then O
fine-tuning O
each O
on O
HMDB B-DAT

layer, O
showing O
some O
improvement O
on O
HMDB B-DAT
while O
requiring O
less O
test O
time O

5k O
steps O
on O
UCF-101 O
and O
HMDB B-DAT

and O
testing O
on O
either O
UCF-101, O
HMDB B-DAT

test O
sets O
of O
UCF-101 O
and O
HMDB B-DAT

UCF-101 O
HMDB B-DAT

testing O
on O
split O
1 O
of O
HMDB B-DAT

rameters O
and O
that O
UCF-101 O
and O
HMDB B-DAT

is O
however O
higher O
than O
on O
HMDB B-DAT

lack O
of O
training O
data O
in O
HMDB B-DAT

on O
UCF-101, O
much O
higher O
on O
HMDB B-DAT

HMDB B-DAT

HMDB B-DAT

HMDB B-DAT

HMDB B-DAT

HMDB B-DAT

directly O
training O
on O
UCF-101 O
and O
HMDB B-DAT

table O
5, O
on O
UCF-101 O
and O
HMDB B-DAT

on O
UCF-101 O
and O
70.3% O
on O
HMDB B-DAT

UCF-101 O
HMDB B-DAT

Performance O
on O
the O
UCF-101 O
and O
HMDB B-DAT

Original: O
train O
on O
UCF-101 O
or O
HMDB B-DAT

layer O
trained O
on O
UCF-101 O
or O
HMDB B-DAT

end-to-end O
fine-tuning O
on O
UCF-101 O
or O
HMDB B-DAT

Model O
UCF-101 O
HMDB B-DAT

state-of-the-art O
on O
the O
UCF-101 O
and O
HMDB B-DAT

on O
UCF-101 O
and O
80.9 O
on O
HMDB B-DAT

HMDB B-DAT

T. O
Poggio, O
and O
T. O
Serre. O
HMDB B-DAT

Iwashita O
et O
al. O
2014) O
and O
HMDB B-DAT
dataset O
(Kuehne O
et O
al. O
2011 O

is O
an O
extremely O
challenging O
dataset. O
HMDB B-DAT
was O
chosen O
due O
to O
its O

DogCentric) O
or O
51 O
nodes O
(for O
HMDB) B-DAT
and O
used O
soft-max. O
The O
network O

HMDB B-DAT
HMDB O
is O
a O
relatively O
large-scale O
video O

the O
ap- O
proaches O
using O
the O
HMDB B-DAT
Dataset. O
In O
this O
experiment, O
we O

per-activity O
temporal O
filters, O
tested O
with O
HMDB B-DAT

T.; O
and O
Serre, O
T. O
2011. O
HMDB B-DAT

HMDB B-DAT

400 B-DAT
[19] O
for O
the O
ablation O
experi O

contains O
about O
260K O
videos O
of O
400 B-DAT
different O
human O
action O
categories. O
We O

400 B-DAT
[19]. O
Sports1M O
is O
a O
large-scale O

Dataset. O
We O
use O
Kinetics-400 B-DAT
[19] O
for O
the O
ablation O
experi O

CSNs O
on O
Sports1M O
[18] O
and O
Kinetics-400 B-DAT
[19]. O
Sports1M O
is O
a O
large-scale O

yet O
accurate. O
On O
Sports1M O
and O
Kinetics, B-DAT
our O
CSNs O
are O
com- O
parable O

art O
methods O
on O
Sports1M O
and O
Kinetics B-DAT
while O
being O
2–3 O
times O
faster O

Dataset. O
We O
use O
Kinetics B-DAT

experi- O
ments O
in O
this O
section. O
Kinetics B-DAT
is O
a O
standard O
benchmark O
for O

model O
ob- O
tains O
69.7% O
on O
Kinetics B-DAT
validation O
(vs. O
70.3% O
of O
vanilla O

for O
ip-CSN-101 O
and O
ResNet3D-101 O
on O
Kinetics B-DAT

video O
top-1 O
accuracy O
on O
the O
Kinetics B-DAT
validation O
set O
vs O
the O
model O

Video O
top-1 O
accuracy O
on O
the O
Kinetics B-DAT
validation O
set O
against O
computation O
cost O

CSNs O
on O
Sports1M O
[18] O
and O
Kinetics B-DAT

provided O
with O
the O
dataset. O
For O
Kinetics, B-DAT
we O
use O
the O
train O
split O

for O
video O
prediction. O
On O
Kinetics, B-DAT
since O
the O
30 O
crops O
eval O

from O
our O
ablation. O
Results O
on O
Kinetics B-DAT

Accuracy O
is O
measured O
on O
the O
Kinetics B-DAT
validation O
set. O
For O
fair O
evaluation O

two O
major O
benchmarks: O
Sports1M O
and O
Kinetics B-DAT

Dataset. O
We O
use O
Kinetics-400 B-DAT
[19] O
for O
the O
ablation O
experi O

CSNs O
on O
Sports1M O
[18] O
and O
Kinetics-400 B-DAT
[19]. O
Sports1M O
is O
a O
large-scale O

with O
260k O
videos O
[19] O
of O
400 B-DAT
human O
action O
classes. O
We O
use O

the O
best O
audio-visual O
results O
on O
Kinetics B-DAT
in O
Table O
2. O
Pre-training O
fails O

Kinetics B-DAT

late O
fusion O
multi-modal O
networks O
on O
Kinetics B-DAT
and O
mini-Sports O
using O
video O
top-1 O

a O
multi-modal O
network O
(RGB+Audio) O
on O
Kinetics B-DAT

state-of-the-art O
accuracy O
on O
benchmarks O
including O
Kinetics, B-DAT
Sports1M, O
and O
AudioSet. O
It O
applies O

datasets O
for O
our O
ablation O
experiments: O
Kinetics, B-DAT
mini-Sports, O
and O
mini- O
AudioSet. O
Kinetics O

the O
optimal O
weights O
(8% O
for O
Kinetics B-DAT
and O
mini-Sports, O
13% O
for O
mini-AudioSet O

curves O
of O
these O
models O
on O
Kinetics B-DAT
(left) O
and O
mini-Sports O
(right). O
On O

both O
Kinetics B-DAT
and O
mini-Sports, O
the O
audio O
model O

Kinetics B-DAT
Learning O
Curve O

of O
naive O
audio-video O
models O
on O
Kinetics B-DAT
and O
mini-Sports. O
The O
learning O
curves O

joint O
audio-video O
(AV) O
model O
on O
Kinetics B-DAT
(left) O
and O
mini-Sports O
(right). O
Solid O

and O
single O
best O
modality O
on O
Kinetics B-DAT

on O
different O
multi-modal O
problems O
on O
Kinetics B-DAT

On O
Kinetics, B-DAT
we O
study O
all O
combinations O
of O

and O
benchmarks: O
action O
recog- O
nition O
(Kinetics), B-DAT
sport O
classification O
(mini-Sports), O
and O
acoustic O

by O
significant O
margins O
on O
both O
Kinetics B-DAT
and O
mini-Sports. O
On O
mini-AudioSet, O
Gradient-Blend O

failures O
of O
auxiliary O
loss O
on O
Kinetics B-DAT
and O
mini-Sports O
demonstrates O
that O
the O

and O
bottom O
20 O
classes O
on O
Kinetics B-DAT
where O
Gradient-Blend O
makes O
the O
most O

Dataset O
Kinetics B-DAT
mini-Sports O
mini-AudioSet O
Method O
Clip O
V@1 O

well O
as O
single-modal O
networks O
on O
Kinetics, B-DAT
mini-Sports, O
and O
mini-AudioSet. O
G-Blend O
consistently O

them O
with O
state-of-the-art O
methods O
on O
Kinetics, B-DAT
Sports1M, O
and O
AudioSet. O
Our O
G-Blend O

for O
Sports-1M O
and O
AudioSet. O
For O
Kinetics, B-DAT
we O
follow O
the O
same O
30-crop O

with O
current O
state-of-the-art O
methods O
on O
Kinetics B-DAT

and O
achieves O
state-of-the-art O
accuracy O
on O
Kinetics B-DAT

competitive O
methods O
reporting O
results O
on O
Kinetics, B-DAT
due O
to O
the O
space O
limit O

Comparison O
with O
state-of-the-art O
methods O
on O
Kinetics B-DAT

or O
more O
FC O
layers O
on O
Kinetics B-DAT

block O
2 O
is O
unfeasible. O
On O
Kinetics, B-DAT
we O
found O
3-D O
concat O
after O

times. O
We O
found O
that O
on O
Kinetics, B-DAT
it O
works O
the O
best O
when O

Kinetics B-DAT

400 B-DAT
and O
Something-Something-V1, O
the O
two O
major O

action O
classification O
benchmarks: O
Kinetics-400 B-DAT
[15] O
(400 O
classes) O
and O
Something-Something-V1 O
[11] O
(174 O

is O
the O
number O
of O
classes O
(400 B-DAT
for O
Kinetics-400 O
and O
174 O
for O

400 B-DAT
[15] O
and O
Something-Something- O
V1 O
[11 O

]. O
Kinetics-400 B-DAT
consists O
of O
400 O
actions. O
Its O
videos O
are O
from O

videos, O
well O
balanced O
across O
all O
400 B-DAT
classes. O
The O
test O
set O
labels O

400 B-DAT
is O
an O
excellent O
dataset O
for O

400 B-DAT
and O
their O
duration O
typically O
spans O

400, B-DAT
temporal O
information O
and O
tempo- O
ral O

400 B-DAT
(e.g., O
’building O
cabi- O
net’), O
they O

400 B-DAT
dataset. O
Our O
baseline, O
which O
consists O

400, B-DAT
on O
Something- O
Something-V1 O
we O
follow O

400 B-DAT
dataset. O
This O
dataset O
provides O
a O

of O
a O
large O
set O
of O
400 B-DAT
classes. O
First, O
we O
present O
an O

are O
defined O
by O
the O
Kinetics- O
400 B-DAT
dataset O
and O
were O
originally O
generated O

by O
manually O
clus- O
tering O
the O
400 B-DAT
classes O
into O
38 O
parent O
classes O

400 B-DAT
(table O
4). O
We O
compare O
against O

400 B-DAT
dataset O

400 B-DAT
I3D O
+ O
GCN O
[30] O
(ECCV’18 O

400 B-DAT
Non-local O
I3D O
+ O
GCN O
[30 O

400 B-DAT
TrajectoryNet O
[39] O
(NIPS’18) O
ResNet18 O
44.0 O

400 B-DAT
Our O
baseline O
(GB, O
sec. O
3.1 O

400 B-DAT
dataset. O
While O

we O
obtain O
state-of-the-art O
performance O
on O
Kinetics-400 B-DAT
and O
Something-Something-V1, O
the O
two O
major O

large- O
scale O
action O
classification O
benchmarks: O
Kinetics-400 B-DAT
[15] O
(400 O
classes) O
and O
Something-Something-V1 O

number O
of O
classes O
(400 O
for O
Kinetics-400 B-DAT
and O
174 O
for O
Something-Something-V1), O
while O

largest O
datasets O
for O
action O
recognition: O
Kinetics-400 B-DAT
[15] O
and O
Something-Something- O
V1 O
[11 O

]. O
Kinetics-400 B-DAT
consists O
of O
400 O
actions. O
Its O

performance O
on O
the O
validation O
set. O
Kinetics-400 B-DAT
is O
an O
excellent O
dataset O
for O

on O
average O
than O
those O
of O
Kinetics-400 B-DAT
and O
their O
duration O
typically O
spans O

two O
datasets O
is O
that, O
on O
Kinetics-400, B-DAT
temporal O
information O
and O
tempo- O
ral O

they O
are O
very O
specific O
on O
Kinetics-400 B-DAT
(e.g., O
’building O
cabi- O
net’), O
they O

of O
our O
model O
on O
the O
Kinetics-400 B-DAT
dataset. O
Our O
baseline, O
which O
consists O

the O
standard O
testing O
protocol O
for O
Kinetics-400, B-DAT
on O
Something- O
Something-V1 O
we O
follow O

proach O
on O
the O
Kinetics-400 B-DAT
dataset. O
This O
dataset O
provides O
a O

report O
Top-1 O
and O
Top-5 O
accuracy. O
Kinetics-400 B-DAT
(table O
4). O
We O
compare O
against O

in O
the O
literature O
on O
the O
Kinetics-400 B-DAT
dataset O

42] O
(ECCV’18) O
BN-Inception+ResNet18 O
46.4 O
– O
Kinetics-400 B-DAT
I3D O
+ O
GCN O
[30] O
(ECCV’18 O

) O
ResNet50 O
43.3 O
75.1 O
Kinetics-400 B-DAT
Non-local O
I3D O
+ O
GCN O
[30 O

] O
(ECCV’18) O
ResNet50 O
46.1 O
76.8 O
Kinetics-400 B-DAT
TrajectoryNet O
[39] O
(NIPS’18) O
ResNet18 O
44.0 O

39] O
(NIPS’18) O
ResNet18 O
47.8 O
– O
Kinetics-400 B-DAT
Our O
baseline O
(GB, O
sec. O
3.1 O

3.8% O
by O
pre-training O
on O
the O
Kinetics-400 B-DAT
dataset. O
While O

we O
obtain O
state-of-the-art O
performance O
on O
Kinetics B-DAT

large- O
scale O
action O
classification O
benchmarks: O
Kinetics B-DAT

number O
of O
classes O
(400 O
for O
Kinetics B-DAT

largest O
datasets O
for O
action O
recognition: O
Kinetics B-DAT

15] O
and O
Something-Something- O
V1 O
[11]. O
Kinetics B-DAT

performance O
on O
the O
validation O
set. O
Kinetics B-DAT

on O
average O
than O
those O
of O
Kinetics B-DAT

two O
datasets O
is O
that, O
on O
Kinetics B-DAT

they O
are O
very O
specific O
on O
Kinetics B-DAT

of O
our O
model O
on O
the O
Kinetics B-DAT

the O
standard O
testing O
protocol O
for O
Kinetics B-DAT

proach O
on O
the O
Kinetics B-DAT

These O
are O
defined O
by O
the O
Kinetics B-DAT

report O
Top-1 O
and O
Top-5 O
accuracy. O
Kinetics B-DAT

in O
the O
literature O
on O
the O
Kinetics B-DAT

42] O
(ECCV’18) O
BN-Inception+ResNet18 O
46.4 O
– O
Kinetics B-DAT

30] O
(ECCV’18) O
ResNet50 O
43.3 O
75.1 O
Kinetics B-DAT

30] O
(ECCV’18) O
ResNet50 O
46.1 O
76.8 O
Kinetics B-DAT

39] O
(NIPS’18) O
ResNet18 O
47.8 O
– O
Kinetics B-DAT

3.8% O
by O
pre-training O
on O
the O
Kinetics B-DAT

a O
new O
model O
and O
the O
Kinetics B-DAT
dataset. O
In O
IEEE O
Conference O
on O

we O
obtain O
state-of-the-art O
performance O
on O
Kinetics-400 B-DAT
and O
Something-Something-V1, O
the O
two O
major O

large- O
scale O
action O
classification O
benchmarks: O
Kinetics-400 B-DAT
[15] O
(400 O
classes) O
and O
Something-Something-V1 O

number O
of O
classes O
(400 O
for O
Kinetics-400 B-DAT
and O
174 O
for O
Something-Something-V1), O
while O

largest O
datasets O
for O
action O
recognition: O
Kinetics-400 B-DAT
[15] O
and O
Something-Something- O
V1 O
[11 O

]. O
Kinetics-400 B-DAT
consists O
of O
400 O
actions. O
Its O

performance O
on O
the O
validation O
set. O
Kinetics-400 B-DAT
is O
an O
excellent O
dataset O
for O

on O
average O
than O
those O
of O
Kinetics-400 B-DAT
and O
their O
duration O
typically O
spans O

two O
datasets O
is O
that, O
on O
Kinetics-400, B-DAT
temporal O
information O
and O
tempo- O
ral O

they O
are O
very O
specific O
on O
Kinetics-400 B-DAT
(e.g., O
’building O
cabi- O
net’), O
they O

of O
our O
model O
on O
the O
Kinetics-400 B-DAT
dataset. O
Our O
baseline, O
which O
consists O

the O
standard O
testing O
protocol O
for O
Kinetics-400, B-DAT
on O
Something- O
Something-V1 O
we O
follow O

proach O
on O
the O
Kinetics-400 B-DAT
dataset. O
This O
dataset O
provides O
a O

These O
are O
defined O
by O
the O
Kinetics- B-DAT
400 O
dataset O
and O
were O
originally O

report O
Top-1 O
and O
Top-5 O
accuracy. O
Kinetics-400 B-DAT
(table O
4). O
We O
compare O
against O

in O
the O
literature O
on O
the O
Kinetics-400 B-DAT
dataset O

42] O
(ECCV’18) O
BN-Inception+ResNet18 O
46.4 O
– O
Kinetics-400 B-DAT
I3D O
+ O
GCN O
[30] O
(ECCV’18 O

) O
ResNet50 O
43.3 O
75.1 O
Kinetics-400 B-DAT
Non-local O
I3D O
+ O
GCN O
[30 O

] O
(ECCV’18) O
ResNet50 O
46.1 O
76.8 O
Kinetics-400 B-DAT
TrajectoryNet O
[39] O
(NIPS’18) O
ResNet18 O
44.0 O

39] O
(NIPS’18) O
ResNet18 O
47.8 O
– O
Kinetics-400 B-DAT
Our O
baseline O
(GB, O
sec. O
3.1 O

3.8% O
by O
pre-training O
on O
the O
Kinetics-400 B-DAT
dataset. O
While O

on O
public O
datasets O
such O
as O
Kinetics B-DAT
[13] O
and O
HMDB O
[14]. O
We O

large O
video O
datasets O
such O
as O
Kinetics B-DAT
[13]. O
However, O
these O
approaches O
still O

used O
a O
subset O
of O
the O
Kinetics B-DAT
dataset O
[13] O
with O
100k O
videos O

Kinetics B-DAT

Kinetics B-DAT
and O
LowRes-HMDB51 O
datasets O
using O
100 O

Kinetics B-DAT
LowRes-HMDB O

Kinetics B-DAT
LowRes-HMDB O

of O
iterations O
on O
our O
Tiny- O
Kinetics B-DAT
dataset O
for O
learning O
and O
not O

Kinetics B-DAT
dataset O
using O
10 O
iterations O
with O

Kinetics B-DAT

Kinetics B-DAT

Kinetics B-DAT

Kinetics B-DAT
HMDB O

the O
model O
was O
pre-trained O
on O
Kinetics B-DAT
before O
training/testing O
with O
HMDB. O
Missing O

Kinetics B-DAT
HMDB O
HMDB(+Kin) O
Run-time O
(ms O

accuracies O
with O
the O
state-of-the-arts O
on O
Kinetics B-DAT
and O
HMDB. O
For O
this, O
we O

momentum O
set O
to O
0.9. O
For O
Kinetics B-DAT
and O
Tiny-Kinetics, O
the O
initial O
learning O

iterations O
of O
the O
algorithm. O
For O
Kinetics B-DAT
and O
Tiny- O
Kinetics, O
we O
used O

with O
260k O
videos O
[19] O
of O
400 B-DAT
human O
action O
classes. O
We O
use O

the O
best O
audio-visual O
results O
on O
Kinetics B-DAT
in O
Table O
2. O
Pre-training O
fails O

Kinetics B-DAT

late O
fusion O
multi-modal O
networks O
on O
Kinetics B-DAT
and O
mini-Sports O
using O
video O
top-1 O

a O
multi-modal O
network O
(RGB+Audio) O
on O
Kinetics B-DAT

state-of-the-art O
accuracy O
on O
benchmarks O
including O
Kinetics, B-DAT
Sports1M, O
and O
AudioSet. O
It O
applies O

datasets O
for O
our O
ablation O
experiments: O
Kinetics, B-DAT
mini-Sports, O
and O
mini- O
AudioSet. O
Kinetics O

the O
optimal O
weights O
(8% O
for O
Kinetics B-DAT
and O
mini-Sports, O
13% O
for O
mini-AudioSet O

curves O
of O
these O
models O
on O
Kinetics B-DAT
(left) O
and O
mini-Sports O
(right). O
On O

both O
Kinetics B-DAT
and O
mini-Sports, O
the O
audio O
model O

Kinetics B-DAT
Learning O
Curve O

of O
naive O
audio-video O
models O
on O
Kinetics B-DAT
and O
mini-Sports. O
The O
learning O
curves O

joint O
audio-video O
(AV) O
model O
on O
Kinetics B-DAT
(left) O
and O
mini-Sports O
(right). O
Solid O

and O
single O
best O
modality O
on O
Kinetics B-DAT

on O
different O
multi-modal O
problems O
on O
Kinetics B-DAT

On O
Kinetics, B-DAT
we O
study O
all O
combinations O
of O

and O
benchmarks: O
action O
recog- O
nition O
(Kinetics), B-DAT
sport O
classification O
(mini-Sports), O
and O
acoustic O

by O
significant O
margins O
on O
both O
Kinetics B-DAT
and O
mini-Sports. O
On O
mini-AudioSet, O
Gradient-Blend O

failures O
of O
auxiliary O
loss O
on O
Kinetics B-DAT
and O
mini-Sports O
demonstrates O
that O
the O

and O
bottom O
20 O
classes O
on O
Kinetics B-DAT
where O
Gradient-Blend O
makes O
the O
most O

Dataset O
Kinetics B-DAT
mini-Sports O
mini-AudioSet O
Method O
Clip O
V@1 O

well O
as O
single-modal O
networks O
on O
Kinetics, B-DAT
mini-Sports, O
and O
mini-AudioSet. O
G-Blend O
consistently O

them O
with O
state-of-the-art O
methods O
on O
Kinetics, B-DAT
Sports1M, O
and O
AudioSet. O
Our O
G-Blend O

for O
Sports-1M O
and O
AudioSet. O
For O
Kinetics, B-DAT
we O
follow O
the O
same O
30-crop O

with O
current O
state-of-the-art O
methods O
on O
Kinetics B-DAT

and O
achieves O
state-of-the-art O
accuracy O
on O
Kinetics B-DAT

competitive O
methods O
reporting O
results O
on O
Kinetics, B-DAT
due O
to O
the O
space O
limit O

Comparison O
with O
state-of-the-art O
methods O
on O
Kinetics B-DAT

or O
more O
FC O
layers O
on O
Kinetics B-DAT

block O
2 O
is O
unfeasible. O
On O
Kinetics, B-DAT
we O
found O
3-D O
concat O
after O

times. O
We O
found O
that O
on O
Kinetics, B-DAT
it O
works O
the O
best O
when O

Kinetics B-DAT

as O
one O
of O
400 B-DAT
human O
action O
categories. O
Note O
that O

Moments O
in O
Time O
[17] O
and O
Kinetics B-DAT
[2]. O
Ac O

Kinetics. B-DAT
The O
Kinetics O
dataset O
contains O
236763 O
training O

Kinetics B-DAT
dataset O
contains O
a O
bit O
more O

Kinetics B-DAT
CoST(a) O
73.6 O
90.8 O
82.2 O

the O
Moments O
in O
Time O
and O
Kinetics B-DAT

Kinetics B-DAT
73.2 O
90.2 O
81.7 O

validation O
set O
of O
Kinetics B-DAT

frames O
mentioned O
earlier. O
While O
on O
Kinetics, B-DAT
we O
sample O
32 O

on O
the O
validation O
set O
of O
Kinetics B-DAT

On O
the O
Kinetics B-DAT
dataset, O
CoST O
achieves O
state-of-the-art O

and O
0.19 O
respectively. O
While O
on O
Kinetics B-DAT
they O
are O
0.77, O
0.08 O
and O

discriminate O
different O
actions O
than O
Kinetics B-DAT

Moments O
in O
Time O
Kinetics B-DAT

400 B-DAT
dataset), O
which O
has O
about O
25k O

400, B-DAT
and O
compares O
with O
base- O
lines O

400 B-DAT
Nov. O
2018 O
version. O
Note O
that O

400 B-DAT

400 B-DAT
are O
added O
for O
context. O
These O

0 O
200 O
400 B-DAT
600 O
800 O
1000 O
1200 O
1400 O

400 B-DAT

400 B-DAT
accuracy. O
Note O
that O
* O
are O

400 B-DAT
new O
old O

use O
the O
currently O
available O
version O
(Kinetics-400 B-DAT
dataset), O
which O
has O
about O
25k O

accuracy O
of O
our O
algorithm O
on O
Kinetics-400, B-DAT
and O
compares O
with O
base- O
lines O

Table O
3. O
Performances O
on O
Kinetics-400 B-DAT
Nov. O
2018 O
version. O
Note O
that O

than O
the O
initial O
version O
of O
Kinetics-400 B-DAT

V100 O
GPU. O
Accuracy O
numbers O
on O
Kinetics-400 B-DAT
are O
added O
for O
context. O
These O

two O
different O
set- O
tings O
of O
Kinetics-400 B-DAT

Table O
13. O
Kinetics-400 B-DAT
accuracy. O
Note O
that O
* O
are O

Method O
Kinetics-400 B-DAT
new O
old O

datasets O
we O
test, O
including O
HMDB, O
Kinetics, B-DAT
and O
Moments O
in O
Time. O
We O

ResNet-like O
architectures O
obtained O
for O
the O
Kinetics B-DAT
dataset. O
Modules O
are O
repeated O
R O

datasets O
we O
tested, O
including O
HMDB, O
Kinetics, B-DAT
and O
Moments O
in O
time. O
This O

activity O
recognition O
video O
datasets O
including O
Kinetics B-DAT

videos O
of O
51 O
action O
classes. O
Kinetics B-DAT
[10] O
is O
a O
large O
challenging O

use O
the O
currently O
available O
version O
(Kinetics B-DAT

fewer O
training O
videos O
than O
original O
Kinetics B-DAT
dataset O
(i.e., O
missing O
about O
10 O

found O
with O
shorter O
videos O
like O
Kinetics B-DAT

to O
baselines, O
with O
and O
without O
Kinetics B-DAT
pre-training. O
The O
models O
were O
all O

Kinetics B-DAT

accuracy O
of O
our O
algorithm O
on O
Kinetics B-DAT

Table O
3. O
Performances O
on O
Kinetics B-DAT

than O
the O
initial O
version O
of O
Kinetics B-DAT

previously O
reported O
results O
(we O
use O
Kinetics B-DAT
pre-training O
as O
in O
[36]). O
As O

evaluate O
the O
models O
evolved O
on O
Kinetics B-DAT
by O
training O
it O
on O
another O

Method O
Kinetics B-DAT
Charades O
HMDB O
MiT O

Evolved O
on O
Kinetics B-DAT
77.2 O
37.8 O
82.3 O
31.8 O
Evolved O

V100 O
GPU. O
Accuracy O
numbers O
on O
Kinetics B-DAT

from O
different O
hybrid O
meta- O
architectures. O
Kinetics B-DAT
dataset O

and O
have O
longest O
temporal O
duration. O
Kinetics B-DAT
dataset O

when O
evolved O
for O
HMDB O
or O
Kinetics B-DAT

12 O
seconds, O
while O
HMDB O
and O
Kinetics B-DAT
videos O
are O
on O
the O
average O

11. O
Stretching O
iTGM O
kernels O
from O
Kinetics B-DAT
to O
Charades O

Stretched O
(L O
= O
11) O
34.2 O
Kinetics B-DAT
EvaNet O
37.7 O
Kinetics O
EvaNet O
Stretched O

a O
model O
from O
the O
Kinetics B-DAT
dataset O
and O
‘stretch’ O
the O
iTGM O

with O
L O
= O
3 O
on O
Kinetics B-DAT
and O
stretched O
to O
L O

176 O
(for O
HMDB O
and O
Kinetics) B-DAT
or O
64 O
× O
176 O

Table O
12. O
Kinetics B-DAT
performance O
comparison O
to O
baselines O

two O
different O
set- O
tings O
of O
Kinetics B-DAT

-400. O
Note O
that O
Kinetics B-DAT
is O
periodically O
re- O
moving O
some O

Table O
13. O
Kinetics B-DAT

num- O
bers O
on O
the O
initial O
Kinetics B-DAT
dataset, O
which O
is O
no O
longer O

numbers O
based O
on O
the O
new O
Kinetics B-DAT
version O
from O
Nov O
2018. O
The O

Method O
Kinetics B-DAT

architectures O
found O
when O
searching O
on O
Kinetics B-DAT
using O
RGB O
inputs. O
We O
observe O

ini- O
tialized O
with O
ImageNet O
or O
Kinetics B-DAT
weights O

ImageNet O
Kinetics B-DAT

to O
the O
architectures O
found O
on O
Kinetics B-DAT

of O
evolved O
ar- O
chitectures O
per O
Kinetics B-DAT

-RGB O
and O
Kinetics B-DAT

Figure O
10. O
Kinetics B-DAT
RGB O
Top O
1 O
with O
Inception O

Figure O
11. O
Kinetics B-DAT
RGB O
Top O
2 O
with O
Inception O

Figure O
12. O
Kinetics B-DAT
RGB O
Top O
3 O
with O
Inception O

Figure O
13. O
Kinetics B-DAT
optical O
flow O
Top O
1 O
with O

Figure O
14. O
Kinetics B-DAT
optical O
flow O
Top O
2 O
with O

Figure O
15. O
Kinetics B-DAT
optical O
flow O
Top O
3 O
with O

Figure O
19. O
Kinetics B-DAT
RGB O
Top O
1 O
with O
ResNet O

Figure O
20. O
Kinetics B-DAT
RGB O
Top O
2 O
with O
ResNet O

Figure O
21. O
Kinetics B-DAT
RGB O
Top O
3 O
with O
ResNet O

Figure O
22. O
Kinetics B-DAT
optical O
flow O
Top O
1 O
with O

Figure O
23. O
Kinetics B-DAT
optical O
flow O
Top O
2 O
with O

Figure O
24. O
Kinetics B-DAT
optical O
flow O
Top O
3 O
with O

use O
the O
currently O
available O
version O
(Kinetics-400 B-DAT
dataset), O
which O
has O
about O
25k O

accuracy O
of O
our O
algorithm O
on O
Kinetics-400, B-DAT
and O
compares O
with O
base- O
lines O

Table O
3. O
Performances O
on O
Kinetics-400 B-DAT
Nov. O
2018 O
version. O
Note O
that O

than O
the O
initial O
version O
of O
Kinetics-400 B-DAT

V100 O
GPU. O
Accuracy O
numbers O
on O
Kinetics-400 B-DAT
are O
added O
for O
context. O
These O

two O
different O
set- O
tings O
of O
Kinetics-400 B-DAT

Table O
13. O
Kinetics-400 B-DAT
accuracy. O
Note O
that O
* O
are O

Method O
Kinetics-400 B-DAT
new O
old O

of O
evolved O
ar- O
chitectures O
per O
Kinetics- B-DAT

RGB O
and O
Kinetics- B-DAT

400 B-DAT
[19] O
for O
the O
ablation O
experi O

contains O
about O
260K O
videos O
of O
400 B-DAT
different O
human O
action O
categories. O
We O

400 B-DAT
[19]. O
Sports1M O
is O
a O
large-scale O

Dataset. O
We O
use O
Kinetics-400 B-DAT
[19] O
for O
the O
ablation O
experi O

CSNs O
on O
Sports1M O
[18] O
and O
Kinetics-400 B-DAT
[19]. O
Sports1M O
is O
a O
large-scale O

yet O
accurate. O
On O
Sports1M O
and O
Kinetics, B-DAT
our O
CSNs O
are O
com- O
parable O

art O
methods O
on O
Sports1M O
and O
Kinetics B-DAT
while O
being O
2–3 O
times O
faster O

Dataset. O
We O
use O
Kinetics B-DAT

experi- O
ments O
in O
this O
section. O
Kinetics B-DAT
is O
a O
standard O
benchmark O
for O

model O
ob- O
tains O
69.7% O
on O
Kinetics B-DAT
validation O
(vs. O
70.3% O
of O
vanilla O

for O
ip-CSN-101 O
and O
ResNet3D-101 O
on O
Kinetics B-DAT

video O
top-1 O
accuracy O
on O
the O
Kinetics B-DAT
validation O
set O
vs O
the O
model O

Video O
top-1 O
accuracy O
on O
the O
Kinetics B-DAT
validation O
set O
against O
computation O
cost O

CSNs O
on O
Sports1M O
[18] O
and O
Kinetics B-DAT

provided O
with O
the O
dataset. O
For O
Kinetics, B-DAT
we O
use O
the O
train O
split O

for O
video O
prediction. O
On O
Kinetics, B-DAT
since O
the O
30 O
crops O
eval O

from O
our O
ablation. O
Results O
on O
Kinetics B-DAT

Accuracy O
is O
measured O
on O
the O
Kinetics B-DAT
validation O
set. O
For O
fair O
evaluation O

two O
major O
benchmarks: O
Sports1M O
and O
Kinetics B-DAT

Dataset. O
We O
use O
Kinetics-400 B-DAT
[19] O
for O
the O
ablation O
experi O

CSNs O
on O
Sports1M O
[18] O
and O
Kinetics-400 B-DAT
[19]. O
Sports1M O
is O
a O
large-scale O

Kinetics O
contains O
300k O
videos O
spanning O
400 B-DAT
human O
action O
classes O
with O
more O

than O
400 B-DAT
examples O
for O
each O
class. O
The O

action O
datasets: O
HMDB51, O
UCF101, O
and O
Kinetics B-DAT

on O
the O
HMDB51, O
UCF101 O
and O
Kinetics B-DAT
datasets. O
In O
particular, O
if O
the O

it O
outperforms O
models O
pre-trained O
on O
Kinetics B-DAT

and O
beating O
I3D O
pre-trained O
on O
Kinetics, B-DAT
thus O
clearly O
showing O
the O
impact O

UCF101 O
(96.9%), O
HMDB51 O
(74.5%) O
and O
Kinetics B-DAT
(73.5 O

such O
as O
ActivityNet O
[4] O
and O
Kinetics B-DAT
[22]. O
ActivityNet O
contains O
849 O
hours O

video, O
including O
28,000 O
action O
instances. O
Kinetics B-DAT
contains O
300k O
videos O
spanning O
400 O

140K O
’19 O
Kinetics B-DAT
[22] O
- O
- O
600 O

such O
as O
ActivityNet, O
Kinetics, B-DAT
and O
YouTube-8M O
have O
col- O
lected O

category O
taxonomy O
diversity O
of O
Youtube8M, O
Kinetics B-DAT

datasets, O
such O
as, O
Sports1M O
[21], O
Kinetics B-DAT
[22], O
and O
AVA O
[18] O
were O

which O
have O
competitive O
results O
on O
Kinetics B-DAT
and O
UCF101 O
datasets. O
To O
measure O

model O
on O
video O
datasets O
like O
Kinetics B-DAT
to O
fasten O
the O
process O
of O

large O
scale O
datasets, O
HVU O
and O
Kinetics B-DAT

Pre-Training O
Dataset O
UCF101 O
HMDB51 O
Kinetics B-DAT
From O
Scratch O
65.2 O
33.4 O
65.6 O

Kinetics B-DAT
89.8 O
62.1 O
- O
HVU O
91.1 O

performance O
comparison O
of O
HVU O
and O
Kinetics B-DAT
datasets O
for O
transfer O
learning O
generalization O

Dataset O
CNN O
Backbone O
UCF101 O
HMDB51 O
Kinetics B-DAT
Two O
Stream O
(spatial O
stream) O
[36 O

Kinetics B-DAT
Inception O
v3 O
93.2 O
- O
72.5 O

Kinetics B-DAT
Inception O
v1 O
95.6 O
74.8 O
72.1 O

RGB-I3D O
[5] O
Kinetics B-DAT
Inception O
v1 O
95.6 O
74.8 O
71.6 O

ResNet O
101 O
(16 O
frames) O
[19] O
Kinetics B-DAT
ResNet101 O
88.9 O
61.7 O
62.8 O
3D O

ResNext O
101 O
(16 O
frames) O
[19] O
Kinetics B-DAT
ResNext101 O
90.7 O
63.8 O
65.1 O
STC-ResNext O

101 O
(16 O
frames) O
[7] O
Kinetics B-DAT
ResNext101 O
92.3 O
65.4 O
66.2 O
STC-ResNext O

101 O
(64 O
frames) O
[7] O
Kinetics B-DAT
ResNext101 O
96.5 O
74.9 O
68.7 O
C3D O

45] O
Kinetics B-DAT
ResNet18 O
89.8 O
62.1 O
65.6 O
ARTNet O

45] O
Kinetics B-DAT
ResNet18 O
93.5 O
67.6 O
69.2 O
R(2+1)D O

42] O
Kinetics B-DAT
ResNet50 O
96.8 O
74.5 O
72 O
SlowFast O

11] O
Kinetics B-DAT
ResNet50 O
- O
- O
75.6 O
HATNet O

16 O
frames) O
Kinetics B-DAT
ResNet18 O
94.1 O
69.2 O
70.4 O
3D-ResNet18 O

UCF101, O
HMDB51 O
test O
sets O
and O
Kinetics B-DAT
validation O
set. O
The O
results O
on O

over O
three O
splits, O
and O
for O
Kinetics B-DAT
is O
Top-1 O
mAP O
on O
validation O

5.4. O
Transfer O
Learning: O
HVU O
vs O
Kinetics B-DAT

3D-ResNet18 O
using O
Kinetics B-DAT
versus O
using O
HVU O
and O
then O

fine-tuning O
on O
UCF101, O
HMDB51 O
and O
Kinetics B-DAT

on O
smaller O
datasets O
(i.e. O
HVU, O
Kinetics B-DAT

can O
improve O
the O
results O
on O
Kinetics B-DAT
also, O
although O
it O
is O
marginal O

5.5. O
Comparison O
on O
UCF, O
HMDB, O
Kinetics B-DAT

with O
HVU O
and O
another O
with O
Kinetics, B-DAT
and O
then O
fine-tune O
on O
the O

pre-trained O
models O
on O
ImageNet O
and O
Kinetics B-DAT

datasets. O
Note O
that O
for O
Kinetics B-DAT
dataset, O
HATNet O
even O
with O
ResNet18 O

Transfer O
Learning: O
HVU O
vs O
Kinetics B-DAT

Comparison O
on O
UCF, O
HMDB, O
Kinetics B-DAT

category O
taxonomy O
diversity O
of O
Youtube8M, O
Kinetics- B-DAT
600 O
and O
HACS, O
we O
use O

10 O
seconds. O
The O
dataset O
covers O
400 B-DAT
human-centric O
classes O
and O
each O
class O

has O
at O
least O
400 B-DAT
video O
clips. O
For O
unknown O
reasons O

train O
the O
flow O
branch O
on O
Kinetics B-DAT
data O
[14]. O
After O
that, O
considering O

Kinetics B-DAT
[14] O
is O
a O
large-scale O
trimmed O

at O
each O
iteration O
on O
the O
Kinetics B-DAT
dataset O

Comparisons O
with O
non-local O
networks O
on O
Kinetics B-DAT

P3D O
and O
3D-CMA O
models O
on O
Kinetics B-DAT
when O
varying O
the O
count O
of O

We O
train O
CMA O
iter1-R O
on O
Kinetics B-DAT
and O
visualize O
the O
attention O
maps O

These O
samples O
are O
taken O
from O
Kinetics B-DAT
randomly. O
Each O
set O
contains O
three O

Sports1M O
or O
ImageNet). O
Kinetics O
has O
400 B-DAT
action O
classes O
and O
about O
240K O

0 O
200 O
400 B-DAT
600 O
800 O
1000 O
GFLOPS O

pop- O
ular O
datasets, O
such O
as O
Kinetics, B-DAT
UCF-101 O
and O
HMDB-51 O

state-of-the-art O
accuracy O
of O
77.7% O
on O
Kinetics B-DAT
with O
FLOPs O
as O
high O
as O

Accuracy O
of O
clip-level O
backbones O
on O
Kinetics B-DAT

Datasets. O
We O
choose O
the O
Kinetics B-DAT
[20] O
dataset O
as O
the O
major O

testbed O
for O
FASTER. O
Kinetics B-DAT
is O
among O
the O
most O
popular O

simplify, O
all O
reported O
results O
on O
Kinetics B-DAT
are O
trained O
from O
scratch, O
without O

datasets O
(e.g., O
Sports1M O
or O
ImageNet). O
Kinetics B-DAT
has O
400 O
action O
classes O
and O

much O
smaller, O
thus O
we O
use O
Kinetics B-DAT
for O
pre- O
training O
and O
report O

As O
the O
average O
length O
of O
Kinetics B-DAT
videos O
is O
about O
10 O
seconds O

architectures O
for O
ag- O
gregation O
on O
Kinetics B-DAT

in O
differ- O
ent O
settings O
on O
Kinetics B-DAT

state O
of O
the O
art O
on O
Kinetics, B-DAT
UCF-101, O
and O
HMDB-51 O

the O
two O
clip-level O
backbones O
on O
Kinetics B-DAT

measure O
accuracy O
and O
GFLOPs O
on O
Kinetics B-DAT

up O
the O
runtime O
over O
100 O
Kinetics B-DAT
videos. O
The O
results O
are O
listed O

to O
the O
state-of-the-art O
methods O
on O
Kinetics B-DAT

K” O
denotes O
pre- O
training O
on O
Kinetics B-DAT

most O
popular O
video O
datasets, O
i.e., O
Kinetics, B-DAT
UCF-101 O
and O
HMDB-51. O
We O
only O

frames O
as O
inputs. O
Results O
on O
Kinetics B-DAT
are O
shown O
in O
Table O
7 O

GFLOPs O
vs. O
accuracy O
comparisons O
on O
Kinetics B-DAT

SomethingSomethingv1 O
[10]. O
Kinetics400 B-DAT
consists O
of O
400 O

72.7% O
accuracy O
on O
Kinetics B-DAT
compared O
to O
72.0% O
and O
65.6 O

tains O
72.7% O
accuracy O
on O
Kinetics B-DAT
compared O
to O
72.0 O

troduced O
the O
Kinetics B-DAT
dataset O
[18], O
which O
was O
large O

streams O
pretrained O
on O
Kinetics, B-DAT
I3D O
[2] O
achieved O
the O
state O

tion: O
Kinetics400 B-DAT
[18], O
HMDB51 O
[20], O
UCF101 O
[33 O

SomethingSomethingv1 O
[10]. O
Kinetics400 B-DAT
consists O
of O
400 O

Training O
MARS O
on O
Kinetics400 B-DAT

accuracies O
while O
training O
MARS O
on O
Kinetics400 B-DAT
from O
scratch O
for O
α O

curacy. O
For O
Kinetics400 B-DAT
and O
SomethingSomethingv1, O
we O

architecture O
due O
its O
performance O
on O
Kinetics400 B-DAT

train O
on O
Kinetics400 B-DAT
and O
MiniKinetics O
from O
scratch. O
For O

Stream O
MiniKinetics B-DAT
Kinetics400 O
UCF101-1 O
HMDB51-1 O
Something O

using O
16f-clips. O
For O
MiniKinetics B-DAT
and O
Kinetics400, O
all O
the O
streams O
are O
trained O

the O
streams O
are O
finetuned O
from O
Kinetics400 B-DAT
pretrained O
models O

Stream O
Kinetics400 B-DAT
UCF101-1 O
HMDB51-1 O
Something O

Top-1 O
accuracy O
using O
64f-clips. O
For O
Kinetics400, B-DAT
all O
the O
streams O
are O
trained O

are O
finetuned O
from O
Kinetics400 B-DAT
pretrained O
models. O
Optical O

in O
accuracy O
over O
RGB O
on O
Kinetics400, B-DAT
UCF101- O
1, O
HMDB51-1, O
and O
SomethingSomethingv1 O

ever, O
on O
Kinetics400, B-DAT
MARS O
performs O
worse O
than O
the O

Flow O
for O
6 O
classes O
of O
Kinetics400 B-DAT
dataset O

al. O
[22] O
also O
find O
that O
Kinetics400 B-DAT
is O
bi O

pretrained O
on O
Kinetics400 B-DAT
does O
not O
generalize O
well O
enough O

approaches O
in O
Table O
3 O
for O
Kinetics400 B-DAT
and O
in O
Ta O

Flow. O
For O
Kinetics400, B-DAT
when O
using O
only O
RGB O
frames O

and O
Flow O
as O
inputs. O
On O
Kinetics400 B-DAT

results O
for O
Kinetics400 B-DAT
validation O
set. O
(*Calculated O
on O
the O

held-out O
test O
set O
of O
Kinetics400 B-DAT

ResNext101 O
[11] O
RGB O
Kinetics B-DAT
94.5 O
70.1 O

R(2+1)D O
[38] O
RGB O
Kinetics B-DAT
96.8 O
74.5 O

MARS O
RGB O
Kinetics B-DAT
97.4 O
79.3 O
48.7 O

MARS+RGB O
RGB O
Kinetics B-DAT
97.6 O
79.5 O
51.7 O

R(2+1)D O
[38] O
RGB+Flow O
Kinetics B-DAT
97.3 O
78.7 O

Flow O
RGB+Flow O
Kinetics B-DAT
98.1 O
80.9 O
53.0 O

and O
scene-related O
datasets O
(i.e., O
Kinetics- O
400, B-DAT
UCF-101, O
and O
HMDB-51) O
with O
the O

400, B-DAT
UCF-101, O
and O
HMDB-51). O
The O
base O

2) O
scene-related O
datasets, O
including O
Kinetics- O
400 B-DAT
[2], O
UCF-101 O
[23] O
and O
HMDB-51 O

400 B-DAT
dataset O
com- O
pared O
with O
the O

400, B-DAT
UCF-101, O
and O
HMDB-51 O
in O
this O

400 B-DAT
is O
a O
large-scale O
human O
action O

video O
dataset O
with O
400 B-DAT
classes. O
It O
contains O
236,763 O
clips O

400 B-DAT
dataset. O
We O
train O
STM O
with O

Jester) O
and O
scene-related O
datasets O
(i.e., O
Kinetics-400, B-DAT
UCF-101, O
and O
HMDB-51). O
The O
base O

of O
the O
STM O
on O
the O
Kinetics-400 B-DAT
dataset O
com- O
pared O
with O
the O

STM O
on O
three O
scene-related O
datasets: O
Kinetics-400, B-DAT
UCF-101, O
and O
HMDB-51 O
in O
this O

section. O
Kinetics-400 B-DAT
is O
a O
large-scale O
human O
action O

com- O
peting O
methods O
on O
the O
Kinetics-400 B-DAT
dataset. O
We O
train O
STM O
with O

Jester) O
and O
scene-related O
datasets O
(i.e., O
Kinetics B-DAT

large-scale O
video O
datasets O
such O
as O
Kinetics B-DAT
[2]. O
With O
the O
help O
of O

public O
benchmark O
datasets O
including O
Something-Something[11], O
Kinetics B-DAT
[2], O
Jester O
[1], O
UCF101 O
[23 O

the O
help O
of O
high-quality O
large-scale O
Kinetics B-DAT
dataset O
and O
the O

Jester) O
and O
scene-related O
datasets O
(i.e., O
Kinetics B-DAT

lationship; O
(2) O
scene-related O
datasets, O
including O
Kinetics B-DAT

when O
T O
= O
16). O
For O
Kinetics, B-DAT
Something- O
Something O
v1 O
& O
v2 O

Kinetics B-DAT

I3D O
[2] O
3D O
ResNet-50 O
Kinetics B-DAT
32 O
41.6 O
72.2 O

TSN O
[33] O
ResNet-50 O
Kinetics B-DAT
8 O
19.7 O
46.6 O
- O
27.8 O

TSM O
[19] O
ResNet-50 O
Kinetics B-DAT
16 O
44.8 O
74.5 O
- O
58.7 O

UCF-101 O
and O
HMDB-51, O
we O
use O
Kinetics B-DAT
pre-trained O
model O
as O
initialization O
and O

of O
the O
STM O
on O
the O
Kinetics B-DAT

82.3 O
51.6 O
STC O
[4] O
ResNet101 O
Kinetics B-DAT
93.7 O
66.8 O

with O
TSN O
[32] O
3D O
ResNet-18 O
Kinetics B-DAT
94.3 O
70.9 O
ECO O
[42] O
BNInception+3D O

ResNet-18 O
Kinetics B-DAT
94.8 O
72.4 O

Kinetics B-DAT
95.1 O
74.3I3D O
two-stream O
[2] O
X O

Kinetics B-DAT
91.1 O
-TSN O
two-Stream O
[33] O
X O

Kinetics B-DAT
94.5 O
70.7 O
StNet O
[12] O
ResNet50 O

Kinetics B-DAT
93.5 O

Kinetics B-DAT
95.9 O
- O
STM O
ResNet-50 O
ImageNet+Kinetics O

STM O
on O
three O
scene-related O
datasets: O
Kinetics B-DAT

and O
HMDB-51 O
in O
this O
section. O
Kinetics B-DAT

com- O
peting O
methods O
on O
the O
Kinetics B-DAT

temporal-related O
datasets, O
most O
actions O
of O
Kinetics B-DAT
can O
be O
recognized O
by O
scene O

with O
the O
ImageNet O
pre-trained O
model, O
Kinetics B-DAT
pre-train O
can O
significantly O
improve O
the O

on O
UCF101, O
which O
also O
uses O
Kinetics B-DAT
as O
pre-train O
data O
but O
the O

Jester) O
and O
scene-related O
datasets O
(i.e., O
Kinetics- B-DAT
400, O
UCF-101, O
and O
HMDB-51) O
with O

Jester) O
and O
scene-related O
datasets O
(i.e., O
Kinetics-400, B-DAT
UCF-101, O
and O
HMDB-51). O
The O
base O

lationship; O
(2) O
scene-related O
datasets, O
including O
Kinetics- B-DAT
400 O
[2], O
UCF-101 O
[23] O
and O

of O
the O
STM O
on O
the O
Kinetics-400 B-DAT
dataset O
com- O
pared O
with O
the O

STM O
on O
three O
scene-related O
datasets: O
Kinetics-400, B-DAT
UCF-101, O
and O
HMDB-51 O
in O
this O

section. O
Kinetics-400 B-DAT
is O
a O
large-scale O
human O
action O

com- O
peting O
methods O
on O
the O
Kinetics-400 B-DAT
dataset. O
We O
train O
STM O
with O

Sports1M O
or O
ImageNet). O
Kinetics O
has O
400 B-DAT
action O
classes O
and O
about O
240K O

0 O
200 O
400 B-DAT
600 O
800 O
1000 O
GFLOPS O

pop- O
ular O
datasets, O
such O
as O
Kinetics, B-DAT
UCF-101 O
and O
HMDB-51 O

state-of-the-art O
accuracy O
of O
77.7% O
on O
Kinetics B-DAT
with O
FLOPs O
as O
high O
as O

Accuracy O
of O
clip-level O
backbones O
on O
Kinetics B-DAT

Datasets. O
We O
choose O
the O
Kinetics B-DAT
[20] O
dataset O
as O
the O
major O

testbed O
for O
FASTER. O
Kinetics B-DAT
is O
among O
the O
most O
popular O

simplify, O
all O
reported O
results O
on O
Kinetics B-DAT
are O
trained O
from O
scratch, O
without O

datasets O
(e.g., O
Sports1M O
or O
ImageNet). O
Kinetics B-DAT
has O
400 O
action O
classes O
and O

much O
smaller, O
thus O
we O
use O
Kinetics B-DAT
for O
pre- O
training O
and O
report O

As O
the O
average O
length O
of O
Kinetics B-DAT
videos O
is O
about O
10 O
seconds O

architectures O
for O
ag- O
gregation O
on O
Kinetics B-DAT

in O
differ- O
ent O
settings O
on O
Kinetics B-DAT

state O
of O
the O
art O
on O
Kinetics, B-DAT
UCF-101, O
and O
HMDB-51 O

the O
two O
clip-level O
backbones O
on O
Kinetics B-DAT

measure O
accuracy O
and O
GFLOPs O
on O
Kinetics B-DAT

up O
the O
runtime O
over O
100 O
Kinetics B-DAT
videos. O
The O
results O
are O
listed O

to O
the O
state-of-the-art O
methods O
on O
Kinetics B-DAT

K” O
denotes O
pre- O
training O
on O
Kinetics B-DAT

most O
popular O
video O
datasets, O
i.e., O
Kinetics, B-DAT
UCF-101 O
and O
HMDB-51. O
We O
only O

frames O
as O
inputs. O
Results O
on O
Kinetics B-DAT
are O
shown O
in O
Table O
7 O

GFLOPs O
vs. O
accuracy O
comparisons O
on O
Kinetics B-DAT

SomethingSomethingv1 O
[10]. O
Kinetics400 B-DAT
consists O
of O
400 O

72.7% O
accuracy O
on O
Kinetics B-DAT
compared O
to O
72.0% O
and O
65.6 O

tains O
72.7% O
accuracy O
on O
Kinetics B-DAT
compared O
to O
72.0 O

troduced O
the O
Kinetics B-DAT
dataset O
[18], O
which O
was O
large O

streams O
pretrained O
on O
Kinetics, B-DAT
I3D O
[2] O
achieved O
the O
state O

tion: O
Kinetics400 B-DAT
[18], O
HMDB51 O
[20], O
UCF101 O
[33 O

SomethingSomethingv1 O
[10]. O
Kinetics400 B-DAT
consists O
of O
400 O

Training O
MARS O
on O
Kinetics400 B-DAT

accuracies O
while O
training O
MARS O
on O
Kinetics400 B-DAT
from O
scratch O
for O
α O

curacy. O
For O
Kinetics400 B-DAT
and O
SomethingSomethingv1, O
we O

architecture O
due O
its O
performance O
on O
Kinetics400 B-DAT

train O
on O
Kinetics400 B-DAT
and O
MiniKinetics O
from O
scratch. O
For O

Stream O
MiniKinetics B-DAT
Kinetics400 O
UCF101-1 O
HMDB51-1 O
Something O

using O
16f-clips. O
For O
MiniKinetics B-DAT
and O
Kinetics400, O
all O
the O
streams O
are O
trained O

the O
streams O
are O
finetuned O
from O
Kinetics400 B-DAT
pretrained O
models O

Stream O
Kinetics400 B-DAT
UCF101-1 O
HMDB51-1 O
Something O

Top-1 O
accuracy O
using O
64f-clips. O
For O
Kinetics400, B-DAT
all O
the O
streams O
are O
trained O

are O
finetuned O
from O
Kinetics400 B-DAT
pretrained O
models. O
Optical O

in O
accuracy O
over O
RGB O
on O
Kinetics400, B-DAT
UCF101- O
1, O
HMDB51-1, O
and O
SomethingSomethingv1 O

ever, O
on O
Kinetics400, B-DAT
MARS O
performs O
worse O
than O
the O

Flow O
for O
6 O
classes O
of O
Kinetics400 B-DAT
dataset O

al. O
[22] O
also O
find O
that O
Kinetics400 B-DAT
is O
bi O

pretrained O
on O
Kinetics400 B-DAT
does O
not O
generalize O
well O
enough O

approaches O
in O
Table O
3 O
for O
Kinetics400 B-DAT
and O
in O
Ta O

Flow. O
For O
Kinetics400, B-DAT
when O
using O
only O
RGB O
frames O

and O
Flow O
as O
inputs. O
On O
Kinetics400 B-DAT

results O
for O
Kinetics400 B-DAT
validation O
set. O
(*Calculated O
on O
the O

held-out O
test O
set O
of O
Kinetics400 B-DAT

ResNext101 O
[11] O
RGB O
Kinetics B-DAT
94.5 O
70.1 O

R(2+1)D O
[38] O
RGB O
Kinetics B-DAT
96.8 O
74.5 O

MARS O
RGB O
Kinetics B-DAT
97.4 O
79.3 O
48.7 O

MARS+RGB O
RGB O
Kinetics B-DAT
97.6 O
79.5 O
51.7 O

R(2+1)D O
[38] O
RGB+Flow O
Kinetics B-DAT
97.3 O
78.7 O

Flow O
RGB+Flow O
Kinetics B-DAT
98.1 O
80.9 O
53.0 O

on O
the O
JHMDB, O
HMDB O
and O
UCF101 B-DAT

mance O
on O
JHMDB, O
HMDB, O
UCF101 B-DAT

state O
of O
the O
art O
on O
UCF101 B-DAT

The O
UCF101 B-DAT
dataset O
[34] O
consists O
of O
around O

HMDB, O
JHMDB O
and O
UCF101 B-DAT
have O
3 O
train/test O
splits O

method O
streams O
GT-JHMDB-1 O
JHMDB-1 O
HMDB-1 O
UCF101 B-DAT

JHMDB, O
HMDB O
and O
UCF101 B-DAT
datasets. O
We O
observe O
a O
clear O

on O
HMDB O
and O
+4% O
on O
UCF101 B-DAT

by O
more O
than O
10%. O
On O
UCF101, B-DAT
we O
also O
report O
state-of-the- O
art O

Method O
JHMDB O
HMDB O
UCF101 B-DAT

accuracy O
on O
JHMDB, O
HMDB O
and O
UCF101 B-DAT
averaged O
over O
the O
3 O

UCF101 B-DAT
datasets. O
Future O
work O
includes O
experimenting O

R. O
Zamir, O
and O
M. O
Shah. O
UCF101 B-DAT

on O
the O
JHMDB, O
HMDB O
and O
UCF101 B-DAT

mance O
on O
JHMDB, O
HMDB, O
UCF101 B-DAT

state O
of O
the O
art O
on O
UCF101 B-DAT

The O
UCF101 B-DAT
dataset O
[34] O
consists O
of O
around O

HMDB, O
JHMDB O
and O
UCF101 B-DAT
have O
3 O
train/test O
splits O

method O
streams O
GT-JHMDB-1 O
JHMDB-1 O
HMDB-1 O
UCF101 B-DAT

JHMDB, O
HMDB O
and O
UCF101 B-DAT
datasets. O
We O
observe O
a O
clear O

on O
HMDB O
and O
+4% O
on O
UCF101 B-DAT

by O
more O
than O
10%. O
On O
UCF101, B-DAT
we O
also O
report O
state-of-the- O
art O

Method O
JHMDB O
HMDB O
UCF101 B-DAT

accuracy O
on O
JHMDB, O
HMDB O
and O
UCF101 B-DAT
averaged O
over O
the O
3 O

UCF101 B-DAT
datasets. O
Future O
work O
includes O
experimenting O

R. O
Zamir, O
and O
M. O
Shah. O
UCF101 B-DAT

in O
current O
action O
classification O
datasets O
(UCF B-DAT

on O
HMDB-51 O
and O
98.0% O
on O
UCF B-DAT

previous O
datasets, O
HMDB-51 O
[18] O
and O
UCF B-DAT

fine-tuning O
each O
on O
HMDB-51 O
and O
UCF B-DAT

up O
to O
5k O
steps O
on O
UCF B-DAT

training O
and O
testing O
on O
either O
UCF B-DAT

split O
1 O
test O
sets O
of O
UCF B-DAT

UCF B-DAT

testing O
on O
split O
1 O
of O
UCF B-DAT

of O
pa- O
rameters O
and O
that O
UCF B-DAT

on O
Ki- O
netics O
than O
on O
UCF B-DAT

than O
that O
of O
RGB O
on O
UCF B-DAT

un- O
seen) O
videos O
of O
the O
UCF B-DAT

classifiers O
for O
the O
classes O
of O
UCF B-DAT

each O
net- O
work O
for O
the O
UCF B-DAT

-101/HMDB-51 O
classes O
(using O
the O
UCF B-DAT

and O
again O
evaluate O
on O
the O
UCF B-DAT

performance O
than O
directly O
training O
on O
UCF B-DAT

methods O
in O
table O
5, O
on O
UCF B-DAT

streams, O
and O
gets O
94.6% O
on O
UCF B-DAT

UCF B-DAT

Table O
4. O
Performance O
on O
the O
UCF B-DAT

pretrained O
weights. O
Original: O
train O
on O
UCF B-DAT

the O
last O
layer O
trained O
on O
UCF B-DAT

pre-training O
with O
end-to-end O
fine-tuning O
on O
UCF B-DAT

Model O
UCF B-DAT

Comparison O
with O
state-of-the-art O
on O
the O
UCF B-DAT

overall O
performance O
to O
98.0 O
on O
UCF B-DAT

dataset O
(Kinetics) O
to O
another O
dataset O
(UCF B-DAT

R. O
Zamir, O
and O
M. O
Shah. O
UCF101 B-DAT

R. O
Zamir, O
and O
M. O
Shah. O
UCF101 B-DAT

on O
the O
HMDB51 O
[20] O
and O
UCF101 B-DAT
[33] O
datasets. O
Meth O

tion: O
Kinetics400 O
[18], O
HMDB51 O
[20], O
UCF101 B-DAT
[33], O
and O

splits. O
UCF101 B-DAT
contains O
101 O
actions O
classes O
with O

first O
split O
for O
HMDB51 O
and O
UCF101 B-DAT
as O
HMDB51-1 O
and O

UCF101 B-DAT

UCF101, B-DAT
and O
HMDB51 O
[11]. O
Following O
the O

the O
smaller O
HMDB51 O
and O
UCF101 B-DAT
datasets O

Stream O
MiniKinetics O
Kinetics400 O
UCF101 B-DAT

are O
trained O
from O
scratch. O
For O
UCF101 B-DAT

Stream O
Kinetics400 O
UCF101 B-DAT

are O
trained O
from O
scratch. O
For O
UCF101 B-DAT

accuracy O
over O
RGB O
on O
Kinetics400, O
UCF101 B-DAT

Flow O
streams O
alone O
on O
MiniKinetics, O
UCF101, B-DAT
HMDB51 O

ble O
4 O
for O
UCF101, B-DAT
HMDB51 O
and O
SomethingSomethingv1 O

Method O
Streams O
Pretrain O
UCF101 B-DAT
HMDB51 O
Something O

sults O
of O
UCF101 B-DAT
and O
HMDB51 O
are O
averaged O
over O

ics400, O
UCF101, B-DAT
HMDB51, O
and O
SomethingSomethingv1 O

UCF101 B-DAT

on O
the O
HMDB51 O
[20] O
and O
UCF101 B-DAT
[33] O
datasets. O
Meth O

tion: O
Kinetics400 O
[18], O
HMDB51 O
[20], O
UCF101 B-DAT
[33], O
and O

splits. O
UCF101 B-DAT
contains O
101 O
actions O
classes O
with O

first O
split O
for O
HMDB51 O
and O
UCF101 B-DAT
as O
HMDB51-1 O
and O

UCF101 B-DAT

UCF101, B-DAT
and O
HMDB51 O
[11]. O
Following O
the O

the O
smaller O
HMDB51 O
and O
UCF101 B-DAT
datasets O

Stream O
MiniKinetics O
Kinetics400 O
UCF101 B-DAT

are O
trained O
from O
scratch. O
For O
UCF101 B-DAT

Stream O
Kinetics400 O
UCF101 B-DAT

are O
trained O
from O
scratch. O
For O
UCF101 B-DAT

accuracy O
over O
RGB O
on O
Kinetics400, O
UCF101 B-DAT

Flow O
streams O
alone O
on O
MiniKinetics, O
UCF101, B-DAT
HMDB51 O

ble O
4 O
for O
UCF101, B-DAT
HMDB51 O
and O
SomethingSomethingv1 O

Method O
Streams O
Pretrain O
UCF101 B-DAT
HMDB51 O
Something O

sults O
of O
UCF101 B-DAT
and O
HMDB51 O
are O
averaged O
over O

ics400, O
UCF101, B-DAT
HMDB51, O
and O
SomethingSomethingv1 O

UCF101 B-DAT

challenging O
human O
action O
datasets: O
HMDB51, O
UCF101, B-DAT
and O
Kinetics. O
The O
dataset O
and O

state-of-the-art O
results O
on O
the O
HMDB51, O
UCF101 B-DAT
and O
Kinetics O
datasets. O
In O
particular O

HATNet O
achieves O
remarkable O
performance O
on O
UCF101 B-DAT
(96.9%), O
HMDB51 O
(74.5%) O
and O
Kinetics O

available. O
The O
HMDB51 O
[24] O
and O
UCF101 B-DAT
[37] O
datasets O
are O
currently O
the O

7K O
’11 O
UCF101 B-DAT
[37] O
- O
- O
101 O

KTH O
[25], O
HMDB51 O
[24], O
and O
UCF101 B-DAT
[37] O
that O
inspired O
the O
design O

competitive O
results O
on O
Kinetics O
and O
UCF101 B-DAT
datasets. O
To O
measure O
the O
performance O

Pre-Training O
Dataset O
UCF101 B-DAT
HMDB51 O
Kinetics O
From O
Scratch O
65.2 O

Method O
Pre-Trained O
Dataset O
CNN O
Backbone O
UCF101 B-DAT
HMDB51 O
Kinetics O
Two O
Stream O
(spatial O

7. O
State-of-the-art O
performance O
comparison O
on O
UCF101, B-DAT
HMDB51 O
test O
sets O
and O
Kinetics O

validation O
set. O
The O
results O
on O
UCF101 B-DAT
and O
HMDB51 O
are O
average O
mAP O

HVU O
and O
then O
fine-tuning O
on O
UCF101, B-DAT
HMDB51 O
and O
Kinetics. O
Obvi- O
ously O

UCF101 B-DAT
and O
HMDB51). O
As O
it O
can O

5.5. O
Comparison O
on O
UCF, B-DAT
HMDB, O
Kinetics O

with O
the O
state-of-the-art O
methods O
on O
UCF101, B-DAT
HMDB51 O
and O
Ki- O
netics. O
For O

on O
the O
target O
datasets. O
For O
UCF101 B-DAT
and O
HMDB51, O
we O
report O
the O

5.5 O
. O
Comparison O
on O
UCF, B-DAT
HMDB, O
Kinetics O

challenging O
human O
action O
datasets: O
HMDB51, O
UCF101, B-DAT
and O
Kinetics. O
The O
dataset O
and O

state-of-the-art O
results O
on O
the O
HMDB51, O
UCF101 B-DAT
and O
Kinetics O
datasets. O
In O
particular O

HATNet O
achieves O
remarkable O
performance O
on O
UCF101 B-DAT
(96.9%), O
HMDB51 O
(74.5%) O
and O
Kinetics O

available. O
The O
HMDB51 O
[24] O
and O
UCF101 B-DAT
[37] O
datasets O
are O
currently O
the O

7K O
’11 O
UCF101 B-DAT
[37] O
- O
- O
101 O

KTH O
[25], O
HMDB51 O
[24], O
and O
UCF101 B-DAT
[37] O
that O
inspired O
the O
design O

competitive O
results O
on O
Kinetics O
and O
UCF101 B-DAT
datasets. O
To O
measure O
the O
performance O

Pre-Training O
Dataset O
UCF101 B-DAT
HMDB51 O
Kinetics O
From O
Scratch O
65.2 O

Method O
Pre-Trained O
Dataset O
CNN O
Backbone O
UCF101 B-DAT
HMDB51 O
Kinetics O
Two O
Stream O
(spatial O

7. O
State-of-the-art O
performance O
comparison O
on O
UCF101, B-DAT
HMDB51 O
test O
sets O
and O
Kinetics O

validation O
set. O
The O
results O
on O
UCF101 B-DAT
and O
HMDB51 O
are O
average O
mAP O

HVU O
and O
then O
fine-tuning O
on O
UCF101, B-DAT
HMDB51 O
and O
Kinetics. O
Obvi- O
ously O

UCF101 B-DAT
and O
HMDB51). O
As O
it O
can O

with O
the O
state-of-the-art O
methods O
on O
UCF101, B-DAT
HMDB51 O
and O
Ki- O
netics. O
For O

on O
the O
target O
datasets. O
For O
UCF101 B-DAT
and O
HMDB51, O
we O
report O
the O

three O
challenging O
action O
datasets: O
namely, O
UCF B-DAT

the O
overall O
per- O
formance. O
The O
UCF B-DAT

split O
1 O
test O
set O
of O
UCF B-DAT

results O
in O
Table O
1 O
on O
UCF B-DAT

we O
improve O
by O
9.7% O
on O
UCF B-DAT

overall O
performance O
to O
97.4% O
on O
UCF B-DAT

OF O
STATE-OF-THE-ART O
METHODS O
ON O
THE O
UCF B-DAT

Methods O
Pre-train O
dataset O
UCF B-DAT

increase O
by O
0.4%, O
on O
the O
UCF101 B-DAT
dataset, O
and O
7.8% O
on O
the O

WITH O
DIFFERENT O
COMPONENTS O
ON O
THE O
UCF B-DAT

Base O
Model O
Methods O
UCF B-DAT

We O
conduct O
an O
experiment O
on O
UCF B-DAT

and O
our O
CCS O
network O
on O
UCF B-DAT

DIFFERENT O
MODEL O
PARAMETERS O
VALUES O
ON O
UCF B-DAT

select O
three O
classes O
from O
the O
UCF B-DAT

increase O
by O
0.4%, O
on O
the O
UCF101 B-DAT
dataset, O
and O
7.8% O
on O
the O

studies O
on O
two O
popular O
datasets, O
UCF101 B-DAT
[30] O
and O
Kinet- O
ics O
[14 O

UCF B-DAT

experiments O
from O
Ki- O
netics O
to O
UCF B-DAT

Comparison O
with O
state-of-the-art O
on O
the O
UCF B-DAT

R. O
Zamir, O
and O
M. O
Shah. O
UCF101 B-DAT

studies O
on O
two O
popular O
datasets, O
UCF101 B-DAT
[30] O
and O
Kinet- O
ics O
[14 O

R. O
Zamir, O
and O
M. O
Shah. O
UCF101 B-DAT

scene-related O
datasets O
(i.e., O
Kinetics- O
400, O
UCF B-DAT

Something-Something[11], O
Kinetics O
[2], O
Jester O
[1], O
UCF101 B-DAT
[23] O
and O
HMDB-51 O
[17 O

and O
scene-related O
datasets O
(i.e., O
Kinetics-400, O
UCF B-DAT

datasets, O
including O
Kinetics- O
400 O
[2], O
UCF B-DAT

CMM O
are O
randomly O
initialized. O
For O
UCF B-DAT

Performance O
of O
the O
STM O
on O
UCF B-DAT

Method O
Backbone O
Flow O
Pre-train O
Data O
UCF B-DAT

on O
three O
scene-related O
datasets: O
Kinetics-400, O
UCF B-DAT

and O
19,095 O
clips O
for O
validation. O
UCF B-DAT

6766 O
labeled O
video O
clips. O
For O
UCF B-DAT

also O
conduct O
experiments O
on O
the O
UCF B-DAT

I3D O
with O
RGB O
stream O
on O
UCF101, B-DAT
which O
also O
uses O
Kinetics O
as O

Something-Something[11], O
Kinetics O
[2], O
Jester O
[1], O
UCF101 B-DAT
[23] O
and O
HMDB-51 O
[17 O

I3D O
with O
RGB O
stream O
on O
UCF101, B-DAT
which O
also O
uses O
Kinetics O
as O

on O
the O
HMDB51 O
[20] O
and O
UCF101 B-DAT
[33] O
datasets. O
Meth O

tion: O
Kinetics400 O
[18], O
HMDB51 O
[20], O
UCF101 B-DAT
[33], O
and O

splits. O
UCF101 B-DAT
contains O
101 O
actions O
classes O
with O

first O
split O
for O
HMDB51 O
and O
UCF101 B-DAT
as O
HMDB51-1 O
and O

UCF101 B-DAT

UCF101, B-DAT
and O
HMDB51 O
[11]. O
Following O
the O

the O
smaller O
HMDB51 O
and O
UCF101 B-DAT
datasets O

Stream O
MiniKinetics O
Kinetics400 O
UCF101 B-DAT

are O
trained O
from O
scratch. O
For O
UCF101 B-DAT

Stream O
Kinetics400 O
UCF101 B-DAT

are O
trained O
from O
scratch. O
For O
UCF101 B-DAT

accuracy O
over O
RGB O
on O
Kinetics400, O
UCF101 B-DAT

Flow O
streams O
alone O
on O
MiniKinetics, O
UCF101, B-DAT
HMDB51 O

ble O
4 O
for O
UCF101, B-DAT
HMDB51 O
and O
SomethingSomethingv1 O

Method O
Streams O
Pretrain O
UCF101 B-DAT
HMDB51 O
Something O

sults O
of O
UCF101 B-DAT
and O
HMDB51 O
are O
averaged O
over O

ics400, O
UCF101, B-DAT
HMDB51, O
and O
SomethingSomethingv1 O

UCF101 B-DAT

on O
the O
HMDB51 O
[20] O
and O
UCF101 B-DAT
[33] O
datasets. O
Meth O

tion: O
Kinetics400 O
[18], O
HMDB51 O
[20], O
UCF101 B-DAT
[33], O
and O

splits. O
UCF101 B-DAT
contains O
101 O
actions O
classes O
with O

first O
split O
for O
HMDB51 O
and O
UCF101 B-DAT
as O
HMDB51-1 O
and O

UCF101 B-DAT

UCF101, B-DAT
and O
HMDB51 O
[11]. O
Following O
the O

the O
smaller O
HMDB51 O
and O
UCF101 B-DAT
datasets O

Stream O
MiniKinetics O
Kinetics400 O
UCF101 B-DAT

are O
trained O
from O
scratch. O
For O
UCF101 B-DAT

Stream O
Kinetics400 O
UCF101 B-DAT

are O
trained O
from O
scratch. O
For O
UCF101 B-DAT

accuracy O
over O
RGB O
on O
Kinetics400, O
UCF101 B-DAT

Flow O
streams O
alone O
on O
MiniKinetics, O
UCF101, B-DAT
HMDB51 O

ble O
4 O
for O
UCF101, B-DAT
HMDB51 O
and O
SomethingSomethingv1 O

Method O
Streams O
Pretrain O
UCF101 B-DAT
HMDB51 O
Something O

sults O
of O
UCF101 B-DAT
and O
HMDB51 O
are O
averaged O
over O

ics400, O
UCF101, B-DAT
HMDB51, O
and O
SomethingSomethingv1 O

UCF101 B-DAT

than O
1% O
across O
major O
datasets O
(UCF101 B-DAT
[41], O
HMDB51 O
[22] O
and O
20BN-Something-Something O

choices O
with O
RGB/flow O
stream O
on O
UCF101 B-DAT

recognition O
datasets O
for O
this O
experiment: O
UCF101 B-DAT
and O
HMDB51. O
UCF101 O
[41] O
contains O

RGB O
and O
flow O
stream O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets. O
We O
also O

the O
RGB O
stream. O
For O
both O
UCF101 B-DAT
and O
HMDB51, O
the O
best O
performing O

results O
of O
action O
recognition O
on O
UCF101 B-DAT
[41], O
HMDB51 O
[22] O
and O
a O

large O
scale O
dataset–20BN-V2 O
[28]. O
For O
UCF101 B-DAT
and O
HMDB51, O
we O
report O
mean O

results O
with O
previous O
methods O
on O
UCF101 B-DAT

class O
accuracy O
of O
95.7%/72.0% O
on O
UCF101 B-DAT

Results O
of O
action O
recognition O
on O
UCF101 B-DAT
and O
HMDB51. O
We O
compare O
the O

model O
is O
-1.1% O
worse O
on O
UCF101 B-DAT
and O
-4.1% O
on O
HMDB51. O
This O

of O
results O
remains O
consistent O
as O
UCF101 B-DAT

is O
even O
larger O
(+1.4% O
on O
UCF101 B-DAT
and O
+1.3% O
on O
HMDB51). O
This O

localization O
dataset O
[18]–a O
subset O
of O
UCF101 B-DAT
with O
bounding O
box O
annotations O
for O

UCF101 B-DAT
I3D O
RGB O
94.8 O
94.7 O
0.1 O

for O
all O
testing O
videos O
of O
UCF101 B-DAT
and O
HMDB51. O
We O
compare O
their O

an O
accuracy O
of O
95.1%/71.6% O
on O
UCF101 B-DAT

model O
by O
- O
0.6%/-0.4% O
on O
UCF101 B-DAT

Roshan O
Zamir, O
and O
M. O
Shah. O
UCF101 B-DAT

than O
1% O
across O
major O
datasets O
(UCF101 B-DAT
[41], O
HMDB51 O
[22] O
and O
20BN-Something-Something O

choices O
with O
RGB/flow O
stream O
on O
UCF101 B-DAT

recognition O
datasets O
for O
this O
experiment: O
UCF101 B-DAT
and O
HMDB51. O
UCF101 O
[41] O
contains O

RGB O
and O
flow O
stream O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets. O
We O
also O

the O
RGB O
stream. O
For O
both O
UCF101 B-DAT
and O
HMDB51, O
the O
best O
performing O

results O
of O
action O
recognition O
on O
UCF101 B-DAT
[41], O
HMDB51 O
[22] O
and O
a O

large O
scale O
dataset–20BN-V2 O
[28]. O
For O
UCF101 B-DAT
and O
HMDB51, O
we O
report O
mean O

results O
with O
previous O
methods O
on O
UCF101 B-DAT

class O
accuracy O
of O
95.7%/72.0% O
on O
UCF101 B-DAT

Results O
of O
action O
recognition O
on O
UCF101 B-DAT
and O
HMDB51. O
We O
compare O
the O

model O
is O
-1.1% O
worse O
on O
UCF101 B-DAT
and O
-4.1% O
on O
HMDB51. O
This O

of O
results O
remains O
consistent O
as O
UCF101 B-DAT

is O
even O
larger O
(+1.4% O
on O
UCF101 B-DAT
and O
+1.3% O
on O
HMDB51). O
This O

localization O
dataset O
[18]–a O
subset O
of O
UCF101 B-DAT
with O
bounding O
box O
annotations O
for O

UCF101 B-DAT
I3D O
RGB O
94.8 O
94.7 O
0.1 O

for O
all O
testing O
videos O
of O
UCF101 B-DAT
and O
HMDB51. O
We O
compare O
their O

an O
accuracy O
of O
95.1%/71.6% O
on O
UCF101 B-DAT

model O
by O
- O
0.6%/-0.4% O
on O
UCF101 B-DAT

Roshan O
Zamir, O
and O
M. O
Shah. O
UCF101 B-DAT

finally O
achieve O
leading O
performance O
on O
UCF B-DAT

accuracy O
in O
UCF B-DAT

leading O
performance O
in O
UCF B-DAT

our O
paper O
is O
evaluated O
on O
UCF B-DAT

our O
experiments, O
9537 O
videos O
of O
UCF B-DAT

and O
achieve O
94.3% O
accuracy O
on O
UCF B-DAT

temporal O
features. O
We O
also O
utilize O
UCF B-DAT

evaluated O
on O
UCF B-DAT

UCF101 B-DAT

can O
achieve O
leading O
performance O
on O
UCF B-DAT

the O
UCF B-DAT

R O
, O
Shah O
M O
. O
UCF101 B-DAT

UCF101 B-DAT

R O
, O
Shah O
M O
. O
UCF101 B-DAT

resulted O
in O
significant O
overfitting O
for O
UCF B-DAT

achieved O
94.5% O
and O
70.2% O
on O
UCF B-DAT

Representative O
video O
datasets, O
such O
as O
UCF B-DAT

and O
other O
popular O
video O
datasets O
(UCF B-DAT

The O
HMDB-51 O
[17] O
and O
UCF B-DAT

datasets, O
as O
well O
as O
the O
UCF B-DAT

3D O
CNNs O
trained O
on O
the O
UCF101 B-DAT
and O
HMDB51 O
datasets O
were O
inferior O

experiments O
described O
below O
us- O
ing O
UCF B-DAT

16], O
3D O
CNNs O
trained O
on O
UCF B-DAT

Kinetics O
pretrained O
3D O
CNNs O
on O
UCF B-DAT

perform O
well O
on O
relatively O
small O
UCF B-DAT

study O
focuses O
on O
four O
datasets: O
UCF B-DAT

UCF B-DAT

human O
action O
classes. O
Similar O
to O
UCF B-DAT

a) O
UCF B-DAT

losses. O
The O
validation O
losses O
on O
UCF B-DAT

16], O
3D O
CNNs O
trained O
on O
UCF B-DAT

we O
used O
split O
1 O
of O
UCF B-DAT

figure, O
the O
validation O
losses O
on O
UCF B-DAT

video. O
The O
validation O
accuracies O
of O
UCF B-DAT

3D O
CNNs O
from O
scratch O
on O
UCF B-DAT

Kinetics O
pretrained O
3D O
CNNs O
on O
UCF B-DAT

ResNet-18 O
trained O
from O
scratch, O
in O
UCF B-DAT

Table O
4: O
Top-1 O
accuracies O
on O
UCF B-DAT

Method O
UCF B-DAT

on O
Kinetics O
is O
effective O
on O
UCF B-DAT

Table O
5: O
Top-1 O
accuracies O
on O
UCF B-DAT

Method O
Dim O
UCF B-DAT

resulted O
in O
significant O
overfitting O
for O
UCF B-DAT

outperforms O
complex O
2D O
architectures O
on O
UCF B-DAT

achieved O
94.5% O
and O
70.2% O
on O
UCF B-DAT

Roshan O
Zamir, O
and O
M. O
Shah. O
UCF101 B-DAT

3D O
CNNs O
trained O
on O
the O
UCF101 B-DAT
and O
HMDB51 O
datasets O
were O
inferior O

Roshan O
Zamir, O
and O
M. O
Shah. O
UCF101 B-DAT

large O
number O
of O
experiments O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets. O
Our O
experiments O

performance O
on O
the O
public O
datasets O
UCF101 B-DAT
[24] O
and O
HMDB51 O
[25 O

applied O
on O
video O
‘‘v_Diving_g11_c02.avi’’ O
in O
UCF101 B-DAT
dataset. O
Firstly, O
the O
video O
is O

experiments O
on O
two O
standard O
datasets O
UCF101 B-DAT
and O
HMDB51. O
UCF101 O
dataset O
established O

we O
perform O
several O
tests O
on O
UCF101 B-DAT
andHMDB51 O
datasets O
(over O
all O
splits O

Related O
experiments O
are O
performed O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

with O
different O
sample O
segments O
on O
UCF101 B-DAT
and O
HMDB51 O
dataset O
(over O
all O

different O
attention O
module O
settings O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

testing. O
We O
exper- O
iment O
with O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

different O
numbers O
of O
layers O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

The O
results O
are O
evaluated O
on O
UCF101 B-DAT
dataset O
(over O
all O
splits). O
We O

is O
performed O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

We O
perform O
the O
experiments O
on O
UCF101 B-DAT
dataset O
(over O
all O
splits). O
Segment O

the O
state-of-the-art O
methods O
on O
the O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

We O
perform O
the O
experiments O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets O
with O
R-STAN-101 O

UCF101 B-DAT

large O
number O
of O
experiments O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets. O
Our O
experiments O

performance O
on O
the O
public O
datasets O
UCF101 B-DAT
[24] O
and O
HMDB51 O
[25 O

applied O
on O
video O
‘‘v_Diving_g11_c02.avi’’ O
in O
UCF101 B-DAT
dataset. O
Firstly, O
the O
video O
is O

experiments O
on O
two O
standard O
datasets O
UCF101 B-DAT
and O
HMDB51. O
UCF101 O
dataset O
established O

we O
perform O
several O
tests O
on O
UCF101 B-DAT
andHMDB51 O
datasets O
(over O
all O
splits O

Related O
experiments O
are O
performed O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

with O
different O
sample O
segments O
on O
UCF101 B-DAT
and O
HMDB51 O
dataset O
(over O
all O

different O
attention O
module O
settings O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

testing. O
We O
exper- O
iment O
with O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

different O
numbers O
of O
layers O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

The O
results O
are O
evaluated O
on O
UCF101 B-DAT
dataset O
(over O
all O
splits). O
We O

is O
performed O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

We O
perform O
the O
experiments O
on O
UCF101 B-DAT
dataset O
(over O
all O
splits). O
Segment O

the O
state-of-the-art O
methods O
on O
the O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

We O
perform O
the O
experiments O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets O
with O
R-STAN-101 O

UCF101 B-DAT

two O
large O
benchmarks, O
HMDB51 O
and O
UCF101, B-DAT
demonstrate O
the O
effectiveness O
of O
the O

datasets, O
namely O
HMDB51 O
[38] O
and O
UCF101 B-DAT
[39]. O
The O
HMDB51 O
dataset O
is O

30 O
clips O
for O
testing. O
The O
UCF101 B-DAT
dataset O
consists O
of O
13320 O
video O

on O
the O
HMDB51 O
and O
the O
UCF101 B-DAT
dataset. O
For O
both O
datasets, O
we O

dataset O
and O
94.3%, O
on O
the O
UCF101 B-DAT
dataset. O
In O
comparison O
with O
the O

Method O
HMDB51 O
UCF101 B-DAT
Slow O
Fusion O
CNN O
[41 O

ment O
on O
the O
UCF101 B-DAT
dataset O
is O
smaller. O
The O
performance O

of O
the O
UCF101 B-DAT
dataset O
is O
approaching O
saturation O
(>94 O

94.3% O
on O
the O
HMDB51 O
and O
UCF101 B-DAT
dataset, O
respectively O

two O
large O
benchmarks, O
HMDB51 O
and O
UCF101, B-DAT
demonstrate O
the O
effectiveness O
of O
the O

datasets, O
namely O
HMDB51 O
[38] O
and O
UCF101 B-DAT
[39]. O
The O
HMDB51 O
dataset O
is O

30 O
clips O
for O
testing. O
The O
UCF101 B-DAT
dataset O
consists O
of O
13320 O
video O

on O
the O
HMDB51 O
and O
the O
UCF101 B-DAT
dataset. O
For O
both O
datasets, O
we O

dataset O
and O
94.3%, O
on O
the O
UCF101 B-DAT
dataset. O
In O
comparison O
with O
the O

Method O
HMDB51 O
UCF101 B-DAT
Slow O
Fusion O
CNN O
[41 O

ment O
on O
the O
UCF101 B-DAT
dataset O
is O
smaller. O
The O
performance O

of O
the O
UCF101 B-DAT
dataset O
is O
approaching O
saturation O
(>94 O

94.3% O
on O
the O
HMDB51 O
and O
UCF101 B-DAT
dataset, O
respectively O

in O
current O
action O
classification O
datasets O
(UCF B-DAT

on O
HMDB-51 O
and O
98.0% O
on O
UCF B-DAT

previous O
datasets, O
HMDB-51 O
[18] O
and O
UCF B-DAT

fine-tuning O
each O
on O
HMDB-51 O
and O
UCF B-DAT

up O
to O
5k O
steps O
on O
UCF B-DAT

training O
and O
testing O
on O
either O
UCF B-DAT

split O
1 O
test O
sets O
of O
UCF B-DAT

UCF B-DAT

testing O
on O
split O
1 O
of O
UCF B-DAT

of O
pa- O
rameters O
and O
that O
UCF B-DAT

on O
Ki- O
netics O
than O
on O
UCF B-DAT

than O
that O
of O
RGB O
on O
UCF B-DAT

un- O
seen) O
videos O
of O
the O
UCF B-DAT

classifiers O
for O
the O
classes O
of O
UCF B-DAT

each O
net- O
work O
for O
the O
UCF B-DAT

-101/HMDB-51 O
classes O
(using O
the O
UCF B-DAT

and O
again O
evaluate O
on O
the O
UCF B-DAT

performance O
than O
directly O
training O
on O
UCF B-DAT

methods O
in O
table O
5, O
on O
UCF B-DAT

streams, O
and O
gets O
94.6% O
on O
UCF B-DAT

UCF B-DAT

Table O
4. O
Performance O
on O
the O
UCF B-DAT

pretrained O
weights. O
Original: O
train O
on O
UCF B-DAT

the O
last O
layer O
trained O
on O
UCF B-DAT

pre-training O
with O
end-to-end O
fine-tuning O
on O
UCF B-DAT

Model O
UCF B-DAT

Comparison O
with O
state-of-the-art O
on O
the O
UCF B-DAT

overall O
performance O
to O
98.0 O
on O
UCF B-DAT

dataset O
(Kinetics) O
to O
another O
dataset O
(UCF B-DAT

R. O
Zamir, O
and O
M. O
Shah. O
UCF101 B-DAT

R. O
Zamir, O
and O
M. O
Shah. O
UCF101 B-DAT

large O
number O
of O
experiments O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets. O
Our O
experiments O

performance O
on O
the O
public O
datasets O
UCF101 B-DAT
[24] O
and O
HMDB51 O
[25 O

applied O
on O
video O
‘‘v_Diving_g11_c02.avi’’ O
in O
UCF101 B-DAT
dataset. O
Firstly, O
the O
video O
is O

experiments O
on O
two O
standard O
datasets O
UCF101 B-DAT
and O
HMDB51. O
UCF101 O
dataset O
established O

we O
perform O
several O
tests O
on O
UCF101 B-DAT
andHMDB51 O
datasets O
(over O
all O
splits O

Related O
experiments O
are O
performed O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

with O
different O
sample O
segments O
on O
UCF101 B-DAT
and O
HMDB51 O
dataset O
(over O
all O

different O
attention O
module O
settings O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

testing. O
We O
exper- O
iment O
with O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

different O
numbers O
of O
layers O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

The O
results O
are O
evaluated O
on O
UCF101 B-DAT
dataset O
(over O
all O
splits). O
We O

is O
performed O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

We O
perform O
the O
experiments O
on O
UCF101 B-DAT
dataset O
(over O
all O
splits). O
Segment O

the O
state-of-the-art O
methods O
on O
the O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

We O
perform O
the O
experiments O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets O
with O
R-STAN-101 O

UCF101 B-DAT

large O
number O
of O
experiments O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets. O
Our O
experiments O

performance O
on O
the O
public O
datasets O
UCF101 B-DAT
[24] O
and O
HMDB51 O
[25 O

applied O
on O
video O
‘‘v_Diving_g11_c02.avi’’ O
in O
UCF101 B-DAT
dataset. O
Firstly, O
the O
video O
is O

experiments O
on O
two O
standard O
datasets O
UCF101 B-DAT
and O
HMDB51. O
UCF101 O
dataset O
established O

we O
perform O
several O
tests O
on O
UCF101 B-DAT
andHMDB51 O
datasets O
(over O
all O
splits O

Related O
experiments O
are O
performed O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

with O
different O
sample O
segments O
on O
UCF101 B-DAT
and O
HMDB51 O
dataset O
(over O
all O

different O
attention O
module O
settings O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

testing. O
We O
exper- O
iment O
with O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

different O
numbers O
of O
layers O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

The O
results O
are O
evaluated O
on O
UCF101 B-DAT
dataset O
(over O
all O
splits). O
We O

is O
performed O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

We O
perform O
the O
experiments O
on O
UCF101 B-DAT
dataset O
(over O
all O
splits). O
Segment O

the O
state-of-the-art O
methods O
on O
the O
UCF101 B-DAT
and O
HMDB51 O
datasets O
(over O
all O

We O
perform O
the O
experiments O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets O
with O
R-STAN-101 O

UCF101 B-DAT

method. O
Experimental O
results O
on O
the O
UCF B-DAT

-Sports, O
J-HMDB O
and O
UCF101 B-DAT
action O
detection O
datasets O
show O
that O

detection O
with O
state-of-the-art O
results O
on O
UCF B-DAT

-Sports, O
J-HMDB O
and O
UCF101 B-DAT
datasets. O
Our O
contributions O
are O
as O

Comparision O
of O
frame-level O
proposals O
on O
UCF B-DAT

k O
frame O
optical O
flows. O
Left: O
UCF B-DAT

using O
RGB O
frames. O
Interestingly, O
on O
UCF B-DAT

are O
better O
than O
RPN-a O
on O
UCF B-DAT

significant O
mo- O
tion O
occurring O
on O
UCF B-DAT

action O
detection O
on O
three O
datasets: O
UCF B-DAT

-Sports, O
J-HMDB O
and O
UCF B-DAT

the O
metrics O
used O
for O
evaluation. O
UCF B-DAT

results O
over O
the O
three O
splits. O
UCF B-DAT

split O
only. O
In O
contrast O
to O
UCF B-DAT

trun- O
cated O
to O
the O
action, O
UCF B-DAT

and O
video-AP O
measurement O
on O
the O
UCF B-DAT

0.05, O
0.1, O
0.2, O
0.3] O
on O
UCF101 B-DAT

NMS O
or O
score O
combination–ours). O
Left: O
UCF B-DAT

training O
the O
two-stream O
R-CNN O
on O
UCF B-DAT

the O
mentioned O
iterations O
on O
the O
UCF101 B-DAT
dataset O
empirically O
since O
it O
is O

averaging. O
We O
report O
results O
for O
UCF B-DAT

Test O
scales O
Train O
scales O
UCF B-DAT

-Sports O
J-HMDB O
UCF-Sports B-DAT
J-HMDB O
UCF O

one O
frame O
(Flow-1) O
on O
both O
UCF B-DAT

i.e. O
we O
gain O
10.27% O
on O
UCF B-DAT

decreases O
the O
result O
significantly O
on O
UCF B-DAT

and O
performs O
on O
par O
on O
UCF B-DAT

multi-region O
two-stream O
faster O
R-CNN O
on O
UCF B-DAT

the O
RGB-1 O
R-CNN O
model O
on O
UCF B-DAT

obtains O
82.3% O
and O
56.6% O
on O
UCF B-DAT

models O
in O
Table O
2 O
for O
UCF B-DAT

essential O
for O
an O
action. O
On O
UCF B-DAT

actions O
(everyday O
actions), O
while O
for O
UCF B-DAT

by O
2.21% O
and O
1.6% O
on O
UCF B-DAT

14.1% O
for O
“Golf” O
on O
UCF B-DAT

5.16% O
for O
“Swing O
1” O
on O
UCF B-DAT

IoU O
threshold O
of O
δ O
on O
UCF B-DAT

- O
Sports, O
J-HMDB, O
and O
UCF101 B-DAT
datasets. O
Both O
of O
our O
approaches O

obtain O
94.82% O
and O
70.88% O
on O
UCF B-DAT

but O
are O
the O
similar O
for O
UCF B-DAT

Table O
4: O
Video O
mAP O
on O
UCF B-DAT

-Sports, O
J-HMDB O
and O
UCF101 B-DAT
(split O
1) O
with O
variant O
IoU O

UCF-Sports B-DAT
J-HMDB O
UCF101 O
(with O
temporal O
loc) O
UCF101 O
(w/o O

Table O
5: O
Classification O
results O
on O
UCF B-DAT

UCF B-DAT

R-CNN O
model O
on O
J-HMDB O
and O
UCF101, B-DAT
and O
performs O
similarly O
on O
UCF-Sports O

The O
lack O
in O
improvement O
on O
UCF B-DAT

performance O
actually O
does O
improve O
for O
UCF B-DAT

perform O
temporal O
lo- O
calization O
on O
UCF101 B-DAT

the O
average O
class O
accuracy O
on O
UCF B-DAT

obtains O
95.74% O
and O
71.08% O
on O
UCF B-DAT

the O
state-of-the-art O
results O
on O
both O
UCF B-DAT

on O
par O
with O
[9] O
on O
UCF101 B-DAT

4.3%, O
12.4% O
and O
0.2% O
on O
UCF B-DAT

-Sports, O
J-HMDB O
and O
UCF101, B-DAT
respectively. O
Both O
[8] O
and O
[9 O

UCF-Sports B-DAT
J-HMDB O
UCF101 O
(split O
1 O

for O
difficult O
datasets O
such O
as O
UCF101 B-DAT

9]. O
In O
our O
experiments O
on O
UCF101, B-DAT
we O
observed O
that O
a O
limitation O

K., O
Zamir, O
A.R., O
Shah, O
M.: O
UCF101 B-DAT

on O
the O
UCF-Sports, O
J-HMDB O
and O
UCF101 B-DAT
action O
detection O
datasets O
show O
that O

results O
on O
UCF-Sports, O
J-HMDB O
and O
UCF101 B-DAT
datasets. O
Our O
contributions O
are O
as O

0.05, O
0.1, O
0.2, O
0.3] O
on O
UCF101 B-DAT

the O
mentioned O
iterations O
on O
the O
UCF101 B-DAT
dataset O
empirically O
since O
it O
is O

on O
UCF- O
Sports, O
J-HMDB, O
and O
UCF101 B-DAT
datasets. O
Both O
of O
our O
approaches O

mAP O
on O
UCF-Sports, O
J-HMDB O
and O
UCF101 B-DAT
(split O
1) O
with O
variant O
IoU O

UCF-Sports O
J-HMDB O
UCF101 B-DAT
(with O
temporal O
loc) O
UCF101 O
(w/o O

R-CNN O
model O
on O
J-HMDB O
and O
UCF101, B-DAT
and O
performs O
similarly O
on O
UCF-Sports O

perform O
temporal O
lo- O
calization O
on O
UCF101 B-DAT

on O
par O
with O
[9] O
on O
UCF101 B-DAT

0.2% O
on O
UCF-Sports, O
J-HMDB O
and O
UCF101, B-DAT
respectively. O
Both O
[8] O
and O
[9 O

UCF-Sports O
J-HMDB O
UCF101 B-DAT
(split O
1 O

for O
difficult O
datasets O
such O
as O
UCF101 B-DAT

9]. O
In O
our O
experiments O
on O
UCF101, B-DAT
we O
observed O
that O
a O
limitation O

K., O
Zamir, O
A.R., O
Shah, O
M.: O
UCF101 B-DAT

state O
of O
the O
art O
on O
UCF B-DAT

train O
and O
1530 O
test O
videos. O
UCF B-DAT

all O
splits O
for O
HMDB O
and O
UCF B-DAT
in O
Section O
4.4 O

b) O
UCF B-DAT

previous O
work O
on O
HMDB O
and O
UCF B-DAT

3-split O
avg O
on O
HMDB O
and O
UCF, B-DAT
but O
is O
not O
comparable O
to O

initialization O
methods O
on O
HMDB O
and O
UCF B-DAT

Roshan O
Zamir, O
and O
Mubarak O
Shah. O
UCF101 B-DAT

Roshan O
Zamir, O
and O
Mubarak O
Shah. O
UCF101 B-DAT

capture O
motion O
patterns, O
and O
(3) O
UCF B-DAT

3) O
UCF B-DAT

converges. O
For O
the O
training O
of O
UCF B-DAT

benchmarks O
and O
immune O
to O
over-fitting, O
UCF B-DAT

is O
increased O
to O
0.9 O
for O
UCF B-DAT

in O
[9]. O
For O
Kinetics-600 O
and O
UCF B-DAT

Recognition: O
For O
Kinetics-600 O
and O
UCF B-DAT

XP O
Jetson O
TX2 O
Kinetics-600 O
Jester O
UCF B-DAT

UCF101 B-DAT
dataset O
is O
an O
action O
recognition O

to O
Kinetics-600 O
and O
Jester O
datasets, O
UCF B-DAT

over-fitting. O
For O
the O
evaluation O
of O
UCF B-DAT

used O
only O
split-1. O
We O
selected O
UCF B-DAT

inferior O
results O
in O
Kinetics-600 O
and O
UCF B-DAT

mance O
on O
both O
Kinetics-600 O
and O
UCF B-DAT

UCF101 B-DAT
dataset O
is O
an O
action O
recognition O

the O
top O
layers O
on O
the O
UCF B-DAT

formance O
improvements O
compared O
to O
the O
UCF B-DAT

65.4%, O
up O
from O
41.3%) O
on O
UCF B-DAT

training O
the O
entire O
network O
on O
UCF B-DAT

since O
only O
some O
classes O
in O
UCF B-DAT

apply O
our O
networks O
to O
the O
UCF B-DAT

established O
by O
training O
networks O
on O
UCF B-DAT

commonly O
used O
datasets O
(KTH, O
Weizmann, O
UCF B-DAT
Sports, O
IXMAS, O
Hollywood O
2, O
UCF-50 O

and O
the O
recently O
in- O
troduced O
UCF B-DAT

transfer O
learning O
experi- O
ments O
on O
UCF B-DAT

Table O
3: O
Results O
on O
UCF B-DAT

4.2. O
Transfer O
Learning O
Experiments O
on O
UCF B-DAT

learn- O
ing O
experiments O
on O
the O
UCF B-DAT

net- O
work O
from O
scratch O
on O
UCF B-DAT

Results. O
To O
prepare O
UCF B-DAT

the O
YouTube O
video O
IDs O
of O
UCF B-DAT

Slow O
Fusion O
net- O
work O
on O
UCF B-DAT

dataset O
has O
no O
overlap O
with O
UCF B-DAT

Slow O
Fusion O
network O
in O
our O
UCF B-DAT

of O
classes O
present O
in O
the O
UCF B-DAT

to O
the O
Sports O
categories O
in O
UCF B-DAT

Our O
transfer O
learning O
experiments O
on O
UCF B-DAT

R. O
Zamir, O
and O
M. O
Shah. O
UCF101 B-DAT

R. O
Zamir, O
and O
M. O
Shah. O
UCF101 B-DAT

e.g., O
45.4% O
v.s. O
61.2% O
on O
UCF101 B-DAT

based O
on O
two O
popular O
dataset O
UCF101 B-DAT
[36] O
and O
HMDB51 O
[23]. O
Our O

we O
incorporate O
five O
datasets: O
the O
UCF101 B-DAT
[36], O
the O
Kinetics O
[19], O
the O

Unless O
specifically O
state, O
we O
use O
UCF101 B-DAT
dataset O
for O
our O
model O
pre-training O

UCF101 B-DAT
dataset O
[36] O
consists O
of O
13,320 O

When O
pre-training O
on O
UCF101 B-DAT
train O
split O
1 O
video O
data O

statistics O
for O
action O
recognition O
on O
UCF101 B-DAT

using O
different O
statistics O
design O
on O
UCF101 B-DAT
train O
split O
1. O
For O
local O

finetune O
the O
pre-train O
model O
on O
UCF101 B-DAT
train O
split O
1 O
data O
with O

and O
their O
combina- O
tion O
on O
UCF101 B-DAT
and O
HMDB51 O
dataset O
as O
shown O

a O
useful O
self-supervised O
signals O
for O
UCF101 B-DAT
and O
HMDB51 O
dataset. O
The O
motion O

for O
action O
recognition O
on O
the O
UCF101 B-DAT
dataset O

different O
supervision O
signals O
on O
the O
UCF101 B-DAT
and O
the O
HMDB51 O
datasets O

Domain O
UCF101 B-DAT
acc.(%) O
HMDB51 O
acc. O
(%) O
From O

interesting O
to O
note O
that O
although O
UCF101 B-DAT
only O
improves O
1% O
when O
combined O

with O
the O
state-of-the-art O
both O
on O
UCF101 B-DAT
and O
HMDB51. O
Compared O
with O
methods O

that O
are O
pre- O
trained O
on O
UCF101 B-DAT
dataset, O
we O
improve O
9.3% O
accuracy O

12] O
and O
2.5% O
accuracy O
on O
UCF101 B-DAT
than O
[24]. O
Compared O
with O
the O

also O
achieve O
0.6% O
improvement O
on O
UCF101 B-DAT
and O
5.1% O
improvement O
on O
HMDB51 O

video O
representation O
learning O
methods O
on O
UCF101 B-DAT
and O
HMDB51 O

Method O
UCF101 B-DAT
acc.(%) O
HMDB51 O
acc O

Geometry O
[12] O
55.1 O
23.3 O
Ours O
(UCF101 B-DAT

Use O
the O
C3D O
pre-trained O
on O
UCF101 B-DAT
with O
labels O
as O
feature O
extractor O

Use O
the O
C3D O
pre-trained O
on O
UCF101 B-DAT
with O
our O
self-supervised O
task O
as O

Use O
the O
C3D O
finetuned O
on O
UCF101 B-DAT
on O
our O
self-supervised O
model O
as O

pre-trained O
model O
with O
labels O
on O
UCF101, B-DAT
we O
can O
further O
get O
around O

The O
performance O
on O
UCF101, B-DAT
HMDB51 O
and O
ASLAN O
dataset O
shows O

achieves O
state-of-the-art O
perfor- O
mances O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets. O
This O
strongly O

e.g., O
45.4% O
v.s. O
61.2% O
on O
UCF101 B-DAT

based O
on O
two O
popular O
dataset O
UCF101 B-DAT
[36] O
and O
HMDB51 O
[23]. O
Our O

we O
incorporate O
five O
datasets: O
the O
UCF101 B-DAT
[36], O
the O
Kinetics O
[19], O
the O

Unless O
specifically O
state, O
we O
use O
UCF101 B-DAT
dataset O
for O
our O
model O
pre-training O

UCF101 B-DAT
dataset O
[36] O
consists O
of O
13,320 O

When O
pre-training O
on O
UCF101 B-DAT
train O
split O
1 O
video O
data O

statistics O
for O
action O
recognition O
on O
UCF101 B-DAT

using O
different O
statistics O
design O
on O
UCF101 B-DAT
train O
split O
1. O
For O
local O

finetune O
the O
pre-train O
model O
on O
UCF101 B-DAT
train O
split O
1 O
data O
with O

and O
their O
combina- O
tion O
on O
UCF101 B-DAT
and O
HMDB51 O
dataset O
as O
shown O

a O
useful O
self-supervised O
signals O
for O
UCF101 B-DAT
and O
HMDB51 O
dataset. O
The O
motion O

for O
action O
recognition O
on O
the O
UCF101 B-DAT
dataset O

different O
supervision O
signals O
on O
the O
UCF101 B-DAT
and O
the O
HMDB51 O
datasets O

Domain O
UCF101 B-DAT
acc.(%) O
HMDB51 O
acc. O
(%) O
From O

interesting O
to O
note O
that O
although O
UCF101 B-DAT
only O
improves O
1% O
when O
combined O

with O
the O
state-of-the-art O
both O
on O
UCF101 B-DAT
and O
HMDB51. O
Compared O
with O
methods O

that O
are O
pre- O
trained O
on O
UCF101 B-DAT
dataset, O
we O
improve O
9.3% O
accuracy O

12] O
and O
2.5% O
accuracy O
on O
UCF101 B-DAT
than O
[24]. O
Compared O
with O
the O

also O
achieve O
0.6% O
improvement O
on O
UCF101 B-DAT
and O
5.1% O
improvement O
on O
HMDB51 O

video O
representation O
learning O
methods O
on O
UCF101 B-DAT
and O
HMDB51 O

Method O
UCF101 B-DAT
acc.(%) O
HMDB51 O
acc O

Geometry O
[12] O
55.1 O
23.3 O
Ours O
(UCF101) B-DAT
58.8 O
32.6 O

Use O
the O
C3D O
pre-trained O
on O
UCF101 B-DAT
with O
labels O
as O
feature O
extractor O

Use O
the O
C3D O
pre-trained O
on O
UCF101 B-DAT
with O
our O
self-supervised O
task O
as O

Use O
the O
C3D O
finetuned O
on O
UCF101 B-DAT
on O
our O
self-supervised O
model O
as O

pre-trained O
model O
with O
labels O
on O
UCF101, B-DAT
we O
can O
further O
get O
around O

The O
performance O
on O
UCF101, B-DAT
HMDB51 O
and O
ASLAN O
dataset O
shows O

achieves O
state-of-the-art O
perfor- O
mances O
on O
UCF101 B-DAT
and O
HMDB51 O
datasets. O
This O
strongly O

capture O
motion O
patterns, O
and O
(3) O
UCF B-DAT

3) O
UCF B-DAT

converges. O
For O
the O
training O
of O
UCF B-DAT

benchmarks O
and O
immune O
to O
over-fitting, O
UCF B-DAT

is O
increased O
to O
0.9 O
for O
UCF B-DAT

in O
[9]. O
For O
Kinetics-600 O
and O
UCF B-DAT

Recognition: O
For O
Kinetics-600 O
and O
UCF B-DAT

XP O
Jetson O
TX2 O
Kinetics-600 O
Jester O
UCF B-DAT

UCF101 B-DAT
dataset O
is O
an O
action O
recognition O

to O
Kinetics-600 O
and O
Jester O
datasets, O
UCF B-DAT

over-fitting. O
For O
the O
evaluation O
of O
UCF B-DAT

used O
only O
split-1. O
We O
selected O
UCF B-DAT

inferior O
results O
in O
Kinetics-600 O
and O
UCF B-DAT

mance O
on O
both O
Kinetics-600 O
and O
UCF B-DAT

UCF101 B-DAT
dataset O
is O
an O
action O
recognition O

capture O
motion O
patterns, O
and O
(3) O
UCF B-DAT

3) O
UCF B-DAT

converges. O
For O
the O
training O
of O
UCF B-DAT

benchmarks O
and O
immune O
to O
over-fitting, O
UCF B-DAT

is O
increased O
to O
0.9 O
for O
UCF B-DAT

in O
[9]. O
For O
Kinetics-600 O
and O
UCF B-DAT

Recognition: O
For O
Kinetics-600 O
and O
UCF B-DAT

XP O
Jetson O
TX2 O
Kinetics-600 O
Jester O
UCF B-DAT

UCF101 B-DAT
dataset O
is O
an O
action O
recognition O

to O
Kinetics-600 O
and O
Jester O
datasets, O
UCF B-DAT

over-fitting. O
For O
the O
evaluation O
of O
UCF B-DAT

used O
only O
split-1. O
We O
selected O
UCF B-DAT

inferior O
results O
in O
Kinetics-600 O
and O
UCF B-DAT

mance O
on O
both O
Kinetics-600 O
and O
UCF B-DAT

UCF101 B-DAT
dataset O
is O
an O
action O
recognition O

UCF101 B-DAT

Keywords: O
Action O
Dataset, O
UCF101, B-DAT
UCF50, O
Action O
Recognition O

UCF101 B-DAT

UCF101 B-DAT

We O
introduce O
UCF101 B-DAT
which O
is O
currently O
the O
largest O

the O
best O
of O
our O
knowledge, O
UCF101 B-DAT
is O
currently O
the O
most O
challenging O

e.g. O
KTH O
[11], O
Weizmann O
[3], O
UCF B-DAT
Sports O
[10], O
IXMAS O
[12] O
datasets O

by O
actors; O
HOHA O
[7] O
and O
UCF B-DAT
Sports O
are O
composed O
of O
movie O

and O
clips. O
(HMDB51 O
[5] O
and O
UCF50 B-DAT
[9] O
are O
the O
currently O
the O

for O
6 O
action O
classes O
of O
UCF101 B-DAT

of O
6 O
action O
classes O
from O
UCF101 B-DAT

Action O
Classes: O
UCF101 B-DAT
includes O
total O
number O
of O
101 O

UCF101 B-DAT
is O
an O
extension O
of O
UCF50 O
which O
included O
the O
following O
50 O

UCF101 B-DAT

2. O
101 O
actions O
included O
in O
UCF101 B-DAT
shown O
with O
one O
sample O
frame O

new O
classes O
are O
introduced O
in O
UCF101 B-DAT

for O
each O
action O
class O
of O
UCF101 B-DAT

1. O
Summary O
of O
Characteristics O
of O
UCF101 B-DAT

the O
dataset O
(available O
at O
http://crcv.ucf.edu/data/ O
UCF101 B-DAT

UCF101 B-DAT

UCF101 B-DAT

to O
provide O
baseline O
results O
on O
UCF101 B-DAT

Static O
No O
2005 O
Actor O
Staged O
UCF B-DAT
Sports O
[10] O
9 O
182 O
Dynamic O

Static O
No O
2006 O
Actor O
Staged O
UCF11 B-DAT
[6] O
11 O
1168 O
Dynamic O
Yes O

800 O
Dynamic O
Yes O
2010 O
YouTube O
UCF50 B-DAT
[9] O
50 O
6681 O
Dynamic O
Yes O

Yes O
2011 O
Movies, O
YouTube, O
Web O
UCF101 B-DAT
101 O
13320 O
Dynamic O
Yes O
2012 O

of O
the O
reported O
tests O
on O
UCF101 B-DAT

4. O
Related O
Datasets O
UCF B-DAT
Sports, O
UCF11, O
UCF50 O
and O
UCF101 O
are O
the O

action O
datasets O
compiled O
by O
UCF B-DAT
in O
chronological O
order; O
each O
one O

ifications O
in O
the O
portion O
of O
UCF101 B-DAT
which O
includes O
UCF50 O
videos: O
the O

characteristics O
of O
each. O
Note O
that O
UCF101 B-DAT
is O
remarkably O
larger O
than O
the O

5. O
Conclusion O
We O
introduced O
UCF101 B-DAT
which O
is O
the O
most O
challeng O

outstandingly O
larger O
than O
other O
datasets. O
UCF101 B-DAT
is O
composed O
of O
unconstrained O
videos O

bag O
of O
words O
approach O
on O
UCF101 B-DAT

UCF101 B-DAT

Keywords: O
Action O
Dataset, O
UCF101, B-DAT
UCF50, O
Action O
Recognition O

UCF101 B-DAT

UCF101 B-DAT

We O
introduce O
UCF101 B-DAT
which O
is O
currently O
the O
largest O

the O
best O
of O
our O
knowledge, O
UCF101 B-DAT
is O
currently O
the O
most O
challenging O

for O
6 O
action O
classes O
of O
UCF101 B-DAT

of O
6 O
action O
classes O
from O
UCF101 B-DAT

Action O
Classes: O
UCF101 B-DAT
includes O
total O
number O
of O
101 O

UCF101 B-DAT
is O
an O
extension O
of O
UCF50 O

UCF101 B-DAT

2. O
101 O
actions O
included O
in O
UCF101 B-DAT
shown O
with O
one O
sample O
frame O

new O
classes O
are O
introduced O
in O
UCF101 B-DAT

for O
each O
action O
class O
of O
UCF101 B-DAT

1. O
Summary O
of O
Characteristics O
of O
UCF101 B-DAT

the O
dataset O
(available O
at O
http://crcv.ucf.edu/data/ O
UCF101 B-DAT

UCF101 B-DAT

UCF101 B-DAT

to O
provide O
baseline O
results O
on O
UCF101 B-DAT

Yes O
2011 O
Movies, O
YouTube, O
Web O
UCF101 B-DAT
101 O
13320 O
Dynamic O
Yes O
2012 O

of O
the O
reported O
tests O
on O
UCF101 B-DAT

UCF O
Sports, O
UCF11, O
UCF50 O
and O
UCF101 B-DAT
are O
the O
four O

ifications O
in O
the O
portion O
of O
UCF101 B-DAT
which O
includes O
UCF50 O
videos: O
the O

characteristics O
of O
each. O
Note O
that O
UCF101 B-DAT
is O
remarkably O
larger O
than O
the O

5. O
Conclusion O
We O
introduced O
UCF101 B-DAT
which O
is O
the O
most O
challeng O

outstandingly O
larger O
than O
other O
datasets. O
UCF101 B-DAT
is O
composed O
of O
unconstrained O
videos O

bag O
of O
words O
approach O
on O
UCF101 B-DAT

significant O
im- O
provements O
over O
the O
UCF101 B-DAT
and O
HMDB51 O
benchmarks O

conducted O
on O
two O
widely-used O
benchmarks, O
UCF101 B-DAT
[38] O
and O
HMDB51 O
[18]. O
UCF101 O

Method O
Feature O
Setting O
HMDB51 O
UCF101 B-DAT
ST O
[45] O
BoW O
T O
15.0±3.0 O

tween O
5 O
and O
10 O
minutes. O
UCF101 B-DAT
is O
composed O
of O
realistic O
action O

ping O
classes O
between O
ActivityNet O
and O
UCF101 B-DAT
are O
not O
used O
during O
fine-tuning O

seen/unseen O
splits O
for O
HMDB51 O
and O
UCF101 B-DAT
are O
27/26 O
and O
51/50, O
respectively O

Dataset O
HMDB51 O
UCF101 B-DAT
Setting O
Cross-Dataset O
Transductive O
Cross-Dataset O
Transductive O

6.9% O
in O
transductive O
scenario O
on O
UCF101 B-DAT

the O
concepts O
of O
HMDB51 O
and O
UCF101 B-DAT
are O
not O
very O
distinctive. O
We O

on O
HMDB51 O
and O
1% O
on O
UCF101 B-DAT
be- O
tween O
‘Ours’ O
and O
‘No O

R. O
Zamir, O
and O
M. O
Shah. O
UCF101 B-DAT

significant O
im- O
provements O
over O
the O
UCF101 B-DAT
and O
HMDB51 O
benchmarks O

conducted O
on O
two O
widely-used O
benchmarks, O
UCF101 B-DAT
[38] O
and O
HMDB51 O
[18]. O
UCF101 O

Method O
Feature O
Setting O
HMDB51 O
UCF101 B-DAT
ST O
[45] O
BoW O
T O
15.0±3.0 O

tween O
5 O
and O
10 O
minutes. O
UCF101 B-DAT
is O
composed O
of O
realistic O
action O

ping O
classes O
between O
ActivityNet O
and O
UCF101 B-DAT
are O
not O
used O
during O
fine-tuning O

seen/unseen O
splits O
for O
HMDB51 O
and O
UCF101 B-DAT
are O
27/26 O
and O
51/50, O
respectively O

Dataset O
HMDB51 O
UCF101 B-DAT
Setting O
Cross-Dataset O
Transductive O
Cross-Dataset O
Transductive O

6.9% O
in O
transductive O
scenario O
on O
UCF101 B-DAT

the O
concepts O
of O
HMDB51 O
and O
UCF101 B-DAT
are O
not O
very O
distinctive. O
We O

on O
HMDB51 O
and O
1% O
on O
UCF101 B-DAT
be- O
tween O
‘Ours’ O
and O
‘No O

R. O
Zamir, O
and O
M. O
Shah. O
UCF101 B-DAT

of O
Sports1M, O
which O
we O
name O
miniSports B-DAT

on O
the O
test O
set O
of O
miniSports, B-DAT
since O
our O
aim O
is O
to O

on O
the O
training O
set O
of O
miniSports B-DAT

achieved O
by O
MC3-18 O
on O
the O
miniSports B-DAT
test O
set O
with O
K O

on O
the O
test O
set O
of O
miniSports B-DAT
with O
K O
= O
10 O
sampled O

of O
MC3-18 O
on O
the O
miniSports B-DAT
test O
set O
vs O
the O
number O

Evaluation O
is O
done O
on O
the O
miniSports B-DAT
dataset, O
with O
the O
MC3-18 O
clip O

video-level O
classi- O
fication O
accuracy O
on O
miniSports, B-DAT
obtained O
by O
varying O
the O
number O

of O
Sports1M, O
which O
we O
name O
miniSports B-DAT

on O
the O
test O
set O
of O
miniSports, B-DAT
since O
our O
aim O
is O
to O

on O
the O
training O
set O
of O
miniSports B-DAT

achieved O
by O
MC3-18 O
on O
the O
miniSports B-DAT
test O
set O
with O
K O

on O
the O
test O
set O
of O
miniSports B-DAT
with O
K O
= O
10 O
sampled O

of O
MC3-18 O
on O
the O
miniSports B-DAT
test O
set O
vs O
the O
number O

Evaluation O
is O
done O
on O
the O
miniSports B-DAT
dataset, O
with O
the O
MC3-18 O
clip O

video-level O
classi- O
fication O
accuracy O
on O
miniSports, B-DAT
obtained O
by O
varying O
the O
number O

datasets, O
i.e. O
Moments B-DAT
in I-DAT
Time I-DAT
[17] O
and O
Kinetics O
[2]. O
Ac O

Moments B-DAT
in I-DAT
Time. I-DAT
The O
Moments O
in O
Time O
dataset O
contains O

2, O
on O
both O
of O
the O
Moments B-DAT
in I-DAT
Time I-DAT
and O
Kinetics O

validation O
set O
of O
Moments B-DAT
in I-DAT
Time I-DAT

Their O
performances O
on O
the O
Moments B-DAT
in I-DAT
Time I-DAT
and O
Kinet O

of O
Moments B-DAT
in I-DAT
Time I-DAT

is O
longer O
than O
those O
in O
Moments B-DAT
in I-DAT
Time I-DAT

On O
the O
Moments B-DAT
in I-DAT
Time I-DAT
dataset, O
Table O
7 O
shows O
a O

the O
1st O
place O
in O
the O
Moments B-DAT
in I-DAT
Time I-DAT
Challenge O

Moments B-DAT
in I-DAT
Time, I-DAT
the O
mean O
coefficients O
of O
the O

Moments B-DAT
in I-DAT
Time I-DAT
Kinetics O

in O
Time B-DAT
Challenge O
2018. O
Moreover, O
based O
on O

Time B-DAT
Interest O
Points O

datasets, O
i.e. O
Moments O
in O
Time B-DAT
[17] O
and O
Kinetics O
[2]. O
Ac O

Moments O
in O
Time B-DAT

. O
The O
Moments O
in O
Time B-DAT
dataset O
contains O

both O
of O
the O
Moments O
in O
Time B-DAT
and O
Kinetics O

validation O
set O
of O
Moments O
in O
Time B-DAT

performances O
on O
the O
Moments O
in O
Time B-DAT
and O
Kinet O

of O
Moments O
in O
Time B-DAT

than O
those O
in O
Moments O
in O
Time B-DAT

On O
the O
Moments O
in O
Time B-DAT
dataset, O
Table O
7 O
shows O
a O

place O
in O
the O
Moments O
in O
Time B-DAT
Challenge O

Moments O
in O
Time, B-DAT
the O
mean O
coefficients O
of O
the O

ments O
in O
Time B-DAT
dataset O
depends O
more O
on O
temporal O

Moments O
in O
Time B-DAT
Kinetics O

in O
Time B-DAT
dataset. O
We O
sum O
up O
the O

the O
1st O
place O
in O
the O
Moments B-DAT

datasets, O
i.e. O
Moments B-DAT
in O
Time O
[17] O
and O
Kinetics O

Moments B-DAT
in O
Time. O
The O
Moments O
in O
Time O
dataset O
contains O

Moments B-DAT
CoST(a) O
29.3 O
55.8 O
42.6 O

2, O
on O
both O
of O
the O
Moments B-DAT
in O
Time O
and O
Kinetics O

Moments B-DAT
29.0 O
56.1 O
42.5 O

validation O
set O
of O
Moments B-DAT
in O
Time O

Their O
performances O
on O
the O
Moments B-DAT
in O
Time O
and O
Kinet O

resolution, O
i.e. O
32 O
frames. O
On O
Moments B-DAT
in O

of O
Moments B-DAT
in O
Time. O
Methods O
marked O
in O

is O
longer O
than O
those O
in O
Moments B-DAT
in O
Time O

On O
the O
Moments B-DAT
in O
Time O
dataset, O
Table O
7 O

the O
1st O
place O
in O
the O
Moments B-DAT
in O
Time O
Challenge O

Moments B-DAT
in O
Time, O
the O
mean O
coefficients O

each O
action O
category O
on O
the O
Moments B-DAT

Moments B-DAT
in O
Time O
Kinetics O

Moments B-DAT
in O
time O
dataset: O
one O
million O

for O
action O
recognition O
in B-DAT
videos. O
Existing O
deep O
neural O
net O

learn O
spatial O
and O
temporal O
features O
in B-DAT

and O
won O
the O
1st O
place O
in B-DAT
the O
Moments O

in B-DAT
Time O
Challenge O
2018. O
Moreover, O
based O

attention O
considering B-DAT
its O
potential O
in O
a O
wide O
range O
of O
appli O

task O
lies O
in B-DAT
joint O
spatiotemporal O
feature O
learning. O
The O

in B-DAT
an O
action O
and O
the O
scene O

feature O
captures O
motion O
cues O
embedded O
in B-DAT
the O
evolving O

tion O
information B-DAT
explicitly O
and O
in O
parallel O
to O
spatial O
informa O

exactly O
the O
same O
amount O
of O
in B-DAT

tion O
cues O
directly. O
As O
shown O
in B-DAT
Figure O
2(c), O
by O
fusing O
com O

and O
color O
blobs O
also O
exist O
in B-DAT
tempo O

2) O
Convolution O
kernels O
in B-DAT
C2D O
networks O
are O
inherently O
re O

less O
prone O
to O
overfitting, B-DAT
resulting O
in O
better O
per O

efficient O
for O
each O
channel O
in B-DAT
each O
view, O
which O
allows O
the O

the O
task O
of O
action O
recognition O
in B-DAT
videos, O
experiments O

dynamics O
and O
long O
range O
dependences O
in B-DAT
videos O

tion O
in B-DAT
videos O

is O
shown O
in B-DAT
Figure O
3(a). O
To O
handle O
3D O

video O
action O
recognition O
is O
illustrated O
in B-DAT
Table O
1. O
For O
con O

are O
converted O
to O
3D O
by O
in B-DAT

3. O
As O
ex- O
plored O
in B-DAT
[30], O
given O
a O
residual O
unit O

3 O
(C3D3×3×3) O
as O
shown O
in B-DAT
Figure O
3(b), O
or O
inflate O
the O

1 O
(C3D3×1×1) O
as O
shown O
in B-DAT
Figure O
3(c). O
Experiments O
in O
[30 O

is O
more O
computationally O
efficient. O
Therefore, O
in B-DAT
our O
imple O

W O
for O
spatial O
feature. O
While O
in B-DAT

learned O
during B-DAT
training O
in O
an O
end-to-end O
manner O

CoST(a). O
As O
illustrated O
in B-DAT
Figure O
4, O
the O
coefficients O
α O

with O
back-propagation O
during B-DAT
training. O
During O
in O

architecture O
of O
CoST(b) O
is O
illustrated O
in B-DAT
Figure O
5. O
The O

function O
f O
in B-DAT
Equation O
(3). O
Specifically, O
for O
each O

the O
proposed O
CoST O
is O
shown O
in B-DAT
Fig O

As O
shown O
in B-DAT
Figure O
6(a), O
if O
the O
coefficients O

enabled O
in B-DAT
CoST, O
although O
the O
receptive O
field O

voxels O
in B-DAT
total, O
the O
corresponding O
19 O
parameters O

The O
number O
of O
multiply-adds O
involved B-DAT
in O
the O

voxels O
in B-DAT
the O
receptive O
field O
are O
duplicately O

multiple O
views O
in B-DAT
our O
current O
implementation. O
With O
an O

the O
task O
of O
action O
recognition O
in B-DAT
videos, O
we O
perform O
ex O

datasets, O
i.e. O
Moments B-DAT
in I-DAT
Time I-DAT
[17] O
and O
Kinetics O
[2 O

in B-DAT
all O
experiments O

Moments B-DAT
in I-DAT
Time. I-DAT
The O
Moments O
in O
Time O
dataset O
contains O

frames, O
resulting B-DAT
in O
8 O
frames O
in O
total. O
Next, O
image O
patches O

4 O
GPUs O
are O
employed, O
resulting B-DAT
in O
a O
total O

in B-DAT
Table O
2, O
on O
both O
of O

the O
Moments B-DAT
in I-DAT
Time I-DAT
and O
Kinetics O

mechanism O
introduced B-DAT
in O
our O
model. O
It O
also O
reveals O

is O
adopted O
in B-DAT
the O
following O
experiments O

three O
convolutional O
layers O
in B-DAT
Figure O
5 O
are O
learned O
indepen O

that O
spatiotemporal O
features O
are O
learned O
in B-DAT
a O
de O

coupled O
manner. O
As O
listed O
in B-DAT
Table O
3, O
with O
weight O
sharing O

three O
spatial O
and O
temporal O
views O
in B-DAT

validation O
set O
of O
Moments B-DAT
in I-DAT
Time I-DAT

Their O
performances O
on O
the O
Moments B-DAT
in I-DAT
Time I-DAT
and O
Kinet O

ics O
datasets O
are O
listed O
in B-DAT
Table O
4 O
and O
Table O
5 O

in B-DAT
Section O
3.4 O

i.e. O
32 O
frames. O
On O
Moments O
in B-DAT

of O
Moments B-DAT
in I-DAT
Time. I-DAT
Methods O
marked O
in O
gray O

in B-DAT
this O
dataset O
is O
longer O
than O

those O
in B-DAT
Moments O
in O
Time O

On O
the O
Moments B-DAT
in I-DAT
Time I-DAT
dataset, O
Table O
7 O
shows O

the O
ResNet-50 O
C2D O
baseline B-DAT
reported O
in O
[17] O
by O

2.9% O
and O
5.5% O
in B-DAT
terms O
of O
top-1 O
and O
top-5 O

RGB, O
optical O
flow O
and O
audio) O
in B-DAT
[17] O
by O
a O
large O

which O
won O
the O
1st O
place O
in B-DAT
the O
Moments O
in O
Time O
Challenge O

performance. O
As O
shown O
in B-DAT
Table O
6, O
CoST O
has O
a O

Moments B-DAT
in I-DAT
Time, I-DAT
the O
mean O
coefficients O
of O

ments O
in B-DAT
Time O
dataset O
depends O
more O
on O

three O
views O
in B-DAT
all O
CoST O
layers O
of O
the O

This O
also O
verifies O
the O
conclusion O
in B-DAT
[32] O
that O
temporal O
rep O

Moments B-DAT
in I-DAT
Time I-DAT
Kinetics O

views O
in B-DAT
CoST O
layers O
of O
various O
depths O

in B-DAT
Time O
dataset. O
We O
sum O
up O

in B-DAT
Figure O
7, O
for O
actions O
such O

of O
different O
views O
as O
shown O
in B-DAT

supported O
by O
the O
spacetime O
model O
in B-DAT
the O
context O

jor O
challenge O
for O
action O
recognition O
in B-DAT
videos. O
In O
this O
pa O

in B-DAT
replacement O
for O

Moments O
in B-DAT
time O
dataset: O
one O
million O
videos O

volutional O
networks O
for O
action O
recognition O
in B-DAT
videos. O
In O

Speed-accuracy O
trade-offs O
in B-DAT
video O
classification. O
In O
ECCV O

test, O
including O
HMDB, O
Kinetics, O
and O
Moments B-DAT
in I-DAT
Time I-DAT

standard O
clas- O
sification O
evaluation O
protocol. O
Moments B-DAT
in I-DAT
Time I-DAT
[18] O
is O
a O
large-scale O
dataset O

Transfer O
learned O
architectures O
- O
Moments B-DAT
in I-DAT
Time I-DAT

training O
it O
on O
another O
dataset: O
Moments B-DAT
in I-DAT
Time I-DAT
[18]. O
Table O
5 O
shows O
the O

Time B-DAT
Neural O
Architectures O
for O
Videos O

HMDB, O
Kinetics, O
and O
Moments O
in O
Time B-DAT

Time B-DAT

sification O
evaluation O
protocol. O
Moments O
in O
Time B-DAT
[18] O
is O
a O
large-scale O
dataset O

learned O
architectures O
- O
Moments O
in O
Time B-DAT

on O
another O
dataset: O
Moments O
in O
Time B-DAT
[18]. O
Table O
5 O
shows O
the O

test, O
including O
HMDB, O
Kinetics, O
and O
Moments B-DAT
in O
Time. O
We O
will O
open O

tested, O
including O
HMDB, O
Kinetics, O
and O
Moments B-DAT
in O
time. O
This O
is O
done O

standard O
clas- O
sification O
evaluation O
protocol. O
Moments B-DAT
in O
Time O
[18] O
is O
a O

Transfer O
learned O
architectures O
- O
Moments B-DAT
in O
Time: O
We O
evaluate O
the O

training O
it O
on O
another O
dataset: O
Moments B-DAT
in O
Time O
[18]. O
Table O
5 O

Table O
5. O
Moments B-DAT
in O
time. O
We O
show O
that O

Gutfruend, O
Carl O
Vondrick, O
et O
al. O
Moments B-DAT
in O
time O
dataset: O
one O
million O

Gutfruend, O
Carl O
Vondrick, O
et O
al. O
Moments B-DAT
in O
time O
dataset: O
one O
million O

that O
capture O
rich O
spatio-temporal O
information B-DAT
in O
videos. O
Previous O
work, O
taking O
advantage O

including B-DAT
HMDB, O
Kinetics, O
and O
Moments O
in O
Time. O
We O
will O
open O
source O

with O
many O
successful O
prior O
approaches, O
in B-DAT

capture O
the O
rich O
spatio-temporal O
interactions B-DAT
in O
video O
data O

the O
rich O
spatio-temporal O
information B-DAT
present O
in O
videos. O
Neural O
architecture O
search O
and O

in B-DAT

various O
spatial O
and O
temporal O
interactions B-DAT
in O
videos. O
We O
encourage O
explo- O
ration O

the O
search O
space O
for O
video O
in B-DAT

we O
learn O
2D O
spatial O
filters O
in B-DAT
addition O
to O
the O
temporal O
Gaus O

allow O
learning B-DAT
of O
joint O
features O
in O
3D. O
The O
2D O
filter O
is O

in B-DAT

time O
capture O
longer O
temporal O
information B-DAT
in O
videos O

temporal O
size O
of O
the O
filters O
in B-DAT
each O
module. O
See O
text O
for O

The O
proposed O
algorithm O
results O
in B-DAT
novel O
architectures O
which O
comprise O
interesting O

different O
types O
of O
layers O
combined B-DAT
in O
the O
same O
module O
e.g., O
an O

of O
the O
architecture, O
which O
is O
in B-DAT
contrast O
to O
previous O
handcrafted O
models O

including B-DAT
HMDB, O
Kinetics, O
and O
Moments O
in O
time. O
This O
is O
done O
with O

and O
2D O
convolutional O
lay- O
ers O
in B-DAT
addition O
to O
the O
3D O
layers O

design O
is O
also O
widely O
adopted O
in B-DAT
action O
recognition, O
which O
takes O
optical O

flow O
inputs B-DAT
in O
addition O
to O
raw O
RGBs O
[3 O

on O
capturing B-DAT
longer O
temporal O
information O
in O
continuous O
videos O
using O
pooling O
[19 O

of O
an O
additional O
temporal O
dimension O
in B-DAT
the O
input O
and O
all O
intermediate O

than O
CNNs O
and O
becomes O
prohibitive O
in B-DAT
many O
cases. O
Further, O
expanding O
2D O

2D O
kernels O
L O
times, O
results O
in B-DAT
state-of-the-art O
performance O
[1]. O
(2+1)D O
convolutional O

layer, O
which O
was O
often O
used O
in B-DAT
video O
CNNs O
such O
as O
R(2+1)D O

µ O
is O
constrained B-DAT
to O
be O
in O
[0, O
L), O
µ O
= O
(1/2)(L−1 O

of O
inflated B-DAT
TGMs O
are O
shown O
in O
Fig. O
3 O

in B-DAT

in B-DAT

their O
temporal O
duration O
can O
vary O
in B-DAT
large O
ranges O

courages O
more O
diverse O
architectures O
early O
in B-DAT
the O
search. O
Neural O
architecture O
evolution O

ResNet- O
like O
meta-architecture O
is O
illustrated O
in B-DAT
Figure O
4. O
This O
meta- O
architecture O

and O
pooling B-DAT
layers O
we O
allow O
in O
each O
module, O
and O
N O

9 O
is O
the O
number O
modules O
in B-DAT
the O
meta-architecture. O
There O
are O
2 O

P O
, O
where O
each O
individual B-DAT
in O
the O
population O
is O
a O
particular O

P O
controls O
the O
randomness O
in B-DAT
of O
the O
parent O
selec- O
tion O

population, O
P O
Evaluate O
each O
individual B-DAT
in O
P O
for O
i O
< O
number O

the O
most O
fit O
individual B-DAT
in O
S O
child O
= O
parent O
for O

architecture O
search O
space O
we O
describe O
in B-DAT
Section O
4.1 O
efficiently, O
we O
consider O

Importantly, O
we O
design O
the O
mutation O
in B-DAT
our O
al- O
gorithm O
to O
happen O

applying B-DAT
many O
mu- O
tation O
operators O
in O
the O
early O
stage O
of O
the O

ducing B-DAT
the O
amount O
of O
mutations O
in O
the O
later O
stages, O
which O
is O

to O
controlling B-DAT
the O
learning O
rate O
in O
a O
CNN O
model O
learning. O
As O

described O
in B-DAT
Algorithm O
1, O
we O
apply O
max(d O

operators O
we O
want O
to O
apply O
in B-DAT
the O
beginning, O
and O
r O
controls O

is O
added O
to O
the O
population, O
in B-DAT
order O
to O
maintain O
the O
size O

did O
not O
make O
much O
difference O
in B-DAT
our O
case O

clas- O
sification O
evaluation O
protocol. O
Moments B-DAT
in I-DAT
Time I-DAT
[18] O
is O
a O
large-scale O

derstanding B-DAT
of O
actions O
and O
events O
in O
videos O
(339 O
classes, O
802,264 O
training O

Architecture O
evolution O
is O
done O
in B-DAT
parallel O
on O
smaller O
in- O
put O

iterations. O
Details O
can O
be O
found O
in B-DAT
the O
appendix. O
We O
perform O
evolution O

1) O
uses O
L O
= O
7 O
in B-DAT
the O
first O
3D O
conv. O
layer O

and O
L O
= O
3 O
in B-DAT
all O
the O
other O
3D O
layers O

2) O
uses O
L O
= O
3 O
in B-DAT
all O
its O
layers O

This O
is O
not O
only O
done O
in B-DAT
terms O
of O
recognition O
accuracy O
but O

also O
in B-DAT
terms O
of O
computational O
efficiency. O
As O

shown O
in B-DAT
Table O
7, O
our O
individ- O
ual O

this O
set O
is O
∼10% O
smaller O
(in B-DAT
training/validation O
set O
size) O
than O
the O

we O
use O
Kinetics B-DAT
pre-training O
as O
in O
[36]). O
As O
shown, O
we O
outperform O

use O
RGB O
input B-DAT
(i.e., O
one-stream) O
in O
this O
experiment O

Transfer O
learned O
architectures O
- O
Moments B-DAT
in I-DAT
Time: I-DAT
We O
evaluate O
the O
models O

it O
on O
another O
dataset: O
Moments B-DAT
in I-DAT
Time I-DAT
[18]. O
Table O
5 O
shows O

Table O
5. O
Moments O
in B-DAT
time. O
We O
show O
that O
models O

ran- O
dom O
models. O
As O
shown O
in B-DAT
Table O
13, O
we O
compared O
with O

forming B-DAT
inference O
on O
a O
video O
in O
∼100 O
ms O
(Table O
7). O
Note O

faster, O
258 O
ms, O
than O
previous O
in B-DAT

and O
faster O
runtimes. O
This O
gain B-DAT
in O
runtime O
is O
due O
to O
the O

at O
most O
of O
the O
locations O
in B-DAT
the O
ar- O
chitectures, O
while O
being O

Kinetics. B-DAT
An O
average O
activity O
duration O
in O
Charades O
videos O
are O
around O
12 O

iTGM O
layers O
are O
most O
common O
in B-DAT
the O
best O
models O
(Ta- O
ble O

76.6 O
Arch O
Search O
without O
iTGM O
in B-DAT
space O
76.8 O
EvaNet O
77.2 O

Chen, O
and O
Shuicheng O
Yan. O
Network O
in B-DAT
net- O
work. O
2013. O
3 O

Carl O
Vondrick, O
et O
al. O
Moments O
in B-DAT
time O
dataset: O
one O
million O
videos O

Ryoo. O
Learn- O
ing B-DAT
latent O
sub-events O
in O
activity O
videos O
using O
temporal O
atten O

Laptev, O
and O
Abhinav B-DAT
Gupta. O
Hollywood O
in O
homes: O
Crowdsourcing O
data O
collection O
for O

volutional O
networks O
for O
action O
recognition O
in B-DAT
videos. O
In O
Ad- O
vances O
in O

Dense O
detailed O
labeling B-DAT
of O
actions O
in O
complex O
videos. O
In- O
ternational O
Journal O

Torralba. O
Tem- O
poral O
relational O
reasoning B-DAT
in O
videos. O
arXiv O
preprint O
arXiv:1711.08496, O
2017 O

architectures O
following B-DAT
the O
Inception-like O
meta-architectures O
in O
Figs. O
10, O
11, O
12, O
13 O

validation O
becomes O
the O
‘fitness’ O
used O
in B-DAT
our O
algorithm. O
We O
observed O
that O

and O
we O
used O
this O
setting B-DAT
in O
our O
evolutionary O
algorithm O
to O
reduce O

mutation O
rate. O
As O
we O
described O
in B-DAT
the O
main O
sec- O
tion O
of O

bi/rc O
where O
r O
is O
100 O
in B-DAT
our O
experimental O
setting. O
That O
is O

addition O
to O
the O
experimental O
results O
in B-DAT
the O
main O
paper, O
we O
below O

here O
present O
diverse O
architectures O
evolved O
in B-DAT
the O
fol- O
lowing O
figures. O
The O

inception B-DAT
module O
is O
quite O
different O
in O
all O
three O
networks. O
In O
Figures O

used O
much O
more O
com- O
monly O
in B-DAT
both O
RGB O
and O
optical O
flow O

Chen, O
and O
Shuicheng O
Yan. O
Network O
in B-DAT
net- O
work. O
2013. O
3 O

Carl O
Vondrick, O
et O
al. O
Moments O
in B-DAT
time O
dataset: O
one O
million O
videos O

Ryoo. O
Learn- O
ing B-DAT
latent O
sub-events O
in O
activity O
videos O
using O
temporal O
atten O

Laptev, O
and O
Abhinav B-DAT
Gupta. O
Hollywood O
in O
homes: O
Crowdsourcing O
data O
collection O
for O

volutional O
networks O
for O
action O
recognition O
in B-DAT
videos. O
In O
Ad O

vances O
in B-DAT
Neural O
Information O
Processing O
Systems O
(NIPS O

Dense O
detailed O
labeling B-DAT
of O
actions O
in O
complex O
videos. O
In- O
ternational O
Journal O

Torralba. O
Tem- O
poral O
relational O
reasoning B-DAT
in O
videos. O
arXiv O
preprint O
arXiv:1711.08496, O
2017 O

4.2 O
Datasets O
Moments B-DAT
in I-DAT
Time I-DAT
(MiT) O
Dataset O
The O
Moments O
in O

action O
classification O
accu- O
racies O
on O
Moments B-DAT
in I-DAT
Time I-DAT
[16]. O
Method O
modality O
Top-1 O
Top-5 O

16GB O
per O
core. O
For O
the O
Moments B-DAT
in I-DAT
Time I-DAT
(MiT) O
dataset O
training, O
8 O
videos O

4.2 O
Datasets O
Moments O
in O
Time B-DAT
(MiT) O
Dataset O
The O
Moments O
in O

Time B-DAT
(MiT) O
dataset O
[16] O
is O
a O

accu- O
racies O
on O
Moments O
in O
Time B-DAT
[16]. O
Method O
modality O
Top-1 O
Top-5 O

core. O
For O
the O
Moments O
in O
Time B-DAT
(MiT) O
dataset O
training, O
8 O
videos O

4.2 O
Datasets O
Moments B-DAT
in O
Time O
(MiT) O
Dataset O
The O

Moments B-DAT
in O
Time O
(MiT) O
dataset O
[16 O

action O
classification O
accu- O
racies O
on O
Moments B-DAT
in O
Time O
[16]. O
Method O
modality O

Gutfruend, O
C. O
Vondrick, O
et O
al. O
Moments B-DAT
in O
time O
dataset: O
one O
million O

16GB O
per O
core. O
For O
the O
Moments B-DAT
in O
Time O
(MiT) O
dataset O
training O

Searching B-DAT
for O
Multi-Stream O
Neural O
Connectivity O
in O
Video O
Architectures O

capture O
both O
appearance O
and O
motion O
in B-DAT
videos. O
We O
interpret O
a O
video O

combining B-DAT
representations O
that O
abstract O
different O
in O

approaches O
on O
public O
video O
datasets, O
in B-DAT
some O
cases O
by O
a O
great O

so O
for O
understanding B-DAT
semantic O
contents O
in O
videos O
such O
as O
activity O
recognition O

9, O
7, O
8, O
6]. O
However, O
in B-DAT
most O
cases, O
combining O
appearance O
and O

between O
different O
streams) O
was O
done O
in B-DAT
a O
handcrafted O
way, O
e.g., O
by O

visual O
clues, O
a O
longstanding B-DAT
problem O
in O
video O
understanding. O
We O
propose O
a O

to O
address O
two O
main B-DAT
questions O
in O
video O
representation O
learning: O
(1) O
what O

convolutional O
layers O
to O
be O
repeated O
in B-DAT

the O
1D O
temporal O
conv. O
layers O
in B-DAT
it O

weights O
to O
guide O
the O
evolution, O
in B-DAT
addition O
to O
randomly O
combining/splitting/connecting O
sub-network O

both O
spatial O
and O
temporal O
information B-DAT
in O
the O
video. O
Full O
3D O
space-time O

studied O
replacing B-DAT
2D O
convolutional O
layers O
in O
standard O
image-based O
CNNs O
such O
as O

has O
also O
been O
extremely O
popular O
in B-DAT
video O
CNNs O
[20, O
7–9]. O
The O

capture O
both O
motion O
and O
appearance O
in B-DAT
videos. O
The O
idea O
is O
to O

image). O
On O
the O
other O
hand, O
in B-DAT
this O
work, O
our O
objective O
is O

understanding. B-DAT
We O
experimentally O
confirm O
that O
in O
our O
multi-stream O
video O
CNN O
case O

directed O
acyclic O
graph. O
Each O
node O
in B-DAT
the O
graph O
corresponds O
to O
a O

Nodes. O
A O
node O
in B-DAT
our O
graph O
representation O
is O
a O

sink B-DAT
nodes. O
A O
source O
node O
in O
the O
graph O
directly O
takes O
the O

The O
1D O
conv. O
is O
omitted O
in B-DAT
optical O
flow O
stems. O
A O
sink O

pooling. B-DAT
More O
details O
are O
provided O
in O
Appendix O

Each O
node O
in B-DAT
the O
graph O
also O
has O
two O

of O
the O
temporal O
convolutional O
layers O
in B-DAT
different O
blocks, O
which O
are O
discussed O

the O
channels O
of O
all O
nodes O
in B-DAT
the O
same O
block O
level O
to O

with O
different O
temporal O
resolutions O
as O
in B-DAT
[6] O
or O
by O
using O
temporally O

standard O
2D O
dilated O
convolutions O
used O
in B-DAT
[4] O
or O
[31 O

different O
from O
the O
one O
used O
in B-DAT
[13], O
which O
designed O
a O
specific O

the O
standard O
dilated O
convolutions O
used O
in B-DAT
[4, O
31 O

node O
is O
computed O
as O
F O
in B-DAT

was O
high O
enough O
to O
survive O
in B-DAT
the O
population O
pool O
and O
eventually O

guide’ O
our O
architecture O
evolution O
algorithm O
in B-DAT
a O
preferable O
way, O
which O
we O

discuss O
more O
in B-DAT
Section O
3.2 O

inputs B-DAT
from O
different O
nodes O
differ O
in O
their O
spatial O
size, O
we O
add O

the O
representations O
is O
always O
consistent O
in B-DAT
our O
graphs, O
as O
there O
is O

no O
temporal O
striding B-DAT
in O
our O
formulation O
and O
the O
layers O

in B-DAT
the O
nodes O
are O
fully O
convolutional O

that O
modify O
nodes O
and O
edges O
in B-DAT
architectures O
over O
iterations. O
The O
algorithm O

making B-DAT
the O
number O
of O
channels O
in O
their O
convolutional O
layers O
half O
that O

parent). O
More O
details O
are O
found O
in B-DAT
Appendix. O
As O
a O
result, O
we O

randomly O
added O
which O
were O
not O
in B-DAT
the O
parent O
architecture. O
We O
enumerate O

the O
parent O
architecture O
as O
described O
in B-DAT
Section O
3.2. O
0∼4 O
random O
number O

4.2 O
Datasets O
Moments B-DAT
in I-DAT
Time I-DAT
(MiT) O
Dataset O
The O
Moments O

in B-DAT
Time O
(MiT) O
dataset O
[16] O
is O

dataset O
[19] O
which O
is O
unique O
in B-DAT
the O
activity O
recognition O
domain O
as O

classification O
accu- O
racies O
on O
Moments B-DAT
in I-DAT
Time I-DAT
[16]. O
Method O
modality O
Top-1 O

activities. O
Activities O
may O
temporally O
overlap O
in B-DAT
a O
Charades O
video, O
requiring O
the O

is O
outperforming B-DAT
the O
prior O
works O
in O
both O
datasets, O
setting O
the O
new O

above O
31% O
(with O
all O
previous O
in B-DAT
the O
tens O
and O
twenties). O
We O

ing. B-DAT
Four-stream O
architectures O
are O
reported O
in O
this O
paper O
for O
the O
first O

of O
the O
three O
top-performing B-DAT
models O
in O
each O
pool O

all O
outputs O
of O
the O
blocks O
in B-DAT
the O
2nd O
last O
level. O
(3 O

and O
L. O
Torresani. O
Connectivity O
learning B-DAT
in O
multi-branch O
networks. O
In O
Workshop O
on O

video O
action O
recognition. O
In O
Advances O
in B-DAT
Neural O
Information O
Processing O
Systems O
(NeurIPS O

analysis O
of O
selection O
schemes O
used O
in B-DAT
genetic O
algorithms. O
In O
Foundations O
of O

C. O
Vondrick, O
et O
al. O
Moments O
in B-DAT
time O
dataset: O
one O
million O
videos O

convolutional O
networks O
for O
action O
recognition O
in B-DAT
videos. O
In O
Advances O
in O
Neural O

feature O
learning: B-DAT
Speed- O
accuracy O
trade-offs O
in O
video O
classification. O
In O
Proceedings O
of O

A. O
Torralba. O
Temporal O
relational O
reasoning B-DAT
in O
videos. O
In O
Proceedings O
of O
European O

As O
we O
described O
in B-DAT
the O
main O
paper, O
each O
node O

filters O
of O
the O
convolutional O
layers O
in B-DAT
the O
block. O
When O
splitting O
or O

D4 O
= O
512. O
The O
layers O
in B-DAT
the O
stems O
have O
64 O
channels O

A.2 O
Hand-designed O
models O
used O
in B-DAT
ablation O
study O

hand-designed O
(2+1)D O
CNN O
models O
used O
in B-DAT
our O
ablation O
study. O
We O
also O

connectivity O
within B-DAT
them O
are O
learned O
in O
the O
process. O
We O
observe O
that O

description O
of O
the O
networks O
used O
in B-DAT
the O
main O
paper: O
“Two-stream O
(late O

baseline B-DAT
(2+1)D O
CNN O
models O
used O
in O
our O
ablation O
study O

always O
connected O
to O
every O
node O
in B-DAT
the O
immediate O
next O
level. O
“Flow->RGB O

provide O
the O
final B-DAT
AssembleNet O
model O
in O
table O
form O
in O
Table O
4 O

i.e., O
the O
connections). O
As O
mentioned O
in B-DAT
the O
main O
paper, O
2D O
and O

2+1)D O
residual O
modules O
are O
repeated O
in B-DAT
each O
block. O
The O
number O
of O

max O
pooling B-DAT
layer. O
As O
suggested O
in O
the O
main O
paper, O
the O
model O

the O
number O
of O
video O
classes O
in B-DAT
the O
dataset. O
The O
sink O
node O

per O
core. O
For O
the O
Moments B-DAT
in I-DAT
Time I-DAT
(MiT) O
dataset O
training, O
8 O

we O
used O
is O
12.5 O
fps O
in B-DAT
both O
datasets. O
The O
spatial O
input O

used O
the O
standard O
Momentum O
Optimizer O
in B-DAT
TensorFlow O
with O
the O
learning O
rate O

A.2 O
Hand-designed O
models O
used O
in B-DAT
ablation O
study O

The O
paucity O
of O
videos O
in B-DAT
current O
action O
classification O
datasets O
(UCF-101 O

paper O
re-evaluates O
state-of-the-art O
architec- O
tures O
in B-DAT
light O
of O
the O
new O
Kinetics O

considerably O
improve O
upon O
the O
state-of-the-art O
in B-DAT
action O
classification, O
reaching O
80.9% O
on O

used O
for O
other O
tasks O
and O
in B-DAT
other O
domains. O
One O
of O
the O

challenge O
[10, O
23]. O
Furthermore, O
improvements O
in B-DAT
the O
deep O
architecture, O
changing O
from O

fed O
through O
to O
commensurate O
improvements O
in B-DAT
the O
PASCAL O
VOC O
performance O
[25 O

going? B-DAT
Actions O
can O
be O
ambiguous O
in O
individual O
frames, O
but O
the O
limitations O

will O
give O
a O
similar O
boost O
in B-DAT
performance O
when O
ap- O
plied O
to O

there O
is O
always O
a O
boost O
in B-DAT
performance O
by O
pre-training, O
but O
the O

tures O
has O
matured O
quickly O
in B-DAT
recent O
years, O
there O
is O
still O

of O
the O
ma- O
jor O
differences O
in B-DAT
current O
video O
architectures O
are O
whether O

pre-computed O
opti- O
cal O
flow; O
and, O
in B-DAT
the O
case O
of O
2D O
ConvNets O

architectures O
we O
evaluate O
is O
shown O
in B-DAT
figure O
2 O
and O
the O
specification O

their O
temporal O
interfaces B-DAT
is O
given O
in O
table O
1 O

normalization O
[13], O
and O
morph O
it O
in B-DAT
different O
ways. O
The O
expectation O
is O

that O
with O
this O
back O
bone O
in B-DAT
common, O
we O
will O
be O
able O

whole O
video O
[15]. O
This O
is O
in B-DAT
the O
spirit O
of O
bag O
of O

22, O
33]; O
but O
while O
convenient O
in B-DAT
practice, O
it O
has O
the O
issue O

of O
all O
models O
is O
given O
in B-DAT
table O
1 O

Figure O
2. O
Video O
architectures O
considered O
in B-DAT
this O
paper. O
K O
stands O
for O

the O
total O
number O
of O
frames O
in B-DAT
a O
video, O
whereas O
N O
stands O

112 O
× O
112-pixel O
crops O
as O
in B-DAT
the O
original O
implementation. O
Differently O
from O

to O
the O
original B-DAT
model O
is O
in O
the O
first O
pooling O
layer, O
we O

a O
temporal O
stride O
of O
2 O
in B-DAT

low-level O
motion O
which O
is O
critical O
in B-DAT
many O
cases. O
It O
is O
also O

paper O
approximately O
using B-DAT
Inception-V1. O
The O
in O

including B-DAT
the O
two-stream O
averaging O
process O
in O
the O
original O
model O

it O
will O
be O
shown O
in B-DAT
section O
4 O

been O
developed O
over O
the O
years, O
in B-DAT
part O
through O
painstaking O
trial O
and O

for O
boring B-DAT
videos O
are O
constant O
in O
time, O
the O
outputs O
of O
pointwise O

Pacing B-DAT
receptive O
field O
growth O
in O
space, O
time O
and O
net- O
work O

and O
means O
that O
features O
deeper O
in B-DAT
the O
networks O
are O
equally O
affected O

image O
locations O
increasingly B-DAT
far O
away O
in O
both O
dimensions. O
A O
symmetric O
receptive O

If O
it O
grows O
too O
quickly O
in B-DAT
time O
relative O
to O
space, O
it O

layer, O
besides O
the O
max-pooling B-DAT
layers O
in O
parallel O
Inception O
branches. O
In O
our O

to O
not O
perform O
temporal O
pooling B-DAT
in O
the O
first O
two O
max- O
pooling O

3 O
kernels O
and O
stride O
1 O
in B-DAT
time), O
while O
having O
symmetric O
kernels O

and O
strides O
in B-DAT
all O
other O
max-pooling O
layers. O
The O

The O
overall O
architecture O
is O
shown O
in B-DAT
fig. O
3. O
We O
train O
the O

whereas O
optical O
flow O
algorithms O
are O
in B-DAT
some O
sense O
recurrent O
(e.g. O
they O

a O
two-stream O
configuration O
– O
shown O
in B-DAT
fig. O
2, O
e) O
– O
with O

momen- O
tum O
set O
to O
0.9 O
in B-DAT
all O
cases, O
with O
synchronous O
paralleliza O

All O
the O
models O
were O
implemented O
in B-DAT
Tensor- O
Flow O
[1 O

sizes O
for O
a O
few O
layers O
in B-DAT
the O
network O
are O
provided O
in O

The O
predictions O
are O
obtained B-DAT
convolutionally O
in O
time O
and O
averaged O

it O
was O
built O
is O
given O
in B-DAT
[16 O

chitectures O
described O
in B-DAT
section O
2 O
whilst O
varying O
the O

new O
I3D O
models O
do O
best O
in B-DAT
all O
datasets, O
with O
either O
RGB O

and O
without O
ImageNet O
pretraining. B-DAT
Numbers O
in O
brackets O
() O
are O
the O
Top-5 O

on O
HMDB-51; O
this O
may O
be O
in B-DAT
part O
due O
to O
lack O
of O

training B-DAT
data O
in O
HMDB-51 O
but O
also O
because O
this O

many O
clips O
have O
different O
actions O
in B-DAT
the O
ex- O
act O
same O
scene O

substantially O
lower O
on O
Kinetics. B-DAT
Visual O
in O

discern O
actions O
from O
flow O
alone O
in B-DAT
Kinetics, O
and O
this O
was O
rarely O

the O
value O
of O
training B-DAT
models O
in O
Kinet- O
ics O
starting O
from O
ImageNet-pretrained O

the O
results O
are O
shown O
in B-DAT
table O
3. O
It O
can O
be O

that O
ImageNet O
pre-training B-DAT
still O
helps O
in O
all O
cases O
and O
this O
is O

flow O
network O
filters, O
the O
one O
in B-DAT
the O
middle O
shows O
filters O
from O

Inception-v1 O
filters, O
while O
the O
filters O
in B-DAT
the O
RGB O
I3D O
network O
are O

Best O
seen O
on O
the O
computer, O
in B-DAT
colour O
and O
zoomed O
in O

The O
results O
are O
given O
in B-DAT
table O
4. O
The O
clear O
outcome O

of O
the O
models O
after O
pretraining B-DAT
in O
Ki- O
netics O
(Fixed) O
also O
leads O

as O
much O
from O
the O
images O
in B-DAT
ImageNet. O
The O
difference O
over O
the O

els O
and O
previous O
state-of-the-art O
methods O
in B-DAT
table O
5, O
on O
UCF-101 O
and O

the O
trained B-DAT
models O
are O
shown O
in O
fig. O
4 O

return O
to O
the O
question O
posed O
in B-DAT
the O
introduction, O
“is O
there O
a O

benefit O
in B-DAT
transfer O
learning O
from O
videos?”. O
It O

there O
is O
a O
considerable O
benefit O
in B-DAT
pre-training O
on O

there O
has O
been O
such O
benefits O
in B-DAT
pre-training O
ConvNets O
on O
ImageNet O
for O

if O
there O
is O
a O
benefit O
in B-DAT
using O
Kinetics O
pre-training O
for O
other O

dataset’s O
release O
to O
facilitate O
research O
in B-DAT
this O
area O

mechanisms O
[20] O
to O
fo- O
cus O
in B-DAT
on O
the O
human O
actors. O
Recent O

by O
incorporating B-DAT
linked O
object O
detections O
in O

gone O
out O
of O
the O
box O
in B-DAT
attempts O
to O
capture O
this O
relationship O

to O
also O
include B-DAT
these O
models O
in O
our O
comparison O
but O
we O
could O

on O
the O
Kinetics B-DAT
project O
and O
in O
particular O
Brian O
Zhang O
and O
Tim O

Hu- O
man O
focused O
action O
localization O
in B-DAT
video. O
In O
International O
Workshop O
on O

region O
proposal O
networks. O
In O
Advances O
in B-DAT
neural O
information O
processing O
systems, O
pages O

in B-DAT
videos. O
British O
Machine O
Vision O
Conference O

convolutional O
networks O
for O
action O
recognition O
in B-DAT
videos. O
In O
Advances O
in O
Neural O

human O
actions O
classes O
from O
videos O
in B-DAT
the O
wild. O
arXiv O
preprint O
arXiv:1212.0402 O

train O
deep O
neural O
networks O
[25]. O
Time B-DAT
contrast O
networks O
are O
used O
for O

Time B-DAT

C., O
Hsu, O
J., O
Levine, O
S.: O
Time B-DAT

Dataset O
UCF O
Kinetics O
Moments B-DAT
Something O
Jester O
Charades O

Temporal O
Relational O
Reasoning B-DAT
in O
Videos O

to O
dis- O
cover O
temporal O
relations O
in B-DAT
videos. O
Through O
only O
sparsely O
sampled O

can O
accurately O
predict O
human-object O
interactions B-DAT
in O
the O
Something-Something O
dataset O
and O
identify O

networks O
and O
3D O
convolution O
networks O
in B-DAT
recognizing O
daily O
activities O
in O
the O

interpretable B-DAT
visual O
common O
sense O
knowledge O
in O
videos1 O

Activity O
recognition O
in B-DAT
videos O
has O
been O
one O
of O

the O
core O
topics O
in B-DAT
computer O
vision. O
However, O
it O
remains O

convolutional O
neural O
networks O
still O
struggle O
in B-DAT
situations O
where O
data O
and O
observations O

beyond O
the O
appearance O
of O
objects O
in B-DAT
the O
frames O
and O
the O
optical O

that O
enables O
temporal O
relational O
reasoning B-DAT
in O
neural O
networks. O
This O
module O
is O

by O
the O
relational O
network O
proposed O
in B-DAT
[7], O
but O
instead O
of O
modeling O

the O
temporal O
relations O
between O
observations O
in B-DAT
videos. O
Thus, O
TRN O
can O
learn O

module O
that O
can O
be O
used O
in B-DAT
a O
plug-and-play O
fashion O
with O
any O

Temporal O
Relational O
Reasoning B-DAT
in O
Videos O
3 O

Activity O
Recognition. O
Activity O
recog- O
nition O
in B-DAT
videos O
is O
a O
core O
problem O

in B-DAT
computer O
vision. O
With O
the O
rise O

also O
used O
to O
recognize O
activities O
in B-DAT
videos O
[14]. O
Recently, O
I3D O
networks O

computationally O
expensive, O
given O
the O
redundancy O
in B-DAT
consecutive O
frames; O
3) O
Since O
sequences O

Temporal O
Information O
in B-DAT
Activity O
Recognition. O
For O
activity O
recogni O

For O
recognizing B-DAT
the O
complex O
activities O
in O
these O
three O
datasets, O
it O
is O

to O
learn O
the O
temporal O
relations O
in B-DAT
end-to-end O
training. O
One O
relevant O
work O

on O
modeling B-DAT
the O
cause-effect O
in O
videos O
is O
[23]. O
[23] O
uses O

the O
multiple O
frames O
information, B-DAT
both O
in O
training O
and O
testing O

modeling B-DAT
the O
multi-scale O
temporal O
relations O
in O
videos. O
In O
the O
domain O
of O

to O
learn O
various O
temporal O
relations O
in B-DAT
videos O
in O
a O
supervised O
learning O

Temporal O
Relational O
Reasoning B-DAT
in O
Videos O
5 O

sense O
knowledge O
to O
recognize O
activities O
in B-DAT
videos O

overall O
network O
framework O
is O
illustrated O
in B-DAT
Fig.2 O

networks O
is O
able O
to O
run O
in B-DAT
real-time O
on O
a O
desktop O
to O

competitive O
results O
on O
activity O
classification O
in B-DAT
the O
Charades O
dataset O
[11], O
outperforming O

2nd O
release O
of O
the O
dataset O
in B-DAT
early O
July O
2018) O
[9,28], O
Jester O

Charades O
dataset O
[11] O
are O
listed O
in B-DAT
Table O
1. O
All O
three O
datasets O

are O
crowd-sourced, O
in B-DAT
which O
the O
videos O
are O
collected O

activities. O
Unlike O
the O
Youtube-type O
videos O
in B-DAT
UCF101 O
and O
Kinetics, O
there O
is O

and O
end O
of O
each O
activity O
in B-DAT
the O
crowd-sourced O
video, O
emphasizing O
the O

Statistics O
of O
the O
datasets O
used O
in B-DAT
evaluating O
the O
TRNs O

features O
play O
an O
important O
factor O
in B-DAT
visual O
recognition O
tasks O
[29]. O
Features O

module O
for O
temporal O
relational O
reasoning B-DAT
in O
videos. O
Thus, O
we O
fix O
the O

Temporal O
Relational O
Reasoning B-DAT
in O
Videos O
7 O

BN-Inception) O
pretrained B-DAT
on O
ImageNet O
used O
in O
[31] O
because O
of O
its O
balance O

after O
global O
pooling B-DAT
as O
used O
in O
[16]. O
We O
keep O
the O
network O

We O
set O
k O
= O
3 O
in B-DAT
the O
experiments O
as O
the O
number O

of O
accumulated O
relation O
triples O
in B-DAT
each O
relation O
module. O
gφ O
is O

the O
training B-DAT
can O
be O
finished O
in O
less O
than O
24 O
hours O
for O

TRN O
(thus O
N O
= O
8 O
in B-DAT
Eq.3), O
as O
including O
higher O
frame O

the O
objects O
characterize O
the O
activities O
in B-DAT
the O
dataset O

Something- B-DAT
V2 O
datasets O
are O
listed O
in O
Table O
2a. O
The O
baseline O
is O

average O
pooling B-DAT
of O
frames O
used O
in O
TSN O
[16] O
achieves O
better O
score O

set, O
the O
results O
are O
shown O
in B-DAT
Table O
2.a O

the O
temporal O
ordering B-DAT
of O
patterns O
in O
the O
features. O
We O
keep O
all O

As O
shown O
in B-DAT
Table O
2b, O
our O
models O
outperform O

see O
that O
additional O
frames O
included B-DAT
in O
the O
relation O
bring O
further O
significant O

the O
Something-V1. B-DAT
TRN O
outperforms O
TSN O
in O
a O
large O
margin O
as O
the O

the O
Jester O
dataset O
are O
listed O
in B-DAT
Table O
3a. O
The O
result O
on O

the O
top O
methods O
are O
listed O
in B-DAT
Table O
3b. O
MultiScale O
TRN O
again O

recognition. O
The O
results O
are O
listed O
in B-DAT
Table O
4. O
Our O
method O
outperforms O

the O
three O
datasets O
are O
shown O
in B-DAT
Figure O
3. O
The O
examples O
in O

Temporal O
Relational O
Reasoning B-DAT
in O
Videos O
9 O

shown O
in B-DAT
reverse. O
Moreover, O
the O
successful O
prediction O

of O
categories O
in B-DAT
which O
an O
individual O
pretends O
to O

something B-DAT
into O
something’ O
as O
shown O
in O
the O
second O
row) O
suggests O
that O

of O
several O
lower-level O
actions O
contained B-DAT
in O
short O
segments O
conveys O
crucial O
semantic O

in B-DAT

in B-DAT

section, O
we O
have O
a O
more O
in B-DAT

TRNs O
to O
recognize O
an O
activity O
in B-DAT
the O
same O
video. O
We O
can O

some O
de- O
gree O
of O
confidence O
in B-DAT
the O
correct O
action, O
but O
is O

the O
network O
needs O
additional O
frames O
in B-DAT
the O
TRNs O
to O
correctly O
recognize O

example O
is O
the O
last O
video O
in B-DAT
Figure O
4: O
The O
action’s O
context O

motion O
of O
the O
individuals B-DAT
hand O
in O
the O
third O
frame O
of O
the O

the O
object, O
thus, O
solidifying B-DAT
confidence O
in O
the O
correct O
class O
prediction O

in B-DAT

in B-DAT

in B-DAT

in B-DAT

in B-DAT

Temporal O
Relational O
Reasoning B-DAT
in O
Videos O
11 O

Zooming B-DAT
in O
with O
full O
hand O
Pushing O
hand O

the O
scenario O
with O
input B-DAT
frames O
in O
temporal O
order O
and O
in O
shuffled O

training B-DAT
the O
TRNs, O
as O
shown O
in O
Figure O
6a. O
For O
training O
the O

we O
randomly O
shuffle O
the O
frames O
in B-DAT
the O
relation O
modules. O
The O
significant O

importance O
of O
the O
temporal O
order O
in B-DAT
the O
activity O
recognition. O
More O
interestingly O

recognition O
for O
the O
Youtube-type O
videos O
in B-DAT
UCF101 O
doesn’t O
necessarily O
require O
the O

ordering B-DAT
influences O
activity O
recogni- O
tion O
in O
TRN, O
we O
examine O
and O
plot O

show O
the O
largest O
dif- O
ferences O
in B-DAT
the O
class O
accuracy O
between O
ordered O

drawn O
from O
the O
Something-Something B-DAT
dataset, O
in O
Figure O
6b. O
In O
general, O
actions O

severe O
if O
penalizing B-DAT
at O
all O
in O
some O
cases, O
with O
several O
categories O

make O
the O
correct O
prediction. O
Particularly O
in B-DAT
challenging O
ambiguous O
cases, O
for O
example O

rise O
to O
a O
curious O
difference O
in B-DAT
accuracy O
for O
that O
action O

activity. O
But O
rec- O
ognizing B-DAT
activities O
in O
UCF101 O
does O
not O
necessarily O
require O

evaluate O
the O
two O
pool O
strategies O
in B-DAT
detail O
as O
shown O
in O
Table O

5. O
The O
difference O
in B-DAT
the O
performance O
using O
average O
pool O

the O
importance O
of O
temporal O
orders O
in B-DAT
a O
video O
dataset. O
The O
tested O

Temporal O
Relational O
Reasoning B-DAT
in O
Videos O
13 O

be O
due O
to O
that O
activity O
in B-DAT
the O
randomly O
trimmed O
Youtube O
videos O

TRN O
can O
better O
differentiate O
activities O
in B-DAT
Something-Something O
dataset O

15 O
most O
frequent O
activity O
classes O
in B-DAT
the O
validation O
set. O
We O
can O

observe O
the O
similarity O
among O
categories O
in B-DAT
the O
visualization O
map. O
For O
example O

by O
the O
first O
frame O
shown O
in B-DAT
the O
left O
column, O
is O
used O

forecasts O
and O
corresponding B-DAT
probabilities O
listed O
in O
the O
middle O
column. O
The O
ground O

lenging B-DAT
yet O
less O
explored O
problem O
in O
activity O
recognition. O
Here O
we O
evaluate O

and O
50% O
of O
the O
frames O
in B-DAT
each O
validation O
video. O
Results O
are O

shown O
in B-DAT
Table O
6. O
For O
comparison, O
we O

to O
enable O
temporal O
relational O
reasoning B-DAT
in O
neural O
networks O
for O
videos. O
We O

discover O
visual O
common O
sense O
knowledge O
in B-DAT
videos O

A.O.. O
It O
is O
also O
supported O
in B-DAT
part O
by O
the O
Intelligence O
Advanced O

Temporal O
Relational O
Reasoning B-DAT
in O
Videos O
15 O

for O
un- O
derstanding B-DAT
human O
actions O
in O
videos? O
arXiv O
preprint O
arXiv:1708.02696 O
(2017 O

human O
actions O
classes O
from O
videos O
in B-DAT
the O
wild. O
Proc. O
CVPR O
(2012 O

networks O
for O
action O
recog- O
nition O
in B-DAT
videos. O
In: O
In O
Advances O
in O

I., O
Gupta, O
A.: O
Hol- O
lywood O
in B-DAT
homes: O
Crowdsourcing O
data O
collection O
for O

volutional O
neural O
networks. O
In: O
Advances O
in B-DAT
neural O
information O
processing O
systems. O
(2012 O

using B-DAT
places O
database. O
In: O
Advances O
in O
neural O
information O
processing O
systems. O
(2014 O

of O
intuitive B-DAT
physics. O
In: O
Advances O
in O
Neural O
Information O
Processing O
Systems. O
(2016 O

Temporal O
Relational O
Reasoning B-DAT
in O
Videos O

Segment O
Networks O
for O
Action O
Recognition O
in B-DAT
Videos O

recognition. O
However, O
for O
action O
recognition O
in B-DAT
videos, O
their O
advantage O
over O
traditional O

framework O
for O
learning B-DAT
action O
models O
in O
videos. O
This O
method, O
called O
temporal O

easily O
adapted O
for O
action O
recognition O
in B-DAT
both O
trimmed O
and O
untrimmed O
videos O

5], O
owing B-DAT
to O
its O
applications O
in O
many O
areas O
like O
security O
and O

behavior O
analysis. O
For O
action O
recognition O
in B-DAT
videos, O
there O
are O
two O
crucial O

6] O
have O
achieved O
great O
success O
in B-DAT
classifying O
images O
of O
objects O
[7 O

representations O
from O
raw O
visual O
data O
in B-DAT
large- O
scale O
supervised O
datasets O
(e.g O

of O
ConvNets O
to O
action O
recognition O
in B-DAT
unconstrained O
videos O
is O
impeded O
by O

for O
understanding B-DAT
the O
dy- O
namics O
in O
traditional O
methods O
[19], O
[20], O
[21 O

considered O
as O
a O
critical O
factor O
in B-DAT
deep O
ConvNet O
frameworks O
[1], O
[15 O

deploy O
the O
learned O
action O
models O
in B-DAT
a O
realistic O
setting O
we O
often O

action O
models O
to O
action O
recognition O
in B-DAT
untrimmed O
videos O

remain B-DAT
limited O
in O
both O
size O
and O
diversity, O
making O

the O
action O
recogni- O
tion O
problem O
in B-DAT
this O
paper O
from O
the O
following O

framework O
for O
learning B-DAT
action O
models O
in O
videos. O
It O
is O
based O
on O

be O
more O
favorable O
and O
efficient O
in B-DAT
this O
case. O
The O
TSN O
framework O

structures O
over O
the O
whole O
video, O
in B-DAT
a O
way O
that O
its O
computational O

To O
tackle O
the O
practical O
difficulties O
in B-DAT
learning O
and O
apply O

to O
perform O
Batch O
Normalization O
(BN) O
in B-DAT
a O
fine-tuning O
scenario, O
denoted O
as O

which O
has O
numerous O
potential O
applications O
in B-DAT
real-world O
problems O

our O
method O
for O
action O
recognition O
in B-DAT
both O
trimmed O
and O
untrimmed O
videos O

method O
secures O
the O
1st O
place O
in B-DAT
untrimmed O
video O
classification O
at O
the O

different O
aspects O
of O
the O
problems O
in B-DAT
efficiently O
and O
effectively O
learning O
and O

extends O
our O
previous O
work O
[31] O
in B-DAT
a O
number O
of O
aspects. O
First O

Challenge O
2016, O
which O
ranks O
#1 O
in B-DAT
untrimmed O
video O
classification O
among O
24 O

recognition O
has O
been O
studied O
extensively O
in B-DAT
recent O
years O
and O
readers O
can O

For O
action O
recognition O
in B-DAT
videos, O
the O
visual O
representation O
plays O

a O
set O
of O
mid-level O
patches O
in B-DAT
a O
strongly-supervised O
manner. O
Similar O
to O

ConvNet O
architectures O
for O
action O
recognition O
in B-DAT
videos O
[1], O
[4], O
[5], O
[15 O

use O
recurrent O
neural O
networks O
(RNN), O
in B-DAT
particular O
LSTM, O
to O
model O
the O

frame O
features O
for O
action O
recognition O
in B-DAT
videos O

to O
learn O
the O
model O
parameters O
in B-DAT
an O
iterative O
approach. O
Wang O
et O

As O
discussed O
in B-DAT
Sec. O
1, O
long-range O
temporal O
modeling O

is O
important O
for O
action O
understanding B-DAT
in O
videos. O
The O
existing O
deep O
architectures O

1], O
[16], O
it O
still O
suffers O
in B-DAT
both O
computational O
and O
modeling O
aspects O

it O
totally O
samples O
64 O
frames O
in B-DAT
the O
work O
of O
[23] O
and O

the O
frames O
are O
densely O
recorded O
in B-DAT
the O
videos, O
the O
content O
changes O

to O
model O
the O
temporal O
structures O
in B-DAT
a O
human O
action. O
Normally, O
the O

a O
video-level O
framework O
as O
shown O
in B-DAT
Figure O
1, O
which O
would O
be O

explained B-DAT
in O
the O
next O
subsection O

its O
corresponding B-DAT
segment. O
Each O
snippet O
in O
this O
sequence O
produces O
its O
own O

details O
on O
these O
consensus O
functions O
in B-DAT
the O
next O
subsection O

K O
is O
number O
of O
segments O
in B-DAT
temporal O
segment O
net- O
work. O
When O

function O
is O
an O
important O
component O
in B-DAT
our O
temporal O
segment O
net- O
work O

video. O
On O
the O
other O
hand, O
in B-DAT
particular O
for O
noisy O
videos O
with O

phases O
may O
play O
different O
roles O
in B-DAT
recogniz- O
ing O
action O
classes. O
This O

the O
basic O
back- O
propagation O
formula O
in B-DAT
Eq. O
3 O
should O
be O
rectified O

3.4 O
TSN O
in B-DAT
Practice O

other O
ConvNet O
architectures O
de- O
ployed O
in B-DAT
videos O
[1], O
[16], O
this O
architecture O

the O
potential O
of O
TSN O
framework O
in B-DAT
video O
classification O

work, O
we O
extend O
this O
approach O
in B-DAT
two O
aspects, O
namely O
accuracy O
and O

speed. O
As O
shown O
in B-DAT
Figure O
2, O
in O
addition O
to O

help O
to O
improve O
the O
accuracy O
in B-DAT
motion O
perception O
and O
thus O
boost O

16] O
and O
motion O
vector O
[17] O
in B-DAT
motion O
modeling, O
we O
revisit O
the O

work O
on O
dense O
optical O
flow O
in B-DAT
[60], O
the O
partial O
derivatives O
of O

to O
time O
play O
critical O
roles O
in B-DAT
computing O
optical O
flow. O
It O
is O

the O
power O
of O
optical O
flow O
in B-DAT
representing O
motion O
could O
be O
learned O

for O
action O
recognition O
are O
limited O
in B-DAT
terms O
of O
sizes. O
In O
practice O

strategies O
to O
improve O
the O
training B-DAT
in O
the O
temporal O
segment O
network O
framework O

networks O
take O
RGB O
images O
as O
in B-DAT

models O
across O
the O
RGB O
channels O
in B-DAT
the O
first O
layer O
and O
replicate O

increases B-DAT
the O
risk O
of O
over-fitting O
in O
the O
transfer O
learning O
process, O
due O

number O
of O
training B-DAT
sam- O
ples O
in O
target O
dataset. O
Therefore, O
after O
initialization O

dropout O
ratio O
(set O
as O
0.8 O
in B-DAT
experiment) O
after O
the O
global O
pooling O

scale O
jittering B-DAT
technique O
[8] O
used O
in O
ImageNet O
classification O
to O
action O
recognition O

this O
framework O
to O
recognize O
actions O
in B-DAT
realistic O
videos. O
In O
this O
section O

devise O
a O
series O
of O
techniques O
in B-DAT
order O
to O
improve O
the O
robustness O

4.1 O
Action O
Recognition O
in B-DAT
Trimmed O
Video O

share O
the O
model O
pa- O
rameters O
in B-DAT
temporal O
segment O
networks, O
the O
learned O

determined B-DAT
empirically. O
It O
is O
described O
in O
Sec. O
3.2 O
that O
the O
segmental O

normalization. O
To O
test O
the O
models O
in B-DAT
compliance O
with O
their O
training, O
we O

4.2 O
Action O
Recognition O
in B-DAT
Untrimmed O
Videos O

major O
obstacle O
for O
action O
recognition O
in B-DAT
untrimmed O
videos O
is O
the O
large O

portion O
of O
irrelevant O
content O
in B-DAT
the O
input O
videos. O
Since O
our O

averaging B-DAT
scores O
from O
every O
location O
in O
a O
video, O
has O
a O
high O

risk O
of O
factoring B-DAT
in O
the O
unpredictable O
responses O
of O
the O

Background O
issue: O
the O
irrelevant O
content O
in B-DAT
a O
video O
can O
have O
high O

snippets O
from O
the O
input B-DAT
videos O
in O
a O
fixed O
sampling O
rate O
(e.g O

on O
these O
sampled O
snippets. O
Then, O
in B-DAT
order O
to O
cover O
the O
highly O

Formally, O
for O
a O
video O
in B-DAT
length O
of O
M O
seconds, O
we O

the O
results O
of O
our O
approach O
in B-DAT
the O
ActivityNet O
challenge O
2016 O
and O

not O
specifically O
noted, O
the O
experiments O
in B-DAT
the O
section O
are O
conducted O
with O

set O
a O
smaller O
learning B-DAT
rate O
in O
our O
experiments. O
On O
the O
dataset O

and O
scale O
jittering, B-DAT
as O
specified O
in O
Section O
3.4. O
For O
the O
extraction O

of O
the O
good O
practices O
described O
in B-DAT
Sec. O
3.4, O
including O
the O
train O

propose O
two O
new O
training B-DAT
strategies O
in O
Section O
3.4, O
namely O
cross O
modality O

only O
pre-train B-DAT
spatial O
stream O
as O
in O
[1]; O
(3) O
with O
cross O
modality O

dropout. O
The O
results O
are O
summarized O
in B-DAT
Table O
1. O
First, O
we O
see O

nition O
performance O
to O
92.0%. O
Therefore, O
in B-DAT
the O
remaining O
experiments, O
we O
employ O

two O
new O
types O
of O
modalities O
in B-DAT
Section O
3.4: O
RGB O
difference O
and O

modalities O
and O
report O
the O
results O
in B-DAT
Table O
2. O
These O
experiments O
are O

all O
the O
good O
practices O
verified O
in B-DAT
Table O
1 O

which O
is O
the O
basic O
combination B-DAT
in O
the O
two-stream O
ConvNets O
also O
works O

action O
recognition O
methods O
as O
shown O
in B-DAT
Table O
2. O
This O
suggests O
that O

segment O
network O
framework. O
As O
described O
in B-DAT
Sec O
3, O
the O
framework O
has O

the O
temporal O
segment O
network O
framework O
in B-DAT

flow O
fields O
for O
input B-DAT
modalities O
in O
this O
exploration. O
Finally, O
to O
demonstrate O

the O
importance O
of O
TSN O
in B-DAT
long-range O
modeling O

of O
different O
segment O
numbers O
K O
in B-DAT
temporal O
segment O
networks O
on O
the O

use O
the O
average O
consensus O
function O
in B-DAT
these O
experiments. O
Dataset O
UCF101 O
ActivityNet O

segment O
number O
K O
as O
7 O
in B-DAT
these O
experiments. O
Dataset O
UCF101 O
ActivityNet O

the O
sparse O
snippet O
sampling B-DAT
scheme O
in O
TSN O
is O
the O
number O
of O

approaches. O
The O
results O
are O
summarized O
in B-DAT
Table O
3. O
We O
observe O
that O

we O
set O
K O
= O
7 O
in B-DAT
the O
following O
experiments O

The O
experimental O
results O
are O
summarized O
in B-DAT
Table O
4. O
On O
UCF101, O
which O

pooling B-DAT
for O
complex O
videos O
(ActivityNet) O
in O
later O
experiments O

and O
the O
results O
are O
summarized O
in B-DAT
Table O
5. O
We O
use O
K O

1 O
in B-DAT
these O
experiments, O
which O
is O
equivalent O

TABLE O
7 O
Winning B-DAT
entries O
in O
the O
untrimmed O
video O
classification O
task O

We O
present O
the O
recognition O
accuracies O
in B-DAT
the O
form O
of O
mAP O
values O

values O
in B-DAT
the O
challenge. O
Team O
mAP O
Top1 O

the O
effect O
of O
the O
components O
in B-DAT
temporal O
segment O
networks O
and O
coming O

UCF101. O
The O
results O
are O
summarized O
in B-DAT
the O
left O
columns O
of O
Table O

v1.2. O
The O
results O
are O
summarized O
in B-DAT
the O
right O
columns O
of O
Table O

7 O
and O
the O
aggregation O
function O
in B-DAT
TSN O
is O
top-K O
pooling. O
Our O

with O
TSN O
also O
perform O
well O
in B-DAT
untrimmed O
videos, O
given O
a O
reasonable O

testing B-DAT
scheme, O
as O
described O
in O
Sec. O
4.2 O

network O
framework O
is O
further O
verified O
in B-DAT
the O
ActivityNet O
large O
scale O
activity O

we O
follow O
the O
approach O
described O
in B-DAT
Sec. O
4.2. O
Understanding O
that O
the O

architecture O
plays O
an O
important O
role O
in B-DAT
boosting O
the O
performance, O
we O
also O

and O
test O
the O
recognition O
accuracy O
in B-DAT
terms O
of O
mean O
average O
precision O

on O
validation O
set O
are O
summarized O
in B-DAT
Table O
6. O
We O
observe O
that O

the O
testing B-DAT
set O
are O
summarized O
in O
Table O
7. O
Out O
entry O
“CES O

other O
participants O
of O
this O
challenge O
in B-DAT
Table O
7. O
It O
is O
worth O

efficiency O
of O
TSN, O
our O
models O
in B-DAT
the O
challenge O
can O
be O
trained O

time O
visualize O
interesting B-DAT
class O
information O
in O
action O
recognition O
ConvNet O
models. O
We O

left) O
and O
y O
(right) O
directions O
in B-DAT
gray-scales. O
Note O
all O
these O
images O

purely O
random O
pixels. O
Left: O
classes O
in B-DAT
UCF101. O
Right: O
classes O
in O
ActivityNet O

ing. B-DAT
The O
results O
are O
shown O
in O
Fig. O
3. O
For O
both O
RGB O

the O
scenery O
patterns O
and O
objects O
in B-DAT
the O
videos O
as O
significant O
evidences O

for O
action O
recognition. O
For O
example, O
in B-DAT
the O
class O
“Diving”, O
the O
single-frame O

models O
focus O
more O
on O
humans O
in B-DAT
the O
videos, O
and O
seem O
to O

different O
poses O
can O
be O
identified O
in B-DAT
the O
image, O
depicting O
various O
stages O

Similar O
observation O
would O
be O
identified O
in B-DAT
other O
action O
classes O
such O
as O

better, O
which O
is O
well O
reflected O
in B-DAT
our O
quantitative O
experiments O

good O
practices O
that O
we O
explored O
in B-DAT
this O
work. O
The O
former O
provides O

works O
for O
action O
recognition O
in B-DAT
videos,” O
in O
NIPS, O
2014, O
pp O

trajectories,” O
in B-DAT
ICCV, O
2013, O
pp. O
3551–3558. O
[3 O

human O
motion O
recognition,” O
in B-DAT
CVPR, O
2013, O
pp. O
2674–2681. O
[4 O

net- O
works O
for O
video O
classification,” O
in B-DAT
CVPR, O
2015, O
pp. O
4694–4702 O

with O
trajectory- O
pooled O
deep-convolutional O
descriptors,” O
in B-DAT
CVPR, O
2015, O
pp. O
4305– O
4314 O

with O
deep O
convolutional O
neural O
networks,” O
in B-DAT
NIPS, O
2012, O
pp. O
1106–1114 O

works O
for O
large-scale O
image O
recognition,” O
in B-DAT
ICLR, O
2015, O
pp. O
1–14 O

Rabinovich, B-DAT
“Going O
deeper O
with O
convolutions,” O
in O
CVPR, O
2015, O
pp. O
1–9 O

scene O
recognition O
using B-DAT
places O
database,” O
in O
NIPS, O
2014, O
pp. O
487–495 O

of O
deep O
convolutional O
neural O
networks,” O
in B-DAT
ECCV, O
2016, O
pp. O
467–482 O

images O
by O
fusing B-DAT
deep O
channels,” O
in O
CVPR, O
2015, O
pp. O
1600–1609 O

neural O
networks O
for O
event O
recognition O
in B-DAT
still O
images,” O
CoRR, O
vol. O
abs/1609.00162 O

classification O
with O
convolutional O
neural O
networks,” O
in B-DAT
CVPR, O
2014, O
pp. O
1725–1732 O

with O
3d O
convolutional O
net- O
works,” O
in B-DAT
ICCV, O
2015, O
pp. O
4489–4497 O

with O
enhanced O
motion O
vector O
CNNs,” O
in B-DAT
CVPR, O
2016, O
pp. O
2718–2726 O

A O
large-scale O
hierarchical O
image O
database,” O
in B-DAT
CVPR, O
2009, O
pp. O
248– O
255 O

motion O
segments O
for O
activity O
classification,” O
in B-DAT
ECCV, O
2010, O
pp. O
392–405 O

video O
evolution O
for O
action O
recognition,” O
in B-DAT
CVPR, O
2015, O
pp. O
5378–5387 O

for O
visual O
recognition O
and O
description,” O
in B-DAT
CVPR, O
2015, O
pp. O
2625–2634 O

action O
recog- O
nition O
for O
videos O
in B-DAT
the O
wild,” O
Computer O
Vision O
and O

for O
human O
activity O
under- O
standing,” B-DAT
in O
CVPR, O
2015, O
pp. O
961–970 O

human O
actions O
classes O
from O
videos O
in B-DAT
the O
wild,” O
CoRR, O
vol. O
abs/1212.0402 O

database O
for O
human O
motion O
recognition,” O
in B-DAT
ICCV, O
2011, O
pp. O
2556–2563 O

residual O
learning B-DAT
for O
image O
recognition,” O
in O
CVPR, O
2016, O
pp. O
770–778 O

inception B-DAT
architecture O
for O
computer O
vision,” O
in O
CVPR, O
2016, O
pp. O
2818–2826 O

practices O
for O
deep O
action O
recognition,” O
in B-DAT
ECCV, O
2016, O
pp. O
20–36 O

motion O
synthesis,” O
Foundations O
and O
Trends O
in B-DAT
Computer O
Graphics O
and O
Vision, O
vol O

scale-invariant B-DAT
spatio-temporal O
interest O
point O
detector,” O
in O
ECCV, O
2008, O
pp. O
650–663 O

nition O
via O
sparse O
spatio-temporal O
features,” O
in B-DAT
IEEE O
International O
Workshop O
on O
PETS O

Action O
recognition O
by O
dense O
trajectories,” O
in B-DAT
CVPR, O
2011, O
pp. O
3169–3176 O

realistic O
human O
actions O
from O
movies,” O
in B-DAT
CVPR, O
2008, O
pp. O
1–8 O

spatio-temporal O
descriptor O
based O
on O
3D-gradients,” O
in B-DAT
BMVC, O
2008, O
pp. O
1–12 O

categorization O
with O
bags O
of O
keypoints,” B-DAT
in O
ECCV O
Workshop O
on O
statistical O
learning O

in B-DAT
computer O
vision, O
2004, O
pp. O
1–22 O

super O
vector O
for O
action O
recognition,” O
in B-DAT
CVPR, O
2014, O
pp. O
596–603 O

parts O
from O
mid-level O
video O
representations,” O
in B-DAT
CVPR, O
2012, O
pp. O
1242–1249 O

videos O
using B-DAT
mid-level O
discriminative O
patches,” O
in O
CVPR, O
2013, O
pp. O
2571–2578 O

for O
detailed O
action O
under- O
standing,” B-DAT
in O
ICCV, O
2013, O
pp. O
2248–2255 O

Tu, O
“Action O
recognition O
with O
actons,” O
in B-DAT
ICCV, O
2013, O
pp. O
3559–3566 O

high-level O
represen- O
tation O
of O
activity O
in B-DAT
video,” O
in O
CVPR, O
2012, O
pp O

using B-DAT
3d O
human O
pose O
annotations,” O
in O
ICCV, O
2009, O
pp. O
1365–1372 O

using B-DAT
factorized O
spatio-temporal O
convolutional O
networks,” O
in O
ICCV, O
2015, O
pp. O
4597–4605 O

Xue, O
“Modeling B-DAT
spatial- O
temporal O
clues O
in O
a O
hybrid O
deep O
learning O
framework O

for O
video O
classification,” O
in B-DAT
ACM O
Multimedia, O
2015, O
pp. O
461–470 O

fusion O
for O
video O
action O
recognition,” O
in B-DAT
CVPR, O
2016, O
pp. O
1933–1941 O

of O
actions O
with O
segmental O
grammars,” O
in B-DAT
CVPR, O
2014, O
pp. O
612–619 O

action O
detection O
with O
relational O
dynamic-poselets,” O
in B-DAT
ECCV, O
2014, O
pp. O
565–580 O

by O
reducing B-DAT
internal O
covariate O
shift,” O
in O
ICML, O
2015, O
pp. O
448–456 O

for O
realtime O
tv-L1 O
optical O
flow,” O
in B-DAT
29th O
DAGM O
Symposium O
on O
Pattern O

Zisserman, O
“Return O
of O
the O
devil O
in B-DAT
the O
details: O
Delving O
deep O
into O

convolutional O
nets,” O
in B-DAT
BMVC, O
2014 O

networks O
for O
video O
action O
recognition,” O
in B-DAT
NIPS, O
2016, O
pp. O
3468–3476 O

for O
the O
thu- O
mos O
workshop,” O
in B-DAT
ICCV O
Workshop O
on O
THUMOS O
Challenge O

about O
classifying B-DAT
and O
localizing O
actions?” O
in O
CVPR, O
2015, O
pp. O
46–55 O

depth O
for O
large-scale O
action O
recognition,” O
in B-DAT
ECCV. O
Springer, O
2016, O
pp. O
668–684 O

recognition O
via O
trajectory O
group O
selection,” O
in B-DAT
CVPR, O
2015, O
pp. O
3698–3706 O

deep O
framework O
for O
action O
recognition,” O
in B-DAT
CVPR, O
2016, O
pp. O
1991– O
1999 O

3.4 O
TSN O
in B-DAT
Practice O

4.1 O
Action O
Recognition O
in B-DAT
Trimmed O
Video O

4.2 O
Action O
Recognition O
in B-DAT
Untrimmed O
Videos O

Segment O
Networks O
for O
Action O
Recognition O
in B-DAT
Videos O

recognition. O
However, O
for O
action O
recognition O
in B-DAT
videos, O
their O
advantage O
over O
traditional O

framework O
for O
learning B-DAT
action O
models O
in O
videos. O
This O
method, O
called O
temporal O

easily O
adapted O
for O
action O
recognition O
in B-DAT
both O
trimmed O
and O
untrimmed O
videos O

5], O
owing B-DAT
to O
its O
applications O
in O
many O
areas O
like O
security O
and O

behavior O
analysis. O
For O
action O
recognition O
in B-DAT
videos, O
there O
are O
two O
crucial O

6] O
have O
achieved O
great O
success O
in B-DAT
classifying O
images O
of O
objects O
[7 O

representations O
from O
raw O
visual O
data O
in B-DAT
large- O
scale O
supervised O
datasets O
(e.g O

of O
ConvNets O
to O
action O
recognition O
in B-DAT
unconstrained O
videos O
is O
impeded O
by O

for O
understanding B-DAT
the O
dy- O
namics O
in O
traditional O
methods O
[19], O
[20], O
[21 O

considered O
as O
a O
critical O
factor O
in B-DAT
deep O
ConvNet O
frameworks O
[1], O
[15 O

deploy O
the O
learned O
action O
models O
in B-DAT
a O
realistic O
setting O
we O
often O

action O
models O
to O
action O
recognition O
in B-DAT
untrimmed O
videos O

remain B-DAT
limited O
in O
both O
size O
and O
diversity, O
making O

the O
action O
recogni- O
tion O
problem O
in B-DAT
this O
paper O
from O
the O
following O

framework O
for O
learning B-DAT
action O
models O
in O
videos. O
It O
is O
based O
on O

be O
more O
favorable O
and O
efficient O
in B-DAT
this O
case. O
The O
TSN O
framework O

structures O
over O
the O
whole O
video, O
in B-DAT
a O
way O
that O
its O
computational O

To O
tackle O
the O
practical O
difficulties O
in B-DAT
learning O
and O
apply O

to O
perform O
Batch O
Normalization O
(BN) O
in B-DAT
a O
fine-tuning O
scenario, O
denoted O
as O

which O
has O
numerous O
potential O
applications O
in B-DAT
real-world O
problems O

our O
method O
for O
action O
recognition O
in B-DAT
both O
trimmed O
and O
untrimmed O
videos O

method O
secures O
the O
1st O
place O
in B-DAT
untrimmed O
video O
classification O
at O
the O

different O
aspects O
of O
the O
problems O
in B-DAT
efficiently O
and O
effectively O
learning O
and O

extends O
our O
previous O
work O
[31] O
in B-DAT
a O
number O
of O
aspects. O
First O

Challenge O
2016, O
which O
ranks O
#1 O
in B-DAT
untrimmed O
video O
classification O
among O
24 O

recognition O
has O
been O
studied O
extensively O
in B-DAT
recent O
years O
and O
readers O
can O

For O
action O
recognition O
in B-DAT
videos, O
the O
visual O
representation O
plays O

a O
set O
of O
mid-level O
patches O
in B-DAT
a O
strongly-supervised O
manner. O
Similar O
to O

ConvNet O
architectures O
for O
action O
recognition O
in B-DAT
videos O
[1], O
[4], O
[5], O
[15 O

use O
recurrent O
neural O
networks O
(RNN), O
in B-DAT
particular O
LSTM, O
to O
model O
the O

frame O
features O
for O
action O
recognition O
in B-DAT
videos O

to O
learn O
the O
model O
parameters O
in B-DAT
an O
iterative O
approach. O
Wang O
et O

As O
discussed O
in B-DAT
Sec. O
1, O
long-range O
temporal O
modeling O

is O
important O
for O
action O
understanding B-DAT
in O
videos. O
The O
existing O
deep O
architectures O

1], O
[16], O
it O
still O
suffers O
in B-DAT
both O
computational O
and O
modeling O
aspects O

it O
totally O
samples O
64 O
frames O
in B-DAT
the O
work O
of O
[23] O
and O

the O
frames O
are O
densely O
recorded O
in B-DAT
the O
videos, O
the O
content O
changes O

to O
model O
the O
temporal O
structures O
in B-DAT
a O
human O
action. O
Normally, O
the O

a O
video-level O
framework O
as O
shown O
in B-DAT
Figure O
1, O
which O
would O
be O

explained B-DAT
in O
the O
next O
subsection O

its O
corresponding B-DAT
segment. O
Each O
snippet O
in O
this O
sequence O
produces O
its O
own O

details O
on O
these O
consensus O
functions O
in B-DAT
the O
next O
subsection O

K O
is O
number O
of O
segments O
in B-DAT
temporal O
segment O
net- O
work. O
When O

function O
is O
an O
important O
component O
in B-DAT
our O
temporal O
segment O
net- O
work O

video. O
On O
the O
other O
hand, O
in B-DAT
particular O
for O
noisy O
videos O
with O

phases O
may O
play O
different O
roles O
in B-DAT
recogniz- O
ing O
action O
classes. O
This O

the O
basic O
back- O
propagation O
formula O
in B-DAT
Eq. O
3 O
should O
be O
rectified O

3.4 O
TSN O
in B-DAT
Practice O

other O
ConvNet O
architectures O
de- O
ployed O
in B-DAT
videos O
[1], O
[16], O
this O
architecture O

the O
potential O
of O
TSN O
framework O
in B-DAT
video O
classification O

work, O
we O
extend O
this O
approach O
in B-DAT
two O
aspects, O
namely O
accuracy O
and O

speed. O
As O
shown O
in B-DAT
Figure O
2, O
in O
addition O
to O

help O
to O
improve O
the O
accuracy O
in B-DAT
motion O
perception O
and O
thus O
boost O

16] O
and O
motion O
vector O
[17] O
in B-DAT
motion O
modeling, O
we O
revisit O
the O

work O
on O
dense O
optical O
flow O
in B-DAT
[60], O
the O
partial O
derivatives O
of O

to O
time O
play O
critical O
roles O
in B-DAT
computing O
optical O
flow. O
It O
is O

the O
power O
of O
optical O
flow O
in B-DAT
representing O
motion O
could O
be O
learned O

for O
action O
recognition O
are O
limited O
in B-DAT
terms O
of O
sizes. O
In O
practice O

strategies O
to O
improve O
the O
training B-DAT
in O
the O
temporal O
segment O
network O
framework O

networks O
take O
RGB O
images O
as O
in B-DAT

models O
across O
the O
RGB O
channels O
in B-DAT
the O
first O
layer O
and O
replicate O

increases B-DAT
the O
risk O
of O
over-fitting O
in O
the O
transfer O
learning O
process, O
due O

number O
of O
training B-DAT
sam- O
ples O
in O
target O
dataset. O
Therefore, O
after O
initialization O

dropout O
ratio O
(set O
as O
0.8 O
in B-DAT
experiment) O
after O
the O
global O
pooling O

scale O
jittering B-DAT
technique O
[8] O
used O
in O
ImageNet O
classification O
to O
action O
recognition O

this O
framework O
to O
recognize O
actions O
in B-DAT
realistic O
videos. O
In O
this O
section O

devise O
a O
series O
of O
techniques O
in B-DAT
order O
to O
improve O
the O
robustness O

4.1 O
Action O
Recognition O
in B-DAT
Trimmed O
Video O

share O
the O
model O
pa- O
rameters O
in B-DAT
temporal O
segment O
networks, O
the O
learned O

determined B-DAT
empirically. O
It O
is O
described O
in O
Sec. O
3.2 O
that O
the O
segmental O

normalization. O
To O
test O
the O
models O
in B-DAT
compliance O
with O
their O
training, O
we O

4.2 O
Action O
Recognition O
in B-DAT
Untrimmed O
Videos O

major O
obstacle O
for O
action O
recognition O
in B-DAT
untrimmed O
videos O
is O
the O
large O

portion O
of O
irrelevant O
content O
in B-DAT
the O
input O
videos. O
Since O
our O

averaging B-DAT
scores O
from O
every O
location O
in O
a O
video, O
has O
a O
high O

risk O
of O
factoring B-DAT
in O
the O
unpredictable O
responses O
of O
the O

Background O
issue: O
the O
irrelevant O
content O
in B-DAT
a O
video O
can O
have O
high O

snippets O
from O
the O
input B-DAT
videos O
in O
a O
fixed O
sampling O
rate O
(e.g O

on O
these O
sampled O
snippets. O
Then, O
in B-DAT
order O
to O
cover O
the O
highly O

Formally, O
for O
a O
video O
in B-DAT
length O
of O
M O
seconds, O
we O

the O
results O
of O
our O
approach O
in B-DAT
the O
ActivityNet O
challenge O
2016 O
and O

not O
specifically O
noted, O
the O
experiments O
in B-DAT
the O
section O
are O
conducted O
with O

set O
a O
smaller O
learning B-DAT
rate O
in O
our O
experiments. O
On O
the O
dataset O

and O
scale O
jittering, B-DAT
as O
specified O
in O
Section O
3.4. O
For O
the O
extraction O

of O
the O
good O
practices O
described O
in B-DAT
Sec. O
3.4, O
including O
the O
train O

propose O
two O
new O
training B-DAT
strategies O
in O
Section O
3.4, O
namely O
cross O
modality O

only O
pre-train B-DAT
spatial O
stream O
as O
in O
[1]; O
(3) O
with O
cross O
modality O

dropout. O
The O
results O
are O
summarized O
in B-DAT
Table O
1. O
First, O
we O
see O

nition O
performance O
to O
92.0%. O
Therefore, O
in B-DAT
the O
remaining O
experiments, O
we O
employ O

two O
new O
types O
of O
modalities O
in B-DAT
Section O
3.4: O
RGB O
difference O
and O

modalities O
and O
report O
the O
results O
in B-DAT
Table O
2. O
These O
experiments O
are O

all O
the O
good O
practices O
verified O
in B-DAT
Table O
1 O

which O
is O
the O
basic O
combination B-DAT
in O
the O
two-stream O
ConvNets O
also O
works O

action O
recognition O
methods O
as O
shown O
in B-DAT
Table O
2. O
This O
suggests O
that O

segment O
network O
framework. O
As O
described O
in B-DAT
Sec O
3, O
the O
framework O
has O

the O
temporal O
segment O
network O
framework O
in B-DAT

flow O
fields O
for O
input B-DAT
modalities O
in O
this O
exploration. O
Finally, O
to O
demonstrate O

the O
importance O
of O
TSN O
in B-DAT
long-range O
modeling O

of O
different O
segment O
numbers O
K O
in B-DAT
temporal O
segment O
networks O
on O
the O

use O
the O
average O
consensus O
function O
in B-DAT
these O
experiments. O
Dataset O
UCF101 O
ActivityNet O

segment O
number O
K O
as O
7 O
in B-DAT
these O
experiments. O
Dataset O
UCF101 O
ActivityNet O

the O
sparse O
snippet O
sampling B-DAT
scheme O
in O
TSN O
is O
the O
number O
of O

approaches. O
The O
results O
are O
summarized O
in B-DAT
Table O
3. O
We O
observe O
that O

we O
set O
K O
= O
7 O
in B-DAT
the O
following O
experiments O

The O
experimental O
results O
are O
summarized O
in B-DAT
Table O
4. O
On O
UCF101, O
which O

pooling B-DAT
for O
complex O
videos O
(ActivityNet) O
in O
later O
experiments O

and O
the O
results O
are O
summarized O
in B-DAT
Table O
5. O
We O
use O
K O

1 O
in B-DAT
these O
experiments, O
which O
is O
equivalent O

TABLE O
7 O
Winning B-DAT
entries O
in O
the O
untrimmed O
video O
classification O
task O

We O
present O
the O
recognition O
accuracies O
in B-DAT
the O
form O
of O
mAP O
values O

values O
in B-DAT
the O
challenge. O
Team O
mAP O
Top1 O

the O
effect O
of O
the O
components O
in B-DAT
temporal O
segment O
networks O
and O
coming O

UCF101. O
The O
results O
are O
summarized O
in B-DAT
the O
left O
columns O
of O
Table O

v1.2. O
The O
results O
are O
summarized O
in B-DAT
the O
right O
columns O
of O
Table O

7 O
and O
the O
aggregation O
function O
in B-DAT
TSN O
is O
top-K O
pooling. O
Our O

with O
TSN O
also O
perform O
well O
in B-DAT
untrimmed O
videos, O
given O
a O
reasonable O

testing B-DAT
scheme, O
as O
described O
in O
Sec. O
4.2 O

network O
framework O
is O
further O
verified O
in B-DAT
the O
ActivityNet O
large O
scale O
activity O

we O
follow O
the O
approach O
described O
in B-DAT
Sec. O
4.2. O
Understanding O
that O
the O

architecture O
plays O
an O
important O
role O
in B-DAT
boosting O
the O
performance, O
we O
also O

and O
test O
the O
recognition O
accuracy O
in B-DAT
terms O
of O
mean O
average O
precision O

on O
validation O
set O
are O
summarized O
in B-DAT
Table O
6. O
We O
observe O
that O

the O
testing B-DAT
set O
are O
summarized O
in O
Table O
7. O
Out O
entry O
“CES O

other O
participants O
of O
this O
challenge O
in B-DAT
Table O
7. O
It O
is O
worth O

efficiency O
of O
TSN, O
our O
models O
in B-DAT
the O
challenge O
can O
be O
trained O

time O
visualize O
interesting B-DAT
class O
information O
in O
action O
recognition O
ConvNet O
models. O
We O

left) O
and O
y O
(right) O
directions O
in B-DAT
gray-scales. O
Note O
all O
these O
images O

purely O
random O
pixels. O
Left: O
classes O
in B-DAT
UCF101. O
Right: O
classes O
in O
ActivityNet O

ing. B-DAT
The O
results O
are O
shown O
in O
Fig. O
3. O
For O
both O
RGB O

the O
scenery O
patterns O
and O
objects O
in B-DAT
the O
videos O
as O
significant O
evidences O

for O
action O
recognition. O
For O
example, O
in B-DAT
the O
class O
“Diving”, O
the O
single-frame O

models O
focus O
more O
on O
humans O
in B-DAT
the O
videos, O
and O
seem O
to O

different O
poses O
can O
be O
identified O
in B-DAT
the O
image, O
depicting O
various O
stages O

Similar O
observation O
would O
be O
identified O
in B-DAT
other O
action O
classes O
such O
as O

better, O
which O
is O
well O
reflected O
in B-DAT
our O
quantitative O
experiments O

good O
practices O
that O
we O
explored O
in B-DAT
this O
work. O
The O
former O
provides O

works O
for O
action O
recognition O
in B-DAT
videos,” O
in O
NIPS, O
2014, O
pp O

trajectories,” O
in B-DAT
ICCV, O
2013, O
pp. O
3551–3558. O
[3 O

human O
motion O
recognition,” O
in B-DAT
CVPR, O
2013, O
pp. O
2674–2681. O
[4 O

net- O
works O
for O
video O
classification,” O
in B-DAT
CVPR, O
2015, O
pp. O
4694–4702 O

with O
trajectory- O
pooled O
deep-convolutional O
descriptors,” O
in B-DAT
CVPR, O
2015, O
pp. O
4305– O
4314 O

with O
deep O
convolutional O
neural O
networks,” O
in B-DAT
NIPS, O
2012, O
pp. O
1106–1114 O

works O
for O
large-scale O
image O
recognition,” O
in B-DAT
ICLR, O
2015, O
pp. O
1–14 O

Rabinovich, B-DAT
“Going O
deeper O
with O
convolutions,” O
in O
CVPR, O
2015, O
pp. O
1–9 O

scene O
recognition O
using B-DAT
places O
database,” O
in O
NIPS, O
2014, O
pp. O
487–495 O

of O
deep O
convolutional O
neural O
networks,” O
in B-DAT
ECCV, O
2016, O
pp. O
467–482 O

images O
by O
fusing B-DAT
deep O
channels,” O
in O
CVPR, O
2015, O
pp. O
1600–1609 O

neural O
networks O
for O
event O
recognition O
in B-DAT
still O
images,” O
CoRR, O
vol. O
abs/1609.00162 O

classification O
with O
convolutional O
neural O
networks,” O
in B-DAT
CVPR, O
2014, O
pp. O
1725–1732 O

with O
3d O
convolutional O
net- O
works,” O
in B-DAT
ICCV, O
2015, O
pp. O
4489–4497 O

with O
enhanced O
motion O
vector O
CNNs,” O
in B-DAT
CVPR, O
2016, O
pp. O
2718–2726 O

A O
large-scale O
hierarchical O
image O
database,” O
in B-DAT
CVPR, O
2009, O
pp. O
248– O
255 O

motion O
segments O
for O
activity O
classification,” O
in B-DAT
ECCV, O
2010, O
pp. O
392–405 O

video O
evolution O
for O
action O
recognition,” O
in B-DAT
CVPR, O
2015, O
pp. O
5378–5387 O

for O
visual O
recognition O
and O
description,” O
in B-DAT
CVPR, O
2015, O
pp. O
2625–2634 O

action O
recog- O
nition O
for O
videos O
in B-DAT
the O
wild,” O
Computer O
Vision O
and O

for O
human O
activity O
under- O
standing,” B-DAT
in O
CVPR, O
2015, O
pp. O
961–970 O

human O
actions O
classes O
from O
videos O
in B-DAT
the O
wild,” O
CoRR, O
vol. O
abs/1212.0402 O

database O
for O
human O
motion O
recognition,” O
in B-DAT
ICCV, O
2011, O
pp. O
2556–2563 O

residual O
learning B-DAT
for O
image O
recognition,” O
in O
CVPR, O
2016, O
pp. O
770–778 O

inception B-DAT
architecture O
for O
computer O
vision,” O
in O
CVPR, O
2016, O
pp. O
2818–2826 O

practices O
for O
deep O
action O
recognition,” O
in B-DAT
ECCV, O
2016, O
pp. O
20–36 O

motion O
synthesis,” O
Foundations O
and O
Trends O
in B-DAT
Computer O
Graphics O
and O
Vision, O
vol O

scale-invariant B-DAT
spatio-temporal O
interest O
point O
detector,” O
in O
ECCV, O
2008, O
pp. O
650–663 O

nition O
via O
sparse O
spatio-temporal O
features,” O
in B-DAT
IEEE O
International O
Workshop O
on O
PETS O

Action O
recognition O
by O
dense O
trajectories,” O
in B-DAT
CVPR, O
2011, O
pp. O
3169–3176 O

realistic O
human O
actions O
from O
movies,” O
in B-DAT
CVPR, O
2008, O
pp. O
1–8 O

spatio-temporal O
descriptor O
based O
on O
3D-gradients,” O
in B-DAT
BMVC, O
2008, O
pp. O
1–12 O

categorization O
with O
bags O
of O
keypoints,” B-DAT
in O
ECCV O
Workshop O
on O
statistical O
learning O

in B-DAT
computer O
vision, O
2004, O
pp. O
1–22 O

super O
vector O
for O
action O
recognition,” O
in B-DAT
CVPR, O
2014, O
pp. O
596–603 O

parts O
from O
mid-level O
video O
representations,” O
in B-DAT
CVPR, O
2012, O
pp. O
1242–1249 O

videos O
using B-DAT
mid-level O
discriminative O
patches,” O
in O
CVPR, O
2013, O
pp. O
2571–2578 O

for O
detailed O
action O
under- O
standing,” B-DAT
in O
ICCV, O
2013, O
pp. O
2248–2255 O

Tu, O
“Action O
recognition O
with O
actons,” O
in B-DAT
ICCV, O
2013, O
pp. O
3559–3566 O

high-level O
represen- O
tation O
of O
activity O
in B-DAT
video,” O
in O
CVPR, O
2012, O
pp O

using B-DAT
3d O
human O
pose O
annotations,” O
in O
ICCV, O
2009, O
pp. O
1365–1372 O

using B-DAT
factorized O
spatio-temporal O
convolutional O
networks,” O
in O
ICCV, O
2015, O
pp. O
4597–4605 O

Xue, O
“Modeling B-DAT
spatial- O
temporal O
clues O
in O
a O
hybrid O
deep O
learning O
framework O

for O
video O
classification,” O
in B-DAT
ACM O
Multimedia, O
2015, O
pp. O
461–470 O

fusion O
for O
video O
action O
recognition,” O
in B-DAT
CVPR, O
2016, O
pp. O
1933–1941 O

of O
actions O
with O
segmental O
grammars,” O
in B-DAT
CVPR, O
2014, O
pp. O
612–619 O

action O
detection O
with O
relational O
dynamic-poselets,” O
in B-DAT
ECCV, O
2014, O
pp. O
565–580 O

by O
reducing B-DAT
internal O
covariate O
shift,” O
in O
ICML, O
2015, O
pp. O
448–456 O

for O
realtime O
tv-L1 O
optical O
flow,” O
in B-DAT
29th O
DAGM O
Symposium O
on O
Pattern O

Zisserman, O
“Return O
of O
the O
devil O
in B-DAT
the O
details: O
Delving O
deep O
into O

convolutional O
nets,” O
in B-DAT
BMVC, O
2014 O

networks O
for O
video O
action O
recognition,” O
in B-DAT
NIPS, O
2016, O
pp. O
3468–3476 O

for O
the O
thu- O
mos O
workshop,” O
in B-DAT
ICCV O
Workshop O
on O
THUMOS O
Challenge O

about O
classifying B-DAT
and O
localizing O
actions?” O
in O
CVPR, O
2015, O
pp. O
46–55 O

depth O
for O
large-scale O
action O
recognition,” O
in B-DAT
ECCV. O
Springer, O
2016, O
pp. O
668–684 O

recognition O
via O
trajectory O
group O
selection,” O
in B-DAT
CVPR, O
2015, O
pp. O
3698–3706 O

deep O
framework O
for O
action O
recognition,” O
in B-DAT
CVPR, O
2016, O
pp. O
1991– O
1999 O

3.4 O
TSN O
in B-DAT
Practice O

4.1 O
Action O
Recognition O
in B-DAT
Trimmed O
Video O

4.2 O
Action O
Recognition O
in B-DAT
Untrimmed O
Videos O

Segment O
Networks O
for O
Action O
Recognition O
in B-DAT
Videos O

recognition. O
However, O
for O
action O
recognition O
in B-DAT
videos, O
their O
advantage O
over O
traditional O

framework O
for O
learning B-DAT
action O
models O
in O
videos. O
This O
method, O
called O
temporal O

easily O
adapted O
for O
action O
recognition O
in B-DAT
both O
trimmed O
and O
untrimmed O
videos O

5], O
owing B-DAT
to O
its O
applications O
in O
many O
areas O
like O
security O
and O

behavior O
analysis. O
For O
action O
recognition O
in B-DAT
videos, O
there O
are O
two O
crucial O

6] O
have O
achieved O
great O
success O
in B-DAT
classifying O
images O
of O
objects O
[7 O

representations O
from O
raw O
visual O
data O
in B-DAT
large- O
scale O
supervised O
datasets O
(e.g O

of O
ConvNets O
to O
action O
recognition O
in B-DAT
unconstrained O
videos O
is O
impeded O
by O

for O
understanding B-DAT
the O
dy- O
namics O
in O
traditional O
methods O
[19], O
[20], O
[21 O

considered O
as O
a O
critical O
factor O
in B-DAT
deep O
ConvNet O
frameworks O
[1], O
[15 O

deploy O
the O
learned O
action O
models O
in B-DAT
a O
realistic O
setting O
we O
often O

action O
models O
to O
action O
recognition O
in B-DAT
untrimmed O
videos O

remain B-DAT
limited O
in O
both O
size O
and O
diversity, O
making O

the O
action O
recogni- O
tion O
problem O
in B-DAT
this O
paper O
from O
the O
following O

framework O
for O
learning B-DAT
action O
models O
in O
videos. O
It O
is O
based O
on O

be O
more O
favorable O
and O
efficient O
in B-DAT
this O
case. O
The O
TSN O
framework O

structures O
over O
the O
whole O
video, O
in B-DAT
a O
way O
that O
its O
computational O

To O
tackle O
the O
practical O
difficulties O
in B-DAT
learning O
and O
apply O

to O
perform O
Batch O
Normalization O
(BN) O
in B-DAT
a O
fine-tuning O
scenario, O
denoted O
as O

which O
has O
numerous O
potential O
applications O
in B-DAT
real-world O
problems O

our O
method O
for O
action O
recognition O
in B-DAT
both O
trimmed O
and O
untrimmed O
videos O

method O
secures O
the O
1st O
place O
in B-DAT
untrimmed O
video O
classification O
at O
the O

different O
aspects O
of O
the O
problems O
in B-DAT
efficiently O
and O
effectively O
learning O
and O

extends O
our O
previous O
work O
[31] O
in B-DAT
a O
number O
of O
aspects. O
First O

Challenge O
2016, O
which O
ranks O
#1 O
in B-DAT
untrimmed O
video O
classification O
among O
24 O

recognition O
has O
been O
studied O
extensively O
in B-DAT
recent O
years O
and O
readers O
can O

For O
action O
recognition O
in B-DAT
videos, O
the O
visual O
representation O
plays O

a O
set O
of O
mid-level O
patches O
in B-DAT
a O
strongly-supervised O
manner. O
Similar O
to O

ConvNet O
architectures O
for O
action O
recognition O
in B-DAT
videos O
[1], O
[4], O
[5], O
[15 O

use O
recurrent O
neural O
networks O
(RNN), O
in B-DAT
particular O
LSTM, O
to O
model O
the O

frame O
features O
for O
action O
recognition O
in B-DAT
videos O

to O
learn O
the O
model O
parameters O
in B-DAT
an O
iterative O
approach. O
Wang O
et O

As O
discussed O
in B-DAT
Sec. O
1, O
long-range O
temporal O
modeling O

is O
important O
for O
action O
understanding B-DAT
in O
videos. O
The O
existing O
deep O
architectures O

1], O
[16], O
it O
still O
suffers O
in B-DAT
both O
computational O
and O
modeling O
aspects O

it O
totally O
samples O
64 O
frames O
in B-DAT
the O
work O
of O
[23] O
and O

the O
frames O
are O
densely O
recorded O
in B-DAT
the O
videos, O
the O
content O
changes O

to O
model O
the O
temporal O
structures O
in B-DAT
a O
human O
action. O
Normally, O
the O

a O
video-level O
framework O
as O
shown O
in B-DAT
Figure O
1, O
which O
would O
be O

explained B-DAT
in O
the O
next O
subsection O

its O
corresponding B-DAT
segment. O
Each O
snippet O
in O
this O
sequence O
produces O
its O
own O

details O
on O
these O
consensus O
functions O
in B-DAT
the O
next O
subsection O

K O
is O
number O
of O
segments O
in B-DAT
temporal O
segment O
net- O
work. O
When O

function O
is O
an O
important O
component O
in B-DAT
our O
temporal O
segment O
net- O
work O

video. O
On O
the O
other O
hand, O
in B-DAT
particular O
for O
noisy O
videos O
with O

phases O
may O
play O
different O
roles O
in B-DAT
recogniz- O
ing O
action O
classes. O
This O

the O
basic O
back- O
propagation O
formula O
in B-DAT
Eq. O
3 O
should O
be O
rectified O

3.4 O
TSN O
in B-DAT
Practice O

other O
ConvNet O
architectures O
de- O
ployed O
in B-DAT
videos O
[1], O
[16], O
this O
architecture O

the O
potential O
of O
TSN O
framework O
in B-DAT
video O
classification O

work, O
we O
extend O
this O
approach O
in B-DAT
two O
aspects, O
namely O
accuracy O
and O

speed. O
As O
shown O
in B-DAT
Figure O
2, O
in O
addition O
to O

help O
to O
improve O
the O
accuracy O
in B-DAT
motion O
perception O
and O
thus O
boost O

16] O
and O
motion O
vector O
[17] O
in B-DAT
motion O
modeling, O
we O
revisit O
the O

work O
on O
dense O
optical O
flow O
in B-DAT
[60], O
the O
partial O
derivatives O
of O

to O
time O
play O
critical O
roles O
in B-DAT
computing O
optical O
flow. O
It O
is O

the O
power O
of O
optical O
flow O
in B-DAT
representing O
motion O
could O
be O
learned O

for O
action O
recognition O
are O
limited O
in B-DAT
terms O
of O
sizes. O
In O
practice O

strategies O
to O
improve O
the O
training B-DAT
in O
the O
temporal O
segment O
network O
framework O

networks O
take O
RGB O
images O
as O
in B-DAT

models O
across O
the O
RGB O
channels O
in B-DAT
the O
first O
layer O
and O
replicate O

increases B-DAT
the O
risk O
of O
over-fitting O
in O
the O
transfer O
learning O
process, O
due O

number O
of O
training B-DAT
sam- O
ples O
in O
target O
dataset. O
Therefore, O
after O
initialization O

dropout O
ratio O
(set O
as O
0.8 O
in B-DAT
experiment) O
after O
the O
global O
pooling O

scale O
jittering B-DAT
technique O
[8] O
used O
in O
ImageNet O
classification O
to O
action O
recognition O

this O
framework O
to O
recognize O
actions O
in B-DAT
realistic O
videos. O
In O
this O
section O

devise O
a O
series O
of O
techniques O
in B-DAT
order O
to O
improve O
the O
robustness O

4.1 O
Action O
Recognition O
in B-DAT
Trimmed O
Video O

share O
the O
model O
pa- O
rameters O
in B-DAT
temporal O
segment O
networks, O
the O
learned O

determined B-DAT
empirically. O
It O
is O
described O
in O
Sec. O
3.2 O
that O
the O
segmental O

normalization. O
To O
test O
the O
models O
in B-DAT
compliance O
with O
their O
training, O
we O

4.2 O
Action O
Recognition O
in B-DAT
Untrimmed O
Videos O

major O
obstacle O
for O
action O
recognition O
in B-DAT
untrimmed O
videos O
is O
the O
large O

portion O
of O
irrelevant O
content O
in B-DAT
the O
input O
videos. O
Since O
our O

averaging B-DAT
scores O
from O
every O
location O
in O
a O
video, O
has O
a O
high O

risk O
of O
factoring B-DAT
in O
the O
unpredictable O
responses O
of O
the O

Background O
issue: O
the O
irrelevant O
content O
in B-DAT
a O
video O
can O
have O
high O

snippets O
from O
the O
input B-DAT
videos O
in O
a O
fixed O
sampling O
rate O
(e.g O

on O
these O
sampled O
snippets. O
Then, O
in B-DAT
order O
to O
cover O
the O
highly O

Formally, O
for O
a O
video O
in B-DAT
length O
of O
M O
seconds, O
we O

the O
results O
of O
our O
approach O
in B-DAT
the O
ActivityNet O
challenge O
2016 O
and O

not O
specifically O
noted, O
the O
experiments O
in B-DAT
the O
section O
are O
conducted O
with O

set O
a O
smaller O
learning B-DAT
rate O
in O
our O
experiments. O
On O
the O
dataset O

and O
scale O
jittering, B-DAT
as O
specified O
in O
Section O
3.4. O
For O
the O
extraction O

of O
the O
good O
practices O
described O
in B-DAT
Sec. O
3.4, O
including O
the O
train O

propose O
two O
new O
training B-DAT
strategies O
in O
Section O
3.4, O
namely O
cross O
modality O

only O
pre-train B-DAT
spatial O
stream O
as O
in O
[1]; O
(3) O
with O
cross O
modality O

dropout. O
The O
results O
are O
summarized O
in B-DAT
Table O
1. O
First, O
we O
see O

nition O
performance O
to O
92.0%. O
Therefore, O
in B-DAT
the O
remaining O
experiments, O
we O
employ O

two O
new O
types O
of O
modalities O
in B-DAT
Section O
3.4: O
RGB O
difference O
and O

modalities O
and O
report O
the O
results O
in B-DAT
Table O
2. O
These O
experiments O
are O

all O
the O
good O
practices O
verified O
in B-DAT
Table O
1 O

which O
is O
the O
basic O
combination B-DAT
in O
the O
two-stream O
ConvNets O
also O
works O

action O
recognition O
methods O
as O
shown O
in B-DAT
Table O
2. O
This O
suggests O
that O

segment O
network O
framework. O
As O
described O
in B-DAT
Sec O
3, O
the O
framework O
has O

the O
temporal O
segment O
network O
framework O
in B-DAT

flow O
fields O
for O
input B-DAT
modalities O
in O
this O
exploration. O
Finally, O
to O
demonstrate O

the O
importance O
of O
TSN O
in B-DAT
long-range O
modeling O

of O
different O
segment O
numbers O
K O
in B-DAT
temporal O
segment O
networks O
on O
the O

use O
the O
average O
consensus O
function O
in B-DAT
these O
experiments. O
Dataset O
UCF101 O
ActivityNet O

segment O
number O
K O
as O
7 O
in B-DAT
these O
experiments. O
Dataset O
UCF101 O
ActivityNet O

the O
sparse O
snippet O
sampling B-DAT
scheme O
in O
TSN O
is O
the O
number O
of O

approaches. O
The O
results O
are O
summarized O
in B-DAT
Table O
3. O
We O
observe O
that O

we O
set O
K O
= O
7 O
in B-DAT
the O
following O
experiments O

The O
experimental O
results O
are O
summarized O
in B-DAT
Table O
4. O
On O
UCF101, O
which O

pooling B-DAT
for O
complex O
videos O
(ActivityNet) O
in O
later O
experiments O

and O
the O
results O
are O
summarized O
in B-DAT
Table O
5. O
We O
use O
K O

1 O
in B-DAT
these O
experiments, O
which O
is O
equivalent O

TABLE O
7 O
Winning B-DAT
entries O
in O
the O
untrimmed O
video O
classification O
task O

We O
present O
the O
recognition O
accuracies O
in B-DAT
the O
form O
of O
mAP O
values O

values O
in B-DAT
the O
challenge. O
Team O
mAP O
Top1 O

the O
effect O
of O
the O
components O
in B-DAT
temporal O
segment O
networks O
and O
coming O

UCF101. O
The O
results O
are O
summarized O
in B-DAT
the O
left O
columns O
of O
Table O

v1.2. O
The O
results O
are O
summarized O
in B-DAT
the O
right O
columns O
of O
Table O

7 O
and O
the O
aggregation O
function O
in B-DAT
TSN O
is O
top-K O
pooling. O
Our O

with O
TSN O
also O
perform O
well O
in B-DAT
untrimmed O
videos, O
given O
a O
reasonable O

testing B-DAT
scheme, O
as O
described O
in O
Sec. O
4.2 O

network O
framework O
is O
further O
verified O
in B-DAT
the O
ActivityNet O
large O
scale O
activity O

we O
follow O
the O
approach O
described O
in B-DAT
Sec. O
4.2. O
Understanding O
that O
the O

architecture O
plays O
an O
important O
role O
in B-DAT
boosting O
the O
performance, O
we O
also O

and O
test O
the O
recognition O
accuracy O
in B-DAT
terms O
of O
mean O
average O
precision O

on O
validation O
set O
are O
summarized O
in B-DAT
Table O
6. O
We O
observe O
that O

the O
testing B-DAT
set O
are O
summarized O
in O
Table O
7. O
Out O
entry O
“CES O

other O
participants O
of O
this O
challenge O
in B-DAT
Table O
7. O
It O
is O
worth O

efficiency O
of O
TSN, O
our O
models O
in B-DAT
the O
challenge O
can O
be O
trained O

time O
visualize O
interesting B-DAT
class O
information O
in O
action O
recognition O
ConvNet O
models. O
We O

left) O
and O
y O
(right) O
directions O
in B-DAT
gray-scales. O
Note O
all O
these O
images O

purely O
random O
pixels. O
Left: O
classes O
in B-DAT
UCF101. O
Right: O
classes O
in O
ActivityNet O

ing. B-DAT
The O
results O
are O
shown O
in O
Fig. O
3. O
For O
both O
RGB O

the O
scenery O
patterns O
and O
objects O
in B-DAT
the O
videos O
as O
significant O
evidences O

for O
action O
recognition. O
For O
example, O
in B-DAT
the O
class O
“Diving”, O
the O
single-frame O

models O
focus O
more O
on O
humans O
in B-DAT
the O
videos, O
and O
seem O
to O

different O
poses O
can O
be O
identified O
in B-DAT
the O
image, O
depicting O
various O
stages O

Similar O
observation O
would O
be O
identified O
in B-DAT
other O
action O
classes O
such O
as O

better, O
which O
is O
well O
reflected O
in B-DAT
our O
quantitative O
experiments O

good O
practices O
that O
we O
explored O
in B-DAT
this O
work. O
The O
former O
provides O

works O
for O
action O
recognition O
in B-DAT
videos,” O
in O
NIPS, O
2014, O
pp O

trajectories,” O
in B-DAT
ICCV, O
2013, O
pp. O
3551–3558. O
[3 O

human O
motion O
recognition,” O
in B-DAT
CVPR, O
2013, O
pp. O
2674–2681. O
[4 O

net- O
works O
for O
video O
classification,” O
in B-DAT
CVPR, O
2015, O
pp. O
4694–4702 O

with O
trajectory- O
pooled O
deep-convolutional O
descriptors,” O
in B-DAT
CVPR, O
2015, O
pp. O
4305– O
4314 O

with O
deep O
convolutional O
neural O
networks,” O
in B-DAT
NIPS, O
2012, O
pp. O
1106–1114 O

works O
for O
large-scale O
image O
recognition,” O
in B-DAT
ICLR, O
2015, O
pp. O
1–14 O

Rabinovich, B-DAT
“Going O
deeper O
with O
convolutions,” O
in O
CVPR, O
2015, O
pp. O
1–9 O

scene O
recognition O
using B-DAT
places O
database,” O
in O
NIPS, O
2014, O
pp. O
487–495 O

of O
deep O
convolutional O
neural O
networks,” O
in B-DAT
ECCV, O
2016, O
pp. O
467–482 O

images O
by O
fusing B-DAT
deep O
channels,” O
in O
CVPR, O
2015, O
pp. O
1600–1609 O

neural O
networks O
for O
event O
recognition O
in B-DAT
still O
images,” O
CoRR, O
vol. O
abs/1609.00162 O

classification O
with O
convolutional O
neural O
networks,” O
in B-DAT
CVPR, O
2014, O
pp. O
1725–1732 O

with O
3d O
convolutional O
net- O
works,” O
in B-DAT
ICCV, O
2015, O
pp. O
4489–4497 O

with O
enhanced O
motion O
vector O
CNNs,” O
in B-DAT
CVPR, O
2016, O
pp. O
2718–2726 O

A O
large-scale O
hierarchical O
image O
database,” O
in B-DAT
CVPR, O
2009, O
pp. O
248– O
255 O

motion O
segments O
for O
activity O
classification,” O
in B-DAT
ECCV, O
2010, O
pp. O
392–405 O

video O
evolution O
for O
action O
recognition,” O
in B-DAT
CVPR, O
2015, O
pp. O
5378–5387 O

for O
visual O
recognition O
and O
description,” O
in B-DAT
CVPR, O
2015, O
pp. O
2625–2634 O

action O
recog- O
nition O
for O
videos O
in B-DAT
the O
wild,” O
Computer O
Vision O
and O

for O
human O
activity O
under- O
standing,” B-DAT
in O
CVPR, O
2015, O
pp. O
961–970 O

human O
actions O
classes O
from O
videos O
in B-DAT
the O
wild,” O
CoRR, O
vol. O
abs/1212.0402 O

database O
for O
human O
motion O
recognition,” O
in B-DAT
ICCV, O
2011, O
pp. O
2556–2563 O

residual O
learning B-DAT
for O
image O
recognition,” O
in O
CVPR, O
2016, O
pp. O
770–778 O

inception B-DAT
architecture O
for O
computer O
vision,” O
in O
CVPR, O
2016, O
pp. O
2818–2826 O

practices O
for O
deep O
action O
recognition,” O
in B-DAT
ECCV, O
2016, O
pp. O
20–36 O

motion O
synthesis,” O
Foundations O
and O
Trends O
in B-DAT
Computer O
Graphics O
and O
Vision, O
vol O

scale-invariant B-DAT
spatio-temporal O
interest O
point O
detector,” O
in O
ECCV, O
2008, O
pp. O
650–663 O

nition O
via O
sparse O
spatio-temporal O
features,” O
in B-DAT
IEEE O
International O
Workshop O
on O
PETS O

Action O
recognition O
by O
dense O
trajectories,” O
in B-DAT
CVPR, O
2011, O
pp. O
3169–3176 O

realistic O
human O
actions O
from O
movies,” O
in B-DAT
CVPR, O
2008, O
pp. O
1–8 O

spatio-temporal O
descriptor O
based O
on O
3D-gradients,” O
in B-DAT
BMVC, O
2008, O
pp. O
1–12 O

categorization O
with O
bags O
of O
keypoints,” B-DAT
in O
ECCV O
Workshop O
on O
statistical O
learning O

in B-DAT
computer O
vision, O
2004, O
pp. O
1–22 O

super O
vector O
for O
action O
recognition,” O
in B-DAT
CVPR, O
2014, O
pp. O
596–603 O

parts O
from O
mid-level O
video O
representations,” O
in B-DAT
CVPR, O
2012, O
pp. O
1242–1249 O

videos O
using B-DAT
mid-level O
discriminative O
patches,” O
in O
CVPR, O
2013, O
pp. O
2571–2578 O

for O
detailed O
action O
under- O
standing,” B-DAT
in O
ICCV, O
2013, O
pp. O
2248–2255 O

Tu, O
“Action O
recognition O
with O
actons,” O
in B-DAT
ICCV, O
2013, O
pp. O
3559–3566 O

high-level O
represen- O
tation O
of O
activity O
in B-DAT
video,” O
in O
CVPR, O
2012, O
pp O

using B-DAT
3d O
human O
pose O
annotations,” O
in O
ICCV, O
2009, O
pp. O
1365–1372 O

using B-DAT
factorized O
spatio-temporal O
convolutional O
networks,” O
in O
ICCV, O
2015, O
pp. O
4597–4605 O

Xue, O
“Modeling B-DAT
spatial- O
temporal O
clues O
in O
a O
hybrid O
deep O
learning O
framework O

for O
video O
classification,” O
in B-DAT
ACM O
Multimedia, O
2015, O
pp. O
461–470 O

fusion O
for O
video O
action O
recognition,” O
in B-DAT
CVPR, O
2016, O
pp. O
1933–1941 O

of O
actions O
with O
segmental O
grammars,” O
in B-DAT
CVPR, O
2014, O
pp. O
612–619 O

action O
detection O
with O
relational O
dynamic-poselets,” O
in B-DAT
ECCV, O
2014, O
pp. O
565–580 O

by O
reducing B-DAT
internal O
covariate O
shift,” O
in O
ICML, O
2015, O
pp. O
448–456 O

for O
realtime O
tv-L1 O
optical O
flow,” O
in B-DAT
29th O
DAGM O
Symposium O
on O
Pattern O

Zisserman, O
“Return O
of O
the O
devil O
in B-DAT
the O
details: O
Delving O
deep O
into O

convolutional O
nets,” O
in B-DAT
BMVC, O
2014 O

networks O
for O
video O
action O
recognition,” O
in B-DAT
NIPS, O
2016, O
pp. O
3468–3476 O

for O
the O
thu- O
mos O
workshop,” O
in B-DAT
ICCV O
Workshop O
on O
THUMOS O
Challenge O

about O
classifying B-DAT
and O
localizing O
actions?” O
in O
CVPR, O
2015, O
pp. O
46–55 O

depth O
for O
large-scale O
action O
recognition,” O
in B-DAT
ECCV. O
Springer, O
2016, O
pp. O
668–684 O

recognition O
via O
trajectory O
group O
selection,” O
in B-DAT
CVPR, O
2015, O
pp. O
3698–3706 O

deep O
framework O
for O
action O
recognition,” O
in B-DAT
CVPR, O
2016, O
pp. O
1991– O
1999 O

3.4 O
TSN O
in B-DAT
Practice O

4.1 O
Action O
Recognition O
in B-DAT
Trimmed O
Video O

4.2 O
Action O
Recognition O
in B-DAT
Untrimmed O
Videos O

for O
activity O
detection, O
such O
as O
THUMOS B-DAT
(Jiang O
et O
al., O
2014), O
Ac O

4.2. O
THUMOS B-DAT
/ O
MultiTHUMOS O

an O
extended O
version O
of O
the O
THUMOS B-DAT
dataset O
that O
densely O
annotates O
the O

classes, O
compared O
to O
20 O
in O
THUMOS, B-DAT
and O
contains O
on O
average O
10.5 O

age O
∼1 O
activity O
per O
video. O
THUMOS B-DAT
and O
MultiTHUMOS O
consists O
of O
YouTube O

and O
us- O
ing O
IoU=0.5 O
in O
THUMOS B-DAT

on O
the O
continuous O
version O
of O
THUMOS B-DAT

duration O
of O
activities O
in O
Multi- O
THUMOS B-DAT
(3.3 O
seconds), O
we O
find O
that O

learned O
TGM O
kernels. O
On O
Multi- O
THUMOS, B-DAT
it O
learns O
to O
focus O
on O

Shah, O
M., O
and O
Sukthankar, O
R. O
THUMOS B-DAT
chal- O
lenge: O
Action O
recognition O
with O

THUMOS14 B-DAT

THUMOS14 B-DAT

Multi B-DAT

short O
duration O
of O
activities O
in O
Multi B-DAT

several O
learned O
TGM O
kernels. O
On O
Multi B-DAT

datasets O
and O
challenges O
such O
as O
THUMOS B-DAT
[12], O
ActivityNet O
[6] O
and O
Charades O

is O
an O
extension O
of O
the O
THUMOS B-DAT
[12] O
dataset O
with O
the O
untrimmed O

classes. O
Unlike O
Activi- O
tyNet O
and O
THUMOS, B-DAT
MultiTHUMOS O
has O
on O
average O
10.5 O

the O
segmented O
training O
videos O
from O
THUMOS, B-DAT
since O
super- O
event O
learning O
is O

M. O
Shah, O
and O
R. O
Sukthankar. O
THUMOS B-DAT
chal- O
lenge: O
Action O
recognition O
with O

THUMOS14 B-DAT

THUMOS14 B-DAT

Two-stream O
+ O
LSTM O
[31] O
28.1 O
Multi B-DAT

Charades B-DAT
Dataset O
We O
further O
test O
on O

the O
popular O
Charades B-DAT
dataset O
[19] O
which O
is O
unique O

all O
with O
higher-than-50% O
accuracy O
on O
Charades B-DAT

action O
classification O
per- O
formances O
on O
Charades B-DAT
[19]. O
Method O
modality O
mAP O

may O
temporally O
overlap O
in O
a O
Charades B-DAT
video, O
requiring O
the O
model O
to O

trained O
and O
tested O
on O
the O
Charades B-DAT
and O
MiT O
datasets. O
For O
Charades O

note O
that O
the O
performances O
on O
Charades B-DAT
is O
even O
more O
impressive O
at O

Architecture O
MiT O
Charades B-DAT

A. O
Farhadi, O
and O
K. O
Alahari. O
Charades B-DAT

The O
batch O
size O
used O
for O
Charades B-DAT
is O
128 O
with O
128 O
frames O

the O
entire O
video. O
For O
the O
Charades B-DAT
dataset O
where O
each O
video O
duration O

major O
video O
recognition O
benchmarks, O
Kinetics, O
Charades B-DAT
and O
AVA. O
Code O
will O
be O

the O
Kinetics-400 O
[27], O
Kinetics-600 O
[2], O
Charades B-DAT
[40] O
and O
AVA O
[17] O
datasets O

recent O
Kinetics- O
600 O
[2], O
and O
Charades B-DAT
[40]. O
For O
action O
detection O
experiments O

Charades B-DAT
[40] O
has O
∼9.8k O
training O
videos O

Charades B-DAT
[40] O
is O
a O
dataset O
with O

Comparison O
with O
the O
state-of-the-art O
on O
Charades B-DAT

For O
Charades, B-DAT
we O
fine-tune O
from O
Kinetics O
pre-trained O

video O
datasets: O
AVA, O
EPIC-Kitchens, O
and O
Charades B-DAT

action O
classifica- O
tion O
[6], O
and O
Charades B-DAT
video O
classification O
[38]. O
Our O
abla O

6. O
Experiments O
on O
Charades B-DAT

evaluate O
our O
approach O
on O
the O
Charades B-DAT
dataset O
[38]. O
The O
Charades O
dataset O

Table O
3. O
Training O
schedule O
on O
Charades B-DAT

4. O
Action O
recognition O
accuracy O
on O
Charades B-DAT

NL′ O
to O
work O
better O
on O
Charades, B-DAT
so O
we O
adopt O
it O
in O

top-1) O
AVA O
EPIC O
Verbs O
(top-1) O
Charades B-DAT

For O
Charades, B-DAT
we O
experiment O
with O
both O
ResNet-50-I3D O

test O
sets. O
The O
improvement O
on O
Charades B-DAT
is O
not O
as O
large O
as O

to O
60 O
seconds O
is O
useful. O
Charades B-DAT
videos O
are O
much O
shorter O
(∼30 O

datasets O
like O
AVA, O
EPIC-Kitchens, O
and O
Charades B-DAT

used O
for O
computing O
L, O
so O
Charades B-DAT

Appendix O
H. O
Charades B-DAT
Training O
Schedule O

Appendix O
I. O
Charades B-DAT
NL O
Block O
Details O

Pre-activation O
vs. O
post-activation O
NL′ O
on O
Charades B-DAT

choose O
post-activation O
as O
default O
for O
Charades B-DAT
due O
to O
the O
stronger O
performance O

achieve O
state-of-the-art O
results O
on O
both O
Charades B-DAT
and O
Something-Something O
datasets. O
Especially O
for O

our O
experiments O
in O
the O
challenging O
Charades B-DAT
[20] O
and O
20BN-Something- O
Something O
[21 O

action O
recognition. O
Especially O
in O
the O
Charades B-DAT
dataset, O
we O
obtain O
4.4% O
boost O

on O
two O
recent O
challenging O
datasets: O
Charades B-DAT
[20] O
and O
Something-Something O
[21]. O
We O

on O
our O
target O
datasets O
(e.g. O
Charades B-DAT
or O
Something- O
Something) O
as O
following O

d O
= O
512. O
Since O
both O
Charades B-DAT
and O
Something-Something O
dataset O
are O
in O

loss O
functions O
when O
training O
for O
Charades B-DAT
and O
Something-Something O
datasets. O
For O
Something-Something O

the O
softmax O
loss O
function. O
For O
Charades, B-DAT
we O
apply O
binary O
sigmoid O
loss O

two O
different O
datasets. O
As O
for O
Charades, B-DAT
the O
scenes O
are O
more O
cluttered O

we O
sample O
10 O
clips O
for O
Charades B-DAT
and O
2 O
clips O
for O
Something-Something O

Table O
2. O
Ablations O
on O
Charades B-DAT

6.2 O
Experiments O
on O
Charades B-DAT

In O
the O
Charades B-DAT
experiments, O
following O
the O
official O
split O

actually O
very O
small. O
In O
the O
Charades B-DAT
dataset, O
our O
graph O
is O
defined O

specifically, O
for O
each O
video O
in O
Charades, B-DAT
besides O
the O
action O
class O
labels O

Classification O
mAP O
(%) O
in O
the O
Charades B-DAT
dataset O
[20]. O
NL O
is O
short O

is O
very O
different O
from O
the O
Charades B-DAT
dataset. O
In O
the O
Charades O
dataset O

gains O
we O
have O
in O
the O
Charades B-DAT
dataset. O
The O
reason O
is O
mainly O

ble O
to O
the O
previous O
version. O
Charades B-DAT
[26] O
is O
an O
activity O
recognition O

seconds O
on O
average. O
We O
chose O
Charades B-DAT
to O
particularly O
confirm O
whether O
our O

Table O
4. O
Charades B-DAT
classification O
results O
against O
state-of-the-arts O

Charades B-DAT

our O
approach O
on O
the O
popular O
Charades B-DAT
dataset. O
Table O
4 O
compares O
against O

Method O
Kinetics O
Charades B-DAT
HMDB O
MiT O

37.8 O
82.3 O
31.8 O
Evolved O
on O
Charades B-DAT
76.5 O
38.1 O
81.8 O
31.1 O
Evolved O

or O
11) O
when O
evolved O
for O
Charades, B-DAT
while O
they O
only O
had O
a O

An O
average O
activity O
duration O
in O
Charades B-DAT
videos O
are O
around O
12 O
seconds O

iTGM O
kernels O
from O
Kinetics O
to O
Charades B-DAT

Stretched O
(L O
= O
11) O
38.1 O
Charades B-DAT
EvaNet O
38.1 O

layers O
and O
apply O
it O
to O
Charades, B-DAT
which O
has O
activities O
with O
much O

to O
L O
= O
11 O
on O
Charades, B-DAT
which O
shows O
similar O
performance. O
Evolution O

176 O
× O
176 O
(for O
Charades) B-DAT
where O
32 O
and O
64 O
are O

12, O
this O
time O
using O
the O
Charades B-DAT
dataset O

Table O
14. O
Charades B-DAT
performance O
comparison O
to O
baselines, O
all O

Inception- O
like O
architectures O
evolved O
on O
Charades B-DAT

Charades, B-DAT
the O
architectures O
generally O
capture O
longer O

Figure O
16. O
Charades B-DAT
RGB O
Top O
1 O

Figure O
17. O
Charades B-DAT
RGB O
Top O
2 O

Figure O
18. O
Charades B-DAT
RGB O
Top O
3 O

Something- O
Something, O
Jester, O
and O
Charades B-DAT
- O
which O
fundamentally O
depend O
on O

Something-Something O
[9], O
Jester O
[10], O
and O
Charades B-DAT
[11]), O
which O
are O
constructed O
for O

Left’, O
and O
‘Turning O
hand O
counterclockwise’. O
Charades B-DAT
dataset O
is O
also O
a O
high-level O

on O
activity O
classification O
in O
the O
Charades B-DAT
dataset O
[11], O
outperforming O
the O
Flow+RGB O

9,28], O
Jester O
dataset O
[10], O
and O
Charades B-DAT
dataset O
[11] O
are O
listed O
in O

27 O
148,092 O
human O
hand O
gesture O
Charades B-DAT
157 O
9,848 O
daily O
indoor O
activity O

3.3 O
Results O
on O
Jester O
and O
Charades B-DAT

MultiScale O
TRN O
on O
the O
recent O
Charades B-DAT
dataset O
for O
daily O
activity O
recognition O

Table O
4: O
Results O
on O
Charades B-DAT
Activity O
Classification O

2 O
predictions O
are O
shown O
above O
Charades B-DAT
frames O

UCF O
Kinetics O
Moments O
Something O
Jester O
Charades B-DAT

mAP O
of O
22.4% O
on O
the O
Charades B-DAT
[43] O
benchmark, O
outperforming O
the O
state-of-the-art O

22.4% O
mAP O
on O
the O
challenging O
Charades B-DAT
[43] O
benchmark O

our O
evaluation, O
we O
chose O
the O
Charades B-DAT
dataset O
[43]. O
This O
dataset O
is O

follow O
the O
training O
setup O
in O
Charades B-DAT
[43] O
and O
con- O
sider O
a O

Qi(X) O
at O
each O
frame. O
The O
Charades B-DAT
dataset O
has O
the O
property O
that O

1. O
Video O
classification O
results O
on O
Charades B-DAT
[43]. O
The O
left O
shows O
the O

as O
a O
part O
of O
the O
Charades B-DAT
dataset O
(allenai.org/plato/charades O

charades B-DAT

on O
the O
UCF-101, O
HMDB-51, O
and O
Charades B-DAT
dataset O

34], O
HMDB- O
51 O
[18], O
and O
Charades B-DAT
[32], O
our O
approach O
significantly O
out O

UCF-101 O
[34], O
HMDB-51 O
[18], O
and O
Charades B-DAT
[32]. O
UCF-101 O
and O
HMDB-51 O
contain O

annotated O
with O
one O
action O
label. O
Charades B-DAT
contains O
longer O
(∼ O
30- O
second O

un- O
less O
otherwise O
stated. O
The O
Charades B-DAT
dataset O
contains O
9,848 O
videos O
split O

softmax O
following O
TSN O
[44]. O
On O
Charades B-DAT
we O
use O
mean O
average O
precision O

videos O
to O
340× O
256. O
As O
Charades B-DAT
con- O
tains O
both O
portrait O
and O

UCF- O
101/HMDB-51 O
and O
0.03 O
for O
Charades B-DAT

Table O
7: O
Accuracy O
on O
Charades B-DAT
[32]. O
Without O
using O
ad- O
ditional O

evaluate O
our O
method O
on O
the O
Charades B-DAT
dataset O
(Table O
7). O
As O
Charades O

other O
state- O
of-the-art O
methods O
on O
Something-Something B-DAT
V1 I-DAT
dataset. O
Single O
crop O
STM O
beats O

on O
both O
temporal-related O
datasets O
(i.e., O
Something-Something B-DAT
v1 O
& O
v2 O
and O
Jester O

several O
public O
benchmark O
datasets O
including O
Something-Something B-DAT

conduct O
abundant O
ablation O
studies O
with O
Something-Something B-DAT
v1 O
to O
analyze O
the O
effectiveness O

categories: O
(1) O
temporal-related O
datasets, O
including O
Something-Something B-DAT
v1 O
& O
v2 O
[11] O
and O

of O
the O
STM O
on O
the O
Something-Something B-DAT
v1 O
and O
v2 O
datasets O
compared O

Method O
Backbone O
Flow O
Pretrain O
Frame O
Something-Something B-DAT
v1 O
Something-Something O
v2top-1 O
val O
top-5 O

methods O
on O
temporal-related O
datasets O
including O
Something-Something B-DAT
v1 O
& O
v2 O
and O
Jester O

174 O
classes O
with O
108,499 O
videos. O
Something-Something B-DAT
v2 O
is O
an O
updated O
version O

compared O
with O
the O
state-of-the-art O
on O
Something-Something B-DAT
v1 O
and O
v2. O
The O
results O

on O
Something- O
Something O
v1. O
On O
Something-Something B-DAT
v2, O
STM O
also O
gains O
34.5 O

valida- O
tion O
sets O
of O
both O
Something-Something B-DAT
v1 O
and O
v2, O
and O
just O

our O
pro- O
posed O
STM O
on O
Something-Something B-DAT
v1 O
dataset. O
All O
the O
ablation O

other O
state- O
of-the-art O
methods O
on O
Something-Something B-DAT
V1 O
dataset. O
Single O
crop O
STM O

and O
several O
state-of-the-art O
methods O
on O
Something-Something B-DAT
v1 O
dataset. O
All O
evaluations O
are O

on O
both O
temporal-related O
datasets O
(i.e., O
Something B-DAT

Something B-DAT
v1 O
& O
v2 O
and O
Jester O

several O
public O
benchmark O
datasets O
including O
Something B-DAT

Something B-DAT

on O
both O
temporal-related O
datasets O
(i.e., O
Something B-DAT

- O
Something B-DAT
v1 O
& O
v2 O
and O
Jester O

conduct O
abundant O
ablation O
studies O
with O
Something B-DAT

Something B-DAT
v1 O
to O
analyze O
the O
effectiveness O

categories: O
(1) O
temporal-related O
datasets, O
including O
Something B-DAT

Something B-DAT
v1 O
& O
v2 O
[11] O
and O

T O
= O
16). O
For O
Kinetics, O
Something B-DAT

- O
Something B-DAT
v1 O
& O
v2 O
and O
Jester O

of O
the O
STM O
on O
the O
Something B-DAT

Something B-DAT
v1 O
and O
v2 O
datasets O
compared O

Method O
Backbone O
Flow O
Pretrain O
Frame O
Something B-DAT

-Something B-DAT
v1 O
Something O

Something B-DAT
v2top-1 O
val O
top-5 O
val O
top-1 O

methods O
on O
temporal-related O
datasets O
including O
Something B-DAT

Something B-DAT
v1 O
& O
v2 O
and O
Jester O

. O
Something- B-DAT
Something O
v1 O
is O
a O
large O
collection O

174 O
classes O
with O
108,499 O
videos. O
Something B-DAT

Something B-DAT
v2 O
is O
an O
updated O
version O

compared O
with O
the O
state-of-the-art O
on O
Something B-DAT

Something B-DAT
v1 O
and O
v2. O
The O
results O

16 O
frames O
inputs O
respectively O
on O
Something B-DAT

- O
Something B-DAT
v1. O
On O
Something O

Something B-DAT
v2, O
STM O
also O
gains O
34.5 O

valida- O
tion O
sets O
of O
both O
Something B-DAT

Something B-DAT
v1 O
and O
v2, O
and O
just O

our O
pro- O
posed O
STM O
on O
Something B-DAT

Something B-DAT
v1 O
dataset. O
All O
the O
ablation O

other O
state- O
of-the-art O
methods O
on O
Something B-DAT

Something B-DAT
V1 O
dataset. O
Single O
crop O
STM O

and O
several O
state-of-the-art O
methods O
on O
Something B-DAT

Something B-DAT
v1 O
dataset. O
All O
evaluations O
are O

Something B-DAT
V2 I-DAT
dataset). O
The O
three O
kinds O
of O

Something B-DAT
V2, I-DAT
the O
epoch O
number O
is O
halved O

Something B-DAT
V2 I-DAT
with O
the O
same O
experimental O
settings O

Something B-DAT
V2 I-DAT

of O
experi- O
ments O
on O
Something O
Something B-DAT
V2 I-DAT
and O
Charades O
datasets O
with O
the O

with O
Inception-V3 O
backbone O
on O
Something- O
Something B-DAT
V2, I-DAT
and O
improve O
the O
mAP O
from O

Something B-DAT
V2 I-DAT
(left) O
and O
Charades O
(right) O
datasets O

the O
CAM O
[54] O
on O
Something- O
Something B-DAT
V2 I-DAT
dataset. O
CAM O
can O
visualize O
the O

Something B-DAT
V2 I-DAT
validation O
set. O
The O
representations O
are O

Something B-DAT
V2 I-DAT
dataset O
with O
20 O
randomly O
selected O

the O
samples O
are O
originated O
from O
Something-Something B-DAT
V2 I-DAT
dataset). O
The O
three O
kinds O
of O

procedure O
takes O
100 O
epochs. O
For O
Something-Something B-DAT
V2, I-DAT
the O
epoch O
number O
is O
halved O

Something-Something O
V1 O
and O
2.5% O
on O
Something-Something B-DAT
V2 I-DAT
with O
the O
same O
experimental O
settings O

Something-Something O
V1 O
and O
61.3% O
on O
Something-Something B-DAT
V2 I-DAT

Inception O
or O
Inception-V3 O
architecture O
on O
Something-Something B-DAT
V2 I-DAT
(left) O
and O
Charades O
(right) O
datasets O

of O
the O
video O
representation O
on O
Something-Something B-DAT
V2 I-DAT
validation O
set. O
The O
representations O
are O

The O
experiment O
was O
conducted O
on O
Something-Something B-DAT
V2 I-DAT
dataset O
with O
20 O
randomly O
selected O

samples O
are O
originated O
from O
Something-Something O
V2 B-DAT
dataset). O
The O
three O
kinds O
of O

data O
sets: O
Something-Something O
V1 O
[9], O
V2 B-DAT
[16] O
and O
Charades O
[29]. O
Both O

recognition: O
Something- O
Something O
V1 O
[9], O
V2 B-DAT
[16] O
and O
Charades O
[29]. O
We O

V2 B-DAT
[16]: O
The O
dataset O
is O
twice O

takes O
100 O
epochs. O
For O
Something-Something O
V2, B-DAT
the O
epoch O
number O
is O
halved O

Every O
video O
in O
Something-Something-V1 O
and O
V2 B-DAT
datasets O
is O
assigned O
to O
a O

results O
on O
Something-Something O
V1 O
and O
V2 B-DAT
validation O
set. O
Considering O
that O
they O

V1 O
and O
2.5% O
on O
Something-Something O
V2 B-DAT
with O
the O
same O
experimental O
settings O

V1 O
and O
61.3% O
on O
Something-Something O
V2 B-DAT

ON O
THE O
SOMETHING-SOMETHING O
V1 O
AND O
V2 B-DAT
DATASETS. O
WE O
ONLY O
REPORT O
THE O

OF O
OUR O
MODEL O
ON O
SOMETHING-SOMETHING O
V2 B-DAT
DATASET O

experi- O
ments O
on O
Something O
Something O
V2 B-DAT
and O
Charades O
datasets O
with O
the O

Inception-V3 O
backbone O
on O
Something- O
Something O
V2, B-DAT
and O
improve O
the O
mAP O
from O

or O
Inception-V3 O
architecture O
on O
Something-Something O
V2 B-DAT
(left) O
and O
Charades O
(right) O
datasets O

CAM O
[54] O
on O
Something- O
Something O
V2 B-DAT
dataset. O
CAM O
can O
visualize O
the O

the O
video O
representation O
on O
Something-Something O
V2 B-DAT
validation O
set. O
The O
representations O
are O

experiment O
was O
conducted O
on O
Something-Something O
V2 B-DAT
dataset O
with O
20 O
randomly O
selected O

the O
samples O
are O
originated O
from O
Something-Something B-DAT
V2 I-DAT
dataset). O
The O
three O
kinds O
of O

three O
large- O
scale O
data O
sets: O
Something-Something B-DAT
V1 I-DAT
[9], O
V2 O
[16] O
and O
Charades O

procedure O
takes O
100 O
epochs. O
For O
Something-Something B-DAT
V2, I-DAT
the O
epoch O
number O
is O
halved O

shows O
all O
the O
results O
on O
Something-Something B-DAT
V1 I-DAT
and O
V2 O
validation O
set. O
Considering O

a O
large O
margin O
4.1% O
on O
Something-Something B-DAT
V1 I-DAT
and O
2.5% O
on O
Something-Something O
V2 O

overall O
performance O
to O
49.8% O
on O
Something-Something B-DAT
V1 I-DAT
and O
61.3% O
on O
Something-Something O
V2 O

Inception O
or O
Inception-V3 O
architecture O
on O
Something-Something B-DAT
V2 I-DAT
(left) O
and O
Charades O
(right) O
datasets O

of O
the O
video O
representation O
on O
Something-Something B-DAT
V2 I-DAT
validation O
set. O
The O
representations O
are O

The O
experiment O
was O
conducted O
on O
Something-Something B-DAT
V2 I-DAT
dataset O
with O
20 O
randomly O
selected O

widely-used O
large-scale O
datasets, O
such O
as O
Something-Something B-DAT
and O
Charades, O
and O
the O
results O

the O
samples O
are O
originated O
from O
Something-Something B-DAT
V2 O
dataset). O
The O
three O
kinds O

three O
large- O
scale O
data O
sets: O
Something-Something B-DAT
V1 O
[9], O
V2 O
[16] O
and O

Something-Something B-DAT

Something-Something B-DAT

procedure O
takes O
100 O
epochs. O
For O
Something-Something B-DAT
V2, O
the O
epoch O
number O
is O

Every O
video O
in O
Something-Something B-DAT

A O
baseline O
model O
provided O
in O
Something-Something B-DAT
dataset O
exploits O
multi-layer O
3D O
convolution O

The O
results O
on O
Something-Something B-DAT
[9] O
and O
Charades O
[28], O
[29 O

shows O
all O
the O
results O
on O
Something-Something B-DAT
V1 O
and O
V2 O
validation O
set O

a O
large O
margin O
4.1% O
on O
Something-Something B-DAT
V1 O
and O
2.5% O
on O
Something-Something O

overall O
performance O
to O
49.8% O
on O
Something-Something B-DAT
V1 O
and O
61.3% O
on O
Something-Something O

Inception O
or O
Inception-V3 O
architecture O
on O
Something-Something B-DAT
V2 O
(left) O
and O
Charades O
(right O

of O
the O
video O
representation O
on O
Something-Something B-DAT
V2 O
validation O
set. O
The O
representations O

The O
experiment O
was O
conducted O
on O
Something-Something B-DAT
V2 O
dataset O
with O
20 O
randomly O

evaluated O
the O
proposed O
model O
on O
Something-Something B-DAT
and O
Charades O
datasets O
and O
estab O

MODULE O
OF O
OUR O
MODEL O
ON O
SOMETHING-SOMETHING B-DAT
V2 I-DAT
DATASET O

widely-used O
large-scale O
datasets, O
such O
as O
Something B-DAT

Something B-DAT
and O
Charades, O
and O
the O
results O

the O
samples O
are O
originated O
from O
Something B-DAT

Something B-DAT
V2 O
dataset). O
The O
three O
kinds O

three O
large- O
scale O
data O
sets: O
Something B-DAT

Something B-DAT
V1 O
[9], O
V2 O
[16] O
and O

Covering O
Something B-DAT

with O
Something B-DAT

benchmark O
datasets O
for O
activity O
recognition: O
Something B-DAT

- O
Something B-DAT
V1 O
[9], O
V2 O
[16] O
and O

Something B-DAT

Something B-DAT

Something B-DAT

Something B-DAT

procedure O
takes O
100 O
epochs. O
For O
Something B-DAT

Something B-DAT
V2, O
the O
epoch O
number O
is O

temporal O
axis O
for O
Charades O
and O
Something B-DAT

- O
Something B-DAT
dataset O
respectively O
and O
the O
prediction O

Every O
video O
in O
Something B-DAT

Something B-DAT

A O
baseline O
model O
provided O
in O
Something B-DAT

Something B-DAT
dataset O
exploits O
multi-layer O
3D O
convolution O

The O
results O
on O
Something B-DAT

Something B-DAT
[9] O
and O
Charades O
[28], O
[29 O

shows O
all O
the O
results O
on O
Something B-DAT

Something B-DAT
V1 O
and O
V2 O
validation O
set O

a O
large O
margin O
4.1% O
on O
Something B-DAT

-Something B-DAT
V1 O
and O
2.5% O
on O
Something O

Something B-DAT
V2 O
with O
the O
same O
experimental O

overall O
performance O
to O
49.8% O
on O
Something B-DAT

-Something B-DAT
V1 O
and O
61.3% O
on O
Something O

Something B-DAT
V2. O
Compare O
with O
the O
most O

serial O
of O
experi- O
ments O
on O
Something B-DAT
Something O
V2 O
and O
Charades O
datasets O

54.7% O
with O
Inception-V3 O
backbone O
on O
Something B-DAT

- O
Something B-DAT
V2, O
and O
improve O
the O
mAP O

Inception O
or O
Inception-V3 O
architecture O
on O
Something B-DAT

Something B-DAT
V2 O
(left) O
and O
Charades O
(right O

visualize O
the O
CAM O
[54] O
on O
Something B-DAT

- O
Something B-DAT
V2 O
dataset. O
CAM O
can O
visualize O

of O
the O
video O
representation O
on O
Something B-DAT

Something B-DAT
V2 O
validation O
set. O
The O
representations O

The O
experiment O
was O
conducted O
on O
Something B-DAT

Something B-DAT
V2 O
dataset O
with O
20 O
randomly O

evaluated O
the O
proposed O
model O
on O
Something B-DAT

Something B-DAT
and O
Charades O
datasets O
and O
estab O

Something B-DAT
V2 I-DAT
dataset). O
The O
three O
kinds O
of O

Something B-DAT
V2, I-DAT
the O
epoch O
number O
is O
halved O

Something B-DAT
V2 I-DAT
with O
the O
same O
experimental O
settings O

Something B-DAT
V2 I-DAT

of O
experi- O
ments O
on O
Something O
Something B-DAT
V2 I-DAT
and O
Charades O
datasets O
with O
the O

with O
Inception-V3 O
backbone O
on O
Something- O
Something B-DAT
V2, I-DAT
and O
improve O
the O
mAP O
from O

Something B-DAT
V2 I-DAT
(left) O
and O
Charades O
(right) O
datasets O

the O
CAM O
[54] O
on O
Something- O
Something B-DAT
V2 I-DAT
dataset. O
CAM O
can O
visualize O
the O

Something B-DAT
V2 I-DAT
validation O
set. O
The O
representations O
are O

Something B-DAT
V2 I-DAT
dataset O
with O
20 O
randomly O
selected O

the O
samples O
are O
originated O
from O
Something-Something B-DAT
V2 I-DAT
dataset). O
The O
three O
kinds O
of O

procedure O
takes O
100 O
epochs. O
For O
Something-Something B-DAT
V2, I-DAT
the O
epoch O
number O
is O
halved O

Something-Something O
V1 O
and O
2.5% O
on O
Something-Something B-DAT
V2 I-DAT
with O
the O
same O
experimental O
settings O

Something-Something O
V1 O
and O
61.3% O
on O
Something-Something B-DAT
V2 I-DAT

Inception O
or O
Inception-V3 O
architecture O
on O
Something-Something B-DAT
V2 I-DAT
(left) O
and O
Charades O
(right) O
datasets O

of O
the O
video O
representation O
on O
Something-Something B-DAT
V2 I-DAT
validation O
set. O
The O
representations O
are O

The O
experiment O
was O
conducted O
on O
Something-Something B-DAT
V2 I-DAT
dataset O
with O
20 O
randomly O
selected O

samples O
are O
originated O
from O
Something-Something O
V2 B-DAT
dataset). O
The O
three O
kinds O
of O

data O
sets: O
Something-Something O
V1 O
[9], O
V2 B-DAT
[16] O
and O
Charades O
[29]. O
Both O

recognition: O
Something- O
Something O
V1 O
[9], O
V2 B-DAT
[16] O
and O
Charades O
[29]. O
We O

V2 B-DAT
[16]: O
The O
dataset O
is O
twice O

takes O
100 O
epochs. O
For O
Something-Something O
V2, B-DAT
the O
epoch O
number O
is O
halved O

Every O
video O
in O
Something-Something-V1 O
and O
V2 B-DAT
datasets O
is O
assigned O
to O
a O

results O
on O
Something-Something O
V1 O
and O
V2 B-DAT
validation O
set. O
Considering O
that O
they O

V1 O
and O
2.5% O
on O
Something-Something O
V2 B-DAT
with O
the O
same O
experimental O
settings O

V1 O
and O
61.3% O
on O
Something-Something O
V2 B-DAT

ON O
THE O
SOMETHING-SOMETHING O
V1 O
AND O
V2 B-DAT
DATASETS. O
WE O
ONLY O
REPORT O
THE O

OF O
OUR O
MODEL O
ON O
SOMETHING-SOMETHING O
V2 B-DAT
DATASET O

experi- O
ments O
on O
Something O
Something O
V2 B-DAT
and O
Charades O
datasets O
with O
the O

Inception-V3 O
backbone O
on O
Something- O
Something O
V2, B-DAT
and O
improve O
the O
mAP O
from O

or O
Inception-V3 O
architecture O
on O
Something-Something O
V2 B-DAT
(left) O
and O
Charades O
(right) O
datasets O

CAM O
[54] O
on O
Something- O
Something O
V2 B-DAT
dataset. O
CAM O
can O
visualize O
the O

the O
video O
representation O
on O
Something-Something O
V2 B-DAT
validation O
set. O
The O
representations O
are O

experiment O
was O
conducted O
on O
Something-Something O
V2 B-DAT
dataset O
with O
20 O
randomly O
selected O

the O
samples O
are O
originated O
from O
Something-Something B-DAT
V2 I-DAT
dataset). O
The O
three O
kinds O
of O

three O
large- O
scale O
data O
sets: O
Something-Something B-DAT
V1 I-DAT
[9], O
V2 O
[16] O
and O
Charades O

procedure O
takes O
100 O
epochs. O
For O
Something-Something B-DAT
V2, I-DAT
the O
epoch O
number O
is O
halved O

shows O
all O
the O
results O
on O
Something-Something B-DAT
V1 I-DAT
and O
V2 O
validation O
set. O
Considering O

a O
large O
margin O
4.1% O
on O
Something-Something B-DAT
V1 I-DAT
and O
2.5% O
on O
Something-Something O
V2 O

overall O
performance O
to O
49.8% O
on O
Something-Something B-DAT
V1 I-DAT
and O
61.3% O
on O
Something-Something O
V2 O

Inception O
or O
Inception-V3 O
architecture O
on O
Something-Something B-DAT
V2 I-DAT
(left) O
and O
Charades O
(right) O
datasets O

of O
the O
video O
representation O
on O
Something-Something B-DAT
V2 I-DAT
validation O
set. O
The O
representations O
are O

The O
experiment O
was O
conducted O
on O
Something-Something B-DAT
V2 I-DAT
dataset O
with O
20 O
randomly O
selected O

widely-used O
large-scale O
datasets, O
such O
as O
Something-Something B-DAT
and O
Charades, O
and O
the O
results O

the O
samples O
are O
originated O
from O
Something-Something B-DAT
V2 O
dataset). O
The O
three O
kinds O

three O
large- O
scale O
data O
sets: O
Something-Something B-DAT
V1 O
[9], O
V2 O
[16] O
and O

Something-Something B-DAT

Something-Something B-DAT

procedure O
takes O
100 O
epochs. O
For O
Something-Something B-DAT
V2, O
the O
epoch O
number O
is O

Every O
video O
in O
Something-Something B-DAT

A O
baseline O
model O
provided O
in O
Something-Something B-DAT
dataset O
exploits O
multi-layer O
3D O
convolution O

The O
results O
on O
Something-Something B-DAT
[9] O
and O
Charades O
[28], O
[29 O

shows O
all O
the O
results O
on O
Something-Something B-DAT
V1 O
and O
V2 O
validation O
set O

a O
large O
margin O
4.1% O
on O
Something-Something B-DAT
V1 O
and O
2.5% O
on O
Something-Something O

overall O
performance O
to O
49.8% O
on O
Something-Something B-DAT
V1 O
and O
61.3% O
on O
Something-Something O

Inception O
or O
Inception-V3 O
architecture O
on O
Something-Something B-DAT
V2 O
(left) O
and O
Charades O
(right O

of O
the O
video O
representation O
on O
Something-Something B-DAT
V2 O
validation O
set. O
The O
representations O

The O
experiment O
was O
conducted O
on O
Something-Something B-DAT
V2 O
dataset O
with O
20 O
randomly O

evaluated O
the O
proposed O
model O
on O
Something-Something B-DAT
and O
Charades O
datasets O
and O
estab O

MODULE O
OF O
OUR O
MODEL O
ON O
SOMETHING-SOMETHING B-DAT
V2 I-DAT
DATASET O

widely-used O
large-scale O
datasets, O
such O
as O
Something B-DAT

Something B-DAT
and O
Charades, O
and O
the O
results O

the O
samples O
are O
originated O
from O
Something B-DAT

Something B-DAT
V2 O
dataset). O
The O
three O
kinds O

three O
large- O
scale O
data O
sets: O
Something B-DAT

Something B-DAT
V1 O
[9], O
V2 O
[16] O
and O

Covering O
Something B-DAT

with O
Something B-DAT

benchmark O
datasets O
for O
activity O
recognition: O
Something B-DAT

- O
Something B-DAT
V1 O
[9], O
V2 O
[16] O
and O

Something B-DAT

Something B-DAT

Something B-DAT

Something B-DAT

procedure O
takes O
100 O
epochs. O
For O
Something B-DAT

Something B-DAT
V2, O
the O
epoch O
number O
is O

temporal O
axis O
for O
Charades O
and O
Something B-DAT

- O
Something B-DAT
dataset O
respectively O
and O
the O
prediction O

Every O
video O
in O
Something B-DAT

Something B-DAT

A O
baseline O
model O
provided O
in O
Something B-DAT

Something B-DAT
dataset O
exploits O
multi-layer O
3D O
convolution O

The O
results O
on O
Something B-DAT

Something B-DAT
[9] O
and O
Charades O
[28], O
[29 O

shows O
all O
the O
results O
on O
Something B-DAT

Something B-DAT
V1 O
and O
V2 O
validation O
set O

a O
large O
margin O
4.1% O
on O
Something B-DAT

-Something B-DAT
V1 O
and O
2.5% O
on O
Something O

Something B-DAT
V2 O
with O
the O
same O
experimental O

overall O
performance O
to O
49.8% O
on O
Something B-DAT

-Something B-DAT
V1 O
and O
61.3% O
on O
Something O

Something B-DAT
V2. O
Compare O
with O
the O
most O

serial O
of O
experi- O
ments O
on O
Something B-DAT
Something O
V2 O
and O
Charades O
datasets O

54.7% O
with O
Inception-V3 O
backbone O
on O
Something B-DAT

- O
Something B-DAT
V2, O
and O
improve O
the O
mAP O

Inception O
or O
Inception-V3 O
architecture O
on O
Something B-DAT

Something B-DAT
V2 O
(left) O
and O
Charades O
(right O

visualize O
the O
CAM O
[54] O
on O
Something B-DAT

- O
Something B-DAT
V2 O
dataset. O
CAM O
can O
visualize O

of O
the O
video O
representation O
on O
Something B-DAT

Something B-DAT
V2 O
validation O
set. O
The O
representations O

The O
experiment O
was O
conducted O
on O
Something B-DAT

Something B-DAT
V2 O
dataset O
with O
20 O
randomly O

evaluated O
the O
proposed O
model O
on O
Something B-DAT

Something B-DAT
and O
Charades O
datasets O
and O
estab O

V2 B-DAT
[16] O
to O
evaluate O
the O
overall O

V2 B-DAT
dataset. O
For O
a O
fair O
comparison O

V2 B-DAT

achieves O
the O
state-of-the-art O
performance O
on O
Something-Something B-DAT
and O
Jester. O
We O
pro- O
vide O

5.4. O
Results O
on O
Something-Something B-DAT

Something-Something B-DAT
[12] O
is O
a O
recently O
released O

a) O
Something-Something B-DAT
v2 O
Results O

on O
a O
flat O
surface” O
from O
Something-Something B-DAT
v2 O
validation O
set O

and O
our O
CPNet O
model O
on O
Something-Something B-DAT
and O
Jester O
datasets. O
Lastly O
in O

E. O
Architecture O
used O
in O
Something-Something B-DAT
and O
Jester O
Experiments O

F. O
Per-class O
accuracy O
of O
Something-Something B-DAT
and O
Jester O
models O

8: O
CPNet O
Architectures O
used O
in O
Something-Something B-DAT
and O
Jester O
dataset O
experiments O

Jester O
in O
Figure O
8 O
and O
Something-Something B-DAT
in O
Figure O
10 O

On O
Something-Something B-DAT
dataset O
[12], O
the O
largest O
ac O

Kinetics O
[18] O
in O
Figure O
11, O
Something-Something B-DAT
[12] O
in O
Figure O
12 O
and O

accuracy O
gain O
in O
percentage O
on O
Something-Something B-DAT
v2 O
dataset O
due O
to O
CP O

Turning O
something O
upside O
down” O
from O
Something-Something B-DAT
v2 O
validation O
set O

label O
“Picking O
something O
up” O
from O
Something-Something B-DAT
v2 O
validation O
set O

label O
“Moving O
something O
down” O
from O
Something-Something B-DAT
v2 O
validation O
set O

something O
next O
to O
something” O
from O
Something-Something B-DAT
v2 O
validation O
set O

on O
our O
final O
models O
on O
Something-Something B-DAT
v2 O
dataset. O
Approach O
is O
the O

achieves O
the O
state-of-the-art O
performance O
on O
Something B-DAT

Something B-DAT
and O
Jester. O
We O
pro- O
vide O

published O
methods O
on O
action-centric O
datasets O
Something B-DAT

- O
Something B-DAT
[12] O
and O
Jester O
[30] O
with O

ex- O
periments O
on O
action-centric O
datasets O
Something B-DAT

5.4. O
Results O
on O
Something B-DAT

Something B-DAT

Something B-DAT

Something B-DAT
[12] O
is O
a O
recently O
released O

a) O
Something B-DAT

Something B-DAT
v2 O
Results O

on O
a O
flat O
surface” O
from O
Something B-DAT

Something B-DAT
v2 O
validation O
set O

on O
the O
architecture O
used O
in O
Something B-DAT

- O
Something B-DAT
and O
Jester O
experiments O
(main O
paper O

and O
our O
CPNet O
model O
on O
Something B-DAT

Something B-DAT
and O
Jester O
datasets. O
Lastly O
in O

E. O
Architecture O
used O
in O
Something B-DAT

Something B-DAT
and O
Jester O
Experiments O

the O
CPNet O
architectures O
used O
in O
Something B-DAT

- O
Something B-DAT
[12] O
and O
Jester O
[30] O
experiments O

F. O
Per-class O
accuracy O
of O
Something B-DAT

Something B-DAT
and O
Jester O
models O

8: O
CPNet O
Architectures O
used O
in O
Something B-DAT

Something B-DAT
and O
Jester O
dataset O
experiments O

Jester O
in O
Figure O
8 O
and O
Something B-DAT

Something B-DAT
in O
Figure O
10 O

On O
Something B-DAT

Something B-DAT
dataset O
[12], O
the O
largest O
ac O

Kinetics O
[18] O
in O
Figure O
11, O
Something B-DAT

Something B-DAT
[12] O
in O
Figure O
12 O
and O

Something B-DAT
being O
deflected O
from O
something O

Something B-DAT
falling O
like O
a O
rock O

Something B-DAT
colliding O
with O
something O
and O
both O

Something B-DAT
falling O
like O
a O
feather O
or O

Something B-DAT
colliding O
with O
something O
and O
both O

accuracy O
gain O
in O
percentage O
on O
Something B-DAT

Something B-DAT
v2 O
dataset O
due O
to O
CP O

Turning O
something O
upside O
down” O
from O
Something B-DAT

Something B-DAT
v2 O
validation O
set O

label O
“Picking O
something O
up” O
from O
Something B-DAT

Something B-DAT
v2 O
validation O
set O

label O
“Moving O
something O
down” O
from O
Something B-DAT

Something B-DAT
v2 O
validation O
set O

something O
next O
to O
something” O
from O
Something B-DAT

Something B-DAT
v2 O
validation O
set O

on O
our O
final O
models O
on O
Something B-DAT

Something B-DAT
v2 O
dataset. O
Approach O
is O
the O

V2 B-DAT
[28] O
where O
the O
Something-V2 O
is O

V2 B-DAT
174 O
220,847 O
human-object O
interaction O
Jester O

set O
of O
Something-V1 O
and O
Something- O
V2 B-DAT
datasets O
are O
listed O
in O
Table O

V2 B-DAT
respec- O
tively. O
We O
further O
submit O

V2 B-DAT
Val O
Test O
Val O
Test O

V2 B-DAT
Dataset O
(Both O
Top1 O
and O
Top5 O

predict O
human-object O
interactions O
in O
the O
Something-Something B-DAT
dataset O
and O
identify O
various O
human O

on O
three O
recent O
video O
datasets O
(Something-Something B-DAT
[9], O
Jester O
[10], O
and O
Charades O

on O
sequential O
activity O
recog- O
nition: O
Something-Something B-DAT
dataset O
[9] O
is O
collected O
for O

highly O
competitive O
results O
on O
the O
Something-Something B-DAT
dataset O
for O
human-interaction O
recognition O
[9 O

statistics O
of O
the O
three O
datasets O
Something-Something B-DAT
dataset O
(Something- O
V1 O
[9] O
and O

3.2 O
Results O
on O
Something-Something B-DAT
Dataset O

Something-Something B-DAT
is O
a O
recent O
video O
dataset O

3: O
Prediction O
examples O
on O
a) O
Something-Something, B-DAT
b) O
Jester, O
and O
c) O
Cha O

For O
each O
example O
drawn O
from O
Something-Something B-DAT
and O
Jester, O
the O
top O
two O

the O
validation O
set O
of O
the O
Something-Something B-DAT
dataset O

of O
videos O
from O
the O
(a) O
Something-Something B-DAT
and O
(b) O
Jester O
datasets O
using O

The O
significant O
difference O
on O
the O
Something-Something B-DAT
dataset O
shows O
the O
importance O
of O

shuffled O
inputs O
drawn O
from O
the O
Something-Something B-DAT
dataset, O
in O
Figure O
6b. O
In O

Something-Something B-DAT
Ordered O
Shuffled O

frames O
and O
shuffled O
frames, O
on O
Something-Something B-DAT
and O
UCF101 O
dataset O
respectively. O
On O

can O
better O
differentiate O
activities O
in O
Something-Something B-DAT
dataset O

three O
recent O
video O
datasets O
- O
Something B-DAT

- O
Something, B-DAT
Jester, O
and O
Charades O
- O
which O

predict O
human-object O
interactions O
in O
the O
Something B-DAT

Something B-DAT
dataset O
and O
identify O
various O
human O

on O
three O
recent O
video O
datasets O
(Something B-DAT

Something B-DAT
[9], O
Jester O
[10], O
and O
Charades O

on O
sequential O
activity O
recog- O
nition: O
Something B-DAT

Something B-DAT
dataset O
[9] O
is O
collected O
for O

highly O
competitive O
results O
on O
the O
Something B-DAT

Something B-DAT
dataset O
for O
human-interaction O
recognition O
[9 O

statistics O
of O
the O
three O
datasets O
Something B-DAT

-Something B-DAT
dataset O
(Something O

- O
V1 O
[9] O
and O
Something B-DAT

-V2 O
[28] O
where O
the O
Something B-DAT

Something-V1 B-DAT
174 O
108,499 O
human-object O
interaction O
Something O

3.2 O
Results O
on O
Something B-DAT

Something B-DAT
Dataset O

Something B-DAT

Something B-DAT
is O
a O
recent O
video O
dataset O

are O
challenging, O
such O
as O
‘Tearing O
Something B-DAT
into O
two O
pieces’ O
versus O
‘Tearing O

Something B-DAT
just O
a O
little O
bit’, O
‘Turn O

set O
and O
test O
set O
of O
Something B-DAT

-V1 O
and O
Something B-DAT

the O
valida- O
tion O
set O
of O
Something B-DAT

-v1 O
and O
Something B-DAT

on O
the O
validation O
set O
of O
Something B-DAT

-V1 O
and O
Something B-DAT

Something-V1 B-DAT
Something O

and O
test O
set O
of O
the O
Something B-DAT

-V1 O
Dataset O
(Top1 O
Accuracy) O
and O
Something B-DAT

the O
validation O
set O
of O
the O
Something B-DAT

3: O
Prediction O
examples O
on O
a) O
Something B-DAT

Something, B-DAT
b) O
Jester, O
and O
c) O
Cha O

For O
each O
example O
drawn O
from O
Something B-DAT

Something B-DAT
and O
Jester, O
the O
top O
two O

highlight O
a O
pattern O
characteristic O
to O
Something B-DAT

the O
validation O
set O
of O
the O
Something B-DAT

Something B-DAT
dataset O

of O
videos O
from O
the O
(a) O
Something B-DAT

Something B-DAT
and O
(b) O
Jester O
datasets O
using O

The O
significant O
difference O
on O
the O
Something B-DAT

Something B-DAT
dataset O
shows O
the O
importance O
of O

shuffled O
inputs O
drawn O
from O
the O
Something B-DAT

Something B-DAT
dataset, O
in O
Figure O
6b. O
In O

Something B-DAT

Something B-DAT
Ordered O
Shuffled O

frames O
and O
shuffled O
frames, O
on O
Something B-DAT

Something B-DAT
and O
UCF101 O
dataset O
respectively. O
On O

Something- B-DAT
Something, O
the O
temporal O
order O
is O
critical O

Dataset O
UCF O
Kinetics O
Moments O
Something B-DAT
Jester O
Charades O

can O
better O
differentiate O
activities O
in O
Something B-DAT

Something B-DAT
dataset O

using O
the O
MultiScale O
TRN O
on O
Something B-DAT

- O
Something B-DAT
and O
Jester O
dataset. O
Only O
the O

Something B-DAT
Jester O
Frames O
baseline O
TRN O
baseline O

Something B-DAT
V2 I-DAT
[12]. O
We O
show O
that O
when O

Something B-DAT
V2 I-DAT
[12 O

Something B-DAT
V2 I-DAT
[12] O
are O
constructed O
to O
serve O

Something B-DAT
V2, I-DAT
we O
construct O
a O
few-shot O
dataset O

Something B-DAT
V2 I-DAT
dataset O
incorporates O
an O
assumption O
of O

Kinetics O
Something B-DAT
V2 I-DAT
Method O
1-shot O
5-shot O
1-shot O
5-shot O

both O
the O
Kinetics O
and O
Something- O
Something B-DAT
V2 I-DAT
datasets O
are O
listed O
in O
Table O

Kinetics O
Something B-DAT
V2 I-DAT
matching O
type O
1-shot O
5-shot O
1-shot O

Something B-DAT
V2 I-DAT
than O
that O
of O
Kinetics, O
so O

Something B-DAT
V2, I-DAT
while O
the O
gap O
is O
closed O

Something B-DAT
V2 I-DAT

recognition O
datasets: O
Kinetics O
[17] O
and O
Something-Something B-DAT
V2 I-DAT
[12]. O
We O
show O
that O
when O

recognition O
datasets, O
Kinetics O
[17] O
and O
Something-Something B-DAT
V2 I-DAT
[12 O

Kinetics O
[17] O
and O
Something-Something B-DAT
V2 I-DAT
[12] O
are O
constructed O
to O
serve O

split O
for O
few-shot O
classification O
on O
Something-Something B-DAT
V2, I-DAT
we O
construct O
a O
few-shot O
dataset O

training. O
Since O
the O
label O
in O
Something-Something B-DAT
V2 I-DAT
dataset O
incorporates O
an O
assumption O
of O

clues O
in O
each O
frame O
of O
Something-Something B-DAT
V2 I-DAT
than O
that O
of O
Kinetics, O
so O

using O
Mean O
is O
prominent O
for O
Something-Something B-DAT
V2, I-DAT
while O
the O
gap O
is O
closed O

Something-Something B-DAT
V2 I-DAT

V2, B-DAT
and O
show O
that O
our O
model O

datasets: O
Kinetics O
[17] O
and O
Something-Something O
V2 B-DAT
[12]. O
We O
show O
that O
when O

V2 B-DAT
[12], O
in O
which O
the O
videos O

datasets, O
Kinetics O
[17] O
and O
Something-Something O
V2 B-DAT
[12 O

Kinetics O
[17] O
and O
Something-Something O
V2 B-DAT
[12] O
are O
constructed O
to O
serve O

for O
few-shot O
classification O
on O
Something-Something O
V2, B-DAT
we O
construct O
a O
few-shot O
dataset O

Since O
the O
label O
in O
Something-Something O
V2 B-DAT
dataset O
incorporates O
an O
assumption O
of O

Kinetics O
Something O
V2 B-DAT
Method O
1-shot O
5-shot O
1-shot O
5-shot O

the O
Kinetics O
and O
Something- O
Something O
V2 B-DAT
datasets O
are O
listed O
in O
Table O

Kinetics O
Something O
V2 B-DAT
matching O
type O
1-shot O
5-shot O
1-shot O

in O
each O
frame O
of O
Something-Something O
V2 B-DAT
than O
that O
of O
Kinetics, O
so O

Mean O
is O
prominent O
for O
Something-Something O
V2, B-DAT
while O
the O
gap O
is O
closed O

Something-Something O
V2 B-DAT

recognition O
datasets: O
Kinetics O
[17] O
and O
Something-Something B-DAT
V2 I-DAT
[12]. O
We O
show O
that O
when O

crowd-sourced O
videos: O
Jester[1], O
Charades O
[34], O
Something-Something B-DAT
V1 I-DAT

recognition O
datasets, O
Kinetics O
[17] O
and O
Something-Something B-DAT
V2 I-DAT
[12 O

Kinetics O
[17] O
and O
Something-Something B-DAT
V2 I-DAT
[12] O
are O
constructed O
to O
serve O

split O
for O
few-shot O
classification O
on O
Something-Something B-DAT
V2, I-DAT
we O
construct O
a O
few-shot O
dataset O

training. O
Since O
the O
label O
in O
Something-Something B-DAT
V2 I-DAT
dataset O
incorporates O
an O
assumption O
of O

clues O
in O
each O
frame O
of O
Something-Something B-DAT
V2 I-DAT
than O
that O
of O
Kinetics, O
so O

using O
Mean O
is O
prominent O
for O
Something-Something B-DAT
V2, I-DAT
while O
the O
gap O
is O
closed O

Something-Something B-DAT
V2 I-DAT

recognition O
datasets: O
Kinetics O
[17] O
and O
Something-Something B-DAT
V2 O
[12]. O
We O
show O
that O

crowd-sourced O
videos: O
Jester[1], O
Charades O
[34], O
Something-Something B-DAT
V1&V2 O
[12], O
in O
which O
the O

recognition O
datasets, O
Kinetics O
[17] O
and O
Something-Something B-DAT
V2 O
[12 O

Kinetics O
[17] O
and O
Something-Something B-DAT
V2 O
[12] O
are O
constructed O
to O

split O
for O
few-shot O
classification O
on O
Something-Something B-DAT
V2, O
we O
construct O
a O
few-shot O

training. O
Since O
the O
label O
in O
Something-Something B-DAT
V2 O
dataset O
incorporates O
an O
assumption O

clues O
in O
each O
frame O
of O
Something-Something B-DAT
V2 O
than O
that O
of O
Kinetics O

using O
Mean O
is O
prominent O
for O
Something-Something B-DAT
V2, O
while O
the O
gap O
is O

Something-Something B-DAT
V2 O

challenging O
real-world O
datasets, O
Kinetics O
and O
Something B-DAT

- O
Something B-DAT

recognition O
datasets: O
Kinetics O
[17] O
and O
Something B-DAT

Something B-DAT
V2 O
[12]. O
We O
show O
that O

crowd-sourced O
videos: O
Jester[1], O
Charades O
[34], O
Something B-DAT

Something B-DAT
V1&V2 O
[12], O
in O
which O
the O

recognition O
datasets, O
Kinetics O
[17] O
and O
Something B-DAT

Something B-DAT
V2 O
[12 O

Kinetics O
[17] O
and O
Something B-DAT

Something B-DAT
V2 O
[12] O
are O
constructed O
to O

split O
for O
few-shot O
classification O
on O
Something B-DAT

Something B-DAT
V2, O
we O
construct O
a O
few-shot O

training. O
Since O
the O
label O
in O
Something B-DAT

Something B-DAT
V2 O
dataset O
incorporates O
an O
assumption O

Kinetics O
Something B-DAT
V2 O
Method O
1-shot O
5-shot O
1-shot O

on O
both O
the O
Kinetics O
and O
Something B-DAT

- O
Something B-DAT
V2 O
datasets O
are O
listed O
in O

Kinetics O
Something B-DAT
V2 O
matching O
type O
1-shot O
5-shot O

clues O
in O
each O
frame O
of O
Something B-DAT

Something B-DAT
V2 O
than O
that O
of O
Kinetics O

using O
Mean O
is O
prominent O
for O
Something B-DAT

Something B-DAT
V2, O
while O
the O
gap O
is O

Something B-DAT

Something B-DAT
V2 O

Something-Something B-DAT
(ours) O
human-object O
interaction O
108,499 O
4.03s O

Something B-DAT

Something B-DAT
(ours) O
human-object O
interaction O
108,499 O
4.03s O

Something B-DAT

Something B-DAT

Something B-DAT

Something B-DAT

Something B-DAT

surface O
without O
it O
falling O
down O
Something B-DAT
(not) O
falling O
over O
an O
edgeMoving O

Something B-DAT

like O
a O
feather O
or O
paper O
Something B-DAT
falling[Something] O
falling O
like O
a O
rock O

they O
collide O
with O
each O
other O
Something B-DAT
passing/hitting O
another O
thingMoving O
[something] O
and O

V2 B-DAT
[28]. O
For O
UCF101 O
and O
HMDB51 O

V2) B-DAT
[28] O
dataset. O
20BN-V2 O
has O
over O

V2 B-DAT
dataset O
[28]. O
We O
report O
top-1/top-5 O

V2 B-DAT

V2 B-DAT
in O
Table O
3. O
With O
1/5 O

Something-Something B-DAT
[11, O
28]) O
with O
almost O
no O

on O
the O
chal- O
lenging O
20BN O
Something-Something B-DAT

Something B-DAT

Something B-DAT
[11, O
28]) O
with O
almost O
no O

on O
the O
chal- O
lenging O
20BN O
Something B-DAT

Something B-DAT

Evaluations O
of O
pipelines O
on O
the O
HMDB B-DAT

HMDB B-DAT

Evaluations O
of O
pipelines O
on O
the O
HMDB B-DAT

1 O
presents O
results O
on O
the O
HMDB B-DAT

the O
classification O
accuracy O
on O
the O
HMDB B-DAT

Figure O
5b O
illustrates O
on O
the O
HMDB B-DAT

4 O
shows O
results O
on O
the O
HMDB B-DAT

fig. O
5b) O
sketching O
on O
the O
HMDB B-DAT

state O
of O
the O
art O
on O
HMDB B-DAT

For O
HMDB B-DAT

and O
ground O
truth O
representations O
on O
HMDB B-DAT

Tomaso O
Poggio, O
and O
Thomas O
Serre. O
HMDB B-DAT

Evaluations O
of O
pipelines O
on O
the O
HMDB-51 B-DAT
dataset. O
We O
compare O
(HAF O
only O

HMDB-51 B-DAT
[41] O
consists O
of O
6766 O
internet O

Evaluations O
of O
pipelines O
on O
the O
HMDB-51 B-DAT
dataset. O
We O
compare O
(HAF+BoW/FV O
halluc O

1 O
presents O
results O
on O
the O
HMDB-51 B-DAT
dataset. O
As O
expected, O
the O
(HAF O

the O
classification O
accuracy O
on O
the O
HMDB-51 B-DAT
dataset O
(split O
1) O
when O
our O

Figure O
5b O
illustrates O
on O
the O
HMDB-51 B-DAT
dataset O
(split O
1) O
that O
applying O

4 O
shows O
results O
on O
the O
HMDB-51 B-DAT
dataset. O
For O

fig. O
5b) O
sketching O
on O
the O
HMDB-51 B-DAT
dataset O
(split O
1 O
only O

state O
of O
the O
art O
on O
HMDB-51 B-DAT

For O
HMDB-51 B-DAT
and O
YUP++, O
we O
use O
the O

and O
ground O
truth O
representations O
on O
HMDB-51 B-DAT
(split O
1). O
Experi- O
ments O
in O

51 B-DAT

51 B-DAT
dataset. O
We O
compare O
(HAF O
only O

51 B-DAT
[41] O
consists O
of O
6766 O
internet O

videos O
over O
51 B-DAT
classes; O
each O
video O
has O
∼20–1000 O

51 B-DAT
dataset. O
We O
compare O
(HAF+BoW/FV O
halluc O

51 B-DAT
dataset. O
As O
expected, O
the O
(HAF O

51 B-DAT
dataset O
(split O
1) O
when O
our O

51 B-DAT
dataset O
(split O
1) O
that O
applying O

51 B-DAT
dataset. O
For O

51 B-DAT
dataset O
(split O
1 O
only O

51 B-DAT

denotes O
human-centric O
pre-processing O
for O
512 B-DAT
pixels O
(height O

firstly O
resize O
RGB O
frames O
to O
512 B-DAT
pixels O
(height) O
rather O
than O
256 O

but O
computed O
for O
the O
increased O
512 B-DAT
pixel O
resolution O
(height) O
denoted O
by O

51 B-DAT
and O
YUP++, O
we O
use O
the O

51 B-DAT
(split O
1). O
Experi- O
ments O
in O

513, B-DAT
Mar. O
2011. O
3 O

51 B-DAT

Evaluations O
of O
pipelines O
on O
the O
HMDB-51 B-DAT
dataset. O
We O
compare O
(HAF O
only O

HMDB-51 B-DAT
[41] O
consists O
of O
6766 O
internet O

Evaluations O
of O
pipelines O
on O
the O
HMDB-51 B-DAT
dataset. O
We O
compare O
(HAF+BoW/FV O
halluc O

1 O
presents O
results O
on O
the O
HMDB-51 B-DAT
dataset. O
As O
expected, O
the O
(HAF O

the O
classification O
accuracy O
on O
the O
HMDB-51 B-DAT
dataset O
(split O
1) O
when O
our O

Figure O
5b O
illustrates O
on O
the O
HMDB-51 B-DAT
dataset O
(split O
1) O
that O
applying O

4 O
shows O
results O
on O
the O
HMDB-51 B-DAT
dataset. O
For O

fig. O
5b) O
sketching O
on O
the O
HMDB-51 B-DAT
dataset O
(split O
1 O
only O

state O
of O
the O
art O
on O
HMDB-51 B-DAT

For O
HMDB-51 B-DAT
and O
YUP++, O
we O
use O
the O

and O
ground O
truth O
representations O
on O
HMDB-51 B-DAT
(split O
1). O
Experi- O
ments O
in O

classifying O
“drink” O
and O
“eat” O
from O
HMDB B-DAT

from O
experiment O
of O
TSN O
on O
HMDB B-DAT

baseline O
model O
TSN O
[38] O
on O
HMDB B-DAT

240 O
spatial O
resolution. O
The O
HMDB B-DAT

test O
set O
of O
UCF-101 O
and O
HMDB B-DAT

in O
Table O
1 O
on O
UCF-101, O
HMDB B-DAT

on O
UCF-101 O
and O
by O
22.9%on O
HMDB B-DAT

on O
UCF-101 O
and O
81.9% O
on O
HMDB B-DAT

Overall, O
our O
result O
81.9% O
on O
HMDB B-DAT

METHODS O
ON O
THE O
UCF-101 O
AND O
HMDB B-DAT

Methods O
Pre-train O
dataset O
UCF-101 O
HMDB B-DAT

dataset, O
and O
7.8% O
on O
the O
HMDB51, B-DAT
which O
demonstrates O
that O
conducting O
modality O

COMPONENTS O
ON O
THE O
UCF-101 O
, O
HMDB B-DAT

achieves O
the O
best O
result O
on O
HMDB B-DAT

classifying O
“drink” O
and O
“eat” O
from O
HMDB-51 B-DAT
[15] O
dataset. O
The O
input O
consist O

from O
experiment O
of O
TSN O
on O
HMDB-51 B-DAT
dataset O

baseline O
model O
TSN O
[38] O
on O
HMDB-51 B-DAT
[15], O
of O
that O
case. O
Our O

240 O
spatial O
resolution. O
The O
HMDB-51 B-DAT
dataset O
has O
6766 O
video O
clips O

test O
set O
of O
UCF-101 O
and O
HMDB-51 B-DAT
datasets O

in O
Table O
1 O
on O
UCF-101, O
HMDB-51 B-DAT
(split O
1) O
and O
something-something-V2 O
dataset O

on O
UCF-101 O
and O
by O
22.9%on O
HMDB-51 B-DAT

on O
UCF-101 O
and O
81.9% O
on O
HMDB-51 B-DAT

Overall, O
our O
result O
81.9% O
on O
HMDB-51 B-DAT
clearly O
sets O
a O
new O
state-of-the-art O

METHODS O
ON O
THE O
UCF-101 O
AND O
HMDB-51 B-DAT
DATASETS O
(SPLIT O
1). O
WE O
REPORT O

Methods O
Pre-train O
dataset O
UCF-101 O
HMDB- B-DAT

COMPONENTS O
ON O
THE O
UCF-101 O
, O
HMDB-51 B-DAT
DATASETS. O
“METHOD” O
DENOTES O
THE O
COMPONENT O

achieves O
the O
best O
result O
on O
HMDB- B-DAT
51 O
dataset. O
This O
verifies O
that O

51 B-DAT
[15] O
dataset. O
The O
input O
consist O

51 B-DAT
dataset O

51 B-DAT
[15], O
of O
that O
case. O
Our O

51 B-DAT
dataset O
has O
6766 O
video O
clips O

with O
51 B-DAT
categories. O
something-something-v2 O
an O
interesting O
temporal O

51 B-DAT
datasets O

51 B-DAT
(split O
1) O
and O
something-something-V2 O
dataset O

51 B-DAT

51 B-DAT

51 B-DAT
clearly O
sets O
a O
new O
state-of-the-art O

51 B-DAT
DATASETS O
(SPLIT O
1). O
WE O
REPORT O

30] O
sports-1M O
82.3 O
- O
- O
51 B-DAT

Baseline O
51 B-DAT

51 B-DAT
DATASETS. O
“METHOD” O
DENOTES O
THE O
COMPONENT O

the O
best O
result O
on O
HMDB- O
51 B-DAT
dataset. O
This O
verifies O
that O
the O

516 B-DAT

classifying O
“drink” O
and O
“eat” O
from O
HMDB-51 B-DAT
[15] O
dataset. O
The O
input O
consist O

from O
experiment O
of O
TSN O
on O
HMDB-51 B-DAT
dataset O

baseline O
model O
TSN O
[38] O
on O
HMDB-51 B-DAT
[15], O
of O
that O
case. O
Our O

240 O
spatial O
resolution. O
The O
HMDB-51 B-DAT
dataset O
has O
6766 O
video O
clips O

test O
set O
of O
UCF-101 O
and O
HMDB-51 B-DAT
datasets O

in O
Table O
1 O
on O
UCF-101, O
HMDB-51 B-DAT
(split O
1) O
and O
something-something-V2 O
dataset O

on O
UCF-101 O
and O
by O
22.9%on O
HMDB-51 B-DAT

on O
UCF-101 O
and O
81.9% O
on O
HMDB-51 B-DAT

Overall, O
our O
result O
81.9% O
on O
HMDB-51 B-DAT
clearly O
sets O
a O
new O
state-of-the-art O

METHODS O
ON O
THE O
UCF-101 O
AND O
HMDB-51 B-DAT
DATASETS O
(SPLIT O
1). O
WE O
REPORT O

COMPONENTS O
ON O
THE O
UCF-101 O
, O
HMDB-51 B-DAT
DATASETS. O
“METHOD” O
DENOTES O
THE O
COMPONENT O

larly O
on O
HMDB51 B-DAT
(split-1), O
MARS O
obtains O
80.1% O
ac O

art O
on O
the O
HMDB51 B-DAT
[20] O
and O
UCF101 O
[33] O
datasets O

tion: O
Kinetics400 O
[18], O
HMDB51 B-DAT
[20], O
UCF101 O
[33], O
and O

troduced O
by O
[44]. O
HMDB51 B-DAT
consists O
of O
51 O
action O
classes O

the O
first O
split O
for O
HMDB51 B-DAT
and O
UCF101 O
as O
HMDB51-1 O
and O

UCF101, O
and O
HMDB51 B-DAT
[11]. O
Following O
the O
setting O
of O

the O
smaller O
HMDB51 B-DAT
and O
UCF101 O
datasets O

Stream O
MiniKinetics O
Kinetics400 O
UCF101-1 O
HMDB51 B-DAT

trained O
from O
scratch. O
For O
UCF101-1, O
HMDB51 B-DAT

Stream O
Kinetics400 O
UCF101-1 O
HMDB51 B-DAT

HMDB51 B-DAT

RGB O
on O
Kinetics400, O
UCF101- O
1, O
HMDB51 B-DAT

streams O
alone O
on O
MiniKinetics, O
UCF101, O
HMDB51 B-DAT

HMDB51 B-DAT

In O
some O
cases, O
e.g., O
on O
HMDB51 B-DAT
with O
64f O

ble O
4 O
for O
UCF101, O
HMDB51 B-DAT
and O
SomethingSomethingv1 O

where O
motion O
is O
important, O
as O
HMDB51 B-DAT

not O
important. O
In O
contrast, O
on O
HMDB51 B-DAT
and O
SomethingSomethingv1, O
the O
gain O
is O

Method O
Streams O
Pretrain O
UCF101 O
HMDB51 B-DAT
Something O

sults O
of O
UCF101 O
and O
HMDB51 B-DAT
are O
averaged O
over O
3 O
splits O

ics400, O
UCF101, O
HMDB51, B-DAT
and O
SomethingSomethingv1 O

Tomaso O
Poggio, O
and O
Thomas O
Serre. O
HMDB B-DAT

by O
[44]. O
HMDB51 B-DAT
consists O
of O
51 O
action O
classes O

RGB+Flow O
74.5 O
97.5 O
79.8 O
51 B-DAT

MARS+RGB O
74.8 O
97.3 O
80.6 O
51 B-DAT

MARS+RGB O
RGB O
Kinetics O
97.6 O
79.5 O
51 B-DAT

on O
challenging O
human O
action O
datasets: O
HMDB51, B-DAT
UCF101, O
and O
Kinetics. O
The O
dataset O

achieves O
state-of-the-art O
results O
on O
the O
HMDB51, B-DAT
UCF101 O
and O
Kinetics O
datasets. O
In O

remarkable O
performance O
on O
UCF101 O
(96.9%), O
HMDB51 B-DAT
(74.5%) O
and O
Kinetics O
(73.5 O

influential O
action O
datasets O
available. O
The O
HMDB51 B-DAT
[24] O
and O
UCF101 O
[37] O
datasets O

Event O
Attribute O
Concept O
#Videos O
Year O
HMDB51 B-DAT
[24] O
- O
- O
51 O

first O
ones O
were O
KTH O
[25], O
HMDB51 B-DAT
[24], O
and O
UCF101 O
[37] O
that O

Pre-Training O
Dataset O
UCF101 O
HMDB51 B-DAT
Kinetics O
From O
Scratch O
65.2 O
33.4 O

Pre-Trained O
Dataset O
CNN O
Backbone O
UCF101 O
HMDB51 B-DAT
Kinetics O
Two O
Stream O
(spatial O
stream O

State-of-the-art O
performance O
comparison O
on O
UCF101, O
HMDB51 B-DAT
test O
sets O
and O
Kinetics O
validation O

The O
results O
on O
UCF101 O
and O
HMDB51 B-DAT
are O
average O
mAP O
over O
three O

and O
then O
fine-tuning O
on O
UCF101, O
HMDB51 B-DAT
and O
Kinetics. O
Obvi- O
ously, O
there O

datasets O
(i.e. O
HVU, O
Kinetics⇒UCF101 O
and O
HMDB51 B-DAT

5.5. O
Comparison O
on O
UCF, O
HMDB, B-DAT
Kinetics O

the O
state-of-the-art O
methods O
on O
UCF101, O
HMDB51 B-DAT
and O
Ki- O
netics. O
For O
our O

target O
datasets. O
For O
UCF101 O
and O
HMDB51, B-DAT
we O
report O
the O
average O
accuracy O

5.5 O
. O
Comparison O
on O
UCF, O
HMDB, B-DAT
Kinetics O

10, O
16, O
38, O
39, O
48, O
51 B-DAT

51 B-DAT

Year O
HMDB51 B-DAT
[24] O
- O
- O
51 O
- O
- O
- O
7K O
’11 O

C3D O
[40] O
Sport1M O
VGG11 O
82.3 O
51 B-DAT

51 B-DAT

such O
as O
Kinetics, O
UCF-101 O
and O
HMDB B-DAT

results O
on O
UCF-101 O
[31] O
and O
HMDB B-DAT

art O
on O
Kinetics, O
UCF-101, O
and O
HMDB B-DAT

Model O
Pre-train O
UCF-101 O
HMDB B-DAT

8: O
Comparisons O
on O
UCF-101 O
and O
HMDB B-DAT

datasets, O
i.e., O
Kinetics, O
UCF-101 O
and O
HMDB B-DAT

also O
achieved O
on O
UCF-101 O
and O
HMDB B-DAT

such O
as O
Kinetics, O
UCF-101 O
and O
HMDB-51 B-DAT

results O
on O
UCF-101 O
[31] O
and O
HMDB-51 B-DAT
[23]. O
These O
datasets O
are O
much O

art O
on O
Kinetics, O
UCF-101, O
and O
HMDB-51 B-DAT

Model O
Pre-train O
UCF-101 O
HMDB- B-DAT

8: O
Comparisons O
on O
UCF-101 O
and O
HMDB-51 B-DAT

datasets, O
i.e., O
Kinetics, O
UCF-101 O
and O
HMDB-51 B-DAT

also O
achieved O
on O
UCF-101 O
and O
HMDB-51 B-DAT
(Table O
8). O
It O
shows O
learning O

51 B-DAT

1×1×1, O
1281×3×3, O
128 O
1×1×1, O
512 B-DAT

1×3×3, O
288 O
3×1×1, O
128 O
1×1×1, O
512 B-DAT

1×1×1, O
5121×3×3, B-DAT
512 O
1×1×1, O
2048 O

1×1×1, O
512 B-DAT
1×3×3, O
1152 O
3×1×1, O
512 O

51 B-DAT
[23]. O
These O
datasets O
are O
much O

51 B-DAT

51 B-DAT

51 B-DAT

51 B-DAT
(Table O
8). O
It O
shows O
learning O

such O
as O
Kinetics, O
UCF-101 O
and O
HMDB-51 B-DAT

results O
on O
UCF-101 O
[31] O
and O
HMDB-51 B-DAT
[23]. O
These O
datasets O
are O
much O

art O
on O
Kinetics, O
UCF-101, O
and O
HMDB-51 B-DAT

8: O
Comparisons O
on O
UCF-101 O
and O
HMDB-51 B-DAT

datasets, O
i.e., O
Kinetics, O
UCF-101 O
and O
HMDB-51 B-DAT

also O
achieved O
on O
UCF-101 O
and O
HMDB-51 B-DAT
(Table O
8). O
It O
shows O
learning O

HMDB B-DAT

exper- O
iment O
10-times O
on O
the O
HMDB B-DAT

experiments O
use O
ResNet-152 O
features O
on O
HMDB B-DAT

HMDB B-DAT

HMDB B-DAT

We O
used O
three O
splits O
for O
HMDB B-DAT

achieve O
9% O
improvement O
on O
the O
HMDB B-DAT

on O
each O
dataset. O
On O
the O
HMDB B-DAT

and O
when O
fine-tuned O
on O
the O
HMDB B-DAT

sequences O
of O
increasing O
lengths O
in O
HMDB B-DAT

81.5%) O
is O
not O
significant O
(on O
HMDB B-DAT

To O
this O
end, O
we O
re-categorized O
HMDB B-DAT

raw O
RGB O
frames) O
on O
an O
HMDB B-DAT

to O
raw O
RGB O
frames O
from O
HMDB B-DAT

E., O
Poggio, O
T., O
Serre, O
T.: O
HMDB B-DAT

end-to-end O
learning O
setup O
on O
the O
HMDB B-DAT

our O
end-to-end O
training O
setup O
on O
HMDB B-DAT

performance O
of O
this O
experiment O
on O
HMDB B-DAT

classifying O
the O
DSP O
descriptors O
(on O
HMDB B-DAT

and O
SVM-based O
DSP O
classification O
on O
HMDB B-DAT

L)inear O
and O
non-linear O
(NL) O
on O
HMDB B-DAT

bi-directional) O
rank O
pooling O
(GRP) O
on O
HMDB B-DAT

HMDB-51 B-DAT
[29]: O
is O
a O
popular O
video O

exper- O
iment O
10-times O
on O
the O
HMDB-51 B-DAT
split-1 O
features. O
In O
Figure O
2(a O

experiments O
use O
ResNet-152 O
features O
on O
HMDB-51 B-DAT
split-1 O
with O
a O
fooling O
rate O

HMDB-51 B-DAT
NTU-RGBD O
YUP O

HMDB-51 B-DAT

We O
used O
three O
splits O
for O
HMDB-51 B-DAT

achieve O
9% O
improvement O
on O
the O
HMDB-51 B-DAT
dataset O
split-1 O
and O
5%−8% O
improvement O

on O
each O
dataset. O
On O
the O
HMDB-51 B-DAT
dataset, O
we O
also O
report O
accuracy O

and O
when O
fine-tuned O
on O
the O
HMDB-51 B-DAT
leads O
to O
about O
80.9% O
accuracy O

sequences O
of O
increasing O
lengths O
in O
HMDB-51 B-DAT
split-1 O

81.5%) O
is O
not O
significant O
(on O
HMDB-51 B-DAT

To O
this O
end, O
we O
re-categorized O
HMDB-51 B-DAT
into O
subsets O
of O
sequences O
according O

raw O
RGB O
frames) O
on O
an O
HMDB-51 B-DAT
video O
sequences. O
First O
column O
shows O

to O
raw O
RGB O
frames O
from O
HMDB-51 B-DAT
videos O
– O
i.e., O
instead O
of O

end-to-end O
learning O
setup O
on O
the O
HMDB-51 B-DAT
dataset O
split1 O
using O
a O
ResNet-152 O

our O
end-to-end O
training O
setup O
on O
HMDB-51 B-DAT
split1 O

performance O
of O
this O
experiment O
on O
HMDB-51 B-DAT
split-1. O
For O
the O
MLP, O
we O

classifying O
the O
DSP O
descriptors O
(on O
HMDB-51 B-DAT
split1 O

and O
SVM-based O
DSP O
classification O
on O
HMDB-51 B-DAT
split-1 O
with O
ResNet152 O

L)inear O
and O
non-linear O
(NL) O
on O
HMDB-51 B-DAT
split-1 O
with O
ResNet152. O
Right:Comparison O
of O

bi-directional) O
rank O
pooling O
(GRP) O
on O
HMDB-51 B-DAT
(two-stream O
ResNet-152), O
NTU, O
and O
YUP O

51 B-DAT
[29]: O
is O
a O
popular O
video O

of O
6766 O
Internet O
videos O
over O
51 B-DAT
classes; O
each O
video O
is O
about O

51 B-DAT
split-1 O
features. O
In O
Figure O
2(a O

51 B-DAT
split-1 O
with O
a O
fooling O
rate O

51 B-DAT
NTU-RGBD O
YUP O

51 B-DAT

51 B-DAT

51 B-DAT
dataset O
split-1 O
and O
5%−8% O
improvement O

51 B-DAT
dataset, O
we O
also O
report O
accuracy O

51 B-DAT

51 B-DAT
leads O
to O
about O
80.9% O
accuracy O

classes O
51 B-DAT
49 O
34 O
27 O
23 O
21 O

51 B-DAT
split-1 O

51) B-DAT
in O
comparison O
to O
our O
results O

51 B-DAT
into O
subsets O
of O
sequences O
according O

51 B-DAT
video O
sequences. O
First O
column O
shows O

51 B-DAT
videos O
– O
i.e., O
instead O
of O

51 B-DAT

51 B-DAT
dataset O
split1 O
using O
a O
ResNet-152 O

51 B-DAT
split1 O

51 B-DAT
split-1. O
For O
the O
MLP, O
we O

then O
passed O
through O
a O
d× O
51 B-DAT
weight O
matrix O
learned O
against O
the O

51 B-DAT
split1 O

51 B-DAT
split-1 O
with O
ResNet152 O

51 B-DAT
split-1 O
with O
ResNet152. O
Right:Comparison O
of O

51 B-DAT
(two-stream O
ResNet-152), O
NTU, O
and O
YUP O

HMDB-51 B-DAT
[29]: O
is O
a O
popular O
video O

exper- O
iment O
10-times O
on O
the O
HMDB-51 B-DAT
split-1 O
features. O
In O
Figure O
2(a O

experiments O
use O
ResNet-152 O
features O
on O
HMDB-51 B-DAT
split-1 O
with O
a O
fooling O
rate O

HMDB-51 B-DAT
NTU-RGBD O
YUP O

HMDB-51 B-DAT

We O
used O
three O
splits O
for O
HMDB-51 B-DAT

achieve O
9% O
improvement O
on O
the O
HMDB-51 B-DAT
dataset O
split-1 O
and O
5%−8% O
improvement O

on O
each O
dataset. O
On O
the O
HMDB-51 B-DAT
dataset, O
we O
also O
report O
accuracy O

and O
when O
fine-tuned O
on O
the O
HMDB-51 B-DAT
leads O
to O
about O
80.9% O
accuracy O

sequences O
of O
increasing O
lengths O
in O
HMDB-51 B-DAT
split-1 O

81.5%) O
is O
not O
significant O
(on O
HMDB-51) B-DAT
in O
comparison O
to O
our O
results O

To O
this O
end, O
we O
re-categorized O
HMDB-51 B-DAT
into O
subsets O
of O
sequences O
according O

raw O
RGB O
frames) O
on O
an O
HMDB-51 B-DAT
video O
sequences. O
First O
column O
shows O

to O
raw O
RGB O
frames O
from O
HMDB-51 B-DAT
videos O
– O
i.e., O
instead O
of O

end-to-end O
learning O
setup O
on O
the O
HMDB-51 B-DAT
dataset O
split1 O
using O
a O
ResNet-152 O

our O
end-to-end O
training O
setup O
on O
HMDB-51 B-DAT
split1 O

performance O
of O
this O
experiment O
on O
HMDB-51 B-DAT
split-1. O
For O
the O
MLP, O
we O

classifying O
the O
DSP O
descriptors O
(on O
HMDB-51 B-DAT
split1 O

and O
SVM-based O
DSP O
classification O
on O
HMDB-51 B-DAT
split-1 O
with O
ResNet152 O

L)inear O
and O
non-linear O
(NL) O
on O
HMDB-51 B-DAT
split-1 O
with O
ResNet152. O
Right:Comparison O
of O

bi-directional) O
rank O
pooling O
(GRP) O
on O
HMDB-51 B-DAT
(two-stream O
ResNet-152), O
NTU, O
and O
YUP O

case O
into O
Fig. O
UCF101 O
HMDB51 B-DAT

HMDB51 B-DAT
under O
different O
cross-stream O
connections O

videos O
in O
101 O
categories O
and O
HMDB51 B-DAT
[18], O
consisting O

appearance, O
scale O
and O
pose. O
HMDB51 B-DAT
[18] O
is O
a O
more O
chal O

HMDB51 B-DAT

of O
UCF101 O
and O
HMDB51, B-DAT
resp. O
In O
comparison, O
merely O

first O
split O
of O
UCF101 O
and O
HMDB51 B-DAT

of O
a O
centre O
initialization O
on O
HMDB51 B-DAT

to O
the O
temporal O
nature O
of O
HMDB51 B-DAT
in O

comparison O
to O
UCF101: O
HMDB51 B-DAT
exhibits O
a O
higher O
degree O

temporal O
init. O
pool O
time O
UCF101 O
HMDB51 B-DAT

HMDB51 B-DAT
under O
different O
temporal O
filtering O
layers O

UCF101 O
HMDB51 B-DAT

and O
HMDB51 B-DAT

for O
the O
spatial O
network O
on O
HMDB51, B-DAT
which O
might O
be O
due O
to O

UCF101 O
and O
HMDB51, B-DAT
respectively. O
Here, O
the O
relative O

and O
HMDB51 B-DAT
datasets. O
We O
train O
50 O
and O

diction O
layer O
outputs. O
On O
HMDB51 B-DAT
we O
weight O
the O
temporal O

pearance O
network O
degrades O
performance O
on O
HMDB51 B-DAT
(we O

sizable O
gain O
on O
HMDB51 B-DAT

Method O
UCF101 O
HMDB51 B-DAT

HMDB51 B-DAT
and O
UCF101 O

and O
72.2% O
on O
HMDB51 B-DAT

still O
a O
3.7% O
increase O
on O
HMDB51, B-DAT
which O
we O
think O

to O
dominant O
camera O
motion O
in O
HMDB51 B-DAT
that O

HMDB B-DAT

1×1, O
128 O
3×3, O
128 O
1×1, O
512 B-DAT

1×1, O
512 B-DAT
3×3, O
512 O
1×1, O
2048 O

128 O
3×3, O
128 O
‡ O
1×1, O
512 B-DAT

1×1, O
512 B-DAT
3×3, O
512 O

1×1, O
128 O
3×3, O
128 O
1×1, O
512 B-DAT

1×1, O
512 B-DAT
3×3, O
512 O
1×1, O
2048 O

of O
6766 O
videos O
in O
51 B-DAT
categories. O
UCF101 O
provides O
videos O

511 B-DAT

513 B-DAT

i.e., O
Kinetics- O
400, O
UCF-101, O
and O
HMDB B-DAT

Jester O
[1], O
UCF101 O
[23] O
and O
HMDB B-DAT

datasets O
(i.e., O
Kinetics-400, O
UCF-101, O
and O
HMDB B-DAT

400 O
[2], O
UCF-101 O
[23] O
and O
HMDB B-DAT

randomly O
initialized. O
For O
UCF-101 O
and O
HMDB B-DAT

the O
STM O
on O
UCF-101 O
and O
HMDB B-DAT

Backbone O
Flow O
Pre-train O
Data O
UCF-101 O
HMDB B-DAT

scene-related O
datasets: O
Kinetics-400, O
UCF-101, O
and O
HMDB B-DAT

and O
13,320 O
clips O
in O
total. O
HMDB B-DAT

video O
clips. O
For O
UCF-101 O
and O
HMDB B-DAT

experiments O
on O
the O
UCF-101 O
and O
HMDB B-DAT

T. O
Poggio, O
and O
T. O
Serre. O
HMDB B-DAT

i.e., O
Kinetics- O
400, O
UCF-101, O
and O
HMDB-51 B-DAT

Jester O
[1], O
UCF101 O
[23] O
and O
HMDB-51 B-DAT
[17 O

datasets O
(i.e., O
Kinetics-400, O
UCF-101, O
and O
HMDB-51 B-DAT

400 O
[2], O
UCF-101 O
[23] O
and O
HMDB-51 B-DAT
[17] O
where O
the O
back- O
ground O

randomly O
initialized. O
For O
UCF-101 O
and O
HMDB-51, B-DAT
we O
use O
Kinetics O
pre-trained O
model O

the O
STM O
on O
UCF-101 O
and O
HMDB-51 B-DAT
compared O
with O
the O
state-of-the-art O
methods O

Backbone O
Flow O
Pre-train O
Data O
UCF-101 O
HMDB-51 B-DAT

scene-related O
datasets: O
Kinetics-400, O
UCF-101, O
and O
HMDB-51 B-DAT
in O
this O
section. O
Kinetics-400 O
is O

and O
13,320 O
clips O
in O
total. O
HMDB-51 B-DAT
is O
also O
a O
small O
video O

video O
clips. O
For O
UCF-101 O
and O
HMDB-51, B-DAT
we O
followed O
[33] O
to O
adopt O

experiments O
on O
the O
UCF-101 O
and O
HMDB-51 B-DAT
to O
study O
the O
generalization O
ability O

51) B-DAT
with O
the O
help O
of O
encoding O

51 B-DAT
[17 O

51 B-DAT

51 B-DAT
[17] O
where O
the O
back- O
ground O

51, B-DAT
we O
use O
Kinetics O
pre-trained O
model O

51 B-DAT
compared O
with O
the O
state-of-the-art O
methods O

51 B-DAT

27] O
3D O
VGG-11 O
Sports-1M O
82.3 O
51 B-DAT

51 B-DAT
in O
this O
section. O
Kinetics-400 O
is O

51 B-DAT
is O
also O
a O
small O
video O

dataset O
with O
51 B-DAT
classes O
and O
6766 O
labeled O
video O

51, B-DAT
we O
followed O
[33] O
to O
adopt O

51 B-DAT
to O
study O
the O
generalization O
ability O

516 B-DAT
66.5G O
32.0 O
V/s O
49.8 O

i.e., O
Kinetics- O
400, O
UCF-101, O
and O
HMDB-51) B-DAT
with O
the O
help O
of O
encoding O

Jester O
[1], O
UCF101 O
[23] O
and O
HMDB-51 B-DAT
[17 O

datasets O
(i.e., O
Kinetics-400, O
UCF-101, O
and O
HMDB-51 B-DAT

400 O
[2], O
UCF-101 O
[23] O
and O
HMDB-51 B-DAT
[17] O
where O
the O
back- O
ground O

randomly O
initialized. O
For O
UCF-101 O
and O
HMDB-51, B-DAT
we O
use O
Kinetics O
pre-trained O
model O

the O
STM O
on O
UCF-101 O
and O
HMDB-51 B-DAT
compared O
with O
the O
state-of-the-art O
methods O

Backbone O
Flow O
Pre-train O
Data O
UCF-101 O
HMDB-51 B-DAT

scene-related O
datasets: O
Kinetics-400, O
UCF-101, O
and O
HMDB-51 B-DAT
in O
this O
section. O
Kinetics-400 O
is O

and O
13,320 O
clips O
in O
total. O
HMDB-51 B-DAT
is O
also O
a O
small O
video O

video O
clips. O
For O
UCF-101 O
and O
HMDB-51, B-DAT
we O
followed O
[33] O
to O
adopt O

experiments O
on O
the O
UCF-101 O
and O
HMDB-51 B-DAT
to O
study O
the O
generalization O
ability O

across O
major O
datasets O
(UCF101 O
[41], O
HMDB51 B-DAT
[22] O
and O
20BN-Something-Something O
[11, O
28 O

Method O
Mean O
Class O
AccuracyUCF101 O
HMDB51 B-DAT

HMDB51 B-DAT

for O
this O
experiment: O
UCF101 O
and O
HMDB51 B-DAT

videos O
from O
101 O
action O
categories. O
HMDB51 B-DAT
[22] O
includes O
6,766 O
videos O
from O

flow O
stream O
on O
UCF101 O
and O
HMDB51 B-DAT
datasets. O
We O
also O
include O
results O

stream. O
For O
both O
UCF101 O
and O
HMDB51, B-DAT
the O
best O
performing O
method O
is O

performance O
of O
RGB O
stream O
on O
HMDB51 B-DAT

action O
recognition O
on O
UCF101 O
[41], O
HMDB51 B-DAT
[22] O
and O
a O
large O
scale O

dataset–20BN-V2 O
[28]. O
For O
UCF101 O
and O
HMDB51, B-DAT
we O
report O
mean O
class O
accu O

HMDB51 B-DAT

HMDB51 B-DAT

Method O
Mean O
Class O
AccuracyUCF101 O
HMDB51 B-DAT

action O
recognition O
on O
UCF101 O
and O
HMDB51 B-DAT

bet- O
ter O
than O
ResNet50 O
on O
HMDB51 B-DAT
[14]. O
The O
performance O
of O
our O

on O
UCF101 O
and O
-4.1% O
on O
HMDB51 B-DAT

Method O
Mean O
Class O
AccuracyUCF101 O
HMDB51 B-DAT
I3D O
RGB O
(backbone) O
94.8 O
70.9 O

HMDB51 B-DAT

on O
UCF101 O
and O
+1.3% O
on O
HMDB51 B-DAT

HMDB51 B-DAT
I3D O
RGB O
70.9 O
70.2 O
0.7 O

testing O
videos O
of O
UCF101 O
and O
HMDB51 B-DAT

HMDB51, B-DAT
under-performing O
our O
model O
by O

HMDB51 B-DAT

T. O
Poggio, O
and O
T. O
Serre. O
HMDB B-DAT

51 B-DAT

51 B-DAT

differs O
from O
[12, O
9, O
26, O
51 B-DAT

51, B-DAT
12, O
26] O
did O
not O
consider O

51, B-DAT
26 O

22] O
includes O
6,766 O
videos O
from O
51 B-DAT
action O
cat- O
egories. O
We O
evaluate O

51 B-DAT

51 B-DAT

51 B-DAT

51 B-DAT

51 B-DAT

51 B-DAT

51 B-DAT

Saliency O
Map O
(DSS O
[17]) O
51 B-DAT

51 B-DAT

a) O
Something-v1 O
(b) O
EPIC-KITCHENS O
(c) O
HMDB51 B-DAT

Something-v1 O
[3], O
EPIC-KITCHENS O
[23] O
and O
HMDB51 B-DAT
[24]. O
Something-v1 O
consists O
of O
86K O

obtained O
on O
the O
test O
set. O
HMDB51 B-DAT
dataset O
consists O
of O
videos O
collected O

to O
both O
actions O
and O
objects. O
HMDB51 B-DAT
is O
a O
smaller O
dataset O
with O

Model O
Backbone O
Pre-training O
Accuracy O
(%)Something-v1 O
HMDB51 B-DAT
TDN O
[2] O
ResNet-50 O
ImageNet O

validation O
set O
of O
something-v1 O
and O
HMDB51 B-DAT
datasets O

a) O
Something-v1 O
dataset O
(b) O
HMDB51 B-DAT
dataset O

on O
(a) O
Something-v1 O
and O
(b) O
HMDB51, B-DAT
plotted O
against O
the O
computational O
complexity O

Something-v1 O
and O
HMDB51 B-DAT

state-of-the-art O
techniques O
on O
Something-v1 O
and O
HMDB51 B-DAT
datasets. O
For O
Something-v1 O
dataset, O
in O

For O
HMDB51, B-DAT
we O
augment O
three O
baselines, O
TSN O

three O
baselines. O
As O
mentioned O
previously, O
HMDB51 B-DAT
consists O
of O
actions O
with O
shorter O

HMDB51 B-DAT

around O
6000 O
video O
clips O
from O
51 B-DAT
action O
categories. O
The O
dataset O
is O

51 B-DAT
2.06 O
TSN O
(FUSION) O
[1] O
48.23 O

51 B-DAT
6.73 O
73.40 O
33.77 O
18.64 O
19.98 O

of O
experiments O
on O
UCF101 O
and O
HMDB51 B-DAT
datasets. O
Our O
experiments O
show O
that O

public O
datasets O
UCF101 O
[24] O
and O
HMDB51 B-DAT
[25 O

two O
standard O
datasets O
UCF101 O
and O
HMDB51 B-DAT

accuracy O
over O
the O
three O
splits. O
HMDB51 B-DAT
con- O
tains O
6766 O
videos O
clips O

are O
performed O
on O
UCF101 O
and O
HMDB51 B-DAT
datasets O
(over O
all O
splits). O
The O

sample O
segments O
on O
UCF101 O
and O
HMDB51 B-DAT
dataset O
(over O
all O
splits). O
We O

module O
settings O
on O
UCF101 O
and O
HMDB51 B-DAT
datasets O
(over O
all O
splits). O
‘‘Baseline O

exper- O
iment O
with O
UCF101 O
and O
HMDB51 B-DAT
datasets O
(over O
all O
splits), O
and O

of O
layers O
on O
UCF101 O
and O
HMDB51 B-DAT
datasets O
(over O
all O
splits). O
We O

is O
performed O
on O
UCF101 O
and O
HMDB51 B-DAT
datasets O
(over O
all O
splits). O
As O

methods O
on O
the O
UCF101 O
and O
HMDB51 B-DAT
datasets O
(over O
all O
splits). O
The O

the O
experiments O
on O
UCF101 O
and O
HMDB51 B-DAT
datasets O
with O
R-STAN-101 O
framework. O
Using O

HMDB B-DAT

The O
videos O
are O
divided O
into O
51 B-DAT
classes O
indicate O
51 O
action O
categories O

recognition O
task O
we O
employ O
the O
HMDB51 B-DAT
dataset O
[38] O
that O
includes O
6766 O

task O
over O
all O
splits O
of O
HMDB51 B-DAT

of O
our O
method O
on O
the O
HMDB51 B-DAT

lower O
recognition O
scores O
(comparing O
with O
HMDB51 B-DAT

51 B-DAT

5179 B-DAT
2.1607 O
0.8599 O
0.6400 O
0.3593 O
2.0644 O

that O
includes O
6766 O
video O
from O
51 B-DAT
human O
action O
classes. O
We O
decided O

Kinetics O
62.7 O
C3D O
[55] O
Sports-1M O
51 B-DAT

51 B-DAT

1-task O
multi O
BMI O
- O
- O
51 B-DAT

5142 B-DAT

5154, B-DAT
2018 O

51 B-DAT

of O
experiments O
on O
UCF101 O
and O
HMDB51 B-DAT
datasets. O
Our O
experiments O
show O
that O

public O
datasets O
UCF101 O
[24] O
and O
HMDB51 B-DAT
[25 O

two O
standard O
datasets O
UCF101 O
and O
HMDB51 B-DAT

accuracy O
over O
the O
three O
splits. O
HMDB51 B-DAT
con- O
tains O
6766 O
videos O
clips O

are O
performed O
on O
UCF101 O
and O
HMDB51 B-DAT
datasets O
(over O
all O
splits). O
The O

sample O
segments O
on O
UCF101 O
and O
HMDB51 B-DAT
dataset O
(over O
all O
splits). O
We O

module O
settings O
on O
UCF101 O
and O
HMDB51 B-DAT
datasets O
(over O
all O
splits). O
‘‘Baseline O

exper- O
iment O
with O
UCF101 O
and O
HMDB51 B-DAT
datasets O
(over O
all O
splits), O
and O

of O
layers O
on O
UCF101 O
and O
HMDB51 B-DAT
datasets O
(over O
all O
splits). O
We O

is O
performed O
on O
UCF101 O
and O
HMDB51 B-DAT
datasets O
(over O
all O
splits). O
As O

methods O
on O
the O
UCF101 O
and O
HMDB51 B-DAT
datasets O
(over O
all O
splits). O
The O

the O
experiments O
on O
UCF101 O
and O
HMDB51 B-DAT
datasets O
with O
R-STAN-101 O
framework. O
Using O

HMDB B-DAT

The O
videos O
are O
divided O
into O
51 B-DAT
classes O
indicate O
51 O
action O
categories O

over O
from-scratch O
training O
on O
the O
HMDB B-DAT
dataset, O
getting O
almost O
half-way O
to O

experiment O
on O
smaller O
datasets O
like O
HMDB, B-DAT
we O
use O
Sync-SGD O
with O
8 O

art O
on O
UCF-101 O
[46] O
and O
HMDB B-DAT

in-the-wild O
videos. O
Target O
test O
videos: O
HMDB B-DAT

3 O
splits. O
We O
use O
the O
HMDB B-DAT
split O
1 O
for O
ablative O
experiments O

performance O
on O
all O
splits O
for O
HMDB B-DAT
and O
UCF O
in O
Section O
4.4 O

using O
percentage O
accuracy O
on O
the O
HMDB B-DAT

dot O
represents O
a O
video O
from O
HMDB B-DAT
training O
set, O
and O
is O
color O

evaluated O
using O
percentage O
accuracy O
on O
HMDB B-DAT

evaluated O
using O
percentage O
accuracy O
on O
HMDB B-DAT

DistInit, O
and O
show O
the O
downstream O
HMDB B-DAT

that O
set. O
Performance O
reported O
on O
HMDB B-DAT

it O
two O
times. O
Reported O
on O
HMDB B-DAT

PlaceNet O
[60], O
and O
obtain O
36.8% O
HMDB B-DAT
fine-tuning O
performance O
as O
opposed O
to O

a) O
HMDB B-DAT

Comparison O
with O
previous O
work O
on O
HMDB B-DAT
and O
UCF. O
We O
split O
the O

and O
95.1% O
3-split O
avg O
on O
HMDB B-DAT
and O
UCF, O
but O
is O
not O

models O
and O
initialization O
methods O
on O
HMDB B-DAT
and O
UCF. O
For O
these O
comparisons O

Figure O
7: O
HMDB B-DAT
classes O
with O
largest O
gain O
using O

of O
inflation. O
The O
plot O
shows O
HMDB B-DAT
per-class O
accuracy O
differ- O
ence O
between O

Tomaso O
Poggio, O
and O
Thomas O
Serre. O
HMDB B-DAT

art O
on O
UCF-101 O
[46] O
and O
HMDB-51 B-DAT
[28] O
in O
Section O
4.4 O

in-the-wild O
videos. O
Target O
test O
videos: O
HMDB-51 B-DAT
[28] O
contains O
6766 O
realistic O
and O

using O
percentage O
accuracy O
on O
the O
HMDB-51 B-DAT
dataset, O
Split O
1. O
The O
models O

evaluated O
using O
percentage O
accuracy O
on O
HMDB-51 B-DAT
split O
1 O

evaluated O
using O
percentage O
accuracy O
on O
HMDB-51 B-DAT
split O
1 O

DistInit, O
and O
show O
the O
downstream O
HMDB-51 B-DAT
split-1 O
performance O
in O
the O
line O

that O
set. O
Performance O
reported O
on O
HMDB-51 B-DAT
split O
1 O

it O
two O
times. O
Reported O
on O
HMDB-51 B-DAT
split O
1 O

a) O
HMDB-51 B-DAT
Model O
Architecture O
#frames O
Pre-training O
Split O

8, O
9, O
15, O
16, O
44, O
51 B-DAT

51 B-DAT

51 B-DAT

51, B-DAT
56]. O
However, O
these O
models O
no O

51 B-DAT

51 B-DAT

51 B-DAT
[28] O
in O
Section O
4.4 O

51 B-DAT
[28] O
contains O
6766 O
realistic O
and O

varied O
video O
clips O
from O
51 B-DAT
action O
classes. O
Evaluation O
is O
performed O

R(2+1)D-18 O
Kinetics O
pre-training O
- O
51 B-DAT

51 B-DAT
dataset, O
Split O
1. O
The O
models O

51 B-DAT

51 B-DAT
split O
1 O

51 B-DAT
split O
1 O

51 B-DAT

51 B-DAT
split-1 O
performance O
in O
the O
line O

51 B-DAT
split O
1 O

51 B-DAT
split O
1 O

51 B-DAT

51 B-DAT
Model O
Architecture O
#frames O
Pre-training O
Split O

C3D O
[3] O
Custom O
16 O
Scratch O
51 B-DAT

51 B-DAT

51 B-DAT

art O
on O
UCF-101 O
[46] O
and O
HMDB-51 B-DAT
[28] O
in O
Section O
4.4 O

in-the-wild O
videos. O
Target O
test O
videos: O
HMDB-51 B-DAT
[28] O
contains O
6766 O
realistic O
and O

using O
percentage O
accuracy O
on O
the O
HMDB-51 B-DAT
dataset, O
Split O
1. O
The O
models O

evaluated O
using O
percentage O
accuracy O
on O
HMDB-51 B-DAT
split O
1 O

evaluated O
using O
percentage O
accuracy O
on O
HMDB-51 B-DAT
split O
1 O

DistInit, O
and O
show O
the O
downstream O
HMDB-51 B-DAT
split-1 O
performance O
in O
the O
line O

that O
set. O
Performance O
reported O
on O
HMDB-51 B-DAT
split O
1 O

it O
two O
times. O
Reported O
on O
HMDB-51 B-DAT
split O
1 O

a) O
HMDB-51 B-DAT
Model O
Architecture O
#frames O
Pre-training O
Split O

provements O
over O
the O
UCF101 O
and O
HMDB51 B-DAT
benchmarks O

widely-used O
benchmarks, O
UCF101 O
[38] O
and O
HMDB51 B-DAT
[18]. O
UCF101 O
and O
HMDB51 O
contain O

Method O
Feature O
Setting O
HMDB51 B-DAT
UCF101 O
ST O
[45] O
BoW O
T O

an O
average O
duration O
of O
7.2s. O
HMDB51 B-DAT
includes O
6766 O
videos O
of O
51 O

scenario. O
The O
seen/unseen O
splits O
for O
HMDB51 B-DAT
and O
UCF101 O
are O
27/26 O
and O

Dataset O
HMDB51 B-DAT
UCF101 O
Setting O
Cross-Dataset O
Transductive O
Cross-Dataset O

rich O
and O
the O
concepts O
of O
HMDB51 B-DAT
and O
UCF101 O
are O
not O
very O

is O
large O
(roughly O
5% O
on O
HMDB51 B-DAT
and O
1% O
on O
UCF101 O
be O

T. O
Poggio, O
and O
T. O
Serre. O
HMDB B-DAT

the O
success O
of O
CNNs O
[49, O
51, B-DAT
48]. O
With O
the O
help O
of O

28.9±1.2 O
20.1±1.4 O
Ours O
GMIL-D O
CD O
51 B-DAT

HMDB51 B-DAT
includes O
6766 O
videos O
of O
51 O
action O
classes O
extracted O
from O
a O

34.2 O
Kodirov O
et O
al. O
[17] O
51 B-DAT
50 O
10 O
14.0 O
Liu O
et O

al. O
[22] O
51 B-DAT
50 O
5 O
14.9 O
Xu O
et O

al. O
[47] O
51 B-DAT
50 O
50 O
22.9 O
Li O
et O

al. O
[21] O
51 B-DAT
50 O
30 O
26.8 O
Mettes O
and O

Snoek O
[28] O
- O
20 O
10 O
51 B-DAT

and O
UCF101 O
are O
27/26 O
and O
51 B-DAT

13.6 O
No O
TJM O
48.9 O
50.5 O
51 B-DAT

36.6 O
38.1 O
38.6 O
Ours O
49.6 O
51 B-DAT

51 B-DAT

popular O
dataset O
UCF101 O
[36] O
and O
HMDB51 B-DAT
[23]. O
Our O
method O
achieves O
the O

36], O
the O
Kinetics O
[19], O
the O
HMDB51 B-DAT
[23], O
the O
ASLAN O
[21], O
and O

HMDB51 B-DAT
dataset O
[23] O
is O
a O
smaller O

parison O
with O
others, O
we O
use O
HMDB51 B-DAT
train O
split O
1 O
to O
fine O

action O
recog- O
nition O
accuracy O
on O
HMDB51 B-DAT
test O
split O
1 O

also O
finetune O
the O
C3D O
with O
HMDB51 B-DAT
train O
split O
1 O
to O
get O

combina- O
tion O
on O
UCF101 O
and O
HMDB51 B-DAT
dataset O
as O
shown O
in O
Table O

self-supervised O
signals O
for O
UCF101 O
and O
HMDB51 B-DAT
dataset. O
The O
motion O
statistics O
is O

on O
the O
UCF101 O
and O
the O
HMDB51 B-DAT
datasets O

Domain O
UCF101 O
acc.(%) O
HMDB51 B-DAT
acc. O
(%) O
From O
scratch O
45.4 O

combined O
motion O
and O
appearance, O
the O
HMDB51 B-DAT
dataset O
benefits O
a O
lot O
from O

state-of-the-art O
both O
on O
UCF101 O
and O
HMDB51 B-DAT

we O
improve O
9.3% O
accuracy O
on O
HMDB51 B-DAT
than O
[12] O
and O
2.5% O
accuracy O

UCF101 O
and O
5.1% O
improvement O
on O
HMDB51 B-DAT

learning O
methods O
on O
UCF101 O
and O
HMDB51 B-DAT

Method O
UCF101 O
acc.(%) O
HMDB51 B-DAT
acc O

The O
performance O
on O
UCF101, O
HMDB51 B-DAT
and O
ASLAN O
dataset O
shows O
that O

perfor- O
mances O
on O
UCF101 O
and O
HMDB51 B-DAT
datasets. O
This O
strongly O
supports O
that O

con- O
tains O
6766 O
videos O
and O
51 B-DAT
action O
classes. O
It O
also O
consists O

21] O
60.9 O
C3D, O
random O
initialization O
51 B-DAT

dom O
initialization O
model O
can O
achieve O
51 B-DAT

V1, B-DAT
the O
two O
major O
large-scale O
action O

V1 B-DAT
[11] O
(174 O
classes). O
Our O
results O

V1), B-DAT
while O
N O
is O
a O
hyper-parameter O

recognition: O
Kinetics-400 O
[15] O
and O
Something-Something- O
V1 B-DAT
[11]. O
Kinetics-400 O
consists O
of O
400 O

V1 B-DAT
consists O
of O
174 O
actions O
and O

V1 B-DAT
and O
a O
simple O
2D O
RGB O

are O
relatively O
generic O
on O
Something-Something- O
V1 B-DAT
(e.g., O
’plugging O
something O
into O
something O

V1 B-DAT
we O
follow O
the O
standard O
practice O

V1 B-DAT
dataset O

V1 B-DAT
(table O
5). O
We O
compare O
against O

V1 B-DAT
using O
different O
backbone O
architectures, O
which O

state-of-the-art O
performance O
on O
Kinetics-400 O
and O
Something-Something B-DAT

Kinetics-400 O
[15] O
(400 O
classes) O
and O
Something-Something B-DAT

for O
Kinetics-400 O
and O
174 O
for O
Something-Something B-DAT

action O
recognition: O
Kinetics-400 O
[15] O
and O
Something-Something B-DAT

extensive O
set O
of O
action O
classes. O
Something-Something B-DAT

they O
are O
relatively O
generic O
on O
Something-Something B-DAT

in O
the O
literature O
on O
the O
Something-Something B-DAT

Something-Something B-DAT

state-of-the-art O
performance O
on O
Kinetics-400 O
and O
Something B-DAT

Something B-DAT

Kinetics-400 O
[15] O
(400 O
classes) O
and O
Something B-DAT

Something B-DAT

for O
Kinetics-400 O
and O
174 O
for O
Something B-DAT

Something B-DAT

action O
recognition: O
Kinetics-400 O
[15] O
and O
Something B-DAT

Something B-DAT

extensive O
set O
of O
action O
classes. O
Something B-DAT

Something B-DAT

temporal O
information O
is O
essential O
for O
Something B-DAT

- O
Something B-DAT

they O
are O
relatively O
generic O
on O
Something B-DAT

Something B-DAT

testing O
protocol O
for O
Kinetics-400, O
on O
Something B-DAT

- O
Something B-DAT

in O
the O
literature O
on O
the O
Something B-DAT

Something B-DAT

Something B-DAT

Something B-DAT

the O
literature O
reports O
results O
on O
Something B-DAT

- O
Something B-DAT

Stream O
MiniKinetics O
Kinetics400 O
UCF101-1 O
HMDB51-1 O
Something B-DAT

Stream O
Kinetics400 O
UCF101-1 O
HMDB51-1 O
Something B-DAT

Method O
Streams O
Pretrain O
UCF101 O
HMDB51 O
Something B-DAT

Something B-DAT
V1 I-DAT
dataset. O
Single O
crop O
STM O
beats O

other O
state- O
of-the-art O
methods O
on O
Something-Something B-DAT
V1 I-DAT
dataset. O
Single O
crop O
STM O
beats O

other O
state- O
of-the-art O
methods O
on O
Something-Something B-DAT
V1 I-DAT
dataset. O
Single O
crop O
STM O
beats O

state- O
of-the-art O
methods O
on O
Something-Something O
V1 B-DAT
dataset. O
Single O
crop O
STM O
beats O

on O
both O
temporal-related O
datasets O
(i.e., O
Something-Something B-DAT
v1 O
& O
v2 O
and O
Jester O

several O
public O
benchmark O
datasets O
including O
Something-Something B-DAT

conduct O
abundant O
ablation O
studies O
with O
Something-Something B-DAT
v1 O
to O
analyze O
the O
effectiveness O

categories: O
(1) O
temporal-related O
datasets, O
including O
Something-Something B-DAT
v1 O
& O
v2 O
[11] O
and O

of O
the O
STM O
on O
the O
Something-Something B-DAT
v1 O
and O
v2 O
datasets O
compared O

Method O
Backbone O
Flow O
Pretrain O
Frame O
Something-Something B-DAT
v1 O
Something-Something O
v2top-1 O
val O
top-5 O

methods O
on O
temporal-related O
datasets O
including O
Something-Something B-DAT
v1 O
& O
v2 O
and O
Jester O

174 O
classes O
with O
108,499 O
videos. O
Something-Something B-DAT
v2 O
is O
an O
updated O
version O

compared O
with O
the O
state-of-the-art O
on O
Something-Something B-DAT
v1 O
and O
v2. O
The O
results O

on O
Something- O
Something O
v1. O
On O
Something-Something B-DAT
v2, O
STM O
also O
gains O
34.5 O

valida- O
tion O
sets O
of O
both O
Something-Something B-DAT
v1 O
and O
v2, O
and O
just O

our O
pro- O
posed O
STM O
on O
Something-Something B-DAT
v1 O
dataset. O
All O
the O
ablation O

other O
state- O
of-the-art O
methods O
on O
Something-Something B-DAT
V1 O
dataset. O
Single O
crop O
STM O

and O
several O
state-of-the-art O
methods O
on O
Something-Something B-DAT
v1 O
dataset. O
All O
evaluations O
are O

on O
both O
temporal-related O
datasets O
(i.e., O
Something B-DAT

Something B-DAT
v1 O
& O
v2 O
and O
Jester O

several O
public O
benchmark O
datasets O
including O
Something B-DAT

Something B-DAT

on O
both O
temporal-related O
datasets O
(i.e., O
Something B-DAT

- O
Something B-DAT
v1 O
& O
v2 O
and O
Jester O

conduct O
abundant O
ablation O
studies O
with O
Something B-DAT

Something B-DAT
v1 O
to O
analyze O
the O
effectiveness O

categories: O
(1) O
temporal-related O
datasets, O
including O
Something B-DAT

Something B-DAT
v1 O
& O
v2 O
[11] O
and O

T O
= O
16). O
For O
Kinetics, O
Something B-DAT

- O
Something B-DAT
v1 O
& O
v2 O
and O
Jester O

of O
the O
STM O
on O
the O
Something B-DAT

Something B-DAT
v1 O
and O
v2 O
datasets O
compared O

Method O
Backbone O
Flow O
Pretrain O
Frame O
Something B-DAT

-Something B-DAT
v1 O
Something O

Something B-DAT
v2top-1 O
val O
top-5 O
val O
top-1 O

methods O
on O
temporal-related O
datasets O
including O
Something B-DAT

Something B-DAT
v1 O
& O
v2 O
and O
Jester O

. O
Something- B-DAT
Something O
v1 O
is O
a O
large O
collection O

174 O
classes O
with O
108,499 O
videos. O
Something B-DAT

Something B-DAT
v2 O
is O
an O
updated O
version O

compared O
with O
the O
state-of-the-art O
on O
Something B-DAT

Something B-DAT
v1 O
and O
v2. O
The O
results O

16 O
frames O
inputs O
respectively O
on O
Something B-DAT

- O
Something B-DAT
v1. O
On O
Something O

Something B-DAT
v2, O
STM O
also O
gains O
34.5 O

valida- O
tion O
sets O
of O
both O
Something B-DAT

Something B-DAT
v1 O
and O
v2, O
and O
just O

our O
pro- O
posed O
STM O
on O
Something B-DAT

Something B-DAT
v1 O
dataset. O
All O
the O
ablation O

other O
state- O
of-the-art O
methods O
on O
Something B-DAT

Something B-DAT
V1 O
dataset. O
Single O
crop O
STM O

and O
several O
state-of-the-art O
methods O
on O
Something B-DAT

Something B-DAT
v1 O
dataset. O
All O
evaluations O
are O

40], O
ActivityNet O
[8], O
Kinetics O
[25], O
Something-Something B-DAT
[14], O
AVA O
[15], O
and O
Charades O

UCF101 O
[40], O
HMDB51 O
[29] O
and O
Something-Something B-DAT
[14]. O
During O
training O
we O
randomly O

pouring", O
etc.). O
The O
results O
on O
Something-Something B-DAT
show O
that O
pretraining O
on O
Moments O

40], O
ActivityNet O
[8], O
Kinetics O
[25], O
Something B-DAT

Something B-DAT
[14], O
AVA O
[15], O
and O
Charades O

Fine-Tuned O
Pretrained O
UCF O
HMDB O
Something B-DAT

UCF101 O
[40], O
HMDB51 O
[29] O
and O
Something B-DAT

Something B-DAT
[14]. O
During O
training O
we O
randomly O

pouring", O
etc.). O
The O
results O
on O
Something B-DAT

Something B-DAT
show O
that O
pretraining O
on O
Moments O

Something B-DAT
V1 I-DAT
[9], O
V2 O
[16] O
and O
Charades O

datasets O
for O
activity O
recognition: O
Something- O
Something B-DAT
V1 I-DAT
[9], O
V2 O
[16] O
and O
Charades O

Something B-DAT
V1 I-DAT
and O
V2 O
validation O
set. O
Considering O

Something B-DAT
V1 I-DAT
and O
2.5% O
on O
Something-Something O
V2 O

Something B-DAT
V1 I-DAT
and O
61.3% O
on O
Something-Something O
V2 O

three O
large- O
scale O
data O
sets: O
Something-Something B-DAT
V1 I-DAT
[9], O
V2 O
[16] O
and O
Charades O

shows O
all O
the O
results O
on O
Something-Something B-DAT
V1 I-DAT
and O
V2 O
validation O
set. O
Considering O

a O
large O
margin O
4.1% O
on O
Something-Something B-DAT
V1 I-DAT
and O
2.5% O
on O
Something-Something O
V2 O

overall O
performance O
to O
49.8% O
on O
Something-Something B-DAT
V1 I-DAT
and O
61.3% O
on O
Something-Something O
V2 O

the O
samples O
are O
originated O
from O
Something-Something B-DAT
V2 I-DAT
dataset). O
The O
three O
kinds O
of O

three O
large- O
scale O
data O
sets: O
Something-Something B-DAT
V1 I-DAT
[9], O
V2 O
[16] O
and O
Charades O

procedure O
takes O
100 O
epochs. O
For O
Something-Something B-DAT
V2, I-DAT
the O
epoch O
number O
is O
halved O

shows O
all O
the O
results O
on O
Something-Something B-DAT
V1 I-DAT
and O
V2 O
validation O
set. O
Considering O

a O
large O
margin O
4.1% O
on O
Something-Something B-DAT
V1 I-DAT
and O
2.5% O
on O
Something-Something O
V2 O

overall O
performance O
to O
49.8% O
on O
Something-Something B-DAT
V1 I-DAT
and O
61.3% O
on O
Something-Something O
V2 O

Inception O
or O
Inception-V3 O
architecture O
on O
Something-Something B-DAT
V2 I-DAT
(left) O
and O
Charades O
(right) O
datasets O

of O
the O
video O
representation O
on O
Something-Something B-DAT
V2 I-DAT
validation O
set. O
The O
representations O
are O

The O
experiment O
was O
conducted O
on O
Something-Something B-DAT
V2 I-DAT
dataset O
with O
20 O
randomly O
selected O

OF O
STATE-OF-ART O
METHODS O
ON O
THE O
SOMETHING-SOMETHING B-DAT
V1 I-DAT
AND O
V2 O
DATASETS. O
WE O
ONLY O

large- O
scale O
data O
sets: O
Something-Something O
V1 B-DAT
[9], O
V2 O
[16] O
and O
Charades O

for O
activity O
recognition: O
Something- O
Something O
V1 B-DAT
[9], O
V2 O
[16] O
and O
Charades O

V1 B-DAT
[9]: O
The O
dataset O
is O
a O

twice O
as O
many O
videos O
as O
V1, B-DAT
collected O
by O
workers O
with O
humans O

V1 B-DAT
and O
V2 O
datasets O
is O
assigned O

all O
the O
results O
on O
Something-Something O
V1 B-DAT
and O
V2 O
validation O
set. O
Considering O

large O
margin O
4.1% O
on O
Something-Something O
V1 B-DAT
and O
2.5% O
on O
Something-Something O
V2 O

performance O
to O
49.8% O
on O
Something-Something O
V1 B-DAT
and O
61.3% O
on O
Something-Something O
V2 O

STATE-OF-ART O
METHODS O
ON O
THE O
SOMETHING-SOMETHING O
V1 B-DAT
AND O
V2 O
DATASETS. O
WE O
ONLY O

Frames O
# O
Params O
FLOPs O
V1 B-DAT
V2top-1 O
top-5 O
top-1 O
top-5 O

widely-used O
large-scale O
datasets, O
such O
as O
Something-Something B-DAT
and O
Charades, O
and O
the O
results O

the O
samples O
are O
originated O
from O
Something-Something B-DAT
V2 O
dataset). O
The O
three O
kinds O

three O
large- O
scale O
data O
sets: O
Something-Something B-DAT
V1 O
[9], O
V2 O
[16] O
and O

Something-Something B-DAT

Something-Something B-DAT

procedure O
takes O
100 O
epochs. O
For O
Something-Something B-DAT
V2, O
the O
epoch O
number O
is O

Every O
video O
in O
Something-Something B-DAT

A O
baseline O
model O
provided O
in O
Something-Something B-DAT
dataset O
exploits O
multi-layer O
3D O
convolution O

The O
results O
on O
Something-Something B-DAT
[9] O
and O
Charades O
[28], O
[29 O

shows O
all O
the O
results O
on O
Something-Something B-DAT
V1 O
and O
V2 O
validation O
set O

a O
large O
margin O
4.1% O
on O
Something-Something B-DAT
V1 O
and O
2.5% O
on O
Something-Something O

overall O
performance O
to O
49.8% O
on O
Something-Something B-DAT
V1 O
and O
61.3% O
on O
Something-Something O

Inception O
or O
Inception-V3 O
architecture O
on O
Something-Something B-DAT
V2 O
(left) O
and O
Charades O
(right O

of O
the O
video O
representation O
on O
Something-Something B-DAT
V2 O
validation O
set. O
The O
representations O

The O
experiment O
was O
conducted O
on O
Something-Something B-DAT
V2 O
dataset O
with O
20 O
randomly O

evaluated O
the O
proposed O
model O
on O
Something-Something B-DAT
and O
Charades O
datasets O
and O
estab O

widely-used O
large-scale O
datasets, O
such O
as O
Something B-DAT

Something B-DAT
and O
Charades, O
and O
the O
results O

the O
samples O
are O
originated O
from O
Something B-DAT

Something B-DAT
V2 O
dataset). O
The O
three O
kinds O

three O
large- O
scale O
data O
sets: O
Something B-DAT

Something B-DAT
V1 O
[9], O
V2 O
[16] O
and O

Covering O
Something B-DAT

with O
Something B-DAT

benchmark O
datasets O
for O
activity O
recognition: O
Something B-DAT

- O
Something B-DAT
V1 O
[9], O
V2 O
[16] O
and O

Something B-DAT

Something B-DAT

Something B-DAT

Something B-DAT

procedure O
takes O
100 O
epochs. O
For O
Something B-DAT

Something B-DAT
V2, O
the O
epoch O
number O
is O

temporal O
axis O
for O
Charades O
and O
Something B-DAT

- O
Something B-DAT
dataset O
respectively O
and O
the O
prediction O

Every O
video O
in O
Something B-DAT

Something B-DAT

A O
baseline O
model O
provided O
in O
Something B-DAT

Something B-DAT
dataset O
exploits O
multi-layer O
3D O
convolution O

The O
results O
on O
Something B-DAT

Something B-DAT
[9] O
and O
Charades O
[28], O
[29 O

shows O
all O
the O
results O
on O
Something B-DAT

Something B-DAT
V1 O
and O
V2 O
validation O
set O

a O
large O
margin O
4.1% O
on O
Something B-DAT

-Something B-DAT
V1 O
and O
2.5% O
on O
Something O

Something B-DAT
V2 O
with O
the O
same O
experimental O

overall O
performance O
to O
49.8% O
on O
Something B-DAT

-Something B-DAT
V1 O
and O
61.3% O
on O
Something O

Something B-DAT
V2. O
Compare O
with O
the O
most O

serial O
of O
experi- O
ments O
on O
Something B-DAT
Something O
V2 O
and O
Charades O
datasets O

54.7% O
with O
Inception-V3 O
backbone O
on O
Something B-DAT

- O
Something B-DAT
V2, O
and O
improve O
the O
mAP O

Inception O
or O
Inception-V3 O
architecture O
on O
Something B-DAT

Something B-DAT
V2 O
(left) O
and O
Charades O
(right O

visualize O
the O
CAM O
[54] O
on O
Something B-DAT

- O
Something B-DAT
V2 O
dataset. O
CAM O
can O
visualize O

of O
the O
video O
representation O
on O
Something B-DAT

Something B-DAT
V2 O
validation O
set. O
The O
representations O

The O
experiment O
was O
conducted O
on O
Something B-DAT

Something B-DAT
V2 O
dataset O
with O
20 O
randomly O

evaluated O
the O
proposed O
model O
on O
Something B-DAT

Something B-DAT
and O
Charades O
datasets O
and O
estab O

Something B-DAT
V1 I-DAT
[9], O
V2 O
[16] O
and O
Charades O

datasets O
for O
activity O
recognition: O
Something- O
Something B-DAT
V1 I-DAT
[9], O
V2 O
[16] O
and O
Charades O

Something B-DAT
V1 I-DAT
and O
V2 O
validation O
set. O
Considering O

Something B-DAT
V1 I-DAT
and O
2.5% O
on O
Something-Something O
V2 O

Something B-DAT
V1 I-DAT
and O
61.3% O
on O
Something-Something O
V2 O

three O
large- O
scale O
data O
sets: O
Something-Something B-DAT
V1 I-DAT
[9], O
V2 O
[16] O
and O
Charades O

shows O
all O
the O
results O
on O
Something-Something B-DAT
V1 I-DAT
and O
V2 O
validation O
set. O
Considering O

a O
large O
margin O
4.1% O
on O
Something-Something B-DAT
V1 I-DAT
and O
2.5% O
on O
Something-Something O
V2 O

overall O
performance O
to O
49.8% O
on O
Something-Something B-DAT
V1 I-DAT
and O
61.3% O
on O
Something-Something O
V2 O

the O
samples O
are O
originated O
from O
Something-Something B-DAT
V2 I-DAT
dataset). O
The O
three O
kinds O
of O

three O
large- O
scale O
data O
sets: O
Something-Something B-DAT
V1 I-DAT
[9], O
V2 O
[16] O
and O
Charades O

procedure O
takes O
100 O
epochs. O
For O
Something-Something B-DAT
V2, I-DAT
the O
epoch O
number O
is O
halved O

shows O
all O
the O
results O
on O
Something-Something B-DAT
V1 I-DAT
and O
V2 O
validation O
set. O
Considering O

a O
large O
margin O
4.1% O
on O
Something-Something B-DAT
V1 I-DAT
and O
2.5% O
on O
Something-Something O
V2 O

overall O
performance O
to O
49.8% O
on O
Something-Something B-DAT
V1 I-DAT
and O
61.3% O
on O
Something-Something O
V2 O

Inception O
or O
Inception-V3 O
architecture O
on O
Something-Something B-DAT
V2 I-DAT
(left) O
and O
Charades O
(right) O
datasets O

of O
the O
video O
representation O
on O
Something-Something B-DAT
V2 I-DAT
validation O
set. O
The O
representations O
are O

The O
experiment O
was O
conducted O
on O
Something-Something B-DAT
V2 I-DAT
dataset O
with O
20 O
randomly O
selected O

OF O
STATE-OF-ART O
METHODS O
ON O
THE O
SOMETHING-SOMETHING B-DAT
V1 I-DAT
AND O
V2 O
DATASETS. O
WE O
ONLY O

large- O
scale O
data O
sets: O
Something-Something O
V1 B-DAT
[9], O
V2 O
[16] O
and O
Charades O

for O
activity O
recognition: O
Something- O
Something O
V1 B-DAT
[9], O
V2 O
[16] O
and O
Charades O

V1 B-DAT
[9]: O
The O
dataset O
is O
a O

twice O
as O
many O
videos O
as O
V1, B-DAT
collected O
by O
workers O
with O
humans O

V1 B-DAT
and O
V2 O
datasets O
is O
assigned O

all O
the O
results O
on O
Something-Something O
V1 B-DAT
and O
V2 O
validation O
set. O
Considering O

large O
margin O
4.1% O
on O
Something-Something O
V1 B-DAT
and O
2.5% O
on O
Something-Something O
V2 O

performance O
to O
49.8% O
on O
Something-Something O
V1 B-DAT
and O
61.3% O
on O
Something-Something O
V2 O

STATE-OF-ART O
METHODS O
ON O
THE O
SOMETHING-SOMETHING O
V1 B-DAT
AND O
V2 O
DATASETS. O
WE O
ONLY O

Frames O
# O
Params O
FLOPs O
V1 B-DAT
V2top-1 O
top-5 O
top-1 O
top-5 O

widely-used O
large-scale O
datasets, O
such O
as O
Something-Something B-DAT
and O
Charades, O
and O
the O
results O

the O
samples O
are O
originated O
from O
Something-Something B-DAT
V2 O
dataset). O
The O
three O
kinds O

three O
large- O
scale O
data O
sets: O
Something-Something B-DAT
V1 O
[9], O
V2 O
[16] O
and O

Something-Something B-DAT

Something-Something B-DAT

procedure O
takes O
100 O
epochs. O
For O
Something-Something B-DAT
V2, O
the O
epoch O
number O
is O

Every O
video O
in O
Something-Something B-DAT

A O
baseline O
model O
provided O
in O
Something-Something B-DAT
dataset O
exploits O
multi-layer O
3D O
convolution O

The O
results O
on O
Something-Something B-DAT
[9] O
and O
Charades O
[28], O
[29 O

shows O
all O
the O
results O
on O
Something-Something B-DAT
V1 O
and O
V2 O
validation O
set O

a O
large O
margin O
4.1% O
on O
Something-Something B-DAT
V1 O
and O
2.5% O
on O
Something-Something O

overall O
performance O
to O
49.8% O
on O
Something-Something B-DAT
V1 O
and O
61.3% O
on O
Something-Something O

Inception O
or O
Inception-V3 O
architecture O
on O
Something-Something B-DAT
V2 O
(left) O
and O
Charades O
(right O

of O
the O
video O
representation O
on O
Something-Something B-DAT
V2 O
validation O
set. O
The O
representations O

The O
experiment O
was O
conducted O
on O
Something-Something B-DAT
V2 O
dataset O
with O
20 O
randomly O

evaluated O
the O
proposed O
model O
on O
Something-Something B-DAT
and O
Charades O
datasets O
and O
estab O

widely-used O
large-scale O
datasets, O
such O
as O
Something B-DAT

Something B-DAT
and O
Charades, O
and O
the O
results O

the O
samples O
are O
originated O
from O
Something B-DAT

Something B-DAT
V2 O
dataset). O
The O
three O
kinds O

three O
large- O
scale O
data O
sets: O
Something B-DAT

Something B-DAT
V1 O
[9], O
V2 O
[16] O
and O

Covering O
Something B-DAT

with O
Something B-DAT

benchmark O
datasets O
for O
activity O
recognition: O
Something B-DAT

- O
Something B-DAT
V1 O
[9], O
V2 O
[16] O
and O

Something B-DAT

Something B-DAT

Something B-DAT

Something B-DAT

procedure O
takes O
100 O
epochs. O
For O
Something B-DAT

Something B-DAT
V2, O
the O
epoch O
number O
is O

temporal O
axis O
for O
Charades O
and O
Something B-DAT

- O
Something B-DAT
dataset O
respectively O
and O
the O
prediction O

Every O
video O
in O
Something B-DAT

Something B-DAT

A O
baseline O
model O
provided O
in O
Something B-DAT

Something B-DAT
dataset O
exploits O
multi-layer O
3D O
convolution O

The O
results O
on O
Something B-DAT

Something B-DAT
[9] O
and O
Charades O
[28], O
[29 O

shows O
all O
the O
results O
on O
Something B-DAT

Something B-DAT
V1 O
and O
V2 O
validation O
set O

a O
large O
margin O
4.1% O
on O
Something B-DAT

-Something B-DAT
V1 O
and O
2.5% O
on O
Something O

Something B-DAT
V2 O
with O
the O
same O
experimental O

overall O
performance O
to O
49.8% O
on O
Something B-DAT

-Something B-DAT
V1 O
and O
61.3% O
on O
Something O

Something B-DAT
V2. O
Compare O
with O
the O
most O

serial O
of O
experi- O
ments O
on O
Something B-DAT
Something O
V2 O
and O
Charades O
datasets O

54.7% O
with O
Inception-V3 O
backbone O
on O
Something B-DAT

- O
Something B-DAT
V2, O
and O
improve O
the O
mAP O

Inception O
or O
Inception-V3 O
architecture O
on O
Something B-DAT

Something B-DAT
V2 O
(left) O
and O
Charades O
(right O

visualize O
the O
CAM O
[54] O
on O
Something B-DAT

- O
Something B-DAT
V2 O
dataset. O
CAM O
can O
visualize O

of O
the O
video O
representation O
on O
Something B-DAT

Something B-DAT
V2 O
validation O
set. O
The O
representations O

The O
experiment O
was O
conducted O
on O
Something B-DAT

Something B-DAT
V2 O
dataset O
with O
20 O
randomly O

evaluated O
the O
proposed O
model O
on O
Something B-DAT

Something B-DAT
and O
Charades O
datasets O
and O
estab O

40], O
ActivityNet O
[8], O
Kinetics O
[25], O
Something-Something B-DAT
[14], O
AVA O
[15], O
and O
Charades O

UCF101 O
[40], O
HMDB51 O
[29] O
and O
Something-Something B-DAT
[14]. O
During O
training O
we O
randomly O

pouring", O
etc.). O
The O
results O
on O
Something-Something B-DAT
show O
that O
pretraining O
on O
Moments O

40], O
ActivityNet O
[8], O
Kinetics O
[25], O
Something B-DAT

Something B-DAT
[14], O
AVA O
[15], O
and O
Charades O

Fine-Tuned O
Pretrained O
UCF O
HMDB O
Something B-DAT

UCF101 O
[40], O
HMDB51 O
[29] O
and O
Something B-DAT

Something B-DAT
[14]. O
During O
training O
we O
randomly O

pouring", O
etc.). O
The O
results O
on O
Something B-DAT

Something B-DAT
show O
that O
pretraining O
on O
Moments O

filters O
used O
by O
the O
Inception O
V1 B-DAT
ar- O
chitecture O
[2] O
into O
3D O

several O
action O
classification O
benchmarks O
(Kinetics, O
Something B-DAT

of O
Sports-1M O
[5], O
Kinetics O
[6], O
Something B-DAT

qualitatively O
different O
kinds O
of O
datasets: O
Something B-DAT

tion O
datasets, O
such O
as O
Kinetics, O
Something B-DAT

The O
second O
main O
dataset O
is O
Something B-DAT

step O
70k O
to O
0.001. O
Since O
Something B-DAT

Mini- O
Kinetics-200 O
and O
64 O
for O
Something B-DAT

model O
on O
the O
Kinetics-Full O
and O
Something B-DAT

the O
Kinetics O
dataset O
and O
the O
Something B-DAT

We O
believe O
this O
is O
because O
Something B-DAT

Kinetics-Full O
Something B-DAT

Top-1 O
accuracy O
on O
Kinetics-Full O
and O
Something B-DAT

on O
this O
dataset. O
However, O
on O
Something B-DAT

models O
on O
Mini- O
Kinetics-200 O
and O
Something B-DAT

frames. O
Left: O
Mini-Kinetics-200 O
dataset. O
Right: O
Something B-DAT

also O
outperforms O
I3D O
on O
the O
Something B-DAT

from O
45.8% O
to O
47.3% O
for O
Something B-DAT

derived O
from O
images O
in O
the O
Something B-DAT

the O
S3D O
model O
on O
the O
Something B-DAT

and O
feature O
gating O
on O
the O
Something B-DAT

outperforms O
S3D O
and O
I3D O
on O
Something B-DAT

to O
the O
complexity O
of O
the O
Something-Something B-DAT
dataset, O
we O
finetune O
the O
network O

Comparison O
with O
state-of- O
the-arts O
on O
Something-Something B-DAT
dataset. O
Last O
row O
shows O
the O

datasets O
Kinetics O
[1] O
and O
Something-Something B-DAT
[3]. O
Moreover, O
we O
applied O
the O

a O
much O
heavier O
network. O
On O
Something-Something, B-DAT
it O
outperforms O
the O
other O
methods O

or O
fewer O
samples. O
On O
the O
Something-Something B-DAT
dataset, O
where O
the O
temporal O
context O

Fig. O
5: O
Examples O
from O
the O
Something-Something B-DAT
dataset. O
In O
this O
dataset, O
the O

Kinetics O
[1], O
ActivityNet O
[2], O
and O
Something B-DAT

- O
Something B-DAT
[3] O
have O
contributed O
more O
diversity O

to O
the O
complexity O
of O
the O
Something B-DAT

Something B-DAT
dataset, O
we O
finetune O
the O
network O

Comparison O
with O
state-of- O
the-arts O
on O
Something B-DAT

Something B-DAT
dataset. O
Last O
row O
shows O
the O

datasets O
Kinetics O
[1] O
and O
Something B-DAT

Something B-DAT
[3]. O
Moreover, O
we O
applied O
the O

a O
much O
heavier O
network. O
On O
Something B-DAT

Something, B-DAT
it O
outperforms O
the O
other O
methods O

or O
fewer O
samples. O
On O
the O
Something B-DAT

Something B-DAT
dataset, O
where O
the O
temporal O
context O

Fig. O
5: O
Examples O
from O
the O
Something B-DAT

Something B-DAT
dataset. O
In O
this O
dataset, O
the O

action O
recognition O
datasets O
(Jester O
and O
Something-Something) B-DAT
and O
achieved O
competitive O
performances O
for O

action O
recognition O
datasets, O
Jester O
(top), O
Something-Something B-DAT
(middle), O
and O
UCF101 O
(bottom O

these O
datasets, O
Jester O
[1] O
and O
Something-Something B-DAT
[8] O
include O
more O
detailed O
physical O

these O
datasets. O
In O
particular, O
the O
Something-Something B-DAT
dataset O
has O
little O
corre- O
lation O

As O
datasets, O
Jester O
[1] O
and O
Something-Something B-DAT
[8] O
are O
used O
because O
these O

the O
datasets O
of O
Jester O
and O
Something-Something, B-DAT
RGB O
images O
extracted O
from O
videos O

version O
(MFNet-S) O
on O
Jester O
and O
Something-Something B-DAT
validation O
sets. O
All O
models O
use O

Dataset O
Jester O
Something-Something B-DAT

better O
performance O
on O
Jester O
and O
Something-Something B-DAT
datasets O
with O
the O
temporal O
sampling O

743 O
videos O
for O
testing. O
The O
Something-Something B-DAT

values O
are O
on O
JESTER O
and O
Something-Something B-DAT
validation O
sets. O
All O
models O
are O

Dataset O
Jester O
Something-Something B-DAT

vs. O
17.4% O
on O
Jester O
and O
Something-Something B-DAT
datasets O
respectively. O
The O
trend O
is O

to O
2.8%) O
than O
MFNet-S O
on O
Something-Something B-DAT
dataset. O
On O
the O
other O
hand O

any O
action-related O
visual O
features O
in O
Something-Something B-DAT
dataset O

and O
ResNet-101) O
on O
Jester O
and O
Something-Something B-DAT
datasets. O
We O
re-implemented O
the O
I3D O

Dataset O
Jester O
Something-Something B-DAT

significant O
effect O
on O
performance. O
In O
Something-Something B-DAT
case, O
accuracy O
gets O
also O
saturated O

scratch O
for O
fair O
comparison O
on O
Something-Something B-DAT
dataset. O
We O
followed O
data O
augmentation O

of O
frames O
in O
Jester O
and O
Something-Something B-DAT
datasets, O
we O
trained O
I3D O
with O

TRN O
[37]. O
Because O
Jester O
and O
Something-Something B-DAT
are O
recently O
released O
datasets O
in O

top-1 O
accuracies O
on O
Jester O
and O
Something-Something B-DAT
test O
datasets O
respectively O
on O
official O

Dataset O
Jester O
Something-Something B-DAT

results O
on O
the O
Jester O
and O
Something-Something B-DAT
datasets O
from O
the O
official O
leaderboards O

Jester O
Something-Something B-DAT

b) O
Something-Something B-DAT

on O
two O
datasets, O
Jester O
and O
Something-Something, B-DAT
and O
obtain O
outperforming O
results O
compared O

action O
recognition O
datasets O
(Jester O
and O
Something B-DAT

Something) B-DAT
and O
achieved O
competitive O
performances O
for O

action O
recognition O
datasets, O
Jester O
(top), O
Something B-DAT

Something B-DAT
(middle), O
and O
UCF101 O
(bottom O

these O
datasets, O
Jester O
[1] O
and O
Something B-DAT

Something B-DAT
[8] O
include O
more O
detailed O
physical O

these O
datasets. O
In O
particular, O
the O
Something B-DAT

Something B-DAT
dataset O
has O
little O
corre- O
lation O

As O
datasets, O
Jester O
[1] O
and O
Something B-DAT

Something B-DAT
[8] O
are O
used O
because O
these O

the O
datasets O
of O
Jester O
and O
Something B-DAT

Something, B-DAT
RGB O
images O
extracted O
from O
videos O

version O
(MFNet-S) O
on O
Jester O
and O
Something B-DAT

Something B-DAT
validation O
sets. O
All O
models O
use O

Dataset O
Jester O
Something B-DAT

Something B-DAT

better O
performance O
on O
Jester O
and O
Something B-DAT

Something B-DAT
datasets O
with O
the O
temporal O
sampling O

743 O
videos O
for O
testing. O
The O
Something B-DAT

Something B-DAT

values O
are O
on O
JESTER O
and O
Something B-DAT

Something B-DAT
validation O
sets. O
All O
models O
are O

Dataset O
Jester O
Something B-DAT

Something B-DAT

vs. O
17.4% O
on O
Jester O
and O
Something B-DAT

Something B-DAT
datasets O
respectively. O
The O
trend O
is O

to O
2.8%) O
than O
MFNet-S O
on O
Something B-DAT

Something B-DAT
dataset. O
On O
the O
other O
hand O

any O
action-related O
visual O
features O
in O
Something B-DAT

Something B-DAT
dataset O

and O
ResNet-101) O
on O
Jester O
and O
Something B-DAT

Something B-DAT
datasets. O
We O
re-implemented O
the O
I3D O

Dataset O
Jester O
Something B-DAT

Something B-DAT

significant O
effect O
on O
performance. O
In O
Something B-DAT

Something B-DAT
case, O
accuracy O
gets O
also O
saturated O

scratch O
for O
fair O
comparison O
on O
Something B-DAT

Something B-DAT
dataset. O
We O
followed O
data O
augmentation O

of O
frames O
in O
Jester O
and O
Something B-DAT

Something B-DAT
datasets, O
we O
trained O
I3D O
with O

TRN O
[37]. O
Because O
Jester O
and O
Something B-DAT

Something B-DAT
are O
recently O
released O
datasets O
in O

top-1 O
accuracies O
on O
Jester O
and O
Something B-DAT

Something B-DAT
test O
datasets O
respectively O
on O
official O

various O
methods O
on O
Jester O
and O
Something B-DAT

Dataset O
Jester O
Something B-DAT

Something B-DAT

results O
on O
the O
Jester O
and O
Something B-DAT

Something B-DAT
datasets O
from O
the O
official O
leaderboards O

Jester O
Something B-DAT

Something B-DAT

performances O
on O
the O
Jester O
and O
Something B-DAT

- O
Something B-DAT
datasets O

MFNet-C50 O
on O
Jester O
(left) O
and O
Something B-DAT

- O
Something B-DAT
(right) O
datasets. O
As O
discussed O
in O

b) O
Something B-DAT

Something B-DAT

on O
two O
datasets, O
Jester O
and O
Something B-DAT

Something, B-DAT
and O
obtain O
outperforming O
results O
compared O

three O
datasets O
Something-Something O
dataset O
(Something- O
V1 B-DAT
[9] O
and O
Something-V2 O
[28] O
where O

V1 B-DAT
174 O
108,499 O
human-object O
interaction O
Something-V2 O

V1 B-DAT
and O
Something- O
V2 O
datasets O
are O

V1 B-DAT
and O
Something-V2 O
respec- O
tively. O
We O

V1 B-DAT
Something-V2 O
Val O
Test O
Val O
Test O

V1 B-DAT
Dataset O
(Top1 O
Accuracy) O
and O
Something-V2 O

V1 B-DAT

predict O
human-object O
interactions O
in O
the O
Something-Something B-DAT
dataset O
and O
identify O
various O
human O

on O
three O
recent O
video O
datasets O
(Something-Something B-DAT
[9], O
Jester O
[10], O
and O
Charades O

on O
sequential O
activity O
recog- O
nition: O
Something-Something B-DAT
dataset O
[9] O
is O
collected O
for O

highly O
competitive O
results O
on O
the O
Something-Something B-DAT
dataset O
for O
human-interaction O
recognition O
[9 O

statistics O
of O
the O
three O
datasets O
Something-Something B-DAT
dataset O
(Something- O
V1 O
[9] O
and O

3.2 O
Results O
on O
Something-Something B-DAT
Dataset O

Something-Something B-DAT
is O
a O
recent O
video O
dataset O

3: O
Prediction O
examples O
on O
a) O
Something-Something, B-DAT
b) O
Jester, O
and O
c) O
Cha O

For O
each O
example O
drawn O
from O
Something-Something B-DAT
and O
Jester, O
the O
top O
two O

the O
validation O
set O
of O
the O
Something-Something B-DAT
dataset O

of O
videos O
from O
the O
(a) O
Something-Something B-DAT
and O
(b) O
Jester O
datasets O
using O

The O
significant O
difference O
on O
the O
Something-Something B-DAT
dataset O
shows O
the O
importance O
of O

shuffled O
inputs O
drawn O
from O
the O
Something-Something B-DAT
dataset, O
in O
Figure O
6b. O
In O

Something-Something B-DAT
Ordered O
Shuffled O

frames O
and O
shuffled O
frames, O
on O
Something-Something B-DAT
and O
UCF101 O
dataset O
respectively. O
On O

can O
better O
differentiate O
activities O
in O
Something-Something B-DAT
dataset O

three O
recent O
video O
datasets O
- O
Something B-DAT

- O
Something, B-DAT
Jester, O
and O
Charades O
- O
which O

predict O
human-object O
interactions O
in O
the O
Something B-DAT

Something B-DAT
dataset O
and O
identify O
various O
human O

on O
three O
recent O
video O
datasets O
(Something B-DAT

Something B-DAT
[9], O
Jester O
[10], O
and O
Charades O

on O
sequential O
activity O
recog- O
nition: O
Something B-DAT

Something B-DAT
dataset O
[9] O
is O
collected O
for O

highly O
competitive O
results O
on O
the O
Something B-DAT

Something B-DAT
dataset O
for O
human-interaction O
recognition O
[9 O

statistics O
of O
the O
three O
datasets O
Something B-DAT

-Something B-DAT
dataset O
(Something O

- O
V1 O
[9] O
and O
Something B-DAT

-V2 O
[28] O
where O
the O
Something B-DAT

Something-V1 B-DAT
174 O
108,499 O
human-object O
interaction O
Something O

3.2 O
Results O
on O
Something B-DAT

Something B-DAT
Dataset O

Something B-DAT

Something B-DAT
is O
a O
recent O
video O
dataset O

are O
challenging, O
such O
as O
‘Tearing O
Something B-DAT
into O
two O
pieces’ O
versus O
‘Tearing O

Something B-DAT
just O
a O
little O
bit’, O
‘Turn O

set O
and O
test O
set O
of O
Something B-DAT

-V1 O
and O
Something B-DAT

the O
valida- O
tion O
set O
of O
Something B-DAT

-v1 O
and O
Something B-DAT

on O
the O
validation O
set O
of O
Something B-DAT

-V1 O
and O
Something B-DAT

Something-V1 B-DAT
Something O

and O
test O
set O
of O
the O
Something B-DAT

-V1 O
Dataset O
(Top1 O
Accuracy) O
and O
Something B-DAT

the O
validation O
set O
of O
the O
Something B-DAT

3: O
Prediction O
examples O
on O
a) O
Something B-DAT

Something, B-DAT
b) O
Jester, O
and O
c) O
Cha O

For O
each O
example O
drawn O
from O
Something B-DAT

Something B-DAT
and O
Jester, O
the O
top O
two O

highlight O
a O
pattern O
characteristic O
to O
Something B-DAT

the O
validation O
set O
of O
the O
Something B-DAT

Something B-DAT
dataset O

of O
videos O
from O
the O
(a) O
Something B-DAT

Something B-DAT
and O
(b) O
Jester O
datasets O
using O

The O
significant O
difference O
on O
the O
Something B-DAT

Something B-DAT
dataset O
shows O
the O
importance O
of O

shuffled O
inputs O
drawn O
from O
the O
Something B-DAT

Something B-DAT
dataset, O
in O
Figure O
6b. O
In O

Something B-DAT

Something B-DAT
Ordered O
Shuffled O

frames O
and O
shuffled O
frames, O
on O
Something B-DAT

Something B-DAT
and O
UCF101 O
dataset O
respectively. O
On O

Something- B-DAT
Something, O
the O
temporal O
order O
is O
critical O

Dataset O
UCF O
Kinetics O
Moments O
Something B-DAT
Jester O
Charades O

can O
better O
differentiate O
activities O
in O
Something B-DAT

Something B-DAT
dataset O

using O
the O
MultiScale O
TRN O
on O
Something B-DAT

- O
Something B-DAT
and O
Jester O
dataset. O
Only O
the O

Something B-DAT
Jester O
Frames O
baseline O
TRN O
baseline O

V1 B-DAT
ImageNet O
48.2 O
- O
TSN O
[1 O

of O
24.2% O
is O
obtained O
on O
Something B-DAT

a) O
Something B-DAT

on O
the O
validation O
set O
of O
Something B-DAT

three O
standard O
action O
recognition O
benchmarks, O
Something B-DAT

EPIC-KITCHENS O
[23] O
and O
HMDB51 O
[24]. O
Something B-DAT

on O
the O
three O
splits. O
Both O
Something B-DAT

videos O
with O
actions O
involving O
objects. O
Something B-DAT

3: O
(a) O
Action O
classes O
in O
Something B-DAT

on O
the O
validation O
set O
of O
Something B-DAT

Something B-DAT

a) O
Something B-DAT

various O
state-of-the-art O
techniques O
on O
(a) O
Something B-DAT

Something B-DAT

approach O
with O
state-of-the-art O
techniques O
on O
Something B-DAT

-v1 O
and O
HMDB51 O
datasets. O
For O
Something B-DAT

M. O
Mueller-Freitag, O
et O
al., O
“The” O
Something B-DAT

Something B-DAT

Stream O
MiniKinetics O
Kinetics400 O
UCF101-1 O
HMDB51-1 O
Something B-DAT

Stream O
Kinetics400 O
UCF101-1 O
HMDB51-1 O
Something B-DAT

Method O
Streams O
Pretrain O
UCF101 O
HMDB51 O
Something B-DAT

Baseline O
22.3 O
25.0 O
29.7 O
13.6 O
14 B-DAT

14 B-DAT

14, B-DAT
with O
IoU=0.5 O

1412 B-DAT

1412 B-DAT

long O
and O
the O
maximum O
is O
14 B-DAT

Table O
14 B-DAT

11.2 O
I3D O
+ O
3 O
TGMs O
14 B-DAT

3 O
TGMs O
+ O
super-events O
14 B-DAT

In O
Table O
14, B-DAT
we O
present O
the O
results O
of O

Charades B-DAT
Dataset O
We O
further O
test O
on O

the O
popular O
Charades B-DAT
dataset O
[19] O
which O
is O
unique O

all O
with O
higher-than-50% O
accuracy O
on O
Charades B-DAT

action O
classification O
per- O
formances O
on O
Charades B-DAT
[19]. O
Method O
modality O
mAP O

may O
temporally O
overlap O
in O
a O
Charades B-DAT
video, O
requiring O
the O
model O
to O

trained O
and O
tested O
on O
the O
Charades B-DAT
and O
MiT O
datasets. O
For O
Charades O

note O
that O
the O
performances O
on O
Charades B-DAT
is O
even O
more O
impressive O
at O

Architecture O
MiT O
Charades B-DAT

A. O
Farhadi, O
and O
K. O
Alahari. O
Charades B-DAT

The O
batch O
size O
used O
for O
Charades B-DAT
is O
128 O
with O
128 O
frames O

the O
entire O
video. O
For O
the O
Charades B-DAT
dataset O
where O
each O
video O
duration O

to O
crop O
video O
around O
humans. O
Charades B-DAT
[55] O
consist O
of O
of O
9848 O

of O
our O
methods O
on O
the O
Charades B-DAT
dataset O

new O
state O
of O
the O
art. O
Charades B-DAT

video O
datasets: O
AVA, O
EPIC-Kitchens, O
and O
Charades B-DAT

action O
classifica- O
tion O
[6], O
and O
Charades B-DAT
video O
classification O
[38]. O
Our O
abla O

6. O
Experiments O
on O
Charades B-DAT

evaluate O
our O
approach O
on O
the O
Charades B-DAT
dataset O
[38]. O
The O
Charades O
dataset O

Table O
3. O
Training O
schedule O
on O
Charades B-DAT

4. O
Action O
recognition O
accuracy O
on O
Charades B-DAT

NL′ O
to O
work O
better O
on O
Charades, B-DAT
so O
we O
adopt O
it O
in O

top-1) O
AVA O
EPIC O
Verbs O
(top-1) O
Charades B-DAT

For O
Charades, B-DAT
we O
experiment O
with O
both O
ResNet-50-I3D O

test O
sets. O
The O
improvement O
on O
Charades B-DAT
is O
not O
as O
large O
as O

to O
60 O
seconds O
is O
useful. O
Charades B-DAT
videos O
are O
much O
shorter O
(∼30 O

datasets O
like O
AVA, O
EPIC-Kitchens, O
and O
Charades B-DAT

used O
for O
computing O
L, O
so O
Charades B-DAT

Appendix O
H. O
Charades B-DAT
Training O
Schedule O

Appendix O
I. O
Charades B-DAT
NL O
Block O
Details O

Pre-activation O
vs. O
post-activation O
NL′ O
on O
Charades B-DAT

choose O
post-activation O
as O
default O
for O
Charades B-DAT
due O
to O
the O
stronger O
performance O

i.e., O
JHMDB, O
HMDB, O
and O
Charades) B-DAT
show O
that, O
PA3D O
out O

marks, O
i.e., O
JHMDB, O
HMDB O
and O
Charades B-DAT

involves O
daily O
activities. O
Charades B-DAT
[23] O
is O
a O
recent O
large O

Charades B-DAT
instead O
of O
Kinetics O
[13] O
with O

the O
other O
hand, O
Charades B-DAT
contains O
activities O
of O
267 O
differ O

durations. O
All O
these O
facts O
make O
Charades B-DAT
reason O

Charades, B-DAT
under O
the O
implementation O

Charades B-DAT

video O
contains O
multiple O
labels O
in O
Charades B-DAT

vs. O
pretraining O
on O
the O
large-scale O
Charades B-DAT

formance O
on O
JHMDB, O
HMDB O
and O
Charades B-DAT

Approaches O
Charades B-DAT

Table O
8. O
State-of-the-art O
on O
Charades B-DAT
(mAP). O
† O
denotes O
our O
repro O

periments O
on O
JHMDB, O
HMDB O
and O
Charades, B-DAT
where O
our O

datasets, O
such O
as O
Something-Something O
and O
Charades, B-DAT
and O
the O
results O
show O
that O

V1 O
[9], O
V2 O
[16] O
and O
Charades B-DAT
[29]. O
Both O
three O
datasets O
are O

V1 O
[9], O
V2 O
[16] O
and O
Charades B-DAT
[29]. O
We O
first O
introduce O
the O

Charades B-DAT
[29]: O
The O
dataset O
is O
composed O

along O
its O
temporal O
axis O
for O
Charades B-DAT
and O
Something- O
Something O
dataset O
respectively O

As O
Charades B-DAT
is O
a O
multi-label O
action O
classification O

results O
on O
Something-Something O
[9] O
and O
Charades B-DAT
[28], O
[29] O
are O
shown O
in O

the O
evaluation O
results O
on O
the O
Charades B-DAT
dataset, O
a O
multi-label O
activity O
recognition O

on O
Something O
Something O
V2 O
and O
Charades B-DAT
datasets O
with O
the O
different O
head O

with O
Inception- O
V3 O
backbone O
on O
Charades B-DAT
dataset. O
The O
intuition O
behind O
this O

on O
Something-Something O
V2 O
(left) O
and O
Charades B-DAT
(right) O
datasets O

most O
in O
our O
experiments O
on O
Charades B-DAT
datasets O
(24 O
FPS). O
Uniformly O
spare O

Figure O
6. O
The O
performance O
on O
Charades B-DAT
dataset O
with O
different O
position O
to O

proposed O
model O
on O
Something-Something O
and O
Charades B-DAT
datasets O
and O
estab- O
lished O
competitive O

IN O
TERMS O
OF O
ON O
THE O
CHARADES B-DAT
DATASET. O
”x/y“ O
IN O
THE O
THIRD O

DIFFERENT O
VIDEO O
SAMPLING O
STRATEGY O
ON O
CHARADES B-DAT
DATASETS O

achieve O
state-of-the-art O
results O
on O
both O
Charades B-DAT
and O
Something-Something O
datasets. O
Especially O
for O

our O
experiments O
in O
the O
challenging O
Charades B-DAT
[20] O
and O
20BN-Something- O
Something O
[21 O

action O
recognition. O
Especially O
in O
the O
Charades B-DAT
dataset, O
we O
obtain O
4.4% O
boost O

on O
two O
recent O
challenging O
datasets: O
Charades B-DAT
[20] O
and O
Something-Something O
[21]. O
We O

on O
our O
target O
datasets O
(e.g. O
Charades B-DAT
or O
Something- O
Something) O
as O
following O

d O
= O
512. O
Since O
both O
Charades B-DAT
and O
Something-Something O
dataset O
are O
in O

loss O
functions O
when O
training O
for O
Charades B-DAT
and O
Something-Something O
datasets. O
For O
Something-Something O

the O
softmax O
loss O
function. O
For O
Charades, B-DAT
we O
apply O
binary O
sigmoid O
loss O

two O
different O
datasets. O
As O
for O
Charades, B-DAT
the O
scenes O
are O
more O
cluttered O

we O
sample O
10 O
clips O
for O
Charades B-DAT
and O
2 O
clips O
for O
Something-Something O

Table O
2. O
Ablations O
on O
Charades B-DAT

6.2 O
Experiments O
on O
Charades B-DAT

In O
the O
Charades B-DAT
experiments, O
following O
the O
official O
split O

actually O
very O
small. O
In O
the O
Charades B-DAT
dataset, O
our O
graph O
is O
defined O

specifically, O
for O
each O
video O
in O
Charades, B-DAT
besides O
the O
action O
class O
labels O

Classification O
mAP O
(%) O
in O
the O
Charades B-DAT
dataset O
[20]. O
NL O
is O
short O

is O
very O
different O
from O
the O
Charades B-DAT
dataset. O
In O
the O
Charades O
dataset O

gains O
we O
have O
in O
the O
Charades B-DAT
dataset. O
The O
reason O
is O
mainly O

on O
the O
UCF-101, O
HMDB-51, O
and O
Charades B-DAT
dataset O

34], O
HMDB- O
51 O
[18], O
and O
Charades B-DAT
[32], O
our O
approach O
significantly O
out O

UCF-101 O
[34], O
HMDB-51 O
[18], O
and O
Charades B-DAT
[32]. O
UCF-101 O
and O
HMDB-51 O
contain O

annotated O
with O
one O
action O
label. O
Charades B-DAT
contains O
longer O
(∼ O
30- O
second O

un- O
less O
otherwise O
stated. O
The O
Charades B-DAT
dataset O
contains O
9,848 O
videos O
split O

softmax O
following O
TSN O
[44]. O
On O
Charades B-DAT
we O
use O
mean O
average O
precision O

videos O
to O
340× O
256. O
As O
Charades B-DAT
con- O
tains O
both O
portrait O
and O

UCF- O
101/HMDB-51 O
and O
0.03 O
for O
Charades B-DAT

Table O
7: O
Accuracy O
on O
Charades B-DAT
[32]. O
Without O
using O
ad- O
ditional O

evaluate O
our O
method O
on O
the O
Charades B-DAT
dataset O
(Table O
7). O
As O
Charades O

Something- O
Something, O
Jester, O
and O
Charades B-DAT
- O
which O
fundamentally O
depend O
on O

Something-Something O
[9], O
Jester O
[10], O
and O
Charades B-DAT
[11]), O
which O
are O
constructed O
for O

Left’, O
and O
‘Turning O
hand O
counterclockwise’. O
Charades B-DAT
dataset O
is O
also O
a O
high-level O

on O
activity O
classification O
in O
the O
Charades B-DAT
dataset O
[11], O
outperforming O
the O
Flow+RGB O

9,28], O
Jester O
dataset O
[10], O
and O
Charades B-DAT
dataset O
[11] O
are O
listed O
in O

27 O
148,092 O
human O
hand O
gesture O
Charades B-DAT
157 O
9,848 O
daily O
indoor O
activity O

3.3 O
Results O
on O
Jester O
and O
Charades B-DAT

MultiScale O
TRN O
on O
the O
recent O
Charades B-DAT
dataset O
for O
daily O
activity O
recognition O

Table O
4: O
Results O
on O
Charades B-DAT
Activity O
Classification O

2 O
predictions O
are O
shown O
above O
Charades B-DAT
frames O

UCF O
Kinetics O
Moments O
Something O
Jester O
Charades B-DAT

We O
test O
our O
model O
on O
YouTube-8M B-DAT
Large-Scale O
Video O
Understand- O
ing O
dataset O

remained O
to O
be O
solved. O
Googles O
YouTube-8M B-DAT
team O
introduced O
a O
large O
multi-label O

We O
test O
our O
model O
on O
YouTube B-DAT

remained O
to O
be O
solved. O
Googles O
YouTube B-DAT

We O
train O
these O
models O
on O
YouTube B-DAT

8M B-DAT
Large-Scale O
Video O
Understand- O
ing O
dataset O

8M B-DAT
team O
introduced O
a O
large O
multi-label O

youtube-8m B-DAT
2018 O

youtube-8m B-DAT

YouTube-8M B-DAT

In O
this O
paper, O
we O
introduce O
YouTube-8M, B-DAT
the O
largest O
multi-label O
video O
classification O

unprecedented O
scale O
and O
diversity O
of O
YouTube-8M B-DAT
will O
lead O
to O
ad- O
vances O

Figure O
1: O
YouTube-8M B-DAT
is O
a O
large-scale O
benchmark O
for O

In O
this O
paper, O
we O
introduce O
YouTube-8M B-DAT
1, O
a O
large-scale O
bench- O
mark O

Therefore, O
unlike O
Sports-1M O
and O
ActivityNet, O
YouTube-8M B-DAT
is O
not O
restricted O
to O
action O

Overall, O
YouTube-8M B-DAT
contains O
more O
than O
8 O
million O

illus- O
trates O
the O
scale O
of O
YouTube-8M, B-DAT
compared O
to O
existing O
image O
and O

YouTube-8M B-DAT
fills O
the O
gap O
in O
video O

3. O
YOUTUBE-8M O
DATASET O
YouTube-8M B-DAT
is O
a O
benchmark O
dataset O
for O

Dataset O
Train O
Validate O
Test O
Total O
YouTube-8M B-DAT
5,786,881 O
1,652,167 O
825,602 O
8,264,650 O

3.4 O
Dataset O
Statistics O
The O
YouTube-8M B-DAT
dataset O
contains O
4, O
800 O
classes O

Top-level O
category O
statistics O
of O
the O
YouTube-8M B-DAT
dataset O

multi-label O
classification O
approaches O
on O
the O
YouTube-8M B-DAT
dataset. O
We O
then O
evaluate O
the O

rated O
test O
set O
of O
the O
YouTube-8M B-DAT
dataset. O
A O
comparison O
with O
the O

5.2 O
Results O
on O
YouTube-8M B-DAT
Table O
3 O
shows O
results O
for O

all O
approaches O
on O
the O
YouTube-8M B-DAT

learned O
using O
the O
YouTube-8M B-DAT
dataset O
and O
perform O
transfer O
learn O

PCA O
matrix O
learned O
on O
the O
YouTube-8M B-DAT
dataset, O
and O
train O
MoE O
or O

in O
performance O
by O
pre-training O
on O
YouTube-8M B-DAT
or O
using O
the O
transfer O
learnt O

video O
representations O
learned O
on O
the O
YouTube-8M B-DAT
dataset O
to O
the O
(a) O
Sports-1M O

LSTM O
layers O
pre-trained O
on O
the O
YouTube-8M B-DAT
task, O
and O
fine-tune O
them O
on O

dataset O
(1M O
videos), O
pre-training O
on O
YouTube-8M B-DAT
still O
helps, O
and O
improves O
the O

ActivityNet O
dataset O
against O
pre-training O
on O
YouTube-8M B-DAT
for O
aggregation O
based O
and O
LSTM O

shows O
that O
features O
learned O
on O
YouTube-8M B-DAT
generalize O
very O
well O
to O
other O

of O
the O
videos O
present O
in O
YouTube-8M B-DAT

In O
this O
paper, O
we O
introduce O
YouTube-8M, B-DAT
a O
large-scale O
video O

classification O
and O
representation O
learning. O
With O
YouTube-8M, B-DAT
our O
goal O
is O
to O
advance O

YouTube B-DAT

In O
this O
paper, O
we O
introduce O
YouTube B-DAT

multiple) O
labels, O
we O
used O
a O
YouTube B-DAT
video O
annotation O
system, O
which O
labels O

unprecedented O
scale O
and O
diversity O
of O
YouTube B-DAT

Figure O
1: O
YouTube B-DAT

In O
this O
paper, O
we O
introduce O
YouTube B-DAT

Therefore, O
unlike O
Sports-1M O
and O
ActivityNet, O
YouTube B-DAT

appear O
as O
topic O
annotations O
for O
YouTube B-DAT
videos O
based O
on O
the O
YouTube O

combination O
of O
their O
popularity O
on O
YouTube B-DAT
and O
manual O
ratings O
of O
their O

Overall, O
YouTube B-DAT

illus- O
trates O
the O
scale O
of O
YouTube B-DAT

YouTube B-DAT

3. O
YOUTUBE-8M O
DATASET O
YouTube B-DAT

a O
video. O
We O
start O
with O
YouTube B-DAT
videos O
since O
they O
are O
a O

many O
more. O
We O
use O
the O
YouTube B-DAT
video O
annotation O
system O
[2] O
to O

1, O
000 O
views, O
using O
the O
YouTube B-DAT
video O
annotation O
system O
[2]. O
We O

the O
YouTube B-DAT
video O
annotation O
system. O
This O
completes O

Dataset O
Train O
Validate O
Test O
Total O
YouTube B-DAT

3.4 O
Dataset O
Statistics O
The O
YouTube B-DAT

Top-level O
category O
statistics O
of O
the O
YouTube B-DAT

Set O
The O
annotations O
from O
the O
YouTube B-DAT
video O
annotation O
system O
can O

multi-label O
classification O
approaches O
on O
the O
YouTube B-DAT

various O
benchmark O
baselines O
on O
the O
YouTube B-DAT

rated O
test O
set O
of O
the O
YouTube B-DAT

5.2 O
Results O
on O
YouTube B-DAT

for O
all O
approaches O
on O
the O
YouTube B-DAT

learned O
using O
the O
YouTube B-DAT

sports O
activities O
with O
1.2 O
million O
YouTube B-DAT
videos O
and O
is O
one O
of O

PCA O
matrix O
learned O
on O
the O
YouTube B-DAT

in O
performance O
by O
pre-training O
on O
YouTube B-DAT

video O
representations O
learned O
on O
the O
YouTube B-DAT

LSTM O
layers O
pre-trained O
on O
the O
YouTube B-DAT

dataset O
(1M O
videos), O
pre-training O
on O
YouTube B-DAT

ActivityNet O
dataset O
against O
pre-training O
on O
YouTube B-DAT

shows O
that O
features O
learned O
on O
YouTube B-DAT

of O
the O
videos O
present O
in O
YouTube B-DAT

In O
this O
paper, O
we O
introduce O
YouTube B-DAT

classification O
and O
representation O
learning. O
With O
YouTube B-DAT

from O
popularity O
sig- O
nals O
on O
YouTube B-DAT
as O
well O
as O
manual O
curation O

3. O
YOUTUBE-8M B-DAT
DATASET O
YouTube-8M O
is O
a O
benchmark O

8M B-DAT

8M, B-DAT
the O
largest O
multi-label O
video O
classification O

8M B-DAT
will O
lead O
to O
ad- O
vances O

8M B-DAT
is O
a O
large-scale O
benchmark O
for O

8M B-DAT
1, O
a O
large-scale O
bench- O
mark O

8M B-DAT
is O
not O
restricted O
to O
action O

8M B-DAT
contains O
more O
than O
8 O
million O

8M, B-DAT
compared O
to O
existing O
image O
and O

8M B-DAT
fills O
the O
gap O
in O
video O

8M B-DAT
DATASET O
YouTube-8M O
is O
a O
benchmark O

8M B-DAT
5,786,881 O
1,652,167 O
825,602 O
8,264,650 O

8M B-DAT
dataset O
contains O
4, O
800 O
classes O

8M B-DAT
dataset O

8M B-DAT
dataset. O
We O
then O
evaluate O
the O

benchmark O
baselines O
on O
the O
YouTube- O
8M B-DAT
dataset. O
We O
find O
that O
binary O

8M B-DAT
dataset. O
A O
comparison O
with O
the O

8M B-DAT
Table O
3 O
shows O
results O
for O

8M B-DAT

8M B-DAT
dataset O
and O
perform O
transfer O
learn O

8M B-DAT
dataset, O
and O
train O
MoE O
or O

8M B-DAT
(4.1.3) O
67.6 O
65.7 O
86.2 O

8M B-DAT
74.1 O
72.5 O
89.3 O

8M B-DAT
77.6 O
74.9 O
91.6 O

8M B-DAT
(4.1.3) O
75.6 O
74.2 O
92.4 O

8M B-DAT
or O
using O
the O
transfer O
learnt O

8M B-DAT
dataset O
to O
the O
(a) O
Sports-1M O

8M B-DAT
task, O
and O
fine-tune O
them O
on O

8M B-DAT
still O
helps, O
and O
improves O
the O

8M B-DAT
for O
aggregation O
based O
and O
LSTM O

8M B-DAT
generalize O
very O
well O
to O
other O

8M B-DAT

8M, B-DAT
a O
large-scale O
video O

8M, B-DAT
our O
goal O
is O
to O
advance O

iments O
on O
multiple O
datasets, O
including O
Charades B-DAT
and O
MultiTHUMOS, O
confirm O
the O
effectiveness O

public O
datasets O
including O
MultiTHUMOS O
and O
Charades, B-DAT
and O
was O
able O
to O
outperform O

Kay O
et O
al., O
2017), O
and O
Charades B-DAT
(Sigurdsson O
et O
al., O
2016b) O
provided O

Table O
7. O
Per-frame O
mAP O
on O
Charades, B-DAT
evaluated O
with O
the O
‘Cha- O
rades_v1_localize O

4.3. O
Charades B-DAT
Dataset O
Charades O
(Sigurdsson O
et O
al., O
2016b) O
is O

experiments, O
we O
follow O
the O
original O
Charades B-DAT
detec- O
tion O
setting O
(i.e., O
Charades_v1_localize O

original O
localization O
setting O
of O
the O
Charades B-DAT
dataset. O
Notably, O
it O
is O
performing O

Charades B-DAT
L=30Charades O
L=15 O

to O
capture O
shorter O
events. O
On O
Charades, B-DAT
the O
Gaussians O
have O
a O
larger O

1-D O
convolution. O
Note O
that O
for O
Charades, B-DAT
the O
temporal O
ker- O
nels O
learned O

datasets O
including O
MultiTHU- O
MOS O
and O
Charades, B-DAT
obtaining O
the O
best O
known O
performance O

of O
M O
on O
MultiTHUMOS O
and O
Charades B-DAT
using O
RGB O
I3D O
features. O
For O

MultiTHUMOS O
Charades B-DAT

of O
Cout O
on O
MultiTHUMOS O
and O
Charades B-DAT
using O
RGB O
I3D O
features. O
For O

MultiTHUMOS O
Charades B-DAT

1-D O
convolution. O
Note O
that O
for O
Charades, B-DAT
the O
temporal O
kernels O
are O
learned O

of O
3.3 O
seconds O
long. O
On O
Charades, B-DAT
the O
TGM O
kernels O
learn O
to O

of O
L O
on O
MultiTHUMOS O
and O
Charades B-DAT
using O
only O
RGB O
I3D O
features O

MultiTHUMOS O
Charades B-DAT
1 O
Layer O
3 O
Layers O
1-D O

movies. O
Existing O
datasets, O
such O
as O
Charades, B-DAT
have O
very O
specific O
actions O
that O

Charades B-DAT
L=30Charades O
L=15 O

to O
capture O
shorter O
events. O
On O
Charades, B-DAT
the O
Gaussians O
have O
a O
larger O

tiTHUMOS, O
Charades B-DAT
and O
MLB-YouTube O
by O
measuring O
per-frame O

as O
the O
average O
activity O
in O
charades B-DAT
is O
12.8 O
seconds O
and O
larger O

as O
the O
average O
activity O
in O
charades B-DAT
is O
12.8 O
seconds O
and O
larger O

THUMOS O
[12], O
ActivityNet O
[6] O
and O
Charades B-DAT
[25] O
provided O
these O
approaches O
training O

actions/activities, O
including O
MultiTHU- O
MOS O
[31], O
Charades B-DAT
[25], O
and O
AVA O
[10 O

what O
they O
used O
for O
the O
Charades B-DAT
Chal- O
lenge O
2017. O
We O
also O

4.3. O
Charades B-DAT
dataset O

Dataset: O
Charades B-DAT
[25] O
is O
a O
large O
scale O

experiments, O
we O
follow O
the O
original O
Charades B-DAT
test O
setting O
(i.e., O
Charades O
v1 O

stead, O
we O
followed O
the O
original O
Charades B-DAT
localization O
test O
setting O
(v1) O
from O

baselines O
and O
LSTMs O
on O
the O
Charades B-DAT
dataset O
v1. O
These O
num- O
bers O

Table O
4. O
Results O
on O
Charades B-DAT
original O
dataset O
(i.e., O
Cha- O
rades O

bit O
differ- O
ent O
from O
the O
Charades B-DAT
Challenge O
2017 O
competition O
setting, O
whose O

the O
localization O
setting O
of O
the O
Charades B-DAT
dataset. O
Notably, O
it O
is O
performing O

Results O
from O
a O
video O
in O
Charades B-DAT

drawn O
from O
192 O
movies. O
Unlike O
Charades, B-DAT
which O
has O
individual O
activity O
classes O

of O
spatial O
location. O
Identical O
to O
Charades B-DAT
(and O
MultiTHUMOS), O
we O
evaluate O
our O

detection O
datasets, O
including O
MultiTHUMOS O
and O
Charades B-DAT

for O
the O
stand-up O
action O
in O
Charades B-DAT

human O
word O
er- O
ror O
rate O
(WER) B-DAT
of O
5.9%/11.3% O
on O
the O
Switchboard/CallHome O

with O
[2] O
which O
quotes O
a O
WER B-DAT
of O
4%, O
the O
5.9% O
estimate O

achieved O
a O
surprisingly O
low O
6.8% O
WER B-DAT
for O
CallHome O
(we O
were O
expecting O

latest O
ASR O
system O
achieves O
5.5%/10.3% O
WER B-DAT
on O
SWB/CH. O
This O
means O
that O

as O
well O
as O
the O
human O
WER B-DAT
reported O
in O
[1]. O
Unsurprisingly, O
there O

WER B-DAT
SWB O
WER O
CH O
Transcriber O
1 O
raw O
6.1 O

3 O
QC O
5.2 O
7.6 O
Human O
WER B-DAT
from O
[1] O
5.9 O
11.3 O

checking O
contrasted O
with O
the O
human O
WER B-DAT
reported O
in O
[1 O

way O
(without O
speaker-adversarial O
MTL). O
The O
WER B-DAT
improve- O
ment O
from O
adding O
the O

strong O
complementarity O
which O
improves O
the O
WER B-DAT
for O
all O
testsets O

WER B-DAT
[%] O
SWB O
CH O

Table O
8: O
WER B-DAT
on O
SWB O
and O
CH O
with O

Table O
8 O
shows O
WER B-DAT
on O
SWB O
and O
CH O
with O

LMs, O
word-LSTM-MTL O
achieved O
the O
best O
WER B-DAT
of O
5.6% O
and O
10.3% O
on O

we O
achieved O
5.5% O
and O
10.3% O
WER B-DAT
for O
SWB O
and O
CH O
respectively O

ASR O
performance O
is O
the O
average O
WER B-DAT
across O
all O
testsets O
which O
is O

argument O
is O
that O
the O
human O
WER B-DAT
of O
expert O
transcribers O
that O
were O

eval O
transcripts; O
word O
error O
rates O
(WER) B-DAT
on O
the O
NIST O
2000 O
Switchboard O

Language O
model O
PPL O
WER B-DAT
4-gram O
LM O
(baseline) O
69.4 O
8.6 O

Configuration O
WER B-DAT

to O
obtain O
the O
lowest O
possible O
WER B-DAT
on O
the O
Switchboard O
dataset O
re O

Model O
WER B-DAT
SWB O
WER O
CH O
RNN O
sigmoid O
(CE) O
10.8 O

themselves, O
they O
achieve O
a O
similar O
WER B-DAT
as O
our O
previous O
best O
model O

Table O
2: O
WER B-DAT
on O
the O
SWB O
part O
of O

Model O
WER B-DAT
SWB O
WER O
CH O
1-layer O
1024 O
bottleneck O
11.8 O

Model O
WER B-DAT
SWB O
WER O
CH O
CE O
ST O
CE O
ST O

strong O
complementarity O
which O
improves O
the O
WER B-DAT
by O
0.6% O
and O
0.9% O
on O

Model O
WER B-DAT
SWB O
WER O
CH O
RNN O
maxout O
9.3 O
15.4 O

trained O
on O
more O
data. O
The O
WER B-DAT
improved O
by O
1.0% O
for O
SWB O

LM O
WER B-DAT
SWB O
WER O
CH O
30K O
vocab, O
4M O
n-grams O

to O
a O
0.4%-0.7% O
decrease O
in O
WER B-DAT
over O
the O
LSTM. O
Multi- O
layer O

yield O
the O
word O
error O
rate O
(WER B-DAT

this O
baseline O
by O
2.4% O
absolute O
WER B-DAT
and O
13.0% O
relative. O
The O
model O

DNN-HMM O
FSH) O
[28] O
achieves O
19.9% O
WER B-DAT
when O
trained O
on O
the O
Fisher O

WER) B-DAT
on O
Switchboard O
dataset O
splits. O
The O

perform O
about O
the O
same, O
9.2% O
WER B-DAT
and O
9.0% O
WER O
for O
the O

the O
noisy O
model O
achieves O
22.6% O
WER B-DAT
over O
the O
clean O
model’s O
28.7 O

% O
WER, B-DAT
a O
6.1% O
absolute O
and O
21.3 O

WER) B-DAT
for O
5 O
systems O
evaluated O
on O

task O
performance, O
word O
error O
rate O
(WER), B-DAT
and O
the O
proximal O
task O
of O

GMMs, O
yielded O
substantial O
reductions O
in O
WER B-DAT
on O
multiple O
challenging O
LVCSR O
tasks O

units O
in O
DNNs O
leads O
to O
WER B-DAT
gains O
and O
simpler O
training O
procedures O

be O
improved. O
We O
analyze O
the O
WER B-DAT
and O
classification O
errors O
made O
by O

ultimately O
lead O
to O
overall O
system O
WER B-DAT
improvements. O
Further, O
we O
look O
at O

minimize O
the O
word O
error O
rate O
(WER) B-DAT
of O
the O
final O
LVCSR O
system O

. O
WER B-DAT
measures O
mistakes O
at O
the O
word O

frame O
level O
and O
overall O
system O
WER B-DAT
is O
complex O
and O
not O
well O

frame-level O
error O
metrics O
and O
system-level O
WER B-DAT
to O
elicit O
insights O
about O
the O

Table O
I O
shows O
frame-level O
and O
WER B-DAT
evaluations O
of O
acoustic O
models O
of O

always O
a O
good O
proxy O
for O
WER B-DAT
performance O
of O
a O
final O
system O

. O
We O
evaluate O
WER B-DAT
on O
a O
subset O
of O
the O

DNN O
acoustic O
models O
substantially O
reduce O
WER B-DAT
on O
the O
training O
set. O
Indeed O

suggest O
that O
further O
training O
set O
WER B-DAT
reductions O
are O
possible O
by O
continuing O

on O
the O
training O
set O
in O
WER B-DAT
do O
not O
translate O
to O
large O

plot O
training O
and O
eval- O
uation O
WER B-DAT
performance O
during O
DNN O
training. O
Figure O

4 O
shows O
WER B-DAT
performance O
for O
our O
100M O
and O

training. O
We O
find O
that O
training O
WER B-DAT
reduces O
fairly O
dramatically O
at O
first O

23] O
found O
a O
reduction O
in O
WER B-DAT
when O
using O
dropout O
on O
a O

Dev O
CrossEnt O
Dev O
Acc(%) O
Train O
WER B-DAT
SWBD O
WER O
CH O
WER O
EV O

WER B-DAT

4. O
Train O
and O
test O
set O
WER B-DAT
as O
a O
function O
of O
training O

with O
dropout O
to O
compare O
generalization O
WER B-DAT
performance O
against O
that O
of O
the O

to O
0.4% O
reduction O
in O
absolute O
WER B-DAT
on O
the O
test O
set. O
While O

analyzing O
the O
training O
and O
test O
WER B-DAT
curves O
in O
Figure O
4 O
we O

select O
the O
lowest O
test O
set O
WER B-DAT
the O
system O
achieves O
during O
DNN O

200M O
parameter O
DNN O
achieves O
20.7% O
WER B-DAT
on O
the O
EV O
subset O

achieves O
only O
a O
0.5% O
absolute O
WER B-DAT
reduction O
over O
the O
much O
smaller O

small O
reduction O
in O
final O
system O
WER B-DAT
[2 O

Fig. O
5. O
WER B-DAT
as O
a O
function O
of O
DNN O

Early O
realignment O
leads O
to O
better O
WER B-DAT
performance O
than O
all O
models O
we O

5 O
shows O
training O
and O
test O
WER B-DAT
curves O
for O
100M O
parameter O
DNN O

realignment O
both O
train O
and O
test O
WER B-DAT
increase O
briefly. O
This O
is O
not O

of O
DNN O
training O
to O
reduce O
WER B-DAT
while O
requiring O
minimal O
additional O
training O

time. O
The O
training O
time O
and O
WER B-DAT
reduction O
of O
DNNs O
with O
early O

Model O
Features O
Acc(%) O
SWBD O
WER B-DAT
CH O
WER O
EV O
WER O

Results: O
Table O
IV O
shows O
both O
WER B-DAT
performance O
and O
classification O
accuracy O
of O

outperforms O
the O
CM O
optimizer, O
but O
WER B-DAT
performance O
across O
all O
evaluation O
sets O

with O
� O
“ O
0.001 O
had O
WER B-DAT
more O
than O
1% O
absolute O
higher O

shows O
the O
frame O
classification O
and O
WER B-DAT
performance O
of O
5 O
hidden O
layer O

Optimizer O
µmax O
Acc(%) O
SWBD O
WER B-DAT
CH O
WER O
EV O
WER O
RT03 O

WER B-DAT

of O
both O
frame O
classification O
and O
WER B-DAT
across O
all O
evaluation O
sets. O
Unlike O

to O
significant O
over-fitting O
problems O
in O
WER B-DAT

a O
3.8% O
relative O
gain O
in O
WER B-DAT
from O
the O
100M O
DNN O
as O

200M O
DNN O
there O
is O
relative O
WER B-DAT
gain O
of O
2.5%. O
Finally O
the O

total O
parameters O
yields O
a O
relative O
WER B-DAT
gain O
of O
1%. O
There O
are O

of O
diminishing O
relative O
gains O
in O
WER B-DAT
also O
occurs O
on O
the O
RT03 O

both O
frame O
clas- O
sification O
and O
WER B-DAT
is O
the O
performance O
gain O
of O

VIII. O
WER B-DAT
AND O
FRAME O
CLASSIFICATION O
ERROR O
ANALYSIS O

of O
frame O
classification O
accuracy O
and O
WER B-DAT
into O
their O
constituent O
compo- O
nents O

which O
have O
the O
same O
final O
WER B-DAT
may O
have O
different O
rates O
of O

the O
constituent O
components O
of O
the O
WER B-DAT
metric O

Layers O
Layer O
Size O
Acc(%) O
SWBD O
WER B-DAT
CH O
WER O
EV O
WER O
RT03 O

WER B-DAT

Figure O
6 O
shows O
decomposed O
WER B-DAT
performance O
of O
HMM- O
DNN O
systems O

see O
that O
decreases O
in O
overall O
WER B-DAT
as O
a O
function O
of O
DNN O

the O
smaller O
components O
of O
overall O
WER B-DAT

to O
senones. O
While O
the O
three O
WER B-DAT
sub-components O
are O
linked, O
it O
is O

layer O
models O
presented O
in O
our O
WER B-DAT
analysis O
of O
Figure O
6 O
and O

Sub O
Del O
Ins O
WER B-DAT
0 O

Fig. O
6. O
Eval2000 O
WER B-DAT
of O
5 O
hidden O
layer O
DNN O

of O
varying O
total O
parameter O
count. O
WER B-DAT
is O
broken O
into O
its O
sub-components O

WER B-DAT
performance, O
but O
only O
up O
to O

frame O
classification O
and O
final O
system O
WER B-DAT

on O
both O
frame O
classification O
and O
WER B-DAT
when O
the O
training O
corpus O
was O

VIII O
WER B-DAT
and O
Frame O
Classification O
Error O
Analysis O

level O
of O
5.5%/10.3% O
on O
the O
Switchboard B-DAT

WER) O
of O
5.9%/11.3% O
on O
the O
Switchboard B-DAT

is O
attainable O
on O
this O
particular O
Switchboard B-DAT
subset O
(although O
not O
achieved O
yet O

CallHome O
task. O
What O
makes O
the O
Switchboard B-DAT
and O
CallHome O
testsets O
so O
different O

collected O
by O
LDC O
under O
the O
Switchboard B-DAT
and O
Fisher O
protocols O
is O
almost O

entirely O
Switchboard B-DAT

hours O
of O
Switchboard B-DAT
1 O
audio O
with O
transcripts O
provided O

data O
from O
LDC, O
in- O
cluding O
Switchboard, B-DAT
Fisher, O
Gigaword, O
and O
Brodcast O
News O

im- O
provements O
to O
our O
English O
Switchboard B-DAT
system O
that O
resulted O
in O
a O

since O
the O
release O
of O
the O
Switchboard B-DAT
corpus O
in O
the O
1990s. O
In O

disfluencies O
that O
are O
pervasive. O
The O
Switchboard B-DAT
[10] O
and O
later O
Fisher O
[11 O

with, O
specifically O
the O
CallHome O
and O
Switchboard B-DAT
portions O
of O
the O
NIST O
eval O

cite. O
The O
error O
rate O
on O
Switchboard B-DAT
is O
about O
5.9%, O
and O
for O

numbers O
are O
5.9% O
for O
the O
Switchboard B-DAT
portion, O
and O
11.3% O
for O
the O

human O
error O
rate O
of O
the O
Switchboard B-DAT
data. O
Interestingly, O
the O
same O
informality O

from O
the O
DARPA O
EARS O
program: O
Switchboard B-DAT
(3M O
words), O
BBN O
Switchboard-2 O
transcripts O

WER) O
on O
the O
NIST O
2000 O
Switchboard B-DAT
test O
set O

maximally O
complemen- O
tary. O
The O
RT-02 O
Switchboard B-DAT
set O
was O
used O
for O
this O

the O
commonly O
used O
English O
CTS O
(Switchboard B-DAT
and O
Fisher) O
corpora. O
Evaluation O
is O

test O
set, O
which O
comprises O
both O
Switchboard B-DAT
(SWB) O
and O
CallHome O
(CH) O
subsets O

of O
the O
CallHome O
conversations.3 O
The O
Switchboard B-DAT

com- O
mon O
words O
in O
the O
Switchboard B-DAT
and O
Fisher O
corpora. O
The O
de O

WER O
of O
2.7% O
on O
the O
Switchboard B-DAT
portion O
of O
the O
NIST O
2000 O

deal O
of O
variability O
between O
the O
Switchboard B-DAT
and O
CallHome O
subsets, O
with O
5.8 O

of O
the O
art O
on O
the O
Switchboard B-DAT
recognition O
task. O
Inspired O
by O
machine O

6.9% O
on O
the O
NIST O
2000 O
Switchboard B-DAT
task. O
The O
combined O
system O
has O

WER) O
on O
the O
NIST O
2000 O
Switchboard B-DAT
test O
set O

from O
the O
DARPA O
EARS O
program: O
Switchboard B-DAT
(3M O
words), O
BBN O
Switchboard-2 O
transcripts O

The O
RNNLMs O
were O
trained O
on O
Switchboard B-DAT
and O
Fisher O
tran- O
scripts O
as O

are O
maximally O
complementary. O
The O
RT-02 O
Switchboard B-DAT
set O
was O
used O
for O
this O

the O
commonly O
used O
English O
CTS O
(Switchboard B-DAT
and O
Fisher) O
corpora. O
Evaluation O
is O

test O
set, O
which O
comprises O
both O
Switchboard B-DAT
(SWB) O
and O
CallHome O
(CH) O
subsets O

. O
The O
Switchboard B-DAT

most O
common O
words O
in O
the O
Switchboard B-DAT
and O
Fisher O
corpora. O
The O
decoder O

6.9% O
on O
the O
NIST O
2000 O
Switchboard B-DAT
set. O
We O
believe O
this O
is O

art O
to O
6.2% O
on O
the O
Switchboard B-DAT
test O
data O

of O
the O
art O
on O
the O
Switchboard B-DAT
recognition O
task. O
Inspired O
by O
machine O

6.9% O
on O
the O
NIST O
2000 O
Switchboard B-DAT
task. O
The O
combined O
system O
has O

WER) O
on O
the O
NIST O
2000 O
Switchboard B-DAT
test O
set O

from O
the O
DARPA O
EARS O
program: O
Switchboard B-DAT
(3M O
words), O
BBN O
Switchboard-2 O
transcripts O

The O
RNNLMs O
were O
trained O
on O
Switchboard B-DAT
and O
Fisher O
tran- O
scripts O
as O

are O
maximally O
complementary. O
The O
RT-02 O
Switchboard B-DAT
set O
was O
used O
for O
this O

the O
commonly O
used O
English O
CTS O
(Switchboard B-DAT
and O
Fisher) O
corpora. O
Evaluation O
is O

test O
set, O
which O
comprises O
both O
Switchboard B-DAT
(SWB) O
and O
CallHome O
(CH) O
subsets O

. O
The O
Switchboard B-DAT

most O
common O
words O
in O
the O
Switchboard B-DAT
and O
Fisher O
corpora. O
The O
decoder O

6.9% O
on O
the O
NIST O
2000 O
Switchboard B-DAT
set. O
We O
believe O
this O
is O

art O
to O
6.2% O
on O
the O
Switchboard B-DAT
test O
data O

a O
record O
6.6% O
on O
the O
Switchboard B-DAT
subset O
of O
the O
Hub5 O
2000 O

lowest O
possible O
WER O
on O
the O
Switchboard B-DAT
dataset O
re- O
gardless O
of O
other O

and O
maxout O
RNNs O
on O
the O
Switchboard B-DAT
and O
CallHome O
subsets O
of O
Hub5’00 O

with O
such O
models O
on O
the O
Switchboard B-DAT
task O
using O
the O
Torch O
toolkit O

text O
data O
from O
LDC, O
including O
Switchboard, B-DAT
Fisher, O
Gigaword, O
and O
Broadcast O
News O

3. O
Conclusion O
In O
our O
previous O
Switchboard B-DAT
system O
paper O
[11] O
we O
have O

achieving O
human O
performance O
on O
the O
Switchboard B-DAT
data O
(estimated O
to O
be O
around O

since O
the O
release O
of O
the O
Switchboard B-DAT
corpus O
in O
the O
1990s. O
In O

disfluencies O
that O
are O
pervasive. O
The O
Switchboard B-DAT
[10] O
and O
later O
Fisher O
[11 O

with, O
specifically O
the O
CallHome O
and O
Switchboard B-DAT
portions O
of O
the O
NIST O
eval O

cite. O
The O
error O
rate O
on O
Switchboard B-DAT
is O
about O
5.9%, O
and O
for O

numbers O
are O
5.9% O
for O
the O
Switchboard B-DAT
portion, O
and O
11.3% O
for O
the O

human O
error O
rate O
of O
the O
Switchboard B-DAT
data. O
Interestingly, O
the O
same O
informality O

from O
the O
DARPA O
EARS O
program: O
Switchboard B-DAT
(3M O
words), O
BBN O
Switchboard-2 O
transcripts O

WER) O
on O
the O
NIST O
2000 O
Switchboard B-DAT
test O
set O

maximally O
complemen- O
tary. O
The O
RT-02 O
Switchboard B-DAT
set O
was O
used O
for O
this O

the O
commonly O
used O
English O
CTS O
(Switchboard B-DAT
and O
Fisher) O
corpora. O
Evaluation O
is O

test O
set, O
which O
comprises O
both O
Switchboard B-DAT
(SWB) O
and O
CallHome O
(CH) O
subsets O

of O
the O
CallHome O
conversations.3 O
The O
Switchboard B-DAT

com- O
mon O
words O
in O
the O
Switchboard B-DAT
and O
Fisher O
corpora. O
The O
de O

WER O
of O
2.7% O
on O
the O
Switchboard B-DAT
portion O
of O
the O
NIST O
2000 O

deal O
of O
variability O
between O
the O
Switchboard B-DAT
and O
CallHome O
subsets, O
with O
5.8 O

system O
of O
7.5% O
WER. O
For O
Switchboard, B-DAT
we O
achieve O
7.2%/14.6% O
on O
the O

Switchboard B-DAT

test-other O
by O
22% O
relatively. O
On O
Switchboard B-DAT
300h O
(LDC97S62) O
[22], O
we O
obtain O

7.2% O
WER O
on O
the O
Switchboard B-DAT
portion O
of O
the O
Hub5’00 O
(LDC2002S09 O

obtain O
6.8%/14.1% O
WER O
on O
the O
Switchboard B-DAT

basic O
(LB), O
LibriSpeech O
double O
(LD), O
Switchboard B-DAT
mild O
(SM) O
and O
Switchboard O
strong O

for O
LibriSpeech O
and O
1k O
for O
Switchboard B-DAT

training O
set O
transcripts. O
For O
the O
Switchboard B-DAT
300h O
task, O
transcripts O
from O
the O

is O
subsequently O
turned O
off. O
For O
Switchboard B-DAT
300h, O
label O
smoothing O
is O
turned O

For O
Switchboard, B-DAT
we O
use O
a O
two-layer O
RNN O

transcripts O
of O
the O
Fisher O
and O
Switchboard B-DAT
datasets. O
We O
find O
the O
fusion O

our O
experiments O
on O
LibriSpeech O
and O
Switchboard B-DAT
with O
SpecAugment. O
We O
report O
state-of-the-art O

4.2. O
Switchboard B-DAT
300h O

For O
Switchboard B-DAT
300h, O
we O
use O
the O
Kaldi O

com- O
bined O
vocabulary O
of O
the O
Switchboard B-DAT
and O
Fisher O
transcripts O

and O
without O
label O
smoothing O
for O
Switchboard B-DAT
300h O
on O
the O
Switchboard O
and O

Table O
4: O
Switchboard B-DAT
300h O
WER O
(%) O
evaluated O
for O

We O
then O
train O
LAS-6-1280 O
on O
Switchboard B-DAT
300h O
with O
schedule O
L O
(training O

Switchboard, B-DAT
whose O
fusion O
parameters O
are O
obtained O

Table O
5: O
Switchboard B-DAT
300h O
WERs O

the O
Lib- O
riSpeech O
960h O
and O
Switchboard B-DAT
300h O
tasks O
on O
end-to-end O
LAS O

Switchboard B-DAT
300h O

a O
record O
6.6% O
on O
the O
Switchboard B-DAT
subset O
of O
the O
Hub5 O
2000 O

lowest O
possible O
WER O
on O
the O
Switchboard B-DAT
dataset O
re- O
gardless O
of O
other O

and O
maxout O
RNNs O
on O
the O
Switchboard B-DAT
and O
CallHome O
subsets O
of O
Hub5’00 O

with O
such O
models O
on O
the O
Switchboard B-DAT
task O
using O
the O
Torch O
toolkit O

text O
data O
from O
LDC, O
including O
Switchboard, B-DAT
Fisher, O
Gigaword, O
and O
Broadcast O
News O

3. O
Conclusion O
In O
our O
previous O
Switchboard B-DAT
system O
paper O
[11] O
we O
have O

achieving O
human O
performance O
on O
the O
Switchboard B-DAT
data O
(estimated O
to O
be O
around O

of O
the O
art O
on O
the O
Switchboard B-DAT
recognition O
task. O
Inspired O
by O
machine O

6.9% O
on O
the O
NIST O
2000 O
Switchboard B-DAT
task. O
The O
combined O
system O
has O

WER) O
on O
the O
NIST O
2000 O
Switchboard B-DAT
test O
set O

from O
the O
DARPA O
EARS O
program: O
Switchboard B-DAT
(3M O
words), O
BBN O
Switchboard-2 O
transcripts O

The O
RNNLMs O
were O
trained O
on O
Switchboard B-DAT
and O
Fisher O
tran- O
scripts O
as O

are O
maximally O
complementary. O
The O
RT-02 O
Switchboard B-DAT
set O
was O
used O
for O
this O

the O
commonly O
used O
English O
CTS O
(Switchboard B-DAT
and O
Fisher) O
corpora. O
Evaluation O
is O

test O
set, O
which O
comprises O
both O
Switchboard B-DAT
(SWB) O
and O
CallHome O
(CH) O
subsets O

. O
The O
Switchboard B-DAT

most O
common O
words O
in O
the O
Switchboard B-DAT
and O
Fisher O
corpora. O
The O
decoder O

6.9% O
on O
the O
NIST O
2000 O
Switchboard B-DAT
set. O
We O
believe O
this O
is O

art O
to O
6.2% O
on O
the O
Switchboard B-DAT
test O
data O

word O
error O
rate O
on O
the O
Switchboard B-DAT
part O
of O
the O
Hub5-2000 O
evaluation O

versus O
Gaussian O
mixture O
models, O
the O
Switchboard B-DAT
corpus O
has O
become O
the O
de O

result O
in O
im- O
provements O
on O
Switchboard B-DAT
tend O
to O
work O
well O
on O

which O
were O
developed O
first O
on O
Switchboard B-DAT
and O
then O
became O
ubiquitous O
as O

Since O
Switchboard B-DAT
is O
such O
a O
well-studied O
corpus O

achieved O
a O
43% O
WER O
on O
Switchboard B-DAT
[3]. O
In O
2000, O
Cambridge O
University O

follows: O
262 O
hours O
from O
the O
Switchboard B-DAT
1 O
data O
collection, O
1698 O
hours O

21.4K O
words, O
40 O
speakers) O
of O
Switchboard B-DAT
data O
and O
1.6 O
hours O
(21.6K O

12% O
relative O
gain O
on O
a O
Switchboard B-DAT
300 O
hours O
setup O
over O
the O

e.g. O
LDC) O
training O
data, O
including O
Switchboard, B-DAT
Fisher, O
Gigaword, O
and O
Broadcast O
News O

for O
significant O
contributions O
to O
the O
Switchboard B-DAT
system O

system O
on O
the O
Switchboard B-DAT
corpus,” O
in O
Proceedings O
of O
Speech O

benchmark O
datasets O
Broadcast O
News O
and O
Switchboard B-DAT
300 O
[16]. O
However, O
in O
contrast O

from O
training O
on O
a O
non-standard O
Switchboard B-DAT

Babel O
in O
3.1 O
and O
on O
Switchboard B-DAT
in O
3.2 O

3.2. O
Switchboard B-DAT
300 O

Hub5’00 O
SWB O
(table O
5). O
The O
Switchboard B-DAT
experiments O
focus O
on O
the O
very O

use O
multi-scale O
features O
in O
the O
Switchboard B-DAT
experiments, O
but O
did O
use O
speaker-dependent O

Switchboard B-DAT
300 O

results O
on O
the O
widely O
studied O
Switchboard B-DAT
Hub5’00, O
achieving O
16.0% O
error O
on O

previously O
published O
methods O
on O
the O
Switchboard B-DAT
Hub5’00 O
corpus, O
achieving O
16.0% O
error O

of O
16.0% O
on O
the O
full O
Switchboard B-DAT
Hub5’00 O
test O
set—the O
best O
published O

WSJ O
read O
80 O
280 O
Switchboard B-DAT
conversational O
300 O
4000 O
Fisher O
conversational O

Speech. O
The O
Wall O
Street O
Journal, O
Switchboard B-DAT
and O
Fisher O
[3] O
corpora O
are O

5.1 O
Conversational O
speech: O
Switchboard B-DAT
Hub5’00 O
(full O

split O
this O
set O
into O
“easy” O
(Switchboard) B-DAT
and O
“hard” O
(CallHome) O
instances, O
often O

on O
only O
the O
300 O
hour O
Switchboard B-DAT
conversational O
telephone O
speech O
dataset O
and O

trained O
on O
both O
Switchboard B-DAT
(SWB) O
and O
Fisher O
(FSH) O
[3 O

in O
a O
similar O
manner O
as O
Switchboard B-DAT

only O
with O
300 O
hours O
from O
Switchboard B-DAT
conversational O
telephone O
speech O
when O
testing O

Since O
the O
Switchboard B-DAT
and O
Fisher O
corpora O
are O
distributed O

when O
trained O
on O
300 O
hour O
Switchboard B-DAT

trained O
on O
the O
Fisher O
and O
Switchboard B-DAT
transcriptions. O
Again, O
hyperparameters O
for O
the O

Published O
error O
rates O
(%WER) O
on O
Switchboard B-DAT
dataset O
splits. O
The O
columns O
labeled O

several O
hundreds O
of O
hours O
(e.g. O
Switchboard B-DAT
and O
Broadcast O
News). O
Larger O
benchmark O

5.1 O
Conversational O
speech: O
Switchboard B-DAT
Hub5'00 O
(full O

results O
on O
the O
widely O
studied O
Switchboard B-DAT
Hub5’00, O
achieving O
16.0% O
error O
on O

previously O
published O
methods O
on O
the O
Switchboard B-DAT
Hub5’00 O
corpus, O
achieving O
16.0% O
error O

of O
16.0% O
on O
the O
full O
Switchboard B-DAT
Hub5’00 O
test O
set—the O
best O
published O

WSJ O
read O
80 O
280 O
Switchboard B-DAT
conversational O
300 O
4000 O
Fisher O
conversational O

Speech. O
The O
Wall O
Street O
Journal, O
Switchboard B-DAT
and O
Fisher O
[3] O
corpora O
are O

5.1 O
Conversational O
speech: O
Switchboard B-DAT
Hub5’00 O
(full O

split O
this O
set O
into O
“easy” O
(Switchboard) B-DAT
and O
“hard” O
(CallHome) O
instances, O
often O

on O
only O
the O
300 O
hour O
Switchboard B-DAT
conversational O
telephone O
speech O
dataset O
and O

trained O
on O
both O
Switchboard B-DAT
(SWB) O
and O
Fisher O
(FSH) O
[3 O

in O
a O
similar O
manner O
as O
Switchboard B-DAT

only O
with O
300 O
hours O
from O
Switchboard B-DAT
conversational O
telephone O
speech O
when O
testing O

Since O
the O
Switchboard B-DAT
and O
Fisher O
corpora O
are O
distributed O

when O
trained O
on O
300 O
hour O
Switchboard B-DAT

trained O
on O
the O
Fisher O
and O
Switchboard B-DAT
transcriptions. O
Again, O
hyperparameters O
for O
the O

Published O
error O
rates O
(%WER) O
on O
Switchboard B-DAT
dataset O
splits. O
The O
columns O
labeled O

several O
hundreds O
of O
hours O
(e.g. O
Switchboard B-DAT
and O
Broadcast O
News). O
Larger O
benchmark O

5.1 O
Conversational O
speech: O
Switchboard B-DAT
Hub5'00 O
(full O

exper- O
iments O
use O
the O
standard O
Switchboard B-DAT
benchmark O
corpus, O
which O
contains O
approximately O

training O
data O
by O
combining O
the O
Switchboard B-DAT
and O
Fisher O
corpora. O
This O
larger O

LVCSR O
system O
by O
combining O
the O
Switchboard B-DAT
and O
Fisher O
corpora. O
This O
results O

presents O
our O
experiments O
on O
the O
Switchboard B-DAT
corpus, O
which O
focus O
on O
regularization O

present O
experiments O
on O
the O
combined O
Switchboard B-DAT
and O
Fisher O
corpora O
in O
Section O

in O
separate O
experiments O
using O
the O
Switchboard B-DAT
300 O
hour O
corpus O
and O
a O

size O
and O
overfitting O
on O
the O
Switchboard B-DAT
corpus O
while O
Section O
VI O
uses O

the O
same O
baseline O
Switchboard B-DAT
system O
to O
compare O
DCNN O
and O

experiments O
on O
the O
300 O
hour O
Switchboard B-DAT
conversational O
telephone O
speech O
corpus O
(LDC97S62 O

set O
consisting O
of O
both O
the O
Switchboard B-DAT
and O
CallHome O
subsets O
of O
the O

technique O
we O
evaluated O
on O
the O
Switchboard B-DAT
corpus. O
We O
note O
that O
only O

acoustic O
models O
using O
the O
same O
Switchboard B-DAT
training O
data O
as O
used O
for O

COMBINED O
LARGE O
CORPUS O
On O
the O
Switchboard B-DAT
300 O
hour O
corpus O
we O
observed O

transcription O
task, O
we O
combine O
the O
Switchboard B-DAT
corpus O
with O
the O
larger O
Fisher O

accurate O
than O
those O
of O
the O
Switchboard B-DAT
corpus O

Switchboard B-DAT
training O
set O
contain O
8725 O
tied O

the O
Fisher O
transcripts O
and O
the O
Switchboard B-DAT
Mississippi O
State O
transcripts. O
Kneser-Ney O
smoothing O

to O
evaluate O
systems O
on O
the O
Switchboard B-DAT
300hr O
task. O
This O
evaluation O
set O

corpus O
to O
those O
trained O
on O
Switchboard B-DAT
alone. O
Second, O
we O
use O
the O

sets. O
Unlike O
with O
our O
smaller O
Switchboard B-DAT
training O
corpus O
experiments, O
increasing O
DNN O

overall O
as O
compared O
with O
our O
Switchboard B-DAT
corpus O
DNNs. O
We O
believe O
this O

a O
certain O
point. O
For O
the O
Switchboard B-DAT
corpus, O
we O
found O
that O
regularization O

V O
Switchboard B-DAT
300 O
Hour O
Corpus O

DNNS, O
DCNNs, O
and O
DLUNNS O
on O
Switchboard B-DAT

exper- O
iments O
use O
the O
standard O
Switchboard B-DAT
benchmark O
corpus, O
which O
contains O
approximately O

training O
data O
by O
combining O
the O
Switchboard B-DAT
and O
Fisher O
corpora. O
This O
larger O

LVCSR O
system O
by O
combining O
the O
Switchboard B-DAT
and O
Fisher O
corpora. O
This O
results O

presents O
our O
experiments O
on O
the O
Switchboard B-DAT
corpus, O
which O
focus O
on O
regularization O

present O
experiments O
on O
the O
combined O
Switchboard B-DAT
and O
Fisher O
corpora O
in O
Section O

in O
separate O
experiments O
using O
the O
Switchboard B-DAT
300 O
hour O
corpus O
and O
a O

size O
and O
overfitting O
on O
the O
Switchboard B-DAT
corpus O
while O
Section O
VI O
uses O

the O
same O
baseline O
Switchboard B-DAT
system O
to O
compare O
DCNN O
and O

experiments O
on O
the O
300 O
hour O
Switchboard B-DAT
conversational O
telephone O
speech O
corpus O
(LDC97S62 O

set O
consisting O
of O
both O
the O
Switchboard B-DAT
and O
CallHome O
subsets O
of O
the O

technique O
we O
evaluated O
on O
the O
Switchboard B-DAT
corpus. O
We O
note O
that O
only O

acoustic O
models O
using O
the O
same O
Switchboard B-DAT
training O
data O
as O
used O
for O

COMBINED O
LARGE O
CORPUS O
On O
the O
Switchboard B-DAT
300 O
hour O
corpus O
we O
observed O

transcription O
task, O
we O
combine O
the O
Switchboard B-DAT
corpus O
with O
the O
larger O
Fisher O

accurate O
than O
those O
of O
the O
Switchboard B-DAT
corpus O

Switchboard B-DAT
training O
set O
contain O
8725 O
tied O

the O
Fisher O
transcripts O
and O
the O
Switchboard B-DAT
Mississippi O
State O
transcripts. O
Kneser-Ney O
smoothing O

to O
evaluate O
systems O
on O
the O
Switchboard B-DAT
300hr O
task. O
This O
evaluation O
set O

corpus O
to O
those O
trained O
on O
Switchboard B-DAT
alone. O
Second, O
we O
use O
the O

sets. O
Unlike O
with O
our O
smaller O
Switchboard B-DAT
training O
corpus O
experiments, O
increasing O
DNN O

overall O
as O
compared O
with O
our O
Switchboard B-DAT
corpus O
DNNs. O
We O
believe O
this O

a O
certain O
point. O
For O
the O
Switchboard B-DAT
corpus, O
we O
found O
that O
regularization O

V O
Switchboard B-DAT
300 O
Hour O
Corpus O

DNNS, O
DCNNs, O
and O
DLUNNS O
on O
Switchboard B-DAT

results O
on O
the O
widely O
studied O
Switchboard B-DAT
Hub5’00, O
achieving O
16.0% O
error O
on O

previously O
published O
methods O
on O
the O
Switchboard B-DAT
Hub5’00 O
corpus, O
achieving O
16.0% O
error O

of O
16.0% O
on O
the O
full O
Switchboard B-DAT
Hub5’00 O
test O
set—the O
best O
published O

WSJ O
read O
80 O
280 O
Switchboard B-DAT
conversational O
300 O
4000 O
Fisher O
conversational O

Speech. O
The O
Wall O
Street O
Journal, O
Switchboard B-DAT
and O
Fisher O
[3] O
corpora O
are O

5.1 O
Conversational O
speech: O
Switchboard B-DAT
Hub5’00 O
(full O

split O
this O
set O
into O
“easy” O
(Switchboard) B-DAT
and O
“hard” O
(CallHome) O
instances, O
often O

on O
only O
the O
300 O
hour O
Switchboard B-DAT
conversational O
telephone O
speech O
dataset O
and O

trained O
on O
both O
Switchboard B-DAT
(SWB) O
and O
Fisher O
(FSH) O
[3 O

in O
a O
similar O
manner O
as O
Switchboard B-DAT

only O
with O
300 O
hours O
from O
Switchboard B-DAT
conversational O
telephone O
speech O
when O
testing O

Since O
the O
Switchboard B-DAT
and O
Fisher O
corpora O
are O
distributed O

when O
trained O
on O
300 O
hour O
Switchboard B-DAT

trained O
on O
the O
Fisher O
and O
Switchboard B-DAT
transcriptions. O
Again, O
hyperparameters O
for O
the O

Published O
error O
rates O
(%WER) O
on O
Switchboard B-DAT
dataset O
splits. O
The O
columns O
labeled O

several O
hundreds O
of O
hours O
(e.g. O
Switchboard B-DAT
and O
Broadcast O
News). O
Larger O
benchmark O

5.1 O
Conversational O
speech: O
Switchboard B-DAT
Hub5'00 O
(full O

Table O
2: O
LibriSpeech B-DAT
test I-DAT
WER O
(%) O
evaluated O
for O
varying O

achieve O
state-of-the-art O
performance O
on O
the O
LibriSpeech B-DAT
960h O
and O
Swichboard O
300h O
tasks O

outperforming O
all O
prior O
work. O
On O
LibriSpeech, B-DAT
we O
achieve O
6.8% O
WER O
on O

of O
Language O
Models O
(LMs). O
On O
LibriSpeech B-DAT
[20], O
we O
achieve O
2.8% O
Word O

an O
LM O
trained O
on O
the O
LibriSpeech B-DAT
LM O
corpus, O
we O
are O
able O

a O
series O
of O
hand-crafted O
policies, O
LibriSpeech B-DAT
basic O
(LB), O
LibriSpeech O
double O
(LD O

of O
vocabulary O
size O
16k O
for O
LibriSpeech B-DAT
and O
1k O
for O
Switchboard. O
The O

WPM O
for O
LibriSpeech B-DAT
960h O
is O
con- O
structed O
using O

si O
= O
140k O
for O
LibriSpeech B-DAT
960h, O
and O
is O
subsequently O
turned O

For O
LibriSpeech, B-DAT
we O
use O
a O
two-layer O
RNN O

which O
is O
trained O
on O
the O
LibriSpeech B-DAT
LM O
corpus. O
We O
use O
identical O

we O
describe O
our O
experiments O
on O
LibriSpeech B-DAT
and O
Switchboard O
with O
SpecAugment. O
We O

4.1. O
LibriSpeech B-DAT
960h O

For O
LibriSpeech, B-DAT
we O
use O
the O
same O
setup O

LAS-6- O
1280 O
are O
trained O
on O
LibriSpeech B-DAT
960h O
with O
a O
combination O
of O

Table O
2: O
LibriSpeech B-DAT
test O
WER O
(%) O
evaluated O
for O

Table O
3: O
LibriSpeech B-DAT
960h O
WERs O

the O
RT- O
03 O
corpus. O
Unlike O
LibriSpeech, B-DAT
the O
fusion O
parameters O
do O
not O

rate O
is O
being O
decayed O
for O
LibriSpeech B-DAT
when O
label O
smoothing O
is O
applied O

the O
learning O
rate O
schedule O
for O
LibriSpeech B-DAT

Figure O
3: O
LAS-6-1280 O
on O
LibriSpeech B-DAT
with O
schedule O
D O

LibriSpeech B-DAT
960h O

been O
synthe- O
sised O
via O
superimposing O
clean B-DAT
audio O
with O
a O
noisy O
audio O

Rate O
(WER) O
on O
the O
test- O
clean B-DAT
set O
and O
6.8% O
WER O
on O

clean B-DAT
and O
5.8% O
WER O
on O
test O

clean B-DAT
other O
clean O
other O

clean B-DAT
other O
clean O
other O

per- O
formance O
(2.5% O
WER O
on O
test-clean B-DAT
and O
5.8% O
WER O
on O
test O

RWTH O
ASR O
Systems O
for O
LibriSpeech B-DAT

LibriSpeech B-DAT
task. O
Detailed O
descriptions O
of O
the O

when O
training O
on O
the O
full O
LibriSpeech B-DAT
training O
set, O
are O
the O
best O

comparison O
shows O
that O
on O
the O
LibriSpeech B-DAT
960h O

a O
reduced O
100h-subset O
of O
the O
LibriSpeech B-DAT
training O

tention, O
LibriSpeech B-DAT

the O
LibriSpeech B-DAT
task O

The O
LibriSpeech B-DAT
task O
comprises O
English O
read O
speech O

End-to-end O
results O
on O
LibriSpeech B-DAT
were O
presented O
in O
[5–9 O

obtained O
on O
the O
LibriSpeech B-DAT
task O
reflect O
state-of-the-art O
per O

model O
officially O
distributed O
with O
the O
LibriSpeech B-DAT
dataset O
[2 O

perform O
the O
best O
LibriSpeech B-DAT
attention O
system O
presented O
in O
[6 O

and O
hybrid O
DNN/HMM O
results O
on O
LibriSpeech B-DAT
with O
12k O
CART O
labels O
and O

the O
LibriSpeech B-DAT
corpus. O
For O
comparison, O
also O
a O

with O
the O
LibriSpeech B-DAT
corpus: O
dev-clean, O
dev-other, O
test-clean O

2: O
Hybrid O
DNN/HMM O
results O
on O
LibriSpeech B-DAT
with O
differ O

decoder-attention O
model O
results O
on O
LibriSpeech B-DAT
with O
different O

results O
from O
other O
papers O
on O
LibriSpeech B-DAT
960 O
h. O
CDp O
are O

hybrid O
and O
attention-based O
models O
on O
LibriSpeech, B-DAT
to O
the O
best O

two O
ASR O
systems O
for O
the O
LibriSpeech B-DAT

state-of-the-art O
performance O
on O
the O
LibriSpeech B-DAT
960h O
task O
in O

comparison O
shows O
that O
on O
the O
LibriSpeech B-DAT
960h O

a O
reduced O
100h-subset O
of O
the O
LibriSpeech B-DAT
training O

on O
the O
full O
LibriSpeech B-DAT
training O
set, O
are O
the O
best O

by O
15% O
relative O
on O
the O
clean B-DAT
and O
40% O
relative O
on O

clean, B-DAT
and O

clean, B-DAT
and O
filter O
randomly O
such O
that O

clean B-DAT

clean B-DAT
other O
clean O
other O

clean B-DAT
and O
dev-other O
sets, O
then O

clean B-DAT
and O
test-other O
sets. O
We O

Transformer O
model, O
respectively O
on O
the O
clean B-DAT
and O
other O
sets O

clean, B-DAT
dev-other, O
test-clean O

and O
test-other. O
The O
difference O
between O
clean B-DAT
and O
other O
is O
the O

clean B-DAT
quality O
is O
higher O
than O
the O

system O
only O
shows O
improvements O
on O
clean B-DAT
but O
degradation O
on O

and O
SAT O
gives O
mixed O
WER: O
clean B-DAT

clean B-DAT
other O
clean O
other O

clean B-DAT

clean B-DAT
other O
clean O
other O

clean B-DAT

clean B-DAT

clean B-DAT
other O
clean O
other O

clean B-DAT
and O
9.9 O

clean B-DAT
and O
5.5% O
on O
test-other. O
Rescor O

clean B-DAT
and O
5.0% O
on O
test-other. O
The O

clean B-DAT
and O
by O
3.9% O
relative O
WER O

clean B-DAT
and O
by O
27.6% O
relative O
WER O

clean B-DAT
and O
by O
9.1% O
relative O
WER O

clean B-DAT
and O
by O
over O
40% O
relative O

clean B-DAT
and O
by O
13.8% O
relative O
WER O

clean B-DAT
and O

clean B-DAT

by O
15% O
relative O
on O
the O
clean B-DAT
and O
40% O
relative O

respectively O
apply O
them O
to O
the O
test-clean B-DAT
and O
test-other O
sets. O
We O

the O
LibriSpeech O
corpus: O
dev-clean, O
dev-other, O
test-clean B-DAT

a O
WER O
of O
3.2% O
on O
test-clean B-DAT
and O
9.9 O

WER O
of O
2.6% O
on O
test-clean B-DAT
and O
5.5% O
on O
test-other. O
Rescor O

WER O
of O
are O
2.3% O
on O
test-clean B-DAT
and O
5.0% O
on O
test-other. O
The O

WER O
on O
test-clean B-DAT
and O
by O
3.9% O
relative O
WER O

on O
test-clean B-DAT
and O
by O
27.6% O
relative O
WER O

11.5% O
relative O
WER O
on O
test-clean B-DAT
and O
by O
9.1% O
relative O
WER O

15% O
relative O
WER O
on O
test-clean B-DAT
and O
by O
over O
40% O
relative O

WER O
on O
test-clean B-DAT
and O
by O
13.8% O
relative O
WER O

test-clean B-DAT

with O
a O
greedy O
decoder O
on O
LibriSpeech B-DAT
test I-DAT

results O
among O
end-to-end O
models1 O
on O
LibriSpeech B-DAT
test I-DAT

we O
achieve O
3.86% O
WER O
on O
LibriSpeech B-DAT
test I-DAT

improve O
the O
SOTA O
WER O
on O
LibriSpeech B-DAT
test I-DAT

we O
report O
state-of-the-art O
results O
on O
LibriSpeech B-DAT
among O
end-to-end O
speech O
recognition O
models O

with O
a O
greedy O
decoder O
on O
LibriSpeech B-DAT
test-clean. O
We O
also O
report O
competitive O

end-to- O
end O
models O
on O
the O
LibriSpeech B-DAT
and O
2000hr O
Fisher+Switchboard O
tasks. O
Like O

new O
state-of-the-art O
(SOTA) O
results O
on O
LibriSpeech B-DAT
[13] O
test-clean O
of O
2.95% O
WER O

results O
among O
end-to-end O
models1 O
on O
LibriSpeech B-DAT
test-other. O
We O
show O
competitive O
results O

we O
achieve O
3.86% O
WER O
on O
LibriSpeech B-DAT
test-clean O

improve O
the O
SOTA O
WER O
on O
LibriSpeech B-DAT
test-clean O

for O
WSJ O
and O
64 O
for O
LibriSpeech B-DAT
and O
F+S O

3: O
Sequence O
Masking: O
Greedy O
WER, O
LibriSpeech B-DAT
for O
Jasper O
10x4 O
after O
50 O

better O
on O
specific O
subsets O
of O
LibriSpeech B-DAT

4: O
Residual O
Connections: O
Greedy O
WER, O
LibriSpeech B-DAT
for O
Jasper O
10x3 O
after O
400 O

3: O
LM O
perplexity O
vs O
WER. O
LibriSpeech B-DAT
dev-other. O
Vary- O
ing O
perplexity O
is O

Table O
5: O
LibriSpeech, B-DAT
WER O

creased O
the O
WER O
on O
dev-clean O
LibriSpeech B-DAT
from O
4.00% O
to O
3.64%, O
a O

on O
two O
read O
speech O
datasets: O
LibriSpeech B-DAT
and O
Wall O
Street O
Journal O
(WSJ O

leads O
to O
SOTA O
results O
on O
LibriSpeech B-DAT
and O
competitive O
results O
on O
other O

clean B-DAT

clean B-DAT
of O
2.95% O
WER O
and O
SOTA O

clean B-DAT

clean B-DAT

clean B-DAT
dev-other O
test-clean O
test-other O

clean B-DAT
LibriSpeech O
from O
4.00% O
to O
3.64 O

clean B-DAT
subset O
and O
SOTA O
among O
end-to-end O

a O
greedy O
decoder O
on O
LibriSpeech O
test-clean B-DAT

SOTA) O
results O
on O
LibriSpeech O
[13] O
test-clean B-DAT
of O
2.95% O
WER O
and O
SOTA O

achieve O
3.86% O
WER O
on O
LibriSpeech O
test-clean B-DAT

the O
SOTA O
WER O
on O
LibriSpeech O
test-clean B-DAT

Model O
E2E O
LM O
dev-clean O
dev-other O
test-clean B-DAT
test-other O

achieve O
SOTA O
performance O
on O
the O
test-clean B-DAT
subset O
and O
SOTA O
among O
end-to-end O

with O
a O
greedy O
decoder O
on O
LibriSpeech B-DAT
test-clean I-DAT

we O
achieve O
3.86% O
WER O
on O
LibriSpeech B-DAT
test-clean I-DAT

improve O
the O
SOTA O
WER O
on O
LibriSpeech B-DAT
test-clean I-DAT

Table O
2: O
LibriSpeech B-DAT
test I-DAT
WER O
(%) O
evaluated O
for O
varying O

achieve O
state-of-the-art O
performance O
on O
the O
LibriSpeech B-DAT
960h O
and O
Swichboard O
300h O
tasks O

outperforming O
all O
prior O
work. O
On O
LibriSpeech, B-DAT
we O
achieve O
6.8% O
WER O
on O

of O
Language O
Models O
(LMs). O
On O
LibriSpeech B-DAT
[20], O
we O
achieve O
2.8% O
Word O

an O
LM O
trained O
on O
the O
LibriSpeech B-DAT
LM O
corpus, O
we O
are O
able O

a O
series O
of O
hand-crafted O
policies, O
LibriSpeech B-DAT
basic O
(LB), O
LibriSpeech O
double O
(LD O

of O
vocabulary O
size O
16k O
for O
LibriSpeech B-DAT
and O
1k O
for O
Switchboard. O
The O

WPM O
for O
LibriSpeech B-DAT
960h O
is O
con- O
structed O
using O

si O
= O
140k O
for O
LibriSpeech B-DAT
960h, O
and O
is O
subsequently O
turned O

For O
LibriSpeech, B-DAT
we O
use O
a O
two-layer O
RNN O

which O
is O
trained O
on O
the O
LibriSpeech B-DAT
LM O
corpus. O
We O
use O
identical O

we O
describe O
our O
experiments O
on O
LibriSpeech B-DAT
and O
Switchboard O
with O
SpecAugment. O
We O

4.1. O
LibriSpeech B-DAT
960h O

For O
LibriSpeech, B-DAT
we O
use O
the O
same O
setup O

LAS-6- O
1280 O
are O
trained O
on O
LibriSpeech B-DAT
960h O
with O
a O
combination O
of O

Table O
2: O
LibriSpeech B-DAT
test O
WER O
(%) O
evaluated O
for O

Table O
3: O
LibriSpeech B-DAT
960h O
WERs O

the O
RT- O
03 O
corpus. O
Unlike O
LibriSpeech, B-DAT
the O
fusion O
parameters O
do O
not O

rate O
is O
being O
decayed O
for O
LibriSpeech B-DAT
when O
label O
smoothing O
is O
applied O

the O
learning O
rate O
schedule O
for O
LibriSpeech B-DAT

Figure O
3: O
LAS-6-1280 O
on O
LibriSpeech B-DAT
with O
schedule O
D O

LibriSpeech B-DAT
960h O

been O
synthe- O
sised O
via O
superimposing O
clean B-DAT
audio O
with O
a O
noisy O
audio O

Rate O
(WER) O
on O
the O
test- O
clean B-DAT
set O
and O
6.8% O
WER O
on O

clean B-DAT
and O
5.8% O
WER O
on O
test O

clean B-DAT
other O
clean O
other O

clean B-DAT
other O
clean O
other O

per- O
formance O
(2.5% O
WER O
on O
test-clean B-DAT
and O
5.8% O
WER O
on O
test O

best O
sequence-to-sequence O
model O
[9]. O
On O
clean B-DAT
speech, O
the O
im- O
provement O
is O

which O
contains O
80 O
hours O
of O
clean B-DAT
read O
speech, O
and O
Librispeech O
[25 O

with O
separate O
train/dev/test O
splits O
for O
clean B-DAT
and O
noisy O
speech. O
Each O
dataset O

clean B-DAT
and O
train- O
other. O
The O
validation O

dev-clean B-DAT
when O
testing O
on O
test- O
clean, O
and O
dev-other O
when O
testing O
on O

system, O
selected O
either O
on O
dev- O
clean B-DAT
or O
dev-other. O
The O
sequence-to-sequence O
baseline O

over O
CAPIO O
(Single) O
on O
the O
clean B-DAT
part, O
and O
is O
the O
current O

LM O
improves O
similarly O
on O
the O
clean B-DAT
and O
noisier O
parts, O
learning O
the O

gives O
similar O
performance O
on O
the O
clean B-DAT
part O
but O
significantly O
improves O
the O

clean B-DAT

clean B-DAT
dev-other O

clean B-DAT
and O
dev-other O
also O
decreases O
following O

conduct O
exper- O
iments O
on O
the O
LibriSpeech B-DAT
100hr, O
460hr, O
and O
960hr O
tasks O

the O
three O
subsets O
of O
the O
LibriSpeech B-DAT
task O
[19]: O
100hr, O
460hr, O
and O

4. O
LibriSpeech B-DAT
Experimental O
Setup O
4.1. O
Dataset O
The O

LibriSpeech B-DAT
task O
[19] O
has O
three O
subsets O

not O
be O
as O
relevant O
for O
LibriSpeech B-DAT
evaluation O
as O
we O
use O
the O

official O
LibriSpeech B-DAT
lexicon O
without O
modification O

especially O
for O
tasks O
such O
as O
LibriSpeech B-DAT
which O
feature O
long O
utterances O
(∼15s O

WERs O
from O
previous O
work O
on O
LibriSpeech B-DAT
960hr; O
for O
fair O
comparison, O
systems O

has O
limited O
benefits O
for O
the O
LibriSpeech B-DAT
task. O
Table O
8: O
WERs O

Conclusion O
Our O
experiments O
on O
different O
LibriSpeech B-DAT
subsets O
show O
that O
word-piece O
and O

LibriSpeech B-DAT
Experimental O
Setup O

data O
are O
both O
split O
into O
clean B-DAT
and O
other O
subsets, O
each O
of O

dev O
test O
data O
(h) O
Size O
clean B-DAT
other O
clean O
other O

Unit O
Param. O
dev O
testclean B-DAT
other O
clean O
other O

clean, B-DAT
earlier O
than O
on O
the O
dev-other O

Unit O
LM O
dev O
testclean B-DAT
other O
clean O
other O
Phoneme O
3-gram O
5.6 O
15.8 O

clean B-DAT
and O
test-other O
sets. O
For O
fur O

clean B-DAT
and O
dev-other O
WERs. O
We O
obtain O

clean, B-DAT
and O
10.3% O
on O
the O
test-other O

clean B-DAT

clean B-DAT

scenarios. O
Train O
Unit O
dev O
testdata O
clean B-DAT
other O
clean O
other O

clean B-DAT
WER O
(which O
typi- O
cally O
also O

clean B-DAT
set; O
on O
the O
test-other O
set O

dev O
test O
clean B-DAT
other O
clean O
other O

Num. O
dev O
test O
hyp. O
clean B-DAT
other O
clean O
other O

a O
well O
brushed O
hat O
and O
clean B-DAT
shoes”, O
where O
bozzle O
is O
not O

clean B-DAT
other O
clean O
other O
Param. O
Word-Piece O
(WP) O
4.4 O

4.5% O
and O
13.3% O
on O
the O
test-clean B-DAT
and O
test-other O
sets. O
For O
fur O

WERs O
of O
3.6% O
on O
the O
test-clean, B-DAT
and O
10.3% O
on O
the O
test-other O

2.5, O
7.7) O
on O
the O
dev-clean/other, O
test-clean B-DAT

in O
both O
cases O
on O
the O
test-clean B-DAT
set; O
on O
the O
test-other O
set O

on O
the O
Switchboard O
300h O
and O
LibriSpeech B-DAT
1000h O

clean O
evaluation O
subsets O
of O
LibriSpeech B-DAT

300h-Switchboard O
and O
LibriSpeech B-DAT
[49]. O
In O
particular O
on O
Lib O

of O
transcriptions O
was O
used. O
For O
LibriSpeech, B-DAT
we O

for O
Switchboard O
and O
LibriSpeech B-DAT
(the O
weight O
on O
the O
attention O

For O
LibriSpeech, B-DAT
we O
also O
train O
Kneser-Ney O
smoothed O

and O
dev-other O
sets O
of O
LibriSpeech B-DAT

and O
LibriSpeech, B-DAT
we O
first O
used O
the O
BPE O

be O
optimal O
for O
Switchboard O
and O
LibriSpeech B-DAT
respec O

6.3. O
LibriSpeech B-DAT
1000h O

LibriSpeech B-DAT
training O
dataset O
consist O
of O
about O

for O
systems O
trained O
only O
using O
LibriSpeech B-DAT
data O

Table O
3: O
Comparisons O
on O
LibriSpeech B-DAT
1000h. O
The O
attention O

LibriSpeech B-DAT
task, O
we O
obtained O
competitive O
results O

only O
the O
official O
LibriSpeech B-DAT
training O
data O
is O
used O

LibriSpeech B-DAT
1000h O

clean B-DAT
and O
3.82% O
on O
the O
test O

clean B-DAT
evaluation O
subsets O
of O
LibriSpeech. O
We O

clean B-DAT

clean B-DAT
evaluation O
subsets, O
which O
are O
the O

clean B-DAT

clean B-DAT

clean B-DAT
set O
and O
restricting O
it O
to O

clean B-DAT
and O
3.82% O
on O
the O
test-clean O

clean B-DAT
other O
clean O
other O

clean B-DAT
other O
clean O
other O

clean B-DAT
and O
3.82% O
on O
the O

clean B-DAT
subsets O
are O
the O
best O
results O

and O
3.82% O
on O
the O
test-clean B-DAT
evaluation O
subsets, O
which O
are O
the O

dev-clean O
and O
3.82% O
on O
the O
test-clean B-DAT
subsets O

test-clean B-DAT
subsets O
are O
the O
best O
results O

WSJ O
eval92 O
LibriSpeech B-DAT
test-clean I-DAT
LibriSpeech O
test O

of O
the O
art O
performance O
on O
LibriSpeech B-DAT
(Panayotov O
et O
al., O
2015 O

both O
for O
the O
WSJ O
and O
LibriSpeech B-DAT
datasets O
(Panayotov O
et O
al., O
2015 O

of O
the O
art O
systems O
on O
LibriSpeech B-DAT
(960h O

experimental O
results O
on O
WSJ O
and O
LibriSpeech B-DAT

of O
the O
art O
models O
on O
LibriSpeech B-DAT
also O
employ O
this O
approach O
(Panayotov O

LibriSpeech B-DAT
Low O
Dropout O
17 O
0.25/0.25 O
200/750 O

of O
labeled O
audio O
data) O
and O
LibriSpeech B-DAT
(Panayotov O
et O
al., O
2015) O
(about O

and O
EVAL92 O
for O
evaluation. O
For O
LibriSpeech, B-DAT
we O
considered O
the O
two O
available O

model1 O
(about O
200K O
words) O
for O
LibriSpeech B-DAT

mini-batches O
of O
4 O
utterances O
on O
LibriSpeech B-DAT

about O
17M O
trainable O
parameters. O
For O
LibriSpeech, B-DAT
architectures O
have O
about O
130M O
and O

Figure O
3: O
LibriSpeech B-DAT
Letter O
Error O
Rate O
(LER) O
and O

on O
(a) O
WSJ O
and O
(b) O
LibriSpeech B-DAT

b) O
LibriSpeech B-DAT

LER O
and O
WER O
on O
the O
LibriSpeech B-DAT
development O
sets, O
for O
the O
first O

we O
report O
WERs O
on O
the O
LibriSpeech B-DAT
development O
sets, O
both O
for O
our O

the O
art O
ASR O
systems O
on O
LibriSpeech B-DAT

letter-based O
approaches O
on O
WSJ O
and O
LibriSpeech B-DAT

performance O
for O
letter-based O
models O
on O
LibriSpeech B-DAT
is O
held O
by O
DEEP O
SPEECH O

it O
achieves O
6.7% O
WER O
on O
LibriSpeech B-DAT
clean O
data O
even O
with O
no O

Concerning O
LibriSpeech, B-DAT
we O
summarize O
existing O
state O
of O

other O
systems O
on O
WSJ O
and O
LibriSpeech B-DAT

WSJ O
eval92 O
LibriSpeech B-DAT
test-clean O
LibriSpeech O
test-other O

models), O
both O
on O
WSJ O
and O
LibriSpeech B-DAT

system’s O
performance O
is O
competitive O
on O
LibriSpeech, B-DAT
suggesting O
pronunciations O
is O
implicitly O
well O

clean, B-DAT
(b) O
on O
dev-other O

clean B-DAT
dev-other O
model O
LER O
WER O
LER O

clean B-DAT

al., O
2018) O
on O
noisy O
and O
clean B-DAT
subsets O
respectively. O
On O
WSJ O
state O

outperforms O
DEEP O
SPEECH O
2 O
on O
clean B-DAT
data, O
even O
though O
our O
system O

achieves O
6.7% O
WER O
on O
LibriSpeech O
clean B-DAT
data O
even O
with O
no O
decoder O

clean B-DAT
LibriSpeech O
test-other O

WSJ O
eval92 O
LibriSpeech O
test-clean B-DAT
LibriSpeech O
test-other O

WSJ O
eval92 O
LibriSpeech B-DAT
test-clean I-DAT
LibriSpeech O
test-other O

WSJ O
eval’93 O
6.94 O
4.98 O
8.08 O
LibriSpeech B-DAT
test I-DAT

-clean O
7.89 O
5.33 O
5.83 O
LibriSpeech B-DAT
test I-DAT

conversational O
300 O
Fisher O
conversational O
2000 O
LibriSpeech B-DAT
read O
960 O
Baidu O
read O
5000 O

the O
Linguistic O
Data O
Consortium. O
The O
LibriSpeech B-DAT
dataset O
[46] O
is O
available O
free O

advantage O
of O
the O
recently O
developed O
LibriSpeech B-DAT
corpus O
constructed O
using O
audio O
books O

WSJ O
eval’93 O
6.94 O
4.98 O
8.08 O
LibriSpeech B-DAT
test-clean O
7.89 O
5.33 O
5.83 O
LibriSpeech O

providing O
a O
small O
benefit O
on O
clean B-DAT
data. O
The O
change O
from O
one O

system O
to O
further O
improve O
on O
clean B-DAT
read O
speech O
without O
further O
domain O

clean B-DAT
7.89 O
5.33 O
5.83 O
LibriSpeech O
test-other O

CHiME O
eval O
clean B-DAT
6.30 O
3.34 O
3.46 O
CHiME O
eval O

on O
noisy O
speech. O
“CHiME O
eval O
clean B-DAT

similar O
noise O
synthetically O
added O
to O
clean B-DAT
speech. O
Note O
that O
we O
use O

VoxForge O
(http://www.voxforge.org) O
dataset, O
which O
has O
clean B-DAT
speech O
read O
from O
speakers O
with O

of O
synthetically O
adding O
noise O
to O
clean B-DAT
speech O

eval’93 O
6.94 O
4.98 O
8.08 O
LibriSpeech O
test-clean B-DAT
7.89 O
5.33 O
5.83 O
LibriSpeech O
test-other O

WSJ O
eval’93 O
6.94 O
4.98 O
8.08 O
LibriSpeech B-DAT
test-clean I-DAT
7.89 O
5.33 O
5.83 O
LibriSpeech O
test-other O

Wall O
Street O
Journal O
(WSJ) O
and O
LibriSpeech B-DAT
[19] O
datasets. O
The O
input O
to O

On O
LibriSpeech B-DAT
dataset, O
the O
model O
is O
trained O

Table O
2. O
Performance O
from O
LibriSpeech B-DAT
dataset. O
Policy O
de- O
notes O
model O

6]* O
3.60% O
Ours O
5.53% O
Ours O
(LibriSpeech) B-DAT
4.67 O

methods O
on O
WSJ O
eval92 O
dataset. O
LibriSpeech B-DAT
denotes O
model O
trained O
using O
LibriSpeech O

with O
other O
end-to-end O
methods O
on O
LibriSpeech B-DAT
dataset. O
Amodei O
et O
al. O
used O

Comparative O
results O
from O
WSJ O
and O
LibriSpeech B-DAT
dataset O
are O
illustrated O
in O
tables O

Amodei O
et O
al. O
[6] O
on O
LibriSpeech B-DAT
without O
using O
additional O
data. O
To O

generalizes, O
we O
also O
tested O
our O
LibriSpeech B-DAT
model O
on O
the O
WSJ O
dataset O

relative O
performance O
on O
WSJ O
and O
LibriSpeech B-DAT
by O
13% O
and O
4% O
over O

clean B-DAT
and O
test-other O
set, O
respectively O

clean B-DAT
and O
test-other O
sets O

clean B-DAT
CER O
1.76% O
1.69%WER O
5.33% O
5.10 O

clean B-DAT
CER O
1.87% O
1.75%WER O
5.67% O
5.42 O

clean B-DAT
and O
dev-other O
are O
used O
for O

clean B-DAT
test-other O

5.42% O
and O
14.70% O
on O
Librispeech O
test-clean B-DAT
and O
test-other O
set, O
respectively O

and O
14.70% O
WER O
on O
Librispeech O
test-clean B-DAT
and O
test-other O
sets O

test-clean B-DAT
CER O
1.87% O
1.75%WER O
5.67% O
5.42 O

Method O
test-clean B-DAT
test-other O

experiments O
were O
performed O
with O
the O
LibriSpeech B-DAT
[7] O
dataset. O
We O
used O
the O

with O
TIMIT, O
DIRHA, O
CHiME, O
and O
LibriSpeech B-DAT
datasets. O
As O
a O
showcase O
to O

DIRHA O
CHiME O
LibriSpeech B-DAT
MLP O
26.1 O
18.7 O
6.5 O
LSTM O

clean B-DAT
set O
for O
the O
hyperparameter O
search O

clean B-DAT
part O
using O
the O
fglarge O
decoding O

are O
re- O
ported O
on O
the O
test-clean B-DAT
part O
using O
the O
fglarge O
decoding O

large O
vocabulary O
setup, O
on O
the O
LibriSpeech B-DAT
evaluation O
dataset O
[38], O
chosen O
because O

are O
trained O
only O
on O
the O
LibriSpeech B-DAT
training O
set O
(or O
on O
a O

architecture O
trained O
and O
evaluated O
on O
LibriSpeech B-DAT

on O
different O
subsets O
of O
the O
LibriSpeech B-DAT
dataset, O
with O
and O
without O
data O

the O
train-clean O
split O
of O
the O
LibriSpeech B-DAT
dataset O
and O
500h O
in O
the O

not O
have O
much O
impact O
on O
LibriSpeech B-DAT

architecture O
trained O
and O
evaluated O
on O
LibriSpeech B-DAT

clean B-DAT
split O
of O
the O
LibriSpeech O
dataset O

was O
only O
applied O
to O
the O
clean B-DAT
data. O
For O
example O
460x2 O
means O

460h O
of O
clean B-DAT
data O
+ O
460h O
of O
augmented O

clean B-DAT
dev-other O
test-clean O
test-other O
460 O
6.3 O

have O
much O
impact O
on O
LibriSpeech’s O
clean B-DAT
test O
sets O
(dev-clean O
and O
test-clean O

clean B-DAT
dev-other O
test-clean O
test-other O
nnet-256 O
7.3 O

Num. O
hours O
dev-clean O
dev-other O
test-clean B-DAT
test-other O
460 O
6.3 O
21.8 O
6.6 O

clean O
test O
sets O
(dev-clean O
and O
test-clean B-DAT

Model O
dev-clean O
dev-other O
test-clean B-DAT
test-other O
nnet-256 O
7.3 O
19.2 O
7.6 O

WSJ B-DAT
eval’92 I-DAT
4.94 O
3.60 O
5.03 O
WSJ O
eval O

WSJ B-DAT
read O
80 O
Switchboard O
conversational O
300 O

English. O
The O
Wall O
Street O
Journal O
(WSJ), B-DAT
Switchboard O
and O
Fisher O
[13] O
corpora O

from O
the O
Wall O
Street O
Journal O
(WSJ) B-DAT
corpus O
of O
read O
news O
articles O

WSJ B-DAT
eval’92 O
4.94 O
3.60 O
5.03 O
WSJ O
eval’93 O
6.94 O
4.98 O
8.08 O
LibriSpeech O

has O
1320 O
utterances O
from O
the O
WSJ B-DAT
test O
set O
read O
in O
various O

the O
Wall O
Street O
Journal O
dataset O
(WSJ) B-DAT
and O
on O
the O
1000h O
Librispeech O

the O
best O
end-to-end O
systems; O
on O
WSJ, B-DAT
our O
results O
are O
competitive O
with O

show O
additional O
improvements O
on O
both O
WSJ B-DAT
and O
Librispeech O
by O
varying O
the O

of O
the O
Wall O
Street O
Journal O
(WSJ) B-DAT
dataset O
[24], O
which O
contains O
80 O

contain O
37 O
million O
tokens O
for O
WSJ, B-DAT
800 O
million O
tokens O
for O
Librispeech O

the O
open O
vocabulary O
task O
of O
WSJ B-DAT

Training/test O
splits O
On O
WSJ, B-DAT
models O
are O
trained O
on O
si284 O

front-end O
for O
our O
approach). O
On O
WSJ, B-DAT
we O
use O
the O
lighter O
version O

and O
linear O
layer O
on O
both O
WSJ B-DAT
and O
Librispeech. O
The O
kernel O
size O

the O
words O
(162K) O
in O
the O
WSJ B-DAT
training O
corpus, O
while O
only O
the O

Word O
Error O
Rates O
(WER) O
on O
WSJ B-DAT
for O
the O
cur- O
rent O
state-of-the-art O

end-to-end O
systems O
trained O
only O
on O
WSJ, B-DAT
and O
hence O
the O
most O

consistent O
with O
our O
results O
on O
WSJ B-DAT

of O
our O
best O
models O
on O
WSJ B-DAT
and O
Librispeech. O
The O
fig- O
ure O

SCALE O
MEL O
SCALE O
LEARNT O
SCALE O
(WSJ B-DAT

SCALE O
(LIBRI/40 O
FILTERS) O
LEARNT O
SCALE O
(WSJ B-DAT

End-to-end B-DAT
speech O
recognition O
using O
lattice-free O

MMI B-DAT
Hossein O
Hadian1,2 O
∗, O
Hossein O
Sameti1 O

, B-DAT
Daniel O
Povey2,3, O
Sanjeev O
Khudanpur2,3 O
1Department O
of O
Computer O
Engineering, O
Sharif O

University B-DAT
of O
Technology, O
Tehran, O
Iran O

, B-DAT
2Center O
for O
Language O
and O

Speech B-DAT
Processing, O
Johns O
Hopkins O
University, O

Baltimore, B-DAT
MD, O
USA, O
3Human O
Language O
Technology O
Center O
of O

Excellence, B-DAT
Johns O
Hopkins O
University, O
Baltimore O

, B-DAT
USA. O
hhadian@jhu.edu, O
sameti@sharif.edu, O
dpovey@gmail.com, O

khudanpur@jhu.edu B-DAT
Abstract O
We O
present O
our O
work O

on B-DAT
end-to-end O
training O
of O
acoustic O

models B-DAT
using O
the O
lattice-free O
maximum O

mutual B-DAT
information O
(LF-MMI) O
objective O
function O

in B-DAT
the O
context O
of O
hidden O

Markov B-DAT
models. O
By O
end-to-end O
training O

, B-DAT
we O
mean O
flat-start O
training O

of B-DAT
a O
single O
DNN O
in O

one B-DAT
stage O
without O
using O
any O

previously B-DAT
trained O
models, O
forced O
alignments, O

or B-DAT
building O
state-tying O
decision O
trees. O

We B-DAT
use O
full O
biphones O
to O

enable B-DAT
context-dependent O
modeling O
with- O
out O

trees, B-DAT
and O
show O
that O
our O

end-to-end B-DAT
LF-MMI O
approach O
can O
achieve O

comparable B-DAT
results O
to O
regular O
LF-MMI O

on B-DAT
well-known O
large O
vocabulary O
tasks. O

We B-DAT
also O
compare O
with O
other O

end-to-end B-DAT
methods O
such O
as O
CTC O

in B-DAT
character-based O
and O
lexicon-free O
set- O

tings B-DAT
and O
show O
5 O
to O
25 O
percent O
relative O
reduction O
in O
word O

er- B-DAT
ror O
rates O
on O
different O

large B-DAT
vocabulary O
tasks O
while O
using O

signifi- B-DAT
cantly O
smaller O
models. O
Index O

Terms: B-DAT
Hidden O
Markov O
model, O
end-to-end O

, B-DAT
automatic O
speech O
recognition, O
lattice-free O

MMI, B-DAT
flat-start O
1. O
Introduction O
In O
recent O
years O

, B-DAT
end-to-end O
approaches O
to O
automatic O

speech B-DAT
recognition O
have O
received O
a O

lot B-DAT
of O
attention. O
These O
methods O

typ- B-DAT
ically O
aim O
to O
train O

a B-DAT
neural-network-based O
acoustic O
model O
in O

one B-DAT
stage O
without O
relying O
on O

alignments B-DAT
from O
an O
initial O
model O
( O

usu- B-DAT
ally O
an O
HMM-GMM O

model) B-DAT
[1] O
[2] O
[3]. O
For O

simplicity, B-DAT
it O
is O
desirable O
to O

avoid B-DAT
using O
a O
lexicon O
or O

language B-DAT
model O
in O
these O
approaches; O

however B-DAT
using O
a O
language O
model O

usually B-DAT
improves O
the O

results B-DAT
[4] O
[2] O
[5] O
[6]. O
On O
the O
other O
hand, O
conventional O

DNN-based B-DAT
speech O
recog- O
nition O
methods O

(i.e. B-DAT
CD-DNN-HMM) O
rely O
on O
alignments O

and B-DAT
phonetic O
decision O
trees O
from O

an B-DAT
HMM-GMM O
system O
[7]. O
These O

methods B-DAT
usually O
use O
a O
frame-level O

objective B-DAT
function O
– O
such O
as O

cross-entropy B-DAT
– O
for O
training O
the O

DNN B-DAT
using O
the O
alignments O

. B-DAT
Currently, O
three O
popular O
end-to-end O
approaches O

are B-DAT
Con- O
nectionist O
Temporal O
Classification O

(CTC), B-DAT
RNN-Transducers O
and O
attention-based O
methods O

[8]. B-DAT
CTC O
introduces O
a O
sequence O

- B-DAT
level O
objective O
function O
to O

enable B-DAT
training O
a O
neural O
network O

on B-DAT
sequences O
of O
speech O
signals O

without B-DAT
using O
prior O
alignments O
[9] O

and B-DAT
RNN-Transducer O
is O
an O
extension O

of B-DAT
CTC O
with O
two O
sepa- O

rate B-DAT
RNNs O
[10]. O
CTC O
was O

a B-DAT
pioneering O
approach O
in O
end-to-end O

speech B-DAT
recognition O
and O
state-of-the-art O
results O

were B-DAT
achieved O
on O
the O
challenging O

Fisher+Switchboard B-DAT
task O
[11] O
when O
it O

was B-DAT
used O
with O
deep O
recurrent O

neural B-DAT
networks. O
By O
contrast, O
attention-based O
models O
use O

a B-DAT
novel O
structure O
based O
on O

an B-DAT
encoder O
network O
which O
maps O

the B-DAT
input O
sequence O
into O
a O

fixed-sized B-DAT
vector O
and O
a O
decoder O

network B-DAT
which, O
using O
an O
attention O

mechanism, B-DAT
generates O
the O
output O
sequence O

using B-DAT

The B-DAT
first O
author O
performed O
the O

work B-DAT
while O
at O
CLSP, O
Johns O

Hop- B-DAT
kins O
University. O
This O
work O

was B-DAT
partially O
supported O
by O
IARPA O

MATE- B-DAT
RIAL O
award O
no. O
FA8650-17-C-9115 O

and B-DAT
NSF O
grant O
no. O
CRI-1513128. O
this O
vector O
as O
its O
input O

. B-DAT
These O
models O
have O
performed O

very B-DAT
well O
in O
a O
few O

tasks B-DAT
such O
as O
machine O

translation B-DAT
[12] O
but, O
unless O
the O

training B-DAT
data O
is O
very O
large, O

they B-DAT
have O
not O
been O
as O

effective B-DAT
for O
speech O
recognition O

tasks B-DAT
[6]. O
Currently O
the O
lattice-free O
MMI O
(i.e O

. B-DAT
LF-MMI) O
method O
[13] O
achieves O

state-of-the-art B-DAT
results O
on O
many O
speech O

recognition B-DAT
tasks O
[13, O
14, O
15, O
16 O

]. B-DAT
This O
method, O
like O
CTC, O

uses B-DAT
a O
sentence- O
level O
posterior O

for B-DAT
training O
the O
neural O
network O

but B-DAT
unlike O
end- O
to-end O
approaches, O

still B-DAT
loosely O
relies O
on O
alignments O

from B-DAT
an O
HMM-GMM O
model. O
The O

objective B-DAT
function O
used O
in O
this O

method B-DAT
is O
maximum O
mutual O
information O
( O

MMI) B-DAT
in O
the O
context O
of O

hidden B-DAT
Markov O
models O
[17]. O
In O
the O
work O
presented O
here O

, B-DAT
we O
aim O
to O
train O

these B-DAT
powerful O
models O
without O
running O

the B-DAT
common O
HMM-GMM O
training O
and O

tree-building B-DAT
pipeline O
(i.e. O
in O
a O

flat-start B-DAT
manner). O
Two O
prior O

studies B-DAT
[18, O
19] O
performed O
GMM-free O

training, B-DAT
but O
used O
state- O
tying O

decision B-DAT
trees O
(created O
using O
alignments O

from B-DAT
the O
DNN O
model) O
for O

context B-DAT
dependent O
(CD) O
modeling. O
However, O

we B-DAT
do O
not O
use O
state-tying O

trees, B-DAT
and O
we O
perform O
the O

entire B-DAT
training O
pro- O
cess O
in O

one B-DAT
stage O
(i.e. O
without O
generating O

re-alignments, B-DAT
build- O
ing O
trees, O
or O

performing B-DAT
prior O
estimation). O
Another O
difference O

is B-DAT
that O
we O
use O
the O

LF-MMI B-DAT
objective O
function O
instead O
of O

maxi- B-DAT
mum O
likelihood O
(ML) O
for O

training B-DAT
the O
network. O
In O
our O

recently B-DAT
accepted O
journal O
paper O
[20], O

flat-start B-DAT
LF-MMI O
was O
investigated O
in O

a B-DAT
phoneme-based O
setting. O
In O
this O

study, B-DAT
we O
explore O
character- O
based O

training B-DAT
and O
lexicon-free O
decoding O
and O

show B-DAT
that O
the O
end- O
to-end O

LF-MMI B-DAT
setup O
outperforms O
other O
end-to-end O

approaches B-DAT
under O
similar O
conditions. O
In O
the O
two O
following O
sections O

, B-DAT
regular O
LF-MMI O
and O
CTC O

will B-DAT
be O
briefly O
described, O
and O

then B-DAT
in O
Section O
4 O
we O

will B-DAT
describe O
the O
end-to-end O
LF-MMI O

setup. B-DAT
The O
experimental O
setup O
and O

re- B-DAT
sults O
will O
be O
presented O

in B-DAT
Section O
5. O
Finally, O
the O

conclusions B-DAT
appear O
in O
Section O
6. O
2. O
Regular O
LF-MMI O
The O
hidden O

Markov B-DAT
model O
(HMM) O
is O
a O

generative B-DAT
model O
com- O
monly O
used O

for B-DAT
speech O
recognition. O
It O
is O

usually B-DAT
used O
jointly O
with O
a O

Gaussian B-DAT
mixture O
model O
(GMM), O
or O

a B-DAT
DNN O
to O
model O
acoustic O

data. B-DAT
A O
common O
approach O
for O

learning B-DAT
the O
HMM O
parameters O
is O

through B-DAT
maximum O
likelihood O
(ML) O
estimation O

which B-DAT
has O
the O
following O
objective O

function B-DAT

: B-DAT
FML O
= O
U O

∑ B-DAT
u=1 O

log B-DAT
pλ(x O
(u)|Mw(u)) O
= O
U O

∑ B-DAT
u=1 O

log B-DAT
∑ O
s∈M O
w(u O

) B-DAT
Tu−1 O

∏ B-DAT
t=0 O

p(st+1|st)p(x(u)t B-DAT
|st) O
(1) O
where O
λ O
is O
the O
set O

of B-DAT
all O
HMM O
parameters, O
U O

is B-DAT
the O
total O
number O
of O

training B-DAT
utterances, O
and O
x(u) O
is O

the B-DAT
uth O
speech O
utterance O
with O

Interspeech B-DAT
2018 O
2-6 O
September O
2018, O

Hyderabad B-DAT
12 O
10.21437/Interspeech.2018-1423 O

1423 B-DAT

transcription B-DAT
w(u) O
and O
with O
length O

Tu. B-DAT
The O
composite O
HMM O
graph O

Mw(u) B-DAT
represents O
all O
the O
possible O

state B-DAT
sequences O
s O
per- O
taining O

to B-DAT
the O
transcriptionw(u). O
An O
alternative O
objective O
function O
is O

maximum B-DAT
mutual O
in- O
formation O
(MMI O

). B-DAT
MMI O
is O
a O
discriminative O

objective B-DAT
function O
which O
aims O
to O

maximize B-DAT
the O
probability O
of O
the O

reference B-DAT
tran- O
scription, O
while O
minimizing O

the B-DAT
probability O
of O
all O
other O

tran- B-DAT
scriptions: O
FMMI O
= O
U O

∑ B-DAT
u=1 O

log B-DAT
pλ(x O
(u)|Mw(u)) O
pλ(x(u O

)) B-DAT
(2 O

) B-DAT
The O
denominator O
can O
be O
approximated O

as B-DAT

: B-DAT
pλ(x O
(u O

w B-DAT
pλ(x O
(u)|Mw) O
≈ O
pλ(x(u)|Mden) O
(3 O

) B-DAT
where O
Mden O
is O
an O
HMM O

graph B-DAT
that O
includes O
all O
possible O

se- B-DAT
quences O
of O
words. O
This O

is B-DAT
called O
the O
denominator O
graph O

, B-DAT
as O
op- O
posed O
to O

Mw(u) B-DAT
which O
is O
called O
the O

numerator B-DAT
graph. O
The O
denominator O
graph O
has O
traditionally O

been B-DAT
estimated O
us- O
ing O
n-best O

lists B-DAT
and O
later O
using O
lattices O

[21][22]. B-DAT
That O
is O
because O
a O

full B-DAT
denominator O
graph O
can O
make O

the B-DAT
computations O
slow. O
Us- O
ing O

a B-DAT
full O
denominator O
graph O
has O

been B-DAT
investigated O
in O
[23] O
with O

HMM-GMM B-DAT
models. O
More O
recently O
Povey O

et. B-DAT
al O
[13] O
used O
MMI O

training B-DAT
with O
HMM-DNN O
models O
using O

a B-DAT
full O
denomi- O
nator O
graph O

by B-DAT
adopting O
a O
few O
different O

techniques B-DAT
such O
as O
us- O
ing O

a B-DAT
phone O
language O
model O
(LM O

) B-DAT
(instead O
of O
a O
word O

LM) B-DAT
for O
the O
denominator O
graph O

and B-DAT
most O
importantly O
performing O
the O

de- B-DAT
nominator O
computation O
on O
GPU O

hardware. B-DAT
The O
phone O
LM O
for O

the B-DAT
denominator O
graph O
was O
a O

pruned B-DAT
n-gram O
LM O
trained O
using O

the B-DAT
phone O
alignments O
of O
the O

training B-DAT
data. O
Also, O
the O
composite O

HMM B-DAT
was O
not O
used O
as O

the B-DAT
numerator O
graph O
and O
instead O

a B-DAT
spe- O
cial O
acyclic O
graph O

was B-DAT
used O
which O
could O
exploit O

the B-DAT
alignment O
information O
from O
a O

previous B-DAT
HMM-GMM O
model. O
More O
specif- O

ically, B-DAT
the O
numerator O
graph O
in O

the B-DAT
regular O
LF-MMI O
method O
is O

an B-DAT
expanded O
version O
of O
the O

composite B-DAT
HMM, O
where O
the O
amount O

of B-DAT
expansion O
of O
the O
self-loops O

for B-DAT
each O
utterance O
is O
determined O

according B-DAT
to O
its O
alignment O
(i.e. O

it B-DAT
has O
no O
self-loops). O
The O

phone B-DAT
model O
used O
with O
regular O

LF-MMI B-DAT
is O
a O
2-state O
HMM O

as B-DAT
shown O
in O
Figure O
1c. O
3. O
CTC O
The O
CTC O
method O

uses B-DAT
a O
blank O
label O

– B-DAT
which O
can O
appear O
between O

characters B-DAT
– O
to O
define O
an O

objective B-DAT
function O
which O
sums O
over O

all B-DAT
possible O
alignments O
of O
the O

reference B-DAT
label O
sequence O
with O
the O

input B-DAT
sequence O
of O
speech O
frames O

[9 B-DAT

]: B-DAT
FCTC O
= O
U O

∑ B-DAT
u=1 O

log B-DAT
p(w(u)|x(u)) O
= O
U O

∑ B-DAT
u=1 O

log B-DAT
∑ O
π∈B−1(w(u O

)) B-DAT
Tu−1 O

∏ B-DAT
t=0 O

p(πt|x(u)) B-DAT
(4) O
where O
p(πt|x(u)) O
is O
the O
network O

output B-DAT
for O
label O
sequence O
π O

at B-DAT
time O
t O
given O
utterance O

x(u), B-DAT
and O
B O
is O
a O

many-to-one B-DAT
map O
that O
removes O
repetitive O

labels B-DAT
and O
then O
blanks O
from O

a B-DAT
label O
sequence O

. B-DAT
3.1. O
Relation O
to O
HMM O

The B-DAT
CTC O
objective O
function O
can O

be B-DAT
thought O
of O
as O
the O

HMM B-DAT
likelihood O
over O
a O
composite O

HMM, B-DAT
where O
each O
label O
(e.g. O

a B-DAT
character, O
in O
character-based O
CTC) O

has B-DAT
a O
special O
2-state O
HMM O
topology O
as O
shown O
in O
Figure O

1a B-DAT
[24]. O
If O
we O
create O

the B-DAT
compos- O
ite O
HMM O
by O

starting B-DAT
with O
a O
blank O
state O

(with B-DAT
a O
self-loop O
and O
a O

forward B-DAT
null O
transition) O
and O
concatenate O

the B-DAT
label O
HMMs, O
while O
inserting O

a B-DAT
single O
blank O
state O
between O

repetitive B-DAT
labels, O
we O
can O
see O

that B-DAT
the O
set O
of O
all O

paths B-DAT
in O
this O
composite O
HMM O

is B-DAT
identical O
to O
the O
set O

{π|π B-DAT
∈ O
B−1(w(u))}. O
Therefore, O
comparing O

equations B-DAT
1 O
and O
4 O
we O

can B-DAT
see O
that O
CTC O
is O

a B-DAT
special O
case O
of O
HMM O

, B-DAT
when O
the O
state O
priors, O

observation B-DAT
priors, O
and O
transition O
probabilities O

are B-DAT
all O
uniform O
and O
fixed. O

Since B-DAT
CTC O
was O
the O
first O

successful B-DAT
method O
used O
for O
end-to-end O

speech B-DAT
recognition, O
we O
will O
use O

its B-DAT
HMM O
topology O
in O
our O

setup B-DAT
to O
compare O
with O
the O

other B-DAT
HMM O
topologies O
shown O
in O

Figure B-DAT
1. O
4. O
End-to-end O
LF-MMI O
In O
regular O

LF-MMI, B-DAT
the O
DNN O
outputs O
correspond O

to B-DAT
tied O
bi- O
phone O
or O

triphone B-DAT
HMM O
states, O
where O
the O

tying B-DAT
is O
done O
accord- O
ing O

to B-DAT
a O
context-dependency O
tree. O
This O

tree B-DAT
is O
in O
turn O
created O

using B-DAT
alignments O
from O
an O
HMM-GMM O

system B-DAT
[25]. O
We O
re- O
move O

this B-DAT
prerequisite O
by O
using O
monophones O

or B-DAT
full O
biphones O
(cf. O
Section O

4.1). B-DAT
Moreover, O
we O
use O
the O

composite B-DAT
HMM O
(with O
self-loops) O
as O

the B-DAT
numerator O
graph O
instead O
of O

the B-DAT
special O
acyclic O
graph O
used O

in B-DAT
regular O
LF-MMI O

. B-DAT
As O
a O
result, O
unlike O
regular O

LF-MMI, B-DAT
there O
is O
no O
prior O

align- B-DAT
ment O
information O
in O
the O

numerator B-DAT
graph O
and O
there O
is O

no B-DAT
re- O
striction O
on O
the O

self-loops B-DAT
so O
there O
is O
much O

more B-DAT
freedom O
for O
the O
neural O

network B-DAT
to O
learn O
the O
alignments O

. B-DAT
Since O
we O
do O
not O

have B-DAT
alignments O
for O
the O
training O

data, B-DAT
we O
estimate O
the O
phone O

language B-DAT
model O
for O
the O
denominator O

graph B-DAT
using O
the O
training O
transcriptions O
( O

choosing B-DAT
a O
random O
pronunciation O
for O

words B-DAT
with O
alternative O
pronunciations O
in O

the B-DAT
phoneme-based O
setting), O
after O
inserting O

silence B-DAT
phones O
with O
probability O
0.2 O

between B-DAT
the O
words O
and O
with O

probability B-DAT
0.8 O
at O
the O
beginning O

and B-DAT
end O
of O
the O
sen- O

tences. B-DAT
The O
derivatives O
for O
MMI O

are B-DAT
as O
follows: O
∂FMMI O
∂y O

u) B-DAT
t O
(s) O
= O
NUMγ O
(u) O
t O
(s O

)− B-DAT
DENγ(u)t O
(s) O
(5) O
where O
y(u)t O
(s) O
is O
the O

network B-DAT
output O
for O
state O
s O

at B-DAT
time O
t O
given O
in O

- B-DAT
put O
utterance O
u O
which O

we B-DAT
interpret O
as O
the O
logarithm O

of B-DAT
an O
HMM O
state O
likelihood O
( O

i.e. B-DAT
log O
p(xt|s)) O
since O
state O

priors B-DAT
have O
no O
ef- O
fect O

in B-DAT
MMI O
training O
[13]. O
NUMγ(u)t O
( O

s) B-DAT
is O
the O
numerator O
HMM O

occupation B-DAT
probability O
for O
state O
s O

at B-DAT
time O
t O
for O
utterance O

u, B-DAT
and O
DENγ O
(u) O
t O
(s) O
is O
defined O

similarly B-DAT
for O
the O
denominator O
graph O

. B-DAT
The O
HMM O
transition O
probabilities O
are O

fixed B-DAT
in O
our O
setup. O
Training O

these B-DAT
shouldn’t O
make O
a O
difference O

as B-DAT
long O
as O
there O
is O

no B-DAT
state-tying O
because O
their O
effect O

can B-DAT
be O
fully O
replicated O
by O

the B-DAT
neural O
network O
output O
(i.e O

. B-DAT
the O
transition O
probabilities O
act O

like B-DAT
a O
scale O
for O
the O

state B-DAT
likelihoods). O
In O
other O
words, O

the B-DAT
network O
will O
ignore O
them. O
4.1. O
Tree-free O
context-dependent O
modeling O

Our B-DAT
initial O
experiments O
with O
monophone O

end-to-end B-DAT
LF-MMI O
showed O
a O
remarkable O

gap B-DAT
between O
the O
results O
of O

end-to-end B-DAT
and O
regular O
LF-MMI. O
To O

enable B-DAT
context-dependent O
modeling O
in O
an O

end-to-end B-DAT
manner, O
we O
adopt O
a O

simple B-DAT
approach O
where O
we O
use O

full B-DAT
left O
biphones O
(or O
bichars O

in B-DAT
the O
character-based O
case). O
This O

is B-DAT
implemented O
as O
a O
trivial O

full B-DAT
biphone O
tree. O
This O
tree O

is B-DAT
not O
pruned O
at O
all O
( O

and B-DAT
does O
not O
do O
any O

tying), B-DAT
so O
there O
is O
no O

need B-DAT
for O
alignments O
and O
the O

approach B-DAT
may O
be O
considered O
end-to-end O
( O

in B-DAT
the O
sense O
of O
not O

requiring B-DAT
any O
previously O
trained O
models). O

In B-DAT
other O
words, O
we O
assume O

a B-DAT
separate O
HMM O
model O
for O

each B-DAT
and O
every O
possible O
pair O

of B-DAT
phonemes O
(or O
characters O
in O

character- B-DAT
13 O

based B-DAT
conditions). O
1 O
This O
will O

create B-DAT
biphones O
that O
never O
occur O

in B-DAT
the O
training O
data, O
but O

they B-DAT
are O
never O
activated O
during O

training B-DAT
and O
the O
network O
learns O

to B-DAT
ignore O

a) B-DAT
CTC’s O
HMM O
topology O
(b) O
1 O

-state B-DAT
HMM O
topology O
(c) O
2-state O
HMM O
topology O
(d O

) B-DAT
3-state O
HMM O
topology O
Figure O
1: O
Different O
HMM O
topologies O

. B-DAT
The O
state O
marked O

with B-DAT
“-” O
is O
CTC’s O
blank O

state B-DAT
and O
is O
shared O
across O

all B-DAT
the O
labels. O
5. O
Experiments O
5.1. O
Experimental O
setup O

We B-DAT
do O
most O
of O
our O

experiments B-DAT
on O
two O
ASR O
corpora: O

Switch- B-DAT
board O
[26], O
and O
WSJ O
( O

Wall B-DAT
Street O
Journal) O
[27]. O
Switchboard O

is B-DAT
a O
database O
with O
300 O

hours B-DAT
of O
transcribed O
speech. O
We O

eval- B-DAT
uate O
on O
the O

Hub5 B-DAT
’00 O
set O
(also O
known O

as B-DAT
eval2000). O
We O
re- O
port O

word B-DAT
error O
rates O
(WER) O
on O

the B-DAT
”switchboard” O
portion O
of O
eval2000 O
( O

indicated B-DAT
as O
SW) O
but O
where O

stated, B-DAT
we O
also O
report O
WER O

on B-DAT
the O
Callhome O
subset O
(indicated O

as B-DAT
CH). O
Where O
stated, O
we O

use B-DAT
the O
Fisher O
data O
for O

acoustic B-DAT
modeling O
as O
well O
(together O

with B-DAT
Switchboard, O
a O
total O
of O
2000 O
hours). O
For O
decoding O
we O
use O

the B-DAT
Fisher+Switchboard O
training O
transcriptions O
to O

train B-DAT
a O
4-gram O
word O
LM O

(in B-DAT
lexicon-based O
settings) O
or O
a O

9-gram B-DAT
character O
LM O
(in O
lexicon-free O

settings). B-DAT
WSJ O
is O
a O
database O

with B-DAT
80 O
hours O
of O
transcribed O

speech. B-DAT
We O
test O
on O
the O

”eval92” B-DAT
subset. O
For O
lexicon- O
based O

decoding, B-DAT
we O
use O
a O
3-gram O

LM B-DAT
trained O
on O
the O
WSJ O

train- B-DAT
ing O
set O
transcriptions O
using O

an B-DAT
extended O
lexicon O
as O
in O

[4 B-DAT

]. B-DAT
For O
running O
the O
experiments, O
we O

use B-DAT
Kaldi O
[28]2. O
We O
do O

not B-DAT
use O
i-vectors O
or O
other O

speaker B-DAT
adaptation O
techniques O
in O
any O

of B-DAT
the O
experiments. O
In O
all O

the B-DAT
experiments O
we O
use O
a O

TDNN- B-DAT
LSTM O
structure O
[14], O
which O

has B-DAT
interleaving O
LSTM O
[29] O
and O

TDNN B-DAT
layers O
[30]; O
please O
refer O

to B-DAT
[14] O
for O
more O
details O

. B-DAT
As O
in O
[13], O
we O

use B-DAT
a O
frame O
subsampling O
factor O

of B-DAT
3 O
which O
speeds O
up O

training B-DAT
by O
a O
factor O
of O
2 O

. B-DAT
We O
also O
augment O
the O

data B-DAT
with O
2-fold O
speed O
perturbation O

in B-DAT
all O
the O
experiments O
[31] O

unless B-DAT
otherwise O
stated. O
In O
all O
the O
end-to-end O
experiments O

, B-DAT
we O
use O
SGD O
to O

train B-DAT
the O
network O
(in O
a O

single B-DAT
stage, O
for O
4 O
epochs), O

on B-DAT
40-dimensional O
MFCC O
features O
extracted O

from B-DAT
25ms O
frames O
every O
10ms. O

The B-DAT
features O
are O
normalized O
on O

a B-DAT
per-speaker O
basis O
to O
have O

zero B-DAT
mean O
and O
unit O
variance; O

no B-DAT
other O
feature O
normalization O
or O

fea- B-DAT
ture O
transform O
is O
used. O

The B-DAT
network O
parameters O
are O
initialized O

randomly B-DAT
to O
have O
zero O
mean O

and B-DAT
a O
small O
variance. O
Unlike O

other B-DAT
work, O
we O
do O
not O

perform B-DAT
re-alignments O
during O
training. O
In O
regular O
LF-MMI, O
all O
utterances O

are B-DAT
split O
into O
chunks O
of O

150 B-DAT
frames O
to O
make O
GPU O

computations B-DAT
efficient. O
However, O
in O
end-to-end O

LF-MMI, B-DAT
we O
can’t O
split O
the O

utterances B-DAT
because O
we O
don’t O
have O

alignments. B-DAT
Instead, O
we O
ensure O
that O

all B-DAT
the O
utter- O
ances O
are O

modified B-DAT
to O
be O
one O
of O

around B-DAT
30 O
distinct O
lengths O
(i.e O

. B-DAT
1For O
example, O
on O
WSJ, O
which O

has B-DAT
42 O
phonemes O
(including O
silence O

), B-DAT
we O
will O
have O
a O

total B-DAT
of O
43*42*2 O
= O
3612 O

HMM B-DAT
states O
(which O
are O
not O

tied) B-DAT
when O
using O
a O
2-state O

HMM B-DAT
topology. O
2This O
toolkit O
is O
open-source O
and O

the B-DAT
source O
code O
related O
to O

this B-DAT
study O
are O
available O
online O

for B-DAT
reproducing O
the O
results O

. B-DAT
Table O
1: O
Effect O
of O
using O

different B-DAT
HMM O
topologies O
in O
end-to-end O

LF-MMI. B-DAT
1state O
means O
1-state O
HMM O

topology B-DAT
and O
so O
on O
(as O

in B-DAT
Figure O
1). O
CT O
means O

CTC’s B-DAT
equivalent O
HMM O
topology O
(Figure O

1a). B-DAT
These O
results O
are O
without O

CD B-DAT
modeling O

. B-DAT
Phoneme O
Character O
1state O
2state O
3state O

CT B-DAT
1state O
2state O
3state O

Switchboard B-DAT
11.7 O
10.7 O
10.7 O
14.5 O
14 O

.2 B-DAT
13.3 O
13.2 O
WSJ O
3.1 O
3 O

.1 B-DAT
3.3 O
5.4 O
5.3 O
5.2 O
5 O

.4 B-DAT
Table O
2: O
Effect O
of O
full O

tree-free B-DAT
biphone/bichar O
modeling O
in O
end O

- B-DAT
to-end O
LF-MMI O
(EE-LF-MMI). O
Switchboard O
WSJ O
Phone O
Char O
Phone O

Char B-DAT

Regular B-DAT
LF-MMI O
9.1 O
10.4 O
2.8 O
3 O

.5 B-DAT
EE-LF-MMI O
(monophone) O
10.7 O
13.3 O
3 O

.1 B-DAT
5.2 O
EE-LF-MMI O
(full O
biphone) O
9 O

.6 B-DAT
10.9 O
3.0 O
4.1 O
EE-LF-MMI O
( O

regular B-DAT
biphone)* O
9.3 O
10.5 O
2.9 O
3 O

.7 B-DAT
* O
This O
uses O
regular O

LF-MMI’s B-DAT
context-dependency O
tree. O
buckets). O
When O
using O
speed O
perturbation O

, B-DAT
we O
modify O
the O
length O

of B-DAT
each O
utterance O
to O
the O

nearest B-DAT
of O
the O
distinct O
lengths. O

Other- B-DAT
wise, O
we O
can O
pad O

each B-DAT
utterance O
with O
silence O
to O

reach B-DAT
one O
of O
the O
distinct O

lengths. B-DAT
5.2. O
Phone/Character O
HMM O
Topology O

One B-DAT
of O
the O
advantages O
of O

using B-DAT
HMM O
is O
that O
we O

can B-DAT
poten- O
tially O
improve O
the O

alignment B-DAT
learning O
process O
by O
designing O

the B-DAT
HMM O
topology O
for O
the O

phones B-DAT
(or O
characters). O
We O
compare O

three B-DAT
topologies O
as O
shown O
in O

Figure B-DAT
1{b,c,d} O
in O
Table O
1, O

both B-DAT
in O
character-based O
and O
phoneme-based O

setups. B-DAT
For O
the O
character- O
based O

setup, B-DAT
we O
also O
test O
with O

CTC’s B-DAT
equivalent O
HMM O
topology O
(Figure O

1a). B-DAT
It O
can O
be O
seen O

that B-DAT
CTC’s O
topology O
performs O
sim- O

ilar B-DAT
to O
a O
1-state O
HMM. O

Also B-DAT
a O
2-state O
model O
performs O

remark- B-DAT
ably O
better O
than O
a O

single B-DAT
state O
model O
but O
a O
3 O

-state B-DAT
model O
does O
not O
significantly O

outperform B-DAT
the O
2-state O
model. O
For O

the B-DAT
rest O
of O
the O
experiments O

in B-DAT
this O
paper, O
we O
use O

the B-DAT
2-state O
HMM O
topology. O
5.3. O
Tree-free O
full O
biphone O
modeling O

The B-DAT
first O
two O
rows O
of O

Table B-DAT
2 O
compare O
monophone O
end-to-end O

LF-MMI B-DAT
results O
with O
regular O
LF-MMI O
( O

using B-DAT
regular O
pruned O
biphone O
tree) O

results. B-DAT
Note O
that O
with O
regular O

LF-MMI, B-DAT
conven- O
tional O
biphone O
and O

triphone B-DAT
trees O
lead O
to O
similar O

WERs B-DAT
(not O
shown). O
We O
can O

see B-DAT
there O
is O
a O
large O

gap B-DAT
between O
regular O
and O
end-to-end O

LF-MMI B-DAT
in O
all O
cases O
except O

in B-DAT
phoneme-based O
WSJ O
which O
is O

fairly B-DAT
easier O
than O
other O
tasks. O

The B-DAT
third O
row O
of O
Table O
2 O
shows O
the O
impact O
of O
full O

CD B-DAT
(i.e. O
context-dependent) O
model- O
ing O

using B-DAT
biphones/bichars O
as O
explained O
in O

Section B-DAT
4.1, O
which O
has O
helped O

significantly. B-DAT
In O
particular, O
for O
Switchboard O

it B-DAT
has O
improved O
the O
WER O

by B-DAT
1.1% O
in O
phoneme-based O
and O

2.4% B-DAT
in O
character-based O
setups. O
This O

means B-DAT
that O
in O
a O
phoneme-based O

setup, B-DAT
end-to-end O
LF-MMI O
is O
only O

0.5% B-DAT
worse O
than O
regular O
LF O

- B-DAT
MMI O
on O
the O
300hr O

Switchboard B-DAT
task O
and O
almost O
the O

same B-DAT
on O
WSJ. O
For O
WSJ, O

there B-DAT
is O
no O
improvement O
in O

the B-DAT
phoneme-based O
setup O
but O
the O

WER B-DAT
has O
been O
improved O
more O

than B-DAT
1% O
in O
the O
character-based O

setup. B-DAT
For O
comparison, O
we O
also O

show B-DAT
the O
re- O
sult O
of O

using B-DAT
regular O
LF-MMI’s O
tree O
(which O

is B-DAT
a O
pruned O
context- O
dependency O

tree B-DAT
built O
using O
HMM-GMM O
alignments) O

in B-DAT
our O
ap- O
proach. O
Note O

that B-DAT
this O
is O
not O
end-to-end O

any B-DAT
more. O
We O
can O
see O

that B-DAT
our O
simple O
full O
CD O

technique B-DAT
performs O
almost O
as O
well O

as B-DAT
common O
tree-based O
CD O
modeling. O
14 O

Table B-DAT
3: O
Comparison O
of O
WER O

for B-DAT
character-based O
end-to-end O
LF-MMI O
(EE-LF-MMI) O

and B-DAT
CTC O
on O
the O
300hr O

Switchboard. B-DAT
Method O
Parameters O
Lexicon O
LM O
SW O

CH B-DAT
CTC O
[32] O
50M O
N O

Char B-DAT
NG O
19.8 O
32.1 O
EE-LF-MMI O

26M B-DAT
N O
Char O
NG O
14.4 O

25.2 B-DAT
EE-LF-MMI O
26M O
N O
Char O

RNN B-DAT
13.0 O
23.6 O
CTC O
[32 O

] B-DAT
50M O
Y O
Word O
NG O
15 O

.1 B-DAT
26.3 O
EE-LF-MMI O
26M O
Y O

Word B-DAT
NG O
10.9 O
20.6 O

CTC B-DAT
[32] O
50M O
Y O
Word O

RNN B-DAT
14.0 O
25.3 O
EE-LF-MMI O
26M O

Y B-DAT
Word O
RNN O
9.3 O
18.9 O

EE-LF-MMI B-DAT
no-SP O
26M O
Y O
Word O

RNN B-DAT
10.2 O
20.0 O
Table O
4: O
Comparison O
of O
WER O

for B-DAT
character-based O
end-to-end O
LF-MMI O
(EE-LF-MMI O

) B-DAT
and O
related O
methods O
on O

the B-DAT
2000hr O
Fisher+Switchboard O
task. O
no-SP O

means B-DAT
no O
speed O
perturbation. O
Tot O

means B-DAT
on O
all O
of O
eval2000. O
Method O
Params O
Lex. O
LM O
SW O

CH B-DAT
Tot† O
CTC O
[32] O
50M O

N B-DAT
Char O
NG O
13.8 O
21.8 O

17.8 B-DAT
Attention* O
[33] O
100M O
N O

N B-DAT
8.6 O
17.8 O
13.2 O
RNN-T O

* B-DAT
[33] O
120M O
N O
N O
8 O

.5 B-DAT
16.4 O
12.5 O
EE-LF-MMI O
26M O

N B-DAT
Char O
NG O
12.1 O
21.7 O
16 O

.9 B-DAT
EE-LF-MMI O
26M O
N O
Char O

RNN B-DAT
12.0 O
21.9 O
17.0 O

CTC B-DAT
[32] O
50M O
Y O
Word O

NG B-DAT
11.3 O
18.7 O
15.0 O

RNN-T* B-DAT
[33] O
120M O
Y O
Word O

NG B-DAT
8.1 O
17.5 O
12.8 O
EE-LF-MMI O

26M B-DAT
Y O
Word O
NG O
9.3 O
18 O

.6 B-DAT
14.0 O
EE-LF-MMI O
no-SP O
26M O

Y B-DAT
Word O
NG O
9.7 O
19.0 O
14 O

.4 B-DAT
CTC O
[32] O
50M O
Y O

Word B-DAT
RNN O
10.2 O
17.7 O
14.0 O

EE-LF-MMI B-DAT
26M O
Y O
Word O
RNN O
8 O

.0 B-DAT
17.6 O
12.8 O
Phone O

CTC B-DAT
[34] O
– O
Y O
Word O

NG B-DAT
10.2 O
16.5 O
13.3 O
Phone O

EE-LF-MMI B-DAT
26M O
Y O
Word O
NG O
8 O

.6 B-DAT
15.5 O
12.0 O
Phone O
EE-LF-MMI O

26M B-DAT
Y O
Word O
RNN O
7.5 O
14 O

.6 B-DAT
11.0 O
Regular O
LF-MMI O
28M O

Y B-DAT
Word O
NG O
8.3 O
15.0 O
11 O

.6 B-DAT
Regular O
LF-MMI O
28M O
Y O

Word B-DAT
RNN O
7.3 O
14.2 O
10 O

.7 B-DAT
* O
These O
use O
data O

augmentation B-DAT
by O
adding O
background O

noise. B-DAT
† O
The O
total O
eval2000 O

WER B-DAT
for O
CTC O
and O
Attention O

is B-DAT
the O
average O
of O
SW O

and B-DAT
CH O
(as O
it O
is O

not B-DAT
reported). O
5.4. O
Comparison O
to O
other O
end-to-end O

approaches B-DAT

In B-DAT
this O
section O
we O
compare O

end-to-end B-DAT
LF-MMI O
with O
other O
end-to-end O

methods. B-DAT
Tables O
3 O
and O
4 O

show B-DAT
the O
results O
on O
the O

300hr B-DAT
Switchboard O
and O
2000hr O
Fisher+Switchboard O

tasks B-DAT
re- O
spectively, O
and O
Table O
5 O
shows O
the O
results O
on O
WSJ O

. B-DAT
The O
“LM” O
column O
shows O

what B-DAT
LM O
was O
used O
for O

decoding. B-DAT
The O
characters O
we O
use O

in B-DAT
character-based O
modeling O
are O
the O

digits, B-DAT
the O
letters, O
apostrophe, O
and O

space. B-DAT
We O
report O
WER O
for O

both B-DAT
lexicon-based O
and O
lexicon-free O
decoding. O

In B-DAT
lexicon-free O
decoding O
we O
decode O

characters B-DAT
and O
separate O
the O
words O

by B-DAT
the O
decoded O
space O
charac- O

ters. B-DAT
The O
LMs O
we O
use O

for B-DAT
lexicon-free O
decoding O
are O
character O

n-grams B-DAT
(Char O
NG) O
and O
character O

RNN-LMs. B-DAT
We O
use O
a O
9-gram O

character B-DAT
LM O
trained O
on O
the O

training B-DAT
transcriptions. O
On O
the O
larger O
2000hr O
Fisher+Switchboard O

, B-DAT
end-to-end O
LF- O
MMI O
has O

achieved B-DAT
around O
1% O
improvement O
(on O

all B-DAT
of O
eval2000) O
over O
CTC O

in B-DAT
the O
lexicon-free O
decoding O
case O

but B-DAT
the O
best O
results O
are O

for B-DAT
RNN-Transducer. O
When O
using O
lexicon-based O

decod- B-DAT
ing, O
character-based O
end-to-end O
LF-MMI O

and B-DAT
RNN-Transducer O
achieve O
the O
same O

result B-DAT
(12.8) O
outperforming O
CTC O
(14.0). O

How- B-DAT
ever, O
the O
best O
overall O

results B-DAT
are O
for O
the O
phoneme-based O

end- B-DAT
to-end O
LF-MMI O
achieving O
a O

total B-DAT
WER O
of O
11.0 O
on O

eval2000 B-DAT
(7.5 O
on O
the O
Switchboard O

subset). B-DAT
For O
comparison, O
the O
WERs O

for B-DAT
regular O
LF-MMI O
(which O
is O

not B-DAT
end-to-end) O
are O
also O
shown O
Table O
5: O
Comparison O
of O
WER O

for B-DAT
character-based O
end-to-end O
LF-MMI O
(EE-LF-MMI O

) B-DAT
and O
related O
methods O
on O

WSJ. B-DAT
Method O
Parameters O
Lexicon O
LM O
WER O

Phone B-DAT
CTC O
[4] O
– O
Y O

Word B-DAT
NG O
7.3 O
Attention O
[35 O

] B-DAT
6.6M O
Y O
Word O
NG O
6 O

.7 B-DAT
EE-LF-MMI O
8.2M O
Y O
Word O

NG B-DAT
4.1 O
EE-LF-MMI O
no-SP O
8.2M O

Y B-DAT
Word O
NG O
5.3 O
EE-LF-MMI O
8 O

.2M B-DAT
N O
Char O
NG O
5.4 O
at O
the O
end O
of O
Table O

4. B-DAT
Note O
that O
the O
models O

used B-DAT
in O
end-to- O
end O
LF-MMI O

are B-DAT
considerably O
smaller. O
Also O
note O

that B-DAT
in O
train- O
ing O
the O

attention-based B-DAT
and O
RNN-Transducer O
models O
(which O

are B-DAT
substantially O
larger) O
[33], O
data O

augmentation B-DAT
with O
background O
noise O
has O

been B-DAT
applied. O
For O
comparison, O
we O

have B-DAT
included O
re- O
sults O
without O

speed B-DAT
perturbation O
(no-SP) O
too. O
Meanwhile O

, B-DAT
we O
see O
significant O
improvements O

on B-DAT
the O
smaller O
300hr O
Switchboard O

and B-DAT
80hr O
WSJ O
task, O
in O

both B-DAT
lexicon-free O
and O
lexicon-based O
de- O

coding. B-DAT
Specifically, O
we O
see O
4 O

to B-DAT
5 O
percent O
absolute O
improve- O

ment B-DAT
in O
WER O
on O
the O

300hr B-DAT
Switchboard O
task, O
and O
1.4 O

percent B-DAT
improvement O
on O
WSJ O
in O

similar B-DAT
conditions O
(i.e. O
no-SP). O
5.5. O
Training O
and O
decoding O
speed O

Even B-DAT
though O
the O
LF-MMI O
objective O

function B-DAT
requires O
a O
denom- O
inator O

computation B-DAT
which O
is O
nontrivial, O
the O

training B-DAT
is O
quite O
fast O
because O

we B-DAT
can O
use O
considerably O
smaller O

models B-DAT
(compared O
to O
other O
end-to-end O

models B-DAT
or O
CD-HMM-DNN O
models O
which O

use B-DAT
cross-entropy) O
while O
achieving O
better O

results. B-DAT
For O
exam- O
ple, O
the O

training B-DAT
speed O
of O
end-to-end O
LF-MMI O

on B-DAT
the O
2000hr O
Fisher+Switchboard O
task O

is B-DAT
approximately O
2.1 O
hours O
of O

speech B-DAT
per O
minute O
on O
a O

GeForce B-DAT
GTX O
1080 O
Ti O
GPU. O

The B-DAT
overall O
data O
preparation O
and O

feature B-DAT
extraction O
takes O
about O
20 O

hours B-DAT
on O
a O
32- O
core O

machine B-DAT
and O
the O
overall O
network O

training B-DAT
lasts O
about O
3 O
days O

on B-DAT
a O
machine O
with O
8 O

GTX B-DAT
1080 O
Ti O
GPUs. O
The O

decoding B-DAT
real- O
time O
factor O
is O
0 O

.9 B-DAT
on O
Switchboard O
and O
0.4 O

on B-DAT
WSJ O
(on O
CPU). O
6. O
Conclusions O

In B-DAT
this O
study, O
we O
described O

a B-DAT
simple O
HMM-based O
end-to-end O
method O

for B-DAT
ASR O
and O
evaluated O
it O

on B-DAT
well-known O
large O
vo- O
cabulary O

speech B-DAT
recognition O
tasks. O
This O
acoustic O

model B-DAT
is O
all-neural O
(except O
for O

decoding/LM B-DAT
part), O
GMM-free, O
tree- O
free, O

and B-DAT
is O
trained O
in O
a O

flat-start B-DAT
manner O
in O
one O
stage O
( O

us- B-DAT
ing O
lattice-free O
MMI) O
without O

requiring B-DAT
any O
initial O
alignments, O
pre-training, O

prior B-DAT
estimation, O
or O
transition O
training. O

Through B-DAT
experiments, O
we O
showed O
that O

our B-DAT
end-to-end O
method O
outper- O
forms O

other B-DAT
end-to-end O
methods O
in O
similar O

settings, B-DAT
especially O
on O
smaller O
databases O

such B-DAT
as O
the O
300hr O
Switchboard O

or B-DAT
80hr O
WSJ O
tasks O
where O

the B-DAT
relative O
improvements O
in O
WER O

range B-DAT
from O
15 O
to O
25 O

percent. B-DAT
By O
training O
our O
end-to-end O

model B-DAT
on O
the O
2000hr O
Fisher+Switchboard O

database, B-DAT
we O
achieved O
a O
WER O

of B-DAT
12.8 O
on O
all O
of O

eval2000 B-DAT
(8.0 O
on O
the O
Switchboard O

subset) B-DAT
in O
the O
character-based O
case, O

and B-DAT
a O
WER O
of O
11.0 O

on B-DAT
all O
of O
eval2000 O
(7.5 O

on B-DAT
the O
Switchboard O
subset) O
in O

the B-DAT
phoneme-based O
setting. O
We O
also O

showed B-DAT
that O
by O
using O
a O

full B-DAT
biphone O
modeling O
technique, O
our O

approach B-DAT
can O
perform O
almost O
as O

well B-DAT
as O
regular O
LF-MMI. O
7. O
Acknowledgements O

The B-DAT
authors O
would O
like O
to O

thank B-DAT
Pegah O
Ghahremani, O
Vimal O
Manohar, O

and B-DAT
Arlo O
Faria O
for O
their O

valuable B-DAT
comments. O
15 O

8. B-DAT
References O
[1] O
A. O
Graves, O

A.-r. B-DAT
Mohamed, O
and O
G. O

Hinton, B-DAT
“Speech O
recognition O
with O
deep O
recurrent O
neural O
networks O

,” B-DAT
in O
Proceedings O
of O
IEEE O

In- B-DAT
ternational O
Conference O
on O
Acoustics, O

Speech B-DAT
and O
Signal O
Process- O
ing O
( O

ICASSP). B-DAT
IEEE, O
2013, O
pp. O
6645–6649. O
[2] O
A. O
Graves O
and O
N O

. B-DAT
Jaitly, O
“Towards O
end-to-end O
speech O

recognition B-DAT
with O
recurrent O
neural O
networks,” O

in B-DAT
Proceedings O
of O
the O
31st O

Inter- B-DAT
national O
Conference O
on O
Machine O

Learning, B-DAT
2014, O
pp. O
1764–1772. O
[3] O
A. O
Hannun, O
C. O
Case O

, B-DAT
J. O
Casper, O
B. O
Catanzaro, O

G. B-DAT
Diamos, O
E. O
Elsen, O
R. O

Prenger, B-DAT
S. O
Satheesh, O
S. O
Sengupta, O

A. B-DAT
Coates O
et O
al., O
“Deep O

speech: B-DAT
Scaling O
up O
end-to-end O
speech O

recognition,” B-DAT
arXiv O
preprint O
arXiv:1412.5567, O
2014. O
[4] O
Y. O
Miao, O
M. O
Gowayyed O

, B-DAT
and O
F. O
Metze, O
“EESEN: O

End-to-end B-DAT
speech O
recognition O
using O
deep O

RNN B-DAT
models O
and O
WFST-based O
de- O

coding,” B-DAT
in O
Proceedings O
of O
IEEE O

Workshop B-DAT
on O
Automatic O
Speech O
Recognition O

and B-DAT
Understanding O
(ASRU). O
IEEE, O
2015, O

pp. B-DAT
167– O
174. O
[5] O
A. O
Maas, O
Z. O
Xie O

, B-DAT
D. O
Jurafsky, O
and O
A. O

Ng, B-DAT
“Lexicon-free O
conversa- O
tional O
speech O

recognition B-DAT
with O
neural O
networks,” O
in O

Proceedings B-DAT
of O
Conference O
of O
the O

North B-DAT
American O
Chapter O
of O
the O

Association B-DAT
for O
Computational O
Linguistics: O
Human O

Language B-DAT
Technologies, O
2015, O
pp. O
345–354. O
[6] O
D. O
Bahdanau, O
J. O
Chorowski O

, B-DAT
D. O
Serdyuk, O
P. O
Brakel, O

and B-DAT
Y. O
Ben- O
gio, O
“End-to-end O

attention-based B-DAT
large O
vocabulary O
speech O
recog- O

nition,” B-DAT
in O
Proceedings O
of O
IEEE O

International B-DAT
Conference O
on O
Acoustics, O
Speech O

and B-DAT
Signal O
Processing O
(ICASSP). O
IEEE, O
2016, O
pp. O
4945–4949 O

. B-DAT
[7] O
G. O
E. O
Dahl, O
D O

. B-DAT
Yu, O
L. O
Deng, O
and O

A. B-DAT
Acero, O
“Context-dependent O
pre-trained O
deep O

neural B-DAT
networks O
for O
large-vocabulary O
speech O

recognition,” B-DAT
IEEE O
Transactions O
on O
Audio, O

Speech, B-DAT
and O
Language O
Processing, O
vol. O
20, O
no. O
1, O
pp. O
30–42, O
2012 O

. B-DAT
[8] O
J. O
Chorowski, O
D. O
Bahdanau O

, B-DAT
K. O
Cho, O
and O
Y. O

Bengio, B-DAT
“End-to- O
end O
continuous O
speech O

recognition B-DAT
using O
attention-based O
recurrent O
NN: O

first B-DAT
results,” O
arXiv O
preprint O
arXiv:1412.1602, O
2014 O

. B-DAT
[9] O
A. O
Graves, O
S. O
Fernández O

, B-DAT
F. O
Gomez, O
and O
J. O

Schmidhuber, B-DAT
“Con- O
nectionist O
temporal O
classification: O

labelling B-DAT
unsegmented O
se- O
quence O
data O

with B-DAT
recurrent O
neural O
networks,” O
in O

Proceedings B-DAT
of O
International O
Conference O
on O

Machine B-DAT
Learning. O
ACM, O
2006, O
pp. O
369 O

–376. B-DAT
[10] O
A. O
Graves, O
“Sequence O
transduction O

with B-DAT
recurrent O
neural O
net- O
works O

,” B-DAT
arXiv O
preprint O
arXiv:1211.3711, O
2012. O
[11] O
A. O
Hannun, O
C. O
Case O

, B-DAT
J. O
Casper, O
B. O
Catanzaro, O

G. B-DAT
Diamos, O
E. O
Elsen, O
R. O

Prenger, B-DAT
S. O
Satheesh, O
S. O
Sengupta, O

A. B-DAT
Coates O
et O
al., O
“Deep O

speech: B-DAT
Scaling O
up O
end-to-end O
speech O

recognition,” B-DAT
arXiv O
preprint O
arXiv:1412.5567, O
2014. O
[12] O
D. O
Bahdanau, O
K. O
Cho O

, B-DAT
and O
Y. O
Bengio, O
“Neural O

machine B-DAT
trans- O
lation O
by O
jointly O

learning B-DAT
to O
align O
and O
translate,” O

arXiv B-DAT
preprint O
arXiv:1409.0473, O
2014. O
[13] O
D. O
Povey, O
V. O
Peddinti O

, B-DAT
D. O
Galvez, O
P. O
Ghahrmani, O

V. B-DAT
Manohar, O
X. O
Na, O
Y. O

Wang, B-DAT
and O
S. O
Khudanpur, O
“Purely O

sequence-trained B-DAT
neu- O
ral O
networks O
for O

ASR B-DAT
based O
on O
lattice-free O
MMI,” O

in B-DAT
Proceedings O
of O
INTERSPEECH, O
2016. O
[14] O
V. O
Peddinti, O
Y. O
Wang O

, B-DAT
D. O
Povey, O
and O
S. O

Khudanpur, B-DAT
“Low O
latency O
acoustic O
modeling O

using B-DAT
temporal O
convolution O
and O
LSTMs,” O

IEEE B-DAT
Signal O
Processing O
Letters, O
2017. O
[15] O
K. O
J. O
Han, O
S O

. B-DAT
Hahm, O
B.-H. O
Kim, O
J. O

Kim, B-DAT
and O
I. O
Lane, O
“Deep O

learning-based B-DAT
telephony O
speech O
recognition O
in O

the B-DAT
wild,” O
in O
Pro- O
ceedings O

of B-DAT
INTERSPEECH, O
2017, O
pp. O
1323–1327. O
[16] O
W. O
Xiong, O
J. O
Droppo O

, B-DAT
X. O
Huang, O
F. O
Seide, O

M. B-DAT
Seltzer, O
A. O
Stolcke, O
D. O

Yu, B-DAT
and O
G. O
Zweig, O
“The O

Microsoft B-DAT
2016 O
conversational O
speech O
recognition O

system,” B-DAT
in O
Proceedings O
of O
IEEE O

International B-DAT
Con- O
ference O
on O
Acoustics, O

Speech B-DAT
and O
Signal O
Processing O
(ICASSP). O

IEEE, B-DAT
2017, O
pp. O
5255–5259. O
[17] O
L. O
Bahl, O
“Maximum O
mutual O

information B-DAT
estimation O
of O
hidden O
Markov O

model B-DAT
parameters O
for O
speech O
recognition O

,” B-DAT
in O
Proceed- O
ings O
of O

IEEE B-DAT
International O
Conference O
on O
Acoustics, O

Speech B-DAT
and O
Signal O
Processing O
(ICASSP). O

IEEE, B-DAT
1986, O
pp. O
701–704. O
[18] O
A. O
Senior, O
G. O
Heigold O

, B-DAT
M. O
Bacchiani, O
and O
H. O

Liao, B-DAT
“GMM-free O
DNN O
acoustic O
model O

training,” B-DAT
in O
Proceedings O
of O
IEEE O

Interna- B-DAT
tional O
Conference O
on O
Acoustics, O

Speech B-DAT
and O
Signal O
Processing O
(ICASSP). O

IEEE, B-DAT
2014, O
pp. O
5602–5606. O
[19] O
C. O
Zhang O
and O
P O

. B-DAT
C. O
Woodland, O
“Standalone O
training O

of B-DAT
context- O
dependent O
deep O
neural O

network B-DAT
acoustic O
models,” O
in O
Proceedings O

of B-DAT
IEEE O
International O
Conference O
on O

Acoustics, B-DAT
Speech O
and O
Sig- O
nal O

Processing B-DAT
(ICASSP). O
IEEE, O
2014, O
pp. O
5597 O

–5601. B-DAT
[20] O
H. O
Hadian, O
H. O
Sameti O

, B-DAT
D. O
Povey, O
and O
S. O

Khudanpur, B-DAT
“Flat- O
start O
single-stage O
discriminatively O

trained B-DAT
HMM-based O
models O
for O
ASR,” O

IEEE B-DAT
Transactions O
on O
Audio, O
Speech, O

and B-DAT
Language O
Pro- O
cessing O
(accepted), O
2018 O

. B-DAT
[21] O
V. O
Valtchev, O
J. O
Odell O

, B-DAT
P. O
C. O
Woodland, O
and O

S. B-DAT
J. O
Young, O
“Lattice- O
based O

discriminative B-DAT
training O
for O
large O
vocabulary O

speech B-DAT
recogni- O
tion,” O
in O
Proceedings O

of B-DAT
IEEE O
International O
Conference O
on O

Acous- B-DAT
tics, O
Speech, O
and O
Signal O

Processing B-DAT
(ICASSP), O
vol. O
2. O
IEEE, O
1996, O
pp. O
605–608 O

. B-DAT
[22] O
P. O
C. O
Woodland O
and O

D. B-DAT
Povey, O
“Large O
scale O
discriminative O

train- B-DAT
ing O
of O
hidden O
Markov O

models B-DAT
for O
speech O
recognition,” O
Computer O

Speech B-DAT
& O
Language, O
vol. O
16 O

, B-DAT
no. O
1, O
pp. O
25–47, O
2002 O

. B-DAT
[23] O
S. O
F. O
Chen, O
B O

. B-DAT
Kingsbury, O
L. O
Mangu, O
D. O

Povey, B-DAT
G. O
Saon, O
H. O
Soltau, O

and B-DAT
G. O
Zweig, O
“Advances O
in O

speech B-DAT
transcription O
at O
IBM O
under O

the B-DAT
DARPA O
EARS O
program,” O
IEEE O

Transactions B-DAT
on O
Audio, O
Speech, O
and O

Language B-DAT
Processing, O
vol. O
14, O
no. O
5, O
pp. O
1596–1608, O
2006 O

. B-DAT
[24] O
A. O
Zeyer, O
E. O
Beck O

, B-DAT
R. O
Schlüter, O
and O
H. O

Ney, B-DAT
“CTC O
in O
the O
con- O

text B-DAT
of O
generalized O
full-sum O
HMM O

training,” B-DAT
in O
Proceedings O
of O
INTERSPEECH, O
2017, O
pp. O
944–948 O

. B-DAT
[25] O
S. O
J. O
Young, O
J O

. B-DAT
J. O
Odell, O
and O
P. O

C. B-DAT
Woodland, O
“Tree-based O
state O
tying O

for B-DAT
high O
accuracy O
acoustic O
modelling,” O

in B-DAT
Proceedings O
of O
The O
Workshop O

on B-DAT
Human O
Language O
Technology. O
Association O

for B-DAT
Computational O
Linguistics, O
1994, O
pp. O
307 O

–312. B-DAT
[26] O
J. O
J. O
Godfrey, O
E O

. B-DAT
C. O
Holliman, O
and O
J. O

McDaniel, B-DAT
“SWITCH- O
BOARD: O
Telephone O
speech O

corpus B-DAT
for O
research O
and O
develop- O

ment,” B-DAT
in O
Proceedings O
of O
IEEE O

International B-DAT
Conference O
on O
Acoustics, O
Speech O

and B-DAT
Signal O
Processing O
(ICASSP), O
vol. O
1 O

. B-DAT
IEEE, O
1992, O
pp. O
517–520. O
[27] O
D. O
B. O
Paul O
and O

J. B-DAT
M. O
Baker, O
“The O
design O

for B-DAT
the O
Wall O
Street O
Journal-based O

CSR B-DAT
corpus,” O
in O
Proceedings O
of O

The B-DAT
Workshop O
on O
Speech O
and O

Natural B-DAT
Language. O
Association O
for O
Computational O

Linguistics, B-DAT
1992, O
pp. O
357–362 O

. B-DAT
[28] O
D. O
Povey, O
A. O
Ghoshal O

, B-DAT
G. O
Boulianne, O
L. O
Burget, O

O. B-DAT
Glembek, O
N. O
Goel, O
M. O

Hannemann, B-DAT
P. O
Motlicek, O
Y. O
Qian, O

P. B-DAT
Schwarz O
et O
al., O
“The O

Kaldi B-DAT
speech O
recognition O
toolkit,” O
in O

Proceedings B-DAT
of O
IEEE O
2011 O
Workshop O

on B-DAT
Automatic O
Speech O
Recognition O
and O

Under- B-DAT
standing, O
no. O
EPFL-CONF-192584. O
IEEE O

Signal B-DAT
Processing O
So- O
ciety, O
2011. O
[29] O
S. O
Hochreiter O
and O
J O

. B-DAT
Schmidhuber, O
“Long O
short-term O
memory,” O

Neural B-DAT
Computation, O
vol. O
9, O
no. O
8, O
pp. O
1735–1780, O
1997 O

. B-DAT
[30] O
A. O
Waibel, O
T. O
Hanazawa O

, B-DAT
G. O
Hinton, O
K. O
Shikano, O

and B-DAT
K. O
J. O
Lang, O
“Phoneme O

recognition B-DAT
using O
time-delay O
neural O
networks,” O

in B-DAT
Readings O
in O
Speech O
Recognition. O

Elsevier, B-DAT
1990, O
pp. O
393–404. O
[31] O
T. O
Ko, O
V. O
Peddinti O

, B-DAT
D. O
Povey, O
and O
S. O

Khudanpur, B-DAT
“Audio O
augmen- O
tation O
for O

speech B-DAT
recognition,” O
in O
Proceedings O
of O

INTERSPEECH, B-DAT
2015, O
pp. O
3586–3589. O
[32] O
G. O
Zweig, O
C. O
Yu O

, B-DAT
J. O
Droppo, O
and O
A. O

Stolcke, B-DAT
“Advances O
in O
all-neural O
speech O

recognition,” B-DAT
in O
Proceedings O
of O
IEEE O

Interna- B-DAT
tional O
Conference O
on O
Acoustics, O

Speech, B-DAT
and O
Signal O
Processing O
(ICASSP). O

IEEE, B-DAT
2017, O
pp. O
4805–4809. O
[33] O
E. O
Battenberg, O
J. O
Chen O

, B-DAT
R. O
Child, O
A. O
Coates, O

Y. B-DAT
Gaur, O
Y. O
Li, O
H. O

Liu, B-DAT
S. O
Satheesh, O
D. O
Seetapun, O

A. B-DAT
Sriram O
et O
al., O
“Exploring O

neural B-DAT
transducers O
for O
end-to-end O
speech O

recognition,” B-DAT
arXiv O
preprint O
arXiv:1707.07413, O
2017. O
[34] O
K. O
Audhkhasi, O
B. O
Ramabhadran O

, B-DAT
G. O
Saon, O
M. O
Picheny, O

and B-DAT
D. O
Na- O
hamoo, O
“Direct O

acoustics-to-word B-DAT
models O
for O
english O
conversa- O

tional B-DAT
speech O
recognition,” O
in O
Proceedings O

of B-DAT
INTERSPEECH, O
2017, O
pp. O
959–963. O
[35] O
J. O
Chorowski O
and O
N O

. B-DAT
Jaitly, O
“Towards O
better O
decoding O

and B-DAT
lan- O
guage O
model O
integration O

in B-DAT
sequence O
to O
sequence O
models,” O

in B-DAT
Pro- O
ceedings O
of O
INTERSPEECH, O
2017 O

. B-DAT
16 O

regular O
LF- O
MMI O
on O
the O
300hr B-DAT
Switchboard O
task O
and O
almost O
the O

EE-LF-MMI) O
and O
CTC O
on O
the O
300hr B-DAT
Switchboard O

show O
the O
results O
on O
the O
300hr B-DAT
Switchboard O
and O
2000hr O
Fisher+Switchboard O
tasks O

significant O
improvements O
on O
the O
smaller O
300hr B-DAT
Switchboard O
and O
80hr O
WSJ O
task O

ment O
in O
WER O
on O
the O
300hr B-DAT
Switchboard O
task, O
and O
1.4 O
percent O

smaller O
databases O
such O
as O
the O
300hr B-DAT
Switchboard O
or O
80hr O
WSJ O
tasks O

Switchboard B-DAT
task O
[11] O
when O
it O
was O

WSJ O
(Wall O
Street O
Journal) O
[27]. O
Switchboard B-DAT
is O
a O
database O
with O
300 O

modeling O
as O
well O
(together O
with O
Switchboard, B-DAT
a O
total O
of O
2000 O
hours O

Switchboard B-DAT
training O
transcriptions O
to O
train O
a O

Switchboard B-DAT
11.7 O
10.7 O
10.7 O
14.5 O
14.2 O

Switchboard B-DAT
WSJ O
Phone O
Char O
Phone O
Char O

helped O
significantly. O
In O
particular, O
for O
Switchboard B-DAT
it O
has O
improved O
the O
WER O

LF- O
MMI O
on O
the O
300hr O
Switchboard B-DAT
task O
and O
almost O
the O
same O

and O
CTC O
on O
the O
300hr O
Switchboard B-DAT

Switchboard B-DAT
task. O
no-SP O
means O
no O
speed O

the O
results O
on O
the O
300hr O
Switchboard B-DAT
and O
2000hr O
Fisher+Switchboard O
tasks O
re O

Switchboard, B-DAT
end-to-end O
LF- O
MMI O
has O
achieved O

on O
eval2000 O
(7.5 O
on O
the O
Switchboard B-DAT
subset). O
For O
comparison, O
the O
WERs O

improvements O
on O
the O
smaller O
300hr O
Switchboard B-DAT
and O
80hr O
WSJ O
task, O
in O

in O
WER O
on O
the O
300hr O
Switchboard B-DAT
task, O
and O
1.4 O
percent O
improvement O

Switchboard B-DAT
task O
is O
approximately O
2.1 O
hours O

time O
factor O
is O
0.9 O
on O
Switchboard B-DAT
and O
0.4 O
on O
WSJ O
(on O

databases O
such O
as O
the O
300hr O
Switchboard B-DAT
or O
80hr O
WSJ O
tasks O
where O

Switchboard B-DAT
database, O
we O
achieved O
a O
WER O

of O
eval2000 O
(8.0 O
on O
the O
Switchboard B-DAT
subset) O
in O
the O
character-based O
case O

of O
eval2000 O
(7.5 O
on O
the O
Switchboard B-DAT
subset) O
in O
the O
phoneme-based O
setting O

regular O
LF- O
MMI O
on O
the O
300hr B-DAT
Switchboard O
task O
and O
almost O
the O

EE-LF-MMI) O
and O
CTC O
on O
the O
300hr B-DAT
Switchboard O

show O
the O
results O
on O
the O
300hr B-DAT
Switchboard O
and O
2000hr O
Fisher+Switchboard O
tasks O

significant O
improvements O
on O
the O
smaller O
300hr B-DAT
Switchboard O
and O
80hr O
WSJ O
task O

ment O
in O
WER O
on O
the O
300hr B-DAT
Switchboard O
task, O
and O
1.4 O
percent O

smaller O
databases O
such O
as O
the O
300hr B-DAT
Switchboard O
or O
80hr O
WSJ O
tasks O

experiments O
was O
performed O
with O
the O
TIMIT B-DAT
corpus, O
considering O
the O
standard O
phoneme O

for O
the O
test O
set O
of O
TIMIT B-DAT
with O
various O
neural O
architectures O

discuss O
the O
baselines O
obtained O
with O
TIMIT, B-DAT
DIRHA, O
CHiME, O
and O
LibriSpeech O
datasets O

the O
experimental O
validation O
conducted O
on O
TIMIT B-DAT

Table O
2: O
PER(%) O
obtained O
on O
TIMIT B-DAT
when O
progressively O
applying O
some O
techniques O

the O
best-published O
performance O
on O
the O
TIMIT B-DAT
test-set O

firming O
our O
previous O
achievements O
on O
TIMIT B-DAT

experiments O
was O
performed O
with O
the O
TIMIT B-DAT
corpus, O
considering O
the O
standard O
phoneme O

for O
the O
test O
set O
of O
TIMIT B-DAT
with O
various O
neural O
architectures O

discuss O
the O
baselines O
obtained O
with O
TIMIT, B-DAT
DIRHA, O
CHiME, O
and O
LibriSpeech O
datasets O

the O
experimental O
validation O
conducted O
on O
TIMIT B-DAT

Table O
2: O
PER(%) O
obtained O
on O
TIMIT B-DAT
when O
progressively O
applying O
some O
techniques O

the O
best-published O
performance O
on O
the O
TIMIT B-DAT
test-set O

firming O
our O
previous O
achievements O
on O
TIMIT B-DAT

experiments O
was O
performed O
with O
the O
TIMIT B-DAT
corpus, O
considering O
the O
standard O
phoneme O

for O
the O
test O
set O
of O
TIMIT B-DAT
with O
various O
neural O
architectures O

discuss O
the O
baselines O
obtained O
with O
TIMIT, B-DAT
DIRHA, O
CHiME, O
and O
LibriSpeech O
datasets O

the O
experimental O
validation O
conducted O
on O
TIMIT B-DAT

Table O
2: O
PER(%) O
obtained O
on O
TIMIT B-DAT
when O
progressively O
applying O
some O
techniques O

the O
best-published O
performance O
on O
the O
TIMIT B-DAT
test-set O

firming O
our O
previous O
achievements O
on O
TIMIT B-DAT

for O
a O
GRU O
trained O
on O
TIMIT B-DAT
in O
a O
chunk O
of O
the O

for O
a O
GRU O
trained O
on O
TIMIT B-DAT

Features O
Dataset. O
TIMIT B-DAT
DIRHA O
CHiME O
TED O

set O
of O
experiments O
with O
the O
TIMIT B-DAT
corpus O
was O
performed O
to O
test O

will O
then O
be O
reported O
for O
TIMIT, B-DAT
DIRHA-English, O
CHiME O
as O
well O
as O

MFCC O
features O
and O
trained O
with O
TIMIT B-DAT

with O
and O
without O
batch O
normalization O
(TIMIT B-DAT
dataset, O
MFCC O
features O

results O
of O
CTC O
on O
the O
TIMIT B-DAT
data O
set. O
In O
these O
experiments O

for O
the O
test O
set O
of O
TIMIT B-DAT
with O
various O
CTC O
RNN O
architectures O

for O
the O
test O
set O
of O
TIMIT B-DAT
with O
various O
RNN O
architectures O

E. O
Other O
results O
on O
TIMIT B-DAT

ASR O
performance O
obtained O
with O
the O
TIMIT B-DAT
dataset. O
The O
first O
row O
reports O

best O
published O
performance O
on O
the O
TIMIT B-DAT
test-set O

TABLE O
VI: O
PER(%) O
of O
the O
TIMIT B-DAT
dataset O
(MFCC O
features) O
split O
into O

layers O
and O
neurons O
for O
each O
TIMIT B-DAT
RNN O
model. O
The O
outcome O
of O

first O
set O
of O
experiments O
on O
TIMIT, B-DAT
in O
this O
sub-section O
we O
assess O

comparable O
to O
that O
observed O
for O
TIMIT, B-DAT
confirming O
that O
Li-GRU O
still O
outperform O

XXXXXXXXArch. O
Dataset. O
TIMIT B-DAT
DIRHA O
CHiME O
TED O

V-E O
Other O
results O
on O
TIMIT B-DAT

experiments O
was O
performed O
with O
the O
TIMIT B-DAT
corpus, O
considering O
the O
standard O
phoneme O

for O
the O
test O
set O
of O
TIMIT B-DAT
with O
various O
neural O
architectures O

discuss O
the O
baselines O
obtained O
with O
TIMIT, B-DAT
DIRHA, O
CHiME, O
and O
LibriSpeech O
datasets O

the O
experimental O
validation O
conducted O
on O
TIMIT B-DAT

Table O
2: O
PER(%) O
obtained O
on O
TIMIT B-DAT
when O
progressively O
applying O
some O
techniques O

the O
best-published O
performance O
on O
the O
TIMIT B-DAT
test-set O

firming O
our O
previous O
achievements O
on O
TIMIT B-DAT

experiments O
was O
performed O
with O
the O
TIMIT B-DAT
corpus, O
considering O
the O
standard O
phoneme O

for O
the O
test O
set O
of O
TIMIT B-DAT
with O
various O
neural O
architectures O

discuss O
the O
baselines O
obtained O
with O
TIMIT, B-DAT
DIRHA, O
CHiME, O
and O
LibriSpeech O
datasets O

the O
experimental O
validation O
conducted O
on O
TIMIT B-DAT

Table O
2: O
PER(%) O
obtained O
on O
TIMIT B-DAT
when O
progressively O
applying O
some O
techniques O

the O
best-published O
performance O
on O
the O
TIMIT B-DAT
test-set O

firming O
our O
previous O
achievements O
on O
TIMIT B-DAT

experiments O
was O
performed O
with O
the O
TIMIT B-DAT
corpus, O
considering O
the O
standard O
phoneme O

for O
the O
test O
set O
of O
TIMIT B-DAT
with O
various O
neural O
architectures O

discuss O
the O
baselines O
obtained O
with O
TIMIT, B-DAT
DIRHA, O
CHiME, O
and O
LibriSpeech O
datasets O

the O
experimental O
validation O
conducted O
on O
TIMIT B-DAT

Table O
2: O
PER(%) O
obtained O
on O
TIMIT B-DAT
when O
progressively O
applying O
some O
techniques O

the O
best-published O
performance O
on O
the O
TIMIT B-DAT
test-set O

firming O
our O
previous O
achievements O
on O
TIMIT B-DAT

experiments O
was O
performed O
with O
the O
TIMIT B-DAT
corpus, O
considering O
the O
standard O
phoneme O

for O
the O
test O
set O
of O
TIMIT B-DAT
with O
various O
neural O
architectures O

discuss O
the O
baselines O
obtained O
with O
TIMIT, B-DAT
DIRHA, O
CHiME, O
and O
LibriSpeech O
datasets O

the O
experimental O
validation O
conducted O
on O
TIMIT B-DAT

Table O
2: O
PER(%) O
obtained O
on O
TIMIT B-DAT
when O
progressively O
applying O
some O
techniques O

the O
best-published O
performance O
on O
the O
TIMIT B-DAT
test-set O

firming O
our O
previous O
achievements O
on O
TIMIT B-DAT

experiments O
was O
performed O
with O
the O
TIMIT B-DAT
corpus, O
considering O
the O
standard O
phoneme O

for O
the O
test O
set O
of O
TIMIT B-DAT
with O
various O
neural O
architectures O

discuss O
the O
baselines O
obtained O
with O
TIMIT, B-DAT
DIRHA, O
CHiME, O
and O
LibriSpeech O
datasets O

the O
experimental O
validation O
conducted O
on O
TIMIT B-DAT

Table O
2: O
PER(%) O
obtained O
on O
TIMIT B-DAT
when O
progressively O
applying O
some O
techniques O

the O
best-published O
performance O
on O
the O
TIMIT B-DAT
test-set O

firming O
our O
previous O
achievements O
on O
TIMIT B-DAT

for O
a O
GRU O
trained O
on O
TIMIT B-DAT
in O
a O
chunk O
of O
the O

for O
a O
GRU O
trained O
on O
TIMIT B-DAT

Features O
Dataset. O
TIMIT B-DAT
DIRHA O
CHiME O
TED O

set O
of O
experiments O
with O
the O
TIMIT B-DAT
corpus O
was O
performed O
to O
test O

will O
then O
be O
reported O
for O
TIMIT, B-DAT
DIRHA-English, O
CHiME O
as O
well O
as O

MFCC O
features O
and O
trained O
with O
TIMIT B-DAT

with O
and O
without O
batch O
normalization O
(TIMIT B-DAT
dataset, O
MFCC O
features O

results O
of O
CTC O
on O
the O
TIMIT B-DAT
data O
set. O
In O
these O
experiments O

for O
the O
test O
set O
of O
TIMIT B-DAT
with O
various O
CTC O
RNN O
architectures O

for O
the O
test O
set O
of O
TIMIT B-DAT
with O
various O
RNN O
architectures O

E. O
Other O
results O
on O
TIMIT B-DAT

ASR O
performance O
obtained O
with O
the O
TIMIT B-DAT
dataset. O
The O
first O
row O
reports O

best O
published O
performance O
on O
the O
TIMIT B-DAT
test-set O

TABLE O
VI: O
PER(%) O
of O
the O
TIMIT B-DAT
dataset O
(MFCC O
features) O
split O
into O

layers O
and O
neurons O
for O
each O
TIMIT B-DAT
RNN O
model. O
The O
outcome O
of O

first O
set O
of O
experiments O
on O
TIMIT, B-DAT
in O
this O
sub-section O
we O
assess O

comparable O
to O
that O
observed O
for O
TIMIT, B-DAT
confirming O
that O
Li-GRU O
still O
outperform O

XXXXXXXXArch. O
Dataset. O
TIMIT B-DAT
DIRHA O
CHiME O
TED O

V-E O
Other O
results O
on O
TIMIT B-DAT

We O
performed O
experiments O
on O
the O
TIMIT B-DAT
dataset. O
We O
achieved O
17.3% O
phone O

experiments O
were O
performed O
on O
the O
TIMIT B-DAT
dataset, O
and O
we O
achieved O
17.3 O

We O
used O
the O
TIMIT B-DAT
dataset O
to O
evaluate O
the O
segmental O

the O
standard O
protocol O
of O
the O
TIMIT B-DAT
dataset, O
and O
our O
experiments O
were O

to O
the O
fact O
that O
the O
TIMIT B-DAT
dataset O
is O
small O
and O
it O

using O
segmental O
CRFs O
on O
the O
TIMIT B-DAT
dataset O
is O
reported O
in O
[28 O

experiments O
were O
performed O
on O
the O
TIMIT B-DAT
dataset, O
and O
we O
achieved O
strong O

error O
rate O
(PER) O
on O
the O
TIMIT B-DAT
phoneme O
recognition O
task, O
it O
can O

task O
using O
the O
widely- O
used O
TIMIT B-DAT
dataset. O
At O
each O
time O
step O

the O
conventional O
approaches O
on O
the O
TIMIT B-DAT
dataset. O
Moreover, O
we O
propose O
a O

experiments O
were O
performed O
on O
the O
TIMIT B-DAT
corpus O
[19]. O
We O
used O
the O

split O
from O
the O
Kaldi O
[20] O
TIMIT B-DAT
s5 O
recipe. O
We O
trained O
on O

ground O
truth O
phone O
location O
from O
TIMIT B-DAT

As O
TIMIT B-DAT
is O
a O
relatively O
small O
dataset O

and O
N. O
L. O
Dahlgren. O
DARPA O
TIMIT B-DAT
acoustic O
phonetic O
continuous O
speech O
corpus O

ground O
truth O
phone O
location O
from O
TIMIT B-DAT

ground O
truth O
phone O
location O
from O
TIMIT B-DAT

ARSG. O
Results O
of O
force-aligning O
concatenated O
TIMIT B-DAT
utterances. O
Each O
dot O
represents O
a O

error O
of O
17.7% O
on O
the O
TIMIT B-DAT
phoneme O
recognition O
benchmark, O
which O
to O

experiments O
were O
performed O
on O
the O
TIMIT B-DAT
corpus O
[25]. O
The O
standard O
462 O

Table O
1. O
TIMIT B-DAT
Phoneme O
Recognition O
Results. O
‘Epochs’ O
is O

the O
filterbank O
inputs O
(bottom). O
The O
TIMIT B-DAT
ground O
truth O
segmentation O
is O
shown O

phoneme O
recog- O
nition O
on O
the O
TIMIT B-DAT
database. O
An O
obvious O
next O
step O

25] O
DARPA-ISTO, O
The O
DARPA O
TIMIT B-DAT
Acoustic-Phonetic O
Continuous O
Speech O
Corpus O
(TIMIT O

and O
D. O
S. O
Pallett, O
“Darpa O
timit B-DAT
acoustic-phonetic O
continous O
speech O
corpus O
cd-rom O

phoneme O
recognition O
experiments O
with O
the O
TIMIT B-DAT
corpus. O
More O
precisely, O
QCNNs O
obtain O

equivalent O
real-valued O
model O
on O
the O
TIMIT B-DAT
[10] O
phonemes O
recognition O
task O
(Section O

The O
conducted O
experiments O
on O
the O
TIMIT B-DAT
dataset O
yielded O
a O
phoneme O
error O

4.1. O
TIMIT B-DAT
dataset O
and O
acoustic O
features O
of O

The O
TIMIT B-DAT
[10] O
dataset O
is O
composed O
of O

phoneme O
recognition O
task O
of O
the O
TIMIT B-DAT
dataset O
are O
reported O
in O
Table O

CNN O
based O
models O
on O
the O
TIMIT B-DAT
phoneme O
recognition O
task. O
The O
results O

for O
recognizing O
phonemes O
in O
the O
TIMIT B-DAT
corpus O
using O
RBM O
for O
feature O

our O
approach O
on O
two O
datasets: O
TIMIT B-DAT
(Garofolo O
et O
al., O
1993) O
and O

Speech O
recognition O
on O
the O
TIMIT B-DAT
dataset O
involves O
tran- O
scribing O
the O

to O
the O
60 O
phonemes O
from O
TIMIT B-DAT
plus O
special O
start-of-sequence O
and O
end-of-sequence O

Phone O
error O
rate O
on O
the O
TIMIT B-DAT
dataset O
for O
different O
online O
methods O

small O
size O
of O
the O
TIMIT B-DAT
dataset O
and O
the O
resulting O
variabil O

of O
the O
dataset, O
performance O
on O
TIMIT B-DAT
is O
often O
highly O
dependent O
on O

and O
Pallett, O
David O
S. O
DARPA O
TIMIT B-DAT
acoustic-phonetic O
continous O
speech O
corpus. O
1993 O

D.1.1. O
TIMIT B-DAT

small O
small O
training O
set, O
like O
TIMIT B-DAT
[2]. O
Also, O
we O
introduce O
a O

baseline. O
We O
decided O
to O
use O
TIMIT B-DAT
in O
our O
exper- O
iments O
as O

experiments O
were O
performed O
on O
the O
TIMIT B-DAT
corpus. O
For O
training O
we O
used O

different O
fea- O
ture O
sets O
on O
TIMIT B-DAT
corpus O

alignments O
for O
phones O
provided O
with O
TIMIT, B-DAT
thus O
phone O
error O
rates O
(PER O

TIMIT B-DAT
is O
a O
small O
dataset O
with O

Table O
2: O
Phones O
recognition O
on O
TIMIT B-DAT
corpus. O
LAS O
– O
a O
model O

sequence-level O
articulatory O
features O
detection O
on O
TIMIT B-DAT

tors O
to O
waveform O
segments O
from O
TIMIT B-DAT
markup O
using O
attention O
peaks O
(the O

It O
is O
worth O
noting O
that O
TIMIT B-DAT
has O
explicit O
phone O
sequences O
for O

TIMIT B-DAT
Acoustic-phonetic O
Continuous O
Speech O
Corpus,” O
in O

RWTH O
ASR O
Systems O
for O
LibriSpeech B-DAT

LibriSpeech B-DAT
task. O
Detailed O
descriptions O
of O
the O

when O
training O
on O
the O
full O
LibriSpeech B-DAT
training O
set, O
are O
the O
best O

comparison O
shows O
that O
on O
the O
LibriSpeech B-DAT
960h O

a O
reduced O
100h-subset O
of O
the O
LibriSpeech B-DAT
training O

tention, O
LibriSpeech B-DAT

the O
LibriSpeech B-DAT
task O

The O
LibriSpeech B-DAT
task O
comprises O
English O
read O
speech O

End-to-end O
results O
on O
LibriSpeech B-DAT
were O
presented O
in O
[5–9 O

obtained O
on O
the O
LibriSpeech B-DAT
task O
reflect O
state-of-the-art O
per O

model O
officially O
distributed O
with O
the O
LibriSpeech B-DAT
dataset O
[2 O

perform O
the O
best O
LibriSpeech B-DAT
attention O
system O
presented O
in O
[6 O

and O
hybrid O
DNN/HMM O
results O
on O
LibriSpeech B-DAT
with O
12k O
CART O
labels O
and O

the O
LibriSpeech B-DAT
corpus. O
For O
comparison, O
also O
a O

with O
the O
LibriSpeech B-DAT
corpus: O
dev-clean, O
dev-other, O
test-clean O

2: O
Hybrid O
DNN/HMM O
results O
on O
LibriSpeech B-DAT
with O
differ O

decoder-attention O
model O
results O
on O
LibriSpeech B-DAT
with O
different O

results O
from O
other O
papers O
on O
LibriSpeech B-DAT
960 O
h. O
CDp O
are O

hybrid O
and O
attention-based O
models O
on O
LibriSpeech, B-DAT
to O
the O
best O

two O
ASR O
systems O
for O
the O
LibriSpeech B-DAT

state-of-the-art O
performance O
on O
the O
LibriSpeech B-DAT
960h O
task O
in O

comparison O
shows O
that O
on O
the O
LibriSpeech B-DAT
960h O

a O
reduced O
100h-subset O
of O
the O
LibriSpeech B-DAT
training O

on O
the O
full O
LibriSpeech B-DAT
training O
set, O
are O
the O
best O

them O
to O
the O
test-clean O
and O
test-other B-DAT
sets. O
We O

and O
test-other B-DAT

on O
test-other B-DAT
(Table O
4). O
Evaluating O
our O
sequence O

on O
test-clean O
and O
5.5% O
on O
test-other B-DAT

on O
test-clean O
and O
5.0% O
on O
test-other B-DAT

by O
3.9% O
relative O
WER O
on O
test-other B-DAT

by O
27.6% O
relative O
WER O
on O
test-other B-DAT

on O
test-other B-DAT

on O
test-other B-DAT

by O
13.8% O
relative O
WER O
on O
test-other B-DAT

the O
other B-DAT
test O
sets O
in O
terms O
of O

clean O
other B-DAT
clean O
other O

other B-DAT
sets, O
then O

other B-DAT
sets. O
We O

respectively O
on O
the O
clean O
and O
other B-DAT
sets O

other, B-DAT
test-clean O

other B-DAT

The O
difference O
between O
clean O
and O
other B-DAT
is O
the O

quality O
is O
higher O
than O
the O
other B-DAT

other B-DAT

improves, O
other B-DAT
degrades. O
Introducing O
an O
hybrid O
DNN/HMM O

clean O
other B-DAT
clean O
other O

clean O
other B-DAT
clean O
other O

models O
and O
important O
results O
from O
other B-DAT
papers O
on O
LibriSpeech O
960 O
h O

clean O
other B-DAT
clean O
other O

other B-DAT
(Table O
4). O
Evaluating O
our O
sequence O

other B-DAT

other B-DAT

other B-DAT

other B-DAT

other B-DAT

other B-DAT

other B-DAT

GMM/HMM O
system, O
the O
other B-DAT
system O
was O
an O
attention-based O

to O
other B-DAT
end-to-end O
approaches O

on O
the O
other B-DAT
test O
sets. O
Our O
hybrid O
system O

Table O
2: O
LibriSpeech B-DAT
test I-DAT
WER O
(%) O
evaluated O
for O
varying O

achieve O
state-of-the-art O
performance O
on O
the O
LibriSpeech B-DAT
960h O
and O
Swichboard O
300h O
tasks O

outperforming O
all O
prior O
work. O
On O
LibriSpeech, B-DAT
we O
achieve O
6.8% O
WER O
on O

of O
Language O
Models O
(LMs). O
On O
LibriSpeech B-DAT
[20], O
we O
achieve O
2.8% O
Word O

an O
LM O
trained O
on O
the O
LibriSpeech B-DAT
LM O
corpus, O
we O
are O
able O

a O
series O
of O
hand-crafted O
policies, O
LibriSpeech B-DAT
basic O
(LB), O
LibriSpeech O
double O
(LD O

of O
vocabulary O
size O
16k O
for O
LibriSpeech B-DAT
and O
1k O
for O
Switchboard. O
The O

WPM O
for O
LibriSpeech B-DAT
960h O
is O
con- O
structed O
using O

si O
= O
140k O
for O
LibriSpeech B-DAT
960h, O
and O
is O
subsequently O
turned O

For O
LibriSpeech, B-DAT
we O
use O
a O
two-layer O
RNN O

which O
is O
trained O
on O
the O
LibriSpeech B-DAT
LM O
corpus. O
We O
use O
identical O

we O
describe O
our O
experiments O
on O
LibriSpeech B-DAT
and O
Switchboard O
with O
SpecAugment. O
We O

4.1. O
LibriSpeech B-DAT
960h O

For O
LibriSpeech, B-DAT
we O
use O
the O
same O
setup O

LAS-6- O
1280 O
are O
trained O
on O
LibriSpeech B-DAT
960h O
with O
a O
combination O
of O

Table O
2: O
LibriSpeech B-DAT
test O
WER O
(%) O
evaluated O
for O

Table O
3: O
LibriSpeech B-DAT
960h O
WERs O

the O
RT- O
03 O
corpus. O
Unlike O
LibriSpeech, B-DAT
the O
fusion O
parameters O
do O
not O

rate O
is O
being O
decayed O
for O
LibriSpeech B-DAT
when O
label O
smoothing O
is O
applied O

the O
learning O
rate O
schedule O
for O
LibriSpeech B-DAT

Figure O
3: O
LAS-6-1280 O
on O
LibriSpeech B-DAT
with O
schedule O
D O

LibriSpeech B-DAT
960h O

we O
achieve O
6.8% O
WER O
on O
test-other B-DAT
without O
the O
use O
of O
a O

and O
6.8% O
WER O
on O
the O
test-other B-DAT
set, O
without O
the O
use O
of O

state O
of O
the O
art O
on O
test-other B-DAT
by O
22% O
relatively. O
On O
Switchboard O

F O
mF O
T O
p O
mT O
test-other B-DAT
test O

other B-DAT
without O
the O
use O
of O
a O

in O
the O
time O
direction. O
The O
other B-DAT
two O
augmentations, O
inspired O
by O
“Cutout O

other B-DAT
set, O
without O
the O
use O
of O

and O
5.8% O
WER O
on O
test- O
other), B-DAT
improving O
the O
current O
state O
of O

other B-DAT
by O
22% O
relatively. O
On O
Switchboard O

while O
the O
confidence O
of O
the O
other B-DAT
labels O
are O
increased O
accordingly. O
As O

learning O
rate O
sched- O
ules, O
all O
other B-DAT
hyperparameters O
were O
fixed, O
and O
no O

other B-DAT
set O
in O
Table O
2. O
We O

other B-DAT
performance. O
State O
of O
the O
art O

clean O
other B-DAT
clean O
other O

clean O
other B-DAT
clean O
other O

5 O
summarizes O
our O
results O
with O
other B-DAT
work. O
We O
also O
apply O
shallow O

other B-DAT
test O

corpora O
show O
that O
Kaldi-RNNLM O
rivals O
other B-DAT
recurrent O
neural O
net- O
work O
language O

work O
has O
moved O
on O
to O
other B-DAT
topologies, O
such O
as O
LSTMs O
(e.g O

Kaldi-RNNLM O
achieves O
better O
perplexities O
than O
other B-DAT
toolk- O
its, O
and O
we O
hypothesize O

into O
how O
to O
in- O
corporate O
other B-DAT
types O
of O
subword O
information O
for O

with O
a O
greedy O
decoder O
on O
LibriSpeech B-DAT
test I-DAT

results O
among O
end-to-end O
models1 O
on O
LibriSpeech B-DAT
test I-DAT

we O
achieve O
3.86% O
WER O
on O
LibriSpeech B-DAT
test I-DAT

improve O
the O
SOTA O
WER O
on O
LibriSpeech B-DAT
test I-DAT

we O
report O
state-of-the-art O
results O
on O
LibriSpeech B-DAT
among O
end-to-end O
speech O
recognition O
models O

with O
a O
greedy O
decoder O
on O
LibriSpeech B-DAT
test-clean. O
We O
also O
report O
competitive O

end-to- O
end O
models O
on O
the O
LibriSpeech B-DAT
and O
2000hr O
Fisher+Switchboard O
tasks. O
Like O

new O
state-of-the-art O
(SOTA) O
results O
on O
LibriSpeech B-DAT
[13] O
test-clean O
of O
2.95% O
WER O

results O
among O
end-to-end O
models1 O
on O
LibriSpeech B-DAT
test-other. O
We O
show O
competitive O
results O

we O
achieve O
3.86% O
WER O
on O
LibriSpeech B-DAT
test-clean O

improve O
the O
SOTA O
WER O
on O
LibriSpeech B-DAT
test-clean O

for O
WSJ O
and O
64 O
for O
LibriSpeech B-DAT
and O
F+S O

3: O
Sequence O
Masking: O
Greedy O
WER, O
LibriSpeech B-DAT
for O
Jasper O
10x4 O
after O
50 O

better O
on O
specific O
subsets O
of O
LibriSpeech B-DAT

4: O
Residual O
Connections: O
Greedy O
WER, O
LibriSpeech B-DAT
for O
Jasper O
10x3 O
after O
400 O

3: O
LM O
perplexity O
vs O
WER. O
LibriSpeech B-DAT
dev-other. O
Vary- O
ing O
perplexity O
is O

Table O
5: O
LibriSpeech, B-DAT
WER O

creased O
the O
WER O
on O
dev-clean O
LibriSpeech B-DAT
from O
4.00% O
to O
3.64%, O
a O

on O
two O
read O
speech O
datasets: O
LibriSpeech B-DAT
and O
Wall O
Street O
Journal O
(WSJ O

leads O
to O
SOTA O
results O
on O
LibriSpeech B-DAT
and O
competitive O
results O
on O
other O

results O
among O
end-to-end O
models1 O
on O
LibriSpeech B-DAT
test-other I-DAT

among O
end-to-end O
models1 O
on O
LibriSpeech O
test-other B-DAT

E2E O
LM O
dev-clean O
dev-other O
test-clean O
test-other B-DAT

speech O
recogni- O
tion O
models O
on O
test-other B-DAT

using O
time-delay O
neural O
networks O
(TDNN), O
other B-DAT
forms O
of O
convolu- O
tional O
neural O

ReLU O
and O
batch O
normalization O
outperform O
other B-DAT
activation O
and O
normalization O
schemes O
that O

other B-DAT

ReLU O
and O
batch O
norm O
outperform O
other B-DAT
combi- O
nations O
for O
regularization O
and O

batch O
norm O
with O
ReLU O
outperformed O
other B-DAT
choices. O
Thus, O
leading O
us O
to O

other B-DAT

other B-DAT
test-clean O
test-other O

other B-DAT

50 O
epochs. O
We O
compare O
to O
other B-DAT
models O
trained O
using O
the O
same O

LibriSpeech O
and O
competitive O
results O
on O
other B-DAT
bench- O
marks. O
Our O
Jasper O
architecture O

tion O
with O
bidirectional O
lstm O
and O
other B-DAT
neural O
network O
architec- O
tures,” O
Neural O

and O
dev-other O
when O
testing O
on O
test-other B-DAT

best O
end-to-end O
sys- O
tem O
on O
test-other B-DAT
with O
an O
improvement O
of O
2.3 O

parallel O
the O
prior O
results O
on O
other B-DAT
application O
domains O
that O
convolutional O
archi O

The O
other B-DAT
hyper-parameters O
α, O
β, O
γ O

concatenation O
of O
train-clean O
and O
train- O
other B-DAT

other B-DAT
when O
testing O
on O
test-other O

other B-DAT

other B-DAT
with O
an O
improvement O
of O
2.3 O

other B-DAT

other B-DAT

other B-DAT
also O
decreases O
following O
the O
same O

quency O
does O
not O
inform O
on O
other B-DAT
important O
characteristics O
such O
as O
its O

WSJ O
eval’93 O
6.94 O
4.98 O
8.08 O
LibriSpeech B-DAT
test I-DAT

-clean O
7.89 O
5.33 O
5.83 O
LibriSpeech B-DAT
test I-DAT

conversational O
300 O
Fisher O
conversational O
2000 O
LibriSpeech B-DAT
read O
960 O
Baidu O
read O
5000 O

the O
Linguistic O
Data O
Consortium. O
The O
LibriSpeech B-DAT
dataset O
[46] O
is O
available O
free O

advantage O
of O
the O
recently O
developed O
LibriSpeech B-DAT
corpus O
constructed O
using O
audio O
books O

WSJ O
eval’93 O
6.94 O
4.98 O
8.08 O
LibriSpeech B-DAT
test-clean O
7.89 O
5.33 O
5.83 O
LibriSpeech O

LibriSpeech O
test-clean O
7.89 O
5.33 O
5.83 O
LibriSpeech B-DAT
test-other I-DAT
21.74 O
13.25 O
12.69 O

test-clean O
7.89 O
5.33 O
5.83 O
LibriSpeech O
test-other B-DAT
21.74 O
13.25 O
12.69 O

also O
yielded O
great O
advances O
in O
other B-DAT
appli- O
cation O
areas O
such O
as O

The O
other B-DAT
commonly O
used O
technique O
for O
mapping O

Units O
(GRU) O
[11], O
though O
many O
other B-DAT
variations O
exist. O
A O
recent O
comprehensive O

variants O
are O
competitive O
with O
each O
other B-DAT
[32]. O
We O
decided O
to O
examine O

input O
features O
prior O
to O
any O
other B-DAT
processing, O
can O
slightly O
improve O
ASR O

for O
porting O
speech O
systems O
to O
other B-DAT
languages O
[59]. O
Direct O
output O
to O

is O
available O
free O
on-line. O
The O
other B-DAT
datasets O
are O
internal O
Baidu O
corpora O

while O
keeping O
the O
depth O
and O
other B-DAT
architectural O
parameters O
constant. O
We O
evaluate O

other B-DAT
21.74 O
13.25 O
12.69 O

transcription. O
We O
can O
solve O
the O
other B-DAT
problems O
by O
modifying O
our O
network O

a O
corresponding−∞ O
value O
from O
the O
other B-DAT
matrix, O
which O
results O
in−∞, O
effectively O

large O
vocabulary O
setup, O
on O
the O
LibriSpeech B-DAT
evaluation O
dataset O
[38], O
chosen O
because O

are O
trained O
only O
on O
the O
LibriSpeech B-DAT
training O
set O
(or O
on O
a O

architecture O
trained O
and O
evaluated O
on O
LibriSpeech B-DAT

on O
different O
subsets O
of O
the O
LibriSpeech B-DAT
dataset, O
with O
and O
without O
data O

the O
train-clean O
split O
of O
the O
LibriSpeech B-DAT
dataset O
and O
500h O
in O
the O

not O
have O
much O
impact O
on O
LibriSpeech B-DAT

architecture O
trained O
and O
evaluated O
on O
LibriSpeech B-DAT

Num. O
hours O
dev-clean O
dev-other O
test-clean O
test-other B-DAT
460 O
6.3 O
21.8 O
6.6 O
23.1 O

marked O
as O
other O
(dev- O
other, O
test-other B-DAT

Model O
dev-clean O
dev-other O
test-clean O
test-other B-DAT
nnet-256 O
7.3 O
19.2 O
7.6 O
19.6 O

open O
source O
[50], O
while O
the O
other B-DAT
components O
will O
be O
opensourced O
in O

other B-DAT
split). O
The O
data O
augmentation O
was O

other B-DAT
test-clean O
test-other O
460 O
6.3 O
21.8 O

on O
the O
datasets O
marked O
as O
other B-DAT
(dev- O
other, O
test-other). O
In O
general O

other B-DAT
test-clean O
test-other O
nnet-256 O
7.3 O
19.2 O

These O
tradeoffs O
and O
comparison O
with O
other B-DAT
trained O
models O
led O
us O
to O

that O
it O
can O
share O
with O
other B-DAT
intents. O
For O
instance, O
the O
room O

may O
be O
replaced O
by O
any O
other B-DAT

hand, O
and O
G O
on O
the O
other B-DAT

On O
the O
other B-DAT
hand, O
crowdsourcing O
– O
widely O
used O

contributors O
can O
be O
reached O
easily, O
other B-DAT
languages O
such O
as O
French, O
German O

recognition: O
word O
error O
minimization O
and O
other B-DAT
applications O
of O
confusion O
networks. O
Computer O

eval92 B-DAT

corpora: O
Switch- O
board O
[26], O
and O
WSJ B-DAT
(Wall O
Street O
Journal) O
[27]. O
Switchboard O

character O
LM O
(in O
lexicon-free O
settings). O
WSJ B-DAT
is O
a O
database O
with O
80 O

3-gram O
LM O
trained O
on O
the O
WSJ B-DAT
train- O
ing O
set O
transcriptions O
using O

1For O
example, O
on O
WSJ, B-DAT
which O
has O
42 O
phonemes O
(including O

10.7 O
14.5 O
14.2 O
13.3 O
13.2 O
WSJ B-DAT
3.1 O
3.1 O
3.3 O
5.4 O
5.3 O

Switchboard O
WSJ B-DAT
Phone O
Char O
Phone O
Char O

all O
cases O
except O
in O
phoneme-based O
WSJ B-DAT
which O
is O
fairly O
easier O
than O

and O
almost O
the O
same O
on O
WSJ B-DAT

. O
For O
WSJ, B-DAT
there O
is O
no O
improvement O
in O

5 O
shows O
the O
results O
on O
WSJ B-DAT

EE-LF-MMI) O
and O
related O
methods O
on O
WSJ B-DAT

smaller O
300hr O
Switchboard O
and O
80hr O
WSJ B-DAT
task, O
in O
both O
lexicon-free O
and O

and O
1.4 O
percent O
improvement O
on O
WSJ B-DAT
in O
similar O
conditions O
(i.e. O
no-SP O

on O
Switchboard O
and O
0.4 O
on O
WSJ B-DAT
(on O
CPU O

the O
300hr O
Switchboard O
or O
80hr O
WSJ B-DAT
tasks O
where O
the O
relative O
improvements O

the O
Wall O
Street O
Journal O
dataset O
(WSJ) B-DAT
and O
on O
the O
1000h O
Librispeech O

the O
best O
end-to-end O
systems; O
on O
WSJ, B-DAT
our O
results O
are O
competitive O
with O

show O
additional O
improvements O
on O
both O
WSJ B-DAT
and O
Librispeech O
by O
varying O
the O

of O
the O
Wall O
Street O
Journal O
(WSJ) B-DAT
dataset O
[24], O
which O
contains O
80 O

contain O
37 O
million O
tokens O
for O
WSJ, B-DAT
800 O
million O
tokens O
for O
Librispeech O

the O
open O
vocabulary O
task O
of O
WSJ B-DAT

Training/test O
splits O
On O
WSJ, B-DAT
models O
are O
trained O
on O
si284 O

front-end O
for O
our O
approach). O
On O
WSJ, B-DAT
we O
use O
the O
lighter O
version O

and O
linear O
layer O
on O
both O
WSJ B-DAT
and O
Librispeech. O
The O
kernel O
size O

the O
words O
(162K) O
in O
the O
WSJ B-DAT
training O
corpus, O
while O
only O
the O

Word O
Error O
Rates O
(WER) O
on O
WSJ B-DAT
for O
the O
cur- O
rent O
state-of-the-art O

end-to-end O
systems O
trained O
only O
on O
WSJ, B-DAT
and O
hence O
the O
most O

consistent O
with O
our O
results O
on O
WSJ B-DAT

of O
our O
best O
models O
on O
WSJ B-DAT
and O
Librispeech. O
The O
fig- O
ure O

SCALE O
MEL O
SCALE O
LEARNT O
SCALE O
(WSJ B-DAT

SCALE O
(LIBRI/40 O
FILTERS) O
LEARNT O
SCALE O
(WSJ B-DAT

knowledge, O
this O
is O
the O
best O
WSJ B-DAT
eval92 I-DAT
per- O
formance O
without O
sequence O
training O

acoustic O
model O
architecture. O
On O
the O
WSJ B-DAT
eval92 I-DAT
task, O
we O
report O
a O
3.47 O

knowledge, O
this O
is O
the O
best O
WSJ B-DAT
eval92 I-DAT
per- O
formance O
without O
sequence O
training O

acoustic O
model O
architecture. O
On O
the O
WSJ B-DAT
eval92 I-DAT
task, O
we O
report O
a O
3.47 O

the O
Wall O
Street O
Journal O
(WSJ) O
eval92 B-DAT
task O
or O
more O
than O
8 O

as O
our O
development O
set O
and O
eval92 B-DAT
as O
our O
test O
set. O
We O

verged O
dev93 O
and O
the O
corresponding O
eval92 B-DAT
WERs. O
We O
use O
the O
same O

Model O
dev93 O
WER O
eval92 B-DAT
WER O
GMM O
Kaldi O
tri4b O
9.39 O

Cell O
Size O
Layers O
dev93 O
WER O
eval92 B-DAT
WER O
128 O
1 O
8.19 O
5.19 O

Model O
dev93 O
WER O
eval92 B-DAT
WER O
DNN-BLSTM O
7.40 O
3.92 O
BLSTM-DNN O

this O
is O
the O
best O
WSJ O
eval92 B-DAT
per- O
formance O
without O
sequence O
training O

Epochs O
Time O
(hrs) O
dev93 O
WER O
eval92 B-DAT
WER O
SGD O
17 O
51.5 O
6.58 O

SGD O
dev93 O
SGD O
eval92 B-DAT
3x O
ASGD O
dev93 O
3x O
ASGD O

eval92 B-DAT

model O
architecture. O
On O
the O
WSJ O
eval92 B-DAT
task, O
we O
report O
a O
3.47 O

on O
the O
Wall O
Street O
Journal O
(WSJ) B-DAT
eval92 O
task O
or O
more O
than O

for O
the O
Wall O
Street O
Journal O
(WSJ) B-DAT
corpus O

Results O
We O
experiment O
with O
the O
WSJ B-DAT
dataset. O
We O
use O
si284 O
with O

first O
is O
the O
Kaldi O
s5 O
WSJ B-DAT
recipe O
with O
sigmoid O
DNN O
model O

knowledge, O
this O
is O
the O
best O
WSJ B-DAT
eval92 O
per- O
formance O
without O
sequence O

acoustic O
model O
architecture. O
On O
the O
WSJ B-DAT
eval92 O
task, O
we O
report O
a O

WSJ B-DAT
eval’92 I-DAT
4.94 O
3.60 O
5.03 O
WSJ O
eval O

WSJ B-DAT
read O
80 O
Switchboard O
conversational O
300 O

English. O
The O
Wall O
Street O
Journal O
(WSJ), B-DAT
Switchboard O
and O
Fisher O
[13] O
corpora O

from O
the O
Wall O
Street O
Journal O
(WSJ) B-DAT
corpus O
of O
read O
news O
articles O

WSJ B-DAT
eval’92 O
4.94 O
3.60 O
5.03 O
WSJ O
eval’93 O
6.94 O
4.98 O
8.08 O
LibriSpeech O

has O
1320 O
utterances O
from O
the O
WSJ B-DAT
test O
set O
read O
in O
various O

10, B-DAT
contain O
respectively O
15k/1.3k O
and O
123k/37 O

10 B-DAT
123k O
37 O
1M O
5k O
5k O

experiments O
on O
a O
Quadro O
GP O
100 B-DAT
GPU. O
The O
code O
is O
available O

10, B-DAT
which O
is O
larger O
in O
scale O

10 B-DAT

10 B-DAT
MRR O
H@10 O
MRR O
H@10 O
MRR O

10 B-DAT
MRR O
H@10 O
Pa O

consisted O
of O
two O
learning O
rates: O
10 B-DAT

−1 O
and O
10 B-DAT

−2, O
two O
batch-sizes: O
25 O
and O
100, B-DAT
and O
regularization O
co- O
efficients O
in O

0, O
10−3, B-DAT
5.10−3, O
10−2, O
5.10−2, O
10 O

10 B-DAT

10, B-DAT
we O
limited O
our O
models O
to O

rank O
1000 B-DAT
and O
used O
batch-sizes O
500 O
and O

10 B-DAT
(Bordes O
et O
al. O
(2013)). O
We O

epoch O
for O
ComplEx O
with O
batch-size O
100 B-DAT
on O
FB15K O
took O
about O
110s O

of O
25. O
We O
trained O
for O
100 B-DAT
epochs O
to O
ensure O
convergence, O
reported O

10 B-DAT
benchmarks, O
the O
latter O
because O
of O

10 B-DAT
may O
be O
slightly O
underestimated, O
since O

10 B-DAT

10 B-DAT

0 O
20 O
40 O
60 O
80 O
100 B-DAT
Epochs O

mini-batch O
size O
= O
100 B-DAT
mini-batch O
size O
= O
25 O

0 O
20 O
40 O
60 O
80 O
100 B-DAT
Epochs O

mini-batch O
size O
= O
100 B-DAT
mini-batch O
size O
= O
25 O

difference O
is O
large O
even O
after O
100 B-DAT
epochs O
and O
the O
effect O
is O

10), B-DAT
pp. O
471–478, O
2010 O

10881, B-DAT
2017 O

Graphs. O
Proceedings O
of O
the O
IEEE, O
104 B-DAT

1010 B-DAT

1031 B-DAT

1068, B-DAT
2016 O

10 B-DAT
0.97 O
Ma O
et O
al. O
(2017 O

10 B-DAT
0.51 O
Dettmers O
et O
al. O
(2017 O

10 B-DAT
0.93 O
Shen O
et O
al. O
(2016 O

10 B-DAT
0.49 O
Dettmers O
et O
al. O
(2017 O

10 B-DAT
MRR O
0.52 O
Dettmers O
et O
al O

10 B-DAT
0.66 O
Dettmers O
et O
al. O
(2017 O

we O
experiment O
on, O
FB15K O
and O
YAGO3 B-DAT

15k O
237 O
272k O
18k O
20k O
YAGO3 B-DAT

2017) O
also O
introduced O
the O
dataset O
YAGO3 B-DAT

Model O
WN18 O
WN18RR O
FB15K O
FB15K-237 O
YAGO3 B-DAT

10−2, O
5.10−2, O
10−1, O
5.10−1]. O
On O
YAGO3 B-DAT

performances O
on O
the O
WN18RR O
and O
YAGO3 B-DAT

DistMult O
on O
FB15K-237, O
WN18RR O
and O
YAGO3 B-DAT

improvements O
observed O
on O
FB15K O
and O
YAGO3 B-DAT

harder O
datasets O
WN18RR, O
FB15K-237 O
and O
YAGO3 B-DAT

YAGO3 B-DAT

we O
experiment O
on, O
FB15K O
and O
YAGO3-10, B-DAT
contain O
respectively O
15k/1.3k O
and O
123k/37 O

15k O
237 O
272k O
18k O
20k O
YAGO3-10 B-DAT
123k O
37 O
1M O
5k O
5k O

2017) O
also O
introduced O
the O
dataset O
YAGO3-10, B-DAT
which O
is O
larger O
in O
scale O

Model O
WN18 O
WN18RR O
FB15K O
FB15K-237 O
YAGO3-10 B-DAT

10−2, O
5.10−2, O
10−1, O
5.10−1]. O
On O
YAGO3-10, B-DAT
we O
limited O
our O
models O
to O

performances O
on O
the O
WN18RR O
and O
YAGO3-10 B-DAT
benchmarks, O
the O
latter O
because O
of O

DistMult O
on O
FB15K-237, O
WN18RR O
and O
YAGO3-10 B-DAT
may O
be O
slightly O
underestimated, O
since O

improvements O
observed O
on O
FB15K O
and O
YAGO3-10 B-DAT

harder O
datasets O
WN18RR, O
FB15K-237 O
and O
YAGO3-10 B-DAT

YAGO3-10 B-DAT
MRR O
0.52 O
Dettmers O
et O
al O

10 B-DAT
(Dettmers O
et O
al., O
2018) O
is O

each O
entity O
contains O
at O
least O
10 B-DAT
relations O

10 B-DAT
123k O
37 O
1M O
5k O
5k O

popular O
metrics O
filtered O
HITS@1, O
3, O
10 B-DAT
and O
mean O
reciprocal O
rank O
(MRR O

MRR O
1 O
3 O
10 B-DAT
1 O
3 O
10 O

regularization O
coefficient O
λ O
∈ O
[10−5, B-DAT
10 O

−4, O
10 B-DAT

with O
batch O
sizes O
∈ O
[512, O
1024, B-DAT
2048] O
and O
negative O
sample O
ratio O

1, O
6, O
10 B-DAT

10, B-DAT
2L O
= O
200 O
for O
WN18 O

10 B-DAT

1000 B-DAT

1000 B-DAT

10 B-DAT
HITS@N O

MRR O
1 O
3 O
10 B-DAT
1 O
3 O
10 O
1 O
3 O

10 B-DAT

100 B-DAT

100 B-DAT

100 B-DAT

100 B-DAT

100 B-DAT

100 B-DAT

100 B-DAT

1067 B-DAT
https://www.aclweb.org/anthology/D14-1067 O
https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf O
https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf O
https://openreview.net/forum?id=Syg-YfWCW O
https://openreview.net/forum?id=Syg-YfWCW O

1038 B-DAT
https://www.aclweb.org/anthology/P17-2088 O
https://www.aclweb.org/anthology/P17-2088 O
https://www.aclweb.org/anthology/P17-2088 O
https://www.aclweb.org/anthology/P17-1162 O
https://www.aclweb.org/anthology/P17-1162 O

1082 B-DAT
https://aclweb.org/anthology/D15-1082 O
https://aclweb.org/anthology/D15-1082 O
https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571 O
https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571 O
http://proceedings.mlr.press/v70/liu17d/liu17d.pdf O

1016 B-DAT
https://www.aclweb.org/anthology/P15-1016 O
https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12484 O
https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12484 O
http://www.icml-2011.org/papers/438_icmlpaper.pdf O
http://www.icml-2011.org/papers/438_icmlpaper.pdf O

1060 B-DAT
https://www.aclweb.org/anthology/D17-1060 O
https://arxiv.org/abs/1412.6575 O
https://arxiv.org/abs/1412.6575 O
https://arxiv.org/abs/1412.6575 O
https://openreview.net/pdf?id=Skh4jRcKQ O

many-to- O
many O
such O
as O
actor_in_film). O
YAGO3 B-DAT

2018) O
is O
a O
subset O
of O
YAGO3 B-DAT
(Suchanek O
et O
al., O
2007) O
with O

15k O
237 O
273k O
18k O
20k O
YAGO3 B-DAT

600 O
for O
both O
FB15K-237 O
and O
YAGO3 B-DAT

ConvE O
in O
FB15K, O
WN18RR O
and O
YAGO3 B-DAT

WN18RR O
FB15K-237 O
YAGO3 B-DAT

many-to- O
many O
such O
as O
actor_in_film). O
YAGO3-10 B-DAT
(Dettmers O
et O
al., O
2018) O
is O

15k O
237 O
273k O
18k O
20k O
YAGO3-10 B-DAT
123k O
37 O
1M O
5k O
5k O

600 O
for O
both O
FB15K-237 O
and O
YAGO3-10, B-DAT
2L O
= O
200 O
for O
WN18 O

ConvE O
in O
FB15K, O
WN18RR O
and O
YAGO3-10 B-DAT

WN18RR O
FB15K-237 O
YAGO3-10 B-DAT
HITS@N O

go- O
ing O
from O
N O
= O
100, B-DAT
000 O
to O
N O
= O
1 O

that O
is, O
scoring O
against O
10 B-DAT

10 B-DAT
(Mahdisoltani, O
Biega, O
and O
Suchanek O
2015 O

which O
have O
a O
minimum O
of O
10 B-DAT
relations O
each. O
It O
has O
123,182 O

10 B-DAT

100, B-DAT
200}, O
batch O
size O
{64, O
128 O

10 B-DAT
@3 O
@1 O

10 B-DAT
and O
FB15k: O
embed- O
ding O
dropout O

10) B-DAT
and O
AUC-PR O
(Countries) O
statistics O
on O

variance, O
as O
such O
we O
average O
10 B-DAT
runs O
and O
produce O
95% O
confidence O

use O
an O
embedding O
size O
of O
100, B-DAT
AdaGrad O
(Duchi, O
Hazan, O
and O
Singer O

10 B-DAT
and O
Countries O
are O
shown O
in O

on O
inverse O
relations O
for O
YAGO3- O
10 B-DAT
and O
FB15k-237. O
The O
procedure O
used O

10, B-DAT
for O
some O
metrics O
on O
FB15k O

10 B-DAT

10 B-DAT
@3 O
@1 O
MR O
MRR O
@10 O

10 B-DAT
@3 O
@1 O
MR O
MRR O
@10 O

10 B-DAT
with O
more O
than O
8M O
parame O

10 B-DAT
and O
FB15k-237 O
compared O
to O
WN18RR O

has O
an O
indegree O
of O
over O
10, B-DAT

000. O
Many O
of O
these O
10, B-DAT

10 B-DAT
vs O
DistMult O
0.728 O
Hits@10; O
for O

10 B-DAT
vs O
DistMult O
0.938 O
Hits@10. O
This O

10 B-DAT
and O
Countries O

10 B-DAT
Countries O
Hits O
AUC-PR O

10 B-DAT
@3 O
@1 O
S1 O
S2 O
S3 O

10 B-DAT
0.43±0.07 O
ConvE O
1676 O
.44 O
.62 O

10 B-DAT

10 B-DAT
of O
ConvE O
wrt. O
DistMult O

104 B-DAT
0.06 O
WN18 O
0.125 O
0.45 O
FB15k O

10 B-DAT
0.988 O
0.21 O
Countries O
S3 O
1.415 O

10 B-DAT

10 B-DAT
and O
Countries, O
and O
about O
4 O

10 B-DAT
between O
DistMult O
and O
ConvE O
is O

In O
Proceedings O
of O
NIPS O
2012, O
1097 B-DAT

graphs. O
Proceedings O
of O
the O
IEEE O
104 B-DAT

10 B-DAT
results. O
The O
new O
results O
are O

10 B-DAT
@3 O
@1 O

Hits@k O
(%): O
100 B-DAT

graphs O
such O
as O
Freebase O
and O
YAGO3 B-DAT

YAGO3 B-DAT

2015) O
is O
a O
subset O
of O
YAGO3 B-DAT
which O
consists O
of O
entities O
which O

instead O
recommend O
FB15k-237, O
WN18RR, O
and O
YAGO3 B-DAT

parameters O
works O
well O
on O
WN18, O
YAGO3 B-DAT

mean O
reciprocal O
rank O
(WN18, O
FB15k, O
YAGO3 B-DAT

in O
Table O
4; O
results O
on O
YAGO3 B-DAT

up O
on O
inverse O
relations O
for O
YAGO3 B-DAT

formance O
for O
all O
metrics O
on O
YAGO3 B-DAT

our O
model O
on O
datasets O
like O
YAGO3 B-DAT

5: O
Link O
prediction O
results O
for O
YAGO3 B-DAT

YAGO3 B-DAT

0.599 O
0.04 O
FB15-237 O
0.733 O
0.16 O
YAGO3 B-DAT

the O
most O
central O
nodes O
in O
YAGO3 B-DAT

and O
Suchanek, O
F. O
M. O
2015. O
YAGO3 B-DAT

2018-07-04: O
– O
Added O
new O
YAGO3 B-DAT

YAGO3-10 B-DAT
(Mahdisoltani, O
Biega, O
and O
Suchanek O
2015 O

instead O
recommend O
FB15k-237, O
WN18RR, O
and O
YAGO3-10 B-DAT

parameters O
works O
well O
on O
WN18, O
YAGO3-10 B-DAT
and O
FB15k: O
embed- O
ding O
dropout O

mean O
reciprocal O
rank O
(WN18, O
FB15k, O
YAGO3-10) B-DAT
and O
AUC-PR O
(Countries) O
statistics O
on O

in O
Table O
4; O
results O
on O
YAGO3-10 B-DAT
and O
Countries O
are O
shown O
in O

formance O
for O
all O
metrics O
on O
YAGO3-10, B-DAT
for O
some O
metrics O
on O
FB15k O

our O
model O
on O
datasets O
like O
YAGO3-10 B-DAT
and O
FB15k-237 O
compared O
to O
WN18RR O

5: O
Link O
prediction O
results O
for O
YAGO3-10 B-DAT
and O
Countries O

YAGO3-10 B-DAT
Countries O
Hits O
AUC-PR O

0.599 O
0.04 O
FB15-237 O
0.733 O
0.16 O
YAGO3-10 B-DAT
0.988 O
0.21 O
Countries O
S3 O
1.415 O

the O
most O
central O
nodes O
in O
YAGO3-10 B-DAT
and O
Countries, O
and O
about O
4 O

2018-07-04: O
– O
Added O
new O
YAGO3-10 B-DAT
results. O
The O
new O
results O
are O

the-art O
results O
on O
the O
WN18RR B-DAT
dataset, O
a O
chal O

WN18RR B-DAT
dataset O
(Dettmers O
et O
al., O
2018 O

4.1 O
WN18RR B-DAT
Dataset O

leased O
the O
WN18RR B-DAT
set, O
removing O
seven O
relations O

relations O
and O
constructed O
corresponding O
subsets: O
WN18RR B-DAT
with O
11 O
relations O
and O
FB15K-237 O

41k O
18 O
141k O
5k O
5k O
WN18RR B-DAT
41k O
11 O
87k O
3k O
3k O

200 O
for O
WN18 O
and O
WN18RR B-DAT

and O
even O
ConvE O
in O
FB15K, O
WN18RR B-DAT
and O
YAGO3-10. O
The O
result O
demonstrates O

WN18RR B-DAT
FB15K-237 O
YAGO3-10 O
HITS@N O

4: O
Link O
prediction O
results O
on O
WN18RR B-DAT
and O
FB15K-237 O
datasets. O
Results O
marked O

WN18RR B-DAT
41k O
11 O
87k O
3k O
3k O

mod- O
ified O
datasets: O
FB15K-237 O
and O
WN18RR B-DAT

Model O
WN18 O
WN18RR B-DAT
FB15K O
FB15K-237 O
YAGO3-10 O

on O
the O
FB15K, O
FB15K-237, O
WN18, O
WN18RR B-DAT
datasets, O
with O
a O
rank O
set O

2017) O
includes O
performances O
on O
the O
WN18RR B-DAT
and O
YAGO3-10 O
benchmarks, O
the O
latter O

performances O
of O
DistMult O
on O
FB15K-237, O
WN18RR B-DAT
and O
YAGO3-10 O
may O
be O
slightly O

appear O
on O
the O
harder O
datasets O
WN18RR, B-DAT
FB15K-237 O
and O
YAGO3-10. O
We O
checked O

on O
WN18RR B-DAT
the O
significance O
of O
that O
gain O

WN18RR B-DAT
MRR O
0.46 O
Dettmers O
et O
al O

and O
an O
asymmetric O
(hy- O
pernym) O
WN18RR B-DAT
relation. O
Wderivationally O
related O
form O
is O

lexical O
re- O
lations O
between O
words. O
WN18RR B-DAT
(Dettmers O
et O
al., O
2018) O
is O

200. O
For O
WN18 O
and O
WN18RR, B-DAT
which O
both O
contain O
a O
sig O

risk O
of O
overfitting O
(WN18 O
and O
WN18RR), B-DAT
whereas O
higher O
dropout O
values O
(0.3 O

WN18 O
and O
(0.01, O
1.0) O
for O
WN18RR B-DAT
(see O
Table O
5 O
in O
the O

14,541 O
237 O
WN18 O
40,943 O
18 O
WN18RR B-DAT
40,943 O
11 O

and O
dr O
= O
30 O
on O
WN18RR B-DAT
(∼9.4 O
vs∼16.4 O
million), O
TuckER O
consistently O

more O
parameters O
than O
TuckER O
on O
WN18RR B-DAT

WN18RR B-DAT
FB15k-237 O

3: O
Link O
prediction O
results O
on O
WN18RR B-DAT
and O
FB15k-237. O
The O
RotatE O
(Sun O

30 O
0.2 O
0.1 O
0.2 O
0.1 O
WN18RR B-DAT
0.01 O
1.0 O
200 O
30 O
0.2 O

els O
using O
standard O
datasets O
(FB15k-237, O
WN18RR, B-DAT
FB15k, O
WN18, O
YAGO3-10), O
across O
which O

WN18RR B-DAT
[3] O
a O
subset O
of O
WN18 O

40,943 O
18 O
FB15k-237 O
14,541 O
237 O
WN18RR B-DAT
40,943 O
11 O
YAGO3-10 O
123,182 O
37 O

WN18 O
and O
mean O
rank O
on O
WN18RR, B-DAT
FB15k-237, O
WN18, O
and O
YAGO3-10. O
Given O

report O
the O
obtained O
results O
on O
WN18RR B-DAT
in O
Table O
7. O
We O
perform O

4. O
Link O
prediction O
results O
on O
WN18RR B-DAT
and O
FB15k-237. O
The O
RotatE O
[19 O

WN18RR B-DAT
FB15k-237 O

7. O
Link O
prediction O
results O
on O
WN18RR B-DAT

WN18RR B-DAT
MR O
MRR O
H@10 O
H@3 O
H@1 O

with O
and O
without O
hypernetwork O
on O
WN18RR B-DAT
and O
FB15k-237 O

WN18RR B-DAT
FB15k-237 O

we O
compare O
HypER O
results O
on O
WN18RR B-DAT
and O
FB15k-237 O
with O
the O
hypernetwork O

of O
no O
benefit, O
especially O
on O
WN18RR B-DAT

improvement O
in O
prediction O
scores O
for O
WN18RR B-DAT

WN18RR B-DAT
FB15k-237 O

Knowledge O
Base O
Completion O
We O
use O
WN18RR B-DAT
and O
NELL995 O
knowledge O
graph O
datasets O

for O
evaluation. O
WN18RR B-DAT
[6] O
is O
created O
from O
the O

protocol O
as O
in O
[6] O
for O
WN18RR B-DAT
and O
in O
[38, O
5] O
for O

as O
the O
evaluation O
metrics O
for O
WN18RR, B-DAT
and O
use O
mean O
average O
precision O

the O
metrics O
on O
NELL995 O
and O
WN18RR B-DAT
datasets. O
Additional O
experiments O
on O
the O

2: O
The O
results O
on O
the O
WN18RR B-DAT
dataset, O
in O
the O
form O
of O

We O
observed O
similar O
trends O
on O
WN18RR B-DAT

and O
different O
search O
horizons O
on O
WN18RR B-DAT
dataset, O
with O
results O
shown O
in O

to O
traditional O
methods O
on O
the O
WN18RR B-DAT
dataset. O
The O
first O
question O
is O

about O
20–60 O
unique O
candidates O
on O
WN18RR B-DAT

hyperparameter O
and O
error O
analysis O
on O
WN18RR B-DAT

both O
training O
and O
testing O
on O
WN18RR B-DAT
with O
different O
values O
of O
search O

WN18RR B-DAT
86,835 O
3,134 O
11 O
40,943 O
2.19 O

unique O
entities O
and O
200 O
relations. O
WN18RR B-DAT
contains O
93, O
003 O
triples O
with O

dataset O
and O
5 O
in O
the O
WN18RR B-DAT
dataset. O
After O
the O
STOP O
action O

128 O
MCTS O
paths O
in O
the O
WN18RR B-DAT
dataset. O
We O
use O
the O
ADAM O

results O
for O
the O
NELL995 O
and O
WN18RR B-DAT
tasks O
to O
support O
our O
analysis O

hyperparameter O
and O
error O
analysis O
on O
WN18RR B-DAT

with O
WN18. O
We O
thus O
create O
WN18RR B-DAT
to O
reclaim O
WN18 O
as O
a O

of O
the O
complete O
knowledge O
graph. O
WN18RR1 B-DAT
contains O
93,003 O
triples O
with O
40,943 O

WN18 O
and O
instead O
recommend O
FB15k-237, O
WN18RR, B-DAT
and O
YAGO3-10 O

of O
our O
inverse O
model O
on O
WN18RR, B-DAT
which O
was O
derived O
using O
the O

4: O
Link O
prediction O
results O
for O
WN18RR B-DAT
and O
FB15k-237 O

WN18RR B-DAT
FB15k-237 O
Hits O
Hits O

YAGO3-10 O
and O
FB15k-237 O
compared O
to O
WN18RR, B-DAT
is O
that O
these O
datasets O
contain O

average O
relation- O
specific O
indegree O
(like O
WN18RR B-DAT
and O
WN18), O
a O
shallow O
model O

less O
complex O
graphs O
(e.g. O
WN18 O
WN18RR B-DAT

WN18RR B-DAT
0.104 O
0.06 O
WN18 O
0.125 O
0.45 O

investigated O
datasets O
exists, O
we O
derive O
WN18RR B-DAT

tested O
thus O
far O
(Kinship, O
WN18, O
WN18RR, B-DAT
FB15k-237). O
• O
2018-03-28 O

els O
on O
two O
benchmark O
datasets O
WN18RR B-DAT
and O
FB15k-237 O

ConvKB O
on O
two O
benchmark O
datasets: O
WN18RR B-DAT
(Dettmers O
et O
al., O
2018) O
and O

and O
the O
highest O
Hits@10 O
on O
WN18RR, B-DAT
and O
pro- O
duces O
the O
highest O

R O
| O
#Triples O
in O
train/valid/test O
WN18RR B-DAT
40,943 O
11 O
86,835 O
3,034 O
3,134 O

ConvKB O
on O
two O
benchmark O
datasets: O
WN18RR B-DAT
(Dettmers O
et O
al., O
2018) O
and O

237 O
(Toutanova O
and O
Chen, O
2015). O
WN18RR B-DAT
and O
FB15k-237 O
are O
correspondingly O
subsets O

Dettmers O
et O
al. O
(2018). O
Therefore, O
WN18RR B-DAT
and O
FB15k- O
237 O
are O
created O

2 O
presents O
the O
statistics O
of O
WN18RR B-DAT
and O
FB15k-237 O

Method O
WN18RR B-DAT
FB15k-237 O
MR O
MRR O
H@10 O
MR O

Table O
3: O
Experimental O
results O
on O
WN18RR B-DAT
and O
FB15k-237 O
test O
sets. O
MRR O

to O
2 O
decimal O
places O
on O
WN18RR B-DAT

and O
k O
= O
50 O
for O
WN18RR, B-DAT
and O
using O
l1-norm, O
learning O
rate O

learning O
rate O
at O
1e−4 O
on O
WN18RR B-DAT

and O
highest O
Hits@10 O
scores O
on O
WN18RR B-DAT
and O
also O
the O
highest O

and O
Hits@10 O
than O
ConvE O
on O
WN18RR B-DAT

both O
datasets O
(except O
MRR O
on O
WN18RR B-DAT
and O
MR O
on O
FB15k-237), O
thus O

models O
on O
two O
benchmark O
datasets O
WN18RR B-DAT
and O
FB15k-237. O
Our O
code O
is O

has O
been O
noted O
that O
the O
WN18 B-DAT
and O
FB15k O
datasets O
suffer O
from O

achieve O
state-of-the-art O
results O
on O
both O
WN18 B-DAT
and O
FB15k. O
To O
ensure O
that O

WN18 B-DAT
(Bordes O
et O
al. O
2013a) O
is O

and, O
for O
such O
a O
reason, O
WN18 B-DAT
tends O
to O
follow O
a O
strictly O

Toutanova O
and O
Chen O
(2015) O
that O
WN18 B-DAT
and O
FB15k O
suffer O
from O
test O

ing O
state-of-the-art O
results O
on O
both O
WN18 B-DAT
and O
FB15k. O
In O
order O
to O

we O
also O
find O
flaws O
with O
WN18 B-DAT

thus O
create O
WN18RR B-DAT
to O
reclaim O
WN18 O
as O
a O
dataset, O
which O
cannot O

recommend O
against O
using O
FB15k O
and O
WN18 B-DAT
and O
instead O
recommend O
FB15k-237, O
WN18RR O

of O
parameters O
works O
well O
on O
WN18, B-DAT
YAGO3-10 O
and O
FB15k: O
embed- O
ding O

to O
the O
mean O
reciprocal O
rank O
(WN18, B-DAT
FB15k, O
YAGO3-10) O
and O
AUC-PR O
(Countries O

that O
the O
training O
datasets O
of O
WN18 B-DAT
and O
FB15k O
have O
94% O
and O

standard O
bench- O
marks O
FB15k O
and O
WN18 B-DAT
are O
shown O
in O
Table O
3 O

metrics O
for O
both O
FB15k O
and O
WN18 B-DAT

and O
it O
does O
well O
on O
WN18 B-DAT

3: O
Link O
prediction O
results O
for O
WN18 B-DAT
and O
FB15k O

WN18 B-DAT
FB15k O
Hits O
Hits O

specific O
indegree O
(like O
WN18RR B-DAT
and O
WN18), O
a O
shallow O
model O
like O
DistMult O

WN18) B-DAT
and O
high O
(high-FB15k) O
relation-specific O
in O

WN18) B-DAT
and O
low O
(low- O
FB15k) O
relation-specific O

WN18 B-DAT
we O
have O
ConvE O
0.952 O
Hits@10 O

model O
less O
complex O
graphs O
(e.g. O
WN18 B-DAT
WN18RR O

WN18RR B-DAT
0.104 O
0.06 O
WN18 O
0.125 O
0.45 O
FB15k O
0.599 O
0.04 O

the O
most O
central O
nodes O
in O
WN18 B-DAT
have O
a O
PageRank O
value O
more O

leakage O
through O
inverse O
relations O
of O
WN18 B-DAT
and O
FB15k O
was O
first O
reported O

can O
achieve O
state-of-the-art O
results O
on O
WN18 B-DAT
and O
FB15k. O
To O
ensure O
robust O

results O
are O
slightly O
better O
for O
WN18 B-DAT
and O
slightly O
worse O
for O
FB15k O

we O
tested O
thus O
far O
(Kinship, O
WN18, B-DAT
WN18RR, O
FB15k-237). O
• O
2018-03-28 O

has O
been O
noted O
that O
the O
WN18 B-DAT
and O
FB15k O
datasets O
suffer O
from O

achieve O
state-of-the-art O
results O
on O
both O
WN18 B-DAT
and O
FB15k. O
To O
ensure O
that O

WN18 B-DAT
(Bordes O
et O
al. O
2013a) O
is O

and, O
for O
such O
a O
reason, O
WN18 B-DAT
tends O
to O
follow O
a O
strictly O

Toutanova O
and O
Chen O
(2015) O
that O
WN18 B-DAT
and O
FB15k O
suffer O
from O
test O

ing O
state-of-the-art O
results O
on O
both O
WN18 B-DAT
and O
FB15k. O
In O
order O
to O

we O
also O
find O
flaws O
with O
WN18 B-DAT

thus O
create O
WN18RR B-DAT
to O
reclaim O
WN18 O
as O
a O
dataset, O
which O
cannot O

recommend O
against O
using O
FB15k O
and O
WN18 B-DAT
and O
instead O
recommend O
FB15k-237, O
WN18RR O

of O
parameters O
works O
well O
on O
WN18, B-DAT
YAGO3-10 O
and O
FB15k: O
embed- O
ding O

to O
the O
mean O
reciprocal O
rank O
(WN18, B-DAT
FB15k, O
YAGO3-10) O
and O
AUC-PR O
(Countries O

that O
the O
training O
datasets O
of O
WN18 B-DAT
and O
FB15k O
have O
94% O
and O

standard O
bench- O
marks O
FB15k O
and O
WN18 B-DAT
are O
shown O
in O
Table O
3 O

metrics O
for O
both O
FB15k O
and O
WN18 B-DAT

and O
it O
does O
well O
on O
WN18 B-DAT

3: O
Link O
prediction O
results O
for O
WN18 B-DAT
and O
FB15k O

WN18 B-DAT
FB15k O
Hits O
Hits O

specific O
indegree O
(like O
WN18RR B-DAT
and O
WN18 O

WN18 B-DAT

WN18 B-DAT

WN18 B-DAT
we O
have O
ConvE O
0.952 O
Hits@10 O

model O
less O
complex O
graphs O
(e.g. O
WN18 B-DAT
WN18RR O

WN18RR B-DAT
0.104 O
0.06 O
WN18 O
0.125 O
0.45 O
FB15k O
0.599 O
0.04 O

the O
most O
central O
nodes O
in O
WN18 B-DAT
have O
a O
PageRank O
value O
more O

leakage O
through O
inverse O
relations O
of O
WN18 B-DAT
and O
FB15k O
was O
first O
reported O

can O
achieve O
state-of-the-art O
results O
on O
WN18 B-DAT
and O
FB15k. O
To O
ensure O
robust O

results O
are O
slightly O
better O
for O
WN18 B-DAT
and O
slightly O
worse O
for O
FB15k O

we O
tested O
thus O
far O
(Kinship, O
WN18, B-DAT
WN18RR, O
FB15k-237). O
• O
2018-03-28 O

simple O
models O
to O
do O
well. O
WN18 B-DAT
(Bordes O
et O
al., O
2013) O
is O

2018) O
is O
a O
subset O
of O
WN18, B-DAT
created O
by O
removing O
the O
inverse O

dr O
= O
200. O
For O
WN18 B-DAT
and O
WN18RR, O
which O
both O
contain O

thus O
less O
risk O
of O
overfitting O
(WN18 B-DAT
and O
WN18RR), O
whereas O
higher O
dropout O

FB15k- O
237, O
(0.005, O
0.995) O
for O
WN18 B-DAT
and O
(0.01, O
1.0) O
for O
WN18RR O

14,951 O
1,345 O
FB15k-237 O
14,541 O
237 O
WN18 B-DAT
40,943 O
18 O
WN18RR O
40,943 O
11 O

datasets O
(apart O
from O
hits@10 O
on O
WN18 B-DAT
where O

WN18 B-DAT
FB15k O

4: O
Link O
prediction O
results O
on O
WN18 B-DAT
and O
FB15k O

200 O
0.3 O
0.4 O
0.5 O
0.1 O
WN18 B-DAT
0.005 O
0.995 O
200 O
30 O
0.2 O

simple O
models O
to O
do O
well. O
WN18 B-DAT
(Bordes O
et O
al., O
2013) O
is O

2018) O
is O
a O
subset O
of O
WN18, B-DAT
created O
by O
removing O
the O
inverse O

dr O
= O
200. O
For O
WN18 B-DAT
and O
WN18RR, O
which O
both O
contain O

thus O
less O
risk O
of O
overfitting O
(WN18 B-DAT
and O
WN18RR), O
whereas O
higher O
dropout O

FB15k- O
237, O
(0.005, O
0.995) O
for O
WN18 B-DAT
and O
(0.01, O
1.0) O
for O
WN18RR O

14,951 O
1,345 O
FB15k-237 O
14,541 O
237 O
WN18 B-DAT
40,943 O
18 O
WN18RR O
40,943 O
11 O

datasets O
(apart O
from O
hits@10 O
on O
WN18 B-DAT
where O

WN18 B-DAT
FB15k O

4: O
Link O
prediction O
results O
on O
WN18 B-DAT
and O
FB15k O

200 O
0.3 O
0.4 O
0.5 O
0.1 O
WN18 B-DAT
0.005 O
0.995 O
200 O
30 O
0.2 O

standard O
datasets O
(FB15k-237, O
WN18RR, B-DAT
FB15k, O
WN18, O
YAGO3-10), O
across O
which O
it O
consistently O

WN18 B-DAT
[1] O
a O
subset O
of O
WordNet O

test O
sets O
of O
FB15k O
and O
WN18 B-DAT
contain O
the O
inverse O
of O
many O

WN18RR B-DAT
[3] O
a O
subset O
of O
WN18, O
created O
by O
removing O
the O
inverse O

FB15k O
14,951 O
1,345 O
WN18 B-DAT
40,943 O
18 O
FB15k-237 O
14,541 O
237 O

from O
mean O
reciprocal O
rank O
on O
WN18 B-DAT
and O
mean O
rank O
on O
WN18RR O

, O
FB15k-237, O
WN18, B-DAT
and O
YAGO3-10. O
Given O
that O
mean O

5. O
Link O
prediction O
results O
on O
WN18 B-DAT
and O
FB15k O

WN18 B-DAT
FB15k O

standard O
datasets O
(FB15k-237, O
WN18RR, B-DAT
FB15k, O
WN18, O
YAGO3-10), O
across O
which O
it O
consistently O

WN18 B-DAT
[1] O
a O
subset O
of O
WordNet O

test O
sets O
of O
FB15k O
and O
WN18 B-DAT
contain O
the O
inverse O
of O
many O

WN18RR B-DAT
[3] O
a O
subset O
of O
WN18, O
created O
by O
removing O
the O
inverse O

FB15k O
14,951 O
1,345 O
WN18 B-DAT
40,943 O
18 O
FB15k-237 O
14,541 O
237 O

from O
mean O
reciprocal O
rank O
on O
WN18 B-DAT
and O
mean O
rank O
on O
WN18RR O

, O
FB15k-237, O
WN18, B-DAT
and O
YAGO3-10. O
Given O
that O
mean O

5. O
Link O
prediction O
results O
on O
WN18 B-DAT
and O
FB15k O

WN18 B-DAT
FB15k O

datasets O
such O
as O
FB15K O
or O
WN18 B-DAT
are O
surprising. O
First, O
let O
us O

have O
a O
small O
impact. O
On O
WN18 B-DAT
however, O
they O
make O
up O
60 O

on O
the O
hierarchical O
predicates O
of O
WN18 B-DAT
despite O
its O
symmetricity O
assumption O

N O
P O
Train O
Valid O
Test O
WN18 B-DAT
41k O
18 O
141k O
5k O
5k O

WN18 B-DAT
and O
FB15K O
are O
popular O
benchmarks O

Model O
WN18 B-DAT
WN18RR O
FB15K O
FB15K-237 O
YAGO3-10 O

regularizers O
on O
the O
FB15K, O
FB15K-237, O
WN18, B-DAT
WN18RR O
datasets, O
with O
a O
rank O

and O
the O
extensive O
experiments O
on O
WN18 B-DAT
and O
FB15K O
reported O
in O
Kadlec O

to O
0.95 O
filtered O
MRR O
on O
WN18, B-DAT
or O
from O
0.46 O
to O
0.86 O

WN18 B-DAT
MRR O
0.94 O
Trouillon O
et O
al O

datasets O
such O
as O
FB15K O
or O
WN18 B-DAT
are O
surprising. O
First, O
let O
us O

have O
a O
small O
impact. O
On O
WN18 B-DAT
however, O
they O
make O
up O
60 O

on O
the O
hierarchical O
predicates O
of O
WN18 B-DAT
despite O
its O
symmetricity O
assumption O

N O
P O
Train O
Valid O
Test O
WN18 B-DAT
41k O
18 O
141k O
5k O
5k O

WN18 B-DAT
and O
FB15K O
are O
popular O
benchmarks O

Model O
WN18 B-DAT
WN18RR O
FB15K O
FB15K-237 O
YAGO3-10 O

regularizers O
on O
the O
FB15K, O
FB15K-237, O
WN18, B-DAT
WN18RR O
datasets, O
with O
a O
rank O

and O
the O
extensive O
experiments O
on O
WN18 B-DAT
and O
FB15K O
reported O
in O
Kadlec O

to O
0.95 O
filtered O
MRR O
on O
WN18, B-DAT
or O
from O
0.46 O
to O
0.86 O

as O
hyponym O
or O
hypernym O
in O
WN, B-DAT
we O
expect O
the O
filtered O
MRR O

WN18 B-DAT
MRR O
0.94 O
Trouillon O
et O
al O

for O
generic O
facts O
and O
WordNet O
(WN18) B-DAT
for O
lexical O
relationships O
between O
words O

14,951 O
1,345 O
483,142 O
50,000 O
59,071 O
WN18 B-DAT
40,943 O
18 O
141,442 O
5,000 O
5,000 O

Dataset O
statistics O
for O
FB15K O
and O
WN18 B-DAT

filt.) O
of O
all O
models O
on O
WN18 B-DAT
and O
FB15K O
cate- O
gories O
into O

Models O
WN18 B-DAT
FB15K O

The O
resulting O
hyperparameters O
for O
the O
WN18 B-DAT
dataset O
are O
m O
= O
200 O

subset O
of O
representative O
models O
on O
WN18 B-DAT
and O
FB15K. O
The O
performance O
scores O

WN18 B-DAT
FB15 O

in O
the O
literature O
on O
the O
WN18 B-DAT
and O
FB15K O
datasets. O
For O
the O

8.3x O
speedup O
on O
FB15K O
and O
WN18, B-DAT
re- O
spectively, O
on O
a O
single O

FB15K O
WN18 B-DAT

FB15K O
WN18 B-DAT

for O
generic O
facts O
and O
WordNet O
(WN18 B-DAT

14,951 O
1,345 O
483,142 O
50,000 O
59,071 O
WN18 B-DAT
40,943 O
18 O
141,442 O
5,000 O
5,000 O

Dataset O
statistics O
for O
FB15K O
and O
WN18 B-DAT

filt.) O
of O
all O
models O
on O
WN18 B-DAT
and O
FB15K O
cate- O
gories O
into O

Models O
WN18 B-DAT
FB15K O

The O
resulting O
hyperparameters O
for O
the O
WN18 B-DAT
dataset O
are O
m O
= O
200 O

subset O
of O
representative O
models O
on O
WN18 B-DAT
and O
FB15K. O
The O
performance O
scores O

WN18 B-DAT
FB15 O

in O
the O
literature O
on O
the O
WN18 B-DAT
and O
FB15K O
datasets. O
For O
the O

8.3x O
speedup O
on O
FB15K O
and O
WN18, B-DAT
re- O
spectively, O
on O
a O
single O

FB15K O
WN18 B-DAT

FB15K O
WN18 B-DAT

experiments O
on O
two O
standard O
benchmarks: O
WN18 B-DAT
a O
subset O
of O
Wordnet O
[24 O

train/valid/test O
sets O
as O
in O
[4]. O
WN18 B-DAT
contains O
40, O
943 O
entities, O
18 O

Table O
1: O
Results O
on O
WN18 B-DAT
and O
FB15k. O
Best O
results O
are O

WN18 B-DAT
FB15k O

set O
the O
learning O
rate O
for O
WN18 B-DAT
to O
0.1 O
and O
for O
FB15k O

example O
per O
positive O
example O
for O
WN18 B-DAT
and O
10 O
negative O
examples O
per O

set O
every O
50 O
iterations O
for O
WN18 B-DAT
and O
every O
100 O
iterations O
for O

size O
and O
λ O
values O
on O
WN18 B-DAT
for O
SimplE-ignr O
were O
200 O
and O

baselines O
on O
both O
datasets. O
On O
WN18, B-DAT
SimplE-ignr O
and O
SimplE O
perform O
as O

FB15k O
and O
has O
part O
in O
WN18 B-DAT

we O
conducted O
an O
experiment O
on O
WN18 B-DAT
in O
which O
we O
incorporated O
several O

experiments O
on O
two O
standard O
benchmarks: O
WN18 B-DAT
a O
subset O
of O
Wordnet O
[24 O

train/valid/test O
sets O
as O
in O
[4]. O
WN18 B-DAT
contains O
40, O
943 O
entities, O
18 O

Table O
1: O
Results O
on O
WN18 B-DAT
and O
FB15k. O
Best O
results O
are O

WN18 B-DAT
FB15k O

set O
the O
learning O
rate O
for O
WN18 B-DAT
to O
0.1 O
and O
for O
FB15k O

example O
per O
positive O
example O
for O
WN18 B-DAT
and O
10 O
negative O
examples O
per O

set O
every O
50 O
iterations O
for O
WN18 B-DAT
and O
every O
100 O
iterations O
for O

size O
and O
λ O
values O
on O
WN18 B-DAT
for O
SimplE-ignr O
were O
200 O
and O

baselines O
on O
both O
datasets. O
On O
WN18, B-DAT
SimplE-ignr O
and O
SimplE O
perform O
as O

FB15k O
and O
has O
part O
in O
WN18 B-DAT

we O
conducted O
an O
experiment O
on O
WN18 B-DAT
in O
which O
we O
incorporated O
several O

4.2. O
Datasets: O
FB15K O
and O
WN18 B-DAT

E| O
|R| O
#triples O
in O
Train/Valid/Test O
WN18 B-DAT
40,943 O
18 O
141,442 O
/ O
5,000 O

split O
for O
the O
FB15K O
and O
WN18 B-DAT
datasets O

model O
on O
the O
FB15K O
and O
WN18 B-DAT
datasets. O
FB15K O
is O
a O
subset O

KB O
of O
general O
facts, O
whereas O
WN18 B-DAT
is O
a O
subset O
of O
Wordnet O

WN18 B-DAT
describes O
lexical O
and O
semantic O
hierarchies O

considered O
and O
each O
relation O
of O
WN18, B-DAT
confirming O
the O
advantage O
of O
our O

WN18 B-DAT
FB15K O
MRR O
Hits O
at O
MRR O

tested O
on O
the O
FB15K O
and O
WN18 B-DAT
datasets. O
Hits@m O
metrics O
are O
filtered O

relation O
of O
the O
Wordnet O
dataset O
(WN18 B-DAT

second. O
CP O
performs O
poorly O
on O
WN18 B-DAT
due O
to O
the O
small O
number O

negatives), O
but O
not O
much O
on O
WN18 B-DAT

while O
not O
so O
much O
on O
WN18 B-DAT

B. O
WN18 B-DAT
embeddings O
visualization O
We O
used O
principal O

relations O
of O
the O
wordnet O
dataset O
(WN18 B-DAT

Most O
of O
WN18 B-DAT
relations O
describe O
hierarchies, O
and O
are O

fourth O
(Bottom) O
components O
of O
the O
WN18 B-DAT
relations O
embeddings O
using O
PCA. O
Left O

4.2. O
Datasets: O
FB15K O
and O
WN18 B-DAT

E| O
|R| O
#triples O
in O
Train/Valid/Test O
WN18 B-DAT
40,943 O
18 O
141,442 O
/ O
5,000 O

split O
for O
the O
FB15K O
and O
WN18 B-DAT
datasets O

model O
on O
the O
FB15K O
and O
WN18 B-DAT
datasets. O
FB15K O
is O
a O
subset O

KB O
of O
general O
facts, O
whereas O
WN18 B-DAT
is O
a O
subset O
of O
Wordnet O

WN18 B-DAT
describes O
lexical O
and O
semantic O
hierarchies O

considered O
and O
each O
relation O
of O
WN18, B-DAT
confirming O
the O
advantage O
of O
our O

WN18 B-DAT
FB15K O
MRR O
Hits O
at O
MRR O

tested O
on O
the O
FB15K O
and O
WN18 B-DAT
datasets. O
Hits@m O
metrics O
are O
filtered O

relation O
of O
the O
Wordnet O
dataset O
(WN18 B-DAT

second. O
CP O
performs O
poorly O
on O
WN18 B-DAT
due O
to O
the O
small O
number O

negatives), O
but O
not O
much O
on O
WN18 B-DAT

while O
not O
so O
much O
on O
WN18 B-DAT

B. O
WN18 B-DAT
embeddings O
visualization O
We O
used O
principal O

relations O
of O
the O
wordnet O
dataset O
(WN18 B-DAT

Most O
of O
WN18 B-DAT
relations O
describe O
hierarchies, O
and O
are O

fourth O
(Bottom) O
components O
of O
the O
WN18 B-DAT
relations O
embeddings O
using O
PCA. O
Left O

Datasets O
We O
used O
the O
WordNet O
(WN) B-DAT
and O
Freebase O
(FB15k) O
datasets O
introduced O

in O
(Bordes O
et O
al., O
2013b). O
WN B-DAT
contains O
151, O
442 O
triplets O
with O

and O
T O
“ O
300 O
on O
WN B-DAT
(T O
was O
determined O
based O
on O

FB15k O
FB15k-401 O
WN B-DAT
MRR O
HITS@10 O
MRR O
HITS@10 O
MRR O

performance O
on O
both O
FB O
and O
WN, B-DAT
which O
suggests O
overfitting. O
Compared O
to O

and O
90.9% O
vs. O
89.2% O
on O
WN) B-DAT
using O
the O
same O
evaluation O
metric O

performance O
than O
TransE, O
especially O
on O
WN B-DAT

. O
Note O
that O
WN B-DAT
contains O
much O
more O
entities O
than O

comparable O
performance O
to O
Bilinear O
on O
WN B-DAT

is O
relatively O
small O
(compared O
to O
WN), B-DAT
the O
simple O
form O
of O
BILINEAR-DIAG O

R| O
# O
train O
# O
test O
WN18 B-DAT
40,943 O
18 O
141,442 O
5,000 O
FB15k O

104 O
WN18 B-DAT

k O
=∞), O
the O
performance O
for O
WN18 B-DAT
does O
not O
deteriorate O
as O
it O

entity O
prediction O
experiments. O
Data O
Set O
WN18 B-DAT
FB15K O
Metric O
Mean O
rank O
Hits@10 O

objects O
in O
WN18 B-DAT
have O
on O
average O
few O
neighbors O

Since O
most O
object O
pairs O
of O
WN18 B-DAT
have O
a O
1-neighborhood O
whose O
size O

R| O
# O
train O
# O
test O
WN18 B-DAT
40,943 O
18 O
141,442 O
5,000 O
FB15k O

104 O
WN18 B-DAT

k O
=∞), O
the O
performance O
for O
WN18 B-DAT
does O
not O
deteriorate O
as O
it O

entity O
prediction O
experiments. O
Data O
Set O
WN18 B-DAT
FB15K O
Metric O
Mean O
rank O
Hits@10 O

objects O
in O
WN18 B-DAT
have O
on O
average O
few O
neighbors O

Since O
most O
object O
pairs O
of O
WN18 B-DAT
have O
a O
1-neighborhood O
whose O
size O

measure O
of O
71.5% O
on O
the O
DAVIS B-DAT
2017 O
validation O
set. O
We O
make O

and O
the O
introduction O
of O
the O
DAVIS B-DAT
datasets O
[29, O
31], O
there O
has O

ponents. O
For O
example, O
the O
2018 O
DAVIS B-DAT
challenge O
was O
won O
by O
PReMVOS O

65% O
J&F O
score O
on O
the O
DAVIS B-DAT
2017 O
vali- O
dation O
set O

tuning O
on O
the O
DAVIS B-DAT
2017 O
validation O
dataset O
with O
a O

algorithm O
and O
won O
the O
2018 O
DAVIS B-DAT
Challenge O
[2] O
and O
also O
the O

training O
data O
we O
use O
the O
DAVIS B-DAT
2017 O
[31] O
training O
set O
(60 O

network O
is O
evaluated O
on O
the O
DAVIS B-DAT

2016 O
[29] O
validation O
set, O
the O
DAVIS B-DAT
2017 O
[31] O
validation O
and O
test-dev O

YouTube-Objects O
[32, O
19] O
dataset. O
The O
DAVIS B-DAT
2016 O
validation O
set O
consists O
of O

single O
instance O
is O
annotated. O
The O
DAVIS B-DAT
2017 O
dataset O
comprises O
a O
training O

set O
that O
ex- O
tends O
the O
DAVIS B-DAT
2016 O
validation O
set O
to O
a O

2. O
Quantitative O
results O
on O
the O
DAVIS B-DAT
2017 O
validation O
set. O
FT O
denotes O

seconds. O
†: O
timing O
extrapolated O
from O
DAVIS B-DAT
2016 O
assuming O
linear O
scaling O
in O

3. O
Quantitative O
results O
on O
the O
DAVIS B-DAT
2017 O
test-dev O
set. O
FT O
denotes O

seconds. O
†: O
timing O
extrapolated O
from O
DAVIS B-DAT
2016 O
assuming O
linear O
scaling O
in O

4. O
Quality O
versus O
timing O
on O
DAVIS B-DAT
2017. O
The O
pro- O
posed O
FEELVOS O

with O
multiple O
instances O
annotated. O
The O
DAVIS B-DAT
2017 O
test- O
dev O
set O
also O

the O
evaluation O
measures O
defined O
by O
DAVIS B-DAT
[29]. O
The O
first O
evaluation O
criterion O

compare O
our O
results O
on O
the O
DAVIS B-DAT
2017 O
validation O
and O
test-dev O
sets O

4. O
Quantitative O
results O
on O
the O
DAVIS B-DAT
2016 O
validation O
set. O
FT O
denotes O

does O
not O
provide O
results O
for O
DAVIS B-DAT
2017. O
On O
the O
val- O
idation O

slow O
PReMVOS O
[26]. O
On O
the O
DAVIS B-DAT
2017 O
test-dev O
set, O
FEELVOS O
achieves O

the O
results O
on O
the O
simpler O
DAVIS B-DAT
2016 O
validation O
set O
which O
only O

Table O
6. O
Ablation O
study O
on O
DAVIS B-DAT
2017. O
FF O
and O
PF O
denote O

components O
of O
FEELVOS O
on O
the O
DAVIS B-DAT
2017 O
val- O
idation O
set. O
For O

we O
only O
use O
the O
smaller O
DAVIS B-DAT
2017 O
training O
set O
as O
training O

5. O
Qualitative O
results O
on O
the O
DAVIS B-DAT
2017 O
validation O
set O
and O
on O

results O
of O
FEELVOS O
on O
the O
DAVIS B-DAT
2017 O
validation O
set O
and O
the O

the O
art O
re- O
sults O
on O
DAVIS B-DAT
2017 O
for O
VOS O
without O
fine-tuning O

and O
J. O
Pont-Tuset. O
The O
2018 O
DAVIS B-DAT
challenge O
on O
video O
object O
segmentation O

object O
segmentation O
2018. O
The O
2018 O
DAVIS B-DAT
Challenge O
on O
Video O
Object O
Segmentation O

L. O
Van O
Gool. O
The O
2017 O
DAVIS B-DAT
chal- O
lenge O
on O
video O
object O

neural O
networks O
for O
the O
2017 O
DAVIS B-DAT
challenge O
on O
video O
object O
segmentation O

. O
The O
2017 O
DAVIS B-DAT
Challenge O
on O
Video O
Object O
Segmentation O

of O
71.5% O
on O
the O
DAVIS O
2017 B-DAT
validation O
set. O
We O
make O
our O

J&F O
score O
on O
the O
DAVIS O
2017 B-DAT
vali- O
dation O
set O

tuning O
on O
the O
DAVIS O
2017 B-DAT
validation O
dataset O
with O
a O
J&F O

data O
we O
use O
the O
DAVIS O
2017 B-DAT
[31] O
training O
set O
(60 O
videos O

29] O
validation O
set, O
the O
DAVIS O
2017 B-DAT
[31] O
validation O
and O
test-dev O
sets O

instance O
is O
annotated. O
The O
DAVIS O
2017 B-DAT
dataset O
comprises O
a O
training O
set O

Quantitative O
results O
on O
the O
DAVIS O
2017 B-DAT
validation O
set. O
FT O
denotes O
fine-tuning O

Quantitative O
results O
on O
the O
DAVIS O
2017 B-DAT
test-dev O
set. O
FT O
denotes O
fine-tuning O

Quality O
versus O
timing O
on O
DAVIS O
2017 B-DAT

multiple O
instances O
annotated. O
The O
DAVIS O
2017 B-DAT
test- O
dev O
set O
also O
contains O

our O
results O
on O
the O
DAVIS O
2017 B-DAT
validation O
and O
test-dev O
sets O
to O

not O
provide O
results O
for O
DAVIS O
2017 B-DAT

PReMVOS O
[26]. O
On O
the O
DAVIS O
2017 B-DAT
test-dev O
set, O
FEELVOS O
achieves O
a O

6. O
Ablation O
study O
on O
DAVIS O
2017 B-DAT

of O
FEELVOS O
on O
the O
DAVIS O
2017 B-DAT
val- O
idation O
set. O
For O
simplicity O

only O
use O
the O
smaller O
DAVIS O
2017 B-DAT
training O
set O
as O
training O
data O

Qualitative O
results O
on O
the O
DAVIS O
2017 B-DAT
validation O
set O
and O
on O
YouTube-Objects O

of O
FEELVOS O
on O
the O
DAVIS O
2017 B-DAT
validation O
set O
and O
the O
YouTube-Objects O

art O
re- O
sults O
on O
DAVIS O
2017 B-DAT
for O
VOS O
without O
fine-tuning. O
Overall O

object O
seg- O
mentation. O
In O
CVPR, O
2017 B-DAT

fully O
con- O
nected O
crfs. O
PAMI, O
2017 B-DAT

segmenta- O
tion. O
arXiv O
preprint O
arXiv:1706.05587, O
2017 B-DAT

separa- O
ble O
convolutions. O
In O
CVPR, O
2017 B-DAT

metric O
learning. O
arXiv O
preprint O
arXiv:1703.10277, O
2017 B-DAT

appli- O
cations. O
arXiv O
preprint O
arXiv:1704.04861, O
2017 B-DAT

video O
object O
segmentation. O
In O
NeurIPS, O
2017 B-DAT

In O
arXiv O
preprint O
arXiv: O
1703.09554, O
2017 B-DAT

from O
static O
images. O
In O
CVPR, O
2017 B-DAT

in O
street O
scenes. O
In O
CVPR, O
2017 B-DAT

and O
L. O
Van O
Gool. O
The O
2017 B-DAT
DAVIS O
chal- O
lenge O
on O
video O

object O
segmentation. O
arXiv O
preprint O
arXiv:1704.00675, O
2017 B-DAT

coco O
detection O
and O
segmentation O
challenge O
2017 B-DAT
entry. O
ICCV O
COCO O
Chal- O
lenge O

Workshop, O
2017 B-DAT

lutional O
neural O
networks O
for O
the O
2017 B-DAT
DAVIS O
challenge O
on O
video O
object O

segmentation. O
The O
2017 B-DAT
DAVIS O
Challenge O
on O
Video O
Object O

Segmentation O
- O
CVPR O
Workshops, O
2017 B-DAT

video O
object O
segmentation. O
In O
BMVC, O
2017 B-DAT

40,943 O
18 O
141,442 O
5,000 O
5,000 O
FB15K B-DAT
14,951 O
1,345 O
483,142 O
50,000 O
59,071 O

in O
each O
split O
for O
the O
FB15K B-DAT
and O
WN18 O
data O
sets O

5.3 O
Real O
Sparse O
Data O
Sets: O
FB15K B-DAT
and O
WN18 O

we O
evaluated O
ComplEx O
on O
the O
FB15K B-DAT
and O
WN18 O
data O
sets, O
as O

for O
the O
link O
prediction O
task. O
FB15K B-DAT
is O
a O
subset O
of O
Freebase O

max-margin O
ranking O
loss, O
especially O
on O
FB15K B-DAT

WN18 O
FB15K B-DAT
MRR O
Hits O
at O
MRR O
Hits O

the O
models O
tested O
on O
the O
FB15K B-DAT
and O
WN18 O
data O
sets. O
Hits@N O

Shimbo, O
2017), O
score O
divergence O
on O
FB15K B-DAT
is O
only O
due O
to O
the O

1), O
and O
three O
hours O
on O
FB15K B-DAT
(K O
= O
200, O
η O

On O
FB15K, B-DAT
the O
gap O
is O
much O
more O

not O
so O
many O
parameters. O
On O
FB15K B-DAT
though, O
it O
probably O
overfits O
due O

on O
the O
filtered O
MRR O
on O
FB15K B-DAT
(up O
to O
+0.08 O
improvement O
from O

to O
be O
very O
important O
on O
FB15K, B-DAT
while O
not O
so O

MRR O
for O
ComplEx O
on O
the O
FB15K B-DAT
and O
WN18 O
data O
sets O
for O

matrices O
and O
not O
entities. O
On O
FB15K, B-DAT
the O
difference O
is O
much O
more O

time, O
on O
WN18 O
(top) O
and O
FB15K B-DAT
(bottom) O
for O
each O
model O
for O

RESCAL O
on O
WN18, O
TransE O
on O
FB15K B-DAT

four O
days O
to O
train O
on O
FB15K, B-DAT
whereas O
other O
models O
took O
between O

on O
larger O
data O
sets, O
as O
FB15K B-DAT
is O
but O
a O
small O
subset O

training O
time O
to O
convergence O
on O
FB15K B-DAT
for O
the O
ComplEx O
model O
withK O

leads O
to O
better O
results O
on O
FB15K B-DAT

the O
performance O
of O
ComplEx O
on O
FB15K B-DAT

and O
achieves O
good O
performance O
on O
FB15K B-DAT
and O
WN18 O
data O
sets. O
However O

5.3 O
Real O
Sparse O
Data O
Sets: O
FB15K B-DAT
and O
WN18 O

237) B-DAT
dataset O

237 B-DAT
(Toutanova O
et O
al., O
2015), O
NELL-995 O

237 B-DAT
were O
created O
to O
resolve O
the O

93,003 O
2.12 O
1 O
FB15k-237 B-DAT
14,541 O
237 O
272,115 O
17,535 O
20,466 O
310,116 O
18.71 O

237 B-DAT
Hits@N O
Hits@N O

237 B-DAT
test O
sets. O
Hits@N O
values O
are O

237, B-DAT
and O
on O
two O
metrics O
for O

237 B-DAT

237 B-DAT
dataset. O
Y-axis O
represents O
attention O
values O

237 B-DAT
6.87 O
0.237 O
UMLS O
740 O
0.247 O

on O
the O
popular O
Free- O
base O
(FB15K-237) B-DAT
dataset O

WN18RR O
FB15K-237 B-DAT
Hits@N O
Hits@N O

Experimental O
results O
on O
WN18RR O
and O
FB15K-237 B-DAT
test O
sets. O
Hits@N O
values O
are O

process O
of O
our O
model O
on O
FB15K-237 B-DAT
dataset. O
Y-axis O
represents O
attention O
values O

WN18RR O
(Dettmers O
et O
al., O
2018), O
FB15k B-DAT

ing O
subset O
datasets O
WN18RR O
and O
FB15k B-DAT

3034 O
3134 O
93,003 O
2.12 O
1 O
FB15k B-DAT

results O
on O
five O
metrics O
for O
FB15k B-DAT

5 O
shows O
this O
distribution O
on O
FB15k B-DAT

1.32 O
0.025 O
WN18RR O
2.44 O
-0.01 O
FB15k B-DAT

WN18RR O
(Dettmers O
et O
al., O
2018), O
FB15k-237 B-DAT
(Toutanova O
et O
al., O
2015), O
NELL-995 O

ing O
subset O
datasets O
WN18RR O
and O
FB15k-237 B-DAT
were O
created O
to O
resolve O
the O

3034 O
3134 O
93,003 O
2.12 O
1 O
FB15k-237 B-DAT
14,541 O
237 O
272,115 O
17,535 O
20,466 O

results O
on O
five O
metrics O
for O
FB15k-237, B-DAT
and O
on O
two O
metrics O
for O

5 O
shows O
this O
distribution O
on O
FB15k-237 B-DAT

1.32 O
0.025 O
WN18RR O
2.44 O
-0.01 O
FB15k-237 B-DAT
6.87 O
0.237 O
UMLS O
740 O
0.247 O

FB15K-237 B-DAT
15k O
237 O
272k O
18k O
20k O
YAGO3-10 O
123k O

237 B-DAT
and O
WN18RR. O
These O
datasets O
are O

237 B-DAT
YAGO3-10 O

237, B-DAT
WN18, O
WN18RR O
datasets, O
with O
a O

237, B-DAT
WN18RR O
and O
YAGO3-10 O
may O
be O

237 B-DAT
and O
YAGO3-10. O
We O
checked O
on O

237 B-DAT
MRR O
0.32 O
Dettmers O
et O
al O

FB15K-237 B-DAT
15k O
237 O
272k O
18k O
20k O

created O
two O
mod- O
ified O
datasets: O
FB15K-237 B-DAT
and O
WN18RR. O
These O
datasets O
are O

Model O
WN18 O
WN18RR O
FB15K O
FB15K-237 B-DAT
YAGO3-10 O

and O
regularizers O
on O
the O
FB15K, O
FB15K-237, B-DAT
WN18, O
WN18RR O
datasets, O
with O
a O

The O
performances O
of O
DistMult O
on O
FB15K-237, B-DAT
WN18RR O
and O
YAGO3-10 O
may O
be O

on O
the O
harder O
datasets O
WN18RR, O
FB15K-237 B-DAT
and O
YAGO3-10. O
We O
checked O
on O

FB15K-237 B-DAT
MRR O
0.32 O
Dettmers O
et O
al O

237 B-DAT
(Toutanova O
et O
al., O
2015) O
was O

237, B-DAT
we O
set O
entity O
and O
relation O

compared O
to O
FB15k O
and O
FB15k- O
237, B-DAT
we O
set O
de O
= O
200 O

237 B-DAT

FB15k, O
(0.0005, O
1.0) O
for O
FB15k- O
237, B-DAT
(0.005, O
0.995) O
for O
WN18 O
and O

FB15k O
14,951 O
1,345 O
FB15k-237 B-DAT
14,541 O
237 O
WN18 O
40,943 O
18 O
WN18RR O
40,943 O

237), B-DAT
making O
their O
results O
incomparable O
to O

237 B-DAT

237 B-DAT
HypER O
(Balažević O
et O
al., O
2019 O

237 B-DAT

237 B-DAT
with O
embedding O
sizes O
de O

237 B-DAT

237 B-DAT
0.0005 O
1.0 O
200 O
200 O
0.3 O

237, B-DAT
used O
to O
produce O
the O
result O

237 B-DAT

diction O
datasets O
(see O
Table O
2): O
FB15k B-DAT
(Bordes O
et O
al., O
2013) O
is O

database O
of O
real O
world O
facts. O
FB15k B-DAT

al., O
2015) O
was O
created O
from O
FB15k B-DAT
by O
removing O
the O
inverse O
of O

FB15k B-DAT
and O
FB15k O

ber O
of O
relations O
compared O
to O
FB15k B-DAT
and O
FB15k- O
237, O
we O
set O

0.4, O
0.5) O
are O
required O
for O
FB15k B-DAT
and O
FB15k-237. O
We O
choose O
the O

best O
results: O
(0.003, O
0.99) O
for O
FB15k, B-DAT
(0.0005, O
1.0) O
for O
FB15k- O
237 O

FB15k B-DAT
14,951 O
1,345 O
FB15k O

relations. O
For O
example, O
improvement O
on O
FB15k B-DAT
is O
+14% O
over O
ComplEx O
and O

TuckER O
on O
WN18RR; O
5.5x O
on O
FB15k B-DAT

WN18RR O
FB15k B-DAT

prediction O
results O
on O
WN18RR O
and O
FB15k B-DAT

WN18 O
FB15k B-DAT

prediction O
results O
on O
WN18 O
and O
FB15k B-DAT

train O
all O
three O
models O
on O
FB15k B-DAT

for O
different O
embeddings O
sizes O
on O
FB15k B-DAT

FB15k B-DAT
0.003 O
0.99 O
200 O
200 O
0.2 O

0.2 O
0.3 O
0. O
FB15k B-DAT

for O
ComplEx O
and O
SimplE O
on O
FB15k B-DAT

for O
ComplEx O
and O
SimplE O
on O
FB15k B-DAT

database O
of O
real O
world O
facts. O
FB15k-237 B-DAT
(Toutanova O
et O
al., O
2015) O
was O

FB15k O
and O
FB15k-237, B-DAT
we O
set O
entity O
and O
relation O

are O
required O
for O
FB15k O
and O
FB15k-237 B-DAT

FB15k O
14,951 O
1,345 O
FB15k-237 B-DAT
14,541 O
237 O
WN18 O
40,943 O
18 O

TuckER O
on O
WN18RR; O
5.5x O
on O
FB15k-237), B-DAT
making O
their O
results O
incomparable O
to O

WN18RR O
FB15k-237 B-DAT

prediction O
results O
on O
WN18RR O
and O
FB15k-237 B-DAT

train O
all O
three O
models O
on O
FB15k-237 B-DAT
with O
embedding O
sizes O
de O

for O
different O
embeddings O
sizes O
on O
FB15k-237 B-DAT

200 O
0.2 O
0.2 O
0.3 O
0. O
FB15k-237 B-DAT
0.0005 O
1.0 O
200 O
200 O
0.3 O

for O
ComplEx O
and O
SimplE O
on O
FB15k-237, B-DAT
used O
to O
produce O
the O
result O

for O
ComplEx O
and O
SimplE O
on O
FB15k-237 B-DAT

237, B-DAT
WN18RR, O
FB15k, O
WN18, O
YAGO3-10), O
across O

237 B-DAT
dataset, O
which O
determines O
ne O
and O

237 B-DAT

237 B-DAT
created O
by O
[21], O
noting O
that O

237 B-DAT
is O
a O
subset O
of O
FB15k O

WN18 O
40,943 O
18 O
FB15k-237 B-DAT
14,541 O
237 O
WN18RR O
40,943 O
11 O
YAGO3-10 O
123,182 O

237, B-DAT
where O
we O
set O
input O
dropout O

237 B-DAT
takes O
approximately O
12 O
seconds O
on O

237, B-DAT
WN18, O
and O
YAGO3-10. O
Given O
that O

237 B-DAT

237 B-DAT

237 B-DAT
M-Walk O
[16] O
− O
.437 O

237 B-DAT

237 B-DAT

237 B-DAT
with O
the O
hypernetwork O
component O
removed O

237, B-DAT
from O
which O
they O
deem O
label O

237 B-DAT

mod- O
els O
using O
standard O
datasets O
(FB15k B-DAT

-237, O
WN18RR, O
FB15k, B-DAT
WN18, O
YAGO3-10), O
across O
which O
it O

ConvE O
and O
HypER O
(for O
the O
FB15k B-DAT

for O
ConvE O
and O
HypER O
on O
FB15k B-DAT

FB15k B-DAT
[1] O
a O
subset O
of O
Freebase O

FB15k B-DAT

validation O
and O
test O
sets O
of O
FB15k B-DAT
and O
WN18 O
contain O
the O
inverse O

simple O
models O
to O
do O
well. O
FB15k B-DAT

-237 O
is O
a O
subset O
of O
FB15k B-DAT
with O
the O
inverse O
relations O
removed O

FB15k B-DAT
14,951 O
1,345 O
WN18 O
40,943 O
18 O

FB15k B-DAT

hidden O
dropout O
0.3, O
apart O
from O
FB15k B-DAT

size O
128. O
One O
epoch O
on O
FB15k B-DAT

and O
mean O
rank O
on O
WN18RR, O
FB15k B-DAT

prediction O
results O
on O
WN18RR O
and O
FB15k B-DAT

WN18RR O
FB15k B-DAT

prediction O
results O
on O
WN18 O
and O
FB15k B-DAT

WN18 O
FB15k B-DAT

without O
hypernetwork O
on O
WN18RR O
and O
FB15k B-DAT

WN18RR O
FB15k B-DAT

HypER O
results O
on O
WN18RR O
and O
FB15k B-DAT

on O
mean O
reciprocal O
rank O
for O
FB15k B-DAT

a O
negative O
influence O
on O
the O
FB15k B-DAT
scores O
and O
as O
such, O
exclude O

WN18RR O
FB15k B-DAT

mod- O
els O
using O
standard O
datasets O
(FB15k-237, B-DAT
WN18RR, O
FB15k, O
WN18, O
YAGO3-10), O
across O

ConvE O
and O
HypER O
(for O
the O
FB15k-237 B-DAT
dataset, O
which O
determines O
ne O
and O

for O
ConvE O
and O
HypER O
on O
FB15k-237 B-DAT

FB15k-237 B-DAT
created O
by O
[21], O
noting O
that O

simple O
models O
to O
do O
well. O
FB15k-237 B-DAT
is O
a O
subset O
of O
FB15k O

14,951 O
1,345 O
WN18 O
40,943 O
18 O
FB15k-237 B-DAT
14,541 O
237 O
WN18RR O
40,943 O
11 O

hidden O
dropout O
0.3, O
apart O
from O
FB15k-237, B-DAT
where O
we O
set O
input O
dropout O

size O
128. O
One O
epoch O
on O
FB15k-237 B-DAT
takes O
approximately O
12 O
seconds O
on O

and O
mean O
rank O
on O
WN18RR, O
FB15k-237, B-DAT
WN18, O
and O
YAGO3-10. O
Given O
that O

prediction O
results O
on O
WN18RR O
and O
FB15k-237 B-DAT

WN18RR O
FB15k-237 B-DAT

without O
hypernetwork O
on O
WN18RR O
and O
FB15k-237 B-DAT

WN18RR O
FB15k-237 B-DAT

HypER O
results O
on O
WN18RR O
and O
FB15k-237 B-DAT
with O
the O
hypernetwork O
component O
removed O

on O
mean O
reciprocal O
rank O
for O
FB15k-237, B-DAT
from O
which O
they O
deem O
label O

WN18RR O
FB15k-237 B-DAT

237 B-DAT
with O
8x O
and O
17x O
fewer O

237 B-DAT
– O
a O
subset O
of O
FB15k O

237, B-DAT
we O
also O
find O
flaws O
with O

237, B-DAT
WN18RR, O
and O
YAGO3-10 O

237 B-DAT

237 B-DAT
does O
not O
remove O
certain O
symmetric O

237, B-DAT
we O
could O
not O
replicate O
the O

237 B-DAT
with O
0.23M O
parameters O
performs O
better O

237 B-DAT
with O
0.425 O
Hits@10. O
Comparing O
to O

237 B-DAT

237 B-DAT
Hits O
Hits O

237 B-DAT
Inverse O
Model O
13526 O
.35 O
.35 O

237 B-DAT
compared O
to O
WN18RR, O
is O
that O

237), B-DAT
but O
that O
shal- O
low O
models O

237 B-DAT
0.733 O
0.16 O
YAGO3-10 O
0.988 O
0.21 O

237 B-DAT

237 B-DAT

noted O
that O
the O
WN18 O
and O
FB15k B-DAT
datasets O
suffer O
from O
test O
set O

results O
on O
both O
WN18 O
and O
FB15k B-DAT

than O
DistMult O
and O
R-GCNs O
on O
FB15k B-DAT

a O
con- O
volution O
model O
on O
FB15k B-DAT
– O
one O
of O
the O
dataset O

FB15k B-DAT
(Bordes O
et O
al. O
2013a) O
is O

Chen O
(2015) O
that O
WN18 O
and O
FB15k B-DAT
suffer O
from O
test O
leakage O
through O

Toutanova O
and O
Chen O
(2015) O
introduced O
FB15k B-DAT

-237 O
– O
a O
subset O
of O
FB15k B-DAT
where O
inverse O
relations O
are O
removed O

results O
on O
both O
WN18 O
and O
FB15k B-DAT

to O
each O
dataset. O
Apart O
from O
FB15k, B-DAT
which O
was O
cor- O
rected O
by O

FB15k B-DAT

research, O
we O
recommend O
against O
using O
FB15k B-DAT
and O
WN18 O
and O
instead O
recommend O

FB15k B-DAT

well O
on O
WN18, O
YAGO3-10 O
and O
FB15k B-DAT

the O
mean O
reciprocal O
rank O
(WN18, O
FB15k, B-DAT
YAGO3-10) O
and O
AUC-PR O
(Countries) O
statistics O

training O
datasets O
of O
WN18 O
and O
FB15k B-DAT
have O
94% O
and O
81% O
test O

on O
the O
standard O
bench- O
marks O
FB15k B-DAT
and O
WN18 O
are O
shown O
in O

many O
different O
metrics O
for O
both O
FB15k B-DAT
and O
WN18. O
How- O
ever, O
it O

relations O
for O
YAGO3- O
10 O
and O
FB15k B-DAT

and O
Chen O
(2015) O
to O
derive O
FB15k B-DAT

YAGO3-10, O
for O
some O
metrics O
on O
FB15k, B-DAT
and O
it O
does O
well O
on O

For O
FB15k B-DAT

can O
see O
that O
ConvE O
for O
FB15k B-DAT

achieves O
state-of-the- O
art O
results O
on O
FB15k B-DAT

prediction O
results O
for O
WN18 O
and O
FB15k B-DAT

WN18 O
FB15k B-DAT
Hits O
Hits O

prediction O
results O
for O
WN18RR O
and O
FB15k B-DAT

WN18RR O
FB15k B-DAT

on O
datasets O
like O
YAGO3-10 O
and O
FB15k B-DAT

FB15k) B-DAT
relation-specific O
in- O
degree O
and O
reverse O

high O
(high-WN18) O
and O
low O
(low- O
FB15k) B-DAT
relation-specific O
indegree O
datasets O
by O
deleting O

FB15k B-DAT
we O
have O
ConvE O
0.586 O
Hits@10 O

model O
more O
complex O
graphs O
(e.g. O
FB15k B-DAT
and O
FB15k-237), O
but O
that O
shal O

0.104 O
0.06 O
WN18 O
0.125 O
0.45 O
FB15k B-DAT
0.599 O
0.04 O
FB15-237 O
0.733 O
0.16 O

Table O
7: O
Ablation O
study O
for O
FB15k B-DAT

the O
most O
central O
nodes O
in O
FB15k B-DAT

inverse O
relations O
of O
WN18 O
and O
FB15k B-DAT
was O
first O
reported O
by O
Toutanova O

state-of-the-art O
results O
on O
WN18 O
and O
FB15k B-DAT

WN18 O
and O
slightly O
worse O
for O
FB15k B-DAT

I O
was O
unable O
to O
replicate O
FB15k B-DAT
scores O
that O
I O
initially O
reported.4 O

thus O
far O
(Kinship, O
WN18, O
WN18RR, O
FB15k B-DAT

than O
DistMult O
and O
R-GCNs O
on O
FB15k-237 B-DAT
with O
8x O
and O
17x O
fewer O

Toutanova O
and O
Chen O
(2015) O
introduced O
FB15k-237 B-DAT
– O
a O
subset O
of O
FB15k O

which O
was O
cor- O
rected O
by O
FB15k-237, B-DAT
we O
also O
find O
flaws O
with O

and O
WN18 O
and O
instead O
recommend O
FB15k-237, B-DAT
WN18RR, O
and O
YAGO3-10 O

relations O
for O
YAGO3- O
10 O
and O
FB15k-237 B-DAT

and O
Chen O
(2015) O
to O
derive O
FB15k-237 B-DAT
does O
not O
remove O
certain O
symmetric O

For O
FB15k-237, B-DAT
we O
could O
not O
replicate O
the O

can O
see O
that O
ConvE O
for O
FB15k-237 B-DAT
with O
0.23M O
parameters O
performs O
better O

achieves O
state-of-the- O
art O
results O
on O
FB15k-237 B-DAT
with O
0.425 O
Hits@10. O
Comparing O
to O

prediction O
results O
for O
WN18RR O
and O
FB15k-237 B-DAT

WN18RR O
FB15k-237 B-DAT
Hits O
Hits O

on O
datasets O
like O
YAGO3-10 O
and O
FB15k-237 B-DAT
compared O
to O
WN18RR, O
is O
that O

complex O
graphs O
(e.g. O
FB15k O
and O
FB15k-237), B-DAT
but O
that O
shal- O
low O
models O

Table O
7: O
Ablation O
study O
for O
FB15k-237 B-DAT

thus O
far O
(Kinship, O
WN18, O
WN18RR, O
FB15k-237 B-DAT

instance, O
on O
the O
benchmark O
dataset O
FB15K B-DAT
(Bordes O
et O
al., O
2013), O
the O

largest O
datasets O
we O
experiment O
on, O
FB15K B-DAT
and O
YAGO3-10, O
contain O
respectively O
15k/1.3k O

non- O
symmetric O
datasets O
such O
as O
FB15K B-DAT
or O
WN18 O
are O
surprising. O
First O

such O
as O
capital_of O
. O
In O
FB15K, B-DAT
those O
type O
of O
problematic O
queries O

two O
minutes O
per O
epoch O
on O
FB15K, B-DAT
we O
decided O
to O
use O
the O

of O
the O
nuclear O
3-norm O
on O
FB15K B-DAT

41k O
11 O
87k O
3k O
3k O
FB15K B-DAT
15k O
1k O
500k O
50k O
60k O

FB15K B-DAT

WN18 O
and O
FB15K B-DAT
are O
popular O
benchmarks O
in O
the O

created O
two O
mod- O
ified O
datasets: O
FB15K B-DAT

Model O
WN18 O
WN18RR O
FB15K B-DAT
FB15K O

algorithms O
and O
regularizers O
on O
the O
FB15K, B-DAT
FB15K-237, O
WN18, O
WN18RR O
datasets, O
with O

ComplEx O
with O
batch-size O
100 O
on O
FB15K B-DAT
took O
about O
110s O
and O
325s O

of O
the O
good O
performances O
on O
FB15K B-DAT
of O
DistMult O
and O
the O
extensive O

experiments O
on O
WN18 O
and O
FB15K B-DAT
reported O
in O
Kadlec O
et O
al O

The O
performances O
of O
DistMult O
on O
FB15K B-DAT

to O
0.46 O
filtered O
MRR O
on O
FB15K B-DAT
for O
CP O
and O
0.70 O
to O

MRR O
per O
relation O
type O
on O
FB15K B-DAT

to O
0.86 O
filtered O
MRR O
on O
FB15K B-DAT

the O
biggest O
improvements O
observed O
on O
FB15K B-DAT
and O
YAGO3-10. O
Following O
the O
analysis O

on O
the O
harder O
datasets O
WN18RR, O
FB15K B-DAT

Effect O
of O
the O
batch-size O
on O
FB15K B-DAT
in O
the O
Standard O
(top) O
and O

the O
mean O
reciprocal O
rank O
on O
FB15K B-DAT
for O
CP) O
nor O
the O
impact O

FB15K B-DAT
MRR O
0.84 O
Kadlec O
et O
al O

FB15K B-DAT

diction O
datasets O
(see O
Table O
2): O
FB15k B-DAT
(Bordes O
et O
al., O
2013) O
is O

database O
of O
real O
world O
facts. O
FB15k B-DAT

al., O
2015) O
was O
created O
from O
FB15k B-DAT
by O
removing O
the O
inverse O
of O

FB15k B-DAT
and O
FB15k O

ber O
of O
relations O
compared O
to O
FB15k B-DAT
and O
FB15k- O
237, O
we O
set O

0.4, O
0.5) O
are O
required O
for O
FB15k B-DAT
and O
FB15k-237. O
We O
choose O
the O

best O
results: O
(0.003, O
0.99) O
for O
FB15k, B-DAT
(0.0005, O
1.0) O
for O
FB15k- O
237 O

FB15k B-DAT
14,951 O
1,345 O
FB15k O

relations. O
For O
example, O
improvement O
on O
FB15k B-DAT
is O
+14% O
over O
ComplEx O
and O

TuckER O
on O
WN18RR; O
5.5x O
on O
FB15k B-DAT

WN18RR O
FB15k B-DAT

prediction O
results O
on O
WN18RR O
and O
FB15k B-DAT

WN18 O
FB15k B-DAT

prediction O
results O
on O
WN18 O
and O
FB15k B-DAT

train O
all O
three O
models O
on O
FB15k B-DAT

for O
different O
embeddings O
sizes O
on O
FB15k B-DAT

FB15k B-DAT
0.003 O
0.99 O
200 O
200 O
0.2 O

0.2 O
0.3 O
0. O
FB15k B-DAT

for O
ComplEx O
and O
SimplE O
on O
FB15k B-DAT

for O
ComplEx O
and O
SimplE O
on O
FB15k B-DAT

mod- O
els O
using O
standard O
datasets O
(FB15k B-DAT

-237, O
WN18RR, O
FB15k, B-DAT
WN18, O
YAGO3-10), O
across O
which O
it O

ConvE O
and O
HypER O
(for O
the O
FB15k B-DAT

for O
ConvE O
and O
HypER O
on O
FB15k B-DAT

FB15k B-DAT
[1] O
a O
subset O
of O
Freebase O

FB15k B-DAT

validation O
and O
test O
sets O
of O
FB15k B-DAT
and O
WN18 O
contain O
the O
inverse O

simple O
models O
to O
do O
well. O
FB15k B-DAT

-237 O
is O
a O
subset O
of O
FB15k B-DAT
with O
the O
inverse O
relations O
removed O

FB15k B-DAT
14,951 O
1,345 O
WN18 O
40,943 O
18 O

FB15k B-DAT

hidden O
dropout O
0.3, O
apart O
from O
FB15k B-DAT

size O
128. O
One O
epoch O
on O
FB15k B-DAT

and O
mean O
rank O
on O
WN18RR, O
FB15k B-DAT

prediction O
results O
on O
WN18RR O
and O
FB15k B-DAT

WN18RR O
FB15k B-DAT

prediction O
results O
on O
WN18 O
and O
FB15k B-DAT

WN18 O
FB15k B-DAT

without O
hypernetwork O
on O
WN18RR O
and O
FB15k B-DAT

WN18RR O
FB15k B-DAT

HypER O
results O
on O
WN18RR O
and O
FB15k B-DAT

on O
mean O
reciprocal O
rank O
for O
FB15k B-DAT

a O
negative O
influence O
on O
the O
FB15k B-DAT
scores O
and O
as O
such, O
exclude O

WN18RR O
FB15k B-DAT

subset O
of O
Wordnet O
[24], O
and O
FB15k B-DAT
a O
subset O
of O
Freebase O
[2 O

and O
5, O
000 O
test O
triples. O
FB15k B-DAT
contains O
14, O
951 O
entities, O
1 O

1: O
Results O
on O
WN18 O
and O
FB15k B-DAT

WN18 O
FB15k B-DAT

WN18 O
to O
0.1 O
and O
for O
FB15k B-DAT
to O
0.05 O
and O
used O
adagrad O

examples O
per O
positive O
example O
in O
FB15k B-DAT

and O
every O
100 O
iterations O
for O
FB15k B-DAT
and O
selected O
the O
iteration O
that O

size O
and O
λ O
values O
on O
FB15k B-DAT
for O
SimplE-ignr O
were O
200 O
and O

state-of-the-art O
tensor O
factorization O
model. O
On O
FB15k, B-DAT
SimplE O
outperforms O
the O
existing O
baselines O

for O
the O
friendship O
relation O
in O
FB15k, B-DAT
if O
an O
entity O
e1 O
is O

for, O
e.g., O
netflix O
genre O
in O
FB15k B-DAT
and O
has O
part O
in O
WN18 O

40,943 O
18 O
141,442 O
5,000 O
5,000 O
FB15K B-DAT
14,951 O
1,345 O
483,142 O
50,000 O
59,071 O

in O
each O
split O
for O
the O
FB15K B-DAT
and O
WN18 O
data O
sets O

5.3 O
Real O
Sparse O
Data O
Sets: O
FB15K B-DAT
and O
WN18 O

we O
evaluated O
ComplEx O
on O
the O
FB15K B-DAT
and O
WN18 O
data O
sets, O
as O

for O
the O
link O
prediction O
task. O
FB15K B-DAT
is O
a O
subset O
of O
Freebase O

max-margin O
ranking O
loss, O
especially O
on O
FB15K B-DAT

WN18 O
FB15K B-DAT
MRR O
Hits O
at O
MRR O
Hits O

the O
models O
tested O
on O
the O
FB15K B-DAT
and O
WN18 O
data O
sets. O
Hits@N O

Shimbo, O
2017), O
score O
divergence O
on O
FB15K B-DAT
is O
only O
due O
to O
the O

1), O
and O
three O
hours O
on O
FB15K B-DAT
(K O
= O
200, O
η O

On O
FB15K, B-DAT
the O
gap O
is O
much O
more O

not O
so O
many O
parameters. O
On O
FB15K B-DAT
though, O
it O
probably O
overfits O
due O

on O
the O
filtered O
MRR O
on O
FB15K B-DAT
(up O
to O
+0.08 O
improvement O
from O

to O
be O
very O
important O
on O
FB15K, B-DAT
while O
not O
so O

MRR O
for O
ComplEx O
on O
the O
FB15K B-DAT
and O
WN18 O
data O
sets O
for O

matrices O
and O
not O
entities. O
On O
FB15K, B-DAT
the O
difference O
is O
much O
more O

time, O
on O
WN18 O
(top) O
and O
FB15K B-DAT
(bottom) O
for O
each O
model O
for O

RESCAL O
on O
WN18, O
TransE O
on O
FB15K B-DAT

four O
days O
to O
train O
on O
FB15K, B-DAT
whereas O
other O
models O
took O
between O

on O
larger O
data O
sets, O
as O
FB15K B-DAT
is O
but O
a O
small O
subset O

training O
time O
to O
convergence O
on O
FB15K B-DAT
for O
the O
ComplEx O
model O
withK O

leads O
to O
better O
results O
on O
FB15K B-DAT

the O
performance O
of O
ComplEx O
on O
FB15K B-DAT

and O
achieves O
good O
performance O
on O
FB15K B-DAT
and O
WN18 O
data O
sets. O
However O

5.3 O
Real O
Sparse O
Data O
Sets: O
FB15K B-DAT
and O
WN18 O

noted O
that O
the O
WN18 O
and O
FB15k B-DAT
datasets O
suffer O
from O
test O
set O

results O
on O
both O
WN18 O
and O
FB15k B-DAT

than O
DistMult O
and O
R-GCNs O
on O
FB15k B-DAT

a O
con- O
volution O
model O
on O
FB15k B-DAT
– O
one O
of O
the O
dataset O

FB15k B-DAT
(Bordes O
et O
al. O
2013a) O
is O

Chen O
(2015) O
that O
WN18 O
and O
FB15k B-DAT
suffer O
from O
test O
leakage O
through O

Toutanova O
and O
Chen O
(2015) O
introduced O
FB15k B-DAT

-237 O
– O
a O
subset O
of O
FB15k B-DAT
where O
inverse O
relations O
are O
removed O

results O
on O
both O
WN18 O
and O
FB15k B-DAT

to O
each O
dataset. O
Apart O
from O
FB15k, B-DAT
which O
was O
cor- O
rected O
by O

FB15k B-DAT

research, O
we O
recommend O
against O
using O
FB15k B-DAT
and O
WN18 O
and O
instead O
recommend O

FB15k B-DAT

well O
on O
WN18, O
YAGO3-10 O
and O
FB15k B-DAT

the O
mean O
reciprocal O
rank O
(WN18, O
FB15k, B-DAT
YAGO3-10) O
and O
AUC-PR O
(Countries) O
statistics O

training O
datasets O
of O
WN18 O
and O
FB15k B-DAT
have O
94% O
and O
81% O
test O

on O
the O
standard O
bench- O
marks O
FB15k B-DAT
and O
WN18 O
are O
shown O
in O

many O
different O
metrics O
for O
both O
FB15k B-DAT
and O
WN18. O
How- O
ever, O
it O

relations O
for O
YAGO3- O
10 O
and O
FB15k B-DAT

and O
Chen O
(2015) O
to O
derive O
FB15k B-DAT

YAGO3-10, O
for O
some O
metrics O
on O
FB15k, B-DAT
and O
it O
does O
well O
on O

For O
FB15k B-DAT

can O
see O
that O
ConvE O
for O
FB15k B-DAT

achieves O
state-of-the- O
art O
results O
on O
FB15k B-DAT

prediction O
results O
for O
WN18 O
and O
FB15k B-DAT

WN18 O
FB15k B-DAT
Hits O
Hits O

prediction O
results O
for O
WN18RR O
and O
FB15k B-DAT

WN18RR O
FB15k B-DAT

on O
datasets O
like O
YAGO3-10 O
and O
FB15k B-DAT

FB15k) B-DAT
relation-specific O
in- O
degree O
and O
reverse O

high O
(high-WN18) O
and O
low O
(low- O
FB15k) B-DAT
relation-specific O
indegree O
datasets O
by O
deleting O

FB15k B-DAT
we O
have O
ConvE O
0.586 O
Hits@10 O

model O
more O
complex O
graphs O
(e.g. O
FB15k B-DAT
and O
FB15k-237), O
but O
that O
shal O

0.104 O
0.06 O
WN18 O
0.125 O
0.45 O
FB15k B-DAT
0.599 O
0.04 O
FB15-237 O
0.733 O
0.16 O

Table O
7: O
Ablation O
study O
for O
FB15k B-DAT

the O
most O
central O
nodes O
in O
FB15k B-DAT

inverse O
relations O
of O
WN18 O
and O
FB15k B-DAT
was O
first O
reported O
by O
Toutanova O

state-of-the-art O
results O
on O
WN18 O
and O
FB15k B-DAT

WN18 O
and O
slightly O
worse O
for O
FB15k B-DAT

I O
was O
unable O
to O
replicate O
FB15k B-DAT
scores O
that O
I O
initially O
reported.4 O

thus O
far O
(Kinship, O
WN18, O
WN18RR, O
FB15k B-DAT

2] O
and O
also O
the O
2018 O
YouTube B-DAT

set O
(60 O
videos) O
and O
the O
YouTube B-DAT

and O
test-dev O
sets, O
and O
the O
YouTube B-DAT

FEELVOS O
(-YTB-VOS) O
denotes O
training O
without O
YouTube B-DAT

also O
contains O
30 O
sequences. O
The O
YouTube B-DAT

5. O
Quantitative O
results O
on O
the O
YouTube B-DAT

The O
used O
evaluation O
protocol O
for O
YouTube B-DAT

2.4% O
higher O
when O
not O
using O
YouTube B-DAT

5 O
shows O
the O
results O
on O
YouTube B-DAT

2017 O
validation O
set O
and O
on O
YouTube B-DAT

2017 O
validation O
set O
and O
the O
YouTube B-DAT

refinement O
and O
merging O
for O
the O
YouTube B-DAT

S. O
Cohen, O
and O
T. O
Huang. O
YouTube B-DAT

the O
large-scale O
and O
challenging O
NIST O
IJB-A B-DAT
un- O
constrained O
face O
recognition O
benchmark O

of O
our O
submissions O
to O
NIST O
IJB-A B-DAT
2017 O
face O
recognition O
competitions, O
where O

available O
face O
recognition O
datasets O
(e.g., O
IJB-A B-DAT
(15)) O
is O
usually O
unbalanced O
and O

of O
pose O
distribution O
in O
the O
IJB-A B-DAT
(15) O
dataset O
w/o O
and O
w O

NIST) O
IARPA O
Janus O
Benchmark O
A O
(IJB-A) B-DAT
(15) O
unconstrained O
face O
recognition O
benchmark O

identification O
tracks O
in O
the O
NIST O
IJB-A B-DAT
2017 O
face O
recognition O
competitions. O
This O

identification O
tracks O
in O
the O
NIST O
IJB-A B-DAT
2017 O
face O
recognition O
competitions O

unconstrained O
face O
recognition O
benchmark O
dataset O
IJB-A B-DAT
(15). O
IJB-A O
(15) O
contains O
both O

of O
DA-GAN O
with O
state-of-the-arts O
on O
IJB-A B-DAT
(15) O
verification O
protocol. O
For O
all O

i.e., O
verification O
and O
identification) O
on O
IJB-A B-DAT
(15) O
benchmark O
dataset O
with O
three O

baselines O
and O
other O
state-of-the-arts O
on O
IJB-A B-DAT
(15) O
unconstrained O
face O
verification O
and O

and O
identification O
tracks O
in O
NIST O
IJB-A B-DAT
2017 O
face O
recognition O
competitions3. O
This O

and O
identification O
protocols O
to O
NIST O
IJB-A B-DAT
2017 O
face O
recognition O
competition O
committee O

of O
DA-GAN O
with O
state-of-the-arts O
on O
IJB-A B-DAT
(15) O
identification O
protocol. O
For O
FNIR O

identification O
closed O
set O
results O
for O
IJB-A B-DAT
(15) O
split1 O
to O
gain O
insights O

the O
large-scale O
and O
challenging O
NIST O
IJB-A B-DAT
unconstrained O
face O
recognition O
benchmark O
without O

and O
identification O
tracks O
in O
NIST O
IJB-A B-DAT
2017 O
face O
recognition O
competitions. O
It O

on O
26th, O
Apirl, O
2017. O
The O
IJB-A B-DAT
benchmark O
dataset, O
relevant O
information O
and O

the O
large-scale O
and O
challenging O
NIST O
IJB B-DAT

of O
our O
submissions O
to O
NIST O
IJB B-DAT

available O
face O
recognition O
datasets O
(e.g., O
IJB B-DAT

of O
pose O
distribution O
in O
the O
IJB B-DAT

NIST) O
IARPA O
Janus O
Benchmark O
A O
(IJB B-DAT

identification O
tracks O
in O
the O
NIST O
IJB B-DAT

performance O
on O
the O
challenging O
NIST O
IJB B-DAT

identification O
tracks O
in O
the O
NIST O
IJB B-DAT

unconstrained O
face O
recognition O
benchmark O
dataset O
IJB B-DAT

-A O
(15). O
IJB B-DAT

of O
DA-GAN O
with O
state-of-the-arts O
on O
IJB B-DAT

i.e., O
verification O
and O
identification) O
on O
IJB B-DAT

baselines O
and O
other O
state-of-the-arts O
on O
IJB B-DAT

and O
identification O
tracks O
in O
NIST O
IJB B-DAT

and O
identification O
protocols O
to O
NIST O
IJB B-DAT

of O
DA-GAN O
with O
state-of-the-arts O
on O
IJB B-DAT

identification O
closed O
set O
results O
for O
IJB B-DAT

the O
large-scale O
and O
challenging O
NIST O
IJB B-DAT

and O
identification O
tracks O
in O
NIST O
IJB B-DAT

on O
26th, O
Apirl, O
2017. O
The O
IJB B-DAT

A B-DAT
un- O
constrained O
face O
recognition O
benchmark O

A B-DAT
2017 O
face O
recognition O
competitions, O
where O

A B-DAT
(15)) O
is O
usually O
unbalanced O
and O

A B-DAT
(15) O
dataset O
w/o O
and O
w O

Technology O
(NIST) O
IARPA B-DAT
Janus O
Benchmark O
A O
(IJB-A) O
(15) O
unconstrained O
face O
recognition O

A B-DAT
2017 O
face O
recognition O
competitions. O
This O

on O
the O
challenging O
NIST O
IJB- O
A B-DAT
(15) O
unconstrained O
face O
recognition O
benchmark O

A B-DAT
2017 O
face O
recognition O
competitions O

A B-DAT
(15). O
IJB-A O
(15) O
contains O
both O

A B-DAT
(15) O
verification O
protocol. O
For O
all O

A B-DAT
(15) O
benchmark O
dataset O
with O
three O

A B-DAT
(15) O
unconstrained O
face O
verification O
and O

A B-DAT
2017 O
face O
recognition O
competitions3. O
This O

A B-DAT
2017 O
face O
recognition O
competition O
committee O

A B-DAT
(15) O
identification O
protocol. O
For O
FNIR O

A B-DAT
(15) O
split1 O
to O
gain O
insights O

A B-DAT
unconstrained O
face O
recognition O
benchmark O
without O

A B-DAT
2017 O
face O
recognition O
competitions. O
It O

A B-DAT
benchmark O
dataset, O
relevant O
information O
and O

4] O
J.-C. O
Chen, O
R. O
Ranjan, O
A B-DAT

Chollet. O
keras. O
https://github.com/fchollet/keras, O
2015. O
[7] O
A B-DAT

C. O
Stauffer, O
Q. O
Cao, O
and O
A B-DAT

Xu, O
D. O
Warde-Farley, O
S. O
Ozair, O
A B-DAT

Klare, O
B. O
Klein, O
E. O
Taborsky, O
A B-DAT

Cheney, O
K. O
Allen, B-DAT
P. O
Grother, O
A O

A B-DAT

20] O
I. O
Masi, O
A B-DAT

arXiv O
preprint O
arXiv:1411.1784, O
2014. O
[22] O
A B-DAT

2016. O
[23] O
O. O
M. O
Parkhi, O
A B-DAT

. O
Vedaldi, O
and O
A B-DAT

arXiv:1401.4082, O
2014. O
[27] O
S. O
Sankaranarayanan, O
A B-DAT

28] O
A B-DAT

D. O
Wang, O
C. O
Otto, O
and O
A B-DAT

H. O
Lai, O
S. O
Yan, O
and O
A B-DAT

Liu, O
X. O
Nie, O
J. O
Feng, O
A B-DAT

. O
A B-DAT

. O
Kassim, O
and O
S. O
Yan. O
A B-DAT
live O
face O
swapper. O
In O
Proceedings O

margin O
on O
the O
pub- O
lic O
IJB-A B-DAT
[20] O
and O
IJB-B O
[37] O
face O

challenging O
public O
face O
recognition O
datasets O
IJB-A B-DAT
[20] O
and O
IJB-B O
[37] O
are O

as O
[5, O
10, O
18, O
25], O
IJB-A B-DAT
and O
IJB-B O
are O
intended O
for O

consider O
in O
this O
work. O
The O
IJB-A B-DAT
dataset O
contains O
5,712 O
images O
and O

dataset O
is O
an O
extension O
of O
IJB-A B-DAT
with O
a O
total O
of O
11,754 O

the O
standard O
benchmark O
procedure O
for O
IJB-A B-DAT
and O
IJB-B, O
and O
evaluate O
on O

apart O
from O
the O
fact O
that O
IJB-A B-DAT
defines O
10 O
test O
splits, O
while O

two O
galleries O
for O
identification. O
For O
IJB-A B-DAT
and O
for O
IJB-B O
iden- O
tification O

Network O
deployment. O
In O
the O
IJB-A B-DAT
and O
IJB-B O
datasets, O
there O
are O

against O
the O
state-of-the-art O
on O
the O
IJB-A B-DAT
and O
IJB-B O
datasets. O
The O
currently O

identification O
and O
verification O
on O
both O
IJB-A B-DAT
and O
IJB-B O
datasets. O
In O
particular O

surpasses O
[44] O
marginally O
on O
the O
IJB-A B-DAT
verification O
task, O
despite O
the O
fact O

to O
Rank-10 O
for O
identification O
on O
IJB-A B-DAT

; O
but O
this O
is O
because O
IJB-A B-DAT
is O
not O
challenging O
enough O
and O

IJB-A B-DAT

state-of-the-art O
for O
verification O
on O
the O
IJB-A B-DAT
and O
IJB-B O
datasets. O
A O
higher O

IJB-A B-DAT

state-of-the-art O
for O
identification O
on O
the O
IJB-A B-DAT
and O
IJB-B O
datasets. O
A O
higher O

the O
state-of-the-art O
on O
the O
challenging O
IJB-A B-DAT
and O
IJB-B O
benchmarks O
by O
a O

exceeds O
the O
state-of-the-art O
on O
the O
IJB B-DAT

margin O
on O
the O
pub- O
lic O
IJB B-DAT

-A O
[20] O
and O
IJB B-DAT

challenging O
public O
face O
recognition O
datasets O
IJB B-DAT

-A O
[20] O
and O
IJB B-DAT

as O
[5, O
10, O
18, O
25], O
IJB B-DAT

-A O
and O
IJB B-DAT

consider O
in O
this O
work. O
The O
IJB B-DAT

and O
4.2 O
videos, O
respectively. O
The O
IJB B-DAT

dataset O
is O
an O
extension O
of O
IJB B-DAT

the O
standard O
benchmark O
procedure O
for O
IJB B-DAT

-A O
and O
IJB B-DAT

apart O
from O
the O
fact O
that O
IJB B-DAT

defines O
10 O
test O
splits, O
while O
IJB B-DAT

two O
galleries O
for O
identification. O
For O
IJB B-DAT

-A O
and O
for O
IJB B-DAT

Network O
deployment. O
In O
the O
IJB B-DAT

-A O
and O
IJB B-DAT

4.3 O
Ablation O
studies O
on O
IJB B-DAT

it O
to O
baselines O
on O
the O
IJB B-DAT

larger O
and O
more O
challenging O
than O
IJB B-DAT

1: O
Verification O
performance O
on O
the O
IJB B-DAT

2: O
Identification O
performance O
on O
the O
IJB B-DAT

against O
the O
state-of-the-art O
on O
the O
IJB B-DAT

-A O
and O
IJB B-DAT

identification O
and O
verification O
on O
both O
IJB B-DAT

-A O
and O
IJB B-DAT

surpasses O
[44] O
marginally O
on O
the O
IJB B-DAT

to O
Rank-10 O
for O
identification O
on O
IJB B-DAT

-A; O
but O
this O
is O
because O
IJB B-DAT

measures O
on O
the O
more O
challenging O
IJB B-DAT

IJB B-DAT

IJB B-DAT

state-of-the-art O
for O
verification O
on O
the O
IJB B-DAT

-A O
and O
IJB B-DAT

vs O
0.705 O
for O
verification O
on O
IJB B-DAT

0.743 O
for O
iden- O
tification O
on O
IJB B-DAT

vs O
0.671 O
for O
verification O
on O
IJB B-DAT

vs O
0.706 O
for O
verification O
on O
IJB B-DAT

IJB B-DAT

IJB B-DAT

state-of-the-art O
for O
identification O
on O
the O
IJB B-DAT

-A O
and O
IJB B-DAT

Fig. O
3: O
Results O
on O
the O
IJB B-DAT

the O
state-of-the-art O
on O
the O
challenging O
IJB B-DAT

-A O
and O
IJB B-DAT

a O
tem- O
plate O
in O
the O
IJB B-DAT

A B-DAT
straightforward O
method O
to O
tackle O
multiple O

Y. O
Zhong, O
R. O
Arandjelović B-DAT
and O
A O

A B-DAT
[20] O
and O
IJB-B O
[37] O
face O

A B-DAT
few O
methods O
go O
beyond O
simple O

Y. O
Zhong, O
R. O
Arandjelović B-DAT
and O
A O

Feature O
extraction. O
A B-DAT
neural O
network O
is O
used O
to O

Y. O
Zhong, O
R. O
Arandjelović B-DAT
and O
A O

Y. O
Zhong, O
R. O
Arandjelović B-DAT
and O
A O

A B-DAT
[20] O
and O
IJB-B O
[37] O
are O

A B-DAT
and O
IJB-B O
are O
intended O
for O

A B-DAT
dataset O
contains O
5,712 O
images O
and O

A B-DAT
with O
a O
total O
of O
11,754 O

A B-DAT
and O
IJB-B, O
and O
evaluate O
on O

A B-DAT
defines O
10 O
test O
splits, O
while O

A B-DAT
and O
for O
IJB-B O
iden- O
tification O

A B-DAT
and O
IJB-B O
datasets, O
there O
are O

Y. O
Zhong, O
R. O
Arandjelović B-DAT
and O
A O

and O
more O
challenging O
than O
IJB- O
A B-DAT

performance O
on O
the O
IJB-B O
dataset. O
A B-DAT
higher O
value O
of O
TAR O
is O

performance O
on O
the O
IJB-B O
dataset. O
A B-DAT
higher O
value O
of O
TPIR O
is O

Y. O
Zhong, O
R. O
Arandjelović B-DAT
and O
A O

A B-DAT
and O
IJB-B O
datasets. O
The O
currently O

A B-DAT
and O
IJB-B O
datasets. O
In O
particular O

A B-DAT
verification O
task, O
despite O
the O
fact O

A B-DAT

A B-DAT
is O
not O
challenging O
enough O
and O

A B-DAT

the O
IJB-A B-DAT
and O
IJB-B O
datasets. O
A O
higher O
value O
of O
TAR O
is O

Y. O
Zhong, O
R. O
Arandjelović B-DAT
and O
A O

A B-DAT

the O
IJB-A B-DAT
and O
IJB-B O
datasets. O
A O
higher O
value O
of O
TPIR O
is O

A B-DAT
cc O

A B-DAT
and O
IJB-B O
benchmarks O
by O
a O

Arandjelović, B-DAT
R., O
Gronat, O
P., O
Torii, O
A O

3] O
Arandjelović, B-DAT
R., O
Zisserman, O
A O

Xie, O
W., O
Parkhi, O
O.M., O
Zisserman, O
A B-DAT

.: O
VGGFace2: O
A B-DAT
dataset O
for O
recognising O
faces O
across O

Chen, O
J., O
Ranjan, O
R., O
Kumar, O
A B-DAT

Parkhi, O
O.M., O
Cao, O
Q., O
Zisserman, O
A B-DAT

He, O
X., O
Gao, O
J.: O
MS-Celeb-1M: O
A B-DAT
dataset O
and O
benchmark O
for O
large-scale O

17] O
Jégou, O
H., O
Zisserman, O
A B-DAT

Klein, O
B., O
Taborsky, O
E., O
Blanton, O
A B-DAT

Allen, B-DAT
K., O
Grother, O
P., O
Mah, O
A O

., O
Jain, O
A B-DAT

A B-DAT

Parkhi, O
O.M., O
Simonyan, O
K., O
Vedaldi, O
A B-DAT

., O
Zisserman, O
A.: B-DAT
A O
compact O
and O
discriminative O
face O
track O

25] O
Parkhi, O
O.M., O
Vedaldi, O
A B-DAT

., O
Zisserman, O
A B-DAT

Ma, O
S., O
Huang, O
S., O
Karpathy, O
A B-DAT

., O
Khosla, O
A B-DAT

., O
Bernstein, O
M., O
Berg, O
A B-DAT

Kalenichenko, O
D., O
Philbin, O
J.: O
Facenet: O
A B-DAT
unified O
embedding O
for O
face O
recognition O

32] O
Turaga, O
P., O
Veeraraghavan, O
A B-DAT

., O
Srivastava, O
A B-DAT

33] O
Vedaldi, O
A B-DAT

Y. O
Zhong, O
R. O
Arandjelović B-DAT
and O
A O

Whitelam, O
C., O
Taborsky, O
E., O
Blanton, O
A B-DAT

Miller, O
T., O
Kalka, O
N., O
Jain, O
A B-DAT

Xie, O
W., O
Shen, O
L., O
Zisserman, O
A B-DAT

39] O
Xie, O
W., O
Zisserman, O
A B-DAT

Zhang, O
R., O
Isola, O
P., O
Efros, O
A B-DAT

A B-DAT

state-of-the-art O
results O
on O
the O
challenging O
IJB-A B-DAT
dataset, O
achieving O
True O
Accept O
Rate O

algorithms O
on O
the O
publicly O
available O
IJB-A B-DAT
[16] O
dataset. O
Data O
quality O
im O

achieves O
new O
state-of-the-art O
results O
on O
IJB-A B-DAT
dataset, O
and O
competing O
results O
on O

14], O
YouTube O
Face O
[19] O
and O
IJB-A B-DAT
[16 O

of O
0.0001 O
on O
the O
challenging O
IJB-A B-DAT
[16] O
dataset O

a) O
Face O
Verification O
Performance O
on O
IJB-A B-DAT
dataset. O
The O
templates O
are O
divided O

b) O
Sample O
template O
images O
from O
IJB-A B-DAT
dataset O
with O
high, O
medium O
and O

simple O
ex- O
periment O
on O
the O
IJB-A B-DAT
[16] O
dataset O
where O
we O
divide O

six O
different O
sets O
for O
the O
IJB-A B-DAT
face O
verification O
protocol. O
It O
can O

set- O
ting, O
and O
the O
challenging O
IJB-A B-DAT
dataset O
[16] O
on O
the O
1:1 O

for O
1:1 O
verification O
protocol O
on O
IJB-A B-DAT
[16] O
as O
shown O
in O
Table O

Table O
2. O
TAR O
on O
IJB-A B-DAT
1:1 O
Verification O
Protocol O
@FAR O
0.0001 O

it O
improves O
the O
TAR@FAR=0.0001 O
on O
IJB-A B-DAT
by O
more O
than O
10% O
(Table O

Table O
3. O
TAR O
on O
IJB-A B-DAT
1:1 O
Verification O
Protocol O
@FAR O
0.0001 O

14], O
YouTube O
Face O
[34] O
and O
IJB-A B-DAT
[16] O
datasets. O
We O
crop O
and O

The O
IJB-A B-DAT
dataset O
[16] O
contains O
500 O
subjects O

of O
recent O
DCNN-based O
methods O
on O
IJB-A B-DAT
dataset. O
We O
achieve O
state-of-the-art O
result O

using O
the O
training O
splits O
of O
IJB-A B-DAT

FAR O
of O
0.0001 O
on O
IJB-A B-DAT

LFW O
[14], O
YTF O
[34] O
and O
IJB-A B-DAT
[16] O
datasets O
clearly O
suggests O
the O

Experiments O
on O
LFW, O
YTF O
and O
IJB-A B-DAT
datasets O
show O
that O
the O
proposed O

Identification O
and O
Verification O
Evaluation O
on O
IJB-A B-DAT
dataset O

IJB-A B-DAT
Verification O
(TAR@FAR) O
IJB-A O
Identification O
Method O
0.0001 O
0.001 O
0.01 O

state-of-the-art O
results O
on O
the O
challenging O
IJB B-DAT

algorithms O
on O
the O
publicly O
available O
IJB B-DAT

achieves O
new O
state-of-the-art O
results O
on O
IJB B-DAT

14], O
YouTube O
Face O
[19] O
and O
IJB B-DAT

of O
0.0001 O
on O
the O
challenging O
IJB B-DAT

a) O
Face O
Verification O
Performance O
on O
IJB B-DAT

b) O
Sample O
template O
images O
from O
IJB B-DAT

simple O
ex- O
periment O
on O
the O
IJB B-DAT

six O
different O
sets O
for O
the O
IJB B-DAT

set- O
ting, O
and O
the O
challenging O
IJB B-DAT

for O
1:1 O
verification O
protocol O
on O
IJB B-DAT

Table O
2. O
TAR O
on O
IJB B-DAT

it O
improves O
the O
TAR@FAR=0.0001 O
on O
IJB B-DAT

Table O
3. O
TAR O
on O
IJB B-DAT

14], O
YouTube O
Face O
[34] O
and O
IJB B-DAT

The O
IJB B-DAT

of O
recent O
DCNN-based O
methods O
on O
IJB B-DAT

using O
the O
training O
splits O
of O
IJB B-DAT

FAR O
of O
0.0001 O
on O
IJB B-DAT

LFW O
[14], O
YTF O
[34] O
and O
IJB B-DAT

Experiments O
on O
LFW, O
YTF O
and O
IJB B-DAT

achieves O
the O
state-of-the-art O
result O
on O
IJB B-DAT

Identification O
and O
Verification O
Evaluation O
on O
IJB B-DAT

IJB-A B-DAT
Verification O
(TAR@FAR) O
IJB O

deep O
convolutional O
neural O
networks O
(DCNNs). O
A B-DAT
typical O
pipeline O
for O
face O
veri O

A B-DAT
dataset, O
achieving O
True O
Accept O
Rate O

A B-DAT
[16] O
dataset. O
Data O
quality O
im O

A B-DAT
dataset, O
and O
competing O
results O
on O

A B-DAT
[16 O

A B-DAT
[16] O
dataset O

A B-DAT
recent O
approach O
[33] O
introduced O
center O

SphereFace O
[20] O
proposes O
angular O
softmax O
(A B-DAT

to O
its O
correct O
identity O
label. O
A B-DAT
softmax O
loss O
function O
is O
used O

A B-DAT
dataset. O
The O
templates O
are O
divided O

A B-DAT
dataset O
with O
high, O
medium O
and O

Figure O
2. O
A B-DAT
general O
pipeline O
for O
training O
and O

A B-DAT
[16] O
dataset O
where O
we O
divide O

A B-DAT
face O
verification O
protocol. O
It O
can O

upper O
bound O
for O
the O
parameter. O
A B-DAT
better O
performance O
is O
obtained O
by O

maximum O
of O
100K O
iter- O
ations. O
A B-DAT
training O
batch O
size O
of O
256 O

A B-DAT
dataset O
[16] O
on O
the O
1:1 O

A B-DAT
similar O
trend O
is O
observed O
for O

A B-DAT
[16] O
as O
shown O
in O
Table O

A B-DAT
1:1 O
Verification O
Protocol O
@FAR O
0.0001 O

A B-DAT
by O
more O
than O
10% O
(Table O

A B-DAT
1:1 O
Verification O
Protocol O
@FAR O
0.0001 O

A B-DAT
[16] O
datasets. O
We O
crop O
and O

pre-trained O
on O
ImageNet O
[26] O
dataset. O
A B-DAT
fully-connected O
layer O
of O
dimension O
512 O

A B-DAT
dataset O
[16] O
contains O
500 O
subjects O

A B-DAT
dataset. O
We O
achieve O
state-of-the-art O
result O

A B-DAT

A B-DAT

A B-DAT
[16] O
datasets O
clearly O
suggests O
the O

A B-DAT
datasets O
show O
that O
the O
proposed O

the O
state-of-the-art O
result O
on O
IJB- O
A B-DAT
[16] O
dataset. O
In O
conclusion, O
L2-softmax O

A B-DAT
dataset O

A B-DAT
Verification O
(TAR@FAR) O
IJB-A O
Identification O
Method O

References O
[1] O
M. O
Abadi, B-DAT
A O

C. O
Citro, O
G. O
S. O
Corrado, O
A B-DAT

S. O
Ghe- O
mawat, O
I. O
Goodfellow, O
A B-DAT

4] O
J.-C. O
Chen, O
R. O
Ranjan, O
A B-DAT

6] O
A B-DAT

Kavukcuoglu, O
and O
C. O
Farabet. O
Torch7: O
A B-DAT
matlab-like O
environment O
for O
machine O
learning O

C. O
Stauffer, O
Q. O
Cao, O
and O
A B-DAT

He, O
and O
J. O
Gao. O
Ms-celeb-1m: O
A B-DAT
dataset O
and O
benchmark O
for O
large-scale O

10] O
A B-DAT

Labeled O
faces O
in O
the O
wild: O
A B-DAT
database O
for O
studying O
face O
recognition O

Klare, O
B. O
Klein, O
E. O
Taborsky, O
A B-DAT

Cheney, O
K. O
Allen, B-DAT
P. O
Grother, O
A O

. O
Mah, O
and O
A B-DAT

A B-DAT

J.-C. O
Chen, O
and O
R. O
Chellappa. O
A B-DAT
proximity- O
aware O
hierarchical O
clustering O
of O

22] O
I. O
Masi, O
A B-DAT

Sankara- O
narayanan, O
J.-C. O
Chen, O
and O
A B-DAT

24] O
O. O
M. O
Parkhi, O
A B-DAT

. O
Vedaldi, O
and O
A B-DAT

Satheesh, O
S. O
Ma, O
Z. O
Huang, O
A B-DAT

. O
Karpathy, O
A B-DAT

. O
Khosla, O
M. O
Bernstein, O
A B-DAT

27] O
S. O
Sankaranarayanan, O
A B-DAT

Kalenichenko, O
and O
J. O
Philbin. O
Facenet: O
A B-DAT
uni- O
fied O
embedding O
for O
face O

D. O
Wang, O
C. O
Otto, O
and O
A B-DAT

Z. O
Li, O
and O
Y. O
Qiao. O
A B-DAT
discrimina- O
tive O
feature O
learning O
approach O

S. O
Pranata, O
and O
S. O
Shen. O
A B-DAT
good O
practice O
towards O
top O
performance O

task O
(Rank-1 O
accuracy) O
on O
the O
IJB-A B-DAT
dataset O
[4] O
for O
ResNet-18. O
For O

4.2. O
Evaluation O
on O
IJB-A B-DAT
with O
Full O
Pose O
Variation O

called O
IARPA O
Janus O
Benchmark O
A O
(IJB-A) B-DAT
[13] O
that O
cov- O
ers O
full O

2 O
reports O
our O
results O
on O
IJB-A B-DAT

2. O
Comparative O
performance O
analysis O
on O
IJB-A B-DAT
benchmark. O
Results O
reported O
are O
the O

10 O
folds O
specified O
in O
the O
IJB-A B-DAT
protocol. O
Symbol O
‘-’ O
indicates O
that O

MS-Celeb-1M O
and O
directly O
tested O
on O
IJB-A B-DAT
with- O
out O
fine-tuning. O
Data O
augmentation O

on O
the O
training O
splits O
of O
IJB-A B-DAT

ment. O
Extensive O
results O
on O
CFP, O
IJB-A, B-DAT
and O
MS-Celeb-1M O
datasets O
demonstrate O
the O

task O
(Rank-1 O
accuracy) O
on O
the O
IJB B-DAT

4.2. O
Evaluation O
on O
IJB B-DAT

called O
IARPA O
Janus O
Benchmark O
A O
(IJB B-DAT

videos. O
The O
faces O
in O
the O
IJB B-DAT

2 O
reports O
our O
results O
on O
IJB B-DAT

2. O
Comparative O
performance O
analysis O
on O
IJB B-DAT

10 O
folds O
specified O
in O
the O
IJB B-DAT

MS-Celeb-1M O
and O
directly O
tested O
on O
IJB B-DAT

on O
the O
training O
splits O
of O
IJB B-DAT

ment. O
Extensive O
results O
on O
CFP, O
IJB B-DAT

faces O
compared O
to O
frontal O
faces. O
A B-DAT
key O
reason O
is O
that O
the O

Figure O
1. O
A B-DAT
state-of-the-art O
face O
recognition O
model O
[34 O

the O
recognition O
of O
profile O
faces. O
A B-DAT
large O
body O
of O
methods O
normalize O

A B-DAT
dataset O
[4] O
for O
ResNet-18. O
For O

recognition O
models O
[29, O
25, O
33]. O
A B-DAT
fully O
connected O
layer O
is O
then O

A B-DAT
with O
Full O
Pose O
Variation O

benchmark O
called O
IARPA B-DAT
Janus O
Benchmark O
A O
(IJB-A) O
[13] O
that O
cov- O
ers O

The O
faces O
in O
the O
IJB- O
A B-DAT
dataset O
contain O
extreme O
poses O
and O

A B-DAT

A B-DAT
benchmark. O
Results O
reported O
are O
the O

A B-DAT
protocol. O
Symbol O
‘-’ O
indicates O
that O

A B-DAT
with- O
out O
fine-tuning. O
Data O
augmentation O

Figure O
6. O
A B-DAT
comparison O
between O
the O
false O
positive O

A B-DAT

4.3. O
A B-DAT
Further O
Analysis O
on O
Influences O
of O

A, B-DAT
and O
MS-Celeb-1M O
datasets O
demonstrate O
the O

G. O
Duan, O
and O
J. O
Sun. O
A B-DAT
practical O
transfer O
learning O
algorithm O
for O

J. O
Sun. O
Bayesian O
face O
revisited: O
A B-DAT
joint O
formulation. O
In O
ECCV, O
2012 O

O. O
Parkhi, O
Q. O
Cao, O
and O
A B-DAT

Labeled O
faces O
in O
the O
wild: O
A B-DAT
database O
for O
studying O
face O
recognition O

Klare, O
B. O
Klein, O
E. O
Taborsky, O
A B-DAT

Cheney, O
K. O
Allen, B-DAT
P. O
Grother, O
A O

. O
Mah, O
and O
A B-DAT

and O
recognition: O
IARPA B-DAT
Janus O
Benchmark O
A O

facial O
landmarks O
in O
the O
wild: O
A B-DAT
large-scale, O
real- O
world O
database O
for O

15] O
K. O
Lenc O
and O
A B-DAT

20] O
I. O
Masi, O
A B-DAT

21] O
A B-DAT

Nguyen, O
J. O
Yosinski, O
Y. O
Bengio, O
A B-DAT

23] O
S. O
Sankaranarayanan, O
A B-DAT

24] O
S. O
Sankaranarayanan, O
A B-DAT

Kalenichenko, O
and O
J. O
Philbin. O
Facenet: O
A B-DAT
uni- O
fied O
embedding O
for O
face O

N. O
Srivastava, O
G. O
E. O
Hinton, O
A B-DAT

D. O
Wang, O
C. O
Otto, O
and O
A B-DAT

Z. O
Li, O
and O
Y. O
Qiao. O
A B-DAT
discriminative O
fea- O
ture O
learning O
approach O

Y. O
Sugano, O
M. O
Fritz, O
and O
A B-DAT

exposed O
faces. O
The O
experiments O
on O
IJB-A, B-DAT
YouTube O
Face, O
Celebrity-1000 O
video O
face O

YouTube O
Face O
dataset O
[42], O
the O
IJB-A B-DAT
dataset O
[19], O
and O
the O
Celebrity-1000 O

2. O
Face O
images O
in O
the O
IJB-A B-DAT
dataset, O
sorted O
by O
their O
scores O

1. O
Performance O
comparison O
on O
the O
IJB-A B-DAT
dataset. O
TAR/FAR: O
True/False O
Accept O
Rate O

3 O
for O
details) O
in O
the O
IJB-A B-DAT
dataset O
[19] O
on O
the O
extracted O

train O
the O
network O
on O
the O
IJB-A B-DAT
dataset O
again, O
and O
Table O
1 O

the O
IARPA O
Janus O
Benchmark O
A O
(IJB-A) B-DAT
[19], O
the O
YouTube O
Face O
dataset O

3.3. O
Results O
on O
IJB-A B-DAT
dataset O

The O
IJB-A B-DAT
dataset O
[19] O
contains O
face O
images O

2. O
Performance O
evaluation O
on O
the O
IJB-A B-DAT
dataset. O
For O
verification, O
the O
true O

and O
the O
baselines O
on O
the O
IJB-A B-DAT
dataset O
over O
10 O
splits O

compared O
to O
the O
results O
on O
IJB-A B-DAT

exposed O
faces. O
The O
experiments O
on O
IJB B-DAT

YouTube O
Face O
dataset O
[42], O
the O
IJB B-DAT

2. O
Face O
images O
in O
the O
IJB B-DAT

1. O
Performance O
comparison O
on O
the O
IJB B-DAT

3 O
for O
details) O
in O
the O
IJB B-DAT

train O
the O
network O
on O
the O
IJB B-DAT

the O
IARPA O
Janus O
Benchmark O
A O
(IJB B-DAT

3.3. O
Results O
on O
IJB B-DAT

The O
IJB B-DAT

2. O
Performance O
evaluation O
on O
the O
IJB B-DAT

and O
the O
baselines O
on O
the O
IJB B-DAT

compared O
to O
the O
results O
on O
IJB B-DAT

A, B-DAT
YouTube O
Face, O
Celebrity-1000 O
video O
face O

the O
need O
for O
frame-to-frame O
matching. O
A B-DAT
straightforward O
solution O
might O
be O
extracting O

A B-DAT
ug O

A B-DAT
dataset O
[19], O
and O
the O
Celebrity-1000 O

A B-DAT
dataset, O
sorted O
by O
their O
scores O

A B-DAT
dataset. O
TAR/FAR: O
True/False O
Accept O
Rate O

A B-DAT
dataset O
[19] O
on O
the O
extracted O

A B-DAT
dataset O
again, O
and O
Table O
1 O

datasets: O
the O
IARPA B-DAT
Janus O
Benchmark O
A O
(IJB-A) O
[19], O
the O
YouTube O
Face O

A B-DAT
dataset O

A B-DAT
dataset O
[19] O
contains O
face O
images O

A B-DAT
dataset. O
For O
verification, O
the O
true O

A B-DAT
dataset O
over O
10 O
splits O

A B-DAT

7] O
J.-C. O
Chen, O
R. O
Ranjan, O
A B-DAT

9] O
A B-DAT

C. O
Stauffer, O
Q. O
Cao, O
and O
A B-DAT

12] O
A B-DAT

Klare, O
B. O
Klein, O
E. O
Taborsky, O
A B-DAT

Cheney, O
K. O
Allen, B-DAT
P. O
Grother, O
A O

. O
Mah, O
M. O
Burge, O
and O
A B-DAT

25] O
I. O
Masi, O
A B-DAT

O. O
M. O
Parkhi, O
K. O
Simonyan, O
A B-DAT

. O
Vedaldi, O
and O
A. B-DAT
Zisserman. O
A O
compact O
and O
discriminative O
face O
track O

28] O
O. O
M. O
Parkhi, O
A B-DAT

. O
Vedaldi, O
and O
A B-DAT

30] O
S. O
Sankaranarayanan, O
A B-DAT

Kalenichenko, O
and O
J. O
Philbin. O
FaceNet: O
A B-DAT
unified O
embedding O
for O
face O
recognition O

D. O
Erhan, O
V. O
Vanhoucke, O
and O
A B-DAT

37] O
P. O
Turaga, O
A. B-DAT
Veeraraghavan, O
A O

D. O
Wang, O
C. O
Otto, O
and O
A B-DAT

Z. O
Li, O
and O
Y. O
Qiao. O
A B-DAT
discriminative O
fea- O
ture O
learning O
approach O

In O
contrast, O
the O
newly O
released O
IJB-A B-DAT
face O
recognition O
dataset O
[3] O
unifies O

template. O
Extensive O
performance O
evaluations O
on O
IJB-A B-DAT
show O
a O
surprising O
result, O
that O

the O
same O
top O
performance O
on O
IJB-A B-DAT
for O
template-based O
face O
verification O
and O

The O
IJB-A B-DAT
dataset O
[3] O
was O
created O
to O

set O
of O
images O
[14]. O
The O
IJB-A B-DAT
dataset O
is O
the O
only O
public O

Recent O
evaluations O
on O
IJB-A B-DAT
[3] O
are O
also O
based O
on O

publishing O
the O
earliest O
results O
on O
IJB-A B-DAT

and O
worst O
templates O
pairs O
in O
IJB-A B-DAT
for O
verification O
(identification O
errors O
are O

Figure O
2. O
IJB-A B-DAT
Evaluation. O
(top) O
1:1 O
DET O
for O

and O
gallery O
template O
pairs O
in O
IJB-A B-DAT
split O
1 O
and O
CMC O
for O

identification O
on O
IJB-A B-DAT
split O
1 O
(see O
section O
4.2 O

4.2. O
IJB-A B-DAT
Evaluation O

the O
experimental O
system O
on O
the O
IJB-A B-DAT
verification O
and O
iden- O
tification O
protocols O

3]. O
IJB-A B-DAT
contains O
5712 O
images O
and O
2085 O

Performance O
evaluation O
for O
IJB-A B-DAT
requires O
evaluation O
of O
ten O
random O

the O
overall O
evaluation O
results O
on O
IJB-A B-DAT

and O
gallery O
template O
pairs O
in O
IJB-A B-DAT
split O
1 O
and O
CMC O
for O

an O
unmodeled O
dataset O
bias O
for O
IJB-A B-DAT
faces, O
or O
that O
CASIA O
is O

image O
only, O
while O
IJB-A B-DAT
is O
imagery O
and O
videos O

identification O
CMC O
curves O
(right) O
for O
IJB-A B-DAT
split-1. O
(top) O
Template O
adaptation O
compared O

tween O
the O
templates O
are O
the O
IJB-A B-DAT
Template O
IDs O
for O
probe O
and O

art O
per- O
formance O
on O
the O
IJB-A B-DAT
dataset. O
Furthermore, O
we O
showed O
that O

hold O
for O
other O
datasets. O
The O
IJB-A B-DAT
dataset O
is O
currently O
the O
only O

In O
contrast, O
the O
newly O
released O
IJB B-DAT

template. O
Extensive O
performance O
evaluations O
on O
IJB B-DAT

the O
same O
top O
performance O
on O
IJB B-DAT

The O
IJB B-DAT

set O
of O
images O
[14]. O
The O
IJB B-DAT

of O
template O
adaptation O
on O
the O
IJB B-DAT

Recent O
evaluations O
on O
IJB B-DAT

publishing O
the O
earliest O
results O
on O
IJB B-DAT

and O
worst O
templates O
pairs O
in O
IJB B-DAT

Figure O
2. O
IJB B-DAT

and O
gallery O
template O
pairs O
in O
IJB B-DAT

and O
CMC O
for O
identification O
on O
IJB B-DAT

4.2. O
IJB B-DAT

the O
experimental O
system O
on O
the O
IJB B-DAT

and O
iden- O
tification O
protocols O
[3]. O
IJB B-DAT

Performance O
evaluation O
for O
IJB B-DAT

the O
overall O
evaluation O
results O
on O
IJB B-DAT

and O
gallery O
template O
pairs O
in O
IJB B-DAT

and O
CMC O
for O
identification O
on O
IJB B-DAT

not O
provide O
much O
benefit O
on O
IJB B-DAT

an O
unmodeled O
dataset O
bias O
for O
IJB B-DAT

CASIA O
is O
image O
only, O
while O
IJB B-DAT

identification O
CMC O
curves O
(right) O
for O
IJB B-DAT

tween O
the O
templates O
are O
the O
IJB B-DAT

art O
per- O
formance O
on O
the O
IJB B-DAT

hold O
for O
other O
datasets. O
The O
IJB B-DAT

A B-DAT
face O
recognition O
dataset O
[3] O
unifies O

A B-DAT
show O
a O
surprising O
result, O
that O

A B-DAT
for O
template-based O
face O
verification O
and O

age, O
pose, O
illumination O
and O
expression O
(A B-DAT

uncontrolled O
collection O
and O
amount O
of O
A B-DAT

A B-DAT
dataset O
[3] O
was O
created O
to O

A B-DAT
pr O

instead O
of O
image-to-image O
or O
video-to-video. O
A B-DAT
template O
is O
a O
set O
of O

A B-DAT
dataset O
is O
the O
only O
public O

template O
adaptation O
on O
the O
IJB- O
A B-DAT
dataset O
has O
generated O
surprising O
results O

A B-DAT
[3] O
are O
also O
based O
on O

A B-DAT

First, O
we O
provide O
preliminary O
definitions. O
A B-DAT
media O
ob- O
servation O
x O
is O

all O
frames O
in O
a O
video. O
A B-DAT
template O
X O
is O
a O
set O

template O
P O
to O
gallery O
G. O
A B-DAT
gallery O
contains O
templates O
G O

A B-DAT
for O
verification O
(identification O
errors O
are O

A B-DAT
Evaluation. O
(top) O
1:1 O
DET O
for O

A B-DAT
split O
1 O
and O
CMC O
for O

A B-DAT
split O
1 O
(see O
section O
4.2 O

A B-DAT
Evaluation O

A B-DAT
verification O
and O
iden- O
tification O
protocols O

A B-DAT
contains O
5712 O
images O
and O
2085 O

A B-DAT
requires O
evaluation O
of O
ten O
random O

A B-DAT

A B-DAT
split O
1 O
and O
CMC O
for O

identification O
on O
IJB- O
A B-DAT
split O
1. O
This O
study O
shows O

provide O
much O
benefit O
on O
IJB- O
A, B-DAT
in O
contrast O
with O
reported O
performance O

A B-DAT
faces, O
or O
that O
CASIA O
is O

A B-DAT
is O
imagery O
and O
videos O

A B-DAT
split-1. O
(top) O
Template O
adaptation O
compared O

A B-DAT
Template O
IDs O
for O
probe O
and O

A B-DAT
dataset. O
Furthermore, O
we O
showed O
that O

A B-DAT
dataset O
is O
currently O
the O
only O

beled O
faces O
in O
the O
wild: O
A B-DAT
database O
for O
studying O
face O
recog O

Klein, O
B., O
Taborsky, O
E., O
Blanton, O
A B-DAT

Allen, B-DAT
K., O
Grother, O
P., O
Mah, O
A O

., O
Jain, O
A B-DAT

and O
recognition: O
IARPA B-DAT
Janus O
benchmark O
A O

4] O
Parkhi, O
O., O
Vedaldi, O
A B-DAT

., O
Zisserman, O
A B-DAT

Kalenichenko, O
D., O
Philbin, O
J.: O
FaceNet: O
A B-DAT
unified O
embedding O
for O
face O
recognition O

Learned-Miller, O
E., O
Huang, O
G., O
RoyChowdhury, O
A B-DAT

Labeled O
Faces O
in O
the O
Wild: O
A B-DAT
Survey. O
In: O
Advances O
in O
Face O

Otto, O
C., O
Klare, O
B., O
Jain, O
A B-DAT

Hill, O
M., O
Swindle, O
J., O
O’Toole, O
A B-DAT

15] O
Sankaranarayanan, O
S., O
Alavi, B-DAT
A O

18] O
RoyChowdry, O
A B-DAT

19] O
Simonyan, O
K., O
Zisserman, O
A B-DAT

Erhan, O
D., O
Vanhoucke, O
V., O
Rabinovich, O
A B-DAT

Sun, O
J.: O
Bayesian O
face O
revisited: O
A B-DAT
joint O
formulation. O
In: O
ECCV. O
(2012 O

Chen, O
J., O
Ranjan, O
R., O
Kumar, O
A B-DAT

24] O
Pan, O
S.J., O
Yang, O
Q.: O
A B-DAT
survey O
on O
transfer O
learning. O
Knowl O

25] O
Razavian, O
A B-DAT

28] O
Malisiewicz, O
T., O
Gupta, O
A B-DAT

., O
Efros, O
A B-DAT

Wang, O
D., O
Otto, O
C., O
Jain, O
A B-DAT

Parkhi, O
O.M., O
Simonyan, O
K., O
Vedaldi, O
A B-DAT

., O
Zisserman, O
A.: B-DAT
A O
compact O
and O
discriminative O
face O
track O

Wang, O
X., O
Lin, O
C.: O
Liblinear: O
A B-DAT
library O
for O
large O
linear O
classification O

variations O
in O
viewpoint O
and O
illumination O
(IJB-A B-DAT
[23] O
dataset). O
We O
address O
this O

on O
the O
IARPA O
Janus O
Benchmark-A O
(IJB-A) B-DAT
[23] O
dataset. O
The O
dataset O
contains O

For O
IJB-A B-DAT
dataset, O
given O
a O
template O
containing O

Identification O
and O
Verification O
Evaluation O
on O
IJB-A B-DAT
dataset O

IJB-A B-DAT
Verification O
(TAR@FAR) O
IJB-A O
Identification O
Method O
0.001 O
0.01 O
0.1 O

End-to-End O
face O
recognition O
systems O
on O
IJB-A B-DAT

with O
recently O
published O
methods O
on O
IJB-A B-DAT

that O
are O
present O
in O
the O
IJB-A B-DAT
dataset O
for O
better O
image O
clarity O

variations O
in O
viewpoint O
and O
illumination O
(IJB B-DAT

on O
the O
IARPA O
Janus O
Benchmark-A O
(IJB B-DAT

For O
IJB B-DAT

Identification O
and O
Verification O
Evaluation O
on O
IJB B-DAT

IJB-A B-DAT
Verification O
(TAR@FAR) O
IJB O

End-to-End O
face O
recognition O
systems O
on O
IJB B-DAT

with O
recently O
published O
methods O
on O
IJB B-DAT

that O
are O
present O
in O
the O
IJB B-DAT

A B-DAT
[23] O
dataset). O
We O
address O
this O

A B-DAT

Fig. O
2: O
A B-DAT
general O
multitask O
learning O
framework O
for O

map O
size O
of O
6× O
6. O
A B-DAT
dimensionality O
reduction O
layer O
is O
added O

or O
computed O
using O
HyperFace O
[36]. O
A B-DAT
cross- O
entropy O
loss O
LG O
is O

A B-DAT

error O
by O
more O
than O
30%. O
A B-DAT
low O
standard O
deviation O
of O
0.13 O

A B-DAT
(IJB-A) O
[23] O
dataset. O
The O
dataset O

A B-DAT
dataset, O
given O
a O
template O
containing O

descriptors, O
as O
explained O
in O
[41]. O
A B-DAT
naive O
way O
to O
measure O
the O

cosine O
distance O
between O
their O
descriptors. O
A B-DAT
better O
way O
is O
to O
learn O

A B-DAT
dataset O

A B-DAT
Verification O
(TAR@FAR) O
IJB-A O
Identification O
Method O

A B-DAT

A B-DAT

A B-DAT
dataset O
for O
better O
image O
clarity O

6] O
J.-C. O
Chen, O
A B-DAT

R. O
Ranjan, O
V. O
M. O
Patel, O
A B-DAT

Alavi, B-DAT
and O
R. O
Chel- O
lappa. O
A O
cascaded O
convolutional O
neural O
network O
for O

8] O
J.-C. O
Chen, O
R. O
Ranjan, O
A B-DAT

C. O
Stauffer, O
Q. O
Cao, O
and O
A B-DAT

I. O
Goodfellow, O
Y. O
Bengio, O
and O
A B-DAT

H. O
Han, O
C. O
Otto, O
and O
A B-DAT

C. O
Fang, O
and O
X. O
Ding. O
A B-DAT
new O
biologically O
inspired O
active O
appearance O

beled O
faces O
in O
the O
wild: O
A B-DAT
database O
for O
studying O
face O
recognition O

Jain O
and O
E. O
Learned-Miller. O
Fddb: O
A B-DAT
benchmark O
for O
face O
detec- O
tion O

21] O
A B-DAT

Klare, O
B. O
Klein, O
E. O
Taborsky, O
A B-DAT

Cheney, O
K. O
Allen, B-DAT
P. O
Grother, O
A O

. O
Mah, O
M. O
Burge, O
and O
A B-DAT

facial O
landmarks O
in O
the O
wild: O
A B-DAT
large-scale, O
real-world O
database O
for O
facial O

25] O
A B-DAT

26] O
A B-DAT

J. O
Brandt, O
and O
G. O
Hua. O
A B-DAT
convolutional O
neural O
network O
cascade O
for O

30] O
S. O
Liao, O
A B-DAT

. O
Jain, O
and O
S. O
Li. O
A B-DAT
fast O
and O
accurate O
unconstrained O
face O

32] O
I. O
Masi, O
A B-DAT

34] O
O. O
M. O
Parkhi, O
A B-DAT

. O
Vedaldi, O
and O
A B-DAT

M. O
Patel, O
and O
R. O
Chellappa. O
A B-DAT
deep O
pyramid O
deformable O
part O
model O

Patel, O
and O
R. O
Chellappa. O
Hyperface: O
A B-DAT
deep O
multi- O
task O
learning O
framework O

S. O
Zhou, O
J. O
C. O
Chen, O
A B-DAT

. O
Kumar, O
A B-DAT

Jr. O
and O
T. O
Tesafaye. O
Morph: O
A B-DAT
longitudinal O
image O
database O
of O
normal O

41] O
S. O
Sankaranarayanan, O
A B-DAT

Kalenichenko, O
and O
J. O
Philbin. O
Facenet: O
A B-DAT
unified O
embedding O
for O
face O
recognition O

45] O
K. O
E. O
A B-DAT

R. O
Uijlings, O
T. O
Gevers, O
and O
A B-DAT

parts O
responses O
to O
face O
detection: O
A B-DAT
deep O
learning O
approach. O
In O
IEEE O

Face O
alignment O
across O
large O
poses: O
A B-DAT
3d O
solution. O
arXiv O
preprint O
arXiv:1511.07212 O

A B-DAT
Multi-task O
Learning O

A B-DAT

A B-DAT

A B-DAT
Face O
Detection O

face O
recognition O
accuracy O
on O
the O
IJB-A B-DAT
and O
IJB-B O
benchmarks: O
using O
the O

on O
the O
highly O
challeng- O
ing O
IJB-A B-DAT
[22] O
and O
IJB-B O
benchmarks O
[43 O

A O
[22] O
and O
B O
[43] O
(IJB-A B-DAT
and O
IJB-B). O
Importantly, O
these O
benchmarks O

2. O
Verification O
and O
identification O
on O
IJB-A B-DAT
and O
IJB-B, O
com- O
paring O
landmark O

face O
alignment O
methods. O
Three O
baseline O
IJB-A B-DAT
results O
are O
also O
provided O
as O

IJB-A B-DAT
[22] O
Crosswhite O
et O
al. O
[9 O

a) O
ROC O
IJB-A B-DAT
(b) O
CMC O
IJB-A O

Verification O
and O
identification O
results O
on O
IJB-A B-DAT
and O
IJB-B. O
ROC O
and O
CMC O

and O
identification O
results O
on O
both O
IJB-A B-DAT
and O
IJB-B O
are O
provided O
in O

provides, O
as O
reference, O
three O
state-of-the-art O
IJB-A B-DAT
re- O
sults O
[9, O
31, O
36 O

addition, O
our O
verification O
scores O
on O
IJB-A B-DAT
outperform O
the O
scores O
reported O
for O

views O
of O
the O
faces O
in O
IJB-A B-DAT
and O
IJB-B O

face O
recognition O
accuracy O
on O
the O
IJB B-DAT

-A O
and O
IJB B-DAT

on O
the O
highly O
challeng- O
ing O
IJB B-DAT

-A O
[22] O
and O
IJB B-DAT

A O
[22] O
and O
B O
[43] O
(IJB B-DAT

-A O
and O
IJB B-DAT

2. O
Verification O
and O
identification O
on O
IJB B-DAT

-A O
and O
IJB B-DAT

face O
alignment O
methods. O
Three O
baseline O
IJB B-DAT

IJB B-DAT

IJB B-DAT

a) O
ROC O
IJB-A B-DAT
(b) O
CMC O
IJB O

c) O
ROC O
IJB-B B-DAT
(d) O
CMC O
IJB O

Verification O
and O
identification O
results O
on O
IJB B-DAT

-A O
and O
IJB B-DAT

and O
identification O
results O
on O
both O
IJB B-DAT

-A O
and O
IJB B-DAT

provides, O
as O
reference, O
three O
state-of-the-art O
IJB B-DAT

baseline O
results O
from O
[43] O
for O
IJB B-DAT

verification O
and O
identification O
accuracies O
on O
IJB B-DAT

addition, O
our O
verification O
scores O
on O
IJB B-DAT

views O
of O
the O
faces O
in O
IJB B-DAT

-A O
and O
IJB B-DAT

A B-DAT
and O
IJB-B O
benchmarks: O
using O
the O

A B-DAT

A B-DAT
[22] O
and O
IJB-B O
benchmarks O
[43 O

one O
face O
image O
and O
another. O
A B-DAT
different O
interpretation O
of O
align- O
ment O

3. O
A B-DAT
critique O
of O
facial O
landmark O
detection O

A B-DAT
second O
potential O
problem O
lies O
in O

p,1]T O
= O
A B-DAT

where O
A B-DAT
and O
R O
are O
the O
camera O

too O
small O
for O
this O
purpose. O
A B-DAT
key O
problem O
is O
therefore O
obtaining O

the O
images O
in O
this O
set. O
A B-DAT
potential O
danger O
in O
using O
an O

produced O
by O
the O
camera O
matrix O
A, B-DAT
rotation O
matrix O
R, O
and O
the O

face O
recognition: O
IARPA B-DAT
Janus O
Benchmark O
A O
[22] O
and O
B O
[43] O
(IJB-A O

A B-DAT
and O
IJB-B, O
com- O
paring O
landmark O

A B-DAT
results O
are O
also O
provided O
as O

A B-DAT
[22] O
Crosswhite O
et O
al. O
[9 O

A B-DAT
(b) O
CMC O
IJB-A O

A B-DAT
and O
IJB-B. O
ROC O
and O
CMC O

A B-DAT
and O
IJB-B O
are O
provided O
in O

A B-DAT
re- O
sults O
[9, O
31, O
36 O

A B-DAT
outperform O
the O
scores O
reported O
for O

A B-DAT
N/A O
N/A O
N/A O
N/A O
0.6 O

A B-DAT
and O
IJB-B O

References O
[1] O
A B-DAT

4] O
A B-DAT

. O
Bansal, O
B. O
Russell, O
and O
A B-DAT

O. O
Parkhi, O
Q. O
Cao, O
and O
A B-DAT

H. O
Kaya, O
H. O
Dibeklioglu, O
and O
A B-DAT

20] O
A B-DAT

21] O
D. O
E. O
King. O
Dlib-ml: O
A B-DAT
machine O
learning O
toolkit. O
J. O
Mach O

Klare, O
B. O
Klein, O
E. O
Taborsky, O
A B-DAT

Cheney, O
K. O
Allen, B-DAT
P. O
Grother, O
A O

. O
Mah, O
M. O
Burge, O
and O
A B-DAT

A B-DAT

facial O
landmarks O
in O
the O
wild: O
A B-DAT
large-scale, O
real- O
world O
database O
for O

24] O
A B-DAT

25] O
A. B-DAT
Kumar, O
A O

31] O
I. O
Masi, O
T. O
Hassner, O
A B-DAT

33] O
I. O
Masi, O
A B-DAT

34] O
O. O
M. O
Parkhi, O
A B-DAT

. O
Vedaldi, O
and O
A B-DAT

W. O
Liu, O
J. O
Kosecka, O
and O
A B-DAT

42] O
A B-DAT

43] O
C. O
Whitelam, O
E. O
Taborsky, O
A B-DAT

Adams, B-DAT
T. O
Miller, O
N. O
Kalka, O
A O

. O
K. O
Jain, O
J. O
A B-DAT

Guibas, O
and O
S. O
Savarese. O
Objectnet3D: O
A B-DAT
large O
scale O
database O
for O
3D O

and O
S. O
Savarese. O
Beyond O
pascal: O
A B-DAT
benchmark O
for O
3D O
object O
detection O

Z. O
Yang O
and O
R. O
Nevatia. O
A B-DAT
multi-scale O
cascade O
fully O
con- O
volutional O

49] O
A B-DAT

Face O
alignment O
across O
large O
poses: O
A B-DAT
3D O
solution. O
In O
Proc. O
Conf O

Experiments O
on O
the O
challeng- O
ing O
IJB-A B-DAT
dataset O
show O
that O
the O
proposed O

simple O
clustering O
experiments O
on O
both O
IJB-A B-DAT
and O
LFW O
datasets O

algorithms O
on O
the O
publicly O
available O
IJB-A B-DAT
dataset O
([2], O
[3]) O
that O
was O

age. O
In O
the O
case O
of O
IJB-A B-DAT
dataset, O
the O
data O
is O
divided O

4 O
followed O
by O
results O
on O
IJB-A B-DAT
and O
CFP O
datasets O
and O
a O

media O
collection O
from O
LFW O
and O
IJB-A B-DAT
datasets O

Since O
the O
release O
of O
the O
IJB-A B-DAT
dataset O
[2], O
there O
have O
been O

deep O
CNN O
models O
on O
the O
IJB-A B-DAT
dataset O
(Table O
2 O
in O
Results O

Figure O
2: O
Performance O
improvement O
on O
IJB-A B-DAT
split O
1: O
FAR O
(vs) O
TAR O

for O
split O
1 O
of O
the O
IJB-A B-DAT
verify O
protocol O
for O
the O
three O

the O
ten O
splits O
of O
the O
IJB-A B-DAT
dataset O

datasets: O
1. O
IARPA O
Janus O
Benchmark-A O
(IJB-A) B-DAT
[2]: O
This O
dataset O

60). O
The O
faces O
in O
the O
IJB-A B-DAT
dataset O
contain O
extreme O
poses O
and O

Some O
sample O
images O
from O
the O
IJB-A B-DAT
dataset O
are O
shown O
in O
Figure O

An O
additional O
challenge O
of O
the O
IJB-A B-DAT
verification O
protocol O
is O
that O
the O

given O
test O
template O
of O
the O
IJB-A B-DAT
data O
we O
perform O
two O
kinds O

from. O
The O
metadata O
provided O
with O
IJB-A B-DAT
gives O
us O
the O
media O
id O

Figure O
3: O
Images O
from O
the O
IJB-A B-DAT
dataset O

a O
given O
image. O
For O
the O
IJB-A B-DAT
dataset, O
since O
most O
images O
contain O

provided O
bounding O
box. O
In O
the O
IJB-A B-DAT
dataset, O
there O
are O
few O
images O

NVIDIA O
TitanX O
GPU. O
For O
the O
IJB-A B-DAT
dataset, O
we O
use O
the O
training O

Method O
IJB-A B-DAT
Verification O
(FNMR@FMR) O
IJB-A O
Identification O

and O
Verification O
results O
on O
the O
IJB-A B-DAT
dataset. O
For O
identification, O
the O
scores O

More O
specifi- O
cally, O
for O
the O
IJB-A B-DAT
dataset, O
given O
a O
template O
containing O

types O
of O
results O
for O
the O
IJB-A B-DAT
dataset: O
Veri- O
fication O
and O
Identification O

the O
evaluation O
metrics O
for O
the O
IJB-A B-DAT
proto- O
col O
can O
be O
found O

Performance O
on O
IJB-A B-DAT

to O
existing O
results O
for O
the O
IJB-A B-DAT
Verification O
and O
Iden- O
tification O
protocol O

formance O
provided O
along O
with O
the O
IJB-A B-DAT
dataset. O
• O
Parkhi O
et O
al O

many O
test O
images O
of O
the O
IJB-A B-DAT
dataset O
contain O
extreme O
poses, O
harsh O

raw O
features O
specifically O
to O
the O
IJB-A B-DAT
dataset O

and O
identification O
protocols O
of O
the O
IJB-A B-DAT
dataset, O
with O
the O
exception O
of O

the O
training O
data O
of O
the O
IJB-A B-DAT
splits. O
Let’s O
denote O
the O
pooled O

2. O
We O
use O
the O
IJB-A B-DAT
dataset O
and O
cluster O
the O
templates O

for O
each O
split O
in O
the O
IJB-A B-DAT
verify O
protocol O

the O
split O
1 O
of O
the O
IJB-A B-DAT
dataset. O
Top O
row O
(a,b) O
shows O

5: O
Clustering O
metrics O
over O
the O
IJB-A B-DAT
1:1 O
protocol. O
The O
standard O
deviation O

Clustering O
IJB-A:- B-DAT
The O
IJB-A O
dataset O
is O
processed O
as O
de O

Precision-Recall O
(PR) O
curve O
for O
the O
IJB-A B-DAT
clustering O
experiment O
in O
Figure O
6 O

PR O
curve O
for O
clustering O
the O
IJB-A B-DAT
data O
using O
embedded O
features O
exhibits O

method O
on O
two O
challenging O
datasets: O
IJB-A B-DAT
and O
CFP O
and O
achieved O
performance O

algorithm O
on O
the O
LFW O
and O
IJB-A B-DAT
datasets. O
For O
future O
work, O
we O

Experiments O
on O
the O
challeng- O
ing O
IJB B-DAT

simple O
clustering O
experiments O
on O
both O
IJB B-DAT

algorithms O
on O
the O
publicly O
available O
IJB B-DAT

age. O
In O
the O
case O
of O
IJB B-DAT

4 O
followed O
by O
results O
on O
IJB B-DAT

media O
collection O
from O
LFW O
and O
IJB B-DAT

Since O
the O
release O
of O
the O
IJB B-DAT

deep O
CNN O
models O
on O
the O
IJB B-DAT

Figure O
2: O
Performance O
improvement O
on O
IJB B-DAT

for O
split O
1 O
of O
the O
IJB B-DAT

the O
ten O
splits O
of O
the O
IJB B-DAT

datasets: O
1. O
IARPA O
Janus O
Benchmark-A O
(IJB B-DAT

60). O
The O
faces O
in O
the O
IJB B-DAT

Some O
sample O
images O
from O
the O
IJB B-DAT

An O
additional O
challenge O
of O
the O
IJB B-DAT

given O
test O
template O
of O
the O
IJB B-DAT

from. O
The O
metadata O
provided O
with O
IJB B-DAT

Figure O
3: O
Images O
from O
the O
IJB B-DAT

a O
given O
image. O
For O
the O
IJB B-DAT

provided O
bounding O
box. O
In O
the O
IJB B-DAT

NVIDIA O
TitanX O
GPU. O
For O
the O
IJB B-DAT

Method O
IJB-A B-DAT
Verification O
(FNMR@FMR) O
IJB O

and O
Verification O
results O
on O
the O
IJB B-DAT

More O
specifi- O
cally, O
for O
the O
IJB B-DAT

types O
of O
results O
for O
the O
IJB B-DAT

the O
evaluation O
metrics O
for O
the O
IJB B-DAT

Performance O
on O
IJB B-DAT

to O
existing O
results O
for O
the O
IJB B-DAT

formance O
provided O
along O
with O
the O
IJB B-DAT

many O
test O
images O
of O
the O
IJB B-DAT

raw O
features O
specifically O
to O
the O
IJB B-DAT

and O
identification O
protocols O
of O
the O
IJB B-DAT

the O
training O
data O
of O
the O
IJB B-DAT

2. O
We O
use O
the O
IJB B-DAT

for O
each O
split O
in O
the O
IJB B-DAT

the O
split O
1 O
of O
the O
IJB B-DAT

5: O
Clustering O
metrics O
over O
the O
IJB B-DAT

Clustering O
IJB-A:- B-DAT
The O
IJB O

Precision-Recall O
(PR) O
curve O
for O
the O
IJB B-DAT

PR O
curve O
for O
clustering O
the O
IJB B-DAT

method O
on O
two O
challenging O
datasets: O
IJB B-DAT

algorithm O
on O
the O
LFW O
and O
IJB B-DAT

A B-DAT
dataset O
show O
that O
the O
proposed O

A B-DAT
and O
LFW O
datasets O

verification O
problem O
can O
be O
solved. O
A B-DAT
face O
verification O
algorithm O
compares O
two O

A B-DAT
dataset O
([2], O
[3]) O
that O
was O

A B-DAT
dataset, O
the O
data O
is O
divided O

A B-DAT
and O
CFP O
datasets O
and O
a O

A B-DAT
datasets O

A B-DAT
dataset O
[2], O
there O
have O
been O

A B-DAT
dataset O
(Table O
2 O
in O
Results O

A B-DAT
split O
1: O
FAR O
(vs) O
TAR O

A B-DAT
verify O
protocol O
for O
the O
three O

A B-DAT
dataset O

A B-DAT
(IJB-A) O
[2]: O
This O
dataset O

A B-DAT
dataset O
contain O
extreme O
poses O
and O

A B-DAT
dataset O
are O
shown O
in O
Figure O

A B-DAT
verification O
protocol O
is O
that O
the O

A B-DAT
data O
we O
perform O
two O
kinds O

A B-DAT
gives O
us O
the O
media O
id O

A B-DAT
dataset O

A B-DAT
dataset, O
since O
most O
images O
contain O

A B-DAT
dataset, O
there O
are O
few O
images O

A B-DAT
dataset, O
we O
use O
the O
training O

A B-DAT
Verification O
(FNMR@FMR) O
IJB-A O
Identification O

A B-DAT
dataset. O
For O
identification, O
the O
scores O

A B-DAT
dataset, O
given O
a O
template O
containing O

A B-DAT
dataset: O
Veri- O
fication O
and O
Identification O

A B-DAT
proto- O
col O
can O
be O
found O

A B-DAT

A B-DAT
Verification O
and O
Iden- O
tification O
protocol O

A B-DAT
dataset. O
• O
Parkhi O
et O
al O

A B-DAT
dataset O
contain O
extreme O
poses, O
harsh O

A B-DAT
dataset O

A B-DAT
dataset, O
with O
the O
exception O
of O

A B-DAT
splits. O
Let’s O
denote O
the O
pooled O

A B-DAT
dataset O
and O
cluster O
the O
templates O

A B-DAT
verify O
protocol O

A B-DAT
dataset. O
Top O
row O
(a,b) O
shows O

A B-DAT
1:1 O
protocol. O
The O
standard O
deviation O

A B-DAT

A B-DAT
dataset O
is O
processed O
as O
de O

A B-DAT
clustering O
experiment O
in O
Figure O
6 O

A B-DAT
data O
using O
embedded O
features O
exhibits O

A B-DAT
and O
CFP O
and O
achieved O
performance O

A B-DAT
datasets. O
For O
future O
work, O
we O

Labeled O
faces O
in O
the O
wild: O
A B-DAT
database O
for O
studying O
face O
recognition O

B. O
F. O
Klare, O
E. O
Taborsky, O
A B-DAT

Cheney, O
K. O
Allen, B-DAT
P. O
Grother, O
A O

. O
Mah, O
M. O
Burge, O
and O
A B-DAT

Kalenichenko, O
and O
J. O
Philbin, O
“Facenet: O
A B-DAT
unified O
embedding O
for O
face O
recognition O

D. O
Wang, O
C. O
Otto, O
and O
A B-DAT

7] O
O. O
M. O
Parkhi, O
A B-DAT

. O
Vedaldi, O
and O
A B-DAT

K. O
Simonyan, O
O. O
M. O
Parkhi, O
A B-DAT

. O
Vedaldi, O
and O
A B-DAT

S. O
Belongie, O
O. O
Shamir, O
and O
A B-DAT

16] O
A B-DAT

Patel, O
and O
R. O
Chellappa, O
“Hyperface: O
A B-DAT
deep O
multi-task O
learning O
framework O
for O

23] O
I. O
Masi, O
A B-DAT

D. O
Erhan, O
V. O
Vanhoucke, O
and O
A B-DAT

C. O
Stauffer, O
Q. O
Cao, O
and O
A B-DAT

C. O
Otto, O
D. O
Wang, O
and O
A B-DAT

tested O
on O
the O
LFW O
and O
IJB-A B-DAT
(verification O
and O
identification) O
benchmarks O
and O

proach O
on O
the O
LFW O
[11], O
IJB-A B-DAT
(verification O
and O
identification) O
and O
CS2 O

outperforms O
standard O
fusion O
techniques O
on O
IJB-A, B-DAT
in O
which O
subjects O
are O
described O

Fusion↓ O
IJB-A B-DAT
Ver. O
(TAR) O
IJB-A O
Id. O
(Rec. O
Rate) O
Metrics O

fusion O
tech- O
niques O
on O
the O
IJB-A B-DAT
benchmark O
for O
verification O
(ROC) O
and O

test O
time O
matching O
methods O
on O
IJB-A B-DAT

5.1 O
Results O
on O
the O
IJB-A B-DAT
benchmarks O

IJB-A B-DAT
is O
a O
new O
publicly O
available O

identification O
and O
verification O
methods. O
Both O
IJB-A B-DAT
and O
the O
Janus O
CS2 O
benchmark O

and O
illu- O
mination O
variations, O
with O
IJB-A B-DAT
splits O
generally O
considered O
more O
difficult O

than O
those O
in O
CS2. O
The O
IJB-A B-DAT
benchmarks O
consist O
of O
face O
verification O

3 O
IJB-A B-DAT
data O
and O
splits O
are O
available O

Augmentation O
↓ O
IJB-A B-DAT
Ver. O
(TAR) O
IJB-A O
Id. O
(Rec. O
Rate) O
IJB-A O
Ver O

. O
(TAR) O
IJB-A B-DAT
Id. O
(Rec. O
Rate O

Effect O
of O
each O
augmentation O
on O
IJB-A B-DAT
performance O
on O
verification O
(ROC) O
and O

In O
all O
our O
IJB-A B-DAT
and O
Janus O
CS2 O
results O
this O

Image O
type O
↓ O
IJB-A B-DAT
Ver. O
(TAR) O
IJB-A O
Id. O
(Rec O

. O
Rate) O
IJB-A B-DAT
Ver. O
(TAR) O
IJB-A O
Id. O
(Rec. O
Rate O

synthesis O
at O
test-time O
(matching) O
on O
IJB-A B-DAT
dataset O
respectively O
for O
verification O
(ROC O

tech- O
nique O
on O
the O
challenging O
IJB-A B-DAT
dataset. O
Clearly, O
the O
biggest O
contribution O

achieving O
state-of-the-art O
performance O
on O
the O
IJB-A B-DAT
benchmark. O
We O
conjecture O
that O
this O

JANUS O
CS2 O
Id. O
(Rec. O
Rate) O
IJB-A B-DAT
Ver. O
(TAR) O
IJB-A O
Id. O
(Rec O

analysis O
on O
JANUS O
CS2 O
and O
IJB-A B-DAT
respectively O
for O
verification O
(ROC) O
and O

the O
art O
results O
in O
the O
IJB-A B-DAT
benchmark O
and O
Janus O
CS2 O
dataset O

Moreover, O
our O
method O
improves O
in O
IJB-A B-DAT
verification O
over O
[37] O
in O
15 O

tested O
on O
the O
LFW O
and O
IJB B-DAT

proach O
on O
the O
LFW O
[11], O
IJB B-DAT

outperforms O
standard O
fusion O
techniques O
on O
IJB B-DAT

Fusion↓ O
IJB-A B-DAT
Ver. O
(TAR) O
IJB O

fusion O
tech- O
niques O
on O
the O
IJB B-DAT

test O
time O
matching O
methods O
on O
IJB B-DAT

5.1 O
Results O
on O
the O
IJB B-DAT

IJB B-DAT

identification O
and O
verification O
methods. O
Both O
IJB B-DAT

and O
illu- O
mination O
variations, O
with O
IJB B-DAT

than O
those O
in O
CS2. O
The O
IJB B-DAT

3 O
IJB B-DAT

Augmentation O
↓ O
IJB-A B-DAT
Ver. O
(TAR) O
IJB O

-A O
Id. O
(Rec. O
Rate) O
IJB B-DAT

-A O
Ver. O
(TAR) O
IJB B-DAT

Effect O
of O
each O
augmentation O
on O
IJB B-DAT

In O
all O
our O
IJB B-DAT

Image O
type O
↓ O
IJB B-DAT

-A O
Ver. O
(TAR) O
IJB B-DAT

-A O
Id. O
(Rec. O
Rate) O
IJB B-DAT

-A O
Ver. O
(TAR) O
IJB B-DAT

synthesis O
at O
test-time O
(matching) O
on O
IJB B-DAT

tech- O
nique O
on O
the O
challenging O
IJB B-DAT

achieving O
state-of-the-art O
performance O
on O
the O
IJB B-DAT

JANUS O
CS2 O
Id. O
(Rec. O
Rate) O
IJB B-DAT

-A O
Ver. O
(TAR) O
IJB B-DAT

analysis O
on O
JANUS O
CS2 O
and O
IJB B-DAT

the O
art O
results O
in O
the O
IJB B-DAT

Moreover, O
our O
method O
improves O
in O
IJB B-DAT

A B-DAT
(verification O
and O
identification) O
benchmarks O
and O

A B-DAT

2 O
I. O
Masi, O
A B-DAT

A B-DAT
(verification O
and O
identification) O
and O
CS2 O

augmenta- O
tion O
method O
(Sec. O
2): O
A B-DAT
domain O
specific O
data O
augmentation O
approach O

4 O
I. O
Masi, O
A B-DAT

6 O
I. O
Masi, O
A B-DAT

8 O
I. O
Masi, O
A B-DAT

A, B-DAT
in O
which O
subjects O
are O
described O

10 O
I. O
Masi, O
A B-DAT

A B-DAT
Ver. O
(TAR) O
IJB-A O
Id. O
(Rec O

A B-DAT
benchmark O
for O
verification O
(ROC) O
and O

A B-DAT

A B-DAT

A B-DAT

A B-DAT
benchmarks O

A B-DAT
is O
a O
new O
publicly O
available O

A B-DAT
and O
the O
Janus O
CS2 O
benchmark O

A B-DAT
splits O
generally O
considered O
more O
difficult O

A B-DAT
benchmarks O
consist O
of O
face O
verification O

A B-DAT
data O
and O
splits O
are O
available O

12 O
I. O
Masi, O
A B-DAT

A B-DAT
Ver. O
(TAR) O
IJB-A O
Id. O
(Rec O

A B-DAT
Ver. O
(TAR) O
IJB-A O
Id. O
(Rec O

A B-DAT
performance O
on O
verification O
(ROC) O
and O

A B-DAT
and O
Janus O
CS2 O
results O
this O

A B-DAT
Ver. O
(TAR) O
IJB-A O
Id. O
(Rec O

A B-DAT
Ver. O
(TAR) O
IJB-A O
Id. O
(Rec O

A B-DAT
dataset O
respectively O
for O
verification O
(ROC O

A B-DAT
dataset. O
Clearly, O
the O
biggest O
contribution O

A B-DAT
benchmark. O
We O
conjecture O
that O
this O

A B-DAT
Ver. O
(TAR) O
IJB-A O
Id. O
(Rec O

A B-DAT
respectively O
for O
verification O
(ROC) O
and O

times O
for O
each O
training O
split. O
A B-DAT
network O
trained O
once O
with O
our O

A B-DAT
benchmark O
and O
Janus O
CS2 O
dataset O

A B-DAT
verification O
over O
[37] O
in O
15 O

14 O
I. O
Masi, O
A B-DAT

A B-DAT
cc O

16 O
I. O
Masi, O
A B-DAT

3. O
K. O
Chatfield, O
K. O
Simonyan, O
A B-DAT

. O
Vedaldi, O
and O
A B-DAT

8. O
R. O
Hartley O
and O
A B-DAT

Labeled O
faces O
in O
the O
wild: O
A B-DAT
database O
for O
studying O
face O
recognition O

Klare, O
B. O
Klein, O
E. O
Taborsky, O
A B-DAT

Cheney, O
K. O
Allen, B-DAT
P. O
Grother, O
A O

. O
Mah, O
M. O
Burge, O
and O
A B-DAT

and O
recognition: O
IARPA B-DAT
Janus O
Benchmark O
A O

E. O
Taborsky, O
M. O
Burge, O
and O
A B-DAT

16. O
A B-DAT

S. O
Lawrence, O
C. O
L. O
Giles, O
A B-DAT

. O
C. O
Tsoi, O
and O
A B-DAT

. O
D. O
Back. O
Face O
recognition: O
A B-DAT
con- O
volutional O
neural-network O
approach. O
Trans O

M. O
H. O
Nguyen, O
J.-F. O
Lalonde, O
A B-DAT

. O
A B-DAT

O. O
M. O
Parkhi, O
K. O
Simonyan, O
A B-DAT

. O
Vedaldi, O
and O
A. B-DAT
Zisserman. O
A O
compact O
and O
discrim- O
inative O
face O

24. O
O. O
M. O
Parkhi, O
A B-DAT

. O
Vedaldi, O
and O
A B-DAT

S. O
Romdhani, O
and O
T. O
Vetter. O
A B-DAT
3d O
face O
model O
for O
pose O

Satheesh, O
S. O
Ma, O
Z. O
Huang, O
A B-DAT

. O
Karpa- O
thy, O
A B-DAT

28. O
S. O
Sankaranarayanan, O
A B-DAT

Kalenichenko, O
and O
J. O
Philbin. O
Facenet: O
A B-DAT
unified O
embedding O
for O
face O
recognition O

30. O
K. O
Simonyan O
and O
A B-DAT

D. O
Wang, O
C. O
Otto, O
and O
A B-DAT

18 O
I. O
Masi, O
A B-DAT

released O
IARPA O
Janus O
Benchmark O
A O
(IJB-A) B-DAT
dataset O
as O
well O
as O
on O

the O
Wild O
(LFW) O
dataset. O
The O
IJB-A B-DAT
dataset O
includes O
real- O
world O
unconstrained O

of O
experimental O
evaluations O
on O
the O
IJB-A B-DAT
and O
the O
LFW O
datasets O
are O

face O
matchers O
on O
the O
challenging O
IJB-A B-DAT
dataset O
which O
con- O
tains O
significant O

on O
the O
CASIA-WebFace, O
and O
the O
IJB-A B-DAT
datasets O
to O
local- O
ize O
and O

the O
training O
sets O
of O
the O
IJB-A B-DAT
dataset O
and O
the O
DCNN O
features O

traditional O
EM O
is O
that O
for O
IJB-A B-DAT
training O
and O
test O
data, O
some O

image. O
More O
details O
about O
the O
IJB-A B-DAT
dataset O
are O
given O
in O
Section O

27 O
overlapping O
subjects O
with O
the O
IJB-A B-DAT
dataset, O
there O
are O
10548 O
subjects O

challenging O
IARPA O
Janus O
Benchmark O
A O
(IJB-A) B-DAT
[20], O
its O
extended O
version O
Janus O

and O
im- O
ages O
in O
the O
IJB-A B-DAT
but O
also O
the O
original O
videos O

the O
defined O
protocols O
than O
the O
IJB-A B-DAT
dataset. O
The O
receiver O
operating O
character O

4.1. O
JANUS-CS2 O
and O
IJB-A B-DAT

Both O
the O
IJB-A B-DAT
and O
JANUS O
CS2 O
contain O
500 O

the O
JANUS O
CS2 O
dataset. O
The O
IJB-A B-DAT
evaluation O
protocol O
consists O
of O
verification O

The O
main O
differ- O
ences O
between O
IJB-A B-DAT
and O
JANUS O
CS2 O
evaluation O
protocol O

are O
(1) O
IJB-A B-DAT
considers O
the O
open-set O
identification O
problem O

the O
closed-set O
identification O
and O
(2) O
IJB-A B-DAT
considers O
the O
more O
difficult O
pairs O

images O
and O
frames O
from O
the O
IJB-A B-DAT
and O
JANUS O
CS2 O
datasets. O
A O

Both O
the O
IJB-A B-DAT
and O
the O
JANUS O
CS2 O
datasets O

uate O
the O
verification O
performance, O
the O
IJB-A B-DAT
and O
JANUS O
CS2 O
both O
divide O

34], O
the O
images O
in O
the O
IJB-A B-DAT
and O
JANUS O
CS2 O
contain O
extreme O

These O
factors O
essentially O
make O
the O
IJB-A B-DAT
and O
JANUS O
CS2 O
challenging O
face O

4.2. O
Evaluation O
on O
JANUS-CS2 O
and O
IJB-A B-DAT

results O
for O
JANUS O
CS2 O
and O
IJB-A B-DAT
datasets O
obtained O
after O
the O
paper O

and O
evaluates O
it O
on O
the O
IJB-A B-DAT
dataset. O
The O
method O
proposed O
in O

in O
[8] O
previously O
for O
the O
IJB-A B-DAT
iden- O
tification O
task O
because O
one O

time. O
The O
current O
metadata O
of O
IJB-A B-DAT
has O
fixed O
those O
errors O
already O

Figure O
5. O
Results O
on O
the O
IJB-A B-DAT
dataset. O
(a) O
the O
average O
ROC O

curves O
for O
the O
IJB-A B-DAT
verification O
protocol O
and O
(b) O
the O

average O
CMC O
curves O
for O
IJB-A B-DAT
identification O
protocol O
over O
10 O
splits O

IJB-A B-DAT

IJB-A B-DAT

Table O
3. O
Results O
on O
the O
IJB-A B-DAT
dataset. O
The O
TAR O
of O
all O

the O
proposed O
DCNN O
on O
the O
IJB-A B-DAT
dataset O
is O
much O
better O
than O

negative O
pairs O
from O
CASIA-Webface O
and O
IJB-A B-DAT
training O
datasets O
to O
fully O
utilize O

4.1 O
. O
JANUS-CS2 O
and O
IJB-A B-DAT

Evaluation O
on O
JANUS-CS2 O
and O
IJB-A B-DAT

released O
IARPA O
Janus O
Benchmark O
A O
(IJB B-DAT

the O
Wild O
(LFW) O
dataset. O
The O
IJB B-DAT

of O
experimental O
evaluations O
on O
the O
IJB B-DAT

face O
matchers O
on O
the O
challenging O
IJB B-DAT

on O
the O
CASIA-WebFace, O
and O
the O
IJB B-DAT

the O
training O
sets O
of O
the O
IJB B-DAT

traditional O
EM O
is O
that O
for O
IJB B-DAT

image. O
More O
details O
about O
the O
IJB B-DAT

27 O
overlapping O
subjects O
with O
the O
IJB B-DAT

challenging O
IARPA O
Janus O
Benchmark O
A O
(IJB B-DAT

and O
im- O
ages O
in O
the O
IJB B-DAT

the O
defined O
protocols O
than O
the O
IJB B-DAT

4.1. O
JANUS-CS2 O
and O
IJB B-DAT

Both O
the O
IJB B-DAT

the O
JANUS O
CS2 O
dataset. O
The O
IJB B-DAT

The O
main O
differ- O
ences O
between O
IJB B-DAT

CS2 O
evaluation O
protocol O
are O
(1) O
IJB B-DAT

the O
closed-set O
identification O
and O
(2) O
IJB B-DAT

images O
and O
frames O
from O
the O
IJB B-DAT

Both O
the O
IJB B-DAT

uate O
the O
verification O
performance, O
the O
IJB B-DAT

34], O
the O
images O
in O
the O
IJB B-DAT

These O
factors O
essentially O
make O
the O
IJB B-DAT

4.2. O
Evaluation O
on O
JANUS-CS2 O
and O
IJB B-DAT

results O
for O
JANUS O
CS2 O
and O
IJB B-DAT

and O
evaluates O
it O
on O
the O
IJB B-DAT

in O
[8] O
previously O
for O
the O
IJB B-DAT

time. O
The O
current O
metadata O
of O
IJB B-DAT

Figure O
5. O
Results O
on O
the O
IJB B-DAT

average O
ROC O
curves O
for O
the O
IJB B-DAT

the O
average O
CMC O
curves O
for O
IJB B-DAT

IJB B-DAT

IJB B-DAT

Table O
3. O
Results O
on O
the O
IJB B-DAT

the O
proposed O
DCNN O
on O
the O
IJB B-DAT

negative O
pairs O
from O
CASIA-Webface O
and O
IJB B-DAT

4.1 O
. O
JANUS-CS2 O
and O
IJB B-DAT

Evaluation O
on O
JANUS-CS2 O
and O
IJB B-DAT

newly O
released O
IARPA B-DAT
Janus O
Benchmark O
A O
(IJB-A) O
dataset O
as O
well O
as O

A B-DAT
dataset O
includes O
real- O
world O
unconstrained O

A B-DAT
and O
the O
LFW O
datasets O
are O

A B-DAT
dataset O
which O
con- O
tains O
significant O

A B-DAT
datasets O
to O
local- O
ize O
and O

A B-DAT
dataset O
and O
the O
DCNN O
features O

A B-DAT
DCNN O
with O
small O
filters O
and O

A B-DAT
training O
and O
test O
data, O
some O

A B-DAT
dataset O
are O
given O
in O
Section O

A B-DAT
dataset, O
there O
are O
10548 O
subjects O

the O
challenging O
IARPA B-DAT
Janus O
Benchmark O
A O
(IJB-A) O
[20], O
its O
extended O
version O

A B-DAT
but O
also O
the O
original O
videos O

A B-DAT
dataset. O
The O
receiver O
operating O
character O

A B-DAT

A B-DAT
and O
JANUS O
CS2 O
contain O
500 O

A B-DAT
evaluation O
protocol O
consists O
of O
verification O

A B-DAT
and O
JANUS O
CS2 O
evaluation O
protocol O

A B-DAT
considers O
the O
open-set O
identification O
problem O

A B-DAT
considers O
the O
more O
difficult O
pairs O

IJB-A B-DAT
and O
JANUS O
CS2 O
datasets. O
A O
variety O
of O
challenging O
variations O
on O

A B-DAT
and O
the O
JANUS O
CS2 O
datasets O

A B-DAT
and O
JANUS O
CS2 O
both O
divide O

A B-DAT
and O
JANUS O
CS2 O
contain O
extreme O

A B-DAT
and O
JANUS O
CS2 O
challenging O
face O

A B-DAT

A B-DAT
datasets O
obtained O
after O
the O
paper O

A B-DAT
dataset. O
The O
method O
proposed O
in O

A B-DAT
iden- O
tification O
task O
because O
one O

A B-DAT
has O
fixed O
those O
errors O
already O

A B-DAT

A B-DAT
R O

A B-DAT

A B-DAT
R O

A B-DAT
dataset. O
(a) O
the O
average O
ROC O

A B-DAT
verification O
protocol O
and O
(b) O
the O

A B-DAT
identification O
protocol O
over O
10 O
splits O

A B-DAT

A B-DAT

A B-DAT
0.884±0.025 O
0.934±0.016 O
0.954±0.007 O
0.974±0.005 O
0.977±0.007 O

A B-DAT
dataset. O
The O
TAR O
of O
all O

A B-DAT
N/A O
N/A O
99.20% O
Ours O
1 O

face O
verification O
dataset, O
IARPA B-DAT
Benchmark O
A, O
which O
contains O
faces O
with O
full O

A B-DAT
dataset O
is O
much O
better O
than O

A B-DAT
training O
datasets O
to O
fully O
utilize O

References O
[1] O
T. O
Ahonen, B-DAT
A O

2] O
A B-DAT

3] O
A B-DAT

Q. O
Duan, O
and O
J. O
Sun. O
A B-DAT
practical O
transfer O
learning O
algorithm O
for O

J. O
Sun. O
Bayesian O
face O
revisited: O
A B-DAT
joint O
formulation. O
In O
European O
Conference O

Tzeng, O
and O
T. O
Darrell. O
Decaf: O
A B-DAT
deep O
convolutional O
acti- O
vation O
feature O

Klare, O
B. O
Klein, O
E. O
Taborsky, O
A B-DAT

Cheney, O
K. O
Allen, B-DAT
P. O
Grother, O
A O

. O
Mah, O
M. O
Burge, O
and O
A B-DAT

and O
recognition: O
IARPA B-DAT
Janus O
Benchmark O
A O

21] O
A B-DAT

O. O
M. O
Parkhi, O
K. O
Simonyan, O
A B-DAT

. O
Vedaldi, O
and O
A. B-DAT
Zisserman. O
A O
compact O
and O
discriminative O
face O
track O

Kalenichenko, O
and O
J. O
Philbin. O
Facenet: O
A B-DAT
uni- O
fied O
embedding O
for O
face O

K. O
Simonyan, O
O. O
M. O
Parkhi, O
A B-DAT

. O
Vedaldi, O
and O
A B-DAT

27] O
K. O
Simonyan O
and O
A B-DAT

D. O
Erhan, O
V. O
Vanhoucke, O
and O
A B-DAT

Y. O
Taigman, O
M. O
Yang, O
M. O
A B-DAT

D. O
Wang, O
C. O
Otto, O
and O
A B-DAT

Chellappa, O
P. O
J. O
Phillips, O
and O
A B-DAT

. O
Rosenfeld. O
Face O
recognition: O
A B-DAT
literature O
survey. O
ACM O
Computing O
Sur O

A B-DAT

A B-DAT

on O
IARPA’s O
CS2 O
and O
NIST’s O
IJB-A B-DAT
in O
both O
verification O
and O
identification O

Technology’s O
(NIST) O
IARPA O
Janus O
Benchmark-A O
(IJB-A), B-DAT
which O
is O
considered O
much O
more O

IJB-A B-DAT
dataset O
has O
been O
quickly O
adopted O

novel O
approach O
is O
applied O
to O
IJB-A B-DAT
dataset O
and O
is O
shown O
to O

of O
the O
same O
sub- O
ject. O
IJB-A B-DAT
and O
CS2 O
share O
90% O
of O

in O
evaluation O
protocols. O
In O
particular, O
IJB-A B-DAT
in- O
cludes O
protocols O
for O
both O

As O
mentioned O
before, O
since O
the O
IJB-A B-DAT
dataset O
uses O
the O
no- O
tion O

preprocessed O
images. O
Testing O
a O
single O
IJB-A B-DAT

20 O
minutes O
for O
a O
single O
IJB-A B-DAT

IJB-A B-DAT
dataset O
contains O
two O
types O
of O

accuracy O
between O
two O
templates. O
However, O
IJB-A B-DAT
carefully O
designed O
challenging O
template O
pairs O

COTS O
from O
[11] O
on O
the O
IJB-A B-DAT
dataset. O
It O
is O
clear O
that O

involves O
both O
fine O
tuning O
on O
IJB-A B-DAT
data O
and O
uses O
metric O
learning O

using O
labeled O
IJB-A B-DAT
training O
data, O
while O
our O
algorithm O

box O
on O
both O
CS2 O
and O
IJB-A B-DAT
without O
target O
domain O
specific O
tuning O

Table O
8: O
1:N O
Results O
on O
IJB-A B-DAT

face O
recognition O
per- O
formance O
on O
IJB-A B-DAT
benchmark O
compared O
not O
only O
to O

on O
IARPA’s O
CS2 O
and O
NIST’s O
IJB B-DAT

Technology’s O
(NIST) O
IARPA O
Janus O
Benchmark-A O
(IJB B-DAT

IJB B-DAT

novel O
approach O
is O
applied O
to O
IJB B-DAT

22] O
for O
training O
and O
both O
IJB B-DAT

of O
the O
same O
sub- O
ject. O
IJB B-DAT

in O
evaluation O
protocols. O
In O
particular, O
IJB B-DAT

As O
mentioned O
before, O
since O
the O
IJB B-DAT

preprocessed O
images. O
Testing O
a O
single O
IJB B-DAT

20 O
minutes O
for O
a O
single O
IJB B-DAT

IJB B-DAT

accuracy O
between O
two O
templates. O
However, O
IJB B-DAT

COTS O
from O
[11] O
on O
the O
IJB B-DAT

involves O
both O
fine O
tuning O
on O
IJB B-DAT

uses O
metric O
learning O
using O
labeled O
IJB B-DAT

box O
on O
both O
CS2 O
and O
IJB B-DAT

Table O
8: O
1:N O
Results O
on O
IJB B-DAT

face O
recognition O
per- O
formance O
on O
IJB B-DAT

A B-DAT
in O
both O
verification O
and O
identification O

A B-DAT
(IJB-A), O
which O
is O
considered O
much O

A B-DAT
dataset O
has O
been O
quickly O
adopted O

A B-DAT
dataset O
and O
is O
shown O
to O

for O
training O
and O
both O
IJB- O
A B-DAT
[11] O
and O
IARPA’s O
Janus O
CS2 O

A B-DAT
and O
CS2 O
share O
90% O
of O

A B-DAT
in- O
cludes O
protocols O
for O
both O

A B-DAT
facial O
landmark O
detection O
algorithm, O
lmd O

A B-DAT
dataset O
uses O
the O
no- O
tion O

A B-DAT

A B-DAT

A B-DAT
dataset O
contains O
two O
types O
of O

A B-DAT
carefully O
designed O
challenging O
template O
pairs O

A B-DAT
dataset. O
It O
is O
clear O
that O

A B-DAT
data O
and O
uses O
metric O
learning O

A B-DAT
training O
data, O
while O
our O
algorithm O

A B-DAT
without O
target O
domain O
specific O
tuning O

A B-DAT

A B-DAT
benchmark O
compared O
not O
only O
to O

References O
[1] O
T. O
Ahonen, B-DAT
A O

Labeled O
faces O
in O
the O
wild: O
A B-DAT
database O
for O
studying O
face O
recognition O

B. O
F. O
Klare, O
E. O
Taborsky, O
A B-DAT

Cheney, O
K. O
Allen, B-DAT
P. O
Grother, O
A O

. O
Mah, O
M. O
Burge, O
and O
A B-DAT

12] O
A B-DAT

15] O
A B-DAT

18] O
K. O
Simonyan O
and O
A B-DAT

D. O
Wang, O
C. O
Otto, O
and O
A B-DAT

face O
recognition O
benchmarks O
(LFW O
and O
IJB-A B-DAT

the O
BLUFR O
protocol. O
For O
the O
IJB-A B-DAT
benchmark, O
our O
accuracies O
are O
as O

Viola-Jones O
face O
detector), O
and O
the O
IJB-A B-DAT
dataset O
(includes O
faces O
which O
are O

on O
the O
LFW O
[3] O
and O
IJB-A B-DAT
[9] O
face O
datasets O
with O
an O

of O
unconstrained O
face O
recognition, O
the O
IJB-A B-DAT
dataset O
was O
released O
in O
2015 O

9]. O
IJB-A B-DAT
contains O
face O
images O
that O
are O

828 O
4, O
500 O
80M+ O
N/A O
IJB-A B-DAT
[9] O
+ O
LFW O
[3 O

PCSO O
(b) O
LFW O
[3] O
(c) O
IJB-A B-DAT
[9] O
(d) O
CASIA O
[6] O
(e O

our O
experiments: O
PCSO, O
LFW O
[3], O
IJB-A B-DAT
[9], O
CASIA-WebFace O
[6] O
(abbreviated O
as O

IJB-A B-DAT
[9] O
IARPA O
Janus O
Benchmark-A O
(IJB-A) O
contains O
500 O
subjects O
with O
a O

to O
the O
LFW O
dataset, O
the O
IJB-A B-DAT
dataset O
is O
more O
challenging O
due O

locations O
are O
provided O
with O
the O
IJB-A B-DAT
dataset O
(and O
used O
in O
our O

two O
different O
subjects O
in O
the O
IJB-A B-DAT
dataset, O
captured O
in O
various O
conditions O

recognition O
benchmarks O
(LFW O
[3] O
and O
IJB-A B-DAT
[9]) O
to O
establish O
its O
performance O

5.3 O
IJB-A B-DAT
Evaluation O

The O
IJB-A B-DAT
dataset O
[9] O
was O
released O
in O

in O
the O
LFW O
protocols, O
the O
IJB-A B-DAT
dataset O
contains O
more O
challenging O
face O

in O
the O
first O
folder O
of O
IJB-A B-DAT
protocol O
in O
1:N O
face O
search O

One O
unique O
aspect O
of O
the O
IJB-A B-DAT
evaluation O
protocol O
is O
that O
it O

examples O
of O
templates O
in O
the O
IJB-A B-DAT
protocol O
(one O
per O
row), O
with O

template. O
In O
particular, O
in O
the O
IJB-A B-DAT
evaluation O
protocol O
the O
number O
of O

The O
verification O
protocol O
in O
IJB-A B-DAT
consists O
of O
10 O
sets O
of O

or O
video O
frame O
from O
the O
IJB-A B-DAT
dataset, O
we O
first O
attempt O
to O

ground-truth O
landmarks O
provided O
with O
the O
IJB-A B-DAT
protocol. O
All O
possible O
ground O
truth O

of O
web O
images O
in O
the O
IJB-A B-DAT
dataset O
with O
overlayed O
landmarks O
(top O

0 O
ground-truth O
landmarks O
provided O
in O
IJB-A, B-DAT
respectively. O
DLIB O
fails O
to O
output O

these O
two O
categories O
in O
the O
IJB-A B-DAT
dataset O

The O
IJB-A B-DAT
protocol O
allows O
participants O
to O
perform O

for O
each O
fold. O
Since O
the O
IJB-A B-DAT
dataset O
is O
qualitatively O
different O
from O

our O
deep O
model O
using O
the O
IJB-A B-DAT
training O
set. O
The O
final O
face O

and O
one O
re-trained O
(on O
the O
IJB-A B-DAT
training O
set O
following O
the O
protocol O

Since O
all O
the O
IJB-A B-DAT
comparisons O
are O
defined O
between O
sets O

in O
1:N O
search O
protocol O
of O
IJB-A, B-DAT
averaged O
over O
10 O
folds. O
Correct O

we O
use O
include O
LFW O
and O
IJB-A B-DAT
data, O
but O
now O
we O
do O

closed-set O
1:N O
search O
protocol O
of O
IJB-A B-DAT

in O
first O
fold O
of O
the O
IJB-A B-DAT
closed-set O
1:N O
search O
protocol, O
using O

5 O
Recognition O
accuracies O
under O
the O
IJB-A B-DAT
protocol. O
Results O
for O
GOTS O
and O

10 O
folds O
specified O
in O
the O
IJB-A B-DAT
protocol O

We O
use O
the O
LFW O
and O
IJB-A B-DAT
datasets O
to O
construct O
the O
genuine O

a O
single O
image. O
For O
the O
IJB-A B-DAT
dataset, O
a O
similar O
process O
is O

IJB-A B-DAT
based O
probe O
and O
mate O
sets O

Genuine O
Probe O
Set O
IJB-A B-DAT
[3] O
500 O
10,868 O
Mate O
Set O

IJB-A B-DAT
[3] O
500 O
10,626 O

results O
for O
the O
LFW O
and O
IJB-A B-DAT
datasets O
are O
shown O
in O
Figs O

Search O
Evaluation O
on O
LFW O
and O
IJB-A B-DAT
datasets O

N O
(log-scale), O
on O
LFW O
and O
IJB-A B-DAT
datasets. O
The O
performance O
of O
COTS O

For O
both O
LFW O
and O
IJB-A B-DAT
face O
images, O
the O
recognition O
performance O

method O
individually. O
Results O
on O
the O
IJB-A B-DAT
dataset O
are O
overall O
similar O
to O

the O
overall O
performance O
on O
the O
IJB-A B-DAT
dataset O
is O
much O
lower O
than O

given O
the O
nature O
of O
the O
IJB-A B-DAT
dataset O

For O
both O
the O
LFW O
and O
IJB-A B-DAT
datasets, O
the O
open-set O
face O
search O

Search O
Evaluation O
on O
LFW O
and O
IJB-A B-DAT
datasets O

rate O
(FAR) O
on O
LFW O
and O
IJB-A B-DAT
datasets, O
using O
an O
80M O
face O

unconstrained O
face O
dataset, O
and O
the O
IJB-A B-DAT
dataset. O
On O
the O
mugshot O
data O

reported O
in O
[9] O
on O
the O
IJB-A B-DAT
dataset, O
as O
follows: O
TAR O
of O

on O
the O
LFW O
and O
the O
IJB-A B-DAT
benchmarks, O
we O
evaluate O
the O
proposed O

face O
recognition O
benchmarks O
(LFW O
and O
IJB B-DAT

the O
BLUFR O
protocol. O
For O
the O
IJB B-DAT

Viola-Jones O
face O
detector), O
and O
the O
IJB B-DAT

on O
the O
LFW O
[3] O
and O
IJB B-DAT

domain O
datasets O
LFW O
[3] O
and O
IJB B-DAT

of O
unconstrained O
face O
recognition, O
the O
IJB B-DAT

was O
released O
in O
2015 O
[9]. O
IJB B-DAT

828 O
4, O
500 O
80M+ O
N/A O
IJB B-DAT

PCSO O
(b) O
LFW O
[3] O
(c) O
IJB B-DAT

our O
experiments: O
PCSO, O
LFW O
[3], O
IJB B-DAT

IJB-A B-DAT
[9] O
IARPA O
Janus O
Benchmark-A O
(IJB O

to O
the O
LFW O
dataset, O
the O
IJB B-DAT

locations O
are O
provided O
with O
the O
IJB B-DAT

two O
different O
subjects O
in O
the O
IJB B-DAT

recognition O
benchmarks O
(LFW O
[3] O
and O
IJB B-DAT

5.3 O
IJB B-DAT

The O
IJB B-DAT

in O
the O
LFW O
protocols, O
the O
IJB B-DAT

in O
the O
first O
folder O
of O
IJB B-DAT

One O
unique O
aspect O
of O
the O
IJB B-DAT

examples O
of O
templates O
in O
the O
IJB B-DAT

template. O
In O
particular, O
in O
the O
IJB B-DAT

The O
verification O
protocol O
in O
IJB B-DAT

or O
video O
frame O
from O
the O
IJB B-DAT

ground-truth O
landmarks O
provided O
with O
the O
IJB B-DAT

of O
web O
images O
in O
the O
IJB B-DAT

0 O
ground-truth O
landmarks O
provided O
in O
IJB B-DAT

these O
two O
categories O
in O
the O
IJB B-DAT

The O
IJB B-DAT

for O
each O
fold. O
Since O
the O
IJB B-DAT

our O
deep O
model O
using O
the O
IJB B-DAT

and O
one O
re-trained O
(on O
the O
IJB B-DAT

Since O
all O
the O
IJB B-DAT

in O
1:N O
search O
protocol O
of O
IJB B-DAT

we O
use O
include O
LFW O
and O
IJB B-DAT

closed-set O
1:N O
search O
protocol O
of O
IJB B-DAT

in O
first O
fold O
of O
the O
IJB B-DAT

5 O
Recognition O
accuracies O
under O
the O
IJB B-DAT

10 O
folds O
specified O
in O
the O
IJB B-DAT

We O
use O
the O
LFW O
and O
IJB B-DAT

a O
single O
image. O
For O
the O
IJB B-DAT

IJB B-DAT

mate O
sets O
Genuine O
Probe O
Set O
IJB B-DAT

3] O
500 O
10,868 O
Mate O
Set O
IJB B-DAT

results O
for O
the O
LFW O
and O
IJB B-DAT

Deep O
Features O
(IJB−A) B-DAT
COTS O
(IJB O

−A) O
DF O
→ O
COTS O
(IJB B-DAT

Search O
Evaluation O
on O
LFW O
and O
IJB B-DAT

N O
(log-scale), O
on O
LFW O
and O
IJB B-DAT

For O
both O
LFW O
and O
IJB B-DAT

method O
individually. O
Results O
on O
the O
IJB B-DAT

the O
overall O
performance O
on O
the O
IJB B-DAT

given O
the O
nature O
of O
the O
IJB B-DAT

For O
both O
the O
LFW O
and O
IJB B-DAT

COTS O
(LFW) O
Deep O
Features O
(IJB B-DAT

−A) O
DF O
→ O
COTS O
(IJB B-DAT

Search O
Evaluation O
on O
LFW O
and O
IJB B-DAT

rate O
(FAR) O
on O
LFW O
and O
IJB B-DAT

unconstrained O
face O
dataset, O
and O
the O
IJB B-DAT

reported O
in O
[9] O
on O
the O
IJB B-DAT

on O
the O
LFW O
and O
the O
IJB B-DAT

A B-DAT

A B-DAT
benchmark, O
our O
accuracies O
are O
as O

scheme O
in O
the O
feature O
space. O
A B-DAT
vast O
majority O
of O
face O
recognition O

A B-DAT
large-scale O
face O
search O
system, O
leveraging O

A B-DAT
dataset O
(includes O
faces O
which O
are O

A B-DAT
[9] O
face O
datasets O
with O
an O

datasets O
LFW O
[3] O
and O
IJB- O
A B-DAT
[9 O

20K O
individuals O
(outside O
the O
protocol). O
A B-DAT
comparable O
result O
(99.63%) O
was O
achieved O

A B-DAT
dataset O
was O
released O
in O
2015 O

A B-DAT
contains O
face O
images O
that O
are O

A B-DAT
search O
operating O
in O
open-set O
protocol O

TABLE B-DAT
1 O
A O
summary O
of O
face O
search O
systems O

A B-DAT
1M+ O
N/A O
LFW O
[3 O

A B-DAT
FaceScrub O
[12] O
+ O
Yahoo O
Imagesb O

A B-DAT
201, O
196 O
N/A O
FERET O
[14 O

A B-DAT
116, O
028 O
N/A O
FRGC O
[16 O

A B-DAT
LFW O
[3] O
+ O
Web O
Faces O

A B-DAT
IJB-A O
[9] O
+ O
LFW O
[3 O

Fig. O
4. O
A B-DAT
face O
image O
alignment O
example. O
The O

m O
A B-DAT

A B-DAT
[9] O
(d) O
CASIA O
[6] O
(e O

A B-DAT
ve O

A B-DAT
[9], O
CASIA-WebFace O
[6] O
(abbreviated O
as O

A B-DAT
[9] O
IARPA O
Janus O
Benchmark-A O
(IJB-A O

A B-DAT
dataset O
is O
more O
challenging O
due O

A B-DAT
dataset O
(and O
used O
in O
our O

A B-DAT
dataset, O
captured O
in O
various O
conditions O

the O
Viola-Jones O
face O
detector O
[40]. O
A B-DAT
total O
of O
80 O
million O
face O

A B-DAT
[9]) O
to O
establish O
its O
performance O

A B-DAT
N/A O
N/A O
99.20% O
COTS O
N/A O

A B-DAT
N/A O
90.35%±1.30% O
Proposed O
Deep O
Model O

rates O
( O
FAR B-DAT
= O
0.1%). O
A O
number O
of O
new O
protocols O
for O

A B-DAT
58.56% O
36.44% O
Proposed O
Deep O
Model O

A B-DAT
Evaluation O

A B-DAT
dataset O
[9] O
was O
released O
in O

A B-DAT
dataset O
contains O
more O
challenging O
face O

A B-DAT
protocol O
in O
1:N O
face O
search O

A B-DAT
evaluation O
protocol O
is O
that O
it O

A B-DAT
protocol O
(one O
per O
row), O
with O

A B-DAT
evaluation O
protocol O
the O
number O
of O

A B-DAT
consists O
of O
10 O
sets O
of O

A B-DAT
dataset, O
we O
first O
attempt O
to O

A B-DAT
protocol. O
All O
possible O
ground O
truth O

A B-DAT
dataset O
with O
overlayed O
landmarks O
(top O

A, B-DAT
respectively. O
DLIB O
fails O
to O
output O

A B-DAT
dataset O

A B-DAT
protocol O
allows O
participants O
to O
perform O

A B-DAT
dataset O
is O
qualitatively O
different O
from O

A B-DAT
training O
set. O
The O
final O
face O

A B-DAT
training O
set O
following O
the O
protocol O

A B-DAT
comparisons O
are O
defined O
between O
sets O

A, B-DAT
averaged O
over O
10 O
folds. O
Correct O

for O
the O
third O
probe O
template. O
A B-DAT
template O
containing O
a O
single O
poorly-aligned O

A B-DAT
data, O
but O
now O
we O
do O

A B-DAT

A B-DAT
closed-set O
1:N O
search O
protocol, O
using O

A B-DAT
protocol. O
Results O
for O
GOTS O
and O

A B-DAT
protocol O

A B-DAT
datasets O
to O
construct O
the O
genuine O

A B-DAT
dataset, O
a O
similar O
process O
is O

A B-DAT
based O
probe O
and O
mate O
sets O

A B-DAT
[3] O
500 O
10,868 O
Mate O
Set O

A B-DAT
[3] O
500 O
10,626 O

A B-DAT
80,000,000 O

is O
computed O
with O
Eq. O
4. O
A B-DAT
basic O
assumption O
in O
our O
search O

A B-DAT
datasets O
are O
shown O
in O
Figs O

m O
A B-DAT

A) B-DAT
COTS O
(IJB−A) O
DF O
→ O
COTS O

A B-DAT

A B-DAT
datasets O

A B-DAT
datasets. O
The O
performance O
of O
COTS O

A B-DAT
face O
images, O
the O
recognition O
performance O

A B-DAT
dataset O
are O
overall O
similar O
to O

A B-DAT
dataset O
is O
much O
lower O
than O

A B-DAT
dataset O

A B-DAT
datasets, O
the O
open-set O
face O
search O

A B-DAT
ve O

A) B-DAT
DF O
→ O
COTS O
(IJB−A O

A B-DAT
datasets O

A B-DAT
datasets, O
using O
an O
80M O
face O

A B-DAT
0.34 O
0.4 O

A B-DAT
dataset. O
On O
the O
mugshot O
data O

A B-DAT
dataset, O
as O
follows: O
TAR O
of O

A B-DAT
benchmarks, O
we O
evaluate O
the O
proposed O

Labeled O
faces O
in O
the O
wild: O
A B-DAT
database O
for O
studying O
face O
recognition O

4] O
D. O
Wang O
and O
A B-DAT

J. O
Sun, O
“Bayesian O
face O
revisited: O
A B-DAT
joint O
formulation,” O
ECCV, O
2012. O
2 O

Klare, O
B. O
Klein, O
E. O
Taborsky, O
A B-DAT

Cheney, O
K. O
Allen, B-DAT
P. O
Grother, O
A O

. O
Mash, O
M. O
Burge, O
and O
A B-DAT

10] O
N. O
Kumar, O
A B-DAT

. O
C. O
Berg, O
A B-DAT

and O
S. O
M. O
Seitz, O
“Megaface: O
A B-DAT
million O
faces O
for O
recognition O
at O

A B-DAT
data-driven O
approach O
to O
cleaning O
large O

17] O
B. O
Klare, O
A B-DAT

Otto, O
B. O
F. O
Klare, O
and O
A B-DAT

A B-DAT
benchmark O
study O
of O
large-scale O
unconstrained O

20] O
A B-DAT

Kalenichenko, O
and O
J. O
Philbin, O
“Facenet: O
A B-DAT
unified O

A B-DAT
survey O
of O
content-based O

learning O
for O
content-based O
image O
retrieval: O
A B-DAT
comprehensive O
study,” O
ACM O
MM, O
2014 O

28] O
K. O
Simonyan O
and O
A B-DAT

30] O
A B-DAT

31] O
N. O
Srivastava, O
G. O
Hinton, O
A B-DAT

and O
R. O
Salakhut- O
dinov, O
“Dropout: O
A B-DAT
simple O
way O
to O
prevent O
neural O

34] O
A B-DAT

. O
Jain, O
K. O
Nandakumar, O
and O
A B-DAT

Y. O
Taigman, O
M. O
Yang, O
M. O
A B-DAT

42] O
J. O
Klontz O
and O
A B-DAT

A B-DAT
case O
study O
of O
automated O
face O

exceeds O
the O
state-of-the-art O
on O
the O
IJB-B B-DAT
face O
recognition O
dataset. O
This O
is O

pub- O
lic O
IJB-A O
[20] O
and O
IJB-B B-DAT
[37] O
face O
recognition O
benchmarks. O
These O

recognition O
datasets O
IJB-A O
[20] O
and O
IJB-B B-DAT
[37] O
are O
used O
for O
evaluation O

10, O
18, O
25], O
IJB-A O
and O
IJB-B B-DAT
are O
intended O
for O
template-based O
face O

and O
4.2 O
videos, O
respectively. O
The O
IJB-B B-DAT
dataset O
is O
an O
extension O
of O

benchmark O
procedure O
for O
IJB-A O
and O
IJB-B, B-DAT
and O
evaluate O
on O
“1:1 O
face O

defines O
10 O
test O
splits, O
while O
IJB-B B-DAT
only O
has O
one O
split O
for O

identification. O
For O
IJB-A O
and O
for O
IJB-B B-DAT
iden- O
tification, O
we O
report, O
as O

deployment. O
In O
the O
IJB-A O
and O
IJB-B B-DAT
datasets, O
there O
are O
images O
and O

4.3 O
Ablation O
studies O
on O
IJB-B B-DAT

it O
to O
baselines O
on O
the O
IJB-B B-DAT
dataset, O
as O
it O
is O
larger O

1: O
Verification O
performance O
on O
the O
IJB-B B-DAT
dataset. O
A O
higher O
value O
of O

2: O
Identification O
performance O
on O
the O
IJB-B B-DAT
dataset. O
A O
higher O
value O
of O

state-of-the-art O
on O
the O
IJB-A O
and O
IJB-B B-DAT
datasets. O
The O
currently O
best O
performing O

verification O
on O
both O
IJB-A O
and O
IJB-B B-DAT
datasets. O
In O
particular, O
it O
surpasses O

measures O
on O
the O
more O
challenging O
IJB-B B-DAT
benchmark, O
our O
network O
achieves O
the O

IJB-B B-DAT

verification O
on O
the O
IJB-A O
and O
IJB-B B-DAT
datasets. O
A O
higher O
value O
of O

vs O
0.705 O
for O
verification O
on O
IJB-B, B-DAT
and O
TPIR O
at O
FPIR=0.01 O
of O

0.743 O
for O
iden- O
tification O
on O
IJB-B B-DAT

vs O
0.671 O
for O
verification O
on O
IJB-B, B-DAT
and O
TPIR O
at O
FPIR=0.01 O
of O

vs O
0.706 O
for O
verification O
on O
IJB-B B-DAT

IJB-B B-DAT

identification O
on O
the O
IJB-A O
and O
IJB-B B-DAT
datasets. O
A O
higher O
value O
of O

Fig. O
3: O
Results O
on O
the O
IJB-B B-DAT
dataset. O
Our O
SE-GV-4-g1 O
network O
which O

on O
the O
challenging O
IJB-A O
and O
IJB-B B-DAT
benchmarks O
by O
a O
large O
margin O

a O
tem- O
plate O
in O
the O
IJB-B B-DAT
dataset. O
The O
contribution O
(relative O
to O

B B-DAT
face O
recognition O
dataset. O
This O
is O

B B-DAT
[37] O
face O
recognition O
benchmarks. O
These O

B B-DAT
[37] O
are O
used O
for O
evaluation O

B B-DAT
are O
intended O
for O
template-based O
face O

B B-DAT
dataset O
is O
an O
extension O
of O

B, B-DAT
and O
evaluate O
on O
“1:1 O
face O

B B-DAT
only O
has O
one O
split O
for O

B B-DAT
iden- O
tification, O
we O
report, O
as O

B B-DAT
datasets, O
there O
are O
images O
and O

B B-DAT

B B-DAT
dataset, O
as O
it O
is O
larger O

B B-DAT
dataset. O
A O
higher O
value O
of O

B B-DAT
dataset. O
A O
higher O
value O
of O

B B-DAT
datasets. O
The O
currently O
best O
performing O

B B-DAT
datasets. O
In O
particular, O
it O
surpasses O

B B-DAT
benchmark, O
our O
network O
achieves O
the O

B B-DAT

B B-DAT
datasets. O
A O
higher O
value O
of O

B, B-DAT
and O
TPIR O
at O
FPIR=0.01 O
of O

B B-DAT

B, B-DAT
and O
TPIR O
at O
FPIR=0.01 O
of O

B B-DAT

B B-DAT

B B-DAT
datasets. O
A O
higher O
value O
of O

B B-DAT
dataset. O
Our O
SE-GV-4-g1 O
network O
which O

B B-DAT
benchmarks O
by O
a O
large O
margin O

B B-DAT
dataset. O
The O
contribution O
(relative O
to O

6] O
Cevikalp, O
H., O
Triggs, O
B B-DAT

20] O
Klare, O
B., B-DAT
Klein, O
B O

Yu, O
Z., O
Li, O
M., O
Raj, O
B B-DAT

Taborsky, O
E., O
Blanton, B-DAT
A., O
Maze, O
B O

B B-DAT
face O
dataset. O
In: O
CVPR O
Workshop O

exceeds O
the O
state-of-the-art O
on O
the O
IJB B-DAT

margin O
on O
the O
pub- O
lic O
IJB B-DAT

-A O
[20] O
and O
IJB B-DAT

challenging O
public O
face O
recognition O
datasets O
IJB B-DAT

-A O
[20] O
and O
IJB B-DAT

as O
[5, O
10, O
18, O
25], O
IJB B-DAT

-A O
and O
IJB B-DAT

consider O
in O
this O
work. O
The O
IJB B-DAT

and O
4.2 O
videos, O
respectively. O
The O
IJB B-DAT

dataset O
is O
an O
extension O
of O
IJB B-DAT

the O
standard O
benchmark O
procedure O
for O
IJB B-DAT

-A O
and O
IJB B-DAT

apart O
from O
the O
fact O
that O
IJB B-DAT

defines O
10 O
test O
splits, O
while O
IJB B-DAT

two O
galleries O
for O
identification. O
For O
IJB B-DAT

-A O
and O
for O
IJB B-DAT

Network O
deployment. O
In O
the O
IJB B-DAT

-A O
and O
IJB B-DAT

4.3 O
Ablation O
studies O
on O
IJB B-DAT

it O
to O
baselines O
on O
the O
IJB B-DAT

larger O
and O
more O
challenging O
than O
IJB B-DAT

1: O
Verification O
performance O
on O
the O
IJB B-DAT

2: O
Identification O
performance O
on O
the O
IJB B-DAT

against O
the O
state-of-the-art O
on O
the O
IJB B-DAT

-A O
and O
IJB B-DAT

identification O
and O
verification O
on O
both O
IJB B-DAT

-A O
and O
IJB B-DAT

surpasses O
[44] O
marginally O
on O
the O
IJB B-DAT

to O
Rank-10 O
for O
identification O
on O
IJB B-DAT

-A; O
but O
this O
is O
because O
IJB B-DAT

measures O
on O
the O
more O
challenging O
IJB B-DAT

IJB B-DAT

IJB B-DAT

state-of-the-art O
for O
verification O
on O
the O
IJB B-DAT

-A O
and O
IJB B-DAT

vs O
0.705 O
for O
verification O
on O
IJB B-DAT

0.743 O
for O
iden- O
tification O
on O
IJB B-DAT

vs O
0.671 O
for O
verification O
on O
IJB B-DAT

vs O
0.706 O
for O
verification O
on O
IJB B-DAT

IJB B-DAT

IJB B-DAT

state-of-the-art O
for O
identification O
on O
the O
IJB B-DAT

-A O
and O
IJB B-DAT

Fig. O
3: O
Results O
on O
the O
IJB B-DAT

the O
state-of-the-art O
on O
the O
challenging O
IJB B-DAT

-A O
and O
IJB B-DAT

a O
tem- O
plate O
in O
the O
IJB B-DAT

two O
types O
of O
AFLW O
splits, O
AFLW-Full B-DAT
and O
AFLW-Frontal O
following O
[73]. O
AFLW-Full O

same O
training O
samples O
as O
in O
AFLW-Full, B-DAT
but O
only O
use O
the O
1165 O

CCL O
[73] O
Two-Stage O
[31] O
SAN O
AFLW-Full B-DAT
4.05 O
4.35 O
4.25 O
3.92 O
2.72 O

by O
more O
than O
11% O
on O
AFLW-Full B-DAT

Full B-DAT
and O
AFLW-Frontal O
following O
[73]. O
AFLW-Full O

Full, B-DAT
but O
only O
use O
the O
1165 O

Method O
Common O
Challenging O
Full B-DAT
Set O
SDM O
[64] O
5.57 O
15.40 O

Full B-DAT
4.05 O
4.35 O
4.25 O
3.92 O
2.72 O

Full B-DAT

achieve O
79.82 O
AUC@0.08 O
on O
AFLW- O
Full B-DAT
by O
only O
using O
the O
original O

algorithms O
on O
bench- O
mark O
datasets O
AFLW B-DAT
and O
300-W. O
Code O
is O
publicly O

300W-Styles O
(≈ O
12000 O
images) O
and O
AFLW B-DAT

transferring O
the O
300-W O
[46] O
and O
AFLW B-DAT
[23] O
into O
dif- O
ferent O
styles O

AFLW B-DAT
[23]. O
This O
dataset O
contains O
21997 O

excluding O
invisible O
landmark. O
Faces O
in O
AFLW B-DAT
usually O
have O
different O
pose, O
ex O

There O
are O
two O
types O
of O
AFLW B-DAT
splits, O
AFLW-Full O
and O
AFLW-Frontal O
following O

73]. O
AFLW B-DAT

samples O
and O
4386 O
testing O
samples. O
AFLW B-DAT

same O
training O
samples O
as O
in O
AFLW B-DAT

46, O
31, O
7, O
43]. O
For O
AFLW B-DAT
dataset, O
we O
use O
the O
face O

CCL O
[73] O
Two-Stage O
[31] O
SAN O
AFLW B-DAT

AFLW B-DAT

normalized O
mean O
(NME) O
errors O
on O
AFLW B-DAT
dataset O

Results O
on O
AFLW B-DAT

shows O
the O
performance O
comparison O
on O
AFLW B-DAT

by O
more O
than O
11% O
on O
AFLW B-DAT

-Full. O
On O
the O
AFLW B-DAT

mark O
datasets, O
e.g., O
300-W O
and O
AFLW B-DAT

300W-Style O
AFLW B-DAT

datasets O
based O
on O
300-W O
and O
AFLW B-DAT
with O
the O
original O
and O
three O

new O
datasets, O
300W-Style O
and O
AFLW B-DAT

from O
300-W O
for O
our O
300W-Style. O
AFLW B-DAT

as O
300W-Style, O
which O
transfer O
the O
AFLW B-DAT
dataset O
into O
three O
different O
styles O

using O
the O
training O
set O
of O
AFLW B-DAT

and O
the O
testing O
set O
of O
AFLW B-DAT

can O
achieve O
79.82 O
AUC@0.08 O
on O
AFLW B-DAT

by O
only O
using O
the O
original O
AFLW B-DAT
training O
set, O
while O
the O
data O

state-of-the-art O
performance O
on O
300-W O
and O
AFLW B-DAT
datasets O

39] O
J. O
W B-DAT

margin O
on O
the O
pub- O
lic O
IJB-A B-DAT
[20] O
and O
IJB-B O
[37] O
face O

challenging O
public O
face O
recognition O
datasets O
IJB-A B-DAT
[20] O
and O
IJB-B O
[37] O
are O

as O
[5, O
10, O
18, O
25], O
IJB-A B-DAT
and O
IJB-B O
are O
intended O
for O

consider O
in O
this O
work. O
The O
IJB-A B-DAT
dataset O
contains O
5,712 O
images O
and O

dataset O
is O
an O
extension O
of O
IJB-A B-DAT
with O
a O
total O
of O
11,754 O

the O
standard O
benchmark O
procedure O
for O
IJB-A B-DAT
and O
IJB-B, O
and O
evaluate O
on O

apart O
from O
the O
fact O
that O
IJB-A B-DAT
defines O
10 O
test O
splits, O
while O

two O
galleries O
for O
identification. O
For O
IJB-A B-DAT
and O
for O
IJB-B O
iden- O
tification O

Network O
deployment. O
In O
the O
IJB-A B-DAT
and O
IJB-B O
datasets, O
there O
are O

against O
the O
state-of-the-art O
on O
the O
IJB-A B-DAT
and O
IJB-B O
datasets. O
The O
currently O

identification O
and O
verification O
on O
both O
IJB-A B-DAT
and O
IJB-B O
datasets. O
In O
particular O

surpasses O
[44] O
marginally O
on O
the O
IJB-A B-DAT
verification O
task, O
despite O
the O
fact O

to O
Rank-10 O
for O
identification O
on O
IJB-A B-DAT

; O
but O
this O
is O
because O
IJB-A B-DAT
is O
not O
challenging O
enough O
and O

IJB-A B-DAT

state-of-the-art O
for O
verification O
on O
the O
IJB-A B-DAT
and O
IJB-B O
datasets. O
A O
higher O

IJB-A B-DAT

state-of-the-art O
for O
identification O
on O
the O
IJB-A B-DAT
and O
IJB-B O
datasets. O
A O
higher O

the O
state-of-the-art O
on O
the O
challenging O
IJB-A B-DAT
and O
IJB-B O
benchmarks O
by O
a O

exceeds O
the O
state-of-the-art O
on O
the O
IJB B-DAT

margin O
on O
the O
pub- O
lic O
IJB B-DAT

-A O
[20] O
and O
IJB B-DAT

challenging O
public O
face O
recognition O
datasets O
IJB B-DAT

-A O
[20] O
and O
IJB B-DAT

as O
[5, O
10, O
18, O
25], O
IJB B-DAT

-A O
and O
IJB B-DAT

consider O
in O
this O
work. O
The O
IJB B-DAT

and O
4.2 O
videos, O
respectively. O
The O
IJB B-DAT

dataset O
is O
an O
extension O
of O
IJB B-DAT

the O
standard O
benchmark O
procedure O
for O
IJB B-DAT

-A O
and O
IJB B-DAT

apart O
from O
the O
fact O
that O
IJB B-DAT

defines O
10 O
test O
splits, O
while O
IJB B-DAT

two O
galleries O
for O
identification. O
For O
IJB B-DAT

-A O
and O
for O
IJB B-DAT

Network O
deployment. O
In O
the O
IJB B-DAT

-A O
and O
IJB B-DAT

4.3 O
Ablation O
studies O
on O
IJB B-DAT

it O
to O
baselines O
on O
the O
IJB B-DAT

larger O
and O
more O
challenging O
than O
IJB B-DAT

1: O
Verification O
performance O
on O
the O
IJB B-DAT

2: O
Identification O
performance O
on O
the O
IJB B-DAT

against O
the O
state-of-the-art O
on O
the O
IJB B-DAT

-A O
and O
IJB B-DAT

identification O
and O
verification O
on O
both O
IJB B-DAT

-A O
and O
IJB B-DAT

surpasses O
[44] O
marginally O
on O
the O
IJB B-DAT

to O
Rank-10 O
for O
identification O
on O
IJB B-DAT

-A; O
but O
this O
is O
because O
IJB B-DAT

measures O
on O
the O
more O
challenging O
IJB B-DAT

IJB B-DAT

IJB B-DAT

state-of-the-art O
for O
verification O
on O
the O
IJB B-DAT

-A O
and O
IJB B-DAT

vs O
0.705 O
for O
verification O
on O
IJB B-DAT

0.743 O
for O
iden- O
tification O
on O
IJB B-DAT

vs O
0.671 O
for O
verification O
on O
IJB B-DAT

vs O
0.706 O
for O
verification O
on O
IJB B-DAT

IJB B-DAT

IJB B-DAT

state-of-the-art O
for O
identification O
on O
the O
IJB B-DAT

-A O
and O
IJB B-DAT

Fig. O
3: O
Results O
on O
the O
IJB B-DAT

the O
state-of-the-art O
on O
the O
challenging O
IJB B-DAT

-A O
and O
IJB B-DAT

a O
tem- O
plate O
in O
the O
IJB B-DAT

A B-DAT
straightforward O
method O
to O
tackle O
multiple O

Y. O
Zhong, O
R. O
Arandjelović B-DAT
and O
A O

A B-DAT
[20] O
and O
IJB-B O
[37] O
face O

A B-DAT
few O
methods O
go O
beyond O
simple O

Y. O
Zhong, O
R. O
Arandjelović B-DAT
and O
A O

Feature O
extraction. O
A B-DAT
neural O
network O
is O
used O
to O

Y. O
Zhong, O
R. O
Arandjelović B-DAT
and O
A O

Y. O
Zhong, O
R. O
Arandjelović B-DAT
and O
A O

A B-DAT
[20] O
and O
IJB-B O
[37] O
are O

A B-DAT
and O
IJB-B O
are O
intended O
for O

A B-DAT
dataset O
contains O
5,712 O
images O
and O

A B-DAT
with O
a O
total O
of O
11,754 O

A B-DAT
and O
IJB-B, O
and O
evaluate O
on O

A B-DAT
defines O
10 O
test O
splits, O
while O

A B-DAT
and O
for O
IJB-B O
iden- O
tification O

A B-DAT
and O
IJB-B O
datasets, O
there O
are O

Y. O
Zhong, O
R. O
Arandjelović B-DAT
and O
A O

and O
more O
challenging O
than O
IJB- O
A B-DAT

performance O
on O
the O
IJB-B O
dataset. O
A B-DAT
higher O
value O
of O
TAR O
is O

performance O
on O
the O
IJB-B O
dataset. O
A B-DAT
higher O
value O
of O
TPIR O
is O

Y. O
Zhong, O
R. O
Arandjelović B-DAT
and O
A O

A B-DAT
and O
IJB-B O
datasets. O
The O
currently O

A B-DAT
and O
IJB-B O
datasets. O
In O
particular O

A B-DAT
verification O
task, O
despite O
the O
fact O

A B-DAT

A B-DAT
is O
not O
challenging O
enough O
and O

A B-DAT

the O
IJB-A B-DAT
and O
IJB-B O
datasets. O
A O
higher O
value O
of O
TAR O
is O

Y. O
Zhong, O
R. O
Arandjelović B-DAT
and O
A O

A B-DAT

the O
IJB-A B-DAT
and O
IJB-B O
datasets. O
A O
higher O
value O
of O
TPIR O
is O

A B-DAT
cc O

A B-DAT
and O
IJB-B O
benchmarks O
by O
a O

Arandjelović, B-DAT
R., O
Gronat, O
P., O
Torii, O
A O

3] O
Arandjelović, B-DAT
R., O
Zisserman, O
A O

Xie, O
W., O
Parkhi, O
O.M., O
Zisserman, O
A B-DAT

.: O
VGGFace2: O
A B-DAT
dataset O
for O
recognising O
faces O
across O

Chen, O
J., O
Ranjan, O
R., O
Kumar, O
A B-DAT

Parkhi, O
O.M., O
Cao, O
Q., O
Zisserman, O
A B-DAT

He, O
X., O
Gao, O
J.: O
MS-Celeb-1M: O
A B-DAT
dataset O
and O
benchmark O
for O
large-scale O

17] O
Jégou, O
H., O
Zisserman, O
A B-DAT

Klein, O
B., O
Taborsky, O
E., O
Blanton, O
A B-DAT

Allen, B-DAT
K., O
Grother, O
P., O
Mah, O
A O

., O
Jain, O
A B-DAT

A B-DAT

Parkhi, O
O.M., O
Simonyan, O
K., O
Vedaldi, O
A B-DAT

., O
Zisserman, O
A.: B-DAT
A O
compact O
and O
discriminative O
face O
track O

25] O
Parkhi, O
O.M., O
Vedaldi, O
A B-DAT

., O
Zisserman, O
A B-DAT

Ma, O
S., O
Huang, O
S., O
Karpathy, O
A B-DAT

., O
Khosla, O
A B-DAT

., O
Bernstein, O
M., O
Berg, O
A B-DAT

Kalenichenko, O
D., O
Philbin, O
J.: O
Facenet: O
A B-DAT
unified O
embedding O
for O
face O
recognition O

32] O
Turaga, O
P., O
Veeraraghavan, O
A B-DAT

., O
Srivastava, O
A B-DAT

33] O
Vedaldi, O
A B-DAT

Y. O
Zhong, O
R. O
Arandjelović B-DAT
and O
A O

Whitelam, O
C., O
Taborsky, O
E., O
Blanton, O
A B-DAT

Miller, O
T., O
Kalka, O
N., O
Jain, O
A B-DAT

Xie, O
W., O
Shen, O
L., O
Zisserman, O
A B-DAT

39] O
Xie, O
W., O
Zisserman, O
A B-DAT

Zhang, O
R., O
Isola, O
P., O
Efros, O
A B-DAT

A B-DAT

samples O
and O
4386 O
testing O
samples. O
AFLW-Front B-DAT
uses O
the O
same O
training O
samples O

AFLW-Front B-DAT
2.94 O
2.75 O
2.74 O
2.68 O
2.17 O

11% O
on O
AFLW-Full. O
On O
the O
AFLW-Front B-DAT
testing O
set, O
our O
result O
is O

algorithms O
on O
bench- O
mark O
datasets O
AFLW B-DAT
and O
300-W. O
Code O
is O
publicly O

300W-Styles O
(≈ O
12000 O
images) O
and O
AFLW B-DAT

transferring O
the O
300-W O
[46] O
and O
AFLW B-DAT
[23] O
into O
dif- O
ferent O
styles O

AFLW B-DAT
[23]. O
This O
dataset O
contains O
21997 O

excluding O
invisible O
landmark. O
Faces O
in O
AFLW B-DAT
usually O
have O
different O
pose, O
ex O

There O
are O
two O
types O
of O
AFLW B-DAT
splits, O
AFLW-Full O
and O
AFLW-Frontal O
following O

73]. O
AFLW B-DAT

samples O
and O
4386 O
testing O
samples. O
AFLW B-DAT

same O
training O
samples O
as O
in O
AFLW B-DAT

46, O
31, O
7, O
43]. O
For O
AFLW B-DAT
dataset, O
we O
use O
the O
face O

CCL O
[73] O
Two-Stage O
[31] O
SAN O
AFLW B-DAT

AFLW B-DAT

normalized O
mean O
(NME) O
errors O
on O
AFLW B-DAT
dataset O

Results O
on O
AFLW B-DAT

shows O
the O
performance O
comparison O
on O
AFLW B-DAT

by O
more O
than O
11% O
on O
AFLW B-DAT

-Full. O
On O
the O
AFLW B-DAT

mark O
datasets, O
e.g., O
300-W O
and O
AFLW B-DAT

300W-Style O
AFLW B-DAT

datasets O
based O
on O
300-W O
and O
AFLW B-DAT
with O
the O
original O
and O
three O

new O
datasets, O
300W-Style O
and O
AFLW B-DAT

from O
300-W O
for O
our O
300W-Style. O
AFLW B-DAT

as O
300W-Style, O
which O
transfer O
the O
AFLW B-DAT
dataset O
into O
three O
different O
styles O

using O
the O
training O
set O
of O
AFLW B-DAT

and O
the O
testing O
set O
of O
AFLW B-DAT

can O
achieve O
79.82 O
AUC@0.08 O
on O
AFLW B-DAT

by O
only O
using O
the O
original O
AFLW B-DAT
training O
set, O
while O
the O
data O

state-of-the-art O
performance O
on O
300-W O
and O
AFLW B-DAT
datasets O

Front B-DAT
uses O
the O
same O
training O
samples O

Front B-DAT
2.94 O
2.75 O
2.74 O
2.68 O
2.17 O

Front B-DAT
testing O
set, O
our O
result O
is O

new O
facial O
landmark O
detection O
datasets, O
300W B-DAT

300W B-DAT

new O
datasets, O
300W B-DAT

As O
shown O
in O
Figure O
7, O
300W B-DAT

using O
PS. O
Each O
image O
in O
300W B-DAT

provided O
from O
300-W O
for O
our O
300W B-DAT

-Style. O
AFLW-Style O
is O
similar O
as O
300W B-DAT

Comparisons O
of O
NME O
on O
the O
300W B-DAT

Comparisons O
of O
NME O
on O
the O
300W B-DAT

Comparisons O
of O
NME O
on O
the O
300W B-DAT

full O
testing O
set O
of O
the O
300W B-DAT

W B-DAT

W B-DAT
[46] O
and O
AFLW O
[23] O
into O

W B-DAT
dataset. O
Different O
faces O
have O
different O

W B-DAT
[46]. O
However, O
it O
is O
hard O

W B-DAT
[46]. O
This O
dataset O
annotates O
five O

W B-DAT
dataset O

W B-DAT
dataset, O
we O
use O
the O
inte O

W B-DAT

detection O
algorithms O
on O
the O
300- O
W B-DAT

W B-DAT
Common O
Testing O
Set O

W B-DAT
Challenging O
Testing O
Set O

W B-DAT
common O
and O
challenging O
testing O
sets O

W B-DAT
common O
set O
by O
relative O
21.8 O

W B-DAT
and O
AFLW. O
It O
takes O
two O

W B-DAT
common O
and O
testing O
sets. O
As O

W B-DAT
by O
using O
the O
style-discriminative O
features O

of O
k-means O
clustering O
on O
300- O
W B-DAT
dataset. O
300-W O
dataset O
is O
the O

W B-DAT
and O
AFLW O
with O
the O
original O

W B-DAT
datasets, O
and O
the O
other O
three O

W B-DAT
dataset, O
and O
we O
thus O
directly O

W B-DAT
for O
our O
300W-Style. O
AFLW-Style O
is O

W B-DAT
training O
set O
and O
evaluate O
the O

W B-DAT
testing O
sets O
with O
different O
styles O

W B-DAT

W B-DAT
training O
set O
and O
test O
it O

W B-DAT
challenging O
test O
set, O
our O
SAN O

W B-DAT

W B-DAT
and O
AFLW O
datasets O

3] O
P. O
N. O
Belhumeur, O
D. O
W B-DAT

W, B-DAT
2011. O
2, O
5, O
6, O
7 O

37] O
W B-DAT

38] O
W B-DAT

W, B-DAT
2017. O
5 O

W, B-DAT
2013. O
2, O
4, O
5, O
6 O

52] O
C. O
Szegedy, O
W B-DAT

Lin, O
X. O
Dong, O
Y. O
Yan, O
W B-DAT

Feng, O
L. O
Liu, O
X. O
Nie, O
W B-DAT

Xing, O
Z. O
Niu, O
J. O
Huang, O
W B-DAT

65] O
Y. O
Yan, O
F. O
Nie, O
W B-DAT

69] O
R. O
Zhao, O
W B-DAT

the O
near-frontal O
face O
dataset O
of O
300W B-DAT

Fig. O
10. O
4.2. O
Evaluation O
on O
300W B-DAT
dataset O

most O
widely O
used O
near O
frontal O
300W B-DAT
dataset O
[31]. O
300W O
containes O
3 O

NME O
of O
different O
methods O
on O
300W B-DAT
dataset. O
Method O
Common O
Challenging O
Full O

visualization O
block. O
The O
diagonal O
matrix O
W B-DAT
contains O
the O
weights. O
For O
each O

T. O
Wu, B-DAT
Y. O
Wang, O
and O
W O

37] O
W B-DAT

Tenenbaum, O
A. O
Tor- O
ralba, O
and O
W B-DAT

Piramuthu, O
V. O
Jagadeesh, O
D. O
DeCoste, O
W B-DAT

Yu, O
J. O
Huang, O
S. O
Zhang, O
W B-DAT

detection O
accuracy O
measured O
on O
the O
300W B-DAT
benchmark O
does O
not O
necessarily O
imply O

accuracy O
on O
the O
widely O
used O
300W B-DAT
benchmark O
[39] O
does O
not O
imply O

68 O
point O
detection O
accuracies O
on O
300W B-DAT

AFW O
was O
included O
in O
our O
300W B-DAT
test O
set, O
landmark O
detection O
accuracy O

Detection O
accuracy O
on O
the O
300W B-DAT
benchmark. O
We O
evalu- O
ate O
performance O

on O
the O
300W B-DAT
data O
set O
[39], O
the O
most O

training O
sets O
used O
with O
the O
300W B-DAT
benchmark O
(e.g., O
the O
HELEN O
[26 O

1026 O
images, O
collectively, O
form O
the O
300W B-DAT
test O
set. O
Note O
that O
unlike O

detection O
examples. O
Landmarks O
detected O
in O
300W B-DAT
[39] O
images O
by O
projecting O
an O

5] O
P. O
N. O
Belhumeur, O
D. O
W B-DAT

B.-C. O
Chen, O
C.-S. O
Chen, O
and O
W B-DAT

Poirson, O
P. O
Ammirato, O
C.-Y. O
Fu, O
W B-DAT

45] O
Y. O
Xiang, O
W. B-DAT
Kim, O
W O

accuracy O
on O
the O
IJB-A O
and O
IJB-B B-DAT
benchmarks: O
using O
the O
same O
recognition O

challeng- O
ing O
IJB-A O
[22] O
and O
IJB-B B-DAT
benchmarks O
[43]. O
In O
particular, O
recognition O

and O
B O
[43] O
(IJB-A O
and O
IJB-B B-DAT

and O
identification O
on O
IJB-A O
and O
IJB-B, B-DAT
com- O
paring O
landmark O
detection O
based O

IJB-B B-DAT
[43] O
GOTs O
[43]∗ O
16.0 O
33.0 O

c) O
ROC O
IJB-B B-DAT
(d) O
CMC O
IJB-B O
Figure O
5. O
Verification O
and O
identification O

results O
on O
IJB-A O
and O
IJB-B B-DAT

results O
on O
both O
IJB-A O
and O
IJB-B B-DAT
are O
provided O
in O
Table O
2 O

baseline O
results O
from O
[43] O
for O
IJB-B B-DAT
(to O
our O
knowledge, O
we O
are O

verification O
and O
identification O
accuracies O
on O
IJB-B B-DAT

the O
faces O
in O
IJB-A O
and O
IJB-B B-DAT

B B-DAT
benchmarks: O
using O
the O
same O
recognition O

B B-DAT
benchmarks O
[43]. O
In O
particular, O
recognition O

Janus O
Benchmark B-DAT
A O
[22] O
and O
B O
[43] O
(IJB-A O
and O
IJB-B). O
Importantly O

B, B-DAT
com- O
paring O
landmark O
detection O
based O

B B-DAT
[43] O
GOTs O
[43]∗ O
16.0 O
33.0 O

B B-DAT
(d) O
CMC O
IJB-B O
Figure O
5 O

B B-DAT

B B-DAT
are O
provided O
in O
Table O
2 O

B B-DAT
(to O
our O
knowledge, O
we O
are O

B B-DAT

B B-DAT

4] O
A. O
Bansal, B-DAT
B O

8] O
B B-DAT

18] O
G. O
B B-DAT

22] O
B. B-DAT
F. O
Klare, O
B O

Whitelam, O
E. O
Taborsky, O
A. O
Blanton, B-DAT
B O

face O
recognition O
accuracy O
on O
the O
IJB B-DAT

-A O
and O
IJB B-DAT

on O
the O
highly O
challeng- O
ing O
IJB B-DAT

-A O
[22] O
and O
IJB B-DAT

A O
[22] O
and O
B O
[43] O
(IJB B-DAT

-A O
and O
IJB B-DAT

2. O
Verification O
and O
identification O
on O
IJB B-DAT

-A O
and O
IJB B-DAT

face O
alignment O
methods. O
Three O
baseline O
IJB B-DAT

IJB B-DAT

IJB B-DAT

a) O
ROC O
IJB-A B-DAT
(b) O
CMC O
IJB O

c) O
ROC O
IJB-B B-DAT
(d) O
CMC O
IJB O

Verification O
and O
identification O
results O
on O
IJB B-DAT

-A O
and O
IJB B-DAT

and O
identification O
results O
on O
both O
IJB B-DAT

-A O
and O
IJB B-DAT

provides, O
as O
reference, O
three O
state-of-the-art O
IJB B-DAT

baseline O
results O
from O
[43] O
for O
IJB B-DAT

verification O
and O
identification O
accuracies O
on O
IJB B-DAT

addition, O
our O
verification O
scores O
on O
IJB B-DAT

views O
of O
the O
faces O
in O
IJB B-DAT

-A O
and O
IJB B-DAT

exceeds O
the O
state-of-the-art O
on O
the O
IJB-B B-DAT
face O
recognition O
dataset. O
This O
is O

pub- O
lic O
IJB-A O
[20] O
and O
IJB-B B-DAT
[37] O
face O
recognition O
benchmarks. O
These O

recognition O
datasets O
IJB-A O
[20] O
and O
IJB-B B-DAT
[37] O
are O
used O
for O
evaluation O

10, O
18, O
25], O
IJB-A O
and O
IJB-B B-DAT
are O
intended O
for O
template-based O
face O

and O
4.2 O
videos, O
respectively. O
The O
IJB-B B-DAT
dataset O
is O
an O
extension O
of O

benchmark O
procedure O
for O
IJB-A O
and O
IJB-B, B-DAT
and O
evaluate O
on O
“1:1 O
face O

defines O
10 O
test O
splits, O
while O
IJB-B B-DAT
only O
has O
one O
split O
for O

identification. O
For O
IJB-A O
and O
for O
IJB-B B-DAT
iden- O
tification, O
we O
report, O
as O

deployment. O
In O
the O
IJB-A O
and O
IJB-B B-DAT
datasets, O
there O
are O
images O
and O

4.3 O
Ablation O
studies O
on O
IJB-B B-DAT

it O
to O
baselines O
on O
the O
IJB-B B-DAT
dataset, O
as O
it O
is O
larger O

1: O
Verification O
performance O
on O
the O
IJB-B B-DAT
dataset. O
A O
higher O
value O
of O

2: O
Identification O
performance O
on O
the O
IJB-B B-DAT
dataset. O
A O
higher O
value O
of O

state-of-the-art O
on O
the O
IJB-A O
and O
IJB-B B-DAT
datasets. O
The O
currently O
best O
performing O

verification O
on O
both O
IJB-A O
and O
IJB-B B-DAT
datasets. O
In O
particular, O
it O
surpasses O

measures O
on O
the O
more O
challenging O
IJB-B B-DAT
benchmark, O
our O
network O
achieves O
the O

IJB-B B-DAT

verification O
on O
the O
IJB-A O
and O
IJB-B B-DAT
datasets. O
A O
higher O
value O
of O

vs O
0.705 O
for O
verification O
on O
IJB-B, B-DAT
and O
TPIR O
at O
FPIR=0.01 O
of O

0.743 O
for O
iden- O
tification O
on O
IJB-B B-DAT

vs O
0.671 O
for O
verification O
on O
IJB-B, B-DAT
and O
TPIR O
at O
FPIR=0.01 O
of O

vs O
0.706 O
for O
verification O
on O
IJB-B B-DAT

IJB-B B-DAT

identification O
on O
the O
IJB-A O
and O
IJB-B B-DAT
datasets. O
A O
higher O
value O
of O

Fig. O
3: O
Results O
on O
the O
IJB-B B-DAT
dataset. O
Our O
SE-GV-4-g1 O
network O
which O

on O
the O
challenging O
IJB-A O
and O
IJB-B B-DAT
benchmarks O
by O
a O
large O
margin O

a O
tem- O
plate O
in O
the O
IJB-B B-DAT
dataset. O
The O
contribution O
(relative O
to O

B B-DAT
face O
recognition O
dataset. O
This O
is O

B B-DAT
[37] O
face O
recognition O
benchmarks. O
These O

B B-DAT
[37] O
are O
used O
for O
evaluation O

B B-DAT
are O
intended O
for O
template-based O
face O

B B-DAT
dataset O
is O
an O
extension O
of O

B, B-DAT
and O
evaluate O
on O
“1:1 O
face O

B B-DAT
only O
has O
one O
split O
for O

B B-DAT
iden- O
tification, O
we O
report, O
as O

B B-DAT
datasets, O
there O
are O
images O
and O

B B-DAT

B B-DAT
dataset, O
as O
it O
is O
larger O

B B-DAT
dataset. O
A O
higher O
value O
of O

B B-DAT
dataset. O
A O
higher O
value O
of O

B B-DAT
datasets. O
The O
currently O
best O
performing O

B B-DAT
datasets. O
In O
particular, O
it O
surpasses O

B B-DAT
benchmark, O
our O
network O
achieves O
the O

B B-DAT

B B-DAT
datasets. O
A O
higher O
value O
of O

B, B-DAT
and O
TPIR O
at O
FPIR=0.01 O
of O

B B-DAT

B, B-DAT
and O
TPIR O
at O
FPIR=0.01 O
of O

B B-DAT

B B-DAT

B B-DAT
datasets. O
A O
higher O
value O
of O

B B-DAT
dataset. O
Our O
SE-GV-4-g1 O
network O
which O

B B-DAT
benchmarks O
by O
a O
large O
margin O

B B-DAT
dataset. O
The O
contribution O
(relative O
to O

6] O
Cevikalp, O
H., O
Triggs, O
B B-DAT

20] O
Klare, O
B., B-DAT
Klein, O
B O

Yu, O
Z., O
Li, O
M., O
Raj, O
B B-DAT

Taborsky, O
E., O
Blanton, B-DAT
A., O
Maze, O
B O

B B-DAT
face O
dataset. O
In: O
CVPR O
Workshop O

exceeds O
the O
state-of-the-art O
on O
the O
IJB B-DAT

margin O
on O
the O
pub- O
lic O
IJB B-DAT

-A O
[20] O
and O
IJB B-DAT

challenging O
public O
face O
recognition O
datasets O
IJB B-DAT

-A O
[20] O
and O
IJB B-DAT

as O
[5, O
10, O
18, O
25], O
IJB B-DAT

-A O
and O
IJB B-DAT

consider O
in O
this O
work. O
The O
IJB B-DAT

and O
4.2 O
videos, O
respectively. O
The O
IJB B-DAT

dataset O
is O
an O
extension O
of O
IJB B-DAT

the O
standard O
benchmark O
procedure O
for O
IJB B-DAT

-A O
and O
IJB B-DAT

apart O
from O
the O
fact O
that O
IJB B-DAT

defines O
10 O
test O
splits, O
while O
IJB B-DAT

two O
galleries O
for O
identification. O
For O
IJB B-DAT

-A O
and O
for O
IJB B-DAT

Network O
deployment. O
In O
the O
IJB B-DAT

-A O
and O
IJB B-DAT

4.3 O
Ablation O
studies O
on O
IJB B-DAT

it O
to O
baselines O
on O
the O
IJB B-DAT

larger O
and O
more O
challenging O
than O
IJB B-DAT

1: O
Verification O
performance O
on O
the O
IJB B-DAT

2: O
Identification O
performance O
on O
the O
IJB B-DAT

against O
the O
state-of-the-art O
on O
the O
IJB B-DAT

-A O
and O
IJB B-DAT

identification O
and O
verification O
on O
both O
IJB B-DAT

-A O
and O
IJB B-DAT

surpasses O
[44] O
marginally O
on O
the O
IJB B-DAT

to O
Rank-10 O
for O
identification O
on O
IJB B-DAT

-A; O
but O
this O
is O
because O
IJB B-DAT

measures O
on O
the O
more O
challenging O
IJB B-DAT

IJB B-DAT

IJB B-DAT

state-of-the-art O
for O
verification O
on O
the O
IJB B-DAT

-A O
and O
IJB B-DAT

vs O
0.705 O
for O
verification O
on O
IJB B-DAT

0.743 O
for O
iden- O
tification O
on O
IJB B-DAT

vs O
0.671 O
for O
verification O
on O
IJB B-DAT

vs O
0.706 O
for O
verification O
on O
IJB B-DAT

IJB B-DAT

IJB B-DAT

state-of-the-art O
for O
identification O
on O
the O
IJB B-DAT

-A O
and O
IJB B-DAT

Fig. O
3: O
Results O
on O
the O
IJB B-DAT

the O
state-of-the-art O
on O
the O
challenging O
IJB B-DAT

-A O
and O
IJB B-DAT

a O
tem- O
plate O
in O
the O
IJB B-DAT

2. O
Dianping B-DAT
Dataset. O
Dianping O

the O
Criteo O
dataset O
and O
the O
Dianping B-DAT
dataset, O
we O
randomly O
split O
instances O

Dianping B-DAT
1.2M O
18 O
230K O
Bing O
News O

100 O
for O
CIN O
layers O
on O
Dianping B-DAT
and O
Bing O
News O
datasets. O
Since O

Dianping B-DAT
FM O
0.8165 O
0.3558 O
- O
DNN O

of O
different O
models O
on O
Criteo, O
Dianping B-DAT
and O
Bing O
News O
datasets. O
The O

Criteo O
Dianping B-DAT
Bing O
News O
Model O
name O
AUC O

20 O
to O
200, O
while O
on O
Dianping B-DAT
dataset, O
100 O
is O
a O
more O

Dianping B-DAT
0.8365 O

Dianping B-DAT

Dianping B-DAT

Dianping B-DAT

in O
g O
Dianping B-DAT

Dianping B-DAT

6.1 O
Datasets O
and O
Experimental O
Setup O
Amazon B-DAT
Dataset2. O
Amazon O
Dataset O
contains O
product O

reviews O
and O
metadata O
from O
Amazon, B-DAT
which O
is O
used O
as O
benchmark O

mini-batch O
size O
as O
described O
on O
Amazon B-DAT
Dataset O

Amazon B-DAT

Table O
3: O
Model O
Coparison O
on O
Amazon B-DAT
Dataset O
and O
Movie- O
Lens O
Dataset O

Model O
MovieLens. O
Amazon B-DAT

Result O
from O
model O
comparison O
on O
Amazon B-DAT
Dataset O
and O
MovieLens O
Dataset O

3 O
shows O
the O
results O
on O
Amazon B-DAT
dataset O
andMovieLens O
dataset. O
All O
experiments O

the O
competitors. O
Espe- O
cially O
on O
Amazon B-DAT
Dataset O
with O
rich O
user O
behaviors O

dimension O
of O
features O
in O
both O
Amazon B-DAT
Dataset O
and O
Movie- O
Lens O
Dataset O

Result O
from O
model O
comparison O
on O
Amazon B-DAT
Dataset O
and O
MovieLens O
Dataset O

amazon B-DAT

6.1 O
Datasets O
and O
Experimental O
Setup O
Amazon B-DAT
Dataset2. O
Amazon O
Dataset O
contains O
product O

reviews O
and O
metadata O
from O
Amazon, B-DAT
which O
is O
used O
as O
benchmark O

mini-batch O
size O
as O
described O
on O
Amazon B-DAT
Dataset O

Amazon B-DAT

Table O
3: O
Model O
Coparison O
on O
Amazon B-DAT
Dataset O
and O
Movie- O
Lens O
Dataset O

Model O
MovieLens. O
Amazon B-DAT

Result O
from O
model O
comparison O
on O
Amazon B-DAT
Dataset O
and O
MovieLens O
Dataset O

3 O
shows O
the O
results O
on O
Amazon B-DAT
dataset O
andMovieLens O
dataset. O
All O
experiments O

the O
competitors. O
Espe- O
cially O
on O
Amazon B-DAT
Dataset O
with O
rich O
user O
behaviors O

dimension O
of O
features O
in O
both O
Amazon B-DAT
Dataset O
and O
Movie- O
Lens O
Dataset O

Result O
from O
model O
comparison O
on O
Amazon B-DAT
Dataset O
and O
MovieLens O
Dataset O

amazon B-DAT

MovieLens B-DAT
data[11] O
contains O
138,493 O
users, O
27,278 O

Amazon(Electro). O
192,403 O
63,001 O
801 O
1,689,188 O
MovieLens B-DAT

100,000 O
2.14 O
billion O
a O
For O
MovieLens B-DAT
dataset, O
goods O
refer O
to O
be O

Model O
MovieLens B-DAT

comparison O
on O
Amazon O
Dataset O
and O
MovieLens B-DAT
Dataset O

comparison O
on O
Amazon O
Dataset O
and O
MovieLens B-DAT
Dataset O

MovieLens B-DAT
data[11] O
contains O
138,493 O
users, O
27,278 O

Amazon(Electro). O
192,403 O
63,001 O
801 O
1,689,188 O
MovieLens B-DAT

100,000 O
2.14 O
billion O
a O
For O
MovieLens B-DAT
dataset, O
goods O
refer O
to O
be O

Model O
MovieLens B-DAT

comparison O
on O
Amazon O
Dataset O
and O
MovieLens B-DAT
Dataset O

comparison O
on O
Amazon O
Dataset O
and O
MovieLens B-DAT
Dataset O

criteo B-DAT

lowing O
three O
datasets: O
1. O
Criteo B-DAT
Dataset. O
It O
is O
a O
famous O

For O
the O
Criteo B-DAT
dataset O
and O
the O
Dianping O
dataset O

Datasest O
#instances O
#fields O
#features O
(sparse) O
Criteo B-DAT
45M O
39 O
2.3M O

200 O
for O
CIN O
layers O
on O
Criteo B-DAT
dataset, O
and O
100 O
for O
CIN O

of O
individual O
models O
on O
the O
Criteo, B-DAT
Di- O
anping, O
andBingNews O
datasets. O
ColumnDepth O

Model O
name O
AUC O
Logloss O
Depth O
Criteo B-DAT

performance O
of O
different O
models O
on O
Criteo, B-DAT
Dianping O
and O
Bing O
News O
datasets O

Criteo B-DAT
Dianping O
Bing O
News O
Model O
name O

criteo B-DAT

the O
following O
two O
datasets. O
1) O
Criteo B-DAT
Dataset: O
Criteo O
dataset O
5 O
includes O

To O
evaluate O
the O
models O
on O
Criteo B-DAT
dataset, O
we O
follow O
the O
pa O

of O
differ- O
ent O
models O
on O
Criteo B-DAT
dataset O
by O
the O
following O
formula O

prediction O
of O
different O
models O
on O
Criteo B-DAT
dataset O
and O
Company∗ O
dataset O
is O

of O
Logloss) O
on O
Company∗ O
and O
Criteo B-DAT
datasets. O
• O
Learning O
high- O
and O

of O
Logloss) O
on O
Company∗ O
and O
Criteo B-DAT
datasets. O
• O
Learning O
high- O
and O

of O
Logloss) O
on O
Company∗ O
and O
Criteo B-DAT
datasets O

Performance O
on O
CTR O
prediction. O
Company∗ O
Criteo B-DAT

experiments O
on O
two O
real-world O
datasets O
(Criteo B-DAT
dataset O
and O
a O
commercial O
App O

criteo B-DAT

criteo B-DAT

criteo B-DAT

1) O
Criteo: B-DAT
Criteo O
1TB O
click O
log2 O
is O
a O

show O
the O
overall O
performance O
on O
Criteo B-DAT
and O
iPinYou O
datasets, O
respectively. O
In O

I: O
Overall O
Performance O
on O
the O
Criteo B-DAT
Dataset O

perform O
the O
best O
on O
both O
Criteo B-DAT
and O
iPinYou O
datasets. O
As O
for O

IV-A1 O
Criteo B-DAT

criteo B-DAT

criteo B-DAT

criteo B-DAT

1) O
Criteo: B-DAT
Criteo O
1TB O
click O
log2 O
is O
a O

show O
the O
overall O
performance O
on O
Criteo B-DAT
and O
iPinYou O
datasets, O
respectively. O
In O

I: O
Overall O
Performance O
on O
the O
Criteo B-DAT
Dataset O

perform O
the O
best O
on O
both O
Criteo B-DAT
and O
iPinYou O
datasets. O
As O
for O

IV-A1 O
Criteo B-DAT

criteo B-DAT

criteo B-DAT

criteo B-DAT

1) O
Criteo: B-DAT
Criteo O
1TB O
click O
log2 O
is O
a O

show O
the O
overall O
performance O
on O
Criteo B-DAT
and O
iPinYou O
datasets, O
respectively. O
In O

I: O
Overall O
Performance O
on O
the O
Criteo B-DAT
Dataset O

perform O
the O
best O
on O
both O
Criteo B-DAT
and O
iPinYou O
datasets. O
As O
for O

IV-A1 O
Criteo B-DAT

10% O
is O
for O
testing. O
2) O
Company B-DAT

we O
conduct O
exper- O
iment O
on O
Company B-DAT

the O
game O
center O
of O
the O
Company B-DAT

for O
each O
individual O
model O
on O
Company B-DAT

models O
on O
Criteo O
dataset O
and O
Company B-DAT

in O
terms O
of O
Logloss) O
on O
Company B-DAT

in O
terms O
of O
Logloss) O
on O
Company B-DAT

in O
terms O
of O
Logloss) O
on O
Company B-DAT

2: O
Performance O
on O
CTR O
prediction. O
Company B-DAT

of O
AUC O
and O
Logloss O
on O
Company B-DAT

3.9%. O
The O
daily O
turnover O
of O
Company B-DAT

differ- O
ent O
deep O
models, O
on O
Company B-DAT

3. O
Bing B-DAT
News O
Dataset. O
Bing O
News2 O
is O
part O
of O
Microsoft’s O

Bing B-DAT
search O
engine. O
In O
order O
to O

Dianping O
1.2M O
18 O
230K O
Bing B-DAT
News O
5M O
45 O
17K O

CIN O
layers O
on O
Dianping O
and O
Bing B-DAT
News O
datasets. O
Since O
we O
focus O

Bing B-DAT
News O
FM O
0.8223 O
0.2779 O

models O
on O
Criteo, O
Dianping O
and O
Bing B-DAT
News O
datasets. O
The O
column O
Depth O

Criteo O
Dianping O
Bing B-DAT
News O
Model O
name O
AUC O
Logloss O

the O
best O
result O
ON O
the O
Bing B-DAT
News O
dataset O

and O
7b, O
model O
performance O
on O
Bing B-DAT
News O
dataset O
increases O
steadily O
when O

Bing B-DAT
News O

Bing B-DAT
News O

Bing B-DAT
News O

Bing B-DAT
News O

Bing B-DAT
News O

Ensemble O
for O
Click O
Prediction O
in O
Bing B-DAT
Search O
Ads. O
In O
Proceedings O
of O

3. O
Bing B-DAT
News I-DAT
Dataset. O
Bing O
News2 O
is O
part O
of O
Microsoft’s O
Bing O

Dianping O
1.2M O
18 O
230K O
Bing B-DAT
News I-DAT
5M O
45 O
17K O

CIN O
layers O
on O
Dianping O
and O
Bing B-DAT
News I-DAT
datasets. O
Since O
we O
focus O
on O

Bing B-DAT
News I-DAT
FM O
0.8223 O
0.2779 O
- O
DNN O

models O
on O
Criteo, O
Dianping O
and O
Bing B-DAT
News I-DAT
datasets. O
The O
column O
Depth O
presents O

Criteo O
Dianping O
Bing B-DAT
News I-DAT
Model O
name O
AUC O
Logloss O
Depth O

the O
best O
result O
ON O
the O
Bing B-DAT
News I-DAT
dataset O

and O
7b, O
model O
performance O
on O
Bing B-DAT
News I-DAT
dataset O
increases O
steadily O
when O
we O

Bing B-DAT
News I-DAT

Bing B-DAT
News I-DAT

Bing B-DAT
News I-DAT

Bing B-DAT
News I-DAT

Bing B-DAT
News I-DAT

3. O
Bing O
News B-DAT
Dataset. O
Bing O
News2 O
is O
part O
of O
Microsoft’s O
Bing O

Dianping O
1.2M O
18 O
230K O
Bing O
News B-DAT
5M O
45 O
17K O

layers O
on O
Dianping O
and O
Bing O
News B-DAT
datasets. O
Since O
we O
focus O
on O

Bing O
News B-DAT
FM O
0.8223 O
0.2779 O
- O
DNN O

on O
Criteo, O
Dianping O
and O
Bing O
News B-DAT
datasets. O
The O
column O
Depth O
presents O

Criteo O
Dianping O
Bing O
News B-DAT
Model O
name O
AUC O
Logloss O
Depth O

best O
result O
ON O
the O
Bing O
News B-DAT
dataset O

7b, O
model O
performance O
on O
Bing O
News B-DAT
dataset O
increases O
steadily O
when O
we O

Bing O
News B-DAT

Bing O
News B-DAT

Bing O
News B-DAT

sBing O
News B-DAT

Bing O
News B-DAT

Bing O
News B-DAT

Wang, O
“Real-time O
bidding O
benchmarking O
with O
ipinyou B-DAT
dataset,” O
arXiv:1407.7073, O
2014 O

2) O
iPinYou: B-DAT
The O
iPinYou O
dataset3 O
is O
another O
real-world O
dataset O

overall O
performance O
on O
Criteo O
and O
iPinYou B-DAT
datasets, O
respectively. O
In O
FM, O
we O

II: O
Overall O
Performance O
on O
the O
iPinYou B-DAT
Dataset O

best O
on O
both O
Criteo O
and O
iPinYou B-DAT
datasets. O
As O
for O
log O
loss O

to O
the O
training O
iterations O
on O
iPinYou B-DAT
dataset. O
We O
find O
that O
network O

3: O
Learning O
Curves O
on O
the O
iPinYou B-DAT
Dataset O

IV-A2 O
iPinYou B-DAT

Wang, O
“Real-time O
bidding O
benchmarking O
with O
ipinyou B-DAT
dataset,” O
arXiv:1407.7073, O
2014 O

2) O
iPinYou: B-DAT
The O
iPinYou O
dataset3 O
is O
another O
real-world O
dataset O

overall O
performance O
on O
Criteo O
and O
iPinYou B-DAT
datasets, O
respectively. O
In O
FM, O
we O

II: O
Overall O
Performance O
on O
the O
iPinYou B-DAT
Dataset O

best O
on O
both O
Criteo O
and O
iPinYou B-DAT
datasets. O
As O
for O
log O
loss O

to O
the O
training O
iterations O
on O
iPinYou B-DAT
dataset. O
We O
find O
that O
network O

3: O
Learning O
Curves O
on O
the O
iPinYou B-DAT
Dataset O

IV-A2 O
iPinYou B-DAT

Wang, O
“Real-time O
bidding O
benchmarking O
with O
ipinyou B-DAT
dataset,” O
arXiv:1407.7073, O
2014 O

2) O
iPinYou: B-DAT
The O
iPinYou O
dataset3 O
is O
another O
real-world O
dataset O

overall O
performance O
on O
Criteo O
and O
iPinYou B-DAT
datasets, O
respectively. O
In O
FM, O
we O

II: O
Overall O
Performance O
on O
the O
iPinYou B-DAT
Dataset O

best O
on O
both O
Criteo O
and O
iPinYou B-DAT
datasets. O
As O
for O
log O
loss O

to O
the O
training O
iterations O
on O
iPinYou B-DAT
dataset. O
We O
find O
that O
network O

3: O
Learning O
Curves O
on O
the O
iPinYou B-DAT
Dataset O

IV-A2 O
iPinYou B-DAT

L., O
Liu, O
Z., O
Shen, O
X.: O
ipinyou B-DAT
global O
rtb O
bidding O
algorithm O
compe O

evaluate O
our O
models O
based O
on O
iPinYou B-DAT
dataset O
[27], O
a O
public O
real O

SNAP O
collection O
(Youtube, O
DBLP O
and O
Amazon), B-DAT
and O
we O
restrict O
the O
largest O

E| O
GNN O
LGNN O
MPNN∗ O
AGMFit O
Amazon B-DAT
268 O
/ O
52 O
60 O
346 O

Edge O
Ratio O
vs O
Amazon B-DAT
Graph O

graphs O
with O
la- O
beled O
communities O
(amazon, B-DAT
dblp, O
livejournal, O
orkut O
and O
friendster O

amazon B-DAT
[42] O
334,863 O
1,851,744 O
799,080 O
dblp O

set O
of O
widely O
used O
graphs O
(amazon, B-DAT
dblp, O
livejournal, O
orkut, O
friendster) O
that O

amazon B-DAT
0.0374 O
0.0809 O
0.0337 O
0.0310 O
0.0339 O

communities O
in O
the O
graphs O
analyzed: O
amazon B-DAT
27,004; O
dblp O
33,626; O
livejournal O
49,954 O

to O
process O
our O
smaller O
graph O
amazon B-DAT
(resp. O
number O
of O
edges O
in O

amazon B-DAT

amazon B-DAT

80 O
times O
longer O
that O
of O
amazon B-DAT

Easy O
Moderate B-DAT
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O
DoBEM O

Easy O
Moderate B-DAT
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O
DoBEM O

Method O
Easy O
Moderate B-DAT
Hard O
Mono3D O
[4] O
2.53 O
2.31 O

Method O
Easy O
Moderate B-DAT
Hard O
Mono3D O
[4] O
5.22 O
5.19 O

Benchmark O
Easy O
Moderate B-DAT
Hard O
Pedestrian O
(3D O
Detection) O
70.00 O

Easy O
Moderate B-DAT
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O
SWC O

Subset O
Easy O
Moderate B-DAT
Hard O
AP O
(2D) O
for O
cars O

Method O
Easy O
Moderate B-DAT
Hard O
VeloFCN O
[18] O
15.20 O
13.66 O

Method O
Cars B-DAT
Pedestrians O
Cyclists O

Method O
Cars B-DAT
Pedestrians O
Cyclists O

Method O
Cars B-DAT
Pedestrians O
Cyclists O

very O
sparse O
points. O
Evaluated O
on O
KITTI B-DAT
and O
SUN O
RGB-D O
3D O
detection O

method O
achieve O
leading O
positions O
on O
KITTI B-DAT
3D O
ob- O
ject O
detection O
[1 O

ther O
fine-tune O
it O
on O
a O
KITTI B-DAT
2D O
object O
detection O
dataset O
to O

5m O
to O
beyond O
50m O
in O
KITTI B-DAT
data), O
we O
predict O
the O
3D O

for O
3D O
object O
detection O
on O
KITTI B-DAT
[10] O
and O
SUN-RGBD O
[33] O
(Sec O

our O
3D O
object O
detector O
on O
KITTI B-DAT
[11] O
and O
SUN-RGBD O
[33] O
benchmarks O

KITTI B-DAT
Tab. O
1 O
shows O
the O
performance O

our O
3D O
detector O
on O
the O
KITTI B-DAT
test O
set. O
We O
outperform O
previous O

object O
detection O
3D O
AP O
on O
KITTI B-DAT
test O
set. O
DoBEM O
[42] O
and O

AP O
(bird’s O
eye O
view) O
on O
KITTI B-DAT
test O
set. O
3D O
FCN O
[17 O

3D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set O
(cars O
only O

3D O
object O
localization O
AP O
on O
KITTI B-DAT
val O
set O
(cars O
only O

We O
also O
report O
performance O
on O
KITTI B-DAT
val O
set O
(the O
same O
split O

56.32 O
Table O
5. O
Performance O
on O
KITTI B-DAT
val O
set O
for O
pedestrians O
and O

same O
pipeline O
we O
used O
for O
KITTI B-DAT
data O
set, O
we’ve O
achieved O
state-of-the-art O

on O
our O
v1 O
model O
on O
KITTI B-DAT
data O
using O
train/val O
split O
as O

of O
Frustum O
PointNet O
results O
on O
KITTI B-DAT
val O
set O
(best O
viewed O
in O

typical O
2D O
region O
proposal O
from O
KITTI B-DAT
val O
set O
with O
both O
2D O

on O
the O
fly O
(1,024 O
for O
KITTI B-DAT
and O
2,048 O
for O
SUN-RGBD). O
For O

KITTI B-DAT
Training O
The O
object O
detection O
benchmark O

in O
KITTI B-DAT
provides O
synchronized O
RGB O
images O
and O

the O
same O
as O
that O
in O
KITTI B-DAT

much O
lower O
than O
that O
in O
KITTI B-DAT
because O
of O
strong O
occlusions O
and O

compared O
to O
around O
90% O
in O
KITTI B-DAT

pedestrian, O
and O
cy- O
clist O
from O
KITTI B-DAT
dataset. O
The O
final O
model O
takes O

our O
detector’s O
AP O
(2D) O
on O
KITTI B-DAT
test O
set. O
Our O
detector O
has O

than O
current O
leading O
players O
on O
KITTI B-DAT
leader O
board. O
We’ve O
also O
reported O

2D O
object O
detection O
AP O
on O
KITTI B-DAT
test O
set. O
Evaluation O
IoU O
threshold O

the O
first O
place O
winner O
on O
KITTI B-DAT
leader O
board O
for O
pedestrians O
and O

2D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set O

3D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set. O
By O
using O
both O

can O
see O
that O
compared O
with O
KITTI B-DAT
LiDAR O
data, O
depth O
images O
can O

Method O
Modality O
Car O
Pedestrian O
CyclistEasy O
Moderate B-DAT
Hard O
Easy O
Moderate O
Hard O
Easy O

Moderate B-DAT
Hard O
Mono3D O
[3] O
Mono O
5.22 O

Method O
Modality O
Car O
Pedestrian O
CyclistEasy O
Moderate B-DAT
Hard O
Easy O
Moderate O
Hard O
Easy O

Moderate B-DAT
Hard O
Mono3D O
[3] O
Mono O
2.53 O

Benchmark O
Easy O
Moderate B-DAT
Hard O
Car O
(3D O
Detection) O
77.47 O

generate O
detections. O
Experiments O
on O
the O
KITTI B-DAT
car O
detection O
bench- O
mark O
show O

detection O
tasks, O
provided O
by O
the O
KITTI B-DAT
benchmark O
[11]. O
Experimental O
results O
show O

We O
conduct O
experiments O
on O
KITTI B-DAT
benchmark O
and O
show O
that O
VoxelNet O

LiDAR O
specifi- O
cations O
of O
the O
KITTI B-DAT
dataset O
[11]. O
Car O
Detection O
For O

We O
evaluate O
VoxelNet O
on O
the O
KITTI B-DAT
3D O
object O
detection O

average O
precision O
(in O
%) O
on O
KITTI B-DAT
validation O
set O

average O
precision O
(in O
%) O
on O
KITTI B-DAT
validation O
set O

the O
test O
results O
using O
the O
KITTI B-DAT
server O

the O
LiDAR O
data O
provided O
in O
KITTI B-DAT

4.1. O
Evaluation O
on O
KITTI B-DAT
Validation O
Set O

Metrics O
We O
follow O
the O
official O
KITTI B-DAT
evaluation O
protocol, O
where O
the O
IoU O

4.2. O
Evaluation O
on O
KITTI B-DAT
Test O
Set O

We O
evaluated O
VoxelNet O
on O
the O
KITTI B-DAT
test O
set O
by O
submit- O
ting O

other O
leading O
methods O
listed O
in O
KITTI B-DAT
benchmark O
use O
both O
RGB O
images O

Table O
3. O
Performance O
evaluation O
on O
KITTI B-DAT
test O
set O

grid. O
Our O
experiments O
on O
the O
KITTI B-DAT
car O
detection O
task O
show O
that O

to O
some O
interesting O
failure O
modes. O
Pedestrians B-DAT
and O
cyclists O
are O
commonly O
misclassified O

3D O
and O
bird’s O
eye O
view O
KITTI B-DAT
bench- O
marks. O
This O
detection O
performance O

PointPillars, O
PP O
method O
on O
the O
KITTI B-DAT
[5] O
test O
set. O
Lidar-only O
methods O

are O
top O
methods O
from O
the O
KITTI B-DAT
leader- O
board: O
M O
: O
MV3D O

PointPillars O
network O
on O
the O
public O
KITTI B-DAT
detection O
challenges O
which O
require O
detection O

We O
conduct O
experiments O
on O
the O
KITTI B-DAT
dataset O
and O
demonstrate O
state O
of O

the O
range O
typically O
used O
in O
KITTI B-DAT
for O
∼ O
97% O
sparsity. O
This O

Figure O
3. O
Qualitative O
analysis O
of O
KITTI B-DAT
results. O
We O
show O
a O
bird’s-eye O

Figure O
4. O
Failure O
cases O
on O
KITTI B-DAT

All O
experiments O
use O
the O
KITTI B-DAT
object O
detection O
bench- O
mark O
dataset O

the O
remaining O
6733 O
samples. O
The O
KITTI B-DAT
benchmark O
requires O
detections O
of O
cars O

the O
standard O
literature O
practice O
on O
KITTI B-DAT
[11, O
31, O
28], O
we O
train O

Table O
1. O
Results O
on O
the O
KITTI B-DAT
test O
BEV O
detection O
benchmark O

Table O
2. O
Results O
on O
the O
KITTI B-DAT
test O
3D O
detection O
benchmark O

the O
KITTI B-DAT
benchmark O
[28, O
30, O
2]. O
First O

mea- O
sured O
using O
the O
official O
KITTI B-DAT
evaluation O
detection O
metrics O
which O
are O

for O
2D O
de- O
tections. O
The O
KITTI B-DAT
dataset O
is O
stratified O
into O
easy O

hard O
difficulties, O
and O
the O
official O
KITTI B-DAT
leaderboard O
is O
ranked O
by O
performance O

Table O
3. O
Results O
on O
the O
KITTI B-DAT
test O
average O
orientation O
similarity O
(AOS O

to O
an O
artifact O
of O
the O
KITTI B-DAT
ground O
truth O
annotations, O
only O
lidar O

vs O
speed O
(Hz) O
on O
the O
KITTI B-DAT
[5] O
val O
set O
across O
pedestrians O

measured O
as O
BEV O
mAP O
on O
KITTI B-DAT
val. O
Learned O
encoders O
clearly O
beat O

We O
demonstrate O
that O
on O
the O
KITTI B-DAT
chal- O
lenge, O
PointPillars O
dominates O
all O

for O
au- O
tonomous O
driving? O
the O
KITTI B-DAT
vision O
benchmark O
suite. O
In O
CVPR O

Easy O
Moderate B-DAT
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O
DoBEM O

Easy O
Moderate B-DAT
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O
DoBEM O

Method O
Easy O
Moderate B-DAT
Hard O
Mono3D O
[4] O
2.53 O
2.31 O

Method O
Easy O
Moderate B-DAT
Hard O
Mono3D O
[4] O
5.22 O
5.19 O

Benchmark O
Easy O
Moderate B-DAT
Hard O
Pedestrian O
(3D O
Detection) O
70.00 O

Easy O
Moderate B-DAT
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O
SWC O

Subset O
Easy O
Moderate B-DAT
Hard O
AP O
(2D) O
for O
cars O

Method O
Easy O
Moderate B-DAT
Hard O
VeloFCN O
[18] O
15.20 O
13.66 O

Method O
Cars O
Pedestrians O
Cyclists B-DAT

Method O
Cars O
Pedestrians O
Cyclists B-DAT

Method O
Cars O
Pedestrians O
Cyclists B-DAT

very O
sparse O
points. O
Evaluated O
on O
KITTI B-DAT
and O
SUN O
RGB-D O
3D O
detection O

method O
achieve O
leading O
positions O
on O
KITTI B-DAT
3D O
ob- O
ject O
detection O
[1 O

ther O
fine-tune O
it O
on O
a O
KITTI B-DAT
2D O
object O
detection O
dataset O
to O

5m O
to O
beyond O
50m O
in O
KITTI B-DAT
data), O
we O
predict O
the O
3D O

for O
3D O
object O
detection O
on O
KITTI B-DAT
[10] O
and O
SUN-RGBD O
[33] O
(Sec O

our O
3D O
object O
detector O
on O
KITTI B-DAT
[11] O
and O
SUN-RGBD O
[33] O
benchmarks O

KITTI B-DAT
Tab. O
1 O
shows O
the O
performance O

our O
3D O
detector O
on O
the O
KITTI B-DAT
test O
set. O
We O
outperform O
previous O

object O
detection O
3D O
AP O
on O
KITTI B-DAT
test O
set. O
DoBEM O
[42] O
and O

AP O
(bird’s O
eye O
view) O
on O
KITTI B-DAT
test O
set. O
3D O
FCN O
[17 O

3D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set O
(cars O
only O

3D O
object O
localization O
AP O
on O
KITTI B-DAT
val O
set O
(cars O
only O

We O
also O
report O
performance O
on O
KITTI B-DAT
val O
set O
(the O
same O
split O

56.32 O
Table O
5. O
Performance O
on O
KITTI B-DAT
val O
set O
for O
pedestrians O
and O

same O
pipeline O
we O
used O
for O
KITTI B-DAT
data O
set, O
we’ve O
achieved O
state-of-the-art O

on O
our O
v1 O
model O
on O
KITTI B-DAT
data O
using O
train/val O
split O
as O

of O
Frustum O
PointNet O
results O
on O
KITTI B-DAT
val O
set O
(best O
viewed O
in O

typical O
2D O
region O
proposal O
from O
KITTI B-DAT
val O
set O
with O
both O
2D O

on O
the O
fly O
(1,024 O
for O
KITTI B-DAT
and O
2,048 O
for O
SUN-RGBD). O
For O

KITTI B-DAT
Training O
The O
object O
detection O
benchmark O

in O
KITTI B-DAT
provides O
synchronized O
RGB O
images O
and O

the O
same O
as O
that O
in O
KITTI B-DAT

much O
lower O
than O
that O
in O
KITTI B-DAT
because O
of O
strong O
occlusions O
and O

compared O
to O
around O
90% O
in O
KITTI B-DAT

pedestrian, O
and O
cy- O
clist O
from O
KITTI B-DAT
dataset. O
The O
final O
model O
takes O

our O
detector’s O
AP O
(2D) O
on O
KITTI B-DAT
test O
set. O
Our O
detector O
has O

than O
current O
leading O
players O
on O
KITTI B-DAT
leader O
board. O
We’ve O
also O
reported O

2D O
object O
detection O
AP O
on O
KITTI B-DAT
test O
set. O
Evaluation O
IoU O
threshold O

the O
first O
place O
winner O
on O
KITTI B-DAT
leader O
board O
for O
pedestrians O
and O

2D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set O

3D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set. O
By O
using O
both O

can O
see O
that O
compared O
with O
KITTI B-DAT
LiDAR O
data, O
depth O
images O
can O

Method O
Modality O
Car O
Pedestrian O
CyclistEasy O
Moderate B-DAT
Hard O
Easy O
Moderate O
Hard O
Easy O

Moderate B-DAT
Hard O
Mono3D O
[3] O
Mono O
5.22 O

Method O
Modality O
Car O
Pedestrian O
CyclistEasy O
Moderate B-DAT
Hard O
Easy O
Moderate O
Hard O
Easy O

Moderate B-DAT
Hard O
Mono3D O
[3] O
Mono O
2.53 O

Benchmark O
Easy O
Moderate B-DAT
Hard O
Car O
(3D O
Detection) O
77.47 O

generate O
detections. O
Experiments O
on O
the O
KITTI B-DAT
car O
detection O
bench- O
mark O
show O

detection O
tasks, O
provided O
by O
the O
KITTI B-DAT
benchmark O
[11]. O
Experimental O
results O
show O

We O
conduct O
experiments O
on O
KITTI B-DAT
benchmark O
and O
show O
that O
VoxelNet O

LiDAR O
specifi- O
cations O
of O
the O
KITTI B-DAT
dataset O
[11]. O
Car O
Detection O
For O

We O
evaluate O
VoxelNet O
on O
the O
KITTI B-DAT
3D O
object O
detection O

average O
precision O
(in O
%) O
on O
KITTI B-DAT
validation O
set O

average O
precision O
(in O
%) O
on O
KITTI B-DAT
validation O
set O

the O
test O
results O
using O
the O
KITTI B-DAT
server O

the O
LiDAR O
data O
provided O
in O
KITTI B-DAT

4.1. O
Evaluation O
on O
KITTI B-DAT
Validation O
Set O

Metrics O
We O
follow O
the O
official O
KITTI B-DAT
evaluation O
protocol, O
where O
the O
IoU O

4.2. O
Evaluation O
on O
KITTI B-DAT
Test O
Set O

We O
evaluated O
VoxelNet O
on O
the O
KITTI B-DAT
test O
set O
by O
submit- O
ting O

other O
leading O
methods O
listed O
in O
KITTI B-DAT
benchmark O
use O
both O
RGB O
images O

Table O
3. O
Performance O
evaluation O
on O
KITTI B-DAT
test O
set O

grid. O
Our O
experiments O
on O
the O
KITTI B-DAT
car O
detection O
task O
show O
that O

Modality O
Car O
Pedestrian O
CyclistEasy O
Moderate O
Hard B-DAT
Easy O
Moderate O
Hard O
Easy O
Moderate O

Hard B-DAT
Mono3D O
[3] O
Mono O
5.22 O
5.19 O

Modality O
Car O
Pedestrian O
CyclistEasy O
Moderate O
Hard B-DAT
Easy O
Moderate O
Hard O
Easy O
Moderate O

Hard B-DAT
Mono3D O
[3] O
Mono O
2.53 O
2.31 O

Benchmark O
Easy O
Moderate O
Hard B-DAT
Car O
(3D O
Detection) O
77.47 O
65.11 O

generate O
detections. O
Experiments O
on O
the O
KITTI B-DAT
car O
detection O
bench- O
mark O
show O

detection O
tasks, O
provided O
by O
the O
KITTI B-DAT
benchmark O
[11]. O
Experimental O
results O
show O

We O
conduct O
experiments O
on O
KITTI B-DAT
benchmark O
and O
show O
that O
VoxelNet O

LiDAR O
specifi- O
cations O
of O
the O
KITTI B-DAT
dataset O
[11]. O
Car O
Detection O
For O

We O
evaluate O
VoxelNet O
on O
the O
KITTI B-DAT
3D O
object O
detection O

average O
precision O
(in O
%) O
on O
KITTI B-DAT
validation O
set O

average O
precision O
(in O
%) O
on O
KITTI B-DAT
validation O
set O

the O
test O
results O
using O
the O
KITTI B-DAT
server O

the O
LiDAR O
data O
provided O
in O
KITTI B-DAT

4.1. O
Evaluation O
on O
KITTI B-DAT
Validation O
Set O

Metrics O
We O
follow O
the O
official O
KITTI B-DAT
evaluation O
protocol, O
where O
the O
IoU O

4.2. O
Evaluation O
on O
KITTI B-DAT
Test O
Set O

We O
evaluated O
VoxelNet O
on O
the O
KITTI B-DAT
test O
set O
by O
submit- O
ting O

other O
leading O
methods O
listed O
in O
KITTI B-DAT
benchmark O
use O
both O
RGB O
images O

Table O
3. O
Performance O
evaluation O
on O
KITTI B-DAT
test O
set O

grid. O
Our O
experiments O
on O
the O
KITTI B-DAT
car O
detection O
task O
show O
that O

Easy O
Moderate O
Hard B-DAT
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O
DoBEM O
[42 O

Easy O
Moderate O
Hard B-DAT
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O
DoBEM O
[42 O

Method O
Easy O
Moderate O
Hard B-DAT
Mono3D O
[4] O
2.53 O
2.31 O
2.31 O

Method O
Easy O
Moderate O
Hard B-DAT
Mono3D O
[4] O
5.22 O
5.19 O
4.13 O

Benchmark O
Easy O
Moderate O
Hard B-DAT
Pedestrian O
(3D O
Detection) O
70.00 O
61.32 O

Easy O
Moderate O
Hard B-DAT
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O
SWC O
90.82 O

Subset O
Easy O
Moderate O
Hard B-DAT
AP O
(2D) O
for O
cars O
96.48 O

Method O
Easy O
Moderate O
Hard B-DAT
VeloFCN O
[18] O
15.20 O
13.66 O
15.98 O

Method O
Cars B-DAT
Pedestrians O
Cyclists O

Method O
Cars B-DAT
Pedestrians O
Cyclists O

Method O
Cars B-DAT
Pedestrians O
Cyclists O

very O
sparse O
points. O
Evaluated O
on O
KITTI B-DAT
and O
SUN O
RGB-D O
3D O
detection O

method O
achieve O
leading O
positions O
on O
KITTI B-DAT
3D O
ob- O
ject O
detection O
[1 O

ther O
fine-tune O
it O
on O
a O
KITTI B-DAT
2D O
object O
detection O
dataset O
to O

5m O
to O
beyond O
50m O
in O
KITTI B-DAT
data), O
we O
predict O
the O
3D O

for O
3D O
object O
detection O
on O
KITTI B-DAT
[10] O
and O
SUN-RGBD O
[33] O
(Sec O

our O
3D O
object O
detector O
on O
KITTI B-DAT
[11] O
and O
SUN-RGBD O
[33] O
benchmarks O

KITTI B-DAT
Tab. O
1 O
shows O
the O
performance O

our O
3D O
detector O
on O
the O
KITTI B-DAT
test O
set. O
We O
outperform O
previous O

object O
detection O
3D O
AP O
on O
KITTI B-DAT
test O
set. O
DoBEM O
[42] O
and O

AP O
(bird’s O
eye O
view) O
on O
KITTI B-DAT
test O
set. O
3D O
FCN O
[17 O

3D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set O
(cars O
only O

3D O
object O
localization O
AP O
on O
KITTI B-DAT
val O
set O
(cars O
only O

We O
also O
report O
performance O
on O
KITTI B-DAT
val O
set O
(the O
same O
split O

56.32 O
Table O
5. O
Performance O
on O
KITTI B-DAT
val O
set O
for O
pedestrians O
and O

same O
pipeline O
we O
used O
for O
KITTI B-DAT
data O
set, O
we’ve O
achieved O
state-of-the-art O

on O
our O
v1 O
model O
on O
KITTI B-DAT
data O
using O
train/val O
split O
as O

of O
Frustum O
PointNet O
results O
on O
KITTI B-DAT
val O
set O
(best O
viewed O
in O

typical O
2D O
region O
proposal O
from O
KITTI B-DAT
val O
set O
with O
both O
2D O

on O
the O
fly O
(1,024 O
for O
KITTI B-DAT
and O
2,048 O
for O
SUN-RGBD). O
For O

KITTI B-DAT
Training O
The O
object O
detection O
benchmark O

in O
KITTI B-DAT
provides O
synchronized O
RGB O
images O
and O

the O
same O
as O
that O
in O
KITTI B-DAT

much O
lower O
than O
that O
in O
KITTI B-DAT
because O
of O
strong O
occlusions O
and O

compared O
to O
around O
90% O
in O
KITTI B-DAT

pedestrian, O
and O
cy- O
clist O
from O
KITTI B-DAT
dataset. O
The O
final O
model O
takes O

our O
detector’s O
AP O
(2D) O
on O
KITTI B-DAT
test O
set. O
Our O
detector O
has O

than O
current O
leading O
players O
on O
KITTI B-DAT
leader O
board. O
We’ve O
also O
reported O

2D O
object O
detection O
AP O
on O
KITTI B-DAT
test O
set. O
Evaluation O
IoU O
threshold O

the O
first O
place O
winner O
on O
KITTI B-DAT
leader O
board O
for O
pedestrians O
and O

2D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set O

3D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set. O
By O
using O
both O

can O
see O
that O
compared O
with O
KITTI B-DAT
LiDAR O
data, O
depth O
images O
can O

Car O
Pedestrian O
CyclistEasy B-DAT
Moderate O
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O

Car O
Pedestrian O
CyclistEasy B-DAT
Moderate O
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O

Benchmark O
Easy B-DAT
Moderate O
Hard O
Car O
(3D O
Detection O

generate O
detections. O
Experiments O
on O
the O
KITTI B-DAT
car O
detection O
bench- O
mark O
show O

detection O
tasks, O
provided O
by O
the O
KITTI B-DAT
benchmark O
[11]. O
Experimental O
results O
show O

We O
conduct O
experiments O
on O
KITTI B-DAT
benchmark O
and O
show O
that O
VoxelNet O

LiDAR O
specifi- O
cations O
of O
the O
KITTI B-DAT
dataset O
[11]. O
Car O
Detection O
For O

We O
evaluate O
VoxelNet O
on O
the O
KITTI B-DAT
3D O
object O
detection O

average O
precision O
(in O
%) O
on O
KITTI B-DAT
validation O
set O

average O
precision O
(in O
%) O
on O
KITTI B-DAT
validation O
set O

the O
test O
results O
using O
the O
KITTI B-DAT
server O

the O
LiDAR O
data O
provided O
in O
KITTI B-DAT

4.1. O
Evaluation O
on O
KITTI B-DAT
Validation O
Set O

Metrics O
We O
follow O
the O
official O
KITTI B-DAT
evaluation O
protocol, O
where O
the O
IoU O

4.2. O
Evaluation O
on O
KITTI B-DAT
Test O
Set O

We O
evaluated O
VoxelNet O
on O
the O
KITTI B-DAT
test O
set O
by O
submit- O
ting O

other O
leading O
methods O
listed O
in O
KITTI B-DAT
benchmark O
use O
both O
RGB O
images O

Table O
3. O
Performance O
evaluation O
on O
KITTI B-DAT
test O
set O

grid. O
Our O
experiments O
on O
the O
KITTI B-DAT
car O
detection O
task O
show O
that O

Method O
Cars B-DAT
Pedestrians O
Cyclists O

Method O
Cars B-DAT
Pedestrians O
Cyclists O

Method O
Cars B-DAT
Pedestrians O
Cyclists O

Easy B-DAT
Moderate O
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O

Easy B-DAT
Moderate O
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O

Method O
Easy B-DAT
Moderate O
Hard O
Mono3D O
[4] O
2.53 O

Method O
Easy B-DAT
Moderate O
Hard O
Mono3D O
[4] O
5.22 O

Benchmark O
Easy B-DAT
Moderate O
Hard O
Pedestrian O
(3D O
Detection O

Easy B-DAT
Moderate O
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O

Subset O
Easy B-DAT
Moderate O
Hard O
AP O
(2D) O
for O

Method O
Easy B-DAT
Moderate O
Hard O
VeloFCN O
[18] O
15.20 O

very O
sparse O
points. O
Evaluated O
on O
KITTI B-DAT
and O
SUN O
RGB-D O
3D O
detection O

method O
achieve O
leading O
positions O
on O
KITTI B-DAT
3D O
ob- O
ject O
detection O
[1 O

ther O
fine-tune O
it O
on O
a O
KITTI B-DAT
2D O
object O
detection O
dataset O
to O

5m O
to O
beyond O
50m O
in O
KITTI B-DAT
data), O
we O
predict O
the O
3D O

for O
3D O
object O
detection O
on O
KITTI B-DAT
[10] O
and O
SUN-RGBD O
[33] O
(Sec O

our O
3D O
object O
detector O
on O
KITTI B-DAT
[11] O
and O
SUN-RGBD O
[33] O
benchmarks O

KITTI B-DAT
Tab. O
1 O
shows O
the O
performance O

our O
3D O
detector O
on O
the O
KITTI B-DAT
test O
set. O
We O
outperform O
previous O

object O
detection O
3D O
AP O
on O
KITTI B-DAT
test O
set. O
DoBEM O
[42] O
and O

AP O
(bird’s O
eye O
view) O
on O
KITTI B-DAT
test O
set. O
3D O
FCN O
[17 O

3D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set O
(cars O
only O

3D O
object O
localization O
AP O
on O
KITTI B-DAT
val O
set O
(cars O
only O

We O
also O
report O
performance O
on O
KITTI B-DAT
val O
set O
(the O
same O
split O

56.32 O
Table O
5. O
Performance O
on O
KITTI B-DAT
val O
set O
for O
pedestrians O
and O

same O
pipeline O
we O
used O
for O
KITTI B-DAT
data O
set, O
we’ve O
achieved O
state-of-the-art O

on O
our O
v1 O
model O
on O
KITTI B-DAT
data O
using O
train/val O
split O
as O

of O
Frustum O
PointNet O
results O
on O
KITTI B-DAT
val O
set O
(best O
viewed O
in O

typical O
2D O
region O
proposal O
from O
KITTI B-DAT
val O
set O
with O
both O
2D O

on O
the O
fly O
(1,024 O
for O
KITTI B-DAT
and O
2,048 O
for O
SUN-RGBD). O
For O

KITTI B-DAT
Training O
The O
object O
detection O
benchmark O

in O
KITTI B-DAT
provides O
synchronized O
RGB O
images O
and O

the O
same O
as O
that O
in O
KITTI B-DAT

much O
lower O
than O
that O
in O
KITTI B-DAT
because O
of O
strong O
occlusions O
and O

compared O
to O
around O
90% O
in O
KITTI B-DAT

pedestrian, O
and O
cy- O
clist O
from O
KITTI B-DAT
dataset. O
The O
final O
model O
takes O

our O
detector’s O
AP O
(2D) O
on O
KITTI B-DAT
test O
set. O
Our O
detector O
has O

than O
current O
leading O
players O
on O
KITTI B-DAT
leader O
board. O
We’ve O
also O
reported O

2D O
object O
detection O
AP O
on O
KITTI B-DAT
test O
set. O
Evaluation O
IoU O
threshold O

the O
first O
place O
winner O
on O
KITTI B-DAT
leader O
board O
for O
pedestrians O
and O

2D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set O

3D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set. O
By O
using O
both O

can O
see O
that O
compared O
with O
KITTI B-DAT
LiDAR O
data, O
depth O
images O
can O

3D O
and O
bird’s O
eye O
view O
KITTI B-DAT
bench- O
marks. O
This O
detection O
performance O

PointPillars, O
PP O
method O
on O
the O
KITTI B-DAT
[5] O
test O
set. O
Lidar-only O
methods O

are O
top O
methods O
from O
the O
KITTI B-DAT
leader- O
board: O
M O
: O
MV3D O

PointPillars O
network O
on O
the O
public O
KITTI B-DAT
detection O
challenges O
which O
require O
detection O

We O
conduct O
experiments O
on O
the O
KITTI B-DAT
dataset O
and O
demonstrate O
state O
of O

the O
range O
typically O
used O
in O
KITTI B-DAT
for O
∼ O
97% O
sparsity. O
This O

Figure O
3. O
Qualitative O
analysis O
of O
KITTI B-DAT
results. O
We O
show O
a O
bird’s-eye O

Figure O
4. O
Failure O
cases O
on O
KITTI B-DAT

All O
experiments O
use O
the O
KITTI B-DAT
object O
detection O
bench- O
mark O
dataset O

the O
remaining O
6733 O
samples. O
The O
KITTI B-DAT
benchmark O
requires O
detections O
of O
cars O

the O
standard O
literature O
practice O
on O
KITTI B-DAT
[11, O
31, O
28], O
we O
train O

Table O
1. O
Results O
on O
the O
KITTI B-DAT
test O
BEV O
detection O
benchmark O

Table O
2. O
Results O
on O
the O
KITTI B-DAT
test O
3D O
detection O
benchmark O

the O
KITTI B-DAT
benchmark O
[28, O
30, O
2]. O
First O

mea- O
sured O
using O
the O
official O
KITTI B-DAT
evaluation O
detection O
metrics O
which O
are O

for O
2D O
de- O
tections. O
The O
KITTI B-DAT
dataset O
is O
stratified O
into O
easy O

hard O
difficulties, O
and O
the O
official O
KITTI B-DAT
leaderboard O
is O
ranked O
by O
performance O

Table O
3. O
Results O
on O
the O
KITTI B-DAT
test O
average O
orientation O
similarity O
(AOS O

to O
an O
artifact O
of O
the O
KITTI B-DAT
ground O
truth O
annotations, O
only O
lidar O

vs O
speed O
(Hz) O
on O
the O
KITTI B-DAT
[5] O
val O
set O
across O
pedestrians O

measured O
as O
BEV O
mAP O
on O
KITTI B-DAT
val. O
Learned O
encoders O
clearly O
beat O

We O
demonstrate O
that O
on O
the O
KITTI B-DAT
chal- O
lenge, O
PointPillars O
dominates O
all O

for O
au- O
tonomous O
driving? O
the O
KITTI B-DAT
vision O
benchmark O
suite. O
In O
CVPR O

Easy O
Moderate O
Hard B-DAT
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O
DoBEM O
[42 O

Easy O
Moderate O
Hard B-DAT
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O
DoBEM O
[42 O

Method O
Easy O
Moderate O
Hard B-DAT
Mono3D O
[4] O
2.53 O
2.31 O
2.31 O

Method O
Easy O
Moderate O
Hard B-DAT
Mono3D O
[4] O
5.22 O
5.19 O
4.13 O

Benchmark O
Easy O
Moderate O
Hard B-DAT
Pedestrian O
(3D O
Detection) O
70.00 O
61.32 O

Easy O
Moderate O
Hard B-DAT
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O
SWC O
90.82 O

Subset O
Easy O
Moderate O
Hard B-DAT
AP O
(2D) O
for O
cars O
96.48 O

Method O
Easy O
Moderate O
Hard B-DAT
VeloFCN O
[18] O
15.20 O
13.66 O
15.98 O

Method O
Cars O
Pedestrians O
Cyclists B-DAT

Method O
Cars O
Pedestrians O
Cyclists B-DAT

Method O
Cars O
Pedestrians O
Cyclists B-DAT

very O
sparse O
points. O
Evaluated O
on O
KITTI B-DAT
and O
SUN O
RGB-D O
3D O
detection O

method O
achieve O
leading O
positions O
on O
KITTI B-DAT
3D O
ob- O
ject O
detection O
[1 O

ther O
fine-tune O
it O
on O
a O
KITTI B-DAT
2D O
object O
detection O
dataset O
to O

5m O
to O
beyond O
50m O
in O
KITTI B-DAT
data), O
we O
predict O
the O
3D O

for O
3D O
object O
detection O
on O
KITTI B-DAT
[10] O
and O
SUN-RGBD O
[33] O
(Sec O

our O
3D O
object O
detector O
on O
KITTI B-DAT
[11] O
and O
SUN-RGBD O
[33] O
benchmarks O

KITTI B-DAT
Tab. O
1 O
shows O
the O
performance O

our O
3D O
detector O
on O
the O
KITTI B-DAT
test O
set. O
We O
outperform O
previous O

object O
detection O
3D O
AP O
on O
KITTI B-DAT
test O
set. O
DoBEM O
[42] O
and O

AP O
(bird’s O
eye O
view) O
on O
KITTI B-DAT
test O
set. O
3D O
FCN O
[17 O

3D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set O
(cars O
only O

3D O
object O
localization O
AP O
on O
KITTI B-DAT
val O
set O
(cars O
only O

We O
also O
report O
performance O
on O
KITTI B-DAT
val O
set O
(the O
same O
split O

56.32 O
Table O
5. O
Performance O
on O
KITTI B-DAT
val O
set O
for O
pedestrians O
and O

same O
pipeline O
we O
used O
for O
KITTI B-DAT
data O
set, O
we’ve O
achieved O
state-of-the-art O

on O
our O
v1 O
model O
on O
KITTI B-DAT
data O
using O
train/val O
split O
as O

of O
Frustum O
PointNet O
results O
on O
KITTI B-DAT
val O
set O
(best O
viewed O
in O

typical O
2D O
region O
proposal O
from O
KITTI B-DAT
val O
set O
with O
both O
2D O

on O
the O
fly O
(1,024 O
for O
KITTI B-DAT
and O
2,048 O
for O
SUN-RGBD). O
For O

KITTI B-DAT
Training O
The O
object O
detection O
benchmark O

in O
KITTI B-DAT
provides O
synchronized O
RGB O
images O
and O

the O
same O
as O
that O
in O
KITTI B-DAT

much O
lower O
than O
that O
in O
KITTI B-DAT
because O
of O
strong O
occlusions O
and O

compared O
to O
around O
90% O
in O
KITTI B-DAT

pedestrian, O
and O
cy- O
clist O
from O
KITTI B-DAT
dataset. O
The O
final O
model O
takes O

our O
detector’s O
AP O
(2D) O
on O
KITTI B-DAT
test O
set. O
Our O
detector O
has O

than O
current O
leading O
players O
on O
KITTI B-DAT
leader O
board. O
We’ve O
also O
reported O

2D O
object O
detection O
AP O
on O
KITTI B-DAT
test O
set. O
Evaluation O
IoU O
threshold O

the O
first O
place O
winner O
on O
KITTI B-DAT
leader O
board O
for O
pedestrians O
and O

2D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set O

3D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set. O
By O
using O
both O

can O
see O
that O
compared O
with O
KITTI B-DAT
LiDAR O
data, O
depth O
images O
can O

Modality O
Car O
Pedestrian O
CyclistEasy O
Moderate O
Hard B-DAT
Easy O
Moderate O
Hard O
Easy O
Moderate O

Hard B-DAT
Mono3D O
[3] O
Mono O
5.22 O
5.19 O

Modality O
Car O
Pedestrian O
CyclistEasy O
Moderate O
Hard B-DAT
Easy O
Moderate O
Hard O
Easy O
Moderate O

Hard B-DAT
Mono3D O
[3] O
Mono O
2.53 O
2.31 O

Benchmark O
Easy O
Moderate O
Hard B-DAT
Car O
(3D O
Detection) O
77.47 O
65.11 O

generate O
detections. O
Experiments O
on O
the O
KITTI B-DAT
car O
detection O
bench- O
mark O
show O

detection O
tasks, O
provided O
by O
the O
KITTI B-DAT
benchmark O
[11]. O
Experimental O
results O
show O

We O
conduct O
experiments O
on O
KITTI B-DAT
benchmark O
and O
show O
that O
VoxelNet O

LiDAR O
specifi- O
cations O
of O
the O
KITTI B-DAT
dataset O
[11]. O
Car O
Detection O
For O

We O
evaluate O
VoxelNet O
on O
the O
KITTI B-DAT
3D O
object O
detection O

average O
precision O
(in O
%) O
on O
KITTI B-DAT
validation O
set O

average O
precision O
(in O
%) O
on O
KITTI B-DAT
validation O
set O

the O
test O
results O
using O
the O
KITTI B-DAT
server O

the O
LiDAR O
data O
provided O
in O
KITTI B-DAT

4.1. O
Evaluation O
on O
KITTI B-DAT
Validation O
Set O

Metrics O
We O
follow O
the O
official O
KITTI B-DAT
evaluation O
protocol, O
where O
the O
IoU O

4.2. O
Evaluation O
on O
KITTI B-DAT
Test O
Set O

We O
evaluated O
VoxelNet O
on O
the O
KITTI B-DAT
test O
set O
by O
submit- O
ting O

other O
leading O
methods O
listed O
in O
KITTI B-DAT
benchmark O
use O
both O
RGB O
images O

Table O
3. O
Performance O
evaluation O
on O
KITTI B-DAT
test O
set O

grid. O
Our O
experiments O
on O
the O
KITTI B-DAT
car O
detection O
task O
show O
that O

3D O
and O
bird’s O
eye O
view O
KITTI B-DAT
bench- O
marks. O
This O
detection O
performance O

PointPillars, O
PP O
method O
on O
the O
KITTI B-DAT
[5] O
test O
set. O
Lidar-only O
methods O

are O
top O
methods O
from O
the O
KITTI B-DAT
leader- O
board: O
M O
: O
MV3D O

PointPillars O
network O
on O
the O
public O
KITTI B-DAT
detection O
challenges O
which O
require O
detection O

We O
conduct O
experiments O
on O
the O
KITTI B-DAT
dataset O
and O
demonstrate O
state O
of O

the O
range O
typically O
used O
in O
KITTI B-DAT
for O
∼ O
97% O
sparsity. O
This O

Figure O
3. O
Qualitative O
analysis O
of O
KITTI B-DAT
results. O
We O
show O
a O
bird’s-eye O

Figure O
4. O
Failure O
cases O
on O
KITTI B-DAT

All O
experiments O
use O
the O
KITTI B-DAT
object O
detection O
bench- O
mark O
dataset O

the O
remaining O
6733 O
samples. O
The O
KITTI B-DAT
benchmark O
requires O
detections O
of O
cars O

the O
standard O
literature O
practice O
on O
KITTI B-DAT
[11, O
31, O
28], O
we O
train O

Table O
1. O
Results O
on O
the O
KITTI B-DAT
test O
BEV O
detection O
benchmark O

Table O
2. O
Results O
on O
the O
KITTI B-DAT
test O
3D O
detection O
benchmark O

the O
KITTI B-DAT
benchmark O
[28, O
30, O
2]. O
First O

mea- O
sured O
using O
the O
official O
KITTI B-DAT
evaluation O
detection O
metrics O
which O
are O

for O
2D O
de- O
tections. O
The O
KITTI B-DAT
dataset O
is O
stratified O
into O
easy O

hard O
difficulties, O
and O
the O
official O
KITTI B-DAT
leaderboard O
is O
ranked O
by O
performance O

Table O
3. O
Results O
on O
the O
KITTI B-DAT
test O
average O
orientation O
similarity O
(AOS O

to O
an O
artifact O
of O
the O
KITTI B-DAT
ground O
truth O
annotations, O
only O
lidar O

vs O
speed O
(Hz) O
on O
the O
KITTI B-DAT
[5] O
val O
set O
across O
pedestrians O

measured O
as O
BEV O
mAP O
on O
KITTI B-DAT
val. O
Learned O
encoders O
clearly O
beat O

We O
demonstrate O
that O
on O
the O
KITTI B-DAT
chal- O
lenge, O
PointPillars O
dominates O
all O

for O
au- O
tonomous O
driving? O
the O
KITTI B-DAT
vision O
benchmark O
suite. O
In O
CVPR O

Easy B-DAT
Moderate O
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O

Easy B-DAT
Moderate O
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O

Method O
Easy B-DAT
Moderate O
Hard O
Mono3D O
[4] O
2.53 O

Method O
Easy B-DAT
Moderate O
Hard O
Mono3D O
[4] O
5.22 O

Benchmark O
Easy B-DAT
Moderate O
Hard O
Pedestrian O
(3D O
Detection O

Easy B-DAT
Moderate O
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O

Subset O
Easy B-DAT
Moderate O
Hard O
AP O
(2D) O
for O

Method O
Easy B-DAT
Moderate O
Hard O
VeloFCN O
[18] O
15.20 O

Method O
Cars O
Pedestrians B-DAT
Cyclists O

Method O
Cars O
Pedestrians B-DAT
Cyclists O

Method O
Cars O
Pedestrians B-DAT
Cyclists O

very O
sparse O
points. O
Evaluated O
on O
KITTI B-DAT
and O
SUN O
RGB-D O
3D O
detection O

method O
achieve O
leading O
positions O
on O
KITTI B-DAT
3D O
ob- O
ject O
detection O
[1 O

ther O
fine-tune O
it O
on O
a O
KITTI B-DAT
2D O
object O
detection O
dataset O
to O

5m O
to O
beyond O
50m O
in O
KITTI B-DAT
data), O
we O
predict O
the O
3D O

for O
3D O
object O
detection O
on O
KITTI B-DAT
[10] O
and O
SUN-RGBD O
[33] O
(Sec O

our O
3D O
object O
detector O
on O
KITTI B-DAT
[11] O
and O
SUN-RGBD O
[33] O
benchmarks O

KITTI B-DAT
Tab. O
1 O
shows O
the O
performance O

our O
3D O
detector O
on O
the O
KITTI B-DAT
test O
set. O
We O
outperform O
previous O

object O
detection O
3D O
AP O
on O
KITTI B-DAT
test O
set. O
DoBEM O
[42] O
and O

AP O
(bird’s O
eye O
view) O
on O
KITTI B-DAT
test O
set. O
3D O
FCN O
[17 O

3D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set O
(cars O
only O

3D O
object O
localization O
AP O
on O
KITTI B-DAT
val O
set O
(cars O
only O

We O
also O
report O
performance O
on O
KITTI B-DAT
val O
set O
(the O
same O
split O

56.32 O
Table O
5. O
Performance O
on O
KITTI B-DAT
val O
set O
for O
pedestrians O
and O

same O
pipeline O
we O
used O
for O
KITTI B-DAT
data O
set, O
we’ve O
achieved O
state-of-the-art O

on O
our O
v1 O
model O
on O
KITTI B-DAT
data O
using O
train/val O
split O
as O

of O
Frustum O
PointNet O
results O
on O
KITTI B-DAT
val O
set O
(best O
viewed O
in O

typical O
2D O
region O
proposal O
from O
KITTI B-DAT
val O
set O
with O
both O
2D O

on O
the O
fly O
(1,024 O
for O
KITTI B-DAT
and O
2,048 O
for O
SUN-RGBD). O
For O

KITTI B-DAT
Training O
The O
object O
detection O
benchmark O

in O
KITTI B-DAT
provides O
synchronized O
RGB O
images O
and O

the O
same O
as O
that O
in O
KITTI B-DAT

much O
lower O
than O
that O
in O
KITTI B-DAT
because O
of O
strong O
occlusions O
and O

compared O
to O
around O
90% O
in O
KITTI B-DAT

pedestrian, O
and O
cy- O
clist O
from O
KITTI B-DAT
dataset. O
The O
final O
model O
takes O

our O
detector’s O
AP O
(2D) O
on O
KITTI B-DAT
test O
set. O
Our O
detector O
has O

than O
current O
leading O
players O
on O
KITTI B-DAT
leader O
board. O
We’ve O
also O
reported O

2D O
object O
detection O
AP O
on O
KITTI B-DAT
test O
set. O
Evaluation O
IoU O
threshold O

the O
first O
place O
winner O
on O
KITTI B-DAT
leader O
board O
for O
pedestrians O
and O

2D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set O

3D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set. O
By O
using O
both O

can O
see O
that O
compared O
with O
KITTI B-DAT
LiDAR O
data, O
depth O
images O
can O

Car O
Pedestrian O
CyclistEasy B-DAT
Moderate O
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O

Car O
Pedestrian O
CyclistEasy B-DAT
Moderate O
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O

Benchmark O
Easy B-DAT
Moderate O
Hard O
Car O
(3D O
Detection O

generate O
detections. O
Experiments O
on O
the O
KITTI B-DAT
car O
detection O
bench- O
mark O
show O

detection O
tasks, O
provided O
by O
the O
KITTI B-DAT
benchmark O
[11]. O
Experimental O
results O
show O

We O
conduct O
experiments O
on O
KITTI B-DAT
benchmark O
and O
show O
that O
VoxelNet O

LiDAR O
specifi- O
cations O
of O
the O
KITTI B-DAT
dataset O
[11]. O
Car O
Detection O
For O

We O
evaluate O
VoxelNet O
on O
the O
KITTI B-DAT
3D O
object O
detection O

average O
precision O
(in O
%) O
on O
KITTI B-DAT
validation O
set O

average O
precision O
(in O
%) O
on O
KITTI B-DAT
validation O
set O

the O
test O
results O
using O
the O
KITTI B-DAT
server O

the O
LiDAR O
data O
provided O
in O
KITTI B-DAT

4.1. O
Evaluation O
on O
KITTI B-DAT
Validation O
Set O

Metrics O
We O
follow O
the O
official O
KITTI B-DAT
evaluation O
protocol, O
where O
the O
IoU O

4.2. O
Evaluation O
on O
KITTI B-DAT
Test O
Set O

We O
evaluated O
VoxelNet O
on O
the O
KITTI B-DAT
test O
set O
by O
submit- O
ting O

other O
leading O
methods O
listed O
in O
KITTI B-DAT
benchmark O
use O
both O
RGB O
images O

Table O
3. O
Performance O
evaluation O
on O
KITTI B-DAT
test O
set O

grid. O
Our O
experiments O
on O
the O
KITTI B-DAT
car O
detection O
task O
show O
that O

Method O
Cars O
Pedestrians O
Cyclists B-DAT

Method O
Cars O
Pedestrians O
Cyclists B-DAT

Method O
Cars O
Pedestrians O
Cyclists B-DAT

Easy B-DAT
Moderate O
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O

Easy B-DAT
Moderate O
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O

Method O
Easy B-DAT
Moderate O
Hard O
Mono3D O
[4] O
2.53 O

Method O
Easy B-DAT
Moderate O
Hard O
Mono3D O
[4] O
5.22 O

Benchmark O
Easy B-DAT
Moderate O
Hard O
Pedestrian O
(3D O
Detection O

Easy B-DAT
Moderate O
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O

Subset O
Easy B-DAT
Moderate O
Hard O
AP O
(2D) O
for O

Method O
Easy B-DAT
Moderate O
Hard O
VeloFCN O
[18] O
15.20 O

very O
sparse O
points. O
Evaluated O
on O
KITTI B-DAT
and O
SUN O
RGB-D O
3D O
detection O

method O
achieve O
leading O
positions O
on O
KITTI B-DAT
3D O
ob- O
ject O
detection O
[1 O

ther O
fine-tune O
it O
on O
a O
KITTI B-DAT
2D O
object O
detection O
dataset O
to O

5m O
to O
beyond O
50m O
in O
KITTI B-DAT
data), O
we O
predict O
the O
3D O

for O
3D O
object O
detection O
on O
KITTI B-DAT
[10] O
and O
SUN-RGBD O
[33] O
(Sec O

our O
3D O
object O
detector O
on O
KITTI B-DAT
[11] O
and O
SUN-RGBD O
[33] O
benchmarks O

KITTI B-DAT
Tab. O
1 O
shows O
the O
performance O

our O
3D O
detector O
on O
the O
KITTI B-DAT
test O
set. O
We O
outperform O
previous O

object O
detection O
3D O
AP O
on O
KITTI B-DAT
test O
set. O
DoBEM O
[42] O
and O

AP O
(bird’s O
eye O
view) O
on O
KITTI B-DAT
test O
set. O
3D O
FCN O
[17 O

3D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set O
(cars O
only O

3D O
object O
localization O
AP O
on O
KITTI B-DAT
val O
set O
(cars O
only O

We O
also O
report O
performance O
on O
KITTI B-DAT
val O
set O
(the O
same O
split O

56.32 O
Table O
5. O
Performance O
on O
KITTI B-DAT
val O
set O
for O
pedestrians O
and O

same O
pipeline O
we O
used O
for O
KITTI B-DAT
data O
set, O
we’ve O
achieved O
state-of-the-art O

on O
our O
v1 O
model O
on O
KITTI B-DAT
data O
using O
train/val O
split O
as O

of O
Frustum O
PointNet O
results O
on O
KITTI B-DAT
val O
set O
(best O
viewed O
in O

typical O
2D O
region O
proposal O
from O
KITTI B-DAT
val O
set O
with O
both O
2D O

on O
the O
fly O
(1,024 O
for O
KITTI B-DAT
and O
2,048 O
for O
SUN-RGBD). O
For O

KITTI B-DAT
Training O
The O
object O
detection O
benchmark O

in O
KITTI B-DAT
provides O
synchronized O
RGB O
images O
and O

the O
same O
as O
that O
in O
KITTI B-DAT

much O
lower O
than O
that O
in O
KITTI B-DAT
because O
of O
strong O
occlusions O
and O

compared O
to O
around O
90% O
in O
KITTI B-DAT

pedestrian, O
and O
cy- O
clist O
from O
KITTI B-DAT
dataset. O
The O
final O
model O
takes O

our O
detector’s O
AP O
(2D) O
on O
KITTI B-DAT
test O
set. O
Our O
detector O
has O

than O
current O
leading O
players O
on O
KITTI B-DAT
leader O
board. O
We’ve O
also O
reported O

2D O
object O
detection O
AP O
on O
KITTI B-DAT
test O
set. O
Evaluation O
IoU O
threshold O

the O
first O
place O
winner O
on O
KITTI B-DAT
leader O
board O
for O
pedestrians O
and O

2D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set O

3D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set. O
By O
using O
both O

can O
see O
that O
compared O
with O
KITTI B-DAT
LiDAR O
data, O
depth O
images O
can O

Car O
Pedestrian O
CyclistEasy B-DAT
Moderate O
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O

Car O
Pedestrian O
CyclistEasy B-DAT
Moderate O
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O

Benchmark O
Easy B-DAT
Moderate O
Hard O
Car O
(3D O
Detection O

generate O
detections. O
Experiments O
on O
the O
KITTI B-DAT
car O
detection O
bench- O
mark O
show O

detection O
tasks, O
provided O
by O
the O
KITTI B-DAT
benchmark O
[11]. O
Experimental O
results O
show O

We O
conduct O
experiments O
on O
KITTI B-DAT
benchmark O
and O
show O
that O
VoxelNet O

LiDAR O
specifi- O
cations O
of O
the O
KITTI B-DAT
dataset O
[11]. O
Car O
Detection O
For O

We O
evaluate O
VoxelNet O
on O
the O
KITTI B-DAT
3D O
object O
detection O

average O
precision O
(in O
%) O
on O
KITTI B-DAT
validation O
set O

average O
precision O
(in O
%) O
on O
KITTI B-DAT
validation O
set O

the O
test O
results O
using O
the O
KITTI B-DAT
server O

the O
LiDAR O
data O
provided O
in O
KITTI B-DAT

4.1. O
Evaluation O
on O
KITTI B-DAT
Validation O
Set O

Metrics O
We O
follow O
the O
official O
KITTI B-DAT
evaluation O
protocol, O
where O
the O
IoU O

4.2. O
Evaluation O
on O
KITTI B-DAT
Test O
Set O

We O
evaluated O
VoxelNet O
on O
the O
KITTI B-DAT
test O
set O
by O
submit- O
ting O

other O
leading O
methods O
listed O
in O
KITTI B-DAT
benchmark O
use O
both O
RGB O
images O

Table O
3. O
Performance O
evaluation O
on O
KITTI B-DAT
test O
set O

grid. O
Our O
experiments O
on O
the O
KITTI B-DAT
car O
detection O
task O
show O
that O

Easy O
Moderate B-DAT
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O
DoBEM O

Easy O
Moderate B-DAT
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O
DoBEM O

Method O
Easy O
Moderate B-DAT
Hard O
Mono3D O
[4] O
2.53 O
2.31 O

Method O
Easy O
Moderate B-DAT
Hard O
Mono3D O
[4] O
5.22 O
5.19 O

Benchmark O
Easy O
Moderate B-DAT
Hard O
Pedestrian O
(3D O
Detection) O
70.00 O

Easy O
Moderate B-DAT
Hard O
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O
SWC O

Subset O
Easy O
Moderate B-DAT
Hard O
AP O
(2D) O
for O
cars O

Method O
Easy O
Moderate B-DAT
Hard O
VeloFCN O
[18] O
15.20 O
13.66 O

Method O
Cars O
Pedestrians B-DAT
Cyclists O

Method O
Cars O
Pedestrians B-DAT
Cyclists O

Method O
Cars O
Pedestrians B-DAT
Cyclists O

very O
sparse O
points. O
Evaluated O
on O
KITTI B-DAT
and O
SUN O
RGB-D O
3D O
detection O

method O
achieve O
leading O
positions O
on O
KITTI B-DAT
3D O
ob- O
ject O
detection O
[1 O

ther O
fine-tune O
it O
on O
a O
KITTI B-DAT
2D O
object O
detection O
dataset O
to O

5m O
to O
beyond O
50m O
in O
KITTI B-DAT
data), O
we O
predict O
the O
3D O

for O
3D O
object O
detection O
on O
KITTI B-DAT
[10] O
and O
SUN-RGBD O
[33] O
(Sec O

our O
3D O
object O
detector O
on O
KITTI B-DAT
[11] O
and O
SUN-RGBD O
[33] O
benchmarks O

KITTI B-DAT
Tab. O
1 O
shows O
the O
performance O

our O
3D O
detector O
on O
the O
KITTI B-DAT
test O
set. O
We O
outperform O
previous O

object O
detection O
3D O
AP O
on O
KITTI B-DAT
test O
set. O
DoBEM O
[42] O
and O

AP O
(bird’s O
eye O
view) O
on O
KITTI B-DAT
test O
set. O
3D O
FCN O
[17 O

3D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set O
(cars O
only O

3D O
object O
localization O
AP O
on O
KITTI B-DAT
val O
set O
(cars O
only O

We O
also O
report O
performance O
on O
KITTI B-DAT
val O
set O
(the O
same O
split O

56.32 O
Table O
5. O
Performance O
on O
KITTI B-DAT
val O
set O
for O
pedestrians O
and O

same O
pipeline O
we O
used O
for O
KITTI B-DAT
data O
set, O
we’ve O
achieved O
state-of-the-art O

on O
our O
v1 O
model O
on O
KITTI B-DAT
data O
using O
train/val O
split O
as O

of O
Frustum O
PointNet O
results O
on O
KITTI B-DAT
val O
set O
(best O
viewed O
in O

typical O
2D O
region O
proposal O
from O
KITTI B-DAT
val O
set O
with O
both O
2D O

on O
the O
fly O
(1,024 O
for O
KITTI B-DAT
and O
2,048 O
for O
SUN-RGBD). O
For O

KITTI B-DAT
Training O
The O
object O
detection O
benchmark O

in O
KITTI B-DAT
provides O
synchronized O
RGB O
images O
and O

the O
same O
as O
that O
in O
KITTI B-DAT

much O
lower O
than O
that O
in O
KITTI B-DAT
because O
of O
strong O
occlusions O
and O

compared O
to O
around O
90% O
in O
KITTI B-DAT

pedestrian, O
and O
cy- O
clist O
from O
KITTI B-DAT
dataset. O
The O
final O
model O
takes O

our O
detector’s O
AP O
(2D) O
on O
KITTI B-DAT
test O
set. O
Our O
detector O
has O

than O
current O
leading O
players O
on O
KITTI B-DAT
leader O
board. O
We’ve O
also O
reported O

2D O
object O
detection O
AP O
on O
KITTI B-DAT
test O
set. O
Evaluation O
IoU O
threshold O

the O
first O
place O
winner O
on O
KITTI B-DAT
leader O
board O
for O
pedestrians O
and O

2D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set O

3D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set. O
By O
using O
both O

can O
see O
that O
compared O
with O
KITTI B-DAT
LiDAR O
data, O
depth O
images O
can O

Method O
Modality O
Car O
Pedestrian O
CyclistEasy O
Moderate B-DAT
Hard O
Easy O
Moderate O
Hard O
Easy O

Moderate B-DAT
Hard O
Mono3D O
[3] O
Mono O
5.22 O

Method O
Modality O
Car O
Pedestrian O
CyclistEasy O
Moderate B-DAT
Hard O
Easy O
Moderate O
Hard O
Easy O

Moderate B-DAT
Hard O
Mono3D O
[3] O
Mono O
2.53 O

Benchmark O
Easy O
Moderate B-DAT
Hard O
Car O
(3D O
Detection) O
77.47 O

generate O
detections. O
Experiments O
on O
the O
KITTI B-DAT
car O
detection O
bench- O
mark O
show O

detection O
tasks, O
provided O
by O
the O
KITTI B-DAT
benchmark O
[11]. O
Experimental O
results O
show O

We O
conduct O
experiments O
on O
KITTI B-DAT
benchmark O
and O
show O
that O
VoxelNet O

LiDAR O
specifi- O
cations O
of O
the O
KITTI B-DAT
dataset O
[11]. O
Car O
Detection O
For O

We O
evaluate O
VoxelNet O
on O
the O
KITTI B-DAT
3D O
object O
detection O

average O
precision O
(in O
%) O
on O
KITTI B-DAT
validation O
set O

average O
precision O
(in O
%) O
on O
KITTI B-DAT
validation O
set O

the O
test O
results O
using O
the O
KITTI B-DAT
server O

the O
LiDAR O
data O
provided O
in O
KITTI B-DAT

4.1. O
Evaluation O
on O
KITTI B-DAT
Validation O
Set O

Metrics O
We O
follow O
the O
official O
KITTI B-DAT
evaluation O
protocol, O
where O
the O
IoU O

4.2. O
Evaluation O
on O
KITTI B-DAT
Test O
Set O

We O
evaluated O
VoxelNet O
on O
the O
KITTI B-DAT
test O
set O
by O
submit- O
ting O

other O
leading O
methods O
listed O
in O
KITTI B-DAT
benchmark O
use O
both O
RGB O
images O

Table O
3. O
Performance O
evaluation O
on O
KITTI B-DAT
test O
set O

grid. O
Our O
experiments O
on O
the O
KITTI B-DAT
car O
detection O
task O
show O
that O

Easy O
Moderate O
Hard B-DAT
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O
DoBEM O
[42 O

Easy O
Moderate O
Hard B-DAT
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O
DoBEM O
[42 O

Method O
Easy O
Moderate O
Hard B-DAT
Mono3D O
[4] O
2.53 O
2.31 O
2.31 O

Method O
Easy O
Moderate O
Hard B-DAT
Mono3D O
[4] O
5.22 O
5.19 O
4.13 O

Benchmark O
Easy O
Moderate O
Hard B-DAT
Pedestrian O
(3D O
Detection) O
70.00 O
61.32 O

Easy O
Moderate O
Hard B-DAT
Easy O
Moderate O
Hard O
Easy O
Moderate O
Hard O
SWC O
90.82 O

Subset O
Easy O
Moderate O
Hard B-DAT
AP O
(2D) O
for O
cars O
96.48 O

Method O
Easy O
Moderate O
Hard B-DAT
VeloFCN O
[18] O
15.20 O
13.66 O
15.98 O

Method O
Cars O
Pedestrians B-DAT
Cyclists O

Method O
Cars O
Pedestrians B-DAT
Cyclists O

Method O
Cars O
Pedestrians B-DAT
Cyclists O

very O
sparse O
points. O
Evaluated O
on O
KITTI B-DAT
and O
SUN O
RGB-D O
3D O
detection O

method O
achieve O
leading O
positions O
on O
KITTI B-DAT
3D O
ob- O
ject O
detection O
[1 O

ther O
fine-tune O
it O
on O
a O
KITTI B-DAT
2D O
object O
detection O
dataset O
to O

5m O
to O
beyond O
50m O
in O
KITTI B-DAT
data), O
we O
predict O
the O
3D O

for O
3D O
object O
detection O
on O
KITTI B-DAT
[10] O
and O
SUN-RGBD O
[33] O
(Sec O

our O
3D O
object O
detector O
on O
KITTI B-DAT
[11] O
and O
SUN-RGBD O
[33] O
benchmarks O

KITTI B-DAT
Tab. O
1 O
shows O
the O
performance O

our O
3D O
detector O
on O
the O
KITTI B-DAT
test O
set. O
We O
outperform O
previous O

object O
detection O
3D O
AP O
on O
KITTI B-DAT
test O
set. O
DoBEM O
[42] O
and O

AP O
(bird’s O
eye O
view) O
on O
KITTI B-DAT
test O
set. O
3D O
FCN O
[17 O

3D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set O
(cars O
only O

3D O
object O
localization O
AP O
on O
KITTI B-DAT
val O
set O
(cars O
only O

We O
also O
report O
performance O
on O
KITTI B-DAT
val O
set O
(the O
same O
split O

56.32 O
Table O
5. O
Performance O
on O
KITTI B-DAT
val O
set O
for O
pedestrians O
and O

same O
pipeline O
we O
used O
for O
KITTI B-DAT
data O
set, O
we’ve O
achieved O
state-of-the-art O

on O
our O
v1 O
model O
on O
KITTI B-DAT
data O
using O
train/val O
split O
as O

of O
Frustum O
PointNet O
results O
on O
KITTI B-DAT
val O
set O
(best O
viewed O
in O

typical O
2D O
region O
proposal O
from O
KITTI B-DAT
val O
set O
with O
both O
2D O

on O
the O
fly O
(1,024 O
for O
KITTI B-DAT
and O
2,048 O
for O
SUN-RGBD). O
For O

KITTI B-DAT
Training O
The O
object O
detection O
benchmark O

in O
KITTI B-DAT
provides O
synchronized O
RGB O
images O
and O

the O
same O
as O
that O
in O
KITTI B-DAT

much O
lower O
than O
that O
in O
KITTI B-DAT
because O
of O
strong O
occlusions O
and O

compared O
to O
around O
90% O
in O
KITTI B-DAT

pedestrian, O
and O
cy- O
clist O
from O
KITTI B-DAT
dataset. O
The O
final O
model O
takes O

our O
detector’s O
AP O
(2D) O
on O
KITTI B-DAT
test O
set. O
Our O
detector O
has O

than O
current O
leading O
players O
on O
KITTI B-DAT
leader O
board. O
We’ve O
also O
reported O

2D O
object O
detection O
AP O
on O
KITTI B-DAT
test O
set. O
Evaluation O
IoU O
threshold O

the O
first O
place O
winner O
on O
KITTI B-DAT
leader O
board O
for O
pedestrians O
and O

2D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set O

3D O
object O
detection O
AP O
on O
KITTI B-DAT
val O
set. O
By O
using O
both O

can O
see O
that O
compared O
with O
KITTI B-DAT
LiDAR O
data, O
depth O
images O
can O

Modality O
Car O
Pedestrian O
CyclistEasy O
Moderate O
Hard B-DAT
Easy O
Moderate O
Hard O
Easy O
Moderate O

Hard B-DAT
Mono3D O
[3] O
Mono O
5.22 O
5.19 O

Modality O
Car O
Pedestrian O
CyclistEasy O
Moderate O
Hard B-DAT
Easy O
Moderate O
Hard O
Easy O
Moderate O

Hard B-DAT
Mono3D O
[3] O
Mono O
2.53 O
2.31 O

Benchmark O
Easy O
Moderate O
Hard B-DAT
Car O
(3D O
Detection) O
77.47 O
65.11 O

generate O
detections. O
Experiments O
on O
the O
KITTI B-DAT
car O
detection O
bench- O
mark O
show O

detection O
tasks, O
provided O
by O
the O
KITTI B-DAT
benchmark O
[11]. O
Experimental O
results O
show O

We O
conduct O
experiments O
on O
KITTI B-DAT
benchmark O
and O
show O
that O
VoxelNet O

LiDAR O
specifi- O
cations O
of O
the O
KITTI B-DAT
dataset O
[11]. O
Car O
Detection O
For O

We O
evaluate O
VoxelNet O
on O
the O
KITTI B-DAT
3D O
object O
detection O

average O
precision O
(in O
%) O
on O
KITTI B-DAT
validation O
set O

average O
precision O
(in O
%) O
on O
KITTI B-DAT
validation O
set O

the O
test O
results O
using O
the O
KITTI B-DAT
server O

the O
LiDAR O
data O
provided O
in O
KITTI B-DAT

4.1. O
Evaluation O
on O
KITTI B-DAT
Validation O
Set O

Metrics O
We O
follow O
the O
official O
KITTI B-DAT
evaluation O
protocol, O
where O
the O
IoU O

4.2. O
Evaluation O
on O
KITTI B-DAT
Test O
Set O

We O
evaluated O
VoxelNet O
on O
the O
KITTI B-DAT
test O
set O
by O
submit- O
ting O

other O
leading O
methods O
listed O
in O
KITTI B-DAT
benchmark O
use O
both O
RGB O
images O

Table O
3. O
Performance O
evaluation O
on O
KITTI B-DAT
test O
set O

grid. O
Our O
experiments O
on O
the O
KITTI B-DAT
car O
detection O
task O
show O
that O

dis- O
cover O
various O
relations, O
including O
domain B-DAT

produced O
better O
results O
on O
a O
domain B-DAT

supervised O
baseline O
trained O
on O
the O
domain B-DAT

10Several O
of O
the O
domain B-DAT

illustrates O
this O
in O
the O
language O
domain, B-DAT
where O
sparse O
features O
are O
inter O

dings O
spaces, O
and O
integrating O
a O
domain B-DAT
clus- O
tering O
algorithm, O
our O
model O

one O
hand, O
formally O
representing O
a O
domain B-DAT
of O
knowledge O
(e.g. O
Food), O
and O

In O
domain B-DAT
knowledge O
formalization, O
prominent O
work O
has O

aware O
transformation O
matrix O
for O
each O
domain B-DAT
of O
knowledge. O
Our O
best O
configuration O

semantically O
related O
terms O
for O
collecting O
domain B-DAT

take O
ad- O
vantage O
of O
a O
domain B-DAT

the O
training O
set O
into O
a O
domain B-DAT
cluster O
C O
(Section O
4.1). O
Then O

sensitive O
to O
a O
predefined O
knowledge O
domain, B-DAT
under O
the O
assump- O
tion O
that O

synset O
into O
its O
most O
representative O
domain, B-DAT
which O
is O
achieved O
by O
exploiting O

Wikipedia O
featured O
articles O
page O
each O
domain B-DAT
is O
composed O
of O
128 O
Wikipedia O

concepts O
as- O
sociated O
with O
each O
domain, B-DAT
we O
leverage O
NASARI10 O

lexical O
vector O
for O
each O
Wikipedia O
domain B-DAT
by O
concatenating O
all O
Wikipedia O
pages O

representing O
the O
given O
domain B-DAT
into O
a O
single O
text. O
Finally O

lexical O
vector O
and O
all O
the O
domain B-DAT
vectors, O
selecting O
the O
domain O
leading O

is O
the O
vector O
of O
the O
domain B-DAT
d O
∈ O
D, O
~b O
is O

a O
highly O
reliable O
set O
of O
domain B-DAT
labels, O
those O

synsets O
are O
labelled O
with O
a O
domain B-DAT

BabelNet O
synsets O
labelled O
with O
the O
domain B-DAT
d. O
For O
each O
domain-wise O
expanded O

a O
transformation O
matrix O
for O
each O
domain B-DAT
cluster O
Cd O
by O
minimizing O

For O
each O
domain, B-DAT
we O
retain O
5k, O
10k, O
15k O

extra O
OIE-derived O
training O
pairs O
per O
domain B-DAT
(gen- O
erating O
two O
more O
systems O

pair O
(~xd,~yd) O
of O
a O
given O
domain B-DAT
d. O
Then, O
we O
aver- O
age O

global O
vector O
~Vd O
for O
the O
domain B-DAT
d. O
Finally, O
given O
a O
test O

11Using O
the O
25k O
domain B-DAT

main O
clusters O
and O
metrics, O
with O
domain B-DAT

domains B-DAT
are O
concerned, O
the O
biology O
domain O
seems O
to O
be O
easier O
to O

100krwd O
configuration O
performs O
on O
this O
domain B-DAT

. O
This O
is O
the O
only O
domain B-DAT
in O
which O
training O
with O
no O

illustrative O
example O
from O
the O
transport O
domain B-DAT

P@k O
scores O
for O
the O
transport O
domain B-DAT

our O
best O
run O
in O
each O
domain, B-DAT
and O
estimated O
precision O
over O
them O

the O
first O
one O
includes O
25k O
domain B-DAT

test O
BabelNet O
synsets O
(20 O
per O
domain) B-DAT
whose O
hypernyms O
are O
missing O
in O

treatment O
planning O
in O
the O
health O
domain B-DAT
or O
decoration O
for O
molding O
in O

the O
art O
domain)14 B-DAT

there O
exists, O
for O
a O
given O
domain B-DAT

BabelNet O
synsets O
into O
a O
predefined O
domain B-DAT
of O
knowledge. O
Then, O
we O
collect O

increase O
training O
data. O
Our O
best O
domain B-DAT

we O
see O
potential O
in O
the O
domain B-DAT
clustering O
approach O
for O
im- O
proving O

taxonomy O
are O
for O
a O
specific O
domain B-DAT

ex- O
tending, O
taxonomizing O
and O
semantifying O
domain B-DAT
ter- O
minologies. O
AAAI O

domain B-DAT
textual O
question O
an- O
swering O
techniques O

dings O
spaces, O
and O
integrating O
a O
domain B-DAT
clus- O
tering O
algorithm, O
our O
model O

one O
hand, O
formally O
representing O
a O
domain B-DAT
of O
knowledge O
(e.g. O
Food), O
and O

In O
domain B-DAT
knowledge O
formalization, O
prominent O
work O
has O

aware O
transformation O
matrix O
for O
each O
domain B-DAT
of O
knowledge. O
Our O
best O
configuration O

semantically O
related O
terms O
for O
collecting O
domain B-DAT

take O
ad- O
vantage O
of O
a O
domain B-DAT

the O
training O
set O
into O
a O
domain B-DAT
cluster O
C O
(Section O
4.1). O
Then O

sensitive O
to O
a O
predefined O
knowledge O
domain, B-DAT
under O
the O
assump- O
tion O
that O

synset O
into O
its O
most O
representative O
domain, B-DAT
which O
is O
achieved O
by O
exploiting O

Wikipedia O
featured O
articles O
page O
each O
domain B-DAT
is O
composed O
of O
128 O
Wikipedia O

concepts O
as- O
sociated O
with O
each O
domain, B-DAT
we O
leverage O
NASARI10 O

lexical O
vector O
for O
each O
Wikipedia O
domain B-DAT
by O
concatenating O
all O
Wikipedia O
pages O

representing O
the O
given O
domain B-DAT
into O
a O
single O
text. O
Finally O

lexical O
vector O
and O
all O
the O
domain B-DAT
vectors, O
selecting O
the O
domain O
leading O

is O
the O
vector O
of O
the O
domain B-DAT
d O
∈ O
D, O
~b O
is O

a O
highly O
reliable O
set O
of O
domain B-DAT
labels, O
those O

synsets O
are O
labelled O
with O
a O
domain B-DAT

BabelNet O
synsets O
labelled O
with O
the O
domain B-DAT
d. O
For O
each O
domain-wise O
expanded O

a O
transformation O
matrix O
for O
each O
domain B-DAT
cluster O
Cd O
by O
minimizing O

For O
each O
domain, B-DAT
we O
retain O
5k, O
10k, O
15k O

extra O
OIE-derived O
training O
pairs O
per O
domain B-DAT
(gen- O
erating O
two O
more O
systems O

pair O
(~xd,~yd) O
of O
a O
given O
domain B-DAT
d. O
Then, O
we O
aver- O
age O

global O
vector O
~Vd O
for O
the O
domain B-DAT
d. O
Finally, O
given O
a O
test O

11Using O
the O
25k O
domain B-DAT

main O
clusters O
and O
metrics, O
with O
domain B-DAT

domains B-DAT
are O
concerned, O
the O
biology O
domain O
seems O
to O
be O
easier O
to O

100krwd O
configuration O
performs O
on O
this O
domain B-DAT

. O
This O
is O
the O
only O
domain B-DAT
in O
which O
training O
with O
no O

illustrative O
example O
from O
the O
transport O
domain B-DAT

P@k O
scores O
for O
the O
transport O
domain B-DAT

our O
best O
run O
in O
each O
domain, B-DAT
and O
estimated O
precision O
over O
them O

the O
first O
one O
includes O
25k O
domain B-DAT

test O
BabelNet O
synsets O
(20 O
per O
domain) B-DAT
whose O
hypernyms O
are O
missing O
in O

treatment O
planning O
in O
the O
health O
domain B-DAT
or O
decoration O
for O
molding O
in O

the O
art O
domain)14 B-DAT

there O
exists, O
for O
a O
given O
domain B-DAT

BabelNet O
synsets O
into O
a O
predefined O
domain B-DAT
of O
knowledge. O
Then, O
we O
collect O

increase O
training O
data. O
Our O
best O
domain B-DAT

we O
see O
potential O
in O
the O
domain B-DAT
clustering O
approach O
for O
im- O
proving O

taxonomy O
are O
for O
a O
specific O
domain B-DAT

ex- O
tending, O
taxonomizing O
and O
semantifying O
domain B-DAT
ter- O
minologies. O
AAAI O

domain B-DAT
textual O
question O
an- O
swering O
techniques O

task O
including O
the O
general-purpose O
and O
domain B-DAT

general-purpose O
substask O
for O
English O
and O
domain B-DAT

subtask O
for O
English O
and O
0.5h O
domain B-DAT

-specific O
domain B-DAT

domain B-DAT
sub- O
task O
for O
English. O
All O

3: O
Gold O
standard O
evaluation O
on O
domain B-DAT

sense O
embedding, O
showing O
that O
in O
domain B-DAT

ex- O
tending, O
taxonomizing O
and O
semantifying O
domain B-DAT
terminologies. O
In O
Proceedings O
of O
the O

domain B-DAT
ques- O
tion O
answering. O
In O
Proceedings O

scored O
highest O
in O
the O
medical O
domain B-DAT
among O
the O
competing O
unsu- O
pervised O

poorly O
on O
the O
music O
industry O
domain B-DAT

the O
vocabulary O
of O
a O
specific O
domain B-DAT
(Espinosa-Anke O
et O
al., O
2016). O
This O

main O
vocabulary O
and O
two O
specialised O
domain B-DAT
vo- O
cabularies O
in O
English: O
medical O

English O
vo- O
cabularies, O
general O
language O
domain B-DAT
vocabularies O
for O
Spanish O
and O
Italian O

show O
that O
for O
the O
medical O
domain B-DAT
subtask, O
our O
system O
beats O
the O

of O
eighteen O
on O
the O
medical O
domain B-DAT
subtask O
with O
a O
Mean O
Average O

On O
the O
music O
indus- O
try O
domain B-DAT
subtask, O
our O
system O
ranked O
13th O

representations. O
82% O
of O
the O
medical O
domain B-DAT
input O
words O
have O
at O
least O

92% O
of O
the O
music O
industry O
domain B-DAT
input O
words O
have O
multi-word O
ex O

system O
scored O
highest O
in O
the O
medical B-DAT
domain I-DAT
among O
the O
competing O
unsu- O
pervised O

we O
show O
that O
for O
the O
medical B-DAT
domain I-DAT
subtask, O
our O
system O
beats O
the O

out O
of O
eighteen O
on O
the O
medical B-DAT
domain I-DAT
subtask O
with O
a O
Mean O
Average O

tor O
representations. O
82% O
of O
the O
medical B-DAT
domain I-DAT
input O
words O
have O
at O
least O

constructed O
by O
crawling O
the O
.uk O
domain, B-DAT
and O
WaCkypedia O
EN O
(Baroni O
et O

dis- O
cover O
various O
relations, O
including O
domain B-DAT

produced O
better O
results O
on O
a O
domain B-DAT

supervised O
baseline O
trained O
on O
the O
domain B-DAT

10Several O
of O
the O
domain B-DAT

illustrates O
this O
in O
the O
language O
domain, B-DAT
where O
sparse O
features O
are O
inter O

dings O
spaces, O
and O
integrating O
a O
domain B-DAT
clus- O
tering O
algorithm, O
our O
model O

one O
hand, O
formally O
representing O
a O
domain B-DAT
of O
knowledge O
(e.g. O
Food), O
and O

In O
domain B-DAT
knowledge O
formalization, O
prominent O
work O
has O

aware O
transformation O
matrix O
for O
each O
domain B-DAT
of O
knowledge. O
Our O
best O
configuration O

semantically O
related O
terms O
for O
collecting O
domain B-DAT

take O
ad- O
vantage O
of O
a O
domain B-DAT

the O
training O
set O
into O
a O
domain B-DAT
cluster O
C O
(Section O
4.1). O
Then O

sensitive O
to O
a O
predefined O
knowledge O
domain, B-DAT
under O
the O
assump- O
tion O
that O

synset O
into O
its O
most O
representative O
domain, B-DAT
which O
is O
achieved O
by O
exploiting O

Wikipedia O
featured O
articles O
page O
each O
domain B-DAT
is O
composed O
of O
128 O
Wikipedia O

concepts O
as- O
sociated O
with O
each O
domain, B-DAT
we O
leverage O
NASARI10 O

lexical O
vector O
for O
each O
Wikipedia O
domain B-DAT
by O
concatenating O
all O
Wikipedia O
pages O

representing O
the O
given O
domain B-DAT
into O
a O
single O
text. O
Finally O

lexical O
vector O
and O
all O
the O
domain B-DAT
vectors, O
selecting O
the O
domain O
leading O

is O
the O
vector O
of O
the O
domain B-DAT
d O
∈ O
D, O
~b O
is O

a O
highly O
reliable O
set O
of O
domain B-DAT
labels, O
those O

synsets O
are O
labelled O
with O
a O
domain B-DAT

BabelNet O
synsets O
labelled O
with O
the O
domain B-DAT
d. O
For O
each O
domain-wise O
expanded O

a O
transformation O
matrix O
for O
each O
domain B-DAT
cluster O
Cd O
by O
minimizing O

For O
each O
domain, B-DAT
we O
retain O
5k, O
10k, O
15k O

extra O
OIE-derived O
training O
pairs O
per O
domain B-DAT
(gen- O
erating O
two O
more O
systems O

pair O
(~xd,~yd) O
of O
a O
given O
domain B-DAT
d. O
Then, O
we O
aver- O
age O

global O
vector O
~Vd O
for O
the O
domain B-DAT
d. O
Finally, O
given O
a O
test O

11Using O
the O
25k O
domain B-DAT

main O
clusters O
and O
metrics, O
with O
domain B-DAT

domains B-DAT
are O
concerned, O
the O
biology O
domain O
seems O
to O
be O
easier O
to O

100krwd O
configuration O
performs O
on O
this O
domain B-DAT

. O
This O
is O
the O
only O
domain B-DAT
in O
which O
training O
with O
no O

illustrative O
example O
from O
the O
transport O
domain B-DAT

P@k O
scores O
for O
the O
transport O
domain B-DAT

our O
best O
run O
in O
each O
domain, B-DAT
and O
estimated O
precision O
over O
them O

the O
first O
one O
includes O
25k O
domain B-DAT

test O
BabelNet O
synsets O
(20 O
per O
domain) B-DAT
whose O
hypernyms O
are O
missing O
in O

treatment O
planning O
in O
the O
health O
domain B-DAT
or O
decoration O
for O
molding O
in O

the O
art O
domain)14 B-DAT

there O
exists, O
for O
a O
given O
domain B-DAT

BabelNet O
synsets O
into O
a O
predefined O
domain B-DAT
of O
knowledge. O
Then, O
we O
collect O

increase O
training O
data. O
Our O
best O
domain B-DAT

we O
see O
potential O
in O
the O
domain B-DAT
clustering O
approach O
for O
im- O
proving O

taxonomy O
are O
for O
a O
specific O
domain B-DAT

ex- O
tending, O
taxonomizing O
and O
semantifying O
domain B-DAT
ter- O
minologies. O
AAAI O

domain B-DAT
textual O
question O
an- O
swering O
techniques O

task O
including O
the O
general-purpose O
and O
domain B-DAT

general-purpose O
substask O
for O
English O
and O
domain B-DAT

subtask O
for O
English O
and O
0.5h O
domain B-DAT

-specific O
domain B-DAT

domain B-DAT
sub- O
task O
for O
English. O
All O

3: O
Gold O
standard O
evaluation O
on O
domain B-DAT

sense O
embedding, O
showing O
that O
in O
domain B-DAT

ex- O
tending, O
taxonomizing O
and O
semantifying O
domain B-DAT
terminologies. O
In O
Proceedings O
of O
the O

domain B-DAT
ques- O
tion O
answering. O
In O
Proceedings O

scored O
highest O
in O
the O
medical O
domain B-DAT
among O
the O
competing O
unsu- O
pervised O

poorly O
on O
the O
music O
industry O
domain B-DAT

the O
vocabulary O
of O
a O
specific O
domain B-DAT
(Espinosa-Anke O
et O
al., O
2016). O
This O

main O
vocabulary O
and O
two O
specialised O
domain B-DAT
vo- O
cabularies O
in O
English: O
medical O

English O
vo- O
cabularies, O
general O
language O
domain B-DAT
vocabularies O
for O
Spanish O
and O
Italian O

show O
that O
for O
the O
medical O
domain B-DAT
subtask, O
our O
system O
beats O
the O

of O
eighteen O
on O
the O
medical O
domain B-DAT
subtask O
with O
a O
Mean O
Average O

On O
the O
music O
indus- O
try O
domain B-DAT
subtask, O
our O
system O
ranked O
13th O

representations. O
82% O
of O
the O
medical O
domain B-DAT
input O
words O
have O
at O
least O

92% O
of O
the O
music O
industry O
domain B-DAT
input O
words O
have O
multi-word O
ex O

constructed O
by O
crawling O
the O
.uk O
domain, B-DAT
and O
WaCkypedia O
EN O
(Baroni O
et O

hypernym O
discovery O
(Camacho-Collados, O
2017). O
The O
general B-DAT
idea O
is O
to O
learn O
a O

of O
the O
5 O
sub-tasks: O
1A O
(general), B-DAT
2A O
(medical), O
and O
2B O
(music O

so O
training O
our O
system O
on O
general B-DAT

Evaluation O
shows O
that O
the O
general B-DAT
trend O
is O
that O
our O
hypernym O

and O
David O
Weir. O
2003. O
A O
general B-DAT
frame- O
work O
for O
distributional O
similarity O

general B-DAT

tend O
to O
refer O
to O
more O
general B-DAT
concepts O
and O
more O
general O
hypernymy O

the O
most O
frequent O
hypernyms O
in O
general B-DAT

hyperym O
discovery O
task O
including O
the O
general B-DAT

Our O
hypernym O
discovery O
experiments O
include O
general B-DAT

980Ti), O
with O
roughly O
1.5h O
for O
general B-DAT

2 O
shows O
the O
result O
on O
general B-DAT

2: O
Gold O
standard O
evaluation O
on O
general B-DAT

verifies O
our O
hypothesis O
in O
the O
general B-DAT

though O
they O
work O
closely O
in O
general B-DAT

relia- O
bility. O
Being O
based O
on O
general B-DAT
linguistic O
hypotheses O
and O
independent O
from O

easily O
interpretable, O
being O
based O
on O
general B-DAT
linguistic O
hypotheses O

likely O
to O
occur O
in O
more O
general B-DAT
contexts O
than O
their O
hyponyms O

fact O
that, O
even O
though—being O
more O
general B-DAT

cases O
the O
holonym O
is O
more O
general B-DAT
than O
the O
meronym O
(Shwartz O
et O

should O
therefore O
not O
be O
too O
general B-DAT

which O
the O
holonym O
is O
more O
general B-DAT
than O
the O
meronym O
by O
definition O

that O
y O
is O
a O
more O
general B-DAT
word O
than O
x, O
which O
is O

y O
is O
also O
not O
more O
general B-DAT
than O
x. O
We O
observed O
that O

ys, O
many O
of O
which O
are O
general B-DAT
words O
like O
animal O
and O
object O

of O
these O
pairs O
re- O
veals O
general B-DAT
words O
that O
appear O
in O
many O

2010. O
Distri- O
butional O
memory: O
A O
general B-DAT
framework O
for O
corpus- O
based O
semantics O

babi B-DAT

5.2 O
BABI B-DAT
TASKS O

C O
ADDITIONAL O
RESULTS O
ON O
BABI B-DAT
TASKS O

different O
memory O
hops O
for O
the O
bAbi B-DAT
tasks. O
The O
model O
is O
PE+LS+RN O

C O
QUESTION O
ANSWERING O
BABI B-DAT
TASK O

5.1 O
BABI B-DAT
STORY-BASED O
QA O

5.2 O
BABI B-DAT
DIALOG O

VoxCeleb2 B-DAT

ablation O
studies, O
while O
by O
using O
VoxCeleb2 B-DAT

our O
method O
on O
a O
larger O
VoxCeleb2 B-DAT

from O
test O
videos O
of O
the O
VoxCeleb2 B-DAT

our O
best O
models O
on O
the O
VoxCeleb2 B-DAT

poses O
were O
taken O
from O
the O
VoxCeleb2 B-DAT

was O
trained O
only O
for O
the O
VoxCeleb2 B-DAT

extended O
qualitative O
comparisons O
on O
the O
VoxCeleb2 B-DAT

extended O
qualitative O
comparison O
on O
the O
VoxCeleb2 B-DAT

learning B-DAT
on O
a O
large O
dataset O
of O

to O
frame O
few- O
and O
one-shot O
learning B-DAT
of O
neural O
talking O
head O
models O

fields O
synthesized O
using O
ma- O
chine O
learning B-DAT
(including O
deep O
learning) O
[11, O
29 O

of O
photographs O
(so-called O
few- O
shot O
learning) B-DAT
and O
with O
limited O
training O
time O

on O
a O
single O
photograph O
(one-shot O
learning), B-DAT
while O
adding O
a O
few O
more O

The O
few-shot O
learning B-DAT
ability O
is O
obtained O
through O
exten O

learning) B-DAT
on O
a O
large O
corpus O
of O

learning, B-DAT
our O
sys- O
tem O
simulates O
few-shot O

learning B-DAT
tasks O
and O
learns O
to O
trans O

sets O
up O
a O
new O
adversarial O
learning B-DAT
problem O
with O
high-capacity O
generator O
and O

learning B-DAT

and, O
more O
recently, O
with O
deep O
learning B-DAT
[22, O
25] O
(to O
name O
just O

learning B-DAT
stage O
uses O
the O
adaptive O
instance O

learning B-DAT
to O
obtain O
the O
initial O
state O

learning B-DAT

learning B-DAT
[41] O
use O
adversarially- O
trained O
networks O

learning B-DAT
stage. O
While O
these O
methods O
are O

adversarial O
fine-tuning O
into O
the O
meta- O
learning B-DAT
framework. O
The O
former O
is O
applied O

learning B-DAT
stage O

4, O
18]. O
Their O
setting O
(few-shot O
learning B-DAT
of O
generative O
models) O
and O
some O

domain, O
the O
use O
of O
adversarial O
learning, B-DAT
its O
specific O
adaptation O
to O
the O

learning B-DAT
process O
and O
numerous O
im- O
plementation O

learning B-DAT
architecture O
involves O
the O
embedder O
network O

learning, B-DAT
we O
pass O
sets O
of O
frames O

learning B-DAT
stage O
of O
our O
approach O
assumes O

its O
t-th O
frame. O
During O
the O
learning B-DAT
process, O
as O
well O
as O
during O

learning B-DAT
stage O
of O
our O
approach, O
the O

learning B-DAT
stage. O
In O
general, O
during O
meta-learning O

learning, B-DAT
only O
ψ O
are O
trained O
directly O

learning B-DAT
stage O

learning B-DAT
stage O
of O
our O
approach, O
the O

3.3. O
Few-shot O
learning B-DAT
by O
fine-tuning O

learning B-DAT
has O
converged, O
our O
system O
can O

learning B-DAT
stage. O
As O
before, O
the O
synthe O

learning B-DAT
stage O

learning B-DAT
stage. O
A O
straightforward O
way O
to O

learning B-DAT
with O
a O
single O
video O
sequence O

learning B-DAT
stage O
to O
initialize O
ψ′, O
i.e O

learning B-DAT
stage. O
The O
initialization O
of O
w O

learning B-DAT
stage O

learning B-DAT
stage. O
For O
the O
intiailization, O
we O

learning B-DAT
dataset). O
However, O
the O
match O
term O

learning B-DAT
process O
ensures O
the O
similarity O
between O

Once O
the O
new O
learning B-DAT
problem O
is O
set O
up, O
the O

follow O
directly O
from O
the O
meta- O
learning B-DAT
variants. O
Thus, O
the O
generator O
parameters O

learning B-DAT
stage O
is O
also O
crucial. O
As O

Adam O
[21]. O
We O
set O
the O
learning B-DAT
rate O
of O
the O
embedder O
and O

different O
datasets O
with O
multiple O
few-shot O
learning B-DAT
settings. O
Please O
re- O
fer O
to O

fine-tune O
all O
models O
on O
few-shot O
learning B-DAT
sets O
of O
size O
T O
for O

learning B-DAT
(or O
pretraining) O
stage. O
After O
the O

few-shot O
learning, B-DAT
the O
evaluation O
is O
performed O
on O

T O
used O
in O
few- O
shot O
learning B-DAT

we O
perform O
one- O
and O
few-shot O
learning B-DAT
on O
a O
video O
of O
a O

learning B-DAT
or O
pretraining. O
We O
set O
the O

the O
comparison O
of O
the O
few-shot O
learning B-DAT
timings. O
Both O
are O
provided O
in O

allow O
to O
trade O
off O
few-shot O
learning B-DAT
speed O
versus O
the O
results O
quality O

per- O
forms O
better O
for O
low-shot O
learning B-DAT
(e.g. O
one-shot), O
while O
the O
FT O

variant O
allows O
fast O
(real-time) O
few-shot O
learning B-DAT
of O
new O
avatars, O
fine-tuning O
ultimately O

learning B-DAT
of O
ad O

and O
S. O
Levine. O
Model-agnostic O
meta- O
learning B-DAT
for O
fast O
adaptation O
of O
deep O

Y. O
Wu, O
et O
al. O
Transfer O
learning B-DAT
from O
speaker O
verification O
to O
multispeaker O

I. O
Kemelmacher- O
Shlizerman. O
Synthesizing O
Obama: O
learning B-DAT
lip O
sync O
from O
au- O
dio O

and O
Y. O
Wang. O
Adversarial O
meta- O
learning B-DAT

An O
adversarial O
approach O
to O
few-shot O
learning B-DAT

Pix2pixHD O
and O
our O
method, O
few-shot O
learning B-DAT
was O
done O
via O
fine-tuning O
for O

Method O
(T) O
Time, O
s O
Few-shot O
learning B-DAT

2: O
Quantitative O
comparison O
of O
few-shot O
learning B-DAT
and O
inference O
timings O
for O
the O

learning B-DAT

multiple O
training O
frames O
in O
few-shot O
learning B-DAT
problems, O
like O
in O
our O
final O

learning B-DAT
configu- O
ration, O
which O
turned O
out O

learning, B-DAT
we O
randomly O
initialize O
the O
person-specific O

learning B-DAT
objective O
and O
initialize O
the O
embedding O

learning B-DAT
or O
pretraining. O
We O
used O
eight O

shot O
learning B-DAT
problem O
formulation. O
The O
notation O
for O

learning B-DAT
or O
pretraining. O
We O
used O
eight O

shot O
learning B-DAT
problem O
formulation. O
The O
notation O
for O

8 O
32 B-DAT

8 O
32 B-DAT

8 O
32 B-DAT

8 O
32 B-DAT

8 O
32 B-DAT

8 O
32 B-DAT

8 O
32 B-DAT

8 O
32 B-DAT

videos O
at O
1 O
fps) O
and O
VoxCeleb2 B-DAT
[8] O
(224p O
videos O
at O
25 O

VoxCeleb2 B-DAT
Ours-FF O
(1) O
46.1 O
0.61 O
0.42 O

ablation O
studies, O
while O
by O
using O
VoxCeleb2 B-DAT
we O
show O
the O
full O
potential O

our O
method O
on O
a O
larger O
VoxCeleb2 B-DAT
dataset. O
Here, O
we O
train O
two O

from O
test O
videos O
of O
the O
VoxCeleb2 B-DAT
dataset. O
We O
rank O
these O
videos O

our O
best O
models O
on O
the O
VoxCeleb2 B-DAT
dataset. O
The O
number O
of O
training O

poses O
were O
taken O
from O
the O
VoxCeleb2 B-DAT
dataset. O
Digital O
zoom O
recommended O

was O
trained O
only O
for O
the O
VoxCeleb2 B-DAT
dataset. O
The O
comparison O
was O
car O

extended O
qualitative O
comparisons O
on O
the O
VoxCeleb2 B-DAT
dataset. O
Here, O
the O
comparison O
is O

extended O
qualitative O
comparison O
on O
the O
VoxCeleb2 B-DAT
dataset. O
Here, O
we O
compare O
qualitative O

- B-DAT

- B-DAT

- B-DAT
tional O
neural O
networks O
to O
generate O

- B-DAT
ate O
a O
personalized O
talking O
head O

- B-DAT

- B-DAT

- B-DAT
ter O
that O
is O
able O
to O

- B-DAT
and O
one-shot O
learning O
of O
neural O

- B-DAT
sarial O
training O
problems O
with O
high O

- B-DAT

- B-DAT
alized O
photorealistic O
talking O
head O
models O

- B-DAT

- B-DAT
sions O
and O
mimics O
of O
a O

- B-DAT
ically, O
we O
consider O
the O
problem O

- B-DAT
alistic O
personalized O
head O
images O
given O

- B-DAT
marks, O
which O
drive O
the O
animation O

- B-DAT
conferencing O
and O
multi-player O
games, O
as O

- B-DAT
fects O
industry. O
Synthesizing O
realistic O
talking O

- B-DAT
ity. O
This O
complexity O
stems O
not O

- B-DAT
man O
visual O
system O
towards O
even O

- B-DAT
pearance O
modeling O
of O
human O
heads O

- B-DAT

- B-DAT
takes O
explains O
the O
current O
prevalence O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferencing O
systems O

- B-DAT
posed O
to O
synthesize O
articulated O
head O

- B-DAT
chine O
learning O
(including O
deep O
learning O

- B-DAT

- B-DAT
age, O
the O
amount O
of O
motion O

- B-DAT

- B-DAT

- B-DAT
vNets) O
presents O
the O
new O
hope O

- B-DAT
ever, O
to O
succeed, O
such O
methods O

- B-DAT
lions O
of O
parameters O
for O
each O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
phisticated O
physical O
and O
optical O
modeling O

- B-DAT
cessive O
for O
most O
practical O
telepresence O

- B-DAT
els O
with O
as O
little O
effort O

- B-DAT

- B-DAT
shot O
learning) O
and O
with O
limited O

- B-DAT

- B-DAT
larly O
to O
[16, O
20, O
37 O

- B-DAT
yond O
the O
abilities O
of O
warping-based O

- B-DAT

- B-DAT
sive O
pre-training O
(meta-learning) O
on O
a O

- B-DAT
ing O
head O
videos O
corresponding O
to O

- B-DAT
verse O
appearance. O
In O
the O
course O

- B-DAT

- B-DAT
tem O
simulates O
few-shot O
learning O
tasks O

- B-DAT
form O
landmark O
positions O
into O
realistically-looking O

- B-DAT
alized O
photographs, O
given O
a O
small O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ficient O
realism O
and O
personalization O
fidelity O

- B-DAT
ing O
head O
models, O
including O
video O

- B-DAT
ing O
of O
the O
appearance O
of O

- B-DAT

- B-DAT

- B-DAT
tension O
of O
the O
face O
modeling O

- B-DAT
ability O
and O
higher O
complexity O
than O

- B-DAT
ple, O
the O
results O
of O
face O

- B-DAT
fledged O
talking O
head O
system O

- B-DAT
tecture O
uses O
adversarial O
training O
[12 O

- B-DAT
ing O
projection O
discriminators O
[32]. O
Our O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sifier, O
from O
which O
it O
can O

- B-DAT
fiers O
of O
unseen O
classes, O
given O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
GAN O
[43], O
adversarial O
meta-learning O
[41 O

- B-DAT
trained O
networks O
to O
generate O
additional O

- B-DAT

- B-DAT

- B-DAT
mance, O
our O
method O
deals O
with O

- B-DAT
ation O
models O
using O
similar O
adversarial O

- B-DAT
marize, O
we O
bring O
the O
adversarial O

- B-DAT

- B-DAT
learning O
framework. O
The O
former O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cation O
domain, O
the O
use O
of O

- B-DAT

- B-DAT
plementation O
details O

- B-DAT

- B-DAT
marks) O
to O
the O
embedding O
vectors O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
quence O
and O
with O
xi(t) O
its O

- B-DAT

- B-DAT
ity O
of O
the O
face O
landmarks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

these O
inputs O
into O
an O
N O
- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
yi(t) O
for O
the O
video O

- B-DAT
sized O
video O
frame O
x̂i(t). O
The O

- B-DAT
imize O
the O
similarity O
between O
its O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trix O
P: O
ψ̂i O
= O
Pêi O

landmark O
image O
into O
an O
N O
- B-DAT

- B-DAT
criminator O
predicts O
a O
single O
scalar O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rameters O
of O
all O
three O
networks O

- B-DAT

- B-DAT
ing O
(K O
= O
8 O
in O

- B-DAT
domly O
draw O
a O
training O
video O

- B-DAT
ditional O
K O
frames O
s1, O
s2 O

- B-DAT

- B-DAT
ding O
by O
simply O
averaging O
the O

- B-DAT

- B-DAT
tion O
x̂i(t) O
using O
the O
perceptual O

- B-DAT
responding O
to O
VGG19 O
[30] O
network O

- B-DAT
sentially O
is O
a O
perceptual O
similarity O

- B-DAT
respond O
to O
individual O
videos. O
The O

maps O
its O
inputs O
to O
anN O
- B-DAT

- B-DAT

- B-DAT
criminator. O
The O
match O
term O
LMCH(φ,W O

- B-DAT

- B-DAT
ters O
θ,W,w0, O
b O
of O
the O

- B-DAT
courages O
the O
increase O
of O
the O

- B-DAT
ample O
x̂i(t) O
and O
the O
real O

- B-DAT
nating O
updates O
of O
the O
embedder O

- B-DAT
imize O
the O
losses O
LCNT,LADV O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sis O
is O
conditioned O
on O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
timate O
the O
embedding O
for O
the O

- B-DAT

- B-DAT
sponding O
to O
new O
landmark O
images O

- B-DAT
erator O
using O
the O
estimated O
embedding O

- B-DAT
learned O
parameters O
ψ, O
as O
well O

- B-DAT
able O
identity O
gap O
that O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sult O
of O
the O
meta-learning O
stage O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tors O
computed O
by O
the O
embedder O

- B-DAT
tions O
of O
the O
fine-tuning O
stage O

- B-DAT
learning O
variants. O
Thus, O
the O
generator O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
son O
et. O
al. O
[19], O
but O

- B-DAT
malization O
[15] O
replaced O
by O
instance O

- B-DAT

- B-DAT
efficients O
of O
instance O
normalization O
layers O

- B-DAT

- B-DAT
ization O
layers O
in O
the O
downsampling O

- B-DAT
mark O
images O
yi(t O

- B-DAT
tional O
part O
of O
the O
discriminator O

- B-DAT
out O
normalization O
layers). O
The O
discriminator O

- B-DAT
pared O
to O
the O
embedder, O
has O

- B-DAT

- B-DAT
serted O
at O
32×32 O
spatial O
resolution O

- B-DAT
tween O
activations O
of O
Conv1,6,11,20,29 O
VGG19 O

- B-DAT
nally, O
for O
LMCH O
we O
set O

- B-DAT
tional O
layers O
to O
64 O
and O

- B-DAT
tor O
has O
38 O
million O
parameters O

- B-DAT
titative O
and O
qualitative O
evaluation: O
VoxCeleb1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fer O
to O
the O
text O
for O

- B-DAT

- B-DAT

- B-DAT
son O
not O
seen O
during O
meta-learning O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
reenactment O
scenario). O
For O
the O
evaluation O

- B-DAT
out O
frames O
for O
each O
of O

- B-DAT

- B-DAT

- B-DAT
realism O
and O
identity O
preservation O
of O

- B-DAT

- B-DAT

- B-DAT
bedding O
vectors O
of O
the O
state-of-the-art O

- B-DAT
work O
[9] O
for O
measuring O
identity O

- B-DAT
tual O
similarity O
and O
realism O
of O

- B-DAT
man O
respondents. O
We O
show O
people O

- B-DAT

- B-DAT
not O
spot O
fakes O
based O
on O

- B-DAT

- B-DAT

- B-DAT
Celeb1 O
dataset). O
For O
Pix2pixHD, O
we O

- B-DAT

- B-DAT
shot O
learning. O
X2Face, O
as O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
which O
arguably O
gives O
X2Face O

- B-DAT
lines O
in O
three O
different O
setups O

- B-DAT

- B-DAT

- B-DAT
dom O
from O
the O
other O
video O

- B-DAT

- B-DAT

- B-DAT
perform O
our O
method O
on O
the O

- B-DAT
imizes O
only O
perceptual O
metric, O
without O

- B-DAT
and O
few-shot O
learning O
on O
a O

- B-DAT

- B-DAT
ter O
correlates O
with O
visual O
quality O

- B-DAT
ble O
1-Top O
with O
the O
results O

- B-DAT
alism O
and O
personalization O
degree O
achieved O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
out O
fine-tuning O
(by O
simply O
predicting O

- B-DAT
ant O
is O
trained O
for O
half O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults, O
where O
animation O
is O
driven O

- B-DAT
ent O
video O
of O
the O
same O

- B-DAT
tary O
material O
and O
in O
Figure O

- B-DAT
ble O
1-Bottom) O
and O
the O
visual O

- B-DAT
forms O
better O
for O
low-shot O
learning O

- B-DAT

- B-DAT
ial O
fine-tuning O

- B-DAT

- B-DAT
sons O
with O
similar O
geometry O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tographs O
in O
the O
source O
column O

- B-DAT

- B-DAT
realistic O
virtual O
talking O
heads O
in O

- B-DAT
ization O
score O
in O
our O
user O

- B-DAT
ics O
representation O
(in O
particular, O
the O

- B-DAT
son O
leads O
to O
a O
noticeable O

- B-DAT
ing O
a O
different O
person O
and O

- B-DAT
proach O
already O
provides O
a O
high-realism O

- B-DAT
puter O
Graphics O
and O
Applications, O
30(4):20–31 O

- B-DAT
sarial O
networks. O
In O
Artificial O
Neural O

Networks O
and O
Machine O
Learning O
- B-DAT
ICANN, O
pages O
594–603, O
2018. O
2 O

- B-DAT

- B-DAT

- B-DAT
thesis O
of O
3d O
faces. O
In O

- B-DAT
29, O
2017, O
pages O
1021–1030, O
2017 O

- B-DAT

- B-DAT
learning O
for O
fast O
adaptation O
of O

- B-DAT
ulation. O
In O
European O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
nets. O
In O
Advances O

- B-DAT

- B-DAT
wanathan, O
and O
R. O
Garnett, O
editors O

- B-DAT
formation O
Processing O
Systems O
30, O
pages O

- B-DAT
time O
with O
adaptive O
instance O
normalization O

- B-DAT
ternational O
Conference O
on O
Machine O
Learning O

- B-DAT

- B-DAT

- B-DAT
shick, O
S. O
Guadarrama, O
and O
T O

- B-DAT
tional O
architecture O
for O
fast O
feature O

- B-DAT

- B-DAT
speech O
synthesis. O
In O
Proc. O
NIPS O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
SPEECH, O
2017. O
5 O

- B-DAT
gios, O
and O
I. O
Kokkinos. O
Deforming O

- B-DAT
vised O
disentangling O
of O
shape O
and O

- B-DAT
ropean O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
Shlizerman. O
Synthesizing O
Obama: O
learning O
lip O

- B-DAT
dio. O
ACM O
Transactions O
on O
Graphics O

- B-DAT
tral O
normalization O
for O
generative O
adversarial O

- B-DAT

- B-DAT
erator O
architecture O
for O
generative O
adversarial O

- B-DAT

- B-DAT
actment O
of O
RGB O
videos. O
In O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
2018. O
4, O
6 O

- B-DAT
learning. O
CoRR, O
abs/1806.03316, O
2018. O
2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ried O
out O
on O
a O
single O

- B-DAT

- B-DAT

- B-DAT
surement O
was O
averaged O
over O
100 O

- B-DAT

- B-DAT

- B-DAT
ing O
personalization O
fidelity O
and O
realism O

- B-DAT

- B-DAT

- B-DAT
duction O
of O
a O
training O
scheduler O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vided O
by O
the O
embedder O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
specific O
initialization O
of O
the O
discriminator O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ration, O
which O
turned O
out O
to O

- B-DAT

- B-DAT

- B-DAT
tice O
that O
the O
results O
for O

- B-DAT
sonalization O
fidelity. O
We, O
therefore, O
came O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

shot B-DAT
learning I-DAT
of O
neural O
talking O
head O
models O

handful O
of O
photographs O
(so-called O
few- O
shot B-DAT
learning) I-DAT
and O
with O
limited O
training O
time O

shot B-DAT
learning), I-DAT
while O
adding O
a O
few O
more O

shot B-DAT
learning I-DAT
ability O
is O
obtained O
through O
exten O

shot B-DAT
learning I-DAT
tasks O
and O
learns O
to O
trans O

shot B-DAT
learning I-DAT
of O
generative O
models) O
and O
some O

shot B-DAT
learning I-DAT
by O
fine-tuning O

shot B-DAT
learning I-DAT
settings. O
Please O
re- O
fer O
to O

shot B-DAT
learning I-DAT
sets O
of O
size O
T O
for O

shot B-DAT
learning, I-DAT
the O
evaluation O
is O
performed O
on O

frames O
T O
used O
in O
few- O
shot B-DAT
learning I-DAT

shot B-DAT
learning I-DAT
on O
a O
video O
of O
a O

shot B-DAT
learning I-DAT
timings. O
Both O
are O
provided O
in O

shot B-DAT
learning I-DAT
speed O
versus O
the O
results O
quality O

shot B-DAT
learning I-DAT
(e.g. O
one-shot), O
while O
the O
FT O

shot B-DAT
learning I-DAT
of O
new O
avatars, O
fine-tuning O
ultimately O

shot B-DAT
learning I-DAT

shot B-DAT
learning I-DAT
was O
done O
via O
fine-tuning O
for O

shot B-DAT
learning I-DAT

shot B-DAT
learning I-DAT
and O
inference O
timings O
for O
the O

shot B-DAT
learning I-DAT
problems, O
like O
in O
our O
final O

or O
pretraining. O
We O
used O
eight O
shot B-DAT
learning I-DAT
problem O
formulation. O
The O
notation O
for O

or O
pretraining. O
We O
used O
eight O
shot B-DAT
learning I-DAT
problem O
formulation. O
The O
notation O
for O

VoxCeleb2 B-DAT

ablation O
studies, O
while O
by O
using O
VoxCeleb2 B-DAT

our O
method O
on O
a O
larger O
VoxCeleb2 B-DAT

from O
test O
videos O
of O
the O
VoxCeleb2 B-DAT

our O
best O
models O
on O
the O
VoxCeleb2 B-DAT

poses O
were O
taken O
from O
the O
VoxCeleb2 B-DAT

was O
trained O
only O
for O
the O
VoxCeleb2 B-DAT

extended O
qualitative O
comparisons O
on O
the O
VoxCeleb2 B-DAT

extended O
qualitative O
comparison O
on O
the O
VoxCeleb2 B-DAT

learning B-DAT
on O
a O
large O
dataset O
of O

to O
frame O
few- O
and O
one-shot O
learning B-DAT
of O
neural O
talking O
head O
models O

fields O
synthesized O
using O
ma- O
chine O
learning B-DAT
(including O
deep O
learning) O
[11, O
29 O

of O
photographs O
(so-called O
few- O
shot O
learning) B-DAT
and O
with O
limited O
training O
time O

on O
a O
single O
photograph O
(one-shot O
learning), B-DAT
while O
adding O
a O
few O
more O

The O
few-shot O
learning B-DAT
ability O
is O
obtained O
through O
exten O

learning) B-DAT
on O
a O
large O
corpus O
of O

learning, B-DAT
our O
sys- O
tem O
simulates O
few-shot O

learning B-DAT
tasks O
and O
learns O
to O
trans O

sets O
up O
a O
new O
adversarial O
learning B-DAT
problem O
with O
high-capacity O
generator O
and O

learning B-DAT

and, O
more O
recently, O
with O
deep O
learning B-DAT
[22, O
25] O
(to O
name O
just O

learning B-DAT
stage O
uses O
the O
adaptive O
instance O

learning B-DAT
to O
obtain O
the O
initial O
state O

learning B-DAT

learning B-DAT
[41] O
use O
adversarially- O
trained O
networks O

learning B-DAT
stage. O
While O
these O
methods O
are O

adversarial O
fine-tuning O
into O
the O
meta- O
learning B-DAT
framework. O
The O
former O
is O
applied O

learning B-DAT
stage O

4, O
18]. O
Their O
setting O
(few-shot O
learning B-DAT
of O
generative O
models) O
and O
some O

domain, O
the O
use O
of O
adversarial O
learning, B-DAT
its O
specific O
adaptation O
to O
the O

learning B-DAT
process O
and O
numerous O
im- O
plementation O

learning B-DAT
architecture O
involves O
the O
embedder O
network O

learning, B-DAT
we O
pass O
sets O
of O
frames O

learning B-DAT
stage O
of O
our O
approach O
assumes O

its O
t-th O
frame. O
During O
the O
learning B-DAT
process, O
as O
well O
as O
during O

learning B-DAT
stage O
of O
our O
approach, O
the O

learning B-DAT
stage. O
In O
general, O
during O
meta-learning O

learning, B-DAT
only O
ψ O
are O
trained O
directly O

learning B-DAT
stage O

learning B-DAT
stage O
of O
our O
approach, O
the O

3.3. O
Few-shot O
learning B-DAT
by O
fine-tuning O

learning B-DAT
has O
converged, O
our O
system O
can O

learning B-DAT
stage. O
As O
before, O
the O
synthe O

learning B-DAT
stage O

learning B-DAT
stage. O
A O
straightforward O
way O
to O

learning B-DAT
with O
a O
single O
video O
sequence O

learning B-DAT
stage O
to O
initialize O
ψ′, O
i.e O

learning B-DAT
stage. O
The O
initialization O
of O
w O

learning B-DAT
stage O

learning B-DAT
stage. O
For O
the O
intiailization, O
we O

learning B-DAT
dataset). O
However, O
the O
match O
term O

learning B-DAT
process O
ensures O
the O
similarity O
between O

Once O
the O
new O
learning B-DAT
problem O
is O
set O
up, O
the O

follow O
directly O
from O
the O
meta- O
learning B-DAT
variants. O
Thus, O
the O
generator O
parameters O

learning B-DAT
stage O
is O
also O
crucial. O
As O

Adam O
[21]. O
We O
set O
the O
learning B-DAT
rate O
of O
the O
embedder O
and O

different O
datasets O
with O
multiple O
few-shot O
learning B-DAT
settings. O
Please O
re- O
fer O
to O

fine-tune O
all O
models O
on O
few-shot O
learning B-DAT
sets O
of O
size O
T O
for O

learning B-DAT
(or O
pretraining) O
stage. O
After O
the O

few-shot O
learning, B-DAT
the O
evaluation O
is O
performed O
on O

T O
used O
in O
few- O
shot O
learning B-DAT

we O
perform O
one- O
and O
few-shot O
learning B-DAT
on O
a O
video O
of O
a O

learning B-DAT
or O
pretraining. O
We O
set O
the O

the O
comparison O
of O
the O
few-shot O
learning B-DAT
timings. O
Both O
are O
provided O
in O

allow O
to O
trade O
off O
few-shot O
learning B-DAT
speed O
versus O
the O
results O
quality O

per- O
forms O
better O
for O
low-shot O
learning B-DAT
(e.g. O
one-shot), O
while O
the O
FT O

variant O
allows O
fast O
(real-time) O
few-shot O
learning B-DAT
of O
new O
avatars, O
fine-tuning O
ultimately O

learning B-DAT
of O
ad O

and O
S. O
Levine. O
Model-agnostic O
meta- O
learning B-DAT
for O
fast O
adaptation O
of O
deep O

Y. O
Wu, O
et O
al. O
Transfer O
learning B-DAT
from O
speaker O
verification O
to O
multispeaker O

I. O
Kemelmacher- O
Shlizerman. O
Synthesizing O
Obama: O
learning B-DAT
lip O
sync O
from O
au- O
dio O

and O
Y. O
Wang. O
Adversarial O
meta- O
learning B-DAT

An O
adversarial O
approach O
to O
few-shot O
learning B-DAT

Pix2pixHD O
and O
our O
method, O
few-shot O
learning B-DAT
was O
done O
via O
fine-tuning O
for O

Method O
(T) O
Time, O
s O
Few-shot O
learning B-DAT

2: O
Quantitative O
comparison O
of O
few-shot O
learning B-DAT
and O
inference O
timings O
for O
the O

learning B-DAT

multiple O
training O
frames O
in O
few-shot O
learning B-DAT
problems, O
like O
in O
our O
final O

learning B-DAT
configu- O
ration, O
which O
turned O
out O

learning, B-DAT
we O
randomly O
initialize O
the O
person-specific O

learning B-DAT
objective O
and O
initialize O
the O
embedding O

learning B-DAT
or O
pretraining. O
We O
used O
eight O

shot O
learning B-DAT
problem O
formulation. O
The O
notation O
for O

learning B-DAT
or O
pretraining. O
We O
used O
eight O

shot O
learning B-DAT
problem O
formulation. O
The O
notation O
for O

1 O
8 B-DAT

1 O
8 B-DAT

already O
provides O
a O
high-realism O
solution. O
8 B-DAT

1 O
8 B-DAT

1 O
8 B-DAT

1 O
8 B-DAT

1 O
8 B-DAT

1 O
8 B-DAT

1 O
8 B-DAT

videos O
at O
1 O
fps) O
and O
VoxCeleb2 B-DAT
[8] O
(224p O
videos O
at O
25 O

VoxCeleb2 B-DAT
Ours-FF O
(1) O
46.1 O
0.61 O
0.42 O

ablation O
studies, O
while O
by O
using O
VoxCeleb2 B-DAT
we O
show O
the O
full O
potential O

our O
method O
on O
a O
larger O
VoxCeleb2 B-DAT
dataset. O
Here, O
we O
train O
two O

from O
test O
videos O
of O
the O
VoxCeleb2 B-DAT
dataset. O
We O
rank O
these O
videos O

our O
best O
models O
on O
the O
VoxCeleb2 B-DAT
dataset. O
The O
number O
of O
training O

poses O
were O
taken O
from O
the O
VoxCeleb2 B-DAT
dataset. O
Digital O
zoom O
recommended O

was O
trained O
only O
for O
the O
VoxCeleb2 B-DAT
dataset. O
The O
comparison O
was O
car O

extended O
qualitative O
comparisons O
on O
the O
VoxCeleb2 B-DAT
dataset. O
Here, O
the O
comparison O
is O

extended O
qualitative O
comparison O
on O
the O
VoxCeleb2 B-DAT
dataset. O
Here, O
we O
compare O
qualitative O

- B-DAT

- B-DAT

- B-DAT
tional O
neural O
networks O
to O
generate O

- B-DAT
ate O
a O
personalized O
talking O
head O

- B-DAT

- B-DAT

- B-DAT
ter O
that O
is O
able O
to O

- B-DAT
and O
one-shot O
learning O
of O
neural O

- B-DAT
sarial O
training O
problems O
with O
high O

- B-DAT

- B-DAT
alized O
photorealistic O
talking O
head O
models O

- B-DAT

- B-DAT
sions O
and O
mimics O
of O
a O

- B-DAT
ically, O
we O
consider O
the O
problem O

- B-DAT
alistic O
personalized O
head O
images O
given O

- B-DAT
marks, O
which O
drive O
the O
animation O

- B-DAT
conferencing O
and O
multi-player O
games, O
as O

- B-DAT
fects O
industry. O
Synthesizing O
realistic O
talking O

- B-DAT
ity. O
This O
complexity O
stems O
not O

- B-DAT
man O
visual O
system O
towards O
even O

- B-DAT
pearance O
modeling O
of O
human O
heads O

- B-DAT

- B-DAT
takes O
explains O
the O
current O
prevalence O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferencing O
systems O

- B-DAT
posed O
to O
synthesize O
articulated O
head O

- B-DAT
chine O
learning O
(including O
deep O
learning O

- B-DAT

- B-DAT
age, O
the O
amount O
of O
motion O

- B-DAT

- B-DAT

- B-DAT
vNets) O
presents O
the O
new O
hope O

- B-DAT
ever, O
to O
succeed, O
such O
methods O

- B-DAT
lions O
of O
parameters O
for O
each O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
phisticated O
physical O
and O
optical O
modeling O

- B-DAT
cessive O
for O
most O
practical O
telepresence O

- B-DAT
els O
with O
as O
little O
effort O

- B-DAT

- B-DAT
shot O
learning) O
and O
with O
limited O

- B-DAT

- B-DAT
larly O
to O
[16, O
20, O
37 O

- B-DAT
yond O
the O
abilities O
of O
warping-based O

- B-DAT

- B-DAT
sive O
pre-training O
(meta-learning) O
on O
a O

- B-DAT
ing O
head O
videos O
corresponding O
to O

- B-DAT
verse O
appearance. O
In O
the O
course O

- B-DAT

- B-DAT
tem O
simulates O
few-shot O
learning O
tasks O

- B-DAT
form O
landmark O
positions O
into O
realistically-looking O

- B-DAT
alized O
photographs, O
given O
a O
small O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ficient O
realism O
and O
personalization O
fidelity O

- B-DAT
ing O
head O
models, O
including O
video O

- B-DAT
ing O
of O
the O
appearance O
of O

- B-DAT

- B-DAT

- B-DAT
tension O
of O
the O
face O
modeling O

- B-DAT
ability O
and O
higher O
complexity O
than O

- B-DAT
ple, O
the O
results O
of O
face O

- B-DAT
fledged O
talking O
head O
system O

- B-DAT
tecture O
uses O
adversarial O
training O
[12 O

- B-DAT
ing O
projection O
discriminators O
[32]. O
Our O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sifier, O
from O
which O
it O
can O

- B-DAT
fiers O
of O
unseen O
classes, O
given O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
GAN O
[43], O
adversarial O
meta-learning O
[41 O

- B-DAT
trained O
networks O
to O
generate O
additional O

- B-DAT

- B-DAT

- B-DAT
mance, O
our O
method O
deals O
with O

- B-DAT
ation O
models O
using O
similar O
adversarial O

- B-DAT
marize, O
we O
bring O
the O
adversarial O

- B-DAT

- B-DAT
learning O
framework. O
The O
former O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cation O
domain, O
the O
use O
of O

- B-DAT

- B-DAT
plementation O
details O

- B-DAT

- B-DAT
marks) O
to O
the O
embedding O
vectors O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
quence O
and O
with O
xi(t) O
its O

- B-DAT

- B-DAT
ity O
of O
the O
face O
landmarks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

these O
inputs O
into O
an O
N O
- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
yi(t) O
for O
the O
video O

- B-DAT
sized O
video O
frame O
x̂i(t). O
The O

- B-DAT
imize O
the O
similarity O
between O
its O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trix O
P: O
ψ̂i O
= O
Pêi O

landmark O
image O
into O
an O
N O
- B-DAT

- B-DAT
criminator O
predicts O
a O
single O
scalar O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rameters O
of O
all O
three O
networks O

- B-DAT

- B-DAT
ing O
(K O
= O
8 O
in O

- B-DAT
domly O
draw O
a O
training O
video O

- B-DAT
ditional O
K O
frames O
s1, O
s2 O

- B-DAT

- B-DAT
ding O
by O
simply O
averaging O
the O

- B-DAT

- B-DAT
tion O
x̂i(t) O
using O
the O
perceptual O

- B-DAT
responding O
to O
VGG19 O
[30] O
network O

- B-DAT
sentially O
is O
a O
perceptual O
similarity O

- B-DAT
respond O
to O
individual O
videos. O
The O

maps O
its O
inputs O
to O
anN O
- B-DAT

- B-DAT

- B-DAT
criminator. O
The O
match O
term O
LMCH(φ,W O

- B-DAT

- B-DAT
ters O
θ,W,w0, O
b O
of O
the O

- B-DAT
courages O
the O
increase O
of O
the O

- B-DAT
ample O
x̂i(t) O
and O
the O
real O

- B-DAT
nating O
updates O
of O
the O
embedder O

- B-DAT
imize O
the O
losses O
LCNT,LADV O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sis O
is O
conditioned O
on O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
timate O
the O
embedding O
for O
the O

- B-DAT

- B-DAT
sponding O
to O
new O
landmark O
images O

- B-DAT
erator O
using O
the O
estimated O
embedding O

- B-DAT
learned O
parameters O
ψ, O
as O
well O

- B-DAT
able O
identity O
gap O
that O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sult O
of O
the O
meta-learning O
stage O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tors O
computed O
by O
the O
embedder O

- B-DAT
tions O
of O
the O
fine-tuning O
stage O

- B-DAT
learning O
variants. O
Thus, O
the O
generator O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
son O
et. O
al. O
[19], O
but O

- B-DAT
malization O
[15] O
replaced O
by O
instance O

- B-DAT

- B-DAT
efficients O
of O
instance O
normalization O
layers O

- B-DAT

- B-DAT
ization O
layers O
in O
the O
downsampling O

- B-DAT
mark O
images O
yi(t O

- B-DAT
tional O
part O
of O
the O
discriminator O

- B-DAT
out O
normalization O
layers). O
The O
discriminator O

- B-DAT
pared O
to O
the O
embedder, O
has O

- B-DAT

- B-DAT
serted O
at O
32×32 O
spatial O
resolution O

- B-DAT
tween O
activations O
of O
Conv1,6,11,20,29 O
VGG19 O

- B-DAT
nally, O
for O
LMCH O
we O
set O

- B-DAT
tional O
layers O
to O
64 O
and O

- B-DAT
tor O
has O
38 O
million O
parameters O

- B-DAT
titative O
and O
qualitative O
evaluation: O
VoxCeleb1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fer O
to O
the O
text O
for O

- B-DAT

- B-DAT

- B-DAT
son O
not O
seen O
during O
meta-learning O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
reenactment O
scenario). O
For O
the O
evaluation O

- B-DAT
out O
frames O
for O
each O
of O

- B-DAT

- B-DAT

- B-DAT
realism O
and O
identity O
preservation O
of O

- B-DAT

- B-DAT

- B-DAT
bedding O
vectors O
of O
the O
state-of-the-art O

- B-DAT
work O
[9] O
for O
measuring O
identity O

- B-DAT
tual O
similarity O
and O
realism O
of O

- B-DAT
man O
respondents. O
We O
show O
people O

- B-DAT

- B-DAT
not O
spot O
fakes O
based O
on O

- B-DAT

- B-DAT

- B-DAT
Celeb1 O
dataset). O
For O
Pix2pixHD, O
we O

- B-DAT

- B-DAT
shot O
learning. O
X2Face, O
as O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
which O
arguably O
gives O
X2Face O

- B-DAT
lines O
in O
three O
different O
setups O

- B-DAT

- B-DAT

- B-DAT
dom O
from O
the O
other O
video O

- B-DAT

- B-DAT

- B-DAT
perform O
our O
method O
on O
the O

- B-DAT
imizes O
only O
perceptual O
metric, O
without O

- B-DAT
and O
few-shot O
learning O
on O
a O

- B-DAT

- B-DAT
ter O
correlates O
with O
visual O
quality O

- B-DAT
ble O
1-Top O
with O
the O
results O

- B-DAT
alism O
and O
personalization O
degree O
achieved O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
out O
fine-tuning O
(by O
simply O
predicting O

- B-DAT
ant O
is O
trained O
for O
half O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults, O
where O
animation O
is O
driven O

- B-DAT
ent O
video O
of O
the O
same O

- B-DAT
tary O
material O
and O
in O
Figure O

- B-DAT
ble O
1-Bottom) O
and O
the O
visual O

- B-DAT
forms O
better O
for O
low-shot O
learning O

- B-DAT

- B-DAT
ial O
fine-tuning O

- B-DAT

- B-DAT
sons O
with O
similar O
geometry O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tographs O
in O
the O
source O
column O

- B-DAT

- B-DAT
realistic O
virtual O
talking O
heads O
in O

- B-DAT
ization O
score O
in O
our O
user O

- B-DAT
ics O
representation O
(in O
particular, O
the O

- B-DAT
son O
leads O
to O
a O
noticeable O

- B-DAT
ing O
a O
different O
person O
and O

- B-DAT
proach O
already O
provides O
a O
high-realism O

- B-DAT
puter O
Graphics O
and O
Applications, O
30(4):20–31 O

- B-DAT
sarial O
networks. O
In O
Artificial O
Neural O

Networks O
and O
Machine O
Learning O
- B-DAT
ICANN, O
pages O
594–603, O
2018. O
2 O

- B-DAT

- B-DAT

- B-DAT
thesis O
of O
3d O
faces. O
In O

- B-DAT
29, O
2017, O
pages O
1021–1030, O
2017 O

- B-DAT

- B-DAT
learning O
for O
fast O
adaptation O
of O

- B-DAT
ulation. O
In O
European O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
nets. O
In O
Advances O

- B-DAT

- B-DAT
wanathan, O
and O
R. O
Garnett, O
editors O

- B-DAT
formation O
Processing O
Systems O
30, O
pages O

- B-DAT
time O
with O
adaptive O
instance O
normalization O

- B-DAT
ternational O
Conference O
on O
Machine O
Learning O

- B-DAT

- B-DAT

- B-DAT
shick, O
S. O
Guadarrama, O
and O
T O

- B-DAT
tional O
architecture O
for O
fast O
feature O

- B-DAT

- B-DAT
speech O
synthesis. O
In O
Proc. O
NIPS O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
SPEECH, O
2017. O
5 O

- B-DAT
gios, O
and O
I. O
Kokkinos. O
Deforming O

- B-DAT
vised O
disentangling O
of O
shape O
and O

- B-DAT
ropean O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
Shlizerman. O
Synthesizing O
Obama: O
learning O
lip O

- B-DAT
dio. O
ACM O
Transactions O
on O
Graphics O

- B-DAT
tral O
normalization O
for O
generative O
adversarial O

- B-DAT

- B-DAT
erator O
architecture O
for O
generative O
adversarial O

- B-DAT

- B-DAT
actment O
of O
RGB O
videos. O
In O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
2018. O
4, O
6 O

- B-DAT
learning. O
CoRR, O
abs/1806.03316, O
2018. O
2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ried O
out O
on O
a O
single O

- B-DAT

- B-DAT

- B-DAT
surement O
was O
averaged O
over O
100 O

- B-DAT

- B-DAT

- B-DAT
ing O
personalization O
fidelity O
and O
realism O

- B-DAT

- B-DAT

- B-DAT
duction O
of O
a O
training O
scheduler O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vided O
by O
the O
embedder O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
specific O
initialization O
of O
the O
discriminator O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ration, O
which O
turned O
out O
to O

- B-DAT

- B-DAT

- B-DAT
tice O
that O
the O
results O
for O

- B-DAT
sonalization O
fidelity. O
We, O
therefore, O
came O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

shot B-DAT
learning I-DAT
of O
neural O
talking O
head O
models O

handful O
of O
photographs O
(so-called O
few- O
shot B-DAT
learning) I-DAT
and O
with O
limited O
training O
time O

shot B-DAT
learning), I-DAT
while O
adding O
a O
few O
more O

shot B-DAT
learning I-DAT
ability O
is O
obtained O
through O
exten O

shot B-DAT
learning I-DAT
tasks O
and O
learns O
to O
trans O

shot B-DAT
learning I-DAT
of O
generative O
models) O
and O
some O

shot B-DAT
learning I-DAT
by O
fine-tuning O

shot B-DAT
learning I-DAT
settings. O
Please O
re- O
fer O
to O

shot B-DAT
learning I-DAT
sets O
of O
size O
T O
for O

shot B-DAT
learning, I-DAT
the O
evaluation O
is O
performed O
on O

frames O
T O
used O
in O
few- O
shot B-DAT
learning I-DAT

shot B-DAT
learning I-DAT
on O
a O
video O
of O
a O

shot B-DAT
learning I-DAT
timings. O
Both O
are O
provided O
in O

shot B-DAT
learning I-DAT
speed O
versus O
the O
results O
quality O

shot B-DAT
learning I-DAT
(e.g. O
one-shot), O
while O
the O
FT O

shot B-DAT
learning I-DAT
of O
new O
avatars, O
fine-tuning O
ultimately O

shot B-DAT
learning I-DAT

shot B-DAT
learning I-DAT
was O
done O
via O
fine-tuning O
for O

shot B-DAT
learning I-DAT

shot B-DAT
learning I-DAT
and O
inference O
timings O
for O
the O

shot B-DAT
learning I-DAT
problems, O
like O
in O
our O
final O

or O
pretraining. O
We O
used O
eight O
shot B-DAT
learning I-DAT
problem O
formulation. O
The O
notation O
for O

or O
pretraining. O
We O
used O
eight O
shot B-DAT
learning I-DAT
problem O
formulation. O
The O
notation O
for O

T) O
FID↓ O
SSIM↑ O
CSIM↑ O
USER↓ O
VoxCeleb1 B-DAT
X2Face I-DAT
(1) O
45.8 O
0.68 O
0.16 O

than O
the O
former. O
VoxCeleb1 B-DAT

Methods. O
On O
the O
VoxCeleb1 B-DAT

Figure O
3: O
Comparison O
on O
the O
VoxCeleb1 B-DAT

extended O
qualitative O
comparisons O
on O
the O
VoxCeleb1 B-DAT

extended O
qualitative O
comparison O
on O
the O
VoxCeleb1 B-DAT

people O
and O
even O
portrait O
paintings. O
1 B-DAT

motion, O
head O
rotation, O
and O
disocclusion O
1 B-DAT

1 O
· B-DAT
10−2 I-DAT
for O
VGG19 O
and O

1 O
· B-DAT
101. I-DAT
Fi- O
nally, O
for O

NeurIPS, O
pages O
2371–2380, O
2018. O
2 O
10 B-DAT

the O
rest O
of O
the O
figures. O
11 B-DAT

32 O
1 B-DAT

results O
from O
our O
final O
model. O
12 B-DAT

sequence O
of O
a O
different O
person. O
13 B-DAT

sequence O
with O
the O
same O
person. O
14 B-DAT

sequence O
of O
the O
same O
person. O
15 B-DAT

32 O
1 B-DAT

3 O
in O
the O
main O
paper. O
16 B-DAT

3 O
in O
the O
main O
paper. O
17 B-DAT

32 O
1 B-DAT

4 O
in O
the O
main O
paper. O
18 B-DAT

4 O
in O
the O
main O
paper. O
19 B-DAT

quan- O
titative O
and O
qualitative O
evaluation: O
VoxCeleb1 B-DAT
[26] O
(256p O
videos O
at O
1 O

T) O
FID↓ O
SSIM↑ O
CSIM↑ O
USER↓ O
VoxCeleb1 B-DAT

than O
the O
former. O
VoxCeleb1 B-DAT
is O
used O
for O
comparison O
with O

Methods. O
On O
the O
VoxCeleb1 B-DAT
dataset O
we O
compare O
our O
model O

Figure O
3: O
Comparison O
on O
the O
VoxCeleb1 B-DAT
dataset. O
For O
each O
of O
the O

to O
smaller-scale O
models O
trained O
on O
VoxCeleb1 B-DAT

extended O
qualitative O
comparisons O
on O
the O
VoxCeleb1 B-DAT
dataset. O
Here, O
the O
comparison O
is O

extended O
qualitative O
comparison O
on O
the O
VoxCeleb1 B-DAT
dataset. O
Here, O
we O
compare O
qualitative O

shot B-DAT
learning I-DAT
of O
neural O
talking O
head O
models O

handful O
of O
photographs O
(so-called O
few- O
shot B-DAT
learning) I-DAT
and O
with O
limited O
training O
time O

shot B-DAT
learning), I-DAT
while O
adding O
a O
few O
more O

shot B-DAT
learning I-DAT
ability O
is O
obtained O
through O
exten O

shot B-DAT
learning I-DAT
tasks O
and O
learns O
to O
trans O

shot B-DAT
learning I-DAT
of O
generative O
models) O
and O
some O

shot B-DAT
learning I-DAT
by O
fine-tuning O

shot B-DAT
learning I-DAT
settings. O
Please O
re- O
fer O
to O

shot B-DAT
learning I-DAT
sets O
of O
size O
T O
for O

shot B-DAT
learning, I-DAT
the O
evaluation O
is O
performed O
on O

frames O
T O
used O
in O
few- O
shot B-DAT
learning I-DAT

shot B-DAT
learning I-DAT
on O
a O
video O
of O
a O

shot B-DAT
learning I-DAT
timings. O
Both O
are O
provided O
in O

shot B-DAT
learning I-DAT
speed O
versus O
the O
results O
quality O

shot B-DAT
learning I-DAT
(e.g. O
one-shot), O
while O
the O
FT O

shot B-DAT
learning I-DAT
of O
new O
avatars, O
fine-tuning O
ultimately O

shot B-DAT
learning I-DAT

shot B-DAT
learning I-DAT
was O
done O
via O
fine-tuning O
for O

shot B-DAT
learning I-DAT

shot B-DAT
learning I-DAT
and O
inference O
timings O
for O
the O

shot B-DAT
learning I-DAT
problems, O
like O
in O
our O
final O

or O
pretraining. O
We O
used O
eight O
shot B-DAT
learning I-DAT
problem O
formulation. O
The O
notation O
for O

or O
pretraining. O
We O
used O
eight O
shot B-DAT
learning I-DAT
problem O
formulation. O
The O
notation O
for O

learning B-DAT
on O
a O
large O
dataset O
of O

to O
frame O
few- O
and O
one-shot O
learning B-DAT
of O
neural O
talking O
head O
models O

fields O
synthesized O
using O
ma- O
chine O
learning B-DAT
(including O
deep O
learning) O
[11, O
29 O

of O
photographs O
(so-called O
few- O
shot O
learning) B-DAT
and O
with O
limited O
training O
time O

on O
a O
single O
photograph O
(one-shot O
learning), B-DAT
while O
adding O
a O
few O
more O

The O
few-shot O
learning B-DAT
ability O
is O
obtained O
through O
exten O

learning) B-DAT
on O
a O
large O
corpus O
of O

learning, B-DAT
our O
sys- O
tem O
simulates O
few-shot O

learning B-DAT
tasks O
and O
learns O
to O
trans O

sets O
up O
a O
new O
adversarial O
learning B-DAT
problem O
with O
high-capacity O
generator O
and O

learning B-DAT

and, O
more O
recently, O
with O
deep O
learning B-DAT
[22, O
25] O
(to O
name O
just O

learning B-DAT
stage O
uses O
the O
adaptive O
instance O

learning B-DAT
to O
obtain O
the O
initial O
state O

learning B-DAT

learning B-DAT
[41] O
use O
adversarially- O
trained O
networks O

learning B-DAT
stage. O
While O
these O
methods O
are O

adversarial O
fine-tuning O
into O
the O
meta- O
learning B-DAT
framework. O
The O
former O
is O
applied O

learning B-DAT
stage O

4, O
18]. O
Their O
setting O
(few-shot O
learning B-DAT
of O
generative O
models) O
and O
some O

domain, O
the O
use O
of O
adversarial O
learning, B-DAT
its O
specific O
adaptation O
to O
the O

learning B-DAT
process O
and O
numerous O
im- O
plementation O

learning B-DAT
architecture O
involves O
the O
embedder O
network O

learning, B-DAT
we O
pass O
sets O
of O
frames O

learning B-DAT
stage O
of O
our O
approach O
assumes O

its O
t-th O
frame. O
During O
the O
learning B-DAT
process, O
as O
well O
as O
during O

learning B-DAT
stage O
of O
our O
approach, O
the O

learning B-DAT
stage. O
In O
general, O
during O
meta-learning O

learning, B-DAT
only O
ψ O
are O
trained O
directly O

learning B-DAT
stage O

learning B-DAT
stage O
of O
our O
approach, O
the O

3.3. O
Few-shot O
learning B-DAT
by O
fine-tuning O

learning B-DAT
has O
converged, O
our O
system O
can O

learning B-DAT
stage. O
As O
before, O
the O
synthe O

learning B-DAT
stage O

learning B-DAT
stage. O
A O
straightforward O
way O
to O

learning B-DAT
with O
a O
single O
video O
sequence O

learning B-DAT
stage O
to O
initialize O
ψ′, O
i.e O

learning B-DAT
stage. O
The O
initialization O
of O
w O

learning B-DAT
stage O

learning B-DAT
stage. O
For O
the O
intiailization, O
we O

learning B-DAT
dataset). O
However, O
the O
match O
term O

learning B-DAT
process O
ensures O
the O
similarity O
between O

Once O
the O
new O
learning B-DAT
problem O
is O
set O
up, O
the O

follow O
directly O
from O
the O
meta- O
learning B-DAT
variants. O
Thus, O
the O
generator O
parameters O

learning B-DAT
stage O
is O
also O
crucial. O
As O

Adam O
[21]. O
We O
set O
the O
learning B-DAT
rate O
of O
the O
embedder O
and O

different O
datasets O
with O
multiple O
few-shot O
learning B-DAT
settings. O
Please O
re- O
fer O
to O

fine-tune O
all O
models O
on O
few-shot O
learning B-DAT
sets O
of O
size O
T O
for O

learning B-DAT
(or O
pretraining) O
stage. O
After O
the O

few-shot O
learning, B-DAT
the O
evaluation O
is O
performed O
on O

T O
used O
in O
few- O
shot O
learning B-DAT

we O
perform O
one- O
and O
few-shot O
learning B-DAT
on O
a O
video O
of O
a O

learning B-DAT
or O
pretraining. O
We O
set O
the O

the O
comparison O
of O
the O
few-shot O
learning B-DAT
timings. O
Both O
are O
provided O
in O

allow O
to O
trade O
off O
few-shot O
learning B-DAT
speed O
versus O
the O
results O
quality O

per- O
forms O
better O
for O
low-shot O
learning B-DAT
(e.g. O
one-shot), O
while O
the O
FT O

variant O
allows O
fast O
(real-time) O
few-shot O
learning B-DAT
of O
new O
avatars, O
fine-tuning O
ultimately O

learning B-DAT
of O
ad O

and O
S. O
Levine. O
Model-agnostic O
meta- O
learning B-DAT
for O
fast O
adaptation O
of O
deep O

Y. O
Wu, O
et O
al. O
Transfer O
learning B-DAT
from O
speaker O
verification O
to O
multispeaker O

I. O
Kemelmacher- O
Shlizerman. O
Synthesizing O
Obama: O
learning B-DAT
lip O
sync O
from O
au- O
dio O

and O
Y. O
Wang. O
Adversarial O
meta- O
learning B-DAT

An O
adversarial O
approach O
to O
few-shot O
learning B-DAT

Pix2pixHD O
and O
our O
method, O
few-shot O
learning B-DAT
was O
done O
via O
fine-tuning O
for O

Method O
(T) O
Time, O
s O
Few-shot O
learning B-DAT

2: O
Quantitative O
comparison O
of O
few-shot O
learning B-DAT
and O
inference O
timings O
for O
the O

learning B-DAT

multiple O
training O
frames O
in O
few-shot O
learning B-DAT
problems, O
like O
in O
our O
final O

learning B-DAT
configu- O
ration, O
which O
turned O
out O

learning, B-DAT
we O
randomly O
initialize O
the O
person-specific O

learning B-DAT
objective O
and O
initialize O
the O
embedding O

learning B-DAT
or O
pretraining. O
We O
used O
eight O

shot O
learning B-DAT
problem O
formulation. O
The O
notation O
for O

learning B-DAT
or O
pretraining. O
We O
used O
eight O

shot O
learning B-DAT
problem O
formulation. O
The O
notation O
for O

- B-DAT

- B-DAT

- B-DAT
tional O
neural O
networks O
to O
generate O

- B-DAT
ate O
a O
personalized O
talking O
head O

- B-DAT

- B-DAT

- B-DAT
ter O
that O
is O
able O
to O

- B-DAT
and O
one-shot O
learning O
of O
neural O

- B-DAT
sarial O
training O
problems O
with O
high O

- B-DAT

- B-DAT
alized O
photorealistic O
talking O
head O
models O

- B-DAT

- B-DAT
sions O
and O
mimics O
of O
a O

- B-DAT
ically, O
we O
consider O
the O
problem O

- B-DAT
alistic O
personalized O
head O
images O
given O

- B-DAT
marks, O
which O
drive O
the O
animation O

- B-DAT
conferencing O
and O
multi-player O
games, O
as O

- B-DAT
fects O
industry. O
Synthesizing O
realistic O
talking O

- B-DAT
ity. O
This O
complexity O
stems O
not O

- B-DAT
man O
visual O
system O
towards O
even O

- B-DAT
pearance O
modeling O
of O
human O
heads O

- B-DAT

- B-DAT
takes O
explains O
the O
current O
prevalence O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferencing O
systems O

- B-DAT
posed O
to O
synthesize O
articulated O
head O

- B-DAT
chine O
learning O
(including O
deep O
learning O

- B-DAT

- B-DAT
age, O
the O
amount O
of O
motion O

- B-DAT

- B-DAT

- B-DAT
vNets) O
presents O
the O
new O
hope O

- B-DAT
ever, O
to O
succeed, O
such O
methods O

- B-DAT
lions O
of O
parameters O
for O
each O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
phisticated O
physical O
and O
optical O
modeling O

- B-DAT
cessive O
for O
most O
practical O
telepresence O

- B-DAT
els O
with O
as O
little O
effort O

- B-DAT

- B-DAT
shot O
learning) O
and O
with O
limited O

- B-DAT

- B-DAT
larly O
to O
[16, O
20, O
37 O

- B-DAT
yond O
the O
abilities O
of O
warping-based O

- B-DAT

- B-DAT
sive O
pre-training O
(meta-learning) O
on O
a O

- B-DAT
ing O
head O
videos O
corresponding O
to O

- B-DAT
verse O
appearance. O
In O
the O
course O

- B-DAT

- B-DAT
tem O
simulates O
few-shot O
learning O
tasks O

- B-DAT
form O
landmark O
positions O
into O
realistically-looking O

- B-DAT
alized O
photographs, O
given O
a O
small O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ficient O
realism O
and O
personalization O
fidelity O

- B-DAT
ing O
head O
models, O
including O
video O

- B-DAT
ing O
of O
the O
appearance O
of O

- B-DAT

- B-DAT

- B-DAT
tension O
of O
the O
face O
modeling O

- B-DAT
ability O
and O
higher O
complexity O
than O

- B-DAT
ple, O
the O
results O
of O
face O

- B-DAT
fledged O
talking O
head O
system O

- B-DAT
tecture O
uses O
adversarial O
training O
[12 O

- B-DAT
ing O
projection O
discriminators O
[32]. O
Our O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sifier, O
from O
which O
it O
can O

- B-DAT
fiers O
of O
unseen O
classes, O
given O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
GAN O
[43], O
adversarial O
meta-learning O
[41 O

- B-DAT
trained O
networks O
to O
generate O
additional O

- B-DAT

- B-DAT

- B-DAT
mance, O
our O
method O
deals O
with O

- B-DAT
ation O
models O
using O
similar O
adversarial O

- B-DAT
marize, O
we O
bring O
the O
adversarial O

- B-DAT

- B-DAT
learning O
framework. O
The O
former O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cation O
domain, O
the O
use O
of O

- B-DAT

- B-DAT
plementation O
details O

- B-DAT

- B-DAT
marks) O
to O
the O
embedding O
vectors O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
quence O
and O
with O
xi(t) O
its O

- B-DAT

- B-DAT
ity O
of O
the O
face O
landmarks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

these O
inputs O
into O
an O
N O
- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
yi(t) O
for O
the O
video O

- B-DAT
sized O
video O
frame O
x̂i(t). O
The O

- B-DAT
imize O
the O
similarity O
between O
its O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trix O
P: O
ψ̂i O
= O
Pêi O

landmark O
image O
into O
an O
N O
- B-DAT

- B-DAT
criminator O
predicts O
a O
single O
scalar O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rameters O
of O
all O
three O
networks O

- B-DAT

- B-DAT
ing O
(K O
= O
8 O
in O

- B-DAT
domly O
draw O
a O
training O
video O

- B-DAT
ditional O
K O
frames O
s1, O
s2 O

- B-DAT

- B-DAT
ding O
by O
simply O
averaging O
the O

- B-DAT

- B-DAT
tion O
x̂i(t) O
using O
the O
perceptual O

- B-DAT
responding O
to O
VGG19 O
[30] O
network O

- B-DAT
sentially O
is O
a O
perceptual O
similarity O

- B-DAT
respond O
to O
individual O
videos. O
The O

maps O
its O
inputs O
to O
anN O
- B-DAT

- B-DAT

- B-DAT
criminator. O
The O
match O
term O
LMCH(φ,W O

- B-DAT

- B-DAT
ters O
θ,W,w0, O
b O
of O
the O

- B-DAT
courages O
the O
increase O
of O
the O

- B-DAT
ample O
x̂i(t) O
and O
the O
real O

- B-DAT
nating O
updates O
of O
the O
embedder O

- B-DAT
imize O
the O
losses O
LCNT,LADV O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sis O
is O
conditioned O
on O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
timate O
the O
embedding O
for O
the O

- B-DAT

- B-DAT
sponding O
to O
new O
landmark O
images O

- B-DAT
erator O
using O
the O
estimated O
embedding O

- B-DAT
learned O
parameters O
ψ, O
as O
well O

- B-DAT
able O
identity O
gap O
that O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sult O
of O
the O
meta-learning O
stage O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tors O
computed O
by O
the O
embedder O

- B-DAT
tions O
of O
the O
fine-tuning O
stage O

- B-DAT
learning O
variants. O
Thus, O
the O
generator O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
son O
et. O
al. O
[19], O
but O

- B-DAT
malization O
[15] O
replaced O
by O
instance O

- B-DAT

- B-DAT
efficients O
of O
instance O
normalization O
layers O

- B-DAT

- B-DAT
ization O
layers O
in O
the O
downsampling O

- B-DAT
mark O
images O
yi(t O

- B-DAT
tional O
part O
of O
the O
discriminator O

- B-DAT
out O
normalization O
layers). O
The O
discriminator O

- B-DAT
pared O
to O
the O
embedder, O
has O

- B-DAT

- B-DAT
serted O
at O
32×32 O
spatial O
resolution O

- B-DAT
tween O
activations O
of O
Conv1,6,11,20,29 O
VGG19 O

- B-DAT
nally, O
for O
LMCH O
we O
set O

- B-DAT
tional O
layers O
to O
64 O
and O

- B-DAT
tor O
has O
38 O
million O
parameters O

- B-DAT
titative O
and O
qualitative O
evaluation: O
VoxCeleb1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fer O
to O
the O
text O
for O

- B-DAT

- B-DAT

- B-DAT
son O
not O
seen O
during O
meta-learning O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
reenactment O
scenario). O
For O
the O
evaluation O

- B-DAT
out O
frames O
for O
each O
of O

- B-DAT

- B-DAT

- B-DAT
realism O
and O
identity O
preservation O
of O

- B-DAT

- B-DAT

- B-DAT
bedding O
vectors O
of O
the O
state-of-the-art O

- B-DAT
work O
[9] O
for O
measuring O
identity O

- B-DAT
tual O
similarity O
and O
realism O
of O

- B-DAT
man O
respondents. O
We O
show O
people O

- B-DAT

- B-DAT
not O
spot O
fakes O
based O
on O

- B-DAT

- B-DAT

- B-DAT
Celeb1 O
dataset). O
For O
Pix2pixHD, O
we O

- B-DAT

- B-DAT
shot O
learning. O
X2Face, O
as O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
which O
arguably O
gives O
X2Face O

- B-DAT
lines O
in O
three O
different O
setups O

- B-DAT

- B-DAT

- B-DAT
dom O
from O
the O
other O
video O

- B-DAT

- B-DAT

- B-DAT
perform O
our O
method O
on O
the O

- B-DAT
imizes O
only O
perceptual O
metric, O
without O

- B-DAT
and O
few-shot O
learning O
on O
a O

- B-DAT

- B-DAT
ter O
correlates O
with O
visual O
quality O

- B-DAT
ble O
1-Top O
with O
the O
results O

- B-DAT
alism O
and O
personalization O
degree O
achieved O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
out O
fine-tuning O
(by O
simply O
predicting O

- B-DAT
ant O
is O
trained O
for O
half O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults, O
where O
animation O
is O
driven O

- B-DAT
ent O
video O
of O
the O
same O

- B-DAT
tary O
material O
and O
in O
Figure O

- B-DAT
ble O
1-Bottom) O
and O
the O
visual O

- B-DAT
forms O
better O
for O
low-shot O
learning O

- B-DAT

- B-DAT
ial O
fine-tuning O

- B-DAT

- B-DAT
sons O
with O
similar O
geometry O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tographs O
in O
the O
source O
column O

- B-DAT

- B-DAT
realistic O
virtual O
talking O
heads O
in O

- B-DAT
ization O
score O
in O
our O
user O

- B-DAT
ics O
representation O
(in O
particular, O
the O

- B-DAT
son O
leads O
to O
a O
noticeable O

- B-DAT
ing O
a O
different O
person O
and O

- B-DAT
proach O
already O
provides O
a O
high-realism O

- B-DAT
puter O
Graphics O
and O
Applications, O
30(4):20–31 O

- B-DAT
sarial O
networks. O
In O
Artificial O
Neural O

Networks O
and O
Machine O
Learning O
- B-DAT
ICANN, O
pages O
594–603, O
2018. O
2 O

- B-DAT

- B-DAT

- B-DAT
thesis O
of O
3d O
faces. O
In O

- B-DAT
29, O
2017, O
pages O
1021–1030, O
2017 O

- B-DAT

- B-DAT
learning O
for O
fast O
adaptation O
of O

- B-DAT
ulation. O
In O
European O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
nets. O
In O
Advances O

- B-DAT

- B-DAT
wanathan, O
and O
R. O
Garnett, O
editors O

- B-DAT
formation O
Processing O
Systems O
30, O
pages O

- B-DAT
time O
with O
adaptive O
instance O
normalization O

- B-DAT
ternational O
Conference O
on O
Machine O
Learning O

- B-DAT

- B-DAT

- B-DAT
shick, O
S. O
Guadarrama, O
and O
T O

- B-DAT
tional O
architecture O
for O
fast O
feature O

- B-DAT

- B-DAT
speech O
synthesis. O
In O
Proc. O
NIPS O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
SPEECH, O
2017. O
5 O

- B-DAT
gios, O
and O
I. O
Kokkinos. O
Deforming O

- B-DAT
vised O
disentangling O
of O
shape O
and O

- B-DAT
ropean O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
Shlizerman. O
Synthesizing O
Obama: O
learning O
lip O

- B-DAT
dio. O
ACM O
Transactions O
on O
Graphics O

- B-DAT
tral O
normalization O
for O
generative O
adversarial O

- B-DAT

- B-DAT
erator O
architecture O
for O
generative O
adversarial O

- B-DAT

- B-DAT
actment O
of O
RGB O
videos. O
In O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
2018. O
4, O
6 O

- B-DAT
learning. O
CoRR, O
abs/1806.03316, O
2018. O
2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ried O
out O
on O
a O
single O

- B-DAT

- B-DAT

- B-DAT
surement O
was O
averaged O
over O
100 O

- B-DAT

- B-DAT

- B-DAT
ing O
personalization O
fidelity O
and O
realism O

- B-DAT

- B-DAT

- B-DAT
duction O
of O
a O
training O
scheduler O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vided O
by O
the O
embedder O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
specific O
initialization O
of O
the O
discriminator O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ration, O
which O
turned O
out O
to O

- B-DAT

- B-DAT

- B-DAT
tice O
that O
the O
results O
for O

- B-DAT
sonalization O
fidelity. O
We, O
therefore, O
came O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

framework O
for O
video O
face O
editing. O
1 B-DAT
Introduction I-DAT

considering O
gdA O
or O
gdR O
: O
1 B-DAT

the O
function O
fa→v O
: O
R O
1 B-DAT

10, B-DAT
1755–1758 I-DAT
(2009 O

3D O
scans O
[4], O
or O
by O
learning B-DAT
3DMM O
parameters O
directly O
from O
RGB O

accordingly. O
This O
is O
done O
by O
learning B-DAT
a O
forward O
mapping O
fp→v O
from O

improving O
the O
generated O
results. O
As O
learning B-DAT
the O
function O
fa→v O
: O
R O

with O
L1 O
loss, O
and O
a O
learning B-DAT
rate O
of O
0.001. O
The O
learning O

phase O
is O
started O
with O
a O
learning B-DAT
rate O
of O
0.0001. O
Testing. O
The O

Abbeel, O
P.: O
Infogan: O
Interpretable O
representation O
learning B-DAT
by O
information O
maximizing O
generative O
adversarial O

Denton, O
E.L., O
Birodkar, O
V.: O
Unsupervised O
learning B-DAT
of O
disentangled O
repre- O
sentations O
from O

facial O
animation O
by O
joint O
end-to-end O
learning B-DAT
of O
pose O
and O
emotion. O
ACM O

King, O
D.E.: O
Dlib-ml: O
A O
machine O
learning B-DAT
toolkit. O
The O
Journal O
of O
Machine O

tion O
of O
unconstrained O
faces O
by O
learning B-DAT
efficient O
H-CNN O
regressors. O
In: O
Proc O

S.M., O
Kemelmacher-Shlizerman, O
I.: O
Synthesizing O
Obama: O
learning B-DAT
lip O
sync O
from O
audio. O
ACM O

X., O
Liu, O
X.: O
Disentangled O
representation O
learning B-DAT
gan O
for O
pose-invariant O
face O
recognition O

- B-DAT
phisticated O
video O
and O
image O
editing O

- B-DAT

- B-DAT
pared O
to O
state-of-the-art O
self-supervised/supervised O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
frame, O
audio O
data, O
or O

- B-DAT

- B-DAT
ing O
frames. O
These O
frames O
are O

- B-DAT

- B-DAT
tions. O
First, O
we O
propose O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
described O
in O
Section O
6 O

- B-DAT
imation O
(or O
puppeteering) O
given O
one O

- B-DAT
erature O
on O
supervised/self-supervised O
approaches; O
here O

- B-DAT
marks O
[5, O
12, O
21, O
30 O

- B-DAT

- B-DAT
tors O
of O
variation O
(e.g. O
optical O

- B-DAT

- B-DAT
form O
images O
of O
one O
domain O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work. O
This O
is O
illustrated O
in O

- B-DAT
termine O
how O
to O
map O
from O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
straints O
based O
on O
the O
identity O

- B-DAT
quently, O
we O
introduce O
additional O
loss O

- B-DAT

-5 B-DAT
and O
Conv7 O
layers O
(i.e. O
layers O

-7 B-DAT
layers O
(i.e. O
layers O
encoding O
higher O

- B-DAT

- B-DAT

- B-DAT
figuration O
A) O
[37] O
trained O
on O

- B-DAT

- B-DAT

- B-DAT
tional O
mapping O
fv→p O
is O
needed O

- B-DAT
nected O
layer O
with O
bias O
and O

- B-DAT
connected O
linear O
layer O
with O
bias O

- B-DAT

- B-DAT

- B-DAT
bedding O
learns O
to O
encode O
some O

- B-DAT
tion. O
Given O
driving O
audio O
features O

- B-DAT

- B-DAT
size O
of O
16. O
First, O
it O

- B-DAT

- B-DAT
ing/testing O
setups. O
Lower O
is O
better O

- B-DAT
centage O
improvement O
over O
the O
L1 O

- B-DAT
egy O
and O
using O
additional O
views O

- B-DAT
main O
(in O
this O
case O
a O

- B-DAT
cleGAN O
is O
trained O
on O
pairs O

- B-DAT
ing O
unrealistic O
results. O
Additionally, O
our O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
4.1), O
the O
25, O
993 O

- B-DAT
neutral O
expressions O
in O
the O
source O

- B-DAT
processing O
(X2Face O
+ O
p.-p.) O
can O

- B-DAT

- B-DAT

- B-DAT
mentary O
material. O
Whilst O
some O
artefacts O

- B-DAT

- B-DAT
eration O
using O
another O
face. O
This O

- B-DAT
constrained O
settings O
(e.g. O
an O
unseen O

- B-DAT

- B-DAT
tioned O
on O
other O
modalities, O
the O

- B-DAT
teresting O
avenue O
of O
research: O
how O

- B-DAT

- B-DAT
eration O
quality O
of O
these O
methods O

- B-DAT

- B-DAT
tions/comments. O
This O
work O
was O
funded O

- B-DAT

- B-DAT

-4 B-DAT

- B-DAT

- B-DAT
ment O
networks. O
In: O
Proc. O
ICCV O

- B-DAT

- B-DAT

- B-DAT
sentations O
from O
video. O
In: O
NIPS O

- B-DAT
tional O
neural O
networks. O
In: O
Proc O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
actions O
on O
Graphics O
(TOG) O
(2017 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lutional O
neural O
networks. O
In: O
Proc O

- B-DAT
tional O
inverse O
graphics O
network. O
In O

- B-DAT
tion O
of O
unconstrained O
faces O
by O

- B-DAT

- B-DAT

- B-DAT
tation, O
face O
swapping, O
and O
face O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
strained O
photo O
collections. O
In: O
Proc O

- B-DAT

- B-DAT

- B-DAT
scale O
image O
recognition. O
In: O
International O

- B-DAT
sentations O
(2015 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pretable O
transformations O
with O
encoder-decoder O
networks O

- B-DAT

- B-DAT

- B-DAT
lation O
using O
cycle-consistent O
adversarial O
networks O

VoxCeleb2 B-DAT

ablation O
studies, O
while O
by O
using O
VoxCeleb2 B-DAT

our O
method O
on O
a O
larger O
VoxCeleb2 B-DAT

from O
test O
videos O
of O
the O
VoxCeleb2 B-DAT

our O
best O
models O
on O
the O
VoxCeleb2 B-DAT

poses O
were O
taken O
from O
the O
VoxCeleb2 B-DAT

was O
trained O
only O
for O
the O
VoxCeleb2 B-DAT

extended O
qualitative O
comparisons O
on O
the O
VoxCeleb2 B-DAT

extended O
qualitative O
comparison O
on O
the O
VoxCeleb2 B-DAT

people O
and O
even O
portrait O
paintings. O
1 B-DAT

motion, O
head O
rotation, O
and O
disocclusion O
1 B-DAT

1 O
· B-DAT
10−2 I-DAT
for O
VGG19 O
and O

1 O
· B-DAT
101. I-DAT
Fi- O
nally, O
for O

NeurIPS, O
pages O
2371–2380, O
2018. O
2 O
10 B-DAT

the O
rest O
of O
the O
figures. O
11 B-DAT

32 O
1 B-DAT

results O
from O
our O
final O
model. O
12 B-DAT

sequence O
of O
a O
different O
person. O
13 B-DAT

sequence O
with O
the O
same O
person. O
14 B-DAT

sequence O
of O
the O
same O
person. O
15 B-DAT

32 O
1 B-DAT

3 O
in O
the O
main O
paper. O
16 B-DAT

3 O
in O
the O
main O
paper. O
17 B-DAT

32 O
1 B-DAT

4 O
in O
the O
main O
paper. O
18 B-DAT

4 O
in O
the O
main O
paper. O
19 B-DAT

shot B-DAT
learning I-DAT
of O
neural O
talking O
head O
models O

handful O
of O
photographs O
(so-called O
few- O
shot B-DAT
learning) I-DAT
and O
with O
limited O
training O
time O

shot B-DAT
learning), I-DAT
while O
adding O
a O
few O
more O

shot B-DAT
learning I-DAT
ability O
is O
obtained O
through O
exten O

shot B-DAT
learning I-DAT
tasks O
and O
learns O
to O
trans O

shot B-DAT
learning I-DAT
of O
generative O
models) O
and O
some O

shot B-DAT
learning I-DAT
by O
fine-tuning O

shot B-DAT
learning I-DAT
settings. O
Please O
re- O
fer O
to O

shot B-DAT
learning I-DAT
sets O
of O
size O
T O
for O

shot B-DAT
learning, I-DAT
the O
evaluation O
is O
performed O
on O

frames O
T O
used O
in O
few- O
shot B-DAT
learning I-DAT

shot B-DAT
learning I-DAT
on O
a O
video O
of O
a O

shot B-DAT
learning I-DAT
timings. O
Both O
are O
provided O
in O

shot B-DAT
learning I-DAT
speed O
versus O
the O
results O
quality O

shot B-DAT
learning I-DAT
(e.g. O
one-shot), O
while O
the O
FT O

shot B-DAT
learning I-DAT
of O
new O
avatars, O
fine-tuning O
ultimately O

shot B-DAT
learning I-DAT

shot B-DAT
learning I-DAT
was O
done O
via O
fine-tuning O
for O

shot B-DAT
learning I-DAT

shot B-DAT
learning I-DAT
and O
inference O
timings O
for O
the O

shot B-DAT
learning I-DAT
problems, O
like O
in O
our O
final O

or O
pretraining. O
We O
used O
eight O
shot B-DAT
learning I-DAT
problem O
formulation. O
The O
notation O
for O

or O
pretraining. O
We O
used O
eight O
shot B-DAT
learning I-DAT
problem O
formulation. O
The O
notation O
for O

learning B-DAT
on O
a O
large O
dataset O
of O

to O
frame O
few- O
and O
one-shot O
learning B-DAT
of O
neural O
talking O
head O
models O

fields O
synthesized O
using O
ma- O
chine O
learning B-DAT
(including O
deep O
learning) O
[11, O
29 O

of O
photographs O
(so-called O
few- O
shot O
learning) B-DAT
and O
with O
limited O
training O
time O

on O
a O
single O
photograph O
(one-shot O
learning), B-DAT
while O
adding O
a O
few O
more O

The O
few-shot O
learning B-DAT
ability O
is O
obtained O
through O
exten O

learning) B-DAT
on O
a O
large O
corpus O
of O

learning, B-DAT
our O
sys- O
tem O
simulates O
few-shot O

learning B-DAT
tasks O
and O
learns O
to O
trans O

sets O
up O
a O
new O
adversarial O
learning B-DAT
problem O
with O
high-capacity O
generator O
and O

learning B-DAT

and, O
more O
recently, O
with O
deep O
learning B-DAT
[22, O
25] O
(to O
name O
just O

learning B-DAT
stage O
uses O
the O
adaptive O
instance O

learning B-DAT
to O
obtain O
the O
initial O
state O

learning B-DAT

learning B-DAT
[41] O
use O
adversarially- O
trained O
networks O

learning B-DAT
stage. O
While O
these O
methods O
are O

adversarial O
fine-tuning O
into O
the O
meta- O
learning B-DAT
framework. O
The O
former O
is O
applied O

learning B-DAT
stage O

4, O
18]. O
Their O
setting O
(few-shot O
learning B-DAT
of O
generative O
models) O
and O
some O

domain, O
the O
use O
of O
adversarial O
learning, B-DAT
its O
specific O
adaptation O
to O
the O

learning B-DAT
process O
and O
numerous O
im- O
plementation O

learning B-DAT
architecture O
involves O
the O
embedder O
network O

learning, B-DAT
we O
pass O
sets O
of O
frames O

learning B-DAT
stage O
of O
our O
approach O
assumes O

its O
t-th O
frame. O
During O
the O
learning B-DAT
process, O
as O
well O
as O
during O

learning B-DAT
stage O
of O
our O
approach, O
the O

learning B-DAT
stage. O
In O
general, O
during O
meta-learning O

learning, B-DAT
only O
ψ O
are O
trained O
directly O

learning B-DAT
stage O

learning B-DAT
stage O
of O
our O
approach, O
the O

3.3. O
Few-shot O
learning B-DAT
by O
fine-tuning O

learning B-DAT
has O
converged, O
our O
system O
can O

learning B-DAT
stage. O
As O
before, O
the O
synthe O

learning B-DAT
stage O

learning B-DAT
stage. O
A O
straightforward O
way O
to O

learning B-DAT
with O
a O
single O
video O
sequence O

learning B-DAT
stage O
to O
initialize O
ψ′, O
i.e O

learning B-DAT
stage. O
The O
initialization O
of O
w O

learning B-DAT
stage O

learning B-DAT
stage. O
For O
the O
intiailization, O
we O

learning B-DAT
dataset). O
However, O
the O
match O
term O

learning B-DAT
process O
ensures O
the O
similarity O
between O

Once O
the O
new O
learning B-DAT
problem O
is O
set O
up, O
the O

follow O
directly O
from O
the O
meta- O
learning B-DAT
variants. O
Thus, O
the O
generator O
parameters O

learning B-DAT
stage O
is O
also O
crucial. O
As O

Adam O
[21]. O
We O
set O
the O
learning B-DAT
rate O
of O
the O
embedder O
and O

different O
datasets O
with O
multiple O
few-shot O
learning B-DAT
settings. O
Please O
re- O
fer O
to O

fine-tune O
all O
models O
on O
few-shot O
learning B-DAT
sets O
of O
size O
T O
for O

learning B-DAT
(or O
pretraining) O
stage. O
After O
the O

few-shot O
learning, B-DAT
the O
evaluation O
is O
performed O
on O

T O
used O
in O
few- O
shot O
learning B-DAT

we O
perform O
one- O
and O
few-shot O
learning B-DAT
on O
a O
video O
of O
a O

learning B-DAT
or O
pretraining. O
We O
set O
the O

the O
comparison O
of O
the O
few-shot O
learning B-DAT
timings. O
Both O
are O
provided O
in O

allow O
to O
trade O
off O
few-shot O
learning B-DAT
speed O
versus O
the O
results O
quality O

per- O
forms O
better O
for O
low-shot O
learning B-DAT
(e.g. O
one-shot), O
while O
the O
FT O

variant O
allows O
fast O
(real-time) O
few-shot O
learning B-DAT
of O
new O
avatars, O
fine-tuning O
ultimately O

learning B-DAT
of O
ad O

and O
S. O
Levine. O
Model-agnostic O
meta- O
learning B-DAT
for O
fast O
adaptation O
of O
deep O

Y. O
Wu, O
et O
al. O
Transfer O
learning B-DAT
from O
speaker O
verification O
to O
multispeaker O

I. O
Kemelmacher- O
Shlizerman. O
Synthesizing O
Obama: O
learning B-DAT
lip O
sync O
from O
au- O
dio O

and O
Y. O
Wang. O
Adversarial O
meta- O
learning B-DAT

An O
adversarial O
approach O
to O
few-shot O
learning B-DAT

Pix2pixHD O
and O
our O
method, O
few-shot O
learning B-DAT
was O
done O
via O
fine-tuning O
for O

Method O
(T) O
Time, O
s O
Few-shot O
learning B-DAT

2: O
Quantitative O
comparison O
of O
few-shot O
learning B-DAT
and O
inference O
timings O
for O
the O

learning B-DAT

multiple O
training O
frames O
in O
few-shot O
learning B-DAT
problems, O
like O
in O
our O
final O

learning B-DAT
configu- O
ration, O
which O
turned O
out O

learning, B-DAT
we O
randomly O
initialize O
the O
person-specific O

learning B-DAT
objective O
and O
initialize O
the O
embedding O

learning B-DAT
or O
pretraining. O
We O
used O
eight O

shot O
learning B-DAT
problem O
formulation. O
The O
notation O
for O

learning B-DAT
or O
pretraining. O
We O
used O
eight O

shot O
learning B-DAT
problem O
formulation. O
The O
notation O
for O

videos O
at O
1 O
fps) O
and O
VoxCeleb2 B-DAT
[8] O
(224p O
videos O
at O
25 O

VoxCeleb2 B-DAT
Ours-FF O
(1) O
46.1 O
0.61 O
0.42 O

ablation O
studies, O
while O
by O
using O
VoxCeleb2 B-DAT
we O
show O
the O
full O
potential O

our O
method O
on O
a O
larger O
VoxCeleb2 B-DAT
dataset. O
Here, O
we O
train O
two O

from O
test O
videos O
of O
the O
VoxCeleb2 B-DAT
dataset. O
We O
rank O
these O
videos O

our O
best O
models O
on O
the O
VoxCeleb2 B-DAT
dataset. O
The O
number O
of O
training O

poses O
were O
taken O
from O
the O
VoxCeleb2 B-DAT
dataset. O
Digital O
zoom O
recommended O

was O
trained O
only O
for O
the O
VoxCeleb2 B-DAT
dataset. O
The O
comparison O
was O
car O

extended O
qualitative O
comparisons O
on O
the O
VoxCeleb2 B-DAT
dataset. O
Here, O
the O
comparison O
is O

extended O
qualitative O
comparison O
on O
the O
VoxCeleb2 B-DAT
dataset. O
Here, O
we O
compare O
qualitative O

- B-DAT

- B-DAT

- B-DAT
tional O
neural O
networks O
to O
generate O

- B-DAT
ate O
a O
personalized O
talking O
head O

- B-DAT

- B-DAT

- B-DAT
ter O
that O
is O
able O
to O

- B-DAT
and O
one-shot O
learning O
of O
neural O

- B-DAT
sarial O
training O
problems O
with O
high O

- B-DAT

- B-DAT
alized O
photorealistic O
talking O
head O
models O

- B-DAT

- B-DAT
sions O
and O
mimics O
of O
a O

- B-DAT
ically, O
we O
consider O
the O
problem O

- B-DAT
alistic O
personalized O
head O
images O
given O

- B-DAT
marks, O
which O
drive O
the O
animation O

- B-DAT
conferencing O
and O
multi-player O
games, O
as O

- B-DAT
fects O
industry. O
Synthesizing O
realistic O
talking O

- B-DAT
ity. O
This O
complexity O
stems O
not O

- B-DAT
man O
visual O
system O
towards O
even O

- B-DAT
pearance O
modeling O
of O
human O
heads O

- B-DAT

- B-DAT
takes O
explains O
the O
current O
prevalence O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferencing O
systems O

- B-DAT
posed O
to O
synthesize O
articulated O
head O

- B-DAT
chine O
learning O
(including O
deep O
learning O

- B-DAT

- B-DAT
age, O
the O
amount O
of O
motion O

- B-DAT

- B-DAT

- B-DAT
vNets) O
presents O
the O
new O
hope O

- B-DAT
ever, O
to O
succeed, O
such O
methods O

- B-DAT
lions O
of O
parameters O
for O
each O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
phisticated O
physical O
and O
optical O
modeling O

- B-DAT
cessive O
for O
most O
practical O
telepresence O

- B-DAT
els O
with O
as O
little O
effort O

- B-DAT

- B-DAT
shot O
learning) O
and O
with O
limited O

- B-DAT

- B-DAT
larly O
to O
[16, O
20, O
37 O

- B-DAT
yond O
the O
abilities O
of O
warping-based O

- B-DAT

- B-DAT
sive O
pre-training O
(meta-learning) O
on O
a O

- B-DAT
ing O
head O
videos O
corresponding O
to O

- B-DAT
verse O
appearance. O
In O
the O
course O

- B-DAT

- B-DAT
tem O
simulates O
few-shot O
learning O
tasks O

- B-DAT
form O
landmark O
positions O
into O
realistically-looking O

- B-DAT
alized O
photographs, O
given O
a O
small O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ficient O
realism O
and O
personalization O
fidelity O

- B-DAT
ing O
head O
models, O
including O
video O

- B-DAT
ing O
of O
the O
appearance O
of O

- B-DAT

- B-DAT

- B-DAT
tension O
of O
the O
face O
modeling O

- B-DAT
ability O
and O
higher O
complexity O
than O

- B-DAT
ple, O
the O
results O
of O
face O

- B-DAT
fledged O
talking O
head O
system O

- B-DAT
tecture O
uses O
adversarial O
training O
[12 O

- B-DAT
ing O
projection O
discriminators O
[32]. O
Our O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sifier, O
from O
which O
it O
can O

- B-DAT
fiers O
of O
unseen O
classes, O
given O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
GAN O
[43], O
adversarial O
meta-learning O
[41 O

- B-DAT
trained O
networks O
to O
generate O
additional O

- B-DAT

- B-DAT

- B-DAT
mance, O
our O
method O
deals O
with O

- B-DAT
ation O
models O
using O
similar O
adversarial O

- B-DAT
marize, O
we O
bring O
the O
adversarial O

- B-DAT

- B-DAT
learning O
framework. O
The O
former O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cation O
domain, O
the O
use O
of O

- B-DAT

- B-DAT
plementation O
details O

- B-DAT

- B-DAT
marks) O
to O
the O
embedding O
vectors O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
quence O
and O
with O
xi(t) O
its O

- B-DAT

- B-DAT
ity O
of O
the O
face O
landmarks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

these O
inputs O
into O
an O
N O
- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
yi(t) O
for O
the O
video O

- B-DAT
sized O
video O
frame O
x̂i(t). O
The O

- B-DAT
imize O
the O
similarity O
between O
its O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trix O
P: O
ψ̂i O
= O
Pêi O

landmark O
image O
into O
an O
N O
- B-DAT

- B-DAT
criminator O
predicts O
a O
single O
scalar O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rameters O
of O
all O
three O
networks O

- B-DAT

- B-DAT
ing O
(K O
= O
8 O
in O

- B-DAT
domly O
draw O
a O
training O
video O

- B-DAT
ditional O
K O
frames O
s1, O
s2 O

- B-DAT

- B-DAT
ding O
by O
simply O
averaging O
the O

- B-DAT

- B-DAT
tion O
x̂i(t) O
using O
the O
perceptual O

- B-DAT
responding O
to O
VGG19 O
[30] O
network O

- B-DAT
sentially O
is O
a O
perceptual O
similarity O

- B-DAT
respond O
to O
individual O
videos. O
The O

maps O
its O
inputs O
to O
anN O
- B-DAT

- B-DAT

- B-DAT
criminator. O
The O
match O
term O
LMCH(φ,W O

- B-DAT

- B-DAT
ters O
θ,W,w0, O
b O
of O
the O

- B-DAT
courages O
the O
increase O
of O
the O

- B-DAT
ample O
x̂i(t) O
and O
the O
real O

- B-DAT
nating O
updates O
of O
the O
embedder O

- B-DAT
imize O
the O
losses O
LCNT,LADV O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sis O
is O
conditioned O
on O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
timate O
the O
embedding O
for O
the O

- B-DAT

- B-DAT
sponding O
to O
new O
landmark O
images O

- B-DAT
erator O
using O
the O
estimated O
embedding O

- B-DAT
learned O
parameters O
ψ, O
as O
well O

- B-DAT
able O
identity O
gap O
that O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sult O
of O
the O
meta-learning O
stage O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tors O
computed O
by O
the O
embedder O

- B-DAT
tions O
of O
the O
fine-tuning O
stage O

- B-DAT
learning O
variants. O
Thus, O
the O
generator O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
son O
et. O
al. O
[19], O
but O

- B-DAT
malization O
[15] O
replaced O
by O
instance O

- B-DAT

- B-DAT
efficients O
of O
instance O
normalization O
layers O

- B-DAT

- B-DAT
ization O
layers O
in O
the O
downsampling O

- B-DAT
mark O
images O
yi(t O

- B-DAT
tional O
part O
of O
the O
discriminator O

- B-DAT
out O
normalization O
layers). O
The O
discriminator O

- B-DAT
pared O
to O
the O
embedder, O
has O

- B-DAT

- B-DAT
serted O
at O
32×32 O
spatial O
resolution O

- B-DAT
tween O
activations O
of O
Conv1,6,11,20,29 O
VGG19 O

- B-DAT
nally, O
for O
LMCH O
we O
set O

- B-DAT
tional O
layers O
to O
64 O
and O

- B-DAT
tor O
has O
38 O
million O
parameters O

- B-DAT
titative O
and O
qualitative O
evaluation: O
VoxCeleb1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fer O
to O
the O
text O
for O

- B-DAT

- B-DAT

- B-DAT
son O
not O
seen O
during O
meta-learning O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
reenactment O
scenario). O
For O
the O
evaluation O

- B-DAT
out O
frames O
for O
each O
of O

- B-DAT

- B-DAT

- B-DAT
realism O
and O
identity O
preservation O
of O

- B-DAT

- B-DAT

- B-DAT
bedding O
vectors O
of O
the O
state-of-the-art O

- B-DAT
work O
[9] O
for O
measuring O
identity O

- B-DAT
tual O
similarity O
and O
realism O
of O

- B-DAT
man O
respondents. O
We O
show O
people O

- B-DAT

- B-DAT
not O
spot O
fakes O
based O
on O

- B-DAT

- B-DAT

- B-DAT
Celeb1 O
dataset). O
For O
Pix2pixHD, O
we O

- B-DAT

- B-DAT
shot O
learning. O
X2Face, O
as O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
which O
arguably O
gives O
X2Face O

- B-DAT
lines O
in O
three O
different O
setups O

- B-DAT

- B-DAT

- B-DAT
dom O
from O
the O
other O
video O

- B-DAT

- B-DAT

- B-DAT
perform O
our O
method O
on O
the O

- B-DAT
imizes O
only O
perceptual O
metric, O
without O

- B-DAT
and O
few-shot O
learning O
on O
a O

- B-DAT

- B-DAT
ter O
correlates O
with O
visual O
quality O

- B-DAT
ble O
1-Top O
with O
the O
results O

- B-DAT
alism O
and O
personalization O
degree O
achieved O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
out O
fine-tuning O
(by O
simply O
predicting O

- B-DAT
ant O
is O
trained O
for O
half O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults, O
where O
animation O
is O
driven O

- B-DAT
ent O
video O
of O
the O
same O

- B-DAT
tary O
material O
and O
in O
Figure O

- B-DAT
ble O
1-Bottom) O
and O
the O
visual O

- B-DAT
forms O
better O
for O
low-shot O
learning O

- B-DAT

- B-DAT
ial O
fine-tuning O

- B-DAT

- B-DAT
sons O
with O
similar O
geometry O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tographs O
in O
the O
source O
column O

- B-DAT

- B-DAT
realistic O
virtual O
talking O
heads O
in O

- B-DAT
ization O
score O
in O
our O
user O

- B-DAT
ics O
representation O
(in O
particular, O
the O

- B-DAT
son O
leads O
to O
a O
noticeable O

- B-DAT
ing O
a O
different O
person O
and O

- B-DAT
proach O
already O
provides O
a O
high-realism O

- B-DAT
puter O
Graphics O
and O
Applications, O
30(4):20–31 O

- B-DAT
sarial O
networks. O
In O
Artificial O
Neural O

Networks O
and O
Machine O
Learning O
- B-DAT
ICANN, O
pages O
594–603, O
2018. O
2 O

- B-DAT

- B-DAT

- B-DAT
thesis O
of O
3d O
faces. O
In O

- B-DAT
29, O
2017, O
pages O
1021–1030, O
2017 O

- B-DAT

- B-DAT
learning O
for O
fast O
adaptation O
of O

- B-DAT
ulation. O
In O
European O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
nets. O
In O
Advances O

- B-DAT

- B-DAT
wanathan, O
and O
R. O
Garnett, O
editors O

- B-DAT
formation O
Processing O
Systems O
30, O
pages O

- B-DAT
time O
with O
adaptive O
instance O
normalization O

- B-DAT
ternational O
Conference O
on O
Machine O
Learning O

- B-DAT

- B-DAT

- B-DAT
shick, O
S. O
Guadarrama, O
and O
T O

- B-DAT
tional O
architecture O
for O
fast O
feature O

- B-DAT

- B-DAT
speech O
synthesis. O
In O
Proc. O
NIPS O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
SPEECH, O
2017. O
5 O

- B-DAT
gios, O
and O
I. O
Kokkinos. O
Deforming O

- B-DAT
vised O
disentangling O
of O
shape O
and O

- B-DAT
ropean O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
Shlizerman. O
Synthesizing O
Obama: O
learning O
lip O

- B-DAT
dio. O
ACM O
Transactions O
on O
Graphics O

- B-DAT
tral O
normalization O
for O
generative O
adversarial O

- B-DAT

- B-DAT
erator O
architecture O
for O
generative O
adversarial O

- B-DAT

- B-DAT
actment O
of O
RGB O
videos. O
In O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
2018. O
4, O
6 O

- B-DAT
learning. O
CoRR, O
abs/1806.03316, O
2018. O
2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ried O
out O
on O
a O
single O

- B-DAT

- B-DAT

- B-DAT
surement O
was O
averaged O
over O
100 O

- B-DAT

- B-DAT

- B-DAT
ing O
personalization O
fidelity O
and O
realism O

- B-DAT

- B-DAT

- B-DAT
duction O
of O
a O
training O
scheduler O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vided O
by O
the O
embedder O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
specific O
initialization O
of O
the O
discriminator O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ration, O
which O
turned O
out O
to O

- B-DAT

- B-DAT

- B-DAT
tice O
that O
the O
results O
for O

- B-DAT
sonalization O
fidelity. O
We, O
therefore, O
came O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

T) O
FID↓ O
SSIM↑ O
CSIM↑ O
USER↓ O
VoxCeleb1 B-DAT
X2Face I-DAT
(1) O
45.8 O
0.68 O
0.16 O

than O
the O
former. O
VoxCeleb1 B-DAT

Methods. O
On O
the O
VoxCeleb1 B-DAT

Figure O
3: O
Comparison O
on O
the O
VoxCeleb1 B-DAT

extended O
qualitative O
comparisons O
on O
the O
VoxCeleb1 B-DAT

extended O
qualitative O
comparison O
on O
the O
VoxCeleb1 B-DAT

quan- O
titative O
and O
qualitative O
evaluation: O
VoxCeleb1 B-DAT
[26] O
(256p O
videos O
at O
1 O

T) O
FID↓ O
SSIM↑ O
CSIM↑ O
USER↓ O
VoxCeleb1 B-DAT

than O
the O
former. O
VoxCeleb1 B-DAT
is O
used O
for O
comparison O
with O

Methods. O
On O
the O
VoxCeleb1 B-DAT
dataset O
we O
compare O
our O
model O

Figure O
3: O
Comparison O
on O
the O
VoxCeleb1 B-DAT
dataset. O
For O
each O
of O
the O

to O
smaller-scale O
models O
trained O
on O
VoxCeleb1 B-DAT

extended O
qualitative O
comparisons O
on O
the O
VoxCeleb1 B-DAT
dataset. O
Here, O
the O
comparison O
is O

extended O
qualitative O
comparison O
on O
the O
VoxCeleb1 B-DAT
dataset. O
Here, O
we O
compare O
qualitative O

1 O
8 B-DAT

1 O
8 B-DAT

already O
provides O
a O
high-realism O
solution. O
8 B-DAT

1 O
8 B-DAT

1 O
8 B-DAT

1 O
8 B-DAT

1 O
8 B-DAT

1 O
8 B-DAT

1 O
8 B-DAT

learning B-DAT
on O
a O
large O
dataset O
of O

to O
frame O
few- O
and O
one-shot O
learning B-DAT
of O
neural O
talking O
head O
models O

fields O
synthesized O
using O
ma- O
chine O
learning B-DAT
(including O
deep O
learning) O
[11, O
29 O

of O
photographs O
(so-called O
few- O
shot O
learning) B-DAT
and O
with O
limited O
training O
time O

on O
a O
single O
photograph O
(one-shot O
learning), B-DAT
while O
adding O
a O
few O
more O

The O
few-shot O
learning B-DAT
ability O
is O
obtained O
through O
exten O

learning) B-DAT
on O
a O
large O
corpus O
of O

learning, B-DAT
our O
sys- O
tem O
simulates O
few-shot O

learning B-DAT
tasks O
and O
learns O
to O
trans O

sets O
up O
a O
new O
adversarial O
learning B-DAT
problem O
with O
high-capacity O
generator O
and O

learning B-DAT

and, O
more O
recently, O
with O
deep O
learning B-DAT
[22, O
25] O
(to O
name O
just O

learning B-DAT
stage O
uses O
the O
adaptive O
instance O

learning B-DAT
to O
obtain O
the O
initial O
state O

learning B-DAT

learning B-DAT
[41] O
use O
adversarially- O
trained O
networks O

learning B-DAT
stage. O
While O
these O
methods O
are O

adversarial O
fine-tuning O
into O
the O
meta- O
learning B-DAT
framework. O
The O
former O
is O
applied O

learning B-DAT
stage O

4, O
18]. O
Their O
setting O
(few-shot O
learning B-DAT
of O
generative O
models) O
and O
some O

domain, O
the O
use O
of O
adversarial O
learning, B-DAT
its O
specific O
adaptation O
to O
the O

learning B-DAT
process O
and O
numerous O
im- O
plementation O

learning B-DAT
architecture O
involves O
the O
embedder O
network O

learning, B-DAT
we O
pass O
sets O
of O
frames O

learning B-DAT
stage O
of O
our O
approach O
assumes O

its O
t-th O
frame. O
During O
the O
learning B-DAT
process, O
as O
well O
as O
during O

learning B-DAT
stage O
of O
our O
approach, O
the O

learning B-DAT
stage. O
In O
general, O
during O
meta-learning O

learning, B-DAT
only O
ψ O
are O
trained O
directly O

learning B-DAT
stage O

learning B-DAT
stage O
of O
our O
approach, O
the O

3.3. O
Few-shot O
learning B-DAT
by O
fine-tuning O

learning B-DAT
has O
converged, O
our O
system O
can O

learning B-DAT
stage. O
As O
before, O
the O
synthe O

learning B-DAT
stage O

learning B-DAT
stage. O
A O
straightforward O
way O
to O

learning B-DAT
with O
a O
single O
video O
sequence O

learning B-DAT
stage O
to O
initialize O
ψ′, O
i.e O

learning B-DAT
stage. O
The O
initialization O
of O
w O

learning B-DAT
stage O

learning B-DAT
stage. O
For O
the O
intiailization, O
we O

learning B-DAT
dataset). O
However, O
the O
match O
term O

learning B-DAT
process O
ensures O
the O
similarity O
between O

Once O
the O
new O
learning B-DAT
problem O
is O
set O
up, O
the O

follow O
directly O
from O
the O
meta- O
learning B-DAT
variants. O
Thus, O
the O
generator O
parameters O

learning B-DAT
stage O
is O
also O
crucial. O
As O

Adam O
[21]. O
We O
set O
the O
learning B-DAT
rate O
of O
the O
embedder O
and O

different O
datasets O
with O
multiple O
few-shot O
learning B-DAT
settings. O
Please O
re- O
fer O
to O

fine-tune O
all O
models O
on O
few-shot O
learning B-DAT
sets O
of O
size O
T O
for O

learning B-DAT
(or O
pretraining) O
stage. O
After O
the O

few-shot O
learning, B-DAT
the O
evaluation O
is O
performed O
on O

T O
used O
in O
few- O
shot O
learning B-DAT

we O
perform O
one- O
and O
few-shot O
learning B-DAT
on O
a O
video O
of O
a O

learning B-DAT
or O
pretraining. O
We O
set O
the O

the O
comparison O
of O
the O
few-shot O
learning B-DAT
timings. O
Both O
are O
provided O
in O

allow O
to O
trade O
off O
few-shot O
learning B-DAT
speed O
versus O
the O
results O
quality O

per- O
forms O
better O
for O
low-shot O
learning B-DAT
(e.g. O
one-shot), O
while O
the O
FT O

variant O
allows O
fast O
(real-time) O
few-shot O
learning B-DAT
of O
new O
avatars, O
fine-tuning O
ultimately O

learning B-DAT
of O
ad O

and O
S. O
Levine. O
Model-agnostic O
meta- O
learning B-DAT
for O
fast O
adaptation O
of O
deep O

Y. O
Wu, O
et O
al. O
Transfer O
learning B-DAT
from O
speaker O
verification O
to O
multispeaker O

I. O
Kemelmacher- O
Shlizerman. O
Synthesizing O
Obama: O
learning B-DAT
lip O
sync O
from O
au- O
dio O

and O
Y. O
Wang. O
Adversarial O
meta- O
learning B-DAT

An O
adversarial O
approach O
to O
few-shot O
learning B-DAT

Pix2pixHD O
and O
our O
method, O
few-shot O
learning B-DAT
was O
done O
via O
fine-tuning O
for O

Method O
(T) O
Time, O
s O
Few-shot O
learning B-DAT

2: O
Quantitative O
comparison O
of O
few-shot O
learning B-DAT
and O
inference O
timings O
for O
the O

learning B-DAT

multiple O
training O
frames O
in O
few-shot O
learning B-DAT
problems, O
like O
in O
our O
final O

learning B-DAT
configu- O
ration, O
which O
turned O
out O

learning, B-DAT
we O
randomly O
initialize O
the O
person-specific O

learning B-DAT
objective O
and O
initialize O
the O
embedding O

learning B-DAT
or O
pretraining. O
We O
used O
eight O

shot O
learning B-DAT
problem O
formulation. O
The O
notation O
for O

learning B-DAT
or O
pretraining. O
We O
used O
eight O

shot O
learning B-DAT
problem O
formulation. O
The O
notation O
for O

- B-DAT

- B-DAT

- B-DAT
tional O
neural O
networks O
to O
generate O

- B-DAT
ate O
a O
personalized O
talking O
head O

- B-DAT

- B-DAT

- B-DAT
ter O
that O
is O
able O
to O

- B-DAT
and O
one-shot O
learning O
of O
neural O

- B-DAT
sarial O
training O
problems O
with O
high O

- B-DAT

- B-DAT
alized O
photorealistic O
talking O
head O
models O

- B-DAT

- B-DAT
sions O
and O
mimics O
of O
a O

- B-DAT
ically, O
we O
consider O
the O
problem O

- B-DAT
alistic O
personalized O
head O
images O
given O

- B-DAT
marks, O
which O
drive O
the O
animation O

- B-DAT
conferencing O
and O
multi-player O
games, O
as O

- B-DAT
fects O
industry. O
Synthesizing O
realistic O
talking O

- B-DAT
ity. O
This O
complexity O
stems O
not O

- B-DAT
man O
visual O
system O
towards O
even O

- B-DAT
pearance O
modeling O
of O
human O
heads O

- B-DAT

- B-DAT
takes O
explains O
the O
current O
prevalence O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferencing O
systems O

- B-DAT
posed O
to O
synthesize O
articulated O
head O

- B-DAT
chine O
learning O
(including O
deep O
learning O

- B-DAT

- B-DAT
age, O
the O
amount O
of O
motion O

- B-DAT

- B-DAT

- B-DAT
vNets) O
presents O
the O
new O
hope O

- B-DAT
ever, O
to O
succeed, O
such O
methods O

- B-DAT
lions O
of O
parameters O
for O
each O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
phisticated O
physical O
and O
optical O
modeling O

- B-DAT
cessive O
for O
most O
practical O
telepresence O

- B-DAT
els O
with O
as O
little O
effort O

- B-DAT

- B-DAT
shot O
learning) O
and O
with O
limited O

- B-DAT

- B-DAT
larly O
to O
[16, O
20, O
37 O

- B-DAT
yond O
the O
abilities O
of O
warping-based O

- B-DAT

- B-DAT
sive O
pre-training O
(meta-learning) O
on O
a O

- B-DAT
ing O
head O
videos O
corresponding O
to O

- B-DAT
verse O
appearance. O
In O
the O
course O

- B-DAT

- B-DAT
tem O
simulates O
few-shot O
learning O
tasks O

- B-DAT
form O
landmark O
positions O
into O
realistically-looking O

- B-DAT
alized O
photographs, O
given O
a O
small O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ficient O
realism O
and O
personalization O
fidelity O

- B-DAT
ing O
head O
models, O
including O
video O

- B-DAT
ing O
of O
the O
appearance O
of O

- B-DAT

- B-DAT

- B-DAT
tension O
of O
the O
face O
modeling O

- B-DAT
ability O
and O
higher O
complexity O
than O

- B-DAT
ple, O
the O
results O
of O
face O

- B-DAT
fledged O
talking O
head O
system O

- B-DAT
tecture O
uses O
adversarial O
training O
[12 O

- B-DAT
ing O
projection O
discriminators O
[32]. O
Our O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sifier, O
from O
which O
it O
can O

- B-DAT
fiers O
of O
unseen O
classes, O
given O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
GAN O
[43], O
adversarial O
meta-learning O
[41 O

- B-DAT
trained O
networks O
to O
generate O
additional O

- B-DAT

- B-DAT

- B-DAT
mance, O
our O
method O
deals O
with O

- B-DAT
ation O
models O
using O
similar O
adversarial O

- B-DAT
marize, O
we O
bring O
the O
adversarial O

- B-DAT

- B-DAT
learning O
framework. O
The O
former O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cation O
domain, O
the O
use O
of O

- B-DAT

- B-DAT
plementation O
details O

- B-DAT

- B-DAT
marks) O
to O
the O
embedding O
vectors O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
quence O
and O
with O
xi(t) O
its O

- B-DAT

- B-DAT
ity O
of O
the O
face O
landmarks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

these O
inputs O
into O
an O
N O
- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
yi(t) O
for O
the O
video O

- B-DAT
sized O
video O
frame O
x̂i(t). O
The O

- B-DAT
imize O
the O
similarity O
between O
its O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trix O
P: O
ψ̂i O
= O
Pêi O

landmark O
image O
into O
an O
N O
- B-DAT

- B-DAT
criminator O
predicts O
a O
single O
scalar O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rameters O
of O
all O
three O
networks O

- B-DAT

- B-DAT
ing O
(K O
= O
8 O
in O

- B-DAT
domly O
draw O
a O
training O
video O

- B-DAT
ditional O
K O
frames O
s1, O
s2 O

- B-DAT

- B-DAT
ding O
by O
simply O
averaging O
the O

- B-DAT

- B-DAT
tion O
x̂i(t) O
using O
the O
perceptual O

- B-DAT
responding O
to O
VGG19 O
[30] O
network O

- B-DAT
sentially O
is O
a O
perceptual O
similarity O

- B-DAT
respond O
to O
individual O
videos. O
The O

maps O
its O
inputs O
to O
anN O
- B-DAT

- B-DAT

- B-DAT
criminator. O
The O
match O
term O
LMCH(φ,W O

- B-DAT

- B-DAT
ters O
θ,W,w0, O
b O
of O
the O

- B-DAT
courages O
the O
increase O
of O
the O

- B-DAT
ample O
x̂i(t) O
and O
the O
real O

- B-DAT
nating O
updates O
of O
the O
embedder O

- B-DAT
imize O
the O
losses O
LCNT,LADV O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sis O
is O
conditioned O
on O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
timate O
the O
embedding O
for O
the O

- B-DAT

- B-DAT
sponding O
to O
new O
landmark O
images O

- B-DAT
erator O
using O
the O
estimated O
embedding O

- B-DAT
learned O
parameters O
ψ, O
as O
well O

- B-DAT
able O
identity O
gap O
that O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sult O
of O
the O
meta-learning O
stage O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tors O
computed O
by O
the O
embedder O

- B-DAT
tions O
of O
the O
fine-tuning O
stage O

- B-DAT
learning O
variants. O
Thus, O
the O
generator O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
son O
et. O
al. O
[19], O
but O

- B-DAT
malization O
[15] O
replaced O
by O
instance O

- B-DAT

- B-DAT
efficients O
of O
instance O
normalization O
layers O

- B-DAT

- B-DAT
ization O
layers O
in O
the O
downsampling O

- B-DAT
mark O
images O
yi(t O

- B-DAT
tional O
part O
of O
the O
discriminator O

- B-DAT
out O
normalization O
layers). O
The O
discriminator O

- B-DAT
pared O
to O
the O
embedder, O
has O

- B-DAT

- B-DAT
serted O
at O
32×32 O
spatial O
resolution O

- B-DAT
tween O
activations O
of O
Conv1,6,11,20,29 O
VGG19 O

- B-DAT
nally, O
for O
LMCH O
we O
set O

- B-DAT
tional O
layers O
to O
64 O
and O

- B-DAT
tor O
has O
38 O
million O
parameters O

- B-DAT
titative O
and O
qualitative O
evaluation: O
VoxCeleb1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fer O
to O
the O
text O
for O

- B-DAT

- B-DAT

- B-DAT
son O
not O
seen O
during O
meta-learning O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
reenactment O
scenario). O
For O
the O
evaluation O

- B-DAT
out O
frames O
for O
each O
of O

- B-DAT

- B-DAT

- B-DAT
realism O
and O
identity O
preservation O
of O

- B-DAT

- B-DAT

- B-DAT
bedding O
vectors O
of O
the O
state-of-the-art O

- B-DAT
work O
[9] O
for O
measuring O
identity O

- B-DAT
tual O
similarity O
and O
realism O
of O

- B-DAT
man O
respondents. O
We O
show O
people O

- B-DAT

- B-DAT
not O
spot O
fakes O
based O
on O

- B-DAT

- B-DAT

- B-DAT
Celeb1 O
dataset). O
For O
Pix2pixHD, O
we O

- B-DAT

- B-DAT
shot O
learning. O
X2Face, O
as O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
which O
arguably O
gives O
X2Face O

- B-DAT
lines O
in O
three O
different O
setups O

- B-DAT

- B-DAT

- B-DAT
dom O
from O
the O
other O
video O

- B-DAT

- B-DAT

- B-DAT
perform O
our O
method O
on O
the O

- B-DAT
imizes O
only O
perceptual O
metric, O
without O

- B-DAT
and O
few-shot O
learning O
on O
a O

- B-DAT

- B-DAT
ter O
correlates O
with O
visual O
quality O

- B-DAT
ble O
1-Top O
with O
the O
results O

- B-DAT
alism O
and O
personalization O
degree O
achieved O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
out O
fine-tuning O
(by O
simply O
predicting O

- B-DAT
ant O
is O
trained O
for O
half O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults, O
where O
animation O
is O
driven O

- B-DAT
ent O
video O
of O
the O
same O

- B-DAT
tary O
material O
and O
in O
Figure O

- B-DAT
ble O
1-Bottom) O
and O
the O
visual O

- B-DAT
forms O
better O
for O
low-shot O
learning O

- B-DAT

- B-DAT
ial O
fine-tuning O

- B-DAT

- B-DAT
sons O
with O
similar O
geometry O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tographs O
in O
the O
source O
column O

- B-DAT

- B-DAT
realistic O
virtual O
talking O
heads O
in O

- B-DAT
ization O
score O
in O
our O
user O

- B-DAT
ics O
representation O
(in O
particular, O
the O

- B-DAT
son O
leads O
to O
a O
noticeable O

- B-DAT
ing O
a O
different O
person O
and O

- B-DAT
proach O
already O
provides O
a O
high-realism O

- B-DAT
puter O
Graphics O
and O
Applications, O
30(4):20–31 O

- B-DAT
sarial O
networks. O
In O
Artificial O
Neural O

Networks O
and O
Machine O
Learning O
- B-DAT
ICANN, O
pages O
594–603, O
2018. O
2 O

- B-DAT

- B-DAT

- B-DAT
thesis O
of O
3d O
faces. O
In O

- B-DAT
29, O
2017, O
pages O
1021–1030, O
2017 O

- B-DAT

- B-DAT
learning O
for O
fast O
adaptation O
of O

- B-DAT
ulation. O
In O
European O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
nets. O
In O
Advances O

- B-DAT

- B-DAT
wanathan, O
and O
R. O
Garnett, O
editors O

- B-DAT
formation O
Processing O
Systems O
30, O
pages O

- B-DAT
time O
with O
adaptive O
instance O
normalization O

- B-DAT
ternational O
Conference O
on O
Machine O
Learning O

- B-DAT

- B-DAT

- B-DAT
shick, O
S. O
Guadarrama, O
and O
T O

- B-DAT
tional O
architecture O
for O
fast O
feature O

- B-DAT

- B-DAT
speech O
synthesis. O
In O
Proc. O
NIPS O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
SPEECH, O
2017. O
5 O

- B-DAT
gios, O
and O
I. O
Kokkinos. O
Deforming O

- B-DAT
vised O
disentangling O
of O
shape O
and O

- B-DAT
ropean O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
Shlizerman. O
Synthesizing O
Obama: O
learning O
lip O

- B-DAT
dio. O
ACM O
Transactions O
on O
Graphics O

- B-DAT
tral O
normalization O
for O
generative O
adversarial O

- B-DAT

- B-DAT
erator O
architecture O
for O
generative O
adversarial O

- B-DAT

- B-DAT
actment O
of O
RGB O
videos. O
In O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
2018. O
4, O
6 O

- B-DAT
learning. O
CoRR, O
abs/1806.03316, O
2018. O
2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ried O
out O
on O
a O
single O

- B-DAT

- B-DAT

- B-DAT
surement O
was O
averaged O
over O
100 O

- B-DAT

- B-DAT

- B-DAT
ing O
personalization O
fidelity O
and O
realism O

- B-DAT

- B-DAT

- B-DAT
duction O
of O
a O
training O
scheduler O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vided O
by O
the O
embedder O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
specific O
initialization O
of O
the O
discriminator O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ration, O
which O
turned O
out O
to O

- B-DAT

- B-DAT

- B-DAT
tice O
that O
the O
results O
for O

- B-DAT
sonalization O
fidelity. O
We, O
therefore, O
came O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

shot B-DAT
learning I-DAT
of O
neural O
talking O
head O
models O

handful O
of O
photographs O
(so-called O
few- O
shot B-DAT
learning) I-DAT
and O
with O
limited O
training O
time O

shot B-DAT
learning), I-DAT
while O
adding O
a O
few O
more O

shot B-DAT
learning I-DAT
ability O
is O
obtained O
through O
exten O

shot B-DAT
learning I-DAT
tasks O
and O
learns O
to O
trans O

shot B-DAT
learning I-DAT
of O
generative O
models) O
and O
some O

shot B-DAT
learning I-DAT
by O
fine-tuning O

shot B-DAT
learning I-DAT
settings. O
Please O
re- O
fer O
to O

shot B-DAT
learning I-DAT
sets O
of O
size O
T O
for O

shot B-DAT
learning, I-DAT
the O
evaluation O
is O
performed O
on O

frames O
T O
used O
in O
few- O
shot B-DAT
learning I-DAT

shot B-DAT
learning I-DAT
on O
a O
video O
of O
a O

shot B-DAT
learning I-DAT
timings. O
Both O
are O
provided O
in O

shot B-DAT
learning I-DAT
speed O
versus O
the O
results O
quality O

shot B-DAT
learning I-DAT
(e.g. O
one-shot), O
while O
the O
FT O

shot B-DAT
learning I-DAT
of O
new O
avatars, O
fine-tuning O
ultimately O

shot B-DAT
learning I-DAT

shot B-DAT
learning I-DAT
was O
done O
via O
fine-tuning O
for O

shot B-DAT
learning I-DAT

shot B-DAT
learning I-DAT
and O
inference O
timings O
for O
the O

shot B-DAT
learning I-DAT
problems, O
like O
in O
our O
final O

or O
pretraining. O
We O
used O
eight O
shot B-DAT
learning I-DAT
problem O
formulation. O
The O
notation O
for O

or O
pretraining. O
We O
used O
eight O
shot B-DAT
learning I-DAT
problem O
formulation. O
The O
notation O
for O

3D O
scans O
[4], O
or O
by O
learning B-DAT
3DMM O
parameters O
directly O
from O
RGB O

accordingly. O
This O
is O
done O
by O
learning B-DAT
a O
forward O
mapping O
fp→v O
from O

improving O
the O
generated O
results. O
As O
learning B-DAT
the O
function O
fa→v O
: O
R O

with O
L1 O
loss, O
and O
a O
learning B-DAT
rate O
of O
0.001. O
The O
learning O

phase O
is O
started O
with O
a O
learning B-DAT
rate O
of O
0.0001. O
Testing. O
The O

Abbeel, O
P.: O
Infogan: O
Interpretable O
representation O
learning B-DAT
by O
information O
maximizing O
generative O
adversarial O

Denton, O
E.L., O
Birodkar, O
V.: O
Unsupervised O
learning B-DAT
of O
disentangled O
repre- O
sentations O
from O

facial O
animation O
by O
joint O
end-to-end O
learning B-DAT
of O
pose O
and O
emotion. O
ACM O

King, O
D.E.: O
Dlib-ml: O
A O
machine O
learning B-DAT
toolkit. O
The O
Journal O
of O
Machine O

tion O
of O
unconstrained O
faces O
by O
learning B-DAT
efficient O
H-CNN O
regressors. O
In: O
Proc O

S.M., O
Kemelmacher-Shlizerman, O
I.: O
Synthesizing O
Obama: O
learning B-DAT
lip O
sync O
from O
audio. O
ACM O

X., O
Liu, O
X.: O
Disentangled O
representation O
learning B-DAT
gan O
for O
pose-invariant O
face O
recognition O

- B-DAT
phisticated O
video O
and O
image O
editing O

- B-DAT

- B-DAT
pared O
to O
state-of-the-art O
self-supervised/supervised O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
frame, O
audio O
data, O
or O

- B-DAT

- B-DAT
ing O
frames. O
These O
frames O
are O

- B-DAT

- B-DAT
tions. O
First, O
we O
propose O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
described O
in O
Section O
6 O

- B-DAT
imation O
(or O
puppeteering) O
given O
one O

- B-DAT
erature O
on O
supervised/self-supervised O
approaches; O
here O

- B-DAT
marks O
[5, O
12, O
21, O
30 O

- B-DAT

- B-DAT
tors O
of O
variation O
(e.g. O
optical O

- B-DAT

- B-DAT
form O
images O
of O
one O
domain O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work. O
This O
is O
illustrated O
in O

- B-DAT
termine O
how O
to O
map O
from O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
straints O
based O
on O
the O
identity O

- B-DAT
quently, O
we O
introduce O
additional O
loss O

- B-DAT

-5 B-DAT
and O
Conv7 O
layers O
(i.e. O
layers O

-7 B-DAT
layers O
(i.e. O
layers O
encoding O
higher O

- B-DAT

- B-DAT

- B-DAT
figuration O
A) O
[37] O
trained O
on O

- B-DAT

- B-DAT

- B-DAT
tional O
mapping O
fv→p O
is O
needed O

- B-DAT
nected O
layer O
with O
bias O
and O

- B-DAT
connected O
linear O
layer O
with O
bias O

- B-DAT

- B-DAT

- B-DAT
bedding O
learns O
to O
encode O
some O

- B-DAT
tion. O
Given O
driving O
audio O
features O

- B-DAT

- B-DAT
size O
of O
16. O
First, O
it O

- B-DAT

- B-DAT
ing/testing O
setups. O
Lower O
is O
better O

- B-DAT
centage O
improvement O
over O
the O
L1 O

- B-DAT
egy O
and O
using O
additional O
views O

- B-DAT
main O
(in O
this O
case O
a O

- B-DAT
cleGAN O
is O
trained O
on O
pairs O

- B-DAT
ing O
unrealistic O
results. O
Additionally, O
our O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
4.1), O
the O
25, O
993 O

- B-DAT
neutral O
expressions O
in O
the O
source O

- B-DAT
processing O
(X2Face O
+ O
p.-p.) O
can O

- B-DAT

- B-DAT

- B-DAT
mentary O
material. O
Whilst O
some O
artefacts O

- B-DAT

- B-DAT
eration O
using O
another O
face. O
This O

- B-DAT
constrained O
settings O
(e.g. O
an O
unseen O

- B-DAT

- B-DAT
tioned O
on O
other O
modalities, O
the O

- B-DAT
teresting O
avenue O
of O
research: O
how O

- B-DAT

- B-DAT
eration O
quality O
of O
these O
methods O

- B-DAT

- B-DAT
tions/comments. O
This O
work O
was O
funded O

- B-DAT

- B-DAT

-4 B-DAT

- B-DAT

- B-DAT
ment O
networks. O
In: O
Proc. O
ICCV O

- B-DAT

- B-DAT

- B-DAT
sentations O
from O
video. O
In: O
NIPS O

- B-DAT
tional O
neural O
networks. O
In: O
Proc O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
actions O
on O
Graphics O
(TOG) O
(2017 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lutional O
neural O
networks. O
In: O
Proc O

- B-DAT
tional O
inverse O
graphics O
network. O
In O

- B-DAT
tion O
of O
unconstrained O
faces O
by O

- B-DAT

- B-DAT

- B-DAT
tation, O
face O
swapping, O
and O
face O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
strained O
photo O
collections. O
In: O
Proc O

- B-DAT

- B-DAT

- B-DAT
scale O
image O
recognition. O
In: O
International O

- B-DAT
sentations O
(2015 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pretable O
transformations O
with O
encoder-decoder O
networks O

- B-DAT

- B-DAT

- B-DAT
lation O
using O
cycle-consistent O
adversarial O
networks O

T) O
FID↓ O
SSIM↑ O
CSIM↑ O
USER↓ O
VoxCeleb1 B-DAT
X2Face I-DAT
(1) O
45.8 O
0.68 O
0.16 O

than O
the O
former. O
VoxCeleb1 B-DAT

Methods. O
On O
the O
VoxCeleb1 B-DAT

Figure O
3: O
Comparison O
on O
the O
VoxCeleb1 B-DAT

extended O
qualitative O
comparisons O
on O
the O
VoxCeleb1 B-DAT

extended O
qualitative O
comparison O
on O
the O
VoxCeleb1 B-DAT

quan- O
titative O
and O
qualitative O
evaluation: O
VoxCeleb1 B-DAT
[26] O
(256p O
videos O
at O
1 O

T) O
FID↓ O
SSIM↑ O
CSIM↑ O
USER↓ O
VoxCeleb1 B-DAT

than O
the O
former. O
VoxCeleb1 B-DAT
is O
used O
for O
comparison O
with O

Methods. O
On O
the O
VoxCeleb1 B-DAT
dataset O
we O
compare O
our O
model O

Figure O
3: O
Comparison O
on O
the O
VoxCeleb1 B-DAT
dataset. O
For O
each O
of O
the O

to O
smaller-scale O
models O
trained O
on O
VoxCeleb1 B-DAT

extended O
qualitative O
comparisons O
on O
the O
VoxCeleb1 B-DAT
dataset. O
Here, O
the O
comparison O
is O

extended O
qualitative O
comparison O
on O
the O
VoxCeleb1 B-DAT
dataset. O
Here, O
we O
compare O
qualitative O

learning B-DAT
on O
a O
large O
dataset O
of O

to O
frame O
few- O
and O
one-shot O
learning B-DAT
of O
neural O
talking O
head O
models O

fields O
synthesized O
using O
ma- O
chine O
learning B-DAT
(including O
deep O
learning) O
[11, O
29 O

of O
photographs O
(so-called O
few- O
shot O
learning) B-DAT
and O
with O
limited O
training O
time O

on O
a O
single O
photograph O
(one-shot O
learning), B-DAT
while O
adding O
a O
few O
more O

The O
few-shot O
learning B-DAT
ability O
is O
obtained O
through O
exten O

learning) B-DAT
on O
a O
large O
corpus O
of O

learning, B-DAT
our O
sys- O
tem O
simulates O
few-shot O

learning B-DAT
tasks O
and O
learns O
to O
trans O

sets O
up O
a O
new O
adversarial O
learning B-DAT
problem O
with O
high-capacity O
generator O
and O

learning B-DAT

and, O
more O
recently, O
with O
deep O
learning B-DAT
[22, O
25] O
(to O
name O
just O

learning B-DAT
stage O
uses O
the O
adaptive O
instance O

learning B-DAT
to O
obtain O
the O
initial O
state O

learning B-DAT

learning B-DAT
[41] O
use O
adversarially- O
trained O
networks O

learning B-DAT
stage. O
While O
these O
methods O
are O

adversarial O
fine-tuning O
into O
the O
meta- O
learning B-DAT
framework. O
The O
former O
is O
applied O

learning B-DAT
stage O

4, O
18]. O
Their O
setting O
(few-shot O
learning B-DAT
of O
generative O
models) O
and O
some O

domain, O
the O
use O
of O
adversarial O
learning, B-DAT
its O
specific O
adaptation O
to O
the O

learning B-DAT
process O
and O
numerous O
im- O
plementation O

learning B-DAT
architecture O
involves O
the O
embedder O
network O

learning, B-DAT
we O
pass O
sets O
of O
frames O

learning B-DAT
stage O
of O
our O
approach O
assumes O

its O
t-th O
frame. O
During O
the O
learning B-DAT
process, O
as O
well O
as O
during O

learning B-DAT
stage O
of O
our O
approach, O
the O

learning B-DAT
stage. O
In O
general, O
during O
meta-learning O

learning, B-DAT
only O
ψ O
are O
trained O
directly O

learning B-DAT
stage O

learning B-DAT
stage O
of O
our O
approach, O
the O

3.3. O
Few-shot O
learning B-DAT
by O
fine-tuning O

learning B-DAT
has O
converged, O
our O
system O
can O

learning B-DAT
stage. O
As O
before, O
the O
synthe O

learning B-DAT
stage O

learning B-DAT
stage. O
A O
straightforward O
way O
to O

learning B-DAT
with O
a O
single O
video O
sequence O

learning B-DAT
stage O
to O
initialize O
ψ′, O
i.e O

learning B-DAT
stage. O
The O
initialization O
of O
w O

learning B-DAT
stage O

learning B-DAT
stage. O
For O
the O
intiailization, O
we O

learning B-DAT
dataset). O
However, O
the O
match O
term O

learning B-DAT
process O
ensures O
the O
similarity O
between O

Once O
the O
new O
learning B-DAT
problem O
is O
set O
up, O
the O

follow O
directly O
from O
the O
meta- O
learning B-DAT
variants. O
Thus, O
the O
generator O
parameters O

learning B-DAT
stage O
is O
also O
crucial. O
As O

Adam O
[21]. O
We O
set O
the O
learning B-DAT
rate O
of O
the O
embedder O
and O

different O
datasets O
with O
multiple O
few-shot O
learning B-DAT
settings. O
Please O
re- O
fer O
to O

fine-tune O
all O
models O
on O
few-shot O
learning B-DAT
sets O
of O
size O
T O
for O

learning B-DAT
(or O
pretraining) O
stage. O
After O
the O

few-shot O
learning, B-DAT
the O
evaluation O
is O
performed O
on O

T O
used O
in O
few- O
shot O
learning B-DAT

we O
perform O
one- O
and O
few-shot O
learning B-DAT
on O
a O
video O
of O
a O

learning B-DAT
or O
pretraining. O
We O
set O
the O

the O
comparison O
of O
the O
few-shot O
learning B-DAT
timings. O
Both O
are O
provided O
in O

allow O
to O
trade O
off O
few-shot O
learning B-DAT
speed O
versus O
the O
results O
quality O

per- O
forms O
better O
for O
low-shot O
learning B-DAT
(e.g. O
one-shot), O
while O
the O
FT O

variant O
allows O
fast O
(real-time) O
few-shot O
learning B-DAT
of O
new O
avatars, O
fine-tuning O
ultimately O

learning B-DAT
of O
ad O

and O
S. O
Levine. O
Model-agnostic O
meta- O
learning B-DAT
for O
fast O
adaptation O
of O
deep O

Y. O
Wu, O
et O
al. O
Transfer O
learning B-DAT
from O
speaker O
verification O
to O
multispeaker O

I. O
Kemelmacher- O
Shlizerman. O
Synthesizing O
Obama: O
learning B-DAT
lip O
sync O
from O
au- O
dio O

and O
Y. O
Wang. O
Adversarial O
meta- O
learning B-DAT

An O
adversarial O
approach O
to O
few-shot O
learning B-DAT

Pix2pixHD O
and O
our O
method, O
few-shot O
learning B-DAT
was O
done O
via O
fine-tuning O
for O

Method O
(T) O
Time, O
s O
Few-shot O
learning B-DAT

2: O
Quantitative O
comparison O
of O
few-shot O
learning B-DAT
and O
inference O
timings O
for O
the O

learning B-DAT

multiple O
training O
frames O
in O
few-shot O
learning B-DAT
problems, O
like O
in O
our O
final O

learning B-DAT
configu- O
ration, O
which O
turned O
out O

learning, B-DAT
we O
randomly O
initialize O
the O
person-specific O

learning B-DAT
objective O
and O
initialize O
the O
embedding O

learning B-DAT
or O
pretraining. O
We O
used O
eight O

shot O
learning B-DAT
problem O
formulation. O
The O
notation O
for O

learning B-DAT
or O
pretraining. O
We O
used O
eight O

shot O
learning B-DAT
problem O
formulation. O
The O
notation O
for O

8 O
32 B-DAT

8 O
32 B-DAT

8 O
32 B-DAT

8 O
32 B-DAT

8 O
32 B-DAT

8 O
32 B-DAT

8 O
32 B-DAT

8 O
32 B-DAT

- B-DAT

- B-DAT

- B-DAT
tional O
neural O
networks O
to O
generate O

- B-DAT
ate O
a O
personalized O
talking O
head O

- B-DAT

- B-DAT

- B-DAT
ter O
that O
is O
able O
to O

- B-DAT
and O
one-shot O
learning O
of O
neural O

- B-DAT
sarial O
training O
problems O
with O
high O

- B-DAT

- B-DAT
alized O
photorealistic O
talking O
head O
models O

- B-DAT

- B-DAT
sions O
and O
mimics O
of O
a O

- B-DAT
ically, O
we O
consider O
the O
problem O

- B-DAT
alistic O
personalized O
head O
images O
given O

- B-DAT
marks, O
which O
drive O
the O
animation O

- B-DAT
conferencing O
and O
multi-player O
games, O
as O

- B-DAT
fects O
industry. O
Synthesizing O
realistic O
talking O

- B-DAT
ity. O
This O
complexity O
stems O
not O

- B-DAT
man O
visual O
system O
towards O
even O

- B-DAT
pearance O
modeling O
of O
human O
heads O

- B-DAT

- B-DAT
takes O
explains O
the O
current O
prevalence O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ferencing O
systems O

- B-DAT
posed O
to O
synthesize O
articulated O
head O

- B-DAT
chine O
learning O
(including O
deep O
learning O

- B-DAT

- B-DAT
age, O
the O
amount O
of O
motion O

- B-DAT

- B-DAT

- B-DAT
vNets) O
presents O
the O
new O
hope O

- B-DAT
ever, O
to O
succeed, O
such O
methods O

- B-DAT
lions O
of O
parameters O
for O
each O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
phisticated O
physical O
and O
optical O
modeling O

- B-DAT
cessive O
for O
most O
practical O
telepresence O

- B-DAT
els O
with O
as O
little O
effort O

- B-DAT

- B-DAT
shot O
learning) O
and O
with O
limited O

- B-DAT

- B-DAT
larly O
to O
[16, O
20, O
37 O

- B-DAT
yond O
the O
abilities O
of O
warping-based O

- B-DAT

- B-DAT
sive O
pre-training O
(meta-learning) O
on O
a O

- B-DAT
ing O
head O
videos O
corresponding O
to O

- B-DAT
verse O
appearance. O
In O
the O
course O

- B-DAT

- B-DAT
tem O
simulates O
few-shot O
learning O
tasks O

- B-DAT
form O
landmark O
positions O
into O
realistically-looking O

- B-DAT
alized O
photographs, O
given O
a O
small O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ficient O
realism O
and O
personalization O
fidelity O

- B-DAT
ing O
head O
models, O
including O
video O

- B-DAT
ing O
of O
the O
appearance O
of O

- B-DAT

- B-DAT

- B-DAT
tension O
of O
the O
face O
modeling O

- B-DAT
ability O
and O
higher O
complexity O
than O

- B-DAT
ple, O
the O
results O
of O
face O

- B-DAT
fledged O
talking O
head O
system O

- B-DAT
tecture O
uses O
adversarial O
training O
[12 O

- B-DAT
ing O
projection O
discriminators O
[32]. O
Our O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sifier, O
from O
which O
it O
can O

- B-DAT
fiers O
of O
unseen O
classes, O
given O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
GAN O
[43], O
adversarial O
meta-learning O
[41 O

- B-DAT
trained O
networks O
to O
generate O
additional O

- B-DAT

- B-DAT

- B-DAT
mance, O
our O
method O
deals O
with O

- B-DAT
ation O
models O
using O
similar O
adversarial O

- B-DAT
marize, O
we O
bring O
the O
adversarial O

- B-DAT

- B-DAT
learning O
framework. O
The O
former O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cation O
domain, O
the O
use O
of O

- B-DAT

- B-DAT
plementation O
details O

- B-DAT

- B-DAT
marks) O
to O
the O
embedding O
vectors O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
quence O
and O
with O
xi(t) O
its O

- B-DAT

- B-DAT
ity O
of O
the O
face O
landmarks O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

these O
inputs O
into O
an O
N O
- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
yi(t) O
for O
the O
video O

- B-DAT
sized O
video O
frame O
x̂i(t). O
The O

- B-DAT
imize O
the O
similarity O
between O
its O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trix O
P: O
ψ̂i O
= O
Pêi O

landmark O
image O
into O
an O
N O
- B-DAT

- B-DAT
criminator O
predicts O
a O
single O
scalar O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
rameters O
of O
all O
three O
networks O

- B-DAT

- B-DAT
ing O
(K O
= O
8 O
in O

- B-DAT
domly O
draw O
a O
training O
video O

- B-DAT
ditional O
K O
frames O
s1, O
s2 O

- B-DAT

- B-DAT
ding O
by O
simply O
averaging O
the O

- B-DAT

- B-DAT
tion O
x̂i(t) O
using O
the O
perceptual O

- B-DAT
responding O
to O
VGG19 O
[30] O
network O

- B-DAT
sentially O
is O
a O
perceptual O
similarity O

- B-DAT
respond O
to O
individual O
videos. O
The O

maps O
its O
inputs O
to O
anN O
- B-DAT

- B-DAT

- B-DAT
criminator. O
The O
match O
term O
LMCH(φ,W O

- B-DAT

- B-DAT
ters O
θ,W,w0, O
b O
of O
the O

- B-DAT
courages O
the O
increase O
of O
the O

- B-DAT
ample O
x̂i(t) O
and O
the O
real O

- B-DAT
nating O
updates O
of O
the O
embedder O

- B-DAT
imize O
the O
losses O
LCNT,LADV O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sis O
is O
conditioned O
on O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
timate O
the O
embedding O
for O
the O

- B-DAT

- B-DAT
sponding O
to O
new O
landmark O
images O

- B-DAT
erator O
using O
the O
estimated O
embedding O

- B-DAT
learned O
parameters O
ψ, O
as O
well O

- B-DAT
able O
identity O
gap O
that O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sult O
of O
the O
meta-learning O
stage O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tors O
computed O
by O
the O
embedder O

- B-DAT
tions O
of O
the O
fine-tuning O
stage O

- B-DAT
learning O
variants. O
Thus, O
the O
generator O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
son O
et. O
al. O
[19], O
but O

- B-DAT
malization O
[15] O
replaced O
by O
instance O

- B-DAT

- B-DAT
efficients O
of O
instance O
normalization O
layers O

- B-DAT

- B-DAT
ization O
layers O
in O
the O
downsampling O

- B-DAT
mark O
images O
yi(t O

- B-DAT
tional O
part O
of O
the O
discriminator O

- B-DAT
out O
normalization O
layers). O
The O
discriminator O

- B-DAT
pared O
to O
the O
embedder, O
has O

- B-DAT

- B-DAT
serted O
at O
32×32 O
spatial O
resolution O

- B-DAT
tween O
activations O
of O
Conv1,6,11,20,29 O
VGG19 O

- B-DAT
nally, O
for O
LMCH O
we O
set O

- B-DAT
tional O
layers O
to O
64 O
and O

- B-DAT
tor O
has O
38 O
million O
parameters O

- B-DAT
titative O
and O
qualitative O
evaluation: O
VoxCeleb1 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
fer O
to O
the O
text O
for O

- B-DAT

- B-DAT

- B-DAT
son O
not O
seen O
during O
meta-learning O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
reenactment O
scenario). O
For O
the O
evaluation O

- B-DAT
out O
frames O
for O
each O
of O

- B-DAT

- B-DAT

- B-DAT
realism O
and O
identity O
preservation O
of O

- B-DAT

- B-DAT

- B-DAT
bedding O
vectors O
of O
the O
state-of-the-art O

- B-DAT
work O
[9] O
for O
measuring O
identity O

- B-DAT
tual O
similarity O
and O
realism O
of O

- B-DAT
man O
respondents. O
We O
show O
people O

- B-DAT

- B-DAT
not O
spot O
fakes O
based O
on O

- B-DAT

- B-DAT

- B-DAT
Celeb1 O
dataset). O
For O
Pix2pixHD, O
we O

- B-DAT

- B-DAT
shot O
learning. O
X2Face, O
as O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
which O
arguably O
gives O
X2Face O

- B-DAT
lines O
in O
three O
different O
setups O

- B-DAT

- B-DAT

- B-DAT
dom O
from O
the O
other O
video O

- B-DAT

- B-DAT

- B-DAT
perform O
our O
method O
on O
the O

- B-DAT
imizes O
only O
perceptual O
metric, O
without O

- B-DAT
and O
few-shot O
learning O
on O
a O

- B-DAT

- B-DAT
ter O
correlates O
with O
visual O
quality O

- B-DAT
ble O
1-Top O
with O
the O
results O

- B-DAT
alism O
and O
personalization O
degree O
achieved O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
out O
fine-tuning O
(by O
simply O
predicting O

- B-DAT
ant O
is O
trained O
for O
half O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
sults, O
where O
animation O
is O
driven O

- B-DAT
ent O
video O
of O
the O
same O

- B-DAT
tary O
material O
and O
in O
Figure O

- B-DAT
ble O
1-Bottom) O
and O
the O
visual O

- B-DAT
forms O
better O
for O
low-shot O
learning O

- B-DAT

- B-DAT
ial O
fine-tuning O

- B-DAT

- B-DAT
sons O
with O
similar O
geometry O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tographs O
in O
the O
source O
column O

- B-DAT

- B-DAT
realistic O
virtual O
talking O
heads O
in O

- B-DAT
ization O
score O
in O
our O
user O

- B-DAT
ics O
representation O
(in O
particular, O
the O

- B-DAT
son O
leads O
to O
a O
noticeable O

- B-DAT
ing O
a O
different O
person O
and O

- B-DAT
proach O
already O
provides O
a O
high-realism O

- B-DAT
puter O
Graphics O
and O
Applications, O
30(4):20–31 O

- B-DAT
sarial O
networks. O
In O
Artificial O
Neural O

Networks O
and O
Machine O
Learning O
- B-DAT
ICANN, O
pages O
594–603, O
2018. O
2 O

- B-DAT

- B-DAT

- B-DAT
thesis O
of O
3d O
faces. O
In O

- B-DAT
29, O
2017, O
pages O
1021–1030, O
2017 O

- B-DAT

- B-DAT
learning O
for O
fast O
adaptation O
of O

- B-DAT
ulation. O
In O
European O
Conference O
on O

- B-DAT

- B-DAT

- B-DAT
erative O
adversarial O
nets. O
In O
Advances O

- B-DAT

- B-DAT
wanathan, O
and O
R. O
Garnett, O
editors O

- B-DAT
formation O
Processing O
Systems O
30, O
pages O

- B-DAT
time O
with O
adaptive O
instance O
normalization O

- B-DAT
ternational O
Conference O
on O
Machine O
Learning O

- B-DAT

- B-DAT

- B-DAT
shick, O
S. O
Guadarrama, O
and O
T O

- B-DAT
tional O
architecture O
for O
fast O
feature O

- B-DAT

- B-DAT
speech O
synthesis. O
In O
Proc. O
NIPS O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
SPEECH, O
2017. O
5 O

- B-DAT
gios, O
and O
I. O
Kokkinos. O
Deforming O

- B-DAT
vised O
disentangling O
of O
shape O
and O

- B-DAT
ropean O
Conference O
on O
Computer O
Vision O

- B-DAT

- B-DAT
Shlizerman. O
Synthesizing O
Obama: O
learning O
lip O

- B-DAT
dio. O
ACM O
Transactions O
on O
Graphics O

- B-DAT
tral O
normalization O
for O
generative O
adversarial O

- B-DAT

- B-DAT
erator O
architecture O
for O
generative O
adversarial O

- B-DAT

- B-DAT
actment O
of O
RGB O
videos. O
In O

- B-DAT
ference O
on O
Computer O
Vision O
and O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
2018. O
4, O
6 O

- B-DAT
learning. O
CoRR, O
abs/1806.03316, O
2018. O
2 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ried O
out O
on O
a O
single O

- B-DAT

- B-DAT

- B-DAT
surement O
was O
averaged O
over O
100 O

- B-DAT

- B-DAT

- B-DAT
ing O
personalization O
fidelity O
and O
realism O

- B-DAT

- B-DAT

- B-DAT
duction O
of O
a O
training O
scheduler O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
vided O
by O
the O
embedder O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
specific O
initialization O
of O
the O
discriminator O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ration, O
which O
turned O
out O
to O

- B-DAT

- B-DAT

- B-DAT
tice O
that O
the O
results O
for O

- B-DAT
sonalization O
fidelity. O
We, O
therefore, O
came O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

shot B-DAT
learning I-DAT
of O
neural O
talking O
head O
models O

handful O
of O
photographs O
(so-called O
few- O
shot B-DAT
learning) I-DAT
and O
with O
limited O
training O
time O

shot B-DAT
learning), I-DAT
while O
adding O
a O
few O
more O

shot B-DAT
learning I-DAT
ability O
is O
obtained O
through O
exten O

shot B-DAT
learning I-DAT
tasks O
and O
learns O
to O
trans O

shot B-DAT
learning I-DAT
of O
generative O
models) O
and O
some O

shot B-DAT
learning I-DAT
by O
fine-tuning O

shot B-DAT
learning I-DAT
settings. O
Please O
re- O
fer O
to O

shot B-DAT
learning I-DAT
sets O
of O
size O
T O
for O

shot B-DAT
learning, I-DAT
the O
evaluation O
is O
performed O
on O

frames O
T O
used O
in O
few- O
shot B-DAT
learning I-DAT

shot B-DAT
learning I-DAT
on O
a O
video O
of O
a O

shot B-DAT
learning I-DAT
timings. O
Both O
are O
provided O
in O

shot B-DAT
learning I-DAT
speed O
versus O
the O
results O
quality O

shot B-DAT
learning I-DAT
(e.g. O
one-shot), O
while O
the O
FT O

shot B-DAT
learning I-DAT
of O
new O
avatars, O
fine-tuning O
ultimately O

shot B-DAT
learning I-DAT

shot B-DAT
learning I-DAT
was O
done O
via O
fine-tuning O
for O

shot B-DAT
learning I-DAT

shot B-DAT
learning I-DAT
and O
inference O
timings O
for O
the O

shot B-DAT
learning I-DAT
problems, O
like O
in O
our O
final O

or O
pretraining. O
We O
used O
eight O
shot B-DAT
learning I-DAT
problem O
formulation. O
The O
notation O
for O

or O
pretraining. O
We O
used O
eight O
shot B-DAT
learning I-DAT
problem O
formulation. O
The O
notation O
for O

3D O
scans O
[4], O
or O
by O
learning B-DAT
3DMM O
parameters O
directly O
from O
RGB O

accordingly. O
This O
is O
done O
by O
learning B-DAT
a O
forward O
mapping O
fp→v O
from O

improving O
the O
generated O
results. O
As O
learning B-DAT
the O
function O
fa→v O
: O
R O

with O
L1 O
loss, O
and O
a O
learning B-DAT
rate O
of O
0.001. O
The O
learning O

phase O
is O
started O
with O
a O
learning B-DAT
rate O
of O
0.0001. O
Testing. O
The O

Abbeel, O
P.: O
Infogan: O
Interpretable O
representation O
learning B-DAT
by O
information O
maximizing O
generative O
adversarial O

Denton, O
E.L., O
Birodkar, O
V.: O
Unsupervised O
learning B-DAT
of O
disentangled O
repre- O
sentations O
from O

facial O
animation O
by O
joint O
end-to-end O
learning B-DAT
of O
pose O
and O
emotion. O
ACM O

King, O
D.E.: O
Dlib-ml: O
A O
machine O
learning B-DAT
toolkit. O
The O
Journal O
of O
Machine O

tion O
of O
unconstrained O
faces O
by O
learning B-DAT
efficient O
H-CNN O
regressors. O
In: O
Proc O

S.M., O
Kemelmacher-Shlizerman, O
I.: O
Synthesizing O
Obama: O
learning B-DAT
lip O
sync O
from O
audio. O
ACM O

X., O
Liu, O
X.: O
Disentangled O
representation O
learning B-DAT
gan O
for O
pose-invariant O
face O
recognition O

- B-DAT
phisticated O
video O
and O
image O
editing O

- B-DAT

- B-DAT
pared O
to O
state-of-the-art O
self-supervised/supervised O
methods O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ing O
frame, O
audio O
data, O
or O

- B-DAT

- B-DAT
ing O
frames. O
These O
frames O
are O

- B-DAT

- B-DAT
tions. O
First, O
we O
propose O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion, O
described O
in O
Section O
6 O

- B-DAT
imation O
(or O
puppeteering) O
given O
one O

- B-DAT
erature O
on O
supervised/self-supervised O
approaches; O
here O

- B-DAT
marks O
[5, O
12, O
21, O
30 O

- B-DAT

- B-DAT
tors O
of O
variation O
(e.g. O
optical O

- B-DAT

- B-DAT
form O
images O
of O
one O
domain O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
work. O
This O
is O
illustrated O
in O

- B-DAT
termine O
how O
to O
map O
from O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
straints O
based O
on O
the O
identity O

- B-DAT
quently, O
we O
introduce O
additional O
loss O

- B-DAT

-5 B-DAT
and O
Conv7 O
layers O
(i.e. O
layers O

-7 B-DAT
layers O
(i.e. O
layers O
encoding O
higher O

- B-DAT

- B-DAT

- B-DAT
figuration O
A) O
[37] O
trained O
on O

- B-DAT

- B-DAT

- B-DAT
tional O
mapping O
fv→p O
is O
needed O

- B-DAT
nected O
layer O
with O
bias O
and O

- B-DAT
connected O
linear O
layer O
with O
bias O

- B-DAT

- B-DAT

- B-DAT
bedding O
learns O
to O
encode O
some O

- B-DAT
tion. O
Given O
driving O
audio O
features O

- B-DAT

- B-DAT
size O
of O
16. O
First, O
it O

- B-DAT

- B-DAT
ing/testing O
setups. O
Lower O
is O
better O

- B-DAT
centage O
improvement O
over O
the O
L1 O

- B-DAT
egy O
and O
using O
additional O
views O

- B-DAT
main O
(in O
this O
case O
a O

- B-DAT
cleGAN O
is O
trained O
on O
pairs O

- B-DAT
ing O
unrealistic O
results. O
Additionally, O
our O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tion O
4.1), O
the O
25, O
993 O

- B-DAT
neutral O
expressions O
in O
the O
source O

- B-DAT
processing O
(X2Face O
+ O
p.-p.) O
can O

- B-DAT

- B-DAT

- B-DAT
mentary O
material. O
Whilst O
some O
artefacts O

- B-DAT

- B-DAT
eration O
using O
another O
face. O
This O

- B-DAT
constrained O
settings O
(e.g. O
an O
unseen O

- B-DAT

- B-DAT
tioned O
on O
other O
modalities, O
the O

- B-DAT
teresting O
avenue O
of O
research: O
how O

- B-DAT

- B-DAT
eration O
quality O
of O
these O
methods O

- B-DAT

- B-DAT
tions/comments. O
This O
work O
was O
funded O

- B-DAT

- B-DAT

-4 B-DAT

- B-DAT

- B-DAT
ment O
networks. O
In: O
Proc. O
ICCV O

- B-DAT

- B-DAT

- B-DAT
sentations O
from O
video. O
In: O
NIPS O

- B-DAT
tional O
neural O
networks. O
In: O
Proc O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
actions O
on O
Graphics O
(TOG) O
(2017 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lutional O
neural O
networks. O
In: O
Proc O

- B-DAT
tional O
inverse O
graphics O
network. O
In O

- B-DAT
tion O
of O
unconstrained O
faces O
by O

- B-DAT

- B-DAT

- B-DAT
tation, O
face O
swapping, O
and O
face O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
strained O
photo O
collections. O
In: O
Proc O

- B-DAT

- B-DAT

- B-DAT
scale O
image O
recognition. O
In: O
International O

- B-DAT
sentations O
(2015 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pretable O
transformations O
with O
encoder-decoder O
networks O

- B-DAT

- B-DAT

- B-DAT
lation O
using O
cycle-consistent O
adversarial O
networks O

also O
assessed O
Jpred4 O
on O
the O
2019_test B-DAT
set O
(see O
Table O

predictors O
and O
Jpred4 O
assessed O
on O
2019_test B-DAT
set O
of O
618 O
proteins O

8-states O
Training O
Set O
2017_test O
Set O
2019_test B-DAT
Set O

composition O
of O
Training, O
2017_test O
and O
2019_test B-DAT

second O
smaller O
independent O
test O
set O
(2019_test) B-DAT
based O

an O
additional O
independent O
test O
set O
(2019_test) B-DAT
to O
fairly O
compare O
Porter O
5 O

proteins, O
comprising O
91,375 O
amino O
acids O
(2019_test B-DAT

predictors O
and O
Jpred4 O
assessed O
on O
2019_test B-DAT
set O
of O
618 O
proteins O

composition O
of O
Training, O
2017_test O
and O
2019_test B-DAT

also O
assessed O
Jpred4 O
on O
the O
2019_test B-DAT
set I-DAT
(see O
Table O

predictors O
and O
Jpred4 O
assessed O
on O
2019_test B-DAT
set I-DAT
of O
618 O
proteins O

predictors O
and O
Jpred4 O
assessed O
on O
2019_test B-DAT
set I-DAT
of O
618 O
proteins O

a O
large O
set B-DAT
of O
protein O
structures. O
Porter O
5 O

classes O
on O
a O
large O
independent O
set B-DAT

ical O
limits O
of O
prediction O
– O
set B-DAT
at O
88–90% O
accuracy O
per O
AA O

validation O
experiments O
on O
the O
training O
set B-DAT

adopting O
a O
considerably O
larger O
training O
set B-DAT
but O
without O
evolutionary O
information, O
we O

ensemble O
trained O
on O
a O
smaller O
set34 B-DAT

NN O
architectures O
on O
the O
validation O
set B-DAT

proteins O
in O
our O
case), O
the O
set B-DAT
of O
hyperparameters O
selected O
for O
the O

preliminary O
testing O
by O
splitting O
our O
set B-DAT
into O
1/5 O
for O
testing O
and O

above) O
on O
the O
full O
training O
set B-DAT
rather O
than O
on O
individual O
training O

models O
on O
a O
completely O
independent O
set B-DAT
(see O
“2017_test” O
in O
Methods:Datasets) O
con O

Full O
set B-DAT
(Porter O
5) O
83.42% O
83.39% O
83.49 O

3. O
Assessment O
on O
the O
2017_test O
set B-DAT
of O
three-state O
ensembles O
trained O
on O

full O
set B-DAT

on O
4/5 O
of O
the O
training O
set B-DAT
on O
PSI-BLAST, O
HHblits O
and O
concatenated O

trained O
on O
the O
full O
training O
set B-DAT
achieves O
73.02% O
Q8 O
accuracy O
on O

set B-DAT
described O
in O
Methods:Dataset O
(see O
also O

multiple O
predictors O
on O
independent O
test O
set B-DAT

on O
the O
2017_test O
set B-DAT
we O
created, O
containing O
3,154 O
proteins O

5); O
the O
full O
set B-DAT
of O
3,154 O
proteins O
(Table O

our O
tests O
on O
the O
2017_test O
set B-DAT
with O
3-class O
accu- O
racy O
of O

the O
smaller O
version O
of O
the O
set B-DAT
and O
84.2% O
on O
the O
larger O

AA O
on O
the O
full O
test O
set B-DAT

Performances O
on O
the O
smaller O
2017_test O
set B-DAT
for O
which O
Spider3 O
generates O
predictions O

both O
versions O
of O
the O
2017_test O
set B-DAT

of O
the O
set, B-DAT
respectively. O
Porter O
5 O
is O
also O

5 O
performances O
on O
the O
CASP1328 O
set, B-DAT
and O
on O
6 O
months O
of O

we O
obtained O
on O
the O
2017_test O
set B-DAT

only O
section O
of O
the O
2017_test O
set, B-DAT
which O
is O
roughly O
90% O
of O

is O
selected O
for O
the O
training O
set, B-DAT
while O
a O
further O
150 O
proteins O

not O
included O
in O
the O
training O
set B-DAT
are O
used O
as O
a O
blind O

test O
set B-DAT

trained O
on O
the O
1,348 O
JPred4 O
set B-DAT

set B-DAT

same O
sets. B-DAT
While O
the O
testing O
set O
is O
small O
(150 O
proteins), O
these O

Q3 O
when O
trained O
on O
a O
set B-DAT
which O
is O

smaller O
than O
its O
original O
training O
set B-DAT

assessed O
Jpred4 O
on O
the O
2019_test O
set B-DAT
(see O
Table O

8 O
and O
description O
of O
the O
set B-DAT
in O
the O
following O
section O

predictors. O
In O
a O
separate O
test O
set B-DAT
we O
assessed O
some O
very O
recent O

more O
recent O
than O
our O
2017_test O
set, B-DAT
i.e. O
MUFOLD-SS43, O
NetSurfP-2.048 O
and O
SPOT-1D44 O

generated O
a O
second O
independent O
test O
set B-DAT
(also O
see O
Methods:Datasets) O
starting O
from O

not O
access O
the O
MUFOLD-SS O
training O
set B-DAT

861 O
original O
proteins O
in O
the O
set, B-DAT
Table O

quality O
of O
our O
large O
training O
set B-DAT
(see O
Table O

the O
experiments O
on O
a O
training O
set B-DAT
twice O
as O
large O
as O
the O

of O
a O
larger, O
well-distributed O
training O
set B-DAT

and O
Jpred4 O
assessed O
on O
2019_test O
set B-DAT
of O
618 O
proteins O

on O
a O
first O
independent O
test O
set B-DAT
(2017_test), O
along O
with O
some O
of O

reduction O
of O
our O
final O
test O
set B-DAT
against O
their O
training O
sets35 O

overlapped O
with O
our O
original O
test O
set, B-DAT
we O
generated O
a O
second O
smaller O

independent O
test O
set B-DAT
(2019_test) O
based O

Specifically, O
we O
built O
our O
training O
set B-DAT
from O
the O
PDB O
released O
on O

also O
built O
an O
independent O
test O
set B-DAT
(2017_test) O
from O
the O
PDB O
released O

14, O
2017. O
We O
redundancy-reduced O
this O
set B-DAT
at O
a O
25% O
identity O
threshold O

training O
set B-DAT

internally O
redundancy O
reduced O
the O
resulting O
set B-DAT
at O
a O
25% O
identity O
threshold O

from O
both O
sets. B-DAT
The O
training O
set O
contains O

In O
different O
tests O
the O
training O
set B-DAT
is O
used O
as O
a O
whole O

for O
hyperparameter O
optimization35. O
The O
2017_test O
set B-DAT
is O
only O
used O

curated O
an O
additional O
independent O
test O
set B-DAT
(2019_test) O
to O
fairly O
compare O
Porter O

overlapping O
with O
our O
2017_test O
set B-DAT

2019. O
We O
then O
redundancy-reduced O
this O
set B-DAT

SPOT-1D, O
NetSurfP-2.0 O
and O
our O
training O
set B-DAT
at O
25% O
identity O
threshold. O
Finally O

the O
internal O
redundancy O
of O
this O
set B-DAT
at O
a O
25% O
sequence O
identity O

The O
simple O
idea O
is O
to O
set B-DAT
to O
1 O
the O
position O

The O
number O
of O
hyperparameters O
to O
set B-DAT
in O
a O
BRNN, O
and O
the O

stochastic O
gradient O
descent O
(SGD)65. O
We O
set B-DAT
momentum O
to O
0.9 O
and O
divided O

cross O
entropy O
error O
on O
training O
set B-DAT
had O
not O
decreased O
for O
100 O

epochs. O
The O
train- O
ing O
set B-DAT
is O
shuffled O
at O
the O
end O

size O
of O
a O
mini-batch O
is O
set B-DAT
to O
~10 O
proteins, O
that O
is O

multiple O
predictors O
on O
independent O
test O
set B-DAT

NN O
architectures O
on O
the O
validation O
set B-DAT

3 O
Assessment O
on O
the O
2017_test O
set B-DAT
of O
three-state O
ensembles O
trained O
on O

either O
five-fold O
cross-validation O
or O
full O
set B-DAT

AA O
on O
the O
full O
test O
set B-DAT

Performances O
on O
the O
smaller O
2017_test O
set B-DAT
for O
which O
Spider3 O
generates O
predictions O

and O
Jpred4 O
assessed O
on O
2019_test O
set B-DAT
of O
618 O
proteins O

Table O
3. O
Assessment O
on O
the O
2017_test B-DAT
set I-DAT
of O
three-state O
ensembles O
trained O
on O

on O
the O
2017_test B-DAT
set I-DAT
we O
created, O
containing O
3,154 O
proteins O

in O
our O
tests O
on O
the O
2017_test B-DAT
set I-DAT
with O
3-class O
accu- O
racy O
of O

5. O
Performances O
on O
the O
smaller O
2017_test B-DAT
set I-DAT
for O
which O
Spider3 O
generates O
predictions O

on O
both O
versions O
of O
the O
2017_test B-DAT
set I-DAT

results O
we O
obtained O
on O
the O
2017_test B-DAT
set I-DAT

X-ray O
only O
section O
of O
the O
2017_test B-DAT
set, I-DAT
which O
is O
roughly O
90% O
of O

sets O
more O
recent O
than O
our O
2017_test B-DAT
set, I-DAT
i.e. O
MUFOLD-SS43, O
NetSurfP-2.048 O
and O
SPOT-1D44 O

cross-validation O
for O
hyperparameter O
optimization35. O
The O
2017_test B-DAT
set I-DAT
is O
only O
used O

overlapping O
with O
our O
2017_test B-DAT
set I-DAT

Table O
3 O
Assessment O
on O
the O
2017_test B-DAT
set I-DAT
of O
three-state O
ensembles O
trained O
on O

5 O
Performances O
on O
the O
smaller O
2017_test B-DAT
set I-DAT
for O
which O
Spider3 O
generates O
predictions O

a O
large O
set B-DAT
of O
protein O
structures. O
Porter O
5 O

classes O
on O
a O
large O
independent O
set B-DAT

ical O
limits O
of O
prediction O
– O
set B-DAT
at O
88–90% O
accuracy O
per O
AA O

validation O
experiments O
on O
the O
training O
set B-DAT

adopting O
a O
considerably O
larger O
training O
set B-DAT
but O
without O
evolutionary O
information, O
we O

ensemble O
trained O
on O
a O
smaller O
set34 B-DAT

NN O
architectures O
on O
the O
validation O
set B-DAT

proteins O
in O
our O
case), O
the O
set B-DAT
of O
hyperparameters O
selected O
for O
the O

preliminary O
testing O
by O
splitting O
our O
set B-DAT
into O
1/5 O
for O
testing O
and O

above) O
on O
the O
full O
training O
set B-DAT
rather O
than O
on O
individual O
training O

models O
on O
a O
completely O
independent O
set B-DAT
(see O
“2017_test” O
in O
Methods:Datasets) O
con O

Full O
set B-DAT
(Porter O
5) O
83.42% O
83.39% O
83.49 O

3. O
Assessment O
on O
the O
2017_test O
set B-DAT
of O
three-state O
ensembles O
trained O
on O

full O
set B-DAT

on O
4/5 O
of O
the O
training O
set B-DAT
on O
PSI-BLAST, O
HHblits O
and O
concatenated O

trained O
on O
the O
full O
training O
set B-DAT
achieves O
73.02% O
Q8 O
accuracy O
on O

set B-DAT
described O
in O
Methods:Dataset O
(see O
also O

multiple O
predictors O
on O
independent O
test O
set B-DAT

on O
the O
2017_test O
set B-DAT
we O
created, O
containing O
3,154 O
proteins O

5); O
the O
full O
set B-DAT
of O
3,154 O
proteins O
(Table O

our O
tests O
on O
the O
2017_test O
set B-DAT
with O
3-class O
accu- O
racy O
of O

the O
smaller O
version O
of O
the O
set B-DAT
and O
84.2% O
on O
the O
larger O

AA O
on O
the O
full O
test O
set B-DAT

Performances O
on O
the O
smaller O
2017_test O
set B-DAT
for O
which O
Spider3 O
generates O
predictions O

both O
versions O
of O
the O
2017_test O
set B-DAT

of O
the O
set, B-DAT
respectively. O
Porter O
5 O
is O
also O

5 O
performances O
on O
the O
CASP1328 O
set, B-DAT
and O
on O
6 O
months O
of O

we O
obtained O
on O
the O
2017_test O
set B-DAT

only O
section O
of O
the O
2017_test O
set, B-DAT
which O
is O
roughly O
90% O
of O

is O
selected O
for O
the O
training O
set, B-DAT
while O
a O
further O
150 O
proteins O

not O
included O
in O
the O
training O
set B-DAT
are O
used O
as O
a O
blind O

test O
set B-DAT

trained O
on O
the O
1,348 O
JPred4 O
set B-DAT

set B-DAT

same O
sets. B-DAT
While O
the O
testing O
set O
is O
small O
(150 O
proteins), O
these O

Q3 O
when O
trained O
on O
a O
set B-DAT
which O
is O

smaller O
than O
its O
original O
training O
set B-DAT

assessed O
Jpred4 O
on O
the O
2019_test O
set B-DAT
(see O
Table O

8 O
and O
description O
of O
the O
set B-DAT
in O
the O
following O
section O

predictors. O
In O
a O
separate O
test O
set B-DAT
we O
assessed O
some O
very O
recent O

more O
recent O
than O
our O
2017_test O
set, B-DAT
i.e. O
MUFOLD-SS43, O
NetSurfP-2.048 O
and O
SPOT-1D44 O

generated O
a O
second O
independent O
test O
set B-DAT
(also O
see O
Methods:Datasets) O
starting O
from O

not O
access O
the O
MUFOLD-SS O
training O
set B-DAT

861 O
original O
proteins O
in O
the O
set, B-DAT
Table O

quality O
of O
our O
large O
training O
set B-DAT
(see O
Table O

the O
experiments O
on O
a O
training O
set B-DAT
twice O
as O
large O
as O
the O

of O
a O
larger, O
well-distributed O
training O
set B-DAT

and O
Jpred4 O
assessed O
on O
2019_test O
set B-DAT
of O
618 O
proteins O

on O
a O
first O
independent O
test O
set B-DAT
(2017_test), O
along O
with O
some O
of O

reduction O
of O
our O
final O
test O
set B-DAT
against O
their O
training O
sets35 O

overlapped O
with O
our O
original O
test O
set, B-DAT
we O
generated O
a O
second O
smaller O

independent O
test O
set B-DAT
(2019_test) O
based O

Specifically, O
we O
built O
our O
training O
set B-DAT
from O
the O
PDB O
released O
on O

also O
built O
an O
independent O
test O
set B-DAT
(2017_test) O
from O
the O
PDB O
released O

14, O
2017. O
We O
redundancy-reduced O
this O
set B-DAT
at O
a O
25% O
identity O
threshold O

training O
set B-DAT

internally O
redundancy O
reduced O
the O
resulting O
set B-DAT
at O
a O
25% O
identity O
threshold O

from O
both O
sets. B-DAT
The O
training O
set O
contains O

In O
different O
tests O
the O
training O
set B-DAT
is O
used O
as O
a O
whole O

for O
hyperparameter O
optimization35. O
The O
2017_test O
set B-DAT
is O
only O
used O

curated O
an O
additional O
independent O
test O
set B-DAT
(2019_test) O
to O
fairly O
compare O
Porter O

overlapping O
with O
our O
2017_test O
set B-DAT

2019. O
We O
then O
redundancy-reduced O
this O
set B-DAT

SPOT-1D, O
NetSurfP-2.0 O
and O
our O
training O
set B-DAT
at O
25% O
identity O
threshold. O
Finally O

the O
internal O
redundancy O
of O
this O
set B-DAT
at O
a O
25% O
sequence O
identity O

The O
simple O
idea O
is O
to O
set B-DAT
to O
1 O
the O
position O

The O
number O
of O
hyperparameters O
to O
set B-DAT
in O
a O
BRNN, O
and O
the O

stochastic O
gradient O
descent O
(SGD)65. O
We O
set B-DAT
momentum O
to O
0.9 O
and O
divided O

cross O
entropy O
error O
on O
training O
set B-DAT
had O
not O
decreased O
for O
100 O

epochs. O
The O
train- O
ing O
set B-DAT
is O
shuffled O
at O
the O
end O

size O
of O
a O
mini-batch O
is O
set B-DAT
to O
~10 O
proteins, O
that O
is O

multiple O
predictors O
on O
independent O
test O
set B-DAT

NN O
architectures O
on O
the O
validation O
set B-DAT

3 O
Assessment O
on O
the O
2017_test O
set B-DAT
of O
three-state O
ensembles O
trained O
on O

either O
five-fold O
cross-validation O
or O
full O
set B-DAT

AA O
on O
the O
full O
test O
set B-DAT

Performances O
on O
the O
smaller O
2017_test O
set B-DAT
for O
which O
Spider3 O
generates O
predictions O

and O
Jpred4 O
assessed O
on O
2019_test O
set B-DAT
of O
618 O
proteins O

2017_test B-DAT

Table O
3. O
Assessment O
on O
the O
2017_test B-DAT
set O
of O
three-state O
ensembles O
trained O

73.02% O
Q8 O
accuracy O
on O
the O
2017_test B-DAT

on O
the O
2017_test B-DAT
set O
we O
created, O
containing O
3,154 O

in O
our O
tests O
on O
the O
2017_test B-DAT
set O
with O
3-class O
accu- O
racy O

5. O
Performances O
on O
the O
smaller O
2017_test B-DAT
set O
for O
which O
Spider3 O
generates O

on O
both O
versions O
of O
the O
2017_test B-DAT
set. O
Porter O
5 O
is O
consistently O

results O
we O
obtained O
on O
the O
2017_test B-DAT
set O

X-ray O
only O
section O
of O
the O
2017_test B-DAT
set, O
which O
is O
roughly O
90 O

sets O
more O
recent O
than O
our O
2017_test B-DAT
set, O
i.e. O
MUFOLD-SS43, O
NetSurfP-2.048 O
and O

3-states O
8-states O
Training O
Set O
2017_test B-DAT
Set O
2019_test O
Set O

of O
AA O
composition O
of O
Training, O
2017_test B-DAT
and O
2019_test O

a O
first O
independent O
test O
set O
(2017_test), B-DAT
along O
with O
some O
of O
the O

built O
an O
independent O
test O
set O
(2017_test) B-DAT
from O
the O
PDB O
released O
after O

15,753 O
proteins O
(3,797,426 O
AA) O
and O
2017_test B-DAT
3,154 O
proteins O
(651,594 O
AA), O
among O

cross-validation O
for O
hyperparameter O
optimization35. O
The O
2017_test B-DAT
set O
is O
only O
used O

overlapping O
with O
our O
2017_test B-DAT
set. O
We O
removed O
any O
protein O

Table O
3 O
Assessment O
on O
the O
2017_test B-DAT
set O
of O
three-state O
ensembles O
trained O

5 O
Performances O
on O
the O
smaller O
2017_test B-DAT
set O
for O
which O
Spider3 O
generates O

of O
AA O
composition O
of O
Training, O
2017_test B-DAT
and O
2019_test O

of O
6125 O
proteins, O
(2) O
CB513 B-DAT
of O
513 O

very O
good O
accuracy O
on O
CullPDB, O
CB513 B-DAT
and O
CASP10, O
but O
not O
on O

similar O
performance O
trend O
on O
the O
CB513 B-DAT
test O
set O
(see O
Table O
5 O

five O
tested O
methods O
on O
the O
CB513 B-DAT
and O

4. O
The O
Q8 O
accuracy O
on O
CB513 B-DAT
by O
the O
models O
of O
different O

Figure O
5. O
Q3 O
accuracy O
on O
CB513 B-DAT
and O
two O
CASP O
(CASP10-11) O
test O

methods O
on O
5 O
datasets: O
CullPDB, O
CB513, B-DAT
CASP10, O
CASP11 O
and O
CAMEO. O
The O

CullPDB O
CB513 B-DAT
CASP10 O
CASP11 O
CAMEO O

methods O
on O
5 O
datasets: O
CullPDB, O
CB513, B-DAT
CASP10, O
CASP11 O
and O
CAMEO O

CullPDB O
CB513 B-DAT
CASP10 O
CASP11 O
CAMEO O

methods O
on O
5 O
datasets: O
CullPDB, O
CB513, B-DAT
CASP10, O
CASP11 O
and O
CAMEO O

CullPDB O
CB513 B-DAT
CASP10 O
CASP11 O
CAMEO O

DeepCNF O
and O
ICML2014 O
on O
the O
CB513 B-DAT
dataset O

4. O
The O
Q8 O
accuracy O
on O
CB513 B-DAT
by O
the O
models O
of O
different O

Figure O
5. O
Q3 O
accuracy O
on O
CB513 B-DAT
and O
two O
CASP O
(CASP10-11) O
test O

of O
6125 O
proteins, O
(2) O
CB513 B-DAT
of O
513 O

very O
good O
accuracy O
on O
CullPDB, O
CB513 B-DAT
and O
CASP10, O
but O
not O
on O

similar O
performance O
trend O
on O
the O
CB513 B-DAT
test O
set O
(see O
Table O
5 O

five O
tested O
methods O
on O
the O
CB513 B-DAT
and O

4. O
The O
Q8 O
accuracy O
on O
CB513 B-DAT
by O
the O
models O
of O
different O

Figure O
5. O
Q3 O
accuracy O
on O
CB513 B-DAT
and O
two O
CASP O
(CASP10-11) O
test O

methods O
on O
5 O
datasets: O
CullPDB, O
CB513, B-DAT
CASP10, O
CASP11 O
and O
CAMEO. O
The O

CullPDB O
CB513 B-DAT
CASP10 O
CASP11 O
CAMEO O

methods O
on O
5 O
datasets: O
CullPDB, O
CB513, B-DAT
CASP10, O
CASP11 O
and O
CAMEO O

CullPDB O
CB513 B-DAT
CASP10 O
CASP11 O
CAMEO O

methods O
on O
5 O
datasets: O
CullPDB, O
CB513, B-DAT
CASP10, O
CASP11 O
and O
CAMEO O

CullPDB O
CB513 B-DAT
CASP10 O
CASP11 O
CAMEO O

DeepCNF O
and O
ICML2014 O
on O
the O
CB513 B-DAT
dataset O

4. O
The O
Q8 O
accuracy O
on O
CB513 B-DAT
by O
the O
models O
of O
different O

Figure O
5. O
Q3 O
accuracy O
on O
CB513 B-DAT
and O
two O
CASP O
(CASP10-11) O
test O

set O
are O
used O
as O
a O
blind B-DAT
test O
set. O
The O
first O
version O

correct O
prediction O
on O
the O
JPred4 O
blind B-DAT

We O
also O
assessed O
Jpred4 B-DAT
on O
the O
2019_test O
set O
(see O

classes O
recast O
to O
match O
the O
Jpred4 B-DAT
class O
assignment. O
In O
this O
case O

show O
Q3 O
3.7–4.7% O
higher O
than O
Jpred4 B-DAT
and O
similar O
improvements O
in O
SOV O

Jpred4 B-DAT
77.38% O
72.29% O
64.96% O
N.A. O
N.A O

8. O
Most O
recent O
predictors O
and O
Jpred4 B-DAT
assessed O
on O
2019_test O
set O
of O

8 O
Most O
recent O
predictors O
and O
Jpred4 B-DAT
assessed O
on O
2019_test O
set O
of O

a O
large O
set B-DAT
of O
protein O
structures. O
Porter O
5 O

classes O
on O
a O
large O
independent O
set B-DAT

ical O
limits O
of O
prediction O
– O
set B-DAT
at O
88–90% O
accuracy O
per O
AA O

validation O
experiments O
on O
the O
training O
set B-DAT

adopting O
a O
considerably O
larger O
training O
set B-DAT
but O
without O
evolutionary O
information, O
we O

ensemble O
trained O
on O
a O
smaller O
set34 B-DAT

NN O
architectures O
on O
the O
validation O
set B-DAT

proteins O
in O
our O
case), O
the O
set B-DAT
of O
hyperparameters O
selected O
for O
the O

preliminary O
testing O
by O
splitting O
our O
set B-DAT
into O
1/5 O
for O
testing O
and O

above) O
on O
the O
full O
training O
set B-DAT
rather O
than O
on O
individual O
training O

models O
on O
a O
completely O
independent O
set B-DAT
(see O
“2017_test” O
in O
Methods:Datasets) O
con O

Full O
set B-DAT
(Porter O
5) O
83.42% O
83.39% O
83.49 O

3. O
Assessment O
on O
the O
2017_test O
set B-DAT
of O
three-state O
ensembles O
trained O
on O

full O
set B-DAT

on O
4/5 O
of O
the O
training O
set B-DAT
on O
PSI-BLAST, O
HHblits O
and O
concatenated O

trained O
on O
the O
full O
training O
set B-DAT
achieves O
73.02% O
Q8 O
accuracy O
on O

set B-DAT
described O
in O
Methods:Dataset O
(see O
also O

multiple O
predictors O
on O
independent O
test O
set B-DAT

on O
the O
2017_test O
set B-DAT
we O
created, O
containing O
3,154 O
proteins O

5); O
the O
full O
set B-DAT
of O
3,154 O
proteins O
(Table O

our O
tests O
on O
the O
2017_test O
set B-DAT
with O
3-class O
accu- O
racy O
of O

the O
smaller O
version O
of O
the O
set B-DAT
and O
84.2% O
on O
the O
larger O

AA O
on O
the O
full O
test O
set B-DAT

Performances O
on O
the O
smaller O
2017_test O
set B-DAT
for O
which O
Spider3 O
generates O
predictions O

both O
versions O
of O
the O
2017_test O
set B-DAT

of O
the O
set, B-DAT
respectively. O
Porter O
5 O
is O
also O

5 O
performances O
on O
the O
CASP1328 O
set, B-DAT
and O
on O
6 O
months O
of O

we O
obtained O
on O
the O
2017_test O
set B-DAT

only O
section O
of O
the O
2017_test O
set, B-DAT
which O
is O
roughly O
90% O
of O

is O
selected O
for O
the O
training O
set, B-DAT
while O
a O
further O
150 O
proteins O

not O
included O
in O
the O
training O
set B-DAT
are O
used O
as O
a O
blind O

test O
set B-DAT

trained O
on O
the O
1,348 O
JPred4 O
set B-DAT

set B-DAT

same O
sets. B-DAT
While O
the O
testing O
set O
is O
small O
(150 O
proteins), O
these O

Q3 O
when O
trained O
on O
a O
set B-DAT
which O
is O

smaller O
than O
its O
original O
training O
set B-DAT

assessed O
Jpred4 O
on O
the O
2019_test O
set B-DAT
(see O
Table O

8 O
and O
description O
of O
the O
set B-DAT
in O
the O
following O
section O

predictors. O
In O
a O
separate O
test O
set B-DAT
we O
assessed O
some O
very O
recent O

more O
recent O
than O
our O
2017_test O
set, B-DAT
i.e. O
MUFOLD-SS43, O
NetSurfP-2.048 O
and O
SPOT-1D44 O

generated O
a O
second O
independent O
test O
set B-DAT
(also O
see O
Methods:Datasets) O
starting O
from O

not O
access O
the O
MUFOLD-SS O
training O
set B-DAT

861 O
original O
proteins O
in O
the O
set, B-DAT
Table O

quality O
of O
our O
large O
training O
set B-DAT
(see O
Table O

the O
experiments O
on O
a O
training O
set B-DAT
twice O
as O
large O
as O
the O

of O
a O
larger, O
well-distributed O
training O
set B-DAT

and O
Jpred4 O
assessed O
on O
2019_test O
set B-DAT
of O
618 O
proteins O

on O
a O
first O
independent O
test O
set B-DAT
(2017_test), O
along O
with O
some O
of O

reduction O
of O
our O
final O
test O
set B-DAT
against O
their O
training O
sets35 O

overlapped O
with O
our O
original O
test O
set, B-DAT
we O
generated O
a O
second O
smaller O

independent O
test O
set B-DAT
(2019_test) O
based O

Specifically, O
we O
built O
our O
training O
set B-DAT
from O
the O
PDB O
released O
on O

also O
built O
an O
independent O
test O
set B-DAT
(2017_test) O
from O
the O
PDB O
released O

14, O
2017. O
We O
redundancy-reduced O
this O
set B-DAT
at O
a O
25% O
identity O
threshold O

training O
set B-DAT

internally O
redundancy O
reduced O
the O
resulting O
set B-DAT
at O
a O
25% O
identity O
threshold O

from O
both O
sets. B-DAT
The O
training O
set O
contains O

In O
different O
tests O
the O
training O
set B-DAT
is O
used O
as O
a O
whole O

for O
hyperparameter O
optimization35. O
The O
2017_test O
set B-DAT
is O
only O
used O

curated O
an O
additional O
independent O
test O
set B-DAT
(2019_test) O
to O
fairly O
compare O
Porter O

overlapping O
with O
our O
2017_test O
set B-DAT

2019. O
We O
then O
redundancy-reduced O
this O
set B-DAT

SPOT-1D, O
NetSurfP-2.0 O
and O
our O
training O
set B-DAT
at O
25% O
identity O
threshold. O
Finally O

the O
internal O
redundancy O
of O
this O
set B-DAT
at O
a O
25% O
sequence O
identity O

The O
simple O
idea O
is O
to O
set B-DAT
to O
1 O
the O
position O

The O
number O
of O
hyperparameters O
to O
set B-DAT
in O
a O
BRNN, O
and O
the O

stochastic O
gradient O
descent O
(SGD)65. O
We O
set B-DAT
momentum O
to O
0.9 O
and O
divided O

cross O
entropy O
error O
on O
training O
set B-DAT
had O
not O
decreased O
for O
100 O

epochs. O
The O
train- O
ing O
set B-DAT
is O
shuffled O
at O
the O
end O

size O
of O
a O
mini-batch O
is O
set B-DAT
to O
~10 O
proteins, O
that O
is O

multiple O
predictors O
on O
independent O
test O
set B-DAT

NN O
architectures O
on O
the O
validation O
set B-DAT

3 O
Assessment O
on O
the O
2017_test O
set B-DAT
of O
three-state O
ensembles O
trained O
on O

either O
five-fold O
cross-validation O
or O
full O
set B-DAT

AA O
on O
the O
full O
test O
set B-DAT

Performances O
on O
the O
smaller O
2017_test O
set B-DAT
for O
which O
Spider3 O
generates O
predictions O

and O
Jpred4 O
assessed O
on O
2019_test O
set B-DAT
of O
618 O
proteins O

DeepPrivacy O
Results O
on O
a O
diverse O
set B-DAT
of O
images. O
The O
left O
image O

distribution. O
We O
anonymize O
the O
validation O
set B-DAT
of O
the O
WIDER-Face O
dataset O
[27 O

images O
while O
preserving O
a O
large O
set B-DAT
of O
facial O
attributes. O
This O
is O

techniques O
on O
the O
WIDER-Face O
validation O
set B-DAT

in O
the O
WIDER O
Face O
validation O
set B-DAT

the O
faces O
in O
the O
validation O
set B-DAT

calculate O
FID O
from O
a O
validation O
set B-DAT
of O
50, O
000 O
faces O
from O

irregular O
poses. O
However, O
this O
would O
set B-DAT
restrictions O
on O
the O
pose O
estimator O

on O
the O
anonymized O
WIDER-Face O
validation O
set B-DAT

five O
publicly O
available O
datasets: O
(1) O
CullPDB B-DAT
53 O

CullPDB B-DAT
dataset O
was O
constructed O
before O
CASP10 O

CullPDB B-DAT
into O
two O
subsets O
for O
training O

training O
set O
consists O
of O
~5600 O
CullPDB B-DAT

the O
training O
set O
(containing O
5600 O
CullPDB B-DAT

obtains O
very O
good O
accuracy O
on O
CullPDB, B-DAT
CB513 O
and O
CASP10, O
but O
not O

on O
the O
CullPDB B-DAT
test O
set. O
Both O
methods O
fail O

of O
Q8 O
accuracy O
on O
the O
CullPDB B-DAT
training O
set O
with O
different O

tested O
methods O
on O
5 O
datasets: O
CullPDB, B-DAT
CB513, O
CASP10, O
CASP11 O
and O
CAMEO O

CullPDB B-DAT
CB513 O
CASP10 O
CASP11 O
CAMEO O

tested O
methods O
on O
5 O
datasets: O
CullPDB, B-DAT
CB513, O
CASP10, O
CASP11 O
and O
CAMEO O

CullPDB B-DAT
CB513 O
CASP10 O
CASP11 O
CAMEO O

tested O
methods O
on O
5 O
datasets: O
CullPDB, B-DAT
CB513, O
CASP10, O
CASP11 O
and O
CAMEO O

CullPDB B-DAT
CB513 O
CASP10 O
CASP11 O
CAMEO O

DeepCNF O
and O
ICML2014 O
on O
the O
CullPDB B-DAT
test O
set O

of O
Q8 O
accuracy O
on O
the O
CullPDB B-DAT
training O
set O
with O
different O

the O
three O
categories O
in O
the O
ShapeNet B-DAT
[3] O
dataset: O
airplane, O
chair, O
and O

on O
all O
shapes O
in O
the O
ShapeNet B-DAT
dataset. O
The O
auto-encoder O
is O
trained O

Models O
are O
first O
trained O
on O
ShapeNet B-DAT
to O
learn O
shape O
representations, O
which O

auto-encoder O
trained O
in O
the O
full O
ShapeNet B-DAT
dataset O
and O
train O
a O
linear O

Li O
Yi, O
and O
Fisher O
Yu. O
ShapeNet B-DAT

Chair B-DAT

the O
three O
categories O
in O
the O
ShapeNet B-DAT
[3] O
dataset: O
airplane, O
chair, O
and O

on O
all O
shapes O
in O
the O
ShapeNet B-DAT
dataset. O
The O
auto-encoder O
is O
trained O

Models O
are O
first O
trained O
on O
ShapeNet B-DAT
to O
learn O
shape O
representations, O
which O

auto-encoder O
trained O
in O
the O
full O
ShapeNet B-DAT
dataset O
and O
train O
a O
linear O

Li O
Yi, O
and O
Fisher O
Yu. O
ShapeNet B-DAT

Airplane B-DAT

Airplane B-DAT

3D O
CAD O
models O
such O
as O
ShapeNet B-DAT
[18], O
predicting O
3D O
representations O
such O

multi-view O
and O
single-view O
settings O
on O
ShapeNet B-DAT
[18] O
objects, O
real-world O
scenes O
(KITTI O

Fig. O
2: O
Results O
on O
ShapeNet B-DAT
[18]. O
The O
proposed O
framework O
typically O

test O
the O
proposed O
model O
on O
ShapeNet B-DAT
[18], O
where O
ground O
truth O
views O

Table O
1: O
ShapeNet B-DAT
objects: O
we O
compare O
our O
framework O

synthesis O
approaches O
only O
focus O
on O
ShapeNet, B-DAT
we O
are O
also O
interested O
in O

Views O
Methods O
Car O
Chair B-DAT
L1 O
SSIM O
L1 O
SSIM O

Views O
Methods O
Car O
Chair B-DAT
L1 O
SSIM O
L1 O
SSIM O

Geometry-based O
View B-DAT
Synthesis. O
A O
great O
amount O
of O

nor O
explicit O
3D O
model. O
Novel O
View B-DAT
Synthesis. O
[19,20] O
propose O
to O
directly O

10. O
Seitz, O
S.M., O
Dyer, O
C.R.: O
View B-DAT
morphing. O
In: O
Special O
Interest O
Group O

W., O
Malik, O
J., O
Efros, O
A.A.: O
View B-DAT
synthesis O
by O
appear- O
ance O
flow O

Geometry-based O
View O
Synthesis B-DAT

explicit O
3D O
model. O
Novel O
View O
Synthesis B-DAT

Multi-view O
to O
Novel B-DAT
view: O
Synthesizing O
novel O
views O
with O

Keywords: O
Novel B-DAT
view O
synthesis, O
multi-view O
novel O
view O

supervision O
nor O
explicit O
3D O
model. O
Novel B-DAT
View O
Synthesis. O
[19,20] O
propose O
to O

4.1 O
Novel B-DAT
view O
synthesis O
for O
objects O

4.2 O
Novel B-DAT
view O
synthesis O
for O
scenes O

T., O
Fritz, O
M., O
Tuytelaars, O
T.: O
Novel B-DAT
views O
of O
objects O
from O
a O

ShapeNet O
[18] O
objects, O
real-world O
scenes O
(KITTI B-DAT
Visual O
Odometry O
Dataset O
[41]), O
and O

our O
framework O
on O
both O
real O
(KITTI B-DAT
Visual O
Odometry O
Dataset O
[41]) O
and O

Fig. O
5: O
Synthesized O
scenes O
on O
KITTI B-DAT
[41] O
and O
Synthia O
[42] O
datasets O

KITTI B-DAT
The O
dataset O
[41] O
was O
originally O

the O
same O
preprocessing O
procedures O
as O
KITTI B-DAT
to O
create O
the O
training O
and O

Views O
Methods O
KITTI B-DAT
Synthia O
L1 O
SSIM O
L1 O
SSIM O

to O
[19] O
and O
[22] O
on O
KITTI B-DAT
and O
Syn- O
thia O

Views O
Methods O
KITTI B-DAT
Synthia O
L1 O
SSIM O
L1 O
SSIM O

improvement O
(car: O
26%, O
chair: O
36%, O
KITTI B-DAT

19] O
(car: O
19%, O
chair: O
14%, O
KITTI B-DAT

Geometry-based O
View B-DAT
Synthesis. O
A O
great O
amount O
of O

nor O
explicit O
3D O
model. O
Novel O
View B-DAT
Synthesis. O
[19,20] O
propose O
to O
directly O

10. O
Seitz, O
S.M., O
Dyer, O
C.R.: O
View B-DAT
morphing. O
In: O
Special O
Interest O
Group O

W., O
Malik, O
J., O
Efros, O
A.A.: O
View B-DAT
synthesis O
by O
appear- O
ance O
flow O

Geometry-based O
View O
Synthesis B-DAT

explicit O
3D O
model. O
Novel O
View O
Synthesis B-DAT

41]), O
and O
synthe- O
sized O
scenes O
(Synthia B-DAT
dataset O
[42]). O
We O
benchmark O
against O

by O
FCN O
[43] O
trained O
on O
Synthia B-DAT
dataset O
[42 O

Odometry O
Dataset O
[41]) O
and O
synthetic O
(Synthia B-DAT
Dataset O
[42]) O
scenes O

scenes O
on O
KITTI O
[41] O
and O
Synthia B-DAT
[42] O
datasets. O
Our O
framework O
typically O

Synthia B-DAT
The O
data O
was O
originally O
proposed O

Views O
Methods O
KITTI O
Synthia B-DAT
L1 O
SSIM O
L1 O
SSIM O

Views O
Methods O
KITTI O
Synthia B-DAT
L1 O
SSIM O
L1 O
SSIM O

on O
the O
sequences O
extracted O
from O
Synthia B-DAT
dataset O
[42] O
with O
the O
same O

Fig. O
6: O
Synthia B-DAT
FCN-results O
for O
the O
scenes O
synthesized O

VOC O
and O
fine- O
tuned O
on O
Synthia B-DAT
dataset. O
The O
scores O
are O
estimated O

26%, O
chair: O
36%, O
KITTI: O
10%, O
Synthia B-DAT

19%, O
chair: O
14%, O
KITTI: O
4%, O
Synthia B-DAT

Multi-view O
to O
Novel B-DAT
view: O
Synthesizing O
novel O
views O
with O

Keywords: O
Novel B-DAT
view O
synthesis, O
multi-view O
novel O
view O

supervision O
nor O
explicit O
3D O
model. O
Novel B-DAT
View O
Synthesis. O
[19,20] O
propose O
to O

4.1 O
Novel B-DAT
view O
synthesis O
for O
objects O

4.2 O
Novel B-DAT
view O
synthesis O
for O
scenes O

T., O
Fritz, O
M., O
Tuytelaars, O
T.: O
Novel B-DAT
views O
of O
objects O
from O
a O

the O
three O
categories O
in O
the O
ShapeNet B-DAT
[3] O
dataset: O
airplane, O
chair, O
and O

on O
all O
shapes O
in O
the O
ShapeNet B-DAT
dataset. O
The O
auto-encoder O
is O
trained O

Models O
are O
first O
trained O
on O
ShapeNet B-DAT
to O
learn O
shape O
representations, O
which O

auto-encoder O
trained O
in O
the O
full O
ShapeNet B-DAT
dataset O
and O
train O
a O
linear O

Li O
Yi, O
and O
Fisher O
Yu. O
ShapeNet B-DAT

Car B-DAT

3D O
CAD O
models O
such O
as O
ShapeNet B-DAT
[18], O
predicting O
3D O
representations O
such O

multi-view O
and O
single-view O
settings O
on O
ShapeNet B-DAT
[18] O
objects, O
real-world O
scenes O
(KITTI O

Fig. O
2: O
Results O
on O
ShapeNet B-DAT
[18]. O
The O
proposed O
framework O
typically O

test O
the O
proposed O
model O
on O
ShapeNet B-DAT
[18], O
where O
ground O
truth O
views O

Table O
1: O
ShapeNet B-DAT
objects: O
we O
compare O
our O
framework O

synthesis O
approaches O
only O
focus O
on O
ShapeNet, B-DAT
we O
are O
also O
interested O
in O

Views O
Methods O
Car B-DAT
Chair O
L1 O
SSIM O
L1 O
SSIM O

Views O
Methods O
Car B-DAT
Chair O
L1 O
SSIM O
L1 O
SSIM O

3D O
Motion O
Seg- O
mentation O
Benchmark O
(KT3DMoSeg B-DAT

idea O
of O
its O
performance O
in O
KT3DMoSeg B-DAT
by O
looking O
at O
the O
result O

on O
Hopkins155, O
Hopkins12, O
MTPV62 O
and O
KT3DMoSeg B-DAT
datasets O
evaluated O
as O
clas- O
sification O

31] O
Hopkins12 O
[26] O
MTPV62 O
[23]∗∗ O
KT3DMoSeg B-DAT

results O
on O
some O
sequences O
from O
KT3DMoSeg B-DAT
in O
Fig. O
3 O
to O
better O

The O
classification O
error O
over O
all O
KT3DMoSeg B-DAT
sequences O
v.s. O
λ O
and O
γ O

and O
sensitivity O
to O
parameters O
for O
KT3DMoSeg B-DAT
benchmark O

Examples O
of O
motion O
segmentation O
on O
KT3DMoSeg B-DAT
sequences O

also O
for O
real-world O
sequences O
in O
KT3DMoSeg B-DAT

propose O
a O
new O
dataset, O
the O
KT3DMoSeg B-DAT
dataset, O
to O
reflect O
and O
investigate O

of O
perspective O
effects O
in O
the O
Hopkins155 B-DAT
benchmark O
[31]. O
However, O
it O
is O

mo- O
tion O
segmentation. O
In O
the O
Hopkins155 B-DAT
dataset, O
this O
is O
not O
an O

that O
there O
are O
actually O
some O
Hopkins B-DAT
sequences O
with O
non-negligible O
perspective O
effects O

21] O
dealing O
with O
the O
realistic O
Hopkins155 B-DAT
[31] O
sequences O
almost O
as O
a O

of O
perspective O
effects O
in O
the O
Hopkins B-DAT
sequences. O
Thus, O
these O
later O
works O

mentation O
benchmarks O
including O
the O
Hopkins155 B-DAT
[31], O
the O
Hopkins12 O
[26] O
for O

single-view O
and O
multi-view O
approaches O
on O
Hopkins155 B-DAT
benchmark O
[31]. O
Specifically, O
for O
single-view O

for O
subset O
constrained O
clustering O
on O
Hopkins155 B-DAT

performance O
can O
be O
observed O
on O
Hopkins12 B-DAT
and O
MTPV62 O
as O
well. O
Usu O

The O
limitations O
of O
the O
Hopkins155 B-DAT
dataset O
are O
well- O
known: O
limited O

1: O
Motion O
segmentation O
results O
on O
Hopkins155, B-DAT
Hopkins12, O
MTPV62 O
and O
KT3DMoSeg O
datasets O

Models O
Hopkins155 B-DAT
[31] O
Hopkins12 O
[26] O
MTPV62 O
[23]∗∗ O
KT3DMoSeg O

12 O
clips O
Hopkins B-DAT
50 O
clips O

the O
extant O
datasets O
such O
as O
Hopkins155, B-DAT
but O
also O
for O
real-world O
sequences O

we O
carry O
out O
experiments O
on O
Hopkins155, B-DAT
Hopkins12 O
and O
MTPV62 O
and O
achieved O

of O
perspective O
effects O
in O
the O
Hopkins155 B-DAT
benchmark O
[31]. O
However, O
it O
is O

mo- O
tion O
segmentation. O
In O
the O
Hopkins155 B-DAT
dataset, O
this O
is O
not O
an O

21] O
dealing O
with O
the O
realistic O
Hopkins155 B-DAT
[31] O
sequences O
almost O
as O
a O

mentation O
benchmarks O
including O
the O
Hopkins155 B-DAT
[31], O
the O
Hopkins12 O
[26] O
for O

single-view O
and O
multi-view O
approaches O
on O
Hopkins155 B-DAT
benchmark O
[31]. O
Specifically, O
for O
single-view O

for O
subset O
constrained O
clustering O
on O
Hopkins155 B-DAT

The O
limitations O
of O
the O
Hopkins155 B-DAT
dataset O
are O
well- O
known: O
limited O

1: O
Motion O
segmentation O
results O
on O
Hopkins155, B-DAT
Hopkins12, O
MTPV62 O
and O
KT3DMoSeg O
datasets O

Models O
Hopkins155 B-DAT
[31] O
Hopkins12 O
[26] O
MTPV62 O
[23 O

the O
extant O
datasets O
such O
as O
Hopkins155, B-DAT
but O
also O
for O
real-world O
sequences O

we O
carry O
out O
experiments O
on O
Hopkins155, B-DAT
Hopkins12 O
and O
MTPV62 O
and O
achieved O

that O
represent O
different O
scenarios: O
(i) O
Hopkins155 B-DAT
for O
motion O
segmentation; O
(ii) O
Extended O

sequences O
with O
missing O
data; O
(iv) O
Hopkins B-DAT
outdoor O
sequences O
for O
semi-dense O
motion O

5.1. O
Hopkins155 B-DAT

Hopkins155 B-DAT
[30] O
is O
a O
standard O
benchmark O

data O
gracefully, O
we O
employed O
the O
Hopkins B-DAT
12 O
additional O
sequences O
containing O
incomplete O

5.4. O
Hopkins B-DAT
Outdoor: O
Semi-dense, O
Incomplete O
Data O
with O

are O
21 O
outdoor O
videos O
in O
Hopkins155, B-DAT
the O
tracking O
code O
that O
we O

Clustering O
error O
(in O
%) O
on O
Hopkins B-DAT
155 O

Table O
2: O
Ablation O
study O
on O
Hopkins B-DAT
155 O

Clustering O
error O
(in O
%) O
on O
Hopkins B-DAT
12 O
Real O
Motion O
Sequences O
with O

Appendix O
– O
Hopkins B-DAT
Outdoor: O
Semi-dense, O
Incomplete O
Data O
with O

that O
represent O
different O
scenarios: O
(i) O
Hopkins155 B-DAT
for O
motion O
segmentation; O
(ii) O
Extended O

5.1. O
Hopkins155 B-DAT

Hopkins155 B-DAT
[30] O
is O
a O
standard O
benchmark O

are O
21 O
outdoor O
videos O
in O
Hopkins155, B-DAT
the O
tracking O
code O
that O
we O

of O
Biomedical O
Engineering, O
The O
Johns O
Hopkins B-DAT
University, O
USA. O
E-mail: O
rvi- O
dal@cis.jhu.edu O

Hopkins B-DAT
155 O

Hopkins B-DAT
155 O

Hopkins B-DAT
155 O
Dataset O

prob- O
lem, O
we O
consider O
the O
Hopkins B-DAT
155 O
dataset O
[66], O
which O
consists O

of O
different O
algorithms O
on O
the O
Hopkins B-DAT
155 O
dataset O
with O

small O
principal O
angles. O
In O
the O
Hopkins B-DAT

dataset. O
As O
shown, O
in O
the O
Hopkins B-DAT

can O
conclude O
that O
in O
the O
Hopkins B-DAT
155 O
dataset O
the O
challenge O
is O

do O
so, O
we O
consider O
the O
Hopkins B-DAT
155 O
dataset O
[66] O
that O
consists O

of O
different O
algorithms O
on O
the O
Hopkins B-DAT
155 O
dataset O
with O

of O
SSC O
over O
the O
entire O
Hopkins B-DAT
155 O
dataset. O
Note O
that O
the O

for O
testing O
incomplete O
trajectories O
and O
MTPV62 B-DAT
[23] O
for O
testing O
stronger O
perspective O

be O
observed O
on O
Hopkins12 O
and O
MTPV62 B-DAT
as O
well. O
Usu- O
ally, O
the O

segmentation O
results O
on O
Hopkins155, O
Hopkins12, O
MTPV62 B-DAT
and O
KT3DMoSeg O
datasets O
evaluated O
as O

Models O
Hopkins155 O
[31] O
Hopkins12 O
[26] O
MTPV62 B-DAT
[23]∗∗ O
KT3DMoSeg O

experiments O
on O
Hopkins155, O
Hopkins12 O
and O
MTPV62 B-DAT
and O
achieved O
state-of-the-art O
performances O
on O

for O
testing O
incomplete O
trajectories O
and O
MTPV62 B-DAT
[23] O
for O
testing O
stronger O
perspective O

be O
observed O
on O
Hopkins12 O
and O
MTPV62 B-DAT
as O
well. O
Usu- O
ally, O
the O

segmentation O
results O
on O
Hopkins155, O
Hopkins12, O
MTPV62 B-DAT
and O
KT3DMoSeg O
datasets O
evaluated O
as O

Models O
Hopkins155 O
[31] O
Hopkins12 O
[26] O
MTPV62 B-DAT
[23]∗∗ O
KT3DMoSeg O

experiments O
on O
Hopkins155, O
Hopkins12 O
and O
MTPV62 B-DAT
and O
achieved O
state-of-the-art O
performances O
on O

evaluate O
the O
proposed O
method O
on O
HICO B-DAT

We O
perform O
extensive O
experiments O
on O
HICO B-DAT

three O
Default O
category O
sets O
on O
HICO B-DAT

datasets, O
such O
as O
V-COCO O
[13], O
HICO B-DAT

train O
and O
test O
C O
on O
HICO B-DAT

We O
adopt O
two O
HOI O
datasets O
HICO B-DAT

-DET O
[3] O
and O
V-COCO O
[13]. O
HICO B-DAT

HICO-DET B-DAT
RPDCD O
HICO-DET O
HICO O

-DET O
RPT1CD O
V-COCO O
HICO-DET B-DAT
RPT2CD O
HICO O

-DET, O
V-COCO O
HICO B-DAT

HICO-DET B-DAT
RCD O
- O
HICO O

V-COCO O
RPDCD O
V-COCO O
V-COCO O
RPT1CD O
HICO B-DAT

-DET O
V-COCO O
RPT2CD O
HICO B-DAT

RCD O
- O
V-COCO O
RCT O
- O
HICO B-DAT

24, O
12, O
21, O
9] O
on O
HICO B-DAT

with O
mean O
average O
precision. O
For O
HICO B-DAT

Know O
Object O
Full O
sets O
on O
HICO B-DAT

Table O
2. O
Results O
comparison O
on O
HICO B-DAT

most O
state-of-the- O
art O
performance. O
On O
HICO B-DAT

and O
effectiveness O
of O
interactiveness. O
Since O
HICO B-DAT

transferring O
is O
per- O
formed O
from O
HICO B-DAT

a O
relatively O
smaller O
improvement O
on O
HICO B-DAT

HICO B-DAT

HICO B-DAT

We O
perform O
extensive O
experiments O
on O
HICO-DET B-DAT
[3], O
V-COCO O
[13] O
datasets. O
Our O

three O
Default O
category O
sets O
on O
HICO-DET, B-DAT
4.0 O
and O
3.4 O
mAP O
on O

datasets, O
such O
as O
V-COCO O
[13], O
HICO-DET B-DAT
[3], O
HCVRD O
[31], O
were O
proposed O

train O
and O
test O
C O
on O
HICO-DET B-DAT
(referred O
as O
“RCD”). O
Second, O
we O

We O
adopt O
two O
HOI O
datasets O
HICO-DET B-DAT
[3] O
and O
V-COCO O
[13]. O
HICO-DET O

HICO-DET B-DAT
RPDCD O
HICO-DET O
HICO-DET O
RPT1CD O
V-COCO O
HICO-DET O
RPT2CD O
HICO-DET O

, O
V-COCO O
HICO-DET B-DAT

HICO-DET B-DAT
RCD O
- O
HICO-DET O
RCT O
- O
V-COCO O

V-COCO O
RPDCD O
V-COCO O
V-COCO O
RPT1CD O
HICO-DET B-DAT
V-COCO O
RPT2CD O
HICO-DET, O
V-COCO O
V-COCO O

RCD O
- O
V-COCO O
RCT O
- O
HICO-DET B-DAT

24, O
12, O
21, O
9] O
on O
HICO-DET, B-DAT
and O
four O
methods O
[13, O
12 O

Know O
Object O
Full O
sets O
on O
HICO-DET B-DAT

Table O
2. O
Results O
comparison O
on O
HICO-DET B-DAT
[3]. O
D O
indicates O
the O
default O

most O
state-of-the- O
art O
performance. O
On O
HICO-DET, B-DAT
RPT2CD O
surpasses O
[9] O
by O
2.38 O

and O
effectiveness O
of O
interactiveness. O
Since O
HICO-DET B-DAT
train O
set O
(38K) O
is O
much O

transferring O
is O
per- O
formed O
from O
HICO-DET B-DAT
to O
V-COCO. O
As O
we O
can O

a O
relatively O
smaller O
improvement O
on O
HICO-DET B-DAT
when O
compared O
with O
mode O
RPDCD O

HICO-DET B-DAT
RPDCD O
-65.96% O
RPT1CD O
-62.24% O
RPT2CD O

HICO-DET B-DAT
V-COCO O
Method O
Default O
Full O
KO O

the O
proposed O
method O
on O
HICO- O
DET B-DAT
and O
V-COCO O
datasets. O
Our O
framework O

DET B-DAT
[3], O
V-COCO O
[13] O
datasets. O
Our O

DET, B-DAT
4.0 O
and O
3.4 O
mAP O
on O

DET B-DAT
[3], O
HCVRD O
[31], O
were O
proposed O

DET B-DAT
(referred O
as O
“RCD”). O
Second, O
we O

DET B-DAT
[3] O
and O
V-COCO O
[13]. O
HICO-DET O

DET B-DAT
RPDCD O
HICO-DET O
HICO-DET O
RPT1CD O
V-COCO O

DET B-DAT
RPT2CD O
HICO-DET, O
V-COCO O
HICO-DET O

DET B-DAT
RCD O
- O
HICO-DET O
RCT O

DET B-DAT
V-COCO O
RPT2CD O
HICO-DET, O
V-COCO O
V-COCO O

DET B-DAT

DET, B-DAT
and O
four O
methods O
[13, O
12 O

mean O
average O
precision. O
For O
HICO- O
DET, B-DAT
we O
follow O
the O
settings O
in O

DET B-DAT

DET B-DAT
[3]. O
D O
indicates O
the O
default O

DET, B-DAT
RPT2CD O
surpasses O
[9] O
by O
2.38 O

DET B-DAT
train O
set O
(38K) O
is O
much O

DET B-DAT
to O
V-COCO. O
As O
we O
can O

DET B-DAT
when O
compared O
with O
mode O
RPDCD O

DET B-DAT
RPDCD O
-65.96% O
RPT1CD O
-62.24% O
RPT2CD O

DET B-DAT
V-COCO O
Method O
Default O
Full O
KO O

the O
Verb O
in O
COCO O
and O
HICO B-DAT

Humans O
Interacting O
with O
Common O
Objects O
(HICO B-DAT

on O
V-COCO O
and O
49% O
on O
HICO B-DAT

while O
sitting O
on O
a O
chair. O
HICO B-DAT

is O
a O
subset O
of O
the O
HICO B-DAT
dataset O
[3]. O
HICO-DET O
contains O
600 O

16] O
for O
both O
V-COCO O
and O
HICO B-DAT
datasets. O
The O
goal O
is O
to O

comparison O
with O
the O
state-of-the-arts O
on O
HICO B-DAT

single O
NVIDIA O
P100 O
GPU. O
For O
HICO B-DAT

procedures O
for O
both O
V-COCO O
and O
HICO B-DAT

V-COCO O
in O
Table O
1 O
and O
HICO B-DAT

approaches O
[14, O
16, O
22]. O
For O
HICO B-DAT

the O
V-COCO O
dataset O
and O
the O
HICO B-DAT

Sample O
HOI O
detections O
on O
the O
HICO B-DAT

Jiaxuan O
Wang, O
and O
Jia O
Deng. O
HICO B-DAT

the O
Verb O
in O
COCO O
and O
HICO-DET B-DAT
datasets O
and O
show O
that O
our O

Humans O
Interacting O
with O
Common O
Objects O
(HICO-DET) B-DAT
[4] O
datasets. O
Our O
results O
show O

on O
V-COCO O
and O
49% O
on O
HICO-DET B-DAT
with O
respect O
to O
the O
existing O

while O
sitting O
on O
a O
chair. O
HICO-DET B-DAT
[3] O
is O
a O
subset O
of O

the O
HICO O
dataset O
[3]. O
HICO-DET B-DAT
contains O
600 O
HOI O
categories O
over O

comparison O
with O
the O
state-of-the-arts O
on O
HICO-DET B-DAT
test O
set. O
The O
results O
from O

single O
NVIDIA O
P100 O
GPU. O
For O
HICO-DET, B-DAT
training O
the O
network O
on O
the O

procedures O
for O
both O
V-COCO O
and O
HICO-DET B-DAT
datasets. O
Please O
refer O
to O
the O

V-COCO O
in O
Table O
1 O
and O
HICO-DET B-DAT
in O
Table O
2. O
For O
V-COCO O

approaches O
[14, O
16, O
22]. O
For O
HICO-DET, B-DAT
we O
also O
demonstrate O
that O
our O

the O
V-COCO O
dataset O
and O
the O
HICO-DET B-DAT
dataset. O
We O
highlight O
the O
detected O

Sample O
HOI O
detections O
on O
the O
HICO-DET B-DAT
test O
set. O
Our O
model O
detects O

DET B-DAT
datasets O
and O
show O
that O
our O

DET) B-DAT
[4] O
datasets. O
Our O
results O
show O

DET B-DAT
with O
respect O
to O
the O
existing O

DET B-DAT
[3] O
is O
a O
subset O
of O

DET B-DAT
contains O
600 O
HOI O
categories O
over O

DET B-DAT
test O
set. O
The O
results O
from O

DET, B-DAT
training O
the O
network O
on O
the O

DET B-DAT
datasets. O
Please O
refer O
to O
the O

DET B-DAT
in O
Table O
2. O
For O
V-COCO O

DET, B-DAT
we O
also O
demonstrate O
that O
our O

DET B-DAT
dataset. O
We O
highlight O
the O
detected O

DET B-DAT
test O
set. O
Our O
model O
detects O

benchmarks O
on O
images O
and O
videos: O
HICO B-DAT

on O
three O
HOI O
datasets, O
namely O
HICO B-DAT

HOI O
detection O
from O
im- O
ages O
(HICO B-DAT

first O
experiment O
is O
performed O
on O
HICO B-DAT

connecting O
them. O
Datasets. O
We O
use O
HICO B-DAT

for O
benchmarking O
our O
GPNN O
model. O
HICO B-DAT

HOI O
detection O
results O
(mAP) O
on O
HICO B-DAT

3. O
HOI O
detection O
results O
on O
HICO B-DAT

117)-Sigmoid(·) O
and O
FC(dV O
-26)-Sigmoid(·) O
for O
HICO B-DAT

Following O
the O
standard O
settings O
in O
HICO B-DAT

IoU) O
greater O
than O
0.5. O
For O
HICO B-DAT

all O
600 O
HOI O
categories O
in O
HICO B-DAT
(Full); O
ii) O
138 O
HOI O
categories O

HOI O
category O
sets O
on O
the O
HICO B-DAT

V-COCO O
[17] O
HICO B-DAT

Y., O
Wang, O
J., O
Deng, O
J.: O
HICO B-DAT

benchmarks O
on O
images O
and O
videos: O
HICO-DET, B-DAT
V-COCO, O
and O
CAD-120 O
datasets. O
Our O

on O
three O
HOI O
datasets, O
namely O
HICO-DET B-DAT
[1], O
V-COCO O
[17] O
and O
CAD-120 O

HOI O
detection O
from O
im- O
ages O
(HICO-DET, B-DAT
V-COCO) O
and O
HOI O
recognition O
and O

first O
experiment O
is O
performed O
on O
HICO-DET B-DAT
[1] O
and O
V-COCO O
[17] O
datasets O

connecting O
them. O
Datasets. O
We O
use O
HICO-DET B-DAT
[1] O
and O
V-COCO O
[17] O
datasets O

for O
benchmarking O
our O
GPNN O
model. O
HICO-DET B-DAT
provides O
more O
than O
150K O
annotated O

HOI O
detection O
results O
(mAP) O
on O
HICO-DET B-DAT
dataset O
[1]. O
Higher O
values O
are O

3. O
HOI O
detection O
results O
on O
HICO-DET B-DAT
[1] O
test O
images. O
Human O
and O

117)-Sigmoid(·) O
and O
FC(dV O
-26)-Sigmoid(·) O
for O
HICO-DET B-DAT
and O
V-COCO, O
respectively O

Following O
the O
standard O
settings O
in O
HICO-DET B-DAT
and O
V- O
COCO O
benchmarks, O
we O

IoU) O
greater O
than O
0.5. O
For O
HICO-DET B-DAT
dataset, O
we O
report O
the O
mAP O

V-COCO O
[17] O
HICO-DET B-DAT
[1] O
CAD-120 O
[22 O

DET, B-DAT
V-COCO, O
and O
CAD-120 O
datasets. O
Our O

DET B-DAT
[1], O
V-COCO O
[17] O
and O
CAD-120 O

DET, B-DAT
V-COCO) O
and O
HOI O
recognition O
and O

DET B-DAT
[1] O
and O
V-COCO O
[17] O
datasets O

DET B-DAT
[1] O
and O
V-COCO O
[17] O
datasets O

DET B-DAT
provides O
more O
than O
150K O
annotated O

DET B-DAT
dataset O
[1]. O
Higher O
values O
are O

DET B-DAT
[1] O
test O
images. O
Human O
and O

DET B-DAT
and O
V-COCO, O
respectively O

DET B-DAT
and O
V- O
COCO O
benchmarks, O
we O

DET B-DAT
dataset, O
we O
report O
the O
mAP O

category O
sets O
on O
the O
HICO- O
DET B-DAT
dataset. O
The O
results O
on O
V-COCO O

DET B-DAT
[1] O
CAD-120 O
[22 O

Verbs O
in O
COCO O
(V-COCO) O
and O
HICO B-DAT

improvement O
on O
the O
newly O
released O
HICO B-DAT

Verbs O
in O
COCO) O
[13] O
and O
HICO B-DAT

ous O
components. O
The O
newly O
released O
HICO B-DAT

The O
older O
TUHOI O
[18] O
and O
HICO B-DAT
[4] O
datasets O
only O
have O
image O

Table O
6. O
Results O
on O
HICO B-DAT

HICO B-DAT

We O
additionally O
evaluate O
InteractNet O
on O
HICO B-DAT

9. O
Our O
results O
on O
the O
HICO B-DAT

Appendix O
B: O
HICO B-DAT

also O
test O
InteractNet O
on O
the O
HICO B-DAT

-DET O
dataset O
[3]. O
HICO B-DAT

are O
not O
exhaustively O
annotated O
on O
HICO B-DAT

ground O
truth O
labels O
from O
the O
HICO B-DAT

J. O
Wang, O
and O
J. O
Deng. O
HICO B-DAT

Verbs O
in O
COCO O
(V-COCO) O
and O
HICO-DET B-DAT
datasets, O
where O
we O
show O
quantitatively O

improvement O
on O
the O
newly O
released O
HICO-DET B-DAT
dataset O
[3]. O
Finally, O
our O
method O

Verbs O
in O
COCO) O
[13] O
and O
HICO-DET B-DAT
[3]. O
V- O
COCO O
serves O
as O

ous O
components. O
The O
newly O
released O
HICO-DET B-DAT
[3] O
con- O
tains O
∼48k O
images O

Table O
6. O
Results O
on O
HICO-DET B-DAT
test O
set. O
InteractNet O
outper- O
forms O

HICO-DET B-DAT
Dataset. O
We O
additionally O
evaluate O
InteractNet O

on O
HICO-DET B-DAT
[3] O
which O
contains O
600 O
types O

9. O
Our O
results O
on O
the O
HICO-DET B-DAT
test O
set. O
Each O
image O
shows O

Appendix O
B: O
HICO-DET B-DAT
Dataset O

also O
test O
InteractNet O
on O
the O
HICO-DET B-DAT
dataset O
[3]. O
HICO-DET O
contains O
approximately O

are O
not O
exhaustively O
annotated O
on O
HICO-DET B-DAT

ground O
truth O
labels O
from O
the O
HICO-DET B-DAT
anno- O
tations O
to O
each O
person O

DET B-DAT
datasets, O
where O
we O
show O
quantitatively O

DET B-DAT
dataset O
[3]. O
Finally, O
our O
method O

DET B-DAT
[3]. O
V- O
COCO O
serves O
as O

DET B-DAT
[3] O
con- O
tains O
∼48k O
images O

DET B-DAT
test O
set. O
InteractNet O
outper- O
forms O

DET B-DAT
Dataset. O
We O
additionally O
evaluate O
InteractNet O

DET B-DAT
[3] O
which O
contains O
600 O
types O

DET B-DAT
test O
set. O
Each O
image O
shows O

DET B-DAT
Dataset O

DET B-DAT
dataset O
[3]. O
HICO-DET O
contains O
approximately O

DET B-DAT

DET B-DAT
anno- O
tations O
to O
each O
person O

COCO B-DAT
datasets. O
Our O
framework O
outperforms O
state-of-the-art O

COCO B-DAT
[13] O
datasets. O
Our O
method O
cooperated O

COCO B-DAT

COCO B-DAT
[13], O
HICO-DET O
[3], O
HCVRD O
[31 O

estimate O
his/her O
17 O
keypoints O
(in O
COCO B-DAT
format O
[18]). O
Then, O
we O
link O

COCO B-DAT
HOIs, O
then O
finetune O
C O
for O

COCO B-DAT
train O
set. O
Last, O
we O
test O

COCO B-DAT
test O
set. O
Details O
of O
the O

COCO B-DAT
[13]. O
HICO-DET O
[3] O
includes O
47,776 O

COCO B-DAT
[13] O
provides O
10,346 O
images O
(2,533 O

COCO B-DAT
HICO-DET O
RPT2CD O
HICO-DET, O
V-COCO O
HICO-DET O

COCO B-DAT

COCO B-DAT
RPDCD O
V-COCO O
V-COCO O
RPT1CD O
HICO-DET O

COCO B-DAT
RPT2CD O
HICO-DET, O
V-COCO O
V-COCO O

COCO B-DAT
RCD O
- O
V-COCO O
RCT O

object O
detection O
re- O
sults O
and O
COCO B-DAT
[18] O
pre-trained O
weights O
from O
[9 O

COCO B-DAT

COCO, B-DAT
we O
evaluateAProle O
(24 O
actions O
with O

COCO B-DAT
are O
shown O
in O
Table O
3 O

COCO B-DAT

COCO B-DAT
train O
set O
(2.5K), O
improvement O
is O

COCO B-DAT

COCO, B-DAT
but O
it O
shows O
a O
relatively O

COCO B-DAT
[13]. O
D O
indicates O
the O
default O

COCO B-DAT
RPDCD O
-65.98% O
RPT1CD O
-59.51% O
RPT2CD O

COCO B-DAT
Method O
Default O
Full O
KO O
Full O

method O
on O
HICO- O
DET O
and O
V B-DAT

C O
V B-DAT

extensive O
experiments O
on O
HICO-DET O
[3], O
V B-DAT

4.0 O
and O
3.4 O
mAP O
on O
V B-DAT

large- O
scale O
datasets, O
such O
as O
V B-DAT

9], O
HOI O
graph O
G O
= O
(V, B-DAT
E) O
is O
dense O
connected, O
where O

V B-DAT
includes O
human O
node O
Vh O
and O

that O
fits O
the O
number O
of O
V B-DAT

C O
for O
1 O
epoch O
on O
V B-DAT

test O
this O
new O
C O
on O
V B-DAT

HOI O
datasets O
HICO-DET O
[3] O
and O
V B-DAT

than O
150k O
annotated O
human-object O
pairs. O
V B-DAT

HICO-DET O
RPDCD O
HICO-DET O
HICO-DET O
RPT1CD O
V B-DAT

-COCO O
HICO-DET O
RPT2CD O
HICO-DET, O
V B-DAT

RCD O
- O
HICO-DET O
RCT O
- O
V B-DAT

V-COCO B-DAT
RPDCD O
V-COCO O
V O

-COCO O
RPT1CD O
HICO-DET O
V B-DAT

-COCO O
RPT2CD O
HICO-DET, O
V-COCO B-DAT
V O

V-COCO B-DAT
RCD O
- O
V O

13, O
12, O
21, O
9] O
on O
V B-DAT

and O
Known O
Object O
mode. O
For O
V B-DAT

is O
accordingly O
reduced. O
Results O
on O
V B-DAT

4.0 O
and O
3.4 O
mAP O
on O
V B-DAT

38K) O
is O
much O
bigger O
than O
V B-DAT

per- O
formed O
from O
HICO-DET O
to O
V B-DAT

RPT1CD O
achieves O
obvious O
improvement O
on O
V B-DAT

Table O
3. O
Results O
comparison O
on O
V B-DAT

V B-DAT

HICO-DET O
V B-DAT

2] O
F. O
Caba O
Heilbron, O
V B-DAT

6] O
V B-DAT

method O
on O
HICO- O
DET O
and O
V-COCO B-DAT
datasets. O
Our O
framework O
outperforms O
state-of-the-art O

extensive O
experiments O
on O
HICO-DET O
[3], O
V-COCO B-DAT
[13] O
datasets. O
Our O
method O
cooperated O

4.0 O
and O
3.4 O
mAP O
on O
V-COCO B-DAT

large- O
scale O
datasets, O
such O
as O
V-COCO B-DAT
[13], O
HICO-DET O
[3], O
HCVRD O
[31 O

that O
fits O
the O
number O
of O
V-COCO B-DAT
HOIs, O
then O
finetune O
C O
for O

1 O
epoch O
on O
V-COCO B-DAT
train O
set. O
Last, O
we O
test O

this O
new O
C O
on O
V-COCO B-DAT
test O
set. O
Details O
of O
the O

HOI O
datasets O
HICO-DET O
[3] O
and O
V-COCO B-DAT
[13]. O
HICO-DET O
[3] O
includes O
47,776 O

than O
150k O
annotated O
human-object O
pairs. O
V-COCO B-DAT
[13] O
provides O
10,346 O
images O
(2,533 O

HICO-DET O
RPDCD O
HICO-DET O
HICO-DET O
RPT1CD O
V-COCO B-DAT
HICO-DET O
RPT2CD O
HICO-DET, O
V-COCO O
HICO-DET O

RCD O
- O
HICO-DET O
RCT O
- O
V-COCO B-DAT

V-COCO B-DAT
RPDCD O
V-COCO O
V-COCO O
RPT1CD O
HICO-DET O
V-COCO O
RPT2CD O
HICO-DET O

, O
V-COCO B-DAT
V-COCO O

V-COCO B-DAT
RCD O
- O
V-COCO O
RCT O
- O
HICO-DET O

13, O
12, O
21, O
9] O
on O
V-COCO B-DAT

and O
Known O
Object O
mode. O
For O
V-COCO, B-DAT
we O
evaluateAProle O
(24 O
actions O
with O

is O
accordingly O
reduced. O
Results O
on O
V-COCO B-DAT
are O
shown O
in O
Table O
3 O

4.0 O
and O
3.4 O
mAP O
on O
V-COCO B-DAT

38K) O
is O
much O
bigger O
than O
V-COCO B-DAT
train O
set O
(2.5K), O
improvement O
is O

per- O
formed O
from O
HICO-DET O
to O
V-COCO B-DAT

RPT1CD O
achieves O
obvious O
improvement O
on O
V-COCO, B-DAT
but O
it O
shows O
a O
relatively O

Table O
3. O
Results O
comparison O
on O
V-COCO B-DAT
[13]. O
D O
indicates O
the O
default O

V-COCO B-DAT
RPDCD O
-65.98% O
RPT1CD O
-59.51% O
RPT2CD O

HICO-DET O
V-COCO B-DAT
Method O
Default O
Full O
KO O
Full O

network O
on O
the O
Verb O
in O
COCO B-DAT
and O
HICO-DET O
datasets O
and O
show O

on O
HOI O
detection: O
Verbs O
in O
COCO B-DAT
(V-COCO) O
[16] O
and O
Humans O
Interacting O

COCO B-DAT
and O
49% O
on O
HICO-DET O
with O

COCO B-DAT
[16] O
is O
a O
subset O
of O

the O
COCO B-DAT
dataset O
[26] O
that O
provides O
HOI O

COCO B-DAT
includes O
a O
total O
of O
10,346 O

COCO B-DAT
and O
HICO O
datasets. O
The O
goal O

COCO B-DAT
trainval O
set O
with O
a O
learning O

COCO B-DAT
takes O
16 O
hours O
on O
a O

COCO B-DAT
test O
set O

COCO B-DAT
and O
HICO-DET O
datasets. O
Please O
refer O

COCO B-DAT
in O
Table O
1 O
and O
HICO-DET O

COCO, B-DAT
the O
proposed O
instance-centric O
attention O
network O

COCO B-DAT
dataset O
and O
the O
HICO-DET O
dataset O

COCO B-DAT
test O
set. O
Our O
model O
detects O

COCO B-DAT
test O
dataset O

COCO B-DAT
dataset. O
However, O
this O
comes O
at O

and O
C O
Lawrence O
Zitnick. O
Microsoft O
COCO B-DAT

C O
V B-DAT

HOI O
detection: O
Verbs B-DAT
in O
COCO O
(V O

around O
10% O
relative O
improvement O
on O
V B-DAT

Datasets. O
V B-DAT

26] O
that O
provides O
HOI O
annotations. O
V B-DAT

role O
mAP) O
[16] O
for O
both O
V B-DAT

for O
300K O
iterations O
on O
the O
V B-DAT

0.9. O
Training O
our O
network O
on O
V B-DAT

with O
the O
state-of-the-arts O
on O
the O
V B-DAT

and O
inference O
procedures O
for O
both O
V B-DAT

in O
terms O
of O
AProle O
on O
V B-DAT

HICO-DET O
in O
Table O
2. O
For O
V B-DAT

HOI O
detection O
results O
on O
the O
V B-DAT

Sample O
HOI O
detections O
on O
the O
V B-DAT

3: O
Ablation O
study O
on O
the O
V B-DAT

achieves O
the O
best O
performance O
on O
V B-DAT

HOI O
detection: O
Verbs O
in O
COCO O
(V-COCO) B-DAT
[16] O
and O
Humans O
Interacting O
with O

around O
10% O
relative O
improvement O
on O
V-COCO B-DAT
and O
49% O
on O
HICO-DET O
with O

Datasets. O
V-COCO B-DAT
[16] O
is O
a O
subset O
of O

26] O
that O
provides O
HOI O
annotations. O
V-COCO B-DAT
includes O
a O
total O
of O
10,346 O

role O
mAP) O
[16] O
for O
both O
V-COCO B-DAT
and O
HICO O
datasets. O
The O
goal O

for O
300K O
iterations O
on O
the O
V-COCO B-DAT
trainval O
set O
with O
a O
learning O

0.9. O
Training O
our O
network O
on O
V-COCO B-DAT
takes O
16 O
hours O
on O
a O

with O
the O
state-of-the-arts O
on O
the O
V-COCO B-DAT
test O
set O

and O
inference O
procedures O
for O
both O
V-COCO B-DAT
and O
HICO-DET O
datasets. O
Please O
refer O

in O
terms O
of O
AProle O
on O
V-COCO B-DAT
in O
Table O
1 O
and O
HICO-DET O

in O
Table O
2. O
For O
V-COCO, B-DAT
the O
proposed O
instance-centric O
attention O
network O

HOI O
detection O
results O
on O
the O
V-COCO B-DAT
dataset O
and O
the O
HICO-DET O
dataset O

Sample O
HOI O
detections O
on O
the O
V-COCO B-DAT
test O
set. O
Our O
model O
detects O

3: O
Ablation O
study O
on O
the O
V-COCO B-DAT
test O
dataset O

achieves O
the O
best O
performance O
on O
V-COCO B-DAT
dataset. O
However, O
this O
comes O
at O

COCO, B-DAT
and O
CAD-120 O
datasets. O
Our O
approach O

COCO B-DAT
[17] O
and O
CAD-120 O
[22], O
for O

COCO) B-DAT
and O
HOI O
recognition O
and O
anticipation O

COCO B-DAT
[17] O
datasets, O
showing O
that O
our O

COCO B-DAT
[17] O
datasets O
for O
benchmarking O
our O

COCO B-DAT
[28] O
and O
117 O
action O
categories O

COCO B-DAT
is O
a O
subset O
of O
MS-COCO O

COCO B-DAT
[17] O
dataset. O
Legend: O
Set O
1 O

COCO B-DAT
[17] O
test O
images. O
Human O
and O

COCO, B-DAT
respectively O

settings O
in O
HICO-DET O
and O
V- O
COCO B-DAT
benchmarks, O
we O
evaluate O
HOI O
detection O

COCO B-DAT
dataset, O
since O
we O
concentrate O
on O

COCO B-DAT
dataset O
(in O
Table O
2) O
also O

COCO B-DAT
[17] O
HICO-DET O
[1] O
CAD-120 O
[22 O

on O
images O
and O
videos: O
HICO-DET, O
V B-DAT

C O
V B-DAT

HOI O
datasets, O
namely O
HICO-DET O
[1], O
V B-DAT

detection O
from O
im- O
ages O
(HICO-DET, O
V B-DAT

Formally, O
let O
G O
= O
(V, B-DAT
E O
,Y) O
denote O
the O
complete O

HOI O
graph. O
Nodes O
v O
∈ O
V B-DAT
take O
unique O
values O
from O
{1 O

V B-DAT

e O
= O
(v, O
w) O
∈ O
V B-DAT
× O
V. O
Each O
node O
v O

of O
G, O
where O
Vg B-DAT
⊆ O
V O
and O
Eg O
⊆ O
E O

V B-DAT

V B-DAT

complete O
HOI O
graph O
G O
= O
(V, B-DAT
E O
,Y), O
we O
use O
dV O

V B-DAT
|×|V O
|×(2dV O
+dE) O
(see O
in O

V B-DAT

V B-DAT

hw, O
Γvw) O
= O
[W O
M O
V B-DAT
hv,W O

M O
V B-DAT
hw,W O

performed O
on O
HICO-DET O
[1] O
and O
V B-DAT

We O
use O
HICO-DET O
[1] O
and O
V B-DAT

28] O
and O
117 O
action O
categories. O
V B-DAT

HOI O
detection O
results O
(mAP) O
on O
V B-DAT

4. O
HOI O
detection O
results O
on O
V B-DAT

FC(dV B-DAT
-26)-Sigmoid(·) O
for O
HICO-DET O
and O
V O

standard O
settings O
in O
HICO-DET O
and O
V B-DAT

more O
training O
instances O
(Non-Rare). O
For O
V B-DAT

DET O
dataset. O
The O
results O
on O
V B-DAT

V B-DAT

8. O
Delaitre, O
V B-DAT

Jayasumana, O
S., O
Romera-Paredes, O
B., O
Vineet, B-DAT
V O

on O
images O
and O
videos: O
HICO-DET, O
V-COCO, B-DAT
and O
CAD-120 O
datasets. O
Our O
approach O

HOI O
datasets, O
namely O
HICO-DET O
[1], O
V-COCO B-DAT
[17] O
and O
CAD-120 O
[22], O
for O

detection O
from O
im- O
ages O
(HICO-DET, O
V-COCO) B-DAT
and O
HOI O
recognition O
and O
anticipation O

performed O
on O
HICO-DET O
[1] O
and O
V-COCO B-DAT
[17] O
datasets, O
showing O
that O
our O

We O
use O
HICO-DET O
[1] O
and O
V-COCO B-DAT
[17] O
datasets O
for O
benchmarking O
our O

28] O
and O
117 O
action O
categories. O
V-COCO B-DAT
is O
a O
subset O
of O
MS-COCO O

HOI O
detection O
results O
(mAP) O
on O
V-COCO B-DAT
[17] O
dataset. O
Legend: O
Set O
1 O

4. O
HOI O
detection O
results O
on O
V-COCO B-DAT
[17] O
test O
images. O
Human O
and O

FC(dV O
-26)-Sigmoid(·) O
for O
HICO-DET O
and O
V-COCO, B-DAT
respectively O

more O
training O
instances O
(Non-Rare). O
For O
V-COCO B-DAT
dataset, O
since O
we O
concentrate O
on O

DET O
dataset. O
The O
results O
on O
V-COCO B-DAT
dataset O
(in O
Table O
2) O
also O

V-COCO B-DAT
[17] O
HICO-DET O
[1] O
CAD-120 O
[22 O

neighboring O
entities O
and O
relationships O
from O
REVERB B-DAT
for O
some O
words O
from O
our O

learning O
for O
QA. O
Using O
the O
Reverb B-DAT
KB O
and O
QA O
datasets, O
we O

show O
that O
Reverb B-DAT
facts O
can O
be O
added O
to O

We O
also O
use O
the O
KB O
Reverb B-DAT
as O
a O
secondary O
source O
of O

FB2M O
FB5M O
Reverb B-DAT
ENTITIES O
2,150,604 O
4,904,397 O
2,044,752 O
RELATIONSHIPS O

be O
used O
to O
answer O
using O
Reverb B-DAT

with- O
out O
being O
trained O
on O
Reverb B-DAT
data. O
This O
is O
a O
pure O

setting O
of O
transfer O
learning. O
Reverb B-DAT
is O
interesting O
for O
this O
experiment O

3. O
Connecting O
Reverb B-DAT

adds O
new O
facts O
coming O
from O
Reverb B-DAT
to O
the O
memory. O
This O
is O

Input O
module O
to O
prepro- O
cess O
Reverb B-DAT
facts O
and O
the O
Generalization O
module O

system O
need O
to O
answer, O
and O
Reverb B-DAT
facts O
that O
we O
use, O
in O

Preprocessing O
Reverb B-DAT
facts O
In O
our O
experiments O
with O

Reverb, B-DAT
each O
fact O
y O
= O
(s O

the O
MemNNs O
by O
us- O
ing O
Reverb B-DAT

is O
then O
used O
to O
connect O
Reverb B-DAT
facts O
to O
the O
Freebase-based O
memory O

and O
the O
object O
of O
a O
Reverb B-DAT
fact O
to O
Freebase O
entities, O
we O

one O
alias O
that O
matches O
the O
Reverb B-DAT
entity O
string. O
These O
two O
processes O

lowed O
to O
match O
17% O
of O
Reverb B-DAT
entities O
to O
Free- O
base O
ones O

them O
to O
Freebase O
entities. O
All O
Reverb B-DAT
relation- O
ships O
were O
encoded O
using O

are O
able O
to O
store O
each O
Reverb B-DAT
fact O
as O
a O
bag-of-symbols O
(words O

be O
successfully O
used O
to O
query O
Reverb B-DAT
facts O

scoring O
a O
fact O
y O
from O
Reverb, B-DAT
we O
use O
the O
same O
embeddings O

WebQuestions O
SimpleQuestions O
Reverb B-DAT
TRAIN O
3,000 O
75,910 O
– O
VALID O

ours O
but O
are O
based O
on O
Reverb B-DAT
rather O
than O
Freebase, O
and O
relied O

sets O
of O
WebQuestions, O
SimpleQuestions O
and O
Reverb B-DAT
which O
we O
used O
for O
evalua O

The O
Reverb B-DAT
test O
set, O
based O
on O
the O

WebQuestions O
SimpleQuestions O
Reverb B-DAT
F1-SCORE O
(%) O
ACCURACY O
(%) O
ACCURACY O

MEMORY O
NETWORKS O
(never O
trained O
on O
Reverb B-DAT
– O
only O
transfer) O
KB O
TRAIN O

Transfer O
learning O
on O
Reverb B-DAT
In O
this O
set O
of O
ex O

- O
periments, O
all O
Reverb B-DAT
facts O
are O
added O
to O
the O

our O
model O
without O
training O
on O
Reverb B-DAT
against O
meth- O
ods O
specifically O
developed O

impact O
on O
the O
performance O
on O
Reverb B-DAT

and O
are O
well O
formed, O
while O
Reverb B-DAT
questions O
have O
more O
syntactic O
and O

while O
the O
largest O
existing O
benchmark, O
WebQuestions, B-DAT
con- O
tains O
less O
than O
6k O

lent O
results O
on O
the O
benchmark O
WebQuestions B-DAT

QA O
task O
on O
Freebase. O
On O
WebQuestions, B-DAT
a O
benchmark O
not O
primarily O
designed O

entities O
not O
appearing O
in O
either O
WebQuestions B-DAT
or O
SimpleQuestions. O
Statistics O
of O
FB2M O

WebQuestions B-DAT
SimpleQuestions O
Reverb O
TRAIN O
3,000 O
75,910 O

of O
the O
test O
sets O
of O
WebQuestions, B-DAT
SimpleQuestions O
and O
Reverb O
which O
we O

used O
for O
evalua- O
tion. O
On O
WebQuestions, B-DAT
we O
evaluate O
against O
previous O
results O

to O
maximize O
the O
F1-score O
on O
WebQuestions B-DAT
validation O
set, O
in- O
dependently O
of O

4. O
On O
the O
main O
benchmark O
WebQuestions, B-DAT
our O
best O
re- O
sults O
use O

WebQuestions B-DAT
SimpleQuestions O
Reverb O
F1-SCORE O
(%) O
ACCURACY O

SIQ O
and O
PRP O
stand O
for O
WebQuestions, B-DAT
SimpleQuestions O
and O
Paraphrases O
respectively. O
More O

their O
result O
(35.3% O
F1-score O
on O
WebQuestions) B-DAT
should O
be O
compared O
to O
our O

to O
train O
on O
FB5M. O
On O
WebQuestions, B-DAT
not O
specifically O
designed O
as O
a O

for O
the O
model O
trained O
on O
WebQuestions B-DAT
only), O
which O
shows O
that O
the O

seem O
to O
help O
much O
on O
WebQuestions B-DAT
and O
SimpleQuestions, O
ex- O
cept O
when O

on O
Reverb. O
This O
is O
because O
WebQuestions B-DAT
and O
SimpleQuestions O
questions O
follow O
simple O

state-of-the-art O
on O
the O
popular O
benchmark O
WebQuestions B-DAT

order O
of O
magnitude O
bigger O
than O
WebQuestions B-DAT

We O
use O
WebQuestions B-DAT
[1] O
as O
our O
evaluation O
bemchmark O

WebQuestions B-DAT
This O
dataset O
is O
built O
using O

WebQuestions B-DAT
– O
Train. O
ex. O
2,778 O

was O
appearing O
in O
either O
the O
WebQuestions B-DAT
training/validation O
set O
or O
in O
ClueWeb O

1 O
WebQuestions B-DAT
contains O
∼2k O
entities, O
hence O
restricting O

WebQuestions B-DAT
what O
is O
henry O
clay O
known O

Table O
3. O
Results O
on O
the O
WebQuestions B-DAT
test O
set O

have O
been O
selected O
on O
the O
WebQuestions B-DAT
valida O

performance O
on O
the O
competitive O
benchmark O
WebQuestions B-DAT

answers O
for O
questions O
from O
the O
WebQuestions B-DAT
test O
set, O
among O
the O
whole O

5.3 O
Evaluation O
on O
WebQuestions B-DAT

We O
chose O
the O
data O
set O
WebQuestions B-DAT
[3], O
which O
consists O
of O
natural O

from O
the O
test O
set O
of O
WebQuestions B-DAT
and O
obtained O
1,538 O
questions O
labeled O

is O
that O
most O
questions O
of O
WebQuestions, B-DAT
such O
as O
Who O
was O
vice-president O

5.3 O
Evaluation O
on O
WebQuestions B-DAT

based O
on O
a O
KB, O
called O
SimpleQuestions B-DAT

extracted O
from O
the O
new O
dataset O
SimpleQuestions B-DAT
introduced O
in O
this O
paper. O
Actual O

2.2 O
The O
SimpleQuestions B-DAT
dataset O

task O
of O
simple O
QA O
called O
SimpleQuestions B-DAT

We O
collected O
SimpleQuestions B-DAT
in O
two O
phases. O
The O
first O

in O
addition O
to O
the O
new O
SimpleQuestions B-DAT
dataset O
described O
in O
Section O
2 O

appearing O
in O
either O
WebQuestions O
or O
SimpleQuestions B-DAT

Unlike O
for O
SimpleQuestions B-DAT
or O
the O
synthetic O
QA O
data O

WebQuestions O
SimpleQuestions B-DAT
Reverb O
TRAIN O
3,000 O
75,910 O

the O
test O
sets O
of O
WebQuestions, O
SimpleQuestions B-DAT
and O
Reverb O
which O
we O
used O

previous O
result O
was O
published O
on O
SimpleQuestions, B-DAT
we O
only O
compare O
different O
versions O

of O
MemNNs. O
SimpleQuestions B-DAT
questions O
are O
labeled O
with O
their O

WebQuestions O
SimpleQuestions B-DAT
Reverb O
F1-SCORE O
(%) O
ACCURACY O

and O
PRP O
stand O
for O
WebQuestions, O
SimpleQuestions B-DAT
and O
Paraphrases O
respectively. O
More O
details O

model O
(41.2%). O
On O
the O
new O
SimpleQuestions B-DAT
dataset, O
the O
best O
models O
achieve O

set O
for O
about O
86% O
of O
SimpleQuestions B-DAT
questions. O
This O
shows O
that O
MemNNs O

does O
not O
change O
performance O
on O
SimpleQuestions B-DAT
because O
it O
was O
based O
on O

help O
much O
on O
WebQuestions O
and O
SimpleQuestions, B-DAT
ex- O
cept O
when O
training O
only O

This O
is O
because O
WebQuestions O
and O
SimpleQuestions B-DAT
questions O
follow O
simple O
pat- O
terns O

also O
introduced O
the O
new O
dataset O
SimpleQuestions, B-DAT
which, O
with O
100k O
examples, O
is O

2.2 O
The O
SimpleQuestions B-DAT
dataset O

AMR O
2.0 O
(76.3% O
F1 O
on O
LDC2017T10) B-DAT
and O
AMR O
1.0 O
(70.2% O
F1 O

SMATCH O
scores: O
76.3% O
F1 O
on O
LDC2017T10 B-DAT
and O
70.2% O
F1 O
on O
LDC2014T12 O

all O
LDC O
subscribers): O
AMR O
2.0 O
(LDC2017T10) B-DAT
and O
AMR O
1.0 O
(LDC2014T12). O
Our O

large-scale O
activity O
benchmarks, O
such O
as O
HICO B-DAT
[13], O
HICO- O
DET O
[12], O
AVA O

existing O
well-designed O
datasets, O
for O
example, O
HICO B-DAT

16], O
OpenImage O
[17], O
HCVRD O
[22], O
HICO B-DAT
[13], O
MPII O
[1], O
AVA O
[10 O

just O
report O
the O
results O
on O
HICO B-DAT
[13] O
to O
evaluate O
the O
im O

HICO B-DAT
[13] O
contains O
38,116 O
images O
in O

Comparison O
with O
previous O
methods O
on O
HICO B-DAT

means O
one-shot O
prob- O
lem. O
On O
HICO B-DAT
[13], O
there O
is O
obvious O
positive O

over O
the O
state-of-the-art O
result O
on O
HICO B-DAT
[13]. O
And O
when O
the O
part O

achieve O
surprising O
62.5 O
mAP O
on O
HICO B-DAT
dataset. O
That O
is, O
if O
we O

than O
5 O
and O
10) O
of O
HICO, B-DAT
our O
method O
can O
achieve O
more O

in O
HOI O
recognition O
on O
the O
HICO B-DAT
dataset. O
We O
will O
make O
our O

evaluate O
our O
model O
on O
the O
HICO B-DAT
dataset O
[5] O
and O
the O
MPII O

10% O
relatively O
in O
mAP O
on O
HICO B-DAT
dataset O

the O
predefined O
list. O
In O
the O
HICO B-DAT
dataset, O
there O
can O
be O
multiple O

Handling O
Multiple O
Objects O
In O
HICO B-DAT
dataset, O
there O
can O
be O
multiple O

two O
frequently O
used O
datasets, O
namely, O
HICO B-DAT
and O
MPII O
dataset. O
HICO O
dataset O

and O
5708 O
test O
images. O
Unlike O
HICO B-DAT
dataset, O
all O
person O
instances O
in O

HICO B-DAT
We O
use O
Faster O
RCNN O
[34 O

the O
HOI O
labels O
in O
the O
HICO B-DAT
dataset O
are O
highly O
imbalanced, O
we O

Similar O
to O
the O
setting O
for O
HICO B-DAT
dataset, O
we O
sample O
a O
maximum O

with O
previous O
results O
on O
the O
HICO B-DAT
test O
set. O
The O
result O
of O

two O
rows O
are O
results O
from O
HICO B-DAT
dataset O
and O
the O
last O
row O

We O
compare O
our O
performance O
on O
HICO B-DAT
testing O
set O
in O
Table O
1 O

model O
achieves O
37.6 O
mAP O
on O
HICO B-DAT
testing O
set O
and O
36.8 O
mAP O

ther O
achieve O
39.9 O
mAP O
on O
HICO B-DAT
testing O
set. O
Since O
[10] O
use O

better O
performance O
than O
[10] O
on O
HICO B-DAT
and O
MPII O
dataset, O
and O
by O

we O
conduct O
several O
experiments O
on O
HICO B-DAT
dataset O
and O
the O
results O
are O

the O
baseline O
networks O
on O
the O
HICO B-DAT
test O
set. O
“union O
box” O
refers O

body O
part O
attention O
model O
on O
HICO B-DAT
training O
set O
with O
different O
value O

20 O
randomly O
picked O
HOIs O
in O
HICO B-DAT
dataset O
with O
and O
without O
the O

randomly O
pick O
20 O
categories O
in O
HICO B-DAT
dataset O
and O
compare O
our O
results O

train O
deep O
networks O
on O
the O
HICO B-DAT
[16] O
and O
MPII O
[17] O
datasets O

must O
be O
automatically O
detected O
in O
HICO B-DAT

HICO B-DAT
Dataset O

We O
train O
CNNs O
on O
the O
HICO B-DAT
and O
MPII O
datasets O
to O
predict O

dataset O
and O
detected O
in O
the O
HICO B-DAT
dataset. O
We O
then O
use O
these O

Humans O
Interacting O
with O
Common O
Objects O
(HICO) B-DAT
dataset O
[16]. O
The O
number O
of O

fit O
for O
general O
VQA. O
The O
HICO B-DAT
dataset O
is O
currently O
the O
largest O

categories. O
Each O
category O
in O
the O
HICO B-DAT
dataset O
is O
composed O
of O
a O

CNNs O
with O
simple O
architectures O
on O
HICO B-DAT
and O
MPII O
datasets, O
and O
show O

One O
limitation O
of O
the O
HICO B-DAT
dataset O
is O
that O
it O
provides O

In O
the O
HICO B-DAT
dataset, O
if O
at O
least O
one O

an O
unbalanced O
dataset. O
For O
the O
HICO B-DAT
dataset, O
we O
have O
to O
learn O

two O
different O
activity O
classification O
datasets: O
HICO B-DAT
[16] O
and O
the O
MPII O
Human O

Pose O
Dataset O
[17]. O
The O
HICO B-DAT
dataset O
contains O
labels O
for O
600 O

are O
not O
provided O
with O
the O
HICO B-DAT
dataset, O
we O
run O
the O
Faster-RCNN O

wrong O
or O
missing O
annotations. O
The O
HICO B-DAT
training O
set O
contains O
38,116 O
images O

for O
393 O
actions. O
Unlike O
in O
HICO, B-DAT
each O
image O
only O
has O
a O

has O
5,709 O
images. O
Similar O
to O
HICO, B-DAT
the O
training O
set O
is O
unbalanced O

of O
various O
networks O
on O
the O
HICO B-DAT
person-activity O
dataset. O
Note O
that O
usage O

HICO B-DAT
Results. O
On O
the O
HICO O
dataset, O
we O
compare O
the O
networks O

1 O
presents O
our O
comparison. O
As O
HICO B-DAT
is O
fairly O
new, O
the O
only O

of O
training O
data O
than O
in O
HICO B-DAT

best- O
performing O
network O
on O
the O
HICO B-DAT
dataset. O
In O
spite O
of O
the O

of O
our O
system O
on O
the O
HICO B-DAT
dataset. O
Unusual O
use-cases O
of O
an O

our O
Fusion-2 O
model O
on O
the O
HICO B-DAT
test O
set. O
Detected O
person O
instances O

Fig. O
4: O
Failure O
examples O
on O
HICO B-DAT

HICO B-DAT

68.74 O
72.06 O
77.25 O
54.10 O
59.77 O
HICO B-DAT

HICO B-DAT

HICO B-DAT

full-image O
network O
trained O
on O
the O
HICO B-DAT
dataset, O
we O
obtain O
gains O
of O

our O
Fusion-2 O
network O
trained O
on O
HICO B-DAT
(row O
5) O
help O
improve O
the O

class O
label O
predictions O
from O
both O
HICO B-DAT
and O
MPII O
(last O
row O
of O

are O
complementary O
to O
those O
of O
HICO, B-DAT
especially O
in O
the O
cases O
when O

picture’) O
is O
absent O
from O
the O
HICO B-DAT
and O
MPII O
datasets O
so O
the O

single-frame O
models) O
and O
competitive O
on O
HICO B-DAT

image O
based O
datasets O
such O
as O
HICO B-DAT
[7] O
and O
MPII O
[34] O
are O

images O
and O
videos, O
namely O
MPII, O
HICO B-DAT
and O
HMDB51. O
MPII O
Human O
Pose O

to O
equally O
weight O
all O
classes. O
HICO B-DAT
[7] O
is O
a O
recently O
introduced O

HICO B-DAT

We O
train O
our O
model O
on O
HICO B-DAT
similar O
to O
MPII, O
and O
compare O

Multi-label O
HOI O
classification O
performance O
on O
HICO B-DAT
dataset. O
The O
top-half O
compares O
our O

not O
help O
as O
much O
on O
HICO B-DAT
or O
MPII; O
HICO O
and O
MPII O

results O
on O
the O
publicly O
available O
ECG5000 B-DAT
electrocardio- O
gram O
dataset O
show O
the O

with O
our O
model O
in O
electrocardiogram O
(ECG) B-DAT
time O
series. O
Our O
contributions O
in O

problem O
of O
finding O
sequences O
(e.g., O
ECG B-DAT
heartbeats) O
that O
do O
not O
conform O

neural O
network O
for O
classification O
of O
ECG B-DAT
time O
series. O
Vig O
et O
al O

networks O
for O
anomaly O
detection O
in O
ECG B-DAT
data. O
Malhotra O
et O
al. O
[15 O

the O
proposed O
model O
to O
electrocardiogram O
(ECG) B-DAT
time O
series O
data. O
The O
dataset O

is O
the O
ECG5000, B-DAT
which O
was O
donated O
by O
Eamonn O

II O
RESULTS O
OBTAINED O
ON O
THE O
ECG5000 B-DAT
DATASET O

L. O
Vig, O
“Anomaly O
Detection O
in O
ECG B-DAT
Time O
Signals O
via O
Deep O
Long O

results O
on O
the O
publicly O
available O
ECG5000 B-DAT
electrocardio- O
gram O
dataset O
show O
the O

data. O
The O
dataset O
is O
the O
ECG5000, B-DAT
which O
was O
donated O
by O
Eamonn O

II O
RESULTS O
OBTAINED O
ON O
THE O
ECG5000 B-DAT
DATASET O

0.8099 O
1.0000* O
0.8070 O
0.8070 O
CinC O
ECG B-DAT
0.9949 O
0.8862 O
0.9094 O
0.9058 O
0.9058 O

ECG200 B-DAT
0.9200 O
0.9000 O
0.9200* O
0.9100 O
0.9200 O

ECG5000 B-DAT
0.9482 O
0.9473 O
0.9478 O
0.9484 O
0.9496 O

0.9200 O
0.9000 O
0.9200* O
0.9100 O
0.9200 O
ECG5000 B-DAT
0.9482 O
0.9473 O
0.9478 O
0.9484 O
0.9496 O

de- O
mand, O
space O
shuttle, O
and O
ECG, B-DAT
and O
two O
real- O
world O
engine O

ECG B-DAT
Yes O
1 O
Quasi-periodic O
1 O
215 O

ECG B-DAT
208 O
45 O
0.05 O
1.0 O
0.005 O

power O
demand, O
space O
shuttle O
valve, O
ECG, B-DAT
and O
engine O
(see O
Table O
1 O

i) O
ECG-N B-DAT
(j) O
ECG O

Engine-P O
and O
61% O
for O
Engine-NP. O
ECG B-DAT
dataset O
contains O
quasi-periodic O
time-series O
(duration O

also O
consider O
a O
quasi-periodic O
time-series O
(ECG B-DAT

extracted O
from O
ECGs O
in O
the O
2017 B-DAT
Atrial O
Fibrillation O
challenge O
[20]. O
The O

network O
dynamics. O
Scientific O
reports, O
7:44037, O
2017 B-DAT

Overview O
and O
Comparative O
Analysis. O
Springer, O
2017 B-DAT

389–409. O
Springer O
International O
Publishing, O
Cham, O
2017 B-DAT

physionet O
computing O
in O
cardiology O
challenge O
2017 B-DAT

. O
Computing O
in O
Cardiology, O
2017 B-DAT

denoising O
autoencoders. O
arXiv O
preprint O
arXiv:1705.02737, O
2017 B-DAT

Image O
Analysis, O
pages O
419–430, O
Cham, O
2017 B-DAT

series O
prediction. O
arXiv O
preprint O
arXiv:1704.02971, O
2017 B-DAT

labeling O
tasks. O
arXiv O
preprint O
arXiv:1707.06799, O
2017 B-DAT

Pattern O
Recognition, O
63:397 O
– O
405, O
2017 B-DAT

The O
second O
medical O
dataset O
is O
Physionet, B-DAT
which O
contains O
time O
series O
of O

223 O
2 O
20 O
20 O
[47] O
Physionet B-DAT
2 O
8524 O
298 O
4 O
5 O

the O
real-world O
data O
from O
the O
Physionet B-DAT
dataset. O
By O
following O
a O
commonly O

from O
ECGs O
in O
the O
2017 O
Atrial B-DAT
Fibrillation O
challenge O
[20]. O
The O
MTS O

ECGs O
in O
the O
2017 O
Atrial O
Fibrillation B-DAT
challenge O
[20]. O
The O
MTS O
are O

a O
new O
state-of-the-art O
in O
the O
BUCC B-DAT
shared O
task O
for O
3 O
of O

ML- O
Doc O
dataset), O
bitext O
mining O
(BUCC B-DAT
dataset) O
and O
multilingual O
similarity O
search O

in O
different O
tasks. O
Some O
tasks O
(BUCC, B-DAT
MLDoc) O
tend O
to O
perform O
better O

4.2), O
and O
bi- O
text O
mining O
(BUCC, B-DAT
Section O
4.3). O
However, O
all O
these O

5: O
F1 O
scores O
on O
the O
BUCC B-DAT
mining O
task O

4.3 O
BUCC B-DAT

our O
sentence O
embeddings O
on O
the O
BUCC B-DAT
mining O
task O
(Zweigen- O
baum O
et O

While O
XNLI, O
MLDoc O
and O
BUCC B-DAT
are O
well O
estab- O
lished O
benchmarks O

Depth O
Tatoeba O
BUCC B-DAT
MLDoc O
XNLI-en O
XNLI-xxErr O
[%] O
F1 O

and O
Tatoeba. O
The O
effect O
in O
BUCC B-DAT
is O
negligible O

NLI O
Tatoeba O
BUCC B-DAT
MLDoc O
XNLI-en O
XNLI-xx O
obj. O
Err O

langs O
WMT O
BUCC B-DAT
MLDoc O
XNLI-en O
XNLI-xxErr O
[%] O
F1 O

MLDoc), O
and O
bitext O
min- O
ing O
(BUCC B-DAT

Alignment O
of O
Comparable O
Sentences. O
In O
BUCC, B-DAT
pages O
41–45 O

with O
STACC O
Vari- O
ants. O
In O
BUCC B-DAT

BUCC18 B-DAT

Using O
Multilingual O
Sentence O
Embeddings. O
In O
BUCC B-DAT

lel O
Sentence O
Identification O
Model. O
In O
BUCC B-DAT

Grégoire O
and O
Philippe O
Langlais. O
2017. O
BUCC B-DAT
2017 O
Shared O
Task: O
a O
First O

tences O
in O
Comparable O
Corpora. O
In O
BUCC, B-DAT
pages O
46– O
50 O

in O
Chinese-English O
Comparable O
Corpora. O
In O
BUCC, B-DAT
pages O
51–55 O

2017. O
Overview O
of O
the O
Second O
BUCC B-DAT
Shared O
Task: O
Spotting O
Parallel O
Sentences O

in O
Comparable O
Corpora. O
In O
BUCC, B-DAT
pages O
60–67 O

2018. O
Overview O
of O
the O
Third O
BUCC B-DAT
Shared O
Task: O
Spotting O
Parallel O
Sentences O

in O
Comparable O
Corpora. O
In O
BUCC B-DAT

the O
resulting O
sentence O
embeddings O
using O
English B-DAT
annotated O
data O
only, O
and O
transfer O

model O
from O
one O
language O
(e.g. O
English) B-DAT
to O
another, O
and O
the O
possi O

which O
usu- O
ally O
consider O
separate O
English B-DAT

pair- O
wise O
joint O
embeddings O
with O
English B-DAT
and O
one O
for- O
eign O
language O

3.10 O
4.30 O
1000 O
eng O
en O
English B-DAT
Germanic O
Latin O
2.6M O
n/a O
n/a O

two O
target O
languages. O
We O
choose O
English B-DAT
and O
Spanish O
for O
that O
purpose O

with O
one O
alignment O
only, O
usually O
English B-DAT

the O
well-established O
evaluation O
frameworks O
for O
English B-DAT
sentence O
representations O
(Conneau O
et O
al O

multi- O
lingual O
sentence O
embedding O
using O
English B-DAT
train- O
ing O
data, O
and O
evaluate O

a O
dataset O
similar O
to O
the O
English B-DAT
MultiNLI O
for O
several O
languages O
(Conneau O

sentences O
have O
been O
translated O
from O
English B-DAT
into O
14 O
languages O
by O
professional O

different O
systems O
are O
to O
use O
English B-DAT
training O
data O
from O
MultiNLI, O
and O

hyperparameters O
were O
optimized O
on O
the O
English B-DAT
XNLI O
development O
corpus, O
and O
then O

i.e. O
training O
a O
classifier O
on O
English B-DAT
data O
and O
applying O
it O
to O

lower O
than O
the O
one O
on O
English, B-DAT
including O
distant O
languages O
like O
Arabic O

achieves O
excel- O
lent O
results O
on O
English, B-DAT
outperforming O
our O
system O
by O
7.5 O

Translate O
test, O
one O
English B-DAT
NLI O
system: O
Conneau O
et O
al O

by O
aligning O
it O
to O
the O
English B-DAT
one O

translate O
the O
test O
data O
into O
English B-DAT
and O
apply O
the O
English O
NLI O

classifier, O
or O
2) O
translate O
the O
English B-DAT
training O
data O
and O
train O
a O

multilingual O
encoder O
us- O
ing O
the O
English B-DAT
training O
data, O
optimizing O
hyper- O
parameters O

on O
the O
English B-DAT
development O
set, O
and O
evaluating O
the O

a O
com- O
parable O
corpus O
between O
English B-DAT
and O
four O
foreign O
languages: O
German O

pairs O
with O
the O
exception O
of O
English B-DAT

models O
covering O
4 O
languages O
each O
(English B-DAT

/French/Spanish/German O
and O
English B-DAT

consists O
of O
up O
to O
1,000 O
English B-DAT

re- O
port O
the O
accuracy O
on O
English B-DAT

to O
be O
helpful O
to O
learn O
English B-DAT
sentence O
embeddings O
(Subrama O

a O
better O
performance O
on O
the O
English B-DAT
NLI O
test O
set, O
but O
this O

similarity O
error O
rate. O
This O
covers O
English, B-DAT
Czech, O
French, O
German O
and O
Spanish O

Parity O
on O
Automatic O
Chinese O
to O
English B-DAT
News O
Translation. O
arXiv:1803.05567 O

English B-DAT
Comparable O
Corpora. O
In O
BUCC, O
pages O

A O
community O
supported O
collection O
of O
English B-DAT
sentences O
and O
translations O
into O
more O

is O
an O
open O
collection O
of O
English B-DAT
sen- O
tences O
and O
high O
quality O

to O
1,000 O
aligned O
sentences O
with O
English B-DAT

stressed O
that, O
in O
general, O
the O
English B-DAT
sentences O
are O
not O
the O
same O

with O
less O
than O
20%, O
respectively O
(English B-DAT
included). O
The O
languages O
with O
less O

13.40 O
10.00 O
1000 O
ang O
Old O
English B-DAT
Germanic O
Latin O
none O
58.96 O
65.67 O

We O
introduce O
an O
architecture O
to B-DAT
learn O
joint O
multilingual O
sentence O
representations O

for O
93 O
languages, O
belonging O
to B-DAT
more O
than O
30 O
dif- O
ferent O

parallel O
corpora. O
This O
enables O
us O
to B-DAT
learn O
a O
classifier O
on O
top O

data O
only, O
and O
transfer O
it O
to B-DAT
any O
of O
the O
93 O
languages O

of O
deep O
learning O
has O
led O
to B-DAT
impressive O
progress O
in O
Natural O
Language O

NLP), O
these O
techniques O
are O
known O
to B-DAT
be O
par- O
ticularly O
data O
hungry O

scenarios. O
An O
increasingly O
popular O
approach O
to B-DAT
alleviate O
this O
issue O
is O
to O

language O
and O
are O
thus O
unable O
to B-DAT
leverage O
information O
across O
different O
languages O

are O
gen- O
eral O
with O
respect O
to B-DAT
two O
dimensions: O
the O
input O
lan O

over O
many O
languages, O
the O
desire O
to B-DAT
per- O
form O
zero-shot O
transfer O
of O

from O
one O
language O
(e.g. O
English) O
to B-DAT
another, O
and O
the O
possi- O
bility O

to B-DAT
handle O
code-switching. O
We O
achieve O
this O

languages, O
and O
is O
usually O
limited O
to B-DAT
a O
few O
(most O
often O
only O

substantially O
improve O
on O
previous O
work O
to B-DAT
learn O
joint O
multilingual O
sentence O
represen O

a O
shared O
space, O
in O
contrast O
to B-DAT
most O
other O
works O
which O
usu O

train O
the O
entire O
system O
end-to-end B-DAT
to O
predict O
the O
surrounding O
sentences O
over O

This O
was O
recently O
ex- O
tended O
to B-DAT
multitask O
learning, O
combining O
different O
training O

becom- O
ing O
increasingly O
popular O
is O
to B-DAT
train O
word O
embed- O
dings O
independently O

corpora, O
and O
then O
map O
them O
to B-DAT
a O
shared O
space O
based O
on O

em- O
beddings O
are O
often O
used O
to B-DAT
build O
bag-of-word O
rep- O
resentations O
of O

that O
we O
follow O
here O
is O
to B-DAT
use O
a O
sequence-to-sequence O
encoder- O
decoder O

end-to-end B-DAT
on O
parallel O
corpora O
akin O
to O
neural O
ma- O
chine O
translation: O
the O

is O
used O
by O
the O
decoder O
to B-DAT
create O
the O
target O
sequence. O
This O

and O
the O
encoder O
is O
kept O
to B-DAT
embed O
sentences O
in O
any O
of O

work O
is O
either O
lim- O
ited O
to B-DAT
few, O
rather O
close O
languages O
(Schwenk O

number O
of O
languages O
is O
limited O
to B-DAT
word O
embeddings O
(Ammar O
et O
al O

1: O
Architecture O
of O
our O
system O
to B-DAT
learn O
multilingual O
sentence O
embeddings O

language O
agnostic O
BiLSTM O
en- O
coder O
to B-DAT
build O
our O
sentence O
embeddings, O
which O

parallel O
corpora. O
From O
Section O
3.1 O
to B-DAT
3.3, O
we O
describe O
its O
architecture O

, O
our O
training O
strategy O
to B-DAT
scale O
to O
up O
to O
93 O

to B-DAT

morphologically O
complex O
language O
might O
correspond O
to B-DAT
several O
words O
of O
a O
morphologically O

sentence O
em- O
beddings O
are O
used O
to B-DAT
initialize O
the O
decoder O
LSTM O
through O

and O
are O
also O
con- O
catenated O
to B-DAT
its O
input O
embeddings O
at O
every O

information O
of O
the O
input O
sequence O
to B-DAT
be O
captured O
by O
the O
sentence O

input O
language O
is, O
encouraging O
it O
to B-DAT
learn O
language O
independent O
representations. O
In O

ding O
that O
specifies O
the O
language O
to B-DAT
generate, O
which O
is O
concatenated O
to O

Scaling O
up O
to B-DAT
almost O
hundred O
languages, O
which O
use O

paper, O
we O
limit O
our O
study O
to B-DAT
a O
stacked O
BiLSTM O
with O
1 O

to B-DAT
5 O
layers, O
each O
512-dimensional. O
The O

embed- O
ding O
size O
is O
set O
to B-DAT
320, O
while O
the O
language O
ID O

While O
this O
approach O
was O
shown O
to B-DAT
learn O
high-quality O
representations, O
it O
poses O

two O
obvious O
drawbacks O
when O
trying O
to B-DAT
scale O
to O
a O
large O
number O

out O
of O
93 O
languages O
used O
to B-DAT
trained O
the O
proposed O
model O
with O

cor- O
pus, O
which O
is O
difficult O
to B-DAT
obtain O
for O
all O
languages. O
Second O

a O
quadratic O
cost O
with O
respect O
to B-DAT
the O
number O
of O
languages, O
making O

target O
languages O
– O
two O
seem O
to B-DAT
be O
enough.2 O
At O
the O
same O

not O
require O
each O
source O
sentence O
to B-DAT
be O
translated O
into O
the O
two O

of O
0.001 O
and O
dropout O
set O
to B-DAT
0.1, O
and O
train O
for O
a O

use O
of O
its O
multi-GPU O
support O
to B-DAT
train O
on O
16 O
NVIDIA O
V100 O

target O
language, O
the O
only O
way O
to B-DAT
train O
the O
encoder O
for O
that O

auto- B-DAT
encoding, O
which O
we O
observe O
to O
work O
poorly. O
Having O
two O
tar O

Some O
tasks O
(BUCC, O
MLDoc) O
tend O
to B-DAT
perform O
better O
when O
the O
encoder O

informal O
sentences. O
In O
an O
attempt O
to B-DAT
achieve O
a O
general O
purpose O
sentence O

yet O
a O
commonly O
accepted O
standard O
to B-DAT
eval- O
uate O
multilingual O
sentence O
embeddings O

an O
NLI O
test O
set O
similar O
to B-DAT
MultiNLI O
(Williams O
et O
al., O
2017 O

languages O
(Section O
4.1). O
So O
as O
to B-DAT
obtain O
a O
more O
complete O
picture O

become O
a O
widely O
used O
task O
to B-DAT
evaluate O
sentence O
representations O
(Bowman O
et O

XNLI O
is O
a O
recent O
effort O
to B-DAT
create O
a O
dataset O
similar O
to O

provided; O
instead, O
different O
systems O
are O
to B-DAT
use O
English O
training O
data O
from O

the O
same O
classifier O
was O
applied O
to B-DAT
all O
languages O
of O
the O
XNLI O

English O
data O
and O
applying O
it O
to B-DAT
all O
other O
lan- O
guages) O
for O

Conneau O
et O
al. O
(2018c) O
correspond O
to B-DAT
max-pooling, O
which O
outperforms O
the O
last-state O

points O
for O
our O
system, O
compared O
to B-DAT
19.3 O
and O
17.6 O
points O
for O

lan- O
guage O
by O
aligning O
it O
to B-DAT
the O
English O
one O

is O
worth O
mentioning O
that, O
thanks O
to B-DAT
its O
multilingual O
nature, O
our O
system O

multilingual O
representations. O
In O
or- O
der O
to B-DAT
evaluate O
our O
sentence O
embeddings O
in O

on O
Japanese O
can O
be O
attributed O
to B-DAT
the O
domain O
and O
sentence O
length O

notion O
of O
margin O
is O
related O
to B-DAT
CSLS O
as O
proposed O
in O
Conneau O

2018a). O
The O
reader O
is O
referred O
to B-DAT
Artetxe O
and O
Schwenk O
(2018) O
for O

We O
use O
this O
method O
to B-DAT
evaluate O
our O
sentence O
embeddings O
on O

Schwenk O
(2018). O
The O
goal O
is O
to B-DAT
extract O
parallel O
sentences O
from O
a O

The O
dataset O
consists O
of O
150K O
to B-DAT
1.2M O
sentences O
for O
each O
language O

four O
languages O
increased O
from O
93.27 O
to B-DAT
93.92. O
Not O
only O
are O
our O

it O
can O
potentially O
be O
used O
to B-DAT
mine O
bi- O
text O
for O
any O

guages. O
So O
as O
to B-DAT
better O
assess O
the O
performance O
of O

The O
dataset O
consists O
of O
up O
to B-DAT
1,000 O
English-aligned O
sentence O
pairs O
for O

in O
the O
other O
language O
according O
to B-DAT
cosine O
similarity O
and O
computing O
the O

in O
Section O
5.3. O
In O
relation O
to B-DAT
that, O
Appendix O
E O
reports O
similarity O

our O
encoder O
can O
also O
generalize O
to B-DAT
unseen O
languages O
to O
some O
extent O

layers. O
We O
were O
not O
able O
to B-DAT
achieve O
good O
convergence O
with O
deeper O

STM O
has O
not O
enough O
capacity O
to B-DAT
encode O
so O
many O
languages O

Multitask O
learning O
has O
been O
shown O
to B-DAT
be O
helpful O
to O
learn O
English O

adding O
an O
additional O
NLI O
objective O
to B-DAT
our O
system O
with O
different O
weighting O

7, O
the O
NLI O
objective O
leads O
to B-DAT
a O
better O
performance O
on O
the O

So O
as O
to B-DAT
better O
understand O
how O
our O
architecture O

scales O
to B-DAT
a O
large O
amount O
of O
languages O

lan- O
guages, O
and O
compare O
it O
to B-DAT
our O
main O
model O
trained O
on O

the O
WMT O
2014 O
test O
set O
to B-DAT
evaluate O
the O
multi- O
lingual O
similarity O

the O
joint O
training O
also O
yields O
to B-DAT
overall O
better O
rep- O
resentations O

paper, O
we O
propose O
an O
architecture O
to B-DAT
learn O
multilingual O
sentence O
embeddings O
for O

available O
parallel O
corpora O
and O
applied O
to B-DAT
different O
downstream O
tasks O
without O
any O

the O
future, O
we O
would O
like O
to B-DAT
explore O
alterna- O
tive O
architectures O
for O

encoder. O
In O
particular, O
we O
plan O
to B-DAT
replace O
our O
BiLSTM O
with O
the O

former, O
which O
has O
been O
shown O
to B-DAT
work O
better O
in O
different O
settings O

2018). O
Moreover, O
we O
would O
like O
to B-DAT
explore O
possible O
strategies O
to O
exploit O

train- O
ing O
data O
in O
addition O
to B-DAT
parallel O
corpora, O
such O
as O
using O

2018). O
Finally, O
we O
would O
like O
to B-DAT
replace O
our O
language- O
specific O
tokenization O

a O
language O
agnostic O
approach O
similar O
to B-DAT
Sentence- O
Piece.11 O

Human O
Parity O
on O
Automatic B-DAT
Chinese O
to O
English O
News O
Translation. O
arXiv:1803.05567 O

The O
size O
varies O
from O
400k O
to B-DAT
2M O
sentence O
pairs, O
in O
function O

e.g. O
Ar- O
menian O
or O
Kazakh) O
to B-DAT
more O
than O
50 O
million O
(e.g O

We O
use O
this O
cor- O
pus O
to B-DAT
extract O
a O
separate O
test O
set O

of O
up O
to B-DAT
1,000 O
sentences O
for O
many O
languages O

only O
kept O
93 O
different O
languages O
to B-DAT
train O
the O
multilingual O
sentence O
embeddings O

numbers O
in O
the O
diagonal O
correspond O
to B-DAT
the O
main O
results O
reported O
in O

observe O
that O
our O
approach O
seems O
to B-DAT
han- O
dle O
the O
combination O
of O

created O
test O
sets O
of O
up O
to B-DAT
1,000 O
aligned O
sentences O
with O
English O

iting O
the O
number O
of O
sentences O
to B-DAT
500, O
we O
increase O
the O
coverage O

to B-DAT
101 O
languages, O
and O
even O
141 O

less O
than O
20% O
error O
belong O
to B-DAT
20 O
different O
families O
and O
use O

different O
scripts. O
It O
is O
nice O
to B-DAT
find O
six O
languages O
in O
this O

less O
than O
100 O
thousand O
sentences O
to B-DAT
train O
them. O
This O
is O
a O

that O
we O
would O
be O
able O
to B-DAT
train O
a O
good O
sentence O
embedding O

Georgian. O
This O
makes O
it O
difficult O
to B-DAT
bene- O
fit O
from O
joint O
training O

high O
error O
rates. O
We O
hope O
to B-DAT
improve O
their O
performance O
in O
the O

We O
extend O
our O
Tatoeba B-DAT
experiments O
to O
29 O
lan- O
guages O
without O
any O

This O
en- O
ables O
our O
encoder O
to B-DAT
perform O
reasonably O
well. O
We O
can O

results O
in O
the O
diagonal O
correspond O
to B-DAT
the O
accuracies O
reported O
in O
Table O

best O
published O
results O
on O
the O
BUCC B-DAT
mining O
task O
and O
the O
UN O

two O
French O
sentences O
on O
the O
BUCC B-DAT
training O
set O
along O
with O
their O

present O
our O
results O
on O
the O
BUCC B-DAT
min- O
ing O
task, O
UN O
corpus O

To O
cover O
all O
languages O
in O
BUCC, B-DAT
we O
use O
a O
separate O
En O

4.1 O
BUCC B-DAT
mining O
task O
The O
shared O
task O

Building O
and O
Using O
Comparable O
Corpora O
(BUCC) B-DAT
is O
a O
well- O
established O
evaluation O

Table O
2: O
BUCC B-DAT
results O
(precision, O
recall O
and O
F1 O

by O
the O
organizers O
of O
the O
BUCC B-DAT
workshop. O
We O
have O
done O
one O

Table O
3: O
BUCC B-DAT
results O
(F1) O
on O
the O
test O

best O
published O
results O
on O
the O
BUCC B-DAT
mining O
task, O
out- O
performing O
previous O

Alignment O
of O
Comparable O
Sentences. O
In O
BUCC, B-DAT
pages O
41–45 O

with O
STACC O
Vari- O
ants. O
In O
BUCC B-DAT

BUCC18 B-DAT

Using O
Multilingual O
Sentence O
Embeddings. O
In O
BUCC B-DAT

Grégoire O
and O
Philippe O
Langlais. O
2017. O
BUCC B-DAT
2017 O
Shared O
Task: O
a O
First O

tences O
in O
Comparable O
Corpora. O
In O
BUCC, B-DAT
pages O
46– O
50 O

2017. O
Overview O
of O
the O
Second O
BUCC B-DAT
Shared O
Task: O
Spotting O
Parallel O
Sentences O

in O
Comparable O
Corpora. O
In O
BUCC, B-DAT
pages O
60–67 O

2018. O
Overview O
of O
the O
Third O
BUCC B-DAT
Shared O
Task: O
Spotting O
Parallel O
Sentences O

in O
Comparable O
Corpora. O
In O
BUCC B-DAT

sion O
points, O
respectively. O
Filtering O
the O
English B-DAT

All O
ex- O
periments O
use O
an O
English B-DAT

mine O
for O
parallel O
sentences O
between O
English B-DAT
and O
four O
foreign O
languages: O
German O

NMT O
Finally, O
we O
filter O
the O
English B-DAT

Figure O
2: O
English B-DAT

Table O
5: O
Results O
on O
English B-DAT

Table O
6: O
Results O
on O
English B-DAT

obtain O
31.2 O
BLEU O
points O
for O
English B-DAT

Parity O
on O
Automatic O
Chinese O
to O
English B-DAT
News O
Translation. O
arXiv:1803.05567 O

English B-DAT
News O
Arti- O
cles O
and O
Sentences O

Machine O
translation O
is O
highly O
sensitive O
to B-DAT
the O
size O
and O
quality O
of O

training O
data, O
which O
has O
led O
to B-DAT
an O
increasing O
interest O
in O
collect O

sentence O
embed- O
dings. O
In O
contrast O
to B-DAT
previous O
approaches, O
which O
rely O
on O

standard O
benchmarks, O
it O
is O
known O
to B-DAT
be O
particularly O
sen- O
sitive O
to O

this O
context, O
effective O
ap- O
proaches O
to B-DAT
mine O
and O
filter O
parallel O
corpora O

are O
crucial O
to B-DAT
apply O
NMT O
in O
practical O
settings O

over O
bag-of-word O
features O
to B-DAT
distinguish O
between O
ground O
truth O
translations O

combined O
with O
set O
expansion O
operations O
to B-DAT
score O
translation O
candidates O
through O
the O

use O
an O
NMT O
inspired O
encoder-decoder O
to B-DAT
train O
sentence O
embeddings O
on O
existing O

are O
then O
directly O
ap- O
plied O
to B-DAT
retrieve O
and O
filter O
new O
parallel O

2018), O
who O
learn O
an O
encoder O
to B-DAT
score O
known O
translation O
pairs O
above O

and O
train O
a O
separate O
model O
to B-DAT
dynamically O
scale O
and O
shift O
the O

of O
a O
larger O
system, O
either O
to B-DAT
obtain O
an O
initial O
alignment O
that O

to B-DAT

shell. O
0.498 O
While O
the O
risk O
to B-DAT
those O
working O
in O
ceramics O
is O

discovered O
they O
are O
not O
free O
to B-DAT
speak O
their O
minds O

1 O
shows O
our O
encoder-decoder O
architecture O
to B-DAT
learn O
multilingual O
sentence O
embeddings, O
which O

ways: O
1) O
they O
are O
used O
to B-DAT
initialize O
its O
hidden O
and O
cell O

2) O
they O
are O
concate- O
nated O
to B-DAT
the O
input O
embeddings O
at O
every O

of O
0.001 O
and O
dropout O
set O
to B-DAT
0.1. O
We O
use O
a O
sin O

input O
embeddings O
size O
is O
set O
to B-DAT
512, O
while O
the O
lan O

2Prior O
to B-DAT
BPE O
segmentation, O
we O
tokenize O
and O

and O
the O
encoder O
is O
used O
to B-DAT
map O
a O
sentence O
to O
a O

multilingual O
encoder O
can O
be O
used O
to B-DAT
mine O
par- O
allel O
sentences O
by O

in O
the O
target O
side O
according O
to B-DAT
cosine O
similarity, O
and O
filtering O
those O

this O
approach O
has O
been O
reported O
to B-DAT
be O
competitive O
(Schwenk, O
2018), O
we O

one, O
thus O
making O
it O
impossible O
to B-DAT
filter O
it O
through O
a O
fixed O

range, O
it O
is O
still O
susceptible O
to B-DAT
concentrate O
around O
dif- O
ferent O
values O

do, O
filtering O
them O
is O
unlikely O
to B-DAT
cause O
any O
major O
harm O

1: O
Architecture O
of O
our O
system O
to B-DAT
learn O
multilingual O
sentence O
embeddings O

the O
average. O
This O
is O
equivalent O
to B-DAT
cosine O
similar- O
ity O
and O
thus O

given O
candidate. O
This O
is O
proportional O
to B-DAT
the O
CSLS O
score O
(Conneau O
et O

2018), O
which O
was O
originally O
motivated O
to B-DAT
mitigate O
the O
hub- O
ness O
problem O

we O
explore O
the O
following O
strategies O
to B-DAT
generate O
candidates O

causes O
the O
hubness O
problem. O
Thanks O
to B-DAT
its O
bidirec- O
tional O
nature, O
our O

Backward: O
Equivalent O
to B-DAT
the O
forward O
strat- O
egy, O
but O

candidates O
are O
then O
sorted O
according O
to B-DAT
their O
margin O
scores, O
and O
a O

the O
development O
data, O
or O
adjusted O
to B-DAT
obtain O
the O
desired O
corpus O
size O

on O
the O
training O
set, O
used O
to B-DAT
optimize O
the O
filtering O
threshold O

to B-DAT
mine O
for O
parallel O
sentences O
between O

and O
Chinese. O
There O
are O
150K O
to B-DAT
1.2M O
sentences O
for O
each O
language O

UN O
model O
in O
compar- O
ison O
to B-DAT
previous O
work.9 O
Our O
proposed O
system O

standard O
information O
was O
exclusively O
used O
to B-DAT
optimize O
the O
filtering O
threshold O
for O

filtering O
threshold O
was O
opti- O
mized O
to B-DAT
maximize O
the O
F1 O
score O
on O

UN O
corpus O
reconstruction O
So O
as O
to B-DAT
compare O
our O
method O
to O
the O

we O
first O
pre- O
process O
it O
to B-DAT
remove O
all O
duplicated O
sentence O
pairs O

de O
to B-DAT

results O
(newstest2013) O
using O
different O
thresholds O
to B-DAT
filter O
ParaCrawl O

corpus O
size O
from O
4.59 O
billion O
to B-DAT
64.4 O
million O
sentence O
pairs, O
mostly O

due O
to B-DAT
dedu- O
plication. O
We O
then O
score O

the O
top B-DAT
scoring O
entries O
up O
to O
the O
desired O
size. O
Figure O
2 O

Table O
6 O
compares O
our O
results O
to B-DAT
previ- O
ous O
works O
in O
the O

train- O
ing O
data. O
In O
addition O
to B-DAT
our O
ParaCrawl O
system, O
we O
include O

on O
English-German O
newstest2014 O
in O
comparison O
to B-DAT
previous O
work. O
wmt O
for O
WMT O

our O
improvement O
can O
be O
attributed O
to B-DAT
a O
better O
fil- O
tering O
of O

use O
a O
sequence-to-sequence B-DAT
ar- O
chitecture O
to O
train O
a O
multilingual O
sentence O
encoder O

tion, O
our O
method O
obtains O
up O
to B-DAT
85% O
precision O
at O
reconstructing O
the O

our O
improvements O
also O
carry O
over O
to B-DAT
downstream O
machine O
transla- O
tion, O
as O

the O
difference O
cannot O
be O
attributed O
to B-DAT
implementation O
details O

Use O
of O
Comparable O
Corpora O
to B-DAT
Improve O
SMT O
per- O
formance. O
In O

N O
Dauphin. O
2017. O
Convolutional O
Sequence O
to B-DAT
Sequence O
Learning. O
In O
ICML, O
pages O

Human O
Parity O
on O
Automatic B-DAT
Chinese O
to O
English O
News O
Translation. O
arXiv:1803.05567 O

on O
the O
WMT O
15 O
task O
English B-DAT

English B-DAT
(+2.1–3.4 O
BLEU), O
ob- O
taining O
new O

for O
the O
IWSLT O
15 O
task O
English B-DAT

with O
additional O
monolingual O
data, O
on O
English B-DAT

English, B-DAT
using O
training O
and O
test O
data O

from O
WMT O
15 O
for O
English B-DAT

↔German, O
IWSLT O
15 O
for O
English B-DAT

English B-DAT

For O
English B-DAT

English, B-DAT
we O
report O
case-sensitive O
BLEU O
on O

Table O
1: O
English B-DAT

4.1.1 O
English B-DAT

monolin- O
gual O
data O
set O
into O
English B-DAT

English B-DAT
system O
used O
for O
this O
is O

English, B-DAT
we O
back-translate O
4 O
200 O
000 O

monolingual O
English B-DAT
sentences O
into O
German, O
us- O
ing O

the O
English B-DAT

English B-DAT
training O
data O

English B-DAT

lower-resourced O
trans- O
lation O
setting O
than O
English B-DAT

For O
both O
Turkish O
and O
English, B-DAT
we O
represent O
rare O
words O
(or O

from O
Gigaword. O
We O
use O
an O
English B-DAT

English B-DAT
baseline O
system O

problem O
than O
with O
the O
larger O
English B-DAT

English, B-DAT
we O
use O
gradient O
clipping O
with O

1 O
that O
we O
use O
for O
English B-DAT

4.2.1 O
English B-DAT

Table O
3 O
shows O
English B-DAT

4.2.2 O
English B-DAT

Table O
4 O
shows O
English B-DAT

Table O
3: O
English B-DAT

Table O
4: O
English B-DAT

English B-DAT
system O
trained O
on O
WMT O
data O

English B-DAT
translation O
perfor- O
mance O
(BLEU) O
on O

English B-DAT
WMT O
15 O

English B-DAT
on O
the O
WMT O
15 O
data O

English B-DAT
IWSLT O
14 O

English B-DAT

English B-DAT
systems O

Table O
7: O
English B-DAT

English B-DAT
system O
that O
was O
itself O
trained O

English B-DAT
sys- O
tems, O
and O
of O
the O

resulting O
English B-DAT

English B-DAT
back-translation O
differs O
substantially, O
with O
a O

on O
new- O
stest2015. O
Regarding O
the O
English B-DAT

English B-DAT
translation O
performance O
(tokenized O
BLEU) O
on O

Table O
8: O
Phrase-based O
SMT O
results O
(English B-DAT

English B-DAT
training O
and O
develop- O
ment O
set O

Figure O
2: O
English B-DAT

English B-DAT
models. O
For O
comparability, O
we O
measure O

Figure O
2 O
shows O
cross-entropy O
for O
English B-DAT

training O
data O
is O
available O
for O
English B-DAT

natural O
according O
to O
native O
speaker. O
English B-DAT

natural O
words. O
For O
instance, O
the O
English B-DAT

→German O
sys- O
tems O
translate O
the O
English B-DAT
phrase O
civil O
rights O
pro- O
tections O

data O
for O
NMT. O
In O
contrast O
to B-DAT
previous O
work, O
which O
combines O
NMT O

architectures O
already O
have O
the O
capacity O
to B-DAT
learn O
the O
same O
information O
as O

model, O
and O
we O
explore O
strategies O
to B-DAT
train O
with O
monolin- O
gual O
data O

data, O
or O
data O
more O
similar O
to B-DAT
the O
translation O
task O

rationale, O
adding O
a O
language O
model O
to B-DAT
compensate O
for O
the O
independence O
assumptions O

and O
we O
expect O
monolingual O
data O
to B-DAT
be O
especially O
helpful O
if O
parallel O

In O
contrast O
to B-DAT
previous O
work, O
which O
integrates O
a O

al., O
2015), O
we O
explore O
strategies O
to B-DAT
include O
monolingual O
training O
data O
in O

This O
makes O
our O
approach O
applicable O
to B-DAT
different O
NMT O
architectures O

we O
investigate O
two O
different O
methods O
to B-DAT
fill O
the O
source O
side O
of O

we O
successfully O
adapt O
NMT O
models O
to B-DAT
a O
new O
domain O
by O
fine-tuning O

our O
approach O
is O
not O
specific O
to B-DAT
this O
architecture O

h O
j O
are O
concatenated O
to B-DAT
obtain O
the O
annotation O
vector O
hj O

probability O
that O
yi O
is O
aligned O
to B-DAT
xj O
. O
The O
alignment O
model O

or O
monolingual O
data O
more O
similar O
to B-DAT
the O
test O
set O

serves O
to B-DAT
improve O
the O
estimate O
of O
the O

into B-DAT
account. O
In O
con- O
trast O
to O
(Gülçehre O
et O
al., O
2015), O
who O

deep O
fusion, O
we O
propose O
techniques O
to B-DAT
train O
the O
main O
NMT O
model O

words. O
We O
describe O
two O
strategies O
to B-DAT
do O
this: O
providing O
monolingual O
training O

language, O
which O
we O
will O
refer O
to B-DAT
as O
back-translation O

first O
technique O
we O
employ O
is O
to B-DAT
treat O
mono- O
lingual O
training O
examples O

for O
which O
the O
network O
has O
to B-DAT
fully O
rely O
on O
the O
previous O

to B-DAT

single-word O
dummy O
source O
side O
<null> O
to B-DAT
allow O
processing O
of O
both O
parallel O

force O
the O
context O
vector B-DAT
ci O
to O
be O
0 O
for O
monolin- O
gual O

sets O
of O
20 O
minibatches O
according O
to B-DAT
length. O
This O
also O
groups O
monolin O

the O
output O
layer O
remains O
sensitive O
to B-DAT
the O
source O
context, O
and O
that O

from O
monolingual O
data, O
we O
propose O
to B-DAT
pair O
monolingual O
training O
instances O
with O

text O
with O
mteval-v13a.pl O
for O
comparison O
to B-DAT
official O
WMT O
and O
IWSLT O
results O

text O
with O
multi-bleu.perl O
for O
comparison O
to B-DAT
results O
by O
Gülçehre O
et O
al O

not O
ensembles. O
We O
leave O
it O
to B-DAT
fu- O
ture O
work O
to O
explore O

training O
with O
synthetic O
data O
is O
to B-DAT
the O
quality O
of O
the O
back O

15 O
test O
sets O
to B-DAT
investigate O
a O
cross-domain O
setting.5 O

2007), O
and O
removal O
of O
non-surface O
to B-DAT

We O
found O
overfitting O
to B-DAT
be O
a O
bigger O
problem O
than O

et O
al. O
(2015), O
in O
contrast O
to B-DAT
the O
threshold O
1 O
that O
we O

as O
long O
as O
the O
baseline O
to B-DAT
provide O
the O
training O
al- O
gorithm O

the O
quality O
improvement O
is O
due O
to B-DAT
the O
monolingual O
training O
instances, O
and O

best O
single O
system O
achieves O
a O
to B-DAT

- O
kenized O
BLEU O
(as O
opposed O
to B-DAT
untokenized O
scores O
reported O
in O
Table O

of O
training O
instances O
varies O
due O
to B-DAT
differences O
in O
training O
time O
and O

if O
it O
can O
be O
used O
to B-DAT
adapt O
a O
model O
to O
a O

system O
trained O
on O
WMT O
data O
to B-DAT
translating O
TED O
talks O

Systems O
1 O
and O
2 O
correspond O
to B-DAT
systems O
in O
Table O
3, O
trained O

system O
trained O
on O
WMT O
data O
to B-DAT
the O
TED O
do- O
main. O
By O

BLEU O
on O
average. O
To O
compare O
to B-DAT
what O
extent O
syn- O
thetic O
data O

of O
the O
parallel O
training O
text O
to B-DAT
obtain O
the O
training O
corpus O
parallelsynth O

despite O
being O
out-of-domain O
in O
relation O
to B-DAT
the O
test O
sets. O
We O
speculate O

monolin- O
gual O
data O
would O
lead O
to B-DAT
even O
higher O
improvements O

monolingual O
data, O
but O
this O
led O
to B-DAT
decreased O
BLEU O
scores O

difference O
in O
back-translation O
quality O
leads O
to B-DAT
a O
0.6–0.7 O
BLEU O
difference O
in O

size, O
and O
we O
leave O
it O
to B-DAT
future O
research O
to O
explore O
how O

of O
each O
training O
run). O
Thanks O
to B-DAT
the O
increased O
diversity O
of O
the O

4.3 O
Contrast O
to B-DAT
Phrase-based O
SMT O

data O
into B-DAT
the O
source O
language O
to O
produce O
synthetic O
parallel O
text O
has O

of O
training O
instances O
varies O
due O
to B-DAT
early O
stopping O

In O
contrast O
to B-DAT
phrase-based O
SMT, O
which O
can O
make O

so O
far O
not O
been O
able O
to B-DAT
use O
mono- O
lingual O
data O
to O

parallel O
data O
is O
not O
limited O
to B-DAT
domain O
adaptation, O
and O
that O
even O

hypothesis O
that O
domain O
adaptation O
contributes O
to B-DAT
the O
effectiveness O
of O
adding O
synthetic O

data O
to B-DAT
NMT O
training O

in O
data, O
or O
natural O
according O
to B-DAT
native O
speaker. O
English→German; O
newstest2015; O
ensemble O

ency, O
its O
ability O
to B-DAT
produce O
natural O
target-language O
sentences. O
As O

a O
proxy O
to B-DAT
sentence-level O
flu- O
ency, O
we O
investigate O

of O
each O
sys- O
tem O
according O
to B-DAT
their O
naturalness13 O
, O
distinguish- O
ing O

state O
of O
the O
language O
model O
to B-DAT
the O
de- O
coder O
state O
of O

synthetic O
parallel O
texts O
bears O
resemblance O
to B-DAT
data O
augmentation O
techniques O
used O
in O

is O
that O
self-training O
typically O
refers O
to B-DAT
scenario O
where O
the O
training O
set O

continued O
training O
has O
been O
shown O
to B-DAT
be O
effective O
for O
neural O
language O

and O
in O
work O
par- O
allel O
to B-DAT
ours, O
for O
neural O
translation O
models O

2015). O
We O
are O
the O
first O
to B-DAT
show O
that O
we O
can O
effectively O

we O
propose O
two O
simple O
methods O
to B-DAT
use O
monolingual O
training O
data O
during O

NMT O
systems, O
with O
no O
changes O
to B-DAT
the O
network O

dummy O
source O
context O
was O
successful O
to B-DAT
some O
ex- O
tent, O
but O
we O

the O
neural O
net- O
work O
architecture O
to B-DAT
integrate O
monolingual O
train- O
ing O
data O

approach O
can O
be O
easily O
applied O
to B-DAT
other O
NMT O
systems. O
We O
expect O

on O
the O
amount O
(and O
similarity O
to B-DAT
the O
test O
set) O
of O
available O

Machine O
Translation O
by O
Jointly O
Learning O
to B-DAT
Align O
and O
Trans- O
late. O
In O

Roossin. O
1990. O
A O
Statistical O
Approach O
to B-DAT
Machine O
Translation. O
Computational O
Linguistics, O
16(2):79–85 O

Manning. O
2015. O
Effective O
Ap- O
proaches O
to B-DAT
Attention-based O
Neural O
Machine O
Trans- O
lation O

Quoc O
V. O
Le. O
2014. O
Sequence O
to B-DAT
Sequence O
Learning O
with O
Neural O
Networks O

4.3 O
Contrast O
to B-DAT
Phrase-based O
SMT O

a O
new O
state-of-the-art O
in O
the O
BUCC B-DAT
shared O
task O
for O
3 O
of O

ML- O
Doc O
dataset), O
bitext O
mining O
(BUCC B-DAT
dataset) O
and O
multilingual O
similarity O
search O

in O
different O
tasks. O
Some O
tasks O
(BUCC, B-DAT
MLDoc) O
tend O
to O
perform O
better O

4.2), O
and O
bi- O
text O
mining O
(BUCC, B-DAT
Section O
4.3). O
However, O
all O
these O

5: O
F1 O
scores O
on O
the O
BUCC B-DAT
mining O
task O

4.3 O
BUCC B-DAT

our O
sentence O
embeddings O
on O
the O
BUCC B-DAT
mining O
task O
(Zweigen- O
baum O
et O

While O
XNLI, O
MLDoc O
and O
BUCC B-DAT
are O
well O
estab- O
lished O
benchmarks O

Depth O
Tatoeba O
BUCC B-DAT
MLDoc O
XNLI-en O
XNLI-xxErr O
[%] O
F1 O

and O
Tatoeba. O
The O
effect O
in O
BUCC B-DAT
is O
negligible O

NLI O
Tatoeba O
BUCC B-DAT
MLDoc O
XNLI-en O
XNLI-xx O
obj. O
Err O

langs O
WMT O
BUCC B-DAT
MLDoc O
XNLI-en O
XNLI-xxErr O
[%] O
F1 O

MLDoc), O
and O
bitext O
min- O
ing O
(BUCC B-DAT

Alignment O
of O
Comparable O
Sentences. O
In O
BUCC, B-DAT
pages O
41–45 O

with O
STACC O
Vari- O
ants. O
In O
BUCC B-DAT

BUCC18 B-DAT

Using O
Multilingual O
Sentence O
Embeddings. O
In O
BUCC B-DAT

lel O
Sentence O
Identification O
Model. O
In O
BUCC B-DAT

Grégoire O
and O
Philippe O
Langlais. O
2017. O
BUCC B-DAT
2017 O
Shared O
Task: O
a O
First O

tences O
in O
Comparable O
Corpora. O
In O
BUCC, B-DAT
pages O
46– O
50 O

in O
Chinese-English O
Comparable O
Corpora. O
In O
BUCC, B-DAT
pages O
51–55 O

2017. O
Overview O
of O
the O
Second O
BUCC B-DAT
Shared O
Task: O
Spotting O
Parallel O
Sentences O

in O
Comparable O
Corpora. O
In O
BUCC, B-DAT
pages O
60–67 O

2018. O
Overview O
of O
the O
Third O
BUCC B-DAT
Shared O
Task: O
Spotting O
Parallel O
Sentences O

in O
Comparable O
Corpora. O
In O
BUCC B-DAT

the O
resulting O
sentence O
embeddings O
using O
English B-DAT
annotated O
data O
only, O
and O
transfer O

model O
from O
one O
language O
(e.g. O
English) B-DAT
to O
another, O
and O
the O
possi O

which O
usu- O
ally O
consider O
separate O
English B-DAT

pair- O
wise O
joint O
embeddings O
with O
English B-DAT
and O
one O
for- O
eign O
language O

3.10 O
4.30 O
1000 O
eng O
en O
English B-DAT
Germanic O
Latin O
2.6M O
n/a O
n/a O

two O
target O
languages. O
We O
choose O
English B-DAT
and O
Spanish O
for O
that O
purpose O

with O
one O
alignment O
only, O
usually O
English B-DAT

the O
well-established O
evaluation O
frameworks O
for O
English B-DAT
sentence O
representations O
(Conneau O
et O
al O

multi- O
lingual O
sentence O
embedding O
using O
English B-DAT
train- O
ing O
data, O
and O
evaluate O

a O
dataset O
similar O
to O
the O
English B-DAT
MultiNLI O
for O
several O
languages O
(Conneau O

sentences O
have O
been O
translated O
from O
English B-DAT
into O
14 O
languages O
by O
professional O

different O
systems O
are O
to O
use O
English B-DAT
training O
data O
from O
MultiNLI, O
and O

hyperparameters O
were O
optimized O
on O
the O
English B-DAT
XNLI O
development O
corpus, O
and O
then O

i.e. O
training O
a O
classifier O
on O
English B-DAT
data O
and O
applying O
it O
to O

lower O
than O
the O
one O
on O
English, B-DAT
including O
distant O
languages O
like O
Arabic O

achieves O
excel- O
lent O
results O
on O
English, B-DAT
outperforming O
our O
system O
by O
7.5 O

Translate O
test, O
one O
English B-DAT
NLI O
system: O
Conneau O
et O
al O

by O
aligning O
it O
to O
the O
English B-DAT
one O

translate O
the O
test O
data O
into O
English B-DAT
and O
apply O
the O
English O
NLI O

classifier, O
or O
2) O
translate O
the O
English B-DAT
training O
data O
and O
train O
a O

multilingual O
encoder O
us- O
ing O
the O
English B-DAT
training O
data, O
optimizing O
hyper- O
parameters O

on O
the O
English B-DAT
development O
set, O
and O
evaluating O
the O

a O
com- O
parable O
corpus O
between O
English B-DAT
and O
four O
foreign O
languages: O
German O

pairs O
with O
the O
exception O
of O
English B-DAT

models O
covering O
4 O
languages O
each O
(English B-DAT

/French/Spanish/German O
and O
English B-DAT

consists O
of O
up O
to O
1,000 O
English B-DAT

re- O
port O
the O
accuracy O
on O
English B-DAT

to O
be O
helpful O
to O
learn O
English B-DAT
sentence O
embeddings O
(Subrama O

a O
better O
performance O
on O
the O
English B-DAT
NLI O
test O
set, O
but O
this O

similarity O
error O
rate. O
This O
covers O
English, B-DAT
Czech, O
French, O
German O
and O
Spanish O

Parity O
on O
Automatic O
Chinese O
to O
English B-DAT
News O
Translation. O
arXiv:1803.05567 O

English B-DAT
Comparable O
Corpora. O
In O
BUCC, O
pages O

A O
community O
supported O
collection O
of O
English B-DAT
sentences O
and O
translations O
into O
more O

is O
an O
open O
collection O
of O
English B-DAT
sen- O
tences O
and O
high O
quality O

to O
1,000 O
aligned O
sentences O
with O
English B-DAT

stressed O
that, O
in O
general, O
the O
English B-DAT
sentences O
are O
not O
the O
same O

with O
less O
than O
20%, O
respectively O
(English B-DAT
included). O
The O
languages O
with O
less O

13.40 O
10.00 O
1000 O
ang O
Old O
English B-DAT
Germanic O
Latin O
none O
58.96 O
65.67 O

We O
introduce O
an O
architecture O
to B-DAT
learn O
joint O
multilingual O
sentence O
representations O

for O
93 O
languages, O
belonging O
to B-DAT
more O
than O
30 O
dif- O
ferent O

parallel O
corpora. O
This O
enables O
us O
to B-DAT
learn O
a O
classifier O
on O
top O

data O
only, O
and O
transfer O
it O
to B-DAT
any O
of O
the O
93 O
languages O

of O
deep O
learning O
has O
led O
to B-DAT
impressive O
progress O
in O
Natural O
Language O

NLP), O
these O
techniques O
are O
known O
to B-DAT
be O
par- O
ticularly O
data O
hungry O

scenarios. O
An O
increasingly O
popular O
approach O
to B-DAT
alleviate O
this O
issue O
is O
to O

language O
and O
are O
thus O
unable O
to B-DAT
leverage O
information O
across O
different O
languages O

are O
gen- O
eral O
with O
respect O
to B-DAT
two O
dimensions: O
the O
input O
lan O

over O
many O
languages, O
the O
desire O
to B-DAT
per- O
form O
zero-shot O
transfer O
of O

from O
one O
language O
(e.g. O
English) O
to B-DAT
another, O
and O
the O
possi- O
bility O

to B-DAT
handle O
code-switching. O
We O
achieve O
this O

languages, O
and O
is O
usually O
limited O
to B-DAT
a O
few O
(most O
often O
only O

substantially O
improve O
on O
previous O
work O
to B-DAT
learn O
joint O
multilingual O
sentence O
represen O

a O
shared O
space, O
in O
contrast O
to B-DAT
most O
other O
works O
which O
usu O

train O
the O
entire O
system O
end-to-end B-DAT
to O
predict O
the O
surrounding O
sentences O
over O

This O
was O
recently O
ex- O
tended O
to B-DAT
multitask O
learning, O
combining O
different O
training O

becom- O
ing O
increasingly O
popular O
is O
to B-DAT
train O
word O
embed- O
dings O
independently O

corpora, O
and O
then O
map O
them O
to B-DAT
a O
shared O
space O
based O
on O

em- O
beddings O
are O
often O
used O
to B-DAT
build O
bag-of-word O
rep- O
resentations O
of O

that O
we O
follow O
here O
is O
to B-DAT
use O
a O
sequence-to-sequence O
encoder- O
decoder O

end-to-end B-DAT
on O
parallel O
corpora O
akin O
to O
neural O
ma- O
chine O
translation: O
the O

is O
used O
by O
the O
decoder O
to B-DAT
create O
the O
target O
sequence. O
This O

and O
the O
encoder O
is O
kept O
to B-DAT
embed O
sentences O
in O
any O
of O

work O
is O
either O
lim- O
ited O
to B-DAT
few, O
rather O
close O
languages O
(Schwenk O

number O
of O
languages O
is O
limited O
to B-DAT
word O
embeddings O
(Ammar O
et O
al O

1: O
Architecture O
of O
our O
system O
to B-DAT
learn O
multilingual O
sentence O
embeddings O

language O
agnostic O
BiLSTM O
en- O
coder O
to B-DAT
build O
our O
sentence O
embeddings, O
which O

parallel O
corpora. O
From O
Section O
3.1 O
to B-DAT
3.3, O
we O
describe O
its O
architecture O

, O
our O
training O
strategy O
to B-DAT
scale O
to O
up O
to O
93 O

to B-DAT

morphologically O
complex O
language O
might O
correspond O
to B-DAT
several O
words O
of O
a O
morphologically O

sentence O
em- O
beddings O
are O
used O
to B-DAT
initialize O
the O
decoder O
LSTM O
through O

and O
are O
also O
con- O
catenated O
to B-DAT
its O
input O
embeddings O
at O
every O

information O
of O
the O
input O
sequence O
to B-DAT
be O
captured O
by O
the O
sentence O

input O
language O
is, O
encouraging O
it O
to B-DAT
learn O
language O
independent O
representations. O
In O

ding O
that O
specifies O
the O
language O
to B-DAT
generate, O
which O
is O
concatenated O
to O

Scaling O
up O
to B-DAT
almost O
hundred O
languages, O
which O
use O

paper, O
we O
limit O
our O
study O
to B-DAT
a O
stacked O
BiLSTM O
with O
1 O

to B-DAT
5 O
layers, O
each O
512-dimensional. O
The O

embed- O
ding O
size O
is O
set O
to B-DAT
320, O
while O
the O
language O
ID O

While O
this O
approach O
was O
shown O
to B-DAT
learn O
high-quality O
representations, O
it O
poses O

two O
obvious O
drawbacks O
when O
trying O
to B-DAT
scale O
to O
a O
large O
number O

out O
of O
93 O
languages O
used O
to B-DAT
trained O
the O
proposed O
model O
with O

cor- O
pus, O
which O
is O
difficult O
to B-DAT
obtain O
for O
all O
languages. O
Second O

a O
quadratic O
cost O
with O
respect O
to B-DAT
the O
number O
of O
languages, O
making O

target O
languages O
– O
two O
seem O
to B-DAT
be O
enough.2 O
At O
the O
same O

not O
require O
each O
source O
sentence O
to B-DAT
be O
translated O
into O
the O
two O

of O
0.001 O
and O
dropout O
set O
to B-DAT
0.1, O
and O
train O
for O
a O

use O
of O
its O
multi-GPU O
support O
to B-DAT
train O
on O
16 O
NVIDIA O
V100 O

target O
language, O
the O
only O
way O
to B-DAT
train O
the O
encoder O
for O
that O

auto- B-DAT
encoding, O
which O
we O
observe O
to O
work O
poorly. O
Having O
two O
tar O

Some O
tasks O
(BUCC, O
MLDoc) O
tend O
to B-DAT
perform O
better O
when O
the O
encoder O

informal O
sentences. O
In O
an O
attempt O
to B-DAT
achieve O
a O
general O
purpose O
sentence O

yet O
a O
commonly O
accepted O
standard O
to B-DAT
eval- O
uate O
multilingual O
sentence O
embeddings O

an O
NLI O
test O
set O
similar O
to B-DAT
MultiNLI O
(Williams O
et O
al., O
2017 O

languages O
(Section O
4.1). O
So O
as O
to B-DAT
obtain O
a O
more O
complete O
picture O

become O
a O
widely O
used O
task O
to B-DAT
evaluate O
sentence O
representations O
(Bowman O
et O

XNLI O
is O
a O
recent O
effort O
to B-DAT
create O
a O
dataset O
similar O
to O

provided; O
instead, O
different O
systems O
are O
to B-DAT
use O
English O
training O
data O
from O

the O
same O
classifier O
was O
applied O
to B-DAT
all O
languages O
of O
the O
XNLI O

English O
data O
and O
applying O
it O
to B-DAT
all O
other O
lan- O
guages) O
for O

Conneau O
et O
al. O
(2018c) O
correspond O
to B-DAT
max-pooling, O
which O
outperforms O
the O
last-state O

points O
for O
our O
system, O
compared O
to B-DAT
19.3 O
and O
17.6 O
points O
for O

lan- O
guage O
by O
aligning O
it O
to B-DAT
the O
English O
one O

is O
worth O
mentioning O
that, O
thanks O
to B-DAT
its O
multilingual O
nature, O
our O
system O

multilingual O
representations. O
In O
or- O
der O
to B-DAT
evaluate O
our O
sentence O
embeddings O
in O

on O
Japanese O
can O
be O
attributed O
to B-DAT
the O
domain O
and O
sentence O
length O

notion O
of O
margin O
is O
related O
to B-DAT
CSLS O
as O
proposed O
in O
Conneau O

2018a). O
The O
reader O
is O
referred O
to B-DAT
Artetxe O
and O
Schwenk O
(2018) O
for O

We O
use O
this O
method O
to B-DAT
evaluate O
our O
sentence O
embeddings O
on O

Schwenk O
(2018). O
The O
goal O
is O
to B-DAT
extract O
parallel O
sentences O
from O
a O

The O
dataset O
consists O
of O
150K O
to B-DAT
1.2M O
sentences O
for O
each O
language O

four O
languages O
increased O
from O
93.27 O
to B-DAT
93.92. O
Not O
only O
are O
our O

it O
can O
potentially O
be O
used O
to B-DAT
mine O
bi- O
text O
for O
any O

guages. O
So O
as O
to B-DAT
better O
assess O
the O
performance O
of O

The O
dataset O
consists O
of O
up O
to B-DAT
1,000 O
English-aligned O
sentence O
pairs O
for O

in O
the O
other O
language O
according O
to B-DAT
cosine O
similarity O
and O
computing O
the O

in O
Section O
5.3. O
In O
relation O
to B-DAT
that, O
Appendix O
E O
reports O
similarity O

our O
encoder O
can O
also O
generalize O
to B-DAT
unseen O
languages O
to O
some O
extent O

layers. O
We O
were O
not O
able O
to B-DAT
achieve O
good O
convergence O
with O
deeper O

STM O
has O
not O
enough O
capacity O
to B-DAT
encode O
so O
many O
languages O

Multitask O
learning O
has O
been O
shown O
to B-DAT
be O
helpful O
to O
learn O
English O

adding O
an O
additional O
NLI O
objective O
to B-DAT
our O
system O
with O
different O
weighting O

7, O
the O
NLI O
objective O
leads O
to B-DAT
a O
better O
performance O
on O
the O

So O
as O
to B-DAT
better O
understand O
how O
our O
architecture O

scales O
to B-DAT
a O
large O
amount O
of O
languages O

lan- O
guages, O
and O
compare O
it O
to B-DAT
our O
main O
model O
trained O
on O

the O
WMT O
2014 O
test O
set O
to B-DAT
evaluate O
the O
multi- O
lingual O
similarity O

the O
joint O
training O
also O
yields O
to B-DAT
overall O
better O
rep- O
resentations O

paper, O
we O
propose O
an O
architecture O
to B-DAT
learn O
multilingual O
sentence O
embeddings O
for O

available O
parallel O
corpora O
and O
applied O
to B-DAT
different O
downstream O
tasks O
without O
any O

the O
future, O
we O
would O
like O
to B-DAT
explore O
alterna- O
tive O
architectures O
for O

encoder. O
In O
particular, O
we O
plan O
to B-DAT
replace O
our O
BiLSTM O
with O
the O

former, O
which O
has O
been O
shown O
to B-DAT
work O
better O
in O
different O
settings O

2018). O
Moreover, O
we O
would O
like O
to B-DAT
explore O
possible O
strategies O
to O
exploit O

train- O
ing O
data O
in O
addition O
to B-DAT
parallel O
corpora, O
such O
as O
using O

2018). O
Finally, O
we O
would O
like O
to B-DAT
replace O
our O
language- O
specific O
tokenization O

a O
language O
agnostic O
approach O
similar O
to B-DAT
Sentence- O
Piece.11 O

Human O
Parity O
on O
Automatic B-DAT
Chinese O
to O
English O
News O
Translation. O
arXiv:1803.05567 O

The O
size O
varies O
from O
400k O
to B-DAT
2M O
sentence O
pairs, O
in O
function O

e.g. O
Ar- O
menian O
or O
Kazakh) O
to B-DAT
more O
than O
50 O
million O
(e.g O

We O
use O
this O
cor- O
pus O
to B-DAT
extract O
a O
separate O
test O
set O

of O
up O
to B-DAT
1,000 O
sentences O
for O
many O
languages O

only O
kept O
93 O
different O
languages O
to B-DAT
train O
the O
multilingual O
sentence O
embeddings O

numbers O
in O
the O
diagonal O
correspond O
to B-DAT
the O
main O
results O
reported O
in O

observe O
that O
our O
approach O
seems O
to B-DAT
han- O
dle O
the O
combination O
of O

created O
test O
sets O
of O
up O
to B-DAT
1,000 O
aligned O
sentences O
with O
English O

iting O
the O
number O
of O
sentences O
to B-DAT
500, O
we O
increase O
the O
coverage O

to B-DAT
101 O
languages, O
and O
even O
141 O

less O
than O
20% O
error O
belong O
to B-DAT
20 O
different O
families O
and O
use O

different O
scripts. O
It O
is O
nice O
to B-DAT
find O
six O
languages O
in O
this O

less O
than O
100 O
thousand O
sentences O
to B-DAT
train O
them. O
This O
is O
a O

that O
we O
would O
be O
able O
to B-DAT
train O
a O
good O
sentence O
embedding O

Georgian. O
This O
makes O
it O
difficult O
to B-DAT
bene- O
fit O
from O
joint O
training O

high O
error O
rates. O
We O
hope O
to B-DAT
improve O
their O
performance O
in O
the O

We O
extend O
our O
Tatoeba B-DAT
experiments O
to O
29 O
lan- O
guages O
without O
any O

This O
en- O
ables O
our O
encoder O
to B-DAT
perform O
reasonably O
well. O
We O
can O

results O
in O
the O
diagonal O
correspond O
to B-DAT
the O
accuracies O
reported O
in O
Table O

best O
published O
results O
on O
the O
BUCC B-DAT
mining O
task O
and O
the O
UN O

two O
French O
sentences O
on O
the O
BUCC B-DAT
training O
set O
along O
with O
their O

present O
our O
results O
on O
the O
BUCC B-DAT
min- O
ing O
task, O
UN O
corpus O

To O
cover O
all O
languages O
in O
BUCC, B-DAT
we O
use O
a O
separate O
En O

4.1 O
BUCC B-DAT
mining O
task O
The O
shared O
task O

Building O
and O
Using O
Comparable O
Corpora O
(BUCC) B-DAT
is O
a O
well- O
established O
evaluation O

Table O
2: O
BUCC B-DAT
results O
(precision, O
recall O
and O
F1 O

by O
the O
organizers O
of O
the O
BUCC B-DAT
workshop. O
We O
have O
done O
one O

Table O
3: O
BUCC B-DAT
results O
(F1) O
on O
the O
test O

best O
published O
results O
on O
the O
BUCC B-DAT
mining O
task, O
out- O
performing O
previous O

Alignment O
of O
Comparable O
Sentences. O
In O
BUCC, B-DAT
pages O
41–45 O

with O
STACC O
Vari- O
ants. O
In O
BUCC B-DAT

BUCC18 B-DAT

Using O
Multilingual O
Sentence O
Embeddings. O
In O
BUCC B-DAT

Grégoire O
and O
Philippe O
Langlais. O
2017. O
BUCC B-DAT
2017 O
Shared O
Task: O
a O
First O

tences O
in O
Comparable O
Corpora. O
In O
BUCC, B-DAT
pages O
46– O
50 O

2017. O
Overview O
of O
the O
Second O
BUCC B-DAT
Shared O
Task: O
Spotting O
Parallel O
Sentences O

in O
Comparable O
Corpora. O
In O
BUCC, B-DAT
pages O
60–67 O

2018. O
Overview O
of O
the O
Third O
BUCC B-DAT
Shared O
Task: O
Spotting O
Parallel O
Sentences O

in O
Comparable O
Corpora. O
In O
BUCC B-DAT

sion O
points, O
respectively. O
Filtering O
the O
English B-DAT

All O
ex- O
periments O
use O
an O
English B-DAT

mine O
for O
parallel O
sentences O
between O
English B-DAT
and O
four O
foreign O
languages: O
German O

NMT O
Finally, O
we O
filter O
the O
English B-DAT

Figure O
2: O
English B-DAT

Table O
5: O
Results O
on O
English B-DAT

Table O
6: O
Results O
on O
English B-DAT

obtain O
31.2 O
BLEU O
points O
for O
English B-DAT

Parity O
on O
Automatic O
Chinese O
to O
English B-DAT
News O
Translation. O
arXiv:1803.05567 O

English B-DAT
News O
Arti- O
cles O
and O
Sentences O

Machine O
translation O
is O
highly O
sensitive O
to B-DAT
the O
size O
and O
quality O
of O

training O
data, O
which O
has O
led O
to B-DAT
an O
increasing O
interest O
in O
collect O

sentence O
embed- O
dings. O
In O
contrast O
to B-DAT
previous O
approaches, O
which O
rely O
on O

standard O
benchmarks, O
it O
is O
known O
to B-DAT
be O
particularly O
sen- O
sitive O
to O

this O
context, O
effective O
ap- O
proaches O
to B-DAT
mine O
and O
filter O
parallel O
corpora O

are O
crucial O
to B-DAT
apply O
NMT O
in O
practical O
settings O

over O
bag-of-word O
features O
to B-DAT
distinguish O
between O
ground O
truth O
translations O

combined O
with O
set O
expansion O
operations O
to B-DAT
score O
translation O
candidates O
through O
the O

use O
an O
NMT O
inspired O
encoder-decoder O
to B-DAT
train O
sentence O
embeddings O
on O
existing O

are O
then O
directly O
ap- O
plied O
to B-DAT
retrieve O
and O
filter O
new O
parallel O

2018), O
who O
learn O
an O
encoder O
to B-DAT
score O
known O
translation O
pairs O
above O

and O
train O
a O
separate O
model O
to B-DAT
dynamically O
scale O
and O
shift O
the O

of O
a O
larger O
system, O
either O
to B-DAT
obtain O
an O
initial O
alignment O
that O

to B-DAT

shell. O
0.498 O
While O
the O
risk O
to B-DAT
those O
working O
in O
ceramics O
is O

discovered O
they O
are O
not O
free O
to B-DAT
speak O
their O
minds O

1 O
shows O
our O
encoder-decoder O
architecture O
to B-DAT
learn O
multilingual O
sentence O
embeddings, O
which O

ways: O
1) O
they O
are O
used O
to B-DAT
initialize O
its O
hidden O
and O
cell O

2) O
they O
are O
concate- O
nated O
to B-DAT
the O
input O
embeddings O
at O
every O

of O
0.001 O
and O
dropout O
set O
to B-DAT
0.1. O
We O
use O
a O
sin O

input O
embeddings O
size O
is O
set O
to B-DAT
512, O
while O
the O
lan O

2Prior O
to B-DAT
BPE O
segmentation, O
we O
tokenize O
and O

and O
the O
encoder O
is O
used O
to B-DAT
map O
a O
sentence O
to O
a O

multilingual O
encoder O
can O
be O
used O
to B-DAT
mine O
par- O
allel O
sentences O
by O

in O
the O
target O
side O
according O
to B-DAT
cosine O
similarity, O
and O
filtering O
those O

this O
approach O
has O
been O
reported O
to B-DAT
be O
competitive O
(Schwenk, O
2018), O
we O

one, O
thus O
making O
it O
impossible O
to B-DAT
filter O
it O
through O
a O
fixed O

range, O
it O
is O
still O
susceptible O
to B-DAT
concentrate O
around O
dif- O
ferent O
values O

do, O
filtering O
them O
is O
unlikely O
to B-DAT
cause O
any O
major O
harm O

1: O
Architecture O
of O
our O
system O
to B-DAT
learn O
multilingual O
sentence O
embeddings O

the O
average. O
This O
is O
equivalent O
to B-DAT
cosine O
similar- O
ity O
and O
thus O

given O
candidate. O
This O
is O
proportional O
to B-DAT
the O
CSLS O
score O
(Conneau O
et O

2018), O
which O
was O
originally O
motivated O
to B-DAT
mitigate O
the O
hub- O
ness O
problem O

we O
explore O
the O
following O
strategies O
to B-DAT
generate O
candidates O

causes O
the O
hubness O
problem. O
Thanks O
to B-DAT
its O
bidirec- O
tional O
nature, O
our O

Backward: O
Equivalent O
to B-DAT
the O
forward O
strat- O
egy, O
but O

candidates O
are O
then O
sorted O
according O
to B-DAT
their O
margin O
scores, O
and O
a O

the O
development O
data, O
or O
adjusted O
to B-DAT
obtain O
the O
desired O
corpus O
size O

on O
the O
training O
set, O
used O
to B-DAT
optimize O
the O
filtering O
threshold O

to B-DAT
mine O
for O
parallel O
sentences O
between O

and O
Chinese. O
There O
are O
150K O
to B-DAT
1.2M O
sentences O
for O
each O
language O

UN O
model O
in O
compar- O
ison O
to B-DAT
previous O
work.9 O
Our O
proposed O
system O

standard O
information O
was O
exclusively O
used O
to B-DAT
optimize O
the O
filtering O
threshold O
for O

filtering O
threshold O
was O
opti- O
mized O
to B-DAT
maximize O
the O
F1 O
score O
on O

UN O
corpus O
reconstruction O
So O
as O
to B-DAT
compare O
our O
method O
to O
the O

we O
first O
pre- O
process O
it O
to B-DAT
remove O
all O
duplicated O
sentence O
pairs O

de O
to B-DAT

results O
(newstest2013) O
using O
different O
thresholds O
to B-DAT
filter O
ParaCrawl O

corpus O
size O
from O
4.59 O
billion O
to B-DAT
64.4 O
million O
sentence O
pairs, O
mostly O

due O
to B-DAT
dedu- O
plication. O
We O
then O
score O

the O
top B-DAT
scoring O
entries O
up O
to O
the O
desired O
size. O
Figure O
2 O

Table O
6 O
compares O
our O
results O
to B-DAT
previ- O
ous O
works O
in O
the O

train- O
ing O
data. O
In O
addition O
to B-DAT
our O
ParaCrawl O
system, O
we O
include O

on O
English-German O
newstest2014 O
in O
comparison O
to B-DAT
previous O
work. O
wmt O
for O
WMT O

our O
improvement O
can O
be O
attributed O
to B-DAT
a O
better O
fil- O
tering O
of O

use O
a O
sequence-to-sequence B-DAT
ar- O
chitecture O
to O
train O
a O
multilingual O
sentence O
encoder O

tion, O
our O
method O
obtains O
up O
to B-DAT
85% O
precision O
at O
reconstructing O
the O

our O
improvements O
also O
carry O
over O
to B-DAT
downstream O
machine O
transla- O
tion, O
as O

the O
difference O
cannot O
be O
attributed O
to B-DAT
implementation O
details O

Use O
of O
Comparable O
Corpora O
to B-DAT
Improve O
SMT O
per- O
formance. O
In O

N O
Dauphin. O
2017. O
Convolutional O
Sequence O
to B-DAT
Sequence O
Learning. O
In O
ICML, O
pages O

Human O
Parity O
on O
Automatic B-DAT
Chinese O
to O
English O
News O
Translation. O
arXiv:1803.05567 O

on O
the O
WMT O
15 O
task O
English B-DAT

English B-DAT
(+2.1–3.4 O
BLEU), O
ob- O
taining O
new O

for O
the O
IWSLT O
15 O
task O
English B-DAT

with O
additional O
monolingual O
data, O
on O
English B-DAT

English, B-DAT
using O
training O
and O
test O
data O

from O
WMT O
15 O
for O
English B-DAT

↔German, O
IWSLT O
15 O
for O
English B-DAT

English B-DAT

For O
English B-DAT

English, B-DAT
we O
report O
case-sensitive O
BLEU O
on O

Table O
1: O
English B-DAT

4.1.1 O
English B-DAT

monolin- O
gual O
data O
set O
into O
English B-DAT

English B-DAT
system O
used O
for O
this O
is O

English, B-DAT
we O
back-translate O
4 O
200 O
000 O

monolingual O
English B-DAT
sentences O
into O
German, O
us- O
ing O

the O
English B-DAT

English B-DAT
training O
data O

English B-DAT

lower-resourced O
trans- O
lation O
setting O
than O
English B-DAT

For O
both O
Turkish O
and O
English, B-DAT
we O
represent O
rare O
words O
(or O

from O
Gigaword. O
We O
use O
an O
English B-DAT

English B-DAT
baseline O
system O

problem O
than O
with O
the O
larger O
English B-DAT

English, B-DAT
we O
use O
gradient O
clipping O
with O

1 O
that O
we O
use O
for O
English B-DAT

4.2.1 O
English B-DAT

Table O
3 O
shows O
English B-DAT

4.2.2 O
English B-DAT

Table O
4 O
shows O
English B-DAT

Table O
3: O
English B-DAT

Table O
4: O
English B-DAT

English B-DAT
system O
trained O
on O
WMT O
data O

English B-DAT
translation O
perfor- O
mance O
(BLEU) O
on O

English B-DAT
WMT O
15 O

English B-DAT
on O
the O
WMT O
15 O
data O

English B-DAT
IWSLT O
14 O

English B-DAT

English B-DAT
systems O

Table O
7: O
English B-DAT

English B-DAT
system O
that O
was O
itself O
trained O

English B-DAT
sys- O
tems, O
and O
of O
the O

resulting O
English B-DAT

English B-DAT
back-translation O
differs O
substantially, O
with O
a O

on O
new- O
stest2015. O
Regarding O
the O
English B-DAT

English B-DAT
translation O
performance O
(tokenized O
BLEU) O
on O

Table O
8: O
Phrase-based O
SMT O
results O
(English B-DAT

English B-DAT
training O
and O
develop- O
ment O
set O

Figure O
2: O
English B-DAT

English B-DAT
models. O
For O
comparability, O
we O
measure O

Figure O
2 O
shows O
cross-entropy O
for O
English B-DAT

training O
data O
is O
available O
for O
English B-DAT

natural O
according O
to O
native O
speaker. O
English B-DAT

natural O
words. O
For O
instance, O
the O
English B-DAT

→German O
sys- O
tems O
translate O
the O
English B-DAT
phrase O
civil O
rights O
pro- O
tections O

data O
for O
NMT. O
In O
contrast O
to B-DAT
previous O
work, O
which O
combines O
NMT O

architectures O
already O
have O
the O
capacity O
to B-DAT
learn O
the O
same O
information O
as O

model, O
and O
we O
explore O
strategies O
to B-DAT
train O
with O
monolin- O
gual O
data O

data, O
or O
data O
more O
similar O
to B-DAT
the O
translation O
task O

rationale, O
adding O
a O
language O
model O
to B-DAT
compensate O
for O
the O
independence O
assumptions O

and O
we O
expect O
monolingual O
data O
to B-DAT
be O
especially O
helpful O
if O
parallel O

In O
contrast O
to B-DAT
previous O
work, O
which O
integrates O
a O

al., O
2015), O
we O
explore O
strategies O
to B-DAT
include O
monolingual O
training O
data O
in O

This O
makes O
our O
approach O
applicable O
to B-DAT
different O
NMT O
architectures O

we O
investigate O
two O
different O
methods O
to B-DAT
fill O
the O
source O
side O
of O

we O
successfully O
adapt O
NMT O
models O
to B-DAT
a O
new O
domain O
by O
fine-tuning O

our O
approach O
is O
not O
specific O
to B-DAT
this O
architecture O

h O
j O
are O
concatenated O
to B-DAT
obtain O
the O
annotation O
vector O
hj O

probability O
that O
yi O
is O
aligned O
to B-DAT
xj O
. O
The O
alignment O
model O

or O
monolingual O
data O
more O
similar O
to B-DAT
the O
test O
set O

serves O
to B-DAT
improve O
the O
estimate O
of O
the O

into B-DAT
account. O
In O
con- O
trast O
to O
(Gülçehre O
et O
al., O
2015), O
who O

deep O
fusion, O
we O
propose O
techniques O
to B-DAT
train O
the O
main O
NMT O
model O

words. O
We O
describe O
two O
strategies O
to B-DAT
do O
this: O
providing O
monolingual O
training O

language, O
which O
we O
will O
refer O
to B-DAT
as O
back-translation O

first O
technique O
we O
employ O
is O
to B-DAT
treat O
mono- O
lingual O
training O
examples O

for O
which O
the O
network O
has O
to B-DAT
fully O
rely O
on O
the O
previous O

to B-DAT

single-word O
dummy O
source O
side O
<null> O
to B-DAT
allow O
processing O
of O
both O
parallel O

force O
the O
context O
vector B-DAT
ci O
to O
be O
0 O
for O
monolin- O
gual O

sets O
of O
20 O
minibatches O
according O
to B-DAT
length. O
This O
also O
groups O
monolin O

the O
output O
layer O
remains O
sensitive O
to B-DAT
the O
source O
context, O
and O
that O

from O
monolingual O
data, O
we O
propose O
to B-DAT
pair O
monolingual O
training O
instances O
with O

text O
with O
mteval-v13a.pl O
for O
comparison O
to B-DAT
official O
WMT O
and O
IWSLT O
results O

text O
with O
multi-bleu.perl O
for O
comparison O
to B-DAT
results O
by O
Gülçehre O
et O
al O

not O
ensembles. O
We O
leave O
it O
to B-DAT
fu- O
ture O
work O
to O
explore O

training O
with O
synthetic O
data O
is O
to B-DAT
the O
quality O
of O
the O
back O

15 O
test O
sets O
to B-DAT
investigate O
a O
cross-domain O
setting.5 O

2007), O
and O
removal O
of O
non-surface O
to B-DAT

We O
found O
overfitting O
to B-DAT
be O
a O
bigger O
problem O
than O

et O
al. O
(2015), O
in O
contrast O
to B-DAT
the O
threshold O
1 O
that O
we O

as O
long O
as O
the O
baseline O
to B-DAT
provide O
the O
training O
al- O
gorithm O

the O
quality O
improvement O
is O
due O
to B-DAT
the O
monolingual O
training O
instances, O
and O

best O
single O
system O
achieves O
a O
to B-DAT

- O
kenized O
BLEU O
(as O
opposed O
to B-DAT
untokenized O
scores O
reported O
in O
Table O

of O
training O
instances O
varies O
due O
to B-DAT
differences O
in O
training O
time O
and O

if O
it O
can O
be O
used O
to B-DAT
adapt O
a O
model O
to O
a O

system O
trained O
on O
WMT O
data O
to B-DAT
translating O
TED O
talks O

Systems O
1 O
and O
2 O
correspond O
to B-DAT
systems O
in O
Table O
3, O
trained O

system O
trained O
on O
WMT O
data O
to B-DAT
the O
TED O
do- O
main. O
By O

BLEU O
on O
average. O
To O
compare O
to B-DAT
what O
extent O
syn- O
thetic O
data O

of O
the O
parallel O
training O
text O
to B-DAT
obtain O
the O
training O
corpus O
parallelsynth O

despite O
being O
out-of-domain O
in O
relation O
to B-DAT
the O
test O
sets. O
We O
speculate O

monolin- O
gual O
data O
would O
lead O
to B-DAT
even O
higher O
improvements O

monolingual O
data, O
but O
this O
led O
to B-DAT
decreased O
BLEU O
scores O

difference O
in O
back-translation O
quality O
leads O
to B-DAT
a O
0.6–0.7 O
BLEU O
difference O
in O

size, O
and O
we O
leave O
it O
to B-DAT
future O
research O
to O
explore O
how O

of O
each O
training O
run). O
Thanks O
to B-DAT
the O
increased O
diversity O
of O
the O

4.3 O
Contrast O
to B-DAT
Phrase-based O
SMT O

data O
into B-DAT
the O
source O
language O
to O
produce O
synthetic O
parallel O
text O
has O

of O
training O
instances O
varies O
due O
to B-DAT
early O
stopping O

In O
contrast O
to B-DAT
phrase-based O
SMT, O
which O
can O
make O

so O
far O
not O
been O
able O
to B-DAT
use O
mono- O
lingual O
data O
to O

parallel O
data O
is O
not O
limited O
to B-DAT
domain O
adaptation, O
and O
that O
even O

hypothesis O
that O
domain O
adaptation O
contributes O
to B-DAT
the O
effectiveness O
of O
adding O
synthetic O

data O
to B-DAT
NMT O
training O

in O
data, O
or O
natural O
according O
to B-DAT
native O
speaker. O
English→German; O
newstest2015; O
ensemble O

ency, O
its O
ability O
to B-DAT
produce O
natural O
target-language O
sentences. O
As O

a O
proxy O
to B-DAT
sentence-level O
flu- O
ency, O
we O
investigate O

of O
each O
sys- O
tem O
according O
to B-DAT
their O
naturalness13 O
, O
distinguish- O
ing O

state O
of O
the O
language O
model O
to B-DAT
the O
de- O
coder O
state O
of O

synthetic O
parallel O
texts O
bears O
resemblance O
to B-DAT
data O
augmentation O
techniques O
used O
in O

is O
that O
self-training O
typically O
refers O
to B-DAT
scenario O
where O
the O
training O
set O

continued O
training O
has O
been O
shown O
to B-DAT
be O
effective O
for O
neural O
language O

and O
in O
work O
par- O
allel O
to B-DAT
ours, O
for O
neural O
translation O
models O

2015). O
We O
are O
the O
first O
to B-DAT
show O
that O
we O
can O
effectively O

we O
propose O
two O
simple O
methods O
to B-DAT
use O
monolingual O
training O
data O
during O

NMT O
systems, O
with O
no O
changes O
to B-DAT
the O
network O

dummy O
source O
context O
was O
successful O
to B-DAT
some O
ex- O
tent, O
but O
we O

the O
neural O
net- O
work O
architecture O
to B-DAT
integrate O
monolingual O
train- O
ing O
data O

approach O
can O
be O
easily O
applied O
to B-DAT
other O
NMT O
systems. O
We O
expect O

on O
the O
amount O
(and O
similarity O
to B-DAT
the O
test O
set) O
of O
available O

Machine O
Translation O
by O
Jointly O
Learning O
to B-DAT
Align O
and O
Trans- O
late. O
In O

Roossin. O
1990. O
A O
Statistical O
Approach O
to B-DAT
Machine O
Translation. O
Computational O
Linguistics, O
16(2):79–85 O

Manning. O
2015. O
Effective O
Ap- O
proaches O
to B-DAT
Attention-based O
Neural O
Machine O
Trans- O
lation O

Quoc O
V. O
Le. O
2014. O
Sequence O
to B-DAT
Sequence O
Learning O
with O
Neural O
Networks O

4.3 O
Contrast O
to B-DAT
Phrase-based O
SMT O

models O
on O
an O
internal O
US O
English B-DAT
dataset[12], O
which O
contains O
24.6 O
hours O

models O
on O
an O
internal O
US O
English B-DAT
dataset[12], O
which O
contains O
24.6 O
hours O

speech O
databases O
from O
which O
Google’s O
North B-DAT
American O
English O
and O
Mandarin O
Chinese O

TTS O
systems O
are O
built. O
The O
North B-DAT
American O
English O
dataset O
contains O
24.6 O

MOS O
in O
naturalness O
Speech O
samples O
North B-DAT
American O
English O
Mandarin O
Chinese O

8 O
and O
63 O
stimuli O
for O
North B-DAT
American O
English O
and O
Mandarin O
Chinese O

North B-DAT
23.3 O
63.6 O
13.1 O
� O
10−9 O

databases O
from O
which O
Google’s O
North B-DAT
American I-DAT
English I-DAT
and O
Mandarin O
Chinese O
TTS O

systems O
are O
built. O
The O
North B-DAT
American I-DAT
English I-DAT
dataset O
contains O
24.6 O
hours O

in O
naturalness O
Speech O
samples O
North B-DAT
American I-DAT
English I-DAT
Mandarin O
Chinese O

Mandarin O
ChineseNorth B-DAT
American I-DAT
English I-DAT

Mandarin O
ChineseNorth B-DAT
American I-DAT
English I-DAT

Mandarin O
ChineseNorth B-DAT
American I-DAT
English I-DAT

and O
63 O
stimuli O
for O
North B-DAT
American I-DAT
English I-DAT
and O
Mandarin O
Chinese, O
respectively O

23.3 O
63.6 O
13.1 O
� O
10−9 O
American B-DAT
18.7 O
69.3 O
12.0 O
� O
10−9 O

and O
concatenative O
systems O
for O
both O
English B-DAT
and O
Mandarin. O
A O
single O
WaveNet O

on O
text). O
We O
used O
the O
English B-DAT
multi-speaker O
corpus O
from O
CSTR O
voice O

from O
which O
Google’s O
North O
American O
English B-DAT
and O
Mandarin O
Chinese O
TTS O
systems O

are O
built. O
The O
North O
American O
English B-DAT
dataset O
contains O
24.6 O
hours O
of O

to O
0.34 O
(51%) O
in O
US O
English B-DAT
and O
0.42 O
to O
0.13 O
(69 O

naturalness O
Speech O
samples O
North O
American O
English B-DAT
Mandarin O
Chinese O

Mandarin O
ChineseNorth O
American O
English B-DAT

Mandarin O
ChineseNorth O
American O
English B-DAT

Mandarin O
ChineseNorth O
American O
English B-DAT

Yamagishi, O
Junichi. O
English B-DAT
multi-speaker O
corpus O
for O
CSTR O
voice O

for O
HMM-based O
speech O
synthesis O
in O
English, B-DAT
2006. O
URL O
http://hts.sp.nitech.ac.jp/?Download O

63 O
stimuli O
for O
North O
American O
English B-DAT
and O
Mandarin O
Chinese, O
respectively. O
Test O

18.7 O
69.3 O
12.0 O
� O
10−9 O
English B-DAT
7.6 O
82.0 O
10.4 O
� O
10−9 O

speech O
databases O
from O
which O
Google’s O
North B-DAT
American I-DAT
English I-DAT
and O
Mandarin O
Chinese O
TTS O
systems O

are O
built. O
The O
North B-DAT
American I-DAT
English I-DAT
dataset O
contains O
24.6 O
hours O
of O

MOS O
in O
naturalness O
Speech O
samples O
North B-DAT
American I-DAT
English I-DAT
Mandarin O
Chinese O

8 O
and O
63 O
stimuli O
for O
North B-DAT
American I-DAT
English I-DAT
and O
Mandarin O
Chinese, O
respectively. O
Test O

train O
Tacotron O
on O
an O
internal O
North B-DAT
American O
English O
dataset, O
which O
contains O

Tacotron O
on O
an O
internal O
North B-DAT
American I-DAT
English I-DAT
dataset, O
which O
contains O
about O

mean O
opinion O
score O
on O
US O
English, B-DAT
outperforming O
a O
pro- O
duction O
parametric O

score O
(MOS) O
on O
an O
US O
English B-DAT
eval O
set, O
outperforming O
a O
production O

on O
an O
internal O
North O
American O
English B-DAT
dataset, O
which O
contains O
about O
24.6 O

3.82 O
MOS O
score O
on O
US O
English, B-DAT
outperforming O
a O
production O
parametric O
system O

train O
Tacotron O
on O
an O
internal O
North B-DAT
American I-DAT
English I-DAT
dataset, O
which O
contains O
about O
24.6 O

speech O
databases O
from O
which O
Google’s O
North B-DAT
American O
English O
and O
Mandarin O
Chinese O

TTS O
systems O
are O
built. O
The O
North B-DAT
American O
English O
dataset O
contains O
24.6 O

MOS O
in O
naturalness O
Speech O
samples O
North B-DAT
American O
English O
Mandarin O
Chinese O

8 O
and O
63 O
stimuli O
for O
North B-DAT
American O
English O
and O
Mandarin O
Chinese O

North B-DAT
23.3 O
63.6 O
13.1 O
� O
10−9 O

databases O
from O
which O
Google’s O
North B-DAT
American I-DAT
English I-DAT
and O
Mandarin O
Chinese O
TTS O

systems O
are O
built. O
The O
North B-DAT
American I-DAT
English I-DAT
dataset O
contains O
24.6 O
hours O

in O
naturalness O
Speech O
samples O
North B-DAT
American I-DAT
English I-DAT
Mandarin O
Chinese O

Mandarin O
ChineseNorth B-DAT
American I-DAT
English I-DAT

Mandarin O
ChineseNorth B-DAT
American I-DAT
English I-DAT

Mandarin O
ChineseNorth B-DAT
American I-DAT
English I-DAT

and O
63 O
stimuli O
for O
North B-DAT
American I-DAT
English I-DAT
and O
Mandarin O
Chinese, O
respectively O

23.3 O
63.6 O
13.1 O
� O
10−9 O
American B-DAT
18.7 O
69.3 O
12.0 O
� O
10−9 O

and O
concatenative O
systems O
for O
both O
English B-DAT
and O
Mandarin. O
A O
single O
WaveNet O

on O
text). O
We O
used O
the O
English B-DAT
multi-speaker O
corpus O
from O
CSTR O
voice O

from O
which O
Google’s O
North O
American O
English B-DAT
and O
Mandarin O
Chinese O
TTS O
systems O

are O
built. O
The O
North O
American O
English B-DAT
dataset O
contains O
24.6 O
hours O
of O

to O
0.34 O
(51%) O
in O
US O
English B-DAT
and O
0.42 O
to O
0.13 O
(69 O

naturalness O
Speech O
samples O
North O
American O
English B-DAT
Mandarin O
Chinese O

Mandarin O
ChineseNorth O
American O
English B-DAT

Mandarin O
ChineseNorth O
American O
English B-DAT

Mandarin O
ChineseNorth O
American O
English B-DAT

Yamagishi, O
Junichi. O
English B-DAT
multi-speaker O
corpus O
for O
CSTR O
voice O

for O
HMM-based O
speech O
synthesis O
in O
English, B-DAT
2006. O
URL O
http://hts.sp.nitech.ac.jp/?Download O

63 O
stimuli O
for O
North O
American O
English B-DAT
and O
Mandarin O
Chinese, O
respectively. O
Test O

18.7 O
69.3 O
12.0 O
� O
10−9 O
English B-DAT
7.6 O
82.0 O
10.4 O
� O
10−9 O

speech O
databases O
from O
which O
Google’s O
North B-DAT
American I-DAT
English I-DAT
and O
Mandarin O
Chinese O
TTS O
systems O

are O
built. O
The O
North B-DAT
American I-DAT
English I-DAT
dataset O
contains O
24.6 O
hours O
of O

MOS O
in O
naturalness O
Speech O
samples O
North B-DAT
American I-DAT
English I-DAT
Mandarin O
Chinese O

8 O
and O
63 O
stimuli O
for O
North B-DAT
American I-DAT
English I-DAT
and O
Mandarin O
Chinese, O
respectively. O
Test O

speech O
databases O
from O
which O
Google’s O
North B-DAT
American O
English O
and O
Mandarin O
Chinese O

TTS O
systems O
are O
built. O
The O
North B-DAT
American O
English O
dataset O
contains O
24.6 O

MOS O
in O
naturalness O
Speech O
samples O
North B-DAT
American O
English O
Mandarin O
Chinese O

8 O
and O
63 O
stimuli O
for O
North B-DAT
American O
English O
and O
Mandarin O
Chinese O

North B-DAT
23.3 O
63.6 O
13.1 O
� O
10−9 O

databases O
from O
which O
Google’s O
North B-DAT
American I-DAT
English I-DAT
and O
Mandarin O
Chinese O
TTS O

systems O
are O
built. O
The O
North B-DAT
American I-DAT
English I-DAT
dataset O
contains O
24.6 O
hours O

in O
naturalness O
Speech O
samples O
North B-DAT
American I-DAT
English I-DAT
Mandarin O
Chinese O

Mandarin O
ChineseNorth B-DAT
American I-DAT
English I-DAT

Mandarin O
ChineseNorth B-DAT
American I-DAT
English I-DAT

Mandarin O
ChineseNorth B-DAT
American I-DAT
English I-DAT

and O
63 O
stimuli O
for O
North B-DAT
American I-DAT
English I-DAT
and O
Mandarin O
Chinese, O
respectively O

23.3 O
63.6 O
13.1 O
� O
10−9 O
American B-DAT
18.7 O
69.3 O
12.0 O
� O
10−9 O

and O
concatenative O
systems O
for O
both O
English B-DAT
and O
Mandarin. O
A O
single O
WaveNet O

on O
text). O
We O
used O
the O
English B-DAT
multi-speaker O
corpus O
from O
CSTR O
voice O

from O
which O
Google’s O
North O
American O
English B-DAT
and O
Mandarin O
Chinese O
TTS O
systems O

are O
built. O
The O
North O
American O
English B-DAT
dataset O
contains O
24.6 O
hours O
of O

to O
0.34 O
(51%) O
in O
US O
English B-DAT
and O
0.42 O
to O
0.13 O
(69 O

naturalness O
Speech O
samples O
North O
American O
English B-DAT
Mandarin O
Chinese O

Mandarin O
ChineseNorth O
American O
English B-DAT

Mandarin O
ChineseNorth O
American O
English B-DAT

Mandarin O
ChineseNorth O
American O
English B-DAT

Yamagishi, O
Junichi. O
English B-DAT
multi-speaker O
corpus O
for O
CSTR O
voice O

for O
HMM-based O
speech O
synthesis O
in O
English, B-DAT
2006. O
URL O
http://hts.sp.nitech.ac.jp/?Download O

63 O
stimuli O
for O
North O
American O
English B-DAT
and O
Mandarin O
Chinese, O
respectively. O
Test O

18.7 O
69.3 O
12.0 O
� O
10−9 O
English B-DAT
7.6 O
82.0 O
10.4 O
� O
10−9 O

speech O
databases O
from O
which O
Google’s O
North B-DAT
American I-DAT
English I-DAT
and O
Mandarin O
Chinese O
TTS O
systems O

are O
built. O
The O
North B-DAT
American I-DAT
English I-DAT
dataset O
contains O
24.6 O
hours O
of O

MOS O
in O
naturalness O
Speech O
samples O
North B-DAT
American I-DAT
English I-DAT
Mandarin O
Chinese O

8 O
and O
63 O
stimuli O
for O
North B-DAT
American I-DAT
English I-DAT
and O
Mandarin O
Chinese, O
respectively. O
Test O

North O
American O
English O
and O
Mandarin O
Chinese B-DAT
TTS O
systems O
are O
built. O
The O

speech O
data, O
and O
the O
Mandarin O
Chinese B-DAT
dataset O
contains O
34.8 O
hours; O
both O

to O
0.13 O
(69%) O
in O
Mandarin O
Chinese B-DAT

samples O
North O
American O
English O
Mandarin O
Chinese B-DAT

North O
American O
English O
and O
Mandarin O
Chinese, B-DAT
respectively. O
Test O
stimuli O
were O
randomly O

50.6 O
15.6 O
33.8 O
� O
10−9 O
Chinese B-DAT
25.0 O
23.3 O
51.8 O
0.476 O

systems O
for O
both O
English O
and O
Mandarin B-DAT

Google’s O
North O
American O
English O
and O
Mandarin B-DAT
Chinese O
TTS O
systems O
are O
built O

of O
speech O
data, O
and O
the O
Mandarin B-DAT
Chinese O
dataset O
contains O
34.8 O
hours O

0.42 O
to O
0.13 O
(69%) O
in O
Mandarin B-DAT
Chinese O

Speech O
samples O
North O
American O
English O
Mandarin B-DAT
Chinese O

Mandarin B-DAT
ChineseNorth O
American O
English O

Mandarin B-DAT
ChineseNorth O
American O
English O

Mandarin B-DAT
ChineseNorth O
American O
English O

for O
North O
American O
English O
and O
Mandarin B-DAT
Chinese, O
respectively. O
Test O
stimuli O
were O

Mandarin B-DAT
50.6 O
15.6 O
33.8 O
� O
10−9 O

Google’s O
North O
American O
English O
and O
Mandarin B-DAT
Chinese I-DAT
TTS O
systems O
are O
built. O
The O

of O
speech O
data, O
and O
the O
Mandarin B-DAT
Chinese I-DAT
dataset O
contains O
34.8 O
hours; O
both O

0.42 O
to O
0.13 O
(69%) O
in O
Mandarin B-DAT
Chinese I-DAT

Speech O
samples O
North O
American O
English O
Mandarin B-DAT
Chinese I-DAT

for O
North O
American O
English O
and O
Mandarin B-DAT
Chinese, I-DAT
respectively. O
Test O
stimuli O
were O
randomly O

North O
American O
English O
and O
Mandarin O
Chinese B-DAT
TTS O
systems O
are O
built. O
The O

speech O
data, O
and O
the O
Mandarin O
Chinese B-DAT
dataset O
contains O
34.8 O
hours; O
both O

to O
0.13 O
(69%) O
in O
Mandarin O
Chinese B-DAT

samples O
North O
American O
English O
Mandarin O
Chinese B-DAT

North O
American O
English O
and O
Mandarin O
Chinese, B-DAT
respectively. O
Test O
stimuli O
were O
randomly O

50.6 O
15.6 O
33.8 O
� O
10−9 O
Chinese B-DAT
25.0 O
23.3 O
51.8 O
0.476 O

systems O
for O
both O
English O
and O
Mandarin B-DAT

Google’s O
North O
American O
English O
and O
Mandarin B-DAT
Chinese O
TTS O
systems O
are O
built O

of O
speech O
data, O
and O
the O
Mandarin B-DAT
Chinese O
dataset O
contains O
34.8 O
hours O

0.42 O
to O
0.13 O
(69%) O
in O
Mandarin B-DAT
Chinese O

Speech O
samples O
North O
American O
English O
Mandarin B-DAT
Chinese O

Mandarin B-DAT
ChineseNorth O
American O
English O

Mandarin B-DAT
ChineseNorth O
American O
English O

Mandarin B-DAT
ChineseNorth O
American O
English O

for O
North O
American O
English O
and O
Mandarin B-DAT
Chinese, O
respectively. O
Test O
stimuli O
were O

Mandarin B-DAT
50.6 O
15.6 O
33.8 O
� O
10−9 O

Google’s O
North O
American O
English O
and O
Mandarin B-DAT
Chinese I-DAT
TTS O
systems O
are O
built. O
The O

of O
speech O
data, O
and O
the O
Mandarin B-DAT
Chinese I-DAT
dataset O
contains O
34.8 O
hours; O
both O

0.42 O
to O
0.13 O
(69%) O
in O
Mandarin B-DAT
Chinese I-DAT

Speech O
samples O
North O
American O
English O
Mandarin B-DAT
Chinese I-DAT

for O
North O
American O
English O
and O
Mandarin B-DAT
Chinese, I-DAT
respectively. O
Test O
stimuli O
were O
randomly O

North O
American O
English O
and O
Mandarin O
Chinese B-DAT
TTS O
systems O
are O
built. O
The O

speech O
data, O
and O
the O
Mandarin O
Chinese B-DAT
dataset O
contains O
34.8 O
hours; O
both O

to O
0.13 O
(69%) O
in O
Mandarin O
Chinese B-DAT

samples O
North O
American O
English O
Mandarin O
Chinese B-DAT

North O
American O
English O
and O
Mandarin O
Chinese, B-DAT
respectively. O
Test O
stimuli O
were O
randomly O

50.6 O
15.6 O
33.8 O
� O
10−9 O
Chinese B-DAT
25.0 O
23.3 O
51.8 O
0.476 O

systems O
for O
both O
English O
and O
Mandarin B-DAT

Google’s O
North O
American O
English O
and O
Mandarin B-DAT
Chinese O
TTS O
systems O
are O
built O

of O
speech O
data, O
and O
the O
Mandarin B-DAT
Chinese O
dataset O
contains O
34.8 O
hours O

0.42 O
to O
0.13 O
(69%) O
in O
Mandarin B-DAT
Chinese O

Speech O
samples O
North O
American O
English O
Mandarin B-DAT
Chinese O

Mandarin B-DAT
ChineseNorth O
American O
English O

Mandarin B-DAT
ChineseNorth O
American O
English O

Mandarin B-DAT
ChineseNorth O
American O
English O

for O
North O
American O
English O
and O
Mandarin B-DAT
Chinese, O
respectively. O
Test O
stimuli O
were O

Mandarin B-DAT
50.6 O
15.6 O
33.8 O
� O
10−9 O

Google’s O
North O
American O
English O
and O
Mandarin B-DAT
Chinese I-DAT
TTS O
systems O
are O
built. O
The O

of O
speech O
data, O
and O
the O
Mandarin B-DAT
Chinese I-DAT
dataset O
contains O
34.8 O
hours; O
both O

0.42 O
to O
0.13 O
(69%) O
in O
Mandarin B-DAT
Chinese I-DAT

Speech O
samples O
North O
American O
English O
Mandarin B-DAT
Chinese I-DAT

for O
North O
American O
English O
and O
Mandarin B-DAT
Chinese, I-DAT
respectively. O
Test O
stimuli O
were O
randomly O

preprocessing O
around O
3.7B O
comments O
from O
Reddit B-DAT
available O
in O
256M O
conver- O
sational O

4M O
dialogue O
turns. O
Furthermore, O
our O
Reddit B-DAT
corpus O
includes O
2 O
more O
years O

substantially O
larger O
than O
the O
previous O
Reddit B-DAT
dataset O
of O
Al-Rfou O
et O
al O

the O
conversation O
thread O
ID O
in O
Reddit, B-DAT
guaranteeing O
that O
the O
same O
split O

each O
example. O
For O
instance, O
in O
Reddit B-DAT
the O
author O
of O
the O
con O

Using O
the O
default O
quotas, O
the O
Reddit B-DAT
script O

Reddit B-DAT
3.7 O
billion O
comments O
in O
threaded O

in O
the O
public O
repository. O
The O
Reddit B-DAT
data O
is O
taken O
from O
January O

provides O
an O
overview O
of O
the O
Reddit, B-DAT
OpenSubtitles O
and O
AmazonQA O
datasets, O
and O

3.1 O
Reddit B-DAT

Reddit B-DAT
is O
an O
American O
social O
news O

in O
discussions O
on O
these O
posts. O
Reddit B-DAT
is O
extremely O
diverse O
(Schrading O
et O

contexts O
paired O
with O
appropriate O
responses. O
Reddit B-DAT
data O
has O
been O
used O
to O

allows O
generating O
datasets O
from O
the O
Reddit B-DAT
data O
in O
a O
reproducible O
manner O

Reddit B-DAT
conversations O
are O
threaded. O
Each O
post O

in O
response. O
In O
processing, O
each O
Reddit B-DAT
thread O
is O
used O
to O
generate O

Reddit B-DAT
OpenSubtitles O
AmazonQA O

of O
domestic O
abuse O
discourse O
on O
Reddit B-DAT

Reddit B-DAT

PolyAI B-DAT

PolyAI B-DAT
Limited, O
London, O
UK O

PolyAI B-DAT

repository O
is O
available O
at O
github.com/ O
PolyAI B-DAT

PolyAI B-DAT

PolyAI B-DAT

PolyAI B-DAT

AmazonQA B-DAT
over O
3.6 O
million O
question-response O
pairs O

of O
the O
Reddit, O
OpenSubtitles O
and O
AmazonQA B-DAT
datasets, O
and O
fig- O
ure O
3 O

3.3 O
AmazonQA B-DAT

Reddit O
OpenSubtitles O
AmazonQA B-DAT

and O
are O
particularly O
strong O
for O
AmazonQA, B-DAT
possibly O
because O
rare O
words O
such O

AmazonQA B-DAT

PolyAI B-DAT

PolyAI B-DAT
Limited, O
London, O
UK O

PolyAI B-DAT

repository O
is O
available O
at O
github.com/ O
PolyAI B-DAT

PolyAI B-DAT

PolyAI B-DAT

PolyAI B-DAT

PolyAI B-DAT

PolyAI B-DAT
Limited, O
London, O
UK O

PolyAI B-DAT

repository O
is O
available O
at O
github.com/ O
PolyAI B-DAT

PolyAI B-DAT

PolyAI B-DAT

PolyAI B-DAT

of O
valid O
pairs O
in O
the O
OpenSubtitles B-DAT
dataset O
is O
316 O
million. O
To O

OpenSubtitles B-DAT
over O
400 O
million O
lines O
from O

to O
December O
2018, O
and O
the O
OpenSubtitles B-DAT
data O
from O
2018 O

an O
overview O
of O
the O
Reddit, O
OpenSubtitles B-DAT
and O
AmazonQA O
datasets, O
and O
fig O

3.2 O
OpenSubtitles B-DAT

OpenSubtitles B-DAT
is O
a O
growing O
online O
collection O

Reddit O
OpenSubtitles B-DAT
AmazonQA O

OpenSubtitles B-DAT

Numenta B-DAT

Numenta B-DAT

Numenta B-DAT

Numenta B-DAT

Numenta B-DAT

Numenta B-DAT

a O
Numenta, B-DAT
Redwood O
City, O
CA, O
USA O

also O
present O
results O
using O
the O
Numenta B-DAT
Anomaly O
Benchmark O
(NAB O

file O
is O
included O
in O
the O
Numenta B-DAT

EC2 O
instance O
(data O
from O
the O
Numenta B-DAT
Anomaly O
Benchmark O
[2] O
). O
A O

Section O
3 O
we O
review O
the O
Numenta B-DAT
Anomaly O

such O
we O
have O
created O
the O
Numenta B-DAT
Anomaly O
Benchmark O
(NAB O

Source O
Anomaly O
Type O
Numenta B-DAT
HTM O
CAD O
OSE O
KNN O
CAD O

Numenta B-DAT
anomaly O
benchmark, O
in: O
Proceedings O
of O

the O
VP O
of O
Research O
at O
Numenta B-DAT

Direc- O
tor O
of O
Engineering O
at O
Numenta B-DAT

Numenta, B-DAT
and O
will O
join O
Apple O
in O

Anomaly B-DAT
detection O

present O
results O
using O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT
(NAB O

Anomaly B-DAT
Benchmark O
corpus O
[2 O

instance O
(data O
from O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT
[2] O
). O
A O
modification O

Anomaly B-DAT
detection O
in O
time-series O
is O
a O

3 O
we O
review O
the O
Numenta O
Anomaly B-DAT

Anomaly B-DAT
detection O
using O
HTM O

we O
have O
created O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT
(NAB O

windows O
or O
the O
data O
length. O
Anomaly B-DAT

Detector O
Latency O
(ms) O
Spatial O
Anomaly B-DAT
Temporal O
Anomaly O
Concept O
Drift O
Non O

ter’s O
Anomaly B-DAT
Detection, O
Etsy’s O
Skyline, O
Multinomial O
Relative O

Source O
Anomaly B-DAT
Type O
Numenta O
HTM O
CAD O
OSE O

Chandola, O
A. O
Banerjee, O
V. O
Kumar, O
Anomaly B-DAT
detection: O
a O
survey, O
ACM O
Comput O

tomated O
Time-series O
Anomaly B-DAT
Detection, O
in: O
Proceedings O
of O
the O

28] O
P. O
Angelov, O
Anomaly B-DAT
detection O
based O
on O
eccentricity O
analysis O

Lee, O
Y.R. O
Yeh, O
Y.C.F. O
Wang, O
Anomaly B-DAT
detection O
via O
online O
oversampling O
prin O

Engineering: O
Introducing O
Practical O
and O
Robust O
Anomaly B-DAT

Banerjee O
, O
V. O
Kumar O
, O
Anomaly B-DAT
detection: O
A O
survey, O
ACM O
Com O

2 O
Anomaly B-DAT
detection O
using O
HTM O

Benchmark B-DAT
dataset O

results O
using O
the O
Numenta O
Anomaly O
Benchmark B-DAT
(NAB O

Anomaly O
Benchmark B-DAT
corpus O
[2 O

data O
from O
the O
Numenta O
Anomaly O
Benchmark B-DAT
[2] O
). O
A O
modification O
to O

Benchmark B-DAT
(NAB) O
[2] O
, O
a O
rigorous O

have O
created O
the O
Numenta O
Anomaly O
Benchmark B-DAT
(NAB O

1. O
Benchmark B-DAT
dataset O

3.1 O
Benchmark B-DAT
dataset O

also O
present O
results O
using O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT
(NAB O

EC2 O
instance O
(data O
from O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT
[2] O
). O
A O
modification O
to O

such O
we O
have O
created O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT
(NAB O

Numenta B-DAT

Numenta B-DAT

Numenta B-DAT

Numenta B-DAT

Numenta B-DAT

Numenta B-DAT

a O
Numenta, B-DAT
Redwood O
City, O
CA, O
USA O

also O
present O
results O
using O
the O
Numenta B-DAT
Anomaly O
Benchmark O
(NAB O

file O
is O
included O
in O
the O
Numenta B-DAT

EC2 O
instance O
(data O
from O
the O
Numenta B-DAT
Anomaly O
Benchmark O
[2] O
). O
A O

Section O
3 O
we O
review O
the O
Numenta B-DAT
Anomaly O

such O
we O
have O
created O
the O
Numenta B-DAT
Anomaly O
Benchmark O
(NAB O

Source O
Anomaly O
Type O
Numenta B-DAT
HTM O
CAD O
OSE O
KNN O
CAD O

Numenta B-DAT
anomaly O
benchmark, O
in: O
Proceedings O
of O

the O
VP O
of O
Research O
at O
Numenta B-DAT

Direc- O
tor O
of O
Engineering O
at O
Numenta B-DAT

Numenta, B-DAT
and O
will O
join O
Apple O
in O

Anomaly B-DAT
detection O

present O
results O
using O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT
(NAB O

Anomaly B-DAT
Benchmark O
corpus O
[2 O

instance O
(data O
from O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT
[2] O
). O
A O
modification O

Anomaly B-DAT
detection O
in O
time-series O
is O
a O

3 O
we O
review O
the O
Numenta O
Anomaly B-DAT

Anomaly B-DAT
detection O
using O
HTM O

we O
have O
created O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT
(NAB O

windows O
or O
the O
data O
length. O
Anomaly B-DAT

Detector O
Latency O
(ms) O
Spatial O
Anomaly B-DAT
Temporal O
Anomaly O
Concept O
Drift O
Non O

ter’s O
Anomaly B-DAT
Detection, O
Etsy’s O
Skyline, O
Multinomial O
Relative O

Source O
Anomaly B-DAT
Type O
Numenta O
HTM O
CAD O
OSE O

Chandola, O
A. O
Banerjee, O
V. O
Kumar, O
Anomaly B-DAT
detection: O
a O
survey, O
ACM O
Comput O

tomated O
Time-series O
Anomaly B-DAT
Detection, O
in: O
Proceedings O
of O
the O

28] O
P. O
Angelov, O
Anomaly B-DAT
detection O
based O
on O
eccentricity O
analysis O

Lee, O
Y.R. O
Yeh, O
Y.C.F. O
Wang, O
Anomaly B-DAT
detection O
via O
online O
oversampling O
prin O

Engineering: O
Introducing O
Practical O
and O
Robust O
Anomaly B-DAT

Banerjee O
, O
V. O
Kumar O
, O
Anomaly B-DAT
detection: O
A O
survey, O
ACM O
Com O

2 O
Anomaly B-DAT
detection O
using O
HTM O

Benchmark B-DAT
dataset O

results O
using O
the O
Numenta O
Anomaly O
Benchmark B-DAT
(NAB O

Anomaly O
Benchmark B-DAT
corpus O
[2 O

data O
from O
the O
Numenta O
Anomaly O
Benchmark B-DAT
[2] O
). O
A O
modification O
to O

Benchmark B-DAT
(NAB) O
[2] O
, O
a O
rigorous O

have O
created O
the O
Numenta O
Anomaly O
Benchmark B-DAT
(NAB O

1. O
Benchmark B-DAT
dataset O

3.1 O
Benchmark B-DAT
dataset O

also O
present O
results O
using O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT
(NAB O

EC2 O
instance O
(data O
from O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT
[2] O
). O
A O
modification O
to O

such O
we O
have O
created O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT
(NAB O

Algorithms O
– O
the O
Numenta B-DAT
Anomaly O
Benchmark O

Numenta, B-DAT
Inc. O
Redwood O
City, O
CA O

Numenta, B-DAT
Inc. O
Redwood O
City, O
CA O

Numenta B-DAT
Anomaly O
Benchmark,” O
in O
14th O
International O

Algorithms O
– O
the O
Numenta B-DAT
Anomaly O
Benchmark O

Numenta, B-DAT
Inc. O
Redwood O
City, O
CA O

Numenta, B-DAT
Inc. O
Redwood O
City, O
CA O

detectors. O
Here O
we O
propose O
the O
Numenta B-DAT
Anomaly O

paper O
is O
to O
introduce O
the O
Numenta B-DAT
Anomaly O
Benchmark O

algorithms. O
At O
Numenta B-DAT
we O
have O
developed O
an O
anomaly O

four O
primary O
algorithms O
are O
the O
Numenta B-DAT
HTM O
anomaly O

The O
HTM O
detector O
(developed O
by O
Numenta B-DAT
and O
the O

The O
Numenta B-DAT
algorithm O
has O
several O
features O
that O

etc. O
Like O
the O
Numenta B-DAT
algorithm, O
Skyline O
is O
well O
suited O

1. O
Numenta B-DAT
HTM O
64.7 O
56.5 O
69.3 O

technical O
report]. O
Palo O
Alto, O
CA: O
Numenta, B-DAT
Inc O

6] O
(2015) O
Numenta B-DAT
Applications O
[Online O
website]. O
Redwood O
City O

CA: O
Numenta, B-DAT
Inc. O
Available: O
http://numenta.com/#applications O

15] O
Numenta, B-DAT
Inc. O
(2015) O
NAB: O
Numenta O
Anomaly O
Benchmark O

code O
repository]. O
Redwood O
City, O
CA: O
Numenta, B-DAT
Inc O

City, O
CA: O
Numenta, B-DAT
Inc. O
Available O

CA: O
Numenta, B-DAT
Inc. O
Available: O
http://numenta.com/#technology O

Online O
video]. O
Redwood O
City, O
CA: O
Numenta, B-DAT
Inc. O
Available O

20] O
Numenta, B-DAT
Inc. O
(2015) O
NuPIC: O
Numenta O
Platform O
for O
Intelligent O

Numenta, B-DAT
Inc. O
Available: O
https://github.com/numenta/nupic O

Evaluating O
Real-time O
Anomaly B-DAT
Detection O

Algorithms O
– O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

and O
S. O
Ahmad, O
“Evaluating O
Real-time O
Anomaly B-DAT
Detection O
Algorithms O
– O
the O

Numenta B-DAT
Anomaly I-DAT
Benchmark,” I-DAT
in O
14th O
International O
Conference O

Evaluating O
Real-time O
Anomaly B-DAT
Detection O

Algorithms O
– O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

Here O
we O
propose O
the O
Numenta O
Anomaly B-DAT

energy, O
e-commerce, O
and O
social O
media. O
Anomaly B-DAT

is O
to O
introduce O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

Anomaly B-DAT
detection O
in O
real-world O
streaming O
applications O

B. O
Scoring O
Real-Time O
Anomaly B-DAT
Detectors O

Anomaly B-DAT
Detection O

Series O
Anomaly B-DAT
Detection O
[Online O
blog]. O
Available O

Scalable O
Framework O
for O
Automated O
Time-series O
Anomaly B-DAT

Numenta, O
Inc. O
(2015) O
NAB: O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

Anomaly B-DAT
Detection O
[Online O
technical O
report]. O
Redwood O

2014, O
October O
17) O
Science O
of O
Anomaly B-DAT
Detection O

Algorithms O
– O
the O
Numenta O
Anomaly O
Benchmark B-DAT

Numenta O
Anomaly O
Benchmark B-DAT

Algorithms O
– O
the O
Numenta O
Anomaly O
Benchmark B-DAT

Benchmark B-DAT
(NAB), O
which O
attempts O
to O
provide O

to O
introduce O
the O
Numenta O
Anomaly O
Benchmark B-DAT

A. O
Benchmark B-DAT
Dataset O

I. O
Benchmark B-DAT
dataset: O
real O
world O
time-series O
data O

Yahoo O
Labs O
News: O
Announcing O
A O
Benchmark B-DAT
Dataset O
For O
Time O

Inc. O
(2015) O
NAB: O
Numenta O
Anomaly O
Benchmark B-DAT

Algorithms O
– O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

Algorithms O
– O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

paper O
is O
to O
introduce O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

15] O
Numenta, O
Inc. O
(2015) O
NAB: O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

II. O
NUMENTA B-DAT
ANOMALY I-DAT
BENCHMARK I-DAT

Numenta B-DAT

Numenta B-DAT

Numenta B-DAT

Numenta B-DAT

Numenta B-DAT

Numenta B-DAT

a O
Numenta, B-DAT
Redwood O
City, O
CA, O
USA O

also O
present O
results O
using O
the O
Numenta B-DAT
Anomaly O
Benchmark O
(NAB O

file O
is O
included O
in O
the O
Numenta B-DAT

EC2 O
instance O
(data O
from O
the O
Numenta B-DAT
Anomaly O
Benchmark O
[2] O
). O
A O

Section O
3 O
we O
review O
the O
Numenta B-DAT
Anomaly O

such O
we O
have O
created O
the O
Numenta B-DAT
Anomaly O
Benchmark O
(NAB O

Source O
Anomaly O
Type O
Numenta B-DAT
HTM O
CAD O
OSE O
KNN O
CAD O

Numenta B-DAT
anomaly O
benchmark, O
in: O
Proceedings O
of O

the O
VP O
of O
Research O
at O
Numenta B-DAT

Direc- O
tor O
of O
Engineering O
at O
Numenta B-DAT

Numenta, B-DAT
and O
will O
join O
Apple O
in O

Anomaly B-DAT
detection O

present O
results O
using O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT
(NAB O

Anomaly B-DAT
Benchmark O
corpus O
[2 O

instance O
(data O
from O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT
[2] O
). O
A O
modification O

Anomaly B-DAT
detection O
in O
time-series O
is O
a O

3 O
we O
review O
the O
Numenta O
Anomaly B-DAT

Anomaly B-DAT
detection O
using O
HTM O

we O
have O
created O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT
(NAB O

windows O
or O
the O
data O
length. O
Anomaly B-DAT

Detector O
Latency O
(ms) O
Spatial O
Anomaly B-DAT
Temporal O
Anomaly O
Concept O
Drift O
Non O

ter’s O
Anomaly B-DAT
Detection, O
Etsy’s O
Skyline, O
Multinomial O
Relative O

Source O
Anomaly B-DAT
Type O
Numenta O
HTM O
CAD O
OSE O

Chandola, O
A. O
Banerjee, O
V. O
Kumar, O
Anomaly B-DAT
detection: O
a O
survey, O
ACM O
Comput O

tomated O
Time-series O
Anomaly B-DAT
Detection, O
in: O
Proceedings O
of O
the O

28] O
P. O
Angelov, O
Anomaly B-DAT
detection O
based O
on O
eccentricity O
analysis O

Lee, O
Y.R. O
Yeh, O
Y.C.F. O
Wang, O
Anomaly B-DAT
detection O
via O
online O
oversampling O
prin O

Engineering: O
Introducing O
Practical O
and O
Robust O
Anomaly B-DAT

Banerjee O
, O
V. O
Kumar O
, O
Anomaly B-DAT
detection: O
A O
survey, O
ACM O
Com O

2 O
Anomaly B-DAT
detection O
using O
HTM O

Benchmark B-DAT
dataset O

results O
using O
the O
Numenta O
Anomaly O
Benchmark B-DAT
(NAB O

Anomaly O
Benchmark B-DAT
corpus O
[2 O

data O
from O
the O
Numenta O
Anomaly O
Benchmark B-DAT
[2] O
). O
A O
modification O
to O

Benchmark B-DAT
(NAB) O
[2] O
, O
a O
rigorous O

have O
created O
the O
Numenta O
Anomaly O
Benchmark B-DAT
(NAB O

1. O
Benchmark B-DAT
dataset O

3.1 O
Benchmark B-DAT
dataset O

also O
present O
results O
using O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT
(NAB O

EC2 O
instance O
(data O
from O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT
[2] O
). O
A O
modification O
to O

such O
we O
have O
created O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT
(NAB O

Algorithms O
– O
the O
Numenta B-DAT
Anomaly O
Benchmark O

Numenta, B-DAT
Inc. O
Redwood O
City, O
CA O

Numenta, B-DAT
Inc. O
Redwood O
City, O
CA O

Numenta B-DAT
Anomaly O
Benchmark,” O
in O
14th O
International O

Algorithms O
– O
the O
Numenta B-DAT
Anomaly O
Benchmark O

Numenta, B-DAT
Inc. O
Redwood O
City, O
CA O

Numenta, B-DAT
Inc. O
Redwood O
City, O
CA O

detectors. O
Here O
we O
propose O
the O
Numenta B-DAT
Anomaly O

paper O
is O
to O
introduce O
the O
Numenta B-DAT
Anomaly O
Benchmark O

algorithms. O
At O
Numenta B-DAT
we O
have O
developed O
an O
anomaly O

four O
primary O
algorithms O
are O
the O
Numenta B-DAT
HTM O
anomaly O

The O
HTM O
detector O
(developed O
by O
Numenta B-DAT
and O
the O

The O
Numenta B-DAT
algorithm O
has O
several O
features O
that O

etc. O
Like O
the O
Numenta B-DAT
algorithm, O
Skyline O
is O
well O
suited O

1. O
Numenta B-DAT
HTM O
64.7 O
56.5 O
69.3 O

technical O
report]. O
Palo O
Alto, O
CA: O
Numenta, B-DAT
Inc O

6] O
(2015) O
Numenta B-DAT
Applications O
[Online O
website]. O
Redwood O
City O

CA: O
Numenta, B-DAT
Inc. O
Available: O
http://numenta.com/#applications O

15] O
Numenta, B-DAT
Inc. O
(2015) O
NAB: O
Numenta O
Anomaly O
Benchmark O

code O
repository]. O
Redwood O
City, O
CA: O
Numenta, B-DAT
Inc O

City, O
CA: O
Numenta, B-DAT
Inc. O
Available O

CA: O
Numenta, B-DAT
Inc. O
Available: O
http://numenta.com/#technology O

Online O
video]. O
Redwood O
City, O
CA: O
Numenta, B-DAT
Inc. O
Available O

20] O
Numenta, B-DAT
Inc. O
(2015) O
NuPIC: O
Numenta O
Platform O
for O
Intelligent O

Numenta, B-DAT
Inc. O
Available: O
https://github.com/numenta/nupic O

Evaluating O
Real-time O
Anomaly B-DAT
Detection O

Algorithms O
– O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

and O
S. O
Ahmad, O
“Evaluating O
Real-time O
Anomaly B-DAT
Detection O
Algorithms O
– O
the O

Numenta B-DAT
Anomaly I-DAT
Benchmark,” I-DAT
in O
14th O
International O
Conference O

Evaluating O
Real-time O
Anomaly B-DAT
Detection O

Algorithms O
– O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

Here O
we O
propose O
the O
Numenta O
Anomaly B-DAT

energy, O
e-commerce, O
and O
social O
media. O
Anomaly B-DAT

is O
to O
introduce O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

Anomaly B-DAT
detection O
in O
real-world O
streaming O
applications O

B. O
Scoring O
Real-Time O
Anomaly B-DAT
Detectors O

Anomaly B-DAT
Detection O

Series O
Anomaly B-DAT
Detection O
[Online O
blog]. O
Available O

Scalable O
Framework O
for O
Automated O
Time-series O
Anomaly B-DAT

Numenta, O
Inc. O
(2015) O
NAB: O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

Anomaly B-DAT
Detection O
[Online O
technical O
report]. O
Redwood O

2014, O
October O
17) O
Science O
of O
Anomaly B-DAT
Detection O

Algorithms O
– O
the O
Numenta O
Anomaly O
Benchmark B-DAT

Numenta O
Anomaly O
Benchmark B-DAT

Algorithms O
– O
the O
Numenta O
Anomaly O
Benchmark B-DAT

Benchmark B-DAT
(NAB), O
which O
attempts O
to O
provide O

to O
introduce O
the O
Numenta O
Anomaly O
Benchmark B-DAT

A. O
Benchmark B-DAT
Dataset O

I. O
Benchmark B-DAT
dataset: O
real O
world O
time-series O
data O

Yahoo O
Labs O
News: O
Announcing O
A O
Benchmark B-DAT
Dataset O
For O
Time O

Inc. O
(2015) O
NAB: O
Numenta O
Anomaly O
Benchmark B-DAT

Algorithms O
– O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

Algorithms O
– O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

paper O
is O
to O
introduce O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

15] O
Numenta, O
Inc. O
(2015) O
NAB: O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

II. O
NUMENTA B-DAT
ANOMALY I-DAT
BENCHMARK I-DAT

Algorithms O
– O
the O
Numenta B-DAT
Anomaly O
Benchmark O

Numenta, B-DAT
Inc. O
Redwood O
City, O
CA O

Numenta, B-DAT
Inc. O
Redwood O
City, O
CA O

Numenta B-DAT
Anomaly O
Benchmark,” O
in O
14th O
International O

Algorithms O
– O
the O
Numenta B-DAT
Anomaly O
Benchmark O

Numenta, B-DAT
Inc. O
Redwood O
City, O
CA O

Numenta, B-DAT
Inc. O
Redwood O
City, O
CA O

detectors. O
Here O
we O
propose O
the O
Numenta B-DAT
Anomaly O

paper O
is O
to O
introduce O
the O
Numenta B-DAT
Anomaly O
Benchmark O

algorithms. O
At O
Numenta B-DAT
we O
have O
developed O
an O
anomaly O

four O
primary O
algorithms O
are O
the O
Numenta B-DAT
HTM O
anomaly O

The O
HTM O
detector O
(developed O
by O
Numenta B-DAT
and O
the O

The O
Numenta B-DAT
algorithm O
has O
several O
features O
that O

etc. O
Like O
the O
Numenta B-DAT
algorithm, O
Skyline O
is O
well O
suited O

1. O
Numenta B-DAT
HTM O
64.7 O
56.5 O
69.3 O

technical O
report]. O
Palo O
Alto, O
CA: O
Numenta, B-DAT
Inc O

6] O
(2015) O
Numenta B-DAT
Applications O
[Online O
website]. O
Redwood O
City O

CA: O
Numenta, B-DAT
Inc. O
Available: O
http://numenta.com/#applications O

15] O
Numenta, B-DAT
Inc. O
(2015) O
NAB: O
Numenta O
Anomaly O
Benchmark O

code O
repository]. O
Redwood O
City, O
CA: O
Numenta, B-DAT
Inc O

City, O
CA: O
Numenta, B-DAT
Inc. O
Available O

CA: O
Numenta, B-DAT
Inc. O
Available: O
http://numenta.com/#technology O

Online O
video]. O
Redwood O
City, O
CA: O
Numenta, B-DAT
Inc. O
Available O

20] O
Numenta, B-DAT
Inc. O
(2015) O
NuPIC: O
Numenta O
Platform O
for O
Intelligent O

Numenta, B-DAT
Inc. O
Available: O
https://github.com/numenta/nupic O

Evaluating O
Real-time O
Anomaly B-DAT
Detection O

Algorithms O
– O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

and O
S. O
Ahmad, O
“Evaluating O
Real-time O
Anomaly B-DAT
Detection O
Algorithms O
– O
the O

Numenta B-DAT
Anomaly I-DAT
Benchmark,” I-DAT
in O
14th O
International O
Conference O

Evaluating O
Real-time O
Anomaly B-DAT
Detection O

Algorithms O
– O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

Here O
we O
propose O
the O
Numenta O
Anomaly B-DAT

energy, O
e-commerce, O
and O
social O
media. O
Anomaly B-DAT

is O
to O
introduce O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

Anomaly B-DAT
detection O
in O
real-world O
streaming O
applications O

B. O
Scoring O
Real-Time O
Anomaly B-DAT
Detectors O

Anomaly B-DAT
Detection O

Series O
Anomaly B-DAT
Detection O
[Online O
blog]. O
Available O

Scalable O
Framework O
for O
Automated O
Time-series O
Anomaly B-DAT

Numenta, O
Inc. O
(2015) O
NAB: O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

Anomaly B-DAT
Detection O
[Online O
technical O
report]. O
Redwood O

2014, O
October O
17) O
Science O
of O
Anomaly B-DAT
Detection O

Algorithms O
– O
the O
Numenta O
Anomaly O
Benchmark B-DAT

Numenta O
Anomaly O
Benchmark B-DAT

Algorithms O
– O
the O
Numenta O
Anomaly O
Benchmark B-DAT

Benchmark B-DAT
(NAB), O
which O
attempts O
to O
provide O

to O
introduce O
the O
Numenta O
Anomaly O
Benchmark B-DAT

A. O
Benchmark B-DAT
Dataset O

I. O
Benchmark B-DAT
dataset: O
real O
world O
time-series O
data O

Yahoo O
Labs O
News: O
Announcing O
A O
Benchmark B-DAT
Dataset O
For O
Time O

Inc. O
(2015) O
NAB: O
Numenta O
Anomaly O
Benchmark B-DAT

Algorithms O
– O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

Algorithms O
– O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

paper O
is O
to O
introduce O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

15] O
Numenta, O
Inc. O
(2015) O
NAB: O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

II. O
NUMENTA B-DAT
ANOMALY I-DAT
BENCHMARK I-DAT

Subutai O
Ahmad O
SAHMAD@NUMENTA.COM O
Numenta, B-DAT
Inc., O
791 O
Middlefield O
Road, O
Redwood O

Scott O
Purdy O
SPURDY@NUMENTA.COM O
Numenta, B-DAT
Inc., O
791 O
Middlefield O
Road, O
Redwood O

time O
Anomaly O
Detection O
Algorithms O
the O
Numenta B-DAT
Anomaly O
Benchmark. O
In O
14th O
International O

Real-Time O
Anomaly B-DAT
Detection O
for O
Streaming O
Analytics O

Real-Time O
Anomaly B-DAT
Detection O
for O
Streaming O
Analytics O

unique O
constraints O
for O
machine O
learning. O
Anomaly B-DAT
detection O
in O
streaming O
ap- O
plications O

Real-Time O
Anomaly B-DAT
Detection O

2. O
Related O
Work O
Anomaly B-DAT
detection O
in O
time-series O
is O
a O

Real-Time O
Anomaly B-DAT
Detection O

3. O
Anomaly B-DAT
Detection O
Using O
HTM O
Typical O
streaming O

3.1. O
Computing O
the O
Raw O
Anomaly B-DAT
Score O

Real-Time O
Anomaly B-DAT
Detection O

3.2. O
Computing O
Anomaly B-DAT
Likelihood O

Real-Time O
Anomaly B-DAT
Detection O

Real-Time O
Anomaly B-DAT
Detection O

Real-Time O
Anomaly B-DAT
Detection O

Real-Time O
Anomaly B-DAT
Detection O

Real-Time O
Anomaly B-DAT
Detection O

V. O
Comparative O
Eval- O
uation O
of O
Anomaly B-DAT
Detection O
Techniques O
for O
Sequence O
Data O

Banerjee, O
A, O
and O
Kumar, O
V. O
Anomaly B-DAT
detection: O
A O
survey. O
ACM O
Computing O

Real-Time O
Anomaly B-DAT
Detection O

Kleine, O
and O
Priesterjahn, O
Steffen. O
Model-Based O
Anomaly B-DAT
Detection O
for O
Discrete O
Event O
Systems O

Scalable O
Framework O
for O
Automated O
Time-series O
Anomaly B-DAT
Detection. O
In O
Proceedings O
of O
the O

Ahmad, O
Subutai. O
Evaluating O
Real- O
time O
Anomaly B-DAT
Detection O
Algorithms O
the O
Numenta O
Anomaly O

Rinehart, O
Aidan O
W. O
A O
Model-Based O
Anomaly B-DAT
Detection O
Approach O
for O
Analyzing O
Streaming O

Sokolov, O
Grigory. O
Efficient O
Computer O
Network O
Anomaly B-DAT
Detection O
by O
Changepoint O
Detection O
Meth O

4.1. O
Results O
on O
Real-World O
Benchmark B-DAT
Data O

Detection O
Algorithms O
the O
Numenta O
Anomaly O
Benchmark B-DAT

time O
Anomaly O
Detection O
Algorithms O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

Algorithms O
– O
the O
Numenta B-DAT
Anomaly O
Benchmark O

Numenta, B-DAT
Inc. O
Redwood O
City, O
CA O

Numenta, B-DAT
Inc. O
Redwood O
City, O
CA O

Numenta B-DAT
Anomaly O
Benchmark,” O
in O
14th O
International O

Algorithms O
– O
the O
Numenta B-DAT
Anomaly O
Benchmark O

Numenta, B-DAT
Inc. O
Redwood O
City, O
CA O

Numenta, B-DAT
Inc. O
Redwood O
City, O
CA O

detectors. O
Here O
we O
propose O
the O
Numenta B-DAT
Anomaly O

paper O
is O
to O
introduce O
the O
Numenta B-DAT
Anomaly O
Benchmark O

algorithms. O
At O
Numenta B-DAT
we O
have O
developed O
an O
anomaly O

four O
primary O
algorithms O
are O
the O
Numenta B-DAT
HTM O
anomaly O

The O
HTM O
detector O
(developed O
by O
Numenta B-DAT
and O
the O

The O
Numenta B-DAT
algorithm O
has O
several O
features O
that O

etc. O
Like O
the O
Numenta B-DAT
algorithm, O
Skyline O
is O
well O
suited O

1. O
Numenta B-DAT
HTM O
64.7 O
56.5 O
69.3 O

technical O
report]. O
Palo O
Alto, O
CA: O
Numenta, B-DAT
Inc O

6] O
(2015) O
Numenta B-DAT
Applications O
[Online O
website]. O
Redwood O
City O

CA: O
Numenta, B-DAT
Inc. O
Available: O
http://numenta.com/#applications O

15] O
Numenta, B-DAT
Inc. O
(2015) O
NAB: O
Numenta O
Anomaly O
Benchmark O

code O
repository]. O
Redwood O
City, O
CA: O
Numenta, B-DAT
Inc O

City, O
CA: O
Numenta, B-DAT
Inc. O
Available O

CA: O
Numenta, B-DAT
Inc. O
Available: O
http://numenta.com/#technology O

Online O
video]. O
Redwood O
City, O
CA: O
Numenta, B-DAT
Inc. O
Available O

20] O
Numenta, B-DAT
Inc. O
(2015) O
NuPIC: O
Numenta O
Platform O
for O
Intelligent O

Numenta, B-DAT
Inc. O
Available: O
https://github.com/numenta/nupic O

Evaluating O
Real-time O
Anomaly B-DAT
Detection O

Algorithms O
– O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

and O
S. O
Ahmad, O
“Evaluating O
Real-time O
Anomaly B-DAT
Detection O
Algorithms O
– O
the O

Numenta B-DAT
Anomaly I-DAT
Benchmark,” I-DAT
in O
14th O
International O
Conference O

Evaluating O
Real-time O
Anomaly B-DAT
Detection O

Algorithms O
– O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

Here O
we O
propose O
the O
Numenta O
Anomaly B-DAT

energy, O
e-commerce, O
and O
social O
media. O
Anomaly B-DAT

is O
to O
introduce O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

Anomaly B-DAT
detection O
in O
real-world O
streaming O
applications O

B. O
Scoring O
Real-Time O
Anomaly B-DAT
Detectors O

Anomaly B-DAT
Detection O

Series O
Anomaly B-DAT
Detection O
[Online O
blog]. O
Available O

Scalable O
Framework O
for O
Automated O
Time-series O
Anomaly B-DAT

Numenta, O
Inc. O
(2015) O
NAB: O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

Anomaly B-DAT
Detection O
[Online O
technical O
report]. O
Redwood O

2014, O
October O
17) O
Science O
of O
Anomaly B-DAT
Detection O

Algorithms O
– O
the O
Numenta O
Anomaly O
Benchmark B-DAT

Numenta O
Anomaly O
Benchmark B-DAT

Algorithms O
– O
the O
Numenta O
Anomaly O
Benchmark B-DAT

Benchmark B-DAT
(NAB), O
which O
attempts O
to O
provide O

to O
introduce O
the O
Numenta O
Anomaly O
Benchmark B-DAT

A. O
Benchmark B-DAT
Dataset O

I. O
Benchmark B-DAT
dataset: O
real O
world O
time-series O
data O

Yahoo O
Labs O
News: O
Announcing O
A O
Benchmark B-DAT
Dataset O
For O
Time O

Inc. O
(2015) O
NAB: O
Numenta O
Anomaly O
Benchmark B-DAT

Algorithms O
– O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

Algorithms O
– O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

paper O
is O
to O
introduce O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

15] O
Numenta, O
Inc. O
(2015) O
NAB: O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

II. O
NUMENTA B-DAT
ANOMALY I-DAT
BENCHMARK I-DAT

Subutai O
Ahmad O
SAHMAD@NUMENTA.COM O
Numenta, B-DAT
Inc., O
791 O
Middlefield O
Road, O
Redwood O

Scott O
Purdy O
SPURDY@NUMENTA.COM O
Numenta, B-DAT
Inc., O
791 O
Middlefield O
Road, O
Redwood O

time O
Anomaly O
Detection O
Algorithms O
the O
Numenta B-DAT
Anomaly O
Benchmark. O
In O
14th O
International O

Real-Time O
Anomaly B-DAT
Detection O
for O
Streaming O
Analytics O

Real-Time O
Anomaly B-DAT
Detection O
for O
Streaming O
Analytics O

unique O
constraints O
for O
machine O
learning. O
Anomaly B-DAT
detection O
in O
streaming O
ap- O
plications O

Real-Time O
Anomaly B-DAT
Detection O

2. O
Related O
Work O
Anomaly B-DAT
detection O
in O
time-series O
is O
a O

Real-Time O
Anomaly B-DAT
Detection O

3. O
Anomaly B-DAT
Detection O
Using O
HTM O
Typical O
streaming O

3.1. O
Computing O
the O
Raw O
Anomaly B-DAT
Score O

Real-Time O
Anomaly B-DAT
Detection O

3.2. O
Computing O
Anomaly B-DAT
Likelihood O

Real-Time O
Anomaly B-DAT
Detection O

Real-Time O
Anomaly B-DAT
Detection O

Real-Time O
Anomaly B-DAT
Detection O

Real-Time O
Anomaly B-DAT
Detection O

Real-Time O
Anomaly B-DAT
Detection O

V. O
Comparative O
Eval- O
uation O
of O
Anomaly B-DAT
Detection O
Techniques O
for O
Sequence O
Data O

Banerjee, O
A, O
and O
Kumar, O
V. O
Anomaly B-DAT
detection: O
A O
survey. O
ACM O
Computing O

Real-Time O
Anomaly B-DAT
Detection O

Kleine, O
and O
Priesterjahn, O
Steffen. O
Model-Based O
Anomaly B-DAT
Detection O
for O
Discrete O
Event O
Systems O

Scalable O
Framework O
for O
Automated O
Time-series O
Anomaly B-DAT
Detection. O
In O
Proceedings O
of O
the O

Ahmad, O
Subutai. O
Evaluating O
Real- O
time O
Anomaly B-DAT
Detection O
Algorithms O
the O
Numenta O
Anomaly O

Rinehart, O
Aidan O
W. O
A O
Model-Based O
Anomaly B-DAT
Detection O
Approach O
for O
Analyzing O
Streaming O

Sokolov, O
Grigory. O
Efficient O
Computer O
Network O
Anomaly B-DAT
Detection O
by O
Changepoint O
Detection O
Meth O

4.1. O
Results O
on O
Real-World O
Benchmark B-DAT
Data O

Detection O
Algorithms O
the O
Numenta O
Anomaly O
Benchmark B-DAT

time O
Anomaly O
Detection O
Algorithms O
the O
Numenta B-DAT
Anomaly I-DAT
Benchmark I-DAT

achieves O
state-of-the-art O
results O
on O
the O
THUMOS’14 B-DAT
and O
Charades O
datasets. O
We O
further O

publicly O
available O
benchmark O
datasets O
- O
THUMOS’14 B-DAT
[29], O
ActivityNet O
[30] O
and O
Charades O

state-of-the-art O
results O
are O
achieved O
on O
THUMOS’14 B-DAT
and O
Charades, O
while O
the O
detection O

tion O
datasets O
- O
THUMOS’14 B-DAT
[29], O
Charades O
[31] O
and O
Ac O

A. O
Experiments O
on O
THUMOS’14 B-DAT

THUMOS’14 B-DAT
activity O
detection O
dataset O
contains O
over O

C3D O
to O
be O
trained O
on O
THUMOS’14 B-DAT
with O
a O
fixed O
learning O
rate O

TABLE O
I. O
PROPOSAL O
EVALUATION O
ON O
THUMOS’14 B-DAT
DATASET O
(IN O
PERCENTAGE). O
AVERAGE O
AUC O

II. O
ACTIVITY O
DETECTION O
RESULTS O
ON O
THUMOS’14 B-DAT
(IN O
PERCENTAGE). O
MAP O
AT O
DIFFERENT O

REPORTED. O
TOP O
THREE O
PERFORMERS O
ON O
THUMOS’14 B-DAT
CHALLENGE O
LEADERBOARD O

AP) O
for O
each O
class O
on O
THUMOS’14 B-DAT
at O
tIoU O
threshold O
0.5 O
is O

THRESHOLD O
α O
= O
0.5 O
ON O
THUMOS’14 B-DAT
(IN O
PERCENTAGE O

of O
the O
video. O
Compared O
to O
THUMOS’14, B-DAT
this O
is O
a O
large-scale O
dataset O

server. O
Experimental O
Setup: O
Similar O
to O
THUMOS’14, B-DAT
the O
length O
of O
the O
input O

as O
is O
done O
for O
the O
THUMOS’14 B-DAT
dataset O
(ref. O
Sec. O
IV-A). O
We O

the O
improved O
results O
on O
the O
THUMOS’14, B-DAT
we O
choose O
the O
two-way O
buffer O

a) O
THUMOS’14 B-DAT

for O
two O
videos O
each O
on O
THUMOS’14 B-DAT
and O
ActivityNet. O
(c) O
shows O
the O

achieves O
state-of-the-art O
results O
on O
the O
THUMOS’14 B-DAT
and O
Charades O
datasets. O
We O
further O

publicly O
available O
benchmark O
datasets O
- O
THUMOS’14 B-DAT
[29], O
ActivityNet O
[30] O
and O
Charades O

state-of-the-art O
results O
are O
achieved O
on O
THUMOS’14 B-DAT
and O
Charades, O
while O
the O
detection O

tion O
datasets O
- O
THUMOS’14 B-DAT
[29], O
Charades O
[31] O
and O
Ac O

A. O
Experiments O
on O
THUMOS’14 B-DAT

THUMOS’14 B-DAT
activity O
detection O
dataset O
contains O
over O

C3D O
to O
be O
trained O
on O
THUMOS’14 B-DAT
with O
a O
fixed O
learning O
rate O

TABLE O
I. O
PROPOSAL O
EVALUATION O
ON O
THUMOS’14 B-DAT
DATASET O
(IN O
PERCENTAGE). O
AVERAGE O
AUC O

II. O
ACTIVITY O
DETECTION O
RESULTS O
ON O
THUMOS’14 B-DAT
(IN O
PERCENTAGE). O
MAP O
AT O
DIFFERENT O

REPORTED. O
TOP O
THREE O
PERFORMERS O
ON O
THUMOS’14 B-DAT
CHALLENGE O
LEADERBOARD O

AP) O
for O
each O
class O
on O
THUMOS’14 B-DAT
at O
tIoU O
threshold O
0.5 O
is O

THRESHOLD O
α O
= O
0.5 O
ON O
THUMOS’14 B-DAT
(IN O
PERCENTAGE O

of O
the O
video. O
Compared O
to O
THUMOS’14, B-DAT
this O
is O
a O
large-scale O
dataset O

server. O
Experimental O
Setup: O
Similar O
to O
THUMOS’14, B-DAT
the O
length O
of O
the O
input O

as O
is O
done O
for O
the O
THUMOS’14 B-DAT
dataset O
(ref. O
Sec. O
IV-A). O
We O

the O
improved O
results O
on O
the O
THUMOS’14, B-DAT
we O
choose O
the O
two-way O
buffer O

a) O
THUMOS’14 B-DAT

for O
two O
videos O
each O
on O
THUMOS’14 B-DAT
and O
ActivityNet. O
(c) O
shows O
the O

achieves O
state-of-the-art O
results O
on O
the O
THUMOS’14 B-DAT
and O
Charades O
datasets. O
We O
further O

publicly O
available O
benchmark O
datasets O
- O
THUMOS’14 B-DAT
[29], O
ActivityNet O
[30] O
and O
Charades O

state-of-the-art O
results O
are O
achieved O
on O
THUMOS’14 B-DAT
and O
Charades, O
while O
the O
detection O

tion O
datasets O
- O
THUMOS’14 B-DAT
[29], O
Charades O
[31] O
and O
Ac O

A. O
Experiments O
on O
THUMOS’14 B-DAT

THUMOS’14 B-DAT
activity O
detection O
dataset O
contains O
over O

C3D O
to O
be O
trained O
on O
THUMOS’14 B-DAT
with O
a O
fixed O
learning O
rate O

TABLE O
I. O
PROPOSAL O
EVALUATION O
ON O
THUMOS’14 B-DAT
DATASET O
(IN O
PERCENTAGE). O
AVERAGE O
AUC O

II. O
ACTIVITY O
DETECTION O
RESULTS O
ON O
THUMOS’14 B-DAT
(IN O
PERCENTAGE). O
MAP O
AT O
DIFFERENT O

REPORTED. O
TOP O
THREE O
PERFORMERS O
ON O
THUMOS’14 B-DAT
CHALLENGE O
LEADERBOARD O

AP) O
for O
each O
class O
on O
THUMOS’14 B-DAT
at O
tIoU O
threshold O
0.5 O
is O

THRESHOLD O
α O
= O
0.5 O
ON O
THUMOS’14 B-DAT
(IN O
PERCENTAGE O

of O
the O
video. O
Compared O
to O
THUMOS’14, B-DAT
this O
is O
a O
large-scale O
dataset O

server. O
Experimental O
Setup: O
Similar O
to O
THUMOS’14, B-DAT
the O
length O
of O
the O
input O

as O
is O
done O
for O
the O
THUMOS’14 B-DAT
dataset O
(ref. O
Sec. O
IV-A). O
We O

the O
improved O
results O
on O
the O
THUMOS’14, B-DAT
we O
choose O
the O
two-way O
buffer O

a) O
THUMOS’14 B-DAT

for O
two O
videos O
each O
on O
THUMOS’14 B-DAT
and O
ActivityNet. O
(c) O
shows O
the O

achieves O
state-of-the-art O
results O
on O
the O
THUMOS’14 B-DAT
and O
Charades O
datasets. O
We O
further O

publicly O
available O
benchmark O
datasets O
- O
THUMOS’14 B-DAT
[29], O
ActivityNet O
[30] O
and O
Charades O

state-of-the-art O
results O
are O
achieved O
on O
THUMOS’14 B-DAT
and O
Charades, O
while O
the O
detection O

tion O
datasets O
- O
THUMOS’14 B-DAT
[29], O
Charades O
[31] O
and O
Ac O

A. O
Experiments O
on O
THUMOS’14 B-DAT

THUMOS’14 B-DAT
activity O
detection O
dataset O
contains O
over O

C3D O
to O
be O
trained O
on O
THUMOS’14 B-DAT
with O
a O
fixed O
learning O
rate O

TABLE O
I. O
PROPOSAL O
EVALUATION O
ON O
THUMOS’14 B-DAT
DATASET O
(IN O
PERCENTAGE). O
AVERAGE O
AUC O

II. O
ACTIVITY O
DETECTION O
RESULTS O
ON O
THUMOS’14 B-DAT
(IN O
PERCENTAGE). O
MAP O
AT O
DIFFERENT O

REPORTED. O
TOP O
THREE O
PERFORMERS O
ON O
THUMOS’14 B-DAT
CHALLENGE O
LEADERBOARD O

AP) O
for O
each O
class O
on O
THUMOS’14 B-DAT
at O
tIoU O
threshold O
0.5 O
is O

THRESHOLD O
α O
= O
0.5 O
ON O
THUMOS’14 B-DAT
(IN O
PERCENTAGE O

of O
the O
video. O
Compared O
to O
THUMOS’14, B-DAT
this O
is O
a O
large-scale O
dataset O

server. O
Experimental O
Setup: O
Similar O
to O
THUMOS’14, B-DAT
the O
length O
of O
the O
input O

as O
is O
done O
for O
the O
THUMOS’14 B-DAT
dataset O
(ref. O
Sec. O
IV-A). O
We O

the O
improved O
results O
on O
the O
THUMOS’14, B-DAT
we O
choose O
the O
two-way O
buffer O

a) O
THUMOS’14 B-DAT

for O
two O
videos O
each O
on O
THUMOS’14 B-DAT
and O
ActivityNet. O
(c) O
shows O
the O

Facial B-DAT
expression O
recognition O
based O
on O
deep O

FRAME B-DAT
ATTENTION O
NETWORKS O
FOR O
FACIAL O

EXPRESSION B-DAT
RECOGNITION O
IN O
VIDEOS O
Debin O
Meng, O
Xiaojiang O
Peng∗, O
Kai O

Wang, B-DAT
Yu O
Qiao O

Shenzhen B-DAT
Institutes O
of O
Advanced O
Technology, O

Chinese B-DAT
Academy O
of O
Science, O
Shenzhen, O

China B-DAT
Shenzhen O
Key O
Lab O
of O

Computer B-DAT
Vision O
and O
Pattern O
Recognition, O

Shenzhen, B-DAT
China O
University O
of O
Chinese O
Academy O
of O

Sciences, B-DAT
Beijing, O
China O
michaeldbmeng19@outlook.com, O
{xj.peng O

, B-DAT
kai.wang, O
yu.qiao}@siat.ac.cn O
ABSTRACT O
The O
video-based O
facial O
expression O

recognition B-DAT
aims O
to O
classify O
a O

given B-DAT
video O
into O
several O
basic O

emotions. B-DAT
How O
to O
integrate O
facial O

features B-DAT
of O
individual O
frames O
is O

crucial B-DAT
for O
this O
task. O
In O

this B-DAT
paper, O
we O
propose O
the O

Frame B-DAT
Attention O
Networks O
(FAN)1, O
to O

automatically B-DAT
highlight O
some O
discriminative O
frames O

in B-DAT
an O
end-to-end O
framework. O
The O

network B-DAT
takes O
a O
video O
with O

a B-DAT
variable O
number O
of O
face O

images B-DAT
as O
its O
input O
and O

produces B-DAT
a O
fixed-dimension O
representation. O
The O

whole B-DAT
network O
is O
com- O
posed O

of B-DAT
two O
modules. O
The O
feature O

embedding B-DAT
module O
is O
a O
deep O

Convolutional B-DAT
Neural O
Network O
(CNN) O
which O

embeds B-DAT
face O
images O
into O
feature O

vectors. B-DAT
The O
frame O
attention O
module O

learns B-DAT
multiple O
attention O
weights O
which O

are B-DAT
used O
to O
adaptively O
aggregate O

the B-DAT
feature O
vectors O
to O
form O

a B-DAT
single O
discriminative O
video O
representation O

. B-DAT
We O
conduct O
extensive O
experiments O

on B-DAT
CK+ O
and O
AFEW8.0 O
datasets. O

Our B-DAT
proposed O
FAN O
shows O
su- O

perior B-DAT
performance O
compared O
to O
other O

CNN B-DAT
based O
methods O
and O
achieves O

state-of-the-art B-DAT
performance O
on O
CK+. O
Index O
Terms— O
facial O
expression O
recognition O

, B-DAT
audio- O
video O
emotion O
recognition, O

frame B-DAT
attention O
networks, O
CNN, O
AFEW O
1. O
INTRODUCTION O

Automatic B-DAT
facial O
expression O
recognition O
(FER) O

has B-DAT
recently O
attracted O
increasing O
attention O

in B-DAT
academia O
and O
industry O
due O

to B-DAT
its O
wide O
range O
of O

applications B-DAT
such O
as O
affective O
computing, O

intelligent B-DAT
environments, O
and O
multimodal O
human-computer O

interface B-DAT
(HCI). O
Though O
great O
progress O

have B-DAT
been O
made O
re- O
cently, O

facial B-DAT
expression O
recognition O
in O
the O

wild B-DAT
remains O
a O
challenging O
problem O

due B-DAT
to O
large O
head O
pose, O

illumination B-DAT
variance, O
occlusion, O
motion O
blur, O

etc. B-DAT
Video-based O
facial O
expression O
recognition O
aims O

to B-DAT
classify O
a O
video O
into O

several B-DAT
basic O
emotions, O
such O
as O

happy, B-DAT
angry, O
dis O

- B-DAT
∗Xiaojiang O
Peng O
is O
the O
corresponding O

author. B-DAT
Email: O
xj.peng@siat.ac.cn O
This O
work O

was B-DAT
supported O
by O
the O
National O

Natural B-DAT
Science O
Foun O

- B-DAT
dation O
of O
China O
(U1613211, O
U1713208 O

), B-DAT
Shenzhen O
Research O
Pro- O
gram O
( O

JCYJ20170818164704758, B-DAT
JSGG20180507182100698), O
and O
In- O
ternational O

Partnership B-DAT
Program O
of O
Chinese O
Academy O

of B-DAT
Sciences O
(172644KYSB20150019). O
1Code O
is O
available O
at O
https://github.com/Open-Debin/Emotion-FAN O

gust, B-DAT
fear, O
sad, O
neutral, O
and O

surprise. B-DAT
Given O
a O
video, O
the O

pop- B-DAT
ular O
FER O
pipeline O
with O

a B-DAT
visual O
clue O
(FER O
with O

an B-DAT
audio O
clue O
is O
out O

of B-DAT
the O
scope O
of O
this O

paper) B-DAT
mainly O
includes O
three O
steps, O

namely B-DAT
frame O
preprocessing, O
feature O
extraction, O

and B-DAT
classi- O
fication. O
Especially, O
frame O

preprocessing B-DAT
refers O
to O
face O
de- O

tection, B-DAT
alignment, O
illumination O
normalizing O
and O

so B-DAT
on. O
Fea- O
ture O
extraction O

or B-DAT
video O
representation O
is O
the O

key B-DAT
part O
for O
FER O
which O

encodes B-DAT
frames O
or O
sequences O
into O

compact B-DAT
feature O
vec- O
tors. O
These O

feature B-DAT
vectors O
are O
subsequently O
fed O

into B-DAT
a O
classi- O
fier O
for O

prediction. B-DAT
Feature O
extraction O
methods O
for O
video-based O

FER B-DAT
can O
be O
roughly O
divided O

into B-DAT
three O
types: O
static-based O
methods O

, B-DAT
spatial-temporal O
methods, O
and O
geometry-based O

methods. B-DAT
Static-based O
feature O
extraction O
methods O
mainly O

inherit B-DAT
those O
methods O
from O
static O

image B-DAT
emotion O
recognition O
which O
can O

be B-DAT
both O
hand-crafted O
[1, O
2 O

] B-DAT
and O
learned O
[3, O
4, O
5 O

]. B-DAT
For O
the O
hand-crafted O
features, O

Littlewort B-DAT
et O
al. O
[1] O
propose O

to B-DAT
use O
a O
bank O
of O

2D B-DAT
Gabor O
filters O
to O
extract O

facial B-DAT
features O
for O
video- O
based O

FER. B-DAT
Shan O
et O
al. O
[2] O

use B-DAT
local O
binary O
patterns O
(LBP) O

and B-DAT
LBP O
histogram O
for O
facial O

feature B-DAT
extraction. O
For O
the O
learned O

features, B-DAT
Tang O
[3] O
utilizes O
deep O

CNNs B-DAT
for O
feature O
extraction, O
and O

win B-DAT
the O
FER2013. O
Some O
winners O

in B-DAT
audio-video O
emotion O
recognition O
task O

of B-DAT
EmotiW2016 O
and O
EmotiW2017 O
only O

use B-DAT
static O
facial O
features O
from O

deep B-DAT
CNNs O
trained O
on O
large O

face B-DAT
datasets O
or O
trained O
with O

multi-level B-DAT
supervision O
[4, O
5]. O
Spatial-temporal O
methods O
aim O
to O
model O

the B-DAT
temporal O
or O
motion O
information O

in B-DAT
videos. O
The O
Long O
Short-Term O

Mem- B-DAT
ory O
(LSTM) O
[6], O
and O

C3D B-DAT
[7] O
are O
two O
widely-used O

spatial- B-DAT
temporal O
methods O
for O
video-based O

FER. B-DAT
LSTM O
derives O
in- O
formation O

from B-DAT
sequences O
by O
exploiting O
the O

fact B-DAT
that O
feature O
vectors O
are O

connected B-DAT
semantically O
for O
successive O
data O

. B-DAT
This O
pipeline O
is O
widely-used O

in B-DAT
the O
EmotiW O
challenge, O

e.g. B-DAT
[8, O
9, O
10, O
11]. O

C3D, B-DAT
which O
is O
originally O
developed O

for B-DAT
video O
action O
recognition, O
is O

also B-DAT
popular O
in O
the O
EmotiW O

challenge. B-DAT
Geometry O
based O
methods O
[12, O
11 O

] B-DAT
aim O
to O
model O
the O

mo- B-DAT
tions O
of O
key O
points O

in B-DAT
faces O
which O
only O
leverage O

the B-DAT
geometry O
locations O
of O
facial O

landmarks B-DAT
in O
every O
video O
frames. O

Jung B-DAT
et O
al. O
[12] O
propose O

a B-DAT
deep O
temporal O
appearance-geometry O
net- O

work B-DAT
(DTAGN) O
which O
first O
alternately O

concatenates B-DAT
the O
x- O
coordinates O
and O

y-coordinates B-DAT
of O
the O
facial O
landmark O

points B-DAT
Copyright O
2019 O
IEEE. O
Published O
in O

the B-DAT
IEEE O
2019 O
International O
Conference O

on B-DAT
Image O
Processing O
(ICIP O
2019 O

), B-DAT
scheduled O
for O
22-25 O
September O
2019 O
in O
Taipei, O
Taiwan. O
Personal O
use O

of B-DAT
this O
material O
is O
permitted O

. B-DAT
However, O
permission O
to O
reprint/republish O

this B-DAT
material O
for O
advertising O
or O

promotional B-DAT
purposes O
or O
for O
creating O

new B-DAT
collective O
works O
for O
resale O

or B-DAT
redistribution O
to O
servers O
or O

lists, B-DAT
or O
to O
reuse O
any O

copyrighted B-DAT
component O
of O
this O
work O

in B-DAT
other O
works, O
must O
be O

obtained B-DAT
from O
the O
IEEE. O
Contact: O

Manager, B-DAT
Copyrights O
and O
Permissions O
/ O

IEEE B-DAT
Service O
Center O
/ O
445 O

Hoes B-DAT
Lane O
/ O
P.O. O
Box O
1331 O
/ O
Piscataway, O
NJ O
08855-1331, O
USA O

. B-DAT
Telephone: O
+ O
Intl. O
908-562-3966. O
ar O
X O

iv B-DAT
:1 O
90 O
7 O

. B-DAT
00 O
19 O

3v B-DAT
2 O

cs B-DAT
.C O
V O

1 B-DAT
2 O

Se B-DAT
p O

20 B-DAT
19 O

from B-DAT
each O
frame O
after O
normalization O

and B-DAT
then O
concatenates O
these O
normalized O

points B-DAT
over O
time O
for O
a O

one-dimensional B-DAT
tra- O
jectory O
signal O
of O

each B-DAT
sequence. O
Yan O
et O

al. B-DAT
[11] O
construct O
an O
image-like O

map B-DAT
by O
stretching O
all O
the O

normalized B-DAT
facial O
point O
trajectories O
in O

a B-DAT
sequence O
together O
as O
the O

input B-DAT
of O
a O
CNN. O
Among O
all O
the O
above O
methods O

, B-DAT
static-based O
methods O
are O
superior O

to B-DAT
the O
others O
according O
to O

several B-DAT
winner O
solutions O
in O
EmotiW O

challenges. B-DAT
To O
obtain O
a O
video-level O

result B-DAT
with O
varied O
frames, O
a O

frame B-DAT
aggregation O
operation O
is O
necessary O

for B-DAT
static- O
based O
methods. O
For O

frame B-DAT
aggregation, O
Kahou O
et O

al. B-DAT
[13] O
concatenate O
the O
n-class O

probability B-DAT
vectors O
of O
10 O
segments O

to B-DAT
form O
a O
fixed-length O
video O

representation B-DAT
by O
frame O
averag- O
ing O

or B-DAT
frame O
expansion. O
Bargal O
et O

al. B-DAT
[4] O
propose O
a O
statistical O

encoding B-DAT
module O
(STAT) O
to O
aggregate O

frame B-DAT
features O
which O
compute O
the O

mean, B-DAT
variance, O
minimum, O
and O
maximum O

of B-DAT
the O
frame O
feature O
vectors. O
One O
limitation O
of O
these O
existing O

aggregation B-DAT
methods O
is O
that O
they O

ignore B-DAT
the O
importance O
of O
frames O

for B-DAT
FER. O
For O
example, O
some O

faces B-DAT
in O
Figure O
1 O
are O

representative B-DAT
for O
the O
‘happy’ O
category O

while B-DAT
the O
others O
not. O
In O

this B-DAT
paper, O
inspired O
by O
the O

attention B-DAT
mechanism O
[14] O
of O
machine O

translation B-DAT
and O
the O
neural O
aggregation O

networks B-DAT
[15] O
of O
video O
face O

recog- B-DAT
nition, O
we O
propose O
the O

Frame B-DAT
Attention O
Networks O
(FAN) O
to O

adaptively B-DAT
aggregate O
frame O
features. O
The O

FAN B-DAT
is O
designed O
to O
learn O

self-attention B-DAT
kernels O
and O
relation-attention O
kernels O

for B-DAT
frame O
importance O
reasoning O
in O

an B-DAT
end-to-end O
fashion. O
The O
self-attention O

kernels B-DAT
are O
directly O
learned O
from O

frame B-DAT
features O
while O
the O
relation-attention O

kernels B-DAT
are O
learned O
from O
the O

concatenated B-DAT
features O
of O
a O
video-level O

anchor B-DAT
feature O
and O
frame O
features O

. B-DAT
We O
conduct O
extensive O
experiments O

on B-DAT
CK+ O
and O
AFEW8.0 O
(EmotiW2018) O

datasets. B-DAT
Our O
proposed O
FAN O
shows O

superior B-DAT
performance O
compared O
to O
other O

CNN B-DAT
based O
methods O
with O
only O

facial B-DAT
features O
and O
achieves O
state-of-the- O

art B-DAT
performance O
on O
CK+. O
2. O
FRAME O
ATTENTION O
NETWORKS O

We B-DAT
propose O
Frame O
Attention O
Networks O
( O

FAN) B-DAT
for O
video- O
based O
facial O

expression B-DAT
recognition O
(FER). O
Figure O
1 O

illus- B-DAT
trates O
the O
framework O
of O

our B-DAT
proposed O
FAN. O
It O
takes O

a B-DAT
facial O
video O
with O
a O

variable B-DAT
number O
of O
face O
images O

as B-DAT
its O
input O
and O
produces O

a B-DAT
fixed-dimension O
feature O
representation O
for O

FER. B-DAT
The O
whole O
network O
consists O

of B-DAT
two O
modules: O
feature O
em- O

bedding B-DAT
module O
and O
frame O
attention O

module. B-DAT
The O
feature O
embedding O
module O

is B-DAT
a O
deep O
CNN O
which O

embeds B-DAT
each O
face O
image O
into O

a B-DAT
feature O
vector. O
The O
frame O

attention B-DAT
module O
learns O
two-level O
attention O

weights, B-DAT
i.e. O
self-attention O
weights O
and O

relation-attention B-DAT
weights, O
which O
are O
used O

to B-DAT
adaptively O
aggregate O
the O
feature O

vectors B-DAT
to O
form O
a O
single O

discriminative B-DAT
video O
representation. O
Formally, O
we O
denote O
a O
video O

with B-DAT
n O
frames O
as O
V O

, B-DAT
and O
its O
frames O
as O

I1, B-DAT
I2, O
· O
· O
· O
, O
In, O
and O
the O
facial O
frame O

features B-DAT
are O
{f1 O

, B-DAT
· O
· O
· O
, O

fn}. B-DAT
Self-attention O
weights. O
With O
individual O
frame O

features B-DAT

, B-DAT
Fig. O
1. O
Our O
proposed O
frame O

attention B-DAT
network O
architecture O

. B-DAT
our O
FAN O
first O
applies O
a O

FC B-DAT
layer O
and O
a O
sigmoid O

function B-DAT
to O
assign O
coarse O
self-attention O

weights. B-DAT
Mathematically, O
the O
self- O
attention O

weight B-DAT
of O
the O
i-th O
frame O

is B-DAT
defined O
by O

: B-DAT
αi O
= O
σ(f O
T O
i O

q B-DAT

0) B-DAT
(1) O
where O
q0 O
is O
the O
parameter O

of B-DAT
FC, O
σ O
denotes O
the O

sigmoid B-DAT
func- O
tion. O
With O
these O

self-attention B-DAT
weights, O
we O
aggregate O
all O

the B-DAT
input O
frame O
features O
into O

a B-DAT
global O
representation O
f O
′v O

as B-DAT
follows O

, B-DAT
f O
′v O

n B-DAT
i=1 O
αifi∑n O
i=1 O
αi O
. O
(2 O

) B-DAT
We O
use O
f O
′v O
as O

a B-DAT
video-level O
global O
anchor O
for O

learning B-DAT
fur- O
ther O
accurate O
relation-attention O

weights B-DAT

. B-DAT
Relation-attention O
weights. O
We O
believe O
that O

learning B-DAT
weights O
from O
both O
a O

global B-DAT
feature O
and O
local O
features O

is B-DAT
more O
reliable. O
The O
self-attention O

weights B-DAT
are O
learned O
with O
indi O

- B-DAT
vidual O
frame O
features O
and O

non-linear B-DAT
mapping, O
which O
are O
rather O

coarse. B-DAT
Since O
f O
′v O
inherently O

contains B-DAT
the O
contents O
of O
the O

whole B-DAT
video, O
the O
attention O
weights O

can B-DAT
be O
further O
refined O
by O

modeling B-DAT
the O
relation O
between O
frame O

features B-DAT
and O
this O
global O
representation O

f B-DAT
′v O
. O
Inspired O
by O
the O
relation-Net O
in O

low-shot B-DAT
learning O
[16], O
we O
use O

the B-DAT
sample O
concatenation O
and O
another O

FC B-DAT
layer O
to O
esti- O
mate O

new B-DAT
relation-attention O
weights O
for O
frame O

features. B-DAT
The O
relation-attention O
weight O
of O

the B-DAT
i-th O
frame O
is O
formulated O

as B-DAT

, B-DAT
βi O
= O
σ([fi O
: O
f O

′ B-DAT
v O

] B-DAT
Tq1), O
(3 O

) B-DAT
where O
q1 O
is O
the O
parameter O

of B-DAT
FC, O
σ O
denotes O
the O

sigmoid B-DAT
func- O
tion O

. B-DAT
Finally, O
with O
self-attention O
and O
relation-attention O

weights, B-DAT
our O
FAN O
aggregates O
all O

the B-DAT
frame O
features O
into O
a O

new B-DAT
compact O
feature O
as O

, B-DAT
fv O

n B-DAT
i=0 O
αiβi[fi O
: O
f O
′ O
v]∑n O

i=0 B-DAT
αiβi O
. O
(4 O

3. B-DAT
EXPERIMENTS O
3.1. O
Datasets O
and O
Implementation O
Details O

CK+ B-DAT
[17] O
contains O
593 O
video O

sequences B-DAT
from O
123 O
subjects. O
Among O

these B-DAT
videos, O
327 O
sequences O
from O
118 O
subjects O
are O
la- O
beled O
with O

seven B-DAT
basic O
expression O
labels, O
i.e O

. B-DAT
anger, O
contempt, O
disgust, O
fear, O

happiness, B-DAT
sadness, O
and O
surprise. O
Since O

CK+ B-DAT
does O
not O
provide O
training/testing O

splits, B-DAT
most O
of O
the O
algorithms O

evaluated B-DAT
on O
this O
database O
with O
10 O

-fold B-DAT
person-independence O
cross-validation O
experiments. O
We O

constructed B-DAT
10 O
subsets O
by O
sampling O

ID B-DAT
in O
ascending O
order O
with O

a B-DAT
step O
size O
of O
10 O

as B-DAT
in O
several O
previous O

works B-DAT
[18, O
19], O
and O
report O

the B-DAT
overall O
accu- O
racy O
over O
10 O
folds O

. B-DAT
AFEW O
8.0 O
[20] O
served O
as O

an B-DAT
evaluation O
platform O
for O
the O

annual B-DAT
EmotiW O
since O
2013. O
Seven O

emotion B-DAT
labels O
are O
in- O
cluded O

in B-DAT
AFEW, O
i.e. O
anger, O
disgust O

, B-DAT
fear, O
happiness, O
sadness, O
surprise O

and B-DAT
neutral. O
AFEW O
contains O
video O

clips B-DAT
collected O
from O
different O
movies O

and B-DAT
TV O
serials O
with O
spontaneous O

ex- B-DAT
pressions, O
various O
head O
poses, O

occlusions, B-DAT
and O
illuminations. O
AFEW O
8.0 O

is B-DAT
divided O
into O
three O
splits: O

Train B-DAT
(773 O
samples), O
Val O
(383 O

samples) B-DAT
and O
Test O
(653 O
samples), O

which B-DAT
ensures O
data O
in O
the O

three B-DAT
sets O
belong O
to O
mutually O

exclusive B-DAT
movies O
and O
ac- O
tors. O

Since B-DAT
the O
test O
split O
is O

not B-DAT
publicly O
available, O
we O
train O

our B-DAT
model O
on O
training O
split O

and B-DAT
report O
results O
on O
validation O

split. B-DAT
Implementation O
details. O
We O
preprocess O
video O

frames B-DAT
by O
face O
detection O
and O

alignment B-DAT
in O
the O
Dlib O
toolbox O

We B-DAT
extend O
the O
face O
bounding O

box B-DAT
with O
a O
ratio O
of O

25% B-DAT
and O
then O
resize O
the O

cropped B-DAT
faces O
to O
scale O
of O

224×224. B-DAT
We O
implement O
our O
method O

by B-DAT
the O
Pytorch O
toolbox. O
By O

default, B-DAT
for O
feature O
em- O
bedding O

, B-DAT
we O
use O
the O
ResNet18 O

which B-DAT
is O
pre-trained O
on O
MS- O

Celeb-1M B-DAT
[21] O
face O
recognition O
dataset O

and B-DAT
FER O
Plus O
expres- O
sion O

dataset B-DAT
[22]. O
For O
training, O
on O

both B-DAT
CK+ O
and O
AFEW O
8.0, O

we B-DAT
set O
a O
batch O
to O

have B-DAT
48 O
instances O
with O
K O

frames B-DAT
in O
each O
instance. O
For O

frame B-DAT
sampling O
in O
a O
video, O

we B-DAT
first O
split O
the O
video O

into B-DAT
K O
segments O
and O
then O

randomly B-DAT
select O
one O
frame O
from O

each B-DAT
segment. O
By O
default, O
we O

set B-DAT
K O
to O
3. O
We O

use B-DAT
the O
SGD O
method O
for O

optimization B-DAT
with O
a O
momentum O
of O
0 O

.9 B-DAT
and O
a O
weight O
decay O

of B-DAT
10−4. O
On O
CK+, O
we O

initialize B-DAT
the O
learning O
rate O
(lr) O

to B-DAT
0.1, O
and O
modify O
it O

to B-DAT
0.02 O
at O
30 O
epochs, O

and B-DAT
stop O
training O
after O
60 O

epochs. B-DAT
On O
AFEW O
8.0, O
we O

initialize B-DAT
the O
lr O
to O
4e-6, O

and B-DAT
modify O
it O
to O
8e-7 O

at B-DAT
60 O
epochs O
and O
1.6e-7 O

at B-DAT
120 O
epochs, O
and O
stop O

training B-DAT
after O
180 O
epochs. O
3.2. O
Evaluation O
on O
CK O

+ B-DAT
We O
evaluate O
our O
FAN O
on O

CK+ B-DAT
with O
comparisons O
to O
several O

state-of-the-art B-DAT
methods O
in O
Table O
1 O

. B-DAT
On O
CK+, O
due O
to O

the B-DAT
fact O
that O
the O
videos O

show B-DAT
a O
shift O
from O
a O

neutral B-DAT
facial O
expression O
to O
the O

peak B-DAT
expression, O
most O
of O
the O

methods B-DAT
conduct O
data O
selection O
manually. O

Zhang B-DAT
et O
al O
[23] O
propose O

to B-DAT
combine O
a O
spatial O
CNN O

model B-DAT
and O
a O
temporal O
network, O

where B-DAT
the O
spatial O
CNN O
model O

only B-DAT
uses O
the O
last O
peak O

frame. B-DAT
Jung O
et O
al O
[12] O

select B-DAT
a O
fixed O
length O
sequence O

for B-DAT
each O
video O
with O
a O

lipreading B-DAT
method O
[26], O
and O
jointly O

fine-tune B-DAT
a O
deep O
temporal O
Table O
1. O
Evaluation O
of O
our O

FAN B-DAT
with O
a O
comparison O
to O

state- B-DAT
of-the-art O
methods O
on O
CK O

+ B-DAT
database. O
Note O
that O
only O

those B-DAT
methods O
evaluated O
with O
7 O

classes B-DAT
are O
included. O
Method O
Training O
data O
Test O
data O

Acc B-DAT

. B-DAT
ST O
network O
[23] O
S: O
the O

last B-DAT
frame O
T: O
all O
frames O

S: B-DAT
the O
last O
frame O
T: O

all B-DAT
frames O
98.50 O
DTAGN O
[12] O
Fixed O
length O
Fixed O

length B-DAT
97.25 O

CNN+Island B-DAT
loss O
[24] O
The O
last O
three O
frames O
and O

the B-DAT
first O
frame O

The B-DAT
last O
three O
frames O
and O

the B-DAT
first O
frame O
94.35 O

LOMo B-DAT
[25] O
All O
frames O
All O

frames B-DAT
92.00 O
Score O
fusion O
(baseline) O
All O
frames O

All B-DAT
frames O
94.80 O

FAN(w/o B-DAT
Relation- O
attention) O
All O
frames O
All O
frames O
99.08 O

FAN B-DAT
All O
frames O
All O
frames O
99 O

.69 B-DAT
appearance-geometry O
network. O
Cai O
et O
al O

[24] B-DAT
select O
the O
last O
three O

frames B-DAT
and O
the O
first O
frame O

for B-DAT
each O
video, O
and O
train O

CNN B-DAT
models O
with O
a O
new O

Island B-DAT
loss O
function. O
We O
argue O

that B-DAT
manual O
data O
selection O
is O

an B-DAT
ad-hoc O
operation O
on O
CK O

+ B-DAT
and O
it O
is O
impractical O

since B-DAT
we O
can O
not O
know O

which B-DAT
is O
the O
peak O
frame O

beforeahead. B-DAT
Sikka O
et O
al O
[25] O

use B-DAT
all O
frames O
with O
a O

new B-DAT
latent O
ordinal O
model O
which O

extracts B-DAT
CNN/LBP/SIFT O
features O
for O
sub-event O

detection B-DAT
and O
uses O
multi-instance O
SVM O

for B-DAT
ex- O
pression O
classification. O
Our O

baseline B-DAT
method O
uses O
ResNet18 O
to O

generate B-DAT
scores O
for O
individual O
frame O

and B-DAT
applies O
score O
fu- O
sion O
( O

summation) B-DAT
for O
all O
frames. O
It O

achieves B-DAT
94.8% O
which O
is O
2.8% O

better B-DAT
than O
[25]. O
Our O
proposed O

FAN B-DAT
with O
only O
self- O
attention O

gets B-DAT
99.08% O
which O
significantly O
boosts O

the B-DAT
baseline O
by O
4.28%. O
Adding O

relation-attention B-DAT
weights O
further O
im- O
proves O

the B-DAT
accuracy O
to O
99.69% O
which O

sets B-DAT
up O
a O
new O
state O

of B-DAT
the O
art O
on O
CK+. O
3.3. O
Evaluation O
on O
AFEW O
8.0 O

From B-DAT
the O
view O
of O
performance, O

AFEW B-DAT
is O
one O
of O
the O

most B-DAT
challenging O
videos O
FER O
dataset. O

The B-DAT
EmotiW O
challenge O
shares O
the O

same B-DAT
data O
from O
2016 O
to O
2018 O

. B-DAT
Table O
2 O
presents O
the O

evaluation B-DAT
of O
our O
FAN O
on O

AFEW B-DAT
with O
comparisons O
to O
recent O

state-of-the-art B-DAT
methods. O
For O
a O
fair O

comparison, B-DAT
we O
only O
list O
these O

results B-DAT
obtained O
by O
the O
best O

single B-DAT
models O
in O
previous O
works. O

From B-DAT
the O
last O
three O
rows O

of B-DAT
Table O
2, O
our O
proposed O

FAN B-DAT
improves O
the O
baseline O
by O
2 O

.36%. B-DAT
Both O
[27] O
and O
[10] O

use B-DAT
VGGFace O
backbone O
and O
a O

recurrent B-DAT
model O
with O
long-short-term O
memory O

units. B-DAT
These O
methods O
aim O
to O

capture B-DAT
temporal O
dynamic O
information O
for O

videos. B-DAT
Most O
of O
the O
meth- O

ods B-DAT
focus O
on O
improving O
static O

face B-DAT
based O
CNN O
models O

Table B-DAT
2. O
Evaluation O
of O
our O

FAN B-DAT
with O
a O
comparison O
to O

state- B-DAT
of-the-art O
methods O
on O
AFEW O
8 O

.0 B-DAT
database. O
It O
is O
worth O

noting B-DAT
that O
we O
only O
compare O

to B-DAT
the O
best O
single O
models O

of B-DAT
previous O
works. O
Method O
Model O
type O
Accuracy O

CNN-RNN B-DAT
(2016) O
[27] O
Dynamic O
45.43 O

VGGFace B-DAT
+ O
Undirectional O
LSTM O
(2017 O

) B-DAT
[10] O
Dynamic O
48.60 O
HoloNet O
(2016) O
[28] O
Static O
44.57 O

DSN-HoloNet B-DAT
(2017) O
[29] O
Static O
46.47 O

DenseNet-161 B-DAT
(2018) O
[31] O
Static O
51.44 O

DSN-VGGFace B-DAT
(2018) O
[30] O
Static O
48.04 O
Score O
fusion O
(baseline) O
Static O
48.82 O

FAN B-DAT
w/o O
Relation-attention O
Static O
50.92 O

FAN B-DAT
Static O
51.18 O
combine O
scores O
for O
video-level O
FER O

. B-DAT
Both O
[28] O
and O
[29] O

input B-DAT
two O
LBP O
maps O
and O

a B-DAT
gray O
image O
for O
CNN O

models. B-DAT
Deeply- O
supervised O
networks O
are O

used B-DAT
in O
[29] O
and O
[30], O

which B-DAT
add O
supervision O
on O
intermediate O

layers. B-DAT
For O
static O
methods, O
[31] O

gets B-DAT
slightly O
better O
performance O
than O

ours. B-DAT
However, O
[31] O
uses O
DenseNet-161 O

and B-DAT
pretrains O
it O
on O
both O

large-scale B-DAT
face O
datasets O
and O
their O

own B-DAT
Situ O
emotion O
video O
dataset. O

Addition- B-DAT
ally, O
[31] O
applies O
complicated O

post-processing B-DAT
which O
extracts O
frame O
features O

and B-DAT
compute O
their O
mean O
vector, O

max-pooled B-DAT
vector, O
and O
standard O
deviation O

vector. B-DAT
These O
vectors O
are O
then O

concatenated B-DAT
and O
finally O
fed O
into O

an B-DAT
SVM O
classifier. O
Overall, O
our O

FAN B-DAT
improves O
the O
baseline O
significantly O

and B-DAT
achieves O
performance O
comparable O
to O

that B-DAT
of O
the O
best O
previous O

single B-DAT
model. O
3.4. O
Visualization O
and O
Hyper-parameters O

To B-DAT
better O
understand O
the O
self-attention O

and B-DAT
relation-attention O
modules O
in O
our O

FAN, B-DAT
we O
visualize O
the O
attention O

weights B-DAT
in O
Figure O
2. O
Figure O
2 O
shows O
one O
sequence O
for O
each O

category B-DAT
with O
blue O
and O
orange O

weight B-DAT
bars, O
where O
blue O
bars O

represent B-DAT
the O
self-attention O
weights O
(i.e O

. B-DAT
α O
in O
Eq. O
(1)) O

of B-DAT
our O
FAN O
w/o O
relation-attention O

and B-DAT
orange O
bars O
the O
final O

weights B-DAT
(i.e. O
αβ O
in O
Eq. O
(4 O

)) B-DAT
of O
our O
FAN. O
In O

total, B-DAT
both O
kinds O
of O
weights O

can B-DAT
reflect O
the O
importance O
of O

frames. B-DAT
Comparing O
the O
blue O
and O

orange B-DAT
bars, O
we O
find O
that O

the B-DAT
final O
weights O
of O
our O

FAN B-DAT
can O
always O
assign O
higher O

weights B-DAT
to O
the O
more O
obvious O

face B-DAT
frames, O
while O
self-attention O
module O

could B-DAT
assign O
high O
weights O
on O

some B-DAT
ob- O
scure O
face O
frames, O

see B-DAT
the O
1st, O
2th, O
and O

3rd B-DAT
rows O
of O
Figure O
2 O
( O

left). B-DAT
This O
explicitly O
explains O
why O

adding B-DAT
relation-attention O
boost O
performance. O
Evaluation O
of O
Hyper-parameters. O
We O
evaluate O

two B-DAT
hyper-parameters O
of O
our O
FAN O

on B-DAT
CK+, O
i.e. O
backbone O
CNN O

networks B-DAT
and O
the O
parameter O
K O

mentioned B-DAT
in O
implementation O
details, O
to O

validate B-DAT
the O
robustness O
of O
our O

method. B-DAT
For O
the O
parameter O
K O

, B-DAT
besides O
the O
default O
value, O

we B-DAT
try O
several O
other O
values, O

i.e. B-DAT
{2, O
5, O
8}, O
and O

find B-DAT
the O
performance O
is O
not O

sensitive B-DAT
to O
K. O
Specifically, O
our O

FAN B-DAT
obtains O
99.39% O
with O
K={2, O
5 O

}. B-DAT
and O
gets O
99.69% O
with O

K=8. B-DAT
Since O
the O
default O
value, O

K=3 B-DAT
Fig. O
2. O
Visualization O
of O
the O

self-attention B-DAT
weights O
(blue O
bar) O
and O

the B-DAT
final O
weights O
of O
FAN O

(orange B-DAT
bar) O
on O
CK+ O
dataset O

. B-DAT
Fig. O
3. O
Evaluation O
of O
backbone O

CNN B-DAT
models O
and O
training O
strategies O

on B-DAT
CK O

+. B-DAT
gets O
99.69%, O
we O
use O
this O

default B-DAT
setting O
in O
the O
remainder O

of B-DAT
this O
paper O

. B-DAT
For O
the O
backbone O
CNN O
model O

evaluation, B-DAT
we O
try O
the O
VGGFace O

model B-DAT
which O
is O
widely-used O
in O

previous B-DAT
works. O
Similarly, O
we O
also O

pretrain B-DAT
the O
VGGFace O
model O
on O

the B-DAT
FER- O
Plus O
dataset. O
Since O

[5] B-DAT
shows O
that O
it O
is O

better B-DAT
to O
freeze O
all O
the O

feature B-DAT
learning O
layers O
after O
pretrained O

on B-DAT
FERPlus O
for O
VGGFace O
model O

, B-DAT
we O
also O
conduct O
the O

same B-DAT
experiment O
on O
CK+ O
with O

VGGFace. B-DAT
Figure O
3 O
shows O
the O

default B-DAT
com- O
parisons O
with O
different O

backbone B-DAT
CNN O
models. O
On O
CK+, O

compared B-DAT
with O
freezing O
all O
the O

feature B-DAT
layers O
for O
VGGFace, O
it O

gets B-DAT
better O
results O
with O
fine-tuning O

all B-DAT
layers O
which O
may O
be O

explained B-DAT
by O
the O
domain O
discrepancy O

between B-DAT
FERPlus O
and O
CK+. O
Overall, O

the B-DAT
results O
are O
significantly O
improved O

by B-DAT
self-attention O
weights O
and O
further O

improved B-DAT
by O
the O
relation- O
attention O

weights. B-DAT
4. O
CONCLUSION O

We B-DAT
propose O
Frame O
Attention O
Networks O

for B-DAT
video-based O
facial O
expression O
recognition. O

The B-DAT
FAN O
contains O
a O
self-attention O

module B-DAT
and O
a O
relation-attention O
module. O

The B-DAT
experiments O
on O
CK+ O
and O

AFEW B-DAT
show O
that O
our O
FAN O

with B-DAT
only O
self-attention O
improves O
the O

baseline B-DAT
significantly O
and O
adding O
relation- O

attention B-DAT
further O
boosts O
performance. O
With O

a B-DAT
visualization O
on O
CK+, O
we O

demonstrate B-DAT
that O
our O
FAN O
can O

automatically B-DAT
capture O
the O
importance O
of O

frames. B-DAT
Our O
single O
model O
achieves O

performance B-DAT
on O
par O
with O
that O

of B-DAT
state-of-the-art O
methods O
on O
AFEW O

and B-DAT
obtains O
state-of-the-art O
results O
on O

5. B-DAT
REFERENCES O
[1] O
Gwen O
Littlewort, O
Marian O
Stewart O

Bartlett, B-DAT
Ian O
Fasel, O
Joshua O
Susskind O

, B-DAT
and O
Javier O
Movellan, O
“Dynamics O

of B-DAT
facial O
expression O
extracted O
automatically O

from B-DAT
video,” O
in O
IVC, O
2006. O
[2] O
Caifeng O
Shan, O
Shaogang O
Gong O

, B-DAT
and O
Peter O
W. O

Mcowan, B-DAT
“Fa- O
cial O
expression O
recognition O

based B-DAT
on O
local O
binary O
patterns: O

A B-DAT
comprehensive O
study,” O
in O
IVC, O
2009 O

. B-DAT
[3] O
Yichuan O
Tang, O
“Deep O
learning O

using B-DAT
linear O
support O
vector O
ma O

- B-DAT
chines,” O
in O
CS, O
2013. O
[4] O
Sarah O
Adel O
Bargal, O
Emad O

Barsoum, B-DAT
Cristian O
Canton O
Ferrer, O
and O

Cha B-DAT
Zhang, O
“Emotion O
recognition O
in O

the B-DAT
wild O
from O
videos O
using O

images,” B-DAT
in O
ACM O
ICMI, O
2016 O

. B-DAT
[5] O
Boris O
Knyazev, O
Roman O
Shvetsov O

, B-DAT
Natalia O
Efremova, O
and O
Artem O

Kuharenko, B-DAT
“Convolutional O
neural O
networks O
pretrained O

on B-DAT
large O
face O
recognition O
datasets O

for B-DAT
emotion O
classification O
from O
video,” O

in B-DAT
ACM O
ICMI, O
2017. O
[6] O
Sepp O
Hochreiter O
and O
Jrgen O

Schmidhuber, B-DAT
“Long O
short-term O
memory,” O
Neural O

Computation, B-DAT
1997 O

. B-DAT
[7] O
Du O
Tran, O
Lubomir O
Bourdev O

, B-DAT
Rob O
Fergus, O
Lorenzo O
Torresani, O

and B-DAT
Manohar O
Paluri, O
“Learning O
spatiotemporal O

features B-DAT
with O
3d O
convolutional O
networks,” O

in B-DAT
ICCV, O
2015. O
[8] O
Yuanliu O
Liu, O
Yuanliu O
Liu O

, B-DAT
Yuanliu O
Liu, O
and O
Yuanliu O

Liu, B-DAT
“Video-based O
emotion O
recognition O
using O

cnn-rnn B-DAT
and O
c3d O
hy- O
brid O

networks,” B-DAT
in O
ACM O
ICMI, O
2016. O
[9] O
Xi O
Ouyang, O
Shigenori O
Kawaai O

, B-DAT
Ester O
Gue O
Hua O
Goh, O

Sheng- B-DAT
mei O
Shen, O
Wan O
Ding, O

Huaiping B-DAT
Ming, O
and O
Dong-Yan O

Huang, B-DAT
“Audio-visual O
emotion O
recognition O
using O

deep B-DAT
transfer O
learning O
and O
multiple O

temporal B-DAT
models,” O
in O
ACM O
ICMI, O
2017 O

. B-DAT
[10] O
Valentin O
Vielzeuf, O
Stphane O
Pateux O

, B-DAT
and O
Frdric O
Jurie, O
“Temporal O

multimodal B-DAT
fusion O
for O
video O
emotion O

classification B-DAT
in O
the O
wild,” O
in O

ACM B-DAT
ICMI, O
2017. O
[11] O
Jingwei O
Yan, O
Wenming O
Zheng O

, B-DAT
Zhen O
Cui, O
Chuangao O
Tang, O

Tong B-DAT
Zhang, O
and O
Yuan O

Zong, B-DAT
“Multi-cue O
fusion O
for O
emotion O

recognition B-DAT
in O
the O
wild,” O
Neurocomputing, O
2018 O

. B-DAT
[12] O
Heechul O
Jung, O
Sihaeng O
Lee O

, B-DAT
Junho O
Yim, O
Sunjeong O
Park, O

and B-DAT
Junmo O
Kim, O
“Joint O
fine-tuning O

in B-DAT
deep O
neural O
networks O
for O

facial B-DAT
expression O
recognition,” O
in O
ICCV, O
2015 O

. B-DAT
[13] O
Samira O
Ebrahimi O
Kahou, O
Christopher O

Pal, B-DAT
Xavier O
Bouthillier, O
Pierre O
Froumenty O

, B-DAT
Roland O
Memisevic, O
Pascal O
Vincent, O

Aaron B-DAT
Courville, O
Yoshua O
Bengio, O
and O

Raul B-DAT
Chandias O
Ferrari, O
“Com- O
bining O

modality B-DAT
specific O
deep O
neural O
networks O

for B-DAT
emotion O
recognition O
in O
video,” O

in B-DAT
ACM O
ICMI, O
2013. O
[14] O
Ashish O
Vaswani, O
Noam O
Shazeer O

, B-DAT
Niki O
Parmar, O
Jakob O
Uszko- O

reit, B-DAT
Llion O
Jones, O
Aidan O
N O

Gomez, B-DAT
Łukasz O
Kaiser, O
and O
Illia O

Polosukhin, B-DAT
“Attention O
is O
all O
you O

need,” B-DAT
in O
NIPS, O
2017. O
[15] O
Jiaolong O
Yang, O
Peiran O
Ren O

, B-DAT
Dongqing O
Zhang, O
Dong O
Chen, O

Fang B-DAT
Wen, O
Hongdong O
Li, O
and O

Gang B-DAT
Hua, O
“Neural O
aggregation O
network O

for B-DAT
video O
face O
recognition.,” O
in O

CVPR, B-DAT
2017. O
[16] O
Flood O
Sung O
Yongxin O
Yang O

, B-DAT
Li O
Zhang, O
Tao O
Xiang, O

Philip B-DAT
HS O
Torr, O
and O
Timothy O

M B-DAT
Hospedales, O
“Learning O
to O
compare: O

Re- B-DAT
lation O
network O
for O
few-shot O

learning,” B-DAT
in O
CVPR, O
2018. O
[17] O
Patrick O
Lucey, O
Jeffrey O
F O

Cohn, B-DAT
Takeo O
Kanade, O
Jason O
Saragih O

, B-DAT
Zara O
Ambadar, O
and O
Iain O

Matthews, B-DAT
“The O
extended O
cohn- O
kanade O

dataset B-DAT
(ck+): O
A O
complete O
dataset O

for B-DAT
action O
unit O
and O
emotion-specified O

expression,” B-DAT
in O
CVPRW, O
2010. O
[18] O
Mengyi O
Liu, O
Shiguang O
Shan O

, B-DAT
Ruiping O
Wang, O
and O
Xilin O

Chen, B-DAT
“Learning O
expressionlets O
on O
spatio-temporal O

manifold B-DAT
for O
dy- O
namic O
facial O

expression B-DAT
recognition,” O
in O
CVPR, O
2014. O
[19] O
Chieh-Ming O
Kuo, O
Shang-Hong O
Lai O

, B-DAT
and O
Michel O
Sarkis, O
“A O

compact B-DAT
deep O
learning O
model O
for O

robust B-DAT
facial O
expression O
recognition,” O
in O

CVPRW, B-DAT
2018. O
[20] O
Abhinav O
Dhall, O
Amanjot O
Kaur O

, B-DAT
Roland O
Goecke, O
and O
Tom O

Gedeon, B-DAT
“Emotiw O
2018: O
Audio-video, O
student O

engagement B-DAT
and O
group-level O
affect O
prediction,” O

arXiv B-DAT
preprint:1808.07773, O
2018. O
[21] O
Yandong O
Guo, O
Lei O
Zhang O

, B-DAT
Yuxiao O
Hu, O
Xiaodong O
He, O

and B-DAT
Jian- O
feng O
Gao, O
“Ms-celeb-1m: O

A B-DAT
dataset O
and O
benchmark O
for O

large- B-DAT
scale O
face O
recognition,” O
in O

ECCV, B-DAT
2016. O
[22] O
Emad O
Barsoum, O
Cha O
Zhang O

, B-DAT
Cristian O
Canton O
Ferrer, O
and O

Zhengyou B-DAT
Zhang, O
“Training O
deep O
networks O

for B-DAT
facial O
expres- O
sion O
recognition O

with B-DAT
crowd-sourced O
label O
distribution,” O
in O

ACM B-DAT
ICMI, O
2016. O
[23] O
Kaihao O
Zhang, O
Yongzhen O
Huang O

, B-DAT
Yong O
Du, O
and O
Liang O

Wang, B-DAT
“Facial O
expression O
recognition O
based O

on B-DAT
deep O
evolutional O
spatial-temporal O
networks,” O

IEEE B-DAT
TIP, O
2017. O
[24] O
Jie O
Cai, O
Zibo O
Meng O

, B-DAT
Ahmed O
Shehab O
Khan, O
Zhiyuan O

Li, B-DAT
James O
OReilly, O
and O
Yan O

Tong, B-DAT
“Island O
loss O
for O
learning O

discriminative B-DAT
features O
in O
facial O
expression O

recognition,” B-DAT
in O
FG, O
2018. O
[25] O
Karan O
Sikka, O
Gaurav O
Sharma O

, B-DAT
and O
Marian O
Bartlett, O
“Lomo: O

Latent B-DAT
ordinal O
model O
for O
facial O

analysis B-DAT
in O
videos,” O
in O
CVPR, O
2016 O

. B-DAT
[26] O
Ziheng O
Zhou, O
Guoying O
Zhao O

, B-DAT
and O
M. O
Pietikainen, O
“Towards O

a B-DAT
practical O
lipreading O
system,” O
in O

CVPR, B-DAT
2011. O
[27] O
Yin O
Fan, O
Xiangju O
Lu O

, B-DAT
Dian O
Li, O
and O
Yuanliu O

Liu, B-DAT
“Video-based O
emotion O
recognition O
using O

cnn-rnn B-DAT
and O
c3d O
hybrid O
networks,” O

in B-DAT
ACM O
ICMI, O
2016. O
[28] O
Anbang O
Yao, O
Dongqi O
Cai O

, B-DAT
Ping O
Hu, O
Shandong O
Wang, O

Liang B-DAT
Sha, O
and O
Yurong O

Chen, B-DAT
“Holonet: O
towards O
robust O
emotion O

recognition B-DAT
in O
the O
wild,” O
in O

ACM B-DAT
ICMI, O
2016. O
[29] O
Ping O
Hu, O
Dongqi O
Cai O

, B-DAT
Shandong O
Wang, O
Anbang O
Yao, O

and B-DAT
Yurong O
Chen, O
“Learning O
supervised O

scoring B-DAT
ensemble O
for O
emo- O
tion O

recognition B-DAT
in O
the O
wild,” O
in O

ACM B-DAT
ICMI, O
2017. O
[30] O
Yingruo O
Fan, O
Jacqueline O
CK O

Lam, B-DAT
and O
Victor O
OK O
Li O

, B-DAT
“Video- O
based O
emotion O
recognition O

using B-DAT
deeply-supervised O
neural O
net- O
works,” O

in B-DAT
ACM O
ICMI, O
2018. O
[31] O
Chuanhe O
Liu, O
Tianhao O
Tang O

, B-DAT
Kui O
Lv, O
and O
Minghao O

Wang, B-DAT
“Multi-feature O
based O
emotion O
recognition O

for B-DAT
video O
clips,” O
in O
ACM O

ICMI, B-DAT
2018 O

1 B-DAT
Introduction O

2 B-DAT
Frame O
Attention O
Networks O

3 B-DAT
Experiments O

3.1 B-DAT
Datasets O
and O
Implementation O
Details O

3.2 B-DAT
Evaluation O
on O
CK O

3.3 B-DAT
Evaluation O
on O
AFEW O
8.0 O

3.4 B-DAT
Visualization O
and O
Hyper-parameters O

4 B-DAT
Conclusion O

5 B-DAT
References O

extensive O
experiments O
on O
CK+ O
and O
AFEW8 B-DAT

recognition, O
frame O
attention O
networks, O
CNN, O
AFEW B-DAT

extensive O
experiments O
on O
CK+ O
and O
AFEW8 B-DAT

AFEW B-DAT
8.0 O
[20] O
served O
as O
an O

labels O
are O
in- O
cluded O
in O
AFEW, B-DAT
i.e. O
anger, O
disgust, O
fear, O
happiness O

, O
sadness, O
surprise O
and O
neutral. O
AFEW B-DAT
contains O
video O
clips O
collected O
from O

head O
poses, O
occlusions, O
and O
illuminations. O
AFEW B-DAT
8.0 O
is O
divided O
into O
three O

training, O
on O
both O
CK+ O
and O
AFEW B-DAT
8.0, O
we O
set O
a O
batch O

training O
after O
60 O
epochs. O
On O
AFEW B-DAT
8.0, O
we O
initialize O
the O
lr O

3.3. O
Evaluation O
on O
AFEW B-DAT
8.0 O

From O
the O
view O
of O
performance, O
AFEW B-DAT
is O
one O
of O
the O
most O

evaluation O
of O
our O
FAN O
on O
AFEW B-DAT
with O
comparisons O
to O
recent O
state-of-the-art O

to O
state- O
of-the-art O
methods O
on O
AFEW B-DAT
8.0 O
database. O
It O
is O
worth O

The O
experiments O
on O
CK+ O
and O
AFEW B-DAT
show O
that O
our O
FAN O
with O

that O
of O
state-of-the-art O
methods O
on O
AFEW B-DAT
and O
obtains O
state-of-the-art O
results O
on O

Evaluation O
on O
AFEW B-DAT
8.0 O

is O
crucial O
for O
this O
task. O
In B-DAT
this O
paper, O
we O
propose O
the O

Pro- O
gram O
(JCYJ20170818164704758, O
JSGG20180507182100698), O
and O
In B-DAT

category O
while O
the O
others O
not. O
In B-DAT
this O
paper, O
inspired O
by O
the O

I2, O
· O
· O
· O
, O
In, B-DAT
and O
the O
facial O
frame O
features O

Eq. O
(4)) O
of O
our O
FAN. O
In B-DAT
total, O
both O
kinds O
of O
weights O

ABSTRACT O
The B-DAT
video-based O
facial O
expression O
recognition O
aims O

frames O
in O
an O
end-to-end O
framework. O
The B-DAT
network O
takes O
a O
video O
with O

and O
produces O
a O
fixed-dimension O
representation. O
The B-DAT
whole O
network O
is O
com- O
posed O

of O
two O
modules. O
The B-DAT
feature O
embedding O
module O
is O
a O

face O
images O
into O
feature O
vectors. O
The B-DAT
frame O
attention O
module O
learns O
multiple O

or O
motion O
information O
in O
videos. O
The B-DAT
Long O
Short-Term O
Mem- O
ory O
(LSTM O

to O
adaptively O
aggregate O
frame O
features. O
The B-DAT
FAN O
is O
designed O
to O
learn O

reasoning O
in O
an O
end-to-end O
fashion. O
The B-DAT
self-attention O
kernels O
are O
directly O
learned O

fixed-dimension O
feature O
representation O
for O
FER. O
The B-DAT
whole O
network O
consists O
of O
two O

module O
and O
frame O
attention O
module. O
The B-DAT
feature O
embedding O
module O
is O
a O

image O
into O
a O
feature O
vector. O
The B-DAT
frame O
attention O
module O
learns O
two-level O

local O
features O
is O
more O
reliable. O
The B-DAT
self-attention O
weights O
are O
learned O
with O

relation-attention O
weights O
for O
frame O
features. O
The B-DAT
relation-attention O
weight O
of O
the O
i-th O

The B-DAT
last O
three O
frames O
and O
the O

The B-DAT
last O
three O
frames O
and O
the O

most O
challenging O
videos O
FER O
dataset. O
The B-DAT
EmotiW O
challenge O
shares O
the O
same O

for O
video-based O
facial O
expression O
recognition. O
The B-DAT
FAN O
contains O
a O
self-attention O
module O

and O
a O
relation-attention O
module. O
The B-DAT
experiments O
on O
CK+ O
and O
AFEW O

The B-DAT
extended O
cohn- O
kanade O
dataset O
(ck O

extensive O
experiments O
on O
CK+ O
and O
AFEW8 B-DAT

recognition, O
frame O
attention O
networks, O
CNN, O
AFEW B-DAT

extensive O
experiments O
on O
CK+ O
and O
AFEW8 B-DAT

AFEW B-DAT
8.0 O
[20] O
served O
as O
an O

labels O
are O
in- O
cluded O
in O
AFEW, B-DAT
i.e. O
anger, O
disgust, O
fear, O
happiness O

, O
sadness, O
surprise O
and O
neutral. O
AFEW B-DAT
contains O
video O
clips O
collected O
from O

head O
poses, O
occlusions, O
and O
illuminations. O
AFEW B-DAT
8.0 O
is O
divided O
into O
three O

training, O
on O
both O
CK+ O
and O
AFEW B-DAT
8.0, O
we O
set O
a O
batch O

training O
after O
60 O
epochs. O
On O
AFEW B-DAT
8.0, O
we O
initialize O
the O
lr O

3.3. O
Evaluation O
on O
AFEW B-DAT
8.0 O

From O
the O
view O
of O
performance, O
AFEW B-DAT
is O
one O
of O
the O
most O

evaluation O
of O
our O
FAN O
on O
AFEW B-DAT
with O
comparisons O
to O
recent O
state-of-the-art O

to O
state- O
of-the-art O
methods O
on O
AFEW B-DAT
8.0 O
database. O
It O
is O
worth O

The O
experiments O
on O
CK+ O
and O
AFEW B-DAT
show O
that O
our O
FAN O
with O

that O
of O
state-of-the-art O
methods O
on O
AFEW B-DAT
and O
obtains O
state-of-the-art O
results O
on O

Evaluation O
on O
AFEW B-DAT
8.0 O

FRAME B-DAT
ATTENTION O
NETWORKS O
FOR O
FACIAL O

EXPRESSION B-DAT
RECOGNITION O
IN O
VIDEOS O
Debin O
Meng, O
Xiaojiang O
Peng∗, O
Kai O

Wang, B-DAT
Yu O
Qiao O

Shenzhen B-DAT
Institutes O
of O
Advanced O
Technology, O

Chinese B-DAT
Academy O
of O
Science, O
Shenzhen, O

China B-DAT
Shenzhen O
Key O
Lab O
of O

Computer B-DAT
Vision O
and O
Pattern O
Recognition, O

Shenzhen, B-DAT
China O
University O
of O
Chinese O
Academy O
of O

Sciences, B-DAT
Beijing, O
China O
michaeldbmeng19@outlook.com, O
{xj.peng O

, B-DAT
kai.wang, O
yu.qiao}@siat.ac.cn O
ABSTRACT O
The O
video-based O
facial O
expression O

recognition B-DAT
aims O
to O
classify O
a O

given B-DAT
video O
into O
several O
basic O

emotions. B-DAT
How O
to O
integrate O
facial O

features B-DAT
of O
individual O
frames O
is O

crucial B-DAT
for O
this O
task. O
In O

this B-DAT
paper, O
we O
propose O
the O

Frame B-DAT
Attention O
Networks O
(FAN)1, O
to O

automatically B-DAT
highlight O
some O
discriminative O
frames O

in B-DAT
an O
end-to-end O
framework. O
The O

network B-DAT
takes O
a O
video O
with O

a B-DAT
variable O
number O
of O
face O

images B-DAT
as O
its O
input O
and O

produces B-DAT
a O
fixed-dimension O
representation. O
The O

whole B-DAT
network O
is O
com- O
posed O

of B-DAT
two O
modules. O
The O
feature O

embedding B-DAT
module O
is O
a O
deep O

Convolutional B-DAT
Neural O
Network O
(CNN) O
which O

embeds B-DAT
face O
images O
into O
feature O

vectors. B-DAT
The O
frame O
attention O
module O

learns B-DAT
multiple O
attention O
weights O
which O

are B-DAT
used O
to O
adaptively O
aggregate O

the B-DAT
feature O
vectors O
to O
form O

a B-DAT
single O
discriminative O
video O
representation O

. B-DAT
We O
conduct O
extensive O
experiments O

on B-DAT
CK+ O
and O
AFEW8.0 O
datasets. O

Our B-DAT
proposed O
FAN O
shows O
su- O

perior B-DAT
performance O
compared O
to O
other O

CNN B-DAT
based O
methods O
and O
achieves O

state-of-the-art B-DAT
performance O
on O
CK+. O
Index O
Terms— O
facial O
expression O
recognition O

, B-DAT
audio- O
video O
emotion O
recognition, O

frame B-DAT
attention O
networks, O
CNN, O
AFEW O
1. O
INTRODUCTION O

Automatic B-DAT
facial O
expression O
recognition O
(FER) O

has B-DAT
recently O
attracted O
increasing O
attention O

in B-DAT
academia O
and O
industry O
due O

to B-DAT
its O
wide O
range O
of O

applications B-DAT
such O
as O
affective O
computing, O

intelligent B-DAT
environments, O
and O
multimodal O
human-computer O

interface B-DAT
(HCI). O
Though O
great O
progress O

have B-DAT
been O
made O
re- O
cently, O

facial B-DAT
expression O
recognition O
in O
the O

wild B-DAT
remains O
a O
challenging O
problem O

due B-DAT
to O
large O
head O
pose, O

illumination B-DAT
variance, O
occlusion, O
motion O
blur, O

etc. B-DAT
Video-based O
facial O
expression O
recognition O
aims O

to B-DAT
classify O
a O
video O
into O

several B-DAT
basic O
emotions, O
such O
as O

happy, B-DAT
angry, O
dis O

- B-DAT
∗Xiaojiang O
Peng O
is O
the O
corresponding O

author. B-DAT
Email: O
xj.peng@siat.ac.cn O
This O
work O

was B-DAT
supported O
by O
the O
National O

Natural B-DAT
Science O
Foun O

- B-DAT
dation O
of O
China O
(U1613211, O
U1713208 O

), B-DAT
Shenzhen O
Research O
Pro- O
gram O
( O

JCYJ20170818164704758, B-DAT
JSGG20180507182100698), O
and O
In- O
ternational O

Partnership B-DAT
Program O
of O
Chinese O
Academy O

of B-DAT
Sciences O
(172644KYSB20150019). O
1Code O
is O
available O
at O
https://github.com/Open-Debin/Emotion-FAN O

gust, B-DAT
fear, O
sad, O
neutral, O
and O

surprise. B-DAT
Given O
a O
video, O
the O

pop- B-DAT
ular O
FER O
pipeline O
with O

a B-DAT
visual O
clue O
(FER O
with O

an B-DAT
audio O
clue O
is O
out O

of B-DAT
the O
scope O
of O
this O

paper) B-DAT
mainly O
includes O
three O
steps, O

namely B-DAT
frame O
preprocessing, O
feature O
extraction, O

and B-DAT
classi- O
fication. O
Especially, O
frame O

preprocessing B-DAT
refers O
to O
face O
de- O

tection, B-DAT
alignment, O
illumination O
normalizing O
and O

so B-DAT
on. O
Fea- O
ture O
extraction O

or B-DAT
video O
representation O
is O
the O

key B-DAT
part O
for O
FER O
which O

encodes B-DAT
frames O
or O
sequences O
into O

compact B-DAT
feature O
vec- O
tors. O
These O

feature B-DAT
vectors O
are O
subsequently O
fed O

into B-DAT
a O
classi- O
fier O
for O

prediction. B-DAT
Feature O
extraction O
methods O
for O
video-based O

FER B-DAT
can O
be O
roughly O
divided O

into B-DAT
three O
types: O
static-based O
methods O

, B-DAT
spatial-temporal O
methods, O
and O
geometry-based O

methods. B-DAT
Static-based O
feature O
extraction O
methods O
mainly O

inherit B-DAT
those O
methods O
from O
static O

image B-DAT
emotion O
recognition O
which O
can O

be B-DAT
both O
hand-crafted O
[1, O
2 O

] B-DAT
and O
learned O
[3, O
4, O
5 O

]. B-DAT
For O
the O
hand-crafted O
features, O

Littlewort B-DAT
et O
al. O
[1] O
propose O

to B-DAT
use O
a O
bank O
of O

2D B-DAT
Gabor O
filters O
to O
extract O

facial B-DAT
features O
for O
video- O
based O

FER. B-DAT
Shan O
et O
al. O
[2] O

use B-DAT
local O
binary O
patterns O
(LBP) O

and B-DAT
LBP O
histogram O
for O
facial O

feature B-DAT
extraction. O
For O
the O
learned O

features, B-DAT
Tang O
[3] O
utilizes O
deep O

CNNs B-DAT
for O
feature O
extraction, O
and O

win B-DAT
the O
FER2013. O
Some O
winners O

in B-DAT
audio-video O
emotion O
recognition O
task O

of B-DAT
EmotiW2016 O
and O
EmotiW2017 O
only O

use B-DAT
static O
facial O
features O
from O

deep B-DAT
CNNs O
trained O
on O
large O

face B-DAT
datasets O
or O
trained O
with O

multi-level B-DAT
supervision O
[4, O
5]. O
Spatial-temporal O
methods O
aim O
to O
model O

the B-DAT
temporal O
or O
motion O
information O

in B-DAT
videos. O
The O
Long O
Short-Term O

Mem- B-DAT
ory O
(LSTM) O
[6], O
and O

C3D B-DAT
[7] O
are O
two O
widely-used O

spatial- B-DAT
temporal O
methods O
for O
video-based O

FER. B-DAT
LSTM O
derives O
in- O
formation O

from B-DAT
sequences O
by O
exploiting O
the O

fact B-DAT
that O
feature O
vectors O
are O

connected B-DAT
semantically O
for O
successive O
data O

. B-DAT
This O
pipeline O
is O
widely-used O

in B-DAT
the O
EmotiW O
challenge, O

e.g. B-DAT
[8, O
9, O
10, O
11]. O

C3D, B-DAT
which O
is O
originally O
developed O

for B-DAT
video O
action O
recognition, O
is O

also B-DAT
popular O
in O
the O
EmotiW O

challenge. B-DAT
Geometry O
based O
methods O
[12, O
11 O

] B-DAT
aim O
to O
model O
the O

mo- B-DAT
tions O
of O
key O
points O

in B-DAT
faces O
which O
only O
leverage O

the B-DAT
geometry O
locations O
of O
facial O

landmarks B-DAT
in O
every O
video O
frames. O

Jung B-DAT
et O
al. O
[12] O
propose O

a B-DAT
deep O
temporal O
appearance-geometry O
net- O

work B-DAT
(DTAGN) O
which O
first O
alternately O

concatenates B-DAT
the O
x- O
coordinates O
and O

y-coordinates B-DAT
of O
the O
facial O
landmark O

points B-DAT
Copyright O
2019 O
IEEE. O
Published O
in O

the B-DAT
IEEE O
2019 O
International O
Conference O

on B-DAT
Image O
Processing O
(ICIP O
2019 O

), B-DAT
scheduled O
for O
22-25 O
September O
2019 O
in O
Taipei, O
Taiwan. O
Personal O
use O

of B-DAT
this O
material O
is O
permitted O

. B-DAT
However, O
permission O
to O
reprint/republish O

this B-DAT
material O
for O
advertising O
or O

promotional B-DAT
purposes O
or O
for O
creating O

new B-DAT
collective O
works O
for O
resale O

or B-DAT
redistribution O
to O
servers O
or O

lists, B-DAT
or O
to O
reuse O
any O

copyrighted B-DAT
component O
of O
this O
work O

in B-DAT
other O
works, O
must O
be O

obtained B-DAT
from O
the O
IEEE. O
Contact: O

Manager, B-DAT
Copyrights O
and O
Permissions O
/ O

IEEE B-DAT
Service O
Center O
/ O
445 O

Hoes B-DAT
Lane O
/ O
P.O. O
Box O
1331 O
/ O
Piscataway, O
NJ O
08855-1331, O
USA O

. B-DAT
Telephone: O
+ O
Intl. O
908-562-3966. O
ar O
X O

iv B-DAT
:1 O
90 O
7 O

. B-DAT
00 O
19 O

3v B-DAT
2 O

cs B-DAT
.C O
V O

1 B-DAT
2 O

Se B-DAT
p O

20 B-DAT
19 O

from B-DAT
each O
frame O
after O
normalization O

and B-DAT
then O
concatenates O
these O
normalized O

points B-DAT
over O
time O
for O
a O

one-dimensional B-DAT
tra- O
jectory O
signal O
of O

each B-DAT
sequence. O
Yan O
et O

al. B-DAT
[11] O
construct O
an O
image-like O

map B-DAT
by O
stretching O
all O
the O

normalized B-DAT
facial O
point O
trajectories O
in O

a B-DAT
sequence O
together O
as O
the O

input B-DAT
of O
a O
CNN. O
Among O
all O
the O
above O
methods O

, B-DAT
static-based O
methods O
are O
superior O

to B-DAT
the O
others O
according O
to O

several B-DAT
winner O
solutions O
in O
EmotiW O

challenges. B-DAT
To O
obtain O
a O
video-level O

result B-DAT
with O
varied O
frames, O
a O

frame B-DAT
aggregation O
operation O
is O
necessary O

for B-DAT
static- O
based O
methods. O
For O

frame B-DAT
aggregation, O
Kahou O
et O

al. B-DAT
[13] O
concatenate O
the O
n-class O

probability B-DAT
vectors O
of O
10 O
segments O

to B-DAT
form O
a O
fixed-length O
video O

representation B-DAT
by O
frame O
averag- O
ing O

or B-DAT
frame O
expansion. O
Bargal O
et O

al. B-DAT
[4] O
propose O
a O
statistical O

encoding B-DAT
module O
(STAT) O
to O
aggregate O

frame B-DAT
features O
which O
compute O
the O

mean, B-DAT
variance, O
minimum, O
and O
maximum O

of B-DAT
the O
frame O
feature O
vectors. O
One O
limitation O
of O
these O
existing O

aggregation B-DAT
methods O
is O
that O
they O

ignore B-DAT
the O
importance O
of O
frames O

for B-DAT
FER. O
For O
example, O
some O

faces B-DAT
in O
Figure O
1 O
are O

representative B-DAT
for O
the O
‘happy’ O
category O

while B-DAT
the O
others O
not. O
In O

this B-DAT
paper, O
inspired O
by O
the O

attention B-DAT
mechanism O
[14] O
of O
machine O

translation B-DAT
and O
the O
neural O
aggregation O

networks B-DAT
[15] O
of O
video O
face O

recog- B-DAT
nition, O
we O
propose O
the O

Frame B-DAT
Attention O
Networks O
(FAN) O
to O

adaptively B-DAT
aggregate O
frame O
features. O
The O

FAN B-DAT
is O
designed O
to O
learn O

self-attention B-DAT
kernels O
and O
relation-attention O
kernels O

for B-DAT
frame O
importance O
reasoning O
in O

an B-DAT
end-to-end O
fashion. O
The O
self-attention O

kernels B-DAT
are O
directly O
learned O
from O

frame B-DAT
features O
while O
the O
relation-attention O

kernels B-DAT
are O
learned O
from O
the O

concatenated B-DAT
features O
of O
a O
video-level O

anchor B-DAT
feature O
and O
frame O
features O

. B-DAT
We O
conduct O
extensive O
experiments O

on B-DAT
CK+ O
and O
AFEW8.0 O
(EmotiW2018) O

datasets. B-DAT
Our O
proposed O
FAN O
shows O

superior B-DAT
performance O
compared O
to O
other O

CNN B-DAT
based O
methods O
with O
only O

facial B-DAT
features O
and O
achieves O
state-of-the- O

art B-DAT
performance O
on O
CK+. O
2. O
FRAME O
ATTENTION O
NETWORKS O

We B-DAT
propose O
Frame O
Attention O
Networks O
( O

FAN) B-DAT
for O
video- O
based O
facial O

expression B-DAT
recognition O
(FER). O
Figure O
1 O

illus- B-DAT
trates O
the O
framework O
of O

our B-DAT
proposed O
FAN. O
It O
takes O

a B-DAT
facial O
video O
with O
a O

variable B-DAT
number O
of O
face O
images O

as B-DAT
its O
input O
and O
produces O

a B-DAT
fixed-dimension O
feature O
representation O
for O

FER. B-DAT
The O
whole O
network O
consists O

of B-DAT
two O
modules: O
feature O
em- O

bedding B-DAT
module O
and O
frame O
attention O

module. B-DAT
The O
feature O
embedding O
module O

is B-DAT
a O
deep O
CNN O
which O

embeds B-DAT
each O
face O
image O
into O

a B-DAT
feature O
vector. O
The O
frame O

attention B-DAT
module O
learns O
two-level O
attention O

weights, B-DAT
i.e. O
self-attention O
weights O
and O

relation-attention B-DAT
weights, O
which O
are O
used O

to B-DAT
adaptively O
aggregate O
the O
feature O

vectors B-DAT
to O
form O
a O
single O

discriminative B-DAT
video O
representation. O
Formally, O
we O
denote O
a O
video O

with B-DAT
n O
frames O
as O
V O

, B-DAT
and O
its O
frames O
as O

I1, B-DAT
I2, O
· O
· O
· O
, O
In, O
and O
the O
facial O
frame O

features B-DAT
are O
{f1 O

, B-DAT
· O
· O
· O
, O

fn}. B-DAT
Self-attention O
weights. O
With O
individual O
frame O

features B-DAT

, B-DAT
Fig. O
1. O
Our O
proposed O
frame O

attention B-DAT
network O
architecture O

. B-DAT
our O
FAN O
first O
applies O
a O

FC B-DAT
layer O
and O
a O
sigmoid O

function B-DAT
to O
assign O
coarse O
self-attention O

weights. B-DAT
Mathematically, O
the O
self- O
attention O

weight B-DAT
of O
the O
i-th O
frame O

is B-DAT
defined O
by O

: B-DAT
αi O
= O
σ(f O
T O
i O

q B-DAT

0) B-DAT
(1) O
where O
q0 O
is O
the O
parameter O

of B-DAT
FC, O
σ O
denotes O
the O

sigmoid B-DAT
func- O
tion. O
With O
these O

self-attention B-DAT
weights, O
we O
aggregate O
all O

the B-DAT
input O
frame O
features O
into O

a B-DAT
global O
representation O
f O
′v O

as B-DAT
follows O

, B-DAT
f O
′v O

n B-DAT
i=1 O
αifi∑n O
i=1 O
αi O
. O
(2 O

) B-DAT
We O
use O
f O
′v O
as O

a B-DAT
video-level O
global O
anchor O
for O

learning B-DAT
fur- O
ther O
accurate O
relation-attention O

weights B-DAT

. B-DAT
Relation-attention O
weights. O
We O
believe O
that O

learning B-DAT
weights O
from O
both O
a O

global B-DAT
feature O
and O
local O
features O

is B-DAT
more O
reliable. O
The O
self-attention O

weights B-DAT
are O
learned O
with O
indi O

- B-DAT
vidual O
frame O
features O
and O

non-linear B-DAT
mapping, O
which O
are O
rather O

coarse. B-DAT
Since O
f O
′v O
inherently O

contains B-DAT
the O
contents O
of O
the O

whole B-DAT
video, O
the O
attention O
weights O

can B-DAT
be O
further O
refined O
by O

modeling B-DAT
the O
relation O
between O
frame O

features B-DAT
and O
this O
global O
representation O

f B-DAT
′v O
. O
Inspired O
by O
the O
relation-Net O
in O

low-shot B-DAT
learning O
[16], O
we O
use O

the B-DAT
sample O
concatenation O
and O
another O

FC B-DAT
layer O
to O
esti- O
mate O

new B-DAT
relation-attention O
weights O
for O
frame O

features. B-DAT
The O
relation-attention O
weight O
of O

the B-DAT
i-th O
frame O
is O
formulated O

as B-DAT

, B-DAT
βi O
= O
σ([fi O
: O
f O

′ B-DAT
v O

] B-DAT
Tq1), O
(3 O

) B-DAT
where O
q1 O
is O
the O
parameter O

of B-DAT
FC, O
σ O
denotes O
the O

sigmoid B-DAT
func- O
tion O

. B-DAT
Finally, O
with O
self-attention O
and O
relation-attention O

weights, B-DAT
our O
FAN O
aggregates O
all O

the B-DAT
frame O
features O
into O
a O

new B-DAT
compact O
feature O
as O

, B-DAT
fv O

n B-DAT
i=0 O
αiβi[fi O
: O
f O
′ O
v]∑n O

i=0 B-DAT
αiβi O
. O
(4 O

3. B-DAT
EXPERIMENTS O
3.1. O
Datasets O
and O
Implementation O
Details O

CK+ B-DAT
[17] O
contains O
593 O
video O

sequences B-DAT
from O
123 O
subjects. O
Among O

these B-DAT
videos, O
327 O
sequences O
from O
118 O
subjects O
are O
la- O
beled O
with O

seven B-DAT
basic O
expression O
labels, O
i.e O

. B-DAT
anger, O
contempt, O
disgust, O
fear, O

happiness, B-DAT
sadness, O
and O
surprise. O
Since O

CK+ B-DAT
does O
not O
provide O
training/testing O

splits, B-DAT
most O
of O
the O
algorithms O

evaluated B-DAT
on O
this O
database O
with O
10 O

-fold B-DAT
person-independence O
cross-validation O
experiments. O
We O

constructed B-DAT
10 O
subsets O
by O
sampling O

ID B-DAT
in O
ascending O
order O
with O

a B-DAT
step O
size O
of O
10 O

as B-DAT
in O
several O
previous O

works B-DAT
[18, O
19], O
and O
report O

the B-DAT
overall O
accu- O
racy O
over O
10 O
folds O

. B-DAT
AFEW O
8.0 O
[20] O
served O
as O

an B-DAT
evaluation O
platform O
for O
the O

annual B-DAT
EmotiW O
since O
2013. O
Seven O

emotion B-DAT
labels O
are O
in- O
cluded O

in B-DAT
AFEW, O
i.e. O
anger, O
disgust O

, B-DAT
fear, O
happiness, O
sadness, O
surprise O

and B-DAT
neutral. O
AFEW O
contains O
video O

clips B-DAT
collected O
from O
different O
movies O

and B-DAT
TV O
serials O
with O
spontaneous O

ex- B-DAT
pressions, O
various O
head O
poses, O

occlusions, B-DAT
and O
illuminations. O
AFEW O
8.0 O

is B-DAT
divided O
into O
three O
splits: O

Train B-DAT
(773 O
samples), O
Val O
(383 O

samples) B-DAT
and O
Test O
(653 O
samples), O

which B-DAT
ensures O
data O
in O
the O

three B-DAT
sets O
belong O
to O
mutually O

exclusive B-DAT
movies O
and O
ac- O
tors. O

Since B-DAT
the O
test O
split O
is O

not B-DAT
publicly O
available, O
we O
train O

our B-DAT
model O
on O
training O
split O

and B-DAT
report O
results O
on O
validation O

split. B-DAT
Implementation O
details. O
We O
preprocess O
video O

frames B-DAT
by O
face O
detection O
and O

alignment B-DAT
in O
the O
Dlib O
toolbox O

We B-DAT
extend O
the O
face O
bounding O

box B-DAT
with O
a O
ratio O
of O

25% B-DAT
and O
then O
resize O
the O

cropped B-DAT
faces O
to O
scale O
of O

224×224. B-DAT
We O
implement O
our O
method O

by B-DAT
the O
Pytorch O
toolbox. O
By O

default, B-DAT
for O
feature O
em- O
bedding O

, B-DAT
we O
use O
the O
ResNet18 O

which B-DAT
is O
pre-trained O
on O
MS- O

Celeb-1M B-DAT
[21] O
face O
recognition O
dataset O

and B-DAT
FER O
Plus O
expres- O
sion O

dataset B-DAT
[22]. O
For O
training, O
on O

both B-DAT
CK+ O
and O
AFEW O
8.0, O

we B-DAT
set O
a O
batch O
to O

have B-DAT
48 O
instances O
with O
K O

frames B-DAT
in O
each O
instance. O
For O

frame B-DAT
sampling O
in O
a O
video, O

we B-DAT
first O
split O
the O
video O

into B-DAT
K O
segments O
and O
then O

randomly B-DAT
select O
one O
frame O
from O

each B-DAT
segment. O
By O
default, O
we O

set B-DAT
K O
to O
3. O
We O

use B-DAT
the O
SGD O
method O
for O

optimization B-DAT
with O
a O
momentum O
of O
0 O

.9 B-DAT
and O
a O
weight O
decay O

of B-DAT
10−4. O
On O
CK+, O
we O

initialize B-DAT
the O
learning O
rate O
(lr) O

to B-DAT
0.1, O
and O
modify O
it O

to B-DAT
0.02 O
at O
30 O
epochs, O

and B-DAT
stop O
training O
after O
60 O

epochs. B-DAT
On O
AFEW O
8.0, O
we O

initialize B-DAT
the O
lr O
to O
4e-6, O

and B-DAT
modify O
it O
to O
8e-7 O

at B-DAT
60 O
epochs O
and O
1.6e-7 O

at B-DAT
120 O
epochs, O
and O
stop O

training B-DAT
after O
180 O
epochs. O
3.2. O
Evaluation O
on O
CK O

+ B-DAT
We O
evaluate O
our O
FAN O
on O

CK+ B-DAT
with O
comparisons O
to O
several O

state-of-the-art B-DAT
methods O
in O
Table O
1 O

. B-DAT
On O
CK+, O
due O
to O

the B-DAT
fact O
that O
the O
videos O

show B-DAT
a O
shift O
from O
a O

neutral B-DAT
facial O
expression O
to O
the O

peak B-DAT
expression, O
most O
of O
the O

methods B-DAT
conduct O
data O
selection O
manually. O

Zhang B-DAT
et O
al O
[23] O
propose O

to B-DAT
combine O
a O
spatial O
CNN O

model B-DAT
and O
a O
temporal O
network, O

where B-DAT
the O
spatial O
CNN O
model O

only B-DAT
uses O
the O
last O
peak O

frame. B-DAT
Jung O
et O
al O
[12] O

select B-DAT
a O
fixed O
length O
sequence O

for B-DAT
each O
video O
with O
a O

lipreading B-DAT
method O
[26], O
and O
jointly O

fine-tune B-DAT
a O
deep O
temporal O
Table O
1. O
Evaluation O
of O
our O

FAN B-DAT
with O
a O
comparison O
to O

state- B-DAT
of-the-art O
methods O
on O
CK O

+ B-DAT
database. O
Note O
that O
only O

those B-DAT
methods O
evaluated O
with O
7 O

classes B-DAT
are O
included. O
Method O
Training O
data O
Test O
data O

Acc B-DAT

. B-DAT
ST O
network O
[23] O
S: O
the O

last B-DAT
frame O
T: O
all O
frames O

S: B-DAT
the O
last O
frame O
T: O

all B-DAT
frames O
98.50 O
DTAGN O
[12] O
Fixed O
length O
Fixed O

length B-DAT
97.25 O

CNN+Island B-DAT
loss O
[24] O
The O
last O
three O
frames O
and O

the B-DAT
first O
frame O

The B-DAT
last O
three O
frames O
and O

the B-DAT
first O
frame O
94.35 O

LOMo B-DAT
[25] O
All O
frames O
All O

frames B-DAT
92.00 O
Score O
fusion O
(baseline) O
All O
frames O

All B-DAT
frames O
94.80 O

FAN(w/o B-DAT
Relation- O
attention) O
All O
frames O
All O
frames O
99.08 O

FAN B-DAT
All O
frames O
All O
frames O
99 O

.69 B-DAT
appearance-geometry O
network. O
Cai O
et O
al O

[24] B-DAT
select O
the O
last O
three O

frames B-DAT
and O
the O
first O
frame O

for B-DAT
each O
video, O
and O
train O

CNN B-DAT
models O
with O
a O
new O

Island B-DAT
loss O
function. O
We O
argue O

that B-DAT
manual O
data O
selection O
is O

an B-DAT
ad-hoc O
operation O
on O
CK O

+ B-DAT
and O
it O
is O
impractical O

since B-DAT
we O
can O
not O
know O

which B-DAT
is O
the O
peak O
frame O

beforeahead. B-DAT
Sikka O
et O
al O
[25] O

use B-DAT
all O
frames O
with O
a O

new B-DAT
latent O
ordinal O
model O
which O

extracts B-DAT
CNN/LBP/SIFT O
features O
for O
sub-event O

detection B-DAT
and O
uses O
multi-instance O
SVM O

for B-DAT
ex- O
pression O
classification. O
Our O

baseline B-DAT
method O
uses O
ResNet18 O
to O

generate B-DAT
scores O
for O
individual O
frame O

and B-DAT
applies O
score O
fu- O
sion O
( O

summation) B-DAT
for O
all O
frames. O
It O

achieves B-DAT
94.8% O
which O
is O
2.8% O

better B-DAT
than O
[25]. O
Our O
proposed O

FAN B-DAT
with O
only O
self- O
attention O

gets B-DAT
99.08% O
which O
significantly O
boosts O

the B-DAT
baseline O
by O
4.28%. O
Adding O

relation-attention B-DAT
weights O
further O
im- O
proves O

the B-DAT
accuracy O
to O
99.69% O
which O

sets B-DAT
up O
a O
new O
state O

of B-DAT
the O
art O
on O
CK+. O
3.3. O
Evaluation O
on O
AFEW O
8.0 O

From B-DAT
the O
view O
of O
performance, O

AFEW B-DAT
is O
one O
of O
the O

most B-DAT
challenging O
videos O
FER O
dataset. O

The B-DAT
EmotiW O
challenge O
shares O
the O

same B-DAT
data O
from O
2016 O
to O
2018 O

. B-DAT
Table O
2 O
presents O
the O

evaluation B-DAT
of O
our O
FAN O
on O

AFEW B-DAT
with O
comparisons O
to O
recent O

state-of-the-art B-DAT
methods. O
For O
a O
fair O

comparison, B-DAT
we O
only O
list O
these O

results B-DAT
obtained O
by O
the O
best O

single B-DAT
models O
in O
previous O
works. O

From B-DAT
the O
last O
three O
rows O

of B-DAT
Table O
2, O
our O
proposed O

FAN B-DAT
improves O
the O
baseline O
by O
2 O

.36%. B-DAT
Both O
[27] O
and O
[10] O

use B-DAT
VGGFace O
backbone O
and O
a O

recurrent B-DAT
model O
with O
long-short-term O
memory O

units. B-DAT
These O
methods O
aim O
to O

capture B-DAT
temporal O
dynamic O
information O
for O

videos. B-DAT
Most O
of O
the O
meth- O

ods B-DAT
focus O
on O
improving O
static O

face B-DAT
based O
CNN O
models O

Table B-DAT
2. O
Evaluation O
of O
our O

FAN B-DAT
with O
a O
comparison O
to O

state- B-DAT
of-the-art O
methods O
on O
AFEW O
8 O

.0 B-DAT
database. O
It O
is O
worth O

noting B-DAT
that O
we O
only O
compare O

to B-DAT
the O
best O
single O
models O

of B-DAT
previous O
works. O
Method O
Model O
type O
Accuracy O

CNN-RNN B-DAT
(2016) O
[27] O
Dynamic O
45.43 O

VGGFace B-DAT
+ O
Undirectional O
LSTM O
(2017 O

) B-DAT
[10] O
Dynamic O
48.60 O
HoloNet O
(2016) O
[28] O
Static O
44.57 O

DSN-HoloNet B-DAT
(2017) O
[29] O
Static O
46.47 O

DenseNet-161 B-DAT
(2018) O
[31] O
Static O
51.44 O

DSN-VGGFace B-DAT
(2018) O
[30] O
Static O
48.04 O
Score O
fusion O
(baseline) O
Static O
48.82 O

FAN B-DAT
w/o O
Relation-attention O
Static O
50.92 O

FAN B-DAT
Static O
51.18 O
combine O
scores O
for O
video-level O
FER O

. B-DAT
Both O
[28] O
and O
[29] O

input B-DAT
two O
LBP O
maps O
and O

a B-DAT
gray O
image O
for O
CNN O

models. B-DAT
Deeply- O
supervised O
networks O
are O

used B-DAT
in O
[29] O
and O
[30], O

which B-DAT
add O
supervision O
on O
intermediate O

layers. B-DAT
For O
static O
methods, O
[31] O

gets B-DAT
slightly O
better O
performance O
than O

ours. B-DAT
However, O
[31] O
uses O
DenseNet-161 O

and B-DAT
pretrains O
it O
on O
both O

large-scale B-DAT
face O
datasets O
and O
their O

own B-DAT
Situ O
emotion O
video O
dataset. O

Addition- B-DAT
ally, O
[31] O
applies O
complicated O

post-processing B-DAT
which O
extracts O
frame O
features O

and B-DAT
compute O
their O
mean O
vector, O

max-pooled B-DAT
vector, O
and O
standard O
deviation O

vector. B-DAT
These O
vectors O
are O
then O

concatenated B-DAT
and O
finally O
fed O
into O

an B-DAT
SVM O
classifier. O
Overall, O
our O

FAN B-DAT
improves O
the O
baseline O
significantly O

and B-DAT
achieves O
performance O
comparable O
to O

that B-DAT
of O
the O
best O
previous O

single B-DAT
model. O
3.4. O
Visualization O
and O
Hyper-parameters O

To B-DAT
better O
understand O
the O
self-attention O

and B-DAT
relation-attention O
modules O
in O
our O

FAN, B-DAT
we O
visualize O
the O
attention O

weights B-DAT
in O
Figure O
2. O
Figure O
2 O
shows O
one O
sequence O
for O
each O

category B-DAT
with O
blue O
and O
orange O

weight B-DAT
bars, O
where O
blue O
bars O

represent B-DAT
the O
self-attention O
weights O
(i.e O

. B-DAT
α O
in O
Eq. O
(1)) O

of B-DAT
our O
FAN O
w/o O
relation-attention O

and B-DAT
orange O
bars O
the O
final O

weights B-DAT
(i.e. O
αβ O
in O
Eq. O
(4 O

)) B-DAT
of O
our O
FAN. O
In O

total, B-DAT
both O
kinds O
of O
weights O

can B-DAT
reflect O
the O
importance O
of O

frames. B-DAT
Comparing O
the O
blue O
and O

orange B-DAT
bars, O
we O
find O
that O

the B-DAT
final O
weights O
of O
our O

FAN B-DAT
can O
always O
assign O
higher O

weights B-DAT
to O
the O
more O
obvious O

face B-DAT
frames, O
while O
self-attention O
module O

could B-DAT
assign O
high O
weights O
on O

some B-DAT
ob- O
scure O
face O
frames, O

see B-DAT
the O
1st, O
2th, O
and O

3rd B-DAT
rows O
of O
Figure O
2 O
( O

left). B-DAT
This O
explicitly O
explains O
why O

adding B-DAT
relation-attention O
boost O
performance. O
Evaluation O
of O
Hyper-parameters. O
We O
evaluate O

two B-DAT
hyper-parameters O
of O
our O
FAN O

on B-DAT
CK+, O
i.e. O
backbone O
CNN O

networks B-DAT
and O
the O
parameter O
K O

mentioned B-DAT
in O
implementation O
details, O
to O

validate B-DAT
the O
robustness O
of O
our O

method. B-DAT
For O
the O
parameter O
K O

, B-DAT
besides O
the O
default O
value, O

we B-DAT
try O
several O
other O
values, O

i.e. B-DAT
{2, O
5, O
8}, O
and O

find B-DAT
the O
performance O
is O
not O

sensitive B-DAT
to O
K. O
Specifically, O
our O

FAN B-DAT
obtains O
99.39% O
with O
K={2, O
5 O

}. B-DAT
and O
gets O
99.69% O
with O

K=8. B-DAT
Since O
the O
default O
value, O

K=3 B-DAT
Fig. O
2. O
Visualization O
of O
the O

self-attention B-DAT
weights O
(blue O
bar) O
and O

the B-DAT
final O
weights O
of O
FAN O

(orange B-DAT
bar) O
on O
CK+ O
dataset O

. B-DAT
Fig. O
3. O
Evaluation O
of O
backbone O

CNN B-DAT
models O
and O
training O
strategies O

on B-DAT
CK O

+. B-DAT
gets O
99.69%, O
we O
use O
this O

default B-DAT
setting O
in O
the O
remainder O

of B-DAT
this O
paper O

. B-DAT
For O
the O
backbone O
CNN O
model O

evaluation, B-DAT
we O
try O
the O
VGGFace O

model B-DAT
which O
is O
widely-used O
in O

previous B-DAT
works. O
Similarly, O
we O
also O

pretrain B-DAT
the O
VGGFace O
model O
on O

the B-DAT
FER- O
Plus O
dataset. O
Since O

[5] B-DAT
shows O
that O
it O
is O

better B-DAT
to O
freeze O
all O
the O

feature B-DAT
learning O
layers O
after O
pretrained O

on B-DAT
FERPlus O
for O
VGGFace O
model O

, B-DAT
we O
also O
conduct O
the O

same B-DAT
experiment O
on O
CK+ O
with O

VGGFace. B-DAT
Figure O
3 O
shows O
the O

default B-DAT
com- O
parisons O
with O
different O

backbone B-DAT
CNN O
models. O
On O
CK+, O

compared B-DAT
with O
freezing O
all O
the O

feature B-DAT
layers O
for O
VGGFace, O
it O

gets B-DAT
better O
results O
with O
fine-tuning O

all B-DAT
layers O
which O
may O
be O

explained B-DAT
by O
the O
domain O
discrepancy O

between B-DAT
FERPlus O
and O
CK+. O
Overall, O

the B-DAT
results O
are O
significantly O
improved O

by B-DAT
self-attention O
weights O
and O
further O

improved B-DAT
by O
the O
relation- O
attention O

weights. B-DAT
4. O
CONCLUSION O

We B-DAT
propose O
Frame O
Attention O
Networks O

for B-DAT
video-based O
facial O
expression O
recognition. O

The B-DAT
FAN O
contains O
a O
self-attention O

module B-DAT
and O
a O
relation-attention O
module. O

The B-DAT
experiments O
on O
CK+ O
and O

AFEW B-DAT
show O
that O
our O
FAN O

with B-DAT
only O
self-attention O
improves O
the O

baseline B-DAT
significantly O
and O
adding O
relation- O

attention B-DAT
further O
boosts O
performance. O
With O

a B-DAT
visualization O
on O
CK+, O
we O

demonstrate B-DAT
that O
our O
FAN O
can O

automatically B-DAT
capture O
the O
importance O
of O

frames. B-DAT
Our O
single O
model O
achieves O

performance B-DAT
on O
par O
with O
that O

of B-DAT
state-of-the-art O
methods O
on O
AFEW O

and B-DAT
obtains O
state-of-the-art O
results O
on O

5. B-DAT
REFERENCES O
[1] O
Gwen O
Littlewort, O
Marian O
Stewart O

Bartlett, B-DAT
Ian O
Fasel, O
Joshua O
Susskind O

, B-DAT
and O
Javier O
Movellan, O
“Dynamics O

of B-DAT
facial O
expression O
extracted O
automatically O

from B-DAT
video,” O
in O
IVC, O
2006. O
[2] O
Caifeng O
Shan, O
Shaogang O
Gong O

, B-DAT
and O
Peter O
W. O

Mcowan, B-DAT
“Fa- O
cial O
expression O
recognition O

based B-DAT
on O
local O
binary O
patterns: O

A B-DAT
comprehensive O
study,” O
in O
IVC, O
2009 O

. B-DAT
[3] O
Yichuan O
Tang, O
“Deep O
learning O

using B-DAT
linear O
support O
vector O
ma O

- B-DAT
chines,” O
in O
CS, O
2013. O
[4] O
Sarah O
Adel O
Bargal, O
Emad O

Barsoum, B-DAT
Cristian O
Canton O
Ferrer, O
and O

Cha B-DAT
Zhang, O
“Emotion O
recognition O
in O

the B-DAT
wild O
from O
videos O
using O

images,” B-DAT
in O
ACM O
ICMI, O
2016 O

. B-DAT
[5] O
Boris O
Knyazev, O
Roman O
Shvetsov O

, B-DAT
Natalia O
Efremova, O
and O
Artem O

Kuharenko, B-DAT
“Convolutional O
neural O
networks O
pretrained O

on B-DAT
large O
face O
recognition O
datasets O

for B-DAT
emotion O
classification O
from O
video,” O

in B-DAT
ACM O
ICMI, O
2017. O
[6] O
Sepp O
Hochreiter O
and O
Jrgen O

Schmidhuber, B-DAT
“Long O
short-term O
memory,” O
Neural O

Computation, B-DAT
1997 O

. B-DAT
[7] O
Du O
Tran, O
Lubomir O
Bourdev O

, B-DAT
Rob O
Fergus, O
Lorenzo O
Torresani, O

and B-DAT
Manohar O
Paluri, O
“Learning O
spatiotemporal O

features B-DAT
with O
3d O
convolutional O
networks,” O

in B-DAT
ICCV, O
2015. O
[8] O
Yuanliu O
Liu, O
Yuanliu O
Liu O

, B-DAT
Yuanliu O
Liu, O
and O
Yuanliu O

Liu, B-DAT
“Video-based O
emotion O
recognition O
using O

cnn-rnn B-DAT
and O
c3d O
hy- O
brid O

networks,” B-DAT
in O
ACM O
ICMI, O
2016. O
[9] O
Xi O
Ouyang, O
Shigenori O
Kawaai O

, B-DAT
Ester O
Gue O
Hua O
Goh, O

Sheng- B-DAT
mei O
Shen, O
Wan O
Ding, O

Huaiping B-DAT
Ming, O
and O
Dong-Yan O

Huang, B-DAT
“Audio-visual O
emotion O
recognition O
using O

deep B-DAT
transfer O
learning O
and O
multiple O

temporal B-DAT
models,” O
in O
ACM O
ICMI, O
2017 O

. B-DAT
[10] O
Valentin O
Vielzeuf, O
Stphane O
Pateux O

, B-DAT
and O
Frdric O
Jurie, O
“Temporal O

multimodal B-DAT
fusion O
for O
video O
emotion O

classification B-DAT
in O
the O
wild,” O
in O

ACM B-DAT
ICMI, O
2017. O
[11] O
Jingwei O
Yan, O
Wenming O
Zheng O

, B-DAT
Zhen O
Cui, O
Chuangao O
Tang, O

Tong B-DAT
Zhang, O
and O
Yuan O

Zong, B-DAT
“Multi-cue O
fusion O
for O
emotion O

recognition B-DAT
in O
the O
wild,” O
Neurocomputing, O
2018 O

. B-DAT
[12] O
Heechul O
Jung, O
Sihaeng O
Lee O

, B-DAT
Junho O
Yim, O
Sunjeong O
Park, O

and B-DAT
Junmo O
Kim, O
“Joint O
fine-tuning O

in B-DAT
deep O
neural O
networks O
for O

facial B-DAT
expression O
recognition,” O
in O
ICCV, O
2015 O

. B-DAT
[13] O
Samira O
Ebrahimi O
Kahou, O
Christopher O

Pal, B-DAT
Xavier O
Bouthillier, O
Pierre O
Froumenty O

, B-DAT
Roland O
Memisevic, O
Pascal O
Vincent, O

Aaron B-DAT
Courville, O
Yoshua O
Bengio, O
and O

Raul B-DAT
Chandias O
Ferrari, O
“Com- O
bining O

modality B-DAT
specific O
deep O
neural O
networks O

for B-DAT
emotion O
recognition O
in O
video,” O

in B-DAT
ACM O
ICMI, O
2013. O
[14] O
Ashish O
Vaswani, O
Noam O
Shazeer O

, B-DAT
Niki O
Parmar, O
Jakob O
Uszko- O

reit, B-DAT
Llion O
Jones, O
Aidan O
N O

Gomez, B-DAT
Łukasz O
Kaiser, O
and O
Illia O

Polosukhin, B-DAT
“Attention O
is O
all O
you O

need,” B-DAT
in O
NIPS, O
2017. O
[15] O
Jiaolong O
Yang, O
Peiran O
Ren O

, B-DAT
Dongqing O
Zhang, O
Dong O
Chen, O

Fang B-DAT
Wen, O
Hongdong O
Li, O
and O

Gang B-DAT
Hua, O
“Neural O
aggregation O
network O

for B-DAT
video O
face O
recognition.,” O
in O

CVPR, B-DAT
2017. O
[16] O
Flood O
Sung O
Yongxin O
Yang O

, B-DAT
Li O
Zhang, O
Tao O
Xiang, O

Philip B-DAT
HS O
Torr, O
and O
Timothy O

M B-DAT
Hospedales, O
“Learning O
to O
compare: O

Re- B-DAT
lation O
network O
for O
few-shot O

learning,” B-DAT
in O
CVPR, O
2018. O
[17] O
Patrick O
Lucey, O
Jeffrey O
F O

Cohn, B-DAT
Takeo O
Kanade, O
Jason O
Saragih O

, B-DAT
Zara O
Ambadar, O
and O
Iain O

Matthews, B-DAT
“The O
extended O
cohn- O
kanade O

dataset B-DAT
(ck+): O
A O
complete O
dataset O

for B-DAT
action O
unit O
and O
emotion-specified O

expression,” B-DAT
in O
CVPRW, O
2010. O
[18] O
Mengyi O
Liu, O
Shiguang O
Shan O

, B-DAT
Ruiping O
Wang, O
and O
Xilin O

Chen, B-DAT
“Learning O
expressionlets O
on O
spatio-temporal O

manifold B-DAT
for O
dy- O
namic O
facial O

expression B-DAT
recognition,” O
in O
CVPR, O
2014. O
[19] O
Chieh-Ming O
Kuo, O
Shang-Hong O
Lai O

, B-DAT
and O
Michel O
Sarkis, O
“A O

compact B-DAT
deep O
learning O
model O
for O

robust B-DAT
facial O
expression O
recognition,” O
in O

CVPRW, B-DAT
2018. O
[20] O
Abhinav O
Dhall, O
Amanjot O
Kaur O

, B-DAT
Roland O
Goecke, O
and O
Tom O

Gedeon, B-DAT
“Emotiw O
2018: O
Audio-video, O
student O

engagement B-DAT
and O
group-level O
affect O
prediction,” O

arXiv B-DAT
preprint:1808.07773, O
2018. O
[21] O
Yandong O
Guo, O
Lei O
Zhang O

, B-DAT
Yuxiao O
Hu, O
Xiaodong O
He, O

and B-DAT
Jian- O
feng O
Gao, O
“Ms-celeb-1m: O

A B-DAT
dataset O
and O
benchmark O
for O

large- B-DAT
scale O
face O
recognition,” O
in O

ECCV, B-DAT
2016. O
[22] O
Emad O
Barsoum, O
Cha O
Zhang O

, B-DAT
Cristian O
Canton O
Ferrer, O
and O

Zhengyou B-DAT
Zhang, O
“Training O
deep O
networks O

for B-DAT
facial O
expres- O
sion O
recognition O

with B-DAT
crowd-sourced O
label O
distribution,” O
in O

ACM B-DAT
ICMI, O
2016. O
[23] O
Kaihao O
Zhang, O
Yongzhen O
Huang O

, B-DAT
Yong O
Du, O
and O
Liang O

Wang, B-DAT
“Facial O
expression O
recognition O
based O

on B-DAT
deep O
evolutional O
spatial-temporal O
networks,” O

IEEE B-DAT
TIP, O
2017. O
[24] O
Jie O
Cai, O
Zibo O
Meng O

, B-DAT
Ahmed O
Shehab O
Khan, O
Zhiyuan O

Li, B-DAT
James O
OReilly, O
and O
Yan O

Tong, B-DAT
“Island O
loss O
for O
learning O

discriminative B-DAT
features O
in O
facial O
expression O

recognition,” B-DAT
in O
FG, O
2018. O
[25] O
Karan O
Sikka, O
Gaurav O
Sharma O

, B-DAT
and O
Marian O
Bartlett, O
“Lomo: O

Latent B-DAT
ordinal O
model O
for O
facial O

analysis B-DAT
in O
videos,” O
in O
CVPR, O
2016 O

. B-DAT
[26] O
Ziheng O
Zhou, O
Guoying O
Zhao O

, B-DAT
and O
M. O
Pietikainen, O
“Towards O

a B-DAT
practical O
lipreading O
system,” O
in O

CVPR, B-DAT
2011. O
[27] O
Yin O
Fan, O
Xiangju O
Lu O

, B-DAT
Dian O
Li, O
and O
Yuanliu O

Liu, B-DAT
“Video-based O
emotion O
recognition O
using O

cnn-rnn B-DAT
and O
c3d O
hybrid O
networks,” O

in B-DAT
ACM O
ICMI, O
2016. O
[28] O
Anbang O
Yao, O
Dongqi O
Cai O

, B-DAT
Ping O
Hu, O
Shandong O
Wang, O

Liang B-DAT
Sha, O
and O
Yurong O

Chen, B-DAT
“Holonet: O
towards O
robust O
emotion O

recognition B-DAT
in O
the O
wild,” O
in O

ACM B-DAT
ICMI, O
2016. O
[29] O
Ping O
Hu, O
Dongqi O
Cai O

, B-DAT
Shandong O
Wang, O
Anbang O
Yao, O

and B-DAT
Yurong O
Chen, O
“Learning O
supervised O

scoring B-DAT
ensemble O
for O
emo- O
tion O

recognition B-DAT
in O
the O
wild,” O
in O

ACM B-DAT
ICMI, O
2017. O
[30] O
Yingruo O
Fan, O
Jacqueline O
CK O

Lam, B-DAT
and O
Victor O
OK O
Li O

, B-DAT
“Video- O
based O
emotion O
recognition O

using B-DAT
deeply-supervised O
neural O
net- O
works,” O

in B-DAT
ACM O
ICMI, O
2018. O
[31] O
Chuanhe O
Liu, O
Tianhao O
Tang O

, B-DAT
Kui O
Lv, O
and O
Minghao O

Wang, B-DAT
“Multi-feature O
based O
emotion O
recognition O

for B-DAT
video O
clips,” O
in O
ACM O

ICMI, B-DAT
2018 O

1 B-DAT
Introduction O

2 B-DAT
Frame O
Attention O
Networks O

3 B-DAT
Experiments O

3.1 B-DAT
Datasets O
and O
Implementation O
Details O

3.2 B-DAT
Evaluation O
on O
CK O

3.3 B-DAT
Evaluation O
on O
AFEW O
8.0 O

3.4 B-DAT
Visualization O
and O
Hyper-parameters O

4 B-DAT
Conclusion O

5 B-DAT
References O

ABSTRACT O
The B-DAT
video-based O
facial O
expression O
recognition O
aims O

frames O
in O
an O
end-to-end O
framework. O
The B-DAT
network O
takes O
a O
video O
with O

and O
produces O
a O
fixed-dimension O
representation. O
The B-DAT
whole O
network O
is O
com- O
posed O

of O
two O
modules. O
The B-DAT
feature O
embedding O
module O
is O
a O

face O
images O
into O
feature O
vectors. O
The B-DAT
frame O
attention O
module O
learns O
multiple O

or O
motion O
information O
in O
videos. O
The B-DAT
Long O
Short-Term O
Mem- O
ory O
(LSTM O

to O
adaptively O
aggregate O
frame O
features. O
The B-DAT
FAN O
is O
designed O
to O
learn O

reasoning O
in O
an O
end-to-end O
fashion. O
The B-DAT
self-attention O
kernels O
are O
directly O
learned O

fixed-dimension O
feature O
representation O
for O
FER. O
The B-DAT
whole O
network O
consists O
of O
two O

module O
and O
frame O
attention O
module. O
The B-DAT
feature O
embedding O
module O
is O
a O

image O
into O
a O
feature O
vector. O
The B-DAT
frame O
attention O
module O
learns O
two-level O

local O
features O
is O
more O
reliable. O
The B-DAT
self-attention O
weights O
are O
learned O
with O

relation-attention O
weights O
for O
frame O
features. O
The B-DAT
relation-attention O
weight O
of O
the O
i-th O

The B-DAT
last O
three O
frames O
and O
the O

The B-DAT
last O
three O
frames O
and O
the O

most O
challenging O
videos O
FER O
dataset. O
The B-DAT
EmotiW O
challenge O
shares O
the O
same O

for O
video-based O
facial O
expression O
recognition. O
The B-DAT
FAN O
contains O
a O
self-attention O
module O

and O
a O
relation-attention O
module. O
The B-DAT
experiments O
on O
CK+ O
and O
AFEW O

The B-DAT
extended O
cohn- O
kanade O
dataset O
(ck O

We O
conduct O
extensive O
experiments O
on O
CK B-DAT

and O
achieves O
state-of-the-art O
performance O
on O
CK B-DAT

We O
conduct O
extensive O
experiments O
on O
CK B-DAT

achieves O
state-of-the- O
art O
performance O
on O
CK B-DAT

CK B-DAT

happiness, O
sadness, O
and O
surprise. O
Since O
CK B-DAT

22]. O
For O
training, O
on O
both O
CK B-DAT

weight O
decay O
of O
10−4. O
On O
CK B-DAT

3.2. O
Evaluation O
on O
CK B-DAT

We O
evaluate O
our O
FAN O
on O
CK B-DAT

methods O
in O
Table O
1. O
On O
CK B-DAT

to O
state- O
of-the-art O
methods O
on O
CK B-DAT

is O
an O
ad-hoc O
operation O
on O
CK B-DAT

state O
of O
the O
art O
on O
CK B-DAT

hyper-parameters O
of O
our O
FAN O
on O
CK B-DAT

of O
FAN O
(orange O
bar) O
on O
CK B-DAT

models O
and O
training O
strategies O
on O
CK B-DAT

conduct O
the O
same O
experiment O
on O
CK B-DAT

different O
backbone O
CNN O
models. O
On O
CK B-DAT

domain O
discrepancy O
between O
FERPlus O
and O
CK B-DAT

relation-attention O
module. O
The O
experiments O
on O
CK B-DAT

performance. O
With O
a O
visualization O
on O
CK B-DAT

and O
obtains O
state-of-the-art O
results O
on O
CK B-DAT

30] O
Yingruo O
Fan, O
Jacqueline O
CK B-DAT
Lam, O
and O
Victor O
OK O
Li O

Evaluation O
on O
CK B-DAT

third O
dataset O
we O
used. O
The O
street2shop B-DAT
dataset O
contains O
20,000 O
images O
under O

third O
dataset O
we O
used. O
The O
street2shop B-DAT

- B-DAT

- B-DAT
chitecture O
that O
when O
trained O
on O

- B-DAT
bedding O
of O
the O
image O
is O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tive O
filtering O
recommends O
products O
based O

- B-DAT
havior O
on O
the O
platform, O
but O

- B-DAT

- B-DAT
viously, O
but O
are O
noted O
to O

- B-DAT
lution O
here O
would O
be O
the O

- B-DAT

- B-DAT
cal O
embeddings O
giving O
the O
intensity O

- B-DAT

- B-DAT
trieve O
them O
in O
order O
of O

- B-DAT

- B-DAT
tensive O
evaluation O
has O
verified O
that O

- B-DAT
tance O
matrix O
instead O
of O
Euclidean O

- B-DAT
ity O
information, O
we O
can O
achieve O

- B-DAT
log O
similar O
to O
a O
user-uploaded O

- B-DAT
eral O
challenges O
in O
dealing O
with O

- B-DAT
mines O
whether O
a O
given O
set O

- B-DAT

- B-DAT
bedded O
in O
a O
multidimensional O
space O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age. O
In O
order O
to O
project O

- B-DAT

- B-DAT

- B-DAT
butions O
of O
this O
paper O
are O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ity O
between O
sample O
images O

- B-DAT
pare O
our O
proposed O
RankNet O
with O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ods O
for O
different O
datasets. O
The O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
proach, O
i.e O
collect O
an O
image O

- B-DAT
ity O
function O
which O
when O
given O

- B-DAT
lar O
images O
from O
the O
storage O

- B-DAT
age O
data O
in O
order O
to O

- B-DAT
proaches O
were O
little O
efficient O
and O

-11 B-DAT

- B-DAT
tures O
like O
SIFT O
and O
HOG O

- B-DAT
ited. O
Recently O
researchers O
who O
used O

- B-DAT
ral O
networks O
for O
object O
recognition O

- B-DAT
cess O
[15, O
16, O
17]. O
In O

- B-DAT
straction O
level. O
The O
descriptor O
vector O

- B-DAT
sual O
similarity, O
these O
descriptors O
are O

- B-DAT
sual O
similarity O
is O
a O
composite O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cause O
for O
a O
network O
need O

- B-DAT

- B-DAT
ilar O
depending O
on O
the O
ground O

- B-DAT
ity O
which O
we O
are O
addressing O

- B-DAT
nary O
(similar/dissimilar) O
fails O
the O
objective O

- B-DAT
grained O
visual O
similarity. O
Therefore O
in O

- B-DAT

- B-DAT

- B-DAT

to O
10 O
object O
classes O
namely O
- B-DAT
top, O
trouser, O
pullover, O
dress, O
coat O

- B-DAT
ing O
RankNet. O
CIFAR10 O
is O
an O

- B-DAT

- B-DAT
act O
matching O
products O
between O
the O

- B-DAT

- B-DAT
tive(n) O
images O
belong O
to O
the O

- B-DAT
lic O
URL O
links O
for O
those O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
dings O
of O
the O
data. O
The O

- B-DAT
volutional O
neural O
networks O
with O
shared O

- B-DAT
timized O
during O
training O
by O
minimizing O

- B-DAT
eter O
θ, O
x O
= O
f(I;θ O

- B-DAT
eters O
of O
the O
neural O
network O

- B-DAT
ner O
product O
layers. O
The O
number O

- B-DAT

- B-DAT

- B-DAT
ages O
apart. O
The O
network O
takes O

- B-DAT
ther O
apart O
then O
the O
network O

- B-DAT
beddings. O
There O
are O
Y O
layers O

- B-DAT

- B-DAT
tion O
function O
here O
rectified O
linear O

- B-DAT

- B-DAT
rameters O
of O
a O
parameterized O
function O

- B-DAT

- B-DAT
ther O
as O
compared O
to O
similar/positive O

- B-DAT
mensional O
embedding O
subspace O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cided O
empirically. O
When O
m O
is O

- B-DAT
hattan O
distance O
metric O
provides O
the O

- B-DAT
rithmic O
and O
indexing O
methods O
fail O

- B-DAT
tive. O
The O
basic O
concept O
of O

- B-DAT
defined O
as O
the O
contrast O
which O

- B-DAT
ality O
curse O
from O
an O
angle O

- B-DAT
parative O
performance O
is O
optimizing O
the O

- B-DAT
ther O
minimizing O
the O
Contrastive O
loss O

- B-DAT
served O
that O
directly O
optimizing O
a O

- B-DAT
task O
learning. O
Recently O
some O
work O

- B-DAT

- B-DAT

- B-DAT
tive O
to O
scale O
change. O
Other O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
hance O
conventional O
distance O
metric O
learning O

- B-DAT
coding O
it O
as O
a O
representation O

- B-DAT

- B-DAT
ative O
sample O
away O
from O
the O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
lated O
between O
the O
embeddings. O
Also O

- B-DAT

- B-DAT

- B-DAT
curately O
than O
the O
distance-based O
triplet O

- B-DAT
tation O
is O
broad O
and O
can O

- B-DAT
tion O

type O
of O
pair O
of O
images O
- B-DAT
(1) O
Positive O
Pair O
( O
Similar O

- B-DAT
ages O
) O
and O
(2) O
Negative O

- B-DAT
cally O
selected O
from O
the O
datset O

- B-DAT
rate O
or O
good O
at O
recalling O

- B-DAT
trieve O
the O
most O
reasonably O
alike O

- B-DAT
ilar O
to O
the O
query O
image O

- B-DAT

- B-DAT
est O
neighbors O
from O
the O
same O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
age O
whereas O
the O
latter O
refers O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
search, O
we O
retrieved O
the O
in-class O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
Net. O
In O
AlexNet O
and O
PatterNet O

- B-DAT

- B-DAT

- B-DAT
togram), O
and O
then O
after O
that O

- B-DAT
riculum O
learning O
where O
the O
positive O

- B-DAT

- B-DAT

- B-DAT
ages, O
therefore O
we O
used O
a O

- B-DAT
work O
that O
incorporates O
different O
levels O

- B-DAT
ious O
scales O
[26, O
27]. O
Deep O

- B-DAT
fication. O
The O
strong O
invariance O
encoded O

- B-DAT
ally O
grows O
higher O
towards O
the O

- B-DAT
variance O
makes O
it O
hard O
to O

- B-DAT

- B-DAT

- B-DAT
variance O
and O
capture O
the O
semantics O

- B-DAT
cause O
it O
has O
19 O
convolutional O

- B-DAT

- B-DAT
pler O
aspects O
like O
shapes, O
pattern O

- B-DAT
ent O
convolution O
neural O
networks O
instead O

- B-DAT
dependent O
of O
the O
other O
two O

- B-DAT
bined O
with O
a O
4096-dimensional O
linear O

- B-DAT

- B-DAT
ization. O
Final O
results O
show O
that O

- B-DAT

- B-DAT
ral O
networks O
on O
the O
image O

- B-DAT
sponsible O
for O
the O
result O
is O

- B-DAT

- B-DAT

- B-DAT
nal O
layer O
which O
allows O
the O

- B-DAT
works(CNN1 O
and O
CNN2) O
emphasis O
on O

- B-DAT

- B-DAT
tation O
details O
for O
training O
a O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
cerns O
were O
preventing O
and O
detecting O

- B-DAT
egy O
where O
we O
do O
not O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ageNet O
dataset O
and O
fine-tuned O
the O

- B-DAT

- B-DAT
mented O
with O
the O
learning O
rate O

- B-DAT
mentum O
so O
that O
the O
optimizer O

- B-DAT
vent O
the O
network O
from O
getting O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
just O
the O
learning O
rate O
accordingly O

- B-DAT

- B-DAT
gence. O
The O
architecture O
was O
implemented O

- B-DAT
come O
more O
robust O
and O
prevented O

- B-DAT
ployed O
dropout O
in O
our O
architecture O

- B-DAT
cause O
dropout O
not O
only O
prevents O

- B-DAT
tion. O
Therefore O
both O
image O
augmentation O

- B-DAT
sentially O
equivalent O
to O
prevent O
overfitting O

- B-DAT

- B-DAT
itored O
the O
number O
of O
layers O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
parameter O
tuning O
and O
we O
used O

- B-DAT

- B-DAT
tain O
the O
same O
number O
of O

- B-DAT
ing O
data. O
The O
similar O
distribution O

- B-DAT
ized O
performance O
of O
the O
model O

- B-DAT

- B-DAT

- B-DAT
bor O
embedding O
algorithm O
used O
for O

- B-DAT

- B-DAT
mented O
using O
Barnes-Hut O
approximations O
which O

- B-DAT

- B-DAT
ization, O
the O
different O
category O
of O

- B-DAT
mally O
in O
projecting O
similar O
images O

- B-DAT

RankNet O
for O
Street2Shop O
Test O
Data O
- B-DAT
catalog O
images O

-20 B-DAT
recall. O
The O
top-20 O
recall O
evaluation O

-20 B-DAT
recall O
it O
is O
calculated O
that O

- B-DAT
cent O
of O
the O
cases O
the O

- B-DAT

- B-DAT

-20 B-DAT
Recall O
% O
on O
Exact O
Street2Shop O

- B-DAT
scale O
convolutional O
neural O
network O
in O

- B-DAT

- B-DAT
tance O
between O
two O
data O
points O

- B-DAT

- B-DAT
bedding O
space, O
which O
outperforms O
the O

- B-DAT

- B-DAT

- B-DAT
tional O
Networks O
for O
Large-Scale O
Image O

- B-DAT
age O
Retrieval O
Methods O

- B-DAT
age O
matching O
using O
SIFT, O
SURF O

- B-DAT
formance O
comparison O
for O
distorted O
images O

- B-DAT
scale O
sketch-based O
image O
search, O
in O

- B-DAT

- B-DAT

- B-DAT
proved O
image O
search, O
in O
ACM O

- B-DAT

- B-DAT
tection O
for O
web O
image O
search O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
gus. O
2011. O
Learning O
invariance O
through O

- B-DAT
cation O
with O
Deep O
Convolutional O
Neural O

- B-DAT
works. O
In O
Proc. O
NIPS. O
11061114 O

- B-DAT
tional O
Networks O
for O
Large-Scale O
Image O

- B-DAT
houcke, O
and O
Andrew O
Rabinovich. O
2015 O

- B-DAT
MNIST: O
a O
Novel O
Image O
Dataset O

- B-DAT

- B-DAT
man. O
2010. O
Geodesic O
star O
convexity O

- B-DAT
mentation. O
In O
Proc. O
CVPR. O
31293136 O

- B-DAT

-48 B-DAT

- B-DAT
search O
Presentation: O
A O
Theoretical O
Foundation O

- B-DAT
rmsprop: O
Divide O
the O
gradient O
by O

- B-DAT
cent O
magnitude.” O
COURSERA: O
Neural O
networks O

- B-DAT
ing O
Research O
15.1 O
(2014): O
1929-1958 O

- B-DAT
tion O
approach O
Qi O
Quan O
et O

- B-DAT
lyzing O
the O
performance O
of O
multilayer O

- B-DAT
ject O
recognition.” O
European O
Conference O
on O

- B-DAT
2605 O

- B-DAT

- B-DAT
tures: O
Spatial O
pyramid O
matching O
for O

- B-DAT

- B-DAT
tures. O
In O
ICCV, O
volume O
2 O

- B-DAT
lem O
with O
convergence O
rate O
o(1/sqr(k O

- B-DAT

- B-DAT
age O
retrieval O
with O
compressed O
fisher O

- B-DAT
portance O
of O
initialization O
and O
momentum O

- B-DAT
variance O
through O
imitation. O
In O
CVPR O

- B-DAT
ilarity O
from O
flickr O
groups O
using O

- B-DAT
notation: O
learning O
to O
rank O
with O

- B-DAT

- B-DAT
chine O
intelligence O
28.12 O
(2006): O
2037-2041 O

- B-DAT

- B-DAT

- B-DAT
tures O
both O
fine O
and O
coarse O

- B-DAT
ing O
novel O
CNN O
architecture, O
called O

- B-DAT
ing O
levels O
of O
abstraction, O
we O

- B-DAT
proach O
performs O
as O
good O
as O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
els O
with O
only O
one O
third O

- B-DAT
cance O
of O
intermediate O
layers O
on O

- B-DAT
esis O
by O
checking O
the O
impact O

- B-DAT
Net, O
a O
mobile O
model O
(12 O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ers O
visual O
similarity O
at O
Fynd O

- B-DAT
trieval, O
Visual O
Search, O
Recommender O
Systems O

- B-DAT
traction, O
E-Commerce O

Introduction O
Instance-level-image B-DAT
retrieval O
(content O
based O
- O
CBIR) O
also O
known O
as O
visual O

- B-DAT

- B-DAT
mend O
users O
the O
products O
similar O

- B-DAT
sion O
(CR). O
For O
ecommerce, O
two O

- B-DAT
ple O
of O
a O
woman O
dress O

- B-DAT

- B-DAT
lar O
types O
(v O
neck, O
round O

- B-DAT

- B-DAT
uct O
image O
could O
be O
with O

- B-DAT

- B-DAT

- B-DAT
sists O
of O
a O
feature O
extractor O

- B-DAT
sual O
attributes O
needed O
for O
the O

- B-DAT

- B-DAT
ucts O
are O
always O
nearer. O
We O

- B-DAT
curacy. O
A O
deep O
CNN O
is O

- B-DAT
bust. O
Deep O
learning O
has O
achieved O

- B-DAT
ing O
features O
from O
a O
single O

- B-DAT
sification O
task, O
a O
deep O
CNN O

- B-DAT
ance O
which O
grows O
higher O
towards O

- B-DAT

- B-DAT

- B-DAT
straction O
from O
the O
image. O
They O

- B-DAT

- B-DAT

- B-DAT
plex O
relations O
required O
for O
visual O

- B-DAT
age O
in O
real-time O
at O
production O

- B-DAT

- B-DAT
dients, O
to O
eyes, O
nose, O
to O

- B-DAT
ers O
[6]. O
The O
lower O
first O

- B-DAT
tures. O
Since O
multiple O
level O
of O

- B-DAT
Net O
(Multi-Intermediate O
Layers O
Descriptors O
Net O

- B-DAT
gated O
this O
local O
features O
inspired O

- B-DAT

- B-DAT

- B-DAT
lar O
Street2shop O
clothing O
similarity O
data O

- B-DAT

- B-DAT

- B-DAT
ence O
latency. O
We O
used O
accuracy O

- B-DAT
imented O
multiple O
variants O
of O
MILDNet O

- B-DAT
off O
5.4% O
top O
test O
accuracy O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tributes O
and O
relative O
similarity O
is O

- B-DAT
turing O
one’s O
notion O
of O
visual O

- B-DAT
tecture O
of O
MILDNet O
with O
the O

- B-DAT
tion O
of O
this O
work O
is O

- B-DAT

- B-DAT
chitecture. O
Embeddings O
are O
tuned O
to O

- B-DAT
tive O
in O
the O
latent O
space O

- B-DAT
ies O
are O
done O
in O
the O

- B-DAT

- B-DAT
ways O
getting O
embeddings O
from O
passing O

- B-DAT
ding O
space. O
So, O
the O
main O

- B-DAT
tations, O
shape O
descriptors, O
and O
texture O

- B-DAT
ture O
in O
image O
patch) O
has O

- B-DAT
SIS O
[13] O
and O
local O
distance O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
pressive O
power O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ding O
[19], O
have O
also O
shown O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trieval O
in O
various O
prior O
works O

- B-DAT
formance O
lagged O
behind O
that O
of O

- B-DAT

- B-DAT
formation. O
Qualitative O
examples O
of O
retrieval O

- B-DAT
tures O
extracted O
from O
fully-connected O
layers O

- B-DAT
ied O
[15, O
20] O
which O
showed O

- B-DAT

- B-DAT

- B-DAT
derless O
Pooling O
(MOP) O
where O
different O

- B-DAT
connected O
layer O
is O
aggregated O
by O

- B-DAT

- B-DAT
scriptors O
obtained O
by O
the O
max O

- B-DAT

- B-DAT
image O
retrieval O
since O
local O
characteristics O

- B-DAT
ers O
have O
these O
local O
characteristics O

- B-DAT

- B-DAT

- B-DAT
sual O
features O
and O
high-level O
concepts O

- B-DAT

- B-DAT

- B-DAT
perimented O
on O
all O
intermediate O
layers O

- B-DAT
lected O
the O
best O
performant O
layer O

- B-DAT
scale O
image O
dataset O
because O
the O

- B-DAT
strated O
great O
results O
using O
a O

- B-DAT

- B-DAT

- B-DAT
lated O
to O
theirs O
but O
consists O

- B-DAT

- B-DAT
olution O
paths O
which O
demonstrated O
state-of-the O

- B-DAT
mantics. O
A O
triplet-based O
hinge O
loss O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
trastive O
loss O
function O
[27]. O
In O

- B-DAT
alog/shop O
images O
as O
matching O
images O

- B-DAT
ments O
we O
used O
triplet O
loss O

- B-DAT
based O
loss O
function, O
widely O
used O

- B-DAT

- B-DAT
ing O
ecommerce O
product O
based O
on O

- B-DAT
ative O
image O
(relatively O
dissimilar O
to O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ture O
where O
users O
can O
upload O

- B-DAT
alog. O
While O
catalog O
query O
image O

- B-DAT

- B-DAT
played O
to O
users O
in O
an O

- B-DAT
mentation O
we O
used O
wild O
query O

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ployment O
we O
used O
a O
mixture O

- B-DAT

- B-DAT
log O
images O
and O
exact O
street-to-shop O

- B-DAT

- B-DAT
tablished O
manually). O
The O
retrieval O
sets O

- B-DAT
ion O
clothing O
categories, O
but O
only O

- B-DAT
ously O
experimented O
and O
presented O
here O

- B-DAT
ages. O
Negative O
images, O
n O
of O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
class O
triplets O
in O
case O
of O

- B-DAT

- B-DAT
sion O
d, O
which O
is O
sufficient O

- B-DAT

- B-DAT
ture O
Extractor O
(CNN O
Architecture), O
2 O

- B-DAT

- B-DAT

- B-DAT
age O
semantics O
and O
encodes O
strong O

- B-DAT

-2012 B-DAT
dataset O
[31], O
which O
contains O
roughly O

- B-DAT
egories O

- B-DAT

- B-DAT
ance O

- B-DAT

- B-DAT
pendent O
CNNs: O
1. O
Convnet(AlexNet/VGG16/VGG19) O
pre O

- B-DAT
trained O
on O
ImageNet O
dataset O
2 O

- B-DAT
dings O
from O
them O
are O
passed O

- B-DAT

- B-DAT

- B-DAT
ate O
layers. O
As O
from O
many O

- B-DAT

- B-DAT
erage O
pooling O
to O
flatten O
the O

- B-DAT

then O
passed O
through O
an O
FC O
- B-DAT
Dropout O
- O
FC O
layer O
to O

- B-DAT
duce O
the O
model O
size O
to O

- B-DAT
vnet(VGG16/MobileNet) O
pretrained O
on O
ImageNet O
dataset O

This O
are O
passed O
through O
FC O
- B-DAT
dropout O
- O
FC O
layers O
to O

- B-DAT

- B-DAT
beddings O

- B-DAT
iments. O
Three O
of O
which O
are O

- B-DAT

- B-DAT
ants O
of O
our O
proposed O
network O

- B-DAT
ding O
space O

- B-DAT
mize O
the O
mapping O
of O
images O

- B-DAT

- B-DAT

- B-DAT
tor O
~q O
is O
always O
relatively O

- B-DAT
clidean O
distance. O
Let’s O
say O
that O

- B-DAT

- B-DAT
tion O
can O
be O
written O
as O

- B-DAT
tance. O
While O
for O
dissimilar O
images O

- B-DAT
lution. O
Contrastive O
Loss O
function O
have O

- B-DAT

- B-DAT
ally O
similar O
images O
from O
the O

- B-DAT
act O
Nearest O
Neighbour O
is O
O(n2 O

- B-DAT

- B-DAT
ing O
k O
items O
where O
value O

- B-DAT
tion O
dataset O
of O
25460 O
images O

- B-DAT
tive O
model O
architecture O
input O
size O

- B-DAT
mented O
using O
Keras’s O
real-time O
augmentator O

- B-DAT
erator O
class. O
We O
used O
following O

- B-DAT

- B-DAT
configured O
ML O
playground. O
We O
ran O

- B-DAT
ploy O
models O
at O
scale. O
Our O

- B-DAT
tectures O
or O
combination O
of O
losses O

- B-DAT
tures O
are O
presented O
here. O
For O

- B-DAT
ing O
results O
and O
plotting O
graph O

- B-DAT
racy O
and O
average O
inference O
time O

- B-DAT

- B-DAT

- B-DAT
vnet O
as O
VGG16 O
and O
2 O

- B-DAT
nections O

- B-DAT

- B-DAT
spired O
by O
[3 O

-512 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
chitecture O
with O
4 O
skip O
connections O

- B-DAT
centage O
of O
all O
the O
experiments O

- B-DAT
sists O
of O
triplets O
from O
Street2shop O

- B-DAT
ing O
potential O
for O
this O
task O

- B-DAT
chitecture O
as O
base O
CNN O
instead O

- B-DAT
curacy O
to O
89.60%. O
The O
training O

- B-DAT
els O

- B-DAT

- B-DAT

-512 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
tain O
the O
same O
number O
of O

- B-DAT
ing O
data. O
The O
similar O
distribution O

- B-DAT
ized O
performance O
of O
the O
model O

- B-DAT
tion O
and O
gradually O
added O
aggregated O

- B-DAT
Net O
(see O
Figure O
5. O
Table O

- B-DAT
ing O
ability O
and O
inference O
accuracy O

- B-DAT
prehensive O
we O
considered O
all O
the O

- B-DAT
tion. O
The O
2048-d O
embeddings O
are O

- B-DAT
SNE O
[36], O
which O
is O
a O

- B-DAT
ding O
algorithm. O
The O
visualization O
showed O

- B-DAT
cessfully O
shows O
understanding O
of O
both O

- B-DAT

- B-DAT

-512 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
nore O
backgrounds O
in O
the O
images O

- B-DAT
tion O
3.2), O
we O
used O
both O

- B-DAT

- B-DAT
shop O
dataset, O
and O
in-house O
automatically O

- B-DAT
age O
triplets O
from O
results O
of O

- B-DAT
larity O
model. O
Here O
we O
will O

- B-DAT
ommendation O
Model O

- B-DAT
sification O
models O
from O
our O
in-house O

- B-DAT
fication O
task, O
we O
chose O
the O

- B-DAT

- B-DAT
Net O

- B-DAT
trained O
on O
InceptionV3 O
architecture. O
Feature O

- B-DAT
tionV3 O
architecture. O
Feature O
vector O
from O

- B-DAT
ture O
vector O

- B-DAT
resenting O
pattern O
and O
540 O
features O

- B-DAT

- B-DAT
tures O
are O
extracted O
from O
pretrained O

- B-DAT
togram. O
Impact O
of O
these O
features O

- B-DAT
ture O
our O
notion O
of O
visual O

- B-DAT
pen. O
For O
this O
we O
decided O

- B-DAT
ery O
day. O
Our O
system O
is O

- B-DAT
cal O
components O

- B-DAT
zon O
ec2 O
instances O
to O
store O

- B-DAT

- B-DAT

- B-DAT
der+category O
keys O
and O
processed O
in O

- B-DAT
sible O
by O
processing O
the O
data O

- B-DAT

- B-DAT
Net O
with O
only O
512-d O
embedding O

- B-DAT

- B-DAT
beddings O
by O
changing O
the O
number O

- B-DAT
ture O
the O
notion O
of O
visual O

- B-DAT

- B-DAT

- B-DAT
racy O
and O
recall O
while O
reducing O

- B-DAT
tially O
adding O
each O
skip O
connection O

- B-DAT

- B-DAT

- B-DAT
tire O
production O
pipeline O
which O
caters O

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT
ploiting O
local O
features O
from O
deep O

-61 B-DAT

- B-DAT

- B-DAT
sion O
and O
Pattern O
Recognition O
Workshops O

- B-DAT

- B-DAT
tation O
for O
large- O
scale O
image-based O

- B-DAT
volutional O
features O
for O
image O
retrieval O

- B-DAT

- B-DAT
tures. O
In O
ICCV, O
volume O
2 O

- B-DAT
cation O
using O
local O
distance O
functions O

- B-DAT
proach O
to O
object O
matching O
in O

- B-DAT

- B-DAT

- B-DAT
scale O
image O
retrieval O
with O
compressed O

- B-DAT

- B-DAT
tern O
Recognition, O
CVPR, O
pages O
33843391 O

European O
Conference O
on O
Computer O
Vision O
- B-DAT
ECCV, O
pages O
584599, O
2014 O

- B-DAT

- B-DAT
derless O
pooling O
of O
deep O
convolutional O

- B-DAT
son. O
From O
generic O
to O
specific O

- B-DAT
sion O
and O
Pattern O
Recognition O
Workshops O

- B-DAT
multaneous O
feature O
learning O
and O
hash O

- B-DAT
tion O
by O
Learning O
an O
Invariant O

- B-DAT

- B-DAT

- B-DAT
works O
for O
Large-Scale O
Image O
Recognition O

-02 B-DAT

-22 B-DAT

- B-DAT
els: O
http://www.slideshare.net/erikbern/approximate-nearest- O
neighbor-methods-and-vector-models-nyc-ml-meetup O
NYC O
ML O

-02 B-DAT

-22 B-DAT

-02 B-DAT

-22 B-DAT

- B-DAT
scale O
machine O
learning O
on O
heterogeneous O

- B-DAT
2605 O

- B-DAT

- B-DAT

-02 B-DAT

-22 B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

- B-DAT

are O
applied O
to O
the O
acoustic O
mixed-speech B-DAT
spectrogram. O
Results O
show O
that: O
(i O

landmark O
features O
and O
the O
input O
mixed-speech B-DAT
spectrogram O

applied O
to O
clean O
the O
acoustic O
mixed-speech B-DAT
spectrogram O

compressed O
spectrogram O
of O
the O
single-channel O
mixed-speech B-DAT
signal. O
All O
of O
them O
perform O

although O
IAM O
generation O
requires O
the O
mixed-speech B-DAT
spectrogram, O
separate O
spectrograms O
for O
each O

of O
them, O
we O
created O
a O
mixed-speech B-DAT
version O

98 O
utterances O
per O
speaker. O
The O
mixed-speech B-DAT
version O
was O
created O
following O
the O

metric O
values O
of O
the O
input O
mixed-speech B-DAT
signal O

speaker’s O
spectrogram O
from O
the O
acoustic O
mixed-speech B-DAT
spectrogram O

are O
applied O
to O
the O
acoustic O
mixed-speech B-DAT
spectrogram. O
Results O
show O
that: O
(i O

landmark O
features O
and O
the O
input O
mixed-speech B-DAT
spectrogram O

applied O
to O
clean O
the O
acoustic O
mixed-speech B-DAT
spectrogram O

compressed O
spectrogram O
of O
the O
single-channel O
mixed-speech B-DAT
signal. O
All O
of O
them O
perform O

although O
IAM O
generation O
requires O
the O
mixed-speech B-DAT
spectrogram, O
separate O
spectrograms O
for O
each O

of O
them, O
we O
created O
a O
mixed-speech B-DAT
version O

98 O
utterances O
per O
speaker. O
The O
mixed-speech B-DAT
version O
was O
created O
following O
the O

metric O
values O
of O
the O
input O
mixed-speech B-DAT
signal O

speaker’s O
spectrogram O
from O
the O
acoustic O
mixed-speech B-DAT
spectrogram O

Regarding O
the O
GRID O
corpus, B-DAT
for O
each O
of O
the O
33 O

The O
TCD-TIMIT O
corpus B-DAT
consists O
of O
59 O
speakers O
(we O

speaker-dependent O
models O
on O
the O
GRID O
corpus B-DAT
with O
landmark O
motion O
vectors. O
Results O

and O
Xu O
Shao, O
“An O
audio-visual O
corpus B-DAT
for O
speech O
perception O
and O
automatic O

the O
limited O
size O
GRID O
and O
TCD-TIMIT B-DAT
datasets, O
that O
achieve O
speaker-independent O
speech O

on O
the O
GRID O
[9] O
and O
TCD-TIMIT B-DAT
[10] O
datasets O
in O
a O
speaker-independent O

using O
the O
GRID O
[9] O
and O
TCD-TIMIT B-DAT
[10] O
audio-visual O
datasets. O
For O
each O

The O
TCD-TIMIT B-DAT
corpus O
consists O
of O
59 O
speakers O

difference. O
Con- O
trary O
to O
GRID, O
TCD-TIMIT B-DAT
utterances O
have O
different O
dura- O
tion O

TCD-TIMIT) B-DAT
to O
100 O
fps O
to O
match O

Table O
3. O
TCD-TIMIT B-DAT
results O
- O
speaker-independent O

results O
on O
the O
GRID O
and O
TCD-TIMIT B-DAT
datasets O
respectively. O
V2ML O
performs O
significantly O

the O
limited O
size O
GRID O
and O
TCD-TIMIT B-DAT
datasets O
that O
accomplish O
speaker- O
independent O

TCD-TIMIT B-DAT

FACE B-DAT
LANDMARK-BASED O
SPEAKER-INDEPENDENT O
AUDIO-VISUAL O
SPEECH O

ENHANCEMENT B-DAT
IN O
MULTI-TALKER O
ENVIRONMENTS O
Giovanni O
Morrone? O
Luca O
Pasa† O
Vadim O

Tikhanoff B-DAT

Sonia B-DAT
Bergamaschi? O
Luciano O
Fadiga† O
Leonardo O

Badino† B-DAT
?Department O
of O
Engineering O
”Enzo O
Ferrari O

”, B-DAT
University O
of O
Modena O
and O

Reggio B-DAT
Emilia, O
Modena, O
Italy O
†Istituto O

Italiano B-DAT
di O
Tecnologia, O
Ferrara, O
Italy O
ABSTRACT O

In B-DAT
this O
paper, O
we O
address O

the B-DAT
problem O
of O
enhancing O
the O

speech B-DAT
of O
a O
speaker O
of O

interest B-DAT
in O
a O
cocktail O
party O

scenario B-DAT
when O
vi- O
sual O
information O

of B-DAT
the O
speaker O
of O
interest O

is B-DAT
available. O
Contrary O
to O
most O
previous O
studies O

, B-DAT
we O
do O
not O
learn O

visual B-DAT
features O
on O
the O
typically O

small B-DAT
audio-visual O
datasets, O
but O
use O

an B-DAT
already O
available O
face O
landmark O

detector B-DAT
(trained O
on O
a O
sep- O

arate B-DAT
image O
dataset). O
The O
landmarks O
are O
used O
by O

LSTM-based B-DAT
models O
to O
gen- O
erate O

time-frequency B-DAT
masks O
which O
are O
applied O

to B-DAT
the O
acoustic O
mixed-speech O
spectrogram O

. B-DAT
Results O
show O
that: O
(i) O

land- B-DAT
mark O
motion O
features O
are O

very B-DAT
effective O
features O
for O
this O

task, B-DAT
(ii) O
similarly O
to O
previous O

work, B-DAT
reconstruction O
of O
the O
target O

speaker’s B-DAT
spectrogram O
mediated O
by O
masking O

is B-DAT
significantly O
more O
accurate O
than O

direct B-DAT
spectrogram O
reconstruction, O
and O
(iii) O

the B-DAT
best O
masks O
depend O
on O

both B-DAT
motion O
landmark O
features O
and O

the B-DAT
input O
mixed-speech O
spectrogram. O
To O
the O
best O
of O
our O

knowledge, B-DAT
our O
proposed O
models O
are O

the B-DAT
first O
models O
trained O
and O

evaluated B-DAT
on O
the O
limited O
size O

GRID B-DAT
and O
TCD-TIMIT O
datasets, O
that O

achieve B-DAT
speaker-independent O
speech O
enhancement O
in O

a B-DAT
multi-talker O
setting O

. B-DAT
Index O
Terms— O
audio-visual O
speech O
enhancement O

, B-DAT
cock- O
tail O
party O
problem, O

time-frequency B-DAT
mask, O
LSTM, O
face O
land- O

marks B-DAT
1. O
INTRODUCTION O

In B-DAT
the O
context O
of O
speech O

perception, B-DAT
the O
cocktail O
party O

effect B-DAT
[1, O
2] O
is O
the O

ability B-DAT
of O
the O
brain O
to O

recognize B-DAT
speech O
in O
complex O
and O

adverse B-DAT
listening O
conditions O
where O
the O

attended B-DAT
speech O
is O
mixed O
with O

competing B-DAT
sounds/speech. O
Speech O
perception O
studies O
have O
shown O

that B-DAT
watching O
speaker’s O
face O
movements O

could B-DAT
dramatically O
improve O
our O
ability O

at B-DAT
recognizing O
the O
speech O
of O

a B-DAT
target O
speaker O
in O
a O

multi-talker B-DAT
environment O
[3, O
4 O

]. B-DAT
This O
work O
aims O
at O
extracting O

the B-DAT
speech O
of O
a O
target O

speaker B-DAT
from O
single O
channel O
audio O

of B-DAT
several O
people O
talking O
simulta O

- B-DAT
neously. O
This O
is O
an O

ill-posed B-DAT
problem O
in O
that O
many O

differ- B-DAT
ent O
hypotheses O
about O
what O

the B-DAT
target O
speaker O
says O
are O

con- B-DAT
sistent O
with O
the O
mixture O
signal O

. B-DAT
Yet, O
it O
can O
be O

solved B-DAT
by O
ex- O
ploiting O
some O

additional B-DAT
information O
associated O
to O
the O

speaker B-DAT
of O
interest O
and/or O
by O

leveraging B-DAT
some O
prior O
knowledge O
about O

speech B-DAT
signal O
properties O
(e.g., O
[5]). O

In B-DAT
this O
work O
we O
use O

face B-DAT
movements O
of O
the O
target O

speaker B-DAT
as O
additional O
information. O
This O
paper O
(i) O
proposes O
the O

use B-DAT
of O
face O
landmark’s O
move O

- B-DAT
ments, O
extracted O
using O

Dlib B-DAT
[6, O
7] O
and O
(ii) O

compares B-DAT
differ- O
ent O
ways O
of O

mapping B-DAT
such O
visual O
features O
into O

time-frequency B-DAT
(T-F) O
masks, O
then O
applied O

to B-DAT
clean O
the O
acoustic O
mixed-speech O

spectrogram. B-DAT
By O
using O
Dlib O
extracted O
landmarks O

we B-DAT
relieve O
our O
mod- O
els O

from B-DAT
the O
task O
of O
learning O

useful B-DAT
visual O
features O
from O
raw O

pixels. B-DAT
That O
aspect O
is O
particularly O

relevant B-DAT
when O
the O
training O
audio-visual O

datasets B-DAT
are O
small O

. B-DAT
The O
analysis O
of O
landmark-dependent O
masking O

strategies B-DAT
is O
motivated O
by O
the O

fact B-DAT
that O
speech O
enhancement O
mediated O

by B-DAT
an O
explicit O
masking O
is O

often B-DAT
more O
effective O
than O
mask-free O

enhancement B-DAT
[8 O

]. B-DAT
All O
our O
models O
were O
trained O

and B-DAT
evaluated O
on O
the O
GRID O

[9] B-DAT
and O
TCD-TIMIT O
[10] O
datasets O

in B-DAT
a O
speaker-independent O
setting O

. B-DAT
1.1. O
Related O
work O

Speech B-DAT
enhancement O
aims O
at O
extracting O

the B-DAT
voice O
of O
a O
tar- O

get B-DAT
speaker, O
while O
speech O
separation O

refers B-DAT
to O
the O
problem O
of O

separating B-DAT
each O
sound O
source O
in O

a B-DAT
mixture. O
Recently O
pro- O
posed O

audio-only B-DAT
single-channel O
methods O
have O
achieved O

very B-DAT
promising O
results O
[11, O
12, O
13 O

]. B-DAT
However O
the O
task O
still O

remains B-DAT
challenging. O
Additionally, O
audio-only O
systems O

need B-DAT
separate O
models O
in O
order O

to B-DAT
associate O
the O
estimated O
separated O

audio B-DAT
sources O
to O
each O
speaker, O

while B-DAT
vision O
easily O
allow O
that O

in B-DAT
a O
unified O
model. O
Regarding O
audio-visual O
speech O
enhancement O
and O

separa- B-DAT
tion O
methods O
an O
extensive O

review B-DAT
is O
provided O
in O
[14 O

]. B-DAT
Here O
we O
focus O
on O

the B-DAT
deep-learning O
methods O
that O
are O

most B-DAT
related O
to O
the O
present O

work. B-DAT
Our O
first O
architecture O
(Section O
2.1 O

) B-DAT
is O
inspired O
by O
[15], O

where B-DAT
a O
pre-trained O
convolutional O
neural O

network B-DAT
(CNN) O
is O
used O
to O

generate B-DAT
a O
clean O
spectrogram O
from O

silent B-DAT
video O
[16]. O
Rather O
than O

directly B-DAT
computing O
a O
time-frequency O
(T-F) O

mask, B-DAT
ar O
X O

iv B-DAT
:1 O
81 O
1 O

. B-DAT
02 O
48 O

0v B-DAT
3 O

cs B-DAT
.C O
L O

2 B-DAT

M B-DAT

2 B-DAT
01 O
9 O

the B-DAT
mask O
is O
computed O
by O

thresholding B-DAT
the O
estimated O
clean O
spectrogram. O

This B-DAT
approach O
is O
not O
very O

effective B-DAT
since O
the O
pre-trained O
CNN O

is B-DAT
designed O
for O
a O
different O

task B-DAT
(video-to- O
speech O
synthesis). O

In B-DAT
[17] O
a O
CNN O
is O

trained B-DAT
to O
directly O
esti- O
mate O

clean B-DAT
speech O
from O
noisy O
audio O

and B-DAT
input O
video. O
A O
sim- O

ilar B-DAT
model O
is O
used O

in B-DAT
[18], O
where O
the O
model O

jointly B-DAT
generates O
clean O
speech O
and O

input B-DAT
video O
in O
a O
denoising-autoender O

archi- B-DAT
tecture. O
[19] O
shows O
that O
using O
information O

about B-DAT
lip O
positions O
can O
help O

to B-DAT
improve O
speech O
enhancement. O
The O

video B-DAT
feature O
vec- O
tor O
is O

obtained B-DAT
computing O
pair-wise O
distances O
between O

any B-DAT
mouth O
landmarks. O
Similarly O
to O

our B-DAT
approach O
their O
visual O
fea O

- B-DAT
tures O
are O
not O
learned O

on B-DAT
the O
audio-visual O
dataset O
but O

are B-DAT
pro- O
vided O
by O
a O

system B-DAT
trained O
on O
different O
dataset. O

Contrary B-DAT
to O
our O
approach, O
[19] O

uses B-DAT
position-based O
features O
while O
we O

use B-DAT
motion O
features O
(of O
the O

whole B-DAT
face) O
that O
in O
our O

experiments B-DAT
turned O
out O
to O
be O

much B-DAT
more O
effective O
than O
positional O

features. B-DAT
Although O
the O
aforementioned O
audio-visual O
methods O

work B-DAT
well, O
they O
have O
only O

been B-DAT
evaluated O
in O
a O
speaker-dependent O

setting. B-DAT
Only O
the O
availability O
of O

new B-DAT
large O
and O
heterogeneous O
audio-visual O

datasets B-DAT
has O
allowed O
the O
training O

of B-DAT
deep O
neu- O
ral O
network-based O

speaker-independent B-DAT
speech O
enhancement O
models O
[20 O

, B-DAT
21, O
22]. O
The O
present O
work O
shows O
that O

huge B-DAT
audio-visual O
datasets O
are O
not O

a B-DAT
necessary O
requirement O
for O
speaker-independent O

audio-visual B-DAT
speech O
enhancement. O
Although O
we O

have B-DAT
only O
considered O
datasets O
with O

simple B-DAT
visual O
scenarios O
(i.e., O
the O

target B-DAT
speaker O
is O
always O
facing O

the B-DAT
camera), O
we O
expect O
our O

methods B-DAT
to O
perform O
well O
in O

more B-DAT
complex O
scenarios O
thanks O
to O

the B-DAT
robust O
landmark O
extraction O

. B-DAT
2. O
MODEL O
ARCHITECTURES O

We B-DAT
experimented O
with O
the O
four O

models B-DAT
shown O
in O
Fig. O
1. O

All B-DAT
models O
receive O
in O
input O

the B-DAT
target O
speaker’s O
landmark O
mo- O

tion B-DAT
vectors O
and O
the O
power-law O

compressed B-DAT
spectrogram O
of O
the O
single-channel O

mixed-speech B-DAT
signal. O
All O
of O
them O

perform B-DAT
some O
kind O
of O
masking O

operation. B-DAT
2.1. O
VL2M O
model O

At B-DAT
each O
time O
frame, O
the O

video-landmark B-DAT
to O
mask O
(VL2M) O
model O
( O

Fig. B-DAT
1a) O
estimates O
a O
T-F O

mask B-DAT
from O
visual O
features O
only O
( O

of B-DAT
the O
target O
speaker). O
Formally, O

given B-DAT
a O
video O
sequence O

= B-DAT
[v1 O

, B-DAT
. O
. O
. O
, O

vT B-DAT
], O
vt O
∈ O
Rn O

and B-DAT
a O
target O
mask O
sequence O

= B-DAT
[m1 O

, B-DAT
. O
. O
. O
, O

mT B-DAT
], O
mt O
∈ O
Rd, O

VL2M B-DAT
perform O
a O
function O

Fvl2m(v) B-DAT
= O
m̂, O
where O
m̂ O

is B-DAT
the O
estimated O
mask. O
The O
training O
objective O
for O
VL2M O

is B-DAT
a O
Target O
Binary O
Mask O

(TBM) B-DAT
[23, O
24], O
computed O
using O

the B-DAT
spectrogram O
of O
the O
tar O

- B-DAT
get O
speaker O
only. O
This O

is B-DAT
motivated O
by O
our O
goal O

of B-DAT
extracting O
the O
speech O
of O

a B-DAT
target O
speaker O
as O
much O

as B-DAT
possible O
indepen- O
dently O
of O

the B-DAT
concurrent O
speakers, O
so O
that, O

e.g., B-DAT
we O
do O
not O
need O

to B-DAT
estimate O
their O
number. O
An O

additional B-DAT
motivations O
is O
that O
the O

model B-DAT
takes O
as O
only O
input O

the B-DAT
visual O
features O
of O
the O
target O
speaker, O
and O
a O
target O

TBM B-DAT
that O
only O
depends O
on O

the B-DAT
target O
speaker O
allows O
VL2M O

to B-DAT
learn O
a O
function O
(rather O

than B-DAT
approximating O
an O
ill-posed O
one-to-many O

mapping B-DAT

). B-DAT
Given O
a O
clean O
speech O
spectrogram O

of B-DAT
a O
speaker O
s O

= B-DAT
[s1 O

, B-DAT
. O
. O
. O
, O

sT B-DAT
], O
st O
∈ O
Rd, O

the B-DAT
TBM O
is O
defined O
by O

comparing, B-DAT
at O
each O
frequency O
bin O

∈ B-DAT
[1 O

, B-DAT
. O
. O
. O
, O

d], B-DAT
the O
target O
speaker O
value O

st[f B-DAT
] O
vs. O
a O
reference O

threshold B-DAT
τ O
[f O
]. O
As O

in B-DAT
[15], O
we O
use O
a O

function B-DAT
of O
long-term O
average O
speech O

spectrum B-DAT
(LTASS) O
as O
reference O
threshold. O

This B-DAT
threshold O
indicates O
if O
a O

T-F B-DAT
unit O
is O
generated O
by O

the B-DAT
speaker O
or O
refers O
to O

silence B-DAT
or O
noise. O
The O
process O

to B-DAT
compute O
the O
speaker’s O
TBM O

is B-DAT
as O
follows: O
1. O
The O
mean O
π[f O

] B-DAT
and O
the O
standard O
deviation O

σ[f B-DAT
] O
are O
computed O
for O

all B-DAT
frequency O
bins O
of O
all O

seen B-DAT
spectro- O
grams O
in O
speaker’s O

data B-DAT

. B-DAT
2. O
The O
threshold O
τ O
[f O

] B-DAT
is O
defined O
as O
τ O

[f B-DAT
] O
= O
π[f O
]+0.6 O

·σ[f B-DAT
] O
where O
0.6 O
is O

a B-DAT
value O
selected O
by O
manual O

inspection B-DAT
of O
several O
spectrogram-TBM O
pairs O

. B-DAT
3. O
The O
threshold O
is O
applied O

to B-DAT
every O
speaker’s O
speech O
spec O

- B-DAT
trogram O
s. O
mt[f O

1, B-DAT
if O
st[f O
] O
≥ O

τ B-DAT
[f O
], O
0, O
otherwise. O
The O
mapping O
Fvl2m(·) O
is O
carried O

out B-DAT
by O
a O
stacked O
bi O

- B-DAT
directional O
Long O
Short-Term O
Memory O
( O

BLSTM) B-DAT
network O
[25]. O
The O
BLSTM O

outputs B-DAT
are O
then O
forced O
to O

lay B-DAT
within O
the O
[0, O
1] O

range. B-DAT
Finally O
the O
computed O
TBM O

m̂ B-DAT
and O
the O
noisy O
spectrogram O

y B-DAT
are O
element-wise O
multiplied O
to O

ob- B-DAT
tain O
the O
estimated O
clean O

spectrogram B-DAT
ŝm O
= O
m̂ O
◦ O

y, B-DAT
where O

= B-DAT
[y1 O

, B-DAT
. O
. O
. O

yT B-DAT
], O
yt O
∈ O
Rd. O
The O
model O
parameters O
are O
estimated O

to B-DAT
minimize O
the O
loss O

: B-DAT
Jvl2m O
= O
∑T O

t=1 B-DAT
∑d O
f=1−mt[f O
] O
· O
log(m̂t[f O

])− B-DAT
(1−mt[f O
]) O
· O
log(1 O

− B-DAT
m̂t[f O
]) O
2.2. O
VL2M O
ref O
model O

VL2M B-DAT
generates O
T-F O
masks O
that O

are B-DAT
independent O
of O
the O
acous- O

tic B-DAT
context. O
We O
may O
want O

to B-DAT
refine O
the O
masking O
by O

including B-DAT
such O
context. O
This O
is O

what B-DAT
the O
novel O
VL2M O
ref O

does B-DAT
(Fig. O
1b). O
The O
computed O

TBM B-DAT
m̂ O
and O
the O
input O

spectrogram B-DAT
y O
are O
the O
input O

to B-DAT
a O
function O
that O
outputs O

an B-DAT
Ideal O
Amplitude O
Mask O
(IAM) O

p B-DAT
(known O
as O
FFT-MASK O

in B-DAT
[8]). O
Given O
the O
target O

clean B-DAT
spectrogram O
s O
and O
the O

noisy B-DAT
spectrogram O
y, O
the O
IAM O

is B-DAT
defined O
as: O
pt[f O
] O
= O
st[f O

yt[f B-DAT
] O
Note O
that O
although O
IAM O
generation O

requires B-DAT
the O
mixed-speech O
spectrogram, O
separate O

spectrograms B-DAT
for O
each O
concurrent O
speakers O

are B-DAT
not O
required O

. B-DAT
The O
target O
speaker’s O
spectrogram O
s O

is B-DAT
reconstructed O
by O
multiplying O
the O

input B-DAT
spectrogram O
with O
the O
estimated O

IAM. B-DAT
Values O
greater O
than O
10 O

in B-DAT
the O
IAM O
are O
clipped O

to B-DAT
10 O
in O
order O
to O

obtain B-DAT
better O
numerical O
stability O
as O

suggested B-DAT
in O
[8 O

v: B-DAT
video O
input O
y: O
noisy O

spectrogram B-DAT
sm: O
clean O
spectrogram O
TBM O

s: B-DAT
clean O
spectrogram O
IAM O
m: O

TBM B-DAT
p: O
IAM O
STACKED O

BLSTM B-DAT
m O

sm B-DAT
v O

y B-DAT
(a) O
VL2M O

v B-DAT
VL2M O
m O
y O
BLSTM O

BLSTM B-DAT
Fusion O
layer O

BLSTM B-DAT
p O
s O

b) B-DAT
VL2M O
ref O
v O

y B-DAT
p O
STACKED O

BLSTM B-DAT
s O

c) B-DAT
Audio-Visual O
concat O
sm O

y B-DAT
p O
STACKED O

BLSTM B-DAT
s O

v B-DAT
VL2M O
m O
(d) O
Audio-Visual O
concat-ref O

Fig. B-DAT
1. O
Model O
architectures. O
The O
model O
performs O
a O
function O

Fmr(v, B-DAT
y) O
= O
p̂ O
that O

con- B-DAT
sists O
of O
a O
VL2M O

component B-DAT
plus O
three O
different O
BLSTMs O

Gm, B-DAT
Gy O
and O
H O

Gm(Fvl2m(v)) B-DAT
= O
rm O
receives O
the O

VL2M B-DAT
mask O
m̂ O
as O
in- O

put, B-DAT
and O
Gy(y) O
= O
ry O

is B-DAT
fed O
with O
the O
noisy O

spectrogram. B-DAT
Their O
output O
rm, O

ry B-DAT
∈ O
Rz O
are O
fused O

in B-DAT
a O
joint O
audio-visual O
represen- O

tation B-DAT

= B-DAT
[h1 O

, B-DAT
. O
. O
. O
, O

hT B-DAT
], O
where O
ht O
is O

a B-DAT
linear O
combination O
of O
rmt O

and B-DAT
ryt O
: O
ht O
= O

Whm B-DAT
·rmt O
+Why O
·ryt O
+bh. O

h B-DAT
is O
the O
input O
of O

the B-DAT
third O
BLSTM O
H O
( O

h) B-DAT
= O
p̂, O
where O
p̂ O

lays B-DAT
in O
the O
[0,10] O
range. O

The B-DAT
loss O
function O
is: O
Jmr O

T∑ B-DAT
t=1 O
d∑ O
f=1 O

p̂t[f B-DAT
] O
· O
yt[f O
]− O

st[f B-DAT
])2 O
2.3. O
Audio-Visual O
concat O
model O

The B-DAT
third O
model O
(Fig. O
1c) O

performs B-DAT
early O
fusion O
of O
audio- O

visual B-DAT
features. O
This O
model O
consists O

of B-DAT
a O
single O
stacked O
BLSTM O

that B-DAT
computes O
the O
IAM O
mask O

p̂ B-DAT
from O
the O
concate- O

nated B-DAT
[v,y]. O
The O
training O
loss O

is B-DAT
the O
same O
Jmr O
used O

to B-DAT
train O
VL2M O
ref. O
This O

model B-DAT
can O
be O
regarded O
as O

a B-DAT
simplification O
of O
VL2M O
ref, O

where B-DAT
the O
VL2M O
operation O
is O

not B-DAT
performed. O
2.4. O
Audio-Visual O
concat-ref O
model O

The B-DAT
fourth O
model O
(Fig. O
1d) O

is B-DAT
an O
improved O
version O
of O

the B-DAT
model O
described O
in O
section O
2 O

.3. B-DAT
The O
only O
difference O
is O

the B-DAT
input O
of O
the O
stacked O

BLSTM B-DAT
that O
is O
replaced O

by B-DAT
[̂sm,y] O
where O
ŝm O
is O

the B-DAT
denoised O
spectrogram O
returned O
by O

VL2M B-DAT
operation. O
3. O
EXPERIMENTAL O
SETUP O

3.1. B-DAT
Dataset O
All O
experiments O
were O
carried O
out O

using B-DAT
the O
GRID O
[9] O
and O

TCD-TIMIT B-DAT
[10] O
audio-visual O
datasets. O
For O

each B-DAT
of O
them, O
we O
created O

a B-DAT
mixed-speech O
version O

. B-DAT
Regarding O
the O
GRID O
corpus, O
for O

each B-DAT
of O
the O
33 O
speakers O

(one B-DAT
had O
to O
be O
discarded O

) B-DAT
we O
first O
randomly O
selected O
200 O
ut- O
terances O
(out O
of O
1000 O

). B-DAT
Then, O
for O
each O
utterance, O

we B-DAT
created O
3 O
different O
audio-mixed O

samples. B-DAT
Each O
audio-mixed O
sample O
was O

created B-DAT
by O
mixing O
the O
chosen O

utterance B-DAT
with O
one O
utter- O
ance O

from B-DAT
a O
different O
speaker. O
That O
resulted O
in O
600 O
audio-mixed O

samples B-DAT
per O
speaker O

. B-DAT
The O
resulting O
dataset O
was O
split O

into B-DAT
disjoint O
sets O
of O
25/4/4 O

speakers B-DAT
for O
training/validation/testing O
respectively O

. B-DAT
The O
TCD-TIMIT O
corpus O
consists O
of O

59 B-DAT
speakers O
(we O
ex- O
cluded O

3 B-DAT
professionally-trained O
lipspeakers) O
and O
98 O

utterances B-DAT
per O
speaker. O
The O
mixed-speech O

version B-DAT
was O
created O
following O
the O

same B-DAT
procedure O
as O
for O
GRID O

, B-DAT
with O
one O
difference. O
Con- O

trary B-DAT
to O
GRID, O
TCD-TIMIT O
utterances O

have B-DAT
different O
dura- O
tion. O
Thus O
2 O
utterances O
were O
mixed O
only O
if O

their B-DAT
duration O
dif- O
ference O
did O

not B-DAT
exceed O
2 O
seconds. O
For O

each B-DAT
utterance O
pair, O
we O
forced O

the B-DAT
non-target O
speaker’s O
utterance O
to O

match B-DAT
the O
du- O
ration O
of O

the B-DAT
target O
speaker O
utterance. O
If O

it B-DAT
was O
longer, O
the O
utterance O

was B-DAT
cut O
at O
its O
end O

, B-DAT
whereas O
if O
it O
was O

shorter, B-DAT
silence O
samples O
were O
equally O

added B-DAT
at O
its O
start O
and O

end. B-DAT
The O
resulting O
dataset O
was O
split O

into B-DAT
disjoint O
sets O
of O
51/4/4 O

speakers B-DAT
for O
training/validation/testing O
respectively O

. B-DAT
3.2. O
LSTM O
training O

In B-DAT
all O
experiments, O
the O
models O

were B-DAT
trained O
using O
the O
Adam O

optimizer B-DAT
[26]. O
Early O
stopping O
was O

applied B-DAT
when O
the O
error O
on O

the B-DAT
validation O
set O
did O
not O

decrease B-DAT
over O
5 O
consecutive O
epochs. O
VL2M, O
AV O
concat O
and O
AV O

concat-ref B-DAT
had O
5, O
3 O
and O

3 B-DAT
stacked O
BLSTM O
layers O
respectively O

. B-DAT
All O
BLSTMs O
had O
250 O

units. B-DAT
Hyper-parameters O
selection O
was O
performed O

by B-DAT
using O
random O
search O
with O

a B-DAT
limited O
number O
of O
samples, O

therefore B-DAT
all O
the O
reported O
results O

may B-DAT
improve O
through O
a O
deeper O

hyper- B-DAT
parameters O
validation O
phase. O
VL2M O
ref O
and O
AV O
concat-ref O

training B-DAT
was O
performed O
in O
2 O

steps. B-DAT
We O
first O
pre-trained O
the O

models B-DAT
using O
the O
oracle O
TBM O

m. B-DAT
Then O
we O
substituted O
the O

oracle B-DAT
masks O
with O
the O
VL2M O

component B-DAT
and O
retrained O
the O
models O

while B-DAT
freezing O
the O
pa- O
rameters O

of B-DAT
the O
VL2M O
component O

. B-DAT
3.3. O
Audio O
pre- O
and O
post-processing O

The B-DAT
original O
waveforms O
were O
resampled O

to B-DAT
16 O
kHz. O
Short- O
Time O

Fourier B-DAT
Transform O
(STFT) O
x O
was O

computed B-DAT
using O
FFT O
size O
of O
512, O
Hann O
window O
of O
length O
25 O

ms B-DAT
(400 O
samples), O
and O
hop O

length B-DAT
of O
10 O
ms O
(160 O

samples). B-DAT
The O
input O
spectro- O
gram O

was B-DAT
obtained O
taking O
the O
STFT O

magnitude B-DAT
and O
perform- O
ing O
power-law O

compression B-DAT
|x|p O
with O
p O

= B-DAT
0.3. O
Finally O
we O
applied O

per-speaker B-DAT
0-mean O
1-std O
normalization O

. B-DAT
In O
the O
post-processing O
stage, O
the O

enhanced B-DAT
waveform O
gen- O
erated O
by O

the B-DAT
speech O
enhancement O
models O
was O

reconstructed B-DAT

SDR B-DAT
PESQ O
ViSQOL O
Noisy O
−1.06 O
1.81 O
2.11 O
VL2M O

3.17 B-DAT
1.51 O
1.16 O
VL2M O
ref O

6.50 B-DAT
2.58 O
2.99 O
AV O
concat O

6.31 B-DAT
2.49 O
2.83 O
AV O
c-ref O

6.17 B-DAT
2.58 O
2.96 O

Table B-DAT
1. O
GRID O
results O
- O

speaker-dependent. B-DAT
The O
“Noisy” O
row O
refers O

to B-DAT
the O
metric O
values O
of O

the B-DAT
input O
mixed-speech O
signal. O
2 O
Speakers O
3 O
Speakers O
SDR O

PESQ B-DAT
ViSQOL O
SDR O
PESQ O
ViSQOL O

Noisy B-DAT
0.21 O
1.94 O
2.58 O
−5.34 O
1 O

.43 B-DAT
1.62 O
VL2M O
3.02 O
1.81 O
1 O

.70 B-DAT
−2.03 O
1.43 O
1.25 O
VL2M O

ref B-DAT
6.52 O
2.53 O
3.02 O
2.83 O
2 O

.19 B-DAT
2.53 O
AV O
concat O
7.37 O
2 O

.65 B-DAT
3.03 O
3.02 O
2.24 O
2.49 O

AV B-DAT
c-ref O
8.05 O
2.70 O
3.07 O
4 O

.02 B-DAT
2.33 O
2.64 O
Table O
2. O
GRID O
results O

- B-DAT
speaker-independent O

. B-DAT
by O
applying O
the O
inverse O
STFT O

to B-DAT
the O
estimated O
clean O
spectro O

- B-DAT
gram O
and O
using O
the O

phase B-DAT
of O
the O
noisy O
input O

signal. B-DAT
3.4. O
Video O
pre-processing O

Face B-DAT
landmarks O
were O
extracted O
from O

video B-DAT
using O
the O
Dlib O
[7] O

implementation B-DAT
of O
the O
face O
landmark O

estimator B-DAT
described O
in O
[6]. O
It O

returns B-DAT
68 O
x-y O
points, O
for O

an B-DAT
overall O
136 O
values. O
We O

upsampled B-DAT
from O
25/29.97 O
fps O
(GRID/TCD-TIMIT) O

to B-DAT
100 O
fps O
to O
match O

the B-DAT
frame O
rate O
of O
the O

audio B-DAT
spectrogram. O
Upsampling O
was O
carried O

out B-DAT
through O
linear O
interpolation O
over O

time. B-DAT
The O
final O
video O
feature O
vector O

v B-DAT
was O
obtained O
by O
com O

- B-DAT
puting O
the O
per-speaker O
normalized O

motion B-DAT
vector O
of O
the O
face O

landmarks B-DAT
by O
simply O
subtracting O
every O

frame B-DAT
with O
the O
previ- O
ous O

one. B-DAT
The O
motion O
vector O
of O

the B-DAT
first O
frame O
was O
set O

to B-DAT
zero. O
4. O
RESULTS O

In B-DAT
order O
to O
compare O
our O

models B-DAT
to O
previous O
works O
in O

both B-DAT
speech O
enhancement O
and O
separation, O

we B-DAT
evaluated O
the O
perfor- O
mance O

of B-DAT
the O
proposed O
models O
using O

both B-DAT
speech O
separation O
2 O
Speakers O
3 O
Speakers O
SDR O

PESQ B-DAT
ViSQOL O
SDR O
PESQ O
ViSQOL O

Noisy B-DAT
0.21 O
2.22 O
2.74 O
−3.42 O
1 O

.92 B-DAT
2.04 O
VL2M O
2.88 O
2.25 O
2 O

.62 B-DAT
−0.51 O
1.99 O
1.98 O
VL2M O

ref B-DAT
9.24 O
2.81 O
3.09 O
5.27 O
2 O

.44 B-DAT
2.54 O
AV O
concat O
9.56 O
2 O

.80 B-DAT
3.09 O
5.15 O
2.41 O
2.52 O

AV B-DAT
c-ref O
10.55 O
3.03 O
3.21 O
5 O

.37 B-DAT
2.45 O
2.58 O
Table O
3. O
TCD-TIMIT O
results O

- B-DAT
speaker-independent O

. B-DAT
and O
enhancement O
metrics. O
Specifically, O
we O

measured B-DAT
the O
ca- O
pability O
of O

separating B-DAT
the O
target O
utterance O
from O

the B-DAT
concurrent O
utterance O
with O
the O

source-to-distortion B-DAT
ratio O
(SDR) O
[27, O
28 O

]. B-DAT
While O
the O
quality O
of O

estimated B-DAT
target O
speech O
was O
measured O

with B-DAT
the O
perceptual O
PESQ O
[29] O

and B-DAT
ViSQOL O
[30] O
metrics. O
For O

PESQ B-DAT
we O
used O
the O
narrow O

band B-DAT
mode O
while O
for O
ViSQOL O

we B-DAT
used O
the O
wide O
band O

mode. B-DAT
As O
a O
very O
first O
experiment O

we B-DAT
compared O
landmark O
posi- O
tion O

vs. B-DAT
landmark O
motion O
vectors. O
It O

turned B-DAT
out O
that O
landmark O
positions O

performed B-DAT
poorly, O
thus O
all O
results O

reported B-DAT
here O
refer O
to O
landmark O

motion B-DAT
vectors O
only O

. B-DAT
We O
then O
carried O
out O
some O

speaker-dependent B-DAT
experiments O
to O
compare O
our O

models B-DAT
with O
previous O
studies O
as O

, B-DAT
to O
the O
best O
of O

our B-DAT
knowledge, O
there O
are O
no O

reported B-DAT
results O
of O
speaker- O
independent O

systems B-DAT
trained O
and O
tested O
on O

GRID B-DAT
and O
TCD- O
TIMIT O
to O

compare B-DAT
with. O
Table O
1 O
reports O

the B-DAT
test-set O
evalua- O
tion O
of O

speaker-dependent B-DAT
models O
on O
the O
GRID O

corpus B-DAT
with O
landmark O
motion O
vectors. O

Results B-DAT
are O
comparable O
with O
previ- O

ous B-DAT
state-of-the-art O
studies O
in O
an O

almost B-DAT
identical O
setting O
[15, O
17]. O
Table O
2 O
and O
3 O
show O

speaker-independent B-DAT
test-set O
results O
on O
the O

GRID B-DAT
and O
TCD-TIMIT O
datasets O
respectively O

. B-DAT
V2ML O
performs O
significantly O
worse O

than B-DAT
the O
other O
three O
models O

in- B-DAT
dicating O
that O
a O
successful O

mask B-DAT
generation O
has O
to O
depend O

on B-DAT
the O
acoustic O
context. O
The O

performance B-DAT
of O
the O
three O
models O

in B-DAT
the O
speaker-independent O
setting O
is O

comparable B-DAT
to O
that O
in O
the O

speaker-dependent B-DAT
setting. O
AV O
concat-ref O
outperforms O
V2ML O
ref O

and B-DAT
AV O
concat O
for O
both O

datasets. B-DAT
This O
supports O
the O
utility O

of B-DAT
a O
refinement O
strat- O
egy O

and B-DAT
suggests O
that O
the O
refinement O

is B-DAT
more O
effective O
when O
it O

directly B-DAT
refines O
the O
estimated O
clean O

spectrogram, B-DAT
rather O
than O
refining O
the O

estimated B-DAT
mask O

. B-DAT
Finally, O
we O
evaluated O
the O
systems O

in B-DAT
a O
more O
challenging O
testing O

condition B-DAT
where O
the O
target O
utterance O

was B-DAT
mixed O
with O
2 O
utterances O

from B-DAT
2 O
competing O
speakers. O
Despite O

the B-DAT
model O
was O
trained O
with O

mixtures B-DAT
of O
two O
speakers, O
the O

decrease B-DAT
of O
performance O
was O
not O

dramatic B-DAT

. B-DAT
Code O
and O
some O
testing O
examples O

of B-DAT
our O
models O
are O
avail O

- B-DAT
able O
at O
https://goo.gl/3h1NgE. O
5. O
CONCLUSION O

This B-DAT
paper O
proposes O
the O
use O

of B-DAT
face O
landmark O
motion O
vec- O

tors B-DAT
for O
audio-visual O
speech O
enhancement O

in B-DAT
a O
single-channel O
multi-talker O
scenario. O

Different B-DAT
models O
are O
tested O
where O

land- B-DAT
mark O
motion O
vectors O
are O

used B-DAT
to O
generate O
time-frequency O
(T- O

F) B-DAT
masks O
that O
extract O
the O

target B-DAT
speaker’s O
spectrogram O
from O
the O

acoustic B-DAT
mixed-speech O
spectrogram. O
To O
the O
best O
of O
our O

knowledge, B-DAT
some O
of O
the O
proposed O

mod- B-DAT
els O
are O
the O
first O

models B-DAT
trained O
and O
evaluated O
on O

the B-DAT
limited O
size O
GRID O
and O

TCD-TIMIT B-DAT
datasets O
that O
accomplish O
speaker O

- B-DAT
independent O
speech O
enhancement O
in O

the B-DAT
multi-talker O
setting, O
with O
a O

quality B-DAT
of O
enhancement O
comparable O
to O

that B-DAT
achieved O
in O
a O
speaker-dependent O

setting. B-DAT
https://goo.gl/3h1NgE O

6. B-DAT
REFERENCES O
[1] O
E. O
Colin O
Cherry, O
“Some O

experiments B-DAT
on O
the O
recognition O
of O

speech, B-DAT
with O
one O
and O
with O

two B-DAT
ears,” O
The O
Journal O
of O

the B-DAT
Acoustical O
Society O
of O
America O

, B-DAT
vol. O
25, O
no. O
5, O

pp. B-DAT
975–979, O
1953. O
[2] O
Josh O
H O
McDermott, O
“The O

cocktail B-DAT
party O
problem,” O
Current O
Biology O

, B-DAT
vol. O
19, O
no. O
22, O

pp. B-DAT
R1024–R1027, O
2009. O
[3] O
Elana O
Zion O
Golumbic, O
Gregory O

B. B-DAT
Cogan, O
Charles O
E. O
Schroeder O

, B-DAT
and O
David O
Poeppel, O
“Visual O

input B-DAT
enhances O
selective O
speech O
envelope O

tracking B-DAT
in O
auditory O
cortex O
at O

a B-DAT
“cocktail O
party”,” O
Journal O
of O

Neu- B-DAT
roscience, O
vol. O
33, O
no. O
4, O
pp. O
1417–1426, O
2013 O

. B-DAT
[4] O
Wei O
Ji O
Ma, O
Xiang O

Zhou, B-DAT
Lars O
A. O
Ross, O
John O

J. B-DAT
Foxe, O
and O
Lucas O
C O

. B-DAT
Parra, O
“Lip-reading O
aids O
word O

recognition B-DAT
most O
in O
moderate O
noise: O

A B-DAT
bayesian O
explanation O
using O
high-dimensional O

feature B-DAT
space,” O
PLOS O
ONE, O
vol. O
4, O
no. O
3, O
pp. O
1–14, O
03 O

2009 B-DAT

. B-DAT
[5] O
Albert O
S O
Bregman, O
Auditory O

scene B-DAT
analysis: O
The O
perceptual O
organi O

- B-DAT
zation O
of O
sound, O
MIT O

press, B-DAT
1994. O
[6] O
Vahid O
Kazemi O
and O
Josephine O

Sullivan, B-DAT
“One O
millisecond O
face O
align O

- B-DAT
ment O
with O
an O
ensemble O

of B-DAT
regression O
trees,” O
in O
The O

IEEE B-DAT
Conference O
on O
Computer O
Vision O

and B-DAT
Pattern O
Recognition O
(CVPR), O
June O
2014 O

. B-DAT
[7] O
Davis O
E. O
King, O
“Dlib-ml O

: B-DAT
A O
machine O
learning O
toolkit,” O

Journal B-DAT
of O
Machine O
Learning O
Research, O

vol. B-DAT
10, O
pp. O
1755–1758, O
2009. O
[8] O
Yuxuan O
Wang, O
Arun O
Narayanan O

, B-DAT
and O
DeLiang O
Wang, O
“On O

Training B-DAT
Targets O
for O
Supervised O
Speech O

Separation,” B-DAT
IEEE/ACM O
Transactions O
on O
Audio, O

Speech, B-DAT
and O
Language O
Processing, O
vol. O
22, O
no. O
12, O
pp. O
1849–1858, O
Dec O

. B-DAT
2014. O
[9] O
Martin O
Cooke, O
Jon O
Barker O

, B-DAT
Stuart O
Cunningham, O
and O
Xu O

Shao, B-DAT
“An O
audio-visual O
corpus O
for O

speech B-DAT
perception O
and O
automatic O
speech O

recognition,” B-DAT
The O
Journal O
of O
the O

Acoustical B-DAT
Society O
of O
America, O
vol. O
120, O
no. O
5, O
pp. O
2421–2424, O
Nov O

. B-DAT
2006. O
[10] O
Naomi O
Harte O
and O
Eoin O

Gillen, B-DAT
“TCD-TIMIT: O
An O
Audio-Visual O
Cor O

- B-DAT
pus O
of O
Continuous O
Speech,” O

IEEE B-DAT
Transactions O
on O
Multimedia, O
vol. O
17, O
no. O
5, O
pp. O
603–615, O
May O

2015 B-DAT

. B-DAT
[11] O
Z. O
Chen, O
Y. O
Luo O

, B-DAT
and O
N. O
Mesgarani, O
“Deep O

attractor B-DAT
network O
for O
single-microphone O
speaker O

separation,” B-DAT
in O
2017 O
IEEE O
International O

Conference B-DAT
on O
Acoustics, O
Speech O
and O

Signal B-DAT
Processing O
(ICASSP), O
March O
2017, O

pp. B-DAT
246–250. O
[12] O
Yusuf O
Isik, O
Jonathan O
Le O

Roux, B-DAT
Zhuo O
Chen, O
Shinji O
Watanabe O

, B-DAT
and O
John O
R. O

Hershey, B-DAT
“Single-channel O
multi-speaker O
separation O
using O

deep B-DAT
clustering,” O
in O
Interspeech, O
2016. O
[13] O
Morten O
Kolbaek, O
Dong O
Yu O

, B-DAT
Zheng-Hua O
Tan, O
Jesper O
Jensen, O

Morten B-DAT
Kolbaek, O
Dong O
Yu, O
Zheng-Hua O

Tan, B-DAT
and O
Jesper O
Jensen, O
“Multitalker O

speech B-DAT
separation O
with O
utterance-level O
permutation O

invariant B-DAT
training O
of O
deep O
recurrent O

neural B-DAT
networks,” O
IEEE/ACM O
Trans. O
Audio, O

Speech B-DAT
and O
Lang. O
Proc., O
vol. O
25, O
no. O
10, O
pp. O
1901–1913, O
Oct O

. B-DAT
2017. O
[14] O
Bertrand O
Rivet, O
Wenwu O
Wang O

, B-DAT
Syed O
Mohsen O
Naqvi, O
and O

Jonathon B-DAT
Chambers, O
“Audiovisual O
Speech O
Source O

Separation: B-DAT
An O
overview O
of O
key O

methodologies,” B-DAT
IEEE O
Signal O
Processing O
Magazine, O

vol. B-DAT
31, O
no. O
3, O
pp. O
125 O

–134, B-DAT
May O
2014. O
[15] O
Aviv O
Gabbay, O
Ariel O
Ephrat O

, B-DAT
Tavi O
Halperin, O
and O
Shmuel O

Peleg, B-DAT
“Seeing O
through O
noise: O
Visually O

driven B-DAT
speaker O
separation O
and O
enhancement,” O

in B-DAT
ICASSP. O
2018, O
pp. O
3051–3055, O

IEEE. B-DAT
[16] O
Ariel O
Ephrat, O
Tavi O
Halperin O

, B-DAT
and O
Shmuel O
Peleg, O
“Improved O

speech B-DAT
reconstruction O
from O
silent O
video,” O

ICCV B-DAT
2017 O
Workshop O
on O
Computer O

Vision B-DAT
for O
Audio-Visual O
Media, O
2017. O
[17] O
Aviv O
Gabbay, O
Asaph O
Shamir O

, B-DAT
and O
Shmuel O
Peleg, O
“Visual O

speech B-DAT
en- O
hancement,” O
in O
Interspeech. O
2018, O
pp. O
1170–1174, O
ISCA O

. B-DAT
[18] O
Jen-Cheng O
Hou, O
Syu-Siang O
Wang O

, B-DAT
Ying-Hui O
Lai, O
Yu O
Tsao, O

Hsiu-Wen B-DAT
Chang, O
and O
Hsin-Min O

Wang, B-DAT
“Audio-Visual O
Speech O
Enhancement O
Us- O

ing B-DAT
Multimodal O
Deep O
Convolutional O
Neural O

Networks,” B-DAT
IEEE O
Trans- O
actions O
on O

Emerging B-DAT
Topics O
in O
Computational O
Intelligence, O

vol. B-DAT
2, O
no. O
2, O
pp. O
117 O

–128, B-DAT
Apr. O
2018. O
[19] O
Jen-Cheng O
Hou, O
Syu-Siang O
Wang O

, B-DAT
Ying-Hui O
Lai, O
Jen-Chun O
Lin, O

Yu B-DAT
Tsao, O
Hsiu-Wen O
Chang, O
and O

Hsin-Min B-DAT
Wang, O
“Audio-visual O
speech O
enhancement O

using B-DAT
deep O
neural O
networks,” O
in O
2016 O
Asia- O
Pacific O
Signal O
and O
Information O

Processing B-DAT
Association O
Annual O
Sum- O
mit O

and B-DAT
Conference O
(APSIPA), O
Jeju, O
South O

Korea, B-DAT
Dec. O
2016, O
pp. O
1–6 O

, B-DAT
IEEE. O
[20] O
Ariel O
Ephrat, O
Inbar O
Mosseri O

, B-DAT
Oran O
Lang, O
Tali O
Dekel, O

Kevin B-DAT
Wilson, O
Avinatan O
Hassidim, O
William O

T. B-DAT
Freeman, O
and O
Michael O

Rubinstein, B-DAT
“Looking O
to O
Listen O
at O

the B-DAT
Cocktail O
Party: O
A O
Speaker-Independent O

Audio-Visual B-DAT
Model O
for O
Speech O
Separation,” O

ACM B-DAT
Transactions O
on O
Graphics, O
vol. O
37, O
no. O
4, O
pp. O
1–11, O
July O

2018, B-DAT
arXiv: O
1804.03619 O

. B-DAT
[21] O
T. O
Afouras, O
J. O
S O

. B-DAT
Chung, O
and O
A. O

Zisserman, B-DAT
“The O
conversation: O
Deep O
audio-visual O

speech B-DAT
enhancement,” O
in O
Interspeech, O
2018. O
[22] O
Andrew O
Owens O
and O
Alexei O

A B-DAT
Efros, O
“Audio-visual O
scene O
analysis O

with B-DAT
self-supervised O
multisensory O
features,” O
European O

Conference B-DAT
on O
Computer O
Vision O
(ECCV O

), B-DAT
2018. O
[23] O
Michael O
C. O
Anzalone, O
Lauren O

Calandruccio, B-DAT
Karen O
A. O
Doherty, O
and O

Laurel B-DAT
H. O
Carney, O
“Determination O
of O

the B-DAT
potential O
benefit O
of O
time O

- B-DAT
frequency O
gain O
manipulation,” O
Ear O

Hear, B-DAT
vol. O
27, O
no. O
5, O

pp. B-DAT
480–492, O
Oct O
2006, O
16957499[pmid]. O
[24] O
Ulrik O
Kjems, O
Jesper O
B O

. B-DAT
Boldt, O
Michael O
S. O
Pedersen, O

Thomas B-DAT
Lunner, O
and O
DeLiang O

Wang, B-DAT
“Role O
of O
mask O
pattern O

in B-DAT
intelligibility O
of O
ideal O
binary-masked O

noisy B-DAT
speech,” O
The O
Journal O
of O

the B-DAT
Acoustical O
Society O
of O
America, O

vol. B-DAT
126, O
no. O
3, O
pp. O
1415 O

–1426, B-DAT
2009. O
[25] O
A. O
Graves, O
A. O
Mohamed O

, B-DAT
and O
G. O
Hinton, O
“Speech O

recognition B-DAT
with O
deep O
recurrent O
neural O

networks,” B-DAT
in O
2013 O
IEEE O
International O

Con- B-DAT
ference O
on O
Acoustics, O
Speech O

and B-DAT
Signal O
Processing, O
May O
2013, O

pp. B-DAT
6645–6649. O
[26] O
Diederik O
P O
Kingma O
and O

Jimmy B-DAT
Ba, O
“Adam: O
A O
method O

for B-DAT
stochastic O
optimization,” O
arXiv O
preprint O

arXiv:1412.6980, B-DAT
2014 O

. B-DAT
[27] O
E. O
Vincent, O
R. O
Gribonval O

, B-DAT
and O
C. O
Fevotte, O
“Performance O

measure- B-DAT
ment O
in O
blind O
audio O

source B-DAT
separation,” O
IEEE O
Transactions O
on O

Audio, B-DAT
Speech O
and O
Language O
Processing, O

vol. B-DAT
14, O
no. O
4, O
pp. O
1462 O

–1469, B-DAT
July O
2006. O
[28] O
Colin O
Raffel, O
Brian O
McFee O

, B-DAT
Eric O
J O
Humphrey, O
Justin O

Salamon, B-DAT
Oriol O
Nieto, O
Dawen O
Liang, O

Daniel B-DAT
PW O
Ellis, O
and O
C O

Colin B-DAT
Raffel, O
“mir O
eval: O
A O

transparent B-DAT
implementation O
of O
common O
mir O

metrics,” B-DAT
in O
In O
Proceed- O
ings O

of B-DAT
the O
15th O
International O
Society O

for B-DAT
Music O
Information O
Retrieval O
Conference, O

ISMIR. B-DAT
Citeseer, O
2014. O
[29] O
A.W. O
Rix, O
J.G. O
Beerends O

, B-DAT
M.P. O
Hollier, O
and O
A.P. O

Hekstra, B-DAT
“Perceptual O
evaluation O
of O
speech O

quality B-DAT
(PESQ)-a O
new O
method O
for O

speech B-DAT
qual- O
ity O
assessment O
of O

telephone B-DAT
networks O
and O
codecs,” O
in O
2001 O
IEEE O
In- O
ternational O
Conference O
on O

Acoustics, B-DAT
Speech, O
and O
Signal O
Processing O

. B-DAT
Proceedings O
(Cat. O
No.01CH37221), O
Salt O

Lake B-DAT
City, O
UT, O
USA, O
2001, O

vol. B-DAT
2, O
pp. O
749–752, O
IEEE. O
[30] O
A. O
Hines, O
J. O
Skoglund O

, B-DAT
A. O
Kokaram, O
and O
N. O

Harte, B-DAT
“ViSQOL: O
The O
Virtual O
Speech O

Quality B-DAT
Objective O
Listener,” O
in O
IWAENC O
2012 O

; B-DAT
Inter- O
national O
Workshop O
on O

Acoustic B-DAT
Signal O
Enhancement, O
Sept. O
2012, O

pp. B-DAT
1 O

4 B-DAT

1 B-DAT
Introduction O

1.1 B-DAT
Related O
work O

2 B-DAT
MODEL O
ARCHITECTURES O

2.1 B-DAT
VL2M O
model O

2.2 B-DAT
VL2M_ref O
model O

2.3 B-DAT
Audio-Visual O
concat O
model O

2.4 B-DAT
Audio-Visual O
concat-ref O
model O

3 B-DAT
Experimental O
setup O

3.1 B-DAT
Dataset O

3.2 B-DAT
LSTM O
training O

3.3 B-DAT
Audio O
pre- O
and O
post-processing O

3.4 B-DAT
Video O
pre-processing O

4 B-DAT
Results O

5 B-DAT
Conclusion O

6 B-DAT
References O

the O
limited O
size O
GRID O
and O
TCD B-DAT

on O
the O
GRID O
[9] O
and O
TCD B-DAT

using O
the O
GRID O
[9] O
and O
TCD B-DAT

The O
TCD B-DAT

difference. O
Con- O
trary O
to O
GRID, O
TCD B-DAT

TCD B-DAT

Table O
3. O
TCD B-DAT

and O
tested O
on O
GRID O
and O
TCD B-DAT

results O
on O
the O
GRID O
and O
TCD B-DAT

the O
limited O
size O
GRID O
and O
TCD B-DAT

TCD B-DAT

The O
TCD-TIMIT B-DAT
corpus I-DAT
consists O
of O
59 O
speakers O
(we O

are O
applied O
to O
the O
acoustic O
mixed-speech B-DAT
spectrogram. O
Results O
show O
that: O
(i O

landmark O
features O
and O
the O
input O
mixed-speech B-DAT
spectrogram O

applied O
to O
clean O
the O
acoustic O
mixed-speech B-DAT
spectrogram O

compressed O
spectrogram O
of O
the O
single-channel O
mixed-speech B-DAT
signal. O
All O
of O
them O
perform O

although O
IAM O
generation O
requires O
the O
mixed-speech B-DAT
spectrogram, O
separate O
spectrograms O
for O
each O

of O
them, O
we O
created O
a O
mixed-speech B-DAT
version O

98 O
utterances O
per O
speaker. O
The O
mixed-speech B-DAT
version O
was O
created O
following O
the O

metric O
values O
of O
the O
input O
mixed-speech B-DAT
signal O

speaker’s O
spectrogram O
from O
the O
acoustic O
mixed-speech B-DAT
spectrogram O

are O
applied O
to O
the O
acoustic O
mixed-speech B-DAT
spectrogram. O
Results O
show O
that: O
(i O

landmark O
features O
and O
the O
input O
mixed-speech B-DAT
spectrogram O

applied O
to O
clean O
the O
acoustic O
mixed-speech B-DAT
spectrogram O

compressed O
spectrogram O
of O
the O
single-channel O
mixed-speech B-DAT
signal. O
All O
of O
them O
perform O

although O
IAM O
generation O
requires O
the O
mixed-speech B-DAT
spectrogram, O
separate O
spectrograms O
for O
each O

of O
them, O
we O
created O
a O
mixed-speech B-DAT
version O

98 O
utterances O
per O
speaker. O
The O
mixed-speech B-DAT
version O
was O
created O
following O
the O

metric O
values O
of O
the O
input O
mixed-speech B-DAT
signal O

speaker’s O
spectrogram O
from O
the O
acoustic O
mixed-speech B-DAT
spectrogram O

Regarding O
the O
GRID O
corpus, B-DAT
for O
each O
of O
the O
33 O

The O
TCD-TIMIT O
corpus B-DAT
consists O
of O
59 O
speakers O
(we O

speaker-dependent O
models O
on O
the O
GRID O
corpus B-DAT
with O
landmark O
motion O
vectors. O
Results O

and O
Xu O
Shao, O
“An O
audio-visual O
corpus B-DAT
for O
speech O
perception O
and O
automatic O

the O
limited O
size O
GRID O
and O
TCD-TIMIT B-DAT
datasets, O
that O
achieve O
speaker-independent O
speech O

on O
the O
GRID O
[9] O
and O
TCD-TIMIT B-DAT
[10] O
datasets O
in O
a O
speaker-independent O

using O
the O
GRID O
[9] O
and O
TCD-TIMIT B-DAT
[10] O
audio-visual O
datasets. O
For O
each O

The O
TCD-TIMIT B-DAT
corpus O
consists O
of O
59 O
speakers O

difference. O
Con- O
trary O
to O
GRID, O
TCD-TIMIT B-DAT
utterances O
have O
different O
dura- O
tion O

TCD-TIMIT) B-DAT
to O
100 O
fps O
to O
match O

Table O
3. O
TCD-TIMIT B-DAT
results O
- O
speaker-independent O

results O
on O
the O
GRID O
and O
TCD-TIMIT B-DAT
datasets O
respectively. O
V2ML O
performs O
significantly O

the O
limited O
size O
GRID O
and O
TCD-TIMIT B-DAT
datasets O
that O
accomplish O
speaker- O
independent O

TCD-TIMIT B-DAT

FACE B-DAT
LANDMARK-BASED O
SPEAKER-INDEPENDENT O
AUDIO-VISUAL O
SPEECH O

ENHANCEMENT B-DAT
IN O
MULTI-TALKER O
ENVIRONMENTS O
Giovanni O
Morrone? O
Luca O
Pasa† O
Vadim O

Tikhanoff B-DAT

Sonia B-DAT
Bergamaschi? O
Luciano O
Fadiga† O
Leonardo O

Badino† B-DAT
?Department O
of O
Engineering O
”Enzo O
Ferrari O

”, B-DAT
University O
of O
Modena O
and O

Reggio B-DAT
Emilia, O
Modena, O
Italy O
†Istituto O

Italiano B-DAT
di O
Tecnologia, O
Ferrara, O
Italy O
ABSTRACT O

In B-DAT
this O
paper, O
we O
address O

the B-DAT
problem O
of O
enhancing O
the O

speech B-DAT
of O
a O
speaker O
of O

interest B-DAT
in O
a O
cocktail O
party O

scenario B-DAT
when O
vi- O
sual O
information O

of B-DAT
the O
speaker O
of O
interest O

is B-DAT
available. O
Contrary O
to O
most O
previous O
studies O

, B-DAT
we O
do O
not O
learn O

visual B-DAT
features O
on O
the O
typically O

small B-DAT
audio-visual O
datasets, O
but O
use O

an B-DAT
already O
available O
face O
landmark O

detector B-DAT
(trained O
on O
a O
sep- O

arate B-DAT
image O
dataset). O
The O
landmarks O
are O
used O
by O

LSTM-based B-DAT
models O
to O
gen- O
erate O

time-frequency B-DAT
masks O
which O
are O
applied O

to B-DAT
the O
acoustic O
mixed-speech O
spectrogram O

. B-DAT
Results O
show O
that: O
(i) O

land- B-DAT
mark O
motion O
features O
are O

very B-DAT
effective O
features O
for O
this O

task, B-DAT
(ii) O
similarly O
to O
previous O

work, B-DAT
reconstruction O
of O
the O
target O

speaker’s B-DAT
spectrogram O
mediated O
by O
masking O

is B-DAT
significantly O
more O
accurate O
than O

direct B-DAT
spectrogram O
reconstruction, O
and O
(iii) O

the B-DAT
best O
masks O
depend O
on O

both B-DAT
motion O
landmark O
features O
and O

the B-DAT
input O
mixed-speech O
spectrogram. O
To O
the O
best O
of O
our O

knowledge, B-DAT
our O
proposed O
models O
are O

the B-DAT
first O
models O
trained O
and O

evaluated B-DAT
on O
the O
limited O
size O

GRID B-DAT
and O
TCD-TIMIT O
datasets, O
that O

achieve B-DAT
speaker-independent O
speech O
enhancement O
in O

a B-DAT
multi-talker O
setting O

. B-DAT
Index O
Terms— O
audio-visual O
speech O
enhancement O

, B-DAT
cock- O
tail O
party O
problem, O

time-frequency B-DAT
mask, O
LSTM, O
face O
land- O

marks B-DAT
1. O
INTRODUCTION O

In B-DAT
the O
context O
of O
speech O

perception, B-DAT
the O
cocktail O
party O

effect B-DAT
[1, O
2] O
is O
the O

ability B-DAT
of O
the O
brain O
to O

recognize B-DAT
speech O
in O
complex O
and O

adverse B-DAT
listening O
conditions O
where O
the O

attended B-DAT
speech O
is O
mixed O
with O

competing B-DAT
sounds/speech. O
Speech O
perception O
studies O
have O
shown O

that B-DAT
watching O
speaker’s O
face O
movements O

could B-DAT
dramatically O
improve O
our O
ability O

at B-DAT
recognizing O
the O
speech O
of O

a B-DAT
target O
speaker O
in O
a O

multi-talker B-DAT
environment O
[3, O
4 O

]. B-DAT
This O
work O
aims O
at O
extracting O

the B-DAT
speech O
of O
a O
target O

speaker B-DAT
from O
single O
channel O
audio O

of B-DAT
several O
people O
talking O
simulta O

- B-DAT
neously. O
This O
is O
an O

ill-posed B-DAT
problem O
in O
that O
many O

differ- B-DAT
ent O
hypotheses O
about O
what O

the B-DAT
target O
speaker O
says O
are O

con- B-DAT
sistent O
with O
the O
mixture O
signal O

. B-DAT
Yet, O
it O
can O
be O

solved B-DAT
by O
ex- O
ploiting O
some O

additional B-DAT
information O
associated O
to O
the O

speaker B-DAT
of O
interest O
and/or O
by O

leveraging B-DAT
some O
prior O
knowledge O
about O

speech B-DAT
signal O
properties O
(e.g., O
[5]). O

In B-DAT
this O
work O
we O
use O

face B-DAT
movements O
of O
the O
target O

speaker B-DAT
as O
additional O
information. O
This O
paper O
(i) O
proposes O
the O

use B-DAT
of O
face O
landmark’s O
move O

- B-DAT
ments, O
extracted O
using O

Dlib B-DAT
[6, O
7] O
and O
(ii) O

compares B-DAT
differ- O
ent O
ways O
of O

mapping B-DAT
such O
visual O
features O
into O

time-frequency B-DAT
(T-F) O
masks, O
then O
applied O

to B-DAT
clean O
the O
acoustic O
mixed-speech O

spectrogram. B-DAT
By O
using O
Dlib O
extracted O
landmarks O

we B-DAT
relieve O
our O
mod- O
els O

from B-DAT
the O
task O
of O
learning O

useful B-DAT
visual O
features O
from O
raw O

pixels. B-DAT
That O
aspect O
is O
particularly O

relevant B-DAT
when O
the O
training O
audio-visual O

datasets B-DAT
are O
small O

. B-DAT
The O
analysis O
of O
landmark-dependent O
masking O

strategies B-DAT
is O
motivated O
by O
the O

fact B-DAT
that O
speech O
enhancement O
mediated O

by B-DAT
an O
explicit O
masking O
is O

often B-DAT
more O
effective O
than O
mask-free O

enhancement B-DAT
[8 O

]. B-DAT
All O
our O
models O
were O
trained O

and B-DAT
evaluated O
on O
the O
GRID O

[9] B-DAT
and O
TCD-TIMIT O
[10] O
datasets O

in B-DAT
a O
speaker-independent O
setting O

. B-DAT
1.1. O
Related O
work O

Speech B-DAT
enhancement O
aims O
at O
extracting O

the B-DAT
voice O
of O
a O
tar- O

get B-DAT
speaker, O
while O
speech O
separation O

refers B-DAT
to O
the O
problem O
of O

separating B-DAT
each O
sound O
source O
in O

a B-DAT
mixture. O
Recently O
pro- O
posed O

audio-only B-DAT
single-channel O
methods O
have O
achieved O

very B-DAT
promising O
results O
[11, O
12, O
13 O

]. B-DAT
However O
the O
task O
still O

remains B-DAT
challenging. O
Additionally, O
audio-only O
systems O

need B-DAT
separate O
models O
in O
order O

to B-DAT
associate O
the O
estimated O
separated O

audio B-DAT
sources O
to O
each O
speaker, O

while B-DAT
vision O
easily O
allow O
that O

in B-DAT
a O
unified O
model. O
Regarding O
audio-visual O
speech O
enhancement O
and O

separa- B-DAT
tion O
methods O
an O
extensive O

review B-DAT
is O
provided O
in O
[14 O

]. B-DAT
Here O
we O
focus O
on O

the B-DAT
deep-learning O
methods O
that O
are O

most B-DAT
related O
to O
the O
present O

work. B-DAT
Our O
first O
architecture O
(Section O
2.1 O

) B-DAT
is O
inspired O
by O
[15], O

where B-DAT
a O
pre-trained O
convolutional O
neural O

network B-DAT
(CNN) O
is O
used O
to O

generate B-DAT
a O
clean O
spectrogram O
from O

silent B-DAT
video O
[16]. O
Rather O
than O

directly B-DAT
computing O
a O
time-frequency O
(T-F) O

mask, B-DAT
ar O
X O

iv B-DAT
:1 O
81 O
1 O

. B-DAT
02 O
48 O

0v B-DAT
3 O

cs B-DAT
.C O
L O

2 B-DAT

M B-DAT

2 B-DAT
01 O
9 O

the B-DAT
mask O
is O
computed O
by O

thresholding B-DAT
the O
estimated O
clean O
spectrogram. O

This B-DAT
approach O
is O
not O
very O

effective B-DAT
since O
the O
pre-trained O
CNN O

is B-DAT
designed O
for O
a O
different O

task B-DAT
(video-to- O
speech O
synthesis). O

In B-DAT
[17] O
a O
CNN O
is O

trained B-DAT
to O
directly O
esti- O
mate O

clean B-DAT
speech O
from O
noisy O
audio O

and B-DAT
input O
video. O
A O
sim- O

ilar B-DAT
model O
is O
used O

in B-DAT
[18], O
where O
the O
model O

jointly B-DAT
generates O
clean O
speech O
and O

input B-DAT
video O
in O
a O
denoising-autoender O

archi- B-DAT
tecture. O
[19] O
shows O
that O
using O
information O

about B-DAT
lip O
positions O
can O
help O

to B-DAT
improve O
speech O
enhancement. O
The O

video B-DAT
feature O
vec- O
tor O
is O

obtained B-DAT
computing O
pair-wise O
distances O
between O

any B-DAT
mouth O
landmarks. O
Similarly O
to O

our B-DAT
approach O
their O
visual O
fea O

- B-DAT
tures O
are O
not O
learned O

on B-DAT
the O
audio-visual O
dataset O
but O

are B-DAT
pro- O
vided O
by O
a O

system B-DAT
trained O
on O
different O
dataset. O

Contrary B-DAT
to O
our O
approach, O
[19] O

uses B-DAT
position-based O
features O
while O
we O

use B-DAT
motion O
features O
(of O
the O

whole B-DAT
face) O
that O
in O
our O

experiments B-DAT
turned O
out O
to O
be O

much B-DAT
more O
effective O
than O
positional O

features. B-DAT
Although O
the O
aforementioned O
audio-visual O
methods O

work B-DAT
well, O
they O
have O
only O

been B-DAT
evaluated O
in O
a O
speaker-dependent O

setting. B-DAT
Only O
the O
availability O
of O

new B-DAT
large O
and O
heterogeneous O
audio-visual O

datasets B-DAT
has O
allowed O
the O
training O

of B-DAT
deep O
neu- O
ral O
network-based O

speaker-independent B-DAT
speech O
enhancement O
models O
[20 O

, B-DAT
21, O
22]. O
The O
present O
work O
shows O
that O

huge B-DAT
audio-visual O
datasets O
are O
not O

a B-DAT
necessary O
requirement O
for O
speaker-independent O

audio-visual B-DAT
speech O
enhancement. O
Although O
we O

have B-DAT
only O
considered O
datasets O
with O

simple B-DAT
visual O
scenarios O
(i.e., O
the O

target B-DAT
speaker O
is O
always O
facing O

the B-DAT
camera), O
we O
expect O
our O

methods B-DAT
to O
perform O
well O
in O

more B-DAT
complex O
scenarios O
thanks O
to O

the B-DAT
robust O
landmark O
extraction O

. B-DAT
2. O
MODEL O
ARCHITECTURES O

We B-DAT
experimented O
with O
the O
four O

models B-DAT
shown O
in O
Fig. O
1. O

All B-DAT
models O
receive O
in O
input O

the B-DAT
target O
speaker’s O
landmark O
mo- O

tion B-DAT
vectors O
and O
the O
power-law O

compressed B-DAT
spectrogram O
of O
the O
single-channel O

mixed-speech B-DAT
signal. O
All O
of O
them O

perform B-DAT
some O
kind O
of O
masking O

operation. B-DAT
2.1. O
VL2M O
model O

At B-DAT
each O
time O
frame, O
the O

video-landmark B-DAT
to O
mask O
(VL2M) O
model O
( O

Fig. B-DAT
1a) O
estimates O
a O
T-F O

mask B-DAT
from O
visual O
features O
only O
( O

of B-DAT
the O
target O
speaker). O
Formally, O

given B-DAT
a O
video O
sequence O

= B-DAT
[v1 O

, B-DAT
. O
. O
. O
, O

vT B-DAT
], O
vt O
∈ O
Rn O

and B-DAT
a O
target O
mask O
sequence O

= B-DAT
[m1 O

, B-DAT
. O
. O
. O
, O

mT B-DAT
], O
mt O
∈ O
Rd, O

VL2M B-DAT
perform O
a O
function O

Fvl2m(v) B-DAT
= O
m̂, O
where O
m̂ O

is B-DAT
the O
estimated O
mask. O
The O
training O
objective O
for O
VL2M O

is B-DAT
a O
Target O
Binary O
Mask O

(TBM) B-DAT
[23, O
24], O
computed O
using O

the B-DAT
spectrogram O
of O
the O
tar O

- B-DAT
get O
speaker O
only. O
This O

is B-DAT
motivated O
by O
our O
goal O

of B-DAT
extracting O
the O
speech O
of O

a B-DAT
target O
speaker O
as O
much O

as B-DAT
possible O
indepen- O
dently O
of O

the B-DAT
concurrent O
speakers, O
so O
that, O

e.g., B-DAT
we O
do O
not O
need O

to B-DAT
estimate O
their O
number. O
An O

additional B-DAT
motivations O
is O
that O
the O

model B-DAT
takes O
as O
only O
input O

the B-DAT
visual O
features O
of O
the O
target O
speaker, O
and O
a O
target O

TBM B-DAT
that O
only O
depends O
on O

the B-DAT
target O
speaker O
allows O
VL2M O

to B-DAT
learn O
a O
function O
(rather O

than B-DAT
approximating O
an O
ill-posed O
one-to-many O

mapping B-DAT

). B-DAT
Given O
a O
clean O
speech O
spectrogram O

of B-DAT
a O
speaker O
s O

= B-DAT
[s1 O

, B-DAT
. O
. O
. O
, O

sT B-DAT
], O
st O
∈ O
Rd, O

the B-DAT
TBM O
is O
defined O
by O

comparing, B-DAT
at O
each O
frequency O
bin O

∈ B-DAT
[1 O

, B-DAT
. O
. O
. O
, O

d], B-DAT
the O
target O
speaker O
value O

st[f B-DAT
] O
vs. O
a O
reference O

threshold B-DAT
τ O
[f O
]. O
As O

in B-DAT
[15], O
we O
use O
a O

function B-DAT
of O
long-term O
average O
speech O

spectrum B-DAT
(LTASS) O
as O
reference O
threshold. O

This B-DAT
threshold O
indicates O
if O
a O

T-F B-DAT
unit O
is O
generated O
by O

the B-DAT
speaker O
or O
refers O
to O

silence B-DAT
or O
noise. O
The O
process O

to B-DAT
compute O
the O
speaker’s O
TBM O

is B-DAT
as O
follows: O
1. O
The O
mean O
π[f O

] B-DAT
and O
the O
standard O
deviation O

σ[f B-DAT
] O
are O
computed O
for O

all B-DAT
frequency O
bins O
of O
all O

seen B-DAT
spectro- O
grams O
in O
speaker’s O

data B-DAT

. B-DAT
2. O
The O
threshold O
τ O
[f O

] B-DAT
is O
defined O
as O
τ O

[f B-DAT
] O
= O
π[f O
]+0.6 O

·σ[f B-DAT
] O
where O
0.6 O
is O

a B-DAT
value O
selected O
by O
manual O

inspection B-DAT
of O
several O
spectrogram-TBM O
pairs O

. B-DAT
3. O
The O
threshold O
is O
applied O

to B-DAT
every O
speaker’s O
speech O
spec O

- B-DAT
trogram O
s. O
mt[f O

1, B-DAT
if O
st[f O
] O
≥ O

τ B-DAT
[f O
], O
0, O
otherwise. O
The O
mapping O
Fvl2m(·) O
is O
carried O

out B-DAT
by O
a O
stacked O
bi O

- B-DAT
directional O
Long O
Short-Term O
Memory O
( O

BLSTM) B-DAT
network O
[25]. O
The O
BLSTM O

outputs B-DAT
are O
then O
forced O
to O

lay B-DAT
within O
the O
[0, O
1] O

range. B-DAT
Finally O
the O
computed O
TBM O

m̂ B-DAT
and O
the O
noisy O
spectrogram O

y B-DAT
are O
element-wise O
multiplied O
to O

ob- B-DAT
tain O
the O
estimated O
clean O

spectrogram B-DAT
ŝm O
= O
m̂ O
◦ O

y, B-DAT
where O

= B-DAT
[y1 O

, B-DAT
. O
. O
. O

yT B-DAT
], O
yt O
∈ O
Rd. O
The O
model O
parameters O
are O
estimated O

to B-DAT
minimize O
the O
loss O

: B-DAT
Jvl2m O
= O
∑T O

t=1 B-DAT
∑d O
f=1−mt[f O
] O
· O
log(m̂t[f O

])− B-DAT
(1−mt[f O
]) O
· O
log(1 O

− B-DAT
m̂t[f O
]) O
2.2. O
VL2M O
ref O
model O

VL2M B-DAT
generates O
T-F O
masks O
that O

are B-DAT
independent O
of O
the O
acous- O

tic B-DAT
context. O
We O
may O
want O

to B-DAT
refine O
the O
masking O
by O

including B-DAT
such O
context. O
This O
is O

what B-DAT
the O
novel O
VL2M O
ref O

does B-DAT
(Fig. O
1b). O
The O
computed O

TBM B-DAT
m̂ O
and O
the O
input O

spectrogram B-DAT
y O
are O
the O
input O

to B-DAT
a O
function O
that O
outputs O

an B-DAT
Ideal O
Amplitude O
Mask O
(IAM) O

p B-DAT
(known O
as O
FFT-MASK O

in B-DAT
[8]). O
Given O
the O
target O

clean B-DAT
spectrogram O
s O
and O
the O

noisy B-DAT
spectrogram O
y, O
the O
IAM O

is B-DAT
defined O
as: O
pt[f O
] O
= O
st[f O

yt[f B-DAT
] O
Note O
that O
although O
IAM O
generation O

requires B-DAT
the O
mixed-speech O
spectrogram, O
separate O

spectrograms B-DAT
for O
each O
concurrent O
speakers O

are B-DAT
not O
required O

. B-DAT
The O
target O
speaker’s O
spectrogram O
s O

is B-DAT
reconstructed O
by O
multiplying O
the O

input B-DAT
spectrogram O
with O
the O
estimated O

IAM. B-DAT
Values O
greater O
than O
10 O

in B-DAT
the O
IAM O
are O
clipped O

to B-DAT
10 O
in O
order O
to O

obtain B-DAT
better O
numerical O
stability O
as O

suggested B-DAT
in O
[8 O

v: B-DAT
video O
input O
y: O
noisy O

spectrogram B-DAT
sm: O
clean O
spectrogram O
TBM O

s: B-DAT
clean O
spectrogram O
IAM O
m: O

TBM B-DAT
p: O
IAM O
STACKED O

BLSTM B-DAT
m O

sm B-DAT
v O

y B-DAT
(a) O
VL2M O

v B-DAT
VL2M O
m O
y O
BLSTM O

BLSTM B-DAT
Fusion O
layer O

BLSTM B-DAT
p O
s O

b) B-DAT
VL2M O
ref O
v O

y B-DAT
p O
STACKED O

BLSTM B-DAT
s O

c) B-DAT
Audio-Visual O
concat O
sm O

y B-DAT
p O
STACKED O

BLSTM B-DAT
s O

v B-DAT
VL2M O
m O
(d) O
Audio-Visual O
concat-ref O

Fig. B-DAT
1. O
Model O
architectures. O
The O
model O
performs O
a O
function O

Fmr(v, B-DAT
y) O
= O
p̂ O
that O

con- B-DAT
sists O
of O
a O
VL2M O

component B-DAT
plus O
three O
different O
BLSTMs O

Gm, B-DAT
Gy O
and O
H O

Gm(Fvl2m(v)) B-DAT
= O
rm O
receives O
the O

VL2M B-DAT
mask O
m̂ O
as O
in- O

put, B-DAT
and O
Gy(y) O
= O
ry O

is B-DAT
fed O
with O
the O
noisy O

spectrogram. B-DAT
Their O
output O
rm, O

ry B-DAT
∈ O
Rz O
are O
fused O

in B-DAT
a O
joint O
audio-visual O
represen- O

tation B-DAT

= B-DAT
[h1 O

, B-DAT
. O
. O
. O
, O

hT B-DAT
], O
where O
ht O
is O

a B-DAT
linear O
combination O
of O
rmt O

and B-DAT
ryt O
: O
ht O
= O

Whm B-DAT
·rmt O
+Why O
·ryt O
+bh. O

h B-DAT
is O
the O
input O
of O

the B-DAT
third O
BLSTM O
H O
( O

h) B-DAT
= O
p̂, O
where O
p̂ O

lays B-DAT
in O
the O
[0,10] O
range. O

The B-DAT
loss O
function O
is: O
Jmr O

T∑ B-DAT
t=1 O
d∑ O
f=1 O

p̂t[f B-DAT
] O
· O
yt[f O
]− O

st[f B-DAT
])2 O
2.3. O
Audio-Visual O
concat O
model O

The B-DAT
third O
model O
(Fig. O
1c) O

performs B-DAT
early O
fusion O
of O
audio- O

visual B-DAT
features. O
This O
model O
consists O

of B-DAT
a O
single O
stacked O
BLSTM O

that B-DAT
computes O
the O
IAM O
mask O

p̂ B-DAT
from O
the O
concate- O

nated B-DAT
[v,y]. O
The O
training O
loss O

is B-DAT
the O
same O
Jmr O
used O

to B-DAT
train O
VL2M O
ref. O
This O

model B-DAT
can O
be O
regarded O
as O

a B-DAT
simplification O
of O
VL2M O
ref, O

where B-DAT
the O
VL2M O
operation O
is O

not B-DAT
performed. O
2.4. O
Audio-Visual O
concat-ref O
model O

The B-DAT
fourth O
model O
(Fig. O
1d) O

is B-DAT
an O
improved O
version O
of O

the B-DAT
model O
described O
in O
section O
2 O

.3. B-DAT
The O
only O
difference O
is O

the B-DAT
input O
of O
the O
stacked O

BLSTM B-DAT
that O
is O
replaced O

by B-DAT
[̂sm,y] O
where O
ŝm O
is O

the B-DAT
denoised O
spectrogram O
returned O
by O

VL2M B-DAT
operation. O
3. O
EXPERIMENTAL O
SETUP O

3.1. B-DAT
Dataset O
All O
experiments O
were O
carried O
out O

using B-DAT
the O
GRID O
[9] O
and O

TCD-TIMIT B-DAT
[10] O
audio-visual O
datasets. O
For O

each B-DAT
of O
them, O
we O
created O

a B-DAT
mixed-speech O
version O

. B-DAT
Regarding O
the O
GRID O
corpus, O
for O

each B-DAT
of O
the O
33 O
speakers O

(one B-DAT
had O
to O
be O
discarded O

) B-DAT
we O
first O
randomly O
selected O
200 O
ut- O
terances O
(out O
of O
1000 O

). B-DAT
Then, O
for O
each O
utterance, O

we B-DAT
created O
3 O
different O
audio-mixed O

samples. B-DAT
Each O
audio-mixed O
sample O
was O

created B-DAT
by O
mixing O
the O
chosen O

utterance B-DAT
with O
one O
utter- O
ance O

from B-DAT
a O
different O
speaker. O
That O
resulted O
in O
600 O
audio-mixed O

samples B-DAT
per O
speaker O

. B-DAT
The O
resulting O
dataset O
was O
split O

into B-DAT
disjoint O
sets O
of O
25/4/4 O

speakers B-DAT
for O
training/validation/testing O
respectively O

. B-DAT
The O
TCD-TIMIT O
corpus O
consists O
of O

59 B-DAT
speakers O
(we O
ex- O
cluded O

3 B-DAT
professionally-trained O
lipspeakers) O
and O
98 O

utterances B-DAT
per O
speaker. O
The O
mixed-speech O

version B-DAT
was O
created O
following O
the O

same B-DAT
procedure O
as O
for O
GRID O

, B-DAT
with O
one O
difference. O
Con- O

trary B-DAT
to O
GRID, O
TCD-TIMIT O
utterances O

have B-DAT
different O
dura- O
tion. O
Thus O
2 O
utterances O
were O
mixed O
only O
if O

their B-DAT
duration O
dif- O
ference O
did O

not B-DAT
exceed O
2 O
seconds. O
For O

each B-DAT
utterance O
pair, O
we O
forced O

the B-DAT
non-target O
speaker’s O
utterance O
to O

match B-DAT
the O
du- O
ration O
of O

the B-DAT
target O
speaker O
utterance. O
If O

it B-DAT
was O
longer, O
the O
utterance O

was B-DAT
cut O
at O
its O
end O

, B-DAT
whereas O
if O
it O
was O

shorter, B-DAT
silence O
samples O
were O
equally O

added B-DAT
at O
its O
start O
and O

end. B-DAT
The O
resulting O
dataset O
was O
split O

into B-DAT
disjoint O
sets O
of O
51/4/4 O

speakers B-DAT
for O
training/validation/testing O
respectively O

. B-DAT
3.2. O
LSTM O
training O

In B-DAT
all O
experiments, O
the O
models O

were B-DAT
trained O
using O
the O
Adam O

optimizer B-DAT
[26]. O
Early O
stopping O
was O

applied B-DAT
when O
the O
error O
on O

the B-DAT
validation O
set O
did O
not O

decrease B-DAT
over O
5 O
consecutive O
epochs. O
VL2M, O
AV O
concat O
and O
AV O

concat-ref B-DAT
had O
5, O
3 O
and O

3 B-DAT
stacked O
BLSTM O
layers O
respectively O

. B-DAT
All O
BLSTMs O
had O
250 O

units. B-DAT
Hyper-parameters O
selection O
was O
performed O

by B-DAT
using O
random O
search O
with O

a B-DAT
limited O
number O
of O
samples, O

therefore B-DAT
all O
the O
reported O
results O

may B-DAT
improve O
through O
a O
deeper O

hyper- B-DAT
parameters O
validation O
phase. O
VL2M O
ref O
and O
AV O
concat-ref O

training B-DAT
was O
performed O
in O
2 O

steps. B-DAT
We O
first O
pre-trained O
the O

models B-DAT
using O
the O
oracle O
TBM O

m. B-DAT
Then O
we O
substituted O
the O

oracle B-DAT
masks O
with O
the O
VL2M O

component B-DAT
and O
retrained O
the O
models O

while B-DAT
freezing O
the O
pa- O
rameters O

of B-DAT
the O
VL2M O
component O

. B-DAT
3.3. O
Audio O
pre- O
and O
post-processing O

The B-DAT
original O
waveforms O
were O
resampled O

to B-DAT
16 O
kHz. O
Short- O
Time O

Fourier B-DAT
Transform O
(STFT) O
x O
was O

computed B-DAT
using O
FFT O
size O
of O
512, O
Hann O
window O
of O
length O
25 O

ms B-DAT
(400 O
samples), O
and O
hop O

length B-DAT
of O
10 O
ms O
(160 O

samples). B-DAT
The O
input O
spectro- O
gram O

was B-DAT
obtained O
taking O
the O
STFT O

magnitude B-DAT
and O
perform- O
ing O
power-law O

compression B-DAT
|x|p O
with O
p O

= B-DAT
0.3. O
Finally O
we O
applied O

per-speaker B-DAT
0-mean O
1-std O
normalization O

. B-DAT
In O
the O
post-processing O
stage, O
the O

enhanced B-DAT
waveform O
gen- O
erated O
by O

the B-DAT
speech O
enhancement O
models O
was O

reconstructed B-DAT

SDR B-DAT
PESQ O
ViSQOL O
Noisy O
−1.06 O
1.81 O
2.11 O
VL2M O

3.17 B-DAT
1.51 O
1.16 O
VL2M O
ref O

6.50 B-DAT
2.58 O
2.99 O
AV O
concat O

6.31 B-DAT
2.49 O
2.83 O
AV O
c-ref O

6.17 B-DAT
2.58 O
2.96 O

Table B-DAT
1. O
GRID O
results O
- O

speaker-dependent. B-DAT
The O
“Noisy” O
row O
refers O

to B-DAT
the O
metric O
values O
of O

the B-DAT
input O
mixed-speech O
signal. O
2 O
Speakers O
3 O
Speakers O
SDR O

PESQ B-DAT
ViSQOL O
SDR O
PESQ O
ViSQOL O

Noisy B-DAT
0.21 O
1.94 O
2.58 O
−5.34 O
1 O

.43 B-DAT
1.62 O
VL2M O
3.02 O
1.81 O
1 O

.70 B-DAT
−2.03 O
1.43 O
1.25 O
VL2M O

ref B-DAT
6.52 O
2.53 O
3.02 O
2.83 O
2 O

.19 B-DAT
2.53 O
AV O
concat O
7.37 O
2 O

.65 B-DAT
3.03 O
3.02 O
2.24 O
2.49 O

AV B-DAT
c-ref O
8.05 O
2.70 O
3.07 O
4 O

.02 B-DAT
2.33 O
2.64 O
Table O
2. O
GRID O
results O

- B-DAT
speaker-independent O

. B-DAT
by O
applying O
the O
inverse O
STFT O

to B-DAT
the O
estimated O
clean O
spectro O

- B-DAT
gram O
and O
using O
the O

phase B-DAT
of O
the O
noisy O
input O

signal. B-DAT
3.4. O
Video O
pre-processing O

Face B-DAT
landmarks O
were O
extracted O
from O

video B-DAT
using O
the O
Dlib O
[7] O

implementation B-DAT
of O
the O
face O
landmark O

estimator B-DAT
described O
in O
[6]. O
It O

returns B-DAT
68 O
x-y O
points, O
for O

an B-DAT
overall O
136 O
values. O
We O

upsampled B-DAT
from O
25/29.97 O
fps O
(GRID/TCD-TIMIT) O

to B-DAT
100 O
fps O
to O
match O

the B-DAT
frame O
rate O
of O
the O

audio B-DAT
spectrogram. O
Upsampling O
was O
carried O

out B-DAT
through O
linear O
interpolation O
over O

time. B-DAT
The O
final O
video O
feature O
vector O

v B-DAT
was O
obtained O
by O
com O

- B-DAT
puting O
the O
per-speaker O
normalized O

motion B-DAT
vector O
of O
the O
face O

landmarks B-DAT
by O
simply O
subtracting O
every O

frame B-DAT
with O
the O
previ- O
ous O

one. B-DAT
The O
motion O
vector O
of O

the B-DAT
first O
frame O
was O
set O

to B-DAT
zero. O
4. O
RESULTS O

In B-DAT
order O
to O
compare O
our O

models B-DAT
to O
previous O
works O
in O

both B-DAT
speech O
enhancement O
and O
separation, O

we B-DAT
evaluated O
the O
perfor- O
mance O

of B-DAT
the O
proposed O
models O
using O

both B-DAT
speech O
separation O
2 O
Speakers O
3 O
Speakers O
SDR O

PESQ B-DAT
ViSQOL O
SDR O
PESQ O
ViSQOL O

Noisy B-DAT
0.21 O
2.22 O
2.74 O
−3.42 O
1 O

.92 B-DAT
2.04 O
VL2M O
2.88 O
2.25 O
2 O

.62 B-DAT
−0.51 O
1.99 O
1.98 O
VL2M O

ref B-DAT
9.24 O
2.81 O
3.09 O
5.27 O
2 O

.44 B-DAT
2.54 O
AV O
concat O
9.56 O
2 O

.80 B-DAT
3.09 O
5.15 O
2.41 O
2.52 O

AV B-DAT
c-ref O
10.55 O
3.03 O
3.21 O
5 O

.37 B-DAT
2.45 O
2.58 O
Table O
3. O
TCD-TIMIT O
results O

- B-DAT
speaker-independent O

. B-DAT
and O
enhancement O
metrics. O
Specifically, O
we O

measured B-DAT
the O
ca- O
pability O
of O

separating B-DAT
the O
target O
utterance O
from O

the B-DAT
concurrent O
utterance O
with O
the O

source-to-distortion B-DAT
ratio O
(SDR) O
[27, O
28 O

]. B-DAT
While O
the O
quality O
of O

estimated B-DAT
target O
speech O
was O
measured O

with B-DAT
the O
perceptual O
PESQ O
[29] O

and B-DAT
ViSQOL O
[30] O
metrics. O
For O

PESQ B-DAT
we O
used O
the O
narrow O

band B-DAT
mode O
while O
for O
ViSQOL O

we B-DAT
used O
the O
wide O
band O

mode. B-DAT
As O
a O
very O
first O
experiment O

we B-DAT
compared O
landmark O
posi- O
tion O

vs. B-DAT
landmark O
motion O
vectors. O
It O

turned B-DAT
out O
that O
landmark O
positions O

performed B-DAT
poorly, O
thus O
all O
results O

reported B-DAT
here O
refer O
to O
landmark O

motion B-DAT
vectors O
only O

. B-DAT
We O
then O
carried O
out O
some O

speaker-dependent B-DAT
experiments O
to O
compare O
our O

models B-DAT
with O
previous O
studies O
as O

, B-DAT
to O
the O
best O
of O

our B-DAT
knowledge, O
there O
are O
no O

reported B-DAT
results O
of O
speaker- O
independent O

systems B-DAT
trained O
and O
tested O
on O

GRID B-DAT
and O
TCD- O
TIMIT O
to O

compare B-DAT
with. O
Table O
1 O
reports O

the B-DAT
test-set O
evalua- O
tion O
of O

speaker-dependent B-DAT
models O
on O
the O
GRID O

corpus B-DAT
with O
landmark O
motion O
vectors. O

Results B-DAT
are O
comparable O
with O
previ- O

ous B-DAT
state-of-the-art O
studies O
in O
an O

almost B-DAT
identical O
setting O
[15, O
17]. O
Table O
2 O
and O
3 O
show O

speaker-independent B-DAT
test-set O
results O
on O
the O

GRID B-DAT
and O
TCD-TIMIT O
datasets O
respectively O

. B-DAT
V2ML O
performs O
significantly O
worse O

than B-DAT
the O
other O
three O
models O

in- B-DAT
dicating O
that O
a O
successful O

mask B-DAT
generation O
has O
to O
depend O

on B-DAT
the O
acoustic O
context. O
The O

performance B-DAT
of O
the O
three O
models O

in B-DAT
the O
speaker-independent O
setting O
is O

comparable B-DAT
to O
that O
in O
the O

speaker-dependent B-DAT
setting. O
AV O
concat-ref O
outperforms O
V2ML O
ref O

and B-DAT
AV O
concat O
for O
both O

datasets. B-DAT
This O
supports O
the O
utility O

of B-DAT
a O
refinement O
strat- O
egy O

and B-DAT
suggests O
that O
the O
refinement O

is B-DAT
more O
effective O
when O
it O

directly B-DAT
refines O
the O
estimated O
clean O

spectrogram, B-DAT
rather O
than O
refining O
the O

estimated B-DAT
mask O

. B-DAT
Finally, O
we O
evaluated O
the O
systems O

in B-DAT
a O
more O
challenging O
testing O

condition B-DAT
where O
the O
target O
utterance O

was B-DAT
mixed O
with O
2 O
utterances O

from B-DAT
2 O
competing O
speakers. O
Despite O

the B-DAT
model O
was O
trained O
with O

mixtures B-DAT
of O
two O
speakers, O
the O

decrease B-DAT
of O
performance O
was O
not O

dramatic B-DAT

. B-DAT
Code O
and O
some O
testing O
examples O

of B-DAT
our O
models O
are O
avail O

- B-DAT
able O
at O
https://goo.gl/3h1NgE. O
5. O
CONCLUSION O

This B-DAT
paper O
proposes O
the O
use O

of B-DAT
face O
landmark O
motion O
vec- O

tors B-DAT
for O
audio-visual O
speech O
enhancement O

in B-DAT
a O
single-channel O
multi-talker O
scenario. O

Different B-DAT
models O
are O
tested O
where O

land- B-DAT
mark O
motion O
vectors O
are O

used B-DAT
to O
generate O
time-frequency O
(T- O

F) B-DAT
masks O
that O
extract O
the O

target B-DAT
speaker’s O
spectrogram O
from O
the O

acoustic B-DAT
mixed-speech O
spectrogram. O
To O
the O
best O
of O
our O

knowledge, B-DAT
some O
of O
the O
proposed O

mod- B-DAT
els O
are O
the O
first O

models B-DAT
trained O
and O
evaluated O
on O

the B-DAT
limited O
size O
GRID O
and O

TCD-TIMIT B-DAT
datasets O
that O
accomplish O
speaker O

- B-DAT
independent O
speech O
enhancement O
in O

the B-DAT
multi-talker O
setting, O
with O
a O

quality B-DAT
of O
enhancement O
comparable O
to O

that B-DAT
achieved O
in O
a O
speaker-dependent O

setting. B-DAT
https://goo.gl/3h1NgE O

6. B-DAT
REFERENCES O
[1] O
E. O
Colin O
Cherry, O
“Some O

experiments B-DAT
on O
the O
recognition O
of O

speech, B-DAT
with O
one O
and O
with O

two B-DAT
ears,” O
The O
Journal O
of O

the B-DAT
Acoustical O
Society O
of O
America O

, B-DAT
vol. O
25, O
no. O
5, O

pp. B-DAT
975–979, O
1953. O
[2] O
Josh O
H O
McDermott, O
“The O

cocktail B-DAT
party O
problem,” O
Current O
Biology O

, B-DAT
vol. O
19, O
no. O
22, O

pp. B-DAT
R1024–R1027, O
2009. O
[3] O
Elana O
Zion O
Golumbic, O
Gregory O

B. B-DAT
Cogan, O
Charles O
E. O
Schroeder O

, B-DAT
and O
David O
Poeppel, O
“Visual O

input B-DAT
enhances O
selective O
speech O
envelope O

tracking B-DAT
in O
auditory O
cortex O
at O

a B-DAT
“cocktail O
party”,” O
Journal O
of O

Neu- B-DAT
roscience, O
vol. O
33, O
no. O
4, O
pp. O
1417–1426, O
2013 O

. B-DAT
[4] O
Wei O
Ji O
Ma, O
Xiang O

Zhou, B-DAT
Lars O
A. O
Ross, O
John O

J. B-DAT
Foxe, O
and O
Lucas O
C O

. B-DAT
Parra, O
“Lip-reading O
aids O
word O

recognition B-DAT
most O
in O
moderate O
noise: O

A B-DAT
bayesian O
explanation O
using O
high-dimensional O

feature B-DAT
space,” O
PLOS O
ONE, O
vol. O
4, O
no. O
3, O
pp. O
1–14, O
03 O

2009 B-DAT

. B-DAT
[5] O
Albert O
S O
Bregman, O
Auditory O

scene B-DAT
analysis: O
The O
perceptual O
organi O

- B-DAT
zation O
of O
sound, O
MIT O

press, B-DAT
1994. O
[6] O
Vahid O
Kazemi O
and O
Josephine O

Sullivan, B-DAT
“One O
millisecond O
face O
align O

- B-DAT
ment O
with O
an O
ensemble O

of B-DAT
regression O
trees,” O
in O
The O

IEEE B-DAT
Conference O
on O
Computer O
Vision O

and B-DAT
Pattern O
Recognition O
(CVPR), O
June O
2014 O

. B-DAT
[7] O
Davis O
E. O
King, O
“Dlib-ml O

: B-DAT
A O
machine O
learning O
toolkit,” O

Journal B-DAT
of O
Machine O
Learning O
Research, O

vol. B-DAT
10, O
pp. O
1755–1758, O
2009. O
[8] O
Yuxuan O
Wang, O
Arun O
Narayanan O

, B-DAT
and O
DeLiang O
Wang, O
“On O

Training B-DAT
Targets O
for O
Supervised O
Speech O

Separation,” B-DAT
IEEE/ACM O
Transactions O
on O
Audio, O

Speech, B-DAT
and O
Language O
Processing, O
vol. O
22, O
no. O
12, O
pp. O
1849–1858, O
Dec O

. B-DAT
2014. O
[9] O
Martin O
Cooke, O
Jon O
Barker O

, B-DAT
Stuart O
Cunningham, O
and O
Xu O

Shao, B-DAT
“An O
audio-visual O
corpus O
for O

speech B-DAT
perception O
and O
automatic O
speech O

recognition,” B-DAT
The O
Journal O
of O
the O

Acoustical B-DAT
Society O
of O
America, O
vol. O
120, O
no. O
5, O
pp. O
2421–2424, O
Nov O

. B-DAT
2006. O
[10] O
Naomi O
Harte O
and O
Eoin O

Gillen, B-DAT
“TCD-TIMIT: O
An O
Audio-Visual O
Cor O

- B-DAT
pus O
of O
Continuous O
Speech,” O

IEEE B-DAT
Transactions O
on O
Multimedia, O
vol. O
17, O
no. O
5, O
pp. O
603–615, O
May O

2015 B-DAT

. B-DAT
[11] O
Z. O
Chen, O
Y. O
Luo O

, B-DAT
and O
N. O
Mesgarani, O
“Deep O

attractor B-DAT
network O
for O
single-microphone O
speaker O

separation,” B-DAT
in O
2017 O
IEEE O
International O

Conference B-DAT
on O
Acoustics, O
Speech O
and O

Signal B-DAT
Processing O
(ICASSP), O
March O
2017, O

pp. B-DAT
246–250. O
[12] O
Yusuf O
Isik, O
Jonathan O
Le O

Roux, B-DAT
Zhuo O
Chen, O
Shinji O
Watanabe O

, B-DAT
and O
John O
R. O

Hershey, B-DAT
“Single-channel O
multi-speaker O
separation O
using O

deep B-DAT
clustering,” O
in O
Interspeech, O
2016. O
[13] O
Morten O
Kolbaek, O
Dong O
Yu O

, B-DAT
Zheng-Hua O
Tan, O
Jesper O
Jensen, O

Morten B-DAT
Kolbaek, O
Dong O
Yu, O
Zheng-Hua O

Tan, B-DAT
and O
Jesper O
Jensen, O
“Multitalker O

speech B-DAT
separation O
with O
utterance-level O
permutation O

invariant B-DAT
training O
of O
deep O
recurrent O

neural B-DAT
networks,” O
IEEE/ACM O
Trans. O
Audio, O

Speech B-DAT
and O
Lang. O
Proc., O
vol. O
25, O
no. O
10, O
pp. O
1901–1913, O
Oct O

. B-DAT
2017. O
[14] O
Bertrand O
Rivet, O
Wenwu O
Wang O

, B-DAT
Syed O
Mohsen O
Naqvi, O
and O

Jonathon B-DAT
Chambers, O
“Audiovisual O
Speech O
Source O

Separation: B-DAT
An O
overview O
of O
key O

methodologies,” B-DAT
IEEE O
Signal O
Processing O
Magazine, O

vol. B-DAT
31, O
no. O
3, O
pp. O
125 O

–134, B-DAT
May O
2014. O
[15] O
Aviv O
Gabbay, O
Ariel O
Ephrat O

, B-DAT
Tavi O
Halperin, O
and O
Shmuel O

Peleg, B-DAT
“Seeing O
through O
noise: O
Visually O

driven B-DAT
speaker O
separation O
and O
enhancement,” O

in B-DAT
ICASSP. O
2018, O
pp. O
3051–3055, O

IEEE. B-DAT
[16] O
Ariel O
Ephrat, O
Tavi O
Halperin O

, B-DAT
and O
Shmuel O
Peleg, O
“Improved O

speech B-DAT
reconstruction O
from O
silent O
video,” O

ICCV B-DAT
2017 O
Workshop O
on O
Computer O

Vision B-DAT
for O
Audio-Visual O
Media, O
2017. O
[17] O
Aviv O
Gabbay, O
Asaph O
Shamir O

, B-DAT
and O
Shmuel O
Peleg, O
“Visual O

speech B-DAT
en- O
hancement,” O
in O
Interspeech. O
2018, O
pp. O
1170–1174, O
ISCA O

. B-DAT
[18] O
Jen-Cheng O
Hou, O
Syu-Siang O
Wang O

, B-DAT
Ying-Hui O
Lai, O
Yu O
Tsao, O

Hsiu-Wen B-DAT
Chang, O
and O
Hsin-Min O

Wang, B-DAT
“Audio-Visual O
Speech O
Enhancement O
Us- O

ing B-DAT
Multimodal O
Deep O
Convolutional O
Neural O

Networks,” B-DAT
IEEE O
Trans- O
actions O
on O

Emerging B-DAT
Topics O
in O
Computational O
Intelligence, O

vol. B-DAT
2, O
no. O
2, O
pp. O
117 O

–128, B-DAT
Apr. O
2018. O
[19] O
Jen-Cheng O
Hou, O
Syu-Siang O
Wang O

, B-DAT
Ying-Hui O
Lai, O
Jen-Chun O
Lin, O

Yu B-DAT
Tsao, O
Hsiu-Wen O
Chang, O
and O

Hsin-Min B-DAT
Wang, O
“Audio-visual O
speech O
enhancement O

using B-DAT
deep O
neural O
networks,” O
in O
2016 O
Asia- O
Pacific O
Signal O
and O
Information O

Processing B-DAT
Association O
Annual O
Sum- O
mit O

and B-DAT
Conference O
(APSIPA), O
Jeju, O
South O

Korea, B-DAT
Dec. O
2016, O
pp. O
1–6 O

, B-DAT
IEEE. O
[20] O
Ariel O
Ephrat, O
Inbar O
Mosseri O

, B-DAT
Oran O
Lang, O
Tali O
Dekel, O

Kevin B-DAT
Wilson, O
Avinatan O
Hassidim, O
William O

T. B-DAT
Freeman, O
and O
Michael O

Rubinstein, B-DAT
“Looking O
to O
Listen O
at O

the B-DAT
Cocktail O
Party: O
A O
Speaker-Independent O

Audio-Visual B-DAT
Model O
for O
Speech O
Separation,” O

ACM B-DAT
Transactions O
on O
Graphics, O
vol. O
37, O
no. O
4, O
pp. O
1–11, O
July O

2018, B-DAT
arXiv: O
1804.03619 O

. B-DAT
[21] O
T. O
Afouras, O
J. O
S O

. B-DAT
Chung, O
and O
A. O

Zisserman, B-DAT
“The O
conversation: O
Deep O
audio-visual O

speech B-DAT
enhancement,” O
in O
Interspeech, O
2018. O
[22] O
Andrew O
Owens O
and O
Alexei O

A B-DAT
Efros, O
“Audio-visual O
scene O
analysis O

with B-DAT
self-supervised O
multisensory O
features,” O
European O

Conference B-DAT
on O
Computer O
Vision O
(ECCV O

), B-DAT
2018. O
[23] O
Michael O
C. O
Anzalone, O
Lauren O

Calandruccio, B-DAT
Karen O
A. O
Doherty, O
and O

Laurel B-DAT
H. O
Carney, O
“Determination O
of O

the B-DAT
potential O
benefit O
of O
time O

- B-DAT
frequency O
gain O
manipulation,” O
Ear O

Hear, B-DAT
vol. O
27, O
no. O
5, O

pp. B-DAT
480–492, O
Oct O
2006, O
16957499[pmid]. O
[24] O
Ulrik O
Kjems, O
Jesper O
B O

. B-DAT
Boldt, O
Michael O
S. O
Pedersen, O

Thomas B-DAT
Lunner, O
and O
DeLiang O

Wang, B-DAT
“Role O
of O
mask O
pattern O

in B-DAT
intelligibility O
of O
ideal O
binary-masked O

noisy B-DAT
speech,” O
The O
Journal O
of O

the B-DAT
Acoustical O
Society O
of O
America, O

vol. B-DAT
126, O
no. O
3, O
pp. O
1415 O

–1426, B-DAT
2009. O
[25] O
A. O
Graves, O
A. O
Mohamed O

, B-DAT
and O
G. O
Hinton, O
“Speech O

recognition B-DAT
with O
deep O
recurrent O
neural O

networks,” B-DAT
in O
2013 O
IEEE O
International O

Con- B-DAT
ference O
on O
Acoustics, O
Speech O

and B-DAT
Signal O
Processing, O
May O
2013, O

pp. B-DAT
6645–6649. O
[26] O
Diederik O
P O
Kingma O
and O

Jimmy B-DAT
Ba, O
“Adam: O
A O
method O

for B-DAT
stochastic O
optimization,” O
arXiv O
preprint O

arXiv:1412.6980, B-DAT
2014 O

. B-DAT
[27] O
E. O
Vincent, O
R. O
Gribonval O

, B-DAT
and O
C. O
Fevotte, O
“Performance O

measure- B-DAT
ment O
in O
blind O
audio O

source B-DAT
separation,” O
IEEE O
Transactions O
on O

Audio, B-DAT
Speech O
and O
Language O
Processing, O

vol. B-DAT
14, O
no. O
4, O
pp. O
1462 O

–1469, B-DAT
July O
2006. O
[28] O
Colin O
Raffel, O
Brian O
McFee O

, B-DAT
Eric O
J O
Humphrey, O
Justin O

Salamon, B-DAT
Oriol O
Nieto, O
Dawen O
Liang, O

Daniel B-DAT
PW O
Ellis, O
and O
C O

Colin B-DAT
Raffel, O
“mir O
eval: O
A O

transparent B-DAT
implementation O
of O
common O
mir O

metrics,” B-DAT
in O
In O
Proceed- O
ings O

of B-DAT
the O
15th O
International O
Society O

for B-DAT
Music O
Information O
Retrieval O
Conference, O

ISMIR. B-DAT
Citeseer, O
2014. O
[29] O
A.W. O
Rix, O
J.G. O
Beerends O

, B-DAT
M.P. O
Hollier, O
and O
A.P. O

Hekstra, B-DAT
“Perceptual O
evaluation O
of O
speech O

quality B-DAT
(PESQ)-a O
new O
method O
for O

speech B-DAT
qual- O
ity O
assessment O
of O

telephone B-DAT
networks O
and O
codecs,” O
in O
2001 O
IEEE O
In- O
ternational O
Conference O
on O

Acoustics, B-DAT
Speech, O
and O
Signal O
Processing O

. B-DAT
Proceedings O
(Cat. O
No.01CH37221), O
Salt O

Lake B-DAT
City, O
UT, O
USA, O
2001, O

vol. B-DAT
2, O
pp. O
749–752, O
IEEE. O
[30] O
A. O
Hines, O
J. O
Skoglund O

, B-DAT
A. O
Kokaram, O
and O
N. O

Harte, B-DAT
“ViSQOL: O
The O
Virtual O
Speech O

Quality B-DAT
Objective O
Listener,” O
in O
IWAENC O
2012 O

; B-DAT
Inter- O
national O
Workshop O
on O

Acoustic B-DAT
Signal O
Enhancement, O
Sept. O
2012, O

pp. B-DAT
1 O

4 B-DAT

1 B-DAT
Introduction O

1.1 B-DAT
Related O
work O

2 B-DAT
MODEL O
ARCHITECTURES O

2.1 B-DAT
VL2M O
model O

2.2 B-DAT
VL2M_ref O
model O

2.3 B-DAT
Audio-Visual O
concat O
model O

2.4 B-DAT
Audio-Visual O
concat-ref O
model O

3 B-DAT
Experimental O
setup O

3.1 B-DAT
Dataset O

3.2 B-DAT
LSTM O
training O

3.3 B-DAT
Audio O
pre- O
and O
post-processing O

3.4 B-DAT
Video O
pre-processing O

4 B-DAT
Results O

5 B-DAT
Conclusion O

6 B-DAT
References O

the O
limited O
size O
GRID O
and O
TCD B-DAT

on O
the O
GRID O
[9] O
and O
TCD B-DAT

using O
the O
GRID O
[9] O
and O
TCD B-DAT

The O
TCD B-DAT

difference. O
Con- O
trary O
to O
GRID, O
TCD B-DAT

TCD B-DAT

Table O
3. O
TCD B-DAT

and O
tested O
on O
GRID O
and O
TCD B-DAT

results O
on O
the O
GRID O
and O
TCD B-DAT

the O
limited O
size O
GRID O
and O
TCD B-DAT

TCD B-DAT

The O
TCD-TIMIT B-DAT
corpus I-DAT
consists O
of O
59 O
speakers O
(we O

are O
applied O
to O
the O
acoustic O
mixed-speech B-DAT
spectrogram. O
Results O
show O
that: O
(i O

landmark O
features O
and O
the O
input O
mixed-speech B-DAT
spectrogram O

applied O
to O
clean O
the O
acoustic O
mixed-speech B-DAT
spectrogram O

compressed O
spectrogram O
of O
the O
single-channel O
mixed-speech B-DAT
signal. O
All O
of O
them O
perform O

although O
IAM O
generation O
requires O
the O
mixed-speech B-DAT
spectrogram, O
separate O
spectrograms O
for O
each O

of O
them, O
we O
created O
a O
mixed-speech B-DAT
version O

98 O
utterances O
per O
speaker. O
The O
mixed-speech B-DAT
version O
was O
created O
following O
the O

metric O
values O
of O
the O
input O
mixed-speech B-DAT
signal O

speaker’s O
spectrogram O
from O
the O
acoustic O
mixed-speech B-DAT
spectrogram O

are O
applied O
to O
the O
acoustic O
mixed-speech B-DAT
spectrogram. O
Results O
show O
that: O
(i O

landmark O
features O
and O
the O
input O
mixed-speech B-DAT
spectrogram O

applied O
to O
clean O
the O
acoustic O
mixed-speech B-DAT
spectrogram O

compressed O
spectrogram O
of O
the O
single-channel O
mixed-speech B-DAT
signal. O
All O
of O
them O
perform O

although O
IAM O
generation O
requires O
the O
mixed-speech B-DAT
spectrogram, O
separate O
spectrograms O
for O
each O

of O
them, O
we O
created O
a O
mixed-speech B-DAT
version O

98 O
utterances O
per O
speaker. O
The O
mixed-speech B-DAT
version O
was O
created O
following O
the O

metric O
values O
of O
the O
input O
mixed-speech B-DAT
signal O

speaker’s O
spectrogram O
from O
the O
acoustic O
mixed-speech B-DAT
spectrogram O

Regarding O
the O
GRID O
corpus, B-DAT
for O
each O
of O
the O
33 O

The O
TCD-TIMIT O
corpus B-DAT
consists O
of O
59 O
speakers O
(we O

speaker-dependent O
models O
on O
the O
GRID O
corpus B-DAT
with O
landmark O
motion O
vectors. O
Results O

and O
Xu O
Shao, O
“An O
audio-visual O
corpus B-DAT
for O
speech O
perception O
and O
automatic O

Regarding O
the O
GRID B-DAT
corpus, I-DAT
for O
each O
of O
the O
33 O

of O
speaker-dependent O
models O
on O
the O
GRID B-DAT
corpus I-DAT
with O
landmark O
motion O
vectors. O
Results O

FACE B-DAT
LANDMARK-BASED O
SPEAKER-INDEPENDENT O
AUDIO-VISUAL O
SPEECH O

ENHANCEMENT B-DAT
IN O
MULTI-TALKER O
ENVIRONMENTS O
Giovanni O
Morrone? O
Luca O
Pasa† O
Vadim O

Tikhanoff B-DAT

Sonia B-DAT
Bergamaschi? O
Luciano O
Fadiga† O
Leonardo O

Badino† B-DAT
?Department O
of O
Engineering O
”Enzo O
Ferrari O

”, B-DAT
University O
of O
Modena O
and O

Reggio B-DAT
Emilia, O
Modena, O
Italy O
†Istituto O

Italiano B-DAT
di O
Tecnologia, O
Ferrara, O
Italy O
ABSTRACT O

In B-DAT
this O
paper, O
we O
address O

the B-DAT
problem O
of O
enhancing O
the O

speech B-DAT
of O
a O
speaker O
of O

interest B-DAT
in O
a O
cocktail O
party O

scenario B-DAT
when O
vi- O
sual O
information O

of B-DAT
the O
speaker O
of O
interest O

is B-DAT
available. O
Contrary O
to O
most O
previous O
studies O

, B-DAT
we O
do O
not O
learn O

visual B-DAT
features O
on O
the O
typically O

small B-DAT
audio-visual O
datasets, O
but O
use O

an B-DAT
already O
available O
face O
landmark O

detector B-DAT
(trained O
on O
a O
sep- O

arate B-DAT
image O
dataset). O
The O
landmarks O
are O
used O
by O

LSTM-based B-DAT
models O
to O
gen- O
erate O

time-frequency B-DAT
masks O
which O
are O
applied O

to B-DAT
the O
acoustic O
mixed-speech O
spectrogram O

. B-DAT
Results O
show O
that: O
(i) O

land- B-DAT
mark O
motion O
features O
are O

very B-DAT
effective O
features O
for O
this O

task, B-DAT
(ii) O
similarly O
to O
previous O

work, B-DAT
reconstruction O
of O
the O
target O

speaker’s B-DAT
spectrogram O
mediated O
by O
masking O

is B-DAT
significantly O
more O
accurate O
than O

direct B-DAT
spectrogram O
reconstruction, O
and O
(iii) O

the B-DAT
best O
masks O
depend O
on O

both B-DAT
motion O
landmark O
features O
and O

the B-DAT
input O
mixed-speech O
spectrogram. O
To O
the O
best O
of O
our O

knowledge, B-DAT
our O
proposed O
models O
are O

the B-DAT
first O
models O
trained O
and O

evaluated B-DAT
on O
the O
limited O
size O

GRID B-DAT
and O
TCD-TIMIT O
datasets, O
that O

achieve B-DAT
speaker-independent O
speech O
enhancement O
in O

a B-DAT
multi-talker O
setting O

. B-DAT
Index O
Terms— O
audio-visual O
speech O
enhancement O

, B-DAT
cock- O
tail O
party O
problem, O

time-frequency B-DAT
mask, O
LSTM, O
face O
land- O

marks B-DAT
1. O
INTRODUCTION O

In B-DAT
the O
context O
of O
speech O

perception, B-DAT
the O
cocktail O
party O

effect B-DAT
[1, O
2] O
is O
the O

ability B-DAT
of O
the O
brain O
to O

recognize B-DAT
speech O
in O
complex O
and O

adverse B-DAT
listening O
conditions O
where O
the O

attended B-DAT
speech O
is O
mixed O
with O

competing B-DAT
sounds/speech. O
Speech O
perception O
studies O
have O
shown O

that B-DAT
watching O
speaker’s O
face O
movements O

could B-DAT
dramatically O
improve O
our O
ability O

at B-DAT
recognizing O
the O
speech O
of O

a B-DAT
target O
speaker O
in O
a O

multi-talker B-DAT
environment O
[3, O
4 O

]. B-DAT
This O
work O
aims O
at O
extracting O

the B-DAT
speech O
of O
a O
target O

speaker B-DAT
from O
single O
channel O
audio O

of B-DAT
several O
people O
talking O
simulta O

- B-DAT
neously. O
This O
is O
an O

ill-posed B-DAT
problem O
in O
that O
many O

differ- B-DAT
ent O
hypotheses O
about O
what O

the B-DAT
target O
speaker O
says O
are O

con- B-DAT
sistent O
with O
the O
mixture O
signal O

. B-DAT
Yet, O
it O
can O
be O

solved B-DAT
by O
ex- O
ploiting O
some O

additional B-DAT
information O
associated O
to O
the O

speaker B-DAT
of O
interest O
and/or O
by O

leveraging B-DAT
some O
prior O
knowledge O
about O

speech B-DAT
signal O
properties O
(e.g., O
[5]). O

In B-DAT
this O
work O
we O
use O

face B-DAT
movements O
of O
the O
target O

speaker B-DAT
as O
additional O
information. O
This O
paper O
(i) O
proposes O
the O

use B-DAT
of O
face O
landmark’s O
move O

- B-DAT
ments, O
extracted O
using O

Dlib B-DAT
[6, O
7] O
and O
(ii) O

compares B-DAT
differ- O
ent O
ways O
of O

mapping B-DAT
such O
visual O
features O
into O

time-frequency B-DAT
(T-F) O
masks, O
then O
applied O

to B-DAT
clean O
the O
acoustic O
mixed-speech O

spectrogram. B-DAT
By O
using O
Dlib O
extracted O
landmarks O

we B-DAT
relieve O
our O
mod- O
els O

from B-DAT
the O
task O
of O
learning O

useful B-DAT
visual O
features O
from O
raw O

pixels. B-DAT
That O
aspect O
is O
particularly O

relevant B-DAT
when O
the O
training O
audio-visual O

datasets B-DAT
are O
small O

. B-DAT
The O
analysis O
of O
landmark-dependent O
masking O

strategies B-DAT
is O
motivated O
by O
the O

fact B-DAT
that O
speech O
enhancement O
mediated O

by B-DAT
an O
explicit O
masking O
is O

often B-DAT
more O
effective O
than O
mask-free O

enhancement B-DAT
[8 O

]. B-DAT
All O
our O
models O
were O
trained O

and B-DAT
evaluated O
on O
the O
GRID O

[9] B-DAT
and O
TCD-TIMIT O
[10] O
datasets O

in B-DAT
a O
speaker-independent O
setting O

. B-DAT
1.1. O
Related O
work O

Speech B-DAT
enhancement O
aims O
at O
extracting O

the B-DAT
voice O
of O
a O
tar- O

get B-DAT
speaker, O
while O
speech O
separation O

refers B-DAT
to O
the O
problem O
of O

separating B-DAT
each O
sound O
source O
in O

a B-DAT
mixture. O
Recently O
pro- O
posed O

audio-only B-DAT
single-channel O
methods O
have O
achieved O

very B-DAT
promising O
results O
[11, O
12, O
13 O

]. B-DAT
However O
the O
task O
still O

remains B-DAT
challenging. O
Additionally, O
audio-only O
systems O

need B-DAT
separate O
models O
in O
order O

to B-DAT
associate O
the O
estimated O
separated O

audio B-DAT
sources O
to O
each O
speaker, O

while B-DAT
vision O
easily O
allow O
that O

in B-DAT
a O
unified O
model. O
Regarding O
audio-visual O
speech O
enhancement O
and O

separa- B-DAT
tion O
methods O
an O
extensive O

review B-DAT
is O
provided O
in O
[14 O

]. B-DAT
Here O
we O
focus O
on O

the B-DAT
deep-learning O
methods O
that O
are O

most B-DAT
related O
to O
the O
present O

work. B-DAT
Our O
first O
architecture O
(Section O
2.1 O

) B-DAT
is O
inspired O
by O
[15], O

where B-DAT
a O
pre-trained O
convolutional O
neural O

network B-DAT
(CNN) O
is O
used O
to O

generate B-DAT
a O
clean O
spectrogram O
from O

silent B-DAT
video O
[16]. O
Rather O
than O

directly B-DAT
computing O
a O
time-frequency O
(T-F) O

mask, B-DAT
ar O
X O

iv B-DAT
:1 O
81 O
1 O

. B-DAT
02 O
48 O

0v B-DAT
3 O

cs B-DAT
.C O
L O

2 B-DAT

M B-DAT

2 B-DAT
01 O
9 O

the B-DAT
mask O
is O
computed O
by O

thresholding B-DAT
the O
estimated O
clean O
spectrogram. O

This B-DAT
approach O
is O
not O
very O

effective B-DAT
since O
the O
pre-trained O
CNN O

is B-DAT
designed O
for O
a O
different O

task B-DAT
(video-to- O
speech O
synthesis). O

In B-DAT
[17] O
a O
CNN O
is O

trained B-DAT
to O
directly O
esti- O
mate O

clean B-DAT
speech O
from O
noisy O
audio O

and B-DAT
input O
video. O
A O
sim- O

ilar B-DAT
model O
is O
used O

in B-DAT
[18], O
where O
the O
model O

jointly B-DAT
generates O
clean O
speech O
and O

input B-DAT
video O
in O
a O
denoising-autoender O

archi- B-DAT
tecture. O
[19] O
shows O
that O
using O
information O

about B-DAT
lip O
positions O
can O
help O

to B-DAT
improve O
speech O
enhancement. O
The O

video B-DAT
feature O
vec- O
tor O
is O

obtained B-DAT
computing O
pair-wise O
distances O
between O

any B-DAT
mouth O
landmarks. O
Similarly O
to O

our B-DAT
approach O
their O
visual O
fea O

- B-DAT
tures O
are O
not O
learned O

on B-DAT
the O
audio-visual O
dataset O
but O

are B-DAT
pro- O
vided O
by O
a O

system B-DAT
trained O
on O
different O
dataset. O

Contrary B-DAT
to O
our O
approach, O
[19] O

uses B-DAT
position-based O
features O
while O
we O

use B-DAT
motion O
features O
(of O
the O

whole B-DAT
face) O
that O
in O
our O

experiments B-DAT
turned O
out O
to O
be O

much B-DAT
more O
effective O
than O
positional O

features. B-DAT
Although O
the O
aforementioned O
audio-visual O
methods O

work B-DAT
well, O
they O
have O
only O

been B-DAT
evaluated O
in O
a O
speaker-dependent O

setting. B-DAT
Only O
the O
availability O
of O

new B-DAT
large O
and O
heterogeneous O
audio-visual O

datasets B-DAT
has O
allowed O
the O
training O

of B-DAT
deep O
neu- O
ral O
network-based O

speaker-independent B-DAT
speech O
enhancement O
models O
[20 O

, B-DAT
21, O
22]. O
The O
present O
work O
shows O
that O

huge B-DAT
audio-visual O
datasets O
are O
not O

a B-DAT
necessary O
requirement O
for O
speaker-independent O

audio-visual B-DAT
speech O
enhancement. O
Although O
we O

have B-DAT
only O
considered O
datasets O
with O

simple B-DAT
visual O
scenarios O
(i.e., O
the O

target B-DAT
speaker O
is O
always O
facing O

the B-DAT
camera), O
we O
expect O
our O

methods B-DAT
to O
perform O
well O
in O

more B-DAT
complex O
scenarios O
thanks O
to O

the B-DAT
robust O
landmark O
extraction O

. B-DAT
2. O
MODEL O
ARCHITECTURES O

We B-DAT
experimented O
with O
the O
four O

models B-DAT
shown O
in O
Fig. O
1. O

All B-DAT
models O
receive O
in O
input O

the B-DAT
target O
speaker’s O
landmark O
mo- O

tion B-DAT
vectors O
and O
the O
power-law O

compressed B-DAT
spectrogram O
of O
the O
single-channel O

mixed-speech B-DAT
signal. O
All O
of O
them O

perform B-DAT
some O
kind O
of O
masking O

operation. B-DAT
2.1. O
VL2M O
model O

At B-DAT
each O
time O
frame, O
the O

video-landmark B-DAT
to O
mask O
(VL2M) O
model O
( O

Fig. B-DAT
1a) O
estimates O
a O
T-F O

mask B-DAT
from O
visual O
features O
only O
( O

of B-DAT
the O
target O
speaker). O
Formally, O

given B-DAT
a O
video O
sequence O

= B-DAT
[v1 O

, B-DAT
. O
. O
. O
, O

vT B-DAT
], O
vt O
∈ O
Rn O

and B-DAT
a O
target O
mask O
sequence O

= B-DAT
[m1 O

, B-DAT
. O
. O
. O
, O

mT B-DAT
], O
mt O
∈ O
Rd, O

VL2M B-DAT
perform O
a O
function O

Fvl2m(v) B-DAT
= O
m̂, O
where O
m̂ O

is B-DAT
the O
estimated O
mask. O
The O
training O
objective O
for O
VL2M O

is B-DAT
a O
Target O
Binary O
Mask O

(TBM) B-DAT
[23, O
24], O
computed O
using O

the B-DAT
spectrogram O
of O
the O
tar O

- B-DAT
get O
speaker O
only. O
This O

is B-DAT
motivated O
by O
our O
goal O

of B-DAT
extracting O
the O
speech O
of O

a B-DAT
target O
speaker O
as O
much O

as B-DAT
possible O
indepen- O
dently O
of O

the B-DAT
concurrent O
speakers, O
so O
that, O

e.g., B-DAT
we O
do O
not O
need O

to B-DAT
estimate O
their O
number. O
An O

additional B-DAT
motivations O
is O
that O
the O

model B-DAT
takes O
as O
only O
input O

the B-DAT
visual O
features O
of O
the O
target O
speaker, O
and O
a O
target O

TBM B-DAT
that O
only O
depends O
on O

the B-DAT
target O
speaker O
allows O
VL2M O

to B-DAT
learn O
a O
function O
(rather O

than B-DAT
approximating O
an O
ill-posed O
one-to-many O

mapping B-DAT

). B-DAT
Given O
a O
clean O
speech O
spectrogram O

of B-DAT
a O
speaker O
s O

= B-DAT
[s1 O

, B-DAT
. O
. O
. O
, O

sT B-DAT
], O
st O
∈ O
Rd, O

the B-DAT
TBM O
is O
defined O
by O

comparing, B-DAT
at O
each O
frequency O
bin O

∈ B-DAT
[1 O

, B-DAT
. O
. O
. O
, O

d], B-DAT
the O
target O
speaker O
value O

st[f B-DAT
] O
vs. O
a O
reference O

threshold B-DAT
τ O
[f O
]. O
As O

in B-DAT
[15], O
we O
use O
a O

function B-DAT
of O
long-term O
average O
speech O

spectrum B-DAT
(LTASS) O
as O
reference O
threshold. O

This B-DAT
threshold O
indicates O
if O
a O

T-F B-DAT
unit O
is O
generated O
by O

the B-DAT
speaker O
or O
refers O
to O

silence B-DAT
or O
noise. O
The O
process O

to B-DAT
compute O
the O
speaker’s O
TBM O

is B-DAT
as O
follows: O
1. O
The O
mean O
π[f O

] B-DAT
and O
the O
standard O
deviation O

σ[f B-DAT
] O
are O
computed O
for O

all B-DAT
frequency O
bins O
of O
all O

seen B-DAT
spectro- O
grams O
in O
speaker’s O

data B-DAT

. B-DAT
2. O
The O
threshold O
τ O
[f O

] B-DAT
is O
defined O
as O
τ O

[f B-DAT
] O
= O
π[f O
]+0.6 O

·σ[f B-DAT
] O
where O
0.6 O
is O

a B-DAT
value O
selected O
by O
manual O

inspection B-DAT
of O
several O
spectrogram-TBM O
pairs O

. B-DAT
3. O
The O
threshold O
is O
applied O

to B-DAT
every O
speaker’s O
speech O
spec O

- B-DAT
trogram O
s. O
mt[f O

1, B-DAT
if O
st[f O
] O
≥ O

τ B-DAT
[f O
], O
0, O
otherwise. O
The O
mapping O
Fvl2m(·) O
is O
carried O

out B-DAT
by O
a O
stacked O
bi O

- B-DAT
directional O
Long O
Short-Term O
Memory O
( O

BLSTM) B-DAT
network O
[25]. O
The O
BLSTM O

outputs B-DAT
are O
then O
forced O
to O

lay B-DAT
within O
the O
[0, O
1] O

range. B-DAT
Finally O
the O
computed O
TBM O

m̂ B-DAT
and O
the O
noisy O
spectrogram O

y B-DAT
are O
element-wise O
multiplied O
to O

ob- B-DAT
tain O
the O
estimated O
clean O

spectrogram B-DAT
ŝm O
= O
m̂ O
◦ O

y, B-DAT
where O

= B-DAT
[y1 O

, B-DAT
. O
. O
. O

yT B-DAT
], O
yt O
∈ O
Rd. O
The O
model O
parameters O
are O
estimated O

to B-DAT
minimize O
the O
loss O

: B-DAT
Jvl2m O
= O
∑T O

t=1 B-DAT
∑d O
f=1−mt[f O
] O
· O
log(m̂t[f O

])− B-DAT
(1−mt[f O
]) O
· O
log(1 O

− B-DAT
m̂t[f O
]) O
2.2. O
VL2M O
ref O
model O

VL2M B-DAT
generates O
T-F O
masks O
that O

are B-DAT
independent O
of O
the O
acous- O

tic B-DAT
context. O
We O
may O
want O

to B-DAT
refine O
the O
masking O
by O

including B-DAT
such O
context. O
This O
is O

what B-DAT
the O
novel O
VL2M O
ref O

does B-DAT
(Fig. O
1b). O
The O
computed O

TBM B-DAT
m̂ O
and O
the O
input O

spectrogram B-DAT
y O
are O
the O
input O

to B-DAT
a O
function O
that O
outputs O

an B-DAT
Ideal O
Amplitude O
Mask O
(IAM) O

p B-DAT
(known O
as O
FFT-MASK O

in B-DAT
[8]). O
Given O
the O
target O

clean B-DAT
spectrogram O
s O
and O
the O

noisy B-DAT
spectrogram O
y, O
the O
IAM O

is B-DAT
defined O
as: O
pt[f O
] O
= O
st[f O

yt[f B-DAT
] O
Note O
that O
although O
IAM O
generation O

requires B-DAT
the O
mixed-speech O
spectrogram, O
separate O

spectrograms B-DAT
for O
each O
concurrent O
speakers O

are B-DAT
not O
required O

. B-DAT
The O
target O
speaker’s O
spectrogram O
s O

is B-DAT
reconstructed O
by O
multiplying O
the O

input B-DAT
spectrogram O
with O
the O
estimated O

IAM. B-DAT
Values O
greater O
than O
10 O

in B-DAT
the O
IAM O
are O
clipped O

to B-DAT
10 O
in O
order O
to O

obtain B-DAT
better O
numerical O
stability O
as O

suggested B-DAT
in O
[8 O

v: B-DAT
video O
input O
y: O
noisy O

spectrogram B-DAT
sm: O
clean O
spectrogram O
TBM O

s: B-DAT
clean O
spectrogram O
IAM O
m: O

TBM B-DAT
p: O
IAM O
STACKED O

BLSTM B-DAT
m O

sm B-DAT
v O

y B-DAT
(a) O
VL2M O

v B-DAT
VL2M O
m O
y O
BLSTM O

BLSTM B-DAT
Fusion O
layer O

BLSTM B-DAT
p O
s O

b) B-DAT
VL2M O
ref O
v O

y B-DAT
p O
STACKED O

BLSTM B-DAT
s O

c) B-DAT
Audio-Visual O
concat O
sm O

y B-DAT
p O
STACKED O

BLSTM B-DAT
s O

v B-DAT
VL2M O
m O
(d) O
Audio-Visual O
concat-ref O

Fig. B-DAT
1. O
Model O
architectures. O
The O
model O
performs O
a O
function O

Fmr(v, B-DAT
y) O
= O
p̂ O
that O

con- B-DAT
sists O
of O
a O
VL2M O

component B-DAT
plus O
three O
different O
BLSTMs O

Gm, B-DAT
Gy O
and O
H O

Gm(Fvl2m(v)) B-DAT
= O
rm O
receives O
the O

VL2M B-DAT
mask O
m̂ O
as O
in- O

put, B-DAT
and O
Gy(y) O
= O
ry O

is B-DAT
fed O
with O
the O
noisy O

spectrogram. B-DAT
Their O
output O
rm, O

ry B-DAT
∈ O
Rz O
are O
fused O

in B-DAT
a O
joint O
audio-visual O
represen- O

tation B-DAT

= B-DAT
[h1 O

, B-DAT
. O
. O
. O
, O

hT B-DAT
], O
where O
ht O
is O

a B-DAT
linear O
combination O
of O
rmt O

and B-DAT
ryt O
: O
ht O
= O

Whm B-DAT
·rmt O
+Why O
·ryt O
+bh. O

h B-DAT
is O
the O
input O
of O

the B-DAT
third O
BLSTM O
H O
( O

h) B-DAT
= O
p̂, O
where O
p̂ O

lays B-DAT
in O
the O
[0,10] O
range. O

The B-DAT
loss O
function O
is: O
Jmr O

T∑ B-DAT
t=1 O
d∑ O
f=1 O

p̂t[f B-DAT
] O
· O
yt[f O
]− O

st[f B-DAT
])2 O
2.3. O
Audio-Visual O
concat O
model O

The B-DAT
third O
model O
(Fig. O
1c) O

performs B-DAT
early O
fusion O
of O
audio- O

visual B-DAT
features. O
This O
model O
consists O

of B-DAT
a O
single O
stacked O
BLSTM O

that B-DAT
computes O
the O
IAM O
mask O

p̂ B-DAT
from O
the O
concate- O

nated B-DAT
[v,y]. O
The O
training O
loss O

is B-DAT
the O
same O
Jmr O
used O

to B-DAT
train O
VL2M O
ref. O
This O

model B-DAT
can O
be O
regarded O
as O

a B-DAT
simplification O
of O
VL2M O
ref, O

where B-DAT
the O
VL2M O
operation O
is O

not B-DAT
performed. O
2.4. O
Audio-Visual O
concat-ref O
model O

The B-DAT
fourth O
model O
(Fig. O
1d) O

is B-DAT
an O
improved O
version O
of O

the B-DAT
model O
described O
in O
section O
2 O

.3. B-DAT
The O
only O
difference O
is O

the B-DAT
input O
of O
the O
stacked O

BLSTM B-DAT
that O
is O
replaced O

by B-DAT
[̂sm,y] O
where O
ŝm O
is O

the B-DAT
denoised O
spectrogram O
returned O
by O

VL2M B-DAT
operation. O
3. O
EXPERIMENTAL O
SETUP O

3.1. B-DAT
Dataset O
All O
experiments O
were O
carried O
out O

using B-DAT
the O
GRID O
[9] O
and O

TCD-TIMIT B-DAT
[10] O
audio-visual O
datasets. O
For O

each B-DAT
of O
them, O
we O
created O

a B-DAT
mixed-speech O
version O

. B-DAT
Regarding O
the O
GRID O
corpus, O
for O

each B-DAT
of O
the O
33 O
speakers O

(one B-DAT
had O
to O
be O
discarded O

) B-DAT
we O
first O
randomly O
selected O
200 O
ut- O
terances O
(out O
of O
1000 O

). B-DAT
Then, O
for O
each O
utterance, O

we B-DAT
created O
3 O
different O
audio-mixed O

samples. B-DAT
Each O
audio-mixed O
sample O
was O

created B-DAT
by O
mixing O
the O
chosen O

utterance B-DAT
with O
one O
utter- O
ance O

from B-DAT
a O
different O
speaker. O
That O
resulted O
in O
600 O
audio-mixed O

samples B-DAT
per O
speaker O

. B-DAT
The O
resulting O
dataset O
was O
split O

into B-DAT
disjoint O
sets O
of O
25/4/4 O

speakers B-DAT
for O
training/validation/testing O
respectively O

. B-DAT
The O
TCD-TIMIT O
corpus O
consists O
of O

59 B-DAT
speakers O
(we O
ex- O
cluded O

3 B-DAT
professionally-trained O
lipspeakers) O
and O
98 O

utterances B-DAT
per O
speaker. O
The O
mixed-speech O

version B-DAT
was O
created O
following O
the O

same B-DAT
procedure O
as O
for O
GRID O

, B-DAT
with O
one O
difference. O
Con- O

trary B-DAT
to O
GRID, O
TCD-TIMIT O
utterances O

have B-DAT
different O
dura- O
tion. O
Thus O
2 O
utterances O
were O
mixed O
only O
if O

their B-DAT
duration O
dif- O
ference O
did O

not B-DAT
exceed O
2 O
seconds. O
For O

each B-DAT
utterance O
pair, O
we O
forced O

the B-DAT
non-target O
speaker’s O
utterance O
to O

match B-DAT
the O
du- O
ration O
of O

the B-DAT
target O
speaker O
utterance. O
If O

it B-DAT
was O
longer, O
the O
utterance O

was B-DAT
cut O
at O
its O
end O

, B-DAT
whereas O
if O
it O
was O

shorter, B-DAT
silence O
samples O
were O
equally O

added B-DAT
at O
its O
start O
and O

end. B-DAT
The O
resulting O
dataset O
was O
split O

into B-DAT
disjoint O
sets O
of O
51/4/4 O

speakers B-DAT
for O
training/validation/testing O
respectively O

. B-DAT
3.2. O
LSTM O
training O

In B-DAT
all O
experiments, O
the O
models O

were B-DAT
trained O
using O
the O
Adam O

optimizer B-DAT
[26]. O
Early O
stopping O
was O

applied B-DAT
when O
the O
error O
on O

the B-DAT
validation O
set O
did O
not O

decrease B-DAT
over O
5 O
consecutive O
epochs. O
VL2M, O
AV O
concat O
and O
AV O

concat-ref B-DAT
had O
5, O
3 O
and O

3 B-DAT
stacked O
BLSTM O
layers O
respectively O

. B-DAT
All O
BLSTMs O
had O
250 O

units. B-DAT
Hyper-parameters O
selection O
was O
performed O

by B-DAT
using O
random O
search O
with O

a B-DAT
limited O
number O
of O
samples, O

therefore B-DAT
all O
the O
reported O
results O

may B-DAT
improve O
through O
a O
deeper O

hyper- B-DAT
parameters O
validation O
phase. O
VL2M O
ref O
and O
AV O
concat-ref O

training B-DAT
was O
performed O
in O
2 O

steps. B-DAT
We O
first O
pre-trained O
the O

models B-DAT
using O
the O
oracle O
TBM O

m. B-DAT
Then O
we O
substituted O
the O

oracle B-DAT
masks O
with O
the O
VL2M O

component B-DAT
and O
retrained O
the O
models O

while B-DAT
freezing O
the O
pa- O
rameters O

of B-DAT
the O
VL2M O
component O

. B-DAT
3.3. O
Audio O
pre- O
and O
post-processing O

The B-DAT
original O
waveforms O
were O
resampled O

to B-DAT
16 O
kHz. O
Short- O
Time O

Fourier B-DAT
Transform O
(STFT) O
x O
was O

computed B-DAT
using O
FFT O
size O
of O
512, O
Hann O
window O
of O
length O
25 O

ms B-DAT
(400 O
samples), O
and O
hop O

length B-DAT
of O
10 O
ms O
(160 O

samples). B-DAT
The O
input O
spectro- O
gram O

was B-DAT
obtained O
taking O
the O
STFT O

magnitude B-DAT
and O
perform- O
ing O
power-law O

compression B-DAT
|x|p O
with O
p O

= B-DAT
0.3. O
Finally O
we O
applied O

per-speaker B-DAT
0-mean O
1-std O
normalization O

. B-DAT
In O
the O
post-processing O
stage, O
the O

enhanced B-DAT
waveform O
gen- O
erated O
by O

the B-DAT
speech O
enhancement O
models O
was O

reconstructed B-DAT

SDR B-DAT
PESQ O
ViSQOL O
Noisy O
−1.06 O
1.81 O
2.11 O
VL2M O

3.17 B-DAT
1.51 O
1.16 O
VL2M O
ref O

6.50 B-DAT
2.58 O
2.99 O
AV O
concat O

6.31 B-DAT
2.49 O
2.83 O
AV O
c-ref O

6.17 B-DAT
2.58 O
2.96 O

Table B-DAT
1. O
GRID O
results O
- O

speaker-dependent. B-DAT
The O
“Noisy” O
row O
refers O

to B-DAT
the O
metric O
values O
of O

the B-DAT
input O
mixed-speech O
signal. O
2 O
Speakers O
3 O
Speakers O
SDR O

PESQ B-DAT
ViSQOL O
SDR O
PESQ O
ViSQOL O

Noisy B-DAT
0.21 O
1.94 O
2.58 O
−5.34 O
1 O

.43 B-DAT
1.62 O
VL2M O
3.02 O
1.81 O
1 O

.70 B-DAT
−2.03 O
1.43 O
1.25 O
VL2M O

ref B-DAT
6.52 O
2.53 O
3.02 O
2.83 O
2 O

.19 B-DAT
2.53 O
AV O
concat O
7.37 O
2 O

.65 B-DAT
3.03 O
3.02 O
2.24 O
2.49 O

AV B-DAT
c-ref O
8.05 O
2.70 O
3.07 O
4 O

.02 B-DAT
2.33 O
2.64 O
Table O
2. O
GRID O
results O

- B-DAT
speaker-independent O

. B-DAT
by O
applying O
the O
inverse O
STFT O

to B-DAT
the O
estimated O
clean O
spectro O

- B-DAT
gram O
and O
using O
the O

phase B-DAT
of O
the O
noisy O
input O

signal. B-DAT
3.4. O
Video O
pre-processing O

Face B-DAT
landmarks O
were O
extracted O
from O

video B-DAT
using O
the O
Dlib O
[7] O

implementation B-DAT
of O
the O
face O
landmark O

estimator B-DAT
described O
in O
[6]. O
It O

returns B-DAT
68 O
x-y O
points, O
for O

an B-DAT
overall O
136 O
values. O
We O

upsampled B-DAT
from O
25/29.97 O
fps O
(GRID/TCD-TIMIT) O

to B-DAT
100 O
fps O
to O
match O

the B-DAT
frame O
rate O
of O
the O

audio B-DAT
spectrogram. O
Upsampling O
was O
carried O

out B-DAT
through O
linear O
interpolation O
over O

time. B-DAT
The O
final O
video O
feature O
vector O

v B-DAT
was O
obtained O
by O
com O

- B-DAT
puting O
the O
per-speaker O
normalized O

motion B-DAT
vector O
of O
the O
face O

landmarks B-DAT
by O
simply O
subtracting O
every O

frame B-DAT
with O
the O
previ- O
ous O

one. B-DAT
The O
motion O
vector O
of O

the B-DAT
first O
frame O
was O
set O

to B-DAT
zero. O
4. O
RESULTS O

In B-DAT
order O
to O
compare O
our O

models B-DAT
to O
previous O
works O
in O

both B-DAT
speech O
enhancement O
and O
separation, O

we B-DAT
evaluated O
the O
perfor- O
mance O

of B-DAT
the O
proposed O
models O
using O

both B-DAT
speech O
separation O
2 O
Speakers O
3 O
Speakers O
SDR O

PESQ B-DAT
ViSQOL O
SDR O
PESQ O
ViSQOL O

Noisy B-DAT
0.21 O
2.22 O
2.74 O
−3.42 O
1 O

.92 B-DAT
2.04 O
VL2M O
2.88 O
2.25 O
2 O

.62 B-DAT
−0.51 O
1.99 O
1.98 O
VL2M O

ref B-DAT
9.24 O
2.81 O
3.09 O
5.27 O
2 O

.44 B-DAT
2.54 O
AV O
concat O
9.56 O
2 O

.80 B-DAT
3.09 O
5.15 O
2.41 O
2.52 O

AV B-DAT
c-ref O
10.55 O
3.03 O
3.21 O
5 O

.37 B-DAT
2.45 O
2.58 O
Table O
3. O
TCD-TIMIT O
results O

- B-DAT
speaker-independent O

. B-DAT
and O
enhancement O
metrics. O
Specifically, O
we O

measured B-DAT
the O
ca- O
pability O
of O

separating B-DAT
the O
target O
utterance O
from O

the B-DAT
concurrent O
utterance O
with O
the O

source-to-distortion B-DAT
ratio O
(SDR) O
[27, O
28 O

]. B-DAT
While O
the O
quality O
of O

estimated B-DAT
target O
speech O
was O
measured O

with B-DAT
the O
perceptual O
PESQ O
[29] O

and B-DAT
ViSQOL O
[30] O
metrics. O
For O

PESQ B-DAT
we O
used O
the O
narrow O

band B-DAT
mode O
while O
for O
ViSQOL O

we B-DAT
used O
the O
wide O
band O

mode. B-DAT
As O
a O
very O
first O
experiment O

we B-DAT
compared O
landmark O
posi- O
tion O

vs. B-DAT
landmark O
motion O
vectors. O
It O

turned B-DAT
out O
that O
landmark O
positions O

performed B-DAT
poorly, O
thus O
all O
results O

reported B-DAT
here O
refer O
to O
landmark O

motion B-DAT
vectors O
only O

. B-DAT
We O
then O
carried O
out O
some O

speaker-dependent B-DAT
experiments O
to O
compare O
our O

models B-DAT
with O
previous O
studies O
as O

, B-DAT
to O
the O
best O
of O

our B-DAT
knowledge, O
there O
are O
no O

reported B-DAT
results O
of O
speaker- O
independent O

systems B-DAT
trained O
and O
tested O
on O

GRID B-DAT
and O
TCD- O
TIMIT O
to O

compare B-DAT
with. O
Table O
1 O
reports O

the B-DAT
test-set O
evalua- O
tion O
of O

speaker-dependent B-DAT
models O
on O
the O
GRID O

corpus B-DAT
with O
landmark O
motion O
vectors. O

Results B-DAT
are O
comparable O
with O
previ- O

ous B-DAT
state-of-the-art O
studies O
in O
an O

almost B-DAT
identical O
setting O
[15, O
17]. O
Table O
2 O
and O
3 O
show O

speaker-independent B-DAT
test-set O
results O
on O
the O

GRID B-DAT
and O
TCD-TIMIT O
datasets O
respectively O

. B-DAT
V2ML O
performs O
significantly O
worse O

than B-DAT
the O
other O
three O
models O

in- B-DAT
dicating O
that O
a O
successful O

mask B-DAT
generation O
has O
to O
depend O

on B-DAT
the O
acoustic O
context. O
The O

performance B-DAT
of O
the O
three O
models O

in B-DAT
the O
speaker-independent O
setting O
is O

comparable B-DAT
to O
that O
in O
the O

speaker-dependent B-DAT
setting. O
AV O
concat-ref O
outperforms O
V2ML O
ref O

and B-DAT
AV O
concat O
for O
both O

datasets. B-DAT
This O
supports O
the O
utility O

of B-DAT
a O
refinement O
strat- O
egy O

and B-DAT
suggests O
that O
the O
refinement O

is B-DAT
more O
effective O
when O
it O

directly B-DAT
refines O
the O
estimated O
clean O

spectrogram, B-DAT
rather O
than O
refining O
the O

estimated B-DAT
mask O

. B-DAT
Finally, O
we O
evaluated O
the O
systems O

in B-DAT
a O
more O
challenging O
testing O

condition B-DAT
where O
the O
target O
utterance O

was B-DAT
mixed O
with O
2 O
utterances O

from B-DAT
2 O
competing O
speakers. O
Despite O

the B-DAT
model O
was O
trained O
with O

mixtures B-DAT
of O
two O
speakers, O
the O

decrease B-DAT
of O
performance O
was O
not O

dramatic B-DAT

. B-DAT
Code O
and O
some O
testing O
examples O

of B-DAT
our O
models O
are O
avail O

- B-DAT
able O
at O
https://goo.gl/3h1NgE. O
5. O
CONCLUSION O

This B-DAT
paper O
proposes O
the O
use O

of B-DAT
face O
landmark O
motion O
vec- O

tors B-DAT
for O
audio-visual O
speech O
enhancement O

in B-DAT
a O
single-channel O
multi-talker O
scenario. O

Different B-DAT
models O
are O
tested O
where O

land- B-DAT
mark O
motion O
vectors O
are O

used B-DAT
to O
generate O
time-frequency O
(T- O

F) B-DAT
masks O
that O
extract O
the O

target B-DAT
speaker’s O
spectrogram O
from O
the O

acoustic B-DAT
mixed-speech O
spectrogram. O
To O
the O
best O
of O
our O

knowledge, B-DAT
some O
of O
the O
proposed O

mod- B-DAT
els O
are O
the O
first O

models B-DAT
trained O
and O
evaluated O
on O

the B-DAT
limited O
size O
GRID O
and O

TCD-TIMIT B-DAT
datasets O
that O
accomplish O
speaker O

- B-DAT
independent O
speech O
enhancement O
in O

the B-DAT
multi-talker O
setting, O
with O
a O

quality B-DAT
of O
enhancement O
comparable O
to O

that B-DAT
achieved O
in O
a O
speaker-dependent O

setting. B-DAT
https://goo.gl/3h1NgE O

6. B-DAT
REFERENCES O
[1] O
E. O
Colin O
Cherry, O
“Some O

experiments B-DAT
on O
the O
recognition O
of O

speech, B-DAT
with O
one O
and O
with O

two B-DAT
ears,” O
The O
Journal O
of O

the B-DAT
Acoustical O
Society O
of O
America O

, B-DAT
vol. O
25, O
no. O
5, O

pp. B-DAT
975–979, O
1953. O
[2] O
Josh O
H O
McDermott, O
“The O

cocktail B-DAT
party O
problem,” O
Current O
Biology O

, B-DAT
vol. O
19, O
no. O
22, O

pp. B-DAT
R1024–R1027, O
2009. O
[3] O
Elana O
Zion O
Golumbic, O
Gregory O

B. B-DAT
Cogan, O
Charles O
E. O
Schroeder O

, B-DAT
and O
David O
Poeppel, O
“Visual O

input B-DAT
enhances O
selective O
speech O
envelope O

tracking B-DAT
in O
auditory O
cortex O
at O

a B-DAT
“cocktail O
party”,” O
Journal O
of O

Neu- B-DAT
roscience, O
vol. O
33, O
no. O
4, O
pp. O
1417–1426, O
2013 O

. B-DAT
[4] O
Wei O
Ji O
Ma, O
Xiang O

Zhou, B-DAT
Lars O
A. O
Ross, O
John O

J. B-DAT
Foxe, O
and O
Lucas O
C O

. B-DAT
Parra, O
“Lip-reading O
aids O
word O

recognition B-DAT
most O
in O
moderate O
noise: O

A B-DAT
bayesian O
explanation O
using O
high-dimensional O

feature B-DAT
space,” O
PLOS O
ONE, O
vol. O
4, O
no. O
3, O
pp. O
1–14, O
03 O

2009 B-DAT

. B-DAT
[5] O
Albert O
S O
Bregman, O
Auditory O

scene B-DAT
analysis: O
The O
perceptual O
organi O

- B-DAT
zation O
of O
sound, O
MIT O

press, B-DAT
1994. O
[6] O
Vahid O
Kazemi O
and O
Josephine O

Sullivan, B-DAT
“One O
millisecond O
face O
align O

- B-DAT
ment O
with O
an O
ensemble O

of B-DAT
regression O
trees,” O
in O
The O

IEEE B-DAT
Conference O
on O
Computer O
Vision O

and B-DAT
Pattern O
Recognition O
(CVPR), O
June O
2014 O

. B-DAT
[7] O
Davis O
E. O
King, O
“Dlib-ml O

: B-DAT
A O
machine O
learning O
toolkit,” O

Journal B-DAT
of O
Machine O
Learning O
Research, O

vol. B-DAT
10, O
pp. O
1755–1758, O
2009. O
[8] O
Yuxuan O
Wang, O
Arun O
Narayanan O

, B-DAT
and O
DeLiang O
Wang, O
“On O

Training B-DAT
Targets O
for O
Supervised O
Speech O

Separation,” B-DAT
IEEE/ACM O
Transactions O
on O
Audio, O

Speech, B-DAT
and O
Language O
Processing, O
vol. O
22, O
no. O
12, O
pp. O
1849–1858, O
Dec O

. B-DAT
2014. O
[9] O
Martin O
Cooke, O
Jon O
Barker O

, B-DAT
Stuart O
Cunningham, O
and O
Xu O

Shao, B-DAT
“An O
audio-visual O
corpus O
for O

speech B-DAT
perception O
and O
automatic O
speech O

recognition,” B-DAT
The O
Journal O
of O
the O

Acoustical B-DAT
Society O
of O
America, O
vol. O
120, O
no. O
5, O
pp. O
2421–2424, O
Nov O

. B-DAT
2006. O
[10] O
Naomi O
Harte O
and O
Eoin O

Gillen, B-DAT
“TCD-TIMIT: O
An O
Audio-Visual O
Cor O

- B-DAT
pus O
of O
Continuous O
Speech,” O

IEEE B-DAT
Transactions O
on O
Multimedia, O
vol. O
17, O
no. O
5, O
pp. O
603–615, O
May O

2015 B-DAT

. B-DAT
[11] O
Z. O
Chen, O
Y. O
Luo O

, B-DAT
and O
N. O
Mesgarani, O
“Deep O

attractor B-DAT
network O
for O
single-microphone O
speaker O

separation,” B-DAT
in O
2017 O
IEEE O
International O

Conference B-DAT
on O
Acoustics, O
Speech O
and O

Signal B-DAT
Processing O
(ICASSP), O
March O
2017, O

pp. B-DAT
246–250. O
[12] O
Yusuf O
Isik, O
Jonathan O
Le O

Roux, B-DAT
Zhuo O
Chen, O
Shinji O
Watanabe O

, B-DAT
and O
John O
R. O

Hershey, B-DAT
“Single-channel O
multi-speaker O
separation O
using O

deep B-DAT
clustering,” O
in O
Interspeech, O
2016. O
[13] O
Morten O
Kolbaek, O
Dong O
Yu O

, B-DAT
Zheng-Hua O
Tan, O
Jesper O
Jensen, O

Morten B-DAT
Kolbaek, O
Dong O
Yu, O
Zheng-Hua O

Tan, B-DAT
and O
Jesper O
Jensen, O
“Multitalker O

speech B-DAT
separation O
with O
utterance-level O
permutation O

invariant B-DAT
training O
of O
deep O
recurrent O

neural B-DAT
networks,” O
IEEE/ACM O
Trans. O
Audio, O

Speech B-DAT
and O
Lang. O
Proc., O
vol. O
25, O
no. O
10, O
pp. O
1901–1913, O
Oct O

. B-DAT
2017. O
[14] O
Bertrand O
Rivet, O
Wenwu O
Wang O

, B-DAT
Syed O
Mohsen O
Naqvi, O
and O

Jonathon B-DAT
Chambers, O
“Audiovisual O
Speech O
Source O

Separation: B-DAT
An O
overview O
of O
key O

methodologies,” B-DAT
IEEE O
Signal O
Processing O
Magazine, O

vol. B-DAT
31, O
no. O
3, O
pp. O
125 O

–134, B-DAT
May O
2014. O
[15] O
Aviv O
Gabbay, O
Ariel O
Ephrat O

, B-DAT
Tavi O
Halperin, O
and O
Shmuel O

Peleg, B-DAT
“Seeing O
through O
noise: O
Visually O

driven B-DAT
speaker O
separation O
and O
enhancement,” O

in B-DAT
ICASSP. O
2018, O
pp. O
3051–3055, O

IEEE. B-DAT
[16] O
Ariel O
Ephrat, O
Tavi O
Halperin O

, B-DAT
and O
Shmuel O
Peleg, O
“Improved O

speech B-DAT
reconstruction O
from O
silent O
video,” O

ICCV B-DAT
2017 O
Workshop O
on O
Computer O

Vision B-DAT
for O
Audio-Visual O
Media, O
2017. O
[17] O
Aviv O
Gabbay, O
Asaph O
Shamir O

, B-DAT
and O
Shmuel O
Peleg, O
“Visual O

speech B-DAT
en- O
hancement,” O
in O
Interspeech. O
2018, O
pp. O
1170–1174, O
ISCA O

. B-DAT
[18] O
Jen-Cheng O
Hou, O
Syu-Siang O
Wang O

, B-DAT
Ying-Hui O
Lai, O
Yu O
Tsao, O

Hsiu-Wen B-DAT
Chang, O
and O
Hsin-Min O

Wang, B-DAT
“Audio-Visual O
Speech O
Enhancement O
Us- O

ing B-DAT
Multimodal O
Deep O
Convolutional O
Neural O

Networks,” B-DAT
IEEE O
Trans- O
actions O
on O

Emerging B-DAT
Topics O
in O
Computational O
Intelligence, O

vol. B-DAT
2, O
no. O
2, O
pp. O
117 O

–128, B-DAT
Apr. O
2018. O
[19] O
Jen-Cheng O
Hou, O
Syu-Siang O
Wang O

, B-DAT
Ying-Hui O
Lai, O
Jen-Chun O
Lin, O

Yu B-DAT
Tsao, O
Hsiu-Wen O
Chang, O
and O

Hsin-Min B-DAT
Wang, O
“Audio-visual O
speech O
enhancement O

using B-DAT
deep O
neural O
networks,” O
in O
2016 O
Asia- O
Pacific O
Signal O
and O
Information O

Processing B-DAT
Association O
Annual O
Sum- O
mit O

and B-DAT
Conference O
(APSIPA), O
Jeju, O
South O

Korea, B-DAT
Dec. O
2016, O
pp. O
1–6 O

, B-DAT
IEEE. O
[20] O
Ariel O
Ephrat, O
Inbar O
Mosseri O

, B-DAT
Oran O
Lang, O
Tali O
Dekel, O

Kevin B-DAT
Wilson, O
Avinatan O
Hassidim, O
William O

T. B-DAT
Freeman, O
and O
Michael O

Rubinstein, B-DAT
“Looking O
to O
Listen O
at O

the B-DAT
Cocktail O
Party: O
A O
Speaker-Independent O

Audio-Visual B-DAT
Model O
for O
Speech O
Separation,” O

ACM B-DAT
Transactions O
on O
Graphics, O
vol. O
37, O
no. O
4, O
pp. O
1–11, O
July O

2018, B-DAT
arXiv: O
1804.03619 O

. B-DAT
[21] O
T. O
Afouras, O
J. O
S O

. B-DAT
Chung, O
and O
A. O

Zisserman, B-DAT
“The O
conversation: O
Deep O
audio-visual O

speech B-DAT
enhancement,” O
in O
Interspeech, O
2018. O
[22] O
Andrew O
Owens O
and O
Alexei O

A B-DAT
Efros, O
“Audio-visual O
scene O
analysis O

with B-DAT
self-supervised O
multisensory O
features,” O
European O

Conference B-DAT
on O
Computer O
Vision O
(ECCV O

), B-DAT
2018. O
[23] O
Michael O
C. O
Anzalone, O
Lauren O

Calandruccio, B-DAT
Karen O
A. O
Doherty, O
and O

Laurel B-DAT
H. O
Carney, O
“Determination O
of O

the B-DAT
potential O
benefit O
of O
time O

- B-DAT
frequency O
gain O
manipulation,” O
Ear O

Hear, B-DAT
vol. O
27, O
no. O
5, O

pp. B-DAT
480–492, O
Oct O
2006, O
16957499[pmid]. O
[24] O
Ulrik O
Kjems, O
Jesper O
B O

. B-DAT
Boldt, O
Michael O
S. O
Pedersen, O

Thomas B-DAT
Lunner, O
and O
DeLiang O

Wang, B-DAT
“Role O
of O
mask O
pattern O

in B-DAT
intelligibility O
of O
ideal O
binary-masked O

noisy B-DAT
speech,” O
The O
Journal O
of O

the B-DAT
Acoustical O
Society O
of O
America, O

vol. B-DAT
126, O
no. O
3, O
pp. O
1415 O

–1426, B-DAT
2009. O
[25] O
A. O
Graves, O
A. O
Mohamed O

, B-DAT
and O
G. O
Hinton, O
“Speech O

recognition B-DAT
with O
deep O
recurrent O
neural O

networks,” B-DAT
in O
2013 O
IEEE O
International O

Con- B-DAT
ference O
on O
Acoustics, O
Speech O

and B-DAT
Signal O
Processing, O
May O
2013, O

pp. B-DAT
6645–6649. O
[26] O
Diederik O
P O
Kingma O
and O

Jimmy B-DAT
Ba, O
“Adam: O
A O
method O

for B-DAT
stochastic O
optimization,” O
arXiv O
preprint O

arXiv:1412.6980, B-DAT
2014 O

. B-DAT
[27] O
E. O
Vincent, O
R. O
Gribonval O

, B-DAT
and O
C. O
Fevotte, O
“Performance O

measure- B-DAT
ment O
in O
blind O
audio O

source B-DAT
separation,” O
IEEE O
Transactions O
on O

Audio, B-DAT
Speech O
and O
Language O
Processing, O

vol. B-DAT
14, O
no. O
4, O
pp. O
1462 O

–1469, B-DAT
July O
2006. O
[28] O
Colin O
Raffel, O
Brian O
McFee O

, B-DAT
Eric O
J O
Humphrey, O
Justin O

Salamon, B-DAT
Oriol O
Nieto, O
Dawen O
Liang, O

Daniel B-DAT
PW O
Ellis, O
and O
C O

Colin B-DAT
Raffel, O
“mir O
eval: O
A O

transparent B-DAT
implementation O
of O
common O
mir O

metrics,” B-DAT
in O
In O
Proceed- O
ings O

of B-DAT
the O
15th O
International O
Society O

for B-DAT
Music O
Information O
Retrieval O
Conference, O

ISMIR. B-DAT
Citeseer, O
2014. O
[29] O
A.W. O
Rix, O
J.G. O
Beerends O

, B-DAT
M.P. O
Hollier, O
and O
A.P. O

Hekstra, B-DAT
“Perceptual O
evaluation O
of O
speech O

quality B-DAT
(PESQ)-a O
new O
method O
for O

speech B-DAT
qual- O
ity O
assessment O
of O

telephone B-DAT
networks O
and O
codecs,” O
in O
2001 O
IEEE O
In- O
ternational O
Conference O
on O

Acoustics, B-DAT
Speech, O
and O
Signal O
Processing O

. B-DAT
Proceedings O
(Cat. O
No.01CH37221), O
Salt O

Lake B-DAT
City, O
UT, O
USA, O
2001, O

vol. B-DAT
2, O
pp. O
749–752, O
IEEE. O
[30] O
A. O
Hines, O
J. O
Skoglund O

, B-DAT
A. O
Kokaram, O
and O
N. O

Harte, B-DAT
“ViSQOL: O
The O
Virtual O
Speech O

Quality B-DAT
Objective O
Listener,” O
in O
IWAENC O
2012 O

; B-DAT
Inter- O
national O
Workshop O
on O

Acoustic B-DAT
Signal O
Enhancement, O
Sept. O
2012, O

pp. B-DAT
1 O

4 B-DAT

1 B-DAT
Introduction O

1.1 B-DAT
Related O
work O

2 B-DAT
MODEL O
ARCHITECTURES O

2.1 B-DAT
VL2M O
model O

2.2 B-DAT
VL2M_ref O
model O

2.3 B-DAT
Audio-Visual O
concat O
model O

2.4 B-DAT
Audio-Visual O
concat-ref O
model O

3 B-DAT
Experimental O
setup O

3.1 B-DAT
Dataset O

3.2 B-DAT
LSTM O
training O

3.3 B-DAT
Audio O
pre- O
and O
post-processing O

3.4 B-DAT
Video O
pre-processing O

4 B-DAT
Results O

5 B-DAT
Conclusion O

6 B-DAT
References O

evaluated O
on O
the O
limited O
size O
GRID B-DAT
and O
TCD-TIMIT O
datasets, O
that O
achieve O

trained O
and O
evaluated O
on O
the O
GRID B-DAT
[9] O
and O
TCD-TIMIT O
[10] O
datasets O

were O
carried O
out O
using O
the O
GRID B-DAT
[9] O
and O
TCD-TIMIT O
[10] O
audio-visual O

Regarding O
the O
GRID B-DAT
corpus, O
for O
each O
of O
the O

the O
same O
procedure O
as O
for O
GRID, B-DAT
with O
one O
difference. O
Con- O
trary O

to O
GRID, B-DAT
TCD-TIMIT O
utterances O
have O
different O
dura O

Table O
1. O
GRID B-DAT
results O
- O
speaker-dependent. O
The O
“Noisy O

Table O
2. O
GRID B-DAT
results O
- O
speaker-independent O

We O
upsampled O
from O
25/29.97 O
fps O
(GRID B-DAT

systems O
trained O
and O
tested O
on O
GRID B-DAT
and O
TCD- O
TIMIT O
to O
compare O

of O
speaker-dependent O
models O
on O
the O
GRID B-DAT
corpus O
with O
landmark O
motion O
vectors O

speaker-independent O
test-set O
results O
on O
the O
GRID B-DAT
and O
TCD-TIMIT O
datasets O
respectively. O
V2ML O

evaluated O
on O
the O
limited O
size O
GRID B-DAT
and O
TCD-TIMIT O
datasets O
that O
accomplish O

are O
applied O
to O
the O
acoustic O
mixed-speech B-DAT
spectrogram. O
Results O
show O
that: O
(i O

landmark O
features O
and O
the O
input O
mixed-speech B-DAT
spectrogram O

applied O
to O
clean O
the O
acoustic O
mixed-speech B-DAT
spectrogram O

compressed O
spectrogram O
of O
the O
single-channel O
mixed-speech B-DAT
signal. O
All O
of O
them O
perform O

although O
IAM O
generation O
requires O
the O
mixed-speech B-DAT
spectrogram, O
separate O
spectrograms O
for O
each O

of O
them, O
we O
created O
a O
mixed-speech B-DAT
version O

98 O
utterances O
per O
speaker. O
The O
mixed-speech B-DAT
version O
was O
created O
following O
the O

metric O
values O
of O
the O
input O
mixed-speech B-DAT
signal O

speaker’s O
spectrogram O
from O
the O
acoustic O
mixed-speech B-DAT
spectrogram O

are O
applied O
to O
the O
acoustic O
mixed-speech B-DAT
spectrogram. O
Results O
show O
that: O
(i O

landmark O
features O
and O
the O
input O
mixed-speech B-DAT
spectrogram O

applied O
to O
clean O
the O
acoustic O
mixed-speech B-DAT
spectrogram O

compressed O
spectrogram O
of O
the O
single-channel O
mixed-speech B-DAT
signal. O
All O
of O
them O
perform O

although O
IAM O
generation O
requires O
the O
mixed-speech B-DAT
spectrogram, O
separate O
spectrograms O
for O
each O

of O
them, O
we O
created O
a O
mixed-speech B-DAT
version O

98 O
utterances O
per O
speaker. O
The O
mixed-speech B-DAT
version O
was O
created O
following O
the O

metric O
values O
of O
the O
input O
mixed-speech B-DAT
signal O

speaker’s O
spectrogram O
from O
the O
acoustic O
mixed-speech B-DAT
spectrogram O

Regarding O
the O
GRID O
corpus, B-DAT
for O
each O
of O
the O
33 O

The O
TCD-TIMIT O
corpus B-DAT
consists O
of O
59 O
speakers O
(we O

speaker-dependent O
models O
on O
the O
GRID O
corpus B-DAT
with O
landmark O
motion O
vectors. O
Results O

and O
Xu O
Shao, O
“An O
audio-visual O
corpus B-DAT
for O
speech O
perception O
and O
automatic O

Regarding O
the O
GRID B-DAT
corpus, I-DAT
for O
each O
of O
the O
33 O

of O
speaker-dependent O
models O
on O
the O
GRID B-DAT
corpus I-DAT
with O
landmark O
motion O
vectors. O
Results O

FACE B-DAT
LANDMARK-BASED O
SPEAKER-INDEPENDENT O
AUDIO-VISUAL O
SPEECH O

ENHANCEMENT B-DAT
IN O
MULTI-TALKER O
ENVIRONMENTS O
Giovanni O
Morrone? O
Luca O
Pasa† O
Vadim O

Tikhanoff B-DAT

Sonia B-DAT
Bergamaschi? O
Luciano O
Fadiga† O
Leonardo O

Badino† B-DAT
?Department O
of O
Engineering O
”Enzo O
Ferrari O

”, B-DAT
University O
of O
Modena O
and O

Reggio B-DAT
Emilia, O
Modena, O
Italy O
†Istituto O

Italiano B-DAT
di O
Tecnologia, O
Ferrara, O
Italy O
ABSTRACT O

In B-DAT
this O
paper, O
we O
address O

the B-DAT
problem O
of O
enhancing O
the O

speech B-DAT
of O
a O
speaker O
of O

interest B-DAT
in O
a O
cocktail O
party O

scenario B-DAT
when O
vi- O
sual O
information O

of B-DAT
the O
speaker O
of O
interest O

is B-DAT
available. O
Contrary O
to O
most O
previous O
studies O

, B-DAT
we O
do O
not O
learn O

visual B-DAT
features O
on O
the O
typically O

small B-DAT
audio-visual O
datasets, O
but O
use O

an B-DAT
already O
available O
face O
landmark O

detector B-DAT
(trained O
on O
a O
sep- O

arate B-DAT
image O
dataset). O
The O
landmarks O
are O
used O
by O

LSTM-based B-DAT
models O
to O
gen- O
erate O

time-frequency B-DAT
masks O
which O
are O
applied O

to B-DAT
the O
acoustic O
mixed-speech O
spectrogram O

. B-DAT
Results O
show O
that: O
(i) O

land- B-DAT
mark O
motion O
features O
are O

very B-DAT
effective O
features O
for O
this O

task, B-DAT
(ii) O
similarly O
to O
previous O

work, B-DAT
reconstruction O
of O
the O
target O

speaker’s B-DAT
spectrogram O
mediated O
by O
masking O

is B-DAT
significantly O
more O
accurate O
than O

direct B-DAT
spectrogram O
reconstruction, O
and O
(iii) O

the B-DAT
best O
masks O
depend O
on O

both B-DAT
motion O
landmark O
features O
and O

the B-DAT
input O
mixed-speech O
spectrogram. O
To O
the O
best O
of O
our O

knowledge, B-DAT
our O
proposed O
models O
are O

the B-DAT
first O
models O
trained O
and O

evaluated B-DAT
on O
the O
limited O
size O

GRID B-DAT
and O
TCD-TIMIT O
datasets, O
that O

achieve B-DAT
speaker-independent O
speech O
enhancement O
in O

a B-DAT
multi-talker O
setting O

. B-DAT
Index O
Terms— O
audio-visual O
speech O
enhancement O

, B-DAT
cock- O
tail O
party O
problem, O

time-frequency B-DAT
mask, O
LSTM, O
face O
land- O

marks B-DAT
1. O
INTRODUCTION O

In B-DAT
the O
context O
of O
speech O

perception, B-DAT
the O
cocktail O
party O

effect B-DAT
[1, O
2] O
is O
the O

ability B-DAT
of O
the O
brain O
to O

recognize B-DAT
speech O
in O
complex O
and O

adverse B-DAT
listening O
conditions O
where O
the O

attended B-DAT
speech O
is O
mixed O
with O

competing B-DAT
sounds/speech. O
Speech O
perception O
studies O
have O
shown O

that B-DAT
watching O
speaker’s O
face O
movements O

could B-DAT
dramatically O
improve O
our O
ability O

at B-DAT
recognizing O
the O
speech O
of O

a B-DAT
target O
speaker O
in O
a O

multi-talker B-DAT
environment O
[3, O
4 O

]. B-DAT
This O
work O
aims O
at O
extracting O

the B-DAT
speech O
of O
a O
target O

speaker B-DAT
from O
single O
channel O
audio O

of B-DAT
several O
people O
talking O
simulta O

- B-DAT
neously. O
This O
is O
an O

ill-posed B-DAT
problem O
in O
that O
many O

differ- B-DAT
ent O
hypotheses O
about O
what O

the B-DAT
target O
speaker O
says O
are O

con- B-DAT
sistent O
with O
the O
mixture O
signal O

. B-DAT
Yet, O
it O
can O
be O

solved B-DAT
by O
ex- O
ploiting O
some O

additional B-DAT
information O
associated O
to O
the O

speaker B-DAT
of O
interest O
and/or O
by O

leveraging B-DAT
some O
prior O
knowledge O
about O

speech B-DAT
signal O
properties O
(e.g., O
[5]). O

In B-DAT
this O
work O
we O
use O

face B-DAT
movements O
of O
the O
target O

speaker B-DAT
as O
additional O
information. O
This O
paper O
(i) O
proposes O
the O

use B-DAT
of O
face O
landmark’s O
move O

- B-DAT
ments, O
extracted O
using O

Dlib B-DAT
[6, O
7] O
and O
(ii) O

compares B-DAT
differ- O
ent O
ways O
of O

mapping B-DAT
such O
visual O
features O
into O

time-frequency B-DAT
(T-F) O
masks, O
then O
applied O

to B-DAT
clean O
the O
acoustic O
mixed-speech O

spectrogram. B-DAT
By O
using O
Dlib O
extracted O
landmarks O

we B-DAT
relieve O
our O
mod- O
els O

from B-DAT
the O
task O
of O
learning O

useful B-DAT
visual O
features O
from O
raw O

pixels. B-DAT
That O
aspect O
is O
particularly O

relevant B-DAT
when O
the O
training O
audio-visual O

datasets B-DAT
are O
small O

. B-DAT
The O
analysis O
of O
landmark-dependent O
masking O

strategies B-DAT
is O
motivated O
by O
the O

fact B-DAT
that O
speech O
enhancement O
mediated O

by B-DAT
an O
explicit O
masking O
is O

often B-DAT
more O
effective O
than O
mask-free O

enhancement B-DAT
[8 O

]. B-DAT
All O
our O
models O
were O
trained O

and B-DAT
evaluated O
on O
the O
GRID O

[9] B-DAT
and O
TCD-TIMIT O
[10] O
datasets O

in B-DAT
a O
speaker-independent O
setting O

. B-DAT
1.1. O
Related O
work O

Speech B-DAT
enhancement O
aims O
at O
extracting O

the B-DAT
voice O
of O
a O
tar- O

get B-DAT
speaker, O
while O
speech O
separation O

refers B-DAT
to O
the O
problem O
of O

separating B-DAT
each O
sound O
source O
in O

a B-DAT
mixture. O
Recently O
pro- O
posed O

audio-only B-DAT
single-channel O
methods O
have O
achieved O

very B-DAT
promising O
results O
[11, O
12, O
13 O

]. B-DAT
However O
the O
task O
still O

remains B-DAT
challenging. O
Additionally, O
audio-only O
systems O

need B-DAT
separate O
models O
in O
order O

to B-DAT
associate O
the O
estimated O
separated O

audio B-DAT
sources O
to O
each O
speaker, O

while B-DAT
vision O
easily O
allow O
that O

in B-DAT
a O
unified O
model. O
Regarding O
audio-visual O
speech O
enhancement O
and O

separa- B-DAT
tion O
methods O
an O
extensive O

review B-DAT
is O
provided O
in O
[14 O

]. B-DAT
Here O
we O
focus O
on O

the B-DAT
deep-learning O
methods O
that O
are O

most B-DAT
related O
to O
the O
present O

work. B-DAT
Our O
first O
architecture O
(Section O
2.1 O

) B-DAT
is O
inspired O
by O
[15], O

where B-DAT
a O
pre-trained O
convolutional O
neural O

network B-DAT
(CNN) O
is O
used O
to O

generate B-DAT
a O
clean O
spectrogram O
from O

silent B-DAT
video O
[16]. O
Rather O
than O

directly B-DAT
computing O
a O
time-frequency O
(T-F) O

mask, B-DAT
ar O
X O

iv B-DAT
:1 O
81 O
1 O

. B-DAT
02 O
48 O

0v B-DAT
3 O

cs B-DAT
.C O
L O

2 B-DAT

M B-DAT

2 B-DAT
01 O
9 O

the B-DAT
mask O
is O
computed O
by O

thresholding B-DAT
the O
estimated O
clean O
spectrogram. O

This B-DAT
approach O
is O
not O
very O

effective B-DAT
since O
the O
pre-trained O
CNN O

is B-DAT
designed O
for O
a O
different O

task B-DAT
(video-to- O
speech O
synthesis). O

In B-DAT
[17] O
a O
CNN O
is O

trained B-DAT
to O
directly O
esti- O
mate O

clean B-DAT
speech O
from O
noisy O
audio O

and B-DAT
input O
video. O
A O
sim- O

ilar B-DAT
model O
is O
used O

in B-DAT
[18], O
where O
the O
model O

jointly B-DAT
generates O
clean O
speech O
and O

input B-DAT
video O
in O
a O
denoising-autoender O

archi- B-DAT
tecture. O
[19] O
shows O
that O
using O
information O

about B-DAT
lip O
positions O
can O
help O

to B-DAT
improve O
speech O
enhancement. O
The O

video B-DAT
feature O
vec- O
tor O
is O

obtained B-DAT
computing O
pair-wise O
distances O
between O

any B-DAT
mouth O
landmarks. O
Similarly O
to O

our B-DAT
approach O
their O
visual O
fea O

- B-DAT
tures O
are O
not O
learned O

on B-DAT
the O
audio-visual O
dataset O
but O

are B-DAT
pro- O
vided O
by O
a O

system B-DAT
trained O
on O
different O
dataset. O

Contrary B-DAT
to O
our O
approach, O
[19] O

uses B-DAT
position-based O
features O
while O
we O

use B-DAT
motion O
features O
(of O
the O

whole B-DAT
face) O
that O
in O
our O

experiments B-DAT
turned O
out O
to O
be O

much B-DAT
more O
effective O
than O
positional O

features. B-DAT
Although O
the O
aforementioned O
audio-visual O
methods O

work B-DAT
well, O
they O
have O
only O

been B-DAT
evaluated O
in O
a O
speaker-dependent O

setting. B-DAT
Only O
the O
availability O
of O

new B-DAT
large O
and O
heterogeneous O
audio-visual O

datasets B-DAT
has O
allowed O
the O
training O

of B-DAT
deep O
neu- O
ral O
network-based O

speaker-independent B-DAT
speech O
enhancement O
models O
[20 O

, B-DAT
21, O
22]. O
The O
present O
work O
shows O
that O

huge B-DAT
audio-visual O
datasets O
are O
not O

a B-DAT
necessary O
requirement O
for O
speaker-independent O

audio-visual B-DAT
speech O
enhancement. O
Although O
we O

have B-DAT
only O
considered O
datasets O
with O

simple B-DAT
visual O
scenarios O
(i.e., O
the O

target B-DAT
speaker O
is O
always O
facing O

the B-DAT
camera), O
we O
expect O
our O

methods B-DAT
to O
perform O
well O
in O

more B-DAT
complex O
scenarios O
thanks O
to O

the B-DAT
robust O
landmark O
extraction O

. B-DAT
2. O
MODEL O
ARCHITECTURES O

We B-DAT
experimented O
with O
the O
four O

models B-DAT
shown O
in O
Fig. O
1. O

All B-DAT
models O
receive O
in O
input O

the B-DAT
target O
speaker’s O
landmark O
mo- O

tion B-DAT
vectors O
and O
the O
power-law O

compressed B-DAT
spectrogram O
of O
the O
single-channel O

mixed-speech B-DAT
signal. O
All O
of O
them O

perform B-DAT
some O
kind O
of O
masking O

operation. B-DAT
2.1. O
VL2M O
model O

At B-DAT
each O
time O
frame, O
the O

video-landmark B-DAT
to O
mask O
(VL2M) O
model O
( O

Fig. B-DAT
1a) O
estimates O
a O
T-F O

mask B-DAT
from O
visual O
features O
only O
( O

of B-DAT
the O
target O
speaker). O
Formally, O

given B-DAT
a O
video O
sequence O

= B-DAT
[v1 O

, B-DAT
. O
. O
. O
, O

vT B-DAT
], O
vt O
∈ O
Rn O

and B-DAT
a O
target O
mask O
sequence O

= B-DAT
[m1 O

, B-DAT
. O
. O
. O
, O

mT B-DAT
], O
mt O
∈ O
Rd, O

VL2M B-DAT
perform O
a O
function O

Fvl2m(v) B-DAT
= O
m̂, O
where O
m̂ O

is B-DAT
the O
estimated O
mask. O
The O
training O
objective O
for O
VL2M O

is B-DAT
a O
Target O
Binary O
Mask O

(TBM) B-DAT
[23, O
24], O
computed O
using O

the B-DAT
spectrogram O
of O
the O
tar O

- B-DAT
get O
speaker O
only. O
This O

is B-DAT
motivated O
by O
our O
goal O

of B-DAT
extracting O
the O
speech O
of O

a B-DAT
target O
speaker O
as O
much O

as B-DAT
possible O
indepen- O
dently O
of O

the B-DAT
concurrent O
speakers, O
so O
that, O

e.g., B-DAT
we O
do O
not O
need O

to B-DAT
estimate O
their O
number. O
An O

additional B-DAT
motivations O
is O
that O
the O

model B-DAT
takes O
as O
only O
input O

the B-DAT
visual O
features O
of O
the O
target O
speaker, O
and O
a O
target O

TBM B-DAT
that O
only O
depends O
on O

the B-DAT
target O
speaker O
allows O
VL2M O

to B-DAT
learn O
a O
function O
(rather O

than B-DAT
approximating O
an O
ill-posed O
one-to-many O

mapping B-DAT

). B-DAT
Given O
a O
clean O
speech O
spectrogram O

of B-DAT
a O
speaker O
s O

= B-DAT
[s1 O

, B-DAT
. O
. O
. O
, O

sT B-DAT
], O
st O
∈ O
Rd, O

the B-DAT
TBM O
is O
defined O
by O

comparing, B-DAT
at O
each O
frequency O
bin O

∈ B-DAT
[1 O

, B-DAT
. O
. O
. O
, O

d], B-DAT
the O
target O
speaker O
value O

st[f B-DAT
] O
vs. O
a O
reference O

threshold B-DAT
τ O
[f O
]. O
As O

in B-DAT
[15], O
we O
use O
a O

function B-DAT
of O
long-term O
average O
speech O

spectrum B-DAT
(LTASS) O
as O
reference O
threshold. O

This B-DAT
threshold O
indicates O
if O
a O

T-F B-DAT
unit O
is O
generated O
by O

the B-DAT
speaker O
or O
refers O
to O

silence B-DAT
or O
noise. O
The O
process O

to B-DAT
compute O
the O
speaker’s O
TBM O

is B-DAT
as O
follows: O
1. O
The O
mean O
π[f O

] B-DAT
and O
the O
standard O
deviation O

σ[f B-DAT
] O
are O
computed O
for O

all B-DAT
frequency O
bins O
of O
all O

seen B-DAT
spectro- O
grams O
in O
speaker’s O

data B-DAT

. B-DAT
2. O
The O
threshold O
τ O
[f O

] B-DAT
is O
defined O
as O
τ O

[f B-DAT
] O
= O
π[f O
]+0.6 O

·σ[f B-DAT
] O
where O
0.6 O
is O

a B-DAT
value O
selected O
by O
manual O

inspection B-DAT
of O
several O
spectrogram-TBM O
pairs O

. B-DAT
3. O
The O
threshold O
is O
applied O

to B-DAT
every O
speaker’s O
speech O
spec O

- B-DAT
trogram O
s. O
mt[f O

1, B-DAT
if O
st[f O
] O
≥ O

τ B-DAT
[f O
], O
0, O
otherwise. O
The O
mapping O
Fvl2m(·) O
is O
carried O

out B-DAT
by O
a O
stacked O
bi O

- B-DAT
directional O
Long O
Short-Term O
Memory O
( O

BLSTM) B-DAT
network O
[25]. O
The O
BLSTM O

outputs B-DAT
are O
then O
forced O
to O

lay B-DAT
within O
the O
[0, O
1] O

range. B-DAT
Finally O
the O
computed O
TBM O

m̂ B-DAT
and O
the O
noisy O
spectrogram O

y B-DAT
are O
element-wise O
multiplied O
to O

ob- B-DAT
tain O
the O
estimated O
clean O

spectrogram B-DAT
ŝm O
= O
m̂ O
◦ O

y, B-DAT
where O

= B-DAT
[y1 O

, B-DAT
. O
. O
. O

yT B-DAT
], O
yt O
∈ O
Rd. O
The O
model O
parameters O
are O
estimated O

to B-DAT
minimize O
the O
loss O

: B-DAT
Jvl2m O
= O
∑T O

t=1 B-DAT
∑d O
f=1−mt[f O
] O
· O
log(m̂t[f O

])− B-DAT
(1−mt[f O
]) O
· O
log(1 O

− B-DAT
m̂t[f O
]) O
2.2. O
VL2M O
ref O
model O

VL2M B-DAT
generates O
T-F O
masks O
that O

are B-DAT
independent O
of O
the O
acous- O

tic B-DAT
context. O
We O
may O
want O

to B-DAT
refine O
the O
masking O
by O

including B-DAT
such O
context. O
This O
is O

what B-DAT
the O
novel O
VL2M O
ref O

does B-DAT
(Fig. O
1b). O
The O
computed O

TBM B-DAT
m̂ O
and O
the O
input O

spectrogram B-DAT
y O
are O
the O
input O

to B-DAT
a O
function O
that O
outputs O

an B-DAT
Ideal O
Amplitude O
Mask O
(IAM) O

p B-DAT
(known O
as O
FFT-MASK O

in B-DAT
[8]). O
Given O
the O
target O

clean B-DAT
spectrogram O
s O
and O
the O

noisy B-DAT
spectrogram O
y, O
the O
IAM O

is B-DAT
defined O
as: O
pt[f O
] O
= O
st[f O

yt[f B-DAT
] O
Note O
that O
although O
IAM O
generation O

requires B-DAT
the O
mixed-speech O
spectrogram, O
separate O

spectrograms B-DAT
for O
each O
concurrent O
speakers O

are B-DAT
not O
required O

. B-DAT
The O
target O
speaker’s O
spectrogram O
s O

is B-DAT
reconstructed O
by O
multiplying O
the O

input B-DAT
spectrogram O
with O
the O
estimated O

IAM. B-DAT
Values O
greater O
than O
10 O

in B-DAT
the O
IAM O
are O
clipped O

to B-DAT
10 O
in O
order O
to O

obtain B-DAT
better O
numerical O
stability O
as O

suggested B-DAT
in O
[8 O

v: B-DAT
video O
input O
y: O
noisy O

spectrogram B-DAT
sm: O
clean O
spectrogram O
TBM O

s: B-DAT
clean O
spectrogram O
IAM O
m: O

TBM B-DAT
p: O
IAM O
STACKED O

BLSTM B-DAT
m O

sm B-DAT
v O

y B-DAT
(a) O
VL2M O

v B-DAT
VL2M O
m O
y O
BLSTM O

BLSTM B-DAT
Fusion O
layer O

BLSTM B-DAT
p O
s O

b) B-DAT
VL2M O
ref O
v O

y B-DAT
p O
STACKED O

BLSTM B-DAT
s O

c) B-DAT
Audio-Visual O
concat O
sm O

y B-DAT
p O
STACKED O

BLSTM B-DAT
s O

v B-DAT
VL2M O
m O
(d) O
Audio-Visual O
concat-ref O

Fig. B-DAT
1. O
Model O
architectures. O
The O
model O
performs O
a O
function O

Fmr(v, B-DAT
y) O
= O
p̂ O
that O

con- B-DAT
sists O
of O
a O
VL2M O

component B-DAT
plus O
three O
different O
BLSTMs O

Gm, B-DAT
Gy O
and O
H O

Gm(Fvl2m(v)) B-DAT
= O
rm O
receives O
the O

VL2M B-DAT
mask O
m̂ O
as O
in- O

put, B-DAT
and O
Gy(y) O
= O
ry O

is B-DAT
fed O
with O
the O
noisy O

spectrogram. B-DAT
Their O
output O
rm, O

ry B-DAT
∈ O
Rz O
are O
fused O

in B-DAT
a O
joint O
audio-visual O
represen- O

tation B-DAT

= B-DAT
[h1 O

, B-DAT
. O
. O
. O
, O

hT B-DAT
], O
where O
ht O
is O

a B-DAT
linear O
combination O
of O
rmt O

and B-DAT
ryt O
: O
ht O
= O

Whm B-DAT
·rmt O
+Why O
·ryt O
+bh. O

h B-DAT
is O
the O
input O
of O

the B-DAT
third O
BLSTM O
H O
( O

h) B-DAT
= O
p̂, O
where O
p̂ O

lays B-DAT
in O
the O
[0,10] O
range. O

The B-DAT
loss O
function O
is: O
Jmr O

T∑ B-DAT
t=1 O
d∑ O
f=1 O

p̂t[f B-DAT
] O
· O
yt[f O
]− O

st[f B-DAT
])2 O
2.3. O
Audio-Visual O
concat O
model O

The B-DAT
third O
model O
(Fig. O
1c) O

performs B-DAT
early O
fusion O
of O
audio- O

visual B-DAT
features. O
This O
model O
consists O

of B-DAT
a O
single O
stacked O
BLSTM O

that B-DAT
computes O
the O
IAM O
mask O

p̂ B-DAT
from O
the O
concate- O

nated B-DAT
[v,y]. O
The O
training O
loss O

is B-DAT
the O
same O
Jmr O
used O

to B-DAT
train O
VL2M O
ref. O
This O

model B-DAT
can O
be O
regarded O
as O

a B-DAT
simplification O
of O
VL2M O
ref, O

where B-DAT
the O
VL2M O
operation O
is O

not B-DAT
performed. O
2.4. O
Audio-Visual O
concat-ref O
model O

The B-DAT
fourth O
model O
(Fig. O
1d) O

is B-DAT
an O
improved O
version O
of O

the B-DAT
model O
described O
in O
section O
2 O

.3. B-DAT
The O
only O
difference O
is O

the B-DAT
input O
of O
the O
stacked O

BLSTM B-DAT
that O
is O
replaced O

by B-DAT
[̂sm,y] O
where O
ŝm O
is O

the B-DAT
denoised O
spectrogram O
returned O
by O

VL2M B-DAT
operation. O
3. O
EXPERIMENTAL O
SETUP O

3.1. B-DAT
Dataset O
All O
experiments O
were O
carried O
out O

using B-DAT
the O
GRID O
[9] O
and O

TCD-TIMIT B-DAT
[10] O
audio-visual O
datasets. O
For O

each B-DAT
of O
them, O
we O
created O

a B-DAT
mixed-speech O
version O

. B-DAT
Regarding O
the O
GRID O
corpus, O
for O

each B-DAT
of O
the O
33 O
speakers O

(one B-DAT
had O
to O
be O
discarded O

) B-DAT
we O
first O
randomly O
selected O
200 O
ut- O
terances O
(out O
of O
1000 O

). B-DAT
Then, O
for O
each O
utterance, O

we B-DAT
created O
3 O
different O
audio-mixed O

samples. B-DAT
Each O
audio-mixed O
sample O
was O

created B-DAT
by O
mixing O
the O
chosen O

utterance B-DAT
with O
one O
utter- O
ance O

from B-DAT
a O
different O
speaker. O
That O
resulted O
in O
600 O
audio-mixed O

samples B-DAT
per O
speaker O

. B-DAT
The O
resulting O
dataset O
was O
split O

into B-DAT
disjoint O
sets O
of O
25/4/4 O

speakers B-DAT
for O
training/validation/testing O
respectively O

. B-DAT
The O
TCD-TIMIT O
corpus O
consists O
of O

59 B-DAT
speakers O
(we O
ex- O
cluded O

3 B-DAT
professionally-trained O
lipspeakers) O
and O
98 O

utterances B-DAT
per O
speaker. O
The O
mixed-speech O

version B-DAT
was O
created O
following O
the O

same B-DAT
procedure O
as O
for O
GRID O

, B-DAT
with O
one O
difference. O
Con- O

trary B-DAT
to O
GRID, O
TCD-TIMIT O
utterances O

have B-DAT
different O
dura- O
tion. O
Thus O
2 O
utterances O
were O
mixed O
only O
if O

their B-DAT
duration O
dif- O
ference O
did O

not B-DAT
exceed O
2 O
seconds. O
For O

each B-DAT
utterance O
pair, O
we O
forced O

the B-DAT
non-target O
speaker’s O
utterance O
to O

match B-DAT
the O
du- O
ration O
of O

the B-DAT
target O
speaker O
utterance. O
If O

it B-DAT
was O
longer, O
the O
utterance O

was B-DAT
cut O
at O
its O
end O

, B-DAT
whereas O
if O
it O
was O

shorter, B-DAT
silence O
samples O
were O
equally O

added B-DAT
at O
its O
start O
and O

end. B-DAT
The O
resulting O
dataset O
was O
split O

into B-DAT
disjoint O
sets O
of O
51/4/4 O

speakers B-DAT
for O
training/validation/testing O
respectively O

. B-DAT
3.2. O
LSTM O
training O

In B-DAT
all O
experiments, O
the O
models O

were B-DAT
trained O
using O
the O
Adam O

optimizer B-DAT
[26]. O
Early O
stopping O
was O

applied B-DAT
when O
the O
error O
on O

the B-DAT
validation O
set O
did O
not O

decrease B-DAT
over O
5 O
consecutive O
epochs. O
VL2M, O
AV O
concat O
and O
AV O

concat-ref B-DAT
had O
5, O
3 O
and O

3 B-DAT
stacked O
BLSTM O
layers O
respectively O

. B-DAT
All O
BLSTMs O
had O
250 O

units. B-DAT
Hyper-parameters O
selection O
was O
performed O

by B-DAT
using O
random O
search O
with O

a B-DAT
limited O
number O
of O
samples, O

therefore B-DAT
all O
the O
reported O
results O

may B-DAT
improve O
through O
a O
deeper O

hyper- B-DAT
parameters O
validation O
phase. O
VL2M O
ref O
and O
AV O
concat-ref O

training B-DAT
was O
performed O
in O
2 O

steps. B-DAT
We O
first O
pre-trained O
the O

models B-DAT
using O
the O
oracle O
TBM O

m. B-DAT
Then O
we O
substituted O
the O

oracle B-DAT
masks O
with O
the O
VL2M O

component B-DAT
and O
retrained O
the O
models O

while B-DAT
freezing O
the O
pa- O
rameters O

of B-DAT
the O
VL2M O
component O

. B-DAT
3.3. O
Audio O
pre- O
and O
post-processing O

The B-DAT
original O
waveforms O
were O
resampled O

to B-DAT
16 O
kHz. O
Short- O
Time O

Fourier B-DAT
Transform O
(STFT) O
x O
was O

computed B-DAT
using O
FFT O
size O
of O
512, O
Hann O
window O
of O
length O
25 O

ms B-DAT
(400 O
samples), O
and O
hop O

length B-DAT
of O
10 O
ms O
(160 O

samples). B-DAT
The O
input O
spectro- O
gram O

was B-DAT
obtained O
taking O
the O
STFT O

magnitude B-DAT
and O
perform- O
ing O
power-law O

compression B-DAT
|x|p O
with O
p O

= B-DAT
0.3. O
Finally O
we O
applied O

per-speaker B-DAT
0-mean O
1-std O
normalization O

. B-DAT
In O
the O
post-processing O
stage, O
the O

enhanced B-DAT
waveform O
gen- O
erated O
by O

the B-DAT
speech O
enhancement O
models O
was O

reconstructed B-DAT

SDR B-DAT
PESQ O
ViSQOL O
Noisy O
−1.06 O
1.81 O
2.11 O
VL2M O

3.17 B-DAT
1.51 O
1.16 O
VL2M O
ref O

6.50 B-DAT
2.58 O
2.99 O
AV O
concat O

6.31 B-DAT
2.49 O
2.83 O
AV O
c-ref O

6.17 B-DAT
2.58 O
2.96 O

Table B-DAT
1. O
GRID O
results O
- O

speaker-dependent. B-DAT
The O
“Noisy” O
row O
refers O

to B-DAT
the O
metric O
values O
of O

the B-DAT
input O
mixed-speech O
signal. O
2 O
Speakers O
3 O
Speakers O
SDR O

PESQ B-DAT
ViSQOL O
SDR O
PESQ O
ViSQOL O

Noisy B-DAT
0.21 O
1.94 O
2.58 O
−5.34 O
1 O

.43 B-DAT
1.62 O
VL2M O
3.02 O
1.81 O
1 O

.70 B-DAT
−2.03 O
1.43 O
1.25 O
VL2M O

ref B-DAT
6.52 O
2.53 O
3.02 O
2.83 O
2 O

.19 B-DAT
2.53 O
AV O
concat O
7.37 O
2 O

.65 B-DAT
3.03 O
3.02 O
2.24 O
2.49 O

AV B-DAT
c-ref O
8.05 O
2.70 O
3.07 O
4 O

.02 B-DAT
2.33 O
2.64 O
Table O
2. O
GRID O
results O

- B-DAT
speaker-independent O

. B-DAT
by O
applying O
the O
inverse O
STFT O

to B-DAT
the O
estimated O
clean O
spectro O

- B-DAT
gram O
and O
using O
the O

phase B-DAT
of O
the O
noisy O
input O

signal. B-DAT
3.4. O
Video O
pre-processing O

Face B-DAT
landmarks O
were O
extracted O
from O

video B-DAT
using O
the O
Dlib O
[7] O

implementation B-DAT
of O
the O
face O
landmark O

estimator B-DAT
described O
in O
[6]. O
It O

returns B-DAT
68 O
x-y O
points, O
for O

an B-DAT
overall O
136 O
values. O
We O

upsampled B-DAT
from O
25/29.97 O
fps O
(GRID/TCD-TIMIT) O

to B-DAT
100 O
fps O
to O
match O

the B-DAT
frame O
rate O
of O
the O

audio B-DAT
spectrogram. O
Upsampling O
was O
carried O

out B-DAT
through O
linear O
interpolation O
over O

time. B-DAT
The O
final O
video O
feature O
vector O

v B-DAT
was O
obtained O
by O
com O

- B-DAT
puting O
the O
per-speaker O
normalized O

motion B-DAT
vector O
of O
the O
face O

landmarks B-DAT
by O
simply O
subtracting O
every O

frame B-DAT
with O
the O
previ- O
ous O

one. B-DAT
The O
motion O
vector O
of O

the B-DAT
first O
frame O
was O
set O

to B-DAT
zero. O
4. O
RESULTS O

In B-DAT
order O
to O
compare O
our O

models B-DAT
to O
previous O
works O
in O

both B-DAT
speech O
enhancement O
and O
separation, O

we B-DAT
evaluated O
the O
perfor- O
mance O

of B-DAT
the O
proposed O
models O
using O

both B-DAT
speech O
separation O
2 O
Speakers O
3 O
Speakers O
SDR O

PESQ B-DAT
ViSQOL O
SDR O
PESQ O
ViSQOL O

Noisy B-DAT
0.21 O
2.22 O
2.74 O
−3.42 O
1 O

.92 B-DAT
2.04 O
VL2M O
2.88 O
2.25 O
2 O

.62 B-DAT
−0.51 O
1.99 O
1.98 O
VL2M O

ref B-DAT
9.24 O
2.81 O
3.09 O
5.27 O
2 O

.44 B-DAT
2.54 O
AV O
concat O
9.56 O
2 O

.80 B-DAT
3.09 O
5.15 O
2.41 O
2.52 O

AV B-DAT
c-ref O
10.55 O
3.03 O
3.21 O
5 O

.37 B-DAT
2.45 O
2.58 O
Table O
3. O
TCD-TIMIT O
results O

- B-DAT
speaker-independent O

. B-DAT
and O
enhancement O
metrics. O
Specifically, O
we O

measured B-DAT
the O
ca- O
pability O
of O

separating B-DAT
the O
target O
utterance O
from O

the B-DAT
concurrent O
utterance O
with O
the O

source-to-distortion B-DAT
ratio O
(SDR) O
[27, O
28 O

]. B-DAT
While O
the O
quality O
of O

estimated B-DAT
target O
speech O
was O
measured O

with B-DAT
the O
perceptual O
PESQ O
[29] O

and B-DAT
ViSQOL O
[30] O
metrics. O
For O

PESQ B-DAT
we O
used O
the O
narrow O

band B-DAT
mode O
while O
for O
ViSQOL O

we B-DAT
used O
the O
wide O
band O

mode. B-DAT
As O
a O
very O
first O
experiment O

we B-DAT
compared O
landmark O
posi- O
tion O

vs. B-DAT
landmark O
motion O
vectors. O
It O

turned B-DAT
out O
that O
landmark O
positions O

performed B-DAT
poorly, O
thus O
all O
results O

reported B-DAT
here O
refer O
to O
landmark O

motion B-DAT
vectors O
only O

. B-DAT
We O
then O
carried O
out O
some O

speaker-dependent B-DAT
experiments O
to O
compare O
our O

models B-DAT
with O
previous O
studies O
as O

, B-DAT
to O
the O
best O
of O

our B-DAT
knowledge, O
there O
are O
no O

reported B-DAT
results O
of O
speaker- O
independent O

systems B-DAT
trained O
and O
tested O
on O

GRID B-DAT
and O
TCD- O
TIMIT O
to O

compare B-DAT
with. O
Table O
1 O
reports O

the B-DAT
test-set O
evalua- O
tion O
of O

speaker-dependent B-DAT
models O
on O
the O
GRID O

corpus B-DAT
with O
landmark O
motion O
vectors. O

Results B-DAT
are O
comparable O
with O
previ- O

ous B-DAT
state-of-the-art O
studies O
in O
an O

almost B-DAT
identical O
setting O
[15, O
17]. O
Table O
2 O
and O
3 O
show O

speaker-independent B-DAT
test-set O
results O
on O
the O

GRID B-DAT
and O
TCD-TIMIT O
datasets O
respectively O

. B-DAT
V2ML O
performs O
significantly O
worse O

than B-DAT
the O
other O
three O
models O

in- B-DAT
dicating O
that O
a O
successful O

mask B-DAT
generation O
has O
to O
depend O

on B-DAT
the O
acoustic O
context. O
The O

performance B-DAT
of O
the O
three O
models O

in B-DAT
the O
speaker-independent O
setting O
is O

comparable B-DAT
to O
that O
in O
the O

speaker-dependent B-DAT
setting. O
AV O
concat-ref O
outperforms O
V2ML O
ref O

and B-DAT
AV O
concat O
for O
both O

datasets. B-DAT
This O
supports O
the O
utility O

of B-DAT
a O
refinement O
strat- O
egy O

and B-DAT
suggests O
that O
the O
refinement O

is B-DAT
more O
effective O
when O
it O

directly B-DAT
refines O
the O
estimated O
clean O

spectrogram, B-DAT
rather O
than O
refining O
the O

estimated B-DAT
mask O

. B-DAT
Finally, O
we O
evaluated O
the O
systems O

in B-DAT
a O
more O
challenging O
testing O

condition B-DAT
where O
the O
target O
utterance O

was B-DAT
mixed O
with O
2 O
utterances O

from B-DAT
2 O
competing O
speakers. O
Despite O

the B-DAT
model O
was O
trained O
with O

mixtures B-DAT
of O
two O
speakers, O
the O

decrease B-DAT
of O
performance O
was O
not O

dramatic B-DAT

. B-DAT
Code O
and O
some O
testing O
examples O

of B-DAT
our O
models O
are O
avail O

- B-DAT
able O
at O
https://goo.gl/3h1NgE. O
5. O
CONCLUSION O

This B-DAT
paper O
proposes O
the O
use O

of B-DAT
face O
landmark O
motion O
vec- O

tors B-DAT
for O
audio-visual O
speech O
enhancement O

in B-DAT
a O
single-channel O
multi-talker O
scenario. O

Different B-DAT
models O
are O
tested O
where O

land- B-DAT
mark O
motion O
vectors O
are O

used B-DAT
to O
generate O
time-frequency O
(T- O

F) B-DAT
masks O
that O
extract O
the O

target B-DAT
speaker’s O
spectrogram O
from O
the O

acoustic B-DAT
mixed-speech O
spectrogram. O
To O
the O
best O
of O
our O

knowledge, B-DAT
some O
of O
the O
proposed O

mod- B-DAT
els O
are O
the O
first O

models B-DAT
trained O
and O
evaluated O
on O

the B-DAT
limited O
size O
GRID O
and O

TCD-TIMIT B-DAT
datasets O
that O
accomplish O
speaker O

- B-DAT
independent O
speech O
enhancement O
in O

the B-DAT
multi-talker O
setting, O
with O
a O

quality B-DAT
of O
enhancement O
comparable O
to O

that B-DAT
achieved O
in O
a O
speaker-dependent O

setting. B-DAT
https://goo.gl/3h1NgE O

6. B-DAT
REFERENCES O
[1] O
E. O
Colin O
Cherry, O
“Some O

experiments B-DAT
on O
the O
recognition O
of O

speech, B-DAT
with O
one O
and O
with O

two B-DAT
ears,” O
The O
Journal O
of O

the B-DAT
Acoustical O
Society O
of O
America O

, B-DAT
vol. O
25, O
no. O
5, O

pp. B-DAT
975–979, O
1953. O
[2] O
Josh O
H O
McDermott, O
“The O

cocktail B-DAT
party O
problem,” O
Current O
Biology O

, B-DAT
vol. O
19, O
no. O
22, O

pp. B-DAT
R1024–R1027, O
2009. O
[3] O
Elana O
Zion O
Golumbic, O
Gregory O

B. B-DAT
Cogan, O
Charles O
E. O
Schroeder O

, B-DAT
and O
David O
Poeppel, O
“Visual O

input B-DAT
enhances O
selective O
speech O
envelope O

tracking B-DAT
in O
auditory O
cortex O
at O

a B-DAT
“cocktail O
party”,” O
Journal O
of O

Neu- B-DAT
roscience, O
vol. O
33, O
no. O
4, O
pp. O
1417–1426, O
2013 O

. B-DAT
[4] O
Wei O
Ji O
Ma, O
Xiang O

Zhou, B-DAT
Lars O
A. O
Ross, O
John O

J. B-DAT
Foxe, O
and O
Lucas O
C O

. B-DAT
Parra, O
“Lip-reading O
aids O
word O

recognition B-DAT
most O
in O
moderate O
noise: O

A B-DAT
bayesian O
explanation O
using O
high-dimensional O

feature B-DAT
space,” O
PLOS O
ONE, O
vol. O
4, O
no. O
3, O
pp. O
1–14, O
03 O

2009 B-DAT

. B-DAT
[5] O
Albert O
S O
Bregman, O
Auditory O

scene B-DAT
analysis: O
The O
perceptual O
organi O

- B-DAT
zation O
of O
sound, O
MIT O

press, B-DAT
1994. O
[6] O
Vahid O
Kazemi O
and O
Josephine O

Sullivan, B-DAT
“One O
millisecond O
face O
align O

- B-DAT
ment O
with O
an O
ensemble O

of B-DAT
regression O
trees,” O
in O
The O

IEEE B-DAT
Conference O
on O
Computer O
Vision O

and B-DAT
Pattern O
Recognition O
(CVPR), O
June O
2014 O

. B-DAT
[7] O
Davis O
E. O
King, O
“Dlib-ml O

: B-DAT
A O
machine O
learning O
toolkit,” O

Journal B-DAT
of O
Machine O
Learning O
Research, O

vol. B-DAT
10, O
pp. O
1755–1758, O
2009. O
[8] O
Yuxuan O
Wang, O
Arun O
Narayanan O

, B-DAT
and O
DeLiang O
Wang, O
“On O

Training B-DAT
Targets O
for O
Supervised O
Speech O

Separation,” B-DAT
IEEE/ACM O
Transactions O
on O
Audio, O

Speech, B-DAT
and O
Language O
Processing, O
vol. O
22, O
no. O
12, O
pp. O
1849–1858, O
Dec O

. B-DAT
2014. O
[9] O
Martin O
Cooke, O
Jon O
Barker O

, B-DAT
Stuart O
Cunningham, O
and O
Xu O

Shao, B-DAT
“An O
audio-visual O
corpus O
for O

speech B-DAT
perception O
and O
automatic O
speech O

recognition,” B-DAT
The O
Journal O
of O
the O

Acoustical B-DAT
Society O
of O
America, O
vol. O
120, O
no. O
5, O
pp. O
2421–2424, O
Nov O

. B-DAT
2006. O
[10] O
Naomi O
Harte O
and O
Eoin O

Gillen, B-DAT
“TCD-TIMIT: O
An O
Audio-Visual O
Cor O

- B-DAT
pus O
of O
Continuous O
Speech,” O

IEEE B-DAT
Transactions O
on O
Multimedia, O
vol. O
17, O
no. O
5, O
pp. O
603–615, O
May O

2015 B-DAT

. B-DAT
[11] O
Z. O
Chen, O
Y. O
Luo O

, B-DAT
and O
N. O
Mesgarani, O
“Deep O

attractor B-DAT
network O
for O
single-microphone O
speaker O

separation,” B-DAT
in O
2017 O
IEEE O
International O

Conference B-DAT
on O
Acoustics, O
Speech O
and O

Signal B-DAT
Processing O
(ICASSP), O
March O
2017, O

pp. B-DAT
246–250. O
[12] O
Yusuf O
Isik, O
Jonathan O
Le O

Roux, B-DAT
Zhuo O
Chen, O
Shinji O
Watanabe O

, B-DAT
and O
John O
R. O

Hershey, B-DAT
“Single-channel O
multi-speaker O
separation O
using O

deep B-DAT
clustering,” O
in O
Interspeech, O
2016. O
[13] O
Morten O
Kolbaek, O
Dong O
Yu O

, B-DAT
Zheng-Hua O
Tan, O
Jesper O
Jensen, O

Morten B-DAT
Kolbaek, O
Dong O
Yu, O
Zheng-Hua O

Tan, B-DAT
and O
Jesper O
Jensen, O
“Multitalker O

speech B-DAT
separation O
with O
utterance-level O
permutation O

invariant B-DAT
training O
of O
deep O
recurrent O

neural B-DAT
networks,” O
IEEE/ACM O
Trans. O
Audio, O

Speech B-DAT
and O
Lang. O
Proc., O
vol. O
25, O
no. O
10, O
pp. O
1901–1913, O
Oct O

. B-DAT
2017. O
[14] O
Bertrand O
Rivet, O
Wenwu O
Wang O

, B-DAT
Syed O
Mohsen O
Naqvi, O
and O

Jonathon B-DAT
Chambers, O
“Audiovisual O
Speech O
Source O

Separation: B-DAT
An O
overview O
of O
key O

methodologies,” B-DAT
IEEE O
Signal O
Processing O
Magazine, O

vol. B-DAT
31, O
no. O
3, O
pp. O
125 O

–134, B-DAT
May O
2014. O
[15] O
Aviv O
Gabbay, O
Ariel O
Ephrat O

, B-DAT
Tavi O
Halperin, O
and O
Shmuel O

Peleg, B-DAT
“Seeing O
through O
noise: O
Visually O

driven B-DAT
speaker O
separation O
and O
enhancement,” O

in B-DAT
ICASSP. O
2018, O
pp. O
3051–3055, O

IEEE. B-DAT
[16] O
Ariel O
Ephrat, O
Tavi O
Halperin O

, B-DAT
and O
Shmuel O
Peleg, O
“Improved O

speech B-DAT
reconstruction O
from O
silent O
video,” O

ICCV B-DAT
2017 O
Workshop O
on O
Computer O

Vision B-DAT
for O
Audio-Visual O
Media, O
2017. O
[17] O
Aviv O
Gabbay, O
Asaph O
Shamir O

, B-DAT
and O
Shmuel O
Peleg, O
“Visual O

speech B-DAT
en- O
hancement,” O
in O
Interspeech. O
2018, O
pp. O
1170–1174, O
ISCA O

. B-DAT
[18] O
Jen-Cheng O
Hou, O
Syu-Siang O
Wang O

, B-DAT
Ying-Hui O
Lai, O
Yu O
Tsao, O

Hsiu-Wen B-DAT
Chang, O
and O
Hsin-Min O

Wang, B-DAT
“Audio-Visual O
Speech O
Enhancement O
Us- O

ing B-DAT
Multimodal O
Deep O
Convolutional O
Neural O

Networks,” B-DAT
IEEE O
Trans- O
actions O
on O

Emerging B-DAT
Topics O
in O
Computational O
Intelligence, O

vol. B-DAT
2, O
no. O
2, O
pp. O
117 O

–128, B-DAT
Apr. O
2018. O
[19] O
Jen-Cheng O
Hou, O
Syu-Siang O
Wang O

, B-DAT
Ying-Hui O
Lai, O
Jen-Chun O
Lin, O

Yu B-DAT
Tsao, O
Hsiu-Wen O
Chang, O
and O

Hsin-Min B-DAT
Wang, O
“Audio-visual O
speech O
enhancement O

using B-DAT
deep O
neural O
networks,” O
in O
2016 O
Asia- O
Pacific O
Signal O
and O
Information O

Processing B-DAT
Association O
Annual O
Sum- O
mit O

and B-DAT
Conference O
(APSIPA), O
Jeju, O
South O

Korea, B-DAT
Dec. O
2016, O
pp. O
1–6 O

, B-DAT
IEEE. O
[20] O
Ariel O
Ephrat, O
Inbar O
Mosseri O

, B-DAT
Oran O
Lang, O
Tali O
Dekel, O

Kevin B-DAT
Wilson, O
Avinatan O
Hassidim, O
William O

T. B-DAT
Freeman, O
and O
Michael O

Rubinstein, B-DAT
“Looking O
to O
Listen O
at O

the B-DAT
Cocktail O
Party: O
A O
Speaker-Independent O

Audio-Visual B-DAT
Model O
for O
Speech O
Separation,” O

ACM B-DAT
Transactions O
on O
Graphics, O
vol. O
37, O
no. O
4, O
pp. O
1–11, O
July O

2018, B-DAT
arXiv: O
1804.03619 O

. B-DAT
[21] O
T. O
Afouras, O
J. O
S O

. B-DAT
Chung, O
and O
A. O

Zisserman, B-DAT
“The O
conversation: O
Deep O
audio-visual O

speech B-DAT
enhancement,” O
in O
Interspeech, O
2018. O
[22] O
Andrew O
Owens O
and O
Alexei O

A B-DAT
Efros, O
“Audio-visual O
scene O
analysis O

with B-DAT
self-supervised O
multisensory O
features,” O
European O

Conference B-DAT
on O
Computer O
Vision O
(ECCV O

), B-DAT
2018. O
[23] O
Michael O
C. O
Anzalone, O
Lauren O

Calandruccio, B-DAT
Karen O
A. O
Doherty, O
and O

Laurel B-DAT
H. O
Carney, O
“Determination O
of O

the B-DAT
potential O
benefit O
of O
time O

- B-DAT
frequency O
gain O
manipulation,” O
Ear O

Hear, B-DAT
vol. O
27, O
no. O
5, O

pp. B-DAT
480–492, O
Oct O
2006, O
16957499[pmid]. O
[24] O
Ulrik O
Kjems, O
Jesper O
B O

. B-DAT
Boldt, O
Michael O
S. O
Pedersen, O

Thomas B-DAT
Lunner, O
and O
DeLiang O

Wang, B-DAT
“Role O
of O
mask O
pattern O

in B-DAT
intelligibility O
of O
ideal O
binary-masked O

noisy B-DAT
speech,” O
The O
Journal O
of O

the B-DAT
Acoustical O
Society O
of O
America, O

vol. B-DAT
126, O
no. O
3, O
pp. O
1415 O

–1426, B-DAT
2009. O
[25] O
A. O
Graves, O
A. O
Mohamed O

, B-DAT
and O
G. O
Hinton, O
“Speech O

recognition B-DAT
with O
deep O
recurrent O
neural O

networks,” B-DAT
in O
2013 O
IEEE O
International O

Con- B-DAT
ference O
on O
Acoustics, O
Speech O

and B-DAT
Signal O
Processing, O
May O
2013, O

pp. B-DAT
6645–6649. O
[26] O
Diederik O
P O
Kingma O
and O

Jimmy B-DAT
Ba, O
“Adam: O
A O
method O

for B-DAT
stochastic O
optimization,” O
arXiv O
preprint O

arXiv:1412.6980, B-DAT
2014 O

. B-DAT
[27] O
E. O
Vincent, O
R. O
Gribonval O

, B-DAT
and O
C. O
Fevotte, O
“Performance O

measure- B-DAT
ment O
in O
blind O
audio O

source B-DAT
separation,” O
IEEE O
Transactions O
on O

Audio, B-DAT
Speech O
and O
Language O
Processing, O

vol. B-DAT
14, O
no. O
4, O
pp. O
1462 O

–1469, B-DAT
July O
2006. O
[28] O
Colin O
Raffel, O
Brian O
McFee O

, B-DAT
Eric O
J O
Humphrey, O
Justin O

Salamon, B-DAT
Oriol O
Nieto, O
Dawen O
Liang, O

Daniel B-DAT
PW O
Ellis, O
and O
C O

Colin B-DAT
Raffel, O
“mir O
eval: O
A O

transparent B-DAT
implementation O
of O
common O
mir O

metrics,” B-DAT
in O
In O
Proceed- O
ings O

of B-DAT
the O
15th O
International O
Society O

for B-DAT
Music O
Information O
Retrieval O
Conference, O

ISMIR. B-DAT
Citeseer, O
2014. O
[29] O
A.W. O
Rix, O
J.G. O
Beerends O

, B-DAT
M.P. O
Hollier, O
and O
A.P. O

Hekstra, B-DAT
“Perceptual O
evaluation O
of O
speech O

quality B-DAT
(PESQ)-a O
new O
method O
for O

speech B-DAT
qual- O
ity O
assessment O
of O

telephone B-DAT
networks O
and O
codecs,” O
in O
2001 O
IEEE O
In- O
ternational O
Conference O
on O

Acoustics, B-DAT
Speech, O
and O
Signal O
Processing O

. B-DAT
Proceedings O
(Cat. O
No.01CH37221), O
Salt O

Lake B-DAT
City, O
UT, O
USA, O
2001, O

vol. B-DAT
2, O
pp. O
749–752, O
IEEE. O
[30] O
A. O
Hines, O
J. O
Skoglund O

, B-DAT
A. O
Kokaram, O
and O
N. O

Harte, B-DAT
“ViSQOL: O
The O
Virtual O
Speech O

Quality B-DAT
Objective O
Listener,” O
in O
IWAENC O
2012 O

; B-DAT
Inter- O
national O
Workshop O
on O

Acoustic B-DAT
Signal O
Enhancement, O
Sept. O
2012, O

pp. B-DAT
1 O

4 B-DAT

1 B-DAT
Introduction O

1.1 B-DAT
Related O
work O

2 B-DAT
MODEL O
ARCHITECTURES O

2.1 B-DAT
VL2M O
model O

2.2 B-DAT
VL2M_ref O
model O

2.3 B-DAT
Audio-Visual O
concat O
model O

2.4 B-DAT
Audio-Visual O
concat-ref O
model O

3 B-DAT
Experimental O
setup O

3.1 B-DAT
Dataset O

3.2 B-DAT
LSTM O
training O

3.3 B-DAT
Audio O
pre- O
and O
post-processing O

3.4 B-DAT
Video O
pre-processing O

4 B-DAT
Results O

5 B-DAT
Conclusion O

6 B-DAT
References O

evaluated O
on O
the O
limited O
size O
GRID B-DAT
and O
TCD-TIMIT O
datasets, O
that O
achieve O

trained O
and O
evaluated O
on O
the O
GRID B-DAT
[9] O
and O
TCD-TIMIT O
[10] O
datasets O

were O
carried O
out O
using O
the O
GRID B-DAT
[9] O
and O
TCD-TIMIT O
[10] O
audio-visual O

Regarding O
the O
GRID B-DAT
corpus, O
for O
each O
of O
the O

the O
same O
procedure O
as O
for O
GRID, B-DAT
with O
one O
difference. O
Con- O
trary O

to O
GRID, B-DAT
TCD-TIMIT O
utterances O
have O
different O
dura O

Table O
1. O
GRID B-DAT
results O
- O
speaker-dependent. O
The O
“Noisy O

Table O
2. O
GRID B-DAT
results O
- O
speaker-independent O

We O
upsampled O
from O
25/29.97 O
fps O
(GRID B-DAT

systems O
trained O
and O
tested O
on O
GRID B-DAT
and O
TCD- O
TIMIT O
to O
compare O

of O
speaker-dependent O
models O
on O
the O
GRID B-DAT
corpus O
with O
landmark O
motion O
vectors O

speaker-independent O
test-set O
results O
on O
the O
GRID B-DAT
and O
TCD-TIMIT O
datasets O
respectively. O
V2ML O

evaluated O
on O
the O
limited O
size O
GRID B-DAT
and O
TCD-TIMIT O
datasets O
that O
accomplish O

We O
conduct O
experiments O
on O
the O
WIKIBIO B-DAT
dataset O
which O
contains O
over O
700k O

FBI O
Agent), O
etc. O
We O
utilize O
WIKIBIO B-DAT
dataset O
proposed O
by O
Le- O
bret O

structured O
table. O
(3) O
Experiments O
on O
WIKIBIO B-DAT
dataset O
show O
that O
our O
model O

Table O
1: O
Statistics O
of O
WIKIBIO B-DAT
dataset O

and O
testing O
set O
based O
on O
WIKIBIO B-DAT
by O
randomly O
shuf- O
fling O
the O

We O
conduct O
experiments O
on O
the O
WIKIBIO B-DAT
dataset O
which O
contains O
over O
700k O

FBI O
Agent), O
etc. O
We O
utilize O
WIKIBIO B-DAT
dataset O
proposed O
by O
Le- O
bret O

structured O
table. O
(3) O
Experiments O
on O
WIKIBIO B-DAT
dataset O
show O
that O
our O
model O

Table O
1: O
Statistics O
of O
WIKIBIO B-DAT
dataset O

and O
testing O
set O
based O
on O
WIKIBIO B-DAT
by O
randomly O
shuf- O
fling O
the O

new O
dataset O
for O
text O
generation, O
WIKIBIO, B-DAT
a O
corpus O
of O
728,321 O
articles O

GNE: B-DAT
a O
deep O
learning O
framework O

for B-DAT
gene O
network O
inference O
by O

aggregating B-DAT
biological O

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
https://doi.org/10.1186/s12918-019-0694-y O
RESEARCH O
Open O
Access O

GNE: B-DAT
a O
deep O
learning O
framework O

for B-DAT
gene O
network O
inference O
by O

aggregating B-DAT
biological O
information O
Kishan O
KC1*, O

Rui B-DAT
Li1, O
Feng O
Cui2, O
Qi O

Yu1 B-DAT
and O
Anne O
R. O
Haake1 O
From O
The O
17th O
Asia O
Pacific O

Bioinformatics B-DAT
Conference O
(APBC O
2019) O
Wuhan O

, B-DAT
China. O
14–16 O
January O
2019 O
Abstract O

Background: B-DAT
The O
topological O
landscape O
of O

gene B-DAT
interaction O
networks O
provides O
a O

rich B-DAT
source O
of O
information O
for O

inferring B-DAT
functional O
patterns O
of O
genes O

or B-DAT
proteins. O
However, O
it O
is O

still B-DAT
a O
challenging O
task O
to O

aggregate B-DAT
heterogeneous O
biological O
information O
such O

as B-DAT
gene O
expression O
and O
gene O

interactions B-DAT
to O
achieve O
more O
accurate O

inference B-DAT
for O
prediction O
and O
discovery O

of B-DAT
new O
gene O
interactions. O
In O

particular, B-DAT
how O
to O
generate O
a O

unified B-DAT
vector O
representation O
to O
integrate O

diverse B-DAT
input O
data O
is O
a O

key B-DAT
challenge O
addressed O
here. O
Results: O
We O
propose O
a O
scalable O

and B-DAT
robust O
deep O
learning O
framework O

to B-DAT
learn O
embedded O
representations O
to O

unify B-DAT
known O
gene O
interactions O
and O

gene B-DAT
expression O
for O
gene O
interaction O

predictions. B-DAT
These O
low- O
dimensional O
embeddings O

derive B-DAT
deeper O
insights O
into O
the O

structure B-DAT
of O
rapidly O
accumulating O
and O

diverse B-DAT
gene O
interaction O
networks O
and O

greatly B-DAT
simplify O
downstream O
modeling. O
We O

compare B-DAT
the O
predictive O
power O
of O

our B-DAT
deep O
embeddings O
to O
the O

strong B-DAT
baselines. O
The O
results O
suggest O

that B-DAT
our O
deep O
embeddings O
achieve O

significantly B-DAT
more O
accurate O
predictions. O
Moreover O

, B-DAT
a O
set O
of O
novel O

gene B-DAT
interaction O
predictions O
are O
validated O

by B-DAT
up-to-date O
literature-based O
database O
entries. O
Conclusion: O
The O
proposed O
model O
demonstrates O

the B-DAT
importance O
of O
integrating O
heterogeneous O

information B-DAT
about O
genes O
for O
gene O

network B-DAT
inference. O
GNE O
is O
freely O

available B-DAT
under O
the O
GNU O
General O

Public B-DAT
License O
and O
can O
be O

downloaded B-DAT
from O
GitHub O
(https://github.com/kckishan/GNE O

). B-DAT
Keywords: O
Gene O
interaction O
networks, O
Gene O

expression, B-DAT
Network O
embedding, O
Heterogeneous O
data O

integration, B-DAT
Deep O
learning O

Background B-DAT
A O
comprehensive O
study O
of O

gene B-DAT
interactions O
(GIs) O
provides O
means O

to B-DAT
identify O
the O
functional O
relationship O

between B-DAT
genes O
and O
their O
corresponding O

products, B-DAT
as O
well O
as O
insights O

into B-DAT
underlying O
biological O
phenomena O
that O

are B-DAT
critical O
to O
understanding O
phenotypes O

in B-DAT
health O
and O
dis- O
ease O

conditions B-DAT
[1–3]. O
Since O
advancements O
in O

measure- B-DAT
ment O
technologies O
have O
led O

to B-DAT
numerous O
high-throughput O
datasets, O
there O

is B-DAT
a O
great O
value O
in O

developing B-DAT
efficient O
com- O
putational O
methods O

capable B-DAT
of O
automatically O
extracting O
*Correspondence: O
kk3671@rit.edu O
1Golisano O
College O
of O

Computing B-DAT
and O
Information O
Sciences, O
Rochester O

Institute B-DAT
of O
Technology, O
20 O
Lomb O

Memorial B-DAT
Drive, O
14623 O
Rochester, O
New O

York, B-DAT
USA O
Full O
list O
of O

author B-DAT
information O
is O
available O
at O

the B-DAT
end O
of O
the O
article O

and B-DAT
aggregating O
meaningful O
information O
from O

heteroge- B-DAT
neous O
datasets O
to O
infer O

gene B-DAT
interactions. O
Although O
a O
wide O
variety O
of O

machine B-DAT
learning O
models O
have O
been O

developed B-DAT
to O
analyze O
high-throughput O
datasets O

for B-DAT
GI O
prediction O
[4], O
there O

are B-DAT
still O
some O
major O
chal O

- B-DAT
lenges, O
such O
as O
efficient O

analysis B-DAT
of O
large O
heterogeneous O
datasets, O

integration B-DAT
of O
biological O
information, O
and O

effec- B-DAT
tive O
feature O
engineering. O
To O

address B-DAT
these O
challenges, O
we O
propose O

a B-DAT
novel O
deep O
learning O
framework O

to B-DAT
integrate O
diverse O
biological O
information O

for B-DAT
GI O
network O
inference. O
Our O
proposed O
method O
frames O
GI O

network B-DAT
inference O
as O
a O
problem O

of B-DAT
network O
embedding. O
In O
particular O

, B-DAT
we O
rep- O
resent O
gene O

interactions B-DAT
as O
a O
network O
of O

genes B-DAT
and O
their O
interactions O
and O

create B-DAT
a O
deep O
learning O
framework O

to B-DAT
automatically O
learn O
an O
informative O

representation B-DAT
which O
© O
The O
Author(s). O
2019 O
Open O

Access B-DAT
This O
article O
is O
distributed O

under B-DAT
the O
terms O
of O
the O

Creative B-DAT
Commons O
Attribution O
4.0 O
International O

License B-DAT
(http://creativecommons.org/licenses/by/4.0/), O
which O
permits O
unrestricted O

use, B-DAT
distribution, O
and O
reproduction O
in O

any B-DAT
medium, O
provided O
you O
give O

appropriate B-DAT
credit O
to O
the O
original O

author(s) B-DAT
and O
the O
source, O
provide O

a B-DAT
link O
to O
the O
Creative O

Commons B-DAT
license, O
and O
indicate O
if O

changes B-DAT
were O
made. O
The O
Creative O

Commons B-DAT
Public O
Domain O
Dedication O
waiver O

(http://creativecommons.org/publicdomain/zero/1.0/) B-DAT
applies O
to O
the O
data O

made B-DAT
available O
in O
this O
article O

, B-DAT
unless O
otherwise O
stated. O
http://crossmark.crossref.org/dialog/?doi=10.1186/s12918-019-0694-y&domain=pdf O
https://github.com/kckishan/GNE O
mailto: O
kk3671@rit.edu O
http://creativecommons.org/licenses/by/4.0 O

/ B-DAT

1 B-DAT

0 B-DAT

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
2 O
of O
14 O

integrates B-DAT
both O
the O
topological O
property O

and B-DAT
the O
gene O
expression O
property. O

A B-DAT
key O
insight O
behind O
our O

gene B-DAT
net- O
work O
embedding O
method O

is B-DAT
the O
“guilt O
by O
association” O

assumption B-DAT
[5], O
that O
is, O
genes O

that B-DAT
are O
co-localized O
or O
have O

similar B-DAT
topological O
roles O
in O
the O

interaction B-DAT
net- O
work O
are O
likely O

to B-DAT
be O
functionally O
correlated. O
This O

insight B-DAT
not O
only O
allows O
us O

to B-DAT
discover O
similar O
genes O
and O

pro- B-DAT
teins O
but O
also O
to O

infer B-DAT
the O
properties O
of O
unknown O

ones. B-DAT
Our O
network O
embedding O
generates O

a B-DAT
lower-dimensional O
vector O
representation O
of O

the B-DAT
gene O
topological O
character- O
istics. O

The B-DAT
relationships O
between O
genes O
including O

higher- B-DAT
order O
topological O
properties O
are O

captured B-DAT
by O
the O
distances O
between O

genes B-DAT
in O
the O
embedding O
space. O

The B-DAT
new O
low- O
dimensional O
representation O

of B-DAT
a O
GI O
network O
can O

be B-DAT
used O
for O
various O
downstream O

tasks, B-DAT
such O
as O
gene O
function O

pre- B-DAT
diction, O
gene O
interaction O
prediction, O

and B-DAT
gene O
ontology O
reconstruction O
[6]. O
Furthermore, O
since O
the O
network O
embedding O

method B-DAT
can O
only O
preserve O
the O

topological B-DAT
properties O
of O
a O
GI O

network, B-DAT
and O
fails O
to O
generalize O

for B-DAT
genes O
with O
no O
interaction O

infor- B-DAT
mation, O
our O
scalable O
deep O

learning B-DAT
method O
also O
integrates O
heterogeneous O

gene B-DAT
information, O
such O
as O
expression O

data B-DAT
from O
high O
throughput O
technologies O

, B-DAT
into O
the O
GI O
net- O

work B-DAT
inference. O
Our O
method O
projects O

genes B-DAT
with O
similar O
attributes O
closer O

to B-DAT
each O
other O
in O
the O

embedding B-DAT
space, O
even O
if O
they O

may B-DAT
not O
have O
similar O
topological O

properties. B-DAT
The O
results O
show O
that O

by B-DAT
integrating O
additional O
gene O
infor- O

mation B-DAT
in O
the O
network O
embedding O

process, B-DAT
the O
prediction O
performance O
is O

improved B-DAT
significantly. O
GI O
prediction O
is O
a O
long-standing O

problem. B-DAT
The O
pro- O
posed O
machine O

learning B-DAT
methods O
include O
statistical O
corre O

- B-DAT
lation, O
mutual O
information O
[7], O

dimensionality B-DAT
reduction O
[8], O
and O
network-based O

methods B-DAT
(e.g. O
common O
neighbor- O
hood, O

network B-DAT
embedding) O
[4, O
9]. O
Among O

these B-DAT
methods, O
some O
methods O
such O

as B-DAT
statistical O
correlation O
and O
mutual O

information B-DAT
consider O
only O
gene O
expression O

whereas B-DAT
other O
methods O
use O
only O

topological B-DAT
properties O
to O
predict O
GIs. O
Network-based O
methods O
have O
been O
proposed O

to B-DAT
leverage O
topological O
properties O
of O

GI B-DAT
networks O
[10]. O
Neighborhood-based O
methods O

quantify B-DAT
the O
proximity O
between O
genes O

, B-DAT
based O
on O
common O
neighbors O

in B-DAT
GI O
network O
[11]. O
The O

proximity B-DAT
scores O
assigned O
to O
a O

pair B-DAT
of O
genes O
rely O
on O

the B-DAT
number O
of O
neighbors O
that O

the B-DAT
pair O
has O
in O
common. O

Adjacency B-DAT
matrix, O
representing O
the O
interaction O

network, B-DAT
or O
proximity O
matrix, O
obtained O

from B-DAT
neighborhood-based O
methods, O
are O
processed O

with B-DAT
network O
embedding O
methods O
to O

learn B-DAT
embeddings O
that O
preserve O
the O

structural B-DAT
properties O
of O
the O
network. O

Structure-preserving B-DAT
network O
embedding O
methods O
such O

as B-DAT
Isomap O
[12] O
are O
proposed O

as B-DAT
a O
dimensionality O
reduc- O
tion O

technique. B-DAT
Since O
the O
goal O
of O

these B-DAT
methods O
is O
solely O
for O

graph B-DAT
reconstruction, O
the O
embedding O
space O

may B-DAT
not O
be O
suitable O
for O

GI B-DAT
network O
inference. O
Besides, O
these O

meth- B-DAT
ods O
construct O
the O
graphs O

from B-DAT
the O
data O
features O
where O
proximity O
between O
genes O
is O
well O

defined B-DAT
in O
the O
original O
feature O

space B-DAT
[9]. O
On O
the O
other O

hand, B-DAT
in O
GI O
networks, O
gene O

proximities B-DAT
are O
not O
explicitly O
defined O

, B-DAT
and O
they O
depend O
on O

the B-DAT
specific O
analytic O
tasks O
and O

application B-DAT
scenarios. O
Our O
deep O
learning O
method O
allows O

incorporating B-DAT
gene O
expression O
data O
with O

GI B-DAT
network O
topological O
structure O
information O

to B-DAT
preserve O
both O
topological O
and O

attribute B-DAT
proximity O
in O
the O
low-dimensional O

representation B-DAT
for O
GI O
predictions. O
Moreover O

, B-DAT
the O
scalable O
architecture O
enables O

us B-DAT
to O
incorporate O
additional O
attributes. O

Topological B-DAT
prop- O
erties O
of O
GI O

network B-DAT
and O
expression O
profiles O
are O

trans- B-DAT
formed O
into O
two O
separate O

embeddings: B-DAT
ID O
embedding O
(which O
preserves O

the B-DAT
topological O
structure O
proximity) O
and O

attribute B-DAT
embedding O
(which O
preserves O
the O

attribute B-DAT
prox- O
imity) O
respectively. O
With O

a B-DAT
multilayer O
neural O
network, O
we O

then B-DAT
aggregate O
the O
complex O
statistical O

relationships B-DAT
between O
topology O
and O
attribute O

information B-DAT
to O
improve O
GI O
predictions. O
In O
summary, O
our O
contributions O
are O

as B-DAT
follows O

: B-DAT
• O
We O
propose O
a O
novel O

deep B-DAT
learning O
framework O
to O
learn O

lower B-DAT
dimensional O
representations O
while O
preserving O

topological B-DAT
and O
attribute O
proximity O
of O

GI B-DAT
networks O

. B-DAT
• O
We O
evaluate O
the O
prediction O

performance B-DAT
on O
the O
datasets O
of O

two B-DAT
organisms O
based O
on O
the O

embedded B-DAT
representation O
and O
achieve O
significantly O

better B-DAT
predictions O
than O
the O
strong O

baselines B-DAT

. B-DAT
• O
Our O
method O
can O
predict O

new B-DAT
gene O
interactions O
which O
are O

validated B-DAT
on O
an O
up-to-date O
GI O

database B-DAT

. B-DAT
Methods O
Preliminaries O
We O
formally O
define O

the B-DAT
problem O
of O
gene O
network O

infer- B-DAT
ence O
as O
a O
network O

embedding B-DAT
problem O
using O
the O
concepts O

of B-DAT
topological O
and O
attribute O
proximity O

as B-DAT
demonstrated O
in O
Fig. O
1 O

. B-DAT
Definition O
1 O
(Gene O
network) O
Gene O

network B-DAT
can O
be O
rep- O
resented O

as B-DAT
a O
network O
structure, O
which O

represents B-DAT
the O
inter- O
actions O
between O

genes B-DAT
within O
an O
organism. O
The O

interaction B-DAT
between O
genes O
corresponds O
to O

either B-DAT
a O
physical O
interaction O
through O

their B-DAT
gene O
products, O
e.g., O
proteins O

, B-DAT
or O
one O
of O
the O

genes B-DAT
alters O
or O
affects O
the O

activity B-DAT
of O
other O
gene O
of O

interest. B-DAT
We O
denote O
gene O
network O

as B-DAT
G O
= O
(V O
, O

E, B-DAT
A) O
where O

V B-DAT
= O
{vi} O
denotes O
genes O

or B-DAT
proteins, O
E O
= O
{eij} O

denotes B-DAT
edges O
that O
correspond O
to O

interactions B-DAT
between O
genes O
vi O
and O

vj, B-DAT
and O
A O
= O
{Ai} O

represents B-DAT
the O
attributes O
of O
gene O

vi. B-DAT
Edge O
eij O
is O
associated O

with B-DAT
a O
weight O
wij O
≥ O
0 O
indicating O
the O
strength O
of O
the O

connection B-DAT
between O
gene O
vi O
and O

vj. B-DAT
If O
gene O
vi O
and O

vj B-DAT
is O
not O
linked O
by O

an B-DAT
edge, O
wij O
= O
0 O

. B-DAT
We O
name O
interactions O
with O

wij B-DAT
> O
0 O
as O
positive O

interactions B-DAT
and O
wij O
= O
0 O

as B-DAT
negative O
interactions. O
In O
this O

paper, B-DAT
we O
consider O
weights O
wij O

to B-DAT
be O
binary, O
indicating O
whether O

genes B-DAT
vi O
and O
vj O
interact O

or B-DAT

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
3 O
of O
14 O

Fig. B-DAT
1 O
An O
illustration O
of O

gene B-DAT
network O
embedding O
(GNE). O
GNE O

integrates B-DAT
gene O
interaction O
network O
and O

gene B-DAT
expression O
data O
to O
learn O

a B-DAT
lower-dimensional O
representation.The O
nodes O
represent O

genes, B-DAT
and O
the O
genes O
with O

the B-DAT
same O
color O
have O
similar O

expression B-DAT
profiles. O
GNE O
groups O
genes O

with B-DAT
similar O
network O
topology, O
which O

are B-DAT
connected O
or O
have O
a O

similar B-DAT
neighborhood O
in O
the O
graph, O

and B-DAT
attribute O
similarity O
(similar O
expression O

profiles) B-DAT
in O
the O
embedded O
space O
Genes O
directly O
connected O
with O
a O

gene B-DAT
vi O
in O
gene O
network O

denote B-DAT
the O
local O
network O
structure O

of B-DAT
gene O
vi. O
We O
define O

local B-DAT
network O
structures O
as O
the O

first-order B-DAT
proximity O
of O
a O
gene O

. B-DAT
Definition O
2 O
(First-order O
proximity) O
The O

first-order B-DAT
proximity O
in O
a O
gene O

network B-DAT
is O
the O
pairwise O
interactions O

between B-DAT
genes. O
Weight O
wij O
indicates O

the B-DAT
first-order O
proxim- O
ity O
between O

gene B-DAT
vi O
and O
vj. O
If O

there B-DAT
is O
no O
interaction O
between O

gene B-DAT
vi O
and O
vj, O
their O

first-order B-DAT
proximity O
wij O
is O
0 O

. B-DAT
Genes O
are O
likely O
to O
be O

involved B-DAT
in O
the O
same O
cellular O

func- B-DAT
tions O
if O
they O
are O

connected B-DAT
in O
the O
gene O
network O

. B-DAT
On O
the O
other O
hand, O

even B-DAT
if O
two O
genes O
are O

not B-DAT
connected, O
they O
may O
be O

still B-DAT
related O
in O
some O
cellular O

functions. B-DAT
This O
indicates O
the O
need O

for B-DAT
an O
additional O
notion O
of O

proximity B-DAT
to O
preserve O
the O
network O

structure. B-DAT
Studies O
suggest O
that O
genes O

that B-DAT
share O
a O
similar O
neighborhood O

are B-DAT
also O
likely O
to O
be O

related B-DAT
[6]. O
Thus, O
we O
introduce O

second-order B-DAT
proximity O
that O
characterizes O
the O

global B-DAT
network O
structure O
of O
the O

genes. B-DAT
Definition O
3 O
(Second-order O
proximity) O
Second O

order B-DAT
proximity O
denotes O
the O
similarity O

between B-DAT
the O
neighbor- O
hood O
of O

genes. B-DAT
Let O
Ni O
= O
{si,1 O

,. B-DAT
. O
. O
, O
si,i−1, O

si,i+1,. B-DAT
. O
. O
, O
si,M−1} O

denotes B-DAT
the O
first-order O
proximity O
of O

gene B-DAT
vi, O
where O
si,j O
is O

wij B-DAT
if O
there O
is O
direct O

connection B-DAT
between O
gene O
vi O
and O

gene B-DAT
vj, O
otherwise O
0. O
Then, O

the B-DAT
second O
order O
proximity O
is O

the B-DAT
sim- O
ilarity O
between O
Ni O

and B-DAT
Nj. O
If O
there O
is O

no B-DAT
path O
to O
reach O
gene O

vi B-DAT
from O
gene O
vj, O
the O

second B-DAT
proximity O
between O
these O
genes O

is B-DAT
0. O
Integrating O
first-order O
and O
second-order O
proximities O

simultaneously B-DAT
can O
help O
to O
preserve O

the B-DAT
topological O
proper- O
ties O
of O

the B-DAT
gene O
network. O
To O
generate O

a B-DAT
more O
comprehensive O
representation O
of O

the B-DAT
genes, O
it O
is O
crucial O

to B-DAT
integrate O
gene O
expression O
data O

as B-DAT
gene O
attributes O
with O
their O

topological B-DAT
properties. O
Besides O
preserving O
topological O

properties, B-DAT
gene O
expression O
provides O
additional O

information B-DAT
to O
predict O
the O
network O

structure B-DAT

. B-DAT
Definition O
4 O
(Attribute O
proximity) O
Attribute O

proxim- B-DAT
ity O
denotes O
the O
similarity O

between B-DAT
the O
expression O
of O
genes O

. B-DAT
We O
thus O
investigate O
both O
topological O

and B-DAT
attribute O
prox- O
imity O
for O

gene B-DAT
network O
embedding, O
which O
is O

defined B-DAT
as O
follows O

: B-DAT
Definition O
5 O
(Gene O
network O
embedding O

) B-DAT
Given O
a O
gene O
network O

denoted B-DAT
as O
G O
= O
(V O
, O
E, O
A), O
gene O
network O
embedding O

aims B-DAT
to O
learn O
a O
function O

f B-DAT
that O
maps O
gene O
network O

structure B-DAT
and O
their O
attribute O
information O

to B-DAT
a O
d- O
dimensional O
space O

where B-DAT
a O
gene O
is O
represented O

by B-DAT
a O
vector O
yi O

∈ B-DAT
Rd O
where O
d O

� B-DAT
M. O
The O
low O
dimensional O

vectors B-DAT
yi O
and O
yj O
for O

genes B-DAT
vi O
and O
vj O
preserve O

their B-DAT
relationships O
in O
terms O
of O

the B-DAT
network O
topological O
structure O
and O

attribute B-DAT
proximity O

. B-DAT
Gene O
network O
embedding O
(GNE) O
model O

Our B-DAT
deep O
learning O
framework O
as O

shown B-DAT
in O
Fig. O
2 O
jointly O

utilizes B-DAT
gene O
network O
structure O
and O

gene B-DAT
expression O
data O
to O
learn O

a B-DAT
unified O
representation O
for O
the O

genes. B-DAT
Embedding O
of O
a O
gene O

network B-DAT
projects O
genes O
into O
a O

lower B-DAT
dimensional O
space, O
known O
as O

the B-DAT
embedding O
space, O
in O
which O

each B-DAT
gene O
is O
represented O
by O

a B-DAT
vector. O
The O
embeddings O
preserve O

both B-DAT
the O
gene O
network O
structure O

and B-DAT
statistical O
relationships O
of O
gene O

expression. B-DAT
We O
list O
the O
variables O

to B-DAT
specify O
our O
framework O
in O

Table B-DAT
1 O

. B-DAT
Gene O
network O
structure O
modeling O
GNE O

framework B-DAT
preserves O
first-order O
and O
second-order O

proximity B-DAT
of O
genes O
in O
the O

gene B-DAT
network. O
The O
key O
idea O

of B-DAT
network O
structure O
modeling O
is O

to B-DAT
estimate O
the O
pairwise O
proximity O

of B-DAT
genes O
in O
terms O
of O

the B-DAT
network O
structure. O
If O
two O

genes B-DAT
are O
connected O
or O
share O

similar B-DAT
neighborhood O
genes, O
they O
tend O

to B-DAT
be O
related O
and O
should O

be B-DAT
placed O
closer O
to O
each O

other B-DAT
in O
the O
embedding O
space O

. B-DAT
Inspired O
by O
the O
Skip-gram O

model B-DAT
[13], O
we O
use O
one O

hot B-DAT
encoded O
represen- O
tation O
to O

represent B-DAT
topological O
information O
of O
a O

gene. B-DAT

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
4 O
of O
14 O

Fig. B-DAT
2 O
Overview O
of O
Gene O

Network B-DAT
Embedding O
(GNE) O
Framework O
for O

gene B-DAT
interaction O
prediction. O
On O
the O

left,one-hot B-DAT
encoded O
representation O
of O
gene O

is B-DAT
encoded O
to O
dense O
vector O

v(s)i B-DAT
of O
dimension O
d O
× O
1 O
which O
captures O
topological O
properties O
and O

expression B-DAT
vector O
of O
gene O
is O

transformed B-DAT
to O
v(a)i O
of O
dimension O

d B-DAT
× O
1 O
which O
aggregates O

the B-DAT
attribute O
information O
(Step O
1 O

). B-DAT
Next, O
concatenation O
of O
two O

embedded B-DAT
vectors O
(creates O
vector O
with O

dimension B-DAT
2d O
× O
1) O
allows O

to B-DAT
combine O
strength O
of O
both O

network B-DAT
structure O
and O
attribute O
modeling. O

Then, B-DAT
nonlinear O
transformation O
of O
concatenated O

vector B-DAT
enables O
GNE O
to O
capture O

complex B-DAT
statistical O
relationships O
between O
network O

structure B-DAT
and O
attribute O
information O
and O

learn B-DAT
better O
representations O
(Step O
2). O

Finally, B-DAT
these O
learned O
representation O
of O

dimension B-DAT
d O
× O
1 O
is O

transformed B-DAT
into O
a O
probability O
vector O

of B-DAT
length O
M O
× O
1 O

in B-DAT
output O
layer, O
which O
contains O

the B-DAT
predictive O
probability O
of O
gene O

vi B-DAT
to O
all O
the O
genes O

in B-DAT
the O
network. O
Conditional O
probability O

p(vj|vi) B-DAT
on O
output O
layer O
indicates O

the B-DAT
likelihood O
that O
gene O
vj O

is B-DAT
connected O
with O
gene O
vi O
( O

Step B-DAT
3) O
gene O
vi O
in O
the O
network O

is B-DAT
represented O
as O
an O
M-dimensional O

vector B-DAT
where O
only O
the O
ith O

component B-DAT
of O
the O
vector O
is O

1 B-DAT

. B-DAT
To O
model O
topological O
similarity, O
we O

define B-DAT
the O
condi- O
tional O
probability O

of B-DAT
gene O
vj O
on O
gene O

vi B-DAT
using O
a O
softmax O
function O

as B-DAT

: B-DAT
p(vj|vi) O
= O
exp(f O
(vi, O
vj))∑M O

j′=1 B-DAT
exp(f O
(vi, O
vj O

′)) B-DAT
(1 O

) B-DAT
Table O
1 O
Terms O
and O
Notations O

Symbol B-DAT
Definitions O
M O
Total O
number O
of O
genes O

in B-DAT
gene O
network O

E B-DAT
Number O
of O
expression O
values O

for B-DAT
each O
gene O
Ni O
Set O
of O
the O
neighbor O

genes B-DAT
of O
gene O
vi O
v(s)i O

Topological B-DAT
representation O
of O
gene O
vi O

v(a)i B-DAT
Attribute O
representation O
of O
gene O

vi B-DAT
ṽi O
Neighborhood O
representation O
of O

gene B-DAT
vi O
vi O
Concatenated O
representation O

of B-DAT
topological O

properties B-DAT
and O
expression O
data O
k O
Number O
of O
hidden O
layers O

to B-DAT
transform O
concatenated O
representation O
into O

embedding B-DAT
space O

h(k) B-DAT
Output O
of O
kth O
hidden O

layer B-DAT
Wk O
Weight O
matrix O
for O
kth O

hidden B-DAT
layer O

Wid B-DAT
Weight O
matrix O
for O
topological O

structure B-DAT
embedding O
Watt O
Weight O
matrix O
for O
attribute O

embedding B-DAT

Wout B-DAT
Weight O
matrix O
for O
output O

layer B-DAT
which O
measures O
the O
likelihood O
of O

gene B-DAT
vi O
being O
connected O
with O

vj. B-DAT
Let O
function O
f O
represents O

the B-DAT
mapping O
of O
two O
genes O

vi B-DAT
and O
vj O
to O
their O

estimated B-DAT
proximity O
score. O
Let O
p(N O

|v) B-DAT
be O
the O
likelihood O
of O

observing B-DAT
a O
neighborhood O
N O
for O

a B-DAT
gene O
v. O
By O
assuming O

conditional B-DAT
independence, O
we O
can O
factorize O

the B-DAT
likelihood O
so O
that O
the O

likelihood B-DAT
of O
observing O
a O
neighborhood O

gene B-DAT
is O
independent O
of O
observ O

- B-DAT
ing O
any O
other O
neighborhood O

gene, B-DAT
given O
a O
gene O
vi: O
p(Ni|vi O

) B-DAT
= O
∏ O
vj∈Ni O
p(vj|vi) O
(2 O

) B-DAT
where O
Ni O
represents O
the O
neighborhood O

genes B-DAT
of O
the O
gene O
vi O

. B-DAT
Global O
structure O
proximity O
for O

a B-DAT
gene O
vi O
can O
be O

pre- B-DAT
served O
by O
maximizing O
the O

conditional B-DAT
probability O
over O
all O
genes O

in B-DAT
the O
neighborhood. O
Hence, O
we O

can B-DAT
define O
the O
likelihood O
function O

that B-DAT
preserve O
global O
structure O
proximity O

as: B-DAT
L O
= O
M O

∏ B-DAT
i=1 O
p(Ni|vi O

) B-DAT
= O
M O

∏ B-DAT
i=1 O

vj∈Ni B-DAT
p(vj|vi) O
(3) O
Let O
v(s)i O
denotes O
the O
dense O

vector B-DAT
generated O
from O
one-hot O
gene O

ID B-DAT
vector, O
which O
represents O
topological O

informa- B-DAT
tion O
of O
that O
gene O

. B-DAT
GNE O
follows O
direct O
encoding O

methods B-DAT
[13, O
14] O
to O
map O

genes B-DAT
to O
vector O
embeddings, O
simply O

known B-DAT
as O
embedding O

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
5 O
of O
14 O

v(s)i B-DAT
= O
Widvi O
(4) O
where O

Wid B-DAT
∈ O
Rd×M O
is O
a O

matrix B-DAT
containing O
the O
embedding O
vectors O

for B-DAT
all O
genes O
and O

vi B-DAT
∈ O
IM O
is O
a O

one-hot B-DAT
indica- O
tor O
vector O
indicating O

the B-DAT
column O
of O
Wid O
corresponding O

to B-DAT
gene O
vi. O
Gene O
expression O
modeling O
GNE O
encodes O

the B-DAT
expression O
data O
from O
microarray O

exper- B-DAT
iments O
to O
the O
dense O

representation B-DAT
using O
a O
non-linear O
transformation O

. B-DAT
The O
amount O
of O
mRNA O

produced B-DAT
during O
transcription O
measured O
over O

a B-DAT
number O
of O
experiments O
helps O

to B-DAT
identify O
similarly O
expressed O
genes. O

Since B-DAT
expres- O
sion O
data O
have O

inherent B-DAT
noise O
[15], O
transforming O
expres- O

sion B-DAT
data O
using O
a O
non-linear O

transformation B-DAT
can O
be O
helpful O
to O

uncover B-DAT
the O
underlying O
representation. O
Let O

xi B-DAT
be O
the O
vector O
of O

expression B-DAT
values O
of O
gene O
vi O

measured B-DAT
over O
E O
experiments. O
Using O

non-linear B-DAT
transformation, O
we O
can O
capture O

the B-DAT
non-linearities O
of O
expression O
data O

of B-DAT
gene O
vi O
as: O
v(a)i O
= O
δa(Watt O
· O
xi O

) B-DAT
(5) O
where O
v(a)i O
represents O

the B-DAT
lower O
dimensional O
attribute O
rep- O

resentation B-DAT
vector O
for O
gene O
vi. O

Watt B-DAT
, O
and O
δa O
represents O

the B-DAT
weight O
matrix, O
and O
activation O

function B-DAT
of O
attribute O
transformation O
layer O

respectively. B-DAT
We O
use O
the O
deep O
model O

to B-DAT
approximate O
the O
attribute O
proximity O

by B-DAT
capturing O
complex O
statistical O
relationships O

between B-DAT
attributes O
and O
introducing O
non-linearities O

, B-DAT
simi- O
lar O
to O
structural O

embedding. B-DAT
GNE O
integration O
GNE O
models O
the O

integration B-DAT
of O
network O
structure O
and O

attribute B-DAT
information O
to O
learn O
more O

comprehensive B-DAT
embeddings O
for O
gene O
networks O

. B-DAT
GNE O
takes O
two O
inputs: O

one B-DAT
for O
topological O
information O
of O

a B-DAT
gene O
as O
one O
hot O

gene B-DAT
ID O
vector O
and O
another O

for B-DAT
its O
expression O
as O
an O

attribute B-DAT
vector. O
Each O
input O
is O

encoded B-DAT
to O
its O
respective O
embed- O

dings. B-DAT
One O
hot O
representation O
for O

a B-DAT
gene O
vi O
is O
projected O

to B-DAT
the O
dense O
vector O
v(s)i O

which B-DAT
captures O
the O
topological O
properties. O

Non-linear B-DAT
transformation O
of O
attribute O
vec- O

tor B-DAT
generates O
compact O
representation O
vector O

v(a)i B-DAT
. O
Previous O
work O
[16] O

combines B-DAT
heterogeneous O
information O
using O
the O

late B-DAT
fusion O
approach. O
However, O
the O

late B-DAT
fusion O
approach O
is O
the O

approach B-DAT
of O
learning O
separate O
models O

for B-DAT
hetero- O
geneous O
information O
and O

integrating B-DAT
the O
representations O
learned O
from O

separate B-DAT
models. O
On O
the O
other O

hand, B-DAT
the O
early O
fusion O
combines O

heterogeneous B-DAT
information O
and O
train O
the O

model B-DAT
on O
combined O
representations O
[17]. O

We B-DAT
thus O
propose O
to O
use O

the B-DAT
early O
fusion O
approach O
to O

combine B-DAT
them O
by O
concatenating. O
As O

a B-DAT
result, O
learning O
from O
topo- O

logical B-DAT
and O
attribute O
information O
can O

complement B-DAT
each O
other, O
allowing O
the O

model B-DAT
to O
learn O
their O
complex O

statistical B-DAT
relationships O
as O
well. O
Embeddings O

from B-DAT
topological O
and O
attribute O
information O

are B-DAT
concatenated O
into O
a O
vector O

as: B-DAT
vi O

v(s)i B-DAT
λv O
(a) O
i O
] O
(6 O

) B-DAT
where O
λ O
is O
the O
importance O

of B-DAT
gene O
expression O
information O
relative O

to B-DAT
topological O
information O

. B-DAT
The O
concatenated O
vectors O
are O
fed O

into B-DAT
a O
multilayer O
perceptron O
with O

k B-DAT
hidden O
layers. O
The O
hidden O

represen- B-DAT
tations O
from O
each O
hidden O

layer B-DAT
in O
GNE O
are O
denoted O

as B-DAT
h(0)i O
, O
h O

1) B-DAT
i O
, O
....., O
h O
(k) O
i O
, O
which O
can O

be B-DAT
defined O
as O

h(0)i B-DAT
= O
δ O
( O
W0vi O
+ O
b(0 O

h(k)i B-DAT
= O
δk O
( O
Wkh(k−1)i O
+ O
b(k) O
) O
(7 O

) B-DAT
where O
δk O
represents O
the O
activation O

function B-DAT
of O
layer O
k. O
h(0)i O

represents B-DAT
initial O
representation O
and O
h(k)i O

represents B-DAT
final O
representation O
of O
the O

input B-DAT
gene O
vi. O
Transformation O
of O

input B-DAT
data O
using O
multiple O
non-linear O

layers B-DAT
has O
shown O
to O
improve O

the B-DAT
representation O
of O
input O
data O

[18]. B-DAT
Moreover, O
stacking O
multiple O
layers O

of B-DAT
non-linear O
transformations O
can O
help O

to B-DAT
learn O
high-order O
statistical O
relationships O

between B-DAT
topological O
properties O
and O
attributes O

. B-DAT
At O
last, O
final O
representation O
h(k)i O

of B-DAT
a O
gene O
vi O
from O

the B-DAT
last O
hidden O
layer O
is O

transformed B-DAT
to O
probability O
vector, O
which O

contains B-DAT
the O
conditional O
probability O
of O

all B-DAT
other O
genes O
to O
vi O

: B-DAT
oi O
= O
[ O
p(v1|vi), O
p(v2|vi O

), B-DAT
. O
. O
. O
, O

p(vM|vi) B-DAT
] O
(8 O

) B-DAT
where O
p(vj|vi) O
represents O
the O
probability O

of B-DAT
gene O
vi O
being O
related O

to B-DAT
gene O
vj O
and O
oi O

represents B-DAT
the O
output O
probabil- O
ity O

vector B-DAT
with O
the O
conditional O
probability O

of B-DAT
gene O
vi O
being O
connected O

to B-DAT
all O
other O
genes O

. B-DAT
Weight O
matrix O
Wout O
between O
the O

last B-DAT
hidden O
layer O
and O
the O

output B-DAT
layer O
corresponds O
to O
the O

abstractive B-DAT
represen- O
tation O
of O
neighborhood O

of B-DAT
genes. O
A O
jth O
row O

from B-DAT
Wout O
refers O
to O
the O

compact B-DAT
representation O
of O
neighborhood O
of O

gene B-DAT
vj, O
which O
can O
be O

denoted B-DAT
as O
ṽj. O
The O
proximity O

score B-DAT
between O
gene O
vi O
and O

vj B-DAT
can O
be O
defined O
as O

: B-DAT
f O
(vi, O
vj) O
= O
ṽj O

· B-DAT
h(k)i O
(9) O
which O
can O

be B-DAT
replaced O
into O
Eq. O
1 O

to B-DAT
calculate O
the O
condi- O
tional O

probability B-DAT

: B-DAT
p(vj|vi) O
= O
exp O

ṽj B-DAT
· O

( B-DAT

) B-DAT

M B-DAT
j′=1 O
exp O
( O
ṽj′ O
· O
h(k)i O

10) B-DAT
Our O
model O
learns O
two O
latent O

representations B-DAT
h(k)i O
and O
ṽi O
for O

a B-DAT
gene O
vi O
where O
h(k)i O

is B-DAT
the O
representation O
of O
gene O

as B-DAT
a O
node O
and O
ṽi O

is B-DAT
the O
representation O
of O
the O

gene B-DAT
vi O
as O
a O
neighbor O

. B-DAT
Neighborhood O
representation O
ṽi O
can O

be B-DAT
com- O
bined O
with O
node O

representation B-DAT
h(k)i O
by O
addition O
[19, O
20 O

] B-DAT
to O
get O
final O
representation O

for B-DAT
a O
gene O
as: O
yi O
= O
h(k)i O
+ O
ṽi O

(11) B-DAT
which O
returns O
us O
better O

performance B-DAT
results O

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
6 O
of O
14 O

For B-DAT
an O
edge O
connecting O
gene O

vi B-DAT
and O
vj, O
we O
create O

fea- B-DAT
ture O
vector O
by O
combining O

embeddings B-DAT
of O
those O
genes O
using O

Hadamard B-DAT
product. O
Empirical O
evaluation O
shows O

features B-DAT
created O
with O
Hadamard O
product O

gives B-DAT
better O
performance O
over O

concatenation B-DAT
[14]. O
Then, O
we O
train O

a B-DAT
logistic O
classi- O
fier O
on O

these B-DAT
features O
to O
classify O
whether O

genes B-DAT
vi O
and O
vj O
interact O

or B-DAT
not. O
Parameter O
optimization O
To O
optimize O
GNE O

, B-DAT
the O
goal O
is O
to O

maximize B-DAT
objective O
func- O
tion O
mentioned O

in B-DAT
Eq. O
10 O
as O
a O

function B-DAT
of O
all O
parame- O
ters. O

Let B-DAT
� O
be O
the O
parameters O

of B-DAT
GNE O
which O
includes O
{Wid, O

Watt B-DAT
, O
Wout O
, O
�h} O

and B-DAT
�h O
represents O
weight O
matrices O

Wk B-DAT
of O
hidden O
layers. O
We O

train B-DAT
our O
model O
to O
maximize O

the B-DAT
objective O
function O
with O
respect O

to B-DAT
all O
parameters O
� O
: O
argmax O

M∑ B-DAT
i=1 O

vj B-DAT
∈ O
Ni O
log O
exp O

ṽj B-DAT
· O
h(k)i O
) O
∑M O
j′=1 O
exp O

ṽj′ B-DAT
· O

( B-DAT

) B-DAT

12) B-DAT
Maximizing O
this O
objective O
function O
with O

respect B-DAT
to O
� O
is O
computationally O

expensive, B-DAT
which O
requires O
the O
calculation O

of B-DAT
partition O
function O

M B-DAT
j′=1 O
exp O
( O
ṽj′ O
· O
h(k)i O

for B-DAT
each O
gene. O
To O
calculate O
a O
single O
probability O

, B-DAT
we O
need O
to O
aggregate O

all B-DAT
genes O
in O
the O
network. O

To B-DAT
address O
this O
problem, O
we O

adopt B-DAT
the O
approach O
of O
negative O

sampling B-DAT
[13] O
which O
samples O
the O

negative B-DAT
interactions, O
interactions O
with O
no O

evidence B-DAT
of O
their O
existence, O
according O

to B-DAT
some O
noise O
distribution O
for O

each B-DAT
edge O
eij. O
This O
approach O

allows B-DAT
us O
to O
sample O
a O

small B-DAT
subset O
of O
genes O
from O

the B-DAT
network O
as O
negative O
samples O

for B-DAT
a O
gene, O
considering O
that O

the B-DAT
genes O
on O
selected O
subset O

don’t B-DAT
fall O
in O
the O
neighborhood O

Ni B-DAT
of O
the O
gene. O
Above O

objective B-DAT
function O
enhances O
the O
similarity O

of B-DAT
a O
gene O
viwith O
its O

neigh- B-DAT
borhood O
genes O
vj O
∈ O

Ni B-DAT
and O
weakens O
the O
similarity O

with B-DAT
genes O
not O
in O
its O

neighborhood B-DAT
genes O
vj O
/∈ O
Ni. O

It B-DAT
is O
inap- O
propriate O
to O

assume B-DAT
that O
the O
two O
genes O

in B-DAT
the O
network O
are O
not O

related B-DAT
if O
they O
are O
not O

connected. B-DAT
It O
may O
be O
the O

case B-DAT
that O
there O
is O
not O

enough B-DAT
experimental O
evidence O
to O
support O

that B-DAT
they O
are O
related O
yet. O

Thus, B-DAT
forcing O
the O
dissimilarity O
of O

a B-DAT
gene O
with O
all O
other O

genes, B-DAT
not O
in O
its O
neighborhood O

Ni B-DAT
seems O
to O
be O
inappropriate. O
We O
adopt O
Adaptive O
Moment O
Estimation O

(Adam) B-DAT
opti- O
mization O
[21], O
which O

is B-DAT
an O
extension O
to O
stochastic O

gra- B-DAT
dient O
descent, O
for O
optimizing O

Eq. B-DAT
12. O
Adam O
computes O
the O

adaptive B-DAT
learning O
rate O
for O
each O

parameter B-DAT
by O
per- O
forming O
smaller O

updates B-DAT
for O
the O
frequent O
parameters O

and B-DAT
larger O
updates O
for O
the O

infrequent B-DAT
parameters. O
The O
Adam O
method O

provides B-DAT
the O
ability O
of O
AdaGrad O

[22] B-DAT
to O
deal O
with O
sparse O

gradients B-DAT
and O
also O
the O
ability O

of B-DAT
RMSProp O
[23] O
to O
deal O

with B-DAT
non-stationary O
objectives. O
In O
each O

step, B-DAT
Adam O
algorithm O
samples O
mini-batch O

of B-DAT
interactions O
and O
then O

updates B-DAT
GNE’s O
parameters. O
To O
address O

the B-DAT
issue O
of O
over- O
fitting, O

regularization B-DAT
like O
dropout O
[24] O
and O

batch B-DAT
normal- O
ization O
[25] O
is O

added B-DAT
to O
hidden O
layers. O
Proper O

optimization B-DAT
of O
GNE O
gives O
the O

final B-DAT
representation O
for O
each O
gene. O
Experimental O
setup O
We O
evaluate O
our O

model B-DAT
using O
two O
real O
organism O

datasets. B-DAT
We O
take O
gene O
interaction O

network B-DAT
data O
from O
the O
BioGRID O

database B-DAT
[26] O
and O
gene O
expression O

data B-DAT
from O
DREAM5 O
challenge O
[7 O

]. B-DAT
We O
use O
two O
interaction O

datasets B-DAT
from O
BioGRID O
database O
(2017 O

released B-DAT
version O
3.4.153 O
and O
2018 O

released B-DAT
version O
3.4.158) O
to O
evaluate O

the B-DAT
predictive O
performance O
of O
our O

model. B-DAT
Self-interactions O
and O
redun- O
dant O

interactions B-DAT
are O
removed O
from O
interaction O

datasets. B-DAT
The O
statistics O
of O
the O

datasets B-DAT
are O
shown O
in O
Table O
2 O

. B-DAT
We O
evaluate O
the O
learned O
embeddings O

to B-DAT
infer O
gene O
net- O
work O

structure. B-DAT
We O
randomly O
hold O
out O

a B-DAT
fraction O
of O
inter- O
actions O

as B-DAT
the O
validation O
set O
for O

hyper-parameter B-DAT
tuning. O
Then, O
we O
divide O

the B-DAT
remaining O
interactions O
randomly O
into O

training B-DAT
and O
testing O
dataset O
with O

the B-DAT
equal O
number O
of O
interactions O

. B-DAT
Since O
the O
validation O
set O

and B-DAT
the O
test O
set O
con- O

tains B-DAT
only O
positive O
interactions, O
we O

randomly B-DAT
sample O
an O
equal O
number O

of B-DAT
gene O
pairs O
from O
the O

network, B-DAT
consider- O
ing O
the O
missing O

edge B-DAT
between O
the O
gene O
pairs O

represents B-DAT
the O
absence O
of O
interactions. O

Given B-DAT
the O
gene O
network O
G O

with B-DAT
a O
fraction O
of O
missing O

interactions, B-DAT
the O
task O
is O
to O

predict B-DAT
these O
missing O
interactions. O
We O
compare O
the O
GNE O
model O

with B-DAT
five O
competing O
methods. O
Correlation O

directly B-DAT
predicts O
the O
interactions O
between O

genes B-DAT
based O
on O
the O
correlation O

of B-DAT
expression O
pro- O
files. O
Then O

, B-DAT
the O
following O
three O
baselines O
( O

Isomap, B-DAT
LINE, O
and O
node2vec) O
are O

network B-DAT
embedding O
methods. O
Specif- O
ically, O

node2vec B-DAT
is O
the O
strong O
baseline O

for B-DAT
structural O
net- O
work O
embedding. O

We B-DAT
evaluate O
the O
performance O
of O

GNE B-DAT
against O
the O
following O
methods: O
• O
Correlation O
[27] O
It O
computes O

Pearson’s B-DAT
correlation O
coefficient O
between O
all O

genes B-DAT
and O
the O
interactions O
are O

ranked B-DAT
via O
correlation O
scores, O
i.e O

., B-DAT
highly O
correlated O
gene O
pairs O

receive B-DAT
higher O
confidence. O
• O
Isomap O
[10] O
It O
computes O

all-pairs B-DAT
shortest-path O
distances O
to O
create O

a B-DAT
distance O
matrix O
and O
performs O

singular-value B-DAT
decomposition O
of O
that O
matrix O

to B-DAT
learn O
a O
lower-dimensional O
representation O

. B-DAT
Genes O
separated O
Table O
2 O
Statistics O
of O
the O

interaction B-DAT
datasets O
from O
BioGRID O
and O

the B-DAT
gene O
expression O
data O
from O

DREAM5 B-DAT
challenge O

Interactions) B-DAT
Expression O
data O
Datasets O
#(Genes) O
2017 O
version O
2018 O

version B-DAT
#(Experiments O

) B-DAT
Yeast O
5950 O
544,652 O
557,487 O
536 O

E. B-DAT
coli O
4511 O
148,340 O
159,523 O
805 O

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
7 O
of O
14 O

by B-DAT
the O
distance O
less O
than O

threshold B-DAT
� O
in O
embedding O
space O

are B-DAT
considered O
to O
have O
the O

connection B-DAT
with O
each O
other O
and O

the B-DAT
reliability O
index, O
a O
likelihood O

indicating B-DAT
the O
interaction O
between O
two O

genes, B-DAT
is O
computed O
using O

FSWeight B-DAT
[28]. O
• O
LINE O
[16] O
Two O
separate O

embeddings B-DAT
are O
learned O
by O
preserving O

first-order B-DAT
and O
second-order O
proximity O
of O

the B-DAT
network O
structure O
respectively. O
Then O

, B-DAT
these O
embeddings O
are O
concatenated O

to B-DAT
get O
final O
representations O
for O

each B-DAT
node. O
• O
node2vec O
[14] O
It O
learns O

the B-DAT
embeddings O
of O
the O
node O

by B-DAT
applying O
Skip-gram O
model O
to O

node B-DAT
sequences O
generated O
by O
a O

biased B-DAT
random O
walk. O
We O
tuned O

two B-DAT
hyper-parameters O
p O
and O
q O

that B-DAT
control O
the O
random O
walk O

. B-DAT
Note O
that O
the O
competing O
methods O

such B-DAT
as O
Isomap, O
LINE, O
and O

node2vec B-DAT
are O
designed O
to O
capture O

only B-DAT
the O
topological O
properties O
of O

the B-DAT
network. O
For O
the O
fair O

com- B-DAT
parison O
with O
GNE O
that O

additionally B-DAT
integrates O
expression O
data, O
we O

concatenate B-DAT
attribute O
feature O
vector O
with O

learned B-DAT
gene O
representation O
to O
extend O

baselines B-DAT
by O
including O
the O
gene O

expression. B-DAT
We O
name O
these O
variants O

as B-DAT
Isomap+, O
LINE+, O
and O
node2vec O

+. B-DAT
We O
have O
implemented O
GNE O
with O

TensorFlow B-DAT
frame- O
work O
[29]. O
The O

parameter B-DAT
settings O
for O
GNE O
are O

deter- B-DAT
mined O
by O
its O
performance O

on B-DAT
the O
validation O
set. O
We O

randomly B-DAT
initialize O
GNE’s O
parameters, O
optimizing O

with B-DAT
mini-batch O
Adam. O
We O
test O

the B-DAT
batch O
size O
of O
[8 O

, B-DAT
16, O
32, O
64, O
128, O
256 O

] B-DAT
and O
learning O
rate O

of B-DAT
[0.1, O
0.01, O
0.005, O
0.002, O
0 O

.001, B-DAT
0.0001]. O
We O
test O
the O

number B-DAT
of O
negative O
samples O
to O

be B-DAT
[2, O
5, O
10, O
15, O
20 O

] B-DAT
as O
suggested O
by O
[13]. O

We B-DAT
test O
the O
embedding O
dimension O

d B-DAT
of O
[32, O
64, O
128, O
256 O

] B-DAT
for O
all O
methods. O
Also, O

we B-DAT
evaluate O
model’s O
performance O
with O

respect B-DAT
to O
differ- O
ent O
values O

of B-DAT
λ O
[0, O
0.2, O
0.4, O
0 O

.6, B-DAT
0.8, O
1], O
which O
is O

discussed B-DAT
in O
more O
detail O
later. O

The B-DAT
parameters O
are O
selected O
based O

on B-DAT
empirical O
evaluation O
and O
Table O
3 O
summarizes O
the O
optimal O
parameters O
tuned O

on B-DAT
validation O
data O
sets O

. B-DAT
To O
capture O
the O
non-linearity O
of O

gene B-DAT
expression O
data, O
we O
choose O

Exponential B-DAT
Linear O
Unit O
(ELU) O
[30 O

] B-DAT
activation O
function, O
which O
corresponds O

to B-DAT
δa O
in O
Eq. O
5. O

Also, B-DAT
ELU O
acti- O
vation O
avoids O

vanishing B-DAT
gradient O
problem O
and O
provides O

improved B-DAT
learning O
characteristics O
in O
comparison O

to B-DAT
other O
methods. O
We O
use O

a B-DAT
single O
hidden O
layer O
( O

k B-DAT
= O
1) O
with O
hyperbolic O

tangent B-DAT
activation O
(Tanh) O
to O
model O

complex B-DAT
statistical O
relationships O
between O
topological O

properties B-DAT
and O
attributes O
of O
the O
gene O

. B-DAT
The O
choice O
of O
ELU O

for B-DAT
attribute O
transformation O
and O
Tanh O

for B-DAT
hidden O
layer O
shows O
better O

performance B-DAT
upon O
empirical O
evaluation. O
We O
use O
the O
area O
under O

the B-DAT
ROC O
curve O
(AUROC) O
and O

area B-DAT
under O
the O
precision-recall O
curve O

(AUPR) B-DAT
[31] O
to O
eval- O
uate O

the B-DAT
rankings O
generated O
by O
the O

model B-DAT
for O
interactions O
in O
the O

test B-DAT
set. O
These O
metrics O
are O

widely B-DAT
used O
in O
evaluating O
the O

ranked B-DAT
list O
of O
predictions O
in O

gene B-DAT
interaction O
[4 O

]. B-DAT
Results O
and O
discussion O
We O
evaluate O

the B-DAT
ability O
of O
our O
GNE O

model B-DAT
to O
predict O
gene O
interaction O

of B-DAT
two O
real O
organisms. O
We O

present B-DAT
empirical O
results O
of O
our O

proposed B-DAT
method O
against O
other O
methods O

. B-DAT
Analysis O
of O
gene O
embeddings O
We O

visualize B-DAT
the O
embedding O
vectors O
of O

genes B-DAT
learned O
by O
GNE. O
We O

take B-DAT
the O
learned O
embeddings, O
which O

specifi- B-DAT
cally O
model O
the O
interactions O

by B-DAT
preserving O
topological O
and O
attribute O

similarity. B-DAT
We O
embed O
these O
embeddings O

into B-DAT
a O
2D O
space O
using O

t-SNE B-DAT
package O
[32] O
and O
visualize O

them B-DAT
(Fig. O
3). O
For O
comparison O

, B-DAT
we O
also O
visualize O
the O

embeddings B-DAT
learned O
by O
structure-preserving O
deep O

learning B-DAT
methods, O
such O
as O
LINE, O

and B-DAT
node2vec. O
In O
E. O
coli, O
a O
substantial O

fraction B-DAT
of O
functionally O
related O
genes O

are B-DAT
organized O
into O
operons, O
which O

are B-DAT
the O
group O
of O
genes O

that B-DAT
interact O
with O
each O
other O

and B-DAT
are O
co-regulated O
[33]. O
Since O

this B-DAT
concept O
fits O
well O
with O

the B-DAT
topological O
and O
attribute O
proximity O

implemented B-DAT
in O
GNE, O
we O
expect O

GNE B-DAT
to O
place O
genes O
within O

an B-DAT
operon O
close O
to O
each O

other B-DAT
in O
the O
embedding O
space O

. B-DAT
To O
evaluate O
this, O
we O

collect B-DAT
information O
about O
operons O
of O

E. B-DAT
coli O
from O
the O
DOOR O

database B-DAT
and O
visualize O
the O
embeddings O

of B-DAT
genes O
within O
these O
operons O
( O

Fig. B-DAT
3). O
Figure O
3 O
reveals O
the O
clustering O

structure B-DAT
that O
corre- O
sponds O
to O

the B-DAT
operons O
on O
E. O
coli O

. B-DAT
For O
example, O
operon O
with O

operon B-DAT
id O
3306 O
consists O
of O

seven B-DAT
genes: O
rsxA, O
rsxB, O
rsx, O

rsxD, B-DAT
rsxG, O
rsxE, O
and O
nth O

that B-DAT
are O
involved O
in O
electron O

transport. B-DAT
GNE O
infers O
similar O
representations O

for B-DAT
these O
genes, O
resulting O
in O

localized B-DAT
projection O
in O
the O
2D O

space. B-DAT
Similarly, O
other O
operons O
also O

show B-DAT
similar O
patterns O
(Fig. O
3). O
To O
test O
if O
the O
pattern O

in B-DAT
Fig. O
3 O
holds O
across O

all B-DAT
operons, O
we O
compute O
the O

average B-DAT
Euclidean O
distance O
between O
each O

gene’s B-DAT
vector O
representation O
and O
vector O

representations B-DAT
of O
other O
genes O
within O

the B-DAT
same O
operon. O
Genes O
within O

the B-DAT
same O
operon O
have O
significantly O

similar B-DAT
vector O
representa- O
tion O
yi O

than B-DAT
expected O
by O
chance O
(p-value O

= B-DAT
1.75e O
− O
127, O
2-sample O

KS B-DAT
test O

). B-DAT
Table O
3 O
Optimal O
parameter O
settings O

for B-DAT
GNE O
model O

Dataset B-DAT
Learning O
rate O
Batch O
size O

Embedding B-DAT
dimension O
(d) O
Epoch O
Negative O

samples B-DAT
Yeast O
0.005 O
256 O
128 O
20 O

10 B-DAT

E. B-DAT
coli O
0.002 O
128 O
128 O
20 O
10 O

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
8 O
of O
14 O

Fig. B-DAT
3 O
Visualization O
of O
learned O

embeddings B-DAT
for O
genes O
on O
E. O

coli. B-DAT
Genes O
are O
mapped O
to O

the B-DAT
2D O
space O
using O
the O

t-SNE B-DAT
package O
[32] O
with O
learned O

gene B-DAT
representations O
(yi O
, O

i B-DAT
= O
1, O
2, O
. O
. O
. O
, O
M O

) B-DAT
from O
different O
methods: O
a O

GNE, B-DAT
b O
LINE, O
and O
c O

node2vec B-DAT
as O
input. O
Operons O
3203, O
3274, O
3279, O
3306, O
and O
3736 O
of O

E. B-DAT
coli O
are O
visualized O
and O

show B-DAT
clustering O
patterns. O
Best O
viewed O

on B-DAT
screen O

Thus, B-DAT
the O
analysis O
here O
indicates O

that B-DAT
GNE O
can O
learn O
similar O

representations B-DAT
for O
genes O
with O
similar O

topological B-DAT
properties O
and O
expression. O
Gene O
interaction O
prediction O
We O
randomly O

remove B-DAT
50% O
of O
interactions O
from O

the B-DAT
net- O
work O
and O
compare O

various B-DAT
methods O
to O
evaluate O
their O

pre- B-DAT
dictions O
for O
50% O
missing O

interactions. B-DAT
Table O
4 O
shows O
the O

performance B-DAT
of O
GNE O
and O
other O

methods B-DAT
on O
gene O
interac- O
tion O

prediction B-DAT
across O
different O
datasets. O
As O

our B-DAT
method O
significantly O
outperforms O
other O

competing B-DAT
methods, O
it O
indicates O
the O

informativeness B-DAT
of O
gene O
expression O
in O

pre- B-DAT
dicting O
missing O
interactions. O
Also O

, B-DAT
our O
model O
is O
capable O

of B-DAT
integrating O
attributes O
with O
topological O

properties B-DAT
to O
learn O
better O
representations. O
Table O
4 O
Area O
under O
ROC O

curve B-DAT
(AUROC) O
and O
Area O
under O

PR B-DAT
curve O
(AUPR) O
for O
gene O

Interaction B-DAT
Prediction O

Methods B-DAT
Yeast O
E. O
coli O
AUROC O
AUPR O
AUROC O
AUPR O

Correlation B-DAT
0.582 O
0.579 O
0.537 O
0.557 O
Isomap O
0.507 O
0.588 O
0.559 O
0.672 O

LINE B-DAT
0.726 O
0.686 O
0.897 O
0.851 O
node2vec O
0.739 O
0.708 O
0.912 O
0.862 O

Isomap+ B-DAT
0.653 O
0.652 O
0.644 O
0.649 O
LINE+ O
0.745 O
0.713 O
0.899 O
0.856 O

node2vec+ B-DAT
0.751 O
0.716 O
0.871 O
0.826 O
GNE O
(Topology) O
0.787 O
0.784 O
0.930 O

0.931 B-DAT

GNE B-DAT
(our O
model) O
0.825* O
0.821* O
0 O

.940* B-DAT
0.939* O
+ O
indicates O
the O
concatenation O
of O

expression B-DAT
data O
with O
learned O
embeddings O

to B-DAT
create O
final O
representation O

. B-DAT
* O
denotes O
that O
GNE O

significantly B-DAT
outperforms O
node2vec O
at O
0.01 O

level B-DAT
paired O
t-test. O
Note O
that O

method B-DAT
that O
achieves O
the O
best O

performance B-DAT
is O
bold O
faced O
We O
compare O
our O
model O
with O

a B-DAT
correlation-based O
method, O
that O
takes O

only B-DAT
expression O
data O
into O
account O

. B-DAT
Our O
model O
shows O
significant O

improvement B-DAT
of O
0.243 O
(AUROC), O
0.242 O
( O

AUPR) B-DAT
on O
yeast O
and O
0.403 O
( O

AUROC), B-DAT
0.382 O
(AUPR) O
on O
E. O

coli B-DAT
over O
correlation-based O
methods. O
This O

improve- B-DAT
ment O
suggests O
the O
significance O

of B-DAT
the O
topological O
proper- O
ties O

of B-DAT
the O
gene O
network. O
The O
network O
embedding O
method, O
Isomap O

, B-DAT
performs O
poorly O
in O
comparison O

to B-DAT
correlation-based O
methods O
on O
yeast O

because B-DAT
of O
its O
limitation O
on O

network B-DAT
inference. O
Deep O
learning O
based O

network B-DAT
embedding O
methods O
such O
as O

LINE, B-DAT
and O
node2vec O
show O
the O

significant B-DAT
gain O
over O
Isomap O
and O

correlation-based B-DAT
methods. O
node2vec O
out- O
performs O

LINE B-DAT
across O
two O
datasets. O
Moreover, O

GNE B-DAT
trained O
only O
with O
topological O

properties B-DAT
outperforms O
these O
structured-based O
deep O

learning B-DAT
methods O
(Table O
4). O
However, O

these B-DAT
methods O
don’t O
consider O
the O

attributes B-DAT
of O
the O
gene O
that O

we B-DAT
suggest O
to O
contain O
useful O

informa- B-DAT
tion O
for O
gene O
interaction O

prediction. B-DAT
By O
adding O
expres- O
sion O

data B-DAT
with O
topological O
properties, O
GNE O

outperforms B-DAT
structure-preserving O
deep O
embedding O
methods O

across B-DAT
both O
datasets. O
Focusing O
on O
the O
results O
corresponding O

to B-DAT
the O
integra- O
tion O
of O

expression B-DAT
data O
with O
topological O
properties O

, B-DAT
we O
find O
that O
the O

method B-DAT
of O
integrating O
the O
expression O

data B-DAT
plays O
an O
essential O
role O

in B-DAT
the O
performance. O
Performance O
of O

node2vec+ B-DAT
(LINE+, O
Isomap+) O
shows O
little O

improvement B-DAT
with O
the O
integration O
of O

expression B-DAT
data O
on O
yeast. O
How- O

ever, B-DAT
node2vec+ O
(LINE+, O
Isomap+) O
has O

no B-DAT
improvement O
or O
decline O
in O

performance B-DAT
on O
E. O
coli. O
The O

decline B-DAT
in O
per- O
formance O
indicates O

that B-DAT
merely O
concatenating O
the O
expres- O

sion B-DAT
vector O
with O
learned O
representations O

for B-DAT
the O
gene O
is O
insufficient O

to B-DAT
capture O
the O
rich O
information O

in B-DAT
expression O
data. O
The O
late O

fusion B-DAT
approach O
of O
combining O
the O

embed- B-DAT
ding O
vector O
corresponding O
to O

the B-DAT
topological O

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
9 O
of O
14 O

of B-DAT
the O
gene O
network O
and O

the B-DAT
feature O
vector O
represent- O
ing O

expression B-DAT
data O
has O
no O
significant O

improvement B-DAT
in O
the O
performance O
(except O

Isomap). B-DAT
In O
contrast, O
our O
model O

incorporates B-DAT
gene O
expression O
data O
with O

topological B-DAT
prop- O
erties O
by O
the O

early B-DAT
fusion O
method O
and O
shows O

significant B-DAT
improvement O
over O
other O
methods. O
Impact O
of O
network O
sparsity O
We O

investigate B-DAT
the O
robustness O
of O
our O

model B-DAT
to O
network O
sparsity. O
We O

hold B-DAT
out O
10% O
interactions O
as O

the B-DAT
test O
set O
and O
change O

the B-DAT
sparsity O
of O
the O
remaining O

network B-DAT
by O
randomly O
removing O
a O

portion B-DAT
of O
remaining O
interactions. O
Then O

, B-DAT
we O
train O
GNE O
to O

predict B-DAT
interactions O
in O
the O
test O

set B-DAT
and O
eval- O
uate O
the O

change B-DAT
in O
performance O
to O
network O

sparsity. B-DAT
We O
evaluate O
two O
versions O

of B-DAT
our O
implementations: O
GNE O
with O

only B-DAT
topological O
properties O
and O
GNE O

with B-DAT
topological O
properties O
and O
expression O

data. B-DAT
The O
result O
is O
shown O

in B-DAT
Fig. O
4. O
Figure O
4 O
shows O
that O
our O

method’s B-DAT
performance O
improves O
with O
an O

increase B-DAT
in O
the O
number O
of O

training B-DAT
interactions O
across O
datasets. O
Also O

, B-DAT
our O
method’s O
performance O
improves O

when B-DAT
expression O
data O
is O
integrated O

with B-DAT
the O
topological O
structure. O
Specifically, O

GNE B-DAT
trained O
on O
10% O
of O

total B-DAT
inter- O
actions O
and O
attributes O

of B-DAT
yeast O
shows O
a O
significant O

gain B-DAT
of O
0.172 O
AUROC O
(from O
0 O

.503 B-DAT
to O
0.675) O
over O
GNE O

trained B-DAT
only O
with O
10% O
of O

total B-DAT
interactions O
as O
shown O
in O

Fig. B-DAT
4. O
Similarly, O
GNE O
improves O

the B-DAT
AUROC O
from O
0.497 O
to O
0 O

.816 B-DAT
for O
E. O
coli O
with O

the B-DAT
same O
setup O
as O
shown O

in B-DAT
Fig. O
4. O
The O
integration O

of B-DAT
gene O
expression O
data O
results O

in B-DAT
less O
improve- O
ment O
when O

we B-DAT
train O
GNE O
on O
a O

relatively B-DAT
large O
number O
of O
interactions. O
Moreover, O
the O
performance O
of O
GNE O

trained B-DAT
with O
50% O
of O
total O

interactions B-DAT
and O
expression O
data O
is O

comparable B-DAT
to O
be O
trained O
with O

80% B-DAT
of O
total O
interactions O
without O

gene B-DAT
expression O
data O
as O
shown O

in B-DAT
Fig. O
4. O
The O
integration O

of B-DAT
expression O
data O
with O
topological O

properties B-DAT
into O
GNE O
model O
has O

more B-DAT
improvement O
on O
E. O
coli O

than B-DAT
yeast O
when O
we O
train O

with B-DAT
10% O
of O
total O
interactions O

for B-DAT
each O
dataset O

. B-DAT
The O
reason O
for O
this O
is O

likely B-DAT
the O
difference O
in O
the O

number B-DAT
of O
available O
interactions O
for O

yeast B-DAT
and O
E. O
coli O
(Table O

2). B-DAT
This O
indicates O
the O
informativeness O

of B-DAT
gene O
expression O
when O
we O

have B-DAT
few O
interactions O
and O
supports O

the B-DAT
idea O
that O
the O
integration O

of B-DAT
expression O
data O
with O
topological O

properties B-DAT
improves O
gene O
interaction O
prediction O

. B-DAT
Impact O
of O
λ O
GNE O
involves O

the B-DAT
parameter O
λ O
that O
controls O

the B-DAT
impor- O
tance O
of O
gene O

expression B-DAT
information O
relative O
to O
topolog O

- B-DAT
ical O
properties O
of O
gene O

network B-DAT
as O
shown O
in O
Eq. O
6 O

. B-DAT
We O
examine O
how O
the O

choice B-DAT
of O
the O
parameter O
λ O

affects B-DAT
our O
method’s O
performance. O
Figure O
5 O
shows O
the O
comparison O
of O
our O

method’s B-DAT
performance O
with O
different O
values O

of B-DAT
λ O
when O
GNE O
is O

trained B-DAT
on O
varying O
percentage O
of O

total B-DAT
interactions O

. B-DAT
We O
evaluate O
the O
impact O
of O

λ B-DAT
on O
range O
[0, O
0.2 O

, B-DAT
0.4, O
0.6, O
0.8, O
1]. O

When B-DAT
λ O
becomes O
0, O
the O

learned B-DAT
representations O
model O
only O
topological O

properties. B-DAT
In O
contrast, O
setting O
the O

high B-DAT
value O
for O
λ O
makes O

GNE B-DAT
learn O
only O
from O
attributes O

and B-DAT
degrades O
its O
performance. O
Therefore, O

our B-DAT
model O
performs O
well O
when O

λ B-DAT
is O
within O
[0, O
1]. O
Figure O
5 O
shows O
that O
the O

integration B-DAT
of O
expression O
data O
improves O

the B-DAT
performance O
of O
GNE O
to O

predict B-DAT
gene O
interac- O
tions. O
Impact O

of B-DAT
λ O
depends O
on O
the O

number B-DAT
of O
interactions O
used O
to O

train B-DAT
GNE. O
If O
GNE O
is O

trained B-DAT
with O
few O
interactions, O
integration O

of B-DAT
expression O
data O
with O
topological O

properties B-DAT
plays O
a O
vital O
role O

in B-DAT
predicting O
missing O
interactions. O
As O

the B-DAT
number O
of O
training O
interactions O

increases, B-DAT
integration O
of O
expression O
data O

has B-DAT
less O
impact O
but O
still O

improves B-DAT
the O
performance O
over O
only O

topological B-DAT
properties O

. B-DAT
Figures O
4 O
and O
5 O
demonstrate O

that B-DAT
the O
expression O
data O
contributes O

the B-DAT
increase O
in O
AUROC O
by O

nearly B-DAT
0.14 O
when O
interactions O
are O

less B-DAT
than O
40% O
for O
yeast O

and B-DAT
about O
0.32 O
when O
interactions O

are B-DAT
less O
than O
10% O
for O

E. B-DAT
coli. O
More O
topo- O
logical O

properties B-DAT
and O
attributes O
are O
required O

for B-DAT
yeast O
than O
E. O
coli O

. B-DAT
It O
may O
be O
related O

to B-DAT
the O
fact O
that O
yeast O

is B-DAT
a O
more O
complex O
species O

than B-DAT
E. O
coli. O
Moreover, O
we O

can B-DAT
spec- O
ulate O
that O
more O

topological B-DAT
properties O
and O
attributes O
are O
a O
b O

Fig. B-DAT
4 O
AUROC O
comparison O
of O

GNE’s B-DAT
performance O
with O
respect O
to O

network B-DAT
sparsity. O
a O
yeast O
b O

E. B-DAT
coli. O
Integration O
of O
expression O

data B-DAT
with O
topological O
properties O
of O

the B-DAT
gene O
network O
improves O
the O

performance B-DAT
for O
both O

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
10 O
of O
14 O

a B-DAT
b O
Fig. O
5 O
Impact O
of O
λ O

on B-DAT
GNE’s O
performance O
trained O
with O

different B-DAT
percentages O
of O
interactions. O
a O

yeast B-DAT
b O
E. O
coli. O
Different O

lines B-DAT
indicate O
performance O
of O
GNE O

trained B-DAT
with O
different O
percentages O
of O

interactions B-DAT

required B-DAT
for O
higher O
eukaryotes O
like O

humans. B-DAT
In O
humans, O
GNE O
that O

integrates B-DAT
topological O
properties O
with O
attributes O

may B-DAT
be O
more O
successful O
than O

the B-DAT
methods O
that O
only O
use O

either B-DAT
topological O
properties O
or O
attributes. O
This O
demonstrates O
the O
sensitivity O
of O

GNE B-DAT
to O
parame- O
ter O
λ O

. B-DAT
This O
parameter O
λ O
has O

a B-DAT
considerable O
impact O
on O
our O
method’s O
performance O
and O
should O

be B-DAT
appropriately O
selected O

. B-DAT
Investigation O
of O
GNE’s O
predictions O
We O

investigate B-DAT
the O
predictive O
ability O
of O

our B-DAT
model O
in O
iden- O
tifying O

new B-DAT
gene O
interactions. O
For O
this O

aim, B-DAT
we O
consider O

a B-DAT
b O
c O
d O

Fig. B-DAT
6 O
Temporal O
holdout O
validation O

in B-DAT
predicting O
new O
interactions. O
Performance O

is B-DAT
measured O
by O
the O
area O

under B-DAT
the O
ROC O
curve O
and O

the B-DAT
area O
under O
the O
precision-recall O

curve. B-DAT
Shown O
are O
the O
performance O

of B-DAT
each O
method O
based O
on O

the B-DAT
AUROC O
(a, O
b) O
and O

AUPR B-DAT
(c, O
d) O
for O
yeast O

and B-DAT
E. O
coli. O
The O
limit O

of B-DAT
the O
y-axis O
is O
adjusted O

to B-DAT
[0.5, O
1.0] O
for O
the O

precision-recall B-DAT
curve O
to O
make O
the O

difference B-DAT
in O
performance O
more O
visible. O

GNE B-DAT
outperforms O
LINE O
and O

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
11 O
of O
14 O

Table B-DAT
5 O
AUROC O
and O
AUPR O

comparision B-DAT
for O
temporal O
holdout O
validation O
Methods O
Yeast O
E. O
coli O

AUROC B-DAT
AUPR O
AUROC O
AUPR O
LINE O
0.620 O
0.611 O
0.569 O
0.598 O

node2vec B-DAT
0.640 O
0.609 O
0.587 O
0.599 O
GNE O
(our O
model) O
0.710 O
0.683 O

0.653 B-DAT
0.658 O

Note B-DAT
that O
method O
that O
achieves O

the B-DAT
best O
performance O
is O
bold O

faced B-DAT
two O
versions O
of O
BioGRID O
interaction O

datasets B-DAT
at O
two O
dif- O
ferent O

time B-DAT
points O
(2017 O
and O
2018 O

version), B-DAT
where O
the O
older O
version O

is B-DAT
used O
for O
training O
and O

the B-DAT
newer O
one O
is O
used O

for B-DAT
testing O
the O
model O
(temporal O

holdout B-DAT
validation). O
The O
2018 O
version O

contains B-DAT
12,835 O
new O
interactions O
for O

yeast B-DAT
and O
11,185 O
new O
interactions O

for B-DAT
E. O
coli O
than O
the O

2017 B-DAT
ver- O
sion. O
GNE’s O
performance O

trained B-DAT
with O
50% O
and O
80 O

% B-DAT
of O
total O
interactions O
are O

comparable B-DAT
for O
both O
yeast O
and O

E. B-DAT
coli O
(Figs. O
4 O
and O
5 O

). B-DAT
We O
thus O
train O
our O

model B-DAT
with O
50% O
of O
total O

interactions B-DAT
from O
the O
2017 O
version O

to B-DAT
learn O
the O
embed- O
dings O

for B-DAT
genes O
and O
demonstrate O
the O

impact B-DAT
of O
integrating O
expression O
data O

with B-DAT
topological O
properties. O
We O
create O

the B-DAT
test O
set O
with O
new O

interactions B-DAT
from O
the O
2018 O
version O

of B-DAT
BioGRID O
as O
positive O
interactions O

and B-DAT
the O
equal O
number O
of O

negative B-DAT
interactions O
randomly O
sampled. O
We O

make B-DAT
pre- O
dictions O
for O
these O

interactions B-DAT
using O
learned O
embeddings O
and O

create B-DAT
a O
list O
of O
(Gene O

vi, B-DAT
Gene O
vj, O
probability), O
ranked O

by B-DAT
the O
predicted O
probability. O
We O

consider B-DAT
predicted O
gene O
pairs O
with O

the B-DAT
probabilities O
of O
0.5 O
or O

higher B-DAT
but O
are O
miss- O
ing O

from B-DAT
BioGRID O
for O
further O
investigation O

as B-DAT
we O
discuss O
later O
in O

this B-DAT
section. O
The O
temporal O
holdout O
performance O
of O

our B-DAT
model O
in O
comparison O
to O

other B-DAT
methods O
is O
shown O
in O

Fig. B-DAT
6. O
We O

observe B-DAT
that O
GNE O
outperforms O
both O

node2vec B-DAT
and O
LINE O
in O
temporal O

holdout B-DAT
validation O
across O
both O
yeast O

and B-DAT
E. O
coli O
datasets, O
indicating O

GNE B-DAT
can O
accurately O
predict O
new O

genetic B-DAT
interactions. O
Table O
5 O
shows O

that B-DAT
GNE O
achieves O
substantial O
improvement O

of B-DAT
7.0 O
(AUROC), O
7.4 O
(AUPR) O

on B-DAT
yeast O
and O
6.6 O
(AUROC), O
5 O

.9 B-DAT
(AUPR) O
on O
E. O
coli O

datasets. B-DAT
Table O
6 O
shows O
the O
top O

5 B-DAT
interactions O
with O
the O
sig O

- B-DAT
nificant O
increase O
in O
predicted O

probability B-DAT
for O
both O
yeast O
and O

E. B-DAT
coli O
after O
expression O
data O

is B-DAT
integrated. O
We O
also O
provide O

literature B-DAT
evidence O
with O
experimental O
evidence O

code B-DAT
obtained O
from O
the O
BioGRID O

database B-DAT
[26] O
sup- O
porting O
these O

predictions. B-DAT
BioGRID O
compiles O
interaction O
data O

from B-DAT
numerous O
publications O
through O
comprehen- O

sive B-DAT
curation O
efforts. O
Taking O
new O

interactions B-DAT
added O
to O
BioGRID O
(version O
3 O

.4.158) B-DAT
into O
consideration, O
we O
evalu- O

ate B-DAT
the O
probability O
of O
these O

interactions B-DAT
predicted O
by O
GNE O
trained O

with B-DAT
and O
without O
expression O
data. O

Specifically, B-DAT
integration O
of O
expression O
data O

increases B-DAT
the O
probability O
of O
8331 O
( O

out B-DAT
of O
11,185) O
interactions O
for O

E. B-DAT
coli O
(improving O
AUROC O
from O
0 O

.606 B-DAT
to O
0.662) O
and O
6,010 O
( O

out B-DAT
of O
12,835) O
interactions O
for O

yeast B-DAT
(improving O
AUROC O
from O
0.685 O

to B-DAT
0.707). O
Integration O
of O
topology O

and B-DAT
expression O
data O
sig- O
nificantly O

increases B-DAT
the O
probabilities O
of O
true O

interactions B-DAT
between O
genes O
(Table O
6). O
To O
further O
evaluate O
GNE’s O
predictions O

, B-DAT
we O
consider O
the O
new O

version B-DAT
of O
BioGRID O
(version O
3.4.162) O

and B-DAT
evaluate O
2609 O
yeast O
gene O

pairs B-DAT
(Additional O
file O
1: O
Table O

S1) B-DAT
and O
871 O
E. O
coli O

gene B-DAT
pairs O
(Additional O
file O
2: O

Table B-DAT
S2) O
predicted O
by O
GNE O

with B-DAT
the O
probabilities O
of O
0.5 O

or B-DAT
higher. O
We O
find O
that O
128 O
(5%) O
yeast O
gene O
pairs O
and O

78 B-DAT
(9%) O
E. O
coli O
gene O

pairs B-DAT
are O
true O
interactions O
that O

have B-DAT
been O
added O
to O
the O

lat- B-DAT
est O
release O
of O
BioGRID O

. B-DAT
We O
then O
evaluate O
the O

predictive B-DAT
ability O
of O
GNE O
by O

calculating B-DAT
the O
percentage O
of O
true O

inter- B-DAT
actions O
with O
regard O
to O

different B-DAT
probability O
bins O
(Fig. O
7). O

Sixteen B-DAT
percent O
of O
predicted O
yeast O

gene B-DAT
pairs O
and O
17.5% O
Table O
6 O
New O
gene O
interactions O

that B-DAT
are O
assigned O
high O
probability O

by B-DAT
GNE O

Organism B-DAT
Probability O
Gene O
i O
Gene O
j O
Experimental O

evidence B-DAT
code O
Topology O
Topology O

+ B-DAT
Expression O

Yeast B-DAT
0.287 O
0.677 O
TFC8 O
DHH1 O
Affinity O

Capture-RNA B-DAT
[35 O

] B-DAT
0.394 O
0.730 O
SYH1 O
DHH1 O
Affinity O

Capture-RNA B-DAT
[35 O

] B-DAT
0.413 O
0.746 O
CPR7 O
DHH1 O
Affinity O

Capture-RNA B-DAT
[35 O

] B-DAT
0.253 O
0.551 O
MRP10 O
DHH1 O
Affinity O

Capture-RNA B-DAT
[35 O

] B-DAT
0.542 O
0.835 O
RPS13 O
ULP2 O
Affinity O

Capture-MS B-DAT
[36 O

] B-DAT
E. O
coli O

0.014 B-DAT
0.944 O
ATPB O
RFBC O
Affinity O

Capture-MS B-DAT
[37] O
0.012 O
0.941 O
NARQ O
CYDB O
Affinity O

Capture-MS B-DAT
[37 O

] B-DAT
0.013 O
0.937 O
PCNB O
PAND O
Affinity O

Capture-MS B-DAT
[37 O

] B-DAT
0.015 O
0.939 O
FLIF O
CHEY O
Affinity O

Capture-MS B-DAT
[37 O

] B-DAT
0.017 O
0.938 O
YCHM O
PROB O
Affinity O

Capture-MS B-DAT
[37 O

] B-DAT
New O
gene O
interactions O
on O
2018 O

version B-DAT
that O
are O
assigned O
high O

probability B-DAT
by O
GNE O
after O
integration O

of B-DAT
expression O
data. O
We O
provide O

probability B-DAT
predicted O
by O
GNE O
(with/without O

expression B-DAT
data) O
for O
new O
interactions O

in B-DAT
the O
2018 O
version O
and O

evidence B-DAT
supporting O
the O
existence O
of O

predicted B-DAT
interactions O

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
12 O
of O
14 O

Fig. B-DAT
7 O
The O
percentage O
of O

true B-DAT
interactions O
from O
GNE’s O
predictions O

with B-DAT
different O
probability O
bins. O
a O

yeast B-DAT
b O
E. O
coli. O
We O

divide B-DAT
the O
gene O
pairs O
based O

on B-DAT
their O
predicted O
probabilities O
to O

different B-DAT
probability O
ranges O
(as O
shown O

in B-DAT
the O
x-axis) O
and O
identify O

the B-DAT
number O
of O
predicted O
true O

interactions B-DAT
in O
each O
range. O
Each O

bar B-DAT
indicates O
the O
percentage O
of O

true B-DAT
interactions O
out O
of O
predicted O

gene B-DAT
pairs O
in O
that O
probability O

range B-DAT
of O
predicted O
E. O
coli O
gene O

pairs B-DAT
with O
the O
probability O
higher O

than B-DAT
0.9 O
are O
true O
interactions O

. B-DAT
This O
suggests O
that O
gene O

pairs B-DAT
with O
high O
probability O
predicted O

by B-DAT
GNE O
are O
more O
likely O

to B-DAT
be O
true O
interactions. O
To O
support O
our O
finding O
that O

GNE B-DAT
predicted O
gene O
pairs O
have O

high B-DAT
value, O
we O
manually O
check O

gene B-DAT
pairs O
that O
have O
high O

predicted B-DAT
probability O
but O
are O
missing O

from B-DAT
the O
latest O
BioGRID O
release O

. B-DAT
We O
find O
that O
these O

gene B-DAT
pairs O
interact O
with O
the O

same B-DAT
set O
of O
other O
genes. O

For B-DAT
example, O
GNE O
pre- O
dicts O

the B-DAT
interaction O
between O
YDR311W O
and O

YGL122C B-DAT
with O
the O
probability O
of O
0 O

.968. B-DAT
Mining O
BioGRID O
database, O
we O

find B-DAT
that O
these O
genes O
interact O

with B-DAT
the O
same O
set O
of O
374 O
genes. O
Similarly, O
E. O
coli O
genes O

DAMX B-DAT
and O
FLIL O
with O
the O

predicted B-DAT
probability O
of O
0.998 O
share O

320 B-DAT
interacting O
genes. O
In O
this O

way, B-DAT
we O
identify O
all O
interacting O

genes B-DAT
shared O
by O
each O
of O

the B-DAT
predicted O
gene O
pairs O
in O

yeast B-DAT
and O
E. O
coli O
(Additional O

file B-DAT
1: O
Table O
S1 O
and O

Additional B-DAT
file O
2 O

: B-DAT
Table O
S2). O
Figure O
8 O
shows O

the B-DAT
average O
number O
of O
inter O

- B-DAT
acting O
genes O
shared O
by O

a B-DAT
gene O
pair. O
In O
general, O

gene B-DAT
pairs O
with O
a O
high O

GNE B-DAT
probability O
tend O
to O
have O

a B-DAT
large O
number O
of O
interacting O

genes. B-DAT
For O
example, O
gene O
pairs O

with B-DAT
the O
probability O
greater O
than O
0 O

.9 B-DAT
have, O
on O
average, O
82 O

common B-DAT
interacting O
genes O
for O
yeast O

and B-DAT
58 O
for O
E. O
coli. O

Two B-DAT
sample O
t-test O
analysis O
has O

shown B-DAT
that O
there O
is O
a O

significant B-DAT
difference O
in O
the O
number O

of B-DAT
shared O
inter- O
acting O
genes O

with B-DAT
respect O
to O
different O
probability O

bins B-DAT
(Table O
7). O
Moreover, O
we O
search O
the O
literature O

to B-DAT
see O
if O
we O
can O

find B-DAT
supporting O
evidence O
for O
predicted O

interactions. B-DAT
We O
find O
literature O
evidence O

for B-DAT
an O
interaction O
between O
YCL032W O

(STE50) B-DAT
and O
YDL035C O
(GPR1), O
which O

has B-DAT
the O
probability O
of O
0.98 O

predicted B-DAT
by O
GNE. O
STE50 O
is O

an B-DAT
adaptor O
that O
links O
G-protein O

complex B-DAT
in O
cell O
signalling, O
and O

GPR1 B-DAT
is O
a O
G- O
protein O

coupled B-DAT
receptor. O
Both O
STE50 O
and O

GPR1 B-DAT
share O
a O

Fig. B-DAT
8 O
The O
average O
number O

of B-DAT
common O
interacting O
genes O
between O

the B-DAT
gene O
pairs O
predicted O
by O

GNE. B-DAT
a O
yeast O
b O
E. O

coli. B-DAT
We O
divide O
gene O
pairs O

into B-DAT
different O
probability O
groups O
based O

on B-DAT
predicted O
probabilities O
by O
GNE O

and B-DAT
compute O
the O
number O
of O

common B-DAT
interacting O
genes O
shared O
by O

these B-DAT
gene O
pairs. O
We O
categorize O

these B-DAT
gene O
pairs O
into O
different O

probability B-DAT
ranges O
(as O
shown O
in O

the B-DAT
x-axis). O
Each O
bar O
represents O

the B-DAT
average O
number O
of O
common O

interacting B-DAT
genes O
shared O
by O
gene O

pairs B-DAT
in O
each O
probability O

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
13 O
of O
14 O

Table B-DAT
7 O
Results O
of O
two-sample O

t-test B-DAT
Probability O
bin O
for O
Sample O
A O

Probability B-DAT
bin O
for O
Sample O
B O
p-value O
for O
yeast O

p-value B-DAT
for O
E. O
coli O
0.5 O
- O
0.6 O
0.6 O

- B-DAT
0.7 O
9.2e O
− O
14 O

6.9e B-DAT
− O
02 O
0.5 O

- B-DAT
0.6 O
0.7 O
- O
0.8 O

3.02e B-DAT
− O
51 O
1.23e O

− B-DAT
05 O
0.5 O
- O
0.6 O

0.8 B-DAT
- O
0.9 O
6.1e O

− B-DAT
117 O
7.4e O
− O
14 O

0.5 B-DAT
- O
0.6 O
0.9 O

- B-DAT
1.0 O
2.1e O
− O
177 O

3.7e B-DAT
− O
39 O
0.6 O

- B-DAT
0.7 O
0.7 O
- O
0.8 O

8.2e B-DAT
− O
17 O
1.1e O

− B-DAT
02 O
0.6 O
- O
0.7 O

0.8 B-DAT
- O
0.9 O
3.5e O

− B-DAT
69 O
9.2e O
− O
09 O

0.6 B-DAT
- O
0.7 O
0.9 O

- B-DAT
1.0 O
2.1e O
− O
128 O

1.9e B-DAT
− O
30 O
0.7 O

- B-DAT
0.8 O
0.8 O
- O
0.9 O

4.7e B-DAT
− O
28 O
4.8e O

− B-DAT
04 O
0.7 O
- O
0.8 O

0.9 B-DAT
- O
1.0 O
6.2e O

− B-DAT
87 O
7.4e O
− O
23 O

0.8 B-DAT
- O
0.9 O
0.9 O

- B-DAT
1.0 O
4.3e O
− O
35 O

5.1e B-DAT
− O
13 O
We O
divide O

gene B-DAT
pairs O
into O
different O
probability O

groups B-DAT
based O
on O
predicted O
probabilities O

by B-DAT
GNE O
and O
compute O
the O

number B-DAT
of O
common O
interacting O
genes O

shared B-DAT
by O
these O
gene O
pairs O

. B-DAT
Significance O
test O
shows O
there O

is B-DAT
the O
significant O
difference O
between O

average B-DAT
number O
of O
shared O
genes O

in B-DAT
different O
probability O
bins O
common O
function O
of O
cell O
signalling O

via B-DAT
G-protein. O
Besides, O
STE50p O
interacts O

with B-DAT
STE11p O
in O
the O
two-hybrid O

system, B-DAT
which O
is O
a O
cell-based O

system B-DAT
examining O
protein-protein O
interactions O
[34 O

]. B-DAT
Also, O
BioGRID O
has O
evidence O

of B-DAT
30 O
physi- O
cal O
and O
4 O
genetic O
associations O
between O
STE50 O
and O

STE11. B-DAT
Thus, O
STE50 O
is O
highly O

likely B-DAT
to O
interact O
with O
STE11 O

, B-DAT
which O
in O
turn O
interacts O

with B-DAT
GPR1. O
This O
analysis O
demonstrates O
the O
potential O

of B-DAT
our O
method O
in O
the O

discovery B-DAT
of O
gene O
interactions. O
Also O

, B-DAT
GNE O
can O
help O
the O

curator B-DAT
to O
identify O
interactions O
with O

strong B-DAT
potential O
that O
need O
to O

be B-DAT
looked O
at O
with O
experimental O

validation B-DAT
or O
within O
the O
literature. O
Conclusion O
We O
developed O
a O
novel O

deep B-DAT
learning O
framework, O
namely O
GNE O

to B-DAT
perform O
gene O
network O
embedding O

. B-DAT
Specifi- O
cally, O
we O
design O

deep B-DAT
neural O
network O
architecture O
to O

model B-DAT
the O
complex O
statistical O
relationships O

between B-DAT
gene O
interaction O
network O
and O

expression B-DAT
data. O
GNE O
is O
flexible O

to B-DAT
the O
addition O
of O
different O

types B-DAT
and O
num- O
ber O
of O

attributes. B-DAT
The O
features O
learned O
by O

GNE B-DAT
allow O
us O
to O
use O

out-of-the-box B-DAT
machine O
learning O
classifiers O
like O

Logistic B-DAT
Regression O
to O
predict O
gene O

interactions B-DAT
accurately. O
GNE O
relies O
on O
a O
deep O

learning B-DAT
technique O
that O
can O
learn O

the B-DAT
underlying O
patterns O
of O
gene O

interactions B-DAT
by O
integrat- O
ing O
heterogeneous O

data B-DAT
and O
extracts O
features O
that O

are B-DAT
more O
informative O
for O
interaction O

prediction. B-DAT
Experimental O
results O
show O
that O

GNE B-DAT
achieve O
better O
performance O
in O

gene B-DAT
interaction O
prediction O
over O
other O

baseline B-DAT
approaches O
in O
both O
yeast O

and B-DAT
E. O
coli O
organisms. O
Also O

, B-DAT
GNE O
can O
help O
the O

curator B-DAT
to O
identify O
the O
interactions O

that B-DAT
need O
to O
be O
looked O

at. B-DAT
As O
future O
work, O
we O
aim O

to B-DAT
study O
the O
impact O
of O

inte- B-DAT
grating O
other O
sources O
of O

information B-DAT
about O
gene O
such O
as O

transcription B-DAT
factor O
binding O
sites, O
functional O

annota- B-DAT
tions O
(from O
gene O
ontology O

), B-DAT
gene O
sequences, O
metabolic O
pathways, O

etc. B-DAT
into O
GNE O
in O
predicting O

gene B-DAT
interaction. O
Additional O
files O

Additional B-DAT
file O
1: O
Table O
S1. O

Includes B-DAT
yeast O
gene O
pairs O
predicted O

by B-DAT
GNE O
with O
probabilities O
of O
0 O

.5 B-DAT
or O
higher. O
(XLSX O
109 O

kb) B-DAT
Additional O
file O
2: O
Table O
S2 O

. B-DAT
Includes O
E. O
coli O
gene O

pairs B-DAT
predicted O
by O
GNE O
with O

probabilities B-DAT
of O
0.5 O
or O
higher. O

Rows B-DAT
marked O
with O
yellow O
color O

indicate B-DAT
predicted O
interaction O
is O
true O

based B-DAT
on O
latest O
version O
3.4.162 O

of B-DAT
BioGRID O
interaction O
dataset O
released O

on B-DAT
June O
2018. O
(XLSX O
43.7 O

kb) B-DAT
Abbreviations O
AUPR: O
Area O
under O
the O

PR B-DAT
curve; O
AUROC: O
Area O
under O

the B-DAT
ROC O
curve; O
DOOR: O
Database O

of B-DAT
Prokaryotic O
Operons; O
DREAM: O
Dialogue O

on B-DAT
reverse O
Engineering O
assessment O
and O

methods; B-DAT
ELU: O
Exponential O
linear O
unit O

; B-DAT
GI: O
Genetic O
interaction; O
GNE: O

Gene B-DAT
network O
embedding; O
ID: O
Identifier; O

KS: B-DAT
Kolmogrov-Smirnov; O
LINE: O
Large-scale O
information O

network B-DAT
embedding; O
PR: O
Precision O
recall; O

ROC: B-DAT
Receiver O
operator O
characterstic; O
t-SNE: O

t-Distributed B-DAT
stochastic O
neighbor O
embedding O
Acknowledgements O
Not O
applicable O

. B-DAT
Funding O
This O
work O
was O
supported O

by B-DAT
the O
National O
Science O
Foundation O

[1062422 B-DAT
to O
A.H.] O
and O
the O

National B-DAT
Institutes O
of O
Health O
[R15GM116102 O

to B-DAT
F.C.]. O
Publication O
of O
this O

article B-DAT
was O
sponsored O
by O
NSF O

grant B-DAT
(1062422 O

). B-DAT
Availability O
of O
data O
and O
materials O

The B-DAT
datasets O
analysed O
during O
the O

current B-DAT
study O
are O
publicly O
available O

. B-DAT
The O
gene O
expression O
data O

was B-DAT
downloaded O
from O
DREAM5 O
Network O

Challenge B-DAT
https:// O
www.synapse.org/#!Synapse:syn2787209/wiki/70351. O
The O
interaction O

datasets B-DAT
for O
yeast O
and O
E. O

coli B-DAT
were O
downloaded O
from O
BioGRID O

https://thebiogrid.org. B-DAT
About O
this O
supplement O
This O
article O

has B-DAT
been O
published O
as O
part O

of B-DAT
BMC O
Systems O
Biology O
Volume O

13 B-DAT
Supplement O
2, O
2019: O
Selected O

articles B-DAT
from O
the O
17th O
Asia O

Pacific B-DAT
Bioinformatics O
Conference O
(APBC O
2019 O

): B-DAT
systems O
biology. O
The O
full O

contents B-DAT
of O
the O
supplement O
are O

available B-DAT
online O
at O
https://bmcsystbiol.biomedcentral.com/articles/ O
supplements/volume-13-supplement-2. O
Authors’ O
contributions O
KK, O
RL, O
FC O

, B-DAT
and O
AH O
designed O
the O

research. B-DAT
KK O
performed O
the O
research O

and B-DAT
wrote O
the O
manuscript. O
RL, O

FC, B-DAT
QY, O
and O
AH O
improved O

the B-DAT
draft. O
All O
authors O
read O

and B-DAT
approved O
the O
final O
manuscript. O
Ethics O
approval O
and O
consent O
to O

participate B-DAT
Not O
applicable O

. B-DAT
Consent O
for O
publication O
Not O
applicable O

. B-DAT
Competing O
interests O
The O
authors O
declare O

that B-DAT
they O
have O
no O
competing O

interests B-DAT

. B-DAT
Publisher’s O
Note O
Springer O
Nature O
remains O

neutral B-DAT
with O
regard O
to O
jurisdictional O

claims B-DAT
in O
published O
maps O
and O

institutional B-DAT
affiliations O

. B-DAT
Author O
details O
1Golisano O
College O
of O

Computing B-DAT
and O
Information O
Sciences, O
Rochester O

Institute B-DAT
of O
Technology, O
20 O
Lomb O

Memorial B-DAT
Drive, O
14623 O
Rochester, O
New O

York, B-DAT
USA. O
2Thomas O
H. O
Gosnell O

School B-DAT
of O
Life O
Sciences, O
Rochester O

Institute B-DAT
of O
Technology, O
84 O
Lomb O

Memorial B-DAT
Drive, O
14623 O
Rochester, O
New O

York, B-DAT
USA O

. B-DAT
Published: O
5 O
April O
2019 O

https://doi.org/10.1186/s12918-019-0694-y B-DAT
https://doi.org/10.1186/s12918-019-0694-y O
https://www.synapse.org/#!Synapse:syn2787209/wiki/70351 O
https://www.synapse.org/#!Synapse:syn2787209/wiki/70351 O
https://thebiogrid.org O

https://bmcsystbiol.biomedcentral.com/articles/supplements/volume-13-supplement-2 B-DAT

13 B-DAT

2 B-DAT

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
14 O
of O
14 O

References B-DAT
1. O
Mani O
R, O
Onge O

RPS, B-DAT
Hartman O
JL, O
Giaever O
G, O

Roth B-DAT
FP. O
Defining O
genetic O
interaction. O
Proc O
Natl O
Acad O
Sci O

. B-DAT
2008;105(9):3461–6. O
2. O
Boucher O
B, O

Jenna B-DAT
S. O
Genetic O
interaction O
networks: O

better B-DAT
understand O
to O
better O
predict. O
Front O
Genet. O
2013;4:290 O

. B-DAT
3. O
Lage O
K. O
Protein–protein O

interactions B-DAT
and O
genetic O
diseases: O
the O
interactome. O
Biochim O
Biophys O
Acta O
(BBA O

) B-DAT
- O
Mol O
Basis O
Dis. O
2014 O

;1842(10): B-DAT
1971–80. O
4. O
Madhukar O
NS, O
Elemento O
O O

, B-DAT
Pandey O
G. O
Prediction O
of O

genetic B-DAT
interactions O
using O
machine O
learning O

and B-DAT
network O
properties. O
Front O
Bioeng O

Biotechnol. B-DAT
2015;3:172. O
5. O
Oliver O
S. O
Proteomics: O
guilt-by-association O

goes B-DAT
global. O
Nature. O
2000;403(6770):601 O

. B-DAT
6. O
Cho O
H, O
Berger O
B O

, B-DAT
Peng O
J. O
Compact O
integration O

of B-DAT
multi-network O
topology O
for O
functional O

analysis B-DAT
of O
genes. O
Cell O
Syst. O
2016 O

;3(6):540–8. B-DAT
7. O
Marbach O
D, O
Costello O
JC O

, B-DAT
Küffner O
R, O
Vega O
NM, O

Prill B-DAT
RJ, O
Camacho O
DM, O
Allison O

KR, B-DAT
Aderhold O
A, O
Bonneau O
R, O

Chen B-DAT
Y, O
et O
al. O
Wisdom O

of B-DAT
crowds O
for O
robust O
gene O

network B-DAT
inference. O
Nat O
Methods. O
2012;9(8):796. O
8. O
Li O
R, O
KC O
K O

, B-DAT
Cui O
F, O
Haake O
AR. O

Sparse B-DAT
covariance O
modeling O
in O
high O

dimensions B-DAT
with O
gaussian O
processes. O
In: O

Proceedings B-DAT
of O
The O
32nd O
Conference O

on B-DAT
Neural O
Information O
Processing O
Systems O
( O

NIPS). B-DAT
2018. O
9. O
Cui O
P, O
Wang O
X O

, B-DAT
Pei O
J, O
Zhu O
W. O

A B-DAT
survey O
on O
network O
embedding. O

IEEE B-DAT
Trans O
Knowl O
Data O
Eng. O
2018 O

. B-DAT
arXiv O
preprint O
arXiv:1711.08752. O
IEEE. O
10. O
Lei O
Y-K, O
You O
Z-H O

, B-DAT
Ji O
Z, O
Zhu O
L, O

Huang B-DAT
D-S. O
Assessing O
and O
predicting O

protein B-DAT
interactions O
by O
combining O
manifold O

embedding B-DAT
with O
multiple O
information O
integration. O

In: B-DAT
BMC O
Bioinformatics, O
vol. O
13. O

BioMed B-DAT
Central; O
2012. O
p. O
3. O
11. O
Alanis-Lobato O
G, O
Cannistraci O
CV O

, B-DAT
Ravasi O
T. O
Exploitation O
of O

genetic B-DAT
interaction O
network O
topology O
for O

the B-DAT
prediction O
of O
epistatic O
behavior. O

Genomics. B-DAT
2013;102(4):202–8. O
12. O
Tenenbaum O
JB, O
De O
Silva O

V, B-DAT
Langford O
JC. O
A O
global O

geometric B-DAT
framework O
for O
nonlinear O
dimensionality O

reduction. B-DAT
science. O
2000;290(5500):2319–23 O

. B-DAT
13. O
Mikolov O
T, O
Sutskever O
I O

, B-DAT
Chen O
K, O
Corrado O
GS, O

Dean B-DAT
J. O
Distributed O
representations O
of O

words B-DAT
and O
phrases O
and O
their O

compositionality. B-DAT
In: O
Advances O
in O
Neural O

Information B-DAT
Processing O
Systems. O
2013. O
p. O
3111 O

–3119. B-DAT
14. O
Grover O
A, O
Leskovec O
J O

. B-DAT
node2vec: O
Scalable O
feature O
learning O

for B-DAT
networks. O
In: O
Proceedings O
of O

the B-DAT
22nd O
ACM O
SIGKDD O
International O

Conference B-DAT
on O
Knowledge O
Discovery O
and O

Data B-DAT
Mining. O
ACM; O
2016. O
p. O
855 O

–864. B-DAT
15. O
Tu O
Y, O
Stolovitzky O
G O

, B-DAT
Klein O
U. O
Quantitative O
noise O

analysis B-DAT
for O
gene O
expression O
microarray O

experiments. B-DAT
Proc O
Natl O
Acad O
Sci. O
2002 O

;99(22): B-DAT
14031–6. O
16. O
Tang O
J, O
Qu O
M O

, B-DAT
Wang O
M, O
Zhang O
M, O

Yan B-DAT
J, O
Mei O
Q. O
Line: O

Large-scale B-DAT
information O
network O
embedding. O
In: O

Proceedings B-DAT
of O
the O
24th O
International O

Conference B-DAT
on O
World O
Wide O
Web. O

International B-DAT
World O
Wide O
Web O
Conferences O

Steering B-DAT
Committee; O
2015. O
p. O
1067–1077. O
17. O
Snoek O
CG, O
Worring O
M O

, B-DAT
Smeulders O
AW. O
Early O
versus O

late B-DAT
fusion O
in O
semantic O
video O

analysis. B-DAT
In: O
Proceedings O
of O
the O

13th B-DAT
Annual O
ACM O
International O
Conference O

on B-DAT
Multimedia. O
2005. O
p. O
399–402. O

ACM. B-DAT
18. O
He O
K, O
Zhang O
X O

, B-DAT
Ren O
S, O
Sun O
J. O

Deep B-DAT
residual O
learning O
for O
image O

recognition. B-DAT
In: O
Proceedings O
of O
the O

IEEE B-DAT
Conference O
on O
Computer O
Vision O

and B-DAT
Pattern O
Recognition. O
2016. O
p. O
770 O

–778. B-DAT
19. O
Pennington O
J, O
Socher O
R O

, B-DAT
Manning O
C. O
Glove: O
Global O

vectors B-DAT
for O
word O
representation. O
In: O

Proceedings B-DAT
of O
the O
2014 O
Conference O

on B-DAT
Empirical O
Methods O
in O
Natural O

Language B-DAT
Processing O
(EMNLP). O
2014. O
p. O
1532 O

–1543. B-DAT
20. O
Levy O
O, O
Goldberg O
Y O

, B-DAT
Dagan O
I. O
Improving O
distributional O

similarity B-DAT
with O
lessons O
learned O
from O

word B-DAT
embeddings. O
Trans O
Assoc O
Comput O

Linguist. B-DAT
2015;3:211–25. O
21. O
Kingma O
DP, O
Ba O
J O

. B-DAT
Adam: O
A O
method O
for O

stochastic B-DAT
optimization. O
In: O
International O
Conference O

on B-DAT
Learning O
Representations; O
2015. O
22. O
Duchi O
J, O
Hazan O
E O

, B-DAT
Singer O
Y. O
Adaptive O
subgradient O

methods B-DAT
for O
online O
learning O
and O

stochastic B-DAT
optimization. O
J O
Mach O
Learn O

Res. B-DAT
2011;12(Jul): O
2121–59. O
23. O
Tieleman O
T, O
Hinton O
G O

. B-DAT
Lecture O
6.5-rmsprop, O
coursera: O
Neural O

networks B-DAT
for O
machine O
learning. O
University O

of B-DAT
Toronto, O
Technical O
Report. O
2012. O
24. O
Srivastava O
N, O
Hinton O
G O

, B-DAT
Krizhevsky O
A, O
Sutskever O
I, O

Salakhutdinov B-DAT
R. O
Dropout: O
A O
simple O

way B-DAT
to O
prevent O
neural O
networks O

from B-DAT
overfitting. O
J O
Mach O
Learn O

Res. B-DAT
2014;15(1):1929–58. O
25. O
Ioffe O
S, O
Szegedy O
C O

. B-DAT
Batch O
normalization: O
Accelerating O
deep O

network B-DAT
training O
by O
reducing O
internal O

covariate B-DAT
shift. O
In: O
International O
Conference O

on B-DAT
Machine O
Learning; O
2015. O
p. O
448 O

–56. B-DAT
26. O
Stark O
C, O
Breitkreutz O
B-J O

, B-DAT
Reguly O
T, O
Boucher O
L, O

Breitkreutz B-DAT
A, O
Tyers O
M. O
Biogrid: O

a B-DAT
general O
repository O
for O
interaction O

datasets. B-DAT
Nucleic O
Acids O
Res. O
2006;34(suppl_1):535–9. O
27. O
Butte O
A-J, O
Kohane O
I-S O

. B-DAT
Mutual O
information O
relevance O
networks: O

functional B-DAT
genomic O
clustering O
using O
pairwise O

entropy B-DAT
measurements. O
In: O
Biocomputing O
2000. O

World B-DAT
Scientific; O
1999. O
p. O
418–429. O
28. O
Chua O
HN, O
Sung O
W-K O

, B-DAT
Wong O
L. O
Exploiting O
indirect O

neighbours B-DAT
and O
topological O
weight O
to O

predict B-DAT
protein O
function O
from O
protein–protein O

interactions. B-DAT
Bioinformatics. O
2006;22(13):1623–30. O
29. O
Abadi O
M, O
Agarwal O
A O

, B-DAT
Barham O
P, O
Brevdo O
E, O

Chen B-DAT
Z, O
Citro O
C, O
Corrado O

GS, B-DAT
Davis O
A, O
Dean O
J, O

Devin B-DAT
M, O
Ghemawat O
S, O
Goodfellow O

I, B-DAT
Harp O
A, O
Irving O
G, O

Isard B-DAT
M, O
Jia O
Y, O
Jozefowicz O

R, B-DAT
Kaiser O
L, O
Kudlur O
M, O

Levenberg B-DAT
J, O
Mané O
D., O
Monga O

R, B-DAT
Moore O
S, O
Murray O
D, O

Olah B-DAT
C, O
Schuster O
M, O
Shlens O

J, B-DAT
Steiner O
B, O
Sutskever O
I, O

Talwar B-DAT
K, O
Tucker O
P, O
Vanhoucke O

V, B-DAT
Vasudevan O
V, O
Viégas O
F., O

Vinyals B-DAT
O, O
Warden O
P, O
Wattenberg O

M, B-DAT
Wicke O
M, O
Yu O
Y, O

Zheng B-DAT
X. O
TensorFlow: O
Large-Scale O
Machine O

Learning B-DAT
on O
Heterogeneous O
Systems. O
Software O

available B-DAT
from O
tensorflow.org. O
2015. O
https://www.tensorflow.org/ O

Accessed B-DAT
21 O
Dec O
2016. O
30. O
Clevert O
D, O
Unterthiner O
T O

, B-DAT
Hochreiter O
S. O
Fast O
and O

accurate B-DAT
deep O
network O
learning O
by O

exponential B-DAT
linear O
units O
(elus). O
In: O

International B-DAT
Conference O
on O
Learning O
Representations; O
2016 O

. B-DAT
31. O
Davis O
J, O
Goadrich O
M O

. B-DAT
The O
relationship O
between O
precision-recall O

and B-DAT
roc O
curves. O
In: O
Proceedings O

of B-DAT
the O
23rd O
International O
Conference O

on B-DAT
Machine O
Learning. O
ACM; O
2006. O

p. B-DAT
233–240. O
32. O
Maaten O
L. O
v. O
d O

., B-DAT
Hinton O
G. O
Visualizing O
data O

using B-DAT
t-sne. O
J O
Mach O
Learn O

Res. B-DAT
2008;9(Nov):2579–605. O
33. O
Mao O
F, O
Dam O
P O

, B-DAT
Chou O
J, O
Olman O
V, O

Xu B-DAT
Y. O
Door: O
a O
database O

for B-DAT
prokaryotic O
operons. O
Nucleic O
Acids O

Res. B-DAT
2008;37(suppl_1):459–63. O
34. O
Gustin O
MC, O
Albertyn O
J O

, B-DAT
Alexander O
M, O
Davenport O
K. O

Map B-DAT
kinase O
pathways O
in O
the O

yeastsaccharomyces B-DAT
cerevisiae. O
Microbiol O
Mol O
Biol O

Rev. B-DAT
1998;62(4): O
1264–300. O
35. O
Miller O
JE, O
Zhang O
L O

, B-DAT
Jiang O
H, O
Li O
Y, O

Pugh B-DAT
BF, O
Reese O
JC. O
Genome-wide O

mapping B-DAT
of O
decay O
factor–mrna O
interactions O

in B-DAT
yeast O
identifies O
nutrient-responsive O
transcripts O

as B-DAT
targets O
of O
the O
deadenylase O

ccr4. B-DAT
G3: O
Genes, O
Genomes, O
Genet. O
2018 O

;8(1):315–30. B-DAT
36. O
Liang O
J, O
Singh O
N O

, B-DAT
Carlson O
CR, O
Albuquerque O
CP, O

Corbett B-DAT
KD, O
Zhou O
H. O
Recruitment O

of B-DAT
a O
sumo O
isopeptidase O
to O

rdna B-DAT
stabilizes O
silencing O
complexes O
by O

opposing B-DAT
sumo O
targeted O
ubiquitin O
ligase O

activity. B-DAT
Genes O
Dev. O
2017;31(8):802–15. O
37. O
Babu O
M, O
Bundalovic-Torma O
C O

, B-DAT
Calmettes O
C, O
Phanse O
S, O

Zhang B-DAT
Q, O
Jiang O
Y, O
Minic O

Z, B-DAT
Kim O
S, O
Mehla O
J, O

Gagarinova B-DAT
A, O
et O
al. O
Global O

landscape B-DAT
of O
cell O
envelope O
protein O

complexes B-DAT
in O
escherichia O
coli. O
Nat O

Biotechnol. B-DAT
2018;36(1):103. O
https://www.tensorflow.org O

Gene B-DAT
network O
embedding O
(GNE) O

Gene B-DAT
network O
structure O

Gene B-DAT
expression O

GNE B-DAT

Parameter B-DAT

Experimental B-DAT

Results B-DAT
and O

Analysis B-DAT
of O
gene O

Gene B-DAT
interaction O

Impact B-DAT
of O
network O

Impact B-DAT

Investigation B-DAT
of O
GNE's O

Additional B-DAT

Additional B-DAT
file O
1 O

Additional B-DAT
file O
2 O

Availability B-DAT
of O
data O
and O

About B-DAT
this O

Authors' B-DAT

Ethics B-DAT
approval O
and O
consent O
to O

Consent B-DAT
for O

Competing B-DAT

Publisher's B-DAT

Author B-DAT

0.243 O
(AUROC), O
0.242 O
(AUPR) O
on O
yeast B-DAT
and O
0.403 O
(AUROC), O
0.382 O
(AUPR O

comparison O
to O
correlation-based O
methods O
on O
yeast B-DAT
because O
of O
its O
limitation O
on O

integration O
of O
expression O
data O
on O
yeast B-DAT

inter- O
actions O
and O
attributes O
of O
yeast B-DAT
shows O
a O
significant O
gain O
of O

improvement O
on O
E. O
coli O
than O
yeast B-DAT
when O
we O
train O
with O
10 O

number O
of O
available O
interactions O
for O
yeast B-DAT
and O
E. O
coli O
(Table O
2 O

are O
less O
than O
40% O
for O
yeast B-DAT
and O
about O
0.32 O
when O
interactions O

and O
attributes O
are O
required O
for O
yeast B-DAT
than O
E. O
coli. O
It O
may O

related O
to O
the O
fact O
that O
yeast B-DAT
is O
a O
more O
complex O
species O

respect O
to O
network O
sparsity. O
a O
yeast B-DAT
b O
E. O
coli. O
Integration O
of O

different O
percentages O
of O
interactions. O
a O
yeast B-DAT
b O
E. O
coli. O
Different O
lines O

and O
AUPR O
(c, O
d) O
for O
yeast B-DAT
and O
E. O
coli. O
The O
limit O

contains O
12,835 O
new O
interactions O
for O
yeast B-DAT
and O
11,185 O
new O
interactions O
for O

interactions O
are O
comparable O
for O
both O
yeast B-DAT
and O
E. O
coli O
(Figs. O
4 O

temporal O
holdout O
validation O
across O
both O
yeast B-DAT
and O
E. O
coli O
datasets, O
indicating O

7.0 O
(AUROC), O
7.4 O
(AUPR) O
on O
yeast B-DAT
and O
6.6 O
(AUROC), O
5.9 O
(AUPR O

in O
predicted O
probability O
for O
both O
yeast B-DAT
and O
E. O
coli O
after O
expression O

out O
of O
12,835) O
interactions O
for O
yeast B-DAT
(improving O
AUROC O
from O
0.685 O
to O

version O
3.4.162) O
and O
evaluate O
2609 O
yeast B-DAT
gene O
pairs O
(Additional O
file O
1 O

We O
find O
that O
128 O
(5%) O
yeast B-DAT
gene O
pairs O
and O
78 O
(9 O

7). O
Sixteen O
percent O
of O
predicted O
yeast B-DAT
gene O
pairs O
and O
17.5 O

with O
different O
probability O
bins. O
a O
yeast B-DAT
b O
E. O
coli. O
We O
divide O

the O
predicted O
gene O
pairs O
in O
yeast B-DAT
and O
E. O
coli O
(Additional O
file O

82 O
common O
interacting O
genes O
for O
yeast B-DAT
and O
58 O
for O
E. O
coli O

pairs O
predicted O
by O
GNE. O
a O
yeast B-DAT
b O
E. O
coli. O
We O
divide O

p-value O
for O
yeast B-DAT

other O
baseline O
approaches O
in O
both O
yeast B-DAT
and O
E. O
coli O
organisms. O
Also O

file O
1: O
Table O
S1. O
Includes O
yeast B-DAT
gene O
pairs O
predicted O
by O
GNE O

www.synapse.org/#!Synapse:syn2787209/wiki/70351. O
The O
interaction O
datasets O
for O
yeast B-DAT
and O
E. O
coli O
were O
downloaded O

of O
decay O
factor–mrna O
interactions O
in O
yeast B-DAT
identifies O
nutrient-responsive O
transcripts O
as O
targets O

interaction O
network O
data O
from O
the O
BioGRID B-DAT
database O
[26] O
and O
gene O
expression O

use O
two O
interaction O
datasets O
from O
BioGRID B-DAT
database O
(2017 O
released O
version O
3.4.153 O

of O
the O
interaction O
datasets O
from O
BioGRID B-DAT
and O
the O
gene O
expression O
data O

two O
versions O
of O
BioGRID B-DAT
interaction O
datasets O
at O
two O
dif O

from O
the O
2018 O
version O
of O
BioGRID B-DAT
as O
positive O
interactions O
and O
the O

but O
are O
miss- O
ing O
from O
BioGRID B-DAT
for O
further O
investigation O
as O
we O

evidence O
code O
obtained O
from O
the O
BioGRID B-DAT
database O
[26] O
sup- O
porting O
these O

predictions. O
BioGRID B-DAT
compiles O
interaction O
data O
from O
numerous O

Taking O
new O
interactions O
added O
to O
BioGRID B-DAT
(version O
3.4.158) O
into O
consideration, O
we O

consider O
the O
new O
version O
of O
BioGRID B-DAT
(version O
3.4.162) O
and O
evaluate O
2609 O

the O
lat- O
est O
release O
of O
BioGRID B-DAT

are O
missing O
from O
the O
latest O
BioGRID B-DAT
release. O
We O
find O
that O
these O

the O
probability O
of O
0.968. O
Mining O
BioGRID B-DAT
database, O
we O
find O
that O
these O

examining O
protein-protein O
interactions O
[34]. O
Also, O
BioGRID B-DAT
has O
evidence O
of O
30 O
physi O

on O
latest O
version O
3.4.162 O
of O
BioGRID B-DAT
interaction O
dataset O
released O
on O
June O

E. O
coli O
were O
downloaded O
from O
BioGRID B-DAT
https://thebiogrid.org O

GNE: B-DAT
a O
deep O
learning O
framework O

for B-DAT
gene O
network O
inference O
by O

aggregating B-DAT
biological O

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
https://doi.org/10.1186/s12918-019-0694-y O
RESEARCH O
Open O
Access O

GNE: B-DAT
a O
deep O
learning O
framework O

for B-DAT
gene O
network O
inference O
by O

aggregating B-DAT
biological O
information O
Kishan O
KC1*, O

Rui B-DAT
Li1, O
Feng O
Cui2, O
Qi O

Yu1 B-DAT
and O
Anne O
R. O
Haake1 O
From O
The O
17th O
Asia O
Pacific O

Bioinformatics B-DAT
Conference O
(APBC O
2019) O
Wuhan O

, B-DAT
China. O
14–16 O
January O
2019 O
Abstract O

Background: B-DAT
The O
topological O
landscape O
of O

gene B-DAT
interaction O
networks O
provides O
a O

rich B-DAT
source O
of O
information O
for O

inferring B-DAT
functional O
patterns O
of O
genes O

or B-DAT
proteins. O
However, O
it O
is O

still B-DAT
a O
challenging O
task O
to O

aggregate B-DAT
heterogeneous O
biological O
information O
such O

as B-DAT
gene O
expression O
and O
gene O

interactions B-DAT
to O
achieve O
more O
accurate O

inference B-DAT
for O
prediction O
and O
discovery O

of B-DAT
new O
gene O
interactions. O
In O

particular, B-DAT
how O
to O
generate O
a O

unified B-DAT
vector O
representation O
to O
integrate O

diverse B-DAT
input O
data O
is O
a O

key B-DAT
challenge O
addressed O
here. O
Results: O
We O
propose O
a O
scalable O

and B-DAT
robust O
deep O
learning O
framework O

to B-DAT
learn O
embedded O
representations O
to O

unify B-DAT
known O
gene O
interactions O
and O

gene B-DAT
expression O
for O
gene O
interaction O

predictions. B-DAT
These O
low- O
dimensional O
embeddings O

derive B-DAT
deeper O
insights O
into O
the O

structure B-DAT
of O
rapidly O
accumulating O
and O

diverse B-DAT
gene O
interaction O
networks O
and O

greatly B-DAT
simplify O
downstream O
modeling. O
We O

compare B-DAT
the O
predictive O
power O
of O

our B-DAT
deep O
embeddings O
to O
the O

strong B-DAT
baselines. O
The O
results O
suggest O

that B-DAT
our O
deep O
embeddings O
achieve O

significantly B-DAT
more O
accurate O
predictions. O
Moreover O

, B-DAT
a O
set O
of O
novel O

gene B-DAT
interaction O
predictions O
are O
validated O

by B-DAT
up-to-date O
literature-based O
database O
entries. O
Conclusion: O
The O
proposed O
model O
demonstrates O

the B-DAT
importance O
of O
integrating O
heterogeneous O

information B-DAT
about O
genes O
for O
gene O

network B-DAT
inference. O
GNE O
is O
freely O

available B-DAT
under O
the O
GNU O
General O

Public B-DAT
License O
and O
can O
be O

downloaded B-DAT
from O
GitHub O
(https://github.com/kckishan/GNE O

). B-DAT
Keywords: O
Gene O
interaction O
networks, O
Gene O

expression, B-DAT
Network O
embedding, O
Heterogeneous O
data O

integration, B-DAT
Deep O
learning O

Background B-DAT
A O
comprehensive O
study O
of O

gene B-DAT
interactions O
(GIs) O
provides O
means O

to B-DAT
identify O
the O
functional O
relationship O

between B-DAT
genes O
and O
their O
corresponding O

products, B-DAT
as O
well O
as O
insights O

into B-DAT
underlying O
biological O
phenomena O
that O

are B-DAT
critical O
to O
understanding O
phenotypes O

in B-DAT
health O
and O
dis- O
ease O

conditions B-DAT
[1–3]. O
Since O
advancements O
in O

measure- B-DAT
ment O
technologies O
have O
led O

to B-DAT
numerous O
high-throughput O
datasets, O
there O

is B-DAT
a O
great O
value O
in O

developing B-DAT
efficient O
com- O
putational O
methods O

capable B-DAT
of O
automatically O
extracting O
*Correspondence: O
kk3671@rit.edu O
1Golisano O
College O
of O

Computing B-DAT
and O
Information O
Sciences, O
Rochester O

Institute B-DAT
of O
Technology, O
20 O
Lomb O

Memorial B-DAT
Drive, O
14623 O
Rochester, O
New O

York, B-DAT
USA O
Full O
list O
of O

author B-DAT
information O
is O
available O
at O

the B-DAT
end O
of O
the O
article O

and B-DAT
aggregating O
meaningful O
information O
from O

heteroge- B-DAT
neous O
datasets O
to O
infer O

gene B-DAT
interactions. O
Although O
a O
wide O
variety O
of O

machine B-DAT
learning O
models O
have O
been O

developed B-DAT
to O
analyze O
high-throughput O
datasets O

for B-DAT
GI O
prediction O
[4], O
there O

are B-DAT
still O
some O
major O
chal O

- B-DAT
lenges, O
such O
as O
efficient O

analysis B-DAT
of O
large O
heterogeneous O
datasets, O

integration B-DAT
of O
biological O
information, O
and O

effec- B-DAT
tive O
feature O
engineering. O
To O

address B-DAT
these O
challenges, O
we O
propose O

a B-DAT
novel O
deep O
learning O
framework O

to B-DAT
integrate O
diverse O
biological O
information O

for B-DAT
GI O
network O
inference. O
Our O
proposed O
method O
frames O
GI O

network B-DAT
inference O
as O
a O
problem O

of B-DAT
network O
embedding. O
In O
particular O

, B-DAT
we O
rep- O
resent O
gene O

interactions B-DAT
as O
a O
network O
of O

genes B-DAT
and O
their O
interactions O
and O

create B-DAT
a O
deep O
learning O
framework O

to B-DAT
automatically O
learn O
an O
informative O

representation B-DAT
which O
© O
The O
Author(s). O
2019 O
Open O

Access B-DAT
This O
article O
is O
distributed O

under B-DAT
the O
terms O
of O
the O

Creative B-DAT
Commons O
Attribution O
4.0 O
International O

License B-DAT
(http://creativecommons.org/licenses/by/4.0/), O
which O
permits O
unrestricted O

use, B-DAT
distribution, O
and O
reproduction O
in O

any B-DAT
medium, O
provided O
you O
give O

appropriate B-DAT
credit O
to O
the O
original O

author(s) B-DAT
and O
the O
source, O
provide O

a B-DAT
link O
to O
the O
Creative O

Commons B-DAT
license, O
and O
indicate O
if O

changes B-DAT
were O
made. O
The O
Creative O

Commons B-DAT
Public O
Domain O
Dedication O
waiver O

(http://creativecommons.org/publicdomain/zero/1.0/) B-DAT
applies O
to O
the O
data O

made B-DAT
available O
in O
this O
article O

, B-DAT
unless O
otherwise O
stated. O
http://crossmark.crossref.org/dialog/?doi=10.1186/s12918-019-0694-y&domain=pdf O
https://github.com/kckishan/GNE O
mailto: O
kk3671@rit.edu O
http://creativecommons.org/licenses/by/4.0 O

/ B-DAT

1 B-DAT

0 B-DAT

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
2 O
of O
14 O

integrates B-DAT
both O
the O
topological O
property O

and B-DAT
the O
gene O
expression O
property. O

A B-DAT
key O
insight O
behind O
our O

gene B-DAT
net- O
work O
embedding O
method O

is B-DAT
the O
“guilt O
by O
association” O

assumption B-DAT
[5], O
that O
is, O
genes O

that B-DAT
are O
co-localized O
or O
have O

similar B-DAT
topological O
roles O
in O
the O

interaction B-DAT
net- O
work O
are O
likely O

to B-DAT
be O
functionally O
correlated. O
This O

insight B-DAT
not O
only O
allows O
us O

to B-DAT
discover O
similar O
genes O
and O

pro- B-DAT
teins O
but O
also O
to O

infer B-DAT
the O
properties O
of O
unknown O

ones. B-DAT
Our O
network O
embedding O
generates O

a B-DAT
lower-dimensional O
vector O
representation O
of O

the B-DAT
gene O
topological O
character- O
istics. O

The B-DAT
relationships O
between O
genes O
including O

higher- B-DAT
order O
topological O
properties O
are O

captured B-DAT
by O
the O
distances O
between O

genes B-DAT
in O
the O
embedding O
space. O

The B-DAT
new O
low- O
dimensional O
representation O

of B-DAT
a O
GI O
network O
can O

be B-DAT
used O
for O
various O
downstream O

tasks, B-DAT
such O
as O
gene O
function O

pre- B-DAT
diction, O
gene O
interaction O
prediction, O

and B-DAT
gene O
ontology O
reconstruction O
[6]. O
Furthermore, O
since O
the O
network O
embedding O

method B-DAT
can O
only O
preserve O
the O

topological B-DAT
properties O
of O
a O
GI O

network, B-DAT
and O
fails O
to O
generalize O

for B-DAT
genes O
with O
no O
interaction O

infor- B-DAT
mation, O
our O
scalable O
deep O

learning B-DAT
method O
also O
integrates O
heterogeneous O

gene B-DAT
information, O
such O
as O
expression O

data B-DAT
from O
high O
throughput O
technologies O

, B-DAT
into O
the O
GI O
net- O

work B-DAT
inference. O
Our O
method O
projects O

genes B-DAT
with O
similar O
attributes O
closer O

to B-DAT
each O
other O
in O
the O

embedding B-DAT
space, O
even O
if O
they O

may B-DAT
not O
have O
similar O
topological O

properties. B-DAT
The O
results O
show O
that O

by B-DAT
integrating O
additional O
gene O
infor- O

mation B-DAT
in O
the O
network O
embedding O

process, B-DAT
the O
prediction O
performance O
is O

improved B-DAT
significantly. O
GI O
prediction O
is O
a O
long-standing O

problem. B-DAT
The O
pro- O
posed O
machine O

learning B-DAT
methods O
include O
statistical O
corre O

- B-DAT
lation, O
mutual O
information O
[7], O

dimensionality B-DAT
reduction O
[8], O
and O
network-based O

methods B-DAT
(e.g. O
common O
neighbor- O
hood, O

network B-DAT
embedding) O
[4, O
9]. O
Among O

these B-DAT
methods, O
some O
methods O
such O

as B-DAT
statistical O
correlation O
and O
mutual O

information B-DAT
consider O
only O
gene O
expression O

whereas B-DAT
other O
methods O
use O
only O

topological B-DAT
properties O
to O
predict O
GIs. O
Network-based O
methods O
have O
been O
proposed O

to B-DAT
leverage O
topological O
properties O
of O

GI B-DAT
networks O
[10]. O
Neighborhood-based O
methods O

quantify B-DAT
the O
proximity O
between O
genes O

, B-DAT
based O
on O
common O
neighbors O

in B-DAT
GI O
network O
[11]. O
The O

proximity B-DAT
scores O
assigned O
to O
a O

pair B-DAT
of O
genes O
rely O
on O

the B-DAT
number O
of O
neighbors O
that O

the B-DAT
pair O
has O
in O
common. O

Adjacency B-DAT
matrix, O
representing O
the O
interaction O

network, B-DAT
or O
proximity O
matrix, O
obtained O

from B-DAT
neighborhood-based O
methods, O
are O
processed O

with B-DAT
network O
embedding O
methods O
to O

learn B-DAT
embeddings O
that O
preserve O
the O

structural B-DAT
properties O
of O
the O
network. O

Structure-preserving B-DAT
network O
embedding O
methods O
such O

as B-DAT
Isomap O
[12] O
are O
proposed O

as B-DAT
a O
dimensionality O
reduc- O
tion O

technique. B-DAT
Since O
the O
goal O
of O

these B-DAT
methods O
is O
solely O
for O

graph B-DAT
reconstruction, O
the O
embedding O
space O

may B-DAT
not O
be O
suitable O
for O

GI B-DAT
network O
inference. O
Besides, O
these O

meth- B-DAT
ods O
construct O
the O
graphs O

from B-DAT
the O
data O
features O
where O
proximity O
between O
genes O
is O
well O

defined B-DAT
in O
the O
original O
feature O

space B-DAT
[9]. O
On O
the O
other O

hand, B-DAT
in O
GI O
networks, O
gene O

proximities B-DAT
are O
not O
explicitly O
defined O

, B-DAT
and O
they O
depend O
on O

the B-DAT
specific O
analytic O
tasks O
and O

application B-DAT
scenarios. O
Our O
deep O
learning O
method O
allows O

incorporating B-DAT
gene O
expression O
data O
with O

GI B-DAT
network O
topological O
structure O
information O

to B-DAT
preserve O
both O
topological O
and O

attribute B-DAT
proximity O
in O
the O
low-dimensional O

representation B-DAT
for O
GI O
predictions. O
Moreover O

, B-DAT
the O
scalable O
architecture O
enables O

us B-DAT
to O
incorporate O
additional O
attributes. O

Topological B-DAT
prop- O
erties O
of O
GI O

network B-DAT
and O
expression O
profiles O
are O

trans- B-DAT
formed O
into O
two O
separate O

embeddings: B-DAT
ID O
embedding O
(which O
preserves O

the B-DAT
topological O
structure O
proximity) O
and O

attribute B-DAT
embedding O
(which O
preserves O
the O

attribute B-DAT
prox- O
imity) O
respectively. O
With O

a B-DAT
multilayer O
neural O
network, O
we O

then B-DAT
aggregate O
the O
complex O
statistical O

relationships B-DAT
between O
topology O
and O
attribute O

information B-DAT
to O
improve O
GI O
predictions. O
In O
summary, O
our O
contributions O
are O

as B-DAT
follows O

: B-DAT
• O
We O
propose O
a O
novel O

deep B-DAT
learning O
framework O
to O
learn O

lower B-DAT
dimensional O
representations O
while O
preserving O

topological B-DAT
and O
attribute O
proximity O
of O

GI B-DAT
networks O

. B-DAT
• O
We O
evaluate O
the O
prediction O

performance B-DAT
on O
the O
datasets O
of O

two B-DAT
organisms O
based O
on O
the O

embedded B-DAT
representation O
and O
achieve O
significantly O

better B-DAT
predictions O
than O
the O
strong O

baselines B-DAT

. B-DAT
• O
Our O
method O
can O
predict O

new B-DAT
gene O
interactions O
which O
are O

validated B-DAT
on O
an O
up-to-date O
GI O

database B-DAT

. B-DAT
Methods O
Preliminaries O
We O
formally O
define O

the B-DAT
problem O
of O
gene O
network O

infer- B-DAT
ence O
as O
a O
network O

embedding B-DAT
problem O
using O
the O
concepts O

of B-DAT
topological O
and O
attribute O
proximity O

as B-DAT
demonstrated O
in O
Fig. O
1 O

. B-DAT
Definition O
1 O
(Gene O
network) O
Gene O

network B-DAT
can O
be O
rep- O
resented O

as B-DAT
a O
network O
structure, O
which O

represents B-DAT
the O
inter- O
actions O
between O

genes B-DAT
within O
an O
organism. O
The O

interaction B-DAT
between O
genes O
corresponds O
to O

either B-DAT
a O
physical O
interaction O
through O

their B-DAT
gene O
products, O
e.g., O
proteins O

, B-DAT
or O
one O
of O
the O

genes B-DAT
alters O
or O
affects O
the O

activity B-DAT
of O
other O
gene O
of O

interest. B-DAT
We O
denote O
gene O
network O

as B-DAT
G O
= O
(V O
, O

E, B-DAT
A) O
where O

V B-DAT
= O
{vi} O
denotes O
genes O

or B-DAT
proteins, O
E O
= O
{eij} O

denotes B-DAT
edges O
that O
correspond O
to O

interactions B-DAT
between O
genes O
vi O
and O

vj, B-DAT
and O
A O
= O
{Ai} O

represents B-DAT
the O
attributes O
of O
gene O

vi. B-DAT
Edge O
eij O
is O
associated O

with B-DAT
a O
weight O
wij O
≥ O
0 O
indicating O
the O
strength O
of O
the O

connection B-DAT
between O
gene O
vi O
and O

vj. B-DAT
If O
gene O
vi O
and O

vj B-DAT
is O
not O
linked O
by O

an B-DAT
edge, O
wij O
= O
0 O

. B-DAT
We O
name O
interactions O
with O

wij B-DAT
> O
0 O
as O
positive O

interactions B-DAT
and O
wij O
= O
0 O

as B-DAT
negative O
interactions. O
In O
this O

paper, B-DAT
we O
consider O
weights O
wij O

to B-DAT
be O
binary, O
indicating O
whether O

genes B-DAT
vi O
and O
vj O
interact O

or B-DAT

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
3 O
of O
14 O

Fig. B-DAT
1 O
An O
illustration O
of O

gene B-DAT
network O
embedding O
(GNE). O
GNE O

integrates B-DAT
gene O
interaction O
network O
and O

gene B-DAT
expression O
data O
to O
learn O

a B-DAT
lower-dimensional O
representation.The O
nodes O
represent O

genes, B-DAT
and O
the O
genes O
with O

the B-DAT
same O
color O
have O
similar O

expression B-DAT
profiles. O
GNE O
groups O
genes O

with B-DAT
similar O
network O
topology, O
which O

are B-DAT
connected O
or O
have O
a O

similar B-DAT
neighborhood O
in O
the O
graph, O

and B-DAT
attribute O
similarity O
(similar O
expression O

profiles) B-DAT
in O
the O
embedded O
space O
Genes O
directly O
connected O
with O
a O

gene B-DAT
vi O
in O
gene O
network O

denote B-DAT
the O
local O
network O
structure O

of B-DAT
gene O
vi. O
We O
define O

local B-DAT
network O
structures O
as O
the O

first-order B-DAT
proximity O
of O
a O
gene O

. B-DAT
Definition O
2 O
(First-order O
proximity) O
The O

first-order B-DAT
proximity O
in O
a O
gene O

network B-DAT
is O
the O
pairwise O
interactions O

between B-DAT
genes. O
Weight O
wij O
indicates O

the B-DAT
first-order O
proxim- O
ity O
between O

gene B-DAT
vi O
and O
vj. O
If O

there B-DAT
is O
no O
interaction O
between O

gene B-DAT
vi O
and O
vj, O
their O

first-order B-DAT
proximity O
wij O
is O
0 O

. B-DAT
Genes O
are O
likely O
to O
be O

involved B-DAT
in O
the O
same O
cellular O

func- B-DAT
tions O
if O
they O
are O

connected B-DAT
in O
the O
gene O
network O

. B-DAT
On O
the O
other O
hand, O

even B-DAT
if O
two O
genes O
are O

not B-DAT
connected, O
they O
may O
be O

still B-DAT
related O
in O
some O
cellular O

functions. B-DAT
This O
indicates O
the O
need O

for B-DAT
an O
additional O
notion O
of O

proximity B-DAT
to O
preserve O
the O
network O

structure. B-DAT
Studies O
suggest O
that O
genes O

that B-DAT
share O
a O
similar O
neighborhood O

are B-DAT
also O
likely O
to O
be O

related B-DAT
[6]. O
Thus, O
we O
introduce O

second-order B-DAT
proximity O
that O
characterizes O
the O

global B-DAT
network O
structure O
of O
the O

genes. B-DAT
Definition O
3 O
(Second-order O
proximity) O
Second O

order B-DAT
proximity O
denotes O
the O
similarity O

between B-DAT
the O
neighbor- O
hood O
of O

genes. B-DAT
Let O
Ni O
= O
{si,1 O

,. B-DAT
. O
. O
, O
si,i−1, O

si,i+1,. B-DAT
. O
. O
, O
si,M−1} O

denotes B-DAT
the O
first-order O
proximity O
of O

gene B-DAT
vi, O
where O
si,j O
is O

wij B-DAT
if O
there O
is O
direct O

connection B-DAT
between O
gene O
vi O
and O

gene B-DAT
vj, O
otherwise O
0. O
Then, O

the B-DAT
second O
order O
proximity O
is O

the B-DAT
sim- O
ilarity O
between O
Ni O

and B-DAT
Nj. O
If O
there O
is O

no B-DAT
path O
to O
reach O
gene O

vi B-DAT
from O
gene O
vj, O
the O

second B-DAT
proximity O
between O
these O
genes O

is B-DAT
0. O
Integrating O
first-order O
and O
second-order O
proximities O

simultaneously B-DAT
can O
help O
to O
preserve O

the B-DAT
topological O
proper- O
ties O
of O

the B-DAT
gene O
network. O
To O
generate O

a B-DAT
more O
comprehensive O
representation O
of O

the B-DAT
genes, O
it O
is O
crucial O

to B-DAT
integrate O
gene O
expression O
data O

as B-DAT
gene O
attributes O
with O
their O

topological B-DAT
properties. O
Besides O
preserving O
topological O

properties, B-DAT
gene O
expression O
provides O
additional O

information B-DAT
to O
predict O
the O
network O

structure B-DAT

. B-DAT
Definition O
4 O
(Attribute O
proximity) O
Attribute O

proxim- B-DAT
ity O
denotes O
the O
similarity O

between B-DAT
the O
expression O
of O
genes O

. B-DAT
We O
thus O
investigate O
both O
topological O

and B-DAT
attribute O
prox- O
imity O
for O

gene B-DAT
network O
embedding, O
which O
is O

defined B-DAT
as O
follows O

: B-DAT
Definition O
5 O
(Gene O
network O
embedding O

) B-DAT
Given O
a O
gene O
network O

denoted B-DAT
as O
G O
= O
(V O
, O
E, O
A), O
gene O
network O
embedding O

aims B-DAT
to O
learn O
a O
function O

f B-DAT
that O
maps O
gene O
network O

structure B-DAT
and O
their O
attribute O
information O

to B-DAT
a O
d- O
dimensional O
space O

where B-DAT
a O
gene O
is O
represented O

by B-DAT
a O
vector O
yi O

∈ B-DAT
Rd O
where O
d O

� B-DAT
M. O
The O
low O
dimensional O

vectors B-DAT
yi O
and O
yj O
for O

genes B-DAT
vi O
and O
vj O
preserve O

their B-DAT
relationships O
in O
terms O
of O

the B-DAT
network O
topological O
structure O
and O

attribute B-DAT
proximity O

. B-DAT
Gene O
network O
embedding O
(GNE) O
model O

Our B-DAT
deep O
learning O
framework O
as O

shown B-DAT
in O
Fig. O
2 O
jointly O

utilizes B-DAT
gene O
network O
structure O
and O

gene B-DAT
expression O
data O
to O
learn O

a B-DAT
unified O
representation O
for O
the O

genes. B-DAT
Embedding O
of O
a O
gene O

network B-DAT
projects O
genes O
into O
a O

lower B-DAT
dimensional O
space, O
known O
as O

the B-DAT
embedding O
space, O
in O
which O

each B-DAT
gene O
is O
represented O
by O

a B-DAT
vector. O
The O
embeddings O
preserve O

both B-DAT
the O
gene O
network O
structure O

and B-DAT
statistical O
relationships O
of O
gene O

expression. B-DAT
We O
list O
the O
variables O

to B-DAT
specify O
our O
framework O
in O

Table B-DAT
1 O

. B-DAT
Gene O
network O
structure O
modeling O
GNE O

framework B-DAT
preserves O
first-order O
and O
second-order O

proximity B-DAT
of O
genes O
in O
the O

gene B-DAT
network. O
The O
key O
idea O

of B-DAT
network O
structure O
modeling O
is O

to B-DAT
estimate O
the O
pairwise O
proximity O

of B-DAT
genes O
in O
terms O
of O

the B-DAT
network O
structure. O
If O
two O

genes B-DAT
are O
connected O
or O
share O

similar B-DAT
neighborhood O
genes, O
they O
tend O

to B-DAT
be O
related O
and O
should O

be B-DAT
placed O
closer O
to O
each O

other B-DAT
in O
the O
embedding O
space O

. B-DAT
Inspired O
by O
the O
Skip-gram O

model B-DAT
[13], O
we O
use O
one O

hot B-DAT
encoded O
represen- O
tation O
to O

represent B-DAT
topological O
information O
of O
a O

gene. B-DAT

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
4 O
of O
14 O

Fig. B-DAT
2 O
Overview O
of O
Gene O

Network B-DAT
Embedding O
(GNE) O
Framework O
for O

gene B-DAT
interaction O
prediction. O
On O
the O

left,one-hot B-DAT
encoded O
representation O
of O
gene O

is B-DAT
encoded O
to O
dense O
vector O

v(s)i B-DAT
of O
dimension O
d O
× O
1 O
which O
captures O
topological O
properties O
and O

expression B-DAT
vector O
of O
gene O
is O

transformed B-DAT
to O
v(a)i O
of O
dimension O

d B-DAT
× O
1 O
which O
aggregates O

the B-DAT
attribute O
information O
(Step O
1 O

). B-DAT
Next, O
concatenation O
of O
two O

embedded B-DAT
vectors O
(creates O
vector O
with O

dimension B-DAT
2d O
× O
1) O
allows O

to B-DAT
combine O
strength O
of O
both O

network B-DAT
structure O
and O
attribute O
modeling. O

Then, B-DAT
nonlinear O
transformation O
of O
concatenated O

vector B-DAT
enables O
GNE O
to O
capture O

complex B-DAT
statistical O
relationships O
between O
network O

structure B-DAT
and O
attribute O
information O
and O

learn B-DAT
better O
representations O
(Step O
2). O

Finally, B-DAT
these O
learned O
representation O
of O

dimension B-DAT
d O
× O
1 O
is O

transformed B-DAT
into O
a O
probability O
vector O

of B-DAT
length O
M O
× O
1 O

in B-DAT
output O
layer, O
which O
contains O

the B-DAT
predictive O
probability O
of O
gene O

vi B-DAT
to O
all O
the O
genes O

in B-DAT
the O
network. O
Conditional O
probability O

p(vj|vi) B-DAT
on O
output O
layer O
indicates O

the B-DAT
likelihood O
that O
gene O
vj O

is B-DAT
connected O
with O
gene O
vi O
( O

Step B-DAT
3) O
gene O
vi O
in O
the O
network O

is B-DAT
represented O
as O
an O
M-dimensional O

vector B-DAT
where O
only O
the O
ith O

component B-DAT
of O
the O
vector O
is O

1 B-DAT

. B-DAT
To O
model O
topological O
similarity, O
we O

define B-DAT
the O
condi- O
tional O
probability O

of B-DAT
gene O
vj O
on O
gene O

vi B-DAT
using O
a O
softmax O
function O

as B-DAT

: B-DAT
p(vj|vi) O
= O
exp(f O
(vi, O
vj))∑M O

j′=1 B-DAT
exp(f O
(vi, O
vj O

′)) B-DAT
(1 O

) B-DAT
Table O
1 O
Terms O
and O
Notations O

Symbol B-DAT
Definitions O
M O
Total O
number O
of O
genes O

in B-DAT
gene O
network O

E B-DAT
Number O
of O
expression O
values O

for B-DAT
each O
gene O
Ni O
Set O
of O
the O
neighbor O

genes B-DAT
of O
gene O
vi O
v(s)i O

Topological B-DAT
representation O
of O
gene O
vi O

v(a)i B-DAT
Attribute O
representation O
of O
gene O

vi B-DAT
ṽi O
Neighborhood O
representation O
of O

gene B-DAT
vi O
vi O
Concatenated O
representation O

of B-DAT
topological O

properties B-DAT
and O
expression O
data O
k O
Number O
of O
hidden O
layers O

to B-DAT
transform O
concatenated O
representation O
into O

embedding B-DAT
space O

h(k) B-DAT
Output O
of O
kth O
hidden O

layer B-DAT
Wk O
Weight O
matrix O
for O
kth O

hidden B-DAT
layer O

Wid B-DAT
Weight O
matrix O
for O
topological O

structure B-DAT
embedding O
Watt O
Weight O
matrix O
for O
attribute O

embedding B-DAT

Wout B-DAT
Weight O
matrix O
for O
output O

layer B-DAT
which O
measures O
the O
likelihood O
of O

gene B-DAT
vi O
being O
connected O
with O

vj. B-DAT
Let O
function O
f O
represents O

the B-DAT
mapping O
of O
two O
genes O

vi B-DAT
and O
vj O
to O
their O

estimated B-DAT
proximity O
score. O
Let O
p(N O

|v) B-DAT
be O
the O
likelihood O
of O

observing B-DAT
a O
neighborhood O
N O
for O

a B-DAT
gene O
v. O
By O
assuming O

conditional B-DAT
independence, O
we O
can O
factorize O

the B-DAT
likelihood O
so O
that O
the O

likelihood B-DAT
of O
observing O
a O
neighborhood O

gene B-DAT
is O
independent O
of O
observ O

- B-DAT
ing O
any O
other O
neighborhood O

gene, B-DAT
given O
a O
gene O
vi: O
p(Ni|vi O

) B-DAT
= O
∏ O
vj∈Ni O
p(vj|vi) O
(2 O

) B-DAT
where O
Ni O
represents O
the O
neighborhood O

genes B-DAT
of O
the O
gene O
vi O

. B-DAT
Global O
structure O
proximity O
for O

a B-DAT
gene O
vi O
can O
be O

pre- B-DAT
served O
by O
maximizing O
the O

conditional B-DAT
probability O
over O
all O
genes O

in B-DAT
the O
neighborhood. O
Hence, O
we O

can B-DAT
define O
the O
likelihood O
function O

that B-DAT
preserve O
global O
structure O
proximity O

as: B-DAT
L O
= O
M O

∏ B-DAT
i=1 O
p(Ni|vi O

) B-DAT
= O
M O

∏ B-DAT
i=1 O

vj∈Ni B-DAT
p(vj|vi) O
(3) O
Let O
v(s)i O
denotes O
the O
dense O

vector B-DAT
generated O
from O
one-hot O
gene O

ID B-DAT
vector, O
which O
represents O
topological O

informa- B-DAT
tion O
of O
that O
gene O

. B-DAT
GNE O
follows O
direct O
encoding O

methods B-DAT
[13, O
14] O
to O
map O

genes B-DAT
to O
vector O
embeddings, O
simply O

known B-DAT
as O
embedding O

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
5 O
of O
14 O

v(s)i B-DAT
= O
Widvi O
(4) O
where O

Wid B-DAT
∈ O
Rd×M O
is O
a O

matrix B-DAT
containing O
the O
embedding O
vectors O

for B-DAT
all O
genes O
and O

vi B-DAT
∈ O
IM O
is O
a O

one-hot B-DAT
indica- O
tor O
vector O
indicating O

the B-DAT
column O
of O
Wid O
corresponding O

to B-DAT
gene O
vi. O
Gene O
expression O
modeling O
GNE O
encodes O

the B-DAT
expression O
data O
from O
microarray O

exper- B-DAT
iments O
to O
the O
dense O

representation B-DAT
using O
a O
non-linear O
transformation O

. B-DAT
The O
amount O
of O
mRNA O

produced B-DAT
during O
transcription O
measured O
over O

a B-DAT
number O
of O
experiments O
helps O

to B-DAT
identify O
similarly O
expressed O
genes. O

Since B-DAT
expres- O
sion O
data O
have O

inherent B-DAT
noise O
[15], O
transforming O
expres- O

sion B-DAT
data O
using O
a O
non-linear O

transformation B-DAT
can O
be O
helpful O
to O

uncover B-DAT
the O
underlying O
representation. O
Let O

xi B-DAT
be O
the O
vector O
of O

expression B-DAT
values O
of O
gene O
vi O

measured B-DAT
over O
E O
experiments. O
Using O

non-linear B-DAT
transformation, O
we O
can O
capture O

the B-DAT
non-linearities O
of O
expression O
data O

of B-DAT
gene O
vi O
as: O
v(a)i O
= O
δa(Watt O
· O
xi O

) B-DAT
(5) O
where O
v(a)i O
represents O

the B-DAT
lower O
dimensional O
attribute O
rep- O

resentation B-DAT
vector O
for O
gene O
vi. O

Watt B-DAT
, O
and O
δa O
represents O

the B-DAT
weight O
matrix, O
and O
activation O

function B-DAT
of O
attribute O
transformation O
layer O

respectively. B-DAT
We O
use O
the O
deep O
model O

to B-DAT
approximate O
the O
attribute O
proximity O

by B-DAT
capturing O
complex O
statistical O
relationships O

between B-DAT
attributes O
and O
introducing O
non-linearities O

, B-DAT
simi- O
lar O
to O
structural O

embedding. B-DAT
GNE O
integration O
GNE O
models O
the O

integration B-DAT
of O
network O
structure O
and O

attribute B-DAT
information O
to O
learn O
more O

comprehensive B-DAT
embeddings O
for O
gene O
networks O

. B-DAT
GNE O
takes O
two O
inputs: O

one B-DAT
for O
topological O
information O
of O

a B-DAT
gene O
as O
one O
hot O

gene B-DAT
ID O
vector O
and O
another O

for B-DAT
its O
expression O
as O
an O

attribute B-DAT
vector. O
Each O
input O
is O

encoded B-DAT
to O
its O
respective O
embed- O

dings. B-DAT
One O
hot O
representation O
for O

a B-DAT
gene O
vi O
is O
projected O

to B-DAT
the O
dense O
vector O
v(s)i O

which B-DAT
captures O
the O
topological O
properties. O

Non-linear B-DAT
transformation O
of O
attribute O
vec- O

tor B-DAT
generates O
compact O
representation O
vector O

v(a)i B-DAT
. O
Previous O
work O
[16] O

combines B-DAT
heterogeneous O
information O
using O
the O

late B-DAT
fusion O
approach. O
However, O
the O

late B-DAT
fusion O
approach O
is O
the O

approach B-DAT
of O
learning O
separate O
models O

for B-DAT
hetero- O
geneous O
information O
and O

integrating B-DAT
the O
representations O
learned O
from O

separate B-DAT
models. O
On O
the O
other O

hand, B-DAT
the O
early O
fusion O
combines O

heterogeneous B-DAT
information O
and O
train O
the O

model B-DAT
on O
combined O
representations O
[17]. O

We B-DAT
thus O
propose O
to O
use O

the B-DAT
early O
fusion O
approach O
to O

combine B-DAT
them O
by O
concatenating. O
As O

a B-DAT
result, O
learning O
from O
topo- O

logical B-DAT
and O
attribute O
information O
can O

complement B-DAT
each O
other, O
allowing O
the O

model B-DAT
to O
learn O
their O
complex O

statistical B-DAT
relationships O
as O
well. O
Embeddings O

from B-DAT
topological O
and O
attribute O
information O

are B-DAT
concatenated O
into O
a O
vector O

as: B-DAT
vi O

v(s)i B-DAT
λv O
(a) O
i O
] O
(6 O

) B-DAT
where O
λ O
is O
the O
importance O

of B-DAT
gene O
expression O
information O
relative O

to B-DAT
topological O
information O

. B-DAT
The O
concatenated O
vectors O
are O
fed O

into B-DAT
a O
multilayer O
perceptron O
with O

k B-DAT
hidden O
layers. O
The O
hidden O

represen- B-DAT
tations O
from O
each O
hidden O

layer B-DAT
in O
GNE O
are O
denoted O

as B-DAT
h(0)i O
, O
h O

1) B-DAT
i O
, O
....., O
h O
(k) O
i O
, O
which O
can O

be B-DAT
defined O
as O

h(0)i B-DAT
= O
δ O
( O
W0vi O
+ O
b(0 O

h(k)i B-DAT
= O
δk O
( O
Wkh(k−1)i O
+ O
b(k) O
) O
(7 O

) B-DAT
where O
δk O
represents O
the O
activation O

function B-DAT
of O
layer O
k. O
h(0)i O

represents B-DAT
initial O
representation O
and O
h(k)i O

represents B-DAT
final O
representation O
of O
the O

input B-DAT
gene O
vi. O
Transformation O
of O

input B-DAT
data O
using O
multiple O
non-linear O

layers B-DAT
has O
shown O
to O
improve O

the B-DAT
representation O
of O
input O
data O

[18]. B-DAT
Moreover, O
stacking O
multiple O
layers O

of B-DAT
non-linear O
transformations O
can O
help O

to B-DAT
learn O
high-order O
statistical O
relationships O

between B-DAT
topological O
properties O
and O
attributes O

. B-DAT
At O
last, O
final O
representation O
h(k)i O

of B-DAT
a O
gene O
vi O
from O

the B-DAT
last O
hidden O
layer O
is O

transformed B-DAT
to O
probability O
vector, O
which O

contains B-DAT
the O
conditional O
probability O
of O

all B-DAT
other O
genes O
to O
vi O

: B-DAT
oi O
= O
[ O
p(v1|vi), O
p(v2|vi O

), B-DAT
. O
. O
. O
, O

p(vM|vi) B-DAT
] O
(8 O

) B-DAT
where O
p(vj|vi) O
represents O
the O
probability O

of B-DAT
gene O
vi O
being O
related O

to B-DAT
gene O
vj O
and O
oi O

represents B-DAT
the O
output O
probabil- O
ity O

vector B-DAT
with O
the O
conditional O
probability O

of B-DAT
gene O
vi O
being O
connected O

to B-DAT
all O
other O
genes O

. B-DAT
Weight O
matrix O
Wout O
between O
the O

last B-DAT
hidden O
layer O
and O
the O

output B-DAT
layer O
corresponds O
to O
the O

abstractive B-DAT
represen- O
tation O
of O
neighborhood O

of B-DAT
genes. O
A O
jth O
row O

from B-DAT
Wout O
refers O
to O
the O

compact B-DAT
representation O
of O
neighborhood O
of O

gene B-DAT
vj, O
which O
can O
be O

denoted B-DAT
as O
ṽj. O
The O
proximity O

score B-DAT
between O
gene O
vi O
and O

vj B-DAT
can O
be O
defined O
as O

: B-DAT
f O
(vi, O
vj) O
= O
ṽj O

· B-DAT
h(k)i O
(9) O
which O
can O

be B-DAT
replaced O
into O
Eq. O
1 O

to B-DAT
calculate O
the O
condi- O
tional O

probability B-DAT

: B-DAT
p(vj|vi) O
= O
exp O

ṽj B-DAT
· O

( B-DAT

) B-DAT

M B-DAT
j′=1 O
exp O
( O
ṽj′ O
· O
h(k)i O

10) B-DAT
Our O
model O
learns O
two O
latent O

representations B-DAT
h(k)i O
and O
ṽi O
for O

a B-DAT
gene O
vi O
where O
h(k)i O

is B-DAT
the O
representation O
of O
gene O

as B-DAT
a O
node O
and O
ṽi O

is B-DAT
the O
representation O
of O
the O

gene B-DAT
vi O
as O
a O
neighbor O

. B-DAT
Neighborhood O
representation O
ṽi O
can O

be B-DAT
com- O
bined O
with O
node O

representation B-DAT
h(k)i O
by O
addition O
[19, O
20 O

] B-DAT
to O
get O
final O
representation O

for B-DAT
a O
gene O
as: O
yi O
= O
h(k)i O
+ O
ṽi O

(11) B-DAT
which O
returns O
us O
better O

performance B-DAT
results O

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
6 O
of O
14 O

For B-DAT
an O
edge O
connecting O
gene O

vi B-DAT
and O
vj, O
we O
create O

fea- B-DAT
ture O
vector O
by O
combining O

embeddings B-DAT
of O
those O
genes O
using O

Hadamard B-DAT
product. O
Empirical O
evaluation O
shows O

features B-DAT
created O
with O
Hadamard O
product O

gives B-DAT
better O
performance O
over O

concatenation B-DAT
[14]. O
Then, O
we O
train O

a B-DAT
logistic O
classi- O
fier O
on O

these B-DAT
features O
to O
classify O
whether O

genes B-DAT
vi O
and O
vj O
interact O

or B-DAT
not. O
Parameter O
optimization O
To O
optimize O
GNE O

, B-DAT
the O
goal O
is O
to O

maximize B-DAT
objective O
func- O
tion O
mentioned O

in B-DAT
Eq. O
10 O
as O
a O

function B-DAT
of O
all O
parame- O
ters. O

Let B-DAT
� O
be O
the O
parameters O

of B-DAT
GNE O
which O
includes O
{Wid, O

Watt B-DAT
, O
Wout O
, O
�h} O

and B-DAT
�h O
represents O
weight O
matrices O

Wk B-DAT
of O
hidden O
layers. O
We O

train B-DAT
our O
model O
to O
maximize O

the B-DAT
objective O
function O
with O
respect O

to B-DAT
all O
parameters O
� O
: O
argmax O

M∑ B-DAT
i=1 O

vj B-DAT
∈ O
Ni O
log O
exp O

ṽj B-DAT
· O
h(k)i O
) O
∑M O
j′=1 O
exp O

ṽj′ B-DAT
· O

( B-DAT

) B-DAT

12) B-DAT
Maximizing O
this O
objective O
function O
with O

respect B-DAT
to O
� O
is O
computationally O

expensive, B-DAT
which O
requires O
the O
calculation O

of B-DAT
partition O
function O

M B-DAT
j′=1 O
exp O
( O
ṽj′ O
· O
h(k)i O

for B-DAT
each O
gene. O
To O
calculate O
a O
single O
probability O

, B-DAT
we O
need O
to O
aggregate O

all B-DAT
genes O
in O
the O
network. O

To B-DAT
address O
this O
problem, O
we O

adopt B-DAT
the O
approach O
of O
negative O

sampling B-DAT
[13] O
which O
samples O
the O

negative B-DAT
interactions, O
interactions O
with O
no O

evidence B-DAT
of O
their O
existence, O
according O

to B-DAT
some O
noise O
distribution O
for O

each B-DAT
edge O
eij. O
This O
approach O

allows B-DAT
us O
to O
sample O
a O

small B-DAT
subset O
of O
genes O
from O

the B-DAT
network O
as O
negative O
samples O

for B-DAT
a O
gene, O
considering O
that O

the B-DAT
genes O
on O
selected O
subset O

don’t B-DAT
fall O
in O
the O
neighborhood O

Ni B-DAT
of O
the O
gene. O
Above O

objective B-DAT
function O
enhances O
the O
similarity O

of B-DAT
a O
gene O
viwith O
its O

neigh- B-DAT
borhood O
genes O
vj O
∈ O

Ni B-DAT
and O
weakens O
the O
similarity O

with B-DAT
genes O
not O
in O
its O

neighborhood B-DAT
genes O
vj O
/∈ O
Ni. O

It B-DAT
is O
inap- O
propriate O
to O

assume B-DAT
that O
the O
two O
genes O

in B-DAT
the O
network O
are O
not O

related B-DAT
if O
they O
are O
not O

connected. B-DAT
It O
may O
be O
the O

case B-DAT
that O
there O
is O
not O

enough B-DAT
experimental O
evidence O
to O
support O

that B-DAT
they O
are O
related O
yet. O

Thus, B-DAT
forcing O
the O
dissimilarity O
of O

a B-DAT
gene O
with O
all O
other O

genes, B-DAT
not O
in O
its O
neighborhood O

Ni B-DAT
seems O
to O
be O
inappropriate. O
We O
adopt O
Adaptive O
Moment O
Estimation O

(Adam) B-DAT
opti- O
mization O
[21], O
which O

is B-DAT
an O
extension O
to O
stochastic O

gra- B-DAT
dient O
descent, O
for O
optimizing O

Eq. B-DAT
12. O
Adam O
computes O
the O

adaptive B-DAT
learning O
rate O
for O
each O

parameter B-DAT
by O
per- O
forming O
smaller O

updates B-DAT
for O
the O
frequent O
parameters O

and B-DAT
larger O
updates O
for O
the O

infrequent B-DAT
parameters. O
The O
Adam O
method O

provides B-DAT
the O
ability O
of O
AdaGrad O

[22] B-DAT
to O
deal O
with O
sparse O

gradients B-DAT
and O
also O
the O
ability O

of B-DAT
RMSProp O
[23] O
to O
deal O

with B-DAT
non-stationary O
objectives. O
In O
each O

step, B-DAT
Adam O
algorithm O
samples O
mini-batch O

of B-DAT
interactions O
and O
then O

updates B-DAT
GNE’s O
parameters. O
To O
address O

the B-DAT
issue O
of O
over- O
fitting, O

regularization B-DAT
like O
dropout O
[24] O
and O

batch B-DAT
normal- O
ization O
[25] O
is O

added B-DAT
to O
hidden O
layers. O
Proper O

optimization B-DAT
of O
GNE O
gives O
the O

final B-DAT
representation O
for O
each O
gene. O
Experimental O
setup O
We O
evaluate O
our O

model B-DAT
using O
two O
real O
organism O

datasets. B-DAT
We O
take O
gene O
interaction O

network B-DAT
data O
from O
the O
BioGRID O

database B-DAT
[26] O
and O
gene O
expression O

data B-DAT
from O
DREAM5 O
challenge O
[7 O

]. B-DAT
We O
use O
two O
interaction O

datasets B-DAT
from O
BioGRID O
database O
(2017 O

released B-DAT
version O
3.4.153 O
and O
2018 O

released B-DAT
version O
3.4.158) O
to O
evaluate O

the B-DAT
predictive O
performance O
of O
our O

model. B-DAT
Self-interactions O
and O
redun- O
dant O

interactions B-DAT
are O
removed O
from O
interaction O

datasets. B-DAT
The O
statistics O
of O
the O

datasets B-DAT
are O
shown O
in O
Table O
2 O

. B-DAT
We O
evaluate O
the O
learned O
embeddings O

to B-DAT
infer O
gene O
net- O
work O

structure. B-DAT
We O
randomly O
hold O
out O

a B-DAT
fraction O
of O
inter- O
actions O

as B-DAT
the O
validation O
set O
for O

hyper-parameter B-DAT
tuning. O
Then, O
we O
divide O

the B-DAT
remaining O
interactions O
randomly O
into O

training B-DAT
and O
testing O
dataset O
with O

the B-DAT
equal O
number O
of O
interactions O

. B-DAT
Since O
the O
validation O
set O

and B-DAT
the O
test O
set O
con- O

tains B-DAT
only O
positive O
interactions, O
we O

randomly B-DAT
sample O
an O
equal O
number O

of B-DAT
gene O
pairs O
from O
the O

network, B-DAT
consider- O
ing O
the O
missing O

edge B-DAT
between O
the O
gene O
pairs O

represents B-DAT
the O
absence O
of O
interactions. O

Given B-DAT
the O
gene O
network O
G O

with B-DAT
a O
fraction O
of O
missing O

interactions, B-DAT
the O
task O
is O
to O

predict B-DAT
these O
missing O
interactions. O
We O
compare O
the O
GNE O
model O

with B-DAT
five O
competing O
methods. O
Correlation O

directly B-DAT
predicts O
the O
interactions O
between O

genes B-DAT
based O
on O
the O
correlation O

of B-DAT
expression O
pro- O
files. O
Then O

, B-DAT
the O
following O
three O
baselines O
( O

Isomap, B-DAT
LINE, O
and O
node2vec) O
are O

network B-DAT
embedding O
methods. O
Specif- O
ically, O

node2vec B-DAT
is O
the O
strong O
baseline O

for B-DAT
structural O
net- O
work O
embedding. O

We B-DAT
evaluate O
the O
performance O
of O

GNE B-DAT
against O
the O
following O
methods: O
• O
Correlation O
[27] O
It O
computes O

Pearson’s B-DAT
correlation O
coefficient O
between O
all O

genes B-DAT
and O
the O
interactions O
are O

ranked B-DAT
via O
correlation O
scores, O
i.e O

., B-DAT
highly O
correlated O
gene O
pairs O

receive B-DAT
higher O
confidence. O
• O
Isomap O
[10] O
It O
computes O

all-pairs B-DAT
shortest-path O
distances O
to O
create O

a B-DAT
distance O
matrix O
and O
performs O

singular-value B-DAT
decomposition O
of O
that O
matrix O

to B-DAT
learn O
a O
lower-dimensional O
representation O

. B-DAT
Genes O
separated O
Table O
2 O
Statistics O
of O
the O

interaction B-DAT
datasets O
from O
BioGRID O
and O

the B-DAT
gene O
expression O
data O
from O

DREAM5 B-DAT
challenge O

Interactions) B-DAT
Expression O
data O
Datasets O
#(Genes) O
2017 O
version O
2018 O

version B-DAT
#(Experiments O

) B-DAT
Yeast O
5950 O
544,652 O
557,487 O
536 O

E. B-DAT
coli O
4511 O
148,340 O
159,523 O
805 O

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
7 O
of O
14 O

by B-DAT
the O
distance O
less O
than O

threshold B-DAT
� O
in O
embedding O
space O

are B-DAT
considered O
to O
have O
the O

connection B-DAT
with O
each O
other O
and O

the B-DAT
reliability O
index, O
a O
likelihood O

indicating B-DAT
the O
interaction O
between O
two O

genes, B-DAT
is O
computed O
using O

FSWeight B-DAT
[28]. O
• O
LINE O
[16] O
Two O
separate O

embeddings B-DAT
are O
learned O
by O
preserving O

first-order B-DAT
and O
second-order O
proximity O
of O

the B-DAT
network O
structure O
respectively. O
Then O

, B-DAT
these O
embeddings O
are O
concatenated O

to B-DAT
get O
final O
representations O
for O

each B-DAT
node. O
• O
node2vec O
[14] O
It O
learns O

the B-DAT
embeddings O
of O
the O
node O

by B-DAT
applying O
Skip-gram O
model O
to O

node B-DAT
sequences O
generated O
by O
a O

biased B-DAT
random O
walk. O
We O
tuned O

two B-DAT
hyper-parameters O
p O
and O
q O

that B-DAT
control O
the O
random O
walk O

. B-DAT
Note O
that O
the O
competing O
methods O

such B-DAT
as O
Isomap, O
LINE, O
and O

node2vec B-DAT
are O
designed O
to O
capture O

only B-DAT
the O
topological O
properties O
of O

the B-DAT
network. O
For O
the O
fair O

com- B-DAT
parison O
with O
GNE O
that O

additionally B-DAT
integrates O
expression O
data, O
we O

concatenate B-DAT
attribute O
feature O
vector O
with O

learned B-DAT
gene O
representation O
to O
extend O

baselines B-DAT
by O
including O
the O
gene O

expression. B-DAT
We O
name O
these O
variants O

as B-DAT
Isomap+, O
LINE+, O
and O
node2vec O

+. B-DAT
We O
have O
implemented O
GNE O
with O

TensorFlow B-DAT
frame- O
work O
[29]. O
The O

parameter B-DAT
settings O
for O
GNE O
are O

deter- B-DAT
mined O
by O
its O
performance O

on B-DAT
the O
validation O
set. O
We O

randomly B-DAT
initialize O
GNE’s O
parameters, O
optimizing O

with B-DAT
mini-batch O
Adam. O
We O
test O

the B-DAT
batch O
size O
of O
[8 O

, B-DAT
16, O
32, O
64, O
128, O
256 O

] B-DAT
and O
learning O
rate O

of B-DAT
[0.1, O
0.01, O
0.005, O
0.002, O
0 O

.001, B-DAT
0.0001]. O
We O
test O
the O

number B-DAT
of O
negative O
samples O
to O

be B-DAT
[2, O
5, O
10, O
15, O
20 O

] B-DAT
as O
suggested O
by O
[13]. O

We B-DAT
test O
the O
embedding O
dimension O

d B-DAT
of O
[32, O
64, O
128, O
256 O

] B-DAT
for O
all O
methods. O
Also, O

we B-DAT
evaluate O
model’s O
performance O
with O

respect B-DAT
to O
differ- O
ent O
values O

of B-DAT
λ O
[0, O
0.2, O
0.4, O
0 O

.6, B-DAT
0.8, O
1], O
which O
is O

discussed B-DAT
in O
more O
detail O
later. O

The B-DAT
parameters O
are O
selected O
based O

on B-DAT
empirical O
evaluation O
and O
Table O
3 O
summarizes O
the O
optimal O
parameters O
tuned O

on B-DAT
validation O
data O
sets O

. B-DAT
To O
capture O
the O
non-linearity O
of O

gene B-DAT
expression O
data, O
we O
choose O

Exponential B-DAT
Linear O
Unit O
(ELU) O
[30 O

] B-DAT
activation O
function, O
which O
corresponds O

to B-DAT
δa O
in O
Eq. O
5. O

Also, B-DAT
ELU O
acti- O
vation O
avoids O

vanishing B-DAT
gradient O
problem O
and O
provides O

improved B-DAT
learning O
characteristics O
in O
comparison O

to B-DAT
other O
methods. O
We O
use O

a B-DAT
single O
hidden O
layer O
( O

k B-DAT
= O
1) O
with O
hyperbolic O

tangent B-DAT
activation O
(Tanh) O
to O
model O

complex B-DAT
statistical O
relationships O
between O
topological O

properties B-DAT
and O
attributes O
of O
the O
gene O

. B-DAT
The O
choice O
of O
ELU O

for B-DAT
attribute O
transformation O
and O
Tanh O

for B-DAT
hidden O
layer O
shows O
better O

performance B-DAT
upon O
empirical O
evaluation. O
We O
use O
the O
area O
under O

the B-DAT
ROC O
curve O
(AUROC) O
and O

area B-DAT
under O
the O
precision-recall O
curve O

(AUPR) B-DAT
[31] O
to O
eval- O
uate O

the B-DAT
rankings O
generated O
by O
the O

model B-DAT
for O
interactions O
in O
the O

test B-DAT
set. O
These O
metrics O
are O

widely B-DAT
used O
in O
evaluating O
the O

ranked B-DAT
list O
of O
predictions O
in O

gene B-DAT
interaction O
[4 O

]. B-DAT
Results O
and O
discussion O
We O
evaluate O

the B-DAT
ability O
of O
our O
GNE O

model B-DAT
to O
predict O
gene O
interaction O

of B-DAT
two O
real O
organisms. O
We O

present B-DAT
empirical O
results O
of O
our O

proposed B-DAT
method O
against O
other O
methods O

. B-DAT
Analysis O
of O
gene O
embeddings O
We O

visualize B-DAT
the O
embedding O
vectors O
of O

genes B-DAT
learned O
by O
GNE. O
We O

take B-DAT
the O
learned O
embeddings, O
which O

specifi- B-DAT
cally O
model O
the O
interactions O

by B-DAT
preserving O
topological O
and O
attribute O

similarity. B-DAT
We O
embed O
these O
embeddings O

into B-DAT
a O
2D O
space O
using O

t-SNE B-DAT
package O
[32] O
and O
visualize O

them B-DAT
(Fig. O
3). O
For O
comparison O

, B-DAT
we O
also O
visualize O
the O

embeddings B-DAT
learned O
by O
structure-preserving O
deep O

learning B-DAT
methods, O
such O
as O
LINE, O

and B-DAT
node2vec. O
In O
E. O
coli, O
a O
substantial O

fraction B-DAT
of O
functionally O
related O
genes O

are B-DAT
organized O
into O
operons, O
which O

are B-DAT
the O
group O
of O
genes O

that B-DAT
interact O
with O
each O
other O

and B-DAT
are O
co-regulated O
[33]. O
Since O

this B-DAT
concept O
fits O
well O
with O

the B-DAT
topological O
and O
attribute O
proximity O

implemented B-DAT
in O
GNE, O
we O
expect O

GNE B-DAT
to O
place O
genes O
within O

an B-DAT
operon O
close O
to O
each O

other B-DAT
in O
the O
embedding O
space O

. B-DAT
To O
evaluate O
this, O
we O

collect B-DAT
information O
about O
operons O
of O

E. B-DAT
coli O
from O
the O
DOOR O

database B-DAT
and O
visualize O
the O
embeddings O

of B-DAT
genes O
within O
these O
operons O
( O

Fig. B-DAT
3). O
Figure O
3 O
reveals O
the O
clustering O

structure B-DAT
that O
corre- O
sponds O
to O

the B-DAT
operons O
on O
E. O
coli O

. B-DAT
For O
example, O
operon O
with O

operon B-DAT
id O
3306 O
consists O
of O

seven B-DAT
genes: O
rsxA, O
rsxB, O
rsx, O

rsxD, B-DAT
rsxG, O
rsxE, O
and O
nth O

that B-DAT
are O
involved O
in O
electron O

transport. B-DAT
GNE O
infers O
similar O
representations O

for B-DAT
these O
genes, O
resulting O
in O

localized B-DAT
projection O
in O
the O
2D O

space. B-DAT
Similarly, O
other O
operons O
also O

show B-DAT
similar O
patterns O
(Fig. O
3). O
To O
test O
if O
the O
pattern O

in B-DAT
Fig. O
3 O
holds O
across O

all B-DAT
operons, O
we O
compute O
the O

average B-DAT
Euclidean O
distance O
between O
each O

gene’s B-DAT
vector O
representation O
and O
vector O

representations B-DAT
of O
other O
genes O
within O

the B-DAT
same O
operon. O
Genes O
within O

the B-DAT
same O
operon O
have O
significantly O

similar B-DAT
vector O
representa- O
tion O
yi O

than B-DAT
expected O
by O
chance O
(p-value O

= B-DAT
1.75e O
− O
127, O
2-sample O

KS B-DAT
test O

). B-DAT
Table O
3 O
Optimal O
parameter O
settings O

for B-DAT
GNE O
model O

Dataset B-DAT
Learning O
rate O
Batch O
size O

Embedding B-DAT
dimension O
(d) O
Epoch O
Negative O

samples B-DAT
Yeast O
0.005 O
256 O
128 O
20 O

10 B-DAT

E. B-DAT
coli O
0.002 O
128 O
128 O
20 O
10 O

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
8 O
of O
14 O

Fig. B-DAT
3 O
Visualization O
of O
learned O

embeddings B-DAT
for O
genes O
on O
E. O

coli. B-DAT
Genes O
are O
mapped O
to O

the B-DAT
2D O
space O
using O
the O

t-SNE B-DAT
package O
[32] O
with O
learned O

gene B-DAT
representations O
(yi O
, O

i B-DAT
= O
1, O
2, O
. O
. O
. O
, O
M O

) B-DAT
from O
different O
methods: O
a O

GNE, B-DAT
b O
LINE, O
and O
c O

node2vec B-DAT
as O
input. O
Operons O
3203, O
3274, O
3279, O
3306, O
and O
3736 O
of O

E. B-DAT
coli O
are O
visualized O
and O

show B-DAT
clustering O
patterns. O
Best O
viewed O

on B-DAT
screen O

Thus, B-DAT
the O
analysis O
here O
indicates O

that B-DAT
GNE O
can O
learn O
similar O

representations B-DAT
for O
genes O
with O
similar O

topological B-DAT
properties O
and O
expression. O
Gene O
interaction O
prediction O
We O
randomly O

remove B-DAT
50% O
of O
interactions O
from O

the B-DAT
net- O
work O
and O
compare O

various B-DAT
methods O
to O
evaluate O
their O

pre- B-DAT
dictions O
for O
50% O
missing O

interactions. B-DAT
Table O
4 O
shows O
the O

performance B-DAT
of O
GNE O
and O
other O

methods B-DAT
on O
gene O
interac- O
tion O

prediction B-DAT
across O
different O
datasets. O
As O

our B-DAT
method O
significantly O
outperforms O
other O

competing B-DAT
methods, O
it O
indicates O
the O

informativeness B-DAT
of O
gene O
expression O
in O

pre- B-DAT
dicting O
missing O
interactions. O
Also O

, B-DAT
our O
model O
is O
capable O

of B-DAT
integrating O
attributes O
with O
topological O

properties B-DAT
to O
learn O
better O
representations. O
Table O
4 O
Area O
under O
ROC O

curve B-DAT
(AUROC) O
and O
Area O
under O

PR B-DAT
curve O
(AUPR) O
for O
gene O

Interaction B-DAT
Prediction O

Methods B-DAT
Yeast O
E. O
coli O
AUROC O
AUPR O
AUROC O
AUPR O

Correlation B-DAT
0.582 O
0.579 O
0.537 O
0.557 O
Isomap O
0.507 O
0.588 O
0.559 O
0.672 O

LINE B-DAT
0.726 O
0.686 O
0.897 O
0.851 O
node2vec O
0.739 O
0.708 O
0.912 O
0.862 O

Isomap+ B-DAT
0.653 O
0.652 O
0.644 O
0.649 O
LINE+ O
0.745 O
0.713 O
0.899 O
0.856 O

node2vec+ B-DAT
0.751 O
0.716 O
0.871 O
0.826 O
GNE O
(Topology) O
0.787 O
0.784 O
0.930 O

0.931 B-DAT

GNE B-DAT
(our O
model) O
0.825* O
0.821* O
0 O

.940* B-DAT
0.939* O
+ O
indicates O
the O
concatenation O
of O

expression B-DAT
data O
with O
learned O
embeddings O

to B-DAT
create O
final O
representation O

. B-DAT
* O
denotes O
that O
GNE O

significantly B-DAT
outperforms O
node2vec O
at O
0.01 O

level B-DAT
paired O
t-test. O
Note O
that O

method B-DAT
that O
achieves O
the O
best O

performance B-DAT
is O
bold O
faced O
We O
compare O
our O
model O
with O

a B-DAT
correlation-based O
method, O
that O
takes O

only B-DAT
expression O
data O
into O
account O

. B-DAT
Our O
model O
shows O
significant O

improvement B-DAT
of O
0.243 O
(AUROC), O
0.242 O
( O

AUPR) B-DAT
on O
yeast O
and O
0.403 O
( O

AUROC), B-DAT
0.382 O
(AUPR) O
on O
E. O

coli B-DAT
over O
correlation-based O
methods. O
This O

improve- B-DAT
ment O
suggests O
the O
significance O

of B-DAT
the O
topological O
proper- O
ties O

of B-DAT
the O
gene O
network. O
The O
network O
embedding O
method, O
Isomap O

, B-DAT
performs O
poorly O
in O
comparison O

to B-DAT
correlation-based O
methods O
on O
yeast O

because B-DAT
of O
its O
limitation O
on O

network B-DAT
inference. O
Deep O
learning O
based O

network B-DAT
embedding O
methods O
such O
as O

LINE, B-DAT
and O
node2vec O
show O
the O

significant B-DAT
gain O
over O
Isomap O
and O

correlation-based B-DAT
methods. O
node2vec O
out- O
performs O

LINE B-DAT
across O
two O
datasets. O
Moreover, O

GNE B-DAT
trained O
only O
with O
topological O

properties B-DAT
outperforms O
these O
structured-based O
deep O

learning B-DAT
methods O
(Table O
4). O
However, O

these B-DAT
methods O
don’t O
consider O
the O

attributes B-DAT
of O
the O
gene O
that O

we B-DAT
suggest O
to O
contain O
useful O

informa- B-DAT
tion O
for O
gene O
interaction O

prediction. B-DAT
By O
adding O
expres- O
sion O

data B-DAT
with O
topological O
properties, O
GNE O

outperforms B-DAT
structure-preserving O
deep O
embedding O
methods O

across B-DAT
both O
datasets. O
Focusing O
on O
the O
results O
corresponding O

to B-DAT
the O
integra- O
tion O
of O

expression B-DAT
data O
with O
topological O
properties O

, B-DAT
we O
find O
that O
the O

method B-DAT
of O
integrating O
the O
expression O

data B-DAT
plays O
an O
essential O
role O

in B-DAT
the O
performance. O
Performance O
of O

node2vec+ B-DAT
(LINE+, O
Isomap+) O
shows O
little O

improvement B-DAT
with O
the O
integration O
of O

expression B-DAT
data O
on O
yeast. O
How- O

ever, B-DAT
node2vec+ O
(LINE+, O
Isomap+) O
has O

no B-DAT
improvement O
or O
decline O
in O

performance B-DAT
on O
E. O
coli. O
The O

decline B-DAT
in O
per- O
formance O
indicates O

that B-DAT
merely O
concatenating O
the O
expres- O

sion B-DAT
vector O
with O
learned O
representations O

for B-DAT
the O
gene O
is O
insufficient O

to B-DAT
capture O
the O
rich O
information O

in B-DAT
expression O
data. O
The O
late O

fusion B-DAT
approach O
of O
combining O
the O

embed- B-DAT
ding O
vector O
corresponding O
to O

the B-DAT
topological O

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
9 O
of O
14 O

of B-DAT
the O
gene O
network O
and O

the B-DAT
feature O
vector O
represent- O
ing O

expression B-DAT
data O
has O
no O
significant O

improvement B-DAT
in O
the O
performance O
(except O

Isomap). B-DAT
In O
contrast, O
our O
model O

incorporates B-DAT
gene O
expression O
data O
with O

topological B-DAT
prop- O
erties O
by O
the O

early B-DAT
fusion O
method O
and O
shows O

significant B-DAT
improvement O
over O
other O
methods. O
Impact O
of O
network O
sparsity O
We O

investigate B-DAT
the O
robustness O
of O
our O

model B-DAT
to O
network O
sparsity. O
We O

hold B-DAT
out O
10% O
interactions O
as O

the B-DAT
test O
set O
and O
change O

the B-DAT
sparsity O
of O
the O
remaining O

network B-DAT
by O
randomly O
removing O
a O

portion B-DAT
of O
remaining O
interactions. O
Then O

, B-DAT
we O
train O
GNE O
to O

predict B-DAT
interactions O
in O
the O
test O

set B-DAT
and O
eval- O
uate O
the O

change B-DAT
in O
performance O
to O
network O

sparsity. B-DAT
We O
evaluate O
two O
versions O

of B-DAT
our O
implementations: O
GNE O
with O

only B-DAT
topological O
properties O
and O
GNE O

with B-DAT
topological O
properties O
and O
expression O

data. B-DAT
The O
result O
is O
shown O

in B-DAT
Fig. O
4. O
Figure O
4 O
shows O
that O
our O

method’s B-DAT
performance O
improves O
with O
an O

increase B-DAT
in O
the O
number O
of O

training B-DAT
interactions O
across O
datasets. O
Also O

, B-DAT
our O
method’s O
performance O
improves O

when B-DAT
expression O
data O
is O
integrated O

with B-DAT
the O
topological O
structure. O
Specifically, O

GNE B-DAT
trained O
on O
10% O
of O

total B-DAT
inter- O
actions O
and O
attributes O

of B-DAT
yeast O
shows O
a O
significant O

gain B-DAT
of O
0.172 O
AUROC O
(from O
0 O

.503 B-DAT
to O
0.675) O
over O
GNE O

trained B-DAT
only O
with O
10% O
of O

total B-DAT
interactions O
as O
shown O
in O

Fig. B-DAT
4. O
Similarly, O
GNE O
improves O

the B-DAT
AUROC O
from O
0.497 O
to O
0 O

.816 B-DAT
for O
E. O
coli O
with O

the B-DAT
same O
setup O
as O
shown O

in B-DAT
Fig. O
4. O
The O
integration O

of B-DAT
gene O
expression O
data O
results O

in B-DAT
less O
improve- O
ment O
when O

we B-DAT
train O
GNE O
on O
a O

relatively B-DAT
large O
number O
of O
interactions. O
Moreover, O
the O
performance O
of O
GNE O

trained B-DAT
with O
50% O
of O
total O

interactions B-DAT
and O
expression O
data O
is O

comparable B-DAT
to O
be O
trained O
with O

80% B-DAT
of O
total O
interactions O
without O

gene B-DAT
expression O
data O
as O
shown O

in B-DAT
Fig. O
4. O
The O
integration O

of B-DAT
expression O
data O
with O
topological O

properties B-DAT
into O
GNE O
model O
has O

more B-DAT
improvement O
on O
E. O
coli O

than B-DAT
yeast O
when O
we O
train O

with B-DAT
10% O
of O
total O
interactions O

for B-DAT
each O
dataset O

. B-DAT
The O
reason O
for O
this O
is O

likely B-DAT
the O
difference O
in O
the O

number B-DAT
of O
available O
interactions O
for O

yeast B-DAT
and O
E. O
coli O
(Table O

2). B-DAT
This O
indicates O
the O
informativeness O

of B-DAT
gene O
expression O
when O
we O

have B-DAT
few O
interactions O
and O
supports O

the B-DAT
idea O
that O
the O
integration O

of B-DAT
expression O
data O
with O
topological O

properties B-DAT
improves O
gene O
interaction O
prediction O

. B-DAT
Impact O
of O
λ O
GNE O
involves O

the B-DAT
parameter O
λ O
that O
controls O

the B-DAT
impor- O
tance O
of O
gene O

expression B-DAT
information O
relative O
to O
topolog O

- B-DAT
ical O
properties O
of O
gene O

network B-DAT
as O
shown O
in O
Eq. O
6 O

. B-DAT
We O
examine O
how O
the O

choice B-DAT
of O
the O
parameter O
λ O

affects B-DAT
our O
method’s O
performance. O
Figure O
5 O
shows O
the O
comparison O
of O
our O

method’s B-DAT
performance O
with O
different O
values O

of B-DAT
λ O
when O
GNE O
is O

trained B-DAT
on O
varying O
percentage O
of O

total B-DAT
interactions O

. B-DAT
We O
evaluate O
the O
impact O
of O

λ B-DAT
on O
range O
[0, O
0.2 O

, B-DAT
0.4, O
0.6, O
0.8, O
1]. O

When B-DAT
λ O
becomes O
0, O
the O

learned B-DAT
representations O
model O
only O
topological O

properties. B-DAT
In O
contrast, O
setting O
the O

high B-DAT
value O
for O
λ O
makes O

GNE B-DAT
learn O
only O
from O
attributes O

and B-DAT
degrades O
its O
performance. O
Therefore, O

our B-DAT
model O
performs O
well O
when O

λ B-DAT
is O
within O
[0, O
1]. O
Figure O
5 O
shows O
that O
the O

integration B-DAT
of O
expression O
data O
improves O

the B-DAT
performance O
of O
GNE O
to O

predict B-DAT
gene O
interac- O
tions. O
Impact O

of B-DAT
λ O
depends O
on O
the O

number B-DAT
of O
interactions O
used O
to O

train B-DAT
GNE. O
If O
GNE O
is O

trained B-DAT
with O
few O
interactions, O
integration O

of B-DAT
expression O
data O
with O
topological O

properties B-DAT
plays O
a O
vital O
role O

in B-DAT
predicting O
missing O
interactions. O
As O

the B-DAT
number O
of O
training O
interactions O

increases, B-DAT
integration O
of O
expression O
data O

has B-DAT
less O
impact O
but O
still O

improves B-DAT
the O
performance O
over O
only O

topological B-DAT
properties O

. B-DAT
Figures O
4 O
and O
5 O
demonstrate O

that B-DAT
the O
expression O
data O
contributes O

the B-DAT
increase O
in O
AUROC O
by O

nearly B-DAT
0.14 O
when O
interactions O
are O

less B-DAT
than O
40% O
for O
yeast O

and B-DAT
about O
0.32 O
when O
interactions O

are B-DAT
less O
than O
10% O
for O

E. B-DAT
coli. O
More O
topo- O
logical O

properties B-DAT
and O
attributes O
are O
required O

for B-DAT
yeast O
than O
E. O
coli O

. B-DAT
It O
may O
be O
related O

to B-DAT
the O
fact O
that O
yeast O

is B-DAT
a O
more O
complex O
species O

than B-DAT
E. O
coli. O
Moreover, O
we O

can B-DAT
spec- O
ulate O
that O
more O

topological B-DAT
properties O
and O
attributes O
are O
a O
b O

Fig. B-DAT
4 O
AUROC O
comparison O
of O

GNE’s B-DAT
performance O
with O
respect O
to O

network B-DAT
sparsity. O
a O
yeast O
b O

E. B-DAT
coli. O
Integration O
of O
expression O

data B-DAT
with O
topological O
properties O
of O

the B-DAT
gene O
network O
improves O
the O

performance B-DAT
for O
both O

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
10 O
of O
14 O

a B-DAT
b O
Fig. O
5 O
Impact O
of O
λ O

on B-DAT
GNE’s O
performance O
trained O
with O

different B-DAT
percentages O
of O
interactions. O
a O

yeast B-DAT
b O
E. O
coli. O
Different O

lines B-DAT
indicate O
performance O
of O
GNE O

trained B-DAT
with O
different O
percentages O
of O

interactions B-DAT

required B-DAT
for O
higher O
eukaryotes O
like O

humans. B-DAT
In O
humans, O
GNE O
that O

integrates B-DAT
topological O
properties O
with O
attributes O

may B-DAT
be O
more O
successful O
than O

the B-DAT
methods O
that O
only O
use O

either B-DAT
topological O
properties O
or O
attributes. O
This O
demonstrates O
the O
sensitivity O
of O

GNE B-DAT
to O
parame- O
ter O
λ O

. B-DAT
This O
parameter O
λ O
has O

a B-DAT
considerable O
impact O
on O
our O
method’s O
performance O
and O
should O

be B-DAT
appropriately O
selected O

. B-DAT
Investigation O
of O
GNE’s O
predictions O
We O

investigate B-DAT
the O
predictive O
ability O
of O

our B-DAT
model O
in O
iden- O
tifying O

new B-DAT
gene O
interactions. O
For O
this O

aim, B-DAT
we O
consider O

a B-DAT
b O
c O
d O

Fig. B-DAT
6 O
Temporal O
holdout O
validation O

in B-DAT
predicting O
new O
interactions. O
Performance O

is B-DAT
measured O
by O
the O
area O

under B-DAT
the O
ROC O
curve O
and O

the B-DAT
area O
under O
the O
precision-recall O

curve. B-DAT
Shown O
are O
the O
performance O

of B-DAT
each O
method O
based O
on O

the B-DAT
AUROC O
(a, O
b) O
and O

AUPR B-DAT
(c, O
d) O
for O
yeast O

and B-DAT
E. O
coli. O
The O
limit O

of B-DAT
the O
y-axis O
is O
adjusted O

to B-DAT
[0.5, O
1.0] O
for O
the O

precision-recall B-DAT
curve O
to O
make O
the O

difference B-DAT
in O
performance O
more O
visible. O

GNE B-DAT
outperforms O
LINE O
and O

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
11 O
of O
14 O

Table B-DAT
5 O
AUROC O
and O
AUPR O

comparision B-DAT
for O
temporal O
holdout O
validation O
Methods O
Yeast O
E. O
coli O

AUROC B-DAT
AUPR O
AUROC O
AUPR O
LINE O
0.620 O
0.611 O
0.569 O
0.598 O

node2vec B-DAT
0.640 O
0.609 O
0.587 O
0.599 O
GNE O
(our O
model) O
0.710 O
0.683 O

0.653 B-DAT
0.658 O

Note B-DAT
that O
method O
that O
achieves O

the B-DAT
best O
performance O
is O
bold O

faced B-DAT
two O
versions O
of O
BioGRID O
interaction O

datasets B-DAT
at O
two O
dif- O
ferent O

time B-DAT
points O
(2017 O
and O
2018 O

version), B-DAT
where O
the O
older O
version O

is B-DAT
used O
for O
training O
and O

the B-DAT
newer O
one O
is O
used O

for B-DAT
testing O
the O
model O
(temporal O

holdout B-DAT
validation). O
The O
2018 O
version O

contains B-DAT
12,835 O
new O
interactions O
for O

yeast B-DAT
and O
11,185 O
new O
interactions O

for B-DAT
E. O
coli O
than O
the O

2017 B-DAT
ver- O
sion. O
GNE’s O
performance O

trained B-DAT
with O
50% O
and O
80 O

% B-DAT
of O
total O
interactions O
are O

comparable B-DAT
for O
both O
yeast O
and O

E. B-DAT
coli O
(Figs. O
4 O
and O
5 O

). B-DAT
We O
thus O
train O
our O

model B-DAT
with O
50% O
of O
total O

interactions B-DAT
from O
the O
2017 O
version O

to B-DAT
learn O
the O
embed- O
dings O

for B-DAT
genes O
and O
demonstrate O
the O

impact B-DAT
of O
integrating O
expression O
data O

with B-DAT
topological O
properties. O
We O
create O

the B-DAT
test O
set O
with O
new O

interactions B-DAT
from O
the O
2018 O
version O

of B-DAT
BioGRID O
as O
positive O
interactions O

and B-DAT
the O
equal O
number O
of O

negative B-DAT
interactions O
randomly O
sampled. O
We O

make B-DAT
pre- O
dictions O
for O
these O

interactions B-DAT
using O
learned O
embeddings O
and O

create B-DAT
a O
list O
of O
(Gene O

vi, B-DAT
Gene O
vj, O
probability), O
ranked O

by B-DAT
the O
predicted O
probability. O
We O

consider B-DAT
predicted O
gene O
pairs O
with O

the B-DAT
probabilities O
of O
0.5 O
or O

higher B-DAT
but O
are O
miss- O
ing O

from B-DAT
BioGRID O
for O
further O
investigation O

as B-DAT
we O
discuss O
later O
in O

this B-DAT
section. O
The O
temporal O
holdout O
performance O
of O

our B-DAT
model O
in O
comparison O
to O

other B-DAT
methods O
is O
shown O
in O

Fig. B-DAT
6. O
We O

observe B-DAT
that O
GNE O
outperforms O
both O

node2vec B-DAT
and O
LINE O
in O
temporal O

holdout B-DAT
validation O
across O
both O
yeast O

and B-DAT
E. O
coli O
datasets, O
indicating O

GNE B-DAT
can O
accurately O
predict O
new O

genetic B-DAT
interactions. O
Table O
5 O
shows O

that B-DAT
GNE O
achieves O
substantial O
improvement O

of B-DAT
7.0 O
(AUROC), O
7.4 O
(AUPR) O

on B-DAT
yeast O
and O
6.6 O
(AUROC), O
5 O

.9 B-DAT
(AUPR) O
on O
E. O
coli O

datasets. B-DAT
Table O
6 O
shows O
the O
top O

5 B-DAT
interactions O
with O
the O
sig O

- B-DAT
nificant O
increase O
in O
predicted O

probability B-DAT
for O
both O
yeast O
and O

E. B-DAT
coli O
after O
expression O
data O

is B-DAT
integrated. O
We O
also O
provide O

literature B-DAT
evidence O
with O
experimental O
evidence O

code B-DAT
obtained O
from O
the O
BioGRID O

database B-DAT
[26] O
sup- O
porting O
these O

predictions. B-DAT
BioGRID O
compiles O
interaction O
data O

from B-DAT
numerous O
publications O
through O
comprehen- O

sive B-DAT
curation O
efforts. O
Taking O
new O

interactions B-DAT
added O
to O
BioGRID O
(version O
3 O

.4.158) B-DAT
into O
consideration, O
we O
evalu- O

ate B-DAT
the O
probability O
of O
these O

interactions B-DAT
predicted O
by O
GNE O
trained O

with B-DAT
and O
without O
expression O
data. O

Specifically, B-DAT
integration O
of O
expression O
data O

increases B-DAT
the O
probability O
of O
8331 O
( O

out B-DAT
of O
11,185) O
interactions O
for O

E. B-DAT
coli O
(improving O
AUROC O
from O
0 O

.606 B-DAT
to O
0.662) O
and O
6,010 O
( O

out B-DAT
of O
12,835) O
interactions O
for O

yeast B-DAT
(improving O
AUROC O
from O
0.685 O

to B-DAT
0.707). O
Integration O
of O
topology O

and B-DAT
expression O
data O
sig- O
nificantly O

increases B-DAT
the O
probabilities O
of O
true O

interactions B-DAT
between O
genes O
(Table O
6). O
To O
further O
evaluate O
GNE’s O
predictions O

, B-DAT
we O
consider O
the O
new O

version B-DAT
of O
BioGRID O
(version O
3.4.162) O

and B-DAT
evaluate O
2609 O
yeast O
gene O

pairs B-DAT
(Additional O
file O
1: O
Table O

S1) B-DAT
and O
871 O
E. O
coli O

gene B-DAT
pairs O
(Additional O
file O
2: O

Table B-DAT
S2) O
predicted O
by O
GNE O

with B-DAT
the O
probabilities O
of O
0.5 O

or B-DAT
higher. O
We O
find O
that O
128 O
(5%) O
yeast O
gene O
pairs O
and O

78 B-DAT
(9%) O
E. O
coli O
gene O

pairs B-DAT
are O
true O
interactions O
that O

have B-DAT
been O
added O
to O
the O

lat- B-DAT
est O
release O
of O
BioGRID O

. B-DAT
We O
then O
evaluate O
the O

predictive B-DAT
ability O
of O
GNE O
by O

calculating B-DAT
the O
percentage O
of O
true O

inter- B-DAT
actions O
with O
regard O
to O

different B-DAT
probability O
bins O
(Fig. O
7). O

Sixteen B-DAT
percent O
of O
predicted O
yeast O

gene B-DAT
pairs O
and O
17.5% O
Table O
6 O
New O
gene O
interactions O

that B-DAT
are O
assigned O
high O
probability O

by B-DAT
GNE O

Organism B-DAT
Probability O
Gene O
i O
Gene O
j O
Experimental O

evidence B-DAT
code O
Topology O
Topology O

+ B-DAT
Expression O

Yeast B-DAT
0.287 O
0.677 O
TFC8 O
DHH1 O
Affinity O

Capture-RNA B-DAT
[35 O

] B-DAT
0.394 O
0.730 O
SYH1 O
DHH1 O
Affinity O

Capture-RNA B-DAT
[35 O

] B-DAT
0.413 O
0.746 O
CPR7 O
DHH1 O
Affinity O

Capture-RNA B-DAT
[35 O

] B-DAT
0.253 O
0.551 O
MRP10 O
DHH1 O
Affinity O

Capture-RNA B-DAT
[35 O

] B-DAT
0.542 O
0.835 O
RPS13 O
ULP2 O
Affinity O

Capture-MS B-DAT
[36 O

] B-DAT
E. O
coli O

0.014 B-DAT
0.944 O
ATPB O
RFBC O
Affinity O

Capture-MS B-DAT
[37] O
0.012 O
0.941 O
NARQ O
CYDB O
Affinity O

Capture-MS B-DAT
[37 O

] B-DAT
0.013 O
0.937 O
PCNB O
PAND O
Affinity O

Capture-MS B-DAT
[37 O

] B-DAT
0.015 O
0.939 O
FLIF O
CHEY O
Affinity O

Capture-MS B-DAT
[37 O

] B-DAT
0.017 O
0.938 O
YCHM O
PROB O
Affinity O

Capture-MS B-DAT
[37 O

] B-DAT
New O
gene O
interactions O
on O
2018 O

version B-DAT
that O
are O
assigned O
high O

probability B-DAT
by O
GNE O
after O
integration O

of B-DAT
expression O
data. O
We O
provide O

probability B-DAT
predicted O
by O
GNE O
(with/without O

expression B-DAT
data) O
for O
new O
interactions O

in B-DAT
the O
2018 O
version O
and O

evidence B-DAT
supporting O
the O
existence O
of O

predicted B-DAT
interactions O

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
12 O
of O
14 O

Fig. B-DAT
7 O
The O
percentage O
of O

true B-DAT
interactions O
from O
GNE’s O
predictions O

with B-DAT
different O
probability O
bins. O
a O

yeast B-DAT
b O
E. O
coli. O
We O

divide B-DAT
the O
gene O
pairs O
based O

on B-DAT
their O
predicted O
probabilities O
to O

different B-DAT
probability O
ranges O
(as O
shown O

in B-DAT
the O
x-axis) O
and O
identify O

the B-DAT
number O
of O
predicted O
true O

interactions B-DAT
in O
each O
range. O
Each O

bar B-DAT
indicates O
the O
percentage O
of O

true B-DAT
interactions O
out O
of O
predicted O

gene B-DAT
pairs O
in O
that O
probability O

range B-DAT
of O
predicted O
E. O
coli O
gene O

pairs B-DAT
with O
the O
probability O
higher O

than B-DAT
0.9 O
are O
true O
interactions O

. B-DAT
This O
suggests O
that O
gene O

pairs B-DAT
with O
high O
probability O
predicted O

by B-DAT
GNE O
are O
more O
likely O

to B-DAT
be O
true O
interactions. O
To O
support O
our O
finding O
that O

GNE B-DAT
predicted O
gene O
pairs O
have O

high B-DAT
value, O
we O
manually O
check O

gene B-DAT
pairs O
that O
have O
high O

predicted B-DAT
probability O
but O
are O
missing O

from B-DAT
the O
latest O
BioGRID O
release O

. B-DAT
We O
find O
that O
these O

gene B-DAT
pairs O
interact O
with O
the O

same B-DAT
set O
of O
other O
genes. O

For B-DAT
example, O
GNE O
pre- O
dicts O

the B-DAT
interaction O
between O
YDR311W O
and O

YGL122C B-DAT
with O
the O
probability O
of O
0 O

.968. B-DAT
Mining O
BioGRID O
database, O
we O

find B-DAT
that O
these O
genes O
interact O

with B-DAT
the O
same O
set O
of O
374 O
genes. O
Similarly, O
E. O
coli O
genes O

DAMX B-DAT
and O
FLIL O
with O
the O

predicted B-DAT
probability O
of O
0.998 O
share O

320 B-DAT
interacting O
genes. O
In O
this O

way, B-DAT
we O
identify O
all O
interacting O

genes B-DAT
shared O
by O
each O
of O

the B-DAT
predicted O
gene O
pairs O
in O

yeast B-DAT
and O
E. O
coli O
(Additional O

file B-DAT
1: O
Table O
S1 O
and O

Additional B-DAT
file O
2 O

: B-DAT
Table O
S2). O
Figure O
8 O
shows O

the B-DAT
average O
number O
of O
inter O

- B-DAT
acting O
genes O
shared O
by O

a B-DAT
gene O
pair. O
In O
general, O

gene B-DAT
pairs O
with O
a O
high O

GNE B-DAT
probability O
tend O
to O
have O

a B-DAT
large O
number O
of O
interacting O

genes. B-DAT
For O
example, O
gene O
pairs O

with B-DAT
the O
probability O
greater O
than O
0 O

.9 B-DAT
have, O
on O
average, O
82 O

common B-DAT
interacting O
genes O
for O
yeast O

and B-DAT
58 O
for O
E. O
coli. O

Two B-DAT
sample O
t-test O
analysis O
has O

shown B-DAT
that O
there O
is O
a O

significant B-DAT
difference O
in O
the O
number O

of B-DAT
shared O
inter- O
acting O
genes O

with B-DAT
respect O
to O
different O
probability O

bins B-DAT
(Table O
7). O
Moreover, O
we O
search O
the O
literature O

to B-DAT
see O
if O
we O
can O

find B-DAT
supporting O
evidence O
for O
predicted O

interactions. B-DAT
We O
find O
literature O
evidence O

for B-DAT
an O
interaction O
between O
YCL032W O

(STE50) B-DAT
and O
YDL035C O
(GPR1), O
which O

has B-DAT
the O
probability O
of O
0.98 O

predicted B-DAT
by O
GNE. O
STE50 O
is O

an B-DAT
adaptor O
that O
links O
G-protein O

complex B-DAT
in O
cell O
signalling, O
and O

GPR1 B-DAT
is O
a O
G- O
protein O

coupled B-DAT
receptor. O
Both O
STE50 O
and O

GPR1 B-DAT
share O
a O

Fig. B-DAT
8 O
The O
average O
number O

of B-DAT
common O
interacting O
genes O
between O

the B-DAT
gene O
pairs O
predicted O
by O

GNE. B-DAT
a O
yeast O
b O
E. O

coli. B-DAT
We O
divide O
gene O
pairs O

into B-DAT
different O
probability O
groups O
based O

on B-DAT
predicted O
probabilities O
by O
GNE O

and B-DAT
compute O
the O
number O
of O

common B-DAT
interacting O
genes O
shared O
by O

these B-DAT
gene O
pairs. O
We O
categorize O

these B-DAT
gene O
pairs O
into O
different O

probability B-DAT
ranges O
(as O
shown O
in O

the B-DAT
x-axis). O
Each O
bar O
represents O

the B-DAT
average O
number O
of O
common O

interacting B-DAT
genes O
shared O
by O
gene O

pairs B-DAT
in O
each O
probability O

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
13 O
of O
14 O

Table B-DAT
7 O
Results O
of O
two-sample O

t-test B-DAT
Probability O
bin O
for O
Sample O
A O

Probability B-DAT
bin O
for O
Sample O
B O
p-value O
for O
yeast O

p-value B-DAT
for O
E. O
coli O
0.5 O
- O
0.6 O
0.6 O

- B-DAT
0.7 O
9.2e O
− O
14 O

6.9e B-DAT
− O
02 O
0.5 O

- B-DAT
0.6 O
0.7 O
- O
0.8 O

3.02e B-DAT
− O
51 O
1.23e O

− B-DAT
05 O
0.5 O
- O
0.6 O

0.8 B-DAT
- O
0.9 O
6.1e O

− B-DAT
117 O
7.4e O
− O
14 O

0.5 B-DAT
- O
0.6 O
0.9 O

- B-DAT
1.0 O
2.1e O
− O
177 O

3.7e B-DAT
− O
39 O
0.6 O

- B-DAT
0.7 O
0.7 O
- O
0.8 O

8.2e B-DAT
− O
17 O
1.1e O

− B-DAT
02 O
0.6 O
- O
0.7 O

0.8 B-DAT
- O
0.9 O
3.5e O

− B-DAT
69 O
9.2e O
− O
09 O

0.6 B-DAT
- O
0.7 O
0.9 O

- B-DAT
1.0 O
2.1e O
− O
128 O

1.9e B-DAT
− O
30 O
0.7 O

- B-DAT
0.8 O
0.8 O
- O
0.9 O

4.7e B-DAT
− O
28 O
4.8e O

− B-DAT
04 O
0.7 O
- O
0.8 O

0.9 B-DAT
- O
1.0 O
6.2e O

− B-DAT
87 O
7.4e O
− O
23 O

0.8 B-DAT
- O
0.9 O
0.9 O

- B-DAT
1.0 O
4.3e O
− O
35 O

5.1e B-DAT
− O
13 O
We O
divide O

gene B-DAT
pairs O
into O
different O
probability O

groups B-DAT
based O
on O
predicted O
probabilities O

by B-DAT
GNE O
and O
compute O
the O

number B-DAT
of O
common O
interacting O
genes O

shared B-DAT
by O
these O
gene O
pairs O

. B-DAT
Significance O
test O
shows O
there O

is B-DAT
the O
significant O
difference O
between O

average B-DAT
number O
of O
shared O
genes O

in B-DAT
different O
probability O
bins O
common O
function O
of O
cell O
signalling O

via B-DAT
G-protein. O
Besides, O
STE50p O
interacts O

with B-DAT
STE11p O
in O
the O
two-hybrid O

system, B-DAT
which O
is O
a O
cell-based O

system B-DAT
examining O
protein-protein O
interactions O
[34 O

]. B-DAT
Also, O
BioGRID O
has O
evidence O

of B-DAT
30 O
physi- O
cal O
and O
4 O
genetic O
associations O
between O
STE50 O
and O

STE11. B-DAT
Thus, O
STE50 O
is O
highly O

likely B-DAT
to O
interact O
with O
STE11 O

, B-DAT
which O
in O
turn O
interacts O

with B-DAT
GPR1. O
This O
analysis O
demonstrates O
the O
potential O

of B-DAT
our O
method O
in O
the O

discovery B-DAT
of O
gene O
interactions. O
Also O

, B-DAT
GNE O
can O
help O
the O

curator B-DAT
to O
identify O
interactions O
with O

strong B-DAT
potential O
that O
need O
to O

be B-DAT
looked O
at O
with O
experimental O

validation B-DAT
or O
within O
the O
literature. O
Conclusion O
We O
developed O
a O
novel O

deep B-DAT
learning O
framework, O
namely O
GNE O

to B-DAT
perform O
gene O
network O
embedding O

. B-DAT
Specifi- O
cally, O
we O
design O

deep B-DAT
neural O
network O
architecture O
to O

model B-DAT
the O
complex O
statistical O
relationships O

between B-DAT
gene O
interaction O
network O
and O

expression B-DAT
data. O
GNE O
is O
flexible O

to B-DAT
the O
addition O
of O
different O

types B-DAT
and O
num- O
ber O
of O

attributes. B-DAT
The O
features O
learned O
by O

GNE B-DAT
allow O
us O
to O
use O

out-of-the-box B-DAT
machine O
learning O
classifiers O
like O

Logistic B-DAT
Regression O
to O
predict O
gene O

interactions B-DAT
accurately. O
GNE O
relies O
on O
a O
deep O

learning B-DAT
technique O
that O
can O
learn O

the B-DAT
underlying O
patterns O
of O
gene O

interactions B-DAT
by O
integrat- O
ing O
heterogeneous O

data B-DAT
and O
extracts O
features O
that O

are B-DAT
more O
informative O
for O
interaction O

prediction. B-DAT
Experimental O
results O
show O
that O

GNE B-DAT
achieve O
better O
performance O
in O

gene B-DAT
interaction O
prediction O
over O
other O

baseline B-DAT
approaches O
in O
both O
yeast O

and B-DAT
E. O
coli O
organisms. O
Also O

, B-DAT
GNE O
can O
help O
the O

curator B-DAT
to O
identify O
the O
interactions O

that B-DAT
need O
to O
be O
looked O

at. B-DAT
As O
future O
work, O
we O
aim O

to B-DAT
study O
the O
impact O
of O

inte- B-DAT
grating O
other O
sources O
of O

information B-DAT
about O
gene O
such O
as O

transcription B-DAT
factor O
binding O
sites, O
functional O

annota- B-DAT
tions O
(from O
gene O
ontology O

), B-DAT
gene O
sequences, O
metabolic O
pathways, O

etc. B-DAT
into O
GNE O
in O
predicting O

gene B-DAT
interaction. O
Additional O
files O

Additional B-DAT
file O
1: O
Table O
S1. O

Includes B-DAT
yeast O
gene O
pairs O
predicted O

by B-DAT
GNE O
with O
probabilities O
of O
0 O

.5 B-DAT
or O
higher. O
(XLSX O
109 O

kb) B-DAT
Additional O
file O
2: O
Table O
S2 O

. B-DAT
Includes O
E. O
coli O
gene O

pairs B-DAT
predicted O
by O
GNE O
with O

probabilities B-DAT
of O
0.5 O
or O
higher. O

Rows B-DAT
marked O
with O
yellow O
color O

indicate B-DAT
predicted O
interaction O
is O
true O

based B-DAT
on O
latest O
version O
3.4.162 O

of B-DAT
BioGRID O
interaction O
dataset O
released O

on B-DAT
June O
2018. O
(XLSX O
43.7 O

kb) B-DAT
Abbreviations O
AUPR: O
Area O
under O
the O

PR B-DAT
curve; O
AUROC: O
Area O
under O

the B-DAT
ROC O
curve; O
DOOR: O
Database O

of B-DAT
Prokaryotic O
Operons; O
DREAM: O
Dialogue O

on B-DAT
reverse O
Engineering O
assessment O
and O

methods; B-DAT
ELU: O
Exponential O
linear O
unit O

; B-DAT
GI: O
Genetic O
interaction; O
GNE: O

Gene B-DAT
network O
embedding; O
ID: O
Identifier; O

KS: B-DAT
Kolmogrov-Smirnov; O
LINE: O
Large-scale O
information O

network B-DAT
embedding; O
PR: O
Precision O
recall; O

ROC: B-DAT
Receiver O
operator O
characterstic; O
t-SNE: O

t-Distributed B-DAT
stochastic O
neighbor O
embedding O
Acknowledgements O
Not O
applicable O

. B-DAT
Funding O
This O
work O
was O
supported O

by B-DAT
the O
National O
Science O
Foundation O

[1062422 B-DAT
to O
A.H.] O
and O
the O

National B-DAT
Institutes O
of O
Health O
[R15GM116102 O

to B-DAT
F.C.]. O
Publication O
of O
this O

article B-DAT
was O
sponsored O
by O
NSF O

grant B-DAT
(1062422 O

). B-DAT
Availability O
of O
data O
and O
materials O

The B-DAT
datasets O
analysed O
during O
the O

current B-DAT
study O
are O
publicly O
available O

. B-DAT
The O
gene O
expression O
data O

was B-DAT
downloaded O
from O
DREAM5 O
Network O

Challenge B-DAT
https:// O
www.synapse.org/#!Synapse:syn2787209/wiki/70351. O
The O
interaction O

datasets B-DAT
for O
yeast O
and O
E. O

coli B-DAT
were O
downloaded O
from O
BioGRID O

https://thebiogrid.org. B-DAT
About O
this O
supplement O
This O
article O

has B-DAT
been O
published O
as O
part O

of B-DAT
BMC O
Systems O
Biology O
Volume O

13 B-DAT
Supplement O
2, O
2019: O
Selected O

articles B-DAT
from O
the O
17th O
Asia O

Pacific B-DAT
Bioinformatics O
Conference O
(APBC O
2019 O

): B-DAT
systems O
biology. O
The O
full O

contents B-DAT
of O
the O
supplement O
are O

available B-DAT
online O
at O
https://bmcsystbiol.biomedcentral.com/articles/ O
supplements/volume-13-supplement-2. O
Authors’ O
contributions O
KK, O
RL, O
FC O

, B-DAT
and O
AH O
designed O
the O

research. B-DAT
KK O
performed O
the O
research O

and B-DAT
wrote O
the O
manuscript. O
RL, O

FC, B-DAT
QY, O
and O
AH O
improved O

the B-DAT
draft. O
All O
authors O
read O

and B-DAT
approved O
the O
final O
manuscript. O
Ethics O
approval O
and O
consent O
to O

participate B-DAT
Not O
applicable O

. B-DAT
Consent O
for O
publication O
Not O
applicable O

. B-DAT
Competing O
interests O
The O
authors O
declare O

that B-DAT
they O
have O
no O
competing O

interests B-DAT

. B-DAT
Publisher’s O
Note O
Springer O
Nature O
remains O

neutral B-DAT
with O
regard O
to O
jurisdictional O

claims B-DAT
in O
published O
maps O
and O

institutional B-DAT
affiliations O

. B-DAT
Author O
details O
1Golisano O
College O
of O

Computing B-DAT
and O
Information O
Sciences, O
Rochester O

Institute B-DAT
of O
Technology, O
20 O
Lomb O

Memorial B-DAT
Drive, O
14623 O
Rochester, O
New O

York, B-DAT
USA. O
2Thomas O
H. O
Gosnell O

School B-DAT
of O
Life O
Sciences, O
Rochester O

Institute B-DAT
of O
Technology, O
84 O
Lomb O

Memorial B-DAT
Drive, O
14623 O
Rochester, O
New O

York, B-DAT
USA O

. B-DAT
Published: O
5 O
April O
2019 O

https://doi.org/10.1186/s12918-019-0694-y B-DAT
https://doi.org/10.1186/s12918-019-0694-y O
https://www.synapse.org/#!Synapse:syn2787209/wiki/70351 O
https://www.synapse.org/#!Synapse:syn2787209/wiki/70351 O
https://thebiogrid.org O

https://bmcsystbiol.biomedcentral.com/articles/supplements/volume-13-supplement-2 B-DAT

13 B-DAT

2 B-DAT

KC B-DAT
et O
al. O
BMC O
Systems O

Biology B-DAT
2019, O
13(Suppl O
2):38 O
Page O
14 O
of O
14 O

References B-DAT
1. O
Mani O
R, O
Onge O

RPS, B-DAT
Hartman O
JL, O
Giaever O
G, O

Roth B-DAT
FP. O
Defining O
genetic O
interaction. O
Proc O
Natl O
Acad O
Sci O

. B-DAT
2008;105(9):3461–6. O
2. O
Boucher O
B, O

Jenna B-DAT
S. O
Genetic O
interaction O
networks: O

better B-DAT
understand O
to O
better O
predict. O
Front O
Genet. O
2013;4:290 O

. B-DAT
3. O
Lage O
K. O
Protein–protein O

interactions B-DAT
and O
genetic O
diseases: O
the O
interactome. O
Biochim O
Biophys O
Acta O
(BBA O

) B-DAT
- O
Mol O
Basis O
Dis. O
2014 O

;1842(10): B-DAT
1971–80. O
4. O
Madhukar O
NS, O
Elemento O
O O

, B-DAT
Pandey O
G. O
Prediction O
of O

genetic B-DAT
interactions O
using O
machine O
learning O

and B-DAT
network O
properties. O
Front O
Bioeng O

Biotechnol. B-DAT
2015;3:172. O
5. O
Oliver O
S. O
Proteomics: O
guilt-by-association O

goes B-DAT
global. O
Nature. O
2000;403(6770):601 O

. B-DAT
6. O
Cho O
H, O
Berger O
B O

, B-DAT
Peng O
J. O
Compact O
integration O

of B-DAT
multi-network O
topology O
for O
functional O

analysis B-DAT
of O
genes. O
Cell O
Syst. O
2016 O

;3(6):540–8. B-DAT
7. O
Marbach O
D, O
Costello O
JC O

, B-DAT
Küffner O
R, O
Vega O
NM, O

Prill B-DAT
RJ, O
Camacho O
DM, O
Allison O

KR, B-DAT
Aderhold O
A, O
Bonneau O
R, O

Chen B-DAT
Y, O
et O
al. O
Wisdom O

of B-DAT
crowds O
for O
robust O
gene O

network B-DAT
inference. O
Nat O
Methods. O
2012;9(8):796. O
8. O
Li O
R, O
KC O
K O

, B-DAT
Cui O
F, O
Haake O
AR. O

Sparse B-DAT
covariance O
modeling O
in O
high O

dimensions B-DAT
with O
gaussian O
processes. O
In: O

Proceedings B-DAT
of O
The O
32nd O
Conference O

on B-DAT
Neural O
Information O
Processing O
Systems O
( O

NIPS). B-DAT
2018. O
9. O
Cui O
P, O
Wang O
X O

, B-DAT
Pei O
J, O
Zhu O
W. O

A B-DAT
survey O
on O
network O
embedding. O

IEEE B-DAT
Trans O
Knowl O
Data O
Eng. O
2018 O

. B-DAT
arXiv O
preprint O
arXiv:1711.08752. O
IEEE. O
10. O
Lei O
Y-K, O
You O
Z-H O

, B-DAT
Ji O
Z, O
Zhu O
L, O

Huang B-DAT
D-S. O
Assessing O
and O
predicting O

protein B-DAT
interactions O
by O
combining O
manifold O

embedding B-DAT
with O
multiple O
information O
integration. O

In: B-DAT
BMC O
Bioinformatics, O
vol. O
13. O

BioMed B-DAT
Central; O
2012. O
p. O
3. O
11. O
Alanis-Lobato O
G, O
Cannistraci O
CV O

, B-DAT
Ravasi O
T. O
Exploitation O
of O

genetic B-DAT
interaction O
network O
topology O
for O

the B-DAT
prediction O
of O
epistatic O
behavior. O

Genomics. B-DAT
2013;102(4):202–8. O
12. O
Tenenbaum O
JB, O
De O
Silva O

V, B-DAT
Langford O
JC. O
A O
global O

geometric B-DAT
framework O
for O
nonlinear O
dimensionality O

reduction. B-DAT
science. O
2000;290(5500):2319–23 O

. B-DAT
13. O
Mikolov O
T, O
Sutskever O
I O

, B-DAT
Chen O
K, O
Corrado O
GS, O

Dean B-DAT
J. O
Distributed O
representations O
of O

words B-DAT
and O
phrases O
and O
their O

compositionality. B-DAT
In: O
Advances O
in O
Neural O

Information B-DAT
Processing O
Systems. O
2013. O
p. O
3111 O

–3119. B-DAT
14. O
Grover O
A, O
Leskovec O
J O

. B-DAT
node2vec: O
Scalable O
feature O
learning O

for B-DAT
networks. O
In: O
Proceedings O
of O

the B-DAT
22nd O
ACM O
SIGKDD O
International O

Conference B-DAT
on O
Knowledge O
Discovery O
and O

Data B-DAT
Mining. O
ACM; O
2016. O
p. O
855 O

–864. B-DAT
15. O
Tu O
Y, O
Stolovitzky O
G O

, B-DAT
Klein O
U. O
Quantitative O
noise O

analysis B-DAT
for O
gene O
expression O
microarray O

experiments. B-DAT
Proc O
Natl O
Acad O
Sci. O
2002 O

;99(22): B-DAT
14031–6. O
16. O
Tang O
J, O
Qu O
M O

, B-DAT
Wang O
M, O
Zhang O
M, O

Yan B-DAT
J, O
Mei O
Q. O
Line: O

Large-scale B-DAT
information O
network O
embedding. O
In: O

Proceedings B-DAT
of O
the O
24th O
International O

Conference B-DAT
on O
World O
Wide O
Web. O

International B-DAT
World O
Wide O
Web O
Conferences O

Steering B-DAT
Committee; O
2015. O
p. O
1067–1077. O
17. O
Snoek O
CG, O
Worring O
M O

, B-DAT
Smeulders O
AW. O
Early O
versus O

late B-DAT
fusion O
in O
semantic O
video O

analysis. B-DAT
In: O
Proceedings O
of O
the O

13th B-DAT
Annual O
ACM O
International O
Conference O

on B-DAT
Multimedia. O
2005. O
p. O
399–402. O

ACM. B-DAT
18. O
He O
K, O
Zhang O
X O

, B-DAT
Ren O
S, O
Sun O
J. O

Deep B-DAT
residual O
learning O
for O
image O

recognition. B-DAT
In: O
Proceedings O
of O
the O

IEEE B-DAT
Conference O
on O
Computer O
Vision O

and B-DAT
Pattern O
Recognition. O
2016. O
p. O
770 O

–778. B-DAT
19. O
Pennington O
J, O
Socher O
R O

, B-DAT
Manning O
C. O
Glove: O
Global O

vectors B-DAT
for O
word O
representation. O
In: O

Proceedings B-DAT
of O
the O
2014 O
Conference O

on B-DAT
Empirical O
Methods O
in O
Natural O

Language B-DAT
Processing O
(EMNLP). O
2014. O
p. O
1532 O

–1543. B-DAT
20. O
Levy O
O, O
Goldberg O
Y O

, B-DAT
Dagan O
I. O
Improving O
distributional O

similarity B-DAT
with O
lessons O
learned O
from O

word B-DAT
embeddings. O
Trans O
Assoc O
Comput O

Linguist. B-DAT
2015;3:211–25. O
21. O
Kingma O
DP, O
Ba O
J O

. B-DAT
Adam: O
A O
method O
for O

stochastic B-DAT
optimization. O
In: O
International O
Conference O

on B-DAT
Learning O
Representations; O
2015. O
22. O
Duchi O
J, O
Hazan O
E O

, B-DAT
Singer O
Y. O
Adaptive O
subgradient O

methods B-DAT
for O
online O
learning O
and O

stochastic B-DAT
optimization. O
J O
Mach O
Learn O

Res. B-DAT
2011;12(Jul): O
2121–59. O
23. O
Tieleman O
T, O
Hinton O
G O

. B-DAT
Lecture O
6.5-rmsprop, O
coursera: O
Neural O

networks B-DAT
for O
machine O
learning. O
University O

of B-DAT
Toronto, O
Technical O
Report. O
2012. O
24. O
Srivastava O
N, O
Hinton O
G O

, B-DAT
Krizhevsky O
A, O
Sutskever O
I, O

Salakhutdinov B-DAT
R. O
Dropout: O
A O
simple O

way B-DAT
to O
prevent O
neural O
networks O

from B-DAT
overfitting. O
J O
Mach O
Learn O

Res. B-DAT
2014;15(1):1929–58. O
25. O
Ioffe O
S, O
Szegedy O
C O

. B-DAT
Batch O
normalization: O
Accelerating O
deep O

network B-DAT
training O
by O
reducing O
internal O

covariate B-DAT
shift. O
In: O
International O
Conference O

on B-DAT
Machine O
Learning; O
2015. O
p. O
448 O

–56. B-DAT
26. O
Stark O
C, O
Breitkreutz O
B-J O

, B-DAT
Reguly O
T, O
Boucher O
L, O

Breitkreutz B-DAT
A, O
Tyers O
M. O
Biogrid: O

a B-DAT
general O
repository O
for O
interaction O

datasets. B-DAT
Nucleic O
Acids O
Res. O
2006;34(suppl_1):535–9. O
27. O
Butte O
A-J, O
Kohane O
I-S O

. B-DAT
Mutual O
information O
relevance O
networks: O

functional B-DAT
genomic O
clustering O
using O
pairwise O

entropy B-DAT
measurements. O
In: O
Biocomputing O
2000. O

World B-DAT
Scientific; O
1999. O
p. O
418–429. O
28. O
Chua O
HN, O
Sung O
W-K O

, B-DAT
Wong O
L. O
Exploiting O
indirect O

neighbours B-DAT
and O
topological O
weight O
to O

predict B-DAT
protein O
function O
from O
protein–protein O

interactions. B-DAT
Bioinformatics. O
2006;22(13):1623–30. O
29. O
Abadi O
M, O
Agarwal O
A O

, B-DAT
Barham O
P, O
Brevdo O
E, O

Chen B-DAT
Z, O
Citro O
C, O
Corrado O

GS, B-DAT
Davis O
A, O
Dean O
J, O

Devin B-DAT
M, O
Ghemawat O
S, O
Goodfellow O

I, B-DAT
Harp O
A, O
Irving O
G, O

Isard B-DAT
M, O
Jia O
Y, O
Jozefowicz O

R, B-DAT
Kaiser O
L, O
Kudlur O
M, O

Levenberg B-DAT
J, O
Mané O
D., O
Monga O

R, B-DAT
Moore O
S, O
Murray O
D, O

Olah B-DAT
C, O
Schuster O
M, O
Shlens O

J, B-DAT
Steiner O
B, O
Sutskever O
I, O

Talwar B-DAT
K, O
Tucker O
P, O
Vanhoucke O

V, B-DAT
Vasudevan O
V, O
Viégas O
F., O

Vinyals B-DAT
O, O
Warden O
P, O
Wattenberg O

M, B-DAT
Wicke O
M, O
Yu O
Y, O

Zheng B-DAT
X. O
TensorFlow: O
Large-Scale O
Machine O

Learning B-DAT
on O
Heterogeneous O
Systems. O
Software O

available B-DAT
from O
tensorflow.org. O
2015. O
https://www.tensorflow.org/ O

Accessed B-DAT
21 O
Dec O
2016. O
30. O
Clevert O
D, O
Unterthiner O
T O

, B-DAT
Hochreiter O
S. O
Fast O
and O

accurate B-DAT
deep O
network O
learning O
by O

exponential B-DAT
linear O
units O
(elus). O
In: O

International B-DAT
Conference O
on O
Learning O
Representations; O
2016 O

. B-DAT
31. O
Davis O
J, O
Goadrich O
M O

. B-DAT
The O
relationship O
between O
precision-recall O

and B-DAT
roc O
curves. O
In: O
Proceedings O

of B-DAT
the O
23rd O
International O
Conference O

on B-DAT
Machine O
Learning. O
ACM; O
2006. O

p. B-DAT
233–240. O
32. O
Maaten O
L. O
v. O
d O

., B-DAT
Hinton O
G. O
Visualizing O
data O

using B-DAT
t-sne. O
J O
Mach O
Learn O

Res. B-DAT
2008;9(Nov):2579–605. O
33. O
Mao O
F, O
Dam O
P O

, B-DAT
Chou O
J, O
Olman O
V, O

Xu B-DAT
Y. O
Door: O
a O
database O

for B-DAT
prokaryotic O
operons. O
Nucleic O
Acids O

Res. B-DAT
2008;37(suppl_1):459–63. O
34. O
Gustin O
MC, O
Albertyn O
J O

, B-DAT
Alexander O
M, O
Davenport O
K. O

Map B-DAT
kinase O
pathways O
in O
the O

yeastsaccharomyces B-DAT
cerevisiae. O
Microbiol O
Mol O
Biol O

Rev. B-DAT
1998;62(4): O
1264–300. O
35. O
Miller O
JE, O
Zhang O
L O

, B-DAT
Jiang O
H, O
Li O
Y, O

Pugh B-DAT
BF, O
Reese O
JC. O
Genome-wide O

mapping B-DAT
of O
decay O
factor–mrna O
interactions O

in B-DAT
yeast O
identifies O
nutrient-responsive O
transcripts O

as B-DAT
targets O
of O
the O
deadenylase O

ccr4. B-DAT
G3: O
Genes, O
Genomes, O
Genet. O
2018 O

;8(1):315–30. B-DAT
36. O
Liang O
J, O
Singh O
N O

, B-DAT
Carlson O
CR, O
Albuquerque O
CP, O

Corbett B-DAT
KD, O
Zhou O
H. O
Recruitment O

of B-DAT
a O
sumo O
isopeptidase O
to O

rdna B-DAT
stabilizes O
silencing O
complexes O
by O

opposing B-DAT
sumo O
targeted O
ubiquitin O
ligase O

activity. B-DAT
Genes O
Dev. O
2017;31(8):802–15. O
37. O
Babu O
M, O
Bundalovic-Torma O
C O

, B-DAT
Calmettes O
C, O
Phanse O
S, O

Zhang B-DAT
Q, O
Jiang O
Y, O
Minic O

Z, B-DAT
Kim O
S, O
Mehla O
J, O

Gagarinova B-DAT
A, O
et O
al. O
Global O

landscape B-DAT
of O
cell O
envelope O
protein O

complexes B-DAT
in O
escherichia O
coli. O
Nat O

Biotechnol. B-DAT
2018;36(1):103. O
https://www.tensorflow.org O

Gene B-DAT
network O
embedding O
(GNE) O

Gene B-DAT
network O
structure O

Gene B-DAT
expression O

GNE B-DAT

Parameter B-DAT

Experimental B-DAT

Results B-DAT
and O

Analysis B-DAT
of O
gene O

Gene B-DAT
interaction O

Impact B-DAT
of O
network O

Impact B-DAT

Investigation B-DAT
of O
GNE's O

Additional B-DAT

Additional B-DAT
file O
1 O

Additional B-DAT
file O
2 O

Availability B-DAT
of O
data O
and O

About B-DAT
this O

Authors' B-DAT

Ethics B-DAT
approval O
and O
consent O
to O

Consent B-DAT
for O

Competing B-DAT

Publisher's B-DAT

Author B-DAT

interaction O
network O
data O
from O
the O
BioGRID B-DAT
database O
[26] O
and O
gene O
expression O

use O
two O
interaction O
datasets O
from O
BioGRID B-DAT
database O
(2017 O
released O
version O
3.4.153 O

of O
the O
interaction O
datasets O
from O
BioGRID B-DAT
and O
the O
gene O
expression O
data O

two O
versions O
of O
BioGRID B-DAT
interaction O
datasets O
at O
two O
dif O

from O
the O
2018 O
version O
of O
BioGRID B-DAT
as O
positive O
interactions O
and O
the O

but O
are O
miss- O
ing O
from O
BioGRID B-DAT
for O
further O
investigation O
as O
we O

evidence O
code O
obtained O
from O
the O
BioGRID B-DAT
database O
[26] O
sup- O
porting O
these O

predictions. O
BioGRID B-DAT
compiles O
interaction O
data O
from O
numerous O

Taking O
new O
interactions O
added O
to O
BioGRID B-DAT
(version O
3.4.158) O
into O
consideration, O
we O

consider O
the O
new O
version O
of O
BioGRID B-DAT
(version O
3.4.162) O
and O
evaluate O
2609 O

the O
lat- O
est O
release O
of O
BioGRID B-DAT

are O
missing O
from O
the O
latest O
BioGRID B-DAT
release. O
We O
find O
that O
these O

the O
probability O
of O
0.968. O
Mining O
BioGRID B-DAT
database, O
we O
find O
that O
these O

examining O
protein-protein O
interactions O
[34]. O
Also, O
BioGRID B-DAT
has O
evidence O
of O
30 O
physi O

on O
latest O
version O
3.4.162 O
of O
BioGRID B-DAT
interaction O
dataset O
released O
on O
June O

E. O
coli O
were O
downloaded O
from O
BioGRID B-DAT
https://thebiogrid.org O

mel-spectrogram O
generation. O
Experiments O
on O
the O
LJSpeech B-DAT
dataset O
show O
that O
our O
parallel O

We O
conduct O
experiments O
on O
the O
LJSpeech B-DAT
dataset O
to O
test O
FastSpeech. O
The O

We O
conduct O
experiments O
on O
LJSpeech B-DAT
dataset O
[10], O
which O
contains O
13,100 O

a O
duration O
predictor. O
Experiments O
on O
LJSpeech B-DAT
dataset O
demonstrate O
that O
our O
proposed O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
1 O

Recurrent B-DAT
CNN O
for O
3D O
Gaze O

Estimation B-DAT
using O
Appearance O
and O
Shape O

Cues B-DAT
Cristina O
Palmero1,2 O

crpalmec7@alumnes.ub.edu B-DAT
1 O
Dept. O
Mathematics O
and O
Informatics O

Universitat B-DAT
de O
Barcelona, O
Spain O

Javier B-DAT
Selva1 O
javier.selva.castello@est.fib.upc.edu O

2 B-DAT
Computer O
Vision O
Center O
Campus O

UAB, B-DAT
Bellaterra, O
Spain O
Mohammad O
Ali O
Bagheri3,4 O

mohammadali.bagheri@ucalgary.ca B-DAT
3 O
Dept. O
Electrical O
and O
Computer O

Eng. B-DAT
University O
of O
Calgary, O
Canada O

Sergio B-DAT
Escalera1,2 O
sergio@maia.ub.es O

4 B-DAT
Dept. O
Engineering O
University O
of O

Larestan, B-DAT
Iran O
Abstract O

Gaze B-DAT
behavior O
is O
an O
important O

non-verbal B-DAT
cue O
in O
social O
signal O

processing B-DAT
and O
human- O
computer O
interaction. O

In B-DAT
this O
paper, O
we O
tackle O

the B-DAT
problem O
of O
person- O
and O

head B-DAT
pose- O
independent O
3D O
gaze O

estimation B-DAT
from O
remote O
cameras, O
using O

a B-DAT
multi-modal O
recurrent O
convolutional O
neural O

network B-DAT
(CNN). O
We O
propose O
to O

combine B-DAT
face, O
eyes O
region, O
and O

face B-DAT
landmarks O
as O
individual O
streams O

in B-DAT
a O
CNN O
to O
estimate O

gaze B-DAT
in O
still O
images. O
Then, O

we B-DAT
exploit O
the O
dynamic O
nature O

of B-DAT
gaze O
by O
feeding O
the O

learned B-DAT
features O
of O
all O
the O

frames B-DAT
in O
a O
sequence O
to O

a B-DAT
many-to-one O
recurrent O
module O
that O

predicts B-DAT
the O
3D O
gaze O
vector O

of B-DAT
the O
last O
frame. O
Our O

multi-modal B-DAT
static O
solution O
is O
evaluated O

on B-DAT
a O
wide O
range O
of O

head B-DAT
poses O
and O
gaze O
directions, O

achieving B-DAT
a O
significant O
improvement O
of O
14 O

.6% B-DAT
over O
the O
state O
of O

the B-DAT
art O
on O
EYEDIAP O
dataset, O

further B-DAT
improved O
by O
4% O
when O

the B-DAT
temporal O
modality O
is O
included. O
1 O
Introduction O
Eyes O
and O
their O

movements B-DAT
are O
considered O
an O
important O

cue B-DAT
in O
non-verbal O
behavior O
analysis O

, B-DAT
being O
involved O
in O
many O

cognitive B-DAT
processes O
and O
reflecting O
our O

internal B-DAT
state O
[17]. O
More O
specifically, O

eye B-DAT
gaze O
behavior, O
as O
an O

indicator B-DAT
of O
human O
visual O
attention, O

has B-DAT
been O
widely O
studied O
to O

assess B-DAT
communication O
skills O
[28] O
and O

to B-DAT
identify O
possible O
behavioral O

disorders B-DAT
[9]. O
Therefore, O
gaze O
estimation O

has B-DAT
become O
an O
established O
line O

of B-DAT
research O
in O
computer O
vision, O

being B-DAT
a O
key O
feature O
in O

human-computer B-DAT
interaction O
(HCI) O
and O
usability O

research B-DAT
[12, O
20]. O
Recent O
gaze O
estimation O
research O
has O

focused B-DAT
on O
facilitating O
its O
use O

in B-DAT
general O
everyday O
applications O
under O

real-world B-DAT
conditions, O
using O
off-the-shelf O
remote O

RGB B-DAT
cameras O
and O
re- O
moving O

the B-DAT
need O
of O
personal O
calibration O

[26]. B-DAT
In O
this O
setting, O
appearance-based O

methods, B-DAT
which O
learn O
a O
mapping O

from B-DAT
images O
to O
gaze O
directions O

, B-DAT
are O
the O
preferred O

choice B-DAT
[25]. O
How- O
ever, O
they O

need B-DAT
large O
amounts O
of O
training O

data B-DAT
to O
be O
able O
to O

generalize B-DAT
well O
to O
in-the-wild O
situations, O

which B-DAT
are O
characterized O
by O
significant O

variability B-DAT
in O
head O
poses, O
face O

appearances B-DAT
and O
lighting O
conditions. O
In O

recent B-DAT
years, O
CNNs O
have O
been O

reported B-DAT
to O
outperform O
classical O
methods. O

However, B-DAT
most O
existing O
approaches O
have O

only B-DAT
been O
tested O
in O
restricted O

HCI B-DAT
tasks, O
c© O
2018. O
The O
copyright O
of O

this B-DAT
document O
resides O
with O
its O

authors. B-DAT
It O
may O
be O
distributed O

unchanged B-DAT
freely O
in O
print O
or O

electronic B-DAT
forms O

. B-DAT
ar O
X O

iv B-DAT
:1 O
80 O
5 O

. B-DAT
03 O
06 O

4v B-DAT
3 O

cs B-DAT
.C O
V O

1 B-DAT
7 O

Se B-DAT
p O

20 B-DAT
18 O

Citation B-DAT
Citation O
{Liversedge O
and O
Findlay} O
2000 O

Citation B-DAT
Citation O
{Rutter O
and O
Durkin} O
1987 O

Citation B-DAT
Citation O
{Guillon, O
Hadjikhani, O
Baduel, O

and B-DAT
Rog{é}} O
2014 O
Citation O
Citation O
{Jacob O
and O
Karn O

} B-DAT
2003 O
Citation O
Citation O
{Majaranta O
and O
Bulling O

} B-DAT
2014 O
Citation O
Citation O
{Palmero, O
van O
Dam O

, B-DAT
Escalera, O
Kelia, O
Lichtert, O
Noldus, O

Spink, B-DAT
and O
van O
Wieringen} O
2018 O
Citation O
Citation O
{Ono, O
Okabe, O
and O

Sato} B-DAT
2006 O

2 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
Method O
3D O
gaze O
direction O

Unrestricted B-DAT
gaze O
target O
Full O
face O

Eye B-DAT
region O
Facial O
landmarks O

Sequential B-DAT
information O
Zhang O
et O
al. O
(1) O
[42 O

] B-DAT
3 O
7 O
7 O
3 O
7 O
7 O
Krafka O
et O
al. O
[16 O

] B-DAT
7 O
7 O
3 O
3 O
7 O
7 O
Zhang O
et O
al. O
(2 O

) B-DAT
[43] O
3 O
7 O
3 O
7 O
7 O
7 O
Deng O
and O
Zhu O

[4] B-DAT
3 O
3 O
3 O
3 O

7 B-DAT
7 O
Ours O
3 O
3 O

3 B-DAT
3 O
3 O
3 O

Table B-DAT
1: O
Characteristics O
of O
recent O

related B-DAT
work O
on O
person- O
and O

head B-DAT
pose-independent O
appearance-based O
gaze O
estimation O

methods B-DAT
using O
CNNs. O
where O
users O
look O
at O
the O

screen B-DAT
or O
mobile O
phone, O
showing O

a B-DAT
low O
head O
pose O
variability O

. B-DAT
It O
is O
yet O
unclear O

how B-DAT
these O
methods O
would O
perform O

in B-DAT
a O
wider O
range O
of O

head B-DAT
poses. O
On O
a O
different O
note, O
until O

very B-DAT
recently, O
the O
majority O
of O

methods B-DAT
only O
used O
static O
eye O

region B-DAT
appearance O
as O
input. O
State-of-the-art O

approaches B-DAT
have O
demonstrated O
that O
using O

the B-DAT
face O
along O
with O
a O

higher B-DAT
resolution O
image O
of O
the O

eyes B-DAT
[16], O
or O
even O
just O

the B-DAT
face O
itself O
[43], O
increases O

performance. B-DAT
Indeed, O
the O
whole-face O
image O

encodes B-DAT
more O
information O
than O
eyes O

alone, B-DAT
such O
as O
illumination O
and O

head B-DAT
pose. O
Nevertheless, O
gaze O
behavior O

is B-DAT
not O
static. O
Eye O
and O

head B-DAT
movements O
allow O
us O
to O

direct B-DAT
our O
gaze O
to O
target O

locations B-DAT
of O
interest. O
It O
has O

been B-DAT
demonstrated O
that O
humans O
can O

better B-DAT
predict O
gaze O
when O
being O

shown B-DAT
image O
sequences O
of O
other O

people B-DAT
moving O
their O
eyes O
[1 O

]. B-DAT
However, O
it O
is O
still O

an B-DAT
open O
question O
whether O
this O

se- B-DAT
quential O
information O
can O
increase O

the B-DAT
performance O
of O
automatic O
methods. O
In O
this O
work, O
we O
show O

that B-DAT
the O
combination O
of O
multiple O

cues B-DAT
benefits O
the O
gaze O
estimation O

task. B-DAT
In O
particular, O
we O
use O

face, B-DAT
eye O
region O
and O
facial O

landmarks B-DAT
from O
still O
images. O
Facial O

landmarks B-DAT
model O
the O
global O
shape O

of B-DAT
the O
face O
and O
come O

at B-DAT
no O
cost, O
since O
face O

alignment B-DAT
is O
a O
common O
pre-processing O

step B-DAT
in O
many O
facial O
image O

analysis B-DAT
approaches. O
Furthermore, O
we O
present O

a B-DAT
subject-independent, O
free-head O
recurrent O
3D O

gaze B-DAT
regression O
network O
to O
leverage O

the B-DAT
temporal O
information O
of O
image O

sequences. B-DAT
The O
static O
streams O
of O

each B-DAT
frame O
are O
combined O
in O

a B-DAT
late-fusion O
fashion O
using O
a O

multi-stream B-DAT
CNN. O
Then, O
all O
feature O

vectors B-DAT
are O
input O
to O
a O

many-to-one B-DAT
recurrent O
module O
that O
predicts O

the B-DAT
gaze O
vector O
of O
the O

last B-DAT
sequence O
frame O

. B-DAT
In O
summary, O
our O
contributions O
are O

two-fold. B-DAT
First, O
we O
present O
a O

Recurrent-CNN B-DAT
net- O
work O
architecture O
that O

combines B-DAT
appearance, O
shape O
and O
temporal O

information B-DAT
for O
3D O
gaze O
estimation O

. B-DAT
Second, O
we O
test O
static O

and B-DAT
temporal O
versions O
of O
our O

solution B-DAT
on O
the O
EYEDIAP O

dataset B-DAT
[7] O
in O
a O
wide O

range B-DAT
of O
head O
poses O
and O

gaze B-DAT
directions, O
showing O
consistent O
perfor- O

mance B-DAT
improvements O
compared O
to O
related O

appearance-based B-DAT
methods. O
To O
the O
best O

of B-DAT
our O
knowledge, O
this O
is O

the B-DAT
first O
third-person, O
remote O
camera-based O

approach B-DAT
that O
uses O
tempo- O
ral O

information B-DAT
for O
this O
task. O
Table O
1 O
outlines O
our O
main O
method O
characteristics O

compared B-DAT
to O
related O
work. O
Models O

and B-DAT
code O
are O
publicly O
available O

at B-DAT
https://github.com/ O
crisie/RecurrentGaze O

. B-DAT
2 O
Related O
work O
Gaze O
estimation O

methods B-DAT
are O
typically O
categorized O
as O

model-based B-DAT
or O
appearance-based O
[5, O
10 O

, B-DAT
15]. O
Model-based O
approaches O
use O

a B-DAT
geometric O
model O
of O
the O

eye, B-DAT
usually O
requir- O
ing O
either O

high B-DAT
resolution O
images O
or O
a O

person-specific B-DAT
calibration O
stage O
to O
estimate O

personal B-DAT
eye O
parameters O
[22, O
33, O
34, O
37, O
41]. O
In O
contrast, O
appearance-based O

methods B-DAT
learn O
a O
di- O
rect O

mapping B-DAT
from O
intensity O
images O
or O

extracted B-DAT
eye O
features O
to O
gaze O

directions, B-DAT
thus O
being O

Citation B-DAT
Citation O
{Zhang, O
Sugano, O
Fritz, O

and B-DAT
Bulling} O
2015 O
Citation O
Citation O
{Krafka, O
Khosla, O
Kellnhofer O

, B-DAT
Kannan, O
Bhandarkar, O
Matusik, O
and O

Torralba} B-DAT
2016 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2017 O
Citation O
Citation O
{Deng O
and O
Zhu O

} B-DAT
2017 O
Citation O
Citation O
{Krafka, O
Khosla, O
Kellnhofer O

, B-DAT
Kannan, O
Bhandarkar, O
Matusik, O
and O

Torralba} B-DAT
2016 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2017 O
Citation O
Citation O
{Anderson, O
Risko, O
and O

Kingstone} B-DAT
2016 O

Citation B-DAT
Citation O
{Funesprotect O
unhbox O
voidb@x O

penalty B-DAT
@M O
{}Mora, O
Monay, O
and O
Odobez} O
2014 O

{} B-DAT
Citation O
Citation O
{Ferhat O
and O
Vilari{ñ}o O

} B-DAT
2016 O
Citation O
Citation O
{Hansen O
and O
Ji O

} B-DAT
2010 O
Citation O
Citation O
{Kar O
and O
Corcoran O

} B-DAT
2017 O
Citation O
Citation O
{Morimoto, O
Amir, O
and O

Flickner} B-DAT
2002 O

Citation B-DAT
Citation O
{Venkateswarlu O
etprotect O
unhbox O

voidb@x B-DAT
penalty O
@M O
{}al.} O
2003 O

Citation B-DAT
Citation O
{Wang O
and O
Ji} O
2017 O

Citation B-DAT
Citation O
{Wood O
and O
Bulling} O
2014 O

Citation B-DAT
Citation O
{Yoo O
and O
Chung} O
2005 O

https://github.com/crisie/RecurrentGaze B-DAT

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
3 O

potentially B-DAT
applicable O
to O
relatively O
low O

resolution B-DAT
images O
and O
mid-distance O
scenarios. O

Dif- B-DAT
ferent O
mapping O
functions O
have O

been B-DAT
explored, O
such O
as O
neural O

networks B-DAT
[2], O
adaptive O
linear O
regression O
( O

ALR) B-DAT
[19], O
local O
interpolation O
[32], O

gaussian B-DAT
processes O
[30, O
35], O
random O

forests B-DAT
[11, O
31], O
or O
k-nearest O

neighbors B-DAT
[40]. O
Main O
challenges O
of O

appearance-based B-DAT
methods O
for O
3D O
gaze O

estimation B-DAT
are O
head O
pose, O
illumination O

and B-DAT
subject O
invariance O
without O
user-specific O

calibration. B-DAT
To O
handle O
these O
issues, O

some B-DAT
works O
proposed O
compensation O

methods B-DAT
[18] O
and O
warping O
strategies O

that B-DAT
synthesize O
a O
canonical, O
frontal O

looking B-DAT
view O
of O
the O

face B-DAT
[6, O
13, O
21]. O
Hybrid O

approaches B-DAT
based O
on O
analysis-by-synthesis O
have O

also B-DAT
been O
evaluated O
[39]. O
Currently, O
data-driven O
methods O
are O
considered O

the B-DAT
state O
of O
the O
art O

for B-DAT
person- O
and O
head O
pose-independent O

appearance-based B-DAT
gaze O
estimation. O
Consequently, O
a O

number B-DAT
of O
gaze O
es- O
timation O

datasets B-DAT
have O
been O
introduced O
in O

recent B-DAT
years, O
either O
in O
controlled O

[29] B-DAT
or O
semi- O
controlled O
settings O

[8], B-DAT
in O
the O
wild O
[16 O

, B-DAT
42], O
or O
consisting O
of O

synthetic B-DAT
data O
[31, O
38, O
40]. O

Zhang B-DAT
et O
al. O
[42] O
showed O

that B-DAT
CNNs O
can O
outperform O
other O

mapping B-DAT
methods, O
using O
a O
multi- O

modal B-DAT
CNN O
to O
learn O
the O

mapping B-DAT
from O
3D O
head O
poses O

and B-DAT
eye O
images O
to O
3D O

gaze B-DAT
directions. O
Krafka O
et O

al. B-DAT
[16] O
proposed O
a O
multi-stream O

CNN B-DAT
for O
2D O
gaze O
estimation, O

using B-DAT
individual O
eye, O
whole-face O
image O

and B-DAT
the O
face O
grid O
as O

input. B-DAT
As O
this O
method O
was O

limited B-DAT
to O
2D O
screen O
mapping, O

Zhang B-DAT
et O
al. O
[43] O
later O

explored B-DAT
the O
potential O
of O
just O

using B-DAT
whole-face O
images O
as O
input O

to B-DAT
estimate O
3D O
gaze O
directions. O

Using B-DAT
a O
spatial O
weights O
CNN, O

they B-DAT
demonstrated O
their O
method O
to O

be B-DAT
more O
robust O
to O
facial O

appearance B-DAT
variation O
caused O
by O
head O

pose B-DAT
and O
illumina- O
tion O
than O

eye-only B-DAT
methods. O
While O
the O
method O

was B-DAT
evaluated O
in O
the O
wild, O

the B-DAT
subjects O
were O
only O
interacting O

with B-DAT
a O
mobile O
device, O
thus O

restricting B-DAT
the O
head O
pose O
range. O

Deng B-DAT
and O
Zhu O
[4] O
presented O

a B-DAT
two-stream O
CNN O
to O
disjointly O

model B-DAT
head O
pose O
from O
face O

images B-DAT
and O
eye- O
ball O
movement O

from B-DAT
eye O
region O
images. O
Both O

were B-DAT
then O
aggregated O
into O
3D O

gaze B-DAT
direction O
using O
a O
gaze O

transform B-DAT
layer. O
The O
decomposition O
was O

aimed B-DAT
to O
avoid O
head-correlation O
over- O

fitting B-DAT
of O
previous O
data-driven O
approaches. O

They B-DAT
evaluated O
their O
approach O
in O

the B-DAT
wild O
with O
a O
wider O

range B-DAT
of O
head O
poses, O
obtaining O

better B-DAT
performance O
than O
previous O
eye-based O

methods. B-DAT
However, O
they O
did O
not O

test B-DAT
it O
on O
public O
annotated O

benchmark B-DAT
datasets. O
In O
this O
paper, O
we O
propose O

a B-DAT
multi-stream O
recurrent O
CNN O
network O

for B-DAT
person- O
and O
head O
pose-independent O

3D B-DAT
gaze O
estimation O
for O
a O

mid-distance B-DAT
scenario. O
We O
evaluate O
it O

on B-DAT
a O
wider O
range O
of O

head B-DAT
poses O
and O
gaze O
directions O

than B-DAT
screen-targeted O
approaches. O
As O
opposed O

to B-DAT
previous O
methods, O
we O
also O

rely B-DAT
on O
temporal O
information O
inherent O

in B-DAT
sequential O
data O

. B-DAT
3 O
Methodology O

In B-DAT
this O
section, O
we O
present O

our B-DAT
approach O
for O
3D O
gaze O

regression B-DAT
based O
on O
appearance O
and O

shape B-DAT
cues O
for O
still O
images O

and B-DAT
image O
sequences. O
First, O
we O

introduce B-DAT
the O
data O
modalities O
and O

formulate B-DAT
the O
problem. O
Then, O
we O

detail B-DAT
the O
normalization O
procedure O
prior O

to B-DAT
the O
regression O
stage. O
Finally, O

we B-DAT
explain O
the O
global O
network O

topology B-DAT
as O
well O
as O
the O

implementation B-DAT
details. O
An O
overview O
of O

the B-DAT
system O
architecture O
is O
depicted O

in B-DAT
Figure O
1. O
3.1 O
Multi-modal O
gaze O
regression O

Let B-DAT
us O
represent O
gaze O
direction O

as B-DAT
a O
3D O
unit O
vector O

g B-DAT
= O
[gx,gy,gz]T O
∈R3 O
in O

the B-DAT
Camera O
Coor- O
dinate O
System O
( O

CCS), B-DAT
whose O
origin O
is O
the O

central B-DAT
point O
between O
eyeball O
centers. O

Assuming B-DAT
a O
calibrated O
camera, O
and O

a B-DAT
known O
head O
position O
and O

orientation, B-DAT
our O
goal O
is O
to O

estimate B-DAT
g O
from O
a O
sequence O

of B-DAT
images O
{I(i) O
| O

I B-DAT
∈ O
RW×H×3} O
as O
a O

regression B-DAT
problem. O
Citation O
Citation O
{Baluja O
and O
Pomerleau O

} B-DAT
1994 O
Citation O
Citation O
{Lu, O
Sugano, O
Okabe O

, B-DAT
and O
Sato} O
2011{} O
Citation O
Citation O
{Tan, O
Kriegman, O
and O

Ahuja} B-DAT
2002 O

Citation B-DAT
Citation O
{Sugano, O
Matsushita, O
and O

Sato} B-DAT
2013 O
Citation O
Citation O
{Williams, O
Blake, O
and O

Cipolla} B-DAT
2006 O

Citation B-DAT
Citation O
{Huang, O
Veeraraghavan, O
and O

Sabharwal} B-DAT
2017 O
Citation O
Citation O
{Sugano, O
Matsushita, O
and O

Sato} B-DAT
2014 O

Citation B-DAT
Citation O
{Wood, O
Baltru{²}aitis, O
Morency, O

Robinson, B-DAT
and O
Bulling} O
2016{} O
Citation O
Citation O
{Lu, O
Okabe, O
Sugano O

, B-DAT
and O
Sato} O
2011{} O
Citation O
Citation O
{Funes-Mora O
and O
Odobez O

} B-DAT
2016 O
Citation O
Citation O
{Jeni O
and O
Cohn O

} B-DAT
2016 O
Citation O
Citation O
{Mora O
and O
Odobez O

} B-DAT
2012 O
Citation O
Citation O
{Wood, O
Baltru{²}aitis, O
Morency O

, B-DAT
Robinson, O
and O
Bulling} O
2016{} O
Citation O
Citation O
{Smith, O
Yin, O
Feiner O

, B-DAT
and O
Nayar} O
2013 O
Citation O
Citation O
{Funesprotect O
unhbox O
voidb@x O

penalty B-DAT
@M O

Mora, B-DAT
Monay, O
and O
Odobez} O
2014{} O
Citation O
Citation O
{Krafka, O
Khosla, O
Kellnhofer O

, B-DAT
Kannan, O
Bhandarkar, O
Matusik, O
and O

Torralba} B-DAT
2016 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2015 O
Citation O
Citation O
{Sugano, O
Matsushita, O
and O

Sato} B-DAT
2014 O

Citation B-DAT
Citation O
{Wood, O
Baltrusaitis, O
Zhang, O

Sugano, B-DAT
Robinson, O
and O
Bulling} O
2015 O
Citation O
Citation O
{Wood, O
Baltru{²}aitis, O
Morency O

, B-DAT
Robinson, O
and O
Bulling} O
2016{} O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2015 O
Citation O
Citation O
{Krafka, O
Khosla, O
Kellnhofer O

, B-DAT
Kannan, O
Bhandarkar, O
Matusik, O
and O

Torralba} B-DAT
2016 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2017 O
Citation O
Citation O
{Deng O
and O
Zhu O

} B-DAT
2017 O

4 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
Conv O

C B-DAT
on O
ca O
t O
x O
y O
z O
x O
y O

z B-DAT
x O
y O
z O

Individual B-DAT
Fusion O
Temporal O
Individual O
Fusion O

Input B-DAT

Individual B-DAT
Fusion O
Normalization O

.Conv B-DAT

Conv B-DAT
. O
Conv O

Conv B-DAT
. O
FC O

FC B-DAT
FC O
RNN O
RNN O

RNN B-DAT
FC O
Ti O
m O
e O

Figure B-DAT
1: O
Overview O
of O
the O

proposed B-DAT
network. O
A O
multi-stream O
CNN O

jointly B-DAT
models O
full-face, O
eye O
region O

appearance B-DAT
and O
face O
landmarks O
from O

still B-DAT
images. O
The O
combined O
extracted O

fea- B-DAT
tures O
from O
each O
frame O

are B-DAT
fed O
into O
a O
recurrent O

module B-DAT
to O
predict O
last O
frame’s O

gaze B-DAT
direction. O
Gazing O
to O
a O
specific O
target O

is B-DAT
achieved O
by O
a O
combination O

of B-DAT
eye O
and O
head O
movements O

, B-DAT
which O
are O
highly O
coordinated. O

Consequently, B-DAT
the O
apparent O
direction O
of O

gaze B-DAT
is O
influenced O
not O
only O

by B-DAT
the O
location O
of O
the O

irises B-DAT
within O
the O
eyelid O
aperture, O

but B-DAT
also O
by O
the O
position O

and B-DAT
orientation O
of O
the O
face O

with B-DAT
respect O
to O
the O
camera. O

Known B-DAT
as O
the O
Wollaston O

effect B-DAT
[36], O
the O
exact O
same O

set B-DAT
of O
eyes O
may O
appear O

to B-DAT
be O
looking O
in O
different O

directions B-DAT
due O
to O
the O
surrounding O

facial B-DAT
cues. O
It O
is O
therefore O

reasonable B-DAT
to O
state O
that O
eye O

images B-DAT
are O
not O
sufficient O
to O

estimate B-DAT
gaze O
direction. O
Instead, O
whole-face O

images B-DAT
can O
encode O
head O
pose O

or B-DAT
illumination-specific O
information O
across O
larger O

areas B-DAT
than O
those O
available O
just O

in B-DAT
the O
eyes O
region O
[16, O
43 O

]. B-DAT
The O
drawback O
of O
appearance-only O
methods O

is B-DAT
that O
global O
structure O
information O

is B-DAT
not O
explicitly O
considered. O
In O

that B-DAT
sense, O
facial O
landmarks O
can O

be B-DAT
used O
as O
global O
shape O

cues B-DAT
to O
en- O
code O
spatial O

relationships B-DAT
and O
geometric O
constraints. O
Current O

state-of-the-art B-DAT
face O
alignment O
approaches O
are O

robust B-DAT
enough O
to O
handle O
large O

appearance B-DAT
variability, O
extreme O
head O
poses O

and B-DAT
occlusions, O
being O
especially O
useful O

when B-DAT
the O
dataset O
used O
for O

gaze B-DAT
estimation O
does O
not O
contain O

such B-DAT
variability. O
Facial O
landmarks O
are O

mainly B-DAT
correlated O
with O
head O
orientation O

, B-DAT
eye O
position, O
eyelid O
openness, O

and B-DAT
eyebrow O
movement, O
which O
are O

valuable B-DAT
features O
for O
our O
task. O
Therefore, O
in O
our O
approach O
we O

jointly B-DAT
model O
appearance O
and O
shape O

cues B-DAT
(see O
Figure O
1). O
The O

former B-DAT
is O
represented O
by O
a O

whole-face B-DAT
image O
IF O
, O
along O

with B-DAT
a O
higher O
resolution O
image O

of B-DAT
the O
eyes O
IE O
to O

identify B-DAT
subtle O
changes. O
Due O
to O

dealing B-DAT
with O
wide O
head O
pose O

ranges, B-DAT
some O
eye O
images O
may O

not B-DAT
depict O
the O
whole O
eye O

, B-DAT
containing O
mostly O
background O
or O

other B-DAT
surrounding O
facial O
parts O
instead. O

For B-DAT
that O
reason, O
and O
contrary O

to B-DAT
previous O
approaches O
that O
only O

use B-DAT
one O
eye O
image O
[31, O
42 O

], B-DAT
we O
use O
a O
single O

image B-DAT
composed O
of O
two O
patches O

of B-DAT
centered O
left O
and O
right O

eyes. B-DAT
Finally, O
the O
shape O
cue O

is B-DAT
represented O
by O
3D O
face O

landmarks B-DAT
obtained O
from O
a O
68-landmark O

model, B-DAT
denoted O
by O

L B-DAT
= O
{(lx, O
ly, O

) B-DAT

| B-DAT
∀c O
∈ O
[1, O
...,68 O

]}. B-DAT
In O
this O
work O
we O
also O

consider B-DAT
the O
dynamic O
component O
of O

gaze. B-DAT
We O
leverage O
the O
se O

- B-DAT
quential O
information O
of O
eye O

and B-DAT
head O
movements O
such O
that, O

given B-DAT
appearance O
and O
shape O
features O

of B-DAT
consecutive O
frames, O
it O
is O

possible B-DAT
to O
better O
predict O
the O

gaze B-DAT
direction O
of O
the O
cur- O

rent B-DAT
frame. O
Therefore, O
the O
3D O

gaze B-DAT
estimation O
task O
for O
a O
1 O

-frame B-DAT
sequence O
is O
formulated O
Citation O
Citation O
{Wollaston O
etprotect O
unhbox O

voidb@x B-DAT
penalty O
@M O

al.} B-DAT
1824 O
Citation O
Citation O
{Krafka, O
Khosla, O
Kellnhofer O

, B-DAT
Kannan, O
Bhandarkar, O
Matusik, O
and O

Torralba} B-DAT
2016 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2017 O
Citation O
Citation O
{Sugano, O
Matsushita, O
and O

Sato} B-DAT
2014 O

Citation B-DAT
Citation O
{Zhang, O
Sugano, O
Fritz, O

and B-DAT
Bulling} O
2015 O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
5 O

as B-DAT
g(i) O
= O
f O
( O
{IF O
(i)},{IE O
(i)},{L(i O

)} B-DAT
) O
, O
where O
i O
denotes O

the B-DAT
i-th O
frame, O
and O
f O

is B-DAT
the O
regression O

function. B-DAT
3.2 O
Data O
normalization O
Prior O
to O

gaze B-DAT
regression, O
a O
normalization O
step O

in B-DAT
the O
3D O
space O
and O

the B-DAT
2D O
image, O
similar O
to O

[31], B-DAT
is O
carried O
out. O
This O

is B-DAT
performed O
to O
reduce O
the O

appearance B-DAT
variability O
and O
to O
allow O

the B-DAT
gaze O
estimation O
model O
to O

be B-DAT
applied O
regardless O
of O
the O

original B-DAT
camera O
configuration O

. B-DAT
Let O
H O
∈ O
R3x3 O
be O

the B-DAT
head O
rotation O
matrix, O
and O

p B-DAT
= O
[px, O
py, O
pz]T O

∈ B-DAT
R3 O
the O
reference O
face O

location B-DAT
with O
respect O
to O
the O

original B-DAT
CCS. O
The O
goal O
is O

to B-DAT
find O
the O
conversion O
matrix O

M B-DAT
= O
SR O
such O
that O

(a) B-DAT
the O
X-axes O
of O
the O

virtual B-DAT
camera O
and O
the O
head O

become B-DAT
parallel O
using O
the O
rotation O

matrix B-DAT
R, O
and O
(b) O
the O

virtual B-DAT
camera O
looks O
at O
the O

reference B-DAT
location O
from O
a O
fixed O

distance B-DAT
dn O
using O
the O
Z-direction O

scaling B-DAT
matrix O
S O
= O
diag(1,1,dn/‖p O

‖). B-DAT
R O
is O
computed O
as O

a B-DAT
= O
p̂×HT O
e1, O

b B-DAT
= O
â× O
p̂, O

R B-DAT
= O
[â, O
b̂, O
p̂]T O
, O
where O
e1 O
denotes O
the O
first O

orthonormal B-DAT
basis O
and O

〈 B-DAT
·̂ O
〉 O
is O
the O

unit B-DAT
vector O

. B-DAT
This O
normalization O
translates O
into O
the O

image B-DAT
space O
as O
a O
cropped O

image B-DAT
patch O
of O
size O
Wn×Hn O

centered B-DAT
at O
p O
where O
head O

roll B-DAT
rotation O
has O
been O
removed O

. B-DAT
This O
is O
done O
by O

applying B-DAT
a O
perspective O
warping O
to O

the B-DAT
input O
image O
I O
using O

the B-DAT
transformation O
matrix O
W O
= O

CoMCn−1, B-DAT
where O
Co O
and O
Cn O

are B-DAT
the O
original O
and O
virtual O

camera B-DAT
matrices, O
respectively. O
The O
3D O
gaze O
vector O
is O

also B-DAT
normalized O
as O
gn O
=Rg O

. B-DAT
After O
image O
normalization, O
the O

line B-DAT
of O
sight O
can O
be O

represented B-DAT
in O
a O
2D O
space. O

Therefore, B-DAT
gn O
is O
further O
transformed O

to B-DAT
spherical O
coor- O
dinates O
(θ O
, O

φ) B-DAT
assuming O
unit O
length, O
where O

θ B-DAT
and O
φ O
denote O
the O

horizontal B-DAT
and O
vertical O
direc- O
tion O

angles, B-DAT
respectively. O
This O
2D O
angle O

representation, B-DAT
delimited O
in O
the O

range B-DAT
[−π/2,π/2], O
is O
computed O
as O

θ B-DAT
= O
arctan(gx/gz) O
and O

φ B-DAT
= O
arcsin(−gy), O
such O
that O
(0, O

0) B-DAT
represents O
looking O
straight O
ahead O

to B-DAT
the O
CCS O
origin. O
3.3 O
Recurrent O
Convolutional O
Neural O
Network O

We B-DAT
propose O
a O
Recurrent O
CNN O

Regression B-DAT
Network O
for O
3D O
gaze O

estimation. B-DAT
The O
network O
is O
divided O

in B-DAT
3 O
modules: O
(1) O
Individual O

, B-DAT
(2) O
Fusion, O
and O
(3) O

Temporal. B-DAT
First, O
the O
Individual O
module O
learns O

features B-DAT
from O
each O
appearance O
cue O

separately. B-DAT
It O
consists O
of O
a O

two-stream B-DAT
CNN, O
one O
devoted O
to O

the B-DAT
normalized O
face O
image O
stream O

and B-DAT
the O
other O
to O
the O

joint B-DAT
normalized O
eyes O
image. O
Next O

, B-DAT
the O
Fusion O
module O
combines O

the B-DAT
extracted O
features O
of O
each O

appearance B-DAT
stream O
in O
a O
single O

vector B-DAT
along O
with O
the O
normalized O

landmark B-DAT
coordinates. O
Then, O
it O
learns O

a B-DAT
joint O
representation O
between O
modalities O

in B-DAT
a O
late-fusion O
fashion. O
Both O

Individual B-DAT
and O
Fusion O
modules, O
further O

referred B-DAT
to O
as O
Static O
model, O

are B-DAT
applied O
to O
each O
frame O

of B-DAT
the O
sequence. O
Finally, O
the O

resulting B-DAT
feature O
vectors O
of O
each O

frame B-DAT
are O
input O
to O
the O

Temporal B-DAT
module O
based O
on O
a O

many-to-one B-DAT
recurrent O
network. O
This O
module O

leverages B-DAT
sequential O
information O
to O
predict O

the B-DAT
normalized O
2D O
gaze O
angles O

of B-DAT
the O
last O
frame O
of O

the B-DAT
sequence O
using O
a O
linear O

regression B-DAT
layer O
added O
on O
top O

of B-DAT
it. O
3.4 O
Implementation O
details O
3.4.1 O
Network O

details B-DAT

Each B-DAT
stream O
of O
the O
Individual O

module B-DAT
is O
based O
on O
the O

VGG-16 B-DAT
deep O
network O
[27], O
consisting O

of B-DAT
13 O
convolutional O
layers, O
5 O

max B-DAT
pooling O
layers, O
and O
1 O

fully B-DAT
connected O
(FC) O
layer O
with O

Rec- B-DAT
tified O
Linear O
Unit O
(ReLU) O

activations. B-DAT
The O
full-face O
stream O
follows O

the B-DAT
same O
configuration O
Citation O
Citation O
{Sugano, O
Matsushita, O
and O

Sato} B-DAT
2014 O

Citation B-DAT
Citation O
{Parkhi, O
Vedaldi, O
and O

Zisserman} B-DAT
2015 O

6 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
as O
the O
base O
network, O
having O

an B-DAT
input O
of O
224×224 O
pixels O

and B-DAT
a O
4096D O
FC O
layer O

. B-DAT
In O
contrast, O
the O
input O

joint B-DAT
eye O
image O
is O
smaller, O

with B-DAT
a O
final O
size O
of O
120 O

×48 B-DAT
pixels, O
so O
the O
number O

of B-DAT
pa- O
rameters O
is O
decreased O

proportionally. B-DAT
In O
this O
case, O
its O

last B-DAT
FC O
layer O
produces O
a O

1536D B-DAT
vector. O
A O
204D O
landmark O

coordinates B-DAT
vector O
is O
concatenated O
to O

the B-DAT
output O
of O
the O
FC O

layer B-DAT
of O
each O
stream, O
resulting O

in B-DAT
a O
5836D O
feature O
vector. O

Consequently, B-DAT
the O
Fusion O
module O
consists O

of B-DAT
2 O
5836D O
FC O
layers O

with B-DAT
ReLU O
activations O
and O
2 O

dropout B-DAT
layers O
between O
FCs O
as O

regularization. B-DAT
Finally, O
to O
model O
the O

temporal B-DAT
dependencies, O
we O
use O
a O

single B-DAT
GRU O
layer O
with O
128 O

units. B-DAT
The O
network O
is O
trained O
in O

a B-DAT
stage-wise O
fashion. O
First, O
we O

train B-DAT
the O
Static O
model O
and O

the B-DAT
final O
regression O
layer O
end-to-end O

on B-DAT
each O
individual O
frame O
of O

the B-DAT
training O
data. O
The O
convolutional O

blocks B-DAT
are O
pre-trained O
with O
the O

VGG-Face B-DAT
dataset O
[27], O
whereas O
the O

FCs B-DAT
are O
trained O
from O
scratch O

. B-DAT
Second, O
the O
training O
data O

is B-DAT
re-arranged O
by O
means O
of O

a B-DAT
sliding O
window O
with O
stride O
1 O
to O
build O
input O
sequences. O
Each O

sequence B-DAT
is O
composed O
of O
s O

= B-DAT
4 O
consecutive O
frames, O
whose O

gaze B-DAT
direction O
target O
is O
the O

gaze B-DAT
direction O
of O
the O
last O

frame B-DAT
of O
the O
sequence( O
{I(i−s+1 O

), B-DAT
. O
. O
. O
,I(i)}, O

g(i) B-DAT
) O
. O
Using O
this O
re-arranged O

training B-DAT
data, O
we O
extract O
features O

of B-DAT
each O

frame B-DAT
of O
the O
sequence O
from O

a B-DAT
frozen O
Individual O
module, O
fine-tune O

the B-DAT
Fusion O
layers, O
and O
train O

both, B-DAT
the O
Temporal O
module O
and O

a B-DAT
new O
final O
regression O
layer O

from B-DAT
scratch. O
This O
way, O
the O

network B-DAT
can O
exploit O
the O
temporal O

information B-DAT
to O
further O
refine O
the O

fusion B-DAT
weights. O
We O
trained O
the O
model O
using O

ADAM B-DAT
optimizer O
with O
an O
initial O

learning B-DAT
rate O
of O
0.0001, O
dropout O

of B-DAT
0.3, O
and O
batch O
size O

of B-DAT
64 O
frames. O
The O
number O

of B-DAT
epochs O
was O
experimentally O
set O

to B-DAT
21 O
for O
the O
first O

training B-DAT
stage O
and O
10 O
for O

the B-DAT
second. O
We O
use O
the O

average B-DAT
Euclidean O
distance O
between O
the O

predicted B-DAT
and O
ground-truth O
3D O
gaze O

vectors B-DAT
as O
loss O
function O

. B-DAT
3.4.2 O
Input O
pre-processing O

For B-DAT
this O
work O
we O
use O

head B-DAT
pose O
and O
eye O
locations O

in B-DAT
the O
3D O
scene O
provided O

by B-DAT
the O
dataset. O
The O
3D O

landmarks B-DAT
are O
extracted O
using O
the O

state-of-the-art B-DAT
method O
of O
Bulat O
and O

Tzimiropou- B-DAT
los O
[3], O
which O
is O

based B-DAT
on O
stacked O
hourglass O

networks B-DAT
[24]. O
During O
training, O
the O
original O
image O

is B-DAT
pre-processed O
to O
get O
the O

two B-DAT
normalized O
input O
images. O
The O

normalized B-DAT
whole-face O
patch O
is O
centered O

0.1 B-DAT
meters O
ahead O
of O
the O

head B-DAT
center O
in O
the O
head O

coordinate B-DAT
system, O
and O
Cn O
is O

defined B-DAT
such O
that O
the O
image O

has B-DAT
size O
of O
250× O
250 O

pixels. B-DAT
The O
difference O
between O
this O

size B-DAT
and O
the O
final O
input O

size B-DAT
allows O
us O
to O
perform O

random B-DAT
cropping O
and O
zooming O
to O

augment B-DAT
the O
data O
(explained O
in O

Section B-DAT
4.1). O
Similarly, O
each O
normalized O

eye B-DAT
patch O
is O
centered O
in O

their B-DAT
respective O
eye O
center O
locations O

. B-DAT
In O
this O
case, O
the O

virtual B-DAT
camera O
matrix O
is O
defined O

so B-DAT
that O
the O
image O
is O

cropped B-DAT
to O
70×58, O
while O
in O

practice B-DAT
the O
final O
patches O
have O

size B-DAT
of O
60×48. O
Landmarks O
are O

normalized B-DAT
using O
the O
same O
procedure O

and B-DAT
further O
pre-processed O
with O
mean O

subtraction B-DAT
and O
min-max O
normalization O
per O

axis. B-DAT
Finally, O
we O
divide O
them O

by B-DAT
a O
scaling O
factor O
w O

such B-DAT
that O
all O
coordinates O
are O

in B-DAT
the O
range O
[0,w]. O
This O

way, B-DAT
all O
concatenated O
feature O
values O

are B-DAT
in O
a O
similar O
range. O

After B-DAT
inference, O
the O
predicted O
normalized O

2D B-DAT
angles O
are O
de-normalized O
back O

to B-DAT
the O
original O
3D O
space. O
4 O
Experiments O
In O
this O
section O

, B-DAT
we O
evaluate O
the O
cross-subject O

3D B-DAT
gaze O
estimation O
task O
on O

a B-DAT
wide O
range O
of O
head O

poses B-DAT
and O
gaze O
directions. O
Furthermore, O

we B-DAT
validate O
the O
effectiveness O
of O

the B-DAT
proposed O
architecture O
comparing O
both O

static B-DAT
and O
temporal O
approaches. O
We O

report B-DAT
the O
error O
in O
terms O

of B-DAT
mean O
angular O
error O
between O

predicted B-DAT
and O
ground-truth O
3D O
gaze O

vectors. B-DAT
Note O
that O
due O
to O

the B-DAT
requirements O
of O
the O
temporal O

model B-DAT
not O
all O
the O
frames O

obtain B-DAT
a O
prediction. O
Therefore, O
for O

a B-DAT
Citation O
Citation O
{Parkhi, O
Vedaldi, O
and O

Zisserman} B-DAT
2015 O

Citation B-DAT
Citation O
{Bulat O
and O
Tzimiropoulos} O
2017 O

Citation B-DAT
Citation O
{Newell, O
Yang, O
and O

Deng} B-DAT
2016 O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
7 O

60 B-DAT
30 O
0 O
30 O
60 O
60 O

30 B-DAT
0 O

30 B-DAT
60 O

100 B-DAT
101 O

102 B-DAT
60 O
30 O
0 O
30 O
60 O

60 B-DAT
30 O

0 B-DAT
30 O

60 B-DAT
100 O

101 B-DAT
102 O

103 B-DAT
60 O
30 O
0 O
30 O
60 O

60 B-DAT
30 O

0 B-DAT
30 O

60 B-DAT
100 O

101 B-DAT
102 O

60 B-DAT
30 O
0 O
30 O
60 O
60 O

30 B-DAT
0 O

30 B-DAT
60 O

100 B-DAT
101 O

102 B-DAT
103 O

a) B-DAT
g O
(FT O
) O
(b) O

h B-DAT
(FT O
) O
(c) O
g O
( O

CS) B-DAT
(d) O
h O
(CS) O
Figure O
2: O
Ground-truth O
eye O
gaze O

g B-DAT
and O
head O
orientation O
h O

distribution B-DAT
on O
the O
filtered O
EYE O

- B-DAT
DIAP O
dataset O
for O
CS O

and B-DAT
FT O
settings, O
in O
terms O

of B-DAT
x- O
and O
y- O
angles. O
fair O
comparison, O
the O
reported O
results O

for B-DAT
static O
models O
disregard O
such O

frames B-DAT
when O
temporal O
models O
are O

included B-DAT
in O
the O
comparison O

. B-DAT
4.1 O
Training O
data O

There B-DAT
are O
few O
publicly O
available O

datasets B-DAT
devoted O
to O
3D O
gaze O

estimation B-DAT
and O
most O
of O
them O

focus B-DAT
on O
HCI O
with O
a O

limited B-DAT
range O
of O
head O
pose O

and B-DAT
gaze O
directions. O
Therefore, O
we O

use B-DAT
VGA O
videos O
from O
the O

publicly-available B-DAT
EYEDIAP O
dataset O
[7] O
to O

perform B-DAT
the O
experimental O
evaluation, O
as O

it B-DAT
is O
currently O
the O
only O

one B-DAT
containing O
video O
sequences O
with O

a B-DAT
wide O
range O
of O
head O

poses B-DAT
and O
showing O
the O
full O

face. B-DAT
This O
dataset O
consists O
of O
3 O

-minute B-DAT
videos O
of O
16 O
subjects O

looking B-DAT
at O
two O
types O
of O

targets: B-DAT
continuous O
screen O
targets O
on O

a B-DAT
fixed O
monitor O
(CS), O
and O

floating B-DAT
physical O
targets O
(FT O
). O

The B-DAT
videos O
are O
further O
divided O

into B-DAT
static O
(S) O
and O
moving O
( O

M) B-DAT
head O
pose O
for O
each O

of B-DAT
the O
subjects. O
Subjects O
12-16 O

were B-DAT
recorded O
with O
2 O
different O

lighting B-DAT
conditions. O
For O
evaluation, O
we O
filtered O
out O

those B-DAT
frames O
that O
fulfilled O
at O

least B-DAT
one O
of O
the O
following O

conditions: B-DAT
(1) O
face O
or O
landmarks O

not B-DAT
detected; O
(2) O
subject O
not O

looking B-DAT
at O
the O
target; O
(3 O

) B-DAT
3D O
head O
pose, O
eyes O

or B-DAT
target O
location O
not O
properly O

recovered; B-DAT
and O
(4) O
eyeball O
rotations O

violating B-DAT
physical O

constraints B-DAT
(|θ O
| O
≤ O
40 O

◦, B-DAT
|φ O
| O
≤ O
30 O

◦) B-DAT
[23]. O
Note O
that O
we O

purposely B-DAT
do O
not O
filter O
eye O

blinking B-DAT
moments O
to O
learn O
their O

dynamics B-DAT
with O
the O
temporal O
model, O

which B-DAT
may O
produce O
some O
outliers O

with B-DAT
a O
higher O
prediction O
error O

due B-DAT
to O
a O
less O
accurate O

ground B-DAT
truth. O
Figure O
2 O
shows O

the B-DAT
distribution O
of O
gaze O
directions O

and B-DAT
head O
poses O
for O
both O

filtered B-DAT
CS O
and O
FT O
cases. O
We O
applied O
data O
augmentation O
to O

the B-DAT
training O
set O
with O
the O

following B-DAT
random O
transforma- O
tions: O
horizontal O

flip, B-DAT
shifts O
of O
up O
to O

5 B-DAT
pixels, O
zoom O
of O
up O

to B-DAT
2%, O
brightness O
changes O
by O

a B-DAT
factor O
in O
the O
range O

[0.4,1.75], B-DAT
and O
additive O
Gaussian O
noise O

with B-DAT
σ2 O
= O
0.03 O

. B-DAT
4.2 O
Evaluation O
of O
static O
modalities O

First, B-DAT
we O
evaluate O
the O
contribution O

of B-DAT
each O
static O
modality O
on O

the B-DAT
FT O
scenario. O
We O
divided O

the B-DAT
16 O
participants O
into O
4 O

groups, B-DAT
such O
that O
appearance O
variability O

was B-DAT
maximized O
while O
maintaining O
a O

similar B-DAT
number O
of O
training O
samples O

per B-DAT
group. O
Each O
static O
model O

was B-DAT
trained O
end-to-end O
performing O
4-fold O

cross-validation B-DAT
using O
different O
combinations O
of O

input B-DAT
modal- O
ities. O
Since O
the O

number B-DAT
of O
fusion O
units O
depends O

on B-DAT
the O
number O
of O
input O

modalities, B-DAT
we O
also O
compare O
different O

fusion B-DAT
layer O
sizes. O
The O
effect O

of B-DAT
data O
normalization O
is O
also O

evaluated B-DAT
by O
training O
a O
not-normalized O

face B-DAT
model O
where O
the O
input O

image B-DAT
is O
the O
face O
bounding O

box B-DAT
with O
square O
size O
the O

maximum B-DAT
distance O
between O
2D O
landmarks. O
Citation O
Citation O
{Funesprotect O
unhbox O
voidb@x O

penalty B-DAT
@M O

Mora, B-DAT
Monay, O
and O
Odobez} O
2014{} O
Citation O
Citation O
{MSC O

8 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
0 O
1 O
2 O
3 O
4 O

5 B-DAT
6 O
7 O
8 O
9 O

10 B-DAT
11 O
An O
gl O

e B-DAT
er O

ro B-DAT
r O
( O
de O
gr O

ee B-DAT
s) O
6.9 O
6.43 O
5.58 O
5.71 O
5.59 O

5.55 B-DAT
5.52 O

OF-4096 B-DAT
NE-1536 O
NF-4096 O
NF-5632 O
NFL-4300 O

NFE-5632 B-DAT
NFEL-5836 O
Figure O
3: O
Performance O
evaluation O
of O

the B-DAT
Static O
network O
using O
different O

input B-DAT
modali- O
ties O
(O O

- B-DAT
Not O
normalized, O
N O

- B-DAT
Normalized, O
F O
- O
Face O

, B-DAT
E O
- O
Eyes, O

L B-DAT
- O
3D O
Landmarks) O
and O

size B-DAT
of O
fusion O
layers O
on O

the B-DAT
FT O
scenario. O
Floating O
Target O
Screen O
Target O
0 O

1 B-DAT
2 O
3 O
4 O
5 O

6 B-DAT
7 O
8 O
9 O

10 B-DAT
11 O
An O
gl O

e B-DAT
er O

ro B-DAT
r O
( O
de O
gr O

ee B-DAT
s) O
6.36 O
5.43 O
5.19 O
4.2 O
3.38 O

3.4 B-DAT

MPIIGaze B-DAT
Static O
Temporal O
Figure O
4: O
Performance O
comparison O
among O

MPIIGaze B-DAT
method O
[42] O
and O
our O

Static B-DAT
and O
Temporal O
versions O
of O

the B-DAT
proposed O
network O
for O
FT O

and B-DAT
CS O
scenarios O

. B-DAT
As O
shown O
in O
Figure O
3 O

, B-DAT
all O
models O
that O
take O

normalized B-DAT
full-face O
information O
as O
input O

achieve B-DAT
better O
performance O
than O
the O

eyes-only B-DAT
model. O
More O
specifically, O
the O

combination B-DAT
of O
face, O
eyes O
and O

landmarks B-DAT
outperforms O
all O
the O
other O

combinations B-DAT
by O
a O
small O
but O

significant B-DAT
margin O
(paired O
Wilcoxon O
test, O

p B-DAT
< O
0.0001). O
The O
standard O

deviation B-DAT
of O
the O
best-performing O
model O

is B-DAT
reduced O
compared O
to O
the O

face B-DAT
and O
eyes O
model, O
suggesting O

a B-DAT
regularizing O
effect O
due O
to O

the B-DAT
addition O
of O
landmarks. O
The O

not-normalized B-DAT
face-only O
model O
shows O
the O

largest B-DAT
error, O
proving O
the O
impact O

of B-DAT
normalization O
to O
reduce O
the O

appearance B-DAT
variability. O
Furthermore, O
our O
results O

indicate B-DAT
that O
the O
increase O
of O

fusion B-DAT
units O
is O
not O
correlated O

with B-DAT
a O
better O
performance. O
4.3 O
Static O
gaze O
regression: O
comparison O

with B-DAT
existing O
methods O

We B-DAT
compare O
our O
best-performing O
static O

model B-DAT
with O
three O
baselines. O
Head: O

Treating B-DAT
the O
head O
pose O
directly O

as B-DAT
gaze O
direction. O
PR-ALR: O
Method O

that B-DAT
relies O
on O
RGB-D O
data O

to B-DAT
rectify O
the O
eye O
images O

viewpoint B-DAT
into O
a O
canonical O
head O

pose B-DAT
using O
a O
3DMM. O
It O

then B-DAT
learns O
an O
RGB O
gaze O

appearance B-DAT
model O
using O
ALR O
[21]. O

Predicted B-DAT
3D O
vectors O
for O
FT-S O

scenario B-DAT
are O
provided O
by O
EYEDIAP O

dataset. B-DAT
MPIIGaze:. O
State-of-the-art O
full-face O
3D O

gaze B-DAT
estimation O
method O
[42]. O
They O

use B-DAT
an O
Alexnet-based O
CNN O
model O

with B-DAT
spatial O
weights O
to O
enhance O

information B-DAT
in O
different O
facial O
regions. O

We B-DAT
fine-tuned O
it O
with O
the O

filtered B-DAT
EYEDIAP O
subsets O
using O
our O

training B-DAT
parameters O
and O
normalization O
procedure. O
In O
addition O
to O
the O
aforementioned O

FT-based B-DAT
evaluation O
setup, O
we O
also O

evaluate B-DAT
our O
method O
on O
the O

CS B-DAT
scenario. O
In O
this O
case O

there B-DAT
are O
only O
14 O
participants O

available, B-DAT
so O
we O
divided O
them O

in B-DAT
5 O
groups O
and O
performed O

5-fold B-DAT
cross-validation. O
In O
Figure O
4 O

we B-DAT
compare O
our O
method O
to O

MPIIGaze, B-DAT
achieving O
a O
statistically O
significant O

improvement B-DAT
of O
14.6% O
and O
19.5 O

% B-DAT
on O
FT O
and O
CS O

scenarios, B-DAT
respectively O
(paired O
Wilcoxon O
test, O

p B-DAT
< O
0.0001). O
We O
can O

observe B-DAT
that O
a O
re- O
stricted O

gaze B-DAT
target O
benefits O
the O
performance O

of B-DAT
all O
methods, O
compared O
to O

a B-DAT
more O
challenging O
unrestricted O
setting O

with B-DAT
a O
wider O
range O
of O

head B-DAT
poses O
and O
gaze O
directions. O
Table O
2 O
provides O
a O
detailed O

comparison B-DAT
on O
every O
participant, O
performing O

leave-one-out B-DAT
cross-validation O
on O
the O
FT O

scenario B-DAT
for O
static O
and O
moving O

head B-DAT
separately. O
Results O
show O
that O

, B-DAT
as O
expected, O
facial O
appearance O

and B-DAT
head O
pose O
have O
a O

noticeable B-DAT
impact O
on O
gaze O
accuracy, O

with B-DAT
average O
error O
differences O
of O

up B-DAT
to O
7.7◦ O
among O
participants. O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2015 O
Citation O
Citation O
{Mora O
and O
Odobez O

} B-DAT
2012 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2015 O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
9 O

Method B-DAT
1 O
2 O
3 O
4 O
5 O
6 O
7 O
8 O
9 O
10 O

11 B-DAT
12 O
13 O
14 O
15 O

16 B-DAT
Avg. O
Head O
23.5 O
22.1 O

20.3 B-DAT
23.6 O
23.2 O
23.2 O
23.6 O

21.2 B-DAT
26.7 O
23.6 O
23.1 O
24.4 O

23.3 B-DAT
24.0 O
24.5 O
22.8 O
23.3 O

PR-ALR B-DAT
12.3 O
12.0 O
12.4 O
11.3 O

15.5 B-DAT
12.9 O
17.9 O
11.8 O
17.3 O

13.4 B-DAT
13.4 O
14.3 O
15.2 O
13.6 O

14.4 B-DAT
14.6 O
13.9 O
MPIIGaze O
5.3 O

5.1 B-DAT
5.7 O
4.7 O
7.3 O
15.1 O

10.8 B-DAT
5.7 O
9.9 O
7.1 O
5.0 O

5.7 B-DAT
7.4 O
3.8 O
4.8 O
5.5 O

6.8 B-DAT
Static O
3.9 O
4.1 O
4.2 O

3.9 B-DAT
6.0 O
6.4 O
7.2 O
3.6 O

7.1 B-DAT
5.0 O
5.7 O
6.7 O
3.9 O

4.7 B-DAT
5.1 O
4.2 O
5.1 O
Temporal O

4.0 B-DAT
4.9 O
4.3 O
4.1 O
6.1 O

6.5 B-DAT
6.6 O
3.9 O
7.8 O
6.1 O

4.7 B-DAT
5.6 O
4.7 O
3.5 O
5.9 O

4.6 B-DAT
5.2 O
Head O
19.3 O
14.2 O

16.4 B-DAT
19.9 O
16.8 O
21.9 O
16.1 O

24.2 B-DAT
20.3 O
19.9 O
18.8 O
22.3 O

18.1 B-DAT
14.9 O
16.2 O
19.3 O
18.7 O

MPIIGaze B-DAT
7.6 O
6.2 O
5.7 O
8.7 O

10.1 B-DAT
12.0 O
12.2 O
6.1 O
8.3 O

5.9 B-DAT
6.1 O
6.2 O
7.4 O
4.7 O

4.4 B-DAT
6.0 O
7.3 O
Static O
5.8 O

5.7 B-DAT
4.4 O
7.5 O
6.7 O
8.8 O

11.6 B-DAT
5.5 O
8.3 O
5.5 O
5.2 O

6.3 B-DAT
5.3 O
3.9 O
4.3 O
5.6 O

6.3 B-DAT
Temporal O
6.1 O
5.6 O
4.5 O

7.5 B-DAT
6.4 O
8.2 O
12.0 O
5.0 O

7.5 B-DAT
5.4 O
5.0 O
5.8 O
6.6 O

4.0 B-DAT
4.5 O
5.8 O
6.2 O

Table B-DAT
2: O
Gaze O
angular O
error O

comparison B-DAT
for O
static O
(top O
half) O

and B-DAT
moving O
(bottom O
half) O
head O

pose B-DAT
for O
each O
subject O
in O

the B-DAT
FT O
scenario. O
Best O
results O

in B-DAT
bold. O
−80 O
−40 O
0 O
40 O
80−80 O

40 B-DAT
0 O

40 B-DAT
80 O

0 B-DAT
5 O

10 B-DAT
15 O

20 B-DAT
25 O

30 B-DAT
35 O

80 B-DAT
−40 O
0 O
40 O
80−80 O
−40 O

0 B-DAT
40 O

80 B-DAT
−10 O

8 B-DAT
−6 O

4 B-DAT
−2 O

0 B-DAT
2 O

4 B-DAT
6 O

8 B-DAT
10 O

80 B-DAT
−40 O
0 O
40 O
80−80 O
−40 O

0 B-DAT
40 O

80 B-DAT
0 O

5 B-DAT
10 O

15 B-DAT
20 O

25 B-DAT
30 O

35 B-DAT
−80 O
−40 O
0 O
40 O
80−80 O

40 B-DAT
0 O

40 B-DAT
80 O

10 B-DAT
−8 O

6 B-DAT
−4 O

2 B-DAT
0 O

2 B-DAT
4 O

6 B-DAT
8 O

10 B-DAT
(a) O
Gaze O
space O
(b) O
Head O

orientation B-DAT
space O

Figure B-DAT
5: O
Angular O
error O
distribution O

across B-DAT
gaze O
(a) O
and O
head O

orientation B-DAT
(b) O
spaces O
in O
the O

FT B-DAT
setting, O
in O
terms O
of O

x- B-DAT
and O
y- O
angles. O
For O

each B-DAT
space, O
we O
depict O
the O

Static B-DAT
model O
performance O
(left) O
and O

the B-DAT
contribution O
of O
the O
Temporal O

model B-DAT
versus O
Static O
(right). O
In O

the B-DAT
latter, O
positive O
difference O
means O

higher B-DAT
improvement O
of O
the O
Temporal O

model. B-DAT
4.4 O
Evaluation O
of O
the O
temporal O

network B-DAT

In B-DAT
this O
section, O
we O
evaluate O

the B-DAT
contribution O
of O
adding O
the O

temporal B-DAT
module O
to O
the O
static O

model. B-DAT
To O
do O
so, O
we O

trained B-DAT
a O
lower-dimensional O
version O
of O

the B-DAT
static O
network O
with O
compa- O

rable B-DAT
performance O
to O
the O
original, O

reducing B-DAT
the O
number O
of O
units O

of B-DAT
the O
second O
fusion O
layer O

to B-DAT
2918. O
Results O
are O
reported O

in B-DAT
Figure O
4 O
and O
Table O
2 O

. B-DAT
One O
can O
observe O
that O

using B-DAT
sequential O
information O
is O
helpful O

on B-DAT
the O
FT O
scenario, O
outperforming O

the B-DAT
static O
model O
by O
a O

statistically B-DAT
significant O
4.4% O
(paired O
Wilcoxon O

test, B-DAT
p O
< O
0.0001). O
This O

contribution B-DAT
is O
more O
noticeable O
in O

the B-DAT
moving O
head O
setting, O
proving O

that B-DAT
the O
temporal O
model O
can O

benefit B-DAT
from O
head O
motion O
information. O

In B-DAT
contrast, O
such O
information O
seems O

to B-DAT
be O
less O
meaningful O
in O

the B-DAT
CS O
scenario, O
where O
the O

obtained B-DAT
error O
is O
already O
very O

low B-DAT
for O
a O
cross-subject O
setting O

and B-DAT
the O
amount O
of O
head O

movement B-DAT
declines. O
Figure O
5 O
further O
explores O
the O

error B-DAT
distribution O
of O
the O
static O

network B-DAT
and O
the O
impact O
of O

sequential B-DAT
information. O
We O
can O
observe O

that B-DAT
the O
accuracy O
of O
the O

static B-DAT
model O
drops O
with O
extreme O

head B-DAT
poses O
and O
gaze O
directions O

, B-DAT
which O
can O
also O
be O

correlated B-DAT
to O
having O
less O
data O

in B-DAT
those O
areas. O
Compared O
to O

the B-DAT
static O
model, O
the O
temporal O

model B-DAT
particularly O
benefits O
gaze O
targets O

from B-DAT
mid-range O
upwards. O
Its O
contribution O

is B-DAT
less O
clear O
for O
extreme O

targets, B-DAT
probably O
again O
due O
to O

data B-DAT
imbalance. O
Finally, O
we O
evaluated O
the O
effect O

of B-DAT
different O
recurrent O
architectures O
for O

the B-DAT
temporal O
model. O
In O
particular O

, B-DAT
we O
tested O
1 O
(128 O

units) B-DAT
and O
2 O
(256-128 O
units) O

LSTM B-DAT
and O
GRU O
lay- O
ers, O

with B-DAT
1 O
GRU O
layer O
obtaining O

slightly B-DAT
superior O
results O
(up O
to O
0 O

.12◦). B-DAT
We O
also O
assessed O
the O

effect B-DAT
of O
sequence O
length O
fixing O

s B-DAT
in O
the O
range O
{4,7,10}, O

with B-DAT
s O
= O
7 O
performing O

worse B-DAT
than O
the O
other O
two O
( O

up B-DAT
to O
0 O

14 B-DAT

10 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
5 O
Conclusions O
In O
this O
work O

, B-DAT
we O
studied O
the O
combination O

of B-DAT
full-face O
and O
eye O
images O

along B-DAT
with O
facial O
land- O
marks O

for B-DAT
person- O
and O
head O
pose-independent O

3D B-DAT
gaze O
estimation. O
Consequently, O
we O

pro- B-DAT
posed O
a O
multi-stream O
recurrent O

CNN B-DAT
network O
that O
leverages O
the O

sequential B-DAT
information O
of O
eye O
and O

head B-DAT
movements. O
Both O
static O
and O

temporal B-DAT
versions O
of O
our O
approach O

significantly B-DAT
outperform O
current O
state-of-the-art O
3D O

gaze B-DAT
estimation O
methods O
on O
a O

wide B-DAT
range O
of O
head O
poses O

and B-DAT
gaze O
directions. O
We O
showed O

that B-DAT
adding O
geometry O
features O
to O

appearance-based B-DAT
methods O
has O
a O
regularizing O

effect B-DAT
on O
the O
accuracy. O
Adding O

sequential B-DAT
information O
further O
benefits O
the O

final B-DAT
performance O
compared O
to O
static-only O

input, B-DAT
especially O
from O
mid-range O
up- O

wards B-DAT
and O
in O
those O
cases O

where B-DAT
head O
motion O
is O
present. O

The B-DAT
effect O
in O
very O
extreme O

head B-DAT
poses O
is O
not O
clear O

due B-DAT
to O
data O
imbalance, O
suggesting O

the B-DAT
importance O
of O
learning O
from O

a B-DAT
con- O
tinuous, O
balanced O
dataset O

including B-DAT
all O
head O
poses O
and O

gaze B-DAT
directions O
of O
interest. O
To O

the B-DAT
best O
of O
our O
knowledge, O

this B-DAT
is O
the O
first O
attempt O

to B-DAT
exploit O
the O
temporal O
modality O

in B-DAT
the O
context O
of O
gaze O

estimation B-DAT
from O
remote O
cameras. O
As O

future B-DAT
work, O
we O
will O
further O

explore B-DAT
extracting O
meaningful O
temporal O
representations O

of B-DAT
gaze O
dynamics, O
considering O
3DCNNs O

as B-DAT
well O
as O
the O
encoding O

of B-DAT
deep O
features O
around O
particular O

tracked B-DAT
face O
landmarks O
[14]. O
Acknowledgements O
This O
work O
has O
been O

partially B-DAT
supported O
by O
the O
Spanish O

project B-DAT
TIN2016-74946-P O
(MINECO/ O
FEDER, O
UE O

), B-DAT
CERCA O
Programme O
/ O
Generalitat O

de B-DAT
Catalunya, O
and O
the O
FP7 O

people B-DAT
program O
(Marie O
Curie O
Actions), O

REA B-DAT
grant O
agreement O
no O
FP7-607139 O
( O

iCARE B-DAT
- O
Improving O
Children O
Auditory O

REhabilitation). B-DAT
We O
gratefully O
acknowledge O
the O

support B-DAT
of O
NVIDIA O
Corporation O
with O

the B-DAT
donation O
of O
the O
GPU O

used B-DAT
for O
this O
research. O
Portions O

of B-DAT
the O
research O
in O
this O

pa- B-DAT
per O
used O
the O
EYEDIAP O

dataset B-DAT
made O
available O
by O
the O

Idiap B-DAT
Research O
Institute, O
Martigny, O
Switzerland. O
References O
[1] O
Nicola O
C O
Anderson O

, B-DAT
Evan O
F O
Risko, O
and O

Alan B-DAT
Kingstone. O
Motion O
influences O
gaze O

di- B-DAT
rection O
discrimination O
and O
disambiguates O
contradictory O

luminance B-DAT
cues. O
Psychonomic O
bulletin O

& B-DAT
review, O
23(3):817–823, O
2016 O

. B-DAT
[2] O
Shumeet O
Baluja O
and O
Dean O

Pomerleau. B-DAT
Non-intrusive O
gaze O
tracking O
using O

artificial B-DAT
neu- O
ral O
networks. O
In O

Advances B-DAT
in O
Neural O
Information O
Processing O

Systems, B-DAT
pages O
753–760, O
1994 O

. B-DAT
[3] O
Adrian O
Bulat O
and O
Georgios O

Tzimiropoulos. B-DAT
How O
far O
are O
we O

from B-DAT
solving O
the O
2d O

& B-DAT
3d O
face O
alignment O
problem O

? B-DAT
(and O
a O
dataset O
of O
230, O

000 B-DAT
3d O
facial O
landmarks). O
In O

Interna- B-DAT
tional O
Conference O
on O
Computer O

Vision, B-DAT
2017. O
[4] O
Haoping O
Deng O
and O
Wangjiang O

Zhu. B-DAT
Monocular O
free-head O
3d O
gaze O

tracking B-DAT
with O
deep O
learning O
and O

geometry B-DAT
constraints. O
In O
Computer O
Vision O

(ICCV), B-DAT
2017 O
IEEE O
Interna- O
tional O

Conference B-DAT
on, O
pages O
3162–3171. O
IEEE O

, B-DAT
2017. O
[5] O
Onur O
Ferhat O
and O
Fernando O

Vilariño. B-DAT
Low O
cost O
eye O
tracking O

. B-DAT
Computational O
intelligence O
and O
neuroscience, O
2016 O

:17, B-DAT
2016. O
Citation O
Citation O
{Jung, O
Lee, O
Yim O

, B-DAT
Park, O
and O
Kim} O
2015 O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
11 O

6] B-DAT
Kenneth O
A O
Funes-Mora O
and O

Jean-Marc B-DAT
Odobez. O
Gaze O
estimation O
in O

the B-DAT
3D O
space O
using O
RGB-D O

sensors. B-DAT
International O
Journal O
of O
Computer O

Vision, B-DAT
118(2):194–216, O
2016. O
[7] O
Kenneth O
Alberto O
Funes O
Mora O

, B-DAT
Florent O
Monay, O
and O
Jean-Marc O

Odobez. B-DAT
Eyediap: O
A O
database O
for O

the B-DAT
development O
and O
evaluation O
of O

gaze B-DAT
estimation O
algorithms O
from O
rgb O

and B-DAT
rgb-d O
cameras. O
In O
Proceedings O

of B-DAT
the O
ACM O
Symposium O
on O

Eye B-DAT
Tracking O
Research O
and O
Applications. O

ACM, B-DAT
March O
2014. O
doi: O
10.1145/2578153.2578190. O
[8] O
Kenneth O
Alberto O
Funes O
Mora O

, B-DAT
Florent O
Monay, O
and O
Jean-Marc O

Odobez. B-DAT
Eyediap: O
A O
database O
for O

the B-DAT
development O
and O
evaluation O
of O

gaze B-DAT
estimation O
algorithms O
from O
rgb O

and B-DAT
rgb-d O
cameras. O
In O
Proceedings O

of B-DAT
the O
Symposium O
on O
Eye O

Tracking B-DAT
Research O
and O
Applications, O
pages O
255 O

–258. B-DAT
ACM, O
2014. O
[9] O
Quentin O
Guillon, O
Nouchine O
Hadjikhani O

, B-DAT
Sophie O
Baduel, O
and O
Bernadette O

Rogé. B-DAT
Visual O
social O
attention O
in O

autism B-DAT
spectrum O
disorder: O
Insights O
from O

eye B-DAT
tracking O
studies. O
Neu- O

roscience B-DAT
& O
Biobehavioral O
Reviews, O
42:279–297, O
2014 O

. B-DAT
[10] O
Dan O
Witzner O
Hansen O
and O

Qiang B-DAT
Ji. O
In O
the O
eye O

of B-DAT
the O
beholder: O
A O
survey O

of B-DAT
models O
for O
eyes O
and O

gaze. B-DAT
IEEE O
transactions O
on O
pattern O

analysis B-DAT
and O
machine O
intelligence, O
32(3 O

): B-DAT
478–500, O
2010. O
[11] O
Qiong O
Huang, O
Ashok O
Veeraraghavan O

, B-DAT
and O
Ashutosh O
Sabharwal. O
Tabletgaze: O

dataset B-DAT
and O
analysis O
for O
unconstrained O

appearance-based B-DAT
gaze O
estimation O
in O
mobile O

tablets. B-DAT
Machine O
Vision O
and O
Applications, O
28 O

(5-6):445–461, B-DAT
2017. O
[12] O
Robert O
JK O
Jacob O
and O

Keith B-DAT
S O
Karn. O
Eye O
tracking O

in B-DAT
human-computer O
interaction O
and O
usability O

research: B-DAT
Ready O
to O
deliver O
the O

promises. B-DAT
In O
The O
mind’s O
eye O

, B-DAT
pages O
573–605. O
Elsevier, O
2003. O
[13] O
László O
A O
Jeni O
and O

Jeffrey B-DAT
F O
Cohn. O
Person-independent O
3d O

gaze B-DAT
estimation O
using O
face O
frontalization O

. B-DAT
In O
Proceedings O
of O
the O

IEEE B-DAT
Conference O
on O
Computer O
Vision O

and B-DAT
Pattern O
Recognition O
Workshops, O
pages O
87 O

–95, B-DAT
2016. O
[14] O
Heechul O
Jung, O
Sihaeng O
Lee O

, B-DAT
Junho O
Yim, O
Sunjeong O
Park, O

and B-DAT
Junmo O
Kim. O
Joint O
fine- O

tuning B-DAT
in O
deep O
neural O
networks O

for B-DAT
facial O
expression O
recognition. O
In O

Computer B-DAT
Vision O
(ICCV), O
2015 O
IEEE O

International B-DAT
Conference O
on, O
pages O
2983–2991. O

IEEE, B-DAT
2015. O
[15] O
Anuradha O
Kar O
and O
Peter O

Corcoran. B-DAT
A O
review O
and O
analysis O

of B-DAT
eye-gaze O
estimation O
sys- O
tems O

, B-DAT
algorithms O
and O
performance O
evaluation O

methods B-DAT
in O
consumer O
platforms. O
IEEE O

Access, B-DAT
5:16495–16519, O
2017. O
[16] O
Kyle O
Krafka, O
Aditya O
Khosla O

, B-DAT
Petr O
Kellnhofer, O
Harini O
Kannan, O

Suchendra B-DAT
Bhandarkar, O
Wojciech O
Matusik, O
and O

Antonio B-DAT
Torralba. O
Eye O
tracking O
for O

everyone. B-DAT
In O
Computer O
Vision O
and O

Pattern B-DAT
Recognition O
(CVPR), O
2016 O
IEEE O

Conference B-DAT
on, O
pages O
2176–2184. O
IEEE, O
2016 O

. B-DAT
[17] O
Simon O
P O
Liversedge O
and O

John B-DAT
M O
Findlay. O
Saccadic O
eye O

movements B-DAT
and O
cognition. O
Trends O
in O

cognitive B-DAT
sciences, O
4(1):6–14, O
2000 O

. B-DAT
[18] O
Feng O
Lu, O
Takahiro O
Okabe O

, B-DAT
Yusuke O
Sugano, O
and O
Yoichi O

Sato. B-DAT
A O
head O
pose-free O
approach O

for B-DAT
appearance-based O
gaze O
estimation. O
In O

BMVC, B-DAT
pages O
1–11, O
2011 O

12 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
[19] O
Feng O
Lu, O
Yusuke O
Sugano O

, B-DAT
Takahiro O
Okabe, O
and O
Yoichi O

Sato. B-DAT
Inferring O
human O
gaze O
from O

appearance B-DAT
via O
adaptive O
linear O
regression. O

In B-DAT
Computer O
Vision O
(ICCV), O
2011 O

IEEE B-DAT
International O
Conference O
on, O
pages O
153 O

–160. B-DAT
IEEE, O
2011. O
[20] O
Päivi O
Majaranta O
and O
Andreas O

Bulling. B-DAT
Eye O
tracking O
and O
eye-based O

human–computer B-DAT
interaction. O
In O
Advances O
in O

physiological B-DAT
computing, O
pages O
39–65. O
Springer O

, B-DAT
2014. O
[21] O
Kenneth O
Alberto O
Funes O
Mora O

and B-DAT
Jean-Marc O
Odobez. O
Gaze O
estimation O

from B-DAT
multi- O
modal O
kinect O
data O

. B-DAT
In O
Computer O
Vision O
and O

Pattern B-DAT
Recognition O
Workshops O
(CVPRW), O
2012 O

IEEE B-DAT
Computer O
Society O
Conference O
on, O

pages B-DAT
25–30. O
IEEE, O
2012. O
[22] O
Carlos O
Hitoshi O
Morimoto, O
Arnon O

Amir, B-DAT
and O
Myron O
Flickner. O
Detecting O

eye B-DAT
position O
and O
gaze O
from O

a B-DAT
single O
camera O
and O
2 O

light B-DAT
sources. O
In O
Pattern O
Recognition O

, B-DAT
2002. O
Proceedings. O
16th O
International O

Conference B-DAT
on, O
volume O
4, O
pages O
314 O

–317. B-DAT
IEEE, O
2002. O
[23] O
IMO O
MSC. O
Circ. O
982 O

(2000) B-DAT
guidelines O
on O
ergonomic O
criteria O

for B-DAT
bridge O
equipment O
and O
layout O

. B-DAT
[24] O
Alejandro O
Newell, O
Kaiyu O
Yang O

, B-DAT
and O
Jia O
Deng. O
Stacked O

hourglass B-DAT
networks O
for O
hu- O
man O

pose B-DAT
estimation. O
In O
European O
Conference O

on B-DAT
Computer O
Vision, O
pages O
483–499. O

Springer, B-DAT
2016. O
[25] O
Yasuhiro O
Ono, O
Takahiro O
Okabe O

, B-DAT
and O
Yoichi O
Sato. O
Gaze O

estimation B-DAT
from O
low O
resolution O
images. O

In B-DAT
Pacific-Rim O
Symposium O
on O
Image O

and B-DAT
Video O
Technology, O
pages O
178–188. O

Springer, B-DAT
2006. O
[26] O
Cristina O
Palmero, O
Elisabeth O
A O

. B-DAT
van O
Dam, O
Sergio O
Escalera, O

Mike B-DAT
Kelia, O
Guido O
F. O
Lichtert, O

Lucas B-DAT
P.J.J O
Noldus, O
Andrew O
J. O

Spink, B-DAT
and O
Astrid O
van O
Wieringen. O

Automatic B-DAT
mutual O
gaze O
detection O
in O

face-to-face B-DAT
dyadic O
interaction O
videos. O
In O

Proceedings B-DAT
of O
Measuring O
Behavior, O
pages O
158 O

–163, B-DAT
2018. O
[27] O
Omkar O
M. O
Parkhi, O
Andrea O

Vedaldi, B-DAT
and O
Andrew O
Zisserman. O
Deep O

face B-DAT
recognition. O
In O
British O
Machine O

Vision B-DAT
Conference, O
2015 O

. B-DAT
[28] O
Derek O
R O
Rutter O
and O

Kevin B-DAT
Durkin. O
Turn-taking O
in O
mother–infant O

interaction: B-DAT
An O
exam- O
ination O
of O

vocalizations B-DAT
and O
gaze. O
Developmental O
psychology O

, B-DAT
23(1):54, O
1987. O
[29] O
Brian O
A O
Smith, O
Qi O

Yin, B-DAT
Steven O
K O
Feiner, O
and O

Shree B-DAT
K O
Nayar. O
Gaze O
locking O

: B-DAT
passive O
eye O
contact O
detection O

for B-DAT
human-object O
interaction. O
In O
Proceedings O

of B-DAT
the O
26th O
annual O
ACM O

symposium B-DAT
on O
User O
interface O
software O

and B-DAT
technology, O
pages O
271–280. O
ACM, O
2013 O

. B-DAT
[30] O
Yusuke O
Sugano, O
Yasuyuki O
Matsushita O

, B-DAT
and O
Yoichi O
Sato. O
Appearance-based O

gaze B-DAT
es- O
timation O
using O
visual O

saliency. B-DAT
IEEE O
transactions O
on O
pattern O

analysis B-DAT
and O
machine O
intelligence, O
35(2):329–341, O
2013 O

. B-DAT
[31] O
Yusuke O
Sugano, O
Yasuyuki O
Matsushita O

, B-DAT
and O
Yoichi O
Sato. O
Learning-by-synthesis O

for B-DAT
appearance-based O
3d O
gaze O
estimation. O

In B-DAT
Computer O
Vision O
and O
Pattern O

Recognition B-DAT
(CVPR), O
2014 O
IEEE O
Conference O

on, B-DAT
pages O
1821–1828. O
IEEE, O
2014. O
[32] O
Kar-Han O
Tan, O
David O
J O

Kriegman, B-DAT
and O
Narendra O
Ahuja. O
Appearance-based O

eye B-DAT
gaze O
es- O
timation. O
In O

Applications B-DAT
of O
Computer O
Vision, O
2002.(WACV O

2002). B-DAT
Proceedings. O
Sixth O
IEEE O
Workshop O

on, B-DAT
pages O
191–195. O
IEEE, O
2002 O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
13 O

33] B-DAT
Ronda O
Venkateswarlu O
et O
al. O

Eye B-DAT
gaze O
estimation O
from O
a O

single B-DAT
image O
of O
one O
eye. O

In B-DAT
Computer O
Vision, O
2003. O
Proceedings. O

Ninth B-DAT
IEEE O
International O
Conference O
on, O

pages B-DAT
136–143. O
IEEE, O
2003. O
[34] O
Kang O
Wang O
and O
Qiang O

Ji. B-DAT
Real O
time O
eye O
gaze O

tracking B-DAT
with O
3d O
deformable O
eye-face O

model. B-DAT
In O
Proceedings O
of O
the O

IEEE B-DAT
Conference O
on O
Computer O
Vision O

and B-DAT
Pattern O
Recog- O
nition, O
pages O

1003–1011, B-DAT
2017 O

. B-DAT
[35] O
Oliver O
Williams, O
Andrew O
Blake O

, B-DAT
and O
Roberto O
Cipolla. O
Sparse O

and B-DAT
semi-supervised O
visual O
mapping O
with O

the B-DAT
sˆ O
3gp. O
In O
Computer O

Vision B-DAT
and O
Pattern O
Recognition, O
2006 O

IEEE B-DAT
Computer O
Society O
Conference O
on, O

volume B-DAT
1, O
pages O
230–237. O
IEEE, O
2006 O

. B-DAT
[36] O
William O
Hyde O
Wollaston O
et O

al. B-DAT
Xiii. O
on O
the O
apparent O

direction B-DAT
of O
eyes O
in O
a O

portrait. B-DAT
Philosophical O
Transactions O
of O
the O

Royal B-DAT
Society O
of O
London, O
114:247–256 O

, B-DAT
1824. O
[37] O
Erroll O
Wood O
and O
Andreas O

Bulling. B-DAT
Eyetab: O
Model-based O
gaze O
estimation O

on B-DAT
unmodi- O
fied O
tablet O
computers O

. B-DAT
In O
Proceedings O
of O
the O

Symposium B-DAT
on O
Eye O
Tracking O
Research O

and B-DAT
Applications, O
pages O
207–210. O
ACM, O
2014 O

. B-DAT
[38] O
Erroll O
Wood, O
Tadas O
Baltrusaitis O

, B-DAT
Xucong O
Zhang, O
Yusuke O
Sugano, O

Peter B-DAT
Robinson, O
and O
Andreas O
Bulling. O

Rendering B-DAT
of O
eyes O
for O
eye-shape O

registration B-DAT
and O
gaze O
estimation. O
In O

Proceedings B-DAT
of O
the O
IEEE O
International O

Conference B-DAT
on O
Computer O
Vision, O
pages O
3756 O

– B-DAT
3764, O
2015. O
[39] O
Erroll O
Wood, O
Tadas O
Baltrušaitis O

, B-DAT
Louis-Philippe O
Morency, O
Peter O
Robinson, O

and B-DAT
Andreas O
Bulling. O
A O
3d O

morphable B-DAT
eye O
region O
model O
for O

gaze B-DAT
estimation. O
In O
European O
Confer- O

ence B-DAT
on O
Computer O
Vision, O
pages O
297 O

–313. B-DAT
Springer, O
2016. O
[40] O
Erroll O
Wood, O
Tadas O
Baltrušaitis O

, B-DAT
Louis-Philippe O
Morency, O
Peter O
Robinson, O

and B-DAT
Andreas O
Bulling. O
Learning O
an O

appearance-based B-DAT
gaze O
estimator O
from O
one O

million B-DAT
synthesised O
images. O
In O
Proceedings O

of B-DAT
the O
Ninth O
Biennial O
ACM O

Symposium B-DAT
on O
Eye O
Tracking O
Re- O

search B-DAT
& O
Applications, O
pages O
131–138. O

ACM, B-DAT
2016. O
[41] O
Dong O
Hyun O
Yoo O
and O

Myung B-DAT
Jin O
Chung. O
A O
novel O

non-intrusive B-DAT
eye O
gaze O
estimation O
using O

cross-ratio B-DAT
under O
large O
head O
motion O

. B-DAT
Computer O
Vision O
and O
Image O

Understanding, B-DAT
98(1):25–51, O
2005. O
[42] O
Xucong O
Zhang, O
Yusuke O
Sugano O

, B-DAT
Mario O
Fritz, O
and O
Andreas O

Bulling. B-DAT
Appearance-based O
gaze O
estimation O
in O

the B-DAT
wild. O
In O
Proceedings O
of O

the B-DAT
IEEE O
Conference O
on O
Computer O

Vision B-DAT
and O
Pattern O
Recognition, O
pages O
4511 O

–4520, B-DAT
2015. O
[43] O
Xucong O
Zhang, O
Yusuke O
Sugano O

, B-DAT
Mario O
Fritz, O
and O
Andreas O

Bulling. B-DAT
It’s O
written O
all O
over O

your B-DAT
face: O
Full-face O
appearance-based O
gaze O

estimation. B-DAT
In O
Proc. O
IEEE O
International O

Conference B-DAT
on O
Computer O
Vision O
and O

Pattern B-DAT
Recognition O
Workshops O
(CVPRW), O
2017 O

state O
of O
the O
art O
on O
EYEDIAP B-DAT
dataset, O
further O
improved O
by O
4 O

of O
our O
solution O
on O
the O
EYEDIAP B-DAT
dataset O
[7] O
in O
a O
wide O

VGA O
videos O
from O
the O
publicly-available O
EYEDIAP B-DAT
dataset O
[7] O
to O
perform O
the O

FT-S O
scenario O
are O
provided O
by O
EYEDIAP B-DAT
dataset. O
MPIIGaze:. O
State-of-the-art O
full-face O
3D O

fine-tuned O
it O
with O
the O
filtered O
EYEDIAP B-DAT
subsets O
using O
our O
training O
parameters O

this O
pa- O
per O
used O
the O
EYEDIAP B-DAT
dataset O
made O
available O
by O
the O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
1 O

Recurrent B-DAT
CNN O
for O
3D O
Gaze O

Estimation B-DAT
using O
Appearance O
and O
Shape O

Cues B-DAT
Cristina O
Palmero1,2 O

crpalmec7@alumnes.ub.edu B-DAT
1 O
Dept. O
Mathematics O
and O
Informatics O

Universitat B-DAT
de O
Barcelona, O
Spain O

Javier B-DAT
Selva1 O
javier.selva.castello@est.fib.upc.edu O

2 B-DAT
Computer O
Vision O
Center O
Campus O

UAB, B-DAT
Bellaterra, O
Spain O
Mohammad O
Ali O
Bagheri3,4 O

mohammadali.bagheri@ucalgary.ca B-DAT
3 O
Dept. O
Electrical O
and O
Computer O

Eng. B-DAT
University O
of O
Calgary, O
Canada O

Sergio B-DAT
Escalera1,2 O
sergio@maia.ub.es O

4 B-DAT
Dept. O
Engineering O
University O
of O

Larestan, B-DAT
Iran O
Abstract O

Gaze B-DAT
behavior O
is O
an O
important O

non-verbal B-DAT
cue O
in O
social O
signal O

processing B-DAT
and O
human- O
computer O
interaction. O

In B-DAT
this O
paper, O
we O
tackle O

the B-DAT
problem O
of O
person- O
and O

head B-DAT
pose- O
independent O
3D O
gaze O

estimation B-DAT
from O
remote O
cameras, O
using O

a B-DAT
multi-modal O
recurrent O
convolutional O
neural O

network B-DAT
(CNN). O
We O
propose O
to O

combine B-DAT
face, O
eyes O
region, O
and O

face B-DAT
landmarks O
as O
individual O
streams O

in B-DAT
a O
CNN O
to O
estimate O

gaze B-DAT
in O
still O
images. O
Then, O

we B-DAT
exploit O
the O
dynamic O
nature O

of B-DAT
gaze O
by O
feeding O
the O

learned B-DAT
features O
of O
all O
the O

frames B-DAT
in O
a O
sequence O
to O

a B-DAT
many-to-one O
recurrent O
module O
that O

predicts B-DAT
the O
3D O
gaze O
vector O

of B-DAT
the O
last O
frame. O
Our O

multi-modal B-DAT
static O
solution O
is O
evaluated O

on B-DAT
a O
wide O
range O
of O

head B-DAT
poses O
and O
gaze O
directions, O

achieving B-DAT
a O
significant O
improvement O
of O
14 O

.6% B-DAT
over O
the O
state O
of O

the B-DAT
art O
on O
EYEDIAP O
dataset, O

further B-DAT
improved O
by O
4% O
when O

the B-DAT
temporal O
modality O
is O
included. O
1 O
Introduction O
Eyes O
and O
their O

movements B-DAT
are O
considered O
an O
important O

cue B-DAT
in O
non-verbal O
behavior O
analysis O

, B-DAT
being O
involved O
in O
many O

cognitive B-DAT
processes O
and O
reflecting O
our O

internal B-DAT
state O
[17]. O
More O
specifically, O

eye B-DAT
gaze O
behavior, O
as O
an O

indicator B-DAT
of O
human O
visual O
attention, O

has B-DAT
been O
widely O
studied O
to O

assess B-DAT
communication O
skills O
[28] O
and O

to B-DAT
identify O
possible O
behavioral O

disorders B-DAT
[9]. O
Therefore, O
gaze O
estimation O

has B-DAT
become O
an O
established O
line O

of B-DAT
research O
in O
computer O
vision, O

being B-DAT
a O
key O
feature O
in O

human-computer B-DAT
interaction O
(HCI) O
and O
usability O

research B-DAT
[12, O
20]. O
Recent O
gaze O
estimation O
research O
has O

focused B-DAT
on O
facilitating O
its O
use O

in B-DAT
general O
everyday O
applications O
under O

real-world B-DAT
conditions, O
using O
off-the-shelf O
remote O

RGB B-DAT
cameras O
and O
re- O
moving O

the B-DAT
need O
of O
personal O
calibration O

[26]. B-DAT
In O
this O
setting, O
appearance-based O

methods, B-DAT
which O
learn O
a O
mapping O

from B-DAT
images O
to O
gaze O
directions O

, B-DAT
are O
the O
preferred O

choice B-DAT
[25]. O
How- O
ever, O
they O

need B-DAT
large O
amounts O
of O
training O

data B-DAT
to O
be O
able O
to O

generalize B-DAT
well O
to O
in-the-wild O
situations, O

which B-DAT
are O
characterized O
by O
significant O

variability B-DAT
in O
head O
poses, O
face O

appearances B-DAT
and O
lighting O
conditions. O
In O

recent B-DAT
years, O
CNNs O
have O
been O

reported B-DAT
to O
outperform O
classical O
methods. O

However, B-DAT
most O
existing O
approaches O
have O

only B-DAT
been O
tested O
in O
restricted O

HCI B-DAT
tasks, O
c© O
2018. O
The O
copyright O
of O

this B-DAT
document O
resides O
with O
its O

authors. B-DAT
It O
may O
be O
distributed O

unchanged B-DAT
freely O
in O
print O
or O

electronic B-DAT
forms O

. B-DAT
ar O
X O

iv B-DAT
:1 O
80 O
5 O

. B-DAT
03 O
06 O

4v B-DAT
3 O

cs B-DAT
.C O
V O

1 B-DAT
7 O

Se B-DAT
p O

20 B-DAT
18 O

Citation B-DAT
Citation O
{Liversedge O
and O
Findlay} O
2000 O

Citation B-DAT
Citation O
{Rutter O
and O
Durkin} O
1987 O

Citation B-DAT
Citation O
{Guillon, O
Hadjikhani, O
Baduel, O

and B-DAT
Rog{é}} O
2014 O
Citation O
Citation O
{Jacob O
and O
Karn O

} B-DAT
2003 O
Citation O
Citation O
{Majaranta O
and O
Bulling O

} B-DAT
2014 O
Citation O
Citation O
{Palmero, O
van O
Dam O

, B-DAT
Escalera, O
Kelia, O
Lichtert, O
Noldus, O

Spink, B-DAT
and O
van O
Wieringen} O
2018 O
Citation O
Citation O
{Ono, O
Okabe, O
and O

Sato} B-DAT
2006 O

2 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
Method O
3D O
gaze O
direction O

Unrestricted B-DAT
gaze O
target O
Full O
face O

Eye B-DAT
region O
Facial O
landmarks O

Sequential B-DAT
information O
Zhang O
et O
al. O
(1) O
[42 O

] B-DAT
3 O
7 O
7 O
3 O
7 O
7 O
Krafka O
et O
al. O
[16 O

] B-DAT
7 O
7 O
3 O
3 O
7 O
7 O
Zhang O
et O
al. O
(2 O

) B-DAT
[43] O
3 O
7 O
3 O
7 O
7 O
7 O
Deng O
and O
Zhu O

[4] B-DAT
3 O
3 O
3 O
3 O

7 B-DAT
7 O
Ours O
3 O
3 O

3 B-DAT
3 O
3 O
3 O

Table B-DAT
1: O
Characteristics O
of O
recent O

related B-DAT
work O
on O
person- O
and O

head B-DAT
pose-independent O
appearance-based O
gaze O
estimation O

methods B-DAT
using O
CNNs. O
where O
users O
look O
at O
the O

screen B-DAT
or O
mobile O
phone, O
showing O

a B-DAT
low O
head O
pose O
variability O

. B-DAT
It O
is O
yet O
unclear O

how B-DAT
these O
methods O
would O
perform O

in B-DAT
a O
wider O
range O
of O

head B-DAT
poses. O
On O
a O
different O
note, O
until O

very B-DAT
recently, O
the O
majority O
of O

methods B-DAT
only O
used O
static O
eye O

region B-DAT
appearance O
as O
input. O
State-of-the-art O

approaches B-DAT
have O
demonstrated O
that O
using O

the B-DAT
face O
along O
with O
a O

higher B-DAT
resolution O
image O
of O
the O

eyes B-DAT
[16], O
or O
even O
just O

the B-DAT
face O
itself O
[43], O
increases O

performance. B-DAT
Indeed, O
the O
whole-face O
image O

encodes B-DAT
more O
information O
than O
eyes O

alone, B-DAT
such O
as O
illumination O
and O

head B-DAT
pose. O
Nevertheless, O
gaze O
behavior O

is B-DAT
not O
static. O
Eye O
and O

head B-DAT
movements O
allow O
us O
to O

direct B-DAT
our O
gaze O
to O
target O

locations B-DAT
of O
interest. O
It O
has O

been B-DAT
demonstrated O
that O
humans O
can O

better B-DAT
predict O
gaze O
when O
being O

shown B-DAT
image O
sequences O
of O
other O

people B-DAT
moving O
their O
eyes O
[1 O

]. B-DAT
However, O
it O
is O
still O

an B-DAT
open O
question O
whether O
this O

se- B-DAT
quential O
information O
can O
increase O

the B-DAT
performance O
of O
automatic O
methods. O
In O
this O
work, O
we O
show O

that B-DAT
the O
combination O
of O
multiple O

cues B-DAT
benefits O
the O
gaze O
estimation O

task. B-DAT
In O
particular, O
we O
use O

face, B-DAT
eye O
region O
and O
facial O

landmarks B-DAT
from O
still O
images. O
Facial O

landmarks B-DAT
model O
the O
global O
shape O

of B-DAT
the O
face O
and O
come O

at B-DAT
no O
cost, O
since O
face O

alignment B-DAT
is O
a O
common O
pre-processing O

step B-DAT
in O
many O
facial O
image O

analysis B-DAT
approaches. O
Furthermore, O
we O
present O

a B-DAT
subject-independent, O
free-head O
recurrent O
3D O

gaze B-DAT
regression O
network O
to O
leverage O

the B-DAT
temporal O
information O
of O
image O

sequences. B-DAT
The O
static O
streams O
of O

each B-DAT
frame O
are O
combined O
in O

a B-DAT
late-fusion O
fashion O
using O
a O

multi-stream B-DAT
CNN. O
Then, O
all O
feature O

vectors B-DAT
are O
input O
to O
a O

many-to-one B-DAT
recurrent O
module O
that O
predicts O

the B-DAT
gaze O
vector O
of O
the O

last B-DAT
sequence O
frame O

. B-DAT
In O
summary, O
our O
contributions O
are O

two-fold. B-DAT
First, O
we O
present O
a O

Recurrent-CNN B-DAT
net- O
work O
architecture O
that O

combines B-DAT
appearance, O
shape O
and O
temporal O

information B-DAT
for O
3D O
gaze O
estimation O

. B-DAT
Second, O
we O
test O
static O

and B-DAT
temporal O
versions O
of O
our O

solution B-DAT
on O
the O
EYEDIAP O

dataset B-DAT
[7] O
in O
a O
wide O

range B-DAT
of O
head O
poses O
and O

gaze B-DAT
directions, O
showing O
consistent O
perfor- O

mance B-DAT
improvements O
compared O
to O
related O

appearance-based B-DAT
methods. O
To O
the O
best O

of B-DAT
our O
knowledge, O
this O
is O

the B-DAT
first O
third-person, O
remote O
camera-based O

approach B-DAT
that O
uses O
tempo- O
ral O

information B-DAT
for O
this O
task. O
Table O
1 O
outlines O
our O
main O
method O
characteristics O

compared B-DAT
to O
related O
work. O
Models O

and B-DAT
code O
are O
publicly O
available O

at B-DAT
https://github.com/ O
crisie/RecurrentGaze O

. B-DAT
2 O
Related O
work O
Gaze O
estimation O

methods B-DAT
are O
typically O
categorized O
as O

model-based B-DAT
or O
appearance-based O
[5, O
10 O

, B-DAT
15]. O
Model-based O
approaches O
use O

a B-DAT
geometric O
model O
of O
the O

eye, B-DAT
usually O
requir- O
ing O
either O

high B-DAT
resolution O
images O
or O
a O

person-specific B-DAT
calibration O
stage O
to O
estimate O

personal B-DAT
eye O
parameters O
[22, O
33, O
34, O
37, O
41]. O
In O
contrast, O
appearance-based O

methods B-DAT
learn O
a O
di- O
rect O

mapping B-DAT
from O
intensity O
images O
or O

extracted B-DAT
eye O
features O
to O
gaze O

directions, B-DAT
thus O
being O

Citation B-DAT
Citation O
{Zhang, O
Sugano, O
Fritz, O

and B-DAT
Bulling} O
2015 O
Citation O
Citation O
{Krafka, O
Khosla, O
Kellnhofer O

, B-DAT
Kannan, O
Bhandarkar, O
Matusik, O
and O

Torralba} B-DAT
2016 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2017 O
Citation O
Citation O
{Deng O
and O
Zhu O

} B-DAT
2017 O
Citation O
Citation O
{Krafka, O
Khosla, O
Kellnhofer O

, B-DAT
Kannan, O
Bhandarkar, O
Matusik, O
and O

Torralba} B-DAT
2016 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2017 O
Citation O
Citation O
{Anderson, O
Risko, O
and O

Kingstone} B-DAT
2016 O

Citation B-DAT
Citation O
{Funesprotect O
unhbox O
voidb@x O

penalty B-DAT
@M O
{}Mora, O
Monay, O
and O
Odobez} O
2014 O

{} B-DAT
Citation O
Citation O
{Ferhat O
and O
Vilari{ñ}o O

} B-DAT
2016 O
Citation O
Citation O
{Hansen O
and O
Ji O

} B-DAT
2010 O
Citation O
Citation O
{Kar O
and O
Corcoran O

} B-DAT
2017 O
Citation O
Citation O
{Morimoto, O
Amir, O
and O

Flickner} B-DAT
2002 O

Citation B-DAT
Citation O
{Venkateswarlu O
etprotect O
unhbox O

voidb@x B-DAT
penalty O
@M O
{}al.} O
2003 O

Citation B-DAT
Citation O
{Wang O
and O
Ji} O
2017 O

Citation B-DAT
Citation O
{Wood O
and O
Bulling} O
2014 O

Citation B-DAT
Citation O
{Yoo O
and O
Chung} O
2005 O

https://github.com/crisie/RecurrentGaze B-DAT

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
3 O

potentially B-DAT
applicable O
to O
relatively O
low O

resolution B-DAT
images O
and O
mid-distance O
scenarios. O

Dif- B-DAT
ferent O
mapping O
functions O
have O

been B-DAT
explored, O
such O
as O
neural O

networks B-DAT
[2], O
adaptive O
linear O
regression O
( O

ALR) B-DAT
[19], O
local O
interpolation O
[32], O

gaussian B-DAT
processes O
[30, O
35], O
random O

forests B-DAT
[11, O
31], O
or O
k-nearest O

neighbors B-DAT
[40]. O
Main O
challenges O
of O

appearance-based B-DAT
methods O
for O
3D O
gaze O

estimation B-DAT
are O
head O
pose, O
illumination O

and B-DAT
subject O
invariance O
without O
user-specific O

calibration. B-DAT
To O
handle O
these O
issues, O

some B-DAT
works O
proposed O
compensation O

methods B-DAT
[18] O
and O
warping O
strategies O

that B-DAT
synthesize O
a O
canonical, O
frontal O

looking B-DAT
view O
of O
the O

face B-DAT
[6, O
13, O
21]. O
Hybrid O

approaches B-DAT
based O
on O
analysis-by-synthesis O
have O

also B-DAT
been O
evaluated O
[39]. O
Currently, O
data-driven O
methods O
are O
considered O

the B-DAT
state O
of O
the O
art O

for B-DAT
person- O
and O
head O
pose-independent O

appearance-based B-DAT
gaze O
estimation. O
Consequently, O
a O

number B-DAT
of O
gaze O
es- O
timation O

datasets B-DAT
have O
been O
introduced O
in O

recent B-DAT
years, O
either O
in O
controlled O

[29] B-DAT
or O
semi- O
controlled O
settings O

[8], B-DAT
in O
the O
wild O
[16 O

, B-DAT
42], O
or O
consisting O
of O

synthetic B-DAT
data O
[31, O
38, O
40]. O

Zhang B-DAT
et O
al. O
[42] O
showed O

that B-DAT
CNNs O
can O
outperform O
other O

mapping B-DAT
methods, O
using O
a O
multi- O

modal B-DAT
CNN O
to O
learn O
the O

mapping B-DAT
from O
3D O
head O
poses O

and B-DAT
eye O
images O
to O
3D O

gaze B-DAT
directions. O
Krafka O
et O

al. B-DAT
[16] O
proposed O
a O
multi-stream O

CNN B-DAT
for O
2D O
gaze O
estimation, O

using B-DAT
individual O
eye, O
whole-face O
image O

and B-DAT
the O
face O
grid O
as O

input. B-DAT
As O
this O
method O
was O

limited B-DAT
to O
2D O
screen O
mapping, O

Zhang B-DAT
et O
al. O
[43] O
later O

explored B-DAT
the O
potential O
of O
just O

using B-DAT
whole-face O
images O
as O
input O

to B-DAT
estimate O
3D O
gaze O
directions. O

Using B-DAT
a O
spatial O
weights O
CNN, O

they B-DAT
demonstrated O
their O
method O
to O

be B-DAT
more O
robust O
to O
facial O

appearance B-DAT
variation O
caused O
by O
head O

pose B-DAT
and O
illumina- O
tion O
than O

eye-only B-DAT
methods. O
While O
the O
method O

was B-DAT
evaluated O
in O
the O
wild, O

the B-DAT
subjects O
were O
only O
interacting O

with B-DAT
a O
mobile O
device, O
thus O

restricting B-DAT
the O
head O
pose O
range. O

Deng B-DAT
and O
Zhu O
[4] O
presented O

a B-DAT
two-stream O
CNN O
to O
disjointly O

model B-DAT
head O
pose O
from O
face O

images B-DAT
and O
eye- O
ball O
movement O

from B-DAT
eye O
region O
images. O
Both O

were B-DAT
then O
aggregated O
into O
3D O

gaze B-DAT
direction O
using O
a O
gaze O

transform B-DAT
layer. O
The O
decomposition O
was O

aimed B-DAT
to O
avoid O
head-correlation O
over- O

fitting B-DAT
of O
previous O
data-driven O
approaches. O

They B-DAT
evaluated O
their O
approach O
in O

the B-DAT
wild O
with O
a O
wider O

range B-DAT
of O
head O
poses, O
obtaining O

better B-DAT
performance O
than O
previous O
eye-based O

methods. B-DAT
However, O
they O
did O
not O

test B-DAT
it O
on O
public O
annotated O

benchmark B-DAT
datasets. O
In O
this O
paper, O
we O
propose O

a B-DAT
multi-stream O
recurrent O
CNN O
network O

for B-DAT
person- O
and O
head O
pose-independent O

3D B-DAT
gaze O
estimation O
for O
a O

mid-distance B-DAT
scenario. O
We O
evaluate O
it O

on B-DAT
a O
wider O
range O
of O

head B-DAT
poses O
and O
gaze O
directions O

than B-DAT
screen-targeted O
approaches. O
As O
opposed O

to B-DAT
previous O
methods, O
we O
also O

rely B-DAT
on O
temporal O
information O
inherent O

in B-DAT
sequential O
data O

. B-DAT
3 O
Methodology O

In B-DAT
this O
section, O
we O
present O

our B-DAT
approach O
for O
3D O
gaze O

regression B-DAT
based O
on O
appearance O
and O

shape B-DAT
cues O
for O
still O
images O

and B-DAT
image O
sequences. O
First, O
we O

introduce B-DAT
the O
data O
modalities O
and O

formulate B-DAT
the O
problem. O
Then, O
we O

detail B-DAT
the O
normalization O
procedure O
prior O

to B-DAT
the O
regression O
stage. O
Finally, O

we B-DAT
explain O
the O
global O
network O

topology B-DAT
as O
well O
as O
the O

implementation B-DAT
details. O
An O
overview O
of O

the B-DAT
system O
architecture O
is O
depicted O

in B-DAT
Figure O
1. O
3.1 O
Multi-modal O
gaze O
regression O

Let B-DAT
us O
represent O
gaze O
direction O

as B-DAT
a O
3D O
unit O
vector O

g B-DAT
= O
[gx,gy,gz]T O
∈R3 O
in O

the B-DAT
Camera O
Coor- O
dinate O
System O
( O

CCS), B-DAT
whose O
origin O
is O
the O

central B-DAT
point O
between O
eyeball O
centers. O

Assuming B-DAT
a O
calibrated O
camera, O
and O

a B-DAT
known O
head O
position O
and O

orientation, B-DAT
our O
goal O
is O
to O

estimate B-DAT
g O
from O
a O
sequence O

of B-DAT
images O
{I(i) O
| O

I B-DAT
∈ O
RW×H×3} O
as O
a O

regression B-DAT
problem. O
Citation O
Citation O
{Baluja O
and O
Pomerleau O

} B-DAT
1994 O
Citation O
Citation O
{Lu, O
Sugano, O
Okabe O

, B-DAT
and O
Sato} O
2011{} O
Citation O
Citation O
{Tan, O
Kriegman, O
and O

Ahuja} B-DAT
2002 O

Citation B-DAT
Citation O
{Sugano, O
Matsushita, O
and O

Sato} B-DAT
2013 O
Citation O
Citation O
{Williams, O
Blake, O
and O

Cipolla} B-DAT
2006 O

Citation B-DAT
Citation O
{Huang, O
Veeraraghavan, O
and O

Sabharwal} B-DAT
2017 O
Citation O
Citation O
{Sugano, O
Matsushita, O
and O

Sato} B-DAT
2014 O

Citation B-DAT
Citation O
{Wood, O
Baltru{²}aitis, O
Morency, O

Robinson, B-DAT
and O
Bulling} O
2016{} O
Citation O
Citation O
{Lu, O
Okabe, O
Sugano O

, B-DAT
and O
Sato} O
2011{} O
Citation O
Citation O
{Funes-Mora O
and O
Odobez O

} B-DAT
2016 O
Citation O
Citation O
{Jeni O
and O
Cohn O

} B-DAT
2016 O
Citation O
Citation O
{Mora O
and O
Odobez O

} B-DAT
2012 O
Citation O
Citation O
{Wood, O
Baltru{²}aitis, O
Morency O

, B-DAT
Robinson, O
and O
Bulling} O
2016{} O
Citation O
Citation O
{Smith, O
Yin, O
Feiner O

, B-DAT
and O
Nayar} O
2013 O
Citation O
Citation O
{Funesprotect O
unhbox O
voidb@x O

penalty B-DAT
@M O

Mora, B-DAT
Monay, O
and O
Odobez} O
2014{} O
Citation O
Citation O
{Krafka, O
Khosla, O
Kellnhofer O

, B-DAT
Kannan, O
Bhandarkar, O
Matusik, O
and O

Torralba} B-DAT
2016 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2015 O
Citation O
Citation O
{Sugano, O
Matsushita, O
and O

Sato} B-DAT
2014 O

Citation B-DAT
Citation O
{Wood, O
Baltrusaitis, O
Zhang, O

Sugano, B-DAT
Robinson, O
and O
Bulling} O
2015 O
Citation O
Citation O
{Wood, O
Baltru{²}aitis, O
Morency O

, B-DAT
Robinson, O
and O
Bulling} O
2016{} O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2015 O
Citation O
Citation O
{Krafka, O
Khosla, O
Kellnhofer O

, B-DAT
Kannan, O
Bhandarkar, O
Matusik, O
and O

Torralba} B-DAT
2016 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2017 O
Citation O
Citation O
{Deng O
and O
Zhu O

} B-DAT
2017 O

4 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
Conv O

C B-DAT
on O
ca O
t O
x O
y O
z O
x O
y O

z B-DAT
x O
y O
z O

Individual B-DAT
Fusion O
Temporal O
Individual O
Fusion O

Input B-DAT

Individual B-DAT
Fusion O
Normalization O

.Conv B-DAT

Conv B-DAT
. O
Conv O

Conv B-DAT
. O
FC O

FC B-DAT
FC O
RNN O
RNN O

RNN B-DAT
FC O
Ti O
m O
e O

Figure B-DAT
1: O
Overview O
of O
the O

proposed B-DAT
network. O
A O
multi-stream O
CNN O

jointly B-DAT
models O
full-face, O
eye O
region O

appearance B-DAT
and O
face O
landmarks O
from O

still B-DAT
images. O
The O
combined O
extracted O

fea- B-DAT
tures O
from O
each O
frame O

are B-DAT
fed O
into O
a O
recurrent O

module B-DAT
to O
predict O
last O
frame’s O

gaze B-DAT
direction. O
Gazing O
to O
a O
specific O
target O

is B-DAT
achieved O
by O
a O
combination O

of B-DAT
eye O
and O
head O
movements O

, B-DAT
which O
are O
highly O
coordinated. O

Consequently, B-DAT
the O
apparent O
direction O
of O

gaze B-DAT
is O
influenced O
not O
only O

by B-DAT
the O
location O
of O
the O

irises B-DAT
within O
the O
eyelid O
aperture, O

but B-DAT
also O
by O
the O
position O

and B-DAT
orientation O
of O
the O
face O

with B-DAT
respect O
to O
the O
camera. O

Known B-DAT
as O
the O
Wollaston O

effect B-DAT
[36], O
the O
exact O
same O

set B-DAT
of O
eyes O
may O
appear O

to B-DAT
be O
looking O
in O
different O

directions B-DAT
due O
to O
the O
surrounding O

facial B-DAT
cues. O
It O
is O
therefore O

reasonable B-DAT
to O
state O
that O
eye O

images B-DAT
are O
not O
sufficient O
to O

estimate B-DAT
gaze O
direction. O
Instead, O
whole-face O

images B-DAT
can O
encode O
head O
pose O

or B-DAT
illumination-specific O
information O
across O
larger O

areas B-DAT
than O
those O
available O
just O

in B-DAT
the O
eyes O
region O
[16, O
43 O

]. B-DAT
The O
drawback O
of O
appearance-only O
methods O

is B-DAT
that O
global O
structure O
information O

is B-DAT
not O
explicitly O
considered. O
In O

that B-DAT
sense, O
facial O
landmarks O
can O

be B-DAT
used O
as O
global O
shape O

cues B-DAT
to O
en- O
code O
spatial O

relationships B-DAT
and O
geometric O
constraints. O
Current O

state-of-the-art B-DAT
face O
alignment O
approaches O
are O

robust B-DAT
enough O
to O
handle O
large O

appearance B-DAT
variability, O
extreme O
head O
poses O

and B-DAT
occlusions, O
being O
especially O
useful O

when B-DAT
the O
dataset O
used O
for O

gaze B-DAT
estimation O
does O
not O
contain O

such B-DAT
variability. O
Facial O
landmarks O
are O

mainly B-DAT
correlated O
with O
head O
orientation O

, B-DAT
eye O
position, O
eyelid O
openness, O

and B-DAT
eyebrow O
movement, O
which O
are O

valuable B-DAT
features O
for O
our O
task. O
Therefore, O
in O
our O
approach O
we O

jointly B-DAT
model O
appearance O
and O
shape O

cues B-DAT
(see O
Figure O
1). O
The O

former B-DAT
is O
represented O
by O
a O

whole-face B-DAT
image O
IF O
, O
along O

with B-DAT
a O
higher O
resolution O
image O

of B-DAT
the O
eyes O
IE O
to O

identify B-DAT
subtle O
changes. O
Due O
to O

dealing B-DAT
with O
wide O
head O
pose O

ranges, B-DAT
some O
eye O
images O
may O

not B-DAT
depict O
the O
whole O
eye O

, B-DAT
containing O
mostly O
background O
or O

other B-DAT
surrounding O
facial O
parts O
instead. O

For B-DAT
that O
reason, O
and O
contrary O

to B-DAT
previous O
approaches O
that O
only O

use B-DAT
one O
eye O
image O
[31, O
42 O

], B-DAT
we O
use O
a O
single O

image B-DAT
composed O
of O
two O
patches O

of B-DAT
centered O
left O
and O
right O

eyes. B-DAT
Finally, O
the O
shape O
cue O

is B-DAT
represented O
by O
3D O
face O

landmarks B-DAT
obtained O
from O
a O
68-landmark O

model, B-DAT
denoted O
by O

L B-DAT
= O
{(lx, O
ly, O

) B-DAT

| B-DAT
∀c O
∈ O
[1, O
...,68 O

]}. B-DAT
In O
this O
work O
we O
also O

consider B-DAT
the O
dynamic O
component O
of O

gaze. B-DAT
We O
leverage O
the O
se O

- B-DAT
quential O
information O
of O
eye O

and B-DAT
head O
movements O
such O
that, O

given B-DAT
appearance O
and O
shape O
features O

of B-DAT
consecutive O
frames, O
it O
is O

possible B-DAT
to O
better O
predict O
the O

gaze B-DAT
direction O
of O
the O
cur- O

rent B-DAT
frame. O
Therefore, O
the O
3D O

gaze B-DAT
estimation O
task O
for O
a O
1 O

-frame B-DAT
sequence O
is O
formulated O
Citation O
Citation O
{Wollaston O
etprotect O
unhbox O

voidb@x B-DAT
penalty O
@M O

al.} B-DAT
1824 O
Citation O
Citation O
{Krafka, O
Khosla, O
Kellnhofer O

, B-DAT
Kannan, O
Bhandarkar, O
Matusik, O
and O

Torralba} B-DAT
2016 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2017 O
Citation O
Citation O
{Sugano, O
Matsushita, O
and O

Sato} B-DAT
2014 O

Citation B-DAT
Citation O
{Zhang, O
Sugano, O
Fritz, O

and B-DAT
Bulling} O
2015 O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
5 O

as B-DAT
g(i) O
= O
f O
( O
{IF O
(i)},{IE O
(i)},{L(i O

)} B-DAT
) O
, O
where O
i O
denotes O

the B-DAT
i-th O
frame, O
and O
f O

is B-DAT
the O
regression O

function. B-DAT
3.2 O
Data O
normalization O
Prior O
to O

gaze B-DAT
regression, O
a O
normalization O
step O

in B-DAT
the O
3D O
space O
and O

the B-DAT
2D O
image, O
similar O
to O

[31], B-DAT
is O
carried O
out. O
This O

is B-DAT
performed O
to O
reduce O
the O

appearance B-DAT
variability O
and O
to O
allow O

the B-DAT
gaze O
estimation O
model O
to O

be B-DAT
applied O
regardless O
of O
the O

original B-DAT
camera O
configuration O

. B-DAT
Let O
H O
∈ O
R3x3 O
be O

the B-DAT
head O
rotation O
matrix, O
and O

p B-DAT
= O
[px, O
py, O
pz]T O

∈ B-DAT
R3 O
the O
reference O
face O

location B-DAT
with O
respect O
to O
the O

original B-DAT
CCS. O
The O
goal O
is O

to B-DAT
find O
the O
conversion O
matrix O

M B-DAT
= O
SR O
such O
that O

(a) B-DAT
the O
X-axes O
of O
the O

virtual B-DAT
camera O
and O
the O
head O

become B-DAT
parallel O
using O
the O
rotation O

matrix B-DAT
R, O
and O
(b) O
the O

virtual B-DAT
camera O
looks O
at O
the O

reference B-DAT
location O
from O
a O
fixed O

distance B-DAT
dn O
using O
the O
Z-direction O

scaling B-DAT
matrix O
S O
= O
diag(1,1,dn/‖p O

‖). B-DAT
R O
is O
computed O
as O

a B-DAT
= O
p̂×HT O
e1, O

b B-DAT
= O
â× O
p̂, O

R B-DAT
= O
[â, O
b̂, O
p̂]T O
, O
where O
e1 O
denotes O
the O
first O

orthonormal B-DAT
basis O
and O

〈 B-DAT
·̂ O
〉 O
is O
the O

unit B-DAT
vector O

. B-DAT
This O
normalization O
translates O
into O
the O

image B-DAT
space O
as O
a O
cropped O

image B-DAT
patch O
of O
size O
Wn×Hn O

centered B-DAT
at O
p O
where O
head O

roll B-DAT
rotation O
has O
been O
removed O

. B-DAT
This O
is O
done O
by O

applying B-DAT
a O
perspective O
warping O
to O

the B-DAT
input O
image O
I O
using O

the B-DAT
transformation O
matrix O
W O
= O

CoMCn−1, B-DAT
where O
Co O
and O
Cn O

are B-DAT
the O
original O
and O
virtual O

camera B-DAT
matrices, O
respectively. O
The O
3D O
gaze O
vector O
is O

also B-DAT
normalized O
as O
gn O
=Rg O

. B-DAT
After O
image O
normalization, O
the O

line B-DAT
of O
sight O
can O
be O

represented B-DAT
in O
a O
2D O
space. O

Therefore, B-DAT
gn O
is O
further O
transformed O

to B-DAT
spherical O
coor- O
dinates O
(θ O
, O

φ) B-DAT
assuming O
unit O
length, O
where O

θ B-DAT
and O
φ O
denote O
the O

horizontal B-DAT
and O
vertical O
direc- O
tion O

angles, B-DAT
respectively. O
This O
2D O
angle O

representation, B-DAT
delimited O
in O
the O

range B-DAT
[−π/2,π/2], O
is O
computed O
as O

θ B-DAT
= O
arctan(gx/gz) O
and O

φ B-DAT
= O
arcsin(−gy), O
such O
that O
(0, O

0) B-DAT
represents O
looking O
straight O
ahead O

to B-DAT
the O
CCS O
origin. O
3.3 O
Recurrent O
Convolutional O
Neural O
Network O

We B-DAT
propose O
a O
Recurrent O
CNN O

Regression B-DAT
Network O
for O
3D O
gaze O

estimation. B-DAT
The O
network O
is O
divided O

in B-DAT
3 O
modules: O
(1) O
Individual O

, B-DAT
(2) O
Fusion, O
and O
(3) O

Temporal. B-DAT
First, O
the O
Individual O
module O
learns O

features B-DAT
from O
each O
appearance O
cue O

separately. B-DAT
It O
consists O
of O
a O

two-stream B-DAT
CNN, O
one O
devoted O
to O

the B-DAT
normalized O
face O
image O
stream O

and B-DAT
the O
other O
to O
the O

joint B-DAT
normalized O
eyes O
image. O
Next O

, B-DAT
the O
Fusion O
module O
combines O

the B-DAT
extracted O
features O
of O
each O

appearance B-DAT
stream O
in O
a O
single O

vector B-DAT
along O
with O
the O
normalized O

landmark B-DAT
coordinates. O
Then, O
it O
learns O

a B-DAT
joint O
representation O
between O
modalities O

in B-DAT
a O
late-fusion O
fashion. O
Both O

Individual B-DAT
and O
Fusion O
modules, O
further O

referred B-DAT
to O
as O
Static O
model, O

are B-DAT
applied O
to O
each O
frame O

of B-DAT
the O
sequence. O
Finally, O
the O

resulting B-DAT
feature O
vectors O
of O
each O

frame B-DAT
are O
input O
to O
the O

Temporal B-DAT
module O
based O
on O
a O

many-to-one B-DAT
recurrent O
network. O
This O
module O

leverages B-DAT
sequential O
information O
to O
predict O

the B-DAT
normalized O
2D O
gaze O
angles O

of B-DAT
the O
last O
frame O
of O

the B-DAT
sequence O
using O
a O
linear O

regression B-DAT
layer O
added O
on O
top O

of B-DAT
it. O
3.4 O
Implementation O
details O
3.4.1 O
Network O

details B-DAT

Each B-DAT
stream O
of O
the O
Individual O

module B-DAT
is O
based O
on O
the O

VGG-16 B-DAT
deep O
network O
[27], O
consisting O

of B-DAT
13 O
convolutional O
layers, O
5 O

max B-DAT
pooling O
layers, O
and O
1 O

fully B-DAT
connected O
(FC) O
layer O
with O

Rec- B-DAT
tified O
Linear O
Unit O
(ReLU) O

activations. B-DAT
The O
full-face O
stream O
follows O

the B-DAT
same O
configuration O
Citation O
Citation O
{Sugano, O
Matsushita, O
and O

Sato} B-DAT
2014 O

Citation B-DAT
Citation O
{Parkhi, O
Vedaldi, O
and O

Zisserman} B-DAT
2015 O

6 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
as O
the O
base O
network, O
having O

an B-DAT
input O
of O
224×224 O
pixels O

and B-DAT
a O
4096D O
FC O
layer O

. B-DAT
In O
contrast, O
the O
input O

joint B-DAT
eye O
image O
is O
smaller, O

with B-DAT
a O
final O
size O
of O
120 O

×48 B-DAT
pixels, O
so O
the O
number O

of B-DAT
pa- O
rameters O
is O
decreased O

proportionally. B-DAT
In O
this O
case, O
its O

last B-DAT
FC O
layer O
produces O
a O

1536D B-DAT
vector. O
A O
204D O
landmark O

coordinates B-DAT
vector O
is O
concatenated O
to O

the B-DAT
output O
of O
the O
FC O

layer B-DAT
of O
each O
stream, O
resulting O

in B-DAT
a O
5836D O
feature O
vector. O

Consequently, B-DAT
the O
Fusion O
module O
consists O

of B-DAT
2 O
5836D O
FC O
layers O

with B-DAT
ReLU O
activations O
and O
2 O

dropout B-DAT
layers O
between O
FCs O
as O

regularization. B-DAT
Finally, O
to O
model O
the O

temporal B-DAT
dependencies, O
we O
use O
a O

single B-DAT
GRU O
layer O
with O
128 O

units. B-DAT
The O
network O
is O
trained O
in O

a B-DAT
stage-wise O
fashion. O
First, O
we O

train B-DAT
the O
Static O
model O
and O

the B-DAT
final O
regression O
layer O
end-to-end O

on B-DAT
each O
individual O
frame O
of O

the B-DAT
training O
data. O
The O
convolutional O

blocks B-DAT
are O
pre-trained O
with O
the O

VGG-Face B-DAT
dataset O
[27], O
whereas O
the O

FCs B-DAT
are O
trained O
from O
scratch O

. B-DAT
Second, O
the O
training O
data O

is B-DAT
re-arranged O
by O
means O
of O

a B-DAT
sliding O
window O
with O
stride O
1 O
to O
build O
input O
sequences. O
Each O

sequence B-DAT
is O
composed O
of O
s O

= B-DAT
4 O
consecutive O
frames, O
whose O

gaze B-DAT
direction O
target O
is O
the O

gaze B-DAT
direction O
of O
the O
last O

frame B-DAT
of O
the O
sequence( O
{I(i−s+1 O

), B-DAT
. O
. O
. O
,I(i)}, O

g(i) B-DAT
) O
. O
Using O
this O
re-arranged O

training B-DAT
data, O
we O
extract O
features O

of B-DAT
each O

frame B-DAT
of O
the O
sequence O
from O

a B-DAT
frozen O
Individual O
module, O
fine-tune O

the B-DAT
Fusion O
layers, O
and O
train O

both, B-DAT
the O
Temporal O
module O
and O

a B-DAT
new O
final O
regression O
layer O

from B-DAT
scratch. O
This O
way, O
the O

network B-DAT
can O
exploit O
the O
temporal O

information B-DAT
to O
further O
refine O
the O

fusion B-DAT
weights. O
We O
trained O
the O
model O
using O

ADAM B-DAT
optimizer O
with O
an O
initial O

learning B-DAT
rate O
of O
0.0001, O
dropout O

of B-DAT
0.3, O
and O
batch O
size O

of B-DAT
64 O
frames. O
The O
number O

of B-DAT
epochs O
was O
experimentally O
set O

to B-DAT
21 O
for O
the O
first O

training B-DAT
stage O
and O
10 O
for O

the B-DAT
second. O
We O
use O
the O

average B-DAT
Euclidean O
distance O
between O
the O

predicted B-DAT
and O
ground-truth O
3D O
gaze O

vectors B-DAT
as O
loss O
function O

. B-DAT
3.4.2 O
Input O
pre-processing O

For B-DAT
this O
work O
we O
use O

head B-DAT
pose O
and O
eye O
locations O

in B-DAT
the O
3D O
scene O
provided O

by B-DAT
the O
dataset. O
The O
3D O

landmarks B-DAT
are O
extracted O
using O
the O

state-of-the-art B-DAT
method O
of O
Bulat O
and O

Tzimiropou- B-DAT
los O
[3], O
which O
is O

based B-DAT
on O
stacked O
hourglass O

networks B-DAT
[24]. O
During O
training, O
the O
original O
image O

is B-DAT
pre-processed O
to O
get O
the O

two B-DAT
normalized O
input O
images. O
The O

normalized B-DAT
whole-face O
patch O
is O
centered O

0.1 B-DAT
meters O
ahead O
of O
the O

head B-DAT
center O
in O
the O
head O

coordinate B-DAT
system, O
and O
Cn O
is O

defined B-DAT
such O
that O
the O
image O

has B-DAT
size O
of O
250× O
250 O

pixels. B-DAT
The O
difference O
between O
this O

size B-DAT
and O
the O
final O
input O

size B-DAT
allows O
us O
to O
perform O

random B-DAT
cropping O
and O
zooming O
to O

augment B-DAT
the O
data O
(explained O
in O

Section B-DAT
4.1). O
Similarly, O
each O
normalized O

eye B-DAT
patch O
is O
centered O
in O

their B-DAT
respective O
eye O
center O
locations O

. B-DAT
In O
this O
case, O
the O

virtual B-DAT
camera O
matrix O
is O
defined O

so B-DAT
that O
the O
image O
is O

cropped B-DAT
to O
70×58, O
while O
in O

practice B-DAT
the O
final O
patches O
have O

size B-DAT
of O
60×48. O
Landmarks O
are O

normalized B-DAT
using O
the O
same O
procedure O

and B-DAT
further O
pre-processed O
with O
mean O

subtraction B-DAT
and O
min-max O
normalization O
per O

axis. B-DAT
Finally, O
we O
divide O
them O

by B-DAT
a O
scaling O
factor O
w O

such B-DAT
that O
all O
coordinates O
are O

in B-DAT
the O
range O
[0,w]. O
This O

way, B-DAT
all O
concatenated O
feature O
values O

are B-DAT
in O
a O
similar O
range. O

After B-DAT
inference, O
the O
predicted O
normalized O

2D B-DAT
angles O
are O
de-normalized O
back O

to B-DAT
the O
original O
3D O
space. O
4 O
Experiments O
In O
this O
section O

, B-DAT
we O
evaluate O
the O
cross-subject O

3D B-DAT
gaze O
estimation O
task O
on O

a B-DAT
wide O
range O
of O
head O

poses B-DAT
and O
gaze O
directions. O
Furthermore, O

we B-DAT
validate O
the O
effectiveness O
of O

the B-DAT
proposed O
architecture O
comparing O
both O

static B-DAT
and O
temporal O
approaches. O
We O

report B-DAT
the O
error O
in O
terms O

of B-DAT
mean O
angular O
error O
between O

predicted B-DAT
and O
ground-truth O
3D O
gaze O

vectors. B-DAT
Note O
that O
due O
to O

the B-DAT
requirements O
of O
the O
temporal O

model B-DAT
not O
all O
the O
frames O

obtain B-DAT
a O
prediction. O
Therefore, O
for O

a B-DAT
Citation O
Citation O
{Parkhi, O
Vedaldi, O
and O

Zisserman} B-DAT
2015 O

Citation B-DAT
Citation O
{Bulat O
and O
Tzimiropoulos} O
2017 O

Citation B-DAT
Citation O
{Newell, O
Yang, O
and O

Deng} B-DAT
2016 O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
7 O

60 B-DAT
30 O
0 O
30 O
60 O
60 O

30 B-DAT
0 O

30 B-DAT
60 O

100 B-DAT
101 O

102 B-DAT
60 O
30 O
0 O
30 O
60 O

60 B-DAT
30 O

0 B-DAT
30 O

60 B-DAT
100 O

101 B-DAT
102 O

103 B-DAT
60 O
30 O
0 O
30 O
60 O

60 B-DAT
30 O

0 B-DAT
30 O

60 B-DAT
100 O

101 B-DAT
102 O

60 B-DAT
30 O
0 O
30 O
60 O
60 O

30 B-DAT
0 O

30 B-DAT
60 O

100 B-DAT
101 O

102 B-DAT
103 O

a) B-DAT
g O
(FT O
) O
(b) O

h B-DAT
(FT O
) O
(c) O
g O
( O

CS) B-DAT
(d) O
h O
(CS) O
Figure O
2: O
Ground-truth O
eye O
gaze O

g B-DAT
and O
head O
orientation O
h O

distribution B-DAT
on O
the O
filtered O
EYE O

- B-DAT
DIAP O
dataset O
for O
CS O

and B-DAT
FT O
settings, O
in O
terms O

of B-DAT
x- O
and O
y- O
angles. O
fair O
comparison, O
the O
reported O
results O

for B-DAT
static O
models O
disregard O
such O

frames B-DAT
when O
temporal O
models O
are O

included B-DAT
in O
the O
comparison O

. B-DAT
4.1 O
Training O
data O

There B-DAT
are O
few O
publicly O
available O

datasets B-DAT
devoted O
to O
3D O
gaze O

estimation B-DAT
and O
most O
of O
them O

focus B-DAT
on O
HCI O
with O
a O

limited B-DAT
range O
of O
head O
pose O

and B-DAT
gaze O
directions. O
Therefore, O
we O

use B-DAT
VGA O
videos O
from O
the O

publicly-available B-DAT
EYEDIAP O
dataset O
[7] O
to O

perform B-DAT
the O
experimental O
evaluation, O
as O

it B-DAT
is O
currently O
the O
only O

one B-DAT
containing O
video O
sequences O
with O

a B-DAT
wide O
range O
of O
head O

poses B-DAT
and O
showing O
the O
full O

face. B-DAT
This O
dataset O
consists O
of O
3 O

-minute B-DAT
videos O
of O
16 O
subjects O

looking B-DAT
at O
two O
types O
of O

targets: B-DAT
continuous O
screen O
targets O
on O

a B-DAT
fixed O
monitor O
(CS), O
and O

floating B-DAT
physical O
targets O
(FT O
). O

The B-DAT
videos O
are O
further O
divided O

into B-DAT
static O
(S) O
and O
moving O
( O

M) B-DAT
head O
pose O
for O
each O

of B-DAT
the O
subjects. O
Subjects O
12-16 O

were B-DAT
recorded O
with O
2 O
different O

lighting B-DAT
conditions. O
For O
evaluation, O
we O
filtered O
out O

those B-DAT
frames O
that O
fulfilled O
at O

least B-DAT
one O
of O
the O
following O

conditions: B-DAT
(1) O
face O
or O
landmarks O

not B-DAT
detected; O
(2) O
subject O
not O

looking B-DAT
at O
the O
target; O
(3 O

) B-DAT
3D O
head O
pose, O
eyes O

or B-DAT
target O
location O
not O
properly O

recovered; B-DAT
and O
(4) O
eyeball O
rotations O

violating B-DAT
physical O

constraints B-DAT
(|θ O
| O
≤ O
40 O

◦, B-DAT
|φ O
| O
≤ O
30 O

◦) B-DAT
[23]. O
Note O
that O
we O

purposely B-DAT
do O
not O
filter O
eye O

blinking B-DAT
moments O
to O
learn O
their O

dynamics B-DAT
with O
the O
temporal O
model, O

which B-DAT
may O
produce O
some O
outliers O

with B-DAT
a O
higher O
prediction O
error O

due B-DAT
to O
a O
less O
accurate O

ground B-DAT
truth. O
Figure O
2 O
shows O

the B-DAT
distribution O
of O
gaze O
directions O

and B-DAT
head O
poses O
for O
both O

filtered B-DAT
CS O
and O
FT O
cases. O
We O
applied O
data O
augmentation O
to O

the B-DAT
training O
set O
with O
the O

following B-DAT
random O
transforma- O
tions: O
horizontal O

flip, B-DAT
shifts O
of O
up O
to O

5 B-DAT
pixels, O
zoom O
of O
up O

to B-DAT
2%, O
brightness O
changes O
by O

a B-DAT
factor O
in O
the O
range O

[0.4,1.75], B-DAT
and O
additive O
Gaussian O
noise O

with B-DAT
σ2 O
= O
0.03 O

. B-DAT
4.2 O
Evaluation O
of O
static O
modalities O

First, B-DAT
we O
evaluate O
the O
contribution O

of B-DAT
each O
static O
modality O
on O

the B-DAT
FT O
scenario. O
We O
divided O

the B-DAT
16 O
participants O
into O
4 O

groups, B-DAT
such O
that O
appearance O
variability O

was B-DAT
maximized O
while O
maintaining O
a O

similar B-DAT
number O
of O
training O
samples O

per B-DAT
group. O
Each O
static O
model O

was B-DAT
trained O
end-to-end O
performing O
4-fold O

cross-validation B-DAT
using O
different O
combinations O
of O

input B-DAT
modal- O
ities. O
Since O
the O

number B-DAT
of O
fusion O
units O
depends O

on B-DAT
the O
number O
of O
input O

modalities, B-DAT
we O
also O
compare O
different O

fusion B-DAT
layer O
sizes. O
The O
effect O

of B-DAT
data O
normalization O
is O
also O

evaluated B-DAT
by O
training O
a O
not-normalized O

face B-DAT
model O
where O
the O
input O

image B-DAT
is O
the O
face O
bounding O

box B-DAT
with O
square O
size O
the O

maximum B-DAT
distance O
between O
2D O
landmarks. O
Citation O
Citation O
{Funesprotect O
unhbox O
voidb@x O

penalty B-DAT
@M O

Mora, B-DAT
Monay, O
and O
Odobez} O
2014{} O
Citation O
Citation O
{MSC O

8 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
0 O
1 O
2 O
3 O
4 O

5 B-DAT
6 O
7 O
8 O
9 O

10 B-DAT
11 O
An O
gl O

e B-DAT
er O

ro B-DAT
r O
( O
de O
gr O

ee B-DAT
s) O
6.9 O
6.43 O
5.58 O
5.71 O
5.59 O

5.55 B-DAT
5.52 O

OF-4096 B-DAT
NE-1536 O
NF-4096 O
NF-5632 O
NFL-4300 O

NFE-5632 B-DAT
NFEL-5836 O
Figure O
3: O
Performance O
evaluation O
of O

the B-DAT
Static O
network O
using O
different O

input B-DAT
modali- O
ties O
(O O

- B-DAT
Not O
normalized, O
N O

- B-DAT
Normalized, O
F O
- O
Face O

, B-DAT
E O
- O
Eyes, O

L B-DAT
- O
3D O
Landmarks) O
and O

size B-DAT
of O
fusion O
layers O
on O

the B-DAT
FT O
scenario. O
Floating O
Target O
Screen O
Target O
0 O

1 B-DAT
2 O
3 O
4 O
5 O

6 B-DAT
7 O
8 O
9 O

10 B-DAT
11 O
An O
gl O

e B-DAT
er O

ro B-DAT
r O
( O
de O
gr O

ee B-DAT
s) O
6.36 O
5.43 O
5.19 O
4.2 O
3.38 O

3.4 B-DAT

MPIIGaze B-DAT
Static O
Temporal O
Figure O
4: O
Performance O
comparison O
among O

MPIIGaze B-DAT
method O
[42] O
and O
our O

Static B-DAT
and O
Temporal O
versions O
of O

the B-DAT
proposed O
network O
for O
FT O

and B-DAT
CS O
scenarios O

. B-DAT
As O
shown O
in O
Figure O
3 O

, B-DAT
all O
models O
that O
take O

normalized B-DAT
full-face O
information O
as O
input O

achieve B-DAT
better O
performance O
than O
the O

eyes-only B-DAT
model. O
More O
specifically, O
the O

combination B-DAT
of O
face, O
eyes O
and O

landmarks B-DAT
outperforms O
all O
the O
other O

combinations B-DAT
by O
a O
small O
but O

significant B-DAT
margin O
(paired O
Wilcoxon O
test, O

p B-DAT
< O
0.0001). O
The O
standard O

deviation B-DAT
of O
the O
best-performing O
model O

is B-DAT
reduced O
compared O
to O
the O

face B-DAT
and O
eyes O
model, O
suggesting O

a B-DAT
regularizing O
effect O
due O
to O

the B-DAT
addition O
of O
landmarks. O
The O

not-normalized B-DAT
face-only O
model O
shows O
the O

largest B-DAT
error, O
proving O
the O
impact O

of B-DAT
normalization O
to O
reduce O
the O

appearance B-DAT
variability. O
Furthermore, O
our O
results O

indicate B-DAT
that O
the O
increase O
of O

fusion B-DAT
units O
is O
not O
correlated O

with B-DAT
a O
better O
performance. O
4.3 O
Static O
gaze O
regression: O
comparison O

with B-DAT
existing O
methods O

We B-DAT
compare O
our O
best-performing O
static O

model B-DAT
with O
three O
baselines. O
Head: O

Treating B-DAT
the O
head O
pose O
directly O

as B-DAT
gaze O
direction. O
PR-ALR: O
Method O

that B-DAT
relies O
on O
RGB-D O
data O

to B-DAT
rectify O
the O
eye O
images O

viewpoint B-DAT
into O
a O
canonical O
head O

pose B-DAT
using O
a O
3DMM. O
It O

then B-DAT
learns O
an O
RGB O
gaze O

appearance B-DAT
model O
using O
ALR O
[21]. O

Predicted B-DAT
3D O
vectors O
for O
FT-S O

scenario B-DAT
are O
provided O
by O
EYEDIAP O

dataset. B-DAT
MPIIGaze:. O
State-of-the-art O
full-face O
3D O

gaze B-DAT
estimation O
method O
[42]. O
They O

use B-DAT
an O
Alexnet-based O
CNN O
model O

with B-DAT
spatial O
weights O
to O
enhance O

information B-DAT
in O
different O
facial O
regions. O

We B-DAT
fine-tuned O
it O
with O
the O

filtered B-DAT
EYEDIAP O
subsets O
using O
our O

training B-DAT
parameters O
and O
normalization O
procedure. O
In O
addition O
to O
the O
aforementioned O

FT-based B-DAT
evaluation O
setup, O
we O
also O

evaluate B-DAT
our O
method O
on O
the O

CS B-DAT
scenario. O
In O
this O
case O

there B-DAT
are O
only O
14 O
participants O

available, B-DAT
so O
we O
divided O
them O

in B-DAT
5 O
groups O
and O
performed O

5-fold B-DAT
cross-validation. O
In O
Figure O
4 O

we B-DAT
compare O
our O
method O
to O

MPIIGaze, B-DAT
achieving O
a O
statistically O
significant O

improvement B-DAT
of O
14.6% O
and O
19.5 O

% B-DAT
on O
FT O
and O
CS O

scenarios, B-DAT
respectively O
(paired O
Wilcoxon O
test, O

p B-DAT
< O
0.0001). O
We O
can O

observe B-DAT
that O
a O
re- O
stricted O

gaze B-DAT
target O
benefits O
the O
performance O

of B-DAT
all O
methods, O
compared O
to O

a B-DAT
more O
challenging O
unrestricted O
setting O

with B-DAT
a O
wider O
range O
of O

head B-DAT
poses O
and O
gaze O
directions. O
Table O
2 O
provides O
a O
detailed O

comparison B-DAT
on O
every O
participant, O
performing O

leave-one-out B-DAT
cross-validation O
on O
the O
FT O

scenario B-DAT
for O
static O
and O
moving O

head B-DAT
separately. O
Results O
show O
that O

, B-DAT
as O
expected, O
facial O
appearance O

and B-DAT
head O
pose O
have O
a O

noticeable B-DAT
impact O
on O
gaze O
accuracy, O

with B-DAT
average O
error O
differences O
of O

up B-DAT
to O
7.7◦ O
among O
participants. O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2015 O
Citation O
Citation O
{Mora O
and O
Odobez O

} B-DAT
2012 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2015 O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
9 O

Method B-DAT
1 O
2 O
3 O
4 O
5 O
6 O
7 O
8 O
9 O
10 O

11 B-DAT
12 O
13 O
14 O
15 O

16 B-DAT
Avg. O
Head O
23.5 O
22.1 O

20.3 B-DAT
23.6 O
23.2 O
23.2 O
23.6 O

21.2 B-DAT
26.7 O
23.6 O
23.1 O
24.4 O

23.3 B-DAT
24.0 O
24.5 O
22.8 O
23.3 O

PR-ALR B-DAT
12.3 O
12.0 O
12.4 O
11.3 O

15.5 B-DAT
12.9 O
17.9 O
11.8 O
17.3 O

13.4 B-DAT
13.4 O
14.3 O
15.2 O
13.6 O

14.4 B-DAT
14.6 O
13.9 O
MPIIGaze O
5.3 O

5.1 B-DAT
5.7 O
4.7 O
7.3 O
15.1 O

10.8 B-DAT
5.7 O
9.9 O
7.1 O
5.0 O

5.7 B-DAT
7.4 O
3.8 O
4.8 O
5.5 O

6.8 B-DAT
Static O
3.9 O
4.1 O
4.2 O

3.9 B-DAT
6.0 O
6.4 O
7.2 O
3.6 O

7.1 B-DAT
5.0 O
5.7 O
6.7 O
3.9 O

4.7 B-DAT
5.1 O
4.2 O
5.1 O
Temporal O

4.0 B-DAT
4.9 O
4.3 O
4.1 O
6.1 O

6.5 B-DAT
6.6 O
3.9 O
7.8 O
6.1 O

4.7 B-DAT
5.6 O
4.7 O
3.5 O
5.9 O

4.6 B-DAT
5.2 O
Head O
19.3 O
14.2 O

16.4 B-DAT
19.9 O
16.8 O
21.9 O
16.1 O

24.2 B-DAT
20.3 O
19.9 O
18.8 O
22.3 O

18.1 B-DAT
14.9 O
16.2 O
19.3 O
18.7 O

MPIIGaze B-DAT
7.6 O
6.2 O
5.7 O
8.7 O

10.1 B-DAT
12.0 O
12.2 O
6.1 O
8.3 O

5.9 B-DAT
6.1 O
6.2 O
7.4 O
4.7 O

4.4 B-DAT
6.0 O
7.3 O
Static O
5.8 O

5.7 B-DAT
4.4 O
7.5 O
6.7 O
8.8 O

11.6 B-DAT
5.5 O
8.3 O
5.5 O
5.2 O

6.3 B-DAT
5.3 O
3.9 O
4.3 O
5.6 O

6.3 B-DAT
Temporal O
6.1 O
5.6 O
4.5 O

7.5 B-DAT
6.4 O
8.2 O
12.0 O
5.0 O

7.5 B-DAT
5.4 O
5.0 O
5.8 O
6.6 O

4.0 B-DAT
4.5 O
5.8 O
6.2 O

Table B-DAT
2: O
Gaze O
angular O
error O

comparison B-DAT
for O
static O
(top O
half) O

and B-DAT
moving O
(bottom O
half) O
head O

pose B-DAT
for O
each O
subject O
in O

the B-DAT
FT O
scenario. O
Best O
results O

in B-DAT
bold. O
−80 O
−40 O
0 O
40 O
80−80 O

40 B-DAT
0 O

40 B-DAT
80 O

0 B-DAT
5 O

10 B-DAT
15 O

20 B-DAT
25 O

30 B-DAT
35 O

80 B-DAT
−40 O
0 O
40 O
80−80 O
−40 O

0 B-DAT
40 O

80 B-DAT
−10 O

8 B-DAT
−6 O

4 B-DAT
−2 O

0 B-DAT
2 O

4 B-DAT
6 O

8 B-DAT
10 O

80 B-DAT
−40 O
0 O
40 O
80−80 O
−40 O

0 B-DAT
40 O

80 B-DAT
0 O

5 B-DAT
10 O

15 B-DAT
20 O

25 B-DAT
30 O

35 B-DAT
−80 O
−40 O
0 O
40 O
80−80 O

40 B-DAT
0 O

40 B-DAT
80 O

10 B-DAT
−8 O

6 B-DAT
−4 O

2 B-DAT
0 O

2 B-DAT
4 O

6 B-DAT
8 O

10 B-DAT
(a) O
Gaze O
space O
(b) O
Head O

orientation B-DAT
space O

Figure B-DAT
5: O
Angular O
error O
distribution O

across B-DAT
gaze O
(a) O
and O
head O

orientation B-DAT
(b) O
spaces O
in O
the O

FT B-DAT
setting, O
in O
terms O
of O

x- B-DAT
and O
y- O
angles. O
For O

each B-DAT
space, O
we O
depict O
the O

Static B-DAT
model O
performance O
(left) O
and O

the B-DAT
contribution O
of O
the O
Temporal O

model B-DAT
versus O
Static O
(right). O
In O

the B-DAT
latter, O
positive O
difference O
means O

higher B-DAT
improvement O
of O
the O
Temporal O

model. B-DAT
4.4 O
Evaluation O
of O
the O
temporal O

network B-DAT

In B-DAT
this O
section, O
we O
evaluate O

the B-DAT
contribution O
of O
adding O
the O

temporal B-DAT
module O
to O
the O
static O

model. B-DAT
To O
do O
so, O
we O

trained B-DAT
a O
lower-dimensional O
version O
of O

the B-DAT
static O
network O
with O
compa- O

rable B-DAT
performance O
to O
the O
original, O

reducing B-DAT
the O
number O
of O
units O

of B-DAT
the O
second O
fusion O
layer O

to B-DAT
2918. O
Results O
are O
reported O

in B-DAT
Figure O
4 O
and O
Table O
2 O

. B-DAT
One O
can O
observe O
that O

using B-DAT
sequential O
information O
is O
helpful O

on B-DAT
the O
FT O
scenario, O
outperforming O

the B-DAT
static O
model O
by O
a O

statistically B-DAT
significant O
4.4% O
(paired O
Wilcoxon O

test, B-DAT
p O
< O
0.0001). O
This O

contribution B-DAT
is O
more O
noticeable O
in O

the B-DAT
moving O
head O
setting, O
proving O

that B-DAT
the O
temporal O
model O
can O

benefit B-DAT
from O
head O
motion O
information. O

In B-DAT
contrast, O
such O
information O
seems O

to B-DAT
be O
less O
meaningful O
in O

the B-DAT
CS O
scenario, O
where O
the O

obtained B-DAT
error O
is O
already O
very O

low B-DAT
for O
a O
cross-subject O
setting O

and B-DAT
the O
amount O
of O
head O

movement B-DAT
declines. O
Figure O
5 O
further O
explores O
the O

error B-DAT
distribution O
of O
the O
static O

network B-DAT
and O
the O
impact O
of O

sequential B-DAT
information. O
We O
can O
observe O

that B-DAT
the O
accuracy O
of O
the O

static B-DAT
model O
drops O
with O
extreme O

head B-DAT
poses O
and O
gaze O
directions O

, B-DAT
which O
can O
also O
be O

correlated B-DAT
to O
having O
less O
data O

in B-DAT
those O
areas. O
Compared O
to O

the B-DAT
static O
model, O
the O
temporal O

model B-DAT
particularly O
benefits O
gaze O
targets O

from B-DAT
mid-range O
upwards. O
Its O
contribution O

is B-DAT
less O
clear O
for O
extreme O

targets, B-DAT
probably O
again O
due O
to O

data B-DAT
imbalance. O
Finally, O
we O
evaluated O
the O
effect O

of B-DAT
different O
recurrent O
architectures O
for O

the B-DAT
temporal O
model. O
In O
particular O

, B-DAT
we O
tested O
1 O
(128 O

units) B-DAT
and O
2 O
(256-128 O
units) O

LSTM B-DAT
and O
GRU O
lay- O
ers, O

with B-DAT
1 O
GRU O
layer O
obtaining O

slightly B-DAT
superior O
results O
(up O
to O
0 O

.12◦). B-DAT
We O
also O
assessed O
the O

effect B-DAT
of O
sequence O
length O
fixing O

s B-DAT
in O
the O
range O
{4,7,10}, O

with B-DAT
s O
= O
7 O
performing O

worse B-DAT
than O
the O
other O
two O
( O

up B-DAT
to O
0 O

14 B-DAT

10 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
5 O
Conclusions O
In O
this O
work O

, B-DAT
we O
studied O
the O
combination O

of B-DAT
full-face O
and O
eye O
images O

along B-DAT
with O
facial O
land- O
marks O

for B-DAT
person- O
and O
head O
pose-independent O

3D B-DAT
gaze O
estimation. O
Consequently, O
we O

pro- B-DAT
posed O
a O
multi-stream O
recurrent O

CNN B-DAT
network O
that O
leverages O
the O

sequential B-DAT
information O
of O
eye O
and O

head B-DAT
movements. O
Both O
static O
and O

temporal B-DAT
versions O
of O
our O
approach O

significantly B-DAT
outperform O
current O
state-of-the-art O
3D O

gaze B-DAT
estimation O
methods O
on O
a O

wide B-DAT
range O
of O
head O
poses O

and B-DAT
gaze O
directions. O
We O
showed O

that B-DAT
adding O
geometry O
features O
to O

appearance-based B-DAT
methods O
has O
a O
regularizing O

effect B-DAT
on O
the O
accuracy. O
Adding O

sequential B-DAT
information O
further O
benefits O
the O

final B-DAT
performance O
compared O
to O
static-only O

input, B-DAT
especially O
from O
mid-range O
up- O

wards B-DAT
and O
in O
those O
cases O

where B-DAT
head O
motion O
is O
present. O

The B-DAT
effect O
in O
very O
extreme O

head B-DAT
poses O
is O
not O
clear O

due B-DAT
to O
data O
imbalance, O
suggesting O

the B-DAT
importance O
of O
learning O
from O

a B-DAT
con- O
tinuous, O
balanced O
dataset O

including B-DAT
all O
head O
poses O
and O

gaze B-DAT
directions O
of O
interest. O
To O

the B-DAT
best O
of O
our O
knowledge, O

this B-DAT
is O
the O
first O
attempt O

to B-DAT
exploit O
the O
temporal O
modality O

in B-DAT
the O
context O
of O
gaze O

estimation B-DAT
from O
remote O
cameras. O
As O

future B-DAT
work, O
we O
will O
further O

explore B-DAT
extracting O
meaningful O
temporal O
representations O

of B-DAT
gaze O
dynamics, O
considering O
3DCNNs O

as B-DAT
well O
as O
the O
encoding O

of B-DAT
deep O
features O
around O
particular O

tracked B-DAT
face O
landmarks O
[14]. O
Acknowledgements O
This O
work O
has O
been O

partially B-DAT
supported O
by O
the O
Spanish O

project B-DAT
TIN2016-74946-P O
(MINECO/ O
FEDER, O
UE O

), B-DAT
CERCA O
Programme O
/ O
Generalitat O

de B-DAT
Catalunya, O
and O
the O
FP7 O

people B-DAT
program O
(Marie O
Curie O
Actions), O

REA B-DAT
grant O
agreement O
no O
FP7-607139 O
( O

iCARE B-DAT
- O
Improving O
Children O
Auditory O

REhabilitation). B-DAT
We O
gratefully O
acknowledge O
the O

support B-DAT
of O
NVIDIA O
Corporation O
with O

the B-DAT
donation O
of O
the O
GPU O

used B-DAT
for O
this O
research. O
Portions O

of B-DAT
the O
research O
in O
this O

pa- B-DAT
per O
used O
the O
EYEDIAP O

dataset B-DAT
made O
available O
by O
the O

Idiap B-DAT
Research O
Institute, O
Martigny, O
Switzerland. O
References O
[1] O
Nicola O
C O
Anderson O

, B-DAT
Evan O
F O
Risko, O
and O

Alan B-DAT
Kingstone. O
Motion O
influences O
gaze O

di- B-DAT
rection O
discrimination O
and O
disambiguates O
contradictory O

luminance B-DAT
cues. O
Psychonomic O
bulletin O

& B-DAT
review, O
23(3):817–823, O
2016 O

. B-DAT
[2] O
Shumeet O
Baluja O
and O
Dean O

Pomerleau. B-DAT
Non-intrusive O
gaze O
tracking O
using O

artificial B-DAT
neu- O
ral O
networks. O
In O

Advances B-DAT
in O
Neural O
Information O
Processing O

Systems, B-DAT
pages O
753–760, O
1994 O

. B-DAT
[3] O
Adrian O
Bulat O
and O
Georgios O

Tzimiropoulos. B-DAT
How O
far O
are O
we O

from B-DAT
solving O
the O
2d O

& B-DAT
3d O
face O
alignment O
problem O

? B-DAT
(and O
a O
dataset O
of O
230, O

000 B-DAT
3d O
facial O
landmarks). O
In O

Interna- B-DAT
tional O
Conference O
on O
Computer O

Vision, B-DAT
2017. O
[4] O
Haoping O
Deng O
and O
Wangjiang O

Zhu. B-DAT
Monocular O
free-head O
3d O
gaze O

tracking B-DAT
with O
deep O
learning O
and O

geometry B-DAT
constraints. O
In O
Computer O
Vision O

(ICCV), B-DAT
2017 O
IEEE O
Interna- O
tional O

Conference B-DAT
on, O
pages O
3162–3171. O
IEEE O

, B-DAT
2017. O
[5] O
Onur O
Ferhat O
and O
Fernando O

Vilariño. B-DAT
Low O
cost O
eye O
tracking O

. B-DAT
Computational O
intelligence O
and O
neuroscience, O
2016 O

:17, B-DAT
2016. O
Citation O
Citation O
{Jung, O
Lee, O
Yim O

, B-DAT
Park, O
and O
Kim} O
2015 O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
11 O

6] B-DAT
Kenneth O
A O
Funes-Mora O
and O

Jean-Marc B-DAT
Odobez. O
Gaze O
estimation O
in O

the B-DAT
3D O
space O
using O
RGB-D O

sensors. B-DAT
International O
Journal O
of O
Computer O

Vision, B-DAT
118(2):194–216, O
2016. O
[7] O
Kenneth O
Alberto O
Funes O
Mora O

, B-DAT
Florent O
Monay, O
and O
Jean-Marc O

Odobez. B-DAT
Eyediap: O
A O
database O
for O

the B-DAT
development O
and O
evaluation O
of O

gaze B-DAT
estimation O
algorithms O
from O
rgb O

and B-DAT
rgb-d O
cameras. O
In O
Proceedings O

of B-DAT
the O
ACM O
Symposium O
on O

Eye B-DAT
Tracking O
Research O
and O
Applications. O

ACM, B-DAT
March O
2014. O
doi: O
10.1145/2578153.2578190. O
[8] O
Kenneth O
Alberto O
Funes O
Mora O

, B-DAT
Florent O
Monay, O
and O
Jean-Marc O

Odobez. B-DAT
Eyediap: O
A O
database O
for O

the B-DAT
development O
and O
evaluation O
of O

gaze B-DAT
estimation O
algorithms O
from O
rgb O

and B-DAT
rgb-d O
cameras. O
In O
Proceedings O

of B-DAT
the O
Symposium O
on O
Eye O

Tracking B-DAT
Research O
and O
Applications, O
pages O
255 O

–258. B-DAT
ACM, O
2014. O
[9] O
Quentin O
Guillon, O
Nouchine O
Hadjikhani O

, B-DAT
Sophie O
Baduel, O
and O
Bernadette O

Rogé. B-DAT
Visual O
social O
attention O
in O

autism B-DAT
spectrum O
disorder: O
Insights O
from O

eye B-DAT
tracking O
studies. O
Neu- O

roscience B-DAT
& O
Biobehavioral O
Reviews, O
42:279–297, O
2014 O

. B-DAT
[10] O
Dan O
Witzner O
Hansen O
and O

Qiang B-DAT
Ji. O
In O
the O
eye O

of B-DAT
the O
beholder: O
A O
survey O

of B-DAT
models O
for O
eyes O
and O

gaze. B-DAT
IEEE O
transactions O
on O
pattern O

analysis B-DAT
and O
machine O
intelligence, O
32(3 O

): B-DAT
478–500, O
2010. O
[11] O
Qiong O
Huang, O
Ashok O
Veeraraghavan O

, B-DAT
and O
Ashutosh O
Sabharwal. O
Tabletgaze: O

dataset B-DAT
and O
analysis O
for O
unconstrained O

appearance-based B-DAT
gaze O
estimation O
in O
mobile O

tablets. B-DAT
Machine O
Vision O
and O
Applications, O
28 O

(5-6):445–461, B-DAT
2017. O
[12] O
Robert O
JK O
Jacob O
and O

Keith B-DAT
S O
Karn. O
Eye O
tracking O

in B-DAT
human-computer O
interaction O
and O
usability O

research: B-DAT
Ready O
to O
deliver O
the O

promises. B-DAT
In O
The O
mind’s O
eye O

, B-DAT
pages O
573–605. O
Elsevier, O
2003. O
[13] O
László O
A O
Jeni O
and O

Jeffrey B-DAT
F O
Cohn. O
Person-independent O
3d O

gaze B-DAT
estimation O
using O
face O
frontalization O

. B-DAT
In O
Proceedings O
of O
the O

IEEE B-DAT
Conference O
on O
Computer O
Vision O

and B-DAT
Pattern O
Recognition O
Workshops, O
pages O
87 O

–95, B-DAT
2016. O
[14] O
Heechul O
Jung, O
Sihaeng O
Lee O

, B-DAT
Junho O
Yim, O
Sunjeong O
Park, O

and B-DAT
Junmo O
Kim. O
Joint O
fine- O

tuning B-DAT
in O
deep O
neural O
networks O

for B-DAT
facial O
expression O
recognition. O
In O

Computer B-DAT
Vision O
(ICCV), O
2015 O
IEEE O

International B-DAT
Conference O
on, O
pages O
2983–2991. O

IEEE, B-DAT
2015. O
[15] O
Anuradha O
Kar O
and O
Peter O

Corcoran. B-DAT
A O
review O
and O
analysis O

of B-DAT
eye-gaze O
estimation O
sys- O
tems O

, B-DAT
algorithms O
and O
performance O
evaluation O

methods B-DAT
in O
consumer O
platforms. O
IEEE O

Access, B-DAT
5:16495–16519, O
2017. O
[16] O
Kyle O
Krafka, O
Aditya O
Khosla O

, B-DAT
Petr O
Kellnhofer, O
Harini O
Kannan, O

Suchendra B-DAT
Bhandarkar, O
Wojciech O
Matusik, O
and O

Antonio B-DAT
Torralba. O
Eye O
tracking O
for O

everyone. B-DAT
In O
Computer O
Vision O
and O

Pattern B-DAT
Recognition O
(CVPR), O
2016 O
IEEE O

Conference B-DAT
on, O
pages O
2176–2184. O
IEEE, O
2016 O

. B-DAT
[17] O
Simon O
P O
Liversedge O
and O

John B-DAT
M O
Findlay. O
Saccadic O
eye O

movements B-DAT
and O
cognition. O
Trends O
in O

cognitive B-DAT
sciences, O
4(1):6–14, O
2000 O

. B-DAT
[18] O
Feng O
Lu, O
Takahiro O
Okabe O

, B-DAT
Yusuke O
Sugano, O
and O
Yoichi O

Sato. B-DAT
A O
head O
pose-free O
approach O

for B-DAT
appearance-based O
gaze O
estimation. O
In O

BMVC, B-DAT
pages O
1–11, O
2011 O

12 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
[19] O
Feng O
Lu, O
Yusuke O
Sugano O

, B-DAT
Takahiro O
Okabe, O
and O
Yoichi O

Sato. B-DAT
Inferring O
human O
gaze O
from O

appearance B-DAT
via O
adaptive O
linear O
regression. O

In B-DAT
Computer O
Vision O
(ICCV), O
2011 O

IEEE B-DAT
International O
Conference O
on, O
pages O
153 O

–160. B-DAT
IEEE, O
2011. O
[20] O
Päivi O
Majaranta O
and O
Andreas O

Bulling. B-DAT
Eye O
tracking O
and O
eye-based O

human–computer B-DAT
interaction. O
In O
Advances O
in O

physiological B-DAT
computing, O
pages O
39–65. O
Springer O

, B-DAT
2014. O
[21] O
Kenneth O
Alberto O
Funes O
Mora O

and B-DAT
Jean-Marc O
Odobez. O
Gaze O
estimation O

from B-DAT
multi- O
modal O
kinect O
data O

. B-DAT
In O
Computer O
Vision O
and O

Pattern B-DAT
Recognition O
Workshops O
(CVPRW), O
2012 O

IEEE B-DAT
Computer O
Society O
Conference O
on, O

pages B-DAT
25–30. O
IEEE, O
2012. O
[22] O
Carlos O
Hitoshi O
Morimoto, O
Arnon O

Amir, B-DAT
and O
Myron O
Flickner. O
Detecting O

eye B-DAT
position O
and O
gaze O
from O

a B-DAT
single O
camera O
and O
2 O

light B-DAT
sources. O
In O
Pattern O
Recognition O

, B-DAT
2002. O
Proceedings. O
16th O
International O

Conference B-DAT
on, O
volume O
4, O
pages O
314 O

–317. B-DAT
IEEE, O
2002. O
[23] O
IMO O
MSC. O
Circ. O
982 O

(2000) B-DAT
guidelines O
on O
ergonomic O
criteria O

for B-DAT
bridge O
equipment O
and O
layout O

. B-DAT
[24] O
Alejandro O
Newell, O
Kaiyu O
Yang O

, B-DAT
and O
Jia O
Deng. O
Stacked O

hourglass B-DAT
networks O
for O
hu- O
man O

pose B-DAT
estimation. O
In O
European O
Conference O

on B-DAT
Computer O
Vision, O
pages O
483–499. O

Springer, B-DAT
2016. O
[25] O
Yasuhiro O
Ono, O
Takahiro O
Okabe O

, B-DAT
and O
Yoichi O
Sato. O
Gaze O

estimation B-DAT
from O
low O
resolution O
images. O

In B-DAT
Pacific-Rim O
Symposium O
on O
Image O

and B-DAT
Video O
Technology, O
pages O
178–188. O

Springer, B-DAT
2006. O
[26] O
Cristina O
Palmero, O
Elisabeth O
A O

. B-DAT
van O
Dam, O
Sergio O
Escalera, O

Mike B-DAT
Kelia, O
Guido O
F. O
Lichtert, O

Lucas B-DAT
P.J.J O
Noldus, O
Andrew O
J. O

Spink, B-DAT
and O
Astrid O
van O
Wieringen. O

Automatic B-DAT
mutual O
gaze O
detection O
in O

face-to-face B-DAT
dyadic O
interaction O
videos. O
In O

Proceedings B-DAT
of O
Measuring O
Behavior, O
pages O
158 O

–163, B-DAT
2018. O
[27] O
Omkar O
M. O
Parkhi, O
Andrea O

Vedaldi, B-DAT
and O
Andrew O
Zisserman. O
Deep O

face B-DAT
recognition. O
In O
British O
Machine O

Vision B-DAT
Conference, O
2015 O

. B-DAT
[28] O
Derek O
R O
Rutter O
and O

Kevin B-DAT
Durkin. O
Turn-taking O
in O
mother–infant O

interaction: B-DAT
An O
exam- O
ination O
of O

vocalizations B-DAT
and O
gaze. O
Developmental O
psychology O

, B-DAT
23(1):54, O
1987. O
[29] O
Brian O
A O
Smith, O
Qi O

Yin, B-DAT
Steven O
K O
Feiner, O
and O

Shree B-DAT
K O
Nayar. O
Gaze O
locking O

: B-DAT
passive O
eye O
contact O
detection O

for B-DAT
human-object O
interaction. O
In O
Proceedings O

of B-DAT
the O
26th O
annual O
ACM O

symposium B-DAT
on O
User O
interface O
software O

and B-DAT
technology, O
pages O
271–280. O
ACM, O
2013 O

. B-DAT
[30] O
Yusuke O
Sugano, O
Yasuyuki O
Matsushita O

, B-DAT
and O
Yoichi O
Sato. O
Appearance-based O

gaze B-DAT
es- O
timation O
using O
visual O

saliency. B-DAT
IEEE O
transactions O
on O
pattern O

analysis B-DAT
and O
machine O
intelligence, O
35(2):329–341, O
2013 O

. B-DAT
[31] O
Yusuke O
Sugano, O
Yasuyuki O
Matsushita O

, B-DAT
and O
Yoichi O
Sato. O
Learning-by-synthesis O

for B-DAT
appearance-based O
3d O
gaze O
estimation. O

In B-DAT
Computer O
Vision O
and O
Pattern O

Recognition B-DAT
(CVPR), O
2014 O
IEEE O
Conference O

on, B-DAT
pages O
1821–1828. O
IEEE, O
2014. O
[32] O
Kar-Han O
Tan, O
David O
J O

Kriegman, B-DAT
and O
Narendra O
Ahuja. O
Appearance-based O

eye B-DAT
gaze O
es- O
timation. O
In O

Applications B-DAT
of O
Computer O
Vision, O
2002.(WACV O

2002). B-DAT
Proceedings. O
Sixth O
IEEE O
Workshop O

on, B-DAT
pages O
191–195. O
IEEE, O
2002 O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
13 O

33] B-DAT
Ronda O
Venkateswarlu O
et O
al. O

Eye B-DAT
gaze O
estimation O
from O
a O

single B-DAT
image O
of O
one O
eye. O

In B-DAT
Computer O
Vision, O
2003. O
Proceedings. O

Ninth B-DAT
IEEE O
International O
Conference O
on, O

pages B-DAT
136–143. O
IEEE, O
2003. O
[34] O
Kang O
Wang O
and O
Qiang O

Ji. B-DAT
Real O
time O
eye O
gaze O

tracking B-DAT
with O
3d O
deformable O
eye-face O

model. B-DAT
In O
Proceedings O
of O
the O

IEEE B-DAT
Conference O
on O
Computer O
Vision O

and B-DAT
Pattern O
Recog- O
nition, O
pages O

1003–1011, B-DAT
2017 O

. B-DAT
[35] O
Oliver O
Williams, O
Andrew O
Blake O

, B-DAT
and O
Roberto O
Cipolla. O
Sparse O

and B-DAT
semi-supervised O
visual O
mapping O
with O

the B-DAT
sˆ O
3gp. O
In O
Computer O

Vision B-DAT
and O
Pattern O
Recognition, O
2006 O

IEEE B-DAT
Computer O
Society O
Conference O
on, O

volume B-DAT
1, O
pages O
230–237. O
IEEE, O
2006 O

. B-DAT
[36] O
William O
Hyde O
Wollaston O
et O

al. B-DAT
Xiii. O
on O
the O
apparent O

direction B-DAT
of O
eyes O
in O
a O

portrait. B-DAT
Philosophical O
Transactions O
of O
the O

Royal B-DAT
Society O
of O
London, O
114:247–256 O

, B-DAT
1824. O
[37] O
Erroll O
Wood O
and O
Andreas O

Bulling. B-DAT
Eyetab: O
Model-based O
gaze O
estimation O

on B-DAT
unmodi- O
fied O
tablet O
computers O

. B-DAT
In O
Proceedings O
of O
the O

Symposium B-DAT
on O
Eye O
Tracking O
Research O

and B-DAT
Applications, O
pages O
207–210. O
ACM, O
2014 O

. B-DAT
[38] O
Erroll O
Wood, O
Tadas O
Baltrusaitis O

, B-DAT
Xucong O
Zhang, O
Yusuke O
Sugano, O

Peter B-DAT
Robinson, O
and O
Andreas O
Bulling. O

Rendering B-DAT
of O
eyes O
for O
eye-shape O

registration B-DAT
and O
gaze O
estimation. O
In O

Proceedings B-DAT
of O
the O
IEEE O
International O

Conference B-DAT
on O
Computer O
Vision, O
pages O
3756 O

– B-DAT
3764, O
2015. O
[39] O
Erroll O
Wood, O
Tadas O
Baltrušaitis O

, B-DAT
Louis-Philippe O
Morency, O
Peter O
Robinson, O

and B-DAT
Andreas O
Bulling. O
A O
3d O

morphable B-DAT
eye O
region O
model O
for O

gaze B-DAT
estimation. O
In O
European O
Confer- O

ence B-DAT
on O
Computer O
Vision, O
pages O
297 O

–313. B-DAT
Springer, O
2016. O
[40] O
Erroll O
Wood, O
Tadas O
Baltrušaitis O

, B-DAT
Louis-Philippe O
Morency, O
Peter O
Robinson, O

and B-DAT
Andreas O
Bulling. O
Learning O
an O

appearance-based B-DAT
gaze O
estimator O
from O
one O

million B-DAT
synthesised O
images. O
In O
Proceedings O

of B-DAT
the O
Ninth O
Biennial O
ACM O

Symposium B-DAT
on O
Eye O
Tracking O
Re- O

search B-DAT
& O
Applications, O
pages O
131–138. O

ACM, B-DAT
2016. O
[41] O
Dong O
Hyun O
Yoo O
and O

Myung B-DAT
Jin O
Chung. O
A O
novel O

non-intrusive B-DAT
eye O
gaze O
estimation O
using O

cross-ratio B-DAT
under O
large O
head O
motion O

. B-DAT
Computer O
Vision O
and O
Image O

Understanding, B-DAT
98(1):25–51, O
2005. O
[42] O
Xucong O
Zhang, O
Yusuke O
Sugano O

, B-DAT
Mario O
Fritz, O
and O
Andreas O

Bulling. B-DAT
Appearance-based O
gaze O
estimation O
in O

the B-DAT
wild. O
In O
Proceedings O
of O

the B-DAT
IEEE O
Conference O
on O
Computer O

Vision B-DAT
and O
Pattern O
Recognition, O
pages O
4511 O

–4520, B-DAT
2015. O
[43] O
Xucong O
Zhang, O
Yusuke O
Sugano O

, B-DAT
Mario O
Fritz, O
and O
Andreas O

Bulling. B-DAT
It’s O
written O
all O
over O

your B-DAT
face: O
Full-face O
appearance-based O
gaze O

estimation. B-DAT
In O
Proc. O
IEEE O
International O

Conference B-DAT
on O
Computer O
Vision O
and O

Pattern B-DAT
Recognition O
Workshops O
(CVPRW), O
2017 O

state O
of O
the O
art O
on O
EYEDIAP B-DAT
dataset, O
further O
improved O
by O
4 O

of O
our O
solution O
on O
the O
EYEDIAP B-DAT
dataset O
[7] O
in O
a O
wide O

VGA O
videos O
from O
the O
publicly-available O
EYEDIAP B-DAT
dataset O
[7] O
to O
perform O
the O

FT-S O
scenario O
are O
provided O
by O
EYEDIAP B-DAT
dataset. O
MPIIGaze:. O
State-of-the-art O
full-face O
3D O

fine-tuned O
it O
with O
the O
filtered O
EYEDIAP B-DAT
subsets O
using O
our O
training O
parameters O

this O
pa- O
per O
used O
the O
EYEDIAP B-DAT
dataset O
made O
available O
by O
the O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
1 O

Recurrent B-DAT
CNN O
for O
3D O
Gaze O

Estimation B-DAT
using O
Appearance O
and O
Shape O

Cues B-DAT
Cristina O
Palmero1,2 O

crpalmec7@alumnes.ub.edu B-DAT
1 O
Dept. O
Mathematics O
and O
Informatics O

Universitat B-DAT
de O
Barcelona, O
Spain O

Javier B-DAT
Selva1 O
javier.selva.castello@est.fib.upc.edu O

2 B-DAT
Computer O
Vision O
Center O
Campus O

UAB, B-DAT
Bellaterra, O
Spain O
Mohammad O
Ali O
Bagheri3,4 O

mohammadali.bagheri@ucalgary.ca B-DAT
3 O
Dept. O
Electrical O
and O
Computer O

Eng. B-DAT
University O
of O
Calgary, O
Canada O

Sergio B-DAT
Escalera1,2 O
sergio@maia.ub.es O

4 B-DAT
Dept. O
Engineering O
University O
of O

Larestan, B-DAT
Iran O
Abstract O

Gaze B-DAT
behavior O
is O
an O
important O

non-verbal B-DAT
cue O
in O
social O
signal O

processing B-DAT
and O
human- O
computer O
interaction. O

In B-DAT
this O
paper, O
we O
tackle O

the B-DAT
problem O
of O
person- O
and O

head B-DAT
pose- O
independent O
3D O
gaze O

estimation B-DAT
from O
remote O
cameras, O
using O

a B-DAT
multi-modal O
recurrent O
convolutional O
neural O

network B-DAT
(CNN). O
We O
propose O
to O

combine B-DAT
face, O
eyes O
region, O
and O

face B-DAT
landmarks O
as O
individual O
streams O

in B-DAT
a O
CNN O
to O
estimate O

gaze B-DAT
in O
still O
images. O
Then, O

we B-DAT
exploit O
the O
dynamic O
nature O

of B-DAT
gaze O
by O
feeding O
the O

learned B-DAT
features O
of O
all O
the O

frames B-DAT
in O
a O
sequence O
to O

a B-DAT
many-to-one O
recurrent O
module O
that O

predicts B-DAT
the O
3D O
gaze O
vector O

of B-DAT
the O
last O
frame. O
Our O

multi-modal B-DAT
static O
solution O
is O
evaluated O

on B-DAT
a O
wide O
range O
of O

head B-DAT
poses O
and O
gaze O
directions, O

achieving B-DAT
a O
significant O
improvement O
of O
14 O

.6% B-DAT
over O
the O
state O
of O

the B-DAT
art O
on O
EYEDIAP O
dataset, O

further B-DAT
improved O
by O
4% O
when O

the B-DAT
temporal O
modality O
is O
included. O
1 O
Introduction O
Eyes O
and O
their O

movements B-DAT
are O
considered O
an O
important O

cue B-DAT
in O
non-verbal O
behavior O
analysis O

, B-DAT
being O
involved O
in O
many O

cognitive B-DAT
processes O
and O
reflecting O
our O

internal B-DAT
state O
[17]. O
More O
specifically, O

eye B-DAT
gaze O
behavior, O
as O
an O

indicator B-DAT
of O
human O
visual O
attention, O

has B-DAT
been O
widely O
studied O
to O

assess B-DAT
communication O
skills O
[28] O
and O

to B-DAT
identify O
possible O
behavioral O

disorders B-DAT
[9]. O
Therefore, O
gaze O
estimation O

has B-DAT
become O
an O
established O
line O

of B-DAT
research O
in O
computer O
vision, O

being B-DAT
a O
key O
feature O
in O

human-computer B-DAT
interaction O
(HCI) O
and O
usability O

research B-DAT
[12, O
20]. O
Recent O
gaze O
estimation O
research O
has O

focused B-DAT
on O
facilitating O
its O
use O

in B-DAT
general O
everyday O
applications O
under O

real-world B-DAT
conditions, O
using O
off-the-shelf O
remote O

RGB B-DAT
cameras O
and O
re- O
moving O

the B-DAT
need O
of O
personal O
calibration O

[26]. B-DAT
In O
this O
setting, O
appearance-based O

methods, B-DAT
which O
learn O
a O
mapping O

from B-DAT
images O
to O
gaze O
directions O

, B-DAT
are O
the O
preferred O

choice B-DAT
[25]. O
How- O
ever, O
they O

need B-DAT
large O
amounts O
of O
training O

data B-DAT
to O
be O
able O
to O

generalize B-DAT
well O
to O
in-the-wild O
situations, O

which B-DAT
are O
characterized O
by O
significant O

variability B-DAT
in O
head O
poses, O
face O

appearances B-DAT
and O
lighting O
conditions. O
In O

recent B-DAT
years, O
CNNs O
have O
been O

reported B-DAT
to O
outperform O
classical O
methods. O

However, B-DAT
most O
existing O
approaches O
have O

only B-DAT
been O
tested O
in O
restricted O

HCI B-DAT
tasks, O
c© O
2018. O
The O
copyright O
of O

this B-DAT
document O
resides O
with O
its O

authors. B-DAT
It O
may O
be O
distributed O

unchanged B-DAT
freely O
in O
print O
or O

electronic B-DAT
forms O

. B-DAT
ar O
X O

iv B-DAT
:1 O
80 O
5 O

. B-DAT
03 O
06 O

4v B-DAT
3 O

cs B-DAT
.C O
V O

1 B-DAT
7 O

Se B-DAT
p O

20 B-DAT
18 O

Citation B-DAT
Citation O
{Liversedge O
and O
Findlay} O
2000 O

Citation B-DAT
Citation O
{Rutter O
and O
Durkin} O
1987 O

Citation B-DAT
Citation O
{Guillon, O
Hadjikhani, O
Baduel, O

and B-DAT
Rog{é}} O
2014 O
Citation O
Citation O
{Jacob O
and O
Karn O

} B-DAT
2003 O
Citation O
Citation O
{Majaranta O
and O
Bulling O

} B-DAT
2014 O
Citation O
Citation O
{Palmero, O
van O
Dam O

, B-DAT
Escalera, O
Kelia, O
Lichtert, O
Noldus, O

Spink, B-DAT
and O
van O
Wieringen} O
2018 O
Citation O
Citation O
{Ono, O
Okabe, O
and O

Sato} B-DAT
2006 O

2 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
Method O
3D O
gaze O
direction O

Unrestricted B-DAT
gaze O
target O
Full O
face O

Eye B-DAT
region O
Facial O
landmarks O

Sequential B-DAT
information O
Zhang O
et O
al. O
(1) O
[42 O

] B-DAT
3 O
7 O
7 O
3 O
7 O
7 O
Krafka O
et O
al. O
[16 O

] B-DAT
7 O
7 O
3 O
3 O
7 O
7 O
Zhang O
et O
al. O
(2 O

) B-DAT
[43] O
3 O
7 O
3 O
7 O
7 O
7 O
Deng O
and O
Zhu O

[4] B-DAT
3 O
3 O
3 O
3 O

7 B-DAT
7 O
Ours O
3 O
3 O

3 B-DAT
3 O
3 O
3 O

Table B-DAT
1: O
Characteristics O
of O
recent O

related B-DAT
work O
on O
person- O
and O

head B-DAT
pose-independent O
appearance-based O
gaze O
estimation O

methods B-DAT
using O
CNNs. O
where O
users O
look O
at O
the O

screen B-DAT
or O
mobile O
phone, O
showing O

a B-DAT
low O
head O
pose O
variability O

. B-DAT
It O
is O
yet O
unclear O

how B-DAT
these O
methods O
would O
perform O

in B-DAT
a O
wider O
range O
of O

head B-DAT
poses. O
On O
a O
different O
note, O
until O

very B-DAT
recently, O
the O
majority O
of O

methods B-DAT
only O
used O
static O
eye O

region B-DAT
appearance O
as O
input. O
State-of-the-art O

approaches B-DAT
have O
demonstrated O
that O
using O

the B-DAT
face O
along O
with O
a O

higher B-DAT
resolution O
image O
of O
the O

eyes B-DAT
[16], O
or O
even O
just O

the B-DAT
face O
itself O
[43], O
increases O

performance. B-DAT
Indeed, O
the O
whole-face O
image O

encodes B-DAT
more O
information O
than O
eyes O

alone, B-DAT
such O
as O
illumination O
and O

head B-DAT
pose. O
Nevertheless, O
gaze O
behavior O

is B-DAT
not O
static. O
Eye O
and O

head B-DAT
movements O
allow O
us O
to O

direct B-DAT
our O
gaze O
to O
target O

locations B-DAT
of O
interest. O
It O
has O

been B-DAT
demonstrated O
that O
humans O
can O

better B-DAT
predict O
gaze O
when O
being O

shown B-DAT
image O
sequences O
of O
other O

people B-DAT
moving O
their O
eyes O
[1 O

]. B-DAT
However, O
it O
is O
still O

an B-DAT
open O
question O
whether O
this O

se- B-DAT
quential O
information O
can O
increase O

the B-DAT
performance O
of O
automatic O
methods. O
In O
this O
work, O
we O
show O

that B-DAT
the O
combination O
of O
multiple O

cues B-DAT
benefits O
the O
gaze O
estimation O

task. B-DAT
In O
particular, O
we O
use O

face, B-DAT
eye O
region O
and O
facial O

landmarks B-DAT
from O
still O
images. O
Facial O

landmarks B-DAT
model O
the O
global O
shape O

of B-DAT
the O
face O
and O
come O

at B-DAT
no O
cost, O
since O
face O

alignment B-DAT
is O
a O
common O
pre-processing O

step B-DAT
in O
many O
facial O
image O

analysis B-DAT
approaches. O
Furthermore, O
we O
present O

a B-DAT
subject-independent, O
free-head O
recurrent O
3D O

gaze B-DAT
regression O
network O
to O
leverage O

the B-DAT
temporal O
information O
of O
image O

sequences. B-DAT
The O
static O
streams O
of O

each B-DAT
frame O
are O
combined O
in O

a B-DAT
late-fusion O
fashion O
using O
a O

multi-stream B-DAT
CNN. O
Then, O
all O
feature O

vectors B-DAT
are O
input O
to O
a O

many-to-one B-DAT
recurrent O
module O
that O
predicts O

the B-DAT
gaze O
vector O
of O
the O

last B-DAT
sequence O
frame O

. B-DAT
In O
summary, O
our O
contributions O
are O

two-fold. B-DAT
First, O
we O
present O
a O

Recurrent-CNN B-DAT
net- O
work O
architecture O
that O

combines B-DAT
appearance, O
shape O
and O
temporal O

information B-DAT
for O
3D O
gaze O
estimation O

. B-DAT
Second, O
we O
test O
static O

and B-DAT
temporal O
versions O
of O
our O

solution B-DAT
on O
the O
EYEDIAP O

dataset B-DAT
[7] O
in O
a O
wide O

range B-DAT
of O
head O
poses O
and O

gaze B-DAT
directions, O
showing O
consistent O
perfor- O

mance B-DAT
improvements O
compared O
to O
related O

appearance-based B-DAT
methods. O
To O
the O
best O

of B-DAT
our O
knowledge, O
this O
is O

the B-DAT
first O
third-person, O
remote O
camera-based O

approach B-DAT
that O
uses O
tempo- O
ral O

information B-DAT
for O
this O
task. O
Table O
1 O
outlines O
our O
main O
method O
characteristics O

compared B-DAT
to O
related O
work. O
Models O

and B-DAT
code O
are O
publicly O
available O

at B-DAT
https://github.com/ O
crisie/RecurrentGaze O

. B-DAT
2 O
Related O
work O
Gaze O
estimation O

methods B-DAT
are O
typically O
categorized O
as O

model-based B-DAT
or O
appearance-based O
[5, O
10 O

, B-DAT
15]. O
Model-based O
approaches O
use O

a B-DAT
geometric O
model O
of O
the O

eye, B-DAT
usually O
requir- O
ing O
either O

high B-DAT
resolution O
images O
or O
a O

person-specific B-DAT
calibration O
stage O
to O
estimate O

personal B-DAT
eye O
parameters O
[22, O
33, O
34, O
37, O
41]. O
In O
contrast, O
appearance-based O

methods B-DAT
learn O
a O
di- O
rect O

mapping B-DAT
from O
intensity O
images O
or O

extracted B-DAT
eye O
features O
to O
gaze O

directions, B-DAT
thus O
being O

Citation B-DAT
Citation O
{Zhang, O
Sugano, O
Fritz, O

and B-DAT
Bulling} O
2015 O
Citation O
Citation O
{Krafka, O
Khosla, O
Kellnhofer O

, B-DAT
Kannan, O
Bhandarkar, O
Matusik, O
and O

Torralba} B-DAT
2016 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2017 O
Citation O
Citation O
{Deng O
and O
Zhu O

} B-DAT
2017 O
Citation O
Citation O
{Krafka, O
Khosla, O
Kellnhofer O

, B-DAT
Kannan, O
Bhandarkar, O
Matusik, O
and O

Torralba} B-DAT
2016 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2017 O
Citation O
Citation O
{Anderson, O
Risko, O
and O

Kingstone} B-DAT
2016 O

Citation B-DAT
Citation O
{Funesprotect O
unhbox O
voidb@x O

penalty B-DAT
@M O
{}Mora, O
Monay, O
and O
Odobez} O
2014 O

{} B-DAT
Citation O
Citation O
{Ferhat O
and O
Vilari{ñ}o O

} B-DAT
2016 O
Citation O
Citation O
{Hansen O
and O
Ji O

} B-DAT
2010 O
Citation O
Citation O
{Kar O
and O
Corcoran O

} B-DAT
2017 O
Citation O
Citation O
{Morimoto, O
Amir, O
and O

Flickner} B-DAT
2002 O

Citation B-DAT
Citation O
{Venkateswarlu O
etprotect O
unhbox O

voidb@x B-DAT
penalty O
@M O
{}al.} O
2003 O

Citation B-DAT
Citation O
{Wang O
and O
Ji} O
2017 O

Citation B-DAT
Citation O
{Wood O
and O
Bulling} O
2014 O

Citation B-DAT
Citation O
{Yoo O
and O
Chung} O
2005 O

https://github.com/crisie/RecurrentGaze B-DAT

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
3 O

potentially B-DAT
applicable O
to O
relatively O
low O

resolution B-DAT
images O
and O
mid-distance O
scenarios. O

Dif- B-DAT
ferent O
mapping O
functions O
have O

been B-DAT
explored, O
such O
as O
neural O

networks B-DAT
[2], O
adaptive O
linear O
regression O
( O

ALR) B-DAT
[19], O
local O
interpolation O
[32], O

gaussian B-DAT
processes O
[30, O
35], O
random O

forests B-DAT
[11, O
31], O
or O
k-nearest O

neighbors B-DAT
[40]. O
Main O
challenges O
of O

appearance-based B-DAT
methods O
for O
3D O
gaze O

estimation B-DAT
are O
head O
pose, O
illumination O

and B-DAT
subject O
invariance O
without O
user-specific O

calibration. B-DAT
To O
handle O
these O
issues, O

some B-DAT
works O
proposed O
compensation O

methods B-DAT
[18] O
and O
warping O
strategies O

that B-DAT
synthesize O
a O
canonical, O
frontal O

looking B-DAT
view O
of O
the O

face B-DAT
[6, O
13, O
21]. O
Hybrid O

approaches B-DAT
based O
on O
analysis-by-synthesis O
have O

also B-DAT
been O
evaluated O
[39]. O
Currently, O
data-driven O
methods O
are O
considered O

the B-DAT
state O
of O
the O
art O

for B-DAT
person- O
and O
head O
pose-independent O

appearance-based B-DAT
gaze O
estimation. O
Consequently, O
a O

number B-DAT
of O
gaze O
es- O
timation O

datasets B-DAT
have O
been O
introduced O
in O

recent B-DAT
years, O
either O
in O
controlled O

[29] B-DAT
or O
semi- O
controlled O
settings O

[8], B-DAT
in O
the O
wild O
[16 O

, B-DAT
42], O
or O
consisting O
of O

synthetic B-DAT
data O
[31, O
38, O
40]. O

Zhang B-DAT
et O
al. O
[42] O
showed O

that B-DAT
CNNs O
can O
outperform O
other O

mapping B-DAT
methods, O
using O
a O
multi- O

modal B-DAT
CNN O
to O
learn O
the O

mapping B-DAT
from O
3D O
head O
poses O

and B-DAT
eye O
images O
to O
3D O

gaze B-DAT
directions. O
Krafka O
et O

al. B-DAT
[16] O
proposed O
a O
multi-stream O

CNN B-DAT
for O
2D O
gaze O
estimation, O

using B-DAT
individual O
eye, O
whole-face O
image O

and B-DAT
the O
face O
grid O
as O

input. B-DAT
As O
this O
method O
was O

limited B-DAT
to O
2D O
screen O
mapping, O

Zhang B-DAT
et O
al. O
[43] O
later O

explored B-DAT
the O
potential O
of O
just O

using B-DAT
whole-face O
images O
as O
input O

to B-DAT
estimate O
3D O
gaze O
directions. O

Using B-DAT
a O
spatial O
weights O
CNN, O

they B-DAT
demonstrated O
their O
method O
to O

be B-DAT
more O
robust O
to O
facial O

appearance B-DAT
variation O
caused O
by O
head O

pose B-DAT
and O
illumina- O
tion O
than O

eye-only B-DAT
methods. O
While O
the O
method O

was B-DAT
evaluated O
in O
the O
wild, O

the B-DAT
subjects O
were O
only O
interacting O

with B-DAT
a O
mobile O
device, O
thus O

restricting B-DAT
the O
head O
pose O
range. O

Deng B-DAT
and O
Zhu O
[4] O
presented O

a B-DAT
two-stream O
CNN O
to O
disjointly O

model B-DAT
head O
pose O
from O
face O

images B-DAT
and O
eye- O
ball O
movement O

from B-DAT
eye O
region O
images. O
Both O

were B-DAT
then O
aggregated O
into O
3D O

gaze B-DAT
direction O
using O
a O
gaze O

transform B-DAT
layer. O
The O
decomposition O
was O

aimed B-DAT
to O
avoid O
head-correlation O
over- O

fitting B-DAT
of O
previous O
data-driven O
approaches. O

They B-DAT
evaluated O
their O
approach O
in O

the B-DAT
wild O
with O
a O
wider O

range B-DAT
of O
head O
poses, O
obtaining O

better B-DAT
performance O
than O
previous O
eye-based O

methods. B-DAT
However, O
they O
did O
not O

test B-DAT
it O
on O
public O
annotated O

benchmark B-DAT
datasets. O
In O
this O
paper, O
we O
propose O

a B-DAT
multi-stream O
recurrent O
CNN O
network O

for B-DAT
person- O
and O
head O
pose-independent O

3D B-DAT
gaze O
estimation O
for O
a O

mid-distance B-DAT
scenario. O
We O
evaluate O
it O

on B-DAT
a O
wider O
range O
of O

head B-DAT
poses O
and O
gaze O
directions O

than B-DAT
screen-targeted O
approaches. O
As O
opposed O

to B-DAT
previous O
methods, O
we O
also O

rely B-DAT
on O
temporal O
information O
inherent O

in B-DAT
sequential O
data O

. B-DAT
3 O
Methodology O

In B-DAT
this O
section, O
we O
present O

our B-DAT
approach O
for O
3D O
gaze O

regression B-DAT
based O
on O
appearance O
and O

shape B-DAT
cues O
for O
still O
images O

and B-DAT
image O
sequences. O
First, O
we O

introduce B-DAT
the O
data O
modalities O
and O

formulate B-DAT
the O
problem. O
Then, O
we O

detail B-DAT
the O
normalization O
procedure O
prior O

to B-DAT
the O
regression O
stage. O
Finally, O

we B-DAT
explain O
the O
global O
network O

topology B-DAT
as O
well O
as O
the O

implementation B-DAT
details. O
An O
overview O
of O

the B-DAT
system O
architecture O
is O
depicted O

in B-DAT
Figure O
1. O
3.1 O
Multi-modal O
gaze O
regression O

Let B-DAT
us O
represent O
gaze O
direction O

as B-DAT
a O
3D O
unit O
vector O

g B-DAT
= O
[gx,gy,gz]T O
∈R3 O
in O

the B-DAT
Camera O
Coor- O
dinate O
System O
( O

CCS), B-DAT
whose O
origin O
is O
the O

central B-DAT
point O
between O
eyeball O
centers. O

Assuming B-DAT
a O
calibrated O
camera, O
and O

a B-DAT
known O
head O
position O
and O

orientation, B-DAT
our O
goal O
is O
to O

estimate B-DAT
g O
from O
a O
sequence O

of B-DAT
images O
{I(i) O
| O

I B-DAT
∈ O
RW×H×3} O
as O
a O

regression B-DAT
problem. O
Citation O
Citation O
{Baluja O
and O
Pomerleau O

} B-DAT
1994 O
Citation O
Citation O
{Lu, O
Sugano, O
Okabe O

, B-DAT
and O
Sato} O
2011{} O
Citation O
Citation O
{Tan, O
Kriegman, O
and O

Ahuja} B-DAT
2002 O

Citation B-DAT
Citation O
{Sugano, O
Matsushita, O
and O

Sato} B-DAT
2013 O
Citation O
Citation O
{Williams, O
Blake, O
and O

Cipolla} B-DAT
2006 O

Citation B-DAT
Citation O
{Huang, O
Veeraraghavan, O
and O

Sabharwal} B-DAT
2017 O
Citation O
Citation O
{Sugano, O
Matsushita, O
and O

Sato} B-DAT
2014 O

Citation B-DAT
Citation O
{Wood, O
Baltru{²}aitis, O
Morency, O

Robinson, B-DAT
and O
Bulling} O
2016{} O
Citation O
Citation O
{Lu, O
Okabe, O
Sugano O

, B-DAT
and O
Sato} O
2011{} O
Citation O
Citation O
{Funes-Mora O
and O
Odobez O

} B-DAT
2016 O
Citation O
Citation O
{Jeni O
and O
Cohn O

} B-DAT
2016 O
Citation O
Citation O
{Mora O
and O
Odobez O

} B-DAT
2012 O
Citation O
Citation O
{Wood, O
Baltru{²}aitis, O
Morency O

, B-DAT
Robinson, O
and O
Bulling} O
2016{} O
Citation O
Citation O
{Smith, O
Yin, O
Feiner O

, B-DAT
and O
Nayar} O
2013 O
Citation O
Citation O
{Funesprotect O
unhbox O
voidb@x O

penalty B-DAT
@M O

Mora, B-DAT
Monay, O
and O
Odobez} O
2014{} O
Citation O
Citation O
{Krafka, O
Khosla, O
Kellnhofer O

, B-DAT
Kannan, O
Bhandarkar, O
Matusik, O
and O

Torralba} B-DAT
2016 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2015 O
Citation O
Citation O
{Sugano, O
Matsushita, O
and O

Sato} B-DAT
2014 O

Citation B-DAT
Citation O
{Wood, O
Baltrusaitis, O
Zhang, O

Sugano, B-DAT
Robinson, O
and O
Bulling} O
2015 O
Citation O
Citation O
{Wood, O
Baltru{²}aitis, O
Morency O

, B-DAT
Robinson, O
and O
Bulling} O
2016{} O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2015 O
Citation O
Citation O
{Krafka, O
Khosla, O
Kellnhofer O

, B-DAT
Kannan, O
Bhandarkar, O
Matusik, O
and O

Torralba} B-DAT
2016 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2017 O
Citation O
Citation O
{Deng O
and O
Zhu O

} B-DAT
2017 O

4 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
Conv O

C B-DAT
on O
ca O
t O
x O
y O
z O
x O
y O

z B-DAT
x O
y O
z O

Individual B-DAT
Fusion O
Temporal O
Individual O
Fusion O

Input B-DAT

Individual B-DAT
Fusion O
Normalization O

.Conv B-DAT

Conv B-DAT
. O
Conv O

Conv B-DAT
. O
FC O

FC B-DAT
FC O
RNN O
RNN O

RNN B-DAT
FC O
Ti O
m O
e O

Figure B-DAT
1: O
Overview O
of O
the O

proposed B-DAT
network. O
A O
multi-stream O
CNN O

jointly B-DAT
models O
full-face, O
eye O
region O

appearance B-DAT
and O
face O
landmarks O
from O

still B-DAT
images. O
The O
combined O
extracted O

fea- B-DAT
tures O
from O
each O
frame O

are B-DAT
fed O
into O
a O
recurrent O

module B-DAT
to O
predict O
last O
frame’s O

gaze B-DAT
direction. O
Gazing O
to O
a O
specific O
target O

is B-DAT
achieved O
by O
a O
combination O

of B-DAT
eye O
and O
head O
movements O

, B-DAT
which O
are O
highly O
coordinated. O

Consequently, B-DAT
the O
apparent O
direction O
of O

gaze B-DAT
is O
influenced O
not O
only O

by B-DAT
the O
location O
of O
the O

irises B-DAT
within O
the O
eyelid O
aperture, O

but B-DAT
also O
by O
the O
position O

and B-DAT
orientation O
of O
the O
face O

with B-DAT
respect O
to O
the O
camera. O

Known B-DAT
as O
the O
Wollaston O

effect B-DAT
[36], O
the O
exact O
same O

set B-DAT
of O
eyes O
may O
appear O

to B-DAT
be O
looking O
in O
different O

directions B-DAT
due O
to O
the O
surrounding O

facial B-DAT
cues. O
It O
is O
therefore O

reasonable B-DAT
to O
state O
that O
eye O

images B-DAT
are O
not O
sufficient O
to O

estimate B-DAT
gaze O
direction. O
Instead, O
whole-face O

images B-DAT
can O
encode O
head O
pose O

or B-DAT
illumination-specific O
information O
across O
larger O

areas B-DAT
than O
those O
available O
just O

in B-DAT
the O
eyes O
region O
[16, O
43 O

]. B-DAT
The O
drawback O
of O
appearance-only O
methods O

is B-DAT
that O
global O
structure O
information O

is B-DAT
not O
explicitly O
considered. O
In O

that B-DAT
sense, O
facial O
landmarks O
can O

be B-DAT
used O
as O
global O
shape O

cues B-DAT
to O
en- O
code O
spatial O

relationships B-DAT
and O
geometric O
constraints. O
Current O

state-of-the-art B-DAT
face O
alignment O
approaches O
are O

robust B-DAT
enough O
to O
handle O
large O

appearance B-DAT
variability, O
extreme O
head O
poses O

and B-DAT
occlusions, O
being O
especially O
useful O

when B-DAT
the O
dataset O
used O
for O

gaze B-DAT
estimation O
does O
not O
contain O

such B-DAT
variability. O
Facial O
landmarks O
are O

mainly B-DAT
correlated O
with O
head O
orientation O

, B-DAT
eye O
position, O
eyelid O
openness, O

and B-DAT
eyebrow O
movement, O
which O
are O

valuable B-DAT
features O
for O
our O
task. O
Therefore, O
in O
our O
approach O
we O

jointly B-DAT
model O
appearance O
and O
shape O

cues B-DAT
(see O
Figure O
1). O
The O

former B-DAT
is O
represented O
by O
a O

whole-face B-DAT
image O
IF O
, O
along O

with B-DAT
a O
higher O
resolution O
image O

of B-DAT
the O
eyes O
IE O
to O

identify B-DAT
subtle O
changes. O
Due O
to O

dealing B-DAT
with O
wide O
head O
pose O

ranges, B-DAT
some O
eye O
images O
may O

not B-DAT
depict O
the O
whole O
eye O

, B-DAT
containing O
mostly O
background O
or O

other B-DAT
surrounding O
facial O
parts O
instead. O

For B-DAT
that O
reason, O
and O
contrary O

to B-DAT
previous O
approaches O
that O
only O

use B-DAT
one O
eye O
image O
[31, O
42 O

], B-DAT
we O
use O
a O
single O

image B-DAT
composed O
of O
two O
patches O

of B-DAT
centered O
left O
and O
right O

eyes. B-DAT
Finally, O
the O
shape O
cue O

is B-DAT
represented O
by O
3D O
face O

landmarks B-DAT
obtained O
from O
a O
68-landmark O

model, B-DAT
denoted O
by O

L B-DAT
= O
{(lx, O
ly, O

) B-DAT

| B-DAT
∀c O
∈ O
[1, O
...,68 O

]}. B-DAT
In O
this O
work O
we O
also O

consider B-DAT
the O
dynamic O
component O
of O

gaze. B-DAT
We O
leverage O
the O
se O

- B-DAT
quential O
information O
of O
eye O

and B-DAT
head O
movements O
such O
that, O

given B-DAT
appearance O
and O
shape O
features O

of B-DAT
consecutive O
frames, O
it O
is O

possible B-DAT
to O
better O
predict O
the O

gaze B-DAT
direction O
of O
the O
cur- O

rent B-DAT
frame. O
Therefore, O
the O
3D O

gaze B-DAT
estimation O
task O
for O
a O
1 O

-frame B-DAT
sequence O
is O
formulated O
Citation O
Citation O
{Wollaston O
etprotect O
unhbox O

voidb@x B-DAT
penalty O
@M O

al.} B-DAT
1824 O
Citation O
Citation O
{Krafka, O
Khosla, O
Kellnhofer O

, B-DAT
Kannan, O
Bhandarkar, O
Matusik, O
and O

Torralba} B-DAT
2016 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2017 O
Citation O
Citation O
{Sugano, O
Matsushita, O
and O

Sato} B-DAT
2014 O

Citation B-DAT
Citation O
{Zhang, O
Sugano, O
Fritz, O

and B-DAT
Bulling} O
2015 O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
5 O

as B-DAT
g(i) O
= O
f O
( O
{IF O
(i)},{IE O
(i)},{L(i O

)} B-DAT
) O
, O
where O
i O
denotes O

the B-DAT
i-th O
frame, O
and O
f O

is B-DAT
the O
regression O

function. B-DAT
3.2 O
Data O
normalization O
Prior O
to O

gaze B-DAT
regression, O
a O
normalization O
step O

in B-DAT
the O
3D O
space O
and O

the B-DAT
2D O
image, O
similar O
to O

[31], B-DAT
is O
carried O
out. O
This O

is B-DAT
performed O
to O
reduce O
the O

appearance B-DAT
variability O
and O
to O
allow O

the B-DAT
gaze O
estimation O
model O
to O

be B-DAT
applied O
regardless O
of O
the O

original B-DAT
camera O
configuration O

. B-DAT
Let O
H O
∈ O
R3x3 O
be O

the B-DAT
head O
rotation O
matrix, O
and O

p B-DAT
= O
[px, O
py, O
pz]T O

∈ B-DAT
R3 O
the O
reference O
face O

location B-DAT
with O
respect O
to O
the O

original B-DAT
CCS. O
The O
goal O
is O

to B-DAT
find O
the O
conversion O
matrix O

M B-DAT
= O
SR O
such O
that O

(a) B-DAT
the O
X-axes O
of O
the O

virtual B-DAT
camera O
and O
the O
head O

become B-DAT
parallel O
using O
the O
rotation O

matrix B-DAT
R, O
and O
(b) O
the O

virtual B-DAT
camera O
looks O
at O
the O

reference B-DAT
location O
from O
a O
fixed O

distance B-DAT
dn O
using O
the O
Z-direction O

scaling B-DAT
matrix O
S O
= O
diag(1,1,dn/‖p O

‖). B-DAT
R O
is O
computed O
as O

a B-DAT
= O
p̂×HT O
e1, O

b B-DAT
= O
â× O
p̂, O

R B-DAT
= O
[â, O
b̂, O
p̂]T O
, O
where O
e1 O
denotes O
the O
first O

orthonormal B-DAT
basis O
and O

〈 B-DAT
·̂ O
〉 O
is O
the O

unit B-DAT
vector O

. B-DAT
This O
normalization O
translates O
into O
the O

image B-DAT
space O
as O
a O
cropped O

image B-DAT
patch O
of O
size O
Wn×Hn O

centered B-DAT
at O
p O
where O
head O

roll B-DAT
rotation O
has O
been O
removed O

. B-DAT
This O
is O
done O
by O

applying B-DAT
a O
perspective O
warping O
to O

the B-DAT
input O
image O
I O
using O

the B-DAT
transformation O
matrix O
W O
= O

CoMCn−1, B-DAT
where O
Co O
and O
Cn O

are B-DAT
the O
original O
and O
virtual O

camera B-DAT
matrices, O
respectively. O
The O
3D O
gaze O
vector O
is O

also B-DAT
normalized O
as O
gn O
=Rg O

. B-DAT
After O
image O
normalization, O
the O

line B-DAT
of O
sight O
can O
be O

represented B-DAT
in O
a O
2D O
space. O

Therefore, B-DAT
gn O
is O
further O
transformed O

to B-DAT
spherical O
coor- O
dinates O
(θ O
, O

φ) B-DAT
assuming O
unit O
length, O
where O

θ B-DAT
and O
φ O
denote O
the O

horizontal B-DAT
and O
vertical O
direc- O
tion O

angles, B-DAT
respectively. O
This O
2D O
angle O

representation, B-DAT
delimited O
in O
the O

range B-DAT
[−π/2,π/2], O
is O
computed O
as O

θ B-DAT
= O
arctan(gx/gz) O
and O

φ B-DAT
= O
arcsin(−gy), O
such O
that O
(0, O

0) B-DAT
represents O
looking O
straight O
ahead O

to B-DAT
the O
CCS O
origin. O
3.3 O
Recurrent O
Convolutional O
Neural O
Network O

We B-DAT
propose O
a O
Recurrent O
CNN O

Regression B-DAT
Network O
for O
3D O
gaze O

estimation. B-DAT
The O
network O
is O
divided O

in B-DAT
3 O
modules: O
(1) O
Individual O

, B-DAT
(2) O
Fusion, O
and O
(3) O

Temporal. B-DAT
First, O
the O
Individual O
module O
learns O

features B-DAT
from O
each O
appearance O
cue O

separately. B-DAT
It O
consists O
of O
a O

two-stream B-DAT
CNN, O
one O
devoted O
to O

the B-DAT
normalized O
face O
image O
stream O

and B-DAT
the O
other O
to O
the O

joint B-DAT
normalized O
eyes O
image. O
Next O

, B-DAT
the O
Fusion O
module O
combines O

the B-DAT
extracted O
features O
of O
each O

appearance B-DAT
stream O
in O
a O
single O

vector B-DAT
along O
with O
the O
normalized O

landmark B-DAT
coordinates. O
Then, O
it O
learns O

a B-DAT
joint O
representation O
between O
modalities O

in B-DAT
a O
late-fusion O
fashion. O
Both O

Individual B-DAT
and O
Fusion O
modules, O
further O

referred B-DAT
to O
as O
Static O
model, O

are B-DAT
applied O
to O
each O
frame O

of B-DAT
the O
sequence. O
Finally, O
the O

resulting B-DAT
feature O
vectors O
of O
each O

frame B-DAT
are O
input O
to O
the O

Temporal B-DAT
module O
based O
on O
a O

many-to-one B-DAT
recurrent O
network. O
This O
module O

leverages B-DAT
sequential O
information O
to O
predict O

the B-DAT
normalized O
2D O
gaze O
angles O

of B-DAT
the O
last O
frame O
of O

the B-DAT
sequence O
using O
a O
linear O

regression B-DAT
layer O
added O
on O
top O

of B-DAT
it. O
3.4 O
Implementation O
details O
3.4.1 O
Network O

details B-DAT

Each B-DAT
stream O
of O
the O
Individual O

module B-DAT
is O
based O
on O
the O

VGG-16 B-DAT
deep O
network O
[27], O
consisting O

of B-DAT
13 O
convolutional O
layers, O
5 O

max B-DAT
pooling O
layers, O
and O
1 O

fully B-DAT
connected O
(FC) O
layer O
with O

Rec- B-DAT
tified O
Linear O
Unit O
(ReLU) O

activations. B-DAT
The O
full-face O
stream O
follows O

the B-DAT
same O
configuration O
Citation O
Citation O
{Sugano, O
Matsushita, O
and O

Sato} B-DAT
2014 O

Citation B-DAT
Citation O
{Parkhi, O
Vedaldi, O
and O

Zisserman} B-DAT
2015 O

6 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
as O
the O
base O
network, O
having O

an B-DAT
input O
of O
224×224 O
pixels O

and B-DAT
a O
4096D O
FC O
layer O

. B-DAT
In O
contrast, O
the O
input O

joint B-DAT
eye O
image O
is O
smaller, O

with B-DAT
a O
final O
size O
of O
120 O

×48 B-DAT
pixels, O
so O
the O
number O

of B-DAT
pa- O
rameters O
is O
decreased O

proportionally. B-DAT
In O
this O
case, O
its O

last B-DAT
FC O
layer O
produces O
a O

1536D B-DAT
vector. O
A O
204D O
landmark O

coordinates B-DAT
vector O
is O
concatenated O
to O

the B-DAT
output O
of O
the O
FC O

layer B-DAT
of O
each O
stream, O
resulting O

in B-DAT
a O
5836D O
feature O
vector. O

Consequently, B-DAT
the O
Fusion O
module O
consists O

of B-DAT
2 O
5836D O
FC O
layers O

with B-DAT
ReLU O
activations O
and O
2 O

dropout B-DAT
layers O
between O
FCs O
as O

regularization. B-DAT
Finally, O
to O
model O
the O

temporal B-DAT
dependencies, O
we O
use O
a O

single B-DAT
GRU O
layer O
with O
128 O

units. B-DAT
The O
network O
is O
trained O
in O

a B-DAT
stage-wise O
fashion. O
First, O
we O

train B-DAT
the O
Static O
model O
and O

the B-DAT
final O
regression O
layer O
end-to-end O

on B-DAT
each O
individual O
frame O
of O

the B-DAT
training O
data. O
The O
convolutional O

blocks B-DAT
are O
pre-trained O
with O
the O

VGG-Face B-DAT
dataset O
[27], O
whereas O
the O

FCs B-DAT
are O
trained O
from O
scratch O

. B-DAT
Second, O
the O
training O
data O

is B-DAT
re-arranged O
by O
means O
of O

a B-DAT
sliding O
window O
with O
stride O
1 O
to O
build O
input O
sequences. O
Each O

sequence B-DAT
is O
composed O
of O
s O

= B-DAT
4 O
consecutive O
frames, O
whose O

gaze B-DAT
direction O
target O
is O
the O

gaze B-DAT
direction O
of O
the O
last O

frame B-DAT
of O
the O
sequence( O
{I(i−s+1 O

), B-DAT
. O
. O
. O
,I(i)}, O

g(i) B-DAT
) O
. O
Using O
this O
re-arranged O

training B-DAT
data, O
we O
extract O
features O

of B-DAT
each O

frame B-DAT
of O
the O
sequence O
from O

a B-DAT
frozen O
Individual O
module, O
fine-tune O

the B-DAT
Fusion O
layers, O
and O
train O

both, B-DAT
the O
Temporal O
module O
and O

a B-DAT
new O
final O
regression O
layer O

from B-DAT
scratch. O
This O
way, O
the O

network B-DAT
can O
exploit O
the O
temporal O

information B-DAT
to O
further O
refine O
the O

fusion B-DAT
weights. O
We O
trained O
the O
model O
using O

ADAM B-DAT
optimizer O
with O
an O
initial O

learning B-DAT
rate O
of O
0.0001, O
dropout O

of B-DAT
0.3, O
and O
batch O
size O

of B-DAT
64 O
frames. O
The O
number O

of B-DAT
epochs O
was O
experimentally O
set O

to B-DAT
21 O
for O
the O
first O

training B-DAT
stage O
and O
10 O
for O

the B-DAT
second. O
We O
use O
the O

average B-DAT
Euclidean O
distance O
between O
the O

predicted B-DAT
and O
ground-truth O
3D O
gaze O

vectors B-DAT
as O
loss O
function O

. B-DAT
3.4.2 O
Input O
pre-processing O

For B-DAT
this O
work O
we O
use O

head B-DAT
pose O
and O
eye O
locations O

in B-DAT
the O
3D O
scene O
provided O

by B-DAT
the O
dataset. O
The O
3D O

landmarks B-DAT
are O
extracted O
using O
the O

state-of-the-art B-DAT
method O
of O
Bulat O
and O

Tzimiropou- B-DAT
los O
[3], O
which O
is O

based B-DAT
on O
stacked O
hourglass O

networks B-DAT
[24]. O
During O
training, O
the O
original O
image O

is B-DAT
pre-processed O
to O
get O
the O

two B-DAT
normalized O
input O
images. O
The O

normalized B-DAT
whole-face O
patch O
is O
centered O

0.1 B-DAT
meters O
ahead O
of O
the O

head B-DAT
center O
in O
the O
head O

coordinate B-DAT
system, O
and O
Cn O
is O

defined B-DAT
such O
that O
the O
image O

has B-DAT
size O
of O
250× O
250 O

pixels. B-DAT
The O
difference O
between O
this O

size B-DAT
and O
the O
final O
input O

size B-DAT
allows O
us O
to O
perform O

random B-DAT
cropping O
and O
zooming O
to O

augment B-DAT
the O
data O
(explained O
in O

Section B-DAT
4.1). O
Similarly, O
each O
normalized O

eye B-DAT
patch O
is O
centered O
in O

their B-DAT
respective O
eye O
center O
locations O

. B-DAT
In O
this O
case, O
the O

virtual B-DAT
camera O
matrix O
is O
defined O

so B-DAT
that O
the O
image O
is O

cropped B-DAT
to O
70×58, O
while O
in O

practice B-DAT
the O
final O
patches O
have O

size B-DAT
of O
60×48. O
Landmarks O
are O

normalized B-DAT
using O
the O
same O
procedure O

and B-DAT
further O
pre-processed O
with O
mean O

subtraction B-DAT
and O
min-max O
normalization O
per O

axis. B-DAT
Finally, O
we O
divide O
them O

by B-DAT
a O
scaling O
factor O
w O

such B-DAT
that O
all O
coordinates O
are O

in B-DAT
the O
range O
[0,w]. O
This O

way, B-DAT
all O
concatenated O
feature O
values O

are B-DAT
in O
a O
similar O
range. O

After B-DAT
inference, O
the O
predicted O
normalized O

2D B-DAT
angles O
are O
de-normalized O
back O

to B-DAT
the O
original O
3D O
space. O
4 O
Experiments O
In O
this O
section O

, B-DAT
we O
evaluate O
the O
cross-subject O

3D B-DAT
gaze O
estimation O
task O
on O

a B-DAT
wide O
range O
of O
head O

poses B-DAT
and O
gaze O
directions. O
Furthermore, O

we B-DAT
validate O
the O
effectiveness O
of O

the B-DAT
proposed O
architecture O
comparing O
both O

static B-DAT
and O
temporal O
approaches. O
We O

report B-DAT
the O
error O
in O
terms O

of B-DAT
mean O
angular O
error O
between O

predicted B-DAT
and O
ground-truth O
3D O
gaze O

vectors. B-DAT
Note O
that O
due O
to O

the B-DAT
requirements O
of O
the O
temporal O

model B-DAT
not O
all O
the O
frames O

obtain B-DAT
a O
prediction. O
Therefore, O
for O

a B-DAT
Citation O
Citation O
{Parkhi, O
Vedaldi, O
and O

Zisserman} B-DAT
2015 O

Citation B-DAT
Citation O
{Bulat O
and O
Tzimiropoulos} O
2017 O

Citation B-DAT
Citation O
{Newell, O
Yang, O
and O

Deng} B-DAT
2016 O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
7 O

60 B-DAT
30 O
0 O
30 O
60 O
60 O

30 B-DAT
0 O

30 B-DAT
60 O

100 B-DAT
101 O

102 B-DAT
60 O
30 O
0 O
30 O
60 O

60 B-DAT
30 O

0 B-DAT
30 O

60 B-DAT
100 O

101 B-DAT
102 O

103 B-DAT
60 O
30 O
0 O
30 O
60 O

60 B-DAT
30 O

0 B-DAT
30 O

60 B-DAT
100 O

101 B-DAT
102 O

60 B-DAT
30 O
0 O
30 O
60 O
60 O

30 B-DAT
0 O

30 B-DAT
60 O

100 B-DAT
101 O

102 B-DAT
103 O

a) B-DAT
g O
(FT O
) O
(b) O

h B-DAT
(FT O
) O
(c) O
g O
( O

CS) B-DAT
(d) O
h O
(CS) O
Figure O
2: O
Ground-truth O
eye O
gaze O

g B-DAT
and O
head O
orientation O
h O

distribution B-DAT
on O
the O
filtered O
EYE O

- B-DAT
DIAP O
dataset O
for O
CS O

and B-DAT
FT O
settings, O
in O
terms O

of B-DAT
x- O
and O
y- O
angles. O
fair O
comparison, O
the O
reported O
results O

for B-DAT
static O
models O
disregard O
such O

frames B-DAT
when O
temporal O
models O
are O

included B-DAT
in O
the O
comparison O

. B-DAT
4.1 O
Training O
data O

There B-DAT
are O
few O
publicly O
available O

datasets B-DAT
devoted O
to O
3D O
gaze O

estimation B-DAT
and O
most O
of O
them O

focus B-DAT
on O
HCI O
with O
a O

limited B-DAT
range O
of O
head O
pose O

and B-DAT
gaze O
directions. O
Therefore, O
we O

use B-DAT
VGA O
videos O
from O
the O

publicly-available B-DAT
EYEDIAP O
dataset O
[7] O
to O

perform B-DAT
the O
experimental O
evaluation, O
as O

it B-DAT
is O
currently O
the O
only O

one B-DAT
containing O
video O
sequences O
with O

a B-DAT
wide O
range O
of O
head O

poses B-DAT
and O
showing O
the O
full O

face. B-DAT
This O
dataset O
consists O
of O
3 O

-minute B-DAT
videos O
of O
16 O
subjects O

looking B-DAT
at O
two O
types O
of O

targets: B-DAT
continuous O
screen O
targets O
on O

a B-DAT
fixed O
monitor O
(CS), O
and O

floating B-DAT
physical O
targets O
(FT O
). O

The B-DAT
videos O
are O
further O
divided O

into B-DAT
static O
(S) O
and O
moving O
( O

M) B-DAT
head O
pose O
for O
each O

of B-DAT
the O
subjects. O
Subjects O
12-16 O

were B-DAT
recorded O
with O
2 O
different O

lighting B-DAT
conditions. O
For O
evaluation, O
we O
filtered O
out O

those B-DAT
frames O
that O
fulfilled O
at O

least B-DAT
one O
of O
the O
following O

conditions: B-DAT
(1) O
face O
or O
landmarks O

not B-DAT
detected; O
(2) O
subject O
not O

looking B-DAT
at O
the O
target; O
(3 O

) B-DAT
3D O
head O
pose, O
eyes O

or B-DAT
target O
location O
not O
properly O

recovered; B-DAT
and O
(4) O
eyeball O
rotations O

violating B-DAT
physical O

constraints B-DAT
(|θ O
| O
≤ O
40 O

◦, B-DAT
|φ O
| O
≤ O
30 O

◦) B-DAT
[23]. O
Note O
that O
we O

purposely B-DAT
do O
not O
filter O
eye O

blinking B-DAT
moments O
to O
learn O
their O

dynamics B-DAT
with O
the O
temporal O
model, O

which B-DAT
may O
produce O
some O
outliers O

with B-DAT
a O
higher O
prediction O
error O

due B-DAT
to O
a O
less O
accurate O

ground B-DAT
truth. O
Figure O
2 O
shows O

the B-DAT
distribution O
of O
gaze O
directions O

and B-DAT
head O
poses O
for O
both O

filtered B-DAT
CS O
and O
FT O
cases. O
We O
applied O
data O
augmentation O
to O

the B-DAT
training O
set O
with O
the O

following B-DAT
random O
transforma- O
tions: O
horizontal O

flip, B-DAT
shifts O
of O
up O
to O

5 B-DAT
pixels, O
zoom O
of O
up O

to B-DAT
2%, O
brightness O
changes O
by O

a B-DAT
factor O
in O
the O
range O

[0.4,1.75], B-DAT
and O
additive O
Gaussian O
noise O

with B-DAT
σ2 O
= O
0.03 O

. B-DAT
4.2 O
Evaluation O
of O
static O
modalities O

First, B-DAT
we O
evaluate O
the O
contribution O

of B-DAT
each O
static O
modality O
on O

the B-DAT
FT O
scenario. O
We O
divided O

the B-DAT
16 O
participants O
into O
4 O

groups, B-DAT
such O
that O
appearance O
variability O

was B-DAT
maximized O
while O
maintaining O
a O

similar B-DAT
number O
of O
training O
samples O

per B-DAT
group. O
Each O
static O
model O

was B-DAT
trained O
end-to-end O
performing O
4-fold O

cross-validation B-DAT
using O
different O
combinations O
of O

input B-DAT
modal- O
ities. O
Since O
the O

number B-DAT
of O
fusion O
units O
depends O

on B-DAT
the O
number O
of O
input O

modalities, B-DAT
we O
also O
compare O
different O

fusion B-DAT
layer O
sizes. O
The O
effect O

of B-DAT
data O
normalization O
is O
also O

evaluated B-DAT
by O
training O
a O
not-normalized O

face B-DAT
model O
where O
the O
input O

image B-DAT
is O
the O
face O
bounding O

box B-DAT
with O
square O
size O
the O

maximum B-DAT
distance O
between O
2D O
landmarks. O
Citation O
Citation O
{Funesprotect O
unhbox O
voidb@x O

penalty B-DAT
@M O

Mora, B-DAT
Monay, O
and O
Odobez} O
2014{} O
Citation O
Citation O
{MSC O

8 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
0 O
1 O
2 O
3 O
4 O

5 B-DAT
6 O
7 O
8 O
9 O

10 B-DAT
11 O
An O
gl O

e B-DAT
er O

ro B-DAT
r O
( O
de O
gr O

ee B-DAT
s) O
6.9 O
6.43 O
5.58 O
5.71 O
5.59 O

5.55 B-DAT
5.52 O

OF-4096 B-DAT
NE-1536 O
NF-4096 O
NF-5632 O
NFL-4300 O

NFE-5632 B-DAT
NFEL-5836 O
Figure O
3: O
Performance O
evaluation O
of O

the B-DAT
Static O
network O
using O
different O

input B-DAT
modali- O
ties O
(O O

- B-DAT
Not O
normalized, O
N O

- B-DAT
Normalized, O
F O
- O
Face O

, B-DAT
E O
- O
Eyes, O

L B-DAT
- O
3D O
Landmarks) O
and O

size B-DAT
of O
fusion O
layers O
on O

the B-DAT
FT O
scenario. O
Floating O
Target O
Screen O
Target O
0 O

1 B-DAT
2 O
3 O
4 O
5 O

6 B-DAT
7 O
8 O
9 O

10 B-DAT
11 O
An O
gl O

e B-DAT
er O

ro B-DAT
r O
( O
de O
gr O

ee B-DAT
s) O
6.36 O
5.43 O
5.19 O
4.2 O
3.38 O

3.4 B-DAT

MPIIGaze B-DAT
Static O
Temporal O
Figure O
4: O
Performance O
comparison O
among O

MPIIGaze B-DAT
method O
[42] O
and O
our O

Static B-DAT
and O
Temporal O
versions O
of O

the B-DAT
proposed O
network O
for O
FT O

and B-DAT
CS O
scenarios O

. B-DAT
As O
shown O
in O
Figure O
3 O

, B-DAT
all O
models O
that O
take O

normalized B-DAT
full-face O
information O
as O
input O

achieve B-DAT
better O
performance O
than O
the O

eyes-only B-DAT
model. O
More O
specifically, O
the O

combination B-DAT
of O
face, O
eyes O
and O

landmarks B-DAT
outperforms O
all O
the O
other O

combinations B-DAT
by O
a O
small O
but O

significant B-DAT
margin O
(paired O
Wilcoxon O
test, O

p B-DAT
< O
0.0001). O
The O
standard O

deviation B-DAT
of O
the O
best-performing O
model O

is B-DAT
reduced O
compared O
to O
the O

face B-DAT
and O
eyes O
model, O
suggesting O

a B-DAT
regularizing O
effect O
due O
to O

the B-DAT
addition O
of O
landmarks. O
The O

not-normalized B-DAT
face-only O
model O
shows O
the O

largest B-DAT
error, O
proving O
the O
impact O

of B-DAT
normalization O
to O
reduce O
the O

appearance B-DAT
variability. O
Furthermore, O
our O
results O

indicate B-DAT
that O
the O
increase O
of O

fusion B-DAT
units O
is O
not O
correlated O

with B-DAT
a O
better O
performance. O
4.3 O
Static O
gaze O
regression: O
comparison O

with B-DAT
existing O
methods O

We B-DAT
compare O
our O
best-performing O
static O

model B-DAT
with O
three O
baselines. O
Head: O

Treating B-DAT
the O
head O
pose O
directly O

as B-DAT
gaze O
direction. O
PR-ALR: O
Method O

that B-DAT
relies O
on O
RGB-D O
data O

to B-DAT
rectify O
the O
eye O
images O

viewpoint B-DAT
into O
a O
canonical O
head O

pose B-DAT
using O
a O
3DMM. O
It O

then B-DAT
learns O
an O
RGB O
gaze O

appearance B-DAT
model O
using O
ALR O
[21]. O

Predicted B-DAT
3D O
vectors O
for O
FT-S O

scenario B-DAT
are O
provided O
by O
EYEDIAP O

dataset. B-DAT
MPIIGaze:. O
State-of-the-art O
full-face O
3D O

gaze B-DAT
estimation O
method O
[42]. O
They O

use B-DAT
an O
Alexnet-based O
CNN O
model O

with B-DAT
spatial O
weights O
to O
enhance O

information B-DAT
in O
different O
facial O
regions. O

We B-DAT
fine-tuned O
it O
with O
the O

filtered B-DAT
EYEDIAP O
subsets O
using O
our O

training B-DAT
parameters O
and O
normalization O
procedure. O
In O
addition O
to O
the O
aforementioned O

FT-based B-DAT
evaluation O
setup, O
we O
also O

evaluate B-DAT
our O
method O
on O
the O

CS B-DAT
scenario. O
In O
this O
case O

there B-DAT
are O
only O
14 O
participants O

available, B-DAT
so O
we O
divided O
them O

in B-DAT
5 O
groups O
and O
performed O

5-fold B-DAT
cross-validation. O
In O
Figure O
4 O

we B-DAT
compare O
our O
method O
to O

MPIIGaze, B-DAT
achieving O
a O
statistically O
significant O

improvement B-DAT
of O
14.6% O
and O
19.5 O

% B-DAT
on O
FT O
and O
CS O

scenarios, B-DAT
respectively O
(paired O
Wilcoxon O
test, O

p B-DAT
< O
0.0001). O
We O
can O

observe B-DAT
that O
a O
re- O
stricted O

gaze B-DAT
target O
benefits O
the O
performance O

of B-DAT
all O
methods, O
compared O
to O

a B-DAT
more O
challenging O
unrestricted O
setting O

with B-DAT
a O
wider O
range O
of O

head B-DAT
poses O
and O
gaze O
directions. O
Table O
2 O
provides O
a O
detailed O

comparison B-DAT
on O
every O
participant, O
performing O

leave-one-out B-DAT
cross-validation O
on O
the O
FT O

scenario B-DAT
for O
static O
and O
moving O

head B-DAT
separately. O
Results O
show O
that O

, B-DAT
as O
expected, O
facial O
appearance O

and B-DAT
head O
pose O
have O
a O

noticeable B-DAT
impact O
on O
gaze O
accuracy, O

with B-DAT
average O
error O
differences O
of O

up B-DAT
to O
7.7◦ O
among O
participants. O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2015 O
Citation O
Citation O
{Mora O
and O
Odobez O

} B-DAT
2012 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2015 O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
9 O

Method B-DAT
1 O
2 O
3 O
4 O
5 O
6 O
7 O
8 O
9 O
10 O

11 B-DAT
12 O
13 O
14 O
15 O

16 B-DAT
Avg. O
Head O
23.5 O
22.1 O

20.3 B-DAT
23.6 O
23.2 O
23.2 O
23.6 O

21.2 B-DAT
26.7 O
23.6 O
23.1 O
24.4 O

23.3 B-DAT
24.0 O
24.5 O
22.8 O
23.3 O

PR-ALR B-DAT
12.3 O
12.0 O
12.4 O
11.3 O

15.5 B-DAT
12.9 O
17.9 O
11.8 O
17.3 O

13.4 B-DAT
13.4 O
14.3 O
15.2 O
13.6 O

14.4 B-DAT
14.6 O
13.9 O
MPIIGaze O
5.3 O

5.1 B-DAT
5.7 O
4.7 O
7.3 O
15.1 O

10.8 B-DAT
5.7 O
9.9 O
7.1 O
5.0 O

5.7 B-DAT
7.4 O
3.8 O
4.8 O
5.5 O

6.8 B-DAT
Static O
3.9 O
4.1 O
4.2 O

3.9 B-DAT
6.0 O
6.4 O
7.2 O
3.6 O

7.1 B-DAT
5.0 O
5.7 O
6.7 O
3.9 O

4.7 B-DAT
5.1 O
4.2 O
5.1 O
Temporal O

4.0 B-DAT
4.9 O
4.3 O
4.1 O
6.1 O

6.5 B-DAT
6.6 O
3.9 O
7.8 O
6.1 O

4.7 B-DAT
5.6 O
4.7 O
3.5 O
5.9 O

4.6 B-DAT
5.2 O
Head O
19.3 O
14.2 O

16.4 B-DAT
19.9 O
16.8 O
21.9 O
16.1 O

24.2 B-DAT
20.3 O
19.9 O
18.8 O
22.3 O

18.1 B-DAT
14.9 O
16.2 O
19.3 O
18.7 O

MPIIGaze B-DAT
7.6 O
6.2 O
5.7 O
8.7 O

10.1 B-DAT
12.0 O
12.2 O
6.1 O
8.3 O

5.9 B-DAT
6.1 O
6.2 O
7.4 O
4.7 O

4.4 B-DAT
6.0 O
7.3 O
Static O
5.8 O

5.7 B-DAT
4.4 O
7.5 O
6.7 O
8.8 O

11.6 B-DAT
5.5 O
8.3 O
5.5 O
5.2 O

6.3 B-DAT
5.3 O
3.9 O
4.3 O
5.6 O

6.3 B-DAT
Temporal O
6.1 O
5.6 O
4.5 O

7.5 B-DAT
6.4 O
8.2 O
12.0 O
5.0 O

7.5 B-DAT
5.4 O
5.0 O
5.8 O
6.6 O

4.0 B-DAT
4.5 O
5.8 O
6.2 O

Table B-DAT
2: O
Gaze O
angular O
error O

comparison B-DAT
for O
static O
(top O
half) O

and B-DAT
moving O
(bottom O
half) O
head O

pose B-DAT
for O
each O
subject O
in O

the B-DAT
FT O
scenario. O
Best O
results O

in B-DAT
bold. O
−80 O
−40 O
0 O
40 O
80−80 O

40 B-DAT
0 O

40 B-DAT
80 O

0 B-DAT
5 O

10 B-DAT
15 O

20 B-DAT
25 O

30 B-DAT
35 O

80 B-DAT
−40 O
0 O
40 O
80−80 O
−40 O

0 B-DAT
40 O

80 B-DAT
−10 O

8 B-DAT
−6 O

4 B-DAT
−2 O

0 B-DAT
2 O

4 B-DAT
6 O

8 B-DAT
10 O

80 B-DAT
−40 O
0 O
40 O
80−80 O
−40 O

0 B-DAT
40 O

80 B-DAT
0 O

5 B-DAT
10 O

15 B-DAT
20 O

25 B-DAT
30 O

35 B-DAT
−80 O
−40 O
0 O
40 O
80−80 O

40 B-DAT
0 O

40 B-DAT
80 O

10 B-DAT
−8 O

6 B-DAT
−4 O

2 B-DAT
0 O

2 B-DAT
4 O

6 B-DAT
8 O

10 B-DAT
(a) O
Gaze O
space O
(b) O
Head O

orientation B-DAT
space O

Figure B-DAT
5: O
Angular O
error O
distribution O

across B-DAT
gaze O
(a) O
and O
head O

orientation B-DAT
(b) O
spaces O
in O
the O

FT B-DAT
setting, O
in O
terms O
of O

x- B-DAT
and O
y- O
angles. O
For O

each B-DAT
space, O
we O
depict O
the O

Static B-DAT
model O
performance O
(left) O
and O

the B-DAT
contribution O
of O
the O
Temporal O

model B-DAT
versus O
Static O
(right). O
In O

the B-DAT
latter, O
positive O
difference O
means O

higher B-DAT
improvement O
of O
the O
Temporal O

model. B-DAT
4.4 O
Evaluation O
of O
the O
temporal O

network B-DAT

In B-DAT
this O
section, O
we O
evaluate O

the B-DAT
contribution O
of O
adding O
the O

temporal B-DAT
module O
to O
the O
static O

model. B-DAT
To O
do O
so, O
we O

trained B-DAT
a O
lower-dimensional O
version O
of O

the B-DAT
static O
network O
with O
compa- O

rable B-DAT
performance O
to O
the O
original, O

reducing B-DAT
the O
number O
of O
units O

of B-DAT
the O
second O
fusion O
layer O

to B-DAT
2918. O
Results O
are O
reported O

in B-DAT
Figure O
4 O
and O
Table O
2 O

. B-DAT
One O
can O
observe O
that O

using B-DAT
sequential O
information O
is O
helpful O

on B-DAT
the O
FT O
scenario, O
outperforming O

the B-DAT
static O
model O
by O
a O

statistically B-DAT
significant O
4.4% O
(paired O
Wilcoxon O

test, B-DAT
p O
< O
0.0001). O
This O

contribution B-DAT
is O
more O
noticeable O
in O

the B-DAT
moving O
head O
setting, O
proving O

that B-DAT
the O
temporal O
model O
can O

benefit B-DAT
from O
head O
motion O
information. O

In B-DAT
contrast, O
such O
information O
seems O

to B-DAT
be O
less O
meaningful O
in O

the B-DAT
CS O
scenario, O
where O
the O

obtained B-DAT
error O
is O
already O
very O

low B-DAT
for O
a O
cross-subject O
setting O

and B-DAT
the O
amount O
of O
head O

movement B-DAT
declines. O
Figure O
5 O
further O
explores O
the O

error B-DAT
distribution O
of O
the O
static O

network B-DAT
and O
the O
impact O
of O

sequential B-DAT
information. O
We O
can O
observe O

that B-DAT
the O
accuracy O
of O
the O

static B-DAT
model O
drops O
with O
extreme O

head B-DAT
poses O
and O
gaze O
directions O

, B-DAT
which O
can O
also O
be O

correlated B-DAT
to O
having O
less O
data O

in B-DAT
those O
areas. O
Compared O
to O

the B-DAT
static O
model, O
the O
temporal O

model B-DAT
particularly O
benefits O
gaze O
targets O

from B-DAT
mid-range O
upwards. O
Its O
contribution O

is B-DAT
less O
clear O
for O
extreme O

targets, B-DAT
probably O
again O
due O
to O

data B-DAT
imbalance. O
Finally, O
we O
evaluated O
the O
effect O

of B-DAT
different O
recurrent O
architectures O
for O

the B-DAT
temporal O
model. O
In O
particular O

, B-DAT
we O
tested O
1 O
(128 O

units) B-DAT
and O
2 O
(256-128 O
units) O

LSTM B-DAT
and O
GRU O
lay- O
ers, O

with B-DAT
1 O
GRU O
layer O
obtaining O

slightly B-DAT
superior O
results O
(up O
to O
0 O

.12◦). B-DAT
We O
also O
assessed O
the O

effect B-DAT
of O
sequence O
length O
fixing O

s B-DAT
in O
the O
range O
{4,7,10}, O

with B-DAT
s O
= O
7 O
performing O

worse B-DAT
than O
the O
other O
two O
( O

up B-DAT
to O
0 O

14 B-DAT

10 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
5 O
Conclusions O
In O
this O
work O

, B-DAT
we O
studied O
the O
combination O

of B-DAT
full-face O
and O
eye O
images O

along B-DAT
with O
facial O
land- O
marks O

for B-DAT
person- O
and O
head O
pose-independent O

3D B-DAT
gaze O
estimation. O
Consequently, O
we O

pro- B-DAT
posed O
a O
multi-stream O
recurrent O

CNN B-DAT
network O
that O
leverages O
the O

sequential B-DAT
information O
of O
eye O
and O

head B-DAT
movements. O
Both O
static O
and O

temporal B-DAT
versions O
of O
our O
approach O

significantly B-DAT
outperform O
current O
state-of-the-art O
3D O

gaze B-DAT
estimation O
methods O
on O
a O

wide B-DAT
range O
of O
head O
poses O

and B-DAT
gaze O
directions. O
We O
showed O

that B-DAT
adding O
geometry O
features O
to O

appearance-based B-DAT
methods O
has O
a O
regularizing O

effect B-DAT
on O
the O
accuracy. O
Adding O

sequential B-DAT
information O
further O
benefits O
the O

final B-DAT
performance O
compared O
to O
static-only O

input, B-DAT
especially O
from O
mid-range O
up- O

wards B-DAT
and O
in O
those O
cases O

where B-DAT
head O
motion O
is O
present. O

The B-DAT
effect O
in O
very O
extreme O

head B-DAT
poses O
is O
not O
clear O

due B-DAT
to O
data O
imbalance, O
suggesting O

the B-DAT
importance O
of O
learning O
from O

a B-DAT
con- O
tinuous, O
balanced O
dataset O

including B-DAT
all O
head O
poses O
and O

gaze B-DAT
directions O
of O
interest. O
To O

the B-DAT
best O
of O
our O
knowledge, O

this B-DAT
is O
the O
first O
attempt O

to B-DAT
exploit O
the O
temporal O
modality O

in B-DAT
the O
context O
of O
gaze O

estimation B-DAT
from O
remote O
cameras. O
As O

future B-DAT
work, O
we O
will O
further O

explore B-DAT
extracting O
meaningful O
temporal O
representations O

of B-DAT
gaze O
dynamics, O
considering O
3DCNNs O

as B-DAT
well O
as O
the O
encoding O

of B-DAT
deep O
features O
around O
particular O

tracked B-DAT
face O
landmarks O
[14]. O
Acknowledgements O
This O
work O
has O
been O

partially B-DAT
supported O
by O
the O
Spanish O

project B-DAT
TIN2016-74946-P O
(MINECO/ O
FEDER, O
UE O

), B-DAT
CERCA O
Programme O
/ O
Generalitat O

de B-DAT
Catalunya, O
and O
the O
FP7 O

people B-DAT
program O
(Marie O
Curie O
Actions), O

REA B-DAT
grant O
agreement O
no O
FP7-607139 O
( O

iCARE B-DAT
- O
Improving O
Children O
Auditory O

REhabilitation). B-DAT
We O
gratefully O
acknowledge O
the O

support B-DAT
of O
NVIDIA O
Corporation O
with O

the B-DAT
donation O
of O
the O
GPU O

used B-DAT
for O
this O
research. O
Portions O

of B-DAT
the O
research O
in O
this O

pa- B-DAT
per O
used O
the O
EYEDIAP O

dataset B-DAT
made O
available O
by O
the O

Idiap B-DAT
Research O
Institute, O
Martigny, O
Switzerland. O
References O
[1] O
Nicola O
C O
Anderson O

, B-DAT
Evan O
F O
Risko, O
and O

Alan B-DAT
Kingstone. O
Motion O
influences O
gaze O

di- B-DAT
rection O
discrimination O
and O
disambiguates O
contradictory O

luminance B-DAT
cues. O
Psychonomic O
bulletin O

& B-DAT
review, O
23(3):817–823, O
2016 O

. B-DAT
[2] O
Shumeet O
Baluja O
and O
Dean O

Pomerleau. B-DAT
Non-intrusive O
gaze O
tracking O
using O

artificial B-DAT
neu- O
ral O
networks. O
In O

Advances B-DAT
in O
Neural O
Information O
Processing O

Systems, B-DAT
pages O
753–760, O
1994 O

. B-DAT
[3] O
Adrian O
Bulat O
and O
Georgios O

Tzimiropoulos. B-DAT
How O
far O
are O
we O

from B-DAT
solving O
the O
2d O

& B-DAT
3d O
face O
alignment O
problem O

? B-DAT
(and O
a O
dataset O
of O
230, O

000 B-DAT
3d O
facial O
landmarks). O
In O

Interna- B-DAT
tional O
Conference O
on O
Computer O

Vision, B-DAT
2017. O
[4] O
Haoping O
Deng O
and O
Wangjiang O

Zhu. B-DAT
Monocular O
free-head O
3d O
gaze O

tracking B-DAT
with O
deep O
learning O
and O

geometry B-DAT
constraints. O
In O
Computer O
Vision O

(ICCV), B-DAT
2017 O
IEEE O
Interna- O
tional O

Conference B-DAT
on, O
pages O
3162–3171. O
IEEE O

, B-DAT
2017. O
[5] O
Onur O
Ferhat O
and O
Fernando O

Vilariño. B-DAT
Low O
cost O
eye O
tracking O

. B-DAT
Computational O
intelligence O
and O
neuroscience, O
2016 O

:17, B-DAT
2016. O
Citation O
Citation O
{Jung, O
Lee, O
Yim O

, B-DAT
Park, O
and O
Kim} O
2015 O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
11 O

6] B-DAT
Kenneth O
A O
Funes-Mora O
and O

Jean-Marc B-DAT
Odobez. O
Gaze O
estimation O
in O

the B-DAT
3D O
space O
using O
RGB-D O

sensors. B-DAT
International O
Journal O
of O
Computer O

Vision, B-DAT
118(2):194–216, O
2016. O
[7] O
Kenneth O
Alberto O
Funes O
Mora O

, B-DAT
Florent O
Monay, O
and O
Jean-Marc O

Odobez. B-DAT
Eyediap: O
A O
database O
for O

the B-DAT
development O
and O
evaluation O
of O

gaze B-DAT
estimation O
algorithms O
from O
rgb O

and B-DAT
rgb-d O
cameras. O
In O
Proceedings O

of B-DAT
the O
ACM O
Symposium O
on O

Eye B-DAT
Tracking O
Research O
and O
Applications. O

ACM, B-DAT
March O
2014. O
doi: O
10.1145/2578153.2578190. O
[8] O
Kenneth O
Alberto O
Funes O
Mora O

, B-DAT
Florent O
Monay, O
and O
Jean-Marc O

Odobez. B-DAT
Eyediap: O
A O
database O
for O

the B-DAT
development O
and O
evaluation O
of O

gaze B-DAT
estimation O
algorithms O
from O
rgb O

and B-DAT
rgb-d O
cameras. O
In O
Proceedings O

of B-DAT
the O
Symposium O
on O
Eye O

Tracking B-DAT
Research O
and O
Applications, O
pages O
255 O

–258. B-DAT
ACM, O
2014. O
[9] O
Quentin O
Guillon, O
Nouchine O
Hadjikhani O

, B-DAT
Sophie O
Baduel, O
and O
Bernadette O

Rogé. B-DAT
Visual O
social O
attention O
in O

autism B-DAT
spectrum O
disorder: O
Insights O
from O

eye B-DAT
tracking O
studies. O
Neu- O

roscience B-DAT
& O
Biobehavioral O
Reviews, O
42:279–297, O
2014 O

. B-DAT
[10] O
Dan O
Witzner O
Hansen O
and O

Qiang B-DAT
Ji. O
In O
the O
eye O

of B-DAT
the O
beholder: O
A O
survey O

of B-DAT
models O
for O
eyes O
and O

gaze. B-DAT
IEEE O
transactions O
on O
pattern O

analysis B-DAT
and O
machine O
intelligence, O
32(3 O

): B-DAT
478–500, O
2010. O
[11] O
Qiong O
Huang, O
Ashok O
Veeraraghavan O

, B-DAT
and O
Ashutosh O
Sabharwal. O
Tabletgaze: O

dataset B-DAT
and O
analysis O
for O
unconstrained O

appearance-based B-DAT
gaze O
estimation O
in O
mobile O

tablets. B-DAT
Machine O
Vision O
and O
Applications, O
28 O

(5-6):445–461, B-DAT
2017. O
[12] O
Robert O
JK O
Jacob O
and O

Keith B-DAT
S O
Karn. O
Eye O
tracking O

in B-DAT
human-computer O
interaction O
and O
usability O

research: B-DAT
Ready O
to O
deliver O
the O

promises. B-DAT
In O
The O
mind’s O
eye O

, B-DAT
pages O
573–605. O
Elsevier, O
2003. O
[13] O
László O
A O
Jeni O
and O

Jeffrey B-DAT
F O
Cohn. O
Person-independent O
3d O

gaze B-DAT
estimation O
using O
face O
frontalization O

. B-DAT
In O
Proceedings O
of O
the O

IEEE B-DAT
Conference O
on O
Computer O
Vision O

and B-DAT
Pattern O
Recognition O
Workshops, O
pages O
87 O

–95, B-DAT
2016. O
[14] O
Heechul O
Jung, O
Sihaeng O
Lee O

, B-DAT
Junho O
Yim, O
Sunjeong O
Park, O

and B-DAT
Junmo O
Kim. O
Joint O
fine- O

tuning B-DAT
in O
deep O
neural O
networks O

for B-DAT
facial O
expression O
recognition. O
In O

Computer B-DAT
Vision O
(ICCV), O
2015 O
IEEE O

International B-DAT
Conference O
on, O
pages O
2983–2991. O

IEEE, B-DAT
2015. O
[15] O
Anuradha O
Kar O
and O
Peter O

Corcoran. B-DAT
A O
review O
and O
analysis O

of B-DAT
eye-gaze O
estimation O
sys- O
tems O

, B-DAT
algorithms O
and O
performance O
evaluation O

methods B-DAT
in O
consumer O
platforms. O
IEEE O

Access, B-DAT
5:16495–16519, O
2017. O
[16] O
Kyle O
Krafka, O
Aditya O
Khosla O

, B-DAT
Petr O
Kellnhofer, O
Harini O
Kannan, O

Suchendra B-DAT
Bhandarkar, O
Wojciech O
Matusik, O
and O

Antonio B-DAT
Torralba. O
Eye O
tracking O
for O

everyone. B-DAT
In O
Computer O
Vision O
and O

Pattern B-DAT
Recognition O
(CVPR), O
2016 O
IEEE O

Conference B-DAT
on, O
pages O
2176–2184. O
IEEE, O
2016 O

. B-DAT
[17] O
Simon O
P O
Liversedge O
and O

John B-DAT
M O
Findlay. O
Saccadic O
eye O

movements B-DAT
and O
cognition. O
Trends O
in O

cognitive B-DAT
sciences, O
4(1):6–14, O
2000 O

. B-DAT
[18] O
Feng O
Lu, O
Takahiro O
Okabe O

, B-DAT
Yusuke O
Sugano, O
and O
Yoichi O

Sato. B-DAT
A O
head O
pose-free O
approach O

for B-DAT
appearance-based O
gaze O
estimation. O
In O

BMVC, B-DAT
pages O
1–11, O
2011 O

12 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
[19] O
Feng O
Lu, O
Yusuke O
Sugano O

, B-DAT
Takahiro O
Okabe, O
and O
Yoichi O

Sato. B-DAT
Inferring O
human O
gaze O
from O

appearance B-DAT
via O
adaptive O
linear O
regression. O

In B-DAT
Computer O
Vision O
(ICCV), O
2011 O

IEEE B-DAT
International O
Conference O
on, O
pages O
153 O

–160. B-DAT
IEEE, O
2011. O
[20] O
Päivi O
Majaranta O
and O
Andreas O

Bulling. B-DAT
Eye O
tracking O
and O
eye-based O

human–computer B-DAT
interaction. O
In O
Advances O
in O

physiological B-DAT
computing, O
pages O
39–65. O
Springer O

, B-DAT
2014. O
[21] O
Kenneth O
Alberto O
Funes O
Mora O

and B-DAT
Jean-Marc O
Odobez. O
Gaze O
estimation O

from B-DAT
multi- O
modal O
kinect O
data O

. B-DAT
In O
Computer O
Vision O
and O

Pattern B-DAT
Recognition O
Workshops O
(CVPRW), O
2012 O

IEEE B-DAT
Computer O
Society O
Conference O
on, O

pages B-DAT
25–30. O
IEEE, O
2012. O
[22] O
Carlos O
Hitoshi O
Morimoto, O
Arnon O

Amir, B-DAT
and O
Myron O
Flickner. O
Detecting O

eye B-DAT
position O
and O
gaze O
from O

a B-DAT
single O
camera O
and O
2 O

light B-DAT
sources. O
In O
Pattern O
Recognition O

, B-DAT
2002. O
Proceedings. O
16th O
International O

Conference B-DAT
on, O
volume O
4, O
pages O
314 O

–317. B-DAT
IEEE, O
2002. O
[23] O
IMO O
MSC. O
Circ. O
982 O

(2000) B-DAT
guidelines O
on O
ergonomic O
criteria O

for B-DAT
bridge O
equipment O
and O
layout O

. B-DAT
[24] O
Alejandro O
Newell, O
Kaiyu O
Yang O

, B-DAT
and O
Jia O
Deng. O
Stacked O

hourglass B-DAT
networks O
for O
hu- O
man O

pose B-DAT
estimation. O
In O
European O
Conference O

on B-DAT
Computer O
Vision, O
pages O
483–499. O

Springer, B-DAT
2016. O
[25] O
Yasuhiro O
Ono, O
Takahiro O
Okabe O

, B-DAT
and O
Yoichi O
Sato. O
Gaze O

estimation B-DAT
from O
low O
resolution O
images. O

In B-DAT
Pacific-Rim O
Symposium O
on O
Image O

and B-DAT
Video O
Technology, O
pages O
178–188. O

Springer, B-DAT
2006. O
[26] O
Cristina O
Palmero, O
Elisabeth O
A O

. B-DAT
van O
Dam, O
Sergio O
Escalera, O

Mike B-DAT
Kelia, O
Guido O
F. O
Lichtert, O

Lucas B-DAT
P.J.J O
Noldus, O
Andrew O
J. O

Spink, B-DAT
and O
Astrid O
van O
Wieringen. O

Automatic B-DAT
mutual O
gaze O
detection O
in O

face-to-face B-DAT
dyadic O
interaction O
videos. O
In O

Proceedings B-DAT
of O
Measuring O
Behavior, O
pages O
158 O

–163, B-DAT
2018. O
[27] O
Omkar O
M. O
Parkhi, O
Andrea O

Vedaldi, B-DAT
and O
Andrew O
Zisserman. O
Deep O

face B-DAT
recognition. O
In O
British O
Machine O

Vision B-DAT
Conference, O
2015 O

. B-DAT
[28] O
Derek O
R O
Rutter O
and O

Kevin B-DAT
Durkin. O
Turn-taking O
in O
mother–infant O

interaction: B-DAT
An O
exam- O
ination O
of O

vocalizations B-DAT
and O
gaze. O
Developmental O
psychology O

, B-DAT
23(1):54, O
1987. O
[29] O
Brian O
A O
Smith, O
Qi O

Yin, B-DAT
Steven O
K O
Feiner, O
and O

Shree B-DAT
K O
Nayar. O
Gaze O
locking O

: B-DAT
passive O
eye O
contact O
detection O

for B-DAT
human-object O
interaction. O
In O
Proceedings O

of B-DAT
the O
26th O
annual O
ACM O

symposium B-DAT
on O
User O
interface O
software O

and B-DAT
technology, O
pages O
271–280. O
ACM, O
2013 O

. B-DAT
[30] O
Yusuke O
Sugano, O
Yasuyuki O
Matsushita O

, B-DAT
and O
Yoichi O
Sato. O
Appearance-based O

gaze B-DAT
es- O
timation O
using O
visual O

saliency. B-DAT
IEEE O
transactions O
on O
pattern O

analysis B-DAT
and O
machine O
intelligence, O
35(2):329–341, O
2013 O

. B-DAT
[31] O
Yusuke O
Sugano, O
Yasuyuki O
Matsushita O

, B-DAT
and O
Yoichi O
Sato. O
Learning-by-synthesis O

for B-DAT
appearance-based O
3d O
gaze O
estimation. O

In B-DAT
Computer O
Vision O
and O
Pattern O

Recognition B-DAT
(CVPR), O
2014 O
IEEE O
Conference O

on, B-DAT
pages O
1821–1828. O
IEEE, O
2014. O
[32] O
Kar-Han O
Tan, O
David O
J O

Kriegman, B-DAT
and O
Narendra O
Ahuja. O
Appearance-based O

eye B-DAT
gaze O
es- O
timation. O
In O

Applications B-DAT
of O
Computer O
Vision, O
2002.(WACV O

2002). B-DAT
Proceedings. O
Sixth O
IEEE O
Workshop O

on, B-DAT
pages O
191–195. O
IEEE, O
2002 O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
13 O

33] B-DAT
Ronda O
Venkateswarlu O
et O
al. O

Eye B-DAT
gaze O
estimation O
from O
a O

single B-DAT
image O
of O
one O
eye. O

In B-DAT
Computer O
Vision, O
2003. O
Proceedings. O

Ninth B-DAT
IEEE O
International O
Conference O
on, O

pages B-DAT
136–143. O
IEEE, O
2003. O
[34] O
Kang O
Wang O
and O
Qiang O

Ji. B-DAT
Real O
time O
eye O
gaze O

tracking B-DAT
with O
3d O
deformable O
eye-face O

model. B-DAT
In O
Proceedings O
of O
the O

IEEE B-DAT
Conference O
on O
Computer O
Vision O

and B-DAT
Pattern O
Recog- O
nition, O
pages O

1003–1011, B-DAT
2017 O

. B-DAT
[35] O
Oliver O
Williams, O
Andrew O
Blake O

, B-DAT
and O
Roberto O
Cipolla. O
Sparse O

and B-DAT
semi-supervised O
visual O
mapping O
with O

the B-DAT
sˆ O
3gp. O
In O
Computer O

Vision B-DAT
and O
Pattern O
Recognition, O
2006 O

IEEE B-DAT
Computer O
Society O
Conference O
on, O

volume B-DAT
1, O
pages O
230–237. O
IEEE, O
2006 O

. B-DAT
[36] O
William O
Hyde O
Wollaston O
et O

al. B-DAT
Xiii. O
on O
the O
apparent O

direction B-DAT
of O
eyes O
in O
a O

portrait. B-DAT
Philosophical O
Transactions O
of O
the O

Royal B-DAT
Society O
of O
London, O
114:247–256 O

, B-DAT
1824. O
[37] O
Erroll O
Wood O
and O
Andreas O

Bulling. B-DAT
Eyetab: O
Model-based O
gaze O
estimation O

on B-DAT
unmodi- O
fied O
tablet O
computers O

. B-DAT
In O
Proceedings O
of O
the O

Symposium B-DAT
on O
Eye O
Tracking O
Research O

and B-DAT
Applications, O
pages O
207–210. O
ACM, O
2014 O

. B-DAT
[38] O
Erroll O
Wood, O
Tadas O
Baltrusaitis O

, B-DAT
Xucong O
Zhang, O
Yusuke O
Sugano, O

Peter B-DAT
Robinson, O
and O
Andreas O
Bulling. O

Rendering B-DAT
of O
eyes O
for O
eye-shape O

registration B-DAT
and O
gaze O
estimation. O
In O

Proceedings B-DAT
of O
the O
IEEE O
International O

Conference B-DAT
on O
Computer O
Vision, O
pages O
3756 O

– B-DAT
3764, O
2015. O
[39] O
Erroll O
Wood, O
Tadas O
Baltrušaitis O

, B-DAT
Louis-Philippe O
Morency, O
Peter O
Robinson, O

and B-DAT
Andreas O
Bulling. O
A O
3d O

morphable B-DAT
eye O
region O
model O
for O

gaze B-DAT
estimation. O
In O
European O
Confer- O

ence B-DAT
on O
Computer O
Vision, O
pages O
297 O

–313. B-DAT
Springer, O
2016. O
[40] O
Erroll O
Wood, O
Tadas O
Baltrušaitis O

, B-DAT
Louis-Philippe O
Morency, O
Peter O
Robinson, O

and B-DAT
Andreas O
Bulling. O
Learning O
an O

appearance-based B-DAT
gaze O
estimator O
from O
one O

million B-DAT
synthesised O
images. O
In O
Proceedings O

of B-DAT
the O
Ninth O
Biennial O
ACM O

Symposium B-DAT
on O
Eye O
Tracking O
Re- O

search B-DAT
& O
Applications, O
pages O
131–138. O

ACM, B-DAT
2016. O
[41] O
Dong O
Hyun O
Yoo O
and O

Myung B-DAT
Jin O
Chung. O
A O
novel O

non-intrusive B-DAT
eye O
gaze O
estimation O
using O

cross-ratio B-DAT
under O
large O
head O
motion O

. B-DAT
Computer O
Vision O
and O
Image O

Understanding, B-DAT
98(1):25–51, O
2005. O
[42] O
Xucong O
Zhang, O
Yusuke O
Sugano O

, B-DAT
Mario O
Fritz, O
and O
Andreas O

Bulling. B-DAT
Appearance-based O
gaze O
estimation O
in O

the B-DAT
wild. O
In O
Proceedings O
of O

the B-DAT
IEEE O
Conference O
on O
Computer O

Vision B-DAT
and O
Pattern O
Recognition, O
pages O
4511 O

–4520, B-DAT
2015. O
[43] O
Xucong O
Zhang, O
Yusuke O
Sugano O

, B-DAT
Mario O
Fritz, O
and O
Andreas O

Bulling. B-DAT
It’s O
written O
all O
over O

your B-DAT
face: O
Full-face O
appearance-based O
gaze O

estimation. B-DAT
In O
Proc. O
IEEE O
International O

Conference B-DAT
on O
Computer O
Vision O
and O

Pattern B-DAT
Recognition O
Workshops O
(CVPRW), O
2017 O

state O
of O
the O
art O
on O
EYEDIAP B-DAT
dataset, O
further O
improved O
by O
4 O

of O
our O
solution O
on O
the O
EYEDIAP B-DAT
dataset O
[7] O
in O
a O
wide O

VGA O
videos O
from O
the O
publicly-available O
EYEDIAP B-DAT
dataset O
[7] O
to O
perform O
the O

FT-S O
scenario O
are O
provided O
by O
EYEDIAP B-DAT
dataset. O
MPIIGaze:. O
State-of-the-art O
full-face O
3D O

fine-tuned O
it O
with O
the O
filtered O
EYEDIAP B-DAT
subsets O
using O
our O
training O
parameters O

this O
pa- O
per O
used O
the O
EYEDIAP B-DAT
dataset O
made O
available O
by O
the O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
1 O

Recurrent B-DAT
CNN O
for O
3D O
Gaze O

Estimation B-DAT
using O
Appearance O
and O
Shape O

Cues B-DAT
Cristina O
Palmero1,2 O

crpalmec7@alumnes.ub.edu B-DAT
1 O
Dept. O
Mathematics O
and O
Informatics O

Universitat B-DAT
de O
Barcelona, O
Spain O

Javier B-DAT
Selva1 O
javier.selva.castello@est.fib.upc.edu O

2 B-DAT
Computer O
Vision O
Center O
Campus O

UAB, B-DAT
Bellaterra, O
Spain O
Mohammad O
Ali O
Bagheri3,4 O

mohammadali.bagheri@ucalgary.ca B-DAT
3 O
Dept. O
Electrical O
and O
Computer O

Eng. B-DAT
University O
of O
Calgary, O
Canada O

Sergio B-DAT
Escalera1,2 O
sergio@maia.ub.es O

4 B-DAT
Dept. O
Engineering O
University O
of O

Larestan, B-DAT
Iran O
Abstract O

Gaze B-DAT
behavior O
is O
an O
important O

non-verbal B-DAT
cue O
in O
social O
signal O

processing B-DAT
and O
human- O
computer O
interaction. O

In B-DAT
this O
paper, O
we O
tackle O

the B-DAT
problem O
of O
person- O
and O

head B-DAT
pose- O
independent O
3D O
gaze O

estimation B-DAT
from O
remote O
cameras, O
using O

a B-DAT
multi-modal O
recurrent O
convolutional O
neural O

network B-DAT
(CNN). O
We O
propose O
to O

combine B-DAT
face, O
eyes O
region, O
and O

face B-DAT
landmarks O
as O
individual O
streams O

in B-DAT
a O
CNN O
to O
estimate O

gaze B-DAT
in O
still O
images. O
Then, O

we B-DAT
exploit O
the O
dynamic O
nature O

of B-DAT
gaze O
by O
feeding O
the O

learned B-DAT
features O
of O
all O
the O

frames B-DAT
in O
a O
sequence O
to O

a B-DAT
many-to-one O
recurrent O
module O
that O

predicts B-DAT
the O
3D O
gaze O
vector O

of B-DAT
the O
last O
frame. O
Our O

multi-modal B-DAT
static O
solution O
is O
evaluated O

on B-DAT
a O
wide O
range O
of O

head B-DAT
poses O
and O
gaze O
directions, O

achieving B-DAT
a O
significant O
improvement O
of O
14 O

.6% B-DAT
over O
the O
state O
of O

the B-DAT
art O
on O
EYEDIAP O
dataset, O

further B-DAT
improved O
by O
4% O
when O

the B-DAT
temporal O
modality O
is O
included. O
1 O
Introduction O
Eyes O
and O
their O

movements B-DAT
are O
considered O
an O
important O

cue B-DAT
in O
non-verbal O
behavior O
analysis O

, B-DAT
being O
involved O
in O
many O

cognitive B-DAT
processes O
and O
reflecting O
our O

internal B-DAT
state O
[17]. O
More O
specifically, O

eye B-DAT
gaze O
behavior, O
as O
an O

indicator B-DAT
of O
human O
visual O
attention, O

has B-DAT
been O
widely O
studied O
to O

assess B-DAT
communication O
skills O
[28] O
and O

to B-DAT
identify O
possible O
behavioral O

disorders B-DAT
[9]. O
Therefore, O
gaze O
estimation O

has B-DAT
become O
an O
established O
line O

of B-DAT
research O
in O
computer O
vision, O

being B-DAT
a O
key O
feature O
in O

human-computer B-DAT
interaction O
(HCI) O
and O
usability O

research B-DAT
[12, O
20]. O
Recent O
gaze O
estimation O
research O
has O

focused B-DAT
on O
facilitating O
its O
use O

in B-DAT
general O
everyday O
applications O
under O

real-world B-DAT
conditions, O
using O
off-the-shelf O
remote O

RGB B-DAT
cameras O
and O
re- O
moving O

the B-DAT
need O
of O
personal O
calibration O

[26]. B-DAT
In O
this O
setting, O
appearance-based O

methods, B-DAT
which O
learn O
a O
mapping O

from B-DAT
images O
to O
gaze O
directions O

, B-DAT
are O
the O
preferred O

choice B-DAT
[25]. O
How- O
ever, O
they O

need B-DAT
large O
amounts O
of O
training O

data B-DAT
to O
be O
able O
to O

generalize B-DAT
well O
to O
in-the-wild O
situations, O

which B-DAT
are O
characterized O
by O
significant O

variability B-DAT
in O
head O
poses, O
face O

appearances B-DAT
and O
lighting O
conditions. O
In O

recent B-DAT
years, O
CNNs O
have O
been O

reported B-DAT
to O
outperform O
classical O
methods. O

However, B-DAT
most O
existing O
approaches O
have O

only B-DAT
been O
tested O
in O
restricted O

HCI B-DAT
tasks, O
c© O
2018. O
The O
copyright O
of O

this B-DAT
document O
resides O
with O
its O

authors. B-DAT
It O
may O
be O
distributed O

unchanged B-DAT
freely O
in O
print O
or O

electronic B-DAT
forms O

. B-DAT
ar O
X O

iv B-DAT
:1 O
80 O
5 O

. B-DAT
03 O
06 O

4v B-DAT
3 O

cs B-DAT
.C O
V O

1 B-DAT
7 O

Se B-DAT
p O

20 B-DAT
18 O

Citation B-DAT
Citation O
{Liversedge O
and O
Findlay} O
2000 O

Citation B-DAT
Citation O
{Rutter O
and O
Durkin} O
1987 O

Citation B-DAT
Citation O
{Guillon, O
Hadjikhani, O
Baduel, O

and B-DAT
Rog{é}} O
2014 O
Citation O
Citation O
{Jacob O
and O
Karn O

} B-DAT
2003 O
Citation O
Citation O
{Majaranta O
and O
Bulling O

} B-DAT
2014 O
Citation O
Citation O
{Palmero, O
van O
Dam O

, B-DAT
Escalera, O
Kelia, O
Lichtert, O
Noldus, O

Spink, B-DAT
and O
van O
Wieringen} O
2018 O
Citation O
Citation O
{Ono, O
Okabe, O
and O

Sato} B-DAT
2006 O

2 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
Method O
3D O
gaze O
direction O

Unrestricted B-DAT
gaze O
target O
Full O
face O

Eye B-DAT
region O
Facial O
landmarks O

Sequential B-DAT
information O
Zhang O
et O
al. O
(1) O
[42 O

] B-DAT
3 O
7 O
7 O
3 O
7 O
7 O
Krafka O
et O
al. O
[16 O

] B-DAT
7 O
7 O
3 O
3 O
7 O
7 O
Zhang O
et O
al. O
(2 O

) B-DAT
[43] O
3 O
7 O
3 O
7 O
7 O
7 O
Deng O
and O
Zhu O

[4] B-DAT
3 O
3 O
3 O
3 O

7 B-DAT
7 O
Ours O
3 O
3 O

3 B-DAT
3 O
3 O
3 O

Table B-DAT
1: O
Characteristics O
of O
recent O

related B-DAT
work O
on O
person- O
and O

head B-DAT
pose-independent O
appearance-based O
gaze O
estimation O

methods B-DAT
using O
CNNs. O
where O
users O
look O
at O
the O

screen B-DAT
or O
mobile O
phone, O
showing O

a B-DAT
low O
head O
pose O
variability O

. B-DAT
It O
is O
yet O
unclear O

how B-DAT
these O
methods O
would O
perform O

in B-DAT
a O
wider O
range O
of O

head B-DAT
poses. O
On O
a O
different O
note, O
until O

very B-DAT
recently, O
the O
majority O
of O

methods B-DAT
only O
used O
static O
eye O

region B-DAT
appearance O
as O
input. O
State-of-the-art O

approaches B-DAT
have O
demonstrated O
that O
using O

the B-DAT
face O
along O
with O
a O

higher B-DAT
resolution O
image O
of O
the O

eyes B-DAT
[16], O
or O
even O
just O

the B-DAT
face O
itself O
[43], O
increases O

performance. B-DAT
Indeed, O
the O
whole-face O
image O

encodes B-DAT
more O
information O
than O
eyes O

alone, B-DAT
such O
as O
illumination O
and O

head B-DAT
pose. O
Nevertheless, O
gaze O
behavior O

is B-DAT
not O
static. O
Eye O
and O

head B-DAT
movements O
allow O
us O
to O

direct B-DAT
our O
gaze O
to O
target O

locations B-DAT
of O
interest. O
It O
has O

been B-DAT
demonstrated O
that O
humans O
can O

better B-DAT
predict O
gaze O
when O
being O

shown B-DAT
image O
sequences O
of O
other O

people B-DAT
moving O
their O
eyes O
[1 O

]. B-DAT
However, O
it O
is O
still O

an B-DAT
open O
question O
whether O
this O

se- B-DAT
quential O
information O
can O
increase O

the B-DAT
performance O
of O
automatic O
methods. O
In O
this O
work, O
we O
show O

that B-DAT
the O
combination O
of O
multiple O

cues B-DAT
benefits O
the O
gaze O
estimation O

task. B-DAT
In O
particular, O
we O
use O

face, B-DAT
eye O
region O
and O
facial O

landmarks B-DAT
from O
still O
images. O
Facial O

landmarks B-DAT
model O
the O
global O
shape O

of B-DAT
the O
face O
and O
come O

at B-DAT
no O
cost, O
since O
face O

alignment B-DAT
is O
a O
common O
pre-processing O

step B-DAT
in O
many O
facial O
image O

analysis B-DAT
approaches. O
Furthermore, O
we O
present O

a B-DAT
subject-independent, O
free-head O
recurrent O
3D O

gaze B-DAT
regression O
network O
to O
leverage O

the B-DAT
temporal O
information O
of O
image O

sequences. B-DAT
The O
static O
streams O
of O

each B-DAT
frame O
are O
combined O
in O

a B-DAT
late-fusion O
fashion O
using O
a O

multi-stream B-DAT
CNN. O
Then, O
all O
feature O

vectors B-DAT
are O
input O
to O
a O

many-to-one B-DAT
recurrent O
module O
that O
predicts O

the B-DAT
gaze O
vector O
of O
the O

last B-DAT
sequence O
frame O

. B-DAT
In O
summary, O
our O
contributions O
are O

two-fold. B-DAT
First, O
we O
present O
a O

Recurrent-CNN B-DAT
net- O
work O
architecture O
that O

combines B-DAT
appearance, O
shape O
and O
temporal O

information B-DAT
for O
3D O
gaze O
estimation O

. B-DAT
Second, O
we O
test O
static O

and B-DAT
temporal O
versions O
of O
our O

solution B-DAT
on O
the O
EYEDIAP O

dataset B-DAT
[7] O
in O
a O
wide O

range B-DAT
of O
head O
poses O
and O

gaze B-DAT
directions, O
showing O
consistent O
perfor- O

mance B-DAT
improvements O
compared O
to O
related O

appearance-based B-DAT
methods. O
To O
the O
best O

of B-DAT
our O
knowledge, O
this O
is O

the B-DAT
first O
third-person, O
remote O
camera-based O

approach B-DAT
that O
uses O
tempo- O
ral O

information B-DAT
for O
this O
task. O
Table O
1 O
outlines O
our O
main O
method O
characteristics O

compared B-DAT
to O
related O
work. O
Models O

and B-DAT
code O
are O
publicly O
available O

at B-DAT
https://github.com/ O
crisie/RecurrentGaze O

. B-DAT
2 O
Related O
work O
Gaze O
estimation O

methods B-DAT
are O
typically O
categorized O
as O

model-based B-DAT
or O
appearance-based O
[5, O
10 O

, B-DAT
15]. O
Model-based O
approaches O
use O

a B-DAT
geometric O
model O
of O
the O

eye, B-DAT
usually O
requir- O
ing O
either O

high B-DAT
resolution O
images O
or O
a O

person-specific B-DAT
calibration O
stage O
to O
estimate O

personal B-DAT
eye O
parameters O
[22, O
33, O
34, O
37, O
41]. O
In O
contrast, O
appearance-based O

methods B-DAT
learn O
a O
di- O
rect O

mapping B-DAT
from O
intensity O
images O
or O

extracted B-DAT
eye O
features O
to O
gaze O

directions, B-DAT
thus O
being O

Citation B-DAT
Citation O
{Zhang, O
Sugano, O
Fritz, O

and B-DAT
Bulling} O
2015 O
Citation O
Citation O
{Krafka, O
Khosla, O
Kellnhofer O

, B-DAT
Kannan, O
Bhandarkar, O
Matusik, O
and O

Torralba} B-DAT
2016 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2017 O
Citation O
Citation O
{Deng O
and O
Zhu O

} B-DAT
2017 O
Citation O
Citation O
{Krafka, O
Khosla, O
Kellnhofer O

, B-DAT
Kannan, O
Bhandarkar, O
Matusik, O
and O

Torralba} B-DAT
2016 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2017 O
Citation O
Citation O
{Anderson, O
Risko, O
and O

Kingstone} B-DAT
2016 O

Citation B-DAT
Citation O
{Funesprotect O
unhbox O
voidb@x O

penalty B-DAT
@M O
{}Mora, O
Monay, O
and O
Odobez} O
2014 O

{} B-DAT
Citation O
Citation O
{Ferhat O
and O
Vilari{ñ}o O

} B-DAT
2016 O
Citation O
Citation O
{Hansen O
and O
Ji O

} B-DAT
2010 O
Citation O
Citation O
{Kar O
and O
Corcoran O

} B-DAT
2017 O
Citation O
Citation O
{Morimoto, O
Amir, O
and O

Flickner} B-DAT
2002 O

Citation B-DAT
Citation O
{Venkateswarlu O
etprotect O
unhbox O

voidb@x B-DAT
penalty O
@M O
{}al.} O
2003 O

Citation B-DAT
Citation O
{Wang O
and O
Ji} O
2017 O

Citation B-DAT
Citation O
{Wood O
and O
Bulling} O
2014 O

Citation B-DAT
Citation O
{Yoo O
and O
Chung} O
2005 O

https://github.com/crisie/RecurrentGaze B-DAT

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
3 O

potentially B-DAT
applicable O
to O
relatively O
low O

resolution B-DAT
images O
and O
mid-distance O
scenarios. O

Dif- B-DAT
ferent O
mapping O
functions O
have O

been B-DAT
explored, O
such O
as O
neural O

networks B-DAT
[2], O
adaptive O
linear O
regression O
( O

ALR) B-DAT
[19], O
local O
interpolation O
[32], O

gaussian B-DAT
processes O
[30, O
35], O
random O

forests B-DAT
[11, O
31], O
or O
k-nearest O

neighbors B-DAT
[40]. O
Main O
challenges O
of O

appearance-based B-DAT
methods O
for O
3D O
gaze O

estimation B-DAT
are O
head O
pose, O
illumination O

and B-DAT
subject O
invariance O
without O
user-specific O

calibration. B-DAT
To O
handle O
these O
issues, O

some B-DAT
works O
proposed O
compensation O

methods B-DAT
[18] O
and O
warping O
strategies O

that B-DAT
synthesize O
a O
canonical, O
frontal O

looking B-DAT
view O
of O
the O

face B-DAT
[6, O
13, O
21]. O
Hybrid O

approaches B-DAT
based O
on O
analysis-by-synthesis O
have O

also B-DAT
been O
evaluated O
[39]. O
Currently, O
data-driven O
methods O
are O
considered O

the B-DAT
state O
of O
the O
art O

for B-DAT
person- O
and O
head O
pose-independent O

appearance-based B-DAT
gaze O
estimation. O
Consequently, O
a O

number B-DAT
of O
gaze O
es- O
timation O

datasets B-DAT
have O
been O
introduced O
in O

recent B-DAT
years, O
either O
in O
controlled O

[29] B-DAT
or O
semi- O
controlled O
settings O

[8], B-DAT
in O
the O
wild O
[16 O

, B-DAT
42], O
or O
consisting O
of O

synthetic B-DAT
data O
[31, O
38, O
40]. O

Zhang B-DAT
et O
al. O
[42] O
showed O

that B-DAT
CNNs O
can O
outperform O
other O

mapping B-DAT
methods, O
using O
a O
multi- O

modal B-DAT
CNN O
to O
learn O
the O

mapping B-DAT
from O
3D O
head O
poses O

and B-DAT
eye O
images O
to O
3D O

gaze B-DAT
directions. O
Krafka O
et O

al. B-DAT
[16] O
proposed O
a O
multi-stream O

CNN B-DAT
for O
2D O
gaze O
estimation, O

using B-DAT
individual O
eye, O
whole-face O
image O

and B-DAT
the O
face O
grid O
as O

input. B-DAT
As O
this O
method O
was O

limited B-DAT
to O
2D O
screen O
mapping, O

Zhang B-DAT
et O
al. O
[43] O
later O

explored B-DAT
the O
potential O
of O
just O

using B-DAT
whole-face O
images O
as O
input O

to B-DAT
estimate O
3D O
gaze O
directions. O

Using B-DAT
a O
spatial O
weights O
CNN, O

they B-DAT
demonstrated O
their O
method O
to O

be B-DAT
more O
robust O
to O
facial O

appearance B-DAT
variation O
caused O
by O
head O

pose B-DAT
and O
illumina- O
tion O
than O

eye-only B-DAT
methods. O
While O
the O
method O

was B-DAT
evaluated O
in O
the O
wild, O

the B-DAT
subjects O
were O
only O
interacting O

with B-DAT
a O
mobile O
device, O
thus O

restricting B-DAT
the O
head O
pose O
range. O

Deng B-DAT
and O
Zhu O
[4] O
presented O

a B-DAT
two-stream O
CNN O
to O
disjointly O

model B-DAT
head O
pose O
from O
face O

images B-DAT
and O
eye- O
ball O
movement O

from B-DAT
eye O
region O
images. O
Both O

were B-DAT
then O
aggregated O
into O
3D O

gaze B-DAT
direction O
using O
a O
gaze O

transform B-DAT
layer. O
The O
decomposition O
was O

aimed B-DAT
to O
avoid O
head-correlation O
over- O

fitting B-DAT
of O
previous O
data-driven O
approaches. O

They B-DAT
evaluated O
their O
approach O
in O

the B-DAT
wild O
with O
a O
wider O

range B-DAT
of O
head O
poses, O
obtaining O

better B-DAT
performance O
than O
previous O
eye-based O

methods. B-DAT
However, O
they O
did O
not O

test B-DAT
it O
on O
public O
annotated O

benchmark B-DAT
datasets. O
In O
this O
paper, O
we O
propose O

a B-DAT
multi-stream O
recurrent O
CNN O
network O

for B-DAT
person- O
and O
head O
pose-independent O

3D B-DAT
gaze O
estimation O
for O
a O

mid-distance B-DAT
scenario. O
We O
evaluate O
it O

on B-DAT
a O
wider O
range O
of O

head B-DAT
poses O
and O
gaze O
directions O

than B-DAT
screen-targeted O
approaches. O
As O
opposed O

to B-DAT
previous O
methods, O
we O
also O

rely B-DAT
on O
temporal O
information O
inherent O

in B-DAT
sequential O
data O

. B-DAT
3 O
Methodology O

In B-DAT
this O
section, O
we O
present O

our B-DAT
approach O
for O
3D O
gaze O

regression B-DAT
based O
on O
appearance O
and O

shape B-DAT
cues O
for O
still O
images O

and B-DAT
image O
sequences. O
First, O
we O

introduce B-DAT
the O
data O
modalities O
and O

formulate B-DAT
the O
problem. O
Then, O
we O

detail B-DAT
the O
normalization O
procedure O
prior O

to B-DAT
the O
regression O
stage. O
Finally, O

we B-DAT
explain O
the O
global O
network O

topology B-DAT
as O
well O
as O
the O

implementation B-DAT
details. O
An O
overview O
of O

the B-DAT
system O
architecture O
is O
depicted O

in B-DAT
Figure O
1. O
3.1 O
Multi-modal O
gaze O
regression O

Let B-DAT
us O
represent O
gaze O
direction O

as B-DAT
a O
3D O
unit O
vector O

g B-DAT
= O
[gx,gy,gz]T O
∈R3 O
in O

the B-DAT
Camera O
Coor- O
dinate O
System O
( O

CCS), B-DAT
whose O
origin O
is O
the O

central B-DAT
point O
between O
eyeball O
centers. O

Assuming B-DAT
a O
calibrated O
camera, O
and O

a B-DAT
known O
head O
position O
and O

orientation, B-DAT
our O
goal O
is O
to O

estimate B-DAT
g O
from O
a O
sequence O

of B-DAT
images O
{I(i) O
| O

I B-DAT
∈ O
RW×H×3} O
as O
a O

regression B-DAT
problem. O
Citation O
Citation O
{Baluja O
and O
Pomerleau O

} B-DAT
1994 O
Citation O
Citation O
{Lu, O
Sugano, O
Okabe O

, B-DAT
and O
Sato} O
2011{} O
Citation O
Citation O
{Tan, O
Kriegman, O
and O

Ahuja} B-DAT
2002 O

Citation B-DAT
Citation O
{Sugano, O
Matsushita, O
and O

Sato} B-DAT
2013 O
Citation O
Citation O
{Williams, O
Blake, O
and O

Cipolla} B-DAT
2006 O

Citation B-DAT
Citation O
{Huang, O
Veeraraghavan, O
and O

Sabharwal} B-DAT
2017 O
Citation O
Citation O
{Sugano, O
Matsushita, O
and O

Sato} B-DAT
2014 O

Citation B-DAT
Citation O
{Wood, O
Baltru{²}aitis, O
Morency, O

Robinson, B-DAT
and O
Bulling} O
2016{} O
Citation O
Citation O
{Lu, O
Okabe, O
Sugano O

, B-DAT
and O
Sato} O
2011{} O
Citation O
Citation O
{Funes-Mora O
and O
Odobez O

} B-DAT
2016 O
Citation O
Citation O
{Jeni O
and O
Cohn O

} B-DAT
2016 O
Citation O
Citation O
{Mora O
and O
Odobez O

} B-DAT
2012 O
Citation O
Citation O
{Wood, O
Baltru{²}aitis, O
Morency O

, B-DAT
Robinson, O
and O
Bulling} O
2016{} O
Citation O
Citation O
{Smith, O
Yin, O
Feiner O

, B-DAT
and O
Nayar} O
2013 O
Citation O
Citation O
{Funesprotect O
unhbox O
voidb@x O

penalty B-DAT
@M O

Mora, B-DAT
Monay, O
and O
Odobez} O
2014{} O
Citation O
Citation O
{Krafka, O
Khosla, O
Kellnhofer O

, B-DAT
Kannan, O
Bhandarkar, O
Matusik, O
and O

Torralba} B-DAT
2016 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2015 O
Citation O
Citation O
{Sugano, O
Matsushita, O
and O

Sato} B-DAT
2014 O

Citation B-DAT
Citation O
{Wood, O
Baltrusaitis, O
Zhang, O

Sugano, B-DAT
Robinson, O
and O
Bulling} O
2015 O
Citation O
Citation O
{Wood, O
Baltru{²}aitis, O
Morency O

, B-DAT
Robinson, O
and O
Bulling} O
2016{} O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2015 O
Citation O
Citation O
{Krafka, O
Khosla, O
Kellnhofer O

, B-DAT
Kannan, O
Bhandarkar, O
Matusik, O
and O

Torralba} B-DAT
2016 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2017 O
Citation O
Citation O
{Deng O
and O
Zhu O

} B-DAT
2017 O

4 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
Conv O

C B-DAT
on O
ca O
t O
x O
y O
z O
x O
y O

z B-DAT
x O
y O
z O

Individual B-DAT
Fusion O
Temporal O
Individual O
Fusion O

Input B-DAT

Individual B-DAT
Fusion O
Normalization O

.Conv B-DAT

Conv B-DAT
. O
Conv O

Conv B-DAT
. O
FC O

FC B-DAT
FC O
RNN O
RNN O

RNN B-DAT
FC O
Ti O
m O
e O

Figure B-DAT
1: O
Overview O
of O
the O

proposed B-DAT
network. O
A O
multi-stream O
CNN O

jointly B-DAT
models O
full-face, O
eye O
region O

appearance B-DAT
and O
face O
landmarks O
from O

still B-DAT
images. O
The O
combined O
extracted O

fea- B-DAT
tures O
from O
each O
frame O

are B-DAT
fed O
into O
a O
recurrent O

module B-DAT
to O
predict O
last O
frame’s O

gaze B-DAT
direction. O
Gazing O
to O
a O
specific O
target O

is B-DAT
achieved O
by O
a O
combination O

of B-DAT
eye O
and O
head O
movements O

, B-DAT
which O
are O
highly O
coordinated. O

Consequently, B-DAT
the O
apparent O
direction O
of O

gaze B-DAT
is O
influenced O
not O
only O

by B-DAT
the O
location O
of O
the O

irises B-DAT
within O
the O
eyelid O
aperture, O

but B-DAT
also O
by O
the O
position O

and B-DAT
orientation O
of O
the O
face O

with B-DAT
respect O
to O
the O
camera. O

Known B-DAT
as O
the O
Wollaston O

effect B-DAT
[36], O
the O
exact O
same O

set B-DAT
of O
eyes O
may O
appear O

to B-DAT
be O
looking O
in O
different O

directions B-DAT
due O
to O
the O
surrounding O

facial B-DAT
cues. O
It O
is O
therefore O

reasonable B-DAT
to O
state O
that O
eye O

images B-DAT
are O
not O
sufficient O
to O

estimate B-DAT
gaze O
direction. O
Instead, O
whole-face O

images B-DAT
can O
encode O
head O
pose O

or B-DAT
illumination-specific O
information O
across O
larger O

areas B-DAT
than O
those O
available O
just O

in B-DAT
the O
eyes O
region O
[16, O
43 O

]. B-DAT
The O
drawback O
of O
appearance-only O
methods O

is B-DAT
that O
global O
structure O
information O

is B-DAT
not O
explicitly O
considered. O
In O

that B-DAT
sense, O
facial O
landmarks O
can O

be B-DAT
used O
as O
global O
shape O

cues B-DAT
to O
en- O
code O
spatial O

relationships B-DAT
and O
geometric O
constraints. O
Current O

state-of-the-art B-DAT
face O
alignment O
approaches O
are O

robust B-DAT
enough O
to O
handle O
large O

appearance B-DAT
variability, O
extreme O
head O
poses O

and B-DAT
occlusions, O
being O
especially O
useful O

when B-DAT
the O
dataset O
used O
for O

gaze B-DAT
estimation O
does O
not O
contain O

such B-DAT
variability. O
Facial O
landmarks O
are O

mainly B-DAT
correlated O
with O
head O
orientation O

, B-DAT
eye O
position, O
eyelid O
openness, O

and B-DAT
eyebrow O
movement, O
which O
are O

valuable B-DAT
features O
for O
our O
task. O
Therefore, O
in O
our O
approach O
we O

jointly B-DAT
model O
appearance O
and O
shape O

cues B-DAT
(see O
Figure O
1). O
The O

former B-DAT
is O
represented O
by O
a O

whole-face B-DAT
image O
IF O
, O
along O

with B-DAT
a O
higher O
resolution O
image O

of B-DAT
the O
eyes O
IE O
to O

identify B-DAT
subtle O
changes. O
Due O
to O

dealing B-DAT
with O
wide O
head O
pose O

ranges, B-DAT
some O
eye O
images O
may O

not B-DAT
depict O
the O
whole O
eye O

, B-DAT
containing O
mostly O
background O
or O

other B-DAT
surrounding O
facial O
parts O
instead. O

For B-DAT
that O
reason, O
and O
contrary O

to B-DAT
previous O
approaches O
that O
only O

use B-DAT
one O
eye O
image O
[31, O
42 O

], B-DAT
we O
use O
a O
single O

image B-DAT
composed O
of O
two O
patches O

of B-DAT
centered O
left O
and O
right O

eyes. B-DAT
Finally, O
the O
shape O
cue O

is B-DAT
represented O
by O
3D O
face O

landmarks B-DAT
obtained O
from O
a O
68-landmark O

model, B-DAT
denoted O
by O

L B-DAT
= O
{(lx, O
ly, O

) B-DAT

| B-DAT
∀c O
∈ O
[1, O
...,68 O

]}. B-DAT
In O
this O
work O
we O
also O

consider B-DAT
the O
dynamic O
component O
of O

gaze. B-DAT
We O
leverage O
the O
se O

- B-DAT
quential O
information O
of O
eye O

and B-DAT
head O
movements O
such O
that, O

given B-DAT
appearance O
and O
shape O
features O

of B-DAT
consecutive O
frames, O
it O
is O

possible B-DAT
to O
better O
predict O
the O

gaze B-DAT
direction O
of O
the O
cur- O

rent B-DAT
frame. O
Therefore, O
the O
3D O

gaze B-DAT
estimation O
task O
for O
a O
1 O

-frame B-DAT
sequence O
is O
formulated O
Citation O
Citation O
{Wollaston O
etprotect O
unhbox O

voidb@x B-DAT
penalty O
@M O

al.} B-DAT
1824 O
Citation O
Citation O
{Krafka, O
Khosla, O
Kellnhofer O

, B-DAT
Kannan, O
Bhandarkar, O
Matusik, O
and O

Torralba} B-DAT
2016 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2017 O
Citation O
Citation O
{Sugano, O
Matsushita, O
and O

Sato} B-DAT
2014 O

Citation B-DAT
Citation O
{Zhang, O
Sugano, O
Fritz, O

and B-DAT
Bulling} O
2015 O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
5 O

as B-DAT
g(i) O
= O
f O
( O
{IF O
(i)},{IE O
(i)},{L(i O

)} B-DAT
) O
, O
where O
i O
denotes O

the B-DAT
i-th O
frame, O
and O
f O

is B-DAT
the O
regression O

function. B-DAT
3.2 O
Data O
normalization O
Prior O
to O

gaze B-DAT
regression, O
a O
normalization O
step O

in B-DAT
the O
3D O
space O
and O

the B-DAT
2D O
image, O
similar O
to O

[31], B-DAT
is O
carried O
out. O
This O

is B-DAT
performed O
to O
reduce O
the O

appearance B-DAT
variability O
and O
to O
allow O

the B-DAT
gaze O
estimation O
model O
to O

be B-DAT
applied O
regardless O
of O
the O

original B-DAT
camera O
configuration O

. B-DAT
Let O
H O
∈ O
R3x3 O
be O

the B-DAT
head O
rotation O
matrix, O
and O

p B-DAT
= O
[px, O
py, O
pz]T O

∈ B-DAT
R3 O
the O
reference O
face O

location B-DAT
with O
respect O
to O
the O

original B-DAT
CCS. O
The O
goal O
is O

to B-DAT
find O
the O
conversion O
matrix O

M B-DAT
= O
SR O
such O
that O

(a) B-DAT
the O
X-axes O
of O
the O

virtual B-DAT
camera O
and O
the O
head O

become B-DAT
parallel O
using O
the O
rotation O

matrix B-DAT
R, O
and O
(b) O
the O

virtual B-DAT
camera O
looks O
at O
the O

reference B-DAT
location O
from O
a O
fixed O

distance B-DAT
dn O
using O
the O
Z-direction O

scaling B-DAT
matrix O
S O
= O
diag(1,1,dn/‖p O

‖). B-DAT
R O
is O
computed O
as O

a B-DAT
= O
p̂×HT O
e1, O

b B-DAT
= O
â× O
p̂, O

R B-DAT
= O
[â, O
b̂, O
p̂]T O
, O
where O
e1 O
denotes O
the O
first O

orthonormal B-DAT
basis O
and O

〈 B-DAT
·̂ O
〉 O
is O
the O

unit B-DAT
vector O

. B-DAT
This O
normalization O
translates O
into O
the O

image B-DAT
space O
as O
a O
cropped O

image B-DAT
patch O
of O
size O
Wn×Hn O

centered B-DAT
at O
p O
where O
head O

roll B-DAT
rotation O
has O
been O
removed O

. B-DAT
This O
is O
done O
by O

applying B-DAT
a O
perspective O
warping O
to O

the B-DAT
input O
image O
I O
using O

the B-DAT
transformation O
matrix O
W O
= O

CoMCn−1, B-DAT
where O
Co O
and O
Cn O

are B-DAT
the O
original O
and O
virtual O

camera B-DAT
matrices, O
respectively. O
The O
3D O
gaze O
vector O
is O

also B-DAT
normalized O
as O
gn O
=Rg O

. B-DAT
After O
image O
normalization, O
the O

line B-DAT
of O
sight O
can O
be O

represented B-DAT
in O
a O
2D O
space. O

Therefore, B-DAT
gn O
is O
further O
transformed O

to B-DAT
spherical O
coor- O
dinates O
(θ O
, O

φ) B-DAT
assuming O
unit O
length, O
where O

θ B-DAT
and O
φ O
denote O
the O

horizontal B-DAT
and O
vertical O
direc- O
tion O

angles, B-DAT
respectively. O
This O
2D O
angle O

representation, B-DAT
delimited O
in O
the O

range B-DAT
[−π/2,π/2], O
is O
computed O
as O

θ B-DAT
= O
arctan(gx/gz) O
and O

φ B-DAT
= O
arcsin(−gy), O
such O
that O
(0, O

0) B-DAT
represents O
looking O
straight O
ahead O

to B-DAT
the O
CCS O
origin. O
3.3 O
Recurrent O
Convolutional O
Neural O
Network O

We B-DAT
propose O
a O
Recurrent O
CNN O

Regression B-DAT
Network O
for O
3D O
gaze O

estimation. B-DAT
The O
network O
is O
divided O

in B-DAT
3 O
modules: O
(1) O
Individual O

, B-DAT
(2) O
Fusion, O
and O
(3) O

Temporal. B-DAT
First, O
the O
Individual O
module O
learns O

features B-DAT
from O
each O
appearance O
cue O

separately. B-DAT
It O
consists O
of O
a O

two-stream B-DAT
CNN, O
one O
devoted O
to O

the B-DAT
normalized O
face O
image O
stream O

and B-DAT
the O
other O
to O
the O

joint B-DAT
normalized O
eyes O
image. O
Next O

, B-DAT
the O
Fusion O
module O
combines O

the B-DAT
extracted O
features O
of O
each O

appearance B-DAT
stream O
in O
a O
single O

vector B-DAT
along O
with O
the O
normalized O

landmark B-DAT
coordinates. O
Then, O
it O
learns O

a B-DAT
joint O
representation O
between O
modalities O

in B-DAT
a O
late-fusion O
fashion. O
Both O

Individual B-DAT
and O
Fusion O
modules, O
further O

referred B-DAT
to O
as O
Static O
model, O

are B-DAT
applied O
to O
each O
frame O

of B-DAT
the O
sequence. O
Finally, O
the O

resulting B-DAT
feature O
vectors O
of O
each O

frame B-DAT
are O
input O
to O
the O

Temporal B-DAT
module O
based O
on O
a O

many-to-one B-DAT
recurrent O
network. O
This O
module O

leverages B-DAT
sequential O
information O
to O
predict O

the B-DAT
normalized O
2D O
gaze O
angles O

of B-DAT
the O
last O
frame O
of O

the B-DAT
sequence O
using O
a O
linear O

regression B-DAT
layer O
added O
on O
top O

of B-DAT
it. O
3.4 O
Implementation O
details O
3.4.1 O
Network O

details B-DAT

Each B-DAT
stream O
of O
the O
Individual O

module B-DAT
is O
based O
on O
the O

VGG-16 B-DAT
deep O
network O
[27], O
consisting O

of B-DAT
13 O
convolutional O
layers, O
5 O

max B-DAT
pooling O
layers, O
and O
1 O

fully B-DAT
connected O
(FC) O
layer O
with O

Rec- B-DAT
tified O
Linear O
Unit O
(ReLU) O

activations. B-DAT
The O
full-face O
stream O
follows O

the B-DAT
same O
configuration O
Citation O
Citation O
{Sugano, O
Matsushita, O
and O

Sato} B-DAT
2014 O

Citation B-DAT
Citation O
{Parkhi, O
Vedaldi, O
and O

Zisserman} B-DAT
2015 O

6 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
as O
the O
base O
network, O
having O

an B-DAT
input O
of O
224×224 O
pixels O

and B-DAT
a O
4096D O
FC O
layer O

. B-DAT
In O
contrast, O
the O
input O

joint B-DAT
eye O
image O
is O
smaller, O

with B-DAT
a O
final O
size O
of O
120 O

×48 B-DAT
pixels, O
so O
the O
number O

of B-DAT
pa- O
rameters O
is O
decreased O

proportionally. B-DAT
In O
this O
case, O
its O

last B-DAT
FC O
layer O
produces O
a O

1536D B-DAT
vector. O
A O
204D O
landmark O

coordinates B-DAT
vector O
is O
concatenated O
to O

the B-DAT
output O
of O
the O
FC O

layer B-DAT
of O
each O
stream, O
resulting O

in B-DAT
a O
5836D O
feature O
vector. O

Consequently, B-DAT
the O
Fusion O
module O
consists O

of B-DAT
2 O
5836D O
FC O
layers O

with B-DAT
ReLU O
activations O
and O
2 O

dropout B-DAT
layers O
between O
FCs O
as O

regularization. B-DAT
Finally, O
to O
model O
the O

temporal B-DAT
dependencies, O
we O
use O
a O

single B-DAT
GRU O
layer O
with O
128 O

units. B-DAT
The O
network O
is O
trained O
in O

a B-DAT
stage-wise O
fashion. O
First, O
we O

train B-DAT
the O
Static O
model O
and O

the B-DAT
final O
regression O
layer O
end-to-end O

on B-DAT
each O
individual O
frame O
of O

the B-DAT
training O
data. O
The O
convolutional O

blocks B-DAT
are O
pre-trained O
with O
the O

VGG-Face B-DAT
dataset O
[27], O
whereas O
the O

FCs B-DAT
are O
trained O
from O
scratch O

. B-DAT
Second, O
the O
training O
data O

is B-DAT
re-arranged O
by O
means O
of O

a B-DAT
sliding O
window O
with O
stride O
1 O
to O
build O
input O
sequences. O
Each O

sequence B-DAT
is O
composed O
of O
s O

= B-DAT
4 O
consecutive O
frames, O
whose O

gaze B-DAT
direction O
target O
is O
the O

gaze B-DAT
direction O
of O
the O
last O

frame B-DAT
of O
the O
sequence( O
{I(i−s+1 O

), B-DAT
. O
. O
. O
,I(i)}, O

g(i) B-DAT
) O
. O
Using O
this O
re-arranged O

training B-DAT
data, O
we O
extract O
features O

of B-DAT
each O

frame B-DAT
of O
the O
sequence O
from O

a B-DAT
frozen O
Individual O
module, O
fine-tune O

the B-DAT
Fusion O
layers, O
and O
train O

both, B-DAT
the O
Temporal O
module O
and O

a B-DAT
new O
final O
regression O
layer O

from B-DAT
scratch. O
This O
way, O
the O

network B-DAT
can O
exploit O
the O
temporal O

information B-DAT
to O
further O
refine O
the O

fusion B-DAT
weights. O
We O
trained O
the O
model O
using O

ADAM B-DAT
optimizer O
with O
an O
initial O

learning B-DAT
rate O
of O
0.0001, O
dropout O

of B-DAT
0.3, O
and O
batch O
size O

of B-DAT
64 O
frames. O
The O
number O

of B-DAT
epochs O
was O
experimentally O
set O

to B-DAT
21 O
for O
the O
first O

training B-DAT
stage O
and O
10 O
for O

the B-DAT
second. O
We O
use O
the O

average B-DAT
Euclidean O
distance O
between O
the O

predicted B-DAT
and O
ground-truth O
3D O
gaze O

vectors B-DAT
as O
loss O
function O

. B-DAT
3.4.2 O
Input O
pre-processing O

For B-DAT
this O
work O
we O
use O

head B-DAT
pose O
and O
eye O
locations O

in B-DAT
the O
3D O
scene O
provided O

by B-DAT
the O
dataset. O
The O
3D O

landmarks B-DAT
are O
extracted O
using O
the O

state-of-the-art B-DAT
method O
of O
Bulat O
and O

Tzimiropou- B-DAT
los O
[3], O
which O
is O

based B-DAT
on O
stacked O
hourglass O

networks B-DAT
[24]. O
During O
training, O
the O
original O
image O

is B-DAT
pre-processed O
to O
get O
the O

two B-DAT
normalized O
input O
images. O
The O

normalized B-DAT
whole-face O
patch O
is O
centered O

0.1 B-DAT
meters O
ahead O
of O
the O

head B-DAT
center O
in O
the O
head O

coordinate B-DAT
system, O
and O
Cn O
is O

defined B-DAT
such O
that O
the O
image O

has B-DAT
size O
of O
250× O
250 O

pixels. B-DAT
The O
difference O
between O
this O

size B-DAT
and O
the O
final O
input O

size B-DAT
allows O
us O
to O
perform O

random B-DAT
cropping O
and O
zooming O
to O

augment B-DAT
the O
data O
(explained O
in O

Section B-DAT
4.1). O
Similarly, O
each O
normalized O

eye B-DAT
patch O
is O
centered O
in O

their B-DAT
respective O
eye O
center O
locations O

. B-DAT
In O
this O
case, O
the O

virtual B-DAT
camera O
matrix O
is O
defined O

so B-DAT
that O
the O
image O
is O

cropped B-DAT
to O
70×58, O
while O
in O

practice B-DAT
the O
final O
patches O
have O

size B-DAT
of O
60×48. O
Landmarks O
are O

normalized B-DAT
using O
the O
same O
procedure O

and B-DAT
further O
pre-processed O
with O
mean O

subtraction B-DAT
and O
min-max O
normalization O
per O

axis. B-DAT
Finally, O
we O
divide O
them O

by B-DAT
a O
scaling O
factor O
w O

such B-DAT
that O
all O
coordinates O
are O

in B-DAT
the O
range O
[0,w]. O
This O

way, B-DAT
all O
concatenated O
feature O
values O

are B-DAT
in O
a O
similar O
range. O

After B-DAT
inference, O
the O
predicted O
normalized O

2D B-DAT
angles O
are O
de-normalized O
back O

to B-DAT
the O
original O
3D O
space. O
4 O
Experiments O
In O
this O
section O

, B-DAT
we O
evaluate O
the O
cross-subject O

3D B-DAT
gaze O
estimation O
task O
on O

a B-DAT
wide O
range O
of O
head O

poses B-DAT
and O
gaze O
directions. O
Furthermore, O

we B-DAT
validate O
the O
effectiveness O
of O

the B-DAT
proposed O
architecture O
comparing O
both O

static B-DAT
and O
temporal O
approaches. O
We O

report B-DAT
the O
error O
in O
terms O

of B-DAT
mean O
angular O
error O
between O

predicted B-DAT
and O
ground-truth O
3D O
gaze O

vectors. B-DAT
Note O
that O
due O
to O

the B-DAT
requirements O
of O
the O
temporal O

model B-DAT
not O
all O
the O
frames O

obtain B-DAT
a O
prediction. O
Therefore, O
for O

a B-DAT
Citation O
Citation O
{Parkhi, O
Vedaldi, O
and O

Zisserman} B-DAT
2015 O

Citation B-DAT
Citation O
{Bulat O
and O
Tzimiropoulos} O
2017 O

Citation B-DAT
Citation O
{Newell, O
Yang, O
and O

Deng} B-DAT
2016 O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
7 O

60 B-DAT
30 O
0 O
30 O
60 O
60 O

30 B-DAT
0 O

30 B-DAT
60 O

100 B-DAT
101 O

102 B-DAT
60 O
30 O
0 O
30 O
60 O

60 B-DAT
30 O

0 B-DAT
30 O

60 B-DAT
100 O

101 B-DAT
102 O

103 B-DAT
60 O
30 O
0 O
30 O
60 O

60 B-DAT
30 O

0 B-DAT
30 O

60 B-DAT
100 O

101 B-DAT
102 O

60 B-DAT
30 O
0 O
30 O
60 O
60 O

30 B-DAT
0 O

30 B-DAT
60 O

100 B-DAT
101 O

102 B-DAT
103 O

a) B-DAT
g O
(FT O
) O
(b) O

h B-DAT
(FT O
) O
(c) O
g O
( O

CS) B-DAT
(d) O
h O
(CS) O
Figure O
2: O
Ground-truth O
eye O
gaze O

g B-DAT
and O
head O
orientation O
h O

distribution B-DAT
on O
the O
filtered O
EYE O

- B-DAT
DIAP O
dataset O
for O
CS O

and B-DAT
FT O
settings, O
in O
terms O

of B-DAT
x- O
and O
y- O
angles. O
fair O
comparison, O
the O
reported O
results O

for B-DAT
static O
models O
disregard O
such O

frames B-DAT
when O
temporal O
models O
are O

included B-DAT
in O
the O
comparison O

. B-DAT
4.1 O
Training O
data O

There B-DAT
are O
few O
publicly O
available O

datasets B-DAT
devoted O
to O
3D O
gaze O

estimation B-DAT
and O
most O
of O
them O

focus B-DAT
on O
HCI O
with O
a O

limited B-DAT
range O
of O
head O
pose O

and B-DAT
gaze O
directions. O
Therefore, O
we O

use B-DAT
VGA O
videos O
from O
the O

publicly-available B-DAT
EYEDIAP O
dataset O
[7] O
to O

perform B-DAT
the O
experimental O
evaluation, O
as O

it B-DAT
is O
currently O
the O
only O

one B-DAT
containing O
video O
sequences O
with O

a B-DAT
wide O
range O
of O
head O

poses B-DAT
and O
showing O
the O
full O

face. B-DAT
This O
dataset O
consists O
of O
3 O

-minute B-DAT
videos O
of O
16 O
subjects O

looking B-DAT
at O
two O
types O
of O

targets: B-DAT
continuous O
screen O
targets O
on O

a B-DAT
fixed O
monitor O
(CS), O
and O

floating B-DAT
physical O
targets O
(FT O
). O

The B-DAT
videos O
are O
further O
divided O

into B-DAT
static O
(S) O
and O
moving O
( O

M) B-DAT
head O
pose O
for O
each O

of B-DAT
the O
subjects. O
Subjects O
12-16 O

were B-DAT
recorded O
with O
2 O
different O

lighting B-DAT
conditions. O
For O
evaluation, O
we O
filtered O
out O

those B-DAT
frames O
that O
fulfilled O
at O

least B-DAT
one O
of O
the O
following O

conditions: B-DAT
(1) O
face O
or O
landmarks O

not B-DAT
detected; O
(2) O
subject O
not O

looking B-DAT
at O
the O
target; O
(3 O

) B-DAT
3D O
head O
pose, O
eyes O

or B-DAT
target O
location O
not O
properly O

recovered; B-DAT
and O
(4) O
eyeball O
rotations O

violating B-DAT
physical O

constraints B-DAT
(|θ O
| O
≤ O
40 O

◦, B-DAT
|φ O
| O
≤ O
30 O

◦) B-DAT
[23]. O
Note O
that O
we O

purposely B-DAT
do O
not O
filter O
eye O

blinking B-DAT
moments O
to O
learn O
their O

dynamics B-DAT
with O
the O
temporal O
model, O

which B-DAT
may O
produce O
some O
outliers O

with B-DAT
a O
higher O
prediction O
error O

due B-DAT
to O
a O
less O
accurate O

ground B-DAT
truth. O
Figure O
2 O
shows O

the B-DAT
distribution O
of O
gaze O
directions O

and B-DAT
head O
poses O
for O
both O

filtered B-DAT
CS O
and O
FT O
cases. O
We O
applied O
data O
augmentation O
to O

the B-DAT
training O
set O
with O
the O

following B-DAT
random O
transforma- O
tions: O
horizontal O

flip, B-DAT
shifts O
of O
up O
to O

5 B-DAT
pixels, O
zoom O
of O
up O

to B-DAT
2%, O
brightness O
changes O
by O

a B-DAT
factor O
in O
the O
range O

[0.4,1.75], B-DAT
and O
additive O
Gaussian O
noise O

with B-DAT
σ2 O
= O
0.03 O

. B-DAT
4.2 O
Evaluation O
of O
static O
modalities O

First, B-DAT
we O
evaluate O
the O
contribution O

of B-DAT
each O
static O
modality O
on O

the B-DAT
FT O
scenario. O
We O
divided O

the B-DAT
16 O
participants O
into O
4 O

groups, B-DAT
such O
that O
appearance O
variability O

was B-DAT
maximized O
while O
maintaining O
a O

similar B-DAT
number O
of O
training O
samples O

per B-DAT
group. O
Each O
static O
model O

was B-DAT
trained O
end-to-end O
performing O
4-fold O

cross-validation B-DAT
using O
different O
combinations O
of O

input B-DAT
modal- O
ities. O
Since O
the O

number B-DAT
of O
fusion O
units O
depends O

on B-DAT
the O
number O
of O
input O

modalities, B-DAT
we O
also O
compare O
different O

fusion B-DAT
layer O
sizes. O
The O
effect O

of B-DAT
data O
normalization O
is O
also O

evaluated B-DAT
by O
training O
a O
not-normalized O

face B-DAT
model O
where O
the O
input O

image B-DAT
is O
the O
face O
bounding O

box B-DAT
with O
square O
size O
the O

maximum B-DAT
distance O
between O
2D O
landmarks. O
Citation O
Citation O
{Funesprotect O
unhbox O
voidb@x O

penalty B-DAT
@M O

Mora, B-DAT
Monay, O
and O
Odobez} O
2014{} O
Citation O
Citation O
{MSC O

8 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
0 O
1 O
2 O
3 O
4 O

5 B-DAT
6 O
7 O
8 O
9 O

10 B-DAT
11 O
An O
gl O

e B-DAT
er O

ro B-DAT
r O
( O
de O
gr O

ee B-DAT
s) O
6.9 O
6.43 O
5.58 O
5.71 O
5.59 O

5.55 B-DAT
5.52 O

OF-4096 B-DAT
NE-1536 O
NF-4096 O
NF-5632 O
NFL-4300 O

NFE-5632 B-DAT
NFEL-5836 O
Figure O
3: O
Performance O
evaluation O
of O

the B-DAT
Static O
network O
using O
different O

input B-DAT
modali- O
ties O
(O O

- B-DAT
Not O
normalized, O
N O

- B-DAT
Normalized, O
F O
- O
Face O

, B-DAT
E O
- O
Eyes, O

L B-DAT
- O
3D O
Landmarks) O
and O

size B-DAT
of O
fusion O
layers O
on O

the B-DAT
FT O
scenario. O
Floating O
Target O
Screen O
Target O
0 O

1 B-DAT
2 O
3 O
4 O
5 O

6 B-DAT
7 O
8 O
9 O

10 B-DAT
11 O
An O
gl O

e B-DAT
er O

ro B-DAT
r O
( O
de O
gr O

ee B-DAT
s) O
6.36 O
5.43 O
5.19 O
4.2 O
3.38 O

3.4 B-DAT

MPIIGaze B-DAT
Static O
Temporal O
Figure O
4: O
Performance O
comparison O
among O

MPIIGaze B-DAT
method O
[42] O
and O
our O

Static B-DAT
and O
Temporal O
versions O
of O

the B-DAT
proposed O
network O
for O
FT O

and B-DAT
CS O
scenarios O

. B-DAT
As O
shown O
in O
Figure O
3 O

, B-DAT
all O
models O
that O
take O

normalized B-DAT
full-face O
information O
as O
input O

achieve B-DAT
better O
performance O
than O
the O

eyes-only B-DAT
model. O
More O
specifically, O
the O

combination B-DAT
of O
face, O
eyes O
and O

landmarks B-DAT
outperforms O
all O
the O
other O

combinations B-DAT
by O
a O
small O
but O

significant B-DAT
margin O
(paired O
Wilcoxon O
test, O

p B-DAT
< O
0.0001). O
The O
standard O

deviation B-DAT
of O
the O
best-performing O
model O

is B-DAT
reduced O
compared O
to O
the O

face B-DAT
and O
eyes O
model, O
suggesting O

a B-DAT
regularizing O
effect O
due O
to O

the B-DAT
addition O
of O
landmarks. O
The O

not-normalized B-DAT
face-only O
model O
shows O
the O

largest B-DAT
error, O
proving O
the O
impact O

of B-DAT
normalization O
to O
reduce O
the O

appearance B-DAT
variability. O
Furthermore, O
our O
results O

indicate B-DAT
that O
the O
increase O
of O

fusion B-DAT
units O
is O
not O
correlated O

with B-DAT
a O
better O
performance. O
4.3 O
Static O
gaze O
regression: O
comparison O

with B-DAT
existing O
methods O

We B-DAT
compare O
our O
best-performing O
static O

model B-DAT
with O
three O
baselines. O
Head: O

Treating B-DAT
the O
head O
pose O
directly O

as B-DAT
gaze O
direction. O
PR-ALR: O
Method O

that B-DAT
relies O
on O
RGB-D O
data O

to B-DAT
rectify O
the O
eye O
images O

viewpoint B-DAT
into O
a O
canonical O
head O

pose B-DAT
using O
a O
3DMM. O
It O

then B-DAT
learns O
an O
RGB O
gaze O

appearance B-DAT
model O
using O
ALR O
[21]. O

Predicted B-DAT
3D O
vectors O
for O
FT-S O

scenario B-DAT
are O
provided O
by O
EYEDIAP O

dataset. B-DAT
MPIIGaze:. O
State-of-the-art O
full-face O
3D O

gaze B-DAT
estimation O
method O
[42]. O
They O

use B-DAT
an O
Alexnet-based O
CNN O
model O

with B-DAT
spatial O
weights O
to O
enhance O

information B-DAT
in O
different O
facial O
regions. O

We B-DAT
fine-tuned O
it O
with O
the O

filtered B-DAT
EYEDIAP O
subsets O
using O
our O

training B-DAT
parameters O
and O
normalization O
procedure. O
In O
addition O
to O
the O
aforementioned O

FT-based B-DAT
evaluation O
setup, O
we O
also O

evaluate B-DAT
our O
method O
on O
the O

CS B-DAT
scenario. O
In O
this O
case O

there B-DAT
are O
only O
14 O
participants O

available, B-DAT
so O
we O
divided O
them O

in B-DAT
5 O
groups O
and O
performed O

5-fold B-DAT
cross-validation. O
In O
Figure O
4 O

we B-DAT
compare O
our O
method O
to O

MPIIGaze, B-DAT
achieving O
a O
statistically O
significant O

improvement B-DAT
of O
14.6% O
and O
19.5 O

% B-DAT
on O
FT O
and O
CS O

scenarios, B-DAT
respectively O
(paired O
Wilcoxon O
test, O

p B-DAT
< O
0.0001). O
We O
can O

observe B-DAT
that O
a O
re- O
stricted O

gaze B-DAT
target O
benefits O
the O
performance O

of B-DAT
all O
methods, O
compared O
to O

a B-DAT
more O
challenging O
unrestricted O
setting O

with B-DAT
a O
wider O
range O
of O

head B-DAT
poses O
and O
gaze O
directions. O
Table O
2 O
provides O
a O
detailed O

comparison B-DAT
on O
every O
participant, O
performing O

leave-one-out B-DAT
cross-validation O
on O
the O
FT O

scenario B-DAT
for O
static O
and O
moving O

head B-DAT
separately. O
Results O
show O
that O

, B-DAT
as O
expected, O
facial O
appearance O

and B-DAT
head O
pose O
have O
a O

noticeable B-DAT
impact O
on O
gaze O
accuracy, O

with B-DAT
average O
error O
differences O
of O

up B-DAT
to O
7.7◦ O
among O
participants. O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2015 O
Citation O
Citation O
{Mora O
and O
Odobez O

} B-DAT
2012 O
Citation O
Citation O
{Zhang, O
Sugano, O
Fritz O

, B-DAT
and O
Bulling} O
2015 O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
9 O

Method B-DAT
1 O
2 O
3 O
4 O
5 O
6 O
7 O
8 O
9 O
10 O

11 B-DAT
12 O
13 O
14 O
15 O

16 B-DAT
Avg. O
Head O
23.5 O
22.1 O

20.3 B-DAT
23.6 O
23.2 O
23.2 O
23.6 O

21.2 B-DAT
26.7 O
23.6 O
23.1 O
24.4 O

23.3 B-DAT
24.0 O
24.5 O
22.8 O
23.3 O

PR-ALR B-DAT
12.3 O
12.0 O
12.4 O
11.3 O

15.5 B-DAT
12.9 O
17.9 O
11.8 O
17.3 O

13.4 B-DAT
13.4 O
14.3 O
15.2 O
13.6 O

14.4 B-DAT
14.6 O
13.9 O
MPIIGaze O
5.3 O

5.1 B-DAT
5.7 O
4.7 O
7.3 O
15.1 O

10.8 B-DAT
5.7 O
9.9 O
7.1 O
5.0 O

5.7 B-DAT
7.4 O
3.8 O
4.8 O
5.5 O

6.8 B-DAT
Static O
3.9 O
4.1 O
4.2 O

3.9 B-DAT
6.0 O
6.4 O
7.2 O
3.6 O

7.1 B-DAT
5.0 O
5.7 O
6.7 O
3.9 O

4.7 B-DAT
5.1 O
4.2 O
5.1 O
Temporal O

4.0 B-DAT
4.9 O
4.3 O
4.1 O
6.1 O

6.5 B-DAT
6.6 O
3.9 O
7.8 O
6.1 O

4.7 B-DAT
5.6 O
4.7 O
3.5 O
5.9 O

4.6 B-DAT
5.2 O
Head O
19.3 O
14.2 O

16.4 B-DAT
19.9 O
16.8 O
21.9 O
16.1 O

24.2 B-DAT
20.3 O
19.9 O
18.8 O
22.3 O

18.1 B-DAT
14.9 O
16.2 O
19.3 O
18.7 O

MPIIGaze B-DAT
7.6 O
6.2 O
5.7 O
8.7 O

10.1 B-DAT
12.0 O
12.2 O
6.1 O
8.3 O

5.9 B-DAT
6.1 O
6.2 O
7.4 O
4.7 O

4.4 B-DAT
6.0 O
7.3 O
Static O
5.8 O

5.7 B-DAT
4.4 O
7.5 O
6.7 O
8.8 O

11.6 B-DAT
5.5 O
8.3 O
5.5 O
5.2 O

6.3 B-DAT
5.3 O
3.9 O
4.3 O
5.6 O

6.3 B-DAT
Temporal O
6.1 O
5.6 O
4.5 O

7.5 B-DAT
6.4 O
8.2 O
12.0 O
5.0 O

7.5 B-DAT
5.4 O
5.0 O
5.8 O
6.6 O

4.0 B-DAT
4.5 O
5.8 O
6.2 O

Table B-DAT
2: O
Gaze O
angular O
error O

comparison B-DAT
for O
static O
(top O
half) O

and B-DAT
moving O
(bottom O
half) O
head O

pose B-DAT
for O
each O
subject O
in O

the B-DAT
FT O
scenario. O
Best O
results O

in B-DAT
bold. O
−80 O
−40 O
0 O
40 O
80−80 O

40 B-DAT
0 O

40 B-DAT
80 O

0 B-DAT
5 O

10 B-DAT
15 O

20 B-DAT
25 O

30 B-DAT
35 O

80 B-DAT
−40 O
0 O
40 O
80−80 O
−40 O

0 B-DAT
40 O

80 B-DAT
−10 O

8 B-DAT
−6 O

4 B-DAT
−2 O

0 B-DAT
2 O

4 B-DAT
6 O

8 B-DAT
10 O

80 B-DAT
−40 O
0 O
40 O
80−80 O
−40 O

0 B-DAT
40 O

80 B-DAT
0 O

5 B-DAT
10 O

15 B-DAT
20 O

25 B-DAT
30 O

35 B-DAT
−80 O
−40 O
0 O
40 O
80−80 O

40 B-DAT
0 O

40 B-DAT
80 O

10 B-DAT
−8 O

6 B-DAT
−4 O

2 B-DAT
0 O

2 B-DAT
4 O

6 B-DAT
8 O

10 B-DAT
(a) O
Gaze O
space O
(b) O
Head O

orientation B-DAT
space O

Figure B-DAT
5: O
Angular O
error O
distribution O

across B-DAT
gaze O
(a) O
and O
head O

orientation B-DAT
(b) O
spaces O
in O
the O

FT B-DAT
setting, O
in O
terms O
of O

x- B-DAT
and O
y- O
angles. O
For O

each B-DAT
space, O
we O
depict O
the O

Static B-DAT
model O
performance O
(left) O
and O

the B-DAT
contribution O
of O
the O
Temporal O

model B-DAT
versus O
Static O
(right). O
In O

the B-DAT
latter, O
positive O
difference O
means O

higher B-DAT
improvement O
of O
the O
Temporal O

model. B-DAT
4.4 O
Evaluation O
of O
the O
temporal O

network B-DAT

In B-DAT
this O
section, O
we O
evaluate O

the B-DAT
contribution O
of O
adding O
the O

temporal B-DAT
module O
to O
the O
static O

model. B-DAT
To O
do O
so, O
we O

trained B-DAT
a O
lower-dimensional O
version O
of O

the B-DAT
static O
network O
with O
compa- O

rable B-DAT
performance O
to O
the O
original, O

reducing B-DAT
the O
number O
of O
units O

of B-DAT
the O
second O
fusion O
layer O

to B-DAT
2918. O
Results O
are O
reported O

in B-DAT
Figure O
4 O
and O
Table O
2 O

. B-DAT
One O
can O
observe O
that O

using B-DAT
sequential O
information O
is O
helpful O

on B-DAT
the O
FT O
scenario, O
outperforming O

the B-DAT
static O
model O
by O
a O

statistically B-DAT
significant O
4.4% O
(paired O
Wilcoxon O

test, B-DAT
p O
< O
0.0001). O
This O

contribution B-DAT
is O
more O
noticeable O
in O

the B-DAT
moving O
head O
setting, O
proving O

that B-DAT
the O
temporal O
model O
can O

benefit B-DAT
from O
head O
motion O
information. O

In B-DAT
contrast, O
such O
information O
seems O

to B-DAT
be O
less O
meaningful O
in O

the B-DAT
CS O
scenario, O
where O
the O

obtained B-DAT
error O
is O
already O
very O

low B-DAT
for O
a O
cross-subject O
setting O

and B-DAT
the O
amount O
of O
head O

movement B-DAT
declines. O
Figure O
5 O
further O
explores O
the O

error B-DAT
distribution O
of O
the O
static O

network B-DAT
and O
the O
impact O
of O

sequential B-DAT
information. O
We O
can O
observe O

that B-DAT
the O
accuracy O
of O
the O

static B-DAT
model O
drops O
with O
extreme O

head B-DAT
poses O
and O
gaze O
directions O

, B-DAT
which O
can O
also O
be O

correlated B-DAT
to O
having O
less O
data O

in B-DAT
those O
areas. O
Compared O
to O

the B-DAT
static O
model, O
the O
temporal O

model B-DAT
particularly O
benefits O
gaze O
targets O

from B-DAT
mid-range O
upwards. O
Its O
contribution O

is B-DAT
less O
clear O
for O
extreme O

targets, B-DAT
probably O
again O
due O
to O

data B-DAT
imbalance. O
Finally, O
we O
evaluated O
the O
effect O

of B-DAT
different O
recurrent O
architectures O
for O

the B-DAT
temporal O
model. O
In O
particular O

, B-DAT
we O
tested O
1 O
(128 O

units) B-DAT
and O
2 O
(256-128 O
units) O

LSTM B-DAT
and O
GRU O
lay- O
ers, O

with B-DAT
1 O
GRU O
layer O
obtaining O

slightly B-DAT
superior O
results O
(up O
to O
0 O

.12◦). B-DAT
We O
also O
assessed O
the O

effect B-DAT
of O
sequence O
length O
fixing O

s B-DAT
in O
the O
range O
{4,7,10}, O

with B-DAT
s O
= O
7 O
performing O

worse B-DAT
than O
the O
other O
two O
( O

up B-DAT
to O
0 O

14 B-DAT

10 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
5 O
Conclusions O
In O
this O
work O

, B-DAT
we O
studied O
the O
combination O

of B-DAT
full-face O
and O
eye O
images O

along B-DAT
with O
facial O
land- O
marks O

for B-DAT
person- O
and O
head O
pose-independent O

3D B-DAT
gaze O
estimation. O
Consequently, O
we O

pro- B-DAT
posed O
a O
multi-stream O
recurrent O

CNN B-DAT
network O
that O
leverages O
the O

sequential B-DAT
information O
of O
eye O
and O

head B-DAT
movements. O
Both O
static O
and O

temporal B-DAT
versions O
of O
our O
approach O

significantly B-DAT
outperform O
current O
state-of-the-art O
3D O

gaze B-DAT
estimation O
methods O
on O
a O

wide B-DAT
range O
of O
head O
poses O

and B-DAT
gaze O
directions. O
We O
showed O

that B-DAT
adding O
geometry O
features O
to O

appearance-based B-DAT
methods O
has O
a O
regularizing O

effect B-DAT
on O
the O
accuracy. O
Adding O

sequential B-DAT
information O
further O
benefits O
the O

final B-DAT
performance O
compared O
to O
static-only O

input, B-DAT
especially O
from O
mid-range O
up- O

wards B-DAT
and O
in O
those O
cases O

where B-DAT
head O
motion O
is O
present. O

The B-DAT
effect O
in O
very O
extreme O

head B-DAT
poses O
is O
not O
clear O

due B-DAT
to O
data O
imbalance, O
suggesting O

the B-DAT
importance O
of O
learning O
from O

a B-DAT
con- O
tinuous, O
balanced O
dataset O

including B-DAT
all O
head O
poses O
and O

gaze B-DAT
directions O
of O
interest. O
To O

the B-DAT
best O
of O
our O
knowledge, O

this B-DAT
is O
the O
first O
attempt O

to B-DAT
exploit O
the O
temporal O
modality O

in B-DAT
the O
context O
of O
gaze O

estimation B-DAT
from O
remote O
cameras. O
As O

future B-DAT
work, O
we O
will O
further O

explore B-DAT
extracting O
meaningful O
temporal O
representations O

of B-DAT
gaze O
dynamics, O
considering O
3DCNNs O

as B-DAT
well O
as O
the O
encoding O

of B-DAT
deep O
features O
around O
particular O

tracked B-DAT
face O
landmarks O
[14]. O
Acknowledgements O
This O
work O
has O
been O

partially B-DAT
supported O
by O
the O
Spanish O

project B-DAT
TIN2016-74946-P O
(MINECO/ O
FEDER, O
UE O

), B-DAT
CERCA O
Programme O
/ O
Generalitat O

de B-DAT
Catalunya, O
and O
the O
FP7 O

people B-DAT
program O
(Marie O
Curie O
Actions), O

REA B-DAT
grant O
agreement O
no O
FP7-607139 O
( O

iCARE B-DAT
- O
Improving O
Children O
Auditory O

REhabilitation). B-DAT
We O
gratefully O
acknowledge O
the O

support B-DAT
of O
NVIDIA O
Corporation O
with O

the B-DAT
donation O
of O
the O
GPU O

used B-DAT
for O
this O
research. O
Portions O

of B-DAT
the O
research O
in O
this O

pa- B-DAT
per O
used O
the O
EYEDIAP O

dataset B-DAT
made O
available O
by O
the O

Idiap B-DAT
Research O
Institute, O
Martigny, O
Switzerland. O
References O
[1] O
Nicola O
C O
Anderson O

, B-DAT
Evan O
F O
Risko, O
and O

Alan B-DAT
Kingstone. O
Motion O
influences O
gaze O

di- B-DAT
rection O
discrimination O
and O
disambiguates O
contradictory O

luminance B-DAT
cues. O
Psychonomic O
bulletin O

& B-DAT
review, O
23(3):817–823, O
2016 O

. B-DAT
[2] O
Shumeet O
Baluja O
and O
Dean O

Pomerleau. B-DAT
Non-intrusive O
gaze O
tracking O
using O

artificial B-DAT
neu- O
ral O
networks. O
In O

Advances B-DAT
in O
Neural O
Information O
Processing O

Systems, B-DAT
pages O
753–760, O
1994 O

. B-DAT
[3] O
Adrian O
Bulat O
and O
Georgios O

Tzimiropoulos. B-DAT
How O
far O
are O
we O

from B-DAT
solving O
the O
2d O

& B-DAT
3d O
face O
alignment O
problem O

? B-DAT
(and O
a O
dataset O
of O
230, O

000 B-DAT
3d O
facial O
landmarks). O
In O

Interna- B-DAT
tional O
Conference O
on O
Computer O

Vision, B-DAT
2017. O
[4] O
Haoping O
Deng O
and O
Wangjiang O

Zhu. B-DAT
Monocular O
free-head O
3d O
gaze O

tracking B-DAT
with O
deep O
learning O
and O

geometry B-DAT
constraints. O
In O
Computer O
Vision O

(ICCV), B-DAT
2017 O
IEEE O
Interna- O
tional O

Conference B-DAT
on, O
pages O
3162–3171. O
IEEE O

, B-DAT
2017. O
[5] O
Onur O
Ferhat O
and O
Fernando O

Vilariño. B-DAT
Low O
cost O
eye O
tracking O

. B-DAT
Computational O
intelligence O
and O
neuroscience, O
2016 O

:17, B-DAT
2016. O
Citation O
Citation O
{Jung, O
Lee, O
Yim O

, B-DAT
Park, O
and O
Kim} O
2015 O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
11 O

6] B-DAT
Kenneth O
A O
Funes-Mora O
and O

Jean-Marc B-DAT
Odobez. O
Gaze O
estimation O
in O

the B-DAT
3D O
space O
using O
RGB-D O

sensors. B-DAT
International O
Journal O
of O
Computer O

Vision, B-DAT
118(2):194–216, O
2016. O
[7] O
Kenneth O
Alberto O
Funes O
Mora O

, B-DAT
Florent O
Monay, O
and O
Jean-Marc O

Odobez. B-DAT
Eyediap: O
A O
database O
for O

the B-DAT
development O
and O
evaluation O
of O

gaze B-DAT
estimation O
algorithms O
from O
rgb O

and B-DAT
rgb-d O
cameras. O
In O
Proceedings O

of B-DAT
the O
ACM O
Symposium O
on O

Eye B-DAT
Tracking O
Research O
and O
Applications. O

ACM, B-DAT
March O
2014. O
doi: O
10.1145/2578153.2578190. O
[8] O
Kenneth O
Alberto O
Funes O
Mora O

, B-DAT
Florent O
Monay, O
and O
Jean-Marc O

Odobez. B-DAT
Eyediap: O
A O
database O
for O

the B-DAT
development O
and O
evaluation O
of O

gaze B-DAT
estimation O
algorithms O
from O
rgb O

and B-DAT
rgb-d O
cameras. O
In O
Proceedings O

of B-DAT
the O
Symposium O
on O
Eye O

Tracking B-DAT
Research O
and O
Applications, O
pages O
255 O

–258. B-DAT
ACM, O
2014. O
[9] O
Quentin O
Guillon, O
Nouchine O
Hadjikhani O

, B-DAT
Sophie O
Baduel, O
and O
Bernadette O

Rogé. B-DAT
Visual O
social O
attention O
in O

autism B-DAT
spectrum O
disorder: O
Insights O
from O

eye B-DAT
tracking O
studies. O
Neu- O

roscience B-DAT
& O
Biobehavioral O
Reviews, O
42:279–297, O
2014 O

. B-DAT
[10] O
Dan O
Witzner O
Hansen O
and O

Qiang B-DAT
Ji. O
In O
the O
eye O

of B-DAT
the O
beholder: O
A O
survey O

of B-DAT
models O
for O
eyes O
and O

gaze. B-DAT
IEEE O
transactions O
on O
pattern O

analysis B-DAT
and O
machine O
intelligence, O
32(3 O

): B-DAT
478–500, O
2010. O
[11] O
Qiong O
Huang, O
Ashok O
Veeraraghavan O

, B-DAT
and O
Ashutosh O
Sabharwal. O
Tabletgaze: O

dataset B-DAT
and O
analysis O
for O
unconstrained O

appearance-based B-DAT
gaze O
estimation O
in O
mobile O

tablets. B-DAT
Machine O
Vision O
and O
Applications, O
28 O

(5-6):445–461, B-DAT
2017. O
[12] O
Robert O
JK O
Jacob O
and O

Keith B-DAT
S O
Karn. O
Eye O
tracking O

in B-DAT
human-computer O
interaction O
and O
usability O

research: B-DAT
Ready O
to O
deliver O
the O

promises. B-DAT
In O
The O
mind’s O
eye O

, B-DAT
pages O
573–605. O
Elsevier, O
2003. O
[13] O
László O
A O
Jeni O
and O

Jeffrey B-DAT
F O
Cohn. O
Person-independent O
3d O

gaze B-DAT
estimation O
using O
face O
frontalization O

. B-DAT
In O
Proceedings O
of O
the O

IEEE B-DAT
Conference O
on O
Computer O
Vision O

and B-DAT
Pattern O
Recognition O
Workshops, O
pages O
87 O

–95, B-DAT
2016. O
[14] O
Heechul O
Jung, O
Sihaeng O
Lee O

, B-DAT
Junho O
Yim, O
Sunjeong O
Park, O

and B-DAT
Junmo O
Kim. O
Joint O
fine- O

tuning B-DAT
in O
deep O
neural O
networks O

for B-DAT
facial O
expression O
recognition. O
In O

Computer B-DAT
Vision O
(ICCV), O
2015 O
IEEE O

International B-DAT
Conference O
on, O
pages O
2983–2991. O

IEEE, B-DAT
2015. O
[15] O
Anuradha O
Kar O
and O
Peter O

Corcoran. B-DAT
A O
review O
and O
analysis O

of B-DAT
eye-gaze O
estimation O
sys- O
tems O

, B-DAT
algorithms O
and O
performance O
evaluation O

methods B-DAT
in O
consumer O
platforms. O
IEEE O

Access, B-DAT
5:16495–16519, O
2017. O
[16] O
Kyle O
Krafka, O
Aditya O
Khosla O

, B-DAT
Petr O
Kellnhofer, O
Harini O
Kannan, O

Suchendra B-DAT
Bhandarkar, O
Wojciech O
Matusik, O
and O

Antonio B-DAT
Torralba. O
Eye O
tracking O
for O

everyone. B-DAT
In O
Computer O
Vision O
and O

Pattern B-DAT
Recognition O
(CVPR), O
2016 O
IEEE O

Conference B-DAT
on, O
pages O
2176–2184. O
IEEE, O
2016 O

. B-DAT
[17] O
Simon O
P O
Liversedge O
and O

John B-DAT
M O
Findlay. O
Saccadic O
eye O

movements B-DAT
and O
cognition. O
Trends O
in O

cognitive B-DAT
sciences, O
4(1):6–14, O
2000 O

. B-DAT
[18] O
Feng O
Lu, O
Takahiro O
Okabe O

, B-DAT
Yusuke O
Sugano, O
and O
Yoichi O

Sato. B-DAT
A O
head O
pose-free O
approach O

for B-DAT
appearance-based O
gaze O
estimation. O
In O

BMVC, B-DAT
pages O
1–11, O
2011 O

12 B-DAT
PALMERO O
ET O
AL.: O
MULTI-MODAL O

RECURRENT B-DAT
CNN O
FOR O
3D O
GAZE O

ESTIMATION B-DAT
[19] O
Feng O
Lu, O
Yusuke O
Sugano O

, B-DAT
Takahiro O
Okabe, O
and O
Yoichi O

Sato. B-DAT
Inferring O
human O
gaze O
from O

appearance B-DAT
via O
adaptive O
linear O
regression. O

In B-DAT
Computer O
Vision O
(ICCV), O
2011 O

IEEE B-DAT
International O
Conference O
on, O
pages O
153 O

–160. B-DAT
IEEE, O
2011. O
[20] O
Päivi O
Majaranta O
and O
Andreas O

Bulling. B-DAT
Eye O
tracking O
and O
eye-based O

human–computer B-DAT
interaction. O
In O
Advances O
in O

physiological B-DAT
computing, O
pages O
39–65. O
Springer O

, B-DAT
2014. O
[21] O
Kenneth O
Alberto O
Funes O
Mora O

and B-DAT
Jean-Marc O
Odobez. O
Gaze O
estimation O

from B-DAT
multi- O
modal O
kinect O
data O

. B-DAT
In O
Computer O
Vision O
and O

Pattern B-DAT
Recognition O
Workshops O
(CVPRW), O
2012 O

IEEE B-DAT
Computer O
Society O
Conference O
on, O

pages B-DAT
25–30. O
IEEE, O
2012. O
[22] O
Carlos O
Hitoshi O
Morimoto, O
Arnon O

Amir, B-DAT
and O
Myron O
Flickner. O
Detecting O

eye B-DAT
position O
and O
gaze O
from O

a B-DAT
single O
camera O
and O
2 O

light B-DAT
sources. O
In O
Pattern O
Recognition O

, B-DAT
2002. O
Proceedings. O
16th O
International O

Conference B-DAT
on, O
volume O
4, O
pages O
314 O

–317. B-DAT
IEEE, O
2002. O
[23] O
IMO O
MSC. O
Circ. O
982 O

(2000) B-DAT
guidelines O
on O
ergonomic O
criteria O

for B-DAT
bridge O
equipment O
and O
layout O

. B-DAT
[24] O
Alejandro O
Newell, O
Kaiyu O
Yang O

, B-DAT
and O
Jia O
Deng. O
Stacked O

hourglass B-DAT
networks O
for O
hu- O
man O

pose B-DAT
estimation. O
In O
European O
Conference O

on B-DAT
Computer O
Vision, O
pages O
483–499. O

Springer, B-DAT
2016. O
[25] O
Yasuhiro O
Ono, O
Takahiro O
Okabe O

, B-DAT
and O
Yoichi O
Sato. O
Gaze O

estimation B-DAT
from O
low O
resolution O
images. O

In B-DAT
Pacific-Rim O
Symposium O
on O
Image O

and B-DAT
Video O
Technology, O
pages O
178–188. O

Springer, B-DAT
2006. O
[26] O
Cristina O
Palmero, O
Elisabeth O
A O

. B-DAT
van O
Dam, O
Sergio O
Escalera, O

Mike B-DAT
Kelia, O
Guido O
F. O
Lichtert, O

Lucas B-DAT
P.J.J O
Noldus, O
Andrew O
J. O

Spink, B-DAT
and O
Astrid O
van O
Wieringen. O

Automatic B-DAT
mutual O
gaze O
detection O
in O

face-to-face B-DAT
dyadic O
interaction O
videos. O
In O

Proceedings B-DAT
of O
Measuring O
Behavior, O
pages O
158 O

–163, B-DAT
2018. O
[27] O
Omkar O
M. O
Parkhi, O
Andrea O

Vedaldi, B-DAT
and O
Andrew O
Zisserman. O
Deep O

face B-DAT
recognition. O
In O
British O
Machine O

Vision B-DAT
Conference, O
2015 O

. B-DAT
[28] O
Derek O
R O
Rutter O
and O

Kevin B-DAT
Durkin. O
Turn-taking O
in O
mother–infant O

interaction: B-DAT
An O
exam- O
ination O
of O

vocalizations B-DAT
and O
gaze. O
Developmental O
psychology O

, B-DAT
23(1):54, O
1987. O
[29] O
Brian O
A O
Smith, O
Qi O

Yin, B-DAT
Steven O
K O
Feiner, O
and O

Shree B-DAT
K O
Nayar. O
Gaze O
locking O

: B-DAT
passive O
eye O
contact O
detection O

for B-DAT
human-object O
interaction. O
In O
Proceedings O

of B-DAT
the O
26th O
annual O
ACM O

symposium B-DAT
on O
User O
interface O
software O

and B-DAT
technology, O
pages O
271–280. O
ACM, O
2013 O

. B-DAT
[30] O
Yusuke O
Sugano, O
Yasuyuki O
Matsushita O

, B-DAT
and O
Yoichi O
Sato. O
Appearance-based O

gaze B-DAT
es- O
timation O
using O
visual O

saliency. B-DAT
IEEE O
transactions O
on O
pattern O

analysis B-DAT
and O
machine O
intelligence, O
35(2):329–341, O
2013 O

. B-DAT
[31] O
Yusuke O
Sugano, O
Yasuyuki O
Matsushita O

, B-DAT
and O
Yoichi O
Sato. O
Learning-by-synthesis O

for B-DAT
appearance-based O
3d O
gaze O
estimation. O

In B-DAT
Computer O
Vision O
and O
Pattern O

Recognition B-DAT
(CVPR), O
2014 O
IEEE O
Conference O

on, B-DAT
pages O
1821–1828. O
IEEE, O
2014. O
[32] O
Kar-Han O
Tan, O
David O
J O

Kriegman, B-DAT
and O
Narendra O
Ahuja. O
Appearance-based O

eye B-DAT
gaze O
es- O
timation. O
In O

Applications B-DAT
of O
Computer O
Vision, O
2002.(WACV O

2002). B-DAT
Proceedings. O
Sixth O
IEEE O
Workshop O

on, B-DAT
pages O
191–195. O
IEEE, O
2002 O

PALMERO B-DAT
ET O
AL.: O
MULTI-MODAL O
RECURRENT O

CNN B-DAT
FOR O
3D O
GAZE O
ESTIMATION O
13 O

33] B-DAT
Ronda O
Venkateswarlu O
et O
al. O

Eye B-DAT
gaze O
estimation O
from O
a O

single B-DAT
image O
of O
one O
eye. O

In B-DAT
Computer O
Vision, O
2003. O
Proceedings. O

Ninth B-DAT
IEEE O
International O
Conference O
on, O

pages B-DAT
136–143. O
IEEE, O
2003. O
[34] O
Kang O
Wang O
and O
Qiang O

Ji. B-DAT
Real O
time O
eye O
gaze O

tracking B-DAT
with O
3d O
deformable O
eye-face O

model. B-DAT
In O
Proceedings O
of O
the O

IEEE B-DAT
Conference O
on O
Computer O
Vision O

and B-DAT
Pattern O
Recog- O
nition, O
pages O

1003–1011, B-DAT
2017 O

. B-DAT
[35] O
Oliver O
Williams, O
Andrew O
Blake O

, B-DAT
and O
Roberto O
Cipolla. O
Sparse O

and B-DAT
semi-supervised O
visual O
mapping O
with O

the B-DAT
sˆ O
3gp. O
In O
Computer O

Vision B-DAT
and O
Pattern O
Recognition, O
2006 O

IEEE B-DAT
Computer O
Society O
Conference O
on, O

volume B-DAT
1, O
pages O
230–237. O
IEEE, O
2006 O

. B-DAT
[36] O
William O
Hyde O
Wollaston O
et O

al. B-DAT
Xiii. O
on O
the O
apparent O

direction B-DAT
of O
eyes O
in O
a O

portrait. B-DAT
Philosophical O
Transactions O
of O
the O

Royal B-DAT
Society O
of O
London, O
114:247–256 O

, B-DAT
1824. O
[37] O
Erroll O
Wood O
and O
Andreas O

Bulling. B-DAT
Eyetab: O
Model-based O
gaze O
estimation O

on B-DAT
unmodi- O
fied O
tablet O
computers O

. B-DAT
In O
Proceedings O
of O
the O

Symposium B-DAT
on O
Eye O
Tracking O
Research O

and B-DAT
Applications, O
pages O
207–210. O
ACM, O
2014 O

. B-DAT
[38] O
Erroll O
Wood, O
Tadas O
Baltrusaitis O

, B-DAT
Xucong O
Zhang, O
Yusuke O
Sugano, O

Peter B-DAT
Robinson, O
and O
Andreas O
Bulling. O

Rendering B-DAT
of O
eyes O
for O
eye-shape O

registration B-DAT
and O
gaze O
estimation. O
In O

Proceedings B-DAT
of O
the O
IEEE O
International O

Conference B-DAT
on O
Computer O
Vision, O
pages O
3756 O

– B-DAT
3764, O
2015. O
[39] O
Erroll O
Wood, O
Tadas O
Baltrušaitis O

, B-DAT
Louis-Philippe O
Morency, O
Peter O
Robinson, O

and B-DAT
Andreas O
Bulling. O
A O
3d O

morphable B-DAT
eye O
region O
model O
for O

gaze B-DAT
estimation. O
In O
European O
Confer- O

ence B-DAT
on O
Computer O
Vision, O
pages O
297 O

–313. B-DAT
Springer, O
2016. O
[40] O
Erroll O
Wood, O
Tadas O
Baltrušaitis O

, B-DAT
Louis-Philippe O
Morency, O
Peter O
Robinson, O

and B-DAT
Andreas O
Bulling. O
Learning O
an O

appearance-based B-DAT
gaze O
estimator O
from O
one O

million B-DAT
synthesised O
images. O
In O
Proceedings O

of B-DAT
the O
Ninth O
Biennial O
ACM O

Symposium B-DAT
on O
Eye O
Tracking O
Re- O

search B-DAT
& O
Applications, O
pages O
131–138. O

ACM, B-DAT
2016. O
[41] O
Dong O
Hyun O
Yoo O
and O

Myung B-DAT
Jin O
Chung. O
A O
novel O

non-intrusive B-DAT
eye O
gaze O
estimation O
using O

cross-ratio B-DAT
under O
large O
head O
motion O

. B-DAT
Computer O
Vision O
and O
Image O

Understanding, B-DAT
98(1):25–51, O
2005. O
[42] O
Xucong O
Zhang, O
Yusuke O
Sugano O

, B-DAT
Mario O
Fritz, O
and O
Andreas O

Bulling. B-DAT
Appearance-based O
gaze O
estimation O
in O

the B-DAT
wild. O
In O
Proceedings O
of O

the B-DAT
IEEE O
Conference O
on O
Computer O

Vision B-DAT
and O
Pattern O
Recognition, O
pages O
4511 O

–4520, B-DAT
2015. O
[43] O
Xucong O
Zhang, O
Yusuke O
Sugano O

, B-DAT
Mario O
Fritz, O
and O
Andreas O

Bulling. B-DAT
It’s O
written O
all O
over O

your B-DAT
face: O
Full-face O
appearance-based O
gaze O

estimation. B-DAT
In O
Proc. O
IEEE O
International O

Conference B-DAT
on O
Computer O
Vision O
and O

Pattern B-DAT
Recognition O
Workshops O
(CVPRW), O
2017 O

state O
of O
the O
art O
on O
EYEDIAP B-DAT
dataset, O
further O
improved O
by O
4 O

of O
our O
solution O
on O
the O
EYEDIAP B-DAT
dataset O
[7] O
in O
a O
wide O

VGA O
videos O
from O
the O
publicly-available O
EYEDIAP B-DAT
dataset O
[7] O
to O
perform O
the O

FT-S O
scenario O
are O
provided O
by O
EYEDIAP B-DAT
dataset. O
MPIIGaze:. O
State-of-the-art O
full-face O
3D O

fine-tuned O
it O
with O
the O
filtered O
EYEDIAP B-DAT
subsets O
using O
our O
training O
parameters O

this O
pa- O
per O
used O
the O
EYEDIAP B-DAT
dataset O
made O
available O
by O
the O

sub- O
mission O
to O
SemEval O
2017 O
RumourEval B-DAT

curacy O
of O
0.784 O
on O
the O
RumourEval B-DAT
test O
set O
outperforming O
all O
other O

et O
al., O
2016). O
However O
the O
RumourEval B-DAT
task O
is O
different O
as O
it O

system O
for O
subtask-A: O
SDQC O
for O
RumourEval, B-DAT
task- O
8 O
of O
SemEval O
2017 O

The O
objective O
of O
subtask-A O
of O
RumourEval B-DAT
was O
to O
identify O
the O
stance O

the O
evaluation O
metric O
for O
this O
RumourEval B-DAT
subtask. O
However, O
since O
a O
majority O

mance O
for O
subtask O
A O
of O
RumourEval B-DAT

sion O
for O
subtask O
A O
of O
RumourEval B-DAT
in O
which O
the O
participants O
were O

Kanade B-DAT
dataset O
(CK+), O
and O
delivered O
88.7 O

Kanade B-DAT
dataset O
(CK+) O
[10]. O
By O
using O

Kanade B-DAT
dataset O
[10]. O
The O
CK+ O
dataset O

5] O
Y-I O
Tian, O
Takeo O
Kanade, B-DAT
and O
Jeffrey O
F O
Cohn, O
“Recognizing O

Lucey, O
Jeffrey O
F O
Cohn, O
Takeo O
Kanade, B-DAT
Jason O
Saragih, O
Zara O
Am- O
badar O

Kanade B-DAT
dataset O
(CK+): O
A O
complete O
dataset O

facial O
expressions O
in O
the O
extended O
Cohn B-DAT

for O
classification O
in O
the O
extended O
Cohn B-DAT

our O
experiments O
on O
the O
extended O
Cohn B-DAT

Takeo O
Kanade, O
and O
Jeffrey O
F O
Cohn, B-DAT
“Recognizing O
action O
units O
for O
facial O

10] O
Patrick O
Lucey, O
Jeffrey O
F O
Cohn, B-DAT
Takeo O
Kanade, O
Jason O
Saragih, O
Zara O

and O
Iain O
Matthews, O
“The O
extended O
Cohn B-DAT

facial O
expressions O
in O
the O
extended O
Cohn-Kanade B-DAT
dataset O
(CK+), O
and O
delivered O
88.7 O

for O
classification O
in O
the O
extended O
Cohn-Kanade B-DAT
dataset O
(CK+) O
[10]. O
By O
using O

our O
experiments O
on O
the O
extended O
Cohn-Kanade B-DAT
dataset O
[10]. O
The O
CK+ O
dataset O

and O
Iain O
Matthews, O
“The O
extended O
Cohn-Kanade B-DAT
dataset O
(CK+): O
A O
complete O
dataset O

Baldwin O
(2011). O
They O
released O
the O
LexNorm B-DAT
corpus, O
consisting O
of O
549 O
Tweets O

the O
highest O
accuracy O
on O
the O
LexNorm B-DAT
dataset. O
They O
rerank O
the O
results O

to O
the O
annotation O
of O
the O
LexNorm B-DAT
corpus. O
Annotation O
included O
1-N O
and O

Words O
Lang. O
Caps O
Multiword O
%normalized O
LexNorm1 B-DAT

40,560 O
en O
some O
no O
10.5 O
LexNorm2015 B-DAT
Baldwin O
et O
al. O
(2015b) O
44,385 O

Note O
that O
we O
use O
the O
LexNorm1 B-DAT

improvements O
compared O
to O
the O
original O
LexNorm B-DAT
corpus. O
The O
GhentNorm O
corpus O
is O

3 O
is O
taken O
from O
the O
LexNorm2015 B-DAT
corpus. O
This O
annotation O
also O
include O

on O
pronunciation. O
Similar O
to O
the O
LexNorm B-DAT
2015 O
annotation, O
the O
phrasal O
abbreviation O

LexNorm1.2: B-DAT
for O
testing O
on O
the O
LexNorm O
corpus, O
we O
use O
2,000 O
Tweets O

LexNorm2015 B-DAT

performs O
especially O
well O
on O
the O
LexNorm2015 B-DAT
corpus. O
This O
is O
due O
to O

fea- O
ture O
groups O
on O
the O
LexNorm2015 B-DAT
develop- O
ment O
data O

cands O
LiLiu O
LexNorm2015 B-DAT
GhentNorm O
1 O
95.6 O
97.6 O
98.3 O

unique O
correct O
candidates O
for O
the O
LexNorm2015 B-DAT
corpus. O
Furthermore, O
this O
graph O
shows O

benchmarks. O
First O
we O
consider O
the O
LexNorm B-DAT
corpus, O
for O
which O
most O
previous O

we O
test O
our O
performance O
on O
LexNorm2015, B-DAT
which O
was O
used O
in O
the O

the O
ablation O
experiments O
on O
the O
LexNorm2015 B-DAT
corpus. O
In O
this O
setup, O
the O

LiLiu O
LexNorm2015 B-DAT

of O
training O
data O
on O
the O
LexNorm2015 B-DAT
and O
LiLiu O
dataset O

two O
largest O
datasets: O
LiLiu O
and O
LexNorm2015 B-DAT

higher O
F1 O
scores O
on O
the O
LexNorm2015 B-DAT
dataset O
are O
probably O
due O
to O

train O
and O
run O
on O
the O
LexNorm2015 B-DAT
dataset, O
as O
well O
as O
the O

different O
Aspell O
modes O
on O
the O
LexNorm2015 B-DAT
dataset, O
using O
our O
standard O
splits O

Prev. O
source O
Eval O
Prev. O
MoNoise O
LexNorm1 B-DAT

2015) O
acc O
87.58 O
5 O
87.63 O
LexNorm2015 B-DAT
Jin O
(2015) O
F1 O
84.21 O
86.39 O

with O
previous O
work. O
For O
the O
LexNorm1 B-DAT

benchmarks. O
The O
difference O
on O
the O
LexNorm B-DAT
dataset O
is O
rather O
small; O
however O

The O
performance O
gap O
on O
the O
LexNorm2015 B-DAT
dataset O
is O
a O
bit O
bigger O

Recall O
Precision O
F1 O
score O
LexNorm1 B-DAT

.2 O
74.45 O
77.56 O
75.97 O
LexNorm2015 B-DAT
80.26 O
93.53 O
86.39 O
GhentNorm O
28.81 O

slightly O
adapted O
version O
of O
the O
lexnorm1 B-DAT

the O
widely-used O
UNBC-McMaster O
Shoulder- O
Pain O
dataset, B-DAT
achieving O
the O
state-of-the-art O
performance. O
A O

one O
video O
from O
the O
Shoulder-Pain O
dataset B-DAT
[1] O
which O
provides O
per-frame O
observer-rated O

Particularly, O
Shoulder-Pain O
is O
the O
only O
dataset B-DAT
available O
for O
visual O
analysis O
with O

intensities O
(see O
Fig. O
1), O
the O
dataset B-DAT
is O
small, O
the O
label O
is O

Although O
the O
small O
dataset B-DAT
problem O
prevents O
us O
from O
di O

intensity O
visually O
using O
the O
Shoulder-Pain O
dataset B-DAT
only: O
Or- O
dinal O
Support O
Vector O

12]1 O
trained O
using O
the O
CASIA-WebFace O
dataset B-DAT
con- O
taning O
0.5 O
million O
face O

discrete O
values O
in O
the O
Shoulder-Pain O
dataset, B-DAT
it O
is O
natural O
to O
regularize O

Metrics O
Labels O
in O
the O
Shoulder-Pain O
dataset B-DAT
are O
highly O
imbalanced, O
as O
91.35 O

MSE O
(wMSE) O
to O
address O
the O
dataset B-DAT
imbalance O
issue. O
For O
example, O
the O

see’ O
is O
a O
totally O
balanced O
dataset B-DAT

our O
network O
on O
the O
Shoulder-Pain O
dataset B-DAT
[1] O
that O
con- O
tains O
200 O

the O
pain O
intensity O
estimation. O
The O
dataset B-DAT
comes O
with O
four O
types O
of O

is O
trained O
on O
CASIA- O
WebFace O
dataset B-DAT
[16], O
which O
contains O
494,414 O
training O

on O
the O
images O
of O
Shoulder-Pain O
dataset B-DAT

related O
works O
on O
the O
Shoulder-Pain O
dataset B-DAT
for O
the O
estimation O
of O
pain O

way O
to O
address O
over-fitting O
small O
dataset B-DAT

25 O
times O
on O
the O
Shoulder-Pain O
dataset B-DAT
which O
contains O
25 O
sub- O
jects O

tested O
on O
the O
UNBC-McMaster O
Shoulder-Pain O
dataset B-DAT
and O
achieves O
state-of-the-art O
performance O
on O

challenges O
and O
a O
mul- O
timodal O
dataset B-DAT

and O
verified O
on O
the O
widely-used O
UNBC-McMaster B-DAT
Shoulder- O
Pain O
dataset, O
achieving O
the O

layer O
is O
tested O
on O
the O
UNBC-McMaster B-DAT
Shoulder-Pain O
dataset O
and O
achieves O
state-of-the-art O

and O
verified O
on O
the O
widely-used O
UNBC B-DAT

layer O
is O
tested O
on O
the O
UNBC B-DAT

is O
tested O
on O
the O
WireFrame O
dataset B-DAT
[1] O
and O
the O
YorkUrban O
dataset O

4.5 O
percents O
on O
the O
WireFrame O
dataset B-DAT

segment O
maps O
in O
the O
training O
dataset B-DAT
to O
their O
attraction O
field O
maps O

is O
tested O
on O
the O
WireFrame O
dataset B-DAT
[1] O
and O
the O
YorkUrban O
dataset O

by O
4.5% O
on O
the O
WireFrame O
dataset B-DAT

human-level O
performance O
on O
the O
BSDS500 O
dataset B-DAT
[26]. O
Followed O
by O
this O
breakthrough O

goal O
by O
proposing O
a O
large-scale O
dataset B-DAT
with O
high O
quality O
line O
segment O

LSD O
benchmarks, O
the O
Wire- O
Frame O
dataset B-DAT
(with O
4.5% O
significant O
improvement) O
and O

the O
YorkUrban O
dataset B-DAT

field O
representation O
on O
the O
WireFrame O
dataset B-DAT
[1]. O
We O
first O
compute O
the O

the O
protocol O
provided O
in O
the O
dataset B-DAT

N} O
the O
provided O
training O
dataset B-DAT
consisting O
ofN O
pairs O
of O
raw O

N} O
be O
the O
dual O
training O
dataset B-DAT

N} O
the O
final O
training O
dataset B-DAT

train- O
ing O
set O
of O
Wireframe O
dataset B-DAT
[1]. O
Similar O
to O
[1], O
we O

existing O
methods O
on O
the O
WireFrame O
dataset B-DAT
[1] O
and O
YorkUr- O
ban O
dataset O

we O
train O
on O
the O
Wreframe O
dataset B-DAT
[1], O
it O
is O
necessary O
to O

proposed O
method O
on O
its O
testing O
dataset, B-DAT
which O
includes O
462 O
images O
for O

ods O
on O
the O
WireFrame O
[1] O
dataset B-DAT

state-of-the-art O
approaches O
on O
the O
WireFrame O
dataset B-DAT
and O
York O
Urban O
dataset. O
The O

ond O
(FPS) O
on O
the O
WireFrame O
dataset B-DAT

dataset B-DAT
York O
Urban O

dataset B-DAT
FPS O

proposed O
method O
on O
the O
Wireframe O
dataset B-DAT
[1] O
and O
YorkUrban O
dataset O
[2 O

performance. O
Due O
to O
the O
YorkUrban O
dataset B-DAT
aiming O
at O
Man- O
hattan O
frame O

of O
all O
methods O
on O
this O
dataset B-DAT
decreased O

abovementioned O
approaches O
on O
the O
Wireframe O
dataset B-DAT

with O
different O
methods O
on O
Wireframe O
dataset B-DAT
(see O
Figure O
6), O
YorkUrban O
dataset O

segment O
detection O
on O
Wireframe O
[1] O
dataset B-DAT
with O
different O
approaches O
LSD O
[10 O

segment O
detection O
on O
YorkUrban O
[2] O
dataset B-DAT
with O
different O
approaches O
LSD O
[10 O

used O
LSD O
benchmarks, O
the O
WireFrame O
dataset B-DAT
[1] O
and O
the O
YorkUrban O
dataset O

wireframe B-DAT
4https://github.com/NamgyuCho O

line O
segments O
fast. O
The O
deep O
wireframe B-DAT
parser O
[1] O
spends O
much O
time O

wireframe B-DAT
https://github.com/NamgyuCho/Linelet-code-and-YorkUrban-LineSegment-DB O
https://github.com/NamgyuCho/Linelet-code-and-YorkUrban-LineSegment-DB O
http://www.elderlab.yorku.ca/resources/ O
http://www.ipol.im/pub/art/2012/gjmr-lsd O

cinic-10 B-DAT

cinic-10 B-DAT

cinic-10 B-DAT

cinic-10 B-DAT

10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

10 B-DAT
dataset O
as O
a O
plug-in O
extended O

10 B-DAT

10 B-DAT
with O
images O
selected O
and O
downsampled O

this O
reason, O
CIFAR-10 B-DAT
and O
CIFAR- O
100 O
(Krizhevsky, O
2009) O
have O
become O
the O

In O
CIFAR-10, B-DAT
each O
of O
the O
10 O
classes O
has O
6,000 O
examples. O
The O

100 B-DAT
classes O
of O
CIFAR-100 O
only O
have O

100 B-DAT
is O
arguably O
more O
difficult O
than O

10 B-DAT

10 B-DAT

10 B-DAT

10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

10 B-DAT
via O
the O
addition O
of O
downsampled O

10 B-DAT
has O
the O
following O
desirable O
properties O

10 B-DAT
can O
be O
used O
as O
a O

10 B-DAT

10 B-DAT
consists O
of O
images O
from O
both O

10 B-DAT
dataset. O
This O
process O
is O
detailed O

10 B-DAT
The O
original O
CIFAR-10 O
is O
processed O

10 B-DAT

10 B-DAT
class O
(airplane, O
automobile O
etc O

10 B-DAT
data: O
20,000 O
images O
per O
set O

10 B-DAT
data O
among O
all O
three O
sets O

10 B-DAT
test O
set O
contains O
the O
entirety O

10 B-DAT
test O
set, O
as O
well O
as O

10 B-DAT
train O
set. O
CINIC-10’s O
train O
and O

10 B-DAT
can O
be O
fully O
recovered O
from O

10 B-DAT
by O
the O
filename O

10 B-DAT

10 B-DAT
is O
listed O
in O
imagenet- O
contributors.csv O

10 B-DAT
by O
augmenting O
the O
CIFAR-10 O
data O

10 B-DAT
classes O
(airplane, O
automobile, O
etc.). O
synset O

10 B-DAT
data O
and O
the O
remaining O
from O

10 B-DAT
is O
with O
a O
PyTorch5 O
data O

10 B-DAT
(Section O
4.2 O

10 B-DAT
and O
ImageNet O
contributors O
is O
given O

0 O
50 O
100 B-DAT
150 O
200 O
250 O

10 B-DAT

10 B-DAT
(blue) O
and O
ImageNet O
contributions O
(red O

10 B-DAT

10 B-DAT
images O
will O
note O
that O
these O

10 B-DAT
(cows O
and O
goats O
in O
the O

10, B-DAT
automobile O

10, B-DAT
airplane O

10 B-DAT

106 B-DAT
were O
copied O
and O
tested O

10, B-DAT
bird O

10, B-DAT
cat O

10 B-DAT
benchmarks O

10, B-DAT
deer O

10, B-DAT
dog O

10, B-DAT
frog O

10, B-DAT
horse O

Figure O
10 B-DAT

10, B-DAT
ship O

10, B-DAT
truck O

10 B-DAT
in O
this O
technical O
report. O
It O

10 B-DAT
with O
downsam- O
pled O
images O
sourced O

10 B-DAT
(and O
more O
challenging) O
but O
not O

10 B-DAT
Github O
repository O

10 B-DAT

Vision O
(IJCV), O
115(3):211–252, O
2015. O
doi: O
10 B-DAT

1007 B-DAT

10 B-DAT

CINIC B-DAT

CINIC B-DAT

technical O
report O
we O
introduce O
the O
CINIC B-DAT

existing O
benchmarking O
datasets, O
we O
present O
CINIC B-DAT

-10: O
CINIC B-DAT

addition O
of O
downsampled O
ImageNet O
images. O
CINIC B-DAT

as O
in O
CIFAR, O
meaning O
that O
CINIC B-DAT

CINIC B-DAT

our O
method O
of O
constructing O
the O
CINIC B-DAT

among O
all O
three O
sets. O
The O
CINIC B-DAT

from O
the O
CIFAR-10 O
train O
set. O
CINIC B-DAT

can O
be O
fully O
recovered O
from O
CINIC B-DAT

The O
mapping O
from O
sysnsets O
to O
CINIC B-DAT

from O
each O
synset-group O
to O
compile O
CINIC B-DAT

The O
simplest O
way O
to O
use O
CINIC B-DAT

examples O
for O
each O
class O
in O
CINIC B-DAT

show O
randomly O
selected O
samples O
from O
CINIC B-DAT

Figure O
2: O
CINIC B-DAT

Figure O
3: O
CINIC B-DAT

1 O
gives O
benchmark O
results O
on O
CINIC B-DAT

Figure O
4: O
CINIC B-DAT

Figure O
5: O
CINIC B-DAT

Table O
1: O
CINIC B-DAT

Figure O
6: O
CINIC B-DAT

Figure O
7: O
CINIC B-DAT

Figure O
8: O
CINIC B-DAT

Figure O
9: O
CINIC B-DAT

Figure O
10: O
CINIC B-DAT

Figure O
11: O
CINIC B-DAT

We O
presented O
CINIC B-DAT

Antoniou, O
and O
Amos O
J. O
Storkey. O
CINIC B-DAT

CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

technical O
report O
we O
introduce O
the O
CINIC-10 B-DAT
dataset O
as O
a O
plug-in O
extended O

existing O
benchmarking O
datasets, O
we O
present O
CINIC-10 B-DAT

: O
CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

addition O
of O
downsampled O
ImageNet O
images. O
CINIC-10 B-DAT
has O
the O
following O
desirable O
properties O

as O
in O
CIFAR, O
meaning O
that O
CINIC-10 B-DAT
can O
be O
used O
as O
a O

CINIC-10 B-DAT
consists O
of O
images O
from O
both O

our O
method O
of O
constructing O
the O
CINIC-10 B-DAT
dataset. O
This O
process O
is O
detailed O

among O
all O
three O
sets. O
The O
CINIC-10 B-DAT
test O
set O
contains O
the O
entirety O

from O
the O
CIFAR-10 O
train O
set. O
CINIC-10 B-DAT

can O
be O
fully O
recovered O
from O
CINIC-10 B-DAT
by O
the O
filename O

The O
mapping O
from O
sysnsets O
to O
CINIC-10 B-DAT
is O
listed O
in O
imagenet- O
contributors.csv O

from O
each O
synset-group O
to O
compile O
CINIC-10 B-DAT
by O
augmenting O
the O
CIFAR-10 O
data O

The O
simplest O
way O
to O
use O
CINIC-10 B-DAT
is O
with O
a O
PyTorch5 O
data O

examples O
for O
each O
class O
in O
CINIC-10 B-DAT
(Section O
4.2 O

show O
randomly O
selected O
samples O
from O
CINIC-10 B-DAT

Figure O
2: O
CINIC-10, B-DAT
automobile O

Figure O
3: O
CINIC-10, B-DAT
airplane O

1 O
gives O
benchmark O
results O
on O
CINIC-10 B-DAT

Figure O
4: O
CINIC-10, B-DAT
bird O

Figure O
5: O
CINIC-10, B-DAT
cat O

Table O
1: O
CINIC-10 B-DAT
benchmarks O

Figure O
6: O
CINIC-10, B-DAT
deer O

Figure O
7: O
CINIC-10, B-DAT
dog O

Figure O
8: O
CINIC-10, B-DAT
frog O

Figure O
9: O
CINIC-10, B-DAT
horse O

Figure O
10: O
CINIC-10, B-DAT
ship O

Figure O
11: O
CINIC-10, B-DAT
truck O

We O
presented O
CINIC-10 B-DAT
in O
this O
technical O
report. O
It O

Antoniou, O
and O
Amos O
J. O
Storkey. O
CINIC-10 B-DAT
Github O
repository O

CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

technical O
report O
we O
introduce O
the O
CINIC-10 B-DAT
dataset O
as O
a O
plug-in O
extended O

existing O
benchmarking O
datasets, O
we O
present O
CINIC-10 B-DAT

: O
CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

addition O
of O
downsampled O
ImageNet O
images. O
CINIC-10 B-DAT
has O
the O
following O
desirable O
properties O

as O
in O
CIFAR, O
meaning O
that O
CINIC-10 B-DAT
can O
be O
used O
as O
a O

CINIC-10 B-DAT
consists O
of O
images O
from O
both O

our O
method O
of O
constructing O
the O
CINIC-10 B-DAT
dataset. O
This O
process O
is O
detailed O

among O
all O
three O
sets. O
The O
CINIC-10 B-DAT
test O
set O
contains O
the O
entirety O

from O
the O
CIFAR-10 O
train O
set. O
CINIC-10 B-DAT

can O
be O
fully O
recovered O
from O
CINIC-10 B-DAT
by O
the O
filename O

The O
mapping O
from O
sysnsets O
to O
CINIC-10 B-DAT
is O
listed O
in O
imagenet- O
contributors.csv O

from O
each O
synset-group O
to O
compile O
CINIC-10 B-DAT
by O
augmenting O
the O
CIFAR-10 O
data O

The O
simplest O
way O
to O
use O
CINIC-10 B-DAT
is O
with O
a O
PyTorch5 O
data O

examples O
for O
each O
class O
in O
CINIC-10 B-DAT
(Section O
4.2 O

show O
randomly O
selected O
samples O
from O
CINIC-10 B-DAT

Figure O
2: O
CINIC-10, B-DAT
automobile O

Figure O
3: O
CINIC-10, B-DAT
airplane O

1 O
gives O
benchmark O
results O
on O
CINIC-10 B-DAT

Figure O
4: O
CINIC-10, B-DAT
bird O

Figure O
5: O
CINIC-10, B-DAT
cat O

Table O
1: O
CINIC-10 B-DAT
benchmarks O

Figure O
6: O
CINIC-10, B-DAT
deer O

Figure O
7: O
CINIC-10, B-DAT
dog O

Figure O
8: O
CINIC-10, B-DAT
frog O

Figure O
9: O
CINIC-10, B-DAT
horse O

Figure O
10: O
CINIC-10, B-DAT
ship O

Figure O
11: O
CINIC-10, B-DAT
truck O

We O
presented O
CINIC-10 B-DAT
in O
this O
technical O
report. O
It O

Antoniou, O
and O
Amos O
J. O
Storkey. O
CINIC-10 B-DAT
Github O
repository O

cinic-10 B-DAT

cinic-10 B-DAT

cinic-10 B-DAT

cinic-10 B-DAT

10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

10 B-DAT
dataset O
as O
a O
plug-in O
extended O

10 B-DAT

10 B-DAT
with O
images O
selected O
and O
downsampled O

this O
reason, O
CIFAR-10 B-DAT
and O
CIFAR- O
100 O
(Krizhevsky, O
2009) O
have O
become O
the O

In O
CIFAR-10, B-DAT
each O
of O
the O
10 O
classes O
has O
6,000 O
examples. O
The O

100 B-DAT
classes O
of O
CIFAR-100 O
only O
have O

100 B-DAT
is O
arguably O
more O
difficult O
than O

10 B-DAT

10 B-DAT

10 B-DAT

10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

10 B-DAT
via O
the O
addition O
of O
downsampled O

10 B-DAT
has O
the O
following O
desirable O
properties O

10 B-DAT
can O
be O
used O
as O
a O

10 B-DAT

10 B-DAT
consists O
of O
images O
from O
both O

10 B-DAT
dataset. O
This O
process O
is O
detailed O

10 B-DAT
The O
original O
CIFAR-10 O
is O
processed O

10 B-DAT

10 B-DAT
class O
(airplane, O
automobile O
etc O

10 B-DAT
data: O
20,000 O
images O
per O
set O

10 B-DAT
data O
among O
all O
three O
sets O

10 B-DAT
test O
set O
contains O
the O
entirety O

10 B-DAT
test O
set, O
as O
well O
as O

10 B-DAT
train O
set. O
CINIC-10’s O
train O
and O

10 B-DAT
can O
be O
fully O
recovered O
from O

10 B-DAT
by O
the O
filename O

10 B-DAT

10 B-DAT
is O
listed O
in O
imagenet- O
contributors.csv O

10 B-DAT
by O
augmenting O
the O
CIFAR-10 O
data O

10 B-DAT
classes O
(airplane, O
automobile, O
etc.). O
synset O

10 B-DAT
data O
and O
the O
remaining O
from O

10 B-DAT
is O
with O
a O
PyTorch5 O
data O

10 B-DAT
(Section O
4.2 O

10 B-DAT
and O
ImageNet O
contributors O
is O
given O

0 O
50 O
100 B-DAT
150 O
200 O
250 O

10 B-DAT

10 B-DAT
(blue) O
and O
ImageNet O
contributions O
(red O

10 B-DAT

10 B-DAT
images O
will O
note O
that O
these O

10 B-DAT
(cows O
and O
goats O
in O
the O

10, B-DAT
automobile O

10, B-DAT
airplane O

10 B-DAT

106 B-DAT
were O
copied O
and O
tested O

10, B-DAT
bird O

10, B-DAT
cat O

10 B-DAT
benchmarks O

10, B-DAT
deer O

10, B-DAT
dog O

10, B-DAT
frog O

10, B-DAT
horse O

Figure O
10 B-DAT

10, B-DAT
ship O

10, B-DAT
truck O

10 B-DAT
in O
this O
technical O
report. O
It O

10 B-DAT
with O
downsam- O
pled O
images O
sourced O

10 B-DAT
(and O
more O
challenging) O
but O
not O

10 B-DAT
Github O
repository O

10 B-DAT

Vision O
(IJCV), O
115(3):211–252, O
2015. O
doi: O
10 B-DAT

1007 B-DAT

10 B-DAT

CINIC B-DAT

CINIC B-DAT

technical O
report O
we O
introduce O
the O
CINIC B-DAT

existing O
benchmarking O
datasets, O
we O
present O
CINIC B-DAT

-10: O
CINIC B-DAT

addition O
of O
downsampled O
ImageNet O
images. O
CINIC B-DAT

as O
in O
CIFAR, O
meaning O
that O
CINIC B-DAT

CINIC B-DAT

our O
method O
of O
constructing O
the O
CINIC B-DAT

among O
all O
three O
sets. O
The O
CINIC B-DAT

from O
the O
CIFAR-10 O
train O
set. O
CINIC B-DAT

can O
be O
fully O
recovered O
from O
CINIC B-DAT

The O
mapping O
from O
sysnsets O
to O
CINIC B-DAT

from O
each O
synset-group O
to O
compile O
CINIC B-DAT

The O
simplest O
way O
to O
use O
CINIC B-DAT

examples O
for O
each O
class O
in O
CINIC B-DAT

show O
randomly O
selected O
samples O
from O
CINIC B-DAT

Figure O
2: O
CINIC B-DAT

Figure O
3: O
CINIC B-DAT

1 O
gives O
benchmark O
results O
on O
CINIC B-DAT

Figure O
4: O
CINIC B-DAT

Figure O
5: O
CINIC B-DAT

Table O
1: O
CINIC B-DAT

Figure O
6: O
CINIC B-DAT

Figure O
7: O
CINIC B-DAT

Figure O
8: O
CINIC B-DAT

Figure O
9: O
CINIC B-DAT

Figure O
10: O
CINIC B-DAT

Figure O
11: O
CINIC B-DAT

We O
presented O
CINIC B-DAT

Antoniou, O
and O
Amos O
J. O
Storkey. O
CINIC B-DAT

CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

technical O
report O
we O
introduce O
the O
CINIC-10 B-DAT
dataset O
as O
a O
plug-in O
extended O

existing O
benchmarking O
datasets, O
we O
present O
CINIC-10 B-DAT

: O
CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

addition O
of O
downsampled O
ImageNet O
images. O
CINIC-10 B-DAT
has O
the O
following O
desirable O
properties O

as O
in O
CIFAR, O
meaning O
that O
CINIC-10 B-DAT
can O
be O
used O
as O
a O

CINIC-10 B-DAT
consists O
of O
images O
from O
both O

our O
method O
of O
constructing O
the O
CINIC-10 B-DAT
dataset. O
This O
process O
is O
detailed O

among O
all O
three O
sets. O
The O
CINIC-10 B-DAT
test O
set O
contains O
the O
entirety O

from O
the O
CIFAR-10 O
train O
set. O
CINIC-10 B-DAT

can O
be O
fully O
recovered O
from O
CINIC-10 B-DAT
by O
the O
filename O

The O
mapping O
from O
sysnsets O
to O
CINIC-10 B-DAT
is O
listed O
in O
imagenet- O
contributors.csv O

from O
each O
synset-group O
to O
compile O
CINIC-10 B-DAT
by O
augmenting O
the O
CIFAR-10 O
data O

The O
simplest O
way O
to O
use O
CINIC-10 B-DAT
is O
with O
a O
PyTorch5 O
data O

examples O
for O
each O
class O
in O
CINIC-10 B-DAT
(Section O
4.2 O

show O
randomly O
selected O
samples O
from O
CINIC-10 B-DAT

Figure O
2: O
CINIC-10, B-DAT
automobile O

Figure O
3: O
CINIC-10, B-DAT
airplane O

1 O
gives O
benchmark O
results O
on O
CINIC-10 B-DAT

Figure O
4: O
CINIC-10, B-DAT
bird O

Figure O
5: O
CINIC-10, B-DAT
cat O

Table O
1: O
CINIC-10 B-DAT
benchmarks O

Figure O
6: O
CINIC-10, B-DAT
deer O

Figure O
7: O
CINIC-10, B-DAT
dog O

Figure O
8: O
CINIC-10, B-DAT
frog O

Figure O
9: O
CINIC-10, B-DAT
horse O

Figure O
10: O
CINIC-10, B-DAT
ship O

Figure O
11: O
CINIC-10, B-DAT
truck O

We O
presented O
CINIC-10 B-DAT
in O
this O
technical O
report. O
It O

Antoniou, O
and O
Amos O
J. O
Storkey. O
CINIC-10 B-DAT
Github O
repository O

CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

technical O
report O
we O
introduce O
the O
CINIC-10 B-DAT
dataset O
as O
a O
plug-in O
extended O

existing O
benchmarking O
datasets, O
we O
present O
CINIC-10 B-DAT

: O
CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

addition O
of O
downsampled O
ImageNet O
images. O
CINIC-10 B-DAT
has O
the O
following O
desirable O
properties O

as O
in O
CIFAR, O
meaning O
that O
CINIC-10 B-DAT
can O
be O
used O
as O
a O

CINIC-10 B-DAT
consists O
of O
images O
from O
both O

our O
method O
of O
constructing O
the O
CINIC-10 B-DAT
dataset. O
This O
process O
is O
detailed O

among O
all O
three O
sets. O
The O
CINIC-10 B-DAT
test O
set O
contains O
the O
entirety O

from O
the O
CIFAR-10 O
train O
set. O
CINIC-10 B-DAT

can O
be O
fully O
recovered O
from O
CINIC-10 B-DAT
by O
the O
filename O

The O
mapping O
from O
sysnsets O
to O
CINIC-10 B-DAT
is O
listed O
in O
imagenet- O
contributors.csv O

from O
each O
synset-group O
to O
compile O
CINIC-10 B-DAT
by O
augmenting O
the O
CIFAR-10 O
data O

The O
simplest O
way O
to O
use O
CINIC-10 B-DAT
is O
with O
a O
PyTorch5 O
data O

examples O
for O
each O
class O
in O
CINIC-10 B-DAT
(Section O
4.2 O

show O
randomly O
selected O
samples O
from O
CINIC-10 B-DAT

Figure O
2: O
CINIC-10, B-DAT
automobile O

Figure O
3: O
CINIC-10, B-DAT
airplane O

1 O
gives O
benchmark O
results O
on O
CINIC-10 B-DAT

Figure O
4: O
CINIC-10, B-DAT
bird O

Figure O
5: O
CINIC-10, B-DAT
cat O

Table O
1: O
CINIC-10 B-DAT
benchmarks O

Figure O
6: O
CINIC-10, B-DAT
deer O

Figure O
7: O
CINIC-10, B-DAT
dog O

Figure O
8: O
CINIC-10, B-DAT
frog O

Figure O
9: O
CINIC-10, B-DAT
horse O

Figure O
10: O
CINIC-10, B-DAT
ship O

Figure O
11: O
CINIC-10, B-DAT
truck O

We O
presented O
CINIC-10 B-DAT
in O
this O
technical O
report. O
It O

Antoniou, O
and O
Amos O
J. O
Storkey. O
CINIC-10 B-DAT
Github O
repository O

cinic-10 B-DAT

cinic-10 B-DAT

cinic-10 B-DAT

cinic-10 B-DAT

10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

10 B-DAT
dataset O
as O
a O
plug-in O
extended O

10 B-DAT

10 B-DAT
with O
images O
selected O
and O
downsampled O

this O
reason, O
CIFAR-10 B-DAT
and O
CIFAR- O
100 O
(Krizhevsky, O
2009) O
have O
become O
the O

In O
CIFAR-10, B-DAT
each O
of O
the O
10 O
classes O
has O
6,000 O
examples. O
The O

100 B-DAT
classes O
of O
CIFAR-100 O
only O
have O

100 B-DAT
is O
arguably O
more O
difficult O
than O

10 B-DAT

10 B-DAT

10 B-DAT

10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

10 B-DAT
via O
the O
addition O
of O
downsampled O

10 B-DAT
has O
the O
following O
desirable O
properties O

10 B-DAT
can O
be O
used O
as O
a O

10 B-DAT

10 B-DAT
consists O
of O
images O
from O
both O

10 B-DAT
dataset. O
This O
process O
is O
detailed O

10 B-DAT
The O
original O
CIFAR-10 O
is O
processed O

10 B-DAT

10 B-DAT
class O
(airplane, O
automobile O
etc O

10 B-DAT
data: O
20,000 O
images O
per O
set O

10 B-DAT
data O
among O
all O
three O
sets O

10 B-DAT
test O
set O
contains O
the O
entirety O

10 B-DAT
test O
set, O
as O
well O
as O

10 B-DAT
train O
set. O
CINIC-10’s O
train O
and O

10 B-DAT
can O
be O
fully O
recovered O
from O

10 B-DAT
by O
the O
filename O

10 B-DAT

10 B-DAT
is O
listed O
in O
imagenet- O
contributors.csv O

10 B-DAT
by O
augmenting O
the O
CIFAR-10 O
data O

10 B-DAT
classes O
(airplane, O
automobile, O
etc.). O
synset O

10 B-DAT
data O
and O
the O
remaining O
from O

10 B-DAT
is O
with O
a O
PyTorch5 O
data O

10 B-DAT
(Section O
4.2 O

10 B-DAT
and O
ImageNet O
contributors O
is O
given O

0 O
50 O
100 B-DAT
150 O
200 O
250 O

10 B-DAT

10 B-DAT
(blue) O
and O
ImageNet O
contributions O
(red O

10 B-DAT

10 B-DAT
images O
will O
note O
that O
these O

10 B-DAT
(cows O
and O
goats O
in O
the O

10, B-DAT
automobile O

10, B-DAT
airplane O

10 B-DAT

106 B-DAT
were O
copied O
and O
tested O

10, B-DAT
bird O

10, B-DAT
cat O

10 B-DAT
benchmarks O

10, B-DAT
deer O

10, B-DAT
dog O

10, B-DAT
frog O

10, B-DAT
horse O

Figure O
10 B-DAT

10, B-DAT
ship O

10, B-DAT
truck O

10 B-DAT
in O
this O
technical O
report. O
It O

10 B-DAT
with O
downsam- O
pled O
images O
sourced O

10 B-DAT
(and O
more O
challenging) O
but O
not O

10 B-DAT
Github O
repository O

10 B-DAT

Vision O
(IJCV), O
115(3):211–252, O
2015. O
doi: O
10 B-DAT

1007 B-DAT

10 B-DAT

CINIC B-DAT

CINIC B-DAT

technical O
report O
we O
introduce O
the O
CINIC B-DAT

existing O
benchmarking O
datasets, O
we O
present O
CINIC B-DAT

-10: O
CINIC B-DAT

addition O
of O
downsampled O
ImageNet O
images. O
CINIC B-DAT

as O
in O
CIFAR, O
meaning O
that O
CINIC B-DAT

CINIC B-DAT

our O
method O
of O
constructing O
the O
CINIC B-DAT

among O
all O
three O
sets. O
The O
CINIC B-DAT

from O
the O
CIFAR-10 O
train O
set. O
CINIC B-DAT

can O
be O
fully O
recovered O
from O
CINIC B-DAT

The O
mapping O
from O
sysnsets O
to O
CINIC B-DAT

from O
each O
synset-group O
to O
compile O
CINIC B-DAT

The O
simplest O
way O
to O
use O
CINIC B-DAT

examples O
for O
each O
class O
in O
CINIC B-DAT

show O
randomly O
selected O
samples O
from O
CINIC B-DAT

Figure O
2: O
CINIC B-DAT

Figure O
3: O
CINIC B-DAT

1 O
gives O
benchmark O
results O
on O
CINIC B-DAT

Figure O
4: O
CINIC B-DAT

Figure O
5: O
CINIC B-DAT

Table O
1: O
CINIC B-DAT

Figure O
6: O
CINIC B-DAT

Figure O
7: O
CINIC B-DAT

Figure O
8: O
CINIC B-DAT

Figure O
9: O
CINIC B-DAT

Figure O
10: O
CINIC B-DAT

Figure O
11: O
CINIC B-DAT

We O
presented O
CINIC B-DAT

Antoniou, O
and O
Amos O
J. O
Storkey. O
CINIC B-DAT

CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

technical O
report O
we O
introduce O
the O
CINIC-10 B-DAT
dataset O
as O
a O
plug-in O
extended O

existing O
benchmarking O
datasets, O
we O
present O
CINIC-10 B-DAT

: O
CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

addition O
of O
downsampled O
ImageNet O
images. O
CINIC-10 B-DAT
has O
the O
following O
desirable O
properties O

as O
in O
CIFAR, O
meaning O
that O
CINIC-10 B-DAT
can O
be O
used O
as O
a O

CINIC-10 B-DAT
consists O
of O
images O
from O
both O

our O
method O
of O
constructing O
the O
CINIC-10 B-DAT
dataset. O
This O
process O
is O
detailed O

among O
all O
three O
sets. O
The O
CINIC-10 B-DAT
test O
set O
contains O
the O
entirety O

from O
the O
CIFAR-10 O
train O
set. O
CINIC-10 B-DAT

can O
be O
fully O
recovered O
from O
CINIC-10 B-DAT
by O
the O
filename O

The O
mapping O
from O
sysnsets O
to O
CINIC-10 B-DAT
is O
listed O
in O
imagenet- O
contributors.csv O

from O
each O
synset-group O
to O
compile O
CINIC-10 B-DAT
by O
augmenting O
the O
CIFAR-10 O
data O

The O
simplest O
way O
to O
use O
CINIC-10 B-DAT
is O
with O
a O
PyTorch5 O
data O

examples O
for O
each O
class O
in O
CINIC-10 B-DAT
(Section O
4.2 O

show O
randomly O
selected O
samples O
from O
CINIC-10 B-DAT

Figure O
2: O
CINIC-10, B-DAT
automobile O

Figure O
3: O
CINIC-10, B-DAT
airplane O

1 O
gives O
benchmark O
results O
on O
CINIC-10 B-DAT

Figure O
4: O
CINIC-10, B-DAT
bird O

Figure O
5: O
CINIC-10, B-DAT
cat O

Table O
1: O
CINIC-10 B-DAT
benchmarks O

Figure O
6: O
CINIC-10, B-DAT
deer O

Figure O
7: O
CINIC-10, B-DAT
dog O

Figure O
8: O
CINIC-10, B-DAT
frog O

Figure O
9: O
CINIC-10, B-DAT
horse O

Figure O
10: O
CINIC-10, B-DAT
ship O

Figure O
11: O
CINIC-10, B-DAT
truck O

We O
presented O
CINIC-10 B-DAT
in O
this O
technical O
report. O
It O

Antoniou, O
and O
Amos O
J. O
Storkey. O
CINIC-10 B-DAT
Github O
repository O

CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

technical O
report O
we O
introduce O
the O
CINIC-10 B-DAT
dataset O
as O
a O
plug-in O
extended O

existing O
benchmarking O
datasets, O
we O
present O
CINIC-10 B-DAT

: O
CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

addition O
of O
downsampled O
ImageNet O
images. O
CINIC-10 B-DAT
has O
the O
following O
desirable O
properties O

as O
in O
CIFAR, O
meaning O
that O
CINIC-10 B-DAT
can O
be O
used O
as O
a O

CINIC-10 B-DAT
consists O
of O
images O
from O
both O

our O
method O
of O
constructing O
the O
CINIC-10 B-DAT
dataset. O
This O
process O
is O
detailed O

among O
all O
three O
sets. O
The O
CINIC-10 B-DAT
test O
set O
contains O
the O
entirety O

from O
the O
CIFAR-10 O
train O
set. O
CINIC-10 B-DAT

can O
be O
fully O
recovered O
from O
CINIC-10 B-DAT
by O
the O
filename O

The O
mapping O
from O
sysnsets O
to O
CINIC-10 B-DAT
is O
listed O
in O
imagenet- O
contributors.csv O

from O
each O
synset-group O
to O
compile O
CINIC-10 B-DAT
by O
augmenting O
the O
CIFAR-10 O
data O

The O
simplest O
way O
to O
use O
CINIC-10 B-DAT
is O
with O
a O
PyTorch5 O
data O

examples O
for O
each O
class O
in O
CINIC-10 B-DAT
(Section O
4.2 O

show O
randomly O
selected O
samples O
from O
CINIC-10 B-DAT

Figure O
2: O
CINIC-10, B-DAT
automobile O

Figure O
3: O
CINIC-10, B-DAT
airplane O

1 O
gives O
benchmark O
results O
on O
CINIC-10 B-DAT

Figure O
4: O
CINIC-10, B-DAT
bird O

Figure O
5: O
CINIC-10, B-DAT
cat O

Table O
1: O
CINIC-10 B-DAT
benchmarks O

Figure O
6: O
CINIC-10, B-DAT
deer O

Figure O
7: O
CINIC-10, B-DAT
dog O

Figure O
8: O
CINIC-10, B-DAT
frog O

Figure O
9: O
CINIC-10, B-DAT
horse O

Figure O
10: O
CINIC-10, B-DAT
ship O

Figure O
11: O
CINIC-10, B-DAT
truck O

We O
presented O
CINIC-10 B-DAT
in O
this O
technical O
report. O
It O

Antoniou, O
and O
Amos O
J. O
Storkey. O
CINIC-10 B-DAT
Github O
repository O

cinic-10 B-DAT

cinic-10 B-DAT

cinic-10 B-DAT

cinic-10 B-DAT

10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

10 B-DAT
dataset O
as O
a O
plug-in O
extended O

10 B-DAT

10 B-DAT
with O
images O
selected O
and O
downsampled O

this O
reason, O
CIFAR-10 B-DAT
and O
CIFAR- O
100 O
(Krizhevsky, O
2009) O
have O
become O
the O

In O
CIFAR-10, B-DAT
each O
of O
the O
10 O
classes O
has O
6,000 O
examples. O
The O

100 B-DAT
classes O
of O
CIFAR-100 O
only O
have O

100 B-DAT
is O
arguably O
more O
difficult O
than O

10 B-DAT

10 B-DAT

10 B-DAT

10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

10 B-DAT
via O
the O
addition O
of O
downsampled O

10 B-DAT
has O
the O
following O
desirable O
properties O

10 B-DAT
can O
be O
used O
as O
a O

10 B-DAT

10 B-DAT
consists O
of O
images O
from O
both O

10 B-DAT
dataset. O
This O
process O
is O
detailed O

10 B-DAT
The O
original O
CIFAR-10 O
is O
processed O

10 B-DAT

10 B-DAT
class O
(airplane, O
automobile O
etc O

10 B-DAT
data: O
20,000 O
images O
per O
set O

10 B-DAT
data O
among O
all O
three O
sets O

10 B-DAT
test O
set O
contains O
the O
entirety O

10 B-DAT
test O
set, O
as O
well O
as O

10 B-DAT
train O
set. O
CINIC-10’s O
train O
and O

10 B-DAT
can O
be O
fully O
recovered O
from O

10 B-DAT
by O
the O
filename O

10 B-DAT

10 B-DAT
is O
listed O
in O
imagenet- O
contributors.csv O

10 B-DAT
by O
augmenting O
the O
CIFAR-10 O
data O

10 B-DAT
classes O
(airplane, O
automobile, O
etc.). O
synset O

10 B-DAT
data O
and O
the O
remaining O
from O

10 B-DAT
is O
with O
a O
PyTorch5 O
data O

10 B-DAT
(Section O
4.2 O

10 B-DAT
and O
ImageNet O
contributors O
is O
given O

0 O
50 O
100 B-DAT
150 O
200 O
250 O

10 B-DAT

10 B-DAT
(blue) O
and O
ImageNet O
contributions O
(red O

10 B-DAT

10 B-DAT
images O
will O
note O
that O
these O

10 B-DAT
(cows O
and O
goats O
in O
the O

10, B-DAT
automobile O

10, B-DAT
airplane O

10 B-DAT

106 B-DAT
were O
copied O
and O
tested O

10, B-DAT
bird O

10, B-DAT
cat O

10 B-DAT
benchmarks O

10, B-DAT
deer O

10, B-DAT
dog O

10, B-DAT
frog O

10, B-DAT
horse O

Figure O
10 B-DAT

10, B-DAT
ship O

10, B-DAT
truck O

10 B-DAT
in O
this O
technical O
report. O
It O

10 B-DAT
with O
downsam- O
pled O
images O
sourced O

10 B-DAT
(and O
more O
challenging) O
but O
not O

10 B-DAT
Github O
repository O

10 B-DAT

Vision O
(IJCV), O
115(3):211–252, O
2015. O
doi: O
10 B-DAT

1007 B-DAT

10 B-DAT

CINIC B-DAT

CINIC B-DAT

technical O
report O
we O
introduce O
the O
CINIC B-DAT

existing O
benchmarking O
datasets, O
we O
present O
CINIC B-DAT

-10: O
CINIC B-DAT

addition O
of O
downsampled O
ImageNet O
images. O
CINIC B-DAT

as O
in O
CIFAR, O
meaning O
that O
CINIC B-DAT

CINIC B-DAT

our O
method O
of O
constructing O
the O
CINIC B-DAT

among O
all O
three O
sets. O
The O
CINIC B-DAT

from O
the O
CIFAR-10 O
train O
set. O
CINIC B-DAT

can O
be O
fully O
recovered O
from O
CINIC B-DAT

The O
mapping O
from O
sysnsets O
to O
CINIC B-DAT

from O
each O
synset-group O
to O
compile O
CINIC B-DAT

The O
simplest O
way O
to O
use O
CINIC B-DAT

examples O
for O
each O
class O
in O
CINIC B-DAT

show O
randomly O
selected O
samples O
from O
CINIC B-DAT

Figure O
2: O
CINIC B-DAT

Figure O
3: O
CINIC B-DAT

1 O
gives O
benchmark O
results O
on O
CINIC B-DAT

Figure O
4: O
CINIC B-DAT

Figure O
5: O
CINIC B-DAT

Table O
1: O
CINIC B-DAT

Figure O
6: O
CINIC B-DAT

Figure O
7: O
CINIC B-DAT

Figure O
8: O
CINIC B-DAT

Figure O
9: O
CINIC B-DAT

Figure O
10: O
CINIC B-DAT

Figure O
11: O
CINIC B-DAT

We O
presented O
CINIC B-DAT

Antoniou, O
and O
Amos O
J. O
Storkey. O
CINIC B-DAT

CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

technical O
report O
we O
introduce O
the O
CINIC-10 B-DAT
dataset O
as O
a O
plug-in O
extended O

existing O
benchmarking O
datasets, O
we O
present O
CINIC-10 B-DAT

: O
CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

addition O
of O
downsampled O
ImageNet O
images. O
CINIC-10 B-DAT
has O
the O
following O
desirable O
properties O

as O
in O
CIFAR, O
meaning O
that O
CINIC-10 B-DAT
can O
be O
used O
as O
a O

CINIC-10 B-DAT
consists O
of O
images O
from O
both O

our O
method O
of O
constructing O
the O
CINIC-10 B-DAT
dataset. O
This O
process O
is O
detailed O

among O
all O
three O
sets. O
The O
CINIC-10 B-DAT
test O
set O
contains O
the O
entirety O

from O
the O
CIFAR-10 O
train O
set. O
CINIC-10 B-DAT

can O
be O
fully O
recovered O
from O
CINIC-10 B-DAT
by O
the O
filename O

The O
mapping O
from O
sysnsets O
to O
CINIC-10 B-DAT
is O
listed O
in O
imagenet- O
contributors.csv O

from O
each O
synset-group O
to O
compile O
CINIC-10 B-DAT
by O
augmenting O
the O
CIFAR-10 O
data O

The O
simplest O
way O
to O
use O
CINIC-10 B-DAT
is O
with O
a O
PyTorch5 O
data O

examples O
for O
each O
class O
in O
CINIC-10 B-DAT
(Section O
4.2 O

show O
randomly O
selected O
samples O
from O
CINIC-10 B-DAT

Figure O
2: O
CINIC-10, B-DAT
automobile O

Figure O
3: O
CINIC-10, B-DAT
airplane O

1 O
gives O
benchmark O
results O
on O
CINIC-10 B-DAT

Figure O
4: O
CINIC-10, B-DAT
bird O

Figure O
5: O
CINIC-10, B-DAT
cat O

Table O
1: O
CINIC-10 B-DAT
benchmarks O

Figure O
6: O
CINIC-10, B-DAT
deer O

Figure O
7: O
CINIC-10, B-DAT
dog O

Figure O
8: O
CINIC-10, B-DAT
frog O

Figure O
9: O
CINIC-10, B-DAT
horse O

Figure O
10: O
CINIC-10, B-DAT
ship O

Figure O
11: O
CINIC-10, B-DAT
truck O

We O
presented O
CINIC-10 B-DAT
in O
this O
technical O
report. O
It O

Antoniou, O
and O
Amos O
J. O
Storkey. O
CINIC-10 B-DAT
Github O
repository O

CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

technical O
report O
we O
introduce O
the O
CINIC-10 B-DAT
dataset O
as O
a O
plug-in O
extended O

existing O
benchmarking O
datasets, O
we O
present O
CINIC-10 B-DAT

: O
CINIC-10 B-DAT
Is O
Not O
ImageNet O
or O
CIFAR-10 O

addition O
of O
downsampled O
ImageNet O
images. O
CINIC-10 B-DAT
has O
the O
following O
desirable O
properties O

as O
in O
CIFAR, O
meaning O
that O
CINIC-10 B-DAT
can O
be O
used O
as O
a O

CINIC-10 B-DAT
consists O
of O
images O
from O
both O

our O
method O
of O
constructing O
the O
CINIC-10 B-DAT
dataset. O
This O
process O
is O
detailed O

among O
all O
three O
sets. O
The O
CINIC-10 B-DAT
test O
set O
contains O
the O
entirety O

from O
the O
CIFAR-10 O
train O
set. O
CINIC-10 B-DAT

can O
be O
fully O
recovered O
from O
CINIC-10 B-DAT
by O
the O
filename O

The O
mapping O
from O
sysnsets O
to O
CINIC-10 B-DAT
is O
listed O
in O
imagenet- O
contributors.csv O

from O
each O
synset-group O
to O
compile O
CINIC-10 B-DAT
by O
augmenting O
the O
CIFAR-10 O
data O

The O
simplest O
way O
to O
use O
CINIC-10 B-DAT
is O
with O
a O
PyTorch5 O
data O

examples O
for O
each O
class O
in O
CINIC-10 B-DAT
(Section O
4.2 O

show O
randomly O
selected O
samples O
from O
CINIC-10 B-DAT

Figure O
2: O
CINIC-10, B-DAT
automobile O

Figure O
3: O
CINIC-10, B-DAT
airplane O

1 O
gives O
benchmark O
results O
on O
CINIC-10 B-DAT

Figure O
4: O
CINIC-10, B-DAT
bird O

Figure O
5: O
CINIC-10, B-DAT
cat O

Table O
1: O
CINIC-10 B-DAT
benchmarks O

Figure O
6: O
CINIC-10, B-DAT
deer O

Figure O
7: O
CINIC-10, B-DAT
dog O

Figure O
8: O
CINIC-10, B-DAT
frog O

Figure O
9: O
CINIC-10, B-DAT
horse O

Figure O
10: O
CINIC-10, B-DAT
ship O

Figure O
11: O
CINIC-10, B-DAT
truck O

We O
presented O
CINIC-10 B-DAT
in O
this O
technical O
report. O
It O

Antoniou, O
and O
Amos O
J. O
Storkey. O
CINIC-10 B-DAT
Github O
repository O

images O
by O
multiple O
aesthetic O
evaluators. O
AVA B-DAT

Methods O
AVA B-DAT

aesthetic O
evaluators O
(see O
Table O
1,2). O
AVA B-DAT
Dataset O
[31]: O
The O
Aesthetic O
Visual O

Analysis O
(AVA) B-DAT
dataset O
is O
by O
far O
the O

semantic O
tags O
provided O
in O
the O
AVA B-DAT
data O
for O
analysis4, O
covering O
a O

the O
standard O
test O
partition O
of O
AVA B-DAT
(denoted O
as O
Val100) O
for O
evaluation O

to O
nine O
classes O
in O
the O
AVA B-DAT
dataset, O
i.e.,, O
Landscape, O
Seascape, O
Cityscape O

are O
either O
elaboratively O
trained, O
i.e.,, O
AVA B-DAT

Methods O
AVA B-DAT

Marchesotti, O
and O
Florent O
Perronnin. O
2012. O
AVA B-DAT

1,2). O
AVA O
Dataset O
[31]: O
The O
Aesthetic B-DAT
Visual I-DAT
Analysis I-DAT
(AVA) O
dataset O
is O
by O
far O

we O
take O
models O
trained O
on O
COCO B-DAT

COCO B-DAT
78 I-DAT

COCO B-DAT
79 I-DAT

COCO B-DAT
81 I-DAT

trainval. O
”07+12+COCO”: O
first O
train O
on O
COCO B-DAT

COCO B-DAT
75 I-DAT

COCO B-DAT
77 I-DAT

COCO B-DAT
80 I-DAT

trainval. O
”07++12+COCO”: O
first O
train O
on O
COCO B-DAT

3.4 O
COCO B-DAT
To I-DAT
further O
validate O
the O
SSD O

SSD512 O
architec- O
tures O
on O
the O
COCO B-DAT

dataset. O
Since O
objects O
in O
COCO B-DAT

Table O
5: O
COCO B-DAT

show O
some O
detection O
examples O
on O
COCO B-DAT

network O
architecture O
we O
used O
for O
COCO B-DAT

Fig. O
5: O
Detection O
examples O
on O
COCO B-DAT

Method O
VOC2007 O
test O
VOC2012 O
test O
COCO B-DAT

COCO B-DAT
07 I-DAT

COCO B-DAT

region O
proposal O
networks. O
In: O
NIPS. O
(2015 B-DAT

R.: O
Fast O
R-CNN. O
In: O
ICCV. O
(2015) B-DAT
7. O
Erhan, O
D., O
Szegedy, O
C O

arXiv O
preprint O
arXiv:1412.1441 O
v3 O
(2015) B-DAT
9. O
He, O
K., O
Zhang, O
X O

In: O
CVPR. O
(2015) B-DAT
11. O
Hariharan, O
B., O
Arbeláez, O
P O

and O
fine-grained O
localization. O
In: O
CVPR. O
(2015) B-DAT
12. O
Liu, O
W., O
Rabinovich, O
A O

scene O
cnns. O
In: O
ICLR. O
(2015) B-DAT
14. O
Howard, O
A.G.: O
Some O
improvements O

nition. O
In: O
NIPS. O
(2015) B-DAT
16. O
Russakovsky, O
O., O
Deng, O
J O

scale O
visual O
recognition O
challenge. O
IJCV O
(2015 B-DAT

fully O
connected O
crfs. O
In: O
ICLR. O
(2015 B-DAT

results O
on O
the O
PASCAL O
VOC, O
COCO, B-DAT
and O
ILSVRC O
datasets O
confirm O
that O

leading O
results O
on O
PASCAL O
VOC, O
COCO, B-DAT
and O
ILSVRC O
detection O
all O
based O

size O
evaluated O
on O
PASCAL O
VOC, O
COCO, B-DAT
and O
ILSVRC O
and O
are O
compared O

we O
take O
models O
trained O
on O
COCO B-DAT
trainval35k O
as O
described O
in O
Sec O

COCO B-DAT
78.8 O
84.3 O
82.0 O
77.7 O
68.9 O

COCO B-DAT
79.6 O
80.9 O
86.3 O
79.0 O
76.2 O

COCO B-DAT
81.6 O
86.6 O
88.3 O
82.4 O
76.0 O

trainval. O
”07+12+COCO”: B-DAT
first O
train O
on O
COCO O
trainval35k O
then O
fine-tune O
on O
07+12 O

fine-tuned O
from O
models O
trained O
on O
COCO, B-DAT
our O
SSD512 O
achieves O
80.0% O
mAP O

COCO B-DAT
75.9 O
87.4 O
83.6 O
76.8 O
62.9 O

COCO B-DAT
77.5 O
90.2 O
83.3 O
76.3 O
63.0 O

COCO B-DAT
80.0 O
90.7 O
86.8 O
80.5 O
67.8 O

trainval. O
”07++12+COCO”: B-DAT
first O
train O
on O
COCO O
trainval35k O
then O
fine-tune O
on O
07++12 O

3.4 O
COCO B-DAT

SSD512 O
architec- O
tures O
on O
the O
COCO B-DAT
dataset. O
Since O
objects O
in O
COCO O

Table O
5: O
COCO B-DAT
test-dev2015 O
detection O
results O

show O
some O
detection O
examples O
on O
COCO B-DAT
test-dev O
with O
the O
SSD512 O
model O

network O
architecture O
we O
used O
for O
COCO B-DAT
to O
the O
ILSVRC O
DET O
dataset O

Fig. O
5: O
Detection O
examples O
on O
COCO B-DAT
test-dev O
with O
SSD512 O
model. O
We O

Method O
VOC2007 O
test O
VOC2012 O
test O
COCO B-DAT
test-dev2015 O

COCO B-DAT
07++12 O
07++12+COCO O
trainval35k O
0.5 O
0.5 O

accuracy O
on O
PASCAL O
VOC O
and O
COCO, B-DAT
while O
being O
3× O
faster. O
Our O

25. O
COCO B-DAT

i.e. O
Extended O
Cohn-Kanade O
(CKP) O
and O
MMI B-DAT
Facial O
Expression O
Databse O
are O
used O

accuracy O
of O
99.2%. O
For O
the O
MMI B-DAT
dataset, O
currently O
the O
best O
accuracy O

for O
CKP O
and O
98.63% O
for O
MMI, B-DAT
therefore O
performing O
better O
than O
the O

1: O
Example O
images O
from O
the O
MMI B-DAT
(top) O
and O
CKP O
(bottom). O
The O

Section O
4.2) O
and O
on O
the O
MMI B-DAT
Dataset O
(Section O
4.1). O
Typical O
pictures O

4 O
DATASETS O
4.1 O
MMI B-DAT
Dataset O

The O
MMI B-DAT
dataset O
has O
been O
introduced O
by O

shows O
the O
differences O
within O
the O
MMI B-DAT
dataset. O
The O
six O
used O
emotions O

4.3 O
Comparison O
In O
the O
MMI B-DAT
Dataset O
(Fig. O
2) O
the O
emotion O

data O
is O
taken O
from O
the O
MMI B-DAT
set O

MMI: B-DAT
The O
MMI O
Database O
contains O
videos O
of O
peo O

only O
ones O
to O
evaluate O
the O
MMI B-DAT
database O
on O
Emotions O
instead O
of O

in O
emotion O
recognition O
on O
the O
MMI B-DAT
database O
(Section O
4.1 O

experienced O
when O
dealing O
with O
the O
MMI B-DAT
Dataset. O
Since O
the O
first O
two O

10-fold O
cross- O
validation O
on O
the O
MMI B-DAT
Dataset. O
The O
lowest O
accuracy O
is O

dataset O
and O
98.36% O
on O
the O
MMI B-DAT
dataset. O
This O
shows O
that O
the O

and O
Dr. O
Valstar O
for O
the O
MMI B-DAT
data-base O

4.1 O
MMI B-DAT
Dataset O

Allen O
Institute O
for O
Artificial O
Intelligence O
(AI2) B-DAT
for O
a O
re- O
cent O
Kaggle O

Table O
2: O
Performance O
on O
the O
AI2 B-DAT
Kaggle O
questions, O
measured O
by O
precision-at-one O

al. O
(2016) O
also O
tackle O
the O
AI2 B-DAT
Kag- O
gle O
question O
set O
with O

scores O
used O
in O
the O
winning O
Kaggle B-DAT
system O
from O
user O
Cardal O

AI2) O
for O
a O
re- O
cent O
Kaggle B-DAT
challenge. O
The O
training O
set O
contained O

the O
top-performing O
systems O
of O
the O
Kaggle B-DAT
chal- O
lenge. O
These O
consisted O
of O

2: O
Performance O
on O
the O
AI2 O
Kaggle B-DAT
questions, O
measured O
by O
precision-at-one O
(P@1 O

systems O
that O
competed O
in O
the O
Kaggle B-DAT
challenge, O
our O
system O
comes O
in O

Allen O
Institute O
for O
Artificial O
Intelligence O
(AI2) B-DAT
for O
a O
re- O
cent O
Kaggle O

Table O
2: O
Performance O
on O
the O
AI2 B-DAT
Kaggle O
questions, O
measured O
by O
precision-at-one O

al. O
(2016) O
also O
tackle O
the O
AI2 B-DAT
Kag- O
gle O
question O
set O
with O

scores O
used O
in O
the O
winning O
Kaggle B-DAT
system O
from O
user O
Cardal O

AI2) O
for O
a O
re- O
cent O
Kaggle B-DAT
challenge. O
The O
training O
set O
contained O

the O
top-performing O
systems O
of O
the O
Kaggle B-DAT
chal- O
lenge. O
These O
consisted O
of O

2: O
Performance O
on O
the O
AI2 O
Kaggle B-DAT
questions, O
measured O
by O
precision-at-one O
(P@1 O

systems O
that O
competed O
in O
the O
Kaggle B-DAT
challenge, O
our O
system O
comes O
in O

Allen O
Institute O
for O
Artificial O
Intelligence O
(AI2) B-DAT
for O
a O
re- O
cent O
Kaggle O

Table O
2: O
Performance O
on O
the O
AI2 B-DAT
Kaggle O
questions, O
measured O
by O
precision-at-one O

al. O
(2016) O
also O
tackle O
the O
AI2 B-DAT
Kag- O
gle O
question O
set O
with O

scores O
used O
in O
the O
winning O
Kaggle B-DAT
system O
from O
user O
Cardal O

AI2) O
for O
a O
re- O
cent O
Kaggle B-DAT
challenge. O
The O
training O
set O
contained O

the O
top-performing O
systems O
of O
the O
Kaggle B-DAT
chal- O
lenge. O
These O
consisted O
of O

2: O
Performance O
on O
the O
AI2 O
Kaggle B-DAT
questions, O
measured O
by O
precision-at-one O
(P@1 O

systems O
that O
competed O
in O
the O
Kaggle B-DAT
challenge, O
our O
system O
comes O
in O

Allen O
Institute O
for O
Artificial O
Intelligence O
(AI2) B-DAT
for O
a O
re- O
cent O
Kaggle O

Table O
2: O
Performance O
on O
the O
AI2 B-DAT
Kaggle O
questions, O
measured O
by O
precision-at-one O

al. O
(2016) O
also O
tackle O
the O
AI2 B-DAT
Kag- O
gle O
question O
set O
with O

scores O
used O
in O
the O
winning O
Kaggle B-DAT
system O
from O
user O
Cardal O

AI2) O
for O
a O
re- O
cent O
Kaggle B-DAT
challenge. O
The O
training O
set O
contained O

the O
top-performing O
systems O
of O
the O
Kaggle B-DAT
chal- O
lenge. O
These O
consisted O
of O

2: O
Performance O
on O
the O
AI2 O
Kaggle B-DAT
questions, O
measured O
by O
precision-at-one O
(P@1 O

systems O
that O
competed O
in O
the O
Kaggle B-DAT
challenge, O
our O
system O
comes O
in O

proof O
as- O
sistants. O
We O
construct O
CoqGym, B-DAT
a O
large-scale O
dataset O
and O
learning O

that O
AS- O
Tactic O
trained O
on O
CoqGym B-DAT
can O
generate O
effective O
tactics O
and O

CoqGym B-DAT

CoqGym B-DAT
https://github.com/princeton-vl/CoqGym O

CoqGym B-DAT

learning O
envi- O
ronment O
We O
construct O
CoqGym, B-DAT
a O
dataset O
and O
learning O
environment O

The O
learning O
environment O
of O
CoqGym B-DAT
is O
designed O
for O
train- O
ing O

amenable O
to O
learning, O
we O
augment O
CoqGym B-DAT
with O
shorter O
proofs. O
They O
are O

Experimental O
results O
on O
CoqGym B-DAT
show O
that O
ASTactic O
can O
generate O

two- O
fold. O
First, O
we O
build O
CoqGym B-DAT

oped O
a O
similar O
tool O
in O
CoqGym, B-DAT
but O
we O
aim O
for O
a O

theorem O
in O
discrete O
geometry, O
whereas O
CoqGym B-DAT
covers O
more O
diverse O
domains O
including O

4. O
Constructing O
CoqGym B-DAT
CoqGym O
includes O
a O
large-scale O
dataset O
of O

specific O
domains. O
The O
projects O
in O
CoqGym B-DAT
include O
the O
Coq O
standard O
library O

work O
(Huang O
et O
al., O
2019), O
CoqGym B-DAT
supplies O
the O
complete O
environment O
for O

exposing O
its O
internals. O
In O
constructing O
CoqGym, B-DAT
we O
modify O
Coq O
and O
use O

Dataset O
statistics O
CoqGym B-DAT
has O
70,856 O
human-written O
proofs O
from O

training O
and O
testing. O
Statistics O
of O
CoqGym B-DAT
show O
that O
many O
valid O
tactics O

its O
number O
of O
occurrences O
in O
CoqGym B-DAT
and O
manually O
include O
the O
common O

the O
proof O
steps O
extracted O
from O
CoqGym B-DAT

the O
13,137 O
testing O
theorems O
in O
CoqGym B-DAT

in O
Coq. O
We O
have O
constructed O
CoqGym B-DAT

of O
AST. O
Experimental O
results O
on O
CoqGym B-DAT
confirm O
the O
effectiveness O
of O
our O

We O
investigate O
this O
approach O
on O
COMPLEXQUESTIONS, B-DAT
a O
dataset O
designed O
to O
focus O

We O
test O
this O
model O
on O
COMPLEXQUESTIONS B-DAT
(Bao O
et O
al., O
2016), O
a O

obtain O
rea- O
sonable O
performance O
on O
COMPLEXQUESTIONS, B-DAT
and O
analyze O
the O
types O
of O

We O
convert O
COMPLEXQUESTIONS B-DAT
into O
the O
aforementioned O
format, O
and O

Table O
1 O
illustrates O
that O
COMPLEXQUESTIONS B-DAT
is O
dominated O
by O
N-ARY O
questions O

ally O
return O
a O
set, O
in O
COMPLEXQUESTIONS B-DAT
87% O
of O
the O
answers O
are O

most O
common O
non-stop O
words O
in O
COMPLEXQUESTIONS B-DAT

COMPLEXQUESTIONS B-DAT
contains O
1,300 O
training O
examples O
and O

semantic O
parsing O
models O
on O
both O
COMPLEXQUESTIONS B-DAT
and O
WEBQUES- O
TIONS. O
For O
these O

when O
it O
was O
trained O
on O
COMPLEXQUESTIONS, B-DAT
WE- O
BQUESTIONS, O
and O
SIMPLEQUESTIONS. O
In O

fraction O
of O
the O
questions O
in O
COMPLEXQUESTIONS B-DAT
are O
N-ARY). O
Interestingly, O
WEBQA O
performs O

such O
a O
QA O
system O
on O
COMPLEXQUESTIONS B-DAT
and O
find O
that O
it O
obtains O

brain O
tissue O
segmentation O
challenges, O
iSEG O
2017 B-DAT
and O
MRBrainS O
2013, O
with O
the O

brain O
tissue O
segmentation O
challenges, O
iSEG O
2017 B-DAT
and O
MRBrainS O
2013. O
HyperDenseNet O
yielded O

1. O
iSEG O
2017 B-DAT
focuses O
on O
6-month O
infant O
data O

2D O
FCNN O
Chen O
et O
al., O
2017 B-DAT
[19] O
T1,T1-IR,FLAIR O
Brain O
tissue O
3D O

FCNN O
Dolz O
et O
al., O
2017 B-DAT
[17] O
T1,T2 O
Infant O
brain O
tissue O

3D O
FCNN O
Fidon O
et O
al., O
2017 B-DAT
[6] O
T1,T1c,T2,FLAIR O
Brain O
tumor O
CNN O

Kamnitsas O
et O
al., O
2017 B-DAT
[5] O
T1,T1c,T2,FLAIR O

Kamnitsas O
et O
al., O
2017 B-DAT
[22] O
MPRAGE,FLAIR,T2,PD O
Traumatic O
brain O
injuries O

FCNN(Adversarial O
Training) O
Valverde O
et O
al., O
2017 B-DAT
[23] O
T1, O
T2,FLAIR O
Multiple-sclerosis O
3D O

out O
in O
conjunction O
with O
MICCAI O
2017, B-DAT
with O
a O
total O
of O
21 O

2017 B-DAT
organizers O
used O
three O
metrics O
to O

splitting O
the O
10 O
available O
iSEG- O
2017 B-DAT
volumes O
into O
training, O
validation O
and O

2017 B-DAT
challenge O
data. O
The O
first O
point O

2017 B-DAT
challenge O
data. O
The O
first O
point O

2017 B-DAT
data O
for O
HyperDenseNet O
and O
the O

2017 B-DAT
Challenge O
website O
for O
first O
(http://iseg2017.web.unc.edu/rules/results O

2017 B-DAT
for O
6-month O
in- O
fant O
brain O

analysis, O
vol. O
36, O
pp. O
61–78, O
2017 B-DAT

national O
Conference O
on O
MICCAI. O
Springer, O
2017, B-DAT
pp. O
285–293 O

MRI O
segmentation,” O
arXiv O
preprint O
arXiv:1712.05319, O
2017 B-DAT

from O
3D O
MR O
images,” O
NeuroImage, O
2017 B-DAT

International O
Conference O
on O
IPMI. O
Springer, O
2017, B-DAT
pp. O
597–609 O

NeuroImage, O
vol. O
155, O
pp. O
159–168, O
2017 B-DAT

neonatal O
brain O
MRI O
segmentation,” O
NeuroImage, O
2017 B-DAT

MRI: O
A O
large-scale O
study,” O
NeuroImage, O
2017 B-DAT

and O
random O
walk,” O
Medical O
Physics, O
2017 B-DAT

Pattern O
Analysis O
and O
Machine O
Intelligence, O
2017 B-DAT

connections O
on O
learning.” O
in O
AAAI, O
2017, B-DAT
pp. O
4278–4284 O

Proceedings O
of O
the O
IEEE O
CVPR, O
2017 B-DAT

segmentation O
from O
CT O
volumes,” O
arXiv:1709.07330, O
2017 B-DAT

Confer- O
ence O
on O
MICCAI. O
Springer, O
2017, B-DAT
pp. O
287–295 O

Computer O
Vision O
and O
Pattern O
Recognition, O
2017, B-DAT
pp. O
4373–4382 O

in O
Multimedia O
and O
Expo O
(ICME), O
2017 B-DAT
IEEE O
International O
Conference O
on. O
IEEE O

, O
2017, B-DAT
pp. O
355–360 O

mobile O
devices,” O
arXiv O
preprint O
arXiv:1707.01083, O
2017 B-DAT

2017 B-DAT
challenge,” O
IEEE O
Transactions O
on O
Medical O

medical O
images,” O
arXiv O
preprint O
arXiv:1711.06853, O
2017 B-DAT

multi-modal O
brain O
tissue O
segmentation O
challenges, O
iSEG B-DAT
2017 O
and O
MRBrainS O
2013, O
with O

multi-modal O
brain O
tissue O
segmentation O
challenges, O
iSEG B-DAT
2017 O
and O
MRBrainS O
2013. O
HyperDenseNet O

1. O
iSEG B-DAT
2017 O
focuses O
on O
6-month O
infant O

challenges: O
infant O
brain O
tissue O
segmentation, O
iSEG B-DAT
[49], O
and O
adult O
brain O
tissue O

results, O
com- O
piled O
by O
the O
iSEG B-DAT
challenge O
organizers O
on O
testing O
data O

3.1 O
iSEG B-DAT
Challenge O
The O
focus O
of O
this O

3.1.1 O
Evaluation O
The O
iSEG B-DAT

by O
splitting O
the O
10 O
available O
iSEG B-DAT

the O
first O
round O
of O
the O
iSEG B-DAT
Challenge, O
as O
well O
as O
to O

TABLE O
5 O
Results O
on O
the O
iSEG B-DAT

an O
updated O
ranking, O
see O
the O
iSEG B-DAT

SKKU O
teams O
participated O
in O
both O
iSEG B-DAT
and O
MRBrains2013 O
challenges. O
While O
these O

out- O
performed O
HyperDenseNet O
in O
the O
iSEG B-DAT
challenge, O
the O
performance O
of O
our O

the O
two O
modalities O
used O
in O
iSEG, B-DAT
these O
results O
suggest O
that O
Hyper O

with O
two O
modalities, O
for O
both O
iSEG B-DAT
and O
MRBrainS O
chal- O
lenges. O
As O

for O
Hyper- O
DenseNet O
trained O
on O
iSEG B-DAT
(top O
row O
of O
Fig O
8 O

of O
two O
highly O
competitive O
challenges, O
iSEG B-DAT

would O
like O
to O
thank O
both O
iSEG B-DAT
and O
MRBrainS O
organizers O
for O
providing O

in O
HyperDenseNet O
trained O
on O
the O
iSEG B-DAT
(top) O
and O
MRBrainS O
(from O
2nd O

iSEG B-DAT
Images O
were O
acquired O
at O
the O

3.1 O
iSEG B-DAT
Challenge O

3.1 O
iSEG O
Challenge B-DAT
The O
focus O
of O
this O
challenge O

Challenge B-DAT
results: O
Table O
5 O
compares O
the O

first O
round O
of O
the O
iSEG O
Challenge, B-DAT
as O
well O
as O
to O
all O

3.2 O
MRBrainS O
Challenge B-DAT
The O
MRBrainS O
challenge O
was O
initially O

updated O
ranking, O
see O
the O
iSEG-2017 O
Challenge B-DAT
website O
for O
first O
(http://iseg2017.web.unc.edu/rules/results/) O
and O

Challenge B-DAT
results: O
The O
MRBrainS O
challenge O
organiz O

updated O
ranking, O
see O
the O
MRBrainS O
Challenge B-DAT
website O
(http://mrbrains13.isi.uu.nl/results.php O

machine O
learning O
approach,” O
MICCAI O
Grand O
Challenge B-DAT

ceedings O
of O
the O
MICCAI O
Grand O
Challenge B-DAT

3.1 O
iSEG O
Challenge B-DAT

3.2 O
MRBrainS O
Challenge B-DAT

MRI), O
employing O
the O
publicly O
available O
CHAOS B-DAT
dataset. O
Results O
show O
that O
the O

bined O
Healthy O
Abdominal O
Organ O
Segmentation O
(CHAOS) B-DAT
Challenge O
2 O
[52], O
[53], O
[54 O

other O
state-of-the-art O
architectures O
on O
the O
CHAOS B-DAT
dataset. O
The O
values O
show O
the O

Combined O
Healthy O
Abdominal O
Organ O
Segmentation O
(CHAOS) B-DAT
Challenge. O
We O
provided O
extensive O
ex O

on O
several O
subjects O
on O
the O
CHAOS B-DAT
Challenge O
dataset. O
The O
proposed O
multi-scale O

1) O
Dataset B-DAT

IV-A1 O
Dataset B-DAT

segmentation O
on O
magnetic O
resonance O
imaging O
(MRI B-DAT

segmentation O
on O
magnetic O
resonance O
imaging O
(MRI), B-DAT
employing O
the O
publicly O
available O
CHAOS O

1) O
Dataset: O
The O
abdominal O
MRI B-DAT
dataset O
from O
the O
Com- O
bined O

segmentation O
of O
abdominal O
organs O
on O
MRI B-DAT
(T1-DUAL O
in O
phase). O
This O
dataset O

acquired O
by O
a O
1.5T O
Philips O
MRI, B-DAT
producing O
12 O
bit O
DICOM O
images O

Chaos O
dataset O
(multi-organ O
segmentation O
on O
MRI B-DAT
task). O
The O
values O
show O
the O

approach O
we O
conducted O
experiments O
on O
MRI B-DAT
scans O
(T1-DUAL) O
from O
the O
Combined O

Deep O
learning O
techniques O
for O
automatic O
MRI B-DAT
cardiac O
multi-structures O
segmentation O
and O
diagnosis O

of O
subcortical O
brain O
structures O
on O
MRI B-DAT
for O
radiotherapy O
and O
radiosurgery: O
a O

networks O
for O
subcortical O
segmentation O
in O
MRI B-DAT

of O
bladder O
cancer O
structures O
in O
MRI B-DAT
with O
progressive O
dilated O
convolutional O
networks O

hybrid O
transportation O
modes O
from O
sparse O
GPS B-DAT
data O
using O
a O
moving O
window O

hybrid O
transportation O
modes O
from O
sparse O
GPS B-DAT
data O
using O
a O
moving O
window O

the O
data O
collected O
by O
using O
GPS B-DAT
devices O
so O
that O
the O
cost O

solve O
a O
classification O
problem O
of O
GPS B-DAT
data O
into O
different O
transportation O
modes O

framework O
was O
tested O
using O
coarse-grained O
GPS B-DAT
data, O
which O
has O
been O
avoided O

data. O
Among O
these O
practices O
are O
GPS B-DAT

surveys, O
where O
participants O
carry O
a O
GPS B-DAT
device O
for O
a O
certain O
duration O

infer O
the O
transportation O
mode O
from O
GPS B-DAT
data. O
This O
inference O
could O
largely O

and O
acceleration O
and O
length O
between O
GPS B-DAT
fixes. O
However, O
none O
of O
the O

low O
each O
other O
in O
a O
GPS B-DAT
sequence O
and O
that O
every O
two O

GPS B-DAT
consec- O
utive O
fixes O
are O
analysed O

solve O
a O
classification O
problem O
of O
GPS B-DAT
data O
into O
different O
transporta- O
tion O

the O
data-related O
issues O
by O
collecting O
GPS B-DAT
data O
for O
a O
rec- O
ommended O

framework. O
We O
also O
describe O
the O
GPS B-DAT
data O
collected O
for O
this O
re O

to O
be O
used O
to O
classify O
GPS B-DAT
points O
into O
transportation O
modes O
(Mitchell O

modes O
from O
the O
collected O
sparse O
GPS B-DAT
data, O
without O
information O
or O
assumptions O

and O
acceleration O
values O
calculated O
from O
GPS B-DAT
data. O
Due O
to O
its O
high O

research. O
Lastly, O
we O
describe O
the O
GPS B-DAT
data O
collected O
for O
this O
research O

In O
order O
to O
de-construct O
a O
GPS B-DAT
track, O
some O
definitions O
have O
been O

route O
between O
any O
two O
consecutive O
GPS B-DAT
points O
is O
called O
a O
segment O

fer O
the O
transportation O
mode O
from O
GPS B-DAT
data O
collected O
by O
travel O
sur O

Stenneth O
et O
al., O
2011); O
strictly O
GPS B-DAT
devices O
alone O
(Chung O
& O
Shalaby O

the O
temporal O
granularity O
of O
the O
GPS B-DAT
data O
(also O
called O
the O
epoch O

on O
battery O
restrictions O
on O
current O
GPS B-DAT
devices O
or O
smart O
phones, O
but O

these O
data-related O
issues O
by O
collecting O
GPS B-DAT
data O
for O
this O
study O
for O

corresponding O
sample O
size O
procedures O
for O
GPS B-DAT

are O
calculated O
from O
the O
collected O
GPS B-DAT
data O

the O
least O
num- O
ber O
of O
GPS B-DAT
fixes O
since O
nearly O
half O
of O

which O
causes O
the O
loss O
of O
GPS B-DAT
coverage. O
In O
this O
case, O
a O

this O
dataset. O
Outliers O
due O
to O
GPS B-DAT
errors O
are O
accounted O
for O
and O

Sampling O
for O
this O
kind O
of O
GPS B-DAT

framework O
used O
to O
classify O
the O
GPS B-DAT
segments O
into O
transportation O
modes. O
The O

infer O
the O
transportation O
mode O
from O
GPS B-DAT
data O
has O
extended O
from O
logical O

the O
process O
by O
segmenting O
the O
GPS B-DAT
track O
into O
trips, O
based O
on O

dependant O
on O
segmentation O
classify O
each O
GPS B-DAT
segment O
individually O
into O
a O
transportation O

based O
on O
SVMs O
to O
classify O
GPS B-DAT
segments O
into O
respective O
transportation O
modes O

error O
in O
training O
(due O
to O
GPS B-DAT
errors), O
and O
since O
the O
data O

e.g. O
the O
sequence O
of O
a O
GPS B-DAT
trajectory’s O
move- O
ments: O
such O
as O

The O
loss O
of O
GPS B-DAT
coverage O
due O
to O
indoor O
activity O

to O
study O
se- O
quences O
of O
GPS B-DAT
trajectory O
movements O
rather O
than O
each O

the O
learning O
process O
to O
consequent O
GPS B-DAT
data O
that O
represent O
the O
variabil O

there O
will O
not O
be O
any O
GPS B-DAT
fixes O
attainable O
at O
several O
areas O

inferring O
transportation O
mode O
from O
sparse O
GPS B-DAT
data. O
We O
first O
provide O
the O

the O
transportation O
mode O
from O
sparse O
GPS B-DAT
data O
without O
any O
extra O
information O

work. O
Finally, O
different O
rate O
of O
GPS B-DAT
data O
collection O
could O
be O
compared O

M. O
(2009). O
National O
travel O
survey O
GPS B-DAT
feasibility O
study: O
Final O
report. O
Department O

A., O
& O
Cheng, O
T. O
(2010). O
GPS B-DAT
data O
collection O
setting O
for O
pedestrian O

for O
studying O
transportation O
modes O
from O
GPS B-DAT
data. O
In O
Proceedings O
of O
Transport O

A O
trip O
reconstruction O
tool O
for O
GPS B-DAT

Extracting O
places O
and O
activities O
from O
GPS B-DAT
traces O

Axhausen, O
K. O
W. O
(2009). O
Processing O
GPS B-DAT
raw O
data O
without O
additional O
information O

Deducing O
mode O
and O
purpose O
from O
GPS B-DAT
data. O
In O
The O
87th O
annual O

Xu, O
Y. O
(2010). O
Effective O
GPS B-DAT

Understanding O
transportation O
modes O
based O
on O
GPS B-DAT
data O
for O
Web O
applications. O
ACM O

Learning O
transportation O
mode O
from O
raw O
GPS B-DAT
data O
for O
geographic O
applications O
on O

hybrid O
transportation O
modes O
from O
sparse O
GPS B-DAT
data O
using O
a O
moving O
window O

Product-Aware O
Answer B-DAT
Generation O
in O
E-Commerce O
Question-Answering O

Product-Aware O
Answer B-DAT
Generation O
in O
E-CommerceQuestion-Answering O

and O
Rui O
Yan. O
2019. O
Product-Aware O
Answer B-DAT
Generation O
in O
E-Commerce O
Question- O
Answering O

Answer B-DAT
generator. O
(1) O
Review O
reader: O
(See O

Answer B-DAT
Decoder O

Generated O
Answer B-DAT

Generated O
Answer B-DAT

Answer B-DAT
Encoder O

Ground O
Truth O
Answer B-DAT

Ground O
Truth O
Answer B-DAT

ing O
of O
Candidate O
Extraction O
and O
Answer B-DAT
Selection O
for O
Reading O
Comprehension. O
In O

Ming O
Zhou. O
2018. O
S-Net: O
From O
Answer B-DAT
Extraction O
to O
Answer O
Synthesis O
for O

and O
Wai O
Lam. O
2018. O
Aware O
Answer B-DAT
Prediction O
for O
Product-Related O
Questions O
Incorporating O

Product-Aware O
Answer O
Generation O
in O
E-Commerce O
Question B-DAT

CCS O
CONCEPTS O
• O
Information O
systems→ O
Question B-DAT
answering O

KEYWORDS O
Question B-DAT
answering, O
e-commerce, O
product-aware O
answer O
genera O

Product-Aware O
Answer O
Generation O
in O
E-Commerce O
Question B-DAT

Question B-DAT
hidden O
states O

Question B-DAT
aware O
attention O
Question O
aware O
attention O

Question B-DAT
Encoder O

Question B-DAT
Aware O
Review O
Representation O

maps, O
shown O
in O
Figure O
2. O
Question B-DAT
of O
the O
left O
figure O
in O

2017. O
An O
Abstractive O
approach O
to O
Question B-DAT
Answering. O
arXiv O
preprint O
arXiv:1711.06238 O
(2017 O

Ester. O
2011. O
AQA: O
Aspect-based O
Opinion O
Question B-DAT
Answering. O
(2011), O
89–96 O

Summarizing O
Answers O
in O
Non-Factoid O
Community O
Question B-DAT

for O
Transfer O
Learning O
on O
Retrieval-based O
Question B-DAT
Answering O
Systems O
in O
E-commerce. O
In O

Zhaochun O
Ren O
JD B-DAT

Yihong O
Zhao O
JD B-DAT

Dawei O
Yin O
JD B-DAT

performed O
during O
an O
internship O
at O
JD B-DAT

Product B-DAT

Product B-DAT

Yin, O
and O
Rui O
Yan. O
2019. O
Product B-DAT

ing O
comprehension, O
and O
sequence-to-sequence O
architecture. O
Product B-DAT

2016. O
Addressing O
Complex O
and O
Subjective O
Product B-DAT

2018. O
Aware O
Answer O
Prediction O
for O
Product B-DAT

New O
Orleans, O
Louisiana, O
June O
5, O
2018 B-DAT

2018 B-DAT
Association O
for O
Computational O
Linguistics O

SLAM O
2018 B-DAT
focuses O
on O
predicting O
a O
student’s O

The O
SLAM O
2018 B-DAT
Shared O
Task O
is O
primarily O
cen O

acquisi- O
tion O
(Settles O
et O
al., O
2018) B-DAT
of O
non-native O
learners O
of O
English O

M. O
Hagiwara, O
and O
N. O
Madnani. O
2018 B-DAT

SLAM B-DAT
2018 O
focuses O
on O
predicting O
a O

The O
SLAM B-DAT
2018 O
Shared O
Task O
is O
primarily O

SLAM B-DAT
2018 I-DAT
focuses O
on O
predicting O
a O
student’s O

The O
SLAM B-DAT
2018 I-DAT
Shared O
Task O
is O
primarily O
cen O

SLAM B-DAT
2018 I-DAT
focuses O
on O
predicting O
a O
student’s O

The O
SLAM B-DAT
2018 I-DAT
Shared O
Task O
is O
primarily O
cen O

Street B-DAT
Image O
Crawler O
A O
city O
street O

was O
built O
using O
the O
Google O
Street B-DAT
View O
API O
V3.0 O
along O
with O

Street B-DAT
Image O
Crawler O

crawler O
was O
built O
using O
the O
Google B-DAT
Street O
View O
API O
V3.0 O
along O

ShapeNet-Core O
[5], O
Shapenet- O
Part O
[42], O
ModelNet40 B-DAT
[40] O
and O
3DMatch O
benchmark O
[45 O

by O
transfer O
learning O
on O
the O
ModelNet40 B-DAT
dataset. O
Networks O
are O
trained O
out O

ShapeNet-Core O
[5], O
Shapenet- O
Part O
[42], O
ModelNet40 B-DAT
[40] O
and O
3DMatch O
benchmark O
[45 O

by O
transfer O
learning O
on O
the O
ModelNet40 B-DAT
dataset. O
Networks O
are O
trained O
out O

